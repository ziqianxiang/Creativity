{
    "Decision": {
        "metareview": "Dear authors,\n\nThe topic of variance reduction in optimization is timely and the reviewers appreciated your attempt at circumventing the issues faced with the current popular methods.\n\nThey however had a concern about the significance of the results, which I echo:\n- First, there have been previous attempts at variance reduction which share some similarity with yours, for instance \"No more pesky learning rate\", \"Topmoumoute online natural gradient algorithm\" or even Adam (which does variance reduction without mentioning it).\n- The fact that previous similar methods exist is a non-issue should yours perform better. However, the absence of stepsize tuning in the experimental evaluation is a big issue as the performance of an iterative algorithm is highly sensitive to it.\n\nFinally, the link between flatness of the minimum and generalization is dubious, as mentioned for instance by Dinh et al. (2017).\n\nAs a consequence, I cannot accept this work for publication to ICLR but I encourage you to address the points of the reviewers should you wish to resubmit it to a future conference.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Issues with the experiments"
    },
    "Reviews": [
        {
            "title": "Solid algorithm and results, some concerns, tiny font",
            "review": "The paper considers SGD with a scaled norm; in the non-stochastic case (first equation in section 2.1), it is gradient descent in a fixed non-Euclidean norm, but it is the stochastic case that is most interesting. The paper connects this, somewhat, to a Hamilton-Jacobi equation, but then relaxes the implicit step to an explicit step.\n\nThere is solid theory (Prop 2, 3 and 4) for convergence, which makes sense since this is the same as usual SGD but in a different Hilbert space. Since the inner product is stationary, it's just a fixed Hilbert space, so any convergence proofs that work for arbitrary Hilbert space immediately give the result.\n\nThe computational experiments are impressive, and demonstrate a lot of competence with modern neural nets. Some results are hard to interpret (Figs 9, 10) though.\n\nAs for why use the Laplacian, Prop 8 (combined with Prop 6) gives some idea: that we lower the variance, without cheating (ie., we could trivially lower the variance by just multiplying by a small number, but because the operator preserves the sum of the components, it is not \"cheating\").  That is helpful, though it doesn't give a complete picture yet.  The explanation about the link to the \"more convex\" function I find completely inaccurate and misleading (see technical comments for why).\n\nThe writing is mainly fine, though some sentences are written poorly and would benefit from a revision, e.g., 2nd paragraph, \"But none of them is suitable to train deep neural nets (DNNs).\" is quite awkward [also, in this sentence, please explain *why* they are not suitable!]\n\nThe paper circumvents the page limit by using a smaller font (starting on page 3). This might seem like a minor issue, but it is violating the page limit, and not fair to other papers (unless I have misunderstood; the meta-reviewers can probably comment about this).  I do not think it would be unfair to reject the paper on these grounds. It leaves a bad taste in my mouth after reading the paper.\n\n\nTechnical comments:\n\n- page 1, this is called a \"tri-diagonal\" linear system, but it is not, it is circulant due to the upper-right and lower-left entries (the authors are well-aware of this, but the reader maybe confused; especially since if it were tri-diagonal, it would be inverted via the Thomas algorithm not the FFT).\n\n- Section 2: my first impression on reading this is that you've re-discovered the proximal point envelope and the Moreau envelope (and, looking at the proof of Prop. 1, the authors are aware of this connection).  In this context, it's not clear why A_sigma is helpful, as opposed to any positive definite matrix.\n\n- The actual statement of Proposition 1 is unclear. What does \"the ... update ... permits ..\" mean? i.e., \"permits\" is a weird, vague choice of words. What are you actually proving?\n\n- Section 2.1 moves from the proximal point method (in a scaled norm) to the gradient descent method (in a scaled norm). Clearly, these two methods are different, and just as in ODE schemes, the implicit version is unconditionally stable while the explicit one isn't. So motivating your method by \"smoothing\" or \"adding convexity\" is really misleading. You could define equation (2) and the u(w,t) equation by replacing A_sigma with the identity, and as long as tau > 0, this also \"convexifies\", but then if you go from implicit to explicit, you get regular GD, so you haven't really done anything.  So, I do not buy this connection that your method \"convexifies\" the function.\n\n- Using the FFT to invert seems slow (the theoretical flop-count is good, but it's still super-linear, and requires global data movement, so not good for a distributed implementation).  If you really did define A to be tri-diagonal, then you could invert naively in a linear time algorithm with local data movement. Why not use a tri-diagonal A? It might not satisfy prop 8 exactly, but it'd be close, and a lot faster in practice.  From a \"finite-difference\" point-of-view, I don't see an inherent argument about why you want circular boundary conditions.\n\n- Remark 1 seems out-of-place. Why is that included?\n\n- Section 3.2, and Fig. 5.  It's not clear that the improved generalization results are due to broader local minima, or if it's because the methods converged faster on the training data (since they were limited to 200 epochs). Showing the training error, as a function of epoch, would help clarify. Similar comment for other experiments too.\n\n- Section 4 was impressive in the implementations. Nice work.\n\n- The acknowledgments used the boiler-plate latex template text.\n\n** summary **\nQuality: Good\nClarity: OK\nOriginality: mixed\nSignificance: maybe high?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theory and experiments can be improved",
            "review": "This paper proposed a variant of gradient descent that can be approximately understood as gradient descent on a smoothed version of the objective function. The motivation of this work is finding flat minima which could imply better generalization ability of a machine learning model.\nCompared with gradient, the proposed algorithm uses gradient multiplied by a special constant square matrix as update direction. The complexity of the matrix vector multiplication is brought down from O(d^2) to O(d*logd) by exploiting special structure of the matrix using FFT. It is proved that the new update vector has smaller variance and amplitude compared to gradient. \nExperiments on different applications showed that the proposed algorithm may have better generalization ability compared with SGD.\nThis is a clearly written paper, but I have a few questions about theoretical gaps and simulation results in the paper.\n\na). It seems the smoothing explanation at the beginning of section 2 is for implicit scheme (equation (3)). However, the explicit scheme used in practice (the first unnumbered equation in section 2.1) uses a heuristic relaxation which makes the smoothing explanation “approximate” for the explicit scheme. Since the implicit scheme is much more complicated than the explicit scheme, I don’t know if the argument for the implicit scheme will “approximately” hold for the explicit scheme used in practice.\n\nb). The concept flat minimum is only useful in nonconvex optimization, but the convergence of the algorithm is only proved in convex setting. Since the main motivation of the algorithm is finding flat minima, the lack of convergence proof for nonconvex setting concerns me.\n\nc). In the neural net experiment in section 4.1, both gradient descent and smooth gradient descent use the same stepsizes. It is known that the performance of gradient descent is sensitive to the choice of stepsizes, for a fair comparison, one should compare the performance of the two algorithms using optimized stepsizes.\n\nd). In the experiment in section 4.2, the proposed algorithm is only used for the first 40 epochs during training and SGD is used for the later phase of training. Why switching to SGD later? \n\nOverall, I feel the idea of this paper is interesting, but the theory and experiments in the paper are not very strong.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some concerns on experiments and written style",
            "review": "The paper proposes to use a simple tri-diagonal matrix to reduce the variance of stochastic gradient and provide a better generalization property. Such a variant is shown to be equivalent to applying GD on smoothed objective function. Theoretical results show a convergence rate and variance reduction. Various experiments are done in different settings.  I have following comments:\n\n1) In section 2, it is stated that \"This viscosity solution u(w, t) makes f(w) more convex by bringing down the local maxima while retaining the wide minima.\" Besides illustrating such a point on some nicely constructed function f, is there any theory or analysis supporting this statement? Or is there any intuition behind it? In the abstract and Section 1, how to define a function is \"more convex\"? This is one of the fountains of the paper, it worths to spend one or two paragraphs to explain it, or at least introduce some references here. The current statement is not formed in a rigorous way.\n\n2) The main advantages of proposed method that the paper claims are, reduce the variance and improve the generalization accuracy. However, there are few comparisons with other existed methods, besides numerical section. Such comparisons or analysis could help readers understand the difference and novelty.\n\n3) The proof seems fine. Propositions 1-4 try to analyze the convergence rate, which are common techniques in other variance reduction papers on SGD. Propositions 5-9 rely on some nice properties of matrix A_\\sigma and show it can help to reduce the variance. Typos:\nPage 11, \"Proof of Proposition 1\", there is a missing \"-\" in \\nabla_w u(w, t), also in the next equation.\nPage 13, \"Proof of Proposition 6, d = A_\\sigma g\". \n\n4) The proposed method strongly relies on the choice of \\sigma, but discussion on how to choose the value for \\sigma is rare. From Proposition 8, the upper bound on reduced variance is a quadratic function on \\sigma, so it is better to discuss more on it or have some experiments on sensitivity analysis. In Section 4, \\sigma varies (1.0, 3.0, etc) in different experiments, but again there are no explanations.\n\n5) Numerical results in Section 4.3 is not strong enough to support the advantage of the proposed method. It is hard to observe \"visibly less noisy\" in both Figure 8 and 9. Better ways of illustration might be considered.\n\n6) The paper is not nicely written thus cannot be easily read. It seems to be cut and pasted from another version in a short time. Some titles of subsections is missing. The font size is not fixed in the whole paper.\n\nThe above concerns prevents me to give a higher rating at this time.\n\nSummary\nquality: ok\nclarity: good\noriginality: nice\nsignificance: good",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}