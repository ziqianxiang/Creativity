{
    "Decision": {
        "metareview": "This paper focuses on  communication efficient Federated Learning (FL) and proposes an approach for  training  large models on heterogeneous edge devices.   The paper is well-written and the approach is promising, but all reviewers pointed out that both novelty of the approach and empirical evaluation, including comparison with state-of-art, are somewhat limited. We hope that suggestions provided by the reviewers will be helpful for extending and improving this work.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A well-written paper addressing an important problem, but somewhat limited novelty and empirical evaluation"
    },
    "Reviews": [
        {
            "title": "The paper presents some new approaches for communication efficient Federated Learning that allows for training of large models on heterogeneous edge devices.",
            "review": "The paper presents some new approaches for communication efficient Federated Learning (FL) that allows for training of large models on heterogeneous edge devices. In FL, heterogeneous edge devices have access to potentially non-iid samples of data points and try to jointly learn a model by averaging their local models at a parameter server (the cloud). As the bandwidth of the up/downlink-link may be limited communication overheads may become the bottleneck during FL. Moreover, due to the heterogeneity of the hardware, large models may be hard to train on small devices. Due to that, there are several recent approaches that aim to minimize communication via methods of quantization, which also aim to allow for smaller models via methods of compression and model quantization.\n\nIn this paper, the authors suggest a combination of two methods to reduce communication and allow for large model training by 1) using a lossy compressed model when that is communicated from the cloud to the edge devices, and 2) subsampling the gradients, a form of dropout, at the edge device side that allows for an overall smaller model update. The novelty of either of those techniques is quite limited as individually they have been suggested before, but the combination of both of them is interesting. \n\nThe paper is overall well written, however there are two aspects that make the contribution lacking in novelty. First of all, the presented methods are a combination of existing techniques, that although interesting to combine together, are neither theoretically analyzed nor extensively tested. The model/update quantization technique has been used in the past extensively [eg 1-3]. Then, the “federated dropout” can be seen as a “coordinate descent” type of a technique, i.e., randomly zeroing out gradient elements per iteration. \n\nSince this is a more experimental paper, the setup tested is quite limited in its comparisons. For example, one would expect to see extensive comparisons with methods for quantizing gradients, eg QSGD, or Terngrad, and combinations of that with DeepCompression. Although the authors do make an effort to experiment with a different set of hyperparameters (dropout probability, quantization levels, etc), a comparison with state of the art methods is lacking.\n\nOverall, although the combination of the presented ideas has some merit, the lack of extensive experiments that would compare it with the state of the art is not convincing, and the overall effectiveness of this method is unclear at this point.\n\n[1] https://arxiv.org/pdf/1510.00149.pdf\n[2] https://arxiv.org/pdf/1803.03383.pdf\n[4] https://arxiv.org/pdf/1610.05492.pdf",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper adress the ressource issue of federated learning by introducing a lossy compression on the global model and what they coin a Federated Dropout. While not completely familiar with compression schemes, I saw a couple of statements requiring formal support.",
            "review": "The paper tackles a major issue in distributed learning in general (and not only the federated scheme), which is communication bottleneck.\n\nI am not fully qualified to judge and would rather listen to the opinion of more qualified reviewers, I was annoyed by some aspects of the paper:\n\n1) many claims required formal support (proofs), as an example: \"more aggressive dropout rates ted to slow down the convergence rate of the model, even if they sometimes result in a higher accuracy\" is a statement that would benefit from analyzing the dropout out effect on convergence, something that wouldn't be hard to do given the extensive theoretical toolbox on distributed optimization.\n\n2) no comparison with other compression schemes (see e.g. Alistarh et al.'s ZipML (NIPS or ICML 2017) and followups)\n\n3) proving an unbiased-ness guarantee out of the Probabilistic quantization (section 3.1) would have been a minimal requirement in my opinion.\n\nI encourage the authors to further expand those points, but would happily lighten-up my skepticism if more qualified reviewers say that we do not need such guarantees as the one in point 1 and 3. (the few compression papers I know provide that)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper focuses on lossy compression techniques and federated dropout strategies to control the update burden that’s needed to coordinate nodes in a federated learning setting. ",
            "review": "The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. \n\nThe lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed “less” important if a notion of coefficient importance can be derived? \n\nCan you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? \n\nFor the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. \n\nOn the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more \"rounds\" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}