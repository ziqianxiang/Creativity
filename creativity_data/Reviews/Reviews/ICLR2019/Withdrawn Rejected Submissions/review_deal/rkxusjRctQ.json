{
    "Decision": {
        "metareview": "The paper proposes a method that learns mapping implicitly, by using a generative query network of Eslami et al. with an attention mechanism to learn to predict egomotion. The empirical findings is that training for egomotion estimation alongside the generative task of view prediction helps over a discriminative baseline, that does not consoder view prediction. The model is tested in Minecraft environments. \nA comparison to some baseline SLAM-like method, e.g., a method based on bundle adjustment, would be important to include despite beliefs of the authors that eventually learning-based methods would win over geometric methods.  For example, potentially environments with changes can be considered, which will cause the geometric method to fail, but the proposed learning-based method to succeed.\n\nMoreover, there are currently learning based methods for the re-localization problem that the paper would be important to compare against (instead of just cite), such as \"MapNet: An Allocentric Spatial Memory for Mapping Environments\" of Henriques et al.  and \"Active Neural Localization\" of Chaplot et al. . In particular, Mapnet has a generative interpretation by using cross-convolutions as part of its architecture, which generalize very well, and which consider the geometric formation process. The paper makes a big distinction between generative and discriminative, however the architectural details behind the egomotion estimation network are potentially more or equally important to the loss used. This means, different discriminative networks depending on their architecture may perform very differently. Thus, it would be important to present quantitative results against such methods that use cross-convolutions for egomotion estimation/re-localization. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "lack of experiments to obvious geometric baselines and previous learning-based methods for localization "
    },
    "Reviews": [
        {
            "title": "Very informative and great paper",
            "review": "This paper proposes generative approaches to localization without explicit high definition geometric maps. A generative baseline (GQN) and an extension to that with attention is introduced in the context of localization. \n\nThe paper is clearly written, and the relevant previous work is discussed to satisfying degree. \n\nFigures 2 and 3 help understanding the GQN and the proposed attention version a lot. \n\nI am intrigued with the result presented in table 1. especially with the fact that attention is helping the generative method quite a bit, but not so much for the discriminative method. This is not discussed in detail in the paper, I suggest the authors expand their discussion a bit in this direction. \n\nThe second suggestion is in terms of the data. I understand the motivations behind using the minecraft world. however, real data is still quite different than this data, think about various differences in the real world at time of training and test even at the same location texture of sky and lighting will change. On top of this, there is quite a bit of variance in levels of detail compared to the monotonic minecraft world. I suggest using a real dataset for another set of experiments. This can be added as an appendix. \n\nIn general, very good motivation, and intriguing work for the community -- localization with high definition geometric works is tough to scale, and implicit world representations are an important piece in relaxing this dependency. I believe this paper will motive more future work. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak evaluation",
            "review": "**Summary of the paper**\n\nThis paper studies the problem of visual re-localization, where we are interested in estimating the camera pose of a new image from a set of source images and their camera poses. Instead of explicitly designing the structure of a map of 3D scenes (e.g. occupancy grids or point clouds), the paper proposes implicitly learning an abstract map representation. Specifically, the paper proposes a generative method based on Generative Query Networks (GQNs) augmented with an attention mechanism. The authors apply this model to the visual re-localization problem. To train and test the proposed model, the authors introduce the Minecraft random walk dataset, which consists of images and their camera poses extracted from randomly generated trajectories in the Minecraft environment. The proposed model is compared against a discriminative counterpart, which is trained to directly predict the target camera pose and achieves better MSE.\n\n**Clarity**\n\nAbove average\n\n**Significance**\n\nBelow Average\n\n**Detailed comments**\n\n_Paper Strengths_\n\n- The idea of leveraging generative models' knowledge of \"maps\" to perform visual localization is interesting. This gives learning frameworks the flexibility of building a latent representaiton of maps which may yield better performance instead of being restricted to a pre-defined representations.  \n- The paper is very well-written and easy to follow. \n- The authors did a good job presenting the proposed methods. The descriptions and formulations are clear. Both Figure 2 and Figure 3 are helpful for understanding the GQNs and the proposed attention mechanism.\n- The patch dictionary for the attention mechanism seems effective especially when dealing with a set of context images capturing the same scene.\n- The authors are honest about the limitations of the proposed framework compared to classic approaches.\n- The visualization of results are clear. Particularly, Figure 5 and Figure 7 give easily interpretable representations of the results.\n\n_Paper Weaknesses_\n\n- Implicitly learning a map of the scene is mentioned as a strength in the paper, but this comes at the high cost of interpretability. Without an explicit map representation, it is difficult to understand the failure cases - does the model not understand the 3D scene well or does the model have a hard time accurately predicting camera poses?\n- Minecraft is an interesting environment for proof of concept, but lacks much of the subtlety of the real world.\n- Building a framework that is able to perform the localization task from real-world scenes is more interesting. Learning generative models of real-world scenes is known to be difficult, which makes this framework impractical. There are google streetview and indoor datasets authors can try to utilize.\n- The aforementioned point is supported by the fact that the localization performance of the proposed model on real-world scenes is missing.  \n- The reviewer does not find enough novelty from the proposed model, which is an iterative improvement on GQNs.\n- The paper only compares the proposed model against its discriminative couterpart, which is not sufficient. While the authors strongly argue that exploiting the proposed implicitly representations of scenes is more beneficial than utilizing the pre-defined explicit representations, the only baseline is using the same implicit representations. Although the reviewer is aware of that this model does not use complete video sequences, benchmarking against a visual monocular SLAM algorithm, like LSD-SLAM [1], would contextualize the claim.\n- Why quantize the discriminative model's output? This de-correlates nearby pose values. The paper could benefit from an explanation of not using a straightforward regression over pose variables.\n- Overall, the reviewer does not find enough novelty from any aspects except the idea of utilizing a generative model for visual localization with implocitly learned maps, which is not fully demonstrated in the experiment section (i.e. not compare to baselines using explicit maps). \n- A differentiating factor for this paper could be tackling one of the open problems remaining in SLAM as identified in [2], like lifelong learning or semantic mapping.\n\n\n_Reproducibility_\n\n- Given the clear description in the main paper and the details provided in the appendix, the reviewer believes reproducing the results is possible. \n\n_Conclusion_\n\n- Overall, the reviewer believes this paper is well presented and reproducible. However, the paper does not propose to solve a novel problem, nor does it present a very novel method. Although the idea of using existing generative networks for localization is interesting, the paper misses important baselines relying on explicit map representation and is not sufficiently convincing. Moreover, requiring a generative model significantly limits the possibility of utilizing the proposed model for real-world applications. While the paper does present a new dataset built in Minecraft which is suitable for demonstrating the strengths of the proposed method, the reviewer does not find this significant. Therefore, the reviewer recommends a rejection.\n\n_Reference_\n\n[1] Engel, Jakob, Thomas Sch√∂ps, and Daniel Cremers. \"LSD-SLAM: Large-scale direct monocular SLAM.\" European Conference on Computer Vision. Springer, Cham, 2014.\n[2] Cadena, Cesar, et al. \"Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age.\" IEEE Transactions on Robotics 32.6 (2016): 1309-1332.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting but incremental application of Eslami et al. (2018)",
            "review": "Summary:\nEslami et al. (2018) proposed a deep neuronal framework for a scene representation and renderer (the Generative Query Networks: GQN), which generate an image from a scene representation and a query camera pose. In this work, the authors use the GQN to estimate the camera pose from a target image. Existing learning approaches are discriminative, meaning that they are trained to output the camera pose in an end-to-end fashion, while this paper proposes a generative method more in the line of hand-crafted methods which still largely outperform learning approaches. Using the GQN with the proposed attention mechanism, the method captures an implicit mapping of the environment at a more abstract level. This implicit representation is then used to optimize the likelihood of the target pose in a probabilistic graphical model framing. They compare their solution to a discriminative baseline, based on a reversed GQN.\n\nPros:\n- As shown in Figure 7, the generative approach seems to capture better the implicit representation associated to the mapping from the scene geometry and the image. \n- The proposed generative solution seems to be more accurate than the discriminative baseline. \n- As shown in Table 1, the proposed attention mechanism allows to focus on relevant parts of the context images, giving flexibility for more complex scenes.\n- Unlike classical discriminative methods, the proposed solution can be easily used in new scenes (different from the one used for the learning) thanks to the representation network. \n\nCons:\n- The contribution seems incremental with respect to Eslami et al. (2018). \n- Lack of comparisons to state of the art, in particular a comparison with PoseNet is necessary.\n- The results are shown only on simple datasets of small images (32x32 pixels). \n- Tradeoff between precision and time computing is necessary to handle large environments because of space discretization. Then, the method seems to be far to be exploited in a real life SLAM application (e.g. autonomous vehicle). \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}