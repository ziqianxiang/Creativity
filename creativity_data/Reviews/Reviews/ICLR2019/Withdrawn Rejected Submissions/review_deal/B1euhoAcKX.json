{
    "Decision": {
        "metareview": "The paper addresses the complexity issue of Determinantal Point Processes via generative deep models.\n\nThe reviewers and AC note the critical limitation of applicability of this paper to variable ground set sizes, whether authors' rebuttal is not convincing enough.\n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited applicability"
    },
    "Reviews": [
        {
            "title": "comparison with faster algorithms for sampling from DPPs",
            "review": "Determinantal Point Processes provide an efficient and elegant way to sample a subset of diverse items from a ground set. This has found applications in summarization, matrix approximation, minibatch selection. However, the naive algorithm for DPP takes time O(N^3), where N is the size of the ground set. The authors provide an alternative model DPPNet for sampling diverse items that preserves the elegant mathematical properties (closure under conditioning, log-submodularity) of DPPs while having faster sampling algorithms.\n\nThe authors need to compare the performance of DPPNet against faster alternatives to sample from DPPs, e.g., https://arxiv.org/pdf/1509.01618.pdf, as well as compare on applications where there is a significant gap between uniform sampling and DPPs (because there are the applications where DPPs are crucial). The examples in Table 2 and Table 3 do not address this.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting paper with good ideas but limited applicability (in its current form)",
            "review": "This paper proposes a scaleable algorithm for sampling from DppNets, a proposed model which approximates the distribution of a DPP. The approach builds upon a proposed inhibitive attention mechanism and transformer networks.\n\nThe proposed approach and focus on sampling is original as far as I can tell. The problem is also important to parts of the community as DPPs (or similar distributions) are used more and more frequently. However, the applicability of the proposed approach is limited as it is unclear how to deal with varying ground set sizes — the authors briefly discuss this issue in their conclusion referring to circumvent this problem by subsampling (this can however be problematic either requiring to sample from a DPP or incurring high probability of missing „important“ items).\n\nFurthermore the used evaluation method is „biased“ in favor of DppNets as numerical results evaluate the likelihood of samples under the DPP which the DppNet is trained to approximate for. This makes it difficult to draw conclusions from the presented results. I understand that this evaluation is used as there is no standard way of measuring diversity of a subset of items, but it is also clear that „no“ baseline can be competitive. One possibility to overcome this bias would be to consider a downstream task and evaluate performance on that task. \n\nFurthermore, I suggest to make certain aspects of the paper more explicit and provide additional details. For instance, I would suggest to spell out a training algorithm, provide equations for the training criterion and the evaluation criterion. Please comment on the cost of training (constantly computing the marginal probabilities for training should be quite expensive) and the convergence of the training (maybe show a training curve; this would be interesting in the light of Theorem 1 and Corollary 1).\n\nCertain parts of the paper are unclear or details are missing:\n* Table 3: What is „DPP Gao“?\n* How are results for k-medoids computed (including the standard error)? Are these results obtained by computing multiple k-medoids solutions with differing initial conditions?\n* In the paper you say: „Furthermore, greedily sampling the mode from the DPPNET achieves a better NLL than DPP samples themselves.“ What are the implications of this? What is the NLL of an (approximate) mode of the original DPP? Is the statement that you want to make, that the greedy approximation works well?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes DppNet, which approximates determinantal point processes with deep networks by inhibitive attention mechanism. The authors provided a theoretical analysis under some condition that the DppNet is of log-submodularity. Further, some experiments are conducted to show the performance.",
            "review": "Quality (5/10): This paper proposes DppNet, which approximates determinantal point processes with deep networks by inhibitive attention mechanism. The authors provided a theoretical analysis under some condition that the DppNet is of log-submodularity.\n\nClarity (9/10): This paper is well written and provides a clear figure to demonstrate their network architecture.\n\nOriginality (6/10): This paper is mainly based on the work [Vaswani et al, Attention is all you need, 2017]. It computes the dissimilarities by subtracting attention in the original work from one, and then samples a subset by an unrolled recurrent neural network. \n\nSignificance (5/10): This paper uses negative log-likelihood as the measurement to compare DppNet with other methods. Without further application, it is difficult to measure the improvement of this method over other methods.\n\nPros: \n(1) This paper is well written and provides a figure to clearly demonstrate their network architecture.\n\n(2) This paper provides a deep learning way to sample a subset of data from the whole data set and reduce the computation complexity.\n\nThere are some comments.\n(1) Figure 4 shows the sampled digits from Uniform distribution, DppNet (with Mode) and Dpp. How about the sampled digits from k-Medoids? Providing the sampled digits from k-Medoids can make the experiments more complete.\n\n(2) The object of DppNet is to minimize the negative log-likelihood. The DPP and k-Medoids have other motivations, not directly optimizing the negative log-likelihood. This may be the reason why DppNet has a better performance on negative log-likelihood, even than DPP. Could the authors provide some other measures (like the visual comparison in figure 4) to compare these methods?\n\n(3) Does GenDpp Mode in Table 2 mean the greedy mode in Algorithm 1? A clear denotation can make it more clear.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}