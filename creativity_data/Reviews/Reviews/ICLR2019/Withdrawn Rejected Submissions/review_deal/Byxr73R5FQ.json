{
    "Decision": {
        "metareview": "Pros:\n- simple, sensible subgoal discovery method\n- strong inuitions, visualizations\n- detailed rebuttal, 15 appendix sections\n\nCons:\n- moderate novelty\n- lack of ablations\n- assessments don't back up all claims\n- ill-justified/mismatching design decisions\n- inefficiency due to relying on a random policy in the first phase\n\nThere is consensus among the reviewers that the paper is not quite good enough, and should be (borderline) rejected.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "This could be an interesting paper but currently requires a lot of experimental and writing improvements",
            "review": "The paper proposes to use successor features for the purpose of option discovery.  The idea is to start by constructing successor features based on a random policy, cluster them to discover subgoals, learn options that reach these subgoals, then iterate this process.  This could be an interesting proposal, but there are several conceptual problems with the paper, and then many more minor issues, which put it below the threshold at the moment.\n\nBigger comments:\n1. The reward used to train the options (eq 5) could be either positive or negative.  Hence, it is not clear how or why this is related to getting options that go to a goal.\n2. Computing SRs only for a random policy seems like it will waste potentially a lot of data. Why not do off-policy learning of the SR while performing  the option?\n3. The candidate states formula seems very heuristic. It does not favour reaching many places necessarily (eg going to one next state would give a 1/(1-gamma) SR value)\n4. Fig 5 is very confusing. There are large regions of all subgoals and then subgoals that are very spread out.  If so many subgoals are close by, why would an agent explore? It could  just jump randomly in that region for a while. It would have been useful to plot the trajectory distribution of the agent when using the learned options to see what exactly the agent is doing\n5. There are some hacks that detract from the clarity of the results and the merits of the proposed method. For example, options are supposed to be good for exploration, so sampling them less would defeat the purpose of constructing them, but that is exactly what the authors end up doing. This is very strange and seems like a hack. Similarly, the use of auxiliary tasks makes it unclear what is the relative merit of the proposed method. It would have been very useful to avoid using all these bells and whistles and stick as closely as possible to the stated idea.\n6. The experiments need to be described much better. For example, in the grid worlds are action effects deterministic or stochastic? Are start state and goal state drawn at random but maintained fixed across the learning, or each run has a different pair? Are parameters optimized for each algorithm?  In the plots for the DM Lab experiments, what are we looking at? Policies? End states? How do options compare to Q-learning only in this case? Do you still do the non-unit exploration? The network used seems gigantic, was this optimized or was  this the first choice that came to mind? Would this not overfit? What is the nonlinearity?\n\nSmall comments:\n- This synergy enables the rapid learning Successor representations by improving sample efficiency. \n- Argmax a’ before eq 1\n- Inconsistent notation for the transition probability p\n- Eq 3 and 4 are incorrect (you seem to be one-off in the feature vectors used)\n- Figure 2 is unclear, it requires more explanation\n- Eq 6 does not correspond to eq 5\n- In Fig 6 are the 4 panes corresponding top the 4 envs.? Please explain. Also this figure needs error bars \n- It would be useful to plot not just AUC, but actual learning curves, in order to see their shape (eg rising faster and asymptoting lower may give a better AUC). \n- Does primitive Q-learning get the same number of time steps as *all* stages of the proposed algorithm? If not, it is not a fair comparison\n- It would be nice to also have quantitative results corresponding to the experiments in Fig 7.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting ideas but experimental evaluation lacking",
            "review": "The authors propose a method based on successor representations to discover options in a reward-agnostic fashion.  The method suggests to accumulate a set of options by (1) collecting experience according to a random policy, (2) approximating successor representation of or states, (3) clustering the successor representations to yield “proto-typical” cluster centers, and (4) defining new options which are the result of policies learned to “climb” the successor representation of the proto-typical state.  The authors provide a few qualitative and quantitative evaluations of the discovered options.\n\nI found the method for discovering options reasonable and interesting.  The authors largely motivate the method by appealing to intuition rather than mathematical theory, although I understand that many of the related works on option discovery also largely appeal to intuition.  The visualizations in Figure 5 (left) and Figure 7 are quite convincing.\n\nMy concerns focus on the remaining evaluations:\n\n-- Figure 5 (right) is difficult to understand.  How exactly do you convert eigenoptions to sub-goals?  Is there a better way to visualize this?\n\n-- The evaluation method in Figure 6 seems highly non-standard; the acronym AUC is usually not used in this setting.  Why not just plot the line plots themselves?\n\n-- Figure 8 is very difficult to interpret.  For (a), what exactly is being plotted? SR of all tasks or just cluster centers?  What should the reader notice in this visualization?  For (b), I don’t understand the options presented.  Is there a better way to visualize the goal or option policy?\n\n-- Overall, the evaluation is heavy on qualitative results (many of them on simple gridworld tasks) and light on quantitative results (the only quantitative result being on a simple gridworld which is poorly explained).  I would like to see more quantitative results showing the the discovered options actually help solve difficult tasks.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A sound technique, but incremental comparing to previous methods learning eigenoption and learning bottleneck states based on SRs.",
            "review": "This paper studies of the problem of HRL. It proposed an option discovery algorithm based on successor representations (SR). It considers both tabular and function approximation settings. The main idea is to first learn SR representation, and then based on the leant SR representation to clusters states using kmeans++ to identify subgoals. It iterate between this two steps to incrementally learn the SR options. The technique is sound, but incremental comparing to previous methods learning eigenoption and learning bottleneck states based on SRs. \n\nHere are the comments for this manuscript:\n \nHow sensitive is the performance to the number of subgoals? How is the number of option determined?\n\nIt is unclear to me why the termination set of every option is the set of all states satisfying Q(s, a)\\leq 0. Such a definition seems very adhoc.\n\nRegarding the comparison to eigenoptions, arethey learned from SR as well? It seems that the cited paper for eigenoptions is based on the graph laplacian. If that is true, a more recent paper learns eigenoptions based on SR should be discussed and included for the comparison.\n\nMachado et al., Eigenoption Discovery through the Deep Successor Representation, ICLR2018\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea and direction, but both the method and the derived insights need more work and understanding.",
            "review": "\nSummary: This paper tries to tackle the option discovery problem, by building on recent work on successor representation and eigenoptions. Although this is an extreme important problem, I feel the paper fails to deliver on its promise. The authors propose a way of clustering states via their SR/SF representation and they argue that this would lead to the discovery of subgoals that are fundamentally different from the popular choices in literature, like bottleneck states. They argue that this discovery procedure would lead to states “better for exploration”, “provide greater accessibility to a larger number of states”. Both of which sound promising, but I felt the actual evaluation fails to show or even assess either of these rigorously. Overall, after going through the paper, it is not clear what are the properties of these discovered subgoal states and why they would be better for exploration and/or control.\n\nClarity: Can be improved significantly! It requires several reads to get some of the important details. See detailed comments.\n\nOriginality and Significance: Very limited, at least in this version. The quantitative, and in some cases qualitative, evaluation lacks considerably. The comparison with the, probably most related, method (eigenoption) yield some slight improvement. But unfortunately, I was not conceived that this grain of empirical evidence would transfer to other scenarios. I can’t see why that would that be the case, or in which scenarios this might happen. At least those insights seem to be missing from the discussion of the results. \n\n\nDetailed comments and questions:\n\n1) Section 3.1: Latent Learning. There are a couple of design choices here that should have been more well explained or motivated:\ni) The SR were built under the uniformly random policy. This is a design choice that might work well for gridworld/navigation type of domains but there are MDP where the evaluation under this particular policy can yield uninformative evaluations. Nevertheless this is an interesting choice that I think deserved more discussion, especially the connection to previous work on proto-value functions and eigenoptions. For instance, if both of these representations -- eigenoptions and the proposed successor option model -- aim to represent the SR under the uniform policy, why does know do (slightly) better than the other? Moreover, how would these compare under a different policy (non-uniform). \nii) The choice of reward. The notation is a bit confusing here, as it’s somewhat inconsistent with the definitions (2-4). Also, more generally, It is not clear throughout if we are using the discounted or undiscounted version of SR/SFs -- (2-3) introduce the discounted version, (4) seems to be undiscounted. Not clear if (5) refers to the discounted or undiscounted version. Nevertheless, I am guessing this was meant as a shaping reward, thus \\gamma=1 for (5). But if that’s the case, according to eq. (2), most of the time I would expect \\psi_s(s_{t+1}) and \\psi_s(s_{t}) to have the same value. Could you explain why that is not true (at least in your examples)?\niii) Termination set: Q(s,a)<=0. This again seems a bit of an arbitrary choice and it’s not clear which reward this value function takes into account. \n\n2) Figure 2: The first 2 figures representing the SRs for the two room domain: the values for one of the rooms seems to be zero, although one would expect a smoother transition around the ‘doorway’, otherwise the shaping won’t point in the right direction for progression. Again, this might suggest that more informative(control) policy might give you more signal.  \n\n3) Section 3.2: ‘The policy used for learning the SR is augmented with the previously learnt options‘. Can you be more precise about how this is done? Which options used? How many of them? And in which way are they used? This seems like a very important detail. Also is this augmented policy used only for exploration? \n\n4) SRmin < \\sum_{s’} ψ(s, :) < SRmax. Is this meant to be an expectation over all reachable next states or all states in the environment? How is this determined or translated in a non-tabular setting. Not sure why this is a proxy to how ‘developed’ this learning problem or approximation is. Can you please expand on your intuition here?\n\n5) Section 3.3. The reward definition seems to represent how much the progress between \\phi(s_t+1) - \\phi(s) aligns with the direction of the goal. This is very reminest of FuN [2] -- probably a connect worth mentioning and exploring.\n\n6) Figure 4: Can you explain what rho is? It seems to be an intermediate representation for shared representation \\phi. Where is this used?\n\n7) Experiments:\n“a uniformly random policy among the options and actions (typically used in exploration) will result in the agent spending a large fraction of it’s time near these sub-goals”. Surely this is closely linked to the termination condition of the option and the option policy. How is this assessed?\n\n“in order to effectively explore the environment using the exploration policy, it is important to sample actions and options non-uniformly”. It would be good to include such a comparison, or give a reason why this is the case. It’s also not clear how many of the options we are considering in this policy and how extensive their horizons will be. This comes back to the termination condition in Section 3.1 which could use an interpretation. \n\n“In all our experiments, we fix the ratio of sampling an option to an action as 1:19.” This seems to be somewhat contradictory to the assumption that primitive actions are not enough to explore effectively this environment. \n\nFigure 8. I think this experiment could use some a lot more details. Also it would be good to guide the reader through the t-SNE plot in Figure 8a. What’s the observed pattern? How does this compare to the eigenoption counterpart.\n\n8) General comment on the experiments: There seems to be several stages in learning, with non-trivial dependencies. I think the exposition would improve a lot if you were more explicit about these: for instance, if the representation continually refined throughout the process; when the new cluster centers are inferred are the option policies learnt from scratch? Or do they build on the previous ones? Does this procedure converge -- aka do the clusters stabilize?\n\n9) Quantitative performance evaluation was done only for the gridworld scenarios and felt somewhat weak. The proposed tasks (navigation to a goal location) is exactly what SFs are trained to approximate. No composition of (sub)tasks, nor tradeoff-s of goals were studied [1,3] -- although they seem natural scenario of option planning and have been studied in previous SFs work. Moreover, if the SFs are built properly, in these gridworlds acting greedily with respect to the SFs (under the uniformly random policy) should be enough to get you to the goal. Also, probably this should be a baseline to begin with.\n\nReferences:\n[1] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and ´ David Silver. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4055–4065, 2017.\n\n[2] Vezhnevets, A.S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D. and Kavukcuoglu, K., 2017, July. FeUdal Networks for Hierarchical Reinforcement Learning. In International Conference on Machine Learning (pp. 3540-3549).\n\n[3] Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek, A. and Munos, R., 2018, July. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning (pp. 510-519).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}