{
    "Decision": {
        "metareview": "The paper proposes an approach for transfer learning by assigning weights to source samples and learning these jointly with the network parameters. Reviewers had a few concerns about experiments, some of which have been addressed by the authors. The proposed approach is simple which is a positive but it is not evaluated on any of the regular transfer learning benchmarks (eg, the ones used in Kornblith et al., 2018 \"Do Better ImageNet Models Transfer Better?\"). The tasks used in the paper, such as CIFAR noisy -> CIFAR and SVHN0-4 -> MNIST5-9, are artificially constructed, and the paper falls short of demonstrating the effectiveness of the approach on real settings. \n\nThe paper is on the borderline with current scores and the lack of regular transfer learning benchmarks in the evaluations makes me lean towards not recommending acceptance. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A simple approach for transfer learning but limited experimental evaluation"
    },
    "Reviews": [
        {
            "title": "Interesting approach to transfer learning, although the experimental section could have been clearer",
            "review": "PROS:\n* This is an interesting approach of assigning contribution weights to each source sample.\n* Could be very helpful for tasks where we have a noisy and a (small) clean dataset.\n* The method seems to be performing well for the tasks chosen, especially for the CIFAR experiments.\n* Simple idea and relatively easy to implement\n\nCONS:\n* Clarity could be improved, especially in the experimental section\n* The motivation for the SVHN 0-4 to MNIST 5-9 is not clear. It would make more sense to me to transfer between SVHN 0-5 to SVHN 5-9, or from the entire SVHN to the entire MNIST, but this particular transfer seems somewhat irrelevant to the claims. The two domains are particularly dissimilar and trying to select \"good\" SVHN samples according to 20 or 25 MNIST samples seems somewhat ill-posed. It is also particularly surprising to me that 25 MNIST samples were enough to train a LeNet to the point of 84% accuracy on the entire MNIST test set. (I'm referring to the target-only line) Is that really the case, or was a larger training set used for that particular line?\n* There is a claim that \"SOSELETO has superior performance to all of the techniqiues which do not use unlabelled data\", however I'm not sure whether these techniques were used as prescribed and if the comparison was fair. For example, I believe domain adaptation techniques like DANN, largely assume a common label space between the domains.\n* Comparison with previous re-weighting techniques would have been very informative.\n\nQUALITY:\n* The quality of the writing was overall high, with a few exceptions, including the related work and the experimental section.\n* In related work, the \"bilevel optimization\" section could be a bit more descriptive, maybe some of the explanationgiven in Sect. 3 could be moved here?\n* The experiments were convincing, with the exception of the SVHN to MNIST section.\n\nCLARITY:\n* I believe a better synthetic experiment could be chosen to highlight the approach: how about a truly noisy dataset that is not as separable as the \"noisy\" dataset in Figure 1? Maybe you could have the same noisy dataset but with a small portion of random points having the wrong label. For the same experiment, it should be clearly stated that your task is binary classification and what was the classifier used.\n* For the CIFAR experiments, it is very good that it performs well, but it'd be informative to see if SOSELETO can perform even better with 10K samples.\n* It wasn't clear to me whether the a-values of only one batch (32 samples?) at a time were affected. If so, how does this scale to really large datasets like, say, Imagenet?\n* In the CIFAR experiments, it is mentioned that a target batch-size is chosen to be larger to enable more a-values to be affected. This seems like a typo, but it was confusing. (I assume that the source batch-size is chosen to be larger)\n* Figure 2 could use a better caption and a legend. It would also be an easier figure to parse if the x-axis was reversed (eg. if the x-axis was the fraction of data used)\n* It was not clear to me what \"true transfer learning\" means as opposed to domain adaptation.\n\nORIGINALITY:\n* It seems that this idea has been explored before, however I'm not personally familiar with that work. I would have definitely liked to see comparisons with it though.\n\n\nSIGNIFICANCE:\n* This is a simple idea that seems to work well. As I wrote above, it would be great to know how it compares to other re-weighting techniques.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interpretation needed for the weights",
            "review": "This is an interesting paper claiming that on assumptions are made (or explicitly made) on the similarity of distributions. Traditionally, we learned the weights for transfer learning by matching the distributions. I am wondering if there are any relationships between those two methods. It is necessary to show the differences between the weighted source domain and the target domain, and compare them with the traditional matching methods.\n\nMy another concern is about the technical contribution. The model is very intuitive and simple. Some analyses are made for optimization. However, theoretical justifications are lacking, making the technical contribution weak and looks like a simple combination of two existing techniques. I would like to know if the weights are identifiable and what kinds of weights are preferred. \n\nBy searching, I found related papers on transfer learning with label noise and learning with label noise by importance reweighting, e.g., Yu, Xiyu, et al. \"Transfer Learning with Label Noise.\" arXiv preprint arXiv:1707.09724 (2017). and Liu, Tongliang, and Dacheng Tao. \"Classification with noisy labels by importance reweighting.\" IEEE Transactions on pattern analysis and machine intelligence 38.3 (2016): 447-461. However, they are not discussed in the submission. It is curious to see the relationships and differences.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "In this paper, the authors propose a SOSELETO (source selection for target optimization) framework to transfer learning and training with noisy labels. The intuition is some source instances are more informative than the others. Specifically, source instances are weighted and the weights are learned in a bilevel optimization scheme. Experimental studies on both training with noisy label problems and transfer learning problems demonstrate the effectiveness of the proposed SOSELETO.\n\nOverall, this paper is well-written, and easy to follow. The intuition is clear and reasonable, although it is not new. Regarding the technical section, I have the following comments:\n(1)\tThe paper assumes that the source and target domains share the same feature representation parameters \\theta. This is a widely used assumption in the existing works. However, these works usually have a specific part to align two domains to support the assumption, e.g. adversarial loss or MMD. In objective of SOSELETO, I do not see such a domain alignment part. I am wondering whether the assumption is still valid in this case. From the experimental study, I find SOSELETO achieves very good results in transfer learning problems. I am wondering whether the performance would be further improved if a domain alignment objective is added in the weighted source loss.\n(2)\tEach source has a weight, and thus there are n^s \\alpha. As mini-batch is used in the training, I am wondering whether batches are overlapping or not. If overlapping, how to decide the final \\alpha_i for x^s_i as you may obtain several \\alpha_i in batches. \n(3)\tAnother point is abouth \\lambda_p. In the contents, you omit the last term Q \\alpha_m \\lambda_p in eq.(4) as you use the fact that it is very small. I am not convincing on this omission as \\lambda_p is also a weight for the entire derivative. Moreover, if \\lambda_p is very small, the convergence would be very slow. In the experimental studies, you use different \\lambda_p for different problems. Then, whatâ€™s the rule of setting \\lambda_p given a new problem?\n\nRegarding the experimental results, the experimental settings for the section 4.2 are not very clear to me. You may need to clearly state the train and test set (e.g. data size) for each method.  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}