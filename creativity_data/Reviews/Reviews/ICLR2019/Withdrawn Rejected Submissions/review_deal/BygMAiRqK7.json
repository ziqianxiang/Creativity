{
    "Decision": {
        "metareview": "The paper's strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it's not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs --- it seems that the only reasonable instance that allows efficient generator is the case where Y = G(x)+\\xi where \\xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting connection, but lacks clarity",
            "review": "Summary\nThe authors notice that entropy regularized optimal transport produce an upper bound of a certain model likelihood. Then, the authors claim it is possible to leverage that upper bound to come up with a measure of 'sample likelihood', the probability of a certain sample under the model.\n\nEvaluation\nThe idea is certainly interesting and novel, as it allows to bridge two distinct worlds (VAE and GANs). However, I am concerned about the message (or lack of thereof) that is conveyed in the paper. Particularly, the following two points makes me be reluctant to recommend an acceptance:\n\n1)There is no measure on the tightness of the lower bound. How can we tell if this bound isnt tight? All results are dependent on the bound being close to the true value. No comments about this are given.\n2)The sample likelihoods are dependent on a certain \"model\". Here the nomenclature is confusing because I thought GANS were a probabilistic model, but now there is an additional model regarding a function f. How these two relate? What happens if I change f? to which extent the results depend on f?\n3)related to 2): the histograms in figure 2 are interesting, but they are not conclusive that the measure that is being proposed is a 'bona fide' sample likelihood.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but need more polishing",
            "review": "\n1. The assumption made by the authors that \"generator is injective\" is problematic or even wrong, as it is well known that GAN suffers from mode collapsing problem. \n\n2. It is very confusing when the authors mentioned the negative Shannon entropy. Because the equation the authors wrote is the Shannon entropy, not the negative version.\n\n3. In the 5th paragraph in the  introduction section, the paper (Cuturi, 2013) has nothing to do with \"improve computational aspect of GAN\", maybe the authors want to cite this paper \"Learning Generative Models with Sinkhorn Divergences\".\n\n4. The authors failed to discuss their paper with \"ON THE QUANTITATIVE ANALYSIS OF DECODERBASED GENERATIVE MODELS\", which uses AIS to estimate the likelihood.\n\nSuggestion:\n1. Please use \\cdot instead of , i.e. F(\\cdot) instead of F(.)\n2. Typo: in Appendix ?? and ??, in section 4",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting attempt on theory of entropic GANs ",
            "review": "The contribution of the paper is to show that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While the WGAN formulation minimizes the Wasserstein distance of the transformed latent distribution and the empirical distribution which is already a nice measure of \"progress\", having a bound on the likelihood can be interesting.\n\nPros:\n+ I like the entropic GAN formulation and believe it is very interesting as it gives access to the joint distribution of latent and observed variables. \n+ While there are some doubtful statements, overall the paper is well written and easy to read.\n\nCons:\n- The assumption of injectivity of the generator could be problematic, as it might not be fulfilled due to mode collapse.\n- I feel the theory is not very deep. Since one has a closed form of the transportation map (Eq. 3.7), the likelihood of the data is obtained by marginalizing out the latent space. However, this assumes that the inner dual maximization problem is solved to stationarity so that Eq 3.7 holds, which is not the case in practice (5 discriminator updates).\n- Thus in Sec. 4.1 for the likelihood at various points in training it is not clear what is actually happening.\n- Sec 4.3 for unregularized GANs might be problematic. In general, the transportation plan is not a density function, so I'm not certain whether Theorem 1 / Corollary 2 still hold. Furthermore, the heuristic for \"inverting\" G^* is very crude. \n\n- There are also some minor problematic statements in the paper. While they can be easily fixed, they give me doubts:\n  * The original VAE paper is not cited in the introduction for VAEs\n  * The 2013 paper by Cuturi cited on page 2 has nothing to do with \"computational aspects of GANs\". It is about fast computation of approximate OT between two discrete prob. measures. \n  * First-order / second-order Wasserstein distance is I think a bit unusual name for W_1, W_2\n  * On pg. 4, the point of the entropy term is to make the objective strongly convex. Strict convexity has no computational benefits.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}