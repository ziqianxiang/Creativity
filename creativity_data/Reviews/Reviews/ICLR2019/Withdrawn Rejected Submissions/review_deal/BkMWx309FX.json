{
    "Decision": {
        "metareview": "This paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned.  Some experiments are used to support the algorithm, which involves learning the reward perturbation process (the confusion matrix) using existing techniques from the supervised learning (and crowdsourcing) literature.\n\nReviewers found the problem setting new and worth investigating, but had concerns over the scope/significance of this work, mostly about how the confusion matrix is learned.  If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.  Specifically, the work seems to be limited in two substantial ways, both related to how the confusion matrix is learned.\n  * The reward function needs to be deterministic.\n  * Majority voting requires the number of states to be finite.\nThe significance of this work is therefore mostly limited to finite-state problems with deterministic reward, which is quite restricted.\n\nAs the authors pointed out, the paper uses discretization to turn a continuous state space into a finite one, which is how the experiment was done.  But discretization is likely not robust or efficient in many high-dimensional problems.\n\nIt should be noted that the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work.  However, the exact problem setting is not completely clear in the paper, and the limitations of the technical contributions is also somewhat unclear.  The authors are strongly advised to revise the paper accordingly to make their contributions clearer.\n\nMinor questions:\n  - In lemma 2, what if C is not invertible.\n  - The sampling oracle assumed in def 1 is not very practical, as opposed to what the paper claims.\n  - There are more recent work at NIPS and STOC on attacking RL (including bandits) algorithms by manipulating the reward signals.  The authors may want to cite and discuss.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting direction but contributions and significance somewhat limited"
    },
    "Reviews": [
        {
            "title": "An interesting and relatively unexplored variant of RL.",
            "review": "\nThis paper investigates reinforcement learning with a perturbed reward signal. In particular, the paper proposes a particular model for adding noise to the reward function via a confusion matrix, which offers a nuanced notion of reward-noise that is not too complicated so-as to make learning impossible. I take this learning setting to be both novel and interesting for opening up areas for future work. The central contributions of the work are to 1) leverage a simple estimator to prove the convergence of Q-Learning under the reward-perturbed setting along with the sample-complexity of a variant of (Phased) Q-Learning which they call \"Phrased\" Q-Learning, and 2) An algorithmic scheme for learning in the reward-perturbed setting (Algorithm 1), and 3) An expansive set of experiments that explore the impact of various reward models on learning across different environment-algorithm combinations. The sample complexity term extends Phased Q-Learning to incorporate aspects of the reward confusion matrix, and to my knowledge is novel. Further, even though Theorem 1 is unsurprising (as the paper suggests), I take the collection of Theorem 1, 2, and 3 to be collectively novel.\n\nIndeed, the paper focuses on an interesting and relatively unexplored direction for RL. Apart from the work cited by the paper (and perhaps work like Krueger et al. (2016), in which agents must pay some cost to observe true rewards), there is little work on learning settings of this kind. This paper represents a first step in gaining clarity on how to formalize and study this problem. I did, however, find the analysis and the experiments to be relatively disjointed -- the main sample complexity result presented by the paper (Theorem 2) was given for Phased Q-Learning, yet no experiments actually evaluate the performance of Phased Q-Learning. I think the paper could benefit from experiments focused on simple domains that showcase how traditional algorithms do in cases where it is easier to understand (and visualize) the impact of the reward perturbations (simple chain MDPs, grid worlds, etc.) -- and specifically experiments including Phased Q-Learning. \n\nPros:\n\t- General, interesting new learning setting to study.\n\t- Initial convergence and sample complexity results for this new setting.\n\t- Depth and breadth of experimentation (in terms of diversity of algorithms and environments), includes lots of detail about the experimental setup.\n\nCons:\n\t- Clarity of writing: lots of typos and bits of math that could be more clear (see detailed comments below) [Fixed]\n\t- The plots in Section 4 are all extremely jagged. More trials seem to be required. Moreover, I do think simpler domains might help offer insights into the reward perturbed setting. [Fixed]\n\t- The reward perturbation model is relatively simple.\n\nSome high level questions/comments:\n\t- Why was Phrased Q-Learning not experimented with?\n\t- Why use majority voting as the rule? When this was introduced it sounded like any rule might be used. Have you tried/thought about others?\n\t- Your citation to Kakade's thesis needs fixing; it should read:\n\t\t\"Kakade, Sham Machandranath. On the sample complexity of reinforcement learning. Ph.D Thesis. University of London, 2003.\"\n\n\t\t(right now it is cited as \"(Gatsby 2003)\" throughout the paper)\n\t- You might consider picking a new name for Phrased Q-Learning -- right now the name is too similar to Phased Q-Learning from [Kearns and Singh NIPS 1999].\n        - As mentioned in the \"cons\" section, the confusion matrix is still a somewhat simple model of reward noise. I was left wondering: what might be the next most complicated form of adding reward noise? How might the proposed algorithm(s) respond to this slightly more complex model? That is, it's unclear how general the results are, or if they are honed too tightly to the specific proposed reward noise model. I was hoping the authors could respond to this point.\n\n\t\nSection 0) Abstract:\n\t- Not immediately clear what is meant by \"vulnerability\" or \"noisy settings\". Might be better to pick a more clear initial sentence (same can be said of the \"sources of noise...\"\")\n\nSection 1) Introduction:\n\t- \"adversaries in real-world\" --> \"adversaries in the real-world\"\n\t- You might consider citing Loftin et al. (2014) regarding the bulleted point about \"Application-Specific Noise\".\n\t- \"unbiased reward estimator aided reward robust reinforcement learning framework\" --> this was a bit hard to parse. Consider making more concise, like: \"unbiased reward estimator for use in reinforcement learning with perturbed rewards\".\n\t- \"Our solution framework builds on existing reinforcement learning algorithms, including the recently developed DRL ones\" --> cite these up front So, cite: Q-Learning, CEM, SARSA, DQN, Dueling DQN, DDPG, NAF, and PPO, and spell out the acronym for each the first time you introduce them.\n\t- \"layer of explorations\" --> \"layer of exploration\"\n\nSection 2) Problem Formulation\n\t- \"as each shot of our\" --> what is 'shot' in this context?\n\t- \"In what follow,\" --> \"In what follows,\"\n\t- \"where 0 < \\gamma \\leq 1\" --> Usually, $\\gamma \\in [0,1)$, or $[0,1]$. Why can't $\\gamma = 0$?\n\t- The transition notation changes between $\\mathbb{P}_a(s_{t+1} | s_t)$ and $\\mathbb{P}(s_{t+1} | s_t, a_t)$. I'd suggest picking one and sticking with it to improve clarity.\n\t- \"to learn a state-action value function, for example the Q-function\" --> Why is the Q-function just an example? Isn't is *the* state-action value function? That is, I'd suggest replacing \"to learn a state-action value function, for example the Q-function\" with \"to learn a state-action value function, also called the Q-function\"\n\t- \"Q-function calculates\" --> \"The Q-function denotes\"\n\t- \"the reward feedbacks perfectly\" --> \"the reward feedback perfectly\"\n\t- I prefer that the exposition of the perturbed reward MDP be done with C in the tuple. So: $\\tilde{M} = \\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{R}, C, \\mathcal{P}, \\gamma \\rangle$. This seems the most appropriate definition, since the observed rewards will be generated by $C$.\n\t- The setup of the confusion matrix for reward noise over is very clean. It might be worth pointing out that $C$ need not be Markovian. There are cases where C is not just a function of $\\mathcal{S}$ and $\\mathcal{R}$, like the adversarial case you describe early on.\n\n\nSection 3) Learning w/ Perturbed Rewards\n\t- Theorem 1 builds straightforwardly on Q-Learning convergence guarantee (it might be worth phrasing the result in those terms? That is: the addition of the perturbed reward does not destroy the convergence guarantees of Q-Learning.)\n\t- \"we firstly\" --> \"we first\"\n\t- \"value iteration (using Q function)\" --> \"value iteration\"\n\t- \"Definition 2. Phased Q-Learning\" --> \"Definition 2. Phrased Q-Learning\". I think? Unless you're talking about Phased Q from the Kearns and Singh '99 work.\n\t- \"It uses collected m samples\" --> \"It uses the collected m samples\"\n\t- Theorem 2: it would be helpful to define $T$ since it appears in the sample complexity term. Also, I would suggest specifying the domain of $\\epsilon$, as you do with $\\delta$.\n\t- \"convergence to optimal policy\" --> \"convergence to the optimal policy\"\n\t- \"The idea of constructing MDP is similar to\" --> this seems out of place. The idea of constructing which MDP? Similar to Kakade (2003) in what sense?\n\t- \"the unbiasedness\" --> \"the use of unbiased estimators\"\n\t- \"number of state-action pair, which satisfies\" --> \"number of state-action pairs that satisfy\"\n\t- \"The above procedure continues with more observations arriving.\" --> \"The above procedure continues indefinitely as more observation arrives.\" Also, which procedure? Updating $\\tilde{c}_{i,j}$? If so, I would specify.\n\t- \"is nothing different from Eqn. (2) but with replacing a known reward confusion\" --> \"replaces a known reward confusion\"\n\n\n4) Experiments:\n\t- Diverse experiments! That's great. Lots of algorithms, lots of environment types.\n\t- I expected to see Phrased Q-Learning in the experiments. Why was it not included?\n\t- The plots are pretty jagged, so I'm left feeling a bit skeptical about some of the results. The results would be strengthened if the experiments were repeated for more trials.\n\n5) Conclusion:\n\t- \"despite of the fact\" --> \"despite the fact\"\n\t- \"finite sample complexity of Q-Learning with estimated surrogate rewards are given\" --> It's not really Q-Learning, though. It's a variant of Q-Learning. I'd suggest being explicit about that.\n\nAppendix:\n\n\t- \"It is easy to validate the unbiasedness of proposed estimator directly.\" --> \"It is easy to verify that the proposed estimator is unbiased directly.\"\n\t- \"For the simplicity of notations\" --> \"For simplicity\"\n\t- \"the Phrased Q-Learning could converge to near optimal policy\" --> \"\"the algorithm Phrased Q-Learning can converge to the near optimal policy\"\"\n\t- \"Using union bound\" --> \"Using a union bound\"\n\t- Same comment regarding $\\gamma$: it's typically $0 \\leq \\gamma < 1$.\n\t- Bottom of page 16, the second equation from the bottom, far right term: $c.j$ --> $c,j$.\n\t- \"Using CauchySchwarz Inequality\" --> \"Using the Cauchy-Schwarz Inequality\"\n\n\nReferences:\n\tLoftin, Robert, et al. \"Learning something from nothing: Leveraging implicit human feedback strategies.\" Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on. IEEE, 2014.\n\n\tKrueger, D., Leike, J., Evans, O., & Salvatier, J. (2016). Active reinforcement learning: Observing rewards at a cost. In Future of Interactive Learning Machines, NIPS Workshop.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting general surrogate reward which has wide applicability, and can be flexibly included alongside a variety of algorithms.",
            "review": "## Summary\n\nThe authors present work that shows how to deal with noise in reward signals by creating a surrogate reward signal. The work develops a number of results including: showing how the surrogate reward is equal in expectation to the true reward signal, how this doesn't affect the fixed point of the Bellman equation, how to deal with finite and continuous rewards and how the convergence time is affected for different levels of noise. They demonstrate the value of this approach with a variety of early and state-of-the-art algorithms on a variety of domains,, and the results are consistent with the claims.\n\nIt would be useful to outline how prior work approached this same problem and also to evaluate the proposed method with existin approaches to the same problem. I realise that this is the first method that estimates the confusion matrix rather than assuming it is known a priori but there are obvious ways around this, e.g. the authors first experiment assumes the confusion matrix is known, so this would be a good place to compare with other competing techniques. Also, the authors have a way of estimating this, so they could plug it into the other algorithms too.\n\nI also have some concerns about the clarity and precision of the proofs, although I do not have any reason to doubt the Lemma/Theorem correctness (see below).\n\nThe weakest part of the approach is in how the true reward is estimated in order to estiamate the confusion matrix. It uses majority vote (which is only really possible in the case of finite rewards with noise sufficiently low that this will be a robust estimate). Perhaps some other approaches could be explore here too.\n\nFinally, there is discussion about adversarial noise in rewards at the beginning but I am not sure the theory really addresses it nor the evaluations.\n\nNonetheless, given that I do not know whether the claim of originality is true (in terms of the estimation of the confusion matrix). If it is, then the work is a significant and interesting advance, and is clearly widely applicable in domains with noisy rewards. It would be interesting to see a more tractable approach for continous noise too, but this would probably involve assumptions (smoothness? Gaussianity?), and doesn't impact the value of this work.\n\n## Detailed notes\n\nThere is a slight sloppiness in  notation in equation (1). This uses \\tilde{r} as a subscript of e, but r is +1 or -1 and the error variables are e_+ and e_- (not e_{+1} and e_{-1}).\n\n\nThe noise levels in Atari (Figure 3) show something quite interesting which could be commented upon. For noise below 0.5 the surrogate reward works roughly  similarly to the noisy reward, but when the noise level goes above this, the surrogate reward clearly exploits the increased information content (similar to a noisy binary channel with over 0.5 noise). This may have  implications for adversarial noise.\n\nThere are also some issues with the proofs which I spotted outlined below:\n\n### Lemma 1 proof\nThe proof of Lemma 1, I think, fails to achieve its objective. The first pair of equations is not a rewrite of equation (1). I believe that the authors intend for this to be a consequence of Equation (1) but do not really demonstrate this clearly. Also, the authors seem to switch between binary rewards -1 and +1 and two levels of reward r- and r+ leading to some confusion. I would suggest the latter throughout as it is more general but involves no more terms.\n\nI suggest the following as an outline for the proof. It would help for them to define what they mean by the different rhats (as they currently do) and explain that these values are therefore:\n\n  rhat- = [(1 - e+) r- - e- r+ ]/(1 - e+ - e-)\n  rhat+ = [(1 - e-) r+ - e+ r-]/(1- e+ - e-)\n\nfrom equation (1). What is left is for them to actually prove the Lemma, namely that the expected value of rhat is:\n\n  E(rhat ) = p1(rhat=rhat-) rhat- + p(rhat=rhat+) rhat+ = E(r)\n\nwhere the probabilities relate to the surrogate reward taking their respective values. And just stylistically, I would avoid writing \"we could obtain\" and simply write \"we obtain\".\n\nLemma 2 achieves this more clearly with greater generality.\n\n\n### Theorem 1 proof\nAt the end of p13, the proof of the expected value loses track of the chosen action a. I would suggest the authors replace: $$\\mathbb{P}'(s,s',\\hat{r})$$ with $$\\mathbb{P}'(s,a, s',\\hat{r})$$ then define it. Likewise $$\\mathbb{P}(s,s')$$ should be $$\\mathbb{P}(s,a,s')$$ (and also defined).\n\nI am also a little uncomfortable with the switch from: $$max_{b \\in \\mathcal{A}} | Q(s',b) - Q*(s',b)|$$ in the second to last line of p13, which refers to the maximum Q value associated with some state s', to  $$||Q-Q*||_{\\infty}$$ in the next line which is the maximum over all states and actions. The equality should probably be an inequality there too.\n\nThroughout this the notation could be much better defined, including how to interpret the curly F and how it acts in the conditional part of an expectation and variance.\n\nFinally, there is a bit too free a use of the word \"easily\" here. If it were easy, then the authors could do it more clearly I think. Otherwise, please refer to the appropriate result in the literature.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting but seems to tackle a too narrow problem",
            "review": "The paper aims at studying the setting of perturbed rewards in a deep RL setting. Studying the effect of noise in the reward function is interesting. The paper is quite well-written. However the paper studies a rather simple setting, the limitations could be discussed more clearly and there are one or two elements unclear (see below).\n\nThe paper assumes first the interesting case where the generation of the perturbed reward is a function of S*R into the perturbed reward space. But then the confusion matrix does *not* take into account the state, which is justified by \"to let our presentation stay focused (...)\". I believe these elements should at least be clearly discussed. Indeed, in that setting, the theorems given seem to be variations of existing results and it is difficult to understand what is the message behind the theorems.\n\nIn addition, it is assumed that the confusion matrix C is known or estimated from data but it's not clear to me how this can be done in practice.  In equation 4, how do you have access to the predicted true rewards?\n\nAdditional comments:\n- The discount factor can be 0 but can not, in general, be equal to 1. So the equation in paragraph 2.1 \"0 < γ ≤ 1\" is wrong.\n- The paper mention that \"an underwhelming amount of reinforcement learning studies have focused on the settings with perturbed and noisy rewards\" but there are some works on the subject (e.g., https://arxiv.org/abs/1805.03359) and a discussion about the differences with the related work would be interesting.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}