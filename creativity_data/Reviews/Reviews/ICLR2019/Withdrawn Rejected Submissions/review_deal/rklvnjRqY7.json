{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good demonstration of adversarial training but evaluation doesn't seem very promising",
            "review": "This paper proposes a framework which preserves the private information in the image and at the same time doesn’t compromise the usability of the image. This framework is composed of three models - the obfuscator, the classifier, and the reconstructor. While the obfuscator’s objective is to remove the sensitive information from the image and extract feature maps that still have it’s discriminative features, the classifier uses these feature maps to do the classification learning task. On the other hand, the reconstructor is like an attacker which tries to expose the private information by restoring the image. The framework thus uses adversarial training since the obfuscator and the reconstructor have opposite goals. The paper shows that the reconstructed images don’t seem to have crucial sensitive information of the raw image yet it still manages to achieve high classification accuracy. \n\nPros:\n- While we focus on achieving better and better results with Machine Learning, we tend to forget about the private nature of the data we often have to work with. I believe it is a very important issue to address and a great idea to work on.\n- The idea of using the reconstructor as an adversary in the framework seems so intuitive. \nThe visualizations at the end comparing the reconstructed images with the raw images reveal how this training procedure doesn’t allow sensitive information to leak.\n\nCons:\n- I am not very convinced with choosing the image reconstruction quality as a measure for the privacy preservation. I am unable to see how those two can be so strongly related. There can be scenarios I believe, where the image doesn’t reconstruct well but can still retain the sensitive features of the image. \n- I am just curious about a few aspects of the design of the network. Firstly, I would have expected to a deeper network required to remove the private information of the image. I think a visualization on how the feature maps look after 3rd layer when they lose their private traits, with possibly a comparison with an early, say 1st layer would help understand it better. \n- When I try to think of a data with private information, it seems to be pretty different than the MNIST or CIFAR data. I can’t seem to imagine any sort of privacy linked to these datasets (with a digit for example). It would be more helpful to look at the performance of the framework with images having some private content as we have from social networking sites or medical images which contains sensitive details. \n\nQuestions:\n- Are you using any sort of pretrained weights from the VGG network and fine-tuning on top of it or are you just borrowing the architecture and training it from scratch in your experiments?\n- Table 1 shows a sigmoid layer in the last layer of the reconstruction networks. It was a little confusing since we are expecting an image as an output of the network.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea. Need more justification for privacy metric.",
            "review": "This paper proposed a deep learning framework with three components: an obfuscator, a classifier, and a reconstructor, for privacy-preserving image classification. To protect the privacy of the image data, this paper proposed an adversarial training process to optimize the obfuscator such that it achieves high classification accuracy while the \"best\" reconstructor is not able to accurately reconstruct the sensitive input image from the output features. Experiment results showed relatively high accuracy with no sign that an input image can be reconstructed by state-of-the-art reconstruction models.\n\nThe presentation of the paper is pretty good. The problem of learning without leaking sensitive information in data is an important issue, and the paper proposed an interesting framework for it. However, I think some technical aspects need to be more addressed:\nIt seems that the obfuscator+classifier models are obtained by dividing an existing deep CNN architecture, but the paper did not justify the choice of the model or the choice of the layer as the dividing point. These seem to be important decisions in the framework, so I think more intuitions, or some heuristics for choosing them need to be provided.\nIt is not clear to me how reconstruction accuracy (measured by human’s perception) and data privacy are related. I think in general high reconstruction error does not automatically imply that the sensitive information is removed. It is still possible for a machine learning model to extract sensitive information from images that are not readable for human. So I think justifications for this choice of privacy metric need to be provided.\n\nA few things about presentation:\n1. The first sentence of Section 4: \"the our proposed framework\" --> \"the proposed framework of\"\n2. Figure 3: all lines are overlapping. I suggest explaining why and color-code the lines for better visibility.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important topic to study but deeper investigation is suggested",
            "review": "Privacy issues are a burning issue in ML. Elevated awareness to privacy issues, as expressed, for example, by GDPR regulations, might make it hard to collect large datasets for training machine learning models. This current work suggests using adversarial networks to obfuscate images and thus allow collecting them without privacy concerns. \nThe problemed studied in this work is of great interest and there many researchers are studying it today. The popularity of this topic makes it hard to present a good background on the state of current research. However, I fined the method proposed here naïve compared to the methods proposed by previous researchers, even the ones that are cited in the related work section: while methods that use differential privacy, homomorphic encryptions or secure-multi-party-computation provide theoretical guaranties on the privacy provided, the work presented here shows only that the obfuscated data does not allow reconstructing the image using several networks with which such reconstruction was attempted.  \n\nThe problem discussed above breaks into two questions that are left un-answered. First, what is the trust model? That is, under what assumptions can we prove that the private data is protected? The authors here propose using the reconstruction quality is a measure of privacy. The advantage of this measure is that it is easy to evaluate, and it is generic. However, this leads to a second question, why is reconstruction error a good measure of privacy? For example, even a well distorted image may allow identifying the identity of a person in an image.\n\nIn the following there are some detailed comments on specific sections in this work:\n•\tIntroduction \no\t “GDPR stipulates that personal data cannot be stored for long periods of time … In other words, this regulation prevents long-term storage of video/image data” – this is not accurate. GDPR allows for pseudo-anonymization of data. Therefore, for example, encrypted data can be stored for long periods of time.\no\tDoes the obfuscated image qualify as anonymization/pseudo-anonymization according to GDPR?\no\t“Thus the obfuscated intermediate representation can be stored indefinitely” – is this representation not subject to GDPR deletion requests?\no\t“we choose image reconstruction quality as a general measure for privacy preservation” – why is it a good measure of privacy preservation? For example, could it be that the reconstruction quality is low, but it is still possible to identify the person in the image?\no\t“To the best of our knowledge, this is the first study of using the adversarial training methodology for privacy-preserving image classification” – there have been studies in the past of using adversarial networks for privacy tasks. For example, Abadi & Andersen, 2016 studied using adversarial networks as an encryption mechanism without specifying whether the data is an image or not. The work of Yu, Zhang, Kuang, Lin & Fan, 2017 is also very close in nature to the current one. One of the contributions Yu et al. propose is image privacy protection by blurring private sensitive objects from the image.\n•\tRelated Work\no\tThe main limitation of CryptoNets in the context of this work is that it only handles inference and does not handle training of neural networks. There are some attempts to perform training using homomorphic encryptions, for example see Kim, Song, Wang, Xia, and Jiang, 2018.\no\tIt is not accurate to say about DeepSecure that it is limited to cases in which each client sends less than 2600 examples. Instead, what is true about DeepSecure and related methods such as SecureML is that there is a big computation cost.\no\tThere are methods that use Intel’s SGX for training neural networks which provide privacy but have better computation performance, for example see Ohrimenko, Schuste, Fournet, Mehta, Nowozin, Vaswani and Costa, 2016\no\tWhen discussing privacy methods, you should also mention the trust model – that is, what is an adversary allowed to do to try to capture private information. For example, DeepSecure, assumes the non-collusion & semi-honest model which means that the data is protected by splitting it on several servers, each server will try to use the information it can see to infer the private information but multiple parties do not collaborate in trying to get to private information and moreover, each party follows the designed protocol. Other methods use different trust models.\no\tOn the work of Osia et al you say that it can’t be used for training because of the communication costs. The large communication costs are common to techniques that use multi-parties to secure computation, this includes DeepSecure and SecureML. Note, however, that the communication costs do not make it impossible to train, instead, it makes it slow and costly.\n•\tSecurity Analysis \no\tis it necessary the case that there is a single data provider? If there are more than a single data provider, then you may have more that 3 entities in the system. In this case you may say that there are 3 entity types.\no\tWhat are the restrictions on the power of the attacker? According to the description the attacker can be an internal staff which may allow the attacker to obtain access to the data before obfuscation\no\tBesides the nice acronym, why is “Chosen Image Attack” a proper name for an attack that modifies the representation of images?\no\tWhy is the Chosen Image Attack the “most natural and convenient way of launching an attack”? In many cases, privacy leaks because of joining data from multiple sources, see for example the attacks on the Netflix challenge data, see for example Ohm, 2009.\no\t\n•\t Obfuscator-Adversary Framework\no\tA similar model to the one proposed here has been suggested by Moore, Pfeiffer, Wei, Iyer, Charles, Gilad-Bachrach, Boyles and Manavoglu, 2018 in the context of removing biases from datasets.\n•\tExperiments\no\tThe images used in this experiment are very different from the images on which VGG and ResNet were designed to work. The images here are very small around 30x30 pixels while VGG and ResNet were designed to work on large images, for example 200x300 pixels. \no\tIt may be interesting to conduct an experiment on large images (ImageNet, Caltech dataset) and see how the reconstruction error differs between the proposed representation compared to standard representations such as the ones generated by AlexNet, VGG, Inception and others.\n\n\nTherefore, a lot of important and interesting work is reported in this report, however, some additional thinking is required. I advise the authors to keep working in this direction.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}