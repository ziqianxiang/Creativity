{
    "Decision": "",
    "Reviews": [
        {
            "title": "Need improvement in presentation",
            "review": "This paper studied the problem of reducing the training time of neural networks in a data-parallel setting where training data is stored in multiple servers. The main goal is to reduce the network traffic of exchanging the gradients at each step. Gradient dropping achieves this goal by exchanging globally only the sparse and large gradients but accumulating the small gradients locally at each step. It was shown in previous work that this approach can have slow convergence. This paper improves this technique by exploring 3 simple heuristics, SUM, PARTIAL, and ERROR, for combing the global sparse gradients and the locally computed gradients during updates. Experimental results showed that in machine translation tasks, the proposed method achieved significant faster time while the final quality of the model is not affected.\n\nReducing communication cost in distributed learning is a very important problem, and the paper proposes an interesting improvement upon an existing method. However, I think it’s necessary to provide more intuitions or theoretical analysis behind the introduced heuristics. And it might be better to conduct experiments or add more discussion in tasks other than machine translation. The paper is generally easy to follow but the organization needs to be improved. \n\nMore details in terms of presentation:\n- Section 2.1 and Algorithm 1 reviewed gradient dropping which is the main related work to this paper but it is in the middle of the related work section. The notations were also not clearly stated (e.g. the notation of | . |, Gt, and AllReduce). It’s better for algorithm blocks to be more self-contained with notations briefly described inside them. I suggest re-organizing it by combing Section 2.1 with Section 3 into a single technical section and providing a clear definition of the notations.\n- In Section 3, the first part of Algorithm 2 is identical to Algorithm 1, so it makes sense to keep only one of them and explain only the difference. \n- In Section 4 and 5, it is unclear what the baseline method is referring to. Is it DGC or training with a single machine? The result in Figure 1 seems less important. I would suggest placing it and Section 5.1 in a less significant position. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "sparse gradient does not seem justified, and the speedup is small",
            "review": "This paper proposes a 3 modes for combining local and global gradients\nto better use more computing nodes. The paper is well-written and well-motivated.\n\nI question why it is a good idea to use sparse gradient in a neural network where all coordinates (i.e. hidden units) are\nexchangeable.\nThis paper claims that \"gradient values are skewed, as most are close to zero\",\nbut there is no references nor statistics. The only source of sparsity\nfor a typical MT model are those for word vectors, but in that case, perhaps it would be better to develop algorithms that specifically make use of that type of sparsity.\nThe quality degradation in methods like gradient dropping seems to suggest that\nsome arbitrary cutoff for sparsity is not a good idea.\n\nFor the results, the proposed method running on 4 nodes is compared to synchronous SGD, and\nonly got 25% and 45% speedups on the 2 tests.\nNotably, they do not compare to asynchronous SGD with stale gradients. \nSuch a test would help offer empirical support/refutation\non the assumption that the sparse gradient is a good idea in the first place.\nI think these results are not good enough to justify\nthe extra complications in the proposed method.\n\nThe paper is well-written,\nhowever AllReduce seems undefined, and I assumed it aggregates sparse gradients from all nodes.\nI also a bit confused about the characteristics of ApplyOptimizer. In gradient dropping,\nApplyOptimizer takes a sparse gradient, but in the proposed methods, it takes a dense gradient.\nI am a bit confused on how exactly the synchronous update is done, and I'd appreciate it if you can make your model of parallel updates more explicit.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper does not have high enough quality and novelty",
            "review": "The paper looks at the problem of reducing the communication requirement for implementing the distributed optimization techniques, in particular, SGD. This problem has been looked at from multiple angles by many authors. And although there are many unanswered questions in this area, I do not see the authors providing any compelling contribution to answering those questions or providing a meaningful solution.\n\nFirst of all the solution that is proposed in the paper is just a heuristic and does not seem to have a very justified basis. For example, is SUM, and PARTIAL ways of combining the local gradients the authors justify the formulas by not counting things twice. But then in the ERROR they do not care about the same issue. This tells me that it is highly probable that they have tried many things and this is what has kind of worked. I do not think this is acceptable as we would like to know why something works as opposed to just trying every possibility (which is impossible) and then finding out something works (may be only for a set of limited problems).\n\nIn addition to that, I think a much simpler version of what they are proposing has been already proposed in [1]. In [1] the authors propose to run SGD locally and then average the models every once in a while. They empirically show that such a method works well in practice. As it is clear, [1] does not do the weird heuristic of combining the gradients, but still manages to save time and converge relatively fast. So, I am not quite sure if this heuristic combining that is proposed by this paper is any useful.\n\n\n\n[1] McMahan, H. Brendan, et al. \"Communication-efficient learning of deep networks from decentralized data.\" ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}