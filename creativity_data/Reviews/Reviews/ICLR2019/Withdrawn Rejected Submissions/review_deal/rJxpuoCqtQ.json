{
    "Decision": {
        "metareview": "This paper proposes a new permutation invariant loss (where the order doesn't matter), motivated by set autoencoding settings. This is an important problem, and the authors' solution is interesting.  The reviewers, however, found the exposition to be unclear, in particular the explanation on how the loss function is derived was confusing for two of the reviewers. Reviewers also found the experimental results to be not convincing, even after the revision. This is a borderline paper: the idea is valuable and I'd encourage the authors to develop it further, improving exposition and including additional experiments as suggested by the reviewers.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Borderline paper"
    },
    "Reviews": [
        {
            "title": "new loss function for set autoencoders; experiments are not sufficient",
            "review": "This paper proposes an objective function for sets autoencoders such that the loss is permutation invariant with respect to the order of reconstructed inputs. I think that the problem of autoencoding sets is important and designing custom loss functions is a good way to approach it. Thus, I quite like the idea of SCE  from that point of view. However, I find the experiments not convincing for me to accept the paper. \n\nWhile reading Section 3, I found it hard to keep in mind that x and y are discrete probability distributions and the notation like P(x=y) is not making things easier. Actually, I’ve never seen cross entropy written with P(x=y). Though is my personal opinion and I don’t have a suggestion on how to improve the explanations in Eq. 1-8. However, I’m glad there is an example at the end of Section 3.\n\nI have some comments on the Experiments section. \n\n* Puzzles:\n(1) Figure 1 could have been prettier. \n(2) The phrase “The purpose of this experiment is to reproduce the results from (Zaheer et al., 2017)” makes little sense to me.  In Deep Sets, there are many experiments and it’s not clear which experiment is meant here.\n(3) Table 1 gives test error statistics for 10 runs. What is changed in every run? Does the test set stay the same in every run or is a kind of a cross-validation? Or is it just a different random seed for the initial weights? I could not find an explanation in the text, so there is no way I can interpret the results.\n\n* Blocksworld: the reconstructions are nice, but the numbers in Table 2 are difficult to interpret. \nFor example, I cannot estimate how important the difference of 10 points in SH scores is.\n\n* Rule learning ILP tasks: I don’t know enough about learning logic rules tasks to comment on those experiments, but Table 3 seems overwhelming and the concept of 10 runs is still unclear.\n\n--- General comment on the experiments ---\n\nI think an important goal of any autoencoder is to learn a representation that can be useful in other tasks. There is even an example in the paper: “set representation of the environment is crucial in the robotic systems”. Thus, the experiments I would like to see are about evaluating the quality of a representation from an SCE-trained autoencoder compared to other training methods.  Without those experiments, I cannot estimate how valuable the SCE loss function is.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Extension to case of sets with different sample sizes?",
            "review": "The paper is understandable and the question addressed is interesting. The use of log likelihoods to metrize distances between sets, although not new, is used quite effectively to address the issue of label switching in sets. Although the run time is O(N^2), the metric can be computed in a parallelized manner. The question of comparing sets of different sample sizes would be a valuable extension to the work. Although I think the proposed loss function addresses some important issues, would like to defer the question of acceptance/rejection to other reviewers due to lack of expertise in related areas.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Lacks clarity",
            "review": "In the manuscript entitled \"Likelihood-based Permutation Invariant Loss Function for Probability Distributions\" the authors propose a loss function for training against instances in which ordering within the data vector is unimportant.  I do not find the proposed loss function to be well motivated, find a number of confusing points (errors?) in the manuscript, and do not easily follow what was done in the examples.\n\nFirst, it should be noted that this is a very restricted consideration of what it means to compare two sets since only sets of equal size are under consideration; this is fundamentally different to the ambitions of e.g. the Hausdorff measure as used in analysis.  The logsumexp formulation of the proposed measure is unsatisfactory to me as it directly averages over each of the independent probabilities that a given element is a member of the target set, rather than integrating over the combinatorial set of probabilities for each set of complete possible matches.  Moreover, the loss function H() is not necessarily representative of a generative distribution.\n\nThe definition of the Hausdorff distance given is directional and is therefore not a metric, contrary to what is stated on page 2.\n\nI find the description of the problem domain confusing on page 3: the space [0,1]^NxF is described as binary, but then values of log y_i and log (1-y_i) are computed with y in [0,1] so we must imagine these are in fact elements in the open set of reals: (0,1).\n\nClarity of the examples could be greatly improved, in particular by explaining precisely what is the objective of each task and what are the 'ingredients' we begin with.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}