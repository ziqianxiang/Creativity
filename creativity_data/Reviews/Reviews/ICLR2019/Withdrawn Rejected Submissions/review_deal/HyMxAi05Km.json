{
    "Decision": {
        "metareview": "The reviewers vary in their scores but overall there is agreement that this paper is not ready for acceptance. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Theoretical study and empirical testing of improvement of machine translation classifiers using dual and cyclical paths among languages",
            "review": "The paper addresses a means of boosting the accuracy of automatic translators (sentences) by training dual models (a.k.a. language A to B, B to A), multipath (e.g. A to B to C) and cyclical (e.g. A to B to C to A) while starting with well initialized models for translating simple pairs. The idea that additional errors are revealed and allow classifiers to adapt, thus boosting classifier performance, is appealing and intuitive. That a theoretical framework is presented to support this assumption is encouraging, but the assumptions behind this analysis (e.g. Assumption 1) are rather strong. Equation 2 assumes independence. Equations 3-5 can be presented more clearly. Not only are translation errors not uniform in multiclass settings, but they can be highly correlated - this being a possible pitfall of boosting approaches, seen as general classifier errors. The same strong assumptions permeate from the dual to the cyclical case. On the other hand, the (limited) empirical test presented, using a single dataset and baseline classifier method, does support the proposed improvement by boosting, albeit leading to improvements which are slight.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Possibly some interesting ideas here, but has errors, and also has a very strong assumption (that each sentence has a unique translation)",
            "review": "The paper gives theorems concerning \"dual learning\" - that is, making\nuse of round-trip consistency in learning of translation and other\ntasks.\n\nThere are some interesting ideas here. Unfortunately, I think there\nare issues with clarity/choice of notation and correctness (errors\nresulting from problems with the notation - or at least it's very\nhard to figure out if things are correct under some intepretation).\n\nMore specifically, I'm uneasy about the use of x^i and x^j as defined\nin section 2. In some cases x^j is a deterministic function of x^i, in\nsome cases it's a random variable, these cases are mixed. Section 2\nbecomes tangled up in this issue. It would be much better I think to\ndefine a function f_ij(x) for each (i,j) pair that maps a sentence\nx \\in S^i to its correct translation f_ij(x) \\in S^j.\n\nA critical problem with the paper is that Eq. 2 is I think incorrect.\nClearly,\n\nPr(T_ij(x_i) = f_ij(x_i), T_ji(f_ij(x_i)) = x_i)      [1]\n=\nPr(T_ij(x_i) = f_ij(x_i))                             [2]\n*\nPr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i))    [3]\n\nI think [1] is what is meant by the left-hand-side of Eq 2 in the paper -\nthough the use of x^j is ambiguous (this ambiguity is a real issue).\n\nIt can be verified that\n\nPr(T_ij(x_i) = f_ij(x_i)) = p_ij\n\nhowever\n\nPr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i)) \\neq p^r_ji\n\nThe definition of p^r_ji is that of a different quantity.\n\nThis problem unfortunately permeates the statement of theorem 1, and\nthe proof of theorem 1. It is probably fixable but without a\nsignificantly revised version of the paper a reader/reviewer is\nbasically guessing what a corrected version of the paper would\nbe. Unfortunately I think publishing the paper with errors such as\nthis would be a problem.\n\nSome other points:\n\n[1] The theorem really does have to assume that there is a unique\ncorrect translation f_ij(x^i) for each sentence x^i. Having multiple\npossible translations breaks things. The authors say in section 2 \"In\npractice, we may select a threshold BLEU (Papineni et al., 2002a)\nscore, above which the translation is considered correct\": this seems\nto imply that the results apply when multiple translations (above\na certain BLEU score) are possible. But my understanding is that this\nwill completely break the results (or at least require a significant\nmodification of the theory).\n\n[2] A further problem with ambiguity/notation is that T^d is never\nexplicitly defined. Presumably we always have T_ij^s(x^i) = T_ij(x^i) if\nT_ji(T_ij(x^i)) = x^i? That needs to be explicitly stated.\n\n[3] There may be something interesting in theorem 1 - putting aside point\n[1] above - but I am just really uneasy with this theorem and its proof\ngiven that it uses p^r_ji, and the issue with Eq. 2.\n\n[4] Another issue with the definition of p^r_ij: the notation\nP_{X^(j, r) ~ \\mu}(...) where the expression ... does not refer\nto X^{j, r} (instead it refers to x^j) is just really odd,\nand confusing.\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overlooked assumption?",
            "review": "This paper provides a theoretical perspective of the dual learning tasks and proposes two generalizations (multipath/cycle dual learning) that utilize multiple language sets. Through experiments, the paper discusses the relationship between theoretical perspective and actual translation quality.\n\nOverall, the paper is well written and discussed enough. My concern is about Theorem 1 that could be a critical problem.\nIn the proof of Theorem 1, it discussed that the dual learning can minimize Case 2. This assumption is reasonable if the vanilla translator is completely fixed (i.e., no longer updated) but this condition may not be assumed by the authors as far as I looked at Algorithm 2 and 3 that update the parameters of vanilla translators directly. The proof is constructed by only the effect against Case 2. However, if the vanilla translator is directly updated through dual training, there should exist some random changes in also Case 1 and this behavior should also be included in the theorem.\n\nCorrection and suggestions writing:\n* It is better to introduce an additional $\\alpha$, $\\beta$ and $\\gamma$ for the vanilla translation accuracy (e.g., $\\alpha_0 := p_{ij}p_{ji}^r$) so that most formulations in Section 3 can be largely simplified.\n* In Justification of Assumption1 ... \"the probability of each cluster is close to $p_max$\" ->  maybe \"greater than $p_max$\" to satisfy the next inequality.\n* Eq. (3) ... $T_{ji}^d(T_{ij}^d(x^{(i)})$ -> $T_{ji}^d(x^{(j)})$ to adjust other equations.\n* Section 3 Sentence 1: \"translatorby\" -> \"translator by\"\n* Section 4.2: ${\\rm Pr}_{ X^{(3)} \\sim T_{23}^d (X^{(1)}) }$ -> $... (X^{(2)}) }$",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}