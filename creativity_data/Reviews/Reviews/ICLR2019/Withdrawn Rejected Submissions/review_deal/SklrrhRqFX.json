{
    "Decision": {
        "metareview": "The paper suggests a new way to learn a physics prior, in an action-free way from raw frames. The idea is to \"learn the common rules of physics\" in some sense (from purely visual observations) and use that as pre-training. The authors made a number of experiments in response to the reviewer concerns, but the submission still fell short of their expectations. In the post-rebuttal discussion, the reviewers mentioned that it's not clear how SpatialNet is different from a ConvLSTM, mentioned the writing quality and the fact that the \"physics prior\" is really quite close to what others call video prediction in other baselines.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Interesting Idea, Unclear Writing",
            "review": "A method for learning physics priors is proposed for faster learning and better transfer learning. The key idea in learning physics priors using spatial net, which is similar to a convolutional LSTM model for making predictions. Authors propose to improve the sample efficiency of Deep RL algorithms, by augmenting PPO’s state input with 3 future frames predicted by the physics prior model. \n\nAuthors show that using Spatial-Net leads to better prediction of the future as compared to previous methods on simple simulated physics environment and can be incorporated to improve performance on ATARI games. \n\n(a) I am a bit unclear on how Spatial-Net is trained along with the policy in the IPA architecture. In section 5.1 it is mentioned that, “We train both SpatialNet and the policy simultaneously and use Proximal Policy optimization (PPO) as our model free algorithm”, however earlier in Section 3 it is mentioned that first the agent is pre-trained with prediction and then the pre-trained model is used with the RL algorithm. Can the authors clarify the training procedure? Is it the case that the Spatial-Net is first pre-trained with some data and then fine-tuned along with the environment rewards? Do the policy-net and the frame prediction net share any parameters? \n\n(b) Is the comparison in Table 2/Figure 5 fair in terms of number of frames seen by the agent? Let a PPO agent see N frames? How many frames does the IPA agent say (both for training spatial Net + Policy). \n\n(c) How about baselines, where instead of augmenting PPO with any additional frames, the Policy is initialized with weights learned by Spatial Net? Other baseline is to jointly optimize for future frame prediction + environment reward (in this case atleast some parameters between the spatial net and the policy net will be shared), but without augmenting the input state with future predicted frames? \n\nThe Spatial net architecture is similar to convolutional LSTM — and I therefore don’t think that is a significantly novel technical contribution. The application of spatial net to augment frames in the state is although novel in my best knowledge. The above questions will help me understand the experiments better. Right now the method is slightly unclear to me and the results on ATARI (figure 11) are a bit underwhelming. Also, why did the authors chose the specific ATARI games that they reported results on — why not other games too? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comparison with closely related method is necessary",
            "review": "Summary\nThis paper propose to learning a dynamics model with future prediction in video and using it for reinforcement learning.\nThe dynamics models is a variants of convolution LSTM and it is trained mean squared error in the future frame.\nThe way of using dynamics model for reinforcement learning is similar to Weber et al., 2017, where K step prediction of the dynamics model is uses as an augmented input of the policy.\n\nStrength\nTraining dynamic model to understand physic and using it for reinforcement learning is an interesting problem that worth exploring. This paper tackles this problem and demonstrated experimental setting based on physics games. \n\nWeakness\nThe part for understanding dynamics model is very close to existing convolutional LSTM model (Xingjian et al., 2015), which is a popular baseline in video modelling community and how pretrained dynamics model is used for reinforcement learning is similar to Weber et al., 2017, but this paper does not provide comparison to any of these two baseline. \nSince the difference with these existing method is subtle, clear comparison with these method and difference in characteristic is essential to show the novelty of the paper. \n\nOverall comment\nThis paper address the interesting problem of understanding dynamics for solving reinforcement learning, but the suggested method is not novel and comparison with existing close methods are not performed.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting Direction",
            "review": "Quality: The paper proposed a new method to learn some physics prior in an environment along with a new SpatialNetwork Architecture. Instead of learning a specific dynamics model, they propose to learn a dynamics model that is action-free, purely learning the extrinsic dynamics.  They formulate this problem as a video prediction problem. A series of experiments are conducted on PhysWorld (a new physics based simulator) and a subset of Atari games.\nClarity: The writing is good.\nOriginality: This work is original as most of the model-based RL works are focusing on learning one environment instead of common rules of physics.\nsignificance of this work: This work propose an interesting direction to pursue.\n\ncons:\n1. In Figure 4, the authors show that a pretrained model can learn faster than random initialization. However, it is hard to ablate the factor that causes this effect.  Does the dynamics predictor learn the physics priors or is it just because it learn the visual prior of the shape of the objects, etc? \n2. The baseline for atari games is quite limited. First of all, 3 out of 5  atari games  in the original PPO paper show that ACER performs better than PPO. (asteroid, breakout, DemonAttack). I think it is better to make some improvement upon state-of-the-art methods.\n3. All the experiments are shown with only 3 random seeds, without error bar in the main paper. Although the reward plots are shown in Figure 11. \n4. 5 out of 10 atari games are similar to PPO (according to Figure 11). It's hard to be conclusive when half of the experiments are positive and the rest are not. \n5. Lack of discussion about ego-dynamics. There are physics priors for both the environment and the controller. Usually the controller/agent  requires an action to predict its dynamics. Then why should we omit the ego-dynamics and only model the outer world. \n6. Physics prior usually happen in physical environment. The proposed method works well in the physworld environments. But is there some task that are more realistic than atari games that can leverage the power of physics priors more? It's good that this method works in some atari games. But isn't learning the dynamics of atari games a bit off the topic? \n7. The transfer learning experiments should contain a baseline -- maml/reptile. Since you are learning physics prior, it is fair to add meta-learning baselines for comparison.\n\nI think the direction is interesting and the effort is made well. But the experiments are less convincing than the abstract/introduction.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}