{
    "Decision": {
        "metareview": "This paper proposed a method to reduce the memory of training neural nets, in exchange for additional training time. The paper is simple and looks reasonable. It's a natural followup with previous work.  The improvement over previous work is not significant, with some overhead incurred in training time. This is a borderline paper but given the <30% acceptance rate, I need to downgrade the paper to reject. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "borderline"
    },
    "Reviews": [
        {
            "title": "An intuitive idea but execution can be more convincing",
            "review": "I took a look at the revision.  I am glad to see that the authors clarified the meaning of \"optimality\" and added time complexity for each algorithm. The complexities of the algorithms do not seem great (3rd or 4th order polynomial of N) as they appear to be checking things exhaustively, but perhaps they are not a big issue since the decomposition algorithm is run only once and usually N is not huge. I wonder if more efficient algorithms exist.\n\nIt is also nice to see that the time overhead for one training step is not huge (last column of Table 1). While I still think it is better to see more complete training curves, the provided time overhead is a proxy. \n\nI hope the authors can further improve the algorithm description, for example, the pseudo-code of Algorithm 1 is very verbal/ambiguous, and it would be better to have more implementation-friendly pseudo-code.\n\nDespite the above-mentioned flaws, I think this work is still valuable in handling the memory consumption of arbitrary computation graph in a principled manner. \n\n=============================================================================\nThis paper presents a method for reducing the memory cost of training DNNs. The main idea is to divide the computational graph into smaller components (close sets), such that the dependency between components is low, and so one can store only tensors at key vertices in each component during the forward pass. In the backward pass, one needs to re-forward the data within each close set for computing the gradient. \n\nThe idea is quite intuitive and the example in linear computational graph in Section 3 clearly demonstrates the basic idea. The main technical development is in the case of arbitrary computation graph (Section 4), where the authors explain how to divide the computational graph into different types of close sets. I have not read the proofs in appendix, but it is clear that the technical development is mainly in discrete math rather than machine learning. For this part, my suggestions are:\n1. give better algorithm description: what are the inputs and outputs of each algorithm, for the examples in Figure 2-4 (and perhaps with costs for each vertex), which vertices do the final algorithm decide to store\n2. analyze the complexity of each algorithm\n3. provide clear definition of \"optimality\" and its proof: the authors mentioned in a few places about the method being \"optimal\", but the paper needs to be crystal clear about \"for what problem is the method optimal\", \"optimal in what sense (the objective)\".  \n\nThe more concerning part is the experimental evaluation. While I believe that the proposed method can save memory cost, there is no result on the additional time cost for re-forwarding. Therefore, it is not clear if it is worth the extra complication of using re-forwarding in an end-to-end sense: the authors motivated in Section 1 that saving memory cost allows one to use larger mini-batches, and an important benefit of large mini-batch is to accelerate training, see, e.g.,\nGoyal et al. Accurate, large minibatch SGD: Training imagenet in 1 hour. \nHoffer et al. Train longer, generalize better: Closing the generalization gap in large batch training of neural networks.\nIt is less desirable to use re-forwarding it if it causes significant latency in the backward pass and training. An good demonstration of the memory vs. time tradeoff would be the training curves of loss vs. running time.\n\nThe other detail I would appreciate is how the authors implemented the algorithm, and whether the proposed method is likely to be deployed on popular ML frameworks, which would improve the significance of the work.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Incremental improvements. Latency overhead is not reported. ",
            "review": "\nThis paper introduces a re-forwarding method to cut the memory footprint for training neural networks. It improved on top of previous work that did re-forwarding for linear computation graph, and this work also supports non-linear computation graph. \n\nPros:\n(1) This paper is solving an important problem saving the training memory of deep neural networks. GPU price and GPU memory increase non-proportionally, and many machine learning tasks require large GPU memory to train.  Though, there's rich literature in the area of saving the GPU memory for training. \n\n(2) This paper evaluated a large number of neural networks (Alexnet, VGG, ResNet, DenseNet). The evaluation is extensive. \n\nCons: \n(1) this paper has incremental contribution compared with Chen et al. (2016). Chen et al worked on linear computation graph. Extending the algorithm from linear graph to non-linear graph is important but the technical contribution is incremental and thin. \n\n(2) the improvement over previous work is quite marginal. For example, the memory saving went from 2894MB to 2586MB for VGG16, from 2332Mb to 1798MB for ResNet-50, just to pick a few widely used architectures. \n\n(3) The paper only reported the memory saving, but didn't report the latency overhead. It's important to know how much latency overhead is brought by this algorithm. If we save 2x memory but the training time also get much longer, users will simply reduce the virtual batch size by half and do forward the backward twice to simulate the same physical batch size. So, the authors should be clear how much training time is brought by such algorithm and report the *end-to-end training time* in Table 1, with and without re-forwarding. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Manuscript is rough -- suggestion is to reject.  ",
            "review": "In Cutting Down Training Memory by Re-forwarding the authors present a method for structuring the computation graph of a DNN to save training time.  The authors given experiments that show that the method can save up to 80% training memory.\n\nThe idea paper is nice however, this draft needs more writing work to bring to a conference standard in my opinion.\n\nMy objection to the writing begins with the Definition and example of a “Close set” which I details below.  \n\nI also have further suggestions:\n\n\nQuestions & doubts\n\nDefinition 1 : Close set\n\n* A set of vertices and edges that start from v_i and that end at  v_j.\n \n1. Vertices don’t start or end at vertices so I am already confused\n2. No notation has been introduced yet for close set so I am not sure what i and j refer to.  Also I suspect you want “or” and not “and” otherwise there could only be one possible “edge” (i,j)\n\n* sij = {v,e} is called a close set such that ∀v1 ∈ sij, v1 has no edge to any v2 \\not\\in sij ∪ {vi , vj }\n\n1. Do you mean this is a set with two elements v and e?  Probably not?\n2. Is v1 and v2 meant to be generic vertices?  If so it quite unnatural to also use v_i and v_j.  I.e., probably the name of two specific vertices in your graph is “v1” and “v2”\n3. Another question is the close set s_{ij} unique or are their potentially multiple s_{ij}?\n\n* The edges of sij are all the edges between v1 , v2 ∈ sij , all the edges between vi and v ∈ sij \n\n1. Not getting what’s going with v1 and v2 versus vi and vj.\n\nExamples of close set \na Probably you should phrase as there can exist a close s_{2,4} since …\n\nAfter reading this I got general idea that close set correspond to connected component with no connections except to oldest ancestor and youngest child.  But that is a guess — the notation and precision in the definition as well as the examples led me to have too many doubts.\n\nAlso\n\nP3 Case(1) is n and N the same?\n\nWith respect to algorithm 5 Can you discuss its computation time?\n\nIn summary this is potentially interesting work but the writing should be sharper, their should be less ambiguity of interpretation of close set. \n\nI should note — this reviewer lacks confidence in his review in so far as they have next to zero experience with DNNs.  So if given the problems in the manuscript the contribution paper would be for example a key result for DNN training this reviewer would not be able to recognize it as such.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Nice and simple",
            "review": "Summary: The paper suggests a method to reducing the space consumption of training neural nets, in exchange for additional training time. The method stores in memory only a subset of the intermediate tensors computed in the forward step, and then in the backward step it re-computes the missing tensors as they are needed by interpolating forward (again) between the stored tensors. The paper also gives a combinatorial algorithm for choosing which tensors to store on a given computation DAG annotated with vertex costs.\n\nEvaluation: I generally like the paper. The proposed method is simple and straightforward, and seems to lead to a noticeable improvement in space usage during training.\nThe part related to decomposing a DAG into \"close sets\" looks like it might overlap with existing literature in graph theory; I don't have concrete references but the authors may want to check this. The ACG solver algorithm looks somewhat wasteful in terms of the degree of the polynomial running time, but since actual computation graphs are tiny in computational terms, I guess this is not really an issue.\nOne point not addressed in the experiments is what is the overhead incurred in training time by the two space-efficient methods over usual training. I suppose one expects the training time be less than twice longer, since the re-forwards amount to one additional complete forward step per forward-backward pair.\nAnother thing that would be interesting to see is the actual stored vertices (V_R) that were chosen empirically for the rows in table 1 (or at least some rows). Since the computational graphs of the tested networks are small, and some are well-known, I imagine it should doable (though this is merely a suggestion).\n\nConclusion: The paper is simple and looks reasonable, with no major issues that I could detect.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}