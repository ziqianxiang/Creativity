{
    "Decision": {
        "metareview": "Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with \"revise and resubmit\".",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "ultimately, I am not sure there is anything \"Wasserstein\" going on in this new GAN algorithm.",
            "review": "The authors propose a new GAN procedure. It's maybe easier to reverse-engineer it from the simplest of all places, that is p.16 in the appendix which makes explicit the difference between this GAN and the original one: the update in the generator is carried out l times and takes into account points generated in the previous iteration. \n\nTo get there, the authors take the following road: they exploit the celebrated Benamou-Brenier formulation of the W2 distance between probability measures, which involves integrating over a vector field parameterized in time. The W2 distance which is studied here is not exactly that corresponding to the measures associated with these two parameters, but instead an adaptation of BB to parameterized measures (\"constrained\"). This metric defines a Riemannian metric between two parameters, by considering the resulting vector field that solve this equation (I guess evaluated at time 0). The authors propose to use the natural gradient associated with that Riemannian metric (Theorem 2). Using exactly that natural gradient would involve solving an optimal transport problem (compute the optimal displacement field) and inverting the corresponding operator. The authors mention that, equivalently, a JKO type step could also be considered to obtain an update for \\theta. The authors propose two distinct approximations, a \"semi-backward Euler formulation\", and, next, a simplification of the d_W, which, exploiting the fact that one of the parameterized measures is the push foward of a Gaussian, simplifies to a simpler problem (Prop. 4). That problem introduces a new type of constraint (Gradient constraint) which is yet again simplified.\n\nIn the end, the metric considered on the parameter space is fairly trivial and boils down to the r.h.s. of equation 4. It's essentially an expected squared distance between the new and the old parameter under a Gaussian prior for the encoder.  This yields back the simplification laid out in p.16.\n\nI think the paper is head over heels. It can be caricatured as extreme obfuscation for a very simple modification of the basic GAN algorithm. Although I am *not* claiming this is the intention of the authors, and can very well believe that they found it interesting that so many successive simplifications would yield such a simple modification, I believe that a large pool of readers at ICLR will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the Wasserstein distance, nor with a \"Wasserstein natural gradient\".\n\nother comments::\n\n*** \"Wasserstein-2 distance on the full density set\": what do you mean exactly? that d_W(\\theta_0,\\theta_1) \\ne W(p_{\\theta_0},p_{\\theta_1})? Could you elaborate where this analogy breaks down? \n\n*** It is not clear to me why the dependency of \\Phi in t has disappeared in Theorem 2. It is not clear either in your statement whether \\Phi is optimal at all for the problem in Theorem 1.\n\n*** the \"semi-backward Euler method\" is introduced without any context. The fact that it is presented as a proposition using qualitative qualifiers such as \"sufficient regularity\" is suspicious. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Providing an easy-to-implement drop-in regularizer framework, which may simply be viewed as a naive application of the proximal operator.",
            "review": "\n[summary]\nThis paper considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. More concretely, the constrained Wasserstein-2 metric $d_W$, the geodesic distance on the parameter space induced by the Wasserstein-2 distance in the ambient space, is introduced (Theorem 1). The natural gradient on the parameter space with respect to the constrained Wasserstein-2 metric is then derived (Theorem 2). Since direct evaluation of $G^{-1}$ poses difficulty, the authors go on to considering a backward scheme using the proximal operator (3), yielding:\n(i) The Semi-Backward Euler method is proposed via a second-order Taylor approximation of the proximal operator $d_W^2$ (Proposition 3).\n(ii) From an alternative formulation for $d_W$ (Proposition 4), the authors propose dropping the gradient constraint to define a relaxed Wasserstein metric $d$, yielding a simple proximal operator given by the expected squared Euclidean distance in the sample space used as a regularizer (equation (4)). The resulting algorithm is termed the Relaxed Wasserstein Proximal (RWP) algorithm.\n\n[pros]\nThe proposal provides an easy-to-implement drop-in regularizer framework, so that it can straightforwardly be combined with various generator update schemes.\n\n[cons]\nDespite all the theoretical arguments given to justify the proposal, the resulting proposal may simply be viewed as a naive application of the proximal operator.\n\n[Quality]\nSee [Detailed comments] section below.\n\n[Clarity]\nThis paper is basically clearly written.\n\n[Originality]\nProviding justification to the proximal operator approach in GAN learning via natural gradient with respect to the Riemannian structure seems original.\n\n[Significance]\nSee [Detailed comments] section below.\n\n[Detailed comments]\nTo the parameter space $\\Theta\\subset\\mathbb{R}^d$, one can consider introducing several different Riemannian structures, including the conventional Euclidean structure and that induced by the Wasserstein-2 metric. Which Riemannian structure among all these possibilities would be natural and efficient in GAN training would not be evident, and this paper discusses this issue only in the very special single instance in Section 2.3. A more thorough argument supporting superiority of the Riemannian structure induced by the Wasserstein-2 metric would thus be needed in order to justify the proposed approach.\n\nIn relation to this, the result of comparison between WGAN-GP with and without SBE shown in Figure 5 is embarrassing to me, since it might suggest that the proposed framework aiming at performing Wasserstein natural gradient is not so efficient if combined with WGAN-GP. The natural gradient is expected to be efficient when the underlying coordinate system is non-orthonormal (Amari, 1998). Starting with the gradient descent iteration derived from the backward Euler method in (3), which is computationally hard, the argument in this paper goes on to propose two methods: the Semi-Backward Euler method via a second-order Taylor approximation to the backward Euler scheme (Proposition 3), and RWP in (4) via approximation (dropping of the gradient constraint and finite-difference approximation in the integral with respect to $t$) of an alternative simpler formulation for the Wasserstein metric (Proposition 4). These two methods involve different approximations to the Semi-Backward Euler, and one would like to know why the approximations in the latter method is better in performance than those in the former. Discussion on this point is however missing in this paper.\n\nIn Section 3, it would have been better if the performance be compared not only in terms of FID but also the loss considered (i.e., Wasserstein-1), since the latter is exactly what the algorithms are trying to optimize.\n\nMinor points:\n\nPage 4: The line just after equation (4) should be moved to the position following the equation giving $d(\\theta_0,\\theta_1)^2$.\n\nIn the reference list, the NIPS paper by Gulrajani et al. appears twice.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper but need more work in the context of GANs",
            "review": "The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. Starting from the dynamical formulation of optimal transport, the authors propose the Wasserstein proximal operator as a regularization, which is simple in form and fast to compute. The proximal operator is added to training the generator, unlike most other regularizations that focus on the discriminator. This is an interesting direction. \n\nThe motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to? Personally I am not convinced that theoretically the proposed training method is better than the standard SGD. The illustration example in the paper is not very helpful as it didn’t show how the proposed proximal operator works. The proximal operator serves as a regularization and it introduces some error, I would like to know how does this carry over to the whole training procedure. \n\nIn GAN, the optimal discriminator depends on the current generator. Many approaches to GAN training (i.e. WGAN-GP) advocates to update the generator once in every “outer-iteration”. I am not sure how the proposed approach fit in those training schemes.\n\nIn the simulation, the difference is not very significant, especially in FID vs iteration number. This could be due to parameter tuning in standard WGAN-GP. I encourage more simulation studies and take more GAN structures into consideration. \n\nLastly, the stability mentioned in the paper lacks a formal definition. Is it the variance of the curves? Is it how robust the model is against outer iterations?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}