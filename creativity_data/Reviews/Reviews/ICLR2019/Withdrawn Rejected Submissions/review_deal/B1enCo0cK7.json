{
    "Decision": "",
    "Reviews": [
        {
            "title": "No real value of the work",
            "review": "This paper is about understanding robustness of neural networks w.r.t. adversarial examples.\n\nThe authors argue that a good model should be built upon a small subset of input features, with the rest being ignored as noise. If there are many more noisy features (referred as redundant ones), it is harder to find the good relevant features, hence the model becomes more susceptible to any noise introduced in the unfiltered features. This argument is presented theoretically using the notion of minimal sufficient statistics; it is stated that a model that is robust to adversary attacks must compress the input features to minimal sufficient statistics. As an example, even if there is one bit more than the minimal sufficient statistics, the model can overfit, and so the paper title.\n\nWhile the paper is a nice read to understand a new perspective about adversarial examples, it doesn't solve the existing issues with neural networks. At present, it doesn't seem like a complete work to be published.\n\nAs per the definition of adversarial examples cited in the paper, if x' has a very small l2 norm distance w.r.t. x, while x' having a class label different from x, x' is considered to be an adversarial example. As per the decades of machine learning literature, x's should not be considered an adversary example as such. I suppose it is deemed to be a reasonable definition specific to the known and unknown capabiltiies of neural networks.\n\nDepending upon a perspective, one may also find the argument on redundnancy counter intuitive. Ensemble classifiers take an advantage of redundancy while avoiding the problem of adversarial examples by learning each classifier on a subset of features or a subset of dataset. In that context, to understand adversarial examples in a principle manner as this paper attempts, one may also want to relate to the old literature for bagging, boosting, etc.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Discuss an important issue, but empirical investigation is not convincing and discussion is at times incorrect",
            "review": "Summary:\nIn this work, the authors study the problem of adversarial examples in ML from an information-theoretic point of view. They explore connections between feature redundancy and adversarial perturbations. They provide a combination of empirical and theoretical studies to justify this connection.\n\nQuality, Originality and Novelty:\nWhile the high level idea studied in this paper is interesting, however it has been suggested before in prior work [Tsipras et al., 2018].  Thus, I do not find the analysis in this paper novel or convincing. There also seem to be some technical flaws in the examples used to corroborate the claims. The experimental justification provided is lacking, as analysis is largely restricted to simple models and does not include state-of-the-art attacks. \n\nSpecific Comments:\n\nSection 2:\n- The authors are misinformed about the lack of prior work to theoretically understand the cause of adversarial examples. Many aspects of this problem have been analyzed in previous work. Some (non-exhaustive) related work - [Papernot et al., 2016, Fawzi et al., 2018, Schmidt et al., 2018, Bubeck et al., 2018, Tsipras et al., 2018].\n\n-  I do not understand the comment on optical illusions, etc. There seems to be a misunderstanding about the definition of adversarial examples. In the standard setting, adversarial examples are defined as small perturbations, which are imperceptible to humans, that cause models to classify otherwise benign inputs incorrectly. Note that the ground truth label for adversarial perturbations is determined by human classification and thus instances such as optical illusions are not adversarial examples since humans do not consider them to be part of the same class.\n\nSection 3:\n\n- The setting discussed by the authors in Figure 2a does not fit in to the standard adversarial threat model. If you consider the colored regions to be ground truth labels for the points then the colored points are not adversarial examples, but are perturbations that truly switch the underlying label of the data point. The goal of adversarially robust machine learning is not to be resilient to arbitrary magnitudes of noise, but specifically small perturbations that do not change the underlying class of a data point. In fact, in the example provided, if an ML model truly learns the linear separators then it is robust to adversarial examples.\n\n- The authors state that “random noise overflows the separation capability of a network”. As mentioned in comment 1 above, the colored points in Figure 2a are not adversarial examples. Hence it is not clear why this comment about *large* random noise applies to imperceptible adversarial perturbations. Further, it is important to note that the “true” decision boundary of the task is not affected by the presence of adversarial examples. In fact, we expect adversarial examples to capture invariances that we believe would be present in a classifier that was able to learn this true boundary. \n\n- The authors seem to conflate random noise with adversarial perturbations. It is well known that random noise is not an adversarial perturbation for state-of-the-art ML models. \n\n- Section 3.1: The example about the boolean circuit that the authors provide is extremely confusing and also likely incorrect. It is unclear what threat model the authors assume here. The authors seem to consider inputs for which the model predicts an incorrect output as adversarial examples. But these do not come from benign inputs unless we allow very large perturbations (for instance an l_inf eps = 1). Under such large perturbations, a model with both w_1, w_2 = 0 could also be made to “fail” by flipping x_1 or x_2.\n\n- Section 3.2: \n-> I had a hard time trying to understand the discussion in this section. The authors use phrases such as “bit eraser” and “overflowing decision making” which have not been properly defined in the paper. It is also unclear what the threat model being considered is. It seems as though the authors consider standard ML models as bit erasers that can already disregard redundant inputs. If this is the case, it is not clear to me what the authors mean by: “For a black box adversarial attack, we therefore just need to add enough irrelevant input to overflow this bit erasure function.” Is the attacker allowed to append an arbitrary bits to the input at test time? This is not true of standard attack models where the number of bits of the input data are fixed, and hence not clear why adversarial examples would need more bit erasures. \n-> It is well-known that training with random noise is not enough to build robust models. \n-> The authors make a broad claim about transferability, but do not provide sufficient evidence to support it. Given that we train ML models with millions of parameters on a much smaller set of training samples, it is unclear to me why different models would consistently identify the same set of redundant features.\n\n- Section 3.3: The claim that the existence of adversarial examples is a failure of standard generalization is incorrect. In the standard ML setting, we seek to minimize expected population loss on samples from the distribution. In the adversarial setting, we seek to minimize the loss on *worst-case* perturbations of inputs drawn from the distribution. This is a different and much more challenging statistical problem and likely requires more data [Schmidt et al, 2018] and compute [Bubeck et al, 2018] as has been studied out prior works.\n\n-Section 4: \n--> Section 4.1: I do not entirely understand the setup of this experiment. Do the authors retrain after setting weights to 0? Or do they simply try setting different fractions of weights to zero until the train error hits eps. It is not clear to me that setting random weights to zero is a good way to evaluate the complexity of robust models (i.e., those trained on adversarial examples). It has been noted in prior work that these robust models already tend to be sparser than standard models, so setting random weights to zero could indeed have a larger impact on these models. Did the authors verify how many of the weights are away from zero for robust models before clipping the weights? If these were included, the plots might change. The plots in Figure 4 actually contradict the authors claim that adversarial examples are harder to memorize, since the training accuracy plots look very similar for all datasets. The test accuracy plots are indeed different, but this could be because robust learning is a statistically more complex problem and needs more data as has been studied in prior work [Schmidt et al, 2018]. \n--> Section 4.2: The authors should provide more details as to how they estimate the various statistics, at least in the appendix. It is hard to draw inferences from these tables without details about the procedure to generate these statistics, how these adversarial examples were constructed (what was the epsilon, did the authors use code released by prior art) and what the models were on which these examples were generated.\n--> Section 4.3: Similar insights about feature redundancy in robust models have been observed in prior work [Ross et al., 2017, Tsipras et al., 2018].\n\n- Section 5: Using lossy compression as a defense against adversarial examples has been tried in prior art and is known to be unsuccessful [Athalye et al., 2018].\n\n\nReferences:\n\nPapernot, Nicolas, et al. \"Towards the science of security and privacy in machine learning.\" arXiv preprint arXiv:1611.03814 (2016).\n\nFawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018).\n\nBubeck, Sébastien, Eric Price, and Ilya Razenshteyn. \"Adversarial examples from computational constraints.\" arXiv preprint arXiv:1805.10204 (2018).\n\nSchmidt, Ludwig, et al. \"Adversarially Robust Generalization Requires More Data.\" arXiv preprint arXiv:1804.11285 (2018).\n\nRoss, Andrew Slavin, and Finale Doshi-Velez. \"Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients.\" arXiv preprint arXiv:1711.09404 (2017).\n\nTsipras, Dimitris, et al. \"Robustness May Be at Odds with Accuracy.\" arXiv preprint \tarXiv:1805.12152 (2018).\n\nAthalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420 (2018).\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The Claim is not supported appropriately. ",
            "review": "The authors try to provide an information-theoretic explanation of adversarial examples and claim that the possibility of making the redundant network is the key-aspect of the existence of adversarial examples. Unfortunately, the claim is not appropriately supported, and little intuition is provided for authors to make better algorithms tackling the adversarial examples.\n\nIn the paper, a network with minimal sufficient statistic is claimed to have no adversarial example (Contrapositive of Theorem 1), but the claim is far from making actionable ML processes which is mentioned as the contribution at the end of Section 1. How the practitioners can make ML processes from this claim of minimal sufficient statistic?\n\nThe entropy claim does not appropriately support the redundancy claim. Adding any noise will increase the entropy. Adversarial examples can have more entropy than original examples, but data with higher entropy do not necessarily contain adversarial examples. Entropy measure is not the key property of the existence of adversarial examples, and adversarial examples definitely have more entropy because noise is added.\n\nThe redundancy claims do not provide any novel intuition (and looks trivial), and the redundancy claim does not help improve actual algorithms.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}