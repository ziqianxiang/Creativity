{
    "Decision": {
        "metareview": "Interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, MRL) for language understanding. Experiments have been performed on the recently released SNIPS corpus and comparisons have been made with multiple recent multi-task learning approaches. Unfortunately, the proposed approach falls short in comparison to the slot gated attention work by Goo et al.\n\nThe motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose.\n\nExperimental results could be extended to the other available corpora mentioned in the paper (ATIS and GEO).\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Two stage approach for semantic parsing leveraging cross domain schemas"
    },
    "Reviews": [
        {
            "title": "Overall score 3",
            "review": "Overall Score 3\n\nThis paper introduces “Cross domain schemas” (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the user’s high-level intent. Also introduced is a model which leverages CDS to improve semantic parsing of utterances to a meaning representation language (MRL). This model first parses an utterance to CDS, then uses an encoding of the CDS jointly with the utterance encoding to decode a meaning representation. By treating different intents as separate domains, the authors construct a multi-task learning setup for CDS and MRL parsing. Results are provided for the Snips dataset of virtual assistant queries. \n\nUnfortunately, this paper fails to sufficiently explain its main proposal, the CDS. The stated goal is to explicitly define the cross-domain features that would otherwise be implicitly learned by the parameters of a neural network, yet no explicit definition is given. No rough quantification of how many or what percent of features appear across domains is provided. Rather, significant time and space is given to describing a fairly unsophisticated two-decoder model for inserting the mysterious CDS representation into the final decoding task. \n\nThe paper ignores standard semantic parsing datasets (GeoQuery and ATIS) due to their size and scope. However, comparable models (Goo et al. 2018) are trained and tested on ATIS. Moreover, an evaluation on a small, unseen target domain would be the perfect justification for the kind of cross-domain learning proposed here. \n\nInstead, this paper opts only to evaluate on the recent Snips dataset. This dataset seems to be best suited to evaluating intent classification and slot filling (intent-slot), but the current work fails to improve over what Goo et al. 2018 report on this data. In the current work, the Snips dataset is used to evaluate MRL parsing, where the CDS model shows improvements over other seq2seq models. However, since MRL can be parsed from intent-slot format by predefined rules, it is uncertain whether the CDS model outperforms the Goo et al. model at even the task of MRL parsing (no such comparison is provided). \n\nOverall, the paper suffers from some clarity issues especially regarding the definition and value of CDS. The model provided may be slightly original but is quite similar to the model of Dong and Lapata 2018. The significance of this work is questionable due to the poor comparison with recently released baseline models for the more common intent-slot task. \n\nPros\n\nIntroduces “Cross Domain Schemas” (CDS) for semantic parsing, which help improve robustness of semantic parsers by allowing models to learn patterns in one domain for use in another. \n\nThrough the use of CDS, train semantic parsers in a multi-task learning setup\n\nCons\n\nCDS is not described in sufficient detail. In particular, the possible actions and attributes are not defined. \n\nThe model is described as “multi-task learning”, however all tasks are parsing requests made of a virtual assistant. \n\nResults on standard data for semantic parsing such as GEO or ATIS are not reported.\n\nThe model does not appear to improve the results on the Snips dataset compared to the paper that introduces this dataset. Thus, the value of CDS is difficult to judge. \n\nNo per-domain analysis of the impact of CDS is provided.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Where's SRL?",
            "review": "This is a wonderful paper as it seems to have brougth Semantic Role Labelling (SRL) in the context of DL and in the context of voice search. \nResults are interesting but the paper has some major limitations. In fact, the paper totally disregard the work on Semantic Role Labelling and on languages for expressing the general meaning of language in terms of relations and in terms of concepts. \n\nThe first limitation is on the key idea. The key idea of the paper seems to be the existence of an intermediate representation language to encode meaning for utterances. Yet, this intermediate language seems to be the final language with the same relation types (for example, SearchAction) and without representations for the involved concepts (Type that becomes alternatively Film or Weather according to the target final language). This seems to be SRL where the first step is to recognize the relation and, then, the second step is to recognize the roles even if roles are slot filler types in the case of this papers.\n\nThe second limitation is on how the intermediate language has been choosen. What is the relation with FrameNet or VerbNet? Why the authors have not choosen something similar? What are the limitations of these two resources that have forced the authors to disregard them?\n\nMinor problems\n====\n- Why there are not spaces between characters and opening brackets?\n- \"compositional graph based .... language\" is a really large noun compound\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Semantic parsing model with a domain-agnostic intermediate decoding step",
            "review": "This paper describes a two-stage encoder-decoder model for semantic parsing. The model first decodes a cross-domain schema (CDS) representation from the input utterance, then decodes the final logial form from both the utterance and CDS. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset, but is still behind the traditional slot-filling models (Goo et al., 2018).\n\nMy main concern is that it is unclear to me how CDS (cross-domain schema) can be generalized to the other semantic parsing datasets, e.g., the Overnight dataset (Wang et al., 2015), which also contains multiple domains. \n\nI think it would be nice to have some details about the CDS in the paper. For example, I’m wondering 1) how is this CDS designed? 2) how are the CDS annotations derived from the target output? \n\nThere are other details missing regarding the comparisons and the evaluation metrics. In 4.2, the authors mentioned “We use accuracy as the evaluation metric’’, does “accuracy” mean full logical form accuracy or accuracy on execution results?\n\n* More minor comments:\nIn the first paragraph of Section 3, “irrelevant to domain’’ -> “domain-general’’ or “domain-agnostic’’?\n\nIt will be nice to write something more specific than “explore more ways to make it work better” in the future work.\n\nThis paper has some grammatical errors and formatting issues (e.g. missing space before punctuations).\n\n* Missing references:\nNeural semantic parsing over multiple knowledge-bases, Herzig and Berant, ACL 2017 <- This paper explores shared encoder/decoder for multi-domain semantic parsing, which is very related.\n\n(Concurrent) Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing, Herzig and Berant, EMNLP 2018 \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}