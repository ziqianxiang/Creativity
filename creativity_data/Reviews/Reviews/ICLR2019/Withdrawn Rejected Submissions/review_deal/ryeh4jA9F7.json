{
    "Decision": {
        "metareview": "Reviewers mostly recommended to reject after engaging with the authors, with one reviewer slightly suggesting to accept, but with confidence 1. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Paper decision"
    },
    "Reviews": [
        {
            "title": "nicely presented ideas, lacking discussion around guarantees (or not)",
            "review": "Being familiar but not an expert in either game theory or adversarial training, my review will focus on the overall soundness of the proposed method\n\nSummary:\n\nThe authors propose to tackle the problem of adversarial training.\nDeep networks are know to be susceptible to adversarial attacks.\nAdversarial training is concerned with the training of networks that both achieve good performance for the original task while being robust to adversarial attacks.\n\nThey propose to focus on universal adversarial perturbations, as opposed to per-sample perturbations. The latter is a subclass of the former. \nIt doesn’t strike as the most natural scenario: I can’t really think of a practical image classification scenario where one would want to perturb a whole dataset of image with a single perturbation. That said, this focus leads to simpler algorithms (complexity and storage wise) which are worth exploring.\n\nThe authors first present the min-max problem of adversarial training at hand where a classifier f mimizes a loss L for a dataset D, while the conman maximizes the loss over perturbation of the dataset \\epsilon.\nThey then introduce an algorithm to solve it inspired by fictitious play:\nA sequence of classifiers and perturbed datasets are created iteratively by the two players (classifier, conman) and each player uses the complete history of its opponent to make its next move.\n\nThe objective solved by each player  is :\nconman: fool all past classifiers with a single new perturbation\nclassifier: be robust to all past perturbations so far.\n\nAlthough it makes intuitive sense, it is unclear from the manuscript whether this formulation provides any convergence guarantees. It would be great to know whether the connection to fictitious play is purely inspirational or if any of the theoretical guarantees from game theory apply here.\n\nThe conman’s objective to fool all past classifiers is the bottleneck (in terms of storage) and an approximation is proposed: the mean loss over past classifiers is replaced by the loss under a single ‘average’ classifier trained on all past dataset, with the intuition that this average classifier summarizes all past classifiers\n\nA particular algorithm for perturbation learning is described and the proposed algorithm is compared against two baselines: a pre-existing adversarial training algorithm, an non-adversarial algorithm\n\nThe metrics chosen are accuracy and adversarial accuracy.\nOn standard classification tasks, adversarial algorithms perform slightly less well on the original task (accuracy) but are robust to perturbation as expected,\n\nIt would be interesting to know if these good performances extend to per-sample perturbations: Do a network trained on universal perturbations perform well against per sample perturbation? \n\n\nRemarks:\nsgn missing in the adversarial patch update (and who is alpha?)\nintroduce terminology: white box black box\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "A good improvement on robust adversarial training against universal perturbations, but with questions",
            "review": "In this paper, the authors proposed universal perturbation based robust training framework. With the aid of universal perturbation, the conventional robust training framework can be further interpreted as a fictitious play. Interesting algorithm and results are reported in the paper. My detailed comments are listed as follows. \n\n1) Some details of the proposed algorithm 1 are missing. In step 3, is just single SGD step performed? The generation of universal perturbation is not clearly discussed in Sec. 3.4. How to handle the expectation over the parameters of the affine transformation applied to the patch? MC particle-based approximation for these random parameters? If so, how many particles are used? \n\n2) I am confused on Algorithm\\,2 (AT). Is step 5 same as the robust adversarial training algorithm proposed by Madry \n et al.? What I recall is that SGD (for outer minimization) is only performed over perturbed samples, No? Please clarify it.\n\n3) In experiments, the authors mentioned \"The accuracy (dotted line in the plots) is the fraction of examples that have been correctly classified for a batch of 10000 samples randomly chosen in the train, validation and test sets.\" Please clearly define the train/validation/test datasets, e.g., size and how to generate adversarial examples for testing. \n\n4) In Figure 4-6, is only the universal perturbation based attack evaluated? It does not seem a fair comparison, since the proposed min-max problem builds on the generation of universal perturbations. I wonder how robustness of the proposed method against per-sample perturbation, e.g., C\\&W attack. I think it might be important to find a third-party attack method, e.g., C\\&W or physically transformed attacks, to test both fictitious play and robust adversarial training.\n\nIn general, the paper contains interesting ideas and results. However, there exist questions on their implementation details and empirical results. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea, insufficient baselines",
            "review": "The authors focus solely on universal adversarial perturbations, considering both epsilon ball attacks and universal adversarial patches. They propose a modified form of adversarial training inspired by game theory, whereby the training protocol includes adversarial examples from previous updates alongside up to date attacks.\n\nOriginality: I am not familiar with all the literature in this area, but I believe this approach is novel. It seems logical and well motivated.\n\nQuality and significance: The work was of good quality. However I felt the baselines provided in the experiments were insufficient, and I would recommend the authors improve these and resubmit to a future conference.\n\nClarity: The work was mostly clear.\n\nSpecific comments:\n1) At the top of page 5, the authors propose an approximation to fictitious play. I did not follow why this approximation was necessary or how it differed from an stochastic estimate of the full objective. Could the authors clarify?\n\n2) The method proposed by the authors is specifically designed to defend against universal adversarial perturbations, yet all of the baselines provided defend against conventional adversarial perturbations. Thus, I cannot tell whether the gains reported result from the inclusion of \"stale\" attacks in adversarial training, or simply from the restriction to universal perturbations. This is the main weakness of the paper.\n\n3) Note that as a simple baseline, the authors could employ standard adversarial training, for which the pseudo universal pertubations are found across the current SGD minibatch.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}