{
    "Decision": {
        "metareview": "This paper proposes an approach to regularizing classifiers based on invertible networks using concepts from the information bottleneck theory. Because mutual information is invariant under invertible maps, the regularizer only considers the latent representation produced by the last hidden layer in the network and the network parameters that transform that representation into a classification decision. This leads to a combined ℓ1 regularization on the final weights, W, and ℓ2 regularization on W^{T} F(x), where F(x) is the latent representation produced by the last hidden layer. Experiments on CIFAR-100 image classification show that the proposed regularization can improve test performance. The reviewers liked the theoretical analysis, especially proposition 2.1 and its proof, but even after discussion and revision wanted a more careful empirical comparison to established forms of regularization to establish that the proposed approach has practical merit. The authors are encouraged to continue this line of research, building on the fruitful discussions they had with the reviewers.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Fascinating perspective with promising initial results, but needs more careful comparison to other regularization methods"
    },
    "Reviews": [
        {
            "title": "Theoretically grounded regularizer that penalizes confident predictions, experimental section needs to be improved",
            "review": "\nThe authors propose a regularizer placed on the final linear layer of invertible networks that penalizes confident predictions, leading to better generalization. The algorithm is theoretically grounded and even though SOTA networks do not meet some theoretical requirements in practice, it seems to be effective.\n\nThe ideas presented are interesting, but the paper is confusing at times and some motivations seem hand-wavy (see below).\n\nEven though penalizing overly confident predictions is an important topic, it has been attacked by various approaches in the past. It is not clear how the proposed method empirically compares to other approaches from the literature. On the theoretical side, proposition 2.1 and its proof are the main contribution. This very interesting observation could potentially be very useful in many tasks and shows once again why invertible neural networks are an important class of deep networks.\n\nMain concerns:\n\nThe authors do not compare their method to other approaches from the literature with similar goals, such as [1]. Therefore, it is hard to judge the performance of the proposed regularizer.\n\nThe authors claim that their InvNet is approximately invertible but there is no guarantee for this, making empirical conclusions unclear. The experiments would be more conclusive if a network that is fully invertible by construction is used. Such networks exist and perform on par with ResNets [2], so there is no reason not to use them. This would remove the need for analysis or discussion of this matter, as this issue clutters the main contribution and makes the claims rather fuzzy right now.\n\nMinor\n\n- Why are citations displayed in blue? This does not seem to be ICLR formatting standard.\n\n[1] Pereyra et al., \"Regularizing neural networks by penalizing confident output distributions.\"\n[2] Jacobsen et al., \"i-RevNet: Deep Invertible Networks\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper has encouraging experimental result and the formulation is plausible, but I'm confused about how the proposed model tends to overlook irrelevant information.",
            "review": "This paper proposed to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer. They aim to maximize mutual information I(Y, \\hat{Y}) while constraining irrelevant information, which further transfers to regularization on F and w. The authors also spend pages explaining how the hyper-parameters can be chosen.\n\nComments:\n1. The experimental results showed a noticeable improvement on CIFAR-100 and is fairly robust to alpha_2.\n2. The formulation seems plausible. \n3. For Figure 2 and discussion in Section 4.2.1, I'm less convinced that the entries with high feature mean is 'relevant' and the others are not by looking at just digit 9 samples. For example, an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean. \n\n--------UPDATE AFTER READING THE AUTHORS' COMMENTS-----------\n1. Appendix F lacks explanation. So I'm going to say what I meant in details. \n\nIn order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). \nI would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced:\n1) principal components of digit 0 and digit 9 differs a lot AND \n2) w_0 weights components of digit 0 higher but weights those of digit 9 lower\n\n2. \"But for our regularized model, the number of weights with high values is smaller compared to that of normal model ...\"\nI'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review (updated after readng other reviews and other responses)",
            "review": "In this paper, the authors propose to train a model from the point of view of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. They show that the objective can be minimized by looking to make the final layer vectors be as uncorrelated as possible to the final layer representations, and simplify the same by applying Holder’s inequality to make the optimization tractable. They also apply an L1 penalty on the final layer. Experiments on CIFAR and MNIST show that using their regularizer to train DNN models yield  gains in performance. The presence of the L1 penalty also makes the results more interpretable (to the extend possible by looking at a subset of features in the last layer of a DNN). \n\nCOMMENTS:\n\n- Meta Point: To really see that the regularization framework you’re proposing is good, why not just pick a simple, feedforward model or convNet, see the performance and then compare it with the regularizer you’re proposing? That will help hit the point home. \n\n- Page 1: before jumping to equations (1) and (2), please formally define Mutual Information. The actual definition is much later in the text, but it’s better to define first. \n\n- Beyond  referring the user to section 3 on Page 1, please also mention a couple of key references in the appropriate locations. \n\n- Page 3 paragraph 2: “Mutual information is bounded … correct them” : Can you provide some formulas for this and make this concrete? Or perhaps provide some references? This line is vague. \n\n - prop 2.1 and 2.2: can you define what you mean by “empirical version”? Again, it’s probably good to have these terms crisply defined before using them. \n\n- eqn (6) is interesting. Holder’s inequality gives you the product terms. Then you can also apply the AM-GM inequality, and get a sum. So then at the end of it all, you’re left with the standard elastic net penalty and not the product form. In that case, aren’t we back to just the usual regularization strategy? And in which case, should I interpret the results you have in sec 4 as “using L1 penalties with L2 is good” ?\n\n- To the point above, I guess one difference after the AM-GM step is that you will not have a squared L2 norm, but just L2. This is reminiscent of linear models where they use L2 loss instead of squared L2 loss. But on the penalty, squaring just adds smoothness. Can you comment on this? \n\n- sec 4.2.1: I don’t see how fig (2) (L) is “roughly Gaussian”. Can you explain? Maybe plot the histogram? Also for fig (2, R): the coefficients are approximately sparse. It’s not sparse as you claim since there are almost no zeros in the coefficients. \n\n- I don’t get the point of sec 4.3: How does this claim not apply to all deep learning models, regardless of the penalizations you propose?\n\n\nedit: \nI have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers)\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}