{
    "Decision": {
        "metareview": "The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable -- which has an effect on the label -- and which prevents the discriminator from solving its task too quickly. \n\nThere was a healthy amount of back-and-forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that \"We believe our method does not even try to “regularize” the discriminator\"). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Interesting idea, but lacking theoretical support or sufficient empirical analysis.",
            "review": "This paper modifies the GAN objective by defining the TRUE and FAKE labels in terms of both the training sample, and a newly introduced random variable s. The intuition is that by progressively changing the definition of s, and its effect on the label, we can prevent the discriminator network from immediately learning to separate the two classes. \n\nThe paper doesn't give any strong theoretical support for this intuition. And it I found it a bit surprising that the discriminator doesn't immediately learn the one extra bit of information introduced by every new level of augmentation. However, the results do seem to show that this augmentation has a beneficial effect on two different architectures in different data scenarios, although the increase is not uniform over all settings.\n\nThe approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated. Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs. There is also no direct comparison to other methods of weakening the discriminator, although these are mentioned in the related work. I think the paper would be much improved by a thorough investigation of the method's effect on training stability, to go along with the current set of evaluations.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Some major flaws in the approach",
            "review": "This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong.\n\nThe paper is quite well written and clear. However there is several unsupported claims (see below).\n\nA lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator.\n\nPros:\n- The experimental section is quite thorough and the results seem overall good.\n- The paper is quite clear.\n\nCons:\n- There is a major mistake in the derivation of the proposed method. In eq. (6) & (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 <=> JSD(p||q) = 0 <=> p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either.\n\n- The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator.\n\n- I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator.\n\n- The authors claim that their method doesn't \"just memorize the true data distribution\". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. \n\n- The authors states that \"adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training\", can the author give some references to support that statement ?\n\n- According to the author: \"Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime\". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean.\n\nOthers:\n- It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ?\n\nConclusion:\nThe idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved.\n\nReferences:\n[1] Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Provide an additional bitstring to the discriminator which can swap the label for observed and generated samples.",
            "review": "Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.\n\nPro:\n- A simple idea to make the problem progressively more difficult.\n- The writing is relatively easy to follow.\n- Standardized experimental setup.\n\nCon:\n- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.\n- Clarity can be improved: several pages of theory can really be summarized into “learning the joint distribution implies that the marginals are also correctly learned’ (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. \n- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a \"lens\" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.\n- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?\n\n[1] http://proceedings.mlr.press/v80/sajjadi18a.html\n[2] https://arxiv.org/abs/1706.08500",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}