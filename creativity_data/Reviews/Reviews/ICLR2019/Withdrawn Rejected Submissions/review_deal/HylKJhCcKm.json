{
    "Decision": {
        "metareview": "The paper proposes to replace dynamic routing in Capsule networks with a trainable layer that produces routing coefficients. The goal is to improve their scalability. This is promising as a research direction but reviewers have raised several concerns about unclear contributions and lack of a thorough evaluation of the approach. There is also a recent relevant work pointed out by Reviewer 1 that should be discussed. Given these concerns, the paper is not suitable for publication in its current form, however I encourage the authors to use reviewers' comments for improving the paper and resubmit in next venues.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Promising as a direction of research but reviewers have several concerns with the paper"
    },
    "Reviews": [
        {
            "title": "Replacing dynamic routing with trainable layers is interesting, but the contributions of the paper are not very clear.",
            "review": "This paper proposes to replace the dynamic routing layer of the original capsule networks with a trainable neural network layer. The idea is interesting. However, the following problems concern me.\n\nThe model is named as “generalized capsule networks” but it is not very clear what “generalized” means here. Does making the dynamic routing layer trainable generalize capsule?\n\nThe contributions of the paper will be clearer if further comparison is provided. The model is proposed based on capsule nets but it lacks comparison between the proposed model and the original capsule nets. For example, the experiments did not include the original CapsNet models (Sabour et al., 2017 or Hinton et al., 2018), which, if performed, would help understand the differences/advantages of the proposed models.\n\nThe major modification made on capsule nets is on the dynamic routing layer. In order to incorporate routing into the whole trainable process, this paper incorporate the coefficients c_{ij} to the model parameters. It is not clear if the proposed model constrains c_{ij} are further constrained, e.g., (as in the original capsule nets) to sum up to 1 along the dimension i?  \n\nIn general, the paper is well structured and easy to follow. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Reinvention of what is already proposed by Sabour et al.",
            "review": "The paper deals with the idea to generalize the CapsNet architecture from Sabour. Under generalization the authors mean, to define a routing procedure without an iteration parameter.\n\nIn general, your paper has a good length, is well explained and good organized. To be honest, I don’t like your writing style. It seems to be a bit too casual and not formal enough for a scientific work. Additionally, note that:\n-\tYou are not staring with Fig. 1 in the introduction…the counting should start by one.\n-\tAC-GAN is the abbreviation for? \n-\tEq. 2 is outside the page space.\n\nI have several concerns about the contribution. My major concern is that it isn’t new in general. If I break down your method, it is just the basic Dynamic Routing procedure with:\n-\tthe number of iterations defined to be one;\n-\ttrainable initial routing coefficients;\n-\tno softmax normalization over routing coefficients.\nThe usage of trainable initial routing coefficients was already mentioned by Sabour. Thus, the only thing which is new in your method is that you skip the normalization and I’m not sure that this has a positive effect on the process. \n\nMinor concerns/minor mistakes:\n1.\tYou mentioned that the code is public available. Where is the link to a respective repository?\n2.\tPage 1: “[…] so it makes sense to believe that CapsNet has a better generalization ability.” Compared to what?\n3.\tPage 2: “The routing iterations is a meta-parameter that needs to be set manually which limits the scalability of CapsNet […].” Why it should limit the scalability? It has no effect on the model size, etc. It’s just a parameter which has to be defined.\n4.\tPage 3: Are you sure that a linear transformation of a hyper-cube is defined in that way in general?\n5.\tPage 4: What is T_k?\n6.\tPage 7: “Hinton et al. (2018) claimed that CapsNets […]” Are you aware that Hinton worked on Matrix Capsules and not on the CapsNet architecture of Sabour?\n7.\tPage 8: How you can guarantee the convergence of your method? Moreover, the convergence to what?\n8.\tCould you add some histograms plots of your c_ij values after the training?\n9.\tWhy are your performance values so bad compared to CapsNet and Matrix Capsules?\n10.\tCould you add to your tables the inference, training times? If you remove the iteration parameter I would assume that your method should be faster, or?\n11.\tIs the parameter lambda in Eq. 2 the same as in Eq. 6? How you tune that parameter?\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper is poorly written, not clear about the contributions, evaluation is questionable",
            "review": "Pros:\n\nThe paper claims to make CapsuleNet's routing trainable. The proposed G-CapsNet have two variants (within feature map and across feature map). \n\nIt presents evaluation of G-CapsNet in terms of robustness and generalization. It is interesting that Capsule networks are as bad as traditional CNNs for strong white box attacks.\n\nCons:\n\nThe idea is not clearly explained. It seems that the main idea is to relax routing from a discrete problem to a continuous problem. Making the routing assignments as a regularization term in the loss function. The assignment fraction can be trained end-to-end. However, the paper discusses two loss functions in Equation 2 and 6. It is not clear how the two loss functions are related. \n\nIt is not clear why fractional routing is more efficient than the original non-trainable CapsuleNet routing. There is not clear explanation or evaluation.\n\nThe authors did not reproduce the baseline CNN model used in the original CapsuleNet routing. The original one is 0.39% error rate and the authors' implementation is 0.83%. So this makes G-CapsNet result much worse than the original CapsuleNet. So it is not clear what the benefit of G-CapsNet over the original one.\n\nThere is a related paper on approximate routing, see:\nNeural Network Encapsulation, ECCV 2018\nhttp://openaccess.thecvf.com/content_ECCV_2018/papers/Hongyang_Li_Neural_Network_Encapsulation_ECCV_2018_paper.pdf\n\nOverall, the paper does not have enough contributions both in terms of new methods and evaluations. It does not meet the expectation of ICLR acceptance.\n\nResponse to rebuttal:\nThe authors clarified the questions. However, I maintain my rating because the contributions are limited and the paper is very poorly written.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}