{
    "Decision": {
        "metareview": "Strengths:\n\n- well-written \n- strong results for non-autoregressive NMT\n- a novel soft EM version of VQ-VAE\n\nWeaknesses:\n\n-  as pointed out by reviewers, the improvements are mostly not due to the VQ-VAE modification rather due to orthogonal (and not interesting) changes e.g., knowledge distillation. If there is a genuine contribution of VQ-VAE, it is small and required extensive parameter selection\n\n-  the explanations provided in the paper do not match the empirical results\n\nTwo reviewers criticize the experiments / experimental section: rigour / their discussion.  Overall, there is nothing wrong with the method but the experiments are not showing that the modification is particularly beneficial.  Given these results and also given that the method is not particularly novel (switching from EM to Soft EM in VQ-VAE), it is hard for me to argue for accepting the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "a reasonable method but empirical evidence is questionable"
    },
    "Reviews": [
        {
            "title": "interesting parts, but needs more rigour",
            "review": "This paper discusses VQ-VAE for learning discrete latent variables, and its application to NMT with a non-autoregressive decoder to reduce latency (obtained by producing a number of latent variables that is much smaller than the number of target words, and then producing all target words in parallel conditioned on the latent variables and the source text). The authors show the connection between the existing EMA technique for learning the discrete latent states and hard EM, and introduce a Monte-Carlo EM algorithm as a new learning technique. They show strong empirical results on EN-DE NMT with a latent Transformer (Kaiser et al. (2018)).\n\nThe paper is clearly written (excepting the overloaded appendix), and the individual parts of the paper are interesting, including the link between VQ-VAE training and hard EM, the Monte-Carlo EM, and strong empirical results. I'm less convinced that the paper as a whole delivers on what it promises/claims.\n\nThe first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018). The paper attributes this to tuning of the code-book, but the results (table 3) seem to contradict this, with a code-book size of 2^16 even slightly better than the 2^12 that is used subsequently. The reason for the performance difference to Kaiser et al. (2018) remains opaque. While interesting, the empirical effectiveness of Monte-Carlo EM is a bit disappointing, achieving +0.3 BLEU over the best configuration for EN-DE (after extensive hyperparameter tuning, seen in table 4), and -0.1 BLEU on EN-FR. Monte-Carlo EM also seems very sensitive to hyperparameters, namely the sample size (tables 4,5), contradicting the later claim that EM is robust to hyperparameters. The last claimed contribution (using denoising techniques) is hidden in the appendix, an application of an existing technique, and not compared to knowledge distillation (another existing technique).\n\nI'd like to see some of the results in the paper published eventually. However, the claims need to better match the empirical evidence, and for a paper that has \"better understanding\" in the title, I'd like to gain a better understanding of the differences to Kaiser et al. (2018) that make VQ-VAE fail for them, but not in the present case.\n\n+ clearly written paper\n+ interesting, novel EM algorithm for VQ-VAE\n+ strong empirical results on non-autoregressive NMT\n\n- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.\n- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .\n- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " Training procedure for VQ-VAE is equivalent to the EM algorithm",
            "review": "General:\nThe paper presents an alternative view on the training procedure for the VQ-VAE. The authors have noticed that there is a close connection between the original training algorithm and the well-known EM algorithm. Then, they proposed to use the soft EM algorithm. In the experiments the authors showed that the soft EM allows to obtain significantly better results than the standard learning procedure on both image and text datasets.\n\nIn general, the paper shows a neat link between the well-known EM algorithm and the learning method for the VQ-VAE. I like the manner the idea is presented. Additionally, the results are convincing. I believe that the paper will be interesting for the ICLR audience.\n\nPros:\n+ The connection between the EM algorithms and the training procedure for the VQ-VAE is neat.\n+ The paper is very well written, all concepts are clear and properly outlined.\n+ The experiments are properly performed and all results are convincing.\n\nCons:\n- The paper is rather incremental, however, still interesting.\n- The quality of Figure 1, 2 and 3 (especially Figure 3) is unacceptable.\n- There is a typo in Table 6 (row 5: V-VAE → VQ-VAE).\n- I miss two references in the related work on training with discrete variables: REBAR (Tucker et al., 2017) and RELAX (Grathwohl et al., 2018).\n- The paper style is not compliant with the ICLR style.\n\n--REVISION--\nI would like to thank authors for their effort to improve quality of images. In my opinion the paper is nice and I sustain my initial score.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A soft-EM training algorithm for vector-quantized autoencoders",
            "review": "Summary: \n\nThis paper presents a new training algorithm for vector-quantized autoencoders (VQVAE), a discrete latent variable model akin to continuous variational autoencoders.\nThe authors propose a soft-EM training algorithm for this model, that replaces hard assignment of latent codes to datapoints with a weighted soft-assignment.\n\nOverall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.\n\nThe technical presentation of the work by the authors starts only at page 5 (taking less than a full page), after several pages of imprecise presentation of previous and related work. The paper could be significantly improved by making this preceding material more concise and rigorous. \n\nQuantitative experimental evaluation is limited to a machine translation task, which is rather uncommon in the literature on generative latent variable models. I would expect evaluation in terms of held-out data log-likelihood (ie bits-per-dimension) used in probabilistic generative models, and possibly also using measures from the GAN literature such as inception scores. Datasets that are common include CIFAR-10 and resized variants of the imagenet dataset. \t \n\n\nSpecific comments:\n\n- Please adhere to the ICLR template bibliography style, which is far more readable than the style that you used. \n\n- Figure 1 does not seem to be referenced in the text. \n\n- The last paragraph of section 2.1 is unclear. It mentions a sampling a sequence of latent codes. The notion of sequentiality has not been mentioned before, and it is not clear what it refers to in the context of the model defined so far up to that point. \n\n- The technical notation is very sloppy. \n* In numerous places the paper refers to the joint distribution P(x1,…,x_n, z1, …, zn) without defining that the distribution factorizes across the samples (xi,zi), and without specifying the forms of p(zi) and p(xi|zi). \n* This makes that claims such as “computing the expectation in the M step (Equation 11) is computationally infeasible” are not verifiable. \n\n- Please be clear about how much is gained by replacing the exact M-step with a the one based on the samples from the posterior computed in the E-step. \n\n- What is the reason to decode the weighted average of the embedding vectors, rather than decoding all of them, and updating the decoder in a weighted manner?\n\n- reference 14 for Variational autoencoders is incorrect, please use the following citation instead: \n@InProceedings{kingma14iclr,\n  Title                    = {Auto-Encoding Variational {B}ayes},\n  Author                   = {D. Kingma and M. Welling},\n  Booktitle                = {{ICLR}},\n  Year                     = {2014}\n}\n\n- The related work section (4) provides a rather limited overview of relevant related work. \nHalf of it is dedicated to recent advances in machine translation, which does not bear a direct connection to the technical material presented in section 3.\n\n- There is no justification of using *causal* self-attention on the source embedding, is this a typo?\n\n- As for the experimental evaluation results: it seems that distillation is a much more critical factor to achieve good performance than the proposed EM training of the VQ-VAE model. Unfortunately, this fact goes unmentioned when discussing the experimental results. \n\n- What is the significance of the observed differences in BLEU scores? Please report average performance and standard deviations over several runs with randomized parameter initialization and batch scheduling. \n\n- It seems that the tuning of the number of discrete latent codes (table 2 in appendix) and other hyper-parameters (table 3 in appendix) was done on the test set, which is also used to compare to related work. A separate validation set should be used for hyper parameter tuning in machine learning experiments.\n\n- It seems that all curves in figure 3 collapse from about 45 BLEU to values around 17 BLEU, why is this? The figure is hard to read since poor quality, and curves that are superposed. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experimental section",
            "review": "This paper introduces a new way of interpreting the VQ-VAE, \nand proposes a new training algorithm based on the soft EM clustering. \n\nI think the technical aspect of this paper is written concisely. \nIntroducing the interpretation as hard EM seems natural for me, and the extension\nto the soft EM training is sound reasonable. \nMathematical complication is limited, this is also a plus for many non-expert readers. \n\nI'm feeling difficulties in understanding the experimental part.\nTo be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. \nI'm just wondering why this happens, given clean and organized technical sections...\n\nFirst, I'm confusing what is the main competent in the Table 1. \nIn the last paragraph of the page 6, it reads; \n\"Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10).\"\nHowever, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? \n\nSecond, terms \"VQ-VAE\", (soft?)\"EM\" and \"our {model, approach}\" are used in a confusing manner. \nFor example, in Table 1, below the row \"Our Results\", there are:\n- VQ-VAE\n- VQ-VAE with EM\n- VQ-VAE + distillation\n- VQ-VAE with EM + distillation\n\nThe \"VQ-VAE\" is not the proposed model, correct? \nMy understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to \"VQ-VAE with EM\". \n\nThird, a paragraph \"Robustness of EM to Hyperparameters\" is mis-leading. \nThe figure 3 does not show the robustness against a hyperparameter. \nIt shows the BLEU against the number of \"samples\" (in fact, there is no explanation about what the \"samples\" means). \nI think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. \nThe figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. \nHowever, there is no explanation which hyperparameter is tested to assess \"the robustness to hyperparameters\". \n\nFourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. \nIn fact, there is a short paragraph that mentions about the SVHN results, \nbut it only refers to the appendix. \nI think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. \nHowever, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. \nSo, the authors should include the image reconstruction results in the main body of the paper. \nOtherwise, claims about the image reconstructions should be removed from the abstract, etc. \n\n\n+ Insightful understanding of the VQ-VAE as hard EM clustering\n+ Natural and reasonable extension to soft-EM based training of the VQ-VAE\n-- Unorganized experiment section. This simply ruins the quality of the technical part. \n\n\n## after feedback\n\nSome of my concerns are addressed the feedback. \nConsidering the interesting technical parts, I raise the score upward, to the positive side. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}