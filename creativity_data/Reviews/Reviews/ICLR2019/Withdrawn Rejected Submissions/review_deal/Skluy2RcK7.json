{
    "Decision": {
        "metareview": "The paper examined the folk-knowledge that there are highly selective units in popular CNN architectures, and performed a detailed analysis of recent measures of unit selectivity, as well as introducing a novel one. The finding that units are not extremely selective in CNNs was intriguing to some (not all) reviewers. Further, they show recent measures of selectivity dramatically over-estimate selectivity.\n\nThere was not tight agreement amongst the reviewers on the paper's rating, but it trended towards rejection. Weaknesses highlighted by reviewers include lack of visual clarity in their demonstrations, the use of a several-generations-old CNN architecture, as well as a lack of enthusiasm for the findings.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Detailed analysis of unit selectivity, but reviewers unconvinced of impact"
    },
    "Reviews": [
        {
            "title": "Shows that single units are not perfectly class selective, a result that will be intuitive to many",
            "review": "Summary: \n\nThis paper explores different metrics to measure the ‘selectivity’ of single neurons for a class in deep neural networks. Using AlexNet as the model under study, the paper shows strengths and weaknesses of several recent methods in the literature. The paper conducts a psychophysics experiment to see if human subjects can reliably label images generated through activation maximization techniques. \n\nMajor comments:\n\nThis paper undertakes a careful analysis of different ways of measuring single-unit selectivity for a class. The conclusions drawn are that no neurons exhibit true localist selectivity, and most have some more complex selectivity. Some of the specific examples make this point very nicely (for instance, a unit that responds very strongly to several custard apples and would appear to be a custard apple detector, except that it responds extremely weakly to other custard apples). This is a somewhat negative result that may be useful in advancing the field away from single neuron analyses, which may be misleading.\n\nOne worry is that the methods applied are looking for a very strong form of selectivity. In particular, even the output layer is judged to contain a low percentage of selective units according to the definitions in the paper. It may be worth considering slightly weakened versions of the metrics that allow for some errors. \n\nIt would be useful to add discussion of the connections between these metrics and generalization performance. The class conditional selectivity metric, for instance, may not measure localist coding very directly, but it does correlate with important performance metrics like generalization performance. The discussion in Morcos 2018 suggests that high single unit selectivity is detrimental to generalization. Do these correlations persist using other metrics?\n\nThe psychophysics experiment with human subjects appears to have been done to a high standard, and yields the result that only the very highest layers of a network yield interpretable images. This is somewhat interesting but unlikely to be that surprising, as selectivity for objects in lower layers is not a claim made by many works. In these lower layers, selectivity for ‘object parts’ is a claim that has been made and could potentially be addressed by the data collected.\n\nOverall this paper critically analyzes single unit selectivity measures, reaching the conclusion that tuning in modern deep networks is usually far more complex than strict localist coding. The significance of this conclusion may not be so high given that this conclusion is probably already the intuition of many.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Surprising result and raises interesting questions.",
            "review": "Summary - This paper analyzes the selectivity of individual units in CNNs. The authors analyze existing techniques such as precision selectivity, class-conditional mean activity selection and localist sensitivity. These methods are analyzed in the context of AlexNet and ImageNet. The authors also use Activation Maximization (AM) techniques for visualizing single-unit representations in CNNs.\n\n\nPaper strengths\n- The authors have minutely examined each of the metrics and the underlying assumptions they make. \n    - Example - Number of images used for computing the precision threshold in Zhou et al., 2014; The wrongly stated range of CCMAS [0, 1]. Considering the second highest CCMAS class is a good way of handling multiple classes that activate a single unit.\n- The results of this paper are surprising compared to existing work. The authors have made a surprising discovery and done a good job of both presenting it well and experimentally validating it. The paper raises interesting questions and this should inspire future work in understanding networks.\n- Figure 2 is insightful - It compares the various different interpretations of selectivity for a single unit in fc6. It shows how the mean activating class and the maximally activating class can be semantically very different. It also shows that despite the high precision and CCMAS score, the unit cannot be labelled as a detector for the single concept \"custard apple\". More such results are presented in the Appendix (e.g. Fig A6)\n- The human study in Section 3.3 is a good way to evaluate the generated AM images.\n\n\nPaper weaknesses\n- One of the major weaknesses of this paper is that it uses only ImageNet images to evaluate the units. As this is limited to 1000 classes, the authors cannot probe other visual concepts such as color, texture, materials for the units. As an example, Network dissection (Zhou et al., 2017) proposes a dataset called Broden which has many diverse sets of visual concepts labeled. This paper focuses only on one definition of selectivity - selecting objects. This should be made explicit and the authors have not done a good job of clarifying this assumption or showing that it exists.\n- All of the analysis is limited to AlexNet. With modern architectures that use residual/skip connections, it is not clear how well this analysis will generalize. It is an open question if the authors work overfits to AlexNet.\n- The jitterplots are hard to understand especially if there are many overlapping \"dots\" (samples). Since the y-axis values are not really meaningful anyway, using a histogram to see how many samples have a particular activation value is easier. A possible suggestion for Figure 2(a): split into two parts - 1) histogram of all samples; 2) histogram of the highest mean activating class.\n- The organization of the paper could be improved. The sections in the paper are not well connected.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Poorly written with unclear contributions",
            "review": "This is a paper with scattered potentially interesting ideas. But the execution is limited and the writing poor with critical details lacking.  A major limitation of the paper is that it is not clear what contribution it makes. Some of the analyses are indeed interesting but 1) these analyses are mostly descriptive and 2) they are limited to one particular (outdated) architecture. How would batch norm or residual connections or any of the developments that have happened since AlexNet affect these results?\n\nAs a side note, the references/comparisons between AlexNet and recurrent nets (see abstract, etc) are misleading. This is based on the claim that Bowers et al (2014) qualitatively different results but this is for entirely different domains (words). Indeed what could have made potentially the work more relevant would have been to show some kind of benchmarking between AlexNet and alternative architectures (possibly RNNs). As such the current study does not contribute much except for comparing different semi-arbitrary measures of selectivity for one specific (outdated) network architecture trained on a particular problem (ILSVRC).\n\n****\nMinor points:\n\nThe study is limited to correctly classified images as stated on page 3. This seems like a major confound in a study aimed at understanding the visual representations learned. It seems to me that the conclusions of the paper could be heavily biased because of this (when computing any measure based on inter and intraclass responses).\n\nIn general, this is a relatively poorly written paper which would be hard to reproduce. For instance, the image generation for activating units (assuming it is novel) could be interesting but it is not even described with sufficient details so as to reproduce the results.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}