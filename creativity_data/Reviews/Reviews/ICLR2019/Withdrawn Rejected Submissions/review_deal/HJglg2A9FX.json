{
    "Decision": {
        "metareview": "This paper addresses the problem of learning with outliers, which many reviewers agree is an important direction. However, reviewers point to issues with the experiments (missing baselines, ablations, etc.) and are concerned that the assumptions in the theoretical analysis are too strong. These were potentially addressed in a revised version of the paper, but the revisions are so major that I do not think it is appropriate to consider them in the review process (and it is hard to assess to what extent they address the issues without asking reviewers to do a thorough re-appraisal, which goes beyond the scope of their duties). I encourage the authors to take reviewer comments into account and prepare a more polished version of the manuscript for future submission.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "problems with experiments and assumptions; post-deadline revision too large"
    },
    "Reviews": [
        {
            "title": "Tackles important problem but needs more fleshing out",
            "review": "The authors propose an iterative method for discarding outlying training data: first, learn a model on the entire training dataset; second, identify the training examples that have high loss under the learned model; and then alternate between re-learning the model on the training examples that do not have high loss, and re-identifying the training examples with high loss under the new model. This method works for both supervised and unsupervised learning, and the authors show that in theory, their method has some convergence properties in the mixed linear regression and Gaussian mixture model settings. The authors also run some experiments on neural networks and datasets with synthetic noise to show the benefits of their proposed method.\n\nThe problem of noisy datasets is relevant to almost all machine learning problems in the real world, and the authors' method shows promise as a straightforward way to increase performance on such noisy data. However, my opinion is that the authors need to make a stronger case, theoretically and/or experimentally, for why their method should be preferred to other methods. Detailed comments follow.\n\n== Experiments ==\n1) No comparisons are provided to other outlier detector methods (e.g., based on nearest neighbors, distance to centroid, influence functions, etc.) or techniques that also purport to deal with noisy labels (e.g., by modifying the learning algorithm or loss function). While there are too many existing methods to expect the authors to benchmark against all of them, it's important to at least have a couple of representative comparisons. \n\n2) It'd be nice to have an ablative analysis to tease out the factors behind the gain in accuracy. For example, is the iteration important, or would a single pass suffice? How robust is the algorithm to tau, the fraction of data to discard? (The authors do test initializing randomly vs. initializing on the full dataset.) \n\n3) The systematic label noise scenario seems to favor the authors' method (though the authors claim that it is a harder scenario than random label noise). It'd be helpful to see if the method works against random noise.\n\n== Theory ==\n4) The assumptions seem very restrictive. For example, for mixed linear regression (section 4), the features of all examples are assumed to be drawn i.i.d. from an isotropic Gaussian (so even the bad samples are drawn from the same distribution as the good samples; and all features are independent). To my knowledge, this is not a \"standard and widely studied\" assumption. For the Gaussian mixture model (section 5), a similar isotropic Gaussian assumption is made for each mixture. \n\n5) Beyond the independence assumptions mentioned above, the initialization results make additional assumptions on the \"bad\" data (e.g., average distance of the good vs. bad parameters) that I found hard to parse. How strong are these assumptions? Do they hold on real datasets?\n\n6) The convergence results (Theorems 1 and 3) have a constant term sigma in them. This is surprising and seems to me to considerably weaken the result -- one would expect that the dependence on sigma will decrease with n.\n\nI think either a strong experimental or strong theoretical section would be sufficient for me to recommend acceptance. However, the paper currently shows potentially interesting experimental/theoretical results but does not do a comprehensive job of either side.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A well written paper but has major problems",
            "review": "This paper introduces a framework for situation when the training samples are not pure. The idea is a simple approach by training a model and removing a portion of examples from the training set based on the loss of the model. The authors provide some theoretical study on two models: linear regression and Gaussian mixture model and utilize deep neural network to show their framework performs well experimentally. \n\nMy main problem with this work is the difficulty in understanding whether the reason our training model produces a large loss on some examples is due to them being bad examples or is because the model is not good enough and needs improvement. For example, one can always overtrain a classifier such that it classifies the training examples perfectly. Now the question become how much should I train my classifier. In case of Deep Neural Networks for example, the number of epochs can change the loss occurred by classifiers on the examples and it is not easy to know when to stop training in order to utilize the procedure introduced in this work.\n\n\nThe theoretical work is related to linear regression and Gaussian Mixture model but the experiments are relayed to Deep Neural Nets! So I am not sure if this setup makes sense. Either both should be for DNN or neither should be.\n\nI am not sure if I understand Section 5 and the discussion related to the Gaussian Mixture Model. In Gaussian Mixture model, there are multiple components and each commonest has its own parameteres. So not sure (1) why the authors assume only mean parameter. (2) Given that Gaussian mixture model assumes multiple components, doesn't it automatically address the problem by putting the samples from different distribution in a different component?\n\nPage 5 typo: closest point closest to\n\nThe parameter \\tau is set to 5 percent less that the true ratio of good samples (correct labels). This seems a pretty bias choice and implicitly applied that one needs to know the true value of this ratio which is a huge expectation. The authors need to investigate the effect of the changes of this value on the performance of their proposed framework! To me, it seems that the results can be hugely affected by the value of this parameter. \n\nThe experiment with GAN is very wired. How can you expect to have a data set with 20 percent of its examples be bad cases. The authors need to justify that such cased can happen in real applications. \n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "novelty over early work? different angle to view this problem - sparse learning. Proofs/statements need to more rigorous.",
            "review": "This paper provides an algorithm that excludes the bad training data in the training process and obtain a more accurate model for both supervised and unsupervised learning problem. The paper gives the theoretical guarantee for mixed linear regression and Gaussian mixture model, and also conducts the experiments for deep image classification and deep generative models.\n\nMajor Concerns:\n1, As said in related work, a soft version of this paper’s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.\n\n2, In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  “strict inequality” property in the 3rd line in Theorem 2, because it should correspond to the  “strict inequality” property in the 2nd line in Theorem 1.\n\n3. Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to \n\nA Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017\n\nThe comparison to these type of methods need to be included. \n\nMinor Concerns:\n1, Theorem 2 does not give the probability, only mentioning “high probability”. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that\n\n2, In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}