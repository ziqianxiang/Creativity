{
    "Decision": {
        "metareview": "\nThis is an interesting direction and all reviewers thought the idea has merit, but pointed out some significant limitations. The authors did an admirable job at addressing some of these but some remain, including R1’s point 2 which is a significant issue. The authors are encouraged to submit a revised version of their work which addresses all the discussed limitations and will likely be a competitive submission to another top ML conference.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Not quite there yet. "
    },
    "Reviews": [
        {
            "title": "An interesting approach for imitation learning, but need to verify the method thoroughly",
            "review": "In this paper, the author derived a unified approach to utilize demonstration data and the data collected from the interaction between the current policy and the environment, inspired from soft Bellman equation. Different from previous methods such as DQfD (Hester et al., 2017), SQIL does not require the reward signal of the expert data, which is more general and natural for real-world applications such as demonstration from the human.  The author verified SQIL on a toy Lunar Lander environment and a high-dimension image based observation environment, which demonstrate its advantages over behavior cloning and GAIL. Besides the advantages, I have serval concern which may help the author to further improve the paper.\n\n- The core algorithm is simple but I found the derivation is hard to read, which is a little messy from equation (5) to (7), and also the final loss (14) seems to be unrelated to the previous derivations( from equation (11) to (13)).  Also, can you add the gradient updates for $\\theta$ with equation (11) (denoted by SQIL-11 in your paper)? I am looking forward to reading the revised version.\n\n- To demonstrate the advantages of SQIL in high-dimension observations, the author only conducts one simple environment Car Racing, which is not enough to demonstrate its advantages. I wonder if it is possible to run more benchmark environments such as Atari game or Minecraft.\n\n- In the related work, the author argues that methods such as DQfD require reward signal, but it would be great to demonstrate the advantages of SQIL over these methods (including DQfD and NAC (Gao et.al, 2018)).\n\n- In previous imitation methods such as GAIL, they studied the effect of the amount of the demonstration data, which the paper should also conduct similar experiments to verify the advantage of SQIL.\n\n\nHester, Todd, et al. \"Deep Q-learning from Demonstrations.\" arXiv preprint arXiv:1704.03732 (2017).\nGao, Yang, et al. \"Reinforcement learning from imperfect demonstrations.\" arXiv preprint arXiv:1802.05313 (2018).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an imitation learning algorithm framed as an off-policy RL scenario. They introduce all demonstrations into a replay buffer, with positive reward r=1. Subsequent states derived from the agent will be also introduced in the replay buffer but with reward r=0. There is no additional reward involved. The hope is that agents will learn to match the expert in states appearing in the replay buffer and not try to do much else.\n\nOverall, I am not convinced that the paper accurately supports its claims.\n\n1.\tThe authors support their method by saying that extending GAIL and other imitation learning algorithms to support pixel observations has failed. However, there are actually some papers showing quite successful applications of doing that: e.g. see Li et al. 2017 in challenging driving domain in TORCS simulator. \n\n2.\tMore problematic, I think there is a clear flaw with this algorithm: imagine having trained successfully and experiencing many trajectories that are accurately reproducing the behaviour of the expert. Given their method, all these new trajectories will be introduced into the replay buffer with a reward of 0, given they come from the agent. What will the gradients be when provided with state-action pairs with both r=0 and r=1? These situations will have high variance (even though they should be clear r=1 situations) and this will hinder learning, which will tend to  decay the behaviour as time goes on.\nThis actually seems to be happening, see Figure 1 at the end: both SQIL curves appear to slowly starting to decay.\nThis is why GAIL is training its discriminator further, you want to keep updating the distribution of “agent” vs “expert”, I’m not sure how this step can be bypassed?\n\n3.\tHow would you make sure that the agent even starts encountering rewarding states? Do you need deterministic environments where this is more likely to happen? Do you need conditions on how much of the space is spanned by the expert demonstrations?\n\n4.\tAdditionally, Figure 1 indicates BC and GAIL as regions, without learning curves, is that after training? \n\n5.\tI am not convinced of the usefulness of the lengthy derivation, although I could not follow it deeply. Especially given that the lower bound they arrive at does not seem to accurately reflect the mismatch in distributions as explained above. \n\n6.\tThere are no details about the network architecture used, about the size of the replay buffer, about how to insert/delete experience into the replay buffer, how the baselines were set up, etc. There are so few details I cannot trust their comparison is fair. The only detail provided is in Footnote 5, indicating that they sample 50% expert and 50% agent in their mini-batches.\n\nOverall, I think the current work does not offer enough evidence and details to support its claims, and I cannot recommend its publication in this current form\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and promising idea that could be practical",
            "review": "The paper introduces a relatively simple method for imitation learning that seems to be successful despite its simplicity. The method, SQIL, assigns a constant positive reward (r) to the demonstrations and zero reward to generated trajectories. While I like the connections between SQIL and SQL and the simplicity of the idea, I think there are several issues which connections with GAIL that are not discussed; some \"standard\" environments (such as Mujoco) that SQIL has not compared against the baselines. I believe the paper would have a bigger impact after some addressing some of the issues.\n\n(\nUpdate: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.\n\nThe Pong case is also quite interesting, although it seems slightly \"unfair\" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards.\n)\n\n1. The first issue is the similarity with GAIL in the \"realistic\" setting. Since we cannot have infinite expert demonstrations, there would always be some large enough network that could perfectly distinguish the demonstrations (assign reward to 1) and the generated policies (assign reward to 0). Therefore, it would seem to me that from this perspective SQIL is an instance of GAIL where the discriminator is powerful and expert demos are finite (and disjoint from generated trajectories, which is almost always the case for continuous control). In the finite capacity case, I am unsure whether the V and Q networks in SQIL does a similar job as the discriminator in GAIL / AIRL type algorithms, since both seem to extrapolate between demonstrations and generations?\n\n2. Moreover, I don't think SQIL would always recover the expert policy even with infinite demonstrations. For example, lets think about the Reacher environment, where the agent controls a robotic arm to reach a target location. The expert demonstration is the fastest way of reaching the target (move minimum distance between joints). If we consider the MDP to have possibly very large / infinite horizon (only stops when it reaches the target), I could construct a hack policy that produces larger episodic reward compared to the expert. The policy would simply move back and forth between two expert demonstrated states, where it would receive 1 reward in the states for odd time and 0 reward for the states for even time. The reward would be something like 1 / (1 - \\gamma^2) compared to the experts' reward which is \\sum_{i=0..T} \\gamma^{i} = (1 - \\gamma^{T+1}) / (1 - \\gamma). \n\nSome fix would be to set the reward for generated policies to be negative, or introduce some absorbing state where the expert will still receive the positive reward even after reaching the target (but that is not included in demonstrations). Nevertheless, a suitable reward prior seems to be crucial to the success of this SQIL, as with GAIL requiring reward augmentation.\n\n3. Despite the above issues, I think this could be a very practical method due to its (perhaps surprising) simplicity compared to GAIL. However, the experiments only considered two environments that are not typically considered by GAIL; I believe SQIL would make a bigger impact if it is compared with GAIL in Mujoco environments -- seems not very implementation heavy because your code is based on OpenAI baselines anyway. Mujoco with image inputs would also be relevant (see ACKTR paper).\n\nMinor points:\n- What is the underlying RL algorithm for GAIL? It would seem weird if you use Q-learning for SQIL and TRPO for GAIL, which makes it impossible to identify whether Q-learning or SQIL contributed more to the performance. While GAIL used TRPO in the original paper, it would be relatively straightforward to come up with some version of GAIL that uses Q-learning. \n- Some more details in background for MaxEnt RL to make the paper more self contained.\n- More details about the hyperparameters of SQIL in experiments -- e.g. what is \\lambda?\n- Did you pretrain the SQIL / GAIL policies? Either case, it would be important to mention that and be fair in terms of the comparison.\n- Why does SQIL-11 perform worse than SQIL even though it is a tighter bound?\n- wrt. math -- I think the anonymous comment addressed some of my concerns, but I have not read the updated version so cannot be sure.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}