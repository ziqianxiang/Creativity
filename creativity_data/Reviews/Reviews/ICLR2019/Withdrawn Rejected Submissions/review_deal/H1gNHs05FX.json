{
    "Decision": {
        "metareview": "There was discussion of this paper, and the accept reviewer was not willing to argue for acceptance of this paper, while the reject reviewers, specifically pointing to the clarity of the work, argued for rejection. There appear to be many good ideas related to wavelets, and hopefully the authors can work on polishing the paper and resubmitting.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Meta-Review for Clinical Risk paper"
    },
    "Reviews": [
        {
            "title": "It introduces wavelet reconstruction to the temporal point process literature",
            "review": "The authors of this paper propose a point process model that uses wavelet reconstruction to capture complicated dependencies between events. They motivate this approach using experiments in the medical domain, which show how certain dependencies between events is better captured by their proposed model. The primary contribution of the paper is novel and could be useful in medical settings where predicting occurrence of important events such heart attacks could be challenging with alternative methods.\n\nA few recommendation for improving the paper besides a few typos that are present in the manuscript (such as first sentence of section 3 where  \"depicted\" is redundant): 1) A more substantive discussion of the challenges one may encounter while training the proposed model and elaborate on the complexity of the inference procedure in comparison with alternatives. For example, training vanilla Hawkes model is relatively easy and efficient, even though it may not perform as good as the proposed model. 2) Presentation of the model in section 3 lacks sufficient explanation and heavily relies on high level remarks on how the model is developed, a more detailed explanation (even including how the wavelet coefficients are computed) could be far more useful than the schematic view of the network architecture presented in Figure 1.\n\nOverall, I think this paper provides a novel contribution for modeling event data specifically for medical data. The ideas are well presented and experiments provide insights in how the proposed model can be useful for forecasting particular medical events. The overall recommendation is to accept the paper, however, I hope authors would address the concerns provided in the previous paragraph.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-motivated and innovative approach to construct intensity functions using wavelets. But the overall quality is not good enough due to large amount of unclear important content (in both method and experiments).",
            "review": "\n[PROS]\n\n[originality]\n\nThe paper proposed to construct the intensity function of a point process using wavelet, in order to improve its expressiveness, e.g. allowing non-additivity.  \n\nThe authors did extensive experiments to investigate their model performance compared to many appropriate baselines, on both synthetic and real-world datasets. \n\n[CONS]\n\n[clarity]\n\nThe major drawback of this submission is its clarity. The paper is vague at various important points in both method and experiments, thus leaving their correctness and soundness undetermined. \n\nIn the method section, the authors did not specify a well-defined and self-consistent notation system. This makes the paper really hard to understand. For example, one may be easily confused with things like: \n1) How q, q(g(t), t_i), g(t), g_{es}, g_{d} are connected and distinguished? \n2) The function g_{es} maps from t to R, then is t a space or a variable? \n3) The state s seems crucial in the function g_{es}, but why it is only mentioned one time in the paper? How it is defined and how it is used?\n4) How the Hadamard product is applied to two matrices of different sizes? \n5) j=1 is time dimension and j=2 is time and value dimension, then why j=1 is not part of j=2? Time is needed in both cases and it seems natural that the associated parameters are shared. \n6) Figure-3 has t_i in figure, t’ in caption, but the text in main paper mentions t_0 for Figure-3. How are they related? Are they actually the same?\n\nThe most confusing part is the censoring distance c. Its introduction around eqn-(2) suggests that c > 0 and the censoring section clearly mentions that. But c is also used to denote the forecast distance, which is clearly < 0 according to Figure-6 and Figure-7. What’s worse, there is also a term called forecast censoring distance. What are the relationships among these terms? If they are all the same, then is the c actually a model parameter or an evaluation control knob? Such things are very important to clarify. \n\nMoreover, the paper did not clearly explain how the model is trained in each case, especially for (multi-)forecasting. In details:\n1) What is the training objective?.\n2) What is the optimization method for this objective?\n3) How is it implemented and would the code be released?\n\nIt is good that the experimental section lists many appropriate baseline models and multiple evaluation metrics, but it is not clear how they are used. For example:\n1) Fourier methods and Hawkes process do not deal with the value v, then how are they fairly compared to the proposed model which takes v into account as in eqn-(2)? \n2) How is the Goodman-Kruskal gamma exactly computed? On all the instances of the held-out set? What is exactly the rank in this case?\n3) The authors also leave out the positiveness constraints of a Hawkes process to incorporate inhibitory interactions, but how the positivity of the intensity function is ensured in this case? \n\n[quality and significance]\n\nThe method is well-motivated and innovative. But details of the model and experiments are very unclear, so its overall soundness is hard to judge. For example: \n1) The authors claim that, compared to neural models, their model has the advantage of the interpretability (for small datasets), but they also have neural components in their model. So why their model is more interpretable than others (e.g. Mei and Eisner 2017 as they cited) is not clear to me. \n2) It is not clear why the interpretability is associated with the size of the dataset (quote `remains interpretable for small data sets’). What’s worse, the interpretability seems the only advantage of the model over other neural models (please correct me if I am wrong). If this edge could not scale up to large datasets, then does it mean on large datasets, a neural model should always be preferred over this model, because they are supposed to be more expressive? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning shallow Hawkes kernels using wavelets",
            "review": "This paper centers around efficient estimation of the kernel function for the Hawkes process and relaxation of the “linearity” assumption in the original Hawkes process. They rely on a classical sparse generalized linear model using the wavelet basis set and Hawkes loss function to estimate a shallow kernel function. This approach is opposite to the deep function estimation approach which does not rely on a predefined basis set [e.g. see [Du et al, 2016]]. However, it can have an advantage that the learned functions are interpretable, thought the authors never demonstrate it in the paper.\n\nGiven this view of WRNs (an unfortunate coincidence with WideResNets), we understand how LSTM2 outperforms LSTM1 in the results. However, the results tables do have peculiar numbers too. For example, why the Goodman-Kruskal gammas for H. Poisson are exactly -1? Why is it always pointing in the wrong direction? There are other observations in the results table that the authors have listed without much explanation. For example, in Section 5, what is the reason for “The WRN-PPL method excelled particularly in tasks with many target occurrences”?\n\nAnother example is the arguments in the discussion section about the use-case of rate functions. For example, the authors state: “ For example, the rate prediction for the individual denoted in green in Figure 5 (right) suggests that individual may have skipped, missed, or rescheduled 5 to 6 appointments over the last decade.” How did the authors conclude this claim? What is the clinical significance of missing or rescheduling 5-6 appointments in the context of A1c prediction?\n\nWriting can be seriously improved (basically the paper is not ready in the current state). For example, only in Section 6, the authors have introduced the full name of WRN-PPL after using it many times before. \n\nThe motivation for this paper is misleading. There have been several works on “Deep Cox” and “Deep Hawkes” models. I don’t see the novelty in the authors’ contribution in defining the clinical risk. Especially Fig. 3 (left) is already known and does not add much value.\n\nOverall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models. Unfortunately, the authors do not clearly state how many training data points they have. They have a vague statement: “798,818 timestamped events in a study population of 4,732 individuals”, but it does not say exactly how many training examples they have.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}