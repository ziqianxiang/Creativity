{
    "Decision": {
        "metareview": "The manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation. Overall, reviewers and AC agree that the general problem statement is timely and interesting, and the subject is of interest to the ICLR community\n\nThe reviewers and ACs note weakness in the evaluation of the proposed method. In particular, reviewers note that the Parzen-based log-likelihood estimate is known to be unreliable in high-dimensions. This makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated. Reviewers also expressed concerns about the strengths of the baselines compared. Additional concerns are raised with regards to scalability which the authors address in the rebuttal. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Novel and interesting idea, but significant algorithmic and empirical concerns",
            "review": "Two high-level points about my review before going into the details:\n1. This paper was a thoroughly enjoyable and insightful read. Kudos to the authors for attempting such a comprehensive overview of likelihood-based vs. likelihood-free learning.\n2. I’ll be more than happy to revise my current rating if my concerns are addressed by the authors.\n\nWith regards to the technical assessment of this work, the idea of using a nearest neighbors objective for learning a generative model is both intriguing and appealing. What makes this work even more interesting are its connections with maximum likelihood estimation. Novelty aside, I believe there are major theoretical, algorithmic, and empirical concerns in the current work which I discuss below:\n\nTheorem 1 \n- The third condition is true for location-scale family of distributions e.g., Gaussian. But the distribution learned by a generative model p_theta is far from Gaussian or other location-scale distributions. \n- More importantly, I don’t think the upper bound is tight in practice because the likelihoods can vary significantly across the dataset. Take MNIST for example. Compare the log-likelihoods of an autoregressive model or ELBOs of a VAE across the different classes of digits. Straight digits (like 1s) have much higher log-likelihoods on average than curved digits. \n\nAlgorithm\n- While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it’s hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation.\n- Similarly, I was a bit disappointed by the choice of Euclidean distance in a pixel space as the choice of distance metric. The argument that you do not want to use “auxiliary sources of labelled data or leverage domain-specific prior knowledge” is indeed necessary for fair comparisons, but also points to a limitation of the current approach.\n\nEmpirical evaluation \n- Seems too outdated both in terms of baselines and metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches.\n- While it is arguably well-established that Parzen window estimates are misleading (Theis et al.), that’s the only quantitative estimate in this work (Table 1). Hard to think of any recent published work (last 1-2 years) in generative modeling that even reports these estimates.\n- The baselines in Table 1 are all from 2013-15. Clearly, much has happened in the last 3 years that merit the inclusion of more recent baselines.\n-  Even for sample quality, there has been a lot of research in designing and improving metrics. E.g., Inception scores, Frechet Inception Distance, Kernel Inception Distance. I am not looking for state-of-the-art numbers, showing heavily zoomed out samples without any of these metrics is slightly disingenuous.\n- As mentioned before, reporting the computation time/per iteration and number of iterations for convergence for the proposed algorithm in comparison with other approaches  is important.\n- Similarly, the argument about the method avoiding even the other GAN problems (e.g., vanishing gradients, stability in training) can and should be supported by empirical evidence.\n\nAnalysis and discussion\n- One family of generative models that is crucially missing from this work is normalizing flow models. \n- This is somewhat debatable, but I do not agree that the tradeoff between likelihoods and sample quality is due to model capacity. As far as I can tell, the cited work of Grover et al., 2017 provides evidence contrary to what the authors claim. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. So model capacity isn't necessarily the key differentiating factor (which is same for both training algorithms in their experiments), it's more about the choice of the objective function and the optimization procedure.\n\nMinor points for improving presentation:\n- Section 3 can be made more concise and to the point. I’d be especially interested if the precision and recall discussion in this section and elsewhere can be formalized.\n- Use numbered lists instead of bullets for assumptions in Theorem 1, so that the discussion of the assumptions right after the theorem statement are easy to follow.\n- The citation of Grover et al. seems outdated? The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models.\n- In general, avoid making somewhat hard assertions that are speculative. Some of them I’ve highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but there is room for improving the presentation and the strength of the results",
            "review": "The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. The algorithm can be used to implicitly maximize the likelihood of models for which the former quantity is not intractable but for which sampling is easy which is typically the case for implicit models.  The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. However, The paper lacks clarity and the experiments are not really convincing. Here are some remarks:\nExperiments:\n- The estimated likelihood was reported on table 1 using parson window which is known to have bad scaling behavior with the dimension of data. In the end, the table compares methods that maximize different objectives and are evaluated with an unreliable metric. Here are two possible experiments that could be more informative:\n- Consider toy examples for which the likelihood can be evaluated and the MLE obtained easily and then compare with the proposed method. This would already give a good sense of how well the algorithm behaves in simple cases.\n- Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form. This would allow comparing the proposed algorithm to direct likelihood maximization on more complicated datasets as done in [1].\nIt seems like having experiments of this nature is far more convincing than a long justification for why the results are not necessarily state-of-the-art.\n- There are way too many samples on the figures so it is very hard to perform any visual assessment.\n\nTheory:\n- More discussions of the assumptions are needed, concrete examples for which these assumptions hold or not would be very useful. \n- Lemma 2 is a direct consequence of the following result: if p is continuous at x_0 then x_0 is a Lebesgue point.\n\nGeneral remarks on the paper:\n- What complexity is the nearest neighbor algorithm? Since it is crucial for the proposed method to be scalable it is worth presenting this algorithm at a high level in the main paper.\n- The discussion in section 3 could be much more concise if concrete examples and figures were provided. Most of the facts discussed in that section are generally well understood, so conciseness is very appreciated in this case.\n- «  A secondary issue that is more easily solvable is that samples presented in papers are sometimes cherry-picked; as a result, they capture the maximum sample quality, but not necessarily the mean sample quality. » Could you please provide an example of such paper? I would be very interested in having a closer look.\n- In the last paragraph of section 5, it is said that although the samples may not be state of the art in terms of precision, other methods which achieve better precision «  may » have less recall. It would be good to have empirical evidence to back this claim.\n\n\nRevision: Although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:\n- Missing experimental evidence for the efficiency of the NN search algorithm.\n- Experiments are using Parzen window for estimating likelihood which  are known to be unreliable in high dimensions.  \n- None of the suggested experiments were considered. In my opinion these experiments could improve the quality of this work. \n- Moreover, as mentioned by reviewer 1, Grover et al., 2017 provides evidence contrary to what the authors claim but this was never addressed so far in the paper.\n- Theorem 1 makes rather strong assumptions: as pointed out by reviewer 1, assumption 3 is unlikely to hold for the distributions used in practice\n\nFor these reasons I recommend a clear reject. \n\n[1] I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of Maximum Likelihood and GAN-based training of Real NVPs.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice Theory, Questionable Practicality",
            "review": "Summary:\n\nThis paper proposes a nearest-neighbor-based algorithm for implicit maximum likelihood.  Samples are produced by the generator network and then a nearest neighbors algorithm is run to match the samples with their nearest data point.  The generator is then updated using the Euclidean distance between samples and neighbors as the optimization objective.  Six conditions are then provided, and if they are met, then the authors show that this method is performing maximum likelihood on the implied density.  Experiments report Parzen window density estimates, samples from the model, and latent-space interpolations for MNIST, Toronto Faces, and CIFAR-10.\n\nPros: \n\nThe primary contribution of this paper is an algorithm for implicit likelihood maximization with theoretic guarantees.  As far as I’m aware, this is a novel and noteworthy contribution.  Moreover, as each sample must be paired with an observation, it does seem like the algorithm would be somewhat robust to the notorious mode collapse problem.\n\nCons:\n\nMy primary critique of the paper is that there is very little experimental investigation of the crucial details of the algorithm.  Firstly, running the nearest neighbors algorithm seems like it could be a computational bottleneck.  The authors acknowledge this, but then say “this is no longer the case due to recent advances in nearest neighbor search algorithms (Li & Malik 2016; 2017)” (p 3-4).  No other justification is given, from what I can tell.  A simulation showing how the runtime scales with dimensionality or number of data points would be very useful for knowing the scalability and practicality of the algorithm.  In the same vein, showing that the algorithm works well even with a relaxation such as approximate neighbors or random projections would make the algorithm more attractive to adopt.  \n\nMoreover, I found it frustrating that the paper teases a fix to several well-known GAN issues: “The proposed method could sidestep the three issues mentioned above: mode collapse, vanishing gradients and training instability” (p 3).  But the paper never experimentally investigates if the proposed approach indeed is better in these aspects.  I was disappointed since, intuitively, the algorithm does seem like it could be robust to mode collapse.  In addition to this lack of experimental focus, the only quantitative result is the Parzen window estimates in Table 1.  The proposed method does best the others but the other reported results are quite old---all from 2015 or earlier. \n\nMinor points: \n\nThe paper is at 10 pages, and while it is well-written, the writing is verbose and could be use tightening.  \n\n\nEvaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis.  Moreover, the theory seems not too divorced from practice (but I didn't check every detail).  However, the evaluation of this method is where the paper falters.  A big issue (that the authors note themselves) is the practicality of performing repeated nearest neighbor iterations.  No runtimes are report, nor are any approximations considered.  Rather, samples and interpolations are given the most discussion.  Furthermore, there is no demonstrations of training stability or quantitative analysis of mode collapse.  Due to these experimental deficiencies, I recommend rejection, weakly.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}