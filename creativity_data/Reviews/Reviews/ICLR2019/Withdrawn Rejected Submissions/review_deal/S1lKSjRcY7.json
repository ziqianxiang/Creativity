{
    "Decision": {
        "title": "Sensible ideas scattered throughout, but does not engage with similar earlier work.",
        "metareview": "Strengths: This paper provides a useful review of some of the recent work on gradient estimators for discrete variables, and proposes both a computationally more efficient variant of one, and a new estimator based on piecewise linear functions.\n\nWeaknesses:  Many new ideas are scattered throughout the paper.  The notation is a bit dense.  Comparisons to RELAX, which had better results than REBAR, are missing.  Finally, it seems that REBAR was trained with a fixed temperature, instead of optimizing it during training, which is one of the main benefits of the method.\n\nPoints of contention: Only R1 mentioned the omission of REBAR and RELAX.  A discussion and a few comparisons to REBAR were added to the paper, but only in a few experiments.\n\nConsensus:  This paper is borderline.  I agree with R1: quality 6, clarity 8, originality 6, significance 4.  All reviewers agreed that this was a decent paper but I think that R2 and R3 were relatively unfamiliar with the existing literature.\n\nUpdate for clarification:\n=====================\n\nThis section has been added to clarify the reasons for rejection.  The abstract of the paper states:\n\n\"We show that the commonly used Gumbel-Softmax estimator is biased and propose a simple method to reduce it. We also derive a simpler piece-wise linear continuous relaxation that also possesses reduced bias. We demonstrate empirically that reduced bias leads to a better\nperformance in variational inference and on binary optimization tasks.\"\n\nThe fact that Gumbel-Softmax is biased is well-known.  Reducing its bias was the motivation for developing the _exactly_ unbiased REBAR method, which already has similar asymptotic complexity.  A major side-benefit of using an exactly unbiased estimator is that the estimator's hyperparameters can be automatically tuned to reduce variance, as in REBAR and RELAX.\n\nThis paper focuses on methods for reducing bias and variance, but hardly discusses related methods that already achieved its stated aims. This a major weakness of the paper.  The experiments only compared with REBAR, and did not even tune the temperature to reduce variance (removing one of its major advantages).\n\nThis reject decision is not made mainly on lack of experiments or state-of-the-art results.  It's because the idea of reducing the bias of continuous-relaxation-based gradient estimators has already been fruitfully explored, and zero-bias CR estimators have been developed, but this work mostly ignores them.  However, thorough experiments are always going to be necessary for a paper proposing biased estimators, because there are already many such estimators, and little theory to say which ones will work well in which situations.\n\nSuggestions to improve the paper:  Run experiments on all methods that directly measure bias and variance.  Incorporate discussion of REBAR throughout, not just in an appendix.  Run comparisons against REBAR and RELAX without crippling their ability to reduce variance.   Do more to characterize when different estimators will be expected to be effective.",
        "recommendation": "Reject",
        "confidence": "3: The area chair is somewhat confident"
    },
    "Reviews": [
        {
            "title": "Well written; contributions intuitively explained and motivated",
            "review": "After revision:\nThe authors have addressed all points in my review. Although I will not be increasing the score, these fixes certainly increase the confidence of my evaluation and I think it deserves to be accepted.\n\n====================\n\nSummary: This paper analyzes finite-difference and continuous relaxation gradient estimators for discrete random variables and from their analysis develop improvements to these existing methods. They empirically demonstrate the improvement by evaluating the gradient estimators on toy tasks and an autoencoding task.\n\nWriting: I found this paper very well written and explained. It covered an extensive background concisely while introducing all necessary ideas to understand the contributions of the paper.\n\nComments: Overall, I found the ideas presented in this paper interesting and novel, and results sufficiently strong to support the ideas. Though the contributions are not groundbreaking, they will certainly be useful to researchers in this space. I have some minor comments relating to notation and related work.\n\n- I found the notation in Section 3.2 to be a little confusing, namely that $\\zeta$ appears as both a random variable and a continuous function (that takes in one variable in the paragraph after eq11, but takes in two variables in eq15). I understand that the authors may have done this to suppress extra notation, but I found this section harder to understand than the rest due to this choice. There is also a small typo in eq2 where the $\\phi$ from $l_\\phi$ is dropped.\n\n- I think it would be useful to compare IGSM and PWL against a score-function gradient estimator (maybe REBAR, given the similarity in experiment setup). The authors do contextualize the line of work concerning score-function gradient estimators. However, since SF estimators are unbiased but high variance and the authors aim to reduce bias at the cost of variance, I think evaluating SF baselines will better contextualize the tradeoffs made in this paper.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposed to reduce the computation of the Re-parameterization and Marginalization method and the bias of Continuous Relaxation estimator.",
            "review": "The paper proposed a modification to RAM that allows us to trade decreased computational cost for increased variance. It also proposes an improved continuous relaxation (ICR) estimator to reduce the bias of CR, which is extended to categorical variables.\nThe proposed piece-wise linear relaxation (PWL) can be considered as the inverse CDF of the random variable is very interesting. The ICR estimators can also be extended to categorical variables. \nThe paper is well written. I have some questions:\n1.\tHow does the dimension of the variables affect the bias and variance of the proposed estimator?\n2.\tDose the proposed estimators applicable to hierarchical models with multi-discrete latent variables?\n3.\t Whatâ€™s the performance of the proposed method compared with the others in terms of running time?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A paper reviewing and improving different types of gradient estimators",
            "review": "The papers studies estimators of gradients taken from expectations with respect to the distribution parameters. The paper has studied two main types of estimators, Finite Difference and Continuous Relaxation. The paper made several improvements to existing estimators. \n\nMy rating of the paper in different aspects (quality 6, clarity 8, originality 6, significance 4). \n\nPros: \n1. The paper has made a nice introduction of FD and CR estimators. The improvements over previous estimators are concrete -- it is generally clear to see the benefit of these improvements. \n\n2. The first method reduces the running time of the RAM estimator. The second method (IGM) reduces the bias of GM estimator. The first improvement avoids many function evaluations when the probability is extreme. The second improvement helps to correct bias introduced by continuous approximation of \\zeta_i itself. \n\nCons: \n1. the paper content is a little disjointed: the improvement over RAM has not much relation with later improvements. It seems the paper is stacking different things into the paper. \n\n2. All these improvements are not very significant considering a few previous papers on this topic. Some arguments are not rigorous. (see details below)\n\n3. A few important papers are not well discussed and omitted from the experiment section. \n\nDetailed comments\n\n1. The REBAR estimator [Tucker et al., 2017] and the LAX estimator [Grathwohl et al., 2018] use continuous approximation and correct it to be unbiased. These papers in this thread are not well discussed in the paper. They are not compared in the experiment either.  \n\n2. In the equation 7 and above: what does 4 mean? When beta \\neq 4, do you still get unbiased estimation? My understanding is that the estimator is unbiased only when beta=4. (correct me if I'm wrong)\n\n3. The paper argues that the variance of the estimator is mostly decided by the variance of q(zeta)^-1 when the function is smooth. I feel this argument is not very clear. First, what do you mean by saying the function is smooth? The derivative is near a constant in [0, 1]? \n\n4. In the PWL development, the paper argues that we can choose alpha_i \\approx 1/(q_i(1-q_i)) to minimize the variance. However, my understanding is, the smaller alpha_i, the smaller variance.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}