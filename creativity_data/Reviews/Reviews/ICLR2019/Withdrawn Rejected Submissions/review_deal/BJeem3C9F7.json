{
    "Decision": {
        "metareview": "This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.\n\nAnonReviewer3, who initially stated \"My second concern is the results are all on synthetic data, and most shapes are very simple\", remains concerned after the rebuttal, stating \"all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images.\"\n\nThe AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that \"While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’.\"\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Interesting inverse graphics model. Motivation and experiments are lacking.",
            "review": "This paper explored explaining scenes with surfels in a neural recognition model. The authors demonstrated results on image reconstruction, synthesis, and mental shape rotation. \n\nThe paper has many strengths. The model is clearly presented, the implementation is neat, the results on synthetic images are good. In particular, the results on the mental rotation task are interesting and new; I feel we should include more studies like these for scene and object representation learning. \n\nA few concerns remain. First, the motivation of the paper is unclear. The main advantage of the proposed representation, according to the intro, is its `implicitness’, which enables viewpoint extrapolation. I’d like to see more explanation on why ‘explicit’ representations don’t support that. A lot of the intro is currently talking about related work, which can be moved to later sections or to the supp material.\n\nThe paper then moves on to discuss surfels. While it’s new combine surfels with deep nets, I’m not sure how much benefits it brings over voxels, point clouds, or primitives. It’d be good to compare with these scene representations. \n\nMy second concern is the results are all on synthetic data, and most shapes are very simple. While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’. I’d like to see results on more realistic scenes, where the number of objects as well as their shape and material varies.\n\nFor the mental rotation task, the authors should cite and discuss the classic work from Shepard and Metzler and include human performance for calibration.\n\nI’m on the border for this paper. Happy to adjust my rating based on the discussion and revision.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice model but some details missing",
            "review": "This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an \"encoder\" network maps the image to some latent code vector, (2) a \"decoder\" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. \n\nThis is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive.\n\nThe paper seems to claim more ground than it actually covers. The abstract says \"Our method learns the depth and orientation of scene points visible in images\", but really only the depth is learned, and the \"orientation\" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The \"surfel\" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. \n\nTo rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate.\n\nThe paper is also missing some details of the method and evaluation, which I hope can be cleared up easily.\n- What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. \n- How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? \n- What is the exact form of the reconstruction error? An equation would be great.\n- How is the class-conditioning done in 4.2?\n- In Eq. 4, the first usage of D_\\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript.\n- I do not understand the \"interleaved\" training setup in 4.4.1. Please explain that more. \n- It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with \"0 sampled labels\"?\n\nOverall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "learning 3D or depth images from 2D images",
            "review": "The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. \nThe flow makes the paper readable.\n\nOne main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do.\n\nIt would be useful to provide references of equation 1 if used from previous text.\n\nIn the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline.\n\nIn 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). \n\nMissing reference: http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}