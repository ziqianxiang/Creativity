{
    "Decision": "",
    "Reviews": [
        {
            "title": "Lack of comparisons",
            "review": "This paper applies variational dropout to reduce the communication cost of distributed training of neural networks. The authors do experiments on mnist, cifar10 and svhn datasets. The technique is simple and easy to understand.\n\nHowever, I think this paper has some problems.\n1. Novelty\nApplying variational dropout for model compression is not a new idea. As this paper only combines variational dropout and distributed training, I think the novelty is a little thin.\n\nReference:\nLouizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n\n2. Comparisons\nNeed to compare with other methods that reduce the amount of communication by sparsifying gradients. \n\n3. Larger dataset & networks\nCifar10 and SVHN is not interesting for distributed training. The data / model size differs significantly with the data / models that really needs distributed training, e.g., ImageNet. The authors should at least do ImageNet and a few state-of-the-art models to make the paper convincing. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simplistic approach for improving cost of federated learning with unimpresive performance",
            "review": "In federated learning the data are kept privately on each device, gradients from each device are sent to the server, and the server sends back the averaged gradients to each device to update model weights. The authors propose an algorithm that reduces communication costs by sending sparse gradients from device to server and back. The sparsification relies on the variational dropout and sets partial derivatives to zero if the weights are sufficiently uncertain, where the uncertainty threshold is a hyperparameter. As a by-product of the algorithm, the size of the model is also reduced somewhat. The proposed algorithm is evaluated on several benchmark data sets using several benchmark neural network architectures and the results indicate a reduction in communication costs are compared to a non-sparsified variant of federated learning \n\nStrengths:\n•\tThe algorithm jointly leads to compression of a neural network and reduction in communication cost of federated learning.\n•\tThe paper is well organized and clearly written.\n\nWeakness:\n•\tThe novelty is low: the paper is a straightforward application of the variational dropout paper [Molchanov et al., 2017], the federated learning setup is standard , and the architecture in Figure 1 is very similar to distributed deep learning [see Felix Sattler 2018].\n•\tThere are some questions related to threshold T: if it is set too high, the model will not be compressed and communication cost will be low, but if it is set too low, the accuracy could be significantly impacted. It seems reasonable to apply an adjustable threshold, but the paper is not discussing this issue.\n•\tSome details are missing: how to determine threshold T in algorithm 1. is the model compressed in each iteration, is the model compressed using the constant threshold T?\n•\tThe paper does not compare with other state of the art federated learning algorithms: as listed in the related work, there are papers that compress the model, and also papers that sparsify gradients. Is the proposed variational dropout better in compression models or reducing communication cost compared to them?\n•\tThe reported improvements in communication cost and reductions in model size are relatively minor\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Several technical flaws. Experiments are not representative of federated learning.",
            "review": "This paper combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.\n\nMy major concerns are related to technical issues with the algorithm and overly simplified treatment of federated learning. I think almost every section of this paper has problems of varying severeness. I will give comments on every section in the order of its appearance in the paper.\n\nIntroduction:\n\"Federated Learning uses some form of distributed stochastic gradient descent\". Distributed SGD is certainly one of the many approaches to federated learning, but it is hardly suited for the definition of federated learning.\n\n\"Each device then updates the model parameters using the averaged gradients\" - this seems somewhat unconventional to me to be presented in the introduction. More often global server performs the update and sends the parameters back to local learners. The two are equivalent if communication is performed after each iteration, however it is unclear how to implement this scheme if communication frequency needs to be restricted.\n\nWhile model size constraint and communication constraint are important in federated learning, there are multiple other challenges. In particular, data is often unbalanced and non-iid, which is fully ignored by the authors. Privacy and communication frequency are another important aspects of federated learning. Even if authors choose to only address a subset of federated learning challenges, the problem has to be fully described.\n\nPreliminaries:\n\"One popular approximation approach is variational inference, which uses a parametric distribution to approximate the true posterior distribution (Kingma & Welling, 2013).\" While work of Kingma & Welling is certainly important, I do not think it is appropriate as a stand alone citation in the context of the sentence. Variational inference has been studied for over 20 years by many researchers and this should be acknowledged with appropriate references in the general sentence chosen by the authors.\n\n\"In traditional dropout training, either Bernoulli or Gaussian noises are added to the weights\" - dropout is not an additive noise. Weights are multiplied by the corresponding noise variables. This mistake appears at least one more time in the preliminaries section.\n\nEq. 3: I can hardly find it useful to provide equation with several numerical values of unknown origin. Indeed looking at the corresponding reference I could not find an equation with same numbers. Please elaborate or give literature reference with specific equation number.\n\nAlgorithm section:\nEquation for the gradient of $\\alpha$ does not seem to make any sense. It essentially implies that after $\\alpha_{ij}>T$, corresponding $\\alpha_{ij}$ will never be updated again as its gradient is zeroed out. In other words, if the weight is ever masked by dropout, it can never appear again. This issue seems to be confirmed by the later comment \"Second, θij is suppressed by large αij and it can hardly grow back unless DKL in (1) is removed.\"\n\n\"since α is associated with θ which is synchronized every iteration during SGD, α will thus be forced to be almost the same across all the devices\" - I do not see why this is the case. Especially if the authors were to consider imbalanced non-iid partitioning, local α can easily diverge from each other. Moreover, if α differ across local learners, there is no reason for the average of sparse gradients, computed on the server, to be sparse. This seems to defeat the whole purpose of the proposed method since the communication from server to local devices will be inefficient.\n\nExperiments:\n\"five datasets that fit the federated learning setting\" - datasets considered can hardly be associated with federated learning in my opinion. They can be used to simulate federated learning scenario by partitioning with class and size imbalance. Instead \"Each dataset is randomly and evenly divided into N non-overlapping parts for N devices\", which, as I mentioned earlier, is not representative of federated learning. Moreover I would encourage to consider larger number of devices for the experiments.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}