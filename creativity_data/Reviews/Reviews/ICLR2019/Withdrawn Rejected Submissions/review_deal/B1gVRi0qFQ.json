{
    "Decision": "",
    "Reviews": [
        {
            "title": "interesting idea but ends up tackling a simple case and many elements are unclear",
            "review": "The paper aims at developing a methodology that uses the idea of MDP homomorphisms to transform a complex MDP with a continuous state space to a simpler one. \n\nThis idea is interesting but overall the paper ends up tackling a rather simple case, which is solving a deterministic MDP with one reward also called the goal. The problem was first formulated with a MDP with sparse rewards but then, without much explanation, all discussions (and all experiments) are done with \"one goal\". The paper states for instance that the \"agent can choose the best state-action block to execute based on the length of the shortest path to the goal from each node\". So there is in fact no possibility to deal with the case of multiple rewards. Indeed, looking for the shortest path to the closest reward isn't the strategy that will maximize the cumulative discounted returns in general.\n\nIn addition, I have doubts about the scalability of the algorithm. For instance, to deal with the difficult exploration part that is required for the algorithm, the paper uses the vanilla version of the deep Q-network (section 5.4). However DQN is known to have difficulties with tasks that have sparse rewards. And if the task ends up being solved quite well with DQN, it is unclear what the motivation is for the MDP homomorphisms (first sentence in the introduction is \"Abstraction is a useful tool for effective control in complex environments\").\n\nIn addition, the descriptions of the experiments and results seem to lack key elements to  understand them (see comments below).\n\nDetailed comments:\n- It is not clear to me why the word \"online\" is emphasized in the title. In the introduction, you also mention that the \"algorithm does not depend on a particular model–we replace the convolutional network with a decision tree in one experiment\". In that case, you maybe do not need \"deep learning\" in the title either.\n- (minor) Why do you state that you use a \"one hot encoding\" for a binary variable (description of the puck stacking)? You could just use one input that is either 0 or 1 (instead of 01 and 10 that is implied by the idea of one hot encoding)?\n- (minor) It is stated that the replay buffer is limited to 10000 transitions due to memory constraints. But one transition consists in a frame of (28*C)^2 where I did not find the value of C but I guess it would reasonably be less or equal to 4 so that one frame takes less than 10kB. If that is right, 10000 of them would be less than 100MB, which seems a rather low memory constraint?\n- (minor) Section 6.2: a grid of size 3x3 is mentioned but the examples in Fig 1(d) seems to be on a 4x4?\n- Figure 1 (a) and (b): why isn't there any information relative to variance between the results over different random seeds?\n- Figure 2: I'm puzzled by the shape of the cumulative reward curve obtained through the epochs. Why is it that smooth, while there are instabilities usually. In addition, why is there a significant different regime at exactly 1000 and 1500 epochs, respectively for figure (a) and (b)?\n- The discussion on transfer learning/exploration (section 5.5) is not clear to me. In addition discussing settings such as the following do not seem of broad interest/novelty \"If we have the prior knowledge that the goal from the previous task is on the path to the goal, (...)\".",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "promising, but currently limited result",
            "review": "Summary:\nThe paper describes an algorithm for training a classifier to approximate homomorphisms from online trajectory data.  The mappings are then used to solve the resulting abstract deterministic shortest path problem.  A method is also proposed for encapsulating these abstract solutions into options and re-using them in the transfer setting.  Experiments in a puck stacking task and blocks world demonstrate the effectiveness of the new method.\n\nReview:\n\nThe empirical results of the paper are promising but I was discouraged by the restricted setting considered by the authors and the complete lack of theoretical guarantees, even with these restrictions.   \n\nThe introduction of the paper discusses sequential decision making in the realm of MDPs, but the problems tackled in the paper are far from general MDPs.  The environments are restricted to be deterministic and have one or more goal states, with all other states having equal reward.  This is much more like deterministic path planning, and that is ok to do theoretical work in, but no theory results are given in this paper.  In particular, no guarantees are given about the convergence of the algorithm – in fact the authors point out many cases where the algorithm over or under fits he classifier and fails at the underlying task.  A true analysis of the theoretical properties of this approach with a simple classifier in discrete state/action space seems needed, OR experiments showing it can actually succeed in non-deterministic domains.  Without one of these, the result seems incomplete.\n\nThe algorithm itself is also not very clearly explained.  In particular, the classifier seems to be trained to predict what state/action blocks will be encountered from the current state and a selected action?  But then there is wording saying that the classifier is “predicting all state/action blocks can be executed from a given state”.  Does this mean you are “predicting *the outcome* of all state action/blocks that can be executed…”.  Predicting the outcome makes sense but I do not understand why you need to predict what blocks can be executed.  Overall, the description on page 3 and the difficult to define set notation in the pseudocode on page 4 are not clear enough to make the result reproducible.  There should be a more straightforward explanation or a 2-3 state example one can follow here.\n\nI also do not understand why the transfer result was not directly compared to Soni & Singh’s work, referenced in the paper.  That work also builds a homomorphism and uses it in transfer learning yet not even a qualitative comparison of the approaches is really done.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}