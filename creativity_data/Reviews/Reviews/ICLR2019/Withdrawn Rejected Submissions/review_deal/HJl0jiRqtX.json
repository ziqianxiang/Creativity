{
    "Decision": {
        "metareview": "This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design. The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application. The topic is relatively under-explored in deep learning  and the paper appears to attempt to set a valuable baseline. However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity. As such the paper would benefit from a revision and a stronger resubmission.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "valuable baselines though lots of room for improvement"
    },
    "Reviews": [
        {
            "title": "Interesting but difficult to read",
            "review": "----I acknowledge that the authors have made improvements to the paper and have increased my score to 6\n\nThis is still definitely not my area of expertise and so I am leaving my confidence score low. \n---\n\nThe paper presents an algorithm EDDI that uses a a partial VAE and does active feature selection. The authors show quite a bit of experiments that seem to indicate the approach gives positive results.  However, since this is not my main area of expertise I do not know if these tasks are standard evaluation for this task.\n\nFor instance in Section 4.3, 4.4 why don't the authors plot accuracy as a function of steps/number of variables observed. That would seem much more useful than log likelihood.\n\nIn general, I found the methodology in the paper to be difficult to understand and not enough background was given.\nI think the paper would be clearer if it was more self contained.\n\n-For instance, I found much of Section 3 to not have enough background. The authors use lots of terminology around VAEs but don't give enough rigorous background so the paper doesn't feel self contained. \n\n-The same is true regarding \"amortized inference\" which I also feel isn't rigorously defined anywhere but often discussed. \n\n-The task for Section 4.1 (image inpainting) is not quite defined.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "EDDI: EFFICIENT DYNAMIC DISCOVERY OF HIGH-VALUE INFORMATION WITH PARTIAL VAE",
            "review": "The authors present an information discovery approach based on (partial) variational autoencoders and an information theoretic acquisition function that seeks to maximize the expected information gain over a set of unobserved variables. Results are presented on image inpainting, UCI datasets and health data, namely ICU and NHANES.\n\nIt is not clear why multiple recurrent steps improve perfromance. This is not conceptually justified and empirically (see Figure 8), it is also unclear whether PNP5 significantly outperforms PNP1. Further, results seem to support that PNP is always better than PN, so why introduce the methodology around PN or even present it at all. Note that the authors do not offer an explanation about the perfromance differences between PN and PNP.\n\nIn the inpainting regions section, the authors write about well-calibrated uncertainties without any context. What do they mean by calibration, well-calibrated and how can they support their claim about it?\n\nIn Figure 3 it is not clear that PNP+Ours outperforms PNP+SING. For Boston hosing seems to be marginally better but the error bars (which I assume are standard deviations, not stated) make difficult to ascertain whether the differences are significant. Although I understand the value of having \"personalized\" decisions, one wonders whether this personalization comes with any generalizable measurable gains given the results.\n\nThe results in Table 2 need to be clarified and further explained. 1) what are the error bars, considering multiple runs and datasets? 2) How can EDDI be so much better than SING when individual AUICs in Tables 6-11, the only significant difference (accounting for error bars) is on Boston data? 3) according to Tables 6-11, PNP is only the best in 1 of 5 datasets, so how come is the overall beast by a large margin? This being said, the results in Table 2 are at best misleading.\n\nIn Table 4, how can PNP-EDDI be so much better than PNP-SING, when in Figure 6 error bars overlap almost everywhere?\n\nI enjoyed reading the paper, the motivation is clear and the problem is important. The approach is modestly novel compared to existing approaches and in general well explained despite the fact that the need for multiple recurrent steps is not well justified and the differences between PN and PNP, advantages/disadvantages and when to use each are not described or explored in the experiments.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice application model but some unclear points",
            "review": "The paper proposes Partial VAE to handle missing data and a variable-wise active learning method. The model combines Partial VAE with the acquisition function to design an intelligent information acquisition system. The paper nicely combines the missing value problem with an active learning strategy to in an acquisition pipeline and demonstrate the effectiveness on several datasets.\n\nI have following comments/questions:\n\n1.  Does p(x_i | z) include parameters? How do these parameters be trained?\n\n2. Does sample from p(x_i | x_o) follow by sampling z from q(z|x_o) then sample x_i from p(x_i | z)? How to sample from p(x_\\phi | x_i, x_o) in Eq (7)?\n\n3. In Eq (9), it uses q(z_i|x_o), q(z_i | x_i, x_o),  q(z_i | x_i, x_o, x_\\phi) while in Eq (4) it only shows how to learn q(z|x_o). Does it need to learn multiple partial inference networks for all combination of i and \\phi ?\n\n4. The comparison with similar algorithms seems to be weak in the experiment section. RAND is random feature selection, and SING is global feature selection by using the proposed method. These comparison methods cannot provide enough information on how well the proposed methods performs. There are plenty of works in the area of “active feature acquisition” and also many works in feature selection dated back to Lasso which should be considered as comparison targets.\n\n5. In the “personalized” implementation of EDDI on each data instances, is the model trained independently for each data point or share some parameters across different data? If so, what are the shared parameters?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}