{
    "Decision": {
        "metareview": "The reviewers have agreed this work is not ready for publication at ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Interesting ideas, insufficient experimental evaluation.",
            "review": "The paper makes three rather independent contributions: a) a method for constructing adversarial examples (AE) utilizing second-order information, b) a method for certifying classifier robustness, c) a method to improve classifier robustness. I will discuss these three contributions separately.\n\na) Second order attack: Miyato et al. (2017) propose a method for constructing AE for the case where the gradient of the loss is vanishing. In this case, at given a point, the direction of steepest loss ascent can be approximated by the gradient at a randomly sampled nearby point. Miyato et al. (2017) show how this can be derived as a very crude approximation of the power method. The authors of the current paper apply this attack to the adversarial trained networks of Madry et al. (2017). They find that the *L_infinity* trained networks of that work are not as *L_2* robust as originally claimed. I find this result interesting, highlighting a failure case of first-order methods (PGD) for evaluating adversarial robustness. However, it is important to note that these were models that were *not* trained against an L2 attack and thus should not be expected to be very robust to one. Therefore, this result does not identify a failure of adversarial training as the authors seem to suggest but rather a failure of the original evaluation of Madry et al. (2017). It is also worth noting that this finding is specific to MNIST given the results currently presented. This might be explained by the fact that robust MNIST models tend to learn thresholding filters (Madry et al., 2017) which might cause gradient obfuscation.\n\nb) Adversarial robustness certification: The authors proposed a method for certifying the robustness of a model based on the Renyi divergence. The core idea is to define a stochastic classifier that randomly perturbs the input before classifying it. Given such a classifier, one can construct the probability distribution over classes. The authors prove that given the gap between the first and second most likely classes, one can construct a bound on the L2 norm of perturbations required to fool the classifier. This method is able to certify the adversarial accuracy of some classifier to relatively small epsilon values. While I think the theoretical arguments are elegant, I find the overall contribution incremental given the work of Mathias et al. (2018). Both methods seem to certify robustness of roughly the same scale. One component of the experimental evaluation missing is how does the certifiable accuracy differ between robust and non-robust models. Currently there are only results for a single model (Figure 1) and it is not clear from the text which one it is. Given that there exists a section titled \"improved certifiable robustness\" I would at least expect a result where a model with higher certifiable accuracy is constructed. \n\nc) Improved robustness via stability training: The authors propose a method to make a classifier more robust to input noise. They add a regulatization term to the training loss that penalizes a change in the probabilities predicted by the network when the input is randomly perturbed. In particular, they use the cross-entropy loss between the probability distributions predicted at the original and the perturbed point. The goal is to train a model that is more robust to random perturbation which will then hopefully translate to robustness to adversarial perturbation. This method is evaluated against the proposed attack (a) and is found to be more robust to that attack than previous adversarially trained models. Overall, I find the idea of stability training interesting. However I find the current evaluation severely lacking. First of all, these models should be evaluated against a standard PGD adversary (missing from Table 1). Even if that method is unreliable when applying random noise to the input at each step it is still an important sanity check. Additionally, in order to deal with the stochasticity of the model one should experiment with a PGD attack that estimates the gradient using multiple independent noise samples (see https://arxiv.org/abs/1802.00420). Finally, other attacks such as black-box attacks and finite-differences attacks should potentially be considered. Given how other defenses based purely on data augmentation during training or testing were bypassed it is important to apply a certain amount of care when evaluating the robustness of a model.\n\nOverall, while I think the paper contains interesting ideas, I find the current evaluation lacking. I recommend rejection for now but I would be willing to update by score based on author responses. \n\nMinor comments to the authors:\n-- Last paragraph of first page: \"Though successful in adversarial defensing, the underlying mechanism is still unclear.\", adversarial training has a fairly principled and established underlying mechanism, robust optimization. \n-- Figure 2 left: is the natural line PGD or SO?\n-- The standard deviation of the noise used is very large relative to the pixel range. You might want to comment on that in the main text.\n-- Figure 3: How was the Madry model trained? L_inf or L_2?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper consists of two parts: a 2nd-order attack method and a certification for robustness. The paper is well written and easy to follow. However, addressing of similarity and comparison with some previous methods could be improved.\n\nFirst of all, the motivation of 2nd order attack is clear and reasonable: for adversarially trained model at minimax, the gradient is close to vanishing, so 2nd order information helps a lot to find actual adversarial examples in this case. However,\n\n1. A lot of defenses have tried to modify the networks to make even computing the gradient difficult if not impossible. In this case, how effective is the 2nd order attack? I would like to see some discussion of this.\n\n2. While the starting point seems like a powerful attack method. The 2nd order information is only approximately computed via finite differences. A powerful method with weak approximation will sounds more powerful than a weak method to start with, but the actual effectiveness will need more systematic comparison. I think adding some studies of the accuracy or variances of the 2nd order information with the proposed approximation method (under natural setting and maybe also under the setting where the networks are modified to make even 1st order information hard to compute) would definitely help.\n\n3. Also after the approximation, as mentioned in the paper, the algorithm becomes equivalent to EOT attacks with Gaussian noises. The EOT attacks are also more general to allow different types of noises. While it might not make lots of sense to compare with EOT attacks in the experiments as the two algorithms seem to be exactly the same, it would help of more discussions could be devoted to justify how the proposed algorithm is novel given the previously existed EOT attack.\n\n4. In the experiments on adversarially trained models, the adversarial trained models are trained against l_inf attack, while the actual attack is l2. This seems unfair. Since the author mentioned that it is easy to extend their method to l_inf attack. It would be more justifiable if the results with matching attack types are shown instead of the current ones.\n\nThe certified robustness is an interesting take, too. However, the bounds might be too strong: as far as I understand, it does not rely much on the properties of the underlying neural networks f. So in order to be applicable to all kinds of weird non-robust neural networks uniformly, the bounds cannot be too tight. To get useful certificate level, a too heavy noise level sigma might be needed and potentially destroys the classification accuracy of the original model f. This is acknowledged in the 'gap between theory and empirical' section. And it makes the importance of such kind of bounds a bit weak.\n\n5. Also, the 'stability training' procedures derived based on this bounds is quite similar to some previous methods. For example, the objective function is very similar to 'logit pairing', which add an extra term to bound the similarity between two logits from an adversarial or noisy version. The empirical results will be much stronger if more closely related methods are included in the comparison. For example, logit pairing, as well as simple training with Gaussian perturbation on inputs.\n\nIn summary, this paper provide some interesting perspectives to adversarial attacks and certifications. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of some approximation methods adopted",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper lacks clarity and needs to better contrast their work to existing results",
            "review": "This paper makes two different contributions in the field of adversarial training and robustness.\nFirst the authors introduce a new type of attack that exploits second-order information while traditional attacks typically rely on first-order information.\nAnother contribution is a theorem that using the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels.\n\nOverall, I find that the paper lacks clarity and does not properly contrast their work to existing results. They are also some issues with the evaluation results. I provide detailed feedback below.\n\n1) Prior work\na) Connection between adversarial defense and robustness to random noise\nThis connection is established in Fawzi, A., Moosavi-Dezfooli, S. M., & Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems (pp. 1632-1640).\nb) Connection between minimal perturbation required to confuse classifier and its confidence was discussed for the binary classification in Section 4 of\nFawzi, Alhussein, Omar Fawzi, and Pascal Frossard. \"Analysis of classifiers’ robustness to adversarial perturbations.\" Machine Learning 107.3 (2018): 481-508.\nc) The idea to compute the distribution of classifier outputs when the input is convolved with Gaussian noise was already “anticipated” in Section V of the following paper which relates the minimum perturbation needed to fool a model to it’s misclassification rate under Gaussian convolved input:\nLyu, Chunchuan, Kaizhu Huang, and Hai-Ning Liang. \"A unified gradient regularization family for adversarial examples.\" Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015.\n\nThese papers should be discussed in the paper, please elaborate how you see your contribution regarding the results derived there.\n\n2) Second-order attack introduced in the paper\nI think they are a number of important details that are ignored in the presentation.\na) Regarding the assumption that the gradient vanishes in the difference of the loss, I think the authors should elaborate as to why this is a reasonable assumption to make. If we assume that the classifier has been trained to optimality then expanding the function at this (near-)optimum would perhaps indeed yield to a gradient term of small magnitude (assuming the function is smooth). However, nothing guarantees that the magnitude of the gradient term is negligible compared to the second-order information. The boundary of the classifier could very well be in a region of low-curvature.\nb) The approximation of the second-order information is rather crude. However, the update derived is very similar to PGD with additional noise. In optimization, the use of noise is known to extract curvature, see e.g. (Xu & Yang, 2017) who showed that noisy gradient updates act as a noisy Power method that extracts negative curvature direction.\nXu, Y., & Yang, T. (2017). First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. arXiv preprint arXiv:1711.01944.\n\n3) Issue of \"degenerate global minimum\": The authors argue that multistep attacks also suffer from this issue. However, the PGD attack of Madry is also initialized at a random point within the uncertainty ball around x, i.e. PGD attack first adds random noise to x before iteratively ascending the loss function. This PGD update + noise at first iteration seem rather similar to the update derived by the authors that uses random noise at every iteration. It could therefore be that the crude approximation of second-order information is not so different from previous work. This should be further investigated either theoretically or empirically.\n\n4) Lack of details regarding some important aspects in the paper\na) “Note the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedure”\nThe authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which I think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.\n\nb) “the noise is not necessarily added directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer, one can still calculate an upper bound using our analysis. We omit the details here for simplicity”\nWhat exactly changes here? How do you estimate the Lipschitz constant in practice?\n\n\n5) Main theorem needs to be contrasted to previous results\nThe main Theorem uses the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels. There are already many results in the field of robust optimization that already derive similar results, see e.g.\nNamkoong, H., & Duchi, J. C. (2017). Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980).\nGao, R., & Kleywegt, A. J. (2016). Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint arXiv:1604.02199.\nCan you elaborate on the difference between your bounds and these ones? You do mention some of them require strong assumptions such as smoothness but this actually seems like a mild assumption (although some activation functions used in neural nets are indeed not smooth).\n\n6) Adversarial Training Overfit to the Choice of norms\nThe main theorem derived in the paper uses the l_2 norm. What can be said regarding other norms?\n\n7) Experiments:\na) the authors only report accuracies for attacks whose l2-norm is smaller than a fixed constant 0.8. However, this makes the results difficult to interpret and the authors should instead state the signal to noise ratio, i.e. dividing the l2-norm of the perturbation by the l2-norm of the image. Otherwise, it is not clear how strong or weak such perturbations are. (In particular, the norm depends on the dimension of the image, so l2-norms of perturbations for MNIST and CIFAR10 are not comparable).\nb) In Section 6.2, the authors state that an l_infty trained model is vulnerable against l_2 perturbations. Why not training the model under both l_infty and l_2 perturbations?\nc) Figure 1\nBased on the results predicted in Theorem 2, it seems it would be more interesting to evaluate the largest L for which the classifier predictions are the same. Why did you report a different results?\n\n8) Other comments\nsection 2.1: “Note this distribution is different from the one generated from softmax”. Why/How is this different?\nconnection to EOT attack’: authors claim: E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d] = ∇_x E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d]. There is a typo on the RHS where ∇_x is repeated twice. This is also the common reparametrization trick so could cite \nKingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}