{
    "Decision": {
        "metareview": "The extension of convnets to non-Euclidean data is a major theme of research in computer vision and signal processing.  This paper is concerned with Graph structured datasets. The main idea seems to be interesting: to improve graph neural nets by first embedding the graph in a Euclidean space reducing it to a point cloud, and then exploiting the induced topological structure implicit in the point cloud. \n\nHowever, all reviewers found this paper hard to read and improperly motivated due to poor writing quality. The experimental results are somewhat promising but not completely convincing, and the proposed framework lacks a solid theoretical footing. Hence, the AC cannot recommend acceptance at ICLR-2019.  ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Scores low on presentation quality, motivation and experimental results"
    },
    "Reviews": [
        {
            "title": "A paper addressing an interesting problem, but lacks clarity and hard to understand, tech novelty is unknown",
            "review": "This paper proposes a deep GNN network for graph classification problems using their adaptive graph pooling layer. It turns the graph down-sampling problem into a column sampling problem. The approach is applied to several benchmark datasets and achieves good results.\n\nWeakness\n\n1.\tThis paper is poorly written and hard to follow. There are lots of typos even in the abstract. It should be at least proofread by an English-proficient person before submitted. For example, in the last paragraph before Section 3. “In Ying et al. ……. In Ying et al.”\n2.\tIn paragraph 1 of Section 3, there should be 9 pixels around the center pixel including itself in regular 3x3 convolution layers.\n3.\tThe definition of W in Eq(2) is vague. Is this W shared across all nodes? If so, what’s the difference between this and regular GNN layers except for replacing summation with a max function?\n4.\tThe network proposed in this paper is just a simple CNN. GNN can adopt such kind of architectures as well. And I didn’t get numbers of first block in Figure 1. The input d is 64?\n5.\tThe algorithm described in Algorithm 1 is hard to follow. There are some latex tools for coding the algorithms.\n6.\tThe authors claim that improvements on several datasets are strong. But I think the improvement is not that big. For some datasets, the network without pooling layers even performs better at one dataset. The authors didn’t provide enough analysis on these parts.\n\nStrength:\n1.\tThe idea used in this paper for graph nodes sampling is interesting. But it needs more experimental studies to support this idea. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lack of novelty and poorly executed experiments",
            "review": "The authors propose a method for learning representations for graphs. The main purpose is the classification of graphs.\n\nThe topic is timely and should be of interest to the ICLR community. \n\nThe proposed approach consists of four parts: \n\nInitial feature transformation\nLocal features aggregation\nGraph pooling\nFinal aggregator\n\nUnfortunately, each of the part is poorly explained and/or a method that has already been used before. For instance, the local feature aggregation is more or less identical to a GCN as introduced by Kipf and Welling. There are now numerous flavors of GCNs and the proposed aggregation function in (2) is not novel. \n\nGraph pooling is also a relatively well-established idea and has been investigated in several papers before. The authors should provide more details on their approach and compare it to existing graph pooling approaches. \n\nNeither (1) nor (4) are novel contributions. \n\nThe experiments look OK but are not ground-breaking and are not enough to make this paper more than a mere combination of existing methods. \n\nThe experiments do not provide standard deviation. Graph classification problems usually exhibit a large variance of the means. Hence, it is well possible that the difference in mean is not statistically significant. \n\nThe paper could also benefit from a clearer explanation of the method. The explanation of the core parts (e.g., the graph pooling) are difficult to understand and could be made much clearer. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Poorly motivated approach with experimental merits",
            "review": "The authors argue that graph neural networks based on the message passing frameworks are not able to infer the topological structure of graphs. Therefore, they propose to use the node embedding features from DeepWalk as (additional) input for the graph convolution. Moreover, a graph pooling operator is proposed, which clusters node pairs in a greedy fashion based on the l2-distances between feature vectors. The proposed methods are evaluated on seven common benchmark datasets and achieve better or comparable results to existing methods. Moreover, the method is evaluated using synthetic toy examples, showing that the proposed extensions help to infer topological structures.\n\nA main point of criticism is that the authors claim that graph convolution is not able to infer the topological structure of a graph when no labels are present. In fact the graph convolution operator is closely related to the Weisfeiler-Lehman heuristic for graph isomorphism testing and can distinguish most graphs in many practical application. Therefore, it is not clear why DeepWalk features would increase the expressive power of graph convolution. It should be stated clearly which structural properties can be distinguished using DeepWalk features, but no with mere graph convolution.\nThe example on page 4 provides only a weak motivation for the approach: The nodes v_1 and v_2 should be indistinguishable since they are generated using the same generator. Thus, the problem is the mean/max pooling, and not the graph convolution. When using the sum-aggregation and global add pooling, graphs with two clusters and graphs with three clusters are distinguishable again. Further insights how DeepWalk helps to learn more \"meaningful\" topological features are required to justify its use.\n\nClustering nodes that are close in feature space for pooling is a reasonable idea. However, this contradicts the intuition of clustering neighboring nodes in the graph. A short discussion of this phenomenon would strengthen the paper in my opinion.\n\nThere are several other questions that not been answered adequately in the article.\n\n* The 10-fold cross validation is usually performed using an additional validation set. What kind of stopping criteria has bee use? * It would be helpful to provide standard deviations on these small datasets (although a lot of papers sadly dismiss them).\n* I really like the use of synthetic data to show superior expressive power, but I am unsure whether this can be contributed to DeepWalk or the use of the proposed pooling operator (or both). Please divide the results for these toy experiments in \"GEO-DEEP\" and \"GEO-deep no pooling\". As far as I understand, node features in different clusters should be indistinguishable from each other (even when using DeepWalk), so I contribute this success to the proposed pooling operator.\n* A visualization of the graphs obtained by the proposed pooling operator would be helpful. How do the coarsened graphs look like? Given that any nodes can end up in the same cluster, and the neighborhood is defined to be the union of the neighboring nodes of the node pairs, I guess coarsened graphs are quite dense.\n* DiffPool (NIPS 2018, Ying et al.) learns assignment matrices based on a simple GCN model (and thus infers topological structure from message passing). How is the proposed pooling approach related to DiffPool (except that its non-differentiable)? How does it perform when using only the features generated by a GCN? How does it compare to other pooling approaches commonly used, e.g., Graclus? At the moment, it is quite hard to judge the benefits of the proposed pooling operator in comparison to others.\n\n\nIn summary, the paper presents promising experimental results, but lacks a theoretical justification or convincing intuition for the proposed approach. Therefore, at this point I cannot recommend its acceptance.\n\n\nMinor remarks:\n\n* p2: The definition of \"neighbour set\" is needless in its current form.\n* p2: The discussion of graph kernels neglects the fact that many graph kernels compute feature vectors that can be used with linear SVMs.\n\n-----------\nUpdate:\nThe comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}