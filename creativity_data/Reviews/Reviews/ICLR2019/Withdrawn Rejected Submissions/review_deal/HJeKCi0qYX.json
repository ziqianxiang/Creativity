{
    "Decision": {
        "metareview": "Significant spread of scores across the reviewers and unfortunately not much discussion despite prompts from the area chair and the authors. The most positive reviewer is the least confident one. Very close to the decision boundary but after careful consideration by the senior PCs just below the acceptance threshold. There is significant literature already on this topic. The \"thought delta\" created by this paper and the empirical results are also not sufficient for acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Overall Interesting work: clear motivation and nice performance gain",
            "review": "This paper proposes a multi-level embedding (MILE) framework, which can be applied on top of existing network embedding methods and helps them scale to large scale networks with faster speed. To get the backbone structure of graph, MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique, and GCN is used for the refinement of embeddings.\n\n[+] The paper is well-written and the idea is clearly presented.\n[+] MILE is able to reduce computational cost while achieving comparable, or sometimes even better embedding quality. \n[+] MILE is general enough to apply to different underlying embedding strategies.\n[-] Most of the baseline methods are of similar type, since LINE, DeepWalk, node2vec and NetMF can all be unified to matrix factorization framework. There have been many new network embedding methods proposed in the past two years. It would be interesting to see how much MILE can help scale these methods.\n\nOverall, though there have already been hundreds of papers on network embedding in the past 2~3 years, I think this paper can be an interesting addition to this fast-growing area. Therefore, I would recommend to accept it.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Practically useful, but experiments are not convincing",
            "review": "In this submission, the authors propose a three-stage framework for large-scale graph embedding. The proposed method first constructs a small graph by graph coarsening, then applies any existing graph embedding method, and last refines the learned embeddings. It is useful, however, the experimental results are not convincing and cannot support the authors' claims about the proposed method.\n\nFirst, in many places, the authors claim that the embedding quality of the proposed method is improved. For example, the last sentence of Section 1, and \"MILE improves quality\" paragraph on Page 7. However, the experimental results fail to support this. As the proposed method is for the large-scale graph, let's focus on the results of YouTube dataset and Yelp dataset first. For Youtube dataset ((d) of Table 2), when m is set to be 8, for all the cases, the performance drops. For Yelp dataset (Figure 3), the authors do not provide Micro-f1 for the original graph (m = 0) or m = 1, 2, so it is hard or impossible to demonstrate that the quality of the proposed method is still good. \n\nSecond, the comparison with existing methods is not sufficient. For the most important Yelp dataset (as this dataset fits the motivation scenario (large-scale graph) of this submission), the authors fail to report any comparison. Thus it might not be weak to demonstrate the benefit of the proposed method.\n\nThird, some experiment details are missing. For example, how the authors compute the running time of the proposed method? All the three stages are included? How the authors implement the existing methods? Are these implementations good enough to ensure a fair comparison? \n\n*******\nSome other questions:\na) On page 2, the authors mention that the proposed method \"can be easily extended to directed graph\". However, based on my understanding, directly graph will affect both the graph coarsening and embedding refining steps, and it seems not so easy to extend. Do the authors have the solution and experiments for directed graph? It would be interesting to see such results, which enlarges the application scope of the proposed method.\n\nb) The toy example on page 3 is very clear. However, for real-world graphs, does the proposed graph coarsening work well? For example, one property the proposed method utilizes is \"structurally equivalent\". What is the percentage of the nodes that can have such property for real-world graphs? \n\n********\nSome other comments:\nGenerally speaking, this submission studies a very practical task. Although the authors claim that the proposed method has great efficiency while the embedding quality is comparable good or even better than the existing methods, I think that there is an efficiency-quality trade-off based on the experimental results in this submission. When m increases, the graph coarsening step causes more information loss, and thus the quality may decrease. Embedding refining step can be regarded as a procedure to reduce such information loss, but may not improve the embedding quality better than the original graph. So to me, it would be more meaningful to study such efficiency-quality trade-off for large-scale graph embedding.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea and result",
            "review": "This paper proposed a multi-Level framework for learning node embeddings for large-scale graphs. The author first coarsens the graphs into different levels of subgraphs. The low-level subgraphs are obtained with the node embeddings of the higher-level graphs with a graph convolutional neural network. By iteratively applying this procedure, the node embeddings of the original graphs can be obtained. Experimental results on several networks (including one network with ~10M node) prove the effective and efficiency of the proposed method over existing state-of-the-art approaches.   \n\nStrength:\n- scaling up node embedding methods is a very important and practical problem\n- experiments show that the proposed methods seems to be very effective. \nWeakness:\n- the proposed method seems to be very heuristic\n- some claims in the papers are wrong according to existing literatures\n\nOverall, the paper is well written and easy to follow. The proposed method is simple but heuristic.  However, the performance seems to be quite effective according to the experiments. The reasons that why the method works need to be better explained, which can significantly the quality of the paper and its impact in the future.\n\nDetails:\n-- In the introduction part, \"However, such methods rarely scale to large datasets (e.g., graphs with over 1 million nodes) since they are computationally expensive and often memory intensive\". This is not TRUE! In the paper of LINE (Tang et al. 2015). It shows the LINE model can easily scale up to networks with one million nodes with a few hours. \n-- The authors use Equation (7) to learn the parameters of the graph convolutional neural network. I am really surprised that this method works. Especially the learned parameters are shared across different layers. \n-- Have you tried and compared different approaches of graph coarsening?\n-- In Figure 2. (a), according to Equation (1), in the second step, the weight of the edge between A and DE should be 2/sqrt(3)*sqrt(4)?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}