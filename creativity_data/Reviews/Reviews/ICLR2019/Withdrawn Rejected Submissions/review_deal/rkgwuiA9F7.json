{
    "Decision": {
        "metareview": "The reviewers in general like the idea of using the Cramer-Wold kernel, noting that its heavy tails and closed form solution are appealing properties that lead to increased stability and improved training. The main concern was novelty, as this paper can be seen as simply changing the kernel in WAE-MMD. One suggestion is to more heavily highlight the CW-distance, and in particular to find another useful application for it outside of WAE-MMD.\n\nThe paper emphasizes frequently that the closed-form loss function is a critical feature of this approach, however I donâ€™t see any experiments that optimize WAE-MMD under the CW-distance while sampling from the Gaussian. This is important to measure the degree to which any improvement is attributable to a closed-form solution, or to the distance measure itself.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A nice closed-form kernel for WAE-MMD, but concerns about novelty."
    },
    "Reviews": [
        {
            "title": "An interesting derivation of a new distance, but should be compared with MMD",
            "review": "This paper proposes the Cramer-Wold autoencoder. The first contribution of the paper is to propose the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem. More specifically, in order to compute the Cramer-Wold distance, we first find the one dimensional projections of the distributions over random slices, and then compute the average L2 distances of the kernel density estimates of these projections over random slices. The second contribution of the paper is to develop a generative autoencoder which uses the Cramer-Wold distance to match the latent distribution of the data to the prior distribution.\n\nWhile I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel. My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?\n\nThe paper points out that the main theoretical contribution is that in the case of the Gaussian distribution, the Cramer-Wold distance has a closed form. However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.\n\nThe paper further uses this closed form property of the Cramer-Wold distance to propose the Cramer-Wold autoencoder with Gaussian priors. My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required. Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder. I believe the main advantages of methods such as WAE is that they can impose priors that do not have exact analytic forms.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A variation on the Wasserstein Auto-Encoders proposing a specific choice of the divergence penalty.",
            "review": "The paper introduces a novel regularized auto-encoder architecture called the Cramer-Wold AutoEncoders (CWAE). It's objective (Eq. 7) consists of two terms: (i) a standard reconstruction term making sure the the encoder-decoder pair aligns nicely to accurately reconstruct all the training images and (ii) the regularizer, which roughly speaking requires the encoded training distribution to look similar to the standard normal (which is a prior used in the generative model being trained). The main novelty of the paper is in the form of this regularizer. The authors introduce what they call \"the Cramer-Wold distance\" (for definitions see Theorems 3.1 and 3.2) which is defined between two finite sets of D-dimensional points. The authors provide empirical studies showing that the proposed CWAE method achieves the same quality of samples (measured with FID scores) as the WAE-MMD model [1] previously reported in the literature, while running faster (by up to factor of 2 reduction in the training time, as the authors report). \n\nWhile on the AE model / architecture side I feel the contribution is very marginal, I still think that the improvement in the training speed is something useful. Otherwise it is a nicely written and polished piece of work. \n\nDetailed comments:\n(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to \"improve the balance between two terms\", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1]. \n(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2]. In other words: it may be the case that there is a choice of a reproducing kernel k such that Eq. 2 of this paper is an estimate of MMD_k between two distributions based on the i.i.d. samples X and Y. Note that if it is indeed the case, this corresponds to the V-statistic and thus biased: in U-statistic the diagonal terms (that is i = i' and j = j' in forst two terms of eq 2) would be omitted. If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel. \n(3) The authors make a big deal out of their proposed divergence measure not requiring samples from the prior as opposed to WAE-MMD. However, WAE-MMD does not necessarily need to sample from the prior when used with Gaussian prior and Gaussian RBF kernel, because in this case the prior-related parts of the MMD can be computed analytically.  In other words, if the computational advantage of CWAE compared to WAE-MMD comes from CWAE not sampling Pz, the computational overhead of WAE-MMD can be eliminated at least in the above-mentioned setting.\n(4) based on the name \"CW distance\" I would expect the authors to actually prove that it is indeed a distance (i.e. all the main axioms). \n(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution. \n(6) What is image(X) in Remark 4.1?\n\n[1] Tolstikhin et al., Wasserstein Auto-Encoders, 2017.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea & paper, but needs to highlight at least one practical advantage ",
            "review": "This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian). The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).  \n  \nPlugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.  Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score). Some potential options include:\n\n1) Faster training times. It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).  However, the authors only presented per-batch processing times as opposed to overall training time for these models.   \n\n2) Stabler training. Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure. The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.\n\n3) Usefulness of the CW distance outside of the autoencoder context.\nSince the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE). Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?\n\nWithout demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.\n\nOther Comments:\n\n- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians. See for example: \n\nChwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf\n\nwhich like CW-distance is also a quadratic-time closed-form distance between samples and a target density.\n\nBesides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives. \n\n- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?\n\nAfter reading the revision: I have raised my score by 1 point and recommend acceptance.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}