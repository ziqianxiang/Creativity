{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper proposes a PCA based analysis of a pre-trained network to determine the number of neurons that are relevant in a Deep network",
            "review": "\nClaims in the paper are there is no need for iterations or retraining and proposing design heuristics for optimal network design. \nThe analysis is not only about width but also depth without compromissing accuracy.\n\nFrom the paper, one of the findings is a consistent answer independently of the architecture. \n- What sounds interesting to me is finding the depth. However, does seem quite heuristic and not really relevant. \n- The idea of using PCA to analyze the relevance of each filter has been approached in several papers as postprocessing and some others  (for instance Compression-aware training of Neural nets, NIPS 2017) during training and therefore, in this case, no finetuning would be required. \n- How does this differ from those?\n\n- What is really the take home message from the analysis of the layers? In the end, after the architecture is found, there is a need to retrain (as it is still an approximation)\n\n- Results on imagenet are interesting but based on VGG networks that, from related works, are extremely overparameterized and therefore easy to prune. \n\n- As in the conclusions, what will happen with relus and batch norms? I have to say those conclusions seem more a discussion than conclusions\n\n\n- One other claim is this could be used while designing the network for new data. I do not see that point. Why? In the end, you need the network to converge and have proper accuracy, and if that is the case, worth trying to just do regularization, compression aware pruning or simple pruning.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Heuristics for choosing architectures",
            "review": "This paper proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset. The method shows modest accuracy drops and improved parameter counts in several settings.\n\nMajor comments:\n\nThe proposed method is not optimal, in the sense that it does not provide any guarantees on the loss in performance or generalization error. It is largely heuristic, particularly the notion that the depth can be determined to be the layer at which expansion of the representation stops. This itself depends on the initial architecture one uses. \n\nThere is no clear theoretical link between PCA of the filters and performance of the network. In general, important information in features could come from low variance directions. Moreover, the PCA step is meant to give the dimensionality of the transformations, but this will depend on the choice of nonlinearity. Optimizing the method’s design parameter (% variance retained) may be comparably difficult as optimizing the number of filters in the networks directly.\n\nThe experiments show that the proposed method can often reduce the number of operations and parameters, at usually slight decreases in test performance. The experiments could be greatly improved by adding comparison to related parameter reduction mechanisms. It is hard to evaluate the benefit of the proposed method without these. Additionally, the drop in performance highlights the question of what exactly is being optimized if these design principles are ‘optimal.’\n\nIt could be interesting to look at the patterns in reduction of filters in the experimental results run here. Do adjacent layers typically increase the number of filters by certain factors after pruning? \n\nThe paper is mostly clear but the high level organization could be improved.\n\nOverall the proposed method offers a simple heuristic for choosing the architecture of a deep network, but this seems unlikely to yield principled improvements at present.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting investigation based on reasonable heuristics. Benchmarking and comparison with respect to other compression methods is needed ",
            "review": "This paper presents a framework for optimising neural networks architectures through the identification of redundant filters across layers. \nThe intuition behind this work is that the redundancy in a given layer can be modelled through linear transformation of a low-dimensional set of basis functions. This motivates the use of PCA at each layer to identify a novel low-dimensional representation of the filters. \nThe PCA is actually performed on the set of concatenated outputs obtained by convolving a given input patch with each filter. The practical implementation of the scheme is based on the computation of PCA on the activations obtained from a small sample of training images. \n\nThe results provide some interesting insights. The pruning seems to be more effective for deeper layers, where the overall accuracy seems to be less sensitive to filters removal. The experimental validation shows that re-training a network by using the reduced filter set results in a minimal accuracy drop. Overall, the paper is based on a nice intuition and points to interesting research directions. However, the development looks quite heuristic and not completely intuitive. Moreover, the work completely lacks the comparison with respect to the state of the art.\n\nDetailed comments.\n\n- One may argue that the heuristics 1-4 provided in section 3.1 would be enough to effectively reduce the networks parameters. The authors should have compared the proposed framework with respect to the network reduction obtained by using these approaches (even without retraining).  \n\n- The logic from section 3.1 to 3.2 is counterintuitive. The idea is introduced by studying the spatial correlation analysis across filters. \nHowever, the  actual dimensionality reduction is performed on stacked convolution outputs, which are non-trivially related to the filter appearance. Again, the impression is that the development of the work is not clearly motivated from the methodological perspective.\n\n- The procedure based on iterative convolutions on randomised training samples is quite tedious, and again completely dependent on heuristics. Moreover, the required sample size may be prohibitive for many real world applications. \n\n- The post-hoc assessment of the redundancy may lacks of consistency, as it would be beneficial to prune filters and weights directly during training. For example, it is not clear under which sense the proposed compression is optimal. Since no relationship across layers is taken into account it is difficult to assess the impact of pruning across different layers.  \n\n- One of the major issue of this work concerns the lack of comparison with respect to the state of art. For example, one would expect a comparison with respect to some of the several proposed techniques on network compression. \n\n- Section 6. Figure 9 is mentioned, but the authors are perhaps referring to 8b and 8c?\n\n- I found the paper presentation not optimal. The focus of the first part of the paper (Section 3.1) provides some evidence about the paper’s idea. However it distracts the reader from the contribution of the work. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}