{
    "Decision": {
        "metareview": "Adversarial defense is a tricky subject, and the authors are to be commended for their novel approach to this problem. The reviewers all agree that there is promise in this approach. However, after reviewing the discussion, they have all come to the conclusion that the robustness of your generative model needs to be more thoroughly explored. Regarding gradient masking, there are other attacks like a manifold attack that use gradients that can be explored as well. Regarding SPSA, it would be helpful perhaps to also include other numerical gradient attacks to ensure that SPSA is stronger and working as intended.\n\nEssentially, the reviewers would all like to see a more streamlined version of this paper that removes any doubt about the efficacy of the generative approach. Once that is established, additional properties and features can be explored.\n\nAlso note that for the purposes of these reviews and discussion, Schott et al. was considered as concurrent work and not prior work.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Good approach to adversarial robustness, but the reviewers have doubts. A more thorough, streamlined experimental setup would help."
    },
    "Reviews": [
        {
            "title": "Interesting work but confusing presentation and some missing areas ",
            "review": "This paper explores the potential of generative models for adversarial robustness. It presents some interesting and well formulated findings but is lacking in some meaningful ways. \n\n-It does a good job of summarizing the literature though it misses out on some very recent but relevant work\n-It introduces three detection methods and extensively evaluates them  against several zero-knowledge attack types. Notably all the attack types are gradient based methods. \n-They present detailed performance metrics on the zero-knowledge attack, and introduce the perfect knowledge attack but do not present equivalently detailed results.\n-The results are  presented in a confusing manor, and the paper is perhaps done a disservice by the inclusion of a very large number of tables both in the main text and the appendix without the necessary writing required to situate the reader.  \n-The paper also briefly mentions the ongoing debate around \"off-manifold\" conjecture, only to them assume its correctness and based the entire work on the premise. \n\n###Highlights \n*well written introduction and a good overview of generative modeling \n*very good visualizations and explanation of the detection methods \n*thorough results on zero-knowledge gradient based attack detection rates  \n*interesting direction on adversarial examples  \n\n### Areas for improvement \n*more time should be spent on discussing the off manifold conjecture; the paper is based on the correctness of this conjecture though recent work has called its validity into question\n*only gradient based attacks are discussed though the paper title suggests robustness to all attacks\n*perfect-knowledge attacks should be explored in much more detail \n* the paper should streamline the results and findings, the large number of tables and results will confuse the reader and do not add to the argument\n*recent work on generative models for adversarial robustness should be discussed (https://arxiv.org/abs/1805.09190, https://arxiv.org/abs/1811.06969)  \n*the two class cifar10 results are not particularly convincing as this is a substantial simplification of the cifar10 problem \n* the results on the full cifar10 data set make use a the extracted features of a pre-trained discriminative cifar10 network, this introduces the problem at layer activation adversarial attacks, but this is not mentioned in the paper (https://arxiv.org/pdf/1511.05122.pdf)\n\nOverall i think the work shows promise but is not yet ready for acceptance \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good work with thorough experimental study",
            "review": "This work investigates an interesting direction of improving robustness of classifiers against adversarial attacks by using generative models. The authors propose the *deep Bayes classifier*, which is a deep LVM based extension of naive Bayes. Furthermore, the authors extensively explore 7 possible factorisations of the classifier. Thorough experiments are conducted to assess the capability of defending or detecting adversarial examples.  Besides, the authors incorporate discriminative features to generative classifiers and demonstrate clear robustness gain.\n\n### Highlights\n* This work proposes an attractive direction -- the use of generative model in defending or detecting adversarial attacks. I suggest this idea should be follower by more further studies.\n* The presented models are quite straight-forward but exhibit good robustness against attacks listed in the experiments.\n* Various structural possibilities of the graphical model are examined which is preferable and helps assess the effectiveness of generative classifiers.\n\n### Minors\n* Although the major point here is robustness against adversarial attacks, as mentioned by the authors, the performance on clear cases (i.e. no attacks) is unsatisfactory. Also, experiments on CIFAR are too much simplified (only 2 very unlike classes) and therefore not very convincing. \n* For the combination of generative classifier and discriminative features, I’m curious about the results on the clear CIFAR-10 multi-class problem. It should be a very positive plus if results are satisfactory.\n* The writing is sometimes hard to follow. For examples, many ad-hoc abbreviations are used across the paper causing difficulties of understanding the core idea and results.\n\n### Conclusion \nIn general, this paper brings our attention to a previously less investigated but seemingly promising research direction, i.e. robustness of generative model against adversarial attacks. The idea is insightful and proposed models are straight-forward. While only on small-scale  problems (with the presence of attacks), extensive experimental results in this paper can assist further study on this field. Thus, I recommend this paper to be accepted.   \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting direction but some important prior work is missing and evaluation does not yet convincingly support claims",
            "review": "In this work the authors propose and analyse generative models as defences against adversarial examples. In addition, three detection methods are introduced and an extension to deep features is suggested.\n\nMy main concerns are as follows (details below):\n* Important prior work is not mentioned.\n* Evaluation with direct attacks is only based on (very few) gradient-based techniques, many results are not reliable.\n* There are signs of gradient masking (the common problem of robustness evaluation, in particular of only gradient-based techniques are used).\n* The way detection rates are taken into account in the perfect knowledge scenario is confusing.\n\n### Style\nI like the idea of testing many different factorisation structures. However, that comes with the drawback that one needs to constantly check back what the abbreviations mean. Together with the three detection methods, the manuscript is quite confusing at times and should definitely be streamlined. One suggestion: remove the detection methods: I did not find any real conclusion about them but they are definitely side-tracking users away from the main results.\n\n### Prior work\nThere is at least one closely related prior work not mentioned here: the analysis by synthesis model [1]. This model uses a variational auto encoder to learn class conditional distributions and shows high robustness on MNIST. Please make clear what your contribution is over this paper (other than testing several other factorisations).\n\n### Evaluation problems\nThe robustness of models should be evaluated on different direct attacks ranging from gradient-based to score-based (e.g. NES [2]) to decision-based attacks [3]. Please take a look at [1] to see how a very extensive evaluation might look like. The results can be astonishingly different for different attacks, and so basing conclusion on only one or two attacks is dangerous (in particular if you only use gradient-based ones). One can also see that in your results, just check the variations you get between MIM and PGD. Also, rather then discussing (and showing in detail) results for individual attacks, the minimum adversarial distance for a given sample that can be found by any attack is much more comparable between models (which can also streamline the manuscript).\n\nOne can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate).\n\nMore generally, the perfect knowledge case, in which the attacker knows about the detector, should only count samples as adversarials which evade the detector and change the model decision. Thus, the detection rate should be zero. Otherwise I have no idea what trade-off between accuracy and detection rate you are actually targeting and how to compare the results.\n\nAlso, some intermediate results are conflicting with each other. E.g. in 4.1 you state “the usage of bottleneck is beneficial for better robustness”, but for L2 this is not true.\n\nAlso, I am not sure how conclusive the grey-box and black-box scenarios really are: since the substitute is basically a DFX or DFZ, it’s unsurprising that adversarials transfer best to those two models.\n\n### Minor\n * In 4.1 you say “as they fail to find near manifold adversarials”, but I don’t see how there can be L-infty adversarials on MNIST that are on-manifold (remember, MNIST pixel values are basically binary). Plus, in the zero-knowledge scenario there is nothing that enforces staying on this manifold.\n * Result presentation (Figure 3/5 & Table 1) is very different for different attack scenarios, which makes them hard to compare. Please unify.\n * Is the L2 distance you report in Table 1 the mean (or median) distance to adversarial examples. If so, GBZ (for which you state that C&W “failed on attacking” has actually a smaller mean adversarial distance than some other models (for which C&W is actually quite successful).\n * Grey-box scenario doesn’t make a lot of sense: since the substitute is basically a DFX or DFZ, it’s unsurprising that adversarials transfer best to those two models. A similar confounder makes the black-box results difficult to interpret.\n* Also, taking into account that the paper is two pages longer and thus calls for higher standards\n\nTaken together, I find the general direction of the paper very interesting and I’d definitely encourage the authors to go further. At the current stage, however, I feel that (1) contributions are not sufficiently delineated to prior work, (2) the evaluation is not convincingly supporting the claims and that  (3) the manuscript needs to be streamlined (both in terms of text and figures).\n\n[1] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (https://arxiv.org/abs/1805.09190)\n[2] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” ( [https://arxiv.org/abs/1804.08598)](https://arxiv.org/abs/1804.08598)) \n[3] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Solid and insightful experimental study",
            "review": "This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck.\n\nThese 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident).\n\nThe main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier.\n\nThis is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks.\n\n\n[1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf\n[2] https://arxiv.org/abs/1312.6114\n[3] https://arxiv.org/abs/1801.02774",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}