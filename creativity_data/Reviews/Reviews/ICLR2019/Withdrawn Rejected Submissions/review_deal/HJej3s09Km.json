{
    "Decision": {
        "metareview": "I appreciate that the authors are refuting a technical claim in Poole et al., however the paper has garnered zero enthusiasm the way it is written. I suggest to the authors that they rewrite the paper as a refutation of Poole et al., and name it as such.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Too narrow"
    },
    "Reviews": [
        {
            "title": "an abstract analysis that does not aim to derive any conclusions",
            "review": "This paper performs an analysis of the length scale of activations for deep fully-connected neural networks with respect to the activation function in neural networks. The authors show that for a very large class of activation functions, the length process converges in probability.\n\nI am listing my main concerns about this manuscript below.\n\n1. The paper is poorly motivated and does not make an attempt to relate its results to observations in practice or the design of new techniques. It is an abstract analysis of the probability distribution of the activations.\n\n2. Theorem 2, which is the main theoretical contribution of the paper, hinges on fixing the inputs of the neural network with weights sampled randomly from a Gaussian distribution. It is difficult to connect this with practice. This is not unreasonable and indeed common in mean-field analyses. However such analyses go further in their implications, e.g., https://arxiv.org/abs/1606.05340, https://arxiv.org/abs/1806.05393 etc. This is my main concern about the paper, its lack of concrete implications despite the simplifying assumptions.\n\n3. It would be very interesting if the analysis in this manuscript informs new activation functions or new initialization methods for training deep networks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In this paper, the authors studied how the activation function affects the behavior of randomized deep networks. ",
            "review": "* summary\nIn this paper, the authors studied how the activation function affects the behavior of randomized deep networks. \nWhen the activation function is permissible and the weights of DNN are generated from the Gaussian distribution,\nthe output of each layer was related to the so-called length process. When the permissibility is violated,\nthe convergence property may not hold. Some numerical experiments confirm the theoretical findings. \n\n\n* comments\nHowever, The randomized DNN is not clear whether theoretical results in this paper is related to the practical DNN. \nThe authors showed intensive proofs of theorems.\nI think that the relation between DNN in practice and the results in this paper should be pursued more. \n\n* The meaning of Theorem 10 is not clear. What does the theorem reveal about the ReLU function in the practical usage?\n\n* In this paper, a limit theorem in terms of the dimension N is considered.\n  However, the limit theorem in terms of the depth D is also important for the DNN.\n  Some comments on that would be helpful for readers. \n\n* Is there any relation between the analysis in this paper and batch normalization or weight normalization? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technically correct, not well-written",
            "review": "Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. The authors also show why the assumptions on the permissible functions can not be relaxed.\n\nQuality: the paper seems to be technically correct. However, the authors do not discuss any consequence of their result. Why was it important to prove it? What does it tell us about the networks? While the proof may be of interest to the authors of [14] to correct their (possible) mistakes, I think the paper will go under the radar for most people and thus encourage the authors to heavily revise the paper.\n\nClarity: the writing is clear in general. The proofs sometimes jump over non-trivial things and explain easy steps, but that maybe subjective. The paper spends no effort explaining the contribution and its consequences.\n\nOriginality: the proven statements are novel and extend/fix the claims of [14]\n\nSignificance: as said above, I believe that in the current form the paper will have little to no impact. The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that.\n\nMinor comments:\n* when introducing T{i,:,:} the <> notation is not clear. I could guess it from the later usage of the symbol, but these brackets can mean a lot of things, e.g. bracket mean (Section 2.1)\n* it would be beneficial to define the main objects, wide-network limits, in a more formal way (Section 2.3)\n* how the wide regime (large N) is interesting for studying deep NNs? [14] discusses that to some extent, but this should be explained here as well\n* q_0 is never defined\n* it's good practice to add numbers to all equations\n* I believe the claim in the appendix of [14] was meant to be conditionally independent (see also the reviews of [14]). It's clear that preactivations should not be independent and, while technically interesting, spending a page of theorem 10 and on plots seems unnecessary. Even in the paper's example preactivations are uncorrelated in the limit of large N.\n* I don't see the point of having experiments in this paper. The authors have already proven the fact. Also, it is not clear how to read the plots (no axes, little description) and come to the statements from page 9.\n\n********************\nAfter the authors' response:\nIf the main motivation of the paper is to fix the mistakes in [14], then the paper should clearly state so, in addition to explaining why fixing is necessary. While I believe that pointing out other paper's mistakes and correcting them is important, the current state of the paper leads me to keeping my initial score and recommending to reject the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}