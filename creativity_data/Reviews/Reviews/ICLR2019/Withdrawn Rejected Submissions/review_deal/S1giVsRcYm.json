{
    "Decision": {
        "metareview": "This paper was on the borderline. I am sympathetic to the authors' point about computational resources. It is helpful to demonstrate performance gains that offer \"jump start\" performance benefits, as the authors argue. However, the empirical results even on this part are still somewhat mixed-- for example, the proposed approach struggles on Private Eye (doing far worse than DQN) in Table 2. In addition, while it is beneficial to remove the need for training a density model, it would be good to show a place where a density model fails (perhaps because it is so hard to find a good one) compared to their proposed approach. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea, still some more to show potential"
    },
    "Reviews": [
        {
            "title": "An interesting work but with some flaws ",
            "review": "This paper proposed a new exploration strategy, based on the successor representation (SR), which can be used as a pseudo bonus in reinforcement learning. The authors also showed the connection between the state visit count and the SR, in the tabular case. Finally, the proposed algorithm had been tested on simulated examples, and several hard exploration Atari domains.\n\nIn general, there are some interesting ideas in this paper, while the empirical justification may not be strong enough. My pros and cons are summarized as follows. \nPros:\n- The idea of using SR for pseudo count in deep RL is novel.\n- Theorem 1 shows the interesting connection between state visit count and the proposed SR.\n- The experiments on Atari games show some promise for using SR (but not that much).\nCons:\n- There are a few inconsistencies regarding the use of SR. For example, the tabular case used the minus l1 norm as the reward bonus; however, the Atari case instead set the bonus to be the reciprocal of the l2 norm. \n- Other than the Montezuma's Revenge, it's difficult to draw the conclusion that using SR can generally lead to better exploration performance, based on the last two columns of Table 2.\n- The definition of loss L_{SR} is a bit unclear: Is there something similar to the Bellman equation you can say about SR? I also don't quite understand the motivation for the architecture between \\phi and \\psi in Figure 1.\n- A few small comments/questions are listed as follows.\n  1. When discussing the impact of the introduced auxiliary task, it would be more convincing to show the performance of games other than Montezuma's Revenge.\n  2. Why is it true that \"... because a reward of 1 is observed...\", in the second paragraph of Section 4?\n  3. What is the value of \\tau in the loss L_{TD} on Atari domains?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not very novel and rather confusing",
            "review": "Being familiar but not an expert in reinforcement learning, my review will focus on the overall soundness of the proposed method\n\nSummary:\n\nThe authors are interested in the problem of sample efficiency in reinforcement learning, i.e. how to learn a policy achieving good performance (discounted reward) in a RL setting using as little interaction with the environment as possible.\nTo do this the authors propose to learn a policy in a new environment where the reward has changed: an exploration bonus is added to the reward that should bias the agent towards the least frequently visited states.\n\nThe algorithms proposed throughout the manuscript are extensions of a two-part algorithm of the following flavour: 1) An estimate of visitation count is done in an online fashion using a modified version of the successor representation (SR). 2) This estimates parametrizes the exploration bonus of the environment . Both learning algorithms are optimized together.\n\nThis initial algorithm is fairly simple in its description and builds on well established ideas in RL. The authors then ‘evaluate the effectiveness of the proposed exploration bonus in a standard model-based algorithm’ against other baselines. They do explain how the model is learned, but not how the policy is optimized.\n\nThe remainder of the manuscript applies the same idea to different settings.\nFor large state spaces, The SR expected visits are learned using TD along with state action value functions. The counts of visitations are replaced by features that are also learned.\n\nOverall, the manuscript is rather confusing.\nThe SSR theorem is stated (with no real intuition and the actual bounds on n(s) left for the reader to derive). It is not well motivated. Why would we want expected counts and not the discounted version?\nThen the remainder of the paper actually makes no use of this theorem, but only use it as a distant inspiration. Tentative connections are made such as TD underestimating SR thus leading to a result more akin to SSR, which is highly speculative. It is also irrelevant since features are learned anyway.\n\nThe final proposed architecture has many additions to a simple DQN (the reconstruction + the exploration bonus + the MMC). This makes it difficult to understand what the contribution of the exploration bonus is.\nIt does not help that results are manually extracted from histograms found in  papers.\n\nOverall, although the intuition is interesting (though not so new).\nThe overall motivation and structure of this manuscript makes think it does not match the standards of ICLR for publication\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Theoretically grounded work but lacking convincing results",
            "review": "The authors are tackling sample efficiency in the reinforcement learning setting by designing a reward function that encourages exploration. To achieve this they propose you use the successor function which basically counts how often a state has been visited. At first the show this for discrete settings and extend their approach to the continuous state spaces in the Atari 2600 environments. \n\nThe paper is well written and the motivation and methods are clear from the beginning. \n\nMy biggest concerning is regarding the experimental results of this work. In Table 1 the authors show the results for the tabular games River Swim and Six Arms and copare their approach which they dub ESSR to three methods (E3, R-MAX, MBIE). The numbers in the table indicate that their method ESSR is outperforming E3 and R-MAX on both environments but is itself outperformed by MBIE. The authors don't mention this at al in the respective paragraph nor do the provide a reason as to why this could be case. Also, they neither introduce any of these methods nor do the explain the meaning of the acronyms. Only in the section 6 (of 7) they talk about related works are R-MAX and E3 introduced briefly. But yet again, MBIE is not mentioned. \n\nI have similar concerns about the results presented for the Atari benchmarks. In table 2 the authors compare their method to the classic DQN approach and two more approaches. While their approach outperforms DQN in almost all tasks, this does not hold for the remaining algorithms. Their method is being outperformed in all but one (Venture) task, where they report a higher variance and a small performance boost compared to DQN_e^MMC. Also it is not clear to me where the numbers for the DNQ_e^MMC come from. The authors just say \"[...] denotes another baseline used in the comparison\". Is this the proposed method of this work but without the successor representation?\n\nIn my opinion this work is lacking some clear and convincing results.  Is the main benefit of this method that it does not rely on domain-specific knowledge? If so, then it is not communicated clearly. The authors mention this briefly in the conclusion but provide no further analysis",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}