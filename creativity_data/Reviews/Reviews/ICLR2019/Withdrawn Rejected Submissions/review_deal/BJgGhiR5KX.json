{
    "Decision": {
        "metareview": "Pros:\n\n- A new framework for learning sentence representations\n- Solid experiments and analyses\n- En-Zh / XNLI dataset was added, addressing the comment that no distant languages were considered; also ablation tests\n\nCons:\n\n-  The considered components are not novel, and their combination is straightforward\n-  The set of downstream tasks is not very diverse (See R2)\n-  Only high resource languages are considered (interesting to see it applied to real low resource languages)\n\nAll reviewers agree that there is no modeling contribution.  Overall, it is a solid paper but I do not believe that the contribution is sufficient. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "interesting but not very novel framework"
    },
    "Reviews": [
        {
            "title": "A new framework for cross-lingual sentence representation which is an interesting mix of standard building blocks, but more convincing experiments are needed to appreciate the main contributions.",
            "review": "This paper proposes a novel cross-lingual multi-tasking framework based on a dual-encoder model that can learn cross-lingual sentence representations which are useful in monolingual tasks and cross-lingual tasks for both languages involved in the training, as observed on the experiments for three language pairs. The main idea of the approach is to model all tasks as input-response ranking tasks and introduce cross-lingual representation tying through the translation ranking task, introduced by Guo et al. (2018). All components of the framework are quite standard and deja-vu, but I like the paper in general, and the results seem quite encouraging. I have several comments on how to further strengthen the paper and improve the presentation of the main findings.\n\nThe proposed framework does not offer any substantial modeling contribution (i.e., all major components are based on SOTA models), but the framework is still quite interesting as a mixture of these SOTA components. I believe that some additional experiments would make the main contributions clearer and would also provide additional insights into the main properties of the proposed framework: 1) cross-linguality and 2) multi-tasking. \n\n*Most of all, I am surprised not to see any ablation studies. For instance, what happens if we remove one of the two monolingual tasks in each language? How does that reduced model compare to the full model? Which monolingual task is more beneficial for the final performance in downstream tasks? Can we think of adding another monolingual task to boost performance further? I think that this sort of experiment would be more beneficial for the paper than a pretty long analysis from Section 5 (this analysis is still valid, but should be shortened substantially). Evaluating only multi-tasking without any cross-lingual training would also be very beneficial to recognise the extent of improvement achieved by adding cross-linguality to the model.\n\n*How much does the proposed architecture depend on the choice of the encoding model for the function g? Have the authors experimented with other (recent and (near-)SOTA) encoding models? I would like to see a comparative analysis of this 'hyper-parameter'.\n\n*I would like to see more experiments on more distant language pairs. This would make the paper even more interesting imho. I am also curious whether there would be a drop in performance reported conditioned on the distance/proximity between two languages in a language pair.\n\n*I would like to see a more detailed description of the two best performing STS systems (ECNU and BIT). In what respect are these systems state-of-the-art feature engineered and mixed? I am not sure what this means without providing any additional context to the claim and description.\n\n*How does the monolingual English STS model trained with the cross-lingual multi-task framework compare to the work of Conneau et al. (EMNLP 2017) which also used SNLI as the task on which to learn universal sentence representations. This would be a good experiment imho as it would show how much we gain from cross-lingual training and multi-tasking.\n\nMinor:\n*Page 3: Could you add a short footnote discussing how hard-negatives for the translation ranking task are selected? How do you compute similarity here?\n*Do you expect performance to improve further by training MultiNLI instead of SNLI (or combining the two datasets)?\n*\"All hyperparameters are tuned based on preliminary experiments on a development set.\" -> What is used as the development set? More details needed.\n*\"Finally, as an additional training heuristic, we multiply the gradients to the word and character embeddings by a factor of 100.\" -> How is the value for the embedding gradient multiplier determined? Is there an automatic procedure to fine-tune this hyper-parameter or has this been done in a completely empirical way?\n*Table 1: please define the task abbreviations before showing them in the table. It is not clear what each task is by relying only on the abbreviation.\n*This dataset was not available at the time of the submission, but for the revision it would make sense to also evaluate on the new XNLI dataset of Conneau et al. (EMNLP 2018) for multilingual NLI experiments.\n\n(After the first revision) I have raised the score after the very detailed author response (thanks for that!), but this is also conditioned on the authors making the actual revisions promised in their response. I am still quite interested to check how well the method works in a setup with distant language pairs.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "It is an interesting paper which explores multi-task model for simultaneously improving both monolingual and cross-lingual tasks. However, due to missing information and lacking of clarity makes it hard to accept at this point of time.",
            "review": "Summary\n----------\nIn this paper, authors explore learning of cross-lingual sentence representations with their proposed dual-encoder model. Evaluation conducted with learned cross-lingual representations on several tasks such as monolingual, cross-lingual, and zero-shot/few-shot learning show the effectiveness of the proposed approach. Also, they show provide a graph-based analysis of the learned representations.\n\nThree positive and negative points of the paper is presented as follows:\n\npros\n------\n\n1. cross-lingual representation learning by combinining ideas from learning sentence representations and cross-language retrieval.\n2. Multi-task setup of different tasks for improving cross-language and monolingual tasks.\n3. Lot of experimental results.\n\n\ncons\n-----\n1. Claim it works for monolignual tasks in target language such as zero-shot learning for sentiment classification and NLI. Also, for cross-lingual STS and eigen-similarity metric is hard to retrieve from the paper.\n\n2. Many terms, datasets are used without being referenced.\n\n3. Usage of existing approaches to build a single model for many tasks.\n\ncomments to authors\n-----------------------\n\n\n1. Dual-encoder architecture is inspired from Guo et al (2018) which uses encoding of source and target sentence with Deep neural network. However, it is here replaced into multi-task dual-encoder model.\n\n2. What are the tasks that are very specific to source language? \n\n3. Equation-1 is basically a logistic regression or softmax over \\phi. However \\phi is dot product of encodings as similar to Deep averaging networks (Iyyer et al. 2015) ?\n\n4. In Section-2, it is unclear what does symmetric tasks mean? They use parallel corpora?\n\n5. In Section-2.1, it is mentioned that Word embeddings are learned end-to-end. Does this mean they are not initialized with pretrained ones?\n\n6. In Section-2.1, it is mentioned that word and character embeddings are learned in a computationally efficient way, what does it represent? They use less parameters, parallelizable?\n\n7. Why only three layers of transformer, It is understood that 6-12 layers is required for effective encoding of sentences (Al-Rfou et al., 2018)\n\n8. In model configuration, how is convergence decided. Any stopping criterion?\n\n9. What are the splits for reddit, wikipedia datasets?\n\n10. In Table-1, what does MR,CR etc., refer to? They are never mentioned before. Does all tasks only use only English ?\n\n\n\nOverall it is an interesting paper which explores multi-task model for simultaneously improving both monolingual and cross-lingual tasks. However, due to missing information and lacking clarity in some details it is hard to accept at this point of time.\n\nMinor issues\n--------------\n\n1. Sentences are very long and not easily comprehensible.\n2. Target language and repsonse are used without referencing each other. Better to use one of them for better tracking.\n3. No common notation for the model. It is been referenced with different names (cross-lingual multi-task model, multi-task dual-encoder model).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited novelty, strong evaluation, other languages and tasks?",
            "review": "The paper presents an intuitive architecture for learning cross-lingual sentence representations. I see weaknesses and strengths: \n\n(i) The approach is not very novel. Using parallel data and similarity training (siamese, adversarial, etc.) to facilitate transfer has been done before; see [0] and references therein. Sharing encoder parameters across very different tasks is also pretty standard by now, going back to [1] or so. \n(ii) The evaluation is strong, with a nice combination of standard benchmark evaluation, downstream evaluation, and analysis. \n(iii) While the paper is on cross-lingual transfer, the authors only experiment with a small set of high-resource languages, where transfer is relatively easy. \n(iv) I think the datasets used for evaluation are somewhat suboptimal, e.g.: \na) Cross-lingual retrieval and multi-lingual STS are very similar tasks. Other tasks using sentence representations and for which multilingual corpora are available, include discourse parsing, support identification for QA, extractive summarization, stance detection, etc. \nb) Instead of relying on Agic and Schluter (2017), why don’t the authors use the XNLI corpus [2]?\nc) Translating the English STS data using Google NMT to evaluate an architecture that looks a lot like Google NMT sounds a suspicious. \n(v) While I found the experiment with eigen-similarity a nice contribution, there is a lot of alternatives: seeing whether there is a linear transformation from one language to another (using Procrustes, for example), seeing whether the sentence graphs can be aligned using GANs based only on JSD divergence, looking at the geometry of these representations, etc. Did you think about doing the same analysis on the representations learned without the translation task, but using target language training data for the tasks instead? The question would be whether there exists a linear transformation from the sentence graph learned for English while doing NLI, to the sentence graph learned for German while doing NLI. \n\nMinor comments: \n- “Table 3” on page 5 should be Table 2. \n- Table 2 seems unnecessary. Since the results are not interesting on their own, but simply a premise in the motivating argument, I would present these results in-text. \n\n[0] http://aclweb.org/anthology/W18-3023",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}