{
    "Decision": {
        "metareview": "The reviewers found the work interesting and sensible.  The application of latent space constrained autoencoders to wireless positioning certainly seems novel.  Applications can certainly be exciting additions to the conference program.  However, the reviewers weren't convinced that the technical content of the paper was sufficiently novel to be interesting to the ICLR community.  In particular, the reviewers seem concerned that there are no comparisons to more recent methods for dimensionality reduction and learning latent embeddings, such as variational auto-encoders.  Certainly a comparison to more recent work constraining latent representations seems warranted to justify this particular approach.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting application to wireless positioning, but lacks novelty and empirical comparisons"
    },
    "Reviews": [
        {
            "title": "Applies a distance constraint to the latent space of auto-encoders",
            "review": "[I'm a fallback reviewer assigned after initial reviewer failed to submit]\n\nQuality/Clarity:\nThe work is fine. The presentation is clear enough. The experiments are all on simulated data, with 2GHz scattering simulation derived from more sophisticated software suites than the 4 toy manifold problems initially considered.\n\n\nOriginality/Significance:\nThe work does not seem particularly novel. Perhaps the specific application of regularized autoencoders to the channel charting problem is novel. The regularizers end up looking a lot like a variety of margin losses. The idea of imposing some structure on the latent space of an autoencoder is not particularly new either. Consider, for example, conditional VAEs. Or this work from last year's ICLR https://openreview.net/forum?id=Sy8XvGb0- This work is straightforward multi-task learning with dimensionality reduction with similarity loss tasks.\n\nOn the whole, I don't think there is enough novel work for the venue.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful approach, but insufficient experimental validation, and somewhat weak on novelty",
            "review": "Description:\n\nThis paper presents a variant of deep neural network autoencoders for low-dimensional embedding, where pairwise constraints are incorporated, and applies it to wireless positioning.\n\nThe four constraint types are about enforcing pairwise distance between low-dimensional points to be close to a desired value or below a maximal desired value, either as an \"absolute\" constraint where one point is fixed or a \"relative\" constraint where both points are optimized. The constraints are encoded as  nonconvex regularization terms. In addition to the constraints the method has a standard autoencoder cost function.\n\nAuthors point out that if a suitable importance weighting is done, one constraint type yields a parametric version of Sammon’s mapping.\n\nThe method is tested on four simple artificial manifolds and on a wireless positioning task.\n\n\nEvaluation:\n\nCombining autoencoders with suitable additional regularizers can be a meaningful approach. However, I find the evaluation of the proposed method very insufficient: there are no comparisons to any other dimensionality reduction methods. For example, Sammon's mapping is mentioned several times but is not compared to, and a parametric version of t-SNE is also mentioned but not compared to even though it is parametric like the authors' proposed method. I consider that to be a severe problem in a situation where numerous such methods have been proposed previously and would be applicable to the data used here.\n\nIn terms of novelty I find the method somewhat lacking: essentially it is close to simply a weighted combination of an AE cost function and a Sammon's mapping cost function when using the FRD constraints. The other types of constraints add some more novelty, however.\n\n\n\nDetailed comments:\n\n\n\"Autoencoders have been shown to consistently outperform other dimensionality-reduction algorithms on real-world datasets (van der Maaten et al., 2009)\": this is too old a reference, nine years old, and it does not contain numerous dimensionality reduction algorithms proposed more recently, such as any neighbor embedding based dimensionality reduction methods. Moreover, the test in van der Maate et al. 2009 was only on five data sets and in terms of a continuity measure only, too little evidence to claim consistent outperforming of other algorithms.\n\n\"van der Maaten (2009) proposes the use of AEs to learn a parametric mapping between high-dimensional datapoints and low-dimensional representations by enforcing structure obtained via Student-t stochastic neighborhood embedding (t-SNE)\": this is not a correct description, van der Maaten (2009) optimizes the AE using t-SNE cost function (instead of running some separate t-SNE step to yield structural constraints as the description seems to say).\n\n\"the FRD regularizer resembles that of Sammon's mapping\": actually in the general form it resembles the multidimensional scaling stress; it only becomes close to Sammon's mapping if you additionally weight each constraint by the inverse of the original distance as you suggest.\n\nIt is unclear to me where the absolute distance constraints (FAD or MAD) arise from in the synthetic experiments. You write \"for FAD one of the two representations... is a constant known prior to AE learning\": how can you know the desired low-dimensional output coordinate (or distance from such a coordinate) in the synthetic data case?\n\nThis reference is incorrect: \"Laurens van der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: A comparative review. In Journal of Machine Learning Research, volume 10, pp. 66–71, 2009.\" This article has not been published in Journal of Machine Learning Research. It is only available as a technical report of Tilburg University.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning representation-constrained autoencoders",
            "review": "The paper propose to learn autoencoders which incorporate pairwise constraints while learning the representation. Such constraints are motivated from the available side information in a given application. Inducing application-specific structure while training autoencoders allows to learn embeddings with better neighborhood preserving properties. In wireless positioning application, the paper proposes fixed absolute/relative distance and maximum absolute/relative distance constraints. Experiments on synthetic and real-word datasets show improved performance with the proposed approach.\n\nSome comments/questions:\n\n1. Table 1 shows different constraints along with the corresponding regularizers which are employed while training autoencoders. How is the regularization parameter set for (so many) regularizers?\n\n2. Employing constraints (e.g. manifolds) while learning representation has recently attracted attention (see the references below). The proposed approach may benefit from learning the constraints directly on the manifolds (than via regularizers). Some of the constraints discussed in the paper can be modeled on manifolds.\n\nArjovsky et al (2016). Unitary evolution recurrent neural networks\nHuang et al (2017). Orthogonal weight normalization: Solution to optimization over multiple dependent Stiefel manifolds in deep neural networks.\nHuang et al (2018). Building deep networks on Grassmann manifolds\nOzay and Okatani (2018). Training CNNs with normalized kernels.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}