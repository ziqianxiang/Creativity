{
    "Decision": {
        "metareview": "The paper proposes a simple method for detecting out-of-distribution samples. The authors' major finding is that mean and standard deviation within feature maps can be used as an input for classifying out-of-distribution (OOD) samples. The proposed method is simple and practical.\n\nThe reviewers and AC note the following potential weaknesses: (1) limited novelty and somewhat ad-hoc approach, i.e., it is not too surprising to expect that such statistics can be useful for the purpose. Some theoretical justification might help. (2) arguable experimental settings, i.e., the performance highly varies depending on validation (even in the revised draft), and sometimes irrationally good. It also depends on the choice of classifier.\n\nFor (2), I think the whole evaluation should be done assuming that we don't know how it looks the OOD set. Under the setting, the authors should compare the proposed method and existing ones for fair comparisons. AC understands the authors follows the same experimental settings of some previous work addressing this problem, but it's time that this is changed. Indeed, a recent paper by Lee at al. 2018 considers such a setting for detecting more general types of abnormal samples including OOD.\n\nIn overall, the proposed idea is simple and easy to use. However, AC decided that the authors need more significant works to publish the work.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting simple idea, but limited novelty and unfair experimental setups"
    },
    "Reviews": [
        {
            "title": "Appealing results but presentation lacks clarity and correctness of experiment is challenged.",
            "review": "Summary: A relatively simple approach for detecting out-of-distribution samples by having a parallel logistic regression model using simple statistics (mean and variance) over output of each batch normalisation layer, in order to discriminate between in-distribution and out-of-distribution samples. Results are appealing but presentation is lacking clarity at time and some doubts on the correctness of the experiments remain.\n\nWith the goal of detecting out-of-distribution sets, the authors propose to use logistic regression over simple statistics (mean and variance) of each batch normalization layer of CNN in order to discriminate between in-distribution (ID) and out-of-distribution (OOD) samples. They argue that ID and OOD samples can be discriminated with these statistics.\n\nQuality: The motivations of the paper are clear, it aims at having better capacity to detect OOD samples with a method that involves less computations. However, the quality of the experiments is not good enough and I have doubts on their validity.\n\nOriginality: Ok. The proposal is relatively simple and is based on the intuition that statistics for the batch normalization is useful to detect OOD samples. The problem is not new, the approach is relatively ad hoc, but it works.\n\nSignificance of the work: The results reported are unreasonably good. Although the authors claim the improvement of detection of OOD is significant, the results achieved by detecting **all** the out-distribution samples sounds weird and irrational. How rejecting all Tiny-ImageNet is possible while there are several overlap between the classes presented in TinyImageNet vs. Cifar10/Cifar100 (cf. Table 8)? To me, it looks like the model either overfitted something else than the content of the images, maybe the background noise or similar regarding the nature of the data. More experiments with different in-distribution datasets should be made to be convincing. All experiments reported are using either Cifar10 or Cifar100 as in-distribution datasets.\n\nThe author also claim using few samples from a single OOD set is enough for training the regressor that provides OOD-ness score. Is it true for any OOD set or only a carefully chosen OOD set can demonstrate this behavior? What is the criteria for selecting a good OOD set for training the regressor?\n\nDespite of the fact that the proposed method is heavily dependent on the threshold, the authors barely discuss of it. I am assuming that threshold is on OOD-ness score, is that correct? How does look like the OOD-ness score for an ID set over different OOD sets? Providing the OOD-ness score for ID and OOD could reflect how the proposed method is sensitive to a selected threshold. In other words, is selecting a fix threshold will to the TPR / FPR across different OODs.\n\nThe overall writing style is perfectible. I did not found the paper super clear in the presentation and it is difficult to really get all useful information for it. However, the authors appear knowledgeable of the literature and the overall structure is clear.\n\nAn example of lack of clarity in the explanations: in Table 7, I have difficulty to make sense of the 100% achieved for “Ours (pair)” vs “Ours”. Is the “Ours (pair)” the rate obtained with the exact pair used for adjusting the threshold, while “Ours” is on another dataset? If not, what this mean? Moreover, reporting columns all with 100% is not a good practice, it seems to be a stunt to impress the reader, while not carrying much in term of content and understanding.\n\nIn Table 8, I do not understand what the values in parenthesis means. \n\nAnother element: why for training the regressor, the IN and OOD samples are not selected from their corresponding training sets instead of splitting their test sets to a validation and test sets?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper suggests using Z-scores for comparing ID and OOD samples. Simple Idea, and much related work. Needs to address some issues. ",
            "review": "There has been recent interest in using statistics and information summary measures to evaluate what deep nets are trying to do. Following the line of work, the paper suggests to use mean and variance of Z-scores accumulated across all layers/channels as features to distinguish ID and OOD samples. Simple idea but needs some work in its current format. \n\nFirstly, the bulk of content in Sections 2 and 3 can be reduced/shortened since the importance of normalized statistics to understand learning models is well known, and not novel. \n\n1) The choice of datasets/netowrks needs to be understood here. How is the OOD summary changing as more layers are added into computing the score (since the score is basically averaging all layers'/channels contribution)? \n2)  What happens if we split the ID itself into two datasets and train on one, while using the other as OOD?\n3)  (r) is random and (c) is not is it for the TinyImages? Seems to be the other way around. \n4) What is the influence of the dataset? Since the summaries are first order statistics, there can be significant dependance of the 'coverage' of training data (i.e., how many and how good of instances are present for each class)? This is purely a sampling problem and it may reciprocate in the OOD scores (back to first order statistics). This needs to be tested. \n5) Statistical tests of significance needs to be reported for the performance summaries shown in the Tables. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A quick to implement and effective method for OOD sample detection.",
            "review": "The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method.\n\nThere are several aspects of this work that I admire.\n\n- The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed.\n- The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions.\n- The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier.\n- The source code is already included in the submission.\n\nMy only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. \n\nOverall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community.\n------\nRev. In light of the rebuttal and the following discussions I have updated my rating to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}