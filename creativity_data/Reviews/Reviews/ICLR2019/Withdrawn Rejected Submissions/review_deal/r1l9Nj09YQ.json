{
    "Decision": {
        "metareview": "This paper addresses a clear open problem in representation learning for language: the learning of language-agnostic representations for zero-shot cross-lingual transfer. All three reviewers agree that it makes some progress on that problem, and my understanding is that a straightforward presentation of these would likely have been accepted to this conference. However, there were serious issues with the framing and presentation of the paper.\n\nOne reviewer expressed serious concerns about clarity and detail, and two others expressed serious concerns about the paper's framing. I'm more worried about the framing issue: The paper opens with a sweeping discussion about the nature of language and universal grammar and, in the original version, also claims (in vague terms) to have made substantial progress on understanding the nature of language. The most problematic claims have since been removed, but the sweeping introduction remains, and it serves as the only introduction to the paper, leaving little discussion of the substantial points that the paper is trying to make.\n\nI reluctantly have to recommend rejection. These problems should be fixable with a substantial re-write of the paper, but the reviewers were not satisfied with the progress made in that direction so far.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting technical work, but serious issues with framing "
    },
    "Reviews": [
        {
            "title": "Very good work on learning language-agnostic embeddings but makes bold claims that are not verified",
            "review": "This paper starts with the bold aim of extracting Montague's universal grammar from multiple languages. In order to do so, the authors train multiple language models where each LM is explicitly factorized into language-specific and language-independent representations. The authors then apply the GAN framework to the language-independent parts to enforce all languages to share the same latent space. The claim here is that the language independent parameters capture the essence of universal grammar. The authors show that their framework enables effective zero-shot learning of tasks over new languages (for example sentiment classifier learned on top of English data generalize to Chinese when trained on the universal grammar embeddings). \n\nThe paper is overall well written and the experimental results are convincing.\n\nThe gripe, however, I have is that this paper makes the claims that go too far without evaluating them. It is entirely sufficient to claim that you're trying to learn language agnostic parameters/embeddings --  I'd be happy with that. But the paper goes further and claims to be learning a form of universal grammar. To justify this claims, it is not sufficient to show in the experiments that the new representations do better at sentiment and NLI. The authors must show that this captures the \"innate\" language learning abilities akin to human babies. While the paper aims to do some analysis in the discussion section, it is not unsatisfactory. As the paper says in the discussion section \"From a machine learning perspective, weâ€™re interested in extracting informative features and not necessarily a completely grammatical language model. That being said it is of interest to what extent language models capture grammar and furthermore the extent to which models trained toward the universal grammar objective learn grammar.\"\n\nThe problem is that simply comparing LM perplexities is not a solid test of whether this model has learned some form of universal grammar. First, this paper does not define a clear falsifiable hypothesis on the proof of learning universal grammar. One example of testing for learning grammar can be: does this model learn basic syntactic rules of a new language (e.g. as the authors suggested -- head-first or head-final syntax, or rules of conjugation)  with a small amount of data after being training a universal representation with n languages? There have been a series of recent papers on checking if Language Models have appropriately learned syntax. See e.g. Tal Linzen's work https://arxiv.org/pdf/1809.04179.pdf. Just to be clear I am not suggesting citing works in unpublished places but potentially using some of the tests suggested in these papers.\n\nIn conclusion, I think this work is useful but I also think it makes really grandiloquent claims without verifying them. That to me is a dangerous precedent.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The writing is unclear...",
            "review": "This paper proposes the idea of language agnostic representation which could potentially provide zero shot solution if the downstream task is trained using another language. The solution uses linguistic features from every sentence, trains language model for multiple languages simultaneously, and matches distribution by using Wasserstein distance measure. \n\npros:\nThe motivation of this paper is clear. \nThe method proposed looks reasonable. \nThe experimental results also make sense. \n\ncons:\n\nKey technical parts are not clear. The description of the training method is vague, e.g., the author(s) mentioned 'we utilized dropout and locked dropout where appropriate'. What does 'appropriate' mean? The training procedure was described in only few sentences. For example, it is not clear to me if a batch is fully random, or a batch consists of same number of sentences from each language, or a batch consists of same number of sentences from two languages, and how you train the WGAN. It is a bit surprising to me that different lambda gives similar performance. \n\nThe writing of the paper is not clear. Here are some of the reasons:\n1. The last paragraph in Section 2 does not fit into 'related work' section at all, instead, it is almost a repetition of the last paragraph in Section 1. \n2. The notations in Section 3 are very inconsistent. Just to name a few: the input dimension of function $e_j$ defined in the last paragraph in page 2 is not consistent with (1); the '$\\circle$' operation in (1) is not explained (although I can guess what it means); the $j_alpha, j_beta$ are not consistent with the $j^{th}$ language; in the last equation in page 3, the summation should be from 1 to m (instead of 0 to m) if there are m languages, and the superscript in $w$ is not defined. \n3. Key references missing, for example: there is no reference when deriving (4) using the 'Kantarovich-Rubenstein' duality. \n4. The organization for section 4 is not clear. The first sentence is quite confusing, and the content is a mixture of architecture design, training details, and experimental settings. Instead, one should separate these contents and address each of them. \n5. At the beginning of section 5.1, the hypothesis in the sentence 'to test this hypothesis' actually refers to the last paragraph in section 4. Figure 4 should be referred to in the last paragraph in section 5. 'english', 'german', 'chinese' should be 'English', 'German', 'Chinese'. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good method and results overall, with a few questions on analysis",
            "review": "This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.\n\nOverall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.\n\nMy main complaint is around Figure 5, Table 3, and the corresponding analysis.\n1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \\lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies \"representative power of UG-WGAN grows as we increase the number of languages\". It could be that the language-universal impose more constraints to model all languages, so the two variation (\\lambda=0.0 or 0.1) come closer to each other.\n\n2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.\n\nMinor:\n1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \\circ and the second one use parenthesis?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}