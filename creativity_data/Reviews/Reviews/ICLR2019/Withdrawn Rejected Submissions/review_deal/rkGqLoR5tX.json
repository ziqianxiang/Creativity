{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper studied interesting outlier detection algorithm but the novelty is limited",
            "review": "The paper studied interesting outlier detection algorithm but the novelty is limited.\nThis paper studies about the outlier detection under the neural network framework. However, it just applied the idea of partial least squares regression model on top of extracted features from the max-pooling layers of the CNN. The proposed approach is independent of any neural network or machine learning structure. The experiments are shown on fashion-mnist, cats and dogs and speech command recognition experiment using LSTM-model. This kind of direct application of the existing algorithm is not enough for publication in ICLR. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ODIN: outlier detection in neural networks",
            "review": "This paper studies how to better recognise classes that are found in the training set, and identify those that are not in the training set, using neural network methods. They call this as \"outlier detection in neural networks\".  I have several questions about this paper.\n\nFirst, the definition of outlier is ambiguous. The classes that are not in the training set do not necessarily belong to outliers. In this respect, I felt the evaluations are problematic, as the unused data have been considered as outliers. The performance metrics used are therefore not entirely appropriate, as they are used to evaluate how good is the classifier. Even you define the unused data as outliers, such outliers cannot be directly considered as outliers in the network, a drift of the training model from the actual physical model should be quantified with different inputs to the system. \n\nSecond, the actual contribution is the incorporation of equation (7) and (8) into the network layers, based on a linear approximation - something like applying PCA type of operation on the activations in different layers. There is no validation on the assumptions made, or any empirical study on the assumptions made about the \"disentangle manifolds\" and \"approximately linear\". \n\nThird, the paper lacks proofread and there are some typos and grammtical issues visual at several places, such as \"systems raises\", \"that uses reconstruction residual\", \"multitide\"  \"see 3.1\", \"CNN:s\", etc.\n\nFourth, in the experiments, the linear operation is applied to different layers and different networks, what's the reasoning behind this?\n\nSome specific comments:\nHow t_new is obtained in equation (4)?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Applying existing outlier detection techniques to the more informative features output by the NN layers",
            "review": "Summary\n-------\nThe paper applies an existing outlier detection technique (PLS - Partial Least Squares) to intermediate layer(s) of neural networks.\n\n\nMain Comments\n-------------\n\n1. The approach is similar to spectral anomaly detection where reconstruction error in lower dimensional space is used as the anomaly score. The higher layers of the neural network are good at identifying more informative features. Applying existing algorithms to these layers often improve algorithm accuracy. The proposed approach benefits from the same phenomena. The paper therefore lacks novelty.\n\n1. Section 4: \"...using MC-Dropout (Gal & Ghahramani, 2016) since it is well-established and straightforward to implement even though it has received criticism (Osband et al., 2016).\"\n\nThe benchmark methods are not state-of-the-art and were merely convenient to implement. Stronger benchmarks for anomaly detection techniques with images should be used. One traditional LOF variant relevant for comparison here is [1]. Other spectral techniques should also be considered.\n\n2. Section 4.2: \"...experiment, but both Mahalanobis distance and LOF fail to detect both cars and horses using 50 % R2\"\n\nThe comparison and the observation with R2 50% does not convey anything beyond obvious since it is already known that a factor as low as 50% looses almost half the information (variance in data).\n\nComparison of all various R2 (50, 80, 90, 95, 99)% factors is not relevant. These results might be presented for sensitivity analysis. Either the R2 should be fixed to a constant value (like 80 or 90% which performs well consistently) or a single value should be selected by automatic means. Currently, the experiments merely give an illusion of being rigorous without actually being so.\n\n3. The datasets and experiments are not sufficient. Std. errors are also not shown.\n\nReferences:\n[1] J. He, Y. Liu, and R. Lawrence. Graph-based Rare Category Detection. ICDM 2008 (http://faculty.engineering.asu.edu/jingruihe/wp-content/uploads/2015/06/GRCD.pdf)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}