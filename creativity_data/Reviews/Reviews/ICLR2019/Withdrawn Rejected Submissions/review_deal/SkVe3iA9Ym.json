{
    "Decision": {
        "metareview": "Pros:\n- new multi-objective approach to IRL\n- new algorithm\n- strong results\n- real-world dataset\n\nCons:\n- straightforward theoretical extensions\n- unclear motivation\n- inappropriate empirical assessment metrics\n- weak rebuttal\n\nAll the reviewers feel that the paper needs further improvements, and while the authors comment on some of these concerns, their rebuttal and revised paper does not address them sufficiently. So at this stage it is a (borderline) reject.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "interesting paper with some issues",
            "review": "This paper studies inverse reinforcement learning with a vector-valued setting. A key motivation of the paper, as suggested by its title, is to incorporate and analyze the complex human motivations.\n\nThe proposed setting seems new to me, although vectored-valued rewards and Pareto optimality have been studied in the context of RL. The biggest issue of this paper, in my opinion, is it doesn't properly support its claim that it improves the understanding of the agents' motivations and the reward functions. Details comments / questions are listed below.\n\n- Pareto dominance is a rather weak relation. When the number of criteria increases, it is less likely one alternative dominates another. In this case, the binary comparisons defined in Sec. 2.1 becomes less discriminative. Is this a problem to the proposed method?\n\n- Pareto dominance and vector-valued rewards have been studied in preference-based reinforcement learning, such as FÃ¼rnkranz et al. 2012 @ MLJ and Cheng et al. 2011 @ ECML. \n\n- Please fix the citation style in the paper and use \\citep and \\citet properly. \n\n- The empirical study in this paper doesn't properly support the authors' claim. (1) It's questionable to assume the actions of a player in an online game are optimal or even rational. (2) The results presented in Figure 2 is hard to read and the differences look minor. (3) Maybe I miss it, but has Table 2 been referenced and explained in the paper?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work, but need further improvement",
            "review": "This paper presents NMBM, a general inverse reinforcement learning (IRL) model that considers multifaceted human motivations. The authors have motivated and proposed the algorithm (Section 2 and 3), and demonstrated some experiment results based on a real-world dataset (WoWAH, Section 4).\n\n-- Originality and Quality --\n\nTo the best of my knowledge, the proposed NMBM algorithm is new. However, I feel that the derivation of this algorithm is relatively straightforward based on existing literature. Specifically, this algorithm is based on (1) Theorem 3 and (2) the linear program defined in equation 9. My understanding is that both Theorem 3 and the derivation of the linear program in equation 9 are relatively straightforward based on existing literature.\n\nOn the other hand, the experiment results in Section 4 are very strong and interesting. It is the main strength of this paper.\n\n-- Clarity --\n\nMy understanding is that the writing of Section 3 and 4 can be (and should be) further polished.\n\nSome key notations in the paper seem to be wrong:\n\n(1) In Theorem 3, how can the value function v^\\pi(s) be in the convex hull of policies? Also, e_i is not a set.\n\n(2) In equation 9, the linear program, \\eta should be another decision variable. \n\n-- Pros and Cons --\n\nPros:\n\n1) Strong experiments.\n\nCons:\n\n1) Insufficient novelty for algorithm design.\n\n2) No performance analysis for the proposed algorithm.\n\n3) Clarity needs to be further improved.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Beyond Winning and Losing...\"",
            "review": "======== Summary ============\n\nThe authors consider a setup where there is a set of trajectories (s_t, a_t, r_t) where r_t is a *vector* of rewards. They assume that each agent is trying to maximize \\sum_t \\gamma^t (\\phi . r_t) where \\phi is a preference vector that lives on the simplex. Their goal is to calculate \\phi (and maybe also an optimal policy under \\phi?). The \n\nThe authors first prove that this problem can be decomposed into finding Q functions for optimal policies for each component of r_t individually, and then solving for \\phi that rationalizes the trajectory of actions in terms of these Q functions. Given the entire collection of trajectories, they perform off-policy Q-learning on each component of r_t in order to learn the Q function for that component, and then use linear programming to solve for \\phi based on these Q function.\n\n========== Comments =============\n\nI think it's a worthwhile direction to combine IRL with modeling a diversity of preferences among agents. I can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides \"to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors\". Is the goal to do better policy prediction? To do better policy prediction conditional on \\phi? To infer \\phi to understand people's preferences from a social science perspective? These all seems reasonable but not sufficiently teased out in the work. (For comparison, IRL is typically - although not always - interested in learning the reward function in order to construct robust policies that maximize it). The authors also don't seem to solve a particular task of importance on the WoW dataset.\n\nThe theoretical approach seems sound, and I liked the way their algorithm was motivated and the way the problem was decomposed into off-policy Q-learning and then solving for \\phi.\n\nHowever, I found myself quite confused in the experimental section (4.3). The authors evaluate their approach by action prediction. Given the trajectories, is \\phi computed for each player and then compute actions based on that value of \\phi? Is \\phi computed on the same trajectory data used for evaluation or a different subset? Or is action prediction performed in aggregate across the entire population? The experimental setup was never clarified for this (main) experiment.\n\nI was also confused about the motivation for Figure 2 and Appendix D. The authors are showing that their predictions about which reward is motivating the players is consistent with external factors. But wouldn't you see the same thing if you just plotted the observed *rewards* themselves? E.g. players in a guild will achieve more Relationship reward. \nThe proposed approach takes the vector of reward, learns which actions are consistent with achieving each reward, then infers from the actions which reward is trying to be achieved. What advantages does this have vs. just looking at the empirical trajectory of rewards for each player/group?\nI can certainly imagine that the IRL approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.\n\nThe writing could also use some improvement for a future iteration, I've listed a few points below:\n\npg.1, Neither Brown & Sandholm nor Moravcik et al use \"RL algorithms\"\npg.1, Finn et al unmatched )\npg.1, \"a scalar reward despite observed or not\" -> \"a scalar reward whether observed or not\"\npg.2, \"Either the range of\" -> \"Both the range of\" (and this sentence needs further cleanup)\npg.2, \"which records the pathing of players\" ??\nTheorem 3: \"each of the set e_i has an unique element...\" This isn't clear. I think you mean \"For each e_i there is a unique vector v^\\pi(s) for all \\pi \\in \\Pi_{e_i} . The equality holds if these vectors are distinct for each e_i\".\npg. 5 \"If otherwise all elements in \\phi are generative\" how can they be negative if they are on the simplex?\npg.5 \"we do not perform any scalarization on the reward...the model assumption is easier to be satisfied\" I think this is a strange comparison to IRL because in IRL you're trying to find a (possibly parametric) function (s,a) -> R, whereas here you're *given* the vector R and are trying to find \\phi. So while you have more degrees of freedom by adding \\phi, you lose the original degrees of freedom in the reward function.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}