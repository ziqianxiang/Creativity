{
    "Decision": {
        "metareview": "The paper received mixed and divergent reviews. As a paper of unusual topic in ICLR, the presentation of this work would need improvement. For example, it is difficult to understand what's the overall objective function, why a specific design choice was made, etc. It's nice to see that the authors somehow did quite a bit of engineering to make their model work for classification and drawing tasks, but (as an ML person) it’s difficult to get a clear rationale on why the method works other than that it’s biologically motivated. In addition, the proposed model (at a functional level) looks quite similar to Mnih et al.'s \"Recurrent Models of Visual Attention\" work (for classification) and Gregor et al's DRAW model (for generation) in that all these models use sequential/recurrent attention/glimpse mechanisms, but no direct comparisons are made. For classification, the method achieves strong performance on MNIST but this may be due to a better architecture choice compared to Mnih's model but not due to the difference of the memory mechanism. For image generation/reconstruction, the proposed method seems to achieve quite good results but they are not as good as those from DRAW method. Overall, the paper is on the borderline, and while this work has some merits and might be of interest to some subset of audience in ICLR, there are many issues to be addressed in terms of presentation and comparisons. Please see reviews for other detailed comments.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Interesting ideas, but muddled presentation.",
            "review": "Summary: this paper introduces a new  network architecture inspired by visual attentive working memory. The network consists of a recurrent components that generates glimpses of features from a CNN applied to the input (inspired by the DRAW network), and a working memory component that iterative stores memories using Hebbian mechanisms. The authors apply this network to classification tasks (MNIST) as well as using it as a generative model.\n\nThe ideas in the paper are somewhat  interesting, but I have major concerns with the motivation (it is unclear) and the experiments (not convincing):\n\nMotivation: The authors motivate their inclusion of a Hebbian working memory from the perspective of trying to mimic the human visual system. The main problem here is that it is unclear what problem the authors are trying to solve by including this Hebbian mechanism. In the fast-weights paper (Ba et al), they had a clear example of a task that standard recurrent networks could not easily solve, which motivated the inclusion of a working memory mechanism. A similar motivation here is lacking, with the main justification seeming to be to \"move towards a biologically motivated model of vision\". Are the authors interested in more biologically motivated models because they think they will be useful for some task? Or are the authors interested in models of biological vision itself? If the former, it is unclear what new tasks would be solved by their model (all the results focus on tasks that can be solved without these mechanisms). If the latter, there should be some clear goals for what they hope their model to achieve. \"Moving towards biological vision\" is too vague and broad of a justification in order for us to judge progress. Section 2 discusses, at a high level, broad concepts from the visual neuroscience literature, but this also does not clearly motivate why the authors are interested in this particular instantiation of these ideas, indeed, their model is only weakly related to many of the neuroscience ideas discussed.\n\nResults: The authors evaluate their network on two tasks: classification and image generation.\n- For classification, I have a hard time understanding what these results tell us. Very simple models can achieve low test error on MNIST, so it is unclear what the attention or working memory buys you. One simple improvement would be if the authors ablated different parts of their network to show how critical they are to performance. Increasing the window size to 28 helps the model, suggesting that the network is hindered by the glimpses, so I do not feel like I have learned much from these results. In addition, the authors only mention Cifar10 results in a couple of lines, so it is hard to take anything away from those statements.\n- For image generation, similarly, the authors do not compare their model to other standard generative models. Does their model perform better than simple baselines?\nFinally, for all of these results, what is the working memory doing? Why is it necessary? Does it learn something interesting? It is hard to understand the significance of the work without answers to these questions.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More understanding and experiments are needed to better motivate the model.",
            "review": "The paper proposes a novel Hebb-Rosenblatt working memory model to augment the recurrent attention model and achieves competitive results on the MNIST dataset. Some results on CIFAR and Celeb-A datasets are also shown. The code is released anonymously which substantiated the reproducibility of the results; however, I haven’t physically run the code to verify it.\n\nMotivation: First, as much as I appreciate the research direction of combining recurrent attention models and working memory, the use of recurrent attention models is not well motivated throughout the article. It it of course biologically inspired, but the engineering benefit is not obvious. It has the benefit of model compression, using less parameters to process the attended region, yet pooling mechanisms can also achieve similar effect. It also has the benefit of model interpretability, but for vanilla feed-forward counterparts, it is also possible to visualize salient regions that impact the decisions. While feed-forward CNNs process all regions in parallel, sequential models will be much slower. The core question is the following, if the final task is just image classification, why is sequential processing necessary? The author should spend some text explaining why studying recurrent attention model + working memory is important. Ideally, if the author could consider tasks other than image classification, which could potentially highlight the need for sequential processing and working memory.\n\nNotation clarity: The clarity of model notation could be significantly improved. I am confused what is `e`, and my guess it is the layer I input. Figure 1 does not help my understanding. Notations such as \\mu, \\nu, M are left unexplained.\n\nUnderstanding of the model: Although Hebbian learning rule is a well established mechanism, the article doesn’t provide much insight into why the rule is applied here but rather just stipulates them in Section 3.1 as the overall formulation of the memory network. What would be the objective function for such an update rule (e.g. the delta decay term corresponds to a L2 weight decay term)? For the applications studied in the paper, i.e. image classification, why does the model need to ever forget?\n\nCIFAR/Celeb-A experiments: There is no tabulated results for these experiments. I only see CIFAR gets 93.05% accuracy, without mention of any baselines. Since attention mechanisms are used here, it would be a much stronger argument to report results on higher resolution images than CIFAR (which is 32x32).\n\nModel interpretability: Model interpretability is often one of the biggest selling point of attention-based models. However, by examining the glimpse locations on CIFAR datasets, the model learns to look at the whole image for all glimpses, which hurts the argument of using a recurrent attention model. Also for MNIST experiments, the best number is achieved by using S_g=28, which has the glimpse size of the whole image. The author claims that it can learn a good representation despite the not so good looking glimpse visualization, but so can regular feed-forward CNNs.\n\nComparison to VAEs: Since VAE formulations are used, it would be good if the authors can compare the model with vanilla VAE and convolutional VAEs, both in terms of classification accuracy and reconstructed sample quality.\n\nComparison to DRAW: It is also recommended to show more visualization comparison to DRAW, and pinpoint the differences of using a canvas based memory vs. the proposed Hebb-Rosenblatt working memory.\n\nIn conclusion, I think the paper opens a promising direction of combining recurrent attention models and working memory networks. I believe it is a huge amount of engineering effort to make this model to work, as we can see in the Appendix and the released code base. However, there are several issues as I pointed above, most notably the motivation, and understanding of the models. The experiments on CIFAR and Celeb-A could been done more thoroughly, and I also believe that it would make more impact if the authors can show experiments other than image classification that highlight the need for a recurrent attention model equipped with working memory. Based on the above reasons, I think the paper could be better polished for future publication.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"A Biologically Inspired Visual Working Memory for Deep Networks\" (Strong Accept after Revision)",
            "review": "Revision:\n\nThe authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.\n\nI believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work.\n\nOriginal Review below:\n\nSummary: They propose a biologically motivated short term attentive working memory (STAWM) generative model for images. The architecture is based on Hebbian Learning (i.e. associative memories are represented in the weight matrices that are dynamically updated during inference by a modified version of Hebbian learning rule). These memories are sampled from glimpses on an input image (using attention on contextual states, similar to [1]), in addition to a latent, query state. This model learns a representation of images that can be used for sequential reconstruction (via a sequence of updates, like a sketchpad, like DRAW [1], trained in an unsupervised manner). These memories produced by drawing can also be used for semi-supervised classification (achieves very respectable and competitive results for MNIST and CIFAR-10).\n\nThis paper is beautifully written, and the biological inspiration, motivation behind this work, and links to neuroscience literature as well as relation to existing ML work (even recent papers) is well stated. The main strength of this paper is that the author went from a biologically inspired idea to a complete realization of the idea in algorithmic form. The semi-supervised classification results are competitive to SOTA, and although the CIFAR-10 reconstruction results are not great (especially compared to generative adversarial models or recent variation models [2]), I think the approach is coming from a very different angle that is different enough compared to the literature to warrant some attention, or at least a glimpse, so to speak, from the broader community. The method may offer new ways to interpret ML models that is current models lack, which in itself is an important contribution. That being said, the fact that most adversarial generative models achieved a far better performance raises concern on the generalization ability of these memory-inspired learned representations, and I look forward to seeing future work investigate this area in more detail.\n\nThe authors also took great care in writing details for important parts of the experiments in the Appendix section, and open sourced the implementation to reproduce all their experiments. Given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks. Even without the code, I felt I might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.\n\nI highly recommend this paper for acceptance, with a score of 8 (edit: revised to 9 after rebuttal period). The paper might warrant a score of 9 if they had also achieved higher quality results for image generation, on Celeb-A or demonstrated results on ImageNet, and provided more detailed analysis about drawbacks of their approach vs conventional generative models.\n\n[1] https://arxiv.org/abs/1502.04623\n[2] https://arxiv.org/abs/1807.03039\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}