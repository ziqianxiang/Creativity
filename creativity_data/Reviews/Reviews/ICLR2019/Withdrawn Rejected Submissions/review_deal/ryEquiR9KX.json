{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review: APPLICATIONS OF GAUSSIAN PROCESSES IN FINANCE",
            "review": "This paper illustrates how the Gaussian Process Latent Variable Model (GP-LVM) can replace classical linear factor models for the estimation of covariance matrices in portfolio optimization problems. Some experimental results of the global minimal-variance portfolio are presented on a subset of stocks from the S&P 500, comparing against a covariance shrinkage approach. Results from related tasks (missing value imputation and latent space interpretation) are also presented.\n\nIn general, the paper is reasinably easy to understand, although some notation could be better explained (see below). Technically, the paper uses a GP-LVM as a drop-in replacement to a linear factor model for covariance matrix estimation, and does not offer a machine learning contribution. There is also no comparison to Wilson and Ghahramani’s Generalized Wishard Processes. As a contribution to the methodology of portfolio management with machine learning, the paper spends precious pages reviewing well-understood financial theory, and then can offer only superficial evidence of the approach’s benefits. As a contribution to ICLR, the paper should also try to better characterize the quality of the learned representations, against alternatives.\n\nAs such, it would seem fitting for the authors to consider a more financially-oriented venue for presenting their results.\n\nDetailed comments:\n\n* p. 1: Noble Price ==> Nobel Prize (please)\n* p. 2: Start of Section 2.1, surely it is meant $f(\\cdot) \\in R$ instead of $f \\in R$, since $f$, being a function, belongs to some Hilbert space.\n* p. 2: Start of Section 2.2. It would help to state right away that N is the number of assets and D is the number of days (if that’s indeed the case). This is the transpose of the usual data presentation for doing, e.g. PCA, wherein the days are along rows, not columns.\n* p. 2: why is it said that $X \\in R^{N \\times Q}, i.e. it is the number of days whose dimensionality is reduced, and not a smaller number of latent factors driving the cross-section of asset returns. Note that the latter is the assumption behind linear factor models. If the authors intend a different formulation, this should be amply discussed. As it stands, the presentation is confusing.\n* p. 3: the notation $K = k(X,X)$ is confusing. The function $k(\\cdot,cdot)$ was defined for vectors; its extension to matrices should be explained.\n* p. 3: it is stated that the ELBO is “the best approximation to the posterior”. It is not. It is the best approximation within the variational family, under the KL criterion.\n* p. 3: at the end of that section, it is stated that GP-LVM gives an estimate of the covariance matrix between the N points. A quick recap of how that comes about would help the reader here.\n* p. 5: in eq (19), citations for where those kernels come from would be necessary. It also couldn’t hurt to state that “m32” stands for the Matérn 3/2 kernel.\n* p. 5: in eq (19), shouldn’t all those kernels be multiplied by a magnitude hyperparameter $\\sigma^2$ ?\n* p. 7: The results of Table 1 should be subject to more rigorous analysis. In particular, there should be a test to ensure Sharpe differences are significant, e.g. Liu, Y., Rekkas, M., & Wong, A. (2012).\n* p 8: in Figure 3, it is not clear what’s being plotted exactly. Please refer to an earlier equation.\n\n\nReferences:\n\nLiu, Y., Rekkas, M., & Wong, A. (2012). Inference for the Sharpe ratio using a likelihood-based approach. Journal of Probability and Statistics.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Usage of a standard GPLVM for covariance modeling of daily S&P500 time series to optimize portfolios and predict missing values.",
            "review": "Summary\nThe paper uses standard GPLVMs to model the covariance structure and a latent space representation of financial time series. On daily close prices of S&P500 data, the authors demonstrate that the approach is a useful covariance model.\n\nQuality\nAlthough the application is clear and the methodology is well established, the quality of the submission can be improved by a better connection to prior work in the finance domain. Being not an expert here, it seems that there should be more advanced methods one might want to compare to. In particular missing standard deviations in the figures make it hard to judge the relevance of the empirical evaluation.\n\nClarity\nThe manuscript is well written and the technical content and the experimental results are well accessible to the reader. A little bit more attention needs to be paid to properly defining all quantities. See \"Details\".\n\nOriginality\nThe application to financial time series is straight forward but I have not seen it before. The manuscript does not modify the standard GPLVM methodology.\n\nSignificance\nAs the application is obvious and as the evaluation of the methodology does only include very little comparison to existing algorithms, the significance of the presented work is limited.\n\nReproducibility\nThe data can be obtained from Yahoo finance, the implementation is based on scikit-learn. Although the authors do not provide code and data, the work should be reproducible with reasonable effort. Missing details of the GPLVM optimization and the unknown random split might prevent the exact replication of the results.\n\nPros and Cons\n1-) Non-stationarity is not dealt with.\n2-) Figure 1: standard deviations should be added.\n3+) The model is simple and well established.\n4-) Proper optimization can be cumbersome and is inherently plagued by local minima.\n\nDetails\na) Title: The paper is about GPLVMs rather than GPs\nb) Abstract: betas is an undefined term here\nc) Intro: w, r, K are not properly defined\nd) Intro: Gaussian Process -> Gaussian process\ne) Intro: In Bayesian framework -> In a Bayesian framework\nf) Sec 2.1: $f \\in R$ -> $f(x) \\in R$\ng) Eq. 5/6: this is only correct for m(x)=0 and no noise\nh) Sec 3.1: missing reference for CAPM\ni) Sec 4.1: please also mention N and D\nj) References: Neil Lawrence, ... Gaussian process latent ...\nk) References: Bayesian Gaussian",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "lacking technical novelty ",
            "review": "This paper proposes to use a GPLVM to model financial returns. The resulting covariance matrix is used for portfolio optimization, and other applications. \n\nThe weakness of this paper is that there is no technical innovation, and the inference of the GPLVM is unclear. As far as I understand, the inference is exact over the GP, and VI is used for the latent variable locations (though the VI posterior is not specified). This is not the inference used in Titsias and Lawrence 2010 (where sparse VI is used for the GP), which makes the inference confusing as  Titsias and Lawrence 2010 is cited in the VI inference section. \n\nAn important aspect is not made clear: how are predictions made at test points? In Titsias and Lawrence 2010 a further optimization is performed with a partial objective (using the ‘observed’ dimensions only. \n\nThe results come across as disjointed as it is not made clear exactly what problem the model is intending to solve. There are some potentially interesting things here, but in 4.3.1 the difference in the methods appears very small (only a small fraction of a standard deviation), and in Figure 3 there is no comparison to other approaches so it is not clear how useful this apparent clustering might be. \n\n\nFurther comments (numbering is section, paragraph, line)\n\n0,1,5 ‘readily interpretable’ does not seem to me justified\n\n0,1,-4 ‘naturally shrinks the sample covariance matrix’ This claim is not revisited \n\n1,1,2:3: w and K not defined\n\n1,1,-4 ‘Bayesian machine learning methods are used more and more in this domain’ What evidence is there for this claim?\n\n1,1,-1 ‘one can easily[…] suitable priors’. This seems to me a confusing statement as in under the Bayesian paradigm priors are necessary for inference, not additional.\n\n1,3,1 The ‘Bayesian covariance estimator’ here is a bit confusing, as the covariance itself is not the primary object being modelled, but is being estimated as part of a GP model. \n\n2.2,1,2 ‘without losing too much information’ This needs clarification. \n\n2.2,1,: The prior for X is not stated.\n\n2.3,:,: It seems unnecessary to give such a long introduction to VI but then have no details whatsoever on the actual variational posterior being used. It is extremely unclear what inference is being used here. In Lawrence 2005 the form of (8) is used with a MAP estimate for X. The introduction of VI in this subsection suggests that inference is going to follow Titsias and Lawrence 2010, but in this case there is much to say about the variational distribution over the GP, inducing points, kernel expectations, etc.\n\n2.3,-1,1 ‘So, GP-LVM is an algorithm’ I would not describe it as such. I would say that it is a model. \n\n3, 1,0 ‘Now we have a procedure to estimate the covariance matrix’ This procedure has not been made clear, in my view. As I understand it, a (potentially Bayesian) GP-LVM is fitted (potentially following  Titsias and Lawrence 2010, but perhaps following Lawrence 2005), and the K matrix is formed. \n\n3.1,-1,-6 ‘known, that’ perhaps this comma is unintended?\n\n3.1,-1,-2 is B is what was previously known as X?\n\n3.2,1,1:2 ‘, for which he received a Nobel Price in economics’ What is the purpose of this information?\n\n3.2,1,2 ‘how good a given portfolio is’. What does ‘good’ mean, in this context?\n\n3.2,2,1 ‘w’ has not been defined. \n\n3.2,2,4 ‘primarily interested in the estimation of the covariance matrix’. This seems to be a circular justification. \n\n4.1,2,: I would suggest defining the kernels with a variance parameter, to save the cumbersome Sigma notation \n\n4.1,-1,: What variational posteriors are used? \n\n4.2,2,1 The ELBO is only a lower bound to the marginal likelihood, so cannot always be used reliably for model comparison (though in practice it may be a good proxy)\n\n4.2,2,-3 ‘which is a sign of overfitting’ I’m not sure this claim is true. A lower ELBO only indicates that the complexity penalty isn’t being compensated by a better model fit term. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}