{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "# Summary\nThis paper proposes a generative model of visual observations in RL that is capable of generating observations of interests. The idea is to first train a VAE which reconstructs a given observation with a regularizer that encourages the generated images from which the policy can reconstruct its output. This regularizer makes the VAE focus on reconstructing parts of images that are important for the given policy. After training the VAE, this paper proposes to optimize the latent variable of the VAE with respect to a target function (e.g., high/low values, specific actions) to generate states of interest. The experimental result shows that the proposed model can generate realistic images in Atari games that are more interpretable to the agent compared to a vanilla VAE. It is also shown that it is possible to generate states of interest such as high/ow-value states by plugging in different target functions, which allows users to analyze RL agents easily. \n\n[Pros]\n- An interesting attempt to learn a controllable generative model of states to analyze deep RL agents. \n\n[Cons]\n- (Major) The experimental results are not comprehensive.\n- (Minor) The usefulness of the proposed generative model is not clear.\n- (Minor) Inconsistent equations/notations\n\n# Novelty and Significance\n- The proposed regularizer for VAE and gradient descent approach for generating particular types of states are novel and interesting.\n- Although this type of generative model can be useful for analyzing learned RL agents more efficiently, it is unclear how this could be useful for improving AI safety as claimed by this paper. In Section 4.6, for example, this paper shows \"distracted pedestrians\" example and claims that their generative model is useful to verify whether the policy can handle such an adversarial example or not. However, users still need to come up with such a scenario, manually construct such an adversarial example, and test it using the policy. I do not see any role of the proposed generative model here. A more convincing example or experiment would be necessary to support the main contribution of this work. \n\n# Quality and Experiments\n- This paper shows a few generated samples for qualitative evaluation. However, it is unclear whether such samples are cherry-picked or randomly-picked. It would be important to present many \"random\" samples from the generative model to evaluate the quality of samples. \n- Related to the above comment, it would be also important to show interpolation / extrapolation in the latent space to show that the model has learned a reasonable manifold instead of overfitting to the training data.\n- Instead of presenting Table 2 to show generalization performance, it would be more informative to show examples of generative samples + nearest neighbors from the training data, which is a common practice to verify overfitting in generative models.\n- Figure 5 (+ discussion on why activation maximization does not work) looks relatively less important, which could be condensed or moved to the appendix.\n- It would be interesting to see how the generated samples change as the number of gradient-descent steps for the target function increases.\n\n# Clarity and Presentation\n- This paper does not fully describe how the energy function is optimized (initial latent variable, the number of gradient-descent steps).\n- There are some inconsistent and undefined notations, which need to be fixed.\n1) KL divergence should be defined over two probability distributions. However, KL-term in this paper is defined over the output of the network (Equation 1).\n2) What is the form of the generator g? It is introduced as \"g(\\mu, \\sigma, z)\" but described as g(x, z) in Equation 4.\n3) In VAE, we normally define the latent variable z as random variable from N(\\mu, \\sigma^2). But, it seems like this paper uses z as unit Gaussian, which is a bit confusing.\n4) I_n in Equation 1 is not defined. \n5) In page 3, A(s)_a is not a well-defined notation?\n6) Is \\pi(s) a vector or scalar?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A reasonable approach for visualizing states of interest.",
            "review": "Summary.\nThe paper proposes an approach that involves two stages: (1) a variational autoencoder that learns to reconstruct the state space (i.e., input images in Atari games) and (2) an optimization step that finds a set of conditioning parameters to generate a synthetic image. The effectiveness of the proposed method is qualitatively evaluated on Atari games and a driving simulator. Though the approach looks interesting, clarification on claims/evaluations is required. \n\nStrengths.\n- An interesting problem in the current CV/RL community.\n- Well-surveyed related work.\n\nGenerative models vs. Query-based models.\nThough the authors mention several different target functions (T) can be chosen, the paper only explores only one simple function that could easily be achieved in other ways. For example, given a trained agent, it could be feasible to collect observations where the agent outputs high or low Q-values for all possible actions. Also, the following existing paper successfully finds a few critical states in which a certain action is important to be taken. This was done by computing entropy over the policy’s output and by computing the baselined maximum Q-value. \n\n[1] Huang et al., “Establishing Appropriate Trust via Critical States”, IROS 2018.\n\nProviding other possible target functions will be helpful to convince readers about its usefulness.\n\nSynthesizing unseen scenarios.\nI fully agree that synthesizing unseen scenarios is an important and interesting problem (especially for self-driving vehicle controls). However, I am worried about the ability of VAE in generating novel images, which is basically trained to reconstruct the manifold of a domain of training data. Counting the number of different pixels would not be sufficient evaluation metric especially in vehicle controls where small visual cues (i.e., pedestrians, vehicle, etc)  plays an important role. Can authors provide more detailed analysis of the ability to synthesize unseen scenarios?\n\nClarification in the Section Methods.\nNotations are used without providing a careful definition, which makes hard to understand the methodology. For example, ‘z’ and ‘d’ in the definition (2). Also, can authors clarify the term pi(s) - which is defined as a scalar value from the agent network’s output - with the del operator? \n\nMinor concerns.\nTypos",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning\"",
            "review": "This paper introduces a new method for visualizing states which are likely to lead to various outcomes (e.g., high/low reward or a particular agent action). This method is based on a VAE modified to produce samples which not only allow reconstruction of the input, but also contain salient input features (as measured by guided backpropagation) and which will result in the same agent output as the original input. The paper then goes on to use this method to generate inputs for Atari games which result in particular rewards (high/low), actions (e.g., left), or uncertainty (states in which different actions can either lead to very good or bad states). \n\nThe paper is clear and well-written, and a large number of generated samples are provided (especially in the Appendix, which contains examples for all games in the ALE for a variety of outcomes). The method for regularizing the samples is novel to my knowledge. However, while the method for generating samples is interesting, this paper contains several critical issues (detailed below). I therefore think that this paper needs more work and is not yet ready for publication in a venue such as ICLR. \n\nMajor comments: \n\n1) While the visualizations are indeed interesting, it’s unclear whether these visualizations provide information about particular agents or about the task itself. This is a critical distinction, especially in light of recent results (cited by the authors) which suggest that many current visualization methods produce the same outputs regardless of the model studied [1, 2, 3]. Moreover, this issue is especially important given the use of guided backpropagation in the sample generation method itself.\n\n2) Assuming the visualizations are model-specific, it’s unclear what (if any) falsifiable statements can be generated from these visualizations. Section 4.3 takes a stab at this, stating that the visualizations revealed that a particular agent didn’t understand the oxygen mechanic in Sequest, which was confirmed by rolling out the agent (though no data is presented along with this anecdote). This paper would be dramatically strengthened by more experiments like this section (with provided data, of course). Without such a link to the agent’s ultimate behavior (and by extension, ground truth), it’s unclear whether these visualizations actually lead to any understanding at all.\n\n3) Further experiments are needed to demonstrate the benefit of the proposed method over simply analyzing a replay buffer. The experiment included in the manuscript based on pixel differences is flawed as pixel differences are a poor measure of mode collapse since they are sensitive to simple image transformations such as translation. This issue has been investigated in the context of GANs, in which pixel differences were found to work only for centered images such as face datasets [4]. In order to evaluate whether the proposed method is actually advantageous over analyzing the replay buffer, a perceptual metric would need to be used.\n\nMinor comments:\n\n1) The claim in the third paragraph of Section 4.5 that unused conv1 weights lead to more easily “distracted” models is unsubstantiated.\n\n\n[1] Adebayo J, Gilmer J, Muelly M, Goodfellow I, Hardt M, Kim B. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292. 2018 Oct 8.\n\n[2] Hooker S, Erhan D, Kindermans PJ, Kim B. Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. 2018 Jun 28.\n\n[3] Kindermans PJ, Hooker S, Adebayo J, Alber M, Schütt KT, Dähne S, Erhan D, Kim B. The (Un) reliability of saliency methods. arXiv preprint arXiv:1711.00867. 2017 Nov 2.\n\n[4] Arora S, Zhang Y. Do GANs actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224. 2017 Jun 26.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}