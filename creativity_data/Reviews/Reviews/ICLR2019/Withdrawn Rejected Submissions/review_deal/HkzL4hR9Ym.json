{
    "Decision": "",
    "Reviews": [
        {
            "title": "Well motivated simple approach but need more analysis and stronger experiments.",
            "review": "In the paper, the authors tackle the problem of representation learning and aim at building reusable and structured representation. They argue that co-adaption between encoder and decoder in traditional AE yields poor representation and they introduce community based auto-encoders CbAEs. the approach is motivated by linguistic communication and it is straightforward to implement.\n\nHowever, some points deserve to be clarified:\n1. For classification tasks, what is the absolute value of classification accuracy? what happens if we consider larger number of iterations? only few iterations are shows in plots. \n2. Are linear classifiers learned separately and then are their outputs averaged in testing time? Is the observed gain of sample complexity really due to community based training or it is just an effect of averaging an ensemble of AEs?\n3.  How the entropy regularization is tuned in experiments ?  \n4. Experiments would be more convincing if they could show benefits for more challenging transfer learning task such that few-shot learning/ semi-supervised learning.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper presents a community based autoencoder framework. It tries to address co-adaptation of encoders and decoders. And aims at constructing better representations. Overall, the paper needs work.",
            "review": "1) Can we think of this setup as sparse + desoising AE? As opposed to being similar to dropout? \nFor better clarity, describe what the setup reduces to for length 1 AE setting in the CbAE? \n\n2) What is the influence of regularization in AEs, including denoising or contractive criteria? And jumping to experiments, these varieties of AEs are not evaluated? Since the criterion of encoder-decoder selection corresponds to noising, this evaluation is critical.\n\n3) The natural baseline here is an ensemble of AEs. Should be compared.\n\n4) How similar are the representations across the decodes in the community (not the RSA setup used in page 8)? This gives clear info on how big the community should be? And more critically, whether the cross-decoder representations are repetitive/redundant (i.e., an intrinsic fitting issue for CbAE)? \n\n5) Is the statement following eq 3 the other way around? Positive SCG is good or bad? Confusing, explain? \n\n6) What about non-monotonic learning curves? How is SCG formulated for such a setting? A windowed average of learning curve thresholds is a better criterion. A more broader question is why using this criterion makes sense for the goal of better representation building? \n\n7) Why is iteration 64, community size 16 consistently -ve SCG in CIFAR? And for the same community size the RSA is going down after increasing initially in Table 3? Something going on here. Intuitively, this implies bounds on the community size (like with random forests ensemble setting -- a tradeoff of number of trees vs. depth). \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a promising representation paradigm that requires more extensive study ",
            "review": "It is very refreshing to see a submission that investigates the fundamental question of representation: what is considered as a good representation. This works argues that is it the representation learned through a community of autoencoders.  The idea is very well motivated, taking inspiration from linguistic development. The idea is quite intuitive to understand, and makes sense. The narrative is fluent, with some accidental syntax errors. \n\nI believe this line of thinking is original to some extent, although bears some similarity with ideas in Dropout, DropConnect, however, motivated differently. It is, as pointed out by the authors, indeed a generalization of the co-adaption idea.\n\nHere is a list of questions that I have in mind:\n1. The use of the cross-entropy regularization deserves more attention. It is not clear how important this regularization is experimentally. Is there a coefficient to control its contribution?\n2. Experiments on MNIST and CIFAR require some more clarification. First of all, the use of SCG as a metric is OK, but experiments seem to only focus on early iteration comparisons. I would like to see, for example, the entire classification error (or reconstruction error) curve on the test set, epoch by epoch, with standard AEs and your proposed CbAEs side by side. \n3. Secondly, it is not clear from the text of how your test error is obtained from the community as it contains many models. It is important to factor out the effect of model averaging in comparison with standard AEs (or a community of size 1). \n4. Lastly, the negative effects of increasing community size is not clearly explained. I would like to gain more explanation of its source, especially on CIFAR100.  \n\nOverall, I think the premise of this work could be more substantiated by some semi-supervised (for transfer learning) tasks with a more realistic setup and public benchmarks. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}