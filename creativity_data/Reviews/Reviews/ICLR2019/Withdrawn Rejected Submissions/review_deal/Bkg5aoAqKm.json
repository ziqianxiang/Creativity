{
    "Decision": {
        "metareview": "This paper proposes an Optimal Binary Functional Search (OBFS) algorithm for searching with general score functions, which generalizes the standard similarity measures based on Euclidean distances. This yields an extension of the classical approximate nearest neighbor search (ANNS). As observed by the reviewers, this work targets an important research direction. Unfortunately, the reviewers raised several concerns regarding the clarity and significance of the work. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Rejection: good Paper but still require further improvement "
    },
    "Reviews": [
        {
            "title": "Fast Binary Functional Search on Graph",
            "review": "This work extends the approximate nearest neighbor search (ANNS) algorithm to a more general setting. Instead of search with a \"separable\" similarity measure, the authors propose Optimal Binary Functional Search (OBFS), where the scoring function f() is in general non-separable. The exact construction of the Binary Function Graph wrt f() and X is computationally expensive. The specific approximate algorithm of OBFS proposed in the paper is to:\n1) First construct an L2 Delaunay graph for based on the dataset X only and;\n2) Perform greedy search with the L2 Delaunay graph.\n\nThe authors also discuss various conditions under which, the approximation method can achieve close to optimal value.\n\nSome of the concerns I have with this work:\n\n1) The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in  Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.\n\n2) Lately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches.\n\nOverall, this research direction is interesting, but this specific work falls short for a publication at ICLR.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising novel idea; needs further clarification and development",
            "review": "Post-rebuttal\n------------------\nI have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:\n- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.\n- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.\n\n=========================================\nPros\n-------\n[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.\n\nLimitations\n----------------\n[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. \n\n[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. \n\n[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?\n\n[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \\lambda^* > 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. \n\n[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. \n\n\nGeneral comments\n---------------------------\n[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as \"gradient descent in Euclidean space\" (although the authors would need to also precisely explain what they mean by that statement).\n\n[Originality] Some related work that the authors should position their proposed problem/solution against:\n- There is some work on \"max-kernel search\" which can perform similarity search with general notions of similarity (than just Euclidean metrics).\n- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.\n\nMinor comments/typos\n---------------------------------\n- The authors should present the precise SL2G algorithm given the graph in the manuscript.\n- l^2 --> \\ell_2\n- gradient decent --> gradient descent\n- Table 1, f(q, x) --> f(x, q)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}