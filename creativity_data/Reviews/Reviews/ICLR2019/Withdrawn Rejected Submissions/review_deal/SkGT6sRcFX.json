{
    "Decision": {
        "metareview": "The paper studies how to construct infinitely deep infinite-width networks from a theoretical point of view, and uses the results of its theoretical analysis to design a weight initialization scheme for finite-width networks. While the idea is interesting and the paper may contain novel theoretical contributions, the experimental results are weak, as pointed out by all three reviewers from several different perspectives. In particular, it seems that the presented theoretical analysis is useful mainly for weight initialization and hence has limited potential impacts. In addition, the authors have responded to neither the AC's question, nor a detailed anonymous comment that challenges the value of Proposition 1 given the previous work by Aronszajn.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Solid theoretical analysis but unconvincing experiments and limited potential impacts"
    },
    "Reviews": [
        {
            "title": "The idea proposed in the paper is interesting but the paper appears quite incomplete",
            "review": "In this paper, the authors propose deep neural networks of infinite width. The primary challenge in such networks is defining a distribution over the weights connecting two layers of infinite width. The authors tackle this by using Gaussian Processes for these distributions with the covariance functions defined in a canonical manner. Inspired by these networks, the authors propose weight initialization schemes for finite width networks.\n\nWhile the idea proposed in the paper is interesting, the paper appears quite incomplete. In particular, the authors do not mention a single example of a kernel that can be constructed using the process outlined in Section 3. Furthermore, the only application of these infinitely wide networks proposed in this paper is for initialization of the weights of finite width networks. It will perhaps be more interesting if the authors can use the kernels obtained for supervised learning tasks using kernel machines (as done in Cho & Saul 2009) or as the covariance function of a Gaussian process (as done in Wilson et al. 2014).\n\nMoreover, the experiments conducted on finite width networks are not enough to justify the utility of this initialization scheme. It will be useful if the authors can test the performance of the state-of-the-art networks for CIFAR-10/100 and ImageNet, where the weights of the last fully connected layer have been sampled from different distributions. An extension to initialization for convolutional layers will further strengthen the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Systematic theoretical work with some supporting experiments",
            "review": "Summary:\nI'm not very familiar with the work this is building off of, but my summary is as follows:\nThe authors look at the problem of defining multilayer infinite width neural networks. The main challenge is that the weights (which are in some sense now a function) must be appropriately sampled to ensure that norms don't explode. \n\nThis has only been done for two layers before, and the authors derive how to do this for more than two hidden layers using RKHSs. This initialization is called Win-Win, and is compared to different initializations on a few different datasets.\n\nClarity: This paper is quite technical and hard to follow without knowledge of the prior work. I think the authors could have been a little clearer on some of the challenges. E.g. instead of talking about the weights needing to be \"in the same function space\", it would be helpful to remphasise the norm issue.\n\nComments:\nI'm not sure about the high level motivation for developing networks of this kind. In particular, none of the performance numbers are near state of the art (not necessary for developing new promising methods!) but I don't know exactly what the initialization buys in this setting.\n\nAlso, it would have been nice to see whether the Win-Win initialization is only useful for larger width networks compared to smaller width i.e. do other initialization schemes work better in this latter setting?\n\nThe derivations are interesting though, so I still recommend accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "a weight initialization approach to enable infinitely deep and infinite-width networks, experimental results on small datasets",
            "review": "Pros:\n\nThis paper uses kernel mappings between any two layers for weight initialisation. Using the representer theorem, a proper distribution for weights is constructed in H_{k_i} instead of being learned by \\phi_i, and then is formulated as a GP. \n\nCons:\n\nHowever, there are some key issues.\n1. The so-called “infinite width” is just yielded by kernels in RKHS for weight initialization. For practical implementation, the authors use this scheme with random Fourier features to construct finite-width network. A key issue is that how to guarantee that the approximated weights are still in the same space? For example, weights can be in RKHS, but their approximation might be not in RKHS. See in [S1] for details.\n \n[S1] Generalization Properties of Learning with Random Features, NIPS 2017.\n \n2. Experimental part is not very convincing. First, the authors just compare different initialization schemes. The used architectures are simple and not representative. Second, the overall performance is not satisfactory, and the compared classification datasets are quite small. Overall, the experimental results are inadequate and unconvincing.\n\nSummary:\nThe paper attempts to proposal a weight initialization scheme to enable infinite deep infinite-width networks. However, there are some key issues not address such as whether the approximated weights are still in the same space and the limited experimental results.\n\nResponse to rebuttal:\nThe authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. \n\nTherefore the paper is very borderline. However, I would like to bump my rating a bit higher.\n\n \n \n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}