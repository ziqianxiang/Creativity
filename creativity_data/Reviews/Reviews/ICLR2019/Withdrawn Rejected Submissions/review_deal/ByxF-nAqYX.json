{
    "Decision": {
        "metareview": "This  paper presents an LLE-based unsupervised feature selection approach. While one of the reviewers has acknowledged that the paper is well-written with clear mathematical explanations of the key ideas, it also lacks a sufficiently strong theoretical foundation as the authors have acknowledged in their responses; as well as novelty in its tight connection to LLE. When theoretical backbone is weak, the role of empirical results is paramount, but the paper is not convincing in that regard.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Unconvincing novelty and empirical results"
    },
    "Reviews": [
        {
            "title": "Locally Linear Unsupervised Feature Selection",
            "review": "This paper focuses on the problem of unsupervised feature selection, and proposes a method by exploring the locally linear embedding. Experiments are conducted to show the performance of the proposed locally linear unsupervised feature selection method. There are some concerns to be addressed.\n\nFirst, the novelty and motivation of this paper is not clear. This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data. Why uses LLE rather than other methods such as LE? What are the advantages?\n\nSecond, in Section 3.3, authors state that the method might be biased due to the redundancy of the initial features. To my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of f Li et al. (2012) \"Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control\". \n\nThird, how about the computational complexity of the proposed method? It is better to analyze it theoretically and empirically.\n\nFinally, the equation above Eq. 8 may be wrong.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Locally Linear Unsupervised Feature Selection",
            "review": "In this paper, the authors presented Locally Linear Unsupervised Feature Selection (LLUFS), where a dimensionality reduction is first performed to extract data patterns, which are used to evaluate compliance of features to the patterns, applying the idea of Locally Linear Embedding.\n\n1. This work basically assumes that the dataset is (well) clustered. This might be true for most real world dataset, but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. For example, if most data points are concentrated on a particular area not being well clustered, how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.\n\n2. For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD? Theoretical and experimental comparison should be interesting and useful.\n\n3. This paper is well-written, clearly explaining the idea mathematically. It is also good to mention limitation and future direction of this work. It is also good to cover a corner case (XOR problem) in details.\n\n4. Minor comments:\n - Bold face is recommended for vectors and matrices. For instance, 1 = [1, 1, ..., 1]^T, where we usually denote the left-hand 1 in bold-face.\n - It seems x_j is missing in Johnson-Lindenstrauss Lemma formula. As it is, \\sum_j W_{i,j} is subject to be 1, so the formula does not make sense.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The paper lacks a solid motivation",
            "review": "Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.\n\nComments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.\n\nSecond, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?\n\nFinally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?\n\nMinor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?\n\nOverall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.\n\n[1] Venna et al. \"Information retrieval perspective to nonlinear dimensionality reduction for data visualization.\" Journal of Machine Learning Research 11, no. Feb (2010): 451-490.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}