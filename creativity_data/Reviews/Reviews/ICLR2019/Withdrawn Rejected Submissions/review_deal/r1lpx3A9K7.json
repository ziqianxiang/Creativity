{
    "Decision": {
        "metareview": "The reviewers agree the paper is not ready for publication. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary:\nThis paper gives a novel adversarial defense that consists of denoising images before classification. The denoising procedure consists of passing an image through a bidirectional GAN, which the authors use to map inputs to the latent space and then back to the original input space. \n\nNovelty:\nThe exact mechanism through which this paper operates is novel, but many similar defenses have been proposed before that involve a latent space mapping followed by a mapping back to the original space; examples include DefenseGAN and PixelDefend. \n\nConcerns:\n- The evaluation is not thorough enough: Only two attacks are considered (FGSM and PGD, with the former being strictly weaker than the latter)\n- DefenseGAN is similar in defense mechanism but the authors do not attempt to use the attacks of Athalye et al 2018 (ICML 2018) in their evaluation. We thus do not have strong lower bounds on adversarial robustness.\n- In Figure 5b, the attack FGSM performs better than PGD, but FGSM is the single step case of PGD. This indicates that the attacks were not tuned properly, as you should always have PGD as a stronger attacker than FGSM\n- The method does not perform as well as adversarial training in standard defense tasks\n- Several writing/clarity errors (detailed below)\n\nSmaller edits:\nPage 2: paragraph 2: second last line: \"feed\" instead of \"fed\"\nPage 2: bullet 1: under our contribution: line 3: \"which are unchanged\" instead of \"which is unchanged\"\nPage 3: paragraph 3: second last line: \"two distribution\" missing an s (plural)\nPage 3: Section 2.2: paragraph 2: line 2: \"here are two most famous attacks\" missing \"the\" before \"two most famous\"\nPage 4: Section 3.2: first paragraph: line 4: \"the latent codes is decomposed\" should be \"are\" instead of \"is\"\nPage 5: Paragraph 1: line 9: \"E are trained\" should be \"E is trained\"\nPage 5: Section 4: Paragraph 1: last line: \"are those have access \" should be \"are those which have access\" missing which/that\nPage 6: Last paragraph: Line 1: \"the attacker can only access to the classifier\" there is no need for \"to\"\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty and evidence is not yet sufficiently clarified",
            "review": "This work proposes to defend against adversarial examples by “denoising” the input image through an autoencoder (a BiGAN trained similar to InfoGAN) before classifying it with a standard CNN. The robustness of the model is evaluated on the L_infinity metric against FGSM and PGD.\n\nMy main criticism is as follows:\n* Novelty: several defences are based on a similar principle and the contributions of this paper are unclear.\n* Insufficient evidence: The evaluation is minimal (only FGSM and PGD, no decision-, transfer- or score-based attacks) and insufficient to support the claims.\n* Gradient masking: There is at least one clear sign of gradient masking in the results (FGSM performing better than PGD).\n\n### Novelty\nThe only prior work against which the paper compares is DefenseGAN. The only advantage over DefenseGAN being stated is performance (because no intermediate optimisation step is used). However, besides DefenseGAN there are several other defences that project the input onto the learned manifold of “natural” inputs, including (see prior work section in [1] for an up-to-date list):\n\n* Adversarial Perturbation Elimination GAN\n* Robust Manifold Defense\n* PixelDefend (autoregressive probabilistic model)\n* MagNets\n\n### Insufficient evidence\nThe only attacks employed are two gradient-based techniques (FGSM and PGD). It is known that gradient-based techniques may suffer from gradient-masking (see also next point) and that the effectiveness of different attacks various greatly (which is why one should use many different attacks). Hence, a full evaluation of the model should include score-based and decision-based attacks.\n\n### Gradient masking\nIn Figure 5 (b) the FGSM attack performs better than PGD for epsilon = 0.05 (66.4% vs 71.5%). PGD, however, should be strictly more powerful than FGSM if the gradients and the hyperparameters are ok.\n\nGradient masking is the primary reason for why 95% of all proposed defences turned out to be ineffective, and there are good reasons to believe that the same might affect this defence. The robustness evaluation has to be much more thorough and convincing before any substantiated claims about the bidirectional architecture proposed here can be derived. In addition, the difference to prior work has to be made much clearer.\n\n[1] Schott et al. “Towards the first adversarially robust neural network model on MNIST”",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Performs worse than adversarial training",
            "review": "This paper presents a new adversarial defense based on \"cleaning\" images using a round trip through a bidirectional gan.  Specifically, an image is cleaned by mapping it to latent space and back to image space using a bidirectional gan.  To encourage the bidirectional gan to focus on the semantic properties, and ignore the noise, the gan is trained to maximize the mutual information between z and x, similar to the info gan.\n\nPros:\n\t1. The paper presents a novel (as far as I am aware) way to defend against adversarial attacks by cleaning images using a round trip in a bidirectional gan\n\nCons:\n\t1. The method performs significantly worse than existing techniques, specifically adversarial training.\n\t\ta. The authors argue \"Although better than FBGAN, adversarial training has its limitation: if the attack method is harder than the one used in training(PGD is harder than FGSM), or the perturbation is larger, then the defense may totally fail. FBGAN is effective and consistent for any given classifier, regardless of the attack method or perturbation.\"\n\t\tb. I do not buy their argument, however, because one can simply apply the strongest defense (PGD 0.3 in their results) and this outperforms their method in *all* attack scenarios.  And if someone comes out with a new stronger attack there's no guarantee their method will be strong defense against that method\n\t2. The paper is not written that well.  Even though the technique itself is very simple, I was unable to understand it from the introduction, and didn't really understand what they were doing until I reached the 4th page of the paper. \n\t\n\nMissing citation:\nPixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples  (ICLR 2018)\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}