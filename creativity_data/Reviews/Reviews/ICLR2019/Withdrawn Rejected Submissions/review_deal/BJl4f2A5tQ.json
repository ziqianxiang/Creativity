{
    "Decision": {
        "metareview": "The paper addresses questions on the relationship between model-free and model-based reinforcement learning, in particular focusing on planning using learned generative models. The proposed approach, GATS, uses learned generative models for rollouts in MCTS, and provide theoretical insights that show a favorable bias-variance tradeoff. Despite this theoretical advantage, and high-quality models, the proposed approach fails to perform well empirically. This surprising negative results motivates the paper and providing insights on it is the main contribution.\n\nBased on the initial submitted version, the reviewers positively emphasized the need to understand and publish important negative results. All reviewers and the AC appreciate the import role that such a contribution can bring to the research community. Reviewers also note the careful discussion of modeling choices for the generative models. \n\nThe reviewers also noted several potential weaknesses. Central were the need to better motivate and investigate the hypothesis proposed to explain the negative results. Several avenues towards a better understanding were proposed, and many of these were picked up by the authors in the revision and rebuttal. A novel toy domain \"goldfish and gold bucket\" was introduced for empirical analysis, and experiments there show that GATS can outperform DQN when a longer planning horizon is used. \n\nThe introduced toy domain provides additional insights into the relationship between planning horizon and GATS / MCTS performance. However, it does not address key questions around why the negative result is maintained. The authors hypothesize that the Q-value is less accurate in the GATS setting - this is something that can be empirically evaluated, but specific evidence for this hypothesis is not clearly shown. Other forms of analysis that could shed further light on why the specific negative result occurs could be to inspect model errors. For example, if generated frames are sorted by the magnitude of prediction errors - what are the largest mistakes? Could these cause learning performance to deteriorate?\n\nThe reviewers also raised several issues around the theoretical analysis, clarity (especially of captions) and structure - these were largely addressed by the revision. The concern that most strongly affected the final evaluation is the limited insight (and evidence) of the factors that influence performance of the proposed approach. Due to this, the consensus is to not accept the paper for publication at ICLR at this stage.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A valuable direction, needs more systematic analysis into possible causes of negative results "
    },
    "Reviews": [
        {
            "title": "Interesting paper with some missing depths in the analysis of the negative results ",
            "review": "The submitted paper proposes GATS, a RL model combining model-free and model-bases reinforcement learning. Estimated models of rewards and dynamics are used to perform rollouts in MTCS without actually interacting with the true environment. The authors theoretically and empirically evaluate their approach for low depths rollouts. Empirically they show improved sample complexity on the Atari game Pong.\n\nI think publishing negative research results is very important and should be done more often if we can learn from those results. But that is an aspect I feel this paper falls short at. I understand that the authors performed a great deal of effort trying to tune their model and evaluated many possible design choices, but they do not a provide a thorough investigation of the causes which make GATS \"fail\". I suggest that the authors try to understand the problems of MCTS with inaccurate models better with synthetic examples first. This could give insights into what the main sources of the problem are and how they might be circumvented. This would make the negative results much more insightful to the reader as each source of error is fully understood (e.g., beyond an error reate for predicting rewards which does not tell us about the distribution of errors which for example could have a big effect on the author's observations).\n\nAnother issue that needs further investigation is the author's \"hypothesis on negative results\". It would be great to experimentally underline the author's arguments. It is not trivial (at least to me) to fully see the \"expected\" interaction of learning dynamics and depths of rollouts. While MCTS should be optimal with any depths of rollouts given the true Q-function, the learning process seems more difficult to understand.\n\nI would also like the authors to clarify one aspect of their theoretical analysis. e_Q is defined as the error in the Q-estimate for any single state and action in the main text. This appears to be inconsistent with the proof in the appendix, making the bound miss a factor which is exponential in H (all possible x_H that can be reached within H steps). This would change the properties of the bound quite significantly. Maybe I missed something, so please clarify.\n\nOriginality mainly comes from the use of GANs in MCTS and the theoretical analysis.\n\nStrengths:\n- Interesting research at the intersection of model-free and model-based research\n- Lots of effort went into properly evaluating a wide range of possible design choices\n- Mainly well written\n\nWeaknesses:\n- Missing depths in providing a deep understanding of why the author's expectations and empirical findings are inconsistent\n- The authors use many tweaks and ideas to tune each component of their model making it difficult to draw conclusions about the exact contribution of each of these\n- Error in the theoretical analysis (?)\n\nMinor comment:\n* The paper would benefit from improved and more self-contained figure captions.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel idea, but requires more work.",
            "review": "This paper presents Generative Adversarial Tree Search (GATS) that simulates trajectories for the Monte-Carlo Tree Search up to a certain depth. The GATS functions as a model that can simulate the dynamics of the system and predict rewards, effectively removing the need for a real simulator in the MCTS search.\n\nThey prove some favourable theoretical properties regarding the bias variance trade-off.  Specifically, they propose the model based model free trade-off which illustrates the trade-off between selecting longer rollouts, which increases the upper performance bound, and getting a more accurate Q estimate, which decreases the upper performance bound.\n\nThey also propose a pseudo count exploration bonus based on the inverse Wasserstein metric as the exploration strategy for GATS.\n \nThey observe that when tree-search rollouts are short, GATS fails to outperform DQN on 4 different games.\n\nQuality:\nIt is unclear to me how you arrive at the result in Equation (9) of Appendix D. You have assumed in the second equation that the optimal action max_a Q(s,a) and max_a \\hat{Q}(s,a) are the same action a. How do you arrive at this conclusion? Since \\hat{Q} is an approximation of Q, why would the action a be the same?\n\nClarity:\nThe paper is fairly well written. There are many grammatical mistakes, but the overall message is more or less conveyed.\n\nOriginality:\nIt is original in the sense that a generative adversarial network is used as the model for doing the tree search. It is disappointing that this model does not yield better performance than the baseline and the theoretical results are questionable. I would like the authors to specifically address the theory in the rebuttal. \n\nSignificance:\n\nWhile I appreciate negative results and there should be more papers like this,  I do think that this paper falls short in a couple of areas that I think the authors need to address. (1) As mentioned in quality, it is unclear to me that the theoretical derivation is correct. (2) The exploration bonus based on the inverse Wasserstein metric would add much value to the paper if it had an accompanying regret analysis (similar to UCB, for example, but adapted to the sequential MDP setting). \n\nIt appears in your transfer experiments that you do indeed train the GDM faster to adapt to the model dynamics, but it doesn’t appear to help your GATS algorithm actually converge to a good level of performance. Perhaps this paper should be re-written as a paper that focuses specifically on learning models that can easily transfer between domains with low sample complexity?\n\nFor the exploration bonus: If the authors added a regret analysis of the exploration count and can derive a bound of the number of times a sub-optimal action is chosen, then this could definitely strengthen the paper. This analysis could provide theoretical grounding and understanding for why their new exploration account makes sense, rather than basing it on empirical findings.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The idea of the paper is interesting and it is valuable to share negative results but it would be beneficial if the paper would focus more on hypothesis evaluation in a more constraint environment.",
            "review": "This paper proposes to learn a dynamics model (state to pixels using Generative Adversarial Networks), use this model in conjunction with Monte Carlo Tree Search, model-free reinforcement learning (Q-learning) and a reward prediction network essentially combining model-free with model-based learning. The proposed approach is empirically evaluated on a small subset of the Atari games and theoretical analysis for the bias-variance tradeoff are presented.\n\nIt is highly appreciated that this paper presents an idea and discusses why the proposed approach does not result in high performance. This is very valuable and useful for the community. On a high level it would be very useful if Figure 1 would be show less examples but present them much larger since it is almost impossible to see anything in a printout. Further, the caption does not lend itself to understand the figure. Similarly Figure 2 would benefit from a better caption. \n\nThe first part of the discussion (7), the individual building blocks, should be mentioned much earlier in the paper. It would be further useful to add more related work on that abstraction level. This would help to communicate the main contribution of this paper very precisely.\n\nOn the discussion of negative results: It is very interesting that Dyna-Q does not improve the performance and the hypothesis for why this is the case seems reasonable. Yet, it would be very useful to actually perform an experiment in a better controlled environment for which e.g. the dynamics model is based on the oracle and assess the empirical effect of different MCTS horizons and rollout estimates. Further, this scenario would allow to further quantify the importance and the required “quality” of the different learning blocks.\n\nIn its current form the paper has theoretical contributions and experimental results which cannot be presented in the main paper due to space constraints. Albeit the appendix is already very extensive it would be very useful to structure it into the theoretical derivation and then one section per experiment with even more detail on the different aspects of the experiment. The story of the main paper would benefit from referencing the negative results more briefly and better analyzing the different hypothesis on toy like examples. Further, the introduction could be condensed in order to allow for more in detail explanations and discussions without repetition later on.\n\nAs argued in the paper it is clear that image generation is a very expensive simulation mechanism which for games like pong which depend on accurate modeling of small aspects of the image are in itself difficult. Therefore, again although really appreciated, the negative results should be summarized in the main paper and the hypothesis concluded better analyzed. The extensive discussion of hyper parameters and approaches for individual components could be in the appendix and the main paper focuses on the hypothesis analysis.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}