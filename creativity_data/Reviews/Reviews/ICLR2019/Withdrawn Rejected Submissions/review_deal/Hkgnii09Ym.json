{
    "Decision": {
        "metareview": "This paper introduces set transformer for set inputs. The idea is built upon the transformer and introduces the attention mechanism. Major concerns on novelty were raised by the reviewers. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Novelty is limited."
    },
    "Reviews": [
        {
            "title": "A good paper but need some clarifications and improvements",
            "review": "This paper presented an attention-based neural network, namely set transformer, a new neural model \nbased on original transformer designed for set inputs. The basic idea is to introduce the attention\nmechanism in both learning the feature embeddings of the set inputs during “encoding” and aggregating \nthese embeddings during “decoding”. The paper is written clearly and well motivated. The extensive \nset of experiments were conducted to demonstrate the effectiveness of the proposed method. In general, \nI like reading this paper but there are some limitations or unclear parts I need authors to clarify\nand explain. \n\ni) The proposed architecture is mainly adopted from the original transformer but it is highly related\nto the baselines used in the experiments. For instance, it seems like that the current set \ntransformer is a simple combination of Yang et al.(2018) and Mishra et al.(2018) (using Stack of\nSABs) in encoder side and of Ilse et al.(2018) (using PMA and stack of SABs) in the decoder side. \nThis simple combination makes the novelty of this paper unclear. I would like authors to clarify \nmore on the originality w.r.t. these previous works. \n\nii) Although authors proposed a variant of SABs - ISABs using landmark points to accelerate the \ncomputation, there are no any runtime comparisons between SABs and ISABs by fixing other components. \nIt would be interesting to see that ISABs can approach the performance SABs and how it approaches it. \nFor instance, shall we expect that ISABs approach the performance of SABs when increasing the number\nof landmark points (inducing points)? Since in practice most of datasets are relatedly large, I think\nunderstanding the behavior of ISABs is a more interesting problem. \n\niii) After seeing the results in table 6, I have quite concerned about the practical performance of\nset transformer on relatively large datasets (like 1000 points each class in the settings.) It looks\nto me that not only set transformer may have computational issues to scale up, but more importantly\nthat when encoder learned really expressive embeddings with a relatively large number of the set \ninputs it might be little need to leverage attention in pooling anymore. I would like authors to \nconduct some other experiments on relatively large datasets to verify this hypothesis, which is \nimportant for the practical applications of the proposed model. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper that uses attention for set inputs but needs more ablation study",
            "review": "The paper proposes several variants of attention-based algorithms for set inputs. Compared with previous approach that processes each instance separately and then pooling, the proposed algorithm models the interactions among the instances within the set and performs better on tasks where such properties are important.\n\nThe experiments seem promising. The paper compares SAB and ISAB to rFF + pooling over multiple different tasks and SAB and ISAB outperform rFF + pooling in many tasks.\n\nOne drawback of the paper which limits its significance is that there are seemingly too many components and it is not clear which components are most important and which are not unnecessary. The authors can conduct some ablation study by removing some components and compare the performance to understand which parts are essential to the improvements.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Missing comparisons to permutation equivariant DeepSets",
            "review": "This paper looks at stacking attention mechanism for learning over sets.\n\nI think that the paper is well written overall. The architecture put forth is a fairly straightforward implementation of attention. Thus the methodological contribution is incremental. Still, it is nice to see some implementation of an attention model be considered for permutation invariant set embeddings.\n\nHowever, there are some core misrepresentations and omissions that make publication difficult. The main problem is that the paper completely ignores the permutation equivariant mappings discussed in DeepSets (Zaheer 2017). See (4) and (23) of https://arxiv.org/pdf/1703.06114.pdf: \"Since composition of permutation equivariant functions is also permutation equivariant, we can build deep models by stacking layers.\"\nIn practice this is often done by mapping points x_i in a set as x_i -> \\phi(x_i) - max_j \\phi(x_j). Stacking this layer works surprisingly well, typically better than just with a single pool. Thus, the permutation equivalent mappings of Zaheer 2017, which do have higher-order interactions and are linear in the number of points, are a glaring omission of table 1 and all of the experiments. Furthermore, the omission leads to a misrepresentation of the work.\n\nAnother unfortunate omission is previous work that considers set and distribution data through kernels and other nonparametric methods such as: \nMuandet, Krikamol, et al. \"Learning from distributions via support measure machines.\" Advances in neural information processing systems. 2012.\nOliva, Junier, Barnabás Póczos, and Jeff Schneider. \"Distribution to distribution regression.\" International Conference on Machine Learning. 2013.\n\nIt is also odd that the paper compared to DeepSets on modelnet with 100 and 1000 points but not with 5000 points. Will there be code available?\n\nWithout a better description of and comparison to permutation equivariant mappings I would feel hesitant to recommend publication.\n\nedit:\nIn light of the revised experiments and inclusion of permutation equivariant deepset layers, I'm inclined to recommend publication. However, if I could nitpick further, I think it would be nice to make some edit (or addition) to Table 1 to include permutation equivariant deepsets. Moreover, it would be nice to have some additional description of permutation equivariant layers in Section 2.1.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}