{
    "Decision": {
        "metareview": "The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "review",
            "review": "This paper studies the problem of compactly represent the model of a complex dynamic system while preserving information. The method is based on the information bottleneck method. Basically, for a dynamic system whose states changing from X_{k-1}, X_k to X_{k+1}, the \"information bottleneck hierarchy\" method learns a variable B_k and B_{k+1} such that B_k predicts B_{k+1} well, B_k predicts X_k well, and B_{k+1} predicts X_{k+1} well, while minimizing the information of X_{k-1} contained in B_k. \n\nIn my opinion, this is a very interesting framework for representing and learning a dynamic system. The paper then considers simple examples on a linear model with Gaussian noise and show that the IBH method performs better than the one-BN method. The simulation and the experiments on real data all show very good performance (even with the simple linear Gaussian estimator).\n\nThe reason that I give such a rating is that of the confusing writing.\n* In the abstract, it is unclear what the goal is. For example, the second and third sentence do not explain the first sentence: \"the task is a crucial task\".\n* Introduction is also very confusing. It seems there is not a good logic connecting each sentence.\n* The paper does not give a good survey of other methods performing similar tasks, e.g., the ones the paper are comparing to in the experiment section. Therefore, it is hard to compare or to understand why the previous methods are worse.\n* Figure 2: one-BN is not well defined. How do you design the IB locally? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Preliminary, with 1 promising experiment, but unclear and vague",
            "review": "The paper proposes a method to learn the conditional distribution of a random variable in order to minimize and maximize certain mutual information terms.  Interestingly, the proposed method can be applied to sentiment prediction and outperforms a 2018 method based on SVM.\n\nOverall, the ideas seem intriguing, and the results seem promising, but I really cannot understand what the paper is saying, and I think the paper would be much stronger if it was written more clearly (to make individual sentences more clear, but also to make the broader picture more clear). Not only is the writing hard to understand (some sentences lack a verb!), but it is vague, and the notion of a \"complex system\" is never defined.  It seems that the technique can be applied to any (potentially non-stationary) Markov process?\n\nAdditionally, due to the lack of clarity in the writing and lack of mathematical rigor, Theorem 1 does not seem to be true as stated. I think this is an issue of stating the assumptions, and not due to a mistake in the derivation.  Right now, the actual conclusion of theorem 1 is not even clear to me.\n\nQuality: poor/unclear\nClarity: very poor\nOriginality: unclear, perhaps high? Not clear how related it is to the methods of Tishby et al.\nSignificance: unclear, as clarity was poor, and there was minimal discussion of alternative methods.\n\nSpecific points:\n\n- Eq (2), the first term is included because it is for the \"information compression task\", but I do not understand that. Where is the actual compression?  This is not traditional compression (turning a large vector into a smaller vector), but more like turning one PDF into a PDF with lower entropy?\n\n- This paper seems to fall into the subfield of system identification (at which I am not an expert), so I'd expect to see some related literature in the field. The only compared method was the IF method of Tishby et al. from 18 years ago (and the current work seems to be a generalization of that).\n\n- Equation (4): what exactly is the object being minimized? Is it a PDF/probability measure? Is it an *instance* of a random variable?  If it is a PDF, is it the PDF of B_k | X_{k-1} ?\n\n- The statement of Theorem 1 is either too vague or wrong. To say \"The solution... is given by\" makes it sound like you are giving equations that define a unique solution. Perhaps you mean, \"Any solution ... must necessarily satisfy...\" ? And that is not clearly true without more work. You are basically saying that any minimizer must be a stationary point of the objective (since you are not assuming convexity). It seems everything is differentiable?  How do you know solutions even exist -- what if it is unbounded? In that case, these are not necessary conditions.\n\n- Lemma 1: \"The iterative procedure... is convergent.\"  The iterative procedure was never defined, so I don't even know what to make of this.\n\n- Section 3.2: \"As proved by prior work, the optimum solution obtained by a stochastic transformation that is jointly Gaussian with bottleneck's input.\"  I do not know what you are trying to say here. There's no predicate.\n\n- Section 4 wasn't that interesting to me yet, since it was abstract and it seemed possible that you make a model to fit your framework well. But section 5 is much better, since you apply it to a real problem. However, what you are actually solving in section 5 is unclear. The entire setup is poorly described, so I am very confused.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": " ICLR 2019 Conference Paper576 AnonReviewer3",
            "review": "This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.\n\nGenerally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.\n\n1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.\n\n2.  About the \\epsilon_{1,2,3} in formula (3). \\epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \\epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the \"Markov chains are considered\" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.\n\n3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \\Psi and \\Delta? Why \\Delta can be used to predict sentiment intensity in your experiment? It seems that \\Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \\Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.\n\nSo I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}