{
    "Decision": {
        "title": "meta-review",
        "metareview": "The reviewers reached a consense on that the paper is not quite ready for publication at ICRL. The main potential drawback include a) the exposition of the paper can be improved; b) it's not entirely clear that some of the assumptions (such as the threshold for the first layer, the polynomial approximation of higher layers) are meaningful , and it seems that the proof technique exploits heavily some of these assumptions and some of the key intermediate steps won't hold in practice. (see reviewer 3's comment for more details.) The authors clarify the writing and intuitions in the response, but overall the AC decided that the paper is not quite ready for publications at the moment.  ",
        "recommendation": "Reject",
        "confidence": "5: The area chair is absolutely certain"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper gives provable recovery guarantees for a class of neural networks which have high-threshold activation in the first layer, followed by a \"well-behaved\" polynomial, under Gaussian input. The algorithm is based on the approach by Ge et al. (2017), as well as an iterative refinement method.\n\nWhile this could be an interesting result, I have several concerns regarding the assumptions, correctness, and writing.\n\n1) It is required that the threshold is at least sqrt{log d} (Thm. 1), where d is the number of hidden neurons in the first layer. It seems that this essentially zeros out almost all the neurons, since the maximum among d Gaussian random variables is roughly sqrt{log d}. The authors should explain what exactly this model is doing, i.e., what kind of functions it can compute, in order to justify why this is an interesting model.\n\nFurthermore, the authors claim that the studied model is a \"deep\" neural network, but I disagree. As I understand, the difference between this model and two-layer networks is that the second layer here is a polynomial instead of a linear function. This doesn't make it a deep network since the (polynomial) part above the first layer is not modeled in a layer-wise fashion, not to mention that under the setting considered in the paper the polynomial behaves similar to a linear function.\n\n2) It is stated at the end of Section 2 that the angle can be reduced by a factor of 1-1/d ***with constant probability***. How does this ensure you can succeed after O(d log(1/nu)) iterations? As far as I see you need the success probability in one iteration to be at least something like 1-1/Omega(d) so that you can apply a union bound.\n\n3) Even if the issues of motivation and correctness are clarified, I find it very difficult to understand the overall intuition and main technical contributions in this paper. The writing needs to be significantly improved to reach the level of a top conference.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper considers the problem of recovering the lowest layer of a deep neural network whose architecture is ReLU or sign function followed by a polynomial. This paper relies on three assumptions: 1) the lowest layer has a high threshold (\\Omleg(\\sqrt{d})), 2) the polynomial has 1/poly(d) lower bouned and O(1) upper bounded linear terms and is monotone 3) the input is Gaussian. Under these assumptions, this paper shows it is possible to learn the lowest layer in precision \\eps in poly(1/eps, d) time.\n\nThe proposed algorithm has two steps. The first step is based on the landscape design approach proposed by Ge et al. (2017) and the second step is based on checking the correlation. \n\nProvably learning a neural network is a major problem in theoretical machine learning. The assumptions made in this paper are fine for me and I think this paper indeed has some new interesting observation. My major concern is the writing. There are several components of the algorithm. However, it is hard to digest the intuition behind each component and how the assumptions are used. I suggest authors providing a high-level and non-technical description of the whole algorithm at the beginning. If authors can significantly improve the writing, I am happy to re-evaluate my comments and increase my rating.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.\n\nIt is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.\n\nThe proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.\n\nOverall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}