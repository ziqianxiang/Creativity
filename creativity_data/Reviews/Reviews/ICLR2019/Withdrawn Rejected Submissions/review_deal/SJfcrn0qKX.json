{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper studies how perturbations in 3D shape/texture may be used to generate adversarial inputs.  Evaluation is done on densenet, inception-v3 nets.   Other aspects covered include - the specification of a 3D smoothing loss to describe degree of perturbations in the mesh, the study of transferability of adversarial meshes  among different rendering schemes , etc.",
            "review": "\nPros:  Studies how perturbations in 3D shape and albedo maps may be used to generate adversarial inputs and evaluates them on densenet and Inception-v3 nets.   \n\nCons:  The paper should be checked for sentence structure, typos, etc. There are a number of places with linguistic errors.  The paper is hard to read.  The claims in the contributions section (in page 2) are not adequately supported by extensive experimentation.  For instance, the section claims that an in-depth analysis of vulnerable regions in 3D meshes is done in the paper and the discussion in the paper is mainly about a case study visualizing the vertex flow in the given mesh region. While the authors try to cover reasonable ground in terms of studying how adversarial data can be generated starting from mesh data,   a systematic in-depth study of each of the factors outlined in the contribution section will strengthen the paper and could offer insights into how the work can be used to improve robustness of classification.  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "designing adversarial examples in 3D meshes",
            "review": "The paper introduces meshAdv, a new adversarial attack method by manipulating shape and texture on 3D meshes. Experimental results suggest that even with hard-to-notice maipulations, the generated 3D mesh can accurately fool image recognition systems. Such generated 3D meshes can also be rendered to natural images and fool popular object detection algorithms.\n  \nPros:\n- The ideas in this paper are novel and interesting. Manipulating 3D meshes is also an alternative for the adversarial attack, and this paper proposed an approach for designing such algorithms.\n- Experimental results in the paper are strong, and experimental settings are easy to understand. There are experiments to show that meshAdv can attack several vision systems for various tasks, and learned representation can transfer to new renderers and settings. The paper also contains intuitive visual results for readers to understand the manipulation flow of 3D meshes.\n\nQuestions/Suggestions:\n- In the experimental settings in \"outdoor scene\" in sec 4.3, I was wondering why estimating the lighting direction is necessary -- after all, the generated object (Stanford bunny) is not photo-realistic anyway, so it might be sufficient to use any arbitrary lighting for the object rendering step.\n- In figure 4, the \"benign\" detection for the Stanford bunny is \"bird\". This is already wrong, and I didn't find any explanations for the case.\n- One of the issues in image recognition is that 3D objects are rendered in various viewing angles, and such ambiguity is what makes vision systems vulnerable.  I think it would be nice if authors can report performances when the rendered object are facing different directions.\n\nCons:\nThe biggest issue is that the overall presentation of the paper is weak, with several noticeable grammar mistakes and obscure sentences that are hard to follow. As a simple example, in the abstract \"So far adversarial examples have been heavily explored for 2D images, while few work has tried to understand the vulnerabilities of 3D objects which exist in real world, where 3D objects are projected to 2D domains by photo taking for different learning (recognition) tasks\" -- ignoring grammar errors, I find it very hard to understand, and this is just the abstract.\n\nThe paper has some interesting points and novelties, and experimental results are strong. However, paper writing seems to be done in a rush, and I wish the description of the approach can be significantly improved. Therefore I'm hesitating to accept the paper for this round of publication. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not really convinced...",
            "review": "This paper describes a method for generating '3D adversarial meshes'. Existing 3D meshes are deformed and their textures changed to 'fool' an object classifier or an object detector. This is done by maximizing an adversarial loss computed on rendering of the 3D mesh, together with regularization terms also applied on the rendering but also on the 3D perturbations, to make sure the modified mesh is still plausible. The rendering is performed with the Neural 3D Mesh Renderer, to make the loss function differentiable. The final mesh is rendered over real images to test the method. The generated images were also tested on humans, which were not 'fooled' by the adversarial meshes, showing that the perturbations are indeed small.\n\nI found the paper difficult to read, and it is difficult to accept this paper for ICLR because of this lack of clarity. (See for example the sentence from the abstract: \"So far...\")\n\nBecause of this lack of clarity, I am not sure I understand completely the paper. Maybe I am wrong, but it seems that the mesh is perturbed to fool the classifier and the detector from only one viewpoint. If this is true, is it really interesting?  What do we learn from it? We already know that existing networks can make mistakes on adversarial images. Isn't the proposed method a complex way to generate new images?  It would probably be more interesting if the generated mesh was fooling the network independently of the viewpoint, however it seems that the viewpoint is constant.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}