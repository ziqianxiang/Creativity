{
    "Decision": "",
    "Reviews": [
        {
            "title": "Somewhat weak contribution",
            "review": "The paper describes a generative model for natural language, proposing various modifications to the previous work, including: a modified encoder which combines a self-attention model and a bag-of-words model, a modified multi-modal prior, and a modified training procedure. The claim is that the proposed improvements alleviate the \"posterior collapse\" issue, where the decoder ignores the information from the encoder.\n\nTechnical significance:\nAs the paper acknowledges, the proposed modifications have already been introduced in the literature and the current paper applies them to the natural language modeling task, showing that: combining self-attention and bag-of-word encoders, using an additional training task, and using a multi-modal prior improve performance (e.g. as measured by BLUE scores). From this perspective, the contribution is somewhat limited.\n\nEmpirical significance:\nThe main focus is avoiding \"posterior collapse\" and, unfortunately, this point is not illustrated strongly. Do the baselines show mode collapse and the work improves is that respect? Something is missing here: maybe a (toy) data-set where mode-collapse arises when training baselines, but not when the present modifications are introduced (as another comment points out), maybe training curves showing that the modifications provide a mode stable training process, etc.\n\nClarity:\nThe paper is generally well written, with the proposed modifications being clearly explained. The non-VAE sequence prediction task could be described in a bit more details where it's being used, i.e. in section 4.2. or in the caption of Table 1.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Issues in Theory and Experiments",
            "review": "This paper suggests that one way to prevent VAE from ignoring the encoder when trained on text is to make the prior distribution multi-modal. The author further suggests that injecting the encoder's output with BoW features can make the prior distribution multi-modal, and in turn that can help VAE to learn better distribution on text without ignoring the encoder, tested on a self-attention encoder and an LSTM encoder.\n\nThere are several issues with the theory and the experiments that make the paper incorrect on several fronts.\n\nIssue #1 (theoretical): multi-modality of prior distribution can be achieved by injecting the BoW feature.\nThis is incorrect in the sense that the prior distribution Pr(z) is a given distribution for the model. At best, it is an additive shift of the feature vector from the encoder, which parameterizes the mean and standard deviation of Pr(z|x) by assuming it is a parameterized Gaussian, which is still uni-modal. Besides, the experimental setting in section 3.1.1 suggests the same re-parameterization trick from Kingma & Welling (2013) is used, which assumes the prior distribution Pr(z) is N(0,1) -- and it is uni-modal!\n\nIssue #2 (theoretical): prior multi-modality implies that posterior distribution cannot ignore the encoder.\nThis issue is a two-fold problem. 1) In Appendix A, the paper suggests that (assuming p(z) is multi-modal) \"if q(z|x) is a uni-modal distribution, there is no way to satisfy p(z) = q(z|x)\". This is true, however this does not imply that posterior distribution will not ignore the encoder. In the case that Pr(z) is multi-modal, it is entirely possible that the encoder simply performs a clustering on the input x for each mode of z, ignoring the detailed textual information. The posterior collapse is therefore could not be entirely prevented by simply making Pr(z) multi-modal. 2) In Appendix A, the paper suggests \"a hypothesis that the modifiction of the decoer is not necessary if multi-modal prior distribution is used\". This is not only a logical fallacy of circular reasoning between theory and experiments, but also a faulty result because the actual experiments use a uni-modal prior (see issue #1).\n\nIssue #3 (experimental): benchmark methods only reflect the quality of auto-encoding reconstruction\nThe paper's benchmark methods do not reflect performance in text generation. According to section 4.3, the 3 benchmark methods used (reconstruction loss, KL-divergence, and BLEU) are all performed against a given text, which can only benchmark the auto-encoding reconstruction Pr[x|z]Pr[z|x] but not the quality of generated text Pr[x|z]. Here I am assuming the KL-divergence is performed on the output word probabilities given a sample, not on the entire generated sentence (which is not feasible to compute). This defies the purpose of VAE, which is to learn a generative model.\n\nIssue #4 (experimental): the title suggests no weakening of the decoder is needed, but experiments are not performed with decoders of varying sizes\nDespite all the issues above with incorrect theory and experiments, it could still be possible that feature injection with BoW is helpful to prevent posterior collapsing of text VAEs. The paper failed to show this in a convincing manner either, because there is no experiment on different capacities of the decoder. One would expect that increasing the size (capacity) of the decoder will gradually make the posterior collapsing problem worse, and BoW feature injection could help because it can tolerate a larger decoder than a baseline model.\n\nDue to these issues, I recommend rejection for the paper. If the authors did observe that BoW feature injection is helpful for the posterior collapsing phenomenon, I encourage the authors to re-consider the theory, the benchmark methods, and perform ablation study as suggested in issue #4.\n",
            "rating": "1: Trivial or wrong",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper focuses on applying variational autoencoders to text data. A well-known problem in this domain is posterior collapse, where the posterior \"collapses\" to an uninformative prior distribution and the decoder becomes a standard language model which does not utilize the latent code. This paper assembles a few tricks to improve VAEs for text, including using a self-attention-based encoder, a mixture-of-Gaussians prior on the latent code, and a \"multi-task\" loss which attempts to reconstruct a BoW representation in addition to the original sentence. Some experiments are carried out on the Yelp dataset, including some qualitative study.\n\nThe issue of posterior collapse is important and modeling the global structure of text with a latent variable is a valuable endeavor. This paper provides some promising results in that direction. My main issue with the paper is that it is primarily the combination of a few pre-existing tricks to the text-VAE problem. Bringing together multiple tricks and showing that they can enable significant progress is potentially valuable, but only if the results are obviously better or they represent a big step forward in an important application (for example, showing convincing sentence interpolations, attribute vector arithmetic, autoencoding of very long text sequences, etc). This paper does not demonstrate any sufficiently strong result; instead, it just provides some limited evaluation that things get a little better along with some basic qualitative analysis of potential benefits of using a mixture prior. Apart from this, the paper claims that combining these tricks avoids \"weakening the decoder\" but uses a decoder with word dropout in all experiments, and also claims they decrease the number of hyperparameters, which is not shown in any rigorous way. The proposed additions also are described in insufficient detail - how are the bag of word representations constructed? How are they predicted? What is the additional loss term? How is it combined to the standard reconstruction loss for the autoregressive decoder? Etc. Finally, it is missing discussion (and comparison, as baselines) to a great deal of prior work on avoiding posterior collapse. As-is, the paper is not strong enough for publication. Below, I go into a bit more detail about specific issues and suggestions.\n\nSpecific comments:\n\n- The paper blames posterior collapse on teacher forcing (\"Because decoders for textual VAEs are trained with “teacher forcing” (Williams & Zipser, 1989), they can be trained to some extent without relying on latent variables.\") This is a misleading claim. Even with teacher forcing, it could potentially be advantageous to the model to utilize the latent (recall that the latent can capture information about the entire input sequence, so p(x_t | x_1, ... x_{t - 1}) is less informative than p(x_t | x_1, ... x_{t - 1})), but because of the architecture/learning dynamics/hyperparameters/etc the model tends to choose to \"ignore\" z. This is an independent problem of teacher forcing. The main issue caused by teacher forcing is exposure bias, which is a related but different issue. Teacher forcing is not necessary for training latent variable models with autoregressive decoders, it just happens to work well because it's a remarkably good search strategy.\n- \"existing models use a LSTM as the encoder, it is known that this simple model is not sufficient for text generation tasks (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017).\" I assume you are citing these papers because they deal with attention and show that attention improves sequence-to-sequence models. The fact that attention helps is because the decoder is provided with additional conditioning information, not because the encoder is different (in Bahdanau and Luong the decoder remains a simple LSTM).\n- There are many missing related works (and baselines) for avoiding posterior collapse, such as \"Avoiding Latent Variable Collapse with Generative Skip Models\", \"Adversarially Regularized Autoencoders\", \"Variational Lossy Autoencoder\", etc.\n- Your equation for p(x|z) at the beginning of 3.1.1 is a bit odd because you are talking about a deterministic z, and using it to point out that z might not learn good global features. However, when z is a stochastic latent variable the equation would be exactly the same. Writing it in this way does not make this distinction clear. A deterministic z can indeed capture interpretable global structure (see Figure 2 of Sutskever et al. 2014). If you want to claim that using a stochastic z may _better_ capture global structure, I would recommend making this argument from an information theoretic perspective (as was done in \"Variaional Lossy Autoencoder\", for example).\n- That weaking the decoder \"requires additional hyper-parameters specifying decoder capacity\" is not really a compelling arugment in your paper since your approach also requires additional hyperparameters (number of mixture components, coefficients for additional loss terms, etc.)\n- You call posterior collapse a \"trivial local minimum\". Are you sure it's a local minimum? This implies that the gradient of the loss w.r.t. every one of the parameters is zero at this point. I think you are using \"local minimum\" in a loose sense; please don't do this.\n- What are pseudo-inputs? Do you basically mean they are model parameters which are updated w.r.t. the loss function?\n- \"Recent research into text generation has found that simple LSTMs do not have enough capacity to encode information from the whole text.\" Citation needed!\n- I think what you are calling \"self-attention\" is actually the \"feed-forward attention mechanism\" from \"Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems\".\n- \"We simply summarize word representation of all words in the input text and project this vector with a linear layer.\" I don't think this adequately describes what you are doing. How is this summarization done? Via a histogram of the words which are present? An average of the words' embedding vectors? Same with the multi-task part. What exactly are you predicting? How?\n- I presume your BLEU scores in Table 1 are reflecting the BLEU score for the input vs. the reconstructed output. Please clarify.\n- In the appendix it is written that \"We applied 0.4 word dropout for input text to the decoder for our model and the model from Bowman et al. (2015).\" This is definitely \"weakening the decoder\" - that is the primary motivation for word dropout. I don't think you can claim your approach can avoid weakening the decoder if all of your results are with word dropout.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}