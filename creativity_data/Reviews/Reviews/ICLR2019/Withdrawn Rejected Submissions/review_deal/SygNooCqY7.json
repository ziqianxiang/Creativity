{
    "Decision": {
        "title": "The paper was withdrawn",
        "metareview": "The paper was withdrawn",
        "recommendation": "Reject",
        "confidence": "5: The area chair is absolutely certain"
    },
    "Reviews": [
        {
            "title": "Fundamental questions regarding the premises of this paper seem to be unanswered.",
            "review": "In this work, a new method for (adaptively) choosing the noise distribution in a noise-tempered generative adversarial network is proposed. Noise tempering in training GANs has been around for a while. The main argument in those existing work (and also in this paper) is that GAN training gets stuck when gradient vanishes due to non-intersecting support of the real and generated distributions. Hypothetically, if such phenomenon is happening, then adding noise to the samples ensures that the support overlap and hence gradient vanishing is expected to be prevented. Typical scenarios of adding noise has been studied, for example adding a noise with variance vanishing with training iterations. This ensures that later in the training, if the generator is close to the real data, the error from having noisy samples do not propagate to the generator training. \n\nThis paper starts off from this assumption, asking the question of why we need to reduce the noise. This suggests training with non-vanishing noise. Two heuristic reasonings in support of such approach are (i) in existing approach of vanishing noise, it is not clear how to schedule the reduction of noise variance, leading to inefficiency or non-convergence, and (ii) minimizing noisy divergence can also achieve convergence to the true distribution, as long as the noise is not degenerate. In the latter the non-degeneracy can be achieved by using multiple noise distributions. \n\nThe main contribution of this paper is that a neural network based adaptive noise generator is proposed. During the training, the noise (from a family of noise distributions) that makes the real and generated distributions as similar as possible is chosen, while constrained to having a bounded variance. The only evidence that this improves the training is via numerical experiments run by the authors (with no (anonymized) reproducible implementation references, e.g.~github). The FID and IS scores on several datasets are presented, which suggest that NTGAN achieves better scores. \n\nIn table 3, specific architectural and hyper-parameter choices have been made for competing baselines. For example, for SNGAN, the authors of SNGAN report IS score of 8.22 on CIFAR-10 with ResNET and 7.58 with standard CNN. If the settings are fair, then these improves upon the reported IS of NTGAN. Given that no code is available, how does one interpret the resulting table of scores? How do you ensure fairness in different choices made for different architectures?\n\nThe proposed approach of minimizing the loss over the noise with regularizer for the variance is surprising, as the theory in Lemma 1 seems to suggest an alternative (but more natural) approach. That is maximize the loss over the noise with perhaps regularizer that restricts the choice of no-noise or small variance. Can the authors elaborate on that choice?  \n\nThe metric of IS and FID score is too coarse to measure the subtle improvements that are made by the proposed approach. Given that the paper is motivated by the paths the gradients are taking during the training, it seems that analysis of the gradients (with and without the noise) is in order. Such thorough analyses will settle several myths regarding gradient vanishing, such as does gradient vanishing happen in the training of typical GANs, and does noise adding mitigate it. This is particularly interesting, as if WGAN is used with Wasserstein distance, there should be no gradient vanishing due to non-overlapping support. Checking these conjectures numerically is an important task, and is particularly critical for this paper. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting results, but missing comparison to earlier work and questionable theory",
            "review": "This paper presents a new GAN training objective to stabilize training and prevent mode collapse. The new objective trains the generator and discriminator with multiple scales of fixed additive noise, neural network generated noise, and no noise. They experimentally demonstrate the robustness of their approach to hyperparameters, perform extensive ablation studies, and show improvements in FID and IS over prior approaches on CIFAR-10 and CelebA.\n\nOverall, this was a clear paper that presented an interesting idea of combining multiple scales and types of noise to stabilize GAN training. The experiments and ablations were far more thorough than most GAN papers and helped to show which components of the model were most important. However, I have two major concerns: (1) there is a very related and uncited paper from ICML 18 with a similar name and method: Tempered Adversarial Networks from Sajjadi et al., and (2) there is a gap between the theory and practice, in particular the discriminator should depend on the noise being added and vary for different noise types. Given these two concerns, I cannot recommend acceptance at this time. If the authors can clear up the relationship between their approach and Sajjadi et al., and addresss my theoretical complaint then I could be persuaded to increase my rating.\n\nStrengths:\n+ Interesting idea of combining multiple noise types along with learned noise\n+ Clearly written, includes details of architectures and experiments.\n+ Thorough experiments and ablation studies, including sensitivity to optimizer, types of noise, hyerparaemters, and generator architecture. Compares to recent SN-GAN and GAN-GP modelss.\n\nWeaknesses:\n- Missing prior work: no discussion of Tempered Adversarial Networks from Sajjadi et al. They add a neural network that transforms samples and adjust how large the transformations are over the course of training. Please discuss how your work is related to theirs and what the tradeoffs are between these approaches.\n- Theory: The optimal discriminator depends on the distribution of the real and generated images being fed to it. This means that the optimal discriminator for eps1 ~ N(0, 1) will be different than the optimal discriminator for eps2 ~ N(0, 1). However, you train the same shared discriminator across all scales and types of noise. How does this impact the theory presented and the interpretation of your approach? As an alternative, you could imagine conditioning the generator on an an additional input specifying the type of noise, i.e. D(x, noise_type) and then you could learn a D that is optimal for all noise types.\n- No evaluation of how having the multiple losses for different noise types impacts the training time. Even if iteration time is the same as standard GANs, adding more noise could result in slower training. Please add a plot of FID or IS vs. wall clock time to get a better sense of the tradeoffs in different approaches. \n- It looks like clean + noise with fixed or learned sigma (variants d/e) performs quite well, and has much less overhead vs. NTGAN. Can you discuss this result more? Would you advocate using this approach over NTGAN?\n\nMinor comments:\n- Why does NTGAN + SGD (variant b) perform much better than other approaches? Can you provide more details with how you chose the variants in Table 4?\n- Might want to cite Generative Adversarial Trainer, another approach for learning additive noise distributions in the context of adversarial examples",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Noise-Tempered Generative Adversarial Networks\"",
            "review": "The authors make two observations. 1) Matching real and fake with postprocessing by an additive noise model is attained when the real and fake distributions are the same (thus there is a priori no need to anneal the noise like in previous work), however while a necessary condition for Pr = Pg, it is not sufficient. 2) The condition becomes sufficient if P_{r+epsilon} = P_{g + epsilon} for all noise distributions in a certain *class*. Thus, they propose to match multiple noisy versions of probability distributions. This has the known advantageous property that doesn't suffer from the disjoint / negligible intersection of supports problem, but it benefits as well from the fact that if the class of noise models is large enough, the solution to the proposed loss is the real distribution, and that by not anealling the noise term, it doesn't degenerate back to the original ill-conditioned problem.\n\nThen, however, the authors propose two particular kinds of family of noise distributions. In particular, the additive noise is a (random) mixture between a delta at 0 and gaussians with varying sigmas (let's call this family A) or a (random mixture) between a delta at 0 and N(w) with w ~ Gaussian(0, I) with N a learned neural net. Furthermore, for some reason pick the minimum cost over noise distributions (with some regularizer), which is unexplained.\n\nHere are my concerns about this:\n- For these particular kinds of distributions, and this optimization process (minimizing the cost as a function of sigma / N, with this regularizers), why would the minimum be only at Pr=Pg? Namely, the authors point out that P_{r+e} = P_{g+e} for all e dists implies that P_r = P_g, why is that the case when considering only the distribution that minimizes the inner loop and in this limited family of distributions? This seems to be a crucial point. \n- By doing a mixture with the unmodified (probably colapsed to a low dimensional structure) data and fake distributions, the resulting distributions are likely not absolutely continuous. Thus, a priori I don't see why the JSD(P_{r+epsilon}, P_{g+epsilon}) is a continuous quantity as a function of the generator's parameters, or why it provides a usable gradient.\n\nThe authors only provide results in reasonably mid-dimensional benchmarks, (CIFAR, celeb-A, LSUN), and results are not particularly impressive. I would urge the authors to do more controlled experiments: check that for the proposed distributions, the gradients of the cost with respect to the generator don't vanish / explode when the discriminator gets more confident (as in Arjovsky & Bottou). Check that the discriminator doesn't get 'perfect' too quickly. In a 1D or 2D experiment, actually plot the gradients of the cost with respect to the discriminator with respect to the input (i.e. what's the gradient field), or plot the discriminator itself (e.g. figure 1 of wgan) to see if it provides a usable gradient. Make sure to plot these for a very well trained discriminator, since this is what would ensure that no careful balance between gen and disc needs to be maintained. The only controlled experiments (figure 5) are not particularly encouraging, since it seems that the generator has still a quite significant amount of samples away from the modes.\n\nThe paper is in general well written, and the ideas and observations are simple, to the best of my knowledge novel, and have clear consequences. The actual instantiation of these ideas seem to have several caveats, or aspects that need elaboration, as mentioned before. Furthermore, the experiments aren't too convincing. Some parts also require better writing. For example, in points 1 and 2 (noise descriptions) in page 4, delta(epsilon) is not a notation commonly employed to refer to no noise (which is what the authors mention in text). If the noise is additive then this is just delta_0. As well, int delta(eps - N(w)) p(w) dw is hard to understand / nonstandard, the authors should just say N(w) with w ~ N(0, I) or N^# N(0, I) [and clarify that # means push-forward of a distribution].\n\nPet peeve: In the third line of the abstract 'probability density function' -> 'probability distribution'. A low dimensional distribution doesn't have a density :).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}