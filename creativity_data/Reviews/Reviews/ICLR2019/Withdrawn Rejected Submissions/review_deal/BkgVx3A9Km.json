{
    "Decision": {
        "metareview": "Dear authors,\n\nThe reviewers all appreciated your goal of improving dimensionality reduction techniques. This is a field which does not enjoy the popularity it once did but remains nonetheless important.\n\nThey also appreciated the novel loss and the use of triplets.to get the global structure.\n\nHowever, the paper lacks some guidance. In particular, it oscillates between showing qualitative results (robustness to outliers, \"nice\" visualizations) and quantitative ones (running time, classification performance). I agree with the reviewers that the quantitative ones should have used the same preprocessing for t-SNE and TriMap (either PCA or no PCA), regardless of the current implementation in software tools.\n\nGiven that the quantitative results are not that impressive, may I suggest focusing on the qualitative ones for a resubmission? The robustness of the emeddings to the addition or removal of a few points is definitely interesting and worth further investigation, optionally with a corresponding metric.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A new take on dimensionality reduction which deserves a more guided experimental section"
    },
    "Reviews": [
        {
            "title": "Novel loss function but experiments are lacking",
            "review": "Motivated by the observation that most of previous dimensionality reduction methods focus on preserving \nlocal pairwise neighboring probabilities and lack in preserving global properties, this paper proposes a \nmethod called TriMap to optimize a loss function preserving similarities among triplets of data points. A large \nnumber of triplets are sampled either based on nearest neighbor calculations or random sampling. Experimental \nresults on several datasets show that TriMap identifies outliers and preserves global data properties better \nthan previous approaches based on pairwise data point comparisons.\n\nMajor:\n\nThe idea in this paper is well motivated and the loss function based on probability ratio is novel. However, \nthere are some major concerns about method analyses and experimental evaluations,\n\n1. Data embedding based on triplets has been presented in (van der Maaten and Weinberger, 2012). The authors \nneed to present detailed explanations and formal analysis why the proposed method significantly outperforms the \nprevious one. A recent dimensionality reduction method compares data points only to data cluster centers (Parametric \nt-distributed stochastic exemplar centered embedding, Min et al., 2018), does it preserve global data properties? Does \nits trivial combination with standard t-SNE well preserve both local and global data properties?\n\n2. Preserving local pairwise neighborhood structure is often the most important part in high-dimensional data \nvisualization, because only local similarities can be confidently trusted in a high-dimensional space. Even if preserving \nglobal data properties is important, the very local neighborhood structure should also be preserved. However, the \nproposed method TriMap is significantly worse than t-SNE according to AUC under the precision-recall curve. \n\n3. Standard quantitative evaluations based on 1NN error rate and quality scores (van der Maaten & Hinton 2008, Min \net al. 2018) should be added to the experiments. For preserving global data properties, quantitative evaluations on all \nthe datasets will make the experiments much more convincing.\n\n4. In the abstract, the claim that TriMap scales linearly is inaccurate, the triplet sampling requires nearest neighbor \ncalculations, which has computational complexity of at least O(nlogn)\n\n5. This paper proposed two variants of triplet sampling, nearest neighbor triplets and random triplets. Detailed experimental \ncomparisons about them should be provided in the paper.\n\n\n6. The running time comparisons in Table 1 must be wrong or highly biased with improper hyperparameter setting. Based \non tree accelerations, t-SNE can produce impressive visualization on MNIST-scale datasets within 15 minutes (please \ncheck the experimental details PP. 3235-3238 in van der Maaten, Journal of Machine Learning Research 2014).\n\n7. The authors mentioned partial observation, outliers and subclusters in the global information, but the authors do not specifically define \nwhat the global information should be rigorously, and the paper does not theoretically prove or explain via experiments how the global \ninformation is kept by TriMap.\n\n8. In the experiments, the authors applied PCA before TriMap to reduce the dimensionality while PCA is not applied in tSNE and LargeVis. The authors do not explain why the settings are different in the three methods.\n\nMinor:\n\n9. In the algorithm, the authors show different equations for different t and t’, but are not evaluated in experiments.\n\n(After reading the rebuttal, I raised the rating from 5 to 6.)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "TripMap needs more comparison and validation ",
            "review": "In this paper, the authors present a novel dimensionality reduction method named TriMap. TriMap attempts to improve upon the widely-adopted t-SNE algorithm by incorporating global distances through the use of triplets, rather than pairwise comparisons. The authors compare to t-SNE, as well as a newer method called LargeVis which also claims to impose a global distances metric. The authors show that their method is more robust to the addition or removal of clusters and outliers and provides a more meaningful global distance relative to the methods against which they compare.\n\nTechnical Quality\nThe authors’ method is clear and well described and addresses a poignant issue in dimensionality reduction. However, the authors fail to compare their method to a number of relevant dimensionality reduction algorithms which also claim to provide solutions with globally meaningful distances. Such methods include force-directed graph drawing (Fruchterman & Reingold, 1991), diffusion maps (Coifman & Lafon, 2006) and PHATE (Moon et al., 2017). \n\nAdditionally, the handling of outliers is a concern. While the authors claim that the retention of outliers as disconnected from the manifold is a desirable quality of their technique, the presence of many outliers in a dataset (for example, in the Tabula Muris and lyrics datasets) has the potential to mask the interesting portion of the dimensionality reduction. It may be worth commenting on the desirability to identify and remove outliers, and the provision of such a technique in the software upon its release.\n\nFinally, the runtime comparison is of concern. It is common to perform most DR methods on high-dimensional PCA representation of the data, particularly in single-cell genomics (e.g. the Tabula Muris dataset in Part 3.) In this context, both UMAP and PHATE successfully embed the Tabula Muris dataset in less than the reported TriMap time (3.5 and 5 minutes respectively, compared to 15 minutes reported for TriMap.)\n\nNovelty\nThe authors’ method appears to provide improved results over the compared alternatives, however, it is worth noting that triplet-based embedding is not novel in its own right (van der Maaten & Weinberger, 2012), though one could argue novelty is warranted here due to claimed substantial improvements of results. In this case, the authors should include a comparison to competing triplet-based methods, at least in the appendix. \n\nPotential impact\nThe authors’ method has the potential to be used widely across many fields, as a direct replacement for t-SNE. Its adoption is contingent on compelling evidence that it produces results substantially better than UMAP (which is currently heralded as an upcoming replacement for t-SNE in some fields) and other competing methods. The authors may find it worthwhile to provide such comparisons, if not in the main body of the paper at least in the appendix. \n\nClarity\nThe paper is easy to read and makes its point in a reasonably concise manner. Detailed explanation of experiments v) and vi) could be relegated to the appendix. More precise statement of the authors’ tests in Part 2 could be provided by quantifying the results of the tests in a more precise way; it is not clear what the authors seek to achieve by drawing the dotted lines between clusters in Figure 1a, or by providing AUC values in Figure 1.\n\nDetailed Comments\n•\tIn the definition of Equation 2, it is not until one paragraph later than q_{ij}^{~(t’)} is defined – this is confusing and hard to read.\n•\tThe captions for Figures 1 and 3 would be substantially clearer with more detail on the dataset analyzed and in Figure 1, some discussion of the purpose of each subplot.\n•\tThe Figure 3 caption needs a semicolon or period before introducing the bottom panel.\n•\tThe claim that the authors’ heuristic triplet sampling (nearest-neighbor and random sampling) is sufficient to approximate full triplet sampling should be shown in the appendix.\n•\tThe collaboration network analyzed in Part 3 is naturally a graph; it would make sense to cluster and visualize this using a graph-based clustering, rather than coercing it to Euclidean coordinates.\n\n(Note: after reading the revised manuscript I have changed my recommendation from a 6 to a 5)\n\nReferences\nCoifman, R. R., & Lafon, S. (2006). Diffusion maps. Applied and computational harmonic analysis, 21(1), 5-30. https://doi.org/10.1016/j.acha.2006.04.006\nMoon, K. R., van Dijk, D., Wang, Z., Burkhardt, D., Chen, W., van den Elzen, A., ... & Krishnaswamy, S. (2017). Visualizing transitions and structure for high dimensional data exploration. bioRxiv, 120378. https://doi.org/10.1101/120378\nFruchterman, T. M., & Reingold, E. M. (1991). Graph drawing by force‐directed placement. Software: Practice and experience, 21(11), 1129-1164. https://doi.org/10.1002/spe.4380211102\nL. van der Maaten and K. Weinberger. Stochastic triplet embedding. In 2012 IEEE International Workshop on Machine Learning for Signal Processing, pp. 1–6, Sept 2012. doi: 10.1109/MLSP.2012.6349720.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A More Globally Accurate Dimensionality Reduction Method Using Triplets ",
            "review": "Authors propose a new method called TriMap, which captures higher orders of structure with triplet information, and minimize a roust loss function for satisfying the chosen triplets.\n \nThe proposed method is motivated by the misleading selection approach for a dimensionality reduction method using local measurements. And then, authors resort to an evaluation based on visual clues based on a number of transformations. Authors then claim that any DR method preserving the global structure of the data should be able to handle these transformations.  An example on MNIST data illustrate these properties, but it is still not clear what are the visual clues as the criterion to select a good DR method and what are the global structures.\n \nAuthors discussed the results in Figure 4 for six real-world datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.  It will be good to add some convincing evidences for the conclusion.\n \nAs the method highly depends on the subset of sampled triplets, it is interesting to see how the global structure changes if a different set of triplets is used.  In addition, it is unclear why sampled triplets can achieve a global structure of data instead of pairwise relations. From the experiments, triplets are also sampled according to the pairwise nearest neighbor graph.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}