{
    "Decision": {
        "metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\nThe paper \n- tackles an interesting problem\n- makes a concerted effort to provide qualititative results that give insight into the models behaviour.\n- sufficiently cites related work.\n\n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- The model architecture lacks novelty.\n- There was also agreement that the contributions - (i) minor modifications of existing sequential attention-based models, and (ii) application to the RL domain - are minor.\n- A lot of space in the paper (section 4.2) is devoted to exploring the use of this model for image classification and video action recognition. However the proposed model performed poorly compared to SOTA methods for this task and no motivation was given for why the proposed model would be useful for such tasks.\n\nAll three points impacted the final decision.\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nThere was high agreement between the reviewers on the main drawbacks of the paper, before and after the rebuttal.\nThe AC considered the rebuttals by the authors (in which they argued that there was sufficient contribution) but, in the end, agreed with the reviewers' assessments.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be rejected.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "good analysis of the proposed method; lacks novelty"
    },
    "Reviews": [
        {
            "title": "Interesting model, however, the performance on the supervised task is not good. ",
            "review": "\n[Summary]\n\nThis paper proposed a soft, spatial, sequential, top-down attention model which enable the agent and classifier actively select important, task relative information to generate the appropriate output. Given the observations, the proposed method uses a convLSTM to produce the key and value tensor. Different from multi-head attention, the query vector is generated in a top-down fashion. The authors proposed to augment the spatial feature with Fourier bases which is similar to previous work. The authors verify the proposed method on both reinforcement learning and supervised learning. On reinforcement learning, the proposed method outperformed the feedforward baseline and LSTM baseline. On reinforcement learning task, the proposed method achieves compelling result with more interpretable attention map that shows the model's decision. \n\n[Strength]\n1: The proposed model is a straightforward extension of the multi-head attention to visual input. Compare to multi-head attention, it generates the query vector in a top-down manner instead of pure bottom up, and the authors verify the proposed choice is better than LSTM baseline empirically.\n\n2: The authors verify the proposed method by extensive experiments on reinforcement learning tasks and also try supervised learning tasks. The attention visualization and human normalized scores for experts on ATARI show the effectiveness of the proposed method. \n\n[Weakness]\n1: The soft spatial top-down attention is very common in vision and language domain, such as VQA. As the authors mentioned, the proposed method is very similar with MAC for CLEVER. The sequential attention is also explored in previous VQA work. Thus the novelty of the proposed method is limited. \n\n2: Multi-head attention for NLP tasks are usually composed with multiple layers. Will more layer of attention help the performance? The paper is less of this ablation study. \n\n3: The proposed method is worse compared with other baselines on supervised learning tasks, on both imagenet classification and kinetics. I wonder whether the recurrent process is required for those tasks? On table 2, we can observe that with sequence length 8, the performance is much worse,  this may be caused by overfitting. \n\n4: If the recurrent attention is more interpretable, given other visualization methods, such as gradcam, I wonder what is advantage?\n\n5: I would expect that the performance on Kinetics dataset is better since sequential attention is required on video dataset. However, the performance is much worse compare of the baseline in the dataset. I wonder what is the reason? is there ablation study or any other results on this dataset? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting visual attention approach.",
            "review": "Summary.\nThe paper proposes a variant model of existing recurrent attention models. The paper explores the use of query-based attention, spatial basis, and multiple attention modules running in parallel. The effectiveness of the proposed method is demonstrated with various tasks including RL (on Atari games), ImageNet image classification, and action recognition, and shows reasonable performance. \n\nStrengths.\n- An interesting problem in the current CV/RL community.\n- Well-surveyed related work.\n- Supplemental materials and figures were helpful in understanding the idea.\n\nvs. Existing recurrent attention models.\nIn Section 2, the proposed model is explained with emphasizing the differences from existing models, but there needs a careful clarification.\n\nIn this paper, attention weights are computed conditioned on a query vector (which solely depends on the RNN’s state) and the Keys (which are generated by a visual encoder, called vision core). In the landmark work by Xu et al. (ICML ‘15, as already referenced), attention weights are computed very similarly - they used the hidden state of RNN followed by an additional layer (similar to the “query”) and visual features from CNN followed by an additional layer (similar to the “keys”). The only difference seems the use of element-wise multiplication vs. addition, but both are common units in building an attention module. Can authors clarify the main difference from the existing recurrent attention models?\n\nTraining details.\nIn the supervised learning tasks, are these CNN bases (ResNet-50 and ResNet-34) trained from scratch or pre-trained with another dataset?\n\nMissing comparison with existing attention-based models.\nThe main contribution claimed is the attention module, but the paper does not provide any quantitative/qualitative comparison from another attention-based model. This makes hard to determine its effectiveness over others. Notable works may include:\n\n[1] Sharma et al., “Action recognition using visual attention,” ICLR workshop 2016.\n[2] Sorokin et al., “Deep Attention Recurrent Q-network,” NIPS workshop 2015.\n[3] Choi et al., “Multi-Focus Attention Network for Efficient Deep Reinforcement Learning,” AAAI workshop 2017.\n\nMinor concerns.\nThe related work section would be helpful if it proceeds the current Section 2.\nTypos",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting results. Some more experimentation needed",
            "review": "This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model's state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.\n\nPros:\n-Paper was easy to understand\n-Detailed analysis of model behavior. The breakdown analysis between \"what\" and \"where\" was particularly interesting.\n-Attention results appear interpretable as claimed\n\nCons:\n-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.\n-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?\n-I would also recommend including a citation for :\nSukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"End-to-end memory networks.\" NIPS 2015.\n\n\nGeneral questions:\n-Was there an effect of attention grid coarseness on performance?\n-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}