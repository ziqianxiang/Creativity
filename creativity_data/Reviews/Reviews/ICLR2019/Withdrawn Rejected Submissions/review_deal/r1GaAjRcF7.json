{
    "Decision": "",
    "Reviews": [
        {
            "title": "Needs more work",
            "review": "Synopsis: The paper proposes a neural network based model that integrates a\nsubmodular function. The proposal combines gradient based optimization technique\nwith submodular framework named `Differentiable Greedy Network' (DGN). The\nproposal is focused on retrieval tasks especially directed towards the subtask\nof retrieving evidence sentences given a claim on the FEVER dataset. The\nproposal involves building a neural network that recursively selects sentences\nthat support the claim with a greedy algorithm that maximizes the submodular\nfunction. The proposal core of the proposal exploits sums of concave composed\nwith monotone modular functions (SCMM) and ideas from gumbel-softmax based\nliterature to build a submodular function. The proposed submodular function has\nlearnable parameters and the submodular function uses a softmax with a\ntemperature hyperparameter. The paper shows that the proposal is competitive and\noutperfoms a couple of baselines. \n\nPositives: The paper is well motivated and proposes fairly well grounded\nsolution. \n\nProblems: \n* While the paper seems to be well motivated, some of the claims are\njustified in a hand-waving manner and could be better justified with either a\nmore theoretical exposition or empirical analysis. For eg., the utility of ReLU,\nwhile the theoretical justification is clear, how important was the\nnon-linearity in the entire scheme of DGN? Or the relation to attention\narchitectures - while the proposed method is superior to simple attentional architectures \ncan this be related to decomposable attention [1] or the hierarchical attention architecture[2]?\n\n* Ablation between different families of claims: I suppose the submodular\nfunction is going to be very useful when the number of evidences are more than\ntwo. The paper could have benefitted from ablation between different types of\nclaims: claims with one/two sentence as evidence vs claims with more than two\nsentences as evidences.\n\n* The claim that greedy DGN outperforming conventional discrete opitmization\n  algorithms is strong given sparse comparisons to conventional techniques. \n\nQuestions:\n\n* From the experiments in the paper it seems like the temperature is\n  a very delicate hyperparameter and the range from $(3, 6)$ seems like the\n  function is not close to argmax. Would this mean that the trained model is\n  suboptimal? I would also suggest the authors to kindly add the ablations of\n  temperature (with annealing) to the appendix. \n\n* The descriptions of encoder and deepencoder are very confusing. Is the only\n  addition to the deep encoder is an additional non-linear layer? In any case,\n  Table 2 suggests the deep encoder performs comparably to DGN, apart from the\n  problems with class imbalance, given its performance, is it correct to say\n  that deepencoder learns robust representations? In other words, the advantages\n  of DGN are still not clear. Is the greedy algorithm helping in selecting the\n  k sentences? If so, how many of the cases have k>2 and was DGN better there? \n\n* What happens when the similarity is computed using say euclidean distance?\n  How would the temperature hyperparameter change?\n\n\n\nOther Issues: \n* The sections in paper need rewriting, especially section 4. \n\n* Comparisons: it would have been ideal to have the comparisons to attention\n  based approaches (simple attention, hierarchical attention) with standard\n  optimization techniques. \n\n\n[1] Parikh, Ankur P., et al. \"A decomposable attention model for natural language inference.\" arXiv preprint arXiv:1606.01933 (2016).\n[2] Yang, Zichao, et al. \"Hierarchical attention networks for document classification.\" Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs major improvement to be acceptable",
            "review": "This paper uses a differentiable submodular function to the task of selecting evidence sentences for claims. The paper is not ready to be accepted in a top venue in its current form. Details below.\n\n1) Writing: Needs major work. There are 3.5 pages of introduction and unnecessary background on submodular functions and two paragraphs in sec 3.2 of what the method proposed in this paper does. Only mentioning what the submodular function is, and may be a couple of references for it should have been enough. Proposition 1 is redundant, just mentioning the result in a single sentence is more than enough. I do not see the benefit of talking about a substantial subset of the submodular literature. The intro itself needs major revisions to be readable, there are a lot of holes. e.g. what is \"unfolding\" of the greedy algorithm that the  authors refer to?  Use of \\tau in f() should be explicitly written as an equation, so the reader doesnâ€™t have to look at the paper by Jang et al to know that the exact expression is used in the training. The experimental task itself could have been explained better. What is  the DeepEncoder architecture? Why was it used as a baseline? Why cant diversity arguments not be made by easily altering the DeepEncoder cost function? It may not have been designed to have marginal gains over other selections, and may be selects sentences as standalone evidences. The motivation of the proposed approach is unclear, and warrants a separate point. There are minor typos (e.g. see Table 1 caption  \"the recall improves recall \"), which could be ignored for now.\n\n2) Motivation: The authors mention \"We choose the ReLU function specifically because the submodularity of the SCMM function requires non-negativity \". This is not a suitable reason to design experiments. The guarantees should follow from the function that empirically works best, the functions should not be designed to get some theoretical result and show it works \"reasonably well\" in practice. The \"attention mechanism \" motivation is very weak and does not justify the design or the greedy regularization. Why select a subset when the full problem can be tractably solved ? Again this could just be lazy writing. \n\n3) Empirical evaluation: One task and evaluation does not suggest that the proposed methodology is useful. Even ignoring the holes in writing, a single task and empirical evaluation on one dataset with precision/recall/F1 scores at different k is not evidence that the proposed method is verified. It does not even justify the title. Are there no other baselines that the proposed method could be compared against? This paper is not about \"Differentiable Greedy Networks\", it is about \"greedy evidence selection for verifying claims using Recurrent NN\". ",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Incremental method of applying greedy algorithm to the outputs of a shallow network, without strong support from experimental comparisons",
            "review": "This paper proposes a neural network that aims to select a subset of elements, for example, selecting k sentences that are mostly related to a claim from a set of retrieved docs. It firstly computes a multi-dimensional similarity score vector per element. The proposed network, DGN, takes the score vectors of all the candidates as inputs, processes them by linear+ReLU, and then runs the forward greedy algorithm maximizing a submodular function, which selects a subset of score vectors. The submodular function is defined as a weighted sum of multiple submodular functions, each corresponding to a dimension of the score vector, and is a concave function of the sum of the selected elements' scores on that dimension. The weights used to produce the weighted sum are learnable parameters of the submodular function. They show that DGN outperforms some baseline methods on FEVER dataset. \n\nMajor concerns:\n\n1) The idea is incremental: it applies the greedy algorithm to the outputs of a Linear+ReLU layer, whose inputs are the pre-computed similarity scores. Unfolding the greedy algorithm produces a simple RNN with each unit computing the marginal gain conditioned on the previously selected elements (the previous state), but it does not increase the depth and thus does not improve representativeness. \n\n2) Since the transformation applied to the input similarity scores is linear, it might not be very helpful and the final performance might largely depend on the quality of the similarity metric used to compute the input scores. In addition, the idea of learning the weights in a submodular function has been studied in many previous works such as learning submodular mixture and deep submodular functions, and the idea can be applied to more general submodular functions rather than the specific one used in this paper. Hence, the contribution and novelty of this paper are very limited.\n\n3) During training, softmax is used to approximate argmax and make the computational graph differentiable. Why not using Gumbel-softmax (which was studied in the paper that the authors cited to support their use of softmax here), which is a better approximation of argmax? In addition, the differentiability comes from using softmax, but this makes the training not exactly optimizing for the original greedy algorithm but a soft version of it.\n\n4) None of any previous works has been compared in the experiments except baselines created by the authors. At least, attention-based models and RNN such as Transformer and LSTM should be compared, since the proposed model is similar to them on some main ideas: it can be explained as an attention mechanism using the marginal gain of the submodular function as the attention score function; and it can be also explained as an unfolded simple RNN.\n\n5) In experiments, the proposed DGN has the same performance or shows slight advantages when comparing to \"Encoder\", which does not use any greedy procedure at all. Does this suggest that the greedy procedure is not necessary?\n\nMinor comments:\n\n1) Proposition 1 had already been proved in classical submodular textbooks.\n\n2) Is the ground truth labels L in Eq.(5) a subset (as defined above the equation) or a subsequence (as indexed in the equation)? Which one provides better training signals?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}