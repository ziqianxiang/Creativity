{
    "Decision": {
        "metareview": "Improving the staleness of asynchronous SGD is an important topic. This paper proposed an algorithm to restrict the staleness and provided theoretical analysis. However, the reviewers did not consider the proposed algorithm a significant contribution. The paper still did not solve the staleness problem, and it was lack of discussion or experimental comparison with the state of the art ASGD algorithms. Reviewer 3 also found the explanation of the algorithm hard to follow.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Not significant contribution and not sufficient experiments"
    },
    "Reviews": [
        {
            "title": "Interesting paper but the contribution seems not be good enough",
            "review": "Overall, this paper is well written and clearly present their main contribution.\nHowever, the novel asynchronous distributed algorithm seems not be significant enough.\nThe delayed gradient condition has been widely discussed, but there are not enough comparison between these variants.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "missing references, theory is not novel, experiments are not sufficient",
            "review": "The paper proposes an algorithm to restrict the staleness in ASGD (asynchronous SGD), and also provides theoretical analysis. This is an interesting and important topic. However, I do not feel that this paper solves the fundamental issue - the staleness will be still very larger or some workers need to stay idle for a long time in the proposed algorithm if there exists some extremely slow worker. To me, the proposed algorithm is more or less just one implementation of ASGD, rather than a new algorithm. The key trick in the algorithm is collecting all workers' gradients in the master machine and update them at once, while hard limiting the number of updates in each worker. The theoretical analysis is not brand new. The\nline 6 in Algorithm 1 makes the delay a random variable related to the speed of a worker. The faster a worker is, the larger the tau is, which invalidates the assumption implicitly used in the theoretical analysis.\n\nThe experiment is done with up to 4 workers, which is not sufficient to validate the advantages of the proposed algorithm compared to state of the art ASGD algorithms. The comparison to other ASGD implementations is also missing, such as Hogwild! and Allreduce.\n\nIn addition, I am so surprised that this paper only have 10 references (the last one is duplicated). The literature review is quite shallow and many important work about ASGD are missing, e.g.,\n\n- Parallel and distributed computation: numerical methods, 1989.\n- Distributed delayed stochastic optimization, NIPS 2011.\n- Hogwild!, NIPS 2011\n- Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization, NIPS 2015\n- An asynchronous mini-batch algorithm for regularized stochastic optimization, 2016.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "I don't understand why the proposed method is an asynchronous method",
            "review": "This paper tries to propose a so-called hybrid algorithm to eliminate the gradient delay of asynchronous methods. The authors propose algorithm 1 and a simplified version algorithm 2 and prove the convergence of algorithm 2 in the paper.  The paper is very hard to follow, especially the algorithm description part. What I can understand is that the authors want to let the fast workers do more local updates until the computation in the slowest worker is done. The idea is similar to EASGD except that it forces the workers to communicate the server once the slowest one has completed their job.\n\nThe following are my concerns:\n1. Do you consider the overhead in constructing the communication between machines? in your method,  workers are keeping notifying servers that they are done with the computation. \n2. In Algorithm 1 line 9 and line 23, there are two assignments: x_init =x and x_init=ps.x, is there any conflict? \n3. In Algorithm 2,  at line 6 workers wait to receive ps.x, at line 20 server wait for updates. I think there is a bug, and nothing can be received at both ends.\n4. The experiments are too weak. There is no comparison between other related methods, such as downpour, easgd.\n5. The authors test resnet50 on cifar10,  however, there is no accuracy result. They show the result by using googlenet, why not resnet50? I am curious about the experimental settings.\n\nAbove all, the paper is hard to follow and the idea is very trivial. Experiments in the paper are also very weak. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}