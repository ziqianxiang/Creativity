{
    "Decision": {
        "metareview": "The paper proposes a Bayesian extension to existing knowledge base embedding methods (like DistMult and ComplEx), which is applied for for hyperparameter learning. While using Bayesian inference for for hyperparameter tuning for embedding methods is not generally novel, it has not been used in the context of knowledge graph modelling before. The paper could be strengthened by comparing the method to other strategies of hyperparameter selection to prove the significance of the advantage brought by the method.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Intersting idea but significance is not fully clear"
    },
    "Reviews": [
        {
            "title": "Probabilistic Knowledge Graph Embeddings - Review",
            "review": "Summary:\nThe paper presents a probabilistic treatment of knowledge graph embeddings, motivating it in parameter uncertainty estimation and easier hyperparameter optimisation. The authors present density-based DistMult and ComplEx variants, where the posterior parameter distributions for entity and relation embeddings are approximated by diagonal Gaussians q_\\gamma. Variational EM is used to infer the variational parameters \\gamma as well as the per-entity/per-relation precision (\\lambda) hyperparameters. The training process proposed by the authors consists of three phases: (1) pretraining a MAP estimate that’s used as initial means of the posterior approximating Gaussians, (2) variational EM (see above) to find better hyperparameters and (3) another MAP training phase that uses the updated per-entity/per-relation hyperparameters. Finally, experimental results indicate a slight improvement in MRR and HITS@10 across FB and WN datasets.\n\nOriginality:\nTo the best of the reviewer’s knowledge, the presented approach is novel for knowledge graph embeddings.\n\nDiscussion:\nWhile the task is relevant, it is unclear how significant the improvements are. While overall, the proposed method seems to indicate small improvement upon a very strong baseline, in some cases it’s very close (96.2 vs 96.4 HITS@10 on WN18, 36.4 vs 36\n.5 MRR on FB15K237), or worse (85.8 vs 85.4 MRR on FB15K). \nIt is unclear how adequate some details in the experimental setup are for verifying the main hyperparameter optimization claim. In particular, what is “a reasonable choice of hyperparameters” in the first training phase? From figure 3b it seems the initial lambda’s are set proportionately to the frequency, as in the baseline. Are the initial hyperparameter values in EM set the same as the hyperparameter values used for MAP in the reported results? If the claim is to optimize hyperparameters, shouldn’t their initial values be set as uninformed as possible? How do different initial hyperparameter values affect final performance?\nThe authors claim that the improvement is most notable for entities with fewer training points, however, this is only investigated by using a balanced MRR, where the results are again very close, the same (WN18) or worse (FB15K) for ComplEx. Wouldn’t it be clearer to perform a separate evaluation only considering low-frequency entities to verify this claim?\nParameter uncertainty is not further handled in the paper, the final approach is a point estimate, which discards the uncertainties obtained by VI. Authors mention (last paragraph of Sec. 4) that for a large embedding dimension, bayesian predictions are worse, while for small dimension, they are better. The author’s hypothesis is that a more flexible posterior approximation could solve this issue. No concrete numbers or further analysis are provided.\n\n\nClarity and presentation:\nThe result tables should be merged and formatted better. \nFigures need some work (Fig. 2 looks poorly scaled, all figures should be in vector format for scalability, typos in Fig. 1)\n\nQuestions:\n- How much additional computation is needed to achieve the reported results?\n- Would it be possible to group the entities in bins by frequencies (say 6-10 bins) and assign each bin a hyperparameter, and run grid search over just 6-10 hyperparameters, and then interpolate between the bins to set hyperparameters per entity as a function of its frequency?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Sound method, but the scope is limited to hyperparameter tuning, and no comparisons with other methods are provided",
            "review": "In this paper, authors propose a probabilistic extension of classic Neural Link Prediction models, such as DistMult and ComplEx. The underlying assumption is that the entity embeddings and the relation embeddings are sampled from a prior Multivariate Normal distribution, whose (hyper-)parameters can be estimated via maximum likelihood. In this paper, authors use Variational Inference (VI) for approximating the posterior distribution over the embeddings, and use Stochastic VI for maximising the Evidence Lower BOund (ELBO) while scaling to large datasets. In Sect. 3, authors introduce the generative process, and show how MAP estimation of the embedding matrices can recover the original models. In Sect. 4, authors start from the intractable marginal likelihood over the data (Eq. 5) for deriving the corresponding ELBO (Eq. 6), which is defined over:\n- The \"hyperparameters\" gamma, which define the parameters of the prior Multivariate Normal distribution over the embeddings, and\n- The parameters gamma of the variational distributions.\n\nQuestion: why the ELBO (Eq. 6) is not used anywhere in Algorithm 1?\n\nThe model does not mention a number of significantly more accurate models proposed in the literature, such as [1].\n\nFurthermore, it seems to me that the point of the whole paper is finding efficient ways of estimating the hyperparameters efficiently. In that sense, there are other methods that were not considered, either simple (e.g. random sampling or black-box optimization techniques [2]) or more complex (e.g. hypergradient descent [3]).\n\n[1] https://arxiv.org/abs/1707.01476\n[2] https://github.com/hyperopt/hyperopt\n[3] https://arxiv.org/abs/1703.04782",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An adaptive hyperparameter tuning method for knowledge graph embeddings",
            "review": "This paper proposes a Bayesian extension to knowledge base embedding methods, which can be used for hyperparameter learning. My rating is based on following aspects.\n\nNovelty. \nApplying Bayesian treatment to embedding methods for uncertainty modelling and hyperparameter tuning is not new (examples include PMF [1] and Bayesian PMF [2]), and Sec 3 can be regarded as a knowledge base extension of them with a different likelihood (MF considers user-item pairs while knowledge base considers head-edge-tile triplets). However, it seems that there is little work considering the hyperparameter tuning problems for knowledge base embeddings.\n\nQuality & Clarity.\nThis paper makes two arguments. 1. Small data problems exist, and needs parameter uncertainty; 2. Bayesian treatment allows efficient optimization over hyperparameters. However, as mentioned in Sec 4 and Sec 5, they still use MAP estimations with tuned hyperparameters instead of variational distribution directly. This does not support the parameter uncertainty argument (since there is no uncertainty in parameters of the final model, i.e., those re-trained in line 10 of algorithm 1). More analysis, both theoretically and experimentally, is needed to address this argument. The hyperparameter tuning argument is well-supported by both theoretical analysis and experiments. \n\nMy questions are mainly about experiments. Overally, I think current experiments cannot support the claims well and further experiments are needed.\n1.\tAs mentioned above, the parameter uncertainty issue hasn’t been well verified (Figure 3 demonstrates the advantages of hyperparameter tunning instead of uncertainty in parameters).\n2.\tTable 1 & 2 demonstrates that hyperparameter tunning using algorithm 1 introduces performance improvement on ComplEx and DistMult. Since the Bayesian treatment is general, such an improvement should be found for other knowledge base embedding methods. \n3.\tTime complexity is not analyzed (since Algorithm 1 requires re-train the models).\n4.\tAlgorithm 1 is a one-EM-step approximation for optimizing the ELBO. How well such a algorithm approximates the optimal solution of ELBO. For example, what will happens if running line 4-10 for multiple times? Does the performance increase or decrease?\n\n[1] Salakhutdinov and Minh, Probabilistic Matrix Factorization, NIPS 2007.\n[2] Salakhutdinov and Minh, Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo, ICML 2008.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}