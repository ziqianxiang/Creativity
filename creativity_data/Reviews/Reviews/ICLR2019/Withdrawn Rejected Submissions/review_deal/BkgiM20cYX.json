{
    "Decision": {
        "metareview": "The paper proposes a novel approach to interfacing robots with humans, or rather vv: by mapping instructions to goals, and goals to robot actions.   A possibly nice idea, and possibly good for more efficient learning.\n\nBut the technical realisation is less strong than the initial idea.  The original idea merits a good evaluation, and the authors are strongly encouraged to follow up on this idea and realise it, towards a stronger publication.\n\nIt be noted that the authors refrained from using the rebuttal phase.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "nice, but unripe"
    },
    "Reviews": [
        {
            "title": "Overall idea is interesting, but novelty is limited and evaluation is poor",
            "review": "The paper proposes a modular approach to the problem of mapping instructions to robot actions. The first of two modules is responsible for learning a goal embedding of a given instruction using a learned distance function. The second module is responsible for mapping goals from this embedding space to control policies. Such a modular approach has the advantage that the instruction-to-goal and goal-to-policy mappings can be trained separately and, in principle, allow for swapping in different modules. The paper evaluates the method in various simulated domains and compares against RL and IL baselines.\n\nSTRENGTHS\n\n+ Decoupling instruction-to-action mapping by introducing goals as a learned intermediate representation has advantages, particularly for goal-directed instructions. Notably, these together with the ability to train the components separately will generally increase the efficiency of learning.\n\n\nWEAKNESSES\n\n- The algorithmic contribution is relatively minor, while the technical merits of the approach are questionable.\n\n- The goal-policy mapping approach would presumably restrict the robot to goals experienced during training, preventing generalization to new goals. This is in contrast to semantic parsing and symbol grounding models, which exploit the compositionality of language to generalize to new instructions.\n\n- The trajectory encoder operates differently for goal-oriented vs. trajectory-oriented instructions, however it is not clear how a given instruction is identified as being goal- vs. trajectory-oriented.\n\n- While there are advantages to training the modules separately, there is a risk that they are reasoning over different portions of the goal space.\n\n- A contrastive loss would seemingly be more appropriate for learning the instruction-goal distance function.\n\n- The goal search process relies on a number of user-defined parameters\n\n- The nature of the instructions used for experimental evaluations is unclear. Are they free-form instructions? How many are there? Where do they come from? How different are the familiar and unfamiliar instructions?\n\n- Similarly, what is the nature of the different action spaces?\n\n- The domains considered for experimental evaluation are particularly simple. It would be better to evaluate on one of the few common benchmarks for robot language understanding, e.g., the SAIL corpus, which considers trajectory-oriented instructions.\n\n- The paper provides insufficient details regarding the RL and IL baselines, making it impossible to judge their merits.\n\n- The paper initially states that this distance function is computed from learned embeddings of human demonstrations, however these are presumably instructions rather than demonstrations.\n\n- I wouldn't consider the results reported in Section 4.5 to be ablative studies.\n\n- The paper incorrectly references Mei et al. 2016 when stating that methods require a large amount of human supervision (data annotation) and/or linguistic knowledge. In fact Mei et al. 2016 requires no human annotation or linguistic knowledge.\n\n- Relevant to the discussion of learning from demonstration for language understanding is the following paper by Duvallet et al.\n\nDuvalet, Kollar, and Stentz, \"Imitation learning for natural language direction following through unknown environments,\" ICRA 2014\n\n- The paper is overly verbose and redundant in places.\n\n- There are several grammatical errors\n\n- The captions for Figures 3 and 4 are copied from Figure 1.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Proposed method has several limitations, experimental setup is unclear and the results are not convincing.",
            "review": "This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions. The authors claim to achieve better data efficiency, adaptability, and generalization as compared to the baselines.\n\nHere are some comments/questions:\n- One of the biggest limitations of the proposed method is that it can only work for one-to-one or many-to-one mapping of instructions to goals. As I understand (please correct me if I am wrong), the method can not work for contextual instructions where the goal depends on the environment and the same instruction can map to different goals, such as 'Go to the largest/farthest object'.\n- Another limitation of the method is that it requires a set of goals G, which is not trivial to obtain especially in partially observable environments such as embodied navigation in 3D space.\n- The experimental setup is unclear and several crucial details are missing:\n\t- \"An instruction for approaching one of the five targets in the arena is generated and passed to the agent at first.\" -> how is the instruction generated?\n\t- There's no example of the environment or the instruction in the submission\n\t- \"Within the instruction become approaching more than one targets, one of two added targets is selected as internal targets pair with one of the remaining targets.\" I do not understand this sentence. How are the targets generated in the trajectory-oriented task? How are the instructions generated in this task?\n- Experimental results are not convincing:\n\t- The introduction motivates the need for understanding human instructions and the abstract says 'Given a human instruction', but I believe experiments do not have any human instructions.\n\t- All the environments seem to be fully-observable, it is not clear whether the method would work in partially-observable environments.\n\t- Only vanilla PPO and BC cloning are used as baselines. There are several competing methods for following instructions which the authors cite such as Hermann et al. 2017, Chaplot et al. 2017, Misra et al. 2017, etc. Why weren't any of these approaches used as a baseline?\n- The submission requires proof-reading, there are several typos in the manuscript (some are listed below), some of them make it very difficult to understand the setting.\n\n- Typos:\n- Sec 3.1 on Pg 4 mentions 'CEM' multiple times, it's not defined until 3.3.2 on Pg 6.\n- Pg 3 Theses sets -> These sets\n- Pg 7 where the Reacher pointing at -> where the Reacher is pointing at\n- Pg 7 What reacher observes the word is its fingertipâ€™s position, coordinates in two dimension. -> something is wrong in this sentence.\n- Pg 7 Then comes to the trajectory-oriented task, there are only a few differences from above -> something is wrong in this sentence.\n- Pg 7 Within the instruction become approaching more than one targets -> something is wrong here",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper presents an instruction-following model consisting of two modules: a\ngoal-prediction model that maps commands to goal representations, and an\nexecution model that maps goal representations to policies. The second module is\ntrained without command supervision via a goal exploration process, while the\nfirst module is trained supervisedly in a metric learning framework.\n\nThis paper contains an important core insight---much of what's hard about\ninstruction following is generic planning behavior that doesn't depend on the\nsemantics of instructions, and pre-learning this behavior makes it possible to\nuse natural language supervision more effectively. However, the paper also\ncontains a number of serious evaluation and presentation issues. It is obviously\nnot ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence,\netc.) and should not have been submitted to ICLR in its present form.\n\nSUPERVISION AND COMPARISONS\n\nI found comparisons between supervision conditions in this paper difficult to\nunderstand. It is claimed that the natural language instruction following\napproaches described in the first paragraph \"require a large amount of human\nsupervision\" in the form of action sequences. This is not exactly true, as some\napproaches (e.g. Artzi 2013), can be trained with only task completion signals.\nMore problematically, all these approaches are contrasted with reinforcement and\nimitation learning approaches, which are claimed to use \"little human\nsupervision\". In fact, most of the approaches listed in this section use exactly\nthe same supervision---either action sequences (imitation learning) or task\ncompletion signals (reinforcement learning). Indeed, the primary distinction is\nthat the \"NLP-style\" approaches are typically evaluated on their ability to\ngeneralize to new instructions, while the \"RL-style\" approaches are evaluated on\nthe (easier) problem of fitting the complete instruction distribution as quickly\nas possible.\n\nThis confusion carries into the evaluation of the approach proposed in this\npaper, which is compared to RL and IL baselines. It's hard to tell from the\ntext, but it appears that this is an \"RL-style\" evaluation setting, where we\nonly care about rapid convergence rather than generalization. But the baselines\nare inadequately described, and it's not clear to me that they condition on the\ncommands at all. More significantly, it's not clear what an evaluation based on\n\"timesteps\" means for a behavior-cloning approach---is this the number of\ndistinct trajectories observed? The number of gradient steps taken? Without\nthese explanations it is impossible to interpret the experimental results.\n\nGENERALITY OF PROPOSED APPROACH\n\nDespite the advantages of the high-level two-phase model proposed, the specific\nimplementation in this paper has two significant shortcomings:\n\n- No evidence that it works with real language: despite numerous claims\n  throughout the paper that the model is designed to interpret \"human\n  instructions\", it is revealed on p7 that these instructions consist of one or two\n  5-way indicator features. This is an extremely impoverished instruction space,\n  especially compared to the numerous papers cited in the introduction that make\n  use of large datasets of complex natural-language strings generated by human\n  annotators. The present experiments do not support the use of the word \"human\"\n  anywhere in the paper.\n\n- No support for combinatorial action spaces. Even if we set aside the\n  distinctions between human-generated instructions and synthetic command\n  languages like used in Hermann Hill & al., the goal -> policy module is\n  defined by a buffer of cached trajectories and goal representations. While\n  this works for the simple environments considered in this paper, it cannot\n  generalize to real-world instruction-following scenarios where the number of\n  distinct goal configurations is too large to tractably enumerate. Again, this\n  is a shortcoming that existing approaches do not suffer from (given\n  appropriate assumptions about the structure of goal space), so the lack of\n  comparisons is problematic.\n\nCLARITY\n\nThe whole paper would benefit from copy-editing by an experienced English\nspeaker, but a few sections are particularly problematic:\n\n- The first paragraph of 4.1.1 is extremely difficult to understand What does\n  the fingertip do? What exactly is the action space?\n\n- The end of the second paragraph is also difficult to understand; after reading\n  it I still don't know what the extra \"position\" targets do.\n\n- 4.1.4 is cut off mid-way through a sentence.\n\n- last sentence of 4.2\n\nThe figures are also impossible to interpret: three of the four are captioned\n\"overview of the proposed framework\", and none are titled.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}