{
    "Decision": {
        "metareview": "The authors propose a method for distilling a student network from a teacher network and while additionally constraining the intermediate representations from the student to match those of the teacher, where the student has the same width, but less depth than the teacher. The main novelty of the work is to use the intermediate representation from the teacher as an input to the student network, and the experimental comparison of the approach against previous work. \n\n The reviewers noted that the method is simple to implement, and the paper is clearly written and easy to follow. The reviewers raised some concerns, most notably that the authors were using validation accuracy to measure performance, and were thus potentially overfitting to the test data, and regarding the novelty of the work. Some of the criticisms were subsequently amended in the revised version where results were reported on a test set (the conclusions are as before).  Overall, the scores for this paper were close to the threshold for acceptance, and while it was a tough decision, the AC ultimately felt that the overall novelty of the work was slightly below the acceptance bar.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Modifies knowledge distillation by training student to match teachers intermediate representation at multiple layers."
    },
    "Reviews": [
        {
            "title": "cute idea but need more analysis",
            "review": "This paper proposes a new approach to compress neural networks by training the student's intermediate representation to match the teacher's.\n\nThe paper is easy to follow. The idea is simple. The motivation and contribution are clear. The experiments are comprehensive.\n\nOne advantage of the proposed approach that the authors did not mention is that LIT without KD can be optimized in parallel, though I'm not sure how useful this is.\n\nOne major weakness of the paper is how the hyperparameters, such as the number of layers, the alpha, beta, tau, and so on, are tuned. It is not clear from the paper that there is a separate development set for tuning these values. If the hyperparameters are tuned on the test set, then it is not surprising LIT works better.\n\nHere are some minor questions:\n\np.5\n\nLIT outperforms KD and hint training on all settings.\n--> what are the training errors (cross entropy) for LIT, KD and hint training? what about the KD objectives (on the training set) of the model trained with LIT and the one trained with KD? this might tell us why LIT is better than the two.\n\nLIT outperforms the recently proposed Born Again procedure ...\n--> what are the training errors (cross entropy) before and after the born again procedure? this might help us understand why LIT is better.\n\nKD degrades the accuracy of student models when the teacher model is the same architecture\n--> again, the training errors (cross entropy) might be able to help us understand what is going on.\n\np.7\n\nAs shown in Table 3, none of the three variants are as effective as LIT or KD.\n--> is this claim statistically significant? some of the differences are very small.\n\nWe additionally pruned ResNets trained from scratch.\n--> what pruning method is being used?\n\nAs shown in Figure 6., LIT models are pareto optimal in accuracy vs model size.\n--> this is a very strong claim. it's better to say we fail to prune the network with the approach, but we don't know whether there exists another approach that can reduce the network size while maintaining accuracy.\n\nAs shown, L2 and L1 do not significantly differ, but smoothed L1 degrades accuracy.\n--> is this claim statistically significant?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A novel approach for compressing deep learning models",
            "review": "This paper proposes to compress the model by depth. It uses hint training and knowledge distillation techniques to compress a \"deep\" network block-wisely. It shows a better compression ratio than knowledge distillation or hint training while achieving comparable accuracy performance.\n\nPros: \n1. This paper considers block-wise compression. For each block, it uses the output of the teacher's last layer as input during training, which improves the learnability of the student models. \n2. The experiments include a large range of tasks, e.g., image classification, sentiment analysis and GAN. \n\nCons:\n1. Validation accuracy is used as the performance metric, which might be over-tuned. How is the performance on testing datasets?\n2. The writing and organization of the paper need some improvement, especially the experiments section.\n3. The compression ratio (3-5) is not very impressive compared with other compression techniques with pruning and quantization techniques, such as Han et al. 2015, Hubara et al. 2016.\n\nIn summary, I think this is an interesting approach to compress deep learning models. But I think the comparisons should be done in terms of testing accuracy. Otherwise, it is hard to judge the performance of this approach. \n\n=== after rebuttal ===\nThanks for the authors' response. Some of my concerns have been clarified. I increased my rating from 5 to 6. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "paper well presented, experimental validation could be further improved",
            "review": "This paper introduces LIT, a network compression framework, which uses multiple intermediate representations from a teacher network to guide the training of a student network. Experiments are designed such that student networks are shallower than teacher networks, while maintaining their width. The method is validated on CIFAR-10 and 100 as well as on Amazon Reviews.\n\nThe paper is clearly written and easy to follow. The main novelty of the paper is essentially using the teacher intermediate representations as input to the student network to stabilize the training, and applying the strategy to recent networks and tasks.\n\nThe authors claim that they are only concerned with knowledge transfer between layers of the same width, that is teacher and student network been designed (by model construction) to have the same number of downsampling operations, while maintaining the same number of stages (referred to as sections in the paper). However, resnet-based architectures have been shown to perform iterative refinement of their features between downsampling operations (see e.g. https://arxiv.org/pdf/1612.07771.pdf and https://arxiv.org/pdf/1710.04773.pdf ). Moreover, these models were also shown to be good regularizers, since they can reduce their model capacity as needed (see https://arxiv.org/pdf/1804.11332.pdf).  Therefore, having experiments skipping stages would be interesting, and may allow to further compress the networks (by skipping layers or stages which do not incorporate much transformation). Following https://arxiv.org/pdf/1804.11332.pdf, for the sake of completeness, it might also be interesting to compare LIT results to the ones obtained by just removing layers in the teacher network which have small weight norms.\n\nIn method, the last sentence before \"knowledge distillation loss\" suggests the training of student networks might not be done end-to-end. Could the authors clarify this?\nIt seems there might be a typo in the KD loss of \"knowledge distillation loss\", equation (2). Shouldn't the second term of the equation be a function of p^T and q^T (with temperature)?\n\nI would suggest changing \"sections\" to stages, as previously introduced in https://arxiv.org/pdf/1612.07771.pdf .\n\nAs for the experiments, it would be more interesting to see this kind of analysis on ImageNet (pretained resnet models are readily available).\nFigure 3, why not add hint training as well?\nFigure 4, what's the dataset used here?\n\nIn Section 4.2, it seems that the choice of the IR layer in the analysis could have a significant impact. How was the layer chosen for the ablation study experiments?\n\nThere are a few overstatements in the paper:\n- page 5, paragraph 2: FitNets proposes a general framework to transfer knowledge from a teacher network to a student network through intermediate layers. Thus, the framework itself does not require the student networks to be deeper and thinner than the teacher network.\n- page 6, \"LIT can compress GANs\": authors claim to overcome limitations of KD when it comes to applying knowledge transfer to pixel-wise architecture that do not output distributions. It seems that changing the loss and using a l2 loss instead is a rather minor change, especially since performing knowledge transfer by means of l2 (although at intermediate layers) has already been explored in FitNets.\n\nPlease add references for inception and FID scores.\nPlease fix references format in page 10.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}