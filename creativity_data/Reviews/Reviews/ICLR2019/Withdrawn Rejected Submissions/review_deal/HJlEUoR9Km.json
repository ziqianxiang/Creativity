{
    "Decision": {
        "metareview": "No reviewer has made a strong case for accepting this paper or championed it so I am recommending rejecting it. The unfavorable reviewers, although they mention real issues, have not highlighted some of the most important barriers to accepting this work.\n\nOne major, but not necessarily dispositive, concern is that the paper only presents results on MNIST. However, even if we put aside this concern, there are several issues with the motivation and approach of this paper. If this technique is actually good at improving the model outside the clean image distribution, then the paper should show that and not just L2 worst case perturbations. To quote the intro of the paper: \"How can deep learning systems successfully generalise and at the same time be extremely vulnerable to minute changes in the input?\" The answer is: they don't generalize and this work does not show us improved generalization. Even a small amount of test error in the data distribution suggests that the closest test error to a given point will often be quite close to the starting point, although this is easier to see with linear models. The best way to fix this work would be to study (average case) error on noisy distributions (as in the concurrent submission https://openreview.net/forum?id=S1xoy3CcYX ).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "No reviewer has championed accepting this paper"
    },
    "Reviews": [
        {
            "title": "Interesting idea looking at generative models and its effect on defense against adversarial attacks. However, there are some key questions left unanswered.",
            "review": "This paper is clearly written and in an interesting domain. The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks. However, there are some key issues with the paper that are not clear.\n\nThe first is regarding the structure of the paper. The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks. The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs. It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.  Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).\n\nIn a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:\nsee paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf \n\nIt would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.\n\nIt is also unclear how the calculation of relative entropy \"D\" was performed in figure 3. Obtaining the normalized marginal density in a BM is very challenging due to the partition function.\n\nThe second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM. This is a very good point, however the paper do not compare or contrast with existing methods. For example, it is curious to see how denoising Auto encoders would perform. In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf\n\n- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box. Defending against black box attacks is considerably easier than defending against white-box attacks.\n\nIn summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage  of the proposed MF BMs in increasing robustness against adversarial attacks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Original Idea, Incomplete set of experiments",
            "review": "Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks. They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor. Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it. The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser. \n\nOne question which is not addressed is the reason for only one RBM layer. In \"Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning\" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST. Attacking CRBMs is highly relevant and should be included as a baseline.\n\nThe only set of experiments are comparisons on first 500 MNIST test images. If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits. Authors should clarify the justification behind experimenting only on 'first 500 test images'. \n\nFurthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input. Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here. The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers. Although their network was resilient toward white box attacks they suffered from black box attacks. The boundary method on MNIST could be  weaker than a black box attack.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Fascinating work, and many questions",
            "review": "The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks. The probabilities are approximated with variational autoencoders. During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.\n\nThis paper focuses on the second part, with a different model. Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification. This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.  This pre-trained model is then incorporated into the neural net for MNIST classification.  The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model. This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)\n\nThe authors compare to the work of Schott for one type of attack. It would be nice to see more detailed experiments as done in Schott.\n\nQuestions:\n1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.\n2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?\n3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.\n4- Could you please add the found J_h's to the appendix. This architecture reminds me of the good old MRFs for image denoising. Could it be that what we are seeing is the attack being denoised?\n\nI am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott. \n\nThanks in advance. I will re-adjust the review rating following your reply.\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}