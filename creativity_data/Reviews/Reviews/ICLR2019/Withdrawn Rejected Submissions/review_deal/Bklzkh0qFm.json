{
    "Decision": {
        "metareview": "The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn't have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "A good submission but not good enough",
            "review": "This paper presented a relational graph attention networks that could consider both node \nfeatures and relational information (edge features) to perform node-level and graph-level \nclassifications. The basic idea is to combine the graph attention networks (Veličković et \nal. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid \nnetworks. This paper is generally easy to follow and written clearly. Several experiments \nare conducted to demonstrate the performance of the proposed model. Although some promising \nresults have been achieved, I think there are several limitations regarding the novelty and \nsignificance of the proposed model. \n\ni) The proposed architecture is mainly adopted from the graph attention networks (Veličković \net al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple\ncombination is a good attempt to incorporate both node features and edge features but the\nnovelty is quite limited. \n\n\nii) In table 2, I don’t really see any promising results compared to baselines. There are \nlittle improvements over the baselines or even significantly worse. More importantly, \ncompared two schemes of this work, the ones with attentions are “almost” identical with ones\nwithout attentions, which implies that the proposed attentions mechanism is not really useful\nin practice. For most of newly proposed graph embedding algorithms, it is hard to convince \nit is indeed better without some significant improvements (at least 2% absolute accuracy more). \n\niii) For MUTAG dataset, the statistical information of this dataset is quite different from\nwhat I used to use. MUTAG is a standard dataset for testing graph-level classification for \nboth graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and \nheteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled \naccording to whether it has mutagenic effect on the gram- negative bacterium Salmonnella \nTyphimurium. Could you explain why your MUTAG is now a single graph and is cast as node \nclassification problem?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Small incremental extension of existing work",
            "review": "This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations. The paper is well written and the equations easy to follow. The results are not strong. And, unfortunately, the model contribution currently is too modest. \n\nInductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach. We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).\n\nMy introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs). Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution. \n\n--- After rebuttal ---\n\nStill not convinced of the value of the work to the community. Will keep my score the same.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A slight reformulation of RGCN with attention mechanisms with mixed results on graph classification and node classification tasks.",
            "review": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.\n\nUnfortunately the paper falls short in two main areas:\n\n- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)\n- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)\n\nHowever, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}