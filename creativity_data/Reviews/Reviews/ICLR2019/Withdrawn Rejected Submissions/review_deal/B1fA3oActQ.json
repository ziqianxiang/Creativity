{
    "Decision": {
        "metareview": "This paper proposes a new method for graph representation in sequence-to-sequence models and validates its results on several tasks. The overall results are relatively strong.\n\nOverall, the reviewers thought this was a reasonable contribution if somewhat incremental. In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small. In addition, as far as I can tell all comparisons with other graph-based baselines actually aren't implemented in the same toolkit with the same hyperparameters, so it's a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences.\n\nI think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at ICLR this year I am leaning in favor of the other very good papers in my area.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Interesting method, if somewhat incremental. Experiments are reasonable but variables potentially not controlled."
    },
    "Reviews": [
        {
            "title": "Interesting idea ",
            "review": "\n\n[Summary]\nThis paper proposes a Graph-Sequence-to-Sequence (GraphSeq2Seq) model to fuse the dependency graph among words into the traditional Seq2Seq framework.\n\n\n\n[clarity]\nThis paper is basically well written though there are several grammatical errors (I guess the authors can fix them).\nMotivation and goal are clear.\n\n\n[originality]\nSeveral previous methods have already tackled to integrate graph structures into seq2seq models.\nTherefore, from this perspective, this study is incremental rather than innovative.\nHowever, the core idea of the proposed method, that is, combining the word representation, sub-graph state, incoming and outgoing representations seems to be novel.\n\n\n\n[significance]\nThe experimental setting used in this paper is slightly out of the current main stream of NMT research.\nFor example, the current top-line NMT systems uses subword unit for input and output sentences, but this paper doesnâ€™t.\nMoreover, the experiments were performed only on the very small datasets, IWSLT-2014 and 2015, which have at most 153K training parallel sentences.\nTherefore, it is unclear whether the proposed method has essential effectiveness to improve the performance on the top-line NMT baselines.\n\nComparing on the small datasets, the proposed method seems to significantly improve the performance over current best results of NPMT+LM.\n\n\n\nOverall, I like the idea of utilizing sub-graphs for simplicity and saving the computational cost to encode a structural (grammatical or semantic) information.\nHowever, I really wonder if this type of technique really works well on the large training datasets...",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Promising results, but use of dependency parser is somewhat concerning",
            "review": "This paper proposes a seq2seq model which incorporates dependency parse information from the source side by embedding each word's subgraph (according to a predetermined dependency parse) using the Graph2Seq model of Song et al. (2018); the authors propose two variants for achieving an encoded source-side representation from the subgraph embeddings, involving bidirectional lstms. The authors show that the proposed approach leads to good results on three translation datasets.\n\nThe paper is generally written fairly clearly, though I think the clarity of section 3.3 could be improved; it took me several reads to understand the architectural difference between this second variant and the original one. The results presented are also impressive: I don't think the IWSLT de-en results are in fact state of the art (e.g., Edunov et al. (NAACL 2018) and Deng et al. (NIPS 2018) outperform these numbers, though both papers use BPE, whereas I assume the current paper does not), but the results on the other two datasets appear to be.\n\nRegarding the approach in general, it would be nice to see how much it depends on the quality of the dependency parse. In particular, while we might expect the en-de and en-vi results to be good because dependency parsers for English are relatively good, how much does performance degrade when considering languages with less good dependency parsers?\n\nPros:\n- Good results, fairly simple model\n\nCons:\n- Somewhat incremental, not clear how much method depends on quality of the dependency parser",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"GraphSeq2Seq: Graph-Sequence-to-Sequence for Neural Machine Translation\"",
            "review": "This paper proposes a method for combining the Graph2Seq and Seq2Seq models into a unified model that captures the benefits of both.  The paper thoroughly describes in series of experiments that demonstrate that the authors' proposed method outperforms several of the other NMT methods on a few translation tasks.\n\nI like the synthesis of methods that the authors' present.  It is a logical and practical implementation that seems to provide solid benefits over the existing state of the art.  I think that many NMT researchers will find this work interesting.\n\nTable 4 begs the question, \"How does one choose the number of highway layers?\"  I presume that the results in that table are from the test data set.  Using the hold out data set, which number gives the best value?\n\nThe paper's readability suffers from poor grammar in some places.  This fact may discourage some readers.\n\nThe authors should fix the missing parentheses in Eqns. (6)-(9).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}