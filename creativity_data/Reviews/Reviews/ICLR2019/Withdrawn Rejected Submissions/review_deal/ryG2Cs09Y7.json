{
    "Decision": {
        "metareview": "The paper proposes an attention mechanism to focus on robust features in the\ncontext of adversarial attacks. Reviewers asked for more intuition, more\nresults, and more experiments with different attack/defense models. Authors\nhave added experimental results and provided some intuition of their proposed\napproach. Overall, reviewers still think the novelty is too thin and recommend\nrejection. I concur with them.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Lack of valid explanation, and insufficient experiment",
            "review": "This paper studies adversarial training of robust classification models. It is based on PGD training in [madry17]. It proposes two points: 1) add attention schemes, 2) add a feature regularization loss. The results on MNIST and CIFAR10 demonstrate the effectiveness. At last, it did some diagnostic study and visualization on the attention maps and gradient maps.\n\n1. Can you provide detailed explanations/intuitions why attention will help train a more robust models?\n\n2. Two related adversarial training papers are missing \"Ensemble Adversarial Training\" (ICLR2018) and \"Adversarial Logit Pairing\" (ICML2018). Also, feature (logit) regularization has been studied in ALP paper on ImageNet.\n\n3. For Table 2 on CIFAR10, I would like to see PGD20 (iterations) + 2 (step size in pixels), PGD100 + 2 and PGD200 + 2. Also, I am interested in seeing CW loss which is based on logit margin. \n\n4. I would like to see results using the \"wide\" model in [madry17] paper for ALP and LRM. I think results from large-capacity models are more convincing.\n\n5. I would like to see results on CIFAR100, which is a harder dataset, 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for justification nowadays (maybe enough one year ago). Since ImageNet is,  to some extent, computationally impossible for schools, I want to see the justification results on CIFAR100.\n\n##### Post-rebuttal\n\nI appreciate the additional results in the rebuttal. I raise the score but it is still slightly below the acceptance. The reasons are 1) incremental novelty; 2) insufficient experiments. Also, I found in table 3 that, the larger-capacity model is less robust than the smaller-capacity model against white-box iterative attacks? This is strange.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "FEATURE PRIORITIZATION AND REGULARIZATION IMPROVE STANDARD ACCURACY AND ADVERSARIAL ROBUSTNESS",
            "review": "Summary: This paper argues that improved resistance to adversarial\nattacks can be achieved by an implicit denoising method in which model\nweights learned during adversarial training are encouraged to stay\nclose to a set of reference weights using the ell_2\npenalty. Additionally, the authors claim that by introducing an\nattention model which focuses the model training on more robust\nfeatures they can further improve performance. Some experiments are\nprovided.\n\nFeedback: My main concerns with the paper are:\n\n* The experimental section is fairly thin. There are at this point a\n  large number of defense methods, of which Madry et al. is only one. In\n  light of these, the experimental section should be expanded. The\n  results should ideally be reported with error bars, which would help\n  in gauging significance of the results.\n\n* The differential impact of the two contributions is not entirely\n  clear. The results in Table 1 suggest that implicit denoising can\n  help, yet at the same time, Table 2 suggests that Black-box\n  performance is better if we just use the attention model. Overall,\n  this conflates the contributions unnecessarily and makes it hard to\n  distingish their individual impact.\n\n* The section on gradient maps is not clear. The authors argue that if\n  the gradient map aligns with the image the model depends solely on\n  the robust features. While this may be (somewhat more) intuitive in\n  the context of simple GLMs, it's not clear why it should carry over\n  to DNNs. I think it would help to make these intuitions much more\n  precise. Secondly, even if this were the case, the methodology of\n  using a neural net to classify gradient maps and from this derive a\n  robustness metric raises precisely the kinds of robustness questions\n  that the paper tries to answer. I.e.: how robust is the neural net\n  classifying the gradient images, and how meaningful are it's\n  predictions when gradient maps deviate from \"clean\" images.\n\nOverall, I feel this paper has some potentially interesting ideas, but\nneeds additional work before it is ready for publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper",
            "review": "This paper proposes a new architecture for adversarial training that is able to improve both accuracy and robustness performances using an attention-based model for feature prioritization and L2 regularization as implicit denoising. The paper is very clear and well written and the contribution is relevant to ICLR. \n\nPros:\n\n- The background, model and experiments are clearly explained. The paper provides fair comparisons with a strong baseline on standard datasets.\n- Using attention mechanisms to improve the model robustness in an adversarial training setting is a strong and novel contribution \n- Both quantitative and qualitative results are interesting. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}