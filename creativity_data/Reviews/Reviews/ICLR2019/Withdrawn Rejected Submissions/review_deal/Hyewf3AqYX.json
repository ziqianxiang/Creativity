{
    "Decision": {
        "metareview": "While there was some support for the ideas presented, the majority of the reviewers did not think the submission is ready for publication at ICLR. Significant concerns were raised about clarity of the exposition.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Not ready for publication at ICLR"
    },
    "Reviews": [
        {
            "title": "Promising results but some questions about experiments",
            "review": "The paper investigates the Frank-Wolfe (FW) algorithm for constructing adversarial examples both in a white-box and black-box setting. The authors provide both a theoretical analysis (convergence to a stationary point) and experiments for an InceptionV3 network on ImageNet. The main claim is that the proposed algorithm can construct adversarial examples faster than various baselines (PGD, I-FGSM, CW, etc.), and from fewer queries in a black-box setting.\n\nThe FW algorithm is a classical method in optimization, but (to the best of my knowledge) has not yet been evaluated yet for constructing adversarial examples. Hence it is a natural question to understand whether FW performs significantly better than current algorithms in this context. Indeed, the authors find that FW is 6x - 20x faster for constructing white-box adversarial examples than a range of relevant baseline, which is a significant speed-up. However, there are several points about the experiments that are unclear to me:\n\n- It is well known that the running times of optimization algorithms are highly dependent on various hyperparameters such as the step size. But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms. Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.\n\n- Other algorithms in the comparison achieve a better distortion (smaller perturbation). Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion. Instead of reporting a single time-vs-distortion data point, the authors could show the full trade-off curve.\n\n- The authors only provide running times, not the number of iterations. In principle all the algorithms should have a similar bottleneck in each iteration (computing a gradient for the input image), but it would be good to verify this with an iteration count vs success rate (or distortion) plot. This would also allow the authors to compare their theoretical iteration bound with experimental data.\n\nIn addition to these three main points, the authors could strengthen their results by providing experiments on another dataset (e.g., CIFAR-10) or model architecture (e.g., a ResNet), and by averaging over a larger number of test data points (currently 200).\n\nOverall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.\n\n\nAdditional comments:\n\nThe introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:\n  \n* The abstract claims that the poor time complexity of adversarial attacks limits their practical usefulness. However, the running time of attacks is typically measured in seconds and should not be the limiting element in real-world attacks on deep learning systems. I am not aware of a setting where the running time of an attack is the main computational bottleneck (outside adversarial training).\n\n* The introduction distinguishes between \"gradient-based methods\" and \"optimization-based methods\". This distinction is potentially confusing to a reader since the gradient-based methods can be seen as optimization algorithms, and the optimization-based methods rely on gradients.\n\n* The introduction claims that black-box attacks need to estimate gradients coordinate-wise. However, this is not the case already in some of the prior work that uses random directions for estimating gradients (e.g., the cited paper by Ilyas et al.)\n\nI encourage the authors to clarify these points in an updated version of their paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A method to produce adversarial attack using a Frank-Wolfe inspired method",
            "review": "This paper provide a method to produce adversarial attack using a Frank-Wolfe inspired method. \n\nI have some concerns about the motivation of this method: \n - What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.  \n - Consequently why did not you compare simple projected gradient method ? (BIM) is not equivalent to the projected gradient method since the direction chosen is the sign of the gradient and not the gradient itself (the first iteration is actually equivalent because we start at the center of the box but after both methods are no longer equivalent).\n - There is no motivations for the use of $\\lambda >1$ neither practical or theoretical since the results are only proven for $\\lambda =1$ whereas the experiments are done with \\lambda = 5,20 or 30.\n - What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?\n \nDepending on the answer to these questions I'm planning to move up or down my grade.\n\n In the experiment there is no details on how you set the hyperparameters of CW and EAD. They use a penalized formulation instead of a constrained one. Consequently the regularization hyperparameters have to be set differently.\n\n The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods. \n\nComment: \n- in the whole paper there is $y$ which is not defined. I guess it is the $y_{tar}$ fixed in the problem formulation Sec 3.2.  In don't see why there is a need to work on any $y$. If it is true,  case assumption 4.5 do not make any sense since $y = y_{tar}$ (we just need to note $\\|\\nabla f(O,y_{tar})\\| = C_g$) and some notation could be simplified setting for instance $f(x,y_{tar})  = f(x)$.\n- In Theorem 4.7 an expectation on g(x_a) is missing\n\nMinor comments: \n- Sec 3.1 theta_i -> x_i\n- Sec 3.3 the argmin is a set, then it is LMO $\\in$ argmin.\n\n===== After rebuttal ======\nThe authors answered some of my questions but I still think it is a borderline submission.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper, a bit problematic experimental set-up",
            "review": "The paper proposes using the Frank-Wolfe algorithm for fast adversarial attacks. They prove upper bounds on the Frank-Wolfe gap and show experimentally that they can attack successfully much faster than other algorithms. In general I find the paper novel (to the best of my somewhat limited knowledge), interesting and well written. However I find the white-box experiments lacking as almost every method has 100% success rate. Fixing this would significantly improve the paper.\n\nMain remarks:\n- Need more motivation for faster white-box attack. One good motivation for example is adversarial training, e.g. Kurakin et al 2017 ‘ADVERSARIAL MACHINE LEARNING AT SCALE’ that would benefit greatly from faster attacks\n\n- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare. Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence. Also stating the 100% success rate in the abstract is a bit misleading for the this reason.\n\n-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other \nattack seems odd. \n\n-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.\n\n- Regarding lambda>1, you write that “we argue this modification makes our algorithm more general, and gives rise to better attack results”. I did not see any theoretical or empirical support for this in the paper. Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial. Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.\n\n- The authors claim that this is the first zeroth-order non-convex FW convergence rate, I am not familiar enough with the field to evaluate this claim and its significance.\n\n- Alg. 1 for T>1 is very similar to I-FGM, but also ‘pulls’ x_t towards x_orig. It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates. This gives nice insight into why this should intuitively work better.\n\n- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?\n\n- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps…”\n\nMinor remarks:\n- In remark 4.8 in the end option I and II are inverted by mistake\n\n- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number. \n\n- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}