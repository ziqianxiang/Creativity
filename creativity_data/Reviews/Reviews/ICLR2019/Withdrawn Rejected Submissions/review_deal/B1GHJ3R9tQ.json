{
    "Decision": {
        "metareview": "All of the reviewers find this paper to contain interesting ideas. Originally, clarity was a major issue, although a few issues remain (see the comments of reviewer 3). The reviewers believe that the paper has been substantially improved from its original form, however there is still room for improvement: more comprehensive comparisons to existing work (reviewer 1), careful ablations (reviewer 3), etc. With a little bit of polish, this paper is likely to be accepted at another venue.\n\nI am certainly not penalizing you for anonymously sharing your code on Github, as this was specifically requested by reviewer 1.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting work that requires a bit of fine tuning."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper proposes a technique for learning a distribution over parameters of a neural network such that samples from the distribution correspond to performant networks. The approach effectively encourages sampled parameters to have low loss on the training set, and also uses an adversarial loss to encourage the distribution of parameters to be Gaussian distributed. This approach can improve performance slightly by using ensembling and can be useful for uncertainty estimates for out-of-distribution examples. The approach is tested on a few simple problems and is shown to work well.\n\nI am definitely in favor of exploring adversarial divergences (using a critic as a differentiable loss to compare two distributions) in unusual settings, and this paper certainly does this. The idea of transforming samples from a prior such that the transformed sample corresponds to useful network parameters is interesting. The results also seem promising. However, currently the mathematical description of this method is completely unclear and ridden with many errors. I can understand at a reasonable level what the approach is doing from Figure 1, but the definitions and equations given in Equation 3 are at times nearly incomprehensible \"mathiness\". I'm giving the paper a borderline accept because the idea is interesting and the results are OK; I will raise my score if Section 3 is dramatically improved. I give some specific examples of issues with Section 3 in my specific comments below. I'd also note that the paper does a somewhat poor job comparing to existing work - only section 4.2 includes a comparison to existing \"uncertainty\" methods. This should also be improved - the authors should implement the existing methods and use them as a point of comparison in all of their experiments. As a final high-level note, the approach is described at various points as an \"autoencoder\" particularly in reference to the adversarial autoencoder. However, the approach does not \"autoencode\" anything - there is no reconstruction term, or input apart from the noise samples. The only thing it has in common with the adversarial autoencoder is the use of a critic to enforce a distributional constraint. Calling it, or comparing it to, an autoencoder is confusing and misleading.\n\nSpecific comments:\n\n- You mention fast weights in related work. I believe Hinton and Plaut were the first to propose fast weights in \"Using Fast Weights to Deblur Old Memories\", and I'd also suggest mentioning \"Using Fast Weights to Attend to the Recent Past\" which is a more recent demonstration that fast weights can be useful on modern problems.\n- The are some issues with your description of Equation 1: First, I don't believe you define G(z) (I assume it is the \"decoder\" network; please define it). Second, in practice I don't believe you actually use JSD or MMD for D_z; you use a critic architecture which in some limit approximates some statistical divergence but in practice they typically don't (see e.g. Arjovsky and Bottou 2017; Fedus et al. 2017; Rosca et al. 2018). Third, writing Q_z \\sim Q(z | x) seems strange to me - Q_z is a distribution, and I don't believe that Q(z | x) is a distribution over distributions, so how are you sampling a distribution (Q_z) from Q(z | x) as suggested by the use of the \\sim notation? I think you simply mean that Q_z is Q(z | x) approximately marginalized over x.\n- Equation 2 is also not clear. First, the sentence before starts \"Suppose the real parameters \\theta^* \\sim \\Theta...\" The equation itself does not include \\theta^* or \\Theta so I don't see what this is referring to. Second, the expression for an m-dimensional is written \\mathcal{N}(0, \\sigma^2, I_m). It's not clear why there is a comma before I_m, and I_m is not defined (though I assume it is the m \\times m identity matrix) - did you mean to multiply I_m by \\sigma^2? Third, it looks like you actually define P_z twice, once as \"an m-dimensional isotropic Gaussian\" and again as \"a Kd-dimensional isotropic Gaussian\"; am I to infer that m = Kd? Why use both? Fourth, you mention the joint P(x, y) but the expectation is taken over P_x and P(y | x). Why call it P_x and not P(x)? And why not compute the expectation over P(x, y)? Fifth, you write \"Here the encoder...\" -- you never define that Q(z) or G is \"the encoder\", I assume Q(z). It is strange to take the expectation over Q(z) (I assume sampling z \\sim Q(z)) but then have the term Q(z) appear in (2). How are Q(z) and Q_z related? On that note, I don't see how (2) is an autoencoder, since there is no Q(z | x) term. It appears instead that you are sampling z from Q(z) which doesn't condition on x. So what is being autoencoded? Related, you write \"all the q_k (that will generate different layers) will be correlated, unlike dimensions of z which are drawn to be independent from each other.\" But if Q(z) = [q_1, ..., q_K] then doesn't the secont term in (2) suggest that they are being enforced to be similar to the prior P_z, and therefore uncorrelated? Note that you also say later on \"The job of the regularizer D_z(P_z, Q_z) is to force each embedding q_n to approximate P_z.\" Frankly at this point I will stop pointing out issues with this equation and discussion since they are so widespread.\n- In your definition of your ensemble scoring rule, you are taking the sum over N + 1 elements (n = 0 to N) but dividing by N.\n- In 4.2, do you use the same model architecture/training/regularization etc. as in previous studies? If not I think comparing the different methods will be conflated by differences in training procedures. Since you do not report results in many experimental settings, I assume you don't.\n- In Figure 3, why not plot the true standard deviation around the true function? It appears you are only plotting +/- 3 stndard deviations for the learned function.\n- Why not include 100 models L2 on Figure 3?\n- It's not clear to me why you define your \"disagreement d\" when it appears the same as the entropy score you used in 4.4.\n- A stronger and more convincing attack would be to attack the ensemble of models, instead of attacking a single model and testing on the ensemble.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good experimental results but lacking rigour",
            "review": "This works propose a new approach to learn to sample (or generate) the parameters of a deep neural networks to solve a task. They propose a new architecture inspired by hyper networks and adversarial auto-encoders, where the parameters of the networks are generated from a low dimensional latent space. By using an ensemble of networks sampled with their approach they're able to get state of the art results on uncertainty estimation.\n\nThe notations are confusing and the paper contains several mistakes. In particular:\n- P_z is used to represent different distributions. It sometimes refers to the distribution of the latent variables and sometimes to the prior over the weight embeddings. Different notation should be used to represent different quantity.\n- D_z sometimes refers to the regularization term or to the discriminator.\n- Eq 2. I believe there is a bug in the equation, the expectation is over Q(z) but it should be P_z (distribution of the latent variable z), otherwise it doesn't make much sense.\n- The equation for the cross entropy is wrong. If y_i are the true labels and F(x_i, theta) is the prediction then it should be y_i*log(F(x_i, theta)).\n- It's not clear if the loss of the discriminator should be maximized for the parameters of the discriminator and minimized with respect to the parameters of the encoder. Furthermore it would be interesting to study what is the impact of this particular choice of loss for the discriminator. In particular I invite the author to compare the loss proposed to the loss in [1].\nFixing these, would make the paper much easier to understand.\n\nThe authors motivates their approach by drawing a link with wasserstein (WAE) and adversarial auto-encoders. While this could be interesting I think this link should be made more formal. \nIndeed, the WAE is derived from the wasserstein distance between the true data distribution and the distribution of the model. However it's not clear if the approach proposed can still be derived from such a principle. I would invite the author to make the link between wasserstein distance minimization and their approach more explicit.\n\nTo my knowledge the method proposed is novel, however using implicit posterior to learn the weights is not novel and several other works have looked at it. In particular I think [1,2] should be discussed in the related work. \nThe difference with traditional bayesian approach such as variational inference should also be discussed, since the approach is really close to approximating the posterior with an implicit distribution and computing the KL term using a GAN (like in [3,4]).\n\nI think one interesting novelty that needs to be emphasized is that the model has both: parameters that are point estimates (the parameters of the generators) and parameters that are sampled from a posterior distribution (the weight embeddings). \n\nPros:\n- Good and promising experimental results.\n\nCons:\n- The paper combines several tricks and ideas but it's not really clear what is important and why such an approach works. For example how important is the latent space and the encoder ? Could we just sample directly the weight embeddings from a gaussian and remove the regularization ?\n- The other points mentioned above about the clarity of the paper.\n\nOthers:\n- The title is misleading, the manifold is not really explored... If the author really want to explore the manifold some interesting questions are:  what happens if we try to interpolate between two latent variables ? What do the latent variables represent ? what's the influence of the dimension of the latent space ?\n- In the experiments: what is the number of networks used for the other methods ?\n- It would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5.\n\nConclusion:\nThe experimental results seem promising however the motivation for the approach is not clear. I think fixing some of the points mentioned above could greatly improve the clarity of the paper and make it a stronger submission. In the current state I don't believe the paper is rigorous enough to be accepted.\n\nReferences:\n[1] Pawlowski, N., Rajchl, M., & Glocker, B. (2017). Implicit weight uncertainty in neural networks. arXiv:1711.01297.\n[2] Wang, K. C., Vicol, P., Lucas, J., Gu, L., Grosse, R., & Zemel, R. (2018, July). Adversarial Distillation of Bayesian Neural Network Posteriors. ICML\n[3] Mescheder, L., Nowozin, S., & Geiger, A. (2017, July). Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML\n[4] Huszár, F. (2017). Variational inference using implicit distributions. arXiv:1702.08235.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea",
            "review": "TL;DR. I find the manuscript to contain interesting ideas, yet I believe there is room for improvement.\n\n* Summary\n\nFor any given specific network architecture, the manuscript aims at learning a distribution over the weights (rather than point-wise estimates of the weights). This is achieved through using a two-steps procedure, in which a \"hypernetwork\" is trained to output weights for the network of interest, and a GAN is then used to (adversarially) generate samples from a distribution $Q$ which is assumed not too far (in a KL sense) from a Gaussian prior $P$.\n\n* Major issues\n\nI find the central idea to be of interest to the ICLR community. However I have found a number of shortcomings to be addressed before I could recommend acceptance. The following list is in no particular order.\n\n- References: 20 out of 22 (!) references are preprints, about half of which are 3+ years old. Most of them are now published in proceedings and I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.\n- Links with Bayesian deep learning: I feel this should be more carefully discussed in the manuscript. The sentence \"We have proposed a generative, non-Bayesian solution [...]\" should be explained, as from what I gather HyperGAN samples (GAN-like) weights (i.e., networks) from a distribution $Q$ which is deemed not too far (in the KL sense) from a prior distribution $P$. How is that not Bayesian?\n- Numerical experiments. Table 4: what are the numbers reported? If single evaluation, I do not believe any conclusion may be drawn. If averages over multiple repetitions, no conclusion can be drawn without reporting standard deviations. In addition, I do not quite grasp the purpose of the 1D toy example.\n- Overall, I think the authors should try and make their contributions and method clearer. For example, a pseudo-code of the whole procedure might help readers understand the gist. \n- Architecture specific: I find the claim that HyperGAN explores the manifold of neural nets too strong. As the whole procedure is architecture-specific, I would find more appropriate to change that claim to \"exploring the weights distribution for a specific architecture\".\n- Code availability: the scope of the paper is diminished by the fact that no code is available by the time of review. A toolbox (not disclosing the authors' identities) should be made available to support the manuscript claims. Last sentence (page 8) is likely to be outdated and should be removed. \n\n* Minor issues\n\n- some typos: architecture (caption figure 1), $G(Q(z))$ (missing parenthesis, page 3), sum index $n$ not used in the last equation (page 7).\n- \"Perhaps the first proposed method...\" (page 2). Such imprecise statements must be avoided.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}