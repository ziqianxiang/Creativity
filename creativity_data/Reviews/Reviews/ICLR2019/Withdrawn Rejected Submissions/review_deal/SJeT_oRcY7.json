{
    "Decision": {
        "metareview": "This paper presents a biologically plausible architecture and learning algorithm for deep neural networks.  The authors then go on to show that the proposed approach achieves competitive results on the MNIST dataset.  In general, the reviewers found that the paper was well written and the motivation compelling.  However, they were not convinced by the experiments, analysis or comparison to existing literature.  In particular, they did not find MNIST to be a particularly interesting problem and had questions about the novelty of this approach over past literature.  Perhaps the paper would be more impactful and convincing if the authors demonstrated competitive performance on a more challenging problem (e.g. machine translation, speech recognition or imagenet) using a biologically plausible approach. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting demonstration of a biologically plausible neural network but analysis and compelling experiments are lacking"
    },
    "Reviews": [
        {
            "title": "Well executed, but not exploring challenging questions",
            "review": "Summary:\nThe authors propose a benchmark of biologically plausible ANNs on the MNIST dataset with an emphasis on local learning rules (ruling out backpropagation, and enforcing small receptive fields). They find that random projection (RP) networks provide good performance close to backpropagation and outperform other local learning rules based on unsupervised learning.\n\n\n\nEvaluation:\nA well-executed work but with major limitations: it is based mostly on MNIST, analysis of spiking network is limited, and deep biologically plausible learning rules are not investigated.\n\nDetailed comments:\n\nWhile the paper reads well, choosing how to evaluate the contribution for such benchmark paper is a bit difficult, as the novelty is by definition more limited than in papers proposing a new approach.\nIn the following I chose to focus on what information such benchmark may bring to the field for addressing to challenges ahead.\n\n1.\tStrengths\nThe authors made the effort of implementing several biologically plausible learning rules, including Feedback alignment, and sparse coding. In particular, the idea of using local unsupervised learning rules as baselines for learning the hidden layer is a good idea to extend the range of tested algorithms.\n\n2.\t“Easy” dataset\nIt is unclear to me in which way MNIST result can help evaluate the next challenges in the field. While it is good to know that simple algorithms can achieve close to state of the art, I am not sure this is enough for a paper submitted in 2018. Ideally, most of the analysis could be reproduced at least for CIFAR10 (as the authors started to do in table 2).\n\n3.\tLimited architectures\nMost of the analysis is restricted to one single layer. However, biologically plausible algorithms have also been proposed that can in principle apply to multiple layers. In addition to feedback alignment (implemented in the manuscript in the single hidden layer case), you can find relatively simple approaches in the literature, for example\n“Balduzzi, David, Hastagiri Vanchinathan, and Joachim M. Buhmann. Kickback Cuts Backprop's Red-Tape: Biologically Plausible Credit Assignment in Neural Networks. AAAI. 2015.” Given the dominant view that depth is key for learning challenging datasets, not exploring this option at all in a benchmark seems a significant weakness.\n\n4.\tSpiking networks\nWhile the authors seem to emphasize spiking as an important aspect of biological plausibility (by using LIF neurons and STDP). The challenges of such approaches seem to be largely unaddressed and the main take home message is a performance similar to the corresponding rate models. It would be very interesting, for example, to see how many spikes (or spikes per neurons) are need per example to achieve a robust classification.\n\n5.\tOverall objective behind biological plausibility\nExtending the previous point, the results are to some extent limited to accuracy. If one wishes to achieve biological plausibility, more aspect can be taken into consideration. For example:\n-\tDuring test: the average number of activated neurons, the average number of activated synapses. \n-\tDuring training: the overall number of activations needed to train the algorithm.\nIn relation to these consideration, a more concrete discussion about the potential benefits of biological plausibility would be helpful.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "No significant contribution",
            "review": "In this work authors benchmark a biologically plausible network architecture for image classification. The employed architecture consists of one hidden layer, where input to hidden layer weights W1 are either trained with PCA or sparse coding, or are kept fixed after random initialization. The output layer units are modeled as leaky integrate-and-fire (LIF) neurons and hidden to output connections W2 are tuned using a rate model that mimics STDP learning dynamics in the LIF neurons. The authors compare classification results on MNIST and CIFAR10 datasets, where they also include results of an equivalent feed-forward network that is trained with standard error backpropagation.\n\nThe authors find that in the bio-plausible network with a large hidden layer, unsupervised training of input to hidden layer weights does not lead to as good of a classification performance as achieved through fixed random projections. They furthermore find that localized patch-style connectivity from input to hidden layer further improves the classification performance.\n\nOverall the paper is well-written and easy to follow, but I fail to see any significant contribution in this work. As compared to the findings of Hubel & Wiesel, how bio-plausible are random projections for low-level feature representation? One may also argue that unsupervised tuning of W1 may require a lot more training data than available in MNIST or CIFAR10. The authors also need to take the capacity of their network into account; they draw conclusions based on a biologically-plausible network, but one that only has two feed-forward layers. It is hard to imagine that a more accurate biologically-plausible vision model would prefer random projections over low-level feature extractors that are well-tuned to the input statistics.\n\nRegarding the observation that localized fields perform better than densely connected layers, I find it simply in line with physiological findings (starting from the work of Hubel & Wiesel) and artificial neural network architectures they inspired like CNNs.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Single-layer SNN training on different unsupervised preprocessing",
            "review": "This article compares different methods to train a two-layer spiking neural network (SNN) in a bio-plausible way on the MNIST dataset, showing that fixed localized random connections that form the hidden layer, in combination with a supervised local learning rule on the output layer can achieve close to state-of-the-art accuracy compared to other SNN architectures. The authors investigate three methods to train the first layer in an unsupervised way: principal component analysis (PCA) on the rates, sparse coding of activations, and fixed random local receptive fields.  Each of the methods is evaluated on the one hand in a time-stepped simulator, using LIF neurons and on the other hand using a rate-approximated model which allows for faster simulations. Results are compared between each other and as reference with  standard backpropagation and feedback alignment.  The main finding is that localized random projections outperform other unsupervised ways of computing first layer features, and with many hidden neurons approaches backpropagation results. These results are summarized in Table 8, which compares results of the paper and other state-of-the-art and bio-plausible SNNs. PCA and sparse coding work worse on MNIST than local random projections, regardless if the network is rate-based, spike-based or a regular ANN trained with the delta rule. Feedback Alignment, although only meant for comparison, performs best of the algorithms investigated in this paper.\n\nIn general the question how to train multi-layer spiking neural networks in a bio-plausible way is very relevant for computational neuroscience, and has attracted some attention from the machine learning community in recent years (e.g. Bengio et al. 2015, Scellier & Bengio 2016, Sacramento et al. 2018). It is therefore a suitable topic for ICLR. Of course the good performance of single-layer random projections is not surprising, because it is essentially the idea of the Extreme Learning Machine, and this concept has been well studied also for neuromorphic approaches (e.g. Yao & Basu, 2017), and versions with local receptive fields exist as well (Huang et al. 2015 \"Local Receptive Fields Based Extreme Learning Machine\"). While the comparison of different unsupervised methods on MNIST is somehow interesting, it fails to show any deeper insights because MNIST is a particularly simple task, and already the CIFAR 10 results are far away from the state-of-the-art (which is >96% using CNNs). Another interesting comparison that is missing is with clustering weights, which has shown good performance for CNNs e.g. in (Coates & Ng, 2012) or (Dundar et al. 2015), and is also unsupervised.\n\nThe motivation is not 100% clear because the first experiment uses spikes, and shows a non-negligible difference to rate models (the authors claim it's almost the same, but for MNIST differences of 0.5% are significant). All later results are purely about rate models. The authors apparently did not explore e.g. conversion techniques as in (Diehl et al. 2015) to make the spiking results match the rate versions better e.g. by weight normalization.\n\nI would rate the significance to the SNN community as average, and to the entire ICLR community as low. The significance would be higher if it was shown that this method scales to deeper networks or at least can be utilized in deeper architectures. Scrutinizing the possibilitites with random projections on the other hand could lead to more interesting results. But the best results here are obtained with 5000 neurons with 10x10 receptive fields on images of size 28x28, thus the representation is more than overcomplete, and of higher complexity than a convolution layer with 3x3 kernels and many input maps.\n\nBecause the results provide only limited insights beyond MNIST I can therefore not support acceptance at ICLR.\n\nPros:\n+ interesting comparison of unsupervised feature learning techniques\n+ interesting topic of bio-plausible deep learning\n\nCons:\n- only MNIST, no indications if method will scale\n- results are not better than state-of-the-art\n\n\nMinor comments:\n\nThe paper is generally well-written and structured, although some of the design choices could have been explained in more detail. Generally, it is not discussed if random connections have any advantage over other spiking models in terms of accuracy, efficiency or speed, besides the obvious fact that one does not have to train this layer. \n\nThe title is a bit confusing. While it's not wrong, I had to read it multiple times to understand what was meant.\n\nThe first sentence in the caption for Fig. 2 is also confusing, mixing the descriptions of panel A and B. Also, in A membrane potentials are shown, but the post-membrane potential seems to integrate a constant current instead of individual spikes. Is this already the rate approximation of Eq. 2? Or is it because of the statement in the caption that they both receive very high external inputs. In general, the figures in panel A and B do not make the dynamics of the network or the supervised STDP much clearer. \n\nPrincipal Component Analysis and Sparse Coding are done algorithmically instead of using a sort of nonlinear Hebbian Learning as in Lillicrap 2016. It would have been interesting to see if this changes the comparatively bad results for PCA and SC.\n\nIn Fig. 3, the curve in the random projections case is not saturated, maybe it would have been interesting to go above n_h = 5000. As there are 784 input neurons, a convolutional neural network with 7 filter banks already would have around 5000 neurons, but in this case each filter would be convolved over the whole image, while with random projections the filter only exists locally. \n\nIn Eq. 1, the notation is a bit ambigous: The first delta-function seems to be the Dirac-delta for continuous t, while the second delta is a Kronecker-delta with discrete t.\n\nIn A.1 and A.4.2 it is stated that the output of a layer is u_{t+1} = W u_t + b but I think in both cases it should be W a_t + b where a_t = phi(u_t). Otherwise, you just have a linear model and no activations. \n\nIn Table 3, a typo: \"eq. equation\"   ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}