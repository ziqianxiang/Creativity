{
    "Decision": {
        "metareview": "Reviewers have concerns about poor writing of the paper, lack of technical novelty, and the methodology taken by the paper not being very principled. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Concerns with writing and technical novelty"
    },
    "Reviews": [
        {
            "title": "Review of SST for SSL",
            "review": "Summary:\nIn the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.\n\nStrong points:\nIt is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.\n\nWeak points:\n1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey. Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter. Therefore, the technical contribution of this paper is moderate.\n\nZhu, Xiaojin. \"Semi-supervised learning literature survey.\" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.\n\n 2) The writing is poor and hard to follow. First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing. For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria? From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations. The descriptions of the datasets used are not clear, e.g., the number of classes for each data. Second, many typos and grammar errors need to fix, e.g., \"the proposed SST is suitable for lifelong learning which make use...\", \"the error 21.44% was lower than\" 18.97?\n\n3) The overall performance of the proposed SST in the experiments is not convincing and not promising. First, the labeled data portion is fixed and is relatively high compared to most standard semi-supervised learning settings. Second, SST itself is only comparable with or even worse than the state-of-art methods. Combining SST with other existing techniques can help. However, the additional cost is expensive. Further demonstrations are necessary for the proposed SST method.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Selective Self-Training for semi-supervised Learning",
            "review": "This is an novel, interesting paper on an important topic: semi-supervised learning.\nEven though the proposed approach seems to have significant potential, the experimental\nis somewhat disorganized,  and it also includes some weak claims that should be removed. \n\nFor example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).  In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.\n \nIn 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?\n\nThe explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for \"stratified SSL.\" Without this extra work, your claim is just a conjecture.\n\nYou should also show the performance of regular SSL methods in the setup on Table 4.\n\nLast but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;\nhowever, you do not provide any evidence for it, so you should avoid making such claims.   \n\nOther comments:\n- on page 2, the two terms classification & selection network appear \"out of the blue;\" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.\n- figures 2 & 3 should be a lot larger in order to be readable\n- 4.1.2 top of page 7: claims such as \"SST could have obtained better performance\" have no place in such a paper; you could instead make a note about the method being \"prohibitively CPU intensive for the time being\"\n- lower on the same page you say: \"SST may get better performance\" - see above\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: how principled the method is and how experimental evaluation confirms the benefits and applicability of the method",
            "review": "This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough. The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection. On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.\nThe paper is written in a way that makes following it a bit difficult, for example, the experimental setups. Also, the writing can be improved by making the writing more concise and formal (examples of informal: \"spoil the network\", \"model is spoiled\", \"problem of increased classes\", \"many recent researches have been conducted\", \"lots of things to consider for training\", \"supervised learning was trained\" etc.). The contributions of the method could also be underlined more clearly in the abstract and introduction. The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments. \nThe idea of selective sampling for self-training is promising and the investigated questions are interesting. As far as I understand, the main contribution of this paper is the use of separate \"selection network\" to estimate the confidence of predictions by \"classification network\". However, as the \"selection network\" uses exactly the same input as \"classification network\", it is hard to imagine how it can learn additional information. For example, imagine the case of binary classification. If the selection network predicts 0 in come cases, it can be used to improve the result of \"classification network\" by flipping the corresponding label. How can you interpret such a thought experiment? One could understand the use of \"selection network\" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of \"selection network\" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds. Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of \"classification network\" is greater than some threshold? Finally, could you show a plot of top-1 prediction of \"classification network\" vs score of \"selection network\" and elaborate on that?\nThen, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes. Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including \"selection network\" with threshold) is not very principled. Ablation study shows that the use of the \"selection network\" strategy does not improve the results without these heuristics. It would be interesting to see how these heuristics would do without \"selection network\", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG. In the current form of evaluation, it is hard to say if there is any benefit of using the \"selection network\" that is the main novelty of the paper.\nIt is very valuable that the experimental results include many recently proposed methods. Besides, the settings are described in details that could help for the reproducibility of the results. However, I have a few concerns about the results. First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3). Besides, as the base classifier is different for various baselines, it is hard to compare the methods. Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2). How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice? Another important parameters is the number of iterations of the algorithm. How was it chosen? Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes? What would happen if you use random class splits or split animal classes (like in a more realistic scenario)? \nTo conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.\n\nSome questions and comments:\n- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\n- In the training procedure of \"selection network\" of Sections 3.1, do you use the same datapoints to train a \"classification network\" and \"selection network\"? If it is the case, how do you insure that the \"classification network\" does not learn to fit the data perfectly and thus all labels s_i are 1?\n- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?\n- What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\n- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.\n- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?\n- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\n- Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?\n- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}