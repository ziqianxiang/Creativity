{
    "Decision": "",
    "Reviews": [
        {
            "title": "Important benchmark for an interesting problem. Some key analysis missing",
            "review": "Summary:\nThis paper introduces a large-scale benchmark for the task of image captioning in which the goal is for the caption to be as engaging as humans as possible. The dataset consists of 200K + captions spanning 200+ personality traits. The dataset includes traits like “Kind”, “Mellow”, “Creative”, “Contradictory” etc.  The authors also analyze the performance of models built on top of recent approaches in image and sentence representations on both caption generation and caption retrieval task.  The paper consists of two main experiments: (1) they evaluate their architecture on MS-COCO and Flickr30K dataset to assess the model’s ability to describe the image factually and in a neutral tone. They show similar performance to existing state-of-the-art approaches on caption-generation task and show best retrieval numbers on Flickr30K dataset. \n(2) They also perform human evaluation of personality-captions and found that humans tend to prefer these captions over the neutral captions\n\nPros:\n- The paper introduces a first large-scale dataset for personality captions that has 215 traits and ~200,000 images. It is significantly bigger than past datasets which consist of a only a few thousand captions covering a small set of categories.\n- The analysis of models is done not only on the personality-captions dataset but these models are also evaluated on the traditional COCO dataset. \n- The paper evaluates both retrieval and caption generation models on both these datasets and also perform human evaluation for completeness. \n- The paper is well written and is backed-up well by details mentioned in the supplementary material.\n\nCons:\n- The paper doesn’t analyze and compare the benchmark with existing datasets. For instance, readers might be interested in  statistics of the dataset compared to existing datasets: \n    - Does it produce more diverse captions (are the captions longer) \n    - Is the caption vocabulary bigger than existing dataset? What’s the overlap with existing dataset. \n    - Does it have more unique words per sentence position) \n- It has been shown in the past that collecting multiple captions for the same image leads to more robust evaluation using automatic metrics. However, the dataset contains just one caption per image. \n- The paper also didn’t perform ablation studies on their model to know the source of performance gains. For instance, it would be interesting to see the performance of captioning model trained on ResNeXT-IG-3.5B and COCO dataset. What is the contribution of supervision of more captions coming from the new dataset vs (better?) image features extracted from ResNeXT-IG-3.5B \n- Although the paper does mention human evaluation of which caption is more engaging, there is no human baseline to compare the models to for personality-caption test set using automatic metrics. \n\n\nSome other questions/remarks:\n- During caption collection, caption-writers were asked to write “a comment” instead of a “caption”. I wonder why this instruction was added? \n- Compared to the StyleNet paper by Gan et al (2017), it seems like the instructions were quite different to collection captions. They observed that collecting captions by just mentioning the style resulted in captions that were not visually grounded. Therefore, they setup the interface such that factual captions are changed to match the style. Can the authors comment on their choice of the interface as compared to the one described in the StyleNet paper?\n- There is also some value in evaluating whether the caption conformed to the personality mentioned while generating caption. Although there is a bit of discussion about this in the diversity of captions in the supplementary, I would be curious to see a bit more analysis on this. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new dataset for image captioning",
            "review": "This paper presents a new image captioning dataset. The image captions were created in the context of a \"personality\" prompt, e.g. \"happy\" or \"sad\". The dataset is used to train personality-aware image captioning models and caption retrieval models. The models are standard image captioning models or image--sentence ranking models with additional encodings of the personality trait. I am sympathetic to the argument that existing resources for image captioning make it a \"neutral\" language generation task, and this dataset may be useful for future research in image captioning. But most of the details of the new dataset are relegated to the appendices, and most of the results are devoted to the value of using pre-trained embeddings or CNNs.\n\nExperiments are conducted on the COCO dataset and the the personality captions dataset. The experiments also focus on the role of using more complex image recogniiton models. The results show that using a better visual recognition model leads to improvements in captioning and retrieval performance. This is not surprising and the same basic result has been down in many papers, going back to Donahue et al. (CVPR 2015), who showed that using VGG was better than using CaffeNet on captioning and retrieval tasks.\n\nThe human evaluation is commendable but there are insufficient details about how you carried out the experiment. Did you conduct this experiment on a crowd-sourcing platform? How reliable are the results of five human annotators? The difference is claimed to be a \"statstically significant result\", so please present the test statistic in the paper.\n\nWhat exactly does it mean that you \"learn an embedding for each train and concatenation it with each input of the encoder\"? Is the personality trait input to the encoder at every timestep? Is it also used to initialise the hidden state of the encoder? Please provide more details.\n\nThere are many references to the language generation model as a \"generative\" model. I was confused by this term, which I expected to be used to describe an actual generative model of p(x, y), whereas your captioning models estimate p(y|x).\n\nThe last sentence of Section 2 is missing a citation \"... large-scale pretraining (?) ...\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The authors approach the image captioning problem conditioned on personality \"traits\"",
            "review": "\nThe technical contribution of this paper is reduced to adding a \"personality embedding\" to approach the image captioning problem (authors also propose a new task and data set, but motivation is not convincing). As side contributions the authors will release a data set on image captioning conditioned on personality \"traits\".  I think the authors fail at motivating the proposed task, see comments below. Also, they fail at emphasizing the technical contribution of the paper (see below as well). Finally, authors do not put in context their work, in terms of CV studies on personality analysis. \n\n\nI think the authors should elaborate more on scenarios/applications were personality-conditioned captions would be of interest to humans!, with reference to the example in Figure 1: why one would a computer to generate a subjective caption biased towards a particular personality aspect (which has to be chosen by the user, I guess)?. There are 7 subjective captions, all of them could be valid, but these are both, subjective and they surely do not cover the whole spectrum one can think of. I am not convinced of the relevance of the task, I mean is an interesting experiment, but what impact will it have in terms of applications? I think this is critical, since one of the contributions of the paper is the new dataset. \n\nAuthors mention that humans prefer their labelings ˜50% of the time (wo statistically significant difference), this actually sounds as low performance, can you please elaborate on why is this a good result?\n\nThe authors make use \"personality traits\" to make reference to a list of adjectives, actions, etc. Why the authors consider these as personality traits? What is the support for that list? There are well established notions of personality traits from the psychology field, why not adhering to them?, authors should also make an effort to put in context their work in terms of personality-analysis efforts from the CV community, see e.g.: Jacques et al. First Impressions: A Survey on Computer Vision-Based Apparent Personality Trait Analysis. ArXiv 2018\n\nWhy are missing in Table 5 the results obtained by the ResNet152 image encoder without personality encoder?\n\nThe description of the personality encoder in section 4.2 is rather brief!, this is actually the only technical contribution of the paper and the authors fail at emphasizing it. \n\nHow do the authors define engagement to annotators? More details on the human evaluation should be provided to determine the validity of the conclusions derived from this study \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}