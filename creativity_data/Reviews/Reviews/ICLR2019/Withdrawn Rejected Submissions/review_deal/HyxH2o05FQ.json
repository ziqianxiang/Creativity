{
    "Decision": "",
    "Reviews": [
        {
            "title": "limited novelty",
            "review": "This paper provides an analysis on transfer learning by fine-tuning. It focuses on the importance of the relation between the content of the source dataset used for pre-training and the following target dataset on which the model will be applied. Several experiments show that this aspect is more important than the cardinality of the source data and this becomes particularly clear when dealing with fine-grained classification tasks in target.\n\nThe main technical contribution of this work is in the application of an importance weighting approach to select the data when training the source model.  The theory in section 3.3 is inherited from previous works and the only variant is in the way in which it is applied when the source and target class set is different: the authors propose a heuristic approximation of P_t(y) based on target self-labeling.\n\n- I find the technical novelty of this work very limited. Indeed here we just see a pre-existing sample-selection based unsupervised domain adaptation method applied on several datasets. \n- more details about the self-labeling procedure should be provided: of course the prediction of the source model on the target varies during model training, being very poor in the beginning and possibly improving in the following epochs. Is he sample selection active since the very first epoch?\n- The comparison in table 4 with state of the art should be better explained. Are the other baseline methods using the same AmoebaNet architecture? If not it is not the comparison might be unfair.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A paper with limited novelty",
            "review": "This paper is of limited technical contribution. The proposed method in Section 3.3 is just too close to covariate shift where importance weighting has been widely used. I donâ€™t know whether authors are aware of these works.\n\nThe findings listed in Section 1.1 are obvious and intuitive. There is no interesting finding.\n\nIt is better to move the introduction of the datasets to the experimental section.\n\nA typo: hand-curated -> handcrafted",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review for \"Domain Adaptive Transfer Learning\"",
            "review": "This paper tackles the problem of transfer learning. The approach\nproposed is simple but effective. It identifies the source training\nexamples that are most relevant for the target task and then over-samples \nthese examples when pre-training the classification network. The\n over-sampling is based on importance weights measuring the ratio \nof the prior probability for each source label in the target and source datasets. As not all source\nlabels will necessarily be present in the target dataset, the target\nprior probability for a source label is estimated by learning a\ngeneric classifier on the source dataset, applying it to each example\nin the target dataset, computing the average output probability for\neach source label an then using this average probability as the source\nlabel's prior probability.  After pre-training, fine-tuning is applied\nwith the labelled target data.\n\nExtensive experiments are performed -  pre-training on a very large source\ndataset and using large classification networks (Inception-v3 and\nAmoebaNet-B) and transferring each pre-trained network to 6 standard\nand relatively large target datasets. \n\nThe results show that pre-training which focuses on the subsets of the\nsource data that are the most similar to the target data is more\neffective, in general, than pre-training on all the source data which\ntreats each example equally. This finding is increasingly relevant the\nmore dissimilar the target and source datasets are and/or the more\n\"irrelevant\" examples for the target task the source dataset contains. \n\n\n\nPros:\n\n+ The experimental results of this paper are its main\n  strength. Results are presented on pre-training on a very large and\n  diverse dataset called \"ANON\" and applying the important sample\n  pre-training approach to both the Inception-v3 and AmoebaNet-B\n  networks.\n\n+ It adds more solid evidence that learning generic classification\nnetworks from diverse datasets do not outperform more specialised\nnetworks learnt from more relevant training data for specific tasks.\n\n+ The importance sampling approach to pre-training is compared to\npre-training on different subsets of the source dataset corresponding\nto images with certain high-level labels. Each subset is (potentially)\nrelevant to at least one particular target task. The importance\nsampling approach does not always outperform pre-training\nexclusively with the most relevant subset approach has a consistently high\nperformance across the board.\n\n+/- A new very large image dataset (which would seem to be a\ncompliment to ImageNet) is introduced though it is unclear\nwhether this dataset will be made available to the research\ncommunity at a later date.\n\n\nCons:\n\n- Details are lacking about the \"ANON\" dataset introduced in this\n  paper (where do the photos come from and the labels, visualization of a few examples...)\n\n- There are not many technical issues discussed in the paper and that\n  is fine as the main idea is relatively simple and its\n  effectiveness is mainly demonstrated empirically, but I\n  feel the paper is missing a discussion about the importance of the initial\n  classifier trained to estimate the target prior probabilities for\n  the source labels and whether it is crucial that it has a certain\n  level of accuracy etc.\n\n- The approach in the paper implies a practitioner should have \n  access to a very large target dataset and the computational and time\n  resources to appropriately pre-train a complex network for each new\n  target task encountered. This is probably not feasible if many\n  target tasks are considered. Unfortunately the paper does not\n  give insights into how pre-training from scratch for each new target\n  could be avoided. \n\n\n- The references in the paper, especially the \"Exploring the limits of\n  weakly supervised pre-training\", demonstrate that it is already\n  known that you do not increase the accuracy for the target task by\n  pre-training with many source examples that are not very relevant to the\n  target task. So one could argue that the findings in the paper are\n  not particularly novel.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}