{
    "Decision": {
        "metareview": "The reviewers found the paper to be well written, the work novel and they appreciated the breadth of the empirical evaluation.  However, they did not seem entirely convinced that the improvements over the baseline are statistically significant.  Reviewer 1 has lingering concerns about the experimental conditions and whether propensity-score matching within a minibatch would provide a substantial improvement over propensity-score matching across the dataset.  Overall the reviewers found this to be a good paper and noted that the discussion was illuminating and demonstrated the merits of this work and interest to the community.  However, no reviewers were prepared to champion the paper and thus it falls just below borderline for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A well composed, novel contribution to counterfactual inference with neural nets but lingering questions remain about empirical significance."
    },
    "Reviews": [
        {
            "title": "Review: Interesting paper and impressive results; why novel vs. standard PSM?",
            "review": "========= Summary =========\n\nThe authors propose a novel method for counterfactual inference (i.e. individual/heterogeneous treatment effect, as well as average treatment effect) with neural networks. They perform propensity score matching within each minibatch in order to match the covariate distributions during training, which leads to a doubly robust model.\n\nPM is evaluated on several standard semi-synthetic datasets (jobs, IHDP, TCGA) and PM shows state-of-the-art performance on some datasets, and overall looks quite promising. \n\n======= Comments =======\n\nThe paper is well-written, presents a novel method of some interest to the community, and shows quite good performance across a range of relevant benchmarks.\n\nI have one major issue with this work: I don't see why propensity-score matching *within* a minibatch should provide a substantial improvement over propensity-score matching across the dataset (Ho et al 2011). I find the cursory explanation given (\"it ensures that every gradient step is done in a way that is approximately unbiased\") unconvincing, since (a) proper SGD training should be robust to per-batch biases during training (the expected loss is identical for both methods, correct?), and (b) biases should go away in the limit of large batch sizes. If indeed SGD required unbiased *minibatches* then standard minibatch SGD wouldn't work at all.\n\nLooking at the experimental details in the appendix, it appears that the MatchIt package was used to do PSM, rather than a careful comparison under the same conditions. Are the exact matching procedure, PS estimator model, choosing \"one of 6 closest  matches by propensity score\", batch size, etc. the same between your PM implementation and MatchIt? I'd be very curious to see the results of a controlled comparison between Alg S1 and S2 under the same conditions (i.e. run your PM implementation on the whole dataset), and perhaps even some more clever experiments illustrating why matching within a minibatch is important. \n\nAnother hypothesis for why PM is better than PSM is that the matching distribution for PM changes at each epoch (at least due to the randomization among the 6 closest matches). Could it be that the advantage of PM is that it actually provides a randomized rather than constant distribution of matched points?\n\nCan the authors provide more motivation for why PM should outperform PSM? Or some more careful comparison of these methods isolating the benefits of PM? I think a convincing justification and comparison here could change my opinion, as I like the paper otherwise. Thanks!\n\nDetailed Comments:\n\n- There is insufficient explanation of the PM method in the main text. The method is only mentioned in a single sentence buried in the middle of a long paragraph \"In PM, we match every sample within a minibatch...\". This should be made more clear, e.g. by moving Algorithm S1 to the main text.\n- The discussion on Model Selection and the argument for nearest-neighbor PEHE is clever and well-supported by the experiments.\n- In Table 3 and 4, it's not clear which numbers are reported by the original authors and which were replicated by the authors.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple idea, the presentation of the method and experiment results can be improved",
            "review": "Summary:\nThis paper proposed to extend TARNET (Shalit et al. 2017), a representation learning approach for counterfactual inference, in the following ways.\n\nFirst, to extend TARNET to multiple treatment setting, k head networks (instead of 2) were constructed following the shared MLP layers, where each head network modeled the outcome of one treatment. This extension seemed quite straightforward.\n\nSecond, during training, for every sample in a minibatch, find its nearest neighbors from all other treatments and add them to the minibatch. The distance was measured by the propensity score, which was defined the probability of a sample being assigned to a treatment group and could be learned by a classification model (such as support vector machine used in this work). Therefore, 1) the augmented minibatch would contain the same number of samples for each treatment group; 2) different treatment group were balanced.\n\nThird, a model selection strategy was proposed by estimating the PEHE using nearest neighbor search.\n\nComments:\nThis paper is well motivated. The key challenges in counterfactual inference is how to adjust for the bias in treatment assignment and the associated discrepancies in the distribution of different treatment groups. \n\nThe main idea of this paper, i.e., augmenting the minibatch through propensity score matching for each sample, is well explained in Section 3. However, it could be better if the introduction of model architecture (in Appendix F) was presented in the method section.\n\nDid the author need to train (k choose 2) SVMs to compute the propensity scores for samples from k treatment groups?\n\nWhen comparing different approaches, as were shown in Table 3, 4 and Figure 3,4, did the author run any statistical test, such as t-test, to confirm the difference between those distributions were significant? The standard deviations of those errors seemed quite large so the difference could be non-significant.\n\nCould the author provide more explanations on why the proposed approach, i.e., minibatch augmentation using propensity score matching, can outperform the TARNET? In TARNET, each sample it only used to update the head network corresponding to the sample's treatment assignment, why would balancing samples in the minibatch can improve the estimation of treatment effect?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall a good paper",
            "review": "This paper proposes an augmentation of traditional neural network learning to allow for the inference of causal effects. Specifically, they modify the data sampling procedure of SGD during training to use matched samples that are paired via propensity score matching. Experimental results on a number of dataset show that the proposed methodology is comparable to alternative machine learning based causal inference methods. \n\nOverall, I think this is a nice idea. I have two main concerns: \n(1) The use of small batches for matching. Figure 2 does alleviate this concern to an extent, but there is a large literature in statistics and the social sciences on the effect that the quality of matches have on the final causal estimand. It is quite possible that this particular dataset is more amenable to PSM. It is also worth noting that while there is bias reduction shown in figure 2, it is not overwhelming. \n\n(2) The use of propensity scores for matching. One of the insights from the heterogeneous treatment effect literature is that it is not difficult to find cases where the propensity of treatment is identical for two sets of covariates that otherwise do not obey any real balance. This can lead to large biases in the final estimate. Given that PSM is still a relatively widely used practice, I don’t think that its use is a ground for rejection in itself, but given that neural networks are often used to estimate complex causal relations when they are used and this paper is interested in individual treatment effects it is worth noting. \n\nI found the experimental setup to do a very good job in covering large portions of the behavior of the algorithm. The final results are a little underwhelming–the proposed method does not appear to clearly define a new state of the art for the tasks it is applied to–but it is often competitive and the paper presents an interesting idea.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}