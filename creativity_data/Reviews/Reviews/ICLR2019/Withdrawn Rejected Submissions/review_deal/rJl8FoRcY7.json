{
    "Decision": {
        "metareview": "This paper suggests a problem with the standard ELBO for the multi-modal case, and proposes a new objective to address this problem.  However, I (and some of the reviewers) disagree with the motivation.  First of all, there's no reason one can't train a separate encoder for every combination of modalities available, at least when there are only 2 or 3.  Failing that, one could simple optimize per-example approximate posteriors without using an encoder.\n\nSecond, once you stop optimizing the ELBO, you've lost the motivating principle for training VAEs, and must justify your new objective empirically.  Almost all of the results are (in my opinion) ambiguous plots of latent encodings.\n\nFinally, a point made throughout the paper and discussions was that different modalities should give the same encodings, which is plainly false.  One of the reviewers made this point: \"The fact that z_a != z_b != z_{a,b} should be expected if a and b provide different information. I don't see the problem with this.\", which you dismiss.  Additionally, the encoder's job is to approximate the true posterior.  The true posteriors will in general be different for different modalities.\n\nI would recommend focusing on ways to train the original ELBO in the presence of different modalities, instead of modifying it based on these intuitions.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Poorly motivated, implications unclear"
    },
    "Reviews": [
        {
            "title": "Disjointed paper on an interesting topic",
            "review": "This paper introduces a new VAE model (JMVAE) for multi-modal data with a\nshared latent representation. An method is also introduced to synthetically\ncreated bi-modal datasets with correlated latent representations.\n\nThe writing was a little awkward to follow at times, and I'm still not\nsure what Ι am suppose to take away from the figures plotting the latent\nrepresentation. The evaluation is fairly qualitative and it's difficult to\nunderstand what we achieving from using JMVAE.\n\nI'm not clear what the contribution of this work provides, as there is already\nplenty done on learning multi-modal representations.\n\nOne weakness with this work is all the examples are fairly toy\nproblems. The article motivates the work as combining raw multi-modal\nsensor datasets, but no real tasks are shown.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "interesting  topic; potential technical error.  ",
            "review": "The paper proposes a multi-modal VAE with a variational bound derived from chain rule. \n\nPros:\nIt is an interesting and important research direction. \nThe presentation is in general clear. \n\nCons:\n1. The re-visit of JMVAE seems not precise. The JMVAE should bound the joint p(a, b) not log p(a|b)p(b|a).\n2. Due to the potential misunderstanding of JMVAE, the paper uses the JMVAE bound for log p(a|b) + log p(b|a) in equation (5), which seems wrong. \nEquation (4) &(5) itself seems confusing alone. It says L_m = log p(a,b) in (4) then L_m = log p(a|b) + log p(b|a) in (5).\n3. If I am not mistaken the error above, the proposed bound is in fact wrong. \n4. Assume that the method is correct, with a massive amount of beta:s, I doubt the method would be very sensitive to beta tuning. The experiments just presented some examples of different betas. Quantitive evaluation of beta and performance is needed. \n5. To generate multi-modal data, other methods such as VAE-CCA or JMVAE are able to that as well. It is not unique to the proposed method. \n6. The experiments are very toyish. The multi-modal data were generated. The method should be evaluated with a real-world benchmarking multi-modal dataset. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unclear about the novelty of the objective. Clarity could be improved.",
            "review": "\nThis paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation. To summarize my understanding of the proposed objective, in the bi-modal case, it combines both objectives of TELBO [1] and JMVAE-kl [2] with some hyperparameters to learn the uni-modal encoders. The terms of Eqns 7,8, and 9 are equivalent to TELBO and Eqns 9 and 10 are JMVAE-kl. It would be very beneficial for the readers if you could more clearly contrast your objective with the related work given how similar they are. \n\nGiven these similarities between objectives, its unclear why JMVAE-Zero was chosen over JMVAE-kl as a baseline. Furthermore, the reasoning for the improvement of the ELBO of M^2VAE over the baselines in Section 5.3 is unclear, given the similarities between the objectives. \n\nThe qualitative figures throughout the paper are hard to interpret. By looking at Fig 4., I cannot tell which latent space is best. \n“one can see from Fig. 4 that the most coherent latent space distribution was learned by the proposed M^2VAE” \nWhat is meant by ‘coherent latent space’? \n\nThis paper was hard to follow and there are a number of typos throughout the paper. For instance, the labels within Fig 4 and the caption contradict themselves. If the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.  \n\n[1] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative Models of Visually GroundedImagination. ArXiv e-prints, May 2017.\n[2] M. Suzuki, K. Nakayama, and Y. Matsuo. Improving Bi-directional Generation betweenDifferent Modalities with Variational Autoencoders. ArXiv e-prints, January 2018\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}