{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper proposes a framework to fully utilize the training data for zero/few-shot learning. The improvement is notable and the motivation for few-shot learning is great. The reference needs improvement.",
            "review": "## There seems to be no response by the authors. Thus, my score stands unchanged. ##\n\n\nPros:\n\n1) The paper points out the issue of learning feature extractor in existing few-shot learning. Such an observation is crucial and the proposed method leads to significant improvement. The authors also include the ablation studies w/ and w/o training features using the dataset-wise information to support the claim.\n\nCons:\n\n1) While the authors tend to sell the idea for both zero-shot (ZSL) and few-shot learning, the novelty to ZSL is actually much small. First of all, the features are pre-trained from ImageNet, so the episode-based issue is not applied. Second, learning a mapping from attributes to classifier weights have already been proposed but not cited or discussed at all. The authors should compare to them and adjust the claim.\n\nJimmy Ba et al., \"Predicting deep zero-shot convolutional neural networks using textual descriptions,\" CVPR 2015\nXiaolong Wang et al., \"Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs,\" CVPR 2018\n(Changpinyo et al., 2016)\n\nThird, the issue of generalized ZSL is raised in the following paper, not in (Romera-Paredes 2015). The following paper has also pointed out a framework for zero-shot and few-shot learning similar to what the paper proposed.\n\nWei-Lun Chao et al., \"An empirical study and analysis of generalized zero-shot learning for object recognition in the wild,\" ECCV, 2016\n\n2) Some implementation details seem to be missing:\n2-1) What is the architecture of g_\\phi? for both ZSL and FSL.\n2-2) Do the authors fine-tune the resnet features for ZSL?\n2-3) Do the authors use the same architecture of f_\\theta as other methods in Table 3? If not, the comparison might be unfair. The authors should explicitly point out what architectures have been used for different methods.\n2-4) The authors should include Ours with episode-based training in Table 3.\n\n3) Improvement on ZSL needs more analysis. Specifically, many zero-shot learning methods do not use the training procedure mentioned in Section 4.1. The authors should thus compare the proposed method/architecture w/o this training procedure to show the difference. This study would benefit the ZSL community.\n\nOther comments:\nI would suggest the authors focusing on FSL.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Little novelty, there are prior works that have the same idea, but this paper fails to discuss any of them",
            "review": "Summary\nThis paper proposes to learn a neural network that predicts classifier weights of unseen classes from attribute embeddings in zero-shot learning or from averaged feature embeddings of  available samples in few-shot learning. The approch is validated on  established zero-shot and few-shot learning benchmarks and results show some significant improvement over the baselines.  \n\nPros\n-Well written\n-Good empirical results\n\nCons\n-Little novelty. This paper fails to discuss the following highly similar related works. [1] learns a nueral network (MLP) to predict classifier weights and convolutional filters from text features in the context of zero-shot learning. [2] predicts the classifier weights of unseen classes by doing graph convolution on top of the semantic knowledge graph and word embedding. [3] tackles few-shot learning by learning a MLP to predict classifier weights of novel classes from image features. In my point of view, the key idea of this paper is highly close to [1] and [3]. Unfortunately, this paper seems to be not aware of those important prior works. \n\n[1] Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions.  Ba et al.  ICCV'15\n[2] Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs. Wang et al. CVPR'18\n[3] Few-Shot Image Recognition by Predicting Parameters from Activations. Qiao et al. CVPR'18\n\n-Stronger zero-shot learning baselines are missing. The numbers in GBU have been improved significantly by some CVPR'18 papers, i.e. [4]. I suspect Table 2 is not current the state-of-the-art.\n[4] Generalized Zero-Shot Learning via Synthesized Examples. Verma et al. CVPR'18\n\nBased on the above analysis, I would rate this paper as \"Clear rejection\". In the rebuttal,  I suggest the authors to discuss the main differences between their approach and [1], [3].",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Hard to evaluate the experimental results.",
            "review": "The paper addresses both zero-shot and few-shot learning in a unified framework. The key idea is to separate the deep network model into two networks. The first, aimed at learning a feature representation, is trained using standard multi-class but using cosine instead of dot product . The second, is trained using episode-based meta learning. \nThe paper suggests that by training the features in a standard way, the representation is more rich and provides better space for learning the ZS and FS classes. \n\nThe idea is very reasonable, and the experimental evaluations show  large improvement over existing baselines. \n\nMy main concern is that evaluating ZSL and FSL, is often tricky, and one should be careful about the details such that no information is leaked to the unseen classes. The paper is written in a way that makes it hard to evaluate if all the experiments were done in a proper way. Some of gains claimed, like the accuracy improvement for GZSL are very large, reporting by +70% or even +150%(aPY). Sometimes this is a sign that the evaluations are not done in the same way as the baselines, or other issues. More careful analysis is expected to convince that the gains are due to the proposed method. \n\nI would have liked to see (1) more detailed analysis of why the approach works (2) Ablation experiments and more convincing evidence that the improvement is indeed due to the way the representation is trained. (3) Evidence that the feature representation is better. (4) more details about the evaluation procedure. \n\nFor example these can include  (1) repeat the experiments with the same setup, but with a feature representation learned in an episodic way. (2) Evaluate the quality of the representation on another task (3) For the GZSL experiments,  report separately the accuracy on seen and unseen classes. (4) qualitative analysis linking wins and losses to changes in the representation. (5) tuning the size of episodic training to support the claim that small episodes lead to worse representation. \n\nOther comments: \nSome more recent baselines are not included,  e.g. better performance on CUB (57.8%) was reported by \nAtzmon et al, Probabilitis AND-OR Attribute Grouping for Zero-Shot Learning, UAI 2018.  Other improvement by \nZhang and Koniusz. Zero-shot kernel learning. CVPR, 2018.\n\n\nMinor comments :\n-- clarify which classes were used in train, validation, test\n-- Eq 6: L is defined as a function of phi. not used. \n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}