{
    "Decision": {
        "metareview": "The paper introduces a setting called high-fidelity imitation where the goal one-shot generalization to new trajectories in a given environment. The authors contrast this with more standard one-shot imitation approaches where one-shot generalization is to a task rather than a precise trajectory. The authors propose a technique that works off of only state information, which is coupled with an RL algorithm that learns from a replay buffer that is populated by the imitator. The authors emphasize that their approach can leverage very large deep learning models, and demonstrate strong empirical performance in a (simulated) robotics setting. \n\nA key weakness of the paper is its clarity. All reviewers were unclear about the precise setting as well as relation to prior work in one-shot imitation learning. As a result, there were substantial challenges in assessing the technical contribution of the paper. There were many requests for clarification, including for the motivation, difference between the present setting and those addressed in previous work, algorithmic details, and experiment details.\n\nI believe that a further concern was the lack of a wide range of baselines. The authors construct several baselines that are relevant in the given setting, but did not consider \"naive baseline\" approaches proposed by the reviewers. For example, behavior cloning is mentioned as a potential baseline several times. The authors argue that this is not applicable as it would require expert actions. Instead of considering it a baseline, BC could be used as an \"oracle\" - performance that could be achieved if demonstration actions were known. As long as the access to additional information is clearly marked, such a comparison with a privileged oracle can be properly placed by the reader. Without including such commonly known reference approaches, it is very challenging to assess the proposed method's performance in the context of the difficulty of the task. Generally, whenever a paper introduces both a new task and a new approach, a lot of care needs to be taken to build up insights into whether the task appropriately reflects the domain / challenge the paper claims to address, how challenging the task is in comparison to those addressed in prior work, and to place the performance of the novel proposed method in the context of prior work. In the present paper, on top of the task and approach being novel, the pure RL baseline D4PG is not yet widely known in the community and it's performance relative to common approaches is not well understood. Including commonly known RL approaches would help put all these results in context.\n\nThe authors took great care to respond to the reviewer comments, providing thorough discussion of related work and clarifications of the task and approach, and these were very helpful to the AC to understand the paper. The AC believes that the paper has excellent potential. At the same time, a much more thorough empirical evaluation is needed to demonstrate the value of the proposed approach in this novel setting, as well as to provide additional conceptual insights into why and in what kinds of settings the algorithm performance well, or where its limitations are. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "a novel approach for a novel task, not sufficiently grounded in prior work"
    },
    "Reviews": [
        {
            "title": "Interesting idea to extend DDPGfD to use only state trajectories, but needs a further experimental validation. ",
            "review": "**Summary**\n\nThe paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: \n1. learning technique for high fidelity one-shot imitation at test time. \n2. Policies to improve the expert performance through RL.  \n\nThe main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. \n\n\n** Comments **\n- The novelty of algorithm block\nThe main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. \n\n1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? \nIf the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. \n\n2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. \n\n* Improved Comparisons\n- Compare with One-Shot Performance\nSince this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. \n\nThis comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018)\n\n- Compare High-Fidelity Performance\nIt is used as a differentiator of this method but without experimental evidence.\nThe results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation.\n\nFurthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful.\n\n- Compare Policy Learning Performance\nIn addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. \n\n\n* Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work!\n\n\n- \"Diverse Novel Skills\" \nThe experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help.\n\n- Bigger networks\n\"In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms\" -- but Fig 3 is a network architecture diagram. It is probably fig 6!\n\n- The authors are commended for presenting a broad overview of imitation based methods in table 2\n\n** Questions **\n\n1.  How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. \n\n2. How is the local context considered in action generation in sec 2.1. \nThe authors reset the simulation environment to o_1 = d_1. \nThen actions are generated with  \\pi_{theta} (o_t, d_{t+1}). \na. Is the environment reset every time step?\nb. If not how is the deviation of the trajectory handled over time? \nc. how is the time horizon for this open loop roll out chosen. \n\n3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. \n\n4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model?\n\nSuggestion:\nThe whole set of experiments are in a simulation. \nThe authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. \n\nReferences:\n1. Neural Task Programming, Xu et al. 2018 (https://arxiv.org/abs/1710.01813)\n2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453)\n3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674)\n4. A survey of robot learning from demonstration, Argall et al. 2009\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Insufficient evidence/experimental validation for the main claims of the paper",
            "review": "Summary\n\nThis work porposes a approach for one-shot imitation with high accuracy, called \"high fidelity imitation learning\" by the authors. Furthermore, the work addresses the common problem of exploration in imitation learning, which would help to rescue from off-policy states.\n\nReview\n\nIn my opinion, the main claims of this paper are not validated sufficiently in the experiments. I would expect the experiments to be designed specifically to support the claims made, but little evidence is provided:\n\n- The authors claim that the method allows one-shot generalization to an unknown trajectory. To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes. I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation.\nUntil then I see no evidence for a \"one-shot\" imitation capability of the proposed method.\n\n- That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states: This is never experimentally validated. This hypothesis could easiliy be validated with an ablation study, were the results of early would not be added to the replay buffer.\n\n- High fidelity imitation: In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos. Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations.\nCould the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated.\n\nMainly due to the missing experimental validation of the claims made I recommend to reject the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "learning from video demonstration; exposition is confusing / misleading.",
            "review": "This paper presents an RL method for learning from video demonstration without access to expert actions. The agent first learn to imitate the expert demonstration (observed image sequence and proprioceptive information) by producing a sequence of actions that will lead to the similar observations (require a renderer that takes actions and outputs images). The imitation loss is a similarity metric. Next, the agent explores the environment with both the imitation policy and task policy being learned; an off-policy RL algorithm D4PG is used for policy learning. Experiments are conducted on a simulated robot block stacking task.\n\nThe paper is really clearly written, but presenting the approach as \"high-fidelity\", \"one-shot\" learning is a bit confusing. First, it's not clear what's the motivation for high-fidelity. To me this is an artifact due to having to imitate the visual observation instead of the actions, which is a legitimate constraint, but not the original goal. Second, the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person; both for the same task. Usually one-shot learning tests on slightly different tasks or environments, whereas here the goal is to generalize to novel demonstrations. It's not clear why do we care imitation per se in addition to the task reward.\n\nWhat I find interesting is the proposed approach for learning for video demonstration without action labels. Currently this requires an executor to render the actions to images, what if we don't have such an executor or only have a noisy / approximate renderer? In the real world it's probably hard to find a good renderer, it would be interesting to see how this constraint can be relaxed.\n\nQuestions:\n- While the authors have shown the average rewards of the two sets are different, I wonder what's the variance of each person's demonstration. \n- In Fig 5, on the validation set, in terms of imitation loss there aren't that much difference between the policies, but in terms of task reward, the 'red' policy goes to zero while others policies' rewards are still similar. Any intuition for why seemingly okay imitation doesn't translate to task reward?\n\nOverall, I enjoyed reading the paper and the experiments are comprehensive. The current presentation angle seems a bit off though.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well presented, but not suitable for ICLR",
            "review": "\nSummary:\nThis paper proposes MetaMimic, an algorithm that does the following:\n(i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task.\n(ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task.\n\nOverall Evaluation:\nThis is a good paper. In my opinion however, it does not pass the bar for ICLR.\n\nPros:\n- The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive.\n- The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful.\n- Interesting pipeline of learning policies that can use demonstrations without actions.\n- The results on the simulated robot arm (block stacking task with two blocks) are good.\n\nCons:\n- The abstracts oversells the contribution a bit when saying that MetaMimic can learn \"policies for high-fidelity one-shot imitation of diverse novel skills\". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better.\n- Experimental results are shown only for one task; block stacking with a robot arm in simulation.\n- Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks.\n\nQuestions:\n- Where does the \"task stochasticity\" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic?\n- The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time?\n- It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.)\n- The x-axis in the figures says \"time (hours)\" - is that computation time or simulated time?\n\nOther Comments:\n- In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? \n- An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum.\n\nNitpick (no influence on score):\n[1. Introduction]\n- I find the first sentence, \"One-shot imitation is a powerful way to show agents how to solve a task\" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like \"Expert demonstrations are a powerful way to show agents how to solve a task.\" works better?\n- Second sentence, the chosen example is \"manufacturing\" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations.\n- Add note that with \"unconditional policy\" you mean not conditioned on a demonstration.\n[2. MetaMimic]\n- [2.1] Third paragraph: write \"Figure 2, Algorithm 1\" or split the algorithm and figure up so you can refer to them separately.\n- [2.1] Last paragraph, second line: remove second \"to\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}