{
    "Decision": {
        "metareview": "The reviewers raise a number of concerns including no methodological novelty, limited experimental evaluation, and relatively uninteresting application with very limited real-world application. This set of facts has been assessed differently by the three reviewers, and the scores range from probable rejection to probable acceptance. I believe that the work as is would not result in a wide interest by the ICLR attendees, mainly because of no methodological novelty and relatively simplistic application. The authors’ rebuttal failed to address these issues and I cannot recommend this work for presentation at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Mostly a dataset paper, writing is not coherent, results are not convincing",
            "review": "In this paper a new task namely CoDraw is introduced. In CoDraw, there is a teller who describes a scene and a drawer who tries to select clip art component and place them on a canvas to draw the description. The drawing environment contains simple objects and a fixed background scene all in cartoon style. The describing language thus does not have sophisticated components and phrases. A metric based on the presence of the components in the original image and the generated image is coined to compute similarity which is used in learning and evaluation.  Authors mention that in order to gain better performance they needed to train the teller and drawer separately on disjoint subsets of the training data which they call it a cross talk.\n\nComments about the task:\nThe introduced task seems to be very simplistic with very limited number of simple objects. From the explanations and examples the dialogs between the teller and drawer are not natural. As explained the teller will always tell ‘ok’ in some of the scenarios. How is this different with a system that generates clip art images based on a “full description”? Generating clip arts based on descriptions is a task that was introduced in the original clip art paper by Zitnick and Parikh 2013. This paper does not clarify how they are different than monologs of generating scenes based on a description.  \n\nComments about the method:\nI couldn’t find anything particularly novel about the method. The network is a combination of a feed forward model and an LSTM and the learning is done with a combination of imitation learning and REINFORCE. \n\n\nComments about the experimental results:\nIt is hard to evaluate whether the obtained results are satisfying or not. The task is somehow simplistic since there a limited number of clip art objects and the scenes are very abstract which does not have complications of natural images and accordingly the dialogs are also very simplistic. All the baselines are based on nearest neighbors. \n\nComments about presentation:\nThe writing of this paper needs to be improved. The current draft is not coherent and it is hard to navigate between different components of the method and different design choices. Some of the design choices are not experimentally proved to be effective: they are mentioned to be observed to be good design choices. It would be more effective to show the effect of these design choices by some ablation study. \nThere are many details about the method which are not fully explained: what are the details of your imitation learning method? Can you formalize your RL fine-tuning part with the use of some formulations? With the current format, the technical part of the paper is not fully understandable.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An artificial task for modeling and evaluation of goal-oriented dialogs",
            "review": "The paper proposes a game of collaborative drawing where a teller is\nto communicate a picture to a drawer via natural language.  The picture\nallows only a small number of components and a fixed and limited set\nof detailed variations of such components.\n\nPros:\n\nThe work contributed a dataset where the task has relatively objective\ncriteria for success.  The dataset itself is a valuable contribution\nto the community interested in the subject.   It may be useful for\npurposes beyond those it was designed for.\n\nThe task is interesting and its visual nature allows for easy inspection\nof the reasons for successes or failures.  It provides reasonable grounding\nfor the dialog.  By restricting the scope of variations through the options\nand parameters, some detailed aspects of the conversation could be explored\nwith carefully controlled experiments.\n\nThe authors identified the need for and proposed a \"crosstalk\" protocol\nthat they believe can prevent leakage via common training data and\nthe development of non-language, shared codebooks that defeat the purpose\nof focusing on the natural language dialog.\n\nThe set up allows for pairing of human and human, machine and machine,\nand human and machine for the two roles, which enables comparison to\nhuman performance baselines in several perspectives.\n\nThe figures give useful examples that are of great help to the readers.\n\nCons.:\n\nDespite the restriction of the task context to creating a picture with\nseverely limited components, the scenario of the dialogs still has many\ndetails to keep track of, and many important facets are missing in the\ndescriptions, especially on the data.\n\nThere is no analysis of the errors.  The presentation of\nexperimental results stops at the summary metrics, leaving many\ndoubts on why they are as such.\n\nThe work feels somewhat pre-mature in its exploration of the models\nand the conclusions to warrant publication.  At times it feels like the\nauthors do not understand enough why the algorithms behave as they do.\nHowever if this is considered as a dataset description paper and\nthe right expectation is set in the openings, it may still be acceptable.\n\nThe completed work warrants a longer report when more solid conclusions\ncan be drawn about the model behavior.\n\nThe writing is not organized enough and it takes many back-and-forth rounds\nof checking during reading to find out about certain details that are given\nlong after their first references in other contexts.  Some examples are\nincluded in the followings.\n\nMisc.\n\nSection 3.2, datasets of 9993 dialogs:\nAre they done by humans?   Later it is understood from further descriptions.\nIt is useful to be more explicit at the first mention of this data collection effort.\nThe way they relate to the 10020 scenes is mentioned as \"one per scene\", with a footnote on some being removed.\nDoes it mean that no scene is described by two different people?  Does this\nlimit the usefulness of the data in understanding inter-personal differences?\n\nLater in the descriptions (e.g. 4.1 on baseline methods) the notion of\ntraining set is mentioned, but up to then there is no mentioning of how\ntraining and testing (novel scenes) data are created.\nIt is also not clear what training data include: scenes only?\nDialogs associated with specific scenes?  Drawer actions?\n\nSection 4.1, what is a drawer action?  How many possibilities are there?\nFrom the description of \"rule-based nearest-neighbor drawer\" they seem to be\ncorresponding to \"teller utterance\".\nHowever it is not clear where they come from.  What is an example of a drawer action?\nAre the draw actions represented using the feature vectors discussed in the later sections?\n\nSection 5.1, the need for the crosstalk protocol is an interesting observation,\nhowever based on the description here, a reader may not be able to understand\nthe problem.  What do you mean by \"only limited generalization has taken place\"?  Any examples?\n\nSection 5, near the end: the description of the dataset splits is too cryptic.\nWhat are being split?  How is val used in this context?\n\nAll in all the data preparation and partitioning descriptions need substantial clarification.\n\nSection 6:  Besides reporting averaged similarity scores, it will be useful to report some error analysis.\nWhat are the very good or very bad cases?  Why did that happen?\nAre the bad scenes constructed by humans the same as those bad scenes\nconstructed by machines?  Do humans and machines tend to make different errors?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Exciting task! Not sure about model results",
            "review": "This paper presents CoDraw, a grounded and goal-driven dialogue environment for collaborative drawing. The authors argue convincingly that an interactive and grounded evaluation environment helps us better measure how well NLG/NLU agents actually understand and use their language — rather than evaluating against arbitrary ground-truth examples of what humans say, we can evaluate the objective end-to-end performance of a system in a well-specified nonlinguistic task. They collect a novel dataset in this grounded and goal-driven communication paradigm, define a success metric for the collaborative drawing task, and present models for maximizing that metric.\n\nThis is a very interesting task and the dataset/models are a very useful contribution to the community. I have just a few comments below:\n\n1. Results:\n1a. I’m not sure how impressed I should be by these results. The human–human similarity score is pretty far above those of the best models, even though MTurkers are not optimized (and likely not as motivated as an NN) to solve this task. You might be able to convince me more if you had a stronger baseline — e.g. a bag-of-words Drawer model which works off of the average of the word embeddings in a scripted Teller input. Have you tried baselines like these?\n1b. Please provide variance measures on your results (within model configuration, across scene examples). Are the machine–machine pairs consistently performing well together? Are the humans? Depending on those variance numbers you might also consider doing a statistical test to argue that the auxiliary loss function and and RL fine-tuning offer certain improvement over the Scene2seq base model.\n\n2. Framing: there is a lot of work in collaborative / multi-agent dialogue models which you have missed — see refs below to start. You should link to this literature (mostly in NLP) and contrast your task/model with theirs.\n\nReferences\nVogel & Jurafsky (2010). Learning to follow navigational directions.\nHe et al. (2017). Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.\nFried et al. (2018). Unified pragmatic models for generating and following instructions.\nFried et al. (2018). Speaker-follower models for vision-and-language navigation.\nLazaridou et al. (2016). The red one!: On learning to refer to things based on their discriminative properties.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}