{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not convincing",
            "review": "This paper aims to provide additional information in addition to class labels, which can be used to explain classification results. Although the direction of this paper makes sense, it has a few critical problems as follows:\n\n1.  There is no semantics in binary classification. and each branch does not have any particular meaning. The LSTMs are probably trained to minimize the risk, which can be defined as making a mistake at the early stages. Also, two branches in the tree seem to always have the exactly (or at least almost) same number of classes, which is very artificial. I am not sure what kind of additional information can be obtained other than some sort of information similar to taxonomy. It is actually not a taxonomy because the binary decision in each step does not correspond to semantic hierarchy as mentioned earlier\n\n2. The proposed method has very weak scalability. Although the paper claims that this method is generic and can be applied to various problems, it is not straightforward to be used in more complex problems such as object detection and image segmentation. In these cases, the algorithm needs to generate binary tree for each object instance or image pixel.\n\n3. Experiment is very weak and the method is tested on very few small datasets. Especially, experiment on MNIST is meaningless because the tree corresponding to the dataset is very shallow according to Figure 2. Evaluation of zero-shot learning is minimal.\n\n4. There should be some discussion about cost to add the proposed network. How many parameters are needed? How much time should we spend more for training and testing?\n\nThis paper should be formulated as taxonomy learning problem and compared with the papers about it further. The main idea may be okay, but the quality of the paper is below standard of the conference.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea. Lacking presentation makes it hard to follow.",
            "review": "n this paper, the authors propose an alternative to the softmax layer — an observer-classifier framework, where the observer has access to the input features as well as a query vector from the classifier and outputs a binary response, and the classifier (modeled as an LSTM) has access to its queries as well as the binary response from the classifier to output further queries as well as its current decision about the class of the input. This process repeats producing a binary decision tree with individual classes represented at the leaf nodes.\n\nThe paper tackles an interesting problem of interpretability of complex models such as CNN, and the proposed approach appears promising in some domains as reflected in the experiments. However, I had a few concerns about the presentation which affected my understanding of the methodology. The biggest issue for me is the lack of a clear explanation of the proposed framework. Several details are skimmed over such as — why do we use Gumbel-softmax estimator? What’s the motivation for two different MLPs in the Attribute Predicting Observer? How exactly are the trees created in the visualizations (Fig. 3 and 4)? How do we use the intermediate predictions if we do at all? And so on. Some of these I could make my way through after some effort while some others are still unclear to me. In a paper proposing a new framework, the space could have been better utilized describing the method crisply, but currently the explanation is done within 1.5 pages. In the experiments section, there is little analysis on parameters like the number of binary decision needed. Fig. 2 plots the observations but there is no insight as to why for example, with the same number of classes CIFAR10 requires 6 binary decisions whereas MNIST just 4. A naive understanding would suggest log(N) decisions to reach leaf nodes for N classes. Fig. 2 would be better drawn with logarithmic x-axis, and the legend to really correspond with the lines in the figure. Fig. 3 and 4 occupy a full page and are near impossible to read even on a large monitor. Authors should investigate better summary visualizations and maybe dedicate this space to developing and explaining the framework. I think Fig. 1 does an excellent job at explaining the framework and it would be great if the accompanying text built on it. The results on zero-shot learning are also promising but the section lacks clarity again. \n\nMinor issues: “saliecy” (page 1, spelling), “by first sampling” (page 2, no follow-up), “log o is the unnormalized output” (page 3, should it be o?), “effects” (page 4, should be affects), “can trained” (page 4, should be can be trained), “seek for” (page 8, should be seek).\n\nIn summary, even though I liked the idea and the approach to the problem, the presentation is quite lacking, and in its current form, the paper might lose impact just by virtue of being hard to understand and subsequently being hard to reproduce. I would encourage the authors to polish the explanation, clean up the figures, and proof read for spelling and grammar errors before resubmitting to a future conference. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A ok paper but not good enough",
            "review": "This paper introduces a hierarchical deep learning model that combines CNN and LSTM through a two agents process. The details of this framework, specially how CNN and LSTM are being trained and used in the process, is not clear (Figure 1 is not also very clear). Even though the experiments support the claim for some interpretability (for example that different levels of classification are reasonable), I am not sure if this level of interpretability is what we desire when we talk about interpretability. There is still little known about how the model does the classification in terms of mapping an input image to a label and we still know little about what is happening through different layers of the deep neural network. In summary, even though the paper could have some qualifications, the details of the framework is not clear and  I am not convinced that it addresses the problem it aims to solve, i.e. interpretability! ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}