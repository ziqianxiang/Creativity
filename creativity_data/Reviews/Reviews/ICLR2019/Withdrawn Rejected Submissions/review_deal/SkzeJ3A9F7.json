{
    "Decision": {
        "metareview": "The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward. The motivation is that this approach will lead to more sample efficient exploration for real robots. The use of a differentiable loss for policy optimization is interesting and has some novelty. However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims. Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Laudable goal, but paper not good enough",
            "review": "Beyond Games: Bringing Exploration to Robots in Real-world\n===========================================================\n\nThis paper tackles the laudable goal of making an algorithm for efficient exploration in \"real-world\" RL.\nTo do this, they augment the \"curiosity\" algorithm of Pathak et al with a differentiable approximation to the reward prediction model.\nThey motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE.\n\n\nThere are several things to like about this paper:\n\n- The problem of making \"real-world\" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning.\n\n- The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines.\n\n- The authors clearly make an effort to survey a wide variety of recent papers in the field\n\n\n\nHowever, there are several important places where this paper falls down:\n\n- In a paper that posits a new, groundbreaking, real-world application of \"exploration\" there is remarkably little discussion of the key issues of \"efficient exploration\". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration.\n  + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds?\n  + Of course, this is not a paper designed for \"tabular MDPs\", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches?\n\n- There is very little *science* in this paper, beyond the experiments pitting \"improved algorithm\" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond \"sample-efficient exploration formulation\") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled.\n\n- A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me:\n  + \"... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ...\"\n  + \"Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient\"\nI would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not!\n\n- Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper.\n\n\nOverall, it is clear that this is an interesting area to do work in.\nThe goal of making a practical algorithm for real-world exploration tasks is exciting.\nHowever, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review: Clear reject",
            "review": "Summary:\n\nThis paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL.\nThe authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot.\n\nComments:\n\nThe paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details.\n\nThe experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is \"multi-step learning\" in Table 2?).\nWithout these details, the results will be difficult to validate and reproduce independently.\n\nThe approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space?\n\nThe authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak.\n\nThe paper contains many factual errors and unsupported claims. For example:\n- \"the field of RL was born out of need to make our robots learn\"\n- \"none of the recent advances have translated to success in the field of robotics\" (see e.g. the proceedings of CoRL 2017 and 2018)\n- \"Building a good model will require enormous number of interactions\" (see e.g. PILCO)\n- \"[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot\" (PILCO and many others)\n- describing Pathak et al. curiosity as a \"gaussian density model\" in eq1; it's a deterministic forward model\n- in sec3, \"regress r^i_t to learn value estimates\", this is probably meant to be the discounted sum of rewards\n- also sec3, \"[REINFORCE] gives no signal as to what action to take\"; the signal has high variance but it works (see all policy gradient work)\n\nThese errors can be easily corrected. However, the contribution of the paper is based on a more serious error:\n- sec3.1, \"If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high.\"\nThis is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al.\nBut in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.\nAs a result, the gradient obtained does not actually move the policy toward higher prediction errors.\n\nTo understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors.\nInstead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}.\n\nThis is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing.\nIt's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by.\nI would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion.\n\nFinally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.).\n\nConclusion:\n\nThe paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. \n\nIn the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn’t tested in robots. However, to my understanding the REINFORCE baseline isn’t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. \n\nMoreover, I think the description of the experiments doesn’t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn’t say what they were. \nAlso, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix).\n\nOverall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.’s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X )\n\n\nComments/Thoughts:\n\n+ I think in the introduction there are some statements that probably need citations. For example, “But the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? \n+ “Yes, 54 environments but no real-world physical robots” —> this and the intro seems like a blogpost at times. That can be fine (some would argue it’s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation.\n+ “Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —> citation/backing? it might be nice to point to the experiment section here to back it (e.g., \"As will be shown in Section X and in \\citet{something}, REINFORCE can be quite sample inefficient\")\n+ “In practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to?\n+ “regress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section” —> regress to \\sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier\n+ What is the actual loss function used for the baseline? Is it the same as Pathak et al.?\n+ What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? \n+ Was a variance-reducing baseline used in REINFORCE?\n+ What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation?\n+ “Hence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. “ —> what were the learning rates?\n\n\nLinguistic/Typos:\n\nAlso, some minor, but frequent, grammatical issues/typos that I’ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I’ve tried to point out below. \n\n+ “This leads to a significantly sample efficient exploration policy. “ —> significantly more (?) sample efficient ?\n\n“Why is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent” —> by the agent?\n\n“Forward model fθF is trained to minimize its loss which amounts to minimizing rti with respect to θF” —> the forward model\n\n“However, policy is optimized to maximize the objective” —> However, the policy\n\n“We can also optimize  for policy parameters θP via differentiable loss function” —> We can also optimize for (the) policy parameters \\theta via (a) differentiable loss function?\n\n“To optimize policy to maximize a discounted sum “ —> To optimize the policy\n\n“How good is Forward Prediction Model” —> How good is the forward prediction model\n\nThere are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}