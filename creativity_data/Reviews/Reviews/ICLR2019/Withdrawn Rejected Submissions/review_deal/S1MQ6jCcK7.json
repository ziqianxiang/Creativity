{
    "Decision": {
        "metareview": "The paper addresses an interesting problem (learning in the presence of noisy labels) and provides extensive experiments. However, while the experiments in some sense cover a good deal of ground, reviewers raised issues with their quality, especially concerning baselines and depth (in terms of realism of the data). The authors provided many additional experiments during the rebuttal, but the reviewers did not find them sufficiently convincing.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "good direction but experiments lacking in some respects"
    },
    "Reviews": [
        {
            "title": "Interesting Approach with Insufficient Results",
            "review": "This paper presents an apparently original method targeted toward models training in the presence of low-quality or corrupted data. To accomplish this they introduce a \"mixture of correlated density network\" (MCDN), which processes representations from a backbone network, and the MCDN models the corrupted data generating process. Evaluation is on a regression problem with an analytic function, two MuJoCo problems, MNIST, and CIFAR-10.\n\nThis paper's primary strength is that the proposed method is a tool quite distinct from recent work, in that it does not use bootstrapping or solely use corruption transition matrices. The paper is typeset well. In addition to this, the experimentation has unusual breadth.\n\nHowever, the synthetic regression task is a nice proof-of-concept, but thorough regression evaluation could perhaps include the Boston Housing Prices dataset or some UCI datasets.\n\nThe hamartia of this paper is that it does not provide sufficient depth in its computer vision experiments. For one, experimentation on CIFAR-100 would be appreciated.\nIn the CIFAR-10 experiments, they consider one label corruption setting and lack experimentation on uniform label corruptions.\nThe related works has thorough coverage on label corruption, but these works do not appear in the experiments. They instead compare their label corruption technique to mixup, a general-purpose network regularizer. It is not clear why it is thought the \"state-of-the-art technique on noisy labels\"; this may be true among network regularization approaches (such as dropout) but not among label correction techniques. For this problem I would expect comparison to at least three label correction techniques, but the comparison is to one technique which was not primarily designed for label corruption.\n\n\nNitpicks:\n-In the related works we are told that a smaller learning rate can improve label corruption robustness. They train their method with a learning rate of 0.001; the baseline gets a learning rate of 0.1.\n-The larger-than-usual batch size is 256 for their 22-4 Wide ResNets, and at the same time they do not use dropout (standard for WRNs of this width) and use less weight decay than is common. Is this because of mixup? If so why is the weight decay two orders of magnitude less for your approach compared to the baseline? How were these various atypical parameters chosen?\n-They also use gradient clipping for their method, which is extremely rare for CIFAR-10 classification. Why is this necessary?\n-This document could be cleaner by eschewing the Theorem of this paper, which \"states that a correlation between two random matrices is invariant to an affine transform.\" For this audience, I suspect this theorem is unnecessary. Likewise the three lines expended for the maths of a Gaussian probability density function could probably be used for other parts of this paper.\n-\"a leverage optimization method which optimizes the leverage of each demonstrations is proposed. Unlike to former study,\" -> \"a leverage optimization method which optimizes the leverage of each demonstration is proposed. Unlike a former study,\"\n-\"In the followings,\" -> \"In the following,\"\n\nEdit: The updated results need consistent baselines. For example, the method of [7] should be consistently compared against.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Approach with Nice Results",
            "review": "The paper presents a framework, called ChoiceNet, for learning when the \nsupervision outputs (e.g., labels) are corrupted by noise. The method relies on \nestimating the correlation between the training data distribution and a \ntarget distribution, where training data distribution is assumed to be a mixture \nof that target distribution and other unknown distributions. The paper also \npresents some compelling results on synthetic and real datasets, for both \nregression and classification problems.\n\nThe proposed idea builds on top of previously published work on Mixture Density \nNetworks (MDNs) and Mixup (Zhang et al, 2017). The main difference is the MDN \nare modified to construct the Mixture of Correlated Density Network (MCDN) \nblock, that forms the main component of ChoiceNets.\n\nI like the overall direction and idea of modelling correlation between the \ntarget distribution and the data distribution to deal with noisy labels. The \nresults are also compelling and I thus lean towards accepting this paper. My \ndecision on \"marginal accept\" is based primarily on my unfamiliarity with this \nspecific area and that some parts of the paper are not very easy or intuitive \nto read through.\n\n== Related Work ==\n\nI like the related work discussion, but would emphasize more the connection to \nMDNs and to Mixup. Only one sentence is mentioned about Mixup but reading \nthrough the abstract and the introduction that is the first paper that came to \nmy mind and thus I believe that it may deserve a bit more discussion.\n\nAlso, there are a couple more papers that felt relevant to this work but are \nnot mentioned:\n  - Estimating Accuracy from Unlabeled Data: A Bayesian Approach, Platanios et al., ICML 2016.\n    I believe this is related in how noisy labels are modeled (i.e., section 3 \n    in the reviewed paper) and in the idea of correlation/consistency as a means \n    to detect errors. There are couple more papers in this line of work that \n    may be relevant.\n  - ADIOS: Architectures Deep In Output Space, Al-Shedivat et al., ICML 2016.\n    I believe this is related in learning some structure in the output space, \n    even though not directly dealing with noisy labels.\n\n== Method ==\n\nI believe the methods section could have been written in a more \nclear/easy-to-follow way, but this may also be due to my unfamiliarity with this \narea. Figure 1 is hard to parse and does not really offer much more than section \n3.2 currently does. If the figure is improved with some more text/labels on \nboxes rather than plain equations, it may go a long way in making the methods \nsection easier to follow.\n\nI would also point out MCDN as the key contribution of this paper as ChoiceNet \nis just any base network with an MCDN block stacked on top of this. Thus, I \nbelieve this should be emphasized more to make your key contribution clear.\n\n== Experiments ==\n\nThe experiments are nicely presented and are quite thorough. A couple minor \ncomments I have are:\n\n  - It would be nice to run regression experiments for bigger real-world \n    datasets, as the ones used seem to be quite small.\n  - I am a bit confused at the fact that in table 3 you compare your method to \n    mixup and in table 4 you also show results when using both your method and \n    mixup combined. Up until that point I thought that mixup was posed as an \n    alternative method, but here it seems it's quite orthogonal and can be used \n    together, which I think makes sense, but would be good to clarify. Also, \n    given that you show combined results in table 4, why not also perform \n    exactly the same analysis for table 3 and also show numbers for CN + Mixup?\n\nIt would also be nice to use the same naming scheme for both tables. I would \nuse: ConvNet, ConvNet + CN, ConvNet + CN + Mixup, and the same with WRN for \ntable 4. This would make the tables easier to read because currently the first \nthing that comes to mind is what may be different between the two setups given \nthat they are presented side-by-side but use different naming conventions.\n\nOne question that comes to mind is that you make certain assumptions on the \nkinds of noise your model can capture, so are there any cases where you have \ngood intuition as to why your model may fail? It would be good to present a \nshort discussion on this to help readers understand whether they can benefit by \nusing your model or not.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ok papers but lacking of related works, important baselines and well-motivated storyline.",
            "review": "This paper formulates a new deep learning method called ChoiceNet for noisy data. Their main idea is to estimate the densities of data distributions using a set of correlated mean functions. They argue that ChoiceNet can robustly infer the target distribution on corrupted data.\n\nPros:\n\n1. The authors find a new angle for learning with noisy labels. For example, the keypoint of ChoiceNet is to design the mixture of correlated density network block. \n\n2. The authors perform numerical experiments to demonstrate the effectiveness of their framework in both regression tasks and classification tasks. And their experimental result support their previous claims.\n\nCons:\n\nWe have three questions in the following.\n\n1. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1-3], estimating noise transition matrix [4-6], and explicit and implicit regularization [7-9]. I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.\n\n2. Experiment: \n2.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong. At the same time, they should compare with VAT [7]. \n\n2.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data [4]. Besides, the current paper only verifies on vision datasets. The authors are encouraged to conduct 1 NLP dataset.\n\n3. Motivation: The authors are encouraged to re-write their paper with more motivated storyline. The current version is okay but not very exciting for idea selling.\n\nReferences:\n\n[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n\n[2] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.\n\n[3] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS, 2018.\n\n[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.\n\n[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.\n\n[6] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015.\n\n[7] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.\n\n[8] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NIPS, 2017.\n\n[9] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}