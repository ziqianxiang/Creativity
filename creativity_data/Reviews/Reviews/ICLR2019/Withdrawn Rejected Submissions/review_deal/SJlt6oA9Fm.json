{
    "Decision": {
        "metareview": "This paper proposed Selective Convolutional Unit (SCU) for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors also show the effectiveness of SCU on CIFAR-10, CIFAR-100 and Imagenet.\n\nThe major concerns from various reviewers are that the design seems the over-complicated as well as the experiments are not state-of-the-art. In response, the authors add some explanations on the design idea and new experiments of DenseNet-BC-190 on CIFAR10/100. But the reviewers’ major concerns are still there and did not change their ratings (6,5,5). Based on current results, the paper is proposed for borderline lean reject.  \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A new design for channel selection, yet over-complicated. "
    },
    "Reviews": [
        {
            "title": "interesting idea, but really hard to read",
            "review": "This paper propose Selective Convolutional Unit (SCU), which can replace the bottleneck in Resnet block. The difference between SCU and bottleneck is that SCU adds Channel Distributor (CD) and Noise Controller (NC) to reduce and replace the channels. This paper also propose Expected channel damage score (ECDS) to measure the importance of a channel to decide weather remove or replace it. Then the experiment shows result on cifar10/100 and imagenet data set with different network architectures.\nThe idea is interesting, however, the parameter flops reduced rate seems not very impressive. The SCU seems too complicated,so I want to know that if the SCU could accelerate the forward process on modern GPU or mobile devices? \nThe result of these networks seems not the state-of-the-art, if the result can be improved, the SCU could be more convincing.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "some concerns need to be clarified",
            "review": "This is an architecture design paper. It proposes a general structure called Selective Convolutional Unit that the authors claim to be useful for various CNN models. The SCU structure contains two major parts: CD and NC. CD for compressing/pruning channels and NC for multiplicative noise. The paper gives a measure, called expected channel damage score, on the change of the output for SCU. It also shows the effectiveness of SCU on CIFAR-10, CIFAR-100 and imagenet.\n\nSome questions and concerns:\n\n1. The paper spends too much space introducing the bottleneck structures and a whole lot of the details on the optimization of NC and CD are put in the appendix. I would suggest to reduce the section of introductory part and put a shorter version of appendix A and B to the main text so that the readers know more about the architecture and how it is optimized. In particular, the description on NC is confusing since without looking at the appendix it is not clear how the prior p(\\theta) is used. \n\n2. The experiment shows improvement on densenet and resnetXT, but the result is not the state-of-the-art. Wide-Resnet seems to get better accuracy on both CIFAR-10 and CIFAR-100 compared to the best accuracy reported in this paper. Also the number reported by the original densenet paper on imagenet seems to be better (densenet-264 has an error rate of 22.15/20.80)\n\n3. In your CD design, channel assignment \\pi is a discrete variable. How is it optimized in the training process?\n\n4. The proof of proposition 1 does not look correct to me. The optimization procedure makes use of the data X to determine your NC variable \\theta so \\theta depends on X. In this way you cannot factorize the expectation in the equation below (20) in your appendix.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "promising idea but over-complicated method. ",
            "review": "The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important.  To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there’s also an experiment on ImageNet\nIt looks to me that the proposed method is overly complicated. It is also described in a complicated manner.  I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers.\nThe results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}