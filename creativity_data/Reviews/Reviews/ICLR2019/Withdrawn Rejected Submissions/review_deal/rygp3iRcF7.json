{
    "Decision": {
        "metareview": "although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors' effort at improving the result further during the rebuttal period. this may be due to a less-than-desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "reject"
    },
    "Reviews": [
        {
            "title": "Some important related studies are missing.",
            "review": "\nI have several concerns about this paper.\n\n[originality]\nSome important related studies are missing.\n\n# Related studies about the perspective of “area”.\nThe consecutive position in sequence is often referred to as “span” in NLP filed, which is identical to what the authors call “area” in this paper.\nThen, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, \ne.g.,\nWenhui Wang, Baobao Chang, “Graph-based dependency parsing with bidirectional lstm”, ACL-2016.\nMitchell Stern, Jacob Andreas, Dan Klein, “A Minimal Span-Based Neural Constituency Parser”, ACL-2017.\nKenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, “End-to-end Neural Coreference Resolution”, EMNLP-2017.\nNikita Kitaev, Dan Klein, “Constituency Parsing with a Self-Attentive Encoder”, ACL-2018.\n\nSimilarly, there are several related studies in image processing field,\ne,g.,\nMarco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, “Areas of Attention for image captioning”, ICCV-2017\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, “Image Captioning with Semantic Attention”, CVPR-2016.\n\n# Related studies about the perspective of “structured attention”. \nSeveral papers about structured attention have already been proposed, \ne.g.,\nYoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. “Structured Attention Networks”, ICLR-2017.\nVlad Niculae, Mathieu Blondel. “A Regularized Framework for Sparse and Structured Neural Attention”, NIPS-2017.\n\n\nI think the authors should explain the relations between their method and the methods proposed in the above listed papers.\n\n\n[significance]\n# Concern about experimental settings\nThe experimental setting for NMT looks unnormal in the community.\nCurrently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings.\n\n\n# computational cost\nThe authors should report the actual calculation speed by comparing with the baseline method and the proposed method.\nIn Sec. 2.2, the authors provided the computational cost. \nI feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method.\n\n\n\nOverall, the proposed method itself seems to be novel and interesting.\nHowever, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write.\nThus, it is a bit hard for me to strongly recommend this paper to be accepted. \n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments are not convincing",
            "review": "[Summary]\nPaper “AREA ATTENTION” extends the current attention models from word level to “area level”, i.e., the combination of adjacent words. Specifically, every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn.(3 or 7) and Eqn. (4), and then the conventional attention models are applied to these new items. The authors work on (char level) NMT and image captioning to verify the algorithm. \n\n[Details]\n1.\tIn the abstract, “… Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task …”, can you provide an example to show why “an area of items” can “better capture the nature of the task”? In particular, you need to show why the conventional attention mechanism fails.\n2.\tIn this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$.\n3.\tTwo different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them?\n4.\tI am not convinced by the experimental results.\n(4a) On WMT’14 En-to-Fr and En-to-De, we know that “transformer_big” can achieve better results than the three settings shown in Table 1 & 2. The results of using transformer_big are not reported. Besides, it is not necessary to use the “tiny” setting for En-to-{De, Fr} translation considering the data size.\n(4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting.\n(4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers.\n(4d) It is really strange for me to “trained on COCO and tested on Flickr” (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too.\ne.\tWhat if we use different area size? I do not find the study in this paper.\n\n[Pros & Cons]\n(+) A new attempt of the attention model that tries to build the attention beyond unigrams.\n(-) Experiments are not convincing.\n(-) The motivation is not strong.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A few concerns",
            "review": "I prefer the idea of using some statistics (such as variances) of multiple items for attention. \nThis direction may lead to better attention units for future works. \n\nI do not fully understand the argument, \"Attention mechanisms are designed to focus on a single item in the entire memory\". \nIn my understanding, the attention formulation has no mathematical bias to focus on a single item. \nI have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. \nDo you have any comments? \n\nCould you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? \nFor example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT.  \nFor the image case, the adjacent area looks like a \"super pixel\". \n\nIt is unfortunate to observe that the gains on BLEU and perplexity are limited. \nSince the authors do not provide any statistical tests, or a confidence interval of the scores, \nI cannot be sure these gains are truly significant. \nFrom my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). \n\nSummary\n+ A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting\n- Claims are not so much convincing for the need of attending multiple adjacent items. \n- Gains in experiments are limited. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}