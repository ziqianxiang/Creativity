{
    "Decision": {
        "metareview": "The paper presents an interesting treatment of transforming a block-sparse fully connected neural networks to a ResNet-type Convolutional Network. Equipped with recent development on approximations of function classes (Barron, Holder) via block-sparse fully connected networks in the optimal rates, this enables us to show the equivalent power of ResNet Convolutional Nets. \n\nThe major weakness in this treatment lies in that the ResNet architecture for realizing the block-sparse fully connected nets is unrealistic. It originates from the recent developments in approximation theory that transforming a fully connected net into a convolutional net via Toeplitz matrix (operator) factorizations. However the convolutional nets or ResNets obtained in this way is different to what have been used successfully in applications. Some special properties associated with convolutions, e.g. translation invariance and local deformation stability, are not natural in original fully connected nets and might be indirect after such a treatment.  \n\nThe presentation of the paper is better polished further. Based on ratings of reviewers, the current version of the paper is on borderline lean reject.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting transformation of block-sparse fully connected net to ResNet Convolutional blocks, yet the ResNet architecture seems unrealistic and indirect. "
    },
    "Reviews": [
        {
            "title": "The block sparse structure seems unnecessary given the results in [Schmidt-Hieber 2017].",
            "review": "This manuscript shows the statistical error of the ERM for nonparametric regression using the family of a Resnet-type of CNNs. Specifically, two results are showed. First, the authors show that any block-sparse fully connected neural network can be embedded in CNNs. Second, they show the covering number of the family of CNNs. Combining with the existing results of the approximation error of neural nets (Klusowski&Barron 2016, Yarotsky 2017, Schmidt-Hieber 2017), they show the L2 statistical risk. \n\nDetailed comments:\n\n1. The intuition of using block-sparse FNN seems unclear. It seems that when $M=1$, it reduces to the sparse NN considered in [Schmidt-Hieber 2017]. In the proof of Corollary 5, the authors directly use the error of approximating Holder smooth function by sparse FNN and show that the construction in [Schmidt-Hieber 2017] is actually block-sparse. Thus, it seems unclear why we should consider such block-sparse family. Can any sparse NN be embedded in the family of CNNs?\n\n2. In the Related Work, the authors only compare with 2 previous work on the approximation error of CNN. Actually, this work is more related to [Schmidt-Hieber 2017] due to borrowing the results. It would be better to see what the novelties are compared with that work, especially in terms of the proof techniques.\n\n3. The authors claim that the construction of approximator for Holder functions in [Schmidt-Hieber 2017] is block sparse. It would be nice to give more details of the construction since this is not claimed in [Schmidt-Hieber 2017].",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Approximate block sparse fully connected neural networks, the Barron class and the Holder class using the Residual CNNs",
            "review": "The authors demonstrate the function expression properties for the Residual type convolutional neural networks to approximate the block sparse fully connected neural networks. Then it is shown that such Res-CNNs can approximate any function as long as it can be expressed by the block-sparse FNNs, including the Barron class and Holder class functions. The price to pay is that the number of parameters is larger than that of the FNNs by a constant factor. \n\nThe idea for connecting the expressive ability of CNNs with FNNs is interesting, which can fully take advantage of the power of FNNs to understand CNNs. However, it is not very clear how the convolutional structure of CNNs help in the analysis of approximating FNNs. For example, in the analysis of C.1 and C.2, it will help better understand why CNNs may work from a high-level intuition when the authors construct the filters. \n\nMoreover, it will also help better understand the expressive power of CNNs if the authors can provide some extended discussion on why approximating the block-sparse FNNs rather than arbitrary feed-forward networks. Is there any fundamental reason (or a counterexample) this cannot be realized, or is there to some extent a technical barrier in the analysis? \n\nMinor issue\n\nOn page 20, “Bounds residual blocks” -> “Bounds for residual blocks”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approximation and estimation results, but considers somewhat unrealistic CNNs",
            "review": "The paper studies approximation and estimation properties of CNNs with residual blocks in the context\nof non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),\nand leveraging previous approximation results for such functions.\nExplicit risk bounds are obtained for regression functions in Barron and Holder classes.\n\nThe main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs\ncontains a class of \"block-sparse\" fully-connected networks, with appropriate constraints on various size quantities.\nThis result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk\n(Theorem 2, which mostly follows Schmidt-Hieber (2017)),\nas well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.\n\nThe construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,\neven with a filter size that is arbitrarily fixed.\nHowever, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,\nsince they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,\nwhich is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).\nIn particular, CNNs are typically used when there is some relevant inductive bias such as equivariance\nto translations (and invariance with pooling operations) to take advantage of,\nso removing this inductive bias by approximating fully-connected architectures seems a bit twisted.\nThe approach of reducing the function class to be approximated would seem more relevant here,\nas in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of\nthe present paper can be useful in such a scenario as well.\n\nSeparately, the presentation of the paper could be significantly improved,\nfor instance by introducing relevant notions more clearly in the introduction and related work sections,\nand by providing more insight and discussion of the obtained results in the main paper.\n\nMore specific comments:\n- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs\n- Section 2: Explain what is \"s\" in the Barron class, or at least point to the relevant definition in the paper\n- Section 3.1:\n  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat\n  * why is the estimator \"regularized\"?\n- Definition 2: shouldn't it be D_m^(0) = D instead of 1?\n- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.\n- Section 4.2: M_1 is the Lipschitz constant of what function?\n- Section 5.1: \"M = 1\" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.\n- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}