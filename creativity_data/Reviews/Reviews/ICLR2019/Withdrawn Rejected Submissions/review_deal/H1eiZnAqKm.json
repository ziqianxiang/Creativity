{
    "Decision": {
        "metareview": "The paper analyses GRUs using dynamic systems theory.  The paper is well-written and the theory seems to be solid.\n\nBut there is agreement amongst the reviewers that the application of the method might not scale well beyond rather simple 1- or 2-D GRUs (i.e., with one or two GRUs).  This limitation, which is an increasingly serious problem in machine-learning papers, should be solved before the paper should be published.  A very recent extension of the simulations to 16 GRUs improves this, but a rigorous analysis of higher-dimensional systems is pending and poses a considerable block for acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "scaling issue"
    },
    "Reviews": [
        {
            "title": "a dynamical systems analysis of 1d and 2d gated recurrent units",
            "review": "This paper analyzes GRUs from a dynamical systems perspective, i.e. phase diagrams, fixed points, and bifurcations. The abstract and intro are well written and motivate the need for more theoretical framework to understand RNNs, especially how well they are able to represent and express temporal features in the training data. The dynamical systems analysis is well presented and visualized nicely. \n\nMost of the paper concentrates on the 1d (one single GRU) and 2d (two GRU's) case.  They show that 2d GRUs can be trained to adopt a variety of fixed points, can approximate a line attractors (an important feature for short-term memory), but cannot mimic a ring attractor.\n\nMy concerns are:\n\n- The derivation of the continuous time dynamical system (Appendix A) is confusing to me. Unless I'm not following the derivation correctly, should there be another \\Delta t in the denominator of the right-hand side of (23), from (22)? It's confusing to me that the continuous-time version in (26) has essentially the same form as the discrete-time version in (22).\n\n- The applicability of this analysis to RNNs of even modest size is unclear. Generically, there's no reason to believe the intuitions from 2d should necessarily generalize to higher dimensions, and rigorous analysis of higher dimensional systems of this kind can be fairly challenging, even if one starts from a continuation analysis.\n\n- Small typo: top of Page 4, figure should refer to 3A, not 2A.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "There are several issues with this interesting analysis ",
            "review": "The authors analyse GRUs with hidden sizes of one and two as continuous-time dynamical systems, claiming that the expressive power of the hidden state representation can provide prior knowledge on how well a GRU will perform on a given dataset. Their analysis shows what kind of hidden state dynamics the GRU can approximate in one and two dimensions. In the experimental part, they show how a GRU with two hidden states trained on a multistep prediction task can learn such dynamics.\n\nAlthough RNNs are important for Machine Learning, the paper seems to contain flaws in the theoretical part, which seem to invalidate some of the claimed results. But we may change our rating in case of a convincing rebuttal.\n\nThe Proof of Lemma 2 claims that h(t) achieves all values on the real set, which is false (h(t) assumes values in (-1,1)). Nevertheless, the theorem should hold since there is always at least one intersection between h and tanh(f(h)).\n\nLemma 1 claims that for any choice of parameters, there exist only finitely many fixed points. However, in the proof the authors only show that the number of fixed points cannot be uncountable, without taking into consideration the possibility that there are countably many fixed points. The proof also omits steps concerning the Taylor expansion which would make the proof clearer: We suggest adding those steps in the appendix. Furthermore, when equation (12) is Taylor-expanded, the authors do not consider the case where the GRU parameters are such that the argument of function “sech” is outside its convergence radius. These might be parameters for which there are infinitely many fixed points, even if we are unable to provide a Taylor expansion. The Lemma may still be correct, but this does not seem to be a complete proof.\n\nThe authors claim that an arbitrarily close approximation of a line attractor can be created using two GRUs, but no proof is provided.\n\nThe experimental part is difficult to evaluate since there are no learning curves for the three tasks. For instance, it is difficult to judge whether the GRUs are unable to learn the dynamics of a ring attractor because of theoretical limitations or because the model has not been properly trained for the specific task.\n\nThe paper is easy to read, except for certain parts where it is not clear if some of the statements are true in general or just have not been proven false by the authors. It is not clear why Figure 3 is representing all possible simple fixed points and bifurcation fixed points: is there a theoretical result stating that these are the only possible topologies, or are these the only ones found? The same question applies for the 36 images in figure 9. The range of the parameters used for finding these configurations is not specified.\n\nSince the hidden state assumes values in (-1,1)^2, why is its range in most of the images (-1.5,1.5)?\n\nWe are not familiar with related work on transformations from discrete to continuous dynamical systems: are the dynamics of the discrete time GRU model preserved in the transformation? If so, is there a reference for this? Are the phase portraits in the middle row of figure 8 generated by letting the discrete GRU system evolve, or is the continuous system used with the parameters of the trained GRU?\n\nWe would like to see more explanations of why various topologies are useful for the applications mentioned in the paper. Given a generic dataset, how can these results help to understand how well a GRU will perform?\n\nWhat is the reason behind the belief that the analysis extends to higher dimensions? The effects of a 1D -> 2D extension are far from trivial - why should that be different for higher dimensions?\n\nThe problem the authors want to solve seems important, and some of the theoretical results are promising, but we think that this paper has to be further polished before acceptance.\n\nIt is possible that we will increase the score if the authors can provide clarifications on the above questions.\n\nAdditional comments:\n\nIntroduction\n\n-The vanishing gradient problem was not discovered in 1994, but in 1991 by Hochreiter: \n\nSepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TU Munich, 1991. Advisor J. Schmidhuber.\n\n- Make clear that GRU is a variant of vanilla LSTM with forget gates (where one gate is missing):\n\nGers et al. “Learning to Forget: Continual Prediction with LSTM.“ Neural Computation, 12(10):2451-2471, 2000. \n\n- The intro says that GRU has become widely popular and cites Britz et al 2017, but Britz et al actually show that LSTM consistently outperforms its variant GRU in Neural Machine Translation. Please clarify this. \n\n- Also mention Weiss et al (“On the Practical Computational Power of Finite Precision RNNs for Language Recognition”) who exhibited basic limitations of GRU when compared to LSTM. \n\n- Is the result by Weiss et al actually related to the result of the authors who found that 2 GRUS cannot accurately a line attractor without near zero constant curvature in the phase space?\n\n\nSection 2\n\n-Wrong brackets in equation (4)\n-Missing bracket before citing Laurent & Brecht\n\nSection 4\n\n-”We conjecture that the system depicted in figure 2A..” Should be figure 3A\n- Lemma1: UZ has capital Z subscript\n\nSection 5.2\n\n-”The added affine transformation allows for a sufficiently long subinterval”: “sufficiently long” is too vague\n\nSection 5.3\n\n“A manifold with without near zero constant curvature”: should be “a manifold without near zero constant curvature”\n\nAppendix A\n\n-Wrong brackets in equation (20)\n\nAppendix B\n\n- In the proof of Theorem 1, the derivative is of (29), not of (12)\n\nAppendix C\n\nFigure 9: “who’s initial conditions” should be “whose initial conditions”\n\nAfter rebuttal:\n\nIt's better now. However, the revised introduction still says:  \"GRU has become wildly popular in the machine learning community thanks to its performance in machine translation (Britz et al., 2017) ... LSTM has been shown to outperform GRU on neural machine translation (Britz et al., 2017).... specifically unbounded counting, come easy to LSTM networks but not to GRU networks (Weiss et al., 2018).\" \n\nSo better remove the first statement on Britz et al: \"GRU has become wildly popular ... in machine translation (Britz et al., 2017)\" because they actually show why GRU is NOT wildly popular in machine translation, as correctly justified later in the same paragraph.\n\nPending the above revision, we'd like to increase our evaluation by 2 points, up to 6!\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for the expressive power of GRUs as a continuous dynamical system.",
            "review": "Here the authors convert the GRU equations into continuous time and use theory and experiments to study 1- and 2-dimensional GRU networks. The authors showcase every variety of dynamical topology available in these systems and point out that the desirable line and ring attractors are not achievable, except in gross approximation.  The paper is extremely well written.\n\nI am deeply conflicted about this paper.  Is the analysis of 1 or 2 dimensional GRUs interesting or significant? That’s a main question of this paper.  There is no question of quality, or clarity, and I am reasonably certain nobody has analyzed the GRU in this way before.\n\nOn the one hand, the authors bring a rigor and language to the discussion of recurrent networks that is both revealing (for these examples) and may to bear fruit in the future.  On the other hand, the paper is exclusively focused on 1- and 2-dimensional examples which have precisely no relevance to the recurrent neural networks as used and studied by machine learning practitioners and researchers, respectively. If the authors have proved something more general for higher dimensional (>2) cases, they should make it as clear as possible.\n \nA second, lesser question of relevance is studying a continuous time version.  It is my understanding that discrete time dynamics may exhibit significantly more complex dynamical phenomenon and again practitioners primarily deploy discrete time GRUs.  I understand that theoretical progress often requires retreating to lower dimensionality and (e.g. linearization, etc.) but in this case it is not clear to me that the end justifies the means.  On the other hand, a publication such as this will not only help to change the language of RNNs in the deep learning community, but also potentially bring in more dynamical systems specialists into the deep learning field, which I thoroughly endorse.\n\nModerate concern\n\n“In order to show this major limitation of GRUs …” but then a 2-gru is used, which means that it’s not a general problem for GRUs with higher dim, right?  Also, won’t approximate slow points would also be fine here? I think this language needs to be more heavily qualified.\n\nMinor\n\nGRU almost always refers to the network, even though it is Gated Recurrrent Unit, this means that when you write ‘two GRUs’, the naive interpretation (to me) is that you are speaking about two networks and not a GRU network with two units.\n\nSide note requiring no response: It might be interesting to study dynamical portrait as a function of training for the two-d GRU.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}