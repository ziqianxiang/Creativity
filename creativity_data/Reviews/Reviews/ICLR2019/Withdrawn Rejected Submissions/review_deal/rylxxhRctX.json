{
    "Decision": {
        "metareview": "The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not enough for acceptance "
    },
    "Reviews": [
        {
            "title": "Additional Review",
            "review": "\nThe paper proposes to alleviate two issues with VAEs and GANs\nMode covering behaviour of the MLE loss used in VAEs causing blurry image samples despite high likelihood scores (Coverage driven training in the paper)\nPoor likelihood scores in GAN based models despite great looking amples (Quality driven training in the paper)\n\nBoth of these issues are well known and have been previously described in the literature (e.g. MLE models: bishop 2006, GANs: arjovsky et. al. 2017 or sonderby et. al. 2017)\n\nThe main contribution of the paper are described in the Eq (8) augmenting the VAE ELBO (L_C term) with a GAN-discriminator based loss (L_Q term). Combining the fact that the L_Q loss (essentially MLE) minimises KL[data|model] and L_Q discriminator loss used minimises KL[model|data] the authors show that this in theory minimizes a lower bound on the forward and reverse KL divergence i.e. somewhere between mode-covering and mode-seeking behavior\n\nAs a secondary contribution the authors show that adding a flow based module to the generative model p(x|z) increases both the likelihood and the fidelity of the samples. \nThe experimental section seems sound with extensive results on CIFAR10 and some additional results on STL, LSUN, CelebA and ImageNet. \n \nComments\nOverall i find the paper well written and easy to understand and the experiments seems sound. My main criticism of the paper is the novelty of the proposed model. Adding a VAE and a GAN has been done before (e.g. Larsen 2016 as the authors cite as well). Outside of generative models the combination of MLE and GAN based losses have been studied in e.g. Super-Resolution (Shi et. al. 2016). In both papers of these papers the GAN based losses are added for exactly the same reasons as provided in this work i.e. to increase the sample fidelity. \n\nI’m unsure if adding a flow to the output of the VAE generative model have been done before, however in spirit the approach is quite similar in the ideas in “Variational Lossy Autoencoder” (Chen 2016) or PixelVAEs (Gulrajani 2016) where local covariance is added through conditional factorization.\n \t \t \t\t\t\t\t\nQuestions\nQ1) One criticism of the results is that the authors claim that the model achieves likelihood values competitive with state-of-the-art where the results seems to suggest that a more sound conclusion is that the the likelihoods are better than GANs but worse than MLE models such as PixelCNN++. Similarly for the Inception/FID scores where the model is better than VAEs but most of the time slightly worse than pure GAN models. ?\n\nQ2) I find the claim “we propose a unified training approach that leverages coverage and quality based criteria” a bit overreaching. The loss in Eq(8) is simply the sum of a VAE and a GAN based loss and as such does not provide any unification of the two ?\n\nQ3) Related to the previous question The KL-divergence interpretation of the GAN based loss assumes a bayes optimal discriminator. In practice this is usually never achieved why it is not really known what divergence GANs minimize if any (see e.g Fedus et. al. 2017 for a non-divergence minimization view). If the KL-divergenve minimization interpretation does not hold the proposed model is essentially a VAE with an auxiliary GAN loss biasing the model towards mode-seeking training? \n\nQ4) There haven’t been many successful GAN usages outside of of the CNN-realm which suggests that a successful GAN is tightly connected to the inductive bias provided by CNNs for images. Have the authors tried there model on something else than images? (I suspect that the GAN based L_Q loss will be hard to apply outside of the realm of images)\n\t\nOverall i find the paper well written and easy to understand and the experiments seems sound.  However I think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model. Given my comments above I think the paper is marginally below acceptance however I could be be convinced otherwise by e.g. a solid use-case for models like this?\n\nLastly, I'm sorry for the late review (I was called upon late to review the paper) - I hope that you will find the time for a proper rebuttal to my questions. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, but there are some unclear issues",
            "review": "It is well known that optimizing divergences of different directions leads to different learning behaviors - the mode covering behavior of maximum likelihood training and the mode missing behavior of GAN training. This paper makes a good presentation in explaining coverage-driven training and quality-driven training. \nTechinically, this paper make two contributions. First, extend VAEs by using deterministic invertible transformation layers to map samples from the decoder to the image space. Second, use the loss Eq. (8) to train the generator.\n\nHowever, there are some unclear issues.\n\nFirst, the differences between losses may not fully explain the behavior of GANs [I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2017], as also seen from some recent studies. For example, using the sum of two divergences [Chen et al., 2018] does not make a significant improvement. \nAlso shown in Table 1, CQ performs better than VAE, but is inferior to GAN.\nUsing the two-term loss Eq. (8) may not be the key for improvement.\n\nOnly after adding the additional flow-based layers, CQF outperforms GAN. Therefore, in Table 1, it would be better to include the result of VAE using the additional flow-based layes for ablation study. \n\nSecond, it is also not clear from the paper that such VAE using the additional flow-based layes is new or not.\n\nThird, the results are not as good as the state-of-the-art. In Table 4, SN-GANs perform the best in three out of four cases.\n\nFourth, the model consists of three networks - encoder, decoder and discriminator. Evaluation about the model's inference capability is necessary in addition to showing its generation capability, since it is equipped with a decoder.\n\nSome typos:\nP5: L_Q(p*)+L_Q(p*) ?\nTable 4: QSF ?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "see below",
            "review": "The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. \nThe paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. \n\nI incline to my current score after reading the response and other reviews.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}