{
    "Decision": {
        "metareview": "The reviewers in general like the paper but has serous reservations regarding relation to other work (novelty) and clarity of presentation. Given non-linear state space models is a crowded field it is perhaps better that these points are dealt with first and then submitted elsewhere.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "borderline - but leaning to reject because of reviewer reservations"
    },
    "Reviews": [
        {
            "title": "Nice algorithm but need better motivation",
            "review": "This paper discusses a algorithm for variational inference of a non-linear dynamical models. In this paper model assumption is to use single stage Markov model in latent space with every latent variable Z_t to be defined Gaussian distributed with mean depends on Z_(t-1) and time invariant variance matrix lambda. The non linearity in transition is encoded in mean of Guassian distribution. For modeling the likelihood and observation model, the Poisson or Normal distribution are used with X_t being sampled from another Gaussian or Poisson distribution with the non-linearty being encoded in the parameters of distribution with variable Z_t.  This way of modeling resembles so of many linear dynamical model with the difference of transition and observation distribution have nonlinearity term encoded in them. \nThe contribution of this paper can be summarized over following points:\n\n- The authors proposed the nonlinear transition and observation model and introduced a tractable inference model using Laplace approximation in which for every given set of model parameter solves for parameters of Laplace approximation of posteriori and then model parameters get updated until converges\n\n-the second point is to show how this model is successful to capture the non-linearity of the data while other linear models do not have that capabilities \n\n\nNovelty and Quality: \nThe main contribution of this paper is summarized above. The paper do not contain any significant theorem or mathematical claims, except derivation steps for finding Laplace approximation of the posteriori. The main challenge here is to address effectiveness of this model in comparison to other non-linear dynamical system that we can name papers as early as Ghahramani, Zoubin, and Sam T. Roweis. \"Learning nonlinear dynamical systems using an EM algorithm.\" Advances in neural information processing systems. 1999. \nor more recent RNN paper LSTM based papers. I think authors need to distinguish what this paper can give to community beside approximate posteriori of latent variables that other competing models are not capable of. If the aim is to have that posteriori, the authors should show what type of interpretation they have drawn from that in experiments.\nThere are lots of literature exist on speech, language models and visual prediction which can be used as reference as well.\n\nClarity: \nThe paper is well written and some previous relevant methods have been reviewed . There are a few issues that are listed below: \n\n1- as mentioned in Quality sections authors should be more clear about what is distinguished in this paper that other non-linear dynamical systems \n\n2- they used short form RM for Recognition model or FPI for fixed point iteration that need need to be defined before being used\n\n\n\nsignificance and experiments: \nThe experiments are extensive and authors have compared their algorithm with some other linear dynamical systems (LDS) competing algorithms and showed improvement in many of the cases for trajectory reconstruction. \nA few points can be addressed better, it can be seen for many of experiments exhaustive search is used for finding dimension of latent variable. This issue is addressed in Kalantari, Rahi, Joydeep Ghosh, and Mingyuan Zhou. \"Nonparametric Bayesian sparse graph linear dynamical systems.\" arXiv preprint arXiv:1802.07434 (2018). That paper can use non-parametric approaches to find best latent dimension, although the paper applied the technique on linear system, same technique could be adopted to non-linear models. Also that model is capable of finding multiple linear system that model the non linearity by switching between diffrent linear system, for switching linear system, this paper can be named as well: Linderman, Scott, et al. \"Bayesian learning and inference in recurrent switching linear dynamical systems.\" Artificial Intelligence and Statistics. 2017.\n\nIt is shown that the model can reconstruct the spikes very well while linear model do not have that power (which is expected), but it is interesting to see how other non-linear models would compare to this model under those certain conditions\n\nIt is desired and interesting to see how the model behave one step ahead and K-step ahead prediction. Please address why it cannot be done if there is difficulties in that.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Excellent method and results but need more comparisons and better writing",
            "review": "I'll start with a disclaimer: I have reviewed the NIPS 2019 submission of this paper which was eventually rejected. Compared to the NIPS version, this manuscript had significantly improved in its completeness. However, the writing still can be improved for rigor, consistency, typos, completeness, and readability.\n\nAuthors propose a novel variational inference method for a locally linear latent dynamical system. The key innovation is in using a structured \"parent distribution\" that can share the nonlinear dynamics operator in the generative model making it more powerful compared. However, this parent distribution is not usable, since it's an intractable variational posterior. Normally, this will prevent variational inference, but the authors take another step by using Laplace approximation to build a \"child distribution\" with a multivariate gaussian form. During the inference, the child distribution is used, but the parameters of the parent distribution can still be updated through the entropy term in the stochastic ELBO and the Laplace approximation. They use a clever trick to formulate the usual optimization in the Laplace approximation as a fixed point update rule and take one fixed point update per ADAM gradient step on the ELBO. This allows the gradient to flow through the Laplace approximation.\n\nSome of the results are very impressive, and some are harder to evaluate due to lack of proper comparison. For all examples, the forward interpolate (really forecasting with smoothed initial condition) provides a lot of information. However, it would be nice to see actual simulations from the learned LLDS for a longer period of time. For example, is the shape of the action potential accurate in the single cell example? (it should be since the 2 ms predictive r^2 shows around 80%).\n\nExcept in Fig 2, the 3 other examples are only compared against GfLDS. Since GfLDS involves nonconvex optimization, it would be reasonable to also request a simple LDS as a baseline to make sure it's not an issue of GfLDS fitting.\n\nFor the r^2=0.49 claim on the left to right brain prediction, how does a baseline FA or CCA model perform?\n\nWas input current ignored in the single cell voltage data? Or you somehow included the input current as observation model?\n\nAs for the comment on Gaussian VIND performing better on explaining variance of the data even though it was actually count data, I think this maybe because you are measuring squared error. If you measured point process likelihood or pseudo-r^2 instead, Poisson VIND may outperform. Both your forecasting and the supplementary results figure show that Poisson VIND is definitely doing much better! (What was the sampling rate of the Guo et al data?)\n\nThe supplementary material is essential for this paper. The main text is not sufficient to understand the method.\n\nThis method relies on the fixed point update rule operating in a contractive regime. Authors mention in the appendix that this can be *guaranteed* throughout training by appropriate choices of hyperparameters and network architecture. This seems to be a crucial detail but is not described!!! Please add this information.\n\nThere's a trial index suddenly appearing in Algorithm 1 that is not mentioned anywhere else.\n\nIs the ADAM gradient descent in Algorithm 1 just one step or multiple?\n\nMSE -> MSE_k in eq 13\n\nLFADS transition function is not deterministic. (page 4)\n\nlog Q_{phi,varphi} is quadratic in Z for the LLDS case. Text shouldn't be 'includes terms quadratic in Z' (misleading).\n\nregular gradient ascent update --> need reference (page 4)\n\nDue to the laplace approximation step, you don't need to infer the normalization term of the parent distribution. This is not described in the methods (page 3).\n\nEq 4 and 5 are inconsistent in notation.\n\nEq (1-6) are not novel but text suggests that it is.\n\nPredict*ive* mean square error (page 2)\n\nIntroduction can use some rewriting.\n\narXiv papers need better citation formatting.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Incremental technical contribution but with extensive experimental evaluation",
            "review": "The paper presents a variational inference approach for locally linear dynamical models. In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,  enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO. Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.  \n\nQuality: The experiments appear to be well designed and support the main claims of the paper. \n\nClarity: The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately. It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way. I also struggled a little to understand what is the difference between forward interpolate and filtering. \n\nOriginality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted. In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.  \n\nSignificance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics. \n\nOverall, this appears to be a board-line paper with weak novelty. On the positive side, the experimental validation seems well done. The clarity of this paper needs to be strengthened.  \n\nMinor comments: \n- abstract: uncover nonlinear observation? -> maybe change \"observation\" to \"latent dynamics\"?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}