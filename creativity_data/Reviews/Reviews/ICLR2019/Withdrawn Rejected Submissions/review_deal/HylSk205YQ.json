{
    "Decision": {
        "metareview": "The paper presents an extension of MADDPG, adding communication between agents. The methods targets extremely noisy observations settings, so that agents need to decide if they communicate their private observations (or not). There is no intrinsic/explicit reward to guide the learning of the communication, only the extrinsic/implicit reward of the downstream task.\n\nThe paper is clear and easy to follow, in particular after the updated writing. I believe some of the reviewers' points were addressed by the rebuttal. Nonetheless, some of the weaknesses of the paper still hold: namely the complexity of the approach compounded with a very specific experimental evaluation. The more complex an approach is (and it may be justified by the complexity of the setting!), the more varied its supporting evidence should be.\n\nIn its current form, the paper would constitute a good workshop contribution (to discuss the approach), but I believe it needs more varied (and/or harder) experiments to be published at ICLR.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "An interesting take on partially observable MARL, without enough supporting evidence"
    },
    "Reviews": [
        {
            "title": "Review for Paper \"Multi-agent Deep Reinforcement Learning with Extremely Noisy Observations\"",
            "review": "This paper addresses the challenge of learning in extremely noisy environments. The fundamental idea is to combine deep reinforcement learning of individuals, in which individuals can choose whether they share information in order to maximise the overall reward, which is a substantial difference from existing solutions in the area. To achieve this, the authors propose a hierarchical approach in which agents learn from experience, before deciding whether to share information. \n\nTo explore the performance, the authors modify an existing scenario and implement baselines that represent idealised outcomes and contemporary approaches with varying levels of communication among agents. The proposed approach performs favourable compared to alternative approaches, despite its strongly decentralised operation, and is surprisingly close (and in some cases exceeds) the ideal solution with optimal communication. \n\nThe paper is well structured and systematic in the introduction of the underlying concepts in order to retrace the complex architectural setup. Experiment and alternative architectures are described in sufficient level of detail. \n\nThe quality of the presentation is high and accessible. Prospects for future work are highlighted. At this stage, observations are limited to a single observation at a time. The authors could be more explicit about potential further challenges in using the current solution and discuss its versatility in other scenarios. However, overall, the described hierarchical approach provides an interesting avenue to address the issue of noisy observations, which warrants discussion. ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Extrinsic reward and intrinsic reward are confusing",
            "review": "This paper studies multi-agent reinforcement learning where the agents need to communicate information when observations are noisy.  The agents thus need to learn what information should be sent to other agents.\n\nThe authors claim \"we do not assume the existence of explicit rewards guiding the communication action,\" which however is questionable.  The \"extrinsic reward\" used to guide the communication action is simply the cumulative reward between two communication actions.  The reward is explicitly given.\n\nThe key assumption is that communication is not performed every step.  Then standard cumulative reward until the next communication can be used as immediate reward for the previous communication.  Should this assumption be considered as an assumption of the domain where the proposed approach can be applied, or is this assumption rather a technique that one should use even when communication can be performed every step?  In the latter case, the effectiveness is sparse communication is not demonstrated.\n\nIn addition, the intrinsic reward for guiding environmental actions is unclear.  In the experiment, the standard reward is simply used as intrinsic reward.  So, intrinsic reward is just standard (extrinsic) reward?  In general, how should we design intrinsic reward?  What is the advantage of not using the standard reward as intrinsic reward?\n\nThe experimental settings are too ideal for the proposed approach, and it is unclear how the proposed approach work in practical settings.  In particular, sequential decision making is not essential in the experimental settings.  What are the real applications in mind?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Their proposed method extends MADDPG with a communication network. Concerned about unnecessary complexity of the algorithm and formalism.",
            "review": "This paper is clearly written and explains everything in a good detail. I have a few questions about the design of the algorithm and experiments that I will explain next. Most importantly, I am confused why the communication actions are modeled with continuous actions. Also, the communicating agent idea is incorporated in MADDPG paper, and the contribution of the proposed network is unclear to me.  Right now, I am leaning toward weak reject now but I might update my evaluations after seeing the authors' feedback.\n\n1) First, your construction of communication medium simply seems to be learning a method for graph sparsification and this deserves some explanation.  Also, I think that using the graph terminology for describing the communication medium structure will significantly improve the clarity of the paper. For example, I assume that by saying that m^t = ...=m^{t+C-1} you mean that you simply fix the communication graph structure for C steps, not the communicating observations. Based on your notations, it is a little bit confusing -- in your notations $m^t$ is the set of observation that flows through the graph which should be different than $m^{t+1}$.\n\n2) Even MADDPG is very challenging to train! Now, this paper utilizes two MADDPG, and that is something that concerns me a lot. I don't think that replicating the results of this paper is possible by other people. How much was the cost of the hyper-parameters search? \n\n3) Why the decision of where to send the observations is modeled with a continuous control action? It can be simply modeled with discrete action in a more efficient way, right? What I mean is that $c_i$ can be a binary which tells whether send an observation or not. Am I missing anything?\n\n4) In section 2, you argue that in the original MADDPG paper, there is no inter-agent communication. As far as I remember, they have some experiments for cooperative communication or covert communication in which the communication is allowed between the agents. I would like to know more about this statement; maybe I am missing something. Why you are not designing the communication network which is a differentiable medium such as Foerster 2015? Isn't that efficient?\n\n5) In alternating case, I don't see (intuitively) why the communication should help to improve upon MADDPG. My intuition is that each agent will be the gifted one 1/3 on average. This means that the agents cannot perceive who gives the correct information and the policy should converge to a point where the communication does not give any new information.\n\n6) I would like to see what will happen with C=1? I think this hyper-parameter deserves some analysis to see how it affects the performance of the proposed method.\n\n7) In section 5, you say that in original DDPG, there is no real need for inter-agent communication\". This is a little bit strict statement, I guess. For example in the case with full observability, the agents can send messages which conveys their intention and help each other.\n\n\n\nMinor:\n* I would suggest using partially observability terminology instead of saying noisy observation because I think it includes a more general class of the problems to solve.\n* \"that a coupled through a communication medium\" -> \"that are coupled through a communication medium\"\n* In section 4.2, it is unclear to me what is the exploration strategy. Please explain more.\n* section 5.2: Using the term lower bound is not accurate. Try changing it to something else or use with quotations: \"lower bound\"\n* What will happen you choose the top-k rule for sending the information? For example, does top-2 (two-to-one) rule improve the results? The experiments might be added in future.\n-----------------------------------\npost rebuttal: the authors have responded to my main questions, and I would like to increase my score, but I cannot agree with them on possile future extensions of this work, e.g. in learning from pixels.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}