{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting Preliminary Idea but Needs More Work",
            "review": "The paper presents a new strategy for unsupervised learning. The idea is that a given a random sample, the encoder returns a compressed representation. The decoder then tries to use this representation to pick out the correct sample in the training set.\n\nThe idea is interesting but I have a number of concerns.\n\n(1) I am not convinced that this approach will not degenerate to just learning some sort of hash function. What is encouraging similar images to have similar representations? \n\nFor instance, if the discriminator had access to the encoder function it could just run the encoder on all the samples in the dataset and then pick out the one most similar to the description it has received. (I agree the discriminator doesn't have access to the encoder, but it can learn to mimic it based on a large number of samples)\n\n\n(2) The math could be written a little more rigorously in Section 2.\n\n\n(3) The experiments are not thorough and there is no comparison to existing approaches. \n\n\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Preliminary but interesting idea which requires more work before it can be published",
            "review": "The authors propose a method to learn \"useful\" latent representations in an unsupervised manner. Their model is different from the previously proposed models of either using an autoencoder architecture or using adversarial training. Instead, in the proposed model, the discriminator plays a cooperative game with an encoder to make it learn a representation which is meaningful. The entire model is trained end-to-end and the authors show some preliminary results on two fairly small scale datasets. \n\nWhile the proposed approach is quite interesting, there are a number of issues I have with the presented manuscript.\n\n- The work presented is quite preliminary and I feel requires more polishing and results before one can deem it suitable for publication. \n- Given that the latent representation of the current input has to be measured against the representations of all the other inputs in the datasets, makes the approach not scalable to large datasets (having millions or more examples). \n- Why did the authors choose to embed the inputs in only two dimensional space? \n- The experimental results are rather weak: tested on only a couple of small scale datasets with very little quantitative experiments. \n- While the authors show clusters in the latent space, there is no verification on how useful these representations might be for the end goal. \n\nGiven the various issues I feel that the paper is not ready for publication. The approach is definitely interesting and I hope the authors can work more to further validate the efficacy of the model and submit again when ready. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "not yet ready to be published",
            "review": "The paper proposes a novel encoder-based unsupervised representation learning. The network consists an encoder and a discriminator. The encoder learns to generate sample-specific codes, and discriminator tries to assign the generated code to the corresponding sample, given the correct sample and a set of incorrect samples. The method is tested on two relatively small datasets: MNIST and Fashion MNIST. \nThe essence of the method is novel and make sense. The text is short but clear. the method technically sounds. However, the experimental part lacks a quantitative evaluation as well as a comparison to state-of-the-art methods, which makes the assessment of the proposed model difficult. Qualitative results are informative but not enough. Another disappointing point in experiments is that the method is evaluated on two relatively small datasets, with very small latent space (d=2). \nI expected two major experiments: (1) applying the model on a bigger dataset, as mentioned in future works, (2) consider this game as a self-supervised representation learning method and use it as an initialization for a supervised task. It gives the opportunity to compare it against others as well.\n\nother points:\n- the paper lacks a comprehensive literature review. the topic is well studied and deserves to cite more than 8 references. \n- some details are missing in the text e.g. fully-connected size, training hyper-parameters, number of incorrect samples fed to discriminator during training, training time, number of parameters.\n- ablation study: which one is a better feature learning network: Encoder or convolutional layers of Discriminator?\n\nIn general, I believe that some significant analyses are missing and it is not possible to accept a paper without having additional insights. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}