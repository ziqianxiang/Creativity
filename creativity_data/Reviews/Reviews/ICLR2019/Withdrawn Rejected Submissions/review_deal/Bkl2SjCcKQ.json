{
    "Decision": {
        "metareview": "The paper points out a statistical properties of GAN samples which allows their identification as synthetic.\n\nThe paper was praised by one reviewer as well-written, easy to follow, and addressing an interesting topic. Another added that the authors did an excellent job of \"probing into different statistical perspectives\", and the fact that they did not confine their investigation to images.\n\nTwo reviewers leveraged the criticism that various properties discovered are not surprising given the loss functions and associated metrics as well as the inductive biases of continuous-valued generator networks. Tests employed were criticized as ad hoc, and reviewers felt that their generality was limited given their reduced sensitivity on certain modalities. (While Figure 10b is raised by the authors several times in the discussion, and the test statistics of samples are noted to be closer to the test data than to the random baseline, the test falsely rejects the null [p-value ~= 0.0] for non-synthetic test data.)\n\nI would encourage the authors to continue this line of inquiry as it is overall agreed to be an interesting topic of relevance and increasing importance, however based on the criticisms of reviewers and the content of the ensuing discussion I do not recommend acceptance at this time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting topic, not ready for publication."
    },
    "Reviews": [
        {
            "title": "Continuous generators in GANs don’t capture discrete statistics?",
            "review": "The paper proposes statistics to identify fake data generated using GANs based on simple marginal (summary) statistics or formal specifications automatically generated from real data. The proposed statistics mostly boil down to the fact that continuous-valued generator neural networks can’t adequately generate data distributions that are topologically different from the distribution in the latent z-space. The differences in the summary data/ feature statistics and statistics corresponding to formal specifications between fake and real data are of the above nature. \n\nThis seems fairly obvious but I haven’t seen this property of GANs being exploited to distinguish between GAN generated and real-data\n\nThis property/ shortcoming of the generator is not surprising at all and has been acknowledged before. See, for example, the discussions in,\n\nKhayatkhoei et al, Disconnected Manifold Learning for Generative Adversarial Networks, arXiv:1806.00880 (NIPS, 2018)\n\nThis has spurred various approaches to mitigate this shortcoming. See, for example,\n\nBen-Yosef and Weinshall, Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images, arXiv:1808.10356\n\nJang, Gu and Poole, Categorical Reparameterization with Gumbel-Softmax, arXiv:1611.01144 (ICLR 2017)\n\nSo, the fact that summary statistics predicated on discreteness of data or discreteness of their dependencies can distinguish GAN-generated data from real data is not surprising at all. In fact, in the paper itself, figure 10 and 11 show that for continuous data like speech, the proposed statistics are unable to distinguish between the fake and real data. \n\nBeyond this, even though it's interesting, there isn’t enough contribution in the paper.  It would be great if the authors can extend this observation and show if such statistics can always be found and tricks like Gumbel-Softmax/ GMM-GANs etc are doomed to fail or if certain extentions of GAN architectures can handle such statistics.\n\nFurthermore, the paper needs to provide more clarity/ clarifications about the following:\n\n1.\tApart from formal specifications, the rest of the statistics are ad-hoc (e.g. the spectral centroid or the spectral slope which are just borrowed from the audio domain) – why should these be good for images? \n2.\tTraining choices do not seem principled – GANs are trained till generated samples look like real samples. Why not use parameter settings and train to produced state of the art results with chosen architectures?\n3.\tFigure 1: Why does the CDF for the real data start in the middle of the figure? The figure purportedly shows bimodal 1D data for which the CDF should be a step function whereas the reference data has an inclined line. Why?\n4.\tUsing the term ‘spectral’ (centroid and slope) for image features is misleading when spectral features are not computed.  Do these features capture spectral properties of images. How? Why are these good features?\n5.\tWhat does an “asymptotically converging activation function” mean?\n\n6. Some typos need to be corrected. Figure # and caption (with dataset name) for Figure 6 needs to be provided, etc.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very interesting topic!",
            "review": "The ability to detect generated samples is a very interesting topic and has recently triggered a lot of discussion. The paper is well written, easy to follow and the authors have done an extensive evaluation in a number of different GAN applications.\n\n\nComments:\n\n1) Methodology - The GANs being evaluated were trained ignoring the statistics that the authors use to detect generated samples, and thus it is expected that there will be a difference. Have the authors attempted to include those in the loss function? It is a fair argument to say that some are not differentiable, but there are ways one could still incorporate them e.g. using REINFORCE. What do the authors thing about that?\n\n2) Following (1), as far as the specifications are known one could train a GAN to fake them. What do the authors think about that? Do the authors think they could detect samples from that or such statistics could be used to devise better GAN losses?\n\n3) The authors conclude that the smoothness and the differentiability of a loss function will always result to an inductive bias. However, that's an assumption given that there are no experiments trying to fake the detection, or experimenting with a large number of different architectures.\n\n4) In CIFAR10 the authors state that the distributions of pixels was quite different specially in the values close to -1. Another way to see neural networks is as differentiable compressors. Many times, value distortions are correlated to the amount of compression. Have the authors seen differences in e.g. larger architectures?\n\n5) On a last note, I would change the title as there is no proof that these tests / assumptions would hold for further research in the field. It would be great to show that the statistics used to detect GAN samples cannot be tricked.\n\nMinor comments:\n1) p.1 In the context of Verified Artificial IntelligenceSeshia [...] - needs a space.\n2) p.3 Spectral centroid in 2 [...] - 2 -> Figure 2.\n3) p.7 Figure 8 doesn't have a caption.\n4) P.7 There are some figures above SPEECH without a figure number.\n5) P.7 Reference to table 9b seems to be missing.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Research that has some interesting observations, but needs to take the next step to have impact on the field",
            "review": "The primary purpose of this paper, from what I understand, is to show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. This can potentially be useful in helping to identify artificial samples in the real world.\n\nI think that the reviewers did an excellent job of probing into different statistical perspectives, such as looking at the continuity of the distribution of pixel intensities and the various higher moments of spectral data. I also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.\n\nOne of the first findings is that, with MNIST and CIFAR, the pixel intensities of fake samples are noticeably different when viewed from the perspective of a Kolgomorov-Smirnov Test or Jensen-Shannon Divergence comparison. This is an interesting observation, but less so than it would be if compared to something such as a variational autoencoder (VAE), which fits a KL distribution explicitly. IWGAN and LSGAN are using different metrics in their loss functions (such as Wassertein and least squares), and thus the result is not surprising or novel. I think if the authors had somehow shown how they worked their metrics into IWGAN or LSGAN to achieve better results, this could have been interesting.\n\nAnother observation the authors make is about the smoothness of the GAN distributions. This may not be so easily wrapped into the loss function, but it seems easily remedied as a post-processing step, or perhaps even a smoothing layer in the network itself. Nevertheless, this is an observation that I have not seen discussed in the literature so there is merit to at least noting the difference. It is confusing that on page 4, the authors state that they hypothesized that the smoothness was due to the pixel values themselves, and chose to alter the distribution of the original pixels in [0,1]. However, they state that in Figure 5, the smoothness remained \"as expected.\" Did the authors misspeak here?\n\nI found the music and speech experiments very interesting. The authors note that the synthetic Bach chorales, for instance, introduce many transitions that are not seen in the training and testing set of real Bach chorales. This, again, is interesting to note, but not surprising as the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized. I do not believe these observations to be paper-worthy by themselves. However, the authors I believe have a good start on creating papers in which they specifically address these issues, showing how they can create better synthetic samples by incorporating their observations.\n\nAs to the writing style, there are many places where the writing is not quite clear. I would suggest getting an additional party to help proofread to avoid grammatical mistakes. I do not believe that the mistakes are so egregious as to impede understanding. However, it could distract from the importance on the authors future innovations if not corrected.\n\nOne last note. The title of the paper is \"TequilaGAN: How to Easily Identify GAN Samples.\" This makes it seem as if the authors were introducing another type of GAN, like LSGAN or DCGAN. However, they are not. As a matter of fact, nowhere else in the paper is the word \"TequilaGAN\" mentioned. This title seems a bit sensational and misleading.\n\nIn the end, although I did find this paper to be an interesting read, I cannot recommend it for publication in ICLR.\n\n----\n\nEdit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}