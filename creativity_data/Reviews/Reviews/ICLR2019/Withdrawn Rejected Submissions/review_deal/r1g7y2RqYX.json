{
    "Decision": {
        "metareview": "This  paper is on graph based semi-supervised learning where the goal is to develop an approach to jointly the node labeling function together with the edge weights. A natural way to formulate this problem as a bi-level optimization problem. However, the authors claim that this approach introduces two main difficulties: (a)  the \"upper\" objective function is itself the solution to the \"lower\" optimization problem (Eq. (2)), and (b) optimization is challenging (Eq. (3)). The AC disagrees. Firstly, there is a close connection between the constrained version and the regression version of the problem (e.g., Belkin, Matveeva and Niyogi) -- the former is infact a special case of the latter for a certain choice of regularization parameter. The latter reduces to an linear system. The outer problem can be optimized using standard gradient descent using the implicit function theorem trick common in bilevel optimization. Reviewers have also raised concerns about clarity, and experimental support in this paper and comparisons with related work.  ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "formulation unconvincing; Clarity, experimental support needs improvement."
    },
    "Reviews": [
        {
            "title": "deep learning architecture for graph semi-supervised learning",
            "review": "This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. \n\nI had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify:\n- section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time?\n- section 3.2, I do not understand equation (13). What is \\theta^\\tau, it does not appear in the right-hand term?\n\nI think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6).\n\nPlease, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss?\n\nOverall, I think this paper requires a lot of clarification before being published.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Method for non-linear label propagation while learning the network weights simultaneously. Insufficient comparison to related methods, insufficient experimental evidence and explanation for the results. ",
            "review": "**** After Revision ***********\nI thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \\tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. \n**** After Revision ***********\n\nThis paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework.  Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. \n- What is the total number of parameters in the proposed network? Please clarify how this is \"relatively few parameters\" as compared to other methods. \n- Please compare how your method for learning weights relates to the following papers and the references therein [1,2]\n[1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. \n[2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. \n- It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). \n- For clarity, please specify that \\theta are the parameters to be learned. \n- Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain?\n- Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label?\n- In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \\tau in the same training procedure? Please justify this design decision?\n- What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. \n- If the node features are available, how are they used in this framework?\n- In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. \n- Please justify the line \"parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). \" Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit?\n- Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper?\n- Why isn't there a comparison to methods like Graph-Sage?\n- Please explain this line \"LPNnobif degrades with large T, and even \\tau slightly above 1 makes a difference\"\n- Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset?\n- It would good to have uncertainty estimates for the results reported in Table 1. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but heuristical.",
            "review": "Summary\nThis paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.\n\nStrong points\n- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).\n- Good performance.\n- The paper is generally well written.\n\nConcerns\n- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.\n- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.\n- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?\n- Too many hyperparameters to tune.\n\nMinor points\n- I think the line above Eq (4) should be like \\tilde w_ij = w_ij / sum_k w_ik.\n- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.\n- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}