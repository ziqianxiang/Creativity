{
    "Decision": "",
    "Reviews": [
        {
            "title": "An attempt at exploiting the reward structure that unfortunately lacks both strong theoretical and empirical support",
            "review": "The algorithm presented here aims at speeding up (reinforcement) learning in situations where the reward is “aligned” with the state space, i.e. can be decomposed into multiple components where each component depends only on one dimension of the state. Such a situation is found e.g. in tasks where the agent needs to reach a specific position in a 2D or 3D space, and the reward can be decomposed into a 2D / 3D reward vector associated to reaching the target coordinate along each axis. The algorithm consists in first pre-training (by collecting data obtained with random action distributions) a so-called “Position Change Prediction Network” (PCPN) to map an action distribution (e.g. mu and sigma if the agent’s policy is Gaussian) to the corresponding state change distribution along each relevant state dimension (for which there is an associated reward). The PCPN is an Implicit Quantile Network (Dabney et al, 2018), so as to model the state change distribution without assuming a specific parametric form. Once trained, the PCPN can be backpropagated through to train the agent distribution so that state changes that lead to better rewards (along each axis) become more frequent, while those leading to worse reward become less frequent (where “better” and “worse” are defined by the advantage, computed with a critic V). Experiments are performed on three toy tasks, and comparisons to A2C show faster learning for the proposed algorithm, especially in higher dimensional state spaces.\n\nThe proposed approach combines two interesting ideas: (1) decomposing the reward into multiple components, and (2) aligning the agent’s action distribution with these components (through the PCPN). The second point is novel to the best of my knowledge. However, the first one is not since there exists a large body of literature on multi-objective reinforcement learning. The authors briefly mention some of it in the “Related Work” section, but unfortunately there is no comparison being made in experiments (the only algorithm being compared against is a “vanilla” A2C that uses a single scalar reward). Although previous work on multi-objective RL may not necessarily take advantage of the alignment between reward components and state dimensions, it is still relevant since the decomposition of the reward function by itself might be enough to speed-up learning, regardless of whether or not it is aligned with the state space. Experiments need to show that using the alignment as proposed here actually brings some benefits over existing multi-objective techniques.\n\nIn addition to the lack of comparison to related multi-objective methods, the empirical evaluation is not very convincing since it is done on toy problems, with the most “realistic” one being a simple 2D task where the improvement brought by the proposed technique is not obvious (Fig. 5). Better results are obtained on high dimensional toy problems but (a) in these problems the relationship between actions and reward components is somewhat trivial, and (b) since the paper was originally motivated by spatial positioning tasks, it is not clear how often in practice one will need to go beyond 2 or 3 dimensions... Finally, since there is no theoretical analysis either (except for some intuition as to what happens in a limit case at the bottom of p. 4), and the optimized objective is essentially heuristically motivated, it is hard to tell how the algorithm would behave across a larger range of applications.\n\nI have two concerns in particular regarding the methodology:\n- Pre-training the PCPN assumes that randomly sampling action distributions will give us enough coverage of the state and action distribution spaces to generalize to those seen during training of the agent. This may not always be the case, requiring training the PCPN online: it would have been nice to verify whether this is feasible in practice.\n- I am afraid that forcing the agent to learn only from the PCPN gradient may prevent it from learning an optimal policy. As a dummy illustrative example, imagine an infinite 2D gridworld where the agent has two actions (action A = increase x coordinate by 1 only if y = 0, action B = increase both x and y by 1 only if y = 0), the reward is +1 when x increases and the agent starts at the origin. The reward is aligned with the x axis, and the optimal policy is to always take action A since as soon as action B is taken the agent can’t move anymore. With the PCPN output being the change along the x axis, its gradient wrt the agent action distribution is zero since both actions A and B increase x equally. Thus the agent will never be able to learn that A is to be preferred. In other words, it seems to me that the agent is limited to learn from changes in the state coordinates aligned with the reward, even if other coordinates are important for transition dynamics that may impact future rewards.\n\nRegarding the clarity of the paper, I found it good enough overall except that it is a bit difficult to follow the narrative. In particular Section 3 breaks the flow by considering a more general setting than the one introduced previously (the note about “action” and “policy” meaning something else is particularly confusing). Using Q to denote the action distribution is also a poor idea since Q is pretty much a “reserved” letter in RL. Overall the generic idea of “Quantile Regression RL” from Section 3 seems potentially interesting, and may be worth exploring on its own, but the current presentation (in the application to multi-objective state-aligned rewards) makes it more a distraction than an asset.\n\nMinor points:\n- The “biological insights” from the introduction are difficult to grasp since a reader versed in RL is unlikely to know what are “viral vector strategies” or “effector specific value estimations”\n- The definition of aligned spaces (“their dimensions correlate”) is pretty vague, a more precise mathematical definition would be helpful (even if the intuition is easy to grasp)\n- In the first one-line equation p. 4 there is a tau that should be a_tau\n- In the definition of R_t,n on p. 5, the sum should be up to t+n-1 (and same with the vector version at the bottom of the page)\n- I wonder if the loss L_mon is really needed, since the quantile regression loss should naturally lead to a monotonous function (assuming enough data and capacity). Are results significantly worse without it?\n- The definition of the Huber loss is incorrect \n- It is not explicitly stated that there is no backpropagation through V_psi in L_a\n- “QRRL can easily be adapted to the off-policy setting”: this is not obvious since you are using n-step returns bootstrapped with a critic V, which requires on-policy data (unless appropriate corrections are applied)\n- Experiments do not clearly define the rewards used by both algorithms \n- Figures would be easier to read if axes were labeled\n- I am surprised by the absence of entropy loss in A2C (beta = 0), was this hyper-parameter carefully tuned?\n- Having diagrams of the networks in Appendix A would be clearer than text explanations\n- The “cosine embedding” should be defined to avoid having to refer to Dabney et al\n- Note that the proposed model ends up having significantly more capacity than the single A2C network, which could raise concerns wrt the fairness of the comparison",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper is smooth to read, but important issues remian.",
            "review": "The paper addresses RL in the continuous action space. The scope is right for ICLR. The main two claimed contributions of the paper are the use of a re-parametrised policy and the use of a novel vector-based training objective (similar to solving each component of a factored MDP).\n\nI have the following concerns about the paper.\n\n(i) First, section 3 seems to re-invent the notion of a re-parametrised probability distribution. Of course you can get any well-behaved probability distribution from the uniform distribution by applying some non-linear transformation. That has, AFAIK been known since probability theory with continuous random variables has been formulated. There is a lot of work in the variational auto-encoder community right now on learning transformations like this. I do not exactly see what the purported novelty of section 3 is. The last equation on page (4) states that the derivative of the inverse function is the reciprocal of the derivative of a function, a well-known fact from calculus (and you cite a paper from 1992 to corroborate it?!). The other equations on page 4 and on top of page 5 seem to manually re-do the math of back-propagating through networks with random inputs, for a special case - this has been known for quite some time as well.\n\n(ii) I do not understand why the learned function \\hat{Q} has to be monotonic (the point of the constraint in equation (2)). Sure, you won't get a correspondence with a CDF if it's not, but why is this a problem? You would still be learning some probability distribution that does what you want?\n\n(iii) Section 4 augments the information available to the agent with the component-wise absolute value between the current position and the goal position and uses that to (purportedly) improve performance. I see two problems with this. On a practical side, this restricts the applicability of the algorithm to settings with clear goal states where the state space is Euclidean (or similar). Also, if we know the goal state, there is the question of not just feed it to the agent directly (as a part of the observation) and avoid having the complicated vector reward structure at all? I do not see a compelling reason for this. On a theatrical side, this can only really work if the state space is (a subset of) R^n. Also, if we can improve each action without regard for other actions, (as the algorithm seems to assume) doesn't this correspond to some kind of assumption that the MDP is factorised?\n\n(iv) The paper has problems with presentation. It is well-written as far as the English goes and it is superficially smooth to read, but the concepts which are crucial to understand the main point are very confounded. There is no pseudocode that explains how the algorithm works. I feel that the paper should include an equation literately states all the loss terms being optimised.\n\n(v) The paper takes a cavalier approach to formalism. It mentions arbitrary metric spaces several times, as well as the notion of dimension induced purely by metric structure, but the equations only really address the Euclidean vector space. This is OK and this is what most RL algorithm do, but you shouldn't claim that you algorithm is more general than what is actually defined in the paper.\n\nUnfortunately, as the paper stands now, I have some concerns about whether it is suitable for publication.\n\nI wanted to encourage the authors to address the following points during the discussion phase. Also, if you feel the review misunderstands the paper, please clarify - I am willing to update my score if the reply is convincing.\n\nSuggestions:\n\n1. Include an equation that mentions all the loss terms in the algorithm.\n2. Figure 2 should be augmented to include where the loss signal comes from.\n3. Contrast section 3 with variational auto-encoders [2].\n4. Contrast section 3 with re-parameterised policy gradients. \n5. Explain why the monotonicity constraint is necessary in section 3.\n5. Contrast the setting of vector reward with literature on factored MDPs [1].\n6. Remove inflated claims about applicability to general metric spaces.\n\n[1] Guestrin et al. Eﬃcient Solution Algorithms for Factored MDPs\n[2] Doersch. Tutorial on variational autoencoders (and further references therein)\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The idea is interesting but the paper should backed with more complete experiments",
            "review": "This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles (PCPN in the paper) . The claim is that this improves the sample efficiency because the backpropagation of collected rewards takes advantage of distributional knowledge we have about the evolution of the world.\n\nI like the idea but if we have a good model of the dynamics of the world (the PCPN) then (stochastic) dynamic programming is likely to be a very hard to beat baseline. This is one of the reason why many RL approaches are avoiding an explicit model of the world.  The presence of this model is very central in the performance of the proposed approach and I do not think that the experiments are conclusive enough. More specifically I think the paper would be better if it would answer some questions \n- How the performance is affected wrt the error made by the PCPN. In the current architecture if the PCPN is bad for some reason then there is no way to recover. Thus the approach cannot be used if we are not able to fit the dynamics\n- More baselines would be added. In particular in this kind setting PPO is known to perform well (but many other could be added ACKTR, interpolated policy gradient, PPO with Stein control variate,...). I know there is possibly too many to run a reasonable experiment but with only A2C as a competitor it is not even possible to know if the extra performance comes from the distributional learning or from the use of the \"aligned states\"\n- The task is not very appealing: classical control often work better on theses tasks (with are variants of acrobot already available as Gym environments). To assert the validity of the approach it could be worth to use Atari tasks (but this is expensive) or some Mujuco/Robotics ones ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}