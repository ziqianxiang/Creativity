{
    "Decision": {
        "metareview": "This paper studies the question of memorization within overparametrised neural networks. Specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders. \n\nAll reviewers agreed that this is an interesting question that deserves further analysis. However, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. In particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. The AC fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Insufficient Rigor"
    },
    "Reviews": [
        {
            "title": "Promising idea that requires substantial improvement in analysis and presentation",
            "review": "The authors conjecture that convolutional downsampling is an underlying mechanism behind sample memorization in over-parameterized convolutional autoencoders. They claim that this effect leads the system to converge to a low-rank solution in contrast to the theoretically possible identity mapping. They support their claim with numerical experiments on linear and non-linear convolution autoencoders.\n\nStrengths:\n- The authors develop their idea in close connection to commonly used architectures.\n\nWeaknesses:\n- The main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer. The paper itself states on page 4 that the results depend on the initialization and cite Gunasekar et al. in the conclusion for an analogy, which, however, explores the implicit regularization effect of a gradient descent optimizer.\n- There is no clear and proved statement despite the suggestive mathematical nature of the writing (Conjecture, Proposition). The claimed 'proof' of the Proposition is conducted via experiment. In light of the above mentioned confounding factors, the current phrasing of the Proposition will not allow a formal proof as it is unclear what the system 'linear Network DS' even is.\n- The boundary between conjectured and inferred statements is very vague. For example, the meaning of  'prefers to learn a point map' is unclear.\n\nOverall, the exposition is insufficient in supporting the conjectured effect. The methodology could be strengthened in two directions:\n1) experimentally: designing numerical experiments that exclude confounding factors and surface the conjectured effect\n2) theoretically: abstracting the idea into a clear mathematical statement that can be proved\n\nI encourage the authors to extend their work for submission to a future venue.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting direction but needs more to be a fully fleshed out paper.",
            "review": "Summary:-\nThe authors investigates downsampling as one method by which autoencoding CNNs may memorize data. The theoretical motivation provided concentrates on linear CNNs. They show that downsampling linear CNNs tent to learn a point-map of the training data, even though (under certain initializations) they are capable of learning identity maps. However, non-downsampling linear CNNs learn identity maps. Given enough data however, the authors claim that the downsampling CNN will learn the identity map.\n\nStrengths:-\n+ Authors present a good exploration of how linear CNNs memorize data when they do downsampling. \n+ A theoretical prediction of the amount of training data needed to counteract data memorization for downsampling linear CNNs is provided, \"Our conjecture also implies that when training a linear downsampling CNN on images of size 3 · 224 · 224, which corresponds to the input image size for VGG and ResNet (He et al. (2016), Simonyan & Zisserman (2015)), the number of linearly independent training examples needs to be at least 3 · 224 · 224 = 153, 228 before the network can learn the identity function.\" \n\nWeaknesses:-\n+ Not enough theoretical proof is provided to support the hypothesis. Which would be fine but some key experiments are missing to make the paper empirically rigorous.\n++ Would be good to see experiments that illustrate the predication that a certain amount of data would allow for learning identity maps, both for linear and non-linear CNNs.\n++ In the non-linear CNN setting, I'd like to see the same early-stopping experiment done for linear CNNs whose results are in Fig. 3. I don't see any obvious theoretical reason why that result form Fig. 3 must extend to the non-linear setting. \n+ Initializations are pointed to as effecting the type of function the network learns. The authors give an example of a hand-designed initialization that allows a downsampling linear CNN to learn the identity map but they don't explain how they arrived at this initialization, or its properties that make it a good initialization. In general however, I think it's alright to assign exploration of effect of initialization to future work, since it seems like a non-trivial task.\n+ It is mentioned that \"the results are not observed for linear networks when using Kaiming initialization,\" which I read to mean the downsampling linear CNNs with Kaiming initialization learn the identity map, not point-map. If this is true, it seems like a vital point and should be included in discussions of future work.\n\nRecommendation:  I think this could be a better short paper. There are some interesting contributions, but maybe not enough for a full length paper. For a full length paper, some further exploration of _why_ downsampling leads to (if indeed there is a causality) data memorization is needed.\n\nMinor stuff:-\nCitation \"Gunasekar et al.\" is missing year (conclusions section)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea but stronger supporting theory and more clarity are needed",
            "review": "The paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. In the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. The authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. Then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.\n\nThe paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.\n\n- Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.\n- Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix.\n- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then.\n\n- Minor comments\n1. “2000 iteration” -> “2000 iterations”\n2. The text says “Network ND trained on frog image” while the following next sentence says that “the network reconstructed the digit 3”. Please clarify.\n3. “Network ND reconstructed the digit 3 with a training loss of 10^-4 and Network ND with loss 10^-2”. It seems that one of these should be “Network D”.\n4. “(with downsamling)” ->  “(with downsampling)”",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}