{
    "Decision": {
        "metareview": "The paper proposes a method that aims to combine the strenghts of VAEs and GANs.\n\nThe paper establishes an interesting bridge between GANs and VAEs. The experimental results are encouraging, even though only relatively small datasets were used. It is encouraging that the method results in better reconstructions then ALI, a related method.\n\nSome reviewers think that the paper contains limited novelty compared to the wealth of recent work on this topic (e.g. ALI/BiGAN). The paper's contribution is seen as incremental; e.g. the training is very similar to InfoGAN. Also, the claims of better sample quality over ALI seem insufficiently supported by the data.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "A Review on Adversarial Inference by Matching Priors and Conditionals",
            "review": "The goal this is work is to develop a generative model that enjoys the strengths of both GAN and VAE without their inherent weaknesses. The paper proposes a learning framework, in which a generating process p is modeled by a neural network called generator, and an inference process q by another neural network encoder. The ultimate goal is to match the joint distributions, p(x, z) and q(x, z), and this is done by attempting to match the priors  p(z) and q(z) and matching the conditionals p(x|z) and q(x|z). As both q(z) and q(x|z) are impossible to sample from, the authors mathematically expand this objective criterion and rewrite to be dependent only on p(x|z), q(x) and q(z|x), that can be easily sampled from. In the main part of the work, the authors use the f-divergence theory (Nowozin et al., 2016) to present the optimization problem as minmax optimization problem, that is learned using an adversarial game, using training and inference algorithms that are proposed by the authors. In experiments, the authors consider both reconstruction and generation tasks using the MNIST, CIFAR10 and CelebA datasets. Results show that the proposed method yields better MSE reconstruction error as better as a higher inception scores for the generated examples, compared to a standard GAN and a few other methods. \n\nThis work establishes an important bridge between the VAE and GAN framework, and has a a good combination of theoretical and experimental aspects. Experiments results are encouraging, even though only relatively simple and small datasets were used. Overall, I would recommend accepting the paper for presentation in the conference. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok paper, some nice comparisons, but too similar to existing models",
            "review": "This paper presents a variant of the adversarial generative modeling\nframework, allowing it to incorporate an inference mechanism. As such it is\nvery much in the same spirit as existing methods such as ALI/BiGAN. The\nauthors go through an information theoretic motivation but end up with the\nstandard GAN objective function plus a latent space (z) reconstruction\nterm. The z-space reconstruction is accomplished by first sampling z from\nits standard normal prior and pushing that sample through the generator to\nget sample in the data space (x), then x is propagated through an encoder\nto get a new latent-space sample z'. Reconstruction is done to reduce the\nerror between z' and z.\n\nNovelty: The space of adversarially trained latent variable models has\ngrown quite crowded in recent years. In light of the existing literature,\nthis paper's contribution can be seen as incremental, with relatively low novelty. \n\nIn the end, the training paradigm is basically the same as InfoGAN, with\nthe difference being that, in the proposed model,  all the latent\nvariables are inferred (in InfoGAN, only a subset of the latent\nvariables are inferred) . This difference was a design decision on the part of the InfoGAN\nauthors and, in my opinion, does not represent a significantly novel\ncontribution on the part of this paper.  \n\nExperiments: The experiments show that the proposed method is\nbetter able to reconstruct examples than does ALI -- a result is not\nnecessarily surprising, but is interesting and worth further\ninvestigation. I would like to understand better why it is that latent\nvariable (z) reconstruction gives rise to better x-space reconstruction.\n\nI did not find the claims of better sample quality of AIM over ALI to be\nwell supported by the data. In this context, it is not entirely clear what\nthe significant difference in inception scores represents, though on this, the\nresults are consistent with those previously published\n\nI really liked the experiment shown in Figure 4 (esp. 4b), it makes the\ndifferences between AIM and ALI very clear. It shows that relative to ALI,\nAIM sacrifices coherence between the \"marginal\" posterior (the distribution\nof latent variables encoded from data samples) and the latent space\nprior, in favor of superior reconstructions. AIM's choice of trade-off is\none that, in many contexts, one would happy to take as it ensures that\ninformation about x is not lost -- as discussed elsewhere in the paper.\nI view this aspect of the paper by far the most interesting. \n\nSummary,\nOverall, the proposed AIM model is interesting and shows promise, but I'm\nnot sure how much impact it will have in light of the existing literature\nin this area. Perhaps more ambitious applications would really show off the\npower of the model and make it standout from the existing crowd. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but needs more work",
            "review": "UPDATE (after author response):\n\nThank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.\n\nThere's one point that the reviewers didn't clearly address:  \"It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.\" Please consider adding such an experiment.\n\nThe current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that \"Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd.\" Showing that the method outperforms other methods would definitely strengthen the paper.\n\nSection 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5.\n\n---------\n\nThe paper proposes a new loss for training deep latent variable models. The novelty seems a bit limited, and the proposed method does not consistently seem to outperform existing methods in the experiments. I'd encourage the authors to add more experiments (see below for suggestions) and resubmit to a different venue.\n\nSection 4:\n- q(z) seems to be undefined. Is it the aggregated posterior?\n- How is equation (1) related to ELBO that is used for training VAEs?\n\nSome relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.\n\nVEEGAN: Reducing mode collapse in GANs using implicit variational learning\nhttps://arxiv.org/pdf/1705.07761.pdf\n\nDistribution Matching in Variational Inference\nhttps://arxiv.org/pdf/1802.06847.pdf\n\n\nSection 5.1:\n- The quantitative comparison measures MSE in pixel space and inception score, neither of which are particularly good measures for measuring the quality of how well the conditionals match. I’d encourage the authors to consider other metrics such as log-likelihood.\n\n- It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.\n\nSection 5.4: \n- The error bars seem quite high. Is there a reason why the method cannot reliably reduce mode collapse?\n\nMinor issues:\n- CIFAT-10 -> CIFAR-10\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}