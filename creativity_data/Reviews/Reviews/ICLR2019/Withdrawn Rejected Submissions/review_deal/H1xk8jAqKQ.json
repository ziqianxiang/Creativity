{
    "Decision": {
        "metareview": "\n-pros:\n- good, sensible idea\n- good evaluations on the domains considered\n- good analysis\n\n-cons:\n- novelty, broader evaluation\n\nI think this is a good and interesting paper and I appreciate the authors' engagment with the reviewers.  I agree with the authors that it is not fair to compare their work to a blog post which hasn't been published and I have taken this into account.  However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for ICLR this year. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Interesting paper but not quite there yet"
    },
    "Reviews": [
        {
            "title": "sensible method, but limited novelty and evaluation is lacking",
            "review": "The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. As training progresses, the initial state distribution is incrementally shifted towards earlier steps in the demonstration, until the agent is trained starting from the original initial state. The authors further provide an analysis of the sample complexity of this method on a simple MDP. The method is demonstrated on a maze navigation task and a challenging game Pommerman.\n\nThe method is simple and sensible, but not particularly novel. As the authors pointed out, a very similar strategy for using demonstrations was previously presented in an OpenAI blogpost, Learning Montezuma’s Revenge from a Single Demonstration. However since that work was not published, it should not be held against this paper. That being said, sampling initial states from demonstrations is a tried-and-true strategy in RL, and the manually designed curriculum is also not particularly novel. Therefore the method is mainly a minor tweak to longstanding techniques. The paper has also acknowledges these previous works. As such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. Florensa et al. 2017 and Aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.\n\nThis work can also benefit from a more diverse set of tasks to better evaluate the effectiveness of the method, and provide more insight on when such a strategy is beneficial. The experiments were conducted only on discrete grid world tasks, and additional experiments in continuous domains could be valuable. In the maze task, Backplay is not significantly better than uniform. Pommerman is a much more compelling task and shows more promising improvements from backflip. However, training seems to have been terminated fairly early, before the performance for most policies have converged. In particular, the standard dense policy in figure 3c seems to be doing pretty well, will it catch up to the backplay policy with more training? It is also pretty unexpected that the uniform policies are doing so poorly, worse than the standard policy for the Pommerman experiments. Do the authors have any intuition on why this might be the case?\n\nIn figure 3, what is the initial state distribution used to evaluate the various methods? Are all policies initialized to the original initial state of a task, or are initial states sampled from the demonstrations? Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. If that is the case, it might not be a fair comparison for the other policies, especially for the standard policies, which are trained under different initial states.\n\nAs detailed in the appendix, the sliding windows for the curriculum do not seem to have a lot of overlap. This might be a potentially problematic design decision, since the sudden change in the initial state distribution, may cause the policy to “forget” about strategies learned for previous initial states. Has the authors experimented with other more gradual transition strategies?\n\nI think this paper in its current form does not yet meet the bar for ICLR. But this line of work could be a potentially promising direction. More thorough evaluation, better baselines, and more diverse tasks can help to strengthen this work. Further analysis on the effects of different initialization strategies could also make for a compelling contribution.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Request for some clarifications. ",
            "review": "Thanks for your submission.\n\nThe  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The authors show the technique reaches an upper bound on sample complexity especially in sparse reward environments. The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. The biggest limitation of the method as with other vanilla model free RL is the lack of generalization. \n\nA bit more motivation on the simplified assumption that function approximation would have been better. Although, such a simplification seems to be a natural candidate to be upper bounded by the longest shortest path from v_0 to v_*; consideration of such simplicaton on the neighbourhood structure of the graph with respect to the maximum vertex degree seems to be missing or cliques. Although, the authors comment about the strong assumptions being made to aid the analysis. \n\nThe authors explain the analysis in a very precise and the analysis seems to work. Although, the part of the analysis where connections are drawn to the reciprocal spectral gap is not very clear. \n\nThe authors discuss the limitation of the analysis in the case of the binary tree, that follows from the arguments before.\n\nIt will be great to see a more systematic approach to deciding how fast/slow the window should be updated to unify some of the findings from the empirical experiments as that seems to affect the way the agent trains using Backplay.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Minor contributions; not a novel idea with limited evaluation",
            "review": "This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. \n\nThe proposed method is closely related to 1) “Learning Montezuma’s Revenge from a Single Demonstration” a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma’s revenge game 2) “Reverse Curriculum Generation for Reinforcement Learning” by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. \n\nThe approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. \n\nThe Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. \n\nOverall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). \n\nI commend the authors for honestly reporting their method’s shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}