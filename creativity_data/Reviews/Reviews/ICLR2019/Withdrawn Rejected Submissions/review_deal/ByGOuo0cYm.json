{
    "Decision": {
        "metareview": "This paper addresses the problem of few shot learning and then domain transfer. The proposed approach consists of combining a known few shot learning model, prototypical nets, together with image to image translation via CycleGAN for domain adaptation.  Thus the algorithmic novelty is minor and amounts to combining two techniques to address a different problem statement. In addition, as mentioned by Reviewer 2, though meta learning could be a solution to learn with few examples, the solution being used in this work is not meta learning and so should not be in the title to avoid confusion. \n\nAs this is a new problem statement the authors apply multiple existing works from few shot learning (and now adaptation) to their setting. The proposed approach does outperform prior work, however this is not surprising as the prior work was not designed for this task. Despite improvements during the rebuttal to address clarity the specific experimental setting is still unclear -- especially the setup of meta test data vs unsupervised da data. \n\nThis paper is borderline. However, since the main contribution consists of proposing a new problem statement and suggesting a combination of prior techniques as a first solution, the paper needs a more thorough ablation of other possible combination of techniques as well as a clearly defined experimental setup before it is ready for publication.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Combination of prior work applied to a new problem statement -- more improvements needed"
    },
    "Reviews": [
        {
            "title": "official review",
            "review": "The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup. The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.\n\nPros:\n1. A new few shot learning with domain shift problem is studied in the paper.\n2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario. The experimental improvements on omniglot seem quite substantial. \n\nCons:\n1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline? It seems that both are using meta-learning with domain adaptation technique. What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?  I feel the baseline in domain adaptation area is a bit limited.\n2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)? \n3. It seems the domain shift in the paper is less dramatic. i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.\n4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.\n\nMinor:\n1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?\n2. In the caption of figure 2, there should be a space after `\":\".",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This is not \"meta domain adaptation\" but \"few-shot learning +domain adaptation\"",
            "review": "This paper proposes to combine unsupervised adversarial domain adaptation with prototypical networks and finds that the proposed model performs well on few-shot learning task with domain shift, much better than other few-shot learning baselines that do not consider. Specifically it tests on Omniglot with natural image background and cliparts to real images.\n\nIt is true that current meta-learning approaches do not address the problem of domain shift, and as a result, the testing domain has to be the same with the training domain. However, this paper rather than proposing solution address the meta-learning problem, albeit the title “meta domain adaptation”, only brings few-shot learning to domain adaptation. Here’s why:\n\nIn order for a meta-learning model to be called “meta domain adaptation,” the type of adaptation cannot be seen during training, and the goal is to test on adaptation that the model has not seen before. Indeed, each task in meta domain adaptation should be seen as a pair of source task and target task. \n\nThe problem with the current model is that during training, it is trained to target at one specific type of test domain--the generator network G aims to generated images that align with the unsupervised  test domain X_test. Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.\n\nIn short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation. Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair. For the rest of my review, I will treat the paper as “few-shot learning with domain adaptation” for more appropriate analysis.\n\nFor the experiments, there seems to have a great win of the proposed algorithm against the baselines. However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison. Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation. Then use the same network to extract the features and then using the nearest neighbor to retrieve the classes. Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.\n\nAnother concern is that the evaluation of domain adaptation does not have much varieties. Only two domains shifts are evaluated in the paper, specifically Omniglot + BSD500 and Office-Home. BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered. Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.\n\nIn conclusion, the paper presents a interesting combination of ProtoNet + Adversarial DA + Cycle consistency. However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers. Therefore, I recommend reject.\n\n---\nNote: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being \"meta domain adaptation\".\n\n===\nAfter rebuttal:\n\nI would like to thank the authors for the response and updating the draft. They have addressed 1) the title issue and 2) adding domain adaptation baselines. Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines. \n\nHowever, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty. The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting setting. Method seems to work, though is not very principled. Questions about reproducibility ",
            "review": "The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set. This scenario is well-motivated by the \"researcher example\" given throughout the paper. The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain. Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).\n\nThe key idea is to learn a mapping from the source domain to the target domain. This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain. In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.  Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.\n\nPros:\n- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.\n- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.\n- The paper reads well and is easy to follow.\n\nCons:\n- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / \"additional improvements\". Further, there a number of experimental details that need to be further elaborated upon. e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this). It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?\n- Some assumptions are not explicitly stated. In particular, it is unclear what the assumption on the size of the unlabelled test set is. This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.\n- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further? Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?\n- The overall method seems to be not very principled, and requires a lot of \"tweaks and tunes\", with additional losses and regularizers, to work.\n\nOverall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research. Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches). Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task. It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}