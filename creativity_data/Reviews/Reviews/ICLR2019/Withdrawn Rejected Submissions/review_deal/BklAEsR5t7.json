{
    "Decision": {
        "metareview": "The paper addresses the problem of large scale fine-grained classification by estimating pairwise potentials in a CRF model. The reviewers believe that the paper has some weaknesses including (1) the motivation for approximate learning is not clear (2) the approximate objective is not well studied and (3) the experiments are not convincing. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account to improve the paper.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "The paper can be improved"
    },
    "Reviews": [
        {
            "title": "Limited significance ",
            "review": "This paper proposed to tackle a large-scale fine-grained object classification problem by approximated CRF. The main motivation is to exploit the spatial conference of object labels to reduce noises in the instance-wise prediction. To this end, the task is formulated by sequential inference problem using CRF. To speed up training, several techniques are applied such as factorized pairwise-potential and approximation of CRF objective.  \n\nAlthough the paper presented a reasonable idea for their particular problem (i.e. classification of products in the store display), the significance of the work is quite limited as the same idea is not generally applicable to other settings (e.g. there is no strong spatial correlation of labels in general images). Also, the performance improvement over the instance object classification is not significant as shown in Figure 5 (Unary vs. Approximate factorized). Due to the limited significance and impact of the work, this reviewer suggests a rejection of this paper. \n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "weak contribution and experiments",
            "review": "This paper tackles the problem of estimating pairwise potentials when the number of labels is large. Two modifications are proposed: one is to factorize the matrix for pairwise potentials, and the other is to approximate the log likelihood objective with the MEMM objective.\n\nThe problem and the proposed approach are well motivated. It is particularly useful to draw the connections between MEMM and piecewise-pseudolikelihood.\n\nThe major weakness of the paper is whether the approximations are necessary. It is hard to see why approximating the log likelihood with MEMM is necessary, because inference and computing the gradients of the log likelihood have the same computational complexity. So the authors could have trained the model with the log likelihood.\n\nRegardless, it is still valuable to compare MEMM and log likelihood for training CRFs. However, the authors fail to show how well MEMM approximates the log likelihood. For example, the authors can compare the solutions when optimizing with the gradients of log likelihood and the with the gradients of MEMM. It is especially important to compute the training log likelihood for the two solutions, as it tells us how well MEMM approximates the log likelihood. This is also true for the low-rank approximation of the pairwise potentials. The authors fail to compare the case with low-rank approximation and the case without. It is important to evaluate the training error first with both methods as they share the same objective. This type of comparison should be apply to batch normalization as well.\n\nApproximating the pairwise potentials with matrix factorization is also not novel.  See the list below. (The list is by no means exhaustive. Please see the citations therein.)\n\nDense and low-rank Gaussian CRFs using deep embeddings\nChandra et al., ICCV 2017\n\nEfficient SDP inference for fully-connected CRFs based on low-rank decomposition\nWang et al., CVPR 2015\n\nNeural CRF parsing\nDurrett and Klein, ACL 2015\n\nFinally, some of the claims made in the paper (listed below) should be more careful.\n\np.4\n\nthe likelihood function, therefore, is log-linear and concave.\n--> concave in what?\n\nthe scoring function is still concave, ...\n--> concave in what?\n\nthe objective function is no longer linear or concave with respect to the network parameters, ...\n--> what are the network parameters?\n\nbut deep learning training techniques have been shown to yield good results ...\n--> this argument is weak. the key is point out that SGD is used, plus SGD has been shown to work well on many matrix factorization problems. see the paper below.\n\nOnline learning for matrix factorization and sparse coding\nMairal et al., JMLR 2010\n\np.5\n\nthe test time inference uses a global normalization ... avoids the label bias problem.\n--> the partition function is not even computed when using Viterbi. I'm also not sure how this avoids the label bias problem.\n\nwhitening the inputs to each layer may also prevent converging into poor local optima.\n--> this is a hand-wavy claim. it would be best if the authors can provide citations to the claim.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Work that introduces new ultrafine-grained classification dataset with a somewhat incremental model",
            "review": "Summary:\nThis paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as “ultrafine-grained” class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of “class embedding” for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.\n\nComments:\nArguably, the primary contribution of this paper is the introduction of a new “ultrafine-grained” classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it’s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It’s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.\n\nThe model introduced is interesting, but its novelty is limited. It’s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an “embedding” is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.\n\nOther Comments:\n-Since you ran a cross-validation, you should add confidence intervals to your reported numbers\n-One additional dataset detail I was hoping to see that you didn’t provide is the mean/standard deviation of the number of instances per class,\n-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well\n-The title of your paper is somewhat misleading - it’s hard to argue that the form of class embedding you use is a “deep” class embedding since it’s just a matrix of parameters that are learned during training.\n\nOverall, I’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.\n\n[1] Ma, Xuezhe, and Eduard Hovy. \"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.\"\n\n\nREVISION:\nThe other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}