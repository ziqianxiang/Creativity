{
    "Decision": "",
    "Reviews": [
        {
            "title": "Strong results on NER, but not an interesting approach",
            "review": "This submission tackles the named entity recognition (NER) task.\nIt formulates the task as sequence labeling (through BIO-tagging), and applies CNN+BILSTM encoder followed by self attention components to extract the representations used in local classification. Strong performance is achieved on benchmark datasets.\n\nThe key of the claimed modeling novelty seems to be the self-attention mechanism.\nI'm not entirely familiar with the sequence labeling literature, but I would be very surprised if there is no existing work applying a similar model a such tasks. Further, some of the arguments seem to be over-claimed. For example, `theoretical justification` is mentioned for many times in abstract and intro, but I don't see any theory through the paper.\n\nPros:\n- Nice numbers.\n\nCons:\n- Noting else to learn other than nice numbers.\n\nDetails:\n- Section 3.3. Does the model try to force consistency during decoding (e.g., by using constrained beam search)? If not, could the authors comment on how often does the model produces ill-formed predictions.\n\n- Two of the claimed contributions involve `theories`. But I don't see any theory through the paper. To be honest, the analysis in Section 6 can be some nice intuition, but it has nothing to do with theory. \n\n- Contribution.2, self-attention is something `applied` in this paper, but not `proposed`.\n\n- I'm confused by Section 6. Is the model proposed here actually used in the experiments? If it is, isn't it better to put it before Section 5?\n\n- Figures 2 are extremely hard to read.\n\n- I'm not sure what points Section 5.4 is trying to make.\n\nMinor:\n\n- \\sigma is usually used to indicate the sigmoid function.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper presents a sequence labeling model that incorporates Bi-LSTM-CNN with multi-head self-attention module. The proposed architecture models cross-interaction between past and future information. Detailed analysis on how the attentions discover the patterns of chunking and entity typing is conducted that provides more understanding of what the model learns.",
            "review": "In general, the paper proposes a deep architecture combining bi-LSTM, CNN and self-attentions to better model the interactions among the contexts for sequence labeling. The model is then evaluated on a sequence-labeling problem (NER) with one dataset. Although effective, the method seems to be a bit trivial through a combination. Evaluating on only one NER dataset is also not too convincing for me. Detailed comments are the following:\n\nPros:\n1. The paper is clearly written with a good organization to fully explain their model. The motivation is clear and comparison with existing work is clearly illustrated.\n2. It is interesting to decouple the problem to entity chunking and entity typing for detailed evaluation, although I do not see any decoupling methodology in terms of the model itself. It would be better to also make the decoupling idea integrated into sequence-labeling models, in this way the model could be more different and novel compared to existing works.\n3. The experimental section provides detailed analysis of the proposed model on the performance of each component as well as some qualitative analysis on what different attention heads learn.\n\nCons:\n1. The methodology itself is a bit trivial to me, since the bi-LSTM-CNN and self-attention mechanism were already proposed previously. It seems to be an improved combination, as also stated in Related Work (\"The subtle difference is that they...\"). It also seems over-stated in the abstract about the functionality on providing insightful understanding of attentions and decoupling NER into two subproblems. To my understanding, the understanding is only qualitative by observing a few examples, and the decoupling does not refer to the method, but the evaluation.\n2. More existing works should be added in Related Work including sequence-labeling models as well as attention models.\n3. It's still unclear if one wants to re-implement the model. Section 5.2 still lacks some important parameters, e.g., learning rate, batch-size, training method, how to parallel etc. More explanations on baseline models should be added to better understand the comparisons. \n4. Section 6.2 is confusing. Does the cross construction used in your experiments? Why do you put this part at the end of the paper. If section 6.2 provides better construction, why not making it integrated in the main part of the paper? Moreover, the authors claim the model's ability to exploit cross-interactions among input sequence, but I do not really get this point. The paper should illustrate this characteristics into more details for the readers to understand. \n\nQuestions:\n1. Since your model is quite similar to Chiu & Nichols (2016), can you illustrate what's the architecture of their model and what's the difference between their work and Bi-LSTM-CNN in Table 2?\n2. Can you explain how do you parallel the computation for self-attentions?\n3. The statistics in Table 3 is quite unexpected to me. Although you mentioned C^{all} is much better than H for classifying \"I\", it did much worse for all the other classes (except \"O\"). In this case, I suppose the attention is actually worse than simply computing LSTM hidden representations (H). Can you explain why? The last column for NativeH actually shows little degradation compared to the full model, may I know how's that happen, and what's the difference between NativeH and H? Section 5.4.1 mentions that Table 3 shows \"per-token recalls\", can you explain more what does this mean? How's the f1 scores?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "well written paper with marginal contribution",
            "review": "This paper proposes to use multi-head attention module with bi-LSTM for sequence labeling name entity recognition. The author carefully introduce the model structure, followed by extensive experiments to demonstrate the advantage of the model. I think the novelty of this paper is not obvious, or at least the authors didn’t clearly demonstrate the contribution of this paper. Multi-head attention mechanism, bi-LSTM are both well established methods. There are also existing works which have already applied related techniques to the NER problem. I’m willing to change my score if the authors can clarify the contribution of this paper.\n\nTo add more comments, I think the building blocks in this draft, including bi-LSTM, self-attention, character and word level CNN, neural network for NER, are all well established mechanisms in the community. There are already many use cases of one or combination of some of these modules. Neither the modules themselves nor the way of combining them are novel to me, though I appreciate the effort to build the system and test its performance against other methods.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}