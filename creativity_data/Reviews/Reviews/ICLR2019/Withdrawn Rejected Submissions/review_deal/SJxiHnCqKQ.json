{
    "Decision": "",
    "Reviews": [
        {
            "title": "these aren't adversarial examples?",
            "review": "This paper describes a black-box attack on text classification systems by replacing some of the characters with homoglyphs (similar looking characters). The proposed approach is simple: important words are identified through MCTS, and one random character in each of these important words is modified to create an \"adversarial\" input. Homoglyphs have been an important issue in computer security, and it's great to see its first(?) application to NLP. That said, I have major concerns about the paper which prevent me from recommending its acceptance.\n\ncomments:\n- The paper has serious presentation issues. It contains a lot of irrelevant and repeated information, and several important sections of the paper (e.g., explanations about the datasets and models) are only provided in the appendix. \n\n- My main criticism of this paper is that the proposed method is really only \"adversarial\" to one particular NLP pipeline. As the authors mention, the perturbed words will likely be certainly be converted to UNK tokens during preprocessing. If important words are converted to UNKs, the downstream classification task is meaningless, so we'd expect a poor result (e.g., \"this is a good movie\" ---> \"this is a <unk> movie\"). I'm not sure how you can call this an adversarial example, as to the neural network the label of the input could have actually *changed* as a result of the perturbation.  It seems that we can easily fix this issue by using character-level models (or word representations generated by a contextualized embedding such as ELMo). If we can't, then the authors need to provide experiments that show character-level models are also fooled by homoglyphs. In sum, I don't know what this paper adds to the existing body of work on adversarial example generation in NLP (much of which is not cited in the paper).  \n\n- The algorithm used to find important words assumes access to the probability distribution produced by the black box system, but this may not be true for many (most?) commercial systems.\n\n- More details about the MCTS would have been nice. Could you have simply tried all possible perturbations for a given input? If not, how many forward passes on average does the network need to converge to a good policy? \n\n- Is the greedy baseline using homoglyph perturbations? It seems to just be doing random character replacements....",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas but I would have liked to see more analyses and discussion",
            "review": "This paper makes two main contributions:\n- homoglyph attacks in adversarial examples (changing a character into another character of similar visual shape),\n- the use of Monte Carlo Tree Search (MTCS) for determining the \"important\" words to perturb (the ones that are more likely to change a classifier prediction).\n\nClarity:\nThe paper has detailed background information and is overall clear. A few parts are hard to read though, including the description of the MTCS algorithm (the main contribution).\n\nOriginality:\nThis work is, as far as I can tell, original work. Both contributions are extensions of previous work (instead of 1: flipping random characters, 2: simpler greedy search algorithms).\n\nSignificance of contribution 1:\nHomoglyph attacks are well-known in security, e.g., making someone follow a malicious URL visually looking like a legitimate one. The authors note that this kind of attack can also be used for generating adversarial samples. It is good for the ML community to be aware of these attacks. \nThe authors draw a connection with adversarial images, in that they are also similar to natural images and providing compelling examples (replacing ascii characters by similar cyrillic characters in English). On second thought, I think the connection is partially misleading: it is hard to detect whether an image is adversarial (although there is some recent research on that). With text, it is possible to ensure all characters are in a given alphabet; other characters can be removed, or transliterated, or stripped of diacritics, .... The adversarial samples may fool humans, but may have a harder time fooling a defensive algorithm. Of course, attackers have the upper hand being able to forge more and more complex examples, at the expense of being less and less visually similar. The algorithm then becomes more similar to existing algorithms flipping random characters. This is a point that would have been worth discussing.\nAlso, the research community is now well-aware of adversarial examples and has been reacting. One response is the use of sub-word models (bytes, characters, ...), which are much more robust than word-based models, in that there are very few OOV items. The authors pointed out that re-training with adversarial examples does not make the model much more robust. This is clear for word-based models, less so for sub-word models. I would have liked to see this point at least mentioned.\n\nSignificance of contribution 2:\nWhen generating adversarial examples, attacks should be focused on \"important\" words, the ones that have the most influence on the classifier's prediction. The authors use MCTS to find the most important subsets of words, extending previous greedy strategies focusing on a singleword (Gao et al). The authors show that extending the search space in this way improves the rate of successful attacks over the SOTA. I think this is the more substantial contribution of this paper. But this comes at a price: using a more elaborate search strategy will increase the number of calls to the classifier. The paper does not discuss that trade-off; theoretical and/or experimental analyses would have been nice.\n\nOverall, the paper introduces two interesting ideas to the problem of black-box adversarial example generation for text classifiers. However, the paper does little to discuss trade-offs or potential downsides of the proposed approaches. I also feel the two ideas could be published in separate papers, leaving more space to move some of the interesting results from the appendices into the main articles.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}