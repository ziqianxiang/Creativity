{
    "Decision": {
        "metareview": "The paper studies learning from complementary labels – the setting when example comes with the label information about one of the classes that the example does not belong to. The paper core contribution is an unbiased risk estimator for arbitrary losses and models under this learning scenario, which is an improvement over the previous work, as rightly acknowledged by R1 and R2.\n\nThe reviewers and AC note the following potential weaknesses: (1) R3 raised an important concern that the core technical contribution is a special case of previously published more general framework which is not cited in the paper. The authors agree with R3 on this matter; (2) the proposed unbiased estimator is not practical, e.g. it leads to overfitting when the cross-entropy loss is used, it is unbounded from below as pointed out by R1; (3) the two proposed modifications of the unbiased estimator are biased estimators, which defeats the motivation of the work and limits its main technical contributions; (4) R2 rightly pointed out that the assumption that complementary label is selected uniformly at random is unrealistic – see R2’s suggestions on how to address this issue. \nWhile all the reviewers acknowledged that the proposed biased estimators show advantageous performance on practice, the AC decides that in its current state the paper does not present significant contributions to the prior work, given (1)-(3), and needs major revision before submitting for another round of reviews. \n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Interesting setting, but problems with the original estimator and limited experimental evaluation weaken the claims",
            "review": "Pros:\n- The authors consider an interesting problem of learning from complementary labels\n- They propose an approach that, assuming that the complementary label is selected uniformly at random, provides an unbiased estimate for any loss function, which is an improvement over the previous work. \n- Experiments show promising results for modifications of the proposed estimate\n\nCons:\n- Having an unbiased estimate doesn't imply that its minimisation is a successful learning strategy. Indeed, the authors show that minimising their original estimate for the cross-entropy loss leads to overfitting. While the authors attribute this behaviour to the fact that the estimate can be negative, I believe the loss being negative is not problem per se (for example, substituting 0/1 loss with -100/-99 loss would not change the learning; similarly, this is not a problem for the losses considered in [Ishida'17]). I would rather attribute the problem to the fact that the proposed estimate is unbounded from below and there are no generalisation guarantees for it. Indeed, assuming there exists a training example that appears in the training set only once, with one complementary label, estimate (8) can be made arbitrary small by just training to predict probability 0 for the provided complementary label on that example ( and any non-zero probability for other classes). \n- to cope with the above mentioned problem, the authors propose two heuristic-based modifications of the estimate, which are potentially biased. This weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation\n- one of the mentioned motivations for unbiased estimates - being able to perform model selection on complementary labeled validation set - is not illustrated in the experiments\n\nQuestions:\n- I believe 1/(K-1) normalisation factor in (5) is not needed\n- there seems to be a mistake in (9) (and its modifications later on) - I would expect either the subscript $j$ of the probability distribution in the last summand to be exchanged with $k$ in the loss, or a factor $\\pi_j/\\pi_k$ added\n- also, I think there are some mistakes in subscripts in (11)\n- what loss is the method from [Ishida'17] optimising in the experiments?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Clear paper on interesting setup, but claims are undermined by issues with first estimator + lack of motivation for assumptions",
            "review": "This paper proposes an improved approach to the \"complementary-label\" form of weak supervision, in which a label that is *not* the true label is marked. Specifically, this paper proposes an unbiased estimator that accepts arbitrary loss functions and models. Noting that this proposed estimator can suffer from overfitting due to unbounded negative loss, a lower-bounded estimator is proposed. Experiments are then performed on several image classification datasets.\n\nPros:\n- This paper addresses a creative form of weak supervision, proposed by prior work, in which labels that are *not* the true label are labeled, in a clear fashion.\n\n- The first proposed estimator is unbiased, as shown by a proof, and accepts arbitrary losses, an improvement over prior approaches\n\n- The overall presentation is clear and clean\n\nCons:\n- One of the main claims of the paper is the proposal of an unbiased estimator. However, this estimator then does not seem to work well enough due to degenerate negative loss.  So then a modified version is proposed- which does not appear to be unbiased?  Either way, no assertion or proof of it being unbiased is given.  So then presumably this also reverses the claim of being able to cross-validate?  This seems like a major weakening of the paper's contributions\n\n- Since the unbiased estimator does not appear to work well, two implementations of a corrected one are proposed, using heuristic approaches without explicit theoretical guarantees.  This shifts the burden to the experimental studies.  These are somewhat thorough, but not extremely so: for example, one set of hyperparameters were used for all of the methods?  This seems like it could implicitly handicap / favor some over others?\n\n- The proposed estimator is based on the assumption that the probability of classes in the complement set (the set of labels other than the one marked as incorrect) is uniformly distributed (e.g. see beginning of Proof of thm 1).  However, this seems like a potentially naive assumption. Indeed, in the related work section, it is mentioned that work in 2018 already considered the case where this uniformity assumption does not hold.\n\n- More broadly, but following from the above: The paper does not provide any real world examples, real or hypothetical, to give the reader an idea of whether the above uniformity assumption---or really any of these assumptions---are well-motivated or empirically justified.  At the bottom of page 3 in the related work, a concrete application used in prior work is mentioned---where crowd workers are shown single labels and vote Y/N, leading to a mix of standard (if Y) and complement-labeled (if N) data---however this mixed setting is not considered explicitly in this paper.  So, how is the reader supposed to get any idea of whether the assumed setup is motivated or justified?  The experiments do not provide this, because the complementary labels are synthetically generated according to the model assumed in the paper.  Additionally, it is briefly mentioned that collecting complementary labeled data is faster, but again no concrete examples are given to support this.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written paper about an interesting problem. The major problem is that the core part of the contribution is a special case of previously published more general framework not cited in the paper. ",
            "review": "pros:\n\n- Clearly written and sound paper.\n- Addresses interesting problem. \n- Improves existing methods used for this learning scenario.  \n\ncons:\n\n- The core contribution is a special case of previously published more general framework which is not cited in the paper.\n\nIt is clearly written paper with a good motivation. The major problem is that the core contribution, namely, the risk reformulation in Theorem 1 and the derived loss (6), are special cases of more general framework published in \n   Jesus Cid-Sueiro et al. Consistency of Losses for Learning from Weak Labels. ECML 2014.\n\nThe work of [Cid-Sueiro2014] proposes a general way how to construct losses for learning from weak labels. They require that the distribution of weak labels is a linear transformation of the true label distribution, i.e. the assumption (3) of the paper under review. According to [Cid-Sueiro2014], the loss on weak labels is constructed by $weak_loss = L*original_loss$, where $L$ is the left inversion of the \"mixing matrix\" $T$ in (3). [Cid-Sueiro2014] also shows that such weak loss is classification calibrated which implies statistical consistency of the method. \n\nLearning from complementary labels is a special case when the mixing matrix is $T=(E-I)/(K-1)$ (E is unitary matrix, I is matrix of ones, K is number of labels). In this case, the left inversion of $T$ is simply $L=- E*(K-1) + I$ and so the weak loss is $weak_loss=L*loss$ which corresponds to the loss (5) proposed in the paper under review (in fact, the loss (5) also adds a constant term (Y-2)/(Y-1) which however has no effect on the minimizer). \n\nThe novel part of the paper is the non-negative risk estimator proposed in sec 3.3 and the online optimization methods addressed in sec 3.4. These extensions, although relatively straightforward, are empirically shown to significantly improve the results.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}