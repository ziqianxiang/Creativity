{
    "Decision": {
        "metareview": "The paper proposes a method of training implicit generative models based on moment matching in the feature spaces of pre-trained feature extractors, derived from autoencoders or classifiers. The authors also propose a trick for tracking the moving averages by appealing to the Adam optimizer and deriving updates based on the implied loss function of a moving average update. \n\nIt was generally agreed that the paper was well written and easy to follow, that empirical results were good, but that the novelty is relatively low. Generative models have been built out of pre-trained classifiers before (e.g. generative plug & play networks), feature matching losses for generator networks have been proposed before (e.g. Salimans et al, 2016).  The contribution here is mainly the  extensive empirical analysis plus the AMA trick.\n\nAfter receiving exclusively confidence score 3 reviews, I sought the opinion of a 4th reviewer, an expert on GANs and GAN-like generative models. Their remaining sticking points, after a rapid rebuttal, are with possible degeneracies in the loss function and class-level information leakage from pre-trained classifiers, making these results are not properly \"unconditional\". The authors rebutted this by suggesting that unlike Salimans et al (2016), there is no signal backpropagated from the label layer, but I find this particularly unconvincing: the objective in that work maximizes a \"none-of-the-above\" class (and thus minimizes *all* classes). The gradient backpropagated to the generator is uninformative about which particular class a sample should imitate, but the features learned by the discriminator needing to discriminate between classes shape those gradients in a particular way all the same, and the result is samples that look like distinct CIFAR classes. In the same way, the gradients used to train GFMN are \"shaped\" by particular class-discriminative features when trained against a classifier feature extractor.\n\nFrom my own perspective, while there is no theory presented to support why this method is a good idea (why matching arbitrary features unconnected with the generative objective should lead to good results), the idea of optimizing a moment matching objective in classifier feature space is rather obvious, and it is unsurprising that with enough \"elbow grease\" it can be made to work. The Adam moving average trick is interesting but a deeper analysis and ablation of why this works would have helped convince the reader that it is principled. \n\nThis paper was very much on the borderline. Aside from quibbles over the fairness of comparisons above, I was forced to ask myself whether I could imagine that this would be a widely read, influential, and frequently cited piece of work. I believe that the carefully done empirical investigation has its merits, but that the core ideas are rather obvious and the added novelty of a poorly understood stabilized moving average is not enough to warrant acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A well-executed piece of empirical work with unfortunately little to offer in the way of novel ideas."
    },
    "Reviews": [
        {
            "title": "-",
            "review": "This paper proposes to learn implicit generative models by a feature matching objective which forces the generator to produce samples that match the means of the data distribution in some fixed feature space, focusing on image generation and feature spaces given by pre-trained image classifiers.\n\nOn the positive side, the paper is well-written and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets. The method is nice in that, unlike GANs and the stability issues that come with them, it minimizes a single loss and requires only a single module, the generator.\n\nOn the other hand, the general applicability of the method is unclear, the novelty is somewhat limited, and the evaluation is missing a few important baselines. In detail:\n\n1) The proposed objective was used as a GAN auxiliary objective in [Salimans et al., 2016] and further explored in [Warde-Farley & Bengio, 2017]. The novel bit here is that the proposed objective doesn’t include the standard GAN term (so no need for an adversarially-optimized discriminator), and the feature extractor is a fixed pre-trained classifier or encoder from an auto-encoder (rather than a discriminator). \n\n2) The method only forces the generator’s sample distribution to match the first moment (the mean) of the data distribution. While the paper shows that this can result in a generator that produces reasonably good samples in practice, it seems like this may have happened due to a “lucky” artifact of the chosen pre-trained feature extractors. For example, a degenerate generator that produces a single image whose features exactly match the mean would be a global optimum under this objective, equally good as a generator that exactly matches the data distribution. Perhaps no such image exists for the chosen pre-trained classifiers, but it’s nonetheless concerning that the objective does nothing to prevent this type of behavior in the general case. (This is similar to the mode collapse problem that often occurs with GAN training in practice, but at least a GAN generator is required to exactly match the full data distribution to achieve the global optimum of that objective.)\n\n3) It’s unclear why the proposed ADAM-based Moving Average (AMA) updates are appropriate for estimate the mean features of the data distribution. Namely, unlike EMA updates, it’s not clear that this is an unbiased estimator (I suspect it’s not); i.e. that the expectation of the resulting estimates is actually the true mean of the dataset features.  It’s therefore not clear whether the stated objective is actually what’s being optimized when these AMA updates are used.\n\n4) Related to (3), an important baseline which is not discussed is the true fixed mean of the dataset distribution. In Sec. 2.4 (on AMA) it’s claimed that “one would need large mini-batches for generating a good estimate of the mean features...this can easily result in memory issues”, but this is not true: one could trivially compute the full exact dataset mean of these fixed features by accumulating a sum over the dataset (e.g., one image a time, with minibatch size 1) and then dividing the result by the number of images in the dataset. Without this baseline, I can’t rule out that the method only works due to its reliance on the stochasticity of the dataset mean estimates to avoid the behavior described in (2), or even the fact that the estimates are biased due to the use of ADAM as described in (3).\n\n5) The best results in Table 3 rely on initializing G with the weights of a decoder pretrained for autoencoding. However, the performance of the decoder itself with no additional training from the GFMN objective is not reported, so it’s possible that most of the result relies on *autoencoder* training rather than feature matching to get a good generator. This explanation seems especially plausible due to the fact that the learning rate is set to a miniscule value (5*10^-6 for ADAM, 1-2 orders of magnitude smaller than typical values). Without the generator pretraining, the next best CIFAR result is an Inception Score of 7.67, lower than the unsupervised result from [Warde-Farley & Bengio, 2017] of 7.72.\n\n6) It is misleading to call the results based on ImageNet-pretrained models “unconditional” -- there is plenty of overlap in the supervision provided by the labeled images of the much larger ImageNet to CIFAR and other datasets explored here. This is especially true given that the reported metrics (Inception Score and FID) are themselves based on ImageNet-pretrained classifiers. If the results were instead compared to prior work on conditional generation (e.g. ProGAN (Karras et al., 2017), which reports CIFAR IS of 8.56), there would be a clear gap between these results and the state of the art.\n\nOverall, the current version of the paper needs additional experiments and clarifying discussion to address these issues.\n\n=======================================\n\nREVISION\n\nBased on the authors' responses, I withdraw points 3-5 from my original review. Thanks to the authors for the additional experiments. On (3), I indeed misunderstood where the moving average was being applied; thanks for the correction. On (4), the additional experiment using the global mean features for real data convinces me that the method does not rely on the stochasticity of the estimates. (Though, given that the global mean works just as well, it seems like it would be more efficient and arguably cleaner to simply have that be the main method. But this isn't a major issue.) On (5), I misread the learning rate specified for \"using the autoencoder features\" as being the learning rate for autoencoder *pretraining*; thanks for the correction. The added results in Appendix 11 do show that the pretrained decoder on its own does not produce good samples.\n\nMy biggest remaining concerns are with points (2) and (6) from my original review.\n\nOn (2), I did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset. That being said, the paper shows that this doesn't tend to happen in practice with existing classifiers, which is an interesting empirical contribution. (It would be nice to also see ablation studies on this point, showing the results of training against features from single layers across the network.)\n\nOn (6), I'm still unconvinced that making use of ImageNet classifiers isn't providing something like a conditional training signal, and that using such classifiers isn't a bit of an \"unfair advantage\" vs. other methods when the metrics themselves are based on an ImageNet classifier. I realize that ImageNet and CIFAR have different label sets, but most if not all of the CIFAR classes are nonetheless represented -- in a finer-grained way -- in ImageNet. If ImageNet and CIFAR were really completely unrelated, an ImageNet classifier could not be used as an evaluation metric for CIFAR generators. (And yes, I saw the CelebA results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old DCGAN results, I can't tell.)\n\nOn the other hand, given that the approach relies on these classifiers, I don't have a good suggestion for how to control for this and make the comparison with prior work completely fair. Still, it would be nice to see acknowledgment and discussion of this caveat in a future revision of the paper.\n\nOverall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is well-written and has some interesting results from its relatively simple approach, I've raised my rating to above acceptance threshold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Generative Feature Matching Networks\"",
            "review": "The paper proposes a non-adversarial feature matching generative model (GFMN). In feature matching GANs, the discriminator extract features that are employed by the generator to match the real data distribution. Through the experiments, the paper shows that the loss function is correlated with the generated image quality, and the same pretrained feature extractor (pre-trained on imagenet) can be employed across a variety of datasets. The paper also discusses the choice of pretrained network or autoencoder as the feature extractor. The paper also introduces an ADAM-based moving average. The paper compares the results with on CIFAR10 and STL10 with a variety of recent State-of-the-art approaches in terms on IS and FID. \n\n+ The paper is well written and easy to follow. - However, there are some typos that should be addressed. Such as:\n“The decoder part of an AE consists exactly in an image generator ”\n“Our proposed approach consists in training G by minimizing”\n“Different past work have shown” -> has\n“in Equation equation 1 by”\n“have also used” better to use the present tense.\n\n+ It suggests a non-adversarial approach to generate images using pre-trained networks. So the training is easier and the quality of the generated images, as well as the FID and IS, are still comparable to the state-of-the-art approaches.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper consists of two contributions: (1) using a fixed pre-trained network as a discriminator in feature matching loss ((Salimans et al., 2016). Since it's fixed there is no GAN-like training procedure. (2) Using \"ADAM\"-moving average to improve the convergency for the feature matching loss.\n\nThe paper is well written and easy to follow but it lack of some intuition for the proposed approach. There also some typo, e.g. \"quiet\" -> quite. Overall, it's a combination of several published method so I would expect a strong performance/analysis on the experimental session.\n\nDetailed Comment:\n\nFor contribution (1):\n\nThe proposed method is very similar to (Li et al. (2015)) as the author pointed out in related work besides this work map to data space directly. Is there any intuition why this is better? \n\nThe proposed loss (the same as (Salimans et al., 2016)) only try to matching first-order momentum. So I assume it is insensitive to higher-order statistics. Does it less successful at producing samples with high visual fidelity?\n\nFor contribution (2):\n\n\"one would need big mini-batches which would result in slowing down the training.\" why larger batch slowing down the training? Is there any qualitative results? Based recent paper e.g. big gan, it seem the model can benefit a lot from larger batch. In the meanwhile, even larger batch make it slower to converge, it can improve throughput. \n\nAgain, can the author provide some intuition for these modification? It's also unclear to me what is ADAM(). Better to link some equation to the original paper or simply write down the formulation and give some explanation on it.\n\nFor experiments:\n\nI'm not an expert to interpret experimental results for image generation. But overall, the results seems not very impressive. Given the best results is using ImageNet as a classifier, I think it should compare with some semi-supervised image generation paper.\n\nFor example, for CIFAR results, it seems worse than (Warde-Farley & Bengio, 2017), Table 1, semi-supervised case. If we compare unsupervised case (autoencoder), it also seems a lot worse. \n\nAppendix A.8 is very interesting / important to apply pre-trained network in GAN framework. However, it only say failed to train without any explanation.\n\nI think even it just comparable with GAN, it is interesting if there is no mode collapsing and easy to train. However, it has no proper imagenet results (it has a subset, but only some generated image shows here). \n\nIn summary, this paper provide some interesting perspectives. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. I tend to give a weak reject or reject for this paper.\n\n---\n\nUpdate post in AC discussion session.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting results supported by experiments",
            "review": "The paper introduces Generative Feature Matching Networks (GFMNs) which is a non-adversarial approach to train generative models based on feature matching. GFMN uses pretrained neural networks such as Autoencoders (AE) and Deep Convolutional Neural Networks (DCNN) to extract features. Equation (1) is the proposed loss function for the generator network. In order to avoid big mini-batches, the GFMN performs feature matching with ADAM moving average.The paper validates its proposed approach with several experiments applied on benchmark datasets such as CIFAR10 and ImageNet.\n\nThe paper is well-written and straight-forward to follow. The problem is well-motivated by fully discussing the literature and the proposed method is clearly introduced. The method is then validated using several different experiments.\n\nTypos:\n** Page 1 -- Paragraph 3 -- Line 8: \"(2) mode collapsing in not an issue\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}