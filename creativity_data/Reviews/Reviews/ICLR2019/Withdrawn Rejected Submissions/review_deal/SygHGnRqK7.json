{
    "Decision": {
        "metareview": "While there was some support for the ideas presented, unfortunately this paper was on the borderline. Significant concerns were raised as to whether the setting studied was realistic, among others.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Borderline paper"
    },
    "Reviews": [
        {
            "title": "Unclear advantage",
            "review": "The paper uses the beta process to do federated neural matching. The brief experimental results show worse performance than the other techniques compared with. Also, the motivation for the hierarchical beta process isn't clear, since each group has a single Bernoulli process. This makes learning each second level beta process a meaningless task. Why not have a single beta-Bernoulli process?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea, but slightly contrived and lacking empirical support",
            "review": "The paper develops a novel solution for federated learning under three constraints, i.e. no data pooling (which distillation violates), infrequent communication (which iterative distributed learning violates), and modest-sized global model (which ensemble model violates). This is acknowledgedly a kind of unique setting, and the proposed solution does fit it well.\n\nHowever, I have the following two main concerns\n1. The major attack on distillation from an ensemble is that it needs to pool data across all sources which has cost and privacy concerns. However I'm not entirely convinced this \"data pooling\" is really necessary. One could argue distillation might as well be performed with simply an extra dataset that could be collected (sampled) elsewhere.\nPlus, even though the proposed solution doesn't need to do \"data pooling\", it is effectively doing \"model pooling\" which may has its own costs and issues, e.g. the assumptions that one has access to all the parameters of the local models, and that all those local models should more or less be homogeneous to allow such pooling to happen, might not hold.\n\n2. The idea of applying Beta-Bernoulli Process to uncover the underlying global model from a pool of local models is interesting. But I would very much like to see comparisons to some other simpler baselines, e.g. using dictionary learning to extract the common set of basis shared among the local models, or perhaps the slightly fancier DP-means (Kulis & Jordan, 2012)? Especially the lack of a meaningful improvement over the compared baselines from the empirical studies makes me wonder whether the BBP is indeed fit for purpose or even necessary for this task.\n\nSome other questions/comments,\n1. I'd be interested to see what the authors think about the connection between their proposed PFNM to Hinton's dropout, which could also be interpreted as performing an implicit \"model pooling\" over an ensemble of local models sharing weights among each other.\n\n2. After introducing the notation for \"-j\", I'd suggest not to abuse \"j\" to keep denoting (dummy) indices in summations (e.g. Eq.(7), (8), etc.) - I might prefer swapping it with e.g. \"j'\" in $B^{j'}_{i,l}$, $v_{j'l}$ and $\\sigma^2_{j'}$ to avoid confusions.\n\n3. When the number of batches J gets larger, which means a smaller batch size and therefore also a larger variance among the local models, would it be beneficial to also increase the noise variances $\\sigma_j$ accordingly to allow a better fit?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting way to combine neural networks trained locally on federated data using a Beta-Bernoulli process",
            "review": "Summary: The paper considers federate learning of neural networks, i.e. data are distributed on multiple machines and the allocation of data points is potentially inhomogenous and unbalanced. The paper proposes a method to combine multiple networks trained locally on different data shards and form a global neural network. The key idea is to identify and reuse neurons that can be used for all shards and add new neurons to the global network if necessary. The matching and combination process is done via MAP inference of a model using a Beta-Bernoulli process. Some experiments on federated learning demonstrate the performance of the proposed method.\n\nGeneral evaluation (+ pro/ - con, more specific comments/questions below):\n+ the paper is very well-written -- the BBP presentation is light but very accessible. The experimental set up seems sound.\n+ the matching procedure is novel for federated training of neural networks, as far as I know, but might not be if you are a Bayesian nonparametric person, as the paper pointed out similar techniques have been used for topic models.\n- the results seem to back up the claim that the proposed is a good candidate for combining networks at the end of training, but the performance is very similar or inferior to naive combination methods and that the global network is way larger than individual local network and nearly as large as simply aggregating all neurons together.\n- the comparison to recent federated learning methods is lacking (e.g. McMahan et al, 2017) (perhaps less communication efficient than the proposed method, but more accurate).\n\nSpecific comments/questions/suggestions:\n- the MAP update for the weights given the assignment matrix is interesting and resembles exactly how the Bayesian committee machine algorithm of Tresp (2000) works, except that the variances are not learnt for each parameter but fixed for each neuron. On this, there are several hyperparameters for the model, e.g. variance sigma_j -- how are these tuned/selected?\n- the local neural networks are very mall (only 50 neurons per layer). How do they perform on the test set on the homogeneous case? Is there a performance loss by combining these networks together?\n- the compression rate is not that fantastic, i.e. the global network tends to add new neuron for each local neuron considered. Is this because it is in general very hard to identify similar neuron and group them together? In the homogeneous case, surely there are some neurons that might be similar. Or is it because of the MAP inference procedure/local optima?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}