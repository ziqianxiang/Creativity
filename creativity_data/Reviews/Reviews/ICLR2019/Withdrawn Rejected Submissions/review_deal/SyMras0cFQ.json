{
    "Decision": {
        "metareview": "This paper shows how to obtain more homogeneous activation of atoms in a dictionary. As reviewers point out, the paper is well written and indeed shows that the propose scheme results in a more uniform activation. However, the value of this contribution rests on making a case that uniformity is indeed a desirable outcome per se. As two reviewers explain, this crucial point is left unaddressed, which makes the paper too weak for ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Unclear what the benefit of this approach is"
    },
    "Reviews": [
        {
            "title": "Well written but poorly motivated",
            "review": "This paper discusses the addition of a regularizer to a standard sparse coding/dictionary learning algorithm to encourage the atoms to be used with uniform frequency.    I do not think this work should be accepted to the conference for the following reasons:\n\n1: The authors show no benefit of this scheme except perhaps faster convergence.  If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.  SPAMS (http://spams-devel.gforge.inria.fr/) can train a model on image patches as the authors do here in a few tens of seconds on a modern computer.  On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.\nIn my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.  It is not even clear that the final compression of the baselines would not be better.  Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.\n\n2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.   The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset.  Even without the \"train to convergence\" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.   ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Solid work but the importance unclear",
            "review": "Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work's novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning,\ncomputational neuroscience and biological vision.  I have experience\nwith neural network models and visual neurophysiology.\n\n2. This paper develops and tests an adaptive homeostatic algorithm for\nunsupervised visual feature learning (for example for learning models\nof early visual processing/V1).\n\n3.The work spends a lot of pages describing the general problem of\nunsupervised feature learning and the history of the base algorithms.\nThe literature review is quite extensive.  The new content appears to\nbe in section 2.2 (Histogram Equalization Homeostasis - HEH), where a\nsimple idea to keep all units with balanced activity over the set of\nnatural images.  The authors also develop a computationally cheaper\nversion they call HAP (Homeostasis on Activation Probability) The\nauthors show that their F function is optimized quicker with the HEH\nand HAP algorithms.  I would like to see how these curves vary with\nthe number of neurons (e.g. can you add X% more neurons and get\nsimilar convergence speed -- and if so which is more computationally\ncostly)?\n\n4. Many groups have developed various homeostatic algorithms for\nunsupervised learning, though I have not seen this exact one before.\n\n5.  The experiments reveal the resulting receptive fields and show the \ndecrease in the F function (error function).    The resulting receptive fields\ndo not seem that different to me between the different methods.  I am also not\nthat convinced that the faster convergence as a function of learning step is that important\nespecially as the learning steps may be more computationally expensive for this method.\n\n6. I am not sure how interesting this work will be for the ICLR audience,\nas it is not clear how important the faster convergence and more even\nutilization of neurons is (and how it would compare computationally\nwith just having more neurons).\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written paper, with good literature review and interesting experiments showing faster unsupervised learning",
            "review": "This paper proposed a bio-inspired sparse coding algorithm where iterations\nfor dictionary updates take into account the past updates. It is argued\nthat time takes a crucial rule in learning.\n\nThe paper is quite well written and contains an extensive literature review\ndemonstrating a good understanding of previous literature in both ML/DL and biological\nvision.\n\nThe idea of using a \"non-linear gain normalization\" to adjust atom selection\nin sparse coding is interesting and as far as I know novel, while providing\ninteresting empirical results: The system learns in an unsupervised way faster.\n\nMisc:\n\n- Using < > for latex brakets is not ideal. I would recommend: $\\langle\\,,\\rangle$\n\n- \"derivable\" I guess you mean \"differentiable\"\n\n- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the\nreference to Pedregosa et al. for sklearn is missing.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}