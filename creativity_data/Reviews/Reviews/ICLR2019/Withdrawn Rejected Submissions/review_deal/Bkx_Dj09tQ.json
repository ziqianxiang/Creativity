{
    "Decision": {
        "metareview": "The authors conduct experiments to study orientation selectivity in neural networks. \n\nThe reviewers generally agreed that the paper was clearly written and easy to follow. Further, the experimental analysis demonstrates that contrary to what was claimed in some previous work, the learned orientation selectivity can be useful for generalization. \n\nHowever, the reviewers also raised a number of concerns: 1) that the conclusions are drawn on the basis of a couple of neural network architectures; the authors attempted to add results using a Resnet50 model, but this analysis was ultimately removed when the authors discovered a bug; 2) in the context of the contributions in neuroscience it was not clear that the limited results on the two artificial networks are sufficient to help draw such conclusions, and that 3) since the network is trained to recognize objects, it would seem natural that the model would learn neurons that are sensitive to orientation and that it is not clear how the author’s observations might lead to better trained models. \nWhile the reviewers were not completely unanimous in their scores, the AC agrees with a majority of the reviewers that the work while interesting could be strengthened by additional experiments on other architectures. \n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Clearly written paper, but unclear impact"
    },
    "Reviews": [
        {
            "title": "Interesting and well-written paper",
            "review": "The paper presents a study on orientation selectivity in DNNs for image classification, arguing that this type of selectivity in the lower layers is crucial for generalization. This hypothesis is tested through an ablation study, which the authors interpret as a suggestion of the existence of a causal relation.\n\nThe authors tackle a very interesting problem that seems to have not received yet enough attention. The paper is quite well-written and clear, making it understandable also to non-experts. The only concern I would have is about the causal claims in Section 3.4. I’m not completely sure the ablation experiments are the correct way to “prove” causality, as opposed to somehow trying to intervene on the orientation selectivity directly. On the other hand, this seems complicated to prove and the approach proposed in the paper seems a pragmatic solution. I would possibly hedge slightly the causal claims.\n\nFrom an outsider point of view, I think the paper provides an interesting contribution to the discussion on orientation selectivity. I particularly appreciated its clarity and reproducibility.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review of Causal importance of orientation selectivity for generalization in image recognition",
            "review": "COMMENTS RELATED TO REVISION:\nThe new analysis that has been added takes a step towards getting at the relationship to invariance. This is a positive. In general comments, the authors state as important contributions: \n\n\"1) to the debate on the harmfulness/importance of selectively activated units in DNNs [1-3] by presenting concrete examples where selectivity is important for generalization, and 2) to the neuroscience community, where, although orientation selectivity has been extensively analyzed for these 60 years since [4], its functional importance in a natural environment has remained unanswered.\"\n\nOn point 2, while this provides an example of an artificial network where orientation plays an important role, it's a stretch to generalize this to conclusions concerning functional importance in neuroscience.\n\nOne point 1, I agree that this paper takes steps in the right direction, but ultimately the overall conclusions still feel as though they are a natural and implied consequence of limiting orientation selectivity. It is noted that  orientation selectivity is \"not just a superficial byproduct of object recognition, but is causally indispensable for object recognition\". The idea that selectivity for oriented edges is indispensible for object recognition again is a conclusion that feels as though it would be shocking if this were not true. The notion of it being a superficial byproduct of object recognition presupposes that the purpose of the system is to recognize objects. Again, it would be surprising if there were vestigial features in the network that are learned, but play no important role - especially among early layers. \n\nI think the paper might be strengthened by re-working this second point to more strongly establish the causality the authors claim. \n\nORIGINAL COMMENTS:\nThis paper presents interesting analysis and an ablation study on orientation selectivity in neural networks. This is analyzed with respect to generalization performance in decisions made. Overall, the paper is well written and interesting. However, I have a number of comments / concerns as described below:\n\nPositives:\n- The paper presents an in depth analysis of the role of orientation selectivity in neural hierarchies. This style of analysis is sorely lacking and fits the theme of learning representations\n- The paper itself is well written and quite polished\n- The authors have taken great care to rule out any possible confounds through experiments that are not identical to the main claims or objective (specifically the study of section 3.3)\n\nTo address:\n- The only concern I have (and it is somewhat significant), is the notion of \"generalization\". This is a rather loaded term, and it is not clear on cursory inspection where this generalization comes from. Is it a function of invariance among higher layers to scene geometry? What are the fundamental underpinnings of these observations outside of correlation to orientation selectivity.\n- It seems almost a tautology that removing orientation selectivity would impair performance. I would be more satisfied, and render a higher rating if I felt there was a \"smoking gun\" with respect to evidence. The conclusion is convincing, but the reasoning comes across as somewhat vague. With that said, it is also understood that this is a non-trivial matter to address and perhaps this paper is an important first step.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Straightforward paper, but unclear what we learn from it",
            "review": "*Update after discussion period*\nI remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks.\n\n\nSummary:\nThe authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network's ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network's generalization performance. \n\nStrengths:\n+ Very straightforward and easy to follow \n+ Technically sound\n\nWeaknesses:\n- Feels trivial\n- The claims seem to be too general\n\nConclusion:\nI'm torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I'm not really sure what we learn from the paper.\n\n\nSpecific comments:\n\n- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?\n\n- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.\n\n- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (\"orientation selectivity [plays] a causally important role in object recognition\" – abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.\n\n- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}