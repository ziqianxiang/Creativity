{
    "Decision": "",
    "Reviews": [
        {
            "title": "Strong results but paper is based somewhat on minor changes from previous work",
            "review": "This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples. It proposes an extension to Learning to Compare (Sung 2017), where Learning to Compare learns a relation module that is parametrized to product a similarity score between whole feature-maps of embedded support and query set examples. Rather than measure similarity between examples at the feature-map level, this work proposes measuring similarity at the object-level. Specifically, an object is represented by each (i,j) position on the final feature-map derived through the embedding network. Then to compare a support image and query image, we compare pairwise across all (i,j) positions on the final feature-map between support and query images by computing object similarity features. The pairwise object similarity features are then summed and mapped to a similarity probability between the support and query example. Note that the network structure used for object comparison here is derived from the Relation network from Santoro 2017. The model is then trained in the same way as Learning to Compare, where we want support and query examples with the same class label to have similarity probability of 1. Experiments are conducted on Omniglot and Mini-Imagenet, comparing the proposed method to previous work.\n\nPros:\n- Decent improvement in Mini-Imagenet results relative to Learning to Compare (close to +9% for 1-shot and +5% for 5-shot relative to Learning To Compare) .\n- One concern I had was that improvement in performance was due to working with 224 x 224 images in this paper (rather than resized 84 x 84 images as most previous work does with Mini-Imagenet); however, additional comparison was performed with Learning To Compare where that model also uses 224 x 224 images but there was no improvement there just by using larger image input, thus establishing that larger image input is not a reason for the improved performance.\n\nCons:\n- The proposed idea is not particularly novel, as it is basically the Learning to Compare model but with pairwise comparison at each location in the final feature map rather than comparing the feature maps as a whole.\n\nRemarks:\n- Figure 4 does not seem to be that useful. It is used to comment that overfitting is not an issue by showing training and testing performance on Omniglot dataset but maybe would be more useful if Learning to Compare was also shown in those plots to compare the level of overfitting between the two models.\n\nSantoro et al. A simple neural network module for relational reasoning. 2017.\nSung et al. Learning to Compare: Relation Network for Few-Shot Learning. 2017.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Minor extension of the existing method",
            "review": "- In evaluation of omniglot, (\"a small improvement over the state-of-the-art should be considered as significant\") since all the results are already quite high, the small difference are not much beneficial in practice. Further, to justify the significance of the proposed method, a statistical test should be conducted and reported at least.\n\n- In comparison to the LearningToCompare, Sung et alâ€™s method compares the query image to all the support images, while the proposed method compares in a pairwise manner.  Thus, from a computational complexity perspective, the proposed method can be much expensive.\u0000\n\n- The statement of \"averaging the object relation features has similar effect as ensemble modeling\" needs theoretical or at least empirical supporting results along with explanations.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theoretical and experimental justifications are weak. ",
            "review": "This paper proposes a few shot learning method by exploiting the object-level relation between different images. The proposed method is based on neared neighbor search and concatenates feature maps of two input images into one feature map by considering all location combinations. Then for each slice of the location combination, the method applies fully connected layers g(). The outputs of g() are summed along location combinations and used as a feature to calculate the similarity of two inputs.  \n\nOverall, the idea of object relation is interesting. However, the proposed method is incremental from Relation Network (Learning2Compare Sun te al. 2017) and lacks theoretical and experimental justification why the proposed model works well. \n\nPros\n+ Interesting idea to improve the performance of few-shot learning. \n+ The performance improvements from Relation Network seems to be high. \n\nCons \n-Relation Network also concatenates feature maps of different objects. The main novelty of this paper seems to be all combination is considered in the concatenation and applied the separate network g() for each combination. However, why this feature extraction process works well is not theoretically justified.  \n\n-Relation Networks use different classification and learning scheme to this paper. Thus, it is not clear which parts of the proposed method, eg, the proposed concatenation procedure or the nearest neighbor model produced better results than Relational Networks. At least, the author should compare the performance when the proposed method concatenates two feature maps as in the Relation Network. \n\n-In the Sec.5, authors wrote that when the feature map size is 1 \\times 1 the proposed method corresponds to Relational Network. However, as far as I read the paper of Relational Network, the size of the feature map is not 1 \\times 1. \n\n-Each location of the feature map seems to be not an object. It needs more explanations why the feature of each location is an object. \n\n-It is not clear if the parameter of g() is common for all location combinations or learn different g() for each location combination. \n\n- Fig.2 lacks the explanation of the concatenation process of feature maps. The process is not only concatenation of feature maps, but concatenating features of all combination of location. It is hard to understand why the column size is w \\times h \\times w \\times h only writing it concatenation.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}