{
    "Decision": {
        "metareview": "This paper proposed an unsupervised learning algorithm for predictive modeling. The key idea of using NCE/CPC for predictive modeling is interesting. However, major concerns were raised by reviewers on the experimental design/empirical comparisons and paper writing.  Overall, this paper cannot be published in its current form, but I think it may be dramatically improved for a future publication. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Concerns about the experiments and paper clarity."
    },
    "Reviews": [
        {
            "title": "Review for \"Neural Belief Representations\"",
            "review": "# Review for \"Neural Belief Representations\"\n\n\n\nThe authors argue in the favor of belief representations for partial observable Markov decision processes. The central argument is that uncertainty needs to be represented to make optimal decision making. For that aim, three belief representations based on sufficient statistics of the future are evaluated and compared in a set of disective studies. Those studies find that predicting the future results in uncertainty being represented in the state representations, although they differ in quality.\n\nI found the paper hard to follow for various reasons. \n\n- NCE is reviewd, while CPC is not. I would have found a review of CPC as well to help my understanding, especially to draw the line between CPC and CPC|Action.\n- In 2.1., $b_t$ is defined as a probability, while it is the output of a neural network later. This is formally incompatible, and I found  the connection not well explained. From my understanding, $b_t$ is a vector that represents the sufficient statistics if learning works. The probability interpretation is thus stretched.\n- The architecture description (starting from the second paragraph on page 4) feels cluttered. It was clearly written as a caption to Figure 1 and hence should be placed as such. Still, stand alone texts are important and in my humble opinion should be augmented with equations instead of drawings. While the latter can help understanding, it lacks precision and makes reproduction hard.\n- The MLP to predict the ground truth is not sufficiently described in the main text. I think it needs to go there, as it is quite central to the evaluation.\n\nSince the manuscript is half a page under the limit, such improvements would have been feasible.\n\nApart from the quality of the manuscipt, I like the fact that a disective study was done in such a way. \n\nHowever, I would have liked to see more comparisons, e.g. in $(x, y, \\theta)$ environments it is also possible to obtain quite good approximations of the true posterior via particle filtering. Also, other more straightforward approaches such as MDN-RNNs can represent multiple maxima in the probability landscape; this would have enabled to examine the benefit of conditioning on actions in a different context.\n\nRight now, it is unclear what the paper is about. On the one hand, it does a focused disective study with well controlled experiments, which would be a good fit if many different models were considered. On the other hand, it advertsises CPC|Action; but then it falls short in evaluating the method in more challenging environments.\n\nTo sum it up, I feel that the paper needs to be clearer in writing and in experimental structure. The currently tested hypothesis, \"does CPC|Action perform better than CPC and FP in a set of well controlled toy environments\" is, imho, not of broad enough interest.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper, some details missing or unclear",
            "review": "** Summary **\nThe authors evaluate three different representation learning algorithms for partially observable environments. In particular, they investigate how well the learned representation encodes the true belief distribution, including its uncertainty. \nThey propose an extension to a previous algorithm and evaluate all three algorithms on a range of tasks.\n\n** Clarity **\nThe paper is well written and overall easy to follow.\n\n** Quality **\nThe paper evaluates the described algorithms on a sufficiently large set of tasks. There is no theoretical analysis.\n\n** Originality & Significance **\nWhile the authors propose a novel extension to an existing algorithm, I believe the value of this work lies in the detailed empirical analysis.\n\n** Missing Citations **\n\nI believe two recent papers (this year's ICML) should be mentioned in the related work section as they propose two representation learning algorithms for POMDPs that, as far as I can tell, are not yet mentioned in the paper but quite relevant to the discussed topic. [1] Because it also uses PSRs and [2] because it explicitly learns a belief state. It would be interesting to see how [2] compares in terms of performance to FP and CPC(|Action).\n\n[1] Hefny, A., Marinho, Z., Sun, W., Srinivasa, S. & Gordon, G.. (2018). Recurrent Predictive State Policy Networks. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:1949-1958\n\n[2] Igl, M., Zintgraf, L., Le, T.A., Wood, F. & Whiteson, S.. (2018). Deep Variational Reinforcement Learning for POMDPs. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:2117-2126\n\n** Question **\n\nI have several questions where I'm not sure I understand the paper correctly:\n\n1.) Why can FP only predict the mean? For example, one could use a PixelCNN as decoder, which would allow to learn an entire distribution, not just the mean over images.\n2.) The problem that CPC and CPC|Action is unable to predict objects if they don't influence the future trajectory doesn't seem surprising to me because whether an image is a positive or negative example can usually be determined by the background, the object is not necessary to do so. In other words, this is a problem of how the negative samples are chosen: If they were constructed using a simulator that shows the same background but without the objects, the belief would need to start encoding the presence of objects. Is this correct or am I missing something?\n3.) Am I correct in thinking that CPC(|Action) would not be applicable to properly estimate the belief distribution in the presence of noise, i.e. for example when estimation the exact location based on sensors with Gaussian noise?\n\n** Overall **\n\n* Pros:\n- Extensive, interesting evaluation\n- Novel CPC|Action algorithm\n\n* Cons:\n- No theoretical analysis/justification for claims\n- There are several subtleties that I am not sure are sufficiently discussed in the paper (see my questions above)",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas, excellent experiments but a big question mark and significance hard to assess",
            "review": "This paper learns a deep model encoding a representation of the state in a POMDP using one-step frame prediction or a  contrastive predictive coding loss function. They evaluate the learned representation and shows it can be used to construct a belief of the state of the agent. \n\nUsing deep networks in POMDP is not new, as the authors pointed out in their related work section. Thus I believe the originality of the paper lies in the type of loss used and the evaluation of the learned representation trough the construction of a belief over current and previous states. I think this method to evaluate the hidden state has the potential to be useful should one wish to evaluate the quality of the hidden representation by itself. In addition, I found the experimental evaluation of this method to be rather extensive and well conducted, as the authors experimented on 3 different (toy) environments and use it to quantify and discuss the performance of the three model architecture they develop.\n\nOn the other hand, the authors mention that other works have already shown that learned representations can improve agent performance, can be learned by supervised learning (= predicting future observations) or can be useful for transfer learning. So in that context, I am not sure the contributions of this paper are highly significant as they are presented. To better highlight the strength the evaluation method, I think it could be interesting to check that the accuracy in belief prediction is correlated with an improvement in agent performance or in transfer learning tasks. To better highlight the interest of the CPC loss, I think it could be interesting to compare it to similar approaches, for example the one by Dosovitskiy & Koltun (2017).\n\nI found the paper reasonably clear. However, the following sentence in the appendix puzzled me. \"The input to the [evaluation] MLP is the concatenation of b_t and a one-hot of the agent’s initial discretised position and orientation.\" I may have missed something, but I do not understand how the model can contain the uncertainty shown in the experimental results if the agent's initial position is provided to the current or past position predictor.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}