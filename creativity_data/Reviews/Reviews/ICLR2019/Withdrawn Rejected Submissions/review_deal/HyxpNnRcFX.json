{
    "Decision": {
        "metareview": "This paper is extending the meta-learning MAML method to the mixture case. Specifically, the global parameters of the method are now modeled as a mixture. The authors also derive the elaborate associated inference for this approach.\n\nThe paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth. \n\nThe results do not convince any of the three reviewers. Rev3 asks for a clearer exposition of the results to increase convincingness. Rev2 and Rev1 also make similar comments.  \n\nRev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated. Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect. \n\nThe reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness. Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade-offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights. \n\nFinally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Timely idea... but paper lacks in results and conclusive insights."
    },
    "Reviews": [
        {
            "title": "A more systematic evaluation is necessary",
            "review": "This paper presents a mixture of hierarchical Bayesian models for meta-learning to modulate transfer between various tasks to be learned. A non-parametric variant is also developed to capture the evolution of a task distribution over time. These are very fundamental and important problems for meta-learning. However, while the proposed model appears to be interesting, the evaluation is less convincing. \n\n1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). Especially, by Table, the proposed model performs much worse than existing methods (50% vs 60%). More discussions and explanations on this experiment are clearly required.\n\n2. A more systematic and realistic evaluation is necessary to justify the proposed method. As a method that aims to cope with heterogeneous or even evolving task distributions, it is expected to work well in practice and outperform those baselines that are designed for a single task distribution. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Promising, but more work needed ",
            "review": "This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). They propose an EM algorithm for joint training of parameter initializations and assignment of tasks to initializations. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet.\n\n[Strengths]\n\n+ Modeling the initialization space is an open research question and the authors make a sound proposal to tackle this.\n+ The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. inevitably saturate model parameters. By dynamically increasing the meta-learner's capacity, this approach can in principle bypass catastrophic forgetting.\n\n[Weaknesses]\n\n- There is nothing in the algorithm that prevents mode collapse, and the only thing breaking symmetry is random initialization. In fact, figure 5 and 6 suggest mode collapse occurs even in the non-parametric case. A closely related paper that may be of interest ( Kim et al., 2018, https://arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD.\n- Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance.\n- Experiments on evolving tasks suggest the method is not able to capture task diversity. In the synthetic experiment (figure 5), the model suffers mode collapse when a sufficiently difficult task is introduced. Ultimately, it performs on par with MAML, despite having three times the capacity. Similarly, on the evolving miniImagenet dataset, figure 6 indicates there is no cluster differentiation across tasks.\n- The paper needs major polishing.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Gradient-base few-shot learning. Extends MAML to a mixture distribution, to allow for internal task clustering. Falls short of recent state-of-art results, while being even a lot slower than MAML",
            "review": "Summary:\n\nThis work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic.\n\nComparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.\n\n- Quality: Paper is technically complex, but based on simple ideas. In the case of\n   infinite mixtures, it is not clear what is done in the end in the experiments.\n   Experimental results are rather poor, given state-of-the-art.\n- Clarity: The paper is not hard to understand. What is done, is done cleanly.\n- Originality: The idea of putting a mixture model on the global parameters is not\n   surprising. Important questions, such as how to make this faster, are not\n   addressed.\n- Significance: The only comparative results on miniImageNet are worse than the\n   state-of-the-art by quite a margin (admittedly, the field moves fast here, but it\n   is also likely these benchmarks are not all that hard). This is even though better\n   performing methods, like Versa, are much cheaper to run\n\nWhile the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea.\n\nState of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53:\n- Versa: https://arxiv.org/abs/1805.09921.\n   Importantly, this method uses a simpler model (logistic regression head models)\n   and is quite a bit faster than MAML, so much faster than what is proposed here\n- BMAML: https://arxiv.org/abs/1806.03836.\n   This is also quite complex and expensive, compared to Versa, but provides good\n   results.\n\nOther points:\n- You use a set of size N+M per task update. In your 5-way, 1-shot experiments,\n   what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5,\n   then I wonder why results are branded as 5-way, 1-shot, which to mean means\n   that each update can use exactly 5 labeled points.\n   Please just be exact in the main paper about what you do, and what main\n   competitors do, in particular about the number of points to use in each task\n   update.\n- Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and\n   uses further approximations (ICM, instead of Gibbs sampling).\n   Can be seen as a heuristic to evolve the number of components.\n   What is given in Algorithm 2, is not compatible with Section 4. How do you merge\n   your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid\n   that there is always one more (L -> L+1) components? Some threshold must be\n   applied somewhere.\n   An alternative would be to use split&merge heuristics for EM.\n- Results reported in Section 5 are potentially interesting, but entirely lack a\n   reference point. The first is artificial, and surely does not need an algorithm of this\n   complexity. The setup in Section 5.2 is potentially interesting, but needs more\n   work, in particular a proper comparison to related work.\n   This type of effort is needed to motivate an extension of MAML which makes\n   everything quite a bit more expensive, and lacks behind the state-of-art, which\n   uses amortized inference networks (Versa, neural processes) rather than\n   gradient-based.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}