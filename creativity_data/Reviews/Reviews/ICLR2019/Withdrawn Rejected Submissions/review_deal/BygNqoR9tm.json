{
    "Decision": {
        "metareview": "The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm.\n\nYet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation.\n\nWhile R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments.\n\nThe AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to \"revise and resubmit\" the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Revise and resubmit"
    },
    "Reviews": [
        {
            "title": "Good motivation but empirical evidence shows limited improvements.",
            "review": "The paper introduces a new cost function for training Wasserstein Autoencoders that combines reconstruction error with Sinkhorn distance on the latent space. Authors provide nice theoretical motivation, yet empirical results seem incremental and do not fully support the effectiveness of this approach.\n\nPros:\n- Theorem 3.1 (although trivial) provides motivation for optimizing Wasserstein distance in the latent space in WAEs.\n- Theorem 3.2 shows sufficiency of optimization over deterministic encoders in WAEs.\n- The proposed SAE virtually does not favor any prior and can preserve some aspects of geometry of the original space. \n\nCons:\n- It is unclear why Sinkhorn algorithm would provide better estimate of Wasserstein distance than e.g. adversarial WGANGP (which would be a variant of GAN-WAE). Sinkhorn convergence is discussed only in terms of sample size and  smoothing regularizer, not in the context of batch training. \n- Quantitative results are on par or marginally better than other methods, they also lack some comparisons (see details below).\n- There is no comparison to relevant models outside VAE scope, e.g. ALI [4]. \n\nThe novelty of this paper is combining WAEs with Sinkhorn algorithm. Overall, it has potential, but the proposed method would probably require clearer evaluation. \n\nDetailed issues:\n- Notation for posterior seems somewhat inconsistent and misleading, namely push-forward G#P_Z = P_G, while Q#P_X = Q_Z.\n- It is unclear why MMD or GAN losses on WAS's latent space are referred to as heuristics, each of these constitutes a divergence in the same way as the proposed Sinkhorn distance.\n- FID scores for MNIST are incomparable due to the use of own network; using LeNet has been proposed [3].\n- It is unclear what ‘Empirical lower bounds’ for MMD mentioned in Table 1. caption mean, as unbiased MMD estimator (e.g. [2]) is available. On the other hand, FID is known to be biased [3], so test-set FID should be provided for comparison.\n- Table 2. lacks comparison of SAE with normal prior even though a) authors note that MMDs are incomparable with different priors, b) SAEs is claimed to be prior-agnostic, c) in such setting MMD-WAE might be advantageous [1]. Again, no test-set FID scores.\n- Samples in Figure 2 too small.\n- MMD lacks citation (e.g. [2]).\n\nTypos:\np.6 line 3 construcetion -> construction\np.6 line 30 Hypersherical -> Hyperspherical \nP.8 line 1 this a sign -> this is a sign\n\n[1] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schölkopf. Wasserstein Auto-Encoders. ICLR 2018.\n[2] Arthur Gretton, Karsten M. Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alex J. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13, 2012a.\n[3] Mikołaj Bińkowski, Dougal J. Sutherland, Michael Arbel, Arthur Gretton. Demystifying MMD GANs. ICLR 2018.\n[4] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky and Aaron Courville. Adversarially Learned Inference. ICLR 2017\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "summary",
            "review": "I tried to understand the paper it seems to me that the paper proposed a new objective function for learning an autoencoder based on Sinkhorn algorithm. Following the idea of Sinkhorn algorithm, the Sinkhorn autoencoder minimizes the W-distance between aggregated posterior and the data prior via integrating the objective of (original) autoencoder and Sinkhorn distance. This looks new, but I did not look insight to the detailed derivations. One thing that I do not like is that Sinkhorn distance needs to be optimized before optimizing the autoencoder distance, if I understand correctly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Contributions should be better explained",
            "review": "Summary\n\nThis paper builds upon the recent Wasserstein Auto-Encoders; the main innovation is the use of the Sinkhorn distance between the prior and the aggregate posterior. This distance, is used as a differentiable and more tractable surrogate for the wasserstein distance, which is proposed as an alternative to heuristics (e.g. MMD) in Wasserstein-auto encoders.\n\nAlso, the authors provide some theoretical results complementing the Wasserstein Auto-Encoders papers and illustrate their results showing better or equal performance than with alternatives, particularly VAE and WAE.\n\nEvaluation.\nIt is certainly a good paper, and well written (but notice several typos). The experimental part is thorough which certainly plays in favor at evaluation time. Theoretical results are a significant contribution, but not quite deep.\n\n But what troubles me is that when I read the paper it was hard for me to understand what the contribution is. My guess on this is what I wrote in the Summary. And if that is the case, I think it is a rather marginal contribution: it would be desired to have a theory for why using the wassertein penalization is better than the ones proposed in the WAE paper (Recall, there, penalizations are introduced to get rid of a constraint that appears in the minimization paper). The authors argue that Sinkhorn does as good as wasserstein and I can believe that (but it would be good if authors could refer to recent results [1] regarding the quality of this approximation, which essentially say that sinkhorn does no do miracles) but the most fundamental question is why a wasserstein penalization is better than the ones proposed in WAE. If no such theory is available, it is hard for me to judge the paper as non-marginal.\n\nTheorem 3.1 is an attempt for such an explanation, but it was not clear at all for me why theorem 3.1 implies that wasserstein distance is the penalization to use. Theorem 3.1 only gives an upper bound. The authors may elaborate on this\n\n\nI hope my criticism is helpful for improving the current version of the manuscript.\n\n\n[1] http://proceedings.mlr.press/v75/weed18a/weed18a.pdf\n\n\n=== after rebuttal ===\n\nThe authors have addressed my concerns and I have updated my score accordingly.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well-written and significant novelty paper ",
            "review": "The paper proposes a new representation of Wasserstein AutoEncoder and provides the formal analysis of learning autoencoders with optimal transport theory. The proposed model, SAE, employs the constraints on the equality of prior and posterior latent spaces with a Sinkhorn distance. Moreover, the proposed model is also backed up with some theoretical guarantees.\n\nThe paper is well-written and easy to follow. The experimental results with different priors have demonstrated the effectiveness of the newly formulated model. However, it is not convinced that what is the advantages of the proposed model with WAE. Can the authors provide more insight and comparison with its counterpart, WAE?\n\nIn term of time-complexity, computing Sinkhorn distance in Alg 1 introduces computation overhead, especially with small \\epsilon. In compared with WAE, what is the computation overhead of the proposed model? Can the authors provide some theoretical analysis of time-complexity and experimental results?\n\nIn Table 2, there are only FID values for WAE with MMD cost. Can the authors show the numbers with WAE-GAN on these datasets?\n\nConclusion: The theoretical and experimental contributions are significant to publish at the venue.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}