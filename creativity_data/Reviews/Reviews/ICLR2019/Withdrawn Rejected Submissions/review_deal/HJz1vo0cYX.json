{
    "Decision": "",
    "Reviews": [
        {
            "title": "CONFIDENCE CALIBRATION IN DEEP NEURAL NETWORKS THROUGH STOCHASTIC INFERENCES",
            "review": "The authors propose a generic framework to calibrate predictions in deep-neural-network-based classifiers. They do this using a new variance-weighted loss function that does not introduce additional (hyper-)parameters. Results on various datasets using various architectures demonstrate how the framework result in models with well calibrated predictions, i.e., not under or overconfident probabilistic estimates of labels.\n\nIn Section 3.3, \\theta is, I think, not a deterministic parameter, though the authors may refer to the fact that it is learned as a point estimate.\n\nEquation (7) should be conditioned on \\theta.\n\nIn section 3.3, expected predictions and their variances can be approximated by MC not \"calculated\" as the authors write.\n\nIn Figure 1 (right), the authors should use the same range for x and y axis to make more evident that the models are poorly calibrated. A diagonal line may also make the deviations from perfect calibration easier to see.\n\nIn Section 4.2, the authors do not provide a theoretical justification for the choice of the variances as the weights for the loss function in (9). Other than that the proposed loss is very similar to that in (10). Also the authors do not properly justify/discuss the use of the Bhattacharyya coefficient as the variance metric or why T=5 samples are enough to get a good estimate of \\alpha.\n\nThe experiments although relatively extensive are not very convincing because results in Table 1 are to be expected (given CI is an average of multiple \\beta values), and Table 2 shows that TS case 2 has better calibration results (ECE) than VWCI, although VWCI has in general better accuracy (at the cost of some calibration loss).\n\nIt is not clear why the authors do not show calibration plots like in Figure 1(right) or Figure 4 in Guo et al, 2017.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Poorly written and confusing paper, albeit with an interesting and worthy aim",
            "review": "I was pleased to be assigned this paper as it sounded like a worthy aim and an interesting topic. However, having attempted to read it several times, I find it hard to take anything from the paper. My first criticism is that the English in the paper is poor, which makes it hard to follow. Take the following sentence from the abstract:\n \"We first analyze relation between variation of multiple model parameters for a single example inference and variance of the corresponding prediction scores by Bayesian modeling of stochastic regularization.\" Despite reading this multiple times I can't make sense of it. The paper is full of sentences such as this.\n\nI had expected the paper to make more of a link to the scoring rule literature given that this is where ideas about how to score predictions, ensuring calibration of predictions etc, has been developed. The Brier score and loglikelihood are used in the results, but the link is not made explicit. The use of the word 'calibration' in this paper is different to how it is used in the scoring rule literature, which is confusing (e.g. 'NLL and Brier score are another metrics for calibration' - whereas usually we'd talk about proper scoring rules, which these both are, as assessing some combination of calibration and sharpness, where these terms are given precise meanings).\n\nThe results, if I've understood them correctly, seem to be positive, but not stellar. A slight improvement is made on the baseline method, but the improvement is very minor.\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Behavior of the new loss at the beginning of the training and weird empirical results",
            "review": "This paper describes a new method to calibrate the accuracy and confidence of the prediction. The authors propose a variance-weighted confidence-integrated loss function to balance the accuracy and confidence via stochastic inference. The empirical results show the proposed method performs better than previous related methods. \n\nCompared to the previous work Pereyra et al. (2017) and Lee et al. (2018), the novelty of this paper is to use different $\\alpha_i$ for different training example instead of a global hyper-parameter $\\beta$. When the variance of prediction is high, the loss weights more on the cross-entropy term with uniform distribution and when the variance of prediction is low, the loss weights more on the cross-entropy loss with ground-truth. \n\nI wonder how this new loss works at the beginning of the training. At the beginning of the training, the variance of the prediction for most samples is high, so the new loss forces the prediction to be close to the uniform distribution. How could the model learn the prediction distribution by optimizing this loss?\n\nThe new loss requires multiple forward passes in order to compute $\\alpha_i$ while the previous method Eq. 10 only requires one pass. So the computational cost of the new method is $n$ times of the previous method where $n$ is the number of forward passes in the new method. It seems unfair to use the same number of epochs for the new and previous methods. I wonder what the comparison would be if training the previous method for longer.\n\nIâ€™m not familiar with the literature of the calibration scores and cannot tell whether the improvement is significant or not. However, the test accuracy of DenseNet on CIFAR-100 is much lower than the number in Huang et al. (2016). And the new loss only achieves a significant improvement on DenseNet while the improvement is marginal on the other architectures. I wonder if the significant improvement is because of some implementation mistakes.\n\nOverall, the idea to use data-dependent weights is interesting and reasonable. However, some issues in terms of both methodology and experiments need to be clarified.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}