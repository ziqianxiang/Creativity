{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting but somewhat limited empirical result.",
            "review": "This paper looks at the brittleness of object detectors to generic texture masks, which are optimized to break object detection in any image. This is a white-box attack, although they also demonstrate transferrability to another object detector.\n\nThe attacks work by adding surface-level artifacts to the image. These could be removed by applying a Gaussian blur to the entire image, although it could also make the image harder to comprehend. How well does Gaussian blur work as a defense?\n\nStrengths:\n- First (I think) work on generic white-box attacks against object detectors\n- Interesting (and aesthetically pleasing) textures resulting from the attacks\n\nWeaknesses:\n- Evaluation was limited to one dataset and two object detectors. These attacks may or may not generalize to other settings.\n- No evaluation of defenses, such as adversarial training or applying blur\n\nMinor comments:\n- Does this attack represent a security risk in any plausible real-world setting?\n- The discussion of monochromatization mentions that it can be used in combination with piling up, but the experiments don't explain if it was ever used without piling up or not.\n- \"Piling\" usually implies overlap. I would suggest the term \"tiled\" instead.\n- Typo or grammar error: \"how the objectiveness is influenced in this setting\" \n- \"A is a randomly composed data augmentation scheme\". What was used for A in the experiments? Was it truly random, or was it optimized as well?  Could it be optimized? \n- Figure 3 has very small fonts and relies on color (bad for printing in B&W, possibly bad for colorblind readers depending on palette)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "incremental",
            "review": "This paper presents a method for hiding objects from an object detector (e.g. Fast R-CNN or YOLO). This differs from prior work, which mostly focuses on object recognition. They investigate two versions of this idea: they estimate a full-size mask, and they estimate a mask that concatenates together smaller patches (an easier optimization problem with fewer parameters).\n\n- The idea of studying adversarial examples for object detectors is interesting and worthwhile.\n\n- As the paper acknowledges, the methods they use are quite similar to prior work (such as Brown et al.). The contributions (e.g. the \"piled up\" adversarial mask and monochrome masks) are fairly straightforward extensions of prior methods - in this case, applied to detection rather than recognition.  \n\n- The experimental results are not very surprising, given their similarity previous work. The experiments are also not particularly extensive, so there is not much insight to be gained from them (e.g. as an experimental paper).\n\n- The writing is not very clear. The description of the method and experiments are verbose and confusing.\n\nOverall: This is a worthwhile problem, and it is good to see a paper that addresses it. However, I do not think that the paper should be accepted. The writing is confusing, the approach is not very novel, and the experiments are fairly minimal.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "simple method that lacks clear motivation and validations",
            "review": "This paper is aimed at creating universal adversaries to fool object detectors by learning an image mask using two simple tricks:  piling-up mask patches and monochromatization. Some extensions are also discussed, such as using multiple PR models or partially converged PR models, though without clear motivation.\n\n\nOverall the method used is quite simple and is not sufficiently validated either conceptually or empirically. It remains unclear how to evaluate the proposed method besides plotting out the learned mask. How such adversaries can be useful in improving the performance of deep object detectors is unclear.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}