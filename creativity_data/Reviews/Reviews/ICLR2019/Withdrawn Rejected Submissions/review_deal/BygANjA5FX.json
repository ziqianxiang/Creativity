{
    "Decision": {
        "metareview": "The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming \"inner ensembles\".\n\nReviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method.\n\nThe AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.  The AC also concurs that a full ensemble baseline would strengthen the paper's claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting method with limited novelty and requiring better baselines."
    },
    "Reviews": [
        {
            "title": "A number of missing comparisons, needs stronger empirical results",
            "review": "IEA proposes to use multiple \"parallel\" convolution groups, which are then averaged to improve performance.\n\nThis fundamental idea of ensembles combined with simple functions has been explored in detail in Maxout (Goodfellow et.  al., https://arxiv.org/abs/1302.4389) in the context of learning activation functions, and greater integration with dropout regularization.\n\nUnder the lens of comparison to Maxout (which should be cited, and is a key comparison point for this work), a number of questions emerge. Does IEA also work for feedforward layers? Does IEA give any performance improvement or have some fundamental synergy with the regularizers used here? Is the performance boost greater than simply using an ensemble of m networks directly (resulting in the equivalent number of parameters overall)? The choice of the mean here seems insufficient for creating the types of complexity in activation which are normally desirable for neural networks, so some description of why a simple mean is a good choice would be beneficial since many, many other functions are possible.\n\nCrucially Maxout seems much too close to this work, and I would like to see an indepth comparison (since it appears to be use of mean() instead of max() is the primary difference). I would also significantly reduce the claims of novelty, such as \"We introduce the usage of such methods, specifically ensemble average inside Convolutional Neural Networks (CNNs) architectures.\" in the abstract, given that this is the exact idea explored in other work including followups to Maxout.\n\nFor example, MNIST performance here matches Maxout (.45% for both, but Maxout uses techniques known in 2013). CIFAR-10 results are better, but again Maxout first appeared 5 years ago. There are more recent followups that continued on the line of work first shown in Maxout, and there should be some greater comparison and literature review on these papers. The CIFAR-10 baseline numbers are not ideal, and since IEA is basically \"plug and play\" in existing architectures, starting from one of these settings instead (such as Wide ResNet https://arxiv.org/abs/1605.07146) and showing a boost would be a stronger indication that this method actually improves results. In addition, there are a number of non-image settings where CNNs are used (text or audio), and showing this idea works on multiple domains would also be good.\n\nThere seems to be a similarity between ResNet and this method - specifically assuming the residual pathway is convolution with an identity activation, the summation that combines the two pathways bears a similarity to IEA. With multiple combined paths (as in Dense ResNet) this equivalence seems stronger still. A discussion of this comparison in greater detail, or even derivation of IEA as a special setting or extension of ResNet (coupled with stronger performance on the datasets) would help ground the work in prior publication.\n\nThe section on visualization and inspection of IEA features seems interesting, but too brief. A greater exploration of this, and possible reduction or removal of the ensemble selection section (which didn't have a clear contribution to the message of the paper, in my opinion) would strengthen the work - and again, comparisons to activations learned by Maxout and followups would make this inspection much stronger.\n\nMy key concerns here are on relation to past work, greater comparison to closely related methods, and improvement of baselines results. Given the close similarity of this work to Maxout and others, a much stronger indication of the benefits and improvements of IEA seems necessary to prove out the concepts here.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Issues of clarity and comparison",
            "review": "This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise.\n\nThere are a few issues that undermine the conclusion that this simple method is an improvement over full-model ensembles:\n\t1. Equation (1) is unclear on the definition of C_layer, a critical detail. In the context, C_layer could be weights, activations before the nonlinearity/pooling/batch-norm, or activations after the nonlinearity/pooling/batch-norm. Averaging only makes sense after some form of non-linearity, otherwise the “ensemble” is merely a linear operation, so hopefully it’s the latter.\n\t2. The headings in the results tables could be clarified. To be sure that I am understanding them correctly, I’ll propose a new notation here. Please note in the comments if I’ve misunderstood! Since “m” is used to represent the number of convolutional layer replications, let’s use “k” to represent the number of full model replications. So, instead of “CNL” and “IEA (ours)” in Table 1 and “Ensemble of models using CNL” and “Ensemble of models using IEA (ours)” in Table 2, I would recommend a single table with these headings: “(m=1, k=1)”,  “(m=3, k=1)”,  “(m=1, k=3)”,  and “(m=3, k=3)”, corresponding to the columns in Tables 1 and 2 in order. Likewise for Tables 3-6.\n\t3. Under this interpretation of the tables---again, correct me if I’m wrong---the proper comparison would be “IEA (ours)” versus “Ensemble of models using CNL”, or  “(m=3, k=1)” versus “(m=1, k=3)” in my notation. This pair share a similar amount of computation and a similar number of parameters. (The k=3 model would be slightly larger on account of any fully-connected layers.) In this case, the “outer ensemble” wins handily in 4 of 5 cases for CIFAR-10.\n\t4. The CNL results, or “(k=1,m=1)”, seem to not be state-of-the-art, adding more uncertainty to the evaluation. See, for instance, https://www.github.com/kuangliu/pytorch-cifar. Apologies that this isn’t a published table. A quick scan of the DenseNets paper and another didn’t yield a matching set of models. In any case, the lack of data augmentation may account for this disparity, but can easily be remedied.\n\nGiven the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I can’t recommend acceptance at this time. \n\nOther notes:\n\t* The wrong LaTeX citation function is used, yielding the “author (year)” form (produced by \\citet), instead of “(author, year)” (produced by \\citep), which seems to be intended. It’s possible that \\cite defaults to \\citet.\n\t* The acronyms CNL and FCL hurt the readability a bit. Since there is ample space available, spelling out “convolutional layer” and “fully-connected layer” would be preferred.\n\t* Other additions to the evaluation could or should include: a plot of test error vs. number of parameters/FLOPS/inference time; additional challenging datasets including CIFAR-100, SVHN, and ImageNet; and consideration of other ways to use additional parameters or computation, such as increased depth or width (perhaps the various depths of ResNet would be useful here).\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper describes a new op: Inner Average Ensemble (IAE). This op is constructed by $m$ convolution ops followed by an averaging op. The author claims using this IAE op is able to improve CNN classification performance. The experiments include MNIST and CIFAR-10 with a few network structures. \n\nFirst of all, this new proposed op is not efficient. Replace a traditional conv layer with one IAE layer it will introduce $m$ times more parameters, while, the performance gain from the authors’ experiment is relatively small, which indicates, most learning capacity is wasted.\n\nSecondly, only MNIST and CIFAR-10 is not convincing that this structure change will be widely useful.\n\nThirdly, this work is not practical to apply on real tasks, because it introduced $m - 1$ times more computation. \n\nOverall, I am not convinced this structure change meets innovation standard. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}