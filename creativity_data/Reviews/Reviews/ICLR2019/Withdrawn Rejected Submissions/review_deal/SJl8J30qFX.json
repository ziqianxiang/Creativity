{
    "Decision": {
        "metareview": "This paper introduces a distillation approach for black-box classifiers that trains generalized additive models (GAM), an additive model over feature shapes, thus providing global explanations for the model. Given the importance of interpretability, the reviewers appreciated the focus of this work. The reviewers also found the experiments, both on real and synthetic datasets, extremely thorough and were impressed by the results. Finally, they also mentioned that the paper was clearly well-written.\n\nThe reviewers and AC note the following potential weaknesses: \n(1) The primary concern, raised by all of the reviewers, is the lack of novelty;the proposed approach is a straightforward application of GAMs to model distillation, where black box output is the training data of the GAM, (2) The reviewers are also concern that the proposed approach is limited in scope to tabular datasets, and would not work for more interesting, complex domains like text or images, and (3) The reviewers are concerned that the interpretability of GAMS is assumed, without describing the limitations, for example, if there are correlated features, the shapes would affect each other in uninterpretable ways. Amongst other concerns, the reviewers were concerned about the formatting of the plots and tables in the paper, which made it difficult to read them, and the lack of a user study to verify the interpretability claims.\n\nIn response to these criticisms, the authors provided comments and a substantial revision to the papers, heavily restructuring the paper to fit extra experiments (comparison to other global explanation techniques, including a user study) and make the figures and tables readable. While the paper was much improved by these changes, and two of the reviewers increased their scores accordingly, concerns about the limited novelty and scope still remained.\n\nUltimately, the reviewers did not reach a conclusion, but the concerns of novelty and scope overwhelmed the clear benefits of the approach and the strong results. This paper was very close to getting accepted, and we strongly urge the authors to submit it to other premier ML conferences.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited scope and novelty"
    },
    "Reviews": [
        {
            "title": "well written with thorough experiments, but limited novelty and scope",
            "review": "Summary:\nThis paper incorporates Generalized Additive Models (GAMs) with model distillation to provide global explanations of neural nets (fully-connected nets as black-box in the paper). It is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of black-box models. However, it lacks novelty and is limited to tabular data as presented.\n\nPros:\n- The paper is well written.\n- The experiments are detailed and thorough with both synthetic and real data.\n\nCons:\n- The novelty is limited. The core consists of GAMs well studied in the literature, e.g. Caruana et al 2015. Admittedly, this work also tries to incorporate model distillation to explain black-box models globally. The concept of student models approximating teacher models is not new either. The originality seems incremental in both directions.\n- The scope is limited. The paper only presents applications in tabular data. Also, it would be better to experiment with black-box models beside simple fully-connected nets.\n- The interpretability is not convincing. It is not sufficient to demonstrate the interpretability of the proposed method, or the expressive advantage of feature shapes. It is encouraged to include studies with human subjective to compare against other existing interpretable approaches.\n\nSpecifics:\n- With Figure 3, it is not convincing that the student model actually explains the teacher model, so the paper tries to elaborate more with Table 1. I think Table 1 also needs more details to help, such as the significance of error difference and '-' elements.\n- Many figures are hard to read mostly because of font, color, and overlap.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well written paper but method lacks novelty",
            "review": "Summary: This paper makes an interesting contribution of providing global explanations of black box models (such as neural nets) using a special class of models called generalized additive models.  While the paper is well written and experiments are quite detailed, I have some problems with some of the basic premises of this work. \n1. The concept of using simpler models to approximate other complex models (model distillation) is not new and has been explored quite a bit already in ML literature. The only new proposition of this work is to use generalized additive models to approximate other complex models. This seems rather incremental. \n2. The premise behind using generalized additive models (GAMs) to explain other complex models is that GAMs are interpretable. I am not convinced about this premise. While I can intuitively see that GAMs might be able to better approximate complex models compared to rules and trees, I highly doubt if they are even interpretable. \n\nPros:\n1. The paper is well written\n2. Experiments are very detailed and thorough\n\nCons:\n1. The proposed approach lacks novelty\n2. Experimentation lacks a user study which helps understand if and when GAMs are at least as interpretable as rule-based approaches.\n\nDetailed Comments: \nI actually like the way this paper is written and executed. The writing is very clear and experiments are quite thorough. But, as discussed earlier, I have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that GAMs are interpretable. I would encourage the authors to discuss these two aspects in their rebuttal. I would strongly encourage the authors to carry out at least a simple user study which compares the interpretability aspect of GAMs with rule-based or prototype based approaches. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "good paper",
            "review": "This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense.\n\nGood things:\n- It is a very elegant method. It is also very simple (in a good way).\n- The paper is really well written.\n- The experiments are carefully conducted and are indeed showing what the authors describe.\n- I think the method is potentially of practical use.\n\nProblems:\n- I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though.\n- The font is too small in many figures. It is impossible to read it. \n- I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network.\n- Only makes sense to apply this to fully connected networks.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}