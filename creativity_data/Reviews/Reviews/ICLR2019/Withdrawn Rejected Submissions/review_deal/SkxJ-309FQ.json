{
    "Decision": {
        "metareview": "Strengths\n\n-  Hallucinations are a problem for seq2seq models, esp trained on small datasets\n\nWeankesses\n\n- Hallucinations are known to exists, the analyses / observations are not very novel \n\n- The considered space of hallucinations source (i.e. added noise) is fairly limited, it is not clear that these are the most natural sources of hallucination and not clear if the methods defined to combat these types would generalize to other types. E.g., I'd rather see hallucinations appearing when running NMT on some natural (albeit noisy) corpus, rather than defining the noise model manually.\n\n-  The proposed approach is not particularly interesting, and may not be general. Alternative techniques (e.g., modeling coverage) have been proposed in the past. \n\n-  A wider variety of language pairs, amounts of data, etc needed to validate the methods. This is an empirical paper, I would expect higher quality of evaluation.\n\nTwo reviewers argued that the baseline system is somewhat weak and the method is not very exciting. \n\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "sufficiently solid but not particularly exciting"
    },
    "Reviews": [
        {
            "title": "interesting analysis",
            "review": "I think this paper conducts several interesting analysis about MT hallucinations and also proposes several different ways of reducing this effect. My questions are as follows:\n\n* I am very curious about how do you decide the chosen noisy words. I am also wondering what is the difference if you do choose different noisy words. Another thing, if the noisy words are unseen in the training set, will it be treated as \"UNK\"?\n* Can you highlight what is changed in the upper right side of fig.4? It would be great if you include gloss in the figure as well.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "about the models",
            "review": "\tMy major concern about the work is that the studied model is quite weak. \n\t\"All models we present are well trained with a BLEU score of at least 20.0 on the test set, a reasonable score for 2-layer models with 256 hidden units.\" \n\t\"We then used the WMT De!En 2016 test set (2,999 examples) to compute the hallucination percentage for each model.\"\n\tI checked the WMT official website http://matrix.statmt.org/matrix. It shows that the best result was a BLEU score of 40.2, which was obtained at 2016. The models used in this work are about 20.0, which are much less than the WMT results reported two years ago. Note that neural machine translation has made remarkable progress in recent two years, not to mention that production systems like Google translator perform much better than research systems. Therefore, the discoveries reported in this work are questionable. I strongly suggest the authors to conduct the studies base on the latest NMT architecture, i.e., Transformer.\n\t\n\tFurthermore, I checked the examples given in  introduction in Google translator and found no hallucination. So I'm not sure whether such hallucinations are really critical to today's NMT systems. I'd like to see that the study on some production translation systems, e.g., applying Algo 1 to Google translator and check its outputs, which can better motivate this work.\n\t\n\tFor the analysis in Section 6.1, if attention is the root cause of hallucinations, some existing methods should have already address this issue. Can you check whether the model trained by the following work still suffers from hallucinations?\nModeling Coverage for Neural Machine Translation, ACL 16.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Adversarial examples in NMT",
            "review": "The authors introduce hallucinations in NMT and propose some algorithms to avoid them. \nThe paper is clear (except section 6.2, which could have been more clearly described) and the work is original. \nThe paper points out hallucination problems in NMT which looks like adversarial examples in the paper \"Explaining and Harnessing Adversarial Examples\". So, the authors might want to compare the perturbed sources to the adversarial examples.\nIf analysis is provided for each hallucination patten, that would be better. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}