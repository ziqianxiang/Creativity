{
    "Decision": {
        "metareview": "This manuscript proposes a gradient-based learning scheme for non-differentiable and non-decomposable metrics. The key idea is to optimize a soft predictor directly (instead of aiming for a deterministic predictor), which results in a differentiable loss for many of these metrics. Theoretical results are provided which describe the performance of this approach.\n\nThe reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and novelty as related to already published work. There was also a concern about the usefulness the main theoretical results due to asymptotic assumptions. The manuscript would be significantly strengthened if the reliance on infinite sample sizes is resolved, or sufficient empirical evidence is provided which suggests that the asymptotic issues are not practically significant.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "An interesting paper with some issues ",
            "review": "This paper proposes a gradient-based learning for F1 measure under the utility maximization framework. F1 is a widely used evaluation metric in information retrieval and machine learning, and it is hard to optimize as it is non-decomposable and non-differentiable. This research direction is hence extremely interesting. \n\nThe paper is well organized and easy to follow. The general methodology seems sound. Below are some detailed comments.\n\n- Page 1, Section 2.1. The notation of the probabilistic classifier is not typed correctly.  \n\n- Page 7. The result strongly depends on how well Eq. (5) holds. Two critical assumptions regarding the data are made here, (1) D -> ∞ and (2) B -> ∞. The first assumption is implicitly confirmed in the experiments, as in Table 1 the proposed method outperforms when the sample size is big. I am a little bit puzzled about the second assumption though. Eq. (7) holds, (and consequently Eq. (5)) when B -> ∞, but it cannot be the case in practice, since B tends to have moderate sizes. I wonder how this impacts the results. Batch size isn't discussed at all in the experiments. The discussion on noise control is nice, but it doesn't contribute to the validation of Eq. (5) or Eq. (7).\n\n- Algorithm 1 & 2. It may be a good idea to be explicit what the outputs of the algorithms are. The algorithms are referenced by their section numbers instead of their algorithm numbers. \n\n- The experimental section can be extended. The paper has extensively discussed other well-behaved metrics and tasks beyond binary classification. None of these are tested empirically. \n\n- If I understand correctly, the GS method, with a much higher computational cost, is near optimal. If so, its results should serve as an empirical upper-bound for F1. Then how come the proposed method outperforms it on 4 over 6 dataset? \n\n- There are additional references on F1 maximization. To name a few: (1) Chai. Expectation of F-measures. SIGIR 2005. (2) Waegeman et al. On the Bayes-Optimality of F-Measure Maximizers. JMLR.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Needs more theoretical or experimental support.",
            "review": "Update: I still feel that the paper should have either strong theory, strong experiments, or some of each to be accepted, but that both are lacking. The revisions required would be too great for acceptance at this time. \n\nOriginal review:\nThe paper proposes a general method to optimize for performance metrics which can be written in terms of the entries of the confusion matrix. The idea is to approximate the entries of the confusion matrix using their expected values for a randomized classifier, plug these estimates into the formula for the desired metric, and optimize that quantity. This is a compelling idea but it needs more support than the theoretical or experimental sections give. \n\nThe simplicity and generality of the method are appealing. Smooth surrogates derived from randomized classifiers have been considered in the context of accuracy [1] and other performance measures [2, 3] and the paper should include some discussion of this prior work, but to my knowledge the broad applicability to non-decomposable and non-differentiable metrics expressible in terms of the confusion matrix is new. \n\nThe theoretical sections could use some improvement. It is worth mentioning that the loss obtained with the proposed method is nonconvex. The first equation in theorem 1 is described with “... where convergence in probability is entry-wise”, when the equation refers to almost sure convergence for a scalar, not convergence in probability for entries of a matrix. \n\nNo convergence rates are given, only asymptotic almost sure convergence as the size of the dataset or the minibatch goes to infinity. For finite datasets these statements are obvious, and while convergence is reassuring for infinite datasets, I imagine the rates will look very different for the loss (a scalar) and the gradient (which may have millions of coordinates). Theorem 3 considers the generalization of a single classifier which is independent of the empirical sample, which makes it irrelevant to cases where the model is learned. Theorem 4, which seeks to give a uniform bound over the model class, only shows that generalization occurs in the limit of infinitely much data (which is not surprising or particularly interesting).\n\nThe experimental section compares the algorithm against a well-known and strong baseline, but without any information about the variance of the results and only for a deep network. Several questions remain: Where the proposed method improves over the baseline, is this improvement due to the new method or the interaction between the method and the model? How would the method perform on e.g. a linear model, which is better understood? How do the results depend on batch size, which affects the bias in the gradients? \n\n[1] Roux, Nicolas Le. \"Tighter bounds lead to improved classifiers.\" arXiv preprint arXiv:1606.09202 (2016).\n[2] Mozer, Michael C., et al. \"Prodding the ROC curve: Constrained optimization of classifier performance.\" Advances in Neural Information Processing Systems. 2002.\n[3] Goh, Gabriel, et al. \"Satisfying real-world goals with dataset constraints.\" Advances in Neural Information Processing Systems. 2016.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Marginally below acceptance",
            "review": "This paper studies the problem of optimizing non-decomposable metric in classification. This topic has been discussed in several recent works mainly under deterministic classifier context, the authors discuss the possibility of training a neural network and learn the model by gradient-based methods, which could result in randomized classifier; and conducted experiments to compare the performance with other existing methodologies. I have the following concerns after reading it.\n\n1.The main idea of the paper has shown in other related works and the authors didn’t convince me why their work solves something that could not be solved in existing work. The related work section missed some relevant recent work including Ref[1], in which the method is also gradient-based and can be applied to neural networks. The well-behaved notion used in Definition 2 seems much weaker than the assumptions shown in Ref[1,2] to guarantee existence or uniqueness of the Bayes classifier, the authors could spend some effort to discuss why they require less assumptions.\n\n2.For the theory part, all the convergence results are proved in an asymptotic way without further discussion in the sample complexity. This becomes problematic for this work because (as shown in eq (7)) mini batch size goes to infinity is an unrealistic assumption in neural network training. Also when the class is unbalanced, empirical mean converging to population also slows down significantly which is required in Eq (4) and other places. I would like to see more discussion on the sample complexity either theoretically or experimentally.\n\n3.The experiments lack details for reproducing the results or generalizing the gain to other problems. For example, batch size, learning rate or how the size of the network influence the performance metrics. This information will be useful for others who want to apply the proposed method.\n \nThere are some minor formatting issues like the leading space in \\citep. Please fix those.\n\nBased on the above reasons, I’ll give this paper a 5.\n\n[Ref 1] Yan, B., Koyejo, S., Zhong, K. & Ravikumar, P.. (2018). Binary Classification with Karmic, Threshold-Quasi-Concave Metrics. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:5531-5540\n[Ref 2] Narasimhan, H., Kar, P., & Jain, P. (2015, June). Optimizing non-decomposable performance measures: a tale of two classes. In International Conference on Machine Learning (pp. 199-208).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}