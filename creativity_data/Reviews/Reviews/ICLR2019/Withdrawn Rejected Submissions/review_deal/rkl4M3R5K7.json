{
    "Decision": {
        "metareview": "Four reviewers have evaluated this paper. The reviewers have raised concerns about the specific formulation used for adversarial example generation which requires further clarity in motivation and interpretation. The reviewers have also made the point that the experimental evaluation is against previous work which tried to solve a different problem (black box based attack) and hence the conclusions are unconvincing.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Unclear motivation and significance of empirical results"
    },
    "Reviews": [
        {
            "title": "Questions about minmax game definition",
            "review": "The paper studies the problem of adversarial examples generation. The authors phrase the following problem: given a set of models  C, we want to find an adversarial perturbation that maximizes the loss on an ensemble of models. However, the ensemble weights are chosen by the learner. In the case that we have one example, this is equivalent to asking that the same adversarial perturbation (or distribution over perturbations) fools all the individual models in my collection. This is a reasonable phrasing of the problem, though it seems different from versions studied in literature. In particular, previous works used uniform ensembles.\nMore generally, the authors consider a set of m examples, and the adversarial player now looks for a (distribution over) perturbations for each of the n examples. The learner player selects mixing weights to minimize the error rate. This is an interesting formulation of the problem: in particular, tying the mixing weights used for all examples is a non-intuitive change and does not have the clean interpretation above any more.\nThis notion of allowing mixing weights on the learner is a change from previous work. The authors would do well to explain why this formulation is chosen and what the interpretation is. It corresponds to a specific attack model where the learner and the adversary make choices in a very specific order, and could use further explanation on when this a reasonable attack model. Note that previous work looked at the setting of all weights being equal, and one natural variant is to allow a set of mixing weights per example, which would correspond to finding a perturbation (or distribution over perturbations) for this example that fool all models in the set C. The version studied here is left unexplained in the current work.\n\nThe authors then argue that we can solve this game by playing MW vs. best response. They propose using best response on the adversarial player. This player is then trying to find the perturbation that maximizes the p-weighted sum of the 0-1 (or rather surrogate) losses, where p represents the mixing weights on C. The authors show that in the convex case, if there is a pure NE, then the best response can be found: in this case we get a convex problem. They study the convex case a bit more, showing that there is at most an exponential number of values for the 0-1 loss, since a {0,1} vector defining which side of each classifier in C x falls in fully defines the loss at x. \n\nFinally, the authors move to the non-convex case where the experiments are done. The authors report interesting results on imagenet and for mnist for the convex case. I had some trouble understanding the imagenet results. For one, it seems fishy that their Madry et al. attack is worse than the FGM for many of the models and suggests strongly that the parameters for the Madry attack were not properly tuned. It is hard to know since the paper does not report on various parameters for these attacks. Second, these attacks are designed for l_infty and modifying them for l_2 would be necessary for a fair comparison. Finally, I am not sure why the authors do not compare to the Carlini and Wagner attacks on Imagenet, which is actually an l2 attack and makes the accuracy 0 at a slightly larger perturbation radius. Also, the authors would do well to emphasize that for larger perturbation radii, there are attacks which make the accuracy zero, and the contribution here seems to be look at smaller radii.\n\nMy primary concern with the work is that it is not clear to me how the specific two player game is motivated. The authors do not justify why it makes sense to allow weights on the ensemble, and also why these weights need to be tied together across examples. For a paper that makes strong claims about its approach being principled, this is a serious shortcoming in my view. Secondarily, the experiments section leaves me worried that the comparison is with improperly tuned versions of previous work. I would therefore not be in favor of accepting this paper.\n\nComments:\npg 1 : \"One of the most pressing ...\" : that is perhaps an unnecessary exaggeration.\npg 2: The name \"NSFW\" is an unfortunate choice, is completely non-informative about the contribution and I strongly recommend the authors reconsider it.\n- As far as I can tell, Tramer et al. do not build an ensemble model at all; the ensemble word there refers to an ensemble of adversarial perturbations.\n- The hinge loss is actually not smooth. However, I don't quite see why you need smoothness there.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and well-written paper but insufficient motivation",
            "review": "Summary: The authors provide a method to attack multiple classifiers, with the key insight that it is insufficient to attack a simple average of the multiple classifier outputs; creating adversarial examples which can fool each classifier independently leads to more success in attacking any defenses that has access to multiple classifiers. Note that white-box access to all the classifiers is assumed. \n\nClarity: Paper is well written and claims are clear and substantiated. \n\nOriginaility: The paper's technical contribution seems limited. They suggest performing PGD to estimate the best response, which is similar to previous work. However, the authors do multiple rounds of this, with different weights on the multiple classifiers at each step. \n\nConcerns: \n(1) Ensembles have mostly been proposed for black-box attacks. The setting where there are multiple classifiers and all of these weights are accessible to the attacker seems unrealistic. What's the advantage for a defense to commit to a set of trained classifiers before hand? \n\n(2) Security concerns aside; it is not surprising that it's possible to find an attack that works for multiple classifiers at the same time, and I believe this has been done in prior work. The theoretical contribution is limited and the technique proposed is just a small modification of existing gradient based algorithms.\n\n(3) The experimental evaluation is against previous work which tried to solve a different problem (black box based attacks).  Hence, they are not convincing. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-motivated approach to an interesting problem",
            "review": "This paper is concerned with the problem of finding adversarial examples for an ensemble of classifiers. This is formulated as the task of finding noise vectors that can be added to a set of examples in such a way that, for each example, the best ensemble element performs as badly as possible (i.e. it’s a maximin problem).\n\nThis is formulated as a two-player game (Equation 1), in which the above description has been relaxed slightly: Equation 1 seeks a *distribution* over noise vectors, instead of only one. This linearizes the game, so that we can seek a mixed Nash equilibrium. Given access to a best response oracle, Algorithm 1 results in such a mixed Nash equilibrium. This is pretty standard stuff (see e.g. “Robust Optimization for Non-Convex Objectives” in NIPS’17, or “A Reductions Approach to Fair Classification” in ICML’18), but the application of this approach to this problem is novel and interesting.\n\nIn Section 2.1, the authors seek to show that they can get provable guarantees for *linear* classifiers, provided that there exists a “pure strategy Nash equilibrium”, which is a set of noise vectors for which *every* classifier misclassifies *every* example. These conditions seem to me to be so strong that I’m not sure that this section is really pulling its weight.\n\nOn the subject of Section 2.1, the authors might consider whether an analysis based on “Two-Player Games for Efficient Non-Convex Constrained Optimization” (on arXiv) could be used here: convert Equation 1 into a constrained optimization problem by adding a slack variable, then reformulate it as a non-zero-sum game, in which one player uses the zero-one loss, and the other uses e.g. the hinge loss.\n\nWhile Algorithm 1 makes an unrealistic oracle assumptions, and I didn’t find Section 2.1 fully satisfying, I think that overall the theoretical portion of the paper is sufficiently convincing that one should be surprised if their experiments don’t show good performance (which they do--extremely good performance, in fact). Overall, this is an interesting and important problem, and a well-motivated approach that seems to work well in practice. I think Section 2.1 is a bit weak, but this is a relatively minor issue.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Application to multiplicative weight update for constructing adversarial noise",
            "review": "Summary: \nThe paper considers finding the most adversarial random noise given multiple classifiers. They formulate the problem as the standard min-max game and apply the multiplicative weight updates. The technical contribution is to clarify the computational complexity of implementing/approximating the response oracle. The authors show experimental results.\n\nComments: \nI am afraid that the main technical result is already known:\n\nYoav Freund Robert E. Schapire: Adaptive game playing using multiplicative weights, Games and Economic Behavior, 29:79-103, 1999.\n\nThe paper shows that a multiplicative update algorithm can approximately solve the min-max game. If you use the result, you can readily obtain the main results of the present paper. \n\nAfter rebuttal:\nI read the authors comments and I understood the technical contribution more and raised my score.  Implementing/appriximating the response oracle is non-trivial. For MWU, I still think that the above paper should be cited (citing the Adaboost paper is not enough) since the paper shows MWU solves the min-max game.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}