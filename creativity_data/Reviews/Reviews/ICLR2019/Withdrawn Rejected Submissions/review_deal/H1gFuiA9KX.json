{
    "Decision": {
        "metareview": "although the proposed method could be considered an interesting application to recently popular hypobolic space to word embeddings, it is unclear why this needs to be done so. experiments also do not support why or whether the application of hyperbolic space to word embedding is necessary.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "motivation is unclear"
    },
    "Reviews": [
        {
            "title": "skip-gram + Minkowski distance -> unclear why it should work",
            "review": "The present paper aims to apply recent developments in hyperbolic embeddings of graphs to the area of word embeddings.\n\nThe paper is relatively clearly written and looks technically correct. The main contribution of the paper is in suggesting the usage of Minkowski dot-product instead of Eucledian dot-product in skip-gram model and derivation of corresponding weight update formulas taking into account the peculiarities of hyperbolic space. The suggested aproach is realtively simple, though requiring adding additional bias parameter, which doesn't look entirely natural for the problem considered. I should note, that all the update equations are relatively straitforward given the results of recent papers on hyperbolic embeddings for graphs. Experimental results show some mild to no improvement over classical skip gram model in word similarity and word analogy problems.\n\nMy main concern about the paper is that it is not entirely clear throughout the text why the proposed model should be better than any of the baselines. Currently it lookss like the paper merging 2 ideas without any clear expalanation why they should work well together. I believe that the proposed approach (or similar one) might be useful for practice of natural language processing, but to asses that one would need to base on clear motivation and support this motivation with some examples showing that hyperbolicity indeed helps to capture semantics better (like famous world analogy examples for word2vec).\n\nPros:\n- clearly written\n- technically correct\nCons:\n- technically straightforward\n- not convincing experiments\n- unclear, why the approach should work",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good theoretical contribution but lack of motivation ",
            "review": "The paper proposes an algorithm that learns word embeddings in hyperbolic space. It adopts the Skip-Gram objective from Word2Vec on the hyperboloid model and derives the update equations for gradients accordingly. \nThe authors also propose to compute the word analogy by parallel transport along geodesics on the hyperboloid.\n\nStrength: The paper is well written, both the background geometry and the derived update method are clearly explained. The novelty and theoretical contribution are adequate. \n\nWeakness: My main concern is the lack of motivation for embedding words on the hyperboloid and the choice of evaluation metrics. For Poincare embeddings, the disc area and circle length grow exponentially with their radius, and the distances on the Poincare disk/ball reflect well the hierarchical structure of symbolic data, which make it natural to embed a graph in this space and lead to great evaluation results. The geometric property of the hyperboloid model, however, does not seem to in favor of encoding non-hierarchical semantics of words and the evaluation on word similarity/analogy tasks. The evaluation results in Table 1 and Table 2 show that the hyperbolic embeddings only performs better than the Euclidean embeddings in low dimensions but worse on higher dimensions (>50), while higher dimension embeddings generally encode more semantics and thus are used in downstream tasks. It will be great if the authors could elaborate on the advantages of learning word embeddings in hyperbolic space and evaluate accordingly. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "word2vec style embeddings in hyperbolic space",
            "review": "This paper presents a technique for embedding words in hyperbolic space, which extends previous non-euclidean methods to non-structured data like free text. The authors provide a new gradient based method for creating the embeddings and then evaluate them on standard word embedding benchmarks. Overall the paper is very well written and well executed. They find that in the low dimensions the approach outperforms standard Euclidean space methods while in higher dimensions this advantage disappears.\n\nThe results do not try to claim state of the art on all benchmarks, which I find refreshing and I appreciate the authors candor in giving an honest presentation of their results. Overall, I enjoyed this paper and am eager to see how the authors develop the approach further. \n\nHowever, along these same lines it would be great to have the authors provide more discussion about the next steps and potential applications for this approach. Is the interest here purely methodological? Are there potential use cases where they believe this approach might be superior to Euclidean approaches? More detail in the discussion and intro about the trajectory of this work would help the reader understand the methodological and application-specific implications. \n\nPros:\n- Clearly written and results are presented in a straightforward manner. \n- Extension of analogy reasoning to non-euclidean spaces.\n\nCons:\n- Lack of clear motivation and compelling use case. \n- It would be nice to have a visualization of the approach in 2-dimensions. While Figure 3 is instructive for how analogies work in this space, it would be great to visualize an entire dataset. I'm sure that the proposed embeddings would result in  a very different space than euclidean embeddings (as the Poincare embedding paper showed), so it would be great to have at least one visualization of an embedded dataset. Presumably this would play to the strengths of the approach as it excels in lower dimensions. \n-  The largest of embedding dimension tested was 100, and it is common to use much larger embeddings of 500-d. Do the trends they observe continue to larger dimensions, e.g. is the performance gap even larger in higher dimensions? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}