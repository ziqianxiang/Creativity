{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting model, but results are not convincing",
            "review": "This paper proposes an unsupervised model for grammar induction by drawing an analogy between a tree-like anto-encoder and the computation from the inside-outside algorithm. The claimed novelty of the proposed model are two-fold: (1) it can extract syntactic information with any supervision or downstream task; (2) it can build representations for internal constituents that obey syntactic and semantic regularities. Although this is an interesting work, the experimental results are not convincing enough to support the claims, especially the second one.\n\nBefore giving my comments on the experiments, here are the ones for the technical content\n\n- It was a little surprise to me that the inside vectors in the training objective are only from word level. In this case, the whole training signal seems to be biased to the outside part. Therefore, I am wondering whether the inside part (compat and compose functions) can get sufficiently trained.\n- It is interesting to see that this work directly employs the CKY parsing without any justification, since the way of computing scores in the proposed model is not \"context-free\". Besides, it is not clear to me what the last sentence in section 2.3 means?\n\n\nAbout experiments\n\n- In section 3.1, there are two depth columns in table 2, which are not discussed in this paper.\n- In section 3.2, is there any explanation about the mixed results? For example, why DIORA performs better then PRPN-UP on VP but worse on NP?\n- In section 3.3, (1) why choose the tasks of CoNLL 2000 and CoNLL 2012? (2) how to obtain phrase representations? From inside vectors, outside vectors or their mixtures? (3) results in table 4 seems not convincing about DIORA can capture some syntactic and semantic regularities. Or how should we understand the results?\n\nAdditional comments:\n\n- W_in in equation (1), S^in in equation (5) and S^out in equation (12) is left unexplained\n- Eq. 6 has a typo\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea and well-written paper. Missing evaluation on downstream applications.",
            "review": "This paper describes a neural latent tree model (DIORA) trained with an auto-encoding objective. The proposed model performs an inside-outside pass to construct vector representations for all possible tree nodes. Full constituency trees can be extracted from the model by doing a CKY pass with the internal node-pair scores.  It  achieves the state of the art on unsupervised constituency parsing. On the other tasks/datasets (unsupervised segmentation, phrase similarity), the model is either on par with or a bit worse than the previous best systems. \n\nStrengths:\n- The proposed model is quite interesting. Judging from the empirical results, it captures syntactic structure better than the other latent tree models.\n\n- The paper is well-written and easy to understand.\n\nWeaknesses:\n- It would be nice to see at least one task that involves the application of the DIORA model. \n\nOther comments and questions below:\n- Table 1 shows that DIORA is trained on sentences with a maximum length of 20, while Section 4 says that the model is evaluated on the full WSJ test set. Is this setup affecting the performance? It would be nice to see some accuracy breakdown by sentence lengths.\n\n- Is it possible to compare to other unsupervised parsing/grammar induction models, despite the fact that they often evaluate on shorter sentences (e.g. 20 or 40 words)?\n\n- This paper reports parsing performance on the MultiNLI dataset against the automatic parser, is there a plan for getting the final accuracy on MultiNLI as well?\n\n- It would be nice to have more discussions in Section 3.4 on the qualitative examples. Moreover, is it possible to show examples where DIORA makes mistakes as well? \n\nIn 3.2.1, what are the possible reasons behind DIORA performing poorly on prepositional phrases?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper combines neural modeling with the inside-outside algorithm for unsupervised dependency parsing. The model is interesting and seems to be novel, but there is a serious lack of knowledge reagrding previous work on unsupervised parsing",
            "review": "This paper proposes a model for unsupervised dependency parsing (latent tree induction) that is based on a combination of the inside-outside algorithm with neural modeling (recursive auto-encoders). The paper is clearly written and the model seems interesting and, as far as I can tell, also novel.\n\nYet, the paper suffers from a major limitation that deems its rejection. Unfortunately, the authors seem to be totally unaware of previous work on unsupervised parsing. It may sound surprising but people have worked on this problem even before 2014 and there are some strong results that do not involve deep learning. I am sorry for the sarcasm, but it was really frustrating to see not only the lack of citation, but also the statement (already in the abstract and throughout the paper) that an F-score of 46.9 set a SOTA for unsupervised parsing with WSJ PennTreebank.\n\nParticularly, the authors ignore works of Cohen and Smith, Spitkovsy et al., Seginer and many others. A quick look at Spitkosky et al., 2013 (EMNLP) would reveal that the reported result is not SOTA (although Spitkovsky et al report results for sentences no longer than 40 words, but given the numbers they report for several models - an F1 score of 54 and more - the full WSJ number is likely to be higher than 46.9). But all the papers cited at Spitkovsky et al. 2013 are even not cited here. \n\nI would also like to refer the authors to work by Cohen and Collins (2012-2013) on latent variable PCFG, which presents a provably consistent parameter estimation for the problem. The presented techniques may also be an interesting point of comparison to this work and phrase representation may also be extracted from that algorithm.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}