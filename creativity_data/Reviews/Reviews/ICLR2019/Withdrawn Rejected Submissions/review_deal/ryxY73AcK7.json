{
    "Decision": {
        "metareview": "This paper presents an interesting and theoretically motivated approach to imposing Lipschitz constraints on functions learned by neural networks. R2 and R3 found the idea interesting, but R1 and R2 both point out several issues with the submitted version, including some problems with the proof--probably fixable--as well as a number of writing issues. The authors submitted a cleaned-up revised version, but upon checking revisions it appears the paper was almost completely re-written after the deadline. I do not think reviewers should be expected to comment a second time on such large changes, so I am okay with R1's decision to not review the updated version. Future reviewers of a more polished version of the paper will be in a better position to assess its merits in detail.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "interesting and potentially impactful idea but needs revisions"
    },
    "Reviews": [
        {
            "title": "Review of \"Universal Lipschitz Functions\"",
            "review": "This paper introduces GroupSort. The motivation is to find a good way to impose Lipschitz constraint to the learning of neural networks. An easy approach is \"atomic construction\", which imposes a norm constraint to the weight matrix of every network layer. Although it guarantees the network to be a Lipschitz function, not all Lipschitz functions are representable under this strong constraint. The authors point out that this is because the activation function of the network doesn't satisfy the so called Jacobian norm preserving property.\n\nThen the paper proposes the GroupSort activation which satisfies the Jacobian norm preserving property. With this activation, it shows that the network is not only Lipschitz, but is also a universal Lipschitz approximator. This is a very nice theoretical result. To my knowledge, it is the first algorithm for learning a universal Lipschitz function under the architecture of neural network. The Wasserstein distance estimation experiment confirms the theory. The GroupSort network has stronger representation power than the other networks with traditional activation functions.\n\nAdmittedly I didn't check the correctness of the proof, but the theoretical argument seems like making sense.\n\nDespite the strong theoretical result, it is a little disappointing to see that the GroupSort doesn't exhibit any significant advantage over traditional activation function on image classification and adversarial learning. This is not surprising though.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper but missing details and some formal polishing required",
            "review": "summary:\n\nA paper that states that a new activation function, which sorts coordinates in a vector by groups, is better than ReLU for the approximation of Lipschtiz functions.\n\npros:\n\n- interesting experiments\n- lots of different problems evaluated with the technique\n\ncons:\n\n- the GroupSort activation is justified from the angle of approximating Lipschitz transformations. While references are given why Lip is good for generalisation, I cannot see why GroupSort does not go *against* the ability of deep architectures to integrate the topology of inputs (see below).\n- the proof of Theorem 1 requires polishing (see below)\n- experiments require some polishing\n\ndetail:\n\n* The proof of Theorem 1 has three problems, first in the main file argument: since ReLU is not differentiable, you cannot use the partial derivative. Maybe a sub differential ? Second, in the RHS after the use of the Cauchy-Schwartz inequality (no equation numberingâ€¦) you claim that the product of all three norms larger than 1 implies *each* of the last two is 1. This is wrong: it tell nothing about the the value of each, only about the *product* of each, which then make the next two identities a sufficient *but not necessary* condition for this to happen and invalidates the last identity. Last, the Theorem uses a three lines appendix result (C) which is absolutely not understandable. Push this in the proof, make it clear.\n\nSection D.1 (proof of Theorem 2) the proof uses group size 2 over a vector of dimension 2. This, unless I am mistaken, is the only place where the group sort activation is used and so the only place where GroupSort can be formally advocated against ReLU. If so, what about just using ReLUs and a single group sort layer somewhere instead of all group sort ? Have the authors tried this experimentally ?\n\nIf I strictly follow Algorithm 1, then GroupSort is carried out by *partitioning* the [d] indexes in g groups of the same size. This looks quite arbitrary and for me is susceptible to impair the capacity of deep architectures to progressively integrate the topology of inputs to generalise well. Table 3 tends to display that this is indeed the case as FullSort does much worse than ReLU.\n\n* Table 5: replace accuracies by errors, to be consistent with other tables.\n\n* in the experiments, you do not always specify the number of groups (Table 4)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially interesting but unfinished work",
            "review": "The paper proposes a new \"sorting\" layer in neural networks that offers\nsome theoretical properties to be able to learn network which are 1-Lipschitz\nfunctions.\n\nThe paper contains what seems to be a nice contribution but the manuscript\nseems to have been written in a rush which makes it full of typos\nand very hard to read. This unfortunately really feels like unfinished work.\n\nJust to name a few:\n\n- Please check the use of \\citep and \\citet. See eg Szegedy ref on page 3.\n\n- Unfinished sentence \"In this work ...\" page 3.\n\n- \"]\" somewhere at the bottom of page 4.\n\n- \"Hence, neural network has cannot to lose Jacobian norm... \" ???\n\netc...\n\nAlthough I would like to offer here a comprehensive review I consider\nthat the authors have not done their job with this submission. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}