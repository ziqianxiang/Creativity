{
    "Decision": "",
    "Reviews": [
        {
            "title": "More work needs to be done before acceptance.",
            "review": "In this paper, the authors proposed a competitive, partial observable environment. The authors also use a distributed policy gradient algorithm, together with 4 techniques, to solve this environment. \n\nThe authors don't have any comparison to other algorithms so it's hard to tell whether it outperforms existing algorithms. More related work should also be included in this paper.\n\nFor writing quality, the authors' explanation of their environment is quite clear. However, the author's explanation to policy gradient (i.e., Eq.5 - Eq.9) is confusing. What is R? What is pi(s), as pi is the policy and only pi(a|s) is known? \n\nIn Algorithm 1, the authors use replay buffer to improve its sample efficiency and stability. However, the policy gradient requires the (state, action) pairs must be sampled using the current policy and the distribution of (s, a) in the replay buffer might differ from real distribution. It's very interesting to address this distribution mismatch problem but I didn't see any efforts the authors have put to solve this issue. Acceptance wouldn't be recommended if the authors don't make their points clear. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting environment, unclear writing",
            "review": "This paper proposed a game environment and a distributed deep policy gradient method to learn to play the game. The game is adversarial, partially observable and has multiple agents. The proposed learning algorithm is a modified version of REINFORCE, which supports training multiple agents simultaneously. \n\nWhile the proposed game is interesting and the algorithm is reasonable, the writing of the paper can be improved. Here are a few suggestions and questions:\n1) Is it possible to provide a link to a video that shows the game play and the learned policies. It is important to have such a video, because it is a straightforward way for readers to understand the rules of the game and the performance of the learned policy.\n2) The pseudo-code of Algorithm 1 is helpful but not clear enough. Is \\tau a trajectory of only one agent (a unit in the game) or a collection of trajectories for all the units? How does \"split\" working given a trajectory \\tau? What is \"s\" in the line \"Populate buffer ...\"? Is it (s_1, s_2, ... s_{agent})? What is the subscript \"agent\"? Does it stand for the number of agents (number of units)?\n3) Is state \"s\" or observation used in the training algorithm. From Algorithm 1, it is state. But since the paper talks a lot about partial observations, I guess that it is observation.\n4) What are the 5 actions in eq 3?\n6) I am confused about \"first-person observation\". Does the agent observe the entire map with fogs, which is shifted so that itself is at the center? Or does the agent only observe a 3x3 or 5x5 grid that surrounds itself?\n7) The symbol \"N\" is used for two meanings? Number of actions in eq 3 and number of agents in s_N in the first line of page 5.\n8) What does \"pre-training\" mean in Table 4.\n9) In Table 4, Does N enemies mean N enemy units or N enemy parties? Why are there only 4 enemies? According to Table 1, there should be 4 UGV's and 2 UAV's. So there should be 6 enemies, right?\n10) What does Map Code mean in Table 1?\n11) Will \"fog of war\" stay cleared once the grid cell has explored by a unit? Or it will become fog again once the unit leaves the cell.\n12) A common terminology of \"a policy transfer approach\" in the 2nd paragraph of page 6 is \"curriculum learning\". \n\nThe above points make it harder for me to understand the methods and the contributions of the paper. For this reason, I would not recommend accepting this paper at this time.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper lacks algorithmic innovation and does not give new theoretical or empirical insights in multi-agent learning.",
            "review": "Summary\n\nThe authors evaluate a standard policy gradient approach on a multi-agent Capture the Flag environment. The agents have partial observability and share rewards. The learning approach also use a curriculum learning approach. The authors report the policy behavior during training, but do not visualize or quantitatively analyze the learned agent behaviors.\n\nPro\n- A new multi-agent environment could be an interesting starting point for future research.\n\nCon\n- The authors do not present a significant new algorithmic or empierical innovation. \n- The authors do not demonstrate compelling empirical insights in the learned behavior of the agents. \n- The authors do not address any inherent difficulties with multi-agent learning (non-stationary environemnt, inducing cooperation, etc).",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}