{
    "Decision": {
        "metareview": "This manuscript presents a reinterpretation of hindsight experience replay which aims to avoid recomputing the reward function, and investigates Floyd-Warshall RL in the function approximation setting.\n\nThe paper was judged as relatively clear. The authors report a slight improvement in computational cost, which some reviewers called into question. However, all of the reviewers pointed out that the experimental evidence for the method's superiority is weak. Two reviewers additionally raised that this wasn't significantly different than the standard formulation of Hindsight Experience Replay, which doesn't require the computation of rewards for relabeled goals.\n\nUltimately, reviewers were in agreement that the novelty of the method and quality of the obtained results rendered the work insufficient for publication. The Area Chair concurs, and urges the authors to consider the reviewers' pointers to the existing literature in order to clarify their contribution for subsequent submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Important subject matter but novelty & results insufficient for acceptance."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper presents a reinterpretation of hindsight experience replay (HER) that avoids recomputing the reward function on resampled hindsight goals in favor of simply forcing the terminal state flag for goal-achieving transitions, referred to by the authors as a \"step loss\".\nThe new proposal is evaluated on two goal-conditioned tasks from low-dimensional observations, and show modest improvements over HER and a function-approximation version of Floyd-Warshall RL, mostly as measured against the number of times the reward function needs to be recomputed.\n\nPros:\n- minor improvement in computational cost\n- investigation of classical FWRL technique in context of deep RL\n\nCons:\n- computational improvement seems very minor\n- sparse-reward implementations of HER already do essentially what this paper proposes\n\nComments:\n\nThe main contribution of the paper appears to be the addition of what the authors refer to as a \"step loss\", which in this case enforces the Q function to correctly incorporate the termination condition when goals are achieved. I.E. the discounted sum of future rewards for states that achieve termination should be exactly equal to the reward at that timestep.\n\nIt's not clear to me how this is fundamentally different than HER. One possible \"sparse reward\" implementation of HER involves no reward function recomputation at all, instead simply replacing the scalar reward and termination flag for resampled transitions with the indicator function for whether the transition achieves the resampled goal.\nIs this not essentially identical to the proposal in this paper? I would consider this a task-dependent implementation detail for an application of HER rather than a research contribution that deserves an entire paper.\n\nThe authors claim the main advantage here is avoiding recomputation of the reward function for resampled goals.\nI do not find this particularly compelling, given that all of the evaluations are done in low-dimensional state space: reward recomputation here is just a low-dimensional euclidean distance computation followed by a simple threshold.\nIn a world where we're doing millions of forward and backward passes of large matrix multiplications, is this a savings that really requires investigation?\nIt is somewhat telling that the results are compared primarily in terms of \"# of reward function evaluations\" rather than wall time. If the savings were significant, I expect a wall time comparison would be more compelling.\nMaybe the authors can come up with a situation in which reward recomputation is truly expensive and worth avoiding?\n\nAll of the experiments in this paper use a somewhat unusual task setup where every timestep has a reward of -1. Have the authors considered other reward structures, such as the indicator function R=(1 if s==g else 0) or a distance-based dense reward?\nWould this proposal work in these cases? If not, how significant is a small change to HER if it can only work for one specific reward function?\n\nConclusion:\n\nIn my view, the main contribution is incremental at best, and potentially identical to many existing implementations of HER.\nThe reconsideration of Floyd-Warshall RL in the context of deep neural networks is a refreshing idea and seems worth investigating, but I would need to see much more careful analysis before I could recommend this for publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper aims to improve on Hindsight Experience Replay by removing the need to compute rewards for reaching a goal. The idea is to frame goal-reaching as a shortest path problem where all rewards are -1 until the goal is reached, removing the need to compute rewards. While similar ideas were explored in a recent arxiv tech report, this paper claims to build on these ideas with new loss functions. The experimental results do not seem to be any better compared to baselines when measured in terms of data efficiency, but the proposed method requires fewer “reward computations”.\n\nClarity:\nWhile the ideas in the paper were easy to follow, there are a number of problems with the writing. The biggest problem is that it wasn’t clear exactly what algorithms were evaluated in the experiments. There is an algorithm box for the proposed method in the appendix, but it’s not clear how the method differs from the FWRL baseline.\n\nAnother major problem is that the paper does a poor job of citing earlier related work on RL. DQN is introduced without mentioning or citing Q-learning. Experience replay is mentioned without citing the work of Long-Ji Lin. There’s no mention of earlier work on shortest-path RL from LP Kaelbling from 1993. \n\nNovelty and Significance:\nAfter reading the paper I am not convinced that there’s anything substantially new in this paper. Here are my main concerns:\n\n1) The shortest path perspective for goal-reaching was introduced in “Learning to Achieve Goals” by LP Kaelbling [1]. This paper should be cited and discussed.\n\n2) I am not convinced that the proposed formulation is any different than what is in Hindsight Experience Replay (HER) paper. Section 3.2 of the HER paper defines the reward function as -1 if the current state is not the same as the goal and 0 if the current state is the same as the goal. Isn’t this exactly the cost-to-go/shortest path reward structure that is used in this paper?\n\n3) This paper claims that the one-step loss (Equation 8) is new, but it is actually the definition of the Q-learning update for transitioning to a terminal state. Since goal states are absorbing/terminal, any transition to a goal state must use the reward as the target without bootstrapping. So the one-step loss is just Q-learning and is not new. This is exactly how it is described in Section 3 of [1].\n\n4) The argument that the proposed method requires fewer reward evaluations than FWRL or HER seems flawed. HER defines the reward to be -1 if the current state and the goal are different and 0 if they are the same. As far as I can tell this paper uses the same reward structure, so how is it saving any computation?\n\nCan the authors comment on these points and clarify what they see as the novelty of this work?\n\nOverall quality:\nUnless the authors can convince me that the method is not equivalent to existing work I don’t see enough novelty or significance for an ICLR paper.\n\n[1] “Learning to Achieve Goals” LP Kaelbling, 1993.\n",
            "rating": "1: Trivial or wrong",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Evaluation and judgement",
            "review": "The paper presents an approach for an approach to addressing multi-goal reinforcement learning, based on what they call \"one-step path rewards\" as an alternative to the use of goal conditioned value function. \nThe idea builds on an extension of a prior work on FWRL. \nThe paper presents empirical comparison of the proposed method with two baselines, FWRL and HER. \nThe experimental results are mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method. \nThe idea of the proposed method is relatively simple, and is not theoretically justified. \n\nBased on these observations, the paper falls short of the conference standard. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}