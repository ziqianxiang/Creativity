{
    "Decision": {
        "metareview": "The paper proposes and approach for model-based reinforcement learning that adds a constraint to encourage the predictions from the model to be consistent with the observations from the environment. The reviewers had substantial concerns about the clarify of the initial submission, which has been significantly improved in revisions of the paper. The experiments have also been improved.\nStrengths: The method is simple, the performance is competitive with state-of-the-art approaches, and the experiments are thorough including comparisons on seven different environments.\nWeaknesses: The main concern of the reviewers is the lack of concrete discussion about how the method compares to prior work. While the paper cites many different prior methods, the paper would be significantly improved by explicitly comparing and contrasting the ideas presented in this paper and those presented in prior work. A secondary weakness is that, while the results appear to be statistically significant, the improvement over prior methods is still relatively small.\nI do not think that this paper meets the bar for publication without an improved discussion of how this work is placed among the existing literature and without more convincing results.\n\nAs a side note, the authors should consider comparing to the below NeurIPS '18 paper, which significantly exceeds the performance of Nagabandi et al '17: https://arxiv.org/abs/1805.12114",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "A small idea, with poor comparisons",
            "review": "\nSummary:\n\nThis paper presents a simple auxiliary loss term for model-based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts.  Simple experiments demonstrate that the constraint slightly improves performance.\n\nQuality:\n\nWhile I think the idea of a consistency constraint is probably reasonable, I consider this a poorly executed exploration of the idea.  The paper makes no serious effort to compare and contrast this idea with other efforts at model-based RL.  The most glaring omission is comparison to very old ideas (such as dyna) and new ideas (such as imagination agents), both of which they cite.\n\nClarity:\n\nThe paper is reasonably clear, although there are some holes.  For example, in the experimental section, it is unclear what model-based RL algorithm is being used, and how it was modified to support the consistency constraint.  (I did not read the appendix).\n\nOriginality:\n\nIt is not clear how novel the central idea is.\n\nSignificance:\n\nThis idea is not significant.\n\nPros:\n+ A simple, straightforward idea\n+ A good topic - progress in model-based RL is always welcome\n\nCons:\n- Unclear how this is significantly different from other related work (such as imagination agents)\n- Experimental setup is poorly executed.\n  - Statistical significance of improvements is unclear\n  - No attempt to relate to any other method in the field\n  - No explanation of what algorithms are being used\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper presents the idea of learning models of the environment while interacting with it, in the form of performing the usual model-based or model-free reinforcement learning, while enforcing consistency between the real world (observations) and the model. The presented motivation is that agents, like people, can benefit through not just observing the environment and learning from it, but also by experimenting---trying actions specifically for learning",
            "review": "---Below is based on the original paper---\nThis paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentation---taking actions mainly for the purpose of learning an improved dynamics model. All of their experiments merely take actions that are best according to the usual model-based or model-free methods, and show that their consistency constraint allows them to learn a better dynamics model, which is not at all surprising. They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around, which is to allow exploration by artificially increasing the reward for the first few times that each state is visited. That would be a good baseline against which to compare their method.\n\nOverall:\nPros:\n1. Clear writing\n2. Good motivation description.\n\nCons:\n1. Failed to connect presented work with the motivation.\n2. No comparison against known methods for exploration.\n\n\n----Below is based on the revision---\n\nThanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs clarification.",
            "review": "\n-------------\nSummary\n-------------\nThe authors propose to train a policy while concurrently learning a dynamics model. In particular, the policy is updated using both the RL loss (rewards from the environment) and the \"consistency constraint\", which the authors introduce. This consistency constraint is a supervised learning signal, which compares trajectories in the environment with trajectories in the imagined world (produced with the dynamics model). \n\n---------------------\nMain Feedback\n---------------------\nI feel like there might be some interesting ideas in this work, and the results suggest that this approach performs well. However, I had a difficult time understanding how exactly the method works, and what its advantages are. These are my main questions:\n\n1) At the beginning of Section 4 the authors write \"The learning agent has two pathways for improving its behaviour: (...) (ii) the open loop path, where it imagines taking actions and hallucinates the state transitions that could happen\". Do you actually do this? This is not mentioned in anywhere. And as far as I understand, the reward function is not learned - hence there will be no training signal in the open loop path. Does the reward signal always come from the true environment?\n2) Is the dynamics model used for anything else than action-selection during training? Planning? If not, I don't really understand the results and why this works at all (k=20 being better than k=5, for example).\n3) Is the dynamics model pre-trained in any way? I find it surprising that the model-free method and the proposed method perform similar at the beginning (Figure 3). If the agent chooses its actions based on the state that is predicted by the dynamics model, this should throw off the learning of the policy at the beginning (when the dynamics model hasn't learned anything sensible yet).\n\n-----------------------\nOther Questions\n-----------------------\n4) How exactly does training without the consistency constraint look? Is this the same as k=1?\n5) Could the authors comment on the evaluation protocol in the experimental section? Are the results averages over multiple runs? If so, it would help to see confidence intervals to make a fair assessment of the results. \n6) For the swimmer in Figure 2, the two lines (with consistency and without consistency) start at different initial returns, why is that so? If the same architecture and seed was used, shouldn't this be the same (or can you just not see it in the graph)?\n\n---------\nClarity\n---------\nThe title and introduction initially gave me a slightly wrong impression on what the paper is going to be about, and several things were not followed up on later in the paper.\nTitle:\n8) \"generative models\" reminds of things like a VAE or GAN; however, I believe the authors mean \"dynamics models\" instead\n9) \"by interaction\" is a bit vague as to what the contribution is (aren't policies and dynamic models in general trained by interacting with the environment?); the main idea of the paper is the consistency constraint\nAbstract / Introduction:\n10) The authors talk about humans carrying out \"experiments via interaction\" to help uncover \"true causal relationships\". This idea is not brought up again in the methods section, and I don't see evidence that with the proposed approach, the policy does targeted experiments to uncover causal relationships. It is not clear to me why this is the intuition that motivates the consistency constraint. \n11) As the authors state in the introduction, the hope of model-based RL is better sample complexity. This is usually achieved by using the model in some way, for example by planning several steps ahead when choosing the current action. Could the authors comment on where they would place their proposed method - how does it address sample complexity?\n12) In the introduction, the authors discuss the problem of compounding errors. These must be a problem in the proposed method as well, especially as k grows. Could the authors comment on that? How come that the performance is so good for k=20?\n13) The authors write that in most model-based approaches, the dynamics model is \"learned with supervised learning techniques, i.e., just by observing the data\" and not via interaction. There's two things I don't understand: (1) in the existing model-based approaches the authors refer to, the policy also interacts with the world to get the data to do supervised learning - what exactly is the difference? (2) The auxiliary loss \"which explicitly seeks to match the generative behaviour to the observed behaviour\" is just a supervised learning loss as well, so how is this different?\n\nFor me, it would help the readability and understanding of the paper if some concepts were introduced more formally.\n14) In Section 2, it would help me to see a formal definition of the MDP and what exactly is optimised. The authors write \"optimise a reward signal\" and \"maximise its expected reward\", however I believe it should be the expected cumulative reward (i.e., return). \n15) The loss function for the dynamics model is not explicitly stated. From the text I assume that it is the mean squared error for the per-step loss, and a GAN loss for the trajectory-wise loss.\n16) Could the authors explicitly state what the overall loss function is, and how the RL and supervised objective are combined? Is the dynamics model f trained only on the supervised loss, and the policy pi only on the RL loss?\n17) In 2.3 the variable z_t is not formally introduced. What does it represent?\n\n------------------------\nOther Comments\n------------------------\n18) I find it problematic to use words such as \"hallucination\" and \"imagination\" when talking about learning algorithms. I would much prefer to see formal/factual language (like saying that the dynamics model is used to do make predictions / do planning, rather than that the agent is hallucinating). \n\n-- edit (19.11.) ---\n- updated score to 5\n- corrected summary",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}