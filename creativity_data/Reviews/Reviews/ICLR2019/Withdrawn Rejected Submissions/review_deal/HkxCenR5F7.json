{
    "Decision": {
        "metareview": "This paper heavily modifies standard time-series-VAE models to improve their representation learning abilities.  However, the resulting model seems like an ad-hoc combination of tricks that lose most of the nice properties of VAEs.  The resulting method does not appear to be useful enough to justify itself, and it's not clear that the same ends couldn't be pursued using simpler, more general, and computationally cheaper approaches.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Many modifications to VAEs with little justification"
    },
    "Reviews": [
        {
            "title": "Needs stronger motivation, better analysis would improve the paper",
            "review": "This is largely an experimental paper, proposing and evaluating various modifications of variational recurrent models towards obtaining sequence data representations that are effective in downstream tasks. The highlighted contribution is a \"stochastic generation\" training procedure in which the training objective evaluates the reconstruction of output sequence elements from individual latent variables independently. The main claim is that the resulting model, augmented with prior updating and/or hierarchical latent variables, improves results w.r.t. the baselines.\n\nMy main concern is that the various choices are not motivated well, e.g. with examples or detailed descriptions of the issues addressed and that the resulting implications are not discussed in detail (see detailed comments below). This could perhaps be alleviated during the rebuttal discussion.\n\nEmpirically, when used in conjunction with prior updating and/or hierarchical latent variables, the proposed \"stochastic generation\" approach improves upon the baselines, but not when used in isolation. This is OK, but it weakens the contribution since it's more unclear what the exact advantage \"stochastic generation\" is, how it takes advantage of prior updating, and so on. Could you maybe discuss this in the rebuttal? The fact that not all model variants considered are evaluated on all settings also contributes to this problem (again, see below).\n\nGeneral questions:\n- \"dependence of observations at each time step on all latent variables\": Unfortunately, this means that the complexity of evaluating the model during training is O(n^2), where n is the sequence size, rather than linear in the standard case. Is that correct? I think this is what is alluded to on the top on page 4. Could you discuss this trade-off?\n- regarding section 2.1.: Multi-modal marginal probabilities are also used due their increased modeling power, and this again seems like a potential limitation of the proposed approach w.r.t. the baseline, and is not discussed.\n- \"the mean of z_t may have very small probability and thus may not be a good choice\": I think this statement requires more context. The mean of z_t can have low probability in both cases (e.g. if the posterior has a high variance). Are you suggesting that the low probability issue is exacerbated by to the sampling of previous z_{t-1}? Or are you comparing to the case where the mean z_{t-1} is used instead of sampling as well?\n\nStochastic generation:\n- While I understand where it's coming from, the term \"stochastic generation\" is somewhat misleading, since stochasticity is already present in the generation process for VAEs;\n- Stochastic generation is introduced as a way to approximate the generation process. However, when it's introduced, it's not clear what the generation process that needs to be approximated is. Introducing the model in eq. (6-7), motivating its use and then showing how it is obtained through stochastic generation second would improve the clarity of the paper.\n- Related to the point above, the implications of using the model in eq. (6-7) are not discussed. The graphical model in Figure 1 suggests that x_k depends jointly on all the (z_t)_{t=1 ... sequence_size}. Instead, in eq. (6-7), each x_k is generated independently from each z_t (for t = 1 ... T, and k sampled from a distribution which depends on t). In particular, if I understand this correctly, the distribution p(x_k | z) = p(x_k | z_1 \\dots z_T) factorizes as p(x_k | z_1) p(x_k | z_2) ... p(x_k | z_T). Could you motivate this choice and its expected effect? It seems to me that this encourages each z_t to capture all the information needed to reconstruct each x_k in the corresponding window.\n\nExperimental results:\n- Table 2: I think this table since it includes most models, but it still misses RecRep (without delta = 0) and StocCon. Could you confirm whether StocCon vs. RecRep have the same setting except the use of recurrent stochastic connections in StocCon vs. using eq. (4) in RecRep with window size 1?\n- In Table 4, the difference between line 5 and line 6 is interesting and I wish it was discussed more, maybe used in the visualization experiment to show how/why \"stochastic generation\" with a larger window improves performance.\n- Figure 3, could it be that the use of hierarchical latent variables (H) accounts for the visual difference? Is a difference still observed when comparing lines 3 and 7 in Table 4, whose settings seem more comparable?\n- \n\nMinor issues:\n- the lack of parenthesis around citations makes the text hard to follow at times (maybe use \\citep whenever the citation mixes with the text?);\n- typo: \"for use in a downstream tasks\"\n- typo: \"with graphical model as described\" => \"with the/a graphical model as described\"\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Method that tries to be a feature extractor and a generative model at the same time.",
            "review": "(best read in typora)\n\nThe authors claim to propose a family of methods and generative models that are suited better for downstream tasks than previously proposed approaches.\n\n## Major points\n\nIt feels as if the proposed method tries to be many things. First, it is used for finding unsupervised representations down stream. Then, it still tries to be a generative model \"of sorts\", which is the reason for the use of variational inference in the first place. Additionally, the approximate posterior necessary to evaluate the ELBO is simultaneously used as a feature extractor.\n\nThe resulting issues are:\n\n  - A \"bad\" variational posterior is used because it is unclear how to get vectorial features otherwise. \n  - An adhoc likelihood function is used, which is not sufficiently well explored theoretically in the paper.  Specifically,\n      - Stochastic generation is claimed to be \"more complex than simple Gaussian\"; the burden of proof is on the authors, as Gaussian density is closed under multiplication. \n      - It appears to be a Monte Carlo approximation to sth that is computable in closed form.\n      - It is not clear if that MC approximation is normalised and if the normalisation is the same at each optimisation step. Does this bias optimisation? What happens to the KL penalty weight?\n  - The ELBO change (prior updating) seems to make the claim that we still have a generative model (as written in the intro) invalid. My intuition is that the KL penalty vanishes for small step rates of the optimiser, reducing the model to that of a noisy auto encoder.\n\n\n## Summary\n\nThe authors want to evaluate variational sequence models for feature extraction for downstream tasks. But why? What is the use of a generative inspired algorithm, when necessary ingredients are discarded? Both goals appear to be at conflict and I am not convinced that the variational ingredient is necessary.\n\nI do not cover the experimental section since the method itself has issues so severe that I don't consider it relevant. \n\n\n## Minor points\n\n- Notation $\\mu_{\\phi_t}$ gives the impression that $\\phi$ is time dependent.\n- Equations (9) and (11) are formatted badly.\n- The approximate posterior used was used first in (Bayer & Osendorfer, \"Learning stochastic recurrent networks\", 2014) not (Chen 2018).\n- Diagrams follow GM notation only half-heartedly.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Incremental contribution of variational recurrent models; big volume of extensions of the proposed method and experiments",
            "review": "This paper proposes a new variational recurrent model for learning sequences. Comparing to existing work, instead of having latent variables that are dependent on the neighbors, this paper proposes to use independent latent variables with observations that are generated from multiple latent variables. \nThe paper further combined the proposed method with multiple existing ideas, such as the shared/prviate representation from VAE-CCAE, adding the hierarchical structure, and prior updating. \n\nPros:\nThe proposed method seems technical correct and reasonable. \nThere are many extensions which are potentially useful for many applications \nThere are many experimental results showing promising performance. \n\nCons:\nThe framework is very incremental. It is novel but limited. \nThe paper claim that the main point to use the simpler variations distribution is to speed up the inference. But no speed comparisons are shown in the experiments section. \nThe evaluation shows that prior updating (one extension) seems contributes to the biggest performance gain, not the main proposed method. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}