{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting area but quite misleading on explanation vs. interpretation",
            "review": "\nSummary:\n=========\nThe paper proposes a technique to explain the embedding space quantitatively and semantically by distilling learned representations into an additive explainable model. Explanation generation for deep neural network models is a growing research area especially for mission critical applications that decision without explanation is unacceptable such as security and medical applications. More importantly, the focus on semantic explanations instead of (in addition to) the usual ‘visual attention or pixel level correlation as explanation’ is interesting. Although, the proposed method is interesting, I have several concerns. I have laid out my comments on the strength of the paper and my concerns as follows.\n\nStrength:\n======== \n- Distilling the representation into a separate model to explain rather than incorporating the explanation generation into a single model helps minimize performance degradation due to the additional constraint of generating explanation.\n\n- The introduction of semantic explanations is interesting, although the ‘explanation’s are not human-like explanations. See below for further discussion on this.\n\nWeakness:\n=========\n- The abstract could use a bit simplification and clarification to make it concise at the same time be inclusive of the key points of the proposed method. More specifically, the second sentence could be simplified to make it easy to follow.\n\n- The need for explanation was not strongly motivated in the introduction in the context of existing literature. Existing explanation generating and/or interpretable networks should be discussed and the proposed method should be presented with what gap in existing literature it is trying to address in a more concise form. It is a bit lengthy in current form.\n\n- Need to cite related literature on making claims such as “…Distilling knowledge from a pre-trained neural network into an additive model usually suffers from the problem of bias-interpreting.”\n\n- The paper mixes interpretation with explanation. Interpretability and explainability are two different things. Invoking attributes of explanations in human cognition, generated explanations should mimic human explanations in that they should be human-understandable, concise, and at the right level of detail. What the introduction promised as semantic explanation was not delivered in the experiments. Explanations for why the network predicts a particular face as attractive are not laundry list of other attributes with positive and negative correlations. Explanations are composed. The authors are learning the inter-attribute relationship (correlations) and this is presented as ‘explanation’. This is disappointing. The title and other earlier discussions need to be revised not to confuse simple interpretation with explanation.\n\n- The ‘explanations’ are not as generic as they were advertised earlier. They are in fact specific to specific tasks. It is not obvious from the experiments on object parts and facial attributes tasks how this interpretation using correlation could be generalized to any task learned through a CNN.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The content must be re-organized and a deeper evaluation is needed",
            "review": " \n=============================\nSUMMARY\n=============================\n\nThe paper proposes a method to address the task of model explanation for deep convolutional neural networks (CNNs). More specifically, given a (performer) model to be explained, it proposes to use pre-trained detectors of visual concepts as means to justify the the predictions made by the performer through an additive (explainer) model. This additive model operates on top of the response, i.e. the score, produced by the concept detectors and its weights are learned also through a deep model, i.e. a ResNet-152. \n\n The proposed method is evaluated on two specific explanation tasks. The first task is related to explaining the occurrence of an object in an image (object recognition) based on object parts. Experiments on this task are conducted on the Pascal-Parts dataset and cover various standard CNN architectures including AlexNet and various variants of VGG (VGG-S, VGG-M and VGG-16). The second task is related to the prediction of face attributes based on other attributes. Experiments on this task are conducted on the CelebFaces Attributes dataset and are based on the VGG-16 CNN architecture.\n Performance on both experiments is reported in terms of prediction accuracy and relative deviation, which compare the performance of the performer model with that of the explainer model.\n \n=============================\nSTRENGHTS\n=============================\n\nThe manuscript proposes to produce explanations derived from semantic concepts. Adding semantic concepts to visual explanations is a desirable characteristic in order to reduce possible ambiguity that may occur when highlighting image regions with strong influence on the decision made by the model. \n\nThe related work section of the manuscript seems is quite detailed and covers a good amount of work from the literature.\n\n=============================\nWEAKNESSES\n=============================\n\nThe content of the manuscript is not properly balanced. On the one hand, the starting sections are quite detailed and extended. On the other hand, the last sections of the are quite reduced. This affects the flow of the manuscript, which has a good start but results in an abrupt end.\n\nSimilar to the network dissection method (Bau et al, CVPR'17), the proposed method has the requirement of a set of visual concepts to be pre-defined. A suboptimal selection of these concepts will result in a set not representative or not sufficiently general concepts which will produce meaningless explanations. That is the possible set of explanations is bounded by the set of selected concepts.\nIn addition, the proposed has the strong requirement of pre-trained detector for each of these visual concepts.\n\nIn several parts of the manuscript, i.e Section 1 and 5, it is claimed that the proposed method allows to provide a semantic explanation of the logic followed by the CNNs in order to make predictions. At this point it is not clear to me how the proposed method effectively mimics the decision-making process followed by the performer models to reach a particular decision.\n\nIn the experiment from Section 4.1 it is stated that several CNN types (AlexNet, VGG-S, VGG-M and VGG-16) are considered. However, besides depth and spatial resolution of some of the layers, all these models are feed-forward CNN architectures. I would be more convinced if CNNs and perhaps another type of architecture, e.g. a RNN, were considered.\n\nIn Section 3.1, details regarding the computation of prior weights are provided. However, a closer analysis of this Section in conjunction with Section 4 suggest that the computation of this priors is tailored for the specific explanation tasks covered in the experiments conducted in Sections 4.1 and 4.2.\nIn addition, for the explanation tasks of Sections 4.1 and 4.2, a very suitable set of visual concepts were pre-defined.\nPerhaps I am missing something, but these two points make wonder about the generability of the proposed method.\n\nIn the quantitative evaluation from Section 4.3 the proposed method is only compared against a single baseline, i.e. the distillation loss to learn the explainer. \nThis evaluation seems quite limited when taking into account the significant amount of visual explanation methods that have been proposed recently and the fact that distillation loss used in the compared baseline is not necessarily designed for model explanation.\nIn addition, the discussion of results covered in Section 4.3 is quite reduced. The manuscript would definitively benefit from an extended discussion of its inner-workings (an ablation  study) and an extended comparison w.r.t. recent methods.\nFurthermore, I would recommend complementing the presented experiments with evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302).\n\nFinally, there are several parts of the manuscript pointing towards the appendix. Some of these pointers, e.g. Sec. 4.3, link toward important content that should be on the main manuscript. Having inspected the appendix, it seems a good amount of experiments were conducted and significant amount of qualitative results are available for the proposed method. In my opinion the manuscript would benefit significantly from moving some of this content to the main manuscript and discussing it adequately.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting task and simple approach but experiments are not formulated well and are not convincing",
            "review": "This work proposes to explain classification decisions via quantitative estimates of the contributions of individual visual concepts to the classification score. It learns an explainer model on top of a performer model (as distillation); the explainer returns the contributions of different concepts. To prevent the model from only selecting a few concepts for the explanation, the authors use a prior-enforcing loss. \n\nThe approach is clear but evidence that the proposed work will have an impact in practice is not convincing. The quantitative results don't show the proposed method is useful. Table 1 shows entropy over concept contributions, but the authors don't convince the reader that this is the right metric to use. They argue using too few concepts (which of course leads to low entropy) is undesirable, but there is no evidence/proof why this is undesirable. Table 2 uses prediction accuracy, which does not seem to be in line with explainability. The qualitative figures are somewhat interesting, but they are not sufficient. It is easy to look at a qualitative figure and say \"sure, this makes sense\" but this does not mean the proposed method is useful, especially when there is no result from a baseline, whose output might also make perfect sense. Finally, the authors claim pixel-level visualizations aren't useful, but there's no proof of why that is. A user study of what's useful, or a quantitative evaluation of the usefulness of different visualizations for downstream tasks, would be good. \n\nIn terms of writing, the first few pages tend to be repetitive yet vague about what exactly will be done (generally ok for introduction, but a bit too vague). It is not clear what \"subjectively visualizing\" means. Finally, I found the end of second para in Sec. 3 unclear.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}