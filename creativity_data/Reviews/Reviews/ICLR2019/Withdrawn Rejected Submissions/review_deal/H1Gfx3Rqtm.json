{
    "Decision": {
        "metareview": "This paper presents a reinforcement learning approach to hierarchical text classification.\n\nPros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning.\n\nCons: The major concensus among all reviewers was that there were various concerns about experimental results, e.g., apple-to-apple comparisons against prior art (R1), proper tuning of hyper-parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3). In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, e.g., what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem).\n\nVerdict: \nReject. While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "author response did not address the major concerns of the reviewers regarding the empirical results"
    },
    "Reviews": [
        {
            "title": "Clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons",
            "review": "This papers uses the label hierarchy to drive the search process over a set of labels using reinforcement learning. The approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons.\n\nHowever, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search.\nI tend to see hierarchical classification as an approach to multi-label classification justified by a greedy decomposition that reduced both training and test time. This view has been outmoded for more than an decade, first as flat approaches became feasible, and now as end-to-end  structured classification is implementable with DNNs (see for instance David Belanger work with McCallum)\n\nCompared to other structured classification approaches whose scope is limited by the complexity of the inference process, this approaches is very attractive. The authors open the optimization black box of the inference process by adding a few very clever tricks that facilitate convergence:\n- Intermediate rewards based on the gain on F1 score\n- Self critical training approach\n- \"Clamped\" pre-training enabled by the use of state embeddings that are multiplied my a transition to any state in the free mode, and just the next states in the hierarchy in the clamped mode\n- Addition of a flat loss to improve the quality of the document representation\n\nWhile those tricks may have been used for other applications, they seem new in the context of hierarchical/multi-label/structured classification.\n\nWhile the experiments appear thorough, they could be the major weakness of this paper. The results the authors quote as representative of other approaches seem in fact entirely reproduced on datasets that were not used on the original papers, and the authors do not try an apple-to-apple comparison to determine if this 'reproduction' is fair. None of the quoted work used the 2018 version of Yelp, and I could only find RCV1 Micro-F1 experiments in Johnson and Yang, who report a 84% micro-F1, far better than the 76.6% reported on their behalf here, and better than the 82.7% reported  by the authors. I read note 4 about the difference in the way the threshold is computed, but I doubt it can explain such a large difference. I did not check everything, but could not find and apple-to-apple comparison?\n\nHave the network architecture been properly optimized in terms of hyper-parameters?\nIn particular, having tried Kim CNN on large label sets, I suspect the author settings using a single layer after the convolution is sub-optimal. I concur with the following paper than an additional hidden layer is essential: Liu et al \"Deep Learning for Extreme Multi-label Text Classification\". I also note the 32 batch size could be way too small for sparse label sets (I tend to use a batch size of 512 on this type of data).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but would like to see clearer set of claims with appropriate evaluation.",
            "review": "This work proposes an RL approach for hierarchical text classification by learning to navigating the hierarchy given a document. Experiments on 3 datasets show better performance. I'm happy to see that it was possible to \n\n1. \"we optimize the holistic metrics over the hierarchy by providing the policy network with holistic rewards\"\n\nI don't quite understand what are the \"holistic metrics\" and \"holistic rewards\". I would like the authors to answer \"what exactly does reinforcement learning get us ?\"\n - Is it optimizing F1 metric or is it the ability to fix inconsistent labeling problem ? \n- If it is the latter, what is an example of inconsistent labeling, what fraction of errors (in table 2/3) are inconsistent errors. Are we really seeing the inconsistent errors drop ?\n- If it is the former, how does this compare to existing approaches for optimizing F1 metric.\n\n2. \"the F1 score of each sample xi\"\n\na. F1 is a population metric, what does it mean to have F1 for a single sample ?\nb. I'm not aware of any work that shows optimizing per-example f_1 minimizes f_1 metric over a sample.\n\n3. with 10 roll-outs per training sample, imho, it seems unrealistic that the expected reward can be computed correctly. Would'nt most of the reward just be zero ? Or is it the case the model is initialized with an MLE pretrained parameters (which seems like it, but im not too sure).\n\nResults analysis,\n- imho, most of the rows in Table 2 does not seem comparable with each other due to pretrained word-embeddings and dataset filtering, e.g. SVM-variants, HLSTM.\n- in addition to above, there is the standard issue of using different #parameters across models which increases/decreases model capacity. This is ok as long as all parameters were tuned on held out set, or using a common well established unfiltered test set - neither of which is clear to me.\n- it is not clear how the F1 metric captures inconsistent labeling, which seems to be the main selling point for hi-lap. \n\nside comment\n- reg textcnn performance, could it be that dropout is too high ? (the code was set to 0.5)\n   ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "reinforcement learning approach for hierarchical text classification",
            "review": "This paper presents an end to end rl approach for hierarchical text classification. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy. It is based on capturing the global hierachical structure during training and prediction phases as against most methods which either exploit the local information or neural net approaches which ignore the hierarchical structure. It is demonstrated the method particularly works well compared to sota methods especially for macro-f1 measure which captures the label weighted performance. The approach seems original, and a detailed experimental analysis is carried out on various datasets. \n\nSome of the concerns that I have regarding this work are :\n - The problem of hierarchical text classification is too specific, and in this regard the impact of the work seems quite limited. \n - The significance is further limited by the scale of the datasets of considered in this paper. The paper needs to evaluate against on much bigger datasets such as LSHTC datasets http://lshtc.iit.demokritos.gr/. For instance, the dataset available under LSHTC3 is in the raw format, and it would be really competitive to evaluate this method against other such as Flat SVM, and HRSVM[4] on this dataset, and those from the challenge.\n- The experimental evaluation seems less convincing such as the results for HRSVM for RCV1 dataset are quite different in this paper, and that given HRSVM paper. It is 81.66/56.56 vs 72.8/38.6 reported in this paper. Given that  81.66/56.56 is not too far from that given by HiLAP, it remains a question if the extra computational complexity, and lack of scalability (?) of the proposed method is really a significant advantage over existing methods.\n - Some of the references related to taxonomy adaptation, such as [3] and reference therein,  which are also based on modifying the given taxonomy for better classification are missing.\n - Comparison with label embedding methods such as [1,2] are missing. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better.\n[1] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, Sparse Local Embeddings for Extreme Multi-label Classification, in NIPS, 2015.\n[2]  H. Yu, P. Jain, P. Kar, and I. Dhillon, Large-scale Multi-label Learning with Missing Labels, in ICML, 2014.\n[3] Learning Taxonomy Adaptation in Large-scale Classification, JMLR 2016.\n[4] Recursive regularization for large-scale classification with hierarchical and graphical dependencies, https://dl.acm.org/citation.cfm?id=2487644",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}