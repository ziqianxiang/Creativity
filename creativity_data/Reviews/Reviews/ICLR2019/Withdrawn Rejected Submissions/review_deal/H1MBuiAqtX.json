{
    "Decision": {
        "metareview": "The authors present an interesting approach but there were multiple significant concerns with the clarity of the presentation, and some concern with the significance of the experimental results.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Significant concerns with current presentation clarity"
    },
    "Reviews": [
        {
            "title": "Presentation hard to parse. ",
            "review": "I tried to parse the paper's details multiple times but it really seems like a hard task. From what I understand, the paper is doing off-policy learning across several environments. The actors are parallelized across the environments (tasks), collecting rollouts, and the update to the learner is done on a GPU which trains on all these rollouts asynchronously collected. The learner uses a UVFA with an LSTM as its architecture. The authors learn on a set of training environments with various goals (associated with picking specific objects in an order) and test this policy's ability to work on new environments with a different set of goals. \n\nOverall, I find the writing really a pain to parse. I wish the authors directly wrote what they are doing quickly: \"We take Schaul's UVFA, make it recurrent, use the IMPALA set up of Esspholt, and show generalization to new combinations of objects to be collected as goals\".\n\nI am still trying to evaluate the paper, but for now, my rating for this is low given that the main novelty in the paper: the environments, the evaluations, the tasks are so unclear because of the verbose presentation style on trying to tell us what we already know, such as goal-conditioned learning, off-policy learning, IMPALA, etc. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Deep dependent continual leaning scenario looks interesting.",
            "review": "The paper proposes a novel architecture in the context of multitask learning and deep dependency structure. They try to solve their target issue in reinforcement learning, thus utilize the useful architecture UVFAs (Schaul et al.) since the model is effective to manage multiple policies into single value function. \n\nThe paper is easy to follow, but with quite long explanation. They verify their architecture UNICORN in various learning scenarios, i.e., multitask-learning, generalization to related tasks, and continual learning. And I have several remarks,\n\n- In the learning scenario, I confused that it looks a little bit far from the common definition of continual learning which is referred in machine learning area(learning various tasks in sequence, not simultaneously) but is more like an attribute learning or something.\n\n- It is ambiguous to ensure that the proposed architecture can show state-of-the-art or comparable performance. The compared baselines, such as glutton and random, look too simple and show minor performance as already described in the paper.\n\nThe deep dependency structure setting is quite interesting, also there are many useful discussion and imposing experiments. It would be great to see much clear competitiveness and identity of the architecture.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but explanation can be clearer, and scope is limited.",
            "review": "Summary\n\nThe authors aim to do continual learning to solve dependent tasks using \"single-stage end-to-end learning\". The resulting \"Unicorn\" agent trains on all tasks simultaneously. The idea is to use multi-task \"off-policy learning\", which uses (old) trajectories (experience) from task A to help learning on a related task B. Authors further distinguish between goals (inputs to Q) and tasks (different reward functions). A goal might a color/shape of an object to pick up.\n\nThe core model is a UVFA that learns a goal-conditioned Q-function Q(s,a,g). \n\nSome technical aspects: \n- use n-step returns.\n- when training Q on goal g_i, authors also trajectories that were generated using Q conditioned on another goal(s) g_j. They then truncate the returns for task i when an action taken is not optimal under Q(s,a,g_j) conditioned on goal j. The intuition (seems) to be that this \n- authors do not use experience replay or a target Q-function, since the parallelized implementation is reported to be stable enough.\n- unicorn sees all train task reward functions during training (but not hold out task rewards).\n- unicorn is tested on several 3d maze environments with key-lock etc semantics. The tasks / goals seem simple, and the dependency is defined by changing colors / shapes of objects to be picked up. Authors argue unicorn has to learn to relate task rewards to these goal features.\n- unicorn is compared against baselines that 1) do single-task learning (expert) 2) learn on a sum of task rewards (glutton), 3) uniformly random baseline. \n- authors show that 1) unicorn performs better on train tasks 2) performs better on hold-out tasks. Also, authors show results for zero-shot transfer learning, with adding abstract tasks (extra reward for picking up any object) improving performance, \n\nPro\n- Simple approach (e.g., no experience replay etc), and uses only a limited set of techniques (e.g., reward truncation). \n- Reward performance suggests the model has more properly related goal features to different payoffs.\n- Analysis of qualitative behavior is nice.\n\nCon\n- The writing is a bit dense in places, e.g., the discussion of baselines is a bit hard to read.\n- Description of algorithm is wrapped in long text, a clear algorithm box would make the approach much clearer.\n- Not clear what kind of hyperparameters are introduced / used / tuned for Unicorn. \n- Authors say \"deep dependency\", but this seems to just refer to different colors / shapes between objects in the env used in the paper. How is \"dependency\" between goals and tasks defined in general? \n- The experimental setting seems a bit limited, authors only show results on a single domain, and do not offer rigorous definitions. This makes the scope of the paper rather limited.  \n\nReproducibility: \n- It's not clear what the variance in the baseline performance is (variance only shown for unicorn).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}