{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not sure about the novelty: a slight generalization of [Hein and Maier 2007]",
            "review": "In classical Label Propagation (LP)-based semi-supervised learning, only the given graph structure and labelled data are taken account (e.g. in citation networks). However, this can be limiting as feature representations for data vertices (given by the matrix X) are not fully exploited. The proposed approach proposes to break this limitation.\n\nThe authors consider LP from a signal processing perspective by showing that it behaves as a low pass filter: It amplifies contributions of small eigenvalues (of the graph Laplacian) and attenuate contributions of large eigenvalues leading to smoother signal.\n\nSuch insight is exploited by replacing the input signal Y by X: Which has the result of smoothing features in X such as data points of the same class would have similar features. Then, a classifier is trained on the labelled data points using the filtered features. This classifier is finally used for classifying unlabelled data points.\n\nThe authors reveal a connection between the proposed approach (GLP) and graph convolutional neural networks (GCN) by showing that the former is a special case of the first, which gives a natural and intuitive explanation to the inner workings of GCNS. \n\nFinally, experiments on 5 datasets have been conducted showing that the proposed approach consistently outperforms recent and classical semi-supervised approaches by a large margin, especially when the number of data points is very small. \n\nThis paper is well motivated and written. However, the novelty is limited as similar ideas have been undertaken by [Hein and Maier 2007] although in different contexts. \n\nAlthough a discussion about [Hein and Maier 2007] has been provided in the related work section, it is not enough to faithfully compare both approaches. The authors should give proper credit to [Hein and Maier 2007] for the original ideas of smoothing the signal X and provide throughput qualitative and quantitative comparisons with [Hein and Maier 2007]:\nAs stated by the authors, this approach is perhaps not directly applicable to citation networks. However, it is applicable to MNIST, and a comparison in this context is necessary. Also, it would be interesting to qualitatively compare the denoising ability of both approaches by showing some example images (on MNIST as provided by [Hein and Maier 2007]).\n\nIn addition, I think this paper would benefit from:\n\n•\tProviding comparisons with recently proposed Gan-based semi-supervised approaches (e.g. Salimans et.al 2016)\n•\tGiving more details about hyper-parameter selection (alpha and k).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper is not very novel and does not have strong results",
            "review": "Review for GENERALIZED LABEL PROPAGATION METHODS FOR SEMI-SUPERVISED LEARNING\nSummary:\nThe paper proposes an extension to label propagation where they replace the label matrix with a feature matrix in the label propagation objective and they replace the Laplacian with functions of the Laplacian. These new features that are the solution to this new objective are then used in supervised classifiers.\nNovelty/Significance:\nIt is not very clear what the novelty of this paper is. The paper proposes a new algorithm for semi-supervised learning, which is claimed to have better performance than current algorithms. Their proposed method is more on the lines of smoothing features using both labeled and unlabeled data so that they can train supervised learners on few labeled points. It is not exactly semi-supervised because there is no synergy between the process that uses all the data and learning the classifier.\nThe main concern about the novelty is that the proposed method seems like a slight variation on label propagation in order to get nicer features and then just using whatever classifier. Overall the amount of content in the paper feels lacking and there seems to be large amounts of review, repetitiveness, and unnecessary details.\nQuestions/Clarity:\nWhat is the intuition behind why GLP which is a “multi-stage” process, works better than jointly modeling graph and feature information? Normally multi-stage methods work worse than joint models because they essentially model independently or in a greedy fashion (1 step before the next).\nIn section 4.1, the paper explains why normalization, 2 layers, and re-normalization are all important due to their effects on the eigenvalues. Is there intuition on why it is better for the eigenvalues to have the certain shaped explain in that section?\nPart of the introduction is repeated in the related works section. This section should be moved to the front as it is wasting space being repeated at the end.\nThe datasets seem to all be ones where more classical learning techniques are known to do worse than neural networks. It seems like the majority of the improvement in accuracy is due to the use of the neural network, not the feature smoothing/learning part. An obvious example of this is that the SVM with GLP features in Table 5 are worse/marginally better than ManifReg, which does semi-supervised SVMs.\nIn the experiments GCN does worse than GLP, but in the paper it is shown that GCN is a special case of GLP with the ReLU function removed from Eq. 9. Why does removing this function make it worse? Is it not the ReLU part, but that GLP uses the Laplacian and not one of the other filters? If this is the case, why could the Laplacian not be replaced with one of the other filters, which are essentially functions of the Laplacian.\nThe filters in section 5 should be summarized in a chart or something and the details left in the appendix. The filters are not new, and the details take unnecessary space in the main part of the paper. \nThe references do not have a consistent format.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Convincing application of graph filtering techniques in classification",
            "review": "This paper gives an overview of the recently proposed techniques for filtering graph signals. Then, the classification performance of several types of graph filters is studied over a few data sets. In my understanding, the main contribution of the paper is an elaboration on the existing graph filtering approaches and different filtering strategies in the problem of classification and studying their links to the recent graph CNNs, rather than the development of a novel and original methodology. Nevertheless, I think that the study might be worth presenting as it is well-written and it offers a nice treatment towards better comprehension of graph filtering techniques in data analysis problems, with convincing performance figures.\n\nSome minor comments:\n\n- Can you please briefly describe what the classification tasks are in the experiments?\n- Typos in the sentence above (1) “The objective OF OF”, and in (1): “Laplcacian”",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}