{
    "Decision": {
        "metareview": "This paper introduces a method for unsupervised abstractive summarization of reviews.\n\nStrengths:\n\n(1) The direction (developing unsupervised multi-document summarization systems) is exciting\n\n(2) There are interesting aspects to the model\n\nWeaknesses:\n\n(1)  The authors are clearly undecided how to position this work: either as introducing a generic document summarization framework or as an approach specific to summarization of reviews. If this is the former, the underlying assumptions, e.g., that the summary looks like a single document in a group is problematic. If this is the latter, then comparison to some more specialized methods are lacking (see comments of R1).\n\n(2) Evaluation, though improved since the first submitted version (when human evaluation was added), is still not great (see R1 / R3). The automatic metrics are not very convincing and do not seem to be very consistent with the results of human eval. I believe that instead or along with human eval, the authors should create human written summaries and evaluate against them. It has been done for extractive multi-document summarization and can be done here. Without this, it would be impossible to compare to this submission in the future work.  \n\n(3) It is not very clear that generating abstractive summaries of the form proposed in the paper is an effective way to summarize documents.  Basically, a good summary should reflect diversity of the opinions rather than reflect an average / most frequent opinion from tin the review collection.  By generating the summary from a review LM, the authors make sure that there is no redundancy (e.g., alternative views) or contradictions. That's not really what one would want from a summary  (See R3 and also non-public discussion with R1)\n\nOverall, I'd definitely like to see this work published but my take is that it is not ready yet.\n\nR1 and R2 are relatively negative and generally in agreement.  R3 is very positive. I share excitement about the research direction with R3 but I believe that concerns of R1 and R2 are valid and need to be addressed before the paper gets published.\n\n\n\n\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting work but not mature enough"
    },
    "Reviews": [
        {
            "title": "Promising unsupervised approach, but clarity issues",
            "review": "Overall and positives:\n\nThe paper investigates the problem of multidocument summarization\nwithout paired documents to summary data, thus using an unsupervised\napproach. The main model is constructed using a pair of locked\nautoencoders and decoders. The model is trained to optimize the\ncombination of 1. Loss between reconstructions of the original reviews\n(from the encoded reviews) and original the reviews, 2. And the\naverage similarity of the encoded version of the docs with the encoded\nrepresentation of the summary, generated from the mean representation\nof the given documents.\n\nBy comparing with a few simple baseline models, the authors were able\nto demonstrate the potential of the design against several naive\napproaches (on real datasets, YELP and AMAZON reviews). \nThe necessity of several model components is demonstrated\nthrough ablation studies. The paper is relatively well structured and\ncomplete. The topic of the paper fits well with ICLR. The paper\nprovides decent technical contributions with some novel ideas about\nmulti-doc summary learning models without a (supervised) paired\ndata set.\n\nComments / Issues\n\n[ issue 6 is most important ]\n\n1.  Problem presentation. The problem was not properly introduced and\nelaborated. In fact, there is not a formal and mathematical\nintroduction of the problem, input, output, dataset and model\nparameters. The notations used are not very clearly defined and are\nquite handwavy, (e.g. what is V, dimensions of inputs x_i was not\nmentioned until much later in the paper). The authors should make\nthese more precise. Similar problem with presentations of the models,\nparameters, and hyperparameters.\n\n3.  How does non-equal weighted linear combinations of l_rec and l_sim\nchange the results? Other variation of the overall loss function? How\ndo we see the loss function interaction in the training, validation\nand test data? With the proposed model, these could be interesting to\nobserve.\n\n4.  In equation two, the decoder seems to be very directly affecting\nthe quality of the output summary. Teacher forcing was used to train\nthe decoder in part (1) of the model, but without ground truth, I\nwould expect more discussions and experiments on how the Gumbel\nsoftmax trick affect or help the performance of the output.\n\n5.  Baseline models and metrics\n\n(1) There should be more details on how the language model is trained,\nsome examples, and how the reviews are generated from the language\nmodel as a base model (in supplement?).\n\n(2). It is difficult to get a sense of how these metrics corresponds\nto the actual perceived quality of the summary from the\npresentation. (see next)\n\n(3). It will be more relevant to evaluate the proposed design\nvs. other neural models, and/or more tested and proved methods.\n\n6. The rating classifier (CLF) is intriguing, but it's not clearly\nexplained and its effect on the evaluation of the performance is not\nclear: One of the key metrics used in the evaluation relies on the\noutput rating of a classifier, CLF, that predicts reader ratings on\nreviews (eg on YELP).  The classifier is said to have 72%\naccuracy. First, the accuracy is not clearly defined, and the details\nof the classifier and its training is not explained (what features are\nits input, is the output ordinal regression).  Equation 4 is not\nexplained clearly: what does 'comparing' in 'by comparing the\npredicted rating given the summary rating..' mean?  The classifier may\nhave good performance, but it's unclear how this accuracy should\naffect the results of the model comparisons.\n\nThe CLF is used to evaluate the rating of output\nreviews from various models. There is no justification these outputs\nare in the same space or generally the same type of document with the\ntraining sample (assuming real Yelp reviews).  That is probably\nparticularly true for concatenation of the reviews, and the CLF classifier\nscores the concatenation very high (or  eq 4 somehow leads to highest value\nfor the concatenation of reviews )... It's not clear whether such a classifier is \nbeneficial in this context.\n\n7. Summary vs Reviews. It seems that the model is built on an implicit\nassumption that the output summary of the multi-doc should be\nsufficiently similar with the individual input docs.  This may be not\ntrue in many cases, which affects whether the approach generalizes.\nDoc inputs could be covering different aspects of the review subject\n(heterogeneity among the input docs, including topics, sentiment etc),\nor they could have very different writing styles or length compared to\na summary.  The evaluation metrics may not work well in such\nscenarios.  Maybe some pre-classification or clustering of the inputs,\nand then doing summarization for each, would  help?  In the conclusions section, the\nauthors do mention summarizing negative and positive reviews\nseparately.\n\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Evaluation methodology and measures are questionable and should not be adopted by the community",
            "review": "This paper proposes a method for multi-document abstractive summarization. The model has two main components, one part is an autoencoder used to help learn encoded document representations which can be used to reconstruct the original documents, and a second component for the summarization step which also aims to ensure that the summary is similar to the original document. \n\nThe biggest problem with this paper is in its evaluation methodology. I don't really know what any of the three evaluation measures are actually measuring, and there is no human subject evaluation back them up.\n- Rating Accuracy seems to depend on the choice of CLF used, and at best says whether the summary conveys the same average opinion as the original reviews. This captures a small amount about the actual contents of the reviews. For example, it does not capture the distribution of opinions, or the actual contents that are conveyed.\n- Word Overlap with the original documents does not seem to be a good measure of quality for abstractive systems, as there could easily be abstractive summaries with low overlap that are nevertheless very good exactly because they aggregate information and generalize. It is certainly not appropriate to use to compare between extractive and abstractive systems.\n-There are many well-known problems with using log likelihood as a measure of fluency and grammaticality, such as biases around length, and frequency of the words.\nIt also seems that these evaluation measures would interact with the length of the summary being evaluated in ways which systems could game.\n\nOther points:\n- Multi-Lead-1: The lead baseline works very well in single-document news summarization. Since this model is being applied in a multi-document setting to something that is not news, it is hard to see how this baseline is justified.\n\n- Despite the fact that the model is only applied to product reviews, and there seem to be modelling decisions tailored to this domain, the paper title does not specify so, which in my opinion is a type of over-claiming.\n\nHaving a paper with poor evaluation measure may set a precedent that causes damage to an entire line of research. For this reason, I am not comfortable with recommending an accept.\n\n\n---\nThank you for responding to my comments and updating the paper. I have slightly raised my score to reflect this effort.\n\nThere are new claims in the results section that do not seem to be warranted given the human evaluation. The claim is that the human evaluation results validate the use of the automatic metrics. The new human evaluation results show that the proposed abstractive model performs on par with the extractive model in terms of conveying the overall sentiment and information (Table 2), whereas it substantially outperforms the extractive model on the automatic measures (Table 1). This seems to be evidence that the automatic measures do not correlate with human judgments, and should not be used as evaluation measures.\n\nI am also glad that the title was changed to reflect the scope of the experiments. I would now suggest comparing against previous work in opinion summarization which do not assume gold-standard summaries for training. Here are two representative papers:\n\nGanesan et al. Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions. COLING 2010.\nCarenini et al. Multi-Document Summarization of Evaluative Text. Computational Intellgience 2012.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel work breaking ground on abstractive unsupervised multi-document summarization",
            "review": "# Positive aspects of this submission\n\n- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.\n\n- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.\n\n- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.\n\n# Criticism\n\n- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.\n\n- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.\n\n- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}