{
    "Decision": {
        "metareview": "The paper studies safer policy improvement based on non-expert demonstrations.  The paper contains some interesting ideas, and is supported by reasonable empirical evidence.  Overall, the work has a good potential.  The author response was also helpful.  That said, after considering the paper and rebuttal, the reviewers were not convinced the paper is ready for publication, as the significance of this work is limited by a rather strong assumption (see reviews for details).  Furthermore, the presentation of the paper also requires some work to improve (see reviews for detailed comments).",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea, but limited applicability"
    },
    "Reviews": [
        {
            "title": "A good paper with interesting theory and algorithmic contribution. The weaknesses are the clarity as well as the limited experiments. ",
            "review": "The paper looks at learning a policy from multiple demonstrators which should also be safely improved by an reinforcement learning signal. They define the policy as a mixture of policies from the single demonstrators. The paper gives a new way to estimate the value function of each policy where the overall policy is defined as mixture of the single policies. The paper subsequently looks at the standard error of the value function estimation and then define the policy improvement step in the presence of value estimation error. The resulting reroute constraint for the policy improvement step is evaluated on the taxi toy task as well as on 4 different atari domains. \n\nThis paper presents an interesting ideas which is also based on an exhaustive theoretical derivation. However, the paper is lacking clarity and motivation which makes it almost impossible to understand at the first pass. Moreover, the presented results are promising but not exhaustive and the resulting algorithm is also restricted to discrete action domains. More comments see below:\n\n- The paper consists of 2 parts, the average behavior policy and its value function and the safe policy improvement step. The relation between these two parts are not clear. Is the policy improvement step only working if the policy is defined as in section 4 and the value function computed as in section 4? \n- Proposition 4.2 needs to be much better motivated and explained. It is totally unclear at this part of the paper why proposition 4.2 is used.\n- Please explain why proposition 4.2 indicates that Q^D \\approx Q^\\beta\n- The selection type of S in the taxi example is also unclear.\n- How would we solve Equation 8 with continuous actions / parametrized policies \\pi? Without this extension, the algorithm is quite restricted. \n- the figure captions need to be much more exhaustive. I am not sure I understand the x axis of Figure 4 (right). What iterations are shown here? We only do one improvement step of the behavior policy, without any resembling, is that right?\n- Could we also use a similar policy update for policy improvement in reinforcement learning?\n- Could you add an algorithm box for estimating the Q-function? Do we estimate every Q-function in isolation using MC estimates and then just use the weighted average?\n- It would be interesting to also compare the value function learning method proposed in the paper in isolation to other value function learning methods such as DQN. while the presented method is simple (learn from MC estimates), this is also known to be very data inefficient.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Contribution to safe RL with a weak empirical validation",
            "review": "The paper plugs the ideas of TRPO/PPO into the value based RL. Though there is no big surprise in terms of the tools used, this is interesting to know that safe policy improvement is possible in this setting. \n\nNevertheless for a conference as ICLR which is interested in the performance of ML tools, I have two concerns:\n\n- The scores obtained on all tests tasks on Atari game are quite far from the state of the art. As an example OpenAI announced to be able to score 74k at Montezuma's revenge with a single demonstration using PPO and a carefull selection of the initializations states (see  blog post https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/). I understand that the setting is not directly comparable but the goal of RL is to learn good policies. This remark would vanish is the authors could come with a real use case where for some reason their approach is the best performer.\n\n- The proposed approach is benchmarked wrt few algorithms while there exist a lot in the safe RL literature. The setting is often slightly different but adaptation is often possible. In particular I'd like more positioning wrt what is proposed by the work of Petrik&all (https://papers.nips.cc/paper/6294-safe-policy-improvement-by-minimizing-robust-baseline-regret.pdf the paper is cited but the first author is incorrect). What are the deep differences that make this paper setting more interesting (in terms of what can be done from an applied perspective) or more challenging in terms of mathematical tools. Here I feel the core difference is a comparison against an average of policies which becomes the new baseline to beat.\n\nAlso not that at EWRL'18 an alternative approach for value based safe RL was presented https://arxiv.org/pdf/1712.06924.pdf\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Assumption in this paper significantly deteriorate the significance of the results",
            "review": "In this paper, the authors study the problem if learning for observation, a reinforcement learning setting where an agent is given a data set of experiences from a potentially arbitrary number of demonstrators. The authors propose a method which deploys these experience to initialize a place. Then estimate the value of this policy in order to improve it.\n\nThe paper is well written and it is easy to follow. \n\nMost of the theoretical results are interesting and the derivations are kinda straightforward but not fully matching the main claim in the paper. Mainly the contribution in this paper heavily depends on an assumption that Q^D and Q^\\beta are close to each other. This assumption simplifies the many things resulting in a simple algorithm. But this assumption is too strong while the main challenge in the line of learning from observation comes from the fact that this assumption does not hold. Under this assumption and the similarity in distributions mentioned in proposition 4.2 make the contribution of this paper significantly weak.\n\nPlease let me know if you do not actually use this assumption in your results and justification.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}