{
    "Decision": {
        "title": "Withdrawn paper shows up as pending task on my slate",
        "metareview": "Paper was withdrawn before decision. ",
        "recommendation": "Reject",
        "confidence": "5: The area chair is absolutely certain"
    },
    "Reviews": [
        {
            "title": "Interesting paper; needs more comparisons, methodological development",
            "review": "Quality: the paper is overall okay, but I would like to see significantly more comparisons and methodological development.\n\nClarity: the paper is mostly clear, though the presentation could be improved in various parts, as I mention below.\n\nOriginality: as far as I am aware the approach itself is novel, but I am having trouble identifying a core methodological advance that you have made.\n\nSignificance: multivariate time series forecasting and spatiotemporal forecasting is important, and you have clearly made a contribution in this area, but I would like more development. I will try to make some suggestions about where to go next below.\n\nSuggestions for improvements:\n\n1) I know the use of the term \"context\" is standard in parts of ML, but I have never been convinced that there is a truly new concept here that cannot be addressed using existing statistical ideas: coding \"context\" simply as indicator variables (covariates) is an obvious starting point, even better to pursue hierarchical modelling (i.e. fixed, random, and mixed effects). Stated another way: in the case of actual time series models, I don't see how \"context\" is a different concept than covariates, which are of course a standard part of time series modelling. And in the case of spatial models, there are a number of approaches which you do not mention (CAR and Gaussian processes being the leading approaches) which explicitly model spatial dependence. \n\n2) Your related work section could be significantly improved to make it clearer what your contribution is.\n\n3) Following up on point #1 above, your data is fundamentally a set of counts observed over space and time. Various appropriate statistical models exist for handling this (VAR with a Poisson or Negative Binomial likelihood, GP regression with a Poisson / NegBinom likelihood, CAR, etc.); certainly there is room to improve all of these statistical methods. And switching from a distribution appropriate for count data (e.g. Poisson) to the squared error loss (aka Gaussian) is potentially a reasonable approximation with high counts. However, I think you need to make this case, at least by mentioning other approaches and possibly by explicitly comparing. Perhaps none of them are scalable! Or your neural net does much better! But we don't know...\n\n4) Spatial RNNs have been proposed before--perhaps you are doing something that distinguishes your approach? You should state this.\n\n5) Returning to the idea of hierarchical modelling---there is a large amount of information that you could be building into the model, for example the data presumably has very marked periodicity throughout the day. Similarly, weekdays are most likely similar to each other, and so are weekends. I do not see any of this being built into the model. Maybe that's ok, you have enough data to learn this, but I would at least like to see plots showing that indeed your model is capturing these important features.\n\n6) How can I interpret the RMSEs? I suppose that after you scaled everything to [0,1], fit the model, and made your predictions, you rescaled back to the true values and calculated RMSE? Thus a baseline RMSE of 32 means that in a 15 minute window the units of 32 are in number of people entering the subway? Reducing this from 32 to 25 is certainly an improvement; were half a million parameters really necessary to do that? A slightly more realistic baseline (i.e. one that takes yesterday's level of activity into account) would be helpful. Your claim is that the model is robust to unseen values, but I'm not sure what unseen values you're referring to. You also claim your model can handle the anomaly on 4 November, but in preprocessing your data you excluded 15 days that were anomalous.\n\n7) Your random train / test split is stratified based on day of week. But since you are randomly leaving out days, your results may be overoptimistic in the fact that a given day that is left out of the data is similar to other days that are left in, i.e. the day right before and right after. This may not matter as much as you're not training a standard time series setting, where interpolation might be very easy but forecasting might be hard. Nevertheless it would be useful to try a harder train/test split, leaving out blocks of, say, 3 days.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper for industrial engineering conference",
            "review": "The authors considered the problem of subway traffic forecasting, i.e. the data has temporal and spatial dimensions.\nThey propose to embed the current data point into the latent space and the, using the decoder, perform the prediction.\nThey considered several variants of this scheme:\n- construct a separate RNN for each subway station\n- perform prediction for all stations simultaneously to take into account dependencies between traffic on different stations\n- take into account a special context variable, characterizing object (subway station in our case) when performing embedding. The authors claimed that these additional variables responsible for the embedding of the object, can be learned simultaneously with the model itself and significantly improves prediction accuracy\nThe authors performed computational experiments using subway traffic data.\n\nComments:\n- the authors concentrated on a very particular engineering problem, related to traffic prediction. Even the notations/indexes, used for the algorithm description, are interpreted in terms of the traffic prediction problem\n- the text is written in such a way, that it is clear, that the main aim of the paper is to solve a particular engineering problem. In case the authors claim that they develop a general algorithm, they should compare it with other analogous approaches on different datasets. But this is not the case for the considered paper\n- the idea to embed to a latent space and then perform prediction in the latent space is not new. E.g. in [3] they motivate such approach by a Koopman operator theory, originated from theoretical physics. In [2] they used exactly such idea to construct a general scheme for approximation of dynamical systems, and then the authors of [2] demonstrated applicability of their approach on different datasets and problems. In [1] to construct the latent space they used matrix factorization, and then simple linear AR model to prediction in the latent space\n- taking into account the previous comment, the idea to introduce a context looks not very fundamental, rather as some simple engineering trick\n- as the baseline the authors considered only prediction by an average data values given their context. However, for such problems (traffic prediction) it is widely accepted that XGboost + feature engineering is the must-to-use tool (see numerous Kaggle competitions). Moreover, for time-series usually it is very difficult to get better results than results, obtained when predicting by a previous time-series value. I think that baselines provided by the authors are too weak. They should add more methods for comparison including those from [1-3]\n- the authors did not explained sufficiently clear how they tuned embeddings for the context\n\nConclusion:\n- I think that the paper, being well and clearly written, is better suited for applied industrial conferences than for a general academic conference such as ICLR, which is mainly about developing general concepts and algorithms\n\n[1] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. “Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction”. In: Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee et al. Curran Associates, Inc., 2016, pp. 847–855. URL: http://papers.nips.cc/paper/6160-temporal- regularized-matrix-factorization-for-high- dimensional-time-series-prediction.pdf.\n\n[2] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. 2015. Embed to control: a locally Linear Latent dynamics model for control from raw images. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'15), C. Cortes, D. D. Lee, M. Sugiyama, and R. Garnett (Eds.), Vol. 2. MIT Press, Cambridge, MA, USA, 2746-2754.\n\n[3] Bethany Lusch, J. Nathan Kutz, Steven L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. https://arxiv.org/abs/1712.09707",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A preliminary work of context-aware forecasting method",
            "review": "This paper proposes a context-aware forecasting method by learning embedding exogenous features within recurrent neural networks, and applies it in forecasting tap-in logs in subway network. Results show that using exogenous features in embedding vectors improves the performance. It is happy to see the transportation application of deep learning. However, the current paper is preliminary. The detailed comments are as follows.\n\n1. A lot of works have been done on applying varies of spatio-temporal features in neural networks for time series forecasting in the literature. The authors are suggested to compare with such methods.\n \n2. In experiments, 15 days out of the total 92 days are pulled out due to disturbed traffic pattern and extreme values are truncated by a bound. This makes the problem easier. However, in real world application we cannot simply do this. This makes the results not convincing and also makes me worry about the real application of the proposed algorithm.\n\n3.  The authors only compare the proposed algorithm with some naive baseline, such as averaged historical values. To show the advantage of this method, more state-of-the-art forecasting methods should be compared.\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}