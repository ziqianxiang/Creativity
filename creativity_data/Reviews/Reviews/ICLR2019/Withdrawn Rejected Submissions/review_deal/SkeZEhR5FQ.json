{
    "Decision": "",
    "Reviews": [
        {
            "title": "The authors proposed an end-to-end trainable framework for joint representation learning and graph decomposition. The proposed method achieved on-par performance consistently across classes.",
            "review": "The paper is very well written and the motivation of the end-to-end trainable framework for graph decomposition problem is very clear and strong. The proposed method is also fairly novel, especially the decision to convert the discrete optimization problem to an unconstrained binary cubic problem. \nFinally, the proposed method applied to human pose estimation achieves on par performance with the state-of-the-art performance on the MPII Human Pose dataset.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable approach to an important problem, but missing baselines, novelty a bit low",
            "review": "The submission describes a way to differentiate through the solution of a minimum cost multicut problem, thus enabling a multicut solver to be used as a module in a neural network, with applications to segmentation tasks in computer vision.  The method starts by approximating the solution to the multicut problem with an unconstrained, penalized version.  This objective is then identified with the log-likelihood of a CRF, at which point the standard trick of differentiating through a small number of mean field updates is applied.  Experiments show modest gains on the task of multiple-human pose estimation.\n\nPros:\n+ Since the minimum multicut problem is a nice abstraction for many computer vision problems, being able to jointly train multicut inference with deep learning is a well-motivated problem\n+ It is a nice observation that the multicut problem has a tractable approximation when the graph is fully connected, which is a useful case for computer vision\n+ The proposed penalized approximation of the problem is simple and straightforward to try\n+ Experiments indicate that the method works as advertised\n\nCons:\n+ There are other perhaps more straightforward ways to differentiate through this problem that were not tried as baselines\n+ Experimental gains are very modest compared to an independently-trained CRF\n+ Novelty is a bit low, since the trick of differentiating through mean-field updates is well-known at this point\n+ Efficiency: the number of variables in the CRF scales with the square of the number of variables in the original graph, which makes this potentially intractable for large problems\n\nDifferentiating the minimum-cost multicut problem is well-motivated, and I think the multiple-human pose estimation problem is a nice application that seems to fit this abstraction well.  It is also not hard to imagine many other computer vision problems where integrating a multicut solver would be useful.  The proposed solution is straightforward.  The experimental results for the pose estimation problem show that the method can be applied to a real-world problem, though the gains are pretty modest.\n\nOne important point that is sort of glossed over in the paper is that the proposed solution is only tractable for complete graphs, since it relies on enumeration of all the chordless cycles in the graph.  This does not seem to be a huge limitation for typical computer vision problems, and it is kind of an interesting observation that this is an instance where the ostensibly harder problem (complete graphs) is easier than the ostensibly simple problem (sparse graphs).  On the other hard, the efficiency argument is a double-edged sword.  Although all chordless cycles can be enumerated in a complete graph, there are O(N^2) edges in a complete graph, which means that the complexity scales at least as N^2, where N is the number of nodes in the graph.  If I’m not mistaken, mean-field updates would then cost O(N^4)—since we must do one update for each of the N^2 edges, and each edge variable is involved in O(N^2) cliques—which would not be feasible for many computer vision problems.  A frank discussion of computational efficiency with some complexity analysis should be included.\n\nAnother significant shortcoming is that this may not even be the most straightforward way to solve this problem.  Since the minimum-cost multicut problem has a straightforward linear programming relaxation, methods such as [A,B] (see citations below) could be used as a straightforward way to differentiate through the solution of the problem.  Furthermore, these methods could potentially leverage existing solvers, and thus be more efficient and accurate.  So, not discussing these methods or comparing to them as baselines seems like an omission.\n\nThe experiments are also a bit disappointing.  First, the MNIST experiments seem a bit strange.  I don’t really understand the motivation for using the validity of the edge labeling as loss function.  In any real problem, we wouldn’t be interested in the accuracy of inference—we would be interested in the task loss.  Using the accuracy of inference as a loss seems odd.  Also, presumably, we can get more accurate inference trivially, by increasing the penalties (gamma), so it is unclear whether this is learning anything useful.\n\nThe human pose experiments are much more interesting, as that problem is perfectly well motivated.  However, the results are disappointing, since the mean AP improvement over the ‘post-processing’ CRF seem very small.  Standard error bars should probably be computed for these metrics.  I’m afraid that these small improvements may not be significant.  Also, the difference between tables 4 and 5 is unclear.  Table 4 is not referenced in the paper (I guess the last ref. to table 5 on page 7 should refer to table 4).  Is table 4 showing validation set results, whereas table 5 is showing test set results?  This is not clear.\n\nOverall, I feel that this is a reasonable approach to solve a problem with wide-ranging applications.  However, the novelty of the solution is a bit on the low side, and one could argue that other ways of differentiating through optimization problems [A-B] are important missing references and baselines for the experiments.  Since the experimental results are also not that compelling compared to even the independent CRF and efficiency could also be a problem, I’m afraid potential impact may be limited at this time.\n\n[A] Amos, Brandon, and J. Zico Kolter. \"Optnet: Differentiable optimization as a layer in neural networks.\" arXiv preprint arXiv:1703.00443 (2017).\n[B] Schulter, Samuel, et al. \"Deep network flow for multi-object tracking.\" Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Less Novel and Lacking Baseline",
            "review": "In this paper, authors propose a deep structured model to solve the problem of learning graph decomposition. They formulate graph decomposition as a minimum cost multi-cut problem which can be regarded as the MAP problem of a properly designed high-order CRF. Authors further approximately solve the MAP by unrolling the mean-field inference. Experiments on MNIST and MPII Human Pose dataset show that the proposed method is promising.\n\nStrength:\n\n1, The idea of formulating the cycle constraint in the minimum cost multi-cut problem as a high order energy function is inspiring.\n\n2, The human pose estimation is an interesting application for the learning based graph decomposition method.\n\nWeakness:\n\n1, How does the formulation of binary cubic programming in Eq. (3) and (4) exactly relate to the energy formulation in Eq. (5)? Especially, it is not clear how the third order term in Eq. (4) is used in the high order term of CRF. Moreover, there is no explanation of what “the set of recognized label configuration” is and how it is constructed in the experiments. Since the high order term is one of the main contribution of the paper, I do think authors need to clarify in a detailed manner.\n\n2, This paper targets at learning graph decomposition. Authors did not introduce the problem setting in the beginning and directly discussed the minimum cost multi-cut formulation. I would imagine there are other formulations for decomposing the graph like spectral clustering. However, I did not see any discussion or comparisons of these alternative formulations. In particular, in the first experiment, since you are doing clustering of images, why not compare your method with some baselines of clustering methods which use the same deep feature?\n\n3, Mean field inference as an approximated MAP solver is itself very crude although it is easy to implement as a RNN. Authors should at least compare it with other MAP solvers like max-product to gain a better understanding of the approximation quality. Also, from Table 2, it is hard to tell whether the mean-field converges or not. It is better to plot the free energy vs. inference iteration.\n\n4, Some closely related work is not discussed. The following paper develops a message passing algorithm for solving the same inference problem.\n\nSwoboda, Paul, and Bjoern Andres. \"A message passing algorithm for the minimum cost multicut problem.\" In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, vol. 3, no. 6. 2017. \n\nOverall, I would like to hear authors’ feedback.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}