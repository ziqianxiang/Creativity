{
    "Decision": {
        "metareview": "This paper attempts to answer its suggestive title by arguing that this generic lack of invariance in large CNN architectures is due to aliasing introduced during the downsampling stages. \nThis paper received mixed reviews. Positive aspects include the clarity and exhaustive empirical setups, whereas negative aspects focused on the lack of substance behind some of the claims. Ultimately, the AC took these considerations into account and made his/her own assessment, summarized here.\n\nThe main claim of this paper implies the following: modern CNNs are unable to build invariance to small shifts, but somehow are able to learn far more complex invariances involving lighting, pose, texture, etc. This must be empirically verified beyond reasonable doubt, and the AC thinks that the current experimental setup does not achieve this threshold. As mentioned by reviewers and by public comments, the preprocessing pipeline is a key factor that may be confounding the analysis, and this should be better analysed. For example, as mentioned in the reviews below, the shift in the image can be either done by inpainting, cropping, or using a fixed background. The authors claim that there are no qualitative differences between those preprocessing choices, but by inspecting Figures 2B and 8C, the AC notices a severe change in 'jaggedness'; in other words, the choice of preprocessing *does* affect the quantitative measures of (un)stability, even though the qualitative assessment (unstable in all setups) is the same. In particular, using non-centered crops should be the default setup, since it requires no preprocessing. It is confusing that it appears in the appendix instead of the inpainting version of figure 2b. This is important, since it implies that the analysis is mixing two perturbations: the actual action of the translation group and the choice of preprocessing, and that the latter is by no means negligible. I would suggest the authors to perform the following experiment to disentangle the effect of translation by the effect of preprocessing. Since the translation forms a group, for any shift applied to the image, one can 'undo' it by applying the inverse shift. Say one applies a shift to image x of d pixels and obtains x'=T(x,+d) as a result (by using whatever border handling procedure). If border effects were negligible, then x''=T(x',-d) should give us back x, so a good measure of how unstable the network is is to measure the difference in prediction between x,x' and x''. If predicting x'' is as unstable as predicting x', it follows that the network is actually unstable to the border effect introduced by T. \n\nGiven this, the AC recommends rejection at this time, and encourages the authors to resubmit their work by addressing the above point. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "empirical study of invariance on modern CNNs"
    },
    "Reviews": [
        {
            "title": "Critical topic, but limited novelty and results",
            "review": "This paper studies the lack of shift invariance in state-of-the-art neural networks, namely, the paper introduces results that show that state-of-the-art deep neural networks are affected by 1-pixel shifts because the convolutional layers in the network poorly sample the feature maps. The topic addressed in the paper is critical for many computer vision systems, as lack of shift invariance is a catastrophic failure mode. The arguments of the paper help clarifying why the networks are so sensitive to small shifts of the objects (poor subsampling) but generalize well (there is a bias in the location of the objects in the dataset).  Both of these arguments and the sensitivity of the networks to small shifts are well known in the literature, but it is great to see a paper that puts them together and tests these arguments in state-of-the-art deep nets for ImageNet.\n\nHowever, the paper could do a much better job providing evidence to support the arguments:\n\n*Few quantiative results on the sensitivity to 1-pixel shift, most results are qualitative. This makes hard to assess whether the reported results are \"accidents\" found in certain images or are general. The results that support that 1-pixel shifts affect state-of-the-art neural networks are in Figure 2b. Yet, these results are unclear, eg. is \"400 Jaggedness\" a lot?, what is the size of the embedded image?, How are the 100 images selected? Is the network performing well in those images? How does the size of the embedded image change the \"Jaggedness\"? \n\n*Something that could help to strengthen the results would be to add networks with better sampling + larger pooling regions and see how this solves the lack of shift invariance. Now it has only been tested increasing the pooling regions, which misses the main point of the paper. Also, the results on these networks with larger pooling regions, are all qualitative.\n\n*The mathematical proof is done for average pooling, which is rarely used nowadays. I would suggest using max pooling. Also, the aforementioned experiment in which the size of the pooling region is increased, is it max pooling or average pooling?\n\n*Limited results on the ImageNet bias. These results are reported in one image category (Figure 5), how general are them?\n\n*The paper assumes that shifting an image embedded an object in a black background is equivalent to shifting an object in a static background. A hypothesis would be that the embedding of the image in the black background creates artificial boundaries that make the network more fragile to 1-pixel shifts than for natural images.\n\nIn summary, I think this is a paper that may arise a lot of interest, although the different arguments are known and the experiments are poorly executed.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice empirical study of invariants in modern CNNs, with quantitative support at all key points.",
            "review": "This paper describes an empirical study of translation and scale\ninvariance properties of modern CNN architectures. The authors conduct\na thorough study of translation invariance in VGG16, ResNet50, and\nInceptionResNet with respect to the Nyquist frequency and shift-\nversus translation-invariance properties of network layers as a\nfunction of depth and subsampling rate. Empirical observations are\nquantified using a variety of metric to measure the stability of\nfeature maps under geometric transformations of the input.\n\nThe paper has the following strong points:\n\n 1. It tells an interesting (and engaging) story about a largely\n    empirical study, and while doing this never pretends to be more\n    than it is.\n 2. Empirical observations are supported by quantitative measures that\n    give compelling evidence for most observations in the paper. The\n    discussion about shiftability versus translatability is\n    particularly interesting with its link to nonlinearity, smoothing\n    and Nyquist limits.\n\nThe paper has the following weak points:\n\n 1. The reliance on inpainting for almost all experiments is somewhat\n    worrying. It is not clear that this procedure isn't introducing\n    its own biases affecting translation and scale invariance. The\n    authors make reference to a separate protocol reported in the\n    appendices, but it isn't clear which results in the appendices\n    they are referring to. A more thorough control study seems in\n    order to verify that inpainting is a reasonable simulation.\n 2. Some figures are scaled down to the limits of legibility.\n\nIn summary: I like this paper a lot, and I think it adds useful\nelements and analytical tools (both theoretical and empirical) to the\ndiscussion on invariants in modeern CNNs.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Although the general idea is interesting, experimental evaluation is not convincing. Similarly, some explanations like the photographer's bias as reason for susceptability to very small image transformation is not entirely convincing. ",
            "review": "Paper summary: \n\nAs is made clear in the title, this paper sets out to answer the following question: “Why do deep convolutional networks generalize so poorly to small image transformations?”. It focuses on natural image transformations on translation and scaling (rotation is missing though).\n\nThe paper proposes two main explanations: \n-\tStrided convolution, called subsampling in the paper, ignores the classical sampling theorem,\n-\tCNNs will not learn invariance because of the (photographers') biases contained in the datasets.\n\nOn a general level, the paper is a good read, it is well written and the figures clearly convey the message they’re intended to. Adversarial attacks and robustness of CNNs in general is a very interesting and important topic in ML. The originality of this work is in the approach of the problem, the paper tries to explain the reasons why CNNs are vulnerable. Related works put more emphasis on coming up with novel attacks/defense strategies. Considering natural attacks as done in this submission is particularly interesting as it is probably a more surprising shortcoming of CNNs compared to optimally designed attacks or highly unnatural perturbations. The argument about subsampling (stride) being the reason of not having translational invariance is nice, especially the theoretical insight with the Shannon-Nyquist theorem and the more figurative example on part detectors. There are nevertheless a few major concerns about this work:\n\nMajor Concerns:\n\nTheoretical arguments:\nThe theoretical argument made in this paper is interesting but to make the point stronger a more in-depth explanation would be needed.\n-\tThe step from Eq (2) to Eq (3) is not entirely clear “K does not depend on x_i”, maybe one extra sentence to explain this step would be useful. \n-\tTerms introduced such as the basis function B and the set of transformations T could be better defined.\n-\tFor the extension to other types of transformations “While the claim focuses on global translation, it can also be extended to piecewise constant transformations.” it would be important to point out what type of natural transformations can be included in this set.\n\nEmpirical evidence:\nExperiments are not fully convincing. Additional empirical evidence would be beneficial and necessary to support the claims of this:\n-\t“A natural criticism of these results is that they are somehow related to the image resizing and inpainting procedures that we used.” This is a very good point and the authors following arguments are not fully convincing. Results with different transformation procedures mentioned in the rest of the paragraph (and probably more) should be included to convince the reader.\n-\tThe theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification. In fact, feature maps of the CNNs that the authors consider do indeed contain many high frequencies.\n-\tThe argument made in part 4 about the photographer’s bias seems valid for general natural transformations, but it does not apply to small transformations such as 1-pixel translations presented in the paper. Also, evidence that datasets without (or less) photographers' bias are less susceptible to natural attacks would make the argument in the paper a lot stronger. \n-\tWhen using 6x6 avg pooling for the VGG16 architecture ”recognition performance decreases somewhat” . Results are only preliminary in the paper, but this statement needs a more thorough experimental backing. It should come with convincing quantitative evidence.\n-\tPlease include some results or citation on other work about test time augmentation to support the statement “still only provides partial invariance”.\n\nReferences and phrasing:\nGenerally previous work is well referenced in this paper, although there are some formulations that can be slightly modified to make a clear distinction between what is novel and what is previous work:\n-\tAs is very well shown in the introduction, there is a lot of work on generating adversarial examples that drastically change the output of a CNN. This should be made clear in the abstract, in fact the sentence “In this paper we show that modern CNNs [...] also happens with other realistic small image transformations”  seems to indicate that this is the novel work in the paper. This is also why I believe the first sentence “Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations.” is somewhat contestable. \n-\t“We find that modern deep CNNs are not invariant to translations, scalings and other realistic image transformations” as the paper points out earlier this is not a novel finding, so I would use a formulation that makes that clear and gives more emphasis to your own arguments as of why this happens.\n\nFurther Comments :\n-\tPart 5 \"Implications for Practical Systems\" could be moved to discussion as there is no new point and it seems more a reflection on what was already stated.\n-\tThe final sentence of the abstract “Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.” is not necessary, this is clearly true but it isn’t really contested in the ML community.\n-\t“despite the architecture being explicitly designed to provide such invariances” I agree that this has motivated the use and design of CNNs in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.\n-\t”jaggedness is greater for the modern, deeper, networks compared to the less modern VGG16 network” might be worth interesting to consider if the residuals have anything to do with it.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}