{
    "Decision": {
        "metareview": "The paper formulates the problem of unsupervised one-to-many image translation and addresses the problem by minimizing  the mutual information.\n\nThe reviewers and AC note the critical limitation of novelty and comparison of this paper to meet the high standard of ICLR. \n\nAC decided that the authors need more works to publish.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Lack of novelty"
    },
    "Reviews": [
        {
            "title": "Good formulation, but not novel and short comparison",
            "review": "==== After rebuttal === \nI thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score. \n====================\n\nUnsupervised image-to-image (I2I) translation is an important issue due to various applications and it is still challenging when applied to diverse image data and data where domain gap is large. This paper employs a neural mutual information estimator (MINE) to deal with I2I translation between two domains where there is a large gap. However, this paper contains several issues.\n1. Pros. and Cons.\n   (+) Mathematical definition of I2I translation\n   (+) Application of mutual information for conserving content.\n   (-) Lack of comparison with recent I2I models\n   (-) Lack of experimental results and ablation studies \n   (-) Unclear novelty\n2. Major comments\n   - The novelty of this paper is not clear. Excluding the mathematical definition, it seems that the proposed TI simply combines DCGAN and MINE-based statistical networks. For clarifying the novelty, the detailed architecture and final objective functions can be helpful. \n   - Recent works on unsupervised I2I translation are omitted including UNIT [1], MUNIT [2], and DRIP [3]. Also, the authors need to clarify the main difference of TI-GAN from comparing models.\n   - It is not clear to relate the mathematical definition of domain transfer to one-to-many translation within large domain gap. \n   - It is not clear how to use mutual information (MINE) for learning. There is no explicit definition of loss function considering MINE term. \n   - It is short of comparing other state-of-the art models such as UNIT, MUNIT, DRIP, and AugCycleGAN. They compared their results with CycleGAN only.\n   - Experiments are not enough to support the authors’ insist. There is not any quantitative metric or qualitative result on generating edge-to-shoes. \n   - It is difficult to read due to inconsistent usage of terms (e.g., Figure 3 and 4 (c)s)\n   - For better understanding, it requires to compare the patterns of MINE loss and adversarial loss. \n   - Experiments on more datasets such as animal, season, faces or USPS datasets. \n   - What is the main difference in the results between DCGAN-based and UNet-based models?\n\n\nMinor\n   - cicle_times symbol looks the product between distribution. But it should be defined before being used.\n   - A reference of CycleGAN is incorrectly cited. \n   - There are some typos in the paper.\n   - page 1: dependent → depend\n   - page 3: by separate → by separating\n   - page 6: S a I → S and I\n\n\n1. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks, CoRR, abs/1703.00848, 2017\n2. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, Multimodal Unsupervised Image-to-Image Translation, CoRR. abs/1804.04732\n3. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, Diverse Image-to-Image Translation via Disentangled Representations, ECCV 2018.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good problem formulation, Not Novel method.",
            "review": "This paper formulated the problem of unsupervised one-to-many image translation and addressed the problem by minimizing  the mutual information. A principle formulation of such problem is quite interesting. However, the novelty of this paper is limited. The proposed the method is a simple extension of InfoGAN, applied to image-to-image translation and replacing the mutual information part with MINE.\n\nThe experiments, which only include edge to shoes and MNIST to SVHN, are also not comprehensive and convincing. This paper also lacks discussion of several quite important related references for one-to-many image translation.\n\nXOGAN: One-to-Many Unsupervised Image-to-Image Translation\nToward Multimodal Image-to-Image Translation\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice problem formulation but limited model novelty and comparisons.",
            "review": "This paper formalizes the problem of unsupervised translation and proposes an augmented GAN framework which uses the mutual information to avoid the degenerate case.\n\nPros:\n* The formulation for the problem of unsupervised translation is insightful.\n* The  paper is well written and easy to follow.\n\nCons:\n* The contribution to the GAN model of this paper is to add the mutual information penalty (MINE, Belghazi et al., 2018) to the GAN loss, which seems incremental. I also wonder if some perceptual losses or latent code regression constraint used in previous works [1,2] can also achieve the same goal.\n* Comparison to “Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data” should be done, since it’s a close related work for unsupervised many-to-many image translation.\n* The visualization results of TI-GAN, TI-GAN+minI, CycleGAN should be listed with the same source input for fair and easy comparison. For example the failure case of figure 8 mentioned in Section 5.2 only appears in Figure 5 (1) not in Figure 5 (2). \n* Minor issues: 1) What does the full name of “TI-GAN” ? 2) Figure 6 is not mentioned in the experiments. 3) What does the “Figure A” mean in Section 4.2 ?\n\n[1] Multimodal Unsupervised Image-to-Image Translation, ECCV’18\n[2] Diverse Image-to-Image Translation via Disentangled Representations, ECCV’18\n\nOverall, this paper proposes a nice formulation for the problem of unsupervised translation. But the contribution to the GAN model seems incremental and comparisons to other methods are not enough. My initial rating is rejection.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}