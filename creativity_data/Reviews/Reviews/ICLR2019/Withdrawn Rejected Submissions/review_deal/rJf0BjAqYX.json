{
    "Decision": {
        "metareview": " The paper presents a sensible algorithm for knowledge distillation (KD) from a larger teacher network to a smaller student network by minimizing the Maximum Mean Discrepancy (MMD) between the distributions over students and teachers network activations. As rightly acknowledged by the R3, the benefits of the proposed approach are encouraging in the object detection task, and are less obvious in classification (R1 and R2).\n \nThe reviewers and AC note the following potential weaknesses:\n(1) low technical novelty in light of prior works “Demystifying Neural Style Transfer” by Li et al 2017 and “Deep Transfer Learning with Joint Adaptation Networks” by Long et al 2017 -- See R2’s detailed explanations; (2) lack of empirical evidence that the proposed method is better than the seminal work on KD by Hinton et al, 2014; (3) important practical issues are not justified (e.g. kernel specifications as requested by R3 and R2; accuracy-efficiency trade-off as suggested by R1); (4) presentation clarity.  \nR3 has raised questions regarding deploying the proposed student models on mobile devices without a proper comparison with the MobileNet and ShuffleNet light architectures. This can be seen as a suggestion for future revisions. \n\nThere is reviewer disagreement on this paper and no author rebuttal. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors did not respond to the concerns of the reviewers. \nAC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Need to improve clarity and interpretability",
            "review": "This submission proposes a novel loss function, based on Maximum Mean Discrepancy (MMD), for knowledge transfer (distillation) from a teacher network to a student network, which matches the spatial distribution of neuron activations between the two.\n\nThe proposed approach is interesting but there is significant room for improvement. In particular:\n\n*Clarity*\n\nIt is not clear how the distribution of neuron activation are matched between the teach and student network. The C_T and C_S are not defined specifically enough. Does it include all layers? Or does it only include a specific layer (such as the last convolution layer)?\n\n*Interpretability*\n\nSection 4.1. tries to interpret the approach but it is still not clear why matching distribution is better. The MMD loss proposed could run into problem if the classification task does not involve \"spatial variation\". For example, for a extremely simple task of classifying three classes \"R\", \"G\" and \"B\" where the whole image has the same color of R, G and B respectively, the spatial distribution is uniform and the proposed MMD loss would be 0 even if the student network's channels do not learn discriminative feature maps. Another example is when a layer has H=W=1.\n\n*Significance*\n\nThe experiment shows that polynomial-two kernel gives better result, but Sec. 4.3.2. mentions that it is equivalent to Li et al. (2017b) in this case.\n\n*Practical usefulness not justified*\n\nIn the experimental section, the student network's number of parameters and FLOPS are not detailed, so it is unclear how much efficiency gain the proposed method achieved. Note in practice small networks such as MobileNet and ShuffleNet have achieved significantly better accuracy-efficiency trade-off than the teacher networks considered here (either for CIFAR10 or for ImageNet1k).\n\n*Improvement not significant*\n\nThe results obtained by the proposed approach is not very significant compared to \"KD\" along.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear presentation, results and novelty",
            "review": "This paper targets knowledge distillation of a large network to a smaller network. The approach is summarized by equations (3) and (4), which in short proposes that one should use the maximum-mean-discrepancy (MMD) of the network activations as a loss term for distillation.  \n\nWhen considering CIFAR image classification tasks, it is shown that only when using a specific quadratic polynomial kernel (which as described in https://arxiv.org/pdf/1701.01036.pdf is tantamount of applying neural style transfer) the proposed approach is able to match the performance of the seminal paper of Hinton et al.  When embarking to imagenet, the proposed approach is only able to match the performance of standard knowledge distillation by adding the quadratic term (texture in neural style synthesis jargon). This is actually a sensible proposal. Yet, the claims about MMD as a way of explaining neural style transfer has appeared in the paper cited above, which the authors mention.\n\nThe idea of transferring from one domain to another using MMD as a regularizer appeared in https://arxiv.org/pdf/1605.06636.pdf by Long et al --- indeed equation (3) of this paper matches exactly equation (10) of Long et al. Note too that Long et al also discuss what kernels work well and which work poorly due to vanishing gradients and propose parametrised solutions. This is something this paper failed to do.\n\nThe two works cited above make me wonder about the novelty of the current paper.  In fact, this paper ends us being an application of the neural style transfer loss function to network distillation. As such this could be useful, if not already done by someone else previously.\n\nI find that the paper is poorly written, with many typos, and lacks focus on a single concrete story. The CIFAR experiments fail to use KD+NST (ie the thing that works for imagenet - neural style transfer) and section 5.3 appears trivial in light of the cited works. For all these reasons, I am inclined to reject this paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple method with good empirical results and some insightful discussion",
            "review": "This paper proposes a simple method for knowledge distillation. The teacher and student models are matched using MMD objectives, the author demonstrates different variants of matching kernels specializes to previously proposed variants of knowledge distillation.\n\n- The extensive evaluation suggests that the MMD with polynomial kernel provides better results than the previously proposed method.\n-  It is interesting to see that MMD based transfer has more advantage on the object detection tasks.\n- Can the author provides more insights into the behavior of different kernels, for example visualizing, the gradient map might help us to understand why certain kernel works better than another one?\n- Did you consider translation invariance or other spatial properties when designing your kernels?\n\nIn summary, this is an interesting paper with good empirical results. The technique being used generalization is quite straightforward, but the paper also includes a good amount of discussion on why the proposed approach could be better and I think that really helps the reader.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}