{
    "Decision": {
        "metareview": "This paper proposes to a simple method for  tuning parameters of HMC by maximizing the log density under the final sample of the MCMC, and apply it for training VAE. The reviews and discussion raises some critical concerns and questions, which unfortunately, which unfortunately, is not adequately addressed. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "issues remain to be addressed"
    },
    "Reviews": [
        {
            "title": "An interesting heuristic and some interesting derivations, but there's a gap between the two.",
            "review": "This paper proposes a simple heuristic for tuning HMC's parameters: just optimize the expected log-density of the Lth sample. It seems to work reasonably well on the problems the authors evaluate on.\n\nThis heuristic is arrived at by a somewhat roundabout derivation, which I found interesting (although many of the same ideas are implicit in Salimans et al. (2014; \"MCMC & VI: Bridging the Gap\")). But ultimately this derivation comes to a head at this very heuristic argument:\n\n“…since qL(zL; φ) converges to pθ(z|x) as L increases, we expect the effect of H[qL(zL; φ)] on φ to be small and that most of the similarity of qL(zL; φ) to pθ(z|x) will be captured by the first term in the RHS of (15). Therefore, we propose to tune φ by optimizing the tractable objective given by the first term…”: \n\nI don’t see why this argument applies to the entropy term and not to the log-joint term. In particular, If q_L has really converged to p(z|x), then there’s no point optimizing φ either, right?\n\nHere’s a concrete example of how I could imagine this procedure going wrong: make q(z0) a delta at the latent vector z* that maximizes the log-joint, and set the step size of the Hamiltonian simulation to 0. This will make the entropy term (which is ignored) -∞, maximize the log-joint term, and I think it even makes D^L_{KL}=0.\n\nIt seems like this isn’t what actually happens experimentally, though—perhaps I’m missing something?\n\nRegarding the experiments, a natural baseline would be something akin to the approach of Hoffman (2017; \"Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo”), simply initializing the HMC sampler with a mean-field Gaussian. I would expect this to produce worse results for small numbers of steps, since the variational Gaussian would choose a single mode, but I’m curious how the quantitative metrics would compare.\n\nSome more minor points:\n\n* “Variational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field\nVI with a Gaussian parametric distribuion and amortization.” I disagree with this terminology—DGMs trained with, say, IAF are routinely called VAEs.\n\n* Section 3.1: It might be good to clarify that you’re describing exact Hamiltonian integration, whereas in practice one always uses a discretized numerical integrator. (The leapfrog integrator is reversible and preserves volume, but doesn’t conserve energy, so this does make the results a bit more complicated.)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, missing some baselines and theoretical justifications",
            "review": "This paper presents an approximate bayesian inference method based on chaining measure preserving transformation with trainable parameters and optimizing for those using an ad-hoc objective based on a lower-bound on the likelihood.\n\nThe paper is clearly written and easy to follow. The proofs seem correct.\n\nIn terms of methods, I still have major questions:\n- The whole premise of the paper is based on chaining transformations that preserve the target density. However, in practice, you use a leapfrog operator without the Metropolis-Hastings step --what happens to the theoretical guarantees in that case? I'm guessing Eq (8), (9) don't hold anymore and neither does Theorem 1.\n- When swapping L for F, could you provide more justifications? You use the argument that p(z|x) ~= q_L so the effect of the entropy term will be negligible. It seems that if they are so similar for large L, why even train the \\phi? It also comes back to my first point that in your experiment, the transformations *do not* preserve the target density. \n- Regarding the use of measure preserving flow, I think it can be quite hurtful in certain settings -- a very simple example would be a mixture of two gaussians with vastly different variance. \n\nI think this paper also lacks recent references on training parameters for MCMC algorithms, most notably Song et al. (2017) and Levy et al. (2018). Both of these work seem quite related and should be mentioned and compared to. I would have also liked to see the authors contrast their work with Salimans (2015), especially the HVI part; is the main difference the reverse model?\n\nIn terms of evaluation, the toy distributions show that the method seems to converge to the right target but does not compare to either vanilla HMC, A-NICE-MC or L2HMC --which all guarantee asymptotic convergence. There should probably also be a mention of one of ESS/Auto-correlation/ESS per sec to get a sense of how helpful the method could be.\n\nFor the generative model experiments, I agree with the comments of AnonReviewer3 in that evaluating HMPF-VAE with AIS while evaluating HVI with IWAE is somewhat unfair as the latter can happen to be much looser. I also think a natural baseline to compare to would be Hoffman (2017) or Levy et al. (2018) where after obtaining an approximate posterior sample, these works run an MCMC algorithm before updating the decoder. The algorithms seem to be related (albeit the objectives are slightly different) and should be talked about I think.\n\nReferences:\n\nHoffman, Matt. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo, ICML 2017.\n\nSong, Jiaming et al. A-NICE-MC: Adversarial Training for MCMC, NIPS 2017.\n\nLevy, Daniel et al. Generalizing Hamiltonian Monte Carlo with Neural Networks, ICLR 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "simple idea that might be useful, but unnecessarily complicated exposition",
            "review": "This paper proposes training latent variable models (as in VAE decoders) by running HMC to approximate the posterior of the latents, and then estimating model parameters by maximizing the complete data log-likelihood. This is not a new idea by itself and is used e.g. as a baseline in Kingma and Welling's original VAE paper. The novelty in this paper is that it proposes tuning the parameters of the HMC inference algo by maximizing the likelihood achieved by the final sample in the MCMC chain. This seems to work well in practice and might be a useful method, but it is not clear under what conditions it should work.\n\nThe paper is written in an unnecessarily complicated and formal way. On first read it seems like the proposed method has much more formal justification than it really has. The discussion up to section 3.5 makes it seem as if there is some new kind of tractable variational bound (the ERLBO) that is optimized, but in practice the actual objective in equation 16 is simply the likelihood at the final sample of the MCMC chain, that is Monte Carlo EM as e.g. used by Kingma & Welling, 2013 as a baseline.  The propositions and theorems seem to apply to an idealized setting, but not to the actual algorithm that is used. They could have been put in an appendix, or even a reference to the exisiting literature on HMC would have sufficed.\n\nThe experiments do not clearly demonstrate that the method is much better than previous methods from the literature, although it is much more expensive. (The reported settings require 150 likelihood evaluations per example per minibatch update, versus 1 likelihood evaluation for a VAE). Also see my previous comments about evaluation in this paper's thread.\n\n- Please explain why tuning the HMC algo by maximizing eq 16 should work. I don't think it is a method that generally would work, e.g. if the initial sample z0 ~ q(z|x) is drawn from a data dependent encoder as in HVI (Salimans et al) then I would expect the step size of the HMC to simply go to zero as the encoder gets good. However in your case this does not happen as the initial sample is unconditional from x. Are there general guidelines or guarantees we can conclude from this?\n\n- The authors write \"Because MPFs are equivalent to ergodic Markov chains, the density obtained at the output of an MPF, that is, qL, will converge to the stationary distribution π as L increases.\"\n\nThis is true for the idealized flow in continuous time, but HMC with finite step size does generally NOT converge to the correct distribution. This is why practical use of HMC includes a Metropolis-Hastings correction step. You omit this step in your algorithm, with the justification that we don't care about asymptotic convergence in this case. Fair enough, but then you should also omit all statements in the paper that claim that your method converges to the correct posterior in the limit. E.g. the writing makes it seem like Proposition 2 and Theorem 1 apply to your algorithm, but it in fact they do not apply for finite step size. Maybe the statements are still correct if we take the limit with L->inf and the stepsize delta->0 at a certain rate? This is not obvious to me.\n\nIn practice, you learn the stepsize delta. Do we have any guarantees this will make delta go to zero at the right rate as we increase the number of steps L? I.e. is this statement from your abstract true? -> \"we propose a novel method which is scalable, like mean-field VI, and, due to its theoretical foundation in ergodic theory, is also asymptotically accurate\". (convergence of uncorrected HMC only holds in the idealized case with step size -> 0)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}