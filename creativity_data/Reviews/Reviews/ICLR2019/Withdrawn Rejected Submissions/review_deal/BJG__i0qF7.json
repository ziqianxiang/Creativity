{
    "Decision": {
        "metareview": "Strengths: Execution of paper well received. Results on new dataset. Convincing demonstration that the proposed approach learns good semantic representations.\n\nWeaknesses: Reviewers felt the positioning with prior work was not as strong as it could be. Reviewers wanted to have seen an ablation study.\n\nContention: Some general agreement among both the one positive reviewer and negative reviewer that the representation of prior work is skewed.\n\nConsensus: With two 5s and one 6 the numerical average of 5.33 is representative of the aggregated consensus opinion which is that the work is just below threshold in its current form.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Promising work, but just below threshold in its current form"
    },
    "Reviews": [
        {
            "title": "Good paper but needs better positioning and presentation",
            "review": "This paper presents a system to map natural language descriptions of scenes containing spatial relations to 3D visualizations of the corresponsing scene. The authors collect a dataset of different scenes containing objects of varying shapes and colors, along with several descriptions from different viewpoints. They train a model based on the Generative Query Network, to generate scenes conditioned on multiple text descriptions as input, along with associated camera angles. Empirical results using human evaluators demonstrate better performance compared to baselines and the authors perform a good analysis of the model, showing that it learns to ground the meaning of spatial words robustly. \n\nPros:\n1. Well-executed paper, with convincing empirical results on the newly collected dataset. \n2. Nice analysis to demonstrate that the model indeed learns good semantic representations for spatial language. \n\nCons:\n1. The positioning of this paper with respect to recent work is disappointing. In both the Introduction and Related Work sections, the authors talk about dated models for spatial reasoning in language (pre-2012). There have been several pieces of work that have looked at learning multimodal representations for spatial reasoning. These are a few examples:\n   a) Misra, Dipendra, John Langford, and Yoav Artzi. \"Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.\" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017.\n   b) Michael Janner, Karthik Narasimhan, and Regina Barzilay. \"Representation Learning for Grounded Spatial Reasoning.\" Transactions of the Association of Computational Linguistics 6 (2018): 49-61.\n   c) Paul, Rohan, et al. \"Grounding abstract spatial concepts for language interaction with robots.\" Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press, 2017.\n   d) Ankit Goyal, Jian Wang, Jia Deng. Think Visually: Question Answering through Virtual Imagery. Annual Meeting of the Association for Computational Linguistics (ACL), 2018 \n\nEven though Gershman & Tenenbaum (2015) demonstrate weaknesses of a specific model, some of the above papers demonstrate models that can understand things like \"A is in front of B\" = \"B is behind A\". A discussion of how this paper relates to some of this prior work, and an empirical comparison (if possible) would be good to have.\n\n2. The introduction reads a bit vague. It would help to clearly state the task considered in this paper upfront i.e. generating 3D scenes from text descriptions at various viewpoints. In the current form, it is hard to understand the task till one arrives at Section 3.\n\nOther comments:\n1. What is the difference between the bar graphs of Figures 5 and 6? Is the one on Figure 5 generated using the sentences (and their transforms) from Gershman & Tenenbaum? If so, how do you handle unseen words that are not present in your training data?\n2. Would be helpful to clearly explain what the red and black arrows represent in Figure 7.\n3. How does the model handle noisy input text i.e. if the object descriptions (shape/color) are off or if some of the input text is incorrect (say a small fraction of the different viewpoints)? \n\n------\nEdit:\nThank you for the author response. Even if you consider the story to be the same across literature (which in this case is not, since the more recent models handle spatial relations that the previous ones failed on), it's still worth doing due diligence to the recent work, especially so that the reader gets a better sense of how to position your work amongst these.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Learning to encode spatial relations from natural language",
            "review": "The main contributions of the work are the new datasets and the overall integration of previous modeling tools in such a way that the final architecture is able to encode semantic spatial relations from textual descriptions. This is demonstrated by an implementation that, given textual descriptions, is able to render images from novel viewpoints. In terms of these two contributions, as I explain below, I believe there is space to improve the datasets and the paper needs further analysis/comments about the merits of the proposed approach. So my current overall rating is below acceptance level.\n\nIn terms of data, authors provide 2 new datasets: i) a large datasets (10M) with synthetic examples (images and descriptions) and ii) a small dataset (6k) with human textual descriptions corresponding to synthetic images. As the main evaluation method of the paper, the author include direct human evaluation of the resulting renderings (3 level qualitative evaluation: perfect-match/partial-match/no-match). I agree that, for this application, human evaluation is more adequate than comparing a pixel-level output with respect to a gold image. In this sense, it is surprising that for the synthetic dataset the perfect match score of human evaluation for ground truth data is only 66%. It will be good to increase this number providing a cleaning dataset. \n\nRelated to the previous comment, it will be good to provide a deeper analysis about the loss function used to train the model.   \n\nIn terms of the input data, it is not clear how the authors decide about the 10 views for each scene.\n\nIn terms of the final model, if I understood correctly, the paper does not claim any contribution, they use a model presented in a previous work (actually information about the model is mostly included as a supplemental material). If there are relevant contributions in terms of model integration and/or training scheme, it will be good to stress this in the text.\n\nWriting is correct, however, authors incorporate important details about the dataset generation process as well as the underlying model in the supplemental material. Given that there is a page limit, I believe the relevant parts of the paper should be self-contain. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new dataset for 3D understanding",
            "review": "The authors present a large synthetic dataset for 3D scenes with templated descriptions.  They then use the model of Eslami 2018 to this new domain.  The previous work appears to already introduce all the necessary mechanisms for 3D generalization from multiple viewpoints, though this work embeds language instead of a scene in the process.  Minor note: A bit more discussion on this distinction would be appreciated.  Also, it appears that the previous work includes many of the rendered scenes also present here, so the primary focus of this paper is on the use of a language encoder (not necessarily a trivial extension).\n\nThe model appears to perform well with synthetic data though very poorly with natural sentences.  This may be in part due to the very small dataset size.  It would be helpful to know how much of the performance gap is due to scaling issues (10M vs 5.6K) versus OOVs, new syntactic constructions, etc.  In particular, the results have ~two deltas of interest (NL vs SYN) and the gap in the upper bound from 0.66 to 0.91.  What do new models need to be able to handle to close these gaps?\n\nRegarding the upper bound, there is some discussion that annotators might have had a strict definition of a perfect match.  Were annotators asked about this? The current examples (B.2), as the authors note, are more indicative of failings with the synthetic language than the human annotators.  This may again be motivation for collecting more natural language which would resolve some of the ambiguity and pragmatics of the synthetic dataset.\n\nIt would also be helpful to have some ablations included in this work. The most obvious being the role of $n$ (number of scene perspectives).  How crucial is it that the model has access to 9 of 10 perspectives?  One would hope that given the limited set of objects and colors, the model would perform well with far fewer examples per scene, learning to generalize across examples.\n\nSince the primary contributions of the paper are a language dataset and a language encoder for the existing model of Eslami 2018, those should be discussed and ablated in the paper rather than relegated to the appendix.\n\nMinor note:  the related work mentions grounding graphs which are core to work from Tellex and Roy, but omits existing fully neural end-to-end models in grounding (e.g. referring expressions work).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}