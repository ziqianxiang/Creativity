{
    "Decision": {
        "metareview": "Unfortunately, this paper fell just below the bar for acceptance.  The reviewers all saw significant promise in this work, stating that it is intriguing, \"novel and provides an interesting solution to a challenging problem\" and that \"many interesting use cases are clear\".  AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training.  A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints.  The other two reviewers unfortunately felt that while the proposed approach was \"interesting\", \"promising\" and \"intriguing\", the quality of the paper, in terms of exposition, was too low to justify acceptance.  Arguably, it seems the writing doesn't do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten.  ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A promising approach to include logical constraints in neural network training, but the writing is not quite ready yet."
    },
    "Reviews": [
        {
            "title": "Interesting generalization of work on incorporating logical queries into neural networks, many compelling use cases",
            "review": "Summary\n-------\nThis paper proposes DL2, a framework for turning queries over parameters and input, output pairs to neural networks into differentiable loss functions, and an associated declarative language for specifying these queries. The motivation for this work is twofold. The first is to allow for the specification of additional domain knowledge during training. For example, if a user expects that the predicted probabilities of some output classes should be correlated for all predictions, this constraint can be enforced during weight learning. Second, it allows users to search for specific inputs that satisfy specified conditions. In this way, DL2 can capture popular applications like searching for adversarial examples by querying for inputs close to a known input of class A but that the network predicts is class B with high confidence.\n\nThe paper provides a concise specification of the query language (a mixture of logical and numeric operators) and asserts a theorem that the given procedure for constructing the query loss produces a function such that anytime the function is 0, the constraints are satisfied. No proof is given, but I cannot see a counterexample. There is also a statement about the converse relationship, that when the loss is above some threshold it implies that the query is not satisfied. \n\nExperiments are conducted on supervised, semi-supervised, and unsupervised computer vision tasks. I particularly liked the experiment on semi-supervised learning with CIFAR-100. By replacing labeled examples with domain knowledge about the relationships among classes in CIFAR-100, the paper demonstrates a compelling use case for DL2.\n\nThe primary technical challenge is the non-convex optimization required to search for a solution to a query. Experiments show that the loss functions created by DL2 are often solved quickly and correctly, but not always\n\nStrengths\n---------\nThe framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection. The experiments cover a range of these use cases, demonstrating that the constructed optimization objectives usually work as intended.\n\nWeaknesses\n-----------\nThe statement in Theorem 1 regarding the converse case is unclear, because it says that the limit of \\delta as \\epsilon approaches zero is zero, but it is not explained what \\epsilon is or how it changes. If \\epsilon is the threshold that can often be used in the query, it is not obvious that every query contains exactly one \\epsilon. If other cases exist, it is unclear how Theorem 1 applies.\n\nIt remains unknown how to handle the case when queries fail. AS the paper points out, if a query fails, it cannot be determined whether no solution exists or if the optimization simply failed to find a solution. Of course, this is a computationally hard in general.\n\nRelated Work\n------------\nThere are a couple of points from related work that would be good to add to the paper.\n\nFirst, the paper \"Adversarial Sets for Regularising Neural Link Predictors\" (Minervini et al., UAI17) is a prior paper that generates adversarial examples to handle restrictions on inputs which may not exist in the training set. The paper claims DL2 is the first to do this, but I believe this paper is an earlier example that does so, albeit for a particular problem. DL2 is certainly more general.\n\nSecond, the description of the limitations of rule distillation (Hu et al., ACL16), particularly in Appendix A is not fully accurate. The expressivity of PSL is greater than stated (see Bach et al., JMLR17 for a full description). In particular, the DL2 loss function for z = (1, 1) can be expressed exactly in PSL using what it calls arithmetic rules. It is not clear that this affects the findings of the semi-supervised learning experiment significantly, although I would appreciate a clarification of the authors. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "still needs improvement ",
            "review": "\nThe paper tackles the interesting problem of combining logical approaches with neural networks in the form of  translating a logical formula into a non-negative loss function for a neural network. \nThe approach is novel and more general than previous approaches and the math is sound. However, I feel that the method is not well presented. Sadly the introduction does not set the method into context or give a motivation. The abstract is very short and misses key information. Indeed, even the more technical parts sometimes lack clarity and assume familiarity with a wide range of methods. \n\nThe experiments are well thought out and show the promise of the method when encoding performance measures such as entropy into the constraints. It would have been interesting to additionally see other kinds of constraints such as purely logical formulas that do not have a specific aim (robustness or performance or otherwise) but simply state preconditions that should be fulfilled. It would furthermore be interesting to inspect the corner cases of the proposed method such as what happens if two constraints are nearly opposing each other and so on. \n\n\n\nTo conclude, the presented method is clearly novel and provides an interesting solution to a challenging problem. However the paper in the current form does not fully adhere to the standards of conferences such as ICLR. I suggest rewriting especially the abstract and the introduction and then submitting to a different venue as the approach itself seems promising. Additionally, as only very limited comparison experiments can be performed the method itself should be more thoroughly inspected by performing, for example, edge-case or time/number of constraints inspections.\n\nMinor remarks:\nHyperparameters such as batch size not reported\nSpelling mistake in line 2, page 2 “Lipschitz condition”\nWhen mentioning “prior work” in the introduction a citation is needed.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints\n\nThe proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved. Indeed I think that the paper is bit too hard to follow. \nThe example at page 2 is not clearly explained.\n\nIn Equation 1 the relationship between constants S_i and the variables z is not clear. Is each S_i an assignment to z?\n\nI do not understand the step from Eq. 4 to Eq. 6. Why does arg min become min?\n\nAt page 4 the authors state \"we sometimes write a predicate \\phi to denote its indicator function 1_\\phi\". I’m a bit confused here, when is the indicator function used in equations 1-6?\n\nWhat kind of architecture is used for implementing DL2? Is a feedforward network used? How many layers does it have? How many neurons for each layer? No information about it is provided by authors.\n\nIt is not clear to me why DL2/training is implemented in PyTorch and DL2/querying in TensorFlow. Are those two separate systems? And why implementing them using different frameworks?\n\nIn conclusion, I’m a bit insecure about the rating to give to this paper, the system seems interesting, but several part are not clear to me.\n\n[Minor comments]\nIt seems strange to me to use the notation L_inf instead of B_\\epsilon to denote a ball.\n\nIn theorem 1. \\delta is a constant, right? It seems strange to me to have a limit over a constant.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}