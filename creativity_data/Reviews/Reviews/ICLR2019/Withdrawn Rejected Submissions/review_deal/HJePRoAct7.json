{
    "Decision": {
        "metareview": "The authors supplied an updated paper resolving the most important reviewer concerns after the deadline for revisions. In part, this was due to reviewers requesting new experiments that take substantial time to complete.\n\nAfter discussion with the reviewers, I believe that if the revised manuscript had arrived earlier, then it should be accepted. Without the new results I would recommend rejecting since I believe the original submission lacked important experiments to justify the approach (inductive setting experiments are very useful).\n\nThe community has an interest in uniform application of the rules surrounding the revision process. It is not fair to other authors to consider revisions past the deadline and we do not want to encourage late revisions. Better to submit a finished piece of work initially and not assume it will be possible to use up a lot of reviewer time and fix during the review process.\n\nWe also don't want to encourage shoddy, rushed experimental work. However, the way we typically handle requests from reviewers that require a lot of work to complete is by rejecting papers and encouraging them to be resubmitted sometime in the future, typically to another similar conference.\n\nThus I am recommending rejecting this paper on policy grounds, not on the merits of the latest draft. I believe that we should base the decision on the state of the paper at the same deadline that applies to all other authors.\n\nHowever, I am asking the program chairs to review this case since ultimately they will be the final arbiters of policy questions like this.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "difficult case"
    },
    "Reviews": [
        {
            "title": "An interesting paper that could benefit from more empirical comparisons",
            "review": "* I have revised my score upwards due to the authors response to my concerns --- particularly the addition of new results on graph classification. The original review remains here, and I respond to the author's response below. \n\nThe authors propose a new technique to add “pooling” and “unpooling” layers to a graph neural network (GNN). To deal with the lack of spatial locality in graphs, the downsampling operation relies on a learned scalar projection vector (which gives the “scores” for selecting different nodes). During upsampling, the model simple relies on storing the un-sampled adjacency matrix. Thorough experimental results on Cora, Citeseer, and Pubmed highlight the utility of the approach, with ablation studies isolating the importance of the pool/unpool operations.\n\nOverall, this is an interesting paper with the possibility of having a moderate impact within the area of GNNs/GCNs, and the method is clearly described. While there are a number of minor modifications made to the standard GCN model, which could potentially confound the results, the authors do provide a sensible ablation study to isolate the importance of their pool/unpool operations. The overall results on the three node classification datasets are also quite strong. \n\nThe primary shortcoming of this paper is that it only evaluates the model on three citation network datasets (Cora, Citseer, and Pubmed). While these datasets are now standard in the GCN/GNN community, they are very small, have few labeled examples, and it would greatly strengthen the paper to use a different dataset or two, e.g., the Reddit or PPI datasets from Hamilton et al. 2017 or the BlogCatolog dataset used in Grover et al. 2016 could be used for node classification. Or the authors could apply the proposed technique to graph classification or link prediction. In this reviewers opinion, it is very hard to judge the general utility of a method when results are only provided on these three very-specific datasets, where the performance differences between methods are now very marginal. \n\nIn a related point, while this work cites other approaches that apply pooling operations in graph neural networks (e.g., Ying et al. 2018, Simonovsky and Komodakis 2018), no comparisons are made against these approaches. One would suppose that these comparisons are not made because this paper only tests the graph U-net for node classification, but it would greatly strengthen this paper to add comparisons to these other pooling operations, e.g., for graph classification. Moreover, it is possible to define analogous unpooling operations for Ying et al. 2018 and Simonovsky and Komodakis 2018, similar to the unpooling operation used in this work (e.g., for Ying et al.’s DiffPool you can just “unpool” to the previous graph and assign each node a feature corresponding to the weighted sum of the features of the assigned clusters). Of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in Ying et al. 2018 and Simonovsky and Komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting. \n\nAs another minor point, whereas unpooling operations can be used to define a generative model in the image setting, this is not the case here, as the unpooling operation relies on knowledge about the input graph (i.e., the model always unpools to the same connectivity structure). This is not necessarily a bad thing, but it could improve the paper to clarify this issue. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting problem of pooling/upsampling graphs, experimental validation and literature review could be significantly improved",
            "review": "This paper proposes pooling and upsampling operations for graph structured data, to be interleaved with graph convolutions, following the spirit of fully convolutional networks for image pixel-wise prediction. Experiments are performed on node classification benchmarks, showing an improvement w.r.t. architectures that do not perform any downsampling/upsampling operations.\n\nGiven that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling, such as graph classification / regression. Moreover, authors should compare to other graph pooling methods.\n\nAuthors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions to see if they have the same effect on the performance when dealing with graphs. \n\nSome choices in the method seem rather arbitrary, such as the tanh non-linearity in \\tilde y. Could the authors elaborate on that? How important is the gating?\n\nIt would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?\n\nThe proposed unpooling operation seems to be the same as unpooling performed to upsample images, that is using skip connections to track indices, by recovering the position where the max value comes from and setting the rest to 0. Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?\n\nWhen skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.\n\nWhat is the influence of the first embedding layer to reduce the dimensionality of the features?\n\nHow do the models in Table 2 compare in terms of number of parameters?\n What's the influence of imposing larger weights on self loop in the graph?\n\nWhat about experiments in inductive settings?\n\nPlease add references for the following claim \"U-Net models with depth 3 or 4 are commonly used...\"\n\nPlease double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures.\n\nThe literature review could be significantly improved, missing relevant papers to discuss include:\n- Gori et al. A new model for learning in graph domains, 2005.\n- Scarselli et al. The graph neural network model, 2009.\n- Bruna et al. Spectral networks and locally connected networks on graphs, 2014.\n- Henaff et al. Deep convolutional networks on graph-structured data, 2015.\n- Niepert et al. Learning convolutional neural networks for graphs, 2016.\n- Atwood and Towsley. Diffusion-convolutional neural networks, 2016.\n- Bronstein et al. Geometric deep learning: going beyond Euclidean data, 2016.\n- Monti et al. Geometric deep learning on graphs and manifolds using mixture model cnns, 2017.\n- Fey et al. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels, 2017.\n- Gama et al. Convolutional Neural Networks Architectures for Signals Supported on Graphs, 2018.\nAs well as other pixel-wise architecture for image-based tasks such as:\n- Long et al. Fully Convolutional Networks for Semantic Segmentation, 2015.\n- Jegou et al. The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation, 2016.\n- Isola et al. Image-to-image translation with conditional adversarial networks, 2016.\n- Zhao et al. Stacked What-Where auto-encoders, 2015.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, clearly written and has some interesting ideas",
            "review": "Summary:\nThis paper introduces an encoder-decoder neural net architecture for arbitrary graphs. The core contribution is pooling and un-pooling operations for respectively graph down and up sampling.\n\nPros:\n+ U-Net like architectures indeed are very successful in vision applications, and having a model that was similar properties on graphs would be very useful.\n+ The paper is clearly written. \n+ I really liked the idea behind the pooling operation: it is simple, seems easy to implement efficiently, and generally makes sense (although see concerns below). \n+ The choice of the baselines is reasonable, and experimental results seem convincing. Ablation studies are also there.\n\nCons:\n- It is not clear why the evaluation seem to only be done for the transductive learning settings. I understand that some of the previous work might have done that, but this application scenario is quite limited.\n- One concern about the g-pool operation is that it is not local: unlike e.g. max pool on 2D which produces local maxima, here the selection is done globally, which could lead to situations where the entire parts of the graph are completely ignored. \n- Another concern, which has \fbeen partially addressed in section 3.4 is that the connectivity is not really taken into account when downsampling the adjacency matrix. The solution which introduces previously non-existing edges and thus kind of modifies the original graph is not very satisfying. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}