{
    "Decision": {
        "metareview": "There was a significant amount of discussion on this paper, both from the reviewers and from unsolicited feedback.  This is a good sign as it demonstrates interest in the work.  Improving exploration in Deep Q-learning through Thompson sampling using uncertainty from the model seems sensible and the empirical results on Atari seem quite impressive.  However, the reviewers and others argued that there were technical flaws in the work, particularly in the proofs.  Also, reviewers noted that clarity of the paper was a significant issue, even more so than a previous submission.  \n\nOne reviewer noted that the authors had significantly improved the paper throughout the discussion phase.  However, ultimately all reviewers agreed that the paper was not quite ready for acceptance.  It seems that the paper could still use some significant editing and careful exposition and justification of the technical content.\n\nNote, one of the reviews was disregarded due to incorrectness and a fourth reviewer was brought in.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A neat idea with impressive results but has technical flaws and issues with clarity"
    },
    "Reviews": [
        {
            "title": "-",
            "review": "This paper proposes a method for more efficient exploration in RL by maintaining uncertainty estimates over the learned Q-value function. It is comparable to Double DQN (DDQN) but uses its learned uncertainty estimates with Thompson sampling for exploration, rather than \\epsilon-greedy. Empirically the method is significantly more sample-efficient for Atari agents than DDQN and other baselines.\n\n=====================================\n\nPros:\n\nIntroduction and preliminaries section give useful background context and motivation. I found it easy to follow despite not having much hands-on background in RL.\n\nProposes a novel (to my knowledge) exploration method for RL which intuitively seems like it should work better than \\epsilon-greedy exploration. The method looks simple to implement on top of existing Q-learning based methods and has minimal computational and memory costs.\n\nStrong empirical performance as compared with appropriate baselines -- especially to DDQN(+) where the comparison is direct with the methods only differing in exploration strategy.\n\nGood discussion of practical implementation issues (architecture, hyperparameters, etc.) in Appendix A.\n\n=====================================\n\nCons/questions/suggestions/nitpicks:\n\nAlgorithm 1 line 11: “Update W^{target} and Cov” -- how? I see only a brief mention of how W^{target} is updated in the last paragraph of Sec. 3, but it’s not obvious to me how the algorithm is actually implemented from this, and I don’t see any mention of how Cov is updated.\n\nAlgorithm 1: I’d like to know more about how sample-efficiency varies with T^{sample} given that T^{sample}>1 is doing something other than true Thompson sampling. Does the regret bound hold with T^{sample}>1? Also, based on the discussion in Appendix A, approximating the episode length seems to be the goal in choosing a setting of T^{sample} -- so why not just always resample at the beginning of each episode instead of using a constant T^{sample}?\n\nTheorem 1: I don’t understand the Theorem statement (let alone the proof) or what it tells me about the proposed BDQN algorithm. First, as a “non-RL person” I wasn’t familiar with PSRL, but I see it’s later defined as “posterior-sampling RL”. This should be clarified earlier for readers that aren't familiar with this line of work. But this still doesn’t fully explain what “the PSRL on \\omega” means. If it means you follow an existing PSRL algorithm to learn \\omega, then how does the theorem relate to the proposed algorithm? I'm sure I'm missing something but the connection was unclear to me.\n\nTheorem 1: there’s a common abuse of big-O notation here that should be fixed for a formal statement -- O(f(n)) by definition is a set corresponding to an upper-bound, so this should probably be written as g(n) \\in O(f(n)) rather than g(n)<=O(f(n)). (Or alternatively, just rewritten without big-O notation.)\n\nTable 2: should be reformatted to make it clear that the rightmost 3 columns are not additional baseline methods (e.g. adding a vertical line would be good enough).\n\nAppendix A.8, “A discussion on safety”: this section should either be much more fleshed out or removed. I didn’t understand the statement at the end at all -- “one can... come up with a criterion for safe RL just by looking at high and low probability events” -- huh? What is even meant by “safe RL” in this context? Nothing is referenced.\n\nOverall, much of the writing seems quite rushed with many typos and grammatical errors throughout. This should be cleaned up for a final version. To give a particularly common example, there are many inline references that do not fit in the sentence and distract from the flow -- these should be changed to \\citep.\n\nHow does this compare with “A Distributional Perspective on Reinforcement Learning” (Bellemare et al., ICML 2017) both in terms of the approach and performance? The proposed method seems to at least superficially share motivation with this work (and uses the same Atari benchmark, as far as I can tell) but it is not discussed or compared.\n\n=====================================\n\nOverall, though many parts of the paper could use significant cleanup and clarification, the paper proposes a novel yet relatively simple and intuitive approach with strong empirical performance gains over comparable baselines.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Major clarity issues",
            "review": "Update after feedback: I would like to thank the authors for huge work done on improving the paper. I appreciate the tight time constrains given during the discussion phase and big steps towards more clear paper, but at the current stage I keep my opinion that the paper is not ready for publication. Also variability of concerns raised by other reviewers does not motivate acceptance.\n\nI would like to encourage the authors to make careful revision and I would be happy to see this work published. It looks very promising. \n\nJust an example of still unclear parts of the paper: the text between eq. (3) and (4). This describes the proposed method, together with theoretical discussions this is the main part of the paper. As a reader I would appreciate this part being written detailed, step by step.\n=========================================================\n\nThe paper proposes the Bayesian version of DQN (by replacing the last layer with Bayesian linear regression) for efficient exploration. \n\nThe paper looks very promising because of a relatively simple methodology (in the positive sense) and impressive results, but I find the paper having big issues with clarity. There are so many mistakes, typos, unclear statements and a questionable structure in the text that it is difficult to understand many parts. In the current version the paper is not ready for publication. \n\nIn details (more in the order of appearance rather than of importance):\n1. It seems that the authors use “sample” for tuples from the experience replay buffer and draws W from its posterior distribution (at least for these two purposes), which is extremely confusing\n2. pp.1-2 “We show that the Bayesian regret is bounded by O(d \\sqrt{N}), after N time steps for a d-dimensional feature map, and this bound is shown to be tight up-to logarithmic factors.” – maybe too many details for an abstract and introduction and it is unclear for a reader anyway at that point\n3. p.1 “A central challenge in reinforcement learning (RL) is to design efficient exploration-exploitation tradeoff” – sounds too strong. Isn’t the central challenge to train an agent to get a maximum reward? It’s better to change to at least “One of central challenges”\n4. p.1 “ε-greedy which uniformly explores over all the non-greedy strategies with 1 − ε probability” – it is possible, but isn’t it more conventional for an epsilon-greedy policy to take a random action with the probability epsilon and acts greedy with the probability 1 – epsilon? Moreover, later in Section 2 the authors state the opposite “where with ε probability it chooses a random action and with 1 − ε probability it chooses the greedy action based on the estimated Q function.”\n5. p.1 “An action is chosen from the posterior distribution of the belief” – a posterior distribution is the belief\n6. p.2 “and follow the same target objective” – if BDQN is truly Bayesian it should find a posterior distribution over weights, whereas in DDQN there is no such concept as a posterior distribution over weights, therefore, this statement does not sound right\n7. p.2 “This can be considered as a surrogate for sample complexity and regret. Indeed, no single measure of performance provides a complete picture of an algorithm, and we present detailed experiments in Section 4” – maybe too many details for introduction (plus missing full stop at the end)\n8. p.2 “This is the cost of inverting a 512 × 512 matrix every 100,000 time steps, which is negligible.” – doesn’t this depend on some parameter choices? Now the claim looks like it is true unconditionally. Also too many details for introduction\n9. p.2 “On the other hand, more sophisticated Bayesian RL techniques are significantly more expensive and have not lead to large gains over DQN and DDQN.” – it would be better to justify the claim with some reference\n10. Previous work presented in Introduction is a bit confusing. If the authors want to focus only on Thompson Sampling approaches, then it is unclear, why they mentioned OFU methods. If they mention OFU methods, then it is unclear why other exploration methods are not covered (in Introduction). It is better to either move OFU methods to Related Work completely, or to give a taste of other methods (for example, from Related Work) in Introduction as well\n11. p.3 “Consider an MDP M as a tuple <X , A, P, P0, R, γ>, with state space X , action space A, the transition kernel P, accompanied with reward function of R, and discount factor 0 ≤ γ < 1.” – P_0 is not defined\n12. p.4 “A common assumption in DNN is that the feature representation is suitable for linear classification or regression (same assumption in DDQN), therefore, building a linear model on the features is a suitable choice.” – the statement is more confusing than explaining. Maybe it is better to state that the last fully connected layer, representing linear relationship, in DQN is replaced with BLR in the proposed model\n13. p.5 In eq. (3) it is better to carry definition of $\\bar{w}_a$ outside the Gaussian distribution, as it is done for $\\Xi_a$\n14. p.5 The text between eq. (3) and (4) seems to be important for the model description and yet it is very unclear: how $a_{TS}$ is used? “we draw $w_a$ follow $a_{TS}$” – do the authors mean “following” (though it is still unclear with “following”)? What does notation $[W^T \\phi^{\\theta} (x_{\\tau})]_{a_{\\tau}}$ denote? Which time steps do the authors mean?\n15. p.5 The paragraph under eq. (4) is also very confusing. “to the mean of the posterior A.6.” – reference to the appendix without proper verbal reference. Cov in Algorithm 1 is undefined, is it equal to $\\Xi$? Notation in step 8 in Algorithm 1 is too complicated.\n16. Algorithm 1 gives a vague idea about the proposed algorithm, but the text should be revised, the current version is very unclear and confusing\n17. pp.5-6 The text of the authors' attempts to reproduce the results of others' work (from \"We also aimed to implement...\" to \"during the course of learning and exploration\") should be formalised\n18. p. 6 \"We report the number of samples\" - which samples? W? from the buffer replay?\n19. p. 6 missing reference for DDQN+\n20. p. 6 definition of SC+ and references for baselines should be moved from the table caption to the main text of the paper\n21. p. 6 Table 3 is never discussed, appears in a random place of the text, there should be note in its reference that it is in the appendix\n22. p.6 Where is the text for footnotes 3-6?\n23. p.6 Table 2 may be transposed to fit the borders\n24. p.6 (and later) It is unclear why exploration in BDQN is called targeted\n25. p.7 Caption of Figure 3 is not very good\n26. p.7 Too small font size of axis labels and titles in plots in Figure 3 (there is still a room for 1.5 pages, moreover the paper is allowed to go beyond 10 pages due to big figures)\n27. p.7 Figure 3. Why Assault has different from the others y-axis? Why in y-axis (for the others) is \"per episode\" and x-axis is \"number of steps\" (wise versa for Assault)?\n27. Section 5 should go before Experiments\n28. p. 7 “Where Ψ is upper triangular matrix all ones 6.” – reference 6 should be surrounded by brackets and/or preceded by \"eq.\" and it is unclear what “all ones” means especially given than the matrix in eq. (6) does not contain only ones\n29. p. 7 “Similar to the linear bandit problems,” – missing citation\n30. p. 7 PSRL appears in the theorem, but is introduced only later in Related work\n31. p. 7 “Proof: in Theorem. B” – proof is given in Appendix B?\n32. p. 8 Theorem discussion, “grows not faster than linear in the dimension, and \\sqrt(HT)” – unclear. Is it linear in the product of dimension (of what?) and \\sqrt(HT)?\n33. p.8 “On lower bound; since for H = 1…” – what on lower bound?\n34. p.8 “our bound is order optimal in d and T” – what do the authors mean by this?\n35. p.8 \"while also the state of the art performance bounds are preserved\" - what does it mean?\n36. p.8 \"To combat these shortcomings, \" - which ones?\n37. p.8 \"one is common with our set of 15 games which BDQN outperformS it...\" - what is it?\n38. p.9 \"Due to the computational limitations...\" - it is better to remove this sentence\n39. p.9 missing connection in \"where the feature representation is fixed, BDQN is given the feature representation\", or some parts of this sentence should be removed?\n40. p.9 PAC is not introduced\n41. pp.13-14 There is no need to divide Appendices A.2 and A.3. In fact, it is more confusing than helpful with the last paragraph in A.2 repeating, sometimes verbatim, the beginning of the first paragraph in A.3\n42. In the experiments, do the authors pre-train their BDQN with DQN? In this case, it is unfair to say that BDQN learns faster than DDQN if the latter is not pre-trained with DQN as well. Or is pre-training with DQN is used only for hyperparameter tuning?\n43. p.14 “Fig. 4 shows that the DDQN with higher learning rates learns as good as BDQN at the very beginning but it can not maintain the rate of improvement and degrade even worse than the original DDQN.” – it seems that the authors tried two learning rates for DDQN, for the one it is clear that it is set to 0.0025, another one is unclear. The word “original” is also unclear in this context. From the legend of Figure 4 it seems that the second choice for the learning rate is 0.00025, but it should be stated in the text more explicitly. The legend label “DDQN-10xlr” is not the best choice either. It is better to specify explicitly the value of the learning rate for both DDQN\n44. p.15 “As it is mentioned in Alg. 1, to update the posterior distribution, BDQN draws B samples from the replay buffer and needs to compute the feature vector of them.” – B samples never mentioned in Algorithm 1\n45. p.15 “during the duration of 100k decision making steps, for the learning procedure,” – i) “during … duration”, ii) what did the authors meant by “decision making steps” and “the learning procedure”?, and iii) too many commas\n46. p.15 “where $\\tilde{T}^{sample}$, the period that of $\\tilde{W}$ is sampled our of posterior” – this text does not make sense. Is “our” supposed to be “out”? “… the number of steps, after which a new $\\tilde{W}$ is sampled from the posterior”?\n47. p.15 “$\\tilde{W}$ is being used just for making Thompson sampling actions” – could the authors be more specific about the actions here?\n48. p.16 “In BDQN, as mentioned in Eq. 3, the prior and likelihood are conjugate of each others.” – it is difficult to imagine that an equation would mention anything and eq. (3) gives just the final formula for the posterior, rather than the prior and likelihood\n49. p.16 The formula after “we have a closed form posterior distribution of the discounted return, ” is unclear\n50. p.17 “we use ω instead of ω to avoid any possible confusion” – are there any differences between two omegas?\n51. p.17 what is $\\hat{b}_t$?\n\nThere are a lot of minor mistakes and typos also, I will add them as a comment since there is a limit of characters for the review.\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Appealing idea; poor delivery.",
            "review": "Summary: The paper proposes an approximate Thompson Sampling method for value function learning when using deep function approximation.\n\nResearch Context: Thompson Sampling is an algorithm for sequential decision making under uncertainty that could provide efficient exploration (or lead to near optimal cumulative regret) under some assumptions. The most critical one is the ability to sample from the posterior distribution over problem models given the already collected data. In most cases, this is not feasible, so we need to rely on approximate posteriors, or, informally, on distributions that somehow assign reasonable mass to plausible models. The paper tries to address this.\n\nMain Idea: In particular, the idea here is to simultaneously train a deep Q network while choosing actions based on samples for the linear parameters of the last layer. On one hand, this seems sensible: a distribution over the last layer weights provides an approximate posterior over Q functions, as needed, and a linear model could work after learning an appropriate representation. On the other, this seems doable: there are close-form updates for Bayesian linear regression when using a Gaussian prior and likelihood, as proposed in the paper.\n\nPros:\n- Simple and elegant algorithm.\n- Strong empirical results in standard benchmarks.\n\nCons:\n- The paper is very poorly written; the number of typos is countless, and in general the paper is quite hard to read and to follow.\n- I share the concerns expressed in the first public comment regarding the correctness of the theoretical statements (Theorem 1) or, at least, the proposed proofs. Notation is very hard to parse, and the meaning of some claims is not clear ('the PSRL on w', 'we use w instead of w', 'the estimated \\hat{b_t}', '\\pi_t(x, a) = a = ...'). I'd appreciate a clear proof strategy outline. In addition, it'd be quite useful if the authors could highlight the specific technical contributions of the proposed analysis, and how they rely on and relate to previous analyses (Abbasi-Yadkori et al., De la Peña et al., Osband et al., etc).\n- I think Table 1, Figure 1, and Figure 2 are not particularly useful and could be removed.\n\nQuestions:\n- Last year, there was a paper published in ICLR [1] that proposed basically the same algorithm for contextual bandits. They reported as essential to also learn the noise levels for different actions, while in this work \\sigma_\\epsilon is assumed known, fixed, and common across actions (see paragraph to the left of Figure 2). I'm curious why not learn it for each action using an Inverse-Gamma prior as proposed in [1], or if this was actually something you tried, and what the performance consequences were. In principle, my hunch is it should have a strong impact on the amount of exploration imposed by the algorithm (see Equation 3) over time.\n- A minor comment: the dimension 'd' in Theorem 1 is a *design choice* in the proposed algorithm. Of course, Theorem 1 relies on some assumptions that may be harder to satisfy for decreasing values of 'd', but I think some further comment can be useful as some readers may think the theorem is indeed suggesting we should set as small 'd' as possible...\n- More generally, what are the expected practical consequences of the mismatch between the proposed algorithm (representation is learned alongside with linear TS) and the setup in Theorem 1 (representation is fixed or known, and prior and likelihood are not misspecified)?\n\nConclusion:\nWhile definitely a promising direction, the paper requires significant further work, writing improvement, and polishing. At this point, I'm unable to certify that the theoretical contribution is correct.\n\n\nI'm willing to change my score if some of the comments above are properly addressed. Thanks.\n\n\n\n\n\n[1] - Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lacks novelty, experiments incomplete, results misinterpreted. Clear reject.",
            "review": "The paper proposes performing Thompson Sampling (TS) using a Bayesian Linear Regressor (BLR) as the action-value function the inputs of which are parameterized as a deterministic neural net. The authors provide a regret bound for the BLR part of their method and provide a comparison against Double Deep Q-Learning (DDQL) on a series of computer games.\n\nStrengths:\n  * The paper presents some strong experimental results.\n\nWeaknesses:\n  * The novelty of the method falls a little short for a full-scale conference paper. After all, it is only a special case of [3] where the random weights are restricted to the top-most layer and the posterior is naturally calculated in closed form. Note that [3] also reports a proof-of-concept experiment on a Thompson Sampling setting.\n\n  * Related to the point above, the paper should have definitely provided a comparison against [3]. It is hard to conclude much from the fact that the proposed method outperforms DDQN, which is by design not meant for sample efficiency and effective exploration. A DDQN with Dropout applied on multiple layers and Thompson Sampling followed as the policy would indeed be both a trivial design and a competitive baseline. Now the authors can argue what they provide on top of this design and how impactful it is.\n\n  * If the main concern is sample efficiency, another highly relevant vein of research is model-based reinforcement learning. The paper should have provided a clear differentiation from the state of the art in this field as well.\n\n  * Key citations to very closely related prior work are missing, for instance [1,2].\n\n  * I have hard time to buy the disclaimers provided for Table 2. What is wrong with reporting results on the evaluation phase? Is that not what actually counts? \n\n  * The appendix includes some material, such as critical experimental results, that are prerequisite for a reviewer to make a decision about its faith. To my take, this is not the Appendices are meant for. As the reviewers do not have to read the Appendices at all, all material required for a decision has to be in the body text. Therefore I deem all such essential material as invalid and make my decision without investigating them.\n\nMinor:\n  * The paper has excessively many typos and misspellings. This both gives negative signals about its level of maturity and calls for a detailed proofread.\n\n[1] R. Dearden et al., Bayesian Q-learning, AAAI, 1998\n\n[2] N. Tziortziotis et al., Linear Bayesian Reinforcement Learning, IJCAI, 2013\n\n[3] Y. Gal, Z. Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ICML, 2016",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}