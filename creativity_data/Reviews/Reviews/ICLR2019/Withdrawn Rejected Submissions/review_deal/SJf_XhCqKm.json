{
    "Decision": {
        "metareview": "This is a very clearly written, well composed paper that does a good job of placing the proposed contribution in the scope of hyperparameter optimization techniques.  This paper certainly appears to have been improved over the version submitted to the previous ICLR.  In particular, the writing is much clearer and easy to follow and the methodology and experiments have been improved.  The ideas are well motivated and it's exciting to see that sampling from a k-DPP can give better low discrepancy sequences than e.g. Sobol.  However,  the reviewers still seem to have two major concerns, namely novelty of the approach (DPPs have been used for Bayesian optimization before) and the empirical evaluation.  \n\nEmpirical evaluation:  As Reviewer1 notes, there are much more recent approaches for Bayesian optimization that have improved significantly over the TPE method, also for conditional parameters.  There are also more recent approaches proposing variants of random search such as hyperband.  \n\nNovelty:  There is some work on using determinantal point processes for Bayesian optimization and related work in optimal experimental design.  Optimal design has a significant amount of literature dedicated to designing a set of experiments according to the determinant of their covariance matrix - i.e. D-Optimal Design.  This work may add some interesting contributions to that literature, including fast sampling from k-DPPs, etc.  It would be useful, however, to add some discussion of that literature in the paper.  Jegelka and Sra's tutorial at NeurIPS on negative dependence had a nice overview of some of this literature.\n\nUnfortunately, two of the three reviewers thought the paper was just below the borderline and none of the reviewers were willing to champion it.  There are very promising and interesting ideas in the paper, however, that have a lot of potential.  In the opinion of the AC, one of the most powerful aspects of DPPs over e.g. low discrepancy sequences, random search, etc.  is the ability to learn a distance over a space under which samples will be diverse.  This can make a search *much* more efficient since (as the authors note when discussing random search vs. grid search) the DPP can sample more densely in areas and dimensions that have higher sensitivity.  It would be exciting to learn kernels specifically for hyperparameter optimization problems (e.g. a kernel specifically for learning rates that can capture e.g. logarithmic scaling).  Taking the objective into account through the quality score, as proposed for future work, also seems very sensible and could significantly improve results as well.  ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A well written, interesting paper on designing experiments for hyperparameter optimization using DPPs, but with lingering concerns over novelty and experiments. "
    },
    "Reviews": [
        {
            "title": "small number of hyperparameters, comparison with spearmint not strong enough",
            "review": "I reviewed the same paper last year. I am appending a few lines based on the changes made by authors.\n\nThe authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al. (2011). The k-DPP sampling algorithm and the concept of k-DPP-RBF over hyperparameters are not new, so the main contribution here is the empirical study. \n\nThe first experiment by the authors shows that k-DPP-RBF gives better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center or an arbitrary corner (Fig. 1).\n\nThe second experiment shows surprisingly that for the hard learning rate range, k-DPP-RBF performs better than uniform random search, and moreover, both of these outperform BO-TPE (Fig. 2, column 1).\n\nThe third experiment shows that on good or stable ranges, k-DPP-RBF and its discrete analog slightly outperform uniform random search and its discrete analog, respectively.\n\nI have a few reservations. First, I do not find these outcomes very surprising or informative, except for the second experiment (Fig. 2, column 1). Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20. The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al. (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al. (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n\nCOMMENTS ON THE CHANGES SINCE THE LAST YEAR\n\nI am not convinced by the comparison with Spearmint added by the authors since the previous version. It is unclear to me if the comparison of wall clock time and accuracy holds for larger number of hyperparameters or against Spearmint with more parallelization.\n\nIn addition the authors do not compare against more recent work, e.g., \n\n@INPROCEEDINGS{falkner-bayesopt17,\n author    = {S. Falkner and A. Klein and F. Hutter},\n title     = {Combining Hyperband and Bayesian Optimization},\n booktitle = {NIPS 2017 Bayesian Optimization Workshop},\n year      = {2017},\n month     = dec,\n}\n\n@InProceedings{falkner-icml-18,\n  title =        {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},\n  author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n  booktitle =    {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},\n  pages =        {1436--1445},\n  year =         {2018},\n  month =        jul,\n}",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting and clear idea on using DPPs for hyperparameter search",
            "review": "- This paper proposes an approach to get samples with high dispersion for hyperparameter optimisation. \n- It theoretically motivates the use of Determinantal Point Processes in yielding such samples.\n- Further, an iterative mixing algorithm is proposed to handle continuous and discrete sample space.\n- Experiments on finding hyperparameter for sentence classification are presented. In terms of accuracy, it performs better than other open-loop methods. In comparison to closed-loop methods, it yields parameter settings with comparable performance but with gains in wall clock time.\n- The distinction from close-loop approaches makes it easy to parallelise.\n\n\nThis paper is novel in its modelling of hyperparameter optimisation with DPP and the theoretical justification and experiments have been clearly presented. It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "recommending rejection because of lack of analyses and questionable novelty",
            "review": "The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. \n\nThis paper covers the related work nicely, with details on both closed loop and open loop methods. The rest of the paper are also clearly written. However, I have some concerns about the proposed method.\n- It is not clear how to define the kernel, the feature function and the quality function for the proposed method. The choices of those seem to have a huge impact on the performance. How was those functions decided and how sensitive is the result to hyperparameters of those functions?\n- If the search space is continuous, what is the mixing rate of Alg. 2? In practice, how is \"mixed\" decided? What exactly is the space and time complexity? I'm not sure where k log(N) comes from in page 7.  \n- Alg. 2 is a straight forward extension of Alg. 1, just with L not explicitly computed. I think it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is. \n\nOther small things:\n- citation format problems in, for example, Sec. 4.1. It should be \\citep instead of \\cite. \n- it would be good to mention Figure 2 in the text first before showing it. \n\n[Post rebuttal]\nI would like to thank the authors for their clarifications. However, I am still concerned with the novelty. The absence of provable mixing rate is also a potential weakness. I think a clearer emphasis on the novelty, e.g. current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}