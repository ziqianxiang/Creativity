{
    "Decision": {
        "metareview": "Several reviewers thought the results were not surprising in light of existing universality results, and thought the results were of limited relevance, given that the formalization is not quite in line with real-world networks for MIL. The authors draw out some further justifications in the rebuttal. These should be reintegrated. I agree with the general criticisms regarding relevance to ICLR. Ultimately, this work may belong in a journal.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Rewrite needed to address importance of result"
    },
    "Reviews": [
        {
            "title": "Useful result on universality. Probably not extremely relevant to ICLR",
            "review": "The paper investigates the approximation properties of a family of neural networks designed to address multi-instance learning (MIL) problems. The authors show that results well-known for standard one layer architectures extend to the MIL models considered. The authors focus on tree-structured domains showing that their analysis applies to these relevant settings. \n\nThe paper is well written and easy to follow. In particular the theoretical analysis is clear and pleasant to read. \n\nThe main concern is related to the relevance of the result to ICLR. As the authors themselves state, the result is not surprising given the standard universality result of one-layer neural networks (and indeed Thm. 2 heavily relies on this fact to prove the universality of MIL architectures). In this sense the current work might be more suited to a journal venue. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but...",
            "review": "This paper generalizes the universal approximation theorem (usually stated for real functions on some Euclidean space) to real functions on the space of measures (at least a compact set of proba. measures).\n\nThis result might be interesting but not really surprising and the paper does not put any new theoretical ideas or proof techniques. The proof is actually almost identical than in the original paper of Hornik, Stinchcombe and White (89) [and not the 91 paper of Hornik as indicated in the paper], the only difference being a trick on the density of f\\circ h instead of just considering cos() function.\n\nAll in all, the contributions is interesting but really incremental",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper proposes a quite straightforward extension of standard results about universal approximation by neural networks on complex domains.",
            "review": "The authors study in this paper the approximation capabilities of neural networks for real valued functions on probability measure spaces (and on tree structured domains). \n\nThe first step of the paper consists in extending standard NN results to probability measure spaces, that is rather than having finite dimensional vectors as inputs, the NN considered here have probability measures as inputs. The extension to this case is straightforward and closely related to older extension on infinite dimensional spaces (see for instance the seminal paper of Stinchcombe https://doi.org/10.1016/S0893-6080(98)00108-7 and e.g. http://dx.doi.org/10.1016/j.neunet.2004.07.001 for an application to NN with functional inputs). Nothing quite new here.\n\nIn addition, and exactly as in the case of functional inputs, the real world neural networks do not implement what is covered by the theorem but only an approximation of it. This is acknowledged by the authors at the end of Section 2 but in a way that is close to hand waving. Indeed while the probability distribution point is valuable and gives interesting tools in the MIL context, the truth is that we have no reason to assume the bag sizes will grow to infinite or even will be under our control. In fact there are many situations were the bag sizes are part of the data (for instance when a text is embedded in a vector space word by word and then represented as a bag of vectors). Thus proving some form of universal approximation in the multiple instance learning context would need to take this fact into account, something that is not done at all here. \n\nTherefore I believe the contribution of this paper to be somewhat limited. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}