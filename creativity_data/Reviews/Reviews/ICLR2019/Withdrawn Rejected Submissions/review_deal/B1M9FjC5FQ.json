{
    "Decision": {
        "metareview": "Reviewers are in a consensus and recommended to reject. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Paper decision"
    },
    "Reviews": [
        {
            "title": "interesting analysis on dropout",
            "review": "This paper gives further analysis on dropout and explains why it works although Hinton et al. already showed some analysis. This paper also introduced a new gradient acceleration in activation function (GAAF).\n\nOn Table 4, the GAAF is a bit worse than dropout although GAAF converges fast. But i am not sure whether GAAF is really useful on large datasets, not on a small dataset, e.g., MINIST here. On table 5, i am not sure whether you compared with dropout or not. Is your base model already including dropout?\n\nIf you want to demonstrate that GAAF is really helpful, i think more experiments and comparisons, especially on larger datsets should be conducted.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "No proper grounding of the presented argument against \" avoiding co-adaptation through dropout\" concept. Very weak experiments.",
            "review": "The authors attempt to propose an alternative explanation for the effect of dropout in a neural network and then present a technique to improve existing activation functions.\n\nSection 3.1 presents a experimental proof of higher co-adaptation in presence of dropout, in my opinion this is an incorrect experiment and request authors to double check. In my experience, using dropout results in sparse representations in the hidden layers which is the effect decreased co-adaptions. Also, a single experiment with MNIST data-set cannot be a proof to reject a theory.\n\nSection 3.2 Table 2 presents a comparison between average gradient flow through layers during training where flow with dropout is higher. This is not very surprising, in my opinion, given the variance of the activation of a neuron in presence of dropout the network tries to optimize the classification cost while trying to reduce the variance. The experimental details are almost nil.\n\nThe experiments section 5 presents very weak results. Very little or no improvement and authors randomly introduce BatchNorm into one of the experiment.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but seems less precise than previous work",
            "review": "This paper offers the argument that dropout works not due to preventing coadaptation, but because it gives more gradient, especially in the saturated region. However, previous works have already characterized how dropout modifies the activation function, and also the gradient in a more precise way than what is proposed in this paper. \n\n## Co-adaptation\nco-adaptation does not seem to mean correlation among the unit activations. It is not too surprising units need more redundancy with dropout, since a highly useful feature might not always be present, but thus need to be replicated elsewhere.\n\nSection 8 of this paper gives a definition of co-adaptation,\nbased on if the loss is reduced or increased based on a simultaneous change in units.\nhttps://arxiv.org/abs/1412.4736\nAnd this work, https://arxiv.org/abs/1602.04484, reached a conclusion similar to yours\nthat for some notion of coadaptation, dropout might increase it.\n\n## Gradient acceleration\nIt does not seem reasonable to measure \"gradient information flow\" simply as the norm of the gradient, which is sensitive to scales, and it is not clear if the authors accounted for scaling factor of dropout in Table 2.\n\nThe proposed resolution, to add this discontinuous step function in (7) with floor is a very interesting idea backed by good experimental results. However, I think the main effect is in adding noise, since the gradient with respect to this function is not meaningful. The main effect is optimizing with respect to the base function, but adding noise when computing the outputs. Previous work have also looked at how dropout noise modifies the effective activation function (and thus its gradient). This work, http://proceedings.mlr.press/v28/wang13a.html, give a more precise characterization instead of treating the effect as adding a function with constant gradient multiplied by an envelop. In fact, the actual gradient with dropout does involve the envelope by chain rule, but the rest is not actually constant as in GAAF. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}