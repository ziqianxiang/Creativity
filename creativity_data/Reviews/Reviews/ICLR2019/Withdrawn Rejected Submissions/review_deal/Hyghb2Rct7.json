{
    "Decision": {
        "metareview": "This paper explores the use of sequential information to improve imitation learning, essentially using recurrent networks (LSTM) instead of a simple NN in several existing imitation learning models (BC, GAIL, etc.). On the positive side, the empirical results are good, showing improvement in terms of attained rewards, convergence speed and stability. There are however some significant issues with the way the way the approach is motivated and positioned with respect to existsing work. In particular, the issue described in the paper is due to the fact they consider POMDPs (not MDPs): this should have been more clearly explained. There are also issues with the Related Work section. For these reasons, the paper is not quite ready for publication.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Paper should discuss and account for partial observability"
    },
    "Reviews": [
        {
            "title": "First review",
            "review": "This paper introduces the use of sequential information (state-action pairs) for enhancing imitation learning, and using recurrent networks (LSTM) in that process.  The authors motivate this by pointing out that while the state information, if Markovian, should contain all information necessary for decision making, with incomplete learners redundant information in the sequential state-action information leading to the current state can be helpful, citing some concrete examples. \nAfter describing a number of variants of this idea, in the context of IRL, BC, etc., the authors conduct a systematic empirical evaluation to assess the effectiveness of the proposal, over the baselines, using a number of RL benchmark problems. \nThe results are favorable and convincingly show that the proposed sequential enhancement can bring significant improvement in terms of attained rewards, convergence speed and stability in many of the tested cases. \nOne suggestion I have is that it would be interesting to investigate into the question of how the addition of sequential information adds value is related to the validity of Markovian assumption in each of the problem being considered. \nIt is a good empirical paper demonstrating the practical use of an idea that is simple but reasonable, and in a way that is substantiated using proper cutting edge framework and baselines. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straightforward extension of learning-from-demonstration approaches to exploit recurrent neural network",
            "review": "The paper puts forward the idea of using a recurrent neural network in algorithms for learning from demonstration in order to take into account sequential information. The authors test it in the inverse reinforcement learning setting and the behavioral cloning setting on different control problems.\n\nI feel the basic idea is really straightforward. Although some promising results are obtained in the experimental setting, I believe the contribution may not be sufficient for a publication at ICLR. Moreover, there are some issues in the writing, e.g., \n\n- classically, as far as I know, RL is not considered to be a metaheuristic, although I understand that someone could make the case for it.\n\n- although there’s not really a consensus on terminology, I think using imitation learning to define the whole class of problems encompassing IRL and behavioral cloning is not the best. Generally, imitation learning is equated to behavioral cloning. I think a better term for this general class is learning from demonstration. For instance, there are some IRL approaches that don’t try to mimic a demonstrated policy, but aim at learning an even better policy.\n\n- the issue described in the paper about the missing sequential information is due to the fact the authors consider POMDPs and not MDPs. This should be made clearer. I think the authors should also cite the following paper:\n\n@article{ChoiKim11,\n\tAuthor = {Jaedeug Choi and Kee-Eung Kim},\n\tJournal = {JMLR},\n\tPages = {691--730},\n\tTitle = {Inverse Reinforcement Learning in Partially Observable Environments},\n\tVolume = {12},\n\tYear = {2011}}\n\n- the related work has to be reworked. Kuderer et al. (2013) is not about urban route planning, but deals with learning driving style; Mnih et al. (2015) is not about training multi-agent systems, but introduces DQN; Silver et al. (2016) is about go, not chess. Are TRPO or PPO really off-policy or asynchronous?\n\n- the last section of Sec.3.4 sounds strange. It’s not MC that assumes that the impact of an action decays with time. The discount factor comes from the choice of the total discounted reward criterion.\n\nOther comments:\n\n- in abstract: BL -> BC\n- notations issues in (2-5)\n- l.6-7, Algo 1: t = T_m?\n- The text should be checked for typos.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "The paper proposes to integrate sequential information into imitation learning techniques.  The assumption is that mostly all the IL techniques are learning a policy which depends on state at time t, while the information contained in this state may be not sufficient to choose the right action (actually, this is the POMDP setting, the notion of POMDP not appearing in the paper....). The authors thus propose to use a recurrent neural network to encode the state by aggregating past information, instead of just using the features of the state at time t. They thus instantiate this idea on different methods and show that, on some problems, this approach can increase the quality of the final policy.\n\nActually, the contribution of the paper is a simple extension of existing methods: using a RNN instead of a simple NN in imitation learning models. First of all, when dealing with classical environments such as Atari, many papers propose to use the last N frames as a state encoding (instead of the last frame), following the same intuition. The studied setting thus corresponds to the PO-MDP case and using a RNN in POMDP is for example what is done in  [Merel etal. 2017]. Moreover, the problem of imitation learning (and particularly inverse RL) in POMDP has been of the interest of many papers like [Choi et al. 2008] for instance and many more, and it is unclear what is the positioning of this paper w.r.t existing works. Since the paper proposes just to encode history with a RNN, the proposed solution lacks of originality, and the contribution of the paper in term of model is quite low.  But the authors explain how this can be instantiated in three different settings (IRL, GAIL and BC) -- note that the section concerning the use of Adaboost is not clear and could be better described -- which can be of the interest of the community. \nConcerning the experiments, I don't understand what is the split between training and testing data. Is it pairs of state-action coming from the experts ? or trajectories ? Moreover, I don't understand why these environments correspond to POMDP cases and the authors have to give details on that. For instance, mountain-car is clearly not a POMDP problem in its classical shape, nor Acrobot. As if, it makes the experiments very difficult to reproduce. The interest of using the RNN to encode history does not seem clear for each of the cases since it often degrades the final performance, so I don't know exactly what insights I can extract from the paper.\n\nPro:\n* The approach is proposed for IRL, GAIL and BC\n\nCons:\n* Lack of positionning w.r.t POMDP litterature\n* Lack of details in the experiments, and lack of good experimental results\n* Low contribution in term of model\n\n\n[Merel et al. 2017]  Learning human behaviors from motion capture\nby adversarial imitation\n[Choi et al.] Inverse Reinforcement Learning in Partially Observable\nEnvironments",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}