{
    "Decision": {
        "metareview": "This paper studies the role of pooling in the success underpinning CNNs. Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training. \n\nAll reviewers agreed that this is a paper asking an important question, and that it is well-written and reproducible. On the other hand, they also agreed that, in its current form, this paper lacks a 'punchline' that can drive further research. In words of R6, \"the paper does not discuss the links between pooling and aliasing\", or in words of R4, \"it seems to very readily jump to unwarranted conclusions\". In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit.  ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Extra iteration needed"
    },
    "Reviews": [
        {
            "title": "Serious empirical study, but somewhat unsurprising and expected conclusions. ",
            "review": "This paper asks what is the role of pooling in the success story of CNNs applied to computer vision. \nThrough several experimental setups, the authors conclude that, indeed, pooling is neither necessary nor sufficient to achieve deformation stability, and that its effect is essentially recovered during training. \n\nThe paper is well-written, it is clear, and appears to be readily reproducible. It addresses an interesting and important question at the interface between signal processing and CNNs. \n\nThat said, the paper does not produce any clear novel results. It does not provide any theoretical result, nor any new algorithm. Its contributions consist of three empirical studies, demonstrating that (i) the benefits of pooling in terms of deformation stability can be achieved through supervised learning the filters instead (sec 3), (ii) the mechanism to obtain stability through learning essentially consists on reducing the bandwidth of (some) filters (sec4), and (iii) that this mechanism is data-dependent (sec 5). None of these studies strike the reviewer as particularly revealing. Moreover, the reviewer felt that the authors could have built on those findings to ask (and hopefully answer) a few interesting questions, such as:\n-- Nowhere in the paper there is a discussion about critical Nyquist sampling and the need to reduce the bandwidth of a signal prior to downsampling it in order to avoid aliasing. Average pooling provably does it, and learnt filters do it provided they indeed become bandlimited. What are the links between deformation stability and the ability to avoid aliasing? \n-- How many lowpass antialiasing filters are needed per layer to provide sufficient stability? \n-- Also, the authors should relate this study with similar works that do the same in speech (e.g. https://www.isca-speech.org/archive/Interspeech_2018/abstracts/1371.html). \n\nIn conclusion, my impression is that this paper requires a major iteration before it can be of widespread interest to the community. I encourage the authors to think about the above points. \n\n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not enough evidence to conclude much about pooling",
            "review": "It is often argued that one of the roles of pooling is to increase the stability of neural networks to \ndeformations. This paper presents empirical evidence to contest this assertion, or at least qualify it.\n\nI appreciate empirical studies that question some of the widely accepted dogmas of deep learning. \nFrom this point of view, the present paper is certainly interesting.\n\nUnfortunately, the actual evidence presented is quite weak, and insufficient to draw far reaching \nconclusions. An obvious objection is the authors only consider two datasets, and a very small number of \nmore or less standard pooling methodologies. The effect of pooling is evaluated in terms of cosine \nsimilarlity, which is not necessarily a good proxy for the actual performance of a network.\n\nA more serious issue is that they seem to very readily jump to unwarranted conclusions. For example, \nthe fact that stability to deformations (by which I necessarily mean the specific type of deformations \nthat they consider) tends to decrease in the middle layers of neural networks during training does not \nmean that starting with a neural network with less stability would be better. Maybe some kind of \nspontaneous coarse-to-fine optimization is going on in the network. Similarly, it is obvious that smoother \nfilters are going to lead to more stable representations. However, they might be less good at discriminative \ntasks. Just because smoother filters are more stable does not automatically mean that they are more desirable.\n\nStability to deformations is an important but subtle topic in computer vision. For starters, it is difficult \nto define what kind of deformations one wants to be insensitive to in the first place. A useful model would \nlikely incorporate some notion of deformations at multiple different length scales. \n\nJust showing that one network is better than another wrt some arbitrarily defined simple class of deformations \nwith no reference to actual recognition performance, speed of training, or interpretation of the nature of \nthe deformations and the learned filters is not very convincing. I would particularly like to emphasize the \nlast point. I would really like to understand what pooling actually does, not just at the level of \"if you \nturn it off, then cosine similarity will decrease by this much or that much.\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ask good questions but need more insightful analysis",
            "review": "The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Ask good questions but need more insightful analysis ",
            "review": "The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}