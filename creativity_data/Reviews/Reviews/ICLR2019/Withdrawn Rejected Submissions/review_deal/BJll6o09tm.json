{
    "Decision": {
        "title": "Meta-review",
        "metareview": "This paper proposes a simple modification of the Adam optimizer, introducing a hyper-parameter 'p' (with value in the range [0,1/2]) parameterizing the parameter update:\ntheta_new = theta_old + m/v^p\nwhere p=1/2 falls back to the standard Adam/Amsgrad optimizer, and p=0 falls back to a variant of SGD with momentum. \n\nThe authors motivate the method by pointing out that:\n - Through the value of 'p', one can interpolate between SGD with momentum and Adam/Amsgrad. By choosing a value of 'p' smaller than 0.5, one can therefore use perform optimization that is 'partially adaptive'.\n - The method shows good empirical performance.\n \nThe paper contains an inaccuracy, which we hope will be solved before the final version. The authors argue that the 1/sqrt(v) term in Adam results in a lower learning rate, and the authors argue that the effective learning rate \"easily explodes\" (section 3) because of this term, and that a \"more aggressive\" learning rate is more appropriate. This last point is false; the value of 1/sqrt(v) can be smaller or larger than 1 depending on the value of 'v', and that a decrease in value of 'p' can result in either an increase or decrease in effective learning rate, depending on the value of v. The value of 'v' is a function of the scale of loss function, which can really be arbitrary. (In case of very high-dimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, e.g. in image or video modeling the loss function tends to be of a much larger scale than with classification.)\n\nThe authors promise to include a comparison to AdamW [Loshchilov, 2017] that includes tuning of the weight decay parameter. The lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to AdamW. However, the methods offer potentially orthogonal (and combinable) advantages.\n\n[Loshchilov, 2017] https://arxiv.org/pdf/1711.05101.pdf\n",
        "recommendation": "Reject",
        "confidence": "4: The area chair is confident but not absolutely certain"
    },
    "Reviews": [
        {
            "title": "simple generalization of AMSgrad/momentum, good test data/models, results not significant/compelling",
            "review": "The idea is simple and promising: generalize AMSgrad and momentum by hyperparameterizing the p=1/2 in denominator of ADAM term to be within [0,1/2], with 0 being momentum case.  It was good to see the experiments use non-MNIST data (e.g. ImageNet, Cifar) and reasonable CNN models (ResNet, VGG).  However, the experimental evaluation is not convincing that this approach will lead to significant improvements in optimizing such modern models in practice.  \n\nOne key concern and flaw in their experimental work, which was not addressed, nor even raised, by the authors as a potential issue, is that their PADAM approach got one extra hyperparameter (p) to tune its performance in their grid search than the competitor optimizers (ADAM, AMSgrad, momentum).  So, it is not at all surprising that given it has one extra parameter, that there will be a setting for p that turns out to be a bit better than 0 or 1/2 for any given data/model setup and weight initialization/trajectory examined.  So at most this paper represents an existence proof that a value of p other than 0 or 1/2 can be best.  It does not provide any guidance on how to find p in a practical way that would lead to wide adoption of PADAM as a replacement for the established competitor optimizers. As Figures 2 and 3 show, momentum ends up converging to as good a solution as PADAM, and so it doesn't seem to matter in the end that PADAM (or ADAM) might seem to converge a bit faster at the very beginning.\n\nThis work might have some value in inspiring follow-on work that could try to make this approach practical, such as adapting p somehow during training to lead to truly significant speedups or better generalization.  But as experimented and reported so far, this paper does not give readers any reason to switch over to this approach, and so the work is very limited in terms of any significance/impact.  Given how simple the modification is, the novelty is also limited, and not sufficient relative to the low significance.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The contribution is relatively minor and an important baseline is missing in comparison.",
            "review": "This paper proposes a small modification to current adaptive gradient methods by introducing a partial adaptive parameter, showing improved generalization performance in several image classification benchmarks.\n\nPros:\n- The modification is simple and easy to implement.\n- The proposed method shows improved performance across different datasets, including ImageNet.\n\nCons:\n- Missing an important baseline - AdamW (https://arxiv.org/pdf/1711.05101.pdf) which shows that Adam can generalize as well as SGD and retain faster training. Basically, the poor generalization performance of Adam is due to the incorrect implementation of weight decay.\n- The experimental results for Adam are not convincing. It's well-known that Adam is good at training but might perform badly on the test data. However, Adam performs much worse than SGD in terms of training loss in all plots, which is contrary to my expectation. I doubt that Adam is not tuned well. One possible explanation is that the training budget is not enough, first-order methods typically require 200 epochs to converge. So I suggest the authors training the networks longer (make sure the training loss levels off before the first drop of learning rate.).\n- Mixing the concept of generalization and test performance. Note that generalization performance typically measures the gap between training and test error. To make the comparison fair, please make sure the training error is zero (I expect both training error and training loss should be close to 0 on CIFAR).\n- In terms of optimization (convergence) performance, I cannot think of any reason that the proposed method would outperform Adam (or Amsgrad). The convergence analysis doesn't say anything meaningful.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Excellent contribution to solving an important problem",
            "review": "The authors propose a modification of existing adaptive variants of SGD to avoid problems with generalization. It is known that adaptive gradient algorithms such as Adam tend to find good parameter values more quickly initially, but in the later phases of training they stop making good progress due to necessarily low learning rates so SGD often outperforms them past a certain point. The suggested algorithm Padam achieves the best of both worlds, quick initial improvements and good performance in the later stages.\n\nThis is potentially a very significant contribution which could become the next state-of-the-art optimization method for deep learning. The paper is very clear and well-written, providing a good overview of existing approaches and explaining the specific issue it addresses. The authors have included the right amount of equations so that they provide the required details but do not obfuscate the explanations. The experiments consist of a comprehensive evaluation of Padam against the popular alternatives and show clear improvements over them.\n\nI have not evaluated the convergence theorem or its proof since this is not my area of expertise. One thing that stood out to me is that I don't see why theta* should be unique.\n\nSome minor suggestions for improving the paper:\n\nTowards the end of section 2 you mention a non-convergence issue of Adam. It would be useful to add a few sentences to explain exactly what the issue is.\n\nI would suggest moving the details of the grid search for p to the main text since many readers would be interested to know what's typically a good value for this parameter.\n\nWould it make sense to try to adapt the value of p, increasing it as the training progresses? Since that's an obvious extension some comment about it would be useful.\n\nOn the bottom of page 6: \"Figures 1 plots\" -> \"Figure 1 plots\".\n\nMake sure to protect the proper names in the bibliography so that they are typeset starting with uppercase letters.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}