{
    "Decision": {
        "metareview": "The paper presents an interesting empirical analysis showing that increasing the batch size beyond a certain point yields no decrease in time to convergence. This is an interesting finding, since it indicates that parallelisation approaches might have their limits. On the other hand, the study does not allow the practitioners to tune their hyperparamters since the optimal batch size is dependent on the model architecture and the dataset. Furthermore, as also pointed out in an anonymous comment, the batch size is VERY large compared to the size of the benchmark sets. Therefore, it would be nice to see if the observation carries over to large-scale data sets, where the number of samples in the mini-batch is still small compared to the total number of samples. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Interestng empirical analysis but insights might be limited"
    },
    "Reviews": [
        {
            "title": "Limited insights in the understanding of the batch size effect",
            "review": "The work presented relates to the impact batch-size on the learning performances of common neural network architectures.\n\nPro: having comprehensive study of the limit of gradient-based methods is very useful in practice. This work can help practitioner to limit the number of machines used for optimization.\n\nCons: very little can be deduced from these experiments:\n- \"Increasing the batch size beyond a certain point yields no improvement in wall-clock time to convergence, even for a system with perfect parallelism.\" was a know fact (they cite Ma et al (2017) who even proved it theoretically.\n- \"Increasing the batch size leads to a significant increase in generalization error, which cannot be mitigated by existing techniques.\". It is not clear that all the regularization techniques have been tried by the authors, the increase of generalization error is very small, and there is no explanation or insight given by the authors to explain this phenomenon, making this finding of limited interest.\n- \"Dataset size is not the only factor determining the computational efficiency of large batch training.\" is something obvious to say, as there are plenty of factors that determine the computational efficiency (network connection, map-reduce implementation, etc.)\n\nEven the suggestions for future work of the authors in the conclusion does not help much: they suggest to look at \"alternative forms of parallelism\", without citing or giving any clue of what could be such alternative forms. \nAlso, there is no discussion around lock-free\n\nThe authors refer to Ma et al. (2017) for a theoretical analysis of the effect of the batch size, but they skip all the past and very relevant literature on the topic of the effect of the batch size on the convergence. For example, it is recommended to increase the size of the batch size as the iterations increase.\n\nFinally, there is no discussion on the lock-free gradient descent, that is often suggested as an alternative to batching.\n\nIn conclusion, I'm not convinced there is enough material to accept this paper at the next ICLR conference.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insightful empirical study of the effect of batch size for convergence speed",
            "review": "This paper empirically investigates the effect of batch size on the convergence speed of the mini-batch stochastic gradient descent of popular deep learning models. The fact that there is a diminishing return of batch size is not very surprising and there is a well-known theory behind it, but the theory doesn't exactly tell when we will start to suffer from the diminishing return. Therefore, it is quite valuable for the community to have an empirical analysis across popular ML tasks and models. In this regard, however, It would've been even nicer if the paper covered more variety of popular ML models such as Machine Translation, Speech Recognition, (Conditional) Image Generation, etc which open source implementations are readily available. Otherwise, experiments in this paper are pretty comprehensive. The only additional experiment I would be interested in is to tune learning rate for each batch size, rather than using a base learning rate everywhere, or simple rules such as LSR or SRSR. Since the theory only gives us asymptotic form of the optimal learning rate, empirically you should be tuning the learning rate for each batch size. And this is not totally unrealistic, because you can use a fraction of computational time to do cross-validation for searching the learning rate.\n\npros:\n* findings provide us useful direction for future research (that data-parallelism centered distributed training is going to hit the limit soon)\n* extensive experiments across 5 datasets and 6 neural network architectures\n\ncons:\n* experiments are a bit too much focused on image classification\n* error bars in figures could've provided greater confidence in robustness of findings",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some interesting empirical results for a popular problem",
            "review": "Summary:\nThe authors present an empirical analysis of how the size of SGD batches affects neural networks' training time.   \n\nStrengths:\nAs mini-batches training is highly popular nowadays, the problem emphasized by the authors may have a high impact in the community. Together with recent analysis on the generalization properties of over-parametrized models, the paper may help understand more general open problems of neural networks' training. A nice contribution of the paper is the observation that different phases of scaling behaviour exist across a range of datasets and architectures.  \n\nWeaknesses:\nBased on empirical evaluation, the paper cannot make any claim about the generality of the obtained results. Even if the authors' analysis is based on a large set of benchmarks, it is hard to asses whether and how the results extend to cases that are not included in Section 4. In particular, it is not clear how the definition of different training phases can help the practitioner to tune the training parameters, as the size and range of the different regimes depend so strongly on the model's architecture and dataset at hand.  \n\nQuestions:\n- have the properties of mini-batches training been explored from a formal/theoretical perspective? do those results match and confirm the proposed empirical evaluation?\n- how are the empirical results obtained in the experiment section expected to depend on the specific dataset/benchmark? For example,  given a particular architecture, what are the key features that define the three training phases (shape of the nonlinearity, number of layers, underlying distribution of the dataset)?\n- what is a  batch size that does not allow one to 'fully utilize our available compute'?\n- does the amount of over-parameterization in the model have any effects on the definition of the training phases? How are the results obtained in the paper linked to the generalization gap phenomenon?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}