{
    "Decision": {
        "metareview": "Two out of three reviews for this paper were provided in detail, but all three reviewers agreed unanimously that this paper is below the acceptance bar for ICLR. The reviewers admired the clarity of writing, and appreciated the importance of the application, but none recommended the paper for acceptance due largely to concerns on the experimental setup.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "No strong reviewer support"
    },
    "Reviews": [
        {
            "title": "Nice writing. Lack of significant contribution. Insufficient experimental evidence.",
            "review": "For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins’ native structure.\nThe authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.\n\nMy overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario. Also, the compared methods don’t really use the validation set from the complex data for training at all. Thus the experiment comparison is not really fair. 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling. Specifically, the testing dataset is fixed. A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.\n\nSince this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis. Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can. Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods. Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.\n\nOverall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.\n\n\nSome minor issues:\n1)\ton page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?\n\n2)\tIt is not clear what the “replicates” refer to in the experiments.\n\n3)\tSome discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Decent application paper and setup for siamese networks",
            "review": "Summary:\nThis paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces. They show improvements in predictive performance over some other recent deep learning methods. \nThe work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.\n\nNovelty:\nThe main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN\n\nClarity:\n- The paper is well written, with ample background into the problem.\n\nSignificance:\n- Their method improves over prior deep learning approaches to this problem. However, the results are a bit misleading in their reporting of the std error. They should try different train/test splits and report the performance.\n- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community\n- Protein conformation information is not required by their method\n\nComments:\n- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)\n\n-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred\n\n- The authors use a balanced ratio of positive and negative examples. The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones. Can they show performance at various ratios of positive:negative examples? In case there is a consistent improvement over prior methods, then this would be a clear winner\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "good idea but unclear model",
            "review": "This manuscript applies transfer learning for protein surface prediction. The problem is important and  the idea is novel and interesting. However, the  transfer learning model is unclear. \nPros:  interesting and novel idea\nCons:  unclear transfer learning model, insufficient experiments. \n\nDetail: section 4 describes the transfer learning model used in the work, but the description is unclear. It is unknown the used model is a new model or existing model. Besides, in the experiments, the proposed method is not compared to other transfer learning methods.  Thus, the evidence of the experiments is not enough. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}