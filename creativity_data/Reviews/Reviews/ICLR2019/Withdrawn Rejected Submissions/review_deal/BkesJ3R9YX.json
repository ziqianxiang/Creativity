{
    "Decision": {
        "metareview": "Strengths: The paper presentation was assessed as being of high quality. Experiments were diverse in terms of datasets and tasks.\n\nWeaknesses: Multiple reviewers commented that the paper does not present substantial novelty compared to previous work.\n\nContention: One reviewer holding out on giving a stronger rating to the paper due to the issue of novelty. \n\nConsensus: Final scores were two 6s one 3. \n\nThis work has merit, but the degree of concern over the level of novelty leads to an aggregate rating that is too low to justify acceptance. Authors are encourage to re-submit to another venue.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Solid work but novelty concerns held back the paper from rising above acceptance threshold"
    },
    "Reviews": [
        {
            "title": "Nice and diverse experiments, slightly limited novelty",
            "review": "# 1. Summary\nThis paper presents a novel spatio-temporal attention mechanism. The spatial attention is decomposed from the temporal attention and acts on each frame independently, while the temporal attention is applied on top of it on the temporal domain. The main contribution of the paper is the introduction of regularisers that improve performance and interpretability of the model.\n\nStrengths:\n* Quality of the paper, although some points need to clarified and expanded a bit more (see #2)\n* Nice diversity of experiments, datasets and tasks that the method is tested on (see #4)\n\nWeaknesses:\n* The paper do not present substantial novelty compared to previous work (see #3)\n\n\n# 2. Clarity and Motivation\nThe paper is in general clear and well motivated, however there are few points that need to be improved:\n* How is the importance mask (Eq. 1) is defined? The authors said “we simply use three convolutional layers to learn the importance mask”, however the convolutional output should be somehow processed to get out the importance map, in order to match the same sizes of X_i. The details of this network are missing to be able to reproduce the model.\n* The authors introduced \\phi(H) and \\phi(X) which are feedforward networks, but their definition and specifics are not mentioned in the paper.\n* It is not clear how Eq. 9 performs regularization of the mask. Can the authors give an intuition about the definition of L_{contrast}? What does it encourages? In which cases might it be useful?\n* Why does L_{unimodal} need to encourage the temporal attention weights to be unimodal? It seems that the assumption is valid because of the nature of the dataset, i.e., the video clips contain only a single action with some “background” frames in the beginning and the end. This is not valid in general. Can the authors discuss about this maybe with an example?\n\n\n# 3. Novelty\nThe main concern of the proposal in this paper is its novelty. Temporal attention pooling have been explored in other papers; just to cite a popular one among others:\n* Long, Xiang, et al. \"Attention clusters: Purely attention based local feature integration for video classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n* Other paper from the youtube8m workshops explore the same ideas: https://research.google.com/youtube8m/workshop2017/ \nSec. 2.2 should be expanded by including papers and discuss how the presented temporal attention differs from that.\n\nMoreover spatio-temporal attention has been previously explored. For example, the following paper also decouple the spatial and temporal component as the proposal:\n* Song, Sijie, et al. \"An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data.\" AAAI. Vol. 1. No. 2. 2017.\nThis is just an example, but there are there are other papers that model the spatio-temporal extent of videos without attention for action recognition. The authors should expand Sec. 2 by including such relevant literature.\n\n\n# 4. Experimentation\nThe experiments are carried on video action recognition task on three public available datasets, including HMDB51, UCF101 and Moments in Time. The authors show a nice ablation study by removing the main components of the proposed method and show nice improvements with respect to some baseline (Table 1). Although the results are not too close to the state of the art for video action recognition on HMDB51 and UCF101, the authors first show nice accuracy on Moments in Time (Table 2).\n\nMoreover the authors show that the model can be useful on the more challenging task of weakly supervised action localization (UCF101-24, THUMOS). Specifically, spatial attention is used to localize the action in each frame by thresholding, showing competitive results (Table 3). Although some more recent references are missing, see the following paper for example:\n* G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin. \"Online Real time Multiple Spatiotemporal Action Localisation and Prediction.\" ICCV, 2017.\nThen the authors tested also for temporal action localization (Table 4).\n\nIn general, the paper is not showing state-of-the-art results, however the diversity of experiments, datasets and tasks that are presented makes it pretty solid and interesting.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Minor novelty",
            "review": "A method for activity recognition in videos is presented, which uses spatial soft attention combined with temporal soft attention. In a nutshell, a pixelwise mask is output and elementwise combined with feature maps for spatial attention, and temporal attention is a distribution over frames. The method is tested on several datasets.\n\nMy biggest concern with the paper is novelty, which is rather low. Attention models are one of the most highly impactful discoveries in deep learning, which have been widely and extensively studied in computer vision, and also in activity recognition. Spatial and temporal attention mechanisms are now widely used by the community. I am not sure to see the exact novelty of the proposed, it seems to be very classic: soft attention over feature maps and frames is not new. Using attention distributions for localization has also been shown in the past.\n\nThis also shows in the related works section, which contains only 3 references for spatial attention and only 2 references for temporal attention out of a vast body of known work.\n\nThe unimodality prior (implemented as log concave prior) is interesting, but uni-modality is a very strong assumption. While it could be argued that spurior attention should be avoided, unimodality is much less clear. For this reason, the prior should be compared with even simpler priors, like total variation over time (similar to what has been done over space).\n\nThe ablation study in the experimental section shows, that the different mechanisms only marginally contribute to the performance of the method: +0.7p on HMDB51, slightly more on UCF101. Similarly, the different loss functions only very marginally contribute to the performance.\n\nThe method is only compared to Sharma 2015 on these datasets, which starts to be dated and is not state of the art anymore. Activity recognition has recently very much benefitted from optimization of convolutional backbones, like I3D and variants.\n\nThe LSTM equations at the end of page are unnecessary because widely known.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A spatial-temporal attention model, missing some baselines. ",
            "review": "The paper propose an end-to-end technique that applies both spatial and temporal attention. The spatial attention is done by training a mask-filter, while the temporal-attention use a soft-attention mechanism.  In addition the authors propose several regularization terms  to directly improve attention. The evaluated datasets are action recognition datasets, such as HMDB51, UCF10, Moments in Time, THUMOS’14. The paper reports SOTA on all three datasets. \n\n\n\nStrengths:\n\nThe paper is well written: easy to follow, and describe the importance of spatial-temporal attention. \n\nThe model is simple, and propose novel attention regularization terms. \n\nThe authors evaluates on several tasks, and shows good qualitative behavior. \n\n\nWeaknesses:\n\nThe reported number on UCF101 and HMDB51 are confusing/misleading.  Even with only RGB, the evaluation miss numbers of models like ActionVLAD with 50% on HMDB51 or Res3D with 88% on UCF101. I’ll also add that there are available models nowadays that achieve over 94% accuracy on UCF101, and over 72% on  HMDB51. The paper should at least have better discussion on those years of progress. The mis-information also continues in THUMOS14, for instance R-C3D beats the proposed model. \n\nIn my opinion the paper should include a flow variant. It is a common setup in action recognition, and a good model should take advantage of these features. Especially for spatial-temporal attention, e.g., VideoLSTM paper by Li. \n\nIn general spatial attention over each frame is extremely demanding. The original image features are now multiplied by 49 factor, this is more demanding in terms of memory consumption than the flow features they chose to ignore.  The authors reports on 15-frames datasets for those short videos. But it will be interesting to see if the model is still useable on longer videos, for instance on Charades dataset. \n\nCan you please explain why you chose a regularized making instead of Soft-attention for spatial attention? \n\nTo conclude: \nThe goal of spatial-temporal attention is important, and the proposed approach behaves well. Yet the model is an extension of known techniques for image attention, which are not trivial to apply on long-videos with many frames. Evaluating only on rgb features is not enough for an action recognition model. Importantly, even when considering only rgb models, the paper still missed many popular stronger baselines. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}