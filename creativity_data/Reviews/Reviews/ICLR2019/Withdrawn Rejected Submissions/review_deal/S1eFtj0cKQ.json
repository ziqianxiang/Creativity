{
    "Decision": {
        "metareview": "This paper presents empirical evaluation and comparison of different generative models (such as GANs and VAE) in the continual learning setting. \nTo avoid catastrophic forgetting, the following strategies are considered: rehearsal, regularization, generative replay and fine-tuning. The empirical evaluations are carried out using three datasets (MNIST, Fashion MNIST and CIFAR). \n\nWhile all reviewers and AC acknowledge the importance and potential usefulness of studying and comparing different generative models in continual learning, they raised several important concerns that place this paper bellow the acceptance bar: (1) in an empirical study paper, an in-depth analysis and more insightful evaluations are required to better understand the benefits and shortcomings of the available models (R1 and R2), e.g. analyzing why generative replay fails to improve VAE, why is rehearsal better for likelihood models, and in general why certain combinations are more effective than others – see more suggestions in R1’s and R2’s comments. The authors discussed in their response to the reviews some of these questions, but a more detailed analysis is required to fully understand the benefits of this empirical study. (2) The evaluation is geared towards quality metrics for the generative models and lacks evaluation for catastrophic forgetting in continual learning (hence it favours GANs models) -- See R3’s suggestion how to improve. \n\nTo conclude, the reviewers and AC suggest that in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "A comprehensive evaluation of existing methods lacking novelty and insight",
            "review": "This paper evaluates and compares various methods for learning GANs in a Continual Learning setting, i.e., only some of the classes are available during training. It evaluates different continual learning methods including rehearsal, EWC and generative replay applied to training several deep generative models, like GAN, CGAN, WGAN, WGAN-GP, VAE and CVAE on MNIST, Fashion MNIST and CIFAR. The authors conclude with these experimental results that generative replay is the most effective method for such a setting, and found it is difficult to generate CIFAR10 images that can be classified successfully by an image classifier.\n\nI appreciate the authors for providing so much detailed experimental results to the community, but this paper lacks novelty in general. All the CL methods the authors evaluate come from other papers that are already using these methods for generative models: rehearsal has been used in VCL Nguyen et al. (2017), EWC comes directly from Seff et al. (2017), and generative replay has been used by Wu et al. (2018a). The authors also fail to provide any valuable insight with these experimental results, e.g., analyzing why generative replay fails to improve VAEs. \n\nI expect to see more exciting results coming from the authors, but the paper is not mature enough for acceptance this time.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical analysis of CL is welcomed, but a few concerns with the experimental set-up.",
            "review": "This paper performs an empirical comparison of models and CL methods in a generative setting. The main motivation of the paper is to make statements about which model/method combinations are best to use for generative tasks in the CL setting. In short, the paper provides an empirical analysis and evaluation of the combination of CL methods and generative models.\n\nThe datasets used for comparison are MNIST, Fashion MNIST, and CIFAR10. For each dataset, sequential (class by class) generative tasks are introduced, aligning with the CL setting. The models investigated are VAEs, GANs, and WGANs, along with their (class) conditional counter-parts. The CL methods investigated are (i) fine-tuning (a simple baseline), (ii) rehearsal methods, (iii) elastic weight consolidation (EWC), and (iv) generative replay (GR). The authors propose to use two evaluation metrics: Fréchet Inception Distance (FID) measures the quality of the generated images, and fitting capacity (FC) measures the usefulness of the images to train classifiers.\n\nPros:\n- The authors are correct in pointing out that most of the work on CL has been restricted to the discriminative case, and that there is value in exploring generative tasks in the CL setting.\n- Empirical and experimental evaluation of this sort are useful, and help the community better understand the relationship between model, CL method, and task. Such an evaluation and in-depth analysis is welcomed in CL, especially in the generative setting.\n- The authors draw a number of useful conclusions e.g., regarding the usefulness and dangers of employing the different CL methods.\n\nCons:\n- My main concern with this paper regards the evaluation metrics used. The authors propose quality metrics for the generative model, both of which (directly or indirectly) measure the quality of the generated images. In this setting, it is unsurprising that GANs outperform VAEs, as they are known to generate higher-quality images. This however, does not necessarily mean that they are better at the continual learning task (i.e., avoiding catastrophic forgetting). It seems to me that one source from which to draw would be [1], which conducted a very rigorous and useful empirical evaluation of generative models, and the methodology followed there (i.e., evaluating marginal log-likelihoods via annealed importance sampling) would be more convincing evidence for empirical comparison of models, as it would somewhat detach the quality of the generated images from the ability of the model to avoid catastrophic forgetting.\n\nUsing their proposed image-quality metrics, the authors make statements such as: \"Our results do not give a clear distinction between conditional and unconditional models. However, adversarial methods perform significantly better than variational methods. GANs variants are able to produce better, sharper quality and variety of samples, as observed in Fig. 13 and 14 in Appendix G. Hence, adversarial methods seem more viable for CL.\" My impression is that this statement on the viability of VAEs vs GANs for CL, which is a major point of the paper, does not follow from the empirical results on the quality of the generated images. It seems quite predictable that the GAN-based models would produce higher quality images, regardless of catastrophic forgetting.\n\nAdditional (minor) comments:\n- Sec. 2 could consist of a more thorough review of the literature, with a more in-depth comparison of the different CL methods proposed and evaluated in the paper.\n- Sec. 2 contains a number of statements of the form: \"restricted to VAEs only\". For many of the cases it is not immediately clear why this is true, and in my opinion the authors should either drop those comments, or make them rigorous.\n- VCL \"use specific weights for each task, which only works for the setting where the number of tasks is known in advance\". Unclear what exactly this means or why this is true.\n- \"while the teacher retains knowledge\" - how does it \"retain knowledge\", how is this then transferred to the student, and why is this restricted to VAEs?\n\nExperimental protocol:\n- Core-sets for the rehearsal as proposed by [1] could be an interesting extension. It is unclear how the samples were selected for rehearsal, and core-sets represent a principled way to do so, that would also be interesting to compare in this setting to a random baseline.\n- For VAEs, a potentially better metric of their ability (other than the log-likelihood as suggested by [2]) would be fitting capacity (or other metric) over learned latent space rather than the reconstructed image-space.\n\nOverall, my impression is that while an empirical analysis of CL methods in the generative setting is a useful concept, the submission in its current form requires some improvement. In particular, I am worried that the choice of evaluation metrics may lead to incorrect (or partially correct) conclusions, which could of course have a negative impact on the research into CL. It also seems that the paper could use some further polishing in both writing and presentation. As such, I encourage the authors to continue the work on this empirical analysis, and perhaps submit in again to future conferences.\n\n[1] - Nguyen et al. Variational Continual Learning, ICLR 2018\n[2] - Wu et al. On the Quantitative Analysis of Decoder-Based Generative Models, ICLR 2017",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially nice empirical study, but needs more work on experimental analysis and discussion",
            "review": "This paper presents an empirical evaluation of continual learning approaches for generative modelling. Noting that much of previous work focuses on supervised tasks, the paper evaluates various combinations of continual learning strategies (EWC, rehearsal/replay-based, or generative replay) and generative models (GANs or likelihood-based).\nThe experiments evaluate all combinations on MNIST and Fashion MNIST, and the resulting best-performing combination on CIFAR.\nThe paper is well-written and structured, and although there are no new proposed algorithms or measures, I think this has the potential to be a useful empirical study on the relatively unstudied topic of continual learning with generative models.\n\nHowever, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate *why* certain combinations are more effective than others. For example:\n- Is the reason GANs are better than likelihood models with generative replay purely because of sample quality? Or is it sufficient for the generator to learn some key characteristics for a class that lead to sufficient discriminability?\n- Why is rehearsal better for likelihood models? (And how does this relate to the hypothesis of overfitting to a few real examples?)\n\nThe CIFAR-10 results also require more work - it is unclear why the existing approaches could not be made to work, and whether this is a fundamental deficiency in the existing approaches or other factors (hyperparameters, architecture choices, lack of time, etc). Presuming the sample quality is as good as in the WGAN-GP work (given the original implementation is used for experiments), why is this insufficient for generative replay? More detailed analysis / discussion, or another combinatorial study, would help for CIFAR too.\n\nSome comments:\n- The poor performance of EWC across the board is concerning. Was this implemented by computing the Fisher of the ELBO with respect to parameters? Was the empirical or true Fisher used? Why does the performance appear so poor compared to Seff et al (2017)? This suggests that either more thought is required on how to best protect parameters of generative models, or the baseline has not been properly implemented/tuned.\n- Given this is an entirely empirical study, I would strongly encourage the authors to release code sooner than the acceptance deadline - this can be achieved using an anonymous repository.\n- Figure 2 and 3 plots are a little difficult to parse without axis labels.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}