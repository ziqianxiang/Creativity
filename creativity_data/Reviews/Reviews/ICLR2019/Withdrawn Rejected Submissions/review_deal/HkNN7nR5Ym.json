{
    "Decision": "",
    "Reviews": [
        {
            "title": "Input-dependent normalization method",
            "review": "The authors proposed a normalization method that the rescaling parameters are predicted by the input data. It is shown to use an auto-encoder like network. As the proposed approach uses input-level normalization, it seems to be robust on changing the batch size during training and also showing better performance compared to BN, GN.\n\n(1) In related works (sec 2.2), it mentioned 'Dynamic Layer Normalization' and  the current proposed method looks similar to it. As far as I know, 'Dynamic Layer Normalization' also predicts the rescaling parameters based on the input feature. What are the differences to it?  \n\n(2) This is also related to FILM or conditional batch normalization\n\n(3) The authors introduced in abstract that an auto-encoder like network is used to predict parameters. However, the main context is quite confusing as a 'auto-encoder' network is used. It would be better not to use the term 'auto-encoder' if any reconstructing procedure is not used.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but too poorly explained",
            "review": "The idea seems rather interesting and the experimental results encouraging (although -- small point-- I found your plotted curves for BatchNorm on Cifar-100 rather suspicious, there was no discontinuity at the 2nd learning rate reduction).\n\nUnfortunately I don't think I can recommend to accept this paper as it was too un-clearly explained.  I think the authors should try to get the involvement of someone with more machine learning experience who might be able to solidify their ideas.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "In this paper, a new method was proposed to improve performance of batch normalization methods.",
            "review": "There are three major problems with the paper;\n\n1. The novelty of the paper is not sufficient since it is an incremental work on batch normalization (BN). In  addition, there are various similar approaches such as hypernetworks, SE-Net, statistical filtering etc. used for similar purposes.\n\n2. Experimental analyses are not sufficient. First, you should compare the proposed method both conceptually and experimentally with the above mentioned methods in detail. Second, you should also compare the results with recent BN methods such as switchable normalization. Third, you should provide the results for other types of DNNs and datasets.\n\n3. There are too many grammatical problems and typo in the paper, some which are addressed below;\n\nPlease define feature map more precisely. Is it vector, matrix etc? Please also define mean, variance etc. appropriately.\n\nPlease define weight and bias precisely. What does \\times used in (2) for \\omega \\times x_s denote?\n\nWhat do you mean by “low association-quality”? Please explain the terms “low association-quality”, “information loss”, “data flow”, “bottleneck” etc. more precisely.\n\nPlease explain how you partition feature maps into N groups in detail.\n\nIs equation (4) complete? What does { denote?\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}