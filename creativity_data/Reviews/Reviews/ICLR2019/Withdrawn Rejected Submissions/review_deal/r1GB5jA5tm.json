{
    "Decision": {
        "metareview": "The paper proposes adversarial sampling for pool-based active learning.\n\nThe reviewers and AC note the critical potential weaknesses on experimental results: it is far from being surprising the proposed method is better than random sampling. Ideally, one has to reduce the complexity under keeping the state-of-art performance. Otherwise, it is hard to claim the proposed method is fundamentally better than prior ones, although their targets might be different.\n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited experimental results"
    },
    "Reviews": [
        {
            "title": "Nice idea for important problem, but requires validation on algorithm speed.",
            "review": "This paper proposes adversarial sampling for pool-based active learning, which is a sublinear-time algorithm based on 1) generating “uncertain” synthetic examples and 2) using the generated example to find “uncertain” real examples from the pool. I liked the whole idea of developing a faster algorithm for active learning based on the nearest neighborhood method. However, my only & major concern is that one has to train GANs before the active learning process, which might cost more than the whole active learning process.  \n\nPros: \n- This paper tackles the important problem of reducing the time complexity needed for active learning with respect to the pool size. I think this is a very important problem that is necessary to be addressed for the application of active learning.\n-The paper is well written and easy to understand.\n-I think the overall idea is novel and useful even though it is very simple. I think this work has a very promising potential to be a building block for future works of fast active learning.\n\nCons:\n-There is no theoretical guarantee on the \"uncertainty\" of obtained real examples. \n-The main contribution of this algorithm is computational complexity, but I am not very persuaded by the idea of using the GAN in order to produce a sublinear (faster) time algorithm for active learning, since training the GAN may sometimes take more time that the whole active learning process. Explicitly describing situations where the proposed method is useful seems necessary. I would expect the proposed algorithm to be beneficial when there is a lot of queries asked and answered, but this seems unlikely to happen in real situations.  \n-Empirical evaluation is weak, since the algorithm only outperforms the random sampling of queries. Especially, given that sublinear nature of the algorithm is the main strength of the paper, it would have been meaningful to evaluate the actual time spent for the whole learning process including the training of GANs. Especially, one could also speed-up max entropy criterion by first sampling subset of data-points from the pool and evaluating upon them. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Nice Idea but Weak Experiments",
            "review": "The paper presents a pool-based active learning method that achieves sub-linear \nruntime complexity while generating high-entropy samples, as opposed to linear \ncomplexity of more traditional uncertainty sampling (i.e., max-entropy) methods. \nThis is achieved by using a generative adversarial network (GAN) to generate \nhigh-entropy samples that are then used by a nearest neighbor method to pick \nsamples from a pool, that are closest to the generated samples. The sub-linear \ncomplexity is achieved through the use of a k-d tree, combined with the fact \nthat similarity is computed on the feature space and samples can thus be indexed \nonce (as the feature space does not change while training).\n\nThe proposed idea builds on top of previously published work on Generative \nAdversarial Active Learning (GAAL). The main difference is the added nearest \nneighbor component, as GAAL is directly using the generated examples, thus \nachieving constant runtime complexity, rather than sub-linear.\n\nI like the overall direction and the idea of being able to perform uncertainty \nsampling in sub-linear time. The approach is interesting. However, the results \npresented in the paper are not strong and I do not see whether or not I should \nbe using this method over uncertainty sampling. Most importantly, the results \nare strongest only for the MNIST experiments, which are over a small dataset. \nGiven that the method is motivated by the scalability argument, I would like to \nsee at least one large scale experiment where it performs well, and more \nspecifically, outperform random sampling. Also, I would really like to see a \nmore principled and thorough experimental investigation with justifications for \nthe configurations used and with more comparisons to alternative approaches, \nsuch as GAAL, which has constant complexity.\n\nI believe that it would be better for your paper if you work a bit more on the \nexperimental evaluation and submit a revised version at a later deadline.\n\n== Background and Method ==\n\nThe background and method sections are clear and easy to follow. One improvement \nI can see is making figure 1 more clear, by maybe explicitly stating in the \nfigure what \"G\" and \"F\" are. One more point of interest is that the way you \nperform sample matching makes some smoothness assumption about the feature \nspace as related to the classifier uncertainty. I perceive this as a smoothness \nassumption on the decision boundary of the classifier and I do not know how true \nis may be for deep neural networks, but I can see how it may be true for \nlogistic regression models and support vector machines (SVMs), depending on the \nkernel used. I believe that this point and main assumption may be worth further \ndiscussion, given that it is also about the main difference your method has with \nrespect to GAAL.\n\nI do not have any other major comments for these sections as my main concerns \nare about the experiments section.\n\n== Experiments ==\n\nIn the experiments, it would be very useful to have plots against execution \ntime, given that the main motivation for this method is scalability. For \nexample, the method outperforms random sampling for small datasets, based on \nnumber of samples, but what happens when you look at execution time? Given that \nrandom sampling is very cheap, I imagine that it probably does better. Also, as \nmentioned earlier, I would like to see at least one experiment using a big \ndataset, where the method outperforms random sampling, as I am not currently \nconvinced of its usefulness.\n\nAlso, you present a lot of results and list observations but I felt there was \nnot much discussion as to why you observe/obtain some of the results. Given that \nyour method is not working very well for CIFAR, I would like to see a more \nthorough investigation as to why that may be the case. This investigation could \nconclude with some \"tips\" on when it may be a good idea to use your method over \nGAAL, or uncertainty sampling, for example.\n\nRegarding the experimental setup, I find lots of configuration choices very \narbitrary and have difficulty understanding how they were chosen. For example:\n\n  - For the two-class MNIST you use classes \"5\" and \"7\" and for the two-class \n    CIFAR you use classes \"automobile\" and \"horse\". Why is that? How did you \n    pick the two classes to use in each case? Do the results match for other \n    class pairs?\n  - \"learning rate of 0.01 that we decay by a factor of 10 at the 130th and \n    140th epochs\" -- end of page 6\n  - \"In contrast to the previous experiments we use a residual Wasserstein GAN \n    with gradient penalty and soft consistency term\" -- page 7 -- why do you \n    make that change?\n\nQuestions:\n  - Why do you think using Wasserstein GANs perform better than using DCGANs? -- section 5.3.1\n  - Why not compare to GAAL in all of figures 3 and 4?\n  - How/why were the number of samples you start with and sample in each round, \n    chosen? Do you observe any difference if you increase/decrease the number of \n    samples sampled in each round or if you start with fewer samples?\n  - How/why were these model architectures chosen?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "GAN-based active learning for query synthesis ",
            "review": "This paper proposed a query-synthesis-based active learning algorithm that uses GAN to generate high entropy sample; instead of annotating the synthesized sample, the paper proposed to find the most similar unlabeled data from the pool via nearest neighbor search, with the latter is the main contribution of the paper.\n\nPros: \n(1)\tthe paper is well written and easy to follow;\n(2)\tevaluations look reasonable and fair\n\nCons:\n(1)\tThe idea of using GAN for active query synthesis isn’t new. As the authors pointed out, this idea is mainly from GAAL (Zhu & Bento 2017). The main difference is sample matching that searches the nearest neighbor from pool and add the real unlabeled data for AL. So the novelty of the paper isn’t significant.\n(2)\tIn terms of accuracy comparison, on Cifar-10-ten classes experiments, all ASAL variants have similar accuracies as random sampling, while traditional pool-based max-entropy clearly works much better. Although the former is much faster (O(1) vs. O(N)), this benefit is mainly due to GAAL (Zhu & Bento 2017).\n\nThe paper provides additional evidence showing that GAN-based active learning might be an interesting research direction for active query synthesis. However, given the reasons above, particularly novelty, I think the authors might need to additional work to improve the method.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}