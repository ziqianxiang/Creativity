{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper is an extension of tensor parametrizing CNN. It is interesting but with limited originality. ",
            "review": "The paper propose fully parametrizing CNNs with a single, low-rank tensor. As compared to the previous work which parametrizing individual layer by tensor representation, this paper combine all parameters from each layers and model them as one tensor. This allows to regularize the whole network and drastically reduce the number of parameters by imposing a low-rank structure on that tensor. The experiments show higher compression rates with negligible drop in accuracy for human pose estimation. \n\nThe paper is well written by firstly introducing basic tensor decomposition and operations, then presenting how to use them to parametrizing CNN.  However, the concept of using tensor decomposition to parametrize the CNN is known, this paper is an extension by considering all layers together. Therefore, the originality is limited and incremental. \n\nThe parameters from different layers may have very different sizes, which make this method not very practical. Although we can manually use the same size of parameters for all layers, the problem of redundant information will become more severe. \n\nIn general, the parameters in different layers are supposed to be very different and uncorrelated. In this paper, the parameters from different layers are put together and low-rank structure is assumed. Hence, one question of why this will work well is not clear.  Furthermore, there is no theoretical support for the method to obtaining the higher compression. \n\nThe authors claim Tucker is easy to control the rank thus considered as the most flexible compression method. The justification of this claim should be provided, or the detailed experiment comparisons should be given. Why Tucker is more flexible is not clear. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "the paper starts with a nice idea, but the development is not clear enough",
            "review": "This paper starts with a nice and appealing idea --- parametrizing a CNN using just one high-order tensor. The modes of the tensor represent different \"coordinates\" of the weights of CNN. This sounds very reasonable.\n\nThe claimed contributions are also appealing: by doing so, the tensor representation can be used for 1) reducing redundancy and saving memory; 2) accelerating testing; 3) training the CNN from scratch.\n\nThe development of the paper is a bit disappointing since the above contributions were not clearly fleshed out. For example, it is very unclear to the reviewer how the parameterization can be used for training the CNN from scratch. This is a very nice and intriguing  point and should be addressed in detail in the paper. It is a little bizarre that the paper starts with a good introduction, and then introduces some preliminaries of tensors, and then directly goes to experiments. \nMany things claimed in the contributions were not addressed in detail. The reviewer is not sure if these things are considered trivial---but for readers this looks confusing.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough technical details and missing theory",
            "review": "In this paper, the authors propose to organize all parameters of a multilayer neural network in a higher order tensor (8 dimension) and use low-rank tensor decomposition models (Tucker and TT/MPS formats) to compress it. They applied this approach to a very specific architecture, the stacked hourglass architecture of Newell et all (2016) full convolutional neural network and used in the specific problem of human pose estimation.\nThe idea of the paper is simple and technically sounded but unfortunately, I found it not well presented with many important technical details missing. Also, there are not any theoretical justification for the method to work, the results are restricted to a very specific network architecture and only one application, which limits their generalization to a broader class of problems. Below, I provide the main comments about the paper:\n\nMajor issues:\n-\tThe paper claims that, for lower compression rates, the proposed method outperforms the uncompressed version of the network. What is the justification for this behavior? Is it related to a regularization effect? How this effect depends on the number of available samples? I think the authors should provide some theoretical insights about this behavior or, at least, a deeper experimental study.\n-\tThe authors do not give details on how the tensorized network is optimized/trained. They state that “All models were trained for 110 epochs using RMSprop (Tieleman & Hinton, 2012). The learning rate was varied from from 2.5e-4 to 1e-6 using a Multi-Step fixed scheduler.” But It is not clear if a back-propagation strategy can be still used after tensorization and what are the updating rules for the parameters in the tensor decomposition model. I think this missing information must be included in the paper, at least as an appendix.\n-\tThe title of section 2.2, “Fully-tensorized architecture” suggests that all design parameters of the network are compressed through a tensor decomposition, however, in the implementation details section, they state that total number of parameters is 15,830,976 where 14,555,776 are in the weights tensor and 1,675,200 (based convolutions) are not considered as part of the tensor decomposition model. It is not clear, why this particular tensorization approach is used.\n-\tThey compared the results of the compressed approach against an uncompressed architecture but reducing the number of parameters by choosing 64 channels of residual blocks instead of 128 as used in the compressed version. I think this difference in number of parameters prevents from obtaining a fair comparison of the results. Why the authors have chosen fewer parameters in the uncompressed version? If it is because there is a memory size limitation, I would suggest to use 64 channels in the compressed version too, so the results can be fairly compared.\n\nMinor comments:\n-\tTensor notation is a bit strange in this paper. Tensors are usually denoted by capital calligraphic letters or underlined bold capital letters but in this work the authors use calligraphic with tilde, which makes the notation a little bit overcrowded. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}