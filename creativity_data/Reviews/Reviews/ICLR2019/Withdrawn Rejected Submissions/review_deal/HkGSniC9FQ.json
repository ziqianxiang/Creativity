{
    "Decision": {
        "metareview": "Dear authors,\n\nAll reviewers pointed out the fact that your result is about the expressivity of the big network rather than its accuracy, a result which is already known for the literature.\n\nI encourage you to carefully read all reviews should you wish to resubmit this work to a future conference.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Limited contribution"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper considers the problem of building a composite network from several pre-trained networks and whether it is possible to ensure that the final output has better accuracy than any of its components. \n\nThe analysis done in the paper is that of a simple linear mixture of the outputs produced by each component and then by showing that if the output of the components are linearly independent then you can find essentially a better ensemble. This is a natural and straightforward statement with a straightforward proof. It is unclear to me what theoretical value does the analysis of the paper add. Further the linear independence assumption in the paper seems very strong to make the results of value. \n\nFurther the paper seems very hastily written with inconsistent notation throughout making the paper very hard to read. Especially the superscript and the subscript on x have been jumbled up throughout the paper. I recommend rejection and encourage the authors to first clean up notation to make it readable. ",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The result seems straight forward",
            "review": "This paper studies composite neural network performance from function composition perspective. In theorems 1, 2 and 3, the authors essentially prove that as the basis functions (pre trained components) increases (satisfying LIC condition), there are more vectors/objects can be represented by the basis. \n\nTo me, this is a very straight forward result. As the basis increases while the LIC condition is satisfied, we can of course represent more objects (the new component is one of them). I don't see any novelties here. The result is straightforward, and this should be a clear rejection.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not ready for publication",
            "review": "The paper aims at justifying the performance gain that is acquired by the use of \"composite\" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).\n\nI found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.\n\nExample from the Abstract:\n\n\"The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from otherâ€™s intelligence and diligence, and the other is saving the efforts in data preparation and resources\nand time in training\"\n\nThe main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., \"components\") in the input of a network then you have \"more information\", and this cannot be bad. Here are the corresponding claims in the Abstract:\n\n\"we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.\"\n\n\"if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved.\"\n\nHowever, this argument seems to be just about expressiveness; adding more features can be statistically problematic. \n\nFurthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.\n\nFinally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.\n\n\nThe motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don't see any connection between this and the use of pre-training.\n\nOther examples unclear statements from the intro:\n\n\"One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.\"\n\n\"Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once.\"\n\nThere are many typos in the paper including this one about X for the XOR function:\n\"Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR\"\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}