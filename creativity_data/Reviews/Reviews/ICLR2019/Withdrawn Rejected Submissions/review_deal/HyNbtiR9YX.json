{
    "Decision": {
        "metareview": "This paper proposes a document classification algorithm based on partitioned word vector averaging.\nI agree with even the most positive reviewer. More experiments would be good. This is a very developed old area.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "borderline paper"
    },
    "Reviews": [
        {
            "title": "A very well written paper with solid technical contribution.",
            "review": "A very well written paper with solid technical contribution. The impact to the community might be incremental.\n\nPros:\n1. I enjoyed reading the paper, very well written, clean, and organized.\n2. Comprehensive literature survey, the authors provided both enough context for the readers to digest the paper, and well explained how this work is different from the existing literature.\n3. Conducted extensive experiments.\n\nCons (quibbles):\nExperiments:\nThe authors didn't compare the proposed method against topic model (vanilla LDA or it’s derivatives discussed in related work). Because most topic models could generate vector representation for document too, and it's interesting to learn additional benefit of local context provided by the word2vec-like model.\n\nMethodology:\n1. About hyperparameters:\na. Are there principled way/guideline of finding sparsity parameters k in practice?\nb. How about the upper bound m (or K, the authors used both notation in the paper)?\n\n2. About scalability:\nHow to handle such large sparse word vectors, as it basically requires K times more resource compared to vanilla word2vec and it's many variants, when it’s used for other large scale downstream cases? (see all the industrial use cases of word vector representations)\n\n3. A potential alternative model: The motivation of this paper is that each word may belong to multiple topics, and one can naturally extend the idea to that \"each sentence may belong to multiple topics\". It might be useful to apply dictionary learning on sentence vectors (e.g., paraphrase2vec) instead of on word vectors, and evaluate the performance between these two models. (future work?)\n\nTypos:\nThe authors mentioned that \"(Le & Mikolov, 2014) use unweighted averaging for representing short phrases\". I guess the authors cited the wrong paper, as in that paper Le & Mikolov proposed PV-DM and PV-DBOW model which treats each sentence as a shared global latent vector (or pseudo word).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The work is incremental even though the experimental results are good and the method is well presented. ",
            "review": "\nPros:\nThe paper shows that we could have a better document/sentence embedding by partitioning the word embedding space based on a topic model and summing the embedding within each partition. The writing and presentation of the paper are clear. The method is simple, intuitive, and the experiments show that this type of method seems to achieve state-of-the-art results on predicting semantic similarity between sentences, especially for longer sentences. \n\nCons:\nThe main concern is the novelty of this work. The method is very similar to SCDV (Mekala et al., 2017). The high-level flow figure in appendix H is nearly identical as the Figure 1 and 2 in Mekala et al., 2017. The main difference seems to be that this paper advocates K-SVD (extensively studies in Arora et al. 2016) as their topic model and SCDV (Mekala et al., 2017) uses GMM. \nHowever, in the semantic similarity experiments (STS12-16 and Twitter15), the results actually use GMM. So I suppose the results tell us that we can achieve state-of-the-art performances if you directly combine tricks in SIF (Arora et al., 2017) and tricks in SCDV (Mekala et al., 2017).\nIn the document classification experiment, the improvement looks small and the baselines are not strong enough. The proposed method should be compared with other strong unsupervised baselines such as ELMo [1] and p-mean [2].\n\nOverall:\nThe direction this paper explores is promising but the contributions in this paper seem to be incremental. I suggest the authors to try either of the following extensions to strengthen the future version of this work. \n1. In addition to documentation classification, show that the embedding is better than the more recent proposed strong baselines like ELMo in various downstream tasks.\n2. Derive some theories. One possible direction is that I guess the measuring the document similarity based on proposed embedding could be viewed as an approximation of Wasserstein similarity between the all the words in both documents. The matching step in Wasserstein is similar to the pooling step in your topic model. You might be able to say something about how good this approximation is. Some theoretical work about doing the nearest neighbor search based on vector quantization might be helpful in this direction.\n\nMinor questions:\n1. I think another common approach in sparse coding is just to apply L1 penalty to encourage sparsity. Does this K-SVD optimization better than this L1 penalty approach? \n2. How does the value k in K-SVD affect the performances?\n3. In Aorora et al. 2016b, they constrain alpha to be non-negative. Did you do the same thing here?\n4. How important this topic modeling is? If you just randomly group words and sum the embedding in the group, is that helpful?\n5. In Figure 2, I would also like to see another curve of performance gain on the sentences with different lengths using K-SVD rather than GMM.\n \nMinor writing suggestions:\n1. In the 4th paragraph of section 3, \"shown in equation equation 2\", and bit-wise should be element-wise\n2. In the 4th paragraph of section 4, I think the citation after alternating minimization should be Arora et al. 2016b and Aharon et al. 2006 rather than Arora et al., 2016a\n3. In the 2nd paragraph of section 6.1, (Jeffrey Pennington, 2014) should be (Pennington et al., 2014). In addition, the author order in the corresponding Glove citation in the reference section is incorrect. The correct order should be Jeffrey Pennington, Richard Socher, Christopher D. Manning.\n4. In the 3rd paragraph of section 6.1, \"Furthermore, Sentence\"\n5. In the 6th paragraph of section 6.1, I thought skip-thoughts and Sent2Vec are unsupervised methods.\n6. In Table 2 and 3, it would be easier to read if the table is transposed and use the longer name for each method (e.g., use skip-thought rather than ST)\n7. In Table 2,3,4,5, it would be better to show the dimensions of embedding for each method\n8. Table 10 should also provide F1\n9. Which version of GMM is used in STS experiment? The one using full or diagonal covariance matrix? \n\n\n[1] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. NAACL\n[2] Rücklé, A., Eger, S., Peyrard, M., & Gurevych, I. (2018). Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations. arXiv preprint arXiv:1803.01400.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review of Unsupervised Document Representation using Partition Word-Vectors Averaging ",
            "review": "Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\\alpha$ weights (see page 4).These weights give \"how much\" this word belongs to a certain topic. For every topic we compute the $\\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   \n\nQuestions and remarks:\n     1) How sensitive is the method to a change in the number of topics (k)?\n    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful\n \nPoints in favor: \n   1) Good results and thorough tests \n    2) Paper is easy to read and follow \n\nPoints against:\nA very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. \n\n\nThe novelty of the paper is not enough to justify its acceptance at the conference.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}