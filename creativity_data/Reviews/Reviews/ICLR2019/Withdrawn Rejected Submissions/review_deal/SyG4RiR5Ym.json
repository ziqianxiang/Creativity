{
    "Decision": {
        "metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review for Neural Distribution Learning"
    },
    "Reviews": [
        {
            "title": "Re: Neural Distribution Learning for generalized time-to-event prediction",
            "review": "The authors propose a parametric framework (HazardNet) for survival analysis with deep learning where they mainly focus on the discrete-time case. The framework allows different popular architectures to learn the representation of the past events with different explicit features. Then, it considers a bunch of parametric families and their mixtures for the distribution of inter-event time. Experiments include a comprehensive comparison between HazardNet and different binary classifiers trained separately at each target time duration. \n\nOverall, the paper is well-written and easy to follow. It seeks to build a strong baseline for the deep survival analysis, which is an hot ongoing research topic recently in literature. However, there are a few weaknesses that should be addressed. \n\n1. In the beginning, the paper motivates the mixtures of distributions from MDN. Because most existing work focuses on the formulation of the intensity function, it is very interesting to approach the problem from the cumulative intensity function instead. Originally, it looks like the paper seeks to formulate a general parametric form based on MDN. However, it is disappointing that in the experiments, it still only considers a few classic parametric distributions. There is lack of solid technical connection between Sec 3.1, 3.2 and Sec 4.\n\n2. The discretization discussion of Sec 3.4 is not clear. Normally, the major motivation for discretization is application-driven, say, in hospital, the doctor regularly triggers the inspection event. However, how to optimally choose a bin-size and how to aggregate the multiple events within each bin is still not clear, which is not sufficiently discussed in the paper. Why is taking the summation of the events in a bin a proper way of aggregation? What if we have highly skewed bins?\n\n3. Although the comparison and experimental setting in Figure 4 is comprehensive, the paper misses a very related work \"Deep Recurrent Survival Analysis, https://arxiv.org/abs/1809.02403\", which also considers the discrete-time version of survival analysis. Only comparing with the binary classifiers is not quite convincing without referring to other survival analysis work.\n\n4. Finally, the authors state that existing temporal point process work \"have little meaning without taking into account censored data\". However, if inspecting the loss function of these work closely, we can see there is a survival term exactly the same as the log-cumulative hazard in Equation 3 that handles the censored case.\n\n5. A typo on the bottom of page 3, should be p(t) = F(t + 1) - F(t)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Deep Hazard Modeling with Mixture of Distributions ",
            "review": "This paper proposes to use a mixture of distributions for hazard modeling. They use the standard censored loss and binning-based discretization for handling irregularities in the time series. \n\nThe evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead. If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead. Moreover, the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling. The authors also donâ€™t report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling.\n\nA major baseline for mixture modeling is always non-parametric modeling. In this case, given that there are existing works on deep Cox hazard modeling, the authors need to show the advantages of their proposed mixture modeling against deep Cox models.\n\nOverall, the methodology in this paper is quite limited and the evaluation is non-standard. Thus, I vote for rejection of the paper.\n\n\n[1] Ranganath, Rajesh, et al. \"Deep Survival Analysis.\" Machine Learning for Healthcare Conference. 2016.\n\n[2] Katzman, Jared L., et al. \"DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.\" BMC medical research methodology 18.1 (2018): 24.\n\n[3] Lee, Changhee, et al. \"Deephit: A deep learning approach to survival analysis with competing risks.\" AAAI, 2018.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important clarifications should be given about the task and the model ",
            "review": "The paper \"Neural Distribution Learning for generalized time-to-event prediction\" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. \n \nFirst of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. \n\nAlso, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like \"Modeling the intensity function of point process via recurrent neural networks\" should be considered in the experiments, though they do not explicitely model censoring but  with slight adapations should be able to work well of experimental data. \n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}