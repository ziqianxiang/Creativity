{
    "Decision": "",
    "Reviews": [
        {
            "title": "Paper needs to be tightened with regard to the exact problem we're trying to solve and how we measure success",
            "review": "- There are several things about the presentation I find confusing.  There are two different objective functions you're considering: the ones based on the 2-Wasserstein distance in equations (3), (4), and (6), and the VAE in equation (2).  So what are you referring to in the abstract when you say \"we find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning\".  It seems like you're using a different objective function, so what do you mean by \"without any modification\"?\n\n- On page 1 you state that \"we seek also to establish that training generative models with OT can be significantly more effective than the traditional VAE approach.\"  Yet you don't really say how you measure this effectiveness.  Very late in the paper, on p7, you suggest using the reconstruction loss from the VAE objective to compare the trained models.  Is this the answer?  If so, I think you need to move it way up in the paper and give some discussion on why this makes sense as an evaluation metric.\n\n- At least a couple times you refer to wanting to get a \"weaker topology\", which I know is an idea developed in Bousquet et al's 2017 paper.  But you should include at least some explanation of what you mean by this and its implications -- \"weak topology\" seems like a pretty fancy term for the mathematical level of this paper.  In first paragraph of section 2.2, what is the \"induced topology\" of maximum likelihood?\n\n- To further elucidate the crux of the issue in Section 2.1, it might be helpful to explain why the EM algorithm, which is just an ELBO maximizer, for basic Gaussian mixture models doesn't typically have issues with a collapsing mixing distribution.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "ok work, would like to see fairer evaluation.",
            "review": "\nThe authors show that using VAE framework using a generative model with a mixture-of-Gaussians latent variable structure does not work naively. They propose a wasserstein-based objective that is better amenable to optimization and showed good qualitative/quantitative results on mnist dataset. I like the presentation of the work, especially answering some of my obvious questions like what happens if the VAE is initialized with wasserstein objective. \n\nI'm on the fence about acceptance due to the following 2 questions\n\n(a) do we really need to impose a structured distribution, i.e. gaussian latent variable structure with K centers. Would'nt using K times as many parameters with a single gaussian distribution and a neural network 'g' (using notation in Kingma & Welling) not be expressive enough to capture the data distribution ? \n\n(b) one of the authors main explanation on why the collapse of the K to 1 occurs is that \"discrete KL term is negligible\". Given that the MMD term in eq(4) is weighted by \\lambda, could the authors have not chosen to weight the KL term by lambda as well ? \n\nI would like the authors to present the results of (a) and (b) to give the baseline VAE an equal footing with the author's proposal. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting investigation of discrete latent variable in VAE objective, but limited discussion.",
            "review": "This paper describes an optimal transport approach for learning autoencoders with discrete and continuous latent variables.\nWasserstein autoencoder (WAE) is developed for better use of the discrete latent variable.\nExperiments using MNIST dataset show how it works.\n\nStrong point of the paper: \nThe empirical investigation to the reason why discrete latent variable is discarded in the VAE objective is interesting.\n\nWeak points:\n* The evaluation is limited to only MNIST datasets.\n* What encourages the use of latent variable is unclear (see below).\n\nThe reviewer has several questions and comments.\n* The choice of \\lambda may depend on the dimensionality of x. I am curious how this affects when applied to other datasets.\n* WAE is different from VAE in three ways: \n(1) l2 reconstruction error written as \\|x - y\\|_2^2, \n(2) MMD with IMQ kernels instead of KL divergence between variational distribution q and prior p, and \n(3) latent variable k being marginalized out.\nThe paper should elaborate more on which factor heps the use of discrete latent variable.\n* Figure 5 (c) is hard to read. Different marker shape (rather than its size) should improve the clarity to distinguish the samples from P and Q.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}