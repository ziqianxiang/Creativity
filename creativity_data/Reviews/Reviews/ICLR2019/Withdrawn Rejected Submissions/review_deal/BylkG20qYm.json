{
    "Decision": {
        "metareview": "This paper present a framework for creating meaning-preserving adversarial examples. It then proposes two attacks within this framework: one based on k-NN in the word embedding space, and another one based on character swapping. \n\nOverall, the goal of constructing such meaning-preserving attacks is very interesting. However, it is unclear how successful the proposed approach really is in the context of this goal. \n\nAdditionally, it is not clear how much novelty there is compared to already existing methods that have a very similar aim.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Important goal but the evaluation and relationship to the previous work needs improvement"
    },
    "Reviews": [
        {
            "title": "Interesting framework but lack of novelty and unclear evaluation of attack",
            "review": "The authors present a framework for creating meaning-preserving adversarial examples, and give two methods for such attacks. One is based on k-nn in the word embedding space, and another is based on character swapping. The authors further study a series of automatic metrics for determining whether semantic meaning in the input space has changed, and find that the chrF method produces scores most correlated with human judgement of semantic meaning. The authors finally give an evaluation of the two methods.\n\n\nPositive:\n- The authors give a framework with the explicit goal of preserving meaning in attacks.\n\nNegative:\n- Unclear novelty: previous work also gives the goal of preserving input meaning in attacks, even if the attacks themselves do not preserve meaning effectively (ie Zhao et al) \n- Unclear attack effectiveness: The chrF scores for CharSwap and kNN methods have higher chrF scores than the \"unconstrained\" method, but it is unclear what this means in context. Similarly, the RDchrF scores show that the average output changes in meaning by some amount, but the authors do not show in context what this really means in terms of meaning. \n\nDetails of negatives:\nUnclear attack effectiveness: \n- Using chrF score as a proxy for human judgement is unmotivated. There is little analysis of the distribution of chrF scores compared to human judgement - the only analysis given is that a) there is a .586 correlation on French and .497 correlation on English, and b) that :\"90% of French sentence pairs to which humans gave a score of 4 or 5 in semantic similarity have a chrF > 78\". It would be good to plot the distribution of chrF score vs human judgement, so that the reader is able to tell what the chrF scores really mean in context here - a correlation score of approximately .5 is difficult to interpret.\n- The chrF/RDchrF scores in the source and target spaces (respectively) as they relate to \"meaning-preservingness\" suffer from uninterpretability as a reader, both because of the point above and also because there are few examples of adversarial examples with their chrF/RDchrF scores given (only two).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough novelty for acceptance",
            "review": "The paper is about meaning-preserving adversarial perturbations in the context of Seq2Seq models. The paper proposes two ways of achieving that: (a) kNN - substituting word with nearest neighbors from the word embedding space, and (b) character swapping. It's debatable if character swapping is really meaning preserving since a lot of typos can really change the word. Similarly a case can be made about kNNs as well. But even if these are the best approximations we have, I have some major issues about the novelty of the work. Firstly, while the authors are trying to pitch the work in a new mold, there's major overlap with Belinkov and Bisk, 2018. The use of character swapping as an adversarial perturbation/noise and the subsequent benefits of training with adversarial noise have already been shown in Belinkov and Bisk, 2018. Secondly, the models tested are operating at word-level whereas most of the state-of-the-art systems nowadays are all using subword-level vocabularies. The character swap method presented would need to be adapted and some of the takeaways from results are hence less relevant for the current SOTA models. Coming to positives, the two real contributions for me are: (a) the result that chrF correlates better with human judgement, and (b) the measurement of adversarial perturbation's success measured via a sum that includes relative decrease in target score and the similarity of source sentence with the perturbed version. However, these are minor contributions and not enough to cover up the major flaws that I discussed above. \n\nSome other minor issues:\n(a) Table 1: The first example has the CharSwap row missing the word \"faire\".\n(b) Section 3.1.1: \"d\" is not defined when discussing time complexity. \n(c) No separate section 3.1.2 required as it can be merged with 3.1.1 and would be more easy to understand without confusing the readers that there's some context change.\n(d) Table 6 entries are not clearly defined. How is robustness measured?\n\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, but significant methodological and experimental problems.",
            "review": "Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. Given source and target metrics for measuring similarity, an attack is deemed successful if the source difference is smaller than the relative decrease in target similarity to the reference. A first experiment measures correlation with human judgements of similarity between original and perturbed sentences, and concludes that chrF is better than BLEU and METEOR for this purpose. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. In comparisons on three language pairs from IWSLT,  the constrained attacks are found to preserve meaning and yield more successful attacks according to the current framework. The Transformer architecture was also found to deal less well with attacks under the 10-closest embedding constraint. Finally, adversarial training with the character-swap constraint confers some robustness to this attack, without degrading performance on normal text.\n\nI think it is a good idea to formalize a method for carrying out and assessing adversarial attacks, but the framework proposed here seems too narrow, as it excludes adversarial inputs that are sensible but not a close perturbation of an existing source/reference pair, or ones that contain varying amounts of noise. It is more difficult to measure output quality for such attacks, but that doesn’t seem like a good reason for excluding them from what is intended to be a general framework. Note also that “more difficult” doesn’t mean impossible, since good attacks can produce severely degraded output that is relatively easy to detect.\n\nI found some of the methodology questionable. Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. Unnecessary because knowing the class of perturbation already gives you a lot of information about semantic distance. Unlikely to succeed because automatic metrics are too coarse to reliably distinguish among different perturbations. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. The experiments that support the viability of automatic metrics in 4.2 do so by measuring correlation with human judgment when the number of perturbed tokens varies from 1 to 3. I think the good correlation is likely due to the metrics being able to detect that, eg, changing 3 tokens makes things worse than changing only one. To be convincing, the experiments would have to be repeated with number of perturbations fixed at 3, to match the setting in the remaining experiments. \n\nApart from the interesting observation about the Transformer’s performance on embedding-neighbor attacks mentioned above, it is difficult to know what conclusions to draw from the experiments. In 4.3 it seems obvious a priori that perturbations intended to be relatively meaning preserving would indeed preserve meaning better than unconstrained ones. Similarly, it is not surprising that character swaps that by design produce an OOV token will cause more damage than choosing a near neighbor in embedding space. In 5.3, training with OOVs (resulting from character swaps) is of course not likely to hurt performance on test sets containing few OOVs, and, as is known from previous work, it will improve robustness to the same kind of noise. A final comment about the experiments is that word-based systems are not state of the art, and it isn’t clear how much we could expect any conclusions to carry over to sub-word models.\n\nTo conclude, although this is an interesting initiative, both the framework and the methodology need to be tightened up.\n\nDetails:\n\nEnd of 2.1: this would be easier to interpret if you had previously specified the allowed range for s_src.\n\n3.2 For kNN, being semantically related doesn’t imply that the relationship is synonymy, as would be required for meaning preservation. It also doesn’t imply that the substitution will be grammatical, which could jeopardize meaning preservation even if the words are synonyms.\n\nCharSwap seems odd. If you’re just going to replace a work with an OOV symbol in any case, why go to the trouble of swapping characters? No matter what actual semantic shift is caused by the swap, the model will always see exactly the same representation.\n\n4.1 “Following previous work on adversarial examples for seq2seq models (Belinkov & Bisk, 2018; Ebrahimi et al., 2018a)” - this is misleading: Ebrahimi et al only work with classification, and don’t use IWLST.\n\n4.1 Should mention the size of the training sets in this section.\n\nTable 1, first sentence, CharSwap example omits “faire”.\n\n4.3, “Adding Constraints Helps Preserve…” last sentence: but here you need to reason in the opposite direction.\n\n5.2 It would be good to also give absolute scores for table 6, so we can judge how much the systems actually benefited, and whether these gains were statistically significant.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An inspiring study on adversarial attacks for natural language",
            "review": "The authors provide a natural definition of adversarial examples for natural language transduction (meaning-preserving on source side while meaning-destroying on target side) and a human judgment task to measure it. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. Finally, they show that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting.\n\nOverall this is a strong paper. It is well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. While the studied attack methods are fairly primitive, the empirical results are still interesting.\n\nComments\n---------------\nI wish the authors would include experiments with CharSwap where OOV is not forced as I'm not sure the assumption that OOV is more meaning-destroying in the target side is necessarily true (one could also argue that since the models are already trained with OOV words, they may be more robust to OOV words than in-vocabulary words in the wrong context).\n\nIt would be nice to add correlation for each type of constraint as well to Table 2. The result would be even stronger if the experiment was replicated in the opposite direction or for another language pair as well.\n\nI don't understand why the adversarial output in the second example in table 4 has a RDchrF of zero (the word July is completely dropped).\n\nFrom Table 6 it looks like random sampling is actually slightly better than adversarial training in terms of robustness to CharSwap attacks in the Transformer model. Moreover, the benefit of adversarial rather than random sampling is quite small in the LSTM model as well. This could be made more clear in the text.\n\nIt would be interesting to see how adversarial training with the CharSwap method fares against the unconstrained and kNN attacks in table 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}