{
    "Decision": {
        "metareview": " The paper proposes an augmented adversarial reconstruction loss for training a stochastic encoder-decoder architecture. It corresponds to a discriminator loss distinguishing between a pair of a sample from the data distribution and its augmentation and pair containing the sample and its reconstruction. The introduction of the augmentation function is an interesting idea, intensively tested in a set of experiments, but, as two of the reviewers pointed out, the paper could be improved by deeper investigation of the augmentation function and the way of choosing it, which would increase significance of the contribution. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Intersting idea that needs a bit more investigation"
    },
    "Reviews": [
        {
            "title": "Adding and encoder for the GANs is studied. But the distinctions from existing models are not obvious.",
            "review": "The paper propose a adversary method to train a bidirectional GAN with both an encoder and decoder. Comparing to the existing works, the main contribution is the introducing of an augmented reconstruction loss by training a discriminator to distinguish the augmentation data from the reconstructed data. Experimental results are demonstrated to show the generating and reconstruction performance.\n\nThe problem studied in this paper is very important, and has drawn a lot of researchers' attentions in recent years.  However, the novelties of this paper is very limited. The techniques used to train a bidirectional GAN are very standard. The only new stuff may be is the proposed reconstruction loss defined on augmented samples and reconstructed ones. But this is also not a big contribution, seems just using a slightly different way to guarantee reconstruction. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Adversarial reconstruction loss is an interesting idea, but the paper need more polishing",
            "review": "==============Updated=====================\nThe authors addressed some of my concern, and I appreciated that they added more experiments to support their argument.\nAlthough I still have the some consideration as R3, I will raise the rating to 6.\n\n===========================================\n\nThis paper is easy to follow. Here are some questions:\n\n1. The argument about ALI and ALICE in the second paragraph of the introduction, “… by introducing a reconstruction loss in the form of a discriminator which classifies pairs (x, x) and (x, G(E(x)))”, however, in ALI and ALICE, they use one discriminator to classify pairs (z, x) and (z, G(z)). Therefore, “… the discriminator tends to detect the fake pair (x, G(E(x))) just by checking the identity of x and G(E(x)) which leads to vanishing gradients” is problematic. Therefore, the motivation in the introduction may be some modification.\n\n2. The authors failed to compare their model with SVAE [1] and MINE [2], which are improved versions of ALICE. And we also have other ways to match the distribution such as Triple-GAN [3] and Triangle-GAN [4], I think the authors need to run some comparison experiments.\n\n3. The authors should discuss more about the augment mapping a(x), i.e., how to choose a(x). I think this is quite important for this paper. At least some empirical results and analysis, for example, how inception score / FID score changes when using different choices of a(x).\n\n4. This paper claims that the proposed method can make the training more robust, but there is no such experiment results to support the argument.\n\n[1] chen et al. Symmetric variational autoencoder and connections to adversarial learning, AISTATS 2018.\n[2] Belghazi et al, Mutual Information Neural Estimation, ICML 2018.\n[3] Li et al. Triple Generative Adversarial Nets, NIPS 2017.\n[4] Gan et al. Triangle generative adversarial networks, NIPS 2017.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Quite a lot of experiments, but the choice of r(y|x) is not well justified, and some theoretical issues",
            "review": "Thank you for an interesting read.\n\nThe paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial \"reconstruction\" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. \n\nExperiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed \"reconstruction\" loss. Improvements has been shown on reconstructing input images with significant numbers.\n\nOverall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.\n\nThere is one theoretical issue for the defined \"reconstruction\" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). \n\nThis is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].\n\nI think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). \n\n1. Indeed matching p_{\\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \\approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.\n\n2. The authors mentioned that picking r(y|x) = \\delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section \"...we used a combination of reflecting 10% pad and the random crop to the same image size...\". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. \n\nIn summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).\n\n[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.\n[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}