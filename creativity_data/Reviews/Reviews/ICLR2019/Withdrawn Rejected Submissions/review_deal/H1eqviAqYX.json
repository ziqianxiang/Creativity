{
    "Decision": {
        "metareview": "This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Important question, but unconvincing treatment here"
    },
    "Reviews": [
        {
            "title": "Interesting problem but extremely bad execution",
            "review": "The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments.  First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. \n\nIn section 3, the authors then introduce the \"max-marginal regularization\" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word \"marginal\" instead of \"margin\" seems quite wrong to say the least.  Secondly, the stated definition seems to be wrong. In the definition the range of values for \\gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \\gamma not equals 0) below and show that both have issues:\n(a) \\gamma > 0: This seems to imply that when the log-likelihood of ground-truth is already \\gamma better than the log-likelihood of the random negative, the loss comes to life. Strange!\n(b) \\gamma < 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. \nI'm assuming the authors swapped y with y^{-} in the \"regularization\" part.\nAnyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well.  \n\nComing to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two \"Where is your location?\" outputs.  The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. \n\nA lot of typos/wrong phrasing/wrong claims and here are some of them:\n(a) Page 1, \"lead to the misrecognition of those common replies as grammatically corrected patterns\"? - No idea what the authors meant.\n(b) Page 1, \"unconsciously preferred\" - I would avoid attaching consciousness before AGI strikes us.\n(c) Page 1, \"Above characters\" -> \"Above characteristics\"\n(d) Page 1, \"most historical\" -> \"most previous\"\n(e) Page 2, \"rest replies\" -> \"rest of the replies\"\n(f) Page 3, \"variational upper bound\" -> Not sure what's variational about the bound\n(g) \"Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance\"? - No idea what the authors meant.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "main contribution: improving neural response by de-emphasizing universal responses by modifying loss/training",
            "review": "The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. The authors show that the approach yields better results in the dataset considered using various measures and human evaluation.\n\nImprovement Points\n- the explanation for low ROGUE measure due to the method favoring non-repetitive words sounds like it can be supported using numerical statistics, than hand-waiving argument\n- for the timing, how much time was taken to tune the additional parameters (how the # of negative responses sampled for each positive response was chosen as four via uniform sampling)\n- some description about\na) how many users are there, what type of conversation/active users/topics etc.\nb) what time frame was used during data collection (this may have implications for lemma asserting zipf)\n- it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies)",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technically unsound work",
            "review": "This paper presents a framework for understanding why seq2seq neural response generators prefer \"universal\"/generic replies. To do so the paper breaks down the response generation probability into the probability of selecting the set of tokens (reflecting the topic of the output) and then selecting an ordering of the tokens (reflecting the syntax of the output.)\n\nThe results presented in this paper are not technically sound. E.g the derivation in Eq(2) derive a meaningless bound. Here is why:\n1. The first equality assumes that the words in a set are independent which is not true.\n2. In the second equality, the authors incorrectly replace the summation of word probability in each sentence with the summation of word probabilities over all unique words (the set) overall sentences. This is simply not true if there are common words shared between sentences.\n3. Perhaps the biggest issue is the incorrect application of Jensen's lemma. JL is often used as \nlog(\\sum_i a_i x_i) > \\sum_i a_i log x_i if \\sum_i a_i = 1. Instead what authors have used is \nlog (\\sum_i x_i) > \\sum_i log(x_i), which is not always true, and is trivially true for all x_i < 1. In fact, this bound is not even tight (unlike Jensen's lemma) and the *worst* part is that the LHS increases if we add more x_i (<1) and the RHS decreases. This means this bound is far from being meaningful and as such should be summarily ignored.\n\nSimilarly, in section 2.3, the technical content is quite poor. Why is this true -- \"the amount of possible queries M of y... 1 << M \\propto N\"? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in \"non-universal\" replies? I am not going more into the details in this section because I think the problems with section 2.2 are themselves dealbreakers.\n\nOverall, given the problems this work is not technically sound to be accepted.",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}