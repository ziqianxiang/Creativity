{
    "Decision": "",
    "Reviews": [
        {
            "title": "Simple solution, large improvements.",
            "review": "This authors propose a solution to the problem of over-smoothing in Graph conv networks that prevents knowledge propagation between distant yet related nodes. Specifically, they allow dense propagation between all related nodes, weighted by the mutual distance. Dense connections ensure influence of related nodes and distance weighting moderates this influence based on proximity of nodes. The scheme is used for knowledge graph based zero-shot learning. Like Wang et. al. 2018, their model learns to predict pre-trained weights of known classifiers and infer, using the knowledge graph, weights for unseen classes.  The simple fix to graph propagation seems to produce large gains over zero-shot baselines (Table 1 & 2). I think the solution is intuitive and results are really good. Therefor, I am in favor of accepting. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not self-contained paper.",
            "review": "This paper proposes a novel graph convolutional neural network to tackle the problem of zero-shot classification. To learn the classifier of unseen classes, the relational structure between classes are used as the input of graph convolutional network (GCN). And then the model imposes the output of GCN to be weights of a classifier learned from a supervised classification model such as ResNet, in the hope of having a good classifier for unseen classes while fitting the known classes.\n\nOverall, the paper is not self-contained. It is almost impossible to understand the overall architecture without prerequisites (GCNZ paper in this context). The description of each module is relatively clear, however, it fails to give an overall connection between different modules used in the proposed paper. For example, it has never been described what would be the input feature of the GCN or DGP modules (such as the number of nodes should be the same as the number of all classes). There is no explanation why the output of GCN should resemble the final weight of the ResNet, and what this means to solve the zero-shot learning problem, and eventually how the knowledge graph would help to solve the problem of zero-shot learning. I personally found the answers to these important questions from the description in GCNZ paper. Therefore, although the model improves the performance of the previous work, I don't think it is publishable in its current condition.\n\n- Missed some of recent work. Recent papers [1,2] incorporates attention mechanism to deal with the potential dilution in GCN. Although DGP uses different weighting based on the distance from a node, the attention mechanism is clearly more expressive than the distant-wise weighting used in this paper.\n- Section 3.5 should be in the experiments section since it describes the details of experiments.\n\n[1] W.L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. arXiv preprint,\narXiv:1603.04467, 2017.\n[2] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903, 1(2).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The motivation is not well justified",
            "review": "This paper uses a collection of methods (with minor improvements) to obtain good prediction performance on ImageNet-subset data. I sincerely recommend authors improving the paper's quality by offering more analysis and insights of their method.\n\n1. Could authors add more experiments on explaining their motivations?\n- The big motivation to their work is \"feature representations will become more similar as depth increases leading to easier classification\" (in the introduction). However, this is only supported with results in a small table in Appendix. C.\n- It is better to show results directly based on GCNZ, and then results on SGCN. \n- It seems that the performance in Table 6 and 1 (hop 2) are not consistent. Are there any reasons for this?\n- Except for accuracy, could author design some other measurement to evaluate the smoothness of the embedding in deeper layers? (perhaps t-SNE as Wang etal 2018).\n- All the above will make the paper sound more principle and better motivated. Currently, the motivation is not well justified.\n\n2. \"Rethinking\" indicates in-depth analysis of existing works, based on Q1, I suggest the author changes their title as well.\n\n3. The connections to ancestors and descendants look tricky. Are there any insight reasons for connecting and training in this way?\n- Specifically, the connectivity patterns in the graph is very complex. The authors have also said there can be DAG in the graph, so, why should we connect in this way?\n\n4. Can the proposed method be used on other kinds of data sets except those from the image domain?\n\n5. The motivation in this paper is inconsistent with experiments in Wang et al 2018. Wang has shown in Section:\"how important is the depth of GCN\" & Table 4 that the model performance increasing with the depth of layers. So, could the authors repeat the same experiments on NELL & NEIL? This will make the motivation more convincing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}