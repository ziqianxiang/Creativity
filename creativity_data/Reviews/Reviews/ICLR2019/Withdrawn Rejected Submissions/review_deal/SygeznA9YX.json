{
    "Decision": "",
    "Reviews": [
        {
            "title": "A large visual QA on plots dataset that should be useful",
            "review": "\n\n\nThe authors propose a large dataset for the task of visual\nunderstanding of plots. Compared to the two previous datasets recently\npublished (FigureQA and DVQA), their proposed dataset (DIP) involves a\nsignificantly larger vocabulary, and realistic words, and the\nquestions and answers (as well as plots) can be more diverse and\ncomplex.\n\nThe novelty of the work lies (see below) in some aspects of creating\nthe data, though new ideas are somewhat marginal, considering the work on previous\ndatasets. The paper is clearly written.  I think the dataset will be very useful.\n\n\n-- Because all questions are generated via templates in particular\n   (and plots also follow a certain generation process, albeit noisy),\n   I am concerned that it would ultimately be easy to invert the\n   process to crack the dataset. Any intuitions why this is unlikely?\n   For instance, eventually Jeopardy was tackled (better than human\n   champion) with a large engineered system..\n\n-- Please provide example mistakes of techniques you tried, eg on the \ntable answering problem. \n\n-- Section 5: why not report performance on the 'test-hard' split (in\n   supplementary), since it's mentioned at all?\n\n-- What is the human performance on your data? Some rough accuracy\nrates will be useful, preferably on each question type.  This would\nhelp understand the room for progress (This is done for FigureQA data).\n\n-- minor: Why not use more sophisticated techniques (eg neural-based\n   relation learning, the work is cited) for the last stage table\n   reasoning?\n\n\nIdeas, novelty: unlike previous work, the plots are generated from\nreal tables (world bank, open government data, ...) and Amazon Turk\nworkers are asked to come up with relatively complex questions, for a\nsmall subset of the generated plots.  Templates for these questions\nare then generated (with some extensive manual work to make sure the\nquestions feel natural), and many plots (270k) and questions (~10\nmillion) are then generated using plot generation and the templates.\n\nThe performances reported are via breaking the problem into stages,\nusing several techniques in a pipeline, from visual elements detection\nto OCR, to table reconstruction, finally table QA. Standard\ntechniques (to best of my knowledge) were used for each stage to\nreport a baseline performance.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " I think this paper is an OK paper, although it can be further improved if more experimental results are provided.",
            "review": "In this paper, the authors proposed a pipeline to solve Data Interpretation over Plots (DIP) problem, involving learning from datasets containing triplets of the form {plot, question, answer}. This paper introduces a new dataset for the DIP problem. The authors develop a pipeline that break down the task into two step, to learn from this dataset. \n\nIn general, this paper is well-written. I am able to understand the DIP problem, the dataset and the proposed pipeline. \n\nThe process of constructing the DIP dataset looks reasonable. The strategy adopted in the proposed pipeline, which break down the task into two step, looks interesting. However, I do not consider this strategy completely novel, since each component of the proposed pipeline is based on existing method.\n\nMy major concerns about this paper might be related to the experiments. In the paper, the authors claim that the dataset is very challenging and beyond existing end-to-end models. I believe this claim might be true; however, I think the authors should still support this claim with experiments, even if end-to-end models give very poor results. I would also suggest the authors test the proposed method on easier dataset, such that it can be fairly compared with existing methods. \n\nIn summary, I think this paper is an OK paper, although it can be further improved if more experimental results are provided.\n\nI found a typo in the paper. In the abstract \"real word\" should be \"real world\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Crowdsourcing idea is interesting but synthetic data is not",
            "review": "Review of \"Data Interpretation and Reasoning Over Scientific Plots\"\n\nPaper summary:\n\nThe paper is concerned with algorithms that can interpret data shown\nin scientific plots. The algorithm is given a plot and a question, and\nis required to predict an answer. The main contribution of this paper\nis a new (synthetic) data set which is much larger than previous data\nsets. It is claimed that the proposed DIP data set is \"more realistic\"\nthan previous data sets. (in terms of variable names, values,\nquestions) The paper also proposes a baseline model/pipeline for these\ndata. \n\nThe paper proposes the (new?) idea of using crowdsourced workers to\ncreate natural language questions. 3500 plots were each shown to 5\ndifferent works, creating a total of 17,500 questions. After that, the\nauthors manually clustered the questions into 5 classes and 74\ntemplates, from which they generated the DIP data set. It contains\n273,821 plots (presumably with\n\nThe model (pipeline) consists of four sub-tasks:\n- visual element detection, which consists of predicting bounding boxes for axis labels, tick marks, legends, etc. A Single Shot Multibox Detector model is trained for this task.\n- optical character recognition is performed using a pre-trained OCR model.\n- Semi-structured information extraction is performed using is done using a \"use the closest\" heuristic.\n- table question answering using a previous method.\n\nDetailed comments:\n\nThe main strong point of the paper is the manually collected\ncrowdsource data. The paper is also quite clear and easy to read and\nunderstand.\n\nHowever there are too many weak points to merit publication.\n\nIt is clear that the model is NOT a significant contribution to the\nmachine learning literature, as it simply uses previous work.\n\nIt is not clear that the data set is a significant contribution to the machine\nlearning literature. Machine learning has made progress in recent\nyears because of the emphasis on analyzing real (not synthetic)\nbenchmark data sets, in which the pattern is unknown. If this kind of\nsynthetic data is interesting in another field of research, maybe this\npaper could be resubmitted elsewhere in the current form?\n\nOtherwise, publishing the full data that were actually collected from\nreal people (not synthetically generated) is a productive research\ndirection for the machine learning community. Would that be possible?\n\nIf you really want to train on synthetic data, then the interesting\nquestion is, can you train on your synthetic data and then get good\nresults on the real data (test-hard) from crowdsourced workers? This\nquestion is not answered in the paper. \"However, we do not use the\ntest-hard split for our experiments.\" why not?\n\nWhy does the test-hard data set only have 1000 observations? Why not\nuse the entire set of 17,500 that you gathered?\n\nA weak point is that the paper offers no empirical comparison with\nprevious models, and simply claims that they are not good enough. \"it\nis infeasible to use any of the existing VQA models on our dataset as\nthey largely treat VQA as a multi-class classification problem where\nthe task is to select the right answer from a fixed vocabulary\"\n\nFigures are not publication quality. In particular the text in figures\n1c, 2 is too small to read.\n\nTables are not publication quality. Table 1 has a confusing comma on\nthe first(train) line -- is there a missing number? Tables 2-5 need\nStandard deviations or confidence intervals. You should use K-fold\ncross validation instead of a fixed train/validation/test split. In\npractice you can do that by publishing a file with a suggested Fold ID\n(say from 1 to 5) for each observation.\n\n\"The data contains positive integers, floating point values,\npercentages and values on a linear scale.\" -- what is a value on a\nlinear scale?\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}