{
    "Decision": {
        "metareview": "This paper introduces a \"scratchpad\" extension to seq2seq models whereby the encoder outputs, typically \"read-only\" during decoding, are editable by the decoder. In practice, this bears quite a lot of similarity—if not in the general concept, then in the the implementation—to a variety of models proposed in the NLP community (see reviews for details). As the technical novelty of the paper is quite limited, and there are issues with the clarity both in the technical contribution and in presenting what exactly is the main contribution of the paper, I must concur with the reviewers and recommend rejection.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not enough novelty relative to neural coverage models"
    },
    "Reviews": [
        {
            "title": "Interesting idea but not novel enough",
            "review": "Overall:\nThis paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form. The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a “scratchpad” memory to keep track of what has been generated so far and to guide future generation. \n\nQuality and Clarity:\n-- The paper is well-written and easy to read. \n-- Consider using a standard fonts for the equations. \n\n\nOriginality :\nThe idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful. \nCompared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write “decoding information” to the “encoder” output. The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively “writes” a message (“coverage”) to the encoder's hidden states.\nIn the original coverage paper (Tu et.al, 2016), they also proposed a “neural network based coverage model” where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder. However, the modification is slightly marginal but seems quite effective. It is better to explain the major difference and the motivation of updating the hidden states.\n\n-------------------\nComments:\n-- In Equation (13), is there an activation function between W1 and W2?\n-- Based on Table 1, why did not evaluate the proposed model with beam-search?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "\n\nThis paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework. The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.\n\nMajor points:\n\nOverall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty. I don’t recommend to accept this paper, at least in the current format.\n\nThe paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is “possible to generate a large high quality (SPARQL query, local form) dataset”. For the second contribution, there isn’t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications. I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.\n\nFor the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?\n\nIn general I find Section 3 pretty difficult to follow. What does “keeping notes” mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage. I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode. \\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.\n\nMinor points:\n- tau Yih et al, 2016 --> Yih et al, 2016\n- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.\n- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Generating questions is an interesting task, but it is a kind of natural language generation task and the paper does not consider and they have proposed very similar ideas already",
            "review": "The paper studies the problem of question generation from sparql queries. The motivation is to generate more training data for knowledge base question answering systems to be trained on. However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:\n- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745\n- Globally Coherent Text Generation with Neural Checklist Models\nChloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032\nThus the main novelty claim of the paper needs to be hedged appropriately. Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.\n\nSome other points:\n- How is the linearization of the inout done? It  typically matters\n- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.\n- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013\n- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used. Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.\n- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. \"what job did jefferson have\" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone. While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express. What were the guidelines used?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}