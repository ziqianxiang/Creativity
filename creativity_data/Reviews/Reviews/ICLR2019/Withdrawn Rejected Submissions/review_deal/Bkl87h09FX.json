{
    "Decision": {
        "title": "perhaps not strong novelty but interesting insights based on extensive experiments on ELMO",
        "metareview": "This paper presents an extensive empirical study to sentence-level pre-training. The paper compares pre-trained language models to other potential alternative pre-training options, and concludes that while pre-trained language models are generally stronger than other alternatives, the robustness and generality of the currently available method is less than ideal, at least with respect to ELMO-based pretraining. \n\nPros:\nThe paper presents an extensive empirical study that offers new insights on pre-trained language models with respect to a variety of sentence-level tasks.   \n\nCons:\nThe primarily contributions of this paper is empirical and technical novelty is relatively weak. Also, the insights are based just on ELMO, which may have a relatively weak empirical impact. The reviews were generally positive but marginally positive, which reflect that insights are interesting but not overwhelmingly interesting. None of these is a deal-breaker per say, but the paper does not provide sufficiently strong novelty, whether based on insights or otherwise, relative to other papers being considered for acceptance.\n\nVerdict:\nLeaning toward reject due to relatively weak novelty and empirical impact.\n\nAdditional note on the final decision: \nThe insights provided by the paper are valuable, thus the paper was originally recommended for an accept. However, during the calibration process across all areas, it became evident that we cannot accept all valuable papers, each presenting different types of hard work and novel contributions. Consequently, some papers with mostly positive (but marginally positive) reviews could not be included in the final cut, despite their unique values, hard work, and novel contributions. ",
        "recommendation": "Reject",
        "confidence": "3: The area chair is somewhat confident"
    },
    "Reviews": [
        {
            "title": "Many experiments on a fast-moving field, without clear conclusion",
            "review": "The work presented in this paper relates to the impact of the dataset on the performance of contextual embedding (namely ELMO in this paper) on many downstream tasks, including GLUE tasks, but also alternative NLP tasks.\n\nThe work is focused on experiments, and draws several conclusions that are interesting, mostly around the amount of gain one can expect and the fact that the choice of the dataset is task-dependent. \n\nOne of the issue is that the authors if seems to believe that ELMO is the best contextual language model. The field is moving so quickly that the experiments might become invalid pretty soon (e.g. see BERT model referenced below).\n\nFinally, the analysis is mostly descriptive and there is few insight by the author about what should be the future work, apart from \"we need a better understanding\".\n\n\nMinor details:\n\nPage 1: \"can yield very strong performance on NLP tasks\" is a very busy way to express the fact that Sentence Encoders work well in practice. \n\nThe field evolves quickly and ELMO has now a competitive models called BERT (arXiv.org > cs > arXiv:1810.04805). I understand that the results of the current papers would hove to be re-run on all these tasks, but I'm afraid the current paper will have a limited impact if it does not use the most effective method at the date of publication...",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents an extremely comprehensive comparison of sentence representation methods.",
            "review": "Only a handful of NLP tasks have an ample amount of labeled data to get state-of-the-art results without using any form of transfer learning. Training sentence representation in an unsupervised manner is hence crucial for real-world NLP applications.\nContextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods.\n\nThis paper's biggest strength is the experimental setting.  The authors cover a lot of ground in comparing a lot of the recent work, both qualitatively and quantitatively -- there are a lot of experiments.\nI do understand the computational limitations of the authors (as they mention on HYPERPARAMETER TUINING) and I do agree with their statement “ The choice not to tune limits our ability to diagnose the causes of poor performance when it occurs”.\nExtensive hyper-parameter tuning can make a substantial different when dealing with NN models, maybe the authors should have considered dropping some of the tasks (the article has more than enough IMHO) and focus on a smaller sub set of tasks with proper hyper-parameter tuning.\nTable 2 is very interesting, the results suggesting that we are indeed very far from fully robust sentence representation method. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Valuable systematic study of pre-training tasks' influence on downstream task performance",
            "review": "This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks.\n\nThis paper represents an impressive amount of experimentation. The study and the experimental results will be useful and interesting to the community. The result that some tasks' performance are negatively correlated with each other is surprising. The paper is clearly written. \n\nOne clarification question I have is about what the \"Single-task\" pre-training means. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. I'm confused by what this means, and how this is different from just training on that task. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}