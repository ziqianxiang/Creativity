{
    "Decision": {
        "metareview": "This paper endeavors to combine genetic evolutionary algorithms with subsampling techniques. As noted by reviewers, this is an interesting topic and the paper is intriguing, but more work is required to make it convincing (fairer baselines, more detailed / clearer presentation, ablation studies to justify the claims made int he paper). Authors are encouraged to strengthen the paper by following reviewers' suggestions.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting topic but requires more work"
    },
    "Reviews": [
        {
            "title": "Evolutionary part is not clear",
            "review": "The paper introduces Sample-Ensemble Genetic Evolutionary Network, which adopts a genetic-evolutionary learning strategy to build a group of unit models. Explanation on the evolutionary network part is not enough. For example, there is no clear explanation on how chromosomes are defined. Also, detailed analysis on computational aspect is needed.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Using Subsampling + Genetic Algorithm for Network Embedding",
            "review": "This paper proposes to subsample a large network into sub-networks, learn a network model (autoencoder) from each subgraph, perform crossover and mutation operations over the network parameters of different model pairs, and combine the latent representations following the ensemble idea.\n\nThe paper is clearly presented. Originality and significance is limited. Putting the three knowns components - subsampling, generation algorithm and ensembling together seems to be the main contribution of this paper. However, the ways of doing subsampling, performing the crossover and mutation operations and doing the ensembling are relatively straightforward ways of applying them. The fact that combining them to obtain better results is not a surprising result. And according to the experimental results, it is not clear how the gain in performance is resulted and to what extent each of the three components is contributing. For instance, I just guess the combination of subsampling + existing network embedding methods (LINE/DeepWalk/...) + ensembling may also give good results. Currently, the performance comparison is done with the original forms of LINE and DeepWalk. That makes the empirical results not very convincing to explain the key strengths of this work.\n\n+ve:\n`1. The paper is clearly presented.\n2. The design is reasonable one.\n3. A number of benchmark datasets are used for the evaluation.\n\n-ve:\n1. The originality and significance is limited.\n2. The performance comparison should be done with references to more competitive candidates as explained above.\n3. The nodes of different sub-networks are essentially projected to different embedding spaces. The validity and interpretation of performing the crossover operation on two different models (two different embedding spaces) will need more justifications.\n4. The proposed methodology is not an end-to-end. The ensembling being evaluated is just simple addition.\n5. The paper claims that \"The unit learning model, genetic algorithm and ensemble learning can all provide the theoretic foundation for SEGEN, which will lead to sound theoretic explanation of both the learning result and the SEGEN model itself\". Individually being sound does not imply that the way to combine them is sound. Currently, I cannot see the uniqueness of this particular combination.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting topic but several issues with the paper",
            "review": "This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., “evolutionary layer”. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.\nThe paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.\n-\tIn page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. \n-\tGiven the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.\n-\tBased on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a naïve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. \n-\tThere are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. \n*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. \n*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.\n*** Less Parameter Tuning. Again experiments could clarify this issue.\n*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. \n-\tTo calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 ‘Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis’). \n-\tOne of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 ‘Massive Multimodality, Deception, and Genetic Algorithms’). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}