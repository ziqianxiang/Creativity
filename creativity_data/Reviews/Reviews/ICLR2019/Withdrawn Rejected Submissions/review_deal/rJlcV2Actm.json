{
    "Decision": {
        "metareview": "The reviewers raised a number of concerns including low readability/ clarity of the presented work and methodology, insufficient and at times unconvincing experimental evaluation of the proposed, and lack of discussion on pros and cons of the presented. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns and reconfirmed that relatively large changes are still needed for the paper to be useful to the readers. Hence, although I believe this could be a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Class hierarchy and cache-based nearest neighbors for many class / few shot",
            "review": "This paper presents methods for (1) adding inductive bias to a classifier through coarse-to-fine prediction along a class hierarchy and (2) learning a memory-based KNN classifier through an intuitive procedure that keeps track of mislabeled instances during learning. Further, the paper motivates focused work on the many class / few shot classification scenario and creates new benchmark datasets from subsets of imagenet and omniglot that match this scenario. Experimental results show gains over popular competing methods on these benchmarks.\nOverall, I like the motivation that this paper provides for many class / few shot and find some of the methods proposed interesting. Yet there are issues with clarity of presentation that made it somewhat difficult to fully understand the exact procedures that were implemented. The model figure is useful, but could be refined to add additional clarity -- particularly in the case of the KNN learning procedure. \nI'm not entirely familiar with recent work in this sub-field, so it is difficult for me to judge the novelty of the proposed procedure. Is it really true that class-hierarchies have never been used to perform coarse-to-fine inference in past work? If so, this should be state clearly. If not, related work should be mentioned and compared against. Finally, while the procedures are intuitive -- the takeaway of this paper could be substantially improved if even simple theoretical analysis were provided. For example, in the limit of infinite data, does the memory-based KNN learning procedure actually produce the right classifier? \nMisc comments questions:\n-The paper says at least twice that coarse classification will be performed with an MLP, while fine classification will use a KNN -- yet, the model section also state that both coarse and fine use both MLP and KNN. It is unclear to me which model setup was used in experiments. \n-Can anything theoretical be shown about the class hierarchy based classification technique? Intuitively, it does add inductive bias through a manually defined taxonomy, but can something more precise be said about how it restricts the hypothesis space? This procedure is simple enough that I would be surprised if similar techniques had not be studied thoroughly in the statistical learning theory literature. \n-The procedure for updating the KNN memory is intuitive, but can anything more be said about it? In isolation, is the KNN learning procedure at least consistent -- i.e. in the limit of large data does it converge to the correct classifier? Maybe this is trivial to prove, but is worth including. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need to clarify some procedures",
            "review": "This paper try to formulate many-class-few-shot classification problem from 2 perspectives: supervised learning and meta-learning. Although solving this problem with class hierarchy is trivial,  combing MLP and KNN in these two ways seems interesting to me. I still have several questions:\n\n1) How the class hierarchy is got , manually set or automatically generate?  Whether the ideas still work if some coarse classes share same fine class?\n2) The so-called attention module is just classic KNN operations, please don't naming it attention just because the concept \"attention\" is hot.\n3) Why different \"attention\" operations are used for supervised learning and meta-learning?\n4) How to get the pre-trained models for supervised learning?\n5) What will happen if alternatively apply supervised learning and meta-learning?\n6) In Table 4, why MahiNet(Mem-2) w/o Attention and Hierarchy performs better than the one w/o attention?\n7) The authors just compare storing the average features and all features, I think results of different prototype number should be given, since one of their claim to apply KNN is to maintain a small memory.\n\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper which may need further enhancement",
            "review": "This study explores the class hierarchy to solve many-class few-short learning problem in both traditional supervised learning and meta-learning. The model integrates both the coarse-class and fine-class label as the supervision information to train the DNN, which aims to leverage coarse-class label to assist fine-class prediction. The core part in the DNN is memory-augmented attention model that includes at KNN classifier and Memory Update mechanism. The re-writable memory slots in KNN classifier aim to maintain multiple prototypes used to describe the data sub-distribution within a class, which is insured by designing the memory utility rate, cache and clustering component in Memory Update mechanism. This study presents a relatively complex system that combines the idea of matching networks and prototypical networks.\n\nOne of the contributions is that the study puts forward a concept of the many-class few-short learning problem in both supervised learning and meta-learning scenarios, and uses a dataset to describe this problem.\n\nUsing the memory-augmented mechanism to maintain multiple prototypes is a good idea. It may be more interesting if its effectiveness can be proved or justified theoretically. Furthermore, it is better to offer some discussion about the learned memory slots in the view of “diverse and representative feature”.\n\nThe experiment results in Table 4 and Table 5 compare the MahiNet with Prototypical Net on the mcfsImageNet and mcfsOmniglot dataset. It is better to compare MahiNet with other state-of-the-art works, such as the Relation Network whose performance is higher than Prototypical Net. In addition, if more  challenging datasets  can be further evaluated in the experiments,  the paper  might be more convincing.\n\nIn my opinion, the hierarchy information provides the guidance to fine-gained classification, which not only can be added to MahiNet but also the other models. Therefore, to prove its effectiveness, it is better to add hierarchy information to other models for comparison. In addition, regarding the results on the column of 50-5 and 50-10 in Table 4, when the number of class increase to 50, the results are just slightly higher than prototypical network. Considering that the memory update mechanism is of the high resource consumption and complexity, it is better to provide more details about clustering, and training and testing time.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}