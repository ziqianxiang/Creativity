{
    "Decision": {
        "metareview": "This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.\n\nThis paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2's feedback, but unfortunately due to the competitive nature of this year's ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.\n\nAs a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Very borderline"
    },
    "Reviews": [
        {
            "title": "Nice paper, well written and through evaluation",
            "review": "This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning. In comparison to the existing approach, this paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The results show a significant gain in performance as well as the capability of the model to generalize composition program to unseen data effectively. The qualitative analysis shows that the proposed model not only get the correct answer but also the correct behavior that leads to the answer.  ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Need improvement on the presentation",
            "review": "This paper proposes a variational neural module networks (V-NMN), which compared to neural module networks (NMNs), is formed in a probabilistic aspect. The authors compare the performance of V-NMN and NMN on SHAPES dataset.\n\nI find the technical part is hard to follow. To optimize the objective function, it involves many challenges. The authors described those challenges as well. It is not clear to me how those challenges are solved in section 2.1. I think that the presentation in section 2.1 needs to provide more details.\n\nIn the experiment, the authors only compare their work with NMNs without comparing it with other approaches for visual question answering. Besides accuracy, does V-NMN provide new applications that NMNs and other VQA models is not applicable because of the probabilistic formulation?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good piece of work",
            "review": "The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.\nIn the paper, both the problems of learning and inference are taken into account.\nTo answer queries, this system takes as input an image and a question, which is a set of word from a given vocabulary. Then the question is modeled by a plan (a series of operation that must be performed to answer the query).  Finally, the found answer with the plan are returned. To learn the parameters of the model, the examples are tuples composed by an image, a question, the answer, and the program.\nExperiments performed on the SHAPES dataset show good performance compared to neural model networks by Johnson et al.\n\nThe paper is well written and clear. I have not found any specific problems in the paper, the quality is high and the approach seems to me to be new and worth studying.\nThe discussion on related work seems to be good, as well as the discussion on the results of the tests conducted.\n\nOn page 5, in equation (3) it seems to me that something is missing in J. Moreover, In Algorithm 1, in lines 4 and 9, the B after the arrow should be written in italic.\n\nOverall, there are several typos that must be corrected. I suggest a double check of the English. For example:\n- page 3, \"as modeling *uncertaintly* should...\"\n- page 6, \"Given this goal, we *consrtuct* a latent *varible* ...\"\n- page 8, in paragraph \"Effect of optimizing the true ELBO\", the word \"that\" is repeated twice in the 3rd row\n- page 13, \"for the\" repeated twice in \"Moving average baseline\" paragraph. Also, in the last line of this paragraph, the sentence seems incomplete.\n\n\n\nPros\n- The results are convincing\n- The approach is clearly explained\n\nCons\n- English must be checked",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}