{
    "Decision": {
        "metareview": "The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not quite enough for acceptance"
    },
    "Reviews": [
        {
            "title": "Simple method for learning sentence representations, with competitive results",
            "review": "\n== Clarity == \nThe primary strength of this paper is the simplicity of the approach.\n\nMain idea #1: corrupt sentences (via random insertions/deletions/permutations), and train a sentence encoder to determine whether a sentence has been corrupted or not.\n\nMain idea #2: split a sentence into two parts (two different ways to do this were proposed). Train a sequence encoder to encode each part such that we can tell whether the two parts came from the same sentence or not.\n\nI can see that this would be very easy for others to implement, perhaps encouraging its adoption.\n\n== Quality of results ==\nThe proposed approach is evaluated on the well-known SentEval benchmark.\n\nIt generally does not outperform supervised approaches such as InferSent and MultiTask. However, this is fine because the proposed approach uses no supervised data, and can be applied in domains/languages where supervised data is not available.\n\nThe approach is competitive with existing state-of-the-art sentence representations such as QuickThoughts. However, it is not definitively better:\n\nOut of the 9 tasks with results for QuickThoughts, this approach (ConsSent) performs better on 3 (MPQA +0.1%, TREC +0.4%, MRPC +0.4%). For the other 6 tasks, ConsSent performs worse (MR -1.8%, CR -1.7%, SUBJ -1%, SST -3.8%, SK-R, -2.4%). Taken together, the losses seem to be larger than the gains.\n\nFurthermore, the QuickThoughts results were obtained with a single model across all SentEval tasks. In contrast, the ConsSent approach requires a different hyperparameter setting for each task in order to achieve comparable results -- there is no single hyperparameter setting that would give state-of-the-art results across all tasks.\n\nThe authors also evaluate on the newly-released linguistic probing tasks in SentEval. They strongly outperform several existing methods on this benchmark. However, it is unclear why they did not compare against QuickThoughts, which was the strongest baseline on the original SentEval tasks.\n\n== Originality ==\nThe proposed approach is simple and straightforward. This is on the whole a great thing, but perhaps not especially surprising from an originality/novelty perspective.\n\nTherefore, the significance and impact of this approach really needs to be carried by the quality of the empirical results.\n\nThe sentence pair based approaches (ConsSent-N and C) are conceptually interesting, but don't seem to be responsible for the best results on the linguistic probing tasks.\n\n== Conclusion ==\n\nPros:\n- conceptual simplicity\n- competitive results (better than many previous unsup. sentence representation methods, excluding QuickThoughts)\n- strong results on SentEval's linguistic probing task\n\nCons:\n- no single hyperparameter value (perturbation method and value for k) gets great results across all tasks\n- some important baselines possibly missing for linguistic probing tasks",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea to learn sentence representations",
            "review": "This submission presents a model for self-supervised learning of sentence representations. The core idea is to train a sentence encoder to predict sequence consistency. Sentences from a text corpus are considered consistent (positive examples), while simple editions of these make the negative samples. Six different ways to edit the sequence are proposed. The network is trained to solve this binary classification task, separately for all six possible editions.\nThe proposed approach is evaluated on SentEval giving encouraging results.\n\n+ The proposed approach is interesting. It is similar in some sense to the self-supervised representation learning literature in computer vision, where the network is trained to say- predict the rotation applied to the image.\n\n- If one considers that sentence encoders can be trained using a pretext task, this paper lacks a very-simple-yet-hard-to-beat baseline. Unlike for images, natural language has a very natural self-supervised task: language modeling. Results reported for language-modeling-based sentence representations outperform results reported in the tables by a big margin. Here is at least one paper that would be worth mentioning:\n- Radford, Alec, Rafal Jozefowicz, and Ilya Sutskever. \"Learning to generate reviews and discovering sentiment.\" arXiv preprint arXiv:1704.01444 (2017). \nIn order to make things comparable, it would be good to provide reference numbers for an LSTM trained with a LM objective on the same data as the experiments in this paper.\n\n- If I understood correctly, all variants are trained separately (for each of the 6 different ways to edit the sequence). This makes the reading of the results very hard. Table 2 should not contain all possible variants, but one single solution that works best according to some criterion. \nTo this end, why would these models be trained separately? First of all, the main result could be an ensemble of all 6, or the model could be made multi-class, or even multi-label, capable of predicting all variants in a single task.\n\nOverall, I think that this paper proposes an interesting alternative for training sentence representations. However, the execution of the paper lacks in several respects outlines above. Therefore, I lean towards rejection, and await the other reviews, comments and answer from the authors to make my final decision.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper presents an unsupervised sentence encoding method trained to classify consistent (original) and inconsistent (corrupted) sentences. The trained encoders are used in a variety of tasks with good performance.",
            "review": "The paper presents an unsupervised sentence encoding method based on automatically generating inconsistent sentences by applying various transformations either to a single sentence or a pair and then training a model to classify the original sentences from the transformed ones.\n\nOverall, I like the paper as it presents a simple method for training unsupervised sentence models which then can be used as part of further NLP tasks.\n\nA few comments on the method and results:\n\n- The results on Table 2 shows that supervised methods outperform unsupervised methods as well as the consistency based models with MultiTask having the largest margin. It would've been interesting to experiment with training multi-task layers on top of the sentence encoder and see how it would've performed.\n- The detail of the architecture is slightly missing in a sense that it's not directly clear from the text if the output of the BiLSTMs is the final sentence encoding or the final layer before softmax?\n- Also I would've thought that the output of LSTMs passed through nonlinear dense layers but the text refers to two linear layers.\n- When I first read the paper, my eyes were looking for the result when you combine all of the transformations and train a single model :) - any reason why you didn't try this experiment?\n- The paper is missing comparison and reference to recent works on universal language models (e.g. Radford et al 2018, Peters et al 2018, Howard et al 2018) as they rely on more elaborate model architectures and training compared to this paper but ultimately you can use them as sentence encoders. \n- One final note, which could be a subsequent paper is to treat these transformations as part of an adversarial setup to further increase the robustness of a language model such as those mentioned previously.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}