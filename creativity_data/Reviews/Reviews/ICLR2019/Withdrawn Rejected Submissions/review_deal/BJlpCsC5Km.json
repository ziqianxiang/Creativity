{
    "Decision": {
        "metareview": "The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Intersting idea, but novelty is limited and experimental analysis could be extended."
    },
    "Reviews": [
        {
            "title": "Regularization of GANs to remove spurious modes - but is this what is needed?",
            "review": "Summary: The paper tries to answer the problems of regularizing GANS. They reparametrize the discriminator to be an explicit function of two densities: the generator probability density function q and a structured Gibbs distribution v.  \n\nComments:\n1: This paper focuses on mode coverage problems, where spurious modes of learned model(q) not supported by target model(p) are pruned off.  It is not clear why this is a significant problem.  GAN trained models typically suffer from mode collapsing, requiring additional noise injection to support generation of diverse data.  This work seems to argue that the opposite is worth paying attention to, focusing on removal of modes.\n\n2: The implementation of the architecture is similar to R2P2, except for the introduction of a new adversarial component. But according the evaluation in table 1 and table 2, we see that baseline model R2P2 performs better in -H(p,q) and for -H(q, pKDE) the value is near equal to their model. \n\n3: They assume the generator is invertible, which enables the analytic evaluation of the q. But no supporting evidence or design architecture for the statement above is provided.\n\n4: The explanation of imposing structure on the model distribution is not clear. In the introduction they first claim “we cannot impose structure directly on the joint distribution of a GAN’s outputs.” But after they claim “we submit that regularizing the structure of a GAN’s generator and discriminator is generally more difficult than imposing meaningful structure directly on the model distribution, which we will refer to as q. These two statements conflict because the model distribution is a joint distribution of GAN’s outputs.\n\n\n6 Typos: \n1)\tin equation(1) we should minimize q for all the terms. \n2)\tin equation(1) first term is unrelated with v. \n3)\tin equation(1) the sup is for the last two terms. \n4)\tin equation(2) in RHS of equation the first term q_ |  should be q_ .\n\n7: Writing could be improved.\n\n8: In table 2 what’s the meaning of evaluation metric Road%\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Decent idea but need more motivating experiments.",
            "review": "The paper proposes to reparameterize the discriminator to be an explicit function of two densities so that one could inject domain specific knowledge easily. As the authors say, that one way to inject domain specific information is by learning an energy function. Making use of this intuition, authors proposed to  regularize the discriminator in  \n(GANs) framework by leveraging structured Gibbs distributions.\n\nI found the introduction a bit hard to read. Otherwise paper is written in a readable way. \n\nSomething which I like about this paper, is authors use the proposed method for actual RL problems as compared to just image generation. I think this is important as well as interesting.  As a community we should be moving towards evaluating generative models for the problems where we actually want to use generative models for.\n\nSome questions:\n\n- I'm not sure if the paper is really novel as the authors themselves point out that it corresponds to adding adversarial component in R2P2. \n- I also did not find results very convincing. As I said, its important to evaluate on RL problems ONLY if it makes sense on toy problems first. Like in the paper, authors made a big claim about reducing spurious modes, but it has not been demonstrated any where per se. May be authors can construct a toy problem in which they can show that the spurious mode issue, and how the proposed method kills these spurious modes.  This also reminds me of the literature in Boltzmann machines and more recently in Variational Walkback [1]. This could also be cited, and could be interesting to authors.\n\n[1] Variational Walkback, https://arxiv.org/abs/1711.02282. The authors in Variational walkback also make the assumption p == q. \n\nWhat would make the paper stronger ?\n- Constructing toy problems in order to illustrate the mode coverage and spurious modes issue would be interesting.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It’s difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. \n\nIt appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper.\n\n1.    Typo in the training objective (Eq .1):  the second (or the first) \"sup\" should be removed? \n\n2.    Section 2.3 is very confusing. Particularly, how is the parameter \\phi introduced? What’s the detailed update of \\phi? \n- \"We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p.\" If v_\\phi(x) is a distribution, what’s the parametric form of v?  \n- \"Generally, this is achieved by structuring the energy function V_\\phi:=\\log v_\\phi.\" It seems that V_\\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\\phi(x), however, why the distribution is self-normalized? Specifically, why \\int \\exp(V_\\phi) dx = 1? Otherwise, how the authors deal with the partition function \\int \\exp(V_\\phi(x)).  \n- It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs?\n\n3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode.\n\nIf the authors can fully address the concerns above, I will consider changing the scores.  \n\nOther comments:\n1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. \n\n2. Duplicate citations – R2P2 [35] [36]\n\n3. Other related papers:\n - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17\n- Tu et al., Learning Approximate Inference Networks for Structured Prediction, ICLR 18",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}