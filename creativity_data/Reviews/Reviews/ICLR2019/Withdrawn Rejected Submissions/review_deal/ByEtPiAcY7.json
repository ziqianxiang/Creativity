{
    "Decision": {
        "metareview": "The presented paper introduces a method to represent neural networks as logical rules of varying complexity, and demonstrate a tradeoff between complexity and error. Reviews yield unanimous reject, with insufficient responses by authors.\n\nPros:\n+ Paper well written\n\nCons:\n- R1 states inadequacy of baselines, which authors do not address.\n- R3&4 raise issues about the novelty of the idea.\n- R2&4 raise issues on limited scope of evaluation, and asked for additional experiments on at least 2 datasets which authors did not provide.\n\nArea chair notes the similarity of this work to other works on network compression, i.e. compression of bits to represent weights and activations. By converting neurons to logical clauses, this is essentially a similar method. Authors should familiarize themselves with this field and use it as a baseline comparison. i.e.: https://arxiv.org/pdf/1609.07061.pdf ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Method converts neural networks into logical rules of varying complexity. Issues around evaluation not addressed."
    },
    "Reviews": [
        {
            "title": "review",
            "review": "This paper is looking at how to extract knowledge from CNNs to help improve explainability and robustness against an adversarial attack.  It is using a known technical call M-of-N rules.\n\nThis problem of explainability of NN's is an important one and rules are a good step in that direction.  \n\n+ The paper is generally well written\n\n- The contribution seems to be relatively small\n- The evaluation is limited, only 1 dataset and only 1 technique evaluated\n\nGeneral advice for work in AI explainability:\nWhen one looks at the problem of AI explainability it is important to describe who the target audience is for the explanation.  Is it a machine learning expert, who wants to debug the model?  Is it an end user who wants to better understand why the prediction was made?  Is it a regulator who is trying to ensure the model's predictions are fair?\n\nEach of these personas will come with different needs and different technical backgrounds, so an assumption that some artifact (a rule set?) is \"explainable\" may apply to one group, but not the other group.  For example, rules are likely to be more explainable to a ML expert, but may not be to an end user, unless they are very small.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "M-of-N rule extraction is back",
            "review": "The paper proposes to rewrite each neuron of a neural network as a M-of-N decision rule. An measure of rule complexity (which takes into the account the number of terms in the rule) is proposed, and an approximate rule induction algorithm which binarizes the neurons using an information gain criterion is provided.\n\nThe paper gives no evaluation of the accuracy of extracted rules on a test set. Instead, the fidelity to network is evaluated on the train set for individual neurons (see below).\n\nSince even for moderately-sized networks the method would result in lots of rules, the authors propose instead to use the proposed algorithm and complexity criterion as a tool for understanding the complexity of concepts detected by a layer. The results, gathered in Figure 1, suggest that for some layers complex rules do approximate the behavior of neurons, while for other layers a neuron can't be replaced with a single but complex M-of-N rule.\n\nThe ideas presented in this paper seem rudimentary and require further exploration before being publishable. First of all, the main result of complexity-vs-layer doesn't differentiate between failures of the approximate rule induction algorithm (the terms in rules are considered for inclusion in a single order, reducing the search space) and the genuine complexity of the rules - this can be verified by evaluating a more exhaustive algorithm on at leas a few neurons (not necessarily on all of them).\n\nSecond, if the rules are extracted in a layerwise fashion,  their errors accumulate for deeper layers. However, Algorithm 1 suggests that each neuron is replaced by a rule independently from others, and moreover that it requires the true value of the neurons in the layer below, not of their rule counterparts. This means that the rules can't be combined and explains why the paper doesn't provide any measure for aggregate rule accuracy.\n\nSimilarly, the robustness of rules to adversarial examples is meaningless - it seems that applying the rules results in a system which is less accurate (only 8/10 of rules mimick their neurons with no rule complexity penalty) overall, but also makes fewer adversarial examples.\n\nMinor remarks.\nPlease don't ever produce Figures such as Figure 1: no legend (description in text), color selection is not black and white friendly, font is so small that the axis labels are hard to read... In fact, the poor quality of the Figure by itself should be sufficient  to reject the paper for not abiding to the author guidelines (sec. 4.3: All artwork must be neat, clean, and legible.)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting contribution but still rudimentary",
            "review": "Summary: This paper proposes a novel knowledge extraction method using M-of-N rules\nto help interpret hidden features in a Convolutional Neural Network (CNN). While the idea itself is interesting, I think that the paper is still in a very early stage and needs more work before it can be accepted. Detailed comments below.\n\nPros: \n1. The paper proposes a new algorithm to interpret CNNs.\n2. The paper is reasonably well written.\n\nCons: \n1. The experimental evaluation is quite weak. The authors present their (partial) results on a single dataset and also seem to generalize some of the findings in a rather misleading way. \n2. The proposed method is not compared against any baseline though there are ample rule-based methods to understand NNs in literature.\n2. It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? \n\nDetailed Comments:\n1. I think that the paper is missing a very clear discussion on what is novel about the proposed method in contrast with recent work on explaining NNs (or black box models) using rule based approaches. Examples of relevant papers include \"Anchors: High-Precision Model-Agnostic Explanations\" by Ribeiro et. al. and \"Interpretable & Explorable Approximations of Black Box Models\" by Lakkaraju et. al. among others. \n2. Another important piece of discussion that is missing is how the proposed search technique for extracting M-of-N rules is novel compared to a lot of prior literature which deals with the same problem\n3. I would strongly encourage the authors to experiment with at least three different datasets and multiple CNN architectures. \n4. I would really like to see the output of the proposed approach. What kinds of rules are being generated at each stage? It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? \n5. It would be good to do a simple user study to demonstrate that human users are able to understand something useful from the generated explanations. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction",
            "review": "Overall, this is a interesting paper on an important topic: knowledge extraction from Neural Networks.\nEven though the authors seem propose a novel approach to knowledge extraction, the paper would \ndramatically benefit from two additions:\n- an empirical evaluation on at least two more datasets (as is, the paper uses a single dataset)\n- an illustrative-but-realistic example of how at least one rule is extracted from each layer of the neural network \n\nOther comments:\n- on page 4 (1st paragraph in 3.3), the authors talk about a \"test set\" that, it turns out, it is extracted from the actual training set (1st paragraph of 4.1); the authors should use a more careful terminology\n- from the paper, it seems that the authors tried a single randomly chosen set 1000 random inputs in 4.1; they should most definitely try several such sets\n- Figure 1 should have a legend in the image, rather than as a 2-line caption\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}