{
    "Decision": {
        "metareview": "This paper introduces a newly collected dataset of natural language interactions between a tourist and a guide for localization and navigation.  The paper also includes baseline experiments with a reasonably novel approach. \nThe task is well motivated (although an open question remains due to GPS, comment by reviewer 1), but the description of the dataset and collection, approach and experiments were not ideal in the first version of the paper. Much of the information was pushed to the appendix and it was hard to follow the paper without going back and forth, and even then some points were missing. Authors rewrote parts of the paper to address these concerns, but there are still some open questions. For example, is it possible to have sub-tasks, given the task is complex and may not be easy to accomplish as a whole? Or could simple LSTM be another baseline (the final review of the third reviewer)?\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Great dataset, but many questions remain open"
    },
    "Reviews": [
        {
            "title": "A realistic dataset on dialogs for navigation, with a report of some early studies",
            "review": "The paper introduces a new dataset \"Talk the Walk\" that are dialogs\nbetween a guide and a tourist, where the guide is to help the tourist\nnavigate to a target location.  The guide has access to a map and the\ntarget location, but he relies on the tourist to communicate her\nstate (location) by natural language.\n\nPros.:\n\nThe task represented in the dataset can be highly challenging, and\nrespectable effort went into creating the dataset based on real city\nneighborhoods.   The description and analysis of the dataset are\ndetailed.  The paper is well written up to the end of page 3.\n\nThe dataset by itself is a good contribution to the scientific\ncommunity when it is shared.  There could be many topics open\nfor studying within the same data.\n\nThe list of references and related work is exceptionally thorough\nand useful for researchers interested in the topic.\n\nCons.:\n\nThe description of the experiments done with the dataset, however,\nsuffers from being overly cryptic.  The methods are not sufficiently\nmotivated, very few alternatives are presented and argued against,\nand the several sections give a dry report of the sequences of things\nthe authors did.  It is not clear how others may find value in the\nresults and conclusions.\n\nWhile the paper opens with the emphasis of a real-world setting,\nafter a series of simplifications (e.g. landmark typing, perfect perception)\nit seems that much of the full complexity of the natural task is taken out,\nand the main goal of the study is no longer clear.\nFor example, since there are only 9 types of landmarks,\nin a small neighborhood there are not many combinations to draw reference to.\nSimple observation sequences of such can easily narrow down the location\nuncertainty.  It is important to highlight what the remaining open\nissues are that make the task still challenging.\n\nMisc.:\n\nExamples are missing in the discussion of the experimental tasks.\ne.g. in the study of emergent language, what could be a message that a\ntourist may generate to describe his location?  what makes it hard for\nthe guide to decode it?  Likewise, what could be an example state of\nthe tourist and the description of that state in natural language?\nWithout the examples, it is difficult for the reader to have a sense\nof the challenges in each task.\n\nTable 8 is the first place where (finally) some utterances are presented.\nHowever the description in the table or in the text is not sufficient\nto convey the point that is supposed to be explained by the example.\n\nThe descriptions of the landmarks are restricted to the type of business\nat the location with 9 possibilities.  Is the list of 9 exhaustive?\nAre there any exceptions  (e.g. schools)?  How are such exceptions\nrepresented and treated in the dialogs?\n\nTo what extent the difficulty of the tasks depends on the variability\nof the combination of landmarks visible at each location?\n\nWhat could be a simplest way to do this without neural-modeling?\ne.g. with the many limitations that are built into the task and its\nrepresentations, will a simple decision tree based instruction method suffice?\nOr a traditional algorithm that relies on repeated exploration and evaluation?\nIt is surprising that such possibilities are not even mentioned.\nA complex neural architecture does not seem to be well justified unless\nit is motivated by the need to overcome limitations of a classical method.\n\nIrrelevant to the research effort, a thought about the dataset is that,\nin these days with popular uses of GPS, the reliance on such dialogs for\nnavigation feels a bit backwards.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A challenging new task and dataset",
            "review": "The paper introduces a new task called \"Talk the Walk\", where a tourist and a guide has to communicate in natural language to reach a common goal. It also introduces strong baselines for the task. The descriptions are thorough and clear. My only worry is that the task is too hard and has too many complexities to be a stand alone task.  Future work will probably focus on sub-parts of the task.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Emergent Language is Easier than Natural Language",
            "review": "The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. \n\nThere are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There's a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It's very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they \"successfully complete the task\" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?\n\nThe primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn't feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's NLU/NLG system is quite weak and maybe there's a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.\n\nAs I understand the paper right now:\n1. Humans talking to one another do very well on the task and achieve success very quickly. \n2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).\n3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.\n\nThere are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn't seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what's going wrong.  The results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. \n\nI haven't touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.\n\nI found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  \n\nMinor: URLs intermittently anonymized page 12 vs 19",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}