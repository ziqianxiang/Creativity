{
    "Decision": {
        "metareview": "The manuscript centers on a critique of IRGAN, a recently proposed extension of GANs to the information retrieval setting, and introduces a competing procedure. \n\nReviewers found the findings and the proposed alternative to be interesting and in one case described the findings as \"illuminating\", but were overall unsatisfied with the depth of the analysis, and in more than one case complained that too much of the manuscript is spent reviewing IRGAN, with not enough emphasis and detailed investigation of the paper's own contribution. Notational issues, certain gaps in the related work and experiments were addressed in a revision but the paper still reads as spending a bit too much time on background relative to the contributions. Two reviewers seemed to agree that IRGAN's significance made at least some of the focus on it justifiable, but one remarked that SIGIR may be a better venue for this line of work (the AC doesn't necessarily agree).\n\nGiven the nature of the changes and the status of the manuscript following revision, it does seem like a more comprehensive rewrite and reframing would be necessary to truly satisfy all reviewer concerns. I therefore recommend against acceptance at this point in time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "An interesting contribution, but lacking in depth."
    },
    "Reviews": [
        {
            "title": "Interesting findings though depth and rigor could have been better",
            "review": "This paper tries to argue that the formulation of IRGAN (a method from 2017 that aimed to use GANs for the standard IR task of estimation query-document relevance) is now well-founded and has inherent weaknesses. Specifically the paper claims that (unlike regular GANs and what was likely intended by the authors or IRGAN) the discriminator and generator are working against each other. The paper then aims to show a couple of more well-founded different (generator-free) setups that perform about as well (if not better) as the original IRGAN work.\n\nOverall I found the work to be quite interesting and the findings to be illuminating. That said I think the paper notably lacked rigor and depth which definitely hurt the quality of the paper.\n\nBelow are my thoughts on the different facets as well as more detailed strengths / weaknesses breakdown:\n\nQuality: Above average\nAs mentioned I think some of the findings are illuminating and thus overage the paper scores well on this aspect.\n\nClarity: Slightly above average\nWhile the paper is largely easy to follow, there are certain key sections that are not well explained / have fundamental errors.\n\nOriginality: Strong\n\nSignificance: Little below average\nMy (main) concern here with this work is that the missing rigor and depth of the work is what is needed for readers to have a deeper understanding of the fundamental issue so as to be able to rectify it in future works.\n\n---\n\nStrengths / Things I liked about the work:\n\n+ The topic / theme of the work: I believe as a community we should encourage more such works that take a critical deep dive into recently proposed methods that may have some inherent weaknesses. As the authors noted the IRGAN work has become quite popular despite some of these (previously unknown) issues.\n\n+ The experimental results in general do a fair job illustrating the likely issue (though I would have liked to see more rigor and depth here as well as detailed below)\n\nWeaknesses / Things that concerned me:\n\n- (W1) Lacking rigor / depth: One of my big concerns with this work is that the analysis to demonstrate the inherent flaws of IRGAN if fairly shallow and not detailed enough. For example, Section 5 (which should have been the key section of the work) is quite poorly written and not rigorous enough. Claiming that log(1-z) can be replaced with - log(z) is incorrect -- how can this substitution be made as is?\n\nOverall my sense after reading the work is that I understand that the IRGAN formulation is not completely well-formed in terms of discriminator/generator synergy (the pairwise formulation has the additional issue of separating real pairs rather than higher rank-lower rank pairs). However I do not buy that the generator and discriminator directly oppose each other as is claimed in the work (I believe this arises only due to the incorrect claim that log(1-z) can be replaced with -log(z)).\n\nThus at the end of the day I feel the reader is willing to buy there is a issue with the formulation, but they do not fully understand it not do they understand enough to understand how to rectify the underlying issues. To me that was unfortunate as the paper would have been an excellent work if it had done so.\n\n- (W2) Missing some experimental results / deeper insights : There were some notable empirical results that were missing or not provided, that raised some concerns in my mind. For instance I don't see the co-training approach listed for the MovieLens dataset .. Why so? The authors make a secondary claim that they are able to improve upon IRGANs via their proposed approach but then they do not substantiate these on all the datasets which seems like a notable oversight.\n\n - (W3) Missing details: To add to the above I think the authors can clearly be more detailed in describing for instance the models for D, G, p_\\psi etc .. Right now I can speculate what they are but I don't think a reader should be expected to speculate in such cases. Likewise empirical details about the datasets and their sizes could easily have been added.\n\nAlso the paper presents the IRGAN pairwise approach and mentions pairs in a couple of places but I don't see an approach that can learn from pairs among the ones proposed.\n\nAnother example is the two proposed models Fig 2a (Only discriminator) vs Fig 2b (cotraining). I don't see an explanation or intuition for why 2b is expected to be better than 2a. Given the claims of the work I would have wanted to understand this better.\n\n- (W4) Significance testing: This is an important experimental process to understand the validity of some of the claims. While I understand it is not the main claim of the paper, understand the significance of these differences helps put things in perspective. I would strongly urge the authors to add this for all of their experimental results not just the ones the proposed models are outperformed by IRGANs.\n\n- Lastly I would urge the authors to be rigid and clear in their notations. For example in the equation in section 4.5, \"o'\" occurs out of nowhere.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper points out the limitation of IRGAN but it  is better to submit to SIGIR.",
            "review": "This paper is closely related to the SIGIR 2017 paper “IRGAN: A minimax game for unifying generative and discriminative information retrieval models”. The SIGIR paper proposed a Generative Adversarial Nets (GAN) model for Information Retrieval (IRGAN). And in this paper, the authors dissect the IRGAN model and figure out that the discriminator and generator of IRGAN are optimizing directly opposite loss functions. They also provide experimental studies and show that the superiority of IRGAN in the experiments are mainly because of the discriminator maximizing the likelihood of the real data and not because of the generator.  \nStrong Points:\nConsidering that the IRGAN becomes popular after it being published, I should say the analyzing of this paper is important for researchers in the IR domain.\n\nConcerns or Suggestions:\n1.\tExcept the analyzing on IRGAN, the contribution of this paper is limited. Most of the parts of this paper introduce GAN and IRGAN. Only Section 5 focuses on the analyzing. The methods claimed new proposed, Single Discriminator and Co-training, are good for supporting the analyzing but they are not quite novel.\n2.\tIt is strange to introduce the two models, Single Discriminator and Co-training in the experimental setting section. I would suggest to separate them out and introduce them earlier.\n3.\tThe topic of this paper is more related to the IR domain. It will be better to publish it in SIGIR, together with the IRGAN paper.\n4.\tBesides, if it is possible, I would suggest researchers who have direct experiences on the implement and study on IRGAN give more comments on this paper. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good idea, but paper suffers on many important points",
            "review": "This paper trains an information retrieval (IR) model by contrasting the joint query-document distributions, p(q, d) with negative samples drawn from a resampling of the product of marginals, p(q) x p(d). They use a second discriminator to provide the re-weighting (I believe picking to top negative sample from the other model) and train this other model in a way that mirrors the first. They also attempt to point out some theoretical problems with a competing model, IRGAN, which uses a generator that is trying to model the joint.\n\nWhile I like the proposal idea, I think the paper has too many problems to warrant publication. First, the story is very disappointing. The authors phrase most of the paper as a critique of IRGAN, but this critique falls short. Really this is more of a paper about where to get negative samples when training a model of the joint (or the log-ratio in this case). Using negative samples from real data with noise contrastive estimation [1] is found in numerous works in NLP [2][3], and has gained some recent attention in the context of representation learning [4][5]. The first algorithm proposed is essentially doing a sort of ranking loss on negative samples, which mirrors similar works [6]. In fact, the generator in IRGAN could be viewed as just a parametric / adaptive negative sampling distribution in the context of NCE for the ultimate purpose of learning an estimate of the log-ratio. The most interesting thing I think of this work here is the co-training, i.e., using another model to help re-sample, and I think this idea should be explored in more detail.\n\nSecond, the paper spends far too much time revisiting prior work than addressing their own model, doing more analysis, providing more insight.\n\nThird, the paper is just poorly written. The notation is confusing, some of the equations are unclear (I have no idea how \"r\" is used in any of this), and the arguments of the baseline in IRGAN don't really doesn't make any sense.\n\nNotes:\nP1\nI don't really follow why IRGAN is so central to this work. Good ideas aren't difficult to motivate, especially if empirically everything works out.\nP2\nI'm having trouble with claims, especially more recently, about GAN instability, particularly since numerous approaches [7][8] seem to have more or less solved the problem.\n\nThe use of \"|\" in G is awfully confusing.\nP3\nAlmost 2 pages of unnecessary background\n\nP4\nWhy are we using \"|\" in functions? What's wrong with \",\"?\ntheta = \\theta\nI don't understand the point of the quote (in italics).\nWhat happened to \"r\" in all of this?\nThe last two equations and their relationship could be more clear.\n\nYou use italics, so is this supposed to be a quote? But then you have a section which attempts to show this.\nP5\nI have no idea what's supposed to be going on in 5). The samples from the real joint don't factor in the generator gradient, or at least it's absolutely not clear that this pops out of the baseline? Then you switch from log (1 - x) to - log x and there's some claim about this violating the adversarial objective?\n\nIt took me more than a few reads to figure out what the equation at the bottom of P5 is doing: is this resampling? It's fairly unclear.\n\n[1] Gutmann, Michael U., and Aapo Hyvärinen. \"Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.\"\n[2] Mnih, Andriy, and Koray Kavukcuoglu. \"Learning word embeddings efficiently with noise-contrastive estimation.\" \n[3] Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\"\n[4] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" \n[5] Hjelm, R. Devon, et al. \"Learning deep representations by mutual information estimation and maximization.\"\n[6] Faghri, Fartash, et al. \"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives.\"\n[7] Miyato, Takeru, et al. \"Spectral normalization for generative adversarial networks.\"\n[8] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}