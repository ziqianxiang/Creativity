{
    "Decision": {
        "metareview": "This paper presents methods for telegraphic summarization, a task that generates extremely short summaries.\n\nThere are concerns about the utility of the task in general, and also the novelty of the modeling framework.\n\nThere is overall consensus between reviewers regarding the paper's assessment the feedback is lukewarm.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Reasonable approach (but somewhat unsurprising) for a not entirely convincing problem",
            "review": "The authors introduce the problem of telegraphic summarization: given a sentence, we want to reduce its size while retaining its meaning, with no penalty for grammatical mistakes. The main application presented by the author is that of summarizing fictional stories and plays.\n\nThe setting proposed by the author prescribes that the summarized sentence can be obtained by the input sentence by dropping some words. So, for example, the simplest baseline for this problem would consist of simply dropping stop words.\n\nThe approach proposed is basically an auto-encoder, consisting of a 2-step encoder-decoder network: in the first step, the sentence is encoded into a vector which is in turn decoded to a (smooth) indicator vector to mask words in the sentence; in the second step, the masked sentence is encoded into a vector, which is in turn decoded into the output (summarized) sentence. \n\nThe optimization is a tradeoff between recoverability of the input sentence and norm of the indicator vector (how many words are dropped). In order for the network not to learn repetitive masking patterns (eg, drop first half of the sentence, or drop every other word), an additional loss is introduced, that penalizes keeping easily inferable words or dropping hard-to-infer words.\n\nConcerns:\n- the problem doesn't seem to be well-motivated. Also, the length of the obtained summarized sentences is ~70% that of the original sentences, which makes the summaries seem not very useful.\n- the proposed complex architecture seems not to justify the goal, especially considering that simply dropping stop words works already quite well. \n- In order for the presented architecture to beat the simple stop-words baseline, an additional loss (L4, linkage loss) with \"retention weights\" which need to be tuned manually (as hyper-parameters) is required. \n- there's not enough discussion about the related work by Malireddy et al, which is extremely similar to this paper. A good part of that work overlaps with this paper.\n- comparison with literature about abstractive summarization is completely missing.\n\nMinor comments:\n- Figure 1: Indicator Encoder should be Indicator Decoder.\n- Are negations part of your stop words? From your discussion, you should make sure that \"not\", \"don't\", \"doesn't\", ... do not belong to your stop word set.\n- How did you optimize the hyper-parameters r (desired compression), the regularization weights, and the retention weights?\n- Were pre-trained word embeddings used as initialization?\n- What's the average compression of golden sentences?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed linkage loss for telegraphic sentence compression is inspired, but some choices still seem arbitrary and the usefulness of the task needs to be better motivated.",
            "review": "The authors consider the problem of telegraphic sentence compression: they train a system in an unsupervised fashion to predict which words can be dropped from a sentence without drastic loss of information. To that end, they propose a new auto-encoding type architecture which uses the extracted words as latent code, and, most importantly, a linkage loss which relates a word's perplexity given the summary of its left context to its likelihood of being retained. The model itself is sober and well motivated, and the linkage loss is, to the best of my knowledge, original. The authors show that their method outperforms some simple baselines in terms of ROUGE and compression on a small human-annotated test set.\n\nThe paper is generally well written, although the initial presentation of the model could be made a little clearer (it is not obvious from the text that the Decoder takes the text as input -- Figure 2 helps, but comes a couple pages later). However, the authors fail to appropriately justify the choice of their hyper-parameters (e.g. \"The optimum value of r for our experiments was found to be 0.65\", \"the best value of b was found to be 5\", \"The weights λ1, λ2, λ3, and λ4 have been set to 3, 2, 50 and 3 respectively for our experiments\" -> how is \"best\" measured on the validation set, which does not have gold references?). The choice of the specific sparsity constraint (one could as well imagine using a simpe L1 regularization for the Binarization loss) and of \\Chi_i (why not simply use the likelihood?) could also be better motivated.\n\nThe model also relies on a hand-crafted rules (Section 3.3) whose effect needs to be made more evident. What weights are used in practice? How were they chosen (\"We observed that...\" needs to be further developed)? The authors claim that \"the quantitative scores are not affected significantly\", but that is presumably only the ROUGE score, what about annotator's preferences?\n\nMost importantly, however, the task of telegraphic sentence compression, whose usefulness is not a priori obvious, is barely motivated.  The author refer to \"Malireddy et al. (2018)\" for a justification, but it is important to note that the latter provides a telegraphic summary of a whole document, with a compression factor of 0.37. The claim is that the concatenation of the telegraphic sentence compression can act as a summary of a whole document, but given the fact that compression for individual sentences is closer to 0.69, this is yet to be demonstrated. And even if that were true, it is unclear whether the cognitive load of reading a sequence of telegraphic sentences would be that much lower than that of reading the original text.\n\nThis paper presents some interesting ideas and is well written, but the content is not quite sufficient for publication. In addition to the clarifications and justifications requested above, the authors are encouraged to apply there methods to full lengths documents, which would make for a more substantial contribution. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty and advantage of the proposed methods are limited",
            "review": "The paper explores unsupervised deep learning model for extractive telegraphic summaries, which extracts text fragments (e.g., fragments of a sentence) as summaries. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.\n\nFirst, in term of methodology (as described in Section 3), the paper has little novelty. There has been intensive study using various deep learning models on summarization. The models described in the paper contain little novelty compared with previous work using autoencoder and LSTM for both extractive and abstractive summarization. \n\nSecond, the paper claims contributions on using deep learning models on telegraphic summarization, but the advantage is not well demonstrated. For example, the advantage of the resulting summary is not compared with state-of-the-art sentence compression models with intrinsic evaluation or (probably better) with extrinsic evaluation. (By the way, it is interesting that the paper argues the advantage of using telegraphic summaries for fictional stories but actually gives an example which looks also very typical in news articles (the “earthquake Tokyo 12 dead” example).)\n\nThird, there has been much work on speech summarization that summarizes with the “telegraphic” style (this is natural, considering speech transcripts are often non-grammatical, and “telegraphic” style summaries focusing on choosing informative fragments actually result in usable summaries.) The author(s) may consider discussing such work and compare the proposed methods to it.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}