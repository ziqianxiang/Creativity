{
    "Decision": {
        "metareview": "The paper presents an explicit memory that directly contributes to more efficient exploration. It stores trajectories to novel states, that serve as training data to learn to reach those states again (through iterative sub-goals). \n\nThe description of the method is quite clear, the method is not completely novel but has some merit. Most weaknesses of the paper come from the experimental section: too specific environments/solutions, lack of points of comparisons, lacking some details.\n\nWe strongly encourage the authors to add additional experimental evidence, and details. In its current form, the paper is not sufficient for publication at ICLR 2019.\n\nReviewers wanted to note that the blog post from Uber (\"Go-Explore\") did _not_ affect their evaluation of this paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting research direction but weak paper"
    },
    "Reviews": [
        {
            "title": "Good idea, good demonstration, good score",
            "review": "This paper is the first showing that achieving self-generated tasks during spontaneous exploration and getting reinforced by self-supervised signals is a promising way for the agent to develop skills itself.\nThe scores are demonstrative on several tasks.\nIt opens interesting direction for further research.\n\nREM: \nfew typos like \"An state\"\nPlease plot in dash the count methods in the graphs (use oracle information)\n\nAnnexe C shall be integrated into the core of the paper. Could be simplified.\nThe cosine metrics shall be better integrated in it.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but rather weak paper. Can be improved a lot with additional writing effort",
            "review": "In this paper, the authors propose an exploration strategy based on the explicit storage and recall of trajectories leading to novel states. A pool of such trajectories is managed over time, and a method is proposed so that the agent can learn how to follow a path corresponding to these trajectories so as to explore novel states. The idea is demonstrated in a set of room experiments, and quickly shown efficient in Montezuma's Revenge and PrivateEye Atari games.\n\nOverall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).\n\nWhat I like most in the paper is the split of exploration methods into 3 categories: adding some \"intrinsic reward\" bonuses to novel states (curiosity-driven exploration) , trying to reach various goals (goal-driven exploration) and using memory to reach again novel states (memory-driven exploration). Actually, this split may be debated. For instance, some frameworks based on goals have been labelled curiosity-driven, e.g. \"Curiosity-Driven Exploration of Learned Disentangled Goal Spaces\" (Laversanne-Finot, Péré and Oudeyer, CoRL 2018), but anyways I find it useful. That said, this aspect of the introduction is reiterated in the \"Related Work\" section in a quite redundant way, whereas both parts could have been better integrated. Furthermore, the related work section is hardly a draft, I'll come to that later.\n\nThe presentation of the method in Section 2 is rather clear and convincing. My only concern is about the assumption that the agent is always starting in the same state. This assumption may not hold in many settings, and the approach appears to be quite dependent on it. A discussion of how it could be extended to a less restricting assumption would be welcome.\n\nThe experimental section is weaker. A few concerns:\n- I could not find much about the number of seeds, trials, the way to depict some variance, the statistical significance of the differences between results presented in Figure 1. The same is true about Figs. 2, 3 and 5.\n- In Fig.2, the claim that the author's method learns better models is hardly supported by the left-hand-side plot, and significance is not assessed.\n- I'm puzzled about the very low performance of baselines in the plots of Fig. 3. Could the author explain why these performances are null.\n- The Atari games section helps figuring out that the framework is not too specific of the rooms environment, but the lack of analysis does not help making sure that this is just the explicit recall mechanism that is responsible for superior performance and why.\n\n\nAnother point about this section is that poor writing does not help understanding some points.\n- to me, the first sentence of Section 3.2.2 does not make sense at all.\n- in the caption of Fig. 4, \"The second row is the heatmaps for states that the number of times being selected as a target state.\": I don't get what it means, thus I don't understand what that row shows.\n- Fig.5 comes with no caption\n\nAbout the related work:\n- The comparison to other methods using memory needs to be expanded. In particular, I would put HER-like mechanisms here rather than in 4.1, as \"explicit recall\" shares some importan ideas with \"experience replay\"\n- Section 4.4 (HRL) is not useful as is.\n\nFinally, in the conclusion, the claim that the method can be combined with \"many sota exploration methods\" is not supported, as the authors have only tried two and did not analyse the results in much details.\n\n\ntypos:\n\n- p4:\nwe can easily counting\n(include borders) => including\nis provide => provided\n\nare less less-visited states: quite inelegant\n\n- p7:\nIn Montezuma's Revenge, Comparing => comparing\nWhere they encourage => remove \"Where\"\n\n- p8:\nrecallcan => recall can\nthe problem of reach goals => reaching\nit succesfully reproduce => reproduces\n\nThe last paragraph of Section 4.2 needs a careful rewriting, as long sentences with parenthese in the middle appear to be some draft version.\n\ncontrol(Pritzel => Missing space\nOur method use memory => uses\nAlthough ..., but => remove but\n\nThe path function can be seen as a form of skills => skill?\nBesides, the \"can be seen\" needs to be further explained...\n\nAppendix\n\nFinally, we provided => provide\n\nis around (math formula) => cannot you be more specific?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Many hacks and heuristics. ",
            "review": "In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. \nThe final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).\n\n\nThe authors mainly combine a few previous approaches \"Self-Imitation Learning,\" \"Automatic Goal Generation for Reinforcement Learning Agents,\" and \"Curiosity-driven exploration by self-supervised prediction\" to design this algorithm which makes this approach less novel.\n\nGeneral comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many others\n\nThe main idea of the algorithm is clear, but the description of the pieces is missing.\n\nIt is not clear in stochastic setting how well this approach will perform. \n\nThe authors state that\n\"Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments.\"\nI took a look at this paper and could not find neither proof or quantification of \"effective\"-ness. Please clarify what the meaning this statement is.\n\nWhy s=s' is ambiguous to the inverse dynamics?\n\nWhat is the definition of acc in fig2?\n\nwhy (consin+1)^3/8 is chosen?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}