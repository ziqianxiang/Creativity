{
    "Decision": {
        "metareview": "This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written. \n\nHowever, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. \n\nFurthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. \n\nThis concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. \n\nThe paper thus requires significantly stronger baselines and attacks.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A novel approach but has significant issues "
    },
    "Reviews": [
        {
            "title": " ",
            "review": "The Paper is written rather well and addresses relevant research questions.\nIn summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). \n\nMy main points of critique are:\n\n1. The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This \nmakes me question  how favorable the trade-off between robustness vs performance is. \n\n2. The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\"\n\nI am not sure if this is true.   One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).\n\n\n\n[1] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach.",
            "review": "This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels.\n\nThe idea is clearly presented and evaluated. \n\n*Details to Improve*\nIt would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.\n\nIt would be great to see this extended to convolutional networks.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea but has limited use case",
            "review": "This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.\n\nWhile the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.\n\nPros:\n- Novel defense technique against very challenging white-box attacks.\n- Sound threat model drawn from traditional security.\n- Clearly written.\n\nCons:\n- Poor clean accuracy makes the technique very impractical.\n- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}