{
    "Decision": {
        "metareview": "a major issue or complaint from the reviewers seems to come from perhaps a wrong framing of this submission. i believe the framing of this work should have been a better language model (or translation model) with word discovery as an awesome side effect, which i carefully guess would've been a perfectly good story assuming that the perplexity result in Table 4 translates to text with blank spaces left in (it is not possible tell whether this is the case from the text alone.) even discounting R1, who i disagree with on quite a few points, the other reviewers also did not see much of the merit of this work, again probably due to the framing issue above. \n\ni highly encourage the authors to change the framing, evaluate it as a usual sequence model on various benchmarks and resubmit it to another venue.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "reject"
    },
    "Reviews": [
        {
            "title": "Some interesting ideas, but comparison to previous work might be misleading",
            "review": "[Note to the authors: I was assigned this paper after the reviewing deadline.]\n\nThe authors train language models on unsegmented text, simultaneously discovering word boundaries\nwithout direct supervision. Given the past history, but ignoring past segmentation decisions\nto keep computations tractable, the model predicts the next character segment (word-like unit)\nby combining a character-level LSTM with a lexical memory. To prevent overusing the\nlexical memory, which would lead to poor generalization, the authors propose a segment\nlength penalty.\n\nStrengths:\n\nThe model architecture is interesting, combining the benefits of a character-level\nmodel (open vocabulary) with those of a lexical model (effective for frequent character\nsequences).\n\nDespite the exponential number of possible segmentations, inference remains tractable\nusing dynamic programming (with some simplifying assumptions).\n\nThe ablation study clearly shows that both the lexical memory and the length penalty\ncontribute significantly.\n\nWeaknesses:\n\nThe writing quality is somewhat weak. Many errors should have been caught when\nproofreading the paper (e.g. \"The segmentation decisions and decisions\" and \n\"The the characters\" on page 1).\n\nI am confused by the key-value pairs of the lexical memory. Shouldn't character\nsequences be keys, and their trainable vector representations be values?\n\nIt is hard to evaluate how good the language models are, as the strength of the\nbaselines is unclear. How well-tuned is the LSTM?\n\nComparison to some other segmentation approaches (not necessarily with language modeling)\nis limited. In particular, adaptor grammars perform very well on the Brent corpus [1].\nHowever, [2] is mentioned briefly. As these other approaches work better for segmentation,\nthe authors should carefully justify why having a single model that does both language\nmodeling and word segmentation well matters. Many neural approaches have also been\nsuggested for Chinese word segmentation (among others [3]). In these papers, results on the\nPKU dataset are much better.  Are these directly comparable with yours?\n\nI would have liked a finer analysis of the impact of the length penalty.\nA plot showing how validation likelihood and segmentation performance vary as\n\\lambda is increased could potentially be interesting.\n\n[1] Johnson and Goldwater. \"Improving nonparameteric Bayesian inference: experiments on\nunsupervised word segmentation with adaptor grammars\", HLT, 2009\n\n[2] Berg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\", NAACL, 2010\n\n[3] Yang et al. Yang, Jie, Yue Zhang, and Fei Dong. \"Neural Word Segmentation with Rich Pretraining\", ACL, 2017",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very little awareness on latest research on unsupervised word segmentation",
            "review": "This paper proposes a neural architecture for segmental language modeling\nthat enables unsupervised word discoveries. The architecture employes a \ntwo-stage architecture that a word might be a type, or a sequence of characters\nof its spellings. \nThis idea is basically similar to Nested Pitman-Yor language models \n(Mochihashi et al. 2009) and two-stage language models (Goldwater et al. 2011),\nbut the authors seem not to notice these previous work.\nExperimental results show some improvements on naive baselines, but clearly \nbelow the state-of-the-art in unsupervised word segmentation.\n\nAs noted above, the crucial drawback of this paper is that the authors are\ncompletely unaware of latest achievements on unsupervised word segmentation\nand discovery, rather than old, simplistic baselines such as Goldwater+ (2009,\nidea is based on Goldwater+ ACL 2006) or Berg-Kirkpatrick (2010).\nThe idea of using characters and words is already exploited in Mochihashi+\n(ACL 2009) in a nonparametric Bayesian framework; it has a better F1 than this\nwork by a large margin. Moreover, it is recently extended (Uchiumi+ TACL \n2015) to also include latent word categories as well as segmentations to yield \nthe state-of-the-art accuracies on F1=81.6 on PKU corpus, as compared to 73.1 \nin this paper. \nNote that they employ a prior distribution on segment lengths as a (mixture of) \nPoisson distributions or negative binomials whose parameters are \nautomatically learned during inference, as compared to a post-hoc regularization\nused in this paper.\n\nIn a Bayesian framework, interpolations between words and characters are\ntheoretically derived and quite carefully learned, and regularizations are\nautomatically adjusted. While neural architectures have some potentials \nto improve over them, current heuristic architectures that have lower \nperformance does not have any advantage over these methods, \nboth theoretically and empicially.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting research direction, novel contributions, and well-written paper",
            "review": "This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. \n\nOn the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. \n\nI think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed.\n\nThe validation and test dataset have been modified to remove \"samples\" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets.\n\nThe use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment \"doyou\" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison.\n\nSome minor comments:\n- A typo in the introduction \"... semi-Markov model. The the characters inside ...\".\n- Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented.\n- Is it possible to efficiently generate a sequence?\n\n[Updated after reconsidering other reviews]\nAlthough this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}