{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice idea with some interesting results but needs more work",
            "review": "OVERVIEW:\nThe paper presents an interesting approach to unsupervised (more accurately weakly supervised) object detection that uses motion as training cue. At a high level, the authors propose to learn what an object looks like from two input videos: (1) a positive example containing the object of interest undergoing some motion and (2) a negative example of the same background scene without the object. They use a spatial encoder network in conjunction with three losses (variation loss, slowness loss, presence loss) to learn the appearance of an object based on motion cues. The only supervision here is at the video level with a label indicating the positive example. Then, given a new video, they are able to detect the object under some nuisance factors. They show four experiments on (1) detection with static camera showing written text, (2) detection of a moving objects like a roomba, drone and toy car with a moving camera, (3) detection of multiple objects, and (4) comparison with baselines of object tracking and template matching.\n\nPROS:\n1. The authors are able to show object detection with video level labels solely using motion cues. This is an interesting idea which can have significant impact with the abundance of unlabeled data in the wild and where video level annotations are much cheaper than bounding box level or even image level annotations.\n2. The spatial encoding (not a contribution of this work) to represent object location is simple but powerful and I feel can be extended to foreground segmentation. The authors use it neatly with reasonable loss functions to formulate the detection problem in an interesting way.\n3. The experiments show qualitatively that their proposed approach works reasonably well under simple settings (distinct objects on uniform background).\n\nCONS:\nThis is a nice idea and some preliminary experiments show positive qualitative results. However, for a publication especially at ICLR, it needs more work. The below points are some negative aspects of the current submission but also things I feel the authors can work on in their next submission:\n1. The Problem Setup: I think the scenario where you have one video of a single moving object in the scene and another video of the same scene without the object is not common. That is not to say that there might be some practical applications where this kind of setup in natural (maybe drones?) but you haven't convinced me about it. It will work in your favor if you are able to convince the reader where this kind of a setup is easily present and data is easily available.\n2. Concern about optimization: You mention in the second paragraph of Sec. 3.3 some weird stuff happening during optimization and how you counter it. It feels very hand-wavy and you need to either (1) investigate this behavior further and resolve the problem or (2) provide a more convincing answer as to why your proposed solutions make sense.\n3. Lack of quantitative evaluation: The experimental evaluation shows qualitative performance. I think the paper will look better (and more mature) with some quantitative metrics, say percentage of frames where the object was detected correctly, AP scores using bounding box annotations on test videos, experiments on larger number of videos with some averaged metrics.\n4. Stronger baselines: This is a continuation of the previous point. The only quantitative evaluation the authors have is MSE loss on position, between their proposed approach and baselines of template matching & object tracking. In my opinion, these baselines are very weak. Seeing the example images/videos, I am almost tempted to say that video registration followed by background subtraction with median image might also work pretty well. An out-of-the-box object detection system, trained on COCO (that hasn't seen these categories) might still return \"good\" bounding boxes with bad category labels. Basically, convince the reader with some stronger experiments than just proof-of-concept (for this kind of idea).\n5. Lack of variety in data: The data looks simple with one or few objects on plain background. I would trust the algorithm more if you show that it works on harder data, say textured background (drone in sky/field, car on road), appearance changes (train on one car, test on another), viewpoint changes, etc. \n6. One Shot Learning: The problem setup sounds very close to one-shot learning where you are seeing a video of the object once before trying to learn it. Some discussion along this direction in related work might be needed.\n\nOVERALL:\nI am rejecting the paper in it's version right now. I feel this is an interesting idea and can be resubmitted (possibly to a Vision conference) with additional work. If this was to be a vision paper, I can see a new dataset introducing this task with your approach and a stronger baseline comparison.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak technical novelty and insight",
            "review": "**Summary of the paper**\n\nThe paper introduces a novel approach for unsupervised object detection based on motion cues from two separate videos: one with and one without the object of interest. The main novelty is the usage of the negative video example which enables unsupervised detection of objects by being robust to distractor objects in the scene as the algorithm is able to differentiate between interesting motion and distractor object motion. The method trains a convolutional network to output dense probability maps for the object location and optimizes for strong/weak detection in positive/negative video respectively along with smoothness and variation objectives. The authors evaluate the proposed method on self-collected video datasets and test detection on held-out videos from the same scene as well as generalization to detecting the same object in new scenes. They compare to template matching and tracking baselines and give suggestions for a possible application of their method as automated demonstration annotation tool for pick-and-place tasks.\n\n\n**Clarity**\n\nAbove average\n\n**Significance**\n\nBelow Average\n\n**Correctness**\n\nThe paper is technically correct.\n\n**Detailed comments**\n\n_Paper Strengths_\n\n- the idea to use a negative video example for unsupervised detection learning seems novel\n- the proposed method is simple and the needed data can be collected with widely available equipments\n- the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example\n- the authors collected real-world data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)\n- the authors compare to a number of non-learning approaches from open-source implementations (the reviewer cannot judge whether any relevant technique is missing)\n- the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes\n\n\n_Paper Weaknesses_\n\n- the authors clearly reduced the horizontal margins of the standard ICLR style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard ICLR style template upon update of their manuscript\n- the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. maker-based approaches could be employed for detecting the object\n- the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multi-object examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is self-occluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)\n- other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging non-training scenes with heavier clutter, non-seen lighting changes and occlusions to support their robustness claim\n- the proposed method cannot operate in-the-wild (e.g. Youtube data) as it makes very strong assumptions about the required input data\n- the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene\n- the authors make no comparison to other unsupervised detection approaches (e.g. to the self-cited Jonschkowski et al. (2017)) to prove short-comings of other methods on the newly generated dataset\n- as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for ego-motion estimation) makes applications to non-video data impossible, also none of the experiments exploit the non-temporal property of the approach to show single frame detection on a more varied set of scenes\n- the experiment showcasing the proposed application to \"learning from demonstration\" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task\n- the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)\n- the reviewer cannot follow the references to object-centric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an object-centric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion\n- the \"random search optimization\" discussed in section 3.3 is not a valid method as it \"solves\" this problem of instable training by picking the best of N runs with varying random seeds\n- Figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach\n\n\n_Reproducibility_\n\n- Given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work.\n\n_Conclusion_\n\n- Overall, the reviewer appreciates the effort that went into the work but sees considerable need for improvement concerning the motivation and possible applications given the strong assumptions that are made about the input data. To truly position the paper against other detection/tracking algorithms more experiments are needed that show zero-shot (without retraining) generalization to substantially different scenes, or even architectural changes to be able to handle novel objects too. If the authors want to motivate the method as an automated tool for demonstration annotation an actual example of policy learning from unsupervisedly annotated sequences would strengthen the argument. The proposed method does not provide considerable technical novelty or insight to compensate for the lack in motivation. In the current state the paper is not convincing enough to warrant acceptance.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "using videos without objects as negative examples to detect objects",
            "review": "The paper introduces a system that uses positive videos (with the objects of interest in motion) and negative videos (devoid of those objects but the same backgrounds) to detect those objects of interest. The motivation in clear and explanation and flow of the paper are good. The technique will allow building object detectors with less labeling effort.\n\nHowever, the labeling of the first frame in many previous tracking and detection techniques can be compared to collecting a negative video (i.e. the agent needs to make sure that the negative video does not have that or related object in the negative video) and so both these approaches are similar. The current strategy can't by itself detect the absence of the object and hence there is supervision needed there.\n\nOne other important aspect I think was missing was the details of the training and test sets. How many training videos and test videos was this tried on? How different were the test videos? Did the test videos have objects similar to the training videos? Did the test videos have different external condition like lighting etc than the training videos? How similar were the backgrounds in these videos?\n\nPrevious approaches like TLD also used positive and negative samples (though from the same video) and track the object with view and lighting changes. When compared to the baseline, why did the current approach do better than these approaches (since both these methods and using a similar strategy of using positive and negative samples).\n\nHow does the system handle videos with different objects moving in the video with similar physical constraints? When does the system break i.e. what happens if objects are moving too fast, the camera motion or panning is not as expected? How would the manually set weight parameters need to be changed? How reliable are these parameters to new objects, far away objects etc?\n\nHow is variation or slowness being computed reliably without use of optical flow or other techniques? i.e. won't other objects due to camera motion negatively affect this?\n\nIn section 3.3, how do you detect if it has fully converged or close to it for the multiple runs especially since the ground truth is missing?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}