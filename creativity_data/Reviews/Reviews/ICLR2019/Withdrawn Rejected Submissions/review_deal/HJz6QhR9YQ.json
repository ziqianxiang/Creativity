{
    "Decision": "",
    "Reviews": [
        {
            "title": "A weak evaluation on an otherwise interesting task",
            "review": "The authors propose a method to learn a hierarchical reinforcement learning algorithm whose objective is to exploit weaknesses in its adversary, though also having weaknesses, itself. In order to encourage exploitation and not simply focus on winning the game, the agent is given a negative reward for every step taken during the game.\n\nThe evaluation was very unconvincing since FloBot was used in both training and testing. Also, there were no tests to demonstrate HASP was indeed exploiting weaknesses in FloBot. The frequency at which HASP selects sub-policies is not given any meaning. What are the situations in which these sub-policies are selected? How is the algorithm using them to exploit weaknesses?\n\nPros:\n- Generals.io is an interesting environment on which to test RL algorithms\n- Exploiting weaknesses instead of creating a generally powerful agent can be useful in situations where a generally powerful agent is difficult to obtain.\nCons:\n- Trained and tested on the same agent. While there may be cause for doing this in particular scenarios, the authors did not give any motivating examples.\n- No evidence is given that any weaknesses are being exploited\n- Lacks clarity\n  - Algorithm 1 was unclear\n  - How many times the achievement reward was used is not clear\n  - Only the reward is shown for Generals.io. How does this translate to win percentage?\n  - The tables were unclear, especially table 3",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Tackles an interesting problem, but fails to convincingly solve it",
            "review": "This paper proposes an algorithm for competitive games, based on the idea of learning then combining multiple sub-policies (each trained to counter a specific opponent). More precisely, in a first phase these sub-policies are trained in a sequential manner, each specialized by learning to beat one specific policy drawn from the pool of opponent policies, then being added to this pool afterwards. Once enough sub-policies have been obtained, a “meta” agent is trained by self-play, where at each time step the actions available to this agent are the trained sub-policies (i.e. the chosen sub-policy is queried to compute the actual move performed by the agent). Experiments on iterated (two-round) Rock-Paper-Scissor (RPS) show that the meta-agent ends up being less exploitable than agents trained directly with “conventional” self-play. Other experiments with the Generals.io strategy game confirm this finding, also showing the meta-agent’s superiority over “the strongest open-sourced scripted bot on the Generals.io online leaderboard” (named FloBot).\n\nAt high level, this paper tackles an interesting research direction, namely “exploring game-playing strategies that intentionally avoid the equilibrium solution and instead ‘learn to exploit’”, so as to reach “more capable, adaptable, and ultimately more human-like artificial agents”. The specific two-phase algorithm proposed here (consisting in first training specialized sub-policies sequentially, then a meta-agent to combine them), is new to the best of my knowledge. However, there exists previous work on training a meta-agent based on a set of sub-policies, and none of it is mentioned here. For instance, in the case of strategy games, the paper “Rock, Paper, StarCraft: Strategy Selection in Real-Time Strategy Games” and references within investigate several related approaches.\n\nUnfortunately, the paper is clearly not ready for publication in its current form, given the many presentation issues. Besides spelling and grammar errors, there are missing items (ex: citations, the t-SNE map mentioned in 4.2, what are “achievement rewards” and the associated “expand_X” agents in 4.3, ...). Overall the paper is somewhat confusing even if the proposed technique seems rather simple. Algorithm 1 in particular is not clear (e.g. a single pi^i is sampled? What does “Update theta to theta’ of Player 1” mean, especially considering that both players play with the same shared parameters theta according to the previous line? How exactly would you solve Phase 2 with Linear Programming in the general case?). There are also zero details on experiments (network architectures, hyper-parameters, etc.) — at least the authors promise to share the code, but it is not available yet.\n\nMore fundamentally, I feel like the proposed technique does not live up to the expectations raised by the introduction. This research is motivated by how humans are able to adapt to varied opponents so as to learn and exploit them. However, here, even if we train varied “exploitative” strategies, the end result is a meta-agent that does not try to adapt to its opponent, instead its goal is to combine the sub-strategies to (ideally) reach the Nash equilibrium. As a result, the benefits of learning sub-policies in the first place becomes somewhat unclear. If we consider the RPS experiments, I wonder why the proposed technique (dubbed HASP) is able to reach a strategy near the Nash equilibrium while “conventional” self-play (which by the way is not described here) is not. Indeed, since each sub-policy is specialized to beat a single opponent, it should be close to deterministic, in which case choosing a sub-policy (the action of the meta-agent) should essentially be the same as directly choosing a move. The authors mention they had trouble with PPO converging too fast to deterministic strategies, but why is it not happening with HASP? Also, note that it can be helpful to train an agent against old copies of itself to stabilize training in self-play (see e.g. “Emergent Complexity via Multi-Agent Competition”, or “Beating the World's Best at Super Smash Bros. Melee with Deep Reinforcement Learning”): this does not seem to be done here, and it might be one reason why “conventional” self-play is not working well. Finally (as far as RPS is concerned) iterating on two rounds does not seem very “interesting” in the sense that there is little opportunity to adapt to your opponent over only two rounds (if that was the goal). In the Generals.io game I can better see how training sub-policies may be beneficial, since there are fewer policies than moves available (thus the action space is reduced compared to directly training an agent by self-play). However the paper does not explicitly mention this point, and in my opinion does not convincingly show that this is indeed helping — especially due to my doubts about how the “conventional” self-play agent has been trained (note also that the random ensemble works as well as HASP, and that the conventional self-play agent performs much better than HASP against FloBot, and that there are no direct head-to-head match statistics between these agents, all of which raises some flags).\n\nAnother high-level concern I have regarding the methodology is that the “sub-policy discovery” mechanism (first phase of the algorithm) seems pretty basic. If for instance strategy B beats A, C beats B, and A beats C, the algorithm may cycle between these three strategies without ever identifying other ones that may exist: there is no explicit mechanism encouraging novelty (or detecting such cycles to know when to stop: as far as I can tell the number of sub-policies being trained is a hyper-parameter that must be set by hand). The “coverage” of the space of strategies the sub-policies can beat may thus be lacking, depending on the specific application. This limitation is not discussed at all in the paper.\n\nRegarding more specific details, there are too many small presentation issues for me to list them all, but here are a few representative samples:\n- Please use “tile” or “cell” to denote individual elements in the grid (not “grid” itself)\n- It is unclear how states are represented as inputs to the model for Generals.io\n- The 77% win rate against FloBot seems to be an important achievement of the paper since it is mentioned in the abstract and introduction... but not in experiments (!)\n- It is not clear what exactly are the numbers in Table 4. A win rate would also be good to add there.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ideas look interesting but paper is not finished.",
            "review": "This paper proposes a two-level hierarchical RL algorithm for games. First, a set of diverse policies is learned. These are supposed to be different possible (and potentially suboptimal) strategies. Then, a higher-level policy is trained to select the lower-level policy to be played. The approach is evaluated on rock-paper-scissors and a partially observable game.\n\nOn the positive side, I think the idea of building a set of strategies and trying to adapt the strategies picked based on the observation is an interesting idea. I also think the chosen game looks like a good experimental setting that allows to experiment quickly.\n\nHowever, I feel that the paper is currently too far for publication to consider acceptance.\n\nI was not able to follow the description of the algorithm. Sections 3.2 and 3.3 describe the general idea of the algorithm but I could not find a sufficient definition of some key notions. For example and unless I missed something, section 3.2 introduces the term “achievement reward” but contains neither a description nor an example. Furthermore, I could not really follow the Counter Self-Play algorithm and I am not sure I could reimplement it. Having a pseudo-algorithm would greatly help. It could go in an appendix if space is a concern. I assume Algorithm 1 describes the algorithm of section 3.3 as they have the same title but as Algorithm 1 is not mentioned in the text before section 4.3 I am not sure. I would also suggest introducing the notation of the algorithm in the body of the text. I also checked the paper website whose link is provided in the paper but it seemed to be empty. In any case, I think the paper should be self-contained. I also had trouble following the experimental section 4.3 Generals.IO.\n\nThere are also a few missing references in the paper, for example for the PPO algorithm, or in page 6. There seem to be a missing figure, I could not find what the following sentence refers to \"We provide a t-SNE projection of the learned policies onto the 2D plane below\".\n\nFor the reasons listed above it is a bit hard to comment on the content of the paper. Nevertheless, I would have liked to see how the proposed approach adapts to the policy the opponent is playing, for example by showing a difference in the distribution of the sub-policies selected. \n\nIt would also be interesting to compare the strategies learned to other methods learning a diverse set of strategies.",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}