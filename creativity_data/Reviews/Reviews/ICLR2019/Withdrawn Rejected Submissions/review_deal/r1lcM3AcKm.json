{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary of paper: Builds off work from Trinh et al. (2018) that proposed a semi-supervising learning model that combined the main task with an auxiliary task of predicting tokens in the input sequence. This paper proposes improving this two-task setup by splitting the hidden representations into two subsets, one for both tasks and one for only the main task. They compare against the baseline of Trinh et al. (2018), and find small improvements in performance.\n\nAt it's core, this paper is simply trying to improve multi-task learning. Despite the heavy focus on the work of Trinh et al. (2018),  fundamentally, what the paper is trying to do is: given multiple tasks, figure out how to effectively model the tasks such that information is shared but the tasks don't harm each other. In other words, the authors mark this paper as a semi-supervised learning paper, but it is more of standard multi-task learning setup.\n\nHowever, multi-task learning (MTL) is never once mentioned in the paper. This is surprising given that there has been a wealth of literature devoted to tackling exactly this type of problem. For example, the usage of different representations for different tasks is a standard trick in MTL (multiple heads). The model is also reminiscent of Progressive Networks [1], with one representation feeding to other tasks. Unfortunately, the authors do not cite any relevant MTL work, which is a big omission. There are also no other MTL baselines in the experiments section.\n\nEven when ignoring the previous issue, the gains from the proposed method are consistently small, and explainable by noise. Furthermore, in the one experiment that there seems be a nontrivial difference (CIFAR), Table 4 shows that the model is extremely sensitive to the sharing percentage (going from 50% to 30% drops accuracy by 2.1%).\n\nTo conclude, the authors miss critical discussion and comparison of prior works in MTL, and the experimental results are unconvincing.\n\n[1] Progressive Neural Networks (https://arxiv.org/abs/1606.04671)",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review ",
            "review": "Summary: \nTraining RNNs on long-sequences is a challenging task as gradients tend to explode or vanish. One way to mitigate this problem to some extent is to use semi-supervised learning in which objective function consists of unsupervised and supervised loss functions. Even though the contribution of unsupervised loss function can be controlled by a coefficient in the objective function, the unsupervised loss term can cause important information about the supervised task to be degraded or erased. This paper proposed a method to mitigate the problem of training of RNNs on long sequences by coordinating supervised and unsupervised loss functions. More specifically, they use two RNNs in which RNNs have a shared feature space for both supervised and unsupervised tasks and allow one of RNN to have a private space dedicated for the supervised task. \n\nStrengths:\n+ The idea of dividing the hidden states into two parts is interesting as it helps the model to control the effect of unsupervised loss on main supervised task.\n+ Shared and private spaces visualization are very informative. \n+ This model shows better results on MNIST and CIFAR-10 in compared with previous methods.\nWeaknesses:\n- Paper writing is good until section 3.1. This section is very confusing. I read it multiple times until understood what is happening. Lots of details are missing in sections 3.2 about how this model forces to not mix up the gradients for shared and private hidden units.\n- There are quite similarities between Trinh et al., 2018 and this paper. The only main difference is dividing the hidden state into shared and private ones. \n- Is there any reason why StanfordDogs and DBpedia are not used in this paper? Given the close relationship between Trinh et al., 2018 and this paper, it would have been better to have some results for these sets.\n- The paper claims that their model trains and evaluates faster. Rather than an argument about fewer parameters for auxiliary tasks, I don't see any justification. Fewer parameters don't necessarily lead to faster training or test time.\n\nComments and Questions\n- Is vanilla RNN used for the experiments? GRU is mentioned but my understanding is that it is only used for auxiliary loss.\n- There should be some detail about model architectures and training e.g. hidden units size, learning rate, dropout if any, etc. \n- It mentions that the model uses different time-scale updating operations of distinct RNNs with different representational spaces, I don't see how. Can you elaborate? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This work needs further analysis on a proper dataset.",
            "review": "This paper introduces a recurrent neural network architecture that minimizes both supervised and unsupervised losses for sequence learning. \n\nThe intuition behind of this work makes sense that the mixing gradients problem would degrade the performance of RNNs on sequence learning tasks. However, the experimental results are not convincing although quite big improvements on CIFAR10 are observed. Particularly, I am not entirely convinced by the authors' argument that adding auxiliary loss is helpful for addressing long-term dependencies. What I can see from Table 2 is that the performance of the proposed method in terms of accuracy increases as the ratio of shared parameters increases. In my opinion, these results can be also interpreted as a regularization effect. \nDatasets like MNIST and CIFAR can be used to show that the proposed idea is working as a proof of concept, but it might be difficult to draw a generalized conclusion from the results in the paper like the use of unsupervised loss during training helps.\nIn my opinion, neural machine translation datasets might be an option if the authors would like to claim the use of an unsupervised loss is helpful to capture better long-range dependencies.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}