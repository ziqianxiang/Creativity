{
    "Decision": {
        "metareview": "The authors have presented an empirical study of generalization and regularization in DQN. They evaluate generalization on different variants of Atari games and show that dropout and L2 regularization are beneficial. The paper does not contain any major revelations, nor does it propose new algorithms or approaches, but it is a well-written and clear demonstration, and it would be interesting to the deep RL community. However, the reviewers did not feel that the paper met the bar for publication at ICLR because the experiments were not more comprehensive, which would be expected for an empirical study. The AC will side with the reviewers but hopes that the authors will expand their study and resubmit to another venue in the future.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Not unknown but nice systematic exploration",
            "review": "I totally disagree with the authors that any of their observations are surprising. Indeed the fact that an RL agent does not generalizes to small modifications of the task (either visual or in the dynamics) is well known. If the agent should generalize though is a different question. And I do not mean this in the sense that it is an undesirable property but rather if it is outside of what “learning one task” means. Particularly I feel this is a very pessimistic view of RL and potentially not even in-line with what happens in supervised learning. \n\nI think one mantra of deep learning (and deep RL needs to obey by it) is that one should test in the same setting as training for things to truly work. For supervised, there is a distribution of data, and the test set are samples from the same distribution. However the testing scenario used here is slightly different. During training, if I do not see car accelerating, I think it makes no sense to expect to generalize to a new game that has this property as it is out-of-distribution. Of course it would be ideal if it could do that. And to clarify, while for us some of these extensions seem very similar and minimal changes, hence it should generalize to rather than transfer to, this is just the effect of imposing our own biases on the learning process. Deep Nets do not learn like we do, and in their universe they have never seen a car accelerating -- it makes sense that it might not to be able to generalize to it. Again, I’m not arguing that we don’t want this, but rather if we should expect it as part of what the system should normally generalize to.\n\nTo that end I think this paper enters in that unresolved dispute of what generalization should be versus what is transfer. At what point do we have truly a new task vs a sample from the same task. I don’t think there is an answer.\n\nGoing back to the observations in this work. I think the fact that the environment is not stochastic reinforces this overfitting (as in the extreme you end up with a policy that just repeats the optimal sequence of actions). I think in this particular case I can see how finetuning to a variation of the task fails. However true stochasticity in the environment (e.g. having a distribution of variations) like is done in Distral paper (where each episode is a different layout) can behave as a regularizer that will mitigate a bit the overfitting. That is to say that I believe the observed behaviour will be less pronounced in complex stochastic setting. \n\nNevertheless the paper seems to highlight an important observation (and back it up with empirical evidence), namely we should use more regularization like L2 or otherwise in practice. Which is mostly absent from publications. And I think this on its own is valuable. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting study but need more experiments. ",
            "review": "Summary: \nThis paper focuses on a \"generalization\" in deep Q-network (DQN). Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment. More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode.  For the experiments, 4 games: FREEWAY, HERO, BREAKOUT, and SPACE INVADERS which have 13 flavours (combinations of a mode and a difficulty) are used.\n\nStrengths:\n+ This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original.\n+ The experiments show that using REGULARIZED FINE-TUNING and FINE-TUNING for a new flavour /mode can help with sample efficiency compared with the models which are trained from scratch [10M frames vs 50M frames and 50M frames vs 100M frames ](Table 3).\n+ The paper is well-written and it can be easily followed.\n\nWeaknesses:\n- In my view, there should be experiments in which the proposed method is compared with other approaches that improve generalization in deep RL like Zhang et al., 2018 and Justesen et al., 2018.\n- According to the paper (at least my understanding) DQN's hyper-parameters are tuned based on default mode/flavour environment. It is possible that results for 'SCRATCH' results (Table 3) can be improved if DQN's hyper-parameters are tuned on the current flavour not default one.\n-The proposed method is only applicable when the default environment and the new one are very similar. If the environments are that similar why even bother to train first on default and then generalize to the new one? Wouldn't be less expensive to just focus on the target environment and find the best model on that?\n\nQuestions:\n- It is mentioned in the paper, that evaluation protocol suggested by Machado et al. (2018) is being followed in this paper. Have you followed the protocol introduced in section 4.2 of Machado et al. (2018)? If yes, the protocol in Machado et al. (2018) is for training time but numbers in Table 1 in this paper are for evaluation time. Can you elaborate?\n- Why only those 6 games were selected for the experiments? \n\nIn summary, I found this paper is interesting but my concern is about the experiments.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Empirical Paper on Evaluating Generalization Properties with Regularized / Non-Regularized DQN",
            "review": "This is an empirical study on the ability for DQNs trained with/without regularization to perform well on variants of the same environment (e.g. increasing difficulty of a game). The paper is well written, the experimental methodology is clear & sound, and the significance is around improved sample efficiency via warm starting from a regularized DQN to fine tune. The error bounds for the regularized models results seem uncomfortably large in some cases. Overall it looks like a good methodological paper that can inform others on taking regularization more seriously when training DQNs. Evaluating on a modified ALE environment is great, but it would have been better to see this having similar impact in real life applications.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}