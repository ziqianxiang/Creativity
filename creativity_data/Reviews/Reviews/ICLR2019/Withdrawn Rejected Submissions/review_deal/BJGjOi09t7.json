{
    "Decision": {
        "metareview": "The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non-negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method.\nAll the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE-NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Somewhat incremental and Missing NMF baselines"
    },
    "Reviews": [
        {
            "title": "lacking motivation and comparisons",
            "review": "The paper is generally well-written (lacking details in some sections though). My main criticism is about the lack of motivation for nonnegative VAE and lack of comparison with NMF.\n\nComments:\n- the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.\n- the paper missed important references to nonnegative auto-encoders, in particular:\nhttps://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf\n- the review of probabilistic NMF works is limited, see e.g.,\nhttps://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf\n- more details are needed about inference in Section 2.4\n\nMinor comments:\n- the notations z and h are sometimes confusing, what about using h every where ?\n- itâ€™s not clear to me how the first term in (1) is equal to the second term in (2",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Experimental evaluations are mostly qualitative",
            "review": "This paper replaces the Gaussian latent variables in standard VAE with the Weibull distribution and therefore presents a VAE solution to nonnegative matrix factorization, under squared Euclidean distance loss function. The literature review summarizes four related work very well. The adopted Weibull distribution provides a tractable inverse CDF function and analytical form of the KL divergence, facilitating VAE inference. In particular, the effects of the entropy term are discussed in detail.  Experiments illustrate the generated data from the model,  the learned part-based dictionaries, and the distributions of latent variables from similar data points.  \n\nQuestions: \n\n1. What is the updating rule for W_f? Is it multiplicative?  In Sec 2.4, The value of W_f is kept to be nonnegative by \"setting negative terms to zero\". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps? \n\n2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?  \n\n3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect? \n\nMinor: \nIn some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well written, interesting new idea, modest technical contribution, limited demonstration.",
            "review": "TITLE\nA VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION\n\nREVIEW SUMMARY\n\nWell written, interesting new idea, modest technical contribution, limited demonstration.\n\nPAPER SUMMARY\n\nThe paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. \n\nQUALITY\n\nThe work appears technically sound except for minor typos. \n\nCLARITY\n\nOverall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.\n\nORIGINALITY\n\nThe method is (to my knowledge) novel. \n\nSIGNIFICANCE\n\nI think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. \n\nFURTHER COMMENTS\n\npage 3\n\nThe presentation of the VAE objective is a bit oblique. The statement \"they require a different objectiv function\" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)\n\n\"with \\hat v  the reconstructed vector\" Not clear. I assume \\hat v is reconstructed from a sample from q given v ?\n\nThere is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\\lambda_2)^(k_2)\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}