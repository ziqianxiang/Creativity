{
    "Decision": {
        "metareview": "The reviewers raised a number of major concerns including lack of explanations, lack of baseline comparisons, and lack of discussion on pros and cons of  the main contribution of this work --  the presented Temporal Gaussian Mixture (TGM) layer. The authors’ rebuttal addressed some of the reviewers’ comments but failed to address all concerns (especially when it comes to the success of TGMs; it remains unclear whether this could be attributed solely to the way TGMs are applied rather than to their fundamental methodological advantage). Having said that, I cannot suggest this paper for presentation at ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "An Official Review: Temporal Gaussian Mixture Layer for Videos",
            "review": "This paper presents Temporal Gaussian Mixture (TGM) layer, efficiently capturing longer-term temporal dependencies with smaller number of parameters. The authors apply this layer to the activity recognition problem, claiming state-of-the-art performance compared against several baselines.\n\nStrong points of this paper:\n - The authors clearly described the proposed layer and model step by step, first explaining TGM layer, followed by single layer model, then generalizing it to multi-layer model.\n - The authors achieved state-of-the-art performance on multiTHUMOS dataset. The results look great in two aspects: the highest MAP scores shown in Table 1, and significantly smaller number of parameters shown in Table 2 to achieve the MAP scores.\n\nQuestions:\n - Basically, the idea in this paper is proposing to parameterize conv layers with Gaussian mixtures, with multiple pairs of mean and variance. Although Gaussian mixtures are powerful to model many cases, it might not be always to perfectly model some datasets. If a dataset is highly multi-modal, Gaussian mixture model also needs to have large M (number of mixture components). It is not clear how the authors decided hyper-paramter M, so it will be nicer for authors to comment the effect of different M, on various dataset/task if possible.\n - Same for the hyper-parameter L, the temporal duration. It will be nicer to have some experiments with varied L, and to discuss how much this model is sensitive to the hyper-parameter.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "TGMs are an interesting idea but need better justification and comparisons to standard 1D convolutions",
            "review": "Summary:\nThis paper proposes a temporal convolution layer called Temporal Gaussian Mixture (TGM) for video classification tasks. Usually, video classification models can be decomposed into a feature extraction step, in which frames are processed separately or in local groups, and a classification step where labels are assigned considering the full video features/longer range dependencies. Typical choices for the classification step are Recurrent Neural Networks (RNNs) and variations, pooling layers and 1D temporal convolutions. TGMs are 1D temporal convolutions with weights defined by taking samples from a mixture of Gaussians. This allows to define large convolutional kernels with a reduced number of parameters -the mean and std of the gaussians- at the cost of having reduced expressiveness. The authors experiment replacing 1D temporal convolutions with TGMs in standard video classifications models and datasets and report state-of-the-art results.\n\nStrengths:\n[+] The idea of defining weights as samples from a Mixture of Gaussians is interesting. It allows to define convolutional kernels that scale with the number of channels (equivalent to the number of Gaussians) instead of the number of channels and receptive field.\n\nWeaknesses:\n[-] The explanation of the TGM layers is very confusing in its current state. \n\nCertain aspects of TGMs that would help understand them are not clearly stated in the text. For example, it is not clear that 1) TGMs are actually restricted 1D convolutions as the kernel is expressed through a mixture of gaussians, 2) In a TGM the number of mixtures corresponds to the number of output channels in a standard 1D convolution and 3) despite TGMs are 1D convolutions, they are applied differently by keeping the number of frame features (output channels of a CNN) constant through them. Section 3 does not explain any of these points clearly and induces confusion.\n\n[-] The comparisons in the experimental section are unfair for the baselines and do not justify the advantages of TGMs. \n\nTGMs are 1D convolutions but they are applied to a different axis - 1D convolutions take as input channels the frame features. Instead, TGMs add a dummy axis, keep the number of frame features constant through them, and finally use a 1x1 convolution to map the resulting tensor to the desired number of classes. There is no reason why TGMs can’t be used as standard 1D convolutions taking as the first input channels the frame features - in other words, consider as many input mixture components as frame features for the first TGM. This is a crucial difference because the number of parameters of TGMs and the baselines is greatly affected by it.\n\nThe implicit argument in the paper is that TGMs perform better than standard 1D convolutions because they have less parameters. However, in the experiments they compare to 1D convolutions that have smaller temporal ranges – TGMs have a temporal kernel of 30 timesteps or 10 per layer (in a stack of 3 TGMs), whereas the 1D convolutions either have 5 or 10 timestep kernels according to section 4. Thus, it is impossible to know whether TGMs work better because 1) in these experiments they have longer temporal range – we would need a comparison to 1D convolutions applied in the same way and with the same temporal range to clarify it, 2) because they are applied differently – we would need a comparison when TGMs are used the same way as 1D convolutions, 3) because the reduced number of parameters leads to less overfitting – we would need to see training metrics and see the generalization gap when compared to equivalent 1D convolutions or 4) because the reduced number of parameters eases the optimization process. To sum up, it's true that TGMs would have a reduced number of parameters compared to the equivalent 1D convolutions with the same temporal range, but how does that translate to better results and, in this case, why is the comparison made with non-equivalent 1D convolutions? What would happen if the authors compared to 1D convolutions used in the same way as TGMs?\n\n[-] To claim SOTA, there are comparisons missing to recently published methods.\nIn particular, [1] and [2] are well-known models that report metrics for the Charades dataset also used in this paper.\n\nRating:\nAt this moment I believe the TGM layer, while a good idea with potential, is not sufficiently well explained in the paper and is not properly compared to other baselines. Thus, I encourage the authors to perform the following changes:\n\n-Rewrite section 3, better comparing to 1D convolutions and justifying why TGMs are not used in the same manner but instead using a dummy axis and a linear/1x1 conv layer at the end and what are the repercussions of it.\n-Show a fair comparison to 1D convolutions as explained above.\n-If claiming SOTA results, compare to [1] or [2] which use different methods to the one proposed in these paper but outperform the baselines used in the experiments.\n\n[1] Wang, Xiaolong, et al. \"Non-local neural networks.\" CVPR 2018\n[2] Zhou, Bolei et al. \"Temporal Relational Reasoning in Videos\" ECCV 2018\n\n\n----------------------\nUpdate: In light of the authors' rebuttal I have updated my rating from 5 to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, need more justification on the learned features by TGM",
            "review": "This paper introduces a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. The kernels of the new layer are controlled by a set of (temporal) Gaussian distribution parameters, which significantly reduce learnable parameters. The results are complete on four benchmarks and show consistent improvement. I just have minor comments. \n\n1. I am curious what the learned feature look like. As the author mentioned, \"The motivation is to make each temporal Gaussian distribution specify (temporally) ‘where to look’ with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features.\" So does the paper achieve this goal? \n\nAnother thing is, can the authors extract the features after TGM layer, and maybe perform action recognition on UCF101 to see if the feature really works? I just want to see some results or visualizations to have an idea of what TGM is learning. \n\n2. How long can the method actually handle? like hundreds of frames? Since the goal of the paper is to capture long term temporal information. \n\n3. It would be interesting to see an application to streaming video. For example, surveillance monitoring of human activities. \n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}