{
    "Decision": {
        "metareview": "The current version of the paper receives a unanimous rejection from reviewers, as the final proposal. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Unanimous rejection."
    },
    "Reviews": [
        {
            "title": "I think the method proposed in this paper might be reasonable. But I do not suggest acceptance, unless the author can improve the writing and include more experimental results.",
            "review": "In this paper, the authors propose a method for pruning the convolutional filters. This method first separates the filters into clusters based on similarities defined with both Activation Maximization (AM) and back-propagation gradients. Then pruning is conducted based on the clustering results, and the contribution index that is calculated based on backward-propagation gradients. The proposed method is compared with a baseline method in the experiments. \n\nI consider the proposed method as novel, since I do not know any filter pruning methods that adopt a similar strategy. Based on my understanding of the proposed method, it might be useful in convolutional filter pruning.\n\nIt seems that \"interpretable\" might not be the most proper word to summarize the method. It looks like that the key concept of this paper, including smilarity defined in Equation (3), and the contribution index defined in Equation (7) are not directly relevant to interpretability. Therefore, I would consider change the title of the paper, for example, to \"Convolutional Filter Pruning Based on Functionality \". \n\nIn terms of writing, I have difficulty understanding some details about the method. \n\nIn filter clustering, how can one run k-means based on pair-wise similarity matrix $S_D$? Do you run kernel k-means, or you  apply PCA to $S_D$ before k-means? What is the criterion of choosing the number of clusters in the process of grid search? \n\nAre filter level pruning, are cluster level pruning and layer level pruning three pruning strategies in the algorithm? It seems to me that you just apply one pruning strategy based on the clusters and contribution index, as shown in Figure 3. \n\nIn the subsubsection \"Cluster Level Pruning\", by \"cluster volume size\", denoted with$length(C^l_c)$, do you mean the size of cluster, i.e., the number of elements in each cluster? This is the first time I see the term \"volume size\". I assume the adaptive pruning rate, denoted by $R_{clt}^{(c,l)}$, is a fraction. But it looks to me that $length(C^l_c)$ is an integer. So how can it be true that $R_{clt}^{(c,l)} = length(C^l_c)$?\n\nIn the subsubsection \"Layer Level Pruning\", how is the value of $r$ determined?\n\nThe authors have conducted several experiments. These experiments help me understand the advantages of the proposed method. However, in the experiments, the proposed method is compared to only one baseline method. In recent years, a large number of convolutional filter pruning methods have been proposed, as mentioned in the related work section. I am not convinced that the proposed method is one of the best methods among all these existing methods. I would suggest the authors provide more experimental comparison, or explain why comparing with these existing methods is irrelevant. \n\nSince the proposed method is heuristic, I would also like the authors to illustrate that each component of the method is important, via experiment. How would the performance of the proposed method be affected, if we define the similarity $S_D$ in Equation (3) using only $V$ or $\\gamma$, rather than both $V$ and $\\gamma$? How would the performance of the proposed method be affected, if we prune randomly, rather than prune based on the contribution index?\n\nIn summary, I think the method proposed in this paper might be reasonable. But I do not suggest acceptance, unless the author can improve the writing and include more experimental results.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea, but needs some improvements.",
            "review": "This paper proposes a new method to prune filters of convolutional nets based on a metric which consider functional similarities between filters. Those similarities are computed based on Activation Maximization and gradient information. The proposed method is better than L1 and activation-based methods in terms of accuracy after pruning at the same pruning ratio. The visualization of pruned filters (Fig. 3) shows the effectiveness of the method intuitively. \n\nOverall, the idea in the paper is pretty intuitive and makes sense. The experimental results support the ideas. I think this paper could be accepted if it is improved on the followings:\n\n1. The paper is not very easy to read although the idea is simple.  \n\nThe equations could be updated and simplified. For example, I'm not sure if S_D in Eq. (3) wants to take V(F_i^(c,l)) and V(F_k^(c,l)) as the arguments. Layer L_l could be just l. \n\nAlgorithm 1 is hard to read. At least, one line should correspond to one processing. k is not initialized. It is difficult to understand what each variable represents.\n\nThe terms used in Section 4.2 may not be very accurate. First of all, I'm not sure if it is a hierarchical method. It does not perform pruning at multiple levels such as filters, clusters, and layers. Rather, it considers information from multiple levels to determine if a filter should be pruned or not. In that sense, everything is filter level pruning and distinguishing (filter|cluster|layer) level pruning just confuse readers. I'd recommend to simplify the section and describe simply what you do.\n\n2. Comparisons with more recent papers\n\nThe proposed method was compared with methods from 2015 and 2016. Model compression is an active area of research and there are a lot of papers. Probably, it makes sense to compare the proposed method against some state-of-the-art methods. Especially, it is interesting to see comparisons against methods with direct optimization of loss function such as (Liu et al. ICCV 2017). We might not need to even consider functionality with such methods.\n\nLiu et al. ICCV 2017: https://arxiv.org/pdf/1708.06519.pdf\n\n\n* Some other thoughts\n\n** If you look at Figure 3 (a), it looks that there are still a lot of redundant filters. Actually, except the last row, I'm not sure if we can visually find any important difference between (a) and (b). I wonder if the most important thing is that you do not prune unique filters (ones which are not clustered with others). It might be interesting to see a result of the L1-based pruning which does not prunes such filters. If you see an interesting result from that, it could add some value to the paper.\n\n** I'd recommend another proofread.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "No comparisons and claiming something known make it hard to accept this paper",
            "review": "This paper claims to have shown some insights about the filters in a neural network. However, it has little contributions that are justifiable to be published and it missed way too many references.\n\nThe visualization of filters is hardly any contribution over [1]. The claim that AM is the best visualization tool is a weird statement given that there are many recent references on visualization, such as [2-4], which the authors all missed.\n\nThe proposed filter pruning is a simplistic approach that bears little technical novelty, and there has been zero comparison against any filter pruning approach/network compression approach, among the cited references and numerous references that the paper didn't cite, e.g. [5-6]. In this form I cannot accept this paper.\n\n[1] D Bau, B Zhou, A Khosla, A Oliva, and A Torralba. Network Dissection: Quantifying the Intepretability of Deep Visual Representations. In CVPR 2017.\n[2] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh Dhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. ICCV 2017\n[3] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff. Top-down Neural Attention by Excitation Backprop. ECCV 2016\n[4] Ruth Fong and Andrea Vedaldi. Interpretable Explanations of Black Box Algorithms by Meaningful Perturbation. ICCV 2017\n[5] Y. Guo, A. Yao and Y. Chen. Dynamic Network Surgery for Efficient DNNs. NIPS 2016\n[6] T.-J. Yang, Y.-H. Chen, V. Sze. Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning. CVPR 2017",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}