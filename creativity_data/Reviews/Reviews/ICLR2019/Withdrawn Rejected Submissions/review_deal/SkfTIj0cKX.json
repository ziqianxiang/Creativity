{
    "Decision": {
        "metareview": "This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective. The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences. The problem suffers from delayed and sparse rewards, which the authors propose to address using self-supervised prediction. The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge.\n\nThe reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice. The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted. R1 also commends the authors' decision to address the challenging cold-start problem.\n\nThe reviewers and AC also note several potential weaknesses. The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated. This is needed, as many supervised learning (and other types) approaches to the problem exist. A performance comparison to current state-of-the-art RL baselines is missing. The proposed approach is related to both imagination augmented (I2A, Racaniere et al. 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al. 2016), but does not compare to either method. Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches. A thorough comparison to these baselines in a real-world application like session-based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess. Reviewers also noted lack of clarity. Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it's conceptual and empirical differences from existing reinforcement learning approaches. R3 mentions missing related work, some of which the authors include in the revision. The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem.\n\nOverall, the paper was assessed as borderline by the reviewers. The ACs view is that there are too many concerns for acceptance at ICLR in the present form, and that the paper will benefit from a thorough revision.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "important application area, not sufficiently placed in the context of prior work (both conceptually and empirically)"
    },
    "Reviews": [
        {
            "title": "The motivations of applying reinforcement learning to recommendation systems are not very convinced. Theory contributions may not be very significant",
            "review": "The paper aimed at improving the performance of recommendation systems via reinforcement learning. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; (2) the trajectory manager (TM) that determines how to roll out the IC under the planning strategy and produces a set of imagined item trajectories; (3) the imagination-augmented executor (IAE) that aggregates the internal data resulting from imagination and external rewarding data to update its action policy.\n\nStrengths of the paper:\n(1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people’s daily lives. \n(2) Experiments were conducted on a publicly available dataset. \n(3) Robustness to cold-start scenario was tested and evaluated in the experiments.\n\nWeaknesses of the paper:\n(1) The motivations of applying reinforcement learning techniques are not convinced to me. There are a lot of supervised learning algorithms to the task of recommendations. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? Is it because reinforcement learning based methods work better than traditional machine learning based ones? The motivations of integrating A3C (Asynchronous Advantage Actor-Critic) but not other techniques into the proposed model are not convinced to me as well. \n(2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines.\n(3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. In equations (2) and (3), what is theta_v? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)?\n(4) The contributions of the paper in terms of theory are somewhat not significant. It seems that the proposed algorithm is built based on and combined by existing algorithms such as A3C. \n\nMinor comments:\n(1) It would be better if the authors can test the proposed model on more datasets. There are many publicly available datasets for testing the performance of recommendation systems.\n(2) Figure 2 is not straightforward. It would be better if the authors can draw the figure in other ways. (I am not sure if the authors have expressed the underlying ideas clearly with Figure 2).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The main idea of the paper was very interesting, but the clarity of the paper needs to be improved significantly",
            "review": "The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. The proposed algorithm with an innovative IRN architecture was intriguing. \n\nThe writing of the paper was not very clear and pretty hard to follow. With this level of clarity, I don’t think it’s easy for other people to reproduce the results in this paper, especially in section 4, where I expect more details about the description of the proposed new architecture. Even though the author has promised to release their implementation upon acceptance, I still think the paper needs a major change to make the proposed algorithm more accessible and easier for reproduce.\n\nSome examples:\nWhat is L_A3C in “L = L_A3C + L_IRN” in the first paragraph of session 4? It looks like a loss from a previous paper, but it’s kind hard to track what it is exactly.\n\n“where Tj,τ is the τ-th imagined item, φ(·) is the input encoder shared by π (for joint feature learning), AE is the autoencoder that reconstructs the input feature, and the discounting factor γ is used to mimic Bellman type operations. … Therefore, we use the one-hot transformation as φ(·) and replace AE with the policy π (excluding the final softmax function), and only back-propagate errors of non-zero entries.”\nThis seems one of the most important components of the proposed algorithm, but I found it’s very hard to understanding what is done here exactly.\n\nRegardless the sketchy description of the algorithm, the empirical results look good, with comprehensive baseline methods for comparison. It’s interesting to see the comparison between different reward function. Maybe the author can also discuss on the impact of the new imagination module on the training time.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "I consider the proposed method interesting, although it is somewhat incremental. There are some conceptual issues with the proposed approach as well as missing related work. Motivation could be improved. The empirical evaluation is strong. ",
            "review": "Summary:\n\nThe paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.\n\n\nComments:\n\nThe proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. However, the motivation of using RL is missing from the technical contribution. Considering a deterministic policy, using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. \n\nThe motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. \n\nThe literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein).\n\nEmpirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. \n\n\nQuestions:\n\n-How are cold-start situations encountered if items are one-hot encoded?\n-Why is there a strong focus on quick adaptation to user sessions? Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests.\n\n\nMinor:\n\n-Proofreading is necessary\n-Table 1 and 2 would be more readable if they were figures\n-Figure 3 seems to be taken from Tensorflow runtime convergence plots, which could be dropped given the limited space",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}