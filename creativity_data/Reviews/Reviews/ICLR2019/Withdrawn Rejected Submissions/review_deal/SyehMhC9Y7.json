{
    "Decision": {
        "metareview": "This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing.  \n\nAs is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers.  Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors' disagreement. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "RL methods anno 2018"
    },
    "Reviews": [
        {
            "title": "This paper combines Imitation Learning (IL) and Model Base Reinforcement Learning (RL) to come up with a novel algorithm that can take in user-defined targets while maintaining expert like behaviors.   This  promising approach that combines the benefits of IL and RL but with result performed only in simulation.",
            "review": "- Does the paper present substantively new ideas or explore an under explored or highly novel\nquestion? \n\nYes, the paper combines two frameworks (Imitation Learning and Model Base\nReinforcement Learning) to incorporate target information while fitting to the expert distribution. Maybe, the idea is novel but experiments are only in simulation. \n\n- Does the results substantively advance the state of the art?\n\n No, the compared methods are not state-of-the-art.\n\n-  Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\n Yes a substantial fraction of ICLR attendees might be interested in reading the paper.\n\n - would I send this paper to one of my colleagues to read?\n\nYes. \n\n\n- Quality: \n\nThe key point of this paper is that the proposed algorithm is novel and combines\nthe advantages of Imitation Learning and Model Base Reinforcement Learning. However, the\nauthors do not address the problem of IL when the stochasticity in the environment and/or model\nresults in trajectories outside of expert’s distribution. Additionally, all experiments are done in\nsimulation only and comparisons are made against components of the proposed algorithm instead\nof the state-of-the-art.  This is definitely a limitation of the paper given recent works on imitation learning and model predictive control  as applied to real robotic systems in the task of agile off-road visual navigation. \n\nIn addition, the paper does not provide any detail on the training procedure (Network architecture, cost\nfunction, etc), which makes results hard to reproduce. In addition, the experiments only compare\nthe proposed algorithm to its components, namely proportional controller, IL only controller and\nModel Basel RL only controller.\n\n- Clarity: \n\nEasy to read. Thorough comparison with existing frameworks (Advantages compared to IL and model\nbased RL). \n\nOriginality: \n\n– Novel algorithm presented with success in simulation. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "[Update] Elegant probabilistic formulation with limited experimental validation",
            "review": "# Summary\n\nThis submission proposes a method to combine the benefits of model-based RL and Imitation Learning (IL) for navigation tasks. The key idea is to i) learn a prior over trajectory distributions from a fixed dataset of demonstrations, and ii) use this learned dynamical model for path planning via probabilistic inference. Reaching target waypoints is done by maximizing the trajectory likelihood conditioned on the planning goal. The prior is learned using R2P2 on LIDAR features and past positions. Experiments using the CARLA driving simulator show that this method can outperform standard control, IL, and model-based RL baselines, while flexibly incorporating test-time goals and costs thanks to its probabilistic formulation.\n\n\n# Strengths\n\nThe method is an elegant way to get the best of both worlds in RL and IL, leveraging the recent R2P2 work to estimate a powerful sequential model used for planning via probabilistic inference. The flexibility of the method in considering test-time cost maps and user-defined goals (e.g. to avoid potholes) is appealing, especially since it does not require on-policy data collection.\n\nThe proposed planning-as-inference method can in theory handle the multi-modality present in human demonstrations by using a probabilistic model of the observed behaviors as prior over undirected expert trajectories.\n\nThe approach seem to outperform both model-based and imitation learning baselines on a simplified version of the CARLA benchmark, including on interesting fine-grained metrics (e.g., comfort based).\n\n\n# Weaknesses\n\nThe main weakness of this submission lies in its experimental evaluation, especially the absence of any dynamic objects in the tested environment (\"static world CARLA\", section 1). It is unclear how this approach would generalize beyond just staying on the road. How would it handle traffic lights, pedestrians, other drivers, weather variations, and more complex driving tasks than waypoint following by traversing mostly free space? How does the prior generalize to more complex behaviors (e.g, by using more contextual information \\phi)? How robust is the method to noise in the demonstrations, i.e. non-expert or suboptimal behavior? It seems that estimating the generative prior on human behavior might suffer from the same issues as behavior cloning, e.g., the sample inefficiency due to the combinatorial explosion of causal factors explaining complex human behaviors. It might be in fact even harder to estimate that generative model than use a direct discriminative approach (e.g., a modular pipeline), at the cost of reduced flexibility at test time of course. The currently reported sample efficiency (7000 training samples) and near perfect success rate seem to suggest that this (non-standard) version of the CARLA benchmark is too simple (no weather variations, no dynamic obstacles). Comparison to the state of the art (beyond the baselines implemented here) on the original CARLA benchmark seems needed (especially in the \"Nav. dynamic\" task).\n\nThe method is only described very succinctly in section 2. I do not believe there are enough details (especially around the learning algorithm, hyper-parameters, and other important technical elements) for reproducibility at this stage. Section 2.1 is also quite dense for people not familiar with the R2P2 paper. As the main contribution of the paper is to leverage that model for planning and control, it would be great to maybe discuss a bit deeper. Finally, the input modalities are not clear, especially for the baselines: the proposed method is using LIDAR and localization whereas the IL baseline seems to use vision (while the others just use the trajectory). This makes the fairness of the comparison really unclear (LIDAR is a much stronger signal for just staying on the road).\n\nMinor remarks:\n- Why use a proportional controller as a baseline instead of the standard PID one?\n- Section 2.3 seems like it's missing the extension of equation 2 to the multi-goal case?\n- Typos in section 3 (\"trail-and-error\"), section 4 (\"autonmous\", \"knowledge to\")\n\n\n# Recommendation\n\nAlthough the theoretical benefits of the method are well-motivated and clear (off-policy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple CARLA test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work. First, adding more details as suggested above and clarifying the experimental protocol seem like a must, but can be easily addressed by an update to the text. Second, it would be ideal to evaluate the approach on the standard CARLA benchmark in order to compare fairly to the prior art. This is much more involved.\n\nI personally like the approach, so although I think it is marginally below the acceptance threshold in its current form, I reserve my judgement for the time being and look forward to the authors' reply.\n\n\n# Update\n\nThe submission has been drastically rewritten (the diff is massive) and I think it is in much better shape, answering some of my concerns around reproducibility and generalization. Furthermore, it reinforces the strengths of the approach (esp. around its flexibility).\n\nI am willing to recommend acceptance, but I have some further questions (hence I have only updated my score to a 6 for now). They are mostly related to the comparison with IL (important to validate the claim in the paper that the proposed approach is quantitatively better than both existing IL and RL methods). See discussion below for details.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper with detailed experiments, but the idea seems lacking novelty",
            "review": "Major Contribution:\nThe paper introduces a method that combines the advantage and of model-based RL and imitation learning and offset their weakness. The method proposes a probabilistic inference approach to analyze the action of the model.\n\nOrganization/Style:\nThe paper is well written, organized, and clear on most points. \n\nTechnical Accuracy:\nI'm not an expert in RL. The method is obscure to me, but from my point of view, the experiments are done quite thoroughly and the results look good.\n\nPresentation:\nGood. \n\nAdequacy of Citations: \nThe author should consider adding the related works include:\nBojarski, Mariusz, et al. \"End to end learning for self-driving cars.\": using CNNs to implement imitation learning for self-driving cars\n\nMultimedia:\nVideos are helpful to understand the method and are well composed.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}