Published as a conference paper at ICLR 2021
Collective Robustness Certificates:
Exploiting Interdependence in
Graph Neural Networks
Jan Schuchardt, Aleksandar Bojchevski, Johannes Gasteiger & Stephan Gunnemann
Technical University of Munich, Germany
{jan.schuchardt,bojchevs,j.gasteiger,guennemann}@in.tum.de
Ab stract
In tasks like node classification, image segmentation, and named-entity recogni-
tion we have a classifier that simultaneously outputs multiple predictions (a vector
of labels) based on a single input, i.e. a single graph, image, or document respec-
tively. Existing adversarial robustness certificates consider each prediction inde-
pendently and are thus overly pessimistic for such tasks. They implicitly assume
that an adversary can use different perturbed inputs to attack different predictions,
ignoring the fact that we have a single shared input. We propose the first col-
lective robustness certificate which computes the number of predictions that are
simultaneously guaranteed to remain stable under perturbation, i.e. cannot be at-
tacked. We focus on Graph Neural Networks and leverage their locality property
-perturbations only affect the predictions in a close neighborhood - to fuse mul-
tiple single-node certificates into a drastically stronger collective certificate. For
example, on the Citeseer dataset our collective certificate for node classification
increases the average number of certifiable feature perturbations from 7 to 351.
1	Introduction
Most classifiers are vulnerable to adversarial attacks (Akhtar & Mian, 2018; Hao-Chen et al., 2020).
Slight perturbations of the data are often sufficient to manipulate their predictions. Even in scenarios
where attackers are not present it is critical to ensure that models are robust since data can be noisy,
incomplete, or anomalous. We study classifiers that collectively output many predictions based on a
single input. This includes node classification, link prediction, molecular property prediction, image
segmentation, part-of-speech tagging, named-entity recognition, and many other tasks.
Various techniques have been proposed to improve the adversarial robustness of such models. One
example is adversarial training (Goodfellow et al., 2015), which has been applied to part-of-speech
tagging (Han et al., 2020), semantic segmentation (Xu et al., 2020b) and node classification (Feng
et al., 2019). Graph-related tasks in particular have spawned a rich assortment of techniques. These
include Bayesian models (Feng et al., 2020), data-augmentation methods (Entezari et al., 2020) and
various robust network architectures (Zhu et al., 2019; Geisler et al., 2020). There are also robust
loss functions which either explicitly model an adversary trying to cause misclassifications (Zhou
& Vorobeychik, 2020) or use regularization terms derived from robustness certificates (ZUgner &
GUnnemann, 2019). Other methods try to detect adversarially perturbed graphs (Zhang et al., 2019;
Xu et al., 2020a) or directly correct perturbations using generative models (Zhang & Ma, 2020).
However, none of these techniques provide guarantees and they can only be evaluated based on
their ability to defend against known adversarial attacks. Once a technique is established, it may
subsequently be defeated using novel attacks (Carlini & Wagner, 2017). We are therefore interested
in deriving adversarial robustness certificates which provably guarantee that a model is robust.
In this work we focus on node classification.1 Here, the goal is to assign a label to each node in a
single (attributed) graph. Node classification can be the target of either local or global adversarial
attacks. Local attacks, such as Nettack (Zugner et al., 2018; Zugner et al., 2020), attempt to alter the
1While we focus on node classification, our approach can easily be applied to other multi-output classifiers.
1
Published as a conference paper at ICLR 2021
Figure 1: Previous certificates consider each node independently. Most nodes cannot be certified
since the adversary can choose a different perturbed graph per node (left). This is impossible in prac-
tice due to mutually exclusive perturbations. Our collective certificate enforces a single perturbed
graph (center). It aggregates the amount of perturbation within each receptive field and then evalu-
ates a single-node certificate to determine whether the corresponding prediction is robust (right).
e.
each Γ
row
encodes
the
receptive
field
of a node
prediction of a particular node in the graph. Global attacks, as proposed by Zugner & Gunnemann
(2019), attempt to alter the predictions of many nodes at once. With global attacks, the attacker is
constrained by the fact that all predictions are based on a single shared input. To successfully attack
some nodes the attacker might need to insert certain edges in the graph, while for another set of
nodes the same edges must not be inserted. With such mutually exclusive adversarial perturbations,
the attacker is forced to make a choice and can attack only one subset of nodes (see Fig. 1).
Existing certificates (Zugner & Gunnemann, 2019; BCjcheVski & Gunnemann, 2019; Bojchevski
et al., 2020) are designed for local attacks, i.e. to certify the predictions of individual nodes. So far,
there is no dedicated certificate for global attacks, i.e. to certify the predictions of many nodes at
once2. A naive certificate for global attacks can be constructed from existing single-node certifi-
cates as follows: One simply certifies each node’s prediction independently and counts how many
are guaranteed to be robust. This, however, implicitly assumes that an adversary can use different
perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input.
We propose a collective robustness certificate for global attacks that directly computes the number of
simultaneously certifiable nodes for which we can guarantee that their predictions will not change.
This certificate explicitly models that the attacker is limited to a single shared input and thus accounts
for the resulting mutual exclusivity of certain attacks. Specifically, we fuse multiple single-node
certificates, which we refer to as base certificates, into a drastically (and provably) stronger collective
one. Our approach is independent of how the base certificates are derived, and any improvements to
the base certificates directly translate to improvements to the collective certificate.
The key property which we exploit is locality. For example, in a k-layer message-passing graph neu-
ral network (Gilmer et al., 2017) the prediction for any given node depends only on the nodes in its
k-hop neighborhood. Similarly, the predicted segment for any pixel depends only on the pixels in its
receptive field, and the named-entity assigned to any word only depends on words in its surrounding.
For classifiers that satisfy locality, perturbations to one part of the graph do not affect all nodes.
Adversaries are thus faced with a budget allocation problem: It might be possible to attack differ-
ent subsets of nodes via perturbations to different subgraphs, but performing all perturbations at
once could exceed their adversarial budget. The naive approach discussed above ignores this, over-
estimating how many nodes can be attacked. We design a simple (mixed-integer) linear program
(LP) that enforces a single perturbed graph. It leverages locality by only considering the amount of
perturbation within each receptive field when evaluating the single-node certificates (see Fig. 1).
We evaluate our approach on different datasets and with different base certificates. We show that in-
corporating locality alone is sufficient to obtain significantly better results. Our proposed certificate:
•	Is the first collective certificate that explicitly models simultaneous attacks on multiple outputs.
•	Fuses individual certificates into a provably stronger certificate by explicitly modeling locality.
•	Is the first node classification certificate that can model not only global and local budgets, but also
the number of adversary-controlled nodes, regardless of whether the base certificates support this.
2Chiang et al. (2020) certify multi-object detection, but they still treat each detected object independently.
2
Published as a conference paper at ICLR 2021
2	Preliminaries
Data and models. We define our unperturbed data as an attributed graph G = (X , A) ∈ G with
G = {0, 1}N ×D ×{0, 1}N×N, consisting ofN D-dimensional feature vectors and a directed N × N
adjacency matrix. Each vertex is assigned one out of C classes by a multi-output classifier f : G 7→
{1, . . . , C}N. In the following, fn(G) = fn(X, A) = fn refers to the prediction for node n.
Collective threat model. Unlike previous certificates, we model an adversary that aims to change
multiple predictions at once. Let BG ⊆ G be a set of admissible perturbed graphs. Given a clean
graph G, the adversary tries to find a G0 ∈ BG that maximizes the number of misclassified nodes,
i.e. Pn∈T 1fn (G)6=fn (G0), for some set of target nodes T ⊆ {1, . . . ,N}.
Following prior work (Zugner & Gunnemann, 2019), We constrain the set of admissible perturbed
graphs BG through global and (optionally) local constraints on the number of changed bits. Our
global constraints are parameterized by scalars rXadd, rXdel , rAadd , rAdel ∈ N0. They are an upper
limit on how many bits can be added (0 → 1) or deleted (1 → 0) when perturbing X and A. Our
local constraints are parameterized by vectors rXadd,loc, rXdel,loc, rAadd,loc, rAdel,loc ∈ N0N. They
are an upper limit on how many bits can be added or deleted per row of X and A, i.e. how much
the attributes of a particular node can change and how many incident edges can be perturbed.
Often, itis reasonable to assume that no adversary has direct control over the entire graph. Instead, a
realistic attacker should only be able to perturb a small (adaptively chosen) subset of nodes. To model
this, we introduce an additional parameter σ ∈ N. For all (X0, A0) ∈ BG, there must be a set of node
indices S ⊆ {1, . . . , N} with |S| ≤ σ such that for all d ∈ {1, . . . , D} and n, m ∈ {1, . . . , N}:
(Xn,d = Xn,d =⇒ n ∈ S) ∧ (An,m = An,m =⇒ n ∈ S ∨ m ∈ S) .	(1)
The set S is not fixed, but chosen by the adversary.3 The resulting set BG is formally defined in
Section B. If the global budget parameters are variable and the remaining parameters are clear from
the context, we treat BG as a function BG : N40 7→ P(G) that, given global budget parameters, returns
the set of all perturbed graphs fulfilling the constraints.
Local predictions. Our certificate exploits the locality of predictions, i.e. the fact that predictions
are only based on a subset of the input data. We characterize the receptive field offn via an indicator
vector ψ(n) ∈ {0, 1}N corresponding to rows in attribute matrix X and an indicator matrix Ψ(n) ∈
{0, 1}N×N corresponding to entries in adjacency matrix A. For all (X0, A0), (X00, A00) ∈ BG:
ND	NN
XXψm(n)1Xm0,d6=Xm00,d+XXΨi(,nj)1A0i,j6=A0i0,j=0 =⇒ fn(X0,A0)=fn(X00,A00). (2)
Eq. 2 enforces that as long as all nodes and edges for which ψ(n) = 1 or Ψ(n) = 1 remain unper-
turbed, the prediction fn does not change. Put differently, changes to the rest of the data do not affect
the prediction. Note that the adversary can alter receptive fields, e.g. add edges to enlarge them. To
capture all potential alterations, ψ(n) and Ψ(n) correspond to all data points that influence fn under
some graph in BG, i.e. the union of all receptive fields achievable under the threat model.
3	Compact representation of Base certificates
Before deriving our collective certificate, we define a representation that allows us to efficiently
evaluate base certificates. A base certificate is any procedure that can provably guarantee that the
prediction fn for a specific node n cannot be changed by any perturbed graph in an admissible set,
such as sparsity-aware smoothing (Bojchevski et al., 2020). As you shall see in the next section,
our collective certificate requires evaluating base certificates for varying adversarial budgets within
L = [rXadd] × [rXdel] × [rAadd] × [rAdel] (with [k] = {0, . . . , k}), the set of vectors that do not
exceed the collective global budget.
A base certificate implicitly partitions L into a set of budgets K(n) ⊆ L for which the prediction f
is certifiably robust and its complement K(n) = L \ K(n) with
KS) ⊆ {ρ ∈ N0 | P ∈ L ∧ ∀(X0, A0) ∈ BG(P) : fn(X, A) = f (X0, A0)}.	⑶
3 Note that the adversary only needs to control one of the nodes incident to an edge in order to perturb it. We
do not associate different costs for perturbing different nodes or edges, but such an extension is straightforward.
3
Published as a conference paper at ICLR 2021
Note the subset relation. The set K(n) does not have to contain all budgets for which fn is robust.
Conversely, a certain budget vector ρ not being part of K(n) does not necessarily mean that fn can be
attacked under threat model BG(P) - its robustness is merely unknown. We now make the following
natural assumption about base certificates: If a classifier is certifiably robust to perturbations with a
large global budget, it should also be certifiably robust to perturbations with a smaller global budget.
∀P ∈ K(n), P0 ∈ L ： [∀d ∈ {1, 2, 3,4} : Pd ≤ Pd] =⇒ [ρ0 ∈ KS)] .	(4)
From a geometric point of view, Eq. 4 means that the budgets K for which the prediction fn is
certifiably robust form a singular enclosed volume around (0 0 0 0)T within the larger volume
L. Determining whether a classifier is robust to perturbations in BG (P) is equivalent to determining
which side of the surface enclosing the volume K the budget vector P lies on. This can be be done
by evaluating linear inequalities, as shown in the following.
First, let us assume that all but one of the budgets are zero, e.g. L = [rXadd] × [0] × [0] × [0], with
rXadd > 0. Due to Eq. 4 there must be a distinct value pn ∈ N0 (the smallest uncertifiable budget)
with ∀P ∈ L : P ∈ KS) 0 P1 ≥ pn . Evaluating the base certificate can thus be performed by
evaluating a single inequality. This approach can be generalized to arbitrary types of perturbations.
Instead of using a single scalar pn , we characterize the volume of budgets K(n) via the pareto front
of points on its enclosing surface:
PS) = {ρ ∈ Kn) | -∃ρ0 ∈ Kn) : P = ρ ∧∀d ∈{1, 2, 3,4} : Pd ≤ Pd} .	(5)
These points fulfill ∀ρ ∈ L (P ∈ K(n) ^⇒ ∃p ∈ P(n), ∀d ∈ {1, 2, 3,4} : Pd ≥ pd) . Here, eval-
uating the base certificate can be performed by evaluating 4|P| inequalities.
In the following, we assume that we are directly given this pareto front (or the smallest uncertifiable
budget). Finding the pareto front can be easily implemented via a flood-fill algorithm that identifies
the surface of volume K(n), followed by a thinning operation (for more details, see Section D).
4	Collective certificate
To improve clarity in this section, we only discuss the global budget constraints. All remaining
constraints from the threat model can be easily modelled as linear constraints. You can find the
Certificate for the full threat model in Section C. We first formalize the naive collective certificate
described in the introduction, which implicitly allows the adversary to use different graphs to attack
different predictions. We then derive the proposed collective certificate, first focusing on attribute
additions before extending it to arbitrary perturbations. We relax the certificate to a linear program
to enable fast computation and show the certificate’s tightness when using a randomized smoothing
base certificate. We conclude by discussing the certificate’s time complexity and limitations.
Naive collective certificate. Assume we are given a clean input (X, A), a multi-output classifier
f , a set T of target nodes and a set of admissible perturbed graphs BG fulfilling collective global
budget constraints given by rXadd, rXdel, rAadd, rAdel. Let L = [rXadd] × [rXdel] × [rAadd] × [rAdel]
be the set of all vectors that that do not exceed the collective global budget. Further assume that the
base certificate guarantees that each classifier fn is certifiable robust to perturbations within a set
of budgets K(n) (see Eq. 3). As discussed, the naive certificate simply counts the predictions whose
robustness to perturbations from BG is guaranteed by the base certificate. Using the representation
of base certificates introduced in Section 3, this can be expressed as Pn∈T 1 K(n) = L . From the
definition of K(n) in Eq. 3, we can directly see that this is a lower bound on the optimal value of
Pn∈T min(X0,A0)∈BG 1 [fn(X, A) = fn(X0, A0)], i.e. the number of predictions guaranteed to be
stable under attack. Note that each summand involves a different minimization problem, meaning
the adversary may use a different graph to attack each of the nodes.
Collective certificate for attribute additions. To improve upon the naive certificate, we want to
determine the number of predictions that are simultaneously robust to attacks with a single graph:
(X0mAi0n)∈B X1[fn(X,A)=fn(X0,A0)].	(6)
(X ,A )∈BG
4
Published as a conference paper at ICLR 2021
Solving this problem is usually not tractable. For simplicity, let us assume that the adversary is only
allowed to perform attribute additions. As before, we can lower-bound the indicator functions using
the base certificates:
min	b 0 0 0)T ∈
(X0,A0)∈BG n∈T
(7)
where b =	(n,d):X =0 Xn0 ,d is the number of attribute additions for a given perturbed graph.
Since this certificate only depends on the number of perturbations, it is sufficient to optimize over
the number of attribute additions while enforcing the global budget constraint:
bm∈iNn X 1 (b 0 0 0)T ∈ K(n)	s.t.b≤rXadd.
∈ 0 n∈T
(8)
There are two limitations: (1) The certificate does not account for locality, but simply considers the
number of perturbations in the entire graph. In this regard, it is no different from the naive collective
certificate; (2) Evaluating the indicator functions, i.e. certifying the individual nodes, might involve
complex optimization problems that are difficult to optimize through. We tackle (1) by evaluating
the base certificates locally.
Lemma 1 Assume multi-output classifier f, corresponding receptive field indicators ψ(n) ∈
{0, 1}N and Ψ(n) ∈ {0, 1}N×N, and a clean graph (X, A). Let K(n) be the set of certifiable
global budgets of prediction fn, as defined in Eq. 3. Let (X0 , A0) be a perturbed graph. Define
X00 ∈ {0, 1}N×D and A00 ∈ {0, 1}N×N as follows:
Xi0,0d = ψi(n)Xi0,d + (1 - ψi(n))Xi,d,	(9)
A0i0,j =Ψi(,nj)A0i,j+(1-Ψi(,nj))Ai,j,	(10)
i.e. use values from the clean graph for bits that are not in fn ’s receptive field. If there exists a vector
of budgets ρ ∈ N04 such that (X00, A00) ∈ BG (ρ) and ρ ∈ K(n), then fn(X, A) = fn (X0, A0).
See proof in Section B. Due to Lemma 1 we can ignore all perturbations outside fn ’s recep-
tive field when evaluating its base certificate. We can thus replace (b 0 0 0)T in Eq. 8 with
(bTψ(n) 0 0 0)T , where the vector b ∈ NN indicates the number of attribute additions at
each node. Optimizing over b yields a collective certificate that accounts for locality:
min X ”(bTψ(n) 0 0 0)T ∈ K叫 s.t. ∣∣b∣∣ι ≤ rXadd.	(11)
∈ 0 n∈T
We now tackle issue (2) by employing the compact representation of base certificates defined in
Section 3. Since we are only allowing one type of perturbation, the base certificate of each classifier
fn is characterized by the smallest uncertifiable radius pn (see Section 3). To evaluate the indicator
function in Eq. 11 we simply have to compare the number of perturbations in fn ’s receptive field to
pn , which can be implemented via the following MILP:
min
b∈N0N,t∈{0,1}N
|T| -	tn
n∈T
(12)
s	.t. bTψ(n) ≥ pntn ∀n ∈ {1, . . . , N}	(13)
||b||1 ≤ rXadd.	(14)
Eq. 14 ensures that the number of perturbations fulfills the global budget constraint. Eq. 13 ensures
that the indicator tn can only be set to 1 if the local perturbation on the l.h.s. exceeds or matches pn,
i.e. fn is not robustly certified by the base certificate. The adversary tries to minimize the number of
robustly certified predictions in T (see Eq. 8), which is equivalent to Eq. 12.
Collective certificate for arbitrary perturbations. Lemma 1 holds for arbitrary perturbations. We
only have to consider the perturbations within a prediction’s receptive field when evaluating its
base certificate. However, when multiple perturbation types are allowed, the base certificate of a
prediction fn is not characterized by a scalar pn, but by its pareto front P(n) (see Eq. 5). Let P (n) ∈
N0P( "×4 be a matrix encoding of the set P(n). To determine if fn is robust We can check whether
5
Published as a conference paper at ICLR 2021
there is some pareto-optimal point Pi(,n: ) such that the amount of perturbation in fn ’s receptive field
matches or exceeds Pi(,n: ) in all four dimensions. This can again be expressed as a MILP (see Eq. 15
to Eq. 23 below).
As before, we use a vector t with tn = 1 indicating that fn is not certified by the base certificate. The
adversary tries to find a budget allocation (parameterized by bXadd, bXdel and BA) that minimizes
the number of robustly certified predictions in T (see Eq. 15). Eq. 20 and Eq. 21 ensure that the
budget allocation is consistent with the global budget parameters characterizing BG . The value of
(n)
tn is determined by the following constraints: First, Eq. 17 to Eq. 19 ensure that Qp,d is only set
to 1 if the local perturbation matches or exceeds the pareto-optimal point corresponding to row p
of P (n) in dimension d. The constraints in Eq. 16 implement logic operations on Q(n) : Indicator
s(pn) can only be set to 1 if ∀d ∈ {1, 2, 3, 4} : Q(pn,d) = 1. Indicator tn can only be set to 1 if
∃p ∈ {1, . . . , |P(n) |} : s(pn) = 1. Combined, these constraints enforce that if tn = 1, there must be
some point in P(n) that is exceeded or matched by the amount of perturbation in all four dimensions.
min	|T|-Xtn	(15)
(Q(n) ,s(n))nN=1 ,bXadd ,bXdel ,BA,t	n∈T
s.t.	||s(n)||1 ≥tn, Q(pn,d) ≥ s(pn),	(16)
(bXadd)Tψ(n) ≥ Qi(,n1)Pp(,n1) ,	(bXdel)Tψ(n) ≥ Q(pn,2)Pp(,n2) ,	(17)
X (1 - Am,m0)(Ψ(n)	BA)m,m0 ≥Q(pn,3)Pp(,n3),	(18)
m,m0 ≤N
X Am,m0(Ψ(n)	BA)m,m0 ≥	Q(pn,4)Pp(,n4),	(19)
m,m0 ≤N
||bXadd ||1 ≤ rXadd,	||bXadd ||1	≤ rXdel,	(20)
BAi,j ≤rAadd,	BAi,j ≤rAdel,	(21)
(i,j):Ai,j =0	(i,j):Ai,j =1
s(n) ∈ {0,1}lP(n)1,	Q(n) ∈ {0, ι}lP(n)l×4	t ∈{0,1}N	(22)
bXadd, bXdel ∈N0N,	BA ∈ {0, 1}N×N.	(23)
LP-relaxation. For large graphs, finding an optimum to the mixed-integer problem is prohibitively
expensive. In practice, we relax integer variables to reals and binary variables to [0, 1]. Semantically,
the relaxation means that bits can be partially perturbed, nodes can be partially controlled by the
attacker and classifiers can be partially uncertified (i.e. 1 > tn > 0). The relaxation yields a linear
program, which can be solved much faster.
Tightness for randomized smoothing. One recent method for robustness certification is random-
ized smoothing (Cohen et al., 2019). In randomized smoothing, a (potentially non-deterministic)
base classifier h : X 7→ Y that maps from some input space X to a set of labels Y is transformed
into a smoothed classifier g(x) with g(x) = argmaxy∈Y Pr [h(φ(x) = y], where φ(x) is some
randomization scheme parameterized by input x. For the smoothed g(x) we can then derive proba-
bilistic robustness certificates. Randomized smoothing is a black-box method that only depends on
h’s expected output behavior under φ(x) and does not require any further assumptions. Building on
prior randomized smoothing work for discrete data by Lee et al. (2019), Bojchevski et al. (2020)
propose a smoothing distribution and corresponding certificate for graphs. Using their method as a
base certificate to our collective certificate, the resulting (non-relaxed) certificate is tight. That is,
our mixed-integer collective certificate is the best certificate we can obtain for the specified threat
model, if we do not use any information other than the classifier’s expected predictions and their
locality. Detailed explanation and proof in Section E.
Time complexity. For our method we need to construct the pareto fronts corresponding to each
prediction’s base certificate. This has to be performed only once and the results can then be reused
in evaluating the collective certificate with varying parameters. We discuss the details of this pre-
processing in Section D. The complexity of the collective certificate is based on the number of
constraints and variables of the underlying (MI)LP. In total, we have 13 PnN=1 |P(n) | + 8N + 2e + 5
6
Published as a conference paper at ICLR 2021
constraints and 5 PnN=1 |P(n) |+4N +e variables, where e are the number of edges in the unperturbed
graph (we disallow edge additions). For single-type perturbations we have O(N + e) terms, linear
in the number of nodes and edges. The relaxed LP takes at most a few seconds to certify robustness
for single-type perturbations and a few minutes for multiple types of perturbations (see Section 5).
Limitations. The proposed approach is designed to exploit locality. Without locality, it is equivalent
to a naive combination of base certificates that sums over perturbations in the entire graph. A non-
obvious limitation is that our notion of locality breaks down if the receptive fields are data-dependent
and can be arbitrarily extended by the adversary. Recall how we specified locality in Eq. 2: The
indicators ψ(n) and Ψ(n) correspond to the union of all achievable receptive fields. Take for example
a two-layer message-passing neural networks and an adversary that can add new edges. Each node
is classified based on its 2-hop neighborhood. For any two nodes n, m, the adversary can construct
a graph such that m is in fn receptive field. We thus have to treat the fn as global, even if for any
single graph they might only process some subgraph. Nonetheless, our method still yields significant
improvements for edge deletions and arbitrary attribute perturbations. As discussed in prior work
(Zui gner & Guinnemann, 2020) edge addition is inherently harder and less relevant in practice.
5 Experimental evaluation
Experimental setup. We evaluate the proposed approach by certifying node classifiers on multi-
ple graphs and with different base certificates. We use 20 nodes per class to construct a train and
a validation set. We certify all remaining nodes. We repeat each experiment five times with dif-
ferent random initializations and data splits. Unless otherwise specified, we do not impose any lo-
cal budget constraints or constraints on the number of attacker-controlled nodes. We compare the
proposed method with the naive collective certificate, which simply counts the number of predic-
tions that are certified to be robust by the base certificate. All experiments are based on the relaxed
linear programming version of the certificate. We assess the integrality gap to the mixed-integer
version in Section A. The code is publicly available under https://www.daml.in.tum.de/
collective-robustness/. We also uploaded the implementation as supplementary material.
Datasets, models and base certificates. We train and certify models on the following datasets:
Cora-ML (McCallum et al. (2000); Bojchevski & Guinnemann (2018); N = 2810, 7981 edges,
7 classes), Citeseer (Sen et al. (2008); N = 2110, 3668 edges, 6 classes), PubMed (Namata et al.
(2012); N = 19717, 44324 edges, 3 classes), Reuters-21578 4 (N = 862, 2586 edges, 4 classes) and
WebKB (Craven et al. (1998); N = 877, 2631 edges, 5 classes). The graphs for the natural language
corpora Reuters and WebKB are constructed using the procedure described in Zhou & Vorobeychik
(2020), resulting in 3-regular graphs. We use five types of classifiers: Graph convolution networks
(GCN)(KiPf & Welling, 2017), graph attention networks (GAr)(VeliCkOViC et al., 2018), APPNP
(Gasteiger et al., 2019), robust graph convolution networks (RGCN) (Zhu et al., 2019) and soft
medoid aggregation networks (SMA) (Geisler et al., 2020). All classifiers are configured to have
two layers, i.e. each node’s classifier is dependent on its two-hop neighborhood. We use two types
of base certificates: (Bojchevski et al., 2020) (randomized smoothing, arbitrary perturbations) and
Zuigner& Gui nnemann (2019) (convex relaxations of network nonlinearities, attribute perturbations).
We provide a summary of all hyperparameters in Section F.
Evaluation metrics. We report the certified ratio on the test set, i.e. the percentage of nodes that are
certifiably robust under a given threat model, averaged over all data splits. We further calculate the
standard sample deviation in certified ratio (visualized as shaded areas in plots) and the average wall-
clock time per collective certificate. For experiments in which only one global budget parameter is
altered, We report the average certifiable radius, i.e. r = (Er=O ω(r) * r/ Er=O ω(r)), where ω(r)
is the certified ratio for value r of the global budget parameter, averaged over all splits.
Attribute perturbations. We first evaluate the certificate for a single perturbation type. Using ran-
domized smoothing as the base certificate, we evaluate the certified ratio of GCN classifiers for
varying global attribute deletion budgets rXdel on the citation graphs Cora, Citeseer and PubMed
(for Reuters and WebKB, see Section A). rhe remaining global budget parameters are set to 0.
Fig. 2 shows that for all datasets, the proposed method yields significantly larger certified ratios than
the naive certificate and can certify robustness for much larger rXd&. The average certifiable radius r
4Distribution 1.0, available from http://www.daviddlewis.com/resources/testcollections/reuters21578
7
Published as a conference paper at ICLR 2021
3
10
2
10
1
10
Attribute deletions
Figure 3: Two-dimensional colective certificate
for smoothed GCN on Cora-ML under varying
rXdel and rAdel . The solid and dotted contour
lines show ratios ≥ 0.5 and ≥ 0.7 for our
vs. the naive certificate respectively. Our method
achieves much larger certified ratios and radii.
0 8 6 4 2 0
♦ ♦♦♦♦♦
Iooooo
J pəhijəo
0 8 6 4 2
-----
Ioooo
O
10

Attribute deletions
Figure 2: Certified ratios for smoothed GCN on
Cora, Citeseer and PubMed, under varying rXdel.
We compare the proposed certificate (solid lines)
to the naive certificate (dotted lines). Our method
certifies orders of magnitude larger radii (note the
logarithmic x-axis).
0 8 6 4 2
-----
Ioooo
J pəujəo
0.0
0	5	10	15	20
Attribute deletions
0.0
0 8 6 4 2
-----
Ioooo
∙J pəujəo
Figure 5: Certifying GCN on Citeseer, un-
der varying rXadd using Zui gner & Gui nnemann
(2019)’s base certificate. Our certificate yields
significantly larger certified ratios and radii.
Figure 4: Comparison of certified ratios for GAT,
GCN and APPNP on Cora-ML under varying
rXdeι for our (solid lines) and the naive (dotted
lines) collective certificate.
on Citeseer increases from 7.18 to 351.73. The results demonstrate the benefit of using the collective
certificate, which explicitly models simultaneous attacks on all predictions. The average wall-clock
time per certificate on Cora, Citeseer and PubMed is 2.0 s, 0.29 s and 336.41 s. Interestingly, the
base certificate yields the highest certifiable ratios on Cora, while the collective certificate yields the
highest certifiable ratios on PubMed. We attribute this to differences in graph structure, which are
explicitly taken into account by the proposed certification procedure.
Simultaneous attribute and graph perturbations. To evaluate the multi-dimensional version of
the certificate, we visualize the certified ratio of randomly smoothed GCN classifiers for different
combinations of rXdel and rAdel on Cora-ML. For an additional experiment on simultaneous at-
tribute additions and deletions, see Section A. Fig. 3 shows that we achieve high certified ratios even
when the attacker is allowed to perturb both the attributes and the structure. Comparing the contour
lines at 50 % the naive certiface can only certify much smaller radii, e.g. at most 6 attribute deletions
compared to 39 for our approach. The average wall-clock time per certificate is 106.90 s.
Different classifiers. Our method is agnostic towards classifier architectures, as long as they are
compatible with the base certificate and their receptive fields can be determined. In Fig. 4 we com-
pare the certified collective robustness of GAT, GCN, and APPNP, using the sparse smoothing cer-
tificate on Cora-ML.5 Better base certificates translate into better collective certificates. For an addi-
tional experiment on the benefits of robust classifier architectures RGCN and SMA, see Section A.
5Our method can certify larger radii, r ≥ 103 (see Fig. 2). Here we show r ≤ 20 to highlight the difference.
8
Published as a conference paper at ICLR 2021
O
♦
1
8 6 4 2
♦ ♦ ♦ ♦
Oooo
J pə^pjəo
0.0
0	200	400	600	800	1000
Edge deletions
Figure 6: Certified ratios for smoothed GCN
on Cora-ML, under varying rAdel and rAdel,loc .
Stricter local budgets yield larger certified ratios.
O
♦
1
8 6 4 2
♦ ♦ ♦ ♦
Oooo
J pəpəo
0.0
0	200	400	600	800	1000
Edge deletions
Figure 7: Certified ratios for smoothed GCN on
Cora-ML. We vary rXdel and σ. The certified ra-
tios remain constant and non-zero for large rAdel.
Different base certificates. Our method is also agnostic to the base certificate type. We show that it
works equally well with base certificates other than randomized smoothing. Specifically, we use the
method from Zugner & Gunnemann (2019). We certify a GCN model for varying rχadd on Cite-
seer. Unlike randomized smoothing, this base certificate models local budget constraints. Using the
default in the base certificate’s reference implementation, we limit the number of attribute additions
per node to b0.01Dc = 21 for both the base and the collective certificate. Fig. 5 shows that the pro-
posed collective certificate is again significantly stronger. The average certified radius ^ increases
from 17.12 to 971.36. The average wall-clock time per certificate is 0.39 s.
Local constraints. We evaluate the effect of additional constraints in our threat model. We can
enforce local budget constraints and limit the number of attacker-controlled nodes, even if they are
not explicitly modeled by the base certificate. In Fig. 6, we use a smoothed GCN on Cora-ML and
vary both the global budget for edge deletions, rAdel, and the local budgets rAdel,loc . Even though
the base certificate does not support local budget constraints, reducing the number of admissible
deletions per node increases the certified ratio as expected. For example, limiting the adversary to
one deletion per node more than doubles the certified ratio at rAdel = 1000. In Fig. 7, we fix a
relatively large local budget of 16 edge deletions per node (only 〜5% of nodes on Cora-ML have
a degree > 16) and vary the number of attacker-controlled nodes. We see that for any given number
of attacker nodes, there is some point rAdel after which the certified ratio curve becomes constant.
This constant value is an an upper limit on the number of classifiers that can be attacked with a given
local budget and number of attacker-controlled nodes. It is independent of the global budget.
6	Conclusion
We propose the first collective robustness certificate. Assuming predictions based on a single shared
input, we leverage the fact that an adversary must use a single adversarial example to attack all
predictions. We focus on Graph Neural Networks, whose locality guarantees that perturbations to
the input graph only affect predictions in a close neighborhood. The proposed method combines
many weak base certificates into a provably stronger collective certificate. It is agnostic towards
network architectures and base certification procedures. We evaluate it on multiple semi-supervised
node classification datasets with different classifier architectures and base certificates. Our empirical
results show that the proposed collective approach yields much stronger certificates than existing
methods, which assume that an adversary can attack predictions independently with different graphs.
7	Acknowledgements
This research was supported by the German Research Foundation, Emmy Noether grant GU 1409/2-
1, the German Federal Ministry of Education and Research (BMBF), grant no. 01IS18036B, and the
TUM International Graduate School of Science and Engineering (IGSSE), GSC 81.
9
Published as a conference paper at ICLR 2021
References
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:
A survey. IEEEAccess, 6:14410-14430, 2018.
Aleksandar Bojchevski and StePhan Gunnemann. Deep gaussian embedding of graphs: UnsUPer-
vised inductive learning via ranking. In International Conference on Learning Representations,
2018.
Aleksandar BojchevSki and Stephan GUnnemann. Certifiable robustness to graph perturbations. In
Advances in Neural Information Processing Systems, volume 32, pp. 8319-8330, 2019.
Aleksandar Bojchevski, Johannes Gasteiger, and Stephan Gunnemann. Efficient robustness certifi-
cates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 1003-1013, 2020.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Workshop on Artificial Intelligence and Security, AISec, 2017.
Ping-yeh Chiang, Michael J. Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, and Tom
Goldstein. Detection as regression: Certified object detection by median smoothing. In Advances
in Neural Information Processing Systems, volume 33, pp. 1275-1286, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 1310-1320, 2019.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, and
Sean Slattery. Learning to extract symbolic knowledge from the world wide web. In Proceedings
of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of
Artificial Intelligence, AAAI ’98/IAAI ’98, pp. 509-516. American Association for Artificial
Intelligence, 1998.
Negin Entezari, Saba A. Al-Sayouri, Amirali Darvishzadeh, and Evangelos E. Papalexakis. All you
need is low (rank): Defending against adversarial attacks on graphs. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp. 169-177, 2020.
Boyuan Feng, Yuke Wang, Zheng Wang, and Yufei Ding. Uncertainty-aware attention graph neural
network for defending adversarial attacks. arXiv preprint arXiv:2009.10235, 2020.
Fuli Feng, Xiangnan He, Jie Tang, and Tat-Seng Chua. Graph adversarial training: Dynamically
regularizing based on graph structure. IEEE Transactions on Knowledge and Data Engineering,
2019.
Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019.
Simon Geisler, Daniel Zugner, and Stephan Gunnemann. Reliable graph neural networks via robust
aggregation. In Advances in Neural Information Processing Systems, volume 33, pp. 13272-
13284, 2020.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263-1272,
2017.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Wenjuan Han, Liwen Zhang, Yong Jiang, and Kewei Tu. Adversarial attack and defense of structured
prediction models. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 2327-2338, 2020.
10
Published as a conference paper at ICLR 2021
Han Xu Yao Ma Hao-Chen, Liu Debayan Deb, Hui Liu Ji-Liang Tang Anil, and K Jain. Adversarial
attacks and defenses in images, graphs and text: A review. International Journal of Automation
and Computing,17(2):151-178, 2020.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. In Advances in Neural Information Processing
Systems 32, pp. 4910-4921. Curran Associates, Inc., 2019.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163,
2000.
Galileo Mark Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven active surveying
for collective classification. In Workshop on Mining and Learning with Graphs, 2012.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI Magazine, 29(3):93, 2008.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks. In International Conference on Learning Representations,
2018.
X XU, Y YU, L Song, C LiU, B KailkhUra, C GUnter, and B Li. Edog: Adversarial edge detection for
graph neUral networks. Technical report, Lawrence Livermore National Lab.(LLNL), Livermore,
CA (United States), 2020a.
Xiaogang XU, HengshUang Zhao, and Jiaya Jia. Dynamic divide-and-conqUer adversarial training
for robUst semantic segmentation. arXiv preprint arXiv:2003.06555, 2020b.
Ao Zhang and Jinwen Ma. Defensevgae: Defending against adversarial attacks on graph data via a
variational graph aUtoencoder. arXiv preprint arXiv:2006.08900, 2020.
YingxUe Zhang, Sakif Hossain Khan, and Mark Coates. Comparing and detecting adversarial at-
tacks for graph deep learning. In Representation Learning on Graphs and Manifolds Workshop,
International Conference on Learning Representations, 2019.
Kai ZhoU and Yevgeniy Vorobeychik. RobUst collective classification against strUctUral attacks. In
Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI), volUme 124
of Proceedings of Machine Learning Research, pp. 250-259, 2020.
DingyUan ZhU, Ziwei Zhang, Peng CUi, and WenwU ZhU. RobUst graph convolUtional networks
against adversarial attacks. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1399-1407, 2019.
Daniel ZUgner and StePhan GUnnemann. Certifiable robustness and robust training for graph con-
volUtional networks. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 2019.
Daniel ZUgner and Stephan GUnnemann. Certifiable robUstness of graph ConvolUtional networks
Under strUctUre pertUrbations. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1656-1665, 2020.
Daniel ZUgner, Amir Akbarnejad, and Stephan GUnnemann. Adversarial attacks on neUral networks
for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2847-2856, 2018.
Daniel ZUgner, Oliver Borchert, Amir Akbarnejad, and Stephan GUnnemann. Adversarial attacks
on graph neUral networks: PertUrbations and their patterns. ACM Transactions on Knowledge
Discovery from Data, 14(5), 2020.
11
Published as a conference paper at ICLR 2021
A	Additional experiments
Robust architectures. Our comparison of different standard classifier architectures demonstrated
that the proposed collective certificate is architecture-agnostic and that better base certificates trans-
late into better collective certificates. In Fig. 8 we assess the benefit of using SMA and RGCN, both
of which are robust architectures meant to improve adversarial robustness. We use GCN as a baseline
for comparison and evaluate the respective certified ratios for varying attribute deletion budgets on
Cora-ML. While RGCN is supposed to be more robust to adversarial attacks, it has a lower certified
ratio than GCN. Soft medoid aggregation on the other hand has a significantly higher certified ratio.
Its base certificate is almost as strong as the collective certificate of RGCN. Its collective certified
ratio at rXdel = 21 is 88.3%, compared to the 76.9% and 74% of GCN and RGCN.
0 8 6 4 2
-----
Ioooo
J pəujəo
0.0
0	5	10	15	20
Attribute deletions
0 8 6 4 2
-----
Ioooo
∙J pətpjəo
0.0
0	100	101	102	103
Attribute deletions
Figure 8:	Certified ratios for smoothed GCN,
RGCN and soft medoid aggregation on Cora-ML
under varying rXdel for our (solid lines) and the
naive (dotted lines) collective certificate.
Figure 9:	Certified ratios for smoothed GCN on
WebKB and Reuters-21578, under varying rXdel
for our (solid lines) and the naive (dotted lines)
collective certificate.
Attribute perturbations on additional datasets. In addition to citation graphs, we also use graphs
constructed from the Reuters-21578 and WebKB natural language corpora to evaluate the proposed
certificate. As in the main experiments section, we use randomized smoothing as a base certificate
for GCN classifiers and assess the certified ratio for varying global attribute deletion budgets. Fig. 9
shows that the certified ratio increases and that much larger rXdel (up to approximately 103) can be
certified when using the collective approach. The average certifiable radius for Reuters and WebKB
increases from 6.54 and 8.08 to 265.62 and 309.32, respectively. With less than 900 nodes each,
both datasets are smaller than our three citation graphs. This leads to even shorter average wall-
clock times per certificate: 0.105 s and 0.116 s.
Simultaneous attribute deletions and additions. In the main experiments section, we applied our
collective certificate to simultaneous certification of attribute and adjacency deletions. Here, we as-
sess how it performs for simultaneous deletions and additions of attributes. We again use a randomly
smoothed GCN classifier on Cora-ML, and perform collective certification for different combina-
tions of rXadd and rXdel on Cora-ML. As shown in Fig. 3, the collective certificate is again much
stronger than the naive collective certificate. For example, We obtain certified ratios between 30%
and 60% at radii for which the naive collective certificate cannot certify any robustness at all. The
average wall-clock time per certificate is 40.51 s.
Integrality gap. For all previous experiments, we used the relaxed linear programming version of
the certificate to reduce the compute time. To assess the integrality gap (i.e. the difference between
the mixed-integer linear programming and the linear programming based certificates), we apply
both versions of the certificate to a single smoothed GCN on Cora. We certify both robustness to
attribute deletions (Fig. 11a) and edge deletions (Fig. 11b). The wall-clock time per certificate for
the MILP increased from 0.24 s to 64h with increasing edge deletion budget (0.35 s to 94 h for
attribute deletions). Due to the exploding runtime for the MILP, we cannot compute the integrality
gap for radii larger than 8 and 12, respectively.6 The integrality gap is small (at most 4% for attribute
deletions, 4.3% for edge deletions), relative to the certified ratio, and appears tobe slightly increasing
with increasing global budget.
6As discussed in the main paper the relaxed LP is fast and efficient to solve, even for radii larger than 1000.
12
Published as a conference paper at ICLR 2021
0.0
0
9 : EZ I。
SUosPPE ωsqxfalv
(a) Proposed collective certificate
Figure 10: Comparison of the proposed collective certificate (Fig. 10a) to the naive collective cer-
tificate (Fig. 10b) for certification of smoothed GCN on Cora-ML, under varying rXadd and rXdel .
Our method achieves much larger certified ratios for all combinations of attack radii.
0 8 6 4 2
-----
Ioooo
J pəujəo
2	4	6	8
Attribute deletions
0 2 4 6 8 10 12 14 16 18 20
Attribute deletions
-1.00
-0.75
-0.50
-0.25
-0.00
(b) Naive collective certificate
0.0
0
0 8 6 4 2
-----
Ioooo
∙J pəujəo
2	4	6	8	10	12
Edge deletions
(a)
(b)
Figure 11: Certified ratios for smoothed GCN on Cora, under varying rXdel (Fig. 11a) and
rAdel (F ig. 11b), using the mixed-integer collective certificate (blue line) and the relaxed linear
programming certificate (orange line). The integrality gap is small, relative to the certified ratio.
B Formal definition of threat model parameters and proofs
Here we define formally the set of admissible perturbed graphs BG described in Section 2. Recall that
we have an unperturbed graph (X,A) ∈ G, global budget parameters rXadd, rXdel, rAadd, rAdel ∈
N0, local budget parameters rXadd,loc, rXdel,loc, rAadd,loc, rAdel,loc ∈ N0N and at most σ adversary-
controlled nodes. Given these parameters, the set of admissible perturbed graphs BG is defined as
follows:
(X0, A0) ∈ BG =⇒
∣{ (n, d) : Xn,d = 0 = Xn,d} I ≤ rXadd ∧ I {(n, d) : Xn,d = 1 = Xn,d} I ≤ rXdei
∧ l{(n, m) : An,m = 0 = An,m } 1 ≤ rAadd ∧ | {(n, m) : An,m = 1 = An,m } 1 ≤ rAdei
∧∀n ∈ {1, . . . , N } :
II{d : Xn,d = 0 6= Xn0 ,d}II ≤ rXadd,ioc n ∧ II{d : Xn,d = 1 6= Xn0 ,d}II ≤ rXdei,ioc n (24)
∧ II{m : An,m = 0 6= Xn0 ,m}II ≤ rAadd,ioc n ∧ II{m : An,m = 1 6= Xn0 ,m}II ≤ rAdei,ioc n
∧ ∃S ⊆ {1, . . . , N} : |S| ≤ σ ∧ ∀d ∈ {1, . . . , D}, n, m ∈ {1, . . . , N} :
(Xn,d = Xn,d =⇒ n ∈ S) ∧ (4冲=An,m =⇒ n ∈ S ∨ m ∈ S)))
Next, we show the proof of Lemma 1 delegated from the main paper.
13
Published as a conference paper at ICLR 2021
Proof: By the definition of receptive fields (Eq. 2) changes outside the receptive field do not influence
the prediction, i.e. fn (X0, A0) = fn(X00, A00). Since (X00, A00) ∈ BG (ρ) and ρ ∈ K(n) , we know
thatfn(X,A) =fn(X00,A00).Bytransitivityfn(X,A) =fn(X0,A0) .
C Full collective certificate
Here we discuss how to incorporate local budget constraints and constraints on the number of
attacker-controlled nodes into the collective certificate for global budget constraints (see Eq. 15).
We also discuss how to adapt it to undirected adjacency matrices (see Section C.1) As before, we
lower-bound the true objective
X0mAi0n B X1[fn(X,A)=fn(X0,A0)].	(25)
(X ,A )∈BG n∈T
by replacing the indicator functions with evaluations of the corresponding base certificates and opti-
mizing over the number of perturbations per node / the perturbed edges. The only difference is that
more constraints are imposed on BG .
The derivation proceeds as before: Lemma 1 still holds, meaning we only have to consider pertur-
bations within fn ’s receptive field in evaluating whether its robustness is guaranteed by the base
certificate. The base certificates can be efficiently evaluated by comparing the perturbation within
fn’s receptive field to all points in the pareto front P(n) characterizing the volume of budgets K(n) .
After encoding P(n) as a matrix P(n) ∈ N|0P(n) ×4, we can solve the following optimization problem
to obtain a lower bound on Eq. 25:
N min	|T| - tn
(Q(n),s(n))n=1,bXadd,bXdel,BA,t	n∈T
s.t.	||s(n)||1 ≥tn,	Q(pn,d) ≥ s(pn),
(bXadd)Tψ(n) ≥ Qi(,n1)Pp(,n1) ,	(bXdel)Tψ(n) ≥ Q(pn,2)Pp(,n2) ,
X (1 - Am,m0)(Ψ(n)	BA)m,m0 ≥Q(pn,3)Pp(,n3),
m,m0 ≤N
X Am,m0(Ψ(n)	BA)m,m0 ≥ Q(pn,4)Pp(,n4),
m,m0 ≤N
(26)
(27)
(28)
(29)
(30)
(31)
(32)
(33)
(34)
(35)
(36)
(37)
(38)
	||bX add||1 ≤ rXadd,	||bXadd ||1 ≤ rXdel,
	BAi,j ≤ rAadd,	BAi,j ≤ rAdel,
	(i,j):Ai,j =0	(i,j):Ai,j =1
	bXadd n ≤ anrXadd,loc n,	bXdel n ≤ anrXdel,loc n,
X	BAn	m + BAm,n ≤ rAadd,locn,	BA n,m + BAm,n ≤ rAdel,loc n
m:An,m =0		m:An,m =1
Bi,j ≤ai+aj∀i,j ∈ {1,...,N},
||a||1 ≤ σ,
s(n) ∈ {0,1}lP(n)1,	Q(n) ∈ {0, ι}lP(n)l×4, t ∈{0,1}N,
a∈{0,1}N,	bXadd, bXdel ∈N0N,	BA ∈ {0, 1}N×N,
∀n ∈ {1,...,N},p ∈ {1,..., |P(n)|}, d ∈ {1,...,4}.
The constraints from Eq. 27 to Eq. 30 are identical to constraints Eq. 16 to Eq. 19 of our collective
certificate for global budget constraints. They simply implement boolean logic to determine whether
there is some pareto-optimal p ∈ P(n) such that the perturbation in fn ’s receptive field matches or
exceeds p in all four dimensions. If this is the case, the base certificate cannot certify the robustness
offn and tn can be set to 1. Eq. 31 and Eq. 32 enforce the global budget constraints. The difference to
the global budget certificate lies in Eq. 33 to Eq. 36. We introduce an additional variable vector a ∈
{0, 1}N that indicates which nodes are attacker controlled. Eq. 33 enforces that the attributes of node
14
Published as a conference paper at ICLR 2021
n remain unperturbed, unless an = 1. If an = 1, the adversary can add or delete at most rXdel,loc n
or rXdel,loc n attribute bits. With edge perturbations, it is sufficient for either incident node to be
attacker-controlled. This is expressed via Eq. 35. The number of added or deleted edges incident to
node n is constrained via Eq. 34. Finally, Eq. 36 ensures that at most σ nodes are attacker-controlled.
C.1 Undirected adjacency matrix
To adapt our certificate to undirected graphs, we simply change the interpretation of the indicator
matrix BA. Now, setting either BAi,j or BAj,i to 1 corresponds to perturbing the undirected edge
{i, j}. An edge should not be perturbed twice, which we express through an additional constraint:
BAi,j + BAj,i ≤ 1∀i,j ∈ {1,...,N}.	(39)
We further combine Eq. 34, which enforced that at least one of the incident nodes of a perturbed edge
has to be attacker controlled, and Eq. 35, which enforced the local budgets for edge perturbations,
into following constraints:
BAn,m ≤ anrAadd,locn	(40)
m:An,m =0
BAn,m ≤ anrAdel,locn	(41)
m:An,m =1
These changes do not affect the optimal value of the mixed-integer linear program. But they are
more effective than Eq. 34 and Eq. 35 when solving the relaxed linear program and the nodes’ local
budgets are small relative to their degree.
D	Determining the pareto front of base certificates
For our collective certificate, we assume that base certificates directly yield the pareto front P(n) of
points enclosing the volume of budgets K(n) for which the prediction fn is certifiably robust:
P(n) = {ρ ∈ K(n^	| -∃ρ0 ∈ K(n^:	ρ0	= P ∧∀d	∈ {1, 2,	3, 4}	: Pd	≤	PdO	(42)
with
K(n) ⊆ {ρ ∈ N4 | P ∈ L ∧ ∀(X0, A0) ∈ BG(P) ： fn(X, A) = fn(X0, A0)} ,	(43)
KS)= L\K(n) and L = [rχadd ] × [rxa®] × 卜 Aadd] × [rAdei] (with [k] = {0,..., k} (See Section 3).
In practice, finding this representation requires some additional processing which we shall discuss
in this section.
Existing certificates for graph-structured data are methods that determine fora specific budget P ∈ L
whether a classifier fn is robust to perturbations in BG (P). In other words: They can only test the
membership relation P ∈ K(n). One possible way of finding the pareto front is through the following
three-step process:
1.	Use a flood-fill algorithm starting at (0 0 0 0)T to determine K(n).
2.	Identify all points in K(n) = L \ K(n) that enclose the volume K(n).
3.	Remove all enclosing points that are not pareto-optimal.
A pseudo-code implementation is provided in algorithm 1. It has a running time in O c K(n) ,
where c is the worst-case case of performing a membership test P ∈ K(n).
15
Published as a conference paper at ICLR 2021
Algorithm 1: Determining the pareto front of base certificates
Result: Set P(n) of pareto-optimal points enclosing the volume of budgets K(n) within
L= [rXadd] × [rXdel] × [rAadd] × [rAdel] (see Section 3).
P0(n) - {} ； / /Potential Pareto points
Closed_set - {} ； / /Visited vectors
Function floodfill( P)
closed_set — closed-set ∪ {ρ};
if - (P ∈ K(n)) then
I P0(n) — P0(n) ∪ {ρ};
else
for P0 ∈ {P+d|d ∈ {0, 1}4 ∧ ||d||1 = 1} ∩Ldo
if - (ρ0 ∈ closedset) then
I flood fill(PD ； //Consider neighboring vectors
end
end
end
floodfill((0 0 0 0)T);
P(n) - {} ; //Pareto front
for P ∈ P(n)0 do
Pareto.optimal — true ;
for P0 ∈ {P - d0|d0 ∈ {0, 1}4 ∧ ||d0||1 ≥ 1} do
if P0 ∈ P(n)0 then
Pareto.optimal — false ; //Pareto-optimality does not allow
decreasing values while staying in K(n)
break;
end
end
if Pareto.optimal then
I PS) - PS) ∪ {ρ}
end
end
E Tightness for randomized smoothing
In this section we prove that if we use the randomized smoothing based certificate from Bojchevski
et al. (2020) as our base certificate, then our collective certificate is tight: If we do not make any fur-
ther assumptions, outside each smoothed classifier’s receptive field and its expected output behavior
under the smoothing distribution, we cannot obtain a better collective certificate. We define the base
certificate and then provide a constructive proof of the resulting collective certificate’s tightness.
E.1	Randomized smoothing for sparse data
Bojchevski et al. (2020) provide a robustness certificate for classification of arbitrary sparse binary
data. Applied to node classification, it can be summarized as follows:
Assume we are given a multi-output classifier h : G 7→ {1, . . . , C}N. Define a smoothed classifier
f : G 7→ {1,...,C}N with
fn(X, A) = argmaxc∈{1,...,C} Pr [hn(φattr(X), φadj(A)) = c]	∀n ∈ {1, . . . , N},	(44)
where φattr and φadj are two independent randomization schemes that assign probability mass to
the set of attribute matrices {0, 1}N×D and adjacency matrices {0, 1}N×N, respectively. The ran-
16
Published as a conference paper at ICLR 2021
domization schemes are defined as follows:
Pr hφattr(X)m,d = 1 - Xm,di =θXXmde,ldθX(1a-dXdm,d) ∀m∈{1,...,N},d∈{1,...,D}	(45)
Prhφadj(A)i,j=1-Ai,ji =θAAid,ejlθA(1a-dAdi,j) ∀i,j∈{1,...,N}.	(46)
Each bit’s probability of being flipped is dependent on its current value, but independent of the other
bits.
An adversarially perturbed graph (X0, A0) is successful in changing prediction yn = fn(X, A), if
yn 6= argmaxc∈{1,...,C} Pr [hn (φattr(X ), φadj (A )) = c] .	(47)
Evaluating this inequality is usually not tractable. We can however relax the problem: Let pn =
Pr [hn(φattr(X), φadj (A)) = yn] and let H be the set of all possible classifiers for graphs in G
(including non-deterministic ones). If
min Pr
∖hneH
[h n(φattr(X 0) ,φadj ( A0)) = y/)
> 0.5
s.t.
Pr [hn(Φattr(X),Φadj(A)) = y/ = Pn,
(48)
(49)
then fn(X0, A0) = fn(X, A). It is easy to see why: The unsmoothed hn is in the
set defined by Eq. 49, so the result of the optimization problem is a lower bound on
Pr [hn(φattr(X0), φadj (A0)) = yn]. If this lower bound is larger than 0.5, then yn is guaranteed
to be the argmax class.
Optimizing over the set of all possible classifiers might appear hard. We can however use the ap-
proach of Lee et al. (2019) to find an optimum. Let X0, A0 be a graph that results from bXadd
attribute additions, bXdel attribute deletions, bAadd edge additions, and bAdel edge additions applied
to (X, A). We can partition the set of all graphs G into (bXadd + bXdel + 1) (bAadd + bAdel + 1)
regions that have a constant likelihood ratio under our smoothing distribution:
nJqX,qA	qX, qA ∈ N0 ∧	qX	≤	b(Xna)dd	+	b(Xnd)el	+ 1 ∧	qA	≤	b(Ana)dd	+	b(And)el	+ 1 o (50)
with
((X00, A00) ∈ JqX,qA ) =⇒
(Pr[Φattr(X) = X00 ∧ Φadj(A) = A00]=
[Pr[φattr (X 0) = X 00 ∧ φadj(A0) = A00] = ^qA ,
(51)
where the η, ∈ R+ are constants. The regions have a particular semantic meaning, which will be
important for our later proof: Any (X00, A00) ∈ JqX ,qA has qX attribute bits and qA adjacency bits
that have the same value in (X, A), and a different value in (X0, A0):
((X00, AOO) ∈ Jqx,qA) ^⇒	(52)
|{m,d|xm,d = χm,d = Xm,d}∣ = qχ ∧ ∣{i, jlAij = Aij = Aij}∣ = qA.
As proven by Lee et al. (2019), we can find an optimal solution to Eq. 48 by optimizing over the
expected output of h within each region of constant likelihood ratio. This can be implemented via
the following linear program:
Λn (bXadd,bXdel,bAadd,bAdel,pn,) :=
(bXadd +bXdel) (bAadd +bAdel)
m(inn)	X	X	Hq(Xn),qAPr[φattr(X0),φadj(A0)]∈JqX,qA)
qX=0	qA=0
(53)
(bX
add+bXdel) (bAadd+bAdel)
s.t. X	X	Hq(Xn),qAPr([φattr(X),φadj(A)) ∈JqX,qA]=pn,	(54)
qX=0	qA=0
H(n) ∈ [0, 1](rXadd +rXdel)×(rAadd +rAdel).	(55)
Any optimal solution HH(n) corresponds to a single-output classifier hn that, given an input graph
(X 00 , A00 ), simply counts the number of attribute bits qX and adjacency bits qA that have the same
17
Published as a conference paper at ICLR 2021
value in (X, A) and a different value in (X0, A0) and then assigns a probability of Hq(Xn),qA to class
fn and 1 - Hq(Xn),qA to the remaining classes.
The optimal value of Eq. 53 being larger than 0.5 for a fixed perturbed graph (X0, A0) only proofs
that this particular graph is not a successful attack on fn . For a robustness certificate, we want to
know the result for a worst-case graph. However, the result is only dependent on the number of per-
turbations bXadd , bXdel, bAadd and bAdel, and not on which specific bits are perturbed. Therefore, we
can solve the problem for an arbitrary fixed perturbed graph with the given number of perturbations,
and obtain a valid robustness certificate.
For use in our collective certificate, we define the set of budgets K(n) for which prediction fn is
certifiably robust as
K(n) = {(bXadd bXdel bAadd bAd")' ∈ L | Λn (bXadd ,bχd,ι ,IbAaii Mel ,Pn) > O# (56)
where Λn is defined as in Eq. 53 andL = [rXadd]×[rXdel]×[rAadd]×[rAdel] (with [k] = {0, . . . , k})
is the set of vectors that do not exceed the available collective budget (see Section 3).
E.2 Tightness proof
With the definition of our base certificate in place, we can now formalize and prove that the re-
sulting collective certificate is tight. Recall that randomized smoothing is a black-box method. The
classifier that is being smoothed is treated as unknown. A robustness certificate based on random-
ized smoothing has to account for the worst-case (i.e. least robust under the given threat model)
classifier. Our collective certificate lower-bounds the number of predictions that are guaranteed to
be simultaneously robust. We show that with the randomized smoothing base certificate from the
previous section, it actually yields the exact number of robust predictions, assuming the worst-case
unsmoothed classifier.
Theorem 1 Let (X, A) be an unperturbed graph. Let h : G 7→ {1, . . . , C}N be a (potentially
non-deterministic) multi-output classifier. Let f : G 7→ {1, . . . , C} be the corresponding smoothed
classifier with
fn(X00,A00)=argmaxc∈{1,...,C}Pr[hn(φattr(X00),φadj(A00))=c],	(57)
yn = fn(X,A),	(58)
pn =Pr[hn(φattr(X),φadj(A)) =yn],	(59)
and randomization schemes φattr (X), φadj (A) defined as in Eq. 45 and Eq. 46. Let
ψ ∈ {0, 1}N, Ψ ∈ {0, 1}N ×N be receptive field indicators corresponding to fn (see
Eq. 2). Let BG be a set of admissible perturbed graphs, constrained by parameters
rχadd, rχdel, rAadd, rAdel, rχadd,loc, rχdel,loc, rAadd,loc, rAdel,loc, σ, as defined in Section 2. Let T
be the indices of nodes targeted by an adversary. Under the given parameters, let o* be the optimal
value of the optimization problem defined in Section C.
Then there are a perturbed graph (X0, A0), a non-deterministic multi-output classifier h and a
corresponding smoothed multi-output classifier f with
fn (X, A) = argmax°∈{i,…,c} Pr [h n(Φattr(X ),Φadj(A)) = Cl ∀n ∈ {1, ...,N}	(60)
such that
I {n ∈ Tlfn(X0, AO) = yn 1| = o*,	(61)
Pr [hn(φattr(X), Φadj(A)) = yn] = Pn,	(62)
and each fn is only dependent on nodes and edgesfor which ψ(n) and Ψ(n) have value 1.
Proof: The optimization problem from Section C has three parameters bχadd , bχdel, BA, which
specify the budget allocation of the adversary. Let bχ dd, bχdθɪ, BA be their value in the optimum.
We can construct a perturbed graph (X0, A0) from the clean graph (X, A) as follows: For every
node n, set the first bχad壮八 zero-valued bits to one and the first bχdeι： non-zero bits to zero. Then,
18
Published as a conference paper at ICLR 2021
flip any entry (n, m) of An,m for which BA八 m=1. The parameters bX dd, bXdθɪ, BA are part of
a feasible solution to the optimization problem. In particular, they must fulfill constraints Eq. 31 to
Eq. 36, which guarantee that the constructed graph is in BG .
Given the perturbed graph (X0, A0), we can calculate the amount of perturbation in the receptive
field of each prediction fn :
UXadd = (bXadd)T ψ(n)	(63)
UXL = (bXd")' Ψ(n)	(64)
UAa)dd =	X	M" BA i,j	(65)
(i,j):Ai,j =0
UAadd=	x	% BAi,j	(66)
(i,j):Ai,j =1
We can now specify the unsmoothed multi-output classifier h. Recall that in the collective cer-
tificate’s optimization problem, each fn is associated with a binary variable tn. In the optimum,
(tn = O) ^⇒ ](uXn)cld UXn)I UAI)Cld UA) 1) ∈ K(n) , i.e. fns robustness is guaranteedby
the base certificate, and o* = Pn∈τ |T| - tn
Case 1:	n ∈/ T. Choose hn = hn . Trivially, constraint Eq. 62 is fulfilled since fn is only dependent
on nodes and edges for which ψ(n) and Ψ(n) have value 1. Whether f is adversarially attacked or
not does not influence Eq. 61, as n ∈/ T.
Case 2:	n ∈ T and tnn = 0. Choose hn = hn. Again, constraint Eq. 62 is fulfilled since fn is only
dependent on nodes and edges for which ψ(n) and Ψ(n) have value 1. Since tnn = 0, We know that
(UXadd UXLUAa)dd UAdel)T ∈ K(n),i.e. fn(X0, A0)= yn.
Case 3:	n ∈ T and tnn = 1. Since tnn = 1, we know that fn is not certified by the base certificate:
(U(n)	U((In)	UT)	Un) )T / K(n) .Let H*(n) ∈ [0,1] (UXadd+uXdel)×(u Aadd+uAdel) be
Xadd	Xdel	Aadd	Adel
the optimum of the linear program underlying the base certificate (see Eq. 53 to Eq. 55). Define hn
to have the following non-deterministic output behavior:
Pr [hn (X 00, A00)= yn] = H * R(XJn)(A00)∀(X00, A00) / G	(67)
qX (X ),qA (A )
Pr [hn(X00, A00)=yn] = 1 - H*(nn)(X〃)门)(A〃)∀(X00, A00) / G	(68)
qX (X ),qA (A )
for some yn0 6= yn and with
qX (X00) = ∣{(n, d)∖xn,d = Xn,d = Xn,d}∣	(69)
qA)(AOO)= |{(n，m)∖An,m = An,m = An,m} | .	(70)
As discussed at the end of Section E.1, this classifier simply counts the number of bits in (X00, A00)
that are within fn’s receptive field and have the same value in the clean graph (X, A) and a
different value in the perturbed graph (X0, A00). Since H* (n) is a valid solution to the linear
program underlying the base certificate, we know that Eq. 62 is fulfilled, as it is equivalent to
Eq. 54 from the base certificate. Since (U(n)	U(n)	U(n)	U(n) )	∈/ K(n), we know
Xadd	Xdel	Aadd	Adel
that Pr [hn(Φattr(X0),φadj(A0)) = yj ≥ 0.5 (see Eq. 56) , i.e. fn is successfully attacked,
~ .... ~ ,
fn(X0, A0)= yn = fn(X, A).
By construction, we have exactly o* nodes for which fn (X0, A0) = yn and the remaining constraints
are fulfilled as well.
19
Published as a conference paper at ICLR 2021
F	Hyperparameters
Training schedule for smoothed classifiers. Training is performed in a semi-supervised fashion
with 20 nodes per class as a train set. Another 20 nodes per class serve as a validation set. Models
are trained with Adam (learning rate = 0.001 [0.01 for SMA], β1 = 0.9, β2 = 0.999, = 10-8,
weight decay = 0.001) for 3000 epochs, using the average cross-entropy loss across all training set
nodes, with a batch size of 1. We employ early stopping, if the validation loss does not decrease for
50 epochs (300 epochs for SMA). In each epoch, a different graph is sampled from the smoothing
distribution. We do not use the KL-divergence based regularization loss proposed for RGCN, as we
found it to decrease the certifiable robustness of the model.
Training schedule for non-smoothed GCN. Training is performed in a semi-supervised fashion
with 20 nodes per class as a train set. Another 20 nodes per class serve as a validation set. For the
first 100 of 1000 epochs, models are trained with Adam (learning rate = 0.01, β1 = 0.9, β2 =
0.999, = 10-8, weight decay = 10-5), using the average cross-entropy loss across all training set
nodes, with a batch size of 8. After 100 episodes, We add the robust loss proposed in (Zugner &
Gunnemann, 2019) (local budget q = 21, global budget Q = 12, training node classification margin
= log(90/10), unlabeled node classification margin = log(60/40)). The gradient for the robust loss
term is accumulated over 5 epochs before each weight update in order to simulate larger batch sizes.
Network parameters. In all models, hidden linear and convolutional layers are followed by a
ReLU nonlinearity. During training, each ReLU nonlinearity is followed by 50% dropout. For GCN
and GAT, we use two convolution layers with 64 hidden activations. The number of attention heads
for GAT is set to 8 for the first layer and 1 for the second layer. RGCN uses independent gaussians
as its internal representation (i.e. each feature dimension has a mean and a variance). For RGCN, we
use one linear layer, followed by two convolutional layers. We set the number of hidden activations
to 32 for the means and 32 for the variances. Dropout is applied to both the means and variances.
For APPNP, we use two linear layers with 64 hidden activations, followed by a propagation layer
based on approximate personalized pagerank (teleport probablity = 0.15, iterations = 10). To ensure
locality, we set all but the top 64 of each row in the approximate pagerank matrix to 0. For SMA,
we first transform each node’s features using a linear layer with 64 hidden activations. We then
apply soft medoid aggregation (k = 64, T = 10) based on the approximate personalized pagerank
matrix (teleport probablity = 0.15, iterations = 2). Note that we use the alternative parameterization
from appendix B.5 of (Geisler et al., 2020) that is designed to improve robustness to attribute
perturbations. After the aggregation, we apply a ReLU nonlinearity. Finally, we apply a second
linear layer to each node independently.
Randomized smoothing. Randomized smoothing introduces four additional hyperparameters
θXadd , θXadd , θAadd , θAdel, which control the probability of flipping bits in the attribute and ad-
jacency matrix under the smoothing distribution. If we only certify attribute perturbations, we set
θAadd = θAdel = 0, θXadd = 0.002 and θXdel = 0.6. If we only certify adjacency perturbations,
we set θXadd = θXdel = 0, θAadd = 0 and θAdel = 0.4. If we jointly certify attribute and ad-
jacency perturbations, we set θXadd = 0.002, θXdel = 0.6, θAadd = 0 and θAdel = 0.4. Exactly
evaluating smoothed classifiers is not possible, they have to be approximated via sampling. We use
1000 samples to determine a classifier’s majority class (yn), followed by 106 samples to estimate
the probability of the majority class (pn) via a Clopper-Pearson confidence interval. Applying Bon-
ferroni correction, the confidence level for each confidence interval is set to 1 - 0.01/N to obtain
an overall confidence level of 99% for all certificates.
20