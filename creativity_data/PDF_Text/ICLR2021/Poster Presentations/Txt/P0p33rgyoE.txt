Published as a conference paper at ICLR 2021
Variational Intrinsic Control Revisited
Taehwan Kwon
NC
kth315@ncsoft.com
Ab stract
In this paper, we revisit variational intrinsic control (VIC), an unsupervised rein-
forcement learning method for finding the largest set of intrinsic options available
to an agent. In the original work by Gregor et al. (2016), two VIC algorithms
were proposed: one that represents the options explicitly, and the other that does
it implicitly. We show that the intrinsic reward used in the latter is subject to bias
in stochastic environments, causing convergence to suboptimal solutions. To cor-
rect this behavior and achieve the maximal empowerment, we propose two meth-
ods respectively based on the transitional probability model and Gaussian mixture
model. We substantiate our claims through rigorous mathematical derivations and
experimental analyses.
1	Introduction
Variational intrinsic control (VIC) proposed by Gregor et al. (2016) is an unsupervised reinforcement
learning algorithm that aims to discover as many intrinsic options as possible, i.e., the policies with
a termination condition that meaningfully affect the world. The main idea of VIC is to maximize the
mutual information between the set of options and final states, called empowerment. The maximum
empowerment is desirable because it maximizes the information about the final states the agent can
achieve with the available options. These options are independent of the extrinsic reward of the
environment, so they can be considered as the agent’s universal knowledge about the environment.
The concept of empowerment has been introduced in (Klyubin et al., 2005; Salge et al., 2014) along
with methods for measuring it based on Expectation Maximization (Arimoto, 1972; Blahut, 1972).
They defined the option as a sequence of a fixed number of actions. Yeung (2008) proposed to
maximize the empowerment using the Blahut & Arimoto (BA) algorithm, but its complexity in-
creases exponentially with the sequence length, rendering it impractical for high dimensional and
long-horizon options. Mohamed & Rezende (2015) adopted techniques from deep learning and vari-
ational inference (Barber & Agakov, 2003) and successfully applied empowerment maximization
for high dimensional and long-horizon control. However, this method maximizes the empowerment
over open-loop options, meaning that the sequence of action is chosen in advance and conducted re-
gardless of the (potentially stochastic) environment dynamics. This often impairs the performance,
as the agent cannot properly react to the environment, leading to a significant underestimation of
empowerment (Gregor et al., 2016).
To overcome this limitation, Gregor et al. (2016) proposed to use the closed-loop options, meaning
that the sequence of action is chosen considering the transited states. This type of options differ from
those in Klyubin et al. (2005), Salge et al. (2014) and Mohamed & Rezende (2015) in that they have
a termination condition, instead of a fixed number of actions. They presented two algorithms: VIC
with explicit and implicit options (we will call them explicit and implicit VIC from here on). Explicit
VIC defines a fixed number of options, and each option is sampled at the beginning of the trajectory,
conditioning the policies of an agent until termination. In other words, both the state and the sampled
option are the input to the policy function of the agent. One clear limitation of explicit VIC is that
it requires the preset number of options. This does not only apply to explicit VIC, but also to
some recent unsupervised learning algorithms that adopt a discrete option or skill with a predefined
set (Machado et al., 2017; Eysenbach et al., 2018). Obviously, presetting the number of options
limits the number of options that an agent can learn, impeding the maximal level of empowerment.
Moreover, choosing a proper number of options is not straightforward, since the maximum of the
objective for a given number of options depends on several unknown environmental factors such as
1
Published as a conference paper at ICLR 2021
the cardinality of the state space and the transitional model. To overcome this issue, Gregor et al.
(2016) proposed implicit VIC which defines the option as the trajectory (i.e., the sequence of states
and actions) until termination. There exist multiple trajectories that lead to the same final state and
implicit VIC learns to maximize the mutual information between the final state and the trajectory by
controlling the latter. As a result, the number of options is no longer limited by the preset number,
and neither is the empowerment. Despite this advantage, however, these implicit options induce
bias in the intrinsic reward and hinder implicit VIC from achieving maximal empowerment. This
effect grows with the stochasticity of the environment, and it may cause serious degradation of
empowerment, limiting the agent from learning the universal knowledge of the environment. In this
paper, we aim to solve such empowerment degradation of implicit VIC under stochastic dynamics.
To this end, we revisit variational intrinsic control and make the following contributions:
1.	We show that the intrinsic reward in implicit VIC suffers from the variational bias in
stochastic environments, causing convergence to suboptimal solutions (Section 2).
2.	To compensate this bias and achieve the maximal empowerment, we suggest two modifica-
tions of implicit VIC: the environment dynamics modeling with the transitional probability
(Section 3) and Gaussian mixture model (Section 4).
2	Variational Bias of implicit VIC in stochastic environments
In this section, we derive the variational bias of implicit VIC under stochastic environment dynamics.
First, we adopt the definition of termination action and final state from Gregor et al. (2016) and
briefly review VIC. The termination action terminates the option and yields the final state sf = st
independently of the environmental action space. VIC aims to maximize the empowerment, i.e., the
mutual information between option Ω and final state Sf, which can be written as follows:
I(Ω, Sf ∣so) = - J^p(Ω∣so)logp(Ω∣so) + Ep(Sf |so, Ω)p(Ω∣so)logp(Ω∣so, Sf).	(1)
Ω
Ω,sf
Since p(Ω∣s0, Sf) is intractable, VIC (Gregor et al., 2016) derives the variational bound IVB ≤ I
and maximizes it instead:
IVB(Ω, Sf ∣So) = -Xp(Ω∣So)logp(Ω∣So) + Xp(Sf 同,Ω)p(Ω∣So)logq(Ω∣So, Sf),	⑵
Ω
Ω,sf
where q(Ω∣S0,Sf) is the inference model to be trained. When IVB is maximized, We have
q(Ω∣So, Sf) = p(Ω∣so, Sf) and achieve the maximum I. As explained in Section 1, explicit VIC
samples an explicit option at the beginning of a trajectory and it conditions policy as π(a∣S, Ω) until
termination. Due to the randomness of policy, the final state is undetermined for a given option
until the policy converges to achieve a specific option. Unlike explicit VIC, implicit VIC defines its
option as a trajectory until termination, and hence, the final state is determined for a given option.
This can be expressed as
",SC=", ofiSA
(3)
where Sf ∣ω is the final state of an option Ω. This is a key characteristic of implicit VIC and is
essential for deriving the main results of this paper. We will often use this equation to eliminate Sf
for reduction. Interestingly, this makes a difference in empowerment maximization between explicit
and implicit VIC, which can be explained by rewriting I(Ω, Sf |so) as follows:
I(Ω, Sf |so) = H(Sf |so) - H(Sf ∣Ω, so).
Note that H(Sf |Ω,s0) is 0 for implicit VIC since Sf is determined for a given Ω. One can notice that
to maximize empowerment, the agent needs to learn (1) maximizing H(Sf|S0) and (2) minimizing
H(sf ∣Ω, so). While explicit VIC needs to learn both (1) and (2), implicit VIC needs to learn only
(1), since (2) is already achieved by the definition of option. This makes the learning of implicit
VIC easier and faster. Moreover, implicit VIC is scalable as explained in Section 1. Despite these
strengths, implicit VIC suffers from variational bias in intrinsic reward under stochastic environ-
ments that can seriously degrade the empowerment. We derive this variational bias by decomposing
p(Ω∣so) andp(Ω∣Sf, so). Since implicit VIC defines the option Ω as the trajectory of an agent, i.e.,
2
Published as a conference paper at ICLR 2021
the sequence of states and actions: Ω = (so, a0, si, aι,…，ST-ι, aτ-ι = af, ST = Sf = ST-ι),
the probability of an option can be decomposed as
P(QISO)=	ɪɪ	p(at ITt)P(St+ιlτt,a/	⑷
(τt,at,st+ι)∈Ω
using Bayes rule with Tt = (s。，。。，…，St). Similarly, p(Ω∣So, Sf) can be expressed as
P(a|S0,Sf|C)=	ɪɪ	P(at↑τt,sf ∣Ω )P(st+1lτt,at,sf ∣Ω).	⑸
(τt,at,st+ι)∈Ω
Note that Sf is replaced by Sf ∣ω since it is determined by the given Ω. Using (4), (5) and (3), We
can rewrite the mutual information (1) of implicit VIC as
I(Ω,Sf |so) = X p(Ω∣so)	X [log
(τt,at,st+ι)∈Ω
P(at|Tt, sf∣Ω) + log P(St+1|Tt,at,Sf|Q)i
p(at ∣Tt)	p(st+i∣Tt,at)	」
The intrinsic reward of implicit VIC (Gregor et al., 2016) is given by
rΩVB
log
(τt,at,st+ι)∈Ω
q(at|Tt ,sf∣Ω )
P(at|Tt)
(6)
(7)
Ω
where q(at∣τt,sf) is inference and p(at∣τt) is policy of an agent. It can be shown that r^B comes
from the first part of (6) (see Appendix A for details). Under deterministic environment dynamics,
the transitional part logp(st+ι ∣τt, at, Sf ∣ω)/P(St+i ∣τt, at) is canceled out since both the nominator
and denominator are always 1. However, this is not possible under stochastic environment dynamics
and it yields the variational bias bVIC in the intrinsic reward:
bVIC _	X	]c_TP(St+1lτt,at,sf∣Ω)
b0	="「P(StnTt,at)
(8)
From (8), we see that this bias comes from the difference between the transitional probabilities with
and without the given final state. This difference is large when Sf ∣ω in the nominator plays a crucial
role, which then causes a large bias. One extreme case is when St+1 is the necessary transition to
reach Sf ∣ω but not the only transition from (τt, at). Then, P(st+ι∣τt, at, Sf ∣ω) is 1 but P(st+ι ∣τt, at)
is not, yielding a large bias. In Section 5, we provide the experimental evidence that this variational
bias leads to a suboptimal training. Even though the original VIC (Gregor et al., 2016) subtracts
b(so) from rI2 to reduce the variance of learning, it cannot compensate this bias since it also
depends on Ω. In the next section, we analyze the mutual information (1) in more detail under
stochastic environment dynamics and define the variational estimate of (1), IVE, for training.
3	Implicit VIC with transitional probability model
In this section, we analyze I(Ω, Sf |so) under stochastic environment dynamics and propose to ex-
plicitly model transitional probabilities to fix variational bias. First, fora given option and final state,
we definePn(Ω∣Sf, s。), Pρ(Ω∣Sf, s。),Pn(Ω∣s0) andPn(Ω∣s0) as follows:
Pn(CISf, so) = ɪɪ P∏ (at|Tt,Sf ), Pp(a|sf , s0) = ɪɪ PP(St+1|Tt, at, sf ),
(τt,at ,St+ι)∈Ω	(τt,αt,St+ι)∈Ω
Pn (Ω∣so) = ɪɪ Pn (at∣Tt),	Pρ(Ω∣So) = ɪɪ Pρ(st+1∣Tt, at)
(τt,at ,St+ι)∈Ω	(τt,at,St+ι)∈Ω
(9)
Note thatP(Ω∣Sf, so) = Pn(Ω∣Sf, so)Pρ(Ω∣Sf, so) is the true distribution of Ω for given Sf where
Pn(Ω∣Sf, so) is a policy-related part and Pρ(Ω∣Sf, s°) is a transitional part of P(Ω∣Sf, s°) and so
do p(Ω∣so), Pn(Ω∣so) andPρ(Ω∣so). It is necessary to consider transitional probabilities since they
induce variational bias in intrinsic reward. Hence we model (9) as follows:
πq (Ω∣sf,so)
∏ πq(at|Tt, sf), Pq(CISf, SO)= ɪɪ Pq(st+1|Tt, at, sf),
(τt,at,st+ι )∈Ω
πp(ΩIso) =	Y	πp(atIτt),
(τt,at,st+ι )∈Ω
(τt,αt,St+ι)∈Ω
Pp(CISO)=	Y	Pp(St+“fat),
(τt,at,st+ι )∈Ω
(10)
3
Published as a conference paper at ICLR 2021
where πq, ρq, πp and ρp are our models to be trained. We know the policy of an agent, so we have
p∏(at∣τt) = πp(at∣τt). For the other probabilities, they are trained based on our algorithms. Now
We can rewrite I(Ω, Sf |s°) as below using (9):
I(Ω, Sf ∣so) = X p(Ω, Sf ∣so) [logPρ(Ω∣Sf, so)p∏(Ω∣Sf, s°) - logpρ(Ω∣so)p∏(Ω∣so)]. (11)
Ω,sf
Using (10), we define IVE as follows:
IVE (Ω,Sf |so) = X p(Ω,Sf |so)h log Pq (Ω∣Sf ,s°)πq (Ω∣Sf ,so) - log ρp(Ω∣so)πp(Ω∣so)i. (12)
Ω,sf
This is an estimate of the mutual information between Ω and Sf with transitional models. Note
that IVE is not a variational lower bound on I unlike IVB, hence the derivation of VIC in Gregor
et al. (2016) is not applicable in this case. To tackle this problem, we start from absolute difference,
|I - IVE | which can be bounded as
II- IVEI ≤ U VE = X P(Sf ∣so)Dkl [p∏ (∙∣Sf, S0)Pρ(∙∣Sf, so)k∏q (忖,S0)Pq (∙g, so)]
sf	(13)
+ DKL [p∏(∙∣S0)Pρ(∙∣S0)k∏p(∙∣S0)ρp(∙∣S0)].
See Appendix B for the derivation. Note that UVE is the sum of positively weighted KL divergences,
which means that it satisfies UVE → 0 if and only if ∏q(∙∣Sf, So) → p∏(∙∣Sf, So),ρq(∙∣Sf, So) →
Pρ(∙∣Sf, So) and ρp(∙∣So) → Pρ(∙∣So) for all Sf. In other words, our estimate of the mutual informa-
tion converges to the true value as our estimates (10) converge to the true distribution (9). It makes
sense that we can estimate the true value of the mutual information if we know the true distribution.
Hence we minimize UVE with respect to πq, ρq and ρp. If πq, ρq and ρp are parameterized by θπq,
θρq and θρp, we can obtain gradients of UVE using (3) as follows:
Vθ∏ UVE = - Xp(Ω∣So)Vθ∏ logπq(Ω∣Sf∣Ω,So),
Ω
Vθq U VE = - fp(Ω∣So)Vθq log Pq (Ω∣Sf∣Ω ,So),
Ω
(14)
VθpUVE = -Ep(Ω∣So)Vθp logρp(Ω∣So),
Ω
which can be estimated from sample mean. Once we have (∏q(∙∣Sf, So), Pq(∙∣Sf, So), ρp(∙∣So)) ≈
(p∏(∙∣Sf, So), pρ(∙∣Sf, So), pρ(∙∣So)) for all Sf, we can update the policy to maximize I. If πp is
parameterized by θπp, the gradients, Vθpπ I and Vθπp IVE can be obtained as follows using (3):
VθpI=Xp(Ω∣So) Pogp∏(Ω∣Sf∣Ω, So)pρ(Ω∣Sf∣Ω, So)-logπp(Ω∣so)p°(Ω∣so)] Vθp logπp(Ω∣so),
Ω	I
{z
rI
rΩ
}
VθπIVE=Xp(Ω∣so) [log
Ω	।--
∏q (Ω∣Sf∣Ω, So)ρq (Ω∣Sf∣Ω,so)- log πp(Ω∣so)ρp(Ω∣so)] V θpr log πp(Ω∣so),
一■――	,
z
r
IVE
Ω
(15)
where p∏ (Ω | s o) is replaced by ∏p(Ω∣so) since we know the true value of policy (see Appendix A for
details). From (15), we see that VθpIVE → VθpI as ∏q(∙∣Sf ,s0) → p∏(∙∣Sf ,s0), ρq(∙∣Sf ,s°) →
Pρ(∙∣Sf, so) and ρp(∙∣so)) → pρ(∙∣so) for all Sf. It means that we can estimate the correct gradient
of mutual information w.r.t policy as our estimates (10) converge to the true distribution (9). Note
that for deterministic environments, we can omit Pq(∙∣Sf∣∙,so) and ρp(∙∣so) since they are always
1. Then we have IVE = IVB and Vθπq UVE = -Vθπq IVB, which means that maximizing IVB
is equivalent to minimizing UVE for θπq (i.e., it is equivalent to the original implicit VIC). Finally,
Algorithm 1 summarizes the modified implicit VIC with transitional probability model. The addi-
tional steps added to the original implicit VIC are marked with (*). Note that Algorithm 1 is not
always practically applicable since it is hard to model p(st+ι ∣τt,at) and p(st+ι ∣τt,at,Sf) when the
cardinality of the state space is unknown. (We can not set the number of nodes for softmax.) In our
experiment of Algorithm 1, we will assume that we know the cardinality of the state space. This
allows us to model pρ(st+ι∣τt, at) andPρ(st+ι∣τt, at,Sf) using Softmax. In the next section, we
propose a practically applicable method that avoids this intractability of the cardinality.
4
Published as a conference paper at ICLR 2021
Algorithm 1 Implicit VIC with transitional probability model
Initialize so, η, Ttrain, θ∏, θ(P, θ∏ and θpp.
for itrain : 1 to Ttrain do
Follow ∏p(at∣τt) result in Ω = (so, ao,…,Sf).
VE
& — Pt [log∏q(at∣τt, Sf) — log∏p(at∣τt)]	. from (15)
rΩVE J rIVE + Pt [logΡq(st+ι∣τt,at,Sf) — logρp(st+ι∣τt, at)]	. from (15), (*)
Update each parameter:
θ∏ - θ∏ + ηrIVE Vθp Pt log ∏p(at∣τt)	. from (15)
θ∏ - θ∏ + Nθ∏ Pt log ∏q(at∣τt, Sf)	. from (14)
θΡ J θΡ + ηVθp Pt log ρp(st+ι∣τt,at)	. from (14), (*)
θp J θp + ηVθq Pt logρq(st+ι∣τt,at, Sf)	. from (14), (*)
end
end for
4 Implicit VIC with Gaussian Mixture Model
In this section, we propose the alternative method to overcome the limitation of Algorithm 1 by
modeling the smoothed transitional distributions. This allows us to use the Gaussian Mixture
Model (GMM) (Pearson, 1894) and other continuous distributional models for modelling transi-
tional distribution. First, We smooth p(st+ι∣τt, at, Sf) and p(st+ι∣τt, at) into fσ(xt+ι∣τt, at, Sf)
and fσ(xt+ι∣τt,at) with xt+ι = St+ι + zt+1 and zt+1 〜N(0,σ2Id):
fσ (xt+ι∣Tt ,at, Sf) =	E	p(S∖τt,at,Sf )fσ (xt+ι 一 S0 ； 0,σ2Id),
s0∈S(τt,at ,sf)
fσ(xt+ι∣Tt,at) =	E	P(SiTt,at)fσ(xt+1 ― S0; 0,σ2Id),
s0∈S(τt ,at)
(16)
where S(τt, at, Sf) = {S0∣p(S0∣τt, at, Sf) > 0}, S(τt, at) = {S0∣p(S0∣τt, at) > 0} and d is the di-
mension of the state. Then, using Gaussian Mixture Model (GMM) (Pearson, 1894), we model (16)
as fσ(xt+ι∣τt,at,Sf) and fP(xt+ι∣τt,at):
ngmm
fσ(xt+i|Tt,at,Sf) = E Wi(Tt,at,Sf )fσ(xt+1； μi(τt,at,Sf ),σ2Id),
i=1
ngmm
fp(xt+i∣τt,at) = E Wi(τt, at)fσ(xt+1； μi(τt,at),σ2Id).
i=1
(17)
Note that if we set ngmm > maxτt,at |S(Tt, at)|, (17) can perfectly fit (16). Now using (17), we
define IσV E, the variational estimate with smoothing as follows:
IVE = X P(Ω, Sf |so) [logπq(Ω∣Sf, So)fσq(Ω∣Sf, So) — logπp(Ω∣So)fp(Ω∣So)].	(18)
Ω,sf
Note that IσVE is not a variational lower bound on I, hence we can not also apply derivation of
implicit VIC in Gregor et al. (2016). As in Section 3, we start from the absolute difference between
I and IσVE . The upper bound on |I — IσVE | can be obtained as follows:
II-IVEI ≤ UVE + UVE with
UVE=X p(Ω"so)log h P¾) ≡⅛ i,
ɑ	L PP(CISO) fσ(CISf,so)」
ω,S f
UσV,2E =	P(Sf ISo)DKL [p∏(1sf, SO)Pρ(1sf, so)||nq(1sf, SO)Pρ("sf, so)].
sf
(19)
See Appendix C for the derivation. Note that UσV,2E differs from the first part of UVE in (13). This
upper bound implies UVE → 0 as fj(∙∣Sf,so"fP(∙∣s0) → pρ(∙∣Sf,so)∕pρ(∙∣so) for all Sf and
5
Published as a conference paper at ICLR 2021
UVE → 0 if and only if πq(∙∣sf ,s0) → p∏(∙∣sf ,s°) for all Sf since UVE is the sum of positively
weighted KL divergences. To estimate the correct value of mutual information from (18), we mini-
mize UσV,1E and UσV,2E . The gradient of UσV,2E can be obtained as below using (3):
Vθ∏ UVE = -E PgISO)Vθ∏ log πq glsf∣Ω,SO)
Ω
(20)
which can be estimated from the sample mean. As Algorithm 1, it satisfies Vθπq UσV,2E = -Vθπq IVB,
which means that minimizing UσV,2E is equivalent to maximizing IVB with respect to θπq. Since UσV,2E
is 0 if and only if πq (∙∣Sf ,so) = p∏ (∙∣Sf ,so) for all Sf, this update will make πq (∙∣Sf ,so) converge
top∏(∙∣Sf, so) for all Sf.
Unlike UσV,2E , it is difficult to directly minimize UσV,1E due to the absolute value function. How-
ever, it can be minimized by estimating the correct ratio between transitional distributions,
Pρ(∙∣Sf, so)∕pρ(∙∣so), even though each of their true values is unknown. To estimate this ratio, we fit
(17)to(16) by minimizing Dkl [fσ (∙∣τt,at,Sf )kfq (∙∣τt,at,Sf)] and Dkl [fσ (∙∣τt,at)kfp(∙∣τt,at)].
If fσq and fσp are parameterized by θρq and θρp , the gradients of KL divergences can be obtained as
follows:
vθqDKLfσ (∙lτt,at,sf )kfσ (∙lτt,at,sf )] = - /	fσ (Xt+1lτt,at,sf )V θq, log fσq (xt+l∖τt, at, s
xt+1
f),
VθpDkl[fσ(∙∣τt,at)kfp(∙∣τt,at)] = - j	fσ(xt+ι∣τt, at)Vθp logfp(xt+ι∣τt, at),
xt+1
(21)
which can be estimated from the sample mean. As our estimated smoothed transitional distribution
(17) converges to the true smoothed transitional distribution (16), it satisfies fσ(∙∖sf, so)/fp(∙∣so)
→ Pρ(∙∖sf, so)∕pρ(∙∖so) which leads to UVE → 0 for finite T and σ《dmin where T is the average
length of trajectories and dmin is the minimal distance between two different states (see appendix
D for details). Hence, for small enough σ and finite length of trajectories, we can minimize UσV,1E
to nearly zero. This implies that if we smooth the original transitional distribution with smaller
σ, the smoothed transition will be sharper and the ratio between the transitional probabilities will
be estimated more accurately using them. Once We have (∏q(∙∖sf, so), f (∙∖sf, so), fP(∙∖so)) ≈
(p∏ (∙∖sf ,so), fσ (∙∖sf ,so),fσ (∙∖so)) for all Sf after the update by (20) and (21), We have IVE ≈ I.
Now, we can update the policy by obtaining Vθπp IσV E using (3):
Vθ∏I =	Xp(Ω∖so)	hlog P咪f,S0)	+ log Pp(窜,S0) i	Vθ∏ log∏p(Ω∖so),
π	y	L ∏p(Ω∖so)	Pρ(Ω∖so)」π
ω	'--------------{Z--------------}
vθpIVE = XP(CISO) hlog 开(?f }：0)+ log f'；；f 厂；。)i vθp logπp(CISO)
π	y	L	πp(Ω∖sο)	fσ (Ω∖so )」π
ω	X------------------{-------------------}
IVE
rΩ
(22)
where Vθπp I is rewritten from (15). From (22), we see that it satisfies Vθπp IσV E → Vθpπ I as
∏q(∙∖sf,so) → Pn(∙∖sf, so) and fj(∙∖sf,so)∕fP(∙∖sο) → Pρ(∙∖sf,sο)∕pρ(∙∖sο) for all Sf, which
can be achieved by (20) and (21) as explained previously. Hence, we can estimate the correct gra-
dient of mutual information w.r.t our policy using the estimated smoothed transitional distributions
for finite T andσ《dmin. However, choosing an appropriate value of σ is not straightforward
since dmin and T are usually unknown and depend on the environment. Besides, too small σ makes
the training of fσq and fσp unstable due to the extreme gradient of Gaussian function near the mean.
Another issue of GMM is the choice of a proper ngmm of (17). As explained previously, we can
perfectly fit (17) to (16) for ngmm > maxτt,at ∖S(τt, at)∖. We may choose a very large ngmm for
the perfect fit but it makes training hard for its complexity. We leave the proper choice of σ and
ngmm as future works and use empirically chosen values (σ = 0.25 and ngmm = 10) in this paper.
Finally, we summarize our method in Algorithm 2. Additional steps added to the original implicit
VIC are marked with (*).
6
Published as a conference paper at ICLR 2021
Algorithm 2 Implicit VIC with Gaussian mixture model
Initialize s。，η,Ti,Tsmooth, θ∏, θq, θ∏ and θp∙
for itrain : 1 to T do
Follow πp(at ∣τt) result in Ω = (s0, a0,…,Sf).
IV E
rΩσ J Pt [log πq(at|Tt,sf) - log πP(at|Tt)]
IV E	IV E
rΩ J rΩ + Et [log fσ⑶+ι |Tt,at,sf) — log fσ⑶+ι |Tt,at)]
Update each parameter:
IV E
θp J θ∏ + ηrΩσ vθρ Pt log πP (at |Tt)
θ∏ J θ∏ + η"θ∏ Pt log πq(at |Tt,sf)
∆θρP J 0
∆θρq J 0
for ismooth : 1 to Tsmooth do
Sample (z1,z2,…,Zf),Zi ~N(0, σ2In)
∆θp J ∆θp + ηVθp Pt log fp(st+1 + zt+1 ∣τt, at)
δ. J δθρ + ηVθq Pt log fσ(st+ι + zt+ι ITt, at, Sf)
end for
θp J θp + △? ITsmooth
θρq J θρq + ∆θρq /Tsmooth
end
end for
. from (22)
. from (22), (*)
. from (22)
. from (20)
. (*)
. (*)
. (*)
. from (21)
. from (21)
. (*)
. (*)
5 Experiments
In this section, we evaluate implicit VIC (Gregor et al., 2016), Algorithm 1 and Algorithm 2. We use
LSTM (Hochreiter & Schmidhuber, 1997) to encode τt = (S0, a0, ..., St) into a vector. We conduct
experiments on both deterministic and stochastic environments and evaluate results by measuring
the mutual information I from samples. To measure I, we rewrite (1) using (3) as follows:
1(ɑ,sf |so) =一工 P(Sf |so )log P(SfIS0) + 工 p(ɑ1so )p(sf iω,so )log P(Sf iω,so )
Ω,sf	Ω,sf
= -	P(SfIS0)logP(SfIS0)
sf
which is maximized when Sf is distributed uniformly. We estimate I using the distribution of Sf
from the samples, i.e., P(Sf ∣s0). We apply exponential moving average (0.99 as a smoothing factor)
to an average of 5 repetitions for estimating P(Sf ∣s0). We manually set Tmax (maximum length
of a trajectory) for each experiment such that the termination action is the only available action at
Tmaxth action. For training GMM of Algorithm 2, the transitional models are trained to predict the
distribution of ∆xt+1 = xt+1 - St instead of xt+1 since predicting difference is usually easier than
predicting the whole state. For the training, we have a warm-up phase which trains the base function
b(S0) in Gregor et al. (2016) and the transitional models. After the warm-up phase, we update the
base function, policy and transitional models simultaneously. Please see Appendix F.1 for details on
the hyper-parameter settings.
(c) Deterministic tree world.
(a)	Deterministic 1D world.
(b)	Deterministic 2D world.
Figure 1: Deterministic environments. Fig. 1a shows the deterministic 1D world. The agent can
go left right. Fig. 1b shows the deterministic 2D. The agent can go left, up, right and down. Fig 1c
shows the deterministic tree world. The agent can go left and right.
7
Published as a conference paper at ICLR 2021
We compare the algorithms in deterministic environments in Fig. 1. Note that although (8) is zero
for deterministic environments, we still train the transitional models of Algorithm 1 and 2 to show
the convergence to the optimum of them. Fig. 2 shows that all three algorithms rapidly achieve the
maximal empowerment. We can observer that all the states in environments are visited uniformly
after training which is achieved when the mutual information is maximized. Fig. 3 shows the
training results of implicit VIC and Algorithm 2 in the 25 × 25 grid world with 4 rooms used in
Gregor et al. (2016). Both implicit VIC and our Algorithm 2 successfully learn passing narrow
doors between rooms and visit almost the whole reachable states for a given Tmax = 25. The
additional results in the Mujoco (Todorov et al., 2012) showing the applicability of our Algorithm 2
are in appendix E.
(a) Deterministic 1D world
with Tmax = 5.
(b) Deterministic 2D world
with Tmax =5.
(c) Deterministic tree world
with Tmax =4.
Figure 2: Estimated empowerment during the training in deterministic environments. Fig. 2a
shows the deterministic 1D world and its training results. The agent can go left right. Fig. 2b shows
the deterministic 2D world and its training results. The agent can go left, up, right and down. Fig
2c shows the deterministic tree world and its training results. The agent can go left and right. Green
shows the distribution of sf.
maximum
random agent
implicit VIC
——implicit VIC + TM
——implicit VIC + GMM
implicit VIC	implicit VIC+GMM
Figure 3: Deterministic grid world with 4 rooms. The environment is a 25 × 25 grid
world with 4 rooms. The agent can go left, up, right and down. The agent starts from (4,
4) and (10, 4) with Tmax = 25. Green shows the distribution of sf .

We compare the algorithms in stochastic environments. Please see appendix F.2 for the details on
the stochasticity of the environments. Fig. 4 shows results in simple stochastic environments. We
see that while implicit VIC converges to sub-optimum, our two algorithms achieve the maximal
empowerment. In Fig. 5, implicit VIC fails to reach far rooms, whereas our Algorithm 2 reaches
every room in the environment. From Fig. 4 and 5, we can notice that implicit VIC fails to reach
far states under the stochastic dynamics. It happens because the variational bias of implicit VIC is
accumulated throughout a trajectory, i.e., it gets larger as the length of the trajectory increases. Fig.
5 also shows the results of training with external rewards. The same mixed reward is used as Gregor
et al. (2016), r = r I + αrE with α = 30. For training the random agent, only αr E is used and
the entropy loss was applied for exploration. While the random agent and implicit VIC converge to
sub-optimum, our Algorithm 2 achieves the optimal solution.
8
Published as a conference paper at ICLR 2021
(a) Stochastic 1D world with
Tmax = 5. Algorithm 1 and 2
achieve ~196% empowerment
gain compared to implicit VIC.
(b) Stochastic 2D world with
Tmax = 5. Algorithm 1 and 2
achieve ~142% empowerment
gain compared to implicit VIC.
(c) Stochastic tree world with
Tmax = 4. Algorithm 1 and 2
achieve ~406% empowerment
gain compared to implicit VIC.
Figure 4: Estimated empowerment during the training in stochastic environments. The envi-
ronments are equal to Fig. 1 except for their stochasticity. Green shows the distribution of sf .
Update step
Figure 5: Stochastic gird world with 35 rooms. The environment is a 15 × 15 grid world with 35
rooms (black cells surrounded by gray walls). We set Tmax = 25. The agent can go up, down and
right. It starts from (0, 6). Once the agent enters a room, the environment returns done and then the
final action is available only. Green shows the distribution of sf . The external reward is composed
of -0.1 for every time step, +1 for entering the normal room and +100 for entering the special room.
The sub-optimal solution is reaching the closest room. The optimal solution is trying to reach the
special room (the room with the red box) while it enters the closest normal room as soon as possible
when it is impossible to reach there due to the stochastic transition.
6 Conclusion
In this work, we revisited variational intrinsic control (VIC) proposed by Gregor et al. (2016). We
showed that for VIC with implicit options, the environmental stochasticity induces a variational
bias in the intrinsic reward, leading to convergence to sub-optima. To reduce this bias and achieve
maximal empowerment, we proposed to model the environment dynamics using either the transi-
tional probability model or the Gaussian mixture model. Evaluations on stochastic environments
demonstrated the superiority of our methods over the original VIC algorithm with implicit options.
Acknowledgments
This research is conducted with the support of NC. We thank Seong Hun Lee at University of
Zaragoza for his sincere feedback on our work, Yujeong Lee at KL Partners for her encouragement
and Seungeun Rho at Seoul National University, Jinyun Chung, Yongchan Park, Hyunsoo Park,
Sangbin Moon, Inseok Oh, Seongho Son and Minkyu Yang at NC for their useful comments.
9
Published as a conference paper at ICLR 2021
References
Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless chan-
nels. IEEE Transactions on Information Theory,18(1):14-20,1972.
David Barber and Felix V Agakov. The im algorithm: a variational approach to information maxi-
mization. In Advances in neural information processing systems, pp. None, 2003.
Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE transactions
on Information Theory, 18(4):460-473, 1972.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2018.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Alexander S Klyubin, Daniel Polani, and ChrystoPher L Nehaniv. EmPowerment: A universal agent-
centric measure of control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, PP.
128-135. IEEE, 2005.
Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laPlacian framework for oP-
tion discovery in reinforcement learning. In International Conference on Machine Learning, PP.
2295-2304, 2017.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. In Advances in neural information processing systems,
PP. 2125-2133, 2015.
Karl Pearson. Iii. contributions to the mathematical theory of evolution. Philosophical Transactions
of the Royal Society of London.(A.), (185):71-110, 1894.
ChristoPh Salge, Cornelius Glackin, and Daniel Polani. EmPowerment-an introduction. In Guided
Self-Organization: Inception, PP. 67-114. SPringer, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, PP. 5026-5033.
IEEE, 2012.
Raymond W Yeung. The blahut-arimoto algorithms. In Information Theory and Network Coding,
PP. 211-228. SPringer, 2008.
A Derivation of intrinsic reward from mutual information
Here we derive the intrinsic reward of VIC by taking the gradient of (1) with resPect to the Parameter
of policy θ. We omit so for simplicity. Note that p(Ω),p(sf) and p(Ω,Sf) can be parameterized by
θ since they are all determined by Policy. We start by rewriting (1):
Ig, Sf) = X Pθg,Sf) [logPθ(Ω, Sf) - logpθ(Ω)pθ(Sf)].
Ω,sf
10
Published as a conference paper at ICLR 2021
Then by taking the gradient with respect to θ, we obtain
VθI(Ω, Sf) = X Pθ(Ω, Sf) [logPθ(Ω, Sf) - logpθ(Ω)pθ(Sf)] NG logpθ(Ω, Sf)
Ω,sf
,G ʌ ΓnΘpθ(C,Sf )
+ 2 Pp 0'Sf" KΩfΓ
ω,s f
Np pp (Ω) Nppp (Sf)
---- ——	；—：—
Pp (C)	Pp (Sf)
X Pp(Ω,Sf )[logPp(Ω,Sf) - logPp(Ω)pp(Sf)] Np logpp(Ω,Sf)
Ω,sf
+ X [NpPp(Ω, Sf) - Pp(Sf ∣Ω)NpPp(Ω) - Pp(Ω∣Sf NpPp(Sf)] ∙
Ω,sf
Using
E NpPp(Ω, Sf)=0, EPp (Sf ∣Ω)Np Pp (Ω) =0, £Pp (Ω∣Sf )Np Pp (Sf) = 0,
Ω,sf	Ω,sf	Ω,sf
and (3) we have
Np I (Ω, Sf) = X Pp (Ω,Sf) [log Pp (Ω, Sf) - log Pp (Ω)Pp (Sf)] Np log Pp (Ω, Sf)
Ω,sf
EPp(Ω)rΩNp logPp(Ω)
Ω
(23)
where
rI = logPp(Ω∣Sf∣ω) - logPp(Ω).
Similarly, we can obtain
Np IVE (Ω, Sf) = X Pp (Ω, Sf) [ log q(Ω∣Sf) - log Pp (Ω)∣Np log Pp (Ω, Sf)
Ω,sf
=XPp(Ω)rΩVENp logPp(Ω)
Ω
where
IV E
Λω	= logq(Ω∣Sf∣Ω) - logPp(Ω).
Using (4) and (5), We can rewrite rΩ as
Pp(at|Tt, Sf|Q)Pp(St+1|Tt' at,Sf|Q)
Pp(at∣τt)P(St+ι∣τt, at)
rΩ =	E log
τt,at,st+ι∈Ω
Since Pp(at∣τt, Sf∣ω) is intractable, We may replace it with variational inference qφ(at∣τt, Sf∣ω)
which result in
rΩVE
log
Tt,at,st+iea
qφ(atlτt, Sf|Q)Pp(St+1|Tt, at,Sf|Q)
Pp (at∣τt)P(St+ι∣τt,at)
For deterministic environment, we have Pp (St+ι∣τt, at, Sf ∣ω) = P(St+ι∣τt,at) = 1 and both rΩ and
VE
rΩ	can be reduced into
rΩ
log
τt,at,st + 1 ∈ω
Pp (⑷丁力町心)
Pp (at∣τt)
rΩVE
log
τt,at,st+ι ∈Ω
goStggfg)
Pp (at∣τt)
=「
11
Published as a conference paper at ICLR 2021
B Derivation of Uve
Here We derive Uve from |I — Ive | with omitted SQ for simplicity:
IIT VEkX m町)[log ∣∣∣⅛≡ — log「]
ω,s f
≤
Ep(Ω,sf )log
Ω,sf
png|sf )ppg|sf )
p∏ (Ω|sf )pP(Ω|sf)
fp(Ω,s∕ )log
Ω,sf
p∏ (Ω)pρ(Ω)
Pn (Ω)pP(Ω)
EP(Sf )pg|sf)log
Ω,sf
p∏(a|sf )pp(a|sf)
Pn g|sf )pP(Ω|sf)
EP(Ω)p(s f |Q)log
Ω,sf
p∏(ω)pp(ω)
P (ω)pP(ω)
+
+
Using (3), (9) and (10), we obtain
I — I ve
I ≤ XP(Sf)P(Ω|Sf)log
Ω,Sf
Pn g|sf )Pp(Q|sf )
Pn (Ω∣sf )pρ(Ω∣sf)
EP(Sf )P(C|sf )log
Ω,Sf
Pn g|sf )Pp(Q|sf )
Pn (Ω|sf )pρ(Ω|sf)
EP(Ω)p(s f |Q)log
Ω,Sf
fp(Ω)log
Ω
Pπ(Ω)pρ(Ω)
Pn (Ω)pP(Ω)
Pn (Ω)pρ(Ω)
Pn (Ω)pp(Ω)
(v (3))
+
+
EP(Sf )Dkl[p∏ (.|sf )pρ(∙g )||pn (.|sf )pρ (.|sf)] + Dkl [Pn(∙)Pρ(∙)HPn (∙)p^(∙)]
Sf
C Derivation of UVE AND UVE
Here we derive (19). We also omit SQ for simplicity here. We start from ∣ I — IVE ∣ :
Ω,Sf
Pn (C|Sf )pQ(C|Sf ) l Pn (C)PP(C)]
pn(Q|Sf)∕q(Q|Sf) - ogPnwpW
X Pd Sf )[ log 制 f 渡||f ) — log 愣](V Pn M=Png))
Ω,Sf	`	,	八一
XT)(O S )[lo。Ppg|Sf)/pg) +l Png|Sf)pp(ɑg)i
，p( , f)[lgPP(Ω)fq(Q|Sf) +l gPn(Q|Sf)pp(Q|Sf)]
ω,s f
< Xn9 q IlC,Pρ(Q|Sf)fp(0
≤ ⅛ PaSf )log PP(WW)
ω,s f
χrr, G CIC OPn (0|Sf )Pp(0|Sf )
⅛ PaSf )log Pn Wf "|Sf)
ω,s f
+
X 力2 q IlC,Pρ(Q|Sf)fpg)
⅛ PaSf )log PP(WW)
ω,s f
UVE + uVE.
X p(Sf )p(ΩSf )log PfΩ⅛
ω,s f
+
12
Published as a conference paper at ICLR 2021
D DERIVATION OF THE BOUNDS ON I - Iσ
Here we derive the bounds on the estimation error of mutual information with smoothing. First, we
derive the upper bound on fσ(st+ι∣τt,at):
fσ (st+ι∣Tt,at) =	E	p(s0∣τt, at)fσ (st+1 — s0; 0,σ2In)
s0∈S(τt,at)
p≡)n(p(st+i|Tt，at)+	X	P(SlTt,3eχpj'st+2σ2s '2
(2πσ )	s06=st+1∈S(τt,at)
1
≤	/
p(2πσ2 )n
(P(St+ι∣τt ,at)+	X	p(s0∣τt,αt)exp(-
s0 6=st+1 ∈S(τt,at)
√(2⅛ (P(St+1|Tt，at)
+ (1 — p(st+ι∣τt,at))exp(一
1
≤	,
P (2πσ2 )n
Obviously, We have √ J 2)“
二(p)(.st^∖τ't,, at)+eχp (- 2σin)).
:p(st+ι∣τt,at) ≤ fσ(st+ι∖τt,at) which results in:
p(st+1 ∖τt, at) ≤ pP(2πσ2)nfσ(st+1 ∖τt, at) ≤ p(st+l∖τt, at) + exp ( min ).	(24)
2σ2
Similarly, we can obtain the bounds of fσ (St+1 ∖τt, at, Sf) as follows:
p(st+ι∖τt,at,Sf) ≤ √(2πσ2)nfσ (st+ι∖τt ,at ,Sf) ≤ p(st+ι∖τt,at,Sf) +exp(- —dσn )	(25)
Combining (24) and (25), we can obtain
P(St+ι∖τt, at, Sf)	≤ fσ(St+ι∖τt, at, Sf) ≤ P(St+ι∖τt, at, Sf) + eχp (--dm2) (26)
p(st+ι∖τt,at) + exp(-dmi2) — fσ (St+i\Tt,at)	—	p(st+1\τt,at,sf)	.
Taking log and using log (a + b) ≤ log a + b for a,b > 0 to (26), we have
log P(St+1∖τt,at,Sf) - -- exp ( — dmin) ≤
P(St+1∖τt, at)	Pmin	2σ2
log fσ (St+l^t,at,sf )
fσ (St+1 ∖τt, at)
≤ log P(St+1 h,at,f )
—	P(St+l∖τt ,at)
1
H--------
Pmin,f
exp (—
which results in
———exp (- dmin) ≤ log P(St+"了,?)
Pmin,f	2σ2	P(St+1∖τt, at)
-log fσ (%+^Fatgf )
fσ (St+l\Tt, at)
≤ -^exp(-
Pmin
(27)
Using (9) and (27) we obtain
TΩ	/ dmin ʌ , 1	夕。^^…。)
------exP ( — -m) ≤ log G 、
PmMnf	2σ2	Pρ(Ω∖S0)
1	fσ (Ω∖Sf ,S0)/ Tω	/ dmin.
- log -fσWT ≤ Pmn exP(-十 ).
By taking the expectation to each side, we have
--Tma^ exp(-dmin) ≤ —- exp(-dmin) ≤
Pmin,f	2σ2	Pmin,f	2σ2
I-Iσ	(28)
≤ ɪ exp(-dmin) ≤ — exp(-
Pmin	2σ	Pmin
with TmaX = maxΩ Tω and T = Pq§ 于 P(Ω,Sf ∖so)TΩ. ThiS implies that the estimation error of
mutual information with smoothing converges to 0 for fine TmaX or T as σ → 0. Also, for finite T
and σ dmin, it satisfies ∖I - Iσ∖ ≈ 0.
13
Published as a conference paper at ICLR 2021
E Additional experiment results
Here we show additional experiment results in HalfCheetah-v3 in the Mujoco environments
(Todorov et al., 2012) to show the applicability of our Algorithm 2. We expect that both implicit
VIC and our Algorithm 2 will show similar results as Fig. 1 and Fig. 2 since it is a deterministic
environment. We fixed the length of the trajectory as T = 100 to force the agent to learn long
enough trajectories. Each motor’s action space is quantized to 5 actions. We can observe that excit-
ing movements (especially triple backflips) are learned by Algorithm 2. Another exciting fact is that
since the number of options it can learn is unlimited, it shows newer behaviors on and on as learning
goes on.
Forward fli P (success)
Forward hop
Forward hop &
Forward hop &
& run
big backflip (fail)
big backflip (fail)
forward flip (fail)
Figure 6: Learned behaviors by Algorithm 2 in HalfCheetah-v3. The question mark is
written when the result of the flip is unknown. The various behaviors such as forward flip,
backflip, etc. are learned by Algorithm 2.
Forward hop &
forward flip (?)
F Experimental details
Here we specify experimental details and environment details.
F.1 Hyper-parameters
Here we show hyper-parameters used in our experiments. We used a learning rate of 1e - 3 for Fig.
2 and Fig. 4 and 1e - 4 otherwise.
14
Published as a conference paper at ICLR 2021
Table 1: Hyper-parameters used for experiments
Hyper-parameter	Value	
Optimizer	Adam
Learning rate	1e-3, 1e-4
Betas	(0.9, 0.999)
Weight initialization	Gaussian with std. 0.1 and mean 0
Batch size	128
Tsmooth	128
σ (GMM)	0.25
ngmm (GMM)	10
F.2 Stochastic environments detail
Here we specify the transition tables used in this paper.
Table 2: Transition table of stochastic environments.
^^∖Transition Actio〕^^^	go left	go right
go left	0.7	0.3
go right	0.3	0.7
(a) Transition table of the stochastic 1D world.
^^^ransition Action^^^	go left	go up	go down	go right
go left	0.7	0.1	0.1	0.1
goup	0.1	0.7	0.1	0.1
go right	0.1	0.1	0.7	0.1
go down	0.1	0.1	0.1	0.7
(b) Transition table of the stochastic 2D world.
^∖Tra∏sition Actio	go left	go right	no move
go left	0.8	0.0	0.2
go right	0.6	0.2	0.2
(c) Transition table of the stochastic tree world.
^^^ransition ActioT^^^	go UP	go down	go right	no move
goup		0.7	0.0	0.0	0.3
go down	0.0	0.7	0.0	0.3
go right	0.0	0.0	0.7	0.3
(d) Transition table of the stochastic grid world with 35 rooms.
15