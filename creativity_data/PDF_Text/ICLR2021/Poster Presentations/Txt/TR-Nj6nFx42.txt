Published as a conference paper at ICLR 2021
A PAC-Bayesian Approach to Generalization
Bounds for Graph Neural Networks
Renjie Liao1,2, Raquel Urtasun1,2,3, Richard Zemel1,2,3
University of Toronto1, Vector Institute3, Canadian Institute for Advanced Research3
{rjliao, urtasun, zemel}@cs.toronto.edu
Abstract
In this paper, we derive generalization bounds for two primary classes of graph
neural networks (GNNs), namely graph convolutional networks (GCNs) and mes-
sage passing GNNs (MPGNNs), via a PAC-Bayesian approach. Our result reveals
that the maximum node degree and the spectral norm of the weights govern the
generalization bounds of both models. We also show that our bound for GCNs
is a natural generalization of the results developed in (Neyshabur et al., 2017)
for fully-connected and convolutional neural networks. For MPGNNs, our PAC-
Bayes bound improves over the Rademacher complexity based bound (Garg et al.,
2020), showing a tighter dependency on the maximum node degree and the maxi-
mum hidden dimension. The key ingredients of our proofs are a perturbation anal-
ysis of GNNs and the generalization of PAC-Bayes analysis to non-homogeneous
GNNs. We perform an empirical study on several synthetic and real-world graph
datasets and verify that our PAC-Bayes bound is tighter than others.
1	Introduction
Graph neural networks (GNNs) (Gori et al., 2005; Scarselli et al., 2008; Bronstein et al., 2017;
Battaglia et al., 2018) have become very popular recently due to their ability to learn powerful
representations from graph-structured data, and have achieved state-of-the-art results in a variety
of application domains such as social networks (Hamilton et al., 2017; Xu et al., 2018), quantum
chemistry (Gilmer et al., 2017; Chen et al., 2019a), computer vision (Qi et al., 2017; Monti et al.,
2017), reinforcement learning (Sanchez-Gonzalez et al., 2018; Wang et al., 2018), robotics (Casas
et al., 2019; Liang et al., 2020), and physics (Henrion et al., 2017). Given a graph along with
node/edge features, GNNs learn node/edge representations by propagating information on the graph
via local computations shared across the nodes/edges. Based on the specific form of local compu-
tation employed, GNNs can be divided into two categories: graph convolution based GNNs (Bruna
et al., 2013; Duvenaud et al., 2015; Kipf & Welling, 2016) and message passing based GNNs (Li
et al., 2015; Dai et al., 2016; Gilmer et al., 2017). The former generalizes the convolution operator
from regular graphs (e.g., grids) to ones with arbitrary topology, whereas the latter mimics message
passing algorithms and parameterizes the shared functions via neural networks.
Due to the tremendous empirical success of GNNs, there is increasing interest in understanding
their theoretical properties. For example, some recent works study their expressiveness (Maron
et al., 2018; Xu et al., 2018; Chen et al., 2019b), that is, what class of functions can be represented
by GNNs. However, only few works investigate why GNNs generalize so well to unseen graphs.
They are either restricted to a specific model variant (Verma & Zhang, 2019; Du et al., 2019; Garg
et al., 2020) or have loose dependencies on graph statistics (Scarselli et al., 2018).
On the other hand, GNNs have close ties to standard feedforward neural networks, e.g., multi-layer
perceptrons (MLPs) and convolutional neural networks (CNNs). In particular, if each i.i.d. sample
is viewed as a node, then the whole dataset becomes a graph without edges. Therefore, GNNs can
be seen as generalizations of MLPs/CNNs since they model not only the regularities within a sample
but also the dependencies among samples as defined in the graph. It is therefore natural to ask if we
can generalize the recent advancements on generalization bounds for MLPs/CNNs (Harvey et al.,
2017; Neyshabur et al., 2017; Bartlett et al., 2017; Dziugaite & Roy, 2017; Arora et al., 2018; 2019)
to GNNs, and how would graph structures affect the generalization bounds?
1
Published as a conference paper at ICLR 2021
In this paper, we answer the above questions by proving generalization bounds for the two primary
classes of GNNs, i.e., graph convolutional networks (GCNs) (Kipf & Welling, 2016) and message-
passing GNNs (MPGNNs) (Dai et al., 2016; Jin et al., 2018).
Our generalization bound for GCNs shows an intimate relationship with the bounds for MLPs/CNNs
with ReLU activations (Neyshabur et al., 2017; Bartlett et al., 2017). In particular, they share the
same term, i.e., the product of the spectral norms of the learned weights at each layer multiplied by
a factor that is additive across layers. The bound for GCNs has an additional multiplicative factor
d(l-1)/2 where d - 1 is the maximum node degree and l is the network depth. Since MLPs/CNNs
are special GNNs operating on graphs without edges (i.e., d- 1=0), the bound for GCNs coincides
with the ones for MLPs/CNNs with ReLU activations (Neyshabur et al., 2017) on such degenerated
graphs. Therefore, our result is a natural generalization of the existing results for MLPs/CNNs.
Our generalization bound for message passing GNNs reveals that the governing terms of the bound
are similar to the ones of GCNs, i.e., the geometric series of the learned weights and the multiplica-
tive factor dl-1. The geometric series appears due to the weight sharing across message passing
steps, thus corresponding to the product term across layers in GCNs. The term dl-1 encodes the
key graph statistics. Our bound improves the dependency on the maximum node degree and the
maximum hidden dimension compared to the recent Rademacher complexity based bound (Garg
et al., 2020). Moreover, we compute the bound values on four real-world graph datasets (e.g., social
networks and protein structures) and verify that our bounds are tighter.
In terms of the proof techniques, our analysis follows the PAC-Bayes framework in the seminal
work of (Neyshabur et al., 2017) for MLPs/CNNs with ReLU activations. However, we make two
distinctive contributions which are customized for GNNs. First, a naive adaptation of the pertur-
bation analysis in (Neyshabur et al., 2017) does not work for GNNs since ReLU is not 1-Lipschitz
under the spectral norm, i.e., kReLU(X)k2 ≤ kXk2 does not hold for some real matrix X. Instead,
we construct the recursion on certain node representations of GNNs like the one with maximum
`2 norm, so that we can perform perturbation analysis with vector 2-norm. Second, in contrast to
(Neyshabur et al., 2017) which only handles the homogeneous networks, i.e., f(ax)=af(x) when
a ≥ 0, we properly construct a quantity of the learned weights which 1) provides a way to satisfy the
constraints of the previous perturbation analysis and 2) induces a finite covering on the range of the
quantity so that the PAC-Bayes bound holds for all possible weights. This generalizes the analysis
to non-homogeneous GNNs like typical MPGNNs.
The rest of the paper is organized as follows. In Section 2, we introduce background material
necessary for our analysis. We then present our generalization bounds and the comparison to existing
results in Section 3. We also provide an empirical study to support our theoretical arguments in
Section 4. At last, we discuss the extensions, limitations and some open problems.
2	Background
In this section, we first explain our analysis setup including notation and assumptions. We then
describe the two representative GNN models in detail. Finally, we review the PAC-Bayes analysis.
2.1	Analysis Setup
In the following analysis, we consider the K-class graph classification problem which is common
in the GNN literature, where given a graph sample z, we would like to classify it into one of the
predefined K classes. We will discuss extensions to other problems like graph regression in Section
5. Each graph sample z is a triplet of an adjacency matrix A, node features X ∈ Rn×h0 and output
label y ∈ R1×K, i.e. z =(A, X, y), where n is the number of nodes and h0 is the input feature
dimension. We start our discussion by defining our notations. Let N+ be the first k positive integers,
i.e., N+ = {1, 2,...,k}, ∣∙∣p the vector p-norm and ∣∣ ∙ ∣∣p the operator norm induced by the vector
p-norm. Further, ∣ ∙ ∣∣f denotes the FrobeniUs norm of a matrix, e the base of the natural logarithm
function log, A[i, j] the (i, j)-th element of matrix A and A[i, :] the i-th row. We use parenthesis
to avoid the ambiguity, e.g., (AB)[i, j] means the (i, j)-th element of the product matrix AB. We
then introduce some terminologies from statistical learning theory and define the sample space as Z,
z =(A, X, y) ∈ Z where X ∈ X (node feature space) and A ∈ G (graph space), data distribution
2
Published as a conference paper at ICLR 2021
iid
D, Z ~ D, hypothesis (or model) fw where fw ∈ H (hypothesis class), and training set S with Size
m, S = {z1,...,zm}. We make the following assumptions which also appear in the literature:
A1
A2
A3
A4
Data, i.e., triplets (A, X, y), are i.i.d. samples drawn from some unknown distribution D.
The maximum hidden dimension across all layers is h.
Node feature of any graph is contained in a '2-ball with radius B. Specifically, We have
∀i ∈ Nn+, the i-th node feature X[i, :] ∈ XB,h = {x ∈ Rh0 | Ph=0 1 x2 ≤ B2}.
We only consider simple graphs (i.e., undirected, no loops1 2, and no multi-edges) with max-
imum node degree as d - 1.
Note that it is straightforward to estimate B and d empirically on real-world graph data.
2.2	Graph Neural Networks (GNNs)
In this part, we describe the details of the GNN models and the loss function we used for the graph
classification problem. The essential idea of GNNs is to propagate information over the graph so
that the learned representations capture the dependencies among nodes/edges. We now review two
classes of GNNs, GCNs and MPGNNs, which have different mechanisms for propagating informa-
tion. We choose them since they are the most popular variants and represent two common types of
neural networks, i.e., feedforward (GCNs) and recurrent (MPGNNs) neural networks. We discuss
the extension of our analysis to other GNN variants in Section 5. For ease of notation, we define the
model to be fw ∈H: X×G→RK where w is the vectorization of all model parameters.
GCNs: Graph convolutional networks (GCNs) (Kipf & Welling, 2016) for the K-class graph clas-
sification problem can be defined as follows,
Hk = σk (LHk-I
Hl = 11nHl-lWl
n
(k-th Graph Convolution Layer)
(Readout Layer),	(1)
where k ∈ N+-1, Hk ∈ Rn×hk are the node representations/states, 1n ∈ R1×n is a all-one vector, l
is the number of layers.2 and Wj is the weight matrix of the j-th layer. The initial node state is the
observed node feature H0 = X. For both GCNs and MPGNNs, we consider l>1 since otherwise
the model degenerates to a linear transformation which does not leverage the graph and is trivial
to analyze. Due to assumption A2, Wj is of size at most h × h, i.e., hk ≤ h, ∀k ∈ N+-1. The
1	1	∙	1	1	T	1	1	1
graph Laplacian L is defined as, A = I + A, L = D 2 AD 2 where D is the degree matrix of
A. Note that the maximum eigenvalue of L is 1 in this case. We absorb the bias into the weight
by appending constant 1 to the node feature. Typically, GCNs use ReLU as the non-linearity, i.e.,
σi(x) = max(0, x), ∀i = 1,…，l - 1. We use the common mean-readout to obtain the graph
representation where Hl-1 ∈ Rn×hl-1, Wl ∈ Rhl-1×K, and Hl ∈ R1×K.
MPGNNs: There are multiple variants of message passing GNNs, e.g., (Li et al., 2015; Dai et al.,
2016; Gilmer et al., 2017), which share the same algorithmic framework but instantiate a few com-
ponents differently, e.g., the node state update function. We choose the same class of models as in
(Garg et al., 2020) which are popular in the literature (Dai et al., 2016; Jin et al., 2018) in order to
fairly compare bounds. This MPGNN model can be written in matrix forms as follows,
Mk = g(Co>utHk-1)
M k = CinMk
Hk = φ (XWι + P (Mk) W2)
Hl = 11nHl-1 Wl
n
(k-th step Message Computation)
(k-th step Message Aggregation)
(k-th step Node State Update)
(Readout Layer),	(2)
1Here loop means an edge that connects a vertex to itself, a.k.a., self-loop.
2We count the readout function as a layer to be consistent with the existing analysis of MLPs/CNNs.
3
Published as a conference paper at ICLR 2021
where k ∈ N+-1 , Hk ∈ Rn×hk are node representations/states and Hl ∈ R1×K is the output
representation. Here we initialize	H0	=	0.	W.l.o.g., we assume ∀k	∈	N+-1,	Hk	∈	Rn×h and
Mk ∈ Rn×h since h is the maximum hidden dimension. Cin ∈ Rn×c and Cout ∈ Rn×c (c is
the number of edges) are the incidence matrices corresponding to incoming and outgoing nodes3
respectively. Specifically, rows and columns of Cin and Cout correspond to nodes and edges respec-
tively. Cin [i, j] = 1 indicates that the incoming node of the j-th edge is the i-th node. Similarly,
Cout [i, j]= 1 indicates that the outgoing node of the j-th edge is the i-th node. g, φ, ρ are non-
linear mappings, e.g., ReLU and Tanh. Technically speaking, g : Rh → Rh, φ : Rh → Rh, and
ρ : Rh → Rh operate on vector-states of individual node/edge. However, since we share these func-
tions across nodes/edges, We can naturally generalize them to matrix-states, e.g., φ: Rn×h → Rn×h
Where φ(X)[i, :] = φ(X [i, :]). By doing so, the same function could be applied to matrices With
varying size of the first dimension. For simplicity, We use g, φ, ρ to denote such generalization to
matrices. We denote the Lipschitz constants ofg,φ,ρunderthe vector 2-norm as Cg,Cφ,Cρ respec-
tively. We also assume g(0) = 0, φ(0) = 0, and ρ(0)=0 and define the percolation complexity as
C = CgCφCρkW2k2 folloWing (Garg et al., 2020).
Multiclass Margin Loss: We use the multi-class γ-margin loss folloWing (Bartlett et al., 2017;
Neyshabur et al., 2017). The generalization error is defined as,
LD,γ (fw XzPDf KA)[y]≤ γ+m=x fw KA)j]),
(3)
Where γ>0 and fw(X, A) is the l-th layer representations, i.e., Hl = fw(X, A). Accordingly, We
can define the empirical error as,
Ls,γ(fw) = — X 1 (fw(X,A)[y] ≤ Y + max fw(X,A)j]).
m zi ∈S	j 6=y
(4)
2.3	Background of PAC-Bayes Analysis
PAC-Bayes (McAllester, 1999; 2003; Langford & ShaWe-Taylor, 2003) takes a Bayesian vieW of
the probably approximately correct (PAC) learning theory (Valiant, 1984). In particular, it assumes
that We have a prior distribution P over the hypothesis class H and obtain a posterior distribution Q
over the same support through the learning process on the training set. Therefore, instead of having
a deterministic model/hypothesis as in common learning formulations, We have a distribution of
models. Under this Bayesian vieW, We define the generalization error and the empirical error as,
LS,γ (Q)= Ew 〜Q[LS,Y (fw 儿	LD,γ (Q)= Ew 〜Q[LD,Y (fw )].
Since many interesting models like neural netWorks are deterministic and the exact form of the
posterior Q induced by the learning process and the prior P is typically unknoWn, it is unclear hoW
one can perform PAC-Bayes analysis. Fortunately, We can exploit the folloWing result from the
PAC-Bayes theory.
Theorem 2.1. (McAllester, 2003) (Two-sided) Let P be a prior distribution over H and let δ ∈
(0, 1). Then, with probability 1 - δ over the choice of an i.i.d. size-m training set S according to D,
for all distributions Q over H and any γ>0, we have
G (Q) ≤ ls,y (Q)+SDKL(Qm-+in -.
Here DKL is the KL-divergence. The nice thing about this result is that the inequality holds for all
possible prior P and posterior Q distributions. Hence, We have the freedom to construct specific
priors and posteriors so that We can Work out the bound. Moreover, McAllester (2003); Neyshabur
et al. (2017) provide a general recipe to construct the posterior such that for a large class of models,
including deterministic ones, the PAC-Bayes bound can be computed. Taking a neural netWork as
an example, We can choose a prior distribution With some knoWn density, e.g., a fixed Gaussian,
3For undirected graphs, We convert each edge into tWo directed edges.
4
Published as a conference paper at ICLR 2021
over the initial weights. After the learning process, we can add random perturbations to the learned
weights from another known distribution as long as the KL-divergence permits an analytical form.
This converts the deterministic model into a distribution of models while still obtaining a tractable
KL divergence. Leveraging Theorem 2.1 and the above recipe, Neyshabur et al. (2017) obtained the
following result which holds for a large class of deterministic models.
Lemma 2.2. (Neyshabur et al., 2017)4 Let fw (x) : X → RK be any model with parameters
w, and let P be any distribution on the parameters that is independent of the training data. For
any w, we construct a posterior Q(w + u) by adding any random perturbation u to w, s.t.,
P(maxx∈x ∣fw+u(x) — fw (x)∣∞ < γ) > 2. Then, for any Y, δ > 0, with probability at least
1 - δ over an i.i.d. size-m training set S according to D,for any w, we have:
LD,0(fw) ≤ Ls,γ(fw) + s2DKL弋+ u—kp) + lθg 8m.
This lemma guarantees that, as long as the change of the output brought by the perturbations is small
with a large probability, one can obtain the corresponding generalization bound.
3 Generalization Bounds
In this section, we present the main results: generalization bounds of GCNs and MPGNNs using a
PAC-Bayesian approach. We then relate them to existing generalization bounds of GNNs and draw
connections to the bounds of MLPs/CNNs. We summarize the key ideas of the proof in the main
text and defer the details to the appendix.
3.1	PAC-Bayes Bounds of GCNs
As discussed above, in order to apply Lemma 2.2, we must ensure that the change of the output
brought by the weight perturbations is small with a large probability. In the following lemma, we
bound this change using the product of the spectral norms of learned weights at each layer and a
term depending on some statistics of the graph.
Lemma 3.1. (GCN Perturbation Bound) For any B>0,l >1, let fw ∈H: X×G→RK be a
l-layer GCN. Then for any w, and x ∈ XB,h , and any perturbation u = vec({Ui}l=1) such that
∀i ∈ N+, ∣∣Uik2 ≤ 11lWiIl 2, the change in the output of GCN is bounded as,
∣fw+u(X,A) — fw(X, A)∣2 ≤ eBdl-1 (口 k WiI?) XX ∣Uk∣2.
The key idea of the proof is to decompose the change of the network output into two terms which
depend on two quantities of GNNs respectively: the maximum change of node representations
maxi IH0-ι[i,:] — H1-1[i,:]] and the maximum node representation maxi ∣Hι-ι[i,:]]. Here the
superscript prime denotes the perturbed model. These two terms can be bounded by an induction on
the layer index. From this lemma, we can see that the most important graph statistic for the stability
of GCNs is the maximum node degree, i.e., d —1. Armed with Lemma 3.1 and Lemma 2.2, we now
present the PAC-Bayes generalization bound of GCNs as Theorem 3.2.
Theorem 3.2. (GCN Generalization Bound) For anyB>0,l >1, let fw ∈H: X×G→RK
be a l layer GCN. Then for any δ, γ > 0, with probability at least 1 — δ over the choice of an i.i.d.
size-m training set S according to D, for any w, we have,
LD,0(fw) ≤ LS,γ(fw)+O
v	l	l	∖
B2dl-1l2hlog(lh) Q kWik2 P (kWikF/kWik2) + log 字
∖ _________________i=1	i=1_______________________
∖	γ 2m
∖ /
4The constants slightly differ from the original paper since we use a two-sided version of Theorem 2.1.
5
Published as a conference paper at ICLR 2021
Since it is easy to show GCNs are homogeneous, the proof of Theorem 3.2 follows the one for
MLPs/CNNs with ReLU activations in (Neyshabur et al., 2017). In particular, we choose the prior
distribution P and the perturbation distribution to be zero-mean Gaussians with the same diagonal
variance σ. The key steps of the proof are: 1) constructing a quantity of learned weights β =
(Ql=ι IlWik2)1〃 ； 2) fixing any ∕β, considering all β that are in the range ∖β - β∣ ≤ β∕l and choosing
σ which depends on β so that one can apply Lemma 3.1 and 2.2 to obtain the PAC-Bayes bound;
3) taking a union bound of the result in the 2nd step by considering multiple choices of β so that
all possible values of β (corresponding to all possible weight w) are covered. Although Lemma 2.2
and 3.1 have their own constraints on the random perturbation, above steps provide a way to set the
variance σ which satisfies these constraints and the independence w.r.t. learned weights. The latter
is important since σ is also the variance of the prior P which should not depend on data.
3.2	PAC-Bayes Bounds of MPGNNs
For MPGNNs, we again need to perform a perturbation analysis to make sure that the change of the
network output brought by the perturbations on weights is small with a large probability. Following
the same strategy adopted in proving Lemma 3.1, we prove the following Lemma.
Lemma 3.3. (MPGNN Perturbation Bound) For any B>0,l>1, let fw ∈H: X×G→RKbea
l-step MPGNN. Then for any w, and x ∈ XB,h , and any perturbation u = vec({U1,U2,Ul}) such
that η = max (旗]2, 1利2,联/2) ≤ 1 ,the change in the output of MPGNN is bounded as,
∖fw+u(X, A)- fw (X, A)12 ≤ eBlηk W1∣∣2∣∣ Wl ∣∣2Cφ(力:-；-
dC - 1
where C = CφCρCgkW2k2.
The proof again involves decomposing the change into two terms which depend on two quantities
respectively: the maximum change of node representations maxi IH0-ι [i,:] - Hι-ι[i,:] [ and the
maximum node representation maxi ∖Hι-ι[i,:]^. Then We perform an induction on the layer index
to obtain their bounds individually. Due to the weight sharing across steps, we have a form of
geometric series ((dC)ι-1 - 1)/(dC - 1) rather than the product of spectral norms of each layer as
in GCNs. Technically speaking, the above lemma only Works With dC6=1. We refer the reader to
the appendix for the special case ofdC =1. We noW provide the generalization bound for MPGNNs.
Theorem 3.4. (MPGNN Generalization Bound) For any B>0,l > 1, let fw ∈H: X×G→RK
be a l-step MPGNN. Then for any δ, γ > 0, with probability at least 1 - δ over the choice ofan i.i.d.
size-m training set S according to D, for any w, we have,
LD,0(fw) ≤ Ls,γ(fw) + O ( J (max(L-)?LhIog(Ih)∖w∖2 +lθg + j
whereζ=min(kW1k2,kW2k2,kWιk2),∖w∖22 = kW1k2F+kW2k2F+kWιk2F,C=CφCρCgkW2k2,
λ = kW1k2 kWιk2 ,and ξ = Cφ (dCC:1τ.
The proof also contains three steps: 1) since MPGNNs are typically non-homogeneous, e.g., When
any of φ, ρ, and g is a bounded non-linearity like Sigmoid or Tanh, We design a special quantity
of learned weights β = max(Z-1, (λξ)1/1). 2) fixing any β, considering all β that are in the range
∖β - β∖ ≤ β∕(l + 1) and choosing σ which depends on β so that one can apply Lemma 3.3 and 2.2
to work out the PAC-Bayes bound; 3) taking a union bound of the previous result by considering
multiple choices of β so that all possible values of β are covered. The case with dC =1is again
included in the appendix. The first step is non-trivial since we do not have the nice construction
as in the homogeneous case, i.e., normalizing the weights so that the spectral norms of weights
across layers are the same while the network output is unchanged. Moreover, the quantity is vital
to the whole proof framework since it determines whether one can 1) satisfy the constraints on the
random perturbation (so that Lemma 2.2 and 3.3 are applicable) and 2) simultaneously induce a
finite covering on its range (so that the bound holds for any w). Since it highly depends on the form
of the perturbation bound and the network architecture, there seems to be no general recipe on how
to construct such a quantity.
6
Published as a conference paper at ICLR 2021
Statistics	Max Node Degree d - 1	Max Hidden Dim h	Spectral Norm of Learned Weights
VC-Dimension (Scarselli et al., 2018) Rademacher	-	O (h4)	-
Complexity (Garg et al., 2020)	O (dl-1Plog(d2l-3))	O (h√ιogh)	O (λCξpiog(∣∣W2∣∣2λξ2))
Ours	O (dl-1)	O (√h log h)	O (λ	1+1 ξ1+1 PWFTWF^W)
Table 1: Comparison of generalization bounds for GNNs. “-” means inapplicable. l is the network
depth. Here C = CφCρCg∣∣W2∣∣2, ξ = Cφ嗯；1, Z = min(kW1k2, kW2k2, ∣W1k2), and
λ = kW1k2 kWl k2. More details about the comparison can be found in Appendix A.5.
3.3	Comparison with Other Bounds
In this section, we compare our generalization bounds with the ones in the GNN literature and draw
connections with existing MLPs/CNNs bounds.
3.3.1	Comparison with Existing GNN Generalization B ounds
We compare against the VC-dimension based bound in (Scarselli et al., 2018) and the most recent
Rademacher complexity based bound in (Garg et al., 2020). Our results are not directly comparable
to (Du et al., 2019) since they consider a “infinite-wide” class of GNNs constructed based on the
neural tangent kernel (Jacot et al., 2018), whereas we focus on commonly-used GNNs. Comparisons
to (Verma & Zhang, 2019) are also difficult since: 1) they only show the bound for one graph
convolutional layer, i.e., it does not depend on the network depth l; and 2) their bound scales as
O (λmTχ/m), where T is the number of SGD steps and λmax is the maximum absolute eigenvalue
of Laplacian L = D - A. Therefore, for certain graphs5, the generalization gap is monotonically
increasing with T, which cannot explain the generalization phenomenon. We compare different
bounds by examining their dependency on three terms: the maximum node degree, the spectral norm
of the learned weights, and the maximum hidden dimension. We summarize the overall comparison
in Table 1 and leave the details such as how we convert bounds into our context to Appendix A.5.
Max Node Degree (d - 1): The Rademacher complexity bound scales as O (dlτplog(d2l-3))
whereas ours scales as O(dl-1)6. Many real-world graphs such as social networks tend to have
large hubs (BarabaSi et al., 2016), which lead to very large node degrees. Thus, our bound would
be significantly better in these scenarios. It is noteworthy that if one further introduces some as-
sumption, e.g., φ is a squashing function like tanh as shown in (Garg et al., 2020), then one can
improve the above exponential dependency on the network depth l for both Rademacher complexity
and PAC-Bayes bounds.
Max Hidden Dimension h: Our bound scales as O(√h log h) which is tighter than the
Rademacher complexity bound O (h√log h) and the VC-dimension bound O(h4).
Spectral Norm of Learned Weights: As shown in Table 1, we cannot compare the dependencies
on the spectral norm of learned weights without knowing the actual values of the learned weights.
Therefore, we perform an empirical study in Section 4.
3.3.2	Connections with Existing Bounds of MLPs/CNNs
As described above, MLPs/CNNs can be viewed as special cases of GNNs. In particular, we have
two ways to show the inclusion relationship. First, we can treat each i.i.d. sample as a node and
the whole dataset as a graph without edges. Then conventional tasks (e.g., classification) become
node-level tasks (e.g., node classification) on this graph. Second, we can treat each i.i.d. sample as
a single-node graph. Then conventional tasks (e.g., classification) becomes graph-level tasks (e.g.,
graph classification). Since we focus on the graph classification, we adopt the second view. In
5Since λmax = maxv6=0(v>(D - A)v)/(v>v), we have λmax ≥ (D - A)[i, i] by choosing v = ei, i.e.,
the i-th standard basis. We can pick any node i which has more than 1 neighbor to make λmax > 1.
6Our bound actually scales as O (d(l+I)(I-2)/l) which is upper bounded by O (dl-1).
7
Published as a conference paper at ICLR 2021
IMDB-B
COLLAB
PAC Bayes
Rademacher
PROT 曰 NS	IMDB-M
PAC Bayes
Rademacher
5 Q.5Q.5.0
7.5.NS7.5.
Illl
φ□-ra> PUnOm 60—1
2.5 I
o.o L
PROTEINS
IMDB-M
IMDB-B
COLLAB
(a) MPGNNS With l = 2.	(b) MPGNNS With l = 4.
Figure 1: Bound evaluations on real-world datasets. The maximum node degrees (i.e., d- 1) of four
dataSetS from left to right are: 25 (PROTEINS), 88 (IMDB-M), 135 (IMDB-B), and 491 (COLLAB).
particular, MLPS/CNNS With ReLU activationS are equivalent to GCNS With the graph Laplacian
L = I (hence d =1). We leave the detailS of thiS converSion to Appendix A.6. We reState the
PAC-BayeS bound for MLPS/CNNS With ReLU activationS in (NeyShabur et al., 2017) aS folloWS,
LD,0(fw) ≤ LS,γ(fw)+O
∖
B2l2hlog(lh) YY kWik2 XX(kWikF∕kWik2) + log m) /γ2m].
i=1	i=1
Comparing it With our bound for GCNS in Theorem 3.2, it iS clear that We only add a factor dl-1 to
the firSt term inSide the Square root Which iS due to the underlying graph Structure of the data. If We
apply GCNS to Single-node graphS, the tWo boundS coincide Since d =1. Therefore, our Theorem
3.2 directly generalizeS the reSult in (NeyShabur et al., 2017) to GCNS, Which iS a Strictly larger claSS
of modelS than MLPS/CNNS With ReLU activationS.
4 Experiments
In thiS Section, We perform an empirical compariSon betWeen our bound and the Rademacher com-
plexity bound for MPGNNS. We experiment on 6 Synthetic dataSetS of random graphS (correSpond-
ing to 6 random graph modelS), 3 Social netWork dataSetS (COLLAB, IMDB-BINARY, IMDB-
MULTI), and a bioinformaticS dataSet PROTEINS from (Yanardag & ViShWanathan, 2015). In
particular, We create synthetic datasets by generating random graphs from the ErdoS-Renyi model
and the StochaStic block model With different SettingS (i.e., number of blockS and edge probabili-
ties). All datesets focus on graph classifications. We repeat all experiments 3 times With different
random initializations and report the means and the standard deviations. Constants are considered
in the bound computation. More details of the experimental setup, dataset statistics, and the bound
computation are provided in Appendix A.7.
As shoWn in Fig. 1 and Fig. 2, our bound is mostly tighter than the Rademacher complexity bound
With varying message passing steps l on both synthetic and real-World datasets. Generally, the larger
the maximum node degree is, the more our bound improves7 over the Rademacher complexity bound
(c.f ., PROTEINS vs. COLLAB). This could be attributed to the better dependency on d of our
bound. For graphs With large node degrees (e.g., social netWorks like TWitter have influential users
With lots of folloWers), the gap could be more significant. Moreover, With the number of steps/layers
increasing, our bound also improves more in most cases. It may not be clear to read from the figures
since the y-axis is in the log domain and its range differ from figure to figure. We also provide the
numerical values of the bound evaluations in the appendix for an exact comparison. The number of
steps is chosen to be no larger than 10 as GNNs are generally shoWn to perform Well With just a feW
steps/layers (Kipf & Welling, 2016; Jin et al., 2018). We found dC > 1 and the geometric series
((dC)l-1 - 1)∕(dC -1)》1 onall datasets which imply learned GNNs are not contraction mappings
(i.e., dC < 1). This also explains Why both bounds becomes larger With more steps. At last, We can
see that bound values are much larger than 1 Which indicates both bounds are still vacuous, similarly
to the cases for regular neural netWorks in (Bartlett et al., 2017; Neyshabur et al., 2017).
7Note that it may not be obvious from the figure as the y axis is in log domain. Please refer to the appendix
Where the actual bound values are listed in the table.
8
Published as a conference paper at ICLR 2021
φ□-ra> PUnOm 60—1① nra> PUno0□601
(a) MPGNNS With l = 2.
(b) MPGNNs with l = 4.
PAC Bayes
(c) MPGNNs with l = 6.	(d) MPGNNs with l = 8.
Figure 2: Bound evaluations on synthetic datasets. The maximum node degrees (i.e., d - 1) of
datasets from left to right are: 25 (ER-1), 48 (ER-2), 69 (ER-3), 87 (ER-4), 25 (SBM-1), and 36
(SBM-2). ‘ER-X' and 'SBM-X' denote the ErdoS-Renyi model and the stochastic block model with
the ‘X’-th setting respectively. Please refer to the appendix for more details.
5 Discussion
In this paper, we present generalization bounds for two primary classes of GNNs, i.e., GCNs and
MPGNNs. We show that the maximum node degree and the spectral norms of learned weights
govern the bound for both models. Our results for GCNs generalize the bounds for MLPs/CNNs
in (Neyshabur et al., 2017), while our results for MPGNNs improve over the state-of-the-art
Rademacher complexity bound in (Garg et al., 2020). Our PAC-Bayes analysis can be general-
ized to other graph problems such as node classification and link prediction since our perturbation
analysis bounds the maximum change of any node representation. Other loss functions (e.g., ones
for regression) could also work in our analysis as long as they are bounded.
However, we are far from being able to explain the practical behaviors of GNNs. Our bound values
are still vacuous as shown in the experiments. Our perturbation analysis is in the worst-case sense
which may be loose for most cases. We introduce Gaussian posterior in the PAC-Bayes framework
to obtain an analytical form of the KL divergence. Nevertheless, the actual posterior induced by the
prior and the learning process may likely to be non-Gaussian. We also do not explicitly consider the
optimization algorithm in the analysis which clearly has an impact on the learned weights.
This work leads to a few interesting open problems for future work: (1) Is the maximum node degree
the only graph statistic that has an impact on the generalization ability of GNNs? Investigating other
graph statistics may provide more insights on the behavior of GNNs and inspire the development
of novel models and algorithms. (2) Would the analysis still work for other interesting GNN ar-
chitectures, such as those with attention (VeliCkoViC et al., 2017) and learnable spectral filters (Liao
et al., 2019)? (3) Can recent advancements for MLPs/CNNs, e.g., the compression technique in
(Arora et al., 2018) and data-dependent prior of (ParradO-Hemandez et al., 2012), help further im-
proVe the bounds for GNNs? (4) What is the impact of the optimization algorithms like SGD on the
generalization ability of GNNs? Would graph structures play a role in the analysis of optimization?
9
Published as a conference paper at ICLR 2021
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Albert-Laszlo Barabasi et al. Network science. Cambridge university press, 2016.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In NIPS, pp. 6240-6249, 2017.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. Spatially-aware graph neural networks
for relational behavior forecasting from sensor data. arXiv preprint arXiv:1910.08233, 2019.
Guangyong Chen, Pengfei Chen, Chang-Yu Hsieh, Chee-Kong Lee, Benben Liao, Renjie Liao,
Weiwen Liu, Jiezhong Qiu, Qiming Sun, Jie Tang, et al. Alchemy: A quantum chemistry dataset
for benchmarking ai models. arXiv preprint arXiv:1906.09427, 2019a.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph iso-
morphism testing and function approximation with gnns. In NeurIPS, pp. 15894-15902, 2019b.
Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-
tured data. In International conference on machine learning, pp. 2702-2711, 2016.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In NeurIPS,
pp. 5723-5733, 2019.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy HirzeL Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In NIPS, pp. 2224-2232, 2015.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Vikas K Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. arXiv preprint arXiv:2002.06157, 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NIPS, pp. 1024-1034, 2017.
10
Published as a conference paper at ICLR 2021
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piece-
wise linear neural networks. In Conference on Learning Theory, pp. 1064-1068, 2017.
Isaac Henrion, Johann Brehmer, Joan Bruna, Kyunghyun Cho, Kyle Cranmer, Gilles Louppe, and
Gaspar Rochette. Neural message passing for jet physics. NIPS Workshop on Deep Learning for
Physical Sciences, 2017.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, pp. 8571-8580, 2018.
Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-
graph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
John Langford and John Shawe-Taylor. Pac-bayes & margins. In NIPS, pp. 439-446, 2003.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning
lane graph representations for motion forecasting. In ECCV, 2020.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In ICLR, 2019.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel ma-
chines, pp. 203-215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170, 1999.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR,
pp. 5115-5124, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. Pac-bayes
bounds with data dependent priors. The Journal of Machine Learning Research, 13(1):3507-
3531, 2012.
Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for
rgbd semantic segmentation. In ICCV, 2017.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller,
Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and
control. arXiv preprint arXiv:1806.01242, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik-chervonenkis dimension
of graph and recursive neural networks. Neural Networks, 108:248-259, 2018.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
11
Published as a conference paper at ICLR 2021
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
SaUrabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolUtional neUral net-
works. InKDD,pp.1539-1548, 2019.
TingwU Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning strUctUred policy with
graph neUral networks. In ICLR, 2018.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In KDD, pp. 1365-1374, 2015.
12