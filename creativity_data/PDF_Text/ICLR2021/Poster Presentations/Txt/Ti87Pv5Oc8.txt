Published as a conference paper at ICLR 2021
Meta-Learning with Neural Tangent Kernels
Yufan Zhou； Zhenyi Wang∖ Jiayi Xian, Changyou Chen, Jinhui Xu *
Department of Computer Science and Engineering, State University of New York at Buffalo
{yufanzho,zhenyiwa,jxian,changyou,jinhui}@buffalo.edu
Ab stract
Model Agnostic Meta-Learning (MAML) has emerged as a standard framework
for meta-learning, where a meta-model is learned with the ability of fast adapting
to new tasks. However, as a double-looped optimization problem, MAML needs to
differentiate through the whole inner-loop optimization path for every outer-loop
training step, which may lead to both computational inefficiency and sub-optimal
solutions. In this paper, we generalize MAML to allow meta-learning to be defined
in function spaces, and propose the first meta-learning paradigm in the Reproducing
Kernel Hilbert Space (RKHS) induced by the meta-model’s Neural Tangent Kernel
(NTK). Within this paradigm, we introduce two meta-learning algorithms in the
RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in
the MAML framework. We achieve this goal by 1) replacing the adaptation with a
fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically
based on the NTK theory. Extensive experimental studies demonstrate advantages
of our paradigm in both efficiency and quality of solutions compared to related
meta-learning algorithms. Another interesting feature of our proposed methods
is that they are demonstrated to be more robust to adversarial attacks and out-of-
distribution adaptation than popular baselines, as demonstrated in our experiments.
1	Introduction
Meta-learning (Schmidhuber, 1987) has made tremendous progresses in the last few years. It aims
to learn abstract knowledge from many related tasks so that fast adaption to new and unseen tasks
becomes possible. For example, in few-shot learning, meta-learning corresponds to learning a
meta-model or meta-parameters so that they can fast adapt to new tasks with a limited number of
data samples. Among all existing meta-learning methods, Model Agnostic Meta-Learning (MAML)
(Finn et al., 2017) is perhaps one of the most popular and flexible ones, with a number of follow-up
works such as (Nichol et al., 2018; Finn et al., 2018; Yao et al., 2019; Khodak et al., 2019a;b;
Denevi et al., 2019; Fallah et al., 2020; Lee et al., 2020; Tripuraneni et al., 2020). MAML adopts
a double-looped optimization framework, where adaptation is achieved by one or several gradient-
descent steps in the inner-loop optimization. Such a framework could lead to some undesirable issues
related to computational inefficiency and sub-optimal solutions. The main reasons are that 1) it is
computationally expensive to back-propagate through a stochastic-gradient-descent chain, and 2)
it is hard to tune the number of adaptation steps in the inner-loop as it can be different for both
training and testing. Several previous works tried to address these issues, but they can only alleviate
them to certain extents. For example, first order MAML (FOMAML) (Finn et al., 2017) ignores the
high-order terms of the standard MAML, which can speed up the training but may lead to deteriorated
performance; MAML with Implicit Gradient (iMAML) (Rajeswaran et al., 2019) directly minimizes
the objective of the outer-loop without performing the inner-loop optimization. But it still needs an
iterative solver to estimate the meta-gradient.
To better address these issues, we propose two algorithms that generalize meta-learning to the
Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model’s Neural Tangent Kernel
(NTK) (Jacot et al., 2018). In this RKHS, instead of using parameter adaptation, we propose to
perform an implicit function adaptation. To this end, we introduce two algorithms to avoid explicit
*The first two authors contribute equally. Correspondence to Changyou Chen (changyou@buffalo.edu).
tThe research of the first and fifth authors was supported in part by NSF through grants CCF-1716400 and
IIS-1910492.
1
Published as a conference paper at ICLR 2021
function adaptation: one replaces the function adaptation step in the inner-loop with a new meta-
objective with a fast-adaptive regularizer inspired by MAML; the other solves the adaptation problem
analytically based on tools from NTK so that the meta-objective can be directly evaluated on samples
in a closed-form. When restricting the function space to be RKHS, the solutions to the proposed
two algorithms become conveniently solvable. In addition, we provide theoretical analysis on our
proposed algorithms in the cases of using fully-connected neural networks and convolutional neural
networks as the meta-model. Our analysis shows close connections between our methods and the
existing ones. Particularly, we prove that one of our algorithms is closely related to MAML with
some high-order terms ignored in the meta-objective function, thus endowing effective optimization.
In summary, our main contributions are:
•	We re-analyze the meta-learning problem and introduce two new algorithms for meta-
learning in RKHS. Different from all existing meta-learning algorithms, our proposed
methods can be solved efficiently without cumbersome chain-based adaptations.
•	We conduct theoretically analysis on the proposed algorithms, which suggests that our pro-
posed algorithms are closely related to the existing MAML methods when fully-connected
neural networks and convolutional neural networks are used as the meta-model.
•	We conduct extensive experiments to validate our algorithms. Experimental results indicate
the effectiveness of our proposed methods, through standard few-shot learning, robustness
to adversarial attacks and out-of-distribution adaptation.
2	Preliminaries
2.1	Meta-Learning
Meta-learning can be roughly categorized as black-box adaptation methods (Andrychowicz et al.,
2016; Graves et al., 2014; Mishra et al., 2018), optimization-based methods (Finn et al., 2017),
non-parametric methods (Vinyals et al., 2016; Snell et al., 2017; Triantafillou et al., 2020) and
Bayesian meta-learning methods (Finn et al., 2018; Yoon et al., 2018; Ravi & Beatson, 2019). In
this paper, we focus on the framework of Model Agnostic Meta-Learning (MAML) (Finn et al.,
2017), which has two key components, meta initialization and fast adaptation. Specifically, MAML
solves the meta-learning problem through a double-looped optimization procedure. In the inner-loop,
MAML runs a task-specific adaptation procedure to transform a meta-parameter, θ, to a task-specific
parameter, {φm}Bm=1, for a total of B different tasks. In the outer-loop, MAML minimizes a total
loss of PBm=1 L(fφm) with respect to meta-parameter θ, where fφm is the model adapted on task m
that is typically represented by a deep neural network. It is worth noting that in MAML, one potential
problem is to compute the meta-gradient Vθ Pm=I L(fφm). It requires one to differentiate through
the whole inner-loop optimization path, which could be very inefficient.
2.2	Gradient Flow
Our proposed method relies on the concept of gradient flow. Generally speaking, gradient flow is a
continuous-time version of gradient descent. In the finite-dimensional parameter space, a gradient
flow is defined by an ordinary differential equation (ODE), dθt/dt = -Vθt F (θt), with a starting
point θ0 and function F : Rd → R. Gradient flow is also known as steepest descent curve.
One can generalize gradient flows to infinite-dimensional function spaces. Specifically, given a
function space H, a functional F : H → R, and a starting point f0 ∈ H, a gradient flow is similarly
defined as the solution of dft/dt = -VftF(ft). This is a curve in the function space H. In this
paper, we use notation VftF(ft), instead of VHF(ft), to denote the general function derivative of
the energy functional F with respect to function ft (Villani, 2008).
2.3	The Neural Tangent Kernel
Neural Tangent Kernel (NTK) is a recently proposed technique for characterizing the dynamics of a
neural network under gradient descent (Jacot et al., 2018; Arora et al., 2019; Lee et al., 2019). NTK
allows one to analyze deep neural networks (DNNs) in RKHS induced by NTK. One immediate
benefit of this is that the loss functional in the function space is often convex, even when it is highly
non-convex in the parameter space (Jacot et al., 2018) *. This property allows one to better understand
the property of DNNs. Specifically, let fθ be a DNN parameterized by θ. The corresponding NTK Θ
*Let H be the function space, F be the realization function for neural network defined in Section 3.2. Note
even if a functional loss (e.g., L2 loss) E : H → R is convex on H, the composition E ◦ F is in general not.
2
Published as a conference paper at ICLR 2021
is defined as: Θ(x1, x2)
∂fθ(Xl) ∂fθ(X2)
∂θ	∂θ
|
, where x1 , x2 are two data points. In our paper, we
will define meta-learning on an RKHS induced by such a kernel.
3	Meta-Learning in RKHS
We first define the meta-learning problem in a general function space, and then restrict the function
space to be an RKHS, where two frameworks will be proposed to make meta-learning feasible
in RKHS, along with some theoretical analysis. For simplicity, in the following we will hide the
superscript time t unless necessary, e.g., when the analysis involves time-changing.
3.1	Meta-Learning in Function Space
Given a function space H, a distribution of tasks P(T), and a loss function L, the goal of meta-
learning is to find a meta function f * ∈ H, so that it performs well after simple adaptation on a
specific task. Let Dmtr and Dmtest be the training and testing sets, respectively, sampled from a data
distribution of task Tm . The meta-learning problem on function space H is defined as:
f* =argminE(f), withE(f) =ETm hLAdapt(f, Dmtr), Dmtesti	(1)
where Adapt denotes some adaptation algorithms, e.g., several steps of gradient descent; E : H → R
is called energy functional, which is used to evaluate the model represented by the function f.
In theory, solving equation 1 is equivalent to solving the gradient flow equation df t/dt = -Vf t E (ft).
However, solving the gradient flow equation is generally infeasible, since i) it is hard to directly
apply optimization methods in function space and ii) the energy functional E contains an adaptation
algorithm Adapt, making the functional gradient infeasible. Thus, a better way is to design a special
energy functional so that it can be directly optimized without running the specific adaptation algorithm.
In the following, we first specify the functional meta-learning problem in RKHS, and then propose
two methods to derive efficient solutions for the problem.
3.2	Meta-Learning in RKHS
We consider a function f that is parameterized by θ ∈ RP, denoted as fθ, with P being the number
of parameters. Define a realization function F : RP → H that maps parameters to a function. With
these, we can then define an energy function in the parameter space as E , E ◦ F : RP → R with
◦ being the composition operator. Consequently, with an initialized θ0, we can define the gradient
flow of E(θt) in parameter space as: dθt/dt = -Vθt E(θt). In the following, we first establish an
equivalence between the gradient flow in RKHS and the gradient flow in the parameter space. We
then propose two algorithms for meta-learning in the RKHS induced by NTK.
Theorem 1 Let H be an RKHS induced by the NTK Θ offθ. With f0 = fθ0, the gradient flow of
E(ft) coincides with the function evolution of fθt driven by the gradient flow of E(θt).
The proof of Theorem 1 relies on the property of NTK (Jacot et al., 2018), and is provided in the
Appendix. Theorem 1 serves as a foundation of our proposed methods, which indicates that solving
the meta-learning problem in RKHS can be done by some appropriate manipulations. In the following,
we describe two different approaches termed Meta-RKHS-I and Meta-RKHS-II, respectively.
3.3	Meta-RKHS-I: Meta-Learning in RKHS without Adaptation
Our goal is to design an energy functional that has no adaptation component, but is capable of
achieving fast adaptation. For this purpose, we first introduce two definitions: empirical loss function
L(fθ, Dm) and expected loss function L(fθ). Let Dm = {xm,i, ym,i}in=1 be a set containing the
data of a regression task Tm. The empirical loss function L(fθ, Dm) and the expected loss function
Lm(fθ) can be defined as:
1n	2	1	2
L(fθ, Dm) = 2nΣllf (Xm,i) — ym,ill2,	Lmf )= Exm4 Df(Xm)- Ym	∙
n i=1
3
Published as a conference paper at ICLR 2021
Our idea is to define a regularized functional such that it endows the ability of fast adaptation in
RKHS. Our solution is based on some property of the standard MAML. We start from analyzing the
meta-objective of MAML with a k-step gradient-descent adaptation, i.e., applying k gradient-descent
steps in the inner-loop. The objective can be formulated as
k-1
θ* = arg minETm [L(fφ, Dmst)] , With φ = θ - α X V%L(f%, Dm),
θ	i=0
where α is the learning rate of the inner-loop, θo = θ, and θi+ι = θi 一 αVθiL(fθi, Dm)1 By
Taylor expansion, We have
k-1
ETm [L(fφ, Dmst)] ≈ ETm L(fθ ,Dtmst)-α X V&Lfa, Dm )Vθ Lf, Dmst)I . (2)
i=0
Since Dmtr and Dmtest come from the same distribution, equation 2 is an unbiased estimator of
k-1
Mk = ETm [Lm(fθ) - Xβi], where 仇=αV&Lm(fθJVeLm(fθ)|.	⑶
i=0
We focus on the case of k = 1, which is M1 = ETm [Lm(fθ)] - αETm [kVθLm(fθ)k2]. The
first term on the RHS is the traditional multi-task loss evaluated at θ for all tasks. The second term
corresponds to the negative gradient norm; minimizing it means choosing a θ with the maximum
gradient norm. Intuitively, when θ is not a stationary point of a task, one should choose the steepest
descent direction to reduce the loss maximally for a specific task, thus leading to fast adaptation.
The above understanding suggests us to propose the following regularized energy functional, Eeα, for
meta-learning in the RKHS induced with the NTK for fast function adaptation:
Ee(α, fθ) = ETm [Lm(fθ)-αkVfθLm(fθ)k2H],	(4)
where ∣∣ ∙ ∣∣h denotes the functional norm in H, and a is a hyper-parameter. The above objective is
inspired by the Taylor expansion of the MAML objective, but is defined in the RKHS induced by the
NTK. Its connection with MAML and some functional-space properties will be discussed later.
Solving the Function Optimization Problem To minimize equation 4, we first derive Theorem 2
to reduce the function optimization problem to a parameter optimization problem.
Theorem 2	Let fθ be a neural network with parameter θ and H be the RKHS induced by the NTK
Θ of fθ. Then, the following are equivalent
Ee(α,fθ) =M1, and∣VfθLm(fθ)∣2H= ∣VθLm(fθ)∣2 .
Theorem 2 is crucial to our approach as it indicates that solving problem equation 4 is no more difficult
than the original parameter-based MAML, although it only considers one-step adaptation case. Next,
we will show that multi-step adaptation in the parameter space can also be well-approximated by our
objective equation 4 but with a scaled regularized parameter α. In the following, we consider the
squared loss L. The case with the cross-entropy loss is discussed in the Appendix. We assume that
fθ is parameterized by either fully-connected or convolutional neural networks, and only consider
the impact of number of hidden layers L in our theoretical results.
Theorem 3	Let fθ be a fully-connected neural network with L hidden layers and ReLU activation
function, s1, ..., sL+1 be the spectral norm of the weight matrices, s = maxh sh, andα be the learning
rate of gradient descent. If α ≤ O (qr) with q = min(1/(LsL), L-1/(L+1)) and r = min(s-L, s),
then the following holds
|Mk -Ee(kα,fθ)| ≤ O(L).
Theorem 4	Let fθ be a convolutional neural network with L - l convolutional layers and l fully-
connected layers and with ReLU activation function, and dx be the input dimension. Denote by
Wh the parameter vector of the convolutional layer for h ≤ L - l, and the weight matrices of the
fully connected layers for L - l + 1 ≤ h ≤ L + 1. ∣∣∙∣∣2 means both the spectral norm ofa matrix
tFor ease of our later notation, we write the gradient Vθi L (thus the parameter as well) as a row vector.
4
Published as a conference paper at ICLR 2021
and the Euclidean norm of a vector. Define Sh = √dXk W hk2 if h = 1,...,L 一 l ,and ∣∣ W h∣∣2 if
L - l + 1 ≤ h ≤ L + 1. Let s = maxh sh and α be the learning rate of gradient descent. If
α ≤ O(qr) with q = min(1/(LsL), L-1/(L+1)) and r = min(s-L, s), the following holds
IMk- Ee(kα, fθ) | ≤ O (L).
The above Theorems indicate that, for a meta-model with fully-connected and convolutional layers,
the proposed Meta-RKHS-I can be an efficient approximation of MAML with a bounded error.
Comparisons with Reptile and MAML Similar to Reptile and MAML, the testing stage of Meta-
RKHS-I also requires gradient-based adaptation on meta-test tasks. By Theorem 1, we known that
gradient flow of an energy functional can be approximated by gradient descent in a parameter space.
Reptile with 1-step adaptation (Nichol et al., 2018) is equivalent to the approximation of the gradient
flow of Ee(α, fθ ) with α = 0, which does not include the fast-adaptation regularization as in our
method. For a fairer comparison on the efficiency, we will discuss the computational complexity later.
From the equivalent parameter-optimization form indicated in Theorem 2, we know that our energy
functional E (α, fθ ) is closely related to MAML. However, with this form, our method does not
need the explicit adaptation steps in training (i.e., the inner-loop of MAML), leading to a simpler
optimization problem. We will show that our proposed method leads to better results.
3.4 Meta-RKHS-II: Meta-Learning in RKHS with a Closed-form Adaptation
In this section, we present our second solution for meta-learning in RKHS by deriving a closed-form
adaptation function, i.e., we focus on a case where Adapt(f, Dmtr) is analytically solvable using the
theory of NTK. Specifically, we are given a loss function L, tasks Tm with randomly split training
set Dmtr = {xtmr,i, ytmr,i}in=1, and testing set Dmtest. Let θmt and fmt ,θ denote the parameters and the
corresponding function at time t adapted by task Tm from the meta parameter θ and meta function
fθ, respectively. From the NTK theory (Jacot et al., 2018; Arora et al., 2019; Lee et al., 2019), we
can write the function/parameter evolution as:
dθt	十 .	dft	θ	dθt	∂ft	θ | A	∂L(ft θ,	Dtr)	.
常=»mLfm° ,Dm),	and K=-dtm 潴f = X afm;Xmm θ(Xm“).
The above differential equation corresponds to the adaptation step, i.e., how to adapt the meta
parameter/function for task m. By the NTK theory, we can show that this admits closed-form
solutions. In our meta-learning settings, this indicates that no explicit adaptation steps are necessary.
To see why this is the case, we first investigate the regression case, where the loss function L is the
squared loss. Let X ∈ Dmtest be a test data point. As shown in Arora et al. (2019); Lee et al. (2019),
with a large enough neural network we can safely assume that NTK will not change too much during
the training. In this case, we can have a closed-form solution for fmt ,θ as
fm,θ(χ) = fθ(χ)+h(χ, Xm)H-1(χm, Xm) (e-tH(Xm,xm) - Df (Xm) - Ytr),⑸
where e is the matrix exponential map, which can be approximated by P ade´ approximation (M.Arioli
et al., 1996). H (Xtmr, Xtmr) is an n × n kernel matrix with its (i, j) element being Θ(χm,i, χm,j),
H(χ, Xtmr) is a 1 × n vector with its i-th element being Θ(χ, χm,i), fθ (Xtmr) ∈ Rn is the predictions
of all training data at the initialization, and Ytr ∈ Rn is the target value of the training data.
Specifically, at time t = ∞, we have
f∞,θ(χ) = fθ(χ)+h(χ, Xm)H-1(χm, Xm)(γtr-fθ(Xm)).	⑹
The above results allow us to directly define an energy functional by substituting Adapt(f, Dmtr ) in
equation 1 with its closed-form solution fmt ,θ. In other words, our new energy functional is
E(t,fθ)= ETm [Lm(fm,θ)],	⑺
where fmt ,θ is defined in equation 5, and Lm(fmt ,θ) is the expectation of L fmt ,θ, Dmtest . For
classification problems, we follow the same strategy as in Arora et al. (2019) to extend regression to
classification. Mores details can be found in the Appendix, including the algorithm in Appendix A.
5
Published as a conference paper at ICLR 2021
Table 1: Running time comparison per iteration with C1		= dxp + Lp2 and C2 = dxp + Ldxp2	
FOMAML	Reptile	Meta-RKHS-I	Meta-RKHS-II
Fully-connected O(n(k + 1)C1)	O(nkCι)	O(nC1)	O(nC1 + n3)
Convolutional	O(n(k + 1)C2)	O(nkC2)	O(nC2)	O(nC2 + n3)
On Potential Robustness of Meta-RKHS-II Our extensive empirical studies show that Meta-
RKHS-II is a more robust model than related baselines. We provide an intuitive explanation on
the potential robustness of Meta-RKHS-II, as we find current theories of both robustness machine
learning and NTK are insufficient for a formal explanation. Our explanation is based on some
properties of both the meta-learning framework and NTK: 1) Strong initialization (meta model): For
NTK to generalize well, we argue that it is necessary to start the model with a good initialization.
This is automatically achieved in our meta-learning setting, where the meta model serves as the
initialization for NTK predictions. Actually, this has been supported by recent research (Fort et al.,
2020), which shows that there is a chaotic stage in the NTK prediction with finite neural networks, and
the NTK regime can be reachable with a good initialization. 2) Low complex classification boundary:
It is known that NTK is a linear model in the NTK regime. Intuitively, generating adversarial samples
with a lower complex model should be relatively harder because there is less data in the vicinity of
the decision boundary compared to a more complex model, making the probability of the model
being attacked smaller. Thus we argue that our model can be more robust than standard meta learning
models. 3) Our NTK-based model is robust enough to adapt with different time steps. And these
finite time steps can be more robust to adversarial attacks than that of the infinite-time limit partly due
to the complexity of back-propagating gradients. We note each of the individual factors might not be
enough to ensure robustness. Instead, we argue it is the combination effect of these factors that lead
to robustness of our model. Formal analysis is out of the scope of this paper and left for future work.
Connection with Meta-RKHS-I The proposed two methods choose different strategies to avoid
explicit adaptation in meta-learning, which seem to be two very different algorithms. We prove below
theorem, which indicates that the difference of the underlying gradient flows of the two algorithms
indeed increases w.r.t. both T and the depth L of a DNN (we only consider impacts of T and L).
Theorem 5 Let fθ be a neural network with L hidden layers, with each layer being either fully-
connected or convolutional. Assume that kLk∞ < ∞. Then, error(T) = |E(T, fθ) 一 E(T, fθ)| Is a
non-decreasing function of T. Furthermore, for arbitrary T > 0 we have error(T) ≤ O T 2L+3 .
Actually, Meta-RKHS-II implicitly contains a term of functional gradient norm because E(T, fθ)=
ETm Lm(fθ) 一 R0T UVθtLm(fm,θ)∣∣ dt . The difference compared to Meta-RKHS-I mainly
comes from the fact that Meta-RKHS-I can be regarded as an approximation of time-discrete
adaptation, while Meta-RKHS-II is based on time-continuous adaptation. In our experiments, we
observe that Meta-RKHS-I is as fast as FOMAML, which means that it is more computationally
efficient than the standard MAML. Meanwhile Meta-RKHS-II is the more robust model in tasks of
adversarial attack and out-of-distribution adaptation.
Connection with iMAML Our proposed method is similar to the iMAML algorithm (Finn &
Levine, 2019) in the sense that both methods try to solve meta-learning without executing the
optimization path. Different from iMAML, which still relies on an iterative solver, our method only
needs to solve a simpler optimization problem due to the closed-form adaptation.
3.5 Time Complexity Analysis
We compare the time complexity of our proposed methods with other first-order meta-learning
methods. Without loss of generality, we analyze the complexity in the case of a L-layer MLP or
L-layer convolutional neural networks. Recall that dx is the input dimension. Assume each layer
has width (filter number) O(p). Let n be the data batch size, k the adaptation steps of inner-loop
optimization. We summarize the time complexity in Table 1, where we simply assume the complexity
of multiplying matrices with sizes a × b and b × c to be O(abc). Note in the meta-learning setting, n
is typically small, indicating the efficiency of our proposed methods.
6
Published as a conference paper at ICLR 2021
(a) Random Initialized
(b) Meta-RKHS-I
(c) Meta-RKHS-II
Figure 1: Performance of random initialized network and our methods. The models before/after
adaptation are shown in dotted/dashed lines, samples used for adaptation are also shown in the figure.
4 Experiments
We conduct a set of experiments to evaluate the effectiveness of our proposed methods, including
a sine wave regression toy experiment, few-shot classification, robustness to adversarial attacks,
out-of-distribution generalization and ablation study. Due to space limit, more results are provided in
the Appendix. We compare our models with related baselines including MAML (Finn et al., 2017),
the first order MAML (FOMAML) (Finn et al., 2017), Reptile (Nichol et al., 2018) and iMAML
(Rajeswaran et al., 2019). Results are reported as mean and variance over three independent runs.
4.1	Regression
Following Finn et al. (2017); Nichol et al. (2018), we first test our proposed methods on the 1-
dimensional sine wave regression problem. This problem is instructive, where a model is trained on
many different sine waves with different amplitudes and phases, and tested by adapting the trained
model to new sine waves with only a few data points using a fixed number of gradient-descent steps.
Following Finn et al. (2017); Nichol et al. (2018), we use a fully-connected neural network with 2
hidden layers and the ReLU activation function. The results are shown in Figure 1.
4.2	Few-shot Image Classification
For this experiment, we choose two popular datasets adopted for meta-learning: Mini-ImageNet
and FC-100 (Oreshkin et al., 2018). The cross-entropy loss is adopted for Meta-RKHS-I; while the
squared loss is used for Meta-RKHS-II following Arora et al. (2019); Novak et al. (2019). Similar to
Finn et al. (2017), the model architecture is set to be a four-layer convolutional neural network with
ReLU activation. The filter number is set to be 32. The Adam optimizer (Kingma & Ba, 2015) is
used to minimize the energy functional. Meta batch size is set to be 16 and learning rates are set to be
0.01 for Meta-RKHS-II.
The results are shown in Table 2. Note
the results of Reptile is different from
those in Nichol et al. (2018), because
we re-evaluate it under the same set-
ting as Finn et al. (2017), i.e., 10 steps
of adaptation is applied during testing.
Our results of iMAML is based on the
implementation of Spigler (2019). It
is observed that our proposed methods
Table 2: Few-shot classification results on Mini-ImageNet
and FC-100.
Algorithm	Mini-ImageNet		FC-100	
	5 Way 1 S hot	5 Way 5 Shots	5 Way 1 S hot	5 Way 5 Shots
MAML	48.70 ± 1.84%	63.11 ± 0.93%	38.00 ± 1.95%	49.34 ± 0.97%
FOMAML	48.07 ± 1.75%	63.15 ± 0.91%	37.73 ± 1.93%	49.05 ± 0.99%
IMAML	49.30 ± 1.88%	64.89 ± 0.95%	38.38 ± 1.70%	49.41 ± 0.80%
Reptile	49.70 ± 1.83%	65.91 ± 0.84%	38.40 ± 1.94%	50.50 ± 0.87%
META-RKHS-I	51.10 ± 1.82%	66.19 ± 0.80%	38.90 ± 1.90%	51.47 ± 0.86%
META-RKHS-II	50.53 ± 2.09%	65.40 ± 0.91%	41.20 ± 2.17%	51.36 ± 0.96
achieve better accuracy than different baselines. Interestingly, our Meta-RKHS-I performs better
than FOMAML (this is also the case in other experiments), although they share a similar objective.
We conjecture the reason is because our Meta-RKHS-I restricts the function to be in an RKHS,
making the functional space smaller thus easier to optimize compared to the unrestricted version of
FOMAML. In terms of our two algorithms, there is not always a winner on all the tasks. We note that
Meta-RKHS-I is more efficient in training. However, we show below that Meta-RKHS-II is better in
terms of robustness to adversarial attacks and out-of-distribution generalization.
4.3	Robustness to Adversarial Attacks
We now compare the adversarial robustness of our methods with other popular baselines. We adopt
both white-box and black-box attacks in this experiment. For the white-box attacks, we adopt strong
attacks including the PGD Attack (Madry et al., 2017), BPDA attack (Athalye et al., 2018) and
7
Published as a conference paper at ICLR 2021
Black-box attack
Black-box attack
Black-box attack
1	10	20	30	40	50	60	1	3	5	7	9	nl3	15 ɪ 50	100	150	200	250	300
Queries	Queries	Queries
——Reptile Meta-RKHS-I ——IMAML ——Meta-RKHS-II	Meta-RKHS-ll.tlOO.PQl	——Meta-RKH5-ll.tlOO.PQ2 ——MAML FOMAML
Figure 2: Black-box attack on Mini-ImageNet and FC-100. Mini-ImageNet 5-way 1-shot (left),
FC-100 5-way 1-shot (middle) and Mini-ImageNet 5-way 5-shot (right).
Figure 3: BPDA attack on Mini-ImageNet 5-way 5-shot (left) and FC-100 5-way 5-shot (right).
Figure 4: SPSA attack on Mini-ImageNet 5-way 5-shot (left) and FC-100 5-way 5-shot (right).
SPSA attack (Uesato et al., 2018). For PGD attack, We use '∞ norm and compare the results on
Mini-imagenet and FC-100. We compare the robust accuracy with different magnitude with 20-step
attack With a step size of 2/255. For BPDA attack, We apply median smoothing, JPEGFilter and
BitSqueezing as input transformation adapted from (Guo et al., 2018) as defense strategies. For SPSA
attack, We folloW (Uesato et al., 2018) and set the Adam learning rate 0.01, perturbation size δ = 0.01.
For Black-box attack, We adopt the strong query efficient attack method (Guo et al., 2019). FolloW
the setting of Guo et al. (2019), We use a fixed step size of 0.2.
We consider both finite-time and infinite-time adaptation in this experiment. For finite-time adaptation,
the Pade approximation with P = Q = 1 and P = Q = 2 to approximate the matrix exponential
are considered (Butcher & Chipman, 1992). We use Meta-RKHS-II_t100_PQ1 and Meta-RKHS-
II_t100_PQ2 to denote methods using finite time t = 100, P = Q = 1 or P = Q = 2, respectively.
We observe other finite time t makes similar predictions, thus we only consider t = 100. The
results from the black-box attack in Figure 2 indicate the robustness of our Meta-RKHS-II. In fact,
the gaps are significantly large, making it the only useful robust model in the adversarial-attack
setting. Our Meta-RKHS-I is not as robust as Meta-RKHS-II, but still slightly outperforms other
baselines. Regarding the white-box attack, results in Figure 3, 4 and 5 again show that our proposed
Meta-RKHS-II is significantly more robust than baselines under the three strong attacks. It is also
interesting to see that our Meta-RKHS-I performs slightly better than Meta-RKHS-II in some rare
cases, e.g., in the Mini-ImageNet 5-way 1-shot case when the attack magnitude is not too small. More
results are presented in the Appendix.
8
Published as a conference paper at ICLR 2021
→- MAML →- FOMAML →- IMAML —Reptile	Meta-RKHS-I →- Meta-RKHS-H →- Meta-RKHS-Il_tlOO_PQl Meta-RKHS-ll_tl00_PQ2
Figure 5: '∞ norm PGD attack on Mini-ImageNet and FC-100. Mini-ImageNet 5-way 5-shot (left),
Mini-ImageNet 5-way 1-shot (middle) and FC-100 5-way 5-shot (right).
Table 4: Meta-RKHS-II with different time t.
	TIME t	t=0.1	t = 1	t=10	t= 100	t=∞
	5 Way 1 Shot	49.67 ± 2.23%	48.27 ± 2.23%	50.53 ± 2.09%	49.13 ± 2.19%	48.70 ± 2.28%
Mini-ImageNet	5 Way 5 Shots	64.51 ± 0.93%	64.28 ± 0.98%	65.40 ± 0.91%	64.24 ± 1.06%	64.95 ± 0.96%
c,r~, ι ∏∏	5 Way 1 Shot	36.50 ± 2.10%	38.80 ± 2.32%	41.20 ± 2.17%	38.80 ± 2.21%	37.60 ± 2.13%
FC-100	5 Way 5 Shots	48.35 ± 1.02%	49.79 ± 1.04%	51.36 ± 0.96%	48.59 ± 1.09%	49.48 ± 0.98%
4.4	Out-of-distribution Generalization
We adopt similar strategy in (Lee et al., 2020) to test a model’s ability of generalizing to out-of-
distribution datasets. In this setting, the state of arts are achieved by Bayesian TAML (Lee et al.,
2020). Different from their setting that considers any-shot learning with maximum number of
examples for each class being as large as 50, we only focus on the standard 1 or 5 shot learning.
We thus modify their code to accommodate our standard setting. The CUB (Wah et al., 2011) and
VGG Flower Nilsback & Zisserman (2008) are fine-grained datasets used in this experiment, where
all images are resized to 84 × 84. We follow Lee et al. (2020) to split these datasets into meta
training/validation/testing sets. We first train all the methods on Mini-ImageNet or FC-100 datasets,
then conduct meta-testing on CUB and VGG Flower datasets. The results are shown in Table 3. Again,
our methods achieve the best results, outperforming the state-of-art method with our Meta-RKHS-II,
indicating the robustness of our proposed methods. More results are presented in the Appendix.
4.5 Ablation Study
We conduct several ablation studies,
including: comparing Reptile with
Meta-RKHS-I under different adap-
tation steps (results shown in the Ap-
pendix), testing the impact of choos-
ing different time t in Meta-RKHS-II
(results shown in Table 4) and the im-
pact of network architecture with dif-
Table 3: Meta testing on different out-of-distribution datasets
with model trained on Mini-ImageNet.
Algorithm	5 way 1 shot		5 way 5 shot	
	CUB	VGG Flower	CUB	VGG Flower
MAML	34.23 ± 1.52%	52.98 ± 1.76%	52.36 ± 0.94%	67.52 ± 1.30%
FOMAML	35.32 ± 1.69%	53.86 ± 1.64%	52.02 ± 0.71%	68.83 ± 1.16%
Reptile	35.61 ± 1.38%	53.57 ± 1.58%	51.93 ± 0.89%	71.62 ± 1.25%
IMAML	40.55 ± 0.61%	54.97 ± 0.80%	46.31 ± 2.03%	60.67 ± 1.91%
BAYESIAN TAML(SOTA)	41.57 ± 0.60%	58.56 ± 0.66%	61.78 ± 0.56%	77.95 ± 0.46%
META-RKHS-I	36.73 ± 1.26%	54.79 ± 1.61%	54.19 ± 0.73%	72.76 ± 1.08%
META-RKHS-II	45.36 ± 0.87%	60.80 ± 1.02%	65.21 ± 0.64%	78.25 ± 0.49%
ferent number of CNN feature channels (results shown in the Appendix). It is interesting to see that
a finite-time (around t = 10) achieves the best accuracy, although the infinite-time case guarantees
a stationary point. This indicates that a stationary point achieved by limited training data in the
adaptation step is not always the best choice, because the limited training data might easily overfit the
model, thus achieving worse test results.
5 Conclusion
We develop meta-learning in RKHS, and propose two practical algorithms allowing efficient adap-
tation in the function space by avoiding some complicated adaptations as in traditional methods.
We show connections between our proposed methods and existing ones. Extensive experiments
suggest that our methods are more effective, achieve better generalization and are more robust against
adversarial attacks and out-of-distribution generalization, compared to popular strong baselines.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. volume 97 of Proceedings ofMachine Learning Research, pp. 242-252,
Long Beach, California, USA, 09-15 JUn 2019. PMLR. URL http://proceedings.mlr.
press/v97/allen-zhu19a.html.
Marcin Andrychowicz, Misha Denil, Sergio G6mez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient
descent. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, pp. 3981-3989. 2016.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018.
J. C. Butcher and F. H. Chipman. Generalized pad6 approximations to the exponential function. BIT
Numerical Mathematics, 32:118-130, 1992.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regularization. In https://arxiv.org/abs/1903.10399, 2019.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based
model-agnostic meta-learning algorithms. In International Conference on Artificial Intelligence
and Statistics, 2020.
Chelsea Finn and Sergey Levine. Meta-learning: from few-shot learning to rapid reinforcement
learning. In ICML 2019 Meta-Learning Tutorial, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems. 2018.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. In Advances in Neural Information
Processing Systems, 2020.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. In
https://arxiv.org/abs/1410.5401, 2014.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018.
Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q. Weinberger.
Simple black-box adversarial attacks. In International Conference on Machine Learning, 2019.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In International Conference on Machine Learning, 2019a.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning
methods. In Advances in Neural Information Processing Systems, 2019b.
10
Published as a conference paper at ICLR 2021
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and Sung Ju
Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks.
In International Conference on Learning Representations, 2020.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems 32, pp. 8572-8583.
Curran Associates, Inc., 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2017.
M.Arioli, B.Codenotti, and C.Fassino. The P ade´ method for computing the matrix exponential.
Linear Algebra and its Applications, June 1996.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In International Conference on Learning Representations, 2018.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. In
https://arxiv.org/abs/1803.02999, 2018.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In Sixth Indian Conference on Computer Vision, Graphics and Image Processing, 2008.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron, Daniel A. Abolafia,
Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2019.
Boris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric
for improved few-shot learning. In Advances in Neural Information Processing Systems, 2018.
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in Neural Information Processing Systems. 2019.
Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In International Conference on
Learning Representations, 2019.
Filippo Santambrogio. Euclidean, Metric, and Wasserstein gradient flows: an overview, 2016.
Jurgen Schmidhuber. Evolutionary principles in self-referential learning. Diploma thesis, Technische
Universitat Munchen, Germany, 14 May 1987.
Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, 2017.
Giacomo Spigler. Meta-learnt priors slow down catastrophic forgetting in neural networks. arXiv
e-prints, art. arXiv:1909.04170, Sep 2019.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-
dataset: A dataset of datasets for learning to learn from few examples. In International Conference
on Learning Representations, 2020.
Nilesh Tripuraneni, Chi Jin, and Michael I. Jordan. Provable meta-learning of linear representations.
In https://arxiv.org/abs/2002.11684, 2020.
Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk
and the dangers of evaluating against weak attacks, 2018.
C Villani. Optimal transport - Old and new, volume 338, pp. xxii+973. 01 2008. doi: 10.1007/
978-3-540-71050-9.
11
Published as a conference paper at ICLR 2021
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In https://arxiv.org/pdf/1606.04080.pdf, 2016.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. In Technical Report CNS-TR-2011-001, California Institute of Technology,
2011.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning. In
International Conference on Machine Learning, 2019.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems.
2018.
12
Published as a conference paper at ICLR 2021
A Algorithms
Our proposed algorithms for meta-learning in the RKHS are summarized in Algorithm 1.
Algorithm 1 Meta-Learning in RKHS
Require: p(T): distribution over tasks, randomly initialized neural network parameters θ.
while not done do
Sample a batch of tasks {Tm}m=ι ~ p(T)
for all Tm do
Sample a batch of data points Dm or Sample two batches of data points Dmtr, Dmtest.
end for
Evaluate the energy functional by equation 4 with {Dm}Bm=1 or Evaluate the energy functional
by equation 7 with {Dmtr, Dmtest}Bm=1. Minimize the energy functional w.r.t θ.
end while
B Proof of Theorem 1
Theorem 1 If fθ is a neural network with parameter θ ∈ RP and H is the Reproducing Kernel
Hilbert Space (RKHS) induced by Θ, where Θ is the Neural Tangent Kernel (NTK) of fθ, then
with initialization f0 = fθ0, the gradient flow of E(ft) coincides with the function evolution of fθt
induced by the gradient flow of E(θt).
Proof Without loss of generality, we can rewrite E(f) = ETm {E(xm,y ) [C(f(xm), ym)]} with
some function C(∙, ∙).
For a neural network fθ with parameter θ ∈ RP , the gradient flow of E in RP is
dθt
Ir = -vθtE(θ ).
We have
dθt
E = -Vθt (E。F)(θt)
-ETm{E(xm,ym) [vθtC(fθt(xm), ym)]}
dC ft (Xm ), ym) fθ (Xm)-
∂fθt (xm)	∂θt
We know that the dynamics offθt is
dfθt	dθt ∂fθt |
	-	2--
dt---------------dt ∂θt
∂C(fθt(Xm), ym) ∂fθt(Xm)
∂fθt(Xm)
∂θt
「dC(fθt (Xm), Nm ∂fθt(X) ∂fθt |]]
_∂fθt (Xm)	∂θr~ 而 ∖ ʃ
「dC(fθt (Xm), Nm
_	dfθt (Xm)
(8)
where Θt is the Neural Tangent Kernel of neural network fθt (Jacot et al., 2018).
If Ht is the Reproducing Kernel Hilbert Space induced by a kernel Θt and Vxm : H → R is the
evaluation functional at Xm , which is defined as
Vxm (f) = f(Xm),
13
Published as a conference paper at ICLR 2021
then for an arbitrary function g and a small perturbation , we have
hVf Vxm (f),gi = iim Vxm (J+⑹-Vxm (J)
→0
hVf Km(f),gi = l→0 f(xm) + eg(χm)-f (Xm)
hVf Vxm (J), gi = g(xm)
hVf Km (f),gi = hΘt(Xm, ∙),gi
V f Vxm (J) = θt(Xm, ∙)
Vff(Xm) = Θt(Xm, ∙).
With an initial function f0 = fθ0 ∈ H, the gradient flow of E in H is
f = -Vft E Ift).
We have
dft
d^j~ = -ETm {E(xm,ym.) [Vft C(J(Xm), ym)]}
=-ETm {E(xm,ym) ]	^ Vft ^m)] }
=-ETm hi j Qfxm⑼
We can complete the proof by comparing equation 8 and equation 9.	■
C	Proof of Theorem 2
Theorem 2 If fθ is a neural network with parameter θ and H is the Reproducing Kernel Hilbert
Space (RKHS) induced by Θ, where Θ is the Neural Tangent Kernel (NTK) of fθ, then
M1 =Ee(α,fθ), and β0 = αkVθLm(fθ)k2 =αkVfθLm(fθ)k2H.
Proof Without loss of generality, we rewrite Lm (fθ) = Exm,ym [C(fθ (Xm), ym)].
In regression task, we have
C (fθ (Xm), Ya) = 1llfθ (Xm)- Nm |f
. In classification task, we have
C (fθ (Xm ), Nm ) = Nm bg(fθ (Xm))|,
where log is element-wise logarithm operation.
kVθLm(fθ)k2
=Vθ Lm(fθ )Vθ Lm(fθ )|
Vθ Exm,ym [C (fθ (Xm), Nm)] Vθ Exm,ym [C (fθ (Xm), Nm)]|
一 ∂C (fθ (Xm), Ym ) ∂fθ (Xm) ] E	j ∂fθ (Xm) | ∂C (fθ (Xm), Ym) 「
一~∂fθ (Xm)	∂θ~∖ xm,ym [~^θ	∂fθ (Xm)_ _
Exm,ym
-dC(fθ (Xm), ym) dfθ (Xm) d fθ (Xm) | dC(fθ (Xm4, Y'm) 1
一	∂fθ (Xm)	∂ θ	∂ θ	∂fθ (Xm)	一
-aC(fθ(Xm), ym) C(Y γ0 ) dC(fθ(Xm), ym) |] 1
一	∂fθ (Xm)	( m, m)	∂fθ (Xm)	J ʃ
14
Published as a conference paper at ICLR 2021
FC (fθ (Xm), ym)
_	∂fθ (Xm)
,dC (fθ (Xm), Ym)
∂fθ (Xm)
Θ(Xm, ∙)
Vfθ fθ(Xm)
,y0m
FCf (Xm), ymJ
-	dfθ(Xm)
-aCfθ(Xm), ymj
-	dfθ(Xm)
Vfθfθ(X0m)
VfθLm(fθ),VfθLm(fθ) H
kVfθLm(fθ)k2H,
where(•，∙)h is the inner product in Reproducing Kernel Hilbert Space (RKHS) H. In the above
equations, we use the definition of Neural Tangent Kernel (NTK), the property of inner product in
RKHS, the definition of evaluation functional and its gradient in RKHS.
Recall that
Ee(α, fθ) = ETm Lm(fθ)-αkVfθLm(fθ)k2H
and
k-1
Mk = ETm Lm(fθ) - Xβi ,
i=0
where βi = aV%LmfNa Lm(fθ )| and θo = θ, θi+ι = θi - aV%Lf, Dm). The result is
straightforward now.
D	Proof of Theorem 3
The proof techniques we use are similar to some previous works such as (Arora et al., 2019; Allen-Zhu
et al., 2019). We summaries some of the differences. Different from previous works that typically
assume a neural network is Gaussian initialized, we do not have such an assumption as we are trying
to learn a good meta-initialization in the meta-learning setting. Previous works try to investigate the
behavior of models during training, while we focus on revealing the connection between different
meta-learning algorithms. Previous work focuses on single-task regression/classification problems,
while we focus on meta-learning problem.
Theorem 3 Let fθ be a fully-connected neural network with L hidden layers and ReLU activation
function, s1, ..., sL+1 be the spectral norm of the weight matrices, s = maxh sh, andα be the learning
rate of gradient descent. If α ≤ O (qr) with q = min(1/(LsL), L-1/(L+1)) and r = min(s-L, s),
then the following holds
lEe(kα, fθ) - Mk | ≤ O (L).
Proof We first prove the case of k = 2, i.e. applying a two-step gradient descent adaptation in
MAML.
We need to prove the following theorem first.
Theorem 6 Let fθ be a fully-connected neural network with L hidden layers, and X be a data
sample. Represent the neural network by fθ (X) = σ(σ(...σ(X W1)...WL-1)WL)WL+1, where
W1, ..., WL+1 denote the weight matrices, and σ is the ReLU activation function. Let s1, ..., sL+1
be the spectral norm of weight matrices, and s = maxh sh. Let α be the learning rate of gradient
descent, and f θ(x) be the resulting value after one step of gradient descent, and k ∙ ∣∣f be the
Frobenius norm.
If α ≤ O(qs-L), where q = min(1/(LsL), L-1/(L+1) ), then
Il dfθ(X) - dfθ(X) Il ≤ O(	1	)
Il ∂θ	∂θ	IIf -	s√l+γ).
Remark 1 Theorem 6 states that for a neural network with L hidden layers, if the learning rate of
gradient descent is bounded, then the norm of derivative w.r.t all the parameters will not change
15
Published as a conference paper at ICLR 2021
too much, although there are O(Lm2) parameters, where m denotes the maximum width of hidden
layers. We use row vector instead of column vector for consistency, while it does not affect our results.
For simplicity, we will write gh(x) as gh. The bias terms in the neural network are introduced by
adding an additional coordinate thus omitted in Theorem 6. Without loss of generality, we can assume
k x k ≤ 1, which can be done by data normalization in pre-processing.
Let gh(x) = σ(σ(...σ(x W1)...Wh-1)Wh) be the activation at hth hidden layer and g0 (x) =
x, gL+1 = fθ(x). Define diagonal matrices Dh, where D(hi,i) = 1{gh-1W h ≥ 0} and
bh	Idy,	ifh=L+1
bh+1 (W h+1)|Dh,	otherwise
where Idy is a dy × dy identity matrix. We first prove the following Lemma.
Lemma 7 Given a neural network as stated in Theorem 6, let ∣∣ ∙ ∣2 denote the spectral norm,
4Wh = Wh 一 Wh denote some perturbation on weight matrices, gh(x) denote the resulting value
after perturbation, and 4gh(x) = gh (x) — gh (x). If S ≥ 1 and ∣∣4W h∣2 ≤ O(s-L∕L) for all h,
then
∣4ghk≤ O(τ⅛+r)；
LSL-h+1
If S < 1 and ∣4Wh∣2 ≤ O(q) for all h, where q = min(1/(LSL), L-1/(L+1)) and r = max(q, S),
then
k4ghk≤ O(rh-1q) = I O(LL小),
O(L-h/(L+1) ),
if 1/(LsL) ≤ L-1/(L+1)
if 1/(LsL) > L-1/(L+1).
Proof Proof of Lemma 7 is based on induction.
We first prove the case of S ≥ 1. Note that g0 = x, thus 4g0 = 0 ≤ O( L l-0+i ) always holds.
For 4g1, we have
k4g1k = kσ(x WI)-σ(x W1 )k
≤ k x W 1 - x W 1 k, due to the property of ReLU activation
≤ kxkk4W1k2
≤ O(J-L).
LsL
Thus, the hypothesis holds for 4g1.
Now, assume that the hypothesis holds for 4gh, then we have
k4gh+1k = kσ(ghWh+1) - σ(ghWh+1)k
≤ kghWh+1 - ghWh+1k, due to the property of ReLU activation
≤ ∣∣ghWh+1 + gh4Wh+1 - ghWh+1k
≤ ∣4gh∣∣Wh+1∣2 + kghk∣4Wh+1∣2
≤ O(s)∣4gh∣ + ∣gh + 4gh∣∣4W h+1∣2
≤ O(s)∣4gh∣ +O(sh)∣4Wh+1∣2+∣4gh∣∣4Wh+1∣2
≤ O(S)O(L⅛)+ O(Sh)O(Lb)+ O(L⅛)O(Lh)
≤ O(LSL-h).
The last three inequalities come from the fact that gh = σ(σ(...σ(x W1)...Wh-1)Wh) ≤ O(Sh)
and S ≥ 1. Thus, we have proved the Lemma in the case S ≥ 1.
Now, we prove the first part of the case of S < 1, i.e. ∣4gh∣ ≤ O(rh-1q). Because 4g0 = 0, thus
the hypothesis for 4g0 always holds.
16
Published as a conference paper at ICLR 2021
For 4g1, we have
k4g1k = kσ(x WI)-σ(x W 1)k
≤ k X W1 - X W 1k
≤ kxkk4W1k2
≤ O(q).
Thus, the hypothesis holds for 4g1.
Now, we assume that the hypothesis holds for 4gh . Then, we have
k4gh+1k = kσ(ghWh+1)- σ(ghWh+1)k
≤ k/Wh+1 - ghWh+1k
≤ k/Wh+1 + gh4Wh+1 - ghWh+1k
≤ k4ghkkWh+1k2 + kghkk4Wh+1k2
≤ O(s)k4ghk + kgh + 4ghkk4W h+1k2
≤ O(s)O(rh-1q) + O(sh)q + qO(rh-1q)
≤ O(rhq).
The last inequality comes from the fact that r = max(q, s) and sh < s < 1.
Next we consider the second part of the case of s < 1.
If 1/(LsL) ≤ L-1/(L+1), we know that q = 1/(LsL) and
1/(LsL) ≤ L-1/(L+1)
L1/(L+1) ≤ LsL
L-L/(L+1) ≤ sL
L-1 ≤ sL+1
L-1s-L ≤ s,
which means q ≤ s, thus r = s. Then, we have
k4ghk = O(rh-1q)=O(ShTq) = O(SlL-Is-L) = O(	).
LsL-h+1
If 1/(LSL) > L-1/(L+1), we know that q = L-1/(L+1) and q > S; then, r = q and
k4ghk = O(rh-1q) = O(qh-1q) = O(qh) = O(L-h/(L+1)).
Thus, We can conclude that Lemma 7 also holds for the case of s < 1, which completes the proof. ■
We now prove a similar Lemma for 4bh .
Lemma 8 Given a neural network as stated in Theorem 6, let k ∙ k2 denote the spectral norm,
4Wh = Wh 一 Wh denote some perturbation on weight matrices, bh denote the resulting value
after perturbation, and 4bh = bh - bh.
IfS ≥ 1 and k4Whk2 ≤ O(S-L/L) for all h, then
k4bhk≤ O(工);
LSh
IfS < 1 and k4Wh k2 ≤ O(q) for all h, where q = min(1/(LSL), L-1/(L+1) ), then
k4bhk ≤	OO((LL(-h1-SL--h1)),/(L+1)),
if 1/(LSL) ≤ L-1/(L+1)
if 1/(LSL) > L-1/(L+1) .
17
Published as a conference paper at ICLR 2021
Proof Recall that
bh	Idy,	ifh=L+1
bh+1 (W h+1)|Dh,	otherwise
where Idy is a dy × dy identity matrix and D(hi,i) = 1{gh-1W h ≥ 0}. It is easy to see that
kbhk ≤ O(sL-h+1), because kDhk2 ≤ 1 and kWhk2 ≤ s.
We first prove the case of s ≥ 1. We know that 4bL+1 = 0 ≤ O(s-L-1/L) always holds.
For h ≤ L, we can re-write bh as
bh = Idy (W L+1)|DL(W L)|DL-1...(W h+1)|Dh.
Then, we have
bh(gh)| = Idy (W L+1)|DL(W L)|DL-1...(W h+1)|Dh(gh)|.	(10)
Because of the fact that
fθ = gL+1 = x W 1D1W 2D2...DLW L+1 = ghW h+1Dh+1...DLW L+1
and gh = ghDh, Dh = (Dh)|. We can re-write equation 10 as
bh(gh)1 = fθ.
Thus,
kbh(gh)1 - bh(gh)lk = kfθ - fθk = k4gL+1k ≤ O(1)
L
by Lemma 7. Consequently, we have
kbh(gh)1 — bh(gh)lk = k4bh(gh)1 + 4bh4(gh)| + bh4(gh)lk ≤ O(1).
L
Since kgh k ≤ O(sh), we know that
k4bhk≤ O(y1h),	k4bhk≤ O(sL-h+1)
Lsh
always hold. Since L ≥ 1, s ≥ 1, we simply have ∣∣4bh∣∣ ≤ O(γ-r).
Lsh
Now, we prove the case of s < 1. Similarly, we have
kbh(gh)1 — bh(gh)lk = kfθ — fθk = k4gL+1k ≤ O(1).
L
Similarly, we must have
k4bhk≤ O(Lh),	k4bhk≤ O(Lr^),
where q = min(1/(LsL), L-1/(L+1)) andr = max(q, s) by Lemma 7.
If 1/(LsL) ≤ L-1/(L+1), then sL+1 ≥ 1/L. We thus have
1	LsL-h+1	sL+1	1
O() = O(-亍—)=O(F) ≥ O(尸).
Lrh-1q	L	sh	Lsh
Hence, we get k4bhk ≤ O(Tɪr).
Lsh
If 1/(LsL) > L-1/(L+1), then sL+1 < 1/L. We have
O(Lr1-1q)=O(Lj Lh/(L+1)) ≤ O(Lj s-h) = O(I).
Thus, we get k4bhk ≤ O(L(h-L-1)/(L+1)).
18
Published as a conference paper at ICLR 2021
Lemma 9 Given a neural network as stated in Theorem 6, let ∣∣ ∙ ∣∣f be the Frobenius norm,
W1,…，W L+1 be the weightmatrices in the neural network, 4Wh = Wh 一 Wh be the perturbation
on weight matrices, θh be the parameter vector containing all the elements in Wh, 4θh = θh — θh
be the perturbation on parameter vectors, and f @(x) be the resulting value after perturbation.
If s ≥ 1 and ∣4Wh∣2 ≤ O(s-L/L) for all h, for any weight matrices the following holds
Il dfθ(X)
Il ∂θh
fx IIf ≤ O( SL);
—
If s < 1 and ∣4Wh∣2 ≤ O(q) for all h, where q = min(1/(LsL),L-1/(L+1)), for any weight
matrices the following holds
ll df θ(X)
ll ∂θh
fx IIf ≤ O( SL).
—
Proof We first prove the case of dy = 1, i.e. the output of neural network is 1-dimensional.
In this case, we know that
IIafθ(χ)	df(X) I Jdfθ(X)	df(X). =ii4dfθ(X) I
ii ∂θh	∂θh llF = ll ∂W h	∂Wh llF = ll	∂Wh llF
and the derivative to Wh is
dfθ (x) = (bh)∣gh-1
∂Wh (b ) g .
Then, we have
Il4 黯 IlF=k(bh)ιgh-1 - (bh)ιgh-1kF
=k(bh)lgh-1 -(bh)lgh-1 + (bh)l4gh-1kF
≤ k(4bh)lgh-1kF + k(bh + 4bh)l4gh-1kF.
Recall the fact that gh ≤ O(Sh) and bh ≤ O(SL+1-h).
When S ≥ 1, from Lemma 7 and Lemma 8 we know that
∣4ghk≤ O(y⅛+r),	∣4bhk≤ O(v1h).
LSL-h+1	LSh
Then, we have
ll4错h ≤ O(ShT)O(LSh) + O(sL+1-h)O(LA+2) + O(LSL1h+2)O(LSh)
≤ OSL).
When S < 1, from Lemma 7 and Lemma 8 we know that
∣4ghk ≤ [ O(LSL-h+1),
O(L-h/(L+1) )
if 1/(LSL) ≤ L-1/(L+1)
if 1/(LSL) > L-1/(L+1)
and
∣4bh ∣ ≤	OO((LL(-h1-SL--h1)),/(L+1)),
if 1/(LSL) ≤ L-1/(L+1)
if 1/(LSL) > L-1/(L+1).
If 1/(LSL) ≤ L-1/(L+1) , we have
4黯llF ≤ O(ShT)O(LSh)+。(「+1)。(L⅛)+ O(LSL-h+2MLSh).
19
Published as a conference paper at ICLR 2021
Since 1/(LsL) ≤ L-1/(L+1) implies L-1 ≤ sL+1 (from proof of Lemma 7), we have
1
LSh ≤
sL-h+1
Then we can conclude that
4 f (X) Il ≤ O(L
4 ∂Wh IIf ≤	(sL).
If 1/(LsL) > L-1/(L+1) , we have
I∣4帑
≤ O(sh-1)O(L(h-L-1)/(L+1)) + O(sL+1-h)O(L-(h-1)/(L+1))
+ O(L-(h-1)/(L+1))O(L(h-L-1)/(L+1))
Since 1/(LsL) > L-1/(L+1) implies L-1 > sL+1 (from proof of Lemma 7), we have
L(h-L-1)/(L+1) > sL-h+1
1
L(h-1"(L+I)
> sh-1
Then we have
Il4 d∂WWx) IIf ≤ O(I) ≤ O( SL )，because s< 1.
We have proved the Lemma for the case of dy = 1.
For the case of dy > 1, we know that
IIdfθ(χ)	f (X) h2 =XII dfθ,i(X)	fθ,i(χ)II2 ≤ O(_dy_)
II ∂θθh	∂θh I∣F 乙 Il	∂0h	∂θh I∣F —	S2L2 ,
i=1
where Wθ,i (X) is the ith dimension of Wθ(X). The last inequality directly comes from the 1-
dimensional case.
Since dyis a constant, we ignore it. Then, we have
Il dfθ(X)
Il ∂θh
fx IIf ≤ O( sL),
—
which completes the proof.
Now We can prove Theorem 6, if Wh is obtained by one step gradient descent starting from Wh, θ is
obtained by one step gradient descent starting from θ, and learning rate is α. Then, for any weight
matrix we have
k4W hk2 = kαVw h L(θ)k2
≤ kαVwhL(θ)kF
= kαVθhL(θ)kF
= αIIP=ι [Wθ (χi)-yW 1IIf
dy	1/2
≤ αPi=ι C X I dfθj(Xi) I2
一n i 2II ∂Wh IIf
j
n
≤ α^i=1 Cipdy O(sL-h+ι)O(sh-1)
n
≤ αO(SL),
where Ci = kWθ(Xi) - yi k are some constants.
20
Published as a conference paper at ICLR 2021
If α ≤ O(s-2l∕L) when S ≥ 1, then for any weight matrix We have
∣∣4Wh∣∣2 ≤ αO(sL) ≤ O(S-L/L).
If α ≤ O(qs-L) where q = min(1/(LSL), L-1∕(L+1)) when s < 1, then for any weight matrix we
have
∣∣4Wh∣∣2 ≤ αO(sL) ≤ O(q).
By Lemma 9, we can conclude that
dfθ(X)
_ ~ ,
∂W h
—
dfθ(x) Il ooi 1 )
焉WKllF ≤ O(Sl).
Then, we have
Il df θ(X)	dfθ(X)	Il	= Γγ1	Il dfΘ(X)	dfθ(X)	∣∣2
l l ∂θ	∂θ	llF	一乙	l l ∂θh ∂θh	llF
Lh=1	.
≤ O(	ɪ ).
_	s√L∏y
When S ≥ 1, we know that
S-L ≤ 1 ≤ Ll∕(l+1).
Then, we have
ɪ ≤ 1 ≤ LT/(L+1).
Lsl — L —
Thus, we know 1∕(Lsl) = min(1/(LSL), LT/(L+1)) when S ≥ 1.
For the case of S ≥ 1, we can rewrite α ≤ O(s-2l∕L) = O(qs-L), where q
min(1/(LSL), L-1/(L+1)), which completes the proof of Theorem 6.
Now, we prove Theorem 3 with k = 2, i.e. two-step gradient descent adaptation. We know that
βι = αVRm(%)Ve Lm(fθ )| JV% Lm(fθ )%=IlVe Lm(fθ )『.
Thus, we have
∣βι- αkVf∕m(a)∣阂
= IaVeLm(fe)VeLm(fθ)| - αVθLm(fθ)VθLm(fθ)τ∣
=α∣∣VeLm(fe) - VeLm(fe)k∣VθLm(fe)k
=α 11 %m,ym){ fe(Xm) Tm] f^ ^ -	&) Tm] d⅛H T} 11 Il VeLm(fe) ∣∣
=α 11 %m,ym){ [fe (Xml-Ym+4fe (Xm)] [ f^ + ʌ ^^ ]'
-[fe (Xm) - ym]	/θ∂θ m' } 11 I I Ve Lm(Je ) I I
。II%m,ym){4fe(Xm) Pgml + ʌ f^
+ [fe(Xm) - ym] △ ʃ^θm } I I I IVeLm(Je) I I
≤α [O(丁)O(SLVL) + O(丁)O( --=) + O( —√=) IVeLm(fe)H
|_ l	l	s√L	s√L _|
≤α [O(—≡) + O(-7=)] 1Ve Lm(Je) 11, because L ≥ 1
L	√L	s√L」
≤ [o(M) + O(号
≤o( √q=) i i Ve Lm(Je) i i .
VeLm(fe) I I , where q = min(1/(LSL),L-1/(L+1)),r = min(s-L,s)
21
Published as a conference paper at ICLR 2021
In the case of dy = 1, we have
∣∣ dfW⅛) ∣∣f=(bh)ιghτ ≤ O(sL)，
which has already been shown in the proof of Lemma 9. Then, we have
∣∣Vθ Lm(fθ )h O L
X1∣∣ 恃 ∣∣2)=Mt
X ∣∣ 黯 ∣∣F )≤ O(SL E
h=1
In the case of dy ≥ 1, the bound is simply scaled by a constant of ʌ/dy.
Thus we have
∣β1 - α∣VfθLmfθ )IlHl ≤ O( √L )∣∣Vθ Lm(fθ )∣∣ ≤ O(qsL) ≤ O( L )
because q = min(1/(LsL)， L-1/(L+1)), which completes the proof for the case of k = 2.
For the case of k > 2, we only need to make sure that the bound on learning rate always holds.
Fortunately, since k is a finite constant, according to what we have already showed in the proof of
previous lemmas, every step of gradient descent will not change the spectral norm of the weight
matrix too much: I4W hI2 ≤ O(s-L/L) for all h if s ≥ 1, and I4W hI2 ≤ O(q) for all h if s < 1,
where q = min(1/(LsL)， L-1/(L+1)). Thus, we may assume that the bound on learning rate always
holds during the adaptation. Using triangle inequality to generalize the results from k = 2 to k > 2,
i.e. for all 1 ≤ i ≤ k - 1, we have
Iei- akVfθ Lm(fθ )kH∖ ≤ o( L ).
Recall that
E(α,fθ) = ETm [Lm(fθ) — α∣VfθLm(fθ)阕
and
k-1
Mk = ETm Lm(fθ) - X βi ,
i=0
where βi = aVe/mfeJVeLmXfθ)| and θo = θ,仇+1 = & - Ο▽d£(f%, Dm). The result is
straightforward now.	■
E Proof of Theorem 4
Theorem 4 Let fθ be a convolutional neural network with L - l convolutional layers and l fully-
connected layers and with ReLU activation function, and dx be the input dimension. Denote by W h
the parameter vector of the convolutional layer for h ≤ L- l, and the weight matrices of the fully
connected layers for L — l + 1 < h ≤ L + 1. ∣∣ ∙ ∣2 means both the SPectral norm ofa matrix and
the Euclidean norm of a vector. Define
==ʃ √dX∣W hk2,	if h = 1,...,L - l
sh=	∣W h∣2,	ifL-l+1 < h≤L+1
and let s = maxh sh and α be the learning rate of gradient descent. If α ≤ O(qr) with q =
min(1/(LsL), L-1/(L+1)) and r = min(s-L, s), the following holds
IMk- E(kα, fθ)1 ≤ O (l).
Proof We prove Theorem 4 by first transforming the convolutional neural network into an equivalent
fully connected neural network and then applying Theorem 3.
22
Published as a conference paper at ICLR 2021
First of all, we assume that there are ch channels in hth convolutional layer’s output
gh(x), where h = 0, ..., L - l. For fully-connected layers, define cL-l = ... = cL+1 = 1. We may
represent the dimensionality of input data by x ∈ Rdxc0. Instead of using matrices, we represent
the output of every convolutional layer by a dxch length vector gh = g1h, g2h, ..., gdh , where every
gih = gih,1, gih,2, ..., gih,c is a ch length vector contains value of different channels at the same
position.
We assume that for every element gih,j of gih, its value is completely determined by elements of set
Qih-1, where Qih-1 contains kch-1 elements with fixed positions in gh-1 for a given i. In other
words, every element of the output of a convolutional layer is determined by some elements with
fixed positions from output of the previous layer. This is exactly how convolutional layer works in
deep learning.
If we use gh-h-11 to represent the concatenation ofgah,-b1 ∈ Qih-1, then gh-h-11 is a kch-1 length vector,
where k is the kernel size. Then we have
gih = σ(gQh-h-11Uih)
where Uih,j ∈ Rkch-1 ×ch is a kch-1 × ch matrix.
For notation simplicity, one can define a matrix Uh ∈ Rdxch-1 ×dxch, where every column of Uh
only has kch-1 non-zero elements, and it satisfies
gh = σ(gh-1Uh)
By the property of convolutional layer, we know the following facts:
•	One can represent Uh by Uh = V1h, V2h, ..., Vdh where Vih ∈ Rdxch-1 ×ch is sub-matrix
of Uh;
•	Every Vih contains the same set of elements as Wh, while these elements are located at
different positions;
•	Every Vih can be obtained by any other Vjh by swapping rows;
Let’s define UL-l = WL-l, ..., UL+1 = WL+1 for the fully-connected layer and out-
put layer. Then we can represent the neural network just as in Theorem 3 by fθ (x) =
σ(σ(...σ(x U1)...UL-1)UL)UL+1, and x ∈ Rdxc0.
Now let th be the spectral norm of Uh, and t = maxh th. By Theorem 3, we know that we want
α ≤ O(qr), where q = min(1/(LsL), L-1/(L+1)), r = min(s-L, s).
Because every Vih contains the same set of elements, we know that every Vih has the same Frobenius
norm. Because every Vih can be obtained by any other Vjh by swapping rows, we know that every
Vih has the same rank.
We know that
√ kVhkF ≤ Mk2 ≤ kU hk2 ≤ kU hkF = PdxkVIhkF = PdxkWhk2
r
where ∣∣ ∙ ∣∣f denotes FrobeniUs norm, r denotes the rank of Vh. The last equality holds because
matrix V1h and vector Wh have the same set of elements.
Let’s define
ς =J √dxkWhk2,	ifh = 1,...,L — l
sh = kWhk2,	ifL-l+1< h≤L+1
and s = maxh sh.
From above We know that th = Θ(sh), because Sh/√dχr ≤ th, ≤ sh So We also have t = Θ(s).
Then the conclusion is straightforward.	■
23
Published as a conference paper at ICLR 2021
F	Revision of Theorem 3 and Theorem 4 in clas sification case
We now show how to obtain similar results of Theorem 3 and Theorem 4 in classification problem,
where cross-entropy loss is used instead of squared loss. We need two more restrictions in the
classification case:
1.	There exist matrix A and B such that gLA ≤ softmax(gLW L+1) ≤ gLB for all data points,
where softmax is the softmax operation at the last layer.
2.	For any data point x whose belongs to cth class, there exists a constant > 0 such that
fθ,c(x) ≥ , i.e. the output of neural network has a lower bound on the true class position.
The proof is actually similar to the proof in regression case. We briefly talk about the differences
here.
Firstly, in the classification case, softmax function is used at the last layer. By the first restriction,
we can get rid of softmax function by introducing new matrices, which further leads to bound of the
learning rate as in regression case.
Secondly, if the loss function is the cross-entropy loss, we have:
1	∂ fθ,cm (xm)
vθLm(fθ ) = E(xm,ym) [fθ,Cm (Xm) 一∂θ 一一
where cm denotes the class of xm, e.g. if xm belongs to the third class, then cm = 3. fθ,cm (xm)
denotes the ctmh dimensional element of fθ(Xm). We want a lower bound of fθ,c(X) exists, so that
the gradient vθLm(fθ) can be further bounded.
Then we can prove similar theorems just follow the steps in regression case.
G	Proof of Theorem 5
Theorem 5 Let fθ be a neural network with L hidden layers, with each layer being either fully-
connected or convolutional. Assume that kLk∞ < ∞. Then, error(T) = |E(T, fθ) 一 E(T, fθ)| Is a
non-decreasing function of T. Furthermore, for arbitrary T > 0 we have:
error(T) ≤ O(T2L+3).
Proof Recall that E(t, fθ) is defined based on fmm ®, which is the resulting function whose parameters
dθt
evolve according to the gradient flow -^~- = -Vθ⅛τ L(fm, θ, Dmtr).
We actually have the following (Santambrogio, 2016):
k4θk = kθ0 — θtk≤ O(√).
For simplicity and clearness, we use 4 to denote the change of any vectors and matrices. Thus, we
know that
k4Whk2 ≤k4WhkF ≤k4θk≤ O(√t).
Just like the proofs of Lemma 7, Lemma 8 and Lemma 9, we show that
k4ghk ≤ O(th/2), k4bhk ≤ O(t(L-h+1"2)J∣4dfθx)L
≤ O(t(L+1"2√L + 1)
by mathematical inductions; we skip the details here. Note that different from some previous theorem,
here we focus on time t, and thus hide the effect of the spectral norms by treating them as constants.
24
Published as a conference paper at ICLR 2021
Then, we have
∣∣4(Vθ Lm(fθ ))∣∣
= kVθt Lm (ftm,θ )-Vθ Lm(fθ )k
= ||E(Xm,ym){ fm,θ (Xm) - ym] /"∂θt m) - [fθ (Xm) - y m] ~^~^§m~ 0||
∣∣w	ʃ ʌ , , Jdfm,θ (xm)(xm)	∂fθ (Xm)]∣,3/ 、	1 ʌ ∂fθ (Xm) | 1 H
= IIE(xm,ym) [4fθ (Xm) —~∂θ-------- + 4 d θ +[fθ (Xm) - ym] 4 Qθ	)||
≤O(tL+1√L+T).
Recall that:
E(T,fθ) = ETm [Lm(fm,θ)]
Lm(fθ)+Z TVtLm(fmt,θ)dt
0
Lm(fθ) + Z dt- vθt Lm(fm,θ)dt
Lm(fθ)-/ ||Vet Lm(fm,θ )||2dt
an
Ee(T,fθ)=ETm [Lm(fθ)-TkVfθLm(fθ)k2H] =ETm [Lm(fθ)-TkVθLm(fθ)k2] .
Because
~, 一, .
E(T,fθ)-E (T,fθ)
VθtLm(fmθ)||2dt - TkVθLm(fθ)k2
VθLm(fθ) + 4 (VθLm(fθ)) ||2dt - TkVθLm(fθ)『
( T 2Vθ Lm(fθ )4(Ve Lm(fθ ))| + ||4(Ve Ln(fe ))||2dt,
we have
error(T) = ∖E(T,fe)-E(T,fθ)| ≤ O(L+-1-T2L+3)=O(T2L+3)
2L+ 3
by simple calculation.
On the other hand, observe that
E(T,fθ )= ETm Lm(fθ) - L T||Va Lm(fm,θ )||：dt ,
E(T,fθ )= ETm [Lm(fθ) - t1Vθ Lm(fθ )||：].
We let	τ
G(τ ) = /1|v* Lm(fmθ )||2 dt,
and assume that Vθt Lm(fmt ,θ) is continuous at t = 0. Then, we have G0(τ) = kVθt Lm(fθt)k2.
||e(T,fθ) -E(T,fθ)|| =||ETm / ||v>Lm(fm,θ)||2dt -7|恒Lmf )||口 ||
TIETm(G(T) - T ∙ G0(0))||,
25
Published as a conference paper at ICLR 2021
where TG0(0) = G(0) + TG0(0) (note that G(0) = 0) is a first order approximation to G(T) at
τ = 0. When T = 1, G(T) - TG0(0) can be taken as a local truncation error (i.e., the error that
occurs in one step of a numerical approximation). When T increases, the difference is no better than
the global truncation error (in T steps):
∣∣g(t)- XX (i - (i - i))G0(i)∣∣ =∣∣xx ∣i i+1 ∣∣Vθt Lm(fm,θ)『-网 Lm(fm=θ )∣∣2dt∣∣
TlXX ∕i+1 2 ∙ 4tLm(fm,θ) ∙ VLm(fm,θ岫||,
where 4tLm(fm,θ) = VθtLm(fm,θ) - VθiLm(fm,θ) as shown previously , i is the i-th time
step, and G0(i) is the gradient of G at time step i. Now We can see that ∣∣E(T, fθ) - E(T, fθ)∣∣
highly relates to the difference between Vθt Lm(fmt ,θ) at different time steps (i.e. 4itLm(fm,θ)),
Vθt Lm(fmt ,θ) and T . The first two terms relate to how flat or sharp the hyperplane of Lm(fm,θ) is
near t = 0. We can wrap it as a constant Co(L, t = 0). Then, the error is at least Co(L, t = 0) ∙ O(T).
For the hyperplane smooth enough, we can further geta first order approximation of 4itLm(fm,θ) and
yield C(L, t = 0)O(T2), where C(L, t = 0) can be analogized as the second order derivative of L. ■
H	Some Experimental Details
H.1 Implementation of Classification for Meta-RKHS-II
As we mentioned earlier, our proposed energy functional with closed form adaptation can not be
directly applied to classification problem. We handle this challenge following Arora et al. (2019).
For a dy class classification problem, every data x is associated with a Rdy one-hot vector y as its
label. For C classes classification problem, its encoding is C dimensional vector and we use -1/C
and (C - 1)/C as its correct and incorrect entries encoding. In the prediction, Y tr is replaced by the
encoding of training data. fθ (x) is replaced by fθ (x)l[1,..., 1] ∈ Rn×dy for dimension consistency.
During the testing time, we compute the encoding of the test data point, and choose the position with
largest value as its predicted class.
I Extra Experimental Results
I.1 Comparison with RBF kernel
One interesting question is, without introducing extra model components or networks, what will the
results of other kernel be? We provide the results of using RBF (Gaussian) kernel here: 42.1 ± 1.9
(5-way 1-shot) and 54.9 ± 1.1 (5-way 5-shot) on Mini-ImageNet, 32.4 ± 2.0 (5-way 1-shot) and
38.2 ± 0.9 (5-way 5-shot) on FC-100, which are worse than the NTK based Meta-RKHS-II, showing
the superiority of using NTK.
I.2 More Results on out-of-distribution Generalization
We provide some more results on out-of-distribution generalization experiments here. From the
results we can find that the proposed methods is more robust and can generalize to different datasets
better.
26
Published as a conference paper at ICLR 2021
Table 5: Meta testing on different out-of-distribution datasets with model trained on FC-100.
Algorithm	5 way 1 shot		5 way 5 shot	
	CUB	VGG Flower	CUB	VGG Flower
MAML	31.58 ± 1.89%	50.82 ± 1.94%	41.72 ± 1.29%	65.19 ± 1.36%
FOMAML	32.34 ± 1.57%	49.90 ± 1.78%	41.96 ± 1.53%	66.87 ± 1.45%
Reptile	33.56 ± 1.40%	46.77 ± 1.81%	42.79 ± 1.38%	67.97 ± 0.71%
IMAML	32.49 ± 1.52%	49.96 ± 1.98%	38.92 ± 1.62%	59.80 ± 1.82%
BAYESIAN TAML(SOTA)	31.82 ± 0.49%	49.58 ± 0.55%	43.97 ± 0.57%	67.36 ± 0.53%
META-RKHS-I	34.12 ± 1.34%	48.81 ± 1.89%	43.31 ± 1.43%	69.02 ± 0.62%
META-RKHS-II	36.35 ± 1.07%	59.75 ± 1.23%	49.92 ± 0.68%	76.32 ± 0.58%
27
Published as a conference paper at ICLR 2021
I.3	More Results on Adversarial Attack
We now show some more extra results on adversarial attack in the following figures. Consistent to
the results in main text, we can find that our proposed methods are more robust to adversarial attacks.
0.40
0.35-
0.30-
0.25-
0.20-
0.15-
0.10-
0.05-
Black-box attack
Auenuue
----Reptile
Meta-RKHS-I
----IMAML
----Meta-RKHS-II
--Meta-RKHS-ll_tlOO_PQl
--Meta-RKHS-ll_tlOO_PQ2
----MAML
FOMAML
10	20	30	40	50	60	70
Queries
Figure 6: FC-100 5-way 5-shot Black-box attacks (left) and 5-way 1-shot PGD '∞ norm attack
(right).
MAML
FOMAML
IMAML
Reptile
Meta-RKHS-I
Meta-RKHS-Il
Meta-RKHS-Il tlOO PQl
I.4	Impact of Gradient Norm in Meta-RKHS -I
In this experiment, we compare between our proposed Meta-RKHS-I and Reptile. We evaluate the
trained models with different adaptation steps in testing-time. The comparison is shown in Figure 7.
As we can see, our Meta-RKHS-I always gets better results than Reptile, which supports our idea
that the learned function should be close to task-specific optimal and have large functional gradient
norm. These two conditions together lead to the ability of fast adaptation.
(a) Mini-ImageNet, 5 (b) Mini-ImageNet, 5 (c) FC-100, 5 Way 1 (d) FC-100, 5 Way 5
Way 1 Shots	Way 5 Shots	Shots	Shots
Figure 7: Reptile (dashed) vs. Meta-RKHS-I (solid) with different testing adaptation steps (x-axis).
I.5 Impact of network architecture for different meta-learning models
In this section, we compare different meta-learning models with feature channels of 100 and 200 of
the CNN network structure with 4 or 5 CNN layers respectively.
28
Published as a conference paper at ICLR 2021
Table 6: Few-shot classification results on Mini-ImageNet with different number of feature
channels of 4 convolution layers.
100	200
Algorithm	5 Way 1 Shot	5 Way 5 Shots	5 Way 1 Shot	5 Way 5 Shots
MAML	49.50 ± 1.58%	64.31 ± 1.07%	48.91 ± 1.69%	63.96 ± 0.82%
FOMAML	48.69 ± 1.62%	63.73 ± 0.76%	48.55 ± 1.86%	63.18 ± 0.96%
IMAML	49.30 ± 1.94%	62.89 ± 0.95%	48.23 ± 1.58%	62.25 ± 0.83%
Reptile	50.20 ± 1.69%	64.12 ± 0.92%	48.72 ± 1.97%	63.67 ± 0.79%
META-RKHS-I	51.23 ± 1.79%	66.69 ± 0.73%	51.54 ± 1.64%	65.92 ± 0.92%
META-RKHS-II	51.37 ± 2.31%	66.97 ± 0.98%	50.96 ± 2.15%	65.21 ± 0.87%
Table 7: Few-shot classification results on Mini-ImageNet with different number of feature
channels of 5 convolution layers.
100	200
Algorithm	5 Way 1 Shot	5 Way 5 Shots	5 Way 1 Shot	5 Way 5 Shots
MAML	49.87 ± 1.65%	65.78 ± 1.18%	48.62 ± 1.82%	63.25 ± 0.75%
FOMAML	48.93 ± 1.71%	64.37 ± 0.80%	48.27 ± 1.74%	62.95 ± 0.83%
IMAML	48.03 ± 1.76%	62.15 ± 0.83%	47.52 ± 1.73%	61.77 ± 0.89%
Reptile	50.62 ± 1.83%	64.53 ± 0.97%	49.33 ± 1.89%	63.26 ± 0.70%
META-RKHS-I	52.45 ± 1.88%	66.07 ± 0.69%	51.37 ± 1.92%	65.39 ± 0.98%
META-RKHS-II	50.92 ± 2.16%	66.45 ± 0.91%	50.43 ± 2.42%	64.17 ± 1.06%
29