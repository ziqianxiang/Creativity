Published as a conference paper at ICLR 2021
Bypassing the Ambient Dimension:	Private
SGD with Gradient Subspace Identification
YingxueZhout, Zhiwei Steven Wu^, Arindam Banerjee §
t Department of Computer Science & Engineering, University of Minnesota
^ School of Computer Science, Carnegie Mellon University.
§ Department of Computer Science, University of Illinois Urbana-Champaign
t zhou0 877@umn.edu, ^zstevenwu@cmu.edu, § arindamb@illinois.edu
Ab stract
Differentially private SGD (DP-SGD) is one of the most popular methods for solv-
ing differentially private empirical risk minimization (ERM). Due to its noisy per-
turbation on each gradient update, the error rate of DP-SGD scales with the ambi-
ent dimension p, the number of parameters in the model. Such dependence can be
problematic for over-parameterized models where p n, the number of training
samples. Existing lower bounds on private ERM show that such dependence on p
is inevitable in the worst case. In this paper, we circumvent the dependence on the
ambient dimension by leveraging a low-dimensional structure of gradient space in
deep networks—that is, the stochastic gradients for deep nets usually stay in a low
dimensional subspace in the training process. We propose Projected DP-SGD that
performs noise reduction by projecting the noisy gradients to a low-dimensional
subspace, which is given by the top gradient eigenspace on a small public dataset.
We provide a general sample complexity analysis on the public dataset for the
gradient subspace identification problem and demonstrate that under certain low-
dimensional assumptions the public sample complexity only grows logarithmi-
cally in p. Finally, we provide a theoretical analysis and empirical evaluations to
show that our method can substantially improve the accuracy of DP-SGD in the
high privacy regime (corresponding to low privacy loss ).
1 Introduction
Many fundamental machine learning tasks involve solving empirical risk minimization (ERM):
given a loss function `, find a model w ∈ Rp that minimizes the empirical risk Ln (w) =
n pn=1 '(w, zi), where zι,..., Zn are i.i.d. examples drawn from a distribution P. In many ap-
plications, the training data may contain highly sensitive information about some individuals. When
the models are given by deep neural networks, their rich representation can potentially reveal fine
details of the private data.
Differential privacy (DP) (Dwork et al., 2006) has by now become the standard approach to provide
principled and rigorous privacy guarantees in machine learning. Roughly speaking, DP is a stability
notion that requires that no individual example has a significant influence on the trained model.
One of the most commonly used algorithm for solving private ERM is the differentially-private
Stochastic gradient descent (DP-SGD) (Abadi et al., 2016; BaSSily et al., 2014; Song et al., 2013)-a
private variant of SGD that perturbs each gradient update with random noise vector drawn from an
isotropic Gaussian distribution N(0, σ2Ip), with appropriately chosen variance σ2.
Due to the gradient perturbation drawn from an isotropic Gaussian distribution, the error rate of
DP-SGD has a dependence on the ambient dimension p—the number of parameters in the model.
In the case of convex loss `, Bassily et al. (2014) show that DP-SGD achieves the optimal empiri-
Cal excess risk of O (√p/(ne)). For non-convex loss ', which is more common in neural network
training, minimizing Ln (w) is in general intractable. However, many (non-private) gradient-based
optimization methods are shown to be effective in practice and can provably find approximate sta-
tionary points with vanishing gradient norm ∣∣VLn(w)k2 (see e.g. Nesterov (2014); Ghadlmi and
1
Published as a conference paper at ICLR 2021
epoch = 1
epoch = 10
epoch = 50
epoch = 1
epoch = 10
epoch = 50
(a) SGD
(b) DP-SGD, σ = 1.0
(c) DP-SGD, σ = 2.0
Figure 1: Top 500 eigen-value spectrum of the gradient second moment matrix along the training
trajectory of SGD, DP-SGD with σ = 1, 2. Dataset: MNIST; model: 2-layer ReLU with 128 nodes
each layer. The network has roughly 130,000 parameters and is trained on MNIST dataset. The
Y-axis is the eigenvalue and X-axis is order of eigenvalues from largest to smallest.
epoch = 50
epoch = 100
Lan (2013)). Moreover, for a Wide family of loss functions Ln under the Polyak-匕ojasιeWiCz Con-
dition (Polyak, 1963), the minimization of gradient norm implies achieving global optimum. With
privaCy Constraint, Wang and Xu (2019) reCently shoWed that DP-SGD minimize the empiriCal gra-
dient norm down to O (p1/4/ √ne) when the loss function ' is smooth. Furthermore, exsiting lower
bounds results on private ERM (Bassily et al., 2014) shoW that suCh dependenCe onp is inevitable in
the worst case. However, many modern machine learning tasks now involve training extremely large
models, with the number of parameters substantially larger than the number of training samples. For
these large models, the error dependence on p can be a barrier to practical private ERM.
In this paper, we aim to overcome such dependence on the ambient dimension p by leveraging the
structure of the gradient space in the training of neural networks. We take inspiration from the
empirical observation from Li et al. (2020); Gur-Ari et al. (2018); Papyan (2019) that even though
the ambient dimension of the gradients is large, the set of sample gradients at most iterations along
the optimization trajectory is often contained in a much lower-dimensional subspace. While this
observation has been made mostly for non-private SGD algorithm, we also provide our empirical
evaluation of this structure (in terms of eigenvalues of the gradient second moments matrix) in
Figure 1. Based on this observation, we provide a modular private ERM optimization framework
with two components. At each iteration t, the algorithm performs the following two steps:
1)	Gradient dimension reduction. Let gt be the mini-batch gradient at iteration t. In general, this
subroutines solves the following problem: given any k < p, find a linear projection Vk(t) ∈ Rp×k
such that the reconstruction error kgt - Vk(t)Vk(t)|gtk is small. To implement this subroutine, we
follow a long line of work that studies private data analysis with access to an auxiliary public dataset
Sh drawn from the same distributionP, for which we don’t need to provide formal privacy guarantee
(Bassily et al., 2019b; 2020; Feldman et al., 2018; Avent et al., 2017; Papernot et al., 2017). In our
case, we compute Vk(t) which is given by the top-k eigenspace of the gradients evaluated on Sh.
Alternatively, this subroutine can potentially be implemented through private subspace identification
on the private dataset. However, to our best knowledge, all existing methods have reconstruction
error scaling with √p (Dwork et al., 2014), which will be propagated to the optimization error.
2)	Projected DP-SGD (PDP-SGD). Given the projection Vk(t), we perturb gradient in the projected
subspace: gt = Vk(t)Vk(t)| (gt + bt), where bt is a p-dimensional Gaussian vector. The projection
mapping provides a large reduction of the noise and enables higher accuracy for PDP-SGD.
Our results. We provide both theoretical analyses and empirical evaluations of PDP-SGD:
Uniform convergence for projections. A key step in our theoretical analysis is to bound the recon-
struction error on the gradients from projection of Vk (t). This reduces to bounding the deviation
kVk (t)Vk (t)| - Vk Vk (t)| k2, where Vk(t) denotes the top-k eigenspace of the population second
moment matrix E[▽'(Wt, Z)▽'(Wt, Z)|]. To handle the adaptivity of the sequence of iterates, we
provide a uniform deviation bound for all w ∈ W , where the set W contains all of the iterates.
By leveraging generic chaining techniques, we provide a deviation bound that scales linearly with
a complexity measure—the γ2 function due to Talagrand (2014)—of the set W . We provide low-
complexity examples of W that are supported by empirical observations and show that their γ2
function only scales logarithmically with p.
2
Published as a conference paper at ICLR 2021
Convergence for convex and non-convex optimization. Building on the reconstruction error bound,
we provide convergence and sample complexity results for our method PDP-SGD in two types of
loss functions, including 1) smooth and non-convex, 2) Lipschitz convex. Under suitable assump-
tions on the gradient space, our rates only scales logarithmically on p.
Empirical evaluation. We provide an empirical evaluation of PDP-SGD on two real datasets. In
our experiments, we construct the “public” datasets by taking very small random sub-samples of
these two datasets (100 samples). While these two public datasets are not sufficient for training
an accurate predictor, we demonstrate that they provide useful gradient subspace projection and
substantial accuracy improvement over DP-SGD.
Related work. Beyond the aforementioned work, there has been recent work on private ERM
that also leverages the low-dimensional structure of the problem. Jain and Thakurta (2014); Song
et al. (2020) show dimension independent excess empirical risk bounds for convex generalized lin-
ear problems, when the input data matrix is low-rank. Kairouz et al. (2020) study unconstrained
convex empirical risk minimization and provide a noisy AdaGrad method that achieves dimension-
free excess risk bound, provided that the gradients along the optimization trajectory lie in a low-
dimensional subspace. In comparison, our work studies both convex and non-convex problems and
our analysis applies to more general low-dimensional structures that can be characterized by small
γ2 functions (Talagrand, 2014; Gunasekar et al., 2015) (e.g., low-rank gradients and fast decay in
the magnitude of the gradient coordinates). Recently, Tramer and Boneh (2021) show that private
learning with features learned on public data from a similar domain can significantly improve the
utility. Zhang et al. (2021) leverage the sparsity of the gradients in deep nets to improve the depen-
dence on dimension in the error rate. We also note a recent work (Yu et al., 2021) that proposes
an algorithm similar to PDP-SGD. However, in addition to perturbing the projected gradient in the
top eigenspaces in the public data, their algorithm also adds noise to the residual gradient. Their
error rate scales with dimension p in general due to the noise added to the full space. To achieve
a dimension independent error bound, their analyses require fresh public samples drawn from the
same distribution at each step, which consequently requires a large public data set with size scaling
linearly with T. In comparison, our analysis does not require fresh public samples at each iteration,
and our experiments demonstrate that a small public data set of size no more than 150 suffices.1
2	Preliminaries
Given a private dataset S = {z1, ..., zn} drawn i.i.d. from the underlying distribution P, we want
to solve the following empirical risk minimization (ERM) problem subject to differential privacy:2
minw L n(W) = 1 Pn=1 '(W, zi). where the parameter w ∈ Rp. We optimize this objective with
an iterative algorithm. At each step t, we write Wt as the algorithm’s iterate and use gt to denote
the mini-batch gradient, and VLn(wt) = 1 Pn=1 V'(wt, Zi) to denote the empirical gradient.
In addition to the private dataset, the algorithm can also freely access to a small public dataset
Sh = {zι,..., Zm} drawn from the same distribution P, without any privacy constraint.
Notation. We write Mt ∈ Rp×p to denote the second moment matrix of gradients evaluated on
public dataset Sh i.e., Mt =煮 Pm=I V' (wt, Zi) V' (wt, Zi)1 and write ∑t ∈ Rp×p to denote the
population second moment matrix, i.e., ∑t = Ez〜P [V' (wt,z) V' (wt, z)|]. Weuse V(t) ∈ Rp×p
as the full eigenspace of ∑t. Weuse Vk(t) ∈ Rp×k as the top-k eigenspace of Mt and Vk (t) ∈ Rp×k
as the top-k eigenspace of Σt . To present our result in the subsequent sections, we introduce the
eigen-gap notation αt, i.e., let λ1(Σt) ≥ ... ≥ λp(Σt) be the eigenvalue of Σt, we use αt to
denote the eigen-gap between λk(Σt) and λk+1(Σt), i.e., λk(Σt) - λk+1(Σt) ≥ αt. We also define
W ∈ Rp as the set that contains all the possible iterates wt ∈ W for t ∈ [T]. Throughout, for any
matrix A and vector v, kAk2 denotes spectral norm and kvk2 denotes '2 norm.
Definition 1 (Differential Privacy (Dwork et al., 2006)) A randomized algorithm R is (, δ)-
differentially private if for any pair of datasets D, D0 differ in exactly one data point and for all event
1 Note that the requirement of a large public data set may remove the need of using the private data in the
first place, since training with the large public data set may already provide an accurate model.
2In this paper, we focus on minimizing the empirical risk. However, by relying on the generalization
guarantee of (, δ)-differential privacy, one can also derive a population risk bound that matches the empirical
risk bound up to a term of order O( + δ) (Dwork et al., 2015; Bassily et al., 2016; Jung et al., 2020).
3
Published as a conference paper at ICLR 2021
Y ⊆ Range(R) in the output range of R, we have P {R(D) ∈ Y} ≤ exp()P {R(D0) ∈ Y} + δ,
where the probability is taken over the randomness of R.
To establish the privacy guarantee of our algorithm, we will combine three standard tools in differ-
ential privacy, including 1) the Gaussian mechanism (Dwork et al., 2006) that releases an aggregate
statistic (e.g., the empirical average gradient) by Gaussian perturbation, 2) privacy amplification via
subsampling (Kasiviswanathan et al., 2008) that reduces the privacy parameters and δ by running
the private computation on a random subsample, and 3) advanced composition theorem (Dwork
et al., 2010) that tracks the cumulative privacy loss over the course of the algorithm.
We analyze our method under two asumptions on the gradients of `.
Assumption 1 Forany W ∈ Rp and example Z, kV'(w, z)k2 ≤ G.
Assumption 2 For any example Z, the gradient V'(w, z) is P-Lipschitz with respect to a suitable
pseudo-metric d : Rp X Rp → R, i.e., kV'(w, z) — V'(w0, z)∣∣2 ≤ ρd(w, w0), ∀w, w0 ∈ Rp.
Note that Assumption 1 implies that Ln(w) is G-Lipschitz and Assumption 2 implies that Ln(w) is
P-Smooth when d is the '2-distance. We will discuss additional assumptions regarding the structure
of the stochastic gradients and the error rate for different type of functions in Section 3.
3	Projected Private Gradient Descent
The PDP-SGD follows the classical noisy gradient descent algorithm DP-SGD (Wang et al., 2017;
Wang and Xu, 2019; Bassily et al., 2014). DP-SGD adds isotropic Gaussian noise bt 〜N(0, σ2Ip)
to the gradient gt, i.e., each coordinate of the gradient gt is perturbed by the Gaussian noise. Given
the dimension of gradient to be p, this method ends up in getting a factor of p in the error rate
(Bassily et al., 2014; 2019a). Our algorithm is inspired by the recent observations that stochastic
gradients stay in a low-dimensional space in the training of deep nets (Li et al., 2020; Gur-Ari et al.,
2018). Such observation is also valid for the private training algorithm, i.e., DP-SGD (Figure 1
(b) and (c)). Intuitively, the most information needed for gradient descent is embedded in the top
eigenspace of the stochastic gradients. Thus, PDP-SGD performs noise reduction by projecting the
noisy gradient gt + bt to an approximation of such a subspace given by a public dataset Sh .
Algorithm 1 Projected DP-SGD (PDP-SGD)
1:	Input: Training set S, public set Sh, certain loss '(∙), initial point W
2:	Set: Noise parameter σ, iteration time T , step size ηt .
3:	for t = 0, ..., T do
4:	Compute top-k eigenspace 匕(t) of Mt = ∣Sh∣ pzi∈Sh V'(wt, Zi)V'(wt, Zi)1.
5:	gt = Pl^ Pza∈βt V'(wt, Zi) with Bt uniformly sampled from S with replacement.
6:	Project noisy gradient using % (t): gt = & (t)&(t)| (gt + bt), where bt 〜N(0,σ2Ip).
7:	Update parameter using projected noisy gradient: wt+1 = Wt — ηtgt.
8:	end for
Thus, our algorithm involves two steps at each iteration, i.e., subspace identification and noisy gra-
dient projection. The pseudo-code of PDP-SGD is given in Algorithm 1. At each iteration t, in
order to obtain an approximated subspace without leaking the information of the private dataset
S, We evaluate the second moment matrix Mt on Sh and compute the top-k eigenvectors V; (t) of
Mt (line 4 in Algorithm 1). Then we project the noisy gradient gt + bt to the top-k eigenspace,
i.e., gt = Vk(t)Vk (t)| (gt + bt) (line 6 in Algorithm 1). Then PDP-SGD uses the projected noisy
gradient gt to update the parameter wt+1 = Wt — ηtgt.3 Let us first state its privacy guarantee.
Theorem 1 (Privacy) Under Assumption 1, there exist constants c1 and c2 so that given the number
of iterations T, for any E ≤ c1q2T, where q = 1B1, PDP-SGD (Algorithm 1) is (e, δ)-differentially
C	G G C G	G2T ln( 1)
prιvatefor any δ > 0, if σ2 ≥ c2 —" δ.
3For convex problem, we consider the typical constrained optimization problem such that the optimal solu-
tion w? is in a set H, where H = {w : kwk ≤ B} and each step we project wt+1 back to the set H.
4
Published as a conference paper at ICLR 2021
(a) DPSGD, σ = 1.0
(b) DPSGD, σ = 2.0
(c) DP-SGD, σ = 4.0
Figure 2: Sorted components of population gradients for DP-SGD with σ = 1.0, 2.0, 4.0. Dataset:
MNIST; model: 2-layer ReLU with 128 nodes each layer. The network has roughly 130,000 param-
eters. Y-axis is the absolute value of sorted gradient coordinates, i.e., |mt(j)|, X-axis is the order of
sorted gradient component.
The privacy proof essentailly follows from the same proof of DP-SGD (Abadi et al., 2016). At
each iteration, the update step PDP-SGD is essentially post-processing of Gaussian Mechanism that
computes a noisy estimate of the gradient gt + bt . Then the privacy guarantee of releasing the
sequence of {gt +bt}t is exactly the same as the privacy proof of Theorem 1 of Abadi et al. (2016).
3.1	Gradient Subspace Identification
We now analyze the gradient deviation between the approximated subspace Vk (t)Vk (t)| and true
(population) subspace Vk(t)Vk(t)l,i.e., ∣∣Vfc(t)Vk(t)| - Vk(t)Vk(t)l∣∣2. To bound ∣∣V(t)Vk(t)| -
Vk (t)Vk (t)| k2, we first bound the deviation between second moment matrix kMt - Σtk (Dwork
et al., 2014; McSherry, 2004). Note that, if Mt = mm pm=1 V' (Wt,Z) V'(Wt,Z)| is
evaluated on fresh public samples, the Σt is the expectation of Mt , and the deviation of
Mt from Σt can be easily analyzed by the Ahlswede-Winter Inequality (Horn and John-
son, 2012; Wainwright, 2019), i.e., at any iteration t, if we have fresh public sample
{z1 (t),..., zm(t)} drawn i.i.d. from the distribution P, with suitable assumptions We have,
Ilm Pm=I V'(wt,Zi(t))V'(wt,Zi(t))| - E [V'(wt,Zi(t))V'(wt,诙(t))l]∣∣2 > u, ∀u ∈ [0,1],
with probability at most pexp(-mu2/4G) and G is as in Assumption 1.
However, this concentration bound does not hold for Wt , ∀t > 0 in general, since the public dataset
Sh is reused over the iterations and the parameter Wt depends on Sh . To handle the dependency
issue, we bound kMt - Σt k2 uniformly over all iterations t ∈ [T] to bound the worst-case coun-
terparts that consider all possible iterates. Our uniform bound analysis is based on generic chaining
(GC) (Talagrand, 2014), an advanced tool from probability theory. Eventually, the error bound is
expressed in terms of a complexity measure called γ2 function (Talagrand, 2014). Note that one
may consider the idea of sample splitting to bypass the dependency issue by splitting m public sam-
ples into T disjoint subsets for each iteration. Based on AhlSWede-Winter Inequality, the deviation
error scales with O(√T) leading to a worse trade-off between the subspace construction error and
optimization error due to the dependence on T .
Definition 2 (γ2 function (Talagrand, 2014)) For a metric space (A, d), an admissible sequence
of A is a collection of subsets of A, Γ = {An : n ≥ 0}, with |A0| = 1 and |An| ≤ 22n for all
n ≥ 1, the γ2 functional is defined by γ2 (A, d) = infΓ supA∈A n≥0 2n/2d(A, An) , where the
infimum is over all admissible sequences of A.
In Theorem 2, we show that the uniform convergence bound of kMt - Σt k2 scales with γ2 (W, d),
where d is the pseudo metric as in Assumption 2 and W ∈ Rp is the set that contains all possible
iterates in the algorithm, i.e., Wt ∈ W for all t ∈ [T].
Based on the majorizing measure theorem (e.g., Theorem 2.4.1 in Talagrand (2014)), if the metric
d is '2-norm, γ2(W, d) can be expressed as Gaussian width (Vershynin, 2018; Wainwright, 2019)
of the set W, i.e., W(W) = Ev [supw∈W(w, v〉] where V 〜 N(0, Ip), which only depends on the
size of the W. In Appendix A.2, we show the complexity measure γ2 (W, d) can be expressed as
the γ2 function measure on the gradient space by mapping the parameter space W to the gradient
space, i.e., f : W 7→ M, where f can be considered as f (W) = Ez∈P [V(W, z)]. To simplify the
5
Published as a conference paper at ICLR 2021
notation, We write m = Ez∈p [V(w, z)] as the population gradient at W and M as the space of the
population gradient. Considering d(m, m0) = km - m0k2 for m, m0 ∈ M, γ2(M, d) will be the
same order as the Gaussian width w(M).
To measure the value of γ2 (M, d), we empirically explore the gradient space M for deep nets.
Figure 2 gives an example of the population gradient along the training trajectory of DP-SGD with
σ = {1, 2, 4} for training a 2-layer ReLU on MNIST dataset. Figure 2 shows that each coordinate
of the gradient is of small value and gradient components decay very fast (Li and Banerjee, 2021).
Thus, it is fair that the gradient space M is a union of ellipsoids, i.e., there exists e ∈ Rp such that
M = {m ∈ Rp | Pjp=1 m(j)2/e(j)2 ≤ 1, e ∈ Rp}, where j denotes the j-th coordinate. Then we
have γ2(M,d) ≤ cιw(M') ≤ c2∣∣e∣∣2 (Talagrand, 2014), where ci and c2 are absolute constants.
If the elements of e are sorted in a decreasing order satisfy e(j) ≤ c3∕√j for all j ∈ [p], then
γ2(M, d) ≤ O (√logP).4 Now we give the uniform convergence bound OfkMt — ∑t∣∣2.
Theorem 2 (Second Moment Concentration) Under Assumption 1, 2, the second moment matrix
of the public gradient Mt = 濡 5∑m=ι V'(wt, Zi)V'(wt, Zi)1 approximates the population Second
moment matrix ∑t = Ez 〜P [V'(wt, z)V'(wt, z)| ] uniformly over all iterations, i.e.,forany U > 0,
sup kMt-∑tk2 ≤ O (uGρ√n√Y2(W，d)),	⑴
t∈[T]	∖	√m	)
with probability at least 1 - c exp -u2 ∕4 , where c is an absolute constant.
Theorem 2 shows that Mt approximates the population second moment matrix Σt uniformly over all
iterations. This uniform bound is derived by the technique GC, which develops sharp upper bounds
to suprema of stochastic processes indexed by a set with a metric structure in terms of γ2 functions.
In our case, kMt - Σt k is treated as the stochastic process indexed by the set W ∈ Rp such that
wt ∈ W, which is the set of all possible iterates. The metric d is the pseudo-metric d : Rp ×
Rp 7→ R defined in Assumption 2. To get a more practical bound, following the above discussion,
instead of working with W over parameters, one can consider working with the set M of population
gradients by defining the pseudo-metric as d (w, w0) = d (f (w), f (w0)) = d (m, m0), where
f (w) = Ez∈P [V(w, z)] that maps the parameter space to gradient space. Thus, the complexity
measure γ2(W, d) can be expressed as the γ2 function measure on the population gradient space, i.e.,
γ2 (M, d). As discussed above, using the '2-norm as d, the γ2 (M, d) will be a constant if assuming
the gradient space is a union of ellipsoids and uniform bound only depends on logarithmically on p.
Using the result in Theorem 2 and Davis-Kahan sin-θ theorem (McSherry, 2004), we obtain the
subspace construction error kVk(t)Vk(t)| - Vk(t)Vk(t)|k2 in the following theorem.
Theorem 3 (Subspace Closeness) Under Assumption 1 and 2, with Vk (t) to be the top-k eigenvec-
tors of the population second moment matrix Σt and αt be the eigen-gap at t-th iterate such that
ʌ	∖ ʌ	∖ 、	r	. 1	TV / ɪ ∖	- a 1	■. 1	1	∙ r	、
λk (Σt)	- λk+1	(Σt) ≥ αt, for	the	Vk(t)	in Algorithm 1,	ifm	≥
θ(GρV⅛Y2(W,d))2
mint αt2
, we have
E [k%(t)Vk(t)| - Vk(t)Vk(t)lk2i ≤ O (GPVzlop√mW,d)) ,∀t ∈ [T].	(2)
Theorem 3 gives the sample complexity of the public sample size and the reconstruction error, i.e.,
the difference between Vk(t)Vk(t)| evaluated on the public dataset Sh and Vk(t)Vk(t)| given by the
population second moment Σt. The sample complexity and the reconstruction error both depend on
the γ2 function and eigen-gap αt. A small eigen-gap αt requires larger public sample m.
3.2	Empirical Risk Convergence Analysis
In this section, we present the error rate of PDP-SGD for non-convex (smooth) functions. The error
rate for convex functions is deferred to Appendix C. For non-convex case, we first give the error
rate of the '2-norm of the principal component of the gradient, i.e., kVk (t)Vk(t)1 VLn (wt) k 2. Then
we show that the gradient norm also converges if the principal component dominates the residual
component of the gradient as suggested by Figure 1 and recent observations (Papyan, 2019; Li et al.,
4In the appendix, we provide more examples M that are consistent with empirical observations of stochastic
gradient distributions and have small γ2 (M, d).
6
Published as a conference paper at ICLR 2021
2020). To present our results, We introduce some new notations here. We write [▽£n(wt)]k =
Vk (t)Vk (t)lVLn(wt) as the principal component of the gradient and [VLn(wt)]⊥ = VLn(wt)-
Vk(t)Vk(t)|VLn(wt) as the residual component.
Theorem 4 (Smooth and Non-convex) For ρ-smooth function Ln (w), under Assumptions 1 and
2,	let Λ = Pt=}1/% ,for any 3 δ > 0, with T = O(n2e2) and η =力,PDP-SGD achieves:
1	XXEkVk(t)Vk(t)|vLn(wt)k2 ≤ O (kρG2) + O (AG%2’2"⑷nP) .	(3)
T	n	m
Additionally, assuming the principal component of the gradient dominates, i.e., there exist c > 0,
such that T PT=I IIVLn(wt)]⊥k2 ≤ c1 PT=I k[VL n (wt)]k III, we have
EkVLn(WR)k2 ≤ O (kρGl) + O (AG4ρlγl(W，d)lnP) ,	(4)
n	m
where wR is uniformly sampled from {w1, ..., wT}.
Theorem 4 shows that PDP-SGD reduces the error rate of a factor of P to k compared to existing
results for non-convex and smooth functions (Wang and Xu, 2019). The error rate also includes
a term depending on the γ2 function and the eigen-gap αt, i.e., Λ = PT=I 1∕ɑ2∕T. This term
comes from the subspace reconstruction error. As discussed in the previous section, as the gradients
stay in a union of ellipsoids, the γ2 is a constant. The term Λ depends on the eigen-gap αt, i.e,
λk (Σt) - λk+1 (Σt) ≥ αt. As shown by the Figure 1, along the training trajectory, there are a few
dominated eigenvalues and the eigen-gap stays significant (even at the last epoch). Then the term
Λ will be a constant and the bound scales logarithmically with p. If one considers the eigen-gap α
decays as training proceed, e.g., α = --114 for t > 0, then we have Λ = O(√T). In this case, with
T = n22, PDP-SGD requires the public data size m = O(n).
4 Experiments
We empirically evaluate PDP-SGD on training neural networks with two datasets: the MNIST (Le-
Cun et al., 1998) and Fashion MNIST (Xiao et al., 2017). We compare the performance of PDP-
SGD with the baseline DP-SGD for various privacy levels . In addition, we also explore a heuristic
method, i.e., DP-SGD with random projection by replacing the projector with a Rk×p Gaussian ran-
dom projector (Bingham and Mannila, 2001; Blocki et al., 2012). We call this method randomly
projected DP-SGD (RPDP-SGD). We present the experimental results after discussing the experi-
mental setup. More details and additional results are in Appendix D.
Datasets and Network Structure. The MNIST and Fashion MNIST datasets both consist of 60,000
training examples and 10,000 test examples. To construct the private training set, we randomly
sample 10, 000 samples from the original training set of MNIST and Fashion MNIST, then we
randomly sample 100 samples from the rest to construct the public dataset. Note that the smaller
private datasets make the private learning problem more challenging. For both datasets, we use a
convolutional neural network that follows the structure in Papernot et al. (2020).
Training and Hyper-parameter Setting. Cross-entropy is used as our loss function throughout
experiments. The mini-batch size is set to be 250 for both MNIST and Fashion MNIST. For the step
size, we follow the grid search method with search space and step size used for DP-SGD, PDP-SGD
and RPDP-SGD listed in Appendix D. For training, a fixed budget on the number of epochs i.e., 30
is assigned for the each task. We repeat each experiments 3 times and report the mean and standard
deviation of the accuracy on the training and test set. For PDP-SGD, we use Lanczos algorithm
to compute the top k eigen-space of the gradient second moment martix on public dataset. We
use k = 50 for MNIST and k = 70 for Fashion MNIST. For RPDP-SGD, we use k = 800 for
both datasets. Instead of doing the projection for all epochs, we also explored a start point for the
projection, i.e., executing the projection from the 1-st epoch, 15-th epoch. We found that for Fashion
MNIST, PDP-SGD and RPDP-SGD perform better when starting projection from the 15-th epoch.
Privacy Parameter Setting: We consider different choices of the noise scale, i.e., σ =
{18, 14, 10, 8, 6, 4} for MNIST and σ = {18, 14, 10, 6, 4, 2} for Fashion MNIST. Since gradient
7
Published as a conference paper at ICLR 2021
(a) MNIST	(b) Fashion MNIST
Figure 3: Training and test accuracy for DP-SGD, PDP-SGD and RPDP-SGD with different privacy
levels for (a) MNIST and (b) Fashion MNIST. The X-axis is the , and the Y-axis is the train/test
accuracy. For small regime, which is more favorable for privacy, PDP-SGD outperforms DP-SGD.
60
40
20
Augmue UQ匕
0	10	20	30
number of epochs
0	10	20	30
number of epochs
(a) MNIST, = 0.23
(b) Fashion MNIST, = 0.30
Figure 4: Training dynamics of DP-SGD, PDP-SGD and RPDP-SGD for (a) MNIST ( = 0.23) and
(b) Fashion MNIST ( = 0.30). The X-axis is the number of epochs, and the Y-axis is the train/test
accuracy. For Fashion MNIST, PDP-SGD and RPDP-SGD start projection at 15-th epoch.
norm bound G is unknow for deep learning, we follow the gradient clipping method in Abadi
et al. (2016) to guarantee the privacy. We choose gradient clip size to be 1.0 for both datasets.
We follow the Moment Accountant (MA) method (Abadi et al., 2016; Bu et al., 2019) to calcu-
late the accumulated privacy cost, which depends on the number of epochs, the batch size, δ, and
noise σ. With 30 epochs, batch size 250, 10, 000 training samples, and fixing δ = 10-5, the is
{2.41, 1.09, 0.72,	0.42,	0.30,	0.23}	for σ	∈ {2, 4, 6, 10, 14,	18} for	Fashion MNIST. For
MNIST, is {1.09,	0.72,	0.53,	0.42,	0.30,	0.23} for σ ∈ {4, 6,	8, 10,	14, 18}. Note that
presented in this paper is w.r.t. a subset i.e., 10, 000 samples from MNIST and Fashion MNIST.
Experimental Results. The training accuracy and test accuracy for different , are reported in
Figure 3. For small regime, i.e., ≤ 0.42 with MNIST (Figure 3 (a)) and ≤ 0.72 with Fashion
MNIST (Figure 3 (b)), PDP-SGD outperforms DP-SGD. For large (small noise scale), we think
DP-SGD performs better than PDP-SGD because the subspace reconstruction error dominates the
error from the injected noise. For most choices of , RPDP-SGD fails to improve the accuracy over
DP-SGD because the subspace reconstruction error introduced by the random projector is larger
than the noise error reduced by projection. To the best of our knowledge, we noticed that when
< 1 for MNIST, PDP-SGD and DP-SGD perform better than the benchmark reported in Papernot
et al. (2020) even with a subset from MNIST (Figure 3 (a)). We acknowledge that Papernot et al.
(2020) report the test accuracy as a training dynamic in terms of privacy loss . Figure 4 provides
two examples of the training dynamics, i.e., MNIST with = 0.23 and Fashion MNIST with =
0.30, showing that PDP-SGD outperforms DP-SGD for large noise scale since PDP-SGD efficiently
reduces the noise. We also validate this observation on larger training samples in Appendix D.
We also study the role of projection dimension k and pubic sample size m. Figure 5(a) and Figure
5(b) present the training and test accuracy for PDP-SGD with k ∈ {10, 20, 30, 50} and PDP-SGD
with m ∈ {50, 100, 150} for = 0.23 for MNIST dataset. Among the choices of k, PDP-SGD with
k = 50 achieves the best accuracy. DP-SGD with k = 10 proceeds slower than the rest, due to the
larger reconstruction error introduced by projecting the gradient to a much smaller subspace, i..e,
k = 10. However, compared to the gradient dimension p ≈ 25, 000, it is impressive that PDP-SGD
with k = 50 can achieve better accuracy than DP-SGD for a certain range of . Figure 5(b) shows
8
Published as a conference paper at ICLR 2021
Ooooooo
8 7 6 5 4 3 2
AUSrme UQb
io
6	10	20	30
number of epochs
6	10	20	30
number of epochs
(a) MNIST, = 0.23
(b) MNIST, = 0.23
80604020
AUgnDUe UQ上
O 10	20	30
number of epochs
0	10	20	30
number of epochs
io
Figure 5: Training accuracy and test accuracy for (a) PDP-SGD with k = {10, 20, 30, 50}; (b) PDP-
SGD with m = {50, 100, 150} for MNIST ( = 0.23). The X-axis and Y-axis refer to Figure 4. The
performance of PDP-SGD increases as projection dimension k and public sample size m increase.
Ooooo
6 5 4 3 2
>u2D00ro UQb
(a) MNIST, = 0.23	(b) Fashion MNIST, = 0.23
Figure 6: Training and test accuracy for PDP-SGD with different frequency of eigen-space com-
putation for (a) MNIST ( = 0.23) and (b) Fashion MNIST ( = 0.23). s = {1, 10, 20} is the
frequency of subspace update, i.e., compute the eigen-space every s iterates. The X-axis and Y-axis
refer to Figure 4. For Fashion MNIST, PDP-SGD starts projection at 15-th epoch. PDP-SGD with a
reduced eigen-space computation also improves the accuracy over DP-SGD.
that the accuracy of PDP-SGD improves as the m increases from 50 to 150. This is consistent with
the theoretical analysis that increasing m helps to reduce the subspace reconstruction error. Also,
PDP-SGD with m = 100 performs similar to PDP-SGD with m = 150. The results suggest that
while a small number of public datasets are not sufficient for training an accurate predictor, they
provide useful gradient subspace projection and accuracy improvement over DP-SGD.
To reduce the computation complexity introduced by eigen-value decomposition, we explored PDP-
SGD with sparse eigen-space computation, i.e., update the projector every s iterates. Note that
PDP-SGD with s = 1 means computing the top eigen-space at every iteration. Figure 6 reports
PDP-SGD with s = {1, 10, 20} for (a) MNIST and (b) Fashion MNIST showing that PDP-SGD
with a reduced eigen-space computation also outperforms DP-SGD, even though there is a mild
decay for PDP-SGD with fewer eigen-space computations.
5 Conclusion
While DP-SGD and variants have been well studied for private ERM, the error rate of DP-SGD
depends on the ambient dimension p. In this paper, we aim to bypass such dependence by leverag-
ing the low-dimensional structure of the observed gradients in the training of deep networks. We
propose PDP-SGD which projects the noisy gradient to an approximated subspace evaluated on a
public dataset. We show theoretically that PDP-SGD can obtain (near) dimension-independent error
rate. We evaluate the proposed algorithms on two popular deep learning tasks and demonstrate the
empirical advantages of PDP-SGD.
Acknowledgement
The research was supported by NSF grants IIS-1908104, OAC-1934634, IIS-1563950, a Google
Faculty Research Award, a J.P. Morgan Faculty Award, and a Mozilla research grant. We would like
to thank the Minnesota Super-computing Institute (MSI) for providing computational resources and
support.
9
Published as a conference paper at ICLR 2021
References
M. Abadi, A. Chu, I. Goodfellow, B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning
with differential privacy. In 23rd ACM Conference on Computer and Communications Security,
pages 308-318, 2016. URL https://arxiv.org/abs/1607.00133.
B.	Avent, A. Korolova, D. Zeber, T. Hovden, and B. Livshits. BLENDER: enabling local search
with a hybrid differential privacy model. In 26th USENIX Security Symposium, pages 747-
764, 2017. URL https://www.usenix.org/conference/usenixsecurity17/
technical-sessions/presentation/avent.
R. Bassily, A. Smith, and A. Thakurta. Private empirical risk minimization: Efficient algorithms and
tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science,
pages 464-473. IEEE, 2014.
R. Bassily, K. Nissim, A. D. Smith, T. Steinke, U. Stemmer, and J. Ullman. Algorithmic stability for
adaptive data analysis. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory
of Computing, pages 1046-1059, 2016. doi: 10.1145/2897518.2897566. URL https://doi.
org/10.1145/2897518.2897566.
R. Bassily, V. Feldman, K. Talwar, and A. G. Thakurta. Private stochastic convex optimization
with optimal rates. In Advances in Neural Information Processing Systems, pages 11282-11291,
2019a.
R. Bassily, S. Moran, and N. Alon. Limits of private learning with ac-
cess to public data. In Advances in Neural Information Processing Sys-
tems, pages 10342-10352,	2019b. URL http://papers.nips.cc/paper/
9222-limits- of-private-learning-with- access-to-public-data.
R. Bassily, A. Cheu, S. Moran, A. Nikolov, J. Ullman, and Z. S. Wu. Private query release assisted by
public data. CoRR, abs/2004.10941, 2020. URL https://arxiv.org/abs/2004.10941.
E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image
and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 245-250, 2001.
J. Blocki, A. Blum, A. Datta, and O. Sheffet. The johnson-lindenstrauss transform itself preserves
differential privacy. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science,
pages 410-419. IEEE, 2012.
Z. Bu, J. Dong, Q. Long, and W. J. Su. Deep learning with gaussian differential privacy. arXiv
preprint arXiv:1911.11607, 2019.
C.	Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data
analysis. In Theory of Cryptography Conference, pages 265-284. Springer, 2006.
C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In 51th Annual
IEEE Symposium on Foundations of Computer Science, pages 51-60. IEEE Computer Society,
2010. doi: 10.1109/FOCS.2010.12. URL https://doi.org/10.1109/FOCS.2010.12.
C. Dwork, K. Talwar, A. Thakurta, and L. Zhang. Analyze gauss: optimal bounds for privacy-
preserving principal component analysis. In Symposium on Theory of Computing, pages 11-
20. ACM, 2014. doi: 10.1145/2591796.2591883. URL https://doi.org/10.1145/
2591796.2591883.
C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L. Roth. Preserving statisti-
cal validity in adaptive data analysis. In Proceedings of the 47th Annual ACM on Symposium
on Theory of Computing, pages 117-126. ACM, 2015. doi: 10.1145/2746539.2746580. URL
https://doi.org/10.1145/2746539.2746580.
V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. Privacy amplification by iteration. In 59th
IEEE Annual Symposium on Foundations of Computer Science, pages 521-532, 2018. doi: 10.
1109/FOCS.2018.00056. URL https://doi.org/10.1109/FOCS.2018.00056.
10
Published as a conference paper at ICLR 2021
S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization,23(4):2341-2368,2013. doi: 10.1137/120880811.
URL https://doi.org/10.1137/120880811.
G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, third
edition, 1996.
S. Gunasekar, A. Banerjee, and J. Ghosh. Unified view of matrix completion under general struc-
tural constraints. In Proceedings of the 28th International Conference on Neural Information
Processing Systems-Volume 1, pages 1180-1188, 2015.
G. Gur-Ari, D. A. Roberts, and E. Dyer. Gradient descent happens in a tiny subspace. CoRR,
abs/1812.04754, 2018. URL http://arxiv.org/abs/1812.04754.
R.	A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012.
P. Jain and A. G. Thakurta. (near) dimension independent risk bounds for differentially pri-
vate learning. volume 32 of Proceedings of Machine Learning Research, pages 476-484, Be-
jing, China, 22-24 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/
jain14.html.
C. Jung, K. Ligett, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and M. Shenfeld. A new analysis of
differential privacy’s generalization guarantees. volume 151, pages 31:1-31:17, 2020. doi: 10.
4230/LIPIcs.ITCS.2020.31. URL https://doi.org/10.4230/LIPIcs.ITCS.2020.
31.
P. Kairouz, M. Ribero, K. Rush, and A. Thakurta. Dimension independence in unconstrained private
ERM via adaptive preconditioning. CoRR, abs/2008.06570, 2020. URL https://arxiv.
org/abs/2008.06570.
S.	P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn
privately? In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, 2008.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
X. Li and A. Banerjee. Experiments with rich regime training for deep learning. arXiv preprint
arXiv:2102.13522, 2021.
X. Li, Q. Gu, Y. Zhou, T. Chen, and A. Banerjee. Hessian based analysis of SGD for deep nets:
Dynamics and generalization. In Proceedings of the 2020 SIAM International Conference on
Data Mining, pages 190-198. SIAM, 2020. doi: 10.1137/1.9781611976236.22. URL https:
//doi.org/10.1137/1.9781611976236.22.
F. McSherry. Spectral methods for data analysis. PhD thesis, University of Washington, 2004.
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916.
N. Papernot, M. Abadi, UJ. Erlingsson, I. J. Goodfellow, and K. Talwar. Semi-supervised knowledge
transfer for deep learning from private training data. In 5th International Conference on Learning
Representations, 2017. URL https://openreview.net/forum?id=HkwoSDPgg.
N. Papernot, A. Thakurta, S. Song, S. Chien, and UJlfar Erlingsson. Tempered sigmoid activations
for deep learning with differential privacy, 2020.
V. Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet hessians. In International Conference on Machine Learning, pages 5012-5021, 2019.
B. Polyak. Gradient methods for the minimisation of functionals. Ussr Computational Mathematics
and Mathematical Physics, 3:864-878, 12 1963. doi: 10.1016/0041-5553(63)90382-3.
11
Published as a conference paper at ICLR 2021
S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient descent with differentially private
updates. In IEEE Global Conference on Signal and Information Processing, pages 245-248.
IEEE, 2013. doi: 10.1109/GlobalSIP.2013.6736861. URL https://doi.org/10.1109/
GlobalSIP.2013.6736861.
S. Song, O. Thakkar, and A. Thakurta. Characterizing private clipped gradient descent on convex
generalized linear problems. arXiv preprint arXiv:2006.06783, 2020.
M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014.
F. Tramer and D. Boneh. Differentially private learning needs better features (or much more data). In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=YTWGvpFOQD-.
R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge University Press, 2018. doi: 10.1017/9781108231596.
M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge
University Press, 2019.
D. Wang and J. Xu. Differentially private empirical risk minimization with smooth non-convex loss
functions: A non-stationary view. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 33, pages 1182-1189, 2019.
D. Wang, M. Ye, and J. Xu. Differentially private empirical risk minimization revisited: Faster and
more general. In Advances in Neural Information Processing Systems, pages 2722-2731, 2017.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
D. Yu, H. Zhang, W. Chen, and T.-Y. Liu. Do not let privacy overbill utility: Gradient embedding
perturbation for private learning. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=7aogOj_VYO0.
H. Zhang, I. Mironov, and M. Hejazinia. Wide network learning with differential privacy. arXiv
preprint arXiv:2103.01294, 2021.
12
Published as a conference paper at ICLR 2021
A Uniform Convergence for Subspaces: Proofs for Section 3.1
In this section, we provide the proofs for Section 3.1. We first show that the second moment matrix
Mt converges to the population second moment matrix Σt uniform over all iterations t ∈ [T], i.e.,
supt∈[T] kMt - Σtk. Then we show that the top-k subspace of Mt uniformly converges to the top-k
subspace of ∑t,i.e., ∣∣Vk(t)Vk(t)| - Vk(t)Vk(t)|k for all t ∈ [T]. Our bound depends on γ2(W,d)
where W is the set of all possible parameters along the training trajectory. In Section A.2, we show
that the bound can be derived by γ2 (M, d) as well, where M is the set of population gradients
along the training trajectory. Then, we provide examples of the set M and corresponding value of
γ2 (M, d).
A.1 Uniform Convergence Bound
Our proofs of Theorem 2 heavily rely on the advanced probability tool, Generic Chaining (GC)
(Talagrand, 2014). Typically the results in generic chaining are characterized by the so-called γ2
function (see Definition 2). Talagrand (2014) shows that for a process (Xt)t∈T and a given metric
space (T, d), if (Xt)t∈T satisfies the increment condition
∀u > 0, P (|Xs - Xt | ≥ u) ≤ 2 exp
(	u2
V 2d(s,t)2
(5)
then the size of the process can be bounded as
E sup Xt ≤ cγ2(T, d),
t∈T
(6)
with c to be an absolute constant.
To apply the GC result to establish the Theorem 2, we treat kMt - Σt k2 as the process Xt
over the iterations. In detail, since Mt = + Pm=I ▽' (wt,Zi) V' (wt,Zi)1, and ∑t =
Ez〜P [V' (wt,z) V' (wt, z)|], the ∣∣Mt - ∑t∣∣2 is a random process indexed by Wt ∈ W, with
W to be the set of all possible iterates obtained by the algorithm.
We first show that the variable ∣Mt - Σt ∣2 satisfies the increment condition as stated in equation 5
in Lemma 1. Before we present the proof of Lemma 1, we introduce the Ahlswede-Winter Inequal-
ity (Horn and Johnson, 2012; Wainwright, 2019), which will be used in the proof of Lemma 1.
Ahlswede-Winter Inequality shows that positive semi-definite random matrix with bounded spectral
norm concentrates to its expectation with high probability.
Theorem 5 (Ahlswede-Winter Inequality) Let Y be a random, symmetric, positive semi-definite
p × p matrix. such that such that ∣E[Y ]∣ ≤ 1. Suppose ∣Y ∣ ≤ R for some fixed scalar R ≥ 1.
Let {Y1, . . . , Ym} be independent copies of Y (i.e., independently sampled matrices with the same
distribution as Y). For any u ∈ [0, 1], we have
P
> u I ≤ 2p ∙ exp (—mu2∕4R).
2
(7)
To make the argument clear, we use a more informative notation for Mt and Σt . Recall the notation
of Mt and Σt such that
1m
Mt = — X V' (wt,Zi) V' (wt,Zi)1,	⑻
m
i=1
and
∑t = Ez〜P [V' (wt,z) V' (wt,z)1],
(9)
given the dataset Sh = {Zι,..,Zm} and distribution P where Zi 〜 P for i ∈ [m], the Mt and ∑t are
functions of parameter wt, so we use M(wt) and Σ(wt) for Mt and Σt interchangeably in the rest
of this section, i..e,
m
M(w) = -X V' (w, Zi) V' (w, Zi)T	(10)
m i=1
13
Published as a conference paper at ICLR 2021
and
Σ(w) = Ez〜P [▽' (w,z) ▽' (w,z)T].	(11)
Lemma 1 With Assumption 2 and 1 hold, for any w, w0 ∈ W and Vu > 0, we have
P (∣∣M(w) — Σ(w)∣b -IlM(W0) — Σ(w0)∣∣2 ≥ √√= ∙ 4Gρd(w, W0)) ≤ 2p ∙ exp (—u2/4),
(12)
where d : W ×W → R is the pseudo-metric in Assumption 2.
Proof: We consider random variable
Xi = v`(w, Zi)v`(w, Zi)T — V'(wz, Zi)V'(wz, Zi)T + 2Gρd(w, w0)Ip,	(13)
where Ip ∈ Rp×p is the identity matrix.
Note that 2Gρd(w, w0) is deterministic and the randomness of Xi comes from V'(w, Zi) and
V'(w0,Zi).
By triangle inequality and the construction of Xi , we have
IlM(W)- ς(W)112 TlM(w') — ς(w')∣∣2
=∣∣m(W)- E[M(W)]|2 - kM(w') - E[M(W')]∣∣2
≤ ∣∣M(w) — E[M(w)] — (M(w0) — E[M(w0)])∣2
=∣∣M (w) — M (W0) — E[(M (w) — M (Wo))]∣2
m
=L X (v`(w, Zi)v`(w, Zi)T — v`(w0, Zi)V'(w0, Zi)T)- E [V'(w, ≡i)V'(w, Ziy — V'(w0, ≡i)V'(w,, Zi)τ]
m z—z
i=1	2
m
= -X (v`(w, Zi)v`(w, Zi)T — v`(w0, Zi)v`(w0, Zi)T + 2Gρd(w, w0)Ip)
i=1
—E [V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)T + 2Gρd(w, w0)Ip]
2
=ɪ X Xi — E[Xi]	(14)
m z—z	2
To apply Theorem 5 for ∣ ∣ * P Xi — E[Xi] ∣ ∣ ?, we first show that the random symmetric matrix Xi
is positive semi-definite.
By Assumption 1 and Assumption 2 and definition
∣∣V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi尸卜
=sup xT (V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)T) X
x:||x|| = 1
=sup XTV'(w, Zi)V'(w, Zi)TX — XTV'(w0, Zi)V'(w0, Zi)Tx
x:||x|| = 1
=sup(X, V'(w,Zi)i2 —〈X, V'(w0,Zi)i2
x:||x|| = 1
=sup ((x, v`(w, Zi)i + (x, v`(w0, Zi)i)((X, v`(w, Z/ — (x, V'(w0, Z∕)
x:||x|| = 1
≤ sup 2G ((x, v`(w, Zi)i — (x, v`(w0,名)〉)
x:||x|| = 1
= 2G∣∣V'(w,Zi)-V'(w0, Zi)Il2
≤ 2Gρd(w, W0)	(15)
14
Published as a conference paper at ICLR 2021
For any non-zero vector x ∈ Rp , we have
XTXiX = x| (V'(w, ∙¾)V'(w, Zi)1 — V'(w0, Zi)V'(w0, Zi)1 + 2Gρd(w, w0)Ip) X
=x| (V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)1) x + 2Gρd(w, w0)∣∣x∣∣2
≥ -∣∣V'(w, Zi)V'(w, Zi)T — V'(w0,Zi)V'(w0, Zi)Tk2 I∣xk2 + 2Gρd(w, w0)kx∣∣2
=(2Gρd(w, w0) — ∣V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)Tk2) I∣xk2
(a)
≥ 0,	(16)
where (a) is true because ∣∣V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)Tk2 ≤ 2Gρd(w, w0) as
shown in equation 15.
Let Yi = 4GρXW w0), with equation 15, We have
IIVll _ ∣V'(w, Zi)V'(w, Zi)T — V'(w0, Zi)V'(w0, Zi)T + 2Gρd(w, w0)Ip∣∣2
kYik2 =	4Gρd(w, w0)
≤ ∣∣V'(w,Zi)V'(w,Zi)T —V'(w0,Zi)V'(w0,Zi)Tk2 + ∣2Gρd(w,w0)Ip∣∣2
—	4Gρd(w, w0)
2Gρd(w, w0) + 2Gρd(w, w0)
—	4Gρd(w, w0)
= 1.	(17)
So that kYik2 ≤ 1 and kE[Yi]k2 ≤ 1. Then, from Theorem 5, with R = 1, we have for any u ∈ [0, 1]
P
> u ≤ 2p ∙ exp (—mu2∕4).
(18)
Notethat ∣∣ ml Pm=I Yi — E [Yi]∣∣ is always bounded by 1 since ∣∣Yi∣∣2 ≤ 1 and ∣∣E[Yi]∣2 ≤ 1. Sothe
above inequality holds for any u > 1 with probability 0 which is bounded by 2p ∙ exp (—mu2/4).
So that we have for any u > 0,
So that for any u > 0,
P
>u
≤ 2p ∙ exp (—mu2∕4).
(19)
P (∣∣ m X Xi-ER
> -u= ∙ 4Gρd(w, w0) ) ≤ 2p ∙ exp (—u2/4).	(20)
Combine equation 20 and equation 14, we have
P (∣∣M(w) — Σ(w)∣2 — ∣M(w0) — Σ(w0)k2 ≥ -u= ∙ 4Gρd(w, w0)J
=P (∣∣M(w) — E[M(w)]∣2 — ∣M(w0) — E[M(w0)]∣2 > u= ∙ 4Gρd(w, w0)
≤P (∣∣ m∙ X Xi—E[Xi]
≤ 2p ∙ exp (—u2/4)
That completes the proof.
u
> -j= ∙ 4Gρd(w, w0)
(21)
Based on the above result, now we come to the proof of Theorem 2. The proof follows the Generic
Chaining argument, i.e., Chapter 2 of Talagrand (2014).
15
Published as a conference paper at ICLR 2021
Theorem 2 (Second Moment Concentration) Under Assumption 1, 2, the second moment matrix
of the public gradient Mt = ml Em=I V'(wt, zi)V'(wt, Zi)1 approximates the population Second
moment matrix ∑t = Ez 〜P [V'(wt, z)V'(wt, z)| ] uniformly over all iterations, i.e.,forany U > 0,
sup kMt-∑tk2 ≤ O (uGρ√√Y2(W㈤)，	(1)
t∈[T]	m
with probability at least 1 - c exp -u2 /4 , where c is an absolute constant.
Proof: Note that equation equation 1 is a uniform bound over iteration t ∈ [T]. To bound
supt∈[T] kMt - Σtk2, it is sufficient to bound
sup kM(w) - Σ(w)k2,	(22)
w∈W
where W contains all the possible trajectorys of w1, ..., wT.
We consider a sequence of subsets Wn of W, and
card Wn ≤ Nn ,	(23)
where N0 = 1; Nn = 22n if n ≥ 1.
Let πn(w) ∈ Wn be the approximation of any w ∈ W. We decompose the kM (w) - Σ(w)k2 as
kM(w) - Σ(w)k2 - kM(π0(w)) - Σ(π0(w))k2
= X (kM (πn (w)) - Σ(πn(w))k2 - kM (πn-1(w)) - Σ(πn-1(w))k2) ,	(24)
n≥1
which holds since πn(w) = w for n large enough.
Based on Lemma 1, for any u > 0, we have
u
∣∣M (∏n(w)) — Σ(∏n(w))k2 一 ||M (∏n-l(w)) — ∑(∏n-l(w))∣∣2 ≥ √m ∙ 4Gρd(∏n (w) ,∏n-1 (w)),
(25)
with probability at most 2p exp( — u2).
For any n > 0 and w ∈ W, the number of possible pairs (πn(w), πn-1(w)) is
card Wn ∙ card Wn-1 ≤ NnNn-I ≤ Nn+1 = 22n+1.	(26)
Apply union bound over all the possible pairs of (πn (w), πn-1(w)), following Talagrand (2014)
(Chapter 2.2), for any n > 0, u > 0, and w ∈ W, we have
∣∣M (∏n(w)) — Σ(∏n(w))∣∣2-∣∣M (∏n-1 (w)) —∑(∏n-1 (w)) ∣∣2 ≥ u2n/2 d(∏n (w) ,∏n-1 (w)) ∙ 4^
2m
(27)
with probability
X 2p ∙ 22n+1 exp (—u22n-2) ≤ c0pexp (— u-) ,	(28)
n≥1
where c0 is a universal constant.
Then we have
X(∣M(πn(w)) — Σ(πn(w))∣2 — ∣M(πn-1(w)) — Σ(πn-1(w))∣2)
n≥1
≥ X u2n72d(∏n(w),∏n-i(w)) ∙ 4^
n≥ι	F
≥ X u2n/2d(w, Wn) ∙ 4Gρ
n≥o	√m
(29)
16
Published as a conference paper at ICLR 2021
with probability at most Cp exp (— u2)
From Theorem 5, let Yi
have
▽'(no(w),2i)V'(n0(w),2i)|
, so that kYi k ≤ 1 and kE[Yi]k ≤ 1. Then we
G
G
∣∣M(∏0(w)) — Σ(∏0(w))k2 ≥ U√m,
(30)
with ptobability at most 2p exp (一 u2).
Combine equation 24, equation 29 and equation 30, we have
sup kM(w) - Σ(w)k2 ≥ sup
w∈W
w∈W
X u2n/2d(w, Wn) ∙ 4Gρ + U-G
n≥0	√m	m
u
G(4ργ2(W,d)+1)
√m
(31)
with probability at most (c0 + 2)p exp (一u2)
That completes the proof.
Now we provide the proof of Theorem 3.
Theorem 3 (Subspace Closeness) Under Assumption 1 and 2, with Vk (t) to be the top-k eigenvec-
tors of the population second moment matrix Σt and αt be the eigen-gap at t-th iterate such that
∖	∖ ʌ	∖ 、	r . 1 TV / ɪ ∖ - a 1 ■. 1 1 ∙ r 、
λk (Σt) - λk+1 (Σt) ≥ αt, for the Vk(t) in Algorithm 1, ifm ≥
0(Gρ√mpγ2(w,d))
mint αt2
2
, we have
Γ.. ʌ , . ʌ , . _	, .	, . ..
E [∣∣V4(t)V4(t)| - Vk(t)Vk(t)lk2
√mpγ2(W ,d)
αt√m
, ∀t ∈ [T].
(2)
Proof: Recall that Vk (t) is the top-k eigenspace of Mt. Let Vk (t) be the top-k eigenspace of ∑t.
⇒
where Π(k)
Mt
and Π(Σk) =
Vfc (t)Vfc (t)| - Vk(t)Vk(t)| = ∏M) (I - Π∑t)) + (I - ∏M) )Π∑t)	(32)
kVfc(t)Vfc(t)| - Vk(t)Vk(t)l∣2 ≤k∏M)(I - Π∑t))∣2 + k(i- ∏M))∏∑t)∣2.	(33)
Vk (t)Vk (t)| denotes the projection to the top-k subspace of the symmetric PSD Mt
Vk (t)Vk (t)| denotes the projection to the top-k subspace of the symmetric PSD Σt.
Then, from Davis-Kahan (Corollary 8 in McSherry (2004)) and using the fact for symmetric PSD
matrices eigen-values and singular values are the same, we have
k∏M)(I -Π∑t))∣2 ≤ 入(IkMt-『')
t	t	λk(Mt) - λk+1 (Σt)
M- πm )π∑t)∣2 ≤ λ *Mt -λ*M).
t t	λk(Σt) - λk+1 (Mt)
(34)
(35)
Recall, from Horn and Johnson (2012) (Section 4.3) and Golub and Van Loan (1996) (Section 8.1.2),
e.g., Corollary 8.1.6, we have
lλk (Mt)-"(夕斓 ≤ IlMt- ςM∣2 .
From Theorem 2 and Lemma 2, with Y = kMt - Σtk2, A = c, and B
4Gρ√ln pγ2(W,d)
√m
(36)
, we have
E [kMt - Σtk2] ≤O
l√lnpγ2 (W, d)
√m
(37)
Let c0
2
O (Gρ√lnpγ2(W, d)). For m ≥ α⅛, We have
E[k Mt-夕”2] ≤ √m ≤ -t-.
(38)
17
Published as a conference paper at ICLR 2021
Then, for equation 34, we have
≤	kMt 一夕”2
一λk(Mt) - λk+1(Σt)
=____________IlMt - ς∕∣2_________
(λk (夕/一λk + 10tD -("(夕/—λk (Mt))
≤	kMt 一夕”2
-α -IMt — ∑t∣2.
Then, for equation 35, we have
k(I- πΜ) )π∑kt)∣2 ≤ λ *Mt『2")
t t	λk(Σt) - λk+1(Mt)
=_______________kMt- Σtk2______________
(λk (夕/一" + 10/ + (4+1(夕/一λk + 1(Mt))
≤	kMt 一夕”2
-α -kMt — ∑t∣2.
(40)
Combining these two bounds and equation 37, with co = O (Gρ√ln pγ2(W, d)), we have
E [kVk(t)Vk(t)| - Vk(t)Vk(t)lk2] ≤
2αt
αt-E[kMt -Σtk2]
2co
√m
八 C0
αt - √m
-2
(41)
≤
Using equation 38 SUCh that -c⅛ ≤ Ot, we have
E hkVk(t)Vk(t)| - Vk(t)Vk(t)lk2i ≤ O (GPMap√mW, d)) .	(42)
That completes the proof.	■
Lemma 2 (Lemma 2.2.3 in Talagrand (2014)) Consider a r.v. Y ≥ 0 which satisfies
∀u > 0,P(Y ≥ U) ≤ A exp (--U2)
for certain numbers A ≥ 2 and B > 0. Then
EY ≤ CBplog A,
(43)
(44)
where C denotes a universal constant.
A.2 GEOMETRY OF GRADIENTS AND γ2 FUNCTIONS.
In this section, we provide more intuitions and explanations of γ2 functions. We justify our assump-
tions about the gradient space and provide more examples of the gradient space structure and the
corresponding γ2 functions.
At a high level, for a metric space (M, d), γ2 (M, d) is related to ,log N(M, d, E) where N(M, d, e)
is the covering number of M with balls with metric d, but it is considerably sharper. Such sharpen-
ing has happened in two stages in the literature: first, based on chaining, which considers an integral
over all E yielding the Dudley bound, and subsequently, based on generic chaining, which consid-
ers a hierarchical covering, developed by Talagrand and colleagues, and which yields the sharpest
bounds of this type. The official perspective of generic chaining is to view γ2 (M, d) as an upper
(and lower) bound on suprema of Gaussian processes indexed on M and with metric d [Theorem
2.4.1 in Talagrand (2014)].
18
Published as a conference paper at ICLR 2021
Considering d to be the `2 norm distance, γ2 (M, d) will be the same order as the Gaussian width
of M (Vershynin, 2018), which is a scaled version of the mean width of M. Structured sets (of
gradients) have small Gaussian widths, e.g., a Li unit ball in Rp has a GaUSSian width of O(√log P,
WheraS a L2 unit ball in Rp has a Gaussian width of O(√p).
To utilize the structure of gradients as example shown in Figure 2, instead of focusing on the
γ2(W, d), we can also derive the uniform convergence bound using the measurement d on the set of
population gradient m = Ez∈p [V(w, z)]. We consider a mapping f : W → M from the param-
eter space W to the gradient space M, where f can be considered as f (W) = Ez∈p [V(w, z)].
With mt = Ez∈P [V(w, z)] and M to be the space of the population gradient mt ∈ M,
the pseudo metric d(W, W0) can be written as d(W, W0) = d(f (W), f (W0)) = d(m, m0) with
m = Ez∈P [V(W, z)] and m0 = Ez∈P [V(W0, z)]. With such a mapping f, the admissible se-
quence ΓW = {Wn : n ≥ 0} of W in the proof of Theorem 2 corresponds to the admissible
sequence ΓM = {Mn : n ≥ 0} ofM. Theγ2(W,d) = inf ΓW supw∈W Pn≥0 2n/2d (W, Wn) =
infΓM supm∈M Pn≥0 2n/2d (m, Mn) = γ2(M, d). Considering d(m, m0) = km - m0k2, the
γ2(M, d) will be the same order as the Gaussian width of M, i.e., w(M) = Ev [supm∈Mhm, vi],
where V 〜N (0, Ip). Below, we provide more examples of the gradient space structure and the γ2
functions.
Ellipsoid. The Gaussian width w(M) depends on the structure of the gradient m. In Figure 2, we
observe that, for each coordinates of the gradient is of small value along the training trajectory and
thus M includes all gradients living in an ellipsoid, i.e., M = {mt ∈ Rp | Pjp=1 mt(j)2/e(j)2 ≤
1, e ∈ Rp}. Then we have γ2(M,d) ≤ cιw(M') ≤ c2∣∣e∣∣2 Talagrand (2014), where ci and c2
are absolute constants. If the elements of e sorted in decreasing order satisfy e(j) ≤ c3∕√j for all
j ∈ [p], then γ2(M, d) ≤ O (√log P).
Composition. Based on the composition properties of γ2 functions Talagrand (2014), one can con-
struct additional examples of the gradient spaces. If M = Mi + M2 = {mi + m2 , mi ∈
Mi, m2 ∈ M2}, the Minkowski sum, then γ2(M, d) ≤ c(γ2(Mi, d) + γ2(M2, d)) (Theorem
2.4.15 in Talagrand (2014)), where c is an absolute constant. If M is a union of several sub-
set, i.e., M = ∪hD=iMh, then by using an union bound on Theorem 2, we have γ2 (M, d) ≤
√log D maxh γ2(Mh, d). Thus, if M is an union of D = ps ellipsoids, i.e., polynomial in p, then
Y2(M, k ∙ ∣∣2) ≤ O (√slogp).
B Proofs for Section 3.2
In this section, we present the proofs for Section 3.2. Then we present the error rate for convex
problems in the subsequent section.
Theorem 4 (Smooth and Non-convex) For ρ-smooth function Ln (W), under Assumptions 1 and
2, let Λ = Pt=?1/% ,for any 3 δ > 0, with T = O(n2e2) and η = √T, PDP-SGD achieves:
T XXEkVk(t)Vk(t)|VLn(Wt)k2 ≤ O (kρG2) + O (AG…mW,d)lnP) .	(3)
Additionally, assuming the principal component of the gradient dominates, i.e., there exist c > 0,
such that T PT=I ∣∣[VLn(wt)]⊥k2 ≤ c1 PT=I ∣∣[VLn(wt)]k ∣∣2, we have
EkVLn(WR)k2 ≤ O (kρG-) + O (A"2"，d)lnP) ,	(4)
n	m
where wR is uniformly sampled from {wi, ..., wT}.
Proof: Recall that gt = ∣^ Pza∈βt V'(wt, zi). With Bt uniformly sampled from S, we have
—-	r	_ ʌ	,	、
Et [gt ]= VLn(Wt).	(45)
Recall that the update of Algorithm 1 is
|
gt = Vk V1 (gt + bt), and Wt+i = Wt — ηtgt.	(46)
19
Published as a conference paper at ICLR 2021
||
Let gt = gt + VkV∣bt, and ∆t = VkVkIgt - gt. Then We have
gt = gt + ∆	(47)
Since bt is a zero mean Gaussian vector, We have
Et [gt] = gt.
(48)
For ρ-smooth 5 function Ln(w), conditioned on wt, We have
EthLn (Wt+l)] ≤ Ln (Wt)+ EthD▽£n(Wt)，Wt+1 — WtEi + P^Et [1岛"2]
=Ln (Wt)- ηtEt hDvLn(Wt), gt + ∆t)] + 2η2Et [kgtk2]
=Ln (Wt)- ηt DVLn(Wt), VLn(Wt) + Eg]) + Pη2Et hkgtk2]
=Ln (Wt)- ηt ∣∣vLn(Wt)∣∣2 - ηt DVLn(Wt),Et[∆t]) + ρη2Et [kgtk2i
(49)
Rearrange the above inequality, We have
ηt
VLn(Wt)∣∣2
+ ηt Et DVLn(Wt), ∆tE ≤ Ln (Wt)-
X------------------}
^{^™
Dt,1
EthLn (Wt+1)] + 2η2 Ethkgtk2]
、--------------------------V-----
Dt,2
(50)
For Dt,1, let
_______________________ʌ	—	-_δ —	―	__ʌ -
VLn(Wt) =∏Qt[VLn(Wt)]+∏Q⊥ [VLn(Wt)]	(51)
1	/、	♦	-r`τ^ ∕>∖τ'τ- ∕>∖τ t i i ∙	.<	∙	∙ol*,ι	Ii	r∙ /、
Where Qt is Vk(t)Vk(t)I and ΠQt is the projection. Qt⊥ is the null space of Qt.
— -____ ʌ , 一 ʌ , . ʌ , ..__ʌ , 、
∏Qt[VL n(Wt)] = Vk (t)Vk (t)lVL n (Wt)	(52)
and
∏Q⊥ [VL n(Wt)] = [I- Vk (t)14 (t)lj VL n(Wt).	(53)
We have
„ ʌ , . ʌ , . _ - „ ʌ , . ʌ , . _ -
△t = ∏Qt [Vk(t)Vk(t)lgt - gt] + ∏Q⊥ [Vk(t)Vk(t)lgt - gt]
= -ΠQt⊥ [gt].	(54)
So that We have
Dt,1 = Et DVLn(Wt), ∆t)
=-Et DnQtVLn(Wt)]+∏Q⊥ [VLn(Wt)], ∏q⊥ [gt])
=-Et D∏q⊥[vLn(Wt)], [i - Vk(t)V4(t)1] gt)
=-([I - Vk(t)Vk (t)lj VL n(Wt)JI- Vk(t)Vk (t)lj VL n(Wt ),
=-VLn(Wt)τ [I - Vk(t)Vk(t)lj VLn(Wt) .	(55)
Bringing the above to equation 50, the right-hand side of equation 50 becomes
ηtkVL n(Wt)k2 + ηtDt,1 = ηtVL n (Wt )l [I] VL n(Wt) - ηtVLn(Wt)T II - Vk(t)Vk (t)lJ VL n(Wt)
=ηtVLn(Wt)T [Vk(t)Vk(t)lj VLn(Wt)
=ηtkVk(t)Vk(t)τVLn(Wt)k2 .	(56)
5The Assumption 2 suggests that the Ln(W) is P-Smooth.
20
Published as a conference paper at ICLR 2021
So that we have
ηt 1% (t)Vk (t)lVL n(wt)k2 ≤ L n (Wt) - Et [Ln (Wt+l)] + ρη2 EtIjlgtk2]
'----------}
{z^^
Dt,2
For Dt,2, we have
Et Ilgtk2 = Et 幅VTgt + VkVTbt
2
=Et "％ VT 旬：+ 11 Vk VT bt
≤ G2 + kσ2.
(57)
(58)
Thus,
Dt,2 = Et [∣∣gt∣∣2 + ∣∣∆t∣∣2] ≤ G2 + kσ2.	(59)
Bringing the upper bound of Dt2 to equation 57, setting η =方,using telescoping sum and taking
the expectation over all iterations, we have
TXXEkVk(t)Vk(t)|VLn(wt)k2 ≤ Ln(WM- Ln + ρ(G2+kσ2)
T t=1	T	2 T
(60)
With triangle inequality and Theorem 3, we have
M(t)Vk(t)lVLn (wt)∣∣2 ≤ 21∣%(t)Vk(t)lVLn (Wt)∣∣2 + 2 ∣∣(Vk(t)Vk(t)| - Vk(t)Vk(t)1) VLn (wt)∣∣2
≤ 2 W (t)Vk (t)|VL n (Wt )∣∣2 + O ( G4"*(W ㈤).	(61)
With equation 60, we have
ɪ XX EkVk (t)Vk (t)∣VL n(wt)k2 ≤ L n(√T/- Ln + RTQ+O ( »2Yy ⑷n P ).
t=1	(62)
Where λ = PT=I α12.
Let [vL n (wt)ik	=	Vk (t)Vk (RVL n(wt) and [vL n(w∕⊥	=	VL n(wt)-
,. ,. ʌ
Vk (t)Vk (t)|VL n(wt).
Assuming there exist c > 0, We have
XX E∣∣[vLn(Wt)i⊥
2 T ∣	k∣2
≤ C X E∣∣[vLn(Wt)]
(63)
Then We have
ɪ XEkVLn(wt)k2 ≤ (1 + C) LLn(√1) - Ln + pG⅛σ^ + O (AG4Lp2Y2(WMlnP)!.
T t=1	T/2	T	m
(64)
EkVLn(Wr)∣∣2 ≤ O
Take T = n2e2, with E [∣∣VLn(wκ)k2] = T PT=IEkVLn(Wt)k2,We have
kρG2 ) + O ( ΛG4ρ2γ2(W, d) ln P
n	m
where WR is uniformly sampled from {W1, ..., WT}.
(65)
21
Published as a conference paper at ICLR 2021
C Error Rate of Convex Problems
For the convex and Lipschitz functions, we consider the low-rank structure of the gradient space, i.e,
the population gradient second momment Σt is of rank-k, which is a special case of the principal
gradient dominate assumption when | [▽£n(wt)]⊥ ∣∣2 = 0.
Theorem 6 (Convex and Lipschitz) For G-Lipschitz and convex function Ln (w), under Assump-
tions 1,2 and assuming ∑t is of rank-k, let Λ = Pt=T 1/at, for any e, δ > 0, with T = O(n2e2),
Step size η =方,the PDP-SGD achieves
E hLn(w)i - Lnw) ≤ O (JkG) + O (AGργ2√m⑷nP),
(66)
PT wt	?
where W = ^t=T—, and w? is the minima of Ln(W).
PDP-SGD also demonstrates an improvement from a factor of P to k compared to the error rate
of DP-SGD for convex functions (Bassily et al., 2014; 2019a). PDP-SGD also has the subspace
reconstruction error, depending on the γ2 function and eigen-gap term Λ = PT=I 1/at/T. From
previous discussions, the γ2 is a constant with suitable assumptions of the gradient structure. For
the eigen-gap term, if αt stays as a constant in the training procedure as shown by Figure 1, Λ will
be a constant and the bound scales logarithmically with p. If one assumes the eigen-gap at decays
as training proceed, e.g., at = t172 for t > 0, then We have Λ = O(√T). In this case, with
T = O(n22), PDP-SGD requires public data size m = O(n).
Recently, Kairouz et al. (2020) propose a noisy version of AdaGrad algorithm for unconstrained con-
vex empirical risk minimization. Their algorithm operates with only private data and also achieves
a dimension-free excess risk bound, i.e., O(r/n), by assuming the gradients along the path of
optimization lie in a low-dimensional subspace with constant rank r. The bounds are not directly
comparable since the assumptions in Kairouz et al. (2020) are different from those in this paper,
i.e., Kairouz et al. (2020) assume that the accumulated gradients along the training process lie in a
constant subspace which requires the rank of the gradient space does not explode when adding more
stochastic gradients. Our work does not impose such a constant subspace assumption on Σt allowing
the subspace of Σt to be different along the training process. In other words, our bounds hold even
when the rank of the accumulated stochastic gradients space increases as training proceeds.
Proof of Theorem 6: By the convexity of Ln(w), we have
Ln(wt) - Ln(w ) ≤ hwt - w?, VLn(Wt)〉.	(67)
||
At iteration t, we have wt+ι = Wt — ηtgt, where gt = Vk Vk|gt + Vk Vk|bt.
||
Let gt = gt + VkV∣bt, and ∆t = VkV∣gt - gt. Then we have
gt = gt + ∆t.	(68)
Recall that gt = ∣^ Pza∈Bt V'(wt, zi). With Bt uniformly sampled from S, we have
—-	r	_ ʌ	,	、
Et [gt ] = VLn(Wt).	(69)
Since bt is a zero mean Gaussian vector, we have Et[gt] = gt.
22
Published as a conference paper at ICLR 2021
By convexity, conditioned at wt , we have
一 . ʌ	.	ʌ	,	、r
Et[Ln(Wt) - Ln (w?)]
≤ Et hwt - w?, gti
=EthWt - w?, gti
=-1 EthWt - w?, Wt - wt+ι - ηt∆ti
ηt
=厂Et (kwt - w?k2 + η2Ilgtk2 - kwt+ι - w? - ηt∆tk2)
2ηt	t
=药 Et (kwt- w?k2 + η2 Ilgtk2 -IlWt+ι- w*k2- η2 心力 k2 + 2hwt+ι- w?,ηt∆ti)
=2n- Et (kwt- w*k2 -kwt+ι- w*k2) + η Et[kgtk2]- η Et[ldk2]
+ Ethwt+1 - w?, ∆ti
(a) 1	η
≤ 药Et (IlWt- w*ll2 -kwt+ι - w*ll2) + y (G2 + kσ) + BEtII∆t|∣2,	(7O)
where (a) is true since
Etkgtk2 ≤ Et[∣∣gt∣∣2 + IVk VT bt∣∣2]
≤ G2 + kσ2,	(71)
and Iwt+1 - w? I2 ≤ B 6.
Let nt = 方,taking the expectation over all iterations and sum over t = 1,.., T, we have
TE XX Ln(Wt) - Ln(w?)"即〜粤。2 + kσ2 + B P1=1TEtk^2 .	(72)
From Theorem 3, we have
Et [I∆tI2] = E h∣∣Vk(t)Vk(t)lgt - V(t)V(t)lgt∣∣2i
≤ E h∣∣Vk(t)Vk(t)τ - v (t)v (t)ι∣∣2 Gi
≤ GE h∣∣Vk(t)Vk(t)T - VUt)Vk(t)τ∣∣2 + ||%(%)%(%)| - V(t)V(t)τ∣U
≤ O ( GPln PY2(W ,d))
- 卜	αt√m	),
(73)
where the last inequality holds because the Σt is of rank k and V (t) = Vk(t).
Bring this to equation 72, use the fact that Iw0 - w? I < B, with Jensen’s inequality we have
E [Ln(W)i - Ln(w?) ≤ B2 +2√T+ kσ2 + O ( AGPln√⅞(W⑼).(74)
Where Λ = p|=i α1t.
With σ2 = G⅞ ,let T = n2e2, we have
E hL n(W)i - Ln(w?) ≤ O (kGB2) + O ( AGP ln√γ2(W ,d) ) .	(75)
That completes the proof.	■
6For convex problem, we consider w ∈ H, where H = {w : kwk ≤ B}.
23
Published as a conference paper at ICLR 2021
D Experimental Setup and Additional Results
Datasets and Network Structure. The MNIST and Fashion MNIST datasets both consist of 60,000
training examples and 10,000 test examples. To construct the private training set, we randomly
sample 10, 000 samples from the original training set of MNIST, then we randomly sample 100
samples from the rest to construct as the public dataset 7. Details refer to Table 2. For both MNIST
and Fashion MNIST, we use a convolutional neural network that follows the structure in Papernot
et al. (2020) whose architecture is described in Table 1. All experiments have been run on NVIDIA
Tesla K40 GPUs.
Table 1: Network architecture for MNIST and Fashion MNIST.
Layer	Parameters
Convolution Max-Pooling Convolution Max-Pooling Fully connected Softmax	16 filters of 8 X 8, strides 2 2×2 32 filters of 4x4, strides 2 2×2 32 units 10 units
Table 2: Neural network and datasets setup.
Dataset	Model	features	classes	Training size	Public size	Test size
MNIST	CNN	28× 28	10	10,000	100	10,000
Fashion MNIST	CNN	28× 28	10	10,000	100	10,000
Hyper-parameter Setting. We consider different choices of the noise scale, i.e., σ =
{18, 14, 10, 8, 6, 4} for MNIST and σ = {18, 14, 10, 6, 4, 2} for Fashion MNIST. Cross-entropy
is used as our loss function throughout experiments. The mini-batch size is set to be 250 for
both MNIST and Fashion MNIST. For the step size, we follow the grid search method with
search space {0.01, 0.05, 0.1, 0.2} to tune the step size for MNIST and the search space is
{0.01, 0.02, 0.05, 0.1, 0.2} for Fashion MNIST. We choose the step size based on the training accu-
racy at the last epoch. The best step sizes for DP-SGD and PDP-SGD for different privacy levels are
presented in Table 3 and Table 4 for MNIST and Fashion MNIST, respectively. For training, a fixed
budget on the number of epochs i.e., 30 is assigned for the each task. We repeat each experiments
3 times and report the mean and standard deviation of the accuracy on the training and test set. For
PDP-SGD, the projection dimension k is a hyper-parameter and it illustrates a trade-off between the
reconstruction error and the noise reduction. A small k implies more noise amount will be reduced,
and a larger reconstruction error will be introduced. We explored k = {20, 30, 50} for MNIST and
k = {30, 50, 70} for Fashion MNIST, and we found that k = 50 and k = 70 achieve the best perfor-
mance for MNIST and Fashion MNIST respectively among the search space we consider. Instead of
doing the projection for all epochs, we also explored a start point for the projection, i.e., executing
the projection from the 1-th epoch, 15-th epoch. The information of projection dimension k and the
starting epoch for projection are also given in Table 3 and Table 4 for MNIST and Fashion MNIST,
respectively.
Privacy Parameter Setting. Since gradient norm bound G is unknow for deep learning, we fol-
low the gradient clipping method in Abadi et al. (2016) to guarantee the privacy. We implement
the micro-batch clipping method in PyTorch 8. We use micro-batch = 1 and micro-batch = 5
for MNIST and Fashion MNIST, respectively. Note that training with micro-batch clipping will
need the noise scaled by micro-batch size to guarantee the same privacy. But it takes less time
than training with per-sample clipping, i.e., micro-batch = 1. We follow the Moment Accoun-
tant (MA) method (Bu et al., 2019) to calculate the accumulated privacy cost, which depends on
the number of epochs, the batch size, δ, and noise variance σ. With 30 epochs, batch size 250,
7PDP-SGD can work with larger training set as well. We randomly sample 10,000 samples due to the
limitation of computation resources, e.g., GPUs.
8We implement the clipping method based on this repository: https://github.com/ChrisWaites/pyvacy.
24
Published as a conference paper at ICLR 2021
Table 3: HyPer-Parameter Settings for DP-SGD and PDP-SGD for MNIST.
	σ=18 ( = 0.23)	σ = 14	σ ( = 0.30) (	= 10 = 0.42)	σ=8 ( = 0.53)	σ=6 ( = 0.72)	σ=4 ( = 1.09)
	DP-SGD							
SteP size	0.05	0.05	0.05	0.05	0.1	0.1
PDP-SGD						
SteP size	0.1	0.2	0.2	0.1	0.1	0.2
Starting ePoch for Projection	1	1	1	15	15	15
Projection dimension k	50	50	50	50	50	50
10, 000 training samples, and fixing δ = 10-5, the is {2.41, 1.09, 0.72, 0.42, 0.30, 0.23} for
σ ∈ {2, 4, 6, 10, 14, 18} for Fashion MNIST. For MNIST, is {1.09, 0.72, 0.53, 0.42, 0.30, 0.23}
corresPonding to σ = {4, 6, 8, 10, 14, 18}. Note that the Presented in this PaPer is w.r.t. a subset
i.e., 10, 000 samPles from MNIST and Fashion MNIST. Also, one can fix the value of and do a
search over the ePochs, batch size and noise scale to boost the Performance for a fixed Privacy level
. We omit such a comPlicated hyPer-Parameter tuning since it has a high risk of Privacy leakage.
Table 4: HyPer-Parameter settings for DP-SGD and PDP-SGD for Fashion MNIST.
	σ=18	σ = 14	σ = 10		σ=6	σ=4	σ=2
	( = 0.23)	( = 0.30) (	= 0.42)	( = 0.72)	( = 1.09)	( = 2.41)
	DP-SGD							
SteP size	0.01	0.01	0.01	0.02	0.02	0.05
PDP-SGD						
SteP size	0.01	0.01	0.02	0.02	0.02	0.05
Starting ePoch for Projection	15	15	15	15	15	15
Projection dimension k	70	70	70	70	70	70
Additional Experimental Results. Training dynamics of DP-SGD and PDP-SGD with different
Privacy levels are Presented in Figure 7 and Figure 8 resPectively for MNIST and Fashion MNIST.
The results suggest that for small , PDP-SGD can effeciently reduce the noise variance injected to
the gradient, which imProves the training and test accuracy over DP-SGD.
In order to understand the role of Projection dimension k, we run PDP-SGD with Projection starting
from the first ePoch. Figure 9 rePorts the PDP-SGD with k ∈ {10, 20, 30, 50} for MNIST with
= 0.30 (Figure 9(a)) and = 0.53 (Figure 9(b)). Among the choice of k, we can see that
PDP-SGD with k = 50 Performs better that the others in terms of the training and test accuracy.
PDP-SGD with k = 10 Proceeds slower than PDP-SGD with k = 20 and k = 50. This is due to the
larger reconstruction error introduced by Projecting the gradient to a much smaller subsPace, i..e,
k = 10. However, comPared to the gradient dimension p ≈ 25, 000, it is imPressive that PDP-SGD
with k = 50 which Projects the gradient to the a much smaller subsPace, can achieve better accuracy
than DP-SGD.
We also emPirically evaluate the effect of the Public samPle size m. Figure 11(a) and Figure 11(b)
Present the training and test accuracy for PDP-SGD with m ∈ {50, 150, 200} for = 0.23 and
= 1.09 for Fashion MNIST dataset. The training and test accuracy of PDP-SGD increases as
the Public samPle size increases from 50 to 150. This is consistent with the theoretical analysis
that increasing m helPs reducing the subsPace reconstruction error as suggested by the theoretical
bound. Also, PDP-SGD with m = 150 and m = 200 Performs slightly better that m = 50 in terms
of the training and test accuracy. The results suggest that while a small amount of Public datasets
are not sufficient for training an accurate Predictor, they Provide useful gradient subsPace Projection
and accuracy imProvement over DP-SGD.
We also comPare PDP-SGD and DP-SGD for different number of training samPles, i.e., MNIST
with 20,000 samPles (Figure 12(a)) and Fashion MNIST with 50,000 samPles (Figure 12(a)) (100
25
Published as a conference paper at ICLR 2021
public samples for both case). The observation that PDP-SGD outperforms DP-SGD for small
regime in Figure 3 also holds for other number of training samples.
We also explore PDP-SGD with sparse eigen-space computation, i.e., update the projector every s
iterates. Note that PDP-SGD with s = 1 means computing the top eigen-space at every iteration.
Figure 13 reports PDP-SGD with s = {1, 10, 20} for (a) MNIST with 50,000 samples and (b)
Fashion MNIST with 50,000 samples showing that there is a mild decay for PDP-SGD with fewer
eigen-space computation. PDP-SGD with a reduced eigen-space computation also improves the
accuracy over DP-SGD.
(a) Training accuracy, = 0.23
(b) Training accuracy, = 0.30
(c) Training accuracy, = 0.42
(d) Test accuracy, = 0.23
80
(f) Test accuracy, = 0.42
Figure 7: Comparison of DP-SGD and PDP-SGD for MNIST. (a-c) report the training accuracy and
(d-f) report the test accuracy for = {0.23, 0.30, 0.42}. The X-axis is the number of epochs, and
the Y-axis is the train/test accuracy. DPD-SGD outperforms DP-SGD for small .
⅛ 60
Lo-
(e) Test accuracy, = 0.30
O 5	10	15	20	25	30
(a) Training accuracy, = 0.23
PDP-SGD
10
15.0 17.5 20.0 22.5 25.0
(b) Training accuracy,
(c) Test accuracy, = 0.23
605040302010
AUE.I Fme-l-lsəl
27.5 30J
0.30
PDP-SGD
15.0 17.5 20.0 22.5 25.0 27.5 30J
(d) Test accuracy, = 0.30
Figure 8: Comparison of DP-SGD and PDP-SGD for Fashion MNIST. (a-b) report the training
accuracy and (c-d) report the test accuracy for = {0.23, 0.30}. Learning rare is 0.01 for both
PDP-SGD and DP-SGD. PDP-SGD starts projection at 15-th epoch. The X-axis is the number of
epochs, and the Y-axis is the train/test accuracy. DPD-SGD outperforms DP-SGD for small .
26
Published as a conference paper at ICLR 2021
6	10	20	30	6	10	20	30
number of epochs	number of epochs
(a) MNIST, = 0.30
6	10	20	30
number of epochs
6	10	20	30
number of epochs
(b) MNST, = 0.53
PDP-SGD1
PDP-SGD1
PDP-SGD1
PDP-SGD1
PDP-SGD1
PDP-SGD1
PDP-SGD1
PDP-SGD1
Figure 9: Training accuracy and test accuracy for PDP-SGD with k = {10, 20, 30, 50} for (a)
MNIST with = 0.30; (b) MNIST with = 0.53. The X-axis and Y-axis refer to Figure 4. PDP-
SGD with k = 50 performs better that the others in terms of the training and test accuracy.
80604020
AUSnDUe u's上
O 10	20	30 O 10	20	30
number of epochs	number of epochs
(a) MNIST, e = 0.30
0	10	20	30	0	10	20	30
number of epochs	number of epochs
(b) MNST, e = 0.42
(a) MNIST, = 0.23
Figure 11: Training accuracy and test accuracy for PDP-SGD with m = {50, 150, 200} for (a)
MNIST with = 0.23; (b) MNIST with = 0.43. The X-axis and Y-axis refer to Figure 4. PDP-
SGD with m = 150 and m = 200 performs slightly better that the other one in terms of the training
and test accuracy.
Figure 10: Training accuracy and test accuracy for PDP-SGD with m = {50, 100, 150} for (a)
MNIST with = 0.30; (b) MNIST with = 0.53. The X-axis and Y-axis refer to Figure 4. PDP-
SGD with m = 150 and m = 100 perform better that the others in terms of the training and test
accuracy.
(b) MNST, = 1.09
Figure 12: Training and test accuracy for DP-SGD and PDP-SGD with different privacy levels for
(a) MNIST with 20,000 samples and (b) Fashion MNIST with 50,000 samples. The X-axis and
Y-axis refer to Figure 3. For small privacy loss , PDP-SGD outperforms DP-SGD.
27
Published as a conference paper at ICLR 2021
2	4	6	8
number of epochs
number of epochs
(a) MNIST, = 0.06	(b) Fashion MNIST, = 0.07
Figure 13: Training and test accuracy for PDP-SGD with different frequency of eigen-space
computation for (a) MNIST with 50,000 samples and (b) Fashion MNIST with 50,000 samples.
s = {5, 10, 20} is the frequency of subspace update, i.e., compute the eigen-space every s iterates.
The X-axis and Y-axis refer to Figure 4. PDP-SGD with a reduced eigen-space computation also
improves the accuracy over DP-SGD.
28