Published as a conference paper at ICLR 2021
On the Universality of the Double Descent
Peak in Ridgeless Regression
David Holzmuller
University of Stuttgart
Faculty of Mathematics and Physics
Institute for Stochastics and Applications
david.holzmueller@mathematik.uni-stuttgart.de
Ab stract
We prove a non-asymptotic distribution-independent lower bound for the expected
mean squared generalization error caused by label noise in ridgeless linear regres-
sion. Our lower bound generalizes a similar known result to the overparameterized
(interpolating) regime. In contrast to most previous works, our analysis applies to
a broad class of input distributions with almost surely full-rank feature matrices,
which allows us to cover various types of deterministic or random feature maps.
Our lower bound is asymptotically sharp and implies that in the presence of label
noise, ridgeless linear regression does not perform well around the interpolation
threshold for any of these feature maps. We analyze the imposed assumptions in
detail and provide a theory for analytic (random) feature maps. Using this the-
ory, we can show that our assumptions are satisfied for input distributions with a
(Lebesgue) density and feature maps given by random deep neural networks with
analytic activation functions like sigmoid, tanh, softplus or GELU. As further ex-
amples, we show that feature maps from random Fourier features and polynomial
kernels also satisfy our assumptions. We complement our theory with further ex-
perimental and analytic results.
1	Introduction
Seeking for a better understanding of the successes of deep learning, Zhang et al. (2016) pointed out
that deep neural networks can achieve very good performance despite being able to fit random noise,
which sparked the interest of many researchers in studying the performance of interpolating learning
methods. Belkin et al. (2018) made a similar observation for kernel methods and showed that clas-
sical generalization bounds are unable to explain this phenomenon. Belkin et al. (2019a) observed a
“double descent” phenomenon in various learning models, where the test error first decreases with
increasing model complexity, then increases towards the “interpolation threshold” where the model
is first able to fit the training data perfectly, and then decreases again in the “overparameterized”
regime where the model capacity is larger than the training set. This phenomenon has also been dis-
covered in several other works (BOs & OPPer,1997; Advani & Saxe, 2017; Neal et al., 2018; SPigler
et al., 2019). Nakkiran et al. (2019) performed a large empirical study on deep neural networks and
found that double descent can not only occur as a function of model caPacity, but also as a function
of the number of training ePochs or as a function of the number of training samPles.
Theoretical investigations of the double descent Phenomenon have mostly focused on sPecific un-
regularized (“ridgeless”) or weakly regularized linear regression models. These linear models can
be described via i.i.d. samPles (x1, y1), . . . , (xn, yn) ∈ Rd, where the covariates xi are maPPed to
feature rePresentations zi = φ(xi) ∈ Rp via a (Potentially random) feature maP φ, and (ridgeless)
linear regression is then Performed on the transformed samPles (zi, yi). While linear regression
with random features can be understood as a simPlified model of fully trained neural networks, it is
also interesting in its own right: For examPle, random Fourier features (Rahimi & Recht, 2008) and
random neural network features (see e.g. Cao et al., 2018; ScardaPane & Wang, 2017) have gained
a notable amount of attention.
1
Published as a conference paper at ICLR 2021
Unfortunately, existing theoretical investigations of double descent are usually limited in one or
more of the following ways:
(1)	They assume that the zi (or a linear transformation thereof) have (centered) i.i.d. compo-
nents. This assumption is made by Hastie et al. (2019), while Advani & Saxe (2017) and
Belkin et al. (2019b) even assume that the zi follow a Gaussian distribution. While the
assumption of i.i.d. components facilitates the application of some random matrix theory
results, it excludes most feature maps: For feature maps φ with d < p, the zi will usually
be concentrated on a d-dimensional submanifold of Rp , and will therefore usually not have
i.i.d. components.
(2)	They assume a (shallow) random feature model with fixed distribution of the xi, e.g. an
isotropic Gaussian distribution or a uniform distribution on a sphere. Examples for this are
the single-layer random neural network feature models by Hastie et al. (2019) in the un-
regularized case and by Mei & Montanari (2019); d’Ascoli et al. (2020a) in the regularized
case. A simple Fourier model with d = 1 has been studied by Belkin et al. (2019b). While
these analyses provide insights for some practically relevant random feature models, the
assumptions on the input distribution prevent them from applying to real-world data.
(3)	Their analysis only applies in a high-dimensional limit where n, p → ∞ and n/p → γ,
where γ ∈ (0, ∞) is a constant. This applies to all works mentioned in (1) and (2) except
the model by Belkin et al. (2019b) where the zi follow a standard Gaussian distribution.
In this paper, we provide an analysis under significantly weaker assumptions. We introduce the basic
setting of our paper in Section 2 and Section 3. Our main contributions are:
•	In Section 4, we show a non-asymptotic distribution-independent lower bound for the ex-
pected excess risk of ridgeless linear regression with (random) features. While the un-
derparameterized bound is adapted from a minimax lower bound in Mourtada (2019), the
overparameterized bound is new and perfectly complements the underparameterized ver-
sion. The obtained general lower bound relies on significantly weaker assumptions than
most previous works and shows that there is only limited potential to reduce the sensitivity
of unregularized linear models to label noise via engineering better feature maps.
•	In Section 5, we show that our lower bound applies to a large class of input distributions and
feature maps including random deep neural networks, random Fourier features and polyno-
mial kernels. This analysis is also relevant for related work where similar assumptions are
not investigated (e.g. Mourtada, 2019; Muthukumar et al., 2020). For random deep neural
networks, our result requires weaker assumptions than a related result by Nguyen & Hein
(2017).
•	In Section 6 and Appendix C, we compare our lower bound to new theoretical and experi-
mental results for specific examples, including random neural network feature maps as well
as finite-width Neural Tangent Kernels (Jacot et al., 2018). We also show that our lower
bound is asymptotically sharp in the limit n, p → ∞.
Similar to this paper, Muthukumar et al. (2020) study the “fundamental price of interpolation” in the
overparameterized regime, providing a probabilistic lower bound for the generalization error under
the assumption of subgaussian features or (suitably) bounded features. We explain the difference
to our lower bound in detail in Appendix L, showing that our overparameterized lower bound for
the expected generalization error requires significantly weaker assumptions, that it is uniform across
feature maps and that it yields a more extreme interpolation peak.
Our lower bound also applies to a large class of kernels if they can be represented using a feature map
with finite-dimensional feature space, i.e. p < ∞. For ridgeless regression with certain classes of
kernels, lower or upper bounds have been derived (Liang & Rakhlin, 2020; Rakhlin & Zhai, 2019;
Liang et al., 2019). However, as explained in more detail in Appendix K, these analyses impose
restrictions on the kernels that allow them to ignore “double descent” type phenomena in the feature
space dimension p.
Beyond Double Descent, a series of papers have studied “Multiple Descent” phenomena theoret-
ically and empirically, both with respect to the number of parameters p and the input dimension
d. Adlam & Pennington (2020) and d’Ascoli et al. (2020b) theoretically investigate Triple Descent
phenomena. Nakkiran et al. (2020) argue that Double Descent can be mitigated by optimal regular-
ization. They also empirically observe a form of Triple Descent in an unregularized model. Liang
2
Published as a conference paper at ICLR 2021
et al. (2019) prove an upper bound exhibiting infinitely many peaks and empirically observe Mul-
tiple Descent. Chen et al. (2020) show that in ridgeless linear regression, the feature distributions
can be designed to control the locations of ascents and descents in the double descent curve for
a “dimension-normalized” noise-induced generalization error. Our lower bound provides a funda-
mental limit to this “designability” of the generalization curve for methods that can interpolate with
probability one in the overparameterized regime.
Proofs for our statements can be found in the appendix. We provide code to reproduce all of our
experimental results at
https://github.com/dholzmueller/universal_double_descent
and we will provide the computed data at https://doi.org/10.18419/darus-1771.
2	Basic Setting and Notation
Following Gyorfi et al. (2002), We consider the scenario where the samples (Xi, yi) ofa data set D =
((x1, y1), . . . , (xn, yn)) ∈ (Rd × R)n are sampled independently from a probability distribution P
on Rd X R, i.e. D 〜Pn.1 We define
Q>∖	(yι ʌ
X :=	. I ∈ Rn×d,	y :=	. I ∈ Rn .
xn>	yn
We also consider random variables (x,y)〜P that are independent of D and denote the distribution
of x by PX . The (least squares) population risk of a function f : Rd → R is defined as
RP(f) := Ex,y(y - f(x))1 2 .
We assume Ey2 < ∞. Then, RP is minimized by the target function fp given by
fP(χ) = E(y∣χ),
we have RP (fp) < ∞, and the excess risk (a.k.a. generalization error) of a function f is
RP(f) - Rp(fP) = Eχ(f(χ) - fp(χ))2.
Notation For two symmetric matrices, we write A	B if A-B is positive semidefinite and A
B if A - B is positive definite. For a symmetric matrix S ∈ Rn×n, we let λ1(S) ≥ . . . ≥ λn(S)
be its eigenvalues in descending order. We denote the trace of A by tr(A) and the Moore-Penrose
pseudoinverse of A by A+. For φ : Rd → Rp and X ∈ Rn×d, we let φ(X) ∈ Rn×p be the matrix
with φ applied to each of the rows of X individually. For a set A, we denote its indicator function
by 1A . For a random variable x, we say that x has a Lebesgue density if PX can be represented
by a probability density function (w.r.t. the Lebesgue measure). We say that x is nonatomic if for
all possible values xe, P(x = xe) = 0. We denote the uniform distribution on a set A, e.g. the
unit sphere Sp-1 ⊆ Rp, by U (A). We denote the normal (Gaussian) distribution with mean μ and
covariance Σ by N(μ, Σ). For n ∈ N, we define [n] := {1,..., n}.
We review relevant matrix facts, e.g. concerning the Moore-Penrose pseudoinverse, in Appendix B.
3	Linear Regression With (Random) Features
The most general setting that we will consider in this paper is ridgeless linear regression in random
features: Given a random variable θ that is independent from the data set D and an associated
random feature map φθ : Rd → Rp , we define the estimator
fX,y,θ (x) := φθ(x)>φθ(X)+y ,
1Although many of our theorems apply to general domains xi ∈ X and not just X = Rd, we set X = Rd
for notational simplicity. We require X = Rd whenever we assume that the distribution of the xi has a
Lebesgue density or work with analytic feature maps.
3
Published as a conference paper at ICLR 2021
which simply performs unregularized linear regression with random features. As a special case, the
feature map φθ may be deterministic, in which case we drop the index θ. An even more specialized
case is ordinary linear regression, where d = p and φθ = id, yielding fX,y (x) = x>X+y.
As described in Hastie et al. (2019), the ridgeless linear regression parameter β := φθ (X)+y
•	has minimal Euclidean norm among all parameters β minimizing kφθ (X)β - yk22,
•	is the limit of gradient descent with sufficiently small step size on L(β) := kφθ(X)β-yk22
with initialization β(0) := 0, and
•	is the limit of ridge regression with regularization λ > 0 for λ & 0:
βb = lim φθ(X)>(φθ(X)φθ(X)> + λIn)-1y .	(1)
λ&0
For a fixed feature map φ, the kernel trick provides a correspondence between ridgeless linear re-
gression with φ and ridgeless kernel regression with the kernel k(x, X) := φ(x)>φ(X) via
fX,y(x) = φ(x)>φ(X)+y = φ(x)>φ(X)>(φ(X)φ(X)>)+y = k(x, X)k(X, X)+y,	(2)
where
k(x, x1)
k(x, X):=	.
k(x, xn)
k(x1, x1) . . . k(x1, xn)
k(X, X ):=	.	...	.
k(xn, x1) . . . k(xn, xn)
4 A Lower B ound
In this section, we prove our main theorem, which provides a non-asymptotic distribution-
independent lower bound on the expected excess risk.
The expected excess risk Eχ,y,θRP(fx,y,θ) - RP(fp) can be decomposed into several different
contributions (see e.g. d’Ascoli et al., 2020a). In the following, we will focus on the contribution
of label noise to the expected excess risk for the estimators fX,y,θ considered in Section 3. Using
a bias-variance decomposition with respect to y, it is not hard to show that the label-noise-induced
error provides a lower bound for the expected excess risk:
ENoise := EX,y,θ,χ (fX,y,θ(X) - EyIX fX,y,θ(X))2
≤ EX,θ,χ hEy|X (fX,y,θ(X) - EyIX fX,y,θ(X))2 + (EyIX fX,y,θ(X) - fP(X))[
=EX,y,θ,χ (fX ,y,θ(X) - fP(X))
=EX,y,θ(RP(fX,y,θ) - RP(fP)) ∙
For linear models as considered here, it is not hard to see that ENoise does not depend on fP and is
equal to the expected excess risk in the special case fP ≡ 0.
In the following, we first consider the setting where the feature map φ is deterministic. We will con-
sider linear regression on z := φ(X) and Z := φ(X) and formulate our assumptions directly w.r.t.
the distribution PZ of z, hiding the dependence on the feature map φ. While the distribution PX of
X is usually fixed and determined by the problem, the distribution PZ can be actively influenced by
choosing a suitable feature map φ. We will analyze in Section 5 how the assumptions on PZ can be
translated back to assumptions on PX and assumptions on φ.
Remark 1. For typical feature maps, we have p > d, PZ is concentrated on a d-dimensional sub-
manifold of Rp and the components of z are not independent. A simple example (cf. Proposition 8)
is a polynomial feature map φ : R1 → Rp, x 7→ (1, x, x2, . . . , xp-1). The imposed assumptions on
PZ should hence allow for such distributions on submanifolds and not require independent compo-
nents.	J
Definition 2. Assuming that Ekzk22 < ∞, i.e. (MOM) in Theorem 3 holds, we can define the
(positive semidefinite) second moment matrix
Σ := Ez〜PZ [zz>] ∈ Rp×p .
If Ez = 0, Σ is also the covariance matrix of z . If Σ is invertible, i.e. (COV) in Theorem 3 holds,
the rows Wi := Σ-1∕2Xi of the “whitened” data matrix W := ZΣ-1/2 satisfy Ewiw> = Ip. J
4
Published as a conference paper at ICLR 2021
With these preparations, we can now state our main theorem. Its assumptions and the obtained lower
bound will be discussed in Section 5 and Section 6, respectively.
Theorem 3 (Main result). Let n,p ≥ 1. Assume that P and φ satisfy:
(INT) Ey2 < ∞ and hence RP (fp) < ∞,
(NOI) Var(y|z) ≥ σ2 almost surely over z,
(MOM) Ekzk22 < ∞, i.e. Σ exists and is finite,
(COV) Σ is invertible,
(FRK) Z ∈ Rn×p almost surely has full rank, i.e. rank Z = min{n, p}.
Then, for the ridgeless linear regression estimator fZ,y(z) = z> Z+y, the following holds:
Ifp ≥ n,
Ifp ≤ n,
ENoise (≥I) σ2EZ tr((Z+)>ΣZ+) (≥II) σ2EZ tr((WW>)-1)
ENoise (≥I) σ2EZ tr((Z+)>ΣZ+) (I=II)σ2EZtr((W>W)-1)
(IV)
≥
(V)
≥
n
σ2-----
p+1-n
σ P
n+1-p
Here, the matrix inverses exist almost surely in the considered cases. Moreover, we have:
•	If (NOI) holds with equality, then (I) holds with equality.
•	If n = p or Σ = λIp for some λ > 0, then (II) holds with equality.
For a discussion on how Σ influences ENoise, we refer to Remark G.1. We can extend Theorem 3 to
random features if it holds for almost all of the random feature maps:
Corollary 4 (Random features). Let θ 〜Pθ be a random variable such that φθ : Rd → Rp is
a random feature map. Consider the random features regression estimator fX,y,θ (x) = zθ>Zθ+y
with zθ := φθ (x) and Zθ := φθ(X). If for PΘ-almost all θ, the assumptions of Theorem 3 are
satisfied for z = zθe and Z = Zθe (with the corresponding matrix Σ = Σθe), then
ENoise
≥	σσ2
n
p+1-n
p
n+1-p
ifp ≥ n,
if p ≤ n.
The main novelty in Theorem 3 is the explicit uniform lower bound (IV) for p ≥ n: The lower
bound (V) for p ≤ n follows by adapting Corollary 1 in Mourtada (2019). Statements similar to
(I), (II) and (III) have also been proven, see e.g. Hastie et al. (2019) and Theorem 1 in Muthuku-
mar et al. (2020). However, as discussed in Section 1, Hastie et al. (2019) make sificiantly stronger
assumptions for computing the expectation. In Appendix L, we explain in more detail that the proba-
bilistic overparameterized lower bound of Muthukumar et al. (2020) is not distribution-independent
and only applies to a smaller class of distributions than our lower bound. For a discussion on how
Theorem 3 applies to kernels with finite-dimensional feature space, we refer to Appendix K.
5 When are the Assumptions Satisfied?
In this section, we want to discuss the assumptions of Theorem 3 and provide different results
helping to verify these assumptions for various input distributions and feature maps. The theory will
be particularly nice for analytic feature maps, which we define now:
Definition 5 (Analytic function). A function f : Rd → R is called (real) analytic if for all z ∈ Rd,
the Taylor series of f around z converges to f in a neighborhood of z . A function f : Rd →
Rp, z 7→ (f1(z), . . . ,fp(z)) is called (real) analytic if f1, . . . ,fp are analytic.	J
Sums, products and compositions of analytic functions are analytic, cf. e.g. Section 2.2 in Krantz &
Parks (2002). We will discuss examples of analytic functions later in this section.
Proposition 6 (Characterization of (COV) and (FRK)). Consider the setting of Theorem 3 and let
FRK(n) be the statement that (FRK) holds for n. Then,
(i) Let n ≥ 1. Then, FRK(n) iffP(z ∈ U) = 0 for all linear subspaces U ⊆ Rp of dimension
min{n, p} - 1.
(ii) Let (MOM) hold. Then, (COV) holds iff P(z ∈ U) < 1 for all linear subspaces U ⊆ Rp of
dimension p - 1.
5
Published as a conference paper at ICLR 2021
Assuming that (MOM) holds such that (COV) is well-defined, consider the following statements:
(a)	FRK(p) holds.
(b)	FRK(n) holds for all n ≥ 1.
(c)	(COV) holds.
×d
(d)	There exists a fixed deterministic matrix X ∈ Rp×d such that det(φ(X)) 6= 0.
We have (a) ⇔ (b) ⇒ (c) ⇒ (d). Furthermore, if x ∈ Rd has a Lebesgue density and φ is analytic,
then (a) - (d) are equivalent.
With this in mind, we can characterize the assumptions now:
•	The assumption (INT) is standard (See e.g. Section 1.6 in Gyorfi et al., 2002) and guarantees
RP(fP) < ∞, such that the excess risk is well-defined.
•	The assumption (NOI) is required to ensure the existence of sufficient label noise. Impor-
tantly, Lemma H.1 shows that (NOI), i.e. Var(y|z) ≥ σ2 almost surely over z, holds if
Var(y|x) ≥ σ2 almost surely over x. All Double Descent papers from Section 1 make the
stronger assumption that the distribution of y - E(y|x) is independent of x or even a fixed
Gaussian.
•	The assumption (MOM) can be reformulated as Ekzk22 = Ekφ(x)k22 = Ek(x, x) < ∞.
For example, if k or equivalently φ are bounded, or if φ is continuous and kxk2 is bounded,
then (MOM) is satisfied. Such assumptions are frequently imposed (see e.g. Chapters 6, 7,
8, 10 in Gyorfi et al., 2002). In this sense, (MOM) is a standard assumption.
•	The assumptions (COV) and FRK(n) are implied by FRK(p) and are even equivalent to
FRK(p) in the underparameterized case p ≤ n. In the following, we will therefore focus
on proving FRK(p) for various φ and PX. In the case p = n, FRK(p) ensures that fX,y
almost surely interpolates the data, or equivalently that the kernel matrix k(X, X) almost
surely has full rank. Importantly, assuming FRK(p) is weaker than assuming a strictly
positive definite kernel, since strictly positive definite kernels require p = ∞. Example D.1
shows that the assumption (FRK) in Theorem 3 cannot be removed.
For the analytic function φ = id with d = p, Proposition 6 yields a simple sufficient criterion: If z
has a Lebesgue density, then (FRK) holds for all n. This assumption is already more realistic than
assuming i.i.d. components. However, Proposition 6 is also very useful for other analytic feature
maps, as we will see in the remainder of this section.
Remark 7. Suppose that φ 6≡ 0 is analytic, x has a Lebesgue density and (INT), (MOM) and (NOI)
are satisfied. If (d) in Proposition 6 does not hold, there exists P < p such that the lower bound
from Theorem 3 holds with P replaced by p: Let U := Span{φ(x) | X ∈ Rd}. Since φ ≡ 0,
P := dim U ≥ 1. Moreover, (d) holds iff dim U = p. Take any isometric isomorphism ψ : U → Rp
and define the feature map φ : Rd → Rp, X → ψ(φ(x)). Then, φ is analytic since ψ is linear, and
∙-v	∙-v	∙-v
φ satisfies (d), hence Theorem 3 can be applied to φ. However, φ and φ lead to the same kernel k
since ψ is isometric, hence to the same estimator fX,y by Eq. (2) and hence to the same ENoise. J
Proposition 8 (Polynomial kernel). Let m, d ≥ 1 and c > 0. For x, X ∈ Rd, define the polynomial
kernel k(x, X) := (x>X + c)m. Then, there exists a feature map φ : Rd → Rp, P := (m^d), such
that:
(a)	k(x, X) = φ(x)>φ(X) forall x, X ∈ Rd, and
(b)	ifX ∈ Rd has a Lebesgue density and we use z = φ(X), then (FRK) is satisfied for all n.
Proposition 8 says that the lower bound from Theorem 3 holds for ridgeless kernel regression with
the polynomial kernel with P := mm+d if X has a Lebesgue density and Ekz k22 = Ek(X, X) =
E(kXk22 + c)m < ∞. The proof of Proposition 8 can be extended to the case c = 0, where one needs
to choose P = m+md-1 . In general, we discuss in Appendix K that Theorem 3 can often be applied
to ridgeless kernel regression, where P needs to be chosen as the minimal feature space dimension
for which k can still be represented.
We can also extend our theory to analytic random feature maps:
Proposition 9 (Random feature maps). Consider feature maps φθ : Rd → Rp with (random)
parameter θ ∈ Rq. Suppose the map (θ, X) 7→ φθ (X) is analytic and that θ and X are independent
6
Published as a conference paper at ICLR 2021
and have Lebesgue densities. If there exist fixed θ ∈ Rq, X ∈ Rp×d with det(φθe (X)) 6= 0, then
almost surely over θ, (FRK) holds for all n for z = φθ (x).
In Appendix C, we demonstrate that Proposition 9 can be used to computationally verify (FRK) for
analytic random feature maps.
Up until now, we have assumed that x has a Lebesgue density. It is desirable to weaken this as-
sumption, such that x can, for example, be concentrated on a submanifold of Rd. It is necessary
for FRK(p) with p ≥ 2 that x is nonatomic, such that the xi are distinct with probability one. In
general, this is not sufficient: For example, if φ = id and x lives on a proper linear subspace of Rd,
FRK(p) is not satisfied. Perhaps surprisingly, we will show next that for random neural network
feature maps, it is indeed sufficient that x is nonatomic.2 Especially, our lower bound in Corollary 4
thus applies to a large class of feedforward neural networks where only the last layer is trained (and
initialized to zero, such that gradient descent converges to the Moore-Penrose pseudoinverse).
Theorem 10 (Random neural networks). Let d, p, L ≥ 1, let σ : R → R be analytic and let the
layer sizes be d0 = d, d1, . . . , dL-1 ≥ 1 and dL = p. Let W(l) ∈ Rdl+1 ×dl for l ∈ {0, . . . , L - 1}
be random variables and consider the two cases where
(a)	σ is nota polynomial with less than p nonzero coefficients, θ := (W(0), . . . , W(L-1)) and
the random feature map φθ : Rd → Rp is recursively defined by
φ(x(0)) := x(L), x(l+1) := σ(W(l)x(l)) .
(b)	σ is not a polynomial of degree < p - 1, θ := (W(0), . . . , W(L-1), b(0), . . . , b(L-1))
with random variables b(l) ∈ Rdl+1 for l ∈ {0, . . . , L - 1}, and the random feature map
φθ : Rd → Rp is recursively defined by
φ(x(0)) := x(L),	x(l+1) := σ(W (l)x(l) + b(l)) .
In both cases, if θ has a Lebesgue density andx is nonatomic, then (FRK) holds for all n and almost
surely over θ.
The assumptions of Theorem 10 on θ are satisfied by the classical initialization methods of He et al.
(2015) and Glorot & Bengio (2010). Possible choices of σ are presented in Table 1. A statement
similar to Theorem 10 has been proven in Lemma 4.4 in Nguyen & Hein (2017). However, their
statement only applies to networks with bias and to a more restricted class of activation functions:
For example, the activation functions RBF, GELU, SiLU/Swish, Mish, sin and cos are not covered
by their assumptions. In Appendix E, we explain that the proofs of many other theorems in the
literature similar to Theorem 10 for single-layer networks are incorrect.
Table 1: Some examples of analytic activation functions that are not polynomials. The CDF of
N(0, 1) is denoted by Φ. Other non-polynomial analytic activation functions are sin, cos or erf.
Activation function	Equation
Sigmoid	Sigmoid(X) = 1/(1 + e-x)
Hyperbolic Tangent	tanh(x) = (ex — e-x )∕(ex + e-x)
Softplus	SoftPlus(X) = log(1 + ex)
-RBF	RBF(X) = exp(—βx2)
GELU (Hendrycks & GimPeL 2016)	GELU(X) = xΦ(x)
SiLU (ElfWing et al., 2018)	SiLU(X) = x Sigmoid(X)
SWish (Ramachandran et al., 2017)	SWish(X) = X Sigmoid(βx)
Mish (Misra, 2019)	Mish(X) = X tanh(softplus(X))
2This appears to be a convenient consequence of randomizing the feature map: For each fixed feature map
φθe, there may be an exceptional set Eθe of nonatomic input distributions PX for which FRK(p) is not satisfied.
However, Theorem 10 shows that for each nonatomic input distribution PX , the set {θ | PX ∈ Eθe} is a
Lebesgue null set. For (deterministic) feature maps where it is not possible to prove FRK(p) for all nonatomic
PX , there is a trick that works in certain cases: If x lives on a submanifold (e.g. a sphere), we might be able
to write x = ψ(v), where v has a Lebesgue density and ψ is analytic (e.g. the stereographic projection for the
sphere). Then, we can apply our theory to the analytic feature map φθ ◦ ψ.
7
Published as a conference paper at ICLR 2021
Under the assumptions of Theorem 10, if kxk2 is bounded, (MOM) holds for all θ . This follows
since any analytic function φ is also continuous, and continuous functions preserve boundedness.
However, the activation functions from Table 1 even satisfy ∣σ(x) | ≤ a|x| + b for some a, b ≥ 0 and
all x ∈ R. In this case, it is not hard to see that (MOM) already holds for all θ if Ekxk22 < ∞.
Theorem 10 does not hold for ReLU, ELU (Clevert et al., 2015), SELU (Klambauer et al., 2017)
or other activation functions with a perfectly linear part. To see this, observe that with nonzero
probability, all weights and inputs are positive. In this case, the feature map acts as a linear map
from Rd to Rp , and if d < p = n, the output matrix φ(X ) cannot be invertible.
In Appendix I, we show that (FRK) is satisfied for random Fourier features if x is nonatomic and
the frequency distribution (i.e. the Fourier transform of the shift-invariant kernel) has a Lebesgue
density.
6 Quality of the Lower Bound
In this section, we discuss how sharp the lower bound from Theorem 3 is. To this end, we assume
that Var(y|z ) = σ2 almost surely over z .
In their Lemma 3, Hastie et al. (2019) consider the case where z has i.i.d. entries with zero mean,
unit variance and finite fourth moment. They then use the Marchenko-Pastur law to show in the limit
p,n →∞,p∕n → γ > 1 that ENoise → σ2 γ-ι. In this limit, our lower bound shows
2n	2
ENoise ≥ σ P + ι 百一σ
1
→ σ2 -ɪɪ
γ-1
p/n + 1/n - 1
hence, in this sense, our overparameterized bound is asymptotically sharp. An analogous argument
shows that the underparameterized bound is also asymptotically sharp. In order to better understand
to which extent our lower bound is non-asymptotically sharp in the over- and underparameterized
regimes, we explicitly compute ENoise = σ2EZ tr((Z+)>ΣZ+) (cf. Theorem 3) for some distribu-
tions PZ :
Theorem 11.	Let PZ = U (Sp-1). Then, PZ satisfies the assumptions (MOM), (COV) and (FRK)
for all n with Σ = PIp. Moreover, for n ≥ P =1 or P ≥ n ≥ 1, we can compute
11 I n		if n ≥ P	= 1,
EZ tr((Z+)>ΣZ+) =	J P	ifP ≥ n	= 1,
	I∞	if 2 ≤ n	≤P ≤ n+ 1,
	I n ∙ P-2	if 2 ≤ n	≤ n + 2 ≤ P.
	1p—1—n p		
The formulas in the next theorem have already been computed by Breiman & Freedman (1983) for
P ≤ n - 2 and by Belkin et al. (2019b) for general P. Our alternative proof circumvents a technical
problem in their proof for P ∈ {n - 1, n, n + 1}, cf. Appendix J.
Theorem 12.	Let PZ = N (0, Ip). Then, PZ satisfies the assumptions (MOM), (COV) and (FRK)
for all n with Σ = Ip. Moreover, for n,P ≥ 1,
{-n-
p-1-n
∞
n-1—p
ifP ≥ n + 2,
ifP ∈ {n - 1, n, n + 1},
ifP ≤ n - 2.
For PZ = N (0, Ip) and the lower bound from Theorem 3, the formulas for the under- and over-
parameterized cases can be obtained from each other by switching the roles of n and P. Numerical
data strongly suggests that this symmetry does not hold exactly for PZ = U (Sp—1).
ForP ≥ n + 2, we can relate our lower bound (Theorem 3), the result for the sphere (Theorem 11)
and the result for the Gaussian distribution (Theorem 12) as follows:
n	n	(P + 1 - n) — 2 ≤ n P 一 2 n
P+1 -n P- 1 -n P+1 -n P-1 -n P P- 1 -n
These values are also shown in Figure 1 together with empirical values of NN feature maps specif-
ically optimized to minimize ENoise. Since Σ affects ENoise only for P > n (cf. Remark G.1), it is
8
Published as a conference paper at ICLR 2021
1.1
(ɪpɔHd日ə) ( d-N2əAQ-əj
1.0
0.9
0.8
0.7
100
101	102
Number of points n
Figure 1: Various estimates and bounds for ENoise relative to ENoise for PZ = U(Sp-1), using
Var(y |z) = 1 and P = 30. The optimized curves correspond to multi-layer NN feature maps whose
parameters were trained to minimize ENoise for n = 15 or n = 60. More experiments and details
on the setup can be found in Appendix C. We do not plot estimates for n ∈ {28, . . . , 32} since they
have high estimation variances.
not surprising that the feature map optimized for n = 60 > 30 = p performs badly in the over-
parameterized regime. The results in Figure 1 support the hypothesis that among all PZ satisfying
(MOM), (COV) and (FRK), PZ = U (Sp-1) minimizes EZ tr((Z+)>ΣZ+). The plausibility of
this hypothesis is further discussed in Remark G.3. We can prove the hypothesis for n = 1 orp = 1
since in this case, the results from Theorem 11 are equal to the lower bound from Theorem 3.
When using a continuous feature map φ : Rd → Rp with d ≤ p - 2, it seems to be unclear at first
whether the low ENoise of PZ = U(Sp-1) can even be achieved. We show in Proposition J.2 that
this is possible using space-filling curve feature maps.
The results in this section and in Appendix C show that while our lower bound presumably does not
fully capture the height of the interpolation peak at p ≈ n, it is quite sharp in the practically relevant
regimes p n and p n irrespective of d.
Acknowledgments
The author would like to thank Ingo Steinwart for proof-reading most of this paper and for pro-
viding helpful comments. Funded by Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016. The author thanks the
International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support.
References
Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74-84. PMLR, 2020.
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in
neural networks. arXiv:1710.03667, 2017.
Herbert Amann and Joachim Escher. Analysis. Springer, 2005.
9
Published as a conference paper at ICLR 2021
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. arXiv:1802.01396, 2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
OfSciences,116(32):15849-15854, 2019a.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features.
arXiv:1903.07571, 2019b.
Rajendra Bhatia. Matrix Analysis, volume 169. Springer Science & Business Media, 2013.
Vladimir I. Bogachev. Measure Theory, Volume 2, volume 1. Springer Science & Business Media,
2007.
Charles Bordenave and Djalil Chafai. Around the circular law. Probability surveys, 9:1-89, 2012.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Leo Breiman and David Freedman. How many variables should be entered in a regression equation?
Journal of the American Statistical Association, 78(381):131-136, 1983.
Siegfried Bos and Manfred Opper. Dynamics of training. In Advances in Neural Information Pro-
cessing Systems, pp. 141-147, 1997.
Weipeng Cao, Xizhao Wang, Zhong Ming, and Jinzhu Gao. A review on neural networks with
random weights. Neurocomputing, 275:278-287, 2018.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve. arXiv:2008.01036, 2020.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv:1511.07289, 2015.
Stephane d’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double
descent: Bias and variance (s) in the lazy regime. arXiv:2003.01054, 2020a.
Stephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv:2006.03509, 2020b.
Rick Durrett. Probability: theory and examples, 5th edition, volume 49. Cambridge university
press, 2019.
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. Neural Networks, 107:3-11, 2018.
Herbert Federer. Geometric Measure Theory. Springer, 1969.
Catherine Forbes, Merran Evans, Nicholas Hastings, and Brian Peacock. Statistical Distributions.
John Wiley & Sons, 2011.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University
Press, Baltimore, USA, 1989.
Ping Guo. A vest of the pseudoinverse learning algorithm. arXiv:1805.07828, 2018.
Ldszl6 Gyofi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of
Nonparametric Regression. Springer Science & Business Media, 2002.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv:1903.08560, 2019.
10
Published as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415, 2016.
Pablo A. Henriquez and Gonzalo A. Ruz. An empirical study of the hidden matrix rank for neu-
ral networks with random weights. In 2017 16th IEEE International Conference on Machine
Learning and Applications (ICMLA), pp. 883-888. IEEE, 2017.
Gao Huang, Guang-Bin Huang, Shiji Song, and Keyou You. Trends in extreme learning machines:
A review. Neural Networks, 61:32-48, 2015.
Guang-Bin Huang. Learning capability and storage capacity of two-hidden-layer feedforward net-
works. IEEE Transactions on Neural Networks, 14(2):274-281, 2003.
Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: theory and
applications. Neurocomputing, 70(1-3):489-501, 2006.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing systems, pp. 971-980, 2017.
Steven G. Krantz and Harold R. Parks. A Primer of Real Analytic Functions. Springer Science &
Business Media, 2002.
Peter Kuchment. An overview of periodic elliptic operators. arXiv:1510.00971, 2015.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gener-
alize. Annals of Statistics, 48(3):1329-1347, 2020.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. arXiv:1908.10292, 2019.
K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate analysis. Probability and mathematical
statistics. Academic Press Inc., 1979.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv:1908.05355, 2019.
Diganta Misra. Mish: A self regularized non-monotonic neural activation function.
arXiv:1908.08681, 2019.
Boris Mityagin. The zero set ofa real analytic function. arXiv:1512.07276, 2015.
Jaouad Mourtada. Exact minimax risk for linear least squares, and the lower tail of sample covari-
ance matrices. arXiv:1912.10754, 2019.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless inter-
polation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory,
2020.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent.
arXiv:1912.07242, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv:1912.02292, 2019.
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. arXiv:2003.01897, 2020.
11
Published as a conference paper at ICLR 2021
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv:1810.08591, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Interna-
tional Conference on Machine Learning, pp. 2603-2612, 2017.
Viet Dang Nguyen. Complex powers of analytic functions and meromorphic renormalization in
QFT. arXiv:1503.00995, 2015.
S. James Press. Applied Multivariate Analysis: Using Bayesian and Frequentist Methods of Infer-
ence. Courier Corporation, 2005.
Robert James Purser, Manuel de Pondeca, and Sei-Young Park. Construction of a Hilbert curve on
the sphere with an isometric parameterization of area. Office Note, 2009.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-
dimensional phenomenon. In Conference on Learning Theory, pp. 2595-2623, 2019.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions.
arXiv:1710.05941, 2017.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In
International Conference on Learning Representations, 2018.
Hans Sagan. Space-filling curves. Springer Science & Business Media, 2012.
Michael A. Sartori and Panos J. Antsaklis. A simple method to derive bounds on the size and to
train multilayer neural networks. IEEE transactions on neural networks, 2(4):467-471, 1991.
Simone Scardapane and Dianhui Wang. Randomness in neural networks: An overview. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(2), 2017.
Wouter F. Schmidt, Martin A. Kraaijveld, and Robert PW Duin. Feed forward neural networks
with random weights. In International Conference on Pattern Recognition, pp. 1-1. IEEE COM-
PUTER SOCIETY PRESS, 1992.
Stefano Spigler, Mario Geiger, StePhane d'Ascoli, Levent Sagun, Giulio Biroli, and MatthieU Wyart.
A jamming transition from under-to over-parametrization affects generalization in deep learning.
Journal of Physics A: Mathematical and Theoretical, 52(47):474001, 2019.
Shin’ichi Tamura and Masahiko Tateishi. Capabilities ofa four-layered feedforward neural network:
four layers versus three. IEEE Transactions on Neural Networks, 8(2):251-255, 1997.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices.
arXiv:1011.3027, 2010.
Dietrich von Rosen. Moments for the inverted Wishart distribution. Scandinavian Journal of Statis-
tics, pp. 97-109, 1988.
Guorong Wang, Yimin Wei, Sanzheng Qiao, Peng Lin, and Yuzhuo Chen. Generalized Inverses:
Theory and Computations, volume 53. Springer, 2018.
Bernard Widrow, Aaron Greenblatt, Youngsik Kim, and Dookun Park. The No-Prop algorithm: A
new learning algorithm for multilayer neural networks. Neural Networks, 37:182-188, 2013.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv:1611.03530, 2016.
12
Published as a conference paper at ICLR 2021
A	Overview
The appendix is structured as follows: In Appendix B, we provide an overview over some ma-
trix identities used throughout the paper. We provide additional numerical experiments for various
(random) feature maps in Appendix C. The counterexample given in Appendix D shows that the
assumption (FRK) cannot be omitted from Theorem 3. In Appendix E, we provide an overview
over full-rank theorems for random neural networks in the literature and explain why their proofs
are incorrect. We then prove our main results in Appendix F before discussing consequences in
Appendix G. In Appendix H, we give proofs for the theorems and propositions from Section 5.
As an addition, we prove (FRK) for random Fourier features in Appendix I. Finally, proofs for the
statements from Section 6 are given in Appendix J.
Whenever we prove theorems or propositions from the main paper (like Theorem 3) in the Appendix,
we repeat their statement before the proof for convenience. In contrast, new theorems or propositions
are numbered according to the section they are stated in, e.g. Proposition I.1.
B Matrix Algebra
In the following, we will present some facts about matrices that are relevant to this paper. For
a general reference, we refer to textbooks on the subject (e.g. Golub & Van Loan, 1989; Bhatia,
2013).
Let n, p ≥ 1 and let k := min{n, p}. The singular value decomposition (SVD) of a matrix A ∈
Rn×p is a decomposition A = UDV > into orthogonal matrices U ∈ Rn×k, V ∈ Rp×k with
U > U = V > V = I k and a diagonal matrix D ∈ Rk×k with non-negative diagonal elements
s1(A) ≥ . . . ≥ sk (A) called singular values.
For a given symmetric square matrix A ∈ Rn×n with eigenvalues λ1(A) ≥ . . . ≥ λn(A), the trace
satisfies
nn
tr(A)=XAii=Xλi.
i=1	i=1
The trace is linear and invariant under cyclical permutations. We use this multiple times in arguments
of the following type: If v ∈ Rp and A ∈ Rp×p are stochastically independent (e.g. because A is
constant), we can write
Evv>Av = Ev tr(v>Av) = Ev tr(Avv>) = tr(AEvvv>) .
Moreover, if A	0, then for all i ∈ [n], λi (A) > 0 and we have
λn+1-i(A-1)
1
λi(A).
For a positive definite matrix Σ ∈ Rp×p, the SVD and the eigendecomposition coincide as Σ =
U diag(λ1(Σ), . . . , λp(Σ))U> and we can define the inverse square root as
Σ-1/2 = Udiag(λι(Σ)-1/2,..., λp(Σ)-")U> A 0 ,
which is the unique s.p.d. matrix satisfying (Σ-1/2)2 = Σ-1.
By the Courant-Fischer-Weyl theorem, two symmetric matrices A, B ∈ Rn×n with A B satisfy
λi(A) = max min v>Av ≤ max min v>Bv = λi(B) .
V⊆Rd subspace v∈V	V⊆Rd subspace v∈V
dim V=i kvk2=1	dim V=i kvk2=1
Let A ∈ Rn×p . The Moore-Penrose pseudoinverse A+ of A satisfies the following relations (see
e.g. Section 1.1.1 in Wang et al., 2018):
•	Suppose A has the SVD A = UDV >, where D = diag(s1, . . . , sk), k := min{n, p}.
Using the convention 1/0 := 0, we can write A+ = V D+U>, where D+ :=
diag(1/s1, . . . , 1/sk).
13
Published as a conference paper at ICLR 2021
•	A+ = (A>A)+A> = A>(AA>)+.
•	If A is invertible, then A+ = A-1.
•	A+(A+)> = (A>A)+.
We will also use a basic fact on the Schur complement that, for example, is outlined in Appendix
A.5.5 in Boyd & Vandenberghe (2004): If
A11
A21
0 Y A
AA12	∈ R(m1+m2)×(m1+m2) ,
then A22	0 and	A11	-	A12A2-21A21	0 and the top-left	m1	×	m1 block of	A-1	is given by
(A11 - A12A2-21A21)-1.
C Experiments
In the following, We experimentally investigate ENoise for different (random) feature maps. We will
first give an overview over the plots and then explain the details of how they were generated and
some implications. More details can be found in the provided code. All empirical curves show one
estimated standard deviation of the mean estimator as a shaded area around the estimated mean, but
this standard deviation is sometimes too small to be visible. We assume Var(y∣z) = 1 almost surely
over Z such that (I) in Theorem 3 holds with equality with σ2 = 1.
(Joxlə pəonpu'əsɪou) ωsδu
----sigmoid NN
----tanhNN
----GELUNN
----SoftplUs NN
——ReLUNN
——SELUNN
——ELUNN
——U(SpT)
----Lower bound
Figure C.1: Estimated ENoise for random neural network feature maps (cf. Theorem 10) with differ-
ent activation functions and d0 = d = 10, d1 = d2 = 256, d3 = p = 30. We include the results
from PZ = U (Sd-1 ) as comparison, cf. Section 6.
Figure C.1 shows ENoise for random three-layer neural network feature maps with p = 30, differ-
ent activation functions and varying n. Note that all neural networks produce higher ENoise than
PZ = U (Sp-1 ). The effect of non-isotropic covariance matrices in the overparameterized regime
can be clearly seen when comparing Figure C.1 to Figure C.2, where features have been whitened
separately for each set of random parameters θ, cf. Remark G.1. Figure C.3 then shows ENoise for
n = 30 and varying p.
Note that double descent is usually plotted as a function of the “model complexity” p as in Belkin
et al. (2019a), but varying p is only possible when we have a (random) feature map φ(θp) : Rd → Rp
for each value of p. For the following NTK and polynomial kernels, there is no canonical way to
14
Published as a conference paper at ICLR 2021
04
3
10
100
101
102
Number of points n
---sigmoid NN
---tanhNN
---GELUNN
---softplus NN
——ReLUNN
——SELUNN
——ELUNN
——U (Sp-1)
---Lower bound




Figure C.2: As in Figure C.1 but with whitened features, i.e. using E tr((WW>)-1) in the overpa-
rameterized case n ≤ p, cf. Theorem 3 and Remark G.1.

104
(Joxləpəonpu'əsɪou) ə-ou
2
10
1 O
O O
lɪ lɪ
100	101	102
Number of parameters P
Sigmoid NN
---tanhNN
---GELUNN
---softplus NN
——ReLUNN
——SELUNN
ELUNN




Figure C.3: As in Figure C.1 but with n = 30 fixed and varying d3 = p.
define such a sequence of feature maps. For this reason, we will plot their results only with varying
n. Double descent as a function of the number of samples n has for example been pointed out by
Nakkiran et al. (2019) and Nakkiran (2019).
Figure C.4 shows ENoise for various random finite-width Neural Tangent Kernels (NTKs), cf. Jacot
et al. (2018). These results mostly exhibit larger ENoise than the random NN feature maps from
Figure C.1, perhaps because of correlations parameter gradients in different layers. However, this
15
Published as a conference paper at ICLR 2021
comparison is not really fair since the NTK feature map uses a much smaller underlying neural
network and the input dimension d is smaller.
(JOxlQ psnpu'QS'su) əsɪou"
Number of points n
---SigmoidNTK
---tanh NTK
---GELUNTK
---SoftplUsNTK
——ReLU NTK
---SELU NTK
——ELU NTK
---U(SpT)
---Lower bound
Figure C.4: Estimated ENoise for Neural Tangent Kernel (NTK) feature maps given by various
random neural networks (cf. Theorem 10) with d0 = d = 4, d1 = 6, d3 = 1, resulting in
P = 4 ∙ 6 + 6 ∙ 1 = 30. We include the empirical results from PZ = U(Sd-I) as comparison,
cf. Section 6.
Figure C.5 and Figure C.6 show ENoise for two variants of random Fourier features for two different
scalings of the random parameters. Figure C.6 shows that for random Gaussian parameters with
large variance (corresponding to an approximated narrow Gaussian kernel), the values of ENoise
for random Fourier features are very close to the values for PZ = U (Sp-1). We decided to plot
these values relative to each other as in Figure 1, since the curves would overlap in a normal plot
like Figure C.5. Note that the the version of random Fourier features with sin and cos features
automatically yields constant kz k2 like for PZ = U(Sp-1).
Finally, Figure C.7 shows that linear regression with the polynomial kernel is quite sensitive to
label noise. We use p = 35 for the polynomial kernel since there are no particularly interesting
polynomial kernels with p = 30.
Neural Network feature maps For Figures C.1, C.2 and C.3, we use random neural network
feature maps without biases as in Theorem 10 with d0 = d = 10, d1 = d2 = 256 and d3 = p.
As the input distribution PX, we use N (0, Id). We initialize the NN weights independently as
Wi(I)〜N(0,1/W, where
V := d0	ifl=0,
l ' (dι Varu〜N(0,i)(σ(u)) if l ≥ 1.
Here, Varu〜N(0,1)(σ(u)) is approximated once by using with 104 samples for u. This initialization
is similar (and for the ReLU activation essentially equivalent) to the initialization method by He
et al. (2015). The initialization variances are chosen such that for fixed input x with kxk2 ≈ 1, the
pre-activations in each layer are approximately N(0, 1)-distributed.
NTK feature maps For Figure C.4, we use a NTK parameterization similar to the original one
proposed by Jacot et al. (2018), but again with activation-dependent scaling. Our neural network is
16
Published as a conference paper at ICLR 2021
(JoJJ°p°cmpu'°s-su)① SIOU2
Number of points n
---- RFF (Cos with bias)
---- RFF (Sin and Cos)
——U(SpT)
----Lower bound
Figure C.5: Estimated ENoise for the two versions of random Fourier features described in Ap-
pendix I. We use d = 10, PX = N(0, Id), p = 30 and the weight vector distribution Pk =
N(0, PId) (cf. Appendix I).
1.1
(ɪpɔHd日ə) (⅛) N2əAQ-əj
1.0
0.9
0.8
---- N(0, IP) (analytic)
---- U (Sp-I) (empirical)
— U (Sp-1), n ≤ P (analytic)
----RFF (Cos with bias)
----RFF (Sin and Cos)
----Lower bound
0.7
100	101	102
Number of points n
Figure C.6: Estimated ENoise for the two versions of random Fourier features described in Ap-
pendix I relative to PZ = U(SpT). We USe d = 10, PX = N(0, Id), P = 30 and the weight vector
distribution Pk = N(0, Id) (cf. Appendix I).
given by3
φθ : Rd → R1, X → √V= W ⑴ σ (√Vo W ⑼
3For random NN feature maps as in Theorem 10, one can interpret the linear regression as being an extra
layer on top of the neural network, and therefore the last layer of the feature map should contain an activation
function. For NTK feature maps, one can instead interpret the linear regression as performing a “linearized”
training of the whole NN, and the whole NN usually does not contain an activation function in the last layer.
17
Published as a conference paper at ICLR 2021
04
3
10
Polynomial kernel
Lower bound
(jojjə pəOnPU'əsɪou)①∙-OU2
2
10
O
1±
00
100
101
102

Number of points n
Figure C.7: Estimated ENoise for the polynomial kernel (cf. Proposition 8) with d = 3, m = 4,
C =1, PX = N(0, Id), resulting in P = (m+d) = (7) = 35.
with do = d = 4,dι = 6,d2 = 1 and W(j) 〜N(0,1) i.i.d. Our input distribution is again
PX = N(0, Id). The NTK feature map is then defined as
∂
φθ : R → RJ X →	φθ(X),
∂θ
where P = 6 ∙ 4 + 1 ∙ 6 = 30 is the number of parameters in θ. Note that moving the variances 吊
outside of the W(l) does not affect the forward pass but only the backward pass (i.e. the derivatives).
While we have not theoretically established the properties (FRK) and (COV) for such feature maps,
we can do this experimentally for analytic activation functions like sigmoid, tanh, softplus and
GELU: Since the random NTK feature map is a derivative of the analytic random NN feature map,
it is also analytic. By Proposition 9, if (FRK) does not hold, then for every fixed θ, the range of
φθe must be contained in a proper linear subspace of Rp, and therefore Z = φθ(X) never has full
rank for n ≥ P. In this case, the singular value sp(Z) would be zero, and even accounting for
s (Z)
numerical errors, the inverse condition number PL〈 should at most be of the order of 64-bit float
s1(Z)
machine precision, i.e. around 10-16. However, among 104 samples of Z for n := 90 ≥ 30 = P,
the maximum observed “inverse condition number” was greater than 10-3 for all of the activation
functions sigmoid, tanh, softplus and GELU.4 Hence, by Proposition 6 and Proposition 9, we can
confidently conclude that (COV) and (FRK) hold for all n almost surely over θ for this network size
and these activation functions.
Estimation of ENoise In order to estimate ENoise , we proceed as follows: Recall from Section 3
that ridgeless regression is the limit of ridge regression for λ & 0. We use a small regularization of
λ = 10-12 in order to improve numerical stability. Also for numerical stability, we use a singular
value decomposition (SVD) Z = U diag(s1, . . . , sk)V > with k := min{n, P} as in Appendix B.
The regularized approximation of Z + is then
Z+ ≈ (Z>Z + λIP)TZT = Vdiag (,…,U U> .	(3)
v2 + λ	Sk + λJ
4We use n > p since this usually improves the “inverse condition number” of Z.
18
Published as a conference paper at ICLR 2021
We can then estimate
tr((Z +)>ΣZ+) = tr(Z +(Z +)>∑) ≈ tr (V diag ((SI ?》尸,...,(Sk +k 入尸)V > ∑) .	(4)
We can then obtain m = 104 (non-ReLU NNs, polynomial kernel, high-variance random Fourier
features) or m = 105 (all other empirical results) sampled estimates for tr((Z+)>ΣZ+) in order to
obtain a Monte-Carlo estimate ofEtr((Z+)>ΣZ+) by repeating the following procedure m times:
(1)	Sample a random parameter θ.
(2)	Sample random matrices X ∈ Rn×d and X ∈ Rl×d, l = 104, with i.i.d. PX-distributed
rows.
∙-v	∙-v
(3)	Compute Z := φθ(X) and Z := φθ(X).
>
(4)	Compute the estimate Σ := j Z Z.
(5)	Compute a regularized estimate of tr((Z+)>ΣZ+) using the SVD of Z and Eq. (4).
For performance reasons, we make the following modification of step (2) and (5): Since we perform
the computation for all n ∈ [256], we sample X ∈ R256×d and then, for all n ∈ [256], perform
the computation for n using the first n rows of Z. Hence, the estimates for different n are not
independent. In Figure C.3, we perform an analogous optimization forp by taking the first p ∈ [256]
of the d3 = 256 output features in Z.
Curious ReLU results The curves for the ReLU NNs and ReLU NTKs in the underparameterized
regime p ≤ n may seem odd. The locally almost constant “plateaus” are presumably an artefact of
the non-independent estimates for different n as explained in the last paragraph. As discussed in
Section 5, networks with ReLU, ELU or SELU activations do not satisfy (FRK). It seems plausi-
ble that, since both “halves” of the ReLU function are linear, ReLU networks have a significantly
higher chance than SELU or ELU networks to be initialized with “bad” parameters that are likely
to generate “degenerate” feature matrices Z that do not have full rank at n = p and only become
full rank for some n > p. When inspecting the data underlying the plots, the estimate of ENoise for
ReLU networks in the underparameterized regime seems to be dominated by few outliers. It seems
that the distribution of tr((Z+)>ΣZ+) for ReLU networks is often so heavy-tailed that computing
more Monte Carlo samples does not significantly reduce the estimation uncertainty.
Whitening For computing E((WW>)-1) = E((ZΣ-1Z>)-1) in the overparameterized case
in Figure C.2, we regularize both matrix inversions on the right-hand side as above: For a symmetric
matrix A ∈ Rm×m, we use a symmetric eigendecomposition A = U diag(S1, . . . , Sm)U> and
approximate
A-1 ≈ U diag
S1
4r U >
S2m+ λ
sj +1'…
Optimization For our optimized feature maps in Figure 1 with p = 30, we use a neural network
feature map with d0 = d = p = 30, d1 = d2 = 256, d3 = p = 30 and tanh activation function. We
use NTK parameterization and zero-initialized biases, leading to
Φθ (x) = σ(b ⑵ + V-1/2W ⑵ σ(b(1) + VT/2 W (1)σ(b(0) + V-1/2W (0)x)))
with independent initialization Wj 〜N(0,1), b(l) = 0. As the input distribution, We use PX =
N(0, Id). For given θ, let Σθ := Exφθ(x)φθ(x)>, i.e. we define the second moment matrix Σ as
depending on θ. We then optimize the loss function
L(θ) := EX tr((φθ(X)+)>Σθφθ(X)+)
using AMSGrad (Reddi et al., 2018) with a learning rate that linearly decays from 10-3 to 0 over
1000 iterations. In order to approximate L(θ) in each iteration, we approximate Σθ using 1000
Monte Carlo points and we draw 1024 different realizations of X (this can be considered as using
batch size 1024). We also use a regularized version as in Eq. (3), but we omit the SVD for reasons
of differentiability.
19
Published as a conference paper at ICLR 2021
D A Counterexample
Example D.1. Let p ≥ 2. Consider the uniform distribution PZ on an orthonormal basis
{eι,..., ep} ⊆ Rp. Then, Σ = P PP=ι eɪe> = PIP and hence (COV) is satisfied. However,
from Proposition 6 it is easy to see that for any n ≥ 2, (FRK) is not satisfied. Indeed, if the vector
ei occurs mi times among the samples z1, . . . , zn, then Z>Z = diag(m1, . . . , mp). Assuming
Var(y∣z) = σ2 := 1 for all z, we then obtain from Theorem 3 with the convention P := 0:
ENoise = EZ tr((Z +)>∑Z+) = 1EZ tr(Z +(Z +)>) = 1EZ tr((Z>Z) + )
pp
1p 1
1 X E ɪ
p mi
pi=P i
E ɪ
mP
where mp follows a binomial distribution with parameters n and P. Using Emp^ ≤ E1 = 1, itis
easy to see that the lower bound is violated for some values of n. For example, Figure D.1 shows
that for p = 30, the lower bound is violated in a large region, especially in the overparameterized
regime.5 The underlying reason is that the function x → P is convex on (0, ∞), but not on [0, ∞).
If Z has singular values si, the pseudo-inverse Z+ has singular values ɪ. If Si = 0 is possible, we
si
cannot use a convexity-based argument anymore.	J
(joɪləpəOnPU'əsɑU)①∙-OU2
O
1±
00
100
101
102
Number of points n
Figure D.1: The counterexample given in Example D.1 for p = 30 often has lower ENoise than the
lower bound from Theorem 3, but violates its assumption (FRK). For the counterexample, ENoise
was approximated as explained in Example D.1 using 106 Monte Carlo samples for each n. We
assume Var(y|z) = 1 almost surely over z.
Remark D.2 (Histogram regression). The distribution PZ in Example D.1 may seem contrived at
first, but such a distribution can arise in histogram regression (cf e.g. Chapter 4 in Gyorfi et al., 2002).
For example, suppose that PX is supported on a domain D ⊆ Rd and this domain is partitioned into
disjoint sets AP, . . . , Ap . Then, performing histogram regression on this partition is equivalent to
performing ridgeless linear regression with the feature map φ : Rd → Rp with φ(x) := ei if
x ∈ Ai. If all partitions are equally likely, i.e. PX(Ai) = 1/p for all i ∈ {1, . . . ,p}, then PZ is the
uniform distribution on {eP, . . . , ep} as in Example D.1.	J
E	Full-rank Results for Random Weight Neural Networks
In the literature on neural networks with random weights (Schmidt et al., 1992) and the later virtually
identical Extreme Learning Machine (ELM) (Huang et al., 2006), properties similar to (FRK) have
5For n = 1, (FRK) holds and it is easy to see that the lower bound holds exactly in this case.
20
Published as a conference paper at ICLR 2021
been investigated for random neural network feature maps. In the following, we review the relevant
approaches known to us and explain why most of them are flawed.
First, Sartori & Antsaklis (1991) state a result where the assumptions are not clearly specified, but
which could look as follows:
Claim 1 (Sartori & Antsaklis (1991), informally discussed after Lemma 1). For n = p, consider a
single-layer random neural network with weights W(0) ∈ R(n-1)×d and signum activation σ that
appends a 1 to its output:
φθ : Rn → Rp,x 7→ (σ(W(0)x), 1) .
If x1, . . . , xn ∈ Rd are distinct, then for almost all W (0), φθ(X) is invertible.
This claim is evidently false for n ≥ 2. For example, the following argument works for n ≥ 3: For
τ ∈ {-1, 0, 1}n, let Uτ := {w ∈ Rn | σ(w>xj) = τj for all j}. Then, Sτ ∈{-1,0,1}n Uτ = Rn
and since {-1,0,1}n is finite, there exists T* such that UT* is not a LebesgUe null set. Hence the
set W := {W⑼ ∈ R(n-1)×n | at least two rows of W⑼ are in UT*} is not a Lebesgue null set.
But for all W(0) ∈ W, the matrix φθ(X) = (σ(X(W(0))>) | 1n×1) has two columns equal to τ*
and is therefore not invertible, contradicting Claim 1.
Second, Tamura & Tateishi (1997) state a result where the assumptions are not clearly formulated,
but which could look as follows:
Claim 2 (Tamura & Tateishi (1997)). For n = p, consider a single-layer random neural network
with weights W(0) ∈ R(n-1)×d, biases b(0) ∈ Rn-1 and sigmoid activation σ that appends a 1 to
its output:
φθ : Rn →Rp,x 7→ (σ(W(0)x + b(0)), 1) .
If x1, . . . , xn ∈ Rd are distinct, then for almost all W(0) and all a < b, there exists b(0) ∈ [a, b]n-1
such that φθ (X) is invertible.
Tamura & Tateishi (1997) attempt to prove this claim via showing that the curve ci : [a, b] →
Rn, bi 7→ σ((wi(0))>xj + bi) j∈[n] is not contained in any (n - 1)-dimensional subspace of
Rn, which would allow them to pick a suitable bias bi for each curve ci such that the ci (bi) and
(1, . . . , 1)> are linearly independent. Although this is part is formulated confusingly, it should work
out. The major problem is that under the counterassumption that ci lies in an (n - 1)-dimensional
subspace, they construct an infinite (overdetermined) system of equations involving derivatives of
the sigmoid function which they claim has no solution. However, in general, overdetermined systems
can have solutions and they do not prove why this would not be the case in their situation. Indeed,
the only properties of the sigmoid function they use is that it is C ∞ and not a polynomial, and it
is not hard to see that the exponential function has these properties as well and leads to a solvable
overdetermined system since its derivatives are all identical.
This leads us to the next claim:
Claim 3 (Huang (2003), Theorem 2.1). Consider the setting of Claim 2 but with W(0) ∈ Rn×d
instead of appending a 1 to all feature vectors. Ifx1, . . . , xn ∈ Rd are distinct and θ has a Lebesgue
density, then φθ (X) is invertible almost surely over θ.
Huang (2003) uses the “proof” by Tamura & Tateishi (1997) to show that from any nontrivial inter-
vals one can choose W(0), b(0) such that φθ(X) is invertible, setting all rows of W(0) to be equal.
He then concludes without further reasoning that φθ (X) must then be invertible almost surely over
θ. This major unexplained step cannot be fixed by a continuity-based argument since all bi(0) were
chosen from the same interval and similarly, all rows of W(0) were chosen from the same interval.
Since the sigmoid function is analytic, it would however be possible to prove this step using the
multivariate identity theorem for analytic functions, Theorem H.3, in a fashion similar to the proof
of (d) ⇒ (a) in Proposition 6. The approach pursued in Huang (2003) thus introduces an additional
problem on top of the problem of Tamura & Tateishi (1997) although this additional problem is in
principle fixable.
A similar strategy is reused in the next claim issued in a particularly popular paper:
21
Published as a conference paper at ICLR 2021
Claim 4 (Huang et al. (2006), Theorem 2.1). Claim 3 holds for all C∞ activation functions σ.
Not only does the “proof” in Huang et al. (2006) inherit the problems from the previous “proofs”,
but now the result is obviously false as well: For σ ≡ 0, the matrix φθ (X) can never be invertible,
and for σ = id, itis also easy to find counterexamples. Moreover, itis not sufficient to exclude (low-
order) polynomials σ: For example, the well-known construction (e.g. Remark 3.4 (d) in Chapter
V.3 in Amann & Escher, 2005)
σ(x) := e0-1/x
,x≤0
,x>0
yields a C∞ function σ that is zero on (-∞, 0] but not a polynomial. For this function, it is not
hard to see that φθ (X) would be singular with nonzero probability since there is a chance that
W(0)
x + b(0) contains only negative components.
Despite these evident problems and the paper’s popularity, Claim 4 is restated years later as Theorem
1 in a survey paper by the same author (Huang et al., 2015). In his Theorem 1, Guo (2018) attempts
to disprove Claim 4. However, this “disproof” is also invalid: For certain saturating activation
functions like tanh, Guo (2018) lets θ → ∞ in a certain fashion, which causes φθ(X) to converge
to a singular matrix. The problem with this approach is that just because the limiting matrix is
singular, the matrices for finite θ do not need to be singular. However, this limiting behavior might
at least explain the empirical results of Henriquez & Ruz (2017), which find in a certain setting with
sigmoid activation that numerically, φθ (X) is usually not a full-rank matrix. In contrast, Widrow
et al. (2013) numerically reach the opposite conclusion. This is not a contradiction, considering that
very small eigenvalues of φθ(X) may be numerically rounded to zero, and the probability of having
such small eigenvalues depends on the chosen distributions of x and θ.
The only previously published correct result known to us is the following one:
Lemma E.5 (Lemma 4.4 in Nguyen & Hein (2017)). Let σ : R → R be analytic, strictly increasing
and let one of the following two conditions be satsified:
(a)	σ is bounded, or
(b)	there exist positive constants ρ1,ρ2 ,ρ3,ρ4 such that ∣σ(t)∣ ≤ ρ1eρ2t for t > 0 and ∣σ(t)∣ ≤
ρ3t + ρ4 for t ≥ 0.
Let φθ be a random NN feature map with biases as in Theorem 10 (b) and let x1, . . . , xn ∈ Rd be
distinct. Ifp ≥ n - 1, then the n × (p + 1) feature matrix
φ Φθ (xι)> 1∖
φθ(x. n)> 1.
has rank n for (Lebesgue-) almost all θ.
In Theorem H.9, we generalize Lemma E.5 (without the appended ones in the feature matrix) to
a substantially larger class of analytic activation functions.6 As argued above, it is not possible to
replace the assumption that σ is analytic with σ ∈ C∞ and as shown in Remark H.17, our exclusion
of certain polynomials for σ is in general necessary.
F Proofs for Section 4
In this section, we prove our main theorem and corollary from Section 4. Recall from Definition 2
that ifEkzk22 < ∞, we can define the second moment matrix
Σ := Ez〜pz [zz>] ∈ Rp×p ,
and if Σ is invertible, We can also define the “whitened" data matrix W := ZΣ-1/2, whose rows
Wi = Σ-1/2 satisfy Ewiw> = Ip.
6It is in principle possible to extend our results to the case with appended ones in the feature matrix by
choosing the activation function for one of the output neurons to be σ ≡ 1. As discussed in Remark H.16, our
arguments have no problem handling different activation functions. In this case, we would only need to adapt
the corresponding Taylor series coefficients in Lemma H.8.
22
Published as a conference paper at ICLR 2021
Theorem 3 (Main result). Let n,p ≥ 1. Assume that P and φ satisfy:
(INT) Ey2 < ∞ and hence RP (fp) < ∞,
(NOI) Var(y|z) ≥ σ2 almost surely over z,
(MOM) Ekzk22 < ∞, i.e. Σ exists and is finite,
(COV) Σ is invertible,
(FRK) Z ∈ Rn×p almost surely has full rank, i.e. rank Z = min{n, p}.
Then, for the ridgeless linear regression estimator fZ,y(z) = z> Z+y, the following holds:
Ifp ≥ n,
Ifp ≤ n,
(I)	(II)	(IV)	n
ENoise ≥ σ2Ez tr((Z +)>ΣZ+) ≥ σ2Ez tr((WW>)-1) ≥ σ2 ——
p+1-n
ENoise ≥ σ2Ez tr((Z +)>ΣZ+) (=I) σ2Ez tr((W>W)-1) (≥) σ2	—
n+1-p
Here, the matrix inverses exist almost surely in the considered cases. Moreover, we have:
•	If (NOI) holds with equality, then (I) holds with equality.
•	If n =p or Σ = λIp for some λ > 0, then (II) holds with equality.
Proof. By (FRK), we only consider the case where Z has full rank. For further details on some
matrix computations, we refer to Appendix B.
Step 1: Preparation. Since the pairs (z1, y1), . . . , (zn, yn) are independent, we have the condi-
tional covariance matrix
/Var(y1lz1)	ʌ (I)
Cοv(y∣Z) =	...	I 占 σ2In
Var(yn|zn)
with equality in (I) if (NOI) holds with equality. Therefore,
ENoise =f EZ,y,θ,z (fZ,y,θ (Z) 一 EyIZ fZ,y,θ (Z))2
=Ez,y,z (z>Z +y — EyIZZ>Z +y)2
= EZ,zEy|ZZ>Z+(y - Ey|Zy)(y - Ey|Zy)>(Z+)>Z
= EZ,zZ>Z+ Cov(y|Z)(Z+)>Z
(≥I) σ2EZ,zZ> Z+ (Z+ )>Z
= σ2EZ,z tr((Z+)>ZZ>Z+)
= σ2EZ tr((Z+)>Σ(Z+)>) .
Step 2.1: Reformulation, underparameterized case. In the case p ≤ n, we have Z+
(Z>Z)-1Z> thanks to (FRK) and thus
tr((Z +)>ΣZ+) = tr(£1/2Z +(Z +)>Σ1/2) = tr(£1/2(Z>Z)-1Σ1/2) = tr((W>W)-1).
Step 2.2: Reformulation, overparameterized case. In the case p ≥ n, we have Z+ =
Z>(ZZ>)-1 thanks to (FRK). For Σ = λIp, we can showtr((Z+)>ΣZ+) = tr((WW>)-1)
similar to Step 2.1. For general Σ, we can obtain a lower bound by “removing a projection”: First,
let
S := (Z+)>ΣZ+ = (ZZ>)-1ZΣZ>(ZZ>)-1 ,
A := £1/2Z> .
Now, since Z and Σ have full rank, we can compute
S-1 = ZZ>(ZΣZ>)-1ZZ> = WA(A>A)-1A>W> .
Since A(A>A)-1A> is the orthogonal projection onto the column space of A, we have S-1
WW> and hence λi(S-1) ≤ λi(WW>) by the Courant-Fischer-Weyl theorem. This yields
tr((Z+)>ΣZ+)
nn
tr(S) = Xi=1 λn+1-i(S) = Xi=1
1
λi(S-1)
23
Published as a conference paper at ICLR 2021
nn
≥ X λi(WW>)=>一((WWT)T) = tr((WWT)T).
For n = p, A(ATA)-1AT projects onto a p-dimensional space and is therefore the identity matrix,
yielding equality.
Step 3.0: Random matrix bound, p = 1 or n = 1. If n ≥ p = 1, wi = wi is a scalar and we have
Etr((WtW)-1) = E —T— = EB~亍 ≥ 而B~亍
WtW	Pn=ι w2 ^ EPn=ι Wl
= P
n+-p
TT
=
Pi=I tr(Ewiw>)	n
by Jensen’s inequality. Similarly, for p ≥ n = , we obtain

Etr((WWT)-1) = E
w1T w1
T T	Tn
=	==
Ew1Tw1	tr(Ew1w1T)	p	p + T - n
Step 3.1: Random matrix bound, overparameterized case. We first consider the overparameter-
ized case p ≥ n ≥ 2 and block-decompose
W
∈ R(1+(n-1))×p
WWT
w1TW 2T
W2 W2T
⇒
Since Z has full rank, W has full rank. Because of n ≤ p, it follows that WWT	0. Therefore,
((WWT)T)II = (w>wi - WTWT(W2WT)TW2W1)T = (WT(IP - P2)wi)-1 ,
where
P2 := W2T (W2W2T)-1 W2 ∈ Rp×p
is the orthogonal projection onto the column space of W2T. Thus, P2 has the eigenvalues T with
multiplicity n - T and 0 with multiplicity p - (n - T), which yields tr(P2) = n - T. Since the zi
are stochastically independent, W1 and W2 are also stochastically independent and we obtain
EW1T(Ip - P 2)W1 = E tr((I p - P 2)(Ew1W1W1T)) = Etr(Ip - P2) = p +T - n .
Using Jensen’s inequality with the convex function (0, ∞) → (0, ∞), x 7→ T/x, we thus find that
E ((WWT)T)	= E (WT(Ip - P2)w1)-1 ≥ 而 T」P、	= t-.
11	EW1T(I p - P2)W1	p + T - n
Since tr((WWT)-1) = Pin=1((WWT)-1)ii and all diagonal entries can be treated in the same
fashion (e.g. via permutation of the Wi ), it follows that
E tr((WW T)T) ≥ p+n-n.
Step 3.2: Random matrix bound, underparameterized case. In the case n ≥ p ≥ T, we follow the
proof of Corollary 1 in Mourtada (2019), which can be applied in our setting as follows: Introduce
a new random variable Wn+1 such that W1 , . . . , Wn+1 are i.i.d. Then,
Etr((WTW)-1) = EW tr((WTW)-1Ewn+1Wn+1WnT+1) = EWnT+1(W TW)-1Wn+1 .
Now, Lemma 1 in Mourtada (2019) uses the Sherman-Morrison formula to show that
WnT+1(W TW)-1Wn+1
ʌ
'n+1
ʌ
1	- 'n+1
with the so-called leverage score
'n+1 ：= W>+l(WTW + Wn+ιW>+ι)-1Wn+ι ∈ [0, 1).
24
Published as a conference paper at ICLR 2021
Since WτW + wn+1wnτ+1 = Pin=+11 wiwiτ and since w1, . . . , wn+1 are i.i.d., we obtain
E%n+1 = Etr ( (X WiwT!	Wn+ιw>+)
1	( ∕n+1	ʌ -1 n+1	ʌ
=n+1E tr I (X WiwTJ X wiwτ J
=-ɪ-E tr(Ip) = —p-.
n+1	n+1
Finally, the function [0,1) → (0, ∞),χ → ɪ-X = ɪ-X - 1 is convex, and Jensen,s inequality yields
ʌ
E tr((W TW )-1) = E 'n+1	≥
1 - 'n+1
ʌ
E'n+1
ʌ	-t
1 - E'n+1	n + 1 - P
p
□
Note that it is not possible to analyze Step 3.2 like Step 3.1 since the corresponding matrix blocks
are not stochastically independent.
Corollary 4 (Random features). Let θ 〜Pθ be a random variable such that φθ : Rd → Rp is
a random feature map. Consider the random features regression estimator fX,y,θ (x) = zθτ Zθ+y
with zθ := φθ (x) and Zθ := φθ(X). If for PΘ-almost all θ, the assumptions of Theorem 3 are
satisfied for z = zθe and Z = Zθe (with the corresponding matrix Σ = Σθe), then
E ≥ (σ2 + ifp ≥ n，
Noise ≥ [σ2 n+p-p ifP ≤ n.
Proof. First, let p ≤ n. Since θ is independent from X , x, y,
ENoise = EX,y,θ,x (z>Z +y - EyIXzθ Z +y)
2	Theorem 3
=Eθ [Eχ,y,x (z>Z +y - EyXz>Z +y) J ≥	Eθσ2
p
σ
The case p ≥ n can be treated analogously.
n+1-p n+1-p.
□
G Discussion of the Main Theorem
Remark G.1 (Dependence on Σ). Hastie et al. (2019) discuss that Σ only influences the expected
excess risk in the overparameterized regime. In the following, we will illustrate that Theorem 3
even implies that in the overparameterized case, Σ = λId yields the lowest ENoise . This fact is also
discussed in Muthukumar et al. (2020) in a slightly different setting. Note that since PX is unknown
in general, Σ is also unknown in general.
Assume that (NOI) holds with equality such that we have ENoise = σ2EZ tr((Z+)>ΣZ+) by
Theorem 3. Suppose that We perform linear regression on the whitened data Z := Σ-1∕2z. Then,
∙-v
ZZ
∙-v
Z Σ-1/2
Σ = EW 无Z
>
W,
∑-1/2 [Ezzzτ] Σ-1/2 = IP ,
-1/2
WZ = ZZ ΣZ -1/2 = ZZ = W .
In the underparameterized case p ≤ n, we then obtain
EZW tr((ZZ+)>ΣZZZ+) = EZW tr((WZ >WZ )-1) = EZ tr((W>W)-1) = EZ tr((Z+)>ΣZ+) .
Therefore, whitening the data does not make a difference if p ≤ n. In contrast, for p > n, we only
know
EZW tr((ZZ+)>ΣZZZ+) (=II) EZW tr((WZ WZ >)-1) = EZ tr((WW>)-1) (≤II) EZ tr((Z+)>ΣZ+)
25
Published as a conference paper at ICLR 2021
since (II) holds with equality for whitened features. From Step 2.1 in the proof of Theorem 3, it
is obvious that (II) in general does not hold with equality. Hence, in the overparameterized case
p > n, whitening the features often reduces and never increases ENoise . Since ENoise is just a lower
bound for the expected excess risk, whitening does not necessarily reduce the expected excess risk.
This phenomenon also has a different kernel-based interpretation: Under the assumptions (MOM)
and (COV), we can choose an ONB u1, . . . , up of eigenvectors of Σ with corresponding eigenvalues
λ1 , . . . , λp > 0. If z = φ(x) and k(x, x0) = φ(x)>φ(x0), we can write
Xnp
uiui> φ(x0) =	λiψi (x)ψi (x0) ,
i=1
where the functions ψi : Rd → R, x → √= u>φ(x) form an orthonormal system in L2(Pχ):
λi
(5)
Ex ψi(x)ψj (X) =	-/	UJ	[Exφ(X)φ(X)T] Uj	=	-/	UJ Σuj	=-八"\ UJ Uj	=	δij .	(6)
λiλj	λiλj	λiλj
Therefore, Eq. (5) is a Mercer representation of k and the eigenvalues λi of Σ are also the eigenval-
ues of the integral operator Tk : L2 (PX ) → L2 (PX) associated with k:
(Tk f)(X) :=	k(X, X0)f(X0) dPX(X0)
n
λi hψi, fiL2(PX)ψi(X) .
i=1
∙-v
We can define a kernel k with flattened eigenspectrum via
p
k(x, x0):= y^ψi(x)ψi(x0).
i=1
Its feature map X 7→ (ψi(X))i∈[n] by Eq. (6) satisfies Σ = Ip. By the above discussion, ENoise(k) ≤
ENoise(k). However, this needs to be taken with caution: Assume for simplicity that for independent
x, X, the feature map φ yields φ(x), φ(X)〜N(0, ɪIp). Then, k(x, x) = 1 but Ek(x, X) = 0
and Var k(X, X) = p2 PP=ι Eu,v〜N(o,i)u2v2 = P. In this sense, for P → ∞, k converges to the
Dirac kernel k(X, X) = δχ χ, which satisfies ENoise = 0 but provides bad interpolation performance
if fp≡ 0.	'	J
The next lemma and its proof are an adaptation of Lemma 4.14 in Bordenave & Chafai (2012).
Lemma G.2. For i ∈ [n], let W-i := Span{wj | j ∈ [n] \ {i}}. Then, under the assumptions of
Theorem 3, in the overparameterized case p ≥ n,
n
tr((WW>)-1) =Xdist(wi,W-i)-2 .	(7)
i=1
Proof. The case n = 1 is trivial, hence let n ≥ 2. In Step 3.1 in the proof of Theorem 3, since P2
is the orthogonal projection onto W-1 , we have
w1>(Ip - P 2)w1 = kw1k22 - kP 2w1k22 Pyth=agoras dist(w1, W-1)2 ,
where dist(wι, W-ι) is the Euclidean distance between wι and W-ι.	□
Remark G.3 (Is U (Sp-1 ) optimal?). From (I) in Theorem 3 it is clear that the best possible lower
bound for ENoise under the assumptions of Theorem 3 given n, p ≥ 1 is
ENoise ≥ σ2	inf	EZ〜Pn tr((Z+)>ΣZ+) .	(8)
Distribution PZ onRp satisfying (MOM), (COV), (FRK)	Z
Here, we want to discuss the hypothesis that the infimum in Eq. (8) is achieved (for example) by
PZ = U (Sp-1 ). Figure 1 shows that we were not able to obtain a lower ENoise by optimizing a neural
network feature map to minimize ENoise . In the following, we want to discuss some theoretical
evidence as to why this is plausible in the overparameterized case p ≥ n. Lemma G.2 shows that
Step 3.1 of the proof of Theorem 3 has a distance-based interpretation. In this interpretation, Step
3.1 then applies Jensen’s inequality to the convex function (0, ∞) → (0, ∞), x 7→ 1/x using
E dist(wi, W-i)2 = p + 1 - n .	(9)
26
PUbHShed as a COnferenCe PaPer at ICLR 2021
We CaIl USe this PerSPeCtiVe to gain insights on how distributionS PZ With Sman SNsse in -he Ove'
ParameteriZed CaSe pln IOok HkFirst OfL Remark G∙l SUggeStS that for minimizing mN。-Se
in the OVerParameteriZed casey M ShoUld be a multiple Of IP. WhiCh is CIearIy SatiSfied for Z2(s七——1)
by Theorem ILSinCe the Iler bound ObtaiIIed from (9) is independent Ofthe distribution Of 三
miniɪniziɪIglEtr ((myτ)L) amountsmimizg the error made by Jensesequalityy WhiCh
essentially amounts to reducing the VarianCe Ofthe random Variables dist(亨 S——• We CaIl decom—
POSediSt: Su一 2 ∙ dist(e∕一 2 w——WherediStE/- 2 WL OlllydePendS
on the angular COmPOIIeIltS £一 £ 一 一 2 for j m 亘∙ ThiS SUggeStS that for a agoodistribution ∫⅛z.
• the radial COmPOIIent 一一 2 ShoUId have Il Variane 尸 and
• the distribution OftheangUIar COmPOIIent §一 2 ShOUId IlOt COlltaiIlluster Ssince
ClUSterS WOUId increase the PrObabihty Of dist(lυVV——being Very SmaIL
clearlyy both Pots are PerfeCtIy achieved for PN u(sp——1▲
H PRooFS FoR SECTIoN 5
InthiS SeCtwe ProVe all theorems and PrOPOSitns from SeCtiOn 5 as WelI as SOme additIIal
results.
H∙l MlSCELLANEoUS
rsL We ProVe a Statement about COnditIIal variance
Lemma H∙L In the SettMg OfTheorem 3 We hae
Var (N)U IE(Var (公一包 一 N) + Var(IE(包一 N) ∙
HenCQyvar() ≥ Q2 almost Sllreiy OVer 和 Ihen var(N) ≥ Q2 almost SIlrely OVer N∙ The
ConyerSe holds》for example ifeInjeCHV
PrOOf FOr ProPertieS Of COndit5'IIaI expectation-WerefertOthe Htemtur---chapter 4」
DUrrett (2019∙SinCeNU^h) is a function of 目 WehaVelE∙-a口旦h(-H-NL Thu-
Var(9 一Z) =IEf (9 — E(9 - Z)) 2瓦
=E ((9 — 同(9一包)+ħ(9h) — IE(IE(9 豆一 Z))) 2 M
= EfE ((9 — E(9H))2H)ɪ
+ eħ((9 — e(9h))ħ(9h) — E(E(9 豆一 Z))h)a
+ eħ(ħ(9 豆 — IE(IE(9 豆一 Z))2h)-z-∙
FOrthe first termy we have H ((公 — IE()2 一UVar (包 by defitIL The SeCOIld term is Z
SinCe() I IE(IE() 一N) is already a function Of 目 We have
IE(Gl 犀包)((—((包- N))亘 UIE(Gl(包包((包—犀犀公一包- N))
U (犀包—犀公一包)((公一包 — IE(IE(- N))
H O •
na=y=he third term is by defitn equal to Var(IE() 一 NLTherefOr
Var (N)U IE(Var (公一包 一 N) + Var(IE(包一 N) ∙
If e is injectiv 尸 then S is also a function Of N and We ObtaiIl Var (N) UVar (
OUrma5'ingredient for anaIyZing anaIytiC activation functionS will be the univariate and multiw
ate identity theorem
TheOrem H,2 (Identity theorem for univariate real anaIytiC functionS)∙ Let j * JlOeMW
Iff is not the 尸 ero func-ion.Ihe SetN〔H m - j(N) H O) has no accllmaln POinL
PartiCar Nis COlmIabl
27
□
Published as a conference paper at ICLR 2021
Proof. See e.g. Corollary 1.2.7 in Krantz & Parks (2002) for the first statement. IfN(f) is uncount-
able, there exists k ∈ Z such that [k, k + 1] ∩ N (f) is also uncountable, hence it contains a strictly
increasing and bounded sequence of points, and the limit of this sequence is an accumulation point
Of N(f).	口
Theorem H.3 (Multivariate version of the identity theorem). Let f : Rd → R be analytic. If f is
not the zero function, then N(f) := {x ∈ Rd | f(x) = 0} is a Lebesgue null set.
Proof. AlthOugh less well-knOwn than the univariate versiOn, this multivariate versiOn has been
prOven several times in the literature. FOr example, different prOOfs are given in SectiOn 3.1.24 in
Federer (1969), Lemma 1.2 in Nguyen (2015) and PrOpOsitiOn 0 in Mityagin (2015). MOre prOOf
strategies have been hinted at in Lemma 5.22 in Kuchment (2015). Here, we prOvide an elementary
prOOf fOllOwing the prOOf strategy briefly mentiOned at the beginning Of SectiOn 4.1 in Krantz &
Parks (2002).
Let λd be the Lebesgue measure On Rd and let λ := λ1. We prOve the statement by inductiOn On
d ≥ 1. FOr d = 1, if λ(N (f)) > 0, then N(f) is uncOuntable and hence f ≡ 0 by TheOrem H.2.
NOw, let the statement hOld fOr d - 1 ≥ 1 and assume λd(N(f)) > 0. FOr a ∈ R, define the
functiOns fa : Rd-1 → R, fa(x) = f(a, x). Then,
0<
λd(N(f)) =	1N(f)dλd
Rp
1N(f) (a, x) dλd-1(x) dλ(a)
/
R
λd-1(N(fa))da .
It fOllOws that the set U := {x ∈ R | λd-1 (N(fx)) > 0} satisfies λ(U) > 0. By inductiOn,
fOr all x ∈ U, we have fx ≡ 0. Then, fOr all x ∈ Rd-1, we can cOnclude that the functiOn
fx : R → R,a → f (a, x) satisfies N(fχ) ⊇ U and therefore λ(N(fχ)) ≥ λ(U) > 0. Using the
case d = 1 again, it follows that fx ≡ 0 and therefore f (a, x) = 0 for all a ∈ R and X ∈ Rd-1. 口
The following lemma provides some intuition about null sets for readers less familiar with measure
theory. Recall that a property Q holds almost surely with respect to a measure P on Rd if there
exists a null set N, i.e. a measurable set with P(N) = 0, such that Q(x) holds for all x ∈ Rd \ N.
Lemma H.4. Let λd be the Lebesgue measure on Rd and let P be a measure on Rd with a Lebesgue
density function (i.e. a probability density function) p. Then, a null set with respect to λd is also a
null set with respect to P. The converse holds if p(x) 6= 0 for (almost) all x.
Proof. A well-known fact from measure and integration theory states that if a measure μ has a
density with respect to a measure ν, then V-null sets are also μ-null sets. Setting μ = P and V = λd
yields the first fact. If P(X) = 0 for (almost) all x, then μ := λd has density 1/p with respect to
v := P, and hence the converse follows.	口
Proposition 6 (Characterization of (COV) and (FRK)). Consider the setting of Theorem 3 and let
FRK(n) be the statement that (FRK) holds for n. Then,
(i)	Let n ≥ 1. Then, FRK(n) iffP(z ∈ U) = 0 for all linear subspaces U ⊆ Rp of dimension
min{n, p} - 1.
(ii)	Let (MOM) hold. Then, (COV) holds iffP(z ∈ U) < 1for all linear subspaces U ⊆ Rp of
dimension p - 1.
Assuming that (MOM) holds such that (COV) is well-defined, consider the following statements:
(a)	FRK(p) holds.
(b)	FRK(n) holds for all n ≥ 1.
(c)	(COV) holds.
×d
(d)	There exists a fixed deterministic matrix X ∈ Rp×d such that det(φ(X)) 6= 0.
We have (a) ⇔ (b) ⇒ (c) ⇒ (d). Furthermore, if x ∈ Rd has a Lebesgue density and φ is analytic,
then (a) - (d) are equivalent.
Proof. Step 1: Prove (i) and (ii).
28
Published as a conference paper at ICLR 2021
(i)	Denote the n (stochastically independent) rows of Z by z1 , . . . , zn . First, assume P (z ∈
U) > 0 for some subspace U of dimension min{n, p} - 1. Then,
P rank(Z) ≤ min{n, p} - 1 ≥ P (z1 , . . . , zn ∈ U) = (P(z ∈ U))n > 0 .
For the converse, it suffices to consider the case n ≤ p since if n > p and if an arbitrary
p × p submatrix of Z ∈ Rn×p almost surely has full rank, then Z also almost surely has full
rank. We prove the statement for n ≤ p by induction on n. For n = 1, the claim is trivial.
Thus, let n > 1 and let z1, . . . , zn be the (stochastically independent) rows of Z. Assume
P (z ∈ U) = 0 for all linear subspaces U ⊆ Rp of dimension n - 1. Then, we also have
P (z ∈ U ) = 0 for all linear subspaces U ⊆ Rp of dimension n - 2 and by the induction
hypothesis, we obtain that z1 , . . . , zn-1 are almost surely linearly independent. Hence,
almost surely, z1, . . . , zn are linearly independentiffzn ∈/ Un-1 := Span{z1, . . . , zn-1}.
Then, since zn and Un-1 are stochastically independent,
P(Z ∈ Rn×p has full rank) = P(z1, . . . , zn are linearly independent)
= P(zn ∈/ Un-1) =	1Unc-1 (zn) dPZ (zn) dP (Un-1)
=	PZ(zn ∈/ Un-1)dP(Un-1) =	1dP(Un-1) = 1 .
For the case p ≤ n, (i) is also proven after Definition 1 in Mourtada (2019).
(ii)	If (COV) does not hold, there exists a vector 0 6= v ∈ Rp with v> Σv = 0, hence
0 = v> (Ezz>) V = E(v>z)2 ,
and therefore z is almost surely orthogonal to v. If U is the orthogonal complement of
Span{v} in Rp, then P (z ∈ U) = 1.
For the converse, if there exists a (p - 1)-dimensional subspace U with P(z ∈ U) = 1,
then we can again take a vector 0 6= v ∈ Rp that is orthogonal to U, reverse the above
computation and obtain v>Σv = 0, hence (COV) does not hold.
Step 2: (a) ⇔ (b). The implication (b) ⇒ (a) is trivial and the implication (a) ⇒ (b) follows
immediately from (i).
Step 3: (b) ⇒ (c). This also follows from (i) and (ii).
Step 4: (c) ⇒ (d). We will prove by induction on n that for all n ∈ {0, . . . , p}, there exist
x1, . . . , xn ∈ Rd such that φ(x1), . . . , φ(xn) are linearly independent. For n = 0, the statement is
trivial. Now assume that the statement holds for x1, . . . , xn-1, where 0 ≤ n - 1 ≤ p - 1. Since
(COV) holds, by (ii) the subspace U := Span{φ(x1), . . . , φ(xn-1)} satisfies P (φ(x) ∈ U) < 1,
hence there is xn such that xn ∈/ U and for this choice, φ(x1), . . . , φ(xn) are linearly independent.
Finally, the statement for n = p yields the existence of X ∈ Rp×d such that φ(X) has linearly
independent rows, which implies det(φ(X)) 6= 0.
Step 5: Analytic feature map. Assume that φ is analytic, x has a Lebesgue density and (d) holds.
d
Let n = p. For X ∈ Rp×d, consider the analytic function f(X) := det(φ(X)). (The determinant
is analytic since it is a polynomial in the matrix entries.) Since (d) holds, Theorem H.3 shows that
f (X) 6= 0 for (Lebesgue-) almost all X. Since x has a Lebesgue density and X has independent
x-distributed rows, X has a Lebesgue density. Therefore, f(X) 6= 0 almost surely over X, hence
(a) holds.	□
Proposition 8 (Polynomial kernel). Let m, d ≥ 1 and c > 0. For x, X ∈ Rd, define the polynomial
kernel k(x, X) := (x>X + c)m. Then, there exists a feature map φ : Rd → Rp, P := (m^d), such
that:
(a)	k(x, X) = φ(x)>φ(X) forall x, X ∈ Rd, and
(b)	ifX ∈ Rd has a Lebesgue density and we use z = φ(X), then (FRK) is satisfied for all n.
Proof. Let M := {(m1, . . . , md+1) ∈ N0d+1
(m1, . . . , md+1) ∈ M, let
C(m) := m1
| m1 + . . . + md+1 = m} and for m
m
. . . md+1
29
Published as a conference paper at ICLR 2021
be the corresponding multinomial coefficient. Then, |M| = md+d = p. Define the feature map
φ : Rd → Rp by
φ(x) = (pc(m)zm1 ∙∙∙Zmd∙(√c)md+1 )m∈M .
(a)	We have
φ(x)>φ(X) = ^X C(m)(xιXι)m1 …(Xdxd)md ∙ cmd+1
m∈M
=(x1x1 + ... + xdxd + c)m = k(χ, X).
(b)	Assume that x has a Lebesgue density. Let U be an arbitrary (p - 1)-dimensional linear
subspace of Rp. Then, there exists 0 6= v ∈ Rp such that U = (Span{v})⊥. Since the
monomials xmm1 2 ∙ ∙ ∙ xmmd for m ∈ M are all distinct, the polynomial
v>Φ(x) = ^X 卜mρc(m)cmd+1xm1 …xmd)
m∈M
is not the zero polynomial. By the identity theorem (Theorem H.3), since x has a Lebesgue
density and since polynomials are analytic,
P(φ(x) ∈ U) = P (v>φ(x) = 0) = 0 .
Hence, Proposition 6 shows that (FRK) is satisfied for n = P and hence for all n.	□
We want to remark at this point that the proof strategy of Proposition 8, where the identity theorem
is applied to the functions v>φ(x) for all 0 6= v ∈ Rp, does not work for random feature maps: The
statements
•	For all 0 6= v ∈ Rp for almost all x for almost all θ, v>φθ (x) 6= 0
•	For almost all θ for all 0 6= v ∈ Rp for almost all x, v>φθ (x) 6= 0
are not equivalent since in the first statement, the null set for θ may depend on v , and the union of
the null sets for all v is an uncountable union. Perhaps the simplest counterexample is n = p = 2,
θ 〜N(0,12) and φθ(x) := θ, which satisfies the first but not the second statement. For our rescue,
we can replace the uncountable analytic function family (X → v>φθ(x))v∈rp∖{o} by the single
analytic function (θ, X) 7→ det(φθ (X)):
Proposition 9 (Random feature maps). Consider feature maps φθ : Rd → Rp with (random)
parameter θ ∈ Rq. Suppose the map (θ, X) 7→ φθ (X) is analytic and that θ and X are independent
×d
and have Lebesgue densities. If there exist fixed θ ∈ Rq, X ∈ Rp×d with det(φθe (X)) 6= 0, then
almost surely over θ, (FRK) holds for all n for z = φθ (X).
Proof. Consider the analytic map (θ, X) 7→ det(φθ (X)). Suppose there exist θ ∈ Rq and X ∈
Rp×d with det(φe(X)) = 0. Then, Theorem H.3 tells us that det(φ(θ, X)) = 0 for almost
all (θ, X). This implies that for almost all θ, we have for almost all X that det(φθ(X)) 6= 0.
Since by assumption, θ has a density, this implies that almost surely over θ, there exists X such
that det(φθ (X)) 6= 0. Since all φθ are analytic, the claim now follows using (d) ⇒ (b) from
Proposition 6. (The proof of (d) ⇒ (a) ⇔ (b) in Proposition 6 does not require (MOM).)	□
H.2 Random Networks with Biases
In order to prove (FRK) for random deep neural networks, we pursue slightly different approaches
for networks with bias (this section) and without bias (Appendix H.3). In both approaches, we
consider a property related to the diversity of the X1 , . . . , Xn ∈ Rd and proceed as follows:
(1) Projections: Ensure that random projections of the X1 , . . . , Xn almost surely preserve the
diversity property, such that it is sufficient to consider the case d = 1.
(2) Propagation: Using the projection result from (1), prove that if the inputs to a layer have
the diversity property, then the outputs also have the diversity property almost surely over
the random parameters of the layer.
30
Published as a conference paper at ICLR 2021
(3) Independence: Prove that if the inputs to the last layer have the diversity property and
n = p, then the outputs of the last layer are almost surely linearly independent.
Our main tools will be the identity theorems for analytic functions (Theorem H.2 and Theorem H.3),
expanding σ into its power series around a point and the Leibniz formula for the determinant of a
n × n matrix, which is based on the permutation group Sn on [n].
We consider two diversity properties:
(a)	The first property is that x1 , . . . , xn are distinct. This is the weakest possible diversity
property. However, it cannot always be used for networks without (random) biases: For
example, if σ is an even function and xi = -xj for some i 6= j, the propagation property
is violated. As another example, if σ(0) = 0 and xi = 0 for some i, the independence
property is violated.
(b)	The second property, which works for networks with and without bias, is the property that
x1, . . . , xn are independent nonatomic random variables.
Using the first property yields shorter proofs and a slightly stronger theorem for networks with bias,
Theorem H.9. If we only care about probability distributions PX on x that almost surely generate
distinct x1 , . . . , xn , these probability distributions are exactly the nonatomic distributions. Hence
from the viewpoint of probability distributions PX on x, which we take in the main part of the paper,
the first property (a) does not provide a benefit over the second property (b) except for the shorter
proofs.
The advantage of having biases is in being able to choose the point in which σ is Taylor-expanded.
The following lemma shows that this choice enables us to make certain coefficients of the Taylor
expansion nonzero:
Lemma H.5. Let m ≥ 1. Let σ : R → R be analytic and not a polynomial of degree less than m.
Then, there exists b ∈ R such that the Taylor expansion
∞
σ(x) = Xak(x - b)k
k=0
of σ around b satisfies a0 , . . . , am 6= 0.
Proof. Since σ is not a polynomial of degree less than m, neither of the derivatives σ(0) , . . . , σ(m)
is the zero function. Since all of these derivatives are analytic, the set
m
[{b∈R | σ(k) (b) =0}
k=0
is (by Theorem H.3) a finite union of Lebesgue null sets and hence a Lebesgue null set. Hence,
there exists b ∈ R such that σ(0) (b) 6= 0, . . . , σ(m) (b) 6= 0. This implies that the corresponding
coefficients ao,...,am in the Taylor expansion around b are nonzero.	□
We now prove our three-step program (1) - (3) from above for the distinctness property.
Lemma H.6 (Projections of distinct variables). If x1, . . . , xn ∈ Rd are distinct, there exists u ∈ Rd
such that u>x1, . . . , u>xd are also distinct.
Proof. We essentially follow the corresponding proof in Lemma 4.3 in Nguyen & Hein (2017). By
the identity theorem (Theorem H.3), the functions
fij : Rd → R, u 7→ u>xi - u>xj
for i, j ∈ [n], i 6= j are nonzero almost everywhere, hence there exists u ∈ Rp such that
u>xι,..., u>Xn are all distinct.	□
Lemma H.7 (Propagation of distinct variables). Let σ : R → R be analytic and non-constant. If
x1, . . . , xn ∈ Rd are distinct, then for (Lebesgue-) almost all (W, b) ∈ Rp×d × Rp, the vectors
σ(W xi + b) (with σ applied element-wise) are also distinct.
31
Published as a conference paper at ICLR 2021
Proof. Step 1: Bias. By assumption, σ is not a polynomial of degree less than m = 2. By
Lemma H.5, there exist (ak)k≥0, b ∈ R and ε > 0 such that a0 , a1 6= 0 and for all x ∈ R
with |x - b| < ε,
∞
σ(x) = X ak(x - b)k .
k=0
Step 2: Weight. By Lemma H.6, we can choose u ∈ Rd such that u>x1 , . . . , u>xn are distinct.
Now, consider i, j ∈ [n] with i 6= j. The function
fij : R → R, λ 7→ σ(λu>xi + b) - σ(λu>xj + b)
satisfies
∞
fij(λ)=Xak((u>xi)k-(u>xj)k)λk
k=0
for sufficiently small ∣λ∣. Here, the coefficient a*((UTxi)k — (UTxj)k) is nonzero for k = 1, and
hence fij is not the zero function. Using the identity theorem again (as in the proof of Lemma H.6,
we find that there exists λ ∈ R with fij (λ) 6= 0 for all i, j ∈ [n] with i 6= j.
Step 3: Generalization. Now, choose
∕λuτ∖	∕b∖
W :=	. I ∈ Rp×d,	b :=	. I ∈ Rp .
λUτ	b
Then, by construction, the first components (σ(Wxi + b))1 for i ∈ [n] are distinct. Hence, the
analytic function
(W,b) 7→ Y ((σ(W xi + b))1 — (σ(W xj + b))1)
i,j∈[n]
i6=j
is not the zero function. By the identity theorem (Theorem H.3), for (Lebesgue-) almost all (W, b),
the first components of the vectors σ(W xi+b) are distinct, and therefore also the vectors themselves
are distinct.	□
Lemma H.8 (Independence from distinct variables). Let p, d ≥ 1. Let σ : R → R be analytic but
not a polynomial of degree less than p — 1. If x1, . . . , xp ∈ Rd are distinct, then for (Lebesgue-
) almost all W ∈ Rp×d and b ∈ Rp, the vectors σ(W x1 + b), . . . , σ(W xp + b) are linearly
independent.
Proof. Step 1: Preparation. By Lemma H.5, there exist (ak)k≥0, b ∈ R and ε > 0 such that
a0, . . . , ap-1 6= 0 and for all x ∈ R with |x — b| < ε,
∞
σ(x) = Xak(x — b)k .
k=0
By Lemma H.6, there exists U ∈ Rd such that xi := Uτxi for i ∈ [p] are all distinct. Using the
fact that Vandermonde matrices of distinct xi are invertible and using the Leibniz formula for the
determinant (with the permutation group Sp on [p]), we obtain
p	/ x0	...	xp ʌ
Dx := X sgn(∏) Y xi-1) = det .	...	. I =0 .	(10)
π∈Sp	i=1	x1p-1	. . . xpp-1
Step 2: Determinant expansion. Define the analytic function
σ(w1x1 + b) . . . σ(wpx1 + b)
f : Rp → R, w → det :	...	:	I .
σ(w1xp + b) . . . σ(wpxp + b)
32
Published as a conference paper at ICLR 2021
For small enough kwk2 , we can use the Leibniz formula for the determinant to write
p∞
f(w) =	sgn(π)π∑ak (wixπ(i))k
π∈Sp	i=1 k=0
p
= X X sgn(π) Y aki wiki xkπi(i)
k1 ,...,kp ≥0 π∈Sp	i=1
=X	(YI ak)( X sgn(∏) YY x∏(i)l wk1 …wpp.
k1 ,...,kp≥0 i=1	π∈Sp	i=1
For ki := i - 1, we find the coefficient of Wk1 …Wpp to be
p
aki
i=1
X sgn(π) ∏ Xni(i)) (=) (∏ a®) ∙ Dx = 0,
π∈Sp	i=1	i=1
hence f is not the zero function and there exists w ∈ Rp such that f(w) 6= 0.
Step 3: Generalization. Consider the analytic function
∕σ(Wxι + b)>∖
g(W, b):=det	. I .
σ(Wxp+	b)>
When setting W := wu> ∈ Rp×d and b := (b, . . . , b)> ∈ Rp, we obtain from Step 2 that
g(W, b) = f(w) 6= 0, hence g is not the zero function. But then, by the identity theorem (Theo-
rem H.3), g is nonzero for (Lebesgue-) almost all (W, b).	口
Theorem H.9. Let p, d ≥ 1, let σ : R → R be analytic but not a polynomial of degree less than
max{1, p - 1} and let x1 , . . . , xp ∈ Rd be distinct. Let L ≥ 1 and d0 := d, d1 , . . . , dL-1 ≥
1, dL := p. For l ∈ {0, . . . , l - 1}, let W(l) ∈ Rdl+1 ×dl and b(l) ∈ Rdl+1 be random variables such
that θ := (W(0), . . . , W(L-1), b(0), . . . , b(L-1)) has a Lebesgue density. Consider the random
feature map given by
φθ (x(0)) := x(L), where x(l+1) := W(l)x(l)+	b(l) .
Then, almost surely over θ, φθ (x1 ), . . . , φθ(xp) are linearly independent.
Proof. By Lemma H.4, it suffices to consider the case where θ has a standard normal distribution,
since a standard normal distribution has a nonzero probability density everywhere. Especially, in
this case, all weights and biases are independent. Using Lemma H.7 and that σ is non-constant, it
follows by induction on l ∈ {0, . . . , L - 1} that x(1l), . . . , x(pl) are distinct almost surely over θ. But
if x(1L-1), . . . , x(pL-1) are distinct, then x(1L), . . . , x(pL) are linearly independent almost surely over
θ by Lemma H.8, which is What We wanted to show.	口
H.3 Random Networks without Biases
As discussed at the beggining of Appendix H.2, we will now consider the property of having inde-
pendent nonatomic random variables x1 , . . . , xn. We will again consider projection and propagation
lemmas first.
Lemma H.10 (Projections of nonatomic random variables). A random variable x ∈ Rd is
nonatomic iff for (Lebesgue-) almost all u ∈ Rd, u>x is nonatomic.
Proof. Step 1: Decompose into subspace contributions. For k ∈ {0, . . . , d}, let
Sk := {w+	V | w ∈ Rd, V linear subspace ofRd, dim V = k}
33
Published as a conference paper at ICLR 2021
be the set of k-dimensional affine subspaces of Rd. Let A0 := {A ∈ S0 | PX (A) > 0}, which cor-
responds to the set of atoms of PX . We then recursively define “minimally contributing subspaces”
for k ∈ [d]:
∙-v	∙-v	∙-v
Ak := {A ∈ Sk | PX(A) > 0 and for all A ∈ Sk-ι with A ⊆ A, PX(A) = 0}.
We can define corresponding sets of “annihilating” vectors as follows: For A = w + V ∈ Sk, let
A⊥ := V ⊥ = {u ∈ Rd | for all v ∈ V , u>v = 0}. This is well-defined since it is independent of
the choice of w. Define
d
U:=[ [ A⊥
k=0 A∈Ak
N := {u ∈ Rd | u>x is not nonatomic} .
We want to show U = N .
Step 2: Show U ⊆ N. Let A = w + V ∈ Ak for some k ∈ {0, . . . , d} and let u ∈ A⊥. Then,
0 < PX (A) = P(x ∈ A) ≤ P(u>x ∈ {u> (w + v) | v ∈ V }) = P(u>x = u>w) ,
which shows u ∈ N.
Step 3: Show N ⊆ U. Let u ∈ N, i.e. there exists a ∈ R such that P(u>x = a) > 0. Define
A := {v ∈ Rd | u>v = a}. Then, A is an affine subspace of Rd and we have PX (A) > 0
∙-v	∙-v
by construction of A. Among all affine subspaces A of A with PX (A > 0), there exists one
with minimal dimension. This subspace then satisfies A ∈ Adim A and it is not hard to ShoW that
U ∈ A⊥ ⊆ A⊥, hence U ∈ U.
Step 4: For all k ∈ {0, . . . , d}, Ak is countable. To derive a contradiction, assume that Ak is
uncountable. Then, there exists ε > 0 such that
Ak,ε := {A ∈Ak | PX (A) ≥ ε}
is also uncountable. Pick an integer n > 1∕ε and pick n distinct sets Aι,...,An ∈ Ak,ε. We will
show by induction on l ∈ [n] that Ai := Ai ∪ ... ∪ Al satisfies
∙-v
PX (Al)= PX (AI) + …+ PX (Al),
which will then yield the contradiction
∙-v
1 ≥ PX(Al) = PX(Ai) + ... + PX(An) ≥ nε > 1 .
∙-v
Obviously, PX(A1) = PX(A1). Assuming that the statement holds for l ∈ [n - 1], we first derive
∙-v	∙-v	∙-v	∙-v
PX(Al+i) = PX(Al ∪ Al+i) = PX(Al) + PX(Al+i) - PX(Al ∩ Al+i)
∙-v
=px(Ai) + …+ px(Al+i) - px(Al ∩ Al+i).
For i 6= j , the intersection Ai ∩ Aj of two distinct k-dimensional affine subspaces is either empty
or an affine subspace of dimension less than k, hence the definition of Ak yields PX (Ai ∩ Aj) = 0.
Therefore,
∙-v
Px (Al ∩ Al+i) = Px ((Ai ∩ Al+i) ∪ ... ∪ (Al ∩ Al+i))
≤ PX (Ai ∩ Al+i) + . . . + PX (Al ∩ Al+i)
=0+...+0=0,
which concludes the induction.
Step 5: Conclusion. Let A ∈ Sk, then A⊥ is a (d - k)-dimensional linear subspace of Rd. If x is
not nonatomic, the set A0 is not empty, and hence
N = U ⊇ [ A⊥ = Rd ,
A∈A0
which means that N is not a Lebesgue null set. Conversely, if x is nonatomic, the set A0 is empty,
and therefore
d
N=U=[ [ A⊥
k=i A∈Ak
is (by Step 4) a countable union of proper affine subspaces of Rd, all of which are Lebesgue null
sets. Therefore, N is a Lebesgue null set.	□
34
Published as a conference paper at ICLR 2021
Lemma H.11 (Propagation of nonatomic random variables). Let the random variable x ∈ Rd be
nonatomic.
(a)	For all p ≥ 1 and (Lebesgue-) almost all W ∈ Rp×d, Wx is nonatomic.
(b)	For all b ∈ Rd, x + b is nonatomic.
(c)	If σ : R → R is analytic and not constant, then σ(x) ∈ Rd is nonatomic, where σ is
applied element-wise.
Proof.
(a)	By Lemma H.10, the set N := {u ∈ Rd | u>x is not nonatomic} is a Lebesgue null set.
If w1 is the first row of W , then clearly,
{W ∈ Rp×d | W x is not nonatomic} ⊆ {W ∈ Rp×d | w1 ∈ N } ,
where the right-hand side is a Lebesgue null set. This proves the claim.
(b)	This is trivial.
(c)	Let z ∈ Rd. Since the function R → R, x 7→ σ(x) -zi is analytic and not the zero function
by assumption, its zero set σ-1 ({zi}) is countable by the identity theorem, Theorem H.2.
Therefore, the set
σ-1({z}) = {x ∈ Rd | σ(x) = z} = σ-1({zι}) × ... X σ-1({zd})
is also countable. Thus, since x is nonatomic, we obtain
P(σ(x) = z) = P(X ∈ σ-1({z})) =	^X	P(X = X)=	^X	0 = 0 .	□
X∈σ-1({z})	X ∈σ-1({z})
For proving independence from nonatomic random variables, we need some preparation. In the
proof of the independence result for the case with biases (Lemma H.8), a Vandermonde matrix
appeared. In the case of networks without bias, we will not be able to use Lemma H.5 to control the
power series coefficients of σ, which requires us to treat Vandermonde matrices with more general
exponents.
Lemma H.12 (Random Vandermonde-type matrices). For n ≥ 1, let x1, . . . , xn be independent
nonatomic R-valued random variables and let k1, . . . , kn be distinct non-negative integers. Then,
the random Vandermonde-type matrix
xk1	xk1
x1	. . . xn
V	:= V kι,...,kn (xi,...,Xn) :=	：	∙..	： I
x1kn	. . . xknn
is invertible with probability one：
Proof： By swapping rows of V, we can assume without loss of generality that 0 ≤ k1 < k2 <
. . . < kn. We prove the statement by induction on n. For n = 1, V is invertible whenever x1 6= 0,
and this happens with probability one. Now assume that the statement holds for n - 1 ≥ 1. For
i ∈ [n], define the submatrices
Vi	:= Vk1 ,...,kn-1 (x1, . . . , xi-1 , xi+1 , . . . , xn) .
Since the statement holds for n - 1, Vb n is invertible with probability one. Now, fix any such
x1, . . . , xn-1 where Vb n is invertible. Especially, C := det(Vb n) is a non-zero constant. We will
show that V is then invertible almost surely over xn . For this, we compute the determinant of V
using the Laplace expansion with respect to the last row of V as
n	n-1
f(xn) := det(V) = X(-1)i+nxikndet(Vbi) = Cxknn + X(-1)i+nxikn det(Vbi) .
i=1	i=1
By the Leibniz formula, for i ∈ [n - 1], det(Vi) is a polynomial in xn of degree ≤ kn-1 < k.
Hence, f is a nonzero polynomial in xn of degree kn and has at most kn zeros. Since any finite set is
a null set with respect to a nonatomic distribution, it follows that det(V) 6= 0 almost surely over xn.
Since the assumptions on xi,..., Xn-ι are also satisfied almost surely, the statement follows. □
35
Published as a conference paper at ICLR 2021
As in the case of networks with biases, we will prove the independence result by first reducing it
to the case d = 1. Since we also want to perform this reduction for random Fourier features in
Proposition I.1, we state the reduction to the d = 1 case as a separate result, Lemma H.14, and
define the d = 1 case in the following definition.
Definition H.13 (Non-degenerate). Let p, q ≥ 1. We call a function f : Rq → Rq non-degenerate if
it is analytic and for arbitrary independent R-valued nonatomic random variables x1, . . . , xp, there
almost surely exists w = wx1,...,xp ∈ Rq with
f ( (wxi)>∖
det . I = 0 .	J
f(wxp)>
Lemma H.14. Let f : Rq → Rp be non-degenerate, let W ∈ Rq×d be a random variable with a
Lebesgue density and let x1 , . . . , xp ∈ Rd be independent nonatomic random variables. Then,
(f (Wxι)>∖
det :	I = 0
f(Wxp)>
almost surely over W and x1 , . . . , xp.
Proof. By Lemma H.10, there exists u ∈ Rd such that for all i ∈ [p], xi := u>xi ∈ R is nonatomic.
Obviously, x1, . . . , xn are independent. Fix x1, . . . , xn such that w = wx1,...,xp ∈ Rq as in Defini-
p>
tion H.13 exists, which is true with probability one since f is non-degenerate. Then, for W := wu>,
we have
_	(((W xι)> ʌ	(f (wxi)τ∖
g(W):= det	. I = det . I = 0 .
((Wfxp)τ	((wxp)τ
Since g is a non-zero analytic function, Theorem H.3 shows that g is only zero on a Lebesgue null
set, and this null set is also a null set with respect to the distribution of W since W has a Lebesgue
density. Hence, g(W) = 0 almost surely over W.	□
The following lemma proves the d = 1 version of the independence result, which can then be
upgraded to the general case using Lemma H.14.
Lemma H.15 (Independence from nonatomic random variables). Letp ≥ 1. Let σ : R → R be an-
alytic and not a polynomial with less than p nonzero coefficients. Then, the elementwise application
function ( : Rp → Rp, x 7→ (σ(x1), . . . , σ(xp))τ is non-degenerate in the sense of Definition H.13.
Proof. Let x1, . . . , xp be independent scalar nonatomic random variables.
Step 1: Power series coefficients. Since σ is analytic, there exists ε > 0 and coefficients (ak)k≥0
such that
∞
σ(z) = Xakzk
k=0
for z ∈ R with |z| < ε. Let K := {k ≥ 0 | ak 6= 0}. Then, |K| ≥ p: Assume that |K| ≤ p - 1,
then the polynomial
h : R → R, z 7→ X akzk
k∈K
equals σ on (-ε, ε). Hence, the function g := σ - h is zero on (-ε, ε). By Theorem H.2, g is
the zero function and hence σ = h is a polynomial with less than p nonzero coefficients, which we
assumed not to be the case.
36
Published as a conference paper at ICLR 2021
Step 2: Condition on the xi. By Step 1, we can choose indices k1 < k2 < . . . < kp with
k1, . . . , kp ∈ K. Then, by Lemma H.12 and the Leibniz formula for the determinant, we have
k1	k1
x1	. . . xp
Dx := X sgn(π)x∏(i) ∙...∙χ∏Pp) = det .	...	. I =0	(II)
π∈Sp	x1kp . . . xpkp
with probability one.
Step 3: Determinant power series. Now, fix a realization of x1 , . . . , xp such that (11) holds. For
w ∈ Rp with sufficiently small kwk∞, we can write
'f(wxι)>)
g(w) := det	. I
f(wxp)>
σ(w1x1) . . . σ(wpx1)
=det .	...	. I
σ(w1xp ) . . . σ(wpxp)
p∞
=	sgn(π)	ak (wixπ(i))k
π∈Sp	i=1 k=0
p
= X X sgn(π)Yakiwikixkπi(i)
k1 ,...,kp ≥0 π∈Sp	i=1
=X (YI aki)	(X sgn(∏) ∏ x∏i(i)∣ wk1∙...∙ WR	(12)
k1 ,...,kp≥0 i=1	π∈Sp i=1
Now, consider the	special values k1, . . . , kp	chosen in Step 2. Since	ki ∈ K, we have aki	6= 0.	The
coefficient of the multivariate monomial wk1 ∙ ... ∙ Wpp in Eq. (12) is
Y aki	X sgn(π) ∏ x∏(i) ) = akι ∙ ... ∙ akp ∙ Dx = 0 .
i=1	π∈Sp	i=1
If g was the zero function, all derivatives of g would be zero and therefore the coefficients of all
monomials would be zero, which is not the case. Hence, there exists w ∈ Rp with g(w) 6= 0. This
shows that f is non-degenerate.	□
H.4 Random Networks: Conclusion
In the following, we will prove Theorem 10 and discuss some possible extensions and limitations.
Theorem 10 (Random neural networks). Let d, p, L ≥ 1, let σ : R → R be analytic and let the
layer sizes be d0 = d, d1, . . . , dL-1 ≥ 1 and dL = p. Let W(l) ∈ Rdl+1 ×dl for l ∈ {0, . . . , L - 1}
be random variables and consider the two cases where
(a)	σ is nota polynomial with less than p nonzero coefficients, θ := (W(0), . . . , W(L-1)) and
the random feature map φθ : Rd → Rp is recursively defined by
φ(x(0)) := x(L), x(l+1) := σ(W(l)x(l)) .
(b)	σ is not a polynomial of degree < p - 1, θ := (W(0), . . . , W(L-1), b(0), . . . , b(L-1))
with random variables b(l) ∈ Rdl+1 for l ∈ {0, . . . , L - 1}, and the random feature map
φθ : Rd → Rp is recursively defined by
φ(x(0)) := x(L), x(l+1) := σ(W (l)x(l) + b(l)) .
In both cases, if θ has a Lebesgue density andx is nonatomic, then (FRK) holds for all n and almost
surely over θ.
37
Published as a conference paper at ICLR 2021
Proof. By Lemma H.4, it suffices to consider the case where θ has a standard normal distribution,
since a standard normal distribution has a nonzero probability density everywhere. Especially, we
can assume that all parameters in θ are independent. By Proposition 6, we only need to prove (FRK)
for n = p. Let x10),..., χP0) 〜PX be i.i.d. nonatomic random variables.
(a)	If p = 1, σ is allowed to be a non-zero constant function. In this case, the feature map φθ
is constant and non-zero with p = 1, which means that (FRK) holds. In the following, we
thus assume that σ is non-constant. Let x(0) 〜PX. Since x(0) is nonatomic and σ is non-
constant, an inductive application of Lemma H.11 yields that almost surely over θ, x(L-1)
is also nonatomic. Hence, x(1L-1), . . . , x(pL-1) are independent and nonatomic almost
surely over θ. But by Lemma H.15, the elementwise application of σ is non-degenerate,
and hence by Lemma H.14, we almost surely have
∕σ(W (LT)XILT) )>∖
det	.	I = 0 ,
σ(W (L-1)x(pL-1))>
which implies that (FRK) holds for n = p almost surely over θ.
(b)	If p = 1 and σ is a polynomial of degree p - 1 = 0, this means that σ is a non-zero constant
function. Like in case (a), this implies that φθ is constant and non-zero with p = 1, which
means that (FRK) holds. In the following, we thus assume again that σ is non-constant, i.e.
nota polynomial of degree less than max{1, p- 1}. Since the distribution of the xi(0) := xi
is non-atomic, they are distinct almost surely. By Theorem H.9, x(1L), . . . , x(pL) are linearly
independent almost surely over θ, which proves (FRK) for n = P almost surely over θ. □
Remark H.16 (Generalizations). The proof technique used in Theorem 10 is quite robust and can be
further generalized. For example, it is easy to incorporate different activation functions for different
neurons, and in all layers but the last layer, the activation functions only need to be analytic and
non-constant, as required by the corresponding propagation lemmas. It is also possible to treat fixed
but nonzero biases using a combination of Lemma H.11 (b) and using shifted activation functions
σi(x) = σ(x + bi) in Lemma H.15. Also, the propagation lemmas, which are used for all layers
except the last one, can be easily extended to DenseNet-like structures where the input of a layer is
concatenated to the output.	J
Remark H.17 (Necessity of the assumptions). For analytic σ, the assumptions on not being a too
simple polynomial in Theorem 10 are necessary. For this, consider the case with L = 1 layer and
d= 1.
(a)	Assume that σ is a polynomial with less than p nonzero coefficients, i.e. σ(x) =
Pk∈K akxk for |K| ≤ p - 1. For arbitrary weights w ∈ Rp×1 and data points
x1, . . . , xp ∈ R, we obtain the feature matrix φw(X) ∈ Rp×p with
φw (X)ij = σ(wjxi) =	akwj xi ,
k∈K
which means that φw(X) is the sum of the |K| ≤ p - 1 matrices (akwjkxik)i,j∈[p], which
have at most rank 1. Hence, φw(X) has at most rank p - 1 and is therefore not invertible.
(b)	Assume that σ is a polynomial of degree less thanp- 1, i.e. σ = Ppk-=20 akxk. For arbitrary
weights w ∈ Rp×1, biases b ∈ Rp and data points x1, . . . , xp ∈ R, we obtain the feature
matrix φw,b(X) ∈ Rp×p with
p-2
φw,b (X)ij = σ(wj xi + bj ) =	ak (wj xi + bj)
k=0
p-2 k k
= XX ak kl bjk-lwjlxli
k=0 l=0
=pXl=-02pXk-=2lakklbjk-lwjl!xli
38
Published as a conference paper at ICLR 2021
p-2
= X u(jl)xli ,
l=0
where ujl (Pp=2 ak (kl IbjTwj does not depend on i. Hence, φw,b(X) is the sum of the
p - 1 matrices (u(jl)xli)i,j∈[p], each of which has rank at most 1. Hence, φw,b(X) has at
most rank p - 1 and is therefore not invertible.	J
I Random Fourier Features
In a celebrated paper, Rahimi & Recht (2008) propose to approximate a shift-invariant positive
definite kernel k(x, x0) = k(x-x0) with a potentially infinite-dimensional feature map by a random
finite-dimensional feature map, yielding so-called random Fourier features. If k is (up to scaling)
the Fourier transform ofa probability distribution Pk on Rd, two versions of random Fourier features
are proposed:
(1)	One version uses φw,b(x) = √2cos(Wx + b), where the rows of W ∈ Rp×d are
independently sampled from Pk and the entries of b ∈ Rp are independently sampled from
the uniform distribution on [0,2π]. This feature map is covered by Theorem 10 and hence,
if Pk has a Lebesgue density and x is nonatomic, (FRK) is satisfied for all n. For example,
ifk is a Gaussian kernel, Pk is a Gaussian distribution and therefore has a Lebesgue density.
(2)	The other version uses
φW (x)	=	csions((WW xx))
with the same distribution over W. Itis not covered by Theorem 10 because of the different
“activation functions” and the “weight sharing” between these activation functions. In the
following proposition, we show that the proof of Theorem 10 can be adjusted to this setting
and the conclusions still hold.
Proposition I.1. For x ∈ Rd, W ∈ Rq×d and p := 2q, define
φW (x) := sin(Wx) ∈ Rp .
W	cos(W x)
If W has a Lebesgue density and x is nonatomic, then (FRK) holds for all n almost surely over W.
Proof. Step 1: Reduction. According to Proposition 6, it suffices to consider the case n = p. By
Lemma H.14, it is then sufficient to prove that the function
f : Rq → R2q, x 7→ (sin(x), cos(x))
is non-degenerate in the sense of Definition H.13.
Step 2: Condition on the xi. We will proceed similar to Lemma H.15. Let x1, . . . , xp be indepen-
dent scalar nonatomic random variables. For i ∈ [q], choose ki := 2i - 1 and kq+i := 2i - 2. Then,
k1, . . . , kp are distinct non-negative integers, and by Lemma H.12 and the Leibniz formula for the
determinant, we have
∕xj1
Dχ := X Sgn⑺Xn(I) ∙... ∙Xnpp) = det .
π∈Sp	x1kp
with probability one.
Step 3: Non-degeneracy. Now, suppose that we are indeed in the case where Dx 6= 0. Take the
power series of sin and cos as
∞
=0
X2k+1
(2k +1)!
∞
:	ak Xk
k=0
39
Published as a conference paper at ICLR 2021
∞	2k
Cos(X) = X(T)k w
∞
: X bkxk .
k=0
Similar to the proof of Lemma H.15, we can compute
f (wx1)>
g(w) := det . I
f(wxp)>
=XX Sgn⑺akl …akqbkq + 1 …bk2q ^1+a + 1 …Wkqqq xΜl)…x^q)
k1,...,k2q≥0 π∈S2q
=X ak1 …akqbkq + 1 …bk2q ( X …x^) ^ W^^ ^^ WbM g
k1,...,k2q ≥0	π∈S2q
Define the set K := {(k1, . . . , k2q ) ∈ N20q | for all i ∈ [q], ki + kq+i = 2i - 1}. Then, the coefficient
C of the monomial w1w3 ∙ ... ∙ WqqT in (13) can be written as
c :=	ck1,...,k2q ,
(k1 ,...,k2q)∈K
ck1,...,k2q
:= ak1 …akqbkq+1
Now, consider (k1, . . . , k2q) ∈ K. Note that ak 6= 0 iff k is odd and bk 6= 0 iffk is even. For the
choice ki := 2i - 1, kq+i := 2i -2 for i ∈ [q] from Step 2, we have (k1, . . . , k2q) ∈ K and
ck1,...,k2q = ak1 …akq bkq + 1 ∙∙∙ bk2q Dx * 0 ∙
In the following, we will show that ck1,...,k2q = 0 for all other (k1, . . . , k2q) ∈ K, which implies
c 6= 0 and therefore yields that g is not the zero function, which is what we want to show. If ki = kj
for some i 6= j , we have
xk1	xk1
x1	. . . xp
X xΠ(l)…xΠ(2q) = det .	...	. I =0 ,
π∈S2q	x1kp . . . xkpp
since the i-th and j-th rows of the matrix are equal, and hence ck1,...,k2q = 0. Now, suppose
that (k1, . . . , k2q) ∈ K with ck1,...,k2q 6= 0. By induction, it is easy to show that {ki, kq+i} =
{2i - 1, 2i - 2} for all i ∈ [q]. But since
ak1 ∙∙∙ akqbkq + 1 ∙∙∙ bk2q * 0
and ak = 0 for even k, we need to have ki = 2i - 1, kq+i = 2i - 2 for all i ∈ [q]. This shows the
claim.	□
J Proofs for Section 6
In this section, we first prove the analytic formulas from Section 6 before discussing the case of low
input dimension d.
Theorem 11.	Let PZ = U (Sp-1). Then, PZ satisfies the assumptions (MOM), (COV) and (FRK)
for all n with Σ = PIp. Moreover, for n ≥ P = 1 or P ≥ n ≥ 1, we can compute
EZ tr((Z+)>ΣZ+)
n
Z -
n 1
1-l- pL
2
-P
p
ifn ≥ p = 1,
ifp ≥ n = 1,
if 2 ≤ n ≤ p ≤ n + 1,
if 2 ≤ n ≤ n + 2 ≤ p.
40
Published as a conference paper at ICLR 2021
Proof. Step 1: Verify (MOM). Let Xi 〜N(0, IP) for i ∈ [n] be independent. Then, Zi :
Kk 〜U(SpT). since Ekzik2 = E1 = 1, (MOM) is satisfied and thus, Σ is well-defined.
Step 2: Compute Σ. We can use rotational invariance as follows: Let V ∈ Rp×p be an arbitrary
fixed orthogonal matrix. Then, Vxi 〜 N(0, VV>) = N(0, Ip) and hence Vzi = -VX^ =
kxi k2
kVXik2 〜U(SpT). Therefore,
Σ = Ezizi> = EV zizi> V> = VΣV>
(14)
If 0 6= v ∈ Rp is an eigenvector of Σ with eigenvalue λ, then Vv must by Eq. (14) also be an
eigenvector of Σ with eigenvalue λ. But since V is an arbitrary orthogonal matrix, this means that
Vv is an arbitrary rotation of v . From this it is easy to conclude that Σ = λIp, and from
pλ = tr(Σ) = Etr(zizi>) = Ezi> zi = E1 = 1 ,
it follows that Σ = PIp. Hence, (COV) is satisfied and Wi = √pzi.
Step 3: Verify (FRK) for all n. By Proposition 6, it is sufficient to verify (FRK) for n = p.
Therefore, let n = p. It is obvious from Proposition 6 with φ = id that N(0, Ip) satisfies (FRK).
Hence, X almost surely has full rank. But then, since kxi k2 > 0 almost surely,
Z=diag (k⅛ '…，击)X
almost surely has full rank as well, which proves (FRK).
Step 4.1: Computation for n ≥ p = 1. In the underparameterized case n ≥ p = 1, we can
compute
EZ tr((Z +)>ΣZ+) = EZ tr((W> W)-1) = EZ J	2 = EZ1 = 1 .
i=1 wi2	n n
Step 4.2: Computation forp ≥ n = 1. In the overparameterized case p ≥ n = 1, we can compute
EZ tr((Z +)>ΣZ+) = EZ tr((WW >)-1) = EZ —— = EZ1 = 1 ,
w1>w1	p p
where We used that since Σ = PIp, w>wι = ∣∣wι∣∣2 = k√Pzιk2 = p.
Step 4.3: Computation for P ≥ n ≥ 2. Now, let P ≥ n ≥ 2. Since Σ = PIp, We have
EZ tr((Z+)>ΣZ+) = EZ tr((W W >)-1)
by Theorem 3. Using that the wi are i.i.d., we obtain from Lemma G.2 that E((WW>)-1) =
nE dist(w1, W-1)-2, where W-1 is the space spanned by w2, . . . , wn. Define the subspace Un :=
{z ∈ Rp | zn = zn+1 = . . . = zp = 0}. By (FRK), we almost surely have dim(W-1) = n - 1.
Thus, there is an orthogonal matrix U -1 depending only on W-1 that rotates W-1 to Un:
Un = U-1W-1 .
Because w1 is stochastically independent from W-1 and U-1 and its distribution is rotationally
symmetric, we have the distributional equivalence (using the zi and xi from Step 1)
dist(w1, W-1)2 = dist(U -1w1, U-1W-1)2 dis=trib. dist(w1, Un)2 = p(z12,n + . . . + z12,p)
x21,n + . . . + x12,p	A
=p -M- = pA+B ,
where A := x21,n+x12,n+1 + . . . + x21,p has a χ2p+1-n distribution andB := x12,1 +. . . + x12,n-1
has a χ2n-1 distribution. Hence,
E((WW>)-1) = nEdist(wι, W-1)-2 = -(1+ EA).
41
Published as a conference paper at ICLR 2021
Since P ≥ n ≥ 2, n - 1 and P + 1 - n are positive. Since A and B are independent, AB(P+「：)
follows a Variance-Ratio F -distribution with parameters n - 1 andp+ 1 - n, whose mean is known
(see e.g. Chapter 20 in Forbes et al., 2011):
B n — 1 E B / (n — 1)	I ∞	,	if P	+ 1 — n ≤	2,
A	P + 1 - n A/(P + 1 - n)	( p+i-n (p++-nh	= p-1-n	,	if P	+ 1 一 n>	2'
(15)
The infinite expectation for P + 1 - n ≤ 2 is not explicitly specified in Forbes et al. (2011), but it
is easy to obtain from the p.d.f. of the F -distribution: The p.d.f. f(x) of the F(a, b)-distribution for
x ≥ 0 is
x(a-2)/2
f (X) = Ca，b (1 + (a∕b)x)(α+b)∕2 = θ(x-b∕2T)	(X → ∞)
for some constant Ca,b (cf. Chapter 20 in Forbes et al., 2011), and the expected value is therefore
∞
0
Xf (X) dX
Θ(X-b∕2) dX
0
which is infinite for P+1-n = b ≤ 2. Forn ∈ {P, P-1}, we therefore obtain E((WW>)-i) = ∞.
For n ≤ P - 2, we compute
E((WW >)-1) = n(1+ n - 1 = = n ∙ P-2
P P-1 -n P P-1 -n
□
In the following, we will prove Theorem 12 using the same proof idea as for Theorem 11. The
formulas in Theorem 12 have in principle already been computed by Breiman & Freedman (1983)
for P ≤ n - 2 and by Belkin et al. (2019b) for general P. However, our proof circumvents a
technical problem in the proof of Belkin et al. (2019b): Consider for example the case P ≤ n. Belkin
et al. (2019b) mention that (W> W)-i has an inverse Wishart distribution, which for P ≤ n - 2
has expectation 冗-二Ip, and then use Etr((W>W)-1) = tr(E(W>W)-1). However, for
P ≥ n - 1, the latter expectation is not specified in common literature7 on the inverse Wishart
distribution (Mardia et al., 1979; Press, 2005; von Rosen, 1988), presumably because it is ∞ for
diagonal elements but is not well-defined for off-diagonal matrix elements.
Theorem 12.	Let PZ = N (0, Ip). Then, PZ satisfies the assumptions (MOM), (COV) and (FRK)
for all n with Σ = Ip. Moreover, for n,P ≥ 1,
{-n-
p-1-n
∞
n— 1—p
if P ≥ n + 2,
ifP ∈ {n - 1,n,n + 1},
if P ≤ n - 2.
Proof. Step 1: Assumptions. Verifying (MOM), (COV) and Σ = Ip is trivial and (FRK) for all n
follows from Proposition 6 with x = z and φ = id.
Step 2: Overparameterized case. For the expectation, we first follow Step 4.3 in the proof of
Theorem 11 in the overparameterized case P ≥ n ≥ 1, the main difference being that instead of
Wi = √p k Xik , We now have Wi = Xi, which translates to the simpler equation
dist(w1, W-1)2 dis=trib. A
with A 〜χp+ι-n. Let B 〜X be independent of A, then we can compute similar to Eq.(15)
Etr((WW>)-1) = nEdist(w1, W-I)-2 = nEɪ = n (EB) (Eɪ ) = nEB
A	AA
n	B/1
-------E------------
P + 1 - n A/(P + 1 - n)
(
∞
n______p+1-n
p+1-n (p+1-n)-2
n
p-1-n
if P + 1 - n ≤ 2,
ifP+ 1 -n > 2.
7Belkin et al. (2019b) do not cite any source on the inverse Wishart distribution.
42
Published as a conference paper at ICLR 2021
This proves the over-parameterized case.
Step 3: Underparameterized case. Since the rows wi of W ∈ Rn×p are independent and follow a
N(0, Ip) distribution, the rows of W> ∈ Rp×n are independent and follow aN(0, In) distribution.
Therefore, the underparameterized case p ≤ n follows from the overparameterized case n ≤ p by
switching the roles of n and p.	□
Remark J.1. An alternative (and presumably similar) way to prove Theorem 12 is to use that the
diagonal elements of a matrix with an inverse Wishart distribution follow an inverse Gamma distri-
bution as specified in Example 5.2.2 in Press (2005).	J
The next proposition shows that a small input dimension d does not necessarily provide a limitation:
Proposition J.2. Let p, d ≥ 1. Then, there exists a probability distribution PX on Rd (with bounded
support) and a Continuousfeature map φ : Rd → Rp such thatfor X 〜Pχ, φ(x)〜U(SpT).
Proof. Forp = 1, the result is trivial, we will therefore assume p ≥ 2. We will prove the result for
any d by a reduction to the case d = 1, although substantially simpler constructions are possible for
d ≥ p - 1. First, introduce the spaces
Sp+-1 := {z ∈ Sp-1 | zp ≥ 0} ⊆ Rp
Bp-1 := {z ∈ Rp-1 | kzk2 ≤ 1}
X := [0, 3] × {0}p-1 ⊆ Rp .
Step 1: Space-filling curve on the sphere. In this step, we show that there exists a continuous
surjective map φ : X → Sp-1. First of all, let f1 : [0, 1] → [0, 1]p-1 be continuous and surjective,
e.g. a Hilbert or Peano curve (see e.g. Sagan, 2012). We define the following maps:
0 if u = 0
f2 ：[0，1]p-1 → Bp-1，u→ 1⅛ Uif U = 0 ,
f3 ： Bp-1 → s+-1, V → 卜,qι-kvk2).
It is not hard to verify that f2 and f3 are continuous and surjective as well. For example, f2 is
continuous in 0 since kUk∞ ≤ kUk2 for all U ∈ Rp-1. Thus, the map f := f3 ◦ f2 ◦ f1 : [0, 1] →
Sp+-1 is continuous and surjective. Define the map
τ : Rp → Rp, z 7→ (z1, . . . , zp-1, -zp) .
By the previous considerations, it is not hard to verify that the map
(f (x)	if X ∈ [0,1]
g : [0, 3] → Sp-1,x→ < χ⅛f (1)+(1- χ⅛) T(f (1)) if X ∈ [1, 2]
IT(f(3 - x))	if x ∈ [2, 3]
is continuous and surjective as well. We can therefore define the continuous and surjective map
φ : X → Sp-1, x 7→ g(X1).
Step 2: Existence ofa pull-back measure. We consider the Borel σ-algebras B(X), B(Sp-1) on X
and Sp-1. The uniform distribution PZ = U(Sp-1) on the sphere is defined with respect to B(Sp-1)
and is therefore a Borel measure. Since φ is continuous, it is Borel measurable. Moreover, since
X and Sp-1 are complete separable metric spaces, they are also Souslin spaces, cf. Section 6.6 in
Bogachev (2007). Since φ is surjective, Theorem 9.1.5 in Bogachev (2007) guarantees the existence
of a measure PX such that if X 〜Pχ, then φ(x)〜U(Sp-1). Since PX (X) = PZ (Sp-1) = 1, PX
is a probability measure.
Step 3: Continuation. We can arbitrarily extend the mapping φ : X → Sp-1 to a continuous
mapping φ : Rd → Rp. Moreover, the domain X of PX can be extended to Rd via PX (A) :=
Pχ(A ∩ X), the support of Pχ is still bounded, and we still have φ(x)〜U(Sp-1) if X 〜Pχ. □
43
Published as a conference paper at ICLR 2021
Remark J.3. The proof of Proposition J.2 could be slightly shorter if We required φ(x) ~ U (S+-1)
instead of φ(x) 〜 U(SpT). This would be of similar interest since the uniform distribution
U(Sp+-1) on the “half-sphere” leads to the same EZ tr((Z+ )>Σ(Z+ )) as the uniform distribution
U (SpT) on the full sphere: If Zi ~ U (S-) and εi ~U ({-1,1}) are stochastically independent,
then 处：=εiZi ~ U(SpT). Therefore, Σ = Σ, Z = diag©,..., εn)Z, and if UDV> is a SVD
of Z, then (diag(ει,..., εn)U)DV> is a SVD of Z. Therefore, Z and Z have the same singular
values, hence W and W have the same singular values, hence tr((WW>)-1) = tr((WW )-1)
>
forP ≥ n and tr((W 1 W)-1) = tr((W W)-1) forP ≤ n.	J
Remark J.4. One might ask whether it is possible in Proposition J.2 to choose PX as a “nice”
distribution, like a uniform distribution on a cube or a Gaussian distribution. The answer to this
question is affirmative if there exists an area-preserving space-filling curve φ ： [0, volume(Sp-1)] →
Sp-1. For P = 3, such a construction is informally described by Purser et al. (2009) and it seems
plausible that such a construction is possible for all P.	J
K Relation to Ridgeless Kernel Regression
In this section, we want to discuss the relation between this paper and recent work on ridgeless
kernel regression. To this end, we need to introduce some terminology on representations of kernels
with finite-dimensional feature maps.
Definition K.1. Let k ： Rd × Rd → R be a kernel, let P be an integer with 1 ≤ P < ∞ and let
φ ： Rd → Rp be a (measurable) function. Then, (P, φ) is called a PX -representation of k if
•	k(x, x) = φ(x)>φ(x) almost surely for independent x, X 〜 PX, and
•	k(x, x) = φ(x)>φ(x) almost surely for X 〜PX.
If k has a PX -representation, then we define
Pk ：=	min	P ,
(p,φ) is a PX -representation of k
i	.e. Pk is the smallest P for which a PX -representation exists.	J
Usually, Pk corresponds to the dimension of the RKHS associated with the restriction of k to the
support of PX , but since the feature map φ only needs to represent the kernel PX -almost surely, Pk
may be smaller for pathological kernels. The following lemma states that Theorem 3, if applicable,
should be applied to ridgeless kernel regression with P = Pk :
Lemma K.2. Let k be a kernel on Rd with PX (k(x, x) 6= 0) > 0. Let (P, φ) be a PX -representation
of k.
(a)	Then, (COV) in Theorem 3 is satisfied iffP = Pk.
(b)	The assumptions of Theorem 3 are satisfied for a PX -representation (Pk, φ) of k if and only
if they are satisfied for all such representations.
(c)	If the assumptions of Theorem 3 are satisfied for any PX -representation of k, then the lower
bound from Theorem 3 also holds for ridgeless kernel regression with P = Pk.
Proof. In the notation of Section 3, the definition of PX -representation implies that k(X, X) =
φ(X)φ(X)> and k(x, X) = φ(x)>φ(X)> almost surely.
(a)	If (COV) is not satisfied and P ≥ 2, it is possible to construct a PX -representation with
smaller P using the construction from Remark 7, hence P > Pk. IfP = 1, (COV) is satisfied
due to the assumption on k.
Conversely, assume p > Pk and let (Pk ,φ) be another PX -representation of k. Set n = p.
Then, we almost surely have
rank φ(X) = rank(φ(X)φ(X)>) = rank(k(X, X)) = rank(φ(X)φ(X)>) ≤ Pk
<P,
hence φ(X) has full rank with probability zero. Since the rows φ(xi) of φ(X) are i.i.d.,
this means that there must be a proper linear subspace U of Rp such that φ(xi) ∈ U with
probability one. But then, according to Proposition 6, (COV) is not satisfied.
44
Published as a conference paper at ICLR 2021
∙-v
(b)	Let (pk, φ) and (pk, φ) be two PX -representations of k such that z = φ(x) satisfies the
assumptions of Theorem 3. We need to show that W = φ(x) also satisfies the assumptions
of Theorem 3: First of all, (INT) and (NOI) hold since they are independent of the feature
map. Moreover, (COV) holds by (a). We find that (MOM) holds due to
EkzWk22 = EφW(x)> φW(x) = Ek(x, x) = Eφ(x)>φ(x) = Ekzk22 < ∞
and (FRK) holds since, almost surely,
rank φW(X) = rank(φW(X)φW(X)>) = rank(k(X, X)) = rank(φ(X)φ(X)>)
= rank φ(X) = min{n, p} .
(c)	Assume that there exists a PX -representation (p, φ) of k that satisfies the assumptions
of Theorem 3. By (a), we have p = pk . By Eq. (2) in Section 3, the ridgeless kernel
regression estimator and the linear regression estimator with the feature map φ are almost
surely equivalent, hence they have the same ENoise.	□
For kernels that cannot be represented with a finite-dimensional feature space, Theorem 3 cannot be
applied. In fact, any distribution-independent lower bound for ridgeless kernel regression must be
zero in this case: For example, the Kronecker delta kernel given by
k(x,xW) = 10
if x = xW
otherwise
yields ENoise = 0 for any nonatomic input distribution PX. Of course, this kernel is not well-suited
for learning since the learned functions are zero almost everywhere. However, there exist results
for ridgeless kernel regression with specific classes of kernels. For example, Rakhlin & Zhai (2019)
show that in certain settings, ridgeless kernel regression with Laplace kernels is inconsistent because
ENoise = Ω(1) as n → ∞. Note that Laplace kernels in general do not allow for finite-dimensional
feature map representations.
Liang & Rakhlin (2020) derive upper bounds for a certain class of kernels and input distributions
with (linearly transformed) i.i.d. components. Their analysis focuses on the high-dimensional limit
d, n → ∞ with 0 < c ≤ d/n ≤ C < ∞ and ignores the “effective dimension” pk of the feature
space. It appears that their analysis is not impacted by Double Descent w.r.t. pk since their assump-
tions on the kernel imply either pk = ∞ or pk/n → ∞ as n, d → ∞: In particular, they consider
kernels of the form
k(x, xW)
h (d hx, Xi
for a suitable smooth function h that is independent of d. Due to the factor 1 and the limit d → ∞,
the kernel behaves essentially like a quadratic kernel
1	12
k(x, x) ≈ ao + aι行〈x, X〉+ a2(不〈x, X〉I =： kquad(x, x),
where the curvature a2 should be positive in order to obtain good upper bounds on ENoise (the
variance term). For a2 , a1, a0 > 0, it is possible to represent this quadratic kernel with a feature
map analogous to that of the polynomial kernel in Proposition 8 with feature space dimension p =
1 + d + d(d+1). An argument similar to the proof of Proposition 8 shows that this feature map
satisfies FRK(p) if x has a Lebesgue density, hence (COV) is satisfied. By Lemma K.2 (a), we have
pkquad = p = Θ(d2) = Θ(n2), which shows pkquad /n → ∞ as n, d → ∞.
Liang et al. (2019) consider a similar setting with d 〜nɑ, α ∈ (0,1), and find that ENoise converges
to zero under suitable assumptions as d, n → ∞. Again, it appears that their assumptions on
the kernel imply at least a strongly overparameterized regime with pk = ∞ or pk/n → ∞ for
d, n → ∞, where our lower bound is vacuous.
L Novelty of the Overparameterized Bound
In their Corollary 1, Muthukumar et al. (2020) provide a lower bound in the case p ≥ n holding
with high probability for ε>(WW>)-1 ε, where ε 〜 N(0, IP) is a noise vector independent of
45
Published as a conference paper at ICLR 2021
W. Since Eε>(WW>)-1ε = EZ tr((WW>)-1Eεεε>) = EZ tr((WW>)-1), their lower
bound yields a lower bound for Etr((WW>)-1). However, the resulting lower bound is weaker
than ours and requires stronger assumptions:
(1)	Assuming that the subgaussian norm kwikψ2 := supv∈Sp-1 supq∈N+ q-1/2(E|v>wi|q)1/q
(cf. Vershynin, 2010) is bounded by a constant K < ∞, they obtain a lower bound of the
form CKσ2 P with a constant CK > 0 that depends on K and is only explicitly specified
for the case of centered Gaussian PZ . They note that kwi kψ2 ≤ K holds, for example,
if the components wi,j of wi are independent and all satisfy kwi,j kψ2 ≤ K. However,
as discussed in Remark 1, such independence assumptions are not realistic. In contrast,
our lower bound is explicit, independent of constants like K and is larger: For example, at
n = p, our lower bound is σ2n and theirs is σ2CK.
(2)	Assuming ∣∣Wik2 ≤ P almost surely, they obtain a lower bound of the form cσ2P惹⑺.
First of all, this lower bound converges to zero as n = p → ∞. Moreover, since we always
have Ekwik22 = E tr(wiwi>) = Etr(Ip) = p, the assumption implies kwik22 = p almost
surely. Although we can sometimes guarantee constant kzi k22, e.g. for certain random
Fourier features, We cannot guarantee the same for Wi = Σ-"Zi since Σ depends on the
unknown input distribution PX .
By inspecting the proof behind (1), one finds that CK → 0 as K → ∞. Hence, lower bound of
Muthukumar et al. (2020) might raise hope that it is possible to achieve low ENoise by choosing
features with a large (or even infinite) subgaussian norm. Our result shows that this is not possible:
Essentially the only possibility to avoid a large ENoise for ridgeless linear regression around n ≈ p is
to violate the property (FRK) that guarantees the ability to interpolate the data in the overparameter-
ized case, see Section 5. Otherwise, in order to achieve ENoise < εσ2, ε 1, it is necessary to make
the model either strongly underparameterized (P < εn) or strongly overparameterized (p > n∕ε).
46