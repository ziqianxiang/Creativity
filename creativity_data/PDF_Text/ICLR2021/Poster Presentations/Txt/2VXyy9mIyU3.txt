Published as a conference paper at ICLR 2021
Learning with Instance-Dependent Label
Noise: A Sample Sieve Approach
Hao Cheng* §, ZhaoWei Zhu*t, Xingyu Lit, Yifei Gong§, Xing Sun§, and Yang Liut
^University of California, Santa Cruz, § Tencent YouTu Lab
{zwzhu,xli279,yangliu}@ucsc.edu,
{louischeng,yifeigong,winfredsun}@tencent.com
Ab stract
Human-annotated labels are often prone to noise, and the presence of such noise
will degrade the performance of the resulting deep neural network (DNN) mod-
els. Much of the literature (with several recent exceptions) of learning with noisy
labels focuses on the case when the label noise is independent of features. Practi-
cally, annotations errors tend to be instance-dependent and often depend on the
difficulty levels of recognizing a certain task. Applying existing results from
instance-independent settings would require a significant amount of estimation
of noise rates. Therefore, providing theoretically rigorous solutions for learning
with instance-dependent label noise remains a challenge. In this paper, we propose
CORES2 (COnfidence REgularized Sample Sieve), which progressively sieves out
corrupted examples. The implementation of CORES2 does not require specifying
noise rates and yet we are able to provide theoretical guarantees of CORES2 in
filtering out the corrupted examples. This high-quality sample sieve allows us to
treat clean examples and the corrupted ones separately in training a DNN solu-
tion, and such a separation is shown to be advantageous in the instance-dependent
noise setting. We demonstrate the performance of CORES2 on CIFAR10 and CI-
FAR100 datasets with synthetic instance-dependent label noise and Clothing1M
with real-world human noise. As of independent interests, our sample sieve pro-
vides a generic machinery for anatomizing noisy datasets and provides a flexi-
ble interface for various robust training techniques to further improve the perfor-
mance. Code is available at https://github.com/UCSC-REAL/cores.
1	Introduction
Deep neural networks (DNNs) have gained popularity in a wide range of applications. The re-
markable success of DNNs often relies on the availability of large-scale datasets. However, data
annotation inevitably introduces label noise, and it is extremely expensive and time-consuming to
clean up the corrupted labels. The existence of label noise can weaken the true correlation between
features and labels as well as introducing artificial correlation patterns. Thus, mitigating the effects
of noisy labels becomes a critical issue that needs careful treatment.
It is challenging to avoid overfitting to noisy labels, especially when the noise depends on both true
labels Y and features X . Unfortunately, this often tends to be the case where human annotations
are prone to different levels of errors for tasks with varying difficulty levels. Recent work has
also shown that the presence of instance-dependent noisy labels imposes additional challenges and
cautions to training in this scenario (Liu, 2021). For such instance-dependent (or feature-dependent,
instance-based) label noise settings, theory-supported works usually focus on loss-correction which
requires estimating noise rates (Xia et al., 2020; Berthon et al., 2020). Recent work by Cheng et al.
(2020) addresses the bounded instance-based noise by first learning the noisy distribution and then
distilling examples according to some thresholds.* 1 However, with a limited size of datasets, learning
an accurate noisy distribution for each example is a non-trivial task. Additionally, the size and the
quality of distilled examples are sensitive to the thresholds for distillation.
* Equal contributions in alphabetical ordering. Hao leads experiments and Zhaowei leads theories.
^Corresponding authors: Y. Liu and Z. Zhu {yangliu,zwzhu}@ucsc.edu.
1The proposed solution is primarily studied for the binary case in Cheng et al. (2020).
1
Published as a conference paper at ICLR 2021
Departing from the above line of works, we design a sample sieve with theoretical guarantees to
provide a high-quality splitting of clean and corrupted examples without the need to estimate noise
rates. Instead of learning the noisy distributions or noise rates, we focus on learning the underlying
clean distribution and design a regularization term to help improve the confidence of the learned
classifier, which is proven to help safely sieve out corrupted examples. With the division between
“clean” and “corrupted” examples, our training enjoys performance improvements by treating the
clean examples (using standard loss) and the corrupted ones (using an unsupervised consistency
loss) separately.
We summarize our main contributions: 1) We propose to train a classifier using a novel confidence
regularization (CR) term and theoretically guarantee that, under mild assumptions, minimizing the
confidence regularized cross-entropy (CE) loss on the instance-based noisy distribution is equiva-
lent to minimizing the pure CE loss on the corresponding “unobservable” clean distribution. This
classifier is also shown to be helpful for evaluating each example to build our sample sieve.2) We
provide a theoretically sound sample sieve that simply compares the example’s regularized loss with
a closed-form threshold explicitly determined by predictions from the above trained model using
our confidence regularized loss, without any extra estimates. 3) To the best of our knowledge, the
proposed CORES2 (COnfidence REgularized Sample Sieve) is the first method that is thoroughly
studied for a multi-class classification problem, has theoretical guarantees to avoid overfitting to
instance-dependent label noise, and provides high-quality division without knowing or estimating
noise rates. 4) By decoupling the regularized loss into separate additive terms, we also provide a
novel and promising mechanism for understanding and controlling the effects of general instance-
dependent label noise. 5) CORES2 achieves competitive performance on multiple datasets, includ-
ing CIFAR-10, CIFAR-100, and Clothing1M, under different label noise settings.
Other related works In addition to recent works by Xia et al. (2020), Berthon et al. (2020), and
Cheng et al. (2020), we briefly overview other most relevant references. Detailed related work is
left to Appendix A. Making the loss function robust to label noise is important for building a robust
machine learning model (Zhang et al., 2016). One popular direction is to perform loss correction,
which first estimates transition matrix (Patrini et al., 2017; Vahdat, 2017; Xiao et al., 2015; Zhu
et al., 2021b; Yao et al., 2020b), and then performs correction/reweighting via forward or back-
ward propagation, or further revises the estimated transition matrix with controllable variations (Xia
et al., 2019). The other line of work focuses on designing specific losses without estimating transi-
tion matrices (Natarajan et al., 2013; Xu et al., 2019; Liu & Guo, 2020; Wei & Liu, 2021). However,
these works assume the label noise is instance-independent which limits their extension. Another
approach is sample selection (Jiang et al., 2017; Han et al., 2018; Yu et al., 2019; Northcutt et al.,
2019; Yao et al., 2020a; Wei et al., 2020; Zhang et al., 2020a), which selects the “small loss” ex-
amples as clean ones. However, we find this approach only works well on the instance-independent
label noise. Approaches such as label correction (Veit et al., 2017; Li et al., 2017; Han et al., 2019) or
semi-supervised learning (Li et al., 2020; Nguyen et al., 2019) also lack guarantees for the instance-
based label noise.
2	CORES2: COnfidence REgularized S ample S ieve
Consider a classification problem on a set of N training examples denoted by D :=
{(xn, yn)}n∈[N], Where [N] := {1,2,…，N} is the set of example indices. Examples (xn, yn)
are drawn according to random variables (X, Y ) ∈ X × Y from a joint distribution D. Let DX and
DY be the marginal distributions of X and Y . The classification task aims to identify a classifier
f : X → Y that maps X to Y accurately. One common approach is minimizing the empirical risk
using DNNs With respect to the cross-entropy loss defined as `(f (x), y) = - ln(fx [y]), y ∈ [K],
Where fx[y] denotes the y-th component of f (x) and K is the number of classes. In real-World appli-
cations, such as human-annotated images (Krizhevsky et al., 2012; Zhang et al., 2017) and medical
diagnosis (AgarWal et al., 2016), the learner can only observe a set of noisy labels. For instance,
human annotators may Wrongly label some images containing cats as ones that contain dogs acciden-
tally or irresponsibly. The label noise of each instance is characterized by a noise transition matrix
T (X), Where each element Tij(X) := P(Ye = j|Y = i, X). The corresponding noisy dataset2 and
distribution are denoted by D := {(xn, yn)}n∈[N] and D. Let 1(∙) be the indicator function taking
2In this paper, the noisy dataset refers to a dataset With noisy examples. A noisy example is either a clean
example (Whose label is true) or a corrupted example (Whose label is Wrong).
2
Published as a conference paper at ICLR 2021
value 1 when the specified condition is satisfied and 0 otherwise. Similar to the goals in surrogate
loss (Natarajan et al., 2013), LDMI (Xu et al., 2019) and peer loss (Liu & Guo, 2020), we aim to
learn a classifier f from the noisy distribution D which also minimizes P(f (X) = Y), (X, Y)〜D.
Beyond their results, we attempt to propose a theoretically sound approach addressing a general
instance-based noise regime without knowing or estimating noise rates.
2.1	Confidence Regularization
In this section, we present a new confidence regularizer (CR). Our design of the CR is mainly
motivated by a recently proposed robust loss function called peer loss (Liu & Guo, 2020). For each
example (xn, yn), peer loss has the following form:
'pL(f (Xn),yn) ：= '(f (Xn),yn) - '(f(Xni ),yn2 ),
where (χm, ym) and (x噂, y.) are two randomly sampled and paired peer examples (with replace-
ment) for n. Let Xn1 and Yn2 be the corresponding random variables. Note Xn1 , Yn2 are two
independent and uniform random variables being each χno ,n0 ∈ [N] and yn∕ ,n0 ∈ [N] with prob-
ability N respectively: P(X^ = χno |De) = P(Yn2 = yuo |De) = N,∀n0 ∈ [N]. Let DY@ be the
distribution of Yn2 given dataset D. Peer loss then has the following equivalent form in expectation:
1
NN ∑Eχn1 ,Yn2IDe['(f(xn),yn)-'(f(Xn1),Yn2)]
n∈[N]
=NN X 卜(f(xn ),yn)- X P(Xni = X〃|D )EDγf ['("，〃),Y)]
n∈[N]	n0∈[N]
1
=NN E '(f(Xn ),yn) - EDYf ['(f(Xn),Y)] ∙
n∈[N]
This result characterizes a new loss denoted by 'ca：
'cA(f (xn),yn):= '(f (xn),yn) - EDY f ['(f(Xn),Y)].	(1)
Though not studied rigorously by Liu & Guo (2020), we show, under conditions3, 'ca defined in
Eqn. (1) encourages confident predictions4 from f by analyzing the gradients:
Theorem 1. For 'ca (∙), solutions satisfying fχn [i] > 0, ∀i ∈ [K] are not locally optimal at (xn, yn).
See Appendix B.2 for the proof. Particularly, in binary cases, we have constraint f(xn)[0] +
f (xn)[1] = 1. Following Theorem 1, we know minimizing 'ca(∕(xn), yn) w.r.t f under this con-
straint leads to either f(xn)[0] → 1 or f (xn)[1] → 1, indicating confident predictions. Therefore,
,ι	1 τ , ∙	f .	τm	Γ λ / r / ∖ -vr∖l ι ι	,ι	r,ιι	iff
the addition of term -EDe f ['(f (xn), Y)] helps improve the confidence of the learned classifier.
Inspired by the above observation, we define the following confidence regularizer:
一.	_ _	_	.	~	，	~ ，	、 、	一 _	…，一，、二、r
ConfidenceRegulanzer: 'cr(∕(x„)) := -β ∙ EDYf ['(f (xn),1e)],
where β is positive and '(∙) refers to the CE loss. The prior probability P(Y|D) is counted directly
from the noisy dataset. In the remaining of this paper, '(∙) indicates the CE loss by default.
Why are confident predictions important? Intuitively, when model fits to the label noise, its pre-
dictions often become less confident, since the noise usually corrupts the signal encoded in the clean
data. From this perspective, encouraging confident predictions plays against fitting to label noise.
Compared to instance-independent noise, the difficulties in estimating the instance-dependent noise
rates largely prevent us from applying existing techniques. In addition, as shown by Manwani &
Sastry (2013), the 0-1 loss function is more robust to instance-based noise but hard to optimize
with. To a certain degree, pushing confident predictions results in a differentiable loss function that
approximates the 0-1 loss, and therefore restores the robustness property. Besides, as observed by
Chatterjee (2020) and Zielinski et al. (2020), gradients from similar examples would reinforce each
other. When the overall label information is dominantly informative that Tii (X) > Tij (X), DNNs
3Detailed conditions for Theorem 1 are specified at the end of our main contents.
4Our observation can also help partially explain the robustness property of peer loss (Liu & Guo, 2020).
3
Published as a conference paper at ICLR 2021
will receive more correct information statistically. Encouraging confident predictions would discour-
age the memorization of the noisy examples (makes it hard for noisy labels to reduce the confidence
of predictions), and therefore further facilitate DNNs to learn the (clean) dominant information.
'cr is NOT the entropy regularization Entropy regularization (ER) is a popular choice for im-
proving confidence of the trained classifiers in the literature (Tanaka et al., 2018; Yi & Wu, 2019).
Given a particular prediction probability p for a class, the ER term is based on the function -plnp,
While our 'cr is built on ln p. Later We show 'cr offers Us favorable theoretical guarantees for
training with instance-dependent label noise, while ER does not. In Appendix C.1, we present both
theoretical and experimental evidences that 'cR serves as a better regularizer compared to ER.
2.2 Confidence Regularized Sample Sieve
Intuitively, label noise misleads the training thus sieving corrupted examples out of datasets is ben-
eficial. Furthermore, label noise introduces high variance during training even With the existence of
'cR (discussed in Section 3.3). Therefore, rather than accomplishing training solely With 'cR, We
Will first leverage its regularization poWer to design an efficient sample sieve. Similar to a general
sieving process in physical Words that compares the size of particles With the aperture of a sieve,
We evaluate the “size” (quality, or a regularized loss) of examples and compare them With some
to-be-specified thresholds, therefore the name sample sieve. In our formulation, the regularized loss
'(f (xn), yn) + 'cR(f (Xn)) is employed to evaluate examples and an is used to specify thresholds.
Specifically, we aim to solve the sample sieve problem in (2).
Confidence Regularized Sample Sieve
min X Vn ['(f (Xn),yn) + 'cR(f (Xn)) —。九]
f ∈F ,
v∈{0,1}N n∈[N]
......... . 一 ................~,
s.t. 'cR(f(Xn)) := -β ∙ EDYf'(f(Xn),Y),
an := ɪ X '(jF(Xn),y) + 'cR(∕(Xn)).
K
y∈[κ]
⑵
The crucial components in (2) are:
Figure 1: Dynamic sample sieves. Green circles
are clean examples. Red hexagons are corrupted
examples.
•	vn ∈ {0, 1} indicates whether example n is clean (vn = 1) or not (vn = 0);
•	αn (mimicking the aperture of a sieve) controls which example should be sieved out;
•	f is a copy of f and does not contribute to the back-propagation. F is the search space of f.
Dynamic sample sieve The problem in (2) is a combinatorial optimization which is hard to solve
directly. A standard solution to (2) is to apply alternate search iteratively as follows:
•	Starting at t = 1, 0')= 1, ∀n ∈ [N].
•	Confidence-regularized model update (at iteration-t):
f(t)=argmin X VntT) ['(f(Xn),yn) + 'cR(f(Xn))] ；	(3)
f ∈F n∈[N]	(3)
• Sample sieve (at iteration-t):
Vnt) = l('(f ⑴(Xn),yn)+ 'cR(f ⑴(Xn)) < an,t),	(4)
where a%t =表 Py∈[κ] '(f(t)(xn), y) + 'cR(f(t)(xn)), f(t) and V⑴ refer to the specific
classifier and weight at iteration-t. Note the values of 'CR(f(t')(χn)) and 'CR(f (t)(χn)) are
the same. We keep both terms to be consistent with the objective in Eq. (2). In DNNs, we
usually update model f with one or several epochs of data instead of completely solving (3).
Figure 1 illustrates the dynamic sample sieve, where the size of each example corresponds to the
regularized loss and the aperture of a sieve is determined by αn,t. In each iteration-t, sample sieve-
4
Published as a conference paper at ICLR 2021
3
,X10^l
(e) CE Sieve (Inst, epoch 20)	(f) CE Sieve (Inst., epoch 70)	(g) CORES2 (Inst, epoch 20)
clean
∞rτυpted
-20
^0	∞	40	60^
(d) CORES2 (symm., epoch 70)
3 ,xlθ*t
-20
0	20	40	60
loss
(h) CORES2 (Inst., epoch 70)
Figure 2: Loss distributions of training on CIFAR-10 with 40% symmetric noise (symm.) or 40%
instance-based noise (inst.). The loss is given by '(f ⑴(Xn), yn) + 'cR(f (t)(xn)) - α%t as (4). CE
Sieve represents the dynamic sample sieve with standard cross-entropy loss (without CR).
t “blocks” some corrupted examples by comparing a regularized example loss with a closed-form
threshold α?t, which can be immediately obtained given current model f(t) and example (xn yn)
(no extra estimation needed). In contrast, most sample selection works (Han et al., 2018; Yu et al.,
2019; Wei et al., 2020) focus on controlling the number of the selected examples using an intuitive
function where the overall noise rate may be required, or directly selecting examples by an empiri-
cally set threshold (Zhang & Sabuncu, 2018). Intuitively, the specially designed thresholds αn,t for
each example should be more accurate than a single threshold for the whole dataset. Besides, the
goal of existing works is often to select clean examples while our sample sieve focuses on removing
the corrupted ones. On a high level, we follow a different philosophy from these sample selection
works. We coin our solution as COnfidence REgularized Sample Sieve (CORES2).
More visualizations of the sample sieve In addition to Figure 1, we visualize the superiority of our
sample sieve with numerical results as Figure 2. The sieved dataset is in the form of two clusters of
examples. Particularly, from Figure 2(b) and Figure 2(f), we observe that CE suffers from providing
a good division of clean and corrupted examples due to overfitting in the final stage of training. On
the other hand, with 'cr, there are two distinct clusters and can be separated by the threshold 0 as
shown in Figure 2(d) and Figure 2(h). Comparing Figure 2(a)-2(d) with Figure 2(e)-2(h), we find
the effect of instance-dependent noise on training is indeed different from the symmetric one, where
the instance-dependent noise is more likely to cause overfitting.
3	Theoretical Guarantees of cORES2
In this section, we theoretically show the advantages of cORES2 . The analyses focus on showing
CORES2 guarantees a quality division, i.e. Vn = l(yn = yn), ∀n, with a properly set β. To show
the effectiveness of this solution, we call a model prediction on xn is better than random guess if
fxn [yn] > 1/K, and call it confident if fxn [y] ∈ {0, 1}, ∀y ∈ [K], where yn is the clean label and
y is an arbitrary label. The quality of sieving out corrupted examples is guaranteed in Theorem 2.
Theorem 2. The sample Sieve defined in(4) ensures that clean examples (xn Qn = yn) Will not be
identified as being corrupted if the model f(t) ’s prediction on xn is better than random guess.
Theorem 2 informs us that our sample sieve can progressively and safely filter out corrupted exam-
ples, and therefore improves division quality, when the model prediction on each xn is better than
random guess. The full proof is left to Appendix B.3. In the next section, we provide evidences that
our trained model is guaranteed to achieve this requirement with sufficient examples.
3.1	Decoupling the Confidence Regularized Loss
The discussion of performance guarantees of the sample sieve focuses on a general instance-based
noise transition matrix T(X), which can induce any specific noise regime such as symmetric
noise and asymmetric noise (Kim et al., 2019; Li et al., 2020). Note the feature-independency
was one critical assumption in state-of-the-art theoretically guaranteed noise-resistant literatures
(Natarajan et al., 2013; Liu & Guo, 2020; Xu et al., 2019) while we do not require. Let Tij :=
ED|Y =i [Tij (X)], ∀i, j ∈ [K]. Theorem 3 explicitly shows the contributions of clean examples,
corrupted examples, and 'cr during training. See Appendix B.1 for the proof.
Theorem 3. (Main Theorem: Decoupling the Expected Regularized CE Loss) In expectation, the
loss with 'CR can be decoupled as three separate additive terms:
5
Published as a conference paper at ICLR 2021
Term-1	Term-2
「〜	^l Z	}|	、	/_	}|	{
EDe ['(f (X), Y) + 'CR(f(X))] = T ∙ Ed['(f (X), Y)] + ∆ ∙ Edδ ['(f (X), Y)]
+ X X P(Y = i)Eo∣γ=i[(Uij(X) - βP(Y = j))`(f(X),j)],
j∈[K] i∈[K]
|-------------------------{-------------------------}
Term-3
(5)
where T ：= mi%∈[κ] Tjj, ∆ := £关的 ∆jP(Y = j), ∆j ：= Tjj- T, Uij (X) = Tij(X), ∀i =
j, Ujj(X) = Tjj(X) — Tjj, andEdδ['(f(X),Y)]：=工。> 0) £关的 ZPf=jEd∣y =∙['(f(X),j)].
Equation (5) provides a generic machinery for anatomizing noisy datasets, where we show the ef-
fects of instance-based label noise on the 'cr regularized loss can be decoupled into three additive
terms: Term-1 reflects the expectation ofCE on clean distribution D, Term-2 shifts the clean distri-
bution by changing the prior probability ofY, and Term-3 characterizes how the corrupted examples
(represented by Uij (X)) might mislead/mis-weight the loss, as well as the regularization ability of
'cr (represented by βP(Y = j)). In addition to the design of sample sieve, this additive decou-
pling structure also provides a novel and promising perspective for understanding and controlling
the effects of generic instance-dependent label noise.
3.2	Guarantees of the Sample Sieve
By decoupling the effects of instance-dependent noise into separate additive terms as shown in
Theorem 3, we can further study under what conditions, minimizing the confidence regularized CE
loss on the (instance-dependent) noisy distribution will be equivalent to minimizing the true loss
incurred on the clean distribution, which is exactly encoded by Term-1. In other words, we would
like to understand when Term-2 and Term-3 in (5) can be controlled not to disrupt the minimization
of Term-1. Our next main result establishes this guarantee but will first need the following two
assumptions.
Assumption 1. (Y * = Y) Clean labels are Bayes optimal (Y * := arg maxi∈[κ] P(Y = i|X)).
Assumption 2. (Informative datasets) The noise rate is bounded as Tii (X) - Tij (X) > 0, ∀i ∈
[K],j ∈ [K],j= i,X 〜DX.
Feasibility of assumptions: 1) Note for many popular image datasets, e.g. CIFAR, the label of each
feature is well-defined and the corresponding distribution is well-separated by human annotation. In
this case, each feature X only belongs to one particular class Y. Thus Assumption 1 is generally
held in classification problems (Liu & Tao, 2015). Technically, this assumption could be relaxed.
We use this assumption for clean presentations. 2) Assumption 2 shows the requirement of noise
rates, i.e., for any feature X, a sufficient number of clean examples are necessary for dominant clean
information. For example, we require Tii (X) - Tij (X) > 0 to ensure examples from class i are
informative (Liu & Chen, 2017).
Before formally presenting the noise-resistant property of training with 'cr, We discuss intuitions
here. As discussed earlier in Section 2.1, our 'cr regularizes the CE loss to generate/incentivize
confident prediction, and thus is able to approximate the 0-1 loss to obtain its robustness property.
More explicitly, from (5), 'cR affects Term-3 with a scale parameter β. Recall that Uij (X) =
Tij(X), ∀i 6= j, which is exactly the noise transition matrix. Although we have no information
about this transition matrix, the confusion brought by Uij (X) can be canceled or reversed by a
sufficiently large β such that Uij (X) - βP(Y = j ) ≤ 0. Intuitively, with an appropriate β, all the
effects of Uij (X), i 6= j can be reversed, and we will get a negative loss punishing the classifier for
predicting class-j when the clean label is i. Formally, Theorem 4 shows the noise-resistant property
of training with 'cR and is proved in Appendix B.4.
Theorem 4. (Robustness of the Confidence Regularized CE Loss) With Assumption 1 and 2, when
max	U(X) ≤ β ≤	min	WX) - TjX)
i,j∈[K],χ〜DX P(Y = j)	P(Y=i)>P(Y=j),X〜DX P(Y = i) - P(Y = j)
(6)
minimizing EDe ['(f (X), Ye) + 'CR (f (X))] is equivalent to minimizing ED ['(f (X), Y)].
Theorem 4 shows a sufficient condition of β for our confidence regularized cE loss to be robust to
instance-dependent label noise. The bound on LHS ensures the confusion from label noise could be
6
Published as a conference paper at ICLR 2021
canceled or reversed by the β weighted confidence regularizer, and the RHS bound guarantees the
model with the minimized regularized loss predicts the most frequent label in each feature w.p. 1.
Theorem 4 also provides guidelines for tuning β. Although we have no knowledge about Tij (X), we
can roughly estimate the range of possible β. One possibly good setting of β is linearly increasing
with the number of classes, e.g. β = 2 for 10 classes and β = 20 for 100 classes.
With infinite model capacity, minimizing ED [`(f (X), Y )] returns the Bayes optimal classifier (since
CE is a calibrated loss) which predicts on each xn better than random guess. Therefore, with a suf-
ficient number of examples, minimizing EDe ['(f (X), Y) + 'cR(f (X))] Will also return a model that
predicts better than random guess, then satisfying the condition required in Theorem 2 to guarantee
the quality of sieved examples. Further, since the Bayes optimal classifier alWays predicts clean la-
bels confidently When Assumption 1 holds, Theorem 4 also guarantees confident predictions. With
such predictions, the sample sieve in (4) Will achieve 100% precision on both clean and corrupted
examples. This guaranteed division is summarized in Corollary 1:
Corollary 1. When conditions in Theorem 4 hold, with infinite model capacity and sufficiently many
examples, CORES2 achieves Vn = l(yn = yn), ∀n ∈ [N], i.e., all the Sieved clean examples are
effectively clean.
3.3	Training with Sieved Samples
We discuss the necessity of a dynamic sample sieve in this subsection. Despite the strong guaran-
tee in expectation as shoWn Theorem 4, performing direct Empirical Risk Minimization (ERM) of
the regularized loss is likely to return a sub-optimal solution. Although Theorem 4 guarantees the
equivalence of minimizing tWo first-order statistics, their second-order statistics are also important
for estimating the expectation when examples are finite. Intuitively, Term-1 T ∙ ED['(f (X),Y)]
primarily helps distinguish a good classifier from a bad one on the clean distribution. The existence
of the leading constant T reduces the power of the above discrimination, as effectively the gap be-
tween the expected losses become smaller as noise increases (T will decrease). Therefore we would
require more examples to recognize the better model. Equivalently, the variance of the selection
becomes larger. In Appendix C.2, we also offer an explanation from the variance’s perspective. For
some instances with extreme label noise, the β satisfying Eqn. (6) in Theorem 4 may not exist.
In such case, these instances cannot be properly used and other auxiliary techniques are necessary
(e.g., sample pruning).
Sieving out the corrupted examples from the clean ones allows us a couple of better solutions. First,
we can focus on performing ERM using these sieved clean examples only. We derive the risk
bound for training with these clean examples in Appendix C.3. Secondly, leveraging the sample
sieve to distinguish clean examples from corrupted ones provides a flexible interface for various
robust training techniques such that the performance can be further improved. For example, semi-
supervised learning techniques can be applied (see section 4 for more details).
4	Experiments
Now we present experimental evidences of how CORES2 works. 5
Datasets: CORES2 is evaluated on three benchmark datasets: CIFAR-10, CIFAR-100 (Krizhevsky
et al., 2009) and Clothing1M (Xiao et al., 2015). Following the convention from Xu et al. (2019),
we use ResNet34 for CIFAR-10 and CIFAR-100 and ResNet50 for Clothing1M.
Noise type: We experiment with three types of label noise: symmetric, asymmetric and instance-
dependent label noise. Symmetric noise is generated by randomly flipping a true label to the other
possible labels w.p. ε (Kim et al., 2019), where ε is called the noise rate. Asymmetric noise is
generated by flipping the true label to the next class (i.e., label i → i+1, mod K) w.p. ε. Instance-
dependent label noise is a more challenging setting and we generate instance-dependent label noise
following the method from Xia et al. (2020) (See Appendix D.3 for details). In expectation, the
noise rate ε for all noise regimes is the overall ratio of corrupted examples in the whole dataset.
Consistency training after the sample sieve: Let τ be the last iteration of CORES2 . Define
L(T):= {n∣n ∈ [N]川"=1}, H(T):= {n∣n ∈ [N],v(τ) = 0}, DL(T) ：= {(xn,yn) ： n ∈
5The logarithmic function in 'cr is adapted to ln(fχ[y] + 10-8) for numerical stability.
7
Published as a conference paper at ICLR 2021
1.0 __
0.8
0.6
0.4
0.2
0.0
20
40
60
80
(aj 20% Symm.
LO^φ-T——I------⅜---
0.6
0.4
0.2
0.0 -
——CORES2
Co-teaching
---Co-teaching+
20^
(d)
60
epoch
20% Inst.
80^
100
IOO
1.0
0.8-产
0.6
0.4
0.2
0.0----
1.0
0.8——
0.6 .-
0.4
0.2
0.0
20
40
60
(cj 60% Symm.
J__+-4-~
^20
40
60
epoch
⑴ 6(5% Inst.
80
100
80
IOO

否


——--一Ψ
Figure 3: F-score comparisons on CIFAR10 under symmetric (Symm.) and instance-based (Inst.)
ILl . I-	2∙Pre∙Re	lʌ C	Pn∈ [N] ɪ(Vn=1，yn = yn)	Λ P1	Pn∈[N ] l(vn=1,yn = yn)
label noise. F-SCOre : = 2PPW，where Pre :=	pn]∈[N] l(Vn = 1)	，and Re :=	Pn∈[N]l(yn=yn)	.
L(T)}, DH(T) ：= {(xn, yη) : n ∈ H(τ)}. ThUs DL(T) is sieved as clean examples and DH(T) is fil-
tered out as corrupted ones. Examples (xn, yn) ∈ DL(T) lead the training direction using the CE loss
as Eη∈L(τ) '(f (xn), yn). Noting the labels in DH(T) are supposed to be corrupted and can distract
the training, we simply drop them. On the other hand, feature information of these examples en-
codes useful information that we can further leverage to improve the generalization ability of models.
There are different ways to use this unsupervised information, in this paper, we chose to minimize
the KL-divergence between predictions on the original feature and the augmented feature to make
predictions consistent. This is a common option as chosen by Li et al. (2019), Xie et al. (2019), and
Zhang et al. (2020b). The consistency loss function in epoch-t is Pη∈H(T) 'kl(∕ (Xn ), f(t)(xn,t)),
where f(t is a copy of the DNN at the beginning of epoch-t but without gradients. Summing the
classification and consistency loss yields the total loss. See Appendix D.1 for an illustration.
Other alternatives: Checking the consistency of noisy predictions is only one possible way to
leverage the additional information after sample sieves. Our basic idea of first sieving the dataset
and then treating corrupted examples differently from clean ones admits other alternatives. There
are many other possible designs after sample sieves, e.g., estimating transition matrix using sieved
examples then applying loss-correction (Patrini et al., 2017; Vahdat, 2017; Xiao et al., 2015), making
the consistency loss as another regularization term and retraining the model (Zhang et al., 2020b),
correcting the sample selection bias in clean examples and retraining (Cheng et al., 2020; Fang et al.,
2020), or relabeling those corrupted examples and retraining, etc. Additionally, clustering methods
on the feature space (Han et al., 2019; Luo et al., 2020) or high-order information (Zhu et al., 2021a)
can also be exploited along with the dynamic sample sieve. Besides, the current structure is ready
to include other techniques such as mixup (Zhang et al., 2018).
Quality of our sample sieve: Figure 3 shows the F-scores of sieved clean examples with training
epochs on the symmetric and the instance-based label noise. F-score quantifies the quality of the
sample sieve by the harmonic mean of precision (ratio of actual cleans examples in sieved clean
ones) and recall (ratio of sieved cleans examples in actual clean ones). We compare CORES2 with
Co-teaching and Co-teaching+. Note the F-scores of CORES2 and Co-teaching are consistently high
on the symmetric noise, while CORES2 achieves higher performance on the challenging instance-
based label noise, especially with the 60% noise rate where the other two methods have low F-scores.
Experiments on CIFAR-10, CIFAR-100 and Clothing1M: In this section, we compare CORES2
with several state-of-the-art methods on CIFAR-10 and CIFAR-100 under instance-based, symmet-
ric and asymmetric label noise settings, which is shown on Table 1 and Table 2. CORES2? denotes
that we apply consistency training on the corrupted examples after the sample sieve. For a fair com-
parison, all the methods use ResNet-34 as the backbone. By comparing the performance of CE on
the symmetric and the instance-based label noise, we note the instance-based label noise is a more
challenging setting. Even though some methods (e.g., LDMI) behaves well on symmetric and asym-
metric label noise, they may reach low test accuracies on the instance-based label noise, especially
when the noise rate is high or the dataset is more complex. However, CORES2 consistently works
well on the instance-based label noise and adding the consistency training gets better results. Ta-
ble 3 verifies CORES2 on Clothing1M, a dataset with real human label noise. Compared to the other
8
Published as a conference paper at ICLR 2021
Table 1: Comparison of test accuracies on clean datasets under instance-based label noise.
Method	ε = 0.2	Inst. CIFAR10 ε = 0.4	ε=0.6	Inst. CIFAR100		
				ε = 0.2	ε =0.4	ε=0.6
Cross Entropy	87.16	75.16 二	44.64	58.72	41.14	25.29
Forward T (Patrini et al., 2017)	88.08	82.67	41.57	58.95	41.68	22.83
LDMI (Xu et al., 2019)	88.80	82.70	70.54	58.66	41.77	28.00
Lq (Zhang & Sabuncu, 2018)	86.45	69.02	32.94	58.18	40.32	23.13
SCE (Wang et al., 2019)	89.11	72.04	44.83	59.87	41.76	23.41
Co-teaching (Han et al., 2018)	88.66	69.50	34.61	43.03	23.13	7.07
Co-teaching+ (Yu et al., 2019)	89.04	69.15	33.33	41.84	24.40	8.74
JoCoR (Wei et al., 2020)	88.71	68.97	30.27	44.28	22.77	7.54
Peer Loss (Liu & Guo, 2020)	89.33	81.09	73.73	59.92	45.76	33.61
CORES2	89.50	82.84	79.66	61.25	47.81	37.85
CORES2?	95.42	88.45	85.53	72.91	70.66	63.08
Table 2: Comparison of test accuracies on clean datasets under symmetric/asymmetric label noise.
NzTpfhnrl eto	Symm.	CIFAR10	Asymm.	CIFAR10	Symm.	CIFAR100	Asymm.	CIFAR100
	ε = 0.4	ε=0.6	ε = 0.2	ε =0.3	ε=0.4	ε= 0.6	ε =0.2	ε = 0.3
Cross Entropy	81.88	74.14	88.59	86.14	48.20	37.41	59.20	51.40
MAE (Ghosh et al., 2017)	61.63	41.98	59.67	57.62	7.68	6.45	11.16	8.97
Forward T (Patrini et al., 2017)	83.27	75.34	89.42	88.25	53.04	41.59	64.86	64.72
Lq (Zhang & Sabuncu, 2018)	87.13	82.54	89.33	85.45	61.77	53.16	66.59	61.45
LDMI (Xu et al., 2019)	83.04	76.51	89.04	87.88	52.32	40.00	60.04	52.82
NLNL (Kim et al., 2019)	92.43	88.32	93.35	91.80	66.39	56.51	63.12	54.87
SELF (Nguyen et al., 2019)	91.13	-	93.75	92.42	66.71	-	70.53	65.09
CORES2?	93.76	89.78	95.18	94.67	72.22	59.16	75.19	73.81
Table 3: The best epoch (clean) test accuracy for each method on Clothing1M.
Method
-_CE	Forward T Co-teaching	JoCoR	LDM	PTD-R-V CORES2
(Baseline) (Patrini et al., 2017) (Han et al., 2018) (Wei et al., 2020) (Xu et al., 2019) (Xia et al., 2020) (our)
Acc.
68.94	70.83
69.21	7030	7246	7167	73.24
approaches, CORES2 also works fairly well on the Clothing1M dataset. See more experiments in
Appendix D. We also provide source codes with detailed instructions in supplementary materials.
5 Conclusions
This paper introduces CORES2, a sample sieve that is guaranteed to be robust to general instance-
dependent label noise and sieve out corrupted examples, but without using explicit knowledge of
the noise rates of labels. The analysis of CORES2 assumed that the Bayes optimal labels are the
same as clean labels. Future directions of this work include extensions to more general cases where
the Bayes optimal labels may differ from clean labels. We are also interested in exploring different
possible designs of robust training with sieved examples.
Acknowledgement This work is partially supported by the National Science Foundation (NSF)
under grant IIS-2007951 and the Office of Naval Research under grant N00014-20-1-22.
Conditions Required for Theorem 1
Theorem 1 holds based on the following three assumptions:
A1. The model capacity is infinite (i.e., it can realize arbitrary variation).
A2. The model is updated using the gradient descent algorithm (i.e. updates follow the direction
ofdecreasing ED ['(f (X), Y)] - EDY [Edx [' (f(X), Y)]]).
A3. The derivative of network function fWw is smooth (i.e. the network function has no
singular point), where wi ’s are model parameters.
9
Published as a conference paper at ICLR 2021
References
Vibhu Agarwal, Tanya Podchiyska, Juan M Banda, Veena Goel, Tiffany I Leung, Evan P Minty,
Timothy E Sweeney, Elsie Gyang, and Nigam H Shah. Learning statistical models of phenotypes
using noisy labeled training data. Journal of the American Medical Informatics Association, 23
(6):1166-1173,2016.
Amr M. Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-
corrected calibration is hard-to-beat at label shift adaptation. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML ’20, 2020.
Ehsan Amid, Manfred KK Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic
loss based on bregman divergences. In Advances in Neural Information Processing Systems, pp.
14987-14996, 2019.
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Antonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Confidence scores
make instance-dependent label-noise learning possible. arXiv preprint arXiv:2001.03772, 2020.
Satrajit Chatterjee. Coherent gradients: An approach to understanding generalization in gradient
descent-based optimization. In International Conference on Learning Representations, 2020.
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with
bounded instance-and label-dependent label noise. In Proceedings of the 37th International Con-
ference on Machine Learning, ICML ’20, 2020.
Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. Rethinking importance weighting for
deep learning under distribution shift. arXiv preprint arXiv:2006.04662, 2020.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Maoguo Gong, Hao Li, Deyu Meng, Qiguang Miao, and Jia Liu. Decomposition-based evolutionary
multiobjective optimization to self-paced learning. IEEE Transactions on Evolutionary Compu-
tation, 23(2):288-302, 2018.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Jiangfan Han, Ping Luo, and Xiaogang Wang. Deep self-learning from noisy labels. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 5138-5147, 2019.
JiayUan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex J Smola. Cor-
recting sample selection bias by unlabeled data. In Advances in neural information processing
systems, pp. 601-608, 2007.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels. arXiv preprint
arXiv:1712.05055, 2017.
Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy
labels. In Proceedings of the IEEE International Conference on Computer Vision, pp. 101-110,
2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
10
Published as a conference paper at ICLR 2021
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy la-
beled data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5051-5059, 2019.
Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJgExaVtwr.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1910-1918, 2017.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Yang Liu. The importance of understanding instance-level noisy labels, 2021.
Yang Liu and Yiling Chen. Machine-learning aided peer prediction. In Proceedings of the 2017
ACM Conference on Economics and Computation, pp. 63-80, 2017.
Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise
rates. In Proceedings of the 37th International Conference on Machine Learning, ICML ’20,
2020.
Yijing Luo, Bo Han, and Chen Gong. A bi-level formulation for label noise learning with spectral
cluster discovery. In International Joint Conference on Artificial Intelligence, pp. 2605-2611,
2020.
Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on
cybernetics, 43(3):1146-1151, 2013.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong
Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to filter noisy labels with self-
ensembling. arXiv preprint arXiv:1910.01842, 2019.
Curtis G Northcutt, Lu Jiang, and Isaac L Chuang. Confident learning: Estimating uncertainty in
dataset labels. arXiv preprint arXiv:1911.00068, 2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
Jun Shu, Qian Zhao, Keyu Chen, Zongben Xu, and Deyu Meng. Learning adaptive loss for robust
learning with noisy labels. arXiv preprint arXiv:2002.06482, 2020.
Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset
shift in machine learning, pp. 3-28, 2009.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5552-5560, 2018.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In Advances in Neural Information Processing Systems, pp. 5596-5605, 2017.
11
Published as a conference paper at ICLR 2021
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition,pp. 839-847, 2017.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross en-
tropy for robust learning with noisy labels. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 322-330, 2019.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A
joint training method with co-regularization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13726-13735, 2020.
Jiaheng Wei and Yang Liu. When optimizing $f$-divergence is robust with label noise. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=WesiCoRVQ15.
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama.
Are anchor points really indispensable in label-noise learning? In Advances in Neural Information
Processing Systems, pp. 6838-6849, 2019.
Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu,
Dacheng Tao, and Masashi Sugiyama. Parts-dependent label noise: Towards instance-dependent
label noise. arXiv preprint arXiv:2006.07836, 2020.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2691-2699, 2015.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data
augmentation. arXiv preprint arXiv:1904.12848, 2019.
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: An information-theoretic noise-
robust loss function. NeurIPS, arXiv:1909.03388, 2019.
Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James T Kwok. Searching to exploit memo-
rization effect in learning with noisy labels. In Proceedings of the 37th International Conference
on Machine Learning, ICML ’20, 2020a.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi
Sugiyama. Dual T: Reducing estimation error for transition matrix in label-noise learning. In
Advances in Neural Information Processing Systems, volume 33, pp. 7260-7271, 2020b.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017-
7025, 2019.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? arXiv preprint arXiv:1901.04215,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb.
Jing Zhang, Victor S Sheng, Tao Li, and Xindong Wu. Improving crowdsourced label quality using
noise correction. IEEE transactions on neural networks and learning systems, 29(5):1675-1688,
2017.
Xuchao Zhang, Xian Wu, Fanglan Chen, Liang Zhao, and Chang-Tien Lu. Self-paced robust learn-
ing for leveraging clean labels in noisy data. In AAAI, pp. 6853-6860, 2020a.
12
Published as a conference paper at ICLR 2021
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in neural information processing Systems, pp. 8778-8788, 2018.
Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. Distilling effective
supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9294-9303, 2020b.
Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-
dependent label noise. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2021a.
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when
learning with noisy labels. arXiv preprint arXiv:2102.05291, 2021b.
Piotr Zielinski, Shankar Krishnan, and Satrajit Chatterjee. Explaining memorization and general-
ization: A large-scale study with coherent gradients. arXiv preprint arXiv:2003.07422, 2020.
13
Published as a conference paper at ICLR 2021
Appendix
The appendices are organized as follows. Section A presents the full version of related works.
Section B details the proofs for our theorems. Section C supplements other necessary evidences to
justify CORES2 . Section D shows more experimental details and results.
A	Full Version of Related Works
Learning with noisy labels has observed exponentially growing interests. Since the traditional cross-
entropy (CE) loss has been proved to easily overfit noisy labels (Zhang et al., 2016), researchers
try to design different loss functions to handle this problem. There were two main perspectives on
designing loss functions. Considering the fact that outputs of logarithm functions in the CE loss grow
explosively when the prediction f(x) approaches zero, some researchers tried to design bounded loss
functions (Amid et al., 2019; Wang et al., 2019; Gong et al., 2018; Ghosh et al., 2017). To avoid
relying on fine-tuning of hyper-parameters in loss functions, a meta-learning method was proposed
bt Shu et al. (2020) to combine the above four loss functions together. However, simply considering
loss function values without discussing the noise type and the corresponding statistics could not be
noise-tolerant as defined by Manwani & Sastry (2013). As a complementary, others started from
noise types and tried to design noise-tolerant loss functions. Based on the assumption that label
noise only depends on the true class (a.k.a. feature-independent or label-dependent), an unbiased
loss function called surrogate loss (Natarajan et al., 2013), an information-based loss function called
LDMI (Xu et al., 2019), and a new family of loss functions to punish agreements between classifiers
and noisy datasets called peer loss (Liu & Guo, 2020) were proposed. They proved theoretically
that training DNNs using their loss functions on feature-independent noisy datasets was equivalent
to training CE on the corresponding unobservable clean datasets. However, surrogate loss focused
on the binary classifications and required knowing noise rates. LDMI and peer loss does not require
knowing noise rates while LDMI may not be easy for extension and multi-class classification of peer
loss requires particular transition matrices.
The correction approach is also popular in handling label noise. Previous works (Patrini et al., 2017;
Vahdat, 2017; Xiao et al., 2015) assumed the feature-independent noise transition matrix was given
or could be estimated and attempted to use it to correct loss functions. For example, Patrini et al.
(2017) first estimated the noise transition matrix and then relied on it to correct forward or backward
propagation during training. However, without a set of clean examples, the noise transition matrix
could be hard to estimate correctly. Instead of correcting loss functions, some methods directly
corrected labels (Veit et al., 2017; Li et al., 2017; Han et al., 2019), whereas it might introduce extra
noise and damage useful information. Recent works (Xia et al., 2020; Berthon et al., 2020) extended
loss-correction from the limited feature-independent label noise to part-dependent or a more general
instance-dependent noise regime while they relied heavily on the noise rate estimation.
Sample selection (Jiang et al., 2017; Han et al., 2018; Yu et al., 2019; Yao et al., 2020a; Wei et al.,
2020) mainly focused on exploiting the memorization of DNNs and treating the “small loss” ex-
amples as clean ones, while they only focused on feature-independent label noise. Cheng et al.
(2020) tried to distill some examples relying on the predictions using the surrogate loss function
(Natarajan et al., 2013). Note estimating noise rates are necessary for both applying surrogate loss
and determining the threshold for distillation. The sample selection methods could be implemented
with some semi-supervised learning techniques to improve the performance, where the corrupted
examples were treated as unlabeled data (Li et al., 2020; Nguyen et al., 2019). However, the training
mechanisms of these methods were still based on the CE loss, which could not be guaranteed to
avoid overfitting to label noise.
B	Proof for Theorems
In this section, we firstly present the proof for Theorem 3 (our main theorem) in Section B.1, which
provides a generic machinery for anatomizing noisy datasets. Then we will respectively prove The-
orem 1 in Section B.2, Theorem 2 in Section B.3, and Theorem 4 in Section B.4 according to the
order they appear.
14
Published as a conference paper at ICLR 2021
B .1 Proof for Theorem 3
Theorem 3. (Main Theorem: Decoupling the Expected Regularized CE Loss) In expectation, the
loss with 'CR can be decoupled as three separate additive terms:
Term-1	Term-2
z z	}|	、	/_	/|	、
E25 [4(f(X), Y) + 'CR(f (X))] = T ∙ Ed['(f(X), Y)] + ∆ ∙ EPa ['(f(X), Y)]
+ X X P(Y = i)Eo∣y=i[(Uij(X) - βP(Y = j))'(f(X),j)],
j∈[K] i∈[K]
|--------------------------------V-----------------------------'
Term-3
⑺
where T := min∙∈[κ] Tjj, ∆ := £关的 ∆jP(Y = j), ∆j ：= Tjj-T Uij(X) = Tij(X), Vi =
j, Ujj(X) = Tjj(X) - Tjj, andEdλ['(f(X),Y)] := l(z∆ > 0) £关的 JW = kd∖y=j['(f(X),j)]∙
Proof∙ The expected form of traditional CE loss on noisy distribution D can be written as
-,~.-
ED ['(f(X ),Y)]
:X X P(Y = i)Eo∣y=i[Tij (X )'(f (X ),j)]
j∈[K] i∈[K]
:X X P(Y = i)TijEp∣κ=i['(f(X),j)]+ X X P(Y = i)Covo∣γ=i(%(X),'(f(X),j)).
j∈[K] i∈[K]	j∈[K] i∈[K]
The first term could be transformed as:
E EP(Y = i)TijEd ∣ y=i['(f(X),j)]
j∈[K] i∈[K]
=X	TjjP(Y = j)Ed ∣Y=j['(f(X),j)]+ X TijP(Y = i)ED∣γ=i['(f(X),j)]
j∈[K] |_	i∈[K],i=j	_
=TED['(f(X),Y)]+∆ED∆['(f(X),Y)]+ X X TijP(Y = i)Eoγ=i['(f(X),j)],
j∈[K] i∈[K],i=j
where
T ：= m⅛ Tjj, ∆ := E ∆jP(Y = j), ∆j := Tjj - T,
j [ ]	j∈[K]
and
ED ['(f(X) Y)] := (Pj∈[κ] j ∆ 力EDIY=j['(f(X),j)], if A > 0,
D△〔	,力.ɪŋ if ∆ = 0.
15
Published as a conference paper at ICLR 2021
Then
-, , . ■—.-
ED ['(f(X ),Y)]
=TEd['(f(X),Y)]+∆Edδ['(f(X),Y)]+ X X TijP(Y = i)ED|Y=i['(f(X),j)],
j∈[K] i∈[K],i6=j
+ X X P(Y = i)C0VD∣γ=i(Tij(X),'(f(X),j))
j∈[K] i∈[K]
=TED['(f(X),Y)]+∆Edδ['(f(X),Y)] + X X TijP(Y = 2)Ed∣y=i['(f(X),j)],
j∈[K] i∈[K],i6=j
+ X X P(Y = i)ED∣Y=i[(Tij(X) - Tij)('(f(X),j) - Ed∣y=i['(f(X),j)])]
j∈[K] i∈[K],i6=j
+ X P(Y = j )Ed∣y=j [(Tjj (X) — Tjj )('(f (X ),j) — Ed∣y=j['(f(X ),j)])]
j∈[K]
=T Ed ['(f(X ),Y )]+∆ Edδ ['(f(X ),Y)]
+ X X P(Y = i)ED|Y=i[(Tij(X) - Tij)('(f (X),j) - Ed∣y=i['(f(X),j)])+ Tij'(f (X),j)]
j∈[K] i∈[K],i6=j
+ X P(Y = j)Ed∣y=j[(Tjj(X) - Tjj)('(f(X),j) - Ed∣y=j['(f(X),j)])]
j∈[K]
=T Ed ['(f(X ),Y )]+∆ Edδ ['(f(X ),Y)] + X X P(Y = 2)Ed∣y=i[Tj (X)'(f (X),j)]
j∈[K] i∈[K],i6=j
+ X P(Y = j)Ed∣y=j[(Tjj(X) - Tjj)`(f(X),j)]
j∈[K]
=TEd['(f(X),Y)]+∆Edδ['(f(X),Y)]+ X X P(Y = 2)Ed∣y=i[Uij(X)'(f(X),j)],
j∈[K] i∈[K]
where
Uij(X) =Tij(X),∀i 6=j,	Ujj(X) =Tjj(X)-Tjj.
The expected form of 'cr on noisy distribution D can be written as
ED ['CRf(Xi))] = -βED IEDYf ['(f (χi),Ye)]i
=-β L [p(D)EDγf['(f (χi),Ye)]i
=-β X P(Ye = j)EDX ['(f (Xi),j)]
j∈[K]
=-XX P(Y = i)ED∣Y=i[βP(Ye = jMf(Xi),j)].
j∈[K] i∈[K]
Thus the expected form of the new regularized loss is
ED h'(f(X),Ye) + 'CR(f(xi))i = TED['(f(X),Y)] + ∆Edδ['(f(X),Y)]
+XXP(Y = i)ED∣γ=i[(Uij(X) - βP(Y = j))'(f(X),j)].	⑻
j∈[K] i∈[K]
□
B.2 Proof for Theorem 1
Theorem 1. For ' ca (∙), solutions satisfying fχn[i] > 0, ∀i ∈ [K ] are not locally optimal at (xn, yn).
Proof. Let '(∙) be the CE loss. Note this proof does not rely on whether the data distribution is clean
or not. We use D to denote any data distribution and D to denote the corresponding dataset. This
16
Published as a conference paper at ICLR 2021
notation applies only to this proof. For any data distribution D, we have
ED ['(f (X),Y) - Edy∣d ['(f(Xn),Y)]]
=ED ['(f(X ),Y)] - EDY [Edx [' (f(X ),Y)]]
= - dx X P(x, y) ln fx[y] + dx X P(x)P(y) ln fx [y]
dx X ln fx [y][P(x, y) - P(x)P(y)].
The dynamical analyses are based on the following three assumptions:
A1. The model capacity is infinite (i.e., it can realize arbitrary variation).
A2. The model is updated using the gradient descent algorithm (i.e. updates follow the direction
Ofdecreasing ED ['(f (X), Y)] - EDY [Edx [' (f (X), Y)]]).
A3. The derivative of network function fWw is smooth (i.e. the network function has no
singular pOint), where wi ’s are mOdel parameters.
Denote the variations of fx [y] during one gradient descent update by ∆y(x). From Lemma 1, it can
be explicitly written as
∆y(x)
fχ[y] ∙ η [
Dx
dx0
y0∈[K]
[P(x0, y0) - P(x0)P(y0)]	Gi(x, y)Gi(x0, y0),
i∈[K]
(9)
where η is the learning rate,
Gi(x, y)
» + X fx[y0]4,
∂wi	∂wi
i	y0∈[K]	i
and gy(x) is the network output before the softmax activation. i.e.
fx[y]
exp(gy (x))
Py,∈[K] exp(gy0 (X))
With ∆y(x), the variation of the regularized loss is
—
△Ed ['(f (X),Y) + 'cr]=-
dx P(x) X
Dx	y∈[K]
△y(x)
P(y∣χ) - P(y)
fx [y]
(10)
If the training reaches a steady state (a.k.a. local optimum), we have △ED ['(f (X), Y) + 'cR] = 0.
To check the property of this variation, consider the following example. For a particular x0 , define
F(x0) :
△y(x0)
y∈[K]
P(y∣χo) - P(y)
fχo [y]
Split the labels y into the following two sets (without loss of generality, we ignore the P(y|x0) -
P(y) = 0 cases):
Yx0;- = {y : P(y|x0) - P(y) < 0}
and
Yx0;+ = {y : P(y|x0) - P(y) > 0}.
By assigning △y(x0) = ay < 0, ∀y ∈ Yx0;- and △y(x0) = by > 0, ∀y ∈ Yx0;+, one finds
F(x0) > 0 since fx0 [y] > 0. Note we have an extra constraint Py △y(x0) = 0 to ensure
Py∈[K] fx0 [y] = 1 after update. It is easy to check our assigned ay and by could maintain this
constraint by introducing a weight Nab to scale b0y as follows.
X ay + Nab X b0y = 0, by = Nabb0y.
y∈Y-	y∈Y+
17
Published as a conference paper at ICLR 2021
Let Be(χo) be a ^-neighbourhood of x°. Since fχ[y] is continuous, We can set ∆y(x) = 1 (1 +
cos πkx-x0k )∆y(xo),∀x ∈ Be(χo) and 0 otherwise. The coefficient 1 (1 + cos πkx-x0k) is added
so that the continuity of fχ[y] preserves. This choice will lead to ∆Ed ['(f (X), Y) + 'cr] < 0.
Therefore, for any 'ca(∕(Xn), yn) with solution fχn [i] > 0, ∀i ∈ [K], we can always find a decreas-
ing direction, indicating the solution is not (steady) locally optimal. Note D can be any distribution
in this proof. Thus the result holds for the noisy distribution D.	□
Lemma 1.
∆y(x)
,“期. η L dχ0 X
[P(x0, y0) - P(x0)P(y0)]	Gi(x, y)Gi(x0, y0).
i∈[K]
Proof. We need to take into account the actual form of activation function, i.e., the softmax function,
as well as the SGD algorithm to demonstrate the correctness of this lemma. The variation ∆y0 (x0)
is caused by the change in network parameters {wi}, i.e.,
∂fX0 [y0]
δjo (X0)= ʌ, —∂W δwi,
i∈[K]	i
where δwi are determined by the SGD algorithm
(11)
δwi = - η
∂Ed ['(f (X ),Y)+ 'cr]
=ηX
X,y
∂wi
P(x, y) - P(x)P(y) ∂fx [y]
fx[y]
∂wi
Plugging back to (11) yields
∆y0(x0) = ηX
P(x, y) - P(x)P(y)
X,y
To proceed, we need to expand d∂Wy
fx[y]
'X dfX0 [y0] dfX [y]
∂wi	∂wi .
i∈[K]	i	i
. Taking into account the activation function, one has
fx[y]
exp(gy (x))
Py0∈[K] exp(gy0 (X))
where gy (x) refers to the network output before passed to the activation function. Recall that, by
our assumption, derivatives df(W；W)
∂fX [y]	∂e-gy (X)	1
are not singular. Now we have
∂wi
dWi	Py0∈[κ] e-gy0(X)
-e-gy (X)	∂gy (x)
Py0∈[K]e-gy0(X) ^w^
+ e-gy(X) -d-(—
+	∂Wi IP
e-gy (x)
1
1 r ∣e-gy∕(X)
y0∈[K] e y
=fx [y]
(Py00∈[K] e-gy00
_ X e-gy0 (x) dgy0 (X)
2 y⅛K]e	dWi
粤侬+ X fxk]粤但
∂wi	∂wi
i	y0∈[K]	i
For simplicity, we can rewrite the above result as
fy=fx[y]Gi(X,y),
where
is a smooth function.
…一甘+ X，x-
y0
combining all the above gives ∆y0 (X0) as follows.
∆y0(χ0) = fχo[yo] ∙ ηX [p(χ,y) - P(X)P(y)] XGi(Xo,yo)Gi(X,y)
—
□
18
Published as a conference paper at ICLR 2021
B.3 Proof for Theorem 2
Theorem 2. The sample Sieve defined in(4) ensures that clean examples (xn, yn = yn) will not be
identified as being corrupted if the model f(t) ’s prediction on xn is better than random guess.
Proof. Let yn be the true label corresponding to feature xn. For a clean sample, We have yn = yn.
Consider an arbitrary DNN model f. With the CE loss, we have '(f (Xn),yn) = - ln(fχn [yn]).
According to Equation (4) in the paper, the necessary and sufficient condition of vn > 0 is
'(f(Xn),yn) + 'CR(f(xn)) < αn ⇔ -ln(fx”[yn]) < -K X ln(fχn[y])
y∈[K]
⇔ - ln(fxn [yn]) < - K-J	X	ln(fxn [y]) .
y∈[K],y6=yn
By Jensen’s inequality we have
- ln 1 -fXn [yn]	= - ln (Ey∈[K],y=yn fxn[y]
-I κ -1	= -	∖ κ -1
≤- K-I	X	in(fχn [y]).
y∈[K],y6=yn
Therefore, when (sufficient condition)
-ln(fXn [yn]) < - ln (- Jxn Iyn ) ⇔ fXn[yn] >
κ-1	κ
we have Vn > 0. Inequality fxn[yn] > Kκ indicates the model prediction is better than random
guess.
□
B.4 Proof for Theorem 4
Before proving Theorem 4, we need to show the effect of adding Term-2 to Term-1 in (5). Let
X < 0.5 be the measure of separation among classes w.r.t feature X in distribution D, i.e., P(Y =
Y *|X) = 1 - EX ,(X, Y)〜D, where Y * := arg maxi∈[κ] P(Y = i|X) is the Bayes optimal label.
Let D0 be the shifted distribution by adding Term-2 to Term-1 and Y0 be the shifted label. Then
P(X|Y) = P(X∣Y0),∀(X,Y)〜D, (X, Y0)〜D0 but P(Y0) may be different from P(Y). Lemma
2 shows the invariant property of this label shift.
Lemma 2. Label shift does not change the Bayes optimal label of feature X when EX <
min∀i,j∈[κ] (Tii++j ).
Proof. Consider the shifted distribution D0 . Let
T ED ['(f (X), Y)] + A ED∆ ['(f (X), Y)] = C Edo ['(f (X ),Y)],
where
ED0 ['(f(X), Y)] := X P(Y0=j)ED0|Y0=j['(f(X),j)],
j∈[K]
and
P(Y0=j):= jYj,
C
where C := Pj∈[K] TjjP(Y = j) is a constant for normalization. For each possible Y = i, we
have P(Y = i|X) ∈ [0, EX] ∪ {1 - EX}, EX < 0.5. Thus
P"Y ]	P(Y = i|X )P(X) U h EX P(X )]∣ UP(X )(1 - EX)]
P(XIY =i)= P(Y = i)	∈ [0,PY=^]∪{ P(Y = i) }.
19
Published as a conference paper at ICLR 2021
Compare D0 and D, we know there is a label shift (Alexandari et al., 2020; Storkey, 2009), where
P(X|Y = i) = P(X|Y 0 = i) but P(Y ) and P(Y 0) may be different. To ensure the label shift does
not change the Bayes optimal label, we need
Y * = argmax P(Y 0 = i|X) = arg max P(X IY 0 = i)P(Y 0 = i), (X,Y 0)〜D.
i∈[K]	i∈[K]	P(X)
One sufficient condition is
EX P(Y 0 = ") < (1-J )P(Y 0= j ⇒ f < min (	Tj	∖
P(Y = i)	p(y = j)	X	∀i,m∈[κ] ∖Tii + Tjjj
□
With Lemma 2, Assumption 1, and Assumption 2, we present the proof for Theorem 4 as follows.
Theorem 4. (Robustness of the Confidence Regularized CE Loss) With Assumption 1 and 2, when
Uij (X) ∕Q∕	Tii(X) - Tij (X)
max	-≤-— ≤ β ≤	min	-^ --------0---
i,j∈[κ],x〜DX P(Y = j) ~	— P(Y=i)>p(Y=j),x〜DX P(Y = i) - P(Y = j)
minimizing ED['(f (X), Y) + 'CR(f (X))] is equivalent to minimizing ED['(f (X), Y)].
Proof. It is easy to check EX = 0, ∀X 〜DX when Assumption 1 holds. Thus adding Term-2
to Term-1 in (5) does not change the Bayes optimal label. With Assumption 1, the Bayes optimal
classifier on the clean distribution should satisfy f * (X)[Y] = 1, ∀(X, Y)〜D. On one hand, when
C 、	τ τ	∕πτι∕^C^∙	∙ ∖	ι
β ≥ maXi,j∈[κ],X〜DX Uj(X)∕P(Y = j), We have
,~ . - -
βij(X):= Uij(X) - βP(Y = j) ≤ 0,∀i,j ∈ [K], X 〜DX.
In this case, minimizing the regularization term results in confident predictions. On the other hand,
to make it unbiased to clean results, β could not be arbitrarily large. We need to find the upper bound
on β such that f * also minimizes the loss defined in the latter regularization term. Assume there
is no loss on confident true predictions and there is one miss-prediction on example (xn, yn = j1),
i.e., the prediction changes from the Bayes optimal prediction fxn [j1] = 1 to fxn [j2] = 1, j2 6= j1.
Compared to the optimal one, the first two terms in the right side of (5) is increased by Tjj 'o,
where `0 > 0 is the regret of one confident wrong prediction. Accordingly, the last term in the right
side of (5) is increased by (万无,无(X) 一 尸比 j2 (X))'o. It is supposed that
TjIjI 'o + (βjι,jι (Xn)- βj1,j2 (Xn))'0 ≥ 0, ∀j1,j2 ∈ [K],
which is equivalent to
β(P(Ye=j1) -P(Ye=j2)) ≤Tj1,j1(Xn) -Tj1,j2(Xn),∀j1,j2 ∈ [K].
Thus
TIjI (X)- Tj1,j2(X)
β≤
min
P(Y=jι)>p(Y=j2),X〜DX P(Y = jι) - P(Y = j2)
By mathematical inductions, it can be generalized to the case with multiple miss-predictions in the
CE term.	□
C Other Justifications
In this section, we first compare 'cr and entropy regularization in Section C.1 and highlight our
superiority with both theoretical and experimental evidence, then show an example for explaining
the variances incurred by label noise in Section C.2, and provide the risk bound in Section C.3 for
training with the sieved examples that satisfy Corollary 1.
20
Published as a conference paper at ICLR 2021
Table 4: Comparing 'cr With ER on CIFAR-10.
Method	0.2	Symm 0.4	0.6	0.1	Asymm 0.2	0.3
Baseline	86.98	81.88	74.14	90.69	88.59	86.14
Baseline + ER	87.61	83.84	80.55	91.36	89.61	87.47
Baseline + 'CR	90.70	88.29	82.10	92.41	91.02	90.53
C.1 COMPARING 'cr WITH ENTROPY REGULARIZATION
For simplicity, We consider tWo-class classification problem. Suppose for a given feature x, the
probability of x belonging to class 1 is p. The entropy regularization (ER) can be Written as:
RER(p) = -(plnp + (1 - p) ln(1 - p)),	(12)
While our regularization term is Written as:
RCR(p) = lnp+ ln(1 -p).	(13)
We have the folloWing proposition:
Proposition 1. 'CR regularizes models stronger than the entropy regularization in terms of gradi-
ents.
Proof. First notice that both RER and RCR are symmetric functions around p = 0.5. Thus We can
only consider the situation Where 0 < p < 0.5. The gradients W.r.t p are:
dRER(P) = -(ln P — ln(1 — P)) = ln(1 - 1),
∂p	p
and
∂Rcr(p) = 1 — ɪ
∂P P 1 — P .
NoW We compare the absolute value of tWo gradients. When 0 < P < 0.5, it is easy to check
dRER(P)=In(1 — 1) < 1 — 2 < 1 — ɪ= dRCR(P),
∂P	P	P P 1 — P ∂P
and both gradients are larger than 0. Therefore, 'cr has larger gradients than the entropy regulariza-
tion, i.e., 'cr has stronger regularization ability than ER.
□
We can also draW a figure to shoW this phenomenon. Figure 4 shoWs the value of RCR and RER
With respect to P. We can see the gradient of our regularization is larger than entropy regularization,
resulting in a more confident prediction. We also perform an experiment to further shoW the evi-
dence. Table 4 records comparison results Which shoW our regularization achieves higher accuracy
compared to the entropy term.
C.2 Calculating VarD ('(fD (X ),Y)) AND VarD ['(fD (X), Y) + 'cr(∕D (X))]
Consider optimal classifier fD := arg minf ED ['(f (X), Y)]. Let 'max be the upper bound of the
'(∙) loss, and 'mi∩ be the lower bound of the '(∙) loss. Denote ε by the over noise rate (ratio of
corrupted examples in all examples).
For VarD('(fD(X), Y)), we know the loss '(fD(χn),yn) = 'min for each example. Thus the vari-
ance is VarD('(fD(X),Y)) = 0.
21
Published as a conference paper at ICLR 2021
Figure 4: Comparing our regularization with entropy regularization .
ForvarD['(fD(X),Y) + 'cR(fD(X))], We know the loss '(fD(Xn)Q = yn) = 'mi∏, and the loss
'(fD(Xn), y = yn) = 'max. Note
'CR
(K ― I)'max + 'mi∏
K
for each example. The expectation is
一 ... .~、 ..................
ED['(fD(X), Y) + 'cR(fD(X))] = ε'max + (1 - ε)'mi∏ + 'CR.
Thus the variance is
- . .. . ~、 .................
VarD ['(fD (X ),Y)+ 'cr(∕D (X))]
=ε('max + 'CR - (ε'max + (1 - ε)'mi∏ + 'CR))2 + (1 - ε)('mi∏ + 'CR - (ε'max + (1 - ε)'mi∏ + 'CR))2
=ε(1 - ε)('max - 'mi∏ ) .
We k∏ow i∏ this example,
VarD ['(fD (X ),Y) + 'cR(fD (X))] = ε(1 - ε)('max - 'min)2》VarD ('(fD (X ),Y)) = 0.
C.3 Analysis for the Risk Bound
Let Dl* and Dl* be the set and the distribution of the sieved clean examples according to
Corollary 1. We k∏ow they are supposed to co∏tai∏ o∏ly clea∏ examples. Defi∏e RD(f) :=
ED['(f (X),Y)], fD := argminf RD(f), rDl* ,γ(f) :=尚 Pn∈L* [γ(xn)'(f (Xn) ,9n)],
fDeL*,γ := arg minf∈F RDeL*,γ(f), where γ(X) := PD(X)/PDeL* (X) stands for the importance
of each example to correct sample bias such that RD(f) = EDe * [γ(X)'(f(X), Y)]. The weight
γ(X) can be estimated by kernel mean matching (Huang et al., 2007) and its DNN adaption (Fang
et al., 2020). Let DeL*,X be the marginal distribution of DeL* on X. For example, with a particular
kernel Φ(X), the optimization problem is:
min	kEDX[Φ(X)] - EDeL*,X[γ(X)Φ(X)]k
s.t. γ(X) >0 and EDeL*,X[γ(X)] = 1.
Note the selection of kernel Φ(∙) is non-trivial, especially for complicated features. See (Fang et al.,
2020) for a detailed DNN solutions.
Corollary 2 provides a risk bound for minimizing CE after sample sieve.
22
Published as a conference paper at ICLR 2021
∕log(1∕δ)
V 2∣L*∣
Corollary 2. If Y ∙ ' is [0, b]-valued, thenfor any δ > 0, with probability at least 1 一 δ, we have
RD(fDL*,γ) - RD(fD) ≤ 2R(γ ◦ '◦F) + 2b
WheretheRademachercompIexity R(Y ◦ ' ◦ F):= EDL* σ[suPf∈f ∣l2^∣ Σ2n∈L* σnγ(xn)'(f (xn), yn)]
and {σn∈L*} are independent Rademacher variables.
Proof. The sieved clean examples may be biased due to the covariate shift caused by instance-
based label noise. One solution to such shift is re-weighting DeL* to match D using importance
re-weighting. Particularly, we need to estimate parameters Y(X) such that
.. .. ..........................~.-
Rd(f) = Rdl*,y(f) := Edl*[γ(X)'(f(X),Y)].
With the optimal Y(X), the ERM should be changed as
ʌ
^
DL* ,γ
arg min RDe * γ(f),
f∈F L ,γ
where
^
RDeL* ,γ
(f) := Ll X [γ(xn)'(f(xn),yn)].
n∈L*
Via Hoeffding’s inequality, ∀f, w.p. at least 1 一 δ, we have
∣rdl* ,γ (f) 一 % ,γ (f )∣ ≤ R(' ◦F)+2b jln(⅛φ.
Following the basic Rademacher bound (Bartlett & Mendelson, 2002) on the maximal deviation
between the expected empirical risks:
^
,ʌ ..
RD (fDL* ,γ)- RD (fD)
=RDeL*,γ
(f
）一
~
DL* ,γ
+
RDeL* ,γ
RDeL*,γ (fDeL*,γ) 一
RDeL*,γ
(fDeL*,γ
.^ , .. _ ,...
≤0 + 2maχ IRDL* ,γ (f ) — rDl* ,γ (f )1
f∈F
≤2R(γ ◦' ◦F ) + 2b∕Iw,
V 2|L |
^
)
where the Rademacher complexity R(γ ◦ ' ◦F) := EDL* ,σ [suPf ∈f μ2q Pn∈L* σnY(xn)'(f (xn), yn)]
and {σn∈L* } are independent Rademacher variables. Therefore, we get Corollary 2.
□
Corollary 2 informs us that, theoretically, the sample sieve is biased and Y(X) is necessary to correct
the selection bias. However, the error induced by estimating Y(X) may degrade the performance.
In addition, it is easy to check the optimal solution of performing direct ERM on the sieved clean
examples is the same as fD in expectation when Assumption 1 holds.
D More Details and Results for Experiments
We firstly show our training framework in Section D.1, then show implementation details and dis-
cussions in Section D.2. The algorithm for generating the instance-dependent label noise is provided
in Section D.3. We show more experiments in Section D.4 and the ablation study in Section D.5.
23
Published as a conference paper at ICLR 2021
D. 1 Illustration of the Training Framework
Our experiments follows the framework shown in Figure 5.
[ɪH
Figure 5: One example of CORES2. L(t): Indices of sieved clean examples. H(t): Indices of
sieved corrupted examples.。工⑴：={(xn,yn) : n ∈ L(t)}, Da(t)：= {(xn,yn) : n ∈ H(t)},
DX,H(τ) := {xn : n ∈ H(τ)}.
D.2 Implementation Details and More Analysis
Implementation details on CIFAR-10 and CIFAR-100 with instance-based label noise: The
basic hyper-parameters settings for CIFAR-10 and CIFAR-100 are listed as follows: mini-batch size
(64), optimizer (SGD), initial learning rate (0.1), momentum (0.9), weight decay (0.0005), number
of epochs (100) and learning rate decay (0.1 at 50 epochs). Standard data augmentation is applied
to each dataset. CORES2 and baseline share the same hyper-parameters setting except for α and β
in equation 2. When perform CORES2, We first train network on the dataset for 10 warm-up epochs
with only CE (Cross Entropy) loss. Then β is linearly increased from 0 to 2 for next 30 epochs and
kept as 2 for the rest of the epochs. The data selection is performed at the 30 epoch and αn,t is set
to KK Py∈[K] '(f⑴(Xn), y) + 'cR(∕(t)(xn)) in epoch-t as the paper suggests.
When performing CORES2?, we used the sieved result at epoch-40. It is worth noting that at that
time, the sample sieve may not reach the highest test accuracy. However, the division property
brought by the confidence regularizer works well at that time. We use the default setting from UDA
(Xie et al., 2019) to apply efficient data augmentation.
Implementation details on Clothing-1M: We train the network for 120 epochs on 1 million noisy
training images. Batch-size is set to 32. The initial learning rate is set as 0.01 and reduced by a
factor of 10 at 30, 60, 90 epochs. For each epoch, we sample 1000 mini-batches from the training
data while ensuring the (noisy) labels are balanced. Mixup strategy is employed to further avoid the
overfitting problem (Zhang et al., 2018; Li et al., 2020). β is set to 0 at first 80 epochs, and linearly
increased to 0.4 for next 20 epochs and kept as 0.4 for the rest of the epochs. It is worth noting
that clothing-1M actually does not satisfy our Assumption 2 since the class “Knitwear” (denoted by
class-i) and the class “Sweater” (denoted by class-j) can not satisfy Tii(X) - Tij (X) > Tii - Tjj .
Note consistency training is not implemented on clothing-1M.
More analysis on β: The value of β mainly affects the sample sieve in cORES2. From Theorem 3
and Theorem 4 in the paper, when β is set to be small, we do not have the good division property.
When β is set to be large, the training is biased to the cE term. Figure 6 visualize this phenomenon.
It can be seen that in the left and right figure, many clean examples and corrupted examples overlap
together located in the left and right clusters, respectively.
24
Published as a conference paper at ICLR 2021
xio*	lower beta
xl04 proper beta
xl04 higher beta
Clean samples ■
corrupted samples
Clean samples
corrupted samples
Clean samples
corrupted samples
0.4-
0.5
0.2-
loss
0.0
loss
loss
Figure 6: Analyzing how the value of β influences the division. We set β = 0.5,2,10 for lower,
proper, and higher beta settings, respectively.
Algorithm 1 Instance-Dependent Label Noise Generation
Input:
1:	Clean examples (xn, yn)nN=1; Noise rate: ε; Size of feature: 1 × S; Number of classes: K.
Iteration:
2:	Sample instance flip rates qn from the truncated normal distribution N(ε, 0.12, [0, 1]);
3:	Sample W ∈ RS×K from the standard normal distribution N(0, 12);
for n
4:
5:
6:
7:
8:
1 to N do
P = Xn ∙ W	// Generate instance dependent flip rates. The size of P is 1 X K.
pyn = -∞	// Only consider entries different from the true label
p = qn ∙ softmax(p)	// Let qn be the probability of getting a wrong label
pyn = 1 - qn	// Keep clean w.p. 1 - qn
Randomly choose a label from the label space as noisy label yn according to p;
end for
Output:
9: Noisy examples (Xi,yn)N=ι.
D.3 Generating the Instance-Dependent Label Noise
In this section, we introduce how to generate instance-based label noise which is illustrated in Al-
gorithm 1. Note this algorithm follows the state-of-the-art method (Xia et al., 2020). Define the
noise rate (the global flipping rate) as ε. First, in order to control ε but without constraining all of
the instances to have a same flip rate, we sample their flip rates from a truncated normal distribution
N(ε, 0.12, [0, 1]), where [0, 1] indicates the range of the truncated normal distribution. Second, we
sample parameters W from the standard normal distribution for generating instance-dependent la-
bel noise. The size of W is S × K, where S denotes the length of each feature. For each instance
(Xn, yn), we use Step 5 and Step 6 to ensure that the probability of getting a wrong label is qn. Step
7 ensures the sum of all the entries of p is 1.
Suppose there are two features: Xi and Xj where Xi = Xj . Then the possibility p of these two
features, calculated by X ∙ W, from the Algorithm 1, would be exactly the same. Thus the label noise
is strongly instance-dependent.
25
Published as a conference paper at ICLR 2021
Table 5: Comparison with the results reported by DivideMix (Li et al., 2020) on CIFAR-10. All
methods use Pre-ResNet18 as the backbone. The last epoch test accuracy for each method is re-
ported. The noise rate is defined as the probability of replacing the label with other labels including
the true label.
Dataset	Method	Symm	
		0.2	0.5
	CE	^8Σ7^	-37.9-
	Bootstrap (Reed et al., 2014)	82.9	58.4
	Forward T (Patrini et al., 2017)	83.1	59.4
	Co-teaching+ (Yu et al., 2019)	88.2	84.1
CIFAR-10	MixUP (Zhang et al., 2018)	92.3	77.6
	P-Correction (Yi & Wu, 2019)	92.0	88.7
	Meta-Learning (Li et al., 2019)	92.0	88.8
	M-correction (Arazo et al., 2019)	93.8	91.9
	DivideMix (Li et al., 2020)	95.7	94.4
		CORES2?		95.9	94.5
Table 6: The best epoch accuracy for each method on Tiny-ImageNet.
Dataset	Model	Method	Symm 0.2	0.5
Tiny-ImageNet	ResNet18	MAE (GhOsh etal., 2017) GCE (Zhang & Sabuncu, 2018) MentorNet (Jiang et al., 2017) 	CORES2?		2.36	1.22= 69.84	66.31 59.12	53.83 73.47	71.07
Table 7: Comparing CORES2 (without consistency training) with other noise-robust methods on
CIFAR-10.
Method	0.2	Symm 0.4	0.6	0.1	Asymm 0.2	0.3
Cross Entropy	86.98	81.88	74.14	90.69	88.59	86.14
Forward T (Patrini et al., 2017)	88.11	83.27	75.34	90.11	89.42	88.25
Truncated Lq (Zhang & Sabuncu, 2018)	89.70	87.62	82.70	90.43	89.45	87.10
LDMI (Xu et al., 2019)	88.74	83.04	76.51	90.28	89.04	87.88
CORES2 (without consistency training)	90.70	88.29	82.10	92.41	91.02	90.53
D.4 More Experiments on CIFAR-10 and Tiny-Imagenet
In this section, we compare CORES2 with more methods on CIFAR-10 and Tiny-Imagenet. Table
5 records the comparison results with recent benchmark methods. Table 6 compares CORES2 with
other methods on Tiny-ImageNet. Both tables show that CORES2 achieves competitive results.
D.5 Ablation Study
CORES2 (without consistency training): By optimizing loss in (2), the model can be forced to
concentrate only on clean examples. Thus even without consistency training, the network trained by
CORES2 is also noise-robust. Table 7 compares CORES2 with other noise-robust methods which
do not apply semi-supervised setting in the framework. We can see CORES2 still achieves the best
performance among all the methods.
CORES2 without confidence regularization or dynamic data selection: The loss in equation 2
consists of data selection strategy and confident regularization term. To see how they influence the
final accuracy, we perform the ablation study to show their effect on Table 8. The first row of Table
8 corresponds to the traditional CE loss. The second row corresponds to the sample sieve with CE
26
Published as a conference paper at ICLR 2021
Table 8: Analysis of each component of CORES2 on CIFAR-10. All the methods use ResNet-34.
Sample Sieve Data selection Regularization		Consistency training	0.2	Symm 0.4	0.6	0.1	Asymm 0.2	0.3
×	×	×	86.67	81.44	74.63	90.18	88.43	87.27
X	×	×	90.15	86.98	78.36	91.59	90.89	88.51
X	X	×	90.70	88.29	82.10	92.41	91.02	90.53
X	X		X		95.73	93.76	89.78	96.05	95.18	94.67
loss. The third row is the typical CORES2. The last row is CORES2?. We can see both the dynamic
sample sieve in (4) and the confidence-regularized model update in (3) show positive effects on the
final accuracy, which suggests the rationality of CORES2 .
27