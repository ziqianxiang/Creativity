Published as a conference paper at ICLR 2021
Combining Physics and Machine Learning for
Network Flow Estimation
Arlei Silva, Furkan Kocayusufoglu
Computer Science Department, UC Santa Barbara, CA 93106-5110, USA
Saber Jafarpour, Francesco Bullo
Mechanical Engineering Department and the Center of Control, Dynamical Systems and
Computation, UC Santa Barbara, CA 93106-5070, USA
Ananthram Swami
U.S. Army Research Lab, Adelphi, MD 20783, USA
Ambuj Singh
Computer Science Department, UC Santa Barbara, CA 93106-5110, USA
Ab stract
The flow estimation problem consists of predicting missing edge flows in a net-
work (e.g., traffic, power, and water) based on partial observations. These missing
flows depend both on the underlying physics (edge features and a flow conser-
vation law) as well as the observed edge flows. This paper introduces an opti-
mization framework for computing missing edge flows and solves the problem
using bilevel optimization and deep learning. More specifically, we learn regu-
larizers that depend on edge features (e.g., number of lanes in a road, resistance
of a power line) using neural networks. Empirical results show that our method
accurately predicts missing flows, outperforming the best baseline, and is able to
capture relevant physical properties in traffic and power networks.
1	Introduction
In many applications, ranging from road traffic to supply chains to power networks, the dynamics
of flows on edges of a graph is governed by physical laws/models (Bressan et al., 2014; Garavello
& Piccoli, 2006). For instance, the LWR model describes equilibrium equations for road traffic
Lighthill & Whitham (1955); Richards (1956). However, it is often difficult to fully observe flows
in these applications and, as a result, they rely on off-the-shelf machine learning models to make
predictions about missing flows (Li et al., 2017; Yu et al., 2018). A key limitation of these machine
learning models is that they disregard the physics governing the flows. So, the question arises: can
we combine physics and machine learning to make better flow predictions?
This paper investigates the problem of predicting missing edge flows based on partial observations
and the underlying domain-specific physics defined by flow conservation and edge features (Jia et al.,
2019). Edge flows depend on the graph topology due to a flow conservation law—i.e. the total in-
flow at every vertex is approximately its total out-flow. Moreover, the flow at an edge also depends
on its features, which might regularize the space of possible flow distributions in the graph. Here,
we propose a model that learns how to predict missing flows from data using bilevel optimization
(Franceschi et al., 2017) and neural networks. More specifically, features are given as inputs to a
neural network that produces edge flow regularizers. Weights of the network are then optimized via
reverse-mode differentiation based on a flow estimation loss from multiple train-validation pairs.
Our work falls under a broader effort towards incorporating physics knowledge to machine learning,
which is relevant for natural sciences and engineering applications where data availability is lim-
ited (Rackauckas et al., 2020). Conservation laws (of energy, mass, momentum, charge, etc.) are
1
Published as a conference paper at ICLR 2021
essential to our understanding of the physical world. The classical Noether’s theorem shows that
such laws arise from symmetries in nature (Hanc et al., 2004). However, flow estimation, which
is an inverse problem (Tarantola, 2005; Arridge et al., 2019), is ill-posed under conservation alone.
Regularization enables us to apply domain-knowledge in the solution of inverse problems.
We motivate our problem and evaluate its solutions using two application scenarios. The first is road
traffic networks (Coclite et al., 2005), where vertices represent locations, edges are road segments,
flows are counts of vehicles that traverse a segment and features include numbers of lanes and speed
limits. The second scenario is electric power networks (Dorfler et al., 2018), where vertices represent
power buses, edges are power lines, flows are amounts of power transmitted and edge features
include resistances and lengths of lines. Irrigation channels, gas pipelines, blood circulation, supply
chains, air traffic, and telecommunication networks are other examples of flow graphs.
Our contributions can be summarized as follows: (1) We introduce a missing flow estimation prob-
lem with applications in a broad class of flow graphs; (2) we propose a model for flow estimation that
is able to learn the physics of flows by combining reverse-mode differentiation and neural networks;
(3) we show that our model outperforms the best baseline by up to 18%; and (4) we provide evi-
dence that our model learns interpretable physical properties, such as the role played by resistance
in a power transmission network and by the number of lanes in a road traffic network.
2	Flow estimation Problem
We introduce the flow estimation problem, which consists of inferring missing flows in a network
based on a flow conservation law and edge features. We provide a list of symbols in the Appendix.
Flow Graph. Let G(V, E, X) be a flow graph with vertices V (n= |V |), edges E (m= |E |), and edge
feature matrix X ∈ Rm×d, where X [e] are the features of edge e. A flow vector f ∈ Rm contains
the (possibly noisy) flow fe for each edge e ∈ E . In case G is directed, f ∈ R+m, otherwise, a flow is
negative if it goes against the arbitrary orientation of its edge. We assume that flows are induced by
the graph, and thus, the total flow—in plus out—at each vertex is approximately conserved:
f(vi,u) ≈	f(u,vo), ∀u ∈ V
(vi,u)∈E	(u,vo)∈E
In the case of a road network, flow conservation implies that vehicles mostly remain on the road.
Flow Estimation Problem. Given a graph G(V, E, X) with partial flow observations f ∈ Rm0 for a
subset E0 ⊆ E of edges (fe is the flow for e ∈ E0, m0 = |E01 < m), predict flows for edges in E \E0.
In our road network example, partial vehicle counts f might be measured by sensors placed at a few
segments, and the goal is to estimate counts at the remaining segments. One would expect flows not
tobe fully conserved in most applications due to the existence of inputs and outputs, such as parking
lots and a power generators/consumers. In case these input and output values are known exactly,
they can be easily incorporated to our problem as flow observations. Moreover, if they are known
approximately, we can apply them as priors (as will be detailed in the next section). For the remain-
ing of this paper, we assume that inputs and outputs are unknown and employ flow conservation as
an approximation of the system. Thus, different from classical flow optimization problems, such as
min-cost flow (Ahuja et al., 1988), we assume that flows are conserved approximately.
Notice that our problem is similar to the one studied in Jia et al. (2019). However, while their
definition also assumes flow conservation, it does not take into account edge features. We claim that
these features play important role in capturing the physics of flows. Our main contribution is a new
model that is able to learn how to regularize flows based on edge features using neural networks.
3	Our Approach: Physics+Learning
In this section, we introduce our approach for the flow estimation problem, which is summarized
in Figure 1. We formulate flow estimation as an optimization problem (Section 3.1), where the
interplay between the flow network topology and edge features is defined by the physics of flow
graphs. Flow estimation is shown to be equivalent to a regularized least-squares problem (Section
2
PUbliShed as a COnferenCePaPer at ICLR 2021
ParanIererS φ
V ObSerVed ——⅛ MiSSing .f Validato∙n -1■1 PrediCred flow
F=w graph
Edge regu≡r-zers
KIfOεCroSS VaHdatioll
F=w estimation
FigUrerSUInmary Of the proposed approach for PrediCting missing nows5a graph based On
PartiaI ObSerVations and edge featurewe Iearncombe features and a nl COnSerVatn IaWS
WhiCh together define the PhySiCS Of the nl graph，A regularization function*J ①)modeled as
a neural network With ParameterS ① takes asPUt edge features N• A nl estimation algorithm
applies the regularizationy PartiaI ObSerVations (prior flows (X(O)) and flow COnSerVatn to PrediCt
IniSSing flows x∙ NetWOrk ParameterS ① are Iearned based On a K—fold CrOSS Validatn IOSS With
respec 二 o Validatn flows * OUrmOdeliS trained end—to—end USing reverse—mode differentiation，
∙2∙MOreOVeLWe describe hl the effect Of edge features and the graph topology can be Iearned
from data USing bilevel OPtimiZatn and neural networksSeCtnnlyswe propose a
reverse—mode differentiation algorithm for flow estimation in SeCtn 34
3」FLoW ESTlMATIoN VlA OPTIMIZATIoN
The extentWhiCh flow COnSerVatn holds for nowsa graph is known as divergence and Can be
measured USing the Orien-ed incidence ma-riX B ∈ Of 9∙ The matriX is defined as follows”
u 1 if 山SUCh that= (Vi”U) ∈ ”u ——1 if 山SUCh that= (fw)” a∩d=
OtherWiSGiven B and f" the divergence at a VerteX U Can be COmPUted aκ
IH M f-l M (1)
(e-u)∈8 (Eeo)∈g
And thus" we Can COmPUte the total (SqUared) divergence in the graph as =f=6 H fττf U
G∈H((f)G)one COUId try to solve the flow estimation ProbIem by IninimiZing =f=6 WhiIe
keeping the ObSerVed flows fixed" however" this problem is ill—POSed——there Inight be mtipie su—
tns to the OPtimiZatn∙ The Standard approach in SUCh a scenario is to resortregularization，In
PartiCUlary We apply a generic regularization function e With ParameterS ① as followK
f* Uargmin =Ef=6 +e(f∙⅛f≡①)st.>u>>∀e m 8、 (2)
f∈
Where Q is the domain Of f“ f (O) ∈ 另 Fa Prr for nows"『6 Ge) are entries Of f (f) for edge
e and the COnStraint guarantees that ObSerVed nls are not changed，Priors f (not be COnfUSed
With ObSerVed nls LShoUld be Set according to the application (e.g: as zee based On a blacbOX
model S historical dataRegarding the domainwe COnSider Q =刃 F and Q =刃The SeCOnd
CaSe is relevant for directed graphs——When nls must fll edge Orientations (g: traffic
In Jia et• (2019L the authors Set e(f" X" f (O)】①)as AK P regarizatn Parameter A WhiCh
implies a UnifOnn ZerOPrr Withan T Penalty OVer edges，We Claim that the regularization function
ays an important re in CaPtUring the PhySiCS Of flow graphAS an example" for a POWer network"
e should account for the resistance Of Iile Iines，ThUSsWePrOPOSe Iearning the regularization from
data，OUr approach is based On a leasSqUareS formulationy WhiCh Wi= be described nex
Published as a conference paper at ICLR 2021
3.2	Regularized Least-squares Formulation
Flow estimation problem can be viewed as an inverse problem (Tarantola, 2005). Let x ∈ Rm-m0
be the vector of missing flows and H ∈ Rm×m-m0 be a matrix such that Hij = 1 if fi maps to
xj (i.e., they are associated to the same edge), and Hi,j = 0, otherwise. Moreover, let f ∈ Rm be
such that fe = fe if e ∈ E0 and fi = 0, otherwise. Using this notation, we define flow estimation as
BHx = -Bf + , where BH is a forward operator, projecting x to a vector of vertex divergences,
and -Bef + is the observed data, capturing (negative) vertex divergences for observed flows. The
error can be interpreted as noise in observations or some level of model misspecification.
We can also define a regularized least-squares problem with the goal of recovering missing flows x:
x* = arg min ||BHX + Be||2 + ||x - x(0)∣∣Q(χ⑼	(3)
x∈Ω0
where Ω0 is a projection of the domain of f to the space of x, ∣∣x∣∣M = XTMX is the matrix-
scaled norm of x and x(0) ∈ Rm-m0 are priors for missing flows. The regularization function
Φ(f, X; f(0), Θ) has the form ||x - x(0)llQ(χ e), where the matrix Q(X; Θ) is a function of param-
eters Θ and edge features X. We focus on the case where Q(X; Θ) is non-negative and diagonal.
Equation 3 has a Bayesian interpretation, with x being a maximum likelihood estimate under a
Gaussian assumption—i.e., X 〜N(X(O), Q(X; Θ)-1) and Be 〜N(0, I) (Tarantola, 2005). Thus,
Q(X; Θ) captures the variance in flow observations f in prior estimates f(0) compared to the one.
This allows the regularization function to adapt to different edges based on their features. For in-
stance, in our road network example, Q(X; Θ) might place a lower weight on flow conservation for
flows at a road segment with a small number of lanes, which are possible traffic bottlenecks.
Given the least-squares formulation described in this section, how do we model the regularization
function Q and learn its parameters Θ? We would like Q to be expressive enough to be able to
capture complex physical properties of flows, while Θ to be computed accurately and efficiently.
We will address these challenges in the remaining of this paper.
3.3	Bilevel Optimization for Meta-learning the Physics of Flows
This section introduces a model for flow estimation that is able to learn the regularization function
Q(X; Θ) in Equation 3 from data using bilevel optimization and neural networks.
Bilevel formulation. We learn the parameters Θ that determine the regularization function Q(X; Θ)
using the following bilevel optimization formulation:
Θ* = arg min E[||X 一 x*||2]	(4)
Θ
st. X* = argmin ||BHX + Be||2 + ||x 一 xo∣∣Q(χ⑼	(5)
x∈Ω0
where the inner (lower) problem is the same as Equation 3 and the outer (upper) problem is the
expected loss with respect to ground truth flows X——which we estimate using cross-validation.
Notice that optimal values for parameters Θ and missing flows x are both unknown in the bilevel
optimization problem. The expectation in Equation 4 is a function of multiple instances of the inner
problem (Equation 5). Each inner problem instance has an optimal solution x* that depends on pa-
rameters Θ. In general, bilevel optimization is not only non-convex but also NP-hard (Colson et al.,
2007). However, recent gradient-based solutions for bilevel optimization have been successfully ap-
plied to large-scale problems, such as hyper-parameter optimization and meta-learning (Franceschi
et al., 2018; Lorraine et al., 2020). We will first describe how we model the function Q(X; Θ) and
then discuss how this problem can be solved efficiently using reverse-mode differentiation.
We propose to model Q(X; Θ) using a neural network, where X are inputs, Θ are learnable weights
and the outputs are diagonal entries of the regularization matrix. This is a natural choice due to the
expressive power of neural nets (Cybenko, 1989; Xu et al., 2018).
Multi-Layer Perceptron (MLP). An MLP-based Q(X; Θ) has the following form:
Q(X; Θ) = diag(M LP (X; Θ))	(6)
4
Published as a conference paper at ICLR 2021
where MLP(X; Θ) ∈ Rm-m0. For instance, Q(X; Θ) can be a 2-layer MLP:
Q(X; Θ) = diag(a(b(XW(1))W(2)))	(7)
where Θ = {W(1), W(2)}, W(1) ∈ Rd×h, W(2) ∈ Rh×1, h is the number of nodes in the hidden
layer, both a and b are activation functions, and the bias was omitted for convenience.
Graph Neural Network (GNN). The MLP-based approach assumes that each entry [Q(X ; Θ)]e,e
associated to an edge e is a function of its features X [e] only. However, we are also interested in how
entries [Q(X; Θ)]e,e might depend on the features of neighborhood of e in the flow graph topology.
Thus, we consider the case where Q(X; Θ) is a GNN, which is described in the Appendix.
3.4 Flow Estimation Algorithm
We now focus on how to solve our bilevel optimization problem (Equations 4 and 5). Our solution
applies gradient-based approaches (e.g., SGD (Bottou & Bousquet, 2008), Adam (Kingma & Ba,
2014)) and, for simplicity, our description will be based on the particular case of Gradient Descent
and assume a zero prior (x(0) = 0). A key challenge in our problem is to efficiently approximate the
gradient of the outer objective with respect to the parameters Θ, which, by the chain rule, depends
on the gradient of the inner objective with respect to Θ.
We first introduce extra notation to describe the outer problem (Equation 4). Let (fk, gk) be one of K
train-validation folds, both containing ground-truth flow values, such that fk ∈ Rp and gk ∈ Rq. For
each fold k, we apply the inner problem (Equation 5) to estimate missing flows xk. Estimates for all
folds are concatenated into a single vector X = [xi； x2;...; XK ] and the same for validation sets g =
叵 i； g2；... gK ]. We define a matrix R ∈ Rq×(m-m0) such that Rij = 1 if prediction Xj corresponds
to validation flow gi and Rij = 0, otherwise. Using this representation, we can approximate the
expectation in the outer objective as Ψ(x, Θ) = (1/K)||Rx - g||2, where X depends implicitly on
Θ. We also introduce ΥΘ (x) as the inner problem objective. Moreover, let Γj(xk,j-1, Θi-1) be
one step of gradient descent for the value of Xk at iteration j with learning rate β :
rj (xk,j-1, θi—1) = xk,j-1 - β^ x γθ (xk,j )
=xk,j-ι - 2β[H]Bl(BHkxk,j-1 + Bek) + 2Qkxkj_1]
where Hk, Qk and fk are the matrix H, a sub-matrix of Q(X; Θi-1) and the observed flows vector
f (see Section 3.2) for the specific fold k. We have assumed the domain (Ω0) of flows XkjtO be the
set of real vectors. For non-negative flows, we add the appropriate proximal operator to Γj.
Our algorithm applies Reverse-Mode Differentiation (RMD) (Domke, 2012; Franceschi et al., 2017)
to estimate ▽㊀ Ψ and optimizes Θ also using an iterative algorithm. The main idea ofRMD is to first
unroll and store a finite number of iterations for the inner problem x1, x2, . . . xJ and then reverse
over those iterations to estimate ▽㊀Ψ, which is computed as follows:
J / J
VxJ,θΨ(xj, Θi) = Vχψ(xj, Θi) X I Y
j=1	s=j+1
∂Γs(xs-1, Θi)
∂xs-1
drj (Xj-1, θi)
∂Θ
In particular, our reverse iteration is based on the following equations:
VχΨ(xj, Θi) = (2∕K)Rt(Rxj - g)
∂Γs(Xs-1, Θi)
∂Xs-1
∂Γj(Xj-1, Θi)
∂Θ
I - 2β(HTBTBH + 2Q(X; Θi))
-4β(∂Q(X ;Θi )∕∂ Θ)xj-1
where ∂ Q(X; Θi)∕∂Θ is the gradient of the regularization function Q(X; Θ) evaluated at Θi. In our
case, this gradient is the same as the neural network gradients and is omitted here for convenience.
Algorithm 1 describes our RMD approach for flow estimation. It receives as inputs the flow net-
work G(V, E, X), K train-validation folds {(fk, gk)}3「and also hyperparameters T, J, α, and β,
5
Published as a conference paper at ICLR 2021
Algorithm 1 RMD Algorithm for Flow Estimation
Require: Flow network G (V, E, X), train-validation folds { fk, gk)} 3ι，number of outer iterations
T and inner iterations J, learning rates α and β
Ensure: Regularization parameters Θ
1:	Initialize parameters Θ0
2:	g - [gi；…gκ]
3:	B J incidence matrix of G
4:	for outer iterations i = 1, . . . T do
5:	Initialize missing flows xk,0 for all k
6:	for inner iterations j = 1, . . . J do
7:	for folds k = 1, . . . K do
8:	xk,j J Xkj-I- 2β[H∣Bl(BHkXk,j-ι + Bfk) + 2QkXkj-1]
9:	end for
10:	Xj J [X1,j； . . .XK,j]
11:	end for
12:	ZJ J (2/K)RT(RXJ - g)
13:	for reverse inner iterations j = J - 1, . . . 1 do
14:	JJJ- 4βzj+ι(∂Q(X; θi-ι)∕∂⑼Xj+i
15:	Zj J Zj+ι[I - 2β(H 1B1BH + Q(X;5-i))]
16:	end for	J
17:	Update θi J θi-i - αJθ-
18:	end for
19:	return parameters θI
corresponding to the number of outer and inner iterations, and learning rates for the outer and inner
problem, respectively. Its output is a vector of optimal parameters θ for the regularization function
Q(X； θ) according to the bilevel objective in Equations 4 and 5. We use θ to indicate our estimate
of ▽㊀Ψ(θi). Iterations of the inner problem are stored for each train-validation fold in lines 4-12.
Reverse steps, which produce an estimate Jθ-, are performed in lines 13-17. We then use Jθ- to update
our estimate of θ in line 17. The time and space complexities of the algorithm are O(TJKm) and
O(J m), respectively, due to the cost of computing and storing the inner problem iterations.
As discussed in the previous section, bilevel optimization is non-convex and thus we cannot guar-
antee that Algorithm 1 will return a global optima. In particular, the learning objective of our
regularization function Q(X； θ) is non-convex—it is a neural network. However, the inner problem
(Equation 5) in our formulation has a convex objective (least-squares). In Franceschi et al. (2018),
the authors have shown that this property implies convergence. We also find that our algorithm often
converges to a good estimate of the parameters in our experiments.
4 Experiments
We evaluate our approaches for the flow estimation problem using two real datasets and a represen-
tative set of baselines and metrics. Due to space limitations, we provide an extended version of this
section, with more details on datasets, experimental settings, and additional results in the Appendix.
4.1 Datasets
This section summarizes the datasets used in our evaluation. We normalize flow values to [0, 1] and
map discrete features to real vector dimensions using one-hot encoding.
Traffic: Vertices represent locations and directed edges represent road segments between two lo-
cations in Los Angeles County, CA. Flows are daily average vehicle counts measured by sensors
placed along highways in the year 2018. We assign each sensor to an edge in the graph based on
proximity and other sensor attributes. Our road network covers the Los Angeles County area, with
5, 749 vertices, 7, 498 edges, of which 2, 879 edges (38%) have sensors. The following features
were mapped to an 18-dimensional vector: lat-long coordinates, number of lanes, max-speed, and
6
Published as a conference paper at ICLR 2021
highway type (motorway, motorway link, trunk, etc.), in-degree, out-degree, and centrality (PageR-
ank). The in-degree and centrality of an edge are computed based on its source vertex. Similarly,
the out-degree of an edge is the out-degree of its target vertex.
Power: Vertices represent buses in Europe, undirected edges are power transmission lines and edge
flows measure the total active power (in MW) being transmitted through the lines. The dataset is
obtained from PyPSA-EUr (HorsCh et al., 2018; Brown et al., 2017)—an optimization model of the
European power transmission system—which generates realistic power flows based on solutions of
optimal linear power flow problems with historical prodUction and consUmption data. DefaUlt valUes
were applied for the PyPSA-EUr settings. The resUlting graph has 2,048 vertices, 2,729 edges, and
14-dimensional featUre vectors captUring resistance, reactance, length, and nUmber of parallel lines,
nominal power, edge degree etc. Please see the Appendix for more details.
4.2	Experimental Settings
Evaluation metrics: We apply Pearson’s correlation (CORR), Mean AbsolUte Percentage Error
(MAPE), Mean AbsolUte Error (MAE), and Root Mean SqUared Error (RMSE) to compare groUnd-
trUth and predicted flows. These metrics are formally defined in the Appendix.
Baselines: Divergence minimization (Div) (Jia et al., 2019) maximizes flow conservation Using a
single regUlarization parameter λ, which we optimize Using line search in a validation set of flows.
MUlti-Layer Perceptron (MLP) is a 2-layer neUral network with ReLU activations for all layers
that learns to predict flows based on edge featUres. Graph ConvolUtional Network (GCN) is a 2-
layer graph neUral network, also with ReLU activations and Chebyshev convolUtions of degree 2,
that learns to predict the flows Using both edge featUres and the topology bUt disregarding flow
conservation (Kipf & Welling, 2016; Defferrard et al., 2016). We also consider two hybrid baselines.
MLP-Div applies the predictions from MLP as priors to Div. Similarly, predictions from GCN are
Used as priors for GCN-Div. For both hybrid models, we also optimize the parameter λ.
Our approaches: We consider three variations of Algorithm 1. However, one important modifica-
tion is that we perform the reverse iterations for each fold—i.e., folds are treated as batches in SGD.
Bil-MLP and Bil-GCN apply oUr reverse-mode differentiation approach Using an MLP and a GCN
as a regUlarizer. Moreover, both approaches Use zero as the prior x(0) . Bil-GCN-Prior applies the
GCN predictions as flow priors. ArchitectUres of the neUral nets are the same as the baselines.
4.3	Flow Estimation Accuracy
Table 1 compares oUr methods and the baselines in terms of several metrics Using the Traffic and
Power datasets. ValUes of CORR achieved by MLP and GCN for Traffic are missing becaUse they
were Undefined—they have generated predictions with zero variance for at least one of the train-test
folds. All methods sUffer from high MAPE errors for Power, which is dUe to an over-estimation of
small flows. Bil-GCN achieves the best resUlts in both datasets in terms of all metrics, with 6% and
18% lower RMSE than the best baseline for Traffic and Power, respectively. However, notice that
Bil-MLP and Bil-GCN achieve very similar performance for Power and Bil-GCN-Prior does not
oUtperform oUr other methods. We also show scatter plots with the trUe vs. predicted flows for some
of the best approaches in FigUre 2. Traffic has shown to be the more challenging dataset, which can
be explained, in part, by training data sparsity—only 38% of edges are labeled.
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(a) GCN, Traffic
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(b) Bil-GCN, Traffic
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(c) Div, Power
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(d) Bil-GCN, Power
Figure 2:	Scatter plots with true (x) and predicted (y) flows for two approaches on each dataset. The
results are consistent with Table 1 and show that our methods are more accurate than the baselines.
7
Published as a conference paper at ICLR 2021
	Traffic				Power			
Method	RMSE	MAE	MAPE	CORR	RMSE	MAE	MAPE	CORR
Div =	0.071	0.041	1.23	0.76	0.034	0.015	1419.2	0.93
MLP	0.083	0.055	1.13	-	0.069	0.043	8334.5	0.61
GCN	0.066	0.040	0.94	-	0.064	0.043	5622.3	0.64
-MLP-Div-	0.066	0.041	1.51	^^0.81	0.033	0.015	1593.5	^^0.93
GCN-Div	0.071	0.048	1.69	0.81	0.033	0.015	1795.2	0.93
Bil-MLP	0.069	0.038	1.05	^^0.79	0.027	0.011	758.0	^^0.95
Bil-GCN	0.062	0.034	0.86	0.82	0.027	0.011	788.5	0.95
Bil-GCN-PriOr	0.062	0.035	0.91	0.82	0.027	0.011	691.5	0.95
Table 1: Average flow estimation accuracy for the baselines (Div, MLP and GCN) and our methods
(Bil-MLP, Bil-GCN and Bil-GCN-Prior) using the Traffic and Power datasets. RMSE, MAE and
MAPE are errors (the lower the better) and CORR is a correlation (the higher the better). Values of
correlation for MLP and GCN using Traffic were undefined. Bil-GCN (ours) outperforms the best
baseline for all the metrics, with up to 20% lower RMSE than Div using Power.
4.4 Analysis of Regularizers
Figure 3 illustrates the regularization function learned by Bil-MLP. We focus on Bil-MLP because
it can be analyzed independently of the topology. Figures 3a-3c show scatter plots where the x
and y axes represent the value of the regularizer and features, respectively. For Power, Bil-MLP
captures the effect of resistance over flows (Fig. 3a). However, only high values of resistance are
mostly affected—that is the reason few points can be seen and also explains the good results for
Div. We did not find a significant correlation for other features, with the exception of reactance,
which is related to resistance. For Traffic, the model learns how the number of lanes constrains the
flow at a road segment (Fig. 3b). Results for speed limit are more surprising, 45mph roads are
less regularized (Fig. 3c). This is evidence that regularization is affecting mostly traffic bottlenecks
in highways—with few lanes but a 65mph speed limit. To further investigate this result, we also
show the regularizers over the Traffic topology in Figure 3d. High regularization overlaps with well-
known congested areas in Los Angeles, CA (e.g., Highway 5, Southeast). These results are strong
evidence that our methods are able to learn the physics of flows in road traffic and power networks.
(got xsJu"--』
-----MoSt POlntS are here
0.1	0.2	0.3	0.4
Regularizer
(a) Resistance, Power
(b) Lanes, Traffic
Regularizer
(c) Speed limit, Traffic
卜75
I 1.50
1.25
1.00
0.75
0.50
10.25
0.00
(d) Visualization, Traffic
Figure 3:	Edge regularizer learned by Bil-MLP vs. features values (a-c) and visualization of regular-
izers on the Traffic topology (d). Our model is able to learn the effect of the resistance for Power. In
Traffic, a higher number of lanes is correlated to less regularization and lower speed roads (45mph)
are less regularized. The regularization is also correlated with congested areas in Los Angeles, CA.
5 Related Work
Flow graphs are quite ubiquitous in engineering, biomedical and social sciences. Two important
properties of flow graphs are that their state space is defined by a graph topology and their dynamics
are governed by the physics (or logic) of the problem of interest. We refer to Bressan et al. (2014)
for a unified characterization of the mathematical treatment of flow graphs. Notice that these studies
do not address the flow inference problem and their applications to real data is limited (Herrera et al.,
8
Published as a conference paper at ICLR 2021
2010; Work et al., 2010). Moreover, we focus on long term flows (e.g. daily vehicle traffic flows)
and not on the dynamics. This simplifies the equations of our model to the conservation law.
Flow inference via divergence minimization was originally proposed in Jia et al. (2019). However,
their work has not considered edge features and instead applied a single regularization parameter
to the norm of the flow vector f in Equation 2. Our work leverages relevant edge features to learn
the interplay between flow conservation and local predictions (priors). Thus, we generalize the
formulation from Jia et al. (2019) to the case of a learnable regularization function Q(Θ, X). Our
experiments show that the proposed approach achieves superior results in two datasets.
Flow optimization problems, such as min-cost flow, max-flow and multi-commodity flow, have a
long history in computer science (Ahuja et al., 1988; Ford Jr & Fulkerson, 2015). These problems
impose flow conservation as a hard constraint, requiring full knowledge of source and sink vertices
and noiseless flow observations. Our approach relaxes these requirements by minimizing the flow
divergence (see Equation 2). Moreover, our problem does not assume edge capacities and costs.
The relationship between flow estimation and inverse problems is of particular interest due to the
role played by regularization (Engl et al., 1996) in the solution of ill-posed problems. Recent work
on inverse problems has also focused on learning to regularize based on data and even learning the
forward operator as well—see Arridge et al. (2019) for a review. The use of the expression “learning
the physics” is also popular in the context of the universal differential equation framework, which
enables the incorporation of domain-knowledge from scientific models to machine learning (Raissi
et al., 2019; Long et al., 2018; Rackauckas et al., 2020).
Bilevel optimization in machine learning has been popularized due its applications in hyperparam-
eter optimization (Bengio, 2000; Larsen et al., 1996). In the last decade, deep learning has moti-
vated novel approaches able to optimize millions of hyperparameters using gradient-based schemes
(Maclaurin et al., 2015; Lorraine et al., 2020; Pedregosa, 2016). Our flow estimation algorithm
is based on reverse-mode differentiation, which is a scalable approach for bilevel optimization
(Franceschi et al., 2017; Domke, 2012; Maclaurin et al., 2015). Another application of bilevel opti-
mization quite related to ours is meta-learning (Franceschi et al., 2018; Grefenstette et al., 2019).
Our problem is also related to semi-supervised learning on graphs (Zhu et al., 2003; Belkin et al.,
2006; Zhou et al., 2004), which is the inference of vertex labels given partial observations. These
approaches can be applied for flow estimation via the line graph transformation (Jia et al., 2019).
The duality between a recent approach for predicting vertex labels Hallac et al. (2015) and min-cost
flows was shown in Jung (2020). However, the same relation does not hold for flow estimation.
Graph neural network models, which generalize deep learning to graph data, have been shown to
outperform traditional semi-supervised learning methods in many tasks (Kipf & Welling, 2016;
Hamilton et al., 2017; VeliCkovic et al., 2018). These models have also been applied for traffic
forecasting (Li et al., 2017; Yu et al., 2018; Yao et al., 2019). Different from our approach, tradi-
tional GNNs do not conserve flows. We show that our models outperform GNNs at flow prediction.
Moreover, we also apply GNNs as a regularization function in our model.
6 Conclusions
We have introduced an approach for flow estimation on graphs by combining a conservation law and
edge features. Our model learns the physics of flows from data by combining bilevel optimization
and deep learning. Experiments using traffic and power networks have shown that the proposed
model outperforms a set of baselines and learns interpretable physical properties of flow graphs.
While we have focused on learning a diagonal regularization matrix, we want to apply our frame-
work to the case of a full matri. We are also interested in combining different edge measurements
in order to learn more complex physical laws, such as described by the fundamental diagram in the
LWR model Lighthill & Whitham (1955); Daganzo (1994; 1995); Garavello & Piccoli (2006).
Acknowledgements
Research partially funded by the grants NSF IIS #1817046 and DTRA #HDTRA1-19-1-0017.
9
Published as a conference paper at ICLR 2021
References
Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network flows. 1988.
1-t ∙	A ∙ 1	1 ʌ	ɪ jγ	zʌ	Kl .	1 z-x	1	1 ʌ ∙1 ♦	1-t	1 .. -I ∙	1	1-t -I ∙	♦	Fl
Simon Arridge, Peter Maass, Ozan Oktem, and Caro山-Bibiane Schonlieb. Solving inverse problems
using data-driven models. Acta Numerica, 28:1-174, 2019.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of Machine Learning Research,
7(Nov):2399-2434, 2006.
Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889-
1900, 2000.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161-168, 2008.
Alberto Bressan, Suncica Canic, Mauro Garavello, Michael Herty, and Benedetto Piccoli. Flows on
networks: recent results and perspectives. EMS Surveys in Mathematical Sciences, 1(1):47-111,
2014.
Tom Brown, Jonas Horsch, and David Schlachtberger. Pypsa: Python for power system analysis.
arXiv preprint arXiv:1707.09913, 2017.
Giuseppe Maria Coclite, Mauro Garavello, and Benedetto Piccoli. Traffic flow on a road network.
SIAM Journal on Mathematical Analysis, 36(6):1862-1886, 2005.
Beno^t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of
operations research, 153(1):235-256, 2007.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Carlos F Daganzo. The cell transmission model: A dynamic representation of highway traffic con-
sistent with the hydrodynamic theory. Transportation Research Part B: Methodological, 28(4):
269-287, 1994.
Carlos F Daganzo. The cell transmission model, part ii: network traffic. Transportation Research
Part B: Methodological, 29(2):79-93, 1995.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and
Statistics, pp. 318-326, 2012.
Florian Dorfler, John W Simpson-Porco, and Francesco Bullo. Electrical networks and algebraic
graph theory: Models, properties, and applications. Proceedings of the IEEE, 106(5):977-1005,
2018.
Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems,
volume 375. Springer Science & Business Media, 1996.
Lester Randolph Ford Jr and Delbert Ray Fulkerson. Flows in networks. Princeton university press,
2015.
L Franceschi, M Donini, P Frasconi, and M Pontil. Forward and reverse gradient-based hyperpa-
rameter optimization. In ICML, volume 70, pp. 1165-1173. JMLR, 2017.
L Franceschi, P Frasconi, S Salzo, R Grazzi, andM Pontil. Bilevel programming for hyperparameter
optimization and meta-learning. In ICML, volume 80, pp. 1563-1572. PMLR (Proceedings of
Machine Learning Research), 2018.
10
Published as a conference paper at ICLR 2021
M. Garavello and B. Piccoli. Traffic Flow on Networks: Conservation Laws Model. AIMS series on
applied mathematics. American Institute of Mathematical Sciences, 2006.
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska
Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-
learning. arXiv preprint arXiv:1910.01727, 2019.
David Hallac, Jure Leskovec, and Stephen Boyd. Network lasso: Clustering and optimization in
large graphs. In Proceedings of the 21th ACM SIGKDD international conference on knowledge
discovery and data mining,pp. 387-396, 2015.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
Jozef Hanc, Slavomir Tuleja, and Martina Hancova. Symmetries and conservation laws: Conse-
quences of noether’s theorem. American Journal of Physics, 72(4):428-435, 2004.
Juan C Herrera, Daniel B Work, Ryan Herring, Xuegang Jeff Ban, Quinn Jacobson, and Alexan-
dre M Bayen. Evaluation of traffic data obtained via GPS-enabled mobile phones: The mobile
century field experiment. Transportation Research Part C: Emerging Technologies, 18(4):568-
583, 2010.
Jonas Horsch, Fabian Hofmann, David Schlachtberger, and Tom Brown. Pypsa-eur: An open op-
timisation model of the european transmission system. Energy Strategy Reviews, 22:207-215,
2018.
Junteng Jia, Michael T. Schaub, Santiago Segarra, and Austin R. Benson. Graph-based semi-
supervised and active learning for edge flows. In Proceedings of the 25th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining, pp. 761-771, 2019.
Alexander Jung. On the duality between network flows and network lasso. IEEE Signal Processing
Letters, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Jan Larsen, Lars Kai Hansen, Claus Svarer, and M Ohlsson. Design and regularization of neural
networks: the optimal use of a validation set. In Neural Networks for Signal Processing VI.
Proceedings of the 1996 IEEE Signal Processing Society Workshop, pp. 62-71. IEEE, 1996.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.
Michael James Lighthill and Gerald Beresford Whitham. On kinematic waves ii. a theory of traffic
flow on long crowded roads. Proceedings of the Royal Society of London. Series A. Mathematical
and Physical Sciences, 229(1178):317-345, 1955.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
35th International Conference on Machine Learning, ICML 2018, pp. 5067-5078. International
Machine Learning Society (IMLS), 2018.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pp.
1540-1552. PMLR, 2020.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
11
Published as a conference paper at ICLR 2021
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings of the
33rd International Conference on International Conference on Machine Learning-Volume 48, pp.
737-746, 2016.
Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar,
Dominic Skinner, and Ali Ramadhan. Universal differential equations for scientific machine
learning. arXiv preprint arXiv:2001.04385, 2020.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
Paul I Richards. Shock waves on the highway. Operations research, 4(1):42-51, 1956.
Albert Tarantola. Inverse problem theory and methods for model parameter estimation, volume 89.
SIAM, 2005.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Minjie Wang, Lingfan YU, Da Zheng, QUan Gan, YU Gai, Zihao Ye, MUfei Li, Jinjing ZhoU,
Qi HUang, Chao Ma, et al. Deep graph library: Towards efficient and scalable deep learning
on graphs. arXiv preprint arXiv:1909.01315, 2019.
Daniel B Work, Sebastien Blandin, Olli-Pekka Tossavainen, Benedetto Piccoli, and Alexandre M
Bayen. A traffic model for velocity data assimilation. Applied Mathematics Research eXpress,
2010(1):1-35, 2010.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In International Conference on Learning Representations, 2018.
HUaxiU Yao, Xianfeng Tang, HUa Wei, GUanjie Zheng, and ZhenhUi Li. Revisiting spatial-temporal
similarity: A deep learning framework for traffic prediction. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volUme 33, pp. 5668-5675, 2019.
Bing YU, Haoteng Yin, and Zhanxing ZhU. Spatio-temporal graph convolUtional networks: a deep
learning framework for traffic forecasting. In Proceedings of the 27th International Joint Confer-
ence on Artificial Intelligence, pp. 3634-3640, 2018.
Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learn-
ing with local and global consistency. In Advances in Neural Information Processing Systems,
pp. 321-328, 2004.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International Conference on Machine
learning (ICML-03), pp. 912-919, 2003.
12
Published as a conference paper at ICLR 2021
Symbol	Meaning
G V n E m E 0 ⊆ E m0 X ∈ Rm×d X [e] ∈ Rd f ∈ Rm fe ∈ R ， ^ ∈ Rm fe ∈ R B Φ(f, X; f(O);㊀)∈ R+ f(O) ∈ Rm Θ Ω X ∈ Rm-m0 X ∈ Rm-m0 X(O) ∈ Rm-m0 H ∈ Rm×m0 f ∈ Rm Q(X; Θ) ∈ R(m-m0)×(m-mO) '	K T J ɑ β Ψ(x, Θ) Yθ(x) rj(Xk,j—1, θi—1) B Hk Qk 	fk		Flow graph Set of vertices in G Size of V Set of edges in G Size of E Set of observed edges Size of E0 Edge feature matrix Features of edge e Complete flow vector Flow for edge e Observed flow vector Observed flow for edge e Incidence matrix of G Regularization function Flow prior Regularization parameters Domain of F Estimated vector of missing flows True vector of missing flows Prior for missing flows Map from f to X Vector with observed flows or 0 otherwise Regularization function (diagonal matrix) Number of folds in cross-validation Number of outer iterations for Algorithm 1 Number of inner iterations for Algorithm 1 Outer learning rate for Algorithm 1 Inner learning rate for Algorithm 1 Outer objective Inner objective One step of SGD Estimate of ▽㊀Ψ(x, Θi-i) Matrix H for fold k Matrix Q for fold k Vector f for fold k
Table 2: Table of the main symbols used in this paper.
A Table of Symbols
Table 2 lists the main symbols used in our paper.
B B ilevel Optimization with Graph Neural Networks
This section is an extension of Section 3.3. Here, we consider the case where Q(X; Θ) is a GNN:
Q(X,Θ) = diag(GN N (X, Θ, G))	(8)
For instance, we apply a 2-layer spectral Graph Convolutional Network (GCN) with Chebyshev
convolutions (Defferrard et al., 2016; Kipf & Welling, 2016; Hammond et al., 2011):
Q(X身=diag (ReLU (二Tz，(L)ReLU (XTz(L)XWF)) Wy)))	⑼
13
Published as a conference paper at ICLR 2021
where L = 2∕λmɑxL - I, L is the normalized Laplacian of the undirected version of the line graph
G0 of G, λmax is the largest eigenvalue of L, Tz(L) is a Chebyshev polynomial of L with order z
and Wz(i) is the matrix of learnable weights for the z-th order polynomial at the layer i. In a line
graph, each vertex represents an edge of the undirected version of G and two vertices are connected
if their corresponding edges in G are adjacent. Morever L = I - D-1/2AD-1/2, where A and D
are the adjacency and degree matrices of G0 . Chebyshev polynomials are defined recursively, with
Tz(y) = 2yTz-1 (y) - Tz-2(y) and T1 (y) = y.
In our experiments, we compare GCN against MLP regularization functions. We have also applied
the more popular non-spectral graph convolutional operator (Kipf & Welling, 2016) but preliminary
results have shown that the Chebyshev operator achieves better performance in flow estimation.
C Extended Experimental Section
This section in an extension of Section 4.
C.1 More Details on Datasets
Traffic: Flow data was collected from the Caltrans—the California Department of Transportation—
PeMS (Performance Measurement System).1 Sensors are placed at major highways in the state. We
use sensor geo-locations and other attributes to approximately match them to a compressed version
of road network extracted from Openstreetmap.2 The compression merges any sequence of segments
without a branch, as these extra edges would not affect the flow estimation results. We emphasize
that this dataset is not ofas high quality as Power, due to possible sensor malfunction and matchings
of sensors to the wrong road segments. This explains why flow estimation is more challenging in
Traffic. Figure 4 is a visualization of our traffic dataset with geographic (lat-long) located vertices
and colors indicating light versus heavy traffic (compared to the average). The road segments in the
graph (approximately) cover the LA County area. We show the map (from Openstreetmap) of the
area covered by our road network in Figure 5.
Power: We will provide more details on how we build the power dataset. PyPSA (Python for Power
System Analsys) is a toolbox for the simulation of power systems (Brown et al., 2017). We applied
the European transmission system (PyPSA-Eur), which covers the ENTSO-E area (Horsch et al.,
2018), to generate a single network snapshot. Besides the PyPSA-Eur original set of edges, which
we will refer to as line edges, we have added a set of bus edges. These extra edges allow us to
represent power generation and consumption as edge flows. For the line edges, we cover the follow-
ing PyPSA attributes (with their respective PyPSA identifiers3): reactance (x), resistance(r), capacity
(s_nom), whether the capacity s_nom can be extended (s_nom_extendable), the capital cost of extend-
ing s_nom (CaPitaLcost), the length of the line (length), the number of parallel lines (num_parallel)
and the optimized capacity (s_nom_opt). For bus lines, the only attribute is the control strategy (PQ,
PV, or Slack). Notice that we create a single vector representation for both line and bus lines by
adding an extra indicator position (line or bus). Moreover, categorical attributes (e.g., the control
strategy) were represented using one-hot encoding. Figure 6 is a visualization of our power dataset
with geographic (lat-long) located vertices and colors indicating high versus low power (compared
to the average).
C.2 Evaluation Metrics
We apply the following evaluation metrics for flow estimation. Let ftrue and fpred be m0 -
dimensional vectors with true and predicted values for missing flows associated to edges in E \ E0 .
Correlation (Corr):
cov(fpred, ftrue) / (σ (fpred) .σ (ftrue))
where cov is the covariance and σ is the standard deviation.
1Source: http://pems.dot.ca.gov/
2Source: https://www.openstreetmap.org
3https://pypsa.readthedocs.io/en/latest/components.html
14
Published as a conference paper at ICLR 2021
Figure 4: Visualization of our traffic network with geo-located vertices. Edges in grey have missing
flows, edges in red have traffic above the average and edges in blue have traffic below the average.
Better seen in color. See Figure 5 for map of the area.
Figure 5: Road map covered by the road network shown in Figure 4 (from Openstreetmap)
Mean Absolute Percentage Error (MAPE):
1
m — m'
V^''' I (ftrue)e - (fpred)e ∣
e∈E∖E o	(ftrue)e
15
Published as a conference paper at ICLR 2021
Figure 6: Visualization of our power network with geo-located vertices. Edges in red have traffic
above the average and edges in blue have traffic below the average. Better seen in color.
Mean Absolute Error (MAE):
1
m0
|(ftrue)e - (fpred)e |
e∈E∖E0
Root Mean Squared Error (RMSE):
∖
-0 X Kftrue)e - (fpred)e]2
m0
e∈E∖E0
Divergence (Div):
X(Xf(u,v) - Xf(v,u))2
vu	u
C.3 More Experimental Settings
Train/test splits: We report results of a 10-fold cross-validation based on the set of labeled flows.
Moreover, we use 10% of training flows for validation.
Implementation4 : We have implemented Algorithm 1 using PyTorch, CUDA, and Higher (Grefen-
stette et al., 2019), a meta-learning framework that greatly facilitates the implementation of bilevel
optimization algorithms by implicitly performing the reverse iterations for a list of optimization al-
gorithms, including SGD. Moreover, our GCN implementation is based on the Deep Graph Library
(DGL) (Wang et al., 2019).
Hardware: We ran our experiments on a single machine with 4 NVIDIA GeForce RTX 2080 GPUs
(each with 8Gb of RAM) and 32 Intel Xeon CPUs (2.10GHz and 128Gb of RAM).
4https://github.com/arleilps/flow-estimation
16
Published as a conference paper at ICLR 2021
Hyperparameter settings: We have selected the parameters based on RMSE for each method using
grid search with learning rate over [100, 10-1, 10-2, 10-3] and number of nodes in the hidden layer
over [4, 8, 16]. The total number of iterations was set to 3000 for Min-Div and 5000 for MLP
and GCN, all with early stop on convergence after 10 iterations. For our methods (both based on
Algorithm 1), we set T = 10, J = 300, α = 10-2, β = 10-2 and K = 10 in all experiments.
C.4 Divergence Results
Although the main goal of flow estimation is to minimize the flow prediction loss, we also evaluate
how our methods and the baselines perform in terms of divergence (or flow conservation) in Table 3.
As expected, MLP and GCN do not conserve the flows. However, interestingly, our methods (Bil-
MLP and Bil-GCN) achieve higher flow conservation than Min-Div. This is due to the regularization
parameter λ, which is tuned based on a set of validation flows.
	Traffic		Power
Min-Div	2.94	2.45
MLP	5.69	2.77
GCN	5.71	2.80
Bil-MLP	2.81	2.43
Bil-GCN	2.83	2.43
Bil-GCN-PriOr	2.43	2.43
Table 3: Divergence results.
D True vs. Predicted Flows
Figure 7 shows scatter plots with the true vs. predicted flows that are missing from Figure 2.
Q'8'64∙2Q
Iooooo
Mo-p-p-d
Q'8'64∙2C
Ioooon
Mop-p-d
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(a) MLP, Traffic
Q'8'64∙2Q
Iooooo
MO-J pp
0.0 0.2 0.4 0.6 0.8 1.0
True flow
Q'8'64∙2Q
Iooooo
Mopp
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(b) Min-Div, Traffic
0.0 0.2 0.4 0.6 0.8 1.0
True flow
Q'8'64∙2Q
Iooooo
Mopp
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(c) Bil-MLP, Traffic
Q'8'64∙2Q
Iooooo
MO-J pp
0.0 0.2 0.4 0.6 0.8 1.0
True flow
Mo=ps-pα,i
MO_J Pepald
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(d) Bil-GCN-Prior, Traffic
0.0 0.2 0.4 0.6 0.8 1.0
True flow
(e) MLP, Power
(f) GCN, Power	(g) Bil-MLP, Power (h) Bil-GCN-Prior, Power
Figure 7:	Scatter plots with true (x) and predicted (y) flows for remaining methods (beyond the ones
shown in Figure 2).
D.1 Visualization of Regularizer for Power
Figure 8 shows the regularizers over the Power network topology. As discussed in Section 4.4, the
regularizer affects mostly a few top resistance edges. For the remaining ones, regularizers have a
small value. Notice that these high resistance edges are associated with lines transmitting small
amounts of power, as shown in Figure Figure 6, and have a large impact on the overall flow estima-
tion accuracy.
17
Published as a conference paper at ICLR 2021
∣0.40
I 0.35
0.30
0.25
0.20
0.15
0.10
∣0.05
U(λOO
Figure 8:	Visualization of regularizers on the Power network topology. We highlight edges with
large vaues of regularizer. Better seen in color.
	Traffic		Power	
Method 一	Training	Test	Training	Test
Min-Div	424.4	W	364.2	00Γ
MLP	21.95	0.10	12.32	0.01
GCN	2.43	0.09	0.77	0.01
Bil-MLP	1860.2^^	0.08-	-3697	00Γ
Bil-GCN	1870.1	0.09	346.7	0.01
Bil-MLP-Prior	1886.1	0.01	334.1	0.01
Table 4: Average training and test times (in seconds) for our methods and the baselines (in seconds).
D.2 Running Time
Table 4 shows the average running times—over the 10-fold cross-validation—of our methods and
the baselines for the Traffic and Power datasets. We show both training and test times. The results
show that our reverse-mode differentiation algorithm adds significant overhead on training time for
Traffic, taking up to 4 times longer than Min-Div to finish. As described in Section 3.4, this is due
mainly to the cost of computing and storing the inner problem iterations. On the other hand, all the
methods are efficient at testing. GCN converged quickly (due to early stopping) for both datasets.
However, it achieved poor results for Power, as shown in Table 1, which is a sign of overfitting or
underfitting. Notice that the results reported are the best in terms of RMSE.
18