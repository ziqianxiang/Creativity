Published as a conference paper at ICLR 2021
Hyperbolic Neural Networks++
Ryohei Shimizu1, Yusuke Mukuta1,2, Tatsuya Harada1,2
1 The University of Tokyo
2RIKEN AIP
{shimizu, mukuta, harada}@mi.t.u-tokyo.ac.jp
Ab stract
Hyperbolic spaces, which have the capacity to embed tree structures without dis-
tortion owing to their exponential volume growth, have recently been applied to
machine learning to better capture the hierarchical nature of data. In this study,
we generalize the fundamental components of neural networks in a single hyper-
bolic geometry model, namely, the Poincare ball model. This novel methodology
constructs a multinomial logistic regression, fully-connected layers, convolutional
layers, and attention mechanisms under a unified mathematical interpretation, with-
out increasing the parameters. Experiments show the superior parameter efficiency
of our methods compared to conventional hyperbolic components, and stability
and outperformance over their Euclidean counterparts.
1	Introduction
Shifting the arithmetic stage of a neural network to a non-Euclidean geometry such as a hyperbolic
space is a promising way to find more suitable geometric structures for representing or processing
data. Owing to its exponential growth in volume with respect to its radius (Krioukov et al., 2009;
2010), a hyperbolic space has the capacity to continuously embed tree structures with arbitrarily
low distortion (Krioukov et al., 2010; Sala et al., 2018). It has been directly utilized, for instance, to
visualize large taxonomic graphs (Lamping et al., 1995), to embed scale-free graphs (Blasius et al.,
2018), or to learn hierarchical lexical entailments (Nickel & Kiela, 2017). Compared to the Euclidean
space, a hyperbolic space shows a higher embedding accuracy under fewer dimensions in such cases.
Because a wide variety of real-world data encompasses some type of latent hierarchical structures
(Katayama & Maina, 2015; Newman, 2005; Lin & Tegmark, 2017; Krioukov et al., 2010), it has
been empirically proven that a hyperbolic space is able to capture such intrinsic features through
representation learning (Krioukov et al., 2010; Ganea et al., 2018b; Nickel & Kiela, 2018; Tifrea
et al., 2019; Law et al., 2019; Balazevic et al., 2019; Gu et al., 2019). Motivated by such expressive
characteristics, various machine learning methods, including support vector machines (Cho et al.,
2019) and neural networks (Ganea et al., 2018a; Gulcehre et al., 2018; Micic & Chu, 2018; Chami
et al., 2019) have derived the analogous benefits from the introduction of a hyperbolic space, aiming
to improve the performance on advanced tasks beyond just representing data.
One of the pioneers in this area is Hyperbolic Neural Networks (HNNs), which introduced an
easy-to-interpret and highly analytical coordinate system of hyperbolic spaces, namely, the Poincare
ball model, with a corresponding gyrovector space to smoothly connect the fundamental functions
common to neural networks into valid functions in a hyperbolic geometry (Ganea et al., 2018a).
Built upon the solid foundation of HNNs, the essential components for neural networks covering the
multinomial logistic regression (MLR), fully-connected (FC) layers, and Recurrent Neural Networks
have been realized. In addition to the formalism, the methods for graphs (Liu et al., 2019), sequential
classification (Micic & Chu, 2018), or Variational Autoencoders (Nagano et al., 2019; Mathieu et al.,
2019; Ovinnikov, 2019; Skopek et al., 2020) are further constructed. Such studies have applied the
Poincare ball model as a natural and viable option in the area of deep learning.
Despite such progress, however, there still remain some unsolved problems and uncovered regions.
In terms of the network architectures, the current formulation of hyperbolic MLR (Ganea et al.,
2018a) requires almost twice the number of parameters compared to its Euclidean counterpart. This
makes both the training and inference costly in cases in which numerous embedded entities should be
1
Published as a conference paper at ICLR 2021
classified or where large hidden dimensions are employed, such as in natural language processing.
The lack of convolutional layers must also be mentioned, because their application is now ubiquitous
and is no longer limited to the field of computer vision.
For the individual functions that are commonly used in machine learning, the split and concatenation
of vectors have yet to be realized in a hyperbolic space in a manner that can fully exploit such
space and allow sub-vectors to achieve a commutative property. Additionally, although several types
of closed-form centroids in a hyperbolic space have been proposed, their geometric relationships
have not yet been analyzed enough. Because a centroid operation has been utilized in many recent
attention-based architectures, the theoretical background for which type of hyperbolic centroid should
be used would be required in order to properly convert such operations into the hyperbolic geometry.
Based on the previous analysis, we reconsider the flow of several extensions to bridge Euclidean
operations into hyperbolic operations and construct alternative or novel methods on the Poincare ball
model. Specifically, the main contributions of this paper are summarized as follows:
1.	We reformulate a hyperbolic MLR to reduce the number of parameters to the same level as
a Euclidean version while maintaining the same range of representational properties.
2.	We further exploit the knowledge of 1 as a replacement of an affine transformation and
propose a novel generalization of the FC layers that can more properly make use of the
hyperbolic nature compared with a previous research (Ganea et al., 2018a).
3.	We generalize the split and concatenation of coordinates to the Poincare ball model by
setting the invariance of the expected value of the vector norm as a criterion.
4.	By combining 2 and 3, we further define a novel generalization scheme of arbitrary dimen-
sional convolutional layers in the Poincare ball model.
5.	We prove the equivalence of the hyperbolic centroids defined in three different hyperbolic
geometry models, and expand the condition of non-negative weights to entire real values.
Moreover, integrating this finding and previous contributions 1, 2, and 3, we give a theoretical
insight into hyperbolic attention mechanisms realized in the Poincare ball model.
We experimentally demonstrate the effectiveness of our methods over existing HNNs and Euclidean
equivalents based on a performance test of MLR functions and experiments with Set Transformer
(Lee et al., 2019) and convolutional sequence to sequence modeling (Gehring et al., 2017).1
2	Hyperb olic Geometry
Riemannian geometry. An n-dimensional manifold M is an n-dimensional topological space that
can be linearly approximated to an n-dimensional real space at any point x ∈ M, and each local
linear space is called a tangent space TxM. A Riemannian manifold is a pairing of a differentiable
manifold and a metric tensor field g as a function of each point x, which is expressed as (M, g ).
Here, g defines an inner product on each tangent space such that ∀u, v ∈ TxM, hu, vix = u>gxv,
where gx is a positive definite symmetric matrix defined on TxM. The norm of a tangent vector
derived from the inner product is defined as ∣∣vkχ =，|〈v, v)χ∣. A metric tensor gχ provides local
information regarding the angle and length of the tangent vectors in TxM, which induces the global
length of the curves on M through an integration. The shortest path connecting two arbitrary points
on M at a constant speed is called a geodesic, the length of which becomes the distance. Along a
geodesic where one of the endpoints is x, the function projecting a tangent vector v ∈ TxM as an
initial velocity vector onto M is denoted as an exponential map expx , and its inverse function is
called a logarithmic map logx . In addition, the concept of parallel transport Px→y : TxM → TyM
is generalized to the specially conditioned unique linear isometry between two tangent spaces. For
more details, please refer to Spivak (1979); Petersen et al. (2006); Andrews & Hopper (2010).
Note that, in this study, we equate g with gx if gx is constant, and denote the Euclidean inner product,
norm, and unit vector for any real vector u, V ∈ Rn as(u, vi, ∣v∣, and [v] = v∕∣v/respectively.
Hyperbolic space. A hyperbolic space is a Riemannian manifold with a constant negative curvature,
the coordinates of which can be represented in several isometric models. The most basic model is an
1The code is available at https://github.com/mil-tokyo/hyperbolic_nn_plusplus.
2
PUbHShed as a ConferenCe PaPer at ICLR 2021
TZ—dimensional hyperboloid modeLWhiCh is a hypersurface 国7 -n P3 (72 + I)—dimensional MinkoWSki
SPaCe 1R7+1 COmPOSed Of One time⅛ke axis and TI SPaCe—like axes ∙ The manifolds OfPoinCare ball
model 悬 aj2d BeItrami—Klein model 典 arm the projections OfthehyPerbOoid model OntOthe
different Tzldimensional SPaCe—like hyperplane? as depicted in FigUre L FOrtheir mathematical
definitions and the isometric isomorphism between their COOrds∙ate∞See APPendiX >∙
PoinCar6 ba= InodeLThe TZldimenSionaI PoinCar力 ba= model Of a
COnStant negative CUrVatUre ——c is defined by (s" gc)“ Where 典 H
它 ∈ 而时一 C=旦一 2 八一〉and 品=>A2zm Hee 典 is an OPen
ball Of radius cl“ ande⅛= 2(1 —— 呈旦一2)——1 is a COnfOrmaI factoL
WhiChindUCeS the inner PrOdUCt〈£ V# = (^)2(s V) and norm
一亘记 He⅛=e= for Fe ∈ Tis7∙ TheeXPonentiaL logarithmic
maps and ParaneItranSPort are denoted as exp”ogc and P乔 y“
respectively” as ShOWns∙APPendiX c∙
Ho OPerate the coordinates as VeCtOr—like mathematical ObjeCt∞the
MdbiUS gyrovector SPaCePrOVideS an algebra that treats them as
gyrovector∞equipped With VariOUS OPerato∙ns≡∙cluding the genera 丁
ized VeCtor addito∙pthat i∞a noncommutative and non—associative
binary operation Ca=Cd the MdbiUS addition ㊉C (Ungan 2009)∙
Hmio ㊉C ConVergeS to + in ConneCtion With a EUCHdean geometry
the CUrVatUre Of WhiCh is Zer0∙ FOrmOre details" See APPendiX B∙
FigUre 1: GeOmetriC relation—
ShiP between 居"典 a^d 热
depictedS∙1R7+1∙
POinCar6 hyperplane∙ AS a specific generaHZation Of a hyperplane into Riemannian geometry” Ganea
et al∙ (20∞a) derived a PoinCare hyperplane gr2 WhiCh is the Set Of all geodesics Conta5∙5∙g an
arbitrary Point p ∈s7 a^d OrthogonaIto an arbitrary tangent VeCtor P ∈ 相@川“ based On the MdbiUS
gyrovector SPaCe∙ AS ShOWn in APPendiX C2 they also extended the distance AC between two points
s∙s7s∙s the distance from a Points∙s7 S P PoinCar 6 hyperplanes∙a COSed form CXPreSSiOP
3 HYPERBoLlC NEURAL NETWoRKs++
Aiming to OVerCOme the difficulties discussed
in SeCtionl "we build a novel SCheme Ofhyl
PerbOHC neural networks5∙the PoinCare ba=
modeLThe COre ConCePtiS re—generalization Of
〈?包 ——b type equations With no increase in the
number Of parameter? WhiCh has the potential
to replace any aRne transformation based On
the Same mathematical PrinCiPle∙ SPeC≡caπy"
this SeCtiOn StartS from the reformulation Of the
hyperbolic MLR“ from WhiCh the VariantS to the
FCbOnVolUto∙naL and multi—head attention lay—
ers are derived∙ Several Other modifications are
also proposed to SUPPOrt neural networks With
broad architectures-
FigUre 2: WhiCheVer Pair Of P and P is ChOSeP
it determines the Same discriminative hyperplane∙
ConSidering One bias point qpr Per One discrimina—
tive hyperplane SOIVeS this OVerIparameterizatiop
3∙ 1 UNlDIRECTIoNAL REPARAMETERlZATIoN OF HYPERBoLIC MLR LAYER
GiVen an input a ∈ 而时“ MLR is an OPeration USed to predict the ProbabiHtieS Of a∏ target OUtComeS
k ∈?2::“ Xyforthe ObjeCtiVe VariabIe y as aog—linear model and is described as follows-
xy U /c -⅛0c exp <⅛(⅛) “ Where f(3H 6f3IbF £ ∈ r∈ 1R∙ (I)
Cir2mvention Of the double VeCtoriZation・ Ho generaHZe the HnearfUnCtion ι¼ S the PoinCare
ball modeL Ganea et al∙ (2018a) HrSt re—parameterized the SCaIartermras a VeCtor Pkm屈TZ -n
the form〈0^9)——r= Laklpk + WL Wherer= {μypk} “ and then discussed the properties
WhiCh must be SatiSHed When SUCh VeCtOrS become MdbiUS gyro vectors ∙ HOWeVeLthiS CaUSeS an
UndeSirabIe increase in the parameters from TZ + 1 to 2τz in each CIaSS k. ASi=UStrated in FigUre 2
(a) “ this reformulation is redundant from the VieWPo5∙t that there exist COUntIeSS ChoiCeS Of Pk to
determine the Same discriminative hyperplane Hakbk = (am国时一(0Fa)——b?r= Oy∙ BeCaUSe the
3
Published as a conference paper at ICLR 2021
I∣!∣l
(a) exp0 (A log0(x))㊉C b
(b) FC(x; Z, r) (ours)
Figure 3: Comparison of FC layers in input spaces Bcn . The values at a certain dimension of output
spaces are illustrated as contour plots. Black arrows depict the orientation parameters, and they are
fixed for the comparison. Their orthogonal curves show discriminative hyperplanes where the values
are zeros. As a bias parameter b or rk changes, the outline of the contour landscape in (a) remains
unchanged, whereas in (b) the focused regions are dynamically squeezed according to the geodesics.
key of this step is to replace all variables with vectors attributed to the same manifold, we introduce
another scalar parameter rk ∈ R instead, which makes the bias vector qak ,rk parallel to ak:
hak,xi - bk	=	hak,	-qak,rk + xi,	where	qak,rk	= rk[ak]	s.t.	bk	= rkkakk.	(2)
One possible realization of pk is adopted to reduce the previously mentioned redundancies without a
loss of generality or representational properties compared to the original affine transformation, and
induces another notation: H。和小：={x ∈ Rn |〈ak, —q.k,小 + Xi =0}=心i|&忆八. Based on
distance d from a point to a hyperplane, Equation 2 can be rewritten as with Lebanon & Lafferty
(2004) in the following form:〈ak, —Qa^k + Xi = sign(〈ak, —qak以 + Xi) d(x,H&卜以)∣∣ak∣∣,
which decomposes the inner product into the product of the norm of an orientation vector ak and the
signed distance between an input vector X ∈ Rn and the hyperplane Hak,rk.
Unidirectional Poincare MLR. Based on the observation that qak 以 starts from the origin and the
concept of Poincare hyperplanes, we can now generalize Vk for x, qak,丁飞 ∈ Bn and αk ∈ Tqak尸卜Bn：
Vk (X) = sign( hak, θc Qak,rk ㊉ C Xi) dc (x,Ha k,rk ) kαk 吟。…％ ,	(3)
where q。%,% = exp0(r® [a®]), Hak,加 := {x ∈ Bn |〈a®, θ 9。％,小 ㊉C Xi = 0},	(4)
which are shown in Figure 2 (b). Importantly, the circular reference between ak ∈ Tqa,r Bcn and
qak ,rk can be unraveled by considering the tangent vector at the origin, zk ∈ T0BCn , from which ak
is parallel transported by PxC→y : TxBCn → TyBCn described in Appendix C.3 as follows:
ak = P0→qak,rk (zk) = sech2 (√crk) zk, Qakrk = exP0(rk[zk]) = Qzk,rk .	(5)
Combining Equations 3, 5, and 23, we conclude the derivation of the unidirectional re-generalization
of MLR, the parameters of which are rk ∈ R and zk ∈ T0BCn = Rn for each class k:
Vk (x) = 2 c-2 kzk k SinhT (λχh√c多，⑶» Cosh R√Crk) — (λχ — 1)sinh (2√crk)) . (6)
For more detailed deformation, see Appendix D.1. Note that we recover the form of the standard
Euclidean MLR in limC→0 Vk(X) = 4(〈ak, Xi — bk), which is proven in Appendix D.2.
3.2	Reformulating FC layers to properly exploit the hyperbolic properties
We next discuss the FC layers, described as a simple affine transformation y = AX — b, in an
element-wise manner with respect to the output space as yk = 〈ak, Xi — bk, where X, ak ∈ Rn and
bk ∈ R . This can be interpreted as an operation that linearly transforms the input X and treats the
output score yk as the coordinate value at, or the signed distance from the hyperplane containing the
origin and orthogonal to, the k-th axis of the output space Rm . Therefore, combining them with a
generalized linear transformation, as described in Section 3.1, we can now generalize the FC layers:
Poincare FC layer. Given an input X ∈ BCn, with the generalized linear transformation Vk in Equation
6 and the parameters composed of Z = {zk ∈ T0BCn = Rn}km=1, which is a generalization of A and
r = {rk ∈ R}km=1 representing the bias terms, the Poincare FC layer outputs the following:
y = Fc(x; Z, r) ：= w(1 + p1 + Ckwk2)-1, where W := (c-2 sinh (√Cvk(x)))®=]. (7)
It can be proven that the signed distance from y to each Poincare hyperplane containing the origin, and
orthogonal to the k-th axis, equals Vk(X), as shown in Appendix D.3, satisfying the aforementioned
properties. We also recover a FC layer in limC→0 yk = 4 (〈ak, Xi — rk kak k).
4
Published as a conference paper at ICLR 2021
Comparison with a previous method. Ganea et al. (2018a) proposed a hyperbolic FC layer op-
erating a matrix-vector multiplication in a tangent space and adding a bias through the following
Mobius addition: y = exp0 (A log0(x))㊉C b, which indicates that the discriminative hyperplane
determined in T0Bcm is projected back to Bcm by the exponential map at the origin. However, such
a surface is no longer a Poincare hyperplane, except for b = 0. Moreover, the basic shape of the
contour surfaces in the output space Bcm is determined only by the orientation of each row vector ak
in A, whereas their norms and a bias term b contribute to the total scale and shift. Conversely, the
parameters in our method cooperate to realize more various contour surfaces. Notably, discriminative
hyperplanes become Poincare hyperplanes, i.e., the set of all geodesics orthogonal to the orientation
zk and containing a point expc0(rk[zk]). As shown in Figure 3, the input space Bcn is separated in a
more meaningful manner as a hyperbolic space for each dimension of the output space Bcm .
3.3	Regularizing split and concatenation
Split and concatenation are essential operations for realizing small process branches in parallel or
combining feature vectors. However, in the Poincare ball model, merely splitting the coordinates
lowers the norms of the output gyrovectors and limits the representational power, and concatenating
them is invalid because the norm of the output can easily exceed the domain of the ball. One simple
solution is to conduct an operation in the tangent space. The aforementioned problem regarding a
split operation, however, remains. Moreover, as the number of inputs to be concatenated increases,
the output gyrovector approaches the boundary of the ball even if the norm of each input is adequately
small. The norm of the gyrovector is crucial in the Poincare ball model owing to its metric. Therefore,
reflecting the orientation of inputs while preserving the scale of the norm is considered to be desirable.
Generalization criterion. In Euclidean neural networks, keeping the variance of feature vectors
constant is an essential criterion (He et al., 2015). As an analogy, keeping the expected values of the
norms constant is a worthy criterion in the Poincare ball because the norm of any Mobius gyrovector
is upper-bounded by the ball radius and the variance of the coordinates cannot necessarily remain
intact when the dimensions in each layer vary. Such a replacement of the statistic invariance target
from each coordinate to the norm is also suggested by Becigneul & Ganea (2019). To satisfy this
criterion, We propose the following generalization scheme with a scalar coefficient βn = B(2, 1),
where B indicates a beta function.
Poincare β-split. First, the input X ∈ Bn is split in the tangent space with integers s.t. PN=I n = n:
x 7→ v = logc0(x) = (v1> ∈ Rn1 , . . . , vN> ∈ RnN)>. Each split tangent vector is then properly
scaled and projected back to the Poincare ball as follows: vi 7→ yi = expc0 βniβn-1vi .
Poincare β-concatenation. Likewise, the inputs {xi ∈ Bcni }iN=1 are first properly scaled and
concatenated in the tangent space, and then projected back to the Poincare ball in the following
manner: xi 7→ vi = logc0(xi) ∈ T0 Bcni, v := (βnβn-11v1>, . . . , βn βn-N1 vN>)> 7→ y = expc0 (v).
We prove the previously mentioned properties under a certain assumption in Appendix D.4. One can
also confirm that the Poincare β-concatenation is the inverse function of the Poincare β-split.
Discussion about the concatenation. Ganea et al. (2018a) generalized a vector concatenation under
the premise that the output must be followed by an FC layer, but such an assumption possibly limits its
usage. Furthermore, it requires Mobius additions N - 1 times sequentially due to the noncommutative
and non-associative properties of the Mobius addition, which incurs a heavy computational cost and
an unbalanced priority in each input gyrovector. Alternatively, our method with a pair of exponential
and logarithmic maps has a lower computational cost regardless of N and treats every input fairly.
3.4	Arbitrary dimensional convolutional layer
The activation of D-dimensional convolutional layers with kernel sizes of {Ki}iD=1 is generally
described as an affine transformation yk = hak, xi - bk for each channel k, where x ∈ RnK is an
input vector per pixel, and is a concatenation of K = Qi Ki feature vectors contained in a receptive
field of the kernel. This notation also includes a dilated operation. It is now natural to generalize the
convolutional layers with Poincare β-concatenation and a Poincare FC layer.
5
Published as a conference paper at ICLR 2021
Poincare convolutional layer. At each pixel in the given feature map, the gyrovectors {xs ∈ Bn}K=ι
contained in a receptive field of the kernel are concatenated into a single gyrovector x ∈ BcnK in the
manner proposed in Section 3.3, which is then operated in the same way as a POinCare FC layer.
3.5	Analysis of hyperbolic attention mechanisms in THE POINCARE ball model
As preparation for constructing hyperbolic attention mechanisms, it is necessary to theoretically
consider the midpoint operation of multiple coordinates in a hyperbolic space. For the Poincare ball
model and Beltrami-Klein model, Ungar (2009) proposed the Mobius gyromidpoint and Einstein
gyromidpoint built upon the framework of gyrovector spaces, respectively, which are represented
in different coordinate systems but are geometrically the same, as shown in Appendix D.5. on the
other hand, Law et al. (2019) proposed another type of hyperbolic centroid based on the minimization
problem of the squared Lorentzian distance defined in the hyperboloid model. Based on the above
situation, for the major concern of which formulation to utilize, we proved the following theorem.
Theorem 1. (The equivalence of three hyperbolic midpoints) The Mobius gyromidpoint, Einstein
gyromidpoint, and the centroid of the squared Lorentzian distance exactly match each other, which
indicates they are the same midpoint operations projected on each manifold.
The proofs are given in Appendix D.5 and D.6. Furthermore, based on this equivalence, we can
characterize the Mobius gyromidpoint as a minimizer of the weighted sum of calibrated squared
gyrometrics, which we proved in Appendix D.7.
With Theorem 1, we can now exploit the Mobius gyromidpoint as a unified option to realize hyperbolic
attention mechanisms. Moreover, we further generalized the Mobius gyromidpoint by extending the
condition of non-negative weights to entire real values by regarding a negative weight as an additive
inverse operation: The midpoint b ∈ Bn of Mobius gyrovectors {bi ∈ Bn}，with the real scalar
weights {νi ∈ R}iN=1 is given by
N
b = ^[bi,νi]c
i=1
1
-0
2 ’
(8)
which is shown in Appendix D.8. Note that the sum of weights does not need to be normalized to one
because any scalar scale is cancelled between the numerator and denominator. on the basis of this
insight, in the following, we describe the construction of a multi-head attention as a specific example,
aiming at a general approach that can be applied to other arbitrary attention schemes.
Multi-head attention. Given a source sequence S ∈ RLs×n of length Ls and target sequence
T ∈ RLt×m of length Lt, the module first projects the target onto query Q ∈ RLt×hd and the source
onto key K ∈ RLs×hd and value V ∈ RLs ×hd with the corresponding FC layers. These are split
into d-dimensional vectors of h heads, which is followed by a similarity function between Qi and Ki
producing a weight Πi = {softmax(d-2q；＞Ki)}L= ι for 1 ≤ i ≤ h. The weights are utilized to
aggregate V i into a centroid, giving Xi = Πi V i . Finally, the features in all heads are concatenated.
Poincare multi-head attention. Given the source and target as sequences of gyrovectors, they are
projected with three Poincare FC layers, followed by Poincare β-splits to produce Qi = {qti ∈
Bcd}tL=t1, Ki = {ksi ∈ Bcd}sL=s 1 and V i = {vsi ∈ Bcd}sL=s 1. Applying a similarity function fc and
activation g, each weight πti,s = g(fc(qti, ksi )) is obtained and the values are aggregated as follows:
Xi =㊀i≤s≤l [v；,∏i,s] c. Finally, the features in all heads are POinCare β-concatenated.
For the similarity function fc, there are mainly two choices to exploit. one is the inner product in the
tangent space indicated by Micic & Chu (2018), which is the naive generalization of the Euclidean
version. Another choice is based on the distance of two points: f c(q, k) = -τ dc(q, k) - γ, where τ
is an inverse temperature and γ is a bias parameter, which was proposed by Gulcehre et al. (2018).
As for the activation g, g(x) = exp (x) is the most basic option because it turns to be a softmax
operation due to the property of gyromidpoint. Gulcehre et al. (2018) also suggested g(x) = σ(x). in
light of the property of the generalized gyromidpoint, g as an identity function is also exploitable.
6
Published as a conference paper at ICLR 2021
Table 1: Test F1 scores for four sub-trees of the WordNet noun hierarchy. The first column indicates
the number of nodes in each sub-tree for the training and test times. For each setting, we report
the 95% confidence intervals for three different trials. Note that the number of parameters of the
Euclidean MLR and our approach is D + 1, whereas for the MLR layer of HNNs, it is 2D.
RootNode	Model	D=2	D=3	D=5	D=10
AnimAl n ∩ 1	Unidirectional (ours)	60.69±4.05	67.88±1.18	86.26±4.66	99.15±0.46
anma.n. 3218 /798	HNNs	59.25±16.88	70.59±1.38	85.89±3.77	99.34±0.39
	Euclidean	39.96±o.89	60.20±0.89	66.20±2.11	98.33±1.12
group.n.01 6649 / 1727	Unidirectional (ours)	74.27±i.5o	63.90±6.46	84.36±1.79	85.60±2.75
	HNNs	76.69±1.82	66.79±1.12	84.44±1.88	86.87±1.26
	Euclidean	47.65±o.65	55.15±0.97	71.21±1.81	81.01±1.81
mammal π ∩ 1	Unidirectional (ours)	63.48±3.76	94.98±3.87	99.30±0.30	99.17±1.55
mamma.n. 953 / 228	HNNs	46.96±13.86	95.18±4.19	98.89±1.29	98.75±0.51
	Euclidean	15.78±0.66	36.88±3.83	60.53±3.27	65.63±2.93
Incatinn π ∩1 ocaton.n.	Unidirectional (ours)	42.60±2.69	66.70±2.67	78.18±5.96	92.34±1.84
2689 / 673	HNNs	42.57±5.03	62.21±26.44	77.26±2.02	85.14±2.86
	Euclidean	34.50±0.34	31.44±0.76	63.86±2.18	82.99±3.35
4	Experiments
In this section, we evaluate our methods in comparisons with HNNs and Euclidean counterparts. The
implementation of hyperbolic architectures is based on the Geoopt (Kochurov et al., 2020).
4.1	Verification of the MLR classification capacity
We first evaluated the performance of our unidirectional Poincare MLR on the same conditioned
experiment designed for the MLR of HNNs, that is, a sub-tree classification on the Poincare ball
model. In this task, the Poincare embeddings of the WordNet noun hierarchy (Nickel & Kiela, 2017)
are utilized as the data set, which contains 82,115 nodes and 743,241 hypernymy relations. We
pre-trained the Poincare embeddings of the same dimensions as the experimental settings in HNNs,
i.e., two, three, five, and ten dimensions, using the open-source implementation2 to extract several
sub-trees whose root nodes are certain abstract hypernymies, e.g., animal. For each sub-tree, MLR
layers learn the binary classification to predict whether each given node is included. All nodes are
divided into 80% training nodes and 20% testing nodes. We trained each model for 30 epochs using
Riemannian Adam (Becigneul & Ganea, 2019) with a learning rate of 0.001 and a batch size of 16.
The F1 scores for the test sets are shown in Table 1. From the results, we can confirm the tendency of
the hyperbolic MLRs to outperform the Euclidean version in all settings, which illustrates that MLR
considering the hyperbolic geometry are better suited to the hyperbolic embeddings. In particular,
our parameter-reduced approach obtains the same level of performance as a conventional hyperbolic
MLR in a more stable training, as can be seen from the relatively narrower confidence intervals.
4.2	Amortized clustering of mixture of Gaussians with Set Transformers
For the evaluation of a Poincare multi-head attention, we utilize Set Transformer, which we consider
is a proper test case to eliminate the implicit influence of unessential operations, e.g., positional
encoding. The task is an amortized clustering of a mixture of Gaussians (MoG). In each sample
in a mini-batch, models take hundreds of two-dimensional points randomly generated by the same
K-component MoG, and directly estimate all the parameters, i.e., the ground truth probabilities,
means, and standard deviations, in a single forward step. We basically follow the model architectures
and experimental settings of the official implementation3, except that we employed the hyperbolic
Gaussian distribution (Ovinnikov, 2019) as well as the Euclidean distribution aiming to verify the
performance of the hyperbolic architectures both for the ordinary settings and for their desirable data
2https://github.com/facebookresearch/poincare-embeddings
3https://github.com/juho-lee/set_transformer
7
Published as a conference paper at ICLR 2021
distributions. When the hyperbolic models need to deal with Euclidean coordinates, the inputs or
outputs are projected by an exponential map or logarithmic map, respectively, with a scalar parameter
for scaling the values to the fixed-radius Poincare ball Bn. Note that We omit ReLU activations for our
models because the hyperbolic operations are inherently non-linear. We also remove normalization
layers because there does not exist enough research on the normalizing criterion in the hyperbolic
space and existing methods possibly reduce the representational poWer of gyrovectors by neutralizing
their norms. For the hyperbolic attentions, based on a preliminary experiment, We choose to utilize
the distance based similarity function and exponential activation.
Table 2: Negative log-likelihood on the test set. For each setting, We report the 95% confidence
intervals for five trials. The numbers in brackets indicate the diverged trials, the final scores of Which
Were higher than 10.0, and those trials are not accounted into the reported scores.
Model	K=4	K=5	K=6	K=7	K=8
	Gaussian distribution on		Euclidean space		
Set Transformer W/o LN	1.556±0.214 (3)	1.912±0.701 (2)	2.032±0.193 (3)	5.066±5.239 (3)	2.608±N/A (4)
Set Transformer	1.558±0.032 (0)	1.776±0.030 (0)	2.046±0.030 (0)	2.297±0.047 (0)	2.519±0.020 (0)
Ours	1.558±0.008 (0)	1.833±0.046 (0)	2.081±0.036 (0)	2.370±0.098 (0)	2.682±0.164 (0)
Generalized Gaussian distribution on the Poincare ball model (Ovinnikov, 2019)					
Set Transformer	3.084±0.305 (0)	3.298±0.414 (0)	3.327±0.316 (0)	3.923±1.632 (0)	3.519±0.125 (0)
Ours	2.920±0.029 (0)	3.087±0.014 (0)	3.252±0.037 (0)	3.375±0.033 (0)	3.462±0.013 (0)
The results are shoWn in Table 2. For the Euclidean distribution, our models achieved almost the
same performance as Set Transformers, While the training of those Without Layer Normalization for
the same conditioned comparison often failed under all settings. This result suggests the intrinsic
normalization properties of our methods, Which We attribute to the computation using vector norms.
For the hyperbolic distribution, our models outperformed the Euclidean counterparts With an order of
magnitude smaller confidence intervals, Which indicates that our hyperbolic architectures are indeed
suited to their assumed data distribution.
4.3	Convolutional sequence to sequence modeling
Finally, We experimented With the convolutional sequence-to-sequence modeling task for machine
translation of WMT’17 English-German (Bojar et al., 2017). Because the architecture is composed of
convolutional layers and attention layers, the hyperbolic version of Which has already been verified
in Section 4.2, it Would provide a comparison focusing on convolutional operations. It also has
a practical aspect as a task of natural language processing, in Which lexical entities are knoWn
to form latent hierarchical structures. We folloW the open-source implementation of Fairseq (Ott
et al., 2019), Where preprocessed training data contains 3.96M sentence pairs With 40K sub-Word
tokenization in each language. In our hyperbolic models, feature vectors are completely treated as
Mobius gyrovectors because token embeddings can be learned directly on the Poincare ball model.
Note that the inputs for the sigmoid functions in Gated Linear Units are logarithmically mapped
just like hyperbolic Gated Recurrent Units proposed by Ganea et al. (2018a). We train various
scaled-doWn models to verify the representational capacity of our hyperbolic architectures, With
Riemannian Adam for 100K iterations. For more implementation details, please check Appendix E.
Table 3: BLEU-4 scores (Papineni et al., 2002) on the test sets neWstest2013. The target sentences
Were decoded using beam search With a beam size of five. D indicates the dimensions of token
embeddings and the final MLR layer.
Model	D=16	D=32	D=64	D=128	D=256
ConvSeq2Seq	2.68	8.43	14.92	20.02	21.84
Ours	9.81	14.11	16.95	19.40	21.76
The results are shoWn in Table 3. Our model demonstrates the significant improvements compared
to the usual Euclidean models in the feWer dimensions, Which reflects the immense embedding
8
Published as a conference paper at ICLR 2021
capacity of hyperbolic spaces. On the other hand, there is no salient differences observed in higher
dimensions, which implies that the Euclidean models with higher dimensions than a certain level can
obtain a sufficient computational complexity through the optimization. This would fill the gap with
the representational properties of hyperbolic spaces. It also implies that the proper construction of
neural networks with the product space of multiple small hyperbolic spaces using our methods has
the potential for the further improvements even in higher dimensional architectures.
5	Conclusion
We showed a novel generalization and construction scheme of the wide range of hyperbolic neural
network architectures in the Poincare ball model, including a parameter-reduced MLR, geodeSic-
aware FC layers, convolutional layers, and attention mechanisms. These were achieved under a
unified mathematical backbone based on the concepts of Riemannian geometry and the Mobius
gyrovector space, which endow our hyperbolic architectures with theoretical consistency. Through the
experiments, we verified the effectiveness of our approaches from diversified tasks and perspectives,
such as an embedded sub-tree classification, amortized clustering of distributed points both on the
Euclidean space and the Poincare ball model, and neural machine translation. We hope that this study
will pave the way for future research in the field of geometric deep learning.
Acknowledgments
We would like to thank Naoyuki Gunji and Yuki Kawana from the University of Tokyo for their
helpful discussions and constructive advice. We would also like to show our gratitude to Dr. Lin
Gu from RIKEN AIP, Kenzo Lobos-Tsunekawa from the University of Tokyo, and Editage4 for
proofreading the manuscript for English language.
This work was partially supported by JST AIP Acceleration Research Grant Number JPMJCR20U3,
JST CREST Grant Number JPMJCR2015, JSPS KAKENHI Grant Number JP19H01115, and Basic
Research Grant (Super AI) of Institute for AI and Beyond of the University of Tokyo.
References
Ben Andrews and Christopher Hopper. The Ricci flow in Riemannian geometry: a complete proof of
the differentiable 1/4-pinching sphere theorem. Springer, 2010.
Ivana Balazevic, Carl Allen, and Timothy Hospedales. Multi-relational Poincare Graph Embeddings.
In Advances in Neural Information Processing Systems 32, pp. 4463-4473. Curran Associates,
Inc., 2019.
Gary Becigneul and Octavian-Eugen Ganea. Riemannian Adaptive Optimization Methods. In
International Conference on Learning Representations, 2019.
Thomas Blasius, Tobias Friedrich, Anton Krohmer, Soren Laue, Anton Krohmer, Soren Laue, Tobias
Friedrich, and Thomas Blasius. Efficient Embedding of Scale-Free Graphs in the Hyperbolic Plane.
IEEE/ACM Trans. Netw., 26(2):920-933, April 2018. ISSN 1063-6692. doi: 10.1109/TNET.2018.
2810186.
Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian
Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri,
Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. Findings of the 2017 Conference on
Machine Translation (WMT17). In Proceedings of the Second Conference on Machine Translation,
Volume 2: Shared Task Papers, pp. 169-214, Copenhagen, Denmark, September 2017. Association
for Computational Linguistics.
Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic Graph Convolutional
Neural Networks. In Advances in Neural Information Processing Systems 32, pp. 4868-4879.
Curran Associates, Inc., 2019.
4http://www.editage.com
9
Published as a conference paper at ICLR 2021
Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-Margin Classification in
Hyperbolic Space. In Proceedings of Machine Learning Research, volume 89 of Proceedings of
Machine Learning Research,pp. 1832-1840. PMLR, 16-18 APr 2019.
Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic Neural Networks. In Advances
in Neural Information Processing Systems 31, pp. 5345-5355. Curran Associates, Inc., 2018a.
Octavian-Eugen Ganea, Gary BecigneUL and Thomas Hofmann. Hyperbolic Entailment Cones for
Learning Hierarchical Embeddings. In ICML, 2018b.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional
Sequence to Sequence Learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of
the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1243-1252, International Convention Centre, Sydney, Australia, 06-11
Aug 2017. PMLR.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning Mixed-Curvature Representa-
tions in Product Spaces. In International Conference on Learning Representations, 2019.
Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz
Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, and Nando de Freitas.
Hyperbolic Attention Networks. arXiv preprint arXiv:1805.09786, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaoru Katayama and Ernest Weke Maina. Indexing Method for Hierarchical Graphs based on
Relation among Interlacing Sequences of Eigenvalues. Journal of Information Processing, 23(2):
210-220, 2015. doi: 10.2197/ipsjjip.23.210.
Max Kochurov, Rasul Karimov, and Sergei Kozlukov. Geoopt: Riemannian Optimization in PyTorch.
arXiv preprint arXiv:2005.02819, 2020.
Dmitri Krioukov, Fragkiskos Papadopoulos, Amin Vahdat, and Marign Bogund. Curvature and
temperature of complex networks. Physical Review E, 80(3):035101, 2009.
Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Maridn Bogund.
Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.
John Lamping, Ramana Rao, and Peter Pirolli. A Focus+Context Technique Based on Hyperbolic
Geometry for Visualizing Large Hierarchies. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’95, pp. 401-408, New York, NY, USA, 1995. ACM
Press/Addison-Wesley Publishing Co. ISBN 0-201-84705-1. doi: 10.1145/223904.223956.
Marc Law, Renjie Liao, Jake Snell, and Richard Zemel. Lorentzian Distance Learning for Hyperbolic
Representations. In Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 3672-3681, Long Beach, California,
USA, 09-15 Jun 2019. PMLR.
Guy Lebanon and John Lafferty. Hyperplane Margin Classifiers on the Multinomial Manifold. In
Proceedings of the Twenty-First International Conference on Machine Learning, ICML ’04, pp.
66. Association for Computing Machinery, 2004.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
3744-3753, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Henry W Lin and Max Tegmark. Critical Behavior in Physics and Probabilistic Formal Languages.
Entropy, 19(7):299, 2017.
10
Published as a conference paper at ICLR 2021
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic Graph Neural Networks. In Advances in
Neural Information Processing Systems 32, pp. 8230-8241. Curran Associates, Inc., 2019.
Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous
Hierarchical Representations with POinCare Variational Auto-Encoders. In Advances in Neural
Information Processing Systems 32, pp. 12565-12576. Curran Associates, Inc., 2019.
Marko Valentin Micic and Hugo Chu. Hyperbolic Deep Learning for Chinese Natural Language
Understanding. arXiv preprint arXiv:1812.10408, 2018.
Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A Wrapped
Normal Distribution on Hyperbolic Space for Gradient-Based Learning. In ICML, 2019.
Mark EJ Newman. Power laws, Pareto distributions and Zipf’s law. Contemporary physics, 46(5):
323-351, 2005.
Maximilian Nickel and Douwe Kiela. Learning Continuous Hierarchies in the Lorentz Model of
Hyperbolic Geometry. In ICML, 2018.
Maximillian Nickel and Douwe Kiela. Poincare Embeddings for Learning Hierarchical Repre-
sentations. In Advances in Neural Information Processing Systems 30, pp. 6338-6347. Curran
Associates, Inc., 2017.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019.
Ivan Ovinnikov. Poincare Wasserstein Autoencoder. arXiv preprint arXiv:1901.01427, 2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Peter Petersen, S Axler, and KA Ribet. Riemannian Geometry, volume 171. Springer, 2006.
Frederic Sala, Chris De Sa, Albert Gu, and Christopher Re. Representation Tradeoffs for Hyperbolic
Embeddings. In Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 4460-4469, Stockholmsmassan, Stockholm
Sweden, 10-15 Jul 2018. PMLR.
Ondrej Skopek, Octavian-Eugen Ganea, and Gary Becigneul. Mixed-curvature Variational Autoen-
coders. In International Conference on Learning Representations, 2020.
Michael Spivak. A Comprehensive Introduction to Differential Geometry. Publish or Perish, 1979.
A. Tifrea, G. Becigneul, and O.-E. Ganea. Poincare GloVe: Hyperbolic Word Embeddings. In 7th
International Conference on Learning Representations (ICLR), May 2019.
Abraham Ungar. Gyrovector Spaces And Their Differential Geometry. Nonlinear Functional Analysis
and Applications, 10, 01 2005.
Abraham Ungar. Einstein’s Special Relativity: The Hyperbolic Geometric Viewpoint. PIRT Confer-
ence Proceedings, 02 2013.
Abraham A Ungar. Hyperbolic Trigonometry and its Application in the Poincare Ball Model of
Hyperbolic Geometry. Computers & Mathematics with Applications, 41(1-2):135-147, 2001.
Abraham A Ungar. Analytic Hyperbolic Geometry and Albert Einstein’s Special Theory of Relativity.
World Scientific, 2008.
Abraham A Ungar. Beyond the Einstein Addition Law and its Gyroscopic Thomas precession: The
Theory of Gyrogroups and Gyrovector Spaces, volume 117. Springer Science & Business Media,
2012.
Abraham Albert Ungar. A Gyrovector Space Approach to Hyperbolic Geometry. Synthesis Lectures
on Mathematics and Statistics, 1(1):1-194, 2009.
11
Published as a conference paper at ICLR 2021
A Hyperb olic Geometry
In this section, We review the definition of the hyperbolic geometry models other than the Poincare
ball model and the relationships between their coordinates.
Hyperboloid model. The n-dimensional hyperboloid model is a hypersurface in an (n + 1)-
dimensional Minkowski space R1n+1, which is equipped with an inner product hx, yiL = x>gLy for
∀x, y ∈ R1n+1, where gL = diag(-1, 1n>). Given a constant negative curvature -c, the manifold of
the hyperboloid model is defined by Hcn = {x = (x0, . . . , xn)> ∈ R1n+1 | chx, xiL = -1, x0 > 0}.
Note that, in this standard (n + 1)-dimensional coordinate system, the metric tensor as a positive
definite matrix for the n-dimensional hyperboloid manifold cannot be defined. Instead, when the
hyperboloid model is represented in a specific n-dimensional coordinates, e.g., hyperbolic polar
coordinates, then its metric tensor has the corresponding representation as the n-dimensional positive
definite matrix.
Isometric isomorphism with the Poincare ball model. The bijection between an arbitrary point
h = (z, k>)> ∈ Hcn and its unique corresponding point b ∈ Bcn, depicted in Figure 1, is given by
the following:
k
Hn→Bn： b = b(h)= ——,	(9)
c c	1 + cz
Bn → Hn	： h = h(b)=(z (b),	k (b)) =	(√=1 +	ckbk2,1	2b, ll2). (io)
c c	c1 -	ckbk2 1 - ckbk2
Beltrami-Klein model. The n-dimensional Beltrami-Klein model of a constant negative curvature
-C is defined by (Kn, gc), where Kn = {x ∈ Rn | Ckxk2 < 1} and gX = (1 - CIlxlI2)-1In + (1 -
Ckxk2)-2xx>. Here, Kn is an open ball of radius 1∕√c.
Isometric isomorphism with the Poincare ball model. The bijection between an arbitrary point
n ∈ Kcn and its unique corresponding point b ∈ Bcn, depicted in Figure 1, is given by the following:
Kcn → Bcn : b = b (n) =
n
1 + pl - Cknk2
Bcn → Kcn : n = n (b) =
2b
1 + Ckbk2
(11)
(12)
B MOBIUS GYROVECTOR SPACE
In this section, we briefly introduce the concept of the Mobius gyrovector space, which is a specific
type of gyrovector spaces. For a rigorous theoretical and detailed mathematical background of this
system, please refer to Ungar (2005; 2009; 2001; 2012).
A gyrovector space is an algebraic structure that endows the points in a hyperbolic space with
vector-like properties based on a special concept called a gyrogroup. This gyrogroup is similar to
ordinary vector spaces that provides a Euclidean space with the well-known vector operations based
on the notion of groups. As a particular example in physics, this helps to understand the mathematical
structure of the Einstein’s theory of special relativity where no possible velocity vectors including the
sum of velocities in an arbitrary additive order can exceed the speed of light (Ungar, 2008; 2013).
Because hyperbolic geometry has several isometric models, a gyrovector space also has some variants
where the Mobius gyrovector space is a variant for the Poincare ball model.
As an abstract mathematical system, a gyrovector space is constructed through the following steps:
(1) Start from a set G. (2) With a certain binary operation ㊉，create a tuple called a groupoid, or
magma (G,㊉).(3) Based on five axioms, define a specific type of magma as a gyrogroup. These
axioms include several important properties of gyrovector spaces, such as the left gyroassociative
law and an operator called a gyrator gyr : G X G → Aut(G,㊉)，which generates an automorphism
Aut(G,㊉)3 gyr[x, y] : G → G given by z → gyr[x, y]z, called a gyration, from two arbitrary
points x and y ∈ G. The notion of the gyrocommutative law and gyrogroup cooperation are given in
this step. (4) Adding ten more axioms related to the statements about a real inner product space and a
scalar multiplication 0, the gyrovector space (G,㊉，0) is thus defined.
12
Published as a conference paper at ICLR 2021
Some of the important properties of a gyrovector space are listed below. Here, x, y, z ∈ G.
Gyroassociative laws. Although the binary operation ㊉ is not necessarily associative in general, it
obeys the left gyroassociative law X ㊉(y ㊉ Z) = (X ㊉ y)㊉ gyr[x, y]z and right gyroassociative law
(X ㊉ y)㊉ z = x ㊉(y ㊉ gyr[y, x]z). These equations also provide a general closed-form expression
of the gyrations: gyr[x, y]z =㊀(X ㊉ y)㊉(X ㊉(y ㊉ z)).
Cases in which gyrations become identity maps. If at least one element for gyr is 0 ∈ G, the
gyrations become an identity map I: gyr[X, 0] = gyr[0, X] = I. With the loop properties of the
gyrations given by gyr[X, y] = gyr[X ㊉ y, y] = gyr[X, y ㊉ x], many other cases can be also derived.
Gyrocommutative law. Although a binary operation ㊉ is not necessarily commutative in general, if
it obeys the equation X ㊉ y = gyr[X, y](y ㊉ x), the gyrogroup is called gyrocommutative.
Gyrogroup cooperation. Regarding ㊉ as the primal binary addition, the second binary addition in
G is defined as the gyrogroup cooperation 田，which is given by X 田 y = X ㊉ gyr[X,㊀y]y. This
has duality symmetries with the first binary operation ㊉，such that X ㊉ y = X 田 gyr[X, y]y. In
addition, corresponding to the left cancellation law ㊀X ㊉(x ㊉ y) = y inherent in ㊉，the gyrogroup
cooperation induces two types of the right cancellation laws: (x ㊉ y)日 y = (x 田 y)㊀ y = x.
In this formalism, the Mobius gyrovector space is then defined as (Bn,㊉c, 0c), where Bn is as
previously introduced in Section 2, and ㊉C and Xc are as shown in the following subsections.
B.1	MOBIUS ADDITION
In the Mobius gyrovector space, the primary binary operation is denoted as the Mobius addition
㊉c : Bn X Bn → Bn，which is a noncommutaive and nonassociative addition, given by the following:
X ㊉ U= (1 + 2chχ, yi + Ckyk2) X +(1-CkXk2) y X θ U = X ㊉ j)	(13)
X ㊉c y =	1 + 2chx, yi + C2kxk2kyk2	，X θc y = X ㊉c ( y).	(13)
B.2	Möbius Gyrator
The expression of gyrations in the Mobius gyrovector space can be expanded using the equation of
the Mobius addition ㊉c, which is described by Ungar (2009) as follows:
gyr[X, y] : z 7→ z
2 (ChX, Zikyk2 — hy, zi (1 + 2chx, y〉))X + (c〈y, ZikXlI2 + m Zi) y
C	1 + 2c(x, yi + c2kxk2kyk2	.
(14)
By writing down all the special operators ㊉c for the gyrovectors in Bn into the normal vector
operations, the expression of the gyrations can be now seen as a general function for the any real
vector Z ∈ Rn. Indeed, gyrations are extended to invertible linear maps ofRn (Ungar, 2009).
The Mobius gyrator endows the Mobius gyrovector space with a gyrocommutative nature.
B.3	Möbius coaddition
The gyrogroup cooperation in the Mobius gyrovector space is called the Mobius coaddition, and is
given by the following:
中	G [ a ]	(1-ckyk2)X + (1-ckxk2) y
X 田c y = X ㊉c gyr[x, θcy]y = ----1-c2kxk2ky∣∣2------.
With the gamma factor Yx = (，1 - CkXk2)-1 for X ∈ Bn，this is also described in the following
manner:
X c y
Yx X + Yy y
γx+Yy- 1.
Note that the Mobius coaddition is not associative but is commutative.
(15)
13
Published as a conference paper at ICLR 2021
B.4 MOBIUS SCALAR MULTIPLICATION
The Mobius scalar multiplication for X ∈ Bn and r ∈ R is given by the following:
r 0c x = √ tanh-1 (r tanh (√c Ilxk)) [x] = exp0 (r log0 (x)).	(16)
In terms of the Riemannian geometry, the Mobius scalar multiplication adjusts the distance of x from
the origin by the scalar multiplier r. The expressions of the logarithmic map logcx and distance in the
Mobius gyrovector space are described in the following subsections.
C Poincare ball model
Owing to the algebraic structure provided by the Mobius gyrovector space, many properties related
to the geometry of the POinCare ball model can be described in implementation-friendly closed-form
expressions.
C.1 Exponential and logarithmic maps
The exponential map expcx : TxBcn → Bcn is described in (Ganea et al., 2018a, Lemma 2) as follows:
expX (V) = x ㊉C √c tanh ("cXxkvk ) [v], ∀χ ∈ Bn, V ∈ T^B3
The logarithmic map logcx = (expcx)-1 : Bcn → TxBcn is also given by the following:
logx (y)	= √λc	tanh-1	(√C∣	㊀C x	㊉C	yk)[㊀Cx	㊉C	y],	∀χ, y ∈ Bn
(17)
(18)
C.2 Distance
C.2. 1 Poincaré distance between two arbitrary points
The distance function dC is originally and preliminary defined as a binary operation for indicating the
distance between two arbitrary points x, y ∈ BCn . Based on the notion of the Mobius addition, the
distance dC : BCn × BCn → R is succinctly described as follows:
2
"。(X，y) = √C tanh 1 √k ©c X ㊉C yk) = k logx (y)kx∙	(19)
Despite the noncommutative aspect of the Mobius addition ㊉c, this distance function in Equation
19 becomes commutative thanks to the commutative aspect of the Euclidean norm of the Mobius
addition, which is expressed as follows:
kx ㊉C yk=S ι+τ(; yix+"kwk`k2, ∀ x, y ∈ Bn.	QO)
C.2.2 Distance from a point to Poincaré hyperplane
In the Euclidean geometry, the generalized concept of two-dimensional plane to a higher dimensional
space Rn is a hyperplane containing an arbitrary point p ∈ Rn and is the set of all straight lines
orthogonal to an arbitrary orientation vector a ∈ Rn . Because straight lines in Euclidean spaces
are geodesics in terms of the Riemannian geometry, a hyperplane can be generalized as another
Riemannian manifold Mn such that the hyperplane contains an arbitrary point p ∈ Mn and is the
set of all geodesics orthogonal to an arbitrary orientation vector at p, namely, the tangent vector
a ∈ TpMn. This concept in the Poincare ball model has been rigorously defined in (Ganea et al.,
2018a, Definition 3.1) forp ∈ BCn, a ∈ TpBCn as follows:
Ha,p = {x ∈ Bn Ihlogp (x), aip = 0} = expp ({a}⊥)	(21)
={χ ∈ Bn ∣ h㊀CP ㊉ C x, a = o}.	(22)
14
Published as a conference paper at ICLR 2021
Note that {a}⊥ is the set of all tangent vectors atp and orthogonal to a.
Ganea et al. (2018a) have proven the closed-form description of the distance from a point x ∈ Bcn
to an arbitrary Pomcare hyperplane Hac,p by considering the minimum distance between X and any
point in Hac,p:
dc χ,Ha,p
inf dc (x，w) = 3 SinhT (	*3* ㊉C x,ai |	).
w∈Ha,p 八，'	√	<(1-ckθc P ㊉。xk2)ka疗
C.3 Parallel transport
The concept of a parallel transport is traditionally derived from differential geometry. In the hyperbolic
geometry, the gyrovector space provides the algebra to formulate the parallel transport of a gyrovector
(Ungar, 2012). When a gyrovector θcX ㊉C W ∈ Bn rooted at a point X ∈ Bn is transported parallel
to another gyrovector θy ㊉C Z ∈ Bn rooted at a point y ∈ Bn along a geodesic connecting X and
y, the equation below is satisfied:
θcy ㊉ C z = gyr[y, θcχ](θcχ ㊉ C W).	(24)
Because the exponential map in the Poincare ball model is a bijective function, the parallel transported
gyrovectors W and z can be regarded as the exponential mapped tangent vectors v ∈ TxBCn rooted at
X and u ∈ TyBCn rooted at y, respectively, that is,
θCy ㊉C expy (u) = gyr[y, θCx](θCX ㊉C expX (V)).	(25)
With Equations 17 and 18 and the properties of the Mobius gyration described in Appendix B, a
succinct expression of the tangent parallel transport PxC→y : TxBCn → TyBCn can be obtained as
follows:
λC
Pχ→y(v) := U = τχgyr[y, θCx]v.	(26)
x→y	λCy
Note that, in a special case in which X = 0 and v ∈ T0BCn, this equation is simplified as follows:
P0→y(V) = λ0V = (1-ckyk2) V.	(27)
λy
One can confirm that Equation 26 deserves to be called a parallel transport in terms of the differential
or Riemannian geometry by checking the covariant derivative associated with the Levi-Civita con-
nection of Pχ→y along a tangent vector field γ(t) on a smooth curve γ(t) from X to y vanishes to
0.
D Supplemental proofs for proposed methods
D.1 FINAL DEFORMATION OF THE PROPOSED UNIDIRECTIONAL POINCARE MLR
Proof. First, we clarify the relation between the Poincare hyperplane HaC ,p, described in Appendix
C.2.2, and the variants 禹/ introduced in Section 3.1:
—_ ~ -
HC = HC .
Ha,r = Ha,qak,rk .
(28)
We then start the derivation of Equation 6 from the variables ak and qak,rk described in Section
3.1. Following Equation 28 and the concept of the distance from a point to a Poincare hyperplane
described in Equation 23, the generalized MLR score function vk in Equation 3 can be written as
follows:
λqak,rk kak k ∙∙h-1 (	2√chθCqak,rk ㊉C x, aki ʌ
一√ — Sin l(1-ckθC qak,rk ㊉C xk2) kakk 八
∀X ∈ BCn .
(29)
With Equation 20, we obtain
Il θC qak,rk ㊉C xk
kxlF - 2hx, qak ,rk i + kqak ,rk IF
1 - 2chx, qak,rk i + NkxlFkqaVkk2
(30)
15
Published as a conference paper at ICLR 2021
Therefore, we can expand the term inside the sinh-1 function in Equation 29 in the following manner:
2√ch㊀cqak,rk ㊉C x, aki
(1-Ck ㊀ C Qak ,rk ㊉ C Xk2) kak k
2√c	- (1 - 2chχ, qak,rk i + CllxlI 2) hqak,rk , ak i + (1 - Ckqak,rk ∣F) hx, ak i ⑶)
Ilak k 1 - 2chx, Qak,rk i + c2kxk2kqak,rk k2 - c (kxk2 - 2hx, Qak,rk i + |小%,"|2)
- - (1 - 2chx, qak ,rk i + Ckxk 2) hqak,rk , [ak]i + (1 - Ckqak,rk |户)hx, [ak]i
1-ckqak ,rkk2- Ckxk2 + C2kxk2kqak,rkk2
2
1-Ckxk2
√ (1 — 2Chx, Qak ,rki + Ckxk2) hlak,rk ,[ak]i
1 - Cgak ,rk k2
+ √C(x, [ak]i
(33)
With Equations 5 and 17, the term in the outer brackets in Equation 33 can be further expanded into
the form using rk and zk described in Section 3.1:
√c (1 — 2C〈x, Qak ,rk i + Ckxk2) hQak,rk ,[ak]i
1 - Cgak,rk k2
+ √C(x, [ak]i
(1 — 2√ctanh (√Crk) hx, [zk]i + Ckxk2) tanh (√Crk)
1 — tanh2 (√Crk)
+ √C(x, [zk]i
(34)
—(1 + Ckxk2) Sinh (√Crk) Cosh (√Crk) + √C(x, [zk]i (1 + 2sinh2 (√Crk))	(35)
1 + Ckxk 2	一/ L 、 L ' . 、、 一/ L 、
----2-- sinh(2√Crk) + √c(x, [zk]i Cosh(2√Crk).
(36)
In addition, we can also expand the term outside the sinh-1 function in Equation 29 using Equations
5 and 17 as follows:
%ak"k kakk =	2 kakk	= 2 Ibech2 (√≡rk) Zkll = 2 kzkk (37)
√c	√c (1-Ckqak,r%k2)	√C (1 — tanh2 (√Crk))	√ '
Combining Equations 29, 33, 36, and 37, we finally conclude the proof through the following:
(、2kzkk . i (2√C<x, [zk]i	/ r 、	1 + Ckxk2 .卜 Sl-八 CQ
Vk(X) = -√τsinh	( 1 -Ckxk2 cosh(27Crk)— T-Wsinh(27Crk))	(38)
=2√√kk sinh-1 (λXh√Cx, [Zk]i cosh (2√Crk) — (λχ — 1)sinh (2√Crk)) .	(39)
□
D.2 Convergence proof OF unidirectional POINCARE MLR TO Euclidean MLR
Proof. For the intended proof, we first introduce the following proposition:
Proposition 1. For x 6= 0, sinh(x) over x converges to 1 in the limit x → 0:
lim sinh(X) = 1.
x→0	x
(40)
Proof. The result can be obtained based on the definition of the differentiation of a scalar function:
sinh(x)
lim--------
x→0	x
Iim ex — e-x — 1 Iim Px - 1	e-x - 1
x→0	2x	2 x→0 x	—x
ex — e0	dex
lim-------= ――	= 1.
x→0 x	dx x=0
(41)
(42)
—
□
From Proposition 1, we derive the following two propositions.
16
Published as a conference paper at ICLR 2021
Proposition 2. For t ∈ R, x 6= 0, sinh(tx) over x converges to t in the limit x → 0:
lim SinMtx) = t.
x→0	x
(43)
Proof. We divide this proof into two cases:
sinh(tx)	0 = t	(t = 0)
lim---------= ∖ s sinh(tx)	,	… .	..	‹、
χ→0	x 11 lim ----------------- = t (t = 0, Proposition 1)
tx→0	tx
Proposition 3. Fort ∈ R, x 6= 0, sinh-1(tx) over x converges to t in the limit x → 0:
lim SinhT(tx) = t.
x→0	x
Proof. We can directly utilize Proposition 1 as follows:
Sinh-1(tx)
lim-----------
x→0	x
1∙ ts
lim .一、
s→0 Sinh(s)
t lim (Sinh(S)「= t
s→0	s
(s = Sinh-1(tx))
(Proposition 1).
(44)
□
(45)
(46)
(47)
□
With Propositions 2 and 3, we can now take the limit of Equation 6 as follows:
lim vk (x)
c→0
lim2⅛
c→0 √C
lim2⅛
c→0 c/C
Sinh-1
2√Chx, [zk])
1 - Ckxk2
Sinh-1
2hx, [zk]i	、J- /- ∖
1 - Ckxk2 cosh(2λzCrk)-
Cosh (2√crk)- T+wsinh (2√crk)
1 + Ckxk2 Sinh (2√crk)
1 - ckxk2	√c
(48)
(49)
= 2 kzkk (2hx, [zk]i - 2rk) = 4(hx,zki - rk kzkk) .
Moreover, with Equation 5, we can confirm that zk matches ak in the limit C → 0:
1
(50)
lim ak = lim sech2 (√crk) Zk
c→0	c→0
㈣ cosh2 (√Crk) Zk = zk
(51)
Combining it with Equations 2 and 50, we finally conclude the proof as follows:
cli→m0vk(x)=4(hx,aki -rkkakk)=4(hak,xi -bk), where bk:= rk kakk.
(52)
Here, the factor 4 is derived from the squared conformal factor (λ0x)2 degenerating into a constant
value. This corresponds to the fact that the POincare ball model Bn converges to the Euclidean space
Rn in the limit C → 0 except for the same multiplier lim(λχ)2 = 4 owing to its metric tensor. □
c→0 x
D.3 PROOF OF THE PROPERTIES OF OUTPUT COORDINATES OF POINCARE FC LAYER
Proof. To check the properties of the Poincare FC layer described in Section 3.2, we first clarify the
Poincare hyperplane containing the origin and orthogonal to the k-th axis in Bcm . The k-th axis is a
geodesic passing through the origin and any point on it except the origin has a non-zero element in
only the k-th coordinates. Therefore, an arbitrary point x ∈ Bcm along the k-th axis can be written as
follows:
x = re(k), where e(k) = (δik)im=1 , r ∈
⊂ R,
(53)
which is as intuitive as in a Euclidean space. Specifically, r = 0 represents the origin.
We can then easily describe the intended Poincare hyperplane as follows:
17
Published as a conference paper at ICLR 2021
Definition 1. (Poincare hyperplane containing the origin and orthogonal to the k-th axis)
He(k) 0 = {x = (x1,x2,... ,Xm)> ∈ Bm | he(k), Xi = Xk = 0},	(54)
which is also intuitively obtained.
With Definition 1, the preparation for constructing y in Equation 7 is complete.
Derivation of y. Let x ∈ Bcn and y = (y1, y2, . . . , ym)> ∈ Bcm be the input and output of the
Poincare FC layer, respectively. Below, We start the proof with the score functions Vk (x) for
∀k = {1, 2, . . . , m} already obtained in the same way as in Equation 6.
To endow y the properties described in Section 3.2, i.e., the signed distance from y to each Poincare
hyperplane containing the origin and orthogonal to the k-th axis is equal to vk (x), we generate a
simultaneous equation for ∀k as follows:
dc (y,HC(k),o) = Vk(x).	(55)
With Equations 54 and 28 and the notion of the distance from a point to a Poincare hyperplane
described in Equation 23, these equations are expanded as follows:
-^sinhT (ι 2√c∣yk∣2 ) = Vk (x).	(56)
√c k1 - ckyk2√
Therefore, we obtain the following notation of the coordinates:
yk = 1 OCkyk sinh (√cvk(x)), ∀k-	(57)
2c
When considering the Euclidean norm ofy using Equation 57, the equation for kyk can be derived
as follows:
kyk
1 — Ckyk2
2√C	∖
m
Esinh2 (√Cvk(x)).
k=1
(58)
This can be succinctly rewritten as
kyk = 1_2yk- Ilwk, where W	=(√c sinh (√cvk(x)))	.	(59)
By solving this quadratic equation, the closed form of ky k is obtained through the following:
kyk = -1⅛ + jc⅛ρ + C.	(60)
Substituting Equations 59 and 60 for Equation 57 leads to Equation 7 in the notation of the coordinates:
yk =_F^ Wk = Ckwk2	—Wk	, ∀k.	(61) 1 + pl + Ckwk2
Confirmation of the existence ofy. Finally, we conclude the proof by checking that y is always
within the domain of the Poincare ball Bcm	=	{y	∈	Rm	|	Ckyk2 <	1}:
1 - Cky k2
2 ('I + Ckwk2 - 1)
Ckwk2
> 0.
(62)
□
18
Published as a conference paper at ICLR 2021
D.4 PROOF OF THE PROPERTIES OF POINCARE Q-SPLIT AND β-CONCATENATION
In this section, We prove the properties of the Poincare β-split and the Poincare β-concatenation
described in Section 3.3. The Poincare ball model is different from Euclidean neural networks,
on the simple calculation of the expected value and the variance of a particular value related to
a feature vector or weight matrix owing to the linearity in their operations. In the Poincare ball
model, calculating such values without any postulate for the probabilistic distribution that the feature
gyrovectors or tangent vectors follow is difficult owing to the nonlinear transformations in the
exponential and logarithmic maps. Thus, we first make the following naive assumption:
Assumption 1. Each coordinate of an n-dimensional tangent vector in T0Bcn follows a normal
distribution centered at zero with a certain variance σn.
The reasons why we assume the distribution on the tangent space rather than on the Poincare ball
model itself are as follows:
1.	It is improper to assume a continuous and smooth distribution onto the space with an upper-
bounded radius because there must be no probability density on or outside the boundary.
The rough idea of discontinuing such probabilities outside the domain of the Poincare ball
and discretely taking only the inside into account seems to lack rationality.
2.	One simple way to avoid the above issue is to apply a uniform distribution from zero to the
ball radius based on the norm of the gyrovector. However, there is no guarantee that such
constancy in the distribution can be realized on a complexly curved geometric structure of
the Poincare ball model.
3.	Conversely, a tangent space is a linear space that is attached to the manifold and can be
treated as an ordinary vector space.
4.	The Poincare ball model is conformal to the Euclidean space, i.e., preserving the same
angles, and at the origin, the gyrovectors having the same norms are projected onto the
tangent vectors which also have the same norms with their angles unchanged.
5.	In Euclidean neural networks, the normal distribution is one of the most popularly considered
priors. The multivariate normal distribution is occasionally approximated as an independent
and identically distributed distribution for easier calculation.
Because the Poincare β-split and the Poincare β-concatenation are inverse functions to each other, it
is sufficient to prove the properties of either one of these operations. Here, we show a proof for the
Poincare β-concatenation. Recalling that βn = B(Tn, 1) and considering the following:
Poincare β-concatenation. The input gyrovectors {xi ∈ Bni }N=1 are first scaled by certain co-
efficients and concatenated in the tangent space, and then projected back to the Poincare ball as
follows:
Xi →	Vi	= log0(xi) ∈	T0Bni,	V := (βn-v>,...,	βn-VN)	→ y = exp0	(V)	∈ Bn.	(63)
βn1	βnN
Proof. At first, we consider the expected value of the norm of each tangent vector Vi , which is the
target of the POincare β-concatenation. Because the value ti := Ckvik follows a χ2 distribution
σni
based on Assumption 1, the expected value of kVi k can be obtained as follows:
E[kvik] = 2矍γ1(ni)/ kvik e-等ti2iTdti
σn.	∞ ti ni-1 T
=Onirl I'.、厂 L ti 2 dti
2亏Γ (号)√c Jo
ni
2 ni+1 γ( ni+1) σ
2 ni Γ (号)√C
(64)
(65)
(66)
(67)
19
Published as a conference paper at ICLR 2021
(68)
Therefore, when the norm of each input tangent vector vi is kept the same by the former part of neural
networks before applying this operation, the standard deviation σni must be expressed as follows:
σni = Cβni , where C = const.	(69)
In addition, using Equation 63, the squared norm of the Poincare β-concatenated tangent vector V is
obtained as follows:
kvk2 = X (子力喃=X βn ¥ C 2 =立 X ti.	W
i=1 βni	i=1 c σni	c i=1
This leads the value t := CkvL, where σn, = Cen which is expressed as follows:
σn
N
t=Xti.	(71)
i=1
Here, t also follows a χ2 distribution, and the expected value of the norm of v is obtained as follows:
E[kvk] = 2n⅛ /	kvk e-112-1dt = V2πσn = HC	(72)
which is the same as the norms of the input tangent vectors. This indicates that each coordinate of v
follows a normal distribution centered at zero with a variance σn, satisfying the Assumption 1.
Based on the results above, the expected value of the norm of each input gyrovector xi is expressed
by the following:
…一 ∕,∞ ..	..	1	ti n-ι
E[kxik] = L Ilxik 2等γ(ni)e 2 ti2	dti
1	∞	1	/ l“ 八 _ti n-1
2等Γ (ni) J0 √C tanh(ck kvik e 2 ti2	dti
1	∞	/	L∖	ni
2等Γ(n) √ J0 tanh Sni田 Cti2
∞∞
Z0	Xj=1
22j (22j - 1) B2j (σn√ti)2jτ
(2j)!
ti ni-1 .
e-百 ti2	dti
1 X 22j (22j - 1) Bjj
2等Γ (号)√c j=1	⑵)！
1	X 22j (22j - 1) Bjj 2j+ni—1 γ (	ni-1 A
2等Γ (号)√ j=1	⑵)!	V 2 J
1 X 22j(22j- 1) B2j f√-Cʌ
√c α一j —(存C)
2j-1 Γ (n)2j-2
Γ (n+1 )2j-1
ni - 1
γ Ij+ —
(73)
(74)
(75)
(76)
(77)
(78)
(79)
1
2彳Γ（号）√
∞
0
Note that, for the calculation between Equations 75 and 76, we utilize the Taylor series expansion
of tanh for a real value. Furthermore, considering the Laurent series expansion at infinity, we can
obtain the following expressions:
n n -, ni-1 ʌ -、—ni j+n
γ ∣j +	) =(2e) 2 ni 2
Γ (n+1)	, 、n 2-n
' 2 2 = (2e)* n； 2
Γ (号)2
1
ni
+O
2jπ + O
(80)
(81)
20
Published as a conference paper at ICLR 2021
Therefore, in the general cases in which ni	1, we can obtain the following approximation:
Γ (j + r )=Γ (j +
Γ (n+1 )2j 1	\	2	)	\
ni
-1 ʌ γ(呼)(γ(号)
2	) Γ (ni)2 Wn2+1)
2j
(82)
(2术-等 22 - √πnj+
2-jnij	Γ
2j
等-等+2-2	£
Γ(ni+1
ni) Yj
(83)
(84)
(n ni-1
√π (2?1」
n i +	n
√∏ (2e)—一厂(ni + 1)号
2j
(85)
2-jnij	(2e)
2-jnij(2e)j
ni-1	∖ 2j
2，—ʌ
Ini + 1)等)
nij(ni-1)
(86)
ni
ni + 1
(ni + 1)jni
(87)
nij
(88)
' ej e-j	(89)
1.
(90)
Note that, for the calculation between Equations 84 and 85, We utilize Stirling,s approximation, i.e.,
Γ(z) ` ʌ∕2π (e )z. In addition, we utilize the definition of Napier,s constant for the approximation
between Equations 88 and 89, i.e., limχ→∞(1 + χ)x = e.
Combining Equations 79 and 90, the expected value of kxi k can be approximately expressed by the
following:
E[kχik]'√c XX	("C j
=√^ tanh (√2πC)
=√1c tanh (√cE[∣Wik]).
(91)
(92)
(93)
In the same way, the expected value of the Poincare β-concatenated gyrovector X is obtained by the
following:
E[kxk] ` √^ tanh (√2πC)
=√c tanh (√CE[kvk]),
(94)
(95)
which concludes the proof.
□
21
Published as a conference paper at ICLR 2021
D.5 THE MOBIUS GYROMIDPOINT AND THE EINSTEIN GYROMIDPOINT
Einstein gyromidpoint. In the Beltrami-Klein model, the midpoint n ∈ Kn among {n∙i ∈ Kn}N=ι
and the non-negative scalar weights {νi ∈ R+ }iN=1 is given as follows:
n
N
νiγini
上1--------, where Yi = /	=
G	P - Cknik2
νiγi
i=1
(96)
This operation is called the Einstein gyromidpoint (Ungar, 2009).
Based on the above, we prove the equivalence of the Mobius gyromidpoint and Einstein gyromidpoint.
Proof. Let the points {bi ∈ Bcn}iN=1 correspond to {ni ∈ Kcn}iN=1, respectively, i.e., bi is a projection
of n∙i to the POinCare ball model using Equation 11. From Equations 12 and 96, we obtain the
following:
γi
1	1 + Ckbik2
Pl-cknik2 = 1 - Ckbik2 .
(97)
Substituting Equations 12 and 97 for Equation 96 leads to the representation of the Einstein midpoint
using the coordinates in the Poincare ball model:
n
XX	2bi
i=1 V I-Ckbik2
XX 1 + Ckbik2
i=1 i I-Ckbik2
N
X	νiλcbibi
i=1
Nv
X Vi (% -1)
i=1
(98)
Therefore, the point b ∈ Bn, which is a projection of n to the Poincare ball model using Equation 11,
is expressed in the following manner:

b
b
1 + P1 二 Ckbk2
2 %c b,
N
X νiλcbi bi
where b = n = Hi=I-----------------
—	N
X Vi (λbi - 1)
i=1
(99)
This concludes the proof.
□
D.6 Möbius gyromidpoint and centroid of squared Lorentzian distance
Weighted centroid in the hyperboloid model (Law et al., 2019). With a Lorentzian norm |kxkL| =
P∣hx, XiL| = P∣kxkL∣ for X ∈ Rn+1, the center of mass h ∈ Hn among {hi = (zi, k>)> ∈
Hcn}iN=1 and the non-negative scalar weights {Vi ∈ R+}iN=1 is given as follows:
- h	3
h = √C≡Γ, where h =3Vihi.
(100)
This is based on the minimization problem of the weighted sum of squared Lorentzian distances
expressed as follows:
h = arg min ^X Vikhi — h∣∣L∙	(101)
h	i=1
In the following, we prove the equivalence of the Mobius gyromidpoint and the weighted centroid in
the hyperboloid model.
22
Published as a conference paper at ICLR 2021
Proof. Expanding Equation 100 with the coordinates, we obtain the following:

h
(102)
The point b ∈ Bn，which is a projection of h to the Poincare ball model using Equation 9, is expressed
in the following manner:

b
N
νiki
1	i=1	_____________
2	N
+	νizi
i=1
(103)
Dividing both the numerator and denominator by	i νizi , this can be rewritten as follows:
N
νiki
b	b	ι 令 AkkI i=1
b =---,	二=一区C b, where b :=	--
1 + Pi-Ckbk2	2	√	√c £
νizi
i1
(104)
Next, considering the points {bi ∈ BCn}iN-1, which also correspond to {hi}iN-1, respectively, we can
transform the expression of b into an expression with only the coordinates in the Poincare ball model:
b
XX bi
2 ⅛1Vi 1-»
XX 1 + Ckbik2
⅛ i I-Ckbik2
N
X	νiλCbibi
i-1
NV
X νi (λbi - 1)
i1
(105)
This concludes the proof.
□
D.7 MOBIUS GYROMIDPOINT AS A SOLUTION OF THE MINIMIZATION PROBLEM
The discovery of the equivalence between the weighted centroid in the hyperboloid model and the
Mobius gyromidpoint enables us to discuss what the Mobius gyromidpoint is a minimizer of. In the
following, we prove that the Mobius gyromidpoint can be regarded as a minimizer of the weighted
sum of calibrated squared gyrometrics.
Theorem 2. The Mobius gyromidpoint is a solution ofthe minimization problem ofthe weighted sum
of calibrated squared gyrometrics, which is expressed as follows:
N
b = arg min X Vi λ^bθɛbj∣ θ0 b ㊉C bik2.	(106)
b i=1 C C ”
Each k θc b Φc bik indicates the norm of the respective gyrovector b viewed from the Mobius
gyromidpoint bb, which equals the gyrodistance of bb to bi and is also called a gyrometric (Ungar,
2009). In addition, each λθ bφ b is a conformal factor of the metric tensor of the POinCare ball
model for such a gyrovector. Therefore, the minimization objective in Equation 106 can be interpreted
as the weighted sum of squared gyrometrics, each of which is calibrated by a scaling factor at the
respective point.
23
Published as a conference paper at ICLR 2021
Proof. Let the point b ∈ Bcn be a projection of the weighted centroid h ∈ Hcn. With Equation 101
and the notation of Equation 10, we obtain the following straightforward expression:
b = argmin£ Vikhi- h(b)kL
3	i=1
Expanding Equation 107 with the coordinates, we obtain the following:
1
b = arg min —2 T Vi I ——ziz(b) + hhi, h(b)i I
b3	i=1 c
1
=arg min T Vi---------+ ziz(b) — hhi, h(b)i .
b3	i=1	c
(107)
(108)
(109)
Considering the points {bi ∈ Bcn}iN=1, which correspond to {hi}iN=1, respectively, we can transform
Equation 109 into an expression with only the coordinates in the Poincare ball model:
N b = arg min b3	i=1	ν(	1	+ 11 +	Ckbik2	1	+ i	y	c c 1 -	c∣∣bik21	—	Ckbk2	, 	H	 - h 	 Ckbk2	1	2bi	2b Λ -CkbiF, 1 - ckbk2 iJ	(110)
N = arg min b3	i=1	2νikbi — bk2 _ ~ , (1 - ckbik2)(1 - ckbk2)			(111)
N = arg min b3	i=1	,ʌ	I i	:	ɪ ιιr) 2νi k ㊀ c b ㊉ c bik .. ~ 	 1 - Cll ㊀C b ㊉C bik2			(112)
N
=argmin X Vi λ葭cb©Cbik Sc b ㊉C bik2.	(113)
b3	i=1
This concludes the proof.	□
D.8 Weight generalization OF the MOBIUS gyromidpoint
As mentioned in Section 3.5, we extend the condition of the weights of the Mobius gyromidpoint to
all real values {Vi ∈ R}iN=1 by regarding a negative weight as an additive inverse operation, that is,
regarding any pair (Vi,bi) as (|Vi|,sign(Vi)bi):
NN
|Vi | λscign(νi)bi sign(Vi)bi	Viλcbi bi
i=1______ _ i=1
N	= Nv
X|Vi|
λsign(νi)bi - 1)X ∣Vi∣ (λbi-1)
i=1	i=1
(114)
E	Implementation Details
E.1 Parameter initialization
Unidirectional Poincare MLR. When the dimensions of the input gyrovector is n, each element
of the weight parameter Z is initialized by a normal distribution centered at zero with a standard
deviation n-2. The bias parameter r is initialized as a zero vector.
Poincare FC layer. When the dimensions of the input gyrovector and the output gyrovector are n
and m, respectively, each element of the weight parameter Z is initialized by a normal distribution
centered at zero with a standard deviation (2nm)- 1. The bias parameter r is initialized as a zero
vector.
24
Published as a conference paper at ICLR 2021
Poincare convolutional layer. When the dimensions of the input gyrovector and the output gy-
rovector are n and m, respectively, and the total kernel size is K, each element of the weight parameter
Z is initialized by a normal distribution centered at zero with a standard deviation (2nKm)-2. The
bias parameter r is initialized as a zero vector.
Embedding on the Poincare ball model. As mentioned by Ganea et al. (2018a), we confirmed
the tendency of the parameters in the POinCare ball model to adjust their angles at the first phase of
the training before increasing their norms. In addition, we consider that, due to the exponentially
growing distance metric of the hyperbolic space, the farther a gyrovector parameter is placed from
the origin, the more costly it moves such a point to another point through the optimization. Therefore,
the embedding parameters on the Poincare ball model should be initialized with a particular small
gain E, given as a hyperparameter, aiming to accelerate such an adjustment and make the later
optimization smooth. We set the value E to be 10-2 in the experiment in Section 4.3.
E.2 Hyperparameters of the experiment in Section 4.2
Optimization. We used the Riemannian Adam optimizer with β1 = 0.9, β2 = 0.999 and = 10-8
for both of the Euclidean and our hyperbolic architectures. The learning rate η was set to 10-3 .
E.3 Hyperparameters of the experiment in Section 4.3
Model architectures. Let D be the dimension of the source and target token embeddings. Each
model for the experiment in Section 4.3 has the encoder and decoder, both of which are composed
of five convolutional layers with a kernel size of three and a channel size of D, five convolutional
layers with a kernel size of three and a channel size of 2D, and two convolutional layers with a
kernel size of one and a channel size of 4D. The output feature maps of the last convolutional layer
in the encoder are projected into D-dimensional feature maps. They are utilized as the key for the
encoder-decoder attentions. Likewise, the output feature maps of the last convolutional layer in the
decoder are projected into D-dimensional feature maps for the final token classification.
Training. In each iteration of the training phase, we fed each model a mini-batch containing
approximately 10,000 tokens at most. In this setting, the batch size, or the number of the sentence
pairs in a mini-batch, dynamically changes.
As a loss function, we utilized the cross entropy function with a label smoothing of 0.1.
Optimization. We used the Riemannian Adam optimizer with β1 = 0.9, β2 = 0.98 and = 10-9
for both of the Euclidean and our hyperbolic architectures. For the scheduling of the learning rate
η, we linearly increased the learning rate for the first 4000 iterations as a warm-up, and utilized the
inverse square root decay with respect to the number of iterations t thereafter as η = (Dt)-1.
25