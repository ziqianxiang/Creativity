Published as a conference paper at ICLR 2021
A Temporal Kernel Approach for Deep Learn-
ing with Continuous-time Information
Da Xu
Walmart Labs
Sunnyvale, CA 94086, USA
DaXu5180@gmail.com
ChuanWei Ruan *
Instacart
San Francisco, CA 94107, USA
Ruanchuanwei@gmail.com
Evren Korpeoglu, Sushant Kuamr, Kannan Achan
Walmart Labs
Sunnyvale, CA 94086, USA
[EKorpeoglu, SKumar4, KAchan]@walmartlabs.com
Ab stract
Sequential deep learning models such as RNN, causal CNN and attention mech-
anism do not readily consume continuous-time information. Discretizing the
temporal data, as we show, causes inconsistency even for simple continuous-time
processes. Current approaches often handle time in a heuristic manner to be con-
sistent with the existing deep learning architectures and implementations. In this
paper, we provide a principled way to characterize continuous-time systems using
deep learning tools. Notably, the proposed approach applies to all the major deep
learning architectures and requires little modifications to the implementation. The
critical insight is to represent the continuous-time system by composing neural
networks with a temporal kernel, where we gain our intuition from the recent
advancements in understanding deep learning with Gaussian process and neural
tangent kernel. To represent the temporal kernel, we introduce the random feature
approach and convert the kernel learning problem to spectral density estimation
under reparameterization. We further prove the convergence and consistency re-
sults even when the temporal kernel is non-stationary, and the spectral density is
misspecified. The simulations and real-data experiments demonstrate the empirical
effectiveness of our temporal kernel approach in a broad range of settings.
1	Introduction
Deep learning models have achieved remarkable performances in sequence learning tasks leveraging
the powerful building blocks from recurrent neural networks (RNN) (Mikolov et al., 2010), long-
short term memory (LSTM) (Hochreiter & Schmidhuber, 1997), causal convolution neural network
(CausalCNN/WaveNet) (Oord et al., 2016) and attention mechanism (Bahdanau et al., 2014; Vaswani
et al., 2017). Their applicability to the continuous-time data, on the other hand, is less explored due
to the complication of incorporating time when the sequence is irregularly sampled (spaced). The
widely-adopted workaround is to study the discretized counterpart instead, e.g. the temporal data is
aggregated into bins and then treated as equally-spaced, with the hope to approximate the temporal
signal using the sequence information. It is perhaps without surprise, as we show in Claim 1, that
even for regular temporal sequence the discretization modifies the spectral structure. The gap can
only be amplified for irregular data, so discretizing the temporal information will almost always
*The work WaS done when the author WaS With Walmart Labs.
1
Published as a conference paper at ICLR 2021
introduce intractable noise and perturbations, which emphasizes the importance to characterize the
continuous-time information directly. Previous efforts to incorporate temporal information for deep
learning include concatenating the time or timespan to feature vector (Choi et al., 2016; Lipton
et al., 2016; Li et al., 2017b), learning the generative model of time series as missing data problems
(Soleimani et al., 2017; Futoma et al., 2017), characterizing the representation of time (Xu et al., 2019;
2020; Du et al., 2016) and using neural point process (Mei & Eisner, 2017; Li et al., 2018). While they
provide different tools to expand neural networks to coupe with time, the underlying continuous-time
system and process are involved explicitly or implicitly. As a consequence, it remains unknown
in what way and to what extend are the continuous-time signals interacting with the original deep
learning model. Explicitly characterizing the continuous-time system (via differential equations),
on the other hand, is the major pursuit of classical signal processing methods such as smoothing
and filtering (Doucet & Johansen, 2009; Sarkka, 2013). The lack of connections is partly due to
the compatibility issues between the signal processing methods and the auto-differential gradient
computation framework of modern deep learning. Generally speaking, for continuous-time systems,
model learning and parameter estimation often rely on the more complicated differential equation
solvers (Raissi & Karniadakis, 2018; Raissi et al., 2018a). Although the intersection of neural network
and differential equations is gaining popularity in recent years, the combined neural differential
methods often require involved modifications to both the modelling part and implementation detail
(Chen et al., 2018; Baydin et al., 2017).
Inspired by the recent advancement in understanding neural network with Gaussian process and the
neural tangent kernel (Yang, 2019; Jacot et al., 2018), we discover a natural connection between the
continuous-time system and the neural Gaussian process after composing with a temporal kernel.
The significance of the temporal kernel is that it fills in the gap between signal processing and deep
learning: we can explicitly characterize the continuous-time systems while maintaining the usual
deep learning architectures and optimization procedures. While the kernel composition is also known
for integrating signals from various domains (Shawe-Taylor et al., 2004), we face the additional
complication of characterizing and learning the unknown temporal kernel in a data-adaptive fashion.
Unlike the existing kernel learning methods where at least the parametric form of the kernel is given
(Wilson et al., 2016), we have little context on the temporal kernel, and aggressively assuming the
parametric form will risk altering the temporal structures implicitly just like discretization. Instead,
we leverage the Bochner’s theorem and its extension (Bochner, 1948; Yaglom, 1987) to first covert
the kernel learning problem to the more reasonable spectral domain where we can direct characterize
the spectral properties with random (Fourier) features. Representing the temporal kernel by random
features is favorable as we show they preserve the existing Gaussian process and NTK properties
of neural networks. This is desired from the deep learning’s perspective since our approach will
not violate the current understandings of deep learning. Then we apply the reparametrization trick
(Kingma & Welling, 2013), which is a standard tool for generative models and Bayesian deep learning,
to jointly optimize the spectral density estimator. Furthermore, we provide theoretical guarantees for
the random-feature-based kernel learning approach when the temporal kernel is non-stationary, and
the spectral density estimator is misspecified. These two scenarios are essential for practical usage but
have not been studied in the previous literature. Finally, we conduct simulations and experiments on
real-world continuous-time sequence data to show the effectiveness of the temporal kernel approach,
which significantly improves the performance of both standard neural architectures and complicated
domain-specific models. We summarize our contributions as follow.
•	We study a novel connection between the continuous-time system and neural network via
the composition with a temporal kernel.
•	We propose an efficient kernel learning method based on random feature representation,
spectral density estimation and reparameterization, and provide strong theoretical guarantees
when the kernel is nonstationary and the spectral density is misspeficied.
•	We analyze the empirical performance of our temporal kernel approach for both the standard
and domain-specific deep learning models through real-data simulation and experiments.
2	Notations and Background
We use bold-font letters to denote vectors and matrices. We use xt and (x, t) interchangeably to
denote a time-sensitive event occurred at time t, with t ∈ T ≡ [0, tmax]. Neural networks are denoted
2
Published as a conference paper at ICLR 2021
by f(θ, x), where x ∈ X ⊂ Rd is the input where diameter(X) ≤ l, and the network parameters θ
are sampled i.i.d from the standard normal distribution at initialization. Without loss of generality, we
study the standard L-layer feedforward neural network with its output at the hth hidden layer given
by f(h) ∈ Rdh. We use and (t) to denote Gaussian noise and continuous-time Gaussian noise
process. By convention, We use 0 and ◦ and to represent the tensor and outer product.
2.1	Understanding the standard neural network
We folloW the settings from Jacot et al. (2018); Yang (2019) to briefly illustrate the limiting Gaus-
sian behavior of f (θ, x) at initialization, and its training trajectory under Weak optimization. As
d1, . . . , dL → ∞, f(h) tend in laW to i.i.d Gaussian processes With covariance Σh ∈ Rdh ×dh:
f (h) 〜N(0, Σh), which we refer to as the neural network kernel to distinguish from the other kernel
notions. Also, given a training dataset {xi, yi}n=ι, let f (θ(s)) = f(θ(s), xι),…，f(θ⑸,Xn))
be the network outputs at the sth training step and y = (y1, . . . , yn ). Using the squared loss
for example, when training with infinitesimal learning rate, the outputs follows: df θ(s) /ds =
-Θ(s) × (f θ(s) - y), where Θ(s) is the neural tangent kernel (NTK). The detailed formulation
of Σh and Θ(s) are provided in Appendix A.2. We introduce the two concepts here because:
1.	instead of incorporating time to f(θ, X), which is then subject to its specific structures, can we
alternatively consider an universal approach which expands the Σh to the temporal domain such as
by composing it with a time-aware kernel?
2.	When jointly optimize the unknown temporal kernel and the model parameters, how can we
preserve the results on the training trajectory with NTK?
In our paper, we show that both goals are achieved by representing a temporal kernel via random
features.
2.2	Difference between continuous-time and its discretization
We now discuss the gap between continuous-time process and its equally-spaced discretization. We
study the simple univariate continuous-time system f (t):
[21 + aoðʃ+ + aιf (t) = boe(t).	(I)
dt2	dt
A discretization with a fixed interval is then given by: f[i] = f(i × interval) for i = 1, 2,   Notice
that f(t) is a second-order auto-regressive process, so both f(t) and f[i] are stationary. Recall that
the covariance function for a stationary process is given by k(t) := cov f(t0), f(t0 + t) , and the
spectral density function (SDF) is defined as s(ω) = -∞∞ exp(-iωt)k(t)dt.
Claim 1. The spectral density function for f(t) and f[i] are different.
The proof is relegated to Appendix A.2.2. The key takeaway from the example is that the spectral
density function, which characterizes the signal on the frequency domain, is altered implicitly even by
regular discretization in this simple case. Hence, we should be cautious about the potential impact of
the modelling assumption, which eventually motivates us to explicitly model the spectral distribution.
3	Methodology
We first explain our intuition using the above example. If we take the Fourier transform on (1)
b0
and rearrange terms, it becomes: f (iω)= ((.尸 十——(.)+—)e(iω), where f (iω) and e(iω)
are the Fourier transform of f(t) and (t). Note that the spectral density of a Gaussian noise
process is constant, i.e.归(iω)∣2 = po, so the spectral density of f (t) is given by: se『(ω)=
Po∣bo∕((iω)2 + ao(iω) + aι)∣2, where we use θτ = [ao, aι, bo] to denote the parameters of the
linear dynamic system defined in (1). The subscript T is added to distinguish from the parameters
of the neural network. The classical Wiener-Khinchin theorem (Wiener et al., 1930) states that the
3
Published as a conference paper at ICLR 2021
Figure 1: (a). The relations between neural network kernel Σ(h), temporal kernel KT and the
neural-temporal kernel Σ(Th). (b). Composing a single-layer RNN with temporal kernel where the
hidden out from GRU cells become f (h) (x, t) ≡ f (h) (x) ◦ φ(x, t). We use the RF-INN blocks to
denote the random feature representation parameterized by INN. The optional outputs y(t) can be
obtained in a way similar to f(h) (x, t).
covariance function for f (t), which is a Gaussian process since the linear differential equation is a
linear operation on (t), is given by the inverse Fourier transform of the spectral density:
Kτ(t,t0) ：= kθτ(t0 — t) = 2∏ /sθT (ω) exp(iωt)dω,	(2)
We defer the discussions on the inverse direction, that given a kernel kθT (t0 — t) we can also construct
a continuous-time system, to Appendix A.3.1. Consequently, there is a correspondence between
the parameterization of a stochastic ODE and the kernel of a Gaussian process. The mapping is not
necessarily one-to-one; however, it may lead to a more convenient way to parameterize a continuous-
time process alternatively using deep learning models, especially knowing the connections between
neural network and Gaussian process (which we highlighted in Section 2.1).
To connect the neural network kernel Σ(h) (e.g. for the hth layer of the FFN) to a continuous-time
system, the critical step is to understand the interplay between the neural network kernel and the
temporal kernel (e.g. the kernel in (2)):
•	the neural network kernel characterizes the covariance structures among the hidden repre-
sentation of data (transformed by the neural network) at any fixed time point;
•	the temporal kernel, which corresponds to some continuous-time system, tells how each static
neural network kernel propagates forward in time. See Figure 1a for a visual illustration.
Continuing with Example 1, it is straightforward construct the integrated continuous-time system as:
a2(x)d f(XJt +aι(x)df(x,t)+ao(x)f (x,t) = b0(x)e(x,t), e(x,t = to)〜N(0, Σ(h), ∀to ∈ T,
dt2	dt
(3)
where we use the neural network kernel Σ(h) to define the Gaussian process (x, t) on the feature
dimension, so the ODE parameters are now functions of the data as well. To see that (3) generalizes the
hth layer of a FFN to the temporal domain, we first consider a2(x) = a1(x) = 0 and a0(x) = b0(x).
Then the continuous-time process f(x, t) exactly follows f(h) at any fixed time point t, and its
trajectory on the time axis is simply a Gaussian process. When a1 (x), a2 (x) 6= 0, f(x, t) still
matches f(h) at the initial point, but its propagation on the time axis becomes nontrivial and is now
characterized by the constructed continuous-time system. We can easily the setting to incorporate
higher-order terms:
dnf(x, t)	dm(x, t)
an(x)	+ …+ a0(x)f (x, t) = bm(x)	+ …+ b0(x)E(x, t).	⑷
dtn	dtm
Keeping the heuristics in mind, an immediate question is what is the structure of the corresponding
kernel function after we combine the continuous-time system with the neural network kernel?
4
Published as a conference paper at ICLR 2021
Claim 2. The kernel function for f (x,t) in (4) is given by: ∑Th)(x,t; x0,t0) = ke『(x,t; x0,t0) ∙
Σ(h)(x, x0), where θτ is the underlying parameterization of {ai(∙)卜=]and {bi(∙)卜=]as functions
of X. When {α,i }n_^ and {b>i }∖ are scalars, ∑Th (x, t; x0, t0) = k,θτ (t, t0) ∙ Σ(h) (x, x0).
We defer the proof and the discussion on the inverse direction from temporal kernel to continuous-
time system to Appendix A.3.1. Claim 2 shows that it is possible to expand any layer of a standard
neural network to the temporal domain, as part of a continuous-time system using kernel composition.
The composition is flexible and can happen at any hidden layer. In particular, given the temporal
kernel KT and neural network kernel Σ(h), we obtain the neural-temporal kernel on X × T:
∑Th) = diag(∑(h) 0 KT), where diag(∙) is the partial diagonalization operation on X:
∑Th)(x, t; x0, t0) = Σ(h)(x, x0) ∙ KT(x, t; X, t0).	(5)
The above argument shows that instead of taking care of both the deep learning and continuous-time
system, which remains challenging for general architectures, we can convert the problem to finding a
suitable temporal kernel. We further point out that when using neural networks, we are parameterizing
the hidden representation (feature lift) in the feature space rather than the kernel function in the kernel
space. Therefore, to give a consistent characterization, we should also study the feature representation
of the temporal kernel and then combine it with the hidden representations of the neural network.
3.1	The random feature representation for temporal kernel
We start by considering the simpler case where the temporal kernel is stationary and independent of
features: KT(t, t0) = k(t0 一 t), for some properly scaled positive even function k(∙). The classical
Bochner’s theorem (Bochner, 1948) states that:
/
R
ψ(t0 一 t)
e-i(t0-t)ωds(ω),
for some probability density function s on R,
(6)
where s(∙) is the spectral density function we highlighted in Section 2.2. To compute the inte-
gral, we may sample (ω1, . . . , ωm ) from s(ω) and use the Monte Carlo method: ψ(t0 一 t) ≈
P Pm=I e-i(t0-t)ω, Since e-i(t0-t)lω = cos ((t0 — t)ω) — i Sin ((t0 — t)ω), for the real part, we let:
φ(t) = √1m [ cos(tωι), sin(tωι) ..., cos(tωm), sin(tωm)],	(7)
and it is easy to check that ψ(t0 —t) ≈ hφ(t), φ(t0)i. Since φ(t) is constructed from random samples,
we refer to it as the random feature representation of KT . Random feature has been extensive studied
in the kernel machine literature, however, we propose a novel application for random features to
parametrize an unknown kernel function. A straightforward idea is to parametrize the spectral density
function s(ω), whose pivotal role has been highlighted in Section 2.2 and Example 1.
Suppose θT is the distribution parameters for s(ω). Then φθτ (t) is also (implicitly) parameterized
by θT through the samples ω(θT)i im_1 from s(ω). The idea resembles the reparameterization
trick for training variational objectives (Kingma & Welling, 2013), which we formalize in the next
section. For now, it remains unknown if we can also obtain the random feature representation for non-
stationary kernels where Bochner’s theorem is not applicable. Note that for a general temporal kernel
KT (X, t; X0, t0), in practice, it is not reasonable to assume stationarity specially for the feature domain.
In Proposition 1, we provide a rigorous result that generalizes the random feature representation for
nonstationary kernels with convergence guarantee.
Proposition 1. For any (scaled) continuous non-stationary PDS kernel KT on X × T, there exists a
joint probability measure with spectral density function s(ω1, ω2), such that KT (X, t), (X0, t0) =
Es(ω1,ω2) [φ(x,t)τφ(x0,t0)] where φ(x,t) is given by:
c√— [..., cos([χ,t]lωι,i) + cos ([x,t]lω2,i), sin ([x,t]lω1,i) + sin([x,t]lω2,i)...],
Here,	(ω1,i, ω2,i) im_1 are the m samples from s(ω1, ω2).	When the sample size m
8(d+I) log(C(d) (l2tmaχσp∕ε) d+3 /δ), with probability at least 1 — δ, for any ε > 0,
sup	IKT((x,t), (x0,t0)) — Φ(x,t)lΦ(x0,t0)∣ ≤ ε,
(x,t),(x0,t0)
(8)
≥
(9)
5
Published as a conference paper at ICLR 2021
where σp2 is the second moment of the spectral density function s(ω1, ω2) and C(d) is a constant.
We defer the proof to Appendix A.3.2. It is obvious that the new random feature representation in (8)
is a generalization of the stationary setting. There are two advantages for using the random feature
representation:
•	the composition in the kernel space suggested by (5) is equivalent to the computationally
efficient operation f(h) (x) ◦ φ(x, t) in the feature space (Shawe-Taylor et al., 2004);
•	we preserve a similar Gaussian process behavior and the neural tangent kernel results that
we discussed in Section 2.1, and we defer the discussion and proof to Appendix A.3.3.
In the forward-passing computations, we simply replace the original hidden representation f(h) (x) by
the time-aware representation f(h) (x) ◦ φ(x, t). Also, the existing methods and results on analyzing
neural networks though Gaussian process and NTK, though not emphasized in this paper, can be
directly carried out to the temporal setting as well (see Appendix A.3.3).
3.2 Reparameterization with the spectral density function
We now present the gradient computation for the parameters of the spectral distribution using only
their samples. We start from the well-studied case where s(ω) is given by a normal distribution
N(μ, Σ) with parameters θτ = [μ, Σ]. When computing the gradients of θτ, instead of sampling
from the intractable distribution s(ω), We reparameterize each sample ωi via: £1/2e + μ, where E is
sampled from a standard multivariate normal distribution. The gradient computations that relied on
ω is now replaced using the easy-to-sample E and θτ = [μ, Σ] now become tractable parameters in
the model given E. We illustrate reparameterization in our setting in the following example.
Example 1. Consider a single-dimension homogeneous linear model: f (θ, x) = f(0) (x) = θx.
Without loss of generality, we use only a single sample ω1 from the s(ω) which corresponds to the
feature-independent temporal kernel kθ(t, t0). Again, we assume s(ω)〜N(μ, σ).
Then the time-aware hidden representation for this layer for datapoint (x1, t1) is given by:
fθ0μ σ(xι, tι) = 1∕√2[θxι cos(t1ω1), θxι sin(t1ω1)], ωι 〜N(μ,σ).
Using the reparameterization, given a sample E1 from the standard normal distribution, we have:
fθ0),σ(x1,t1) = 1∕√2[θxι cos (t1(σ”2E1 + μ)),θxι Sin (tι(σ12ι + μ))],	(10)
so the gradients with respect to all the parameters (θ, μ, σ) can be computed in the usual way.
Despite the computation advantage, the spectral density is now learnt from the data instead of being
given so the convergence result in Proposition 1 does not provide sample-consistency guarantee. In
practice, we may also misspecify the spectral distribution and bring extra intractable factors.
To provide practical guarantees, we first introduce several notations: let KT(S) be the temporal
kernel represented by random features such that KT (S) = E [φlφ], where the expectation is taken
with respect to the data distribution and the random feature vector φ has its samples {ωi }im=1 drawn
from the spectral distribution S. Without abuse of notation, we use φ 〜S to denote the dependency
of the random feature vector φ on the spectral distribution S provided in (8). Given a neural network
kernel Σ(h), the neural temporal kernel is then denoted by: ΣTh)(S) = Σ(h) 0 Kt(S). So the
sample version of ΣTh)(S) for the dataset {M, t)忆]is given by:
∑ Th)(S) =	( ) [) X Σ(h)(xi, Xj )φ(xi,ti)lφ(xj ,tj), φ 〜S.	(11)
n(n - 1) i6=j
If the spectral distribution S is fixed and given, then using standard techniques and Theorem 1 it
is straightforward to show limn→∞ ΣTh)(S) → E[ΣTh)(S)] so the proposed learning schema is
sample-consistent.
In our case, the spectral distribution is learnt from the data, so we need some restrictions on the
spectral distribution in order to obtain any consistency guarantee. The intuition is that if SθT does
not diverge from the true S, e.g. d(SθT kS) ≤ δ for some divergence measure, the guarantee on S
can transfer to SθT with the rate only suffers a discount that does not depend on n.
6
Published as a conference paper at ICLR 2021
Theorem 1. Consider the f-divergence such that d(SθT kS) = c dSθT /dS dS, with the generator
function c(x) = xk - 1 for any k > 0. Given the neural network kernel Σh, let M = Σh ∞, then
2
Pr( d(s□S)<δ" TI)(Sθτ) → EE MSθτ )1 ≥ ε) ≤ √2eχp (64max{-,M}(δ + 1)) + C⑹,
(12)
2〃 产	2d + 2
where C(ε) (X (e/ mmX{4SM}) d+3 exP ( - 32max{1⅛M2}(d+3) ) that does not depend on δ.
The proof is provided in Appendix A.3.4. The key takeaway from (12) is that as long as the
divergence between the learnt SθT and the true spectral distribution is bounded, we still achieve
sample consistency. Therefore, instead of specifying a distribution family which is more likely to
suffer from misspecification, we are motivated to employ some universal distribution approximator
such as the invertible neural network (INN) (Ardizzone et al., 2018). INN consists of a series of
invertible operations that transform samples from a known auxiliary distribution (such as normal
distribution) to arbitrarily complex distributions. The Jacobian that characterize the changes of
distributions are made invertible by the INN, so the gradient flow is computationally tractable similar
to the case in Example 1. We defer the detailed discussions to Appendix A.3.5.
Remark 1. It is clear at this point that the temporal kernel approach applies to all the neural
networks who have a Gaussian process behavior with a valid neural network kernel, which includes
the major architectures such as CNN, RNN and attention mechanism (Yang, 2019).
For implementation, at each forward and backward computation, we first sample from the auxiliary
distribution to construct the random feature representation φ using reparameterization, and then
compose it with the selected hidden layer f(h) such as in (10). We illustrate the computation
architecture in Figure 2, where we adapt the vanilla RNN to the proposed framework. In Algorithm
1, we provide the detailed forward and backward computations, using the L-layer FFN from the
previous sections as an example.
(Hidden) output at step i	(Hidden) output at step i+1
f s) (仇(孙 4)) o Φg.ti)
∣[... Sin(∙) …CoS(•)…]as M eg.⑻I [
{%，ti]τωlj, [xi, ti]τω2j}1^1
∕w(β, (xi+1,⅛7ι)) o 0(%i+ι, Ji) ⊂=>
∣[...siτι(∙)…CoS(∙) ...] asMeq.∕8力 [
{因+"i+ι]%ιj, %+ι, %ι]%2j},ι
:	EL1，%1，...，£l,m, %τn~N(0, D
Figure 2: The computation architecture of the proposed method using RNN as an example (following
Figure 1b). Here, We use f (h) (θ, ∙) to denote the standard GRU cell, and use g(ψ, ∙) to denote the
invertible neural network. At the ith step, the GRU cell takes the feature vector xi , and the hidden
state of the previous steps ~hi , as input. The RF-INN module is called once for each batch.
4 Related work
The earliest Work that discuss training continuous-time neural netWork dates back to LeCun et al.
(1988); Pearlmutter (1995), but no feasible solution Was proposed at that time. The proposed approach
relates to several fields that are under active research.
ODE and neural network. Certain neural architecture such as the residual netWork has been
interpreted as approximate ODE solvers (Lu et al., 2018). More direct approaches have been
proposed to learn differential equations from data (Raissi & Karniadakis, 2018; Raissi et al., 2018a;
7
Published as a conference paper at ICLR 2021
Long et al., 2018), and significant efforts have been spent on developing solvers that combine ODE
and the back-propagation framework (Farrell et al., 2013; Carpenter et al., 2015; Chen et al., 2018).
The closest literature to our work is from Raissi et al. (2018b) who design numerical Gaussian process
resulting from temporal discretization of time-dependent partial differential equations.
Random feature and kernel machine learning. In supervised learning, the kernel trick provides
a powerful tool to characterize non-linear data representations (Shawe-Taylor et al., 2004), but the
computation complexity is overwhelming for large dataset. The random (Fourier) feature approach
proposed by Rahimi & Recht (2008) provides substantial computation benefits. The existing literature
on analyzing the random feature approach all assume the kernel function is fixed and stationary (Yang
et al., 2012; Sutherland & Schneider, 2015; SriPerUmbUdUr & Szab6, 2015; Avron et al., 2θl7).
Reparameterization and INN. Computing the gradient for intractable objectives using samples
from aUxiliary distribUtion dates back to the Policy gradient method in reinforcement learning (SUtton
et al., 2000). In recent years, the aPProach gains PoPUlarity for training generative models (Kingma &
Welling, 2013), other variational objectives (Blei et al., 2017) and Bayesian neUral networks (Snoek
et al., 2015). INN are often emPloyed to Parameterize the normalizing flow that transforms a simPle
distribUtion into a comPlex one by aPPlying a seqUence of invertible transformation fUnctions (Dinh
et al., 2014; Ardizzone et al., 2018; Kingma & Dhariwal, 2018; Dinh et al., 2016).
OUr aPProach characterizes the continUoUs-time ODE via the lens of kernel. It comPlements the
existing neUral ODE methods which are often restricted to sPecific architectUres, relying on ODE
solvers and lacking theoretical Understandings. We also ProPose a novel deeP kernel learning aPProach
by Parameterizing the sPectral distribUtion Under random featUre rePresentation, which is concePtUally
different from Using temPoral kernel for time-series classification (Li & Marlin, 2015). OUr work is a
extension of XU et al. (2019; 2020), which stUdy the case for self-attention mechanism.
Algorithm 1: Forward Pass and Parameter UPdate, Using the L-layer FFN as an examPle.
Input: The FFN f(θ, ∙) = {f ⑴(θ, .),..., f (L)(θ, ∙)}; the invertible neural network g(ψ, ∙);
the selected hidden layer h; the loss `i associated with each inPUt (xi, ti); the aUxilliary
distribUtion P .
for each mini-batch do
SamPle	1,j, 2,j jm=1 from the aUxilliary distribUtion P;
Compute the reparameterized samples ω using the INN g(ψ, ∙), e.g. ω1,j (ψ) := g(ψ, e1,j);
for sample i in the batch do
Construct the random feature representation φψ(xi, ti) using the reparameterized
samples (so φ is now explicitly parameterized by ψ) according to eq. (8);
Forward pass: get f (h)(θ, x/let f (h)((θ, ψ), xi,ti) := f (h)(θ, Xi) ◦ φψ (xi,ti),
then pass it to the following feedforward layers to obtain the final output yi ;
Gradient computation: compute the gradients Vθ'i(y^i)∖e, Vψ'i(yji)∖e for the FFN and
INN respectively, conditioned on the samples from the auxiliary distribution;
end
Update the parameters using the selected optimizer in a standard batch-wise fashion.
end
It is straightforward from Figure 2 and Algorithm 1 that the proposed approach serves as a plug-in
module and do not modify the original network structures of the RNN and FFN.
5	Experiments and Results
We focus on revealing the two major advantages of the proposed temporal kernel approach:
•	the temporal kernel approach consistently improves the performance of deep learning models,
both for the general architectures such as RNN, CausalCNN and attention mechanism as
well as the domain-specific architectures, in the presence of continuous-time information;
•	the improvement is not at the cost of computation efficiency and stability, and we outperform
the alternative approaches who also applies to general deep learning models.
8
Published as a conference paper at ICLR 2021
We point out that the neural point process and the ODE neural networks have only been shown to
work for certain model architectures so we are unable to compare with them for all the settings.
Time series prediction with standard neural networks (real-data and simulation)
We conduct time series prediction task using the vanilla RNN, CausalCNN and self-attention mecha-
nism with our temporal kernel approach (Figure A.1). We choose the classical Jena weather data for
temperature prediction, and the Wikipedia traffic data to predict the number of visits of Wikipedia
pages. Both datasets have vectorized features and are regular-sampled. To illustrate the advantage
of leveraging the temporal information compared with using only sequential information, we first
conduct the ordinary next-step prediction on the regular observations, which we refer to as Case1. To
fully illustrate our capability of handling the irregular continuous-time information, we consider the
two simulation setting that generate irregular continuous-time sequences for prediction:
Case2.	we sample irregularly from the history, i.e. xt1 , . . . , xtq, q ≤ k, to predict xtk+1 ;
Case3.	we use the full history to predict a dynamic future point, i.e. xtk+q for a random q.
We provide the complete data description, preprocessing, and implementation in Appendix B. We use
the following two widely-adopted time-aware modifications for neural networks (denote by NN) as
baselines, as well as the classical vectorized autoregression model (VAR).
NN+time: we directly concatenate the timespan, e.g. tj - ti, to the feature vector. NN+trigo: we
concatenate the learnable sine and cosine features, e.g. [sin(π1t), . . . , sin(πk t)], to the feature vector,
where {πi }ik=1 are free model parameters. We denote our temporal kernel approach by T-NN.
From Figure 3, we see that the temporal kernel outperforms the baselines in all cases when the
time series is irregularly sampled (Case2 and Case3), suggesting the effectiveness of the temporal
kernel approach in capturing and utilizing the continuous-time signals. Even for the regular Case1
reported in Table A.1, the temporal kernel approach gives the best results, which again emphasizes
the advantage of directly characterize the temporal information over discretization. We also show
in the ablation studies (Appendix B.5) that INN is necessary for achieving superior performance
compared with specifying a distribution family. To demonstrate the stability and robustness, we
provide sensitivity analysis in Appendix B.6 for model selection and INN structures.
Figure 3: The mean absolute error on testing data for the standard neural networks: RNN, CausalCNN
(denoted by CNN) and self-attention (denoted by Att), for the temporal kernel approach and the
baselines methods in Case2 and Case3. The numerical results are averaged over five repetitions.
Temporal sequence learning with complex domain models
Now we study the performance of our temporal kernel approach for the sequential recommendation
task with more complicated domain-specific two-tower architectures (Appendix B.2). Temporal
information is known to be critical for understanding customer intentions, so we choose the two
public e-commerce dataset from Alibaba and Walmart.com, and examine the next-purchase recom-
mendation. To illustrate our flexibility, we select the GRU-based, CNN-based and attention-based
recommendation models from the recommender system domain (Hidasi et al., 2015; Li et al., 2017a)
and equip them with the temporal kernel. The detailed settings, ablation studies and sensitivity analy-
sis are all in Appendix B. The results are shown in Table A.2. We observe that the temporal kernel
approach brings various degrees of improvements to the recommendation models by characterizing
9
Published as a conference paper at ICLR 2021
the continuous-time information. The positives results from the recommendation task also suggests
the potential of our approach for making impact in boarder domains.
6 Discussion
In this paper, we discuss the insufficiency of existing work on characterizing continuous-time data
with deep learning models and describe a principled temporal kernel approach that expands neural
networks to characterize continuous-time data. The proposed learning approach has strong theoretical
guarantees, and can be easily adapted to a broad range of applications such as deep spatial-temporal
modelling, outlier and burst detection, and generative modelling for time series data.
Scope and limitation. Although the temporal kernel approach is motivated by the limiting-width
Gaussian behavior of neural networks, in practice, it suffices to use regular widths as we did in our
experiments (see Appendix B.2 for the configurations). Therefore, there are still gaps between our
theoretical understandings and the observed empirical performance, which require more dedicated
analysis. One possible direction is to apply the techniques in Daniely et al. (2016) to characterize the
dual kernel view of finite-width neural networks. The technical detail, however, will be more involved.
It is also arguably true that we build the connection between the temporal kernel view and continuous-
time system in an indirect fashion, compared with the ODE neural networks. However, our approach
is fully compatible with the deep learning subroutines while the end-to-end ODE neural networks
require substantial modifications to the modelling and implementation. Nevertheless, ODE neural
networks are (in theory) capable of modelling more complex systems where the continuous-time
setting is a special case. Our work, on the other hand, is dedicated to the temporal setting.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6155-6166, 2019.
Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S Klessen,
Lena Maier-Hein, Carsten Rother, and Ullrich Kothe. Analyzing inverse problems with invertible
neural networks. arXiv preprint arXiv:1808.04730, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, pp. 8139-8148, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and statistical
guarantees. In International Conference on Machine Learning, pp. 253-262, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. The Journal of Machine Learning
Research, 18(1):5595-5637, 2017.
Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
Science, 59(2):341-357, 2013.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American statistical Association, 112(518):859-877, 2017.
Salomon Bochner. Vorlesungen UberFouriersche integrale. Chelsea Publishing Company, 1948.
10
Published as a conference paper at ICLR 2021
StePhane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic
theory of independence. Oxford university press, 2013.
Peter J Brockwell, Richard A Davis, and StePhen E Fienberg. Time series: theory and methods:
theory and methods. SPringer Science & Business Media, 1991.
Bob CarPenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-
court. The stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint
arXiv:1509.07164, 2015.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Doctor
ai: Predicting clinical events via recurrent neural networks. In Machine Learning for Healthcare
Conference, pp. 301-318, 2016.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Arnaud Doucet and Adam M Johansen. A tutorial on particle filtering and smoothing: Fifteen years
later. Handbook of nonlinear filtering, 12(656-704):3, 2009.
Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of
the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
1555-1564, 2016.
Patrick E Farrell, David A Ham, Simon W Funke, and Marie E Rognes. Automated derivation of the
adjoint of high-level transient finite element programs. SIAM Journal on Scientific Computing, 35
(4):C369-C393, 2013.
Joseph Futoma, Sanjay Hariharan, and Katherine Heller. Learning to detect sepsis with a multitask
gaussian process rnn classifier. arXiv preprint arXiv:1706.04152, 2017.
Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based
recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-propagation.
In Proceedings of the 1988 connectionist models summer school, volume 1, pp. 21-28. CMU,
Pittsburgh, Pa: Morgan Kaufmann, 1988.
11
Published as a conference paper at ICLR 2021
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing Systems, pp. 8570-8581,
2019.
Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive session-
based recommendation. In Proceedings of the 2017 ACM on Conference on Information and
Knowledge Management, pp. 1419-1428, 2017a.
Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point
processes via reinforcement learning. In Advances in neural information processing systems, pp.
10781-10791, 2018.
Steven Cheng-Xian Li and Benjamin M Marlin. Classification of sparse and irregularly sampled time
series with mixtures of expected gaussian kernels and random features. In UAI, pp. 484-493, 2015.
Yang Li, Nan Du, and Samy Bengio. Time-dependent representation for neural event sequence
prediction. arXiv preprint arXiv:1708.00065, 2017b.
Zachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences
with rnns: Improved classification of clinical time series. In Machine Learning for Healthcare
Conference, pp. 253-270, 2016.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International Conference on Machine Learning, pp. 3208-3216, 2018.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. In International Conference on
Machine Learning, pp. 3276-3285. PMLR, 2018.
Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems, pp. 6754-6764,
2017.
Tomds Mikolov, Martin Karafidt, Lukds BurgeL Jan Cernocky, and SanJeev Khudanpur. Recurrent
neural network based language model. In Eleventh annual conference of the international speech
communication association, 2010.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016.
Barak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey. IEEE
Transactions on Neural networks, 6(5):1212-1228, 1995.
Ali Rahimi and BenJamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear
partial differential equations. Journal of Computational Physics, 357:125-141, 2018.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for data-
driven discovery of nonlinear dynamical systems. arXiv preprint arXiv:1801.01236, 2018a.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Numerical gaussian processes for
time-dependent and nonlinear partial differential equations. SIAM Journal on Scientific Computing,
40(1):A172-A198, 2018b.
12
Published as a conference paper at ICLR 2021
Sami Remes, Markus Heinonen, and Samuel Kaski. Non-stationary spectral kernels. In Advances in
Neural Information Processing Systems,pp. 4642-4651, 2017.
Simo Sarkka. Bayesianfiltering and smoothing, volume 3. Cambridge University Press, 2013.
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge
university press, 2004.
Mikhail Aleksandrovich Shubin. Pseudodifferential operators and spectral theory, volume 200.
Springer, 1987.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural
networks. In International conference on machine learning, pp. 2171-2180, 2015.
Hossein Soleimani, James Hensman, and Suchi Saria. Scalable joint models for reliable uncertainty-
aware event prediction. IEEE transactions on pattern analysis and machine intelligence, 40(8):
1948-1963, 2017.
Bharath Sriperumbudur and Zoltan Szab6. Optimal rates for random fourier features. In Advances in
Neural Information Processing Systems, pp. 1144-1152, 2015.
Dougal J Sutherland and Jeff Schneider. On the error of random fourier features. arXiv preprint
arXiv:1506.02785, 2015.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information
processing systems, pp. 1057-1063, 2000.
AShiSh Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Norbert Wiener et al. Generalized harmonic analysis. Acta mathematica, 55:117-258, 1930.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial intelligence and statistics, pp. 370-378, 2016.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Self-attention with
functional time representation learning. In Advances in Neural Information Processing Systems,
pp. 15889-15899, 2019.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representa-
tion learning on temporal graphs. arXiv preprint arXiv:2002.07962, 2020.
Akira Moiseevich Yaglom. Correlation theory of stationary and related random functions. Volume I:
Basic Results., 526, 1987.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystrom method
vs random fourier features: A theoretical and empirical comparison. In Advances in neural
information processing systems, pp. 476-484, 2012.
13
Published as a conference paper at ICLR 2021
A Appendix
We provide the omitted proofs, detailed discussions, extensions and complete numerical results.
A.1 Numerical results for Section 5

T-GRU
T-(CausaI)CNN
T-(SeIf)Attention
O
叫 Transpose
Attention
casual mask weight

Figure A.1: Visual illustrations on how we equip the standard neural architectures with the temporal
kernel using the random Fourier feature with invertible neural network (the RFF-INN blocks).
Case 1
	Weather	Wikipedia
VAR	0.2643	2.31
RNN	0.24877.002 —	0.51427.003 -
RNN-time	0.2629/.001	0.46987.004
RNN-trigo	0.25267.003	0.45427.004
T-RNN	0.2386/.002	0.4330/.002
CNN	0.3103/.002 —	0.49987.003 -
CNN-time	0.29337.003	0.48527.001
CNN-trigo	0.26847.004	0.45567.002
T-CNN	0.2662/.003	0.4399/.003
Attention	0.40527.003 —	0.47957.003 -
Attention-time	0.42987.003	0.48097.002
Attention-trigo	0.28877.002	0.44457.003
T-Attention	0.2674/.004	0.4226/.002
Table A.1: Mean absolute error for time series prediction of the regular scenario of Case 1. We
underline the best results for each neural architecture, and the overall best results are highlighted in
bold-font. The reported results are averaged over five repetitions, with the standard errors provided.
	Alibaba		Walmart	
Metric	Accuracy	DCG	Accuracy	DCG
GRU-Rec	77.817.21	47.127.11	18.097.13	3.447.21
GRU-Rec-time	77.707.24	46.217.13	17.667.16	3.297.23
GRU-Rec-trigo	78.957.19	49.017.11	21.547.13	6.677.18
T-GRU-Rec	79.47/.35_	J9.82Z.40_	23.41/.11	_ 84/21
CNN-Rec	-74.897.33—	一4^3.9"22 —	i5W18	—1.977719 一
CNN-Rec-time	74.857.31	43.887.21	15.957.18	1.967.17
CNN-Rec-trigo	75.977.21	45.867.23	17.747.17	3.807.15
T-CNN-Rec	76.45/.16_	_46.55/.38_	18.59/.33	_ 4.56/.31
-_ ATTN-Rec	-51.82.44—	一304"52 —	20.417.38	—7.527Γ18 -
ATTN-Rec-time	51.847.43	30.457.50	20.437.36	7.547.19
ATTN-Rec-trigo	53.057.30	33.107.29	24.497.15	8.937.13
T-ATTN-Rec	53.49/.31	33.58/.30	25.51/.17	9.22/.15
Table A.2: Accuracy and discounted cumulative gain (DCG) for the domain-specific models on the
temporal recommendation tasks. See Appendix B for detail. We underline the best results for each
neural architecture, and the overall best results are highlighted in bold-font.
14
Published as a conference paper at ICLR 2021
A.2 Supplementary material for Section 2
We discuss the detailed background for the Gaussian process behavior of neural network and the
training trajectory under neural tangent kernel, as well as the proof for Claim 1.
A.2.1 Gaussian process behavior and neural tangent kernel for deep
LEARNING MODELS
The Gaussian process (GP) view of neural networks at random initialization was originally discussed
in (Neal, 2012). Recently, CNN and other standard neural architectures have all been recognized
as functions drawn from GP in the limit of infinite network width (Novak et al., 2018; Yang, 2019).
When trained by gradient descent under infinitesimal step schedule, the gradient flow of the standard
neural architectures can be described by the notion of Neural Tangent Kernel (NTK) whose asymptotic
behavior under infinite network width is known (Jacot et al., 2018). The discovery of NTK has led
to several papers studying the training and generalization properties of neural networks (Allen-Zhu
et al., 2019; Arora et al., 2019a;b).
For a L-layer FNN f(θ, x) = f(L) with hidden dimensions {dh}hL=1 and recursively defined via:
f (L) = W(L) f (L)(X) + b(L),	f S)(X) = √1= W(h)σ(f(hT)(X)) + b㈤，f ⑼(x) = x,
h	(A.1)
for h = 1, 2, . . . , L - 1, where σ(.) is the activation function and the layer weights W(L) ∈ RdL-1,
W(h) ∈ Rdh-1 ×dh and intercepts are initialized by sampling independently from N(0, 1) (without
loss of generality). As d1, . . . , dL → ∞, f(h) tend in law to i.i.d Gaussian processes with covariance
Σh defined recursively as shown by Neal (2012):
Σ(1)(x, X0) = -1-X1X0 + 1,	Σ(h)(x, x0) = Ef〜N(0,∑(h-i)) [σ(f(x))σ(f (x0))] + 1.	(A.2)
h1
We also refer to Σ(h) as the neural network kernel to distinguish from the other kernel notions. Given
a training dataset {xi, yi}in=1, let f θ(s) = f (θ(s), x1), . . . , f (θ(s), xn) be the network outputs
at the sth training step and y = (y1, . . . , yn).
When training the network by minimizing the squared loss `(θ) with infinitesimal learning rate, i.e.
dθds = -V'(θ(s)), the network outputs at training step S follows the evolution (Jacot et al., 2018):
夕= -Θ(s)× (f(θ(s))-y),	[f = D f∂θ山,f∂A E.	(A.3)
ds	∂θ	∂θ
The above Θ(s) is referred to as the NTK, and recent results shows that when the network widths
go to infinity (or sufficiently large), Θ(s) converges to a fixed Θ0 almost surely (or with high
probability).
For a standard L-layer FFN, the NTK Θ0 = Θ(0L) for parameters {W(h), b(h)} on the hth-layer can
also be computed recursively:
Θ(0h)(xi, xj) = Σ(h)(xi,xj)
ς(Jk)(Xi Xj) = Ef〜N(0,∑(k-i))[σf (Xi))σf (Xj))],	(A.4)
and Θ0k)(xi, Xj) = Θ0k-1)(xi, Xj )Σ (k)(xi, Xj) + Σ(k) (xi, Xj),	k = h +1,...,L.
A number of optimization and generalization properties of neural networks can be studied using
NTK, which we refer the interested readers to (Lee et al., 2019; Allen-Zhu et al., 2019; Arora et al.,
2019a;b). We also point out that the above GP and NTK constructions can be carried out on all
standard neural architectures including CNN, RNN and the attention mechanism (Yang, 2019).
A.2.2 Proof for Claim 1
In this part, we denote the continuous-time system by X(t) in order to introduce the full set notations
that are needed for our proof, where the length of the discretized interval is explicitly considered. Note
that we especially construct the example in Section 2 so that the derivations are not too cumbersome.
However, the techniques that we use here can be extended to prove the more complicated settings.
15
Published as a conference paper at ICLR 2021
Proof. Consider X(t) to be the second-order continuous-time autoregressive process with covariance
function k(t) and spectral density function (SDF) s(ω) such that s(ω) = -∞∞ exp(-iωt)k(t)dt.
The covariance function of the discretization Xa[n] = X (na) with any fixed interval a > 0 is then
given by ka [n] = k(na). According to standard results in time series, the SDF of X(t) is given by in
the form of:
s(ω)	=	-2	+ P +-2 + b ,	aι	+	a2	= 0,	&仍2	+	&2优=0.
(A.5)
We assume without loss of generality that b1, b2 are positive numbers. Note that the kernel function
for Xa [i] can also be given by
ka [n] =	exp(ianω)s(ω)dω
-∞
1 ∞	(2k+1)π
=—ɪ2 J	exp(inω)s(ω∕a)dω
(A.6)
1∞	∞
e	exp(inω)	s(
a -∞
-∞	k=-∞
ω + 2kπ
dω,
which suggests that the SDF for the discretization Xa [n] can be given by:
sa (ω)
ω + 2kπ
h
_ aι( e2αb1 - 1
=2 ∖bι∣eabι - eiω∣2
e2ab2 - 1
bi∣eab2 - eiω|2
(A.7)
—
a
a1 (d1 - 2d2 cos(ω))
2b1b2∣(eab1 - eiω)(eαb2 - eiω)|2
where d2 = b2eab2 (e2ab2 - 1) - b1eab1 (e2ab2 - 1). By the definition of discrete-time auto-regressive
process, Xa[n] is a second-order AR process only if d2 = 0, which happens if and only if: b2/b1 =
(eab2 - e-ab2)/(eab1 - e-ab1 ). However, the function g(x) = exp(ax) - exp(-ax) is concave on
[0, ∞) (since the time interval a > 0) and g(0) = 0, the-above equality hold if b1 = b2. However,
this contradicts with (A.5), since a1 + a2 = 0 and a1b22 + a2b12 6= 0 suggests a1(b1 - b2)2 6= 0.
Hence, Xa [n] cannot be a second-order discrete-time auto-regressive process.	□
A.3 Supplementary material for Section 3
We first present the related discussions and proof for Claim 2 on the connection between continuous-
time system and temporal kernel. Then we prove the convergence result in Theorem 1 regarding the
random feature representation for non-stationary kernel. In the sequel, we show the new results for
the Gaussian process behavior and neural tangent kernel under random feature representations, and
discuss the potential usage of our results. Finally, we prove the sample-consistency result when the
spectral distribution is misspecified.
A.3.1 Proof and discussions for Claim 2
Proof. Recall that we study the dynamic system given by:
an(x)d f (x，t)+----+ ao(x)f(x,t) = bm(x) d ：(Xt +-----+ boe(x,t),	(A.8)
dtn	dtm
where e(x,t = to)〜 N(0, Σ(h), ∀to ∈ T. The solution process to the above continuous-time
system is also a Gaussian process, since (x, t) is a Gaussian process and the solution of a linear
different equation is a linear operation on the input. For the sake of notation, we assume b0 (x) = 1
and b1 (x) = 0, . . . , bm(x) = 0, which does not change the arguments in the proof. We apply the
Fourier transformation transformation on both sides and solve the for Fourier transform f(iωx, iω):
f(iωχ, iω) = (-/ 、(.",——1/	. ,―^w)W (iω; ωχ),	(A.9)
∖an(x) ∙ (iω)q + …+ aι(x) ∙ iω + ao(x))
16
Published as a conference paper at ICLR 2021
where W (iω; ωx) is the Fourier transform of (x, t). If we do not make the assumption on
{bj(x)}jm=1, they will simply show up on the numeration in the same fashion as {aj(x)}jn=1. Let
Gθτ (iω; x) = aq(x) ∙ (iω)q + …+ aι(x) ∙ iω + ao(x),
and p(ωχ) = |W(iω; ωχ)∣2 be the spectral density of the Gaussian process corresponding to E
(its spectral density does not depend on ω because is a Gaussian white noise process on the
time dimension). The dependency of G(∙; ∙) on θτ is because we defined θτ to the underlying
parameterization of {aj (∙)}n=ι in the statement of Claim 2. Then the spectral density of the process
f(x,t) is given by
p(ω, ωχ) = C ∙ p(ωχ)∣Gθτ (iω; x)∣2 H p(ωy)pθτ (ω; x),
where C is constant that corresponds to the spectral density of the random Gaussian noise on the time
dimension. Notice that the spectral density function obtained this way is regular, since it has the form
of pθT (ω; x) = constant/(polynomial of ω2).
Therefore, according to the classical Wiener-Khinchin theorem Brockwell et al. (1991), the covariance
function of the solution process is given by the inverse Fourier transform of the spectral density:
ψ(x, t)
H
H
2∏ /p(ω, ωχ)exp (i[ω, ωχ]l[t, x])d(ω, ωX)
JPθτ (ω; x)exp(iωt)dω ∙ Jp(ωχ) exp(iωXχ)dωχ
Kθτ ((χ,t), (χ,t)) ∙ ∑(h)(x, x)∙
(A.10)
And therefore We reach the conclusion in Claim 2 by taking ∑Th)(x, t; x0,t0) = ψ(x 一 x0,t 一 t0). □
The inverse statement of Claim 2 may not be always true, since not all the neural-temporal kernel can
find a exact corresponding continuous-time system in the form of (A.8). However, we may construct
the continuous-time system that approximates the kernel (arbitrarily well) in the following way, using
the polynomial approximation tools such as the Taylor expansion.
For a neural-temporal kernel Σ(Th), we first compute it Fourier transform to obtain the spec-
tral density p(ωx, ω). Note that p(ωx, ω) should be a rational function in the form of
(polynomial in ω2)/(polynomial in ω2), or otherwise it does not have stable spectral factorization
that leads to a linear dynamic system. To achieve the goal, we can always apply Taylor expansion or
Pade approximants that recovers the p(ωx , ω ) arbitrarily well.
Then we conduct a spectral factorization on p(ωx, ω) to find G(iωx, iωt) and p(ωx) such that
p(ωχ,ω) = G(iωχ, iωt)p(ωχ)G(-iωχ, -iω∕. Since p(ωχ,ω) is now in a rational function form
of ω2, we can find G(iωx, iωt) as:
bk(iωχ) ∙	(iω)k	+	…+ bι(iωχ)	∙ (iω) +	bo(iωχ)
aq(iωχ) ∙	(iω)q	+---+ aι(iωχ)	∙ (iω) +	ao(iωχ).
Let αj (x) and βj(x) be the pseudo-differential operators of aj (iωx) and bj(iωx) defined in terms of
their inverse Fourier transforms (Shubin, 1987), then the corresponding continuous-time system is
given by:
αq(x)d fχ,t) +------+ αo(x)f(x,t) = βk(x)dd* +--------+ βo(x)e(t).	(A.11)
For a concrete end-to-end example, we consider the simplified setting where the temporal kernel
function is given by:
2I—θl _______t _ t0 A	___t _ t0
Kθτ (t,t0) := kθ1,θ2,θ3 (t 一 tO) = θ2 Γ(θ]) (P2θ1 θɜ ) 1Bθι (P2θ1 θɜ ),
where Be、(∙) is the Bessel function so Ke『(t, t0) belongs to the well-known Matern family. It is
straightforward to show that the spectral density function is given by:
/、	(2θ∖	2∖-(θι + 1∕2)
s(ω) Z ( B + ω )	.
17
Published as a conference paper at ICLR 2021
As a consequence, We see that s(ω) (X (T + iω) -⑸山2)(中-iω) -⑸山2), so We
directly have GθT (ω)
-(θ1 +1/2)
+ iω
instead of having to seek for polynomial approxi-
mation. NoW We can easily expand GθT (ω) using the binomial formula to find the linear parameters
for the continuous-time system. For instance, When θ1 = 3/2, We have:
d2f ⑴ +2 √2θ1 df(t)
dt2	θ3	dt
A.3.2 Proof for Proposition 1
Proof. We first need to shoW that the random Fourier features for the non-stationary kernel
KT (x, t), (x0, t0) can be given by (11), i.e.
φ(χ,t) = 2⅛
.,cos ([x,t]lω1,i) +cos ([x,t]lω2,i), Sin ([x,t]1 ω1,i) +sin ([x, t]lω2,i)...].
To simplify notations, We let z := [x, t] ∈ Rd+1 and Z = X × T. For non-stationary kernels, their
corresponding Fourier transform can be characterized by the folloWing lemma. Assume Without loss
of generality that KT is differentiable.
Lemma A.1 (Yaglom (1987)). A non-stationary kernel k(z1, z2) is positive definite in Rd if and
only if after scaling, it has the form:
k(z1, z2)
Jexp (i(ω]zι - ωjz2))μ(dω1, dω2),
(A.12)
where μ(dωι, dω2) is some Positive-Semidefinite probability measure with bounded variation.
The above lemma can be think of as the extension of the classical Bochner’s theorem underlies the
random Fourier feature for stationary kernels. Notice that when covariance function for the measure μ
only has non-zero diagonal elements and ω1 = ω2 , then We recover the spectral representation stated
in the Bochner’s theorem. Therefore, We can also approximate (A.12) With the Monte Carlo integral.
However, We need to ensure the positive-semidefiniteness of the spectral density for μ(dω1, dω2),
Which We denote by p(ω1, ω2). It has been suggested in Remes et al. (2017) that We consider another
density function q(ω1, ω2) and letp be taken on the product space of q and then symmetrise:
p(ωι, ω2) = 1 (q(ω1, ω2) + q(ω2, ω1) + q(ω1, ω1) + q(ω?, ω2)).	(A.13)
Then (A.12) suggests that
k(z1,z2) = 4Eq hexp (i(ωj^zι-ωjz2)) + exp (i(ωjz2 — ωj^z1))
+ exp (i(ωj^z1 — 3|z。) + exp (i(ωjz2 - a|Z2))].
Recall that the real part of exp (i(ω∣z1 - ω∣z2)) is given by cos(ω∣z1 - ω∣z2). So with the
Trigonometric equalities, it is straightforward to verify that k(z1, z2) = Eq [φ(z)lφ(z)]. Hence, the
random Fourier features for non-stationary kernel can be given in the form of (11).
Then we show the uniform convergence result as the number of samples goes to infinity when comput-
ing Eq [φ(z)lφ(z)J by the Monte Carlo integral. Let Z = ZXZ, so Z = {(x,t, x0,t,) ∣∣ x, x0 ∈
X; t,t0 ∈ T}. Since diam(X) = l and T = [0, tmax], we have diam(Z) = 12瑞ɑ乂. Let the
approximation error be
∆(z, z0) = φ(z)lφ(z0) — KT(Z.z0).	(A.14)
The strategy is to use a e-net covering for the input space Z, which would require N =
(2l2tm&x/r)d+1 balls of radius r. Let C = {ci}iN=1 be the centers for each e-ball. We first show the
bound for ∣∆(g) | and the Lipschitz constant Lδ of the error function ∆, and then combine them to
get the desired result.
18
Published as a conference paper at ICLR 2021
Since ∆ is continuous and differentiable w.r.t z, z0 according to the definition of φ, we have L∆ =
∣∣V∆(c*)^, where c* = argmaxc∈c ∣∣V∆(c)^. Let c* = (Z,Z0). By checking the regularity
conditions for exchanging the integral and differential operation, we verify that E[Vφ(z)lφ(z0)]=
VE [φ(z)1 φ(z0)] = VE [Kt(z, z0)]. We do not present the details here, since it is easy to check the
regularity of φ(z)lφ(z0) as it consists of the sine and cosine functions who are continuous, bounded
and have continuous bounded derivatives. Hence, we have:
E[L∆] = Ez,zo h∣∣Vφ(Z)lφ(Z0) - VKT(Z,Z0)∣∣2]
=Ez,zo hE∣∣Vφ(Z)lφ(Z0)k2 - 2∣∣VKT(Z,Z0)∣∣∙ ∣∣Vφ(Z)lφ(Z0)∣∣ + IlVKT(Z,引|[2]
≤ Ez,zo [EkVφ(Z)lφ(Z0)k2 -∣VKτ(Z, z0)k2i (by Jensen,s inequality)
≤ E∣∣Vφ(Z)lφ(Z0)k2
=E∣∣V( cos(Zlωι) + c0s(Zlω2))( Cos((Z0)lω1) + Cos((Z0)lω2))
+ (sin(Zlωι) +sin(Zlω2))( sin((Z0)lω1) +sin((Z0)lω2)) ∣∣
=2E∣∣ωι(sin(Zlωι 一(Z0)lωι) + sin((Z0)lω2 — Zlωι))
+ ω2( sin(Zlωι — (Z0)lω2) + sin((Z0)lω2 — Zlω2))∣∣
≤ 8E∣∣[ω1,ω2]∣∣2 = 8σp2.
(A.15)
Hence, by the Markov’s inequality, we have
P(Lδ ≥ 2r) ≤ 8σPC).	(A.16)
Then we notice that for all c ∈ C, ∆(c) is the mean of m/2 terms bounded by [—1, 1], and the
expectation is 0. So applying a union bound and the Hoeffding’s inequality on bounded random
variables, we have:
p( ∪N=ι iδ(Ci)I ≥ 2) ≤ 2Nexp (—16^).	(A.17)
Combining the above results, we get
p sup ∆(Z, Z0) ≤
(z,z0)∈C
≥1 — 32σp — 2r-(d+1) (2l2^max)d+1 exp (— m2
≥ C(d)
l2tmaxσp
2(d+1)/(d+3)
exp
2
m2
8(d + 3)
(A.18)
—
where in the second inequality we optimize over r such that r*
(T)1/(d+3)with
k1 = 2(2l2t2max)d+1 exp(—m2/16) and k2 = 32σp2-2. The constant term is given by C(d) =
7d+9	-	-d-1	2
2κ((d+i) d+3 + (d)d+3).	口
A.3.3 The Gaussian process behavior and neural tangent kernel after
COMPOSING WITH TEMPORAL KERNEL WITH THE RANDOM FEATURE
REPRESENTATION
This section is dedicated to show the infinite-width Gaussian process behavior and neural tangent
kernel properties, similar to what we discussed in Appendix A.2, when composing neural networks
in the feature space with the random feature representation of the temporal kernel.
For brevity, we still consider the standard L-layer FFN of (A.1). Suppose we compose the FFN with
the random feature representation φ(x, t) at the kth layer. It is easy to see that the neural network
19
Published as a conference paper at ICLR 2021
kernel for the first k - 1 layer are unchanged, so we compute them in the usual way as in (A.2). For
the kth layer, it is straightforward to verify that:
lim
dk →∞
E
h-1 DW(k)f (k-1)(θ, x) ◦ φ(x,t), W(k)f (k-1)(θ, x0)
dk
◦ φ(x0, t0)Ef (k-1)i
→ Σ(k)(x, x0) ∙ KT((x,t),(x0,t0)).
The intuition is that the randomness in W (thus f(θ, .)) and φ(., .) are independent, i.e. the former
is caused by network parameter initializations and the later is induced by the random features. The
covariance functions for the subsequent layers can be derived by induction, e.g. for the (k + 1)th
layer we have:
球+1) ((X,t),(X0,tO))= Ef〜N(o,∑(k)0κT) [σ(f Mt))σ(f (XitO))].
In summary, composing the FNN, at any given layer, with the temporal kernel using its random
feature representation does not change the infinite-width Gaussian process behavior. The statement is
true for all the deep learning models who also have the Gaussian process behavior, which includes
most of the standard neural architectures including RNN, CNN and attention mechanism (Yang,
2019).
The derivations for the NTK, however, is more involved since the gradient on all the layers are
affected. We summarize the result for the L-layer FFN in the following proposition and provide the
derivations afterwards.
Proposition A.1. Suppose f(k) (θ, (x, t)) = vec(f (k)(θ, x) ◦ φ(x, t)) in the standard L-layer FFN.
Let ΣTh) = Σ(h) for h = 1,...,k, ΣTk) = ΣTk) 0 KT and ΣTh) = Ef Nc 心、[σ(f )σ(f)] + 1
f ~N (0,ςt )
for h = k + 1, . . . , L. If the activation functions σ have polynomially bounded weak derivatives,
as the network widths d1, . . . , dL → ∞, the neural tangent kernel Θ(L) converges almost surely to
Θ(TL) whose partial application on parameters {W(h), b(h)} in the hth-layer is given recursively by:
ΘTh) = ΣTh), ΘTk) = ΘTk-1) 0 Σ Tk) + ΣTk), k = h +1,...,L.	(A.19)
Proof. The strategies for deriving the NTK and show the convergence has been discussed in Jacot
et al. (2018); Yang (2019); Arora et al. (2019a). The key purpose for us presenting the derivations
here is to show how the convergence results for the neural-temporal Gaussian (Section 4.2) affects
the NTK. To avoid the cumbersome notations induced by the peripheral intercept terms, here we omit
the intercept terms b in the FFN without loss of generality. We let g(h) = √= σ f (h)(x, t)), so the
FFN can be equivalently defined via the recursion: f(h) = W(h)g(h-1) (x, t). For the final output
f θ, (x, t) := W(L) f(L) (x, t), the partial derivative to W(h) can be given by:
fW (XhF = Z㈤(x,t)(g(I)(X,t))T,	(A.20)
with Z(h) defined by;
z(h)(x,t)= { √√= D(h)(x,t)(W(h+1))lz(h+1)(x,t),
dh
h = L,
h=1,...,L-1,
(A.21)
where
Iyh)(X力 J diag(σ(f(h)(X,t))} h = k,...,L —1,
，	[diag(σ(f(h)(x))), h =1,...,k — 1.
Using the above definitions, we have:
D fWx^, fW⅛^ E=Dz(h)(x,t)(g(h-1)(x,t))1, z(h)(xo,to)(g(h-ι)(xo,to))1 E
=<g(h-1)(x,t), g(h-D(x0,t0)) ∙ (z(h)(x,t), z(h)(x0,t0)>
20
Published as a conference paper at ICLR 2021
We have established in Section 4.2 that
〈g(I)(X,t), g(I)(XO,t0)〉→ ∑Th-1) ((x,t), (x0,t0)),
where
(Σ(h)(x, x0)	h = 1,...,k
∑M(x,t), (x0,t0)) = ∖ ς㈤(x，x0) ∙ KT俨",(X0,tO))	h = k
(Ef〜N(0,∑hτ)) σf(X,tDσ(f(X0,t0))_| h =k + 1,...,L.
(A.22)
By the definition of z(h), we get
〈Z(h) (x, t), Z(h) (xO, tO)
=ɪ DDS)(X,t)(W(h+1))
≈ ɪDDS)(X,t)(W(h+1))
TZ(h+I)(X,t),D㈤(x0,t0)(W(h+I))IZ(h+1)
|z(h+I)(X,t),D㈤(x0,t0)(W (h+1))lz(h+1)
(xO,tO)
(xO,tO)E
(A.23)
→ ɪtr(D(h)(x,t)D(h)(x0,t0))〈z(h+1)(x,t),z(h+1)(x0,t0))
→ ∑T((x,t), (x0,t0)〈z(h+1)(x,t),z(h+1)(x0,t0)〉.
The approximation in the third line is made because the W(h+1) in the right half is replaced by its
i.i.d copy under Gaussian initialization. This does not change the limit when dh → ∞ when the
actionvation functions have polynomially bounded weak derivatives Yang (2019) such the ReLU
activation. Carrying out (A.23) recursively, we see that
L-1
〈z(h)(x,t),z(h)(x0,t0)> → Y Σj((x,t), (x0,t0)).
j=h
Finally, we have:
D ∂f(θ, (x,t)) ∂f (θ, (x0,t0)) E _ XX D ∂f(θ, (x,t)) ∂f(θ, (x0,t0))
∖	∂θ	,	∂θ / =	∖ ~∂W(h)~, ~∂Ws)-
h=1
LL
=X (∑Tl) ((x,t), (x0,t0)) ∙ Y ∑j ((x,t), (x0,t0))).
h=1	j=h
(A.24)
Notice that we use a more compact recursive formulation to state the results in Proposition 1. It is
easy to verify that after expansion, We reach the desired results.	□
Compared With the original NTK before composing With the temporal kernel (given by (A.4)), the
results in Proposition A.1 shares a similar recursion structure. As a consequence, the previous results
for NTK can be directly adopted to our setting. We list tWo examples here.
•	FolloWing Jacot et al. (2018), given a training dataset {xi, ti, yi(ti)}in=1, let fT θ(s) =
f (θ(s), x1, t1), . . . , f (θ(s), xn, tn) be the netWork outputs at the sth training step and
yT = y1(t1), . . . , yn(tn) . The analysis on the optimization trajectory under infinitesimal
learning rate can be conducted via:
dfτ (θ(s))
ds
-ΘT (s) × (fT θ(s) - yT),
Where ΘT (s) converges almost surely to the NTK Θ(TL) in Proposition A.1.
•	FolloWing Allen-Zhu et al. (2019) and Arora et al. (2019b), the generalization performance
of the composed time-aWare neural netWork can be explicitly characterized according to the
properties of Θ(TL).
21
Published as a conference paper at ICLR 2021
A.3.4 Proof for Theorem 1
Proof. We first present a technical lemma that is crucial for establishing the duality result under the
distributional constraint df (SθT kS) ≤ δ. Recall that the hidden dimension for the kth layer is dk.
Lemma A.2 (Ben-Tal et al. (2013)). Let f be any closed convex function with domain [0, +∞), and
this conjugate is given by f *(s) = supt≥0{ts — f (t)}. Thenfor any distribution S and anyfunction
g : Rdk+1 → R, we have
sup	g g(ω)dSθτ (ω) = inf ]λ j f *(g(ω)η)dS(ω)+ δλ + η).	(A.25)
SθT :df (Sθt kS)≤δ√	λ≥0,η l J V λ 7	J
We work with a scaled version of the f-divergence under f (t) = 1 (tk — 1) (because its dual function
has a cleaner form), where the constraint set is now equivalent to {Sθτ : df(Sθτ ∣∣S) ≤ δ∕k}. It is
easy to check that f (S) = -17 [s，+ 1 with -17 + 1 = 1.
Similar to the proof for Proposition 1, we let z := [x, t] ∈ Rd+1 and Z = X × T to simplify the
notations. To explicitly annotate the dependency of the random Fourier features on Ω, which is
the random variable corresponding to ω, we define φ(z, Ω) such that φ(z, Ω) = [cos(zlΩι) +
Cos(ZTΩ2), Sin(ZTΩι) + Sin(ZTΩ2)], where Ω = [Ωι, Ω2]. Then the approximation error, when
1	.1	1	1 T-I ∙ Γ∙	Il . 1	∙	∙	1	1	∙	1 1	7 / i-χ∖ ♦	♦	F
replacing the sampled Fourier features φ by the original random variable φ(z, Ω), is given by:
∆n(Ω) :=	. 1 1 . X Σ(k)(xi, Xj)φ(Zi, Ω)τφ(Zj, Ω)
n(n— 1) i6=j
—E[∑(k)(Xi, Xj)Kt,Sθt ((Xi,Ti), (Xj,Tj))]
=(ɪ ) X Σ(k)(xi, Xj)φ(zi, Ω)τΦ(Zj, Ω) — E[∑(k)(X, X0)φ(Z, Ω)τφ(Z0, Ω)].
n(n — 1) i6=j
(A.26)
We first show that sub-Gaussianity of ∆n(Ω) . Let {χi}n=1 be an i.i.d copy of the observations
except for one element j such that xj 6= x0j . Without loss of generality, we assume the last element
is different, i.e. Xn = xn. Let ∆n(Ω) be computed by replacing X and Z with the above x0 and its
corresponding Z0 . Note that
∣∆n(Ω) — ∆n(Ω)∣
=γ⅛1j I X Σ(k)(xi, Xj)Φ(Zi, Ω)τφ(Zj, Ω) — Σ(k)(xi, xj)φ(Zi, Ω)τφ(Zj, Ω)∣
n(n — 1) i6=j
≤	/ 1 1、(X |£(k)(xi, Xn)3(Zi, q|3出,ω) — £(k)(xi, xn)φ(>i,q|证"ωM (N Tn
n(n — 1)	n	n (A.27)
i<n
+ X l∑(k)(Xn, Xj )Φ(Zn, Ω)τφ(Zj, Ω) — Σ(k)(xn, Xj %(*, Ω)τφ(Zj , Ω)∣)
j<n
V 4max{1, M}
n,
where the last inequality comes from the fact that the random Fourier features φ are bounded by 1
and the infinity norm of Σ(k) is bounded by M. The above bounded difference property suggests
that ∆n(Ω) is a 4maxni,M} -SUb-GaUSSian random variable.
22
Published as a conference paper at ICLR 2021
To bound ∆n(Ω), we use:
Sup	∣∣ Sθτ∙df(Sθτ ||S) 1	[∆n(Ω)dSθτ∣ ≤	SuP	八∆n(Ω)∣dSθτ J	1	Sθτ∙df(Sθτ ∣∣S)J ≤ inf nλ-γγ-ES[∣∆n(Ω)∣k0] + "(\+ 1) } (using Lemma2) λ≥0	k0	k =(δ + 1)1/kEs [∣∆n(Ω)∣k0]1/k0(solving for λ* from above) =√δ+lEs [∣∆n(Ω)∣2 ]1/2 (let k = k0 = 1∕2). (A.28)
Therefore, to bound supse『：df 但@『∣∣s) ∣ R ∆n(Ω)dSe/ we simply need to bound [∣∆n(Ω)∣2]. US-
ing the classical results for sub-Gaussian random variables (Boucheron et al., 2013), for λ ≤ n/8, we
have
e[exp (λ∆n(Ω))2] ≤ exp ( — ɪ log(1 — 8max{1, M}λ∕n)).
Then we take the integral over ω
p( Z ∆n(ω)2dS(ω)≥ δ+l)
λ2
≤ E[exp (λ J ∆n(ω)2dS(ω)J exp (一 y+ɪ
1	8 max{1, M}λ	λ2
≤ exp(-2log(1-------n—)— δ+ι>
(Chernoff bound)
(apply Jensen’s inequality).
(A.29)
Finally, let the true approximation error be ∆n(ω) = Σ(k) (Se『)— Σ(k) (Se『).Notice that
∣∆ n(ω)∣ ≤ ∣∆n(Ω)∣ + ( —	X Σ(k)(xi, Xj )∣φ(zi, Ω)lφ(Zj, Ω) — φ(Zi)lφ(Zj )∣.
n(n — 1) i6=j
From (A.28) and (A.29), we are able to bound SuPSeT：df (Sθt∣∣s) ∆n(Ω). For the second
term, recall from Proposition 1 that we have shown the stochastic uniform convergence bound
for ∣φ(zi, Ω)lφ(zj, Ω) — φ(zi)lφ(zj)∣ under any distributions Se『. The desired bound for
P(SuPSeT：df (Sθτ||s) I∆n(ω)∣ ≥ E) is obtained after combining all the above results.
□
A.3.5 Reparametrization with Invertible neural network
In this part, we discuss the idea of constructing and sampling from an arbitrarily complex distribution
from a known auxiliary distribution by a sequence of invertible transformations. Given an auxiliary
random variable z following some know distribution q(z), suppose another random variable X is
constructed via a one-to-one mapping from z: X = f (z), then the density function ofX is given by:
P(X)=q(z)∣d∣∣=q(f-1 (X))If4
(A.30)
We can parameterize the one-to-one function f(.) with free parameters θ and optimize them over the
observed evidence such as by maximizing the log-likelihood. By stacking a sequence of Q one-to-one
mappings, i.e. X = fQ ◦ fQ-1 ◦ . . . f1(z), we can construct complicated density functions. It is easy
to show by chaining that P(X) is given by:
log P(X)
Q	-1
log q(z)—χ∣s∣.
(A.31)
Samples from the auxiliary distribution can be transformed to the unknown target distribution in
the same manner, and the transformed samples are essentially parameterized by the transformation
mappings.
23
Published as a conference paper at ICLR 2021
Unfortunately, most standard neural architectures are non-invertible, so we settle on a specific family
of neural networks - the invertible neural network (INN) Ardizzone et al. (2018). A major component
of INN is the affine coupling layer. With z sampled from the auxiliary distribution, we first divide z
into two halves [z1, z2] and then let:
v1 = z1 exp s1 (γ, z2) + t1 (γ, z2)
v2 = z2 exp s2(γ, z1) + t2(γ, z1),
(A.32)
where si(γ, ∙)), sι(γ, ∙)), tι(γ, ∙)), tι(γ, ∙)) Can be any function parameterized by different parts of
γ. Here,	denotes the element-wise product. Then the outcome is simply given by:
g(γ, z) = [v1, v2].
To see that g(γ, ∙) is invertible so the inverse transform mappings are tractable, it is straightforward
to show that g-1(γ, [v1, v2]) is given by:
z2 = v2 - t1(γ,v1)	exp - s1(γ,v1)
Zi = (vi — t2(γ, V2)) Θ exp ( — SNY V2)).
(A.33)
By stacking multiple affine coupling layers, scalar multiplication and summation actions (which
are all invertible), we are able to construct an INN with enough complexity to characterize any
non-degenerating distribution.
B	Supplementary material for Section 5
We provide the detailed dataset description, experiment setup, model configuration, parameter tuning,
training procedure, validation, testing, sensitivity analysis and model analysis. The reported results
are averaged over five iterations.
B.1	Datasets
•	Jena weather dataset1. The dataset contains 14 different features such as air temperature,
atmospheric pressure, humidity, and other metrics that reflect certain aspect of the weather.
The data were collected between between 2009 and 2016 for every 10 minutes, so there are
6 observations in each hour.
A standard task on this dataset is to use 5 days of observations to predict the temperature 12
hours in the future, which we refer to as the Case 1. We use a sliding window to obtain the
training, validation and testing samples and make sure they have no overlaps (right panel of
Figure A.2).
•	Wikipedia traffic.2 The Wiki web page traffic records the daily number of visits for 550
Wikipedia pages from 2015-07-01 to 2016-12-31. The features are decoded from the
webpage url, where we are able to obtain the project name, e.g. zh.wikipedia.org, the access,
e.g. all-access, and the agent, e.g. spider. We use one-hot encoding to represent the features,
and end up with a 14-dimension feature vector for each webpage.
The feature vectors do not change with time. We use the feature vectors and traffic data from
the past 200 days to predict the traffic of the next 14 days, which is also a standard task for
this dataset. The missing data are treated as zero.
•	Alibaba online shopping data.3. The dataset contains 4,136 online shopping sequences
with a total of 1,552 items. Each shopping sequence has a varying number of time-stamped
user-item interactions, from 11 to 157. We consider the standard next-item recommendation
task, where we make a recommendation based on the past interactions. No user or item
features are available.
•	Walmart.com e-commerce data.4 The session-based online shopping data contains
〜12,000 shopping sessions made by 1,000 frequent users, with a total of 2,034 items.
1https://www.bgc-jena.mpg.de/wetter/
2https://www.kaggle.com/c/web-traffic-time-series-forecasting/data
3https://github.com/musically-ut/tf_rmtpp/tree/master/data/real/ali
4https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs
24
Published as a conference paper at ICLR 2021
The lengths of the sessions vary from 14 to 87. In order to be consistent with the Alibaba
online shopping data, we do not use the provided item and user features. We also consider
the next-item recommendation task.
Preprocessing, train-validation-test split and metric To ensure fair comparisons across the various
models originated from different setting, we minimize the data-specific preprocessing steps, especially
for the time series dataset.
Figure A.2: Left: the walk-forward split. Notice that we do not to specify the train-validation
proportions when using the walk-forward split. Right: side-by-side split with moving window.
•	Jena weather dataset. We do the train-validation-test split by 60%-20%-20% on the time
axis (right panel of Figure A.2). We first standardize the features on the training data, and
then use the mean and standard deviation computed on the training data to standardize the
validation and testing data, so there is no information leak.
For Case 1, we use the observations made at each hour (one every six observations) in the
most recent 120 hours (5 days) to predict the temperature 12 hours in the future.
For Case 2, we randomly sample 120 observations from the most recent 120 hours (with a
total of 720 observations), to predict the temperature 12 hours in the future.
For Case 3, we randomly sample 120 observations from the most recent 120 hours (with a
total of 720 observations), to predict the temperature randomly sampled from 4 to 12 hours
(with a total of 48 observations) in the future.
•	Wikipedia traffic. We use the walk-forward split (illustrated in the left panel of Figure
A.2) to test the performance of the proposed approaches under different train-validation-test
split schema. The walk-forward split is helpful when the length of training time series is
relatively long compared with the full time series. For instance, on the Wiki traffic data, the
full sequence length is 500, and the training sequence length is 200, so it is impossible to
conduct the side-by-side split. The features are all one-hot encoding, so we do not carry out
any preprocessing. The web traffics are standardized in the same fashion as the Jena weather
data.
For Case 1, we use the most recent 200 observations to predict the full web traffics for 14
days in the future.
For Case 2, we randomly sample 200 observations from the past 500 days (with a total of
500 observations), to predict the full web traffics for 14 days in the future.
For Case 3, we randomly sample 200 observations from the past 500 days (with a total of
500 observations), to predict 6 web traffics sampled from 0 to 14 days in the future.
•	Alibaba data. We conduct the standard preprocessing steps used in the recommender
system literature. We first filter out items that have less than 5 total occurrences, and then
filter out shopping sequences that has less then 10 interactions. Using the standard train-
validation-test split in sequential recommendation, for each shopping sequence, we use all
but the last two records as training data, the second-to-last record as validation data, and the
last record as testing data.
All the training/validation/testing samples obtained from the real shopping sequences are
treated as the positive record. For each positive record x(t - k), . . . , x(t), x(t + 1) , we
25
Published as a conference paper at ICLR 2021
randomly sample 100 items {x0i(t + 1)}i1=001, and treat each (x(t - k), . . . , x(t), x0i(t + 1)0)
as a negative sample, which is again a standard implementation in the recommender systems
where no negative labels are available.
•	Walmart.com data. We use the same preprocessing steps and train-validation-test split as
the Alibaba data.
As for the metrics, we use the standard Mean absolute error (MAE) for the time-series prediction
tasks. For the item recommendation tasks, we use the information retrieval metrics accuracy and
discounted cumulative gain (DCG). Recall that for each shopping sequence, there is one positive
sample and 100 negative samples. We rank the candidates x(t + 1), x01(t + 1), . . . , x0100(t+1)
according to the model’s output score on each item. The accuracy checks whether the candidate with
the highest score is the positive x(t + 1), and the DCG is given by 1/ log rank x(t + 1) where
rank x(t + 1) is the position at which the positive x(t + 1) is ranked.
B.2	Model configuration and implementation detail
We observe from the empirical results that using kernel addition (Σ(Th) (x, t; x0, t0) = Σ(h) (x, x0) +
KT (x, t; x0, t0)) and kernel multiplication give similar results in our experiments, but kernel addition
is much faster in both training and inference. Therefore, we choose to use kernel addition as a trick
to expedite the computation. Note that kernel addition corresponds to simply adding up the hidden
representations from neural network, and the random features from the temporal kernel.
We first show the configuration and implementation for the models we use in time-series prediction.
All the models take the same inputs for each experiment with the hyperparamters tuned on the
validation data. Note that the VAR model do not have randomness in model initialization and training,
so their outputs do not have standard deviations.
For the NN+time, NN+trigo and T-NN models, the temporal structures (time feature) are added to
the same part of the neural architectures (illustrated in Figure A.1), and are made to have the same
dimension 32 (except for NN+time). We will conduct sensitivity analysis later on the dimension.
For the proposed T-NN models, we treat the number of INN (affine coupling) blocks used in building
the spectral distribution as tuning parameters. All the remaining model configurations are the same
across all model variants. We do not experiment on using regularizations, dropouts or residual
connections. In terms of reparameterization, we draw samples from the auxiliary distribution once at
the beginning, so we do not have to resample during the training process.
•	VAR. In the vector autoregression model, each variable is modeled as a linear combination
of past values of itself and the past values of other variables in the system. The order, i.e.
the number of past values used, is treated as the hyperparameter, which we select according
according to the AIC criteria on the validation data.
For experiments on the Jena weather dataset, we choose the order from
{20, 40, 60, 80, 100, 120}, since the maximum length of the history used for prediction
is 120 for all three cases. Similarly, for experiments on the Wiki traffic dataset, we choose
the order from {40, 60, 80, . . . , 200}.
•	RNN models. We use the one-layer RNN model the standard RNN cells. The hidden
dimension for the RNN models is selected to be 32 after tuning on the orignal model. To
make time-series prediction, the output of the final state is then passed to a two-layer fully-
connected multi-layer perceptron (MLP) with ReLU as the activation function. We adjust
the hidden dimensions of the MLP for each model variant such that they have approximately
the same number of parameters.
•	CausalCNN models. We adopt the CausalCNN (Wavenet) architecture from Oord et al.
(2016). The original CausalCNN treats the number of filters, filter width, dilation rates and
number of convolutional layers are hyperparameters. Here, to avoid the extensive parameter
tuning for each model variant, we tune the above parameter of the plain CausalCNN model
and adpot them to the other model variants. Specifically, we find that using 16 filters where
each filter has a width of 2, with 5 convolutional layers and the dilation rates given by
{2l}l5=1, gives the best result for all three cases. Similar to the RNN models, we then pass
the output to a two-layer MLP to obtain the prediction.
26
Published as a conference paper at ICLR 2021
•	Attention models. We use a single attention block in the self-attention architecture Vaswani
et al. (2017). Unlike the ordinary attention in sequence-to-sequence learning, here we
need to employ an extra causal mask (shown in Figure 1) to make sure that the model is
not using the future information to predict the past. Other than that, we adopot the same
key-query-value attention setting, with their dimension as the tuning parameter. We find
that dimension=16 gives the best result in all cases for the original model and we adopt this
setting to the rest model variants. Also, we pass the output to a two-layer MLP to obtain the
prediction.
We now discuss the configurations and implementations for the recommendation models. We find out
that the two-tower architecture illustrated in Figure A.3 is widely used for sequential recommendation
(Hidasi et al., 2015; Li et al., 2017a). Each item is first passed through an embedding layer, and
the history is processed by some sequence processing model, while the target is transformed by
some feed-forward neural network (FFN) such as the MLP. The outputs from the two towers are
then combined together, often by concatenating, and then pass to the top-layer FFN to obtain a
prediction score. Hence, we adopt this complex neural architecture to examine our approaches, with
the sequence processing model in Figure A.3 replaced by their time-aware counterparts given in
Figure A.1.
Sequence learning
Embedding layer
Figure A.3: A standard two-tower neural architecture for sequential recommendation. To incorporate
the continuous-time information, we adopt the model architectures in A.1 to replace the sequence
processing model here.
To be consistent across the Alibaba and Walmart.com dataset, we use the same model configurations.
Specifically, the dimension of the embedding layer is chosen to be 100, the FNN on the sequence
learning layer is a single-layer MLP with ReLU as activation, and the prediction layer FNN a two-
layer MLP with ReLU as activation. We keep the modules to be as simple as possible to avoid
overcomplications in model selection. For the T-NN models, we treat the dimension of the random
Fourier features φ, and the number of INN (affine coupling) blocks used in building the spectral
distribution as tuning parameters. We also do not experiment on using regularizations, dropouts or
residual connections.
•	GRU-Rec. We use a single layer RNN with GRU cells as the sequence processing model.
We treat the hidden dimension of GRU as a tuning parameter, and according to the validation
performance evaluated by the accuracy, dimension=32 gives the best outcome for the plain
GRUforRec, and we adopt it for all the RNN variants.
•	CNN-Rec. Similar to the experiments for time-series prediction, we treat the number of
convolutional layers, dilation rates, number of filters and filter width as tuning parameters.
We select the best settings for the plain CNNforRec and adopt them to all the model variants.
•	ATTN-Rec. We also use the single-head single-block self-attention model as the sequence
processing model. We treat the dimension of the key, query, value matrices (which are of
the same dimension) as the turning parameter. It turns out that dimension=20 gives the best
validation performance for ATTNforRec, which we adopt to all the model variants.
27
Published as a conference paper at ICLR 2021
Figure A.4: The training and inference speed comparisons for standard neural architectures (RNN,
CausalCNN, self-attention) equipped with the proposed temporal kernel approach, and concatenating
the time to feature vector.
Unlike the time-series data where the all the samples have a equal sequence length, the shopping
sequences have various lengths. Therefore, we set the maximum length to be 100 and use the masking
layer to take account of the missing entries.
Finally, we mention the implementation to handle the different scales and formats of the timestamps.
The scale of the timespan between events can be very different across datasets, and the absolute
value of the timestamps are often less informative, as they could be the linux epoch time or the
calendar date. Hence, for each sequence given by (x1, t1), . . . , (xk, tk), (xk+1, tk+1) where the
target is to predict (xk+1, tk+1), we convert it to the representation under timespans: ((x1, tk+1 -
t1)), . . . , (xk, tk+1 - tk), (xk+1, 0) . We then transform the scale of the timespans to match with
the problem setting, for instance, in the online shopping data the timespan is measured by the minute,
and in the weather prediction it is measured by the hour.
B.3	Computation
The VAR is implemented using the Python module statsmodels5 . The deep learning models are
implemented using Tensorflow 2.0 on a single Nvidia V100 GPU. We use the Adam optimizer and
adopt the early-stopping training schema where the trainning is stopped if the validation metric stops
improving for 5 epochs. The loss function is mean absolute error for the time series prediction,
and binary cross-entropy loss with the softmax function for the recommendation tasks. The final
metrics are computed on the hold-out test data using model checkpoints saved during training that
has the best validation performance.
We briefly discuss the computation efficiency of our approach. From Figure A.4 we see that the extra
computation time for the proposed temporal kernel approach is almost negligible compared with
concatenating time to the feature vector. The advantage is partly due to the fact that we draw samples
from the auxiliary distribution only once during training. Note that this does not interfere with our
theoretical results, and greatly speeds up the computation.
B.4	Visual results
We visualize the predictions of our approaches on the Jena weather data in Figure A.5. We see that
T-RNN, T-CNN and T-ATTN all capture the temporal signal well. In general, T-RNN gives slightly
better predictions on the Jena weather dataset.
5https://www.statsmodels.org/dev/vector_ar.html
28
Published as a conference paper at ICLR 2021
Figure A.5: The predictions of the T-RNN, T-CNN and T-ATTN approaches. We use the models
trained for Case 1 that predict the temperature 12 hours in the future. The plot is made by using
sliding windows. The timestamp reflects the hours.
0.7
data = Weather ∣ task = case-1
data = Weather ∣ task = case-2
data = Weather ∣ task = case-3
LltaIIblM
0.6
0.5
I 0.4
0.3
0.2
0.7
data = Wiki ∣ task = case-1
data = Wiki ∣ task = case-2
data = Wiki ∣ task = case-3
0.6
0.5
I 0.4
0.3
0.2
0.1

base-model
RNN
■ CNN
Attention
RFF-INN	RFF-Gaussian
time-method
RFF-INN	RFF-Gaussian
time-method
RFF-INN	RFF-Gaussian
time-method
Figure A.6: The ablation study on parameterizing the spectral density with Gaussian distribution, or
with INN.
B.5	Extra ablation study
We show the effectiveness of using INN to characterize the temporal kernel, compared with using
a known distribution family. We focus on the Gaussian distribution for illustration, where both the
mean and (diagonal of) the covariance are treated as free parameters. In short, we now parameterize
the spectral distribution as Gaussian instead of using INN. We denote this approach by NN-RF, to
differentiate with the temporal kernel approach T-NN.
First, we compare the results between T-NN and NN-RF on the time-series prediction tasks (shown in
Figure A.6). We see that T-NN uniformly outperforms NN-RF. The results for the recommendation
task is similar, as we show in Table A.3, where T-NN still achieves the best performance. The
numerical results are consistent with Theorem 1 where a more complex distribution family can lead
to better outcome. It also justifies our usage of the INN to parameterize the spectral distribution.
B.6	Sensitivity analysis
We conduct sensitivity analysis to reveal the stability of the proposed approaches with respect to
the model selections and our proposed learning schema. Specifically, we focus on the dimension
of the random features φ and the number of INN (affine coupling) blocks when parametrizing the
spectral distribution. The results for the time-series prediction tasks are given in Figure A.7, A.8
29
Published as a conference paper at ICLR 2021
Table A.3: The ablation study on parameterizing the spectral density with Gaussian distribution, or
with INN, when composing the recommendation models with temporal kernel. The reported are
the accuracy and discounted cumulative gain (DCG) for the domain-specific models on temporal
recommendation tasks.
	Alibaba	Walmart
Metric	Accuracy	DCG	Accuracy DCG
GRU-Rec-RF T-GRU-Rec	78.05/.22^^47.30/.13^^19.96/.15^^4.82/.21 79.47/.35 _ 4932/当 23.41/.1J_ 8A4/.21_
-CNN-ReC-RF	-75.23/735 — 44.60/.26- ^16.337.20^ -2774/：19 -
ECNN-Rec	76.45/.16 _ 46.55/.38_ J8.59Z.33_ _4.56/.31_
-ATTN-Rec-RF	'5230/747 — 31.51/755 — 22.737.41- -7：95/攵3 -
T-ATTN-Rec	53.49/.31	33.58/.30 2551L1 9.22L15
and A.11. The sensitivity analysis results for the recommendation task on Alibaba and Walmart.com
datasets are provided in Figure A.9 and A.10. From Table A.3 and A.2, we see that the ATTN-Rec
models do not perform well on the Aliababa dataset, and the CNN-Rec models do not perform well
on the Walmart.com dataset. Therefore, we do not provide sensitivty analysis for those models on the
corresponding dataset.
In general, the pattern is clear with respect to the dimension of φ, that the larger the dimension
(within the range we consider), the better the performance. The result is also reasonable, since a
larger dimension for φ may express more temporal information and better characterize the temporal
kernel On the other hand, the pattern for the number of INN (affine coupling layers) blocks is less
uniform, where in some cases too few INN blocks suffer from under-fitting, while in some other
cases too may INN blocks lead to over-fitting. Therefore, we would recommend using the number of
INN blocks as a hyperparameter, and keep the dimension of φ reasonably large.
30
Published as a conference paper at ICLR 2021
dimension
dimension
dimension
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
Figure A.7: Sensitivity analysis on the dimension of φγ for the Jena weather dataset. From top to
bottom are the results for Case 1, Case 2 and Case 3 respectively.
31
Published as a conference paper at ICLR 2021
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
dimension	dimension	dimension
model
T-RNN
RNN-trigo
T-CNN
CNN-trigo
T-Attention
Attention-trigo
Figure A.8: Sensitivity analysis on the dimension of φγ for the Wikipedia web traffic dataset. From
top to bottom are the results for Case 1, Case 2 and Case 3 respectively.

model
T-RNN-Rec
T-CNN-Rec
T-ATTN-Rec
Figure A.9: Sensitivity analysis on the number of INN (affine coupling) layers for the recommendation
task on Aliaba and Walmart.com datasets.
32
Published as a conference paper at ICLR 2021
74
2
8
0 8 6
8 7 7
A。。」FDuP
model
T-RNN-Rec
RNN-Rec-trigo
T-CNN-Rec
CNN-Rec-trigo
model
T-RNN-Rec
RNN-Rec-trigo
T-ATTN-Rec
ATTN-Rec-trigo
Figure A.10: Sensitivity analysis on the dimension of φγ for the recommendation task on
dataset upper panel and the Walmart.com dataset lower panel.
the Alibaba
0.34
0.32
0.30
0.28
0.26
0.24
0.22
model
T-RNN
T-CNN
T-Attention
4 2 0 8 6 4 2
3 3 3 2 2 2 2
CiS0.6 C5o.o.
①BE
task = case-3
ω
#layers
⅛layers
#layers
model
T-RNN
■ T-CNN
T-Attention
Figure A.11: Sensitivity analysis on the number of INN (affine coupling) layers. The upper panel
gives the results on the Jena weather dataset, the lower panel gives the results on the Wikipedia web
traffic dataset.
33