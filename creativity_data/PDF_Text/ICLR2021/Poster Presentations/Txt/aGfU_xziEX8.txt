Published as a conference paper at ICLR 2021
Efficient Inference of Flexible Interaction in
Spiking-neuron Networks
Feng Zhou*, Yixuan Zhangr Jun Zhu**
^Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch Joint ML Center, TsinghUa University
^Data Science Institute, University of Technology Sydney
{zhoufeng6288, dcszj}@tsinghua.edu.cn, yixuan.zhang@uts.edu.au
Ab stract
Hawkes process provides an effective statistical framework for analyzing the time-
dependent interaction of neuronal spiking activities. Although utilized in many
real applications, the classic Hawkes process is incapable of modelling inhibitory
interactions among neurons. Instead, the nonlinear Hawkes process allows for
a more flexible influence pattern with excitatory or inhibitory interactions. In
this paper, three sets of auxiliary latent variables (Polya-Gamma variables, la-
tent marked Poisson processes and sparsity variables) are augmented to make
functional connection weights in a Gaussian form, which allows for a simple it-
erative algorithm with analytical updates. As a result, an efficient expectation-
maximization (EM) algorithm is derived to obtain the maximum a posteriori
(MAP) estimate. We demonstrate the accuracy and efficiency performance of
our algorithm on synthetic and real data. For real neural recordings, we show
our algorithm can estimate the temporal dynamics of interaction and reveal the
interpretable functional connectivity underlying neural spike trains.
1	Introduction
One of the most important tracks in neuroscience is to examine the neuronal activity in the cere-
bral cortex under varying experimental conditions. Recordings of neuronal activity are represented
through a series of action potentials or spike trains. The transmitted information and functional
connection between neurons are considered to be primarily represented by spike trains (Kass et al.,
2014; Kass & Ventura, 2001; Brown et al., 2004; 2002). A spike train is a sequence of recorded
times at which a neuron fires an action potential and each spike may be considered to be a times-
tamp. Spikes occur irregularly both within and across multiple trials, so it is reasonable to consider
a spike train as a point process with the instantaneous firing rate being the intensity function of point
processes (Perkel et al., 1967; Paninski, 2004; Eden et al., 2004). An example of spike trains for
multiple neurons is shown in Fig. 2a in the real data experiment.
Despite many existing applications, the classic point process models, e.g., Poisson processes, ne-
glect the time-dependent interaction within one neuron and between multiple neurons, so fail to
capture the complex temporal dynamics of a neural population. In contrast, Hawkes process is one
type of point processes which is able to model the self-exciting interaction between past and future
events. Existing applications cover a wide range of domains including seismology (Ogata, 1998;
1999), criminology (Mohler et al., 2011; Lewis et al., 2012), financial engineering (Bacry et al.,
2015; Filimonov & Sornette, 2015) and epidemics (Saichev & Sornette, 2011; Rizoiu et al., 2018).
Unfortunately, due to the linearly additive intensity, the vanilla Hawkes process can only represent
the purely excitatory interaction because a negative firing rate may exist with inhibitory interac-
tion. This makes the vanilla version inappropriate in the neuroscience domain where the influence
between neurons is a mixture of excitation and inhibition (Maffei et al., 2004; Mongillo et al., 2018).
In order to reconcile Hawkes process with inhibition, various nonlinear Hawkes process variants
are proposed to allow for both excitatory and inhibitory interactions. The core point of nonlinear
Hawkes process is a nonlinearity which maps the convolution of the spike train with a causal influ-
ential kernel to a nonnegative conditional intensity, such as rectifier (Reynaud-Bouret et al., 2013),
* Corresponding author.
1
Published as a conference paper at ICLR 2021
exponential (Gerhard et al., 2017) and sigmoid (Linderman, 2016; Apostolopoulou et al., 2019). The
sigmoid mapping function has the advantage that the Polya-Gamma augmentation scheme can be
utilized to convert the likelihood into a Gaussian form, which makes the inference tractable. In Lin-
derman (2016), a discrete-time model is proposed to convert the likelihood from a Poisson process
to a Poisson distribution. Then Polya-Gamma random variables are augmented on discrete Obser-
vations to propose a Gibbs sampler. This method is further extended to a continuous-time regime
in Apostolopoulou et al. (2019) by augmenting thinned points and POlya-Gamma random variables
to propose a Gibbs sampler. However, the influence function is limited to be purely exciting or
inhibitive exponential decay. Besides, due to the nonconjugacy of the excitation parameter of expo-
nential decay influence function, a Metropolis-Hastings sampling step has to be embedded into the
Gibbs sampler making the Markov chain Monte Carlo (MCMC) algorithm further inefficient.
To address the parametric and inefficient problems in aforementioned existing works, we develop a
flexible sigmoid nonlinear multivariate Hawkes processes (SNMHP) model in the continuous-time
regime, (1) which can represent the flexible excitation-inhibition-mixture temporal dynamics among
the neural population, (2) with the efficient conjugate inference. An EM inference algorithm is
proposed to fit neural spike trains. Inspired by Donner & Opper (2017; 2018), three auxiliary latent
variable sets: POlya-Gamma variables, latent marked Poisson processes and sparsity variables are
augmented to make functional connection weights in a Gaussian form. As a result, the EM algorithm
has analytical updates with drastically improved efficiency. As shown in experiments, it is even more
efficient than the maximum likelihood estimation (MLE) for the parametric Hawkes process in high
dimensional cases.
2	Our Model
Neurons communicate with each other by action potentials (spikes) and chemical neurotransmitters.
A spike causes the pre-synaptic neuron to release a chemical neurotransmitter that induces impulse
responses, either exciting or inhibiting the post-synaptic neuron from firing its own spikes. The ad-
dition of excitatory and inhibitory influence to a neuron determines whether a spike will occur. At
the same time, the impulse response characterizes the temporal dynamics of the exciting or inhibit-
ing influence which can be complex and flexible (Purves et al., 2014; Squire et al., 2012; Bassett
& Sporns, 2017). Arguably, the flexible nonlinear multivariate Hawkes processes are a suitable
choice for representing the temporal dynamics of mutually excitatory or inhibitory interactions and
functional connectivity of neuron networks.
2.1	Multivariate Hawkes Processes
The vanilla multivariate Hawkes processes (Hawkes, 1971) are sequences of timestamps D =
{{tin}nN=i 1}iM=1 ∈ [0, T ] where tin is the timestamp of n-th event on i-th dimension with Ni being
the number of points on i-th dimension, M the number of dimensions, T the observation window.
The i-th dimensional conditional intensity, the probability of an event occurring on i-th dimension
in [t, t + dt) given all dimensional history before t, is designed in a linear superposition form:
M
λi (t) = μi +ΣΣφij(t -tjn),
j=1 tjn <t
(1)
where μ% > 0 is the baseline rate of i-th dimension and φj(∙) ≥ 0 is the causal influence function
(impulse response) from j -th dimension to i-th dimension which is normally a parameterized func-
tion, e.g., exponential decay. The summation explains the self- and mutual-excitation phenomenon,
i.e., the occurrence of previous events increases the intensity of events in the future. Unfortunately,
one blemish is the vanilla multivariate Hawkes processes allow only nonnegative (excitatory) in-
fluence functions because negative (inhibitory) influence functions may yield a negative intensity
which is meaningless. To reconcile the vanilla version with inhibitory effect and flexible influence
function, we propose the SNMHP.
2
Published as a conference paper at ICLR 2021
2.2	Sigmoid Nonlinear Multivariate Hawkes Processes
Similar to the classic nonlinear multivariate HaWkes processes (BremaUd & Massoulie, 1996), the
i-th dimensional conditional intensity of SNMHP is defined as
M
λi (t) = λiσ(hi(t)),	hi(t) = μi +ΣΣφij (t - tjn),
j=1 tjn <t
(2)
where μ% is the base activation of neuron i, h，(t) is a real-valued activation and σ(∙) is the logistic
(sigmoid) function which maps the activation into a positive real value in (0,1) with λ% being a
upper-bound to scale it to (0, λii). The sigmoid function is chosen because as seen later, the Polya-
Gamma augmentation scheme can be utilized to make the inference tractable. After incorporating
the nonlinearity, it is straightforward to see the influence functions, φj(∙), can be positive or neg-
ative. If φij(∙) is negative, the superposition of φj(∙) will lead to a negative activation h，(t) that
renders the intensity to 0; instead, the intensity tends to λ% with a positive φj(∙).
To achieve a flexible impulse response, the influence function is assumed to be a weighted sum of
basis functions
φij (∙) =〉： wijbφbθ ,
(3)
b=1
where {φb}b3 * * * * B=1 are predefined basis functions and wijb is the weight capturing the influence from
j-th dimension to i-th dimension by b-th basis function with positive indicating excitation and neg-
ative indicating inhibition. The basis functions are nonnegative functions capturing the temporal
dynamics of the interaction. Although basis functions can be in any form, in order for the weights to
represent functional connection strength, basis functions are chosen to be probability densities with
compact support that means they have bounded support [0, Tφ] and the integral is one. As a result,
the i-th dimensional activation is
M
B
hi。=，，+∑∑ EwijbΦb(t- tn)
j=1 tjn <t b=1
MB
MB
μ, + ∑ EwijbE Φb(t- %)
(4)
=μi +EEwijbφjb ⑴=wT ∙φ⑴，
j=1 b=1
whereΦjb(t) is the convolution of j-th dimensional observation with b-th basis function and can
be precomputed; Wi = [μi, wm,..., w，mb ]t and Φ(t) = [1, Φ11(t),..., Φmb (t)]τ, both are
(M B + 1) × 1 vectors. A similar model is used in Linderman (2016) where a binary variable is
included to characterize the sparsity of functional connection. As shown later, the sparsity in our
model is guaranteed by utilizing a Laplace prior on weight instead.
In this paper, the basis functions are scaled (shifted) Beta densities, but alternatives such as Gaussian
or Gamma also can be used. The reason we choose Beta distribution is the inference of weights will
be subject to edge effects with infinite support densities when close to the endpoints of [0, Tφ]. The
weighted sum of Beta densities is a natural choice. With appropriate mixing, it can be used to
approximate functions on bounded intervals arbitrarily well (Kottas, 2006).
3 Inference
The likelihood of a point process model is provided in Daley & Vere-Jones (2003). Correspondingly,
the probability density (likelihood) of SNMHP on the i-th dimension as a function of parameters in
continuous time is
_	Ni _
p(D∣Wi,λi)= ∏ 卫iσ(hi(Pn)) exp
(5)
λiσ(hi(t))dt J .
n=1
It is worth noting that hi(t) depends on Wi and observations on all dimensions. Our goal is to infer
the parameters i.e., weights and intensity upper-bounds, from observations, e.g., neural spike trains,
3
Published as a conference paper at ICLR 2021
over a time interval [0, T]. The functional connectivity in cortical circuits is demonstrated to be
sparse in neuroscience (Thomson & Bannister, 2003; Sjostrom et al., 2001). To include sparsity,
a factorizing Laplace prior is applied on the weights which characterize the functional connection.
With the likelihood Eq. 5 and LaPIace prior PL(Wi) = Qjb * exp (- lwijbl), the log-posterior
corresponds to a L1 penalized log-likelihood. The i-th dimensional MAP estimate can be expressed
as
Wi,λ- = argmax {log p(D∣Wi,λi) + log PL(Wi)},	(6)
一	, 一 T-*
where w* and λi are MAP estimates. The dependency of the log-posterior on parameters is com-
plex because the sigmoid function exists in the log-likelihood term and the absolute value function
exists in the log-prior term. As a result, we have no closed-form solutions for the MAP estimates.
Numerical optimization methods can be applied, but unfortunately, the efficiency is low due to the
high dimensionality of parameters which is (MB + 2) × M. To circumvent this issue, three sets
of auxiliary latent variables: Polya-Gamma variables, latent marked Poisson processes and sparsity
variables are augmented to make the weights appear in a Gaussian form in the posterior. As a result,
an efficient EM algorithm with analytical updates is derived to obtain the MAP estimate.
3.1	Augmentation OF POLYA-GAMMA Variables
Following Polson et al. (2013), the binomial likelihoods parametrized by log odds can be represented
as mixtures of Gaussians w.r.t. a Polya-Gamma distribution. Therefore, We can define a Gaussian
representation of the sigmoid function
∞
σ(z) =
0
e"ω,Z)PPG(ω∣1, 0)dω,
(7)
where f (ω,z) = z∕2-z2ω∕2-log2 and pPG(ω∣1,0) is the Polya-Gamma distribution with ω ∈ R+.
Substituting Eq. 7 into the likelihood Eq. 5, the products of observations σ(hi(tin)) are transformed
into a Gaussian form.
3.2	Augmentation of Marked Poisson Processes
inspired by Donner & Opper (2018), a latent marked Poisson process is augmented to linearize
the exponential integral term in the likelihood. Applying the property of sigmoid function σ(z) =
1 - σ(-z) and Eq.7, the exponential integral term is transformed to
exp (— Z λiσ(hi(t))dt) = exp (— Z Z (1 — e"ω,-hMt))) λiPPG(ω∣1,0)dωdt) . (8)
The right hand side is a characteristic functional of a marked Poisson process. According to the
Campbell’s theorem (Kingman, 2005) (App. i), the exponential integral term can be rewritten as
exp
λiσ(hi(t))dt j = Epλ.
π
(ω,t)∈Π.
ef (ω,-h. (t))
(9)
where Πi = {(ωki , tik)}kK=. 1 denotes a realization ofa marked Poisson process and Pλ. is the proba-
bility measure of the marked Poisson process ∏i with intensity λi(t,ω) = λiPPG(ω∣1,0). The events
{tk }K= ι follow a Poisson process with rate λ% and the latent Polya-Gamma variable ωii denotes the
independent mark at each location tik . We can see that, after substituting Eq. 9 into the likelihood
Eq. 5, the exponential integral term is also transformed into a Gaussian form.
3.3	Augmentation of Sparsity Variables
The augmentation of two auxiliary latent variables above makes the augmented likelihood become
a Gaussian form w.r.t. the weights. However, the absolute value in the exponent of the Laplace
prior hampers the Gaussian form of weights in the posterior. To circumvent this issue, we augment
the third set of auxiliary latent variables: sparsity variables. it has been proved that a Laplace
4
Published as a conference paper at ICLR 2021
distribution can be represented as an infinite mixture of Gaussians (Donner & Opper, 2017; Pontil
et al., 2000)
pL(wijb) = 2α exp (-Iwj ) = ZQ r 2α exp (- 2θb wijt^p(βijb)dβijb,	(IO)
where p(βijb) = (βjb∕2)-2 exp (-1∕(2βjb)). It is straightforward to see the weights are trans-
formed into a Gaussian form in the prior after the augmentation of latent sparsity variables β.
3.4	Augmented Likelihood and Prior
After the augmentation of three sets of latent variables, we obtain the augmented joint likelihood
and prior (derivation in App. II)
Ni
p(D,∏i, ωi∣Wi,λi)= Y [λi(tn,ωn)ef(ωn，hi(tn))] ∙ p^Cni向	Y ef(ω,-hi(t)),	(11a)
n=1	(ω,t)∈Πi
Pwiβi)= Y ri⅛exp (-βabw2jb) (β2b) exp (-2⅛),	(IIb)
where ωi is the vector of ωni on each tin, βi is a CMB + 1) × 1 vector of [βi00, βi11, . . . , βiM B]T,
λi(tn, ωn) = λippG(ωn∣1,0). The motivation of augmenting auxiliary latent variables should now
be clear: the augmented likelihood and prior contain the weights in a Gaussian form, which corre-
sponds to a quadratic expression for the log-posterior (L1 penalized log-likelihood).
3.5	EM Algorithm
The original MAP estimate has been represented by Eq. 6. With the support of auxiliary latent
variables, we propose an analytical EM algorithm to obtain the MAP estimate instead of perform-
ing numerical optimization. In the standard EM algorithm framework, the lower-bound (surrogate
function) of the log-posterior can be represented as
Q(wi,λi∣wsτ,λsT) = E∏i,ωi [logp(D,∏i, ωi∣Wi,λi)] + EeJogp(wi, βi)], (12)
with expectation over posterior distributions p(∏i,ωi∣wi-1,λi 1) and p(βi∣wS-1,λs 1), S 一 1
indicating parameters from last iteration.
E step: Based on joint distributions in Eq. 11, the posterior of latent variables can be derived. The
detailed derivation is provided in App. III. The posterior distributions of Polya-Gamma variables ωi
and sparsity variables βi, and the posterior intensity of marked Poisson process ni are
Ni
p(ωi∣wis-1) = Y pPG(ωni ∣1,his-1(tin)),	(13a)
n=1
Λi(t,ω∣wS-1 ,λS-1)= λS-1σ(-hS-1(t))pPG(ω∣1,hS-1(t)),	(13b)
M B+1
p(βi∣ws-1) = Y PIG(βijb∣^-T, 1),	(13c)
j,b	wijb
where Λi (t, ω) is the posterior intensity of ni, pIG is the inverse Gaussian distribution.
It is worth noting that his-1(t) depends on wis-1. The first order moments, E[ωni ] =
1∕(2his-1(tin)) tanh(his-1(tin)∕2) and E[βijb] = α∕wisj-b1, will be used in the M step.
M step: Substituting Eq. 13 into Eq. 12, we obtain the lower-bound Q(wi, λi∣wS-1, λs ). The
updated parameters can be obtained by maximizing the lower-bound. The detailed derivation is
provided in App. III. Due to the augmentation of auxiliary latent variables, the update of parameters
has a closed-form solution
λS = (Ni + Ki) ∕T,	(14a)
wis = ΣiZT Bi(t)Φ(t)dt,
0
(14b)
5
Published as a conference paper at ICLR 2021
where Ki = RT R∞ Ai(t,ω∣wS-1,λs 1)dωdt, Σi = [/； Ai(t)Φ(t)ΦT(t)dt + diag (α-2E[βi])]
with diag(∙) indicating the diagonal matrix of a vector, Ai(t) = PnN= 1 E[ωη]δ(t - tin) +
R∞ ωΛi(t,ω)dω, Bi(t) = 2 PnN= 1 δ(t - tin) - ɪ R∞ Λi(t,ω)dω with δ(∙) being the Dirac delta
function. It is worth noting that numerical quadrature methods, e.g., Gaussian quadrature, need to
be applied to intractable integrals above.
3.6	Complexity and Hyperparameters
The complexity of our proposed EM algorithm
is O(NNTφB+L(N(MB+1)2 +M(MB+
1)3 )) where N is the number of observations
on all dimensions, NTφ is the the average num-
ber of observations on the support of Tφ on all
dimensions and L is the number of iterations.
The first term is due to the convolution nature
of Hawkes process, the second and third term to
the matrix multiplication and inversion in EM
iterations. For one application, the number of
dimensions M and basis functions B are fixed
and much less than N. Therefore, the complex-
ity can be simplified as O(N (NTφ + L)).
Algorithm 1: EM inference for SNMHP
Result： {λi(t) = λiσ(wT ∙ Φ(t))}Mι
Predefine basis functions {φb(∙)}B=ι;
Initialize the hyperparameter α and {λi, Wi,
ωi, Πi, βi}iM=1;
for Iteration do
for Dimension i do
Update the posterior ofωi by Eq. 13a;
Update the posterior intensity of Πi
by Eq. 13b;
Update the posterior ofβi by Eq. 13c;
Update the intensity upper-bound λ%
by Eq. 14a;
Update the weights wi by Eq. 14b.
end
Update the hyperparameter α.
The hyperparameter α in Laplace prior that en-
codes the sparsity of weights and parameters of
basis functions can be chosen by cross valida-
tion or maximizing the lower-bound Q using
numerical methods. For the number of basis end
functions: in essence, a large number leads to a
more flexible functional space while a small number results in a faster inference. In experiments,
we gradually increase it until no more significant improvement. Similarly, the number of quadra-
ture nodes and EM iterations is also gradually increased until a suitable value. The pseudocode is
provided in Alg. 1.
4 Experiments
We validate the EM algorithm for SNMHP in analyzing both synthetic and real-world spike data
collected from the cat primary visual cortex. For comparison, the following most relevant baselines
are considered: (1) parametric linear multivariate Hawkes processes that are vanilla multivariate
Hawkes processes with exponential decay influence functions, for which the inference is performed
by MLE (Ozaki, 1979); (2) nonparametric linear multivariate Hawkes processes with flexible influ-
ence functions, for which the inference is by majorization minimization Euler-Lagrange (MMEL)
(Zhou et al., 2013); (3) parametric nonlinear multivariate Hawkes processes with exponential de-
cay influence functions, for which the inference is by MCMC based on augmentation and Poisson
thinning (MCMC-Aug) (Apostolopoulou et al., 2019). The implementation of our model is publicly
available at https://github.com/zhoufeng6288/SNMHawkesBeta.
4.1	Synthetic Data
We analyze spike trains obtained from the synthetic network model shown in Fig. 1a. The synthetic
neural network contains four groups of two neurons each. In each group, the 2 neurons are self-
exciting and mutual-inhibitive while groups are independent of each other. We assume 4 scaled
(shifted) Beta distributions as basis functions with support [0, Tφ = 6] in Fig. 1b. For the ground
truth, it is assumed that φ11 = φ33 = φ55 = φ77 = φ1, φ22 = φ44 = φ66 = φ88 = φ4,
φi2 = φ34 = φ56 = φ78 = -2Φ2, Φ21 = φ43 = φ65 = φ87 = - 1 φ3 with positive indicating
excitation and negative indicating inhibition. With base activation {μi}8=1 = 0 and upper-bounds
{λi}8=ι = 5, We use the thinning algorithm (Ogata, 1998) to generate two sets of synthetic spike
6
Published as a conference paper at ICLR 2021
8βs½0<
Basic Funclcms
ami ∙≠ra* rf O IWTBW
ι⅛ww≡n≡⅛
nrπwιwymrτ
广州 W5,TΛr∏CT
Influence tncOore of Neuron-1
Sesf-
(c)	(d)
Influence ⅛nctlcms of NeUrCm-2
Functional Connectivity: Ground Truth Functional Connectivity: Estimation
Neuron j	Neuron i
-xδ⅝⅝ou
-xxfis⅞□
TMdιe<ff between Accuecy end
,一— tut IcaIuINxxi
w∕^	---- 3EM“g
nodes
一所Hgglcβllwd
——IMtlcaIuINxxi
---rumkιg 0m9
# Ofbese ftj∏ςtkwιs
2 SSH Bl⅜lsα:


WM≡2T 二 H

(e)	(f)	(g)	(h)
Figure 1: The synthetic network model and experimental results. (a): The synthetic neural popu-
lation contains 4 independent groups. In each group, the interdependencies between 2 neurons are
self-exciting and mutual-inhibitive with red arrows indicating excitation and blue arrows indicating
inhibition. (b): Four scaled (shifted) Beta densities as basis functions on the support of [0,6]. (c):
The intensities and spike times of 8 neurons in the synthetic data. (d): The estimated influence
functions of 1-st and 2-nd neurons where the estimated φιι, φ12, Φ21,Φ22 are close to the ground
truth, the other ground truth φ13...i8 and φ23...28 are not labeled since they are all zero (GT=Ground
Truth). (e): The heat map of functional connectivity among neural population with ground truth
(left) and estimation (right). (f): The training and test log-likelihood curve w.r.t. EM iterations. (g):
The trade-off between accuracy and efficiency w.r.t. # of quadrature nodes and basis functions for
synthetic data. (h): The running time of 2D data for EM algorithm and alternatives w.r.t. the average
observation number on each dimension (the precomputation of Φ(t) is included).
data on the time window [0, T = 1000] with one being the training dataset in Fig. 1c and the
other one test dataset in App. IV. Each dataset contains 8 sequences and each sequence consists
of 3340 events on average. We aim to identify the functional connectivity of the neural population
and the temporal dynamics of influence functions from statistically dependent spike trains. More
experimental details, e.g., hyperparameters, are given in the App. IV.
The temporal dynamics of interactions among the neural population is shown in Fig. 1d where we
plot the estimated influence functions of 1-st and 2-nd neurons (other neurons are shown in the
A	ɪɪ τ∖ El	, ∙	, 1	1 7	1 ∙1 ∙ , , 1	1 i'	∙ . ∙	1	∙ , 1 ?	F? 1	,
App. IV). The estimated φ11 and φ22 exhibit the self-exciting relation with φ12 and φ21 character-
izing the mutual-inhibitive interactions. All estimated influence functions are in a flexible form and
close to the ground truth. Besides, as shown in Fig. 1e, the estimated functional connectivity re-
covers the ground-truth structure successfully. The functional connectivity is defined as J ∣φj(t)∣dt
meaning there is no connection only if neither excitation nor inhibition exists.
The training and test log-likelihood (LOgL) CUrVeS w.r.t. Table 1: Training/test LogL (×103) of				
EM iterations are shown in Fig. 1f where our EM al- gorithm converges fast with only 50 iterations needed to obtain a plateau. The trade-off between accuracy (LogL) and efficiency (running time) w.r.t. the num-	different models for synthetic data.			
	I MLE	MMEL MCMC-Aug		EM
	Training LogL ∣ 2.051	1.993	2.199	2.465
ber of quadrature nodes and basis functions is shown in	Test LogL j 1.866	1.843	2.278	2.373
Fig. 1g where we can see the accuracy is not sensitive to
the number of quadrature nodes over 100 and the optimal number of basis functions is 4. A larger
number does not significantly improve the accuracy but leads to a longer running time. Moreover,
we compare the running time of our method with alternatives in Fig. 1h where the number of dimen-
sions M is fixed to 2, basis functions B to 4, quadrature nodes to 200 and iterations of all methods
to 200. We can observe that our EM algorithm is the most efficient, even superior to MLE for the
classic parametric case, which verifies its efficiency. Also, we compare our model,s fitting and pre-
7
Published as a conference paper at ICLR 2021
sM232221209 3 7 664 32 10987-
SUoJneN
Co-15
influence function Φ⅛
Infkience ħjπctlon≠⅛<
O
Uq<s≤av
,9
UOl。4
Infkience Iunctlon Φ⅛β
7⅛ (XlQQms)
Functional Connectivity： Estimabon
54321nv0∙87β-o + 32 1o987β54321
一 UOJnaf5J
Neuron i
ReaWata Loflllkellhood CUrVe
POOm=BO-I
7⅛ (XlQOms) 7⅛ (XlQOms)
(a)	(b)	(c)
(d)
UOAeMOq
Figure 2: The real data experimental results. (a): The training spike trains extracted from real data
(test spike trains in App. IV). (b): The estimated influence functions between 8-th and 9-th neurons.
(c): The heat map of estimated functional connectivity among 25 neurons. (d): The training and test
LogL curves w.r.t. EM iterations.
diction ability with baseline models for 1-st and 2-nd neurons. Training and test LogL are shown
in Tab. 1 where our SNMHP with EM inference is the champion due to its superior generalized
expressive ability.
4.2 Real Data
In this section, we analyze our model performance on a real multi-neuron spike train dataset. We aim
to draw some conclusions about the functional connectivity of cortical circuits and make inferences
of the temporal dynamics of influence.
Spike Train Data (Blanche, 2005; Apostolopoulou et al., 2019) Several multi-channel silicon elec-
trode arrays are designed to record simultaneously spontaneous neural activity of multiple isolated
single units in anesthetized paralyzed cat primary visual cortex areas 17. The spike train dataset
contains spike times of 25 simultaneously recorded neurons.
Preliminary Setup We extract the spike times in the time window [0, 300] (time unit: 100ms, the
same applies to the following) as the training data (Fig. 2a) and [300, 600] as the test data (App. IV).
Both datasets contain approximate 7000 timestamps. All hyperparameters are fine tuned to obtain
the maximum test LogL: the scaled (shifted) Beta distribution Beta(α = 50,β = 50, shift = -5)
with support [0, Tφ = 10] is designed as the basis function; the number of quadrature nodes is set to
1000 and EM iterations to 100. More experimental details, e.g., hyperparameters, are given in the
App. IV.
Results 25 × 25 influence functions among the neuron population are estimated in the application.
An example of the influence functions between 8-th and 9-th neurons are plotted in Fig. 2b where
our SNMHP model successfully captures the exciting or inhibitive interaction between neurons. Be-
sides, the estimated functional connectivity is shown in Fig. 2c where we can see the functional
connection structure among neural population is sparse. Unfortunately, because the ground-truth
functional connectivity of cortical circuits is unknown, the estimated functional connectivity cannot
be compared with the ground truth but here we resort to the test LogL to verify whether the estima-
tion is good. The training and test LogL curves are shown in Fig. 2d where they both reach a close
plateau indicating the estimation is appropriate without overfitting or underfitting.
A significant advantage of our EM algorithm is
the efficiency. The 25-dimensional observation in
the real data is a challenge for the inference. For
the running time, our EM algorithm costs 3 min-
utes, the MCMC-Aug costs 1 hour and 45 minutes
with the same number of iterations while MLE and
MMEL cannot finish in 2 days due to the curse of
dimensionality. Moreover, the fitting and prediction
Table 2: Training/test LogL (×103) and run-
ning times of different models for real data.
	I MLE	MMEL	MCMC-Aug	EM
Training LogL	I -	-	-15.328	-5.519
Test LogL	I -	-	-6.133	-5.862
Running Time	> 2 days	> 2 days	1h 45m	3m
ability is compared in Tab. 2. The superior performance of SNMHP w.r.t. training and test LogL
8
Published as a conference paper at ICLR 2021
demonstrates our model can capture the complex mixture of exciting and inhibitive interactions
among neural population which leads to better goodness-of-fit.
5 Discussion and Conclusion
Although we propose a point-estimation method (EM algorithm) in this work, a straightforward
extension to Gibbs sampler is already at hand. Based on the augmented likelihood and prior, we can
obtain the conditional densities of latent variables and parameters in closed form, which constitutes
a Gibbs sampler with better efficiency than MCMC-Aug since the time-consuming Metropolis-
Hasting sampling in MCMC-Aug is not needed. However, the proposed Gibbs sampler is less
efficient than the proposed EM algorithm because the latent Poisson processes have to be sampled
by thinning algorithm in Gibbs sampler which is time consuming. For the model in Apostolopoulou
et al. (2019), a tighter intensity upper-bound is used to reduce the number of thinned points to
accelerate the sampler. Instead, our EM algorithm does not encounter this problem as we compute
the expectation rather than sampling. Moreover, Apostolopoulou et al. (2019) can only use one
basis function, which limits influence functions to be purely exciting or inhibitive exponential decay.
Instead, our model can utilize multiple basis functions to characterize an influence function that is a
mixture of excitation and inhibition.
In this paper, we develop a SNMHP model in the continuous-time regime which can characterize
excitation-inhibition-mixture temporal dependencies among the neural population. Three auxiliary
latent variables are augmented to make the corresponding EM algorithm in a closed form to improve
efficiency. The synthetic and real data experimental results confirm that our model’s accuracy and
efficiency are superior to the state of the arts. From the application perspective, although our model
is proposed in the neuroscience domain, it can be applied to other applications where the inhibition
is a vital factor, e.g., in the coronavirus (COVID-19) spread, the inhibitive effect may represent the
medical treatment or cure, or the forced isolation by government. From the inference perspective,
our EM algorithm is a point-estimation method; other efficient distribution-estimation methods can
be developed, e.g., the Gibbs sampler mentioned above or the mean-field variational inference.
Acknowledgments
The authors would like to thank the anonymous reviewers for insightful comments which greatly im-
proved the paper. This work was supported by NSFC Projects (Nos. 62061136001, 61620106010),
Beijing NSF Project (No. JQ19016), Beijing Academy of Artificial Intelligence (BAAI), Tsinghua-
Huawei Joint Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute
for Intelligent Computing, and the NVIDIA NVAIL Program with GPU/DGX Acceleration. F. Zhou
was partially funded by China Postdoctoral Science Foundation.
References
Ifigeneia Apostolopoulou, Scott Linderman, Kyle Miller, and Artur Dubrawski. Mutually regressive
point processes. In Advances in Neural Information Processing Systems, pp. 5116-5127, 2019.
Emmanuel Bacry, Iacopo Mastromatteo, and Jean-Francois Muzy. HaWkes processes in finance.
Market Microstructure and Liquidity, 1(01):1550005, 2015.
Danielle S Bassett and Olaf Sporns. NetWork neuroscience. Nature neuroscience, 20(3):353, 2017.
Tim Blanche. The neural data Was recorded by Tim Blanche in the laboratory of Nicholas SWindale,
University of British Columbia, and doWnloaded from the NSF-funded CRCNS Data Sharing
Website., 2005.
Pierre Bremaud and Laurent Massoulie. Stability of nonlinear hawkes processes. The AnnalS of
Probability, pp. 1563-1588, 1996.
Emery N Brown, Riccardo Barbieri, Valerie Ventura, Robert E Kass, and Loren M Frank. The time-
rescaling theorem and its application to neural spike train data analysis. Neural computation, 14
(2):325-346, 2002.
9
Published as a conference paper at ICLR 2021
Emery N Brown, Robert E Kass, and Partha P Mitra. Multiple neural spike train data analysis:
state-of-the-art and future challenges. Nature neuroscience, 7(5):456-461, 2004.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes. vol. i. prob-
ability and its applications, 2003.
Christian Donner and Manfred Opper. Inverse Ising problem in continuous time: A latent variable
approach. Physical Review E, 96(6):062104, 2017.
Christian Donner and Manfred Opper. Efficient Bayesian inference of sigmoidal Gaussian Cox
processes. The Journal of Machine Learning Research, 19(1):2710-2743, 2018.
Uri T Eden, Loren M Frank, Riccardo Barbieri, Victor Solo, and Emery N Brown. Dynamic analysis
of neural encoding by point process adaptive filtering. Neural computation, 16(5):971-998, 2004.
Vladimir Filimonov and Didier Sornette. Apparent criticality and calibration issues in the Hawkes
self-excited point process model: application to high-frequency financial data. Quantitative Fi-
nance, 15(8):1293-1314, 2015.
Felipe Gerhard, Moritz Deger, and Wilson Truccolo. On the stability and dynamics of stochastic
spiking neuron models: Nonlinear Hawkes process and point process GLMs. PLoS computational
biology, 13(2), 2017.
Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika,
58(1):83-90, 1971.
Robert E Kass and Valerie Ventura. A spike-train probability model. Neural computation, 13(8):
1713-1720, 2001.
Robert E Kass, Uri T Eden, and Emery N Brown. Analysis of neural data, volume 491. Springer,
2014.
John Frank Charles Kingman. Poisson processes. Encyclopedia of biostatistics, 6, 2005.
Athanasios Kottas. Dirichlet process mixtures of Beta distributions, with applications to density
and intensity estimation. In Workshop on Learning with Nonparametric Bayesian Methods, 23rd
International Conference on Machine Learning (ICML), volume 47, 2006.
Erik Lewis, George Mohler, P Jeffrey Brantingham, and Andrea L Bertozzi. Self-exciting point
process models of civilian deaths in Iraq. Security Journal, 25(3):244-264, 2012.
Scott Warren Linderman. Bayesian Methods for Discovering Structure in Neural Spike Trains. PhD
thesis, Harvard University, 2016.
Arianna Maffei, Sacha B Nelson, and Gina G Turrigiano. Selective reconfiguration of layer 4 visual
cortical circuitry by visual deprivation. Nature neuroscience, 7(12):1353-1359, 2004.
George O Mohler, Martin B Short, P Jeffrey Brantingham, Frederic Paik Schoenberg, and George E
Tita. Self-exciting point process modeling of crime. Journal of the American Statistical Associa-
tion, 106(493):100-108, 2011.
Gianluigi Mongillo, Simon Rumpel, and Yonatan Loewenstein. Inhibitory connectivity defines the
realm of excitatory plasticity. Nature neuroscience, 21(10):1463-1470, 2018.
Yosihiko Ogata. Space-time point-process models for earthquake occurrences. Annals of the Insti-
tute of Statistical Mathematics, 50(2):379-402, 1998.
Yosihiko Ogata. Seismicity analysis through point-process modeling: A review. In Seismicity pat-
terns, their statistical significance and physical meaning, pp. 471-507. Springer, 1999.
Tohru Ozaki. Maximum likelihood estimation of Hawkes’ self-exciting point processes. Annals of
the Institute of Statistical Mathematics, 31(1):145-155, 1979.
Liam Paninski. Maximum likelihood estimation of cascade point-process neural encoding models.
Network: Computation in Neural Systems, 15(4):243-262, 2004.
10
Published as a conference paper at ICLR 2021
Donald H Perkel, George L Gerstein, and George P Moore. Neuronal spike trains and stochastic
point processes: II. Simultaneous spike trains. Biophysical journal, 7(4):419-440, 1967.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using
Polya-Gamma latent variables. Journal of the American statistical Association, 108(504):1339-
1349, 2013.
Massimiliano Pontil, Sayan Mukherjee, and Federico Girosi. On the noise model of support vector
machines regression. In International Conference on Algorithmic Learning Theory, pp. 316-324.
Springer, 2000.
Dale Purves, George J Augustine, David Fitzpatrick, WC Hall, AS LaMantia, JO McNamara, and
L White. Neuroscience, 2008. De Boeck, Sinauer, Sunderland, Mass, pp. 15-16, 2014.
Patricia Reynaud-Bouret, Vincent Rivoirard, and Christine Tuleau-Malot. Inference of functional
connectivity in neurosciences via Hawkes processes. In 2013 IEEE Global Conference on Signal
and Information Processing, pp. 317-320. IEEE, 2013.
Marian-Andrei Rizoiu, Swapnil Mishra, Quyu Kong, Mark Carman, and Lexing Xie. SIR-Hawkes:
linking epidemic models and Hawkes processes to model diffusions in finite populations. In
Proceedings of the 2018 World Wide Web Conference, pp. 419-428, 2018.
AI Saichev and Didier Sornette. Generating functions and stability study of multivariate self-excited
epidemic processes. The European Physical Journal B, 83(2):271, 2011.
Per Jesper Sjostrom, Gina G Turrigiano, and Sacha B Nelson. Rate, timing, and cooperativity jointly
determine cortical synaptic plasticity. Neuron, 32(6):1149-1164, 2001.
Larry Squire, Darwin Berg, Floyd E Bloom, Sascha Du Lac, Anirvan Ghosh, and Nicholas C Spitzer.
Fundamental neuroscience. Academic Press, 2012.
Alex M Thomson andA Peter Bannister. Interlaminar connections in the neocortex. Cerebral cortex,
13(1):5-14, 2003.
Ke Zhou, Hongyuan Zha, and Le Song. Learning triggering kernels for multi-dimensional Hawkes
processes. In International Conference on Machine Learning, pp. 1301-1309, 2013.
11
Published as a conference paper at ICLR 2021
APPENDIX
I Campbell’ s Theorem
Let Πz^ = {(zn, ωn)}N=ι be a marked Poisson process on the product space Z = Z × Ω with
intensity Λ(z, ω) = Λ(z)p(ω∣z). A(Z) is the intensity for the unmarked Poisson process {zn}N=ι
with ωn 〜p(ωn∣Zn) being an independent mark drawn at each Zn. Furthermore, We define a
function h(z, ω) : ZX Ω → R and the sum H(∏z) = P(Z ω)∈π玄 h(z, ω). If A(z, ω) < ∞, then
EnZleXP (ξH(∏z))] = exp
eξh(Z,ω) - 1 A(Z, ω)dωdZ ,
for any ξ ∈ C. The above equation defines the characteristic functional of a marked Poisson process.
This proves Eq.9 in the main paper. The mean is
[H (nZ )1」
EnZ
h(Z, ω)A(Z, ω)dωdZ,
which is used when substituting Eq. 13 into Eq. 12.
II Derivation of Augmented Likelihood and Prior
Substituting Eq.7 and 9 into Eq.5 in the main paper, the augmented likelihood is obtained
p(D∣Wi,λi) = Y λiσ(hi(tn))exp (— Z λiσ(hi(t))dt)
Ni ∞
=Y	λief(ωn,hi(tn))pPG(ωn |1, 0)dωn ∙ Ep%	Y ef®")
n=1	0	(ω,t)∈ni
Ni
Y 卜 i(tn,ωn )ef (ωn ,hi(tn))i ∙ Pλi(∏i∣λi)	Y	ef(ω,-hi(t))dωidΠi.
n=1	(ω,t)∈ni
where ωi is the vector of ωn and λi(t7n,ωnn) = λiPPG(ω7n|1,0). It is straightforward to see the
augmented likelihood is
Ni
p(D,∏i, ωi∣Wi,λi) = Y [λi(tn,ωn )ef (ωn&(%1. p^ (∏i∣λi) Y	ef®"),
n=1	(ω,t)∈ni
which is Eq.11a.
Similarly, the integrand in Eq. 10 is just the augmented prior in Eq. 11b.
III Derivation of EM Algorithm
In the standard EM algorithm framework, the lower-bound of log-posterior has been provided in
Eq. 12. The posterior of latent variables can be derived from the joint distribution in Eq. 11. The
derivation is relatively easy for ωi and βi while Πi is difficult. In the following, s — 1 and s mean
the last and current iteration in the EM algorithm.
E Step
1.	The posterior of POlya-Gamma variables ωi is dependent on the activation hS-1(t) at {tin}nN=i1,
which is further dependent on wis-1 through Eq. 4
Ni
P(ωiWS-1)= Y PPG(ωn ∣1,hS-1(tn)),
n=1
12
Published as a conference paper at ICLR 2021
where We utilize the tilted Polya-Gamma density PPG(ω∣b, C) 8 e-c2ωSppG(ω∣b, 0) (Polson et al.,
2013).
2.	The posterior of sparsity variables βi is an inverse Gaussian distribution which is dependent on
weights wis-1
MB+1
p(βi|ws-1) = Y PIGCeijbI-S-T, 1).
j,b	wijb
3.	The posterior of ∏i is dependent on both hi-1(t) and λs 1
PMKT)= R : ( ∏g]Qref[I：Ii.
The Campbell’s theorem can be applied to convert the denominator, the equation above can be
transformed as
PCni∣wS-1,λs-1)
PIi C∏i∣λs-1)Q(ω,t)∈∏ie"ω,-hs-1M
exp (-RR(I-ef(ω,-hi-Yt)))λS- PPGCω |1, 0)dωdt)
Y	9/(3，—『("；一1PPG(ω∣1,0)) ∙ exp
(ω,t)∈∏i
ef(ω,-hs-1(t))λS-1PPG(ω∣1, 0)dωdt).
The above posterior distribution is in the likelihood form of a marked Poisson process with intensity
function
Λi(t,ω∣wS-1,λS-1) = efM-hsT(t)K-1PPG31, 0) = λS-1σ(-hS-1(t))PPG(ω∣1, hS-1(t)).
M Step
Substituting posterior distributions of latent variables into Eq. 12, we obtain the lower-bound Q.
The first term of Eq. 12 is
E∏i,ωi [logP(D,∏i, ωi∣Wi,λi)]
1T	T
--wT ∙	Ai(t)Φ(t)ΦT(t)dt ∙Wi + WT ∙	Bi(t)Φ(t)dt
20	0
—λ江+
Λi(t, ω)dωdt
log λi + C
where we utilize the mean rule in Campbell’s theorem, C is a constant and
Ni	∞
Ai(t) =	E[ωni ]δ(t - tin) +	ωΛi(t, ω)dω,
n=1	0
1 Ni	1	∞
Bi(t) = 2 E δ(t - tn) - 2 J	Ai(t, ω)dω,
with δ(∙) being the Dirac delta function and E[ω" = 1 /(2hS-1 (tin)) tanh(hS-1(tn)∕2) (Polson
et al., 2013). The integral of intensity function has no closed-form solution but can be solved by
numerical quadrature methods.
The second term of Eq. 12 is
Eβi [logP(wi, βi)] = -2wT ∙ diag (Ee") ∙ Wi + C,
where C is a constant, E[βi] = {E[βijb]}MB+1 = {α∕ws-1 }MB+1 and diag(∙) indicates the
diagonal matrix of a vector.
13
Published as a conference paper at ICLR 2021
The updated parameters λi and Ws can be obtained by setting the gradient of Q to zero. Due to
auxiliary variables augmentation, we can see the weights are in a quadratic form in the lower-bound,
which leads to an analytical expression
λs = (Ni + Ki) /T,
/ T
0
Ws = ∑i
Bi(t)Φ(t)dt,
where Ki = RT R∞ Ai(t,ω∣wf-1,λf L)dωdt, ∑i = [/0T Ai(t)Φ(t)Φτ(t)dt + diag (α-2E[βi])].
It is worth noting that numerical quadrature methods need to be applied to intractable integrals
above.
IV Experimental Details
In this appendix, we elaborate on some experimental details.
Synthetic Data Experiments
For the synthetic data, the intensities and spike times of our simulated training and test data are
shown in Fig. 1. As shown in the experiment of log-likelihood and running time w.r.t. the number
of basis functions, the optimal number of basis functions is 4, which are chosen as the ground truth:
Φ{1,2,3,4} = Beta(α = 50,β = 50, scale = 6, shift = (-2, -1,0,1}). By cross validation, the
hyperparameter α is chosen to be 0.05. As shown in the experiment of log-likelihood and running
time w.r.t. the number of quadrature nodes, the accuracy is not sensitive to the number of quadrature
nodes over 100, so the number of quadrature nodes is set to 2000. The number ofEM iterations is set
to 200 which is large enough for convergence. We plot the estimated influence functions of 8 neurons
in Fig. 2. For comparison, We also plot the estimated influence functions of 8 neurons from vanilla
multivariate Hawkes processes using the MLE algorithm in Fig. 3 and the functional connectivity
graph in Fig. 4. We can see both estimated influence functions and functional connectivity graph
are far from the ground truth. This demonstrates the necessity of incorporating inhibitive interaction
into the model when the Hawkes process is applied in the neuroscience domain. The running time
experiment and the fitting and prediction experiment are both conducted for 2 neurons because the
baseline models cannot finish in 2 days with 8 neurons because of the curse of dimensionality.
Intensities and spikes of 8 neurons
1： rTTWWWT
i： ffWWW≡W
0	200	400	6∞	800	100
Test Dataset
1： FW≡≡
Figure 1: The intensities and spike times of 8 neurons in our synthetic training dataset (left) and test
dataset (right).
14
Published as a conference paper at ICLR 2021
InfIUen8 functions of Neuron-1
1ΛgOJ3
UO-β>pv
lnπuen∞ functions of Neuron-2
15w05g 3
UOT5>PV
uo⅛>pv
lnπuen∞ functions of Neuron-3
UO⅞>pv
lnflueπ∞ functions of Neuron-4
InfIUen8 functions of Neuron-5
InflUen8 functions of Neuron-β
InflUen8 functions of Neuron-7
w05g-05
UO-β>-pv
w05g 3
UO⅞>pv
UO⅞>pv
^77 GT
UO⅞>pv
⅛1乐≠7,m⅛5舐为
lnflueπ∞ functions of Neuroπ-β
Figure 2: The estimated influence functions of all neurons where the estimated φ s are close to the
ground truth and some ground truth are not labeled since they are all zero (GT=Ground Truth).
Influence functions of Neuron-I
co=ra>=⅛
0	1	2	3	4	5	6
Influence functions of Neuron-5
co=ra>=⅛
Influence functions of Neuron-2
0	1	2	3	4	5	6
Tφ
Influence functions of Neuron-6
co=ra>=⅛
's
Influence functions of Neuron-3
0	1	2	3	4	5	6
Influence functions of Neuron-7
influence functions of Neuron-4
co=ra>=⅛
0	1	2	3	4	5	6
T*
Influence functions of Neuron-8
Figure 3: The estimated influence functions of all neurons from vanilla multivariate Hawkes pro-
cesses using MLE; some influence functions are not labelled since they are all zero (GT=Ground
Truth).
Real Data Experiments
For the real spike data in cat primary visual cortex areas 17, it contains spike times of 25 simultane-
ously recorded neurons. We extract the spike times in the time window [0, 300] (time unit: 100ms) as
the training data and [300, 600] as the test data. Both datasets contain approximate 7000 timestamps.
The training and test spike trains are plotted in Fig. 5 below.
All hyperparameters are fine tuned in real data experiments. Specifically, the optimal basis function
is chosen as: φ = Beta(α = 50,β = 50, scale = 10, shift = -5). The hyperparameter α is
optimised to be 0.1 by cross validation. The number of quadrature nodes is chosen to be 1000 for
which the running time is acceptable. The number of EM iterations is set to 100 which is large
enough for convergence.
15
Published as a conference paper at ICLR 2021
Functional Connectivity: Ground Truth Functional Connectivity: MLE Estimation
Figure 4: The heat map of functional connectivity among neural population with ground truth (left)
and estimation from vanilla multivariate Hawkes processes (right).
1.0
0.8
1-0.6
-0.4
-0.2
-0.0
Training Spike Trains
SUQJn8N
SUQJn8N
0	50	100	150	200	250	300	0	50	100	150	200	250	300
T	T
Figure 5: The training and test spike trains in the dataset of cat primary visual cortex areas 17.
16