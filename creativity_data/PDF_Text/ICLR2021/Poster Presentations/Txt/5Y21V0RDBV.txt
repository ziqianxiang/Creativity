Published as a conference paper at ICLR 2021
Generalized Multimodal ELBO
Thomas M. Sutter*	Imant Daunhawer* Julia E. Vogt
Department of Computer Science
ETH Zurich
8092 Zurich, Switzerland
{thomas.sutter,imant.daunhawer,julia.vogt}@inf.ethz.ch
Ab stract
Multiple data types naturally co-occur when describing real-world phenomena and
learning from them is a long-standing goal in machine learning research. How-
ever, existing self-supervised generative models approximating an ELBO are not
able to fulfill all desired requirements of multimodal models: their posterior ap-
proximation functions lead to a trade-off between the semantic coherence and
the ability to learn the joint data distribution. We propose a new, generalized
ELBO formulation for multimodal data that overcomes these limitations. The
new objective encompasses two previous methods as special cases and combines
their benefits without compromises. In extensive experiments, we demonstrate
the advantage of the proposed method compared to state-of-the-art models in self-
supervised, generative learning tasks.
1	Introduction
The availability of multiple data types provides a rich source of information and holds promise for
learning representations that generalize well across multiple modalities (Baltrusaitis et al., 2018).
Multimodal data naturally grants additional self-supervision in the form of shared information con-
necting the different data types. Further, the understanding of different modalities and the interplay
between data types are non-trivial research questions and long-standing goals in machine learning
research. While fully-supervised approaches have been applied successfully (Karpathy & Fei-Fei,
2015; Tsai et al., 2019; Pham et al., 2019; Schoenauer-Sebag et al., 2019), the labeling of multiple
data types remains time consuming and expensive. Therefore, it requires models that efficiently
learn from multiple data types in a self-supervised fashion.
Self-supervised, generative models are suitable for learning the joint distribution of multiple data
types without supervision. We focus on VAEs (Kingma & Welling, 2014; Rezende et al., 2014)
which are able to jointly infer representations and generate new observations. Despite their suc-
cess on unimodal datasets, there are additional challenges associated with multimodal data (Suzuki
et al., 2016; Vedantam et al., 2018). In particular, multimodal generative models need to represent
both modality-specific and shared factors and generate semantically coherent samples across modal-
ities. Semantically coherent samples are connected by the information which is shared between data
types (Shi et al., 2019). These requirements are not inherent to the objective—the evidence lower
bound (ELBO)—of unimodal VAEs. Hence, adaptions to the original formulation are required to
cater to and benefit from multiple data types. Furthermore, to handle missing modalities, there is a
scalability issue in terms of the number of modalities: naively, it requires 2M different encoders to
handle all combinations for M data types. Thus, we restrict our search for an improved multimodal
ELBO to the class of scalable multimodal VAEs.
Among the class of scalable multimodal VAEs, there are two dominant strains of models, based on
either the multimodal variational autoencoder (MVAE, Wu & Goodman, 2018) or the Mixture-of-
Experts multimodal variational autoencoder (MMVAE, Shi et al., 2019). However, we show that
these approaches differ merely in their choice of joint posterior approximation functions. We draw
a theoretical connection between these models, showing that they can be subsumed under the class
* Equal contribution.
1
Published as a conference paper at ICLR 2021
of abstract mean functions for modeling the joint posterior. This insight has practical implications,
because the choice of mean function directly influences the properties of a model (Nielsen, 2019).
The MVAE uses a geometric mean, which enables learning a sharp posterior, resulting in a good
approximation of the joint distribution. On the other hand, the MMVAE applies an arithmetic mean
which allows better learning of the unimodal and pairwise conditional distributions. We generalize
these approaches and introduce the Mixture-of-Products-of-Experts-VAE that combines the benefits
of both methods without considerable trade-offs.
In summary, we derive a generalized multimodal ELBO formulation that connects and generalizes
two previous approaches. The proposed method, termed MoPoE-VAE, models the joint posterior
approximation as a Mixture-of-Products-of-Experts, which encompasses the MVAE (Product-of-
Experts) and MMVAE (Mixture-of-Experts) as special cases (Section 3). In contrast to previous
models, the proposed model approximates the joint posterior for all subsets of modalities, an advan-
tage that we validate empirically in Section 4, where our model achieves state-of-the-art results.
2	Related Work
This work extends and generalizes existing work in self-supervised multimodal generative models
that are scalable in the number of modalities. Scalable in the sense that a single model approximates
the joint distribution over all modalities (including all marginal and conditional distributions) instead
of requiring individual models for every subset of modalities (e.g., Huang et al., 2018; Tian & Engel,
2019; Hsu & Glass, 2018). The latter approach requires a prohibitive number of models, exponential
in number of modalities.
Multimodal VAEs Among multimodal generative models, multimodal VAEs (Suzuki et al., 2016;
Vedantam et al., 2018; Kurle et al., 2019; Tsai et al., 2019; Wu & Goodman, 2018; Shi et al., 2019;
2020; Sutter et al., 2020) have recently been the dominant approach. Multimodal VAEs are not only
suitable to learn a joint distribution over multiple modalities, but also enable joint inference given
a subset of modalities. However, to approximate the joint posterior for all subsets of modalities
efficiently, it is required to introduce additional assumptions on the form of the joint posterior. To
overcome the issue of scalability, previous work relies on either the product (Kurle et al., 2019;
Wu & Goodman, 2018) or the mixture (Shi et al., 2019; 2020) of unimodal posteriors. While both
approaches have their merits, there are also disadvantages associated with them. We unite these
approaches in a generalized formulation—a mixture of products joint posterior—that encapsulates
both approaches and combines their benefits without significant trade-offs.
Multimodal posteriors The MVAE (Wu & Goodman, 2018) assumes that the joint posterior is a
product of unimodal posteriors—a Product-of-Experts (PoE, Hinton, 2002). The PoE has the ben-
efit of aggregating information across any subset of unimodal posteriors and therefore provides an
efficient way of dealing with missing modalities for specific types of unimodal posteriors (e.g., Gaus-
sians). However, to handle missing modalities the MVAE relies on an additional sub-sampling of
unimodal log-likelihoods, which no longer guarantees a valid lower bound on the joint log-likelihood
(Wu & Goodman, 2019). Previous work provides empirical results that exhibit the shortcomings of
the MVAE, attributing them to a precision miscalibration of experts (Shi et al., 2019) or to the av-
eraging over inseparable individual beliefs (Kurle et al., 2019). Our results suggest that the PoE
works well in practice, if it is also applied on all subsets of modalities, which naturally leads to
the proposed Mixture-of-Products-of-Experts (MoPoE) generalization, which yields a valid lower
bound on the joint log-likelihood.
On the other hand, the MMVAE (Shi et al., 2019) assumes that the joint posterior is a mixture of uni-
modal posteriors—a Mixture-of-Experts (MoE). The MMVAE is suitable for the approximation of
unimodal posteriors and for translation between pairs of modalities, however, it cannot take advan-
tage of multiple modalities being present, because it only takes the unimodal posteriors into account
during training. In contrast, the proposed MoPoE-VAE computes the joint posterior for all subsets
of modalities and therefore enables efficient many-to-many translations. Extensions of the MVAE
and MMVAE (Kurle et al., 2019; Daunhawer et al., 2020; Shi et al., 2020; Sutter et al., 2020) have
introduced additional loss terms, however, these are also applicable to and can be added on top of
the proposed model.
2
Published as a conference paper at ICLR 2021
Table 1: Properties of previous scalable multimodal VAEs and our proposed model. Note that to
deal with missing modalities, the MVAE requires sub-sampling of unimodal ELBOs, which yields
an invalid bound on the joint log-likelihood (Wu & Goodman, 2019).
Model	Posterior form	Aggregate modalities	Multi-modal posterior	Missing modalities
MVAE	PoE		X	!✓)
MMVAE	MoE	X	✓	✓
MopoE-VAE (ours)	MoPoE	✓	✓	✓
Table 1 summarizes the properties of previous multimodal VAEs and highlights the benefits of the
proposed model: the ability to aggregate multiple modalities, to learn a multi-modal posterior (in
the statistical sense), and to efficiently handle missing modalities at test time.
3	Method
3.1	Preliminaries
We consider a dataset {X(i) }iN=1 of N i.i.d. samples, each of which is a set of M modalities
X(i) = {x(ji) }jM=1. We assume that the data is generated by some random process involving a
joint hidden random variable z such that inter-modality dependencies are unknown. The marginal
log-likelihood can be decomposed into a sum over marginal log-likelihoods of individual sets
log pθ ({X(i)}iN=1) = PiN=1 logpθ(X(i)), which can be written as:
log Pθ(X(i)) = DκL(qφ(z∣X ⑴)∣∣PΘ (z|X(i))) + L(θ,φ; X(i)),	⑴
With L(θ,φ;X⑴):=Eqφ(z∣χ(i))[logPθ(X⑴⑶]-DκL(qφ(z∣X⑴)∣∣PΘ(z)).	⑵
L(θ, φ; X(i)) is called evidence lower bound (ELBO) on the marginal log-likelihood of the i-th set.
It forms a tractable objective to approximate the joint data distribution logpθ(X(i)). qφ(z∣X(i))
is the posterior approximation distribution With learnable parameters φ. From the non-negativity
of the KL divergence, it folloWs that log pθ (X(i)) ≥ L(θ, φ; X(i)). If the posterior approximation
qφ(z|X(i)) is identical to the true posterior distribution pθ(z|X(i)), the bound holds with equality.
Hence, maximizing the ELBO in Equation (2) minimizes the otherWise intractable KL-divergence
between approximate and true posterior distribution:
arg min Dkl (qφ(z∣X(i))∣∣Pθ (z∣X(i))).	(3)
φ
Adaptations to the ELBO formulation in Equation (2) include an additional hyperparameter β which
weights the KL-divergence relative to the log-likelihood (Higgins et al., 2017). To improve read-
ability, we will omit the superscript (i) in the remaining part of this work.
3.2	Approximatingpθ(z|X) in case of missing data types
For a dataset of M modalities, there are 2M different subsets contained in the powerset P(X).
if, for a particular observation, we only have access to a subset of data types Xk ∈ P (X), the
approximation ofpθ(Xk) would result in a different ELBo formulation L(θ, φk; Xk) where the true
posterior pθ (z|Xk) of subset Xk is approximated. Instead, we are interested in the true posterior
Pθ(z|X) of all data types X, even when only a subset Xk, i.e.加(z|Xk), is available. The desired
ELBo for the available subset Xk is given by
Lk(θ,φk；X)= Egφk(z|Xk)[log(Pθ(X|z)] - Dkl(加(z|Xk)∣∣PΘ(Z)) .	(4)
The subtle but important difference between Lk(θ, φk; X) and L(θ, φk; Xk) is that the former still
yields a valid lower bound on pθ(X), whereas the latter forms a lower bound on logpθ(Xk), which
is no longer a valid bound on the desired log pθ (X).
3
Published as a conference paper at ICLR 2021
Different from previous work, we argue for an optimization of the powerset P (X), i.e., the joint
optimization of all ELBOs Lk (θ,φk; X) defined by the posterior subset approximation @©工(z |Xk).
Since maximizing the ELBO in Equation (2) is equivalent to minimizing the KL-divergence in Equa-
tion (3), the joint optimization of the powerset P(X) is equal to the minimization of the following
convex combination of KL-divergences of the power set P (X).1
argmin EXx ∈P(X)DKL 伍 φ(ZIXk )帆(ZIX))	⑸
Hence, we propose to optimize Equation (4) for all subsets Xk.
Lemma 1. The sum of KL-divergences in Equation (5) describes the joint probability log pθ (X) as
follows:
logpθ㈤=2M	X DKL (Gφ(ZIXk)llpθ(ZIX)) + 2M	X %φ(z∣Xk)
Xk∈P(X)	Xk∈P(X)
log
Pθ (X∣z)pθ (Z)
Gφ(z∣Xk)
Following Lemma 1 (see Appendix A.1 for the proof) and the non-negativity of the KL-divergence,
we see that the convex combination of expectations over the powerset P(X) is an ELBO on the
joint probability log pθ (X). Since this would require 2M different inference networks in a naive
implementation, we use a more efficient approach utilizing abstract mean functions.
3.3	Scalable inference using abstract mean functions
To create a model that is scalable in the number of modalities—a model that breaks the need for
2M different networks—previous works define the joint posterior approximation qφ(ZIX) as a mean
function of the unimodal variational posteriors. The PoE and MoE can be subsumed under the
concept of abstract means (Nielsen, 2019). Abstract means unify multiple mean functions Mf for
a given function f (Niculescu & Persson, 2005):
Mf(P) = f-1 (P X f (Pk)
where P is the number of elements and the function f needs to be injective in order for f-1 to exist.
f(P) = aP + b results in the arithmetic mean, f(P) = logP in the geometric mean.
The choice of mean function directly influences the properties of the learned model as we will
recapitulate with regard to multimodal VAEs in the following. The MVAE (Wu & Goodman, 2018)
employs the PoE, which is a geometric mean of unimodal posteriors. Aggregation through the PoE
results in a sharp posterior approximation (Hinton, 2002), but struggles in optimizing the individual
experts as mentioned by the authors (Wu & Goodman, 2018, p. 3). In contrast, the MMVAE (Shi
et al., 2019) uses the MoE, which is an arithmetic mean of unimodal posteriors. As such, the
MMVAE optimizes individual experts well, but is not able to learn a distribution that is sharper
than any of its experts. Thus, the choice of mean function directly influences the properties of
the resulting model. The MoE is optimizing for conditional distributions based on the unimodal
posterior approximations, while the PoE is optimizing for the approximation of the joint probability
distribution.
For scalable, abstract-mean based models, the set of parameters φk for the posterior approxima-
tion of a subset qGφ(ZIXk) is determined by the unimodal posterior approximations qφj (ZIxj) as
φk = {φj ∀j ∈ {1, . . . ,M} : xj ∈ Xk}.
3.4	Generalized multimodal ELBO
In the following, we first introduce the new ELBO LMoPoE(θ, φ; X) and then prove that its objective
minimizes the convex combination of KL-divergences in Equation (5).
1We omit the subscript k for the parameterization of the posterior approximations when it is clear from
context that only Xk is available, and write φ instead.
4
Published as a conference paper at ICLR 2021
Definition 1.
1.	Let the posterior approximation of subset Xk be
qΦ(zlXk) = POE({qφj (ZIxj) ∀Xj ∈ Xk}) Y ∩x ∈Xfc qφj (ZIxj) .
2.	Let the joint posterior be qφ(z∣X) = 击 PXk ∈p(%)qφ(z∖Xk).
The objective LMoPoE(θ, φ; X) for learning a joint distribution of multiple data types X is defined as
L MoPoE (θ,φ; X) := Eqφ(z∣χ)[lθg(pθ (X∣z)] — DKL (X 7φ(z∣Xk ) ∣∣Pθ (z)) .	(6)
Xk∈P(X)
From Definition 1, Lemma 2 directly follows.
Lemma 2. LMoPoE(θ, φ; X) is a multimodal ELBO, that is log pθ (X) ≥ LMoPoE(θ, φ; X).
Since qφ(Z∣X) is defined as a mixture distribution (i.e., a probability distribution), it directly follows
that LMoPoE(θ, φ; X) is a valid ELBO on logpθ(X), because a variational distribution can be chosen
arbitrarily as long as itis a valid probability distribution. For a proof of Lemma 2, see Appendix A.2.
Lemma 3. Maximizing LMoPoE(θ, φ; X) minimizes the convex combination of KL-divergences of the
powerset P(X) given in Equation (5).
For a proof of Lemma 3, see Appendix A.3. Definition 1 does not put any restrictions on the choice
of posterior approximations qφ(z∖Xk). As We are interested in scalable, multimodal models, We
focus on methods which apply to this restriction and choose the PoE for the posterior approximations
of the subsets Xk ∈ P(X). Other, non-scalable posterior fusion methods are possible using this
frameWork.
3.5	The General Framework
Definition 1 can be interpreted as a hierarchical distribution: first the unimodal posterior approxima-
tions ofa subset qφj (Z∣xj) ∀xj ∈ Xk are combined using a PoE, second the subset approximations
qφ (z∣Xk) ∀Xk ∈ P (X) are combined using aMoE. This allows us to combine the strengths of both
MoE as Well as PoE While circumventing their Weaknesses (see Section 2). For Gaussian posterior
approximations, as is common in VAEs, the PoE can be calculated in closed form, Which makes it a
computationally efficient solution.
In the folloWing, We derive the objectives optimized by the MVAE and MMVAE as special cases of
LMoPoE(θ, φ; X). The MVAE only takes into account the full subset, i.e., the PoE of all data types.
Trivially, this is a MoE With only a single component:
LpoE (θ,φ; X) = Eqφ(z∣χ)[lθg(pθ (X∣z)] — DκL(qφ(z∣X)∣∣Pθ (Z))	⑺
M1
With qφ(Z∣X) Y Y qφj(Z∣xj) = PoE({qφj(Z∣xj)}jM=1) = X P oE({qφj (Z∣xj)}jM=1)	(8)
j=1	k=1
This is equivalent to the MoPoE-VAE of a single subset XK, Which is the full set X.
As the PoE of a single expert is just the expert itself, the MMVAE model (Shi et al., 2019) is the
special case of LMoPoE(θ, φ; X) Which takes only into account the M unimodal subsets:
(1 M	∖
Lmoe(Θ, φ; X) = Eqφ(z∣x)[log(Pθ(X∣z)] — DKL I M	qφj (z∣xj) ∣∣Pθ (Z))	(9)
1M	1M
with qφ(z∣X) = M Xqφj (ZIxj) = M XPoESφ,(ZIxj))	(IO)
5
Published as a conference paper at ICLR 2021
LMoE (θ, φ; X) is equivalent to a MoPoE-VAE of the M unimodal posterior approximations
qφj(z|xj) forj = 1,...,M.
Therefore, the proposed MoPoE-VAE is a generalized formulation of the MVAE and MMVAE,
which accounts for all subsets of modalities. The identified special cases offer a new perspective on
the strengths and weaknesses of prior work: previous models focus on a specific subset of posteriors,
which might lead to a decreased performance on the remaining subsets.
In particular, the MVAE should perform best when all modalities are present, whereas the MMVAE
should be most suitable when only a single modality is observed. We validate this observation
empirically in Section 4.
4	Experiments & Results
We evaluate the proposed method on three different datasets and compare it to state-of-the-art meth-
ods. We introduce a new dataset called PolyMNIST with 5 simplified modalities. Additionally, we
evaluate all models on the trimodal matching digits dataset MNIST-SVHN-Text and the challenging
bimodal Celeba dataset with images and text. The latter two were introduced in Sutter et al. (2020).
We evaluate the models according to three different metrics. We assess the quality of the learned
latent representation using a linear classifier. The coherence of generated samples is evaluated using
pre-trained classifiers. The approximation of the joint data distribution is measured using test set
log-likelihoods.
The datasets and the evaluation of experiments are described in detail in Appendix B.
4.1	MNIST-SVHN-Text
Based on the MNIST-SVHN dataset (Shi et al., 2019),
this trimodal dataset with an additional text modal-
ity forces a model to adapt to multiple data types. It
involves data types of various difficulties. Whereas
MNIST (LeCun & Cortes, 2010) and text are clean
modalities, SVHN (Netzer et al., 2011) is comprised of
noisy images.
Tables 2 and 3 show the superior performance of the
proposed method compared to state-of-the-art methods
regarding the ability to learn meaningful latent rep-
resentations and generate coherent samples. MVAE
reaches superior performance for the generation of the
SVHN modality, while MoPoE-VAE overall achieves
Figure 1: Joint Coherence vs. Log-
Likelihoods for MNIST-SVHN-Text.
best coherence results. Table 4 shows the results for the test log-likelihoods.
The proposed MoPoE-VAE is the only method that is able to reach state-of-the-art coherence, latent
classification accuracies, as well as test log-likelihoods for all combination of inputs. This can
be seen in Figure 1, illustrating the trade-off between test log-likelihoods and joint coherence for
every model. Every point encodes the joint coherence and joint log-likelihood for a different β-
value.2 The goal is to have high coherence and log-likelihoods (i.e., the top right corner). Note that
lower beta values typically correspond to models with a higher log-likelihood but lower coherence.
Overall, the MoPoE-VAE achieves a superior trade-off compared to the baselines. As expected
by our theoretical analysis (Section 3.5), the MVAE achieves good joint log-likelihoods, whereas
MMVAE reaches high joint coherence.
2The β-hyperparameter controls the weight of the KL-divergence in Equation (6). We evaluate the models
using β ∈ {0.5, 1.0, 2.5, 5.0, 10.0, 20.0}.
6
Published as a conference paper at ICLR 2021
Table 2: Linear classification accuracy of latent representations for MNIST-SVHN-Text. We eval-
uate all subsets of modalities Xk where the abbreviations of subsets are as follows: M: MNIST; S:
SVHN; T: Text; M,S: MNIST and SVHN; M,T: MNIST and Text; S,T: SVHN and Text; M,S,T: all.
We report the means and standard deviations over 5 runs.
Model	M	S	T	M,S	M,T	S,T	M,S,T
MVAE	0.90±0.01	0.44±0.01	0.85±0.10	0.89±0.01	0.97±0.02	0.81±0.09	0.96±0.02
MMVAE	0.95±0.01	0.79±0.05	0.99±0.01	0.87±0.03	0.93±0.03	0.84±0.04	0. 86±0.03
MoPoE	0.95±0.01	0.80±0.03	0.99±0.01	0.97±0.01	0.98±0.01	0.99±0.01	0.98±0.01
Table 3: Generation coherence for MNIST-SVHN-Text. For conditional generation, the letter above
the horizontal line indicates the modality which is generated based on the subsets Xk below. We
report the mean values over 5 runs. Standard deviations are included in Appendix C.3.
Model	Joint	M			S			T		
		S	T	S,T	M	T	M,T	M	S	M,S
MVAE	0.12	0.24	0.20	0.32	0.43	0.30	0.75	0.28	0.17	0.29
MMVAE	0.28	0.75	0.99	0.87	0.31	0.30	0.30	0.96	0.76	0.84
MOPOE	0.31	0.74	0.99	0.94	0.36	0.34	0.37	0.96	0.76	0.93
4.2	PolyMNIST
The PolyMNIST dataset consists of sets of MNIST
digits where each set {xj }jM=1 consists of 5 images
with the same digit label but different backgrounds
and different styles of hand writing. An example
of one such tuple is shown in Figure 2. Thus, each
“modality” represents a shuffled set of MNIST digits
overlayed on top of (random crops from) 5 different
background images, which are modality-specific. In
total there are 60, 000 tuples of training examples and
10, 000 tuples of test examples and we make sure that
no two MNIST digits were used in both the training
and test set.3
冷殖国M匿阖国源•暨5
O ∕≡B/芸，7里，
8西口❸中画■物层^
JO V &归并$融〉幻中
圆”陷・死囱/KI幅茗
Figure 2: Ten samples from the PolyMNIST
dataset. Each column depicts one tuple that
consists of five different “modalities”.
The PolyMNIST dataset allows to investigate how well different methods perform given more than
two modalities. Since individual images can be difficult to classify correctly (even for a human
observer) one would expect multimodal models to aggregate information across multiple modalities.
Further, this dataset facilitates the comparison of different models, because it removes the need for
modality-specific architectures and hyperparameters. As such, for a fair comparison, we use the
same architectures and hyperparameter values across all methods. We expect to see that both the
MMVAE and our proposed method are able to aggregate the redundant digit information across
different modalities, whereas the MVAE should not be able to benefit from an increasing number
of modalities, because it does not aggregate unimodal posteriors. Further, we hypothesize that the
MVAE will achieve the best generative performance when all modalities are present, but that it
will struggle with an increasing number of missing modalities. The proposed MoPoE-VAE should
perform well given any subset of modalities.
PolyMNIST results Figure 3 compares the results across different methods. The performance
in terms of three different metrics is shown as a function of the number of input modalities; for
instance, the log-likelihood of all generated modalities given one input modality (averaged over all
possible single input modalities). As expected, both the MVAE and MoPoE-VAE benefit from more
input modalities, whereas the performance of the MVAE stays flat across all metrics. In the limit
of all 5 input modalities, the log-likelihood of MoPoE-VAE is on par with MVAE, but the proposed
method is clearly superior in terms of both latent classification as well as conditional coherence
3Details on how the dataset was generated are included in Appendix D.
7
Published as a conference paper at ICLR 2021
Table 4: Test set log-likelihoods on MNIST-SVHN-Text. We report the test set log-likelihoods of the
joint generative model conditioned on the variational posterior of subsets of modalities qφ(Z∣Xk).
(xM : MNIST; xS: SVHN; xT : Text; X = (xM , xS, xT )).
Model	X	X|xM	X|xS	X|xT	X|xM , xS	X|xM, xT	X|xS, xT
MVAE	-1790±3.3	-2090±3.8	-1895±0.2	-2133±6.9	-1825±2.6	-2050±2.6	-1855±0.3
MMVAE	-1941±5.7	-1987±1.5	-1857±12	-2018±1.6	-1912±7.3	-2002±1.2	-1925±7.7
MOPOE	-1819±5.7	-1991±2.9	-1858±6.2	-2024±2.6	-1822±5.0	-1987±3.1	-1850±5.8
Figure 3: Performance on PolyMNIST as a function of the number of input modalities, averaged
over all subsets of the respective size. Performance is measured in terms of three different metrics
(larger is better) and markers denote the means (error bands denote standard deviations) over five
runs. Left: Linear classification accuracy of digits given the latent representation computed from
the respective subset. Center: Coherence of conditionally generated samples (excluding the input
modality). Right: Log-likelihood of all generated modalities. Not shown: The joint coherence is
3.6 (±1.5), 20.0 (±1.9), and 12.1 (±1.6) percent for MVAE, MMVAE, and MoPoE respectively.
-→- MVAE
-→- MMVAE
▲ MoPoE
across any subset of modalities. Analogously, in the limit of a single input modality, MoPoE-VAE
matches the performance of MMVAE. Only in terms of the joint coherence (see figure legend) the
MMVAE performs better, suggesting that a more flexible prior might be needed for the MoPoE-
VAE. Therefore, the PolyMNIST experiment illustrates that the proposed method does not only
theoretically encompass the other two methods, but that it is superior for most subsets of modalities
and even matches the performance in special cases that favor previous methods.
4.3	Bimodal CelebA
In this dataset, the images displaying faces (Liu et al., 2015) are equipped with additional text de-
scribing the faces using the labeled attributes. Any negatively labeled attribute is completely missing
in the string which makes the text modality more challenging. Compared to previous experiments,
we additionally use modality-specific latent spaces, which were found to improve the generative
quality of a model (Hsu & Glass, 2018; Sutter et al., 2020; Daunhawer et al., 2020).4 Figure 4 dis-
plays qualitative results of images which are generated given text. Table 5 shows the classification
results for the coherence of generated samples as well as the classification of latent representations.
We see that the proposed model is able to match the baselines on this challenging dataset, which
favors the baselines, because it consists of two modalities. Figure 4 shows that attributes like “gen-
der” or “smiling” are learned well, as they manifest in generated samples and can be identified
from the latent representation. Subtle and rare attributes are more difficult to generate consistently;
evaluations specific to the different labels are provided in Appendix E.3.
5	Conclusion
In this work, we propose a new multimodal ELBO formulation. Our contribution is threefold: First,
the proposed MoPoE-VAE generalizes prior works (MVAE, MMVAE) and combines their benefits.
Second, we analyze the strengths and weaknesses of previous works and relate them directly to their
4For more details, see Appendix E.
8
Published as a conference paper at ICLR 2021
Table 5: Classification and coherence results on the bimodal CelebA experiment. For latent rep-
resentations and conditionally generated samples, we report the mean average precision over all
attributes (I: Image; T: Text; Joint: I and T).
Latent Representation	Generation
Attractive,
high
cheekbones,
mouth slightly
open, no
beard, wavy
haiŋ young
Model	I	T	Joint	I→T	T→I
MVAE	0.30	0.31	0.32	0.26	0.33
MMVAE	0.35	0.38	0.35	0.14	0.41
MOPOE	0.40	0.39	0.39	0.15	0.43
Big nose, high
cheekbones,
male, mouth
slightly open,
oval face,
receding
hairline
Big nose, gray
haiŋ male,
mouth slightly
open, no
smiling,
wearing
necktie
Attractive,
bangs, blond
hair; heavy
makeup, no
straight haiŋ
wearing
Iipstickj young
Attractive, big
lips, heavy
makeup, no
wearing
Iipstickz young
Bangs, big
nose, high
cheekbones,
male, mouth
Slightlyopenf
no beardz oval
face, smiling,
young
5 o clock
shadow, bags
under eyes,
black haiŋ
eyebrows,
chubb½
goateez malef
mustache
Attractive,
bangs, brown
haiŋ heavy
makeup, no
beardz pointy
nose, wavy
hair; wearing
lipstick
Mouth slightly
open, no
wearing
earrings,
wearing
Iipstick7 young
Big nose,
brow hair;
male, no
beard, wavy
hair



Figure 4: Qualitative results for bimodal CelebA. The images are conditionally generated by
MoPoE-VAE using the text on top of each column.
objective and choice of posterior approximation function. Finally, in extensive experiments we em-
pirically show the advantages compared to state-of-the-art models and even match their performance
on tasks that favor previous work. In future work, we would like to evaluate previous extensions to
multimodal VAEs. Addtionally, we will explore different types and combinations of abstract mean
functions and investigate their effects on the model and its performance as well as their theoretical
properties (e.g., tightness) compared to existing methods.
Acknowledgments
We would like to thank Ricards MarCinkeVics for helpful discussions and proposing the name
“PolyMNIST”. ID is supported by the SNSF grant #20002L188466.
References
Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning:
A surVey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41
(2):423-443,2018.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-leVel Variational autoencoder:
Learning disentangled representations from grouped obserVations. In Thirty-Second AAAI Con-
ference on Artificial Intelligence, 2018.
Yuri Burda, Roger B Grosse, and Ruslan SalakhutdinoV. Importance Weighted Autoencoders. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.
00519.
Thomas M CoVer and Joy A Thomas. Elements of Information Theory (Wiley Series in Telecommu-
nications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.
9
Published as a conference paper at ICLR 2021
Imant Daunhawer, Thomas M Sutter, Ricards Marcinkevics, and Julia E Vogt. Self-supervised Dis-
entanglement of Modality-specific and Shared Factors Improves Multimodal Generative Models.
In German Conference on Pattern Recognition. Springer, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a
Constrained Variational Framework. ICLR, 2(5):6, 2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Wei-Ning Hsu and James Glass. Disentangling by Partitioning: A Representation Learning Frame-
work for Multimodal Sensory Data. 2018. URL http://arxiv.org/abs/1805.11264.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
172-189, 2018.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3128-3137, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114.
Richard Kurle, StePhan Gunnemann, and Patrick van der Smagt. MUlti-SoUrce Neural Variational
Inference. In The Thirty-Third Conference on Artificial Intelligence, AAAI 2019, pp. 4114-4121.
{AAAI} Press, 2019. doi: 10.1609/aaai.v33i01.33014114. URL https://doi.org/10.
1609/aaai.v33i01.33014114.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. DeeP Learning Face Attributes in the Wild.
In The IEEE International Conference on Computer Vision (ICCV), 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units imProve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), PP. 807-814,
2010.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsuPervised feature learning. 2011.
C Niculescu and L Persson. Convex Functions and Their APPlications: A ContemPorary APProach.
2005.
Frank Nielsen. On the Jensen-Shannon symmetrization of distances relying on abstract means.
Entropy, 2019. ISSN 10994300. doi: 10.3390/e21050485.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. Scikit-
learn: Machine learning in Python. Journal of machine learning research, 12(Oct):2825-2830,
2011.
Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, and Barnabas Poczos. Found
in translation: Learning robust joint rePresentations by cyclic translations between modalities. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6892-6899, 2019.
10
Published as a conference paper at ICLR 2021
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31th International Con-
ference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1278-1286,
2014. URL https://arxiv.org/abs/1401.4082.
Alice Schoenauer-Sebag, Louise Heinrich, Marc Schoenauer, Michele Sebag, Lani F Wu, and
Steve J Altschuler. Multi-domain adversarial learning. In 7th International Conference on Learn-
ing Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 New Orleans, LA, USA,
May 6-9, 2019, 2019. URL https://arxiv.org/abs/1903.09239.
Yuge Shi, N Siddharth, Brooks Paige, and Philip Torr. Variational Mixture-of-Experts Autoencoders
for Multi-Modal Deep Generative Models. In Advances in Neural Information Processing Sys-
tems, pp. 15692-15703, 2019.
Yuge Shi, Brooks Paige, Philip H S Torr, and N Siddharth. Relating by Contrasting: A Data-
efficient Framework for Multimodal Generative Models. 2020. URL https://arxiv.org/
abs/2007.01179.
Thomas M Sutter, Imant Daunhawer, and Julia E Vogt. Multimodal Generative Learning Utilizing
Jensen-Shannon-Divergence. 2020. URL https://arxiv.org/abs/2006.08242.
Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint Multimodal Learning with Deep
Generative Models. pp. 1-12, 2016. URL http://arxiv.org/abs/1611.01891.
Yingtao Tian and Jesse Engel. Latent translation: Crossing modalities by bridging generative mod-
els. 2019. URL https://arxiv.org/abs/1902.08261.
Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Learning Factorized Multimodal Representations. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,
2019. URL https://openreview.net/forum?id=rygqqsA9KX.
Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative Models of
Visually Grounded Imagination. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=HkCsm6lRb.
Mike Wu and Noah Goodman. Multimodal Generative Models for Scalable Weakly-Supervised
Learning. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal,
Canada, pp. 5580-5590, 2 2018. URL http://arxiv.org/abs/1802.05335.
Mike Wu and Noah Goodman. Multimodal Generative Models for Compositional Representation
Learning, 2019.
11
Published as a conference paper at ICLR 2021
A Proofs
A.1 Proof of Lemma 1
EXk∈p(χ) DKL(⅞φ(z∣Xk)∣∣Pθ(z∣X)), i.e., the Sum of KL-divergences in Equation (5) can be used
for describing the joint probability logpθ(X).
Proof. We show that the convex combination of KL-divergences can be directly related to the joint
probability log pθ (X):
X Dkl(设z|Xk)∣∣PΘ(z∣X)) = X E%(z∣χjlog qφ(ZX^
Xk ∈P	(X)	Xk ∈P	(X)	pθ
=X	(%φ(z∣XkJlog qΦ(zX) ] +log Pθ (χ))	(11)
Xk ∈P	(X)	pθ ,
which can be reformulated as an expression of the joint probability logpθ(X):
logpθ(X)=2M	X	DKL (Gφ(ZIXk)llpθ(ZIX))+2M	X %φ(z∣Xk)
Xk∈P	(X)	Xk∈P	(X)
'---------------V----------------}
Equation (5)
log
Pθ (X∣z)pθ (Z)
Gφ(z∣Xk)
From the non-negativity of the KL-divergence, we derive the lower bound to the joint probability
log p(X):
1	pθ(XIZ)pθ(Z)
logpθ(X) ≥ 2M XkX(X) %(ZIXk)怦 ^Xr
(12)
□
A.2 Proof of Lemma 2
LMoPoE(θ, φ; X) is a multimodal ELBO, that is log pθ (X) ≥ LMoPoE(θ, φ; X).
Proof. The sums using index k also sum over all 2M subsets in the power set P(X). We use k only
for better readability.
logP(X) = DKL(qφ(ZIX)IIp(Z|X)) + Eqφ(zIX)[log P(Z,IX))]	(13)
qφ(ZIX)
=Dkl (2MX虱ZIXk)IIp(ZIX)) + E2MPk虱ZIXk) log 2MP(Z3k)	(14)
p(Z X)
≥ E责 Pk 虱ZIXk) Ig 2μ Pk q(ZIXk)]	(15)
=E于φ(z∣X)[log(pθ(XIz)] - DKL (2M X qφk (ZIXk) Μpθ(Z))	(16)
Xk∈P (X)
= L(θ, φ; X)	(17)
□
A.3 Proof of Lemma 3
Maximizing LMoPoE (θ, φ; X) minimizes the convex combination of KL-divergences of the powerset
P(X) given in Equation (5).
12
Published as a conference paper at ICLR 2021
Proof. Lemma 1 shows that the sum of KL-divergences in Equation (5) is able to describe the joint
probability and can be used to form a valid ELBO. Equation (12) is the convex combination of
ELBOs given a subset's posterior approximation qφ(z∖Xk). Utilizing Jensen's inequality, it follows:
2M	X Eaφ(z∣Xk) log
Xk∈P(X)
Pθ (X∣z)pθ (Z)
7φ(Z∣Xk)
≤ E2M Pk qφ(zlXk)
log
Pθ (X∣z)pθ (Z)
2M Pk 欧(z|Xk)
(18)
where the sums on the right hand also iterate over all Xk ∈ P(X). From Equation (18), we see that
the proposed LMoPoE(θ, φ; X) is not only a valid lower bound to the joint log-probability logpθ(X),
but also a tighter one than the convex combination of ELBOs:
logpθ(X) ≥ E2M Pk Qφ(z∣Xk)
log
Pθ (X∣z)pθ(z)
2M Pk 7φ(z∣Xk)
= LMoPoE (θ, φ; X)
≥ 2M	X EaΦ(z∣χk)
Xk∈P(X)
l Pθ (X∣z)pθ (Z)
og qφ(Z∖Xk)
(19)
(20)
(21)
As Equation (19) can be directly derived from Equation (5), maximizing the proposed objective
results in minimizing the convex combination of KL-divergences.
In the following, we derive the inequality in Equation (18) in more detail:
1
2m
2M	X	Eaφ(z∣Xk)
Xk∈P(X)
log
Pθ (X∣z)pθ (Z)-
qφ(z∖Xk)
2M	X %φ(z∣Xk) [logPθ(XIZ)Pθ(Z)- logqφ(z∖Xk)]
Xk∈P(X)
E	%φ(z∣Xk) [logPθ(Xlz)pθ(z)]-
Xk∈P(X)
1
2m
(22)
(23)
___________________ /
=E壶 Pk⅞φ(z∣Xk)[logpθ(XIz)pθ(Z)]
E	Eφ(z∣Xk) [logQφ(z∖Xk)]
Xk∈P(X)
(24)
≥E2m Pk qφ(z∣Xk)[log(2M Pk αφ(zlXk))]
≤ E2⅛ Pk Qφ(z∣Xk) [logPθ(Xlz)Pθ(Z)I-E2⅛
Pk Qφ(z∣Xk)
log (2M X 6φ(ZIXk))]
(25)
E2M Pk Qφ(zlXk )
E2⅛ Pk Qφ(zlXk )
log (Pθ(XIZ)pθ(Z))- log (言 Xqφ(z\χk))]
(26)
log
pθ (X∖Z)pθ(Z)
2M Pk Qφ(z∖Xk)
(27)
}
□
In the minuend of Equation (24), the ordering of expectation and sum can be exchanged due to the
linearity of the expectation. In the subtrahend of Equation (24), the sum of expectation of the pos-
terior approximations of subsets can be reformulated into the expectation of a mixture distribution
using Jensen's inequality. Due to the convexity of the function f(t) = tlogt (Cover & Thomas,
2006, p.29), the expectation of a mixture distribution is a lower bound to the sum over the expec-
tation of posterior approximations as the mixture distribution can be seen as a convex combination
of posterior approximations of subset of modalities qφ(z∖Xk). Hence, the inequality from EqUa-
tion (24) to Equation (25) follows as we decrease the subtrahend in Equation (25).
13
Published as a conference paper at ICLR 2021
B Evaluation of Experiments
For the experiments, we evaluate all models regarding three different metrics: the classification
accuracy (or average precision for CelebA) on the latent representation, the coherence of generated
samples and the test set log-likelihoods.
The latent representations are evaluated using a logistic regression classifier from scikit-learn (Pe-
dregosa et al., 2011). The classifier is trained using 500 samples from the training set which are
encoded using the trained models. The evaluation is done on the full test set and the reported num-
bers are the average performances over all batches in the test set.
The generation coherence is evaluated using the same networks as the unimodal encoders which
were trained beforehand. For every data type, we train a neural network classifier in a supervised
way. The architecture of the classifier is identical to the encoder except from the last layer. For joint
coherence, all generated samples are evaluated by the classifier and if all modalities are classified as
having the same label, they are considered coherent. The coherence accuracy is the ratio of coherent
samples divided by the number of generated samples. For conditional generation, the conditionally
generated samples have to be coherent to the input samples.
The test set log-likelihoods are evaluated using 15 importance samples for all models and the re-
ported numbers are the averages over all test set batches.
If not stated differently, the reported numbers in section 4 are the mean and standard deviations of 5
runs with different random seeds. All models evaluated use the same architectures and numbers of
parameters. The likelihoods of the different modalities are weighted to each other according to the
size of the modality for all experiments. The most dominant modality is set to 1.0. The remaining
ones are scaled up by the ratio of their data dimensions. For example in the MNIST-SVHN-Text
experiment, SVHN is set to 1.0 and MNIST to 3.92 which is the ratio of their data dimensions.
For all UnimodaI posterior approximations, We assume Gaussian distributions N(z; μ, σ2In) where
n is the number of latent space dimensions. In all experiments, the mixture components are equally
Weighted with #Components .
B.1	Comparison to Previous Works
Shi et al. (2019) in the end use a different ELBO objective including importance samples LIWAE
(Burda et al., 2016). We compare all models Without the use of importance samples as these could
be easily introduced to all objectives and are not directly related to the focus of this Work Which is
choice of joint posterior approximation.
Sutter et al. (2020) utlize the Jensen-Shannon divergence as a regularizer instead of the KL-
divergence. This results in the use of a dynamic prior and shoWs promising results. Besides the
dynamic prior, they model the joint posterior approximation as Well using a MoE. Again, We do not
include models utilizing a dynamic prior as this could be introduced to all formulations and is not
the focus of this Work.
B.1.1	Equivalence to ELBO formulation in Shi et al. (2019)
For clarity, We shoW here the equivalence of the formulation in Equation (10) to the formulation
in (Shi et al., 2019, p.5).
14
Published as a conference paper at ICLR 2021
1M
LMoE(θ,φ; X) = Eqφ(z∣X)[log(Pθ(XIz)] - DKL I M	qφj (ZIxj川Pθ (Z))
=Eqφ(z∣X) [lOg(Pθ (XIz)] — E MM PM=1 qφj (z∣Xj ) log M 可 θ (Zj(ZIxj )
=Eqφ(z∣X)[log(Pθ (X|z)] - Eqφ(z∖X) log qI(ZIX)
pθ (Z )
Eqφ(z∣X)[logPθ (XIz)] + Eqφ(z∖X)
Eqφ(z∣x)
l	Pθ(XIz)pθ (z)
.og	qφ(ZIX)
1 XM E	IwPθ(Xiz)pθ (Z)
MM	qφj (ZIxj) [ g	qφ(ZIX)
Pθ(z)'
qφ(ZIX)
(28)
(29)
(30)
(31)
(32)
(33)
where Equation (32) and Equation (33) are equivalent to the first equation on page 5 in Shi et al.
The different formulation on the second line of the first equation on page 5 is coming from their use
of importance samples.
15
Published as a conference paper at ICLR 2021
Table 6: Generation Coherence for MNIST-SVHN-Text. For every subtable, the modality above
the wide horizontal line is generated based on the subsets below the same line—except for joint
coherence. The abbreviations of the different modalities are as follows: M:MNIST; S: SVHN; T:
Text. Combinations thereof separated by commas result in the subsets consisting of the modalities.
We report the mean value and standard deviation of 5 runs.
M
Model	S	T	S,T
MVAE	0.24±0.01	0.2O±0.05	0.32±0.03
MMVAE	0.75±0.06	0.99±0.01	0.87±0.03
MoPoE	0.74±0.04	0.99±0.01	0.94±0.01
		S	
Model	M	T	M,T
MVAE	0.43±0.02	0.30±0.08	0.75±0.04
MMVAE	0.31±0.03	0.30±0.04	0.30±0.03
MoPoE	0.36±0.07	0.34±0.06	0.37±0.06
		T	
Model	M	S	M,S
MVAE	0.28±0.06	0.17±0.02	0.29±0.06
MMVAE	0.96±0.01	0.76±0.04	0.84±0.02
MoPoE	0.96±0.01	0.76±0.03	0.93±0.01
Model	Joint Coherence		
MVAE		0.12±0.02	
MMVAE		0.28±0.01	
MoPoE		0.31±0.03	
C MNIST-SVHN-TEXT
C.1 Dataset
The dataset MNIST-SVHN-Text was introduced and described by Sutter et al. (2020). Equal to Shi
et al. (2019) in their bimodal experiment, we create 20 triples per set resulting in a many-to-many
mapping.
C.2 Experimental Setup
The latent space dimension is set to 20 for all modalities, models and runs. The results in tables 2 to 4
are generated with β = 5.0. We train all models for 150 epochs. We use the same architectures as in
Sutter et al. (2020). For MNIST encoder and decoder, we use fully-connected layers, for SVHN and
text encoders and decoder feed-forward convolutional layers. For all layers, we use ReLU-activation
functions (Nair & Hinton, 2010). The detailed architectures can also be looked up in the released
code. We use an Adam optimizer (Kingma & Ba, 2014) with an initial learning rate 0.001.
C.3 Additional Results
In table 6, we show the coherence results including the standard deviation of the 5 runs which were
removed from the main part due to space restrictions.
Additionally, we perform the analysis of coherence in relation to log-likelihood for conditional gen-
eration as well, similar to the example using random generation in section 4.1. The combination of
coherence and log-likelihoods shows the ability of a model to learn the data distribution as well as
the generation of coherent samples. Every point refers to a different β value. We evaluated the mod-
els for β = [0.5, 1.0, 2.5, 5.0, 10.0, 20.0]. The points in the figures are the mean values of5 different
runs with the lines being the standard deviations in both directions, coherence and log-likelihoods.
Figure 7 displays a qualitative comparison between the three models using 100 randomly generated
samples. The generated samples correspond to the numbers in section 4.1. MVAE is able to best
approximate the joint distribution in terms of sample quality for the price of a limited coherence,
16
Published as a conference paper at ICLR 2021
Log-Likelihood	Log-Likelihood	Log-Likelihood
(a) M,S → T	(b) M,T → S	(c) S,T → M
Figure 5: Coherence and Log-Likelihoods for MNIST-SVHN-Text. The three figures show the
evaluation for the conditional generation of a single modality given the other two in relation to the
joint log-likelihood given these two modalities, e.g. in the first row we generate SVHN samples
conditioned on MNIST and Text. The points in the figures are the mean values of 5 different runs
with the lines being the standard deviations in bopth directions, coherence and log-likelihoods.
(a) M → S
(b) M → T
(c) S → M
(d) S → T
(e) T → M
(f) T → S
Figure 6: Coherence and Log-Likelihoods for MNIST-SVHN-Text. The three rows of figures show
the evaluation for the conditional generation of two modalities given the remaining one in relation
to the joint log-likelihood given this single modality, e.g. in the first row we generate SVHN and
Text samples conditioned on MNIST. The points in the figures are the mean values of 5 different
runs with the lines being the standard deviations in both directions, coherence and log-likelihoods.
while MMVAE shows higher coherence but limited sample quality. MoPoE approximate MVAE’s
sample quality with a start-of-the-art coherence.
17
Published as a conference paper at ICLR 2021
Figure 7: Qualitative comparison of randomly generate MNIST-SVHN-Text samples.
In addition to the theoretical proof of Lemma 3 that Definition 1 minimizes the convex combination
of ELBOs, we compare the performance of a model trained using the objective in Equation (5) to
the proposed method MoPoE-VAE. It can be seen that MoPoE-VAE achieves competitive results to
the model which is optimizing Equation (5). This shows empirically that the proposed method is
indeed minimizing the convex combination of ELBOs in Equation (5). Equation (5) is extensively
minimizing the ELBO of every possible subset. Hence, Equation (5) is computationally much more
expensive to optimize.
Table 7: Comparison of objectives: Equation (5) and Definition 1. We report the test set log-
likelihoods of the joint generative model conditioned on the variational posterior of subsets of
modalities qφ(z∣Xk). (XM: MNIST; XS: SVHN; XT: Text; X = (XM, XS, XT)). For both ob-
jectives we use β = 2.5
Model	X	X|xM	X|xS	X|xT	X|xM, xS	X|xM , xT	X|xS, xT
EQ. (5)	-1810	-1993	-1831	-2039	-1811	-2000	-1839
MOPOE	-1815±12.4	-1990±4.4	-1858±13.2	-2024±1.2	-1819±13.4	-1986±2.5	-1848±11.5
18
Published as a conference paper at ICLR 2021
D POLYMNIST
D. 1 Dataset
For the creation of the PolyMNIST dataset, we fuse each MNIST image with a random crop of size
28x28 from the background image of the respective modality. In particular, we binarize the MNIST
image and invert the colors of the random crop at those locations where the binarized MNIST digit
is visible. We use the following background images:
1.	John Burkardt. Licensed under GNU LGPL. https://people.sc.fsu.edu/
~jburkardt/data/jpg/fractal_tree.jpg [Online; retrieved 27.09.2020]
2.	Edvard Munch. The Scream. Public domain. https://upload.wikimedia.org/
wikipedia/commons/f/f4/The_Scream.jpg [Online; retrieved 27.09.2020]
3.	The Waterloo Image Repository. Lena. Copyright belongs to the author.
http://links.uwaterloo.ca/Repository/TIF/lena3.tif [Online; re-
trieved 27.09.2020]
4.	John Burkardt. Licensed under GNU LGPL. https://people.sc.fsu.edu/
〜jburkardt/data/jpg/star_field.jpg [Online; retrieved 27.09.2020]
5.	John Burkardt. Licensed under GNU LGPL. https://people.sc.fsu.edu/
~ jburkardt/data/jpg/shingles.jpg [Online; retrieved 27.09.2020]
D.2 Experimental Setup
The latent space dimension is set to 512 for all modalities, models and runs. All results in are based
on β = 2.5, which was found to be a reasonable setting for all models. We use the same architectures
for all methods and train all models for 300 epochs. We use an Adam optimizer (Kingma & Ba,
2014) with an initial learning rate 0.001. The architecture is based on straightforward convolutional
neural networks (without bells and whistles); for details, we refer to the released code.
D.3 Qualitative Results
In Figures 8 to 11, we show qualitative results comparing the different methods.
7J
23
/ /■
H∙rr-B∙m<⅛
^■AA般 iil¾∙T∙
■■■33庖
i^D& 8^9夕配

爵百G簿芯
■ ∕∙J*∙G 3
OlJHs Ib
/ &■ 8 IlL
飞12为“ '7 3
•11=0哦■■■■■
运FN 3“苏方十9，公
Q Ut2 3 'U.S G 7 心
忌ΠilH名■宽立M∙
∣5bπe∣∣ ,P 二七
G，/.£4冕春V /版图
0 √ .Z 3 W ，i? Q
一川阳曜皿目■■因学
(a) MoPoE
(b) MMVAE
uTH
(c) MVAE
Q
¥7


叼

Figure 8: Reconstructions across all modalities for all models. In every pair of rows, we show one
row of test images followed by one row of respective reconstructions.
19
Published as a conference paper at ICLR 2021
Figure 9: Ten unconditionally generated images from the respective five modalities for each model.
Column-wise, we use the same latent codes, sampled from the prior. Note that, row-wise, the digits
should not be ordered.
(b) MMVAE
EIEmg
∣,>-H'>附能攵■回国匕
『,周a- d
(c) MVAE

Figure 10:	Conditionally generated images of the first modality given the respective test example
from the second modality shown in the first row. Column-wise, we take different samples from the
approximate posterior, which should result in stylistic variations for generated outputs, but which
should ideally not change the digit labels.
(a) MoPoE
g
⅞ħh ʒ 3 5 3 3 3 3
3 3 3 3
¾∙∙3=4
3间性，
呢■■■一
夕130Oz
整 ■ Ubggouggg?
泡面一幡「
y⅛ 7
(b) MMVAE
3 不曜■目a,BB* f ■ 3 ・
£期^7口 Z
∙∕,∙R∙m
O C, QQHΞ C Ξ- Dn".
4
¾c 0崛0|目 5 * τH6B,⅛5la
¥■131
(c) MVAE
8□xn∙
∙^∙≡r4>9ao0“ayH
/十Γ∙□o7"i≡F⅛⅛J ⑼7


Figure 11:	Conditionally generated images of the first modality given the four test examples from
the remaining modalities shown in the first four rows. Column-wise, we take different samples from
the approximate posterior, which should result in stylistic variations for generated outputs, but which
should ideally not change the digit labels. Compared to the results from Figure 10, the MoPoE-VAE
generates more coherent samples when conditioned on four instead of one input modality.
20
Published as a conference paper at ICLR 2021
E	Bimodal CelebA
E.1 Dataset
The bimodal version of CelebA was introduced by Sutter et al. (2020). The text modality consists
of strings which concatenate the attributes which are present in a face. If an attribute is not present,
it is not present in the string which makes it a more difficult modality. Example strings can be seen
in the top of Figure 4.
E.1.1 Modality-specific Latent Spaces
Modality-specific spaces empirically have empirically shown to be useful (Bouchacourt et al., 2018;
Hsu & Glass, 2018; Daunhawer et al., 2020; Sutter et al., 2020)—especially for the generative qual-
ity of samples. As CelebA is a visually challenging dataset, we adopt this idea and the ELBO for-
mulation changes accordingly. For details, we refer the reader to the beforehand mentioned papers.
The latent space is divided into a shared space qφc (c|X) and modality-specific spaces qφs (sj |xj)
for every modality xj . This allows every modality to encode information—which is specific to this
modality—in a separate latent space.
M
Le(θ, φ; X)= X Eqφc (c∣X)[Eqφsj (Sj∣Xj ) [logPθ (Xj lsj, C)]]	(34)
j=1
M1
-fDKL(lφsj (Sj lxj )llPθ (Sj )) - DKL(2M E ⅞φc(CIXk)llPθ (C))
j=1	Xk∈X
where qφc(c∣X) = 备 PXk∈χ qφc(c|Xk) models the shared information and q@s. (Sj|xj-) the
modality-specific information for every modality.
All posterior approximations, shared and modality-specific, are again assumed to be Gaussian dis-
tributed, see Appendix B.
E.2 Experimental Setup
The latent spaces are set to 32 dimensions for the shared space as well as the modality-specific
spaces, resulting in 64 dimensions per modality in total. We set β = 2.5 for all runs and models. All
models are trained for 200 epochs. Again we use the same architectures as in Sutter et al. (2020): the
encoders and decoders of both image and text use residual blocks (He et al., 2016). We use an Adam
optimizer (Kingma & Ba, 2014) with an initial learning rate 0.0005. The architectures can also be
looked up in the released code. The classification of samples and representations are evaluated using
average precision due to the imbalanced nature of the distribution of labels.
E.3 Additional Results
We show the attribute-specific evaluations in figs. 12 and 13 where the representations and generated
samples are evaluated specific to individual attributes. The evaluations are performed for all subsets
of modalities. We see the differences in averagea precision between attributes in the coherence of
samples as well as the latent representations. The correlation between learned representation and
coherence of samples gives further evidence on the importance of a good representation—also for
the multimodal setting and its task of conditional generation.
Figure 14 displays qualitative results of randomly generated samples. We can see the high quality
samples the proposed model is able to generate which cover a wide variety of attributes. In the
images, minor artefacts can be seen. This suggests that there is still room for improvement doing a
more rigorous hyper-parameter search.
21
Published as a conference paper at ICLR 2021
(a) Img
UQdOl⅞⅛=slq:InoW
ɪu-ew
SBUOCplθaqυl⅛I
P 一名
SvAwI,JgPUnISOeg
EtjE±i<
SMOJq3Aωlp3u:W
30 PeIl3 UoqolS
(b) Text
(c) Img and Text
Figure 12: Coherence of generated bimodal CelebA samples. For every subplot, image and text
are generated conditionally by the the modality or subset of modalities in the caption. We see that
different attributes are not learned equally well.
22
Published as a conference paper at ICLR 2021
6unoM
。一≤vvzl6u=eaM
3ue->pvNI6u 一
*UBSd'□l6u =e3M
-el-Γ6u1=e3M
S6uμ-gωl6uveυM
」«I-ΓA>eΛΛ
」nHJ⅛rabs
6,≡=es
Slunq3pω
SXBalPIASOH
3=H'≡Hlou 一 P3u3α:
QSONIAlU-Od
UpfSl3-ed
3ueLl-l-eΛ0
peaoilON
SaΛLLIIM0.ueN
31PelSnW
U3 dol⅞∙⅛=slq50w
Q-ew
s3uoqJI3BLI3l⅛x
dn*ewlA>e<υH
上 eHIAa!9
əaeOQ
S3sse-63ALLI
U≡tjl3-qno0
AqqnLP
SMQJ q3 AUJIA IlS ng
-l-eHIUΛΛ0-18
u≡c0
」«HlPUO_g
-IβHwem
3SONI6-g
Sdnl6ffl
v)6ueg
Ξs
S3ALLI^J3 PU nls6 eg
9>tJe」Ev
SM0」q3AUJlp3qu.v
Figure 13:	Learned Latent Representations for the bimodal CelebA dataset.
(a) MoPoE: Img
(b) MoPoE: Text
Figure 14:	Qualitative Results of randomly generated CelebA samples.
23