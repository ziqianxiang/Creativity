Published as a conference paper at ICLR 2021
Discovering Non-monotonic Autoregressive
Orderings with Variational Inference
Xuanlin Li； Brandon Trabucco； Dong Huk Park, Michael Luo
University of California, Berkeley
{xuanlinli17, btrabucco, dong.huk.park, michael.luo}@berkeley.edu
Sheng Shen, Trevor Darrell, Yang Gao
University of California, Berkeley; Tsinghua University
{sheng.s, trevordarrell}@berkeley.edu, gy20073@gmail.com
Ab stract
The predominant approach for language modeling is to encode a sequence of to-
kens from left to right, but this eliminates a source of information: the order by
which the sequence was naturally generated. One strategy to recover this infor-
mation is to decode both the content and ordering of tokens. Some prior work
supervises content and ordering with hand-designed loss functions to encourage
specific orders or bootstraps from a predefined ordering. These approaches require
domain-specific insight. Other prior work searches over valid insertion operations
that lead to ground truth sequences during training, which has high time complex-
ity and cannot be efficiently parallelized. We address these limitations with an un-
supervised learner that can be trained in a fully-parallelizable manner to discover
high-quality autoregressive orders in a data driven way without a domain-specific
prior. The learner is a neural network that performs variational inference with the
autoregressive ordering as a latent variable. Since the corresponding variational
lower bound is not differentiable, we develop a practical algorithm for end-to-end
optimization using policy gradients. Strong empirical results with our solution
on sequence modeling tasks suggest that our algorithm is capable of discovering
various autoregressive orders for different sequences that are competitive with or
even better than fixed orders.
1	Introduction
Autoregressive models have a rich history. Early papers that studied autoregressive models, such
as (Uria et al., 2016) and (Germain et al., 2015), showed an interest in designing algorithms that
did not require a gold-standard autoregressive order to be known upfront by researchers. However,
these papers were overshadowed by developments in natural language processing that demonstrated
the power of the left-to-right autoregressive order (Cho et al., 2014; Sutskever et al., 2014a). Since
then, the left-to-right autoregressive order has been essential for application domains such as image
captioning (Vinyals et al., 2015b; Xu et al., 2015), machine translation (Luong et al., 2015; Bahdanau
et al., 2015) and distant fields like image synthesis (van den Oord et al., 2016). However, interest in
non left-to-right autoregressive orders is resurfacing (Welleck et al., 2019b; Stern et al., 2019), and
evidence (Vinyals et al., 2016; Gu et al., 2018; Alvarez-Melis & Jaakkola, 2017) suggests adaptive
orders may produce more accurate autoregressive models. These positive results make designing
algorithms that can leverage adaptive orders an important research domain.
Inferring autoregressive orderings in a data-driven manner is challenging. Modern benchmarks
for machine translation (Stahlberg, 2019) and other tasks (Oda et al., 2015) are not labelled with
gold-standard orders, and left-to-right seems to be the default. This could be explained if domain-
independent methodology for identifying high-quality orders is an open question. Certain ap-
proaches (Stern et al., 2019; Welleck et al., 2019b; Ruis et al., 2020) use hand-designed loss func-
tions to promote a genre of orders—such as balanced binary trees. These loss functions incorporate
* Authors contributed equally.
1
Published as a conference paper at ICLR 2021
certain domain-assumptions: for example, they assume the balanced binary tree order will not dis-
rupt learning. Learning disruption is an important consideration, because prior work shows that
poor orders may prohibitively slow learning (Chen et al., 2018). Future approaches to inferring
autoregressive orders should withhold domain knowledge, to promote their generalization.
To our best knowledge, we propose the first domain-independent unsupervised learner that discovers
high-quality autoregressive orders through fully-parallelizable end-to-end training without domain-
specific tuning. We provide three main contributions that stabilize this learner. First, we propose
an encoder architecture that conditions on training examples to output autoregressive orders using
techniques in combinatorical optimization. Second, we propose Variational Order Inference that
learns an approximate posterior over autoregressive orders. Finally, we develop a practical algorithm
for solving the resulting non-differentiable ELBO end-to-end with policy gradients.
Empirical results with our solution on image captioning, code generation, text summarization, and
machine translation tasks suggest that with similar hyperparameters, our algorithm is capable of
recovering autoregressive orders that are even better than fixed orders. Case studies suggest that
our learned orders depend adaptively on content, and resemble a type of best-first generation order,
which first decodes focal objects and names. Our experimental framework is available at this link.
2	Related Works
Autoregressive Models Autoregressive models decompose the generation of a high dimensional
probability distribution by generating one dimension at a time, with a predefined order. Combined
with high capacity neural networks, this approach to modeling complex distributions has been very
successful (Sutskever et al., 2011; Mikolov et al., 2012). Recent works have achieved great im-
provements with autoregressive models in many applications, including language modeling (Rad-
ford et al., 2018; 2019; Brown et al., 2020), machine translation (Sutskever et al., 2014b) and image
captioning (Karpathy & Fei-Fei, 2015). Most previous works on autoregressive models use a fixed
ordering pre-defined by the designer with left-to-right emerging as the primary choice. In contrast,
our method is capable of learning arbitrary orderings conditioned on data and is more flexible.
Non-Monotonic Autoregressive Orderings Ford et al. (2018b) shows that a sub-optimal ordering
can severely limit the viability of a language model and propose to first generate a partially filled
sentence template and then fill in missing tokens. Previous works have also studied bidirectional
decoding (Sun et al., 2017; Zhou et al., 2019; Mehri & Sigal, 2018) and syntax trees based decoding
(Yamada & Knight, 2001; Charniak et al., 2003; Dyer et al., 2016; Aharoni & Goldberg, 2017; Wang
et al., 2018) in the natural language setting. However, all of the works mentioned above do not learn
the orderings and instead opt to use heuristics to define them. Chan et al. (2019) performs language
modeling according to a known prior, such as balanced binary tree, and does not allow arbitrary
sequence generation orders. Welleck et al. (2019a) proposes to use a tree-based recursive generation
method to learn arbitrary generation orders. However, their performance lags behind that of left-to-
right. Gu et al. (2019a) proposes Transformer-InDIGO to allow non-monotonic sequence generation
by first pretraining with pre-defined orderings, such as left-to-right, then fine-tuning use Searched
Adaptive Order (SAO) to find alternative orderings. They report that without pretraining, the learned
orders degenerate. In addition, they perform beam search when decoding each token during training,
which cannot be efficiently parallelized at the sequence length dimension. Emelianenko et al. (2019)
proposes an alternative to SAO, but suffers from similar poor time complexity. In contrast, our
method learns high-quality autoregressive orderings directly from data under fully-parallelizable
end-to-end training.
Variational Methods Our method optimizes the evidence lower bound, or ELBO in short. ELBO
is a quantity that is widely used as an optimization proxy in the machine learning literature, where
the exact quantity is hard to compute or optimize. Variational methods have achieved great success
in machine learning, such as VAE (Kingma & Welling, 2013) and β-VAE (Higgins et al., 2017).
Combinatorial Optimization Recent works have studied gradient-based optimization in the com-
binatorial space of permutations (Mena et al., 2018; Grover et al., 2019; Linderman et al., 2018).
These works have been applied in tasks such as number sorting, jigsaw puzzle solving, and neural
signal identification in worms. To our best knowledge, we are the first to build on these techniques
to automatically discover autoregressive orderings in vision and language datasets.
2
Published as a conference paper at ICLR 2021
3	Preliminaries
The goal of autoregressive sequence modelling is to model an ordered sequence of target values
y = (y1, y2 . . . , yn) : yi ∈ R, possibly conditioned on an ordered sequence of source values
x = (x1, x2 . . . , xm) : xi ∈ R, where (x, y) is sampled from the dataset D.
Inspired by Vinyals et al. (2015a) and Gu et al. (2019a), we formulate the generation process of y
as a 2n step process, where at time step 2t - 1 we generate a value, and at timestep 2t we select a
not-yet-chosen position in {1,2,…，n} to insert the value. Thus, We introduce the latent sequence
variable z = (z1 , z2 . . . , zn ) : z ∈ Sn, where Sn is the set of one-dimensional permutations of
{1,2,…，n}, and Zt is defined as the absolute position of the value generated at time step 2t - 1 in
the naturally ordered y. Then p(y, z|x) denotes the probability of generating y in the ordering of z
given the source sequence x. We can thus factorize p(y, z|x) using the chain rule:
n
p(y, z|x) = p(yz1 |x)p(z1 |yz1, x)	p(yzi|z<i,yz<i,x)p(zi|z<i,yz<=i,x)	(1)
i=2
For example, p(y1,y2,z1 = 2,z2 = 1|x) = p(y2|x)p(z1|y2,x)p(y1|z1,y2,x)p(z2|y1,z1,y2,x) is
defined as the probability of generating y2 in the first step, then inserting y2 into absolute position
2, then generating y1, and finally inserting y1 into absolute position 1.
Note that in practice, the length of y is usually varied. Therefore, We do not first create a fixed-length
sequence of blanks and then replace the blanks With actual values. Instead, We dynamically insert a
neW value at a position relative to the previous values. One common approach to predict such relative
position is Pointer NetWork (Vinyals et al., 2015a). In other Words, at timestep t, We insert the value
at position rt relative to the previous generated values. Here, for any z ∈ Sn, r = (r1, r2, . . . , rn)
is constructed such that there is a bijection betWeen Sn and the set of all constructed r. Due to such
bijection, We can use z and r interchangeably. We Will use z throughout the paper.
4	Variational Order Inference (VOI)
PoIiCy Gradient ∣7
Γ⅛
IOgqltI(Zly,x)	∣→[ Z >
I PernIUtatiOlI GeneratOr IT X ∣
I % ] y2【…I yn]
Figure 1: Computational Graph for Variational Order Inference
Starting from just the original data y in natural order, We can use variational inference to create an
objective (2) that alloWs us to recover latent order z, parametrized by tWo neural netWorks θ and φ.
The encoder netWork φ samples autoregressive orders given the ground truth data, Which the decoder
netWork θ uses to recover y. More specifically, φ is a non-autoregressive netWork (permutation
generator in Fig. 1) that takes in the source sequence x and the entire ground truth target sequence
y and outputs latent order z in a single forWard pass. θ is an autoregressive netWork (autoregressive
decoder in Fig. 1) that takes in x and predicts both the target sequence y and the ordering z through
the factorization in Equation (1). We name this process Variational Order Inference (VOI).
E(x,y)~D [log pθ (YIX)] = E(x,y)~D log Ez~qφ(z∣y,x) q^ (z,y |χ) ]]	(2)
≥ E(x,y)〜D [Ez〜qφ(z∣y,x) [logPθ (Y, z|x)] + Hqφ 卜卜，X)]
Here, Hqφ is the entropy term. During training, We train φ and θ jointly to maximize the ELBO in
(2). During testing, We only keep the decoder θ.
3
Published as a conference paper at ICLR 2021
To optimize the decoder network θ in (2), for each y, we first sample K latents
zι,z2,...,ZK from qφ(∙∣y, x). We then update θ using the Monte-Carlo gradient estimate
Ey〜D KP PK=I Vθ logpθ(y,Zi∣x)].
Algorithm 1 Variational Order Inference
1:	Given: encoder network φ with learning rate a@, decoder network θ with learning rate a§,
entropy coefficient β, batch of training data (X, Y) = {(xb, yb)}bN=1 sampled from dataset D
2:	Set gradient accumulators gφ = 0, gθ = 0
3:	for (x, y) ∈ (X, Y) do	. In practice, this is done through parallel tensor operations
4:	X = φ(y, x)
5:	Sample K doubly stochastic matrices B1, B2, . . . , BK ∈ Bn×n from G.S.(X, τ)
6:	Obtain P1, P2, . . . , PK ∈ Pn×n from B1, B2, . . . , BK using Hungarian Algorithm
7:	Obtain latents Z1, Z2, ...,ZK = fl-en1(y)(P1),fl-en1(y)(P2),. ..,fl-en1(y)(PK)
8:	gθ = gθ + NK PK=I Vθ logPθ(y,Zi∣x)
9:	Calculate log qφ(zi∣y, x) =(X, Pi)F - log(perm(exp (X)))
≈ hX, PiiF - log(permB (exp(X)))
10:	Calculate b(y, x) = KP PK=I logpθ(y, zi∣x)
11:	gφ = gφ + N1K PK=I Vφ logqφ(zi∖y, x)(logpθ(y, zi∣x) — b(y, x)) + β ∙ V0Hqφ(∙∣y, x)
12:	end for
13:	φ = φ + αφ ∙ gφ
14:	θ = θ + αθ ∙ gθ
Optimizing the encoder network φ is tricky. Since z is a discrete latent variable, the gradient from
log pθ (y, z) does not flow through z. Thus, we formulate (2) in a reinforcement learning setting with
a one-step Markov Decision Process (S, A, R). Under our setting, the state space S = D; for each
state (x,y) ∈ D, the action space A(x,y) = SIength(y) With entropy term Hq4(∙∖y,x); the reward
function R((x, y), z ∈ Slength(y)) = logpθ (y, z∖x). We can then set the optimization objective
L(φ) to be (2). In practice, we find that adding an entropy coefficient β and gradually annealing it
can speed up the convergence of decoder while still obtaining good autoregressive orders.
To compute VφL(φ), we derive the policy gradient with baseline formulation (Sutton et al., 2000):
VφL(φ) = E(x,y)〜D [Ez〜qφ [Vφ logqφ(z∖y, x)(logpθ(y,z∖x) - b(y, x))] + βVφHq.] (3)
where b(y, x) is the baseline function independent of action z. The reason we use a state-dependent
baseline b(y, x) instead ofa global baseline b is that the the length ofy can have a wide range, caus-
ing significant reward scale difference. In particular, We set b(y, x) = EZ 〜q. [log Pθ (y, zi∖x)]. If We
sample K ≥ 2 latents for each y, then we can use its Monte-Carlo estimate K PK=1 logpθ (y, zi∖x).
Since we use policy gradient to optimize φ, we still need a closed form for the distribution
qφ(z∖y, x). Before we proceed, we define Pn×n as the set of n × n permutation matrices, where
exactly one entry in each row and column is 1 and all other entries are 0; Bn×n as the set of n × n
doubly stochastic matrices, i.e. non-negative matrices whose sum of entries in each row and in each
column equals 1; Rn+×n as the set of non-negative n × n matrices. Note that we have the relationship
Pn×n ⊂ Bn×n ⊂ Rn×n .
To obtain qφ(z∖y, x), we first write z in two-dimensional form. For each z ∈ Sn, let fn(z) ∈ Pn×n
be constructed such that fn(z) = one_hot(zi), where fn(z) is the i-th row of fn(z). Thus f is
a natural bijection from Sn to Pn×n , and we can rewrite qφ as a distribution over Pn×n such that
qφ(fn(z)∖y, x) = qφ(z∖y, x).
Next, we need to model the distribution of qφ(∙∖y, x). Inspired by (Mena et al., 2018), we model
qφ(∙∖y, x) as a Gumbel-Matching distribution G.M.(X) over Pn1×n, where X = φ(y, x) ∈ Rn×n
is the output of φ. Then for P ∈ Pn×n ,
qφ(z∖y, x) = qφ(f-1(P)∖y,x) = qφ(P∖y, x) B exp (X,P)f	(4)
where hX, PiF = trace(XT P) is the Frobenius inner product of X and P. To obtain sam-
ples in Pn×n from the Gumbel-Matching distribution, (Mena et al., 2018) relaxes Pn×n to Bn×n
by defining the Gumbel-Sinkhorn distribution G.S.(X, τ) : τ > 0 over Bn×n, and proves that
4
Published as a conference paper at ICLR 2021
G.S.(X, τ) converges almost surely to G.M.(X) as τ → 0+. Therefore, to approximately sample
from G.M.(X), we first sample from G.S.(X, τ), then apply Hungarian algorithm (Munkres, 1957)
to obtain P ∈ G.M.(X). Further details are presented in Appendix A.
The Gumbel-Matching distribution allows us to obtain the numerator for the closed form of
qφ(Zk x) = qφ(f-1(P)|y, x), which equals exphX,P〉f. However, the denominator is in-
tractable to compute and equals PP∈P exp hX, PiF. Upon further examination, we can express
it as perm(exp(X)), the matrix permanent of exp(X), and approximate it using permB (exp(X)),
its Bethe permanent. We present details about matrix permanent and Bethe permanent along with
the proof that the denominator of qφ(∙∣y, x) equals perm(exp(X)) in Appendix B.
After we approximate qφ, we can now optimize φ using the policy gradient in (3). We present
a diagram of our architecture in Figure 1, and a pseudocode of our algorithm in Algorithm 1.
Note that even though latent space Sn is very large and contains n! permutations, in practice, if
Pθ(y, z*|x) ≥ pθ(y, z|x) ∀z ∈ Sn, then pθ(y, z|x) tends to increase as the edit distance between
z and Z decreases. Therefore, φ does not need to search over the entire latent to obtain good
permutations, making variational inference over Sn feasible.
5	Experiments
Encoder and Decoder Architecture. We present encoder and decoder architectures for Variational
Order Inference on conditional sequence generation tasks, which we focus on in this work. Note
that Algorithm 1 is also applicable to unconditional sequence generation domains, such as image
generation, through different encoder and decoder architectures. We leave this for future work.
For decoder θ, we use the Transformer-InDIGO (Gu et al., 2019a) architecture, which builds on
Transformer with relative position representations (Shaw et al., 2018) to allow sequence generation
through insertion operations. Note that “encoder” and “decoder” in this section refer to the two
networks φ and θ in Algorithm 1, respectively, instead of Transformer’s encoder and decoder. Also,
we obtain orderings through the output of encoder instead of through Searched Adaptive Order
(SAO). To our best effort, we were unable to obtain the official implementation of Transformer-
InDIGO, so we reimplemented the algorithm based on the paper’s descriptions.
As a side note, rather than generating y autoregressively in 2n steps as done in Transformer-InDIGO,
it is possible to use a non-autoregressive decoder instead and improve the decoding speed. There
have recently been many works on non-autoregressive conditional sequence generation (Gu et al.,
2018; 2019b; Ma et al., 2019; Bao et al., 2019). To train a non-autoregressive decoder Transformer,
we can incorporate the ordering information generated by our encoder network into the decoder
Transformer’s encoder latent output. We leave this for future work.
For encoder φ, we adopt the Transformer (Vaswani et al., 2017) architecture. Note that our encoder
generates latents based on the entire ground truth target sequence y. Therefore, it does not need to
mask out subsequent positions during attention. We also experiment with different position embed-
ding schemes (see Section 7) and find that Transformer-XL’s (Dai et al., 2019) relative positional
encoding performs the best, so we replace the sinusoid encoding in the original Transformer.
Tasks. We evaluate our approach on challenging sequence generation tasks: natural language to
code generation (NL2Code) (Ling et al., 2016), image captioning, text summarization, and machine
translation. For NL2Code, we use Django (Oda et al., 2015). For image captioning, we use COCO
2017 (Lin et al., 2015). For text summarization, we use English Gigaword (Graff et al., 2003; Rush
et al., 2015). For machine translation, we use WMT16 Romanian-English (Ro-En).
Baselines. We compare our approach with several pre-defined fixed orders: Left-to-Right (L2R)
(Wu et al., 2018), Common-First (Common) (Ford et al., 2018a), Rare-First (Rare) (Ford et al.,
2018a), and Random-Ordering (Random). Here, Common-First order is defined as generating words
with ordering determined by their relative frequency from high to low; Rare-First order is defined
as the reverse of Common-First order; and Random-Ordering is defined as training with a randomly
sampled order for each sample at each time step.
Preprocessing. For Django, we adopt the same preprocessing steps as described in (Gu et al.,
2019a), and we use all unique words as the vocabulary. For MS-COCO, we find that the baseline in
Gu et al. (2019a) is much lower than commonly used in the vision and language community. There-
5
Published as a conference paper at ICLR 2021
fore, instead of using Resnet-18, we use the pretrained Faster-RCNN checkpoint using a ResNet-50
FPN backbone provided by TorchVision to extract 512-dimensional feature vectors for each object
detection. To make our model spatially-aware, we also concatenate the bounding box coordinates
for every detection before feeding into our Transformers’ encoder. For Gigaword and WMT, we
learn 32k byte-pair encoding (BPE, Sennrich et al. (2016)) on tokenized data.
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Runtime Improvement
)sdnoceS( petS reP emi
SAO
L2R
VOI
Training Method
Runtime Improvement
Sequence Length (Tokens)
Runtime Improvement
ro2tcaF 2pudeepS
Sequence Length (Tokens)
Figure 2: Runtime performance improvement. We compare the runtime performance of VOI
(K = 4) with SAO on a single Tesla P100 GPU, in terms of time per training iteration and ordering
search time. VOI outputs latent orderings in a single forward pass, and we observe a significant
runtime improvement over SAO that searches orderings sequentially. The speedup factor linearly
increases with respect to the sequence length.
Order		MS-COCO			Django		Gigaword			WMT16 Ro-En		
	BLEU	Meteor	R-L	CIDEr BLEU Accuracy			R-1	R-2	R-L	BLEU	Meteor TER	
InDIGO - SAO 1	29.3	24.9	54.5	92.9	42.6	32.9	—	—	—	32.5	53.0	49.0
Ours - Random	28.9	24.2	55.2	92.8	21.6	26.9	30.1	11.6	27.6			
Ours - L2R	30.5	25.3	54.5	95.6	40.5	33.7	35.6	17.2	33.2	32.7	54.4	50.2
Ours - Common	28.0	24.8	55.5	90.3	37.1	29.8	33.9	15.0	31.1	27.4	50.1	53.9
Ours - Rare	28.1	24.5	52.9	91.4	31.1	27.9	34.1	15.2	31.3	26.0	48.5	55.1
Ours - VOI	31.0	25.7	56.0	100.6	44.6	34.3	36.6	17.6 34.0		32.9	54.6	49.3
Table 1: Results of MS-COCO, Django, Gigaword, and WMT with fixed orders (L2R, Random,
Common, Rare) as baseline. Here, R-1, R-2, and R-L indicate ROUGE-1, ROUGE-2, and ROUGE-
L, respectively. For TER, lower is better; for all other metrics, higher is better. “一” = not reported.
Training. For our decoder, we set dmodel = 512, dhidden = 2048, 6 layers for both Transformer’s
encoder and decoder, and 8 attention heads. This is the same model configuration as Transformer-
Base (Vaswani et al., 2017) and as described in Gu et al. (2019a). Our encoder also uses the same
configuration. For our model trained with Variational Order Inference , we sample K = 4 latents
for each training sample. An ablation on the choices of K is presented in Section 7. For WMT,
many previous works on nonsequential orderings (Stern et al., 2019) and nonautoregressive sequence
generation (Gu et al., 2019b) have found sequence-level knowledge distillation (Kim & Rush, 2016)
helpful. Therefore, we first train the L2R model on the original WMT corpus, then create a new
training corpus using beam search. We find that this improves the BLEU of VOI model by about
2. Even though the training set changed, the orderings learned by VOI are very similar to the ones
trained on the original corpus. More detailed training processes are described in Appendix C.
During training, our encoder outputs the latent ordering through one single forward pass, and our
decoder can predict all tokens with their positions given by the latent ordering in one single forward
pass. If we let N denote the batch size, l denote the length of each target sequence, and d denote
the size of hidden vector, then one single forward pass of our model has computation complexity
O(N Kdl2), while Transformer-InDIGO trained with SAO has complexity O(N dl3). Since K l
in general, our algorithm has better theoretical computational complexity during training. During
evaluation, we only keep the decoder to iteratively generate the next position and token, which is as
efficient as any standard fixed-order autoregressive models.
1For InDIGO-SAO, we report the results on COCO and Django trained using our own implementation. We
did not attempt SAO on Gigaword or WMT due to the large dataset sizes, which can take 100 days to train.
For WMT, we report the SAO result as in the original paper, and we follow their evaluation scheme (results are
case-sensitive). The BLEU scores are obtained through SacreBLEU. Evaluation scripts are open-sourced.
6
Published as a conference paper at ICLR 2021
We also empirically compare VOI’s runtime with that of SAO and fixed-order baselines (e.g. L2R).
We implement SAO as described in Gu et al. (2019a). We test the runtime on a single GPU in order
to accurately measure the number of ops required. For training speed per iteration, we use a batch
size of 8. For ordering search time, we use a batch size of 1 to avoid padding tokens in the input for
accurate measure. We observe that VOI is significantly faster than SAO, which searches orderings
sequentially. In practice, as we distribute VOI across more GPUs, the K factor in the runtime is
effectively divided by the number of GPUs used (if we ignore the parallelization overhead), so we
can achieve further speedups.
Results. We compare VOI against predefined orderings along with Transformer-InDIGO trained
with SAO in Table 1. The metrics we used include BLEU-4 (Papineni et al., 2002), Meteor
(Denkowski & Lavie, 2014), Rouge (Lin, 2004), CIDEr (Vedantam et al., 2015), and TER (Snover
et al., 2006). The ”accuracy” reported for Django is defined as the percentage of perfect matches in
code generation. Our results illustrate consistently better performance across fixed orderings. Most
notably, CIDEr for MS-COCO, BLEU for Django, and Rouge-1 for Gigaword reveal the largest
improvements in performance.
6 Order Analysis
In this section, we analyze the generation orders learned by Variational Order Inference on a macro
level by comparing the similarity of our learned orders with predefined orders defined in Section 5,
and on a micro level, by inspecting when the model generates certain types of tokens.
Figure 3: Global statistics for learned orders. We compare metrics as a function of the sequence
length of generated captions on the COCO 2017 validation set. On the left, we compare orders
learned with Variational Order Inference to a set of predefined orders (solid lines) using Order Rank
Correlation. As a reference, we provide the Order Rank Correlation between L2R and the same set
of predefined orders (dashed lines). In the right plot, with identical setup, we measure Normalized
Levenshtein Distance. We observe that Variational Order Inference favors left-to-right decoding
above the other predefined orders—this corresponds to the blue lines. However, with a max Order
Rank Correlation of 0.6, it appears left-to-right is not a perfect explanation. The comparably high
Order Rank Correlation of 0.3 with rare-tokens-first order suggests a complex strategy.
6.1	Understanding The Model Globally
We find that prior work (Gu et al., 2019a; Welleck et al., 2019a; Gu et al., 2018) tends to study
autoregressive orders by evaluating performance on validation sets, and by visualizing the model’s
generation steps. We provide similar visualizations in Appendix F.3. However, this does not merit a
quantitative understanding of the strategy that was learned. We address this limitation by introducing
methodology to quantitatively study decoding strategies learned by non-monotonic autoregressive
models. We introduce Normalized Levenshtein Distance and Order Rank Correlation, to measure
similarity between decoding strategies. Given two generation orders w, z ∈ Sn of the same se-
quence y, where n is the length of y, we define the Normalized Levenshtein Distance.
DNLD (w, z) = lev (w, z) /n	(5)
lev (w, z) = 1 + min {lev (w1:,z) ,lev (w, z1:) ,lev(w1:,z1:)}	(6)
The function lev (w, z) is the Levenshtein distance, and z1: removes the first element of z. This
metric has the property that a distance of 0 implies that two orders w and z are the same, while a
distance of 1 implies that the same tokens appear in distant locations in w and z. Our second metric
Order Rank Correlation, is the Spearman’s rank correlation coefficient between w and z.
DORC (w, z) = 1 - 6 ∙ Pn=0 (Wi-Zi) / (n3 - n)	(7)
7
Published as a conference paper at ICLR 2021
A correlation of 1 implies that w and z are the same; a correlation of -1 implies that w and z are
reversed; and a correlation of 0 implies that w and z are not correlated. In Figure 3, we apply these
metrics to analyze our models learnt through Variational Order Inference .
9 8 6 4 2 0
1.。 S S S S
UoQEOoi PeZ=BE-IoN
Figure 4: Local statistics for learned orders. In this figure, we evaluate the normalized generation
indices for different parts of speech in model-predicted captions on the COCO 2017 validation set.
The normalized generation index is defined as the absolute generation index of a particular token,
divided by the final length of predicted sequence. The parts of speech (details in Appendix E) are
sorted in ascending order of their average normalized location. We observe that modifier tokens,
such as “the”, tend to be decoded last, while descriptive tokens, such as nouns and verbs, tend to be
decoded first.
Discussion. The experiment in Figure 3 confirms our model’s behavior is not well explained by
predefined orders. Interestingly, as the generated sequences increase in length, the Normalized Lev-
enshtein Distance decreases, reaching a final value of 0.57, indicating that approximately half of
the tokens are already arranged according to a left-to-right generation order. However, the Order
Rank Correlation barely increases, so we can infer that while individual tokens are close to their
left-to-right generation index, their relative ordering is not preserved. Our hypothesis is that certain
phrases are generated from left-to-right, but their arrangement follows a best-first strategy.
6.2	Understanding The Model Locally
To complement the study of our model at a global level, we perform a similar study on the micro
token level. Our hope is that a per-token metric can help us understand if and when our Variational
Order Inference is adaptively choosing between left-to-right and rare-first order. We also hope to
evaluate our hypothesis that Variational Order Inference is following a best-first strategy.
Discussion. The experiment in Figure 4 demonstrates that Variational Order Inference prefers
decoding descriptive tokens first—such as nouns, numerals, adverbs, verbs, and adjectives. In addi-
tion, the unknown part of speech is typically decoded first, and we find this typically corresponds to
special tokens such as proper names. Our model appears to capture the salient content first, which is
illustrated by nouns ranking second in the generation order statistics. For image captioning, nouns
typically correspond to focal objects, which suggests our model has an object-detection phase. Evi-
dence of this phase supports our previous hypothesis that a best-first strategy is learned.
6.3	Understanding The Model Via Perturbations
In this section, We study the question: to What
extent is the generation order learned by Vari-
ational Order Inference dependent on the con-
tent of the conditioning variable x? This ques-
tion is important because simply knowing that
our model has learned a best-first does not il-
luminate whether that strategy depends only on
the target tokens y being generated, or if it also
depends on the content of x. An adaptive gen-
eration order should depend on both.
Image ID： 000000001584
6 5 4 3 2
■ ■ ■ ■ ■
O O O O O
∙ue3s-α u-UMPdN=euj」Oflj

Discussion. In this experiment, we first obtain a sequence y generated by our VOI given the source
image x. We then freeze y, which allows the model to infer a new generation order for y when
8
Published as a conference paper at ICLR 2021
different features of x are removed. Figure 6.3 shows that for a particular case, removing a single
region-feature (feature number 0, which corresponds to the bus) from x changes the model-predicted
generation order by as much as 0.7 Normalized Levenshtein Distance. These results confirm that our
model appears to learn an adaptive strategy, which depends on both the tokens y being generated
and the content of the conditioning variable x, which is an image in this experiment.
7 Ablation Studies
In Section 5, we introduced the specific
encoder and decoder architectures we use
for conditional sequence generation tasks.
In this section, we present ablation stud-
ies to support the architecture design of
our encoder and modeling qφ with Gumbel-
Matching distribution.
We consider 4 different positional encod-
ing schemes for the encoder Transformer φ:
the sinusoid encoding in the original Trans-
Table 2: Normalized Levenshtein Distance between
the ordering learnt by the encoder and the ground
truth ordering, under different positional encodings
(enc) and modeling distributions of qφ (distrib).
Enc \ Distrib	GUmbel-MatChing	Plackett-LUce
SinUsoid	0.40	0.62
SinUsoid + Pos Attn	0.42	0.58
Relative	0.38	0.53
XL-Relative	0.25	0.57
former (Vaswani et al., 2017), the sinusoid encoding with positional attention module (Gu et al.,
2018), the relative positional encoding in Shaw et al. (2018), and the relative positional encoding
proposed in Transformer-XL (Dai et al., 2019). Besides modeling qφ(∙∣x, y) as GUmbel-Matching
distribution and using Bethe permanent to approximate its denominator, we also consider modeling
Using Plackett-LUce distribUtion (Plackett, 1975; LUce, 1959) and sample Using techniqUes recently
proposed in Grover et al. (2019). Plackett-LUce distribUtion has tractable density, so we can compUte
the exact qφ efficiently withoUt Using approximation techniqUes.
To analyze the encoder’s ability to learn aUtoregressive orderings, we first train a decoder with
Common-First order on one batch of MS-COCO Until it perfectly generates each sentence. We then
fix the decoder and initialize an encoder. We train the encoder for 15k gradient steps Using the
procedUre in Algorithm 1 to recover the groUnd trUth Common-First order, and we report the final
Normalized Levenshtein Distance against the groUnd trUth in Table 2. We observe that modeling qφ
with GUmbel-Matching distribUtion significantly oUtperforms modeling with Plackett-LUce, despite
the former reqUiring denominator approximation. We also observe that Under GUmbel-Matching
modeling distribUtion, the relative position encoding in Transformer-XL significantly oUtperforms
other encoding schemes. ThUs we combine these two techniqUes in oUr architectUre design.
In addition, we analyze how the choice of K, the Table 3:
Normalized Levenshtein Dis-
nUmber of latents per training sample, affects model
performance. We Use the same setting as above
and apply Transformer-XL relative position encod-
ing, and we report the resUlts in Table 3. We observe
that the encoder more accUrately fits to the groUnd
tance between the encoder ordering and the
groUnd trUth with respect to the choice of K.
K I 2	3	4	10 20
DNLD 0.31 0.28 0.25 0.21 0.21
trUth order as K increases, Until a valUe of aroUnd 10.
Since a very large K can slow the model
down while only bringing marginal improvement, a choice of K from 4 to 10 is sUfficient.
8 Conclusion
We propose, to oUr best knowledge, the first UnsUpervised learner that learns high-qUality aUtore-
gressive orders throUgh fUlly-parallelizable end-to-end training withoUt domain-specific tUning. We
propose a procedUre named Variational Order Inference that Uses the Variational Lower BoUnd with
the space of aUtoregressive orderings as latent. BUilding on techniqUes in combinatorical optimiza-
tion, we develop a practical policy gradient algorithm to optimize the encoder of the variational
objective, and we propose an encoder architectUre that conditions on training examples to oUtpUt
aUtoregressive orders. Empirical resUlts demonstrate that oUr model is capable of discovering aU-
toregressive orders that are competitive with or even better than fixed and predefined orders. In
addition, the global and local analysis of the orderings learned throUgh Variational Order Inference
sUggest that they resemble a type of best-first generation order, characterized by prioritizing the
generation of descriptive tokens and deprioritizing the generation of modifier tokens.
9
Published as a conference paper at ICLR 2021
References
Roee Aharoni and Yoav Goldberg. Towards string-to-tree neural machine translation. arXiv preprint
arXiv:1704.04743, 2017.
David Alvarez-Melis and Tommi S. Jaakkola. Tree-structured decoding with doubly-recurrent
neural networks. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL
https://openreview.net/forum?id=HkYhZDqxg.
Nima Anari and Alireza Rezaei. A tight analysis of bethe approximation for permanent. 2019 IEEE
60th Annual Symposium on Foundations ofComputer Science (FOCS), pp.1434-1445, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Con-
ference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.
Yu Bao, Hao Zhou, Jiangtao Feng, Mingxuan Wang, Shujian Huang, Jiajun Chen, and Lei LI. Non-
autoregressive transformer by position learning, 2019.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. Kermit: Generative
insertion-based modeling for sequences, 2019.
Eugene Charniak, Kevin Knight, and Kenji Yamada. Syntax-based language models for statistical
machine translation. In Proceedings ofMT Summit IX, pp. 40-46. Citeseer, 2003.
Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved au-
toregressive generative model. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 863-
871. PMLR, 2018. URL http://proceedings.mlr.press/v80/chen18h.html.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans
(eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pp. 1724-1734. ACL, 2014. doi: 10.3115/v1/d14-1179. URL
https://doi.org/10.3115/v1/d14-1179.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, Florence,
Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL
https://www.aclweb.org/anthology/P19-1285.
Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation
for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine
Translation, 2014.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. arXiv preprint arXiv:1602.07776, 2016.
Dmitrii Emelianenko, Elena Voita, and Pavel Serdyukov. Sequence modeling with un-
constrained generation order. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 32: Annual Conference on Neural In-
formation Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancou-
ver, BC, Canada, pp. 7698-7709, 2019. URL http://papers.nips.cc/paper/
8986-sequence-modeling-with-unconstrained-generation-order.
10
Published as a conference paper at ICLR 2021
N. Ford, Daniel Duckworth, Mohammad Norouzi, and G. Dahl. The importance of generation order
in language modeling. ArXiv, abs/1808.07910, 2018a.
Nicolas Ford, Daniel Duckworth, Mohammad Norouzi, and George E Dahl. The importance of
generation order in language modeling. arXiv preprint arXiv:1808.07910, 2018b.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. volume 37 of Proceedings of Machine Learning Research, pp. 881-889,
Lille, France, 07-09 Jul 2015. PMLR. URL http://Proceedings .mlr.ρress∕v37∕
germain15.html.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword, 2003.
Aditya Grover, E. Wang, Aaron Zweig, and S. Ermon. Stochastic optimization of sorting networks
via continuous relaxations. ArXiv, abs/1903.08850, 2019.
Jetic Gu, Hassan S. Shavarani, and Anoop Sarkar. Top-down tree structured decoding with syntactic
connections for neural machine translation and parsing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, pp. 401-413, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1037. URL
https://www.aclweb.org/anthology/D18-1037.
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. Non-
autoregressive neural machine translation. In 5th International Conference on Learning Rep-
resentations, 2018.
Jiatao Gu, Qi Liu, and Kyunghyun Cho. Insertion-based decoding with automatically inferred gen-
eration order. Transactions of the Association for Computational Linguistics, 7:661-676, 2019a.
Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 32, pp. 11181-11191. Curran Asso-
ciates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/
675f9820626f5bc0afb47b57890b466e-Paper.pdf.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?id=Sy2fzU9gl.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3128-3137, 2015.
Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natural Language Processing, pp. 1317-1327, Austin,
Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139.
URL https://www.aclweb.org/anthology/D16- 1139.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguis-
tics. URL https://www.aclweb.org/anthology/W04- 1013.
11
Published as a conference paper at ICLR 2021
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro
Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects
in context, 2015.
Scott Linderman, Gonzalo Mena, Hal Cooper, Liam Paninski, and John Cunningham. Reparame-
terizing the birkhoff polytope for variational permutation inference. volume 84 of Proceedings of
Machine Learning Research, pp. 1618-1627, Playa Blanca, Lanzarote, Canary Islands, 09-11 Apr
2018. PMLR. URL http://proceedings.mlr.press/v84/linderman18a.html.
W. Ling, P. Blunsom, Edward Grefenstette, K. Hermann, TomaS Kocisky, Fumin Wang, and A. Se-
nior. Latent predictor networks for code generation. ArXiv, abs/1603.06744, 2016.
R. Duncan Luce. Individual Choice Behavior: A Theoretical analysis. Wiley, New York, NY, USA,
1959.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Lluls Marquez, Chris Callison-Burch, Jian Su, Daniele Pighin,
and Yuval Marton (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1412-
1421. The Association for Computational Linguistics, 2015. doi: 10.18653/v1/d15-1166. URL
https://doi.org/10.18653/v1/d15-1166.
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. FlowSeq: Non-
autoregressive conditional sequence generation with generative flow. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4282-
4292, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1437. URL https://www.aclweb.org/anthology/D19- 1437.
Shikib Mehri and Leonid Sigal. Middle-out decoding. In Advances in Neural Information Process-
ing Systems, pp. 5518-5529, 2018.
Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations
with gumbel-sinkhorn networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=Byt3oJ-0W.
Gonzalo Mena, Erdem Varol, Amin Nejatbakhsh, Eviatar Yemini, and Liam Paninski. Sinkhorn
permutation variational marginal inference. volume 118 of Proceedings of Machine Learning
Research, pp. 1-9. PMLR, 08 Dec 2020. URL http://proceedings.mlr.press/v118/
mena20a.html.
TomaS Mikolov et al. Statistical language models based on neural networks. Presentation at Google,
Mountain View, 2nd April, 80:26, 2012.
James R. Munkres. Algorithms for the Assignment and Transportation Problems. Journal of the
Society for Industrial and Applied Mathematics, 5(1):32-38, March 1957.
Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. Learning to generate pseudo-code from source code using statistical machine
translation. In Proceedings of the 2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE), ASE ’15, pp. 574-584, Lincoln, Nebraska, USA, November 2015.
IEEE Computer Society. ISBN 978-1-5090-0025-8. doi: 10.1109/ASE.2015.36. URL https:
//doi.org/10.1109/ASE.2015.36.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002.
Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//www.aclweb.org/anthology/P02-1040.
Robin L Plackett. The analysis of permutations. pp. 193-202, 1975.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018.
12
Published as a conference paper at ICLR 2021
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.
Laura Ruis, Mitchell Stern, Julia Proskurnia, and William Chan. Insertion-deletion transformer.
CoRR, abs/2001.05540, 2020. URL https://arxiv.org/abs/2001.05540.
Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive
sentence summarization. Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, 2015. doi: 10.18653/v1/d15-1044. URL http://dx.doi.org/10.
18653/v1/D15-1044.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.
org/anthology/P16-1162.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-
tions. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.
464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18- 2074.
Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic ma-
trices. Ann. Math. Statist., 35(2):876-879, 06 1964. doi: 10.1214/aoms/1177703591. URL
https://doi.org/10.1214/aoms/1177703591.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of
translation edit rate with targeted human annotation. In In Proceedings of Association for Machine
Translation in the Americas, pp. 223-231, 2006.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. 15(1):1929-1958, January
2014. ISSN 1532-4435.
Felix Stahlberg. Neural machine translation: A review. ArXiv, abs/1912.02047, 2019.
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible
sequence generation via insertion operations. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-
15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research, pp. 5976-5985. PMLR, 2019. URL http://proceedings.mlr.press/v97/
stern19a.html.
Qing Sun, Stefan Lee, and Dhruv Batra. Bidirectional beam search: Forward-backward inference
in neural sequence models for fill-in-the-blank image captioning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 6961-6969, 2017.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural net-
works. In ICML, 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and
Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: An-
nual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon-
treal, Quebec, Canada, pp. 3104-3112, 2014a. URL http://papers.nips.cc/paper/
5346-sequence-to-sequence-learning-with-neural-networks.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014b.
13
Published as a conference paper at ICLR 2021
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy
gradient methods for reinforcement learning with function approximation. In S. A. Solla,
T. K. Leen, and K. Muller (eds.), Advances in Neural Information Processing Systems
12, pp. 1057-1063. MIT Press, 2000. URL http://papers.nips.cc/paper/
1713- policy- gradient- methods- for- reinforcement- learning- with- function- approximatio
pdf.
Benigno Uria, Marc-Alexandre C0te, Karol Gregor, Iain Murray, and Hugo Larochelle. NeU-
ral autoregressive distribution estimation. J. Mach. Learn. Res., 17:205:1-205:37, 2016. URL
http://jmlr.org/papers/v17/16-272.html.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol
Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In
Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Ro-
man Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pp. 4790-4798, 2016. URL http://papers.nips.cc/paper/
6527-conditional-image-generation-with-pixelcnn-decoders.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L UkaSz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neu-
ral Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181- attention- is- all- you- need.pdf.
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In CVPR, pp. 4566-4575. IEEE Computer Society, 2015. ISBN 978-
1-4673-6964-0. URL http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015.
html#VedantamZP15.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Corinna Cortes, Neil D.
Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2692-2700, 2015a. URL
http://papers.nips.cc/paper/5866- pointer- networks.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015,
Boston, MA, USA, June 7-12, 2015, pp. 3156-3164. IEEE Computer Society, 2015b. doi: 10.
1109/CVPR.2015.7298935. URL https://doi.org/10.1109/CVPR.2015.7298935.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representa-
tions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.
URL http://arxiv.org/abs/1511.06391.
P. O. Vontobel. The bethe permanent of a non-negative matrix. In 2010 48th Annual Allerton
Conference on Communication, Control, and Computing (Allerton), pp. 341-346, 2010.
Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham Neubig. A tree-based decoder for neural
machine translation. arXiv preprint arXiv:1808.09374, 2018.
Sean Welleck, Kiante Brantley, Hal DaUme III, and Kyunghyun Cho. Non-monotonic sequential
text generation. arXiv preprint arXiv:1902.02192, 2019a.
Sean Welleck, Kiante Brantley, Hal DaUme III, and Kyunghyun Cho. Non-monotonic sequential
text generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6716-6726. PMLR,
2019b. URL http://proceedings.mlr.press/v97/welleck19a.html.
14
Published as a conference paper at ICLR 2021
Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Beyond error
propagation in neural machine translation: Characteristics of language also matter. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3602-3611,
Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1396. URL https://www.aclweb.org/anthology/D18- 1396.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
ume 37 of JMLR Workshop and Conference Proceedings, pp. 2048-2057. JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/xuc15.html.
Kenji Yamada and Kevin Knight. A syntax-based statistical translation model. In Proceedings of
the 39th Annual Meeting of the Association for Computational Linguistics, pp. 523-530, 2001.
Long Zhou, Jiajun Zhang, and Chengqing Zong. Synchronous bidirectional neural machine transla-
tion. Transactions of the Association for Computational Linguistics, 7:91-105, 2019.
15
Published as a conference paper at ICLR 2021
Appendix
A Gumbel-Matching Distribution and its Sampling
In Section 4, We model the distribution of qφ(∙∣y, x) as a GUmbel-Matching distribution G.M.(X)
over Pn×n, where X = φ(y, x) ∈ Rn×n is the latent output.
To obtain samples in Pn×n from the Gumbel-Matching distribution, Mena et al. (2018) relaxes
Pn×n to Bn×n by defining the Gumbel-Sinkhorn distribution G.S.(X, τ) : τ > 0 over Bn×n. Here
we reproduce the following definitions and theorems with similar notations from Sinkhorn (1964)
and Mena et al. (2018):
Definition A.1. Let X ∈ Rn×n and A ∈ Rn+×n. The Sinkhorn Operator S is defined as
Tr(A) =A	(A1n1Tn)
Tc(A) = A	(1n1TnA)
T(A) =Tr(Tc(A))
S(X) = lim Tn(exp(X))
n→∞
(8)
(9)
(10)
(11)
Here, is the element-wise division between two matrices, and Tr and Tc are row and column
normalizations of a non-negative matrix, respectively. Therefore, iteratively applying T is equivalent
to iteratively normalizing a non-negative matrix by column and row.
Theorem A.2. (Sinkhorn, 1964) The range of S is Bn×n.
Theorem A.3. (Mena et al., 2018) Let X ∈ Rn×n, τ > 0. The Gumbel-Sinkhorn distribution
G.S.(X, τ) is defined as follows:
Gs(X,τ ) = S ( x-+^ )	(12)
τ
where is a matrix of i.i.d. standard Gumbel noise. Moreover, G.S.(X, τ) converges almost surely
to G.M.(X) as τ → 0+.
To approximately sample from G.M.(X), we first sample from G.S.(X, τ). Even though theoreti-
cally, T needs to be applied infinite number of times to obtain a matrix in Bn×n, Mena et al. (2018)
reports that 20 iterations of T are enough in practice. We find that in our experiments, 20 iterations
are not enough to obtain a matrix in Bn×n, but 100 - 200 iterations are enough. After we obtain the
matrix in Bn×n, we apply Hungarian algorithm (Munkres, 1957) to obtain P ∈ G.M.(X).
Finally, we need to calculate the entropy term Hqφ in Lφ in Equation 3. This can be approximated
using the technique in Appendix B.3 of Mena et al. (2018).
B Matrix Permanent and its Approximation with Bethe
Permanent
In this section, we present details about matrix permanent and bethe permanent, which we use as an
approximation to the denominator of qφ(∙∣y, x).
Definition B.1. Let A ∈ Rn×n. The permanent of A is defined as follows:
n
perm(A) = XY
Ai,σi	(13)
σ∈Sn i=1
Theorem B.2. The denominator of qφ(∙∣y, x) equals perm (exp(X)).
16
Published as a conference paper at ICLR 2021
Proof.
n
exp hX, PiF = exp(	Xi,σ(i))
n
= X Y(exp(X))i,σ(i)
σ∈Sn i=1
= perm(exp(X))
□
Definition B.3. (Vontobel, 2010; Anari & Rezaei, 2019) Let A ∈ Rn+×n . The bethe permanent of
A is defined as follows:
permB (A) = exp ( max	(γi,j log Ai,j - γi,j log γi,j + (1 -γi,j)log(1 - γi,j)))	(14)
γ∈Bn×n
i,j
Theorem B.4. (Anari & Rezaei, 2019) Let A ∈ R+×n. Then, √2 nperm(A) ≤ permB (A) ≤
perm(A).
The γ in Definition B.3 can be calculated using the message passing algorithm in Lemma 29 of
Vontobel (2010). An efficient implementation has recently been introduced in Appendix C of Mena
et al. (2020). Therefore, We can use PermB (exp (X)) to approximate the denominator of qφ(∙∣y, x),
and we can then use policy gradient to compute VφL(φ) in Equation (3).
C Detailed Training Process and Hyperparameter Settings
For all experiments, we apply dropout = 0.1 (Srivastava et al., 2014) and label smoothing = 0.1.
We apply Adam Optimizer (Kingma & Ba, 2015) with β1 = 0.99, β2 = 0.999 for MS-COCO, and
β1 = 0.99, β2 = 0.98 for all other tasks. For baseline experiments, we use a batch size of 64 for
Django and MS-COCO, and 128 for Gigaword and WMT. We decrease the learning rate linearly
from 1e-4 to zero. We train the baseline until the performance plateaus.
For our VOI model, we train on Django for a total of 350 epochs (120k gradient steps), MS-COCO
for 20 epochs (350k gradient steps), Gigaword for 16 epochs (1M gradient steps), and WMT16 Ro-
En for 120 epochs (1.3M gradient steps). We use a batch size of 36 for MS-COCO and Django, 50
for Gigaword, and 54 for WMT. We sample K = 4 latents per training sample for the first three
datasets, and K = 3 for WMT. Due to constraints in computational resource, we were unable to
scale WMT to larger batch size and larger K . We also did not experiment with larger batch size for
COCO, Django, and Gigaword. We leave the investigations of larger batch sizes and larger K for
future work.
We set the initial decoder learning rate to be 5e-5 and the encoder learning rate to be 5e-6. We
train the VOI encoder and decoder with shared embedding for about the first 15-20% of steps (i.e.
4 epochs for COCO, 50 epochs for Django, 3 epochs for Gigaword, and 20 epochs for WMT). We
then separate the embeddings for the rest of the training steps. When the embedding is shared, we
set the entropy coefficient β = 0.3 for all tasks.
After we separate the embeddings, for MS-COCO, we anneal β with a log-linear schedule from 0.3
to 0.03. We decrease the learning rate to (3e-5, 3e-6) for the decoder and the encoder respectively
after epoch 13, when the encoder starts to sample very similar permutations fora single training data.
We observe that training either VOI or the fixed ordering models for too long leads to overfitting.
Finetuning VOI with the encoder fixed does not help and causes the performance to slightly drop.
For Django, we set the learning rates to be (3e-5, 3e-6). We log-anneal β to 0.03 for the first 90% of
steps and then anneal β to 0.003 for the rest of the steps. We find that the latter allows the encoder
to commit to a single ordering on sequences of longer length and slightly improves performance.
We finally fix the encoder and finetune the decoder for 50 epochs with a larger batch size of 64 and
learning rate linearly annealing to zero. This finetuning step improves the BLEU score by about 0.6.
For Gigaword and WMT, we add a cosine alignment loss between the decoder and the encoder’s
embedding matrices to the loss of the encoder. We set the cosine alignment loss coefficient to be
17
Published as a conference paper at ICLR 2021
propagate
Gradients using ；
..rfinfωrcf '
Figure 5: This figure demonstrates our algorithm for an image captioning task. The model on the
left is the Permutation Transformer, which maps training examples to permutation matrices. The
model on the right is an autoregressive model that learns to predict tokens and positions.
100.0 for Gigaword and 10.0 for WMT. Intuitively, since the Gigaword and WMT vocabularies are
much larger than those of MS-COCO and Django, and they contain many rare words, this loss allows
the encoder to leverage the semantic information recently-learnt from the decoder to better discover
autoregressive orderings.
For Gigaword, we anneal β log-linearly from 0.3 to 0.03 in 8 epochs (500k gradient steps). We then
fix the encoder and fine-tune the decoder with a batch size of 128 for 5 epochs with learning rate
linearly decreasing from 7e-5 to 0. We observe that, compared to COCO and Django, this finetuning
step significantly improves VOI’s performance and raises the ROUGE score by around 1.5 to 2.0.
For WMT, we anneal β log-linearly from 0.3 to 7e-4 in 80 epochs (900k gradient steps). We decrease
the learning rates from (5e-5, 5e-6) to (3e-5, 3e-6) at epoch 40 when the encoder starts sampling very
similar permutations. We then fix the encoder and finetune the decoder with a batch size of 128 for
20 epochs with learning rate linearly decreasing from 3e-5 to 0. We observe that this finetuning step
also significantly benefits VOI’s performance and improves the BLEU score by around 1.5 points.
Due to resource constraints, we did not tune our hyperparameters and training schedules very care-
fully, and we leave the discovery of better training schemes for future work.
D	Example Architecture for Conditional Sequence Generation
In Section 5, we introduced the specific encoder and decoder architectures used for the conditional
sequence generation tasks in our paper. To further illustrate the architecture of Variational Order
Inference , we present a diagram of the architecture instantiated for COCO 2017.
E	Parts Of Speech Mappings
The parts of speech used in our Order Analysis section correspond to the NLTK Universal Tagset.
In the below table, we provide mappings for the tag identifiers used in our main paper. More in-
formation about the specific NLTK tags can be found at the following url: http://www.nltk.
org/book/ch05.html.
F Visualizations of Sequence Generation
F.1 COCO
We visualize the generation order inferred by Variational Order Inference for COCO. Sequences
are generated using beam search over both tokens and their insertion positions, using a beam size
of 3. Bounding boxes that correspond to region-features calculated using bottom-up attention are
superimposed on the image, with an opacity value proportional to the magnitude of their softmax
attention value in the final cross-attention layer in the language model.
18
Published as a conference paper at ICLR 2021
Tag	Meaning	English Examples
ADJ	adjective	new, good, high, special, big, local
ADP	adposition	on, of, at, with, by, into, under
ADV	adverb	really, already, still, early, now
CONJ	conjunction	and, or, but, if, while, although
DET	determiner, article	the, a, some, most, every, no, which
NOUN	noun	year, home, costs, time, Africa
NUM	numeral	twenty-four, fourth, 1991, 14:24
PRT	particle	at, on, out, over per, that, up, with
PRON	pronoun	he, their, her, its, my, I, us
VERB	verb	is, say, told, given, playing, would
.	punctuation marks	;! .,;
X	other	ersatz, esprit, dunno, gr8, univeristy
Table 4: NLTK Universal Tagset.
Image ID: 000000036539
people
people
two people
two people standing
two people standing
two people standing
two people	standing	in
two people	standing	in
snow
snow
snow
snow
snow
snow snowboards
snow on snowboards
two people standing
in the snow on snowboards
Figure 6:	Generation order inferred by Variational Order Inference. Without supervision over its
generation order, nor a domain-specific initialization, nor a prior to aid learning, the model learns an
adaptive strategy that prioritizes object names—in this case, people and snow.
Decoded Text
Image ID:000000000785
woman
woman
a woman
a woman	riding
a woman	riding
a woman	riding
a woman	riding
a woman	riding
a woman	riding
a woman	riding
skis
skis
skis
skis
skis	down	
skis	down	snow
skis	down	snow
skis	down	snow
skis	down	a snow
slope
covered slope
covered slope
Figure 7:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation set
with the image identifier 000000000785.
19
Published as a conference paper at ICLR 2021
Image ID: 000000000802
Decoded Text
kitchen
kitchen	cabinets		
kitchen			cabinets
kitchen	with		cabinets
kitchen	with		cabinets
kitchen	with		cabinets
kitchen	with		cabinets
kitchen	with		cabinets
kitchen	with	white	cabinets
and
and white
and white
appliances
appliances
appliances
appliances
Figure 8:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation set
with the image identifier 000000000802.
Decoded Text
person
Image ID:000000001268
person
a person
ledge
ledge
a person standing
a	person	standing	on
a	person	standing	on
a	person	standing	on
a	person	standing	on
a	person	standing	on
ledge
ledge
ledge
ledge	water
ledge	near	water
a ledge	near	water
a person standing
on a ledge
Figure 9:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation set
with the image identifier 000000001268.
20
Published as a conference paper at ICLR 2021
Decoded Text
Image ID:000000001296
woman
woman
a woman
a woman holding
a woman holding
phone
phone
phone
cell phone
a woman holding cell phone
a woman	holding	cell phone
a woman	holding	cell phone
a woman holding a cell phone
a woman holding a cell phone
hand
n	hand
n	hand
n her hand
Figure 10:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation
set with the image identifier 000000001296.
Decoded Text
desktop
Image ID: 000000001503
desktop
a desktop
laptop
laptop
a desktop	computer	laptop
a desktop	computer	with	laptop
a desktop	computer	with	laptop	«
a desktop	computer	with	laptop	desk .
a desktop	computer	with	laptop on desk .
a desktop	computer	with a laptop on desk .
a desktop	computer	with a laptop on a desk .
Figure 11:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation
set with the image identifier 000000001503.
21
Published as a conference paper at ICLR 2021
Decoded Text
bedroom
with
with
with
with
bedroom
a
bed
bed
bed
bed
bed
table
with
bedroom
with
bed
bed
bed
and
and
and
table
table
table
a
a
a
a
Figure 12:	Generation order inferred by Ours-VOI for an image from the COCO 2017 validation
set with the image identifier 000000001993.
Decoded Text
a
a young
Image ID:000000000785
a young person
a young person riding
a young
person riding skis
a young person
a young person
riding	skis on
riding	skis on a
a young	person	riding
a young	person	riding
skis on a snowy
skis on a snowy	slope
Figure 13:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000000785.
22
Published as a conference paper at ICLR 2021
Image ID: 000000000802
Decoded Text
a kitchen
a	kitchen	with
a	kitchen	with
a	kitchen	with
white
white appliances
a kitchen
a kitchen
with white
with white
appliances
appliances
and
and wood
a kitchen with white appliances
and wood cabinets
a kitchen with white appliances
and wood cabinets
Figure 14:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000000802.
Decoded Text
a
a couple
a couple
a couple
a couple
a couple
a couple
a couple
a couple
a couple
of
of people
of	people	that
of	people	that
of	people	that
of	people	that
of	people	that
of	people	that
are
are sitting
are sitting
are sitting
are sitting
on
on a
on a bench
Figure 15:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000001268.
23
Published as a conference paper at ICLR 2021
Decoded Text
Image ID:000000001296
a
a woman
a woman
a woman
a woman
a woman
a woman
a woman
a woman
a woman
a woman
n a
n a crowd
n a crowd
n a crowd
n a crowd
n a crowd
n a crowd
n a crowd
s
s using
s	using	her
s	using	her	cell
s	using	her	cell	phone
s	using	her	cell	phone
Figure 16:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000001296.
Decoded Text
desktop
desktop
desktop
desktop
desktop
desktop
desktop
desktop
desktop
computer
computer	sitting
computer	sitting	on
computer	sitting	on top
computer	sitting	on top of
computer	sitting
computer	sitting
computer
on top of a
on top of a desk
on top of a desk
Figure 17:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000001503.
24
Published as a conference paper at ICLR 2021
a bed
a bed and
Decoded Text
Image ID: 000000001993
a bed and a
a bed and a table
a bed and
a bed and
a table in
a table in a
a bed and a table
in a room
a bed and a table
in a room
Figure 18:	Generation order inferred by Ours-L2R for an image from the COCO 2017 validation
set with the image identifier 000000001993.
Decoded Text
Image ID:000000000785
a		
a	on	
a	on	the
a	on	in the
a person	on	in the
a person	on some	in the
a	person	on	some		in	the	snow
a	person	on	some	skis	in	the	snow
Figure 19:	Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000000785.
25
Published as a conference paper at ICLR 2021
Image ID: 000000000802
Decoded Text
a
a				a			
a				a			
a			with	a			
a			with	a			and
a			with	a		l	and
a	kitchen		with	a		,	and
a	kitchen	area	with	a		,	and
a	kitchen	area	with	a		,refrigerator	and
a	kitchen	area	with	a	stove	,refrigerator	and
a	kitchen	area	with	a	stove	,refrigerator	and
dishwasher
Figure 20: Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000000802.
Decoded Text
a
a								a
a								a
a					on			a
a		of			on			a
a		of			on		of	a
a		of		sitting	on		of	a
a		of	people	sitting	on		of	a
a		of	people	sitting	on	top	of	a
a	group	of	people	sitting	on	top	of	a
a	group	of	people	sitting	on	top	of	a pier
Figure 21: Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000001268.
26
Published as a conference paper at ICLR 2021
Decoded Text
Image ID:000000001296
woman
woman
woman
woman
woman
woman
woman
on
on
holding
holding
holding
holding
holding
holding
on
cell
cell
cell
while
on
phone
phone
phone
phone
while
while
while
while
on
on
talking
talking
on
on
a crowd
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
Figure 22:	Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000001296.
Decoded Text
a
a
Image ID: 000000001503
a
a
a
a
a
a
a computer
a computer
a computer
on
on
sitting on
sitting
sitting
sitting
sitting
monitor sitting
a
a
of a
of a
on	top	of	a
on	top	of	a	wooden
on	top	of	a	wooden
on top
on top
of a wooden	desk
of a wooden	desk
Figure 23:	Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000001503.
27
Published as a conference paper at ICLR 2021
Decoded Text
a
a
a bed
a bed
a
	a	a
	a	a
	in a	a
	in a	to a
sitting	in a	to a
sitting	in a	next to a
sitting	in a room	next to a
sitting	in a room	next to a
sitting	in a room	next to a window
Figure 24: Generation order inferred by Ours-Common for an image from the COCO 2017 valida-
tion set with the image identifier 000000001993.
Decoded Text
Image ID:000000000785
a
a
				slope		
		skis				slope
		skis			covered	slope
		skis		snow	covered	slope
	riding	skis		snow	covered	slope
	riding	skis	down	snow	covered	slope
man	riding	skis	down	snow	covered	slope
man	riding	skis	down	snow	covered	slope
man	riding	skis	down	snow	covered	slope
man	riding	skis	down	a snow	covered	slope
Figure 25: Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000000785.
28
Published as a conference paper at ICLR 2021
Image ID: 000000000802
Decoded Text
stove
kitchen
kitchen
kitchen	with
kitchen	with
a	kitchen	with
a	kitchen	with
a	kitchen	with
stove
stove
stove and
stove and
stove and
stove and
a stove and
a stove and
refrigerator
refrigerator
refrigerator
refrigerator
refrigerator
refrigerator
refrigerator
a refrigerator
Figure 26:	Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000000802.
Decoded Text
dock
dock looking
dock looking	water
woman	dock	looking	water
woman	dock	looking	at	water
woman	sitting	dock	looking	at	water
woman	sitting	dock	looking	at the water
woman	sitting on	dock	looking	at the water
woman	sitting on	dock	looking	at the water
a woman	sitting on	dock	looking	at the water
a woman	sitting on a dock	looking	at the water
Figure 27:	Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000001268.
29
Published as a conference paper at ICLR 2021
Image ID:000000001296
Decoded Text
talking
talking
talking
talking
woman talking
woman	is	talking
the	woman	is	talking
the	woman	is	talking
the	woman	is	talking
cell
cell phone
her cell phone
her cell phone
her cell phone
her cell phone
on her cell phone
on her cell phone
Figure 28: Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000001296.
Decoded Text
Image ID:000000001503
mouse
						keyboard		mouse
				desk		keyboard		mouse
	laptop			desk		keyboard		mouse
	laptop			desk		keyboard	and	mouse
	laptop			desk	with	keyboard	and	mouse
	laptop	on		desk	with	keyboard	and	mouse
a	laptop	on		desk	with	keyboard	and	mouse
a	laptop	on	a	desk	with	keyboard	and	mouse
a	laptop	on	a	desk	with	a keyboard	and	mouse
a	laptop	on	a	desk	with	a keyboard	and	a mouse
Figure 29: Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000001503.
30
Published as a conference paper at ICLR 2021
Decoded Text
Image ID: 000000001993
room
room
room
room
room
room
a room
a room
tables
		chairs	tables	
	bed	chairs		tables
	bed	chairs		tables
	bed	,chairs		tables
	bed	,chairs		tables
	bed	,chairs	,and	tables
with	bed	,chairs	,and	tables
with	bed	,chairs	,and	tables
with	bed	,chairs	,and	tables
with	a bed	,chairs	,and	tables
Figure 30:	Generation order inferred by Ours-Rare for an image from the COCO 2017 validation
set with the image identifier 000000001993.
F.2 Django
We visualize the latent generation order inferred by Variational Order Inference for Django. Se-
quences are generated using a beam search over both the tokens and their insertion positions, using
a beam size of 3. Text on which the model is conditioned is provided on the left for each example.
Conditioned Text
raise an AttributeError with an argument
string _STR:0_ , formated with self.name [
self . name ].
	AttributeError	Decoded Text
	AttributeError	name
	AttributeError	(	name
	AttributeError	('_STR:O_'	name
	AttributeError	('_STR:O_'	%	name
	AttributeError	(JSTRQ' % self name
	AttributeError	('_STR:O_'	% self « name
	AttributeError	(JSTRQ' % self . name
raise	AttributeError	('_STR:O_'	% self . name
Figure 31:	Generation order inferred by Ours-VOI for a pseudocode sample from the Django natu-
ral language to code test set with the sample id 154.
F.3 Gigaword
We visualize the latent generation order inferred by Variational Order Inference for Gigaword. Se-
quences are generated using a beam search over both the tokens and their insertion positions, using
a beam size of 3. Text on which the model is conditioned is provided on the left for each example.
31
Published as a conference paper at ICLR 2021
Conditioned Text	Decoded Text
	i	enumerate
	i ,	enumerate
	i , arg	enumerate
	i , arg in enumerate
for every i and arg in enumerated	
iterable args ,	
	i , arg in enumerate	:pass
	i , arg in enumerate	{	:pass
	i , arg in enumerate	( args	:pass
	i , arg in enumerate	( args ) :pass
	for i , arg in enumerate	( args ) :pass
Figure 32:	Generation order inferred by Ours-VOI for a pseudocode sample from the Django natu-
ral language to code test set with the sample id 431.
Conditioned Text	raise	Decoded Text
	raise AttributeError	
	raise AttributeError	(
	raise AttributeError	('_STR：O_'
raise an AttributeError with an argument		
string _STR:O_ , formated with self.name [	raise AttributeError	(JSTRQ' %
self . name ].		
	raise AttributeError	(_STR:OJ % self
	raise AttributeError	('_STR:O_'	% self «
	raise AttributeError	(JSTRQ' % self . name
	raise AttributeError	('_STR:O_'	% self . name
Figure 33:	Generation order inferred by Ours-L2R for a pseudocode sample from the Django natu-
ral language to code test set with the sample id 154.
32
Published as a conference paper at ICLR 2021
Conditioned Text
for
for i
for i ,
for i ,
-________S . _ ._____________j	for i，
for every ι and arg in enumerated
iterable args ,
for i ,
for i ,
for i ,
for i ,
for i ,
Decoded Text
arg					
arg	in				
arg	in	enumerate			
arg	in	enumerate	(		
arg	in	enumerate	(	args	
arg	in	enumerate	(	args	)
arg	in	enumerate	(	args	):pass
Figure 34:	Generation order inferred by Ours-L2R for a pseudocode sample from the Django natu-
ral language to code test set with the sample id 431.
Conditioned Text
Decoded Text
						self	. . . . .
			(			self	. .
			(			self	. .
raise an AttributeError with an argument string _STR:O_ , formated with self.name [			('_STR:O_'			self	. .
self . name ].			('_STR:OJ		name	self	. .
			('_STR:OJ	%	name	self	. .
	raise		('_STR：OJ	%	name	self	. .
	raise		('_STR：OJ	%	name	self	. .
	raise		('_STR：OJ	%	name	self	._class_ .
	raise	AttributeError	('_STR:OJ	%	name	self	, _class_ .
Figure 35:	Generation order inferred by Ours-Common for a pseudocode sample from the Django
natural language to code test set with the sample id 154.
33
Published as a conference paper at ICLR 2021
Conditioned Text
Decoded Text
	(
	()
	():pass
-	S . _ .	j	for for every ι and arg in enumerated	():pass
iterable args ,	
for	in	( ) :pass
for	in args	( ) :pass
for	i in args	( ) :pass
for	arg i in args	( ) :pass
for	arg i in args enumerate	( ) :pass
Figure 36:	Generation order inferred by Ours-Common for a pseudocode sample from the Django
natural language to code test set with the sample id 431.
Conditioned Text	Decoded Text AttributeError
raise an AttributeError with an argument string _STR:O_ , formated with self.name [ self . name ].	raise	AttributeError raise	AttributeError	% raise	AttributeError	%	name raise	AttributeError	'_STR:O_'	%	name raise	AttributeError	'_STR:O_'	%	name	) raise	AttributeError	{ ' STRjOJ	%	name	) raise	self AttributeError	( '_STR:O_'	%	name	)
	raise self AttributeError	( ' STRjOJ % « name )
Figure 37:	Generation order inferred by Ours-Rare for a pseudocode sample from the Django
natural language to code test set with the sample id 154.
34
Published as a conference paper at ICLR 2021
Conditioned Text
Decoded Text
enumerate
arg enumerate
S arg enumerate
for every i and arg in enumerated
iterable args ,
for
for
for
for
for
	arg	enumerate	args			
in	arg	enumerate		args		
in	arg	enumerate		args		
in	arg	enumerate		args		:pass
in	arg	enumerate		args	)	:pass
in	arg	enumerate	(	args	)	:pass
in	arg	enumerate	(	args	)	:pass
Figure 38: Generation order inferred by Ours-Rare for a pseudocode sample from the Django
natural language to code test set with the sample id 431.
Conditioned Text
Decoded Text
prime minister of antigua and bar@@
bud@@ a baldwin spencer left here
monday for hong kong , winding up his
four day visit to shanghai .
antigua
antigua pm
antigua pm
antigua pm
antigua pm leaves
kong
hong kong
hong kong
antigua pm leaves shanghai
antigua pm leaves shanghai
hong kong
for hong kong
Figure 39:	Generation order inferred by Ours-VOI for a text sample from the Gigaword text sum-
marization test set with the sample id 15.
35
Published as a conference paper at ICLR 2021
Conditioned Text	sri	Decoded Text					
	sri	Iankan					
the sri Iankan navy has taken into custody two indian fishing trawlers which were poaching sri Ianka &apos; s northwestern coast and arrested # # indian fishermen on board , a local newspaper reported on Wednesday .	sri sri sri	Iankan Iankan Iankan			two	indian indian	trawlers trawlers trawlers
	sri	Iankan		arrests	two	indian	trawlers
	sri	Iankan	navy	arrests	two	indian	trawlers
Figure 40:	Generation order inferred by Ours-VOI for a text sample from the Gigaword text sum-
marization test set with the sample id 33.
Conditioned Text
Decoded Text
antigua
antigua pm
antigua pm leaves
prime minister of antigua and bar@@
bud@@ a baldwin spencer left here
monday for hong kong , winding up his
four day visit to shanghai .
antigua pm leaves shanghai
antigua pm leaves shanghai
antigua pm leaves shanghai for hong
antigua pm leaves shanghai
for hong kong
Figure 41:	Generation order inferred by Ours-L2R for a text sample from the Gigaword text sum-
marization test set with the sample id 15.
36
Published as a conference paper at ICLR 2021
Conditioned Text
Decoded Text
sri
sri Iankan
sri Iankan navy
the sri Iankan navy has taken into
custody two indian fishing trawlers which	sri	Iankan	navy	arrests				
were poaching sri Ianka &apos; s								
northwestern coast and arrested # # indian fishermen on board , a local newspaper reported on Wednesday .	sri	Iankan	navy	arrests	*			
	sri	Iankan	navy	arrests	*	*		
	sri	Iankan	navy	arrests	*	*	indian	
	sri	Iankan	navy	arrests	*	*	indian	fishermen
Figure 42: Generation order inferred by Ours-L2R for a text sample from the Gigaword text sum-
marization test set with the sample id 33.
Conditioned Text
Decoded Text
antigua
antigua pm
prime minister of antigua and bar@@
bud@@ a baldwin spencer left here
monday for hong kong , winding up his
four day visit to shanghai .
antigua pm ends
antigua pm ends visit
antigua pm ends visit to
antigua pm ends visit to shanghai
Figure 43: Generation order inferred by Ours-Common for a text sample from the Gigaword text
summarization test set with the sample id 15.
37
Published as a conference paper at ICLR 2021
Conditioned Text
Decoded Text
sri
sri Iankan
sri Iankan navy
the sri Iankan navy has taken into
custody two indian fishing trawlers which	sri	Iankan	navy	arrests				
were poaching sri Ianka &apos; s								
northwestern coast and arrested # # indian fishermen on board , a local newspaper reported on Wednesday .	sri	Iankan	navy	arrests	*			
	sri	Iankan	navy	arrests	*	*		
	sri	Iankan	navy	arrests	*	*	indian	
	sri	Iankan	navy	arrests	*	*	indian	fishermen
Figure 44:	Generation order inferred by Ours-Common for a text sample from the Gigaword text
summarization test set with the sample id 33.
Conditioned Text
Decoded Text
antigua
prime minister of antigua and bar@@
bud@@ a baldwin spencer left here
monday for hong kong , winding up his
four day visit to shanghai .
antigua	ends				
antigua	ends			visit	
antigua	ends	prime		visit	
antigua	ends	prime	minister	visit	
antigua	ends	prime	minister	visit	china
antigua	ends	prime	minister	visit	to china
Figure 45:	Generation order inferred by Ours-Rare for a text sample from the Gigaword text sum-
marization test set with the sample id 15.
38
Published as a conference paper at ICLR 2021
Conditioned Text	Decoded Text	fishermen
	arrests	fishermen
	Iankan	arrests	fishermen
the sri Iankan navy has taken into		
custody two indian fishing trawlers which	Iankan navy arrests	fishermen
were poaching sri Ianka &apos; s		
northwestern coast and arrested # #		
indian fishermen on board , a local	sri Iankan navy arrests	fishermen
newspaper reported on Wednesday .		
	sri Iankan navy arrests	indian	fishermen
	sri Iankan navy arrests #	indian	fishermen
	sri Iankan navy arrests # # indian	fishermen
Figure 46:	Generation order inferred by Ours-Rare for a text sample from the Gigaword text sum-
marization test set with the sample id 33.
39