Published as a conference paper at ICLR 2021
Fast convergence of stochastic subgradient
METHOD UNDER INTERPOLATION
Huang Fang, Zhenan Fan & Michael P. Friedlander
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
{hgfang, zhenanf, mpf}@cs.ubc.ca
Ab stract
This paper studies the behaviour of the stochastic subgradient descent (SSGD)
method applied to over-parameterized nonsmooth optimization problems that
satisfy an interpolation condition. By leveraging the composite structure of the
empirical risk minimization problems, we prove that SSGD converges, respectively,
with rates O(1/) and O(log(1/)) for convex and strongly-convex objectives
when interpolation holds. These rates coincide with established rates for the
stochastic gradient descent (SGD) method applied to smooth problems that also
satisfy an interpolation condition. Our analysis provides a partial explanation for
the empirical observation that sometimes SGD and SSGD behave similarly for
training smooth and nonsmooth machine learning models. We also prove that the
rate O(1/) is optimal for the subgradient method in the convex and interpolation
setting.
1	Introduction
Gradient descent (GD) and subgradient descent (subGD) methods are simple and effective first-order
optimization algorithms for training machine learning models. The convergence-rate analyses for
these methods depend crucially on the smoothness of the objective function. It is well understood
that there is a fundamental gap between the convergence rates of gradient-like methods for smooth
and nonsmooth problems (Shor, 1984; Nemirovski & Yudin, 1983; Nesterov, 2005; Bubeck, 2015;
Beck, 2017). Table 1 summarizes the rates for the main problem classes.
However, in the practice of training machine learning models, the nonsmootheness from the model
is not causing much trouble (Glorot et al., 2011; Goodfellow et al., 2016). Neural networks with
nonsmooth activation function such as ReLU activation can usually be trained as fast as the one with
smooth activation function such as softplus, see Figure 1.1 as a motivating experiment from us to
compare the convergence of gradient and subgradient based methods for smooth and nonsmooth
neural networks on the MNIST dataset1 to classify digits 0 and 1. We can see that the gradient and
subgradient based methods, either batch or stochastic version, has similar convergence behaviour
for smooth and nonsmooth neural networks. Therefore there is a discrepancy between theory and
practice.
The success of overparameterized models—such as deep and wide neural networks—has instigated a
trend in the analysis of stochastic variants of gradient descent in the interpolation setting, in which
the model achieves zero training loss, and therefore fits all of the training data (Schmidt & Le Roux,
2013; Bassily et al., 2018; Ma et al., 2018b; Jain et al., 2018; Vaswani et al., 2019a;b; Wu et al.,
2019; Liu & Belkin, 2020). This recent series of papers offer insight into the fast convergence of
SGD and new approaches for algorithm design. This line of analysis, however, focus exclusively on
smooth objective function, and cannot explain the effectiveness of SSGD for training nonsmooth
neural networks.
We present a formal analysis showing that SSGD for nonsmooth objectives could converge as fast as
smooth objectives in the interpolation setting. Our contributions include:
1http://yann.lecun.com/exdb/mnist/
1
Published as a conference paper at ICLR 2021
objective f (x)	smooth	nonsmooth
convex	-^θ(l/ɪ)^^	O(1∕2)
strongly convex	O(log(1∕e))	O(1∕)
Table 1: Worst-case iteration complexity of batch
gradient and subgradient methods.
Figure 1.1: The convergence of gradient-
based (GD and SGD) and subgradient-
based (subGD and SSGD) methods for
smooth and nonsmooth neural networks
•	A description of semi-smoothness properties of a function useful for the iteration complexity
analysis of convex objectives in the interpolation context. Under mild conditions, semi-
smoothness under interpolation allows us to prove that SSGD has iteration complexity
O(1/) for convex objectives, and O(log(1/)) for strongly-convex objectives. These
rates improved the classic bounds O(1/2) and O(1/) for convex and strongly-convex
objectives and match the convergence rates of SGD for convex and smooth objective under
interpolation.
•	Proof that the iteration bound O(1/) is optimal in the convex and interpolation setting. In
contrast to the case with a smooth objective function, subgradient-based methods cannot be
further accelerated for nonsmooth model—even with the interpolation assumption.
2	Related work
2.1	First-order methods for nonsmooth optimization
The subgradient method and its convergence analysis for general convex and nonsmooth problems
was first described by Shor in the late 1960s and 70s; see Shor (1984). Nemirovski & Yudin (1983)
subsequently established that the iteration complexity O(1/2) described by Shor (1984) is optimal
for methods that can only access a subgradient oracle. Subsequent works for minimizing general
convex and nonsmooth problems include stochastic subgradient methods (Polyak, 1987; Shalev-
Shwartz et al., 2007; Shamir & Zhang, 2013), dual averaging (Nesterov, 2009), and acceleration via
smoothing (Nesterov, 2005; Beck & Teboulle, 2012). More recently, Zhang et al. (2020) and Shamir
(2020), among others, described the tractability and complexity of getting an approximate stationary
point for general nonsmooth and nonconvex problems.
A related line of work involves the analysis of optimization algorithms for partially nonsmooth
objectives that take the form f(x) + g(x), where the function f is smooth, and the function g, which
usually represents a regularizer, is convex and nonsmooth. Many models in feature selection and
compressed sensing fall into this category of problems, which are usually solved by variations of
the proximal-gradient method (Nesterov, 2007; Beck & Teboulle, 2009; Xiao, 2010; Parikh & Boyd,
2014; Defazio et al., 2014; Allen-Zhu, 2017). These approaches typically use special properties of
the regularization function g, which generally do not apply in the context we consider in this paper.
Another related line of works focus on compositional optimization (Drusvyatskiy & Paquette, 2019;
Davis & Drusvyatskiy, 2018; Duchi & Ruan, 2018). These works consider the objective as the
compositions of convex functions and smooth maps, which is different from the objective formulation
in this work, see section 3 for details.
2.2	Interpolation helps optimization
The interpolation condition implies that the residual between the prediction of the model and data
vanishes. In the context of nonlinear least-squares, for example, it is known that interpolation
2
Published as a conference paper at ICLR 2021
Algorithm 1 Stochastic subgradient descent. The learning rate function αt : N → R+ returns the
learning rate at iteration t.
1:
2:
3:
4:
5:
6:
Initialize: W* (1) ∈ Rd
for t = 1, 2, . . . do
select i ∈ {1, 2, . . . , n} uniformly at random
compute g(t) ∈ ∂fi(W(t))
W(t+1) = W(t) - αtg(t)
end for
allows the Gauss-Newton method to converge at a local superlinear rate, although in general it can
only achieve a local linear rate (Bertsekas, 1997). Conditional gradient method can also enjoy a
linear convergence rate for least square objective when interpolation holds (Beck & Teboulle, 2004).
Recent interest in over-parameterized models for machine learning problems makes interpolation
(or near interpolation) a relevant assumption for the analysis and development of new algorithms.
Under interpolation, stochastic gradient descent enjoys the same convergence rate as its deterministic
counterpart, gradient descent (Schmidt & Le Roux, 2013; Ma et al., 2018a; Bassily et al., 2018).
Some techniques that originally developed for gradient descent, such as line-search and Nesterov
acceleration, are proven to also hold for SGD when interpolation is satisfied (Vaswani et al., 2019b;a;
Liu & Belkin, 2020; Jain et al., 2018) Related work in this vein includes improved analysis of
stochastic second-order optimization algorithms (Meng et al., 2020) and adaptive stochastic gradient
descent (Loizou et al., 2020; Vaswani et al., 2020) under interpolation. Another line of work (Qian
et al., 2019; Khaled & Richtarik, 2020) in this direction aims to explore assumptions other than the
classic bounded noise assumption for the optimization theory of training over-parameterized model,
such as the expected growth condition (Schmidt & Le Roux, 2013), expected smoothness (Qian et al.,
2019; Gower et al., 2018), etc. All of the work just mentioned involves differentiable and smooth
objective functions, and cannot explain the fast convergence of SSGD for nonsmooth objectives that
is often observed in practice.
3 Problem setting
We consider the unconstrained empirical risk-minimization problem
1n
minimize f (W) := 一 f^ fi(w)
w∈Rd	n
i=1
where each	fi(w) ：= '(hi(w))
(3.1)
and n is the number of data points. Throughout the paper, We use w* to denote any solution of
eq. (3.1), and thus f * := f (w*) is the optimal objective value. We assume that the 1-dimensional
loss function ` : R → R≥0 is nonnegative, inf` = 0, convex, and is 1-smooth, i.e.,
I'0(α) - '0(β)∣ ≤∣α - β∣ ∀α,β ∈ R.
Without loss of generality, we also assume '(0) = 0. Common examples for the loss function include
the 2-norm, logistic loss, and the 2-norm hinge loss function. The n functions hi are Lipschitz
continuous with respect to the fixed parameter L:
|hi(w1) - hi(w2)| ≤ Lkw1 - w2k ∀w1, w2 ∈ Rd, ∀i ∈ [n].
We make no assumption on their smoothness properties. Many key machine learning tasks can be
formulated as (3.1), including training deep neural networks with nonsmooth activations, such as the
ReLU function. Here and throughout, the function k ∙ k is the 2-norm of a vector, unless otherwise
specified.
Our analysis relies on the Clark generalized gradient (Clarke, 1990) of the nonconvex and nonsmooth
function h, defined as the convex hull of all valid limiting gradients:
∂h(w) := Conv { u ∣ U = lim Vh(wk), Wk → W },
k→∞
This definition additionally requires h to be almost everywhere differentiable by Rademacher’s
theorem. We refer readers to Clarke (1990), and more recently, to Zhang et al. (2020), who use this
generalized gradient in a related analysis. These properties of the generalized gradient are needed for
our analysis:
3
Published as a conference paper at ICLR 2021
(chain rule)	∂fi(w) = '0(h(w)) ∙ ∂h(w),
(gradient bound)	k∂hi (w)k ≤ L, ∀w ∈ Rd, ∀i ∈ [n].
The second property follows from the L-Lipschitz continuity of each function hi . We define the norm
of the generalized gradient at a vector w as
k∂hi(w)k = sup{ kzk | z ∈ ∂hi(w) } .
Algorithm 1 describes the stochastic subgradient descent (SSGD) method. Interpolation means that
our model has the ability to fit all training samples perfectly and therefore we can obtain zero training
loss at the solution e.g., f (w*) = 0. The interpolation assumption holds for overparameterized neural
networks and is gaining increasing interest in recent years Jacot et al. (2018).
4 Main results
4.1 B ounds and Lipschitz properties of the generalized gradient
Our analysis hinges on establishing that the objective function f is differentiable at all of its minimiz-
ers, i.e., at all vectors w such that f(w) = 0. This is implied by the following proposition, which
holds even without the interpolation condition.
Proposition 4.1 (Generalized growth condition). For all d-vectors w,
Ildfi(w)k2 ≤ 2L2fi(w)	∀i ∈ [n].	(4.1)
Consequently,
1 n
k∂f (w)k2 ≤ - E k∂fi(w)k2 ≤ 2L2f (w).	(4.2)
i=1
Equation (4.2) implies that if the objective value f(w) = 0, then the generalized gradient contains
only the origin: ∂f(w) = {0}. It thus follows from Clarke (1981, Property 10) that f is differentiable
at any point with zero objective value. This means that if there is a solution with a zero value, then it
must be a fixed point of the subgradient method. This property stands in contrast to a solution where
the function is nonsmooth and makes it possible for the subgradient method to converge to solution
with constant learning rate.
Proofof Proposition 4.1. Fix an arbitrary d-vector w. The subdifferential ∂fi(w) = '0(hi(w)) ∙
∂hi (w). Because each function hi is L-Lipschitz continuous, we can then deduce that
k∂fi(w)k2 ≤ L2 ['0(hi(w))]2
=L2(',(hi(w)) - '0(0))2
(ii)
≤ 2L2('(hi(w)) - '(0))	(4.3)
= 2L2fi(w).
Step (i) follows from the assumption that '(0) = minλ∈R '(λ) = 0, which implies '(0) = 0. Step
(ii)	follows from the fact that any convex L-smooth function u : Rn → R satisfies the bound
u(y) — u(x) — hVu(x),y — Xi ≥ 2L∣∣Vu(y) — Vu(x)∣2;
see Nesterov (2014, Theorem 2.1.5). Make the identifications
u = `, y = hi(w),	x = 0
to immediately obtain eq. (4.3) and thus the proof for eq. (4.1).
Equation (4.2) can be obtained directly from Jensen’s inequality:
k∂f (w)k2=I nX ∂fi(w)∣∣2≤
n i=1
nn
n X Mfi(W)k2 ≤ n X 2L2fi(w) = 2L2f(w).
i=1
i=1
□
4
Published as a conference paper at ICLR 2021
The composite structure of the functions fi allows us develop a semi-Lipschitz bound on their
generalized gradients. Moreover, we show that it is possible to provide a global convex majorant
for each function fi, which holds without assuming convexity. Interestingly, eq. (4.5) overlaps with
the “semi-smoothness” property of over-parameterized neural networks derived by Allen-Zhu et al.
(2019, Theorem 4). In contrast to that result, however, we do not use any special properties of
over-parameterized neural networks. Proposition 4.2 may thus be of more general interest.
Proposition 4.2 (Semi-smoothness). For all vectors wι and w2, and each i ∈ [n],
I∣∂fi(w2) - ∂fi(wι)k ≤ L2kw2 - wιk + 2L，2min{fi(w1),fi(w2)},	(4.4)
and
fi(w2) ≤ fi(wι) + h∂fi(w1),w2 一 wιi + -2-∣∣w2 - wιk2 + 2L∣W2 — wι∣√2fi(wι).
(4.5)
See Appendix A for the proof of this result.
4.2 Convergence rate of stochastic subgradient descent
We now present a global convergence analysis for the SSGD algorithm under the additional assump-
tion that the objective f of eq. (3.1) is convex. We develop a bound on the expected progress of the
objective value that depends on the minimizing value f *, rather than on the Lipschitz bound on the
function itself, which is the usual bound in the literature. Our proof is based on a simple modification
of the classical proof of subgradient descent method.
Theorem 4.3 (Global convergence rate of SSGD). Assume f is convex. Thenfor any positive
integer T and any learning rate function αt that satisfies PT=ι(αt — L2ɑ2) > 0,
min E[f (w(t)) — f *] ≤
∣w⑴一 w*∣∣2 +2L2f* PW α2
2 PLι(αt - L2α2)
(4.6)
Proof. Let gi(t) ∈ ∂fi(w(t)). Then each iterate w(t) satisfies the bound
n
E[∣w(t+1) — w*∣2 | W⑶]= —X kw(t) - αt g(t) - w*k2
i=1
=kw(t) - w*k2 - 2αt *n Xg(t),w㈤ 一 W*+ + 1 X α2kg(t)k2
n i=1	n i=1
(i)	1 n
≤ kw(t) — w*k2 — 2αt(f(w⑴)-f(w*)) + 1 X ɑ2∣git') ∣2	(4.7)
n i=1
(≤ii) IW(t) — W*I2 — 2αt(f(W(t)) — f(W*)) + 2αt2L2f(W(t)),	(4.8)
where (i) follows from the convexity of f, and (ii) follows from Proposition 4.1. Take expectations
on both sides of the inequality (4.8) and rearrange to obtain
(2αt - 2L2α2) ∙ E[(f(w⑴)—f*)] ≤ E[∣w⑴—w*∣2] — E[∣w(t+1) — w*∣2] + 2L2α2f *.
(4.9)
Summing inequality (4.9) over t ∈ {1, 2, . . . , T} yields
TT
X(2αt - 2L2ɑ2)E[f (w(t)) - f *] ≤ ∣w⑴-w*∣2 + 2L2f* Xaj.
t=1	t=1
Divide both sides by 2 PT=ι(αt - L2α2) > 0 to obtain the required result.	口
5
Published as a conference paper at ICLR 2021
This proof mirrors closely the classical proof of SSGD, which assumes that the subdifferential of
each fi is bounded, i.e., k∂fi(w)k ≤ G for some constant G. We use Proposition 4.1 in eq. (4.7) to
avoid the bounded subgradient assumption and to express the convergence rate using the minimal
value f *. This modification allows Us to leverage the interpolation assumption that f * = 0. We use
Theorem 4.3 to immediately deduce the following convergence rate result.
Corollary 4.4 (Global convergence rate of SSGD with constant learning rate). Assume that f is
convex and that the learning rate ɑt = 1∕(2L2) is constant for all t > 0. Thenfor any positive
integer T, the SSGD iterates Wt satisfy
min E[f (w⑴)-f*] ≤ (2L2/T)∣∣w⑴-w*『+ f *.
t∈[T ]
Furthermore, when f * = 0 (interpolation holds),
min E[f (w㈤)]-f*] ≤ (2L2/T)kw⑴-w*k2.
Corollary 4.4 indicates that the SSGD method converges at rate O(1/) when interpolation holds. This
rate matches the convergence rate of SGD for smooth objective functions under interpolation (Schmidt
& Le Roux, 2013). When interpolation does not hold, Corollary 4.4 implies that SSGD, on expectation,
could obtain objective value lower than 2f* + in O(1/) time, which could be close to f* when f*
is close to 0 (interpolation nearly holds).
Next, we derive convergence rate under the stronger assumption that f is strongly convex, which
means that
f(wi) ≥ f(w2) + hg,Wl - w2 + μ ∣∣W1 — W2k2	∀W1,W2 ∈ Rd, ∀g ∈ ∂f(w2)	(4.10)
for some constant μ > 0. Note that recent works indicated that in order to prove linear convergence
rate, the strong convexity assumption can be relaxed to other weaker assumptions that could hold
even for some nonconvex functions (Karimi et al., 2016; Qian et al., 2019). For simplicity, we assume
strong convexity in our analysis.
Theorem 4.5 (Global convergence rate of SSGD under strong convexity). Assume that f is
μ-strongly convex and that the learning rate at = 1/L2 is constantfor all t > 0. Thenfor any
positive integer T, the SSGD iterates W⑴ satisfy
E[∣w(T) - w*k2] ≤(1 - LrTkW(I)- w*k2 + μf*.	(4.11)
Proof. Use the definition of the SSGD iteration to obtain
(i)
(ii)
≤
E[kW(t+1) - W*k2 | W(t)]
kw(t) - w*k2 - 2αt /ɪ X g(t),w(t) - w*∖ +1X α2kg(t)k2
n i=1	n i=1
1n
kw(t) - w*k2 - 2αt(f(w㈤)-f*) - μαtkw* - w(t)∣2 + n £。2|呼)||2
i=1
≤ (1 - μαt) kW(t) - W*k2 -2αt(f(W(t)) - f *) + 2αt2L2f (W(t))
(=) (1- L)kw(t)-w*k2 + Lf*,
(4.12)
(4.13)
where (i) follows from the same argument of the proof of Theorem 4.3; (ii) follows from the μ-strong
convexity of f (see eq. (4.11)); (iii) follows from eq. (4.1); and (iv) follows from the definition of the
learning rate αt = 1/L2.
6
Published as a conference paper at ICLR 2021
Taking expectation to both sides of eq. (4.13) and recursively apply it to t ∈ {1, 2, . . . , T} to deduce
E[kw(T) — w*k2] ≤(1 — L1Tkw(I)- w*k2 + X(1 - L)' Lf *
L	t=0 L L
≤ (1- L)	kw(1)-w*k2+2f*,
where (i) follows from the fact that P∞=0(1 -告)t = L2/μ.
(4.14)
□
Theorem 4.5 indicates that the SSGD can converges to the ball centered at w* with radius 2，f *∕μ
at a linear rate. If additionally interpolation holds, SSGD converges to the solution linearly. Again,
this rate matches the convergence rate of SGD for smooth and strongly convex objectives in the
interpolation setting (Schmidt & Le Roux, 2013; Ma et al., 2018a). Similar to Corollary 4.4, when
interpolation is nearly satisfied, SSGD converges linearly to an 2，f * /9-approximate solution.
Corollary 4.4 and Theorem 4.5 also provide insight into the effect of learning rate schedules on
the performance of SSGD. A learning rate schedule αt that decays as t-1/2 is optimal because
it causes the algorithm to exhibit a complexity bound of O(1/2), which is the theoretical lower
bound (Nemirovski & Yudin, 1983). However, the learning rate schedule O(t-1/2) is slow in
practice. Corollary 4.4 and Theorem 4.5 partially explain the discrepancy between the theory and
practice: many machine learning models exhibit interpolation or near interpolation, and an aggressive
constant learning-rate schedule works better than the conservative worst-case optimal learning rate
αt = O(t-1/2).
Other than the learning rate scheduling αt = 1/L2, we can adopt a more carefully tuned learning
rate scheduling (Stich, 2019) to obtain a refined convergence rate, see more details in Appendix.
4.3 Lower bounds
We have proven that, under interpolation, SGD for smooth problems and SSGD for nonsmooth
problems exhibit the same convergence rates. This causes us to consider the following questions:
•	Is it possible to induce momentum-type acceleration for SSGD under interpolation? Vaswani
et al. (2019a) and Liu & Belkin (2020) showed recently that acceleration is possible for SGD
under interpolation. It seems plausible that similar techniques could be used for nonsmooth
problems.
•	Can the composite structure fi = ` ◦ hi, which is central to our analysis, be used to establish
an improved convergence rate for SSGD without interpolation? In other words, we know the
lower bound on the iteration complexity for any subgradient method for nonsmooth functions
is Ω(1 ∕e2). What, then, is the lower bound for minimizing the structured nonsmooth function
` ◦ h?
Unfortunately, the answer to these questions is “no”, as we show below. First, we derive the lower
iteration bound for any algorithm with access only to a subgradient oracle in the interpolation setting.
Theorem 4.6 (Lower bound with interpolation). Given t < d and positive constants L and R,
and an initial vector w(1). Let' = 11 (∙)2. Then there exists an L-Lipschitzfunction h such that
f := l ◦ h is convex, f * = minw f(w) = 0, kw(1) — w*k ≤ R such that
，、	L2 R2
1i⅛tf (W )-f ≥ 2(t+I).	(4.15)
Proof. With out loss of generality, we assume the initial point w(1) = 0. Let h(w) = Lkw - w* k∞ ,
and
[
R
R
e
R
e
*
w*
√t+Γ+ e, √t + 1 + 2'…'√t+Γ+2t,
0, . . . , 0	,
|
}	1
}
t + 1 entries
'	{z^^
(d - t - 1) entries
7
Published as a conference paper at ICLR 2021
where is some arbitrary constant greater than 0. It is easy to check that ` is 1-smooth, h is
L-Lipschitz, f = ' ◦ h is convex, and f * = 0.
We follow Nemirovski & Yudin (1983) and assume that any algorithm that uses only subgradient infor-
mation generates iterates w(s) ∈ span{∂f(w(1)), . . . , ∂f(w(t))} for all s ≤ t. By our construction,
we can further obtain w(s) ∈ span{e1, e2, . . . , et} for all s ≤ t. Therefore, for all s ≤ t,
f(w(S))- f* ≥ 2
L 2
+ 2S-1)
Note that the above holds ∀ > 0. Taking → 0+, we completes the proof.
□
Theorem 4.6 established Ω(1∕e) iteration complexity for subgradient method under interpolation.
Combining this result with the O(1/) iteration complexity in Corollary 4.4, we can conclude that the
rate O(1/e) is optimal in the interpolation setting and no acceleration is possible for subgradient-based
methods. Then we proceed to the lower bound with assuming interpolation.
Theorem 4.7 (Lower bound without interpolation). Given t < d,L, R > 0 and an initial point
W(I). Let '(∙) = 2(∙)2, and there exist L-Lipschitz function h satisfying f := l ◦ h is convex,
kw⑴ 一 w*k ≤ Rsuch that
mm
1≤s≤t
≥ ,___
一 √t+Γ
(4.16)
Proof. Similar to the proof of Theorem 4.6, we set w(1) = 0. But we change the construction of h(w)
to h(w) = L(kw - w* k∞ + R), and w* is defined in the same way as in the proof of Theorem 4.6:
[
w
R	Re	Re
√t+Γ+ e, √t+zΓ + 2'…'√t + Γ+2t,
、	一■ — ―	,
0, . . . , 0	,
、{z~*}
(d - t - 1) entries
t + 1 entries
where e is some arbitrary constant greater than 0. Following the same analysis of the proof of Theo-
rem 4.6. w(s) ∈ span{e1, e2, . . . , et } ∀s ≤ t and
KW(Ss)- f* ≥ IL (√⅛Γ + 2-1 + R) 一 LL
L2R2
≥ /
一 √t+Γ
∀s ≤ t,
By taking e → 0+, we have kW(1) - W* k ≤ R and completes the proof.
□
Theorem 4.7 provides an Ω(1∕e2) iteration complexity for subgradient-based methods for solving the
structured function ` ◦ h. This matches the lower bound for solving general nonsmooth objective
function with subgradient-based method, and implies that the structure '◦ h itself without interpolation
cannot give us an improved iteration complexity of subgradient-based methods.
5	Numerical experiments
The smoothness of the loss function ` is crucial to our analysis. We now present some numerical
experiments to compare the convergence of SSGD for training ReLU neural networks with smooth
and nonsmooth loss functions. We denote {xi}in=1 as training samples and {yi}in=1 as training labels.
yi stands for the prediction of the i-th sample Xi from the trained model.
5.1	Teacher-student setup
We randomly generate a small one hidden layer neural network with 16 neurons and ReLU activation
as the teacher network, the network takes 16 dimensional vectors as inputs and outputs a scalar. Then
we generate 128 random vectors from the Gaussian distribution as our training data {xi }i1=281 ⊂ R16
and get their corresponding labels {yi}i1=281 as the output of the teacher network. In order to ease the
training and satisfy the interpolation assumption, we overparameterize the student neural network
8
Published as a conference paper at ICLR 2021
(a) Teacher-student training
(b) Classify 4’s and 9’s on MINST dataset.
Figure 5.1: The performance of SSGD with smooth and nonsmooth loss functions.
and set it to be a one hidden layer network with 512 neurons and ReLU activation. We train the
student network with different loss functions: squared loss e.g., 1 En=1(yi - ^i)2 and absolute loss
e.g., 1 En=IIyi — yi| with different learning rates. The training curves are shown in Figure 5.1a. We
can observe that the training curve with squared loss is smoother than the curve with absolute loss.
For absolute loss, the performance of SSGD is more sensitive to the change of learning rate and we
need to decrease the learning rate to obtain a lower objective value. These observations validate the
importance of the smoothness of loss function under the interpolation setting.
5.2	Classify 4’s and 9’s on MNIST dataset
We train the LeNet (Lecun et al., 1998) on the MNIST dataset to classify 4’s and 9’s. To convert this
task to a binary classification problem, we transform the labels {yi}in=1 to {—1, +1}n. Then we run
SSGD to train the model with difference loss functions: logistic loss e.g., 1 En=I log(1+exp(-yiyji))
and L1-hinge loss e.g., 1 En=I max{0,1 — yiyi} and with different learning rates. The training
curves are presented in Figure 5.1b. Different from the teacher-student experiment, SSGD in this
task perform similarly with smooth and nonsmooth loss functions. We conjecture that this is because
the objective function with L1-hinge loss almost satisfies Proposition 4.1 locally at the solution,
namely eq. (4.1) holds for most training samples in a neighbourhood of the solution. Our observation
supports this conjecture. We observe that more that 95% of our final predictions yi,s satisfy the
condition |yi ∣ > 2. Since the L1-hinge loss is locally smooth when |yi ∣ > 2, we can thus say that
most training samples satisfy eq. (4.1) locally at the solution. While for the teacher-student training
problem, its objective is nonsmooth at solution (when zero residual is attained) since the absolute
value function '(x) := ∣x∣ is nonsmooth at 0. Therefore the objective of the teacher-student training
problem does not satisfy Proposition 4.1 locally as solution and running SSGD to solve it could suffer
from slow convergence.
6	Conclusion and discussion
An empirical minimization problem based on composite functions has sufficient structure to allow
for a tight convergence analysis that explains the effectiveness of stochastic subgradient descent
methods on nonsmooth problems with interpolation. Surprisingly, the complexity bounds O(1/)
and O(log(1/)) that we prove under interpolation match those of stochastic gradient descent for
smooth functions.
Our convergence analysis is based on the convexity properties of f . As we mention in connection
with Theorem 4.5, the strong convexity assumption can be relaxed to weaker conditions, but even
these exclude important nonconvex models that appear in neural networks. It is still an open problem
as to whether a linear convergence rate for subgradient methods under a weaker assumptions, such
as the restricted secant inequality (RSI). In Section 4.3 we proved that the rate O(1/) is optimal
for subgradient-based methods under interpolation. However, this lower bound holds only for
subgradient-based algorithms. Smoothing techniques based on Moreau envelopes (Nesterov, 2005)
can sometimes lead to acceleration for nonsmooth optimization, which may be further avenue to
explore for obtaining an accelerated SSGD method.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of
Machine Learning Research,18:221:1-221:51, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of ICML, pp. 242-252, 2019.
Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv e-prints, art. arXiv:1811.02564, Nov 2018.
Amir Beck. First-order methods in optimization, volume 25. SIAM, 2017.
Amir Beck and Marc Teboulle. A conditional gradient method with linear rate of convergence for
solving convex linear systems. Mathematical Methods of Operations Research, 59(2):235-247,
2004.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sci., 2(1):183-202, 2009.
Amir Beck and Marc Teboulle. Smoothing and first order methods: A unified framework. SIAM
Journal on Optimization, 22(2):557-580, 2012.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operation Research Society, 48(3),
1997.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundation and Trends in
Machine Learning, 8(3-4):231-357, 2015.
Frank H Clarke. Generalized gradients of lipschitz functionals. Advances in Mathematics, 40(1):
52-67, 1981.
Frank H. Clarke. Optimization and nonsmooth analysis, volume 5. SIAM, 1990.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1), 2018.
Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Proceedings of NeurIPS,
pp. 1646-1654, 2014.
Dmitriy Drusvyatskiy and Courtney Paquette. Efficiency of minimizing compositions of convex
functions and smooth maps. Mathematical Programming, 178:503—-558, 2019.
John Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: composite optimization
for robust phase retrieval. Information and Inference: A Journal of the IMA, 8:471—-529, 2018.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of AISTATS, pp. 315-323, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. The MIT fPress, 2016. ISBN
0262035618.
Robert M. Gower, Peter Richtarik, and Francis Bach. Stochastic quasi-gradient methods: Variance
reduction via jacobian sketching. arXiv e-prints, art. arXiv:1805.02632, May 2018.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neural networks. In Proceedings of NeurIPS, pp. 8580-8589, 2018.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating
stochastic gradient descent for least squares regression. In Proceedings of COLT, pp. 545-604,
2018.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-lojasiewicz condition. In Proceedings of ECML PKDD, pp.
795-811, 2016.
10
Published as a conference paper at ICLR 2021
Ahmed Khaled and Peter Richtarik. Better theory for Sgd in the nonconvex world. arXiv e-prints, art.
arXiv:2002.03329, Feb 2020.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Chaoyue Liu and Mikhail Belkin. Accelerating SGD with momentum for over-parameterized learning.
In Proceedings of ICLR, 2020.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for sgd: An adaptive learning rate for fast convergence. arXiv e-prints, art. arXiv:2002.10542,
Feb 2020.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of SGD in modern over-parametrized learning. In Proceedings of ICML, pp. 3325-
3334, 2018a.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of SGD in modern over-parametrized learning. In Proceedings of ICML, pp. 3331-
3340, 2018b.
Si Yi Meng, Sharan Vaswani, Issam Hadj Laradji, Mark Schmidt, and Simon Lacoste-Julien. Fast
and furious convergence: Stochastic second order methods under interpolation. In Proceedings of
AISTATS, pp. 1375-1386, 2020.
Arkadi Nemirovski and David Yudin. Problem Complexity and Method Efficiency in Optimization.
Wiley Interscience, 1983.
Yu. Nesterov. Gradient methods for minimizing composite objective function. CORE Discussion Pa-
pers 2007076, UniVerSite catholique de Louvain, Center for Operations Research and Econometrics
(CORE), 2007.
Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming,
120(1):221-259, 2009.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916.
Yurii E. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103
(1):127-152, 2005.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundation and Trends in Optimization, 1(3):
127-239, 2014.
Boris T Polyak. Introduction to optimization. translations series in mathematics and engineering.
Optimization Software, 1987.
Xun Qian, Peter Richtarik, Robert M. Gower, Alibek Sailanbayev, Nicolas Loizou, and Egor Shulgin.
SGD with arbitrary sampling: General analysis and improved rates. In Proceedings of ICML, pp.
5200-5209, 2019.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition. arXiv e-prints, art. arXiv:1308.6370, Aug 2013.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of ICML, pp. 807-814, 2007.
Ohad Shamir. Can we find near-approximately-stationary points of nonsmooth nonconvex functions?
arXiv e-prints, art. arXiv:2002.11962, Feb 2020.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: convergence
results and optimal averaging schemes. In Proceedings of ICML, 2013.
Naum Z Shor. Minimization methods for non-differentiable functions. Springer Series in Computa-
tional Mathematics, Springer, 1984.
11
Published as a conference paper at ICLR 2021
Sebastian U. Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv e-prints, pp.
arXiv:1907.04232, 2019.
Sebastian U. Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed communication. arXiv e-prints, pp. arXiv:1909.05350,
2019.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-
parameterized models and an accelerated perceptron. In Proceedings ofAISTATS, pp. 1195-1204,
2019a.
Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon
Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In
Proceedings of NeurIPS, 2019b.
Sharan Vaswani, Frederik Kunstner, Issam Laradji, Si Yi Meng, Mark Schmidt, and Simon Lacoste-
Julien. Adaptive gradient methods converge faster with over-parameterization (and you can do a
line-search). arXiv e-prints, art. arXiv:2006.06835, Jun 2020.
Xiaoxia Wu, Simon S. Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network. arXiv e-prints, art. arXiv:1902.07111, Feb 2019.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11:2543-2596, 2010.
Jingzhao Zhang, Hongzhou Lin, Suvrit Sra, and Ali Jadbabaie. On complexity of finding stationary
points of nonsmooth nonconvex functions. In Proceedings of ICML, 2020.
Appendix
A The proof of Proposition 4.2
Proof of Proposition 4.2. Given w1, w2 ∈ Rd, ∀i ∈ [n],
k∂fi(w2) - ∂fi(w1)k
≤ k'0(hi(w2))∂hi(w2) - '(hi(wι))∂hi(wι)k
≤ k'0(hi(w2))∂hi(w2) - '0(hi(w2))∂hi(wι) + '0(hi(w2))∂hi(wι) - '0(hi(w1))∂hi(w1)k
≤ k'0(hi(w2))∂hi(w2) - '0(hi(w2))∂hi(w1)∣∣ + k'0(hi(w2))∂hi(w1) - '0(hi(w1))∂hi(w1)k
≤ k'0(hi(w2))(∂hi(w2) - ∂hi(wι))∣∣ + k('0(hi(w2)) - '0(hi(w1)))∂hi(w1)k
≤ ∣'0(hi(w2))∣k∂hi(w2) - ∂hi(wι))∣∣ + ∣'0(hi(w2)) - '0(hi(wι))∣k∂hi(wι)k
≤ 2L∣'0(hi(w2))∣ + ∣hi(w2) - hi(wι)∣ X L
⑴ Z____________
≤ 2LΛ∕2fi(w2) + L kw2 - w1k,	(A」)
where (i) follows the same argument as the proof of Proposition 4.1:
∣'0(hi(w2))∣ = ∣'0(hi(w2)) - 'o(0)∣ ≤ p2('(hi(w2)) - '(o)) = Pfwy
Exchange w1 and w2 , we can also get
Ildfi(w2) - ∂fi(wι)k ≤ 2LP2fi(w1) + L2kw2 - W1∣∣.	(A.2)
Combining Equation (A.2) and Equation (A.1), we finished the proof for Equation (4.4).
Given w1, w2 ∈ Rd, note that fi(w) is almost every differentiable by Rademacher’s Theorem, thus
fi(w2) = fi(w1) +	h∂fi(w1 + τ(w2 - w1)), w2 - w1idτ
0
fi(w1) +	h∂fi(w1),w2	-	w1i	+	h∂fi(w1	+ τ(w2	-	w1)) - ∂fi(w1),w2	-	w1idτ
0
(A.3)
12
Published as a conference paper at ICLR 2021
Note that
h∂fi(w1 +τ(w2 - w1)) - ∂fi(w1),w2 - w1idτ
0
k∂fi(w1 + τ(w2 - w1)) - ∂fi(w1)kkw2 - w1kdτ
0
≤
2LP2fi(wι) + L2∣∣τ(w2 - wι)∣∣) ∣∣w2 - w1 kdτ
_______________ L
2Lkw2 - wιk y∕2fi(w1) + ɪ kw2 - w1『.
Plug Equation (A.4) into Equation (A.3), we finished the proof for Equation (4.5).
(A.4)
□
B Another choice of learning rate s cheduling
The following theorem is based on the result of Stich (2019)’s analysis on refined learning rate
scheduling. Note that the refined learning rate scheduling depends not only on the Lipschitz constant
L, but also rely on the number of iteration T (we need the number of iterations as an input of our
algorithm).
Theorem B.1. Assume that f is μ-strongly Convexfor some μ ≥ 0. For all positive integer T, there
exist a constant learning rate scheduling αt := α ≤ 1/(2L2), such that the SSGD iterates w(t)
satisfy
min E[f(w㈤)-f *] + μE[kw(T+2) - w*k2] ≤ 64L2kw⑴-w*k2exp
t∈[T +1]
+
72L2f *
μT
(B.1)
for μ > 0, and
…t)) - f*]≤ 4l⅛^+4f√T^
(B.2)
for μ = 0.
Proof. We start with eq. (4.12),
eq. (4.12)
=(1 - μαt) kw㈤-w*k2 - 2αt(f(w㈤)-f*) + 2α2L2f (w(t))
=(1 - μαt) kw㈤-w*k2 - 2(αt - atL2」)(f(w9)-f *) + 2αitL2 f *
≤ (1 - μαt) kw㈤一w*k2 - αt(f(w⑶)一f *)+2αt2L2f *,	(B.3)
where (i) is from the fact that 2(αt - αt2L2) ≥ αt given that 0 < αt ≤ 1/(2L2). Note that eq. (B.3)
is in the same form as Eq. (8) from Stich (2019)’s work, then we can directly apply (Stich, 2019,
Lemma 2, Lemma 3): there exist a learning rate scheduling such that
min E[f (w(t)) — f *]+ μE[kw(T+2) — w*k2] ≤ 64L2kw⑴-w*k2 exp
ι 72L2f*
+ μT
(B.4)
≤
≤
Then we can obtain eq. (B.2) by applying (Stich & Praneeth Karimireddy, 2019, Lemma 13) and
therefore completes the proof. Note that our proof is mostly based on the recurrence relation eq. (B.3)
and Stich (2019)’s analysis, we refer interested readers to Stich (2019)’s work for more technical
details.
□
13