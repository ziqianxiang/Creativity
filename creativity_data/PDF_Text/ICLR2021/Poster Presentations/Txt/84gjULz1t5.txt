Published as a conference paper at ICLR 2021
Linear Convergent Decentralized Optimiza-
tion with Compression
Xiaorui Liu1, Yao Li2,3, Rongrong Wang3,2, Jiliang Tang1 & Ming Yan3,2
1	Department of Computer Science and Engineering
2	Department of Mathematics
3	Department of Computational Mathematics, Science and Engineering
Michigan State University, East Lansing, MI 48823, USA
{xiaorui,liyao6,wongron6,tangjili,myan}@msu.edu
Ab stract
Communication compression has become a key strategy to speed up distributed
optimization. However, existing decentralized algorithms with compression
mainly focus on compressing DGD-type algorithms. They are unsatisfactory in
terms of convergence rate, stability, and the capability to handle heterogeneous
data. Motivated by primal-dual algorithms, this paper proposes the first LinEAr
convergent Decentralized algorithm with compression, LEAD. Our theory de-
scribes the coupled dynamics of the inexact primal and dual update as well as
compression error, and we provide the first consensus error bound in such settings
without assuming bounded gradients. Experiments on convex problems validate
our theoretical analysis, and empirical study on deep neural nets shows that LEAD
is applicable to non-convex problems.
1 Introduction
Distributed optimization solves the following optimization problem
1n
X := arg min f (x):=n X ∕i(x)]	(1)
withn computing agents and a communication network. Each fi(x) : Rd → R is a local objective
function of agent i and typically defined on the data Di settled at that agent. The data distributions
{Di } can be heterogeneous depending on the applications such as in federated learning. The vari-
able x ∈ Rd often represents model parameters in machine learning. A distributed optimization
algorithm seeks an optimal solution that minimizes the overall objective functionf(x) collectively.
According to the communication topology, existing algorithms can be conceptually categorized into
centralized and decentralized ones. Specifically, centralized algorithms require global communica-
tion between agents (through central agents or parameter servers). While decentralized algorithms
only require local communication between connected agents and are more widely applicable than
centralized ones. In both paradigms, the computation can be relatively fast with powerful computing
devices; efficient communication is the key to improve algorithm efficiency and system scalability,
especially when the network bandwidth is limited.
In recent years, various communication compression techniques, such as quantization and sparsi-
fication, have been developed to reduce communication costs. Notably, extensive studies (Seide
et al., 2014; Alistarh et al., 2017; Bernstein et al., 2018; Stich et al., 2018; Karimireddy et al., 2019;
Mishchenko et al., 2019; Tang et al., 2019b; Liu et al., 2020) have utilized gradient compression
to significantly boost communication efficiency for centralized optimization. They enable efficient
large-scale optimization while maintaining comparable convergence rates and practical performance
with their non-compressed counterparts. This great success has suggested the potential and signifi-
cance of communication compression in decentralized algorithms.
While extensive attention has been paid to centralized optimization, communication compression
is relatively less studied in decentralized algorithms because the algorithm design and analysis are
1
Published as a conference paper at ICLR 2021
more challenging in order to cover general communication topologies. There are recent efforts
trying to push this research direction. For instance, DCD-SGD and ECD-SGD (Tang et al., 2018a)
introduce difference compression and extrapolation compression to reduce model compression error.
(Reisizadeh et al., 2019a;b) introduce QDGD and QuanTimed-DSGD to achieve exact convergence
with small stepsize. DeepSqueeze (Tang et al., 2019a) directly compresses the local model and
compensates the compression error in the next iteration. CHOCO-SGD (Koloskova et al., 2019;
2020) presents a novel quantized gossip algorithm that reduces compression error by difference
compression and preserves the model average. Nevertheless, most existing works focus on the
compression of primal-only algorithms, i.e., reduce to DGD (Nedic & Ozdaglar, 2009; Yuan et al.,
2016) or P-DSGD (Lian et al., 2017). They are unsatisfying in terms of convergence rate, stability,
and the capability to handle heterogeneous data. Part of the reason is that they inherit the drawback
of DGD-type algorithms, whose convergence rate is slow in heterogeneous data scenarios where the
data distributions are significantly different from agent to agent.
In the literature of decentralized optimization, it has been proved that primal-dual algorithms can
achieve faster converge rates and better support heterogeneous data (Ling et al., 2015; Shi et al.,
2015; Li et al., 2019; Yuan et al., 2020). However, it is unknown whether communication compres-
sion is feasible for primal-dual algorithms and how fast the convergence can be with compression.
In this paper, we attempt to bridge this gap by investigating the communication compression for
primal-dual decentralized algorithms. Our major contributions can be summarized as:
•	We delineate two key challenges in the algorithm design for communication compression in
decentralized optimization, i.e., data heterogeneity and compression error, and motivated by
primal-dual algorithms, we propose a novel decentralized algorithm with compression, LEAD.
•	We prove that for LEAD, a constant stepsize in the range (0,2∕(μ + L)] is sufficient to ensure
linear convergence for strongly convex and smooth objective functions. To the best of our knowl-
edge, LEAD is the first linear convergent decentralized algorithm with compression. Moreover,
LEAD provably works with unbiased compression of arbitrary precision.
•	We further prove that if the stochastic gradient is used, LEAD converges linearly to the O(σ2)
neighborhood of the optimum with constant stepsize. LEAD is also able to achieve exact con-
vergence to the optimum with diminishing stepsize.
•	Extensive experiments on convex problems validate our theoretical analyses, and the empirical
study on training deep neural nets shows that LEAD is applicable for nonconvex problems.
LEAD achieves state-of-art computation and communication efficiency in all experiments and
significantly outperforms the baselines on heterogeneous data. Moreover, LEAD is robust to
parameter settings and needs minor effort for parameter tuning.
2	Related Works
Decentralized optimization can be traced back to the work by Tsitsiklis et al. (1986). DGD (Nedic
& Ozdaglar, 2009) is the most classical decentralized algorithm. It is intuitive and simple but con-
verges slowly due to the diminishing stepsize that is needed to obtain the optimal solution (Yuan
et al., 2016). Its stochastic version D-PSGD (Lian et al., 2017) has been shown effective for train-
ing nonconvex deep learning models. Algorithms based on primal-dual formulations or gradient
tracking are proposed to eliminate the convergence bias in DGD-type algorithms and improve the
convergence rate, such as D-ADMM (Mota et al., 2013), DLM (Ling et al., 2015), EXTRA (Shi
et al., 2015), NIDS (Li et al., 2019), D2 (Tang et al., 2018b), Exact Diffusion (Yuan et al., 2018),
OPTRA(XU et al., 2020), DIGing (Nedic et al., 2017), GSGT (PU & NediC, 2020), etc.
Recently, communication compression is applied to decentralized settings by Tang et al. (2018a). It
proposes two algorithms, i.e., DCD-SGD and ECD-SGD, which require compression of high accu-
racy and are not stable with aggressive compression. Reisizadeh et al. (2019a;b) introduce QDGD
and QuanTimed-DSGD to achieve exact convergence with small stepsize and the convergence is
slow. DeepSqueeze (Tang et al., 2019a) compensates the compression error to the compression in
the next iteration. Motivated by the quantized average consensus algorithms, such as (Carli et al.,
2010), the quantized gossip algorithm CHOCO-Gossip (Koloskova et al., 2019) converges linearly
to the consensual solution. Combining CHOCO-Gossip and D-PSGD leads to a decentralized algo-
rithm with compression, CHOCO-SGD, which converges sublinearly under the strong convexity and
2
Published as a conference paper at ICLR 2021
gradient boundedness assumptions. Its nonconvex variant is further analyzed in (Koloskova et al.,
2020). A new compression scheme using the modulo operation is introduced in (Lu & De Sa, 2020)
for decentralized optimization. A general algorithmic framework aiming to maintain the linear con-
vergence of distributed optimization under compressed communication is considered in (Magnusson
et al., 2020). It requires a contractive property that is not satisfied by many decentralized algorithms
including the algorithm in this paper.
3	Algorithm
We first introduce notations and definitions used in this work. We use bold upper-case letters such
as X to define matrices and bold lower-case letters such as x to define vectors. Let 1 and 0 be
vectors with all ones and zeros, respectively. Their dimensions will be provided when necessary.
Given two matrices X, Y ∈ Rn×d, we define their inner product as hX, Yi = tr(X>Y) and the
norm as ∣∣X∣∣ = P<X,X>. Wefurtherdefine(X, YiP = tr(X>PY) and ∣∣X∣∣p = PhXX P for
any given symmetric positive semidefinite matrix P ∈ Rn×n . For simplicity, we will majorly use
the matrix notation in this work. For instance, each agent i holds an individual estimate xi ∈ Rd of
the global variable X ∈ Rd. Let Xk and VF(Xk) be the collections of {xk}n=ι and {Vfi(xk)}n=ι
which are defined below:
Xk = x1k,...,xkn> ∈ Rn×d, VF(Xk) = Vf1(x1k),...,Vfn(xkn)> ∈ Rn×d.	(2)
We use VF(Xk; ξk) to denote the stochastic approximation of VF(Xk). With these notations, the
update Xk+1 = Xk 一 ηVF(Xk; ξk) means that xk+1 = Xk 一 ηVfi(xjk; ξ) for all i. In this
paper, We need the average of all rows in Xk and VF(Xk), so We define Xk = (1>Xk)/n and
VF(Xk) = (1> VF(Xk))/n. They are row vectors, and we will take a transpose if we need a
column vector. The pseudoinverse of a matrix M is denoted as ML The largest, ith-largest, and
smallest nonzero eigenvalues ofa symmetric matrix M are λmax(M), λi(M), and λmin(M).
Assumption 1 (Mixing matrix). The connected network G = {V, E} consists of a node set V =
{1, 2, . . . , n} and an undirected edge set E. The primitive symmetric doubly-stochastic matrix W =
[wij ] ∈ Rn×n encodes the network structure such that wij = 0 if nodes i and j are not connected
and cannot exchange information.
Assumption 1 implies that -1 < λn(W) ≤ λn-ι(W) ≤ …λ2(W) < λι(W) = 1 and W1 =
1 (Xiao & Boyd, 2004; Shi et al., 2015). The matrix multiplication Xk+1 = WXk describes that
agent i takes a weighted sum from its neighbors and itself, i.e., Xik+1 = Pj∈N ∪{i} wij Xjk, where
Ni denotes the neighbors of agent i.
3.1	The Proposed Algorithm
The proposed algorithm LEAD to solve problem (1) is showed in Alg. 1 with matrix notations for
conciseness. We will refer to the line number in the analysis. A complete algorithm description from
the agent,s perspective can be found in Appendix A. The motivation behind Alg. 1 is to achieve two
goals: (a) consensus (Xk -(Xk)> → 0) and (b) convergence ((Xk)> → x*). We first discuss how
goal (a) leads to goal (b) and then explain how LEAD fulfills goal (a).
In essence, LEAD runs the approximate SGD globally and reduces to the exact SGD under con-
sensus. One key property for LEAD is 1>×1Dk = 0, regardless of the compression error in Yk.
It holds because that for the initialization, we require D1 = (I 一 W)Z for some Z ∈ Rn×d,
e.g., D1 = 0n×d, and that the update of Dk ensures Dk ∈ Range(I 一 W) for all k and
1>×ι(I 一 W) = 0 as we will explain later. Therefore, multiplying (1∕n)1>×ι on both sides of
Line 7 leads to a global average view of Alg. 1:
Xk+1 = Xk 一 ηVF(Xk; ξk),	(3)
which doesn’t contain the compression error. Note that this is an approximate SGD step be-
cause, as shown in (2), the gradient VF(Xk; ξk) is not evaluated on a global synchronized model
Xk. However, if the solution converges to the consensus solution, i.e., Xk —(Xk)> → 0, then
Eξk [VF(Xk; ξk) ― Vf(Xk; ξk)] → 0 and (3) gradually reduces to exact SGD.
3
Published as a conference paper at ICLR 2021
		
Algorithm 1 LEAD		
In O 1 2 3 4 5 6 7 8	put: Stepsize η, parameter (α, γ), X0, H1, D1 = (I utput: XK or 1/n Pin=1 XiK H1w = WH1 X1 =X0 - ηVF(X0; ξ0) for k = 1, 2,…，K 一 1 do Yk = Xk 一 ηVF(xk； ξk) 一 ηDk Yk, YW, Hk+1, HW+1 = COMM(Yk, Hk, Hw) Dk+1 = Dk + 2γη(Yk 一 Yw) Xk+1 = Xk 一 ηVF(Xk; ξk) 一 ηDk+1 end for	一 W)Z for any Z 9:	procedure COMM(Y, H, Hw) 10:	Q = COMPRESS(Y 一 H) 11:	Y = H + Q 12:	Y w = Hw + WQ 13:	H = (1 一 α)H + αY 14:	Hw = (1 一 α)Hw + αYw 15:	Return: Y, Yw , H, Hw 16:	end procedure
With the establishment of how consensus leads to convergence, the obstacle becomes how to achieve
consensus under local communication and compression challenges. It requires addressing two is-
sues, i.e., data heterogeneity and compression error. To deal with these issues, existing algorithms,
such as DCD-SGD, ECD-SGD, QDGD, DeepSqueeze, Moniqua, and CHOCO-SGD, need a dimin-
ishing or constant but small stepsize depending on the total number of iterations. However, these
choices unavoidably cause slower convergence and bring in the difficulty of parameter tuning. In
contrast, LEAD takes a different way to solve these issues, as explained below.
Data heterogeneity. It is common in distributed settings that there exists data heterogeneity among
agents, especially in real-world applications where different agents collect data from different sce-
narios. In other words, we generally have fi (x) 6= fj (x) for i 6= j. The optimality condition of
problem (1) gives 1>×ιVF(X*) = 0, where X* = [x*, ∙∙∙ , x*] is a consensual and optimal So-
lution. The data heterogeneity and optimality condition imply that there exist at least two agents i
and j such that Vfi(x*) 6= 0 and Vfj (x*) 6= 0. As a result, a simple D-PSGD algorithm cannot
converge to the consensual and optimal solution as X* 6= WX* - ηEξVF(X*; ξ) even when the
stochastic gradient variance is zero.
Gradient correction. Primal-dual algorithms or gradient tracking algorithms are able to convergence
much faster than DGD-type algorithms by handling the data heterogeneity issue, as introduced in
Section 2. Specifically, LEAD is motivated by the design of primal-dual algorithm NIDS (Li et al.,
2019) and the relation becomes clear if we consider the two-step reformulation of NIDS adopted
in (Li & Yan, 2019):
Dk+1 = Dk + I - W (Xk - ηVF(Xk) - ηDk),	(4)
2η
Xk+1 = Xk - ηVF(Xk) - ηDk+1,	(5)
where Xk and Dk represent the primal and dual variables respectively. The dual variable Dk plays
the role of gradient correction. As k → ∞, we expect Dk → -VF(X*) and Xk will converge
to X* via the update in (5) since Dk+1 corrects the nonzero gradient VF(Xk) asymptotically.
The key design of Alg. 1 is to provide compression for the auxiliary variable defined as Yk =
Xk - ηVF(Xk) - ηDk. Such design ensures that the dual variable Dk lies in Range(I - W),
which is essential for convergence. Moreover, it achieves the implicit error compression as we will
explain later. To stabilize the algorithm with inexact dual update, we introduce a parameter γ to
control the stepsize in the dual update. Therefore, if we ignore the details of the compression, Alg. 1
can be concisely written as
Yk	Xk	一 ηVF(xk ； ξk )一	ηDk	(6)
Dk+1	Dk	+ ɪ (I 一 W)Yk 2η		(7)
Xk+1	Xk	一 ηVF(xk ； ξk )一	ηDk+1	(8)
where Yk represents the compression of Yk and F(Xk; ξk) denote the stochastic gradients.
4
Published as a conference paper at ICLR 2021
Nevertheless, how to compress the communication and how fast the convergence we can attain with
compression error are unknown. In the following, we propose to carefully control the compression
error by difference compression and error compensation such that the inexact dual update (Line 6)
and primal update (Line 7) can still guarantee the convergence as proved in Section 4.
Compression error. Different from existing works, which typically compress the primal variable
Xk or its difference, LEAD first construct an intermediate variable Yk and apply compression to
obtain its coarse representation Yk as shown in the procedure COMM(Y, H, Hw):
•	Compress the difference between Y and the state variable H as Q;
•	Q is encoded into the low-bit representation, which enables the efficient local communication
step Y W = Hw + WQ. It is the only communication step in each iteration.
i -	1	∙	∙ ,WfW t t z-* i ι W	ɪɪ τV'r
•	Each agent recovers its estimate Y by Y = H + Q and we have Yw = WY.
•	States H and Hw are updated based on Y and Yw , respectively. We have Hw = WH.
By this procedure, We expect when both Yk and Hk converge to X*,the compression error vanishes
asymptotically due to the assumption we make for the compression operator in Assumption 2.
Remark 1. Note that difference compression is also applied in DCD-PSGD (Tang et al., 2018a)
and CHOCO-SGD (Koloskova et al., 2019), but their state update is the simple integration of the
compressed difference. We find this update is usually too aggressive and cause instability as showed
in our experiments. Therefore, we adopt a momentum update H = (1 一 α)H + αY motivatedfrom
DIANA (Mishchenko et al., 2019), which reduces the compression error for gradient compression in
centralized optimization.
Implicit error compensation. On the other hand, even if the compression error exists, LEAD es-
sentially compensates for the error in the inexact dual update (Line 6), making the algorithm more
stable and robust. To illustrate how it works, let Ek = Yk 一 Yk denote the compression error and
eik be its i-th row. The update of Dk gives
Dk+1 = Dk + ɪ(Yk - YW) = Dk + ɪ(i 一 w)Yk + ɪ(Ek - WEk)
2η	w	2η	2η
where -WEk indicates that agent i spreads total compression error 一 Pj∈N∪{i} Wjiek = -ek to
all agents and Ek indicates that each agent compensates this error locally by adding eik back. This
error compensation also explains why the global view in (3) doesn’t involve compression error.
Remark 2. Note that in LEAD, the compression error is compensated into the model Xk+1 through
Line 6 and Line 7 such that the gradient computation in the next iteration is aware ofthe compression
error. This has some subtle but important difference from the error compensation or error feedback
in (Seide et al., 2014; Wu et al., 2018; Stich et al., 2018; Karimireddy et al., 2019; Tang et al., 2019b;
Liu et al., 2020; Tang et al., 2019a), where the error is stored in the memory and only compensated
after gradient computation and before the compression.
Remark 3. The proposed algorithm, LEAD in Alg. 1, recovers NIDS (Li et al., 2019), D2 (Tang
et al., 2018b), Exact Diffusion (Yuan et al., 2018). These connections are established in Appendix B.
4	Theoretical Analysis
In this section, we show the convergence rate for the proposed algorithm LEAD. Before showing
the main theorem, we make some assumptions, which are commonly used for the analysis of decen-
tralized optimization algorithms. All proofs are provided in Appendix E.
Assumption 2 (Unbiased and C-contracted operator). The compression operator Q : Rd → Rd
is unbiased, i.e., EQ(x) = x, and there exists C ≥ 0 such that Ekx 一 Q(x)k22 ≤ C kxk22 for all
x ∈ Rd.
Assumption 3 (Stochastic gradient). The stochastic gradient Vfi(x; ξ) is unbiased, i.e.,
EξVfi(x; ξ) = Vfi(x), and the stochastic gradient variance is bounded: Eξ∣∣Vfi(x; ξ)一
Vfi(X) k2 ≤ σ2 forall i ∈ [n]. Denote σ2 = n1 Pn=I σ2.
5
Published as a conference paper at ICLR 2021
Assumption 4. Each f is L-smooth and μ-strongly convex with L ≥ μ > 0, i.e.,for i = 1, 2,... ,n
and ∀x, y ∈ Rd, we have
fi(y) + hWi(y), X - yi + 2l∣χ - yk2 ≤ fi(χ) ≤ fi(y) + hv∕i(y), X - y + Lkx - yk2.
Theorem 1 (Constant stepsize). Let {Xk, Hk, Dk} be the Sequence generatedfrom Alg. 1 and X*
is the optimal solution with D* = -VF(X*). Under Assumptions 1-4, for any constant stepsize
η ∈ (0, 2∕(μ + L)], if the compression parameters a and Y satisfy
γ∈
0, min
2	2μη(2 — μη)
(3C +1)β, [2 - μη(2 - μη)]Cβ
Cel 1	∙ 「2 - βγ g
2(i+c),aι min∣4-βγ ,μη(2 - μη);
with β := λmax(I-W). Then, in total expectation we have
-ELk+1 ≤ P-ELk + η2σ2,
nn
(9)
(10)
(11)
where
Lk ：= (1 - αια)∣Xk - X*k2 + (2η2∕γ)E∣Dk - D*kji-w)t + a1∣Hk - X*∣2,
C := max ʃ1 - μη(2 - μη) I_________Y
P := I 1 - α1α ,	2λmaχ((I- W)t)
1 - α < 1, a1
4(1 + C)
CeY + 2
The result holds for C → 0.
Corollary 1 (Complexity bounds). Define the condition numbers of the objective function and com-
munication graph as Kf = L and Kg = ：max(I-W), respectively.
μ	λmin(I-W)
Theorem 1, we can choose η = L, γ = min{ CSK于,(i+3c)e }, and a 二
Under the same setting in
O( (i+C)κ于)such that
ρ = max ʃ 1 -O( --1- ),1 -O(/	),1 -O( H .
(1 + C)Kf	(1 + C)Kg	CKf Kg
With full-gradient (i.e., σ = 0), we obtain the following complexity bounds:
•	LEAD converges to the -accurate solution with the iteration complexity
O(((I + C)(Kf + Kg) + CKf Kg) log -).
•	When C = 0 (i.e., there is no compression), we obtain P = max{1 — O(Ky), 1 — O(K-)},
and the iteration complexity O((Kf + Kg) log ɪ^. This exactly recovers the COnvergenCe
rate of NIDS (Li et al., 2019).
•	When C ≤ 勺：++；+长,the asymptotical complexity is O((Kf + Kg) log ：), which also
recovers that of NIDS (Li et al., 2019) and indicates that the compression doesn’t harm the
convergence in this case.
• With C = 0 (or C ≤ V;f+KL ) and fully connected communication graph (i.e.,
KfKg+Kf+Kg
w = 1n>), we have β = 1 and Kg = 1. Therefore, we obtain P = 1 - 0(Ky) and the
complexity bound O(Kf logɪ). This recovers the convergence rate ofgradient descent(Nes-
terov, 2013).
Remark 4. Under the setting in Theorem 1, LEAD converges linearly to the O(σ2) neighborhood
of the optimum and converges linearly exactly to the optimum if full gradient is used, e.g., σ = 0.
The linear convergence of LEAD holds when η < 2∕L, but we omit the proof.
Remark 5 (Arbitrary compression precision). Pick any η ∈ (0,2∕(μ + L)], based on the
compression-related constant C and the network-related constant e, we can select Y and α in cer-
tain ranges to achieve the convergence. It suggests that LEAD supports unbiased compression with
arbitrary precision, i.e., any C > 0.
6
Published as a conference paper at ICLR 2021
Corollary 2 (Consensus error). Under the same setting in Theorem 1, let Xk = n1 PZi Xk be the
averaged model and H0 = H1, then all agents achieve consensus at the rate
1 n	2L0	2σ2
1XEIlXk -Xkll2 ≤ 2L-ρk + f^η2.	(12)
n	n 1 - ρ
i=i
where ρis defined as in Corollary 1 with appropriate parameter settings.
Theorem 2 (Diminishing stepsize). Let {Xk, Hk, Dk} be the sequence generated from Alg. 1 and
X* is the optimal solution with D* = -VF(X*). Under Assumptions 1-4, if ηk = g^jθ%+2 and
Yk = θ4ηk, by taking αk = £]C), in total expectation we have
nX E∣∣χk-χ*∣∣2.0 (1)	(13)
i=i
where θi, θ2, θ3, θ4 and θ5 are constants defined in the proof. The complexity bound for arriving at
the E-accurate solution is 0(ɪ).
Remark 6. Compared with CHOCO-SGD, LEAD requires unbiased compression and the conver-
gence under biased compression is not investigated yet. The analysis of CHOCO-SGD relies on
the bounded gradient assumptions, i.e., kVfi(X)k2 ≤ G, which is restrictive because it conflicts
with the strong convexity while LEAD doesn’t need this assumption. Moreover, in the theorem of
CHOCO-SGD, it requires a specific point set of γ while LEAD only requires γ to be within a rather
large range. This may explain the advantages of LEAD over CHOCO-SGD in terms of robustness
to parameter setting.
5	Numerical Experiment
We consider three machine learning problems - '2-regularized linear regression, logistic regression,
and deep neural network. The proposed LEAD is compared with QDGD (Reisizadeh et al., 2019a),
DeepSqueeze (Tang et al., 2019a), CHOCO-SGD (Koloskova et al., 2019), and two non-compressed
algorithms DGD (Yuan et al., 2016) and NIDS (Li et al., 2019).
Setup. We consider eight machines connected in a ring topology network. Each agent can only
exchange information with its two 1-hop neighbors. The mixing weight is simply set as 1/3. For
compression, we use the unbiased b-bits quantization method with ∞-norm
Q∞(x):= (IlXIl∞2-(bτ)sign(x)) ∙ 2 n |X| + U ，	(14)
kXk∞
where ∙ is the Hadamard product, |x| is the elementwise absolute value of x, and U is a random vec-
tor uniformly distributed in [0,1]d. Only sign(X), norm IXI∞, and integers in the bracket need to be
transmitted. Note that this quantization method is similar to the quantization used in QSGD (Alistarh
et al., 2017) and CHOCO-SGD (Koloskova et al., 2019), but we use the ∞-norm scaling instead of
the 2-norm. This small change brings significant improvement on compression precision as justified
both theoretically and empirically in Appendix C. In this section, we choose 2-bit quantization and
quantize the data blockwise (block size = 512).
For all experiments, we tune the stepsize η from {0.01, 0.05, 0.1, 0.5}. For QDGD, CHOCO-SGD
and Deepsqueeze, γ is tuned from {0.01, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0}. Note that different notations
are used in their original papers. Here we uniformly denote the stepsize as η and the additional
parameter in these algorithms as γ for simplicity. For LEAD, we simply fix α = 0.5 and γ = 1.0 for
all experiments since we find LEAD is robust to parameter settings as we validate in the parameter
sensitivity analysis in Appendix D.1. This indicates the minor effort needed for tuning LEAD.
Detailed parameter settings for all experiments are summarized in Appendix D.3.
Linear regression. We consider the problem: f(X) = Pin=i(IAiX -biI2 +λIXI2). Data matrices
Ai ∈ R200×200 and the true solution X0 is randomly synthesized. The values bi are generated by
adding Gaussian noise to AiX0. We let λ = 0.1 and the optimal solution of the linear regression
problem be X*. We use full-batch gradient to exclude the impact of gradient variance. The perfor-
mance is showed in Fig. 1. The distance to X* in Fig. 1a and the consensus error in Fig. 1c verify
7
Published as a conference paper at ICLR 2021
that LEAD Converges exponentially to the optimal Consensual solution. It signifiCantly outperforms
most baselines and matChes NIDS well under the same number of iterations. Fig. 1b demonstrates
the benefit of Compression when Considering the CommuniCation bits. Fig. 1d shows that the Com-
pression error vanishes for both LEAD and CHOCO-SGD while the Compression error is pretty
large for QDGD and DeepSqueeze beCause they direCtly Compress the loCal models.
O 25 50 75 1∞ 125 150 175
Epoch
(a) kXk- X*kF
2 0 2 4 6
Ww-O-O -O
111
兰 *xlk一
0	1000000 2000000 3000000 4000000 5000000
Bits transmitted
(b) kXk — X*kF
1 1 3 S 7
10-O -O -O -O
1111
,IOJJW snsuφsuoo
O 25	50	75 100 125 150 175
Ep∞h
(C) IIXk - 1n×lXkkF
Figure 1: Linear regression problem.
2 0 2 4 6 8 0
O O-I-I-I-IT
1 1 O O oo-
1 1 1 1 C
1
Jou山 Uo-SSaJdEoo
——QDGD (2 bits)
DeepSqseza (2 bits)
-CHOCO-SGD (2 bits)
—— LEAD (2 bits)
0	25 50	75 1∞ 125 150 175
Epoch
(d) Compression error
Logistic regression. We further Consider a logistiC regression problem on the MNIST dataset. The
regularization parameter is 10-4. We Consider both homogeneous and heterogeneous data settings.
In the homogeneous setting, the data samples are randomly shuffled before being uniformly par-
titioned among all agents suCh that the data distribution from eaCh agent is very similar. In the
heterogeneous setting, the samples are first sorted by their labels and then partitioned among agents.
Due to the spaCe limit, we mainly present the results in heterogeneous setting here and defer the ho-
mogeneous setting to Appendix D.2. The results using full-batCh gradient and mini-batCh gradient
(the mini-batCh size is 512 for eaCh agent) are showed in Fig. 2 and Fig. 3 respeCtively and both
settings shows the faster ConvergenCe and higher preCision of LEAD.
Epoch
(a) Loss f (Xk)
Figure 2: LogistiC regression problem in the heterogeneous Case (full-batCh gradient).
100
6×10-1
4×10-1
3×10-1
0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Bits transmitted	1e10
(b)Loss f (Xk)
8
Published as a conference paper at ICLR 2021
0
10	20	30	40	50	60	70	80
Epoch
(a) Loss f(Xk)
1.1
1.0
0.9
0.8
职07
o
_l
0.6
0.5
0.4
0.3
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Bits transmitted	1e9
(b)Loss f (Xk)
Figure 3: Logistic regression in the heterogeneous case (mini-batch gradient).
Homogeneous data
(a) Loss f(Xk)
Heterogeneous data
(b)Loss f (Xk)
Figure 4: Stochastic optimization on deep neural network (* means divergence).
Neural network. We empirically study the performance of LEAD in optimizing deep neural net-
work by training AlexNet (240 MB) on CIFAR10 dataset. The mini-batch size is 64 for each agents.
Both the homogeneous and heterogeneous case are showed in Fig. 4. In the homogeneous case,
CHOCO-SGD, DeepSqueeze and LEAD perform similarly and outperform the non-compressed
variants in terms of communication efficiency, but CHOCO-SGD and DeepSqueeze need more ef-
forts for parameter tuning because their convergence is sensitive to the setting of γ. In the hetero-
geneous cases, LEAD achieves the fastest and most stable convergence. Note that in this setting,
sufficient information exchange is more important for convergence because models from differ-
ent agents are moving to significantly diverse directions. In such case, DGD only converges with
smaller stepsize and its communication compressed variants, including QDGD, DeepSqueeze and
CHOCO-SGD, diverge in all parameter settings we try.
In summary, our experiments verify our theoretical analysis and show that LEAD is able to handle
data heterogeneity very well. Furthermore, the performance of LEAD is robust to parameter settings
and needs less effort for parameter tuning, which is critical in real-world applications.
6	Conclusion
In this paper, we investigate the communication compression in decentralized optimization. Mo-
tivated by primal-dual algorithms, a novel decentralized algorithm with compression, LEAD, is
proposed to achieve faster convergence rate and to better handle heterogeneous data while enjoy-
ing the benefit of efficient communication. The nontrivial analyses on the coupled dynamics of
inexact primal and dual updates as well as compression error establish the linear convergence of
LEAD when full gradient is used and the linear convergence to the O(σ2) neighborhood of the
optimum when stochastic gradient is used. Extensive experiments validate the theoretical analysis
and demonstrate the state-of-the-art efficiency and robustness of LEAD. LEAD is also applicable
to non-convex problems as empirically verified in the neural network experiments but we leave the
non-convex analysis as the future work.
9
Published as a conference paper at ICLR 2021
Acknowledgements
Xiaorui Liu and Dr. Jiliang Tang are supported by the National Science Foundation (NSF) un-
der grant numbers CNS-1815636, IIS-1928278, IIS-1714741, IIS-1845081, IIS-1907704, and IIS-
1955285. Yao Li and Dr. Ming Yan are supported by NSF grant DMS-2012439 and Facebook
Faculty Research Award (Systems for ML). Dr. Rongrong Wang is supported by NSF grant CCF-
1909523.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems,pp. 1709-1720. 2017.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th In-
ternational Conference on Machine Learning, pp. 559-568, 2018.
Ruggero Carli, Fabio Fagnani, Paolo Frasca, and Sandro Zampieri. Gossip consensus algorithms
via quantized communication. Automatica, 46(1):70-80, 2010.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Urban Stich, and Martin Jaggi. Error feed-
back fixes SignSGD and other gradient compression schemes. In Proceedings of the 36th Inter-
national Conference on Machine Learning, pp. 3252-3261. PMLR, 2019.
Anastasia Koloskova, Sebastian U. Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. In Proceedings of the 36th International
Conference on Machine Learning, pp. 3479-3487. PMLR, 2019.
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning
with arbitrary communication compression. In International Conference on Learning Represen-
tations, 2020.
Yao Li and Ming Yan. On linear convergence of two decentralized algorithms. arXiv preprint
arXiv:1906.07225, 2019.
Zhi Li, Wei Shi, and Ming Yan. A decentralized proximal-gradient method with network indepen-
dent step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, 67
(17):4494-4506, 2019.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Qing Ling, Wei Shi, Gang Wu, and Alejandro Ribeiro. DLM: Decentralized linearized alternating
direction method of multipliers. IEEE Transactions on Signal Processing, 63(15):4051-4064,
2015.
Xiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan. A double residual compression algorithm for
efficient distributed learning. The 23rd International Conference on Artificial Intelligence and
Statistics, 2020.
Yucheng Lu and Christopher De Sa. Moniqua: Modulo quantized communication in decentralized
SGD. In Proceedings of the 37th International Conference on Machine Learning, 2020.
Sindri Magnusson, Hossein Shokri-Ghadikolaei, and Na Li. On maintaining linear convergence of
distributed learning and optimization under limited communication. IEEE Transactions on Signal
Processing, 68:6101-6116, 2020.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takac, and Peter Richtarik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
10
Published as a conference paper at ICLR 2021
Joao FC Mota, Joao MF Xavier, Pedro MQ Aguiar, and Markus PuscheL	D-ADMM: A
communication-efficient distributed algorithm for separable optimization. IEEE Transactions on
SignalProcessing, 61(10):2718-2723, 2013.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.
Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed
optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597-2633, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Shi Pu and Angelia Nedic. Distributed stochastic gradient tracking methods. Mathematical Pro-
gramming, pp. 1-49, 2020.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, and Ramtin Pedarsani. An exact quan-
tized decentralized gradient descent algorithm. IEEE Transactions on Signal Processing, 67(19):
4934-4947, 2019a.
Amirhossein Reisizadeh, Hossein Taheri, Aryan Mokhtari, Hamed Hassani, and Ramtin Pedarsani.
Robust and communication-efficient collaborative learning. In Advances in Neural Information
Processing Systems, pp. 8388-8399, 2019b.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and
application to data-parallel distributed training of speech DNNs. In Interspeech 2014, September
2014.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. EXTRA: An exact first-order algorithm for decen-
tralized consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.
Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
4452-4463, 2018.
Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. Communication compression for
decentralized training. In Advances in Neural Information Processing Systems, pp. 7652-7662.
2018a.
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2 : Decentralized training over
decentralized data. In Proceedings of the 35th International Conference on Machine Learning,
pp. 4848-4856, 2018b.
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang, and Ji Liu. Deepsqueeze:
Decentralization meets error-compensated compression. CoRR, abs/1907.07346, 2019a. URL
http://arxiv.org/abs/1907.07346.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In Proceedings of the 36th
International Conference on Machine Learning, pp. 6155-6165, 2019b.
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic
and stochastic gradient optimization algorithms. IEEE transactions on automatic control, 31(9):
803-812, 1986.
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized SGD
and its applications to large-scale distributed optimization. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, pp. 5325-5333, 2018.
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
Letters, 53(1):65-78, 2004.
Jinming Xu, Ye Tian, Ying Sun, and Gesualdo Scutari. Accelerated primal-dual algorithms for
distributed smooth convex optimization over networks. In International Conference on Artificial
Intelligence and Statistics, pp. 2381-2391. PMLR, 2020.
11
Published as a conference paper at ICLR 2021
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
Journal on OPtimization, 26(3):1835-1854, 2016.
Kun Yuan, Bicheng Ying, Xiaochuan Zhao, and Ali H Sayed. Exact diffusion for distributed opti-
mization and learning—part i: Algorithm development. IEEE Transactions on Signal Processing,
67(3):708-723, 2018.
Kun Yuan, Wei Xu, and Qing Ling. Can primal methods outperform primal-dual methods in decen-
tralized dynamic optimization? arXiv PrePrint arXiv:2003.00816, 2020.
12
Published as a conference paper at ICLR 2021
Contents of Appendix
A	LEAD in agent’s perspective	14
B	Connections with exiting works	14
C	Compression method	15
C.1 p-norm b-bits quantization ................................................ 15
C.2 Compression error ......................................................... 16
D	Experiments	17
D.1 Parameter sensitivity ..................................................... 17
D.2 Experiments in homogeneous setting ........................................ 17
D.3 Parameter settings ........................................................ 18
E	Proofs of the theorems	19
E.1 Illustrative flow ......................................................... 19
E.2 Two central Lemmas ........................................................ 20
E.3 Proof of Lemma 1 .......................................................... 20
E.4 Proof of Lemma 2 .......................................................... 22
E.5 Proof of Theorem 1 ........................................................ 23
E.6 Proof of Theorem 2 ........................................................ 28
13
Published as a conference paper at ICLR 2021
A LEAD in agent’ s perspective
In the main paper, we described the algorithm with matrix notations for concision. Here we further
provide a complete algorithm description from the agents’ perspective.
Algorithm 2 LEAD in Agent’s Perspective
input: stepsize η, compression parameters (α, γ), initial values xi0, hi1, zi, ∀i ∈ {1, 2, . . . , n}
output: XK, ∀i ∈ {1, 2,..., n} or Pi= Xi		
1	: for each agent i ∈ {1, 2, . . . , n} do	
2	:	di1 = zi -	j∈Ni∪{i} wij zj	
3	:	(hw)i1 = Pj∈Ni∪{i} wij (hw)j1	
4	χ1 = χ0 -ηVfi(X0;ξO)	
5	: end for	
6	: for k = 1, 2, . . . , K - 1 do in parallel for all agents i ∈ {1, 2, .	..,n}
7	compute Vfi(Xk; ξk)	B Gradient computation
8	:	yik = Xik - ηVfi(Xik; ξik) - ηdik	
9	:	qik = Compress(yik - hik)	B Compression
10	yk = hk + qk	
11	:	for neighbors j ∈ Ni do	
12	:	Send qik and receive qjk	B Communication
13	:	end for	
14	(yw)i = (hw)i + Pj∈Ni∪{i} Wijqj	
15	hk+1 = (1 - α)hk+ αyk	
16	(hw )k+1 = (1 — α)(hw )k + α(yw )k	
17	dk+1 = d + 2η 侬 — (yw )k)	
18	:	Xik+1 = Xik - ηVfi(Xik; ξik) - ηdik+1	B Model update
19	: end for	
B Connections with exiting works
The non-compressed variant of LEAD in Alg. 1 recovers NIDS (Li et al., 2019), D2 (Tang et al.,
2018b) and Exact Diffusion (Yuan et al., 2018) as shown in Proposition 1. In Corollary 3, we show
that the convergence rate of LEAD exactly recovers the rate of NIDS when C = 0, γ = 1 and σ = 0.
Proposition 1 (Connection to NIDS, D2 and Exact Diffusion). When there is no communication
compression (i.e., Yk = Yk) and Y = 1, Alg. 1 recovers D2:
χk+1 = I +W 0χk - Xk-I - ηVF(Xk； ξk) + ηVF(Xk-1; ξk-1)》	(15)
Furthermore, ifthe stochastic estimator Ofthe gradient VF(Xk; ξk) is replaced by thefull gradient,
it recovers NIDS and Exact Diffusion with specific settings.
Corollary 3 (Consistency with NIDS). When C = 0 (no communication compression), γ = 1 and
σ = 0 (full gradient), LEAD has the convergence consistent with NIDS with η ∈ (0, 2∕(μ + L)]:
Lk+1 ≤ max {1 - μ(2η - μη2),1 - 2λmaχ((I- W)t)} Lk.	(16)
See the proof in E.5.
Proofof Proposition 1. Let Y = 1 and Yk = Yk. Combing Lines 4 and 6 of Alg. 1 gives
Dk+1 = Dk + I - W (Xk — ηVF(Xk; ξk) — ηDk).	(17)
2η
14
Published as a conference paper at ICLR 2021
Based on Line 7, we can represent ηDk from the previous iteration as
ηDk = XkT - xk - ηVF(xk-1; ξk-1).	(18)
Eliminating both Dk and Dk+1 by substituting (17)-(18) into Line 7, we obtain
Xk+1 = Xk - ηVF(Xk; ξk) - (ηDk + I-W(Xk - ηVF(Xk; ξk) - ηDk))	(from (17))
=I+W(Xk -ηVF(Xk; ξk)) - I+WηDk
=1 +W (Xk - ηVF(Xk; ξk)) - 1 +W (XkT - Xk - ηVF(Xk-1; ξk-1)) (from (18))
=I+W(2Xk - XkT - ηVF(Xk; ξk) + ηVF(Xk-1; ξk-1)),	(19)
which is exactly D2. It also recovers Exact Diffusion with A = I+2W and M = ηI in Eq. (97)
of(Yuan etal., 2018).	口
C Compression method
C.1 p-norm b-bits quantization
Theorem 3 (p-norm b-bit quantization). Let us define the quantization operator as
Qp(x) := kxkp sign(x)2-(b-1)
2b-1|x|
Il I + U
kxkp
(20)
where ∙ is the Hadamardproduct, |x| is the elementwise absolute value and U is a random dither
vector uniformly distributed in [0, 1]d. Qp(x) is unbiased, i.e., EQp(x) = x, and the compression
variance is upper bounded by
Ekx - Qp(X)k2 ≤ 1 k Sign(X)2-(I)k2kχkp,	QI)
which suggests that ∞-norm provides the smallest upper bound for the compression variance due
to kXkp ≤ kXkq,∀X if 1 ≤ q ≤ p ≤ ∞.
Remark 7. For the compressor defined in (20), we have the following the compression constant
C
sup
x
k Sign(X)2TbT)『kxkp
W
PrOOf LetdenOte V = ∣∣χkp Sign(X)2-(I), S = 2bχkxl, si = [2bχkxl] and s2 = [2bχ⅛l] . We
can rewrite X as X = S ∙ v.
For any coordinate i such that si = (s1)i, we have Qp(Xi) = (s1)ivi with probability 1. Hence
EQp(X)i = Sivi = Xi and
E(Xi - Qp (X)i ) = (Xi - Sivi ) = 0.
For any coordinate i such that Si 6= (S1)i, we have (S2)i - (S1)i = 1 and Qp(X)i satisfies
(S1 )ivi, w.p. (S2 )i - Si,
Qp(X)i =	(S2)ivi, w.p. Si - (S1)i.
Thus, we derive
EQp(X)i = vi(S1)i(S2 - S)i + vi(S2)i(S - S1)i = viSi(S2 - S1)i = viSi = Xi,
15
Published as a conference paper at ICLR 2021
and
E[xi - Qp(x)i]2 = (xi - vi(s1)i)2(s2 - s)i + (xi - vi(s2)i)2(s - s1)i
=(S2 - s1)ix2 + ((s1)i(s2)i(s1 - s2)i + Si((S2) - (s1)2 ))v2 - 2si(s2 - s1)ixivi
=x2 + ( 一 (s1)i(s2)i + Si(S2 + SI)i)v2 ― 2sixivi
=(Xi - Sivi)2 + ( - (S1)i(S2)i + Si(S2 + S1)i - SIi)Vii
= (xi - Sivi)2 + (S2 - S)i(S - S1)ivi2
(S2 - S)i(S - S1 )ivi2
12
≤ -v2.
4i
Considering both cases, we have EQ(x) = x and
Ekx - Qp(x)k2 = X	E[xi - Qp(x)i]2 +	X	E[xi - Qp(x)i]2
{si=(s1)i}	{si 6=(s1 )i}
≤ 0 + 1 X	vi2
{si6=(s1)i}
≤ 4kvk2
=4 Il Sign(X)2-(bτ)k2kxkp.
□
C.2 Compression error
10^2
101Ow 一
Z=X-W-tx5l X=
1	2	3	4	5	6 inf
p-norm
Figure 5: Relative compression error kx-X(X)k2 for p-norm b-bit quantization
To verify Theorem 3, we compare the compression error of the quantization method defined in (20)
with different norms (p = 1, 2, 3, . . . , 6, ∞). Specifically, we uniformly generate 100 random vec-
tors in R10000 and compute the average compression error. The result shown in Figure 5 verifies
our proof in Theorem 3 that the compression error decreases when p increases. This suggests that
∞-norm provides the best compression precision under the same bit constraint.
Under similar setting, we also compare the compression error with other popular compression meth-
ods, such as top-k and random-k sparsification. The x-axes represents the average bits needed to rep-
resent each element of the vector. The result is showed in Fig. 6. Note that intuitively top-k methods
should perform better than random-k method, but the top-k method needs extra bits to transmit-
ted the index while random-k method can avoid this by using the same random seed. Therefore,
top-k method doesn’t outperform random-k too much under the same communication budget. The
result in Fig. 6 suggests that ∞-norm b-bits quantization provides significantly better compression
precision than others under the same bit constraint.
16
Published as a conference paper at ICLR 2021
1 O
O O
Z=X=K=s<5l X=
2 bits	4 bits	8 bits	16 bits 20 bits
Bits ∞nstraint
10^5
Figure 6: Comparison of compression error
kx-Q(X)k2
kxk2
between different compression methods
D Experiments
D.1 Parameter sensitivity
In the linear regression problem, the convergence of LEAD under different parameter settings of α
and γ are tested. The result showed in Figure 7 indicates that LEAD performs well in most settings
and is robust to the parameter setting. Therefore, in this paper, we simply set α = 0.5 and γ = 1.0
for LEAD in all experiment, which indicates the minor effort needed for parameter tuning.
2o1oooth3-43-5
111ooooo
11111
空 *xlk一
75
150
125
1001
E
75
50
25
——LEAD (α = 0.2)
2 10 12 3
Www-O-O-O
111
兰 *xlk一
—— LEAD (α = 0.2)
LEAD (α = 0∙4)
—— LEAD (α = 0.6)
—— LEAD (α = 0.8)
——LEAD (α=l)
25	50	75 100 125 150 175
Epoch
(b) Y = 0.6
50
25
O
1021000-20-40-6
111
兰 *xlMX=
d1 000-20-4不
111
兰 *xlXX=
75
150
125
Oo
75
50
25
Figure 7: Parameter analysis on linear regression problem.
D.2 Experiments in homogeneous setting
The experiments on logistic regression problem in homogeneous case are showed in Fig. 8 and
Fig. 9. It shows that DeepSqueeze, CHOCO-SGD and LEAD converges similarly while Deep-
17
Published as a conference paper at ICLR 2021
Squeeze and CHOCO-SGD require to tune a smaller γ for convergence as showed in the parameter
setting in Section D.3. Generally, a smaller γ decreases the model propagation between agents since
γ changes the effective mixing matrix and this may cause slower convergence. However, in the
setting where data from different agents are very similar, the models move to close directions such
that the convergence is not affected too much.
(a) Loss f(Xk)
Figure 8: Logistic regression in the homogeneous case (full-batch gradient)
100
10-1
10-1
10-1
0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Bits transmitted	1e10
(b)Loss f (Xk)
(a) Loss f(Xk)
Figure 9: Logistic regression in the homogeneous case (mini-batch gradient)
—— DGD (32 bits)
—— NIDS (32 bits)
——QDGD (2 bits)
--- DeePSqUeeZe (2 bits)
——CHOCO-SGD (2 bits)
—— LEAD (2 bits)
0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4
Bits transmitted	1e9
(b)Loss f (Xk)
D.3 Parameter settings
The best parameter settings we search for all algorithms and experiments are summarized in Ta-
bles 1- 4. QDGD and DeePSqUeeze are more sensitive to Y and CHOCO-SGD is slight more robust.
LEAD is most robust to parameter settings and it works well for the setting α = 0.5 and γ = 1.0 in
all exPeriments in this PaPer.
Algorithm	η	γ	α
DGD	0.1	-	-
NIDS	ɪr	-	-
QDGD 一	ɪr	0.2	-
DeepSqueeze	ɪr	0.2	-
CHOCO-SGD	ɪr	0.8	-
LEAD 一	~0T~	1.0	~05~
Table 1: Parameter settings for the linear regression Problem.
18
Published as a conference paper at ICLR 2021
Algorithm	η	Y	α
DGD	0.1	-	-
NIDS	ɪr	-	-
QDGD 一	ɪr	~(44~	-
DeepSqueeze	ɪr	~(4Γ	-
CHOCO-SGD	ɪr	~(66~	-
LEAD 一	ɪr	"T0~	~00.5~
Algorithm	η	Y	α
DGD	0.1	-	-
NIDS	ɪr	-	-
QDGD 一	ɪr	~(22~	-
DeePSqUeeze	ɪr	^o^	-
CHOCO-SGD	ɪr	^o^	-
LEAD 一	~0T~	"T0~	~05~
Homogeneous case	Heterogeneous case
Table 2: Parameter settings for the logistic regression problem (full-batch gradient).
Algorithm	η	Y	α
DGD	0.1	-	-
NIds	0.1	-	-
QDGD 一	0.05	~(22~	-
DeepSqueeze	0.1	^o^	-
CHOCO-SGD	0.1	^o^	-
LEAD 一	0.1	~Γ0~	~05~
Algorithm	η	Y	α
DGD	0.1	-	-
NIDS	0.1	-	-
QDGD 一	0.05	~(22~	-
DeepSqueeze	0.1	^o^	-
CHOCO-SGD	0.1	^o^	-
LEAD 一	0.1	~Γ0~	~05~
Algorithm	η	Y	α
DGD	0.1	-	-
NIDS	0.1	-	-
QDGD 一	0.05	ɪr	-
DeepSqueeze	0.1	~2Γ	-
CHOCO-SGD	0.1	^Ο^	-
LEAD 一	0.1	~Γ0~	~05~
Homogeneous case	Heterogeneous case
Table 3: Parameter settings for the logistic regression problem (mini-batch gradient).
Algorithm	η	Y	α
DGD	0.05	-	-
NIDS	0.1	-	-
QDGD 一	—*—	*^r~	-
DeepSqueeze	—*—	*^γ~	-
CHOCO-SGD	—*—	*^γ~	-
LEAD 一	0.1	~Γ0~	~05~
Homogeneous case	Heterogeneous case
Table 4: Parameter settings for the deep neural network. (* means divergence for all options we try)
E	Proofs of the theorems
E.1 Illustrative flow
The following flow graph depicts the relation between iterative variables and clarifies the range
of conditional expectation. {Gk }k∞=0 and {Fk }k∞=0 are two σ-algebras generated by the gradient
sampling and the stochastic compression respectively. They satisfy
Go ⊂ Fo ⊂Gι ⊂F1 ⊂…⊂Gk ⊂ Fk ⊂…
(X1, D1, H1)	(X2, D2, H2)	(X3,D3,H3)
、	小、	小
VF(X1iξ1)∈Go E1	VF(X2W2)EGi E2
' Y1 ____	_	> γ2 ——
..	1st round	..
V	U
Fo ------⊂-----> F1 -----
(Xk, Dk, Hk)
小
EkT
-> YkT
7F(Xk iξk)∈Gk-ι∣
∖i RTk
-----、Yk
(k-1)th round
----> Fk-2 -------> Fk-I
The solid and dashed arrows in the top flow illustrate the dynamics of the algorithm, while in the
bottom, the arrows stand for the relation between successive F -σ-algebras. The downward arrows
19
Published as a conference paper at ICLR 2021
determine the range of F -σ -algebras. E.g., up to Ek , all random variables are in Fk-1 and up
to VF(Xk; ξk), all random variables are in Gk-ι with Gk-ι ⊂ Fk-ι. Throughout the appendix,
without specification, E is the expectation conditioned on the corresponding stochastic estimators
given the context.
E.2 Two central Lemmas
Lemma 1 (Fundamental equality). Let X* be the optimal solution, D* := -VF(X*) and Ek
denote the compression error in the kth iteration, that is Ek = Qk — (Yk — Hk) = Yk 一 Yk.
From Alg. 1, we have
kxk+1 — X*k2 + (η2∕γ)kDk+1- D*kM
=kxk — x*k2 + (η2∕γ)kDk — D*kM — (η2∕γ )kDk+1 — DkkM — η2kDk+1 — D*k2
—2ηhXk — X*, VF(Xk; ξk) — VF(X*)i + η2kVF(Xk; ξk) — VF(X*)k2 + 2ηhEk, Dk+1 — D*i,
where M := 2(I — W)* — YI and Y < 2∕λmaχ(I — W) ensures the positive definiteness of M over
range(I — W).
Lemma 2 (State inequality). Let the same assumptions in Lemma 1 hold. From Alg. 1, if we take
the expectation over the compression operator conditioned on the k-th iteration, we have
EkHk+1 —X*k2 ≤ (1 — α)kHk —X*k2 + αEkXk+1 — X*k2 + αη2EkDk+1 — Dkk2
+ 2αη2 EkDk+1 — Dk kM + α2E∣∣Ek『—αγE∣∣Ek k2-W — α(1 — α)∣∣Yk — Hk ∣∣2.
Y-
E.3 Proof of Lemma 1
Before proving Lemma 1, we let Ek = Yk — Yk and introduce the following three Lemmas.
Lemma 3. Let X* be the consensus solution. Then, from Line 4-7 of Alg. 1, we obtain
I - W (Xk+1 — X*) = (I — I - W) (Dk+1 — Dk) — I - WEk.	(22)
2η	Y 2	2η
Proof. From the iterations in Alg. 1, we have
Dk+1 = Dk + ɪ (I — W)Yk	(from Line 6)
2η
=Dk + ɪ (I — W)(Yk + Ek)
2η
=Dk + ɪ (I — W)(Xk — ηVF(Xk; ξk) — ηDk + Ek)	(from Line 4)
2η
=Dk + ɪ (I — W)(Xk — ηVF(Xk; ξk) — ηDk+1 — X* + η(Dk+1 — Dk) + Ek)
2η
=Dk + 工(I — W)(Xk+1 — X*) + Y (I — W)(Dk+1 — Dk) + 工(I — W)Ek,
2η	2	2η
where the fourth equality holds due to (I — W)X* = 0 and the last equality comes from Line 7 of
Alg. 1. Rewriting this equality, and We obtain (22).	□
Lemma 4. Let D* = —VF(X*) ∈ span{I — W}, we have
hXk+1 — X*, Dk+1 — Dki = η∣∣Dk+1 — DkkM — hEk, Dk+1 — Dk〉，	(23)
Y
hXk+1 — X*, Dk+1 — D*i = ηhDk+1 — Dk, Dk+1 — D*〉m — hEk, Dk+1 — D*i,	(24)
Y
where M = 2(I — W)t — γI and Y < 2∕λmaχ(I — W) ensures the positive definiteness of M over
span{I — W}.
20
Published as a conference paper at ICLR 2021
Proof. Since Dk+1 ∈ span{I — W} for any k, we have
hXk+1 - X*, Dk+1 - Dk)
= h(I - W)(Xk+1 - X*), (I - W)t(Dk+1 - Dk)〉
=(；(2I - Y (I - W))(Dk+1 - Dk) - (I - W)Ek, (I - W)t(Dk+1 - Dk )〉	(from (22))
=(；(2(I - W)t - YI)(Dk+1 - Dk) - Ek, Dk+1 - Dk)
=η∣∣Dk+1 - DkIIM - <Ek, Dk+1
Y
Similarly, We have
Dk)
—
(Xk+1 - X*, Dk+1 - D*〉
= h(I - W)(Xk+1 - X*), (I - W)t(Dk+1 - D*)〉
=(；(2I - Y(I - W))(Dk+1 - Dk) - (I - W)Ek, (I - W)t(Dk+1 - D*),
=(；(2(I - W)t - I)(Dk+1 - Dk) - Ek, Dk+1 - D*)
=η(Dk+1 - Dk, Dk+1 - D*〉m - hEk, Dk+1 - D*〉.
Y
To make sure that M is positive definite over span{I - W}, we need Y < 2∕λmaχ(I - W).
Lemma 5. Taking the expectation conditioned on the compression in the kth iteration, we have
2ηE(Ek, Dk+1 -	D*〉= 2〃E	EEk, Dk	+ ɪ(I -	W)Yk	+ ɪ(I - W)Ek	- D*∖
∖	2η	2η	/
=YEhEk, (I - W)Ek〉= YEkEk∣2-w,
2ηE(Ek, Dk+1 - Dk)= 2ηE (Ek, ɪ(I - W)Yk + ɪ (I - W)Ek)
=YEhEk, (I - W)Ek〉= YEkEk∣2-w.
□
Proof. The proof is straightforward and omitted here.
□
ProofofLemma 1. From Alg. 1, we have
2η(Xk - X*, VF(Xk; ξk) - VF(X*)〉
=2(Xk - X*,ηVF(Xk; ξk) - ηVF(X*)i
=2{Xk - X*, Xk - Xk+1 - η(Dk+1 - D*)〉	(from Line 7)
=2{Xk - X*, Xk - Xk+1〉- 2ηhXk - X*, Dk+1 - D*〉
=2{Xk - X*, Xk - Xk+1〉- 2η(Xk - Xk+1, Dk+1 - D*〉- 2η(Xk+1 - X*, Dk+1 - D*〉
=2{Xk - X* - η(Dk+1 - D*), Xk - Xk+1〉- 2η(Xk+1 - X*, Dk+1 - D*〉
=2(Xk+1 - X* + η(VF(Xk; ξk) - VF(X*)), Xk - Xk+1〉- 2η(Xk+1 - X*, Dk+1 - D*〉
=2(Xk+1 - X*, Xk - Xk+1〉+ 2ηhVF(Xk; ξk) - VF(X*), Xk - Xk+1〉
-2ηhXk+1 - X*, Dk+1 - D*〉.	(25)
Then we consider the terms on the right hand side of (25) separately. Using 2(A - B, B - C〉=
∣∣A - C∣2 - ∣B - C∣2 - kA - B∣2, we have
2(Xk+1 - X*, Xk - Xk+1〉=2(X* - Xk+1, Xk+1 - Xk〉
= ∣Xk - X*k2 - ∣Xk+1 - Xkk2 - ∣Xk+1 - X*k2.	(26)
(from Line 7)
21
Published as a conference paper at ICLR 2021
Using 2(A, B) = IlAll2 + IIBll2 -IlA - B∣∣2, We have
2η"F(Xk; ξk) - VF(X*), Xk - Xk+1i
=η2∣∣VF(Xk; ξk) - VF(X*)k2 + ∣∣Xk - Xk+1k2 - ∣∣Xk - Xk+1 - η(VF(Xk; ξk) - VF(X*))k2
=η2∣VF(Xk; ξk) - VF(X*)k2 + ∣∣Xk - Xk+1∣2 - η2∣Dk+1 - D*k2.	(fromLine7)
(27)
Combining (25), (26), (27), and (23), we obtain
2η(Xk - X*, VF(Xk; ξk) - VF(X*))
∣Xk - X*∣2 - ∣Xk+1 - Xk∣∣2 - ∣Xk+1 - X*∣2
I---------------------------------------'
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2(Xk+1-X* ,Xk-Xk+1i
+ η2∣VF(xk；ξk) -VF(X*)∣2 + ∣∣xk - xk+1∣2 -η2∣Dk+1 - d*∣2
I------------------------------------------------------------------------------------------'
2η<VF(Xk5ξk )-VF(X*),Xk-Xk+1i
(Dk+1 - Dk, Dk+1 - D*〉M - 2η(Ek, Dk+1 - D*))
—
|
"^^^^^^^^^^^^^^^^^^^^^^^^^~^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^""^
2n(Xk +1-X*,Dk +1-D*i
= ∣Xk - X*k2 - ∣Xk+1 - XkIl2 - ∣Xk+1 - X*k2
+ η2∣VF(Xk;ξk) -VF(X*)k2 + ∣∣Xk - Xk+1∣2 - η2∣Dk+1 - D*∣2
2
+ - (∣Dk - D*∣M - ∣Dk+1 - D*∣M - ∣Dk+1 - DkkM) +2ηhEk, Dk+1 - D*),
Y I-------------------------------------------------------------'
-2hDfc+i-Dfc ,Dfc+i-D*>M
where the last equality holds because
2(Dk - Dk+1, Dk+1 - D*)m =IIDk- D*∣∣M -∣∣Dk+1 - D*∣∣M -∣∣Dk+1 - Dk∣∣M∙
Thus, we reformulate it as
2
∣Xk+1 - X*∣2 + η∣Dk+1 - D*∣M
Y
22
= ∣Xk - X*∣2 + η-∣Dk - D*∣M - — ∣Dk+1 - Dk∣M - η2∣Dk+1 - D*∣2
Y	Y
-2η(Xk - X*, VF(Xk; ξk) - VF(X*)) + η2∣VF(Xk; ξk) - VF(X*)∣2 + 2η(Ek, Dk+1 - D*),
which completes the proof.	□
E.4 Proof of Lemma 2
ProofofLemma 2. From Alg. 1, we take the expectation conditioned on kth compression and obtain
EkHk+1 - X*∣2
=Ek(I - α)(Hk - X*) + α(Yk - X*) + αEk∣∣2	(fromLine 13)
= ∣(1 - α)(Hk - X*) + α(Yk - X*)∣2 + α2E∣Ek∣∣2
=(1 - α)∣Hk - X*∣2 + α∣Yk - X*∣2 - α(1 - α)∣Hk - Yk∣∣2 + α2EkEk∣∣2.	(28)
In the second equality, we used the unbiasedness of the compression, i.e., EEk = 0. The last
equality holds because of
k(1 - α)A + αBk2 = (1 - α)∣∣A∣∣2 + α∣∣B∣∣2 - α(1 - α)∣A - B∣∣2.
22
Published as a conference paper at ICLR 2021
In addition, by taking the conditional expectation on the compression, We have
∣∣Yfc - X*k2 =∣∣Xk - ηVF(Xk; ξk) - ηDk - X*∣∣2	(from Line 4)
=EkXk+1 + ηDk+1 - ηDk - X* ∣2	(from Line 7)
=EkXk+1 - X*∣2 + η2E∣Dk+1 - Dk∣∣2 + 2ηE(Xk+1 - X*, Dk+1 - Dki
=EkXk+1 - X*∣2 + η2E∣Dk+1 - Dk∣∣2
2η2
+ -ɪEkDk+1 - DkkM - 2ηE(Ek, Dk+1 - Dk).	(from (23))
Y
=EkXk+1 - X*k2 + η2EkDk+1 - Dk∣∣2
十 比EkDk+1 - DkkM - YEkEkk2-w.	(rOmLine 6)	(29)
Y
Combing the above two equations (28) and (29) together, we have
EkHk+1 - X*k2
≤(1 - α)∣∣Hk - X*k2 + αE∣∣Xk+1 - X*k2 + αη2E∣∣Dk+1 - Dk∣∣2 + 2αη2EkDk+1 - Dk∣∣M
Y
-αγE∣Ek∣2-w + α2E∣Ek∣2 - α(1 - α)∣Yk - Hk∣2,	(30)
which completes the proof.	口
E.5 PROOF OF THEOREM 1
ProofofTheorem 1. Combining Lemmas 1, 2, and 5, we have the expectation conditioned on the
compression satisfying
E∣Xk+1 - X*k2 + η2E∣Dk+1 - D*kM + αιE∣Hk+1 - X*∣2
Y
22
≤∣Xk - X*k2 + — ∣∣Dk - D*kM - — EkDk+1 - DkkM - η2E∣Dk+1 - D*∣2
YY
-2η(Xk - X*, VF(Xk; ξk) -VF(X*))+ η2∣VF(Xk; ξk) - VF(X*)∣2 + YEkEk∣∣2-w
+ α1(1 - α)∣Hk - X*∣2 + α1αE∣Xk+1 - X*∣2 + a1 αη2E∣Dk+1 - Dk∣∣2
+ 2αiαη2E∣Dk+1 - DkkM + aια2E∣Ek∣2 - aιαYE∣Ekk2-w - aια(1 - α)∣Yk - Hk∣2
Y
=∣Xk - X*k2 - 2η(Xk - X*, VF(Xk;ξk) - VF(X*)i + η2∣VF(Xk; ξk) - VF(X*)∣2
X-----------------------------V------------------------------}
A
2
+ aιɑE∣Xk+1 - X*k2 + — ∣∣Dk - D*∣M - η2E∣Dk+1 - D*∣2
Y
2
+ a1(1 - α)∣∣Hk - X*k2 -(1 - 2ae)ηE∣∣Dk+1 - Dk∣∣M + a1Qη2E∣∣Dk+1 - Dk||2
Y
---------------------V--------------------'
B
+ a1α2E∣∣Ek∣2 + (1 - a1α)YE∣Ek∣2-w -。1。(1 - α)∣Yk - Hk∣2,	(31)
-----------------------V----------------------'
C
where a1 is a non-negative number to be determined. Then we deal with the three terms on the right
hand side separately. We want the terms B and C to be nonpositive. First, we consider B. Note that
Dk ∈ Range (I - W). If we want B ≤ 0, then, we need 1 - 2a1α > 0, i.e., a[a < 1/2. Therefore
we have
2
B = - (1 - 2a1α)LE∣Dk+1 - DkkM + a1αη2E∣Dk+1 - Dk∣∣2
YM
≤ fa1α - (I- 2a1α)λn-1(M)) η2EkDk+1 - Dkk2,
Y
23
Published as a conference paper at ICLR 2021
where λn-ι(M) > 0 is the second smallest eigenvalue of M. It means that we also need
(2a1α - l)λn-1(M) /n
Q]Q +----------------≤ 0,
Y	_
which is equivalent to
V λn-1(M)	1 K)	O
aια ≤ ——<1/ < 1∕2∙	(32)
Y + 2λn-i(M)
Then we look at C. We have
C =αια2E∣∣Ek∣∣2 + (1- α1 α)γE∣∣Ek∣∣2-w - aiα(1 - α)∣∣Yk - Hk∣∣2
≤((1 - a1α)βγ + α1α2)E∣∣Ek∣∣2 - a1α(1 - α)∣∣Yk - Hk∣∣2
≤C((1 - a1α)βγ + α1α2)∣Yk - Hk∣∣2 - a1α(1 - α)∣Yk - Hk∣∣2
Because we have 1 - a1α > 1/2, so we need
C((1 — a∖α)βγ + aιɑ2) — aɪα(1 — α) = (1 + C)aγα2 — a∖(Cβγ + 1)α + Cβγ ≤ 0.	(33)
That is
ɑ >
aι(Cβγ + 1) - ,a2(Cβγ + 1)2 - 4(1 + C)CO^
2(1 + C )aι
a ≤
aι(Cβγ +1) + ,a2(Cβγ + 1)2 - 4(1 + C)COβf
2(1 + C )aι
(34)
(35)
Next, we look at A. Firstly, by the bounded variance assumption, we have the expectation condi-
tioned on the gradient sampling in kth iteration satisfying
EkXk - X*∣∣2 - 2ηE(Xk - X*, VF(Xk;ξk) - VF(X*)〉+ η2E∣VF(Xk;ξk) -VF(X*)k2
≤∣Xk - X*k2 - 2ηhXk - X*, VF(Xk) -VF(X*))+ η2∣VF(Xk) -VF(X*)k2 + nη2σ2
Then with the smoothness and strong convexity from Assumptions 4, we have the co-coercivity of
Vgi(x) with gi(x) := fi(x) - 2 ∣∣x∣2, which gives
hXk - X*, VF(Xk) - VF(X*)i > -μL-IIXk - X*∣2 + -ɪ-∣∣VF(Xk) - VF(X*)∣2.
μ + L	μ + L
When η ≤ 2∕(μ + L), we have
(Xk - X*, VF(Xk) - VF(X*))
1 η(μ + L)
1 ----------
> μ-
2
ημ(μ + L)
=μ 1
2
ηB
K - X*, VF(Xk) - VF(X*)i + η(μ + L) K - X*, VF(Xk) - VF(X*))
+	∣∣Xk - X*∣2 + η∣∣VF(Xk) -VF(X*)∣2
∣∣Xk - X*∣2 + η∣∣VF(Xk) - VF(X*)∣2.
Therefore, we obtain
-2η(Xk - X*, VF(Xk) - VF(X*))
≤ - η2∣VF(Xk) - VF(X*)∣∣2 - μ(2η - μη2)∣Xk - X*∣∣2.
(36)
Conditioned on the kthe iteration, (i.e., conditioned on the gradient sampling in kth iteration), the
inequality (31) becomes
EkXk+1 - X*∣2 + 也EkDk+1 - D*∣M + a1E∣Hk+1 - X*∣2
Y
≤ (1 - μ(2η - μη2)) ∣Xk - X*『+ a1αE∣Xk+1 - X*∣∣2
2
+ LIlDk - D*∣∣M - η2E∣Dk+1 - D*∣∣2 + a1(1 - α)∣∣Hk - X*∣∣2 + nη2σ2,	(37)
Y
24
Published as a conference paper at ICLR 2021
if the step size satisfies η ≤ μ+^i. Rewriting (37), We have
(1 - aια)EkXk+1 - X*∣∣2 + η2E∣∣Dk+1 - D*∣∣M + η2E∣∣Dk+1 - D*∣∣2 + aιEkHk+1 - X*∣∣2
γ
2
≤ (1 - μ(2η - μη2)) kXk - X*k2 + ηγkDk - D*kM + aι(1 - α)∣∣Hk - X*∣∣2 + nη2σ2,
(38)
and thus
(1 - aια)EkXk+1 - X*『+ η2E∣∣Dk+1 - D*kM+γI +。国旧®+1 - X*『
2
≤(1 - μ(2η - μη2)) kXk - X*k2 + 片kDk - D*kM + aι(1 - α)kHk - X*k2 + nη2σ2.
(39)
With the definition of Lk in (12), we have
ELk+1 ≤ ρLk + nη2σ2,
(40)
with
ʃ 1 一 μ(2η 一 μη2)	λmaχ(M)
P = max \ ——;-------,-ɪʒ——7χvv, 1 — α
1 - a1α	γ + λmax(M)
where
λmaχ(M) = 2λmaχ((I - W)1)- Y
Recall all the conditions on the parameters a1 , α, and γ to make sure that ρ < 1:
λn-1(M)
aι ɑ ≤ --------7——,
一γ + 2λn-i(M),
aiα ≤ μ(2η — μη2),
(41)
(42)
α≥
aι(Cβγ + 1) - √a2(Cβγ + 1)2 - 4(1 + C)Caιβγ
2(1 + C)a1
: α0,
(43)
α≤
aι(Cβγ + 1) + √a2(Cβγ + 1)2 - 4(1 + C)Caιβγ
2(1 + C)a1
: α1.
(44)
In the following, we show that there exist parameters that satisfy these conditions.
Since we can choose any a1, we let
a1
4(1 + C)
Cβγ + 2,
such that
a21(Cβγ+1)2
-4(1 + C)Ca1βγ = a12.
Then we have
Cβγ
α0 =2(1 + C)
Cβγ + 2
→ 0,
as γ → 0,
α1 =2(1 + C)
1
→ 1+C,
as γ → 0.
Conditions (43) and (44) show
a1α ∈
2Cβγ
Cβγ + 2,
2 → [0, 2], if
C = 0 orγ → 0.
25
Published as a conference paper at ICLR 2021
Hence in order to make (41) and (42) satisfied, it’s sufficient to make
2CβY / λ λ	λn-l(M)	小 2、]	. [I - YG 2、1
La ≤ min < ——7μ(2η μ(2η - μη ) > = min < 4----------, μ(2η - μη )》.
CeY + 2	IY + 2λn-ι(M)	J	[4 - γ	J
where We use λn-1 (M) = Imax(I-W) - Y = I - Y.
When C > 0, the condition (45) is equivalent to
(45)
Y ≤ min
((3C + 1) - P(3C +1)2 - 4C
ɪ	Cβ
2μη(2 - μη)
[2 - μη(2 - μη)]Cβ
(46)
}
The first term can be simplified using
(3C +1) - P(3C +1)2 - 4C、	2
Ce	≥ (3C +1)β
due to √1 - X ≤ 1 一 I when X ∈ (0,1).
Therefore, for a given stepsize η, if we choose
Y∈
0, min
2	2μη(2 — μη)
(3C +1)β, [2 - μη(2 - μη)]Cβ
and
α∈
CeY
2(1 + C)
min
CeY + 2 2 - βY CβY + 2 小 ∖CβY + 2
2(1 + C), 4 - βY 4(1 + C), μη(2 μη)4(1 + C)
o,
then, all conditions (41)-(44) hold.
Note that Y < (30+1). implies Y < ∣, which ensures the positive definiteness of M over span{I -
W} in Lemma 4.
Note that η ≤ μ+L ensures
CeY + 2 CeY + 2
μη(2 - μη)4(1+C) ≤ 2(1+C).
(47)
So, we can simplify the bound for α as
α∈
-CeY . 「2 - eY CeY + 2	ʌ CeY + 2
2(1 + C), min 14 - eY 4(1 + C) ,“η( μη) 4(1 + C)
Lastly, taking the total expectation on both sides of (40) and using tower property, we complete the
proof for C > 0.
□
ProofofCorollary 1. Let,s first define Kf = L and Kg = λmax(I-W) = λmaχ(I - W)λmaχ((I -
W)t).	min
We can choose the stepsize η == such that the upper bound of Y is
Yupper
.2__2_______K (2 - M)	21	. ʃ
mint(3C+W, h2-吉(2-焉)iCe,e} ≥mini
(3C⅛,κf⅛}
due to 2xX(--X) ≥ 2-X ≥ X when X ∈ (0, 1).
Hence We can take Y = min{(30+^, f⅛}.
26
Published as a conference paper at ICLR 2021
The bound of α is
J Cβγ . ∫2 - βγCβγ + 2 1	1 Cβγ + 21]
α ∈ ⅛TC), min14-iγ WC),κf (2 - Kf )4(1+C)力
When Y is chosen as K Ce, Pick
Cβγ	1
α =--------=---------.
2(1 + C)	2(1 + C)Kf
(48)
When (3C+i)β ≤ Jae，the UPPer bound of α is
2-βγCβγ+2
αupper = min(4-βγ 4(T+C)
1(2 - ɪ) CβY + 2 ]
f Kf 4(1 + C) J
6C +1	1	1 J	7C + 2
12C + 3, Kf( - Kf )ʃ 4(C +1)(3C +1)
6C + 1	1 1	7C + 2
12C + 3, Kf J 4(C +1)(3C +1).
In this case, we Pick
α = mm[ 口, ɪ
[12C + 3, Kf
7C + 2
4(C +1)(3C +1)
(49)
Note α = O ((1+C)K『)since 粽皋 is lower bounded by 1. Hence in both cases (Eq. (48) and
Eq. (49)), α = O ((i+C)κ于),and the third term of P is upper bounded by
1 L ∫1	1	1	∙ 6 6C +1	11	7C +2	1
α ≤ max {	2(1 + C)Kf ,	m叫 12C + 3 ,Kf J 4(1 + C)(3C +1) J
In two cases of γ, the second term of ρ becomes
1-
_______Y________
2λmaχ((I - W)t)
max{1- 2CKfKg, 1 -
1
(1 + 3C )Kg
Before analysing the first term of ρ, We look at aιa in two cases of γ. When Y = -‰；,
κfCe
2
a1α
2Cβγ _	2	1
CeY + 2	2Kf + 1 — Kf .
When Y
1
(3C+1)β,
a1 α = min
6C +1	_1 ]< _1
(12C + 3), Kfj K Kf
In both cases, aια < K-. Therefore, the first term of P becomes
1 — μη(2 — μη) < 1 - f(2 - κf)= I - 1 - κf = 1 - ɪ
1 一 a1a	1 — K—	Kf — 1	Kf
To summarize, we have
P < 1 - min
1	1	1	1
Kf, 2CKf Kg , (1 + 3C)Kg , 2(1 + C)Kf
min(^C±1 ,与
[12C + 3, Kfj
7C + 2
4(1 + C)(3C +1)
27
Published as a conference paper at ICLR 2021
and therefore
P = max 11 -O(7-——工一),1 — O(K——工一),1 — O(——)(.
(1 + C)κf	(1 + C)κg	Cκf κg
With full-gradient (i.e., σ = 0), we get -accuracy solution with the total number of iterations
k ≥ Oe((1 + C)(κf + κg) + Cκf κg).
When C = 0, i.e., there is no compression, the iteration complexity recovers that of NIDS,
O(κf+κg).
When C ≤ “广++；+长,the complexity is improved to that of NIDS, i.e., the compression doesn't
harm the convergence in terms of the order of the coefficients.	□
ProofofCorollary 2. Note that (Xk)> = Xk and 1n×ιX* = X*, then
n
X Ekxk - Xk k2 = E∣∣xk - in×1Xk∣∣2
i=1
=E∣∣Xk - X* + X* - 1n×lXk∣∣2
≤ EkXk - X*k2
V ρELk-1 + nη2σ2(1 — ρ)-1
1 - a1α
≤ 2ρk L0 + 2 心.	(50)
1-ρ
The last inequality holds because We have aια ≤ 1/2.	□
Proof of Corollary 3. From the proof of Theorem 1, when C = 0, we can set γ = 1, α = 1, and
aι = 0. Plug those values into ρ, and we obtain the convergence rate for NIDS.	□
E.6 Proof of Theorem 2
Proof of Theorem 2. In order to	get exact	convergence,	we	pick diminishing	step-size, set α
CeY aα = 2CβYk θ∣ = _____________1____L and θc =	Ce then
2(1 + C), a1α	Cβγk + 2 , θ1	2λmaχ((I-W)t) and θ2	2(1 + C), Ihen
ρk = max 1 -
μηk(2 - μηk) - aια
1 - a1α
1 - θ1γk, 1 - θ2γk
If we further pick diminishing ηk and Yk such that μηk (2 - μηk) - aια ≥ aια, then
μηk(2 - μηk) - aια ≥ aια = 2Cβγk
1 - a1 α	1 - a1α 2 - Cβγk
≥ Cβγk .
Notice that Cβγ ≤ 3 since (3C + 1) - P(3C + 1)2 - 4C is increasing in C > 0 with limit 2 at
∞.
In this case we only need,
γk ∈ 0, min
(3C +I)- p(3C + 1)2 - 4C	2μηk(2 — μηk)	2 o)
,[4 - μηk(2 - μηk)]Cβ, β)j
Cβ
(51)
And
ρk ≤ max {1 - Cβγk, 1 - θ1γk, 1 - θ2γk} ≤ 1 - θ3γk
28
Published as a conference paper at ICLR 2021
if θ3 = min{θ1, θ2} and note that θ2 ≤ Cβ.
We define
Lk := (1 - aιαk)∣∣Xk - X*∣∣2 + MlY)E∣∣Dk+1 - D*k2i-w)t + a"∣Hk - X*∣∣2.
Hence
ELk+1 ≤ (1 - θ3γk)ELk + nσ2 ηk2 .
From aιa ≤ μηk(2-μηk), We get
4CβYk
CβYk + 2
≤ μηk(2 - μηk).
If We pick γk = θ4ηk, then it’s sufficient to let
2Cβθ4ηk ≤ μηk(2 - μηk).
Hence if θ4 < Ce and let η* = 2(μ-Cβθ4) ,then ηk =若 ∈ (0, η*) guarantees the above discussion
and
ELk+1 ≤ (1 - θ3θ4ηk)ELk + nσ2ηk2 .
So far all restrictions for ηk are
一ʃ 2
ηk ≤ min j 丛 + L ,η*
and	____________
/ 1	. ( (3C +1)- √(3C +1)2 - 4C 2 1
ηk ≤ θ4min∣---------Ce----------，ef
Let θ5 = min ∣μ⅛,η*, (3C+1)-√(3C +1)2-4C,廉}, ηk = B⅛A and D = max {AL0,需
we claim that if we pick B = θ2θ4 and some A, by setting ηk = 6血2+24, we get
ELk ≤
D
Bk + A
Induction:
When k = 0, it’s obvious. Suppose previous k inequalities hold. Then
ELk + 1 ≤ A-	2θ3θ4	ʌ 2D +	4nσ2
≤ '	θ3θ4k + 2A7 θ3θ4k + 2A + (θ3θ4k + 2A)2
Multiply M := (θ3θ4k + θ3θ4 + 2A)(θ3θ4k + 2A)(2D)-1 on both sides, we get
M ELk+1 ≤
1-
2θ3θ4	、
θ3θ4 k + 2A J
(θ3θ4k + θ3θ4 + 2A) +
4nσ2(θ3θ4k + θ3θ4 + 2A)
2D(θ3θ4k + 2A)
2D(θ3θ4k + 2A - 2θ3θ4)(θ3θ4k + θ3θ4 + 2A) + 4nσ2(θ3θ4k + θ3θ4 + 2A)
2D(θ3θ4k + 2A)
2D(θ3θ4k + 2A)2 + 4nσ2(θ3θ4k + 2A) - 4Dθ3θ4(θ3θ4k + 2A) + 2Dθ3θ4(θ3θ4k + 2A)
=	2D(θ3θ4k + 2A)
ι -4D(θ3θ4)2 + 4nσ2θ3θ4
+	2D(θ3 θ4k + 2A)
≤θ3θ4k + 2A.
Hence
ELk+1 ≤
2D
θ3θ4 (k + 1) + 2A
29
Published as a conference paper at ICLR 2021
This induction holds for any A such that ηk is feasible, i.e.
no = 1 ≤ θ5.
Here we summarize the definition of constant numbers:
θ1 = 2λmaχ((I - W)t) , θ2 = 2(1 + C) ,	(52)
θ3 = min{θ1, θ2}, θ4 ∈ (0, 号), 小=迎一FβO" ,	(53)
∖ CeJ	μ2
2 2	(3C +1) - √(3C +1)2 - 4C 2 ]
θ5 =min { E M ()	Cβθ4	), 布卜	(54)
Therefore, let A = θ5 and nk = θ3θ42θ⅛+2,We get
1 ELk ≤ 2max {nL0, 铁o .
n	θ3θ4θ5k +2	.
Since 1 - a1αk ≥ 1/2, We complete the proof.
□
30