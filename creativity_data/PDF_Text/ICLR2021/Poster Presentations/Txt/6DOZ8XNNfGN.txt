Published as a conference paper at ICLR 2021
Graph Traversal with Tensor Functionals:
A meta-Algorithm for Scalable Learning
Elan Markowitz*,1,2, Keshav Balasubramanian*,1, Mehrnoosh Mirtaheri*,1,2, Sami Abu-El-Haija*,1,2
* Equal Contribution
1 University of Southern California, 2USC Information Sciences Institute
Bryan Perozzi
Google Research
Greg Ver Steeg1,2, Aram Galstyan1,2
Ab stract
Graph Representation Learning (GRL) methods have impacted fields from chem-
istry to social science. However, their algorithmic implementations are special-
ized to specific use-cases e.g. message passing methods are run differently from
node embedding ones. Despite their apparent differences, all these methods uti-
lize the graph structure, and therefore, their learning can be approximated with
stochastic graph traversals. We propose Graph Traversal via Tensor Functionals
(GTTF), a unifying meta-algorithm framework for easing the implementation of
diverse graph algorithms and enabling transparent and efficient scaling to large
graphs. GTTF is founded upon a data structure (stored as a sparse tensor) and
a stochastic graph traversal algorithm (described using tensor operations). The
algorithm is a functional that accept two functions, and can be specialized to
obtain a variety of GRL models and objectives, simply by changing those two
functions. We show for a wide class of methods, our algorithm learns in an unbi-
ased fashion and, in expectation, approximates the learning as if the specialized
implementations were run directly. With these capabilities, we scale otherwise
non-scalable methods to set state-of-the-art on large graph datasets while being
more efficient than existing GRL libraries - with only a handful of lines of code
for each method specialization. GTTF and its various GRL implementations are
on: https://github.com/isi-usc-edu/gttf
1	Introduction
Graph representation learning (GRL) has become an invaluable approach for a variety of tasks, such
as node classification (e.g., in biological and citation networks; Velickovic et al.(2018); KiPf &
Welling (2017); Hamilton et al. (2017); Xu et al. (2018)), edge classification (e.g., link prediction
for social and Protein networks; Perozzi et al. (2014); Grover & Leskovec (2016)), entire graPh
classification (e.g., for chemistry and drug discovery Gilmer et al. (2017); Chen et al. (2018a)), etc.
In this work, we ProPose an algorithmic unification of various GRL methods that allows us to
re-imPlement existing GRL methods and introduce new ones, in merely a handful of code lines Per
method. Our algorithm (abbreviated GTTF, Section 3.2), receives graphs as input, traverses them
using efficient tensor1 operations, and invokes specializable functions during the traversal. We show
function specializations for recovering popular GRL methods (Section 3.3). Moreover, since GTTF
is stochastic, these specializations automatically scale to arbitrarily large graphs, without careful
derivation per method. Importantly, such specializations, in expectation, recover unbiased gradient
estimates of the objective w.r.t. model parameters.
1To disambiguate: by tensors, we refer to multi-dimensional arrays, as used in Deep Learning literature; and
by operations, we refer to routines such as matrix multiplication, advanced indexing, etc
1
Published as a conference paper at ICLR 2021
(a) Example graph G
(b) Adjacency matrix
for graph G
(c) CompactAdj for G with
sparse Ab ∈ Zn×n and
dense δ ∈ Zn . We store
IDs of adjacent nodes in Ab
4；	(0
ɪ K
3兀)	(1)(1
/ivɪɪk
4 式2乂0*0工2)(4*
(d) Walk Forest. GTTF in-
Vokes AC C UMU LAT EFN once
per (green) instance.
Figure 1: (c)&(d) Depict our data structure & traVersal algorithm on a toy graph in (a)&(b).
GTTF uses a data structure Ab (Compact Adjacency, Section 3.1): a sparse encoding of the adjacency
matrix. Node v contains its neighbors in row A[v] , Av , notably, in the first degree(v) columns
of A[v]. This encoding allows stochastic graph traVersals using standard tensor operations. GTTF
is a functional, as it accepts functions ACCUMULATEFN and BIASFN, respectiVely, to be proVided
by each GRL specialization to accumulate necessary information for computing the objectiVe, and
optionally to parametrize sampling procedure p(v’s neighbors | v). The traVersal internally constructs
a walk forest as part of the computation graph. Figure 1 depicts the data structure and the computation.
From a generalization perspectiVe, GTTF shares similarities with Dropout (SriVastaVa et al., 2014).
Our contributions are: (i) A stochastic graph traVersal algorithm (GTTF) based on tensor operations
that inherits the benefits of Vectorized computation and libraries such as PyTorch and Tensorflow. (ii)
We list specialization functions, allowing GTTF to approximately recoVer the learning of a broad class
of popular GRL methods. (iii) We proVe that this learning is unbiased, with controllable Variance.
Wor this class of methods, (iV) we show that GTTF can scale preViously-unscalable GRL algorithms,
setting the state-of-the-art on a range of datasets. Finally, (V) we open-source GTTF along with new
stochastic traVersal Versions of seVeral algorithms, to aid practitioners from Various fields in applying
and designing state-of-the-art GRL methods for large graphs.
2	Related Work
We take a broad standpoint in summarizing related work to motiVate our contribution.
Method
əwɔs
ylimaF
Models
GCN, GAT MPl X
node2vec
WYS
gninraeL
exact
NE ✓ approx
NE X
exact
Stochastic Sampling Methods
SAGE	MP	✓	approx
FastGCN	MP	✓	approx
LADIES	MP	✓	approx
GraphSAINT	MP	✓	approx
CluterGCN	MP	✓	heuristic
Sofiware Frameworks
PyG	Both	inherits / re-
DGL	Both	implements
Algorithmic Abstraction (ours)
GTTF Bothl ∕∣ approx
Models for GRL have been proposed, including message
passing (MP) algorithms, such as Graph Convolutional Net-
work (GCN) (Kipf & Welling, 2017), Graph Attention (GAT)
(VelickOVic et al., 2018); as well as node embedding (NE) algo-
rithms, including node2vec (Grover & Leskovec, 2016), WYS
(Abu-El-Haija et al., 2018); among many others (Xu et al., 2018;
Wu et al., 2019; Perozzi et al., 2014). The full-batch GCN of
Kipf & Welling (2017), which drew recent attention and has mo-
tiVated many MP algorithms, was not initially scalable to large
graphs, as it processes all graph nodes at eVery training step.
To scale MP methods to large graphs, researchers proposed
Stochastic Sampling Methods that, at each training step, as-
semble a batch constituting subgraph(s) of the (large) input
graph. Some of these sampling methods yield unbiased gradi-
ent estimates (with some Variance) including SAGE (Hamilton
et al., 2017), FastGCN (Chen et al., 2018b), LADIES (Zou et al.,
2019), and GraphSAINT (Zeng et al., 2020). On the other hand,
ClusterGCN (Chiang et al., 2019) is a heuristic in the sense
that, despite its good performance, it proVides no guarantee of
unbiased gradient estimates of the full-batch learning. Gilmer
et al. (2017) and Chami et al. (2021) generalized many GRL
models into Message Passing and Auto-Encoder frameworks.
2
Published as a conference paper at ICLR 2021
These frameworks prompt bundling of GRL methods under Software Libraries, like PyG (Fey &
Lenssen, 2019) and DGL (Wang et al., 2019), offering consistent interfaces on data formats.
We now position our contribution relative to the above. Unlike generalized message passing (Gilmer
et al., 2017), rather than abstracting the model computation, we abstract the learning algorithm. As a
result, GTTF can be specialized to recover the learning of MP as well as NE methods. Morever, unlike
Software Frameworks, which are re-implementations of many algorithms and therefore inherit the
scale and learning of the copied algorithms, we re-write the algorithms themselves, giving them new
properties (memory and computation complexity), while maintaining (in expectation) the original
algorithm outcomes. Further, while the listed Stochastic Sampling Methods target MP algorithms
(such as GCN, GAT, alike), as their initial construction could not scale to large graphs, our learning
algorithm applies to a wider class of GRL methods, additionally encapsulating NE methods. Finally,
while some NE methods such as node2vec (Grover & Leskovec, 2016) and DeepWalk (Perozzi et al.,
2014) are scalable in their original form, their scalability stems from their multi-step process: sample
many (short) random walks, save them to desk, and then learn node embeddings using positional
embedding methods (e.g., word2vec, Mikolov et al. (2013)) - they are sub-optimal in the sense that
their first step (walk sampling) takes considerable time (before training even starts) and also places an
artificial limit on the number of training samples (number of simulated walks), whereas our algorithm
conducts walks on-the-fly whilst training.
3	Graph Traversal via Tensor Functionals (GTTF)
At its core, GTTF is a stochastic algorithm that recursively conducts graph traversals to build
representations of the graph. We describe the data structure and traversal algorithm below, using the
following notation. G = (V, E) is an unweighted graph with n = |V | nodes and m = |E| edges,
described as a sparse adjacency matrix A ∈ {0, 1}n×n. Without loss of generality, let the nodes be
zero-based numbered i.e. V = {0, . . . , n - 1}. We denote the out-degree vector δ ∈ Zn - it can be
calculated by summing over rows of A as δu = Pv∈V A[u, v]. We assume δu > 0 for all u ∈ V :
pre-processing can add self-connections to orphan nodes. B denotes a batch of nodes.
3.1	Data Structure
Internally, GTTF relies on a reformulation of the adjacency matrix, which we term CompactAdj (for
"Compact Adjacency", Figure 1c). It consists of two tensors:
1.	δ ∈ Zn, a dense out-degree vector (figure 1c, right)
2.	Ab ∈ Zn×n, a sparse edge-list matrix in which the row u contains left-aligned δu non-zero
values. The consecutive entries {A[u, 0], A[u, 1], . . . , A[u, δu - 1]} contain IDs of nodes
receiving an edge from node u. The remaining |V | - δu are left unset, therefore, Ab only
occupies O(m) memory when stored as a sparse matrix (Figure 1c, left).
CompactAdj allows us to concisely describe stochastic traversals using standard tensor operations. To
uniformly sample a neighbor to node U ∈ V, one can draw r 〜U[0..(δu - 1)], then get the neighbor
ID with A[u, r]. In vectorized form, given node batch B and access to continuous U[0, 1), we sample
neighbors for each node in B as: R 〜U[0,1)b, where b = |B|, then B0 = A[B, [R ◦ δ[B]C] is a
b-sized vector, with Bu0 containing a neighbor of Bu, floor operation b.c is applied element-wise, and
◦ is Hadamard product.
3.2	Stochastic Traversal Functional Algorithm
Our traversal algorithm starts from a batch of nodes. It expands from each into a tree, resulting in
a walk forest rooted at the nodes in the batch, as depicted in Figure 1d. In particular, given a node
batch B, the algorithm instantiates |B| seed walkers, placing one at every node in B. Iteratively, each
walker first replicates itself a fanout (f) number of times. Each replica then samples and transitions to
a neighbor. This process repeats a depth (h) number of times. Therefore, each seed walker becomes
the ancestor ofa f -ary tree with height h. Setting f = 1 recovers traditional random walk. In practice,
we provide flexibility by allowing a custom fanout value per depth.
3
Published as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Algorithm 1: Stochastic Traverse Functional, parametrized by AC CUMULATEFN and BI A SFN.
input: U (current node); T J [] (path leading to u, starts empty); F (list of fanouts);
ACCUMULATEFN (function: with side-effects and no return. It is model-specific and
records information for computing model and/or objective, see text);
BIASFN J U (function mapping u to distribution on u’s neighbors, defaults to uniform)
def Traverse(T, u, F, AC C UMULATEFN, BI A SFN):
if F .size() = 0 then return # Base case. Traversed up-to requested depth
f J F .pop() # fanout duplication factor (i.e. breadth) at this depth.
sample_bias J BI ASFN (T, u)
if sample_bias.sum() = 0 then return # Special case. No sampling from zero mass
sample_bias J sample_bias / sample_bias.sum() # valid distribution
K J Sample (A [u,:δu ]], Sample_bias, f) # Sample f nodes from U S neighbors
for k J 0 to f - 1 do
Tnext J concatenate(T, [U])
AC CUMU LAT EFN(Tnext, K [k], f)
Traverse(Tnext, K [k], f, AC C UMULAT EFN, B I A SFN) # Recursion
def Sample(N, W, f):
C J tf.cumsum(W) # Cumulative sum. Last entry must = 1.
coin_flips J tf.random.uniform((f, ), 0, 1)
indices J tf.searchsorted(C, coin_flips)
return N [indices]
Functional Traverse is listed in Algorithm 1. It accepts: a batch of nodes2; a list of fanout values
F (e.g. to F = [3, 5] samples 3 neighbors per U ∈ B, then 5 neighbors for each of those); and
more notably, two functions: AC CUMULATEFN and BI A SFN. These functions will be called by the
functional on every node visited along the traversal, and will be passed relevant information (e.g. the
path taken from root seed node). Custom settings of these functions allow recovering wide classes of
graph learning methods. At a high-level, our functional can be used in the following manner:
1. Construct model & initialize parameters (e.g. to random). Define ACCUMULATEFN and BI A SFN.
2. Repeat (many rounds):
i.	Reset accumulation information (from previous round) and then sample batch B ⊂ V .
ii.	Invoke Traverse on (B, ACCUMULATEFN, BI A SFN), which invokes the FN’s, allowing the first
to accumulate information sufficient for running the model and estimating an objective.
iii.	Use accumulated information to: run model, estimate objective, apply learning rule (e.g. SGD).
ACCUMULATEFN is a function that is used to track necessary information for computing the model
and the objective function. For instance, an implementation of DeepWalk (Perozzi et al., 2014) on top
of GTTF, specializes ACCUMULATEFN to measure an estimate of the sampled softmax likelihood of
nodes’ positional distribution, modeled as a dot-prodct of node embeddings. On the other hand, GCN
(Kipf & Welling, 2017) on top of GTTF uses it to accumulate a sampled adjacency matrix, which it
passes to the underlying model (e.g. 2-layer GCN) as if this were the full adjacency matrix.
BIASFN is a function that customizes the sampling procedure for the stochastic transitions. If
provided, it must yield a probability distribution over nodes, given the current node and the path that
lead to it. If not provided, it defaults to U , transitioning to any neighbor with equal probability. It can
be defined to read edge weights, if they denote importance, or more intricately, used to parameterize
a second order Markov Chain (Grover & Leskovec, 2016), or use neighborhood attention to guide
sampling (VelickoVic et al., 2018), as discussed in the Appendix.
2Our pseudo-code displays the traversal starting from one node rather than a batch only for clarity, as our
actual implementation is Vectorized e.g. u would be a Vector of nodes, T would be a 2D matrix with each row
containing transition path preceeding the corresponding entry in u, ... etc. Refer to Appendix and code.
4
Published as a conference paper at ICLR 2021
3.3 Some Specializations of ACCUMULATEFN & BIASFN
3.3.1	Message Passing: Graph Convolutional variants
These methods, including (Kipf & Welling, 2017; Hamilton et al., 2017; Wu et al., 2019; Abu-El-
Haija et al., 2019; Xu et al., 2018) can be approximated by by initializing A to an empty sparse n × n
matrix, then invoking Traverse (Algorithm 1) with u = B; F to list of fanouts with size h; Thus
ACCUMULATEFN and BIASFN become:
def ROOTEDAD JACC(T, u, f): A[u, T-ι] J 1;	(1)
~1δ
def NOREVI SITBIAS(T, u): return 1[A[u].sum() = 0]--;	(2)
δu
where ~1n is an n-dimensional all-ones vector, and negative indexing T-k is the kth last entry of
T. If a node has been visited through the stochastic traversal, then it already has fanout number
of neighbors and NOREVI S I TBI AS ensures it does not get revisited for efficiency, per line 5 of
Algorithm 1. Afterwards, the accumulated stochastic Ae will be fed3 into the underlying model e.g.
for a 2-layer GCN of Kipf & Welling (2017):
◦◦
GCN(A,X;W1,W2) = softmax(AReLu(AXW1)W2);	renorm trick (3)
^^^^} l^^^^-^^^^^{
withA= D01/2De0-1Ae0D0-1/2; D0 = diag(δ0); δ0 = ~1n>A0; Ae0 = In×n +Ae
Lastly, h should be set to the receptive field required by the model for obtaining output dL-dimensional
features at the labeled node batch. In particular, to the number of GC layers multiplied by the number
of hops each layers access. E.g. hops=1 for GCN but customizable for MixHop and SimpleGCN.
3.3.2	Node Embeddings
Given a batch of nodes B ⊆ V , DeepWalk4 can be implemented in GTTF by first initializing loss L
to the contrastive term estimating the partition function of log-softmax:
L J X log E	[exp(hZu,Zvi)] ,	(4)
U∈B	v~ Pn(V )
where h., .i is dot-product notation, Z ∈ Rn×d is the trainable embedding matrix with Zi ∈ Rd is
d-dimensional embedding for node u ∈ V . In our experiments, we estimate the expectation by taking
3
5 samples and We set the negative distribution Pn(V = V) 8 δVV, following Mikolov et al. (2013).
The functional is invoked with no BI ASFN and AC CUMULATEFN
def DEEPWALKAC C(T, u, f):
LJL-
% X %-k](5)ZT-)
;	[u] J
η[T-ι]
~~T;
(5)
where hyperparameter C indicates maximum window size (inherited from word2vec, Mikolov et al.,
2013), in the summation on k does not access invalid entries of T as CT , min(C, T.size), the scalar
fraction (C-C+1) is inherited from context sampling of word2vec (Section 3.1 in Levy et al., 2015),
and rederived for graph context by Abu-El-Haija et al. (2018), and η[u] stores a scalar per node on
the traversal Walk Forest, which defaults to 1 for non-initialized entries, and is used as a correction
term. DeepWalk conducts random walks (visualized as a straight line) whereas our walk tree has a
branching factor of f. Setting fanout f = 1 recovers DeepWalk’s simulation, though we found f > 1
outperforms within fewer iterations e.g. f = 5, within 1 epoch, outperforms DeepWalk’s published
implementation. Learning can be performed using the accumulated L as: Z J Z 一 KZL;
4	Theoretical Analysis
Due to space limitations, we include the full proofs of all propositions in the appendix.
3Before feeding the batch to model, in practice, we find nodes not reached by traversal and remove their
corresponding rows (and also columns) from X (and A).
4We present more methods in the Appendix.
5
Published as a conference paper at ICLR 2021
4.1	ESTIMATING kTH POWER OF TRANSITION MATRIX
We show that it is possible with GTTF to accumulate an estimate of transition T matrix to power k.
Let Ω denote the walk forest generated by GTTF, Ω(u, k, i) as the ith node in the vector of nodes at
depth k ofthe walk tree rooted at U ∈ B, and tU,v,k as the indicator random variable 1[Ω(u, k, i) = v].
Let the estimate of the kth power of the transition matrix be denoted Tbk. Entry Tbuk,v should be an
unbiased estimate of Tuk,v for u ∈ B, with controllable variance. We define:
(6)
The fraction in Equation 6 counts the number of times the walker starting at U visits V in Ω, divided
by the total number of nodes visited at the kth step from u.
Proposition 1. (UNBIASEDTK) Tbuk,v as defined in Equation 6, is an unbiased estimator of Tuk,v
k1
Proposition 2. (VARIANCETK) Variance ofour estimate is upper-bounded: Var[TU,v] ≤ ɪʃ^
Naive computation of kth powers of the transition matrix can be efficiently computed via repeated
sparse matrix-vector multiplication. Specifically, each column of Tk can be computed in O(mk),
where m is the number of edges in the graph. Thus, computing Tk in its entirety can be accomplished
in O(nmk). However, this can still become prohibitively expensive if the graph grows beyond a
certain size. GTTF on the other hand can estimate Tk in time complexity independent of the size of
the graph, (Prop. 8), with low variance. Transition matrix powers are useful for many GRL methods.
(Qiu et al., 2018)
4.2	Unbiased Learning
As a consequence of Propositions 1 and 2, GTTF enables unbiased learning with variance control for
classes of node embedding methods, and provides a convergence guarantee for graph convolution
models under certain simplifying assumptions.
We start by analyzing node embedding methods. Specifically, we cover two general types. The first is
based on matrix factorization of the power-series of transition matrix. and the second is based on
cross-entropy objectives, e.g., like DeepWalk (Perozzi et al., 2014), node2vec (Grover & Leskovec,
2016), These two are shown in Proposations 3 and 4
Proposition 3. (UNBIASEDTFACTORIZATION) Suppose L = 2 ||LR - Pk CkTk ∣∣F, i.e. factoriza-
tion objective that can be optimized by gradient descent by calculating V l,r L, where cjs are scalar
coefficients. Let its estimate L = 2 ||LR - Pk CkTk ||F, where T is obtained by GTTF according to
Equation 6. Then E[VL,RLb] = VL,RL.
Proposition 4. (UNB IASEDLEARNNE) Learning node embeddings Z ∈ Rn×d with objective
function L, decomposable as L(Z) = Pu∈V L1(Z, U) - Pu,v∈V Pk L2(Tk, U, v)L3(Z, U, v),
where L2 is linear over Tk, then using Tb k yields an unbiased estimate of VZL.
Generally, L1 (and L3) score the similarity between disconnected (and connected) nodes U and
v . The above form of L covers a family of contrastive learning objectives that use cross-entropy
loss and assume a logistic or (sampled-)softmax distributions. We provide, in the Appendix, the
decompositions for the objectives of DeepWalk (Perozzi et al., 2014), node2vec (Grover & Leskovec,
2016) and WYS (Abu-El-Haija et al., 2018).
Proposition 5. (UNB IASEDMP) Given input activations, H (l-1), graph conv layer (l) can use
rooted adjacency A accumulated by ROOTEDAD JACC (1), to provide unbiased pre-activation output,
i.e. E [AkH(IT)W(I)i =(D0T/24D0T/2) H(IT)W(I), With A0 and D0 defined in (3).
Proposition 6. (UNBIASEDLEARNMP) If objective to a graph convolution model is convex and
Lipschitz continous, with minimizer θ*, then utilizing GTTFfor graph convolution converges to θ*.
6
Published as a conference paper at ICLR 2021
4.3	Complexity Analysis
Proposition 7. S TORAGE complexity of GTTF is O(m + n).
Proposition 8. TIME complexity of GTTF is O(bf h) for batch size b, fanout f, and depth h.
Proposition 8 implies the speed of computation is irrespective of graph size. Methods implemented
in GTTF inherit this advantage. For instance, the node embedding algorithm WYS (Abu-El-Haija
et al., 2018) is O(n3), however, we apply its GTTF implementation on large graphs.
5	Experiments
We conduct experiments on 10 different graph datasets, listed in in Table 1. We experimentally
demonstrate the following. (1) Re-implementing baseline method using GTTF maintains performance.
(2) Previously-unscalable methods, can be made scalable when implemented in GTTF. (3) GTTF
achieves good empirical performance when compared to other sampling-based approaches hand-
designed for Message Passing. (4) GTTF consumes less memory and trains faster than other popular
Software Frameworks for GRL. To replicate our experimental results, for each cell of the table in our
code repository, we provide one shell script to produce the metric, except when we indicate that the
metric is copied from another paper. Unless otherwise stated, we used fanout factor of 3 for GTTF
implementations. Learning rates and model hyperparameters are included in the Appendix.
5.1	Node Embeddings for Link Prediction
In link prediction tasks, a graph is partially obstructed by hiding a portion of its edges. The task
is to recover the hidden edges. We follow a popular approach to tackle this task: first learn node
embedding Z ∈ Rn×d from the observed graph, then predict the link between nodes u and v with
score H Z>Zv. We use two ranking metrics for evaluations: ROC-AUC, which is a ranking objective:
how well do methods rank the hidden edges above randomly-sampled negative edges and Mean Rank.
We re-implement Node Embedding methods, DeepWalk (Perozzi et al., 2014) and WYS (Abu-El-
Haija et al., 2018), into GTTF (abbreviated F). Table 2 summarizes link prediction test performance.
LiveJournal and Reddit are large datasets, where original implementation of WYS is unable to scale
to. However, scalable F (WYS) sets new state-of-the-art on these datasets. For PPI and HepTh
datasets, we copy accuracy numbers for DeepWalk and WYS from (Abu-El-Haija et al., 2018). For
LiveJournal, we copy accuracy numbers for DeepWalk and PBG from (Lerer et al., 2019) - note
that a well-engineered approach (PBG, (Lerer et al., 2019)), using a mapreduce-like framework, is
under-performing compared to F (WYS), which is a few lines specialization of GTTF.
5.2	Message Passing for Node Classification
We implement in GTTF the message passing models: GCN (Kipf & Welling, 2017), GraphSAGE
(Hamilton et al., 2017), MixHop (Abu-El-Haija et al., 2019), SimpleGCN (Wu et al., 2019), as their
computation is straight-forward. For GAT (VeIiCkOViC et al., 2018) and GCNn (Chen et al., 2020), as
they are more intricate, we download the authors’ codes, and wrap them as-is with our functional.
We show that we are able to run these models in Table 3 (left and middle), and that GTTF implemen-
tations matches the baselines performance. For the left table, we copy numbers from the published
papers. However, we update GAT to work with TensorFlow 2.0 and we use our updated code (GAT*).
5.3	Experiments comparing against Sampling methods for Message Passing
We now compare models trained with GTTF (where samples are walk forests) against sampling
methods that are especially designed for Message Passing algorithms (GraphSAINT and ClusterGCN),
especially since their sampling strategies do not match ours.
Table 3 (right) shows test performance on node classification accuracy on a large dataset: Products.
We calculate the accuracy for F(SAGE), but copy from (Hu et al., 2020) the accuracy for the baselines:
GraphSAINT (Zeng et al., 2020) and ClusterGCN (Chiang et al., 2019) (both are message passing
methods); and also node2vec (Grover & Leskovec, 2016) (node embedding method).
7
Published as a conference paper at ICLR 2021
Table 1: Dataset summary. Tasks are LP, SSC, FSC, for link prediction, semi- and fully-supervised
classification. Split indicates the train/validate/test paritioning, with (a) = (Abu-El-Haija et al., 2018),
(b) = to be released, (c) = (Hamilton et al., 2017), (d) = (Yang et al., 2016); (e) = (Hu et al., 2020).
Dataset	Split Il #Nodes		# Edges	# Classes	Nodes	Edges	Tasks
PPI	(a)	3,852	20,881	N/A	proteins	interaction	LP
ca-HepTh	(a)	80,638	24,827	N/A	researchers	co-authorship	LP
ca-AstroPh	(a)	17,903	197,031	N/A	researchers	co-authorship	LP
LiveJournal	(b)	4.85M	68.99M	N/A	users	friendship	LP
Reddit	(c)	233,965	11.60M	41	posts	user co-comment	LP/FSC
Amazon	(b)	2.6M	48.33M	31	products	co-purchased	FSC
Cora	(d)	2,708	5,429	7	articles	citation	SSC
CiteSeer	(d)	3,327	4,732	6	articles	citation	SSC
PubMed	(d)	19,717	44,338	3	articles	citation	SSC
Products	(e)	2.45M	61.86M	47	products	co-purchased	SSC
Table 2: Results of node embeddings on Link Prediction. Left: Test ROC-AUC scores. Right: Mean
Rank on the right for consistency with Lerer et al. (2019). *OOM = Out of Memory.
	PPI	HepTh	Reddit		LiveJournal
DeepWalk	70.6	91.8	93.5	DeepWalk	-234.6
F (DeepWalk)	87.9	89.9	95.5	PBG	245.9
WYS	W	93.6	OOM	WYS	OOM*
F (WYS)	90.5	93.5	98.6	F (WYS)	-185.6
Table 3: Node classification tasks. Left: test accuracy scores on semi-supervised classification (SSC)
of citation networks. Middle: test micro-F1 scores for large fully-supervised classification. Right:
test accuracy on an SSC task, showing only scalable baselines. We bold the highest value per column.
	Cora	Citeseer	Pubmeb			
GCN	ɪr	70.3	79.0			
F(GCN)	81.9	69.8	79.4		Reddit	Amazon
MixHop	"8T9^	71.4	80.8	SAGE	95.0	88.3
F (MixHop)	83.1	71.8	80.9	F (SAGE)	95.9	88.5
GAT*	ɪr	72.4	77.7	SimPGCN	94.9	83.4
F (GAT)	83.3	72.5	77.8	F (SimpGCN)	94.8	83.8
GCNII	^855^	73.4	80.3			
F (GCNII)	85.3	74.4	80.2			
	Products
node2vec	^21
ClusterGCN	75.2
GraPhSAINT	77.3
-F (SAGE)	77.0
Table 4: Performance of GTTF against frameworks DGL and PyG. Left: Speed is the per epoch time
in seconds when training GraphSAGE. Memory is the memory in GB used when training GCN. All
experiments conducted using an AMD Ryzen 3 1200 Quad-Core CPU and an Nvidia GTX 1080Ti
GPU. Right: Training curve for GTTF and PyG implementations of Node2Vec.
	Speed (s)		Memory (GB)			
	Reddit	Products	Reddit	Cora	Citeseer	Pubmed
DGL	17.3	13.4	OOM	1.1	1.1	1.1
PyG	5.8	9.2	OOM	1.2	1.3	1.6
GTTF	1.8	1.4	2.44	0.32	0.40	0.43
U 0.8-
6 0.7-
B 0.6-
Time (s)
5.4	Runtime and Memory comparison against optimized S oftware Frameworks
In addition to the accuracy metrics discussed above, we also care about computational performance.
We compare against software frameworks DGL (Wang et al., 2019) and PyG (Fey & Lenssen,
2019). These software frameworks offer implementations of many methods. Table 4 summarizes
the following. First (left), we show time-per-epoch on large graphs of their implementation of
8
Published as a conference paper at ICLR 2021
GraphSAGE, compared with GTTF’s, where we make all hyper parameters to be the same (of model
architecture, and number of neighbors at message passing layers). Second (middle), we run their
GCN implementation on small datasets (Cora, Citeseer, Pubmed) to show peak memory usage. The
run times between GTTF, PyG and DGL are similar for these datasets. The comparison can be
found in the Appendix. While the aforementioned two comparisons are on popular message passing
methods, the third (right) chart shows a popular node embedding method: node2vec’s link prediction
test ROC-AUC in relation to its training runtime.
6	Conclusion
We present a new algorithm, Graph Traversal via Tensor Functionals (GTTF) that can be specialized to
re-implement the algorithms of various Graph Representation Learning methods. The specialization
takes little effort per method, making it straight-forward to port existing methods or introduce new
ones. Methods implemented in GTTF run efficiently as GTTF uses tensor operations to traverse
graphs. In addition, the traversal is stochastic and therefore automatically makes the implementations
scalable to large graphs. We theoretically show that the learning outcome due to the stochastic
traversal is in expectation equivalent to the baseline when the graph is observed at-once, for popular
GRL methods we analyze. Our thorough experimental evaluation confirms that methods implemented
in GTTF maintain their empirical performance, and can be trained faster and using less memory even
compared to software frameworks that have been thoroughly optimized.
7	Acknowledgements
We acknowledge support from the Defense Advanced Research Projects Agency (DARPA) under
award FA8750-17-C-0106.
References
Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. Watch your step:
Learning node embeddings via graph attention. In Advances in Neural Information Processing
Systems, 2018.
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina
Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional archi-
tectures via sparsified neighborhood mixing. In International Conference on Machine Learning,
2019.
Pierre Baldi and Peter Sadowski. The dropout learning algorithm. In Artificial Intelligence, 2014.
Leon Bottou. Online algorithms and stochastic approximations. In Online Learning and Neural
Networks, 1998.
Leon Bottou. Stochastic learning. In Advanced Lectures on Machine Learning, Lecture Notes in
Artificial Intelligence, vol. 3176, Springer Verlag, 2004.
Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ra and Kevin Murphy. Machine
learning on graphs: A model and comprehensive taxonomy, 2021.
Hongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas Blaschke. The rise of
deep learning in drug discovery. In Drug discovery today, 2018a.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. In International Conference on Learning Representation, 2018b.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In International Conference on Machine Learning, 2020.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, 2019.
9
Published as a conference paper at ICLR 2021
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.
William Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, 2017.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In arXiv,
2020.
Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017.
Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, and Alex
Peysakhovich. Pytorch-biggraph: A large-scale graph embedding system. In The Conference on
Systems and Machine Learning, 2019.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned
from word embeddings. In Transactions of the Association for Computational Linguistics, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems, 2013.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social represen-
tations. In ACM SIGKDD international conference on Knowledge discovery & Data Mining,
2014.
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding
as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In ACM International
Conference on Web Search and Data Mining, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. In Journal of Machine
Learning Research, 2014.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lid, and YoshUa
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Petar VeliCkoviC, William FedUS, William L. Hamilton, Pietro Lid, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019.
Minjie Wang, Da Zheng, Zihao Ye, QUan Gan, MUfei Li, Xiang Song, Jinjing ZhoU, Chao Ma,
Lingfan YU, YU Gai, TianjUn Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.
Deep graph library: A graph-CentriC, highly-performant paCkage for graph neUral networks. arXiv
preprint arXiv:1909.01315, 2019.
Felix WU, AmaUri SoUza, Tianyi Zhang, Christopher Fifty, Tao YU, and Kilian Weinberger. Simplify-
ing graph ConvolUtional networks. In International Conference on Machine Learning, 2019.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken iChi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. In International
Conference on Machine Learning, 2018.
Zhilin Yang, William W Cohen, and RUslan SalakhUtdinov. Revisiting semi-sUpervised learning with
graph embeddings. In International Conference on Machine Learning, 2016.
10
Published as a conference paper at ICLR 2021
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
SAINT: Graph sampling based inductive learning method. In International Conference on Learning
Representations, 2020.
Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Few-shot repre-
sentation learning for out-of-vocabulary words. In Advances in Neural Information Processing
Systems, 2019.
Appendix
A Hyperparameters
For the general link prediction tasks we used a |B | = |V |, C = 5, f = 3, 10 negative samples per
edge, Adam optimizer with a learning rate of 0.5, multiplied by a factor of 0.2, every 50 steps, for
200 total iterations. The differences are listed below.
The Reddit dataset was trained using a starting learning rate of 2.0, decaying 50% every 10 iterations.
The LiveJournal task was trained using a fixed learning rate of 0.001, |B| = 5000, f = 2, and 50
negative samples per edge.
For the node classifications tasks:
For F (SimpleGCN) on Amazon, we use f = [15, 15], a batch size of 1024, and a learning rate of
0.02, decaying by a factor 0f 0.2 after 2 and 6 epochs for a total of 25 epochs. On Reddit, it is the
same except f = [25, 25]
For F (SAGE) on Amazon we use f = [20, 10], a two layer model, a batch size of 256, and fixed
learning rates of 0.001 and 0.002 respectively. On reddit we use f = [25, 20], a fixed learning rate
of 0.001, hidden dimension of 256 and a batch size of 1024. On the Products dataset, we used
f = [15, 10], a fixed learning learning rate of 0.001 and a batch size of 1024, a hidden dimension of
256 and a fixed learning rate of 0.003.
For GAT (baseline), we follow the authors code and hyperparameters: for Cora and Citeseer, we
use Adam with learning rate of 0.005, L2 regularization of 0.0005, 8 attention heads on the first
layer and 1 attention head on the output layer. For Pubmed, we use Adam with learning rate of 0.01,
L2 regularization of 0.01, 8 attention heads on the first layer and 8 attention heads on the output
layer. For F (GAT), we use the same aforementioned hyperparameters, a fanout of 3 and traversal
depth of 2 (to cover two layers) i.e. F = [3, 3]. For F (GCN), we use the authors’ recommended
hyperparameters. Learning rate of 0.005, 0.001 L2 regularization, and F = [3, 3], for all datasets.
For both methods, we apply “patience” and stop the training if validation loss does not improve for
100 consecutive epochs, reporting the test accuracy at the best validation loss. For F (MixHop), we
wrap the authors’ script and use their hyperparameters. For F (GCNII), we use F = [5, 5, 5, 5, 5, 5],
as their models are deep (64 layers for Cora). Otherwise, we inherit their network hyperparameters
(latent dimensions, number of layers, dropout factor, and their introduced coefficients), as they have
tuned them per dataset, but we change the learning rate to 0.005 (half of what they use) and we extend
the patience from 100 to 1000, and extend the maximum number of epochs from 1500 to 5000 - this
is because we are presenting a subgraph at each epoch, and therefore we intuitively want to slow
down the learning per epoch, which is similar to the practice when someone applies Dropout to a
neural networks. We re-run their shell scripts, with their code modified to use the Rooted Adjacency
rather than the real adjacency, which is sampled at every epoch.
MLP was trained with 1 layer and a learning rate of 0.01.
11
Published as a conference paper at ICLR 2021
B Proofs
B.1 Proof of Proposition 1
fk	fk	fk
fPk tiu,v,k	P E[tiu,v,k]	PP[tiu,v,k= 1] P Tuk,v
Proof. E[Tkv] = E [i=1f^ I= i=Lfk— = i=1—fk——=i=fl- = Tkv	□
B.2 Proof of Proposition 2
Proo/ Var⅛= P=I fL = ITkv) =
Since 0 ≤ Tuk,v ≤ 1, then Tuk,v(1 - Tuk,v) is maximized with Tuk,v
2. HenceVar[Tkv] ≤ 4fk
B.3 Proof of Proposition 3
Proof. Consider a d-dimensional factorization of Pk ck Tk , where ck ’s are scalar coefficients:
2
L =2 LR - X Ck Tk
k F
(7)
parametrized by L, R> ∈ Rn×d. The gradients of L w.r.t. parameters are:
VLL = LLRv - XCkTk) R> and VRL = L> (lR> - XCkTk
(8)
~	....	Z，Z	1 .	...	ik	..	一二≥∖
Given estimate objective L (replacing T with using GTTF-estimated T):
2
LR -	CkTbk
k
(9)
It follows that:
E VLLb = E	LR> - XCkTbk R>
E	LR>-XkCkTbk
LR> - XCkE hTbki
Scaling property of expectation
Linearity of expectation
LR> -	CkTk R>
VLL
Proof of Proposition 1
□
1
2
F
The above steps can similarly be used to show E VRLb = VRL
□
B.4	Proof of Proposition 4
Proof. We want to show that E[VZ L(Tb k, Z)] = VZL(Tk, Z). Since the terms of L1 are unaf-
fected by T, they are excluded w.l.g. from L in the proof.
E[VZL(Tbk,Z)] =EVZ(-	L2(Tbk, u, v)L3(Z, u, v))
u,v∈V k∈{1..C}
12
Published as a conference paper at ICLR 2021
(by linearity of expectation) = -Vz Σ Σ	L2(E[Tbk], u, v)L3(Z, u, v)
u,v∈V k∈{1..C}
(by Prop 1) = -VZ X X	L2(Tk,u,v)L3(Z,u,v) =VZL(Tk,Z)
u,v∈V k∈{1..C}
□
The following table gives the decomposition for DeepWalk, node2vec, and Watch Your Step.
Node2vec also introduces a biased sampling procedure based on hyperparameters (they name p
and q) instead of uniform transition probabilities. We can equivalently bias the transitions in GTTF to
match node2vec’s. This would then show up as a change in Tb k in the objective. This effect can also
be included in the objective by multiplying hZu, Zvi by the probability of such a transition in L3. In
this format, the p and q variables appear in the objective and can be included in the optimization. For
WYS, Qk are also trainable parameters.
Method	Li	L2	L3
DeepWalk	0	^p≡C+ψT	log(hZu, Zv i)
Node2Vec	log(Pv∈v exp"u,Zv i))	(一) Tk	hZu, Zv i
Watch Your Step	log(1 - σ(hLu, Rvi))	QkTk	log(σ (hLu, Rv i))
Table 5: Decomposition of graph embedding methods to demonstrate unbiased learning. For WYS,
Zu = concatenate(Lu , Ru).
For methods in which the transition distribution is not uniform, such as node2vec, there are two
options for incorporating this distribution in the loss. The obvious choice is to sample from a biased
transition matrix, Tu,v = Wu,v, where W is the transition weights. Alternatively, the transition bias
can be used as a weight on the objective itself. This approach is still unbiased as
Ev~fu [L(v, u)] = Pv∈v Pv~fu [v]L(v, U) = Pv∈v Wu,vL(v, U)
B.5	Proof of Proposition 5
>Λ	CT, Tl ,1	∙ 1 1 1	1	, 1	,	1 1	l l ɔ	11 , ~ 1 ∙	, 1	1
Proof. Let A be the neighborhood patch returned by GTTF, and lete. indicate a measurement based on
the sampled graph, A, such as the degree vector, δ, or diagonal degree matrix, D. For the remainder of
this proof, let all notation for adjacency matrices, A or A, and diagonal degree matrices, D or D, and
degree vector, δ, refer to the corresponding measure on the graph with self loops e.g. A — A + In×n.
We now show that the expectation of the layer output is unbiased.
E AkH(IT)W(I)i = [e [Aki H(IT)W(I)i implies that E [AkH(IT)W(I)] is unbiased if
E Aki = (DT/2ADT/2)k.
E [Aki = E D1/2 (DTA)k DT/2 = D1/2E (DTA)k DT/2
T . /T>1f O,t>F .1	. i' 11	11 r	/	∖ I	_ T T-T	11 . —I	∙ 1∙	. .1 . .1
Let Pu,v,k be the set of all walks {p = (U, v1, ..., vk-1, v)|vi ∈ V }, and let p∃A indicate that the
path p exists in the graph given by Ae. Let tu,v,k be the transition probability from U to v in k steps,
and let tp be the probability of a random walker traversing the graph along path p.
E [Tkvi = PrF，v，k = 1] = X	Pr[p∃A]Pr[tp = 1∣p∃A]
p∈P u,v,k
13
Published as a conference paper at ICLR 2021
k	kk
= X Y 1[A[pi, pi+1] =i]fδ+1 Y(f+i)-1= x Y1[A[pi, pi+1] = 1]δ[pi]-1
p∈Pu,v,k i=1	pi i=1	p∈P u,v,k i=1
= X Pr[tp = 1]= Pr[tu,v,k ] = (T )U,V = (DTA)U,v
p∈P u,v,k
Thus, E [A] = (DT/2ADT/2)k and E AH(IT)W(I)] = (DT/2ADT/2)k H(I-I)W(I)
□
For writing, we assumed nodes have degree, δu ≥ f , though the proof still holds if that is not the case
as the probability of an outgoing edge being present from u becomes 1 and the transition probability
becomes δu-1 i.e. the same as no estimate at all.
B.6	Proof of Proposition 6
GTTF can be seen as a way of applying dropout (Srivastava et al., 2014), and our proof is contingent
on the convergence of dropout, which is shown in Baldi & Sadowski (2014). Our dropout is on the
adjacency, rather than the features. Denote the output of a graph convolution network5 with H:
H= GCNX(A;W) =TXW
We restrict our analysis to GCNs with linear activations. We are interested in quantifying the change
of H as A changes, and therefore the fixed (always visible) features X is placed on the subscript. Let
A denote adjacency accumulated by GTTF’s ROOTEDAD JAC C (Eq. 1).
Hec = GCNX (Aec).
|A|
Let A = {Ac}c=1 denote the (countable) set of all adjacency matrices realizable by GTTF. For the
analysis, assume the graph is α-regular: the assumption eases the notation though it is not needed.
Therefore, degree δu = α for all U ∈ V. Our analysis depends6 on 告 ∑2χ∈∕ A 8 A. i.e. the
average realizable matrix by GTTF is proportional (entry-wise) to the full adjacency. This is can be
shown when considering one-row at a time: given node u with δu = α outgoing neighbors, each of
its neighbors has the same appearance probability =十.Summing over all combinations (δu), makes
each edge appear the same frequency =看 |A|, noting that |A| evenly divides (δu) for all U ∈ V.
We define a dropout module:
|A| of them
d 口 ~	.	. z^—r~Tʌ
A = ʌ, ZcAc with Z 〜CategoriCal I ∣A^, ∣A^,..., ∣A^ ),
(10)
where zc acts as Multinoulli selector over the elements of A, with one of its entries set to 1 and
all others to zero. With this definitions, GCNs can be seen in the droupout framework as: H =
d
GCNX (A). Nonetheless, in order to inherit the analysis of (Baldi & Sadowski, 2014, see their
equations 140 & 141), we need to satisfy two conditions which their analysis is founded upon:
d
(i)	E[GCNX (A)] = GCNX (A): in the usual (feature-wise) dropout, such condition is easily
verified.
(ii)	Backpropagated error signal does not vary too much around around the mean, across all
d
realizations of A.
Condition (i) is satisfied due to proof of Proposition 5. To analyze the error signal, i.e. the gradient of
the error w.r.t. the network, assume loss function L(H), outputs scalar loss, is λ-Lipschitz continuous.
5The following definition averages the node features (uses non-symmetric normalization) and appears in
multiple GCN’s including Hamilton et al. (2017).
6If not α-regular, it would be 战 Pn∈a A (X DT A
14
Published as a conference paper at ICLR 2021
The Liptchitz continuity allows us to bound the difference in error signal between L(H) and L(H):
||VhL(H) - VhL(H)∣∣2≤λ (VhL(H) - NHL(H))>(H - H)	(11)
(b)
≤λ ||VhL(H) - VHL(H)∣∣2 ||H - Hg	(12)
w.p.≥1- Q12	~	丁丁 ,---------
≤ λ ||VhL(H) - VHL(H)∣∣2 W>X>Q√Var[T]XW (13)
=器 ||VhL(H) - VHL(H)∣∣2 ||W||2 ||X||2	(14)
2f
IIVhL(H) - VHL(H)∣∣2 ≤ 器 ||W||2 ∣∣X∣∣2	(15)
2f
where (a) is by LiPschitz continuity, (b) is by CaUchy-SchWarz inequality, “w.p.” means with
probability and uses Chebyshev’s inequality, with the following equality because the variance of
T is shown element-wise in proof for Prop. 2. Finally, we get the last line by dividing both
sides over the common term. This shows that one can make the error signal for the different
realizations arbitrarily small, for example, by choosing a larger fanout value or putting (convex)
norm constraints on W and X e.g. through batchnorm and/or weightnorm. Since we can have
VHL(H) ≈ VHL(HI) ≈ VHL(H2) ≈ ∙∙∙ ≈ VHL(H∣a∣) with high probability, then the
analysis of Baldi & Sadowski (2014) applies. Effectively, it can be thought of as an online learning
algorithm where the elements of A are the stochastic training examples and analyzed per (Bottou,
1998; 2004), as explained by Baldi & Sadowski (2014) .
B.7	Proof of Proposition 7
The storage complexity of CompactAdj is O(sizeof (δ) + sizeof (A)) = O(n + m).
Moreover, for extemely large graphs, the adjacncy can be row-wise partitioned across multiple
machines and therefore admitting linear scaling. However, we acknolwedge that choosing which
rows to partition to which machines can drastically affect the performance. Balanced partitioning is
ideal. It is an NP-hard problem, but many approximations have been proposed. Nonetheless, reducing
inter-communication, when distributing the data structure across machines, is outside our scope.
B.8	Proof of Proposition 8
For each step of GTTF, the computational complexity is O(bhf). This follows trivially from the
GTTF functional: each nodes in batch (b of them) builds a tree with depth h and fanout f i.e. with
hf tree nodes. This calculation assumes random number generation, ACCUMULATEFN and BIASFN
take constant time. The searchsorted function is linear, as it is called on a sorted list: cumulative
sum of probabilities.
C Additional GTTF Implementations
C.1 Message Passing Implementations
C.1.1 Graph Attention Networks (GAT, VELICKOVIC ET al., 2018)
One can implement GAT by following the previous subsection, utilizing AC CUMULATEFN and
BIASFN defined in (1) and (2), but just replacing the model (3) by GAT’s:
◦◦
GAT(A, X; A, Wι, W2) = softmax((A ◦ A) ReLu((A ◦ A)XWI)W2);	(16)
where ◦ is hadamard product and A is an n × n matrix placing a positive scalar (an attention value)
on each edge, parametrized by multi-headed attention described in (Velickovic et al., 2018). However,
for some high-degree nodes that put most of the attention weight on a small subset of their neighbors,
sampling uniformly (with BI A SFN=NORE V I S I TBIAS) might mostly sample neighbors with entries
in A with value ≈ 0, and could require more epochs for convergence. However, our flexible functional
15
Published as a conference paper at ICLR 2021
allows us to propose a sample-efficient alternative, that is in expectation, equivalent to the above:
ʌX	◦  O	◦  O
GAT(A,X; A,W1,W2) = Softmax((TA。A) ReLu((TA。A)XWI)W2);
def GATBIAS (T, u): return
NoRevi s i tBias
(T, U)。Ja[u,A[u]];
(17)
(18)
C.1.2 DEEP Graph INFOMAX (DGL VELICKOVIC ET al., 2019)
DGI implementation on GTTF can use AC CUMULATEFN=ROO TEDAD JAC C, defined in (1). To
create the positive graph: it can sample some nodes B ⊂ V . It would pass to GTTF’s Traverse B,
and utilize the accumulated adjacency A for running: GCN(A, XB ) and GCN(A, Xpermute), where
the second run randomly permutes the order of nodes in X . Finally, the output of those GCNs can
then be fed into a readout function which outputs to a descriminator trying to classify if the readout
latent vector correspond to the real, or the permuted features.
C.2 Node Embedding Implementations
C.2. 1 Node2Vec (Grover & Leskovec, 2016)
A simple implementation follows from above: N2VACC , DEEPWALKACC; but override BIASFN =
def N2VBI AS(T, u): return p-1[i=T-2]q-1[hA[T-2],A[u]i>0] ;	(19)
where 1 denotes indicator function, p, q > 0 are hyperparameters of node2vec assigning (un-
normalized) probabilities for transitioning back to the previous node or to node connected to it.
hA[T-2], A[u]i counts mutual neighbors between considered node u and previous T-2.
An alternative implementation is to not override BIASFN but rather fold it into ACCUMULATEFN,
as:
def N2vAcc(T, u, f): DEEPWALKACC (T, u, f);曲川—n” X N2vBias(T,u);	(20)
Both alternatives are equivalent in expectation. However, the latter directly exposes the parameters
p and q to the objective L: allowing them to be differentiable w.r.t. L and therefore trainable via
gradient descent, rather than by grid-search. Nonetheless, parameterizing p & q is beyond our scope.
C.2.2 WATCH YOUR STEP (WYS, ABU-EL-HAIJA ET AL., 2018)
First, embedding dictionaries R,L ∈ Rn× d can be initialized to random. Then repeatedly over
batches B ⊆ V , the loss L can be initialized to estimate the negative part of the objective:
L^---ɪ2 log σ(-Ev∈U(V) KRu,Lvi + hRv, Lui]),
u∈B
Then call GTTF’s traverse passing the following AC CUMULATEFN=
def WY SAC C(T, u):
if T.size() 6= Q.size(): return;
t — T[0];	U — T[1:] ∪ [u];
ctx_weighted_L J ɪ2 QjLUj; ctx_weighted_R J ɪ2 QjRUj;
jj
L J L - log(σ(hRt, ctx_weighted_Li + hLt, ctx_weighted_Ri));
D Miscellaneous
D.1 Sensitivity
The following figures show the sensitivity of fanout and walk depth for WYS on the Reddit dataset.
16
Published as a conference paper at ICLR 2021
0.87
2	4 β 8
Fanout
0.93
,0.92
,0.91
β 0.90
5 0.89
0.88
0.87
fanout = 1
fanout = 2
Figure 3: Test AUC score when changing the fanout (left) and random walk length (right)
Time (s)
Figure 4: Runtime of GTTF versus PyG for training GCN on citation network datasets. Each line
averages 10 runs on an Nvidia GTX 1080Ti GPU.
D.2 Runtime of GCN on citation networks
Citation networks (Cora, Citeseer, Pubmed) have been popularized by Kipf & Welling (2017), and
for completeness, we report in Figure 4 the runtime versus test accuracy of GCN on these networks.
We compare against PyG, which is optimized for GCN. We find that the methods are somewhat
comparable in terms of training time.
17