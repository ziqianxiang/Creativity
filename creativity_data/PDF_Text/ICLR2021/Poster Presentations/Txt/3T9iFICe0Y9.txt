Published as a conference paper at ICLR 2021
The Recurrent Neural Tangent Kernel
Sina Alemohammad, Zichao Wang, Randall Balestriero, Richard G. Baraniuk
Department of Electrical and Computer Engineering
Rice University
{sa86,zw16,rb42,richb}@rice.edu
Ab stract
The study of deep neural networks (DNNs) in the infinite-width limit, via the
so-called neural tangent kernel (NTK) approach, has provided new insights into the
dynamics of learning, generalization, and the impact of initialization. One key DNN
architecture remains to be kernelized, namely, the recurrent neural network (RNN).
In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK),
which provides new insights into the behavior of overparametrized RNNs. A key
property of the RNTK should greatly benefit practitioners is its ability to compare
inputs of different length. To this end, we characterize how the RNTK weights
different time steps to form its output under different initialization parameters and
nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate
that the RNTK offers significant performance gains over other kernels, including
standard NTKs, across a wide array of data sets.
1	Introduction
The overparameterization of modern deep neural networks (DNNs) has resulted in not only remarkably
good generalization performance on unseen data (Novak et al., 2018; Neyshabur et al., 2019; Belkin
et al., 2019) but also guarantees that gradient descent learning can find the global minimum of
their highly nonconvex loss functions (Du et al., 2019b; Allen-Zhu et al., 2019b;a; Zou et al., 2018;
Arora et al., 2019b). From these successes, a natural question arises: What happens when we take
overparameterization to the limit by allowing the width of a DNN’s hidden layers to go to infinity?
Surprisingly, the analysis of such an (impractical) DNN becomes analytically tractable. Indeed,
recent work has shown that the training dynamics of (infinite-width) DNNs under gradient flow is
captured by a constant kernel called the Neural Tangent Kernel (NTK) that evolves according to a
linear ordinary differential equation (ODE) (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019a).
Every DNN architecture and parameter initialization produces a distinct NTK. The original NTK was
derived from the Multilayer Perceptron (MLP)(Jacot et al., 2018) and was soon followed by kernels
derived from Convolutional Neural Networks (CNTK) (Arora et al., 2019a; Yang, 2019a), Residual
DNNs (Huang et al., 2020), and Graph Convolutional Neural Networks (GNTK) (Du et al., 2019a).
In (Yang, 2020a), a general strategy to obtain the NTK of any architecture is provided.
In this paper, we extend the NTK concept to the important class of overparametrized Recurrent
Neural Networks (RNNs), a fundamental DNN architecture for processing sequential data. We show
that RNN in its infinite-width limit converges to a kernel that we dub the Recurrent Neural Tangent
Kernel (RNTK). The RNTK provides high performance for various machine learning tasks, and an
analysis of the properties of the kernel provides useful insights into the behavior of RNNs in the
following overparametrized regime. In particular, we derive and study the RNTK to answer the
following theoretical questions:
Q: Can the RNTK extract long-term dependencies between two data sequences? RNNs are known
to underperform at learning long-term dependencies due to the gradient vanishing or exploding
(Bengio et al., 1994). Attempted ameliorations have included orthogonal weights (Arjovsky et al.,
2016; Jing et al., 2017; Henaff et al., 2016) and gating such as in Long Short-Term Memory (LSTM)
(Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) RNNs. We
demonstrate that the RNTK can detect long-term dependencies with proper initialization of the
hyperparameters, and moreover, we show how the dependencies are extracted through time via
different hyperparameter choices.
1
Published as a conference paper at ICLR 2021
Q: Do the recurrent weights of the RNTK reduce its representation power compared to other
NTKs? An attractive property of an RNN that is shared by the RNTK is that it can deal with
sequences of different lengths via weight sharing through time. This enables the reduction of the
number of learnable parameters and thus more stable training at the cost of reduced representation
power. We prove the surprising fact that employing tied vs. untied weights in an RNN does not
impact the analytical form of the RNTK.
Q: Does the RNTK generalize well? A recent study has revealed that the use of an SVM classifier
with the NTK, CNTK, and GNTK kernels outperforms other classical kernel-based classifiers and
trained finite DNNs on small data sets (typically fewer than 5000 training samples) (Lee et al., 2020;
Arora et al., 2019a; 2020; Du et al., 2019a). We extend these results to RNTKs to demonstrate that
the RNTK outperforms a variety of classic kernels, NTKs and finite RNNs for time series data sets in
both classification and regression tasks. Carefully designed experiments with data of varying lengths
demonstrate that the RNTK’s performance accelerates beyond other techniques as the difference in
lengths increases. Those results extend the empirical observations from (Arora et al., 2019a; 2020; Du
et al., 2019a; Lee et al., 2020) into finite DNNs, NTK, CNTK, and GNTK comparisons by observing
that their performance-wise ranking depends on the employed DNN architecture.
We summarize our contributions as follows:
[C1] We derive the analytical form for the RNTK of an overparametrized RNN at initialization using
rectified linear unit (ReLU) and error function (erf) nonlinearities for arbitrary data lengths and
number of layers (Section 3.1).
[C2] We prove that the RNTK remains constant during (overparametrized) RNN training and that the
dynamics of training are simplified to a set of ordinary differential equations (ODEs) (Section 3.2).
[C3] When the input data sequences are of equal length, we show that the RNTKs of weight-tied and
weight-untied RNNs converge to the same RNTK (Section 3.3).
[C4] Leveraging our analytical formulation of the RNTK, we empirically demonstrate how correla-
tions between data at different times are weighted by the function learned by an RNN for different
sets of hyperparameters. We also offer practical suggestions for choosing the RNN hyperparameters
for deep information propagation through time (Section 3.4).
[C5] We demonstrate that the RNTK is eminently practical by showing its superiority over classical
kernels, NTKs, and finite RNNs in exhaustive experiments on time-series classification and regression
with both synthetic and 56 real-world data sets (Section 4).
2	Background and Related Work
Notation. We denote [n] = {1, . . . , n}, and Id the identity matrix of size d. [A]i,j represents
the (i, j)-th entry of a matrix, and similarly [a]i represents the i-th entry of a vector. We use
φ(∙) : R → R to represent the activation function that acts coordinate wise on a vector and φ0 to
denote its derivative. We will often use the rectified linear unit (ReLU) φ(x) = max(0, x) and error
function (erf) φ(x) = √2∏ ʃX e-z dz activation functions. N(μ, Σ) represents the multidimensional
Gaussian distribution with the mean vector μ and the covariance matrix Σ.
Recurrent Neural Networks (RNNs). Given an input sequence data x = {xt}tT=1 of length T with
data at time t, xt ∈ Rm , a simple RNN (Elman, 1990) performs the following recursive computation
at each layer ` and each time step t
g(',t)(x) = W⑶h('，tT)(X) + U⑶h('Tt)(X) + b⑶，	h(',t)(x) = φ (g('，t)(x)),
where W(') ∈ Rn×n, b(') ∈ Rn for ' ∈ [L], U⑴ ∈ Rn×m and U(') ∈ Rn×n for ' ≥ 2 are the
RNN parameters. g(',t) (x) is the pre-activation vector at layer ' and time step t, and h(',t) (x) is the
after-activation (hidden state). For the input layer ' = 0, we define h(0,t)(x) := xt. h(',0)(x) as the
initial hidden state at layer ` that must be initialized to start the RNN recursive computation.
The output of an L-hidden layer RNN with linear read out layer is achieved via
fθ(X) = V h(L,T)(X),
where V ∈ Rd×n . Figure 1 visualizes an RNN unrolled through time.
2
Published as a conference paper at ICLR 2021
Figure 1: Visualization of
a simple RNN that high-
lights a cell (purple), a
layer (red) and the initial
hidden state of each layer
(green). (Best viewed in
color.)
Neural Tangent Kernel (NTK). Let fθ (x) ∈ Rd be the output of a DNN with parameters θ. For
two input data sequences x and x0, the NTK is defined as (Jacot et al., 2018)
θ S(X, XO) = hVθs fθs (X), Vθs fθs (χ0)i,
where fθs and θs are the network output and parameters during training at time s.1 Let X and
Y be the set of training inputs and targets, '(y, y) : Rd X Rd → R+ be the loss function, and
L = 责 P(X y)∈χ×γ '(fθs (x), y) be the the empirical loss. The evolution of the parameters θs and
output of the network fθs on a test input using gradient descent with infinitesimal step size (a.k.a
gradient flow) with learning rate η is given by
d∂θs = -ηvθs fθs(X)TTVfes (X)L
dfθs(X)
∂s
-ηVθsfθs(X)Vθsfθs(X)TVfθs(X)L= -ηΘbs(X,X)Vfθs(X)L.
(1)
(2)
Generally, Θs(X, X0), hereafter referred to as the empirical NTK, changes over time during training,
making the analysis of the training dynamics difficult. When fθs corresponds to an infinite-width
MLP, (Jacot et al., 2018) showed that Θs(X, X0) converges to a limiting kernel at initialization and
stays constant during training, i.e.,
lim Θbs (X, X0)
n→∞
lim Θb0(X, X0) := Θ(X, X0) ∀s ,
n→∞
which is equivalent to replacing the outputs of the DNN by their first-order Taylor expansion in
the parameter space (Lee et al., 2019). With a mean-square error (MSE) loss function, the training
dynamics in (1) and (2) simplify to a set of linear ODEs, which coincides with the training dynamics
of kernel ridge regression with respect to the NTK when the ridge term goes to zero. A nonzero ridge
regularization can be conjured up by adding a regularization term λ∣∣θs - θok2 to the empirical loss
(Hu et al., 2020).
3	The Recurrent Neural Tangent Kernel
We are now ready to derive the RNTK. We first prove the convergence of an RNN at initialization
to the RNTK in the infinite-width limit and discuss various insights it provides. We then derive
the convergence of an RNN after training to the RNTK. Finally, we analyze the effects of various
hyperparameter choices on the RNTK. Proofs of all of our results are provided in the Appendices.
3.1	RNTK for an Infinite-Width RNN at Initialization
First we specify the following parameter initialization scheme that follows previous work on NTKs (Ja-
cot et al., 2018), which is crucial to our convergence results:
W⑶=σtW⑶，U⑴=σ1= U⑴，U⑶=σu= U⑶('≥2), V = σv= V, b⑶=σbb⑶,(3)
n	mn	n
where
[W']i,j, [U⑶]i,j, [V]i,j, [b⑶]i 〜N(0,1).	(4)
We will refer to (3) and (4) as the NTK initialization. The choices of the hyperparameters σw , σu ,
σv and σb can significantly impact RNN performance, and we discuss them in detail in Section
1We use s to denote time here, since t is used to index the time steps of the RNN inputs.
3
Published as a conference paper at ICLR 2021
3.4. For the initial (at time t = 0) hidden state at each layer ', We set h(',0) (x) to an i.i.d. copy of
N(0, σh) (Wang et al., 2018) . For convenience, we collect all of the learnable parameters of the
RNN into θ = Vect [{{W('), U⑶,b(')}L=ι, V}].
The derivation of the RNTK at initialization is based on the correspondence betWeen Gaussian
initialized, infinite-Width DNNs and Gaussian Processes (GPs), knoWn as the DNN-GP. In this setting
every coordinate of the DNN output tends to a GP as the number of units/neurons in the hidden layer
(its Width) goes to infinity. The corresponding DNN-GP kernel is computed as
K(x, x0) = M [[fθ(x)]i ∙ [fθ(x0)]i], ∀i ∈ [d].	(5)
First introduced for a single-layer, fully-connected neural netWork by (Neal, 1995), recent Works on
NTKs have extended the results for various DNN architectures (Lee et al., 2018; Duvenaud et al.,
2014; Novak et al., 2019; Garriga-Alonso et al., 2019; Yang, 2019b), Where in addition to the output,
all pre-activation layers of the DNN tends to a GPs in the infinite-Width limit. In the case of RNNs,
each coordinate of the RNN pre-activation g(',t) (x) converges to a centered GP depending on the
inputs With kernel
Σ(',t,t0)(x, x0) = g% [[g(',t)(x)]i ∙ [g(',t0)(x0)]i] ∀i ∈ [n].	(6)
As per (Yang, 2019a), the gradients of random infinite-Width DNNs computed during backpropagation
are also Gaussian distributed. In the case of RNNs, every coordinate of the vector δ(',t)(x):=
√n(Vg(',t)(χ)fθ(x)) converges to a GP with kernel
∏%t,tO)(x, χ0) = g% [[δ(',t)(x)]i ∙ [δ(',t0)(x0)]i] ∀i ∈ [n].	⑺
Both convergences occur independently of the coordinate index i and for inputs of possibly different
lengths, i.e., T 6= T0. With (6) and (7), we now prove that an infinite-width RNN at initialization
converges to the limiting RNTK.
Theorem 1 Let x and x0 be two data sequences of potentially different lengths T andT0, respectively.
Without loss of generality, assume that T ≤ T0, and let τ := T0 - T. Let n be the number of units in
the hidden layers, the empirical RNTK for an L-layer RNN with NTK initialization converges to the
following limiting kernel as n → ∞
lim Θo(x, x0) = Θ(x, x0) = Θ(L,τ,τ )(x, x0) 0 Id ,	(8)
n→∞
where
Θ(L,T,T0)(x, x0) = (XX (∏(',t,t+τ)(x, x0) ∙ Σ(',t,t+τ)(x, x0))) + K(χ, χ0) ,	(9)
V=1 t=1
with K(x, x0), ∑(',t,t+τ) (x, χ0), and Π(',t,t+τ) (x, x0) defined in (5)-(7).
Remarks. Theorem 1 holds generally for any two data sequences, including different lengths ones.
This highlights the RNTK’s ability to produce a similarity measure Θ(x, x0) even if the inputs are of
different lengths, without resorting to heuristics such as zero padding the inputs to the to the max
length of both sequences. Dealing with data of different length is in sharp contrast to common kernels
such as the classical radial basis functions, polynomial kernels, and current NTKs. We showcase this
capability below in Section 4.
To visualize Theorem 1, we plot in the left plot in Figure 2 the convergence of a single layer,
sufficiently wide RNN to its RNTK with the two simple inputs x = {1, -1, 1} of length 3 and
x0 = {cos(α), sin(α)} of length 2, where α = [0, 2π]. For an RNN with a sufficiently large hidden
state (n = 1000), we see clearly that it converges to the RNTK (n = ∞).
RNTK Example for a Single-Layer RNN. We present a concrete example of Theorem 1 by
showing how to recursively compute the RNTK for a single-layer RNN; thus we drop the layer
index for notational simplicity. We compute and display the RNTK for the general case ofa multi-
layer RNN in Appendix B.3. To compute the RNTK Θ(τ,τ0) (x, x0), we need to compute the GP
4
Published as a conference paper at ICLR 2021
2 5
(飞C2)o①
—n = IOOO
—n = oo
)k 0ΘbΘ-Θ k(gol
-→- Weight-untied
-→-Weight-tied


α
2	2.5	3	3.5	4
log(n)
Figure 2: Empirical demonstration of a wide, single-layer RNN converging to its limiting RNTK. Left:
convergence for a pair of different-length inputs x = {1, -1, 1} and x0 = {cos(α), sin(α)}, with varying
α = [0, 2π]. The vertical axis corresponds to the RNTK values for different values of α. Right: convergence of
weight-tied and weight-untied single layer RNN to the same limiting RNTK with increasing width (horizontal
axis). The vertical axis corresponds to the average of the log-normalized error between the empirical RNTK
computed using finite RNNs and the RNTK for 50 Gaussian normal signals of length T = 5.
kernels Σ(t,t+τ) (x, x0) and Π(t,t+τ)(x, x0). We first define the operator Vφ K that depends on the
nonlinearity φ(∙) and a positive semi-definite matrix K ∈ R2 ×2
Vφ [K] = E[φ(zι) ∙ φ(z2)],	(zι, Z2)〜N(0, K).	(10)
Following (Yang, 2019a), we obtain the analytical recursive formula for the GP kernel Σ(t,t+τ) (x, x0)
for a single layer RNN as
2
∑(1,1)(x, x0) = σWσh 1(χ=χθ) + -Uhxι, x；)+ σ2	(11)
m
2
Σ(t,1)(x, x0) = -Uhxt, x；) + σ2	t > 1	(12)
m
Σ(1,to)(x, x0) = σuhxι, xto) + σ2	t0 > 1	(13)
m
2
Σ(t,t')(x, x0) = σWVφ[K(t,t')(x, x0)] + — hxt, Xn + σ2	t,t0 > 1	(14)
m
K(x, x0)= σVVφ [K(T +1,T0+1)(x, x0)],	(15)
where
K(t,t0)(Xx0) = [ £('-1,：-1)出 x)	2(iDQ, χ0) ]	(16)
K	(x, X ) = [ ∑(t-1,t0-1)(χ, X0) ∑(t0T,t0T)(χ0, χ0) J .	(Io)
Similarly, we obtain the analytical recursive formula for the GP kernel Π(t,t+τ) (x, x0) as
∏(T,T0)(x, x0) = σVVφ0 [K(T +1,T+τ+1)(x, x0)]	(17)
Π(t,t+τ)(x, x0) = σWVφo [K(t+1,t+τ+1)(x, χ0)]∏(t+1,t+1+τ)(χ, χ0)	t ∈ [T - 1]	(18)
Π(t,t0) (X, X0) = 0	t0 - t 6= τ.	(19)
For φ = ReLU and φ = erf, we provide analytical expressions for Vφ K and Vφ0 K in Appendix
B.5. These yield an explicit formula for the RNTK that enables fast and point-wise kernel evaluations.
For other activation functions, one can apply the Monte Carlo approximation to obtain Vφ K and
Vφ0 [K] (Novak et al., 2019).
3.2	RNTK for an Infinite-Width RNN during Training
We prove that an infinitely-wide RNN, not only at initialization but also during gradient descent
training, converges to the limiting RNTK at initialization.
Theorem 2 Let n be the number of units of each RNN’s layer. Assume that Θ(X , X) is positive
definite on X such that λmm(Θ(X, X)) > 0. Let η* := 2(λmm(Θ(X, X)) + λmaχ(Θ(X, X)))T.
For an L-layer RNN with NTK initialization as in (3), (4) trained under gradient flow (recall (1) and
(2)) with η < η*, we have with high probability
SUp kθs U0k2, supkΘs(X, X) - Θo(X, X)k2 = O (ɪ).
s ns	n
5
Published as a conference paper at ICLR 2021
tttt
Figure 3: Per time step t (horizontal axis) sensitivity analysis (vertical axis) of the RNTK for the ReLU (top
row) and erf (bottom row) activation functions for various weight noise hyperparameters. We also experiment
with different RNTK hyperparameters in each of the subplots, given by the subplot internal legend. Clearly, the
ReLU (top-row) provides a more stable kernel across time steps (highlighted by the near constant sensitivity
through time). On the other hand, erf (bottom row) sees a more erratic behavior either focusing entirely on early
time-steps or on the latter ones.
Remarks. Theorem 2 states that the training dynamics of an RNN in the infinite-width limit as in
(1), (2) are governed by the RNTK derived from the RNN at its initialization. Intuitively, this is due to
the NTK initialization (3), (4) which positions the parameters near a local minima, thus minimizing
the amount of update that needs to be applied to the weights to obtain the final parameters.
3.3	RNTK for an Infinite-Width RNN Without Weight Sharing
We prove that, in the infinite-width limit, an RNN without weight sharing (untied weights), i.e.,
using independent new weights W(Kt), U(Kt) and b(',t) at each time step t, converges to the same
RNTK as an RNN with weight sharing (tied weights). First, recall that it is a common practice to use
weight-tied RNNs, i.e., in layer ', the weights W('), U(') and b(') are the same across all time steps
t. This practice conserves memory and reduces the number of learnable parameters. We demonstrate
that, when using untied-weights, the RNTK formula remains unchanged.
Theorem 3 For inputs of the same length, an RNN with untied weights converges to the same RNTK
as an RNN with tied weights in the infinite-width (n → ∞) regime.
Remarks. Theorem 3 implies that weight-tied and weight-untied RNNs have similar behaviors in the
infinite-width limit. It also suggests that existing results on the simpler, weight-untied RNN setting
may be applicable for the more general, weight-tied RNN. The plot on the right side of Figure 2
empirically demonstrates the convergence of both the weight-tied and weight-untied RNNs to the
RNTK with increasing hidden layer size n; moreover, the convergence rates are similar.
3.4	Insights into the Roles of the RNTK’s Hyperparameters
Our analytical form for the RNTK is fully determined by a small number of hyperparameters,
which contains the various weight variances collected into S = {σw , σu , σb , σh} and the activation
function.2 In standard supervised-learning settings, one often performs cross-validation to select
the hyperparameters. However, since kernel methods become computationally intractable for large
datasets, we seek a more computationally friendly alternative to cross-validation. Here we conduct a
novel exploratory analysis that provides new insights into the impact of the RNTK hyperparameters
on the RNTK output and suggests a simple method to select them a priori in a deliberate manner.
To visualize the role of the RNTK hyperparameters, we introduce the sensitivity s(t) of the RNTK of
two input sequences x and x0 with respect to the input xt at time t
s(t) = kVχtΘ(x,x0)k2 .	(20)
2From (11) to (18) we emphasize that σv merely scales the RNTK and does not change its overall behavior.
6
Published as a conference paper at ICLR 2021
Table 1: Summary of time series classification results on 56 real-world data sets. The RNTK
outperforms classical kernels, the NTK, and trained RNNs across all metrics. See Appendix A for
detailed description of the metrics.
	RNTK	NTK	RBF	Polynomial	Gaussian RNN	Identity RNN	GRU
Acc. mean ↑	80.15% ± 15.99%	77.74% ± 16.61%	78.15% ± 16.59%	77.69% ± 16.40%	55.98% ± 26.42%	63.08% ± 19.02 %	69.50% ± 22.67
P90 ↑	92.86%	85.71%	87.60%	82.14%	28.57%	42.86%	60.71%
P95 ↑	80.36%	66.07%	75.00%	67.86%	17.86%	21.43%	46.43%
PMA ↑	97.23%	94.30%	94.86%	94.23%	67.06%	78.22%	84.31%
Friedman Rank J	2.38	3.00	2.89	3.46	5.86	5.21	4.21
Here, s(t) indicates how sensitive the RNTK is to the data at time t, i.e., xt, in presence of another data
sequence x0. Intuitively, large/small s(t) indicates that the RNTK is relatively sensitive/insensitive to
the input xt at time t.
The sensitivity is crucial to understanding to which extent the RNTK prediction is impacted by the
input at each time step. In the case where some time indices have a small sensitivity, then any input
variation in those corresponding times will not alter the RNTK output and thus will produce a metric
that is invariant to those changes. This situation can be beneficial or detrimental based on the task at
hand. Ideally, and in the absence of prior knowledge on the data, one should aim to have a roughly
constant sensitivity across time in order to treat all time steps equally in the RNTK input comparison.
Figure 3 plots the normalized sensitivity s(t)/maxt(s(t)) for two data sequences of the same length
T = 100, with s(t) computed numerically for Xt, Xt 〜N(0,1). We repeated the experiments 10000
times; the mean of the sensitivity is shown in Figure 3. Each of the plots shows the changes of
parameters SReLU = {√2,1,0,0} for φ = ReLU and Serf = {1,0.01,0.05,0} for φ = erf.
From Figure 3 we first observe that both ReLU and erf show similar per time step sensitivity measure
s(t) behavior around the hyperparameters SReLU and Serf. If one varies any of the weight variance
parameters, the sensitivity exhibits a wide range of behavior, and in particular with erf. We observe
that σw has a major influence on s(t). For ReLU, a small decrease/increase in σw can lead to
over-sensitivity of the RNTK to data at the last/first times steps, whereas for erf, any changes in σw
leads to over-sensitivity to the last time steps.
Another notable observation is the importance of σh, which is usually set to zero for RNNs. (Wang
et al., 2018) showed that a non-zero σh acts as a regularization that improves the performance
of RNNs with the ReLU nonlinearity. From the sensitivity perspective, a non-zero σh results in
reducing the importance of the first time steps of the input. We also see the same behavior in erf, but
with stronger changes as σh increases. Hence whenever one aims at reinforcing the input pairwise
comparisons, such parameters should be favored.
This sensitivity analysis provides a practical tool for RNTK hyperparameter tuning. In the absence
of knowledge about the data, hyperparameters should be chosen to produce the least time varying
sensitivity. If given a priori knowledge, hyperparameters can be selected that direct the RNTK to the
desired time-steps.
4	Experiments
We now empirically validate the performance of the RNTK compared to classic kernels, NTKs, and
trained RNNs on both classification and regression tasks using a large number of time series data
sets. Of particular interest is the capability of the RNTK to offer high performance even on inputs of
different lengths.
Time Series Classification. The first set of experiments considers time series inputs of the same
lengths from 56 datasets in the UCR time-series classification data repository (Dau et al., 2019).
We restrict ourselves to selected data sets with fewer than 1000 training samples and fewer than
1000 time steps (T) as kernel methods become rapidly intractable for larger datasets. We compare
the RNTK with a variety of other kernels, including the Radial Basis Kernel (RBF), polynomial
kernel, and NTK (Jacot et al., 2018), as well as finite RNNs with Gaussian, identity (Le et al., 2015)
initialization, and GRU (Cho et al., 2014). We use φ = ReLU for both the RNTKs and NTKs. For
each kernel, we train a C-SVM (Chang & Lin, 2011) classifier, and for each finite RNN we use
gradient descent training. For model hyperparameter tuning, we use 10-fold cross-validation. Details
on the data sets and experimental setup are available in Appendix A.1.
7
Published as a conference paper at ICLR 2021
Figure 4: Performance of the RNTK on the synthetic sinusoid and real-world Google stock price data sets
compared to three other kernels. We vary the input lengths (a,c), the input noise level (b), and training set size
(d). We compute the average SNR by repeating each experiment 1000 times. The RNTK clearly outperforms all
of the other kernels under consideration. Figure 4b suggests that the RNTK performs better when input noise
level is low demonstrating one case where time recurrence from RNTK might be sub-optimal as it collects and
accumulate the high noise from each time step as opposed to other kernels treating each independently.
(c) Google stock value
(d) Google stock value
We summarize the classification results over all 56 datasets in Table 1; detailed results on each data
set is available in Appendix A.2. We see that the RNTK outperforms not only the classical kernels
but also the NTK and trained RNNs in all metrics. The results demonstrate the ability of RNTK to
provide increased performances compare to various other methods (kernels and RNNs). The superior
performance of RNTK compared to other kernels, including NTK, can be explained by the internal
recurrent mechanism present in RNTK, allowing time-series adapted sample comparison. In addition,
RNTK also outperforms RNN and GRU. As the datasets we consider are relative small in size, finite
RNNs and GRUs that typically require large amount of data to succeed do not perform well in our
setting. An interesting future direction would be to compare RNTK to RNN/GRU on larger datasets.
Time Series Regression. We now validate the performance of the RNTK on time series inputs of
different lengths on both synthetic data and real data. For both scenarios, the target is to predict the
next time-step observation of the randomly extracted windows of different length using kernel ridge
regression.
We compare the RNTK to other kernels, the RBF and polynomial kernels and the NTK. We also
compare our results with a data independent predictor that requires no training, that is simply to
predict the next time step with previous time step (PTS).
For the synthetic data experiment, we simulate 1000 samples of one period ofa sinusoid and add white
Gaussian noise with default σn = 0.05. From this fixed data, we extract training set size Ntrain = 20
segments of uniform random lengths in the range of [Tfixed, Tfixed + Tvar] with Tfixed = 10. We use
standard kernel ridge regression for this task. The test set is comprised of Ntest = 5000 obtained
from other randomly extracted segments, again of varying lengths. For the real data, we use 975 days
of the Google stock value in the years 2014-2018. As in the simulated signal setup above, We extract
Ntrain segments of different lengths from the first 700 days and test on the Ntest segments from days
701 to 975. Details of the experiment are available in Appendix A.2.
We report the predicted signal-to-noise ratio (SNR) for both datasets in Figures 4a and 4c for various
values of Tvar. We vary the noise level and training set size for fixed Tvar = 10 in Figures 4b and
4d. As We see from Figures 4a and 4c, the RNTK offers substantial performance gains compared
to the other kernels, due to its ability to naturally deal With variable length inputs. Moreover,
the performance gap increases With the amount of length variation of the inputs Tvar . Figure 4d
demonstrates that, unlike the other methods, the RNTK maintains its performance even When the
training set is small. Finally, Figure 4c demonstrates that the impact of noise in the data on the
regression performance is roughly the same for all models but becomes more important for RNTK
With a large σn ; this might be attributed to the recurrent structure of the model alloWing for a time
propagation and amplification of the noise for very loW SNR. These experiments demonstrate the
distinctive advantages of the RNTK over classical kernels, and NTKs for input data sequences of
varying lengths.
In the case of PTS, We expect the predictor to outperform kernel methods When learning from the
training samples is hard, due to noise in the data or small training size Which can lead to over fitting.
In Figure 4a RNTK and Polynomial kernels outperforms PTS for all values of Tvar , but for larger
Tvar, NTK and RBF under perform PTS due to the increasing detrimental effect of zero padding.
8
Published as a conference paper at ICLR 2021
For the Google stock value, we see a superior performance of PTS with respect to all other kernel
methods due to the nature of those data heavily relying on close past data. However, RNTK is able to
reduce the effect of over-fitting, and provide the closest results to PTS among all kernel methods we
employed, with increasing performance as the number of training samples increase.
5	Conclusions
In this paper, we have derived the RNTK based on the architecture of a simple RNN. We have proved
that, at initialization, after training, and without weight sharing, any simple RNN converges to the
same RNTK. This convergence provides new insights into the behavior of infinite-width RNNs,
including how they process different-length inputs, their training dynamics, and the sensitivity of
their output at every time step to different nonlinearities and initializations. We have highlighted
the RNTK’s practical utility by demonstrating its superior performance on time series classification
and regression compared to a range of classical kernels, the NTK, and trained RNNs. There
are many avenues for future research, including developing RNTKs for gated RNNs such as the
LSTM (Hochreiter & Schmidhuber, 1997) and investigating which of our theoretical insights extend
to finite RNNs.
Acknowledgments
This work was supported by NSF grants CCF-1911094, IIS-1838177, and IIS-1730574; ONR grants
N00014-18-12571, N00014-20-1-2787, and N00014-20-1-2534; AFOSR grant FA9550-18-1-0478;
and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In In Advances in Neural Information Processing
Systems, pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in Neural Information Processing Systems, pp. 6676-6688, 2019b.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, pp. 1120-1128, 2016.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150. 2019a.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu.
Harnessing the power of infinitely wide deep nets on small-data tasks. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
rkl8sJBYvH.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849-15854, 2019. URL https://www.pnas.org/content/116/32/15849.
Yoshua. Bengio, Patrice. Simard, and Paolo. Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE Trans. Neural Networks, 5(2):157-166, 1994.
Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherrington-
kirkpatrick model. Communications in Mathematical Physics, 325(1):333-366, 2014.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2:27:1-27:27, 2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
9
Published as a conference paper at ICLR 2021
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems, pp. 342-350, 2009.
Hoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh
Gharghabi, Chotirat Ann Ratanamahatana, Yanping Chen, Bing Hu, Nurjahan Begum, Anthony
Bagnall, Abdullah Mueen, Gustavo Batista, and ML Hexagon. The UCR time series classifica-
tion archive, 2019. URL https://www.cs.ucr.edu/~eamonn/time_series_data_
2018/.
Simon S. Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems, pp. 5724-5734, 2019a.
Simon S. Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019b.
David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very
deep networks. In Artificial Intelligence and Statistics, pp. 202-210, 2014.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990. URL https:
//www.sciencedirect.com/science/article/pii/036402139090002E.
M. Ferndndez-Delgado, M.S. Sirsat, E. Cernadas, S. Alawadi, S. Barro, andM. Febrero-Bande. An
extensive experimental survey of regression methods. Neural Networks, 111:11 - 34, 2019.
Adri鱼 Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional
networks as shallow gaussian processes. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=Bklfsi0cKm.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In International Conference on Machine Learning, pp. 2034-2042. PMLR, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on
noisily labeled data with generalization guarantee. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=Hke3gyHYwH.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize
better than deep feedforward networks?-a neural tangent kernel perspective. arXiv preprint
arXiv:2002.06262, 2020.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljacic. Tunable efficient unitary neural networks (eunn) and their application to rnns. In
International Conference on Machine Learning, pp. 1733-1741, 2017.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.
10
Published as a conference paper at ICLR 2021
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, pp. 8572-8583,
2019.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. arXiv
preprint arXiv:2007.15801, 2020.
Radford M Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of
over-parametrization in generalization of neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=BygfghAcYX.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJC2SzZCW.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels
are gaussian processes. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1g30j0qF7.
Zichao Wang, Randall Balestriero, and Richard Baraniuk. A max-affine spline perspective of
recurrent neural networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=BJej72AqF7.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019a.
Greg Yang. Tensor programs I: Wide feedforward or recurrent neural networks of any architecture
are gaussian processes. arXiv preprint arXiv:1910.12478, 2019b.
Greg Yang. Tensor programs II: Neural tangent kernel for any architecture. arXiv preprint
arXiv:2006.14548, 2020a.
Greg Yang. Tensor programs III: Neural matrix laws. arXiv preprint arXiv:2009.10685, 2020b.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
11
Published as a conference paper at ICLR 2021
Σ(1)
Σ(')(X, x0)
Σ(')(x, x0)
K(')(x, x0)
K(X, X0)
kNTK
A Experiment Details
A. 1 Time series classification
Kernel methods settings. We used RNTK, RBF, polynomial and NTK (Jacot et al., 2018). For data
pre-processing, we normalized the norm of each x to 1. For training we used C-SVM in LIBSVM
library (Chang & Lin, 2011) and for hyperparameter selection we performed 10-fold validation for
splitting the training data into 90% training set and 10% validation test. We then choose the best
performing set of hyperparameters on all the validation sets, retrain the models with the best set of
hyperparameters on the entire training data and finally report the performance on the unseen test data.
The performance of all kernels on each data set is shown in table 2.
For C-SVM we chose the cost function value
C ∈ {0.01, 0.1, 1, 10, 100}
and for each kernel we used the following hyperparameter sets
•	RNTK: We only used single layer RNTK, we φ = ReLU and the following hyperparameter
sets for the variances:
σw ∈ {1.34,1.35,1.36,1.37,1.38,1.39,1.40,1.41,1.42, √2,1.43,1.44,1.45,1.46,1.47}
σu = 1
σb ∈ {0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 1, 2}
σh ∈ {0, 0.01, 0.1, 0.5, 1}
•	NTK: The formula for NTK of L-layer MLP (Jacot et al., 2018) for x, x0 ∈ Rm is:
2
—hx, x0i + σ2
m
σWVφ[K(')(x, XO)] + σ2	' ∈ [L]
σW Vφ0 [K('+1)(x, X0)]	' ∈ [L]
Σ('-1)(x, x)	Σ('-1)(x, x0)
_ Σ('-1)(x, x0) Σ('-1)(x0, x0) _
σv2Vφ[K(L+1)(X,X0)]
X 卜⑶(x, X0) Y ∑ ⑶(χ, X0)) + K(x, X0)
'=1 ∖	'0='	)
and we used the following hyperparamters
L∈ [10]
σw ∈ {0.5,1, √2, 2, 2.5, 3}
σb ∈ {0, 0.01, 0.1, 0.2, 0.5, 0.8, 1, 2, 5}
•	RBF:
kRBF(X, X0) = e(-αkx-x0k22)
α ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 0.6, 0.7, 0.8, 1, 2, 3, 4, 5, 10, 20, 30, 40, 100}
•	Polynomial:
kPolynomial(X, X ) = (r + hX, X i)
d ∈ [5]
r∈ {0, 0.1, 0.2, 0.5, 1, 2}
12
Published as a conference paper at ICLR 2021
Table 2: Test accuracy of each model on 56 time series data set from UCR time-series classification
data repository (Dau et al., 2019).
Dataset	RNTK	NTK	RBF	POLY	Gaussian RNN	Identity RNN	GRU
Strawberry	98.38	97.57	97.03	96.76	94.32	75.4	91.62
ProximalPhalanxOutlineCorrect	89	87.97	87.29	86.94	82.81	74.57	86.94
PowerCons	97.22	97.22	96.67	91.67	96.11	95	99.44
Ham	70.48	71.63	66.67	71.43	53.33	60	60.95
SmallKitchenAppliances	67.47	38.4	40.27	37.87	60.22	76	71.46
ScreenType	41.6	43.2	43.47	38.4	40	41.06	36.26
MiddlePhalanxOutlineCorrect	57.14	57.14	48.7	64.29	76.28	57.04	74.57
RefrigerationDevices	46.93	37.07	36.53	41.07	36	50.93	46.66
Yoga	84.93	84.63	84.63	84.87	46.43	76.66	61.83
Computers	59.2	55.2	58.8	56.4	53.2	55.2	58.8
ECG5000	93.76	94.04	93.69	93.96	88.4	93.15	93.26
Fish	90.29	84	85.71	88	28	38.28	24
UWaveGestureLibraryX	79.59	78.7	78.48	65.83	55.97	75.34	73.64
UWaveGestureLibraryY	71.56	70.63	70.35	70.32	44.5	65.18	65.38
UWaveGestureLibraryZ	73.95	73.87	72.89	71.94	43.29	67.81	70.32
StarLightCurves	95.94	96.19	94.62	94.44	82.13	86.81	96.15
CricketX	60.51	59.49	62.05	62.56	8.46	63.58	26.41
CricketY	63.85	58.97	60.51	59.74	15.89	59.23	36.15
CricketZ	60.26	59.23	62.05	59.23	8.46	57.94	41.28
DistalPhalanxOutlineCorrect	77.54	77.54	75.36	73.91	69.92	69.56	75
Worms	57.14	50.65	55.84	50.65	35.06	49.35	41.55
SyntheticControl	98.67	96.67	98	97.67	92.66	97.66	99
Herring	56.65	59.38	59.38	59.38	23.28	59.37	59.38
MedicalImages	74.47	73.29	75.26	74.61	48.15	64.86	69.07
SwedishLeaf	90.56	91.04	91.36	90.72	59.2	45.92	91.04
ChlorineConcentration	90.76	77.27	86.35	91.54	65.99	55.75	61.14
SmoothSubspace	96	87.33	92	86.67	94	95.33	92.66
TwoPatterns	94.25	90.45	91.25	93.88	99.7	99.9	100
Faceall	74.14	83.33	83.25	82.43	53.66	70.53	70.65
DistalPhalanxTW	66.19	69.78	66.91	67.37	67.62	64.74	69.06
MiddlePhalanxTW	57.79	61.04	59.74	60.39	58.44	58.44	59.09
FacesUCR	81.66	80.2	80.34	82.98	53.21	75.26	79.46
OliveOil	90	86.67	86.67	83.33	66.66	40	40
UMD	91.67	92.36	97.22	90.97	44.44	71.52	100
nsectEPGRegular	99.6	99.2	99.6	96.79	100	100	98.39
Meat	93.33	93.33	93.33	93.33	0.55	55	33.33
Lightning2	78.69	73.77	70.49	68.85	45.9	70.49	67.21
Lightning7	61.64	60.27	63.01	60.27	23.28	69.86	76.71
Car	83.33	78.83	80	80	23.33	58.33	26.66
GunPoint	98	95.33	95.33	94	82	74.66	80.66
Arrowhead	80.57	83.43	80.57	74.86	48	56	37.71
Coffee	100	100	92.86	92.86	100	42.85	57.14
Trace	96	81	76	76	70	71	100
ECG200	93	89	89	86	86	72	76
plane	98.1	96.19	97.14	97.14	96.19	84.76	96.19
GunPointOldVersusYoung	98.73	97.46	98.73	94.6	53.96	52.38	98.41
GunPointMaleVersusFemale	99.05	99.68	99.37	99.68	68.67	52.53	99.68
GunPointAgeSpan	96.52	94.62	95.89	93.99	47.78	47.78	95.56
FreezerRegularTrain	97.44	94.35	96.46	96.84	76.07	7.5	86.59
SemgHandSubjectCh2	84.22	85.33	86.14	86.67	20	36.66	89.11
WormsTwoClass	62.34	62.34	61.04	59.74	51.94	46.75	57.14
Earthquakes	74.82	74.82	74.82	74.82	65.46	76.97	76.97
FiftyWords	68.57	68.57	69.67	68.79	34.28	60.21	65.27
Beef	90	73.33	83.33	93.33	26.67	46.67	36.67
Adiac	76.63	71.87	73.40	77.75	51.4	16.88	60.61
WordSynonyms	57.99	58.46	61.13	62.07	17.71	45.77	53.76
Finite-width RNN settings. We used 3 different RNNs. The first is a ReLU RNN with Gaussian
initialization with the same NTK initialization scheme, where parameter variances are σw = σv = √2,
13
Published as a conference paper at ICLR 2021
σu = 1 and σb = 0. The second is a ReLU RNN with identity initialization following (Le et al.,
2015). The third is a GRU (Cho et al., 2014) with uniform initialization. All models are trained with
RMSProp algorithm for 200 epochs. Early stopping is implemented when the validation set accuracy
does not improve for 5 consecutive epochs.
We perform standard 5-fold cross validation. For each RNN architecture we used hyperparamters of
number of layer, number of hidden units and learning rate as
L∈{1,2}
n ∈ {50, 100, 200, 500}
η ∈ {0.01, 0.001, 0.0001, 0.00001}
Metrics descriptions First, only in this paragraph, let i ∈ {1, 2, ..., N} index a total of N datasets
and j ∈ {1, 2, ..., M} index a total of M classifiers. Let yij be the accuracy of the j-th classifer on
the i-th dataset. We reported results on 4 metrics: average accuracy (Acc. mean), P90, P95, PMA
and Friedman Rank. P90 and P95 is the fraction of datasets that the classifier achieves at least 90%
and 95% of the maximum achievable accuracy for each dataset, i.e.,
P90j = N X 1(yij ≥ 0.9(maxy^)).	(21)
PMA is the accuracy of the classifier on a dataset divided by the maximum achievable accuracy on
that dataset, averaged over all datasets:
PMAj = 1 X yij .	(22)
N	max yij
ij
Friedman Rank (Ferngndez-Delgado et al., 2019) first ranks the accuracy of each classifier on each
dataset and then takes the average of the ranks for each classifier over all datasets, i.e.,
FRj = N X rij,	(23)
i
where rij is the ranking of the j-th classifier on the i-th dataset.
Note that a better classifier achieves a lower Friedman Rank, Higher P/90 and PMA.
Remark. In order to provide insight into the performance of RNTK in long time steps setting,
we picked two datasets with more that 1000 times steps: SemgHandSubjectCh2 (T = 1024) and
StarLightCurves (T = 1024).
A.2 Time Series Regression
For time series regression, we used the 5-fold validation of training set and same hyperparamter sets
for all kernels. For training we kernel ridge regression with ridge term chosen form
λ ∈ {0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.8, 1, 2, 3, 4, 5,6, 7, 8, 10, 100}
B Proofs for Theorems 1 and 3: RNTK Convergence at
Initialization
B.1	Preliminary: Netsor programs
Calculation of NTK in any architecture relies on finding the GP kernels that correspond to each
pre-activation and gradient layers at initialization. For feedforward neural networks with n1 , . . . , nL
number of neurons (channels in CNNs) at each layer the form of this GP kernels can be calculated
via taking the limit of n1 , . . . , nL sequentially one by one. The proof is given by induction, where by
conditioning on the previous layers, each entry of the current layer is sum of infinite i.i.d Gaussian
random variables, and based on Central Limit Theorem (CLT), it becomes a Gaussian process with
14
Published as a conference paper at ICLR 2021
kernel calculated based on the previous layers. Since the first layer is an affine transformation of
input with Gaussian weights, it is a Gaussian process and the proof is completed. See (Lee et al.,
2018; Duvenaud et al., 2014; Novak et al., 2019; Garriga-Alonso et al., 2019) for a formal treatment.
However, due to weight-sharing, sequential limit is not possible and condoning on previous layers
does not result in i.i.d. weights. Hence the aforementioned arguments break. To deal with it, in (Yang,
2019a) a proof using Gaussian conditioning trick (Bolthausen, 2014) is presented which allows use
of recurrent weights in a network. More precisely, it has been demonstrated than neural networks
(without batch normalization) can be expressed and a series of matrix multiplication and (piece
wise) nonlinearity application, generally referred as Netsor programs. It has been shown that any
architecture that can be expressed as Netsor programs that converge to GPs as width goes to infinity
in the same rate, which a general rule to obtain the GP kernels. For completeness of this paper, we
briefly restate the results from (Yang, 2019a) which we will use later for calculation derivation of
RNTK.
There are 3 types of variables in Netsor programs; A-vars, G-vars and H-vars. A-vars are matrices
and vectors with i.i.d Gaussian entries, G-vars are vectors introduced by multiplication of a vector by
an A-var and H -vars are vectors after coordinate wise nonlinearities is applied to G-vars. Generally,
G-vars can be thought of as pre-activation layers which are asymptotically treated as a Gaussian
distributed vectors, H-vars as after-activation layers and A-vars are the weights. Since in neural
networks inputs are immediately multiplied by a weight matrix, it can be thought of as an G-var,
namely gin . Generally Netsor programs supports G-vars with different dimension, however the
asymptotic behavior of a neural networks described by Netsor programs does not change under this
degree of freedom, as long as they go to infinity at the same rate. For simplicity, let the G-vars and
H-vars have the same dimension n since the network of interest is RNN and all pre-activation layers
have the same dimension. We introduce the Netsor programs under this simplification. To produce
the output of a neural network, Netsor programs receive a set of G-vars and A-vars as input, and new
variables are produced sequentially using the three following operators:
•	Matmul : multiplication of an A-var: A with an H -var: h, which produce a new G-var, g.
g=Ah	(24)
•	Lincomp: Linear combination of G-vars, gi, 1 ≤ i ≤ k , with coefficients ai ∈ R 1 ≤
i ≤ k which produce of new G-var:
k
g=Xaigi	(25)
i=1
•	Nonlin: creating a new H -var, h, by using a nonlinear function φ : Rk → R that act
coordinate wise on a set of G-vars, gi , 1 ≤ i ≤ k :
h = ψ(g1,..., gk)	(26)
Any output of the neural network y ∈ R should be expressed as inner product of a new A-var which
has not been used anywhere else in previous computations and an H-var:
y = v>h
Any other output can be produced by another v0 and h0 (possibility the same h or v).
It is assumed that each entry of any A-var : A ∈ Rn×n in the netsor programs computations is drawn
σ2
from N(0, ^a) and the input G-vars are Gaussian distributed. The collection of a specific entry of all
G-vars of in the netsor program converges in probability to a Gaussian vector {[g1]i,..., [gk]i}〜
N(μ, Σ) for all i ∈ [n] as n goes to infinity.
Let μ(g) := E [[g]i] be the mean of a G-var and Σ(g, g0) := E [[g]i∙ [g0]i] be the covariance between
any two G-vars. The general rule for μ(g) is given by the following equations:
,μin(g)	if g is input
k	k
μ(g) = < X aiμ(gi) ifg = X aigi	(27)
i=1	i=1
、0	otherwise
15
Published as a conference paper at ICLR 2021
For g and g0, let G = {g1, . . . , gr} be the set of G-vars that has been introduced before g and g0
with distribution N(μg, Σg), where ∑g ∈ RlGl×lGl containing the pairwise covariances between
the G-vars. Σ(g, g0) is calculated via the following rules:
		Σin(g, g0)	if g and g 0 are inputs	
Σ(g, g0) =	=‹	k X aiΣ(gi, g0) i=1 k X aiΣ(g, gi) i=1 σA	E	、[以 Z)O(Z)] Z〜N(μ,∑G ) 、0	k ifg = X aigi i=1 k ifg0 = X aigi i=1 if g = Ah and g0 = Ah0 otherwise	(28)
Where h =夕(g1,..., gr) and h0 = θ(g1,∙∙∙, gr) are functions of G-Vars in G from possibly
different nonlinearities. This set of rules presents a recursive method for calculating the GP kernels
in a network where the recursive formula starts from data dependent quantities Σin and μin which
are given.
All the above results holds when the nonlinearities are bounded uniformly by e(cx2-α) for some
α > 0 and when their derivatives exist.
Standard vs. NTK initialization. The common practice (which netsor programs uses) is to
initialize DNNs weights [A]i,j with N(0, √√n) (known as standard initialization) where generally n
is the number of units in the previous layer. In this paper we have used a different parameterization
scheme as used in (Jacot et al., 2018) and we factor the standard deviation as shown in 3 and initialize
weights with standard standard Gaussian. This approach does not change the the forward computation
of DNN, but normalizes the backward computation (when computing the gradients) by factor 1,
n
otherwise RNTK will be scales by n. However this problem can be solved by scaling the step size by
W and there is no difference between NTK and standard initialization (Lee et al., 2019).
B.2 Proof for Theorem 1: Single layer Case
We first derive the RNTK in a simpler setting, i.e., a single layer and single output RNN. We then
generalize the results to multi-layer and multi-output RNNs. We drop the layer index ` to simplify
notation. From 3 and 4, the forward pass for computing the output under NTK initialization for each
input x = {xt}tT=1 is given by:
g(t)(x) = √√m Wh(t-1)(x) + √√n Uxt + σbb
h(t)(x) = φ g(t) (x)
fθ (x) = σV= VTh(T )(x)
(29)
(30)
(31)
Note that (29), (30) and (31) use all the introduced operators introduced in 24, 25 and 26 given input
variables W, {Uxt}tT=1, b, v and h(0)(x).
First, we compute the kernels of forward pass Σ(t,t )(x, x0) and backward pass Π(t,t )(x, x0) intro-
duced in (6) and (7) for two input x and x0. Note that based on (27) the mean of all variables is zero
since the inputs are all zero mean. In the forward pass for the intermediate layers we have:
Σ(t,t0)(x, x0) = Σ(g(t)(x), g(t0)(x0))
=∑ (σwWhCT)(X) + ∑u uχt + σbb, fwWh(t0T)(XO) + MUxto + σbb
Σ √σw= Wh(t-1)(x), √σwWhC-I)(x0)) + ∑in
xt,	Uxto ) + Σin (σbb, σbb).
16
Published as a conference paper at ICLR 2021
We have used the second and third rule in (28) to expand the formula, We have also used the first and
fifth rule to set the cross term to zero, i.e.,
Σ (√σwWh(t-1)(x), √√uUxt，) = 0
Σ √wWWh(t-1)(x),σbb) = 0
ς( √σm Uxt，√√n wh(j(XO)j=0
Σ (σbb, √σwWheT)(X0)) = o
Σin (端Uxt,σbb) =0
Σin 卜bb, √σ= Uxt，) = 0.
For the non-zero terms we have
Σin (σbb, σbb) = σb2
∑in (√uuχt, √uUxt，) = σ2hχt, xt，i，
mm	m
which can be achieved by straight forward computation. If t 6= 1 and t0 6= 1, by using the forth rule
in (28) we have
∑ ( √√W Wh(T)(x), .√w Wh(J)(X0)) = σW	E	[φ(z1)φ(z2)]=Vφ[K (t，tO)(x, x0)].
∖ Vn	Vn	)	Z〜N(0,K(t,t0) (x,x0))
With K (t，t，) (x, x0) defined in (16). Otherwise, it will be zero by the fifth rule (if t or t = 1) .
Here the set of previously introduced	G-vars	is G	=
{{g(α)(x)}, Uxα}0=1ι, {g(α0)(x0), Uxa，}tα-=1ι, h(0)(x), h(0)(x0)}, but the dependency is
only on the last layer G-vars, φ({g : g ∈ G}) = φ(g(t-1)(x)), 0(({g : g ∈ G})) = φ(g(t0-1)(x0)),
leading the calculation to the operator defined in (10). As a result
2
∑(t,t' )(x, x0) = σW Vφ[K (t,t' )(x, X0)] + m hxt, xt，i + σ2
To complete the recursive formula, using the same procedure for the first layers we have
2
∑(1,1)(x, x0) = σWσh 1(χ=χ0) + — hxι, x[〉+ σ2,
m
∑(1,t0)(x, x0) = σuhxι, xt，i + σ2,
m
2
Σ(t,1)(x, x0) = — hxt, x1i + σ2.
m
The output GP kernel is calculated via
K(x, x0) = σV Vφ[K(T +1,T，+1) (x, x0)]
The calculation of the gradient vectors δ(t)(x) = √n (Vg(t)(χfθ (x)) in the backward pass is given
by
δ(T)(x)
= σvv	φ0(g(T)(x))
δ(t)(x) = √nW> (φ0(g(t)(x)) Θ δ(t+1)(x))	t ∈ [T — 1]
To calculate the backward pass kernels, we rely on the following Corollary from (Yang, 2020b)
17
Published as a conference paper at ICLR 2021
Corollary 1 In infinitely wide neural networks weights used in calculation of back propagation
gradients (W>) is an i.i.d copy of weights used in forward propagation (W) as long as the last layer
weight (v) is sampled independently from other parameters and has mean 0.
The immediate result of Corollary 1 is that g(t) (x) and δ(t) (x) are two independent Gaussian vector
as their covariance is zero based on the fifth rule in (28). Using this result, we have:
Π(t,t0) (x, x0) = Σ δ(t)(x), δ(t0)(x)
=E [[δ⑴(x" [δ(t0)(x0)]ii
=σWE [[φ0(g⑴⑺儿∙ [δ(t+1)(x)]i ∙ [φ0(g(QxO))]i ∙ [δ(t0+1)(x0)]ii
=σW	E	[φ0(zι) ∙ Φ0(z2)]∙ E [Nt+I)(x)]i∙ [δ(t'+1)(x0)]ii
Z〜N(0,K(t+1,t+10) (x,x0))	L	」
= σw2 Vφ0 K(t+1,t0+1)(x, x0)Π(t+1,t0+1)(x, x0).
If T 0 - t0 = T - t, then the the formula will lead to
Π(T,T0) (x, x0) =E h[δ(T)(x)]i, [δ(T 0)(x0)]ii
=σVE h[v]i ∙ [φ0(g(T)(X))]i ∙ [v]i ∙ [φ0(g(T 0)(xO))]ii
=E [[φ0(g(T)(x))]i ∙ [Φ0(g(T0)(x0))]ii ∙ E [矶矶]
= σv2Vφ0 K(T+1,T+τ+1)(x,x0).
Otherwise it will end to either of two cases for some t00 < T or T 0 and by the fifth rule in (28) we
have:
∑ (δ(to0)(x), δ(T')(x)) = ∑ (√√wnW> (φo(g(t00)(x)) Θ δ(t00+1)(x0)), V Θ φ0(g(T0)(x))) = 0
∑ (δ(T )(x), δ(t00)(x)) =Σ (V Θ φ0(g(T )(x)), σW W> (φ0(g(t")(x0)) Θ δ(t00+1)
0.
Without loss of generality, from now on assume T 0 < T and T 0 - T = τ, the final formula for
computing the backward gradients becomes:
Π(T,T+τ)(x,x0) =σv2Vφ0K(T+1,T+τ+1)(x,x0)
Π(t,t+τ) (x, x0) = σw2Vφ0K(t+1,t+τ+1)(x,x0)Π(t+1,t+1+τ)(x,x0)	t ∈ [T - 1]
Π(t,t0) (x, x0) = 0	t0 - t 6= τ
Now we have derived the single layer RNTK. Recall that θ = Vect {W, U, b, V} contains all of
the network’s learnable parameters. As a result, we have:
∂fθ(x) ∂fθ(x) ∂fθ(x) ∂fθ(x)
∂W , ∂U , ∂b , ∂v
Vθ fθ (x)=Vect [{
As a result
hVθfθ(x),Vθfθ(x0)i
∕∂fθ(x) ∂fθ(x0)∖	∕∂fθ(x) ∂fθ(x0)∖	∕∂fθ(x) ∂fθ(x0)∖
∖ ∂W , ∂W / + ∖ ∂U ,	∂U / + ∖ ∂b ,	∂b /
∂fθ (x0)
∂ V
18
Published as a conference paper at ICLR 2021
Where the gradients of output with respect to weights can be formulated as the following compact
form:
∂W
∂U
∂b
∂fθ(x)
∂fθ(x)
∂fθ(x)
fx) =霓 h(T )(x).
∂v	n
As a result we have:
d∂W, df∂W)=工 X (1 Di δRx0)E) ∙ (σ2 Dh(t-1)(x), h('τ)(x0)
特,W)=E X(i Di "(x0)〉)∙( σ2 hχt, X
fx,?)= X X (1 Df δ(t3E)∙ σ2
t0=1 t=1
∂fθ (X
∂v
∂fθ(x0)
∂v
h(T)(x), h(T0) (x0)
Remember that for any two G-var E [[g]i[g0]i] is independent of index i. Therefore,
1 Dh(I)(x), h(，-I)(XO)E → Vφ[κ(t,t,)(χ, X0)]	t > 1
1 Dh(0)(X), h(0)(XO)E → σ2.
Hence, by summing the above terms in the infinite-width limit we get
(T T	\
XXΠ(t,t,)(x,x0) ∙Σ(E)(X0,x0)	+K(x,x0).	(32)
t0=1 t=1
Since Π(t,t0)(x, x0) = 0 for t0 - t 6= τ it is simplified to
(Vθfθ(x), Vθfθ(x0)i = (XXΠ(t,t+τ)(x, x0) ∙ Σ(t,t+τ)(x0, x0)) + K(x, x0).
Multi-dimensional output. For fθ(X) ∈ Rd, the i-th output for i ∈ [d] is obtained via
fθ(X)]i = √√n v>h(T )(X),
where vi is independent ofvj for i 6= j. As a result, for The RNTK Θ(X, XO) ∈ Rd×d for multi-
dimensional output we have
[Θ(X, XO)]i,j = DVθ [fθ(X)]i ,Vθ [fθ(XO)]jE
For i = j, the kernel is the same as computed in (32) and we denote it as
hVθ[fθ(X)]i,Vθ[fθ(XO)]ii=Θ(T,T0)(X,XO).
19
Published as a conference paper at ICLR 2021
For i 6= j, since vi is independent of vj, Π(T,T0) (x, x0) and all the backward pass gradients become
zero, so
Nθ [fθ(x)]i, vθ [fθ(x0)]jE =0	i = j
which gives us the following formula
Θ(x, x0) = Θ(T,To)(x, x0) 0 Id.
This concludes the proof for Theorem 1 for single-layer case.
B.3 Proof for Theorem 1: Multi-Layer Case
Now we drive the RNTK for multi-layer RNTK. We will only study single output case and the
generalization to multi-dimensional case is identical as the single layer case. The set of equations for
calculation of the output of a L-layer RNN for x = {xt}tT=1 are
一	σ	..... σ ,八	.，八
g(',t)(x) = -w=. W⑶h('，tT)(X) + ~σ= U⑶Xt + σ'b ⑶	' =1
nm
一	σ ....... σ .......... .,八
g(',t)(x)=/W(')h(',t-1)(x) + -UU(')h('-1,t)(x)+ σb(')	' > 1
nn
h(',t)(x) = φ (g(物(x))
fθ (X) = √ v>h(L,T )(x)
n
The forward pass kernels for the first layer is the same as calculated in B.2. For ` ≥ 2 we have:
∑(',t,tO)(x, X0) = Σ(g(',t)(x), g(',tO)(XO))
=Σ (σw= W⑶h('，tT)(X), σw= W⑶h(',t0T)(XO)I
nn
+ ∑ (√√∣U(')h('-1,t)(X), √√∣U(')h('T，t0)(xo)) +Σin 卜'b('),σ'b('))
= (σW)2Vφ [K('，t，t0)(X, X0)]+(σU)2Vφ[K('-1，”'+1)(x, xo)] +(σ')2,
where
K(',t,t0)(X, X0)
-∑ET,tT)(X, x)	∑dtT,t0T)(X, X0)
∑dtτ,t0τ)(X, X0)	∑(',t0-1,t0-1)(xo, X0)
and Σin is defined in (B.2). For the first first time step we have:
ς(',1,1)(x, XO) = (σW)2σh 1(χ=χθ) + (σU)2Vφ[K(',2,2)(x, x0)] + (σ')2 ,
∑(',t,1)(X, X0) = (σU)2Vφ[K(',t+1,2)(X, x0)] + (σb)2 ,
∑(',1,t0)(X, x0) = (σU)2Vφ [K(',2,t0+1)(x, x0)] + (σ')2 .
And the output layer
K(X, X0) = σv2Vφ[K(L,T+1,TO+1)(X, X0)].
Note that because of using new weights at each layer we get
∑(g(',t)(x), g&tO))(X)) =0	' = '0
Now we calculate the backward pass kernels in multi-layer RNTK. The gradients at the last layer is
calculated via
δ(L,T) (X) = σvv	φ0(g(L,T) (X)).
In the last hidden layer for different time steps we have
0(g(L,t)(X))	δ(L,t+1)(X)	t∈ [T - 1]
20
Published as a conference paper at ICLR 2021
In the last time step for different hidden layers We have
δ('τ)(χ) = σ√+1 (U('+I))T (φ(g(',T)(x)) Θ δ('+1,T^x))	' ∈ [L - 1]
At the end for the other layers We have
('㈤(x)) Θ 6('HI)(X))
+ σ√+1 (U('+I))T (φ(g(S(X)) Θ δ('+1't)(x))	' ∈ [L - 1],t ∈ [T - 1]
The recursive formula for the Π(L也')(x, x0) is the same as the single layer, and it is non-zero for
t0 -1 = T- - T = T. As a result we have
Π(LTTH)(x, x0) = σVV∕ [K(L，T +1，TH+1)] (x, x0)
Π(L,t,t+τ)(x, x0) = (σW)2Vφo [K(LHIHT +1)](x, x0) ∙ ∏(LHIHIH)(x, x0) t ∈ [T - 1]
∏(L")(x, x0) = 0	t0 - t = τ
(33)
Similarly by using the same course of arguments used in the single layer setting, for the last time step
we have
Π%t,t+τ)(x, x0) = (σU+1)2Vφ0 [KMT+1，T+τ+1)](x, x0) ∙∏('"τ +1，T+τ+1)(x, x0)	' ∈ [L - 1]
For the other layers we have
Π('")(x, x0) = (σW)2Vφo [K('H1"τ +1)] (x, x0) ∙ Π%+"i+τ)(x, x0)
+ (σf+1)2Vφ0 [K%t+1"τ +1)] (x, x0) ∙ Π('+ι,t+ι,tz+ι)(x, x0).
For t0 - t = τ the recursion continues until it reaches Π(L,T,t“)(x, x0),t” < TT or
Π(L"tO)(X, x0),tt < T and as a result based on (33) we get
Π('")(x, x-)=0	t0 -1 = τ	(34)
For t0 - t = τ it leads to Π(L,t,t')(x, x0) and has a non-zero value.
Now we derive RNTK for multi-layer:
P f (T) v f(	Z)i_	X / ∂fθ (x)	∂fθ (x-)	∖	+ X	/ ∂fθ (x)	∂fθ (x-) ∖
h θfθ( ), θfθ())∂W ∂ ∂W('),	∂W(')	/	+	∖	∂U(') ,	∂U(') /
'=ι '	/	'=ι	'	/
,X / ∂fθ(x) ∂fθ(x0) ∖
+ ⅛ ∖ ∂b(') , ∂b(') / +
∂fθ (x) ∂fθ (x0)
∂v ,	∂v
where
/ fx, f⅛ ∖ = X X (1(淤t)(x), δ",t' )(x-)E) ∙ (S Dh(KtT)(x), h("-I)(XT)E)
∖∂W⑶，∂W⑶ /总 士 ∖n ∖	( ),	( )/) ∖ n ∖	( ),	( )/)
/ 存 % ∖ =工 X (n "W, 淤')(x G (q hxt, xtoi)	'=1
/ ∂fθ (x) ∂fθ (x0)
∖ ∂U(') , ∂U(')
T 0 T L / 一
XX (1B”(x),淤t0)(x')
t0=ι t=ι L '
(M'τ,t)(x), h('T，t0)
'> 1
∖	T0	T / 一	、
)=XX G Wt)(X)淤，)(x-)E)∙(σ')2
/ ∂fθ (x) ∂fθ (x0)
∖ ∂ b⑶,∂b⑶
fX)∖ = (σ2 Dh(T)(x), h(T0)(xτ)
∂ V	/ n n \
21
Published as a conference paper at ICLR 2021
Summing up all the terms and replacing the inner product of vectors with their expectations we get
(LTT	∖
XXX
Π(',t,t0)(x, x0) ∙ Σ(',t,t0)(x, x0) I + K(x, x0).
'=1 t=1 t0 = 1	)
By (34), we can simplify to
Θ(L,T,TO)= (XXΠ(',t,t0)(x, x0) ∙ Σ(',t,t+τ)(x, χ0) J + K(χ, χ0).
V=1 t=1	)
For multi-dimensional output it becomes
Θ(x, x0) = Θ(L,τ,τ )(x, x0) 0 Id.
This concludes the proof for Theorem 1 for the multi-layer case.
B.4 Proof for Theorem 3: Weight-Untied RNTK
The architecture of a weight-untied single layer RNN is
g⑴(x) = √σw W㈤h(I)(X) + √√u U㈤Xt + σbb㈤
h(t)(x) = φ g(t) (x)
fθ (x) = -√v= v>h(T )(x)
n
Where we use new weights at each time step and we index it by time. Like previous sections, we first
derive the forward pass kernels for two same length data X = {Xt}tT=1,X = {X0t0}tT0=1
2
Σ(t,t)(x, X0) = σWVφ[K(t,t)(x, X0)] + -U hxt, Xti + 琮.
wm
Σ(t,t0)(X,X0) = 0	t 6= t0
Since we are using same weight at the same time step, Σ(t,t)(X, X0) can be written as a function of the
previous kernel, which is exactly as the weight-tied RNN. However for different length, it becomes
zero as a consequence of using different weights, unlike weight-tied which has non-zero value. The
kernel of the first time step and output is also the same as weight-tied RNN. For the gradients we
have:
δ(T) (X) = -vv	φ0 (g(T) (X))
δ⑴(x) = √σw(W(t+1))> (φ0(g㈤(x)) Θ 6(t+I)(X))	t ∈ [T — 1]
For t0 = t we have:
Π(t,t) (X, X0) =-w2 Vφ0[K(t+1,t+1)(X,X0)]Π(t+1,t+1)(X,X0)
Π(t,t)(X,X0)=-v2Vφ0[K(T+1,T+τ+1)(X,X0)].
Due to using different weights for t 6= t0, we can immediately conclude that Π(t,t0) (X, X0) = 0. This
set of calculation is exactly the same as the weight-tied case when τ = T - T = 0.
Finally, with θ = Vect[{{W(t), U(t), b(t)}tT=1, v}] we have
(Vθ fθ (x), Vθ fθ(X0)i
X / ∂fθ (x) ∂fθ (x0) ∖ + X / ∂fθ (x)
⅛∖IdW,1W)∣ ⅛∖,
∂fθ(X0) \
∂ U⑴/
t=1
∂fθ(X0)∖ i ∕∂fθ(X) ∂fθ(x0) \
∂b(t) / ∖ ∂v ， ∂v ，/
22
Published as a conference paper at ICLR 2021
with
/f (X) fθ(XO)∖ = (1 Dδ(t)(x) δ(t)(x0)E∖ Jσ2 Dh(t-i)(x)h(t-i)(xo)
∖∂W(t),∂ W(t) / = Vn ∖	(X),δ (X )/) k n ∖	(X), h (X )
/ d∂≡," ∖ = (1 Dδ(t)(X), δ(t)(X0)E)∙ (σ2 hXt, Xti)
∕∂fθ (X)
∂ ∂b㈤,
/ ∂fθ (X)
∖ ∂v ,
∂fθ (x0)
∂b㈤
∂fθ (x0)
∂v
δ(t) (X), δ(t) (X0)
h(T) (X), h(T0) (X0)
As a result we obtain
(Vθfθ(x), VθfθIXIO)) = (XX∏(t,t)(X, X0) ∙ ∑(t,t)(X0, X0)) + K(x, X0),
same as the weight-tied RNN when τ = 0. This concludes the proof for Theorem 3.
B.5 ANALYTICAL FORMULA FOR Vφ [K]
For any positive definite matrix K
K1	K3
K3	K2
we have:
•	φ = ReLU (Cho & Saul, 2009)
Vφ[K] = 2- ^c(π — arccos(c)) + pl - c2)) pK1K2,
Vφo[K ] = ɪ(- — arccos(c)).
where C = K3/ √K1K2
•	φ = erf (Neal, 1995)
Vφ [K]
2
—arcsin
π
Vφ0 [K]
P(I + 2K1)(I + 2K3)1 ,
4
∏√(1 + 2Kι)(1 + 2K2) - 4K32
C Proof for Theorem 2: RNTK Convergence after Training
To prove theorem 2, we use the strategy used in (Lee et al., 2019) which relies on the the local
Iipschitzness of the network Jacobian J(θ, X) = Vθfθ(X) ∈ RlXld×lθl at initialization.
Definition 1 The Jacobian ofa neural network is local Iipschitz atNTK initialization (θo 〜 N (0, 1))
if there is constant K > 0 for every C such that
kJ(θ,X)kF<K
[kJ(θ,X) — J(θ,X)kF <Kkθ — θk
~ .
∀ θ,θ ∈ B(θo,R)
where
B(θ,R):= {θ : ∣∣θo — θk < R}.
Theorem 4 Assume that the network Jacobian is local lipschitz with high probability and the
empirical NTK of the network converges in probability at initialization and it is positive definite
over the input set. For > 0, there exists N such that for n > N when applying gradient flow with
η < 2 (λmin (Θ(X, X)) + λmax(Θ(X, X))-1 with probability at least (1 — ) we have:
Sup⅜θ0k，S *upkθs(X，X) 一 θO(X，X)k
Proof: See (Lee et al., 2019)
23
Published as a conference paper at ICLR 2021
Theorem 4 holds for any network architecture and any cost function and it was used in (Lee et al.,
2019) to show the stability of NTK for MLP during training.
Here we extend the results for RNTK by proving that the Jacobian of a multi-layer RNN under NTK
initialization is local lipschitz with high probability.
To prove it, first, we prove that for any two points θ, θ ∈ B(θ0 , R) there exists constant K1 such that
kg(',t)(x)k2, kδ(',t)(x)k2 ≤ Kι√n	(35)
kg(M(X)-g(M(X)k2,kδ(M(X)-δ(',t)(x)k2 ≤ M-皿 ≤ Kι√nkθ - θk.	(36)
To prove (35) and (36) we use the following lemmas.3
Lemma 1 Let A ∈ Rn×m be a random matrix whose entries are independent standard normal
random variables. Then for every t ≥ 0, with probability at least 1 - e(-ct2 ) for some constant c we
have:
IlAIl2 ≤ √m+√n+1.
Lemma 2 Let a ∈ Rn be a random vector whose entries are independent standard normal random
variables. Then for every t ≥ 0, with probability at least 1 - e(-ct2) for some constant c we have:
Ilall2 ≤ √n+√t.
Setting t = √n for any θ ∈ R(θ0, R). With high probability, We get:
∣W(')∣2, ∣U(')∣2 ≤ 3√n,	kb'∣2 ≤ 2√n,	kh(',0)(x)∣2 ≤ 2σh√n.
We also assume that there exists some finite constant C such that
∣φ(χ)∣ < C|x|,	∣Φ(χ) — φ(χ0)| < C|x — χ0∣,	∣Φ0(x)∣ < C, , ∣φ0(x) — φ0(χ0)∣ < C|x — χ0∣.
The proof is obtained by induction. From noW on assume that all inequalities in (35) and (36) holds
With some k for the previous layers. We have
σ'	σ'
kg(',t)(x)k2 = k-WW⑶h('，tT)(X) + -UU⑶ h('-1,t)(x) + σ'b(')∣2
nn
≤ √wnkW(')k2kφ (g(',tτ)(x)) k2 + √√n∣U(')∣2kΦ (g('τ,t)(x)) k2 + σ'kb(')∣2
≤ (3σWCk + 3σUCk + 2σb) √n.
And the proof for (35) and (36) is completed by shoWing that the first layer is bounded
kg(1,I)(X)I∣2 = kσw= W(')h(1,0)(x) + £U(E) X1 + σ1b⑴∣∣2
nm
≤ (3σWσh +---U= ∣∣x1∣∣2 + 2σb)√n.
m
For the gradient of first layer We have
Iδ(L,T)(X)I2 = Iσvv	φ0(g(L,T)(X))I2
≤ σvkvk2kΦ0(g(L,T)(X))k∞
=2σv C√n.
And similarly We have
∣∣δ(',t)(X)k ≤ (3σwCk0 + 3σuCk0) √n.
3See math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf for proofs
24
Published as a conference paper at ICLR 2021
一 ≈ _ ，- ______ 一
For θ,θ ∈ B(θ0, R) We have
l|g(1,1)(x) -g(Ij)(X)k2 = Il
I) - W⑴)h(I，0)(x) + σu= (U(1) - U(I))h(1，0)(x)|2
√m
≤ (3σWσh +-— ∣∣xl∣∣2) ∣∣θ - θ∣∣2√n.
∖	mJ
∣∣g%t)(x)-g%t)(χ)∣∣2 ≤ ∣∣φ(g%tT)(X))II2∣&(W(') - W(力心
√n
σ'	〜/八	..、
+ I* W 叫2∣φ(aT)(X))- φ(∕tT)(X))∣2
√n
,…、	σ'	,八 …、
+ kφ(g('τ")(χ))∣2k%(Ue)- UC))Il2
√n
σ'
+ I-— U⑶∣2∣φ(gCT")(x)) - φ(g('T")(x))∣2 + σb∣b⑶一b⑶∣
√n
≤ (kσ' + 3σ'Ck + kσ— + 3σ— Ck + σ⅛)∣θ — θ*∣2√n∙
For gradients We have
M(L①)(x) - SMT)(χ)∣2 ≤ σιυkφ,(g(L^T))∣∞∣(v - v)∣2 + σ∣v∣2∣Φ'(g(L,T)(χ)) - Φ,(g(L,τ)(χ))∣2
≤ (σvC + 2σvCk)∣θ — θ∣2√n∙
And similarly using same techniques We have
M(M(X)-淤t)(x)∣2 ≤ (σwC + 3σwCk + σuC + 3σuCk)∣θ - θ∣2√n.
As a result, there exists Ki that is a function of σw,σu, σb, L, T and the norm of the inputs.
NoW We prove the local Lipchitzness of the Jacobian
LT
IlJ(θ,X)IlF ≤XX
δ(',t)(X)卜'h('，tT)(X))T
+ 1	δ(M(X) (σU "T) (x))t J √1n ∣∣δ(',t)(X) ∙ σ'∣∣j
+ X (I]Wt)(X) (σ!h(S)(X))TL
+焉 ∣wi,'T(x) a Xt)T∣∣f+√n W1" ∙ σiL)+√n M(L,τ)(X)IF
LT
≤ (XX(K2Cσ' + K2CσU + σ'K1)
` '=2 t=1
T
+ X(K2Cσ1
t=1
Km e12+σ1K1)+σvCK1).
+
25
Published as a conference paper at ICLR 2021
_ ~ _ , _ _.
And for θ,θ ∈ B(θ0, R) We have
LT /一	丁	丁
IlJ(θ, X)- J(θ, x)∣∣F ≤ XX (1 加)(x) (σWh(D(x))-淤t)(x) (σWh(。。⑺)
'=2 t=1 ×n
F
+ 1 6(e，t)(x) (σjh('，tT)(X))T -并，t)(x) (σjh(KtT)(X))T
+√n∣Mt)(χ)∙ σ -淤t) (χ)∙阊尸
+ X(n 卜1,t)(x) (σWh(1,tT)(X))T- Sat)(X)卜Wh(1,tT)(X))T
+√= M(I,tT)(X)㈤ Xt)T-件T)(X)㈤ Xt)TllF
+ √n l严)(X) ∙ σ -淤t)(X) ∙ σ'∣∣ ) + √ M(L①)(X) - h(Lm(X)IlF
LT
≤ ( X X(4K2S + 4衣2。。' + 播1)
∖ '=2 t=1
T
+ X(4K2C⅛
t=1
+ Kτ⅛ kXtk2+σ1K1)+σv CK1)kθ - θk2∙
The above proof can be generalized to the entire dataset by a straightforward application of the union
bound. This concludes the proof for Theorem 2.
26