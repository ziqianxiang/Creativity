Published as a conference paper at ICLR 2021
On Dyadic Fairness: Exploring and Mitigating
Bias in Graph Connections
Peizhao Li1, Yifei Wang1, Han Zhao2, Pengyu Hong1, Hongfu Liu1
1Brandeis University, 2University of Illinois at Urbana-Champaign
{peizhaoli,yifeiwang,hongpeng,hongfuliu}@brandeis.edu
hanzhao@illinois.edu
Ab stract
Disparate impact has raised serious concerns in machine learning applications and
its societal impacts. In response to the need of mitigating discrimination, fairness
has been regarded as a crucial property in algorithmic designs. In this work,
we study the problem of disparate impact on graph-structured data. Specifically,
we focus on dyadic fairness, which articulates a fairness concept that a predictive
relationship between two instances should be independent of the sensitive attributes.
Based on this, we theoretically relate the graph connections to dyadic fairness on
link predictive scores in learning graph neural networks, and reveal that regulating
weights on existing edges in a graph contributes to dyadic fairness conditionally.
Subsequently, we propose our algorithm, FairAdj, to empirically learn a fair
adjacency matrix with proper graph structural constraints for fair link prediction,
and in the meanwhile preserve predictive accuracy as much as possible. Empirical
validation demonstrates that our method delivers effective dyadic fairness in terms
of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.
1	Introduction
The scale of graph-structured data has grown explosively across disciplines (e.g., social networks,
telecommunication networks, and citation networks), calling for robust computational techniques to
model, discover, and extract complex structural patterns hidden in big graph data. Research work has
been proposed for inference learning on potential connections (Liben-Nowell & Kleinberg, 2007), and
corresponding algorithms can be used for high-quality link prediction and recommendations (Adamic
& Adar, 2003; Sarwar et al., 2001; Qi et al., 2006). In this work, we study the potential disparate
impact in the prediction of dyadic relationships between two instances within a homogeneous graph.
Despite the wide applications of link prediction algorithms, serious concerns raised by disparate
impact (Angwin et al., 2016; Barocas & Selbst, 2016; Bose & Hamilton, 2019a; Liao et al., 2020)
should also be reckoned with by algorithm designers. In an algorithmic context, disparate impact
often describes the disparity in influential decisions which essentially derives from the characteristics
protected by anti-discrimination laws or social norms. Unfortunately, this negative impact derived
from biased data and conventional algorithms occurs in many applications including link prediction.
One example is that a user recommender system follows the proximity principle (individuals are
more likely to interact with similar individuals) or existing connections with intrinsic bias. Such
an operating mode would deliver biased recommendations dominated by sensitive attributes. For
example, users with the same religion or ethnic group are more likely to be recommended to a user,
and consequently generate segregation in social relations by long-term accumulation (Hofstra et al.,
2017). Another example can be noticed in news streaming. When a news app has collected the
political profile from a user, in pursuit of the user preference in news streaming, the system might
only deliver politicking that the user is predisposed to agree with, therefore skews a user’s scope and
narrows the view by selectively displaying reality (Pariser, 2011). To alleviate these concerns, an
algorithm should perform a link prediction without being biased by the sensitive attribute of the two
instances, and should also stream diverse and preferred recommendations.
Motivated by the potential bias in real cases, in this paper we propose dyadic fairness for the link
prediction problem in homogeneous graphs, where the dyadic fairness criterion expects the predictions
1
Published as a conference paper at ICLR 2021
to be statistically independent of the sensitive attributes from the given two vertices. We focus our
scope on Graph Neural Networks (GNNs), which have already shown remarkable capacity in graph
representation learning by message passing along the graph structure (Xu et al., 2018; 2020; Ying
et al., 2018; Wang et al., 2019; Fan et al., 2019; Li et al., 2020). Within the pipeline of GNNs, given
an arbitrary graph, we theoretically analyze the relationship between dyadic fairness and the graph
connections. Our findings suggest adapting weights on existing edges in a graph can contribute to
dyadic fairness conditionally. Continuing with our theoretical findings, we propose FairAdj, an
algorithm to empirically learn a fair adjacency matrix by updating the normalized adjacency matrix
while keeping the original graph structure unchanged. Integrating with a utility objective function,
the proposed algorithm seeks supplied dyadic fairness and link predictive utility simultaneously.
Our definition of dyadic fairness in a graph context is inspired by the statistical metrics in group
fairness (Dwork et al., 2012; Kusner et al., 2017). First, vertices in a graph are categorized into several
groups according to a protected attribute. Then, the dyadic fairness criterion asks some standard
statistics such as positive outcomes or false positive rate on link score to be approximately equalized
across intra and inter groups. Essentially, such a requirement asks for a more diverse prediction
between and within different groups defined by the protected attribute, hence it also allows to mitigate
social segregation by asking for more interactions across different protected groups in the graph.
Empirically, we present studies on six real-world social and citation networks to demonstrate the
effectiveness of the proposed method. We conduct evaluations towards seven measurements of
both utility and dyadic fairness. Comparing to other baseline methods (Kipf & Welling, 2016b;
Grover & Leskovec, 2016; Rahman et al., 2019; Bose & Hamilton, 2019b), we consistently observe
improvements from two aspects. First, dyadic fairness metrics verify that our method can minimize
the statistical gap between the predictions of intra and inter links. Second, in terms of utility, our
results are consistent with the existing literature (Zhao & Gordon, 2019; Fish et al., 2016; Calders
et al., 2009), that satisfying fairness can potentially lead to a decrease in utility. However, our
algorithm enjoys a more favorable fairness-utility tradeoff (same in fairness but less sacrifice in utility,
and vice versa) when compared to previous works. Additionally, to approach the real application
cases, we also showcase a direct product that comes from dyadic fairness: our method can effectively
stream more diverse recommendations containing instances holding different kinds of sensitive
attributes.
2	Related Work
In this section we mainly review some closely related work in both fair machine learning and graph
representation learning. We also briefly describe and discuss several existing works on learning fair
node representations.
Fair Machine Learning. Various types of fairness notions have been proposed and studied, in-
cluding group fairness (Kusner et al., 2017; Kearns et al., 2018; 2019), individual fairness (Dwork
et al., 2012), and preference-based notions (Zafar et al., 2017a; Ustun et al., 2019). Embracing these
definitions, relevant algorithms involving fair constraints have been proposed. Zemel et al. (2013)
propose a method to find a good representation to maximize utility while preserving both group
and individual fairness. Following works on fair representation learning use autoencoder (Madras
et al., 2018) or adversarial training (Zhao & Gordon, 2019; Zhao et al., 2019; Edwards & Storkey,
2015; Louizos et al., 2016) to simultaneously remove the sensitive patterns while preserving enough
information for prediction. Zafar et al. (2017b) optimize for decision boundary fairness through
regularization in logistic regression and support vector machines, and some other works achieve
fairness by optimal transport between sensitive groups (Gordaliza et al., 2019; Jiang et al., 2019) and
fair kernel methods (Donini et al., 2018). However, most proposed learning algorithms for fairness
are mainly built on independent and identically distributed data, which are not suitable to be directly
applied to graph-structured data with dyadic fairness.
Graph Representation Learning. Representation learning on graphs is formulated to convert a
structural graph into a low-dimensional space while preserving the discriminative and structural
representations. Efficient graph analytic methods (Von Luxburg, 2007; Tang et al., 2015; Perozzi
et al., 2014; Grover & Leskovec, 2016; Xu et al., 2019) can benefit a series of downstream appli-
cations including node classification (Wang et al., 2017), node clustering (Nie et al., 2017), link
2
Published as a conference paper at ICLR 2021
prediction (Zhang & Chen, 2018) and graph classification as well. Recently, Graph Neural Net-
works (GNNs) have shown remarkable capacity in graph representation learning, with emergent
varieties (KiPf & Welling, 2016a; Velickovic et al., 2017; Hamilton et al., 2017) consistently deliver-
ing promising results. Our work uses GNNs for graph representation learning but targets improving
dyadic fairness in link Prediction.
Fair Graph Embedding. As fairness in graPh-structured data a relatively new toPic for research,
only a few studies have investigated the fair issues in graPh rePresentation learning. Rahman et al.
(2019) first ProPosed Fairwalk, a random walk based graPh embedding method that revises the
transition Probability according to the vertex’s sensitive attributes. Following the idea of adversarially
removing sensitive Patterns (Madras et al., 2018), Liao et al. (2020) ProPosed to use adversarial
training on vertex rePresentations to minimize the marginal discrePancy. This work mainly focuses
on learning node rePresentations that are free of sensitive attributes, which is different from ours.
Other works includes fair collaborative filtering (Yao & Huang, 2017), item recommendation (Steck,
2018; Chakraborty et al., 2019) in biPartite graPhs, and fair graPh covering Problem (Rahmattalabi
et al., 2019).
3	Preliminaries
Let G := (V, E) as a graPh with a fix set of vertices V and edges E, where vertex features with M
dimensions are rePresented by X ∈ RN ×M . A nonnegative adjacency matrix A ∈ RN ×N describes
the relations between every Pair of vertices. The element avu in A rePresents the weight on the
linkage bridging v and u, and is set to zero if no link exists. Every vertex holds a sensitive attribute,
and we use S(v) to denotes the sensitive attribute as well as the sensitive grouP membershiP of v. Let
Γ(v) be the set of 1-hoP neighbors ofv including self-looP. Edge (v, u) is called intra if S(v) = S(u),
and inter imPlies S(v) 6= S(u). |S| denotes the cardinality of grouP S. For a binary sensitive attribute
with two groups So and Si separated from the graph, So := {v ∈ So | Γ(v) ∩ S∖ = 0} represents
the set of vertices in S0 which locate on the boundary and has connections with S1 , and the same
for S1 . Set U to be the discrete uniform distribution over the set of vertices V. Suppose a bivariate
link prediction function g(∙, ∙) : RD X RD → R, that given two vectors of the embedded vertices
representations, a value is obtained showing the model belief that these two vertices are potentially
linked.
Having these basic notations, we consider the disparity in link prediction bridging on intra and inter
sensitive groups. The general purpose of dyadic fairness is to predict links independently of whether
two vertices having the same sensitive attribute or not. We extend from demographic parity (Edwards
& Storkey, 2015; Kipf & Welling, 2016b; Madras et al., 2018; Zemel et al., 2013) to formulate a
specific criteria for dyadic fairness. In a binary classification problem, demographic parity expects a
classifier gives positive outcomes to two sensitive groups at the same rate. We turn the two groups in
the content of demographic parity into the groups of intra and inter links. Ideally, achieving dyadic
fairness will bring intra and inter link predictions at the same rate from a bag of candidate links.
Having vertices representation v and u, dyadic fairness can be mathematically formulated as
Definition 3.1. A link prediction algorithm satisfies dyadic fairness if the predictive score satisfy
Pr(g(u, v)|S(u) = S(v)) = Pr(g(u, v)|S(u) 6= S(v))	(1)
To quantify the fairness, we establish dyadic fairness on link prediction upon a fixed set of vertices,
and models the expectation of absolute difference in score outcome across the groups of intra and
inter links. Note that we also comprehensively evaluate our model on fairness by other four statistics
gap extended from (Hardt et al., 2016) in Section 6.
4	How Graph Connections Affect Fairnes s
In this section, we propose a chain of theoretical analyses1 established on a variant of demographic
parity and graph neural networks to associate dyadic fairness with graph connections. We first
1Proofs for the proposition and theorem are in Appendix A.
3
Published as a conference paper at ICLR 2021
demonstrate demographic parity in the outcomes of link prediction can be sufficiently reduced to
the achievement of fair vertex representations when employing an inner product function for link
prediction. Suggested by the sufficiency, we reveal how the pipeline of a one-layer graph neural
network can affect the demographic parity, and draw the conclusion that for an arbitrary graph using
GNNs for embedding, properly regulating the weights on existing graph connections can contribute
to fairness conditionally. The theoretical findings motivate our algorithmic design as presented in
the next section. Without loss of generality, in this section, we consider the sensitive attribute to be
binary, where two sensitive groups S0 and S1 can be separated from the graph, but show the cases
with sensitive attributes in multiple categorical values in our experimental section.
Proposition 4.1. For a link prediction function g(∙, ∙) modeled as inner product g(v, U) = v>Σu,
where Σ ∈ S+M+ is a positive-definite matrix, ∃Q > 0, ∀v 〜V, ∣∣v∣∣2 ≤ Q, for Ev〜U[v] ∈ RM, for
dyadic fairness based on demographic parity, if ∣∣Ev〜U[v | V ∈ So] - Ev〜U[v | V ∈ S1]∣∣2 ≤ δ,
δDP ：= lE(v,u)〜u×u[g(V)U) | S(V) = S(U)] - E(v,u)〜u×u[g(V)U) | S(V)= S(U)]| ≤ Qllς∣∣2 ∙ δ.
(2)
Remark 1. Proposition 4.1 can be applied for a general inner product function in Euclidean space,
where Σ directionally and differently scales two input vectors. When setting Σ to an identity matrix,
function g(∙, ∙) reduces to dot product and is widely used in a series of research work on link
prediction (Kipf & Welling, 2016b; Trouillon et al., 2016; Yao & Huang, 2017).
The above proposition implies fair vertex representations is a sufficient condition to achieve demo-
graphic parity in link prediction. Suggested by the sufficiency, the approach to fairness could be
reduced to achieving vertex representations with a small discrepancy between sensitive groups. With
the proposition, we are ready to proceed to understand fairness within graph neural networks and
reveal how the structure or connections ofa graph could affect demographic parity.
A single layer GNN can be generically written as GNNθ(X, A) := ρ (AXWθ), where ρ is a non-
linear activation function, A is the normalized adjacency matrix, and Wθ is the trainable weight
matrix. One GNN layer can be decomposed into two disjoint phases: a vertex feature smoothing phase
over the graph using A, and a feature embedding phase using Wθ and ρ. Concretely, we consider left
1
normalization A = D-1A (D is the degree matrix) for feature smoothing. Equivalently, at an indi-
vidual level, for each vertex it is one-hop mean-aggregation Agg(V) := degw (V)-1 Pu∈Γ(v) avuU,
where degw(V) := Pu∈Γ(v) avu stands for the weighted degree of vertex V.
We respectively abbreviate Ev〜U[v|v ∈ So] and Ev〜U[v|v ∈ Si] as μo and μι. Let σ denotes the
maximal deviation of vertex representations, namely, ∀v ∈ So, ∣v - μo∣∣∞ ≤ σ, and ∀v ∈ Si,
∣∣v - μι∣∞ ≤ σ. Let DmaX ：= maxv∈vdegw(V) be the maximal weighted degree in G, mw :=
PS(v)6=S(u) avu be the summation of weights on inter links. With these notations, we show how the
discrepancy between μo and μι changes after conducting feature smoothing for one time over the
graph.
Theorem 4.1. For an arbitrary graph with nonnegative link weights, after conducting one mean-
aggregation over the graph, the consequent representation discrepancy between two sensitive groups
∆APgr := ∣Ev〜U[Agg(V) | V ∈ So] - Ev〜U[Agg(V) | V ∈ S1]∣∣2 is bounded by
max{aminkμ0 - μι k∞ - 2σ, 0} ≤ AAPgr ≤ αmaχkμ0 - μi k2 + 2VMσ,	⑶
ff
where αmin=min{αi, α2}
, αmax=maχ{α1 ,ɑ2}, α1=l1-Dmwx(|S0| + |S1| )|, α2=l1-图-图|.
Remark 2. Theorem 4.1 shows the lower and upper bound given by the graph structure and the
maximum deviation σ of vertex representations in each sensitive group on demographic parity after
conducting one aggregation function on vertices. The contraction coefficient αmax is a maximum of
two absolute terms αi and α2, where α2 is a constant predetermined by the graph connections. It
is worth pointing out that although in the worst case αmax could be 1, e.g., in a complete bipartite
graph, in most practical graphs it is strictly less than 1, hence the above upper bound corresponds
to a contraction lemma but under some additional error introduced by deviation σ . We also provide
several illustrative graph diagrams for the contraction of this theorem in Appendix B.
To approximate demographic parity after feature smoothing, Theorem 4.1 inspires a strategy to
regulate the weights on graph connections to change αi in αmax so as to minimize the upper bound
4
Published as a conference paper at ICLR 2021
Algorithm 1: Algorithmic routine for FairAdj
Input: vertex features X, adjacency matrix A, GNNs parameters θ, learning rates ηθ and ηAe
Normalize adjacency matrix A J D-1A
Fix the elements with zero in A and select the non-zero elements for optimization
while θ or Ae has not converged do
for t = 1 to T1 . optimize for utility
do
L Compute Lutii by Eq.(5), gθ J JLutii, θ J θ + ηθ ∙ Adam(θ, gθ)
for t = 1 to T2 . optimize for fairness
do
Z J GNNθ(X,A), A J ZZ> . reconstruct graph connections
ComPUteLfair by Eq.(6), g^ J V^Lfair
for v = 1 to N . projected gradient descent
do
Sort n non-zero elements in [A — ηj4gj4]v,* in descending order: eι ≥ e2 ≥ … 已几
Y J Pn=I 1(ej + 1 (1 — Pj=ι ei) ≥ 0) . 1(∙): the indicator function
β J P(I - Pγ=l ei)
for u = 1 to n . update Ae
do
Lr Tl	Γ Γ T	1	, C cl
[A]v,u J max{[A — η3gA‰u + β, 0}
Output: Link predictive score between vertex v and u J sigmoid(GNNθ (v, Ae)>GNNθ (u, Ae))
for one-iayer mean-aggregation. For term α1, obviousiy if the summation of inter weights are too
small (mw → 0) or too large (mw → Dw ∙ min{∣So∣, ∣Sι∣}), αι will approximate to 1. This indicates
that increasing the weights on inter iinks cannot aiways guarantee to achieve a better demographic
parity although inter group connections are always the minority in links, but should regulate this part
to a proper range depending on the size of sensitive groups and the connected situation of a graph.
We combine the upper bound with the second feature embedding phase. Here We denote μi :=
Ev〜U[GNNθ(v, A) | V ∈ Si], i = 0,1, Q = sup{∣∣GNNθ(v, A)k2 | V ∈ V}.
Corollary 4.1. For ∆DP on vertices after passing one layer GNNθ(X, A) = ρ (AXWθ), we have:
δDP ≤ QOllς∣∣2 ∙kμ0 - μ1k2 ≤ ql2∣∣ς∣∣2∣∣Wθ∣∣2 ∙ (α∣∣μo - μιk2 + 2√Mσ),	(4)
where L is the Lipschitz constant for ρ. The first inequality holds by Proposition 4.1, the second
one is by Theorem 4.1 and the definition of spectral norm and Lipschitz constant, also realizing that
Q0 ≤ L∣Wθ∣2Q. Multiple layers of GNNs can be reasoned out similarly.
From the above theorem, we see ∆DP can be processed with a tighter upper bound by regulating the
weights on edges, and it is also dependent on the property of Wθ, ρ, and the error term O(σ). When
setting Wθ fixed, our theoretical findings provide a feasible solution that regulating weights on graph
connections can achieve better demographic parity on link prediction, and as supplementary, also
indicate where or when it cannot perform well with finite layers of GNNs: (1) The solution cannot
guarantee arbitrary fairness if we want to preserve the graph structure due to the resistance of α1 in
lower bound in Theorem 4.1. (2) When it is already fair enough in the original graph data, which
means ∣∣μo — μι ∣2 is small and additional error O(σ) is comparable to it, the upper bound cannot
be reduced significantly and the solution may not further mitigate the bias. We include a dataset to
investigate the potential limitations empirically in response to the above analysis in Appendix D. In
the following sections, we implement the inspired algorithm and demonstrate that multiple real-world
networks accept this solution with favorable results and a better fairness-utility tradeoff.
5
Published as a conference paper at ICLR 2021
5 Learning Fair Graph Connections
The above discussion indicates that when employing GNNs for graph embedding, adjusting the
adjacency matrix can assist the model with achieving fairness conditionally. However, searching
for the optimal adjacency matrix within hierarchical graph neural networks is a non-trivial problem.
In this section, continuing with the preceding analysis, we develop FairAdj algorithm to adjust the
graph connections and learn a fair adjacency matrix by updating Ae while preserving the original
graph structure unchanged. In overview, we implement the algorithm by separately optimizing the
parameter Wθ of GNNs towards utility, and adjusting A towards dyadic fairness by gradient descent
and empirical risk minimization with structural and right stochastic constraints. Therefore, FairAdj
is able to pursue the supplied dyadic fairness and link predictive utility simultaneously.
We employ variational graph autoencoder (Kipf & Welling, 2016b) for feature embedding. A
two-layer graph neural network is used as inference model GNNθ(∙, ∙). Z denotes the embedded
representations, and dot product between embedded representations is the generative model p(∙). The
KL-divergence term KL[∙∣∣∙] punishes the discrepancy between latent distribution and a Gaussian
prior. The objective function to reconstruct graph connections from latent variable can be written as:
max	Lutil ：= Egnnθ(z∣χ,A)[logP(A I Z)] - KL[GNNθ(Z | X,A)∣∣N(0,1)].	(5)
For fairness, we impel Lfair to empirically seek for better graph connections, then update Ae with
constraints. Specifically, we optimize the normalized adjacency matrix A as follows:
min	Lfair := ∣∣Ev,u 〜U×U [avu | S(V)= S(U)] - Ev,u 〜U×U IavU | S(V)= S(U)]k k,
Ae
,.	.≈,	一.一「.\	_	,. .	≈ ,	T _
s.t. (1).	[A]vu	= 0, if [A]vu	= 0,	(2). AI	= 1, A	≥ 0,
(6)
where avu takes value in A = ZZ> and 1 is the all-one vector with size N. The two constraints
are necessary for optimizing A: (1) Elements with zero value should be maintained, meaning no
new links can be established during optimization. This restriction is proposed to preserve utility, due
to adding fictitious links might mislead the directions of message passing, and further corrupt the
representation learning. Therefore, we only adapt weights on existing edges and preserve the original
graph structure. (2) In consistent with the initial left normalization Ae = D-1A, the optimized matrix
should still remain a right stochastic matrix. This can restrict the largest eigenvalue of adjacency
matrix kAek2 = 1, hence avoid numerical instabilities and exploding gradients when training Wθ
towards Lutil. In practice, we observe explosions during training with no constraints applied on Ae.
For the first constraint in Eq. (6), we only compute gradients and update the elements in A which
have non-zero initialization. For the second one, after selecting the variable to optimize, we employ
projected gradient descent by (Wang & Carreira-Perpindn, 2013) to satisfy the constraint while
minimizing Lfai1∙.OnCe given computed gradients on A denoted as VjeLfair, with the corresponding
learning rate ηAe, we have the following optimization problem that update A by projecting A -
ηAeVAeLfair into the feasible region with the minimum Euclidean distance:
~ min	X k [N - ηχVALfair]v,* - [4 - (ηjVNLfair)0]v,* k2
Ae-(η Ae^ AeLfair)0	v	(J)
,≈	一	. ,. .	一 T ， 一	. ,	_
s.t. (A - (ηχVALfair) )1 = 1 and A - (ηχVjLfair) ≥ 0,
1	Γ ^ΛΛ	. t	∙ 1	. ∕' ^Λ	1 11.1	. ∙	^Λ / X—7 C ∖ I
where [A]v,* are the row-wise elements for A, namely, all the connections on v. A 一 (ηχVjeLfair)0
is the update for A after projection. Since A is row-wise independent and the objective function is
strictly convex for this quadratic program, there exists a unique solution for each row. Solution details
for projected gradient descent are restated as a part of our algorithmic pipeline.
The algorithmic routine is elaborated in Algorithm 1. θ and A are optimized iteratively with T1 and
T2 epochs for co-adaptation. Compared to adversarial training method on graph embedding (Bose
& Hamilton, 2019b), which requires a hyperparameter to control the fairness-utility tradeoff, we
also find regulating the convergence of A has a similar effect as well. That is because the more A
changes, the further Ae is away from the original graph connections, and consequently, the more
6
Published as a conference paper at ICLR 2021
Table 1: Statistic for datasets in experiments.							
Dataset	# Vertex	# Edge # Class # Intra		# Inter	Intra Ratio	Inter Ratio	Dis. Ratio
Oklahoma97	3,111	73,230	2	46,368	26,862	1.92e-2	1.11e-2	1.73
UNC28	4,018	65,287	2	36,212	29,075	8.76e-3	7.38e-3	1.19
Facebook#1684	786		14,024	2	7,989	6,035	4.76e-2	4.30e-2	1.11
Cora	2,708	5,278	7	4,275	1,003	6.51e-3	3.30e-4	19.73
Citeseer	3,312	4,660	6	2,089	2,571	2.13e-3	5.70e-4	3.74
Pubmed	19,717	44,327	3	33,443	10,884	4.80e-4	9.00e-5	5.33
Table 2: Experimental results on UNC28.							
Method	AUC ↑	AP ↑	△dp J	△true J	△false J	△FNR J	△TNR J
VGAE	87.63 ± 0.56	88.69 ± 0.65	2.24 ± 0.42	1.50 ± 0.41	0.44 ± 0.36	7.62 ± 0.84	2.18 ± 0.72
node2vec	87.22 ± 0.30	87.10 ± 0.37	2.75 ± 0.78	1.30 ± 0.53	1.05 ± 0.93	12.56 ± 1.12	2.24 ± 0.92
Fairwalk	87.18 ± 0.30	87.07 ± 0.37	2.79 ± 0.70	1.17 ± 0.49	0.90 ± 0.92	12.71 ± 1.11	2.20 ± 0.96
FairAdjT2=5	86.98 ± 0.54	87.75 ± 0.65	1.53 ± 0.35	0.32 ± 0.29	0.41 ± 0.35	2.84 ± 0.74	2.22 ± 0.68
FairAdjT2=20	87.04 ± 0.55	87.80 ± 0.65	1.57 ± 0.36	0.34 ± 0.31	0.42 ± 0.35	2.76 ± 0.75	2.16 ± 0.73
damage in utility but more enhancement in fairness. Thanks to this favorable property, in experiments,
comparing to adversarially remove sensitive attributes, we present multiple options for T2 to control
the convergence and observe a more favorable fairness-utility tradeoff shown by our method.
6	Experiments
We present empirical analysis on six real-world datasets, compared with baseline methods in terms of
seven evaluative metrics on both fairness and utility. Approaching to applications, we testify that our
method can enhance the diversity in recommendations. Due to the space limitation, we only showcase
partial results but defer the rest in Appendix D. Moreover, as an intermediate result, we demonstrate
vertex representations are embedded more fairly assessed by fair clustering in Appendix E.
6.1	Settings
Datasets. We conduct experiments on real-world social networks and citation networks including
Oklahoma97, UNC28 (Traud et al., 2011), Facebook#1684, Cora, Citeseer, and Pubmed. Okla-
homa97 and UNC28 are two school social networks. A link represents a friendship relation in
social media, and every user has a profile for vertex features, including student/faculty status, gender
(sensitive attribute), major, etc. Facebook#1684 is a social ego network from Facebook app. As
the rest three citation networks, each vertex represents an article with bag-of-words descriptions as
features. A link stands for a citation regardless the direction. We set the category of an article as the
sensitive attribute. Statistic for datasets are summarized in Table 1, where #Class is the number of
sensitive groups, #Intra/Inter Ratio represents the ratio that the number of actual intra/inter links v.s.
the number of links if the graph is fully connected. These two terms show the density of intra/inter
links. Dis. Ratio (abbreviated from disparity ratio) is calculated by divide intra ratio into inter ratio.
Dis. Ratio equaling to one implies intra/inter connections are perfectly balanced in existing graph,
and the degree of deviation from 1 indicates how skew the link connections are.
Baselines and Protocols. We involve four baseline methods. Variational graph autoencoder
(VGAE) (Kipf & Welling, 2016b) inherits from variational autoencoder, which uses two GNN-
layer as the inference model and leverages latent variables to reconstruct the graph connections.
Node2vec (Grover & Leskovec, 2016) is a widely used graph embedding approach based on random
walk. Fairwalk (Rahman et al., 2019) is built upon node2vec and designed specifically for fairness
issues. It modifies the transition probability for one vertex according to the sensitive attribute of its
neighbors. The last one is adversarial training on vertex representations (Bose & Hamilton, 2019b),
which aims to minimize the discrepancy between different sensitive groups by optimizing parameters
in GNN. Besides the standard pipeline for utility, it additionally trains the networks to confuse
a discriminator, meanwhile training the discriminator to distinguish the embedded features with
different sensitive attributes. A hyperparameter λ is used in the overall objective function to balance
7
Published as a conference paper at ICLR 2021
Table 3: Experimental results on Citeseer.
Method	AUC ↑	AP ↑	δDP Φ	△true J	△false J	△FNR J	△TNR J
VGAE	81.77 ± 1.23	85.57 ± 1.39	11.24 ± 1.83	3.37 ± 2.33	2.14 ± 1.26	10.81 ± 3.61	11.31 ± 2.99
node2vec	81.21 ± 1.35	84.69 ± 1.26	14.49 ± 3.38	4.02 ± 2.74	6.82 ± 4.62	7.37 ± 3.07	12.50 ± 4.21
Fairwalk	81.69 ± 1.50	84.97 ± 1.23	13.50 ± 2.97	3.30 ± 2.49	5.33 ± 3.83	7.26 ± 3.30	11.34 ± 3.19
FairAdjT2=2	80.45 ± 1.34	84.47 ± 1.43	9.57 ± 1.84	2.55 ± 2.02	1.70 ± 1.47	9.87 ± 3.17	10.27 ± 3.23
FairAdjT2=20	78.84 ± 1.38	82.74 ± 1.46	7.81 ± 1.80	1.85 ± 1.66	1.30 ± 1.41	9.22 ± 2.95	10.01 ± 3.13
Figure 1: Comparison with adversarial training method (Bose & Hamilton, 2019b) in terms of the
tradeoff between utility and fairness. Left: UNC28; Right: Citeseer. Blue points denote FairAdj
with different T2 values, red points represent (Bose & Hamilton, 2019b) with different λ values.
the tradeoff between utility and fairness. We vary λ in experiments and make comparisons to various
results given by adversarial training. For all experiments, we randomly remove 10% links from the
graph and reserve them for evaluation, and equivalently, the same number of false links are sampled
in the evaluation phase. For one dataset, we repeat experiments with different train/test splits for 20
times. Full experimental configurations are available in Appendix C.
Metrics. We evaluate the utility of link prediction using Area Under the Curve (AUC) and Average
Precision (AP). Fairness is evaluated towards ∆DP , as well as the disparity on the expected score on
all the true samples ∆true and false samples ∆false. Besides these, following the suggested fairness
notions (Hardt et al., 2016), we compute the maximum gap of true negative rate (TNR) and false
negative rate (FNR). Illustratively, let the conditional cumulative distribution function of score
R be evaluated by a threshold τ with a given label written as Fys(τ) := Pr(R≤τ |Y =y, S=s),
y∈{0, 1}, s∈{intra, inter}. With these notations, the maximum gap in true negative rate can be
expressed as Δtnr ：= max/∣Fntra(τ) - F0nter(τ)|, and similar for false positive rate Δfnr ：=
maxτ ∣F1ntra(τ) - FInter(T)|. These two terms also reflect the disparity in true positive rate (TPR)
and false positive rate (FPR) due to TPR = 1 - FNR and FPR = 1 - TNR.
6.2	Results and Analysis
Table 2 and 3 list quantitative results on UNC28 and Citeseer comparing to VGAE, node2vec, and
Fairwalk. Two choices of T2 are presented here, where T2 in a smaller value pursues a comparable
performance in utility (a little bit lower in AUC but higher in AP) to random walk based methods,
and at the same time performs much better in fairness. We also present T2 = 20 since we observe a
convergence on the adjacency matrix. The results indicate that FairAdj achieves the best on various
statistics for dyadic fairness and with only a small sacrifice in predictive utility.
A Better Tradeoff. Figure 1 plots every experimental result for our method and adversarial training
with various fairness-utility regulated hyperparameters. Two observations explain why our method
surpasses the adversarial technique. (1) Blue dots are closer to the left top corner than the red on the
whole, meaning the same level of fairness is achieved with less sacrifice in utility. An explanation for
this favorable property is that, the adversarial training method neglects the graph connections and
8
Published as a conference paper at ICLR 2021
EJ≡U- ∙ω.> .IQlU-
method
——VGAE
——FairAdj T2=5
---FairAdjTZ=IO
一 FairAdi T2=20
diversity
method
VGAE
FairAdj T2≡5
FairAdj T2=10
FairAdj T2=2O
0.10	0.15
proportion
diversity
method
FairAdj T2=5
FairAdj T2=10
FairAdi T2=20
method
VGAE
FairAdj T2≡5
FairAdj T2=10
FairAdj T2=20
Figure 2: Diversity and utility in recommendations. Left: UNC28; Right: Citeseer. X-axis ‘propor-
tion’ means we investigate the top x% valued links and check the ratio between inter and intra links
that presented in y-axis.
only diminish the group discrepancy, where two irrelevant instances with no connection but from
different groups may be closely mapped, thus greatly damage the utility. The optimization on A does
facilitate the feature smoothing across groups which is not indicated in the original adjacency matrix,
but still considers the graph connections. (2) Additionally, blue dots are more aggregated, suggesting
our methods escape from the instability of min-max optimization and acting more robust to different
train/test splits. However, as shown, FairAdj cannot achieve arbitrary small in ∆DP as red dots do.
This is indicated in Section 4 as the first potential limitation.
Diversity in Recommendations. We examine the top-scored links in evaluation at a certain pro-
portion in terms of its diversity and utility, shown in Figure 2. This exploration can be useful when
conducting recommendations according to scores in descending order. For a fixed proportion, we
report the diversity as the number of inter links divided by the number of intra links, and the utility
as the recall rate among these recommendations. Figures show that as a direct product by dyadic
fairness, FairAdj enhances the diversity in recommendations but is achieved at a sacrifice of utility.
7	Conclusion
We studied the dyadic fairness in graph-structured data. We theoretically analyzed how the con-
nections in graph links affect dyadic fairness of demographic parity when employing graph neural
networks for representation learning. On the basis of the foregoing analysis, we proposed FairAdj to
learn a fair adjacency matrix, and pursued the dyadic fairness and prediction utility simultaneously.
Empirical validations demonstrated the achievement of fairness and a better fairness-utility tradeoff.
Acknowledgement
We would like to thank Zizhang Chen and Wei Lu for the helpful discussions, and Lizi Liao for
providing the Oklahoma97/UNC28 datasets. This work is partially supported by NSF OAC 1920147.
9
Published as a conference paper at ICLR 2021
References
Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks, 25(3):211-230,
2003.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. ProPublica, May, 23:
2016, 2016.
Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.
Avishek Bose and William Hamilton. Compositional fairness constraints for graph embeddings. In
International Conference on Machine Learning, pp. 715-724. PMLR, 2019a.
Avishek Bose and William Hamilton. Compositional fairness constraints for graph embed-
dings. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learn-
ing Research, pp. 715-724, Long Beach, California, USA, 09-15 Jun 2019b. PMLR. URL
http://proceedings.mlr.press/v97/bose19a.html.
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency
constraints. In 2009 IEEE International Conference on Data Mining Workshops, pp. 13-18. IEEE,
2009.
Abhijnan Chakraborty, Gourab K Patro, Niloy Ganguly, Krishna P Gummadi, and Patrick Loiseau.
Equality of voice: Towards fair representation in crowdsourced top-k recommendations. In
Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 129-138, 2019.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil.
Empirical risk minimization under fairness constraints. In Advances in Neural Information
Processing Systems, pp. 2791-2801, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp.
214-226, 2012.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417-426, 2019.
Benjamin Fish, Jeremy Kun, and Addm D Lelkes. A confidence-based approach for balancing
fairness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining,
pp. 144-152. SIAM, 2016.
Paula Gordaliza, Eustasio Del Barrio, Gamboa Fabrice, and Jean-Michel Loubes. Obtaining fairness
using optimal transport theory. In International Conference on Machine Learning, pp. 2357-2365,
2019.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in neural information processing systems, pp. 1024-1034, 2017.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances
in neural information processing systems, pp. 3315-3323, 2016.
Bas Hofstra, Rense Corten, Frank Van Tubergen, and Nicole B Ellison. Sources of segregation in
social networks: A novel approach using facebook. American Sociological Review, 82(3):625-656,
2017.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. arXiv preprint arXiv:1907.12059, 2019.
10
Published as a conference paper at ICLR 2021
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering:
Auditing and learning for subgroup fairness. In International Conference on Machine Learning,
pp. 2564-2572, 2018.
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An empirical study of rich subgroup
fairness for machine learning. In Proceedings of the Conference on Fairness, Accountability, and
Transparency, pp. 100-109, 2019.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,
2016b.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in Neural Information Processing Systems, pp. 4066-4076, 2017.
Peizhao Li, Han Zhao, and Hongfu Liu. Deep fair clustering for visual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9070-9079, 2020.
Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi Jaakkola, Geoffrey Gordon, Stefanie Jegelka, and
Ruslan Salakhutdinov. Graph adversarial networks: Protecting information against adversarial
attacks. arXiv preprint arXiv:2009.13504, 2020.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of
the American society for information science and technology, 58(7):1019-1031, 2007.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair
autoencoder. In ICLR, 2016. URL http://arxiv.org/abs/1511.00830.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. arXiv preprint arXiv:1802.06309, 2018.
Feiping Nie, Wei Zhu, and Xuelong Li. Unsupervised large graph embedding. In Thirty-first AAAI
conference on artificial intelligence, 2017.
Eli Pariser. The filter bubble: How the new personalized web is changing what we read and how we
think. Penguin, 2011.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701-710, 2014.
Yanjun Qi, Ziv Bar-Joseph, and Judith Klein-Seetharaman. Evaluation of different biological data and
computational classification methods for use in protein interaction prediction. Proteins: Structure,
Function, and Bioinformatics, 63(3):490-500, 2006.
Tahleen Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. Fairwalk: towards fair graph
embedding. In Proceedings of the 28th International Joint Conference on Artificial Intelligence,
pp. 3289-3295. AAAI Press, 2019.
Aida Rahmattalabi, Phebe Vayanos, Anthony Fulginiti, Eric Rice, Bryan Wilder, Amulya Yadav, and
Milind Tambe. Exploring algorithmic fairness in robust graph covering problems. In Advances in
Neural Information Processing Systems, pp. 15750-15761, 2019.
Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th international conference on World Wide
Web, pp. 285-295, 2001.
Harald Steck. Calibrated recommendations. In Proceedings of the 12th ACM conference on recom-
mender systems, pp. 154-162, 2018.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th international conference on world
wide web, pp. 1067-1077, 2015.
11
Published as a conference paper at ICLR 2021
Amanda L Traud, Eric D Kelsic, Peter J Mucha, and Mason A Porter. Comparing community
structure to characteristics in online collegiate social networks. SIAM review, 53(3):526-543, 2011.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
embeddings for simple link prediction. International Conference on Machine Learning (ICML),
2016.
Berk Ustun, Yang Liu, and David Parkes. Fairness without harm: Decoupled classifiers with
preference guarantees. In International Conference on Machine Learning, pp. 6373-6382, 2019.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Weiran Wang and Miguel A Carreira-Perpindn. Projection onto the probability simplex: An efficient
algorithm with a simple proof, and an application. arXiv preprint arXiv:1309.1541, 2013.
Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative
filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and
development in Information Retrieval, pp. 165-174, 2019.
Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community preserving
network embedding. In Thirty-first AAAI conference on artificial intelligence, 2017.
Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning
for graph matching and node embedding. In International Conference on Machine Learning, pp.
6932-6941, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken ichi Kawarabayashi, and Stefanie Jegelka.
What can neural networks reason about? In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=rJxbJeHFPS.
Sirui Yao and Bert Huang. Beyond parity: Fairness objectives for collaborative filtering. In Advances
in Neural Information Processing Systems, pp. 2921-2930, 2017.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
974-983, 2018.
Muhammad Bilal Zafar, Isabel Valera, Manuel Rodriguez, Krishna Gummadi, and Adrian Weller.
From parity to preference-based notions of fairness in classification. In Advances in Neural
Information Processing Systems, pp. 229-239, 2017a.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness
constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp. 962-970.
PMLR, 2017b.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, pp. 5165-5175, 2018.
Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. In Advances in
neural information processing systems, pp. 15649-15659, 2019.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv preprint arXiv:1910.07162, 2019.
12
Published as a conference paper at ICLR 2021
In Appendix, we present proofs in Section A, illustrative diagrams for Theorem 4.1 in Section B,
experimental configurations in Section C, deferred results in Section D, and the demonstration of fair
vertex representation in Section E.
A	Proof
Proposition 4.1. For a link prediction function g(∙, ∙) modeled as inner product g(v, U) = v>Σu,
where Σ ∈ S+Mf is a positive-definite matrix, ∃Q > 0, ∀v 〜V, ∣∣v∣∣2 ≤ Q, for Ev〜U[v] ∈ RM, for
dyadic fairness based on demographic parity, if ∣∣Ev〜U[v | V ∈ So] - Ev〜U[v | V ∈ S1]∣∣2 ≤ δ,
△dp := ∣E(v,u)〜u×u[g(v,u) | S(V) = S(u)] - E(v,u)〜U×U[g(v,u) | S(V) = S(u)]∣ ≤ Qk∑k2 ∙ δ.
(2)
Proof. To simplify the notations, We use P := Ev〜U [v | V ∈ So] ∈ RM and q := Ev〜U [v | V ∈
S1] ∈ RM to denote the expectations in representations for S0 and S1 respectively.
|Eintra -	Einter| =	E[V>Σu	| V ∈	So, u ∈	S1] -	E[V>Σu | V ∈	So, u ∈	So ∨ V ∈	S1, u ∈	S1]
=p>'q- (|So|2 +∣∣S1∣2p>ςp + ∣So∣2+∣∣Si∣2q>"q)l
(	)>	∣S0∣2	ς	ς ʌl
=(q-Pp (fol2 + si2 P-所Tk町
To simplify the notation, we will use α := |So|2/(|So|2 + |S1|2) and β := |S1 |2/(|So|2 + |S1 |2)
≤ l∣q -pk2 ∙ g% -尸叩必
≤ δ ∙k∑∣2 ∙(kɑp∣∣2 + Ileqll2)
=Qk∑k2 ∙ δ,
which completes the proof. The first inequality above is due to Cauchy-Schwarz, and the second
one is by the definition of spectral norm. The last equality holds by the linearity of expectation: if
∀V ∈ V,lVl2 ≤ Q, then lE[V]l2 ≤E[lVl2] ≤ Q.
Theorem 4.1. For an arbitrary graph with nonnegative link weights, after conducting one mean-
aggregation over the graph, the consequent representation discrepancy between two sensitive groups
∆Apgr := ∣Ev〜U[Agg(v) l V ∈ So] - Ev〜U[Agg(v) ∣ V ∈ S1]∣∣2 is bounded by
max{amin∣μo - μι∣∞ - 2σ, 0} ≤ ∆Apgr ≤ αmaχ∣μo - μι∣2 + 2√Mσ,	(3)
ff
where αmin=min{α1, α2}, αmax=maχ{α1 ,a2}, α1=l1-Dmw (|S0|+尚 )|, α2=l1-图-图 |.
Proof. The feature representation ofV after conducting one mean-aggregation is
Agg(V)
E uXU) U = de⅛(u∈rX∩S0 Su + u∈X∩ Ju")
Here we separate the summation of neighbor features into two parts in terms of the sensitive attribute.
We use the bracket notation to abbreviate the range of a vector. That is, if a vector u satisfies
μ - σ ≤ u ≤ μ + σ, we abbreviate this as u ∈ [μ 土 σ].
Consider the unilateral case V ∈ So, we have
Agg(V) ∈[ ⅞WvVT avuμ0+⅞WvVΓ avuμ1 ±σ ∙1]
∈ [(μo + Pudef ∩S) avu (μι-μo))± σ ∙ 1]
where 1 is the all-one vector with proper size.
13
Published as a conference paper at ICLR 2021
The first derivation is due to the fact that each U ∈ So lies in the range of [μo ± σ ∙ 1] and each U ∈ Si
lies in the range of [μι ± σ ∙ 1]. The second one is by the definition of weighted degree.
Using βv =	v∈Γ(v)∩Sopp(v) avu /degw (v) where Sopp(v) is the opposite sensitive group where v
belongs. The expectation ofAgg(v) for S0 is
Ev 〜U[Agg(V) |V ∈ So] ∈[(js^∣
ɪ2 (μo + βv(μι- μO))) ± σ ∙ ɪ]
v∈S0
∈ [(μo + h⅛
E βv(μι - μo)) ± σ ∙ 1].
v∈S0
And for V ∈ S1 we have
Ev〜U[Agg(V) | V ∈ Si] ∈ [(μι + jSj
E βv(μo - μι)) ± σ ∙ 1].
v∈S1
Based on the above two terms, the gap in expectation of two groups after passing one mean-
aggregation layer becomes
Ev〜U[Agg(V) | V ∈ So] - Ev〜U[Agg(V) | V ∈ Si] ∈ [(1 - (τ7j-[ ^X βv + TTTT ^X βv)) ∙ (μo - μI) + 2σ ∙ 1].
|So| v∈S0	|Si| v∈S1
Next we study the range of α0 := 1 - (|So|-i Pv∈S βv + |Si|-i Pv∈S βv). First we consider the
term |So|-i Pv∈S0 βv. Sincedegw(V) ≤ Dmax,∀V ∈ V, we have
Σ
βv=
v∈S0	v∈S0
u∈Γ(v)∩Sι
degw(V)
avu
≥ D^ X X	avu
max
v∈S0 u∈Γ(v)∩S1
mw
Dmax
For non-negative weights,
Dmax ≥ degw (V) =	£ avu +	£ avu ≥	£ avu.
u∈Γ(v)∩S0	u∈Γ(v)∩S1	u∈Γ(v)∩S1
This means for V ∈ So,
R _ Σu∈Γ(u)∩Sι avu
βv=	degw (U) 一 ≤ ,
thus,
X βv = X βv ≤ |Sfo|.
v∈S0	v∈Sf0
The first equality holds because βv = 0 when V ∈ So/So, meaning V doesn’t contain any inter-edges.
Since the analysis for Si is similar, we derive the lower and upper bounds for |Si|-i Pv∈S βv, i =
0, 1	i
1	一 -	1	ICl
∣Sq ∙ Dw ≤ ∣Sτ∣ X βv ≤ (3), i = 0,1.
|Si |
Dmax	|Si| v∈Si	|Si|
Based on the above results, we give the bound for α0 as follows:
,c L	JfoL ISiK 1	mw( 1	1 n
α ∈ [1-( ISI + ISI), 1 - Dmax(ISl + ISI)],
Let αmin and αmax be lower bound and upper bower of Iα0 I, we have
—	ISoI	IfiI	mw 1	1
αmax = max{1 - (ISI + ISI), 1 - Dmx(ISI + ISI)}
—	ISoI	IfiI	mw	1	1
αmin = min{1 - (ISI + ISI), 1 - Dmx(ISI + ISI)}
14
Published as a conference paper at ICLR 2021
Thus we give the upper bound of ∆DAgPgr:
δDPgr ≤ αmaxkμ0 - μi ∣∣2 + 2√Mσ
(8)
where the second part in RHS is due to 2σ ∙∣∣lk2 = 2vzMσ.
Next We consider i-th entrance of μo and μι, denoted as μ0 and μ1 respectively. The i-the entrance
of Ev〜U [Agg(v) | V ∈ So] — Ev〜U [Agg(v) | V ∈ Si] take nonzero values if and only if
|(1-
(⅛∣ XOev + ISl
Eev)) ∙ (μ0 — μi)l≥ 2σ
v ∈S1
Thus we obtain the lower bound of ∆DAgPgr:
∆APgr ≥ max{amin∣μo — μι∣∣∞ — 2σ, 0}
(9)
which completes the proof.
B Complementary diagrams to Theorem 4.1
We provide diagrams to help better understand the upper bound in Theorem 4.1.
Figure 3 provides a common case that the gap in expectation between two sensitive groups shrinks
after mean-aggregation. Here the maximal deviation term σ can be neglected since it is much smaller
than the expectation gap.
Figure 4 provides a case that the term σ is not negligible against the expectation gap between two
sensitive groups. Here σ = 100 and the gap equals to 0. After aggregation, we see the new expectation
gap becomes 20, showing that the discrepancy in representations increases.
Figure 5 provides another case that the contraction coefficient α equals to 1 due to the resistance
of α2 . Here all vertices possess inter links, and the graph is a complete bipartite graph. Then
the aggregation fully exchanges the sensitive information, and thus the representation discrepancy
remains unchanged.
Cases in Figure 4 and 5 are also pointed out by the analysis in Section 4.
Figure 3: An illustrative graph example with two protected groups S0 and S1. All vertices have self-
loop. The expectation gap shrinks after mean aggregation. Here, ∣Ev〜U [v∣v ∈ So] — Ev〜U [v∣v ∈
Si] ∣ = 20, σ = 2 and all link weights are equal. After aggregation, ∣Ev〜U [Agg(v)∣v ∈ So]—
Ev〜U[Agg(v)∣v ∈ Si]∣ = ∣6.15 — (—6.15)∣ = 12.3 < 20.
15
Published as a conference paper at ICLR 2021
Figure 4: Case 1: The maximal deviation term O(σ) is not negligible. Here σ = 100 and all link
weights are equal. All vertices have self-loop. |E。〜U[v|v ∈ So] - Ev〜U[v|v ∈ Si]| = 0. But after
mean-aggregation, |Ev〜U[Agg(v)∣v ∈ So] - Ev〜U[Agg(v)∣v ∈ Si]| = |5 - 25| = 20 > 0.
Figure 5: Case 2: The contraction coefficient α equals to 1. This happens when the graph is a
complete bipartite graph. Mean-aggregation fully exchanges the sensitive information and the gap of
two groups remains unchanged.
C Experimental Configurations
For all experiments, we set Ti = 50 and the total epochs which contain Ti and T2 equal to 4. Graph
neural networks are applied with two hidden layers with size 32 and 16 respectively. ηθ is set to 0.01.
For ηAe for different datasets, we have: Oklahoma97: 0.1; UNC28: 0.1; Cora: 0.2; Citeseer: 0.5.
Experiments are conducted on Nvidia Titan RTX graphics card.
D Additional Results
We present experimental results for Citeseer and UNC28 in this section. All the results deliver similar
conclusions as we state in the main body of this paper. Additionally, we include another dataset
Facebook#1684 in response to the second limitation as indicated in Section 4. In this case, ∆DP,
∆true, ∆false are already small as given by VGAE, and FairAdj is not able to further minimize the gap.
Table 4: Experimental results on Oklahoma97.
Method	AUC ↑	AP ↑	δDP I	∆true I	∆false I	∆FNR I	∆TNR I
VGAE	90.13 ± 0.32	91.24 ± 0.37	8.73 ± 0.38	8.56 ± 0.44	0.40 ± 0.32	36.51 ± 1.41	2.26 ± 0.92
node2vec	86.49 ± 0.35	84.09 ± 0.50	7.23 ± 0.64	3.35 ± 0.45	1.08 ± 0.97	32.55 ± 1.32	2.36 ± 0.69
Fairwalk	86.56 ± 0.32	84.23 ± 0.44	7.31 ± 0.62	3.49 ± 0.47	1.13 ± 0.85	32.77 ± 1.20	2.18 ± 0.69
FairAdjT2=5	84.92 ± 0.81	85.07 ± 0.92	3.60 ± 0.35	0.40 ± 0.32	0.33 ± 0.28	4.00 ± 0.88	2.02 ± 0.76
FairAdjT2=20	81.01 ± 1.01	80.79 ± 0.93	2.96 ± 0.30	0.38 ± 0.31	0.32 ± 0.25	5.61 ± 1.06	2.03 ± 0.92
16
Published as a conference paper at ICLR 2021
Table 5: Experimental results on Cora.
Method	AUC ↑	AP ↑	△dp ]	△true J	△false J	△FNR J	△TNR J
VGAE	88.48 ± 0.88	90.81 ± 0.78	26.74 ± 1.51	9.99 ± 2.32	10.26 ± 1.59	28.25 ± 4.46	26.71 ± 3.83
node2vec	87.93 ± 0.75	87.82 ± 1.06	39.99 ± 2.75	6.63 ± 3.58	27.86 ± 4.94	23.66 ± 4.73	32.96 ± 5.24
Fairwalk	88.04 ± 0.84	88.10 ± 1.20	40.49 ± 2.58	7.30 ± 3.28	29.43 ± 4.86	23.74 ± 4.19	33.79 ± 5.08
FairAdjT2=5	86.00 ± 1.12	88.32 ± 0.86	21.05 ± 1.26	6.99 ± 2.24	6.14 ± 1.59	20.72 ± 3.62	19.46 ± 3.62
FairAdjT2=20	83.85 ± 1.07	86.08 ± 0.93	17.87 ± 1.18	5.40 ± 2.23	3.74 ± 1.46	16.75 ± 4.87	15.37 ± 3.84
Table 6: Experimental results on Pubmed.
Method	AUC ↑	AP ↑	△dp J	△true J	△false J	△FNR J	△TNR J
VGAE	91.20 ± 0.85	91.26 ± 0.80	20.88 ± 1.48	4.19 ± 0.93	8.04 ± 1.83	12.01 ± 2.92	19.18 ± 4.16
node2vec	74.27 ± 1.23	79.24 ± 1.29	19.14 ± 0.93	3.38 ± 2.57	8.90 ± 2.56	6.65 ± 2.21	10.91 ± 1.88
fairwalk	73.43 ± 1.11	78.96 ± 1.24	18.42 ± 1.65	3.11 ± 1.84	7.79 ± 3.49	6.61 ± 2.28	10.93 ± 2.54
FairAdjT2 = 5	88.64 ± 1.09	88.21 ± 1.22	16.06 ± 0.98	1.96 ± 0.82	4.40 ± 1.28	8.93 ± 2.90	12.75 ± 1.56
FairAdjT2 = 20	87.53 ± 1.03	87.10 ± 1.17	14.73 ± 0.98	1.39 ± 0.92	3.17 ± 1.10	9.09 ± 2.10	10.46 ± 1.73
Figure 6: Compare to adversarial training on vertex representations. Left: Oklahoma97; Right: Cora.
Figure 7: Diversity and utility in recommendations. Left: Oklahoma97; Right: Cora.
diversity
0.05 0.10 0.15 0.20
proportion
1.000
0.995
0.990
0.985
S 0.980
0.975
0.970
0.965
0.960
0.05 0.10 0.15 0.20
proportion
17
Published as a conference paper at ICLR 2021
Table 7: Experimental results on Facebook#1684.
Method	AUC	AP	△dp	△true	△false	△FNR	△TNR
VGAE	94.66 ± .55	93.91 ± .68	2.03 ± .81	0.59 ± .49	0.90 ± .57	4.48 ± 1.57	4.94 ± 1.32
node2vec	90.57 ± .74	85.61 ± 1.09	1.70 ± 1.43	0.52 ± .49	2.47 ± 1.52	6.51 ± 2.04	5.06 ± 1.36
fairWalk	90.56 ± .63	85.58 ± .87	1.97 ± 1.51	0.62 ±.47	2.14 ± 1.77	6.92 ± 2.19	5.03 ± 1.46
FairAdjT2 = 1	94.68 ± .48	93.94 ± .62	2.02 ± .82	0.60 ± .50	0.93 ± .60	4.42 ± 1.57	4.82 ± 1.54
FairAdjT2 = 20	94.63 ± .49	93.84 ± .64	1.77 ± .81	0.53 ± .41	0.92 ± .49	5.00 ± 1.52	4.86 ± 1.41
E Fair Vertex Representation
As an intermediate result, we inspect the fairness in vertex representation in Figure 8. To quantify
that, we conduct K-means clustering on vertex representation and evaluate the ratio of samples from
different sensitive groups within each clusters, and the ratio is called balance. We range the number
of clusters from 4 to 8 and report the average balance across all clusters. In general, the higher the
balance, the fairer in vertex representations. Overall the series of FairAdj achieves a higher balance,
which shows the invariant representations on vertices across different sensitive groups.
Figure 8: Evaluations on balance of clusters. Left: Oklahoma97; Right: UNC28.
,67
0.
8
6
0.
O 9
7 6
0.0.
3JUE-eq
F Extend Corollary 4.1 to two-layer GNNs
For ∆(D2P) on vertices after passing two layer GNN(θ2) (X, Ae) := (Ae(GNN(θ1) (X, Ae))Wθ(2)) =
P (AP (AXWθ(1) )W(2)). Here We denote	μi := Ev〜U[GNN^1) (v,A)	| V ∈	Si],	i =	0,1,
Q0 ：= sup{∣∣GNNθ1)(v,A)∣∣2 | V e V}.	μi0 := Ev〜U[GNn[2)(v,N)	| V ∈	Si],	i =	0,1,
Q00 := sup{kGNN(θ2)(v, Ae)k2 | v ∈ V}. Let σ1 be the maximal deviation of {v|v ∈ V},
and σ2 be the maximal deviation of {GNN(θ1)(V, Ae)|V ∈ V}. We have Q0 ≤ LkWθ(1) k2Q,
Q00≤LkWθ(2)k2Q0≤L2kWθ(1)k2kWθ(2)k2Q.
Then
△DP ≤ q00ii∑i∣2 ∙imjk2 ≤ ql3i此2)ι∣2 kw/gmiu -江 + 2√Mσ2)
And
kμ0 - μi k2 ≤ LllW/||2(。||阿-μ1k2 + 2√Mσ1)
Finally We have:
△DP ≤ QL4l∣wΓll2kwΓk2α2kμo -μ1k2 + 2√MQL3∣∣Wy)∣∣2kW9k2(L||W(I)∣∣2 ∙σι + σ)
18