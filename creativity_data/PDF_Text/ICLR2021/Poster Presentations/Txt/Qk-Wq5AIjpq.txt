Published as a conference paper at ICLR 2021
PAC Confidence Predictions for Deep Neural
Network Classifiers
Sangdon Park, Shuo Li, Insup Lee & Osbert Bastani
PRECISE Center
University of Pennsylvania
{sangdonp, lishuo1, lee, obastani}@seas.upenn.edu
Ab stract
A key challenge for deploying deep neural networks (DNNs) in safety critical
settings is the need to provide rigorous ways to quantify their uncertainty. In
this paper, we propose a novel algorithm for constructing predicted classification
confidences for DNNs that comes with provable correctness guarantees. Our ap-
proach uses Clopper-Pearson confidence intervals for the Binomial distribution in
conjunction with the histogram binning approach to calibrated prediction. In addi-
tion, we demonstrate how our predicted confidences can be used to enable down-
stream guarantees in two settings: (i) fast DNN inference, where we demonstrate
how to compose a fast but inaccurate DNN with an accurate but slow DNN in a
rigorous way to improve performance without sacrificing accuracy, and (ii) safe
planning, where we guarantee safety when using a DNN to predict whether a given
action is safe based on visual observations. In our experiments, we demonstrate
that our approach can be used to provide guarantees for state-of-the-art DNNs.
1	Introduction
Due to the recent success of machine learning, there has been increasing interest in using predic-
tive models such as deep neural networks (DNNs) in safety-critical settings, such as robotics (e.g.,
obstacle detection (Ren et al., 2015) and forecasting (Kitani et al., 2012)) and healthcare (e.g., diag-
nosis (Gulshan et al., 2016; Esteva et al., 2017) and patient care management (Liao et al., 2020)).
One of the key challenges is the need to provide guarantees on the safety or performance of DNNs
used in these settings. The potential for failure is inevitable when using DNNs, since they will in-
evitably make some mistakes in their predictions. Instead, our goal is to design tools for quantifying
the uncertainty of these models; then, the overall system can estimate and account for the risk inher-
ent in using the predictions made by these models. For instance, a medical decision-making system
may want to fall back on a doctor when its prediction is uncertain whether its diagnosis is correct, or
a robot may want to stop moving and ask a human for help if it is uncertain to act safely. Uncertainty
estimates can also be useful for human decision-makers—e.g., for a doctor to decide whether to trust
their intuition over the predicted diagnosis.
While many DNNs provide confidences in their predictions, especially in the classification setting,
these are often overconfident. This phenomenon is most likely because DNNs are designed to overfit
the training data (e.g., to avoid local minima (Safran & Shamir, 2018)), which results in the predicted
probabilities on the training data being very close to one for the correct prediction. Recent work has
demonstrated how to calibrate the confidences to significantly reduce overconfidence (Guo et al.,
2017). Intuitively, these techniques rescale the confidences on a held-out calibration set. Because
they are only fitting a small number of parameters, they do not overfit the data as was the case in
the original DNN training. However, these techniques do not provide theoretical guarantees on their
correctness, which can be necessary in safety-critical settings to guarantee correctness.
We propose a novel algorithm for calibrated prediction in the classification setting that provides
theoretical guarantees on the predicted confidences. We focus on on-distribution guarantees—
i.e., where the test distribution is the same as the training distribution. In this setting, we can build
on ideas from statistical learning theory to provide probably approximately correctness (PAC) guar-
antees (Valiant, 1984). Our approach is based on a calibrated prediction technique called histogram
1
Published as a conference paper at ICLR 2021
binning (Zadrozny & Elkan, 2001), which rescales the confidences by binning them and then rescal-
ing each bin independently. We use Clopper-Pearson bounds on the tails of the binomial distribution
to obtain PAC upper/lower bounds on the predicted confidences.
Next, we study how it enables theoretical guarantees in two applications. First, we consider the
problem of speeding up DNN inference by composing a fast but inaccurate model with a slow but
accurate model—i.e., by using the accurate model for inference only if the confidence of the inaccu-
rate one is underconfident (Teerapittayanon et al., 2016). We use our algorithm to obtain guarantees
on accuracy of the composed model. Second, for safe planning, we consider using a DNN to predict
whether or not a given action (e.g., move forward) is safe (e.g., do not run into obstacles) given an
observation (e.g., a camera image). The robot only continues to act if the predicted confidence is
above some threshold. We use our algorithm to ensure safety with high probability. Finally, we
evaluate the efficacy of our approach in the context of these applications.
Related work. Calibrated prediction (Murphy, 1972; DeGroot & Fienberg, 1983; Platt, 1999) has
recently gained attention as a way to improve DNN confidences (Guo et al., 2017). Histogram
binning is a non-parametric approach that sorts the data into finitely many bins and rescales the
confidences per bin (Zadrozny & Elkan, 2001; 2002; Naeini et al., 2015). However, traditional ap-
proaches do not provide theoretical guarantees on the predicted confidences. There has been work
on predicting confidence sets (i.e., predict a set of labels instead of a single label) with theoreti-
cal guarantees (Park et al., 2020a), but this approach does not provide the confidence of the most
likely prediction, as is often desired. There has also been work providing guarantees on the overall
calibration error (Kumar et al., 2019), but this approach does not provide per-prediction guarantees.
There has been work speeding up DNN inference (Hinton et al., 2015). One approach is to allow
intermediate layers to be dynamically skipped (Teerapittayanon et al., 2016; Figurnov et al., 2017;
Wang et al., 2018), which can be thought of as composing multiple models that share a backbone.
Unlike our approach, they do not provide guarantees on the accuracy of the composed model.
There has also been work on safe learning-based control (Akametalu et al., 2014; Fisac et al., 2019;
Bastani, 2019; Li & Bastani, 2020; Wabersich & Zeilinger, 2018; Alshiekh et al., 2018); however,
these approaches are not applicable to perception-based control. The most closely related work
is Dean et al. (2019), which handles perception, but they are restricted to known linear dynamics.
2	PAC Confidence Prediction
In this section, we begin by formalizing the PAC confidence coverage prediction problem; then, we
describe our algorithm for solving this problem based on histogram binning.
Calibrated prediction. Let x ∈ X be an example and y ∈ Y be one of a finite label set, and let
D be a distribution over X × Y. A confidence predictor is a model f : X → PY, where PY is the
space of probability distributions over labels. In particular, f (x)y is the predicted confidence that
the true label for X is y. We let y : X → Y be the corresponding label predictor—i.e., y(x):=
argmaxy∈γ f (x)y —and let p ： X → R≥o be corresponding top-label confidence predictor—
i.e., P(X) := maxy∈γ f (x)y. While traditional DNN classifiers are confidence predictors, a naively
trained DNN is not reliable—i.e., predicted confidence does not match to the true confidence; recent
work has studied heuristics for improving reliability (Guo et al., 2017). In contrast, our goal is to
construct a confidence predictor that comes with theoretical guarantees.
We first introduce the definition of calibration (DeGroot & Fienberg, 1983; Zadrozny & Elkan, 2002;
Park et al., 2020b)—i.e., what we mean for a predicted confidence to be “correct”. In many cases,
the main quantity of interest is the confidence of the top prediction. Thus, we focus on ensuring that
the top-label predicted confidence P(X) is calibrated (Guo et al., 2017); our approach can easily be
extended to providing guarantees on all confidences predicted using f. Then, we say a confidence
predictor f is well-calibrated with respect to distribution D if
F(x,y)〜D [y = y(X) | P(X)= t] = t	(Vt ∈ [0, 1]).
That is, among all examples X such that the label prediction y(X) has predicted confidence t = P(x),
y(X) is the correct label for exactly a t fraction of these examples. Using a change of variables (Park
2
Published as a conference paper at ICLR 2021
. ICCCCF	11 IT . 1 ∙	♦ 1 ..
et al., 2020b), f being well-calibrated is equivalent to
P(X) =	c^(X)= P(χ0,y0)〜D	[y0	= y(XO)	|	P(XO)=	P(X)]	(∀x	∈ X).	(I)
Then, the goal of well-calibration is to make P equal to C^. Note that f appears on both sides
of the equation P(X) = c；(x)—implicitly in P—which is what makes it challenging to satisfy.
Indeed, in general, it is unlikely that (1) holds exactly for all X. Instead, based on the idea of
histogram binning (Zadrozny & Elkan, 2001), we consider a variant where we partition the data into
a fixed number of bins and then construct confidence coverages separately for each bin. In particular,
consider K bins B1, . . . , BK ⊆[0, 1], where B1 =[0, b1] and Bk = (bk-1, bk] for k > 1. Here, K
IC /7/	/7	,7	T	1	.	y- 1 ∙	P 1	-»，	τ>r'l .
and 0 ≤ bi ≤ ∙∙∙ ≤ bκ-ι ≤ bκ = 1 are hyperparameters. Given f, let κ^ : X → {1,..., K} to
denote the index of the bin that contains P(X)— i.e., P(X) ∈ BK^(χ).
Definition 1 We say f is well-calibrated for a distribution D and bins B1 , . . . , BK if
P(x) =	cf(X) := F(X0,y0)〜D	[y0	= y(X) I	P(xo)	∈	BKf⑺]	(∀x	∈	X),	(2)
where we refer to c∕(x) as the true confidence. Intuitively, this definition “coarsens” the calibration
problem across the bins—i.e., rather than sorting the inputs X into a continuum of “bins” P(X) = t
for each t ∈ [0,1] as in (1), we sort them into a finite number of bins P(X) ∈ Bk intuitively, we
have * ≈ Cf if the bin sizes are close to zero. It may not be obvious what downstream guarantees
f
can be obtained based on this definition; we provide examples in Sections 3 & 4.
Problem formulation. We formalize the problem of PAC calibration. We focus on the setting
where the training and test distributions are identical—e.g., we cannot handle adversarial examples
or changes in covariate distribution (e.g., common in reinforcement learning). Importantly, while
we assume a pre-trained confidence predictor f is given, we make no assumptions about f —e.g., it
can be uncalibrated or heuristically calibrated. If f performs poorly, then the predicted confidences
will be close to 1/|Y|—i.e., express no confidence in the predictions. Thus, it is fine if f is poorly
calibrated; the important property is that the confidence predictor f have similar true confidences.
The challenge in formalizing PAC calibration is that quantifying over all X in (2). One approach is
to provide guarantees in expectation over X (Kumar et al., 2019); however, this approach does not
provide guarantees for individual predictions.
Instead, our goal is to find a set of predicted confidences that includes the true confidence with
high probability. Of course, we could simply predict the interval [0, 1], which always contains the
true confidence; thus, simultaneously want to make the size of the interval small. To this end, we
consider a confidence coverage predictor C : X → 2R, where c∕(x) ∈ C(X) with high probability.
In particular, C(x) outputs an interval [c, c] ⊆ R, where c ≤ c, instead of a set. We only consider a
single interval (rather than disjoint intervals) since one suffices to localize the true confidence eʃ.
We are interested in providing theoretical guarantees for an algorithm used to construct confidence
coverage predictor C given a held-out calibration set Z ⊆ X × Y . In addition, we assume the
algorithm is given a pretrained confidence predictor f . Thus, we consider C as depending on Z and
f, which we denote by C(∙; f, Z). Then, we want C to satisfy the following guarantee:
Definition 2 Given δ ∈ R>0 and n ∈ N, C is probably approximately CorreCt (PAC) if for any D,
^
c/(X) ∈ C(χ; f, Z) ≥ 1 - 6.	(3)
x∈X
Note that Cf depends on D. Here, “approximately correct” technically refers to the mean of
C(x; f, Z), which is an estimate of c∕(χ); the interval captures the bound E on the error of this es-
timate; see Appendix A for details. Furthermore, the conjunction over all X ∈ X may seem strong.
We can obtain such a guarantee due to our binning strategy: the property c∕(χ) ∈ C(x; f, Z) only
depends on the bin BK^(χ), so the conjunction is really only over bins k ∈ {1,…,K}.
3
Published as a conference paper at ICLR 2021
Algorithm. We propose a confidence coverage predictor that satisfies the PAC property. The prob-
lem of estimating the confidence interval C(x) of the binned true confidence c∕(x) is closely related
to the binomial proportion confidence interval estimation; consider a Bernoulli random variable
b 〜 B := BernoUlli(θ) for any θ ∈ [0,1], where b = 1 denotes a success and b = 0 denotes a
failure, and θ is unknown. Given a sequence of observations bi：n := (bi,..., bn)〜Bn, the goal is
to construct an interval Θ(b±n) ⊆ R that includes θ With high probability——i.e.,
RbL.Bn [θ ∈ θ(bi:n)] ≥ 1 — α,	(4)
where α ∈ R>0 is a given confidence level. In particular, the Clopper-Pearson interval
<⅛CP(bi：n； α) := inf {θ ∣ 咻[S ≥ s] ≥ Oa }，SUP{" | pθ F ≤ s] ≥ Oa }_|，
guarantees (4) (Clopper & Pearson, 1934; Brown et al., 2001), where s = Pin=1 bi is the number
of observed successes, n is the number of observations, and S is a Binomial random variable S 〜
Binomial(n, θ). Intuitively, the interval is constructed such that the number of observed success falls
in the region with high-probability for any θ in the interval. The following expression is equivalent
due to the relationship between the Binomial and Beta distributions (Hartley & Fitch, 1951; Brown
et al., 2001)——i.e., Pθ[S ≥ s] = Iθ(s, n - S +1), where Iθ is the CDF ofBeta(s, n - S + 1):
a
θCP(b±n; ɑ) = [ — quantile of Beta(s, n - S + 1),
quantile of Beta(s + 1
n-
s) .
Now, for each of the K bins, we apply ΘCP with confidence level α =套——i.e.,
O. , O _	O
C(x; f, Z, δ) := ΘCP
where Wk := ∣l(y(x) = y) | (x, y) ∈ Z s.t. κf(x) = k}.
Here, Wk is the set of observations of successes vs. failures corresponding to the subset of labeled
examples (x, y) ∈ Z such that p^(χ) falls in the bin Bk, where a success is defined to be a correct
prediction y(x) = y. We note that for efficiency, the confidence interval for each of the K bins can
be precomputed. Our construction of C satisfies the following; see Appendix B for a proof:
Theorem 1 Our confidence coverage predictor C is PACfor any δ ∈ R>o and n ∈ N.
Note that Clopper-Pearson intervals are exact, ensuring the size ofC for each bin is small in practice.
Finally, an important special case is when there is a single bin B = [0, 1]——i.e.,
Co(x; f,Z0,δ)=Θcp(W； δ) where W := {1 (y(x0) = y0) | (x0,y0) ∈ Z0}.
Note that Co does not depend on x, so we drop it——i.e., Co(f, Z , δ) := Θcp(W; δ)——i.e., Co com-
putes the Clopper-Pearson interval over Z0, which is a subset of the original set Z.
3 Application to Fast DNN Inference
A key application of predicted confidences is to perform model composition to improve the running
time of DNNs without sacrificing accuracy. The idea is to use a fast but inaccurate model when it
is confident in its prediction, and switch to an accurate but slow model otherwise (Bolukbasi et al.,
2017); we refer to the combination as the composed model. To further improve performance, we can
have the two models share a backbone——i.e., the fast model shares the first few layers of the slow
model (Teerapittayanon et al., 2016). We refer to the decision of whether to skip the slow model as
the exit condition; then, our goal is to construct confidence thresholds for exit conditions in a way
that provides theoretical guarantees on the overall accuracy.
Problem setup. The early-stopping approach for fast DNN inference can be formalized as a se-
quence of branching classifiers organized in a cascading way——i.e.,
外(V 〜	—∕ym(X) if Vm-I (Pi(X) <Yi)∧ (Pm(X) ≥ Ym)(Vm ∈ 口，…,M - 1})
yC (x；Y1:M T):=篙(X) other-wise,
4
Published as a conference paper at ICLR 2021
f1 (x)	f2(x)	f3(x)
Figure 1: A composed model in a cascading way for M = 4.
where M is the number of branches, fm is the confidence predictor, and ym and Pm are the asso-
ciated label and top-label confidence predictor, respectively. For conciseness, we denote the exit
Condition of the mth branch by dm (i.e., dm(x) := I(Vm-I (Pi(X) < Yi) ∧ (Pm(X) ≥ Ym))) With
thresholds γι,...,γm, ∈ [0,1]. The fm share a backbone and are trained in the standard way; see
Appendix F.4 for details. Figure 1 illustrates the composed model for M = 4; the gray area rep-
resents the shared backbone. We refer to an overall model composed in this way as a cascading
classifier.
Desired error bound. Given ξ ∈ R>0, our goal is to choose Y1:M-1 := (Y1, . . . , YM-1) so the
error difference of the cascading classifier yc and the slow classifier r^M is at most ξ—i.e.,
perr := F(x,y)〜D [yC (X)= y] - F(x,y)〜D [yM (X) = y] ≤ ξ∙	(5)
To obtain the desired error bound, an example X exits at the mth branch if r^mj is likely to classify X
correctly, allowing for at most ξ fraction of errors total. Intuitively, if the confidence of r^mj on X is
sufficiently high, then r^m(X) = y with high probability. In this case, r^M either correctly classifies
or misclassifies the same example; if the example is misclassified, it contributes to decrease Perr;
otherwise, we have ym(X) = y = yM(x) with high probability, which contributes to maintain Perr.
Fast inference. To minimize running time, we prefer to allow higher error rates at the lower
branches—i.e., we want to choose Ym as small as possible at lower branches m.
Algorithm. Our algorithm takes prediction branches fm (for m ∈ {1, . . . , M}), the desired relative
error ξ ∈ [0, 1], a confidence level δ ∈ [0, 1], and a calibration set Z ⊆ X × Y, and outputs Y1:M-1
so that (5) holds with probability at least 1 - δ. It iteratively chooses the thresholds from Y1 to
YM-1 ; at each step, it chooses Ym as small as possible subject to Perr ≤ ξ. Note that Ym implicitly
appears in Perr in the constraint due to the dependence of dm(X) on Ym. The challenge is enforcing
the constraint since we cannot evaluate it. To this end, let
em := F(χ,y)〜D [ym(X) = y ∧ Iym(X) = &M (x) ∧ dm (x) = 1]
em :=(X(x,y)〜D [yM (X) = y ∧ ym (X) = yM (X) ∧ dm (X) = 1] ,
then it is possible to show that Perr = PmM=-11 em - e0m (see proof of Theorem 2 in Appendix C).
Then, we can compute bounds on em and e0m using the following:
F [ym(X)= y	|	ym (X) = yM (X)	∧ dm (X)	= 1] ∈	[cm, ðm]	:=	CO	(fm, Zm, 3(M	ɪ)
F [yM (x) = y	|	ym (X) = yM (X)	∧ dm (X)	= 1] ∈	[cm, %]	:=	CO	(fM, Zm, 3(M	^)
δ
F [ym (X) = yM (x) ∧ dm (x) = 1] ∈ [rm,rm] := θCP (Wm; 3(M	1)
where
Zm := { (x, y) ∈ Z 1 ym(X) = yM (x) ∧ dm(X) = 1}
Wm := {l(ym(χ) = yM (χ) ∧ dm (χ) = 1) | (χ, y) ∈ Z}.
Thus, we have em, ≤ ɑmFm and emʃ ≥ cmXm, in which case it suffices to sequentially solve
m
Ym = arg min Y subj. to T或% - cZ ≤ ξ	(6)
γ∈[0,1]	i=ι
Here, Cm, rm,, cm, and rmʃ are implicitly a function of Y, which we can optimize using line search.
We have the following guarantee; see Appendix C for a proof:
5
Published as a conference paper at ICLR 2021
Theorem 2	We have Perr ≤ ξ with probability at least 1 一 δ over Z 〜Dn.
Moreover, the proposed greedy algorithm (6) is actually optimal in reducing inference speed when
M = 2. Intuitively, we are always better off in terms of inference time by classifying more examples
using a faster model. In particular, we have the following theorem; see Appendix D for a proof:
Theorem 3	If M =2, y； minimizes (6), and the classifiers ym are faster for smaller m, then the
resulting yc is thefastest cascading classifier among cascading classifiers that satisfy Perr ≤ ξ.
4 Application to Safe Planning
Robots acting in open world environments must rely on deep learning for tasks such as object
recognition—e.g., detect objects in a camera image; providing guarantees on these predictions is
critical for safe planning. Safety requires not just that the robot is safe while taking the action,
but that it can safely come to a stop afterwards—e.g., that a robot can safely come to a stop be-
fore running into a wall. We consider a binary classification DNN trained to predict a probability
f(x) ∈ [0, 1] of whether the robot is unsafe in this sense.1 If f(x) ≥ γ for some threshold γ ∈ [0, 1],
then the robot comes to a stop (e.g., to ask a human for help). If the label l(f (x) ≥ Y) correctly
predicts safety, then this policy ensures safety as long as the robot starts from a safe state (Li &
Bastani, 2020). We apply our approach to choose γ to ensure safety with high probability.
Problem setup. Given a performant but potentially unsafe policy ∏ (e.g., a DNN policy trained to
navigate to the goal), our goal is to override ∏ as needed to ensure safety. We assume that ∏ is trained
in a simulator, and our goal is to ensure that ∏ is safe according to our model of the environment,
which is already a challenging problem when ∏ is a deep neural network over perceptual inputs. In
particular, we do not address the sim-to-real problem.
Let x ∈ X be the states, Xsafe ⊆ X be the safe states (i.e., our goal is to ensure the robot stays in
Xsafe), o ∈ O be the observations, u ∈ U be the actions, g : X × U → X be the (deterministic)
dynamics, and h : X → O be the observation function. A state xis recoverable (denoted x ∈ Xrec) if
the robot can use ∏ in state X and then safely come to a stop using a backup policy ∏o (e.g., braking).
Then, the shield policy uses ∏ if X ∈ X^c, and ∏o otherwise (Bastani, 2019). This policy guarantees
safety as long as an initial state is recoverable—i.e., x0 ∈ Xrec. The challenge is determining whether
X ∈ Xrec. When we observe X, we can use model-based simulation to perform this check. However,
in many settings, we only have access to observations—e.g., camera images or LIDAR scans—so
this approach does not apply. Instead, we propose to train a DNN to predict recoverability—i.e.,
y(o):
(1 (“un-recoverable”)
0 (“recoverable”)
r> .、
if f(o) ≥ Y
otherwise
where o = h(X),
with the goal that y(o) ≈ y； (x) := l(x ∈ Xrec), resulting in the following the shield policy ∏shield:
∫π(O)	if y(O) = 0
shield	π0 (o)	otherwise.
Safety guarantee. Our main goal is to choose Y so that πshield ensures safety with high probability—
i.e., given ξ ∈ R>0 and any distribution D over initial states X0 ⊆ Xrec, we have
PUnSafe := R~D∏shield [Z ⊆ Xsafe] ≤ ξ,	⑺
where Z(χo,∏) := (χo,χι,...) is a rollout from xo 〜 D generated using ∏—i.e., xt+i =
g(Xt, π(h(Xt))).2 We assume the rollout terminates either once the robot reaches its goal, or once it
switches to ∏o and comes to a stop; in particular, the robot never switches from ∏o back to ∏.
Success rate. To maximize the success rate (i.e., the rate at which the robot achieves its goal), we
need to minimize how often πshield switches to π0, which corresponds to maximizing Y.
1Since |Y| = 2, f can be represented as a map f : X → [0, 1]; the second component is simply 1 - f (x).
2We can handle infinitely long rollouts, but in practice rollouts will be finite (but possibly arbitrarily long).
and Dπshield is an induced distribution over rollouts ζ(x0, πshield).
6
Published as a conference paper at ICLR 2021

XSeJnUOe
0.0	0.2	0.4	0.6	0.8	1.0
confidence
XSeJnUOe
0.0	0.2	0.4	0.6	0.8	1.0
confidence
o=e∙lidluexa
XSeJnUOe
0.0	0.2	0.4	0.6	0.8
confidence
o=e∙lidluexa
I


(a) Naive Softmax	(b) Temperature scaling	(C) Proposed
Figure 2: Calibration comparison; default parameters of C are K = 20, n = 20,000, and δ = 10-2.
Algorithm. Our algorithm takes the confidence predictor f, desired bound ξ ∈ R>0 on the unsafety
probability, confidence level δ ∈ [0,1], calibration set W ⊆ X∞ of rollouts Z 〜Dn, and calibra-
tion set Z ⊆ O of samples from distribution D described below; see Appendix F.5 for details on
sampling ζ and constructing W, Z, and D. We want to maximize γ subject to punsafe ≤ ξ, where
punsafe is implicitly a function of γ. However, we cannot evaluate punsafe, so we need an upper bound.
To this end, consider a rollout that the first unsafe state is encountered on step t (i.e., xt 6∈ Xsafe
but xi ∈ Xsafe for all i < t), which we call an unsafe rollout, and denote the event that the unsafe
rollout is encountered by Et ; we exploit the unsafe rollouts to bound punsafe . In particular, let pt :=
Pz~d∏ [Et], and let P := P∞=0 Pt be the probability that a rollout is unsafe. Then, consider a new
∞
distribution D over O with a probability density function PD (o) := Et=O PDn (o | Et) ∙Pt/P, where
pDn is the original probability density function of Dn; in particular, We can draw an observation
o 〜D by sampling the observation of the first unsafe state from a rollout sample (and rejecting if the
entire rollout is safe). Then, we can show the following (see a proof of Theorem 4 in Appendix E):
PUnSafe ≤ F0~D [y(O) = 0] ' P =: PUnSafe∙	(8)
We use our confidence coverage predictor C to compute bounds on PUnSafe—i.e.,
Fo~D [y(O) =	1] ∈	[C,日：=C0 (f, Z0, 2)	where	z0 := {(o,I) | o ∈ Z}
P ∈	[r,r]	:= ΘCP (W0, 1)	where	W0 := {I(Z ⊆ XSafe) | Z	∈ W}.
Here, Z0 is a labeled version of Z, where “1” denotes “un-recoverable”, n := |W|, and n0 := |Z|,
where n ≥ n0. Then, we have PUnSafe ≤ r ∙ (1 - c), so it suffices to solve the following problem:
Y := arg max γ0 subj. to r ∙ (1 — C) ≤ ξ	(9)
γ0∈[0,i]
Here, C is implicitly a function of γ0; thus, we use line search to solve this optimization problem.
We have the following safety guarantee, see Appendix E for a proof:
Theorem 4 We have PUnsafe ≤ ξ with probability at least 1 — δ over W 〜Dn and Z 〜Dn0.
5 Experiments
We demonstrate that how our proposed approach can be used to obtain provable guarantees in our
two applications: fast DNN inference and safe planning. Additional results are in Appendix G.
5.1	Calibration
We illustrate the calibration properties of our approach using reliability diagrams, which show the
empirical accuracy of each bin as a function of the predicted confidence (Guo et al., 2017). Ideally,
the accuracy should equal the predicted confidence, so the ideal curve is the liney = x. To draw our
predicted confidence intervals in these plots, we need to rescale them; see Appendix F.3.
Setup. We use the ImageNet dataset (Russakovsky et al., 2015) and ResNet101 (He et al., 2016) for
evaluation. We split the ImageNet validation set into 20, 000 calibration and 10, 000 test images.
7
Published as a conference paper at ICLR 2021
method	top-1 error (%)	CPU (μs)	GPU (μs)	MACs
slow network	22.32	214.31	2.95	7.83 × 109
(1 — ξ0)-softmax (baseline)	22.34	104.24	1.78	6.60 × 109
(1 — ξ0)-temperature scaling (baseline)	22.22	109.57	1.81	6.83 × 109
histogram binning (baseline)	24.54	70.72	1.19	4.62 × 109
rigorous (ours)	23.26	85.58	1.34	5.33 × 109
(C) Running time comparison.
Figure 3: Fast DNN inference results; default parameters are n = 20,000, ξ = 0.02, and δ = 10-3.
τr* 1 ∙	-c-r T	∙ 1 ,1	1	1 ∙	∕∙ ∖	∙∙	1',	I' ∕∙ ∙ ∖ ,	,	1 ∙	/c	. 1
Baselines. We consider three baselines: (i) naive softmax of f, (ii) temperature scaling (Guo et al.,
2017), and (iii) histogram binning (Zadrozny & Elkan, 2001); see Appendix F.2 for details. For
histogram binning and our approach, we use K = 20 bins of the same size.
Metric. We use expected calibration error (ECE) and reliability diagrams (see Appendix F.3).
Results. Results are shown in Figure 2. The ECE of the naive softmax baseline is 4.79% (Figure
2a), of temperature scaling enhances this to 1.66% (Figure 2b), and of histogram binning is 0.99%
(Figure 2c). Our approach predicts an interval that include the empirical accuracy in all bins (solid
red lines in Figure 2c); furthermore, the upper/lower bounds of the ECE over values in our bins is
[0.0%, 3.76%], which includes zero ECE. See Appendix G.1 for additional results.
5.2	Fast DNN Inference
Setup. We use the same ImageNet setup along with ResNet101 as the calibration task. For the
cascading classifier, we use the original ResNet101 as the slow network, and add a single exit branch
(i.e., M = 2) at a quarter of the way from the input layer. We train the newly added branch using
the standard procedure for training ResNet101.
Baselines. We compare to naive softmax and to calibrated prediction via temperature scaling, both
using a threshold γ1 = 1 - ξ0, where ξ0 is the sum of ξ and the validation error of the slow model;
intuitively, this threshold is the one we would use if the predicted probabilities are perfectly cali-
brated. We also compare to histogram binning—i.e., our approach but using the means of each bin
instead of the upper/lower bounds. See Appendix F.2 for details.
Metrics. First, we measure test set top-1 classification error (i.e., 1 - accuracy), which we want to
guarantee this lower than a desired error (i.e., the error of the slow model and desired relative error
ξ). To measure inference time, we consider the average number of multiplication-accumulation op-
erations (MACs) used in inference per example. Note that the MACs are averaged over all examples
in the test set since the combined model may use different numbers of MACs for different examples.
Results. The comparison results with the baselines are shown in Figure 3a. The original neural
network model is denoted by “slow network”, our approach (6) by “rigorous”, and our baseline by
“(1 - ξ )-softmax”，“(1 - ξ )-temp."，and “hist. bin.". For each method, We plot the classification
error and time in MACs. The desired error upper bound is plotted as a dotted line; the goal is for the
classification error to be loWer than this line. As can be seen, our method is guaranteed to achieve
the desired error, While improving the inference time by 32% compared to the sloW model. On the
other hand, the histogram baseline improves the inference time but fails to satisfy the desired error.
Asymptotically, histogram binning is guaranteed to be perfectly calibrated, but it makes mistakes
due to finite sample errors. The other baselines do not improve inference time. Next, Figure 3b
shoWs the classification error as We vary the desired relative error ξ; our approach alWays achieves
the desired error on the test set, and is often very close (Which maximizes speed). HoWever, the
8
Published as a conference paper at ICLR 2021
(*)seJ ⅛-fiβw
safety
success
(％)sE Ss38n∙,
so
£=0.20 £=0.15 ξ = 0.10 F= 0.05 F= 0.03

(a) Comparison to baselines. (b) Varying desired safety rate ξ.
Figure 4: Safe planning results; default parameters are: n = 20,000, ξ = 0.1, δ = 10-2.
baselines often violate the desired error bound. Finally, the MACS metric is only an approximation
of the actual inference time. To complement MACs, We also measure CPU and GPU time using the
PyTOrch profiler. In Figure 3c, we show the inference times for each method, where trends are as
before; our approach improves running time by 54%, while only reducing classification error by 1%.
The histogram baseline is faster than our approach, but does not satisfy the error guarantee. These
results include the time needed to compute the intervals, which is negligible.
5.3 Safe Planning
Setup. We evaluate on AI Habitat (Savva et al., 2019), an indoor robot simulator that provides
agents with observations o = h(χ) that are RGB camera images. The safety constraint is to avoid
colliding with obstacles such as furniture in the environment. We use the learned policy ∏ available
with the environment. Then, we train a recoverability predictor, trained using 500 rollouts with a
horizon of 100. We calibrate this model on an additional n rollouts.
Baselines. We compare to three baselines: (i) histogram binning—i.e., our approach but using the
means of each bin rather than upper/lower bounds, (ii) a naive approach of choosing Y = 0.5, and
(iii) a naive but adaptive approach of choosing Y = ξ, called "g-naive”.
Metrics. We measure both the safety rate and the success rate; in particular, a rollout is successful
if the robot reaches its goal state, and a rollout is safe if it does not reach any unsafe states.
Results. We show results in Figure 4a. The desire safety rate ξ is shown by the dotted line—i.e., we
expect the safety rate to be above this line. As can be seen, our approach achieves the desired safety
rate. While it sacrifices success rate, this is because the underlying learned policy ∏ is frequently
unsafe; in particular, it is only safe in about 30% of rollouts. The naive approach fails to satisfy the
safety constraint. The ξ-naive approach tends to be optimistic, and also fails when ξ = 0.03 (Figure
4b). The histogram baseline performs similarly to our approach. The main benefit of our approach
is providing the absolute guarantee on safety, which the histogram baseline does not provide. Thus,
in this case, our approach can provide this guarantee while achieving similar performance. Figure
4b shows the safety rate as we vary the desired safety rate ξ. Both our approach and the baseline
satisfy the desired safety guarantee, whereas the naive approaches do not always do so.
6 Conclusion
We have proposed a novel algorithm for calibrated prediction that provides PAC guarantees, and
demonstrated how our approach can be applied to fast DNN inference and safe planning. There are
many directions for future work—e.g., leveraging these techniques in more application domains and
extending our approach to settings with distribution shift (see Appendix F.1 for a discussion).
Acknowledgments
This work was supported in part by AFRL/DARPA FA8750-18-C-0090, ARO W911NF-20-1-0080, DARPA
FA8750-19-2-0201, and NSF CCF 1910769. Any opinions, findings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect the views of the Air Force
Research Laboratory (AFRL), the Army Research Office (ARO), the Defense Advanced Research Projects
Agency (DARPA), or the Department of Defense, or the United States Government.
9
Published as a conference paper at ICLR 2021
References
A. K. Akametalu, J. F. Fisac, J. H. Gillula, S. Kaynama, M. N. Zeilinger, and C. J. Tomlin.
Reachability-based safe learning with gaussian processes. In 53rd IEEE Conference on Deci-
Sion and Control, pp.1424-1431, 2014.
Mohammed Alshiekh, R. Bloem, R. Ehlers, Bettina Konighofer, S. Niekum, and U. Topcu. Safe
reinforcement learning via shielding. ArXiv, abs/1708.08611, 2018.
Osbert Bastani. Safe planning via model predictive shielding, 2019.
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for
efficient inference. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 527-536. JMLR. org, 2017.
Lawrence D Brown, T Tony Cai, and Anirban DasGupta. Interval estimation for a binomial propor-
tion. Statistical science, pp. 101-117, 2001.
Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C. Duchi. Robust validation: Confident
predictions even when distributions shift, 2020.
Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the
case of the binomial. Biometrika, 26(4):404-413, 1934.
Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. Robust guarantees for perception-based
control, 2019.
Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal
of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12-22, 1983.
Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
nature, 542(7639):115-118, 2017.
Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and
Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1039-1048, 2017.
J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula, and C. J. Tomlin. A general
safety framework for learning-based control in uncertain robotic systems. IEEE Transactions on
Automatic Control, 64(7):2737-2752, 2019.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam
Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al.
Development and validation of a deep learning algorithm for detection of diabetic retinopathy in
retinal fundus photographs. Jama, 316(22):2402-2410, 2016.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. arXiv preprint arXiv:1706.04599, 2017.
HO Hartley and ER Fitch. A chart for the incomplete beta-function and the cumulative binomial
distribution. Biometrika, 38(3/4):423-426, 1951.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting.
In European Conference on Computer Vision, pp. 201-214. Springer, 2012.
10
Published as a conference paper at ICLR 2021
Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in
Neural Information Processing Systems,pp. 3792-3803, 2019.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
S. Li and O. Bastani. Robust model predictive shielding for safe reinforcement learning with stochas-
tic dynamics. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp.
7166-7172, 2020.
Shuo Li and Osbert Bastani. Robust model predictive shielding for safe reinforcement learning
with stochastic dynamics. In 2020 IEEE International Conference on Robotics and Automation
(ICRA), pp. 7166-7172. IEEE, 2020.
Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized heartsteps: A
reinforcement learning algorithm for optimizing physical activity. Proceedings of the ACM on
Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1):1-22, 2020.
Allan H Murphy. Scalar and vector partitions of the probability score: Part i. two-state situation.
Journal of Applied Meteorology, 11(2):273-282, 1972.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated prob-
abilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015.
Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural
networks via calibrated prediction. In International Conference on Learning Representations,
2020a. URL https://openreview.net/forum?id=BJxVI04YvB.
Sangdon Park, Osbert Bastani, James Weimer, and Insup Lee. Calibrated prediction with covariate
shift via unsupervised domain adaptation. In The 23rd International Conference on Artificial
Intelligence and Statistics, 2020b.
John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likeli-
hood methods. Advances in large margin classifiers, 1999.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4433-4441. PMLR, 2018.
Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A
Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), 2019.
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference
via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern
Recognition (ICPR), pp. 2464-2469. IEEE, 2016.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal predic-
tion under covariate shift. Advances in Neural Information Processing Systems, 32:2530-2540,
2019.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
11
Published as a conference paper at ICLR 2021
Kim Wabersich and Melanie Zeilinger. Linear model predictive safety certification for learning-
based control. 03 2018. doi: 10.1109/CDC.2018.8619829.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-
namic routing in convolutional networks. In Proceedings of the European Conference on Com-
Puter Vision (ECCV),pp. 409-424, 2018.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In In Proceedings of the Eighteenth International Conference on
Machine Learning. Citeseer, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pp. 694-699. ACM, 2002.
12
Published as a conference paper at ICLR 2021
A Connection to PAC Learning Theory
We explain the connection to PAC learning theory. First, note that We can represent C asa confidence
interval around the empirical estimate of cf(x)— i.e., ^^(x) ：= P3 y0)∈sx l(y0 = y(χ0))∕∣Sχ∣,
where Sx = {(x0, y0) | p^(x0) ∈ BK^(x)}. Then, we can write
C(x) = [c^(x) - gx,c∕(x) + (≡x]∙
In this case, (3) is equivalent to
FZ 〜Dn ∕∖ Cf(X)- %^(x) ≤ cf(x) ≤ Cf(X) + 疏 ^(x) ≥ 1 - δ,	(IO)
x∈X
for some c「^ι,...,CK, eκ. In this bound, “approximately” refers to the fact that the empirical
estimate Cf(X) is within E of the true value Cf(X), and “probably” refers to the fact that this error
bound holds with high probability over the training data Z 〜Dn. By abuse of terminology, we
refer to the confidence interval predictor C as PAC rather than just Cf(X).
Alternatively, we also have the following connection to PAC learning theory:
Definition 3 Given C, δ ∈ R>0 and n ∈ N, C is probably approximately correct (PAC) if, for any
distribution D, we have
FZ〜Dn [fx〜D [c∕(X) ∈ C(x; f, Z)i ≥ 1 -Ei ≥ 1 - 6.	(II)
The following theorem shows that the proposed confidence coverage predictor C satisfies the PAC
guarantee in Definition 3.
13
Published as a conference paper at ICLR 2021
Theorem 5 Our confidence coverage predictor C satisfies Definition 3 for all e, δ ∈ R>0, and
n ∈ N.
Proof. We exploit the independence of each bin for the proof. Let θκ八功:=c∕(x), which is the
parameter of the Binomial distribution of the κf^(x)th bin, the following holds:
IP IP hc^(x) ∈ C(x; f, Z, δ)i ≥ 1 — E
Z〜Dn X〜D L J	」
P
X〜D
P
Z〜Dn
P
Z〜Dn
'	K
Cf(X) ∈ C(x; f, Z, δ) ∧ V p(x) ∈ Bk
.	k=1	.
≥ 1 — E
K
X IP [c ^(x) ∈ C(xf,Z,δ)八 P(X) ∈
X 〜D L J
Lk=I
≥ 1 — E
K
F	XF	I。^(x)	∈ C(x；f，Zδ)∣	P(X)	∈ Bk] IP [p(x)	∈ Bk]	≥ 1 — e
Z〜Dn	JlX〜D	j j	I	-I X〜D
_k = 1	.
P
Z〜Dn
K
X IP
扃SD
八	A
θk ∈ θcp
P(X) ∈ Bk IP [p(x) ∈ Bk] ≥ 1 — e
X〜D
P
Z〜Dn
≥ IP
Z〜Dn
K
Xi
,k = 1
K
Xi
,k = 1
IP [p(x) ∈ Bk] ≥ 1 — e
X〜D
IP [p(x) ∈ Bk] ≥ 1 — E
X〜D
P
Z〜Dn
K
Xi
,k = 1
八	A
θk ∈ Θcp
P
Z〜Dn
P
Z〜Dn
1≥1
—E
P
Z〜Dn
K
A1
_k = 1
K
P
Z〜Dn
E
K
X IP [p(x) ∈ Bk] ≥ 1 —
%X〜D
Lk=I
K
ʌ
八1 θk ∈ Θcp
,k=1 -
1
ʌ
P
八 1 θk ∈ Θcp
k=1
八	A
θk ∈ Θcp
八 θk ∈ ΘCP
.k=1
K
1
≥ 1 — δ,
where the last inequality holds by the union bound.
□
14
Published as a conference paper at ICLR 2021
B Proof of Theorem 1
We prove this by exploiting the independence of each bin. Recall that C(X) := [c^(χ) - 6χ, Cf(X) +
cx], and the interval is obtained by applying the CIoPPer-Pearson interval with confidence level K
at each bin. Then, the following holds due the Clopper-Pearson interval for all k ∈ {1, 2, . . . , K}:
p h|c/,k- c^,k | > ek i ≤ K
where Cfk := Cf(X) and Cfk := Cf(X) for X such that κ∕(x) = k, and Ek := max(gχ,G). By
applying the union bound, the following also holds:
K
IP ^ lc^,k- c^,k 1 > £k ≤ δ,
k=1
Considering the fact that X is partitioned into K spaces due to the binning and the equivalence form
(10) of the PAC criterion in Definition 3, the claimed statement holds.
C Proof of Theorem 2
We drop probabilities over (x, y)〜 D. First, We decompose the error of a cascading classifier
F [yc (x) = y] as follows:
F [yc(x) = y] = F [y。(x) = y ∧ GC(x) = yM(x) ∨ yc(x) = yM(x))]
=F [(yc(x) = y ∧ yc(x) = yM(x)) ∨ GC(x) = y ∧ yc(x) = yM(x))]
=F [yc (x) = y ∧ ^ (x) = IyM (x)] + F [yc (x) = y ∧ ^ (x) = IyM (x)],
where the last equality holds since the events of y。(x) = r^M (x) and of y/c (x) = yM (x) are disjoint.
Similarly, for the error of a slow classifier IP [yM (x) = y], we have:
F [yM(x) = y] = F [yM(x) = y ∧ yc(X) = yM(x)] + F [yM(X) = y ∧ yc(X)= yM(x)].
Thus, the error difference can be represented as follows:
F [yc (x) = y] -F [yM(X) = y]
=F [yc (x)	= y ∧	yc (X)	=	yM (x)]	- F	[yM (X)	= y ∧ yc (x) =	yM (x)] .	(12)
To complete the proof, we need to upper bound (12) by ξ. Define the following events:
m-1
Dm := ^ (Pi(X) < Yi) ∧ Pm(X) ≥ Ym	(∀m ∈{1,∙∙∙,M - 1})
i=1
M-1
DM := ^ (Pi(X) < Yi)
i=1
EC := yc (x) = yM (x)
Em := ym(x) = yM (x)	(∀m ∈ {1,…,M - 1})
FC = yc (x) = y
Fm = ym(x) = y	(∀m ∈ {1,…,M - 1})
G := yM (x) = y,
15
Published as a conference paper at ICLR 2021
where D1 , D2 , . . . , DM form a partition of a sample space. Then, we have:
F [yc (χ) = y ∧ yc (χ) = vm (χ)] = F [FC ∧ EC]
M
=IP FC ∧ EC ∧ _ Dm
m=1
M
=IP _ (FC ∧ Ec ∧ Dm)
m=1
M
=X [F [FC ∧ EC ∧ Dm]
m=1
M
=X F [Fm ∧ Em ∧ Dm ]
m=1
M
=X F [Fm | Em ∧ Dm] ∙ IP [Em ∧ Dm],
m=1
Similarly, we have:
M
F [^M (x) = y ∧ yC (x) = yM (x)] = X F [G | Em ∧ Dm] ∙ F [Em ∧ Dm].
m=1
Thus, (12) can be rewritten as follows:
F [y^C(χ) = y] - F [yM (X)= y]
M
=X (F [Fm 1 Em ∧ Dm] ∙ F [Em ∧ Dm] -F [G | Em ∧ Dm] ∙ F [Em ∧ Dm])
m=1
M
=	(em - e0m )
m=1
M-1
=	(em - e0m )
m=1
≤ ξ,
where the last equality holds since eM - e0M = 0, and the last inequality holds due to (6) with
probability at least 1 - δ, thus proves the claim.
D	Proof of Theorem 3
Suppose there is γ1 which is different to γj and produces a faster cascading classifier than the
cascading classifier with γj. Since Yi is the optimal solution of (6), γ1 > γj. This further implies
that the less number of examples exits at the first branch of the cascading classifier with γ10 , but these
examples are classified by the upper, slower branch. This means that the overall inference speed of
the cascading classifier with γ10 is slower then that with γ1i , which leads to a contradiction.
E Proof of Theorem 4
For clarity, we use r to denote a state x is “recoverable” (i.e., yi (x) = 0) and u to denote a state
x is “un-recoverable” (i.e., yi(x) = 1). Now, note that a rollout ζ(x0, πshield) := (x0, x1, . . . )
is unsafe if (i) at some step t, We have yi(Xt) = U (i.e., Xt is not recoverable), yet y(ot) = r,
where ot = h(xt) (i.e., y predicts Xt is recoverable), and furthermore (ii) for every step i ≤ t - 1,
y*(xi) = y(oi) = r—i.e.,
∞	t-1
Punsafe =%〜DnShield	V I ^	(j* (Xi)= r ∧ y(Oi)= r)	∧	(y* (Xt)= U ∧	y(Ot)=	r)	)	. (13)
t=0 i=0
16
Published as a conference paper at ICLR 2021
Condition (i) is captured by the second parenthetical inside the probability; intuitively, it says that
y(ot) is a false negative. Condition (ii) is captured by the first parenthetical inside the probability;
intuitively, it says that y(oi) is a true negative for any i ≤ t - 1. Next, let the event Et be
t-1
Et := ^ y*(xi) = r ∧『(Xt) = u,
i=0
then the following holds:
%~D3
t=0	i=0
U ∧ y(ot)
%~。2
WDnshield
∞	t-1	t-1
V ( ^ “(Xi) = r ∧ y*(xt) = U) ∧ I ^ y(θi) = r ∧ y(ot) = r I I
t=0 i=0	i=0
∞	t-1
V I Et ∧ ^ y(θi) = r ∧ y(ot) = r I
t=0	i=0
∞
≤ %~Dnshield	V (Et ∧ y(ot) = r)
t=0
Recall thatP := P= Fξ~D∏[Et] andPD(o) := P∞=opd∏(o | Et) ∙ Fξ~D∏[Et]∕p; then we can
upper-bound (13) as follows:
punsafe ≤ Fξ~D∏shield
∞
V (Et ∧ y(ot) = r)
t=0
∞
=X lpξ~D∏s≡1 [Et ∧ y(ot) = r]
t=0
∞
=X ιpξ~D∏shield[y(ot) = r | Et] ∙ PgmshJEt]
t=0
∞
=X lpξ~D∏shield [y(o) = r 1 Et] ∙ lpξ~D∏shield [Et]
t=0
∞
=X / 1(y(o)=吟∙ PDnShield (O 1 Et) ∙ Pξ~D∏shield[Et] do
t=0
Z∞
1(y(o) = r) X PDnShield (O 1 Et) ∙ Pξ~Dnshield[Et] d0
t=0
Z∞
l(y(o) = r) XPDn(O | Et) ∙ IPξ~Dn [Et] do
t=0
=/ l(y(o) = r)PD(o)Pdo
=P ∙ ≡ζ~D[y(o) = r],
where we use the fact that Et are disjoint by construction for the first equality, and we use O without
time index t for the third equality since it clearly represents the last observation if it is conditioned
on Et. Moreover, the last inequality holds due to the following: (i) PDnShield(o | Et) = PDn (o | Et),
since Et implies that the backup policy of ∏shield isn,t activated UP to the step t, so ∏shield = ∏, and
(ii) Fξ~Dnsh把H [Et] ≤ Fξ~D^ [Et], since ∏shield is less likely to reach unsafe states by its design than
∏. Thus, the constraint in (9) implies PunSafe ≤ ξ with probability at least 1 - δ, so the claim follows.
17
Published as a conference paper at ICLR 2021
F	Additional Discussion
F.1 Limitation to On-Distribution Setting
Our PAC guarantees (i.e., Theorems 1 & 5) transfer to the test distribution if it is identical to the
validation distribution. We believe that providing theoretical guarantees for out-of-distribution data
is an important direction; however, we believe that our work is an important stepping stone to-
wards this goal. In particular, to the best of our knowledge, we do not know of any existing work
that provides theoretical guarantees on calibrated probabilities even for the in-distribution case. One
possible direction is to use our approach in conjunction with covariate shift detectors—e.g., (Gretton
et al., 2012). Alternatively, it may be possible to directly incorporate ideas from recent work on cali-
brated prediction with covariate shift (Park et al., 2020b) or uncertainty set prediction with covariate
shift (Cauchois et al., 2020; Tibshirani et al., 2019). In particular, we can use importance weighting
q(x)/p(x), where p(x) is the training distribution and q(x) is the test distribution, to reweight our
training examples, enabling us to transfer our guarantees from the training set to the test distribution.
The key challenge is when these weights are unknown. In this case, we can estimate them given a
set of unlabeled examples from the test distribution (Park et al., 2020b), but we then need to account
for the error in our estimates.
F.2 Baselines
The following includes brief descriptions on baselines that we used in experiments.
Histogram binning. This algorithm calibrates the top-label confidence prediction of f by sorting the
calibration examples (x, y) into bins Bi based on their predicted top-label confidence—i.e., (x, y)
is associated with Bi if P(X) ∈ Bi. Then, for each bin, it computes the empirical confidence
p^i := ∣s1^ P(X y)∈si l(y(x) = y), where Si is the set of labeled examples that are associated with
bin Bi—i.e., the empirical counterpart of the true confidence in (2). Finally, during the test-time, it
returns a predicted confidence Pi for all future test examples X if P(X) ∈ Bi.
(1 - ξ0 )-softmax. In fast DNN inference, a threshold can be heuristically chosen based on the
desired relative error ξ and the validation error of the slow model. In particular, when a cascading
classifier consists of two branches—i.e., M = 2, the threshold of the first branch is chosen by
γ1 = 1 - ξ0 , where ξ0 is the sum of ξ and the validation error of the slow model. We call this
approach (1 - ξ0 )-softmax.
(1 - ξ0 )-temperature scaling. A more advanced approach is to first calibrate each branch to get
better confidence. We consider using temperature scaling to do so—i.e., we first calibrate each
branch using the temperature scaling, and then use the branch threshold by γ1 = 1 - ξ0 when
M = 2. We call this approach (1 - ξ0)-temperature scaling.
F.3 Calibration: Induced Intervals for ECE and Reliability Diagram
ECE. The expected calibration error (ECE), which is one way to measure calibration performance,
is defined as follows:
W (XXSjP(X)招(XXSjI I y)
where J is the total number of bins for ECE, S ⊆ X × Y is the evaluation set, and Sjis the set of
labeled examples associated to the jth bin—i.e., (χ,y) ∈ Sj if P(X) ∈ Bj.
A confidence coverage predictor C(x) outputs an interval instead of a point estimate P(X) of the
confidence. To evaluate the confidence coverage predictor, we remap intervals using the ECE
formulation. In particular, We equivalently represents C(x) by a mean confidence c^(x) and dif-
ferences from the mean—i.e., C(x) = [c(x), c(x)] = [_c^(χ) - Cχ, c^(x) + 品](see Appendix
A for a description on this equivalent representation). Then, we sort each labeled example into
18
Published as a conference paper at ICLR 2021
bins using C^(χ) to form Sj. Next, We consider an interval instead of P(X) to compute ECE—
i.e., ECEinduced ：= [ECE, EcE], where
ECE = X W Pj %
j=1
ECE = X W P"
Pj
-TSr	X	I(O(X) = y)
1 j 1 (χ,y)∈Sj
-τSτ	X	I(O(X) = y)
|Sj| (x,y)∈Sj
and
Conj := [Confj, Confj ]:=
min c(x), max c(x)
(x,y)∈Sj ~	(x,y)∈Sj	.
Reliability diagram. This evaluation technique is a pictorial summary of the ECE, where the X-
axis represents the mean confidence 嵩 P(X y)∈sj P(X) for each bin, and the y-axis represents the
mean accuracy 尚 P(X 期由§ l(y(χ) = y) for each bin. If an interval from a confidence coverage
predictor is given, then the mean confidence is replaced by Confj, resulting in visualizing an interval
instead of a point.
F.4 Fast DNN Inference: Cascading Classifier Training
We describe a way to train a cascading classifier with M branches. Basically, we consider to inde-
pendently train M different neural networks with a shared backbone. In particular, we first train the
Mth branch using a training set by minimizing a cross-entropy loss. Then, we train the (M - 1)th
branch, which consists of two parts: a backbone part from the Mth branch, and the head of the
(M - 1)th branch. Here, the backbone part is already trained in the previous stage, so we do not
update the backbone part and only train the head of this branch using the same training set by mini-
mizing the same cross-entropy loss (with the same optimization hyperparameters). This step is done
repeatedly down to the first branch.
F.5 Safe Planning: Data Collection from a Simulator
We collect required data from a simulator, where a given policy ∏ is already learned over the simu-
lator. We describe how we form the necessary data from rollouts sampled from the simulator.
First, to sample a rollout Z, we use ∏ from a random initial state x° 〜D; we denote the sequence of
states visited as a rollout ζ(xo,∏) := (χo,χι,...). We denote the induced distribution over rollouts
by Z 〜Dn. Note that the Z contains unsafe states since ∏ is potentially unsafe. However, when
constructing our recoverability classifier, we only use the sequence of safe states, followed by a
single unsafe state. In particular, we let W be a set of i.i.d. sampled rollouts Z 〜Dn. Next, for
a given rollout, we consider the observation in the first unsafe state in that rollout (if one exists);
we denote the distribution over such observations by D. Finally, we take Z to be a set of sampled
observations o 〜D.
G	Additional Experiments
G. 1 Calibration
19
Published as a conference paper at ICLR 2021
o.o
o.o
ECE = 4.79%
AuB-lnuuB
o.o
0.2	0.4	0.6	0.8	1.0
confidence
(a) Naive Softmax
ECE = 0.99%
estimated
----ideal
ratio
confidence
α,-dEex0,
estimated
ideal
ratio
confidence
(b) Temperature Scaling
ECE = 0.99% ∈ [0.00%, 3.76%]
α,-dEex0,
confidence
(c) HiStogram binning	(d) PropoSed
Figure 5: Calibration compariSon in reliability diagramS and ECES. The Size of validation Set for
calibration iS 20, 000. The blue hiStogram repreSentS the example ratio for the correSponding bin.
The diagonal line labeled “ideal” iS the beSt reliability diagram, which produceS the zero ECE.
The eStimated reliability diagram iS repreSented “eStimated” in dotted red. (a) The naive Softmax
output from a neural network iS unreliable in ECE. (b, c) The temperature Scaling and hiStogram
binning are fairly good calibration approacheS, which decreaSeS ECE. (d) The propoSed approach
generateS “induced” intervalS (See Appendix F.3) on top of the hiStogram binning approach, where
each interval containS the ideal diagonal line with high probability. Moreover, the propoSed one alSo
produceS induced ECE interval, where it containS the zero ECE with high probability.
20
Published as a conference paper at ICLR 2021
0.8
AuB-lnuuB
0.2
0.4	0.6
confidence
o.o
AUe-IrDUe
OQe」-dEex
confidence
(a) Naive Softmax
ECE = 2.14%
(c) HiStogram binning
(b) Temperature scaling
ECE = 2.14% ∈ [0.00%, 12.09%]
O-".Iα,-dEex0,

confidence
(d) PropoSed
Figure 6: Calibration compariSon via reliability diagramS and ECE. The Size of validation Set for
calibration iS 2, 000. See the caption of Figure 5 for interpretation. For the induced intervalS, the
length of the interval iS larger than that with n = 20, 000 due to the eStimation error.
Figure 7: Accuracy-confidence plot. ThiS plot iS uSeful for chooSing a proper confidence
threShold that achieveS a deSired conditional accuracy (LakShminarayanan et al., 2017); the x-
axiS iS the confidence threShold t and the y-axiS iS the empirical value of the conditional ac-
curacy P [y(x) = y | P(X) ≥ t]. Since our approach outputs an interval [c(x),c(x)], We plot
IP [y(x) = y | c(x) ≥ t] and IP [y(x) = y | c(x) ≥ t] for the upper and lower bound of the green
area, respectively.
21
Published as a conference paper at ICLR 2021
G.2 Fast DNN Inference
22
5 4
2 2
(a) n = 5, 000
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	×101°
22
5 4
2 2
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	X"10
6 5 4
2 2 2
一estimated trade-off
■ rigorous (ours)
■ hist. bin. (baseline)
■ (1 - ξ,)-softmax (baseline)
ɪ (l-ξ/)-temp. (baseline)
----desired error
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.：
MACS	×10w
(c) n = 15, 000
(b) n = 10, 000
6 5 4
2 2 2
(％)
(d) n = 20, 000
Figure 8: Ablation study on various n for ξ = 0.02, δ = 10-2, and M = 2. Each plot uses the
shown number of samples n with the same ξ and δ. The “estimated trade-off” represents the error
and MACs trade-off depending on threshold γ1. The markers show the trade-off by the baselines.
The “desired error” is a user-specified error bound, where the error of each method need to be below
of this line. The proposed approach reduces the inference speed as the number of sample increases,
while satisfying the desired error bound. The baselines are either fails to satisfy the desired error
bound or overly conservative to satisfy the bound.
22
Published as a conference paper at ICLR 2021
6 5 4
2 2 2
(％)
一estimated trade-off
■	rigorous (ours)
■	hist. bin. (baseline)
■	(1 - ξ,)-softmax (baseline)
I (l-ξ,)-temp. (baseline)
----desired error
6 5 4
2 2 2
(％)
—estimated trade-off
■	rigorous (ours)
■	hist. bin. (baseline)
■	(1 -ξ/)-softma× (baseline)
I (1 -ξ/)-temp. (baseline)
----desired error
6 5 4
2 2 2
一estimated trade-off
rigorous (ours)
hist. bin. (baseline)
(1 -针 ASoftmaX (baseline)
(l-ξ/)-temp. (baseline)
desired error
22
0.4	0.5	0.6	0.7	0.8	0.9
MACS
(c) ξ = 0.03
1.0	1.1	1.2
Xl 0ω
6 5 4
2 2 2
22
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	×101°
(d) ξ = 0.04
—estimated trade-off
■	rigorous (ours)
■	hist. bin. (baseline)
-	ɪ- (1 - ξ/)-softmax (baseline)
ɪ (1 -ξ/)-temp. (baseline)
----desired error
Figure 9: Ablation study on various ξ for n = 20, 000, δ = 10-2, and M = 2. Each plot uses
the shown desired relative error ξ with the same n and δ. The “estimated trade-off” represents the
error and MACs trade-off depending on threshold γ1. The markers show the trade-off by the shown
baselines. The “desired error” is a user-specified error bound, where the error of each method need
to be below of this line. The proposed approach allows more error as at most specified by the desired
error, to reduce the inference speed. Other baselines either overly increase the error or conservatively
maintain overly low error.
23
Published as a conference paper at ICLR 2021
6 5 4
2 2 2
(％)」ol」a
22
0.4	0.5	0.6	0.7	0.8	0.9
MACS
6 5 4
2 2 2
(％)」ol」a
—estimated trade-off
■ rigorous (ours)
■ hist. bin. (baseline)
■ (1 -ξ/)-softma× (baseline)
I (1 -ξ/)-temp. (baseline)
----desired error
(a) δ = 10-1
6 5 4
2 2 2
一estimated trade-off
■ rigorous (ours)
■ hist. bin. (baseline)
■ (1 - ξ,)-softmax (baseline)
ɪ (l-ξ/)-temp. (baseline)
----desired error
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	×10w
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	×10υ,
(b) δ = 10-2
6 5 4
2 2 2
0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
MACS	×10υ,
—estimated trade-off
rigorous (ours)
hist. bin. (baseline)
(l-ξ7)-softmax (baseline)
(1 -ξ/)-temp. (baseline)
desired error
(c) δ = 10-3
(d) δ = 10-4
Figure 10: Ablation study on various δ for n = 20, 000, ξ = 0.02, and M = 2. Each plot uses
the shown misconfidence level δ with the same n and ξ. The “estimated trade-off” represents the
error and MACs trade-off depending on threshold γ1. The markers show the trade-off by the shown
baselines. The “desired error” is a user-specified error bound, where the error of each method need
to be below of this line. The proposed approach produces larger error gap toward the desired error as
we enforce stronger confidence level—i.e., from δ = 10-1 to δ = 10-4. Note that other baselines
do not depend on δ as designed.
24
Published as a conference paper at ICLR 2021
G.3 Safe Planning
(a) n = 20, 000
(b) n = 15, 000
(d) n = 5, 000
(c) n = 10, 000
Figure 11: Ablation study on various n for ξ = 0.1 and δ = 10-2. Each plot uses the shown sample
size n with the same ξ and δ. The “naive”, “g-naive”, and “histogram binning" represent baseline
results on the safety rate and success rate. The proposed approach is labeled “rigorous”. The desired
safety rate is a user-specified rate, where the safety rate of each method need to be above of this line.
The safety rate of the proposed approach is above the desired safety rate, and it tends to be closer to
the desired safety rate as n increases.
25
Published as a conference paper at ICLR 2021
(a) ξ = 0.2
(b) ξ = 0.15
(c) ξ = 0.1
(d) ξ = 0.05
Figure 12: Ablation study on various ξ for n = 20, 000 and δ = 10-2. Each plot uses the shown
desired unsafe rate ξ with the same n and δ. The “naive"，“g-naive"，and “histogram binning”
represent baseline results on the safety rate and success rate. The proposed approach is labeled
“rigorous". The desired safety rate is a user-specified rate， where the safety rate of each method
need to be above of this line. The safety rate of the proposed approach is closely above the desired
safety rate to satisfy the safety constraint. However， the naive can violate the safety constraint， and
the ξ-naive can be overly optimistic. The histogram binning and ξ-naive look empirically fine， but
they can in theory violate desired safety rate (e.g., see Figure 4b).
26
Published as a conference paper at ICLR 2021
(a) δ = 10-1
(b) δ = 10-2
Figure 13: Ablation study on various δ for n = 20, 000 and ξ = 0.1. Each plot uses the shown
misconfidence level ξ with the same n and ξ. The “naive”, “g-naive"，and “histogram binning”
represent baseline results on the safety rate and success rate. The proposed approach is labeled
“rigorous”. The desired safety rate is a user-specified rate, where the safety rate of each method
need to be above of this line. The proposed approach produces more conservative safety rates—
i.e., larger gap between the estimated safety rate and desired safety rate—as we enforce stronger
confidence level—i.e., from δ = 10-1 to δ = 10-4. Note that other baselines do not depend on δ as
designed.
27