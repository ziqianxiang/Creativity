Published as a conference paper at ICLR 2021
Vector-output ReLU Neural Network Prob-
lems are Copositive Programs: Convex Anal-
ysis of Two Layer Networks and Polynomial-
time Algorithms
Arda Sahiner, Tolga Ergen, John Pauly & Mert Pilanci
Department of Electrical Engineering
Stanford University
{sahiner, ergen, pauly, pilanci}@stanford.edu
Ab stract
We describe the convex semi-infinite dual of the two-layer vector-output ReLU
neural network training problem. This semi-infinite dual admits a finite dimen-
sional representation, but its support is over a convex set which is difficult to char-
acterize. In particular, we demonstrate that the non-convex neural network train-
ing problem is equivalent to a finite-dimensional convex copositive program. Our
work is the first to identify this strong connection between the global optima of
neural networks and those of copositive programs. We thus demonstrate how neu-
ral networks implicitly attempt to solve copositive programs via semi-nonnegative
matrix factorization, and draw key insights from this formulation. We describe the
first algorithms for provably finding the global minimum of the vector output neu-
ral network training problem, which are polynomial in the number of samples for
a fixed data rank, yet exponential in the dimension. However, in the case of con-
volutional architectures, the computational complexity is exponential in only the
filter size and polynomial in all other parameters. We describe the circumstances
in which we can find the global optimum of this neural network training prob-
lem exactly with soft-thresholded SVD, and provide a copositive relaxation which
is guaranteed to be exact for certain classes of problems, and which corresponds
with the solution of Stochastic Gradient Descent in practice.
1	Introduction
In this paper, we analyze vector-output two-layer ReLU neural networks from an optimization per-
spective. These networks, while simple, are the building blocks of deep networks which have been
found to perform tremendously well for a variety of tasks. We find that vector-output networks reg-
Ularized with standard weight-decay have a convex semi-infinite strong dual-a convex program with
infinitely many constraints. However, this strong dual has a finite parameterization, though express-
ing this parameterization is non-trivial. In particular, we find that expressing a vector-output neural
network as a convex program requires taking the convex hull of completely positive matrices. Thus,
we find an intimate, novel connection between neural network training and copositive programs,
i.e. programs over the set of completely positive matrices (Anjos & Lasserre, 2011). We describe
algorithms which can be used to find the global minimum of the neural network training problem
in polynomial time for data matrices of fixed rank, which holds for convolutional architectures. We
also demonstrate under certain conditions that we can provably find the optimal solution to the neu-
ral network training problem using soft-thresholded Singular Value Decomposition (SVD). In the
general case, we introduce a relaxation to parameterize the neural network training problem, which
in practice we find to be tight in many circumstances.
1.1	Related work
Our analysis focuses on the optima of finite-width neural networks. This approach contrasts with
certain approaches which have attempted to analyze infinite-width neural networks, such as the
1
Published as a conference paper at ICLR 2021
Neural Tangent Kernel (Jacot et al., 2018). Despite advancements in this direction, infinite-width
neural networks do not exactly correspond to their finite-width counterparts, and thus this method
of analysis is insufficient for fully explaining their success (Arora et al., 2019).
Other works may attempt to optimize neural networks with assumptions on the data distribu-
tion. Of particular interest is (Ge et al., 2018), which demonstrates that a polynomial number of
samples generated from a planted neural network model is sufficient for extracting its parameters
using tensor methods, assuming the inputs are drawn from a symmetric distribution. If the input
distribution to a simple convolutional neural network with one filter is Gaussian, it has also been
shown that gradient descent can find the global optimum in polynomial time (Brutzkus & Glober-
son, 2017). In contrast to these works, we seek to find general principles for learning two-layer
ReLU networks, regardless of the data distribution and without planted model assumptions.
Another line of work aims to understand the success of neural networks via implicit regular-
ization, which analyzes how models trained with Stochastic Gradient Descent (SGD) find solutions
which generalize well, even without explicit control of the optimization objective (Gunasekar et al.,
2017; Neyshabur et al., 2014). In contrast, we consider the setting of explicit regularization, which
is often used in practice in the form of weight-decay, which regularizes the sum of squared norms
of the network weights with a single regularization parameter β, which can be critical for neural
network performance (Golatkar et al., 2019).
Our approach of analyzing finite-width neural networks with a fixed training dataset has
been explored for networks with a scalar output (Pilanci & Ergen, 2020; Ergen & Pilanci,
2020a;d). In fact, our work here can be considered a generalization of these results. We consider a
ReLU-activation two-layer network f : Rd → Rc with m neurons:
m
f (x) =	(x>uj)+vj>
j=1
(1)
where the function (•)+ = max(0, ∙) denotes the ReLU activation, {u7- ∈ Rd}m=ι are the first-layer
weights of the network, and {vj ∈ Rc}jm=1 are the second-layer weights. In the scalar-output case,
the weights vj are scalars, i.e. c = 1. Pilanci & Ergen (2020) find that the neural network training
problem in this setting corresponds to a finite-dimensional convex program.
However, the setting of scalar-output networks is limited. In particular, this setting cannot
account for tasks such as multi-class classification or multi-dimensional regression, which are some
of the most common uses of neural networks. In contrast, the vector-output setting is quite general,
and even greedily training and stacking such shallow vector-output networks can match or even
exceed the performance of deeper networks on large datasets for classification tasks (Belilovsky
et al., 2019). We find that this important task of extending the scalar case to the vector-output case
is an exceedingly non-trivial task, which generates novel insights. Thus, generalizing the results
of Pilanci & Ergen (2020) is an important task for a more complete knowledge of the behavior of
neural networks in practice.
Certain works have also considered technical problems which arise in our analysis, though in
application they are entirely different. Among these is analysis into cone-constrained PCA, as
explored by Deshpande et al. (2014) and Asteris et al. (2014). They consider the following
optimization problem
max u> Ru
s.t Xu ≥ 0; kuk2 = 1
(2)
u
This problem is in general considered NP-hard. Asteris et al. (2014) provide an exponential algo-
rithm which runs in O(nd) time to find the exact solution to (2), where X ∈ Rn×d and R ∈ Sd
is a symmetric matrix. We leverage this result to show that the optimal value of the vector-output
neural network training problem can be found in the worst case in exponential time with respect
to r := rank(X), while in the case of a fixed-rank data matrix our algorithm is polynomial-time.
In particular, convolutional networks with fixed filter sizes (e.g., 3 × 3 × m convolutional kernels)
correspond to the fixed-rank data case (e.g., r = 9). In search of a polynomial-time approximation
2
Published as a conference paper at ICLR 2021
to (2), Deshpande et al. (2014) evaluate a relaxation of the above problem, given as
max hR, Ui
U	(3)
s.t XUX> ≥0; tr(U) = 1; U 0
While the relaxation not tight in all cases, the authors find that in practice it works quite well for
approximating the solution to the original optimization problem. This relaxation, in particular,
corresponds to what we call a copositive relaxation, because it consists of a relaxation of the set
CPCA = {uu> : kuk2 = 1, Xu ≥ 0}. When X = I and the norm constraint is removed, CPCA is
the set of completely positive matrices (Dur, 2010). Optimizing over the set of completely positive
matrices is NP-hard, as is optimizing over its convex hull:
C := conv{uu> : Xu ≥ 0}
Thus, optimizing over C is a convex optimization problem which is nevertheless NP-hard. Various
relaxations to C have been proposed, such as the copositive relaxation used by (Deshpande et al.,
2014) above:
¢:= {U : U 占 0; XUX> ≥ 0}
In fact, this relaxation is tight, given that u ∈ Rd and d ≤ 4 (Burer, 2015; Kogan & Berman, 1993).
However, C ⊂ C for d ≥ 5, so the copositive relaxation provides a lower bound in the general case.
These theoretical results prove insightful for understanding the neural network training objective.
1.2 Contributions
•	We find the semi-infinite convex strong dual for the vector-output two-layer ReLU neural
network training problem, and prove that it has a finite-dimensional exact convex optimiza-
tion representation.
•	We establish anew connection between vector-output neural networks, copositive programs
and cone-constrained PCA problems, yielding new insights into the nature of vector-output
neural network training, which extend upon the results of the scalar-output case.
•	We provide methods that globally solve the vector-output neural network training prob-
lem in polynomial time for data matrices of a fixed rank, but for the full-rank case, the
complexity is necessarily exponential in d assuming P 6= N P.
•	We provide conditions on the training data and labels with which we can find a closed-
form expression for the optimal weights of a vector-output ReLU neural network using
soft-thresholded SVD.
•	We propose a copositive relaxation to establish a heuristic for solving the neural network
training problem. This copositive relaxation is often tight in practice.
2	Preliminaries
In this work, we consider fitting labels Y ∈ Rn×c from inputs X ∈ Rn×d with a two layer neural
network with ReLU activation and m neurons in the hidden layer. This network is trained with
weight decay regularization on all of its weights, with associated parameter β > 0. For some
general loss function '(f (X), Y), this gives us the non-convex primal optimization problem
m
p*= min ɪ '(f(X), Y) + β X(k%k2 + M k2)	(4)
uj ∈Rd 2	2 j=1
vj ∈Rc	j=1
In the simplest case, with a fully-connected network trained with squared loss1, this becomes:
mm
p* = mind 2 k X(Xuj )+v> - Y kF + 2 X (kujk2 + Ilvj-112)	⑸
uj ∈R 2 j 1	2 j 1
vj ∈Rc	j=1	j=1
1Appendix A.6 contains extensions to general convex loss functions.
3
Published as a conference paper at ICLR 2021
However, alternative models can be considered. In particular, for example, Ergen & Pilanci (2020d)
consider two-layer CNNs with global average pooling, for which we can define the patch matrices
{Xk}kK=1, which define the patches which individual convolutions operate upon. Then, the vector-
output neural network training problem with global average pooling becomes
Km	m
Pconv= mind 2 k XX(Xk Uj ) + v> - Y IlF + 2 X (kujk2 + kvjk2)	⑹
uvjj∈∈RRcd 2 k=1j=1	2j=1
We will show that in this convolutional setting, because the rank of the set of patches
M := [Xi, X2,…XK]> cannot exceed the filter size of the convolutions, there exists an
algorithm which is polynomial in all problem dimensions to find the global optimum of this
problem. We note that such matrices typically exhibit rapid singular value decay due to spatial
correlations, which may also motivate replacing it with an approximation of much smaller rank.
In the following section, we will demonstrate how the vector-output neural network problem
has a convex semi-infinite strong dual. To understand how to parameterize this semi-infinite dual in
a finite fashion, we must introduce the concept of hyper-plane arrangements. We consider the set of
diagonal matrices
D := {diag(1Xu≥0) : kuk2 ≤ 1}
This is a finite set of diagonal matrices, dependent on the data matrix X, which indicate the set
of possible arrangement activation patterns for the ReLU non-linearity, where a value of 1 indicates
that the neuron is active, while 0 indicates that the neuron is inactive. In particular, we can enumerate
the set of sign patterns as D = {Di}iP=1, where P depends on X but is in general bounded by
P ≤ 2r(e(n- I))r
forr := rank(X) (Pilanci & Ergen, 2020; Stanley et al., 2004). Note that for a fixed rank r, such as
in the convolutional case above, P is polynomial in n. Using these sign patterns, we can completely
characterize the range space of the first layer after the ReLU:
{(Xu)+ : kuk2 ≤ 1}= {DiXu : kuk2 ≤ 1, (2Di - I)Xu ≥ 0, i ∈ [P]}
We also introduce a class of data matrices X for which the analysis of scalar-output neural net-
works simplifies greatly, as shown in (Ergen & Pilanci, 2020b). These matrices are called spike-free
matrices. In particular, a matrix X is called spike-free ifit holds that
{(Xu)+ : kuk2 ≤ 1} = {Xu : kuk2 ≤ 1}∩Rn+	(7)
When X is spike-free, then, the set of sign patterns D reduces to a single sign pattern, D = {I},
because of the identity in (7). The set of spike-free matrices includes (but is not limited to) diagonal
matrices and whitened matrices for which n ≤ d, such as the output of Zero-phase Component
Analysis (ZCA) whitening. The setting of whitening the data matrix has been shown to improve
the performance of neural networks, even in deeper settings where the whitening transformation is
applied to batches of data at each layer (Huang et al., 2018). We will see that spike-free matrices
provide polynomial-time algorithms for finding the global optimum of the neural network training
problem in both n and d (Ergen & Pilanci, 2020b), though the same does not hold for vector-output
networks.
2.1	Warm-Up: Scalar-Output Networks
We first present strong duality results for the scalar-output case, i.e. the case where c = 1.
Theorem (Pilanci & Ergen, 2020) There exists an m* ≤ n + 1 such that if m ≥ m*, for
all β > 0, the neural network training problem (5) hasa convex semi-infinite strong dual, given by
max	-Ukz - yk2 + Ukyk2
z： ∣z>(Xu)+∣≤β ∀kU∣2≤1	2	2
(8)
Furthermore, the neural network training problem has a convex, finite-dimensional strong bi-dual,
given by
1P	P
p*=(2Di-I)Xwn≥0∀i∈M2k XDiX(Wi- Vi)- yk2+βX kwik2+kvik2	⑼
(2Di-I)Xvi≥0 ∀i∈[P]	i=1	i=1
4
Published as a conference paper at ICLR 2021
This is a convex program with 2dP variables and 2nP linear inequalities. Solving this problem with
standard interior point solvers thus has a complexity of O(d3r3(n)3r), which is thus exponential in
r, but for a fixed rank r is polynomial in n.
In the case of a spike-free X, however, the dual problem simplifies to a single sign pattern
constraint D1 = I . Then the convex strong bi-dual becomes (Ergen & Pilanci, 2020b)
p* = min 1 kX(W -V)- yk2+ β(kwk2 + kvk2)	(IO)
Xw≥0 2
Xv≥0
This convex problem has a much simpler form, with only 2n linear inequality constraints and 2d
variables, which therefore has a complexity of O(nd2). We will see that the results of scalar-output
ReLU neural networks are a specific case of the vector-output case.
3	Strong Duality
3.1	Convex Semi-infinite duality
Theorem 1 There exists an m* ≤ nc + 1 such that if m ≥ m*, for all β > 0, the neural network
training problem (5) has a convex semi-infinite strong dual, given by
p* = d* :=	max 一 “-1 kZ - Y kF + 1 kY kF	(11)
P	z： kz>(Xumk2≤β∀kuk2≤1 2k	kF+2k kF	()
Furthermore, the neural network training problem has a convex, finite-dimensional strong bi-dual,
given by
1P	P
P* = Vi∈κmi∀n∈[P] 2k XDiXVi- YkF +βX kVik*	(12)
i=1	i=1
for convex sets Ki
Ki := conv{ug> : (2Di - I)Xu ≥ 0, kgk2 ≤ 1}	(13)
The strong dual given in (11) is convex, albeit with infinitely many constraints. In contrast, (12) is a
convex problem has finitely many constraints. This convex model learns a sparse set of locally linear
models Vi which are constrained to be in a convex set, for which group sparsity and low-rankness
over hyperplane arrangements is induced by the sum of nuclear-norms penalty. The emergence of
the nuclear norm penalty is particularly interesting, since similar norms have also been used for rank
minimization problems (CandeS & Tao, 2010; Recht et al., 2010), proposed as implicit regularizers
for matrix factorization models (Gunasekar et al., 2017), and draws similarities to nuclear norm
regularization in multitask learning (Argyriou et al., 2008; Abernethy et al., 2009), and trace Lasso
(Grave et al., 2011). We note the similarity of this result to that from Pilanci & Ergen (2020), whose
formulation is a special case of this result with c = 1, where Ki reduce to
Ki = {u : (2Di - I)Xu ≥ 0} ∪ {-u : (2Di - I)Xu ≥ 0}
from which we can obtain the convex program presented by Pilanci & Ergen (2020). Further, this
result extends to CNNs with global average pooling, which is discussed in Appendix A.3.2.
Remark 1.1 It is interesting to observe that the convex program (12) can be interpreted as a piece-
wise low-rank model that is partitioned according to the set of hyperplane arrangements of the data
matrix. In other words, a two-layer ReLU network with vector output is precisely a linear learner
over the features [DιX, ∙…DPX], where convex constraints and group nuclear norm regulariza-
tion PiP=1 kVi k* is applied to the linear model weights. In the case of the CNNs, the piecewise
low-rank model is over the smaller dimensional patch matrices {Xk}kK=1, which result in signifi-
cantly fewer hyperplane arrangements, and therefore, fewer local low-rank models.
5
Published as a conference paper at ICLR 2021
3.2 Provably Solving the Neural Network Training Problem
In this section, we present a procedure for minimizing the convex program as presented in (12) for
general output dimension c. This procedure relies on Algorithm 5 for cone-constrained PCA from
(Asteris et al., 2014), and the Frank-Wolfe algorithm for constrained convex optimization (Frank
et al., 1956). Unlike SGD, which is a heuristic method applied to a non-convex training problem,
this approach is built upon results of convex optimization and provably finds the global minimum of
the objective. In particular, we can solve the problem in epigraph form,
1P
p* = min Vi∈m i∀n∈[P] 2 kX Di XVi- Y kF+βt	(14)
PP=IkVik*≤t	i=1
where we can perform bisection over t in an outer loop to determine the overall optimal value of
(12). Then, we have the following algorithm to solve the inner minimization problem of (14):
Algorithm 1:
1.	Initialize {Vi(0) }iP=1 = 0.
2.	For steps k:
(a)	For each i ∈ [P] solve the following subproblem:
(k)
si
P
hDiXug>,Y-XDjXVj(k)i
j=1
max
kuk2≤1
kgk2≤1
(2Di -I)X u≥0
And define the pairs {(ui, gi)}iP=1 to be the argmaxes of the above subproblems. This
is is a form of semi-nonnegative matrix factorization (semi-NMF) on the residual at
step k (Ding et al., 2008). It can be solved via cone-constrained PCA in O(nr) time
where r = rank(X).
(b)	For the Semi-NMF factorization obtaining the largest objective value, i* :=
arg maxi s(k), form Mik) = u*g>. For all other i = i*, simply let My) = 0.
(c)	For step size α(k) ∈ (0, 2), update
Vi(k+1) = ( -α(k))Vi(k) + tα(k)Mi(k)
The derivations for the method and complexity of Algorithm 1 are found in Appendix A.4. We have
thus described a Frank-Wolfe algorithm which provably minimizes the convex dual problem, where
each step requires a semi-NMF operation, which can be performed in O(nr) time.
3.3 Spike-free Data Matrices and Closed-Form Solutions
As discussed in Section 2, if X is spike-free, the set of sign partitions is reduced to the single
partition D1 = I . Then, the convex program (12) becomes
min
V ∈conv{ug> = Xu≥0,kg∣∣2 ≤1}
2 kXv - Y kF + βlVk*
(15)
This problem can also be solved with Algorithm 1. However, the asymptotic complexity of this
algorithm is unchanged, due to the cone-constrained PCA step. If the constraint on V were
removed, (15) would be identical to optimizing a linear-activation network. However, additional
cone constraint on V allows for a more complex representation, which demonstrates that even in
the spike-free case, a ReLU-activation network is quite different from a linear-activation network.
Recalling that whitened data matrices where n ≤ d are spike-free, for a further simplified
class of data and label matrices, we can find a closed-form expression for the optimal weights.
Theorem 2 Consider a whitened data matrix X ∈ Rn×d where n ≤ d, and labels Y with SVD of
X>Y = ic=1 σiaibi>. If the left-singular vectors of X>Y satisfy Xai ≥ 0 ∀i ∈ {i : σi > β},
there exists a closed-form Solutionfor the optimal V * to problem (15), given by
c
V* = X(σi - β)+aibi>
(16)
6
Published as a conference paper at ICLR 2021
The resulting model is a soft-thresholded SVD of X>Y , which arises as the solution of maximum-
margin matrix factorization (Srebro et al., 2005). The scenario that all the left singular vectors of
X>Y satisfy the affine constraints Xai ≥ 0 ∀i occurs when the all of the left singular vectors Y
are nonnegative, which is the case for example when Y is a one-hot-encoded matrix. In this scenario
where the left-singular vectors of X>Y satisfy Xai ≥ 0 ∀i ∈ {i : σi > β}, we note that the ReLU
constraint on V * is not active, and therefore, the solution of the ReLU-activation network training
problem is identical to that of the linear-activation network. This linear-activation setting has been
well-studied, such as in matrix factorization models by (Cabral et al., 2013; Li et al., 2017), and in
the context of implicit bias of dropout (Mianjy et al., 2018; Mianjy & Arora, 2019). This theorem
thus provides a setting in which ReLU-activation and linear-activation networks perform identically.
4	Neural networks and copositive programs
4.1	An equivalent copositive program
We now present an alternative representation of the neural network training problem with squared
loss, which has ties to copositive programming.
Theorem 3 For all β > 0, the neural network training problem (5) has a convex strong dual, given
by
*
p*
Ui∈m ∀i∈[P] 1tr (Y> (I+2 XX (DiX)Ui(Di X )>)-1Y)+ β2X tr(Ui)
(17)
for convex sets Ci, given by
Ci := conv{uu> : (2Di - I)Xu ≥ 0}	(18)
This is a minimization problem with a convex objective over P sets of convex, completely positive
cones-a copositive program, which is NP-hard. There exists a cutting plane algorithm solves this
problem in O(nr), which is polynomial in n for data matrices of rank r (see Appendix A.5). This
formulation provides a framework for viewing ReLU neural networks as implicit copositive pro-
grams, and we can find conditions during which certain relaxations can provide optimal solutions.
4.2	A copositive relaxation
We consider the copositive relaxation of the sets Ci from (17). We denote this set
Ci := {U : U 上 0, (2Di — I)XUX>(2Di — I) ≥ 0}
In general, Ci ⊆ ©、, with equality when d ≤ 4 (Kogan & Berman, 1993; Dickinson, 2013). We
define the relaxed program as
1	P	-1	P
dCp ：=	min	5tr Y> (I + 2 X(DiXM(DeX)>)	Y + β2 X tr(Ui)	(19)
Ui∈Ci ∀i∈[P]2	∖	'	£	j J	i=ι
Because of the enumeration over sign-patterns, this relaxed program still has a complexity of O(nr)
to solve, and thus does not improve upon the asymptotic complexity presented in Section 3.
4.3	Spike-free data matrices
If X is restricted to be spike-free, the convex program (19) becomes
dcp :=	min	1 tr( Y> (I + 2XUX>) 1Y )+ β2tr(U)	(20)
XUX> ≥0
With spike-free data matrices, the copositive relaxation presents a heuristic algorithm for the neural
network training problem which is polynomial in both n and d. This contrasts with the exact for-
mulations of (12) and (17), for which the neural network training problem is exponential even for a
spike-free X. Table 1 summarizes the complexities of the neural network training problem.
7
Published as a conference paper at ICLR 2021
Table 1: Complexity of global optimization for two-layer ReLU networks with scalar and vector
outputs. Best known upper-bounds are shown where n is the number of samples, d is the dimension
of the samples and r is the rank of the training data. Note that for convolutional networks, r is the
size of a single filter, e.g., a convolutional layer with a kernel size of 3 × 3 corresponds to r = 9.
	Scalar-output	Vector-output (exact)	Vector-output (relaxation)
Spike-free X	O(nd2)	O(nr)	O(n2d4)
General X	O((d )3r)	O(nr (d 产)	O(( d )3r)
5	Experiments
5.1	Does SGD always find the global optimum for neural networks ?
While SGD applied to the non-convex neural network training objective is a heuristic which works
quite well in many cases, there may exist pathological cases where SGD fails to find the global
minimum. Using Algorithm 1, we can now verify whether SGD find the global optimum. In this
experiment, we present one such case where SGD has trouble finding the optimal solution in certain
circumstances. In particular, we generate random inputs X ∈ R25×2, where the elements of X are
drawn from an i.i.d standard Gaussian distribution: Xij 〜N(0,1). We then randomly initialize
a data-generator neural network f with 100 hidden neurons and and an output dimension of 5, and
generate labels Y = f(X) ∈ R25×5 using this model. We then attempt to fit these labels using a
neural network and squared loss, with β = 10-2. We compare the results of training this network for
5 trials with 10 and 50 neurons to the global optimum found by Algorithm 1. In this circumstance,
with 10 neurons, none of the realizations of SGD converge to the global optimum as found by
Algorithm 1, but with 50 neurons, the loss is nearly identical to that found by Algorithm 1.
(a) 10 neurons
(b) 50 neurons
Figure 1: As the number of neurons increases, the solution of SGD approaches the optimal value.
5.2	Maximum-Margin Matrix Factorization
In this section, we evaluate the performance of the soft-thresholded SVD closed-form solution pre-
sented in Theorem 2. In order to evaluate this method, we take a subset of 3000 points from the
CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009). For each dataset, we first de-mean
the data matrix X ∈ R3000×3072 , then whiten the data-matrix using ZCA whitening. We seek to
fit one-hot-encoded labels representing the class labels from these datasets. In Fig. 2, we observe
that the soft-thresholded SVD method from Theorem 2 finds the same solution as SGD in far shorter
time. Appendix A.1.4 contains further details of this experiment.
5.3	Effectiveness of the Copositive Program
In this section, we compare the objective values obtained by SGD, Algorithm 1, and the copositive
program defined in (17). We use an artificially-generated spiral dataset, with X ∈ R60×2 and 3
8
Published as a conference paper at ICLR 2021
(a) CIFAR-10
Figure 2: The maximum-margin SVD from Theorem 2 provides the closed-form solution for the
optimal value of the neural network training problem for whitened CIFAR-10 and CIFAR-100.
(b) CIFAR-100
classes (see Fig. 3(a) for an illustration). In this case, since d ≤ 4, we note that the copositive
relaxation in (19) is tight. Across different values of β , we compare the solutions found by these
three methods. As shown in Fig. 3, the copositive relaxation, the solution found by SGD, and the
solution found by Algorithm 1 all coincide with the same loss across various values of β . This
verifies our theoretical proofs of equivalence of (5), (12), and (19).
(a) 3-class Spiral Dataset
3°	X Algorithm 1	宗※
+ Non-convex SGDsoIuticn
28	♦ Copositive relaxation
26
24
18	X
16	*
IOT	10-ɪ	10β	IO1	IO3
beta
(b) Loss across β
Figure 3: Spiral classification: SGD (1000 neurons), Algorithm 1, and copositive relaxation (19).
6 Conclusion
We studied the vector-output ReLU neural network training problem, and designed the first algo-
rithms for finding the global optimum of this problem, which are polynomial-time in the number of
samples for a fixed data rank. We found novel connections between this vector-output ReLU neu-
ral network problem and a variety of other problems, including semi-NMF, cone-constrained PCA,
soft-thresholded SVD, and copositive programming. Of particular interest is extending these results
to deeper networks, which would further explain the performance of neural networks as they are
often used in practice. One such method to extend the results in this paper to deeper networks is to
greedily train and stack two-layer networks to create one deeper network, which has shown to mimic
the performance of deep networks trained end-to-end. Some preliminary results for convex program
equivalents of deeper training problems are presented under whitened input data assumptions in (Er-
gen & Pilanci, 2020c). Another interesting research direction is investigating efficient relaxations of
our vector output convex programs for larger scale simulations, which have been studied in (Bartan
& Pilanci, 2019; Ergen & Pilanci, 2019b;a; d’Aspremont & Pilanci, 2020). Furthermore, landscapes
of vector output neural networks and dynamics of gradient descent type methods can be analyzed by
leveraging our results. In (Lacotte & Pilanci, 2020), an analysis of the landscape for scalar output
networks based on the convex formulation was given which establishes a direct mapping between
the non-convex and convex objective landscapes. Finally, our copositive programming and semi-
NMF representations of ReLU networks can be used to develop more interpretable neural models.
An investigation of scalar output convex neural models for neural image reconstruction was given
in (Sahiner et al., 2020).
9
Published as a conference paper at ICLR 2021
Acknowledgements
This work was partially supported by the National Science Foundation under grants IIS-1838179 and
ECCS-2037304, the National Institutes of Health under grants R01EB009690 and R01EB0026136,
Facebook Research, Adobe Research and Stanford SystemX Alliance.
References
Jacob Abernethy, Francis Bach, Theodoros Evgeniou, and Jean-Philippe Vert. A new approach
to collaborative filtering: Operator estimation with spectral regularization. Journal of Machine
Learning Research, 10(3), 2009.
Miguel F Anjos and Jean B Lasserre. Handbook on semidefinite, conic and polynomial optimization,
volume 166. Springer Science & Business Media, 2011.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learn-
ing. Machine learning, 73(3):243-272, 2008.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019.
Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse pca
with provable guarantees. In International Conference on Machine Learning, pp. 1728-1736,
2014.
Burak Bartan and Mert Pilanci. Convex relaxations of convolutional neural nets. In ICASSP 2019-
2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.
4928-4932. IEEE, 2019.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. In International conference on machine learning, pp. 583-593. PMLR, 2019.
Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Springer Science & Business Media, 2010.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Samuel Burer. A gentle, geometric introduction to copositive optimization. Mathematical Program-
ming, 151(1):89-116, 2015.
Ricardo Cabral, Fernando De la Torre, Joao P Costeira, and Alexandre Bernardino. Unifying nuclear
norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 2488-2495, 2013.
Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix com-
pletion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.
Alexandre d’Aspremont and Mert Pilanci. Global convergence of frank wolfe on one hidden layer
networks. arXiv preprint arXiv:2002.02208, 2020.
Yash Deshpande, Andrea Montanari, and Emile Richard. Cone-constrained principal component
analysis. In Advances in Neural Information Processing Systems, pp. 2717-2725, 2014.
Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex
optimization. The Journal of Machine Learning Research, 17(1):2909-2913, 2016.
Peter JC Dickinson. The copositive cone, the completely positive cone and their generalisations.
Citeseer, 2013.
Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations.
IEEE transactions on pattern analysis and machine intelligence, 32(1):45-55, 2008.
10
Published as a conference paper at ICLR 2021
Mirjam Dur. CoPositive Programming-a survey. In Recent advances in optimization and its appli-
cations in engineering, pp. 3-20. Springer, 2010.
Tolga Ergen and Mert Pilanci. Convex duality and cutting Plane methods for over-Parameterized
neural networks. In OPT-ML workshop, 2019a.
Tolga Ergen and Mert Pilanci. Convex optimization for shallow neural networks. In 2019 57th
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 79-83.
IEEE, 2019b.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In International Conference on Artificial Intelligence and Statistics, pp.
4024-4033. PMLR, 2020a.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
arXiv preprint arXiv:2002.11219, 2020b.
Tolga Ergen and Mert Pilanci. Convex duality of deep neural networks. arXiv preprint
arXiv:2002.09773, 2020c.
Tolga Ergen and Mert Pilanci. Training convolutional relu neural networks in polynomial time:
Exact convex optimization formulations. arXiv preprint arXiv:2006.14798, 2020d.
Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. arXiv preprint arXiv:1810.06793, 2018.
Nicolas Gillis and Abhishek Kumar. Exact and heuristic algorithms for semi-nonnegative matrix
factorization. SIAM Journal on Matrix Analysis and Applications, 36(4):1404-1424, 2015.
Aditya Sharad Golatkar, Alessandro Achille, and Stefano Soatto. Time matters in regularizing deep
networks: Weight decay and data augmentation affect early learning dynamics, matter little near
convergence. In Advances in Neural Information Processing Systems, pp. 10678-10688, 2019.
Edouard Grave, Guillaume R Obozinski, and Francis R Bach. Trace lasso: a trace norm regu-
larization for correlated designs. In Advances in Neural Information Processing Systems, pp.
2187-2195, 2011.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd
via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-3402,
2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 791-800, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Natalia Kogan and Abraham Berman. Characterization of completely positive graphs. Discrete
Mathematics, 114(1-3):297-304, 1993.
11
Published as a conference paper at ICLR 2021
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Jonathan Lacotte and Mert Pilanci. All local minima are global for two-layer relu neural networks:
The hidden convex optimization landscape. arXiv preprint arXiv:2006.05900, 2020.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Qiuwei Li, Zhihui Zhu, and Gongguo Tang. Geometry of factored nuclear norm regularization.
arXiv preprint arXiv:1704.01265, 2017.
Poorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. arXiv preprint
arXiv:1905.11887, 2019.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. arXiv preprint
arXiv:1806.09777, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time con-
vex optimization formulations for two-layer networks. arXiv preprint arXiv:2002.10553, 2020.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regulariza-
tion behind neural reconstruction. arXiv preprint arXiv:2012.05169, 2020.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? arXiv preprint arXiv:1902.05040, 2019.
Alexander Shapiro. Semi-infinite programming, duality, discretization and optimality conditions.
Optimization, 58(2):133-161, 2009.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In
Advances in neural information processing systems, pp. 1329-1336, 2005.
Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13:
389-496, 2004.
12
Published as a conference paper at ICLR 2021
A	Appendix
A. 1 Additional experimental details
All neural networks in the experiments were trained using the Pytorch deep learning library (Paszke
et al., 2019), using a single NVIDIA GeForce GTX 1080 Ti GPU. Algorithm 1 was trained using
a CPU with 256 GB of RAM, as was the maximum-margin matrix factorization. Unless otherwise
stated, the Frank-Wolfe method from Algorithm 1 used a step size of α(k) = 22k, and all methods
were trained to minimize squared loss.
Unless otherwise stated, the neural networks were trained until full training loss convergence
with SGD with a momentum parameter of 0.95 and a batch size the size of the training set (i.e.
full-batch gradient descent), and the learning rate was decremented by a factor of 2 whenever the
training loss reached a plateau. The initial learning rate was set as high as possible without causing
the training to diverge. All neural networks were initialized with Kaiming uniform initialization
(He et al., 2015).
A.1.1 ADDITIONAL EXPERIMENT: COPOSITIVE RELAXATION WHEN d ≥ 4
The copositive relaxation for the neural network training problem described in (19) is not guaran-
teed to exactly correspond to the objective when d ≥ 4. However, we find that in practice, this
relaxation is tight even in such settings. To demonstrate such an instance, we consider the problem
of generating images from noise.
In particular, we initialize X element-wise from an i.i.d standard Gaussian distribution. To
analyze the spike-free setting, we whitened X using ZCA whitening. Then, we attempted to fit
images Y from the MNIST handwritten digits dataset (LeCun et al., 1998) and CIFAR-10 dataset
(Krizhevsky et al., 2009) respectively. From each dataset, we select 100 random images with
10 samples from each class and flatten them into vectors, to form YMNIST ∈ Y 100×784 and
YCIFAR ∈ Y 100×3072. We allow the noise inputs to have the same shape as the output. Clearly,
in these cases, with d = 784 and d = 3072 respectively, the copositive relaxation (20) is not
guaranteed to correspond exactly to the neural network training optimum.
However, we find across a variety of regularization parameters β, that the solution found by
SGD and this copositive relaxation exactly correspond, as demonstrated in Figure 4.
MNlST image generation
CIFAR-IO image generation
米

X Copositive relaxation
+ Nonconvex SGD solution
IO0
Copositive relaxation
Nonconvex SGD solution
IO0
beta
¥
米
米
米
米
米
(a) MNIST
(b) CIFAR-10
Figure 4: The copositive relaxation (20) and solution found by SGD nearly correspond for almost
all β.
While for the lowest value ofβ, the copositive relaxation does not exactly correspond with the value
obtained by SGD, we note that we showed the objective value of the copositive relaxation to be
a lower bound of the neural network training objective-meaning that the differences seen in this
plot are likely due to a numerical optimization issue, rather than a fundamental one. Non-convex
SGD was trained for 60,000 epochs with 1000 neurons with a learning rate of 5 × 10-5, while the
copositive relaxation was trained using Adam (Kingma & Ba, 2014) with the Geotorch library for
constrained optimization and manifold optimization for deep learning in PyTorch, which allowed us
13
Published as a conference paper at ICLR 2021
to express the PSD constraint, with an additional hinge loss to penalize the violations of the affine
constraints. This copositive relaxation was trained for 60,000 epochs with a learning rate of 10-2
for CIFAR-10 and 4 × 10-2 for MNIST, and β1 = 0.9, β2 = 0.999 and = 10-8 as parameters for
Adam.
A.1.2 Additional Experiment: Comparing ReLU Activation to Linear
Activation in the Case of Spike-Free Matrices
Figure 5: Comparing train and test accuracy of linear and ReLU two-layer networks on generated
continuous labels on CIFAR-10. We note that for β ≥ 1.0, the optimal solution for both networks
is to simply set all weights to zero. The best-case test loss for the ReLU network is nearly half the
best-case test loss for the linear network.
As discussed in Section 3.3., if the data matrix X is spike-free, the resulting convex ReLU model
(15) is similar to a linear-activation network, with the only difference being an additional cone
constraint on the weight matrix V . It stands to wonder whether in the case of spike-free data
matrices, the use of a ReLU network is necessary at all, and whether a linear-activation network
would perform equally well.
In this experiment, we compare the performance of a ReLU-activation network to a linear-
activation one, and demonstrate that even in the spike-free case, there exist instances in which the
ReLU-activation network would be preferred. In particular, we take as our training data 3000 de-
meaned and ZCA-whitened images from the CIFAR-10 dataset to form our spike-free training data
X ∈ R3000×3072. We then generate continuous labels Y ∈ R3000×10 from a randomly-initialized
ReLU two-layer network with 4000 hidden units. We use this same label-generating neural network
to generate labels for images from the full 10,000-sample test set of CIFAR-10 as well, after the test
images are pre-processed with the same whitening transformation used on the training data.
Across different values of β, we measured the training and generalization performance of
both ReLU-activation and linear-activation two-layer neural networks trained with SGD on this
dataset. Both networks used 4000 hidden units, and were trained for 400 epochs with a learning
rate of 10-2 and momentum of 0.95. Our results are displayed in Figure 5.
As we can see, for all values of β, while the linear-activation network has equal or lesser
training loss than the ReLU-activation network, the ReLU-activation network generalizes sig-
nificantly better, achieving orders of magnitude better test loss. We should note that for values
of β = 1.0 and above, both networks learn the zero network (i.e. all weights at optimum
are zero), so both their training and test loss are identical to each other. We can also observe
that the best-case test loss for the linear-activation network is to simply learn the zero network,
whereas for a value ofβ = 10-2 the ReLU-activation network can learn to generalize better than the
zero network (achieving a test loss of 63038, compared to a test loss of 125383 of the zero-network).
These results demonstrate that even for spike-free data matrices, there are reasons to prefer
a ReLU-activation network to a linear-activation network. In particular, because of the cone-
14
Published as a conference paper at ICLR 2021
constraint on the dual weights V , the ReLU network is induced to learn a more complex
representation than the linear network, which would explain its better generalization performance.
The CIFAR-10 dataset consists of 50,000 training images and 10,000 test images of 32 × 32
for 3 RGB channels, with 10 classes (Krizhevsky et al., 2009). These images were normalized by
the per-channel training set mean and standard deviation. To form our training set, selected 3,000
training images from these datasets at random, where each class was equally represented. This data
was then feature-wise de-meaned and transformed using ZCA. This same training class mean and
ZCA transformation was also then used on the 10,000 testing points for evaluation.
A.1.3 Does SGD always find the global optimum for neural networks ?
For these experiments, SGD was trained with an initial learning rate of 4 × 10-5 for 20,000 epochs.
We used a regularization penalty value of β = 10-2 . The value for t for Algorithm 1 was found by
first starting at the value of regularization penalty 1 Pjm=I Ilujll2 + IIvj k2 from the solution from
SGD, then refining this value using manual tuning. A final value of t = 1.495 was chosen. For this
experiment, there were P = 50 sign patterns. Algorithm 1 was run for 30,000 iterations, and took
X seconds to solve.
A.1.4 Maximum-Margin Matrix Factorization
The CIFAR-10 and CIFAR-100 datasets consist of 50,000 training images and 10,000 test images
of 32 × 32 for 3 RGB channels, with 10 and 100 classes respectively (Krizhevsky et al., 2009).
These images were normalized by the per-channel training set mean and standard deviation. To
form our training set, selected 3,000 training images from these datasets at random, where each
class was equally represented. This data was then feature-wise de-meaned and transformed using
ZCA. This same training class mean and ZCA transformation was also then used on the 10,000
testing points for evaluation. For CIFAR-10, we used a regularization parameter value of β = 1.0,
whereas for CIFAR-100, we used a value of β = 5.0.
SGD was trained for 400 epochs with a learning rate of 10-2 with 1000 neurons, trained
with one-hot encoded labels and squared loss. Figure 6 displays the test accuracy of the learned
networks. Surprisingly the whitened classification from only 3,000 images generalizes quite well in
both circumstances, far exceeding performance of the null classifier. For the CIFAR-10 experiments,
the algorithm from Theorem 2 took only 0.018 seconds to solve, whereas for CIFAR-100 it took
0.36 seconds to solve.

(a) CIFAR-10
(b) CIFAR-100
Figure 6: Test accuracy for maximum-margin matrix factorization and SGD for whitened CIFAR-10
and CIFAR-100.

A.1.5 Effectiveness of the Copositive Program
For this classification problem, we use one-hot encoded labels and squared loss. For β < 1, SGD
used a learning rate of 10-3, and otherwise used a learning rate of 2 × 10-3. SGD was trained
for 8,000 epochs with 1000 neurons, while Algorithm 1 ran for 1,000 iterations. The copositive
relaxation was optimized with CVXPY with a first-order solver on a CPU with 256 GB of RAM
15
Published as a conference paper at ICLR 2021
(Diamond & Boyd, 2016). The first-order convex solver for the copositive relaxation used a maxi-
mum of 20,000 iterations. This dataset had P = 114 sign patterns. The value of t for Algorithm 1
was chosen as the regularization penalty 2 Pj=IIlujk2 + Ilvj ∣H from the solution of SGD.
A.2 Note on Data matrices of a Fixed Rank
Consider the neural network training problem
mm
Umin 2 k X(Xuj )+v> - Y kF + 2 X kuj k2 + kvjk2	(21)
uj,vj 2 j=1	2 j=1
Let X = UDV > be the compact SVD of X with rank r, where U ∈ Rn×r, D ∈ Rr×r and
V ∈ Rd×r. Let u0j = V>uj and uj⊥ = V⊥>uj. We note that Xuj = XV u0j and kuj k22 =
uj>(V V> + V⊥V⊥>)uj = ku0j k22 + kuj⊥ k22. Then, we can re-parameterize the problem as
mm
⊥min - k X(XVuj)+v>- Y kF + 2 X kujk2 + ku⊥k2 + kvjk2	(22)
uj⊥,u0j,vj 2 j=1	2 j=1
We note that uj⊥ only appears in the regularization term. Minimizing over uj⊥ thus means simply
setting it to 0. Then, we have
mm
m in 2 k X(XVuj)+v> - Y kF + 2 X kujk2 + kvj k2	(23)
uj ,vj	j=1	j=1
We note that XV = UD ∈ Rn×r and u0j ∈ Rr. Thus, for X of rank r, we can effectively reduce
the dimension of the neural network training problem without loss of generality. This thus holds for
all results concerning the complexity of the neural network training problem with data matrices of a
fixed rank.
A.3 Proofs
A.3. 1 Proof of Theorem 1
We begin with the primal problem (5), repeated here for convenience:
mm
p* = mind 2 k X(Xuj )+v> - Y kF + 2 X (kujk2 + kvj k2)	(24)
uj∈R 2 j 1	2j 1
vj ∈Rc	j=1	j=1
We start by re-scaling the weights in order to obtain a slightly different, equivalent objective, which
has been performed previously in (Pilanci & Ergen, 2020; Savarese et al., 2019).
Lemma 4 The primal problem is equivalent to the following optimization problem
mm
p* =虏 in≤ι jn。2k X(Xuj )+v>- Y kF+β X kvjk2	(25)
Proof: Note that for any γ > 0, we can re-scale the parameters uj = Yjuj-, vj = vj /γj-. Noting
that the network output is unchanged by this re-scaling scheme, we have the equivalent problem
*
P
2m
Umind m›o 2k X(Xuj )+v>
uj∈	j=1
vj ∈Rc
m
-Y IlF + 2 X (γ2luj l∣2 + Ilvj k2/Y2)
j=1
(26)
Minimizing with respect to γj , we thus end up with
mm
P* = mind 2 k X(Xuj )+v> - Y kF + 2 X (kujk2kvj k2)
Uj∈R 2 j 1	2 j 1
vj∈Rc	j=1	j=1
(27)
16
Published as a conference paper at ICLR 2021
We can thus set kuj k2 = 1 without loss of generality. Further, relaxing this constraint to kuj k2 ≤ 1
does not change the optimal solution. In particular, for the problem
mm
虏 in≤ι j%2 k X(Xuj )+v>- Y kF+β X kvjk2	(28)
the constraint kuj k2 = 1 will be active for all non-zero vj . Thus, relaxing the constraint will not
change the objective. This proves the Lemma.
Now, we are ready to prove the first part of Theorem 1, i.e. the equivalence to the semi-infinite
program (11).
Lemma 5 For all β > 0 primal neural network training problem (25) has a strong dual, in the form
of
p* = d* :=	max ，一，-1 kZ - Y kF + 1 kY kF	(29)
P	Z: kz>(Xu)+k2≤β∀ku∣2≤1 2k	kF+ 2k kF	()
Proof: We first form the Lagrangian of the primal problem, by first-reparameterizing the problem
as
mm
Ilmin,min XkRkF+β X kvj k2 s.t. R=X(Xuj )+v> - Y
∣uj ∣2 ≤1 vj,R 2	j=1	j=1
(30)
and then forming the Lagrangian as
1m
min minmαx-kRkF + 尸£ Ilvjk2
∣uj∣2≤1 vj,R Z 2
j=1
m
+Z>Y+Z>R-Z> X(Xuj)+vj>
j=1
(31)
By Sion’s minimax theorem, we can switch the inner maximum and minimum, and minimize over
vj and R. This produces the following problem:
p* = min	max _1 kZ — Y kF + 1 kY kF	(32)
kj∣2≤ι Z: kz>(Xu)+∣2≤β 2l1	11F 2l1 11F	' '
We then simply need to interchange max and min to obtain the desired form. Note that this inter-
change does not change the objective value due to semi-infinite strong duality. In particular, for any
β > 0, this problem is strictly feasible (simply let Z = 0) and the objective value is bounded by
1 kYkF. Then, by Theorem 2.2 of (Shapiro, 2009), We know that strong duality holds, and
p* = d* :=	max ，一，-1 kZ - Y kF + 1 kY kF	(33)
Z: kz>(Xu)+∣2≤β∀ku∣2≤1 211	11F 211 11F	' '
as desired.
Furthermore, by (Shapiro, 2009), for a signed measure μ, we obtain the following strong
dual of the dual program (11):
d* = maχ min -5kZ - Y kF - -kY kF + Z (kZ T(Xu)+k2 - β)dμ(U)	(34)
μ占0 Z∈Rn×d 2	2	B2 ∖	J
where B? defines the unit '2-ball. By discretization arguments in Section 3 of (Shapiro, 2009), and
by Helly’s theorem, there exists some m* ≤ nc + 1 such that this is equivalent to
*
1	1m
d = maχ min -9 kZ	- Y kF - XkY kF	+ X (kZ	(Xui)+k2	- β)μi	(35)
μ≥0	z∈Rn×d,kuik2≤1	2	2	个 ∖	J
i=1
Minimizing with respect to Z, we obtain
**
m*	m*
ll min min min	μi(Xui)+g> - Y kF + 户£〃，
kuik2≤1 μ≥0 kgi∣2≤	i=ι	M
(36)
17
Published as a conference paper at ICLR 2021
which We can minimize with respect to μ to obtain the finite parameterization
*
*
_ _ *	_ _ *
mm
d* = u.. min<ιmgin2k X(Xui)+g> - Y IlF + β X I∣gik2	(37)
i i 2	i=1	i=1
This proves that the semi-infinite dual provides a finite support with at most m* ≤ nc +1 non-zero
neurons. Thus, if the number of neurons of the primal problem m ≥ m*, strong duality holds. Now,
we seek to show the second part of Theorem 1, namely the equivalence to (12). Starting from (11),
we have that the dual constraint is given by
max IZ> (Xu)+ I2 ≤ β	(38)
kuk2≤1
Using the concept of dual norm, we can introduce variable g to further re-express this constraint as
max g>Z>(Xu)+ ≤ β	(39)
kuk2≤1
kgk2≤1
Then, enumerating over sign patterns {DiIiP=1, we have
max max	g> Z> DiXu ≤ β	(40)
i∈[P]	kuk2≤1	i
kgk2≤1
(2Di -I)X u≥0
Now, we express this in terms of an inner product.
max	hZ, Di Xug> i ≤ β	(41)
i∈[P]
(2Di-I)Xu≥0
kuk2≤1
kgk2≤1
Letting V = ug> :
max	hZ, DiXV i ≤ β	(42)
i∈[P]
V =ug>
(2Di-I)Xu≥0
kVk*≤1
Now, we can take the convex hull of the constraint set, noting that since the objective is affine, this
does not change the objective value.
max hZ, DiXV i ≤ β	(43)
i∈[P]
V∈Ki
kVk*≤1
Thus, the dual problem is given by
p*= d* = max -1 kZ - Y kF + 1kY kF
Z2	2
s.t. max hZ, DiXV i ≤ β ∀i ∈ [P]
Vi∈Ki
kVik*≤1
We now form the Lagrangian,
(44)
*
p*
d* = max min	min
Z λ≥0 Vi∈Ki ∀i∈[P]
kVik*≤1
1	1P
-2IIZ - Y IIf + 2IIY IIf + X λi (β
i=1
- hZ, DiXVii
(45)
We note that by Sion’s minimax theorem, we can switch the max and min, and then minimize over
Z. Following this, we obtain
*
p*
d*
min min
Vi ∈Ki ∀i∈[P] λ≥0
kVik*≤1
1P	P
2Il λiDiXVi - YIlF + βɪ3λi
i=1	i=1
(46)
18
Published as a conference paper at ICLR 2021
We can re-scale our variables to obtain
p* = d*
1P	P
min min - k T DiXVi - Y IlF + βfλ%
Vi∈Ki ∀i∈[P] λ≥0 2	i i	F	i
i=1
(47)
Mk*≤λ
i=1
And now minimize over λ to obtain the desired result:
p* = d*
min
Vi∈Ki ∀i∈[P]
1P	P
2 k EDiXVi - Y kF + β∑ IMk*
i=1
i=1
(48)
Remark 5.1 Given the optimal solution (12), it is natural to wonder how these dual variables relate
to the optimal neurons of the neural network training problem (5). Given an optimal {Vi*}iP=1, we
simply need to factor them into Vi* = Pjc=1 hi*j gi*j >, where (2Di - I)X hi*j ≥ 0. This is similar
in flavor to semi-NMF. Since Vi* is in the cone Ki, exact semi-NMF factorization can be performed
in polynomial time (Gillis & Kumar, 2015). Once this factorization is obtained, assuming without
loss of generality that kgi*j k2 = -, the optimal neurons are given by
(Uij, Vj) = ( hj-, g* Jkhjk2), i ∈ [P],j ∈ [c]
khi*j k2
(49)
Thus, given a solution to (12), a polynomial-time algorithm exists for reconstructing weights of the
original neural network training algorithm.
A.3.2 A corollary for CNNs with average pooling
We first introduce additional notation for CNNs with average pooling. In particular, following Ergen
& Pilanci (2020d), we use the set of patch matrices {Xk}kK=1 to define a new data matrix as M :=
[Xi, X2, •…XK]> ∈ RnK×h, where h is the convolutional filter size. This matrix thus has a set of
sign patterns, which we define as
Pconv ≤ 2rc (e(nK-1 )rc
rc
where rc = rank(M) ≤ h. We then can enumerate the set of patch matrices
{Dik : i ∈ [Pconv], k ∈ [K]}
Corollary 5.1 In the case ofa two-layer CNN with global average pooling as in (6), the strong dual
of the neural network training problem
PK	P
P*onv =Wmin 3 2 kXXDikXkVi -Yk2F+βXkVik*	(50)
Vi∈Ki ∀i∈[P] 2 i=1 k=1	i=1
for convex sets Ki, given by
Ki := conv{ug> : (2Dik - I)Xku ≥ 0 ∀k, kgk2 ≤ -}	(51)
This strong dual is convex. For a fixed kernel-size, Kand rc are fixed, and the problem is polynomial
in n, i.e. of complexity proportional O(nrc). Since in practice, M is a tall matrix with relatively
few columns, it is almost always full-rank, in which case the computational complexity of solving
the strong dual is O(nh). This problem can be solved with the same Frank-Wolfe algorithm as
presented in Algorithm 1.
A.3.3 Proof of Theorem 2
We note that whitened data matrices such that n ≤ d satisfy XX> = I. Then, we can solve the
modified problem
min	-kV - X>YkF + β∣V∣*	(52)
V∈conv{ug> =Xu≥0,kgk2≤1} 2
19
Published as a conference paper at ICLR 2021
This is simply by noting that left-multiplying by X > does not change the norm. Now, note that
V ∈conv{ug>X≥0,kgk2≤1} 2 kV-X >Y kF+βkVk* ≥ V ∈conv{mg上kgk2≤1} 2	-X" kF +βkVk*
The relaxed problem on the right-hand side is given exactly by maximum-margin matrix factoriza-
tion (Srebro et al., 2005), i.e. can be expressed as
V=Pruig> 2 kV -X>Y kF+β kVk
*
Furthermore, this has a closed-form solution, given by soft-thresholding the singular values of X>Y
(Hastie et al., 2015). Thus, we have the solution of the relaxed problem as
c
V* = X(σi - β)+aibi>
Noting that Xai ≥ 0 for all i ∈ {i : σi > βi} by assumption, this solution is feasible for the
original problem. Thus, because the optimal value obtained by the solution to the relaxed problem
was a lower bound to (15), it must be the optimal solution to (15).
Remark 5.2 It is interesting to note that when Y is one-hot encoded,
X >Y = [n(i)μ(i) n(2) μ(2)…n(c)M(c)]	(53)
where n(i) refers to the number ofinstances in class i, and 仙⑴ refers to the mean ofall data points
belonging to class i. In this scenario, then, the optimal neural network weights, V *, are found via
the maximum-margin factorization of this matrix.
Further, if YY> is element-wise non-negative, by Perron-Frobenius theorem, its maximal
left singular is non-negative. For such data matrices, if β is chosen to be larger than the second
largest singular value ofY, the solution in (16) is guaranteed to be exact.
A.3.4 Proof of Theorem 3
Start with (11):
*
p*
*
p*
d* = max-2 kZ - Y kF + ∣ ∣∣Y kF
Z2	2
s.tkZ>(Xu)+k2 ≤ β ∀kuk2 ≤1
We can express this as
d* = max - 2 kZ - Y kF + ∣ ∣∣Y kF
Z2	2
s.t max kZ>(Xu)+k22 ≤ β2
kuk2≤1
(54)
(55)
Enumerating over all possible sign patterns i ∈ [P], and noting that kZ>DiXuk22
tr Z>DiXuu>X>DiZ , we have
p* = d*
s.t max tr
i∈[P]
(2Di-I)Xu≥0
kuk2≤1
max - 1 kZ - Y kF + 1kY kF
Z2	2
Z>DiXuu>X>DiZ ≤ β2
(56)
Noting the maximization constraint is linear in uu>, we can take the convex hull of the constraint
set and not change the optimal value. Thus,
p* = d* = max
Z
s.t max tr
i∈[P]
U∈Ci
tr(U)≤1
-1 kZ - Y kF + 1 kY kF
>DiXUX>DiZ ≤ β2
(57)
20
Published as a conference paper at ICLR 2021
Which is then equivalent to
p* = d*=max - 1kZ - Y kF + 1kY kF
Z2	2
s.t max trZ>DiXUiX>DiZ ≤ β2 ∀i ∈ [P]
Ui ∈Ci
tr(Ui)≤1
(58)
We thus have a problem with a convex objective and P constraints, so we can take the Lagrangian
p* = d*
啜XUi∈mi∀i∈[P]-1 kZ - YkF + 1 kYkF +
P
tr(Ui)≤1
λ≥0
i=1
λi β2 - tr Z>DiXUiX>
(59)
Noting that this function is convex over Z, affine over λ, and concave over Ui, and the constraint set
is convex, by Sion’s minimax theorem we can change max and min without changing the objective.
Thus,
(60)
p* = d*
1	1P
min	max --kZ - YkF + -kYkF + Xλi β2 - tr (Z>DiXUiX>
Ui∈Ci ∀i∈[P] Z 2	2
tr(Ui)≤1	i=1
λ≥0
Solving for Z and re-substituting, we thus have
1	P	-1	P
p* = d* = min -tr Y> (I + 2 X %(D,X)Ui(DiX)>)	Y + β X %
Ui∈Ci 2
tr(Ui)≤1	i=1	i=1
λ≥0
as in the proof of Theorem 1, we easily re-scale this to obtain the desired objective:
p* = d*
Ui∈m ∀i∈	1 tr (γ> (I+2 X (DiX Wm X )>)-1Y) +β 2 X tr(Ui)
i=1	i=1
(61)
(62)
A.4 Notes on the Frank-Wolfe Method Applied to (12)
A.4. 1 Derivation of the Frank-Wolfe Algorithm
In general, the Frank-Wolfe algorithm (Frank et al., 1956) aims to solve the problem
min f(x) s.t. x ∈ D	(63)
x
for some convex function f and convex set D. It does so in the following iterative steps k =
1, •…,K with step sizes α(k) and an initial point x(1)
1.	(LMO step) Solve the problem
m(k) ：= argminhs, Vχf (x(k))〉	(64)
s∈D
2.	(Update step) Update the decision variable
x(k+1) = ( - α(k))x(k) + α(k)m(k)	(65)
We will now apply the Frank-Wolfe problem to the inner minimization of (14), i.e., for a fixed value
of t, solving
-P
Vi∈m in∈[p]2 k x DiXM- Y kF+βt	(66)
PP=1 M k*≤t	i=1
21
Published as a conference paper at ICLR 2021
In particular the LMO step becomes:
{Mi(k)}iP=1:= arg S Kmi∀n P XP	Si,(DiX)>XP DjXVj(k)-Y	(67)
Si ∈Ki ∀i∈[P ]
PP=IkSik*≤ti=1 '	j = 1	/
PP
=argSi∈Kmii∀ni∈[P] X DiXSi,XDjXVj(k) -Y	(68)
PPtIkSik*≤ti=1	j = 1
Note that the objective value in the optimization problem of the LMO step is affine with respect to
the variables Si . Thus, the optimal value occurs at an extreme point of the feasible set. Thus, only
one of {Mi}P=ι is active at optimum at index i = i*, and moreover this M%* occurs at an extreme
point of Ki*, i.e Mi* ∈ Ki*. All other Mi where i = i* are zero at optimum. Thus, We can re-write
this problem as
P
Mi* = arg min	DiXS, X Dj XVj(k) - Y	(69)
s∈∈KKi	j=1
kSk*≤t
Recalling that	Ki	:= conv{ug>	:	(2Di	-	I)Xu ≥	0,	kgk2	≤ 1} boundary of Ki is simple to
express, leaving us with Mi* = ui* gi>* , where
P
(ui*,gi*) = arg min	DiXug>, X DjXVj(k) -Y	(70)
(2Di-I)Xu≥0	j=1
kgk2≤1
kuk2≤t
Note that we can change the constraint kuk2 ≤ t to kuk2 ≤ 1 and simply multiply by a factor of t
to the solution afterwards. We can thus write the key Frank-Wolfe LMO step subproblem as
P
(ui* , gi* ) =	max	hDiXug>, Y - X DjXVj(k)i	(71)
i∈[P ]
(2Di-I)Xu≥0	j=1
kgk2≤1
kuk2≤1
as desired. For each i ∈ [P], we thus solve the problem
P
si(k) =	max	hDiXug>, Y - X DjXV (k)i
si	(2Di-mIa)Xx u≥0	i ug ,	- j j
kgk2≤1	j=1
kuk2≤1
(72)
and store the arg maxes (ui , gi ) for each i. Then, from the index which attains the maximum
i* := arg maxi si(k), form Mi(*k) = ui* gi>* . For all other i 6= i*, as stated previously, Mi(k) = 0.
Then, we must re-multiply by the factor t which was removed in (71) to obtain the update rule for
all i ∈ [P]:
Vi(k+1) = (1 - α(k))Vi(k) + tα(k)Mi(k)
A.4.2 Solving each Frank-Wolfe iterate
We discuss here how to solve the subproblem
si(k)
max
kuk2≤1
kgk2≤1
(2Di-I)Xu≥0
hDiXug>, Y
P
- X DiXVi(k)i
j=1
22
Published as a conference paper at ICLR 2021
using cone-constrained PCA. In particular, note that for a fixed u, we can solve for g in closed form.
Let R(k) = Y - PjP=1 DiXVi(k). Then, we have
*/ 、	R(k)>DiXu
g (U) = ------ɪ--------
kR(k)>DiXuk2
Then, re-substituting this back into the objective, we have
si(k) =	kumka2≤x1	kR(k)>DiXuk2
(2Di-I)Xu≥0
max	u>(DiX)>R(k)R(k)>(DiX)u
kuk2≤1
(2Di -I)X u≥0
Without loss of generality, we can square the objective, since objective only takes on non-negative
values. The LMO step of the Frank-Wolfe algorithm is thus identical to the cone-constrained PCA
problem
si(k) = max	u>(DiX)>R(k)R(k)>(DiX)u
i	kuk2≤1
(2Di -I)X u≥0
This problem, via Lemma 7 of Asteris et al. (2014), can be performed in O(nd) time. However, we
note that because we can reduce the dimension of the problem to r without loss of generality (see
Appendix A.2), this thus simplifies to O(nr). We perform P of such maximization problems per
iteration of Frank-Wolfe.
A.5 A cutting plane method for s olving (17)
Consider the dual problem
d*=m∙ax - 1 kZ - Y kF + 1 kY kF
Z2	2
s.t max kZ>(Xu)+k22 ≤ β2
kuk2≤1
Enumerating over sets of hyperplanes, we have
P* = d*=max - 1 kZ — Y kF + 1kY kF
Z2	2
s.t max	kZ>DiXuk22 ≤ β2
(2Di-I)Xu≥0
kuk2≤1
We can express the dual constraint as
max	u>Miu ≤ β2
i∈[P]
(2Di-I)Xu≥0
kuk2≤1
(73)
(74)
(75)
where Mi = X>DiZZ>DiX ∈ Rd×d. For each i, this is a cone-constrained PCA problem in
dimension d with n linear inequality constraints. Using Lemma 7 of (Asteris et al., 2014), there
exists an algorithm which solves the cone-constrained PCA problem in O(nr) time. Thus, solving
the optimization problem in the dual constraint (75) is O(Pnr) time. For a fixed rank r, P has
order O(nr), but otherwise P is O(nd) as well. Thus, determining the feasibility of a dual variable
Z from (75) has complexity O(nd) in the general case, and O(nr) in the rank-r case.
Now, using the Analytic Center Cutting Plane Method (ACCPM), the cone-constrained PCA
procedure above provides an oracle for the feasibility of a dual variable Z. Using this oracle,
ACCPM solves the convex problem (73) in polynomial steps in terms of the dimension of Z, i.e.
poly(nc) steps. Thus, the overall complexity of the cutting plane method is O(ndpoly(nc)) in the
general case, and O(nrpoly(nc)) in the rank-r case, which is polynomial for a fixed r.
We note that while this is similar in complexity to the results of (Pilanci & Ergen, 2020),
there are a few subtle differences. In particular, there are additional terms polynomial in c which do
not appear in their work. However, the broader picture of the analyses align-for full-rank matrices,
solving the convex dual of the neural network training problem is NP-hard, while for matrices of a
fixed rank, a polynomial time algorithm exists.
23
Published as a conference paper at ICLR 2021
A.6 Extensions to general convex loss functions
This follows closely from a similar discussion by Pilanci & Ergen (2020). Consider the objective of
the neural network training problem with a general convex loss function `, given by
mm
p*= min	2'(X(Xuj )+v>, Y ) + 2 X (kujk2 + kvjk2)	(76)
{uj}j=1,{vj}j=1 2 j=1	2 j=1
Then, we can follow the same proof of Theorem 1 exactly, instead substituting the Fenchel dual of
`, defined as
'-(Z) =max(Z, Ri - '(R, Y)	(77)
R
Then, we have the dual objective analog of (11):
max	-'*(Z)	(78)
Z"∣Z>(Xu)+k2≤β ∀kuk2≤1
We can further elucidate the convex constraint on dual variable Z as done in the proof of Theorem
1. Then, We can follow the same steps from this previous proof, noting that by Fenchel-MoreaU
Theorem, `-- = ` (Borwein & Lewis, 2010). Thus, the finite-dimensional convex strong dual is
given by
1P	P
P* = d* = VQmi∀i∈[P] 2'(XDiXVi，Y) + β X kVik*	(79)
i=1	i=1
as desired. We note that the Frank-Wolfe method in Algorithm 1 holds for general convex ` as well,
with a small modification corresponding to the gradient of the loss function.
24