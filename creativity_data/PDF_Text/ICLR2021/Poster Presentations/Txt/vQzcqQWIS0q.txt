Published as a conference paper at ICLR 2021
Learnable Embedding Sizes for Recommender
Systems
SiyiLiu* 1, ChenGao2； YihongChen3, Depeng Jin2, YongLi2
1 University of Electronic Science and Technology of China, Chengdu, China
2Beijing National Research Center for Information Science and Technology,
Department of Electronic Engineering, Tsinghua University, Beijing 100084, China
3 University College London, London, United Kingdom
ssui.liu1022@gmail.com, gc16@mails.tsinghua.edu.cn,
yihong-chen@outlook.com, {jindp,liyong07}@tsinghua.edu.cn
Ab stract
The embedding-based representation learning is commonly used in deep learning
recommendation models to map the raw sparse features to dense vectors. The
traditional embedding manner that assigns a uniform size to all features has two
issues. First, the numerous features inevitably lead to a gigantic embedding table
that causes a high memory usage cost. Second, it is likely to cause the over-fitting
problem for those features that do not require too large representation capacity.
Existing works that try to address the problem always cause a significant drop in
recommendation performance or suffer from the limitation of unaffordable train-
ing time cost. In this paper, we propose a novel approach, named PEP1 (short
for Plug-in Embedding Pruning), to reduce the size of the embedding table while
avoiding the drop of recommendation accuracy. PEP prunes embedding parame-
ter where the pruning threshold(s) can be adaptively learned from data. Therefore
we can automatically obtain a mixed-dimension embedding-scheme by pruning
redundant parameters for each feature. PEP is a general framework that can plug
in various base recommendation models. Extensive experiments demonstrate it
can efficiently cut down embedding parameters and boost the base model’s per-
formance. Specifically, it achieves strong recommendation performance while
reducing 97-99% parameters. As for the computation cost, PEP only brings an
additional 20-30% time cost compared with base models.
1	Introduction
The success of deep learning-based recommendation models (Zhang et al., 2019) demonstrates their
advantage in learning feature representations, especially for the most widely-used categorical fea-
tures. These models utilize the embedding technique to map these sparse categorical features into
real-valued dense vectors to extract users’ preferences and items’ characteristics. The learned vec-
tors are then fed into prediction models, such as the inner product in FM (Rendle, 2010), self-
attention networks in AutoInt (Song et al., 2019), to obtain the prediction results. The embedding
table could contain a large number of parameters and cost huge amounts of memory since there are
always a large number of raw features. Therefore, the embedding table takes the most storage cost.
A good case in point is the YouTube Recommendation Systems (Covington et al., 2016). It demands
tens of millions of parameters for embeddings of the YouTube video IDs. Considering the increas-
ing demand for instant recommendations in today’s service providers, the scale of embedding tables
becomes the efficiency bottleneck of deep learning recommendation models. On the other hand,
features with uniform embedding size may hard to handle the heterogeneity among different fea-
tures. For example, some features are more sparse, and assigning too large embedding sizes is likely
*Chen Gao is the Corresponding Author. The work is performed when Siyi Liu is an intern in Tsinghua
University.
1Codes are available at: https://github.com/ssui-liu/learnable-embed-sizes-for-RecSys
1
Published as a conference paper at ICLR 2021
to result in over-fitting issues. Consequently, recommendation models tend to be sub-optimal when
embedding sizes are uniform for all features.
The existing works towards this problem can be divided into two categories. Some works (Zhang
et al., 2020; Shi et al., 2020; Kang et al., 2020) proposed that some closely-related features can
share parts of embeddings, reducing the whole cost. Some other works (Joglekar et al., 2020; Zhao
et al., 2020b;a; Cheng et al., 2020) proposed to assign embeddings with flexible sizes to different
features relying on human-designed rules (Ginart et al., 2019) or neural architecture search (Joglekar
et al., 2020; Zhao et al., 2020b;a; Cheng et al., 2020). Despite a reduced embedding size table, these
methods still cannot perform well on the two most concerned aspects, recommendation performance
and computation cost. Specifically, these methods either obtain poor recommendation performance
or spend a lot of time and efforts in getting proper embedding sizes.
In this paper, to address the limitations of existing works, we proposed a simple yet effective
pruning-based framework, named Plug-in Embedding Pruning (PEP), which can plug in various
embedding-based recommendation models. Our method adopts a direct manner-pruning those un-
necessary embedding parameters in one shot-to reduce parameter number.
Specifically, we introduce the learnable threshold(s) that can be jointly trained with embedding pa-
rameters via gradient descent. Note that the threshold is utilized to determine the importance of
each parameter automatically. Then the elements in the embedding vector that are smaller than the
threshold will be pruned. Then the whole embedding table is pruned to make sure each feature has
a suitable embedding size. That is, the embedding sizes are flexible. After getting the pruned em-
bedding table, we retrain the recommendation model with the inspiration of the Lottery Ticket Hy-
pothesis (LTH) (Frankle & Carbin, 2018), which demonstrates that a subnetwork can reach higher
accuracy compared with the original network. Based on flexible embedding sizes and the LTH,
our PEP can cuts down embedding parameters while maintaining and even boosting the model’s
recommendation performance. Finally, while there is always a trade-off between recommendation
performance and parameter number, our PEP can obtain multiple pruned embedding tables by run-
ning only once. In other words, our PEP can generate several memory-efficient embedding matrices
once-for-all, which can well handle the various demands for performance or memory-efficiency in
real-world applications. We conduct extensive experiments on three public benchmark datasets:
Criteo, Avazu, and MovieLens-1M. The results demonstrate that our PEP can not only achieve the
best performance compared with state-of-the-art baselines but also reduces 97% to 99% parameter
usage. Further studies show that our PEP is quite computationally-efficient, requiring a few addi-
tional time for embedding-size learning. Furthermore, visualization and interpretability analysis on
learned embedding confirm that our PEP can capture features’ intrinsic properties, which provides
insights for future researches.
2	Related Work
Existing works try to reduce the embedding table size of recommendation models from two perspec-
tives, embedding parameter sharing and embedding size selection.
2.1	Embedding Parameter Sharing
The core idea of these methods is to make different features re-use embeddings via parameter shar-
ing. Kang et al. (2020) proposed MGQE that retrieves embedding fragments from a small size
of shared centroid embeddings and then generates final embedding by concatenating those frag-
ments. Zhang et al. (2020) used the double-hash trick to make low-frequency features share a small
embedding-table while reducing the likelihood of a hash collision. Shi et al. (2020) tried to yield
a unique embedding vector for each feature category from a small embedding table by combining
multiple smaller embedding (called embedding fragments). The combination is usually through
concatenation, add, or element-wise multiplication among embedding fragments.
However, those methods suffer from two limitations. First, engineers are required to carefully design
the parameter-sharing ratio to balance accuracy and memory costs. Second, these rough embedding-
sharing strategies cannot find the redundant parts in the embedding tables, and thus it always causes
a drop in recommendation performance.
2
Published as a conference paper at ICLR 2021
Table 1: Comparison of our PEP and existing works (AutoInt is a base recommendation model and
others are embedding-parameter-redUction methods.)___________________________________
Method	Performance	Parameter Number	Computation Cost
AUtoInt (Song et al., 2019)	√	×	√	一
MDE (Ginart et al., 2019)	×	√	×
NIS (Joglekar et al., 2020)	×	√	×
DartsEmb (Zhao et al., 2020b)		√			√		×
DNIS (Cheng et al., 2020)		√			√		×
OUr PEP	√	√	√	一
In this work, our method automatically chooses suitable embedding usages by learning from data.
Therefore, engineers can be free from massive efforts for designing sharing strategy, and the model
performance can be boosted via removing redUndant parameters and alleviating the over-fitting is-
sUe.
2.2	Emebdding Size Selection
The embedding-sharing methods assign Uniform embedding sizes to every featUre, which may still
fail to deal with the heterogeneity among different featUres. Recently, several methods proposed a
new paradigm of mixed-dimension embedding table. Specifically, different from assigning all fea-
tUres with Uniformed embedding size, different featUres can have different embedding sizes. MDE
(Ginart et al., 2019) proposed a hUman-defined rUle that the embedding size of a featUre is pro-
portional to its popUlarity. However, this rUle-based method is too roUgh and cannot handle those
important featUres with low freqUency. Additionally, there are plenty of hyper-parameters in MDE
reqUiring a lot of trUning efforts. Some other works (Joglekar et al., 2020; Zhao et al., 2020b;a;
Cheng et al., 2020) assigned adaptive embedding sizes to different featUres, relying on the advances
in NeUral ArchitectUre Search (NAS) (Elsken et al., 2019), a significant research direction of AU-
tomated Machine Learning (AUtoML) (HUtter et al., 2019). NIS (Joglekar et al., 2020) Used a
reinforcement learning-based algorithm to search embedding size from a candidate set predefined
by hUman experts. A controller is adopted to generate the probability distribUtion of size for spe-
cific featUre embeddings. This was fUrther extended by DartsEmb (Zhao et al., 2020b) by replacing
the reinforcement learning searching algorithm with differentiable search (LiU et al., 2018). AU-
toDim (Zhao et al., 2020a) allocated different embedding sizes for different featUre fields, rather
than individUal featUres, in a same way as DartsEmb. DNIS (Cheng et al., 2020) made the candi-
date embedding size to be continUoUs withoUt predefined candidate dimensions. However, all these
NAS-based methods reqUire extremely high compUtation costs in the searching procedUre. Even for
methods that adopt differential architectUre search algorithms, the searching cost is still not afford-
able. Moreover, these methods also reqUire a great effort in designing proper search spaces.
Different from these works, oUr prUning-based method can be trained qUite efficiently and does not
reqUire any hUman efforts in determining the embedding-size candidates.
3	Problem Formulation
FeatUre-based recommender system2 is commonly Used in today’s information services. In general,
deep learning recommendation models take varioUs raw featUres, inclUding Users’ profiles and items’
attribUtes, as inpUt and predict the probability that a User like an item. Specifically, models take the
combination of User’s profiles and item’s attribUtes, denoted by x, as its’ inpUt vector, where x is the
concatenation of all fields that coUld defined as follows:
x = [x1;x2; . . . ;xM] ,	(1)
where M denotes the nUmber of total featUre fields, and xi is the featUre representation (one-hot
vector in UsUal) of the i-th field. Then forxi, the embedding-based recommendation models generate
corresponding embedding vector vi via following formUlation:
vi = Vixi ,	(2)
2It is also known as click-throUgh rate prediction.
3
Published as a conference paper at ICLR 2021
gl	、
VI I 0.1 I 0.3 ∣-0.2∣-0.4∣ 0.4 I	VI I 0 I 0.3∣-0.2∣-0.4∣ 0.4 I-►
v2 ∣04T-040∙1l0∙21	pruning	v2 l0∙3l0 H 0 IT-
v310."-0』0 h⅛12l	g⑸=O J v31010101010~l-→
Interaction FUnCtion 0
夕
Vm 卜0.1∣ 0.2∣ 0.5 卜0.3∣ 0.1 I	Vm I 0 I 0.2 I 0.5 I-0.3 I 0 I ►
V	5(V, D)
Figure 1: The basic idea of PEP.
where Vi ∈ Rni ×d is an embedding matrix of i-th field, n denotes the number of features in the
i-th field, and d denotes the size of embedding vectors. The model's embedding matrices V for all
fields of features can be formulated as follows,
V = {Vι, V2,..., Vm},	(3)
The prediction score could be calculated with V and model,s other parameters (mainly refer to the
parameters in prediction model) Θ as follows,
y = φ(x∣v, θ),	(4)
where y is the predicted probability and φ represent the prediction model, such as FM (Rendle,
2010) or AutoInt (Song et al., 2019). As for model training, to learn the models parameters, the
optimizer minimizes the training loss as follows,
min l(v, Θ, D),	(5)
where D = {x, y} represents the data fed into the model, X denotes the input feature, y denotes the
ground truth label, and L is the loss function. The Logloss is the most widely-used loss function in
recommendation tasks (Rendle, 2010; Guo et al., 2017; Song et al., 2019) and calculated as follows,
1	|D|
L = -∣d∣ X (yjlog(yj) + (1 - yj)log(I - yj)),	(6)
|D| j=1
where |D| is the total number of training samples and regularization terms are omitted for simplifi-
cation.
4	Methodology
4.1	Learnable Embedding Sizes through Pruning
As mentioned above, a feasible solution for memory-efficient embedding learning is to automati-
cally assign different embedding sizes d% for different features embeddings Vi, which is our goal.
However, to learn d% directly is infeasible due to its discreteness and extremely-large optimization
space. To address it, we propose a novel idea that enforce column-wise sparsity on V, which equiva-
lently shrinks the embedding size. For example, as it shown in Figure 1, the first value in embedding
v1 is pruned and set to zero, leading to a d1 = d1 - 1 embedding size in effect. Furthermore, some
unimportant feature embeddings, like v3, are dropped by set all values to zero3. Thus our method
can significantly cut down embedding parameters. Note that the technique of sparse matrix storage
help us to significantly save memory usage (Virtanen et al., 2020).
In such a way, we recast the problem of embedding-size selection into learning column-wise sparsity
for the embedding matrix V. To achieve that, we design a sparsity constraint on V as follows,
min L, s.t. ||V||0 ≤ k,	(7)
where ∣∣∙∣∣o denotes the L°-norm, i.e. the number of non-zeros and k is the parameter budget, which
is, the constraint on the total number of embedding parameters.
3Our PEP benefit from such kind of reduction, as demonstrated in Section 5.1, 5.3 and 5.4.
4
Published as a conference paper at ICLR 2021
However, direct optimization of Equation (7) is NP-hard due to the non-convexity of the L0-norm
constraint. To solve this problem, the convex relaxation of L0-norm, called L1-norm, has been
studied for a long time (Taheri & Vorobyov, 2011; Beck & Teboulle, 2009; Jain et al., 2014). For
example, the Projected Gradient Descent (PGD) (Jain et al., 2014) in particular has been proposed
to project parameters to L1 ball to make the gradient computable in almost closed form. Note that
the L1 ball projection is also known as Soft Thresholding (Kusupati et al., 2020). Nevertheless, such
methods are still faced with two major issues. First, the process of projecting the optimization values
onto L1 ball requires too much computation cost, especially when the recommendation model has
millions of parameters. Second, the parameter budget k requires human experts to manually set at a
global level. Considering that features have various importance for recommendation, such operation
is obviously sub-optimal. To tackle those two challenges, inspired by Soft Threshold Reparameter-
ization (Kusupati et al., 2020), we directly optimize the projection of V and adaptively pruning the
V via learnable threshold(s) which can be updated by gradient descent. The re-parameterization of
V can be formulated as follows,
V = S (V, S) = Sign(V)ReLU(|V| - g(s)),
(8)
where V ∈ RN×d denotes the re-parameterized embedding matrix, and g(s) serves as a pruning
threshold value, of which sigmoid function is a simple yet effective solution.4 We set the initial
value of trainable parameter S ∈ R (called Sinit) to make sure that the threshold(s) g start close to
zero. The sign(∙) function converts positive input value to 1 and negative input value to -1, and zero
input will keep unchanged.
As S(V, S) is applied to each element of V, and thus the optimization problem in Equation (5) could
be redefined as follows,
min L(S(V, S), Θ, D).
(9)
Then the trainable pruning parameter S could be jointly optimized with parameters of the recommen-
dation models φ, through the standard back-propagation. Specifically, the gradient descent update
equation for V at t-th step is formulated as follows,
V(t+1) — V㈤一ηtNs(Vs)L (S(V⑴，s), D)Θ VVS(V, s),
(10)
where ηt is t-th step learning rate and Θ denotes the Hadamard product. To solve the non-
differentiablilty of S(∙), we use sub-gradient to reformat the update equation as follows,
V(t+1) — V㈤-ηtVs(Vs)L (S(V㈤,s), D) Θ 1 {δ(V⑴,s) = θ},
(11)
where 1{∙} denotes the indicator function. Then, as long as we choose a continuous function g in
S(∙), then the loss function L (S(V(t), s), D) wouldbe continuous for s. Moreover, the sub-gradient
of L with respect to s can be used of gradient descent on s as well.
Thanks to the automatic differentiation framework like TensorFlow (Abadi et al., 2016) and PyTorch
(Paszke et al., 2019), we are free from above complex gradient computation process. Our PEP
code can be found in Figure 7 of Appendix A.2. As we can see, it is quite simple to incorporate
with existing recommendation models, and there is no need for us to manually design the back-
propagation process.
4.2	Retrain with Lottery Ticket Hypothesis
After pruning the embedding matrix V to the target parameter budget P , we could create a binary
pruning mask m ∈ {0, 1}V that determines which parameter should remain or drop. Then we
retrain the base model with a pruned embedding table. The Lottery Ticket Hypothesis (Frankle &
Carbin, 2018) illustrates that a sub-network in a randomly-initialized dense network can match the
original network, when trained in isolation in the same number of iterations. This sub-network is
called the winning ticket. Hence, instead of randomly re-initializing the weight, we retrain the base
model while re-initializing the weights back to their original (but masked now) weights m Θ V0 .
This initiation strategy can make the training process faster and stable, keeping the performance
consistent, which is shown in Appendix A.6.
4More details about how to choose a suitable g(s) are provided in Appendix A.1.
5
Published as a conference paper at ICLR 2021
(a) FM
(b) DeepFM
Figure 2: AUC-# Parameter curve on MovieLens-1M with three base models.
(c) AutoInt
4.3	Pruning with Finer Granularity
Threshold parameter s in Equation (8) is set to a scalar that values of every dimension will have the
same threshold value. We name this version as global wise pruning. However, different dimensions
in the embedding vector vi may have various importance, and different fields of features may also
have highly various importance. Thus, values in the embedding matrix require different sparsity
budgets, and pruning with a global threshold may not be optimal. To better handle the heterogeneity
among different features/dimensions in V, we design following different threshold tactic with dif-
ferent granularities. (1) Dimension Wise: The threshold parameter s is set as a vector s ∈ Rd. Each
value in an embedding will be pruned individually. (2) Feature Wise: The threshold parameter s is
defined as a vector s ∈ RN . Pruning on each features’ embedding could be done in separate ways.
(3) Feature-Dimension Wise: this variant combines the above genre of threshold to obtain the finest
granularity pruning. Specifically, thresholds are set as a matrix s ∈ RN ×d.
5	Experiments
Dataset. We use three benchmark datasets: MovieLens-1M, Criteo, and Avazu, in our experiments.
Metric. We adopt AUC (Area Under the ROC Curve) and Logloss to measure the performance of
models.
Baselines and Base Recommendation Models. We compared our PEP with traditional UE (short
for Uniform Embedding). We also compare with the recent advances in flexible embedding sizes:
MGQE (Kang et al., 2020), MDE (Ginart et al., 2019), and DartsEmb (Zhao et al., 2020b)5. We
deploy PEP and all baseline methods to three representative feature-based recommendation models:
FM (Rendle, 2010), DeepFM (Guo et al., 2017), and AutoInt (Song et al., 2019), to compare their
performance6.
5.1	Recommendation Accuracy and Parameter Number
We present the curve of recommendation performance and parameter number in Figure 2, 3 and 4,
including our method and state-of-the-art baseline methods. Since there is a trade-off between rec-
ommendation performance and parameter number, the curves are made of points that have different
sparsity demands7.
• Our method reduces the number of parameters significantly. Our PEP achieves the highest
reduce-ratio of parameter number in all experiments, especially in relatively large datasets (Criteo
and Avazu). Specifically, in Criteo and Avazu datasets, our PEP-0 can reduce 99.90% param-
eter usage compared with the best baseline (from the 106 level to the 103 level, which is very
significant.). Embedding matrix with such low parameter usage means that only hundreds of em-
beddings are non-zero. By setting less-important features’ embedding to zero, our PEP can break
the limitation in existing methods that minimum embedding size is one rather than zero. We con-
duct more analysis on the MovieLens dataset in Section 5.3 and 5.4 to help us understand why our
method can achieve such an effective parameter decreasing.
5We do not compare with NIS (Joglekar et al., 2020) since it has not released codes and its reinforcement-
learning based search is really slow.
6More details of implementation and above information could be found in Appendix A.4.
7We report five points of our method, marked from 0 to 4.
6
Published as a conference paper at ICLR 2021
(a) FM	(b) DeepFM	(c) AutoInt
Figure 3:	AUC-# Parameter curve on Criteo with three base models.
(a) FM	(b) DeepFM	(c) AutoInt
Figure 4:	AUC-# Parameter curve on Avazu with three base models.
• Our method achieves strong recommendation performance. Our method consistently outper-
forms the uniform embedding based model and achieves better accuracy than other methods in
most cases. Specifically, for the FM model on the Criteo dataset, the relative performance im-
provement of PEP over UE is 0.59% and over DartsEmb is 0.24% in terms of AUC. Please note
that the improvement of AUC or Logloss at such level is still considerable for feature-based rec-
ommendation tasks (Cheng et al., 2016; Guo et al., 2017), especially considering that we have
reduced a lot of parameters. A similar improvement can also be observed from the experiments
on other datasets and other recommendation models. It is worth noting that our method could
keep a strong AUC performance under extreme sparsity-regime. For example, when the number
of parameters is only in the 103 level (a really small one), the recommendation performance still
remarkably outperforms the Linear Regression model (more details can be found in Appendix
A.5).
To summarize it, with the effectiveness of recommendation accuracy and parameter-size reduction,
the PEP forms a frontier curve encompassing all the baselines at all the levels of parameters. This
verifies the superiority that our method can handle different parameter-size budgets well.
5.2 Efficiency Analysis of our Method
As is shown in Section 5.1, learning a suitable parameter budget can yield a higher-accuracy model
while reducing the model’s parameter number. Nevertheless, it will induce additional time to find
apposite sizes for different features. In this section, we study the computational cost and compare
the runtime of each training epoch between PEP and DartsEmb on the Criteo dataset. We implement
both models with the same batch size and test them on the same platform.
The training time of each epoch on three different models is given in Table 2. We can observe
that our PEP’s additional computation-cost is only 20% to 30%, which is acceptable compared
with the base model. DartsEmb, however, requires nearly double computation time to search a good
embedding size in its bi-level optimization process. Furthermore, DartsEmb needs to search multiple
times to fit different memory budgets, since each one requires a complete re-running. Different
from DartsEmb, our PEP can obtain several embedding schemes, which can be applied in different
application scenarios, in only a single running. As a result, our PEP’s time cost on embedding size
search can be further reduced in real-world systems.
7
Published as a conference paper at ICLR 2021
Table 2: Runtime of each training epoch on Criteo between base model, DartsEmb, and our PEP.
Runtime (Second)	FM	DeepFM	AutoInt	Avg. time increase
Base Model	1,039	1,222	1,642	0
DartsEmb	2,239	2285	3,154	98.02%
PEP	1,341	1,525	1,963	24.47%
without pruning ∣ # param 264256
-0.007 0.05 0.011 0.013 0.∩05 0.∩12 0.003
0.011 0.098 0.087 0.029 0.008 0.036
0.002 0.003 0.005 0.006
(a) VV> on original embedding
0.029 0.045 0.01 0.029 0.002
-0.005 0.035 0.008 0.01 0.005 0.008 0.003
0.036 0.029 0.008 0.06 0.005
# Interactions
with pruning ∣ # param 23675
(b) VV> on sparse embedding
(c) Variation of matrix values be-
tween original and sparse embed-
ding
Figure 5: Interpretable analysis on MovieLens-1M dataset.
(a) PDF of feature frequencies of (b) Sparsity trajectory generated (c) Sparsity heatmap generated by
MovieLens-1M dataset	by PEP on FM	PEP on FM
Figure 6: Correlation between Sparsity and Frequency.
5.3	Interpretable Analysis on Pruned Embeddings
The feature-based recommendation models usually apply the embedding technique to capture two
or high order feature interactions. But how does our method work on features interactions? Does
our method improve model performance by reducing noisy feature interactions? In this section, we
conduct an interpretable analysis by visualizing the feature interaction matrix, calculated by VV> .
Each value in the matrix is the normalized average of the absolute value of those two field features’
dot product result, of which the higher indicates those two fields have a stronger correlation.
Figure 5 (a) and 5 (b) illustrate the interaction matrix without and with pruning respectively, and
5 (c) shows the variation of matrix values. We can see that our PEP can reduce the parameter
number between unimportant field interaction while keeping the significance of those meaningful
field features’ interactions. By denoising those less important feature interactions, the PEP can
reduce embedding parameters while maintaining or improving accuracy.
5.4	Correlation between Sparsity and Frequency
As is shown in Figure 6 (a), feature frequencies among different features are highly diversified.
Thus, using embeddings with uniform size may not handle their heterogeneity, and this property
play an important role in embedding size selection. Hence, some recent works (Zhao et al., 2020b;
Ginart et al., 2019; Cheng et al., 2020; Kang et al., 2020; Zhang et al., 2020; Joglekar et al., 2020)
explicitly utilize the feature frequencies. Different from them, our PEP shrinks the parameter in an
end-to-end automatic way, thus circumvents the complex human manipulation. Nevertheless, the
8
Published as a conference paper at ICLR 2021
frequency of features is one of the factors that determines whether one feature is important or not.
Thus, we study whether our method can detect the influence of frequencies and whether the learned
embedding sizes are relevant to the frequency.
We first analyze the sparsity8 trajectory during training, which is shown in Figure 6 (b), where differ-
ent colors indicate different groups of features divided according to their popularity. For each group,
we first calculate each feature’s sparsity, then compute the average on all features. Shades in pic-
tures represent the variance within a group. We can observe that PEP tends to assign high-frequency
features larger sizes to make sure there is enough representation capacity. For low-frequency fea-
tures, the trends are on the contrary. These results are accord to the postulation that high-frequency
features deserve more embedding parameters while a few parameters are enough for low-frequency
feature embeddings.
Then we probe the relationship between the sparsity of pruned embedding and frequencies of each
feature. From Figure 6 (c), we can observe that the general relationship is concord with the above
analysis. However, as we can see, some low-frequency features are assigned rich parameters, and
some features with larger popularity are assigned small embedding size. This illustrates that simply
allocating more parameters to high-frequency features, as most previous works do, can not handle
the complex connection between features and their popularities. Our method performs pruning based
on data, which can reflect the feature intrinsic proprieties, and thus can cut down parameters in a
more elegant and efficient way.
6	Conclusion
In this paper, we approach the common problem of fixed-size embedding table in today’s feature-
based recommender systems. We propose a general plug-in framework to learn the suitable em-
bedding sizes for different features adaptively. The proposed PEP method is efficient can be easily
applied to various recommendation models. Experiments on three state-of-the-art recommendation
models and three benchmark datasets verify that PEP can achieve strong recommendation perfor-
mance while significantly reducing the parameter number and can be trained efficiently.
7	Acknowledgements
This work was supported in part by The National Key Research and Development Program of China
under grant 2020AAA0106000, the National Natural Science Foundation of China under U1936217,
61971267, 61972223, 61941117, 61861136003.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, and Xiaoqiang Zhang. Tensorflow: A
system for large-scale machine learning. 2016.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. Siam Journal on Imaging Sciences, 2(1):183-202, 2009.
Heng Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado, Wei Chai, and Mustafa Ispir. Wide & deep learning for recom-
mender systems. 2016.
Weiyu Cheng, Yanyan Shen, and Linpeng Huang. Differentiable neural input search for recom-
mender systems. arXiv preprint arXiv:2006.04466, 2020.
Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations.
In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.
Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey. J.
Mach. Learn. Res., 20(55):1-21, 2019.
8We define the sparsity of an embedding by the ratio of the number of non-zero values to its original
embedding size.
9
Published as a conference paper at ICLR 2021
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018.
Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. Mixed dimen-
sion embeddings with application to memory-efficient recommendation systems. arXiv preprint
arXiv:1909.11810, 2019.
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-
machine based neural network for ctr prediction. In Proceedings of the 26th International Joint
Conference on Artificial Intelligence ,pp.1725-1731, 2017.
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, sys-
tems, challenges. Springer Nature, 2019.
Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-
dimensional m-estimation. 2014.
Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams, Pranav Khaitan,
Jiahui Liu, and Quoc V Le. Neural input search for large scale recommendation models. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 2387-2397, 2020.
Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, and
Ed H Chi. Learning multi-granular quantized embeddings for large-vocab categorical features in
recommender systems. In Companion Proceedings of the Web Conference 2020, pp. 562-566,
2020.
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham
Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In
Proceedings of the 37th International Conference on Machine Learning, 2020.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In
International Conference on Learning Representations, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Steffen Rendle. Factorization machines. In 2010 IEEE International Conference on Data Mining,
pp. 995-1000. IEEE, 2010.
Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. Compositional em-
beddings using complementary partitions for memory-efficient recommendation systems. In Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pp. 165-175, 2020.
Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang.
Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceed-
ings of the 28th ACM International Conference on Information and Knowledge Management, pp.
1161-1170, 2019.
Omid Taheri and Sergiy A. Vorobyov. Sparse channel estimation with lp-norm and reweighted
l1-norm penalized least mean squares. In IEEE International Conference on Acoustics, 2011.
Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, and Paul Van Mulbregt. Au-
thor correction: Scipy 1.0: fundamental algorithms for scientific computing in python. Nature
Methods, 17(Suppl. 1):1-12, 2020.
Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Ku-
mar Myana, Deepak Dilipkumar, Suvadip Paul, Ikuhiro Ihara, et al. Model size reduction using
frequency based double hashing for recommender systems. In Fourteenth ACM Conference on
Recommender Systems, pp. 521-526, 2020.
10
Published as a conference paper at ICLR 2021
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey
and new perspectives. ACM Computing Surveys (CSUR), 52(1):1-38,2019.
Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji
Gao, and Bo Long. Memory-efficient embedding for recommendations. arXiv preprint
arXiv:2006.14827, 2020a.
Xiangyu Zhao, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and Jiliang Tang. Au-
toemb: Automated embedding dimensionality search in streaming recommendations. arXiv
preprint arXiv:2002.11252, 2020b.
11
Published as a conference paper at ICLR 2021
A	Appendix
A.1 DESCRIPTION OF g(s)
Following Kusupati et al. (2020), a proper threshold function g(s) should have following three
properties:
1.
g(s) > 0, lim g(s) = 0, and lim g(s) = ∞.
s→-∞	s→∞
2.
∃G ∈ R++ 3 0 < g0 (s) ≤ G ∀s ∈ R.
3.
g0 (sinit ) < 1 which reduce the updating speed of s at the initial pruning.
A.2 PyTorch code of PEP
We present the main codes of PEP here since it is really easy-to-use and can plug in various
embedding-based recommendation models.
import torch
import torch.nn as nn
import torch.nn*functional as F
class PEPCnntModule):
ff use PEP to replace the original embedding module
def _init_(self, opt):
5∣JPer(PEP, self).__init―()
self. thresholdtype = opt[, thresho'ld-type, ] #
self.Iatentdim = opt[rlatent-dim1]	ff
self.feature-πum - sum(opt['field-dims,])	#
init = opt[,threshold_init,]	ff
self.g = torch.sigmoid	ff
self.s = self.init_threshold(init)
(torch.nn. Embedding)
define the granular!ty of s
define the initial embedding size d
total numbers of features
initial value of s
define the threshold function g
ff initialize the embedding matrix V
self.v = nn*Parameter(torch<rand(self.feature_num, self.latent-dirπ))
nn. init.xavier unifoι,m (self .v)
self.sparsev = self.v.data
def
init threshoɪd(selfr init): ff initialize threshold with different granularities
if self.thresholdtype == τdimension*:
s = nn.ParameterCinit *
elif self.threshold type ==
s = nn.ParameterCinit *
elif self. threshold type ==
s = nn.ParameterC init *
else:
s = nn.ParameterCinit *
return s
torch.ones([self.latent dim]))
'featureτ:
torch.ones([self.featurenum, 1]))
'feature-dimensionτ:
torch.ones([self.feature-numf self.latent-dim]))
torch.OneS(I)) # set as global by default
soft threshold(self, vr s): # the reparameterization function 5(% D)
return torch.Sign(V) * toreh.relu(torch.abs(v)-self.g(s))
def forward(seɪff x):
self.sparse-v = self.soft-threshoɪd(self.vτ self.s) # the pruned sparse embedding matrix
xv = F.embedding(xf self.sparse-v) ff retrieving feature' embeddings by given feature x
return xv
Figure 7: PyTorch code of PEP.
A.3 Whole process of PEP
We summarizes the pruning and retrain process by Algorithm 1.
12
Published as a conference paper at ICLR 2021
Algorithm 1 Our PEP
Input: Initial embedding V(0), base model φ, and target parameter P.
Output: Well trained sparsity embedding V.
1:	while do not reach P do
2:	Pruning V through Equation 9.
3:	end while
4:	Obtain binary pruning mask m = 1{V(t)}.
5:	Reset the remaining embedding parameter to initial values.
6:	while do not coverage do
7:	Minimize the training loss L(V(0)	m, D) with SGD.
8:	end while
Table 3: StatiSticS of three UtiliZed benchmark datasets.
Dataset	# Samples	# Fields	# Features
MovieLens-1M	-739, 015^^	7	3, 864
Criteo	45, 840, 617	39	1, 086, 810
AvaZu	40,400, 000	22	645, 394
A.4 Experimental Setup
A.4. 1 Datasets
We experiment with three public benchmark datasets: MovieLens-1M, Criteo, and AvaZu. Table 3
summariZes the statistics of datasets.
•	MovieLens-1M9. It is a widely-used benchmark dataset and contains timestamped user-movie
ratings ranging from 1 to 5. Following AutoInt (Song et al., 2019), we treat samples with a rating
1, 2 as negative samples and samples with a rating 4, 5 as positive samples. Other samples will be
treat as neutral samples and removed.
•	Criteo10. This is a benchmark dataset for feature-based recommendation task, which contains 26
categorical feature fields and 13 numerical feature fields. It has about 45 million users’ clicking
records on displayed ads.
•	Avazu11. AvaZu dataset contains 11 days’ user clicking behaviors which are released for the
Kaggle challenge, There are 22 categorical feature fields in the dataset, and parts of the fields are
anonymous.
Preprocessing Following the general preprocessing steps (Guo et al., 2017; Song et al., 2019), for
numerical feature fields in Criteo, we employ the log transformation of log2 (x) if x > 2 proposed
by the winner of Criteo Competition12 to normaliZe the numerical features. Besides, we consider
features of which the frequency is less than ten as unknown and treat them as a single feature
“unknown” for Criteo and AvaZu datasets. For each dataset, all the samples are randomly divided
into training, validation, and testing set based on the proportion of 80%, 10%, and 10%.
A.4.2 Performance Measures
We evaluate the performance of PEP with the following two metrics:
•	AUC. The area under the Receiver Operating Characteristic or ROC curve (AUC) means the
probability to rank a randomly chosen positive sample higher than a randomly chosen negative
sample. A model with higher AUC indicates the better performance of the model.
9https://grouplens.org/datasets/movielens
10https://www.kaggle.com/c/criteo-display-ad-challenge
11https://www.kaggle.com/c/avazu-ctr-prediction
12https://www.csie.ntu.edu.tw/r01922136/kaggle-2014-criteo.pdf
13
Published as a conference paper at ICLR 2021
•	Logloss. As a loss function widely used in the feature-based recommendation, Logloss on test data
can straight way evaluate the model’s performance. The lower the model’s Logloss, the better the
model’s performance.
A.4.3 Baselines
We compared our proposed method with the following state-of-the-art methods:
•	UE (short for Uniform Embedding). The uniform-embedding manner is commonly accepted in
existing recommender systems, of which all features have uniform embedding sizes.
•	MGQE (Kang et al., 2020). This method retrieves embedding fragments from a small size of
shared centroid embeddings, and then generates final embedding by concatenating those frag-
ments. MGQE learns embeddings with different capacities for different items. This method is the
most strongest baseline among embedding-parameter-sharing methods.
•	MDE (short for Mixed Dimension Embedding (Ginart et al., 2019)). This method is based on
human-crafted rule, and the embedding size of a specific feature is proportional to its popularity.
Higher-frequency features will be assigned larger embedding sizes. This is the state-of-the-art
human-rule-based method.
•	DartsEmb (Zhao et al., 2020b). This is the state-of-the-art neural architecture search-based based
method which allows features to automatically search for the embedding sizes in a given space.
A.4.4 Implementation Details
Following AutoInt (Song et al., 2019) and DeepFM (Guo et al., 2017), we employ Adam optimizer
with the learning rate of 0.001 to optimize model parameters in both the pruning and re-training
stage. For g(s), We apply g(s)= [+；…in all experiments and initialize the S to -15, -150 and
-150 in MovieLens-1M, Criteo and Avazu datasets respectively. Moreover, the granularity of PEP
is set as Dimension-Wise for PEP-2, PEP-3, and PEP-4 on Criteo and Avazu datasets. And others
are set as Feature Dimension-Wise. The base embedding dimension d is set to 64 for all the models
before pruning. We deploy our method and other baseline methods to three state-of-the-art models:
FM (Rendle, 2010), DeepFM (Guo et al., 2017), and AutoInt (Song et al., 2019), to compare their
performance. Besides, in the retrain stage, We exploit the early-stopping technique according to the
loss of validation dataset during training. We use PyTorch (Paszke et al., 2019) to implement our
method and train it With mini-batch size 1024 on a single 12G-Memory NVIDIA TITAN V GPU.
Implementation of Baseline For Uniform Embedding, We test the embedding size varying from
[8, 16, 32, 64], for the MovieLens-1M dataset. For Criteo and Avazu dataset, We vary the embedding
size from [4, 8, 16] because performance starts to drop When d > 16.
For other baseline methods, We first turn the hyper-parameters to make models have the highest
recommendation performance or highest parameter reduction rate. Then We tune those methods
that can balance those tWo aspects. We provide the experimental details of our implementation for
these baseline methods as beloW, folloWing the settings of the original papers. For the grid search
space of MDE, We search the baseline dimension d from [4, 8, 16, 32], the number of blocks K from
[8, 16], and α from [0.1, 0.2, 0.3]. For MGQE, We search the baseline dimension d from [8, 16, 32],
the number of subspace D from [4, 8, 16], and the number of centroids K from [64, 128, 256, 512].
For DartsEmb, We choose three different candidate embedding spaces to meet the different memory
budgets: {1, 2, 8}, {2, 4, 16} and {4, 8, 32}.
A.5 Comparison between PEP-0 and Linear Regression
The Linear Regression (LR) model is an embedding-free model that only makes predictions based on
the linear combination of raW features. Thence, it is Worth comparing our method on the extremely-
sparse level (PEP-0) With LR.
Table 4 shoWs that our PEP-0 significantly outperforms the LR in all cases. This result verity that
our PEP-0 does not depend on the LR part in FM and DeepFM to remain a strong recommendation
performance. Therefore, even at an extremely-sparse level, our PEP still has high application value
in the real-World scenarios.
14
Published as a conference paper at ICLR 2021
Table 4: Performance comparison between PEP-0 and Linear Regression.
Methods	MovieLens-1M		Criteo		Avazu	
	AUC	# Param	AUC	# Param	AUC	# Param
LR	0.7717	0	0.7881	0	0.7499	0
PEP-0 (FM)	0.8368	6,541	0.7941	1,067	0.7598	1,479
PEP-0 (DeepFM)	0.8491	8,604	0.7986	1,227	0.7622	2,215
PEP-0 (AutoInt)	0.8530	9,281	0.7922	3,116	0.7607	2,805
PEP-0 (AutoInt+LR)	-	-	0.7980	1,117	0.7620	2,225
It is worth noting that the AutoInt model does not contain the LR component, so the PEP-0 in
AutoInt on Criteo and Avazu dataset lead to a large performance drop. We try to include LR in
PEP-0 in AutoInt and test the performance13. As we can see, the accuracy on Criteo and Avazu
outperforms the AutoInt without LR; It can be explained that LR helps our PEP-0 acquire a more
stable performance.
A.6 The Lottery Ticket Hypothesis
In the retraining stage in Section 4.2, we rely on the Lottery Ticket Hypothesis to reinitialize the
pruned embeddings table (called winning ticket) into their original initial values. Here we conduct
experiments to verify the effectiveness of this operation in our PEP. We compare our method with
its variation that uses random re-initialization for retraining to examine the influence of initializa-
tion. We also compare the standard PEP with the original base recommendation model to verify
the influence of embedding pruning. To evaluate the importance of retraining, we further test the
performance of PEP with the pruning stage only. We choose FM as the base recommendation model
and use the same settings as the above experiments.
We present the results in Figure 8 and 9. We can observe that the winning ticket with original
initialization parameters can make the training procedure faster and obtain higher recommendation
accuracy compared with random re-initialization. This demonstrates the effectiveness of our design
of retraining. Moreover, the randomly reinitialize winning ticket still outperforms the unpruned
model. By reducing the less-important features’ embedding parameters, model performance could
benefit from denoising those over-parametered embeddings. This can be explained that it is likely
to get over-fitted for those over-parameterized embeddings when embedding sizes are uniform.
Moreover, it is clear that the performance of PEP without retraining gets a little bit downgrade, but
it still outperforms the original models. And the margin between without retrain and the original
model is larger than the margin between with and without retraining. These results demonstrate
that the PEP chiefly benefits from the suitable embedding size selection. We conjecture the benefit
of retraining: during the search stage, less-important elements in embedding matrices are pruned
gradually until the training procedure reaches a convergence. However, in earlier training epochs
when these elements have not been pruned, they may have negative effects on the gradient updates
for those important elements. This may make the learning of those important elements suboptimal.
Thus, a retraining step can eliminate such effects and improve performance.
A.7 Pruning with Finer Granularity
In this section, we analyze the four different thresholds with different granularity mentioned in Sec-
tion 4.3. The experiments are conducted on the MovieLens-1M dataset with base model FM. Figure
10 (a) and (b) demonstrates the varying of embedding parameters and test AUC evolving with train-
ing epoch. As we can see, the Feature-Dimension granularity can reduce much more embedding
parameters than others. Meanwhile, it achieves the highest performance at the retrain stage com-
pared with other granularities. With the minimum granularity, the Feature-Dimension wise pruning
can effectively determine the importance of embedding values. Besides, the Dimension-wise prun-
13We omit the results of AutoInt with LR on the MovieLens-1M dataset because there is no performance
drop for the AutoInt model compared with other models.
15
Published as a conference paper at ICLR 2021
Figure 8: Logloss and AUC as training proceeds on Criteo dataset (choosing FM as the base model).
Avazu
Epoch
Logloss
(a)
0.766
0.765
0.764
0.762
0.761
o
< 0.763
(b) AUC
Figure 9: Logloss and AUC as training proceeds on Avazu dataset (choosing FM as the base model).
ing can achieve comparable AUC with fewer training epochs. Hence we adopt this granularity on
PEP-2, PEP-3, and PEP-4 in large datasets to save time spent on training.
A.8 ABOUT LEARNABLE g(s)
Pruning threshold(s) g(s) can be learned from training data to reduce parameter usage in the em-
bedding matrix. However, why can our PEP learn suitable g(s) with training data? We deduce that
the increase of s in g(s) can decrease the training loss. In other words, our PEP tries to update s in
the optimization process to achieve lower training loss.
In Figure 11, we plot the FM’s training curves with/without PEP on MovieLens-1M and Criteo
datasets to confirm our assumption. Our PEP can achieve much lower training loss when pruning.
Besides, it verifies that our PEP could learn embedding sizes in a stable form.
The stability shown in Figure 11 can be explained that our PEP obtains a relatively stable embedding
parameter number at later stage of pruning (e.g., when epoch is larger than 30 in MovieLens dataset)
as shown in Figure 11. And embedding parameters are well-trained. Thus, the training loss curve
looks relatively stable. Note that the figure shows a sequence of changing thresholds. The point
when we get the embedding table for some sparsity level is not a converged point for this exact
level, which instead requires retraining with a fixed threshold.
16
Published as a conference paper at ICLR 2021
S.J-'uEE.lEd ⅝
MovieLens-IM
——Global
---Dimension
....Feature
----Feature-Dimension
250000
200000
150000
100000
50000
MovieLens-IM
0 15 30 45 60 75 90 105 120 135 150
Epoch
0.86
0.85
U 0.84
<
0.83
0.82
O 20 40 60 80 IOO 120 140 160 180
Epoch
(a) Numbers of embedding parameters evolving (b) Test AUC evolving with training epoch in-
with training epoch increase	crease at retrain stage
Figure 10: Influence of different granularity on MovieLens-1M dataset (Choose FM as base model)
MovieLens-IM
1.4
1.2
j1-θ
6
W 0.8
c
'S
H 0.6
0.4
0.2
Criteo
2000	4000	6000	8000	10000
Gradient Steps
(b) Criteo
0.525
0.500
8 0.475
ɪ 0.450
⊂
S 0.425
0.400
0.375
0.350
0
(a) MovieLens-1M
Figure 11: Training loss of FM with/without PEP
17