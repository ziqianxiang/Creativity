Published as a conference paper at ICLR 2021
Large-width functional asymptotics for deep
Gaussian neural networks
Daniele Bracale1, Stefano Favaro1,2, Sandra Fortini3, Stefano Peluchetti4
1 University of Torino, 2 Collegio Carlo Alberto, 3 Bocconi University, 4 Cogent Labs
Ab stract
In this paper, we consider fully-connected feed-forward deep neural networks
where weights and biases are independent and identically distributed according
to Gaussian distributions. Extending previous results (Matthews et al., 2018a;b;
Yang, 2019) we adopt a function-space perspective, i.e. we look at neural networks
as infinite-dimensional random elements on the input space RI . Under suitable
assumptions on the activation function we show that: i) a network defines a
continuous stochastic process on the input space RI ; ii) a network with re-scaled
weights converges weakly to a continuous Gaussian Process in the large-width limit;
iii) the limiting Gaussian Process has almost surely locally Y-HOlder continuous
paths, for 0 < γ < 1. Our results contribute to recent theoretical studies on the
interplay between infinitely-wide deep neural networks and Gaussian Processes by
establishing weak convergence in function-space with respect to a stronger metric.
1 Introduction
The interplay between infinitely-wide deep neural networks and classes of Gaussian Processes has its
origins in the seminal work of Neal (1995), and it has been the subject of several theoretical studies.
See, e.g., Der & Lee (2006), Lee et al. (2018), Matthews et al. (2018a;b), Yang (2019) and references
therein. Let consider a fully-connected feed-forward neural network with re-scaled weights composed
of L ≥ 1 layers of widths n1 , . . . , nL, i.e.
I
fi(1)(x) = X wi(,1j) xj + bi(1)
j=1
nl-1
fi(l)(x) = √n== X w(ljφ(f(lT)(X)) + b(l)
l-1 j=1
i = 1,. . . ,n1
l = 2, . . . , L, i = 1, . . . , nl
(1)
where φ is a non-linearity and x ∈ RI is a real-valued input of dimension I ∈ N. Neal (1995)
considered the case L = 2, a finite number k ∈ N of fixed distinct inputs (x(1), . . . , x(k)), with
each x(r) ∈ RI, and weights wi(,lj) and biases bi(l) independently and identically distributed (iid) as
Gaussian distributions. Under appropriate assumptions on the activation φ Neal (1995) showed
that: i) for a fixed unit i, the k-dimensional random vector (fi(2) (x(1)), . . . , fi(2) (x(k))) converges
in distribution, as the width n1 goes to infinity, to a k-dimensional Gaussian random vector; ii) the
large-width convergence holds jointly over finite collections of i’s and the limiting k-dimensional
Gaussian random vectors are independent across the index i. These results concerns neural networks
with a single hidden layer, but Neal (1995) also includes preliminary considerations on infinitely-wide
deep neural networks. More recent works, such as Lee et al. (2018), established convergence results
corresponding to Neal (1995) results i) and ii) for deep neural networks under the assumption that
widths n1 , . . . , nL go to infinity sequentially over network layers. Matthews et al. (2018a;b) extended
the work of Neal (1995); Lee et al. (2018) by assuming that the width n grows to infinity jointly over
network layers, instead of sequentially, and by establishing joint convergence over all i and countable
distinct inputs. The joint growth over the layers is certainly more realistic than the sequential growth,
since the infinite Gaussian limit is considered as an approximation of a very wide network. We
operate in the same setting of Matthews et al. (2018b), hence from here onward n ≥ 1 denotes the
1
Published as a conference paper at ICLR 2021
common layer width, i.e. n1, . . . , nL = n. Finally, similar large-width limits have been established
for a great variety of neural network architectures, see for instance Yang (2019).
The assumption of a countable number of fixed distinct inputs is the common trait of the literature
on large-width asymptotics for deep neural networks. Under this assumption, the large-width limit
of a network boils down to the study of the large-width asymptotic behavior of the k-dimensional
random vector (fi(l) (x(1)), . . . , fi(l) (x(k))) over i ≥ 1 for finite k. Such limiting finite-dimensional
distributions describe the large-width distribution of a neural network a priori over any dataset, which
is finite by definition. When the limiting distribution is Gaussian, as it often is, this immediately
paves the way to Bayesian inference for the limiting network. Such an approach is competitive with
the more standard stochastic gradient descent training for the fully-connected architectures object of
our study (Lee et al., 2020). However, knowledge of the limiting finite-dimensional distributions is
not enough to infer properties of the limiting neural network which are inherently uncountable such
as the continuity of the limiting neural network, or the distribution of its maximum over a bounded
interval. Results in this direction give a more complete understanding of the assumptions being made
a priori, and hence whether a given model is appropriate for a specific application. For instance, Van
Der Vaart & Van Zanten (2011) shows that for Gaussian Processes the function smoothness under the
prior should match the smoothness of the target function for satisfactory inference performance.
In this paper we thus consider a novel, and more natural, perspective to the study of large-width
limits of deep neural networks. This is an infinite-dimensional perspective where, instead of fixing
a countable number of distinct inputs, we look at fi(l) (x, n) as a stochastic process over the input
space RI . Under this perspective, establishing large-width limits requires considerable care and,
in addition, it requires to show the existence of both the stochastic process induced by the neural
network and its large-width limit. We start by proving the existence of i) a continuous stochastic
process, indexed by the network width n, corresponding to the fully-connected feed-forward deep
neural network; ii) a continuous Gaussian Process corresponding to the infinitely-wide limit of the
deep neural network. Then, we prove that the stochastic process i) converges weakly, as the width n
goes to infinity, to the Gaussian Process ii) jointly over all units i. As a by-product of our results,
We show that the limiting Gaussian Process has almost surely locally Y-HGlder continuous paths, for
0 < γ < 1. To make the exposition self-contained we include an alternative proof of the main result
of Matthews et al. (2018a;b), i.e. the finite-dimensional limit for full-connected neural networks.
The major difference between our proof and that of Matthews et al. (2018b) is due to the use of the
characteristic function to establish convergence in distribution, instead of relying on a CLT (Blum
et al., 1958) for exchangeable sequences.
The paper is structured as follows. In Section 2 we introduce the setting under which we operate,
whereas in Section 3 we present a high-level overview of the approach taken to establish our results.
Section 4 contains the core arguments of the proof of our large-width functional limit for deep
Gaussian neural networks, which are spelled out in detail in the supplementary material (SM). We
conclude in Section 5.
2 Setting
Let (Ω, H, P) be the probability space on which all random elements of interest are defined. Further-
more, let N(μ, σ2) denote a Gaussian distribution with mean μ ∈ R and strictly positive variance
σ2 ∈ R+, and let Nk (m, Σ) be a k-dimensional Gaussian distribution with mean m ∈ Rk and
covariance matrix Σ ∈ Rk×k. In particular, Rk is equipped with ∣∣ ∙ ∣∣Rk, the euclidean norm induced
by the inner product h∙,)Rk, and R∞ = ×∞=1R is equipped with ∣∙∣r∞, the norm induced by the
distance d(a, b)∞ = Pi≥1r ξ(% — bi|)/2i for a, b ∈ R∞ (Theorem 3.38 of Aliprantis & Border
(2006)), where ξ(t) = t/(1 +1) for all real values t ≥ 0. Note that (R, | ∙ |) and (R∞, ∣ ∙ ∣∣r∞ ) are
Polish spaces, i.e. separable and complete metric spaces (Corollary 3.39 of Aliprantis & Border
(2006)). We choose d∞ since it generates a topology that coincides with the product topology (line
5 of the proof of Theorem 3.36 of Aliprantis & Border (2006)). The space (S, d) will indicate a
generic Polish space such as R or R∞ with the associated distance. We indicate with SRI the space
of functions from RI into S and C(RI; S) ⊂ SRI the space of continuous functions from RI into S.
2
Published as a conference paper at ICLR 2021
Let ωi(,lj) be the random weights of the l-th layer, and assume that they are iid as N(0, σω2 ), i.e.
夕⑴(t)= E[eitω(j] = e-2σ2t2	(2)
ωi,j
is the characteristic function of ωi(,lj), for i ≥ 1, j = 1, . . . , n and l ≥ 1. Let bi(l) be the random biases
of the l-th layer, and assume that they are iid as N (0, σb2), i.e.
夕b(i)(t)= E[eitb(i)] = e-2 σbt2	(3)
is the characteristic function of bi(l), for i ≥ 1 and l ≥ 1. Weights ωi(,lj) are independent of biases bi(l),
for any i ≥ 1, j = 1, . . . , n and l ≥ 1. Let φ : R → R denote a continuous non-linearity. For the
finite-dimensional limit we will assume the polynomial envelop condition
∣Φ(s)∣≤ a + b|s|m,	(4)
for any s ∈ R and some real values a, b > 0 and m ≥ 1. For the functional limit we will use a
stronger assumption on φ, assuming φ to be Lipschitz on R with Lipschitz constant Lφ .
Let Z be a stochastic process on RI, i.e. for each X ∈ RI, Z(x) is defined on (Ω, H, P) and it takes
values in S. For any k ∈ N and x1, . . . , xk ∈ RI, let PxZ1,...,x = P(Z(x1) ∈ A1, . . . , Z(xk) ∈ Ak),
with A1, . . . , Ak ∈ B(S). Then, the family of finite-dimensional distributions of Z(x) is defined as
the family of distributions {PxZ x : x1, . . . , xk ∈ RI and k ∈ N}. See, e.g., Billingsley (1995).
1	,..., k
In Definition 1 and Definition 2 we look at the deep neural network (1) as a stochastic process on
input space RI, that is a stochastic process whose finite-dimensional distributions are determined by
a finite number k ∈ N of fixed distinct inputs (x(1), . . . , x(k)), with each x(r) ∈ RI. The existence
of the stochastic processes of Definition 1 and Definition 2 will be thoroughly discussed in Section 3.
Definition 1. For any fixed l ≥ 2 and i ≥ 1, let (fi(l)(n))n≥1 be a sequence of stochastic processes
on RI. That is, fi(l)(n) : RI → R, with x 7→ fi(l) (x, n), is a stochastic process on RI whose finite-
dimensional distributions are the laws, for any k ∈ N and x(1), . . . , x(k) ∈ RI, of the k-dimensional
random vectors
I
fi(1)(X, n) = fi(1)(X) = [fi(1)(x(1), n),..., fi(1)(x(k), n)]T = X ωi(,1j)xj + bi(1)1	(5)
j=1
n
fi(l)( X, n) = [fi(l)(x(1),n),...,fi(l)(x(k),n)]T = √1n X ω(j(φ • f； lT) (X, n)) + b(I) 1	(6)
j=1
where X = [x(1), . . . , x(k)] ∈ RI×k is a I × k input matrix of k distinct inputs x(r) ∈ RI, 1 denotes
a vector of dimension k × 1 of 1’s, xj denotes the j-th row of the input matrix and φ • X is the
element-wise application of φ to the matrix X. Let fr(,li)(X, n) = 1rT fi(l) (X, n) = fi(l)(x(r), n) denote
the r-th component of the k × 1 vector fi(l)(X, n), being 1r a vector of dimension k × 1 with 1 in the
r-the entry and 0 elsewhere.
Remark: in contrast to (1), we have defined (5)-(6) over an infinite number of units i ≥ 1 over each
layer l, but the dependency on each previous layer l - 1 remains limited to the first n components.
Definition 2. For any fixed l ≥ 2, let (F(l)(n))n≥1 be a sequence of stochastic processes on RI. That
is, F(l)(n) : RI → R∞, with x 7→ F(l)(x, n), is a stochastic process on RI whose finite-dimensional
distributions are the laws, for any k ∈ N and x(1), . . . , x(k) ∈ RI, of the k-dimensional random
vectors
[f ⑴(X )= [f(1)( X ),f21)( X),..J
[F(II (X, n) = [f(l) (X, n) f (X ,n),... ] T.
Remark: for k inputs, the vectorF(l)(X, n) is an ∞ × k array, and for a single input x(r), F(l) (x(r), n)
can be written as [f1(l) (x(r), n), f2(l)(x(r), n), . . . ]T ∈ R∞×1. We define F(rl)(X, n) = F(l) (x(r), n)
the r-th column of F(l) (X, n). When we write hF(l-1) (x, n), F(l-1) (y, n)iRn (see (8)) we treat
F(l)(x, n) and F(l) (y, n) as elements in Rn and not in R∞, i.e. we consider only the first n compo-
nents of F(l)(x, n) and F(l) (y, n).
3
Published as a conference paper at ICLR 2021
3	Plan sketch
We start by recalling the notion of convergence in law, also referred to as convergence in distribution or
weak convergence, for a sequence of stochastic processes. See Billingsley (1995) for a comprehensive
account.
Definition 3 (convergence in distribution). Suppose that f and (f (n))n≥1 are random elements in
a topological space C. Then, (f (n))n≥1 is said to converge in distribution to f, if E[h(f (n))] →
E[h(f)] as n → ∞ for every bounded and continuous function h : C → R. In that case we write
f (n) →d f.
In this paper, we deal with continuous and real-valued stochastic processes. More precisely, we
consider random elements defined on C(RI; S), with (S, d) Polish space. Our aim is to study in
C(RI; S) the convergence in distribution as the width n goes to infinity for:
i)	the sequence (f(l)(n))n≥ι for a fixed l ≥ 2 and i ≥ 1 with (S, d) = (R, ∣∙ |), i.e. the neural
network process for a single unit;
ii)	the sequence (F(I)(n))n≥ι for a fixed l ≥ 2 with (S, d) = (R∞, k ∙ ∣∣∞), i.e. the neural network
process for all units.
Since applying Definition 3 in a function space is not easy, we need, proved in SM F, the following
proposition.
Proposition 1 (convergence in distribution in C(RI; S), (S, d) Polish). Suppose that f and
(f (n))n≥1 are random elements in C(RI; S) with (S, d) Polish space. Then, f(n) →d f if: i)
f(n) →fd f and ii) the sequence (f (n))n≥1 is uniformly tight.
We denoted with →fd the convergence in law of the finite-dimensional distributions of a sequence of
stochastic processes. The notion of tightness formalizes the concept that the probability mass is not
allowed to “escape at infinity”: a single random element f in a topological space C is said to be tight
if for each > 0 there exists a compact T ⊂ C such that P[f ∈ C \ T] < . If a metric space (C, ρ)
is Polish any random element on the Borel σ-algebra of C is tight. A sequence of random elements
(f (n))n≥1 in a topological space C is said to be uniformly tight1 if for every > 0 there exists a
compact T ⊂ C such that P[f (n) ∈ C \ T] < for all n.
According to Proposition 1, to achieve convergence in distribution in function spaces we need the
following Steps A-D:
Step A) to establish the existence of the finite-dimensional weak-limit f on RI . We will rely on
Theorem 5.3 of Kallenberg (2002), known as Levy theorem.
Step B) to establish the existence of the stochastic processes f and (f (n))n≥1 as elements in SRI the
space of function from RI into S. We make use of Daniell-Kolmogorov criterion (Kallenberg, 2002,
Theorem 6.16): given a family of multivariate distributions {PI probability measure on Rdim(I) |
I ⊂ {x(1), . . . , x(k)}x(z)∈RI,k∈N} there exists a stochastic process with {PI} as finite-dimensional
distributions if {PI} satisfies the projective property: Pj(∙ X Rj\i) = PI(∙),I ⊂ J ⊂
{x(1), . . . , x(k)}x(z) ∈RI,k∈N. That is, it is required consistency with respect to the marginaliza-
tion over arbitrary components. In this step we also suppose, for a moment, that the stochastic
processes (f (n))n≥1 and f belong to C(RI; S) and we establish the existence of such stochastic
processes in C(RI; S) endowed with a σ-algebra and a probability measure that will be defined.
Step C) to show that the stochastic processes (f (n))n≥1 and f belong to C(RI; S) ⊂ SRI . With
regards to (f (n))n≥1 this is a direct consequence of (5)-(6) and the continuity of φ. With regards
to the limiting process f, with an additional Lipschitz assumption on φ, we rely on the following
Kolmogorov-Chentsov criterion (Kallenberg, 2002, Theorem 3.23):
1Kallenberg (2002) uses the same term “tightness” for both cases ofa single random element and of sequences
of random elements; we find that the introduction of “uniform tightness” brings more clarity.
4
Published as a conference paper at ICLR 2021
Proposition 2 (continuous version and Iocal-Holderianity, (S, d) complete). Let f be a process on
RI with values in a complete metric space (S, d), and assume that there exist a, b, H > 0 such that,
E[d(f(x), f(y))a] ≤ Hkx - yk(I+b),	x,y∈RI
Then f has a continuous version (i.e. f belongs to C(RI; S)), and the latter is a.s. locally Holder
continuous with exponent c for any c ∈ (0, b/a).
Step D) the uniform tightness of (f (n))n≥1 in C(RI; S). We rely on an extension of the Kolmogorov-
Chentsov criterion (Kallenberg, 2002, Corollary 16.9), which is stated in the following proposition.
Proposition 3 (uniform tightness in C(RI; S), (S, d) Polish). Suppose that (f (n))n≥1 are random
elements in C(RI; S) with (S, d) Polish space. Assume that f(0RI, n)n≥1 (i.e. f(n) evaluated at the
origin) is uniformly tight in S and that there exist a, b, H > 0 such that,
E[d(f (x, n), f(y, n))a] ≤ Hkx - yk(I+b),	x,y ∈ RI,n ∈ N
uniformly in n. Then (f (n))n≥1 is uniformly tight in C(RI; S).
4	Large-width functional limits
4.1	Limit on C(RI； S), WITH (S, d) = (R, ∣∙ |), FOR A fixed unit i ≥ 1 and layer l
Lemma 1 (finite-dimensional limit). If φ satisfies (4) then there exists a stochastic process fi(l) :
RI → R such that (fi(l) (n))n≥1 →fd fi(l) as n → ∞.
Proof. Fix l ≥ 2 and i ≥ 1. Fixed k inputs X = [x(1), . . . , x(k)], we show that as n → +∞
fi(l) (X, n) →d Nk(0, Σ(l)),	(7)
where Σ(l) denotes the k × k covariance matrix, which can be computed through the recursion:
Σ(1)i,j = σb2 + σω2hx(i),x(j)iRI, Σ(l)i,j = σb2 + σω2 R φ(fi)φ(fj)q(l-1)(df), where q(l-1) =
Nk(0, Σ(l - 1)). By means of (2), (3), (5) and (6),
f(I)(X)=Nk(O, ∑(i)),	∑(i)i,j=σ+σhx⑴,x⑶iri
fi(l)(X,n)|f1(,l.-..1,n) =d Nk (0, Σ(l, n)),	forl≥2,	(8)
[Σ(l, n)i,j = σ2 + σ2《。• F(T)(X, n)), (φ • FjlT)(X, n)))Rn
We prove (7) using Levy’s theorem, that is the point-wise convergence of the sequence of characteristic
functions of (8). We defer to SM A for the complete proof.	□
Lemma 1 proves Step A. This proof gives an alternative and self-contained proof of the main
result of Matthews et al. (2018b), under the more general assumption that the activation function
φ satisfies the polynomial envelop (4). Now we prove Step B, i.e. the existence of the stochastic
processes fi(l)(n) and fi(l) on the space RRI, for each layer l ≥ 1, unit i ≥ 1 and n ∈ N. in SM E.1
we show that the finite-dimensional distributions of fi(l) (n) satisfies Daniell-Kolmogorov criterion
(Kallenberg, 2002, Theorem 6.16), and hence the stochastic process fi(l)(n) exists. in SM E.2 we
prove a similar result for the finite-dimensional distributions of the limiting process fi(l). in SM E.3
we prove that, if these stochastic processes are continuous, they are naturally defined in C(RI; R).
in order to prove the continuity, i.e. Step C note that fi(1) (x) = PjI=1 ωi(,1j)xj + bi(1) is continuous
by construction, thus by induction on l, if fi(l-1) (n) are continuous for each i ≥ 1 and n, then
f(I)(X,n) = √n Pn=Iωi(,lj)φ(fj(l-1)(x, n)) + bi(l) is continuous being composition of continuous
functions. For the limiting process fi(l) we assume φ to be Lipschitz with Lipschitz constant Lφ . in
particular we have the following:
Lemma 2 (continuity). If φ is Lipschitz on R then fi(l) (1), fi(l) (2), . . . are P-a.s. Lipschitz on RI,
while the limiting process fi(l) is P-a.s. continuous on RI and locally γ-Holder continuous for each
0<γ<1.
5
Published as a conference paper at ICLR 2021
Proof. Here we present a sketch of the proof, and we defer to SM B.1 and SM B.2 for the complete
proof. For (fi(l)(n))n≥1 it is trivial to show that for each n
|fi(l)(x,n) - fi(l)(y, n)| ≤ Hi(l)(n)kx - ykRI, x,y ∈ RI,P - a.s.	(9)
where Hi(l) (n) denotes a suitable random variable, which is defined by the following recursion over l
(Hi(1)(n) = PjI=1 ωi(,1j)	(10)
1H(l)(n) = √n Pn=Jω(lj∣H尸)(n)	()
To establish the continuity of the limiting process fi(l) we rely on Proposition 2. Take two inputs
x,y ∈ RI. From ⑺ We get that [f(I)(X),f(l)(y)]〜N2(0, Σ(l)) where
Σ(1) = σ2	1	1	+σ2	kxk2RI	hx, yiRI
Σ(1) =σb	1	1	+σω	hx,yiRI	kyk2RI	,
Σ(l)= σ"l 1] + σ2 "状吸 φ(“φ(v)] q(l-1)(du, dv),
v b b [1 1_|	ω J [φ(u)φ(v)	∣φ(v)∣2
where q(lT) = N2(0, Σ(l - 1)), Defining aτ = [1, -1], from (7) we know that f(l)(y) - f(l)(x)〜
N(aT0, aT Σ(l)a).Thus
Ifi(I)(y) - fi(I)(X)∣2θ ~ I JaT∑(l)aN(0,1)∣2θ ~ (aT∑(l)a)θ|N(0,1)∣2θ.
We proceed by induction over the layers. For l = 1,
Eh∣fi(1)(y) - fi(1)(X)∣2θi = Cθ(aTΣ(1)a)θ
= Cθ(σω2 kyk2RI - 2σω2 hy, XiRI +σω2 kXk2RI)θ
=Cθ(σω2)θ(kyk2RI -2hy,XiRI+kXk2RI)θ
=Cθ(σω2)θky-Xk2RθI,
where Cθ = E[∣N (0, 1)∣2θ]. By hypothesis induction there exists a constant H (l-1) > 0 such that
R ∣u- v∣2θq(l-1)(du, dv) ≤ H (l-1) ky - Xk2RθI. Then,
Ifi(I)(y) - fi(I)(X) ∣2θ ~ ∣N(0,1)∣2θ(aTΣ(l)a)θ
= ∣N(0,1)∣2θσω2 Z [∣φ(u)∣2 - 2φ(u)φ(v) + ∣φ(v)∣2]q(l-1)(du, dv)θ
≤ ∣N(0,1)∣2θ(σω2L2φ)θZ ∣u - v∣2θq(l-1)(du, dv)
≤ ∣N(0,1)∣2θ(σω2L2φ)θH(l-1)ky-Xk2RθI.
where we used ∣φ(u)∣2 - 2φ(u)φ(v) + ∣φ(v)∣2 = ∣φ(u) - φ(v)∣2 ≤ L2φ∣u - v∣2 and the Jensen
inequality. Thus,
E ∣fi(l)(y) - fi(l)(X)∣2θ ≤H(l)ky-Xk2RθI,	(11)
where the constant H(l) can be explicitly derived by solving the following system
H(1) =Cθ(σω2)θ
H(l) = Cθ(σω2L2φ)θH(l-1).
(12)
It is easy to get H(l) = Cθl (σω2 )lθ(L2φ)(l-1)θ. Observe that H(l) does not depend on i (this will
be helpful in establishing the uniformly tightness of (fi(l)(n))n≥1 and the continuity of F(l)). By
Proposition 2, setting α = 2θ, and β = 2θ - I (since β needs to be positive, it is sufficient to
choose θ > I/2) we get that f(l has a continuous version and the latter is P-a,s locally Y-Holder
continuous for every 0 < γ < 1 -击,for each θ > I/2. Taking the limit as θ → +∞ we conclude
the proof.	口
6
Published as a conference paper at ICLR 2021
Lemma 3 (uniform tightness). If φ is Lipschitz on R then (fi(l) (n))n≥1 is uniformly tight in
C(RI; R).
Proof. We defer to SM B.3 for details. Fix i ≥ 1, l ≥ 1. We apply Proposition 3 to show the uniform
tightness of the sequence (fi(l) (n))n≥1 in C(RI; R). By Lemma 2 fi(l)(1), fi(l)(2), . . . are random
elements in C(RI; R). Since (R, | ∙ |) is Polish, every probability measure is tight, then f (0ri , n) is
tight in R for every n. Moreover, by Lemma 1 fi (0RI, n)n≥1 →d fi(l)(0RI ), therefore by (Dudley,
2002, Theorem 11.5.3), f (0RI , n)n≥1 is uniformly tight in R.
It remains to show that there exist two values α > 0 and β > 0, and a constant H (l) > 0 such that
E	∖fi(l)(y, n) -	fi(l)(x, n)∖α	≤H(l)ky-xkIR+Iβ,	x,y∈RI,n∈N
uniformly in n. Take two points x, y ∈ RI. From (8) we know that fi(l) (y, n)∖f1(,l.-..1,n)
〜
N(0, σ2(l, n)) and f(l)(x, n)∣f([R 〜N(0, σ2(l, n)) Withjoint distribution N2(0, Σ(l, n)), where
Σ(1)
σx2 (1)	Σ(1)x,y
Σ(1)x,y	σy2 (1)
Σ l	σx2 (l, n)	Σ(l, n)x,y
,	Σ(l) = Σ(l, n)x,y	σy2 (l, n)	,
with,
Defining aT
'σ2 ⑴=σ2+σω kxkRι,
σ2(1) = σ2 + σ2 IlykRi,
夕⑴/〃 =σ2 + σω hx, yiRi,
σ2Gn) = σ2 + σω Pn=ιlφ ◦ fj(~11(x,n)∖2,
σ2(l,n) = σ2 + σPjn=ι ∖φ◦ f(l-%,nX2,
、夕Gn)X,y = σ2 + σnω Pn=I φf(lT)(X,n))。(ClT)(y,n))
[1, -1] we have that fi(l)(y, n)∖f1(,l.-..1,n) - fi(l)(x, n)∖f1(,l.-..1,n) is distributed as
N(aT0, aT Σ(l, n)a), where aT Σ(l, n)a = σy2(l, n) - 2Σ(l, n)x,y + σx2 (l, n). Consider α = 2θ
with θ integer. Thus
If(I)(y,n)lf(lU - fi(l)(x,n)lf仁力2θ 〜Za∑(l,n)aN(0,1)∣2θ 〜(aTΣ(l,n)a)θ|N(0,1)∣2θ.
As in previous theorem, for l = 1 we get E ∖fi(1)(y, n) - fi(1)(x, n)∖2θ = Cθ(σω2 )θky - xk2RθI where
Cθ = E[∖N (0, 1)2θ∖]. Set H(1) = Cθ(σω2 )θ and by hypothesis induction suppose that for every j ≥ 1
Eh∖fj(l-1)(y,n)-fj(l-1)(x,n)∖2θi ≤ H(l-1)ky - xk2RθI.
By hypothesis φ is Lipschitz, then
Eh∖fi(l)(y, n) - fi(l)(x, n)∖2θIIIf1(,l.-..1,n) i =Cθ(aTΣ(l,n)a)θ
= Cθσy2(l, n) - 2Σ(l, n)x,y + σx2 (l, n)
2 n	2θ
=Cθ (才 X ∣φ ◦ fj - )(y,n) - φ ◦ fj - )(x,n)∣ )
n j=1
≤ Cθ (σωnLφ X⅛(lτ)(y,n)Ylτ)(x,n)∣2)θ
j=1
Cθ
≤ Cθ
W (X⅛(lτ)(y,n)Ylτ)(x,n)∣2)θ
j=1
^L^ XXMlT)(y,n)- f尸)(x,n)∣2θ.
n	j=1
7
Published as a conference paper at ICLR 2021
Using the induction hypothesis
Ehf(I)(y,n) -fj(x,n)∣2θi = EhEh严(y,n)--也小产|(二；[]]
≤ Cθ(σωLφ)θ- XXEf尸)(y,n) - fj(lτ)(x,n)∣2θi
n j=1
≤Cθ(σω2L2φ)θH(l-1)ky-xk2RθI.
We can get the constant H(l) by solving the same system as (12), obtaining H(l) =
Cθl (σω2 )lθ(L2φ)(l-1)θ which does not depend on n. By Proposition 3 setting α = 2θ and β = 2θ - I,
since β must be a positive constant, it is sufficient to take θ > I/2 and this concludes the proof.
Note that Lemma 3 provides the last Step D that allows us to prove the desired result which is
explained in the theorem that follows:
Theorem 1 (functional limit). If φ is Lipschitz on R then fi(l)(n) →d fi(l) on C(RI; R).
Proof. We apply Proposition 1 to (fi(l)(n))n≥1. By Lemma 2, we have that fi(l), (fi(l)(n))n≥1 belong
to C(RI; R). From Lemma 1 we have the convergence of the finite-dimensional distributions of
(fF) (n))n≥ι, and form Lemma 3 we have the uniform tightness of (f(I) (n))n≥ι.	□
4.2 Limit on C(RI； S), WITH (S, d) = (R∞, k∙ ∣∣r∞), FOR A fixed layer l
As in the previous section we prove Steps A-D for the sequence (F(l) (n))n≥1. Remark that
each stochastic process F(l), F(l) (1), F(l) (2), . . . defines on C(RI; R∞) a joint measure whose
i-th marginal is the measure induced respectively by fi(l), fi(l)(n), fi(2)(n), . . . (see SM E.1 -SM E.4).
Let F(l) =d Ni∞=1 fi(l), where N denotes the product measure.
Lemma 4 (finite-dimensional limit). If φ satisfies (4) then F(l)(n) →fd F(l) as n → ∞.
Proof. The proof follows by Lemma 1 and Cramer-Wold theorem for finite-dimensional projection of
F(l)(n): it is sufficient to establish the large n asymptotic of linear combinations of the fi(l)(X, n)’s
for i ∈ L ⊂ N. in particular, we show that for any choice of inputs elements X, as n → +∞
∞
F(l)(X,n) →d O Nk (0, Σ(l)),	(13)
i=1
where Σ(l) is defined in (7). The proof is reported in SM C.	□
Lemma 5 (continuity). If φ is Lipschitz on R then F(l), (F(l)(n))n≥1 belong to C(RI; R∞). More
precisely F(l) (1), F(l) (2), . . . are P-a.s. Lipschitz on RI, while the limiting process F(l) is P-a.s.
continuous on RI and locally Y-Holder Continuousfor each 0 < γ < 1.
Proof. it derives immediately from Lemma 2. We defer to SM D.1 and SM D.2 for details. The
continuity of the sequence process immediately follows from the Lipschitzianity of each component
in (9) while the continuity of the limiting process F(l) is proved by applying Proposition 2. Take two
inputs x, y ∈ RI and fix α ≥ 1 even integer. Since ξ(t) ≤ t for all t ≥ 0, and by Jensen inequality
d(F(I)(x),F(I)(y))∞ ≤ (XXq|fi(l)(X)-*(y)∣)α ≤ X∞ 2i∣fi(l)(χ) - fill(y)∖α
i=1	i=1
Thus, by applying monotone convergence theorem to the positive increasing sequence g(N) =
PN=I 击Ifi(I)(X) — fi(l)(y)∖α (which allows to exchange E and P∞=) we get
8
Published as a conference paper at ICLR 2021
∞N
E[d(F(I)(x),F(I)(y))∞] ≤ E[X 2|f(I)(X)- f(l)(y)∣α] = Nime[X 2严(x) - f(l)(y)∣α]
i=1 2	→∞	i=1 2
=X 22i Ehf(I) (X)- fill(y)∖a] = X 22i H(I)kx - ykRι = H(I)kx - ykRι
i=1 2	i=1 2
where we used (11) and the fact that H(l) does not depend on i (see (12)). Therefore, by Proposition 2,
for each α > I, setting β = α - I (since β needs to be positive, it is sufficient to choose α > I) F(l)
has a continuous version F(I)(θ) which is P-a.s locally Y-Holder continuous for every 0 <γ< 1 一 ɑ.
Letting α → ∞ we conclude.	□
Theorem 2 (functional limit). If φ is Lipschitz on R then (F(l) (n))n≥1 →d F(l) as n → ∞ on
C(RI; R∞).	一
Proof. This is Proposition 1 applied to (F(l)(n))n≥1. From Lemma 4 and Lemma 5 it remains to
show the uniform tightness of the sequence (F(l) (n))n≥1 in C(RI; R∞). Let > 0 and let (i)i≥1
be a positive sequence such that Pi∞=1 i = /2 . We have established the uniform tightness of
each component (Lemma 3). Therefore for each i ∈ N there exists a compact Ki ⊂ C(RI; R)
such that P[fi(l)(n) ∈ C(RI; R) \ Ki] < i for each n ∈ N (such compact depends on i). Set
K = ×i∞=1Ki which is compact by Tychonoff theorem. Note that this is a compact on the product
space ×i∞=1 C (RI; R) with associated product topology, and this is also a compact on C(RI; R∞)
(see SM E.4). Then PhF(I)(n) ∈ C(RI;R∞) \ K] = P[U∞=1{fi(I)(n) ∈ C(RI;R) \ Ki}] ≤
P∞=1 Phfi(I)(n) ∈ C(RI; R) \ Ki] ≤ P∞=1 ei < e which concludes the proof.	□
5 Discussion
We looked at deep Gaussian neural networks as stochastic processes, i.e. infinite-dimensional
random elements, on the input space RI, and we showed that: i) a network defines a stochastic
process on the input space RI; ii) under suitable assumptions on the activation function, a network
with re-scaled weights converges weakly to a Gaussian Process in the large-width limit. These
results extend previous works (Neal, 1995; Der & Lee, 2006; Lee et al., 2018; Matthews et al.,
2018a;b; Yang, 2019) that investigate the limiting distribution of neural network over a countable
number of distinct inputs. From the point of view of applications, the convergence in distribution
is the starting point for the convergence of expectations. Let consider a continuous function g :
C(RI; R∞) → R. By the continuous function mapping theorem (Billingsley, 1999, Theorem 2.7),
we have g(F(l) (n)) →d g(F(l)) as n → +∞, and under uniform integrability (Billingsley, 1999,
Section 3), we have (Billingsley, 1999, Theorem 3.5) E[g(F(l) (n))] → E[g(F(l))] as n → +∞. See
also Dudley (2002) and references therein.
As a by-product of our results we showed that, under a Lipschitz activation function, the limiting
Gaussian Process has almost surely locally γ-Holder continuous paths, for 0 < γ < 1. This raises the
question on whether it is possible to strengthen our results to cover the case γ = 1, or even the case
of local Lipschitzianity of the paths of the limiting process. In addition, if the activation function is
differentiable, does this property transfer to the limiting process? We leave these questions to future
research. Finally, while fully-connected deep neural networks represent an ideal starting point for
theoretical analysis, modern neural network architectures are composed of a much richer class of
layers which includes convolutional, residual, recurrent and attention components. The technical
arguments followed in this paper are amenable to extensions to more complex network architectures.
Providing a mathematical formulation of network’s architectures and convergence results in a way
that it allows for extensions to arbitrary architectures, instead of providing an ad-hoc proof for each
specific case, is a fundamental research problem. Greg Yang’s work on Tensor Programs (Yang,
2019) constitutes an important step in this direction.
9
Published as a conference paper at ICLR 2021
References
Charalambos D. Aliprantis and Kim Border. Infinite Dimensional Analysis: A Hitchhiker’s Guide.
Springer-Verlag Berlin and Heidelberg GmbH & Company KG, 2006.
Patrick Billingsley. Probability and Measure. John Wiley & Sons, 3rd edition, 1995.
Patrick Billingsley. Convergence of Probability Measures. Wiley-Interscience, 2nd edition, 1999.
JR Blum, H Chernoff, M Rosenblatt, and H Teicher. Central Limit Theorems for Interchangeable
Processes. Canadian Journal of Mathematics, 10:222-229, 1958.
Ricky Der and Daniel D Lee. Beyond Gaussian Processes: On the Distributions of Infinite Networks.
In Advances in Neural Information Processing Systems, pp. 275-282, 2006.
RM Dudley. Real Analysis and Probability. Cambridge University Press, 2002.
Olav Kallenberg. Foundations of Modern Probability. Springer Science & Business Media, 2nd
edition, 2002.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep Neural Networks as Gaussian Processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite Versus Infinite Neural Networks: an Empirical Study. volume 33,
2020.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on
Learning Representations, 2018a.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian Process Behaviour in Wide Deep Neural Networks, 2018b.
Radford M Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.
Aad Van Der Vaart and Harry Van Zanten. Information Rates of Nonparametric Gaussian Process
Methods. Journal of Machine Learning Research, 12(6), 2011.
Greg Yang. Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian
Processes. In Advances in Neural Information Processing Systems, volume 32, 2019.
10