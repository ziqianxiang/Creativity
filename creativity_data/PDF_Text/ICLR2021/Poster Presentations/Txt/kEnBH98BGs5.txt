Published as a conference paper at ICLR 2021
Estimating informativeness of samples with
Smooth Unique Information
Hrayr Harutyunyan1,2'Alessandro Achille1, Giovanni Paolini1, Orchid Majumder1,
Avinash Ravichandran1 , Rahul Bhotika1 , Stefano Soatto1
1 Amazon Web Services, 2 USC Information Sciences Institute
hrayrhar@usc.edu, {aachille, paoling, orchid}@amazon.com,
{ravinash, bhotikar, soattos}@amazon.com
Ab stract
We define a notion of information that an individual sample provides to the train-
ing of a neural network, and we specialize it to measure both how much a sam-
ple informs the final weights and how much it informs the function computed
by the weights. Though related, we show that these quantities have a qualita-
tively different behavior. We give efficient approximations of these quantities
using a linearized network and demonstrate empirically that the approximation
is accurate for real-world architectures, such as pre-trained ResNets. We apply
these measures to several problems, such as dataset summarization, analysis of
under-sampled classes, comparison of informativeness of different data sources,
and detection of adversarial and corrupted examples. Our work generalizes ex-
isting frameworks but enjoys better computational properties for heavily over-
parametrized models, which makes it possible to apply it to real-world networks.
1	Introduction
Training a deep neural network (DNN) entails extracting information from samples in a dataset and
storing it in the weights of the network, so that it may be used in future inference or prediction. But
how much information does a particular sample contribute to the trained model? The answer can
be used to provide strong generalization bounds (if no information is used, the network is not mem-
orizing the sample), privacy bounds (how much information the network can leak about a particular
sample), and enable better interpretation of the training process and its outcome. To determine the
information content of samples, we need to define and compute information. In the classical sense,
information is a property of random variables, which may be degenerate for the deterministic pro-
cess of computing the output of a trained DNN in response to a given input (inference). So, even
posing the problem presents some technical challenges. But beyond technicalities, how can we know
whether a given sample is memorized by the network and, if it is, whether it is used for inference?
We propose a notion of unique sample information that, while rooted in information theory, cap-
tures some aspects of stability theory and influence functions. Unlike most information-theoretic
measures, ours can be approximated efficiently for large networks, especially in the case of transfer
learning, which encompasses many real-world applications of deep learning. Our definition can be
applied to either “weight space” or “function space.” This allows us to study the non-trivial dif-
ference between information the weights possess (weight space) and the information the network
actually uses to make predictions on new samples (function space).
Our method yields a valid notion of information without relying on the randomness of the training
algorithm (e.g., stochastic gradient descent, SGD), and works even for deterministic training algo-
rithms. Our main work-horse is a first-order approximation of the network. This approximation is
accurate when the network is pre-trained (Mu et al., 2020) — as is common in practical applications
—oris randomly initialized but very wide (Lee et al., 2019), and can be used to obtain a closed-form
expression of the per-sample information. In addition, our method has better scaling with respect to
the number of parameters than most other information measures, which makes it applicable to mas-
*Work conducted at Amazon Web Services.
1
Published as a conference paper at ICLR 2021
sively over-parametrized models such as DNNs. Our information measure can be computed without
actually training the network, making it amenable to use in problems like dataset summarization.
We apply our method to remove a large portion of uninformative examples from a training set with
minimum impact on the accuracy of the resulting model (dataset summarization). We also apply our
method to detect mislabeled samples, which we show carry more unique information.
To summarize, our contributions are (1) We introduce a notion of unique information that a sample
contributes to the training of a DNN, both in weight space and in function space, and relate it with the
stability of the training algorithm; (2) We provide an efficient method to compute unique information
even for large networks using a linear approximation of the DNN, and without having to train a
network; (3) We show applications to dataset summarization and analysis. The implementation
of the proposed method and the code for reproducing the experiments is available at https://
github.com/awslabs/aws-cv-unique-information.
Prerequisites and Notation. Consider a dataset of n labeled examples S = {zi}in=i, where zi =
(xi, yi), xi ∈ X and yi ∈ Rk and a neural network model fw : X 7→ Rk with parameters w ∈
Rd. Throughout the paper S-i = {zi, . . . , zi-i, zi+i, . . . , zn } denotes the set excluding the i-
th sample; fwt is often shortened to ft ; the concatenation of all training examples is denoted by
X; the concatenation of all training labels by Y ∈ Rnk ; and the concatenation of all outputs by
fw(X) ∈ Rnk. The loss on the i-th example is denoted by Li(W) and is equal to 2∣∣fω(Xi) - yik2,
unless specified otherwise. This choice is useful when dealing with linearized models and is justified
by Hui & Belkin (2020), who showed that the mean-squared error (MSE) loss is as effective as cross-
entropy for classification tasks. The total loss is L(W) = P§=i Li(w) + 2 ∣∣w 一 wo∣∣, where λ ≥ 0
is a weight decay regularization coefficient and W0 is the weight initialization point. Note that the
regularization term differs from standard weight decay ∣W∣22 and is more appropriate for linearized
neural networks, as it allows us to derive the dynamics analytically (see Sec. F of the appendix).
Finally, a (possibly stochastic) training algorithm is denoted with a mapping A : S → W, which
maps a training dataset S to classifier weights W = A(S). Since the training algorithm can be
stochastic, W is a random variable. The distribution of possible weights W after training with the
algorithm A on the dataset S is denoted with pA(W | S). We use several information-theoretic
quantities, such as entropy: H(X) = -E log p(x) , mutual information: I(X; Y ) = H(X) +
H (Y) 一 H (X,Y), Kullback-Leibler divergence: KL(P(X)∣∣q(χ)) = Eχ~p(χ) [log(p(χ)∕q(χ))] and
their conditional variants (Cover & Thomas, 2006). If y ∈ Rm and X ∈ Rn, then the Jacobian 祭 is
an m X n matrix. The gradient Nxy denotes transpose of the Jacobian ∂y, an n X m matrix.
2	Related Work
Our work is related to information-theoretic stability notions (Bassily et al., 2016; Raginsky
et al., 2016; Feldman & Steinke, 2018) that seek to measure the influence of a sample on the
output, and to measure generalization. Raginsky et al. (2016) define information stability as
ES [n1 Pn=ι I(W； Zi | S-i)], the expected average amount of unique (Shannon) information that
weights have about an example. This, without the expectation over S, is also our starting point
(eq. 1). Bassily et al. (2016) define KL-stability supS,S0 KL(pA(w | S) k pA(w | S0)), where S and
S0 are datasets that differ by one example, while Feldman & Steinke (2018) define average leave-
one-out KL stability as SuPS § PZi KL(PA(W | S) k Pa(w | S-i)). The latter closely resembles
our definition (eq. 4). Unfortunately, while the weights are continuous, the optimization algorithm
(such as SGD) is usually discrete. This generally makes the resulting quantities degenerate (infinite).
Most works address this issue by replacing the discrete optimization algorithm with a continuous
one, such as stochastic gradient Langevin dynamics (Welling & Teh, 2011) or continuous stochastic
differential equations that approximate SGD (Li et al., 2017) in the limit. We aim to avoid such
assumptions and give a definition that is directly applicable to real networks trained with standard
algorithms. To do this, we apply a smoothing procedure to a standard discrete algorithm. The final
result can still be interpreted as a valid bound on Shannon mutual information, but for a slightly
modified optimization algorithm. Our definitions relate informativeness ofa sample to the notion of
algorithmic stability (Bousquet & Elisseeff, 2002; Hardt et al., 2015), where a training algorithm A
is called stable if A(S) is close to A(S0) when the datasets S and S0 differ by only one sample.
2
Published as a conference paper at ICLR 2021
To ensure our quantities are well-defined, we apply a smoothing technique which is reminiscent of a
soft discretization of weight space. In Section 4, we show that a canonical discretization is obtained
using the Fisher information matrix, which relates to classical results of Rissanen (1996) on optimal
coding length. It also relates to the use of a post-distribution in Achille et al. (2019), who however
use it to estimate the total amount of information in the weights of a network.
We use a first-order approximation (linearization) inspired by the Neural Tangent Kernel (NTK) (Ja-
cot et al., 2018; Lee et al., 2019) to efficiently estimate informativeness of a sample. While NTK
predicts that, in the limit of an infinitely wide network, the linearized model is an accurate approxi-
mation, we do not observe this on more realistic architectures and datasets. However, we show that,
when using pre-trained networks as common in practice, linearization yields an accurate approxi-
mation, similarly to what is observed by Mu et al. (2020). Shwartz-Ziv & Alemi (2020) study the
total information contained by an ensemble of randomly initialized linearized networks. They notice
that, while considering ensembles makes the mutual information finite, it still diverges to infinity as
training time goes to infinity. On the other hand, we consider the unique information about a single
example, without the neeed for ensembles, by considering smoothed information, which remains
bounded for any time. Other complementary works study how information about an input sample
propagates through the network (Shwartz-Ziv & Tishby, 2017; Achille & Soatto, 2018; Saxe et al.,
2019) or total amount of information (complexity) of a classification dataset (Lorena et al., 2019),
rather than how much information the sample itself contains.
In terms of applications, our work is related to works that estimate influence of an example (Koh
& Liang, 2017; Toneva et al., 2019; Katharopoulos & Fleuret, 2018; Ghorbani & Zou, 2019; Yoon
et al., 2019). This can be done by estimating the change in weights if a sample is removed from the
training set, which is addressed by several works (Koh & Liang, 2017; Golatkar et al., 2020; Wu
et al., 2020). Influence functions (Cook, 1977; Koh & Liang, 2017) model removal of a sample as
reducing its weight infinitesimally in the loss function, and show an efficient first-order approxima-
tion of its effect on other measures (such as test time predictions). We found influence functions to
be prohibitively slow for the networks and data regimes we consider. Basu et al. (2020) found that
influence functions are not accurate for large DNNs. Additionally, influence functions assume that
the training has converged, which is not usually the case in practice. We instead use linearization of
neural networks to estimate the effect of removing an example efficiently. We find that this approx-
imation is accurate in realistic settings, and that the computational cost scales better with network
size, making it applicable to very large neural networks.
Our work is orthogonal to that of feature selection: while we aim to evaluate the informativeness for
the final weights of a subset of training samples, feature selection aims to quantify the informative-
ness for the task variable ofa subset of features. However, they share some high-level similarities. In
particular, Kohavi et al. (1997) propose the notion of strongly-relevant feature as one that changes
the discriminative distribution when it is excluded. This notion is similar to the notion of unique
sample information in eq. (1).
3	Unique information of a sample in the weights
Consider a (possibly stochastic) training algorithm A that, given a training dataset S, returns the
weights W of a classifier fw . From an information-theoretic point of view, the amount of unique
information a sample zi = (xi, yi) provides about the weights is given by the conditional point-wise
mutual information:
I(W;Zi	=zi	|	S-i	=	S-i)	= KL(pA(w	| S=S)kr(w | S-i	=	S-i)),	(1)
where S denotes the random variable whose sample is the particular dataset S, and
r(W | S—i = S-i) = RpA(W | S = S-i, Zi)dP(Zi) = Ez0~p(z) [pA(W | S = S—i, Zi)] denotes the
marginal distribution of the weights over all possible sampling of Zi.1 Computing the distribution
r(W | S—i) is challenging because of the high-dimensionality and the cost of training algorithm A for
multiple samples. One can address this problem by using the following upper bound (Lemma B.1):
KL(pA(W | S) k r(W | S—i)) = KL(pA(W | S) k q(W | S—i)) - KL(r(W | S—i) k q(W | S—i))
≤ KL(PA(W | S) Ilq(W | ST)),	⑵
1Hereafter, to avoid notational clutter, we will shorten “S-i = S-i” to S-i and “Zi = zi” to just zi in all
conditionals and information-theoretic functionals.
3
Published as a conference paper at ICLR 2021
which is valid for any distribution q(w | S-i). Choosing q(w | S-i) = pA(w | S-i), the distribution
of the weights after training on S-i, gives a reasonable upper bound (see Sec. A.1 for details):
I(W;zi | S-i) ≤ KL(pA(w | S) kpA(w | S-i)).	(3)
We call SI(zi, A) , KL(pA(w | S) k pA(w | S-i)) the sample information of zi w.r.t. algorithm A.
Smoothed Sample Information. The formulation above is valid in theory but, in practice, even
SGD is used in a deterministic fashion by fixing the random seed and, in the end, we obtain just
one set of weights rather than a distribution of them. Under these circumstances, all the above
KL divergences are degenerate, as they evaluate to infinity. It is common to address the problem
by assuming that A is a continuous stochastic optimization algorithm, such as stochastic gradient
Langevin dynamics (SGLD) or a continuous approximation of SGD which adds Gaussian noise to
the gradients. However, this creates a disconnect with the practice, where such approaches do not
perform at the state-of-the-art. Our definitions below aim to overcome this disconnect.
Definition 3.1 (Smooth sample information). Let A be a possibly stochastic algorithm. Following
eq. (3), we define the smooth sample information with smoothing Σ:
SI∑(zi,A) = KL(pa∑(w | S) kpa∑(w | S-i)).	(4)
where we define smoothed weights A∑(S)，A(S) + ξ, with ξ 〜N(0, Σ).
Note that if the algorithm A is continuous, we can pick Σ → 0, which will make SIΣ (zi, A) →
SI(zi, A). The following proposition shows how to compute the value of SIΣ in practice.
Proposition 3.2. Let A be a deterministic training algorithm. Then, we have:
SI∑(zi,A) = 2(w - w-i)T∑-1(w - w-i),	(5)
where w = A(S) and w-i = A(S-i) are the weights obtained by training respectively with and
without the training sample zi. That is, the value of SIΣ (zi) depends on the distance between the
solutions obtained training with and without the sample zi, rescaled by Σ.
The smoothing of the weights by a matrix Σ can be seen as a form of soft-discretization. Rather than
simply using an isotropic discretization Σ = σ2I - since different filters have different norms and/or
importance for the final output of the network - it makes sense to discretize them differently. In
Sections 4 and 5 we show two canonical choices for Σ. One is the inverse of the Fisher information
matrix, which discounts weights not used for classification, and the other is the covariance of the
steady-state distribution of SGD, which respects the level of SGD noise and flatness of the loss.
4	Unique Information in the Predictions
SIΣ (zi, A) measures how much information an example zi provides to the weights. Alternatively,
instead of working in weight-space, we can approach the problem in function-space, and measure the
informativeness of a training example for the network outputs or activations. The unique information
that zi provides to the predictions on a test example x is:
I(zi;yb | x, S-i) = ES KL(q(yb | x, S) k r(yb | x, S-i)),
where X 〜p(χ) is a previously unseen test sample, y'〜q(∙ | x, S) is the network output on
input x after training on S,	and r(yb	| x, S-i)	=	q(yb	|	x, S-i, zi0)dP (zi0)	=	Ezi0 q(yb	| x, S-i,	zi0).
Following the reasoning in the previous section, we arrive at
I(zi;yb | x, S-i) ≤ KL(q(yb | x, S) k q(yb | x, S-i)).	(6)
Again, when training with a discrete algorithm and/or when the output of the network is determin-
istic, the above quantity may be infinite. Similar to smooth sample information, we define:
Definition 4.1 (Smooth functional sample information). Let A be a possibly stochastic training
algorithm and let yb be the prediction on a test example x after training on S. We define the smooth
functional sample information (F-SI) as:
F-SIσ(Zi,A)= E(χ,y)[KL(qσ(bσ | X,S) k 9σ (bσ | X,S-i))]|,	⑺
where bσ = b(x, S) + n, with n 〜N(0, σ2I) and qσ (bσ | x, S) being the distribution of yσ.
4
Published as a conference paper at ICLR 2021
We now describe a first-order approximation of the value of F-SIΣ for deterministic algorithms.
Proposition 4.2. Let A be a deterministic algorithm, w = A(S) and w-i = A(S-i) be the weights
obtained training respectively with and without sample zi. Then,
F-SIσ (zi,A) = 2σ2 Ex 〜p(x) kfw(X)- fw-i (x)k2	⑻
≈ 2112(W - w-i)TF(w)(w - w-i),	(9)
with F(W) = Ex [Vwfw(x)Vwfw(x)T] being the Fisher information matrix of qσ=ι(b | x, S).
By comparing eq. (5) and eq. (9), we see that the functional sample information is approximated
by using the inverse of the Fisher information matrix to smooth the weight space. However, this
smoothing is not isotropic as it depends on the point W.
5	Exact S olution for Linearized Networks
In this section, we derive a close-form expression for SIΣ and F-SIΣ using a linear approximation of
the network around the initial weights. We show that this approximation can be computed efficiently
and, as we validate empirically in Sec. 6, correlates well with the actual informativeness values. We
also show that the covariance matrix of SGD’s steady-state distribution is a canonical choice for the
smoothing matrix Σ of SIΣ.
Linearized Network. Linearized neural networks are a class of neural networks obtained by taking
the first-order Taylor expansion of a DNN around the initial weights (Lee et al., 2019):
fwlin(x) , fw0 (x) + Vwfw(x)T |w=w0(W - W0).
These networks are linear with respect to their parameters W, but can be highly non-linear with
respect to their input x. One of the advantages of linearized neural networks is that the dynamics
of continuous-time or discrete-time gradient descent can be written analytically if the loss function
is the mean squared error (MSE). In particular, for continuous-time gradient descent with constant
learning rate η > 0, we have (Lee et al., 2019):
Wt= Vw fo(X)Θ-1 (I - e-ηθ0t) (fo(X) - Y),	(10)
ftin(x) = fo(x) + Θo(x, X)Θ-1 (I - e-ηθ0t) (Y - fo(X)),	(11)
where Θ0 = Vwf0(X)TVwf0(X) ∈ Rnk×nk is the Neural Tangent Kernel (NTK) (Jacot et al.,
2018; Lee et al., 2019) and Θ0(x, X) = Vwf0(x)TVwf0(X). The expressions for networks trained
with weight decay is essentially the same (see Sec. F). To keep the notation simple, we will use
fw(x) to indicate fwlin(x) from now on.
Stochastic Gradient Descent. As mentioned in Sec. 3, a popular alternative approach to make
information quantities well-defined is to use continuous-time SGD, which is defined by (Li et al.,
2017; Mandt et al., 2017):
dWt
-ηVw LW (wt)dt + η yb Λ(wt)dn(t),
(12)
where η is the learning rate, b is the batch size, n(t) is a Brownian motion, and Λ(Wt) is the co-
variance matrix of the per-sample gradients (see Sec. C for details). Let ASGD be the algorithm that
returns a random sample from the steady-state distribution of (12), and let AERM be the deterministic
algorithm that returns the global minimum w* of the loss L(W) (for a regularized linearized network
L(W) is strictly convex). We now show that the non-smooth sample information SI(zi, ASGD) is
the same as the smooth sample information using SGD’s steady-state covariance as the smoothing
matrix and AERM as the training algorithm.
Proposition 5.1. Let the loss function be regularized MSE, W* be the global minimum of it, and
algorithms ASGD and AERM be defined as above. Assuming Λ(W) is approximately constant around
W* and SGD’s steady-state covariance remains constant after removing an example, we have
SI(zi,ASGD) = SI∑(%Aerm) = 1(w* - W-i)T∑-1(w* - W-i),	(13)
5
Published as a conference paper at ICLR 2021
where Σ is the solution of
H ∑ + ∑H T = η Λ(w*),
with H = (Vw fo(X )Vw fo(X )T + λI) being the Hessian of the loss function.
(14)
This proposition motivates the use of SGD’s steady-state covariance as a smoothing matrix. From
equations (13) and (14) we see that SGD’s steady-state covariance is proportional to the flatness of
the loss at the minimum, the learning rate, and to SGD’s noise, while inversely proportional to the
batch size. When H is positive definite, as in our case when using weight decay, the continuous
Lyapunov equation (14) has a unique solution, which can be found in O(d3) time using the Bartels-
Stewart algorithm (Bartels & Stewart, 1972). One particular case when the solution can be found
analytically is when Λ(w*) and H commute, in which case Σ = 金ΛH-1. For example, this is the
case for Langevin dynamics, for which Λ(w) = σ2I in equation (12). In this case, we have
SI(zi,AsGD) = SI∑(zi,AERM) = ɪ-(w* 一 W-i)TH(w* 一 W-i),	(15)
ησ2
which was already suggested by Cook (1977) as a way to measure the importance of a datum in
linear regression.
Functional Sample Information. The definition in Section 4 simplifies for linearized neural net-
works: The step from eq. (8) to eq. (9) becomes exact, and the Fisher information matrix becomes
independent of W and equal to F = Eχ~p(χ) [Vw fo(χ)Vwfo(χ)T]. This shows that functional sam-
ple information can be seen as weight sample information with discretization Σ equal to F-1. The
functional sample information depends on the training data distribution, which is usually unknown.
We can estimate it using a validation set:
nval
F-SIσ (Zi,A) ≈ 2σ^ X IIfw (Xja) - fw-i(χval)∣∣
(16)
(17)
2σ1 (W ― W-i)T(Hval ― λI)(w ― W-i).
Itis instructive to compare the sample weight information of (15) and functional sample information
of (17). Besides the constants, the former uses the Hessian of the training loss, while the latter uses
the Hessian of the validation loss (without the `2 regularization term). One advantage of the latter is
computational cost: As demonstrated in the next section, we can use equation (16) to compute the
prediction information, entirely in the function space, without any costly operation on weights. For
this reason, we focus on the linearized F-SI approximation in our experiments. Since σ-2 is just a
multiplicative factor in (17) we set σ = 1. We also focus on the case where the training algorithm
A is discrete gradient descent running for t epochs (equations 10 and 11).
Efficient Implementation. To compute the proposed sample information measures for linearized
neural networks, we need to compute the change in weights W ― W-i (or change in predictions
fw(x) ― fwi (x)) after excluding an example from the training set. This can be done without retrain-
ing using the analytical expressions of weight and prediction dynamics of linearized neural networks
eq. (10) and eq. (11), which also work when the algorithm has not yet converged (t < ∞). We now
describe a series of measures to make the problem tractable. First, to compute the NTK matrix we
would need to store the Jacobian Vf0(xi) of all training points and compute Vwf0(X)TVwf0(X).
This is prohibitively slow and memory consuming for large DNNs. Instead, similarly to Zancato
et al. (2020), we use low-dimensional random projections of per-example Jacobians to obtain prov-
ably good approximations of dot products (Achlioptas, 2003; Li et al., 2006). We found that just
taking 2000 random weights coordinates per layer provides a good enough approximation of the
NTK matrix. Importantly, we consider each layer separately, as different layers may have different
gradient magnitudes. With this method, computing the NTK matrix takes O(nkd + n2k2d0) time,
where d0 ≈ 104 is the number of sub-sampled weight indices (d0 d). We also need to recom-
pute Θ0-1 after removing an example from the training set. This can be done in quadratic time by
using rank-one updates of the inverse (see Sec. E). Finally, when t 6= ∞ we need to recompute
e-ηΘ0t after removing an example. This can be done in O(n2k2) time by downdating the eigen-
decomposition of Θ0 (Gu & Eisenstat, 1995). Overall, the complexity of computing W ― Wi for
all training examples is O(n2k2d0 + n(n2k2 + C)), C is the complexity of a single pass over the
6
Published as a conference paper at ICLR 2021
	Reg.		Method	MNIST MLP	MNIST CNN		Cats and Dogs	
				scratch	scratch	pretrained	pr. ResNet-18	pr. ResNet-50
weights	λ=	0	Linearization	0.987	0.193	0.870	0.895	0.968
			Infl. functions	0.935	0.319	0.736	0.675	0.897
	λ=	103	Linearization	0.977	-0.012	0.964	0.940	0.816
			Infl. functions	0.978	0.069	0.979	0.858	0.912
predictions	λ= λ=	0 3	Linearization	0.993	0.033	0.875	0.877	0.895
			Infl. functions Linearization	0.920 0.993	0.647 0.070	0.770 0.974	0.530 0.931	0.715 0.519
		10	Infl. functions	0.990	0.407	0.954	0.753	0.506
Table 1: Pearson correlations of weight change kw - w-i k22 and validation prediction change
kfw(Xval) - fw-i (Xval)k22 norms computed with influence functions and linearized neural networks
with their corresponding measures computed for standard neural networks with retraining.
Figure 1: Functional sample information of samples in the iCassava classification task with 1000
samples, where the network is a pretrained ResNet-18. A: histogram of sample informations, B: 10
least informative samples, C: 10 most informative samples.
training dataset. The complexity of computing functional sample information for m test samples is
O(C + nmk2d0 + n(mnk2 + n2k2)). This depends on the network size lightly, only through C.
6	Experiments
In this section, we test the validity of linearized network approximation in terms of estimating the
effects of removing an example and show several applications of the proposed information measures.
Additional results and details are provided in the supplementary Sec. A.
Accuracy of the linearized network approximation. We measure kw - w-ik22 and kfw (Xval) -
fw-i (Xval)k22 for each sample zi by training with and without that example. Then, instead of re-
training, we use the efficient linearized approximation in Sec. 6 to estimate the same quantities and
measure their correlation with the ground-truth values (Table 1). For comparison, we also estimate
these quantities using influence functions (Koh & Liang, 2017). We consider two classification
tasks: (a) a toy MNIST 4 vs 9 classification task and (b) Kaggle Dogs vs. Cats classification task
(Kaggle, 2013), both with 1000 examples. For MNIST we consider a fully connected network with
a single hidden layer of 1024 ReLU units (MLP) and a small 4-layer convolutional network (CNN),
either trained from scratch or pretrained on EMNIST letters (Cohen et al., 2017). For cats vs dogs
classification, we consider ResNet-18 and ResNet-50 networks (He et al., 2016) pretrained on Ima-
geNet. In both tasks, we train both with and without weight decay (`2 regularization). The results
in Table 1 shows that linearized approximation correlates well with ground-truth when the network
is wide enough (MLP) and/or pretraining is used (CNN with pretraining and pretrained ResNets).
This is expected, as wider networks can be approximated with linearized ones better (Lee et al.,
2019), and pretraining decreases the distance from initialization, making the Taylor approximation
more accurate. Adding regularization also keeps the solution close to initialization, and generally
increases the accuracy of the approximation. Furthermore, in most cases linearization gives better
results compared to influence functions, while also being around 30 times faster in our settings.
Which examples are informative? Fig. 1, and Fig. 4 of the supplementary, plot the top 10 least and
most important samples in iCassava plant disease classification (Mwebaze et al., 2019), the MNIST
4 vs 9, and Kaggle cats vs dogs classification tasks. Especially in the case of the last two, we see
that the least informative samples look typical and easy, while the most informative ones look more
7
Published as a conference paper at ICLR 2021
(a) Comparing data sources
(b) Detecting mislabeled examples
(c) Summarizing datasets
Figure 2: Applications of functional sample information. (a) Different sources of data for the same
task (digit classification) can have a vastly different amount of information. (b) As expected, samples
with the wrong labels carry more unique information. (c) Test accuracy as a function of the ratio of
removed training examples using different strategies.
challenging and atypical. In the case of iCassava, the informative samples are more zoomed on
features that are important for classification (e.g., the plant disease spots). We observe that most
samples have small unique information, possibly because they are easier or because the dataset may
have many similar-looking examples. While in the case of MNIST 4 vs 9 and Cats vs. Dogs, the
two classes have on average similar information scores, in Fig. 1a we see that in iCassava examples
from rare classes (such as ‘healthy’ and ‘cbb’) are on average more informative.
Which data source is more informative? In this experiment, we train for 10-way digit classifica-
tion where both the training and validation sets consist of 1000 samples, 500 from MNIST and 500
from SVHN. We compute functional sample information for a pretrained ResNet-18 network. The
results presented in Fig. 2a tell that SVHN examples are much more informative than MNIST exam-
ples. The same result holds if we change the network to be a pretrained DenseNet-121 (see Fig. 7a).
This is intuitive, as SVHN examples have more variety. One can go further and use our information
measures for estimating informativeness of examples of dataset A for training a classifier for task
B, similar to Torralba & Efros (2011).
Detecting mislabeled examples. We expect a mislabeled example to carry more unique informa-
tion, since the network needs to memorize unique features of that particular example to classify it.
To test this, we add 10% uniform label noise to MNIST 4 vs 9, Kaggle cats vs dogs, and iCassava
classification tasks (all with 1000 examples in total), while keeping the validation sets clean. Fig. 2b
plots the histogram of functional sample information for both correct and mislabeled examples in
the case of iCassava classification task with a pretrained ResNet-18, while Fig. 8a and 8b plot that
for MNIST 4 vs 9 and Kaggle cats vs dogs tasks, respectively. The results indicate that mislabeled
examples are much more informative on average. This suggests that our information measures can
be used to detect outliers or corrupted examples.
Data summarization. We subsample the training dataset by removing a fraction of the least infor-
mative examples and measure the test performance of the resulting model. We expect that removing
the least informative training samples should not affect the performance of the model. Note how-
ever that, since we are considering the unique information, removing one sample can increase the
informativeness of another. For this reason, we consider two strategies: In one we compute the
informativeness scores once, and remove a given percentage of the least informative samples. In
the other we remove 5% of the least informative samples, recompute the scores, and iterate until
we remove the desired number of samples. For comparison, we also consider removing the most
informative examples (“top” baseline) and randomly selected examples (“random” baseline). The
results on MNIST 4 vs 9 classification task with the one-hidden-layer network described earlier, are
shown in Fig. 2c. Indeed, removing the least informative training samples has little effect on the
test error, while removing the top examples has the most impact. Also, recomputing the information
scores after each removal steps (“bottom iterative”) greatly improves the performance when many
samples are removed, confirming that SI and F-SI are good practical measures of unique informa-
tion in a sample, but also that the total information in a large group is not simply the sum of the
unique information of its samples. Interestingly, removing more than 80% of the least informative
examples degrades the performance more than removing the same number of the most informative
examples. In the former case, we are left with a small number of most informative examples, some
8
Published as a conference paper at ICLR 2021
of which are outliers, while in the latter case we are left with the same number of least informative
examples, most of which are typical. Consequently, the performance is better in the latter case.
Detecting under-sampled sub-classes. Using CIFAR-10 images, we create a dataset of “Pets vs
Deer”: Pets has 4200 samples and deer 4000. The class “pets” consists of two unlabeled sub-classes,
cats (200) and dogs (4000). Since there are relatively few cat images, we expect each to carry more
unique information. This is confirmed when we compute F-SI for a pretrained ResNet-18 (see Fig. 9
of the supplementary), suggesting that the F-SI can help to detect when an unlabeled sub-class of a
larger class is under-sampled.
7	Discussion and Future Work
The smooth (functional) sample information depends not only on the example itself, but on the net-
work architecture, initialization, and the training procedure (i.e. the training algorithm). This has to
be the case, since an example can be informative with respect to one algorithm or architecture, but
not informative for another one. Similarly, some examples may be more informative at the beginning
of the training (e.g., simpler examples) rather than at the end. Nevertheless, we found out that F-SI
still captures something inherent in the example. In particular, as shown Sec. A.4 of the supplemen-
tary, F-SI scores computed with respect to different architectures are significantly correlated to each
other (e.g. around 45% correlation in case of ResNet-50 and DenseNet-121). Furthermore, F-SI
scores computed for different initializations are significantly correlated (e.g. around 36%). Simi-
larly, F-SI scores computed for different training lengths are strongly correlated (see Fig. 5b). This
suggests that F-SI computed with respect to one network can reveal useful information for another
one. Indeed, this is verified in Sec. A.4 of the supplementary, where we redo the data summarization
experiment, but with a slight change that F-SI scores are computed for one network, but another net-
work is trained to check the accuracy. As shown in the Fig. 7b the results look qualitatively similar
to those of the original experiment.
The proposed sample information measure only the unique information provided by an example.
For this reason, it is not surprising that typical examples are usually the least informative, while
atypical and rare ones are more informative. This holds not only for visual data (as shown above),
but also for textual data (see Sec. A.8 of the supplementary). While the typical examples are usually
less informative according to the proposed measures, they still provide information about the deci-
Sion functions, which is evident in the data summarization experiment - removing lots of typical
examples was worse than removing the same number of random examples. Generalizing sample
information to capture this kind of contributions is an interesting direction for future work. Simi-
lar to Data Shapley (Ghorbani & Zou, 2019)), one can look at the average unique information an
example provides when considering along with a random subset of data. One can also consider
common information, high-order information, synergistic information, and other notions of infor-
mation between samples. The relation among these quantities is complex in general, even for 3
variables (Williams & Beer, 2010) and is an open challenge.
8	Conclusion
There are many notions of information that are relevant to understanding the inner workings of neu-
ral networks. Recent efforts have focused on defining information in the weights or activations that
do not degenerate for deterministic training. We look at the information in the training data, which
ultimately affects both the weights and the activations. In particular, we focus on the most elemen-
tary case, which is the unique information contained ina sample, because it can be the foundation for
understanding more complex notions. However, our approach can be readily generalized to unique
information of a group of samples. Unlike most previously introduced information measures, ours
is tractable even for real datasets used to train standard network architectures, and does not require
restriction to limiting cases. In particular, we can approximate our quantities without requiring the
limit of small learning rate (continuous training time), or the limit of infinite network width.
Acknowledgments
We thank the anonymous reviewers whose comments/suggestions helped improve and clarify this
manuscript.
9
Published as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Alessandro Achille, Giovanni Paolini, and Stefano Soatto. Where is the information in a deep neural
network? arXiv preprint arXiv:1905.12213, 2019.
Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary
coins. Journal of computer and System Sciences, 66(4):671-687, 2003.
R. H. Bartels and G. W. Stewart. Solution of the matrix equation ax + xb = c [f4]. Commun. ACM,
15(9):820-826, September 1972. ISSN 0001-0782. doi: 10.1145/361573.361582.
Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman.
Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM
symposium on Theory of Computing, pp. 1046-1059, 2016.
Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile.
arXiv preprint arXiv:2006.14651, 2020.
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499-526, 2002.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
R Dennis Cook. Detection of inflUential observation in linear regression. Technometrics, 19(1):
15-18, 1977.
Thomas M Cover and Joy A Thomas. Elements of information theory. Wiley-Interscience, 2006.
Vitaly Feldman and Thomas Steinke. Calibrating noise to variance in adaptive data analysis. In
Conference On Learning Theory, pp. 535-544, 2018.
Amirata Ghorbani and James ZoU. Data shapley: EqUitable valUation of data for machine learn-
ing. volUme 97 of Proceedings of Machine Learning Research, pp. 2242-2251, Long Beach,
California, USA, 09-15 JUn 2019. PMLR.
Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting oUtside the box: ScrUbbing
deep networks of information accessible from inpUt-oUtpUt observations. Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), 2020.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
Ming GU and Stanley C Eisenstat. Downdating the singUlar valUe decomposition. SIAM Journal on
Matrix Analysis and Applications, 16(3):793-810, 1995.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Like HUi and Mikhail Belkin. EvalUation of neUral architectUres trained with sqUare loss vs cross-
entropy in classification tasks. arXiv preprint arXiv:2006.07322, 2020.
ArthUr Jacot, Franck Gabriel, and Clement Hongler. NeUral tangent kernel: Convergence and gen-
eralization in neUral networks. In S. Bengio, H. Wallach, H. Larochelle, K. GraUman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 8571-
8580. CUrran Associates, Inc., 2018.
10
Published as a conference paper at ICLR 2021
Kaggle. Dogs vs. Cats, 2013. URL https://www.kaggle.com/c/dogs-vs-cats/
overview.
Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. volume 80 of Proceedings ofMachine Learning Research, pp. 2525-2534,
Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. vol-
ume 70 of Proceedings of Machine Learning Research, pp. 1885-1894, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Ron Kohavi, George H John, et al. Wrappers for feature subset selection. Artificial intelligence, 97
(1-2):273-324, 1997.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8572-8583,
2019.
Ping Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In Proceedings
of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 287-296, 2006.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic
gradient algorithms. volume 70 of Proceedings of Machine Learning Research, pp. 2101-2110,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Ana C Lorena, Luls PF Garcia, JenS Lehmann, Marcilio CP Souto, and Tin Kam Ho. HoW complex
is your classification problem? a survey on measuring classification complexity. ACM Computing
Surveys (CSUR), 52(5):1-34, 2019.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning.
arXiv preprint arXiv:2004.05529, 2020.
Ernest Mwebaze, Timnit Gebru, Andrea Frome, Solomon Nsumba, and Jeremy Tusubira. icassava
2019fine-grained visual categorization challenge. arXiv preprint arXiv:1908.02900, 2019.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
2014. URL http://www.aclweb.org/anthology/D14-1162.
Maxim Raginsky, Alexander Rakhlin, Matthew Tsao, Yihong Wu, and Aolin Xu. Information-
theoretic analysis of stability and bias of learning algorithms. In 2016 IEEE Information Theory
Workshop (ITW), pp. 26-30. IEEE, 2016.
J. J. Rissanen. Fisher information and stochastic complexity. IEEE Transactions on Information
Theory, 42(1):40-47, Jan 1996. ISSN 0018-9448. doi: 10.1109/18.481776.
Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
Ravid Shwartz-Ziv and Alexander A Alemi. Information in infinite ensembles of infinitely-wide
neural networks. volume 118 of Proceedings of The 2nd Symposium on Advances in Approximate
Bayesian Inference, pp. 1-17. PMLR, 08 Dec 2020.
11
Published as a conference paper at ICLR 2021
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network
learning. In International Conference on Learning Representations, 2019.
Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011 ,pp.1521-1528.
IEEE, 2011.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, ICML’11, pp. 681-688, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.
Paul L. Williams and Randall D. Beer. Nonnegative decomposition of multivariate information.
CoRR, abs/1004.2515, 2010.
Yinjun Wu, Edgar Dobriban, and Susan B Davidson. Deltagrad: Rapid retraining of machine learn-
ing models. arXiv preprint arXiv:2006.14755, 2020.
Jinsung Yoon, Sercan O Arik, and Tomas Pfister. Data valuation using reinforcement learning. arXiv
preprint arXiv:1909.11671, 2019.
Luca Zancato, Alessandro Achille, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Pre-
dicting training time without training. Advances in Neural Information Processing Systems 33,
2020.
A	Additional results and details
In this section we present additional results and details that were not included in the main paper due
to the space constraint.
A. 1 Approximating unique information with leave-one-out KL divergence
In the main text we discussed that I(W ; zi | S-i) can be upper bounded with KL(pA(w |
S) k pA (w | S-i)). In this subsection we evaluate these quantities on a toy 2D dataset. The dataset
has two classes, each with 40 examples, generated from a Gaussian distribution (see Fig. 3a). We
consider training a linear regression on this dataset using stochastic gradient descent for 200 epochs,
with batch size equal to 5 and 0.1 learning rate. Fig. 3b plots the distribution pA (w | S), Fig. 3c the
distribution pA(w | S-i), while Fig. 3d plots the distribution Ez0 pA(w | S-i, zi0). On this example
we get I(W; zi | S-i) ≈ 1.3 and KL(pA(w | S) k pA(w | S-i)) ≈ 3.0.
-0.46
-0.47
-0.48
-0.49
-0.50
-0.51
0.32	0.34	0.36
(a) Data	(b) pA (w | S)	(c) pA (w | S-i)	(d) Ezi0 pA (w | S-i, zi0)
Figure 3: A toy dataset and key distributions involved in upper bounding the unique sample infor-
mation with leave-one-out KL divergence.
12
Published as a conference paper at ICLR 2021
A.2 Which examples are informative?
In this subsection we present the most and least informative examples for two more classification
tasks: MNIST 4 vs 9 classification with MLP and Kaggle cat vs dog classification with a pretrained
ResNet-18 (Fig. 4). The results indicate that most informative examples are often the challenging
and atypical ones, while the least informative ones are easy and typical ones. For the MLP network
on MNIST 4 vs 9 we set t = 2000 and η = 0.001; for the ResNet-18 on cats vs dog classification
t = 1000 and η = 0.001; and for the ResNet-18 on iCassava dataset t = 5000 and η = 0.0003.
Figure 4: Functional sample information of samples in MNIST 4 vs 9 classification task (top) and
Dogs vs. Cats (bottom), with A: histogram of sample informations, B: 10 least informative samples,
C: 10 most informative samples.
A.3 Accuracy of the linearized network approximation
In this subsection we present additional details of experiments presented in Table 1. In all datasets
used the validation set also has 1000 samples. The fully connected network, MLP, consists of a
single layer of 1024 ReLU units. The architecture of the CNN used for the experiments in Table 1
is as follow.
Layer	Layer parameters
Conv. 1	32 filters, 4x4 kernel, stride = 2, padding = 1, ReLU activation
Conv. 2	32 filters, 4x4 kernel, stride = 2, padding = 1, ReLU activation
Conv. 3	64 filters, 3x3 kernel, stride = 1, padding = 0, ReLU activation
Conv. 4	256 filters, 3x3 kernel, stride = 1, padding = 0, ReLU activation
Fully connected	256 ReLU neurons
Fully connected	1 linear unit
In all our experiments, when using pretrained ResNets, we disable the exponential averaging of
batch statistics in batch norm layers. The exact details of running influence functions and linearized
neural network predictions are presented in Table 2.
A.4 How much does sample information depend on algorithm?
The proposed information measures depend on the training algorithm, which includes the archi-
tecture, seed, initialization, and training length. This is unavoidable as one example can be more
informative for one algorithm and less informative for another. Nevertheless, in this subsection, we
test how much does informativeness depend on the network, initialization, and training time. We
consider the Kaggle cats vs dogs classification task with 1000 training examples. First, fixing the
training time t = 1000, we consider four pretrained architectures: ResNet-18, ResNet-34, ResNet-
50, and DenseNet-121. The correlations between F-SI scores computed for the four architectures
13
Published as a conference paper at ICLR 2021
Experiment	Method	Details	
MNIST MLP (scratch)	Brute force Infl. functions Linearization	2000 epochs, learning rate = 0.001, batch size LiSSA algorithm, 1000 recursion steps, scale = t = 2000, learning rate = 0.001	= 1000 1000
MNIST CNN (scratch)	Brute force Infl. functions Linearization	1000 epochs, learning rate = 0.01, batch size = LiSSA algorithm, 1000 recursion steps, scale = t = 1000, learning rate = 0.01	1000 1000
MNIST CNN (pretrained)	Brute force Infl. functions Linearization	1000 epochs, learning rate = 0.002, batch size LiSSA algorithm, 1000 recursion steps, scale = t = 1000, learning rate = 0.002	= 1000 1000
Cats and dogs	Brute force Infl. functions Linearization	500 epochs, learning rate = 0.001, batch size = 500 LiSSA algorithm, 50 recursion steps, scale = 1000 t = 1000, learning rate = 0.001	
Table 2: Details of experiments presented in Table 1. For influence functions, we add a dumping
term with magnitude 0.01 whenever `2 regularization is not used (i.e. λ = 0).
ReSNet-18
ResNet-34
ResNet-50
DenSeNet-121
1.000	0.234	0.373	0.319
0.234	1.000	0.333	0.287
0.373	0.333	1.000	0.452
0.319	0.287	0.452	1.000
(a) Varying architecture
t =250
t =500
t = 1000
t =2000
t =4000
t = ∞
1.000
0.975
0.919
0.856
0.810
0.975
0.919
0.856
0.810
0.788
1.000
0.981
0.937
0.898
0.981
0.937
0.898
0.876
1.000
0.986
0.960
0.943
0.986
0.960
1.000
0.992
0.982
0.992
1.000
0.998
0.788
0.876
0.943
0.982
0.998
1.000
(b) Varying training length
Figure 5: Correlations between functional sample information scores computed for different archi-
tectures and training lengths. On the left: correlations between F-SI scores of the 4 pretrained
networks, all computed with setting t = 1000 and η = 0.001. On the right: correlations between
F-SI scores computed for pretrained ResNet-18s, with learning rate η = 0.001, but varying training
lengths t. All reported correlations are averages over 10 different runs. The training dataset consists
of 1000 examples from the Kaggle cats vs dogs classification task.
are presented in Fig. 5a. We see that F-SI scores computed for two completely different architec-
tures, such as ResNet-50 and DenseNet-121 have significant correlation, around 45%. Furthermore,
there is a significant overlap in top 10 most informative examples for these networks (see Fig. 6).
Next, fixing the network to be a pretrained ResNet-18 and fixing the training length t = 1000, we
consider changing initialization of the classification head (which is not pretrained). In this case the
correlation between F-SI scores is 0.364 ± 0.066. Finally, fixing the network to be a ResNet-18
and fixing the initialization of the classification head, we consider changing the number of iterations
in the training. We find strong correlations between F-SI scores of different training lengths (see
Fig. 5b).
MNIST vs SVHN experiment for DenseNet-121. In this paragraph we redo the MNIST vs
SVHN experiment (Fig 2a) but for a different network, a pretrained DenseNet-121, to test the de-
pendence of the results on the architecture choice. The results are presented in Fig. 7a and are
qualitatively identical to the results derived with a pretrained ResNet18 (Fig. 2a).
Data summarization with a change of architecture. To test how much sample information
scores computed for one network are useful for another network, we reconsider the MNIST 4 vs
9 data summarization experiment (Fig. 2c). This time we compute F-SI scores for the original net-
14
Published as a conference paper at ICLR 2021
(a) ResNet-18
Cat	Cat	Cat
Cat
Cat	Cat
(c) DenseNet-121
Figure 6: Top 10 most informative examples from Kaggle cats vs dogs classification task for three
pretrained networks: ResNet-18, ResNet-50, and DenseNet-121.
(a) MNVT vs SVHN experiment for DenseNet-121
Figure 7: Testing how much F-SI scores computed for different networks are qualitatively differ-
ent. On the left: the MNIST vs SVHN experiment with a pretrained DesneNet-121 instead of a
pretrained ResNet-18. On the right: Data summarization for the MNIST 4 vs 9 classification task,
where the F-SI scores are computed for a one-hidden-layer network, but a two-hidden-layer network
is trained to produce the test accuracies.
Ratio of removed examples
(b) MNIST 4 vs 9 data summarization
work with one hidden layer, but train a two-hidden-layer neural network (both layers having 1024
ReLU units). The data summarization results presented in Fig. 7b are qualitatively and quantitively
almost identical to the original results presented in the main text. This confirms that F-Si scores
computed for one network can be useful for another network.
A.5 Detecting incorrect examples
In this subsection we present results on detecting mislabeled examples for two more classification
tasks: MNIST 4 vs 9 classification with an MLP (Fig. 8a) and Kaggle cat vs dog classification with
a pretrained ResNet-18 (Fig. 8b). The results are qualitatively the same and assert that, indeed,
mislabeled examples are more informative, on average. For the MLP network we set t = 2000 and
η = 0.001; for the ResNet-18 on the cats vs dog classification task t = 1000 and η = 0.001; and for
the ResNet-18 on the iCassava dataset (in the main text) we set t = 10000 and η = 0.0001.
A.6 Detecting under-sampled sub-classes
Using CIFAR-10 images, we create a dataset of “Pets vs Deer”: Pets has 4200 samples and deer
4000. The class pets consists of two unlabeled sub-classes, cats (200) and dogs (4000). Since there
are relatively few cat images, we expect each to carry more unique information. Indeed, Fig. 9
15
Published as a conference paper at ICLR 2021
(a) MNIST 4 vs 9	(b) Kaggle Dogs vs Cats
Figure 8: Additional results comparing the information content of samples with correct and incorrect
labels on MNIST 4 vs 9 and Dogs vs Cats.
shows that this is the case, suggesting that the F-SI can help detecting when an unlabeled sub-class
of a larger class is under-sampled. In this experiment we set t = 10000 and η = 0.0001.
Informativeness of an example 1e-6
Figure 9: Histogram of the functional sample information of samples from the three subclasses of
the Pets vs Deer. Since the sub-class “cat” is under-represented in the dataset, cat images tend to
have on average more unique information than dog images, even if they belong to the same class.
A.7 Detecting adversarial examples
Szegedy et al. (2014) shows that imperceptible perturbation to an image can fool a neural network
into predicting the wrong label (adversarial examples). Goodfellow et al. (2015) further shows that
adding adversarial examples to the training dataset improves adversarial robustness and general-
ization (adversarial training), which suggests that adversarial examples may be informative for the
training process. To test how informative adversarial examples are with respect to normal ones, we
consider the Kaggle Cats vs Dogs classification task, consisting of 1000 examples. On this task we
fine-tune a pretrained ResNet-18 and for 10% of examples create successful adversarial examples
using the FGSM method of (Goodfellow et al., 2015) with = 0.01. Then, for these 10% of ex-
amples, we replace the original images with the corresponding adversarial ones. With the resulting
dataset, we consider a new pretrained ResNet-18 and compute F-SI for all training examples, setting
t = 1000 and η = 0.001. The results reported in Fig. 10 confirm that adversarial examples are on
average more informative than normal ones. Furthermore, the results indicate that one can use F-SI
to successfully detect adversarial examples.
A.8 Informative examples in sentiment analysis
So far we focused on image classification tasks. To test whether the proposed method works for
other modalities, we consider a sentiment analysis task, consisting of 2000 samples from the IMDB
movie reviews dataset (Maas et al., 2011). We convert words to 300-dimensional vectors using
the pretrained GloVe 6B word vectors (Pennington et al., 2014). Then each review is mapped to a
vector by averaging all of its word vectors. On the resulting vector we apply a fully connected neural
16
Published as a conference paper at ICLR 2021
Figure 10: Histogram of the functional sample information of samples of the Kaggle cats vs dogs
classification task, where 10% of examples are adversarial examples.
network with one hidden layer, consisting of 2000 ReLU units. We then compute F-SI scores for all
training examples, setting t = 10000 and η = 0.003. Likewise to the case of images, we find that
least informative examples are typical and easy (often containing many sentiment-specific words),
while the most informative examples are more nuanced and hard. Additionally, we find that there
is -16% correlation between the informativeness and length of review, possibly because averaging
many word vectors makes long reviews similar to each other.
A.9 Remaining details
In the experiment of measuring which data source is more informative (Fig. 2a) we set t = 20000
and η = 0.0001. In the experiment of data summarization (Fig. 2c) we set t = 2000 and η = 0.001.
B Proofs
The proof of eq. (2) follows from the following more general lemma, applied to the conditional
distribution pA(w|S-i, zi) and its marginal r(x|S-i).
Lemma B.1. Let p(x|y) be a conditional probability density function, and let p(x) =
p(x|y)dP (y) denote its marginal distribution. Then, for any distribution q(x) we have
KL(p(x|y) k p(x)) = KL(p(x|y) k q(x)) - KL(p(x) k q(x))
≤ KL(p(x|y) k q(x)),
where KL(P(XIy) ∣∣ q(x))，R R log Pqxxy)p(x∣y)dxdP(y) denotes the Conditional version of the
KL divergence (Cover & Thomas, 2006, Section 2.5).
Proof. The last inequality follows from the fact that the KL-divergence is always non-negative. We
now prove the first equality:
KL(p(x|y) ∣ q(x)) - KL(p(x) ∣ q(x))
=ZZ log p(XXy) p(χ∣y)dχdP (y) — Z log P(X) p(χ)dχ
(a)
(b)
J, / log Pqxy)p(x∣y)dxdP(y) - / log Px) ( /p(x∣y)dP(y))dx
/ l log P(XIy)p(x∖y)dxdP(y) - [ l log Px)p(x∖y)dxdP(y)
q(X)	q(X)
Z I hlog Pqxxy) - log p(x∣y)dxdP(y)
Z [ log Pxy)P(x∣y)dxdP(y)
P(x)
KL(P(xIy) ∣ P(x)),
17
Published as a conference paper at ICLR 2021
where in (a) we use that by definition p(x) = p(x|y)dP (y) and in (b) we exchange the order of
integration.	□
Propositions 3.2 and 4.2 are trivial, since they just assert that KL divergence between two Gaus-
sian distributions with equal covariance matrices Σ and with means μι and μ2 is equal to
1 (μι - μ2)τ∑-1(μ1 - μ2). We present the proof of Proposition 5.1 here.
Proposition B.2 (Prop. 5.1 restated). Let the loss function be regularized MSE, w* be the global
minimum of it, and algorithms ASGD and AERM be defined as in the main text. Assuming Λ(w)
is approximately constant around w* and SGD’s steady-state covariance remains constant after
removing an example, we have
SI(Zi,ASGD) = SI∑(zi,AERM) = 1(w* - W-i)T∑-1(w* - W-i),	(18)
where Σ is the solution of
H ∑ + ∑H T = η Λ(w*),	(19)
with H = (Vw fo(X )Vw fo(X )T + λI) being the Hessian of the loss function.
Proof. Assuming Λ(W) is approximately constant around W*, the steady-state distributions of (12)
is a Gaussian distribution with mean W* and covariance Σ such that:
H ∑ + ∑H T = η Λ(w*),	(20)
b
where H = (Vwf0(X)Vwf0(X)T + λI) is the Hessian of the loss function (Mandt et al., 2017).
This can be verified by checking that the distribution N(∙; w*, Σ) satisfies the Fokker-Planck equa-
tion (see Sec. D). Having pASGD (W | S) = N(W; W*, Σ) and pASGD (W | S-i) = N(W; W-* i, Σ-i),
we have that
SI(%Asgd) = 2 ((W* - W-i)T∑-1(w* - W-i) + tr(Σ-1∑) + log ∣∑-i∑-1∣ - d) .	(21)
By the assumption that SGD steady-state covariance stays constant after removing an example, i.e.
Σ-i = Σ, equation (21) simplifies to:
KL(PASGD (W 1 S) k PASGD (W 1 S-i)) = I(W* - W-i)T ςT(W* - W-i).	(22)
By the definition of the AERM algorithm and smooth sample information, this is exactly equal to
SI∑ (zi, Aerm).	□
C SGD noise covariance
Assume We have n examples and the batch size is b. Let gi，VwLi(W), g，§ PZi gi, and
g，b Pb=I gki, where k are sampled independently from {1,...,n} uniformly at. Then
Cov[<7,g] = E ] (b X gki- g) (b X gki- g
1b
=庐 E E [(gki - g)(gki - g)τ]
b i=1
T
T
18
Published as a conference paper at ICLR 2021
We denote the Per-SamPle covariance, b ∙ Cov ∖g, g] with Λ(w):
Λ(w) = ： (X gigT) - ggT = 1GGT- ggT,
where G ∈ Rd×n has gi ’s as its columns. We can see that whenever the number of samPles times
number of outPuts is less than number of Parameters (nk < d), then Λ(w) will be rank deficient.
Also, note that if we add weight decay to the total loss then covariance Λ(w) will not change, as all
gradients will be shifted by the same vector.
D Steady-state covariance of SGD
In this section we verify that the normal distributionN(∙; w*, Σ), with w* being the global minimum
of the regularization MSE loss and covariance matrix Σ satisfying the continuous LyaPunov equation
ΣH + HΣ = bΛ(w*), is the steady-state distribution of the stochastic differential equation of eq.
(12). We assume that (a) Λ(w) is constant in a small neighborhood of w* and (b) the steady-state
distribution is unique. We start with the Fokker-Planck equation:
∂p(w, t)	n ∂	η2 n n ∂2
~^∂Γ~ = X ∂w [ML(W)P(W，t)] + 2b XX ∂wi∂wj Mw)ijp(w,t)].
If P(W) = N(w; w*, Σ) = 1 exp {-2(w - w*)TΣ-1(w - w*)} is the steady-state distribution,
then the Fokker-Planck becomes:
d ∂	η2 d d	∂2
0 = X 两 [ML(W)P(W)] + 2 XX ∂wi∂wj [A(W)ijp(W)].	(23)
In the case of MSE loss:
n
▽wL(w) = X Vfo(xk)(f(xk)-期k) + λw = Vfo(X)(f (X) - Y) + λw,
k=1
n
V2wL(w) =XVf0(xk)+λIdVf0(xk)T = Vf0(X)Vf0(X)T + λI.
k=1
Additionally, for P(w ) the following two statements hold:
∂
Tj-P(W) = -P(W)2 1(w — w ),
∂wi
∂2
∂wi ∂wj
P(w) = -P(w)Σi-,j1 + P(w)Σj-1(w - w*)Σi-1(w - w*),
where Σi-1 is the i-th row of Σ-1. Let’s comPute the first term of (23):
dd
X^— [VWiL(W)P(W)] = X [p(W)(VfO(X)ivfo(X)T + λwi) - VwiL(W) ∙ P(W)ς-1(w - w*)]
i=1 ∂wi	i=1
d
=P(w)tr (Vfo(X )Vfo(X )t + λI) - p(w) X (Vfo(X )i(f (X) - Y)+ λwi) Σ-1(w - w*)
i=1
=P(w)tr(H) - P(W) ((f(X) - Y)tVfo(X)t + λwτ) Σ-1(w - w*).	(24)
As w* is a critical point of L(w), We have that Vfo(X)(fw* (X) - Y) + λw* = 0. Therefore, We
can subtractp(w) ((fw* (X) - Y)tVfo(X)t + λ(w*)τ) Σ-1(w - w*) from (24):
d∂
E 丁 [VWiL(W)P(W)] =
i=1 ∂wi
=P(w)tr(H) -P(w) ((f (X) - fw* (X))tVfo(X)t + λ(w - w*)τ) Σ-1(w - w*)
=P(w)tr(H) -p(w)(w - w*)τ (VfO(X)Vfo(X)t + λI) Σ-1(w - w*)
= P(w)tr(H) - P(w)(w - w*)THΣ-1(w - w*).	(25)
19
Published as a conference paper at ICLR 2021
ISOTROPIC CASE: Λ(w) = σ2Id
In the case when Λ(w) = σ2Id, we have
∂2
Λ~Λ- [A(W)i,jP(W)] = σ2tr(VwP(W)) = -σ2p(W)tr(£ I) + σ2p(W)∣∣ς I(W - W )k2.
∂wi ∂wj
i,j
Putting everything together in the Fokker-Planck we get:
η (P(W)tr(H) — P(W)(W — W*)THΣ-1(w — w*)))
2
+ 2b (—σ2p(w)tr(Σ-1) + σ2p(w)k∑-1(w — w*)k2) = 0.
It is easy to verify that Σ-1 = nbH is a valid inverse covariance matrix and satisfies the equation
above. Hence, it is the unique steady-state distribution of the stochastic differential equation. The
result confirms that variance is high when batch size is low or learning rate is large. Additionally,
the variance is low along directions of low curvature.
Non-isotropic case
We assume Λ(W) is constant around W* and is equal toΛ. This assumption is acceptable to a degree,
because SGD converges to a relatively small neighborhood, in which we can assume Λ(W) to not
change much. With this assumption,
X	d2
∂Wi ∂Wj
i,j
[ʌi,jP(w)] = £Ai,j I-P(W)ς-1 + P(W)ς-I(W — w*)ς-I(W — w*)]
i,j
-	P(W)tr(Σ-1A) + p(w) ^Xʌi,j(Σ-1(w — w*)(w — w*)tΣ-1)i,j
i,j
-	P(W)tr(Σ-1A) + P(w)tr(Σ-1 (w — w*)(w — w*)t Σ-1A)
-	P(W)tr(Σ-1A) + p(w)(w — w*)t Σ-1AΣ-1(w — w*)).
(26)
It is easy to verify that if ΣH + HΣ = bʌ, then terms in equations (25) and (26) will be negatives
of each other UP to a constant 卷,implying that P(W) satisfies the Fokker-Planck equation. Note that
Σ-1 = 2ηbHʌ-1 also satisfies the Fokker-Planck, but will not be positive definite unless H and ʌ
commute.
E Fast update of NTK inverse after data removal
For computing weights or predictions of a linearized network at some time t, we need to compute
the inverse of the NTK matrix. To compute the informativeness scores, we need to do this inversion
n time, each time with one data point excluded. In this section, we describe how to update the
inverse of NTK matrix after removing one example in O(n2k3) time, instead of doing the straight-
forward O(n3k3) computation. Without loss of generality let’s assume we remove last r rows and
corresponding columns from the NTK matrix. We can represent the NTK matrix as a block matrix:
A11	A12
A21	A22
The goal is to compute A1-11 from Θ0-1. We start with the block matrix inverse formula:
Θ-1	A11	A12-1	F1-11	—F1-11A12A2-21
Θ0 = A21	A22	= —A2-21A21F1-11	F2-21
where
F11 = A11 — A12 A2-21 A21 ,
F22 = A22 — A21 A1-11 A12.
(27)
(28)
(29)
20
Published as a conference paper at ICLR 2021
From (28) we have A11 = F11 + A12A2-21A21. Applying the Woodbury matrix identity on this we
get:
A1-11 = F1-11 -F1-11A12(A22 + A21F1-11A12)-1A21F1-11.	(30)
This Eq. (30) gives the recipe for computing A1-11. Note that F1-11 can be read from Θ0-1 using (27),
A12, A21, and A22 can be read from Θ. Finally, the complexity of computing A1-11 using (30) is
O(n2k3) if we remove one example.
F	Adding weight decay to linearized neural network training
Let us consider the loss function L(W) = Pn=ι Li(w) + 2∣∣w - wok2. In this case continuous-time
gradient descent is described by the following ODE:
w(t) = -ηVw fo(xMft(X) - Y) - ηλ(w(t) - w0)	(31)
=-ηVwfo(x)(Vwfo(x)T(w(t) - wo) + fo(x) - Y) - ηλ(w(t) - w0)	(32)
= -η(Vwf0(X)Vwf0(X)T + λI)(w(t) - w0) + ηVwf0(X) (-f0(X) + Y).	(33)
×---------V----------}	X-------V-------}
Ab
Let ω(t) , w(t) - w0, then we have
ω(t) = Aω(t) + b.	(34)
Since all eigenvalues ofA are negative, this ODE is stable and has steady-state
ω* = -ATb	(35)
= (Vwf0(x)Vwf0(x)T + λI)-1Vwf0(x)(Y - f0(x)).	(36)
The solution ω(t) is given by:
ω(t) = ω* + eAt(ωo - ω*)	(37)
=(I - eAt)ω*.	(38)
Let Θw , Vwf0(x)Vwf0(x)T and Θ0 , Vwf0(x)TVwf0(x). If the SVD ofVwf0(x) is
UDV T, then Θw = UDUT and Θ0 = V DV T. Additionally, we can extend the columns of U to
full basis of Rd (denoted with U) and append zeros to D (denoted with D) to write down the eigen
decomposition Θw = UDDUT. With this, We have (Θw + λI)-1 = U(D + λI)-1UT. Continuing
(38) we have
ω(t) = (I - eAt)ω*	(39)
= (I - eAt)(Θw + λI)-1Vwf0(x)(Y - f0(x))	(40)
=(I - eAt)U(D + λI)-1UTUDVT(Y - f0(X))	(41)
=U(I - e-ηt(D+λI))UTU(D + λI)-1UTUDVT(Y - f0(X))	(42)
=U(I - e-ηt(D+λI))(D + λI)-1Id×nkDVT(Y - f0(X))	(43)
=U(I - e-ηt(D+λI))ZVT(Y - fo(X)),	(44)
where Z= (D + λ0I) D ∈ Rd×nk. Denoting Z，(D + λI)-1D and continuing,
~	./二，、八	~ m
ω(t) = U(I - e-巾(D+λI))ZVT(Y - fo(X))	(45)
= U(I-e-ηt(D+λI))ZVT(Y- f0(X))	(46)
= UZ(I - e-ηt(D+λI))VT(Y- f0(X))	(47)
= UZVTV(I - e-ηt(D+λI))VT(Y- f0(X))	(48)
= UZVT(I-e-ηt(Θ0+λI))(Y -f0(X))	(49)
=Vwf0(X)(Θ0+λI)-1(I-e-ηt(Θ0+λI))(Y-f0(X)).	(50)
21
Published as a conference paper at ICLR 2021
Solving for outputs. Having w(t) derived, we can write down dynamics of ft(x) for any x:
ft(x) = fo(x) + Vw fo(x)τ ω(t)	(51)
=fo(x) + Vwfo(x)TVwfo(X)(Θo + λI)-1(I — e-η*θ0+λI))(Y - f0(χ))	(52)
= f0(x) + Θ0(x, X)(Θ0 + λI)-1(I - e-ηt(Θ0+λI))(Y - f0(X)).	(53)
22