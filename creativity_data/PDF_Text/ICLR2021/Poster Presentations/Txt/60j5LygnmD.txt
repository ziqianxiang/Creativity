Published as a conference paper at ICLR 2021
Meta-learning with negative learning rates
Alberto Bernacchia
MediaTek Research
alberto.bernacchia@mtkresearch.com
Ab stract
Deep learning models require a large amount of data to perform well. When data
is scarce for a target task, we can transfer the knowledge gained by training on
similar tasks to quickly learn the target. A successful approach is meta-learning,
or learning to learn a distribution of tasks, where learning is represented by an
outer loop, and to learn by an inner loop of gradient descent. However, a num-
ber of recent empirical studies argue that the inner loop is unnecessary and more
simple models work equally well or even better. We study the performance of
MAML as a function of the learning rate of the inner loop, where zero learning
rate implies that there is no inner loop. Using random matrix theory and exact
solutions of linear models, we calculate an algebraic expression for the test loss of
MAML applied to mixed linear regression and nonlinear regression with overpa-
rameterized models. Surprisingly, while the optimal learning rate for adaptation
is positive, we find that the optimal learning rate for training is always negative,
a setting that has never been considered before. Therefore, not only does the per-
formance increase by decreasing the learning rate to zero, as suggested by recent
work, but it can be increased even further by decreasing the learning rate to neg-
ative values. These results help clarify under what circumstances meta-learning
performs best.
1	Introduction
Deep Learning models represent the state-of-the-art in several machine learning benchmarks (Le-
Cun et al. (2015)), and their performance does not seem to stop improving when adding more data
and computing resources (Rosenfeld et al. (2020), Kaplan et al. (2020)). However, they require a
large amount of data and compute to start with, which are often not available to practitioners. The
approach of fine-tuning has proved very effective to address this limitation: pre-train a model on a
source task, for which a large dataset is available, and use this model as the starting point for a quick
additional training (fine-tuning) on the small dataset of the target task (Pan & Yang (2010), Donahue
et al. (2014), Yosinski et al. (2014)). This approach is popular because pre-trained models are often
made available by institutions that have the resources to train them.
In some circumstances, multiple source tasks are available, all of which have scarce data, as opposed
to a single source task with abundant data. This case is addressed by meta-learning, in which a model
gains experience over multiple source tasks and uses it to improve its learning of future target tasks.
The idea of meta-learning is inspired by the ability of humans to generalize across tasks, without
having to train on any single task for long time. A meta-learning problem is solved by a bi-level
optimization procedure: an outer loop optimizes meta-parameters across tasks, while an inner loop
optimizes parameters within each task (Hospedales et al. (2020)).
The idea of meta-learning has gained some popularity, but a few recent papers argue that a simple
alternative to meta-learning is just good enough, in which the inner loop is removed entirely (Chen
et al. (2020a), Tian et al. (2020), Dhillon et al. (2020), Chen et al. (2020b), Raghu et al. (2020)).
Other studies find the opposite (Goldblum et al. (2020), Collins et al. (2020), Gao & Sener (2020)).
It is hard to resolve the debate because there is little theory available to explain these findings.
In this work, using random matrix theory and exact solutions of linear models, we derive an algebraic
expression of the average test loss of MAML, a simple and successful meta-learning algorithm (Finn
et al. (2017)), as a function of its hyperparameters. In particular, we study its performance as a
1
Published as a conference paper at ICLR 2021
function of the inner loop learning rate during meta-training. Setting this learning rate to zero is
equivalent to removing the inner loop, as advocated by recent work (Chen et al. (2020a), Tian et al.
(2020), Dhillon et al. (2020), Chen et al. (2020b), Raghu et al. (2020)). Surprisingly, we find that
the optimal learning rate is negative, thus performance can be increased by reducing the learning
rate below zero. In particular, we find the following:
•	In the problem of mixed linear regression, we prove that the optimal learning rate is always
negative in overparameterized models. The same result holds in underparameterized mod-
els provided that the optimal learning rate is small in absolute value. We validate the theory
by running extensive experiments.
•	We extend these results to the case of nonlinear regression and wide neural networks, in
which the output can be approximated by a linear function of the parameters (Jacot et al.
(2018), Lee et al. (2019)). While in this case we cannot prove that the optimal learning
rate is always negative, preliminary experiments suggest that the result holds in this case as
well.
2	Related work
The field of meta-learning includes a broad range of problems and solutions, see Hospedales et al.
(2020) for a recent review focusing on neural networks and deep learning. In this context, meta-
learning received increased attention in the past few years, several new benchmarks have been in-
troduced, and a large number of algorithms and models have been proposed to solve them (Vinyals
et al. (2017), Bertinetto et al. (2019), Triantafillou et al. (2020)). Despite the surge in empirical
work, theoretical work is still lagging behind.
Similar to our work, a few other studies used random matrix theory and exact solutions to calculate
the average test loss for the problem of linear regression (Advani & Saxe (2017), Hastie et al. (2019),
Nakkiran (2019)). To our knowledge, our study is the first to apply this technique to the problem of
meta-learning with multiple tasks. Our results reduce to those of linear regression in the case of one
single task. Furthermore, we are among the first to apply the framework of Neural Tangent Kernel
(Jacot et al. (2018), Lee et al. (2019)) to the problem of meta-learning (a few papers appeared after
our submission: Yang & Hu (2020), Wang et al. (2020a), Zhou et al. (2021)).
Similar to us, afew theoretical studies looked at the problem of mixed linear regression in the context
of meta-learning. In Denevi et al. (2018), Bai et al. (2021), a meta-parameter is used to bias the task-
specific parameters through a regularization term. Kong et al. (2020) looks at whether many tasks
with small data can compensate for a lack of tasks with big data. Tripuraneni et al. (2020), Du et al.
(2020) study the sample complexity of representation learning. However, none of these studies look
into the effect of learning rate on performance, which is our main focus.
In this work, we focus on MAML, a simple and successful meta-learning algorithm (Finn et al.
(2017)). A few theoretical studies have investigated MAML, looking at: universality of the opti-
mization algorithm (Finn & Levine (2018)), bayesian inference interpretation (Grant et al. (2018)),
proof of convergence (Ji et al. (2020)), difference between convex and non-convex losses (Saunshi
et al. (2020)), global optimality (Wang et al. (2020b)), effect of the inner loop (Collins et al. (2020),
Gao & Sener (2020)). Again, none of these studies look at the effect of the learning rate, the main
subject of our work. The theoretical work of Khodak et al. (2019) connects the learning rate to task
similarity, while the work of Li et al. (2017) meta-learns the learning rate.
3	Meta-learning and MAML
In this work, we follow the notation of Hospedales et al. (2020) and we use MAML (Finn et al.
(2017)) as the meta-learning algorithm. We assume the existence of a distribution of tasks τ and, for
each task, a loss function Lτ and a distribution of data points Dτ = {xτ, yτ} with input xτ and label
yτ. We assume that the loss function is the same for all tasks, Lτ = L, but each task is characterized
by a different distribution of the data. The empirical meta-learning loss is evaluated on a sample of
2
Published as a conference paper at ICLR 2021
m tasks, and a sample of nv validation data points for each task:
1 m nv
Lmeta (ω;Dt,Dv) = — XXL (θ(ω;Df));xj(i,y：⑺)	⑴
mnv i=1 j =1
The training set Dt(i) = nxtj(i) , yjt(i) o	and validation set Dv(i) = nxjv(i),
are
j=1:nv
drawn independently from the same distribution in each task i. The function θ represents the adap-
tation of the meta-parameter ω, which is evaluated on the training set. Different meta-learning
algorithms correspond to a different choice of θ, we describe below the choice of MAML (Eq.3),
the subject of this study. During meta-training, the loss of Eq.1 is optimized with respect to the
meta-parameter ω, usually by stochastic gradient descent, starting from an initial point ω0 . The
optimum is denoted as ω*(Dt, Dv). This optimization is referred to as the outer loop, while Com-
putation of θ is referred to as the inner loop of meta-learning. During meta-testing, a new (target)
task is given and θ adapts on a set Dr of nr target data points. The final performance of the model
is computed on test data Ds of the target task. Therefore, the test loss is equal to
Ltest = Lmeta (ω*(Dt, Dv)； Dr, Ds)
(2)
In MAML, the inner loop corresponds to a few steps of gradient descent, with a given learning rate
αt. In this work we consider the simple case of a single gradient step:
(3)
If the learning rate αt is zero, then parameters are not adapted during meta-training and θ(ω) = ω.
In that case, a single set of parameters in learned across all data and there is no inner loop. However,
it is important to note that a distinct learning rate αr is used during meta-testing. A setting similar
to this has been advocated in a few recent studies (Chen et al. (2020a), Tian et al. (2020), Dhillon
et al. (2020), Chen et al. (2020b), Raghu et al. (2020)).
We show that, intuitively, the optimal learning rate at
meta-testing (adaptation) time αr is always positive.
Surprisingly, in the family of problems considered
in this study, we find that the optimal learning rate
during meta-training αt is instead negative. We note
that the setting αt = 0 effectively does not use the
nt training data points, therefore we could in princi-
ple add this data to the validation set, but we do not
consider this option here since we are interested in
a wide range of possible values of αt as opposed to
the specific case αt = 0.
4	Mixed linear regression
We study MAML applied to the problem of mixed
linear regression. Note that the goal here is not to
solve the problem of mixed linear regression, but to
probe the performance of MAML as a function of its
hyperparameters.
In mixed linear regression, each task is character-
ized by a different linear function, and a model is
evaluated by the mean squared error loss function.
We assume a generative model in the form of y =
xT w + z, where x is the input vector (of dimension
p), y is the output (scalar), z is noise (scalar), and w
Figure 1: Graphical model of data generation
in mixed linear regression
is a vector of generating parameters (of dimension p), therefore p represents both the number of
parameters and the input dimension. All distributions are assumed Gaussian:
ν2
W 〜N w0,——Ip
p
X 〜N (0,Ip)	y|x, w 〜N (XTw,σ2)
(4)
3
Published as a conference paper at ICLR 2021
where Ip is the p×p identity matrix, σ is the label noise, w0 is the task mean and ν represents the task
variability. Different meta-training tasks i correspond to different draws of generating parameters
w(i), while the parameters for the meta-testing task are denoted by w0. We denote by superscripts t,
v, r, s the training, validation, target and test data, respectively. A graphical model of data generation
is shown in Figure 1.
Using random matrix theory and exact solutions of linear models, we calculate the test loss as a func-
tion of the following hyperparameters: the number of training tasks m, number of data points per
task for training (nt), validation (nv) and target (nr), learning rate for training αt and for adaptation
to target αr . Furthermore, we have the hyperparameters specific to the mixed linear regression prob-
lem: p, ν, σ, w0 . Since we use exact solutions to the linear problem, our approach is equivalent to
running the outer loop optimization until convergence (see section 7.1 in the Appendix for details).
We derive results in two cases: overparameterized p > nvm and underparameterized p < nvm.
5 Results
5.1	Overparameterized case
In the overparameterized case, the number of parameters p is larger than the total number of valida-
tion data across tasks nvm. In this case, since the data does not fully constrain the parameters, the
optimal value of ω found during meta-training depends on the initial condition used for optimiza-
tion, which we call ω0.
Theorem 1. Consider the algorithm of section 3 (MAML one-step), and the data generating model
of section 4 (mixed linear regression). Let p > nvm. Let p(ξ) and nt(ξ) be any function of order
O(ξ) as ξ → ∞. Let ∣ωo — w0| be oforder O(ξ-1/4). Then the test loss ofEq.2, averaged over the
entire data distribution (see Eq.27 in the Appendix) is equal to
+ hr
2
2
ν2
2
L^test	σ
w0|2 +
σ2nvm
2p
1+誓
ht
+O
(5)
where we define the following expressions
ht = (I- at)2 +α2 pn+tl
hr = (1 — αr )2 + α p + ɪ
r nr
(6)
(7)
Proof. The proof of this Theorem can be found in the Appendix, sections 7.3, 7.3.1.	□
The loss always increases with the output noise σ and task variability ν. Overfitting is expressed in
Eq.5 by the term ∣ω0 — wo|, the distance between the initial condition for the optimization of ω0
and the ground truth mean of the generating model w0. Adding more validation data nv and tasks m
may increase or decrease the loss depending on the size of this term relative to the noise (Nakkiran
(2019)), as it does reducing the number of parameters p. However, the loss always decreases with
the number of data points for the target tasknr, as that data only affects the adaptation step.
Our main focus is studying how the loss is affected by the learning rates, during training αt and
adaptation αr. The loss is a quadratic and convex function ofαr, therefore it has a unique minimum.
While it is possible to compute the optimal value of αr from Eq.5, here we just note that the loss
is a sum of two quadratic functions, one has a minimum at αr = 0 and another has a minimum
at αr = 1/ (1 + (p+ 1)/nr), therefore the optimal learning rate is in between the two values and
is always positive. This is intuitive, since a positive learning rate for adaptation implies that the
parameters get closer to the optimum for the target task. An example of the loss as a function of the
adaptation learning rate αr is shown in Figure 2a, where we also show the results of experiments
4
Published as a conference paper at ICLR 2021
in which we run MAML empirically. The good agreement between theory and experiment suggest
that Eq.5 is accurate.
However, the training learning rate αt shows the opposite: by taking the derivative of Eq.5 with
respect to αt, it is possible to show that it has a unique absolute minimum for a negative value of αt.
This can be proved by noting that this function has the same finite value for large positive or negative
αt, its derivative is always positive at αt = 0, and it has one minimum (-) and one maximum (+)
at values
nt + 1
2p
± ZiP
(8)
Note that the argmax αt+ is always positive, while the argmin αt- is always negative. This result
is counter-intuitive, since a negative learning rate pushes parameters towards higher values of the
loss. However, learning of the meta-parameter ω is performed by the outer loop (minimize Eq.1),
for which there is no learning rate since we are using the exact solution to the linear problem and
thus we are effectively training to convergence. Therefore, it remains unclear whether the inner loop
(Eq.3) should push parameters towards higher or lower values of the loss. An example of the loss
as a function of the training learning rate αr is shown in Figure 2b, where we also show the results
of experiments in which we run MAML empirically. Here the theory slightly underestimate the
experimental loss, but the overall shapes of the curves are in good agreement, suggesting that Eq.5
is accurate. Additional experiments are shown in the Appendix, Figure 6.
Figure 2: Average test loss of MAML as a function of the learning rate, on overparameterized
mixed linear regression, as predicted by our theory and confirmed in experiments. a) Effect of
learning rate αr during adaptation. b) Effect of learning rate αt during training. The optimal learning
rate during adaptation is positive, while that during training is negative. Values of parameters:
nt = 30, nv = 2, nr = 20, m = 3, p = 60, σ = 1., ν = 0.5, ω0 = 0, w0 = 0. In panel a) we set
αt = 0.2, in panel b) we set αr = 0.2. In the experiments, each run is evaluated on 100 test tasks of
50 data points each, and each point is an average over 100 runs (a) or 1000 runs (b).
5.2	Underparameterized case
In the underparameterized case, the number of parameters p is smaller than the total number of
validation data across tasks nvm. In this case, since the data fully constrains the parameters, the
optimal value of ω found during meta-training is unique. We prove the following result.
Theorem 2. Consider the algorithm of section 3 (MAML one-step), and the data generating model
of section 4 (mixed linear regression). Letp < nvm. Let nv (ξ) and nt(ξ) be any function of order
O(ξ). For ξ, m → ∞, the test loss of Eq.2, averaged over the entire data distribution (see Eq.27 in
the Appendix) is equal to
5
Published as a conference paper at ICLR 2021
+R ɪ^2
2ht2 nvm
hrν2
十 丁+
2
h +--- [(nv + I) gl + pg2]
nt
Ltest = σ22 (ι +
2
+ — [(nv + 1) g3 + Pg4]} + O ((mξ)-3/2)
(9)
where hr, ht are defined as in previous section, Eqs.6, 7, and gi are order O(1) polynomials in αt,
see Eqs.98-101 in the Appendix.
Proof. The proof of this Theorem can be found in the Appendix, sections 7.3, 7.3.2.	□
Again, the loss always increases with the output noise σ and task variability ν. Furthermore, in this
case the loss always decreases with the number of data points nv , nr, and tasks m. Note that, for
a very large number of tasks m, the loss does not depend on meta-training hyperparameters αt, nv ,
nt. When the number of tasks is infinite, it doesn’t matter whether we run the inner loop, and how
much data we have for each task.
As in the overparameterized case, the loss is a quadratic and convex function of the adaptation
learning rate αr, and there is a unique minimum. While the value of the argmin is different, in
this case as well the loss is a sum of two quadratic functions, one with minimum at αr = 0 and
another with a minimum at αr = 1/ (1 + (p + 1)/nr), therefore the optimal learning rate is again
in between the same two values and is always positive. Similar comments applies to this case: a
positive learning rate for adaptation implies that the parameters get closer to the optimum for the
target task. An example of the loss as a function of the adaptation learning rate αr is shown in Figure
3a, where we also show the results of experiments in which we run MAML empirically. The good
agreement between theory and experiment suggest that Eq.9 is accurate.
As a function of the training learning rate αt, the loss Eq.9 is the ratio of two fourth order polyno-
mials, therefore it is not straightforward to determine its behaviour. However, it is possible to show
that the following holds
∂Ltest
∂αt
σ2p
=—p ≥ 0
nv m
αt=0
(10)
suggesting that performance is always better for negative values of αt around zero. Even if counter-
intuitive, this finding aligns with that of previous section, and similar comments apply. An example
of the loss as a function of the training learning rate αr is shown in Figure 3b, where we also show
the results of experiments in which we run MAML empirically. A good agreement is observed
between theory and experiment, again suggesting that Eq.9 is accurate. Additional experiments are
shown in the Appendix, Figure 6.
5.3	Non-Gaussian theory in overparameterized models
In previous sections we studied the performance of MAML applied to the problem of mixed linear
regression. It remains unclear whether the results in the linear case are relevant for the more in-
teresting case of nonlinear problems. Inspired by recent theoretical work, we consider the case of
nonlinear regression with squared loss
L (ω) = EE 1[y - f (x, ω)]2	(11)
x y|x 2
where y is a target output and f (x, ω) the output of a neural network with input x and parameters
ω. The introduction of the Neural Tangent Kernel showed that, in the limit of infinitely wide neural
networks, the output is a linear function of its parameters during the entire course of training (Jacot
et al. (2018), Lee et al. (2019)). This is expressed by a first order Taylor expansion
f(x,ω) ' f(x,ω0) +k(x,ω0)T (ω -ω0)	(12)
k (x, 30)= Vω f(x, ω)lχ,ω0	(13)
6
Published as a conference paper at ICLR 2021
Figure 3: Average test loss as a function of the learning rate, on underparameterized mixed linear
regression, as predicted by our theory and confirmed in experiments. a) Effect of learning rate αr
during testing. b) Effect of learning rate αt during training. The optimal learning rate during testing
is always positive, while that during training is negative. Values of parameters: nt = 5, nv =
25, nr = 10, m = 40,p = 30, σ = 0.2, ν = 0.2. In panel a) we set αt = 0.2, in panel b) we set
αr = 0.2. In the experiments, the model is evaluated on 100 tasks of 50 data points each, and each
point is an average over 100 (a) or 1000 (b) runs.
The parameters ω remain close to the initial condition ω0 during the entire course of training, a
phenomenon referred to as lazy training (Chizat et al. (2020)), and therefore the output can be
linearized around ω0 . Intuitively, in a model that is heavily overparameterized, the data does not
constrain the parameters, and a parameter that minimizes the loss in Eq.11 can be found in the
vicinity of any initial condition ω0 . Note that, while the output of the neural network is linear in the
parameters, it remains a nonlinear function of its input, through the vector of nonlinear functions k
in Eq.13.
By substituting Eq.12 into Eq.11, the nonlinear regression becomes effectively linear, in the sense
that the loss is a quadratic function of the parameters ω , and all nonlinearities are contained in the
functions k in Eq.13, that are fixed by the initial condition ω0. This suggests that we can carry over
the theory developed in the previous section to this problem. However, in this case the input to the
linear regression problem is effectively k (x), and some of the assumptions made in the previous
section are not acceptable. In particular, even if we assume that x is Gaussian, k (x) is a nonlinear
function of x and cannot be assumed Gaussian. We prove the following result, where we generalize
the result of section 5.1 to non-Gaussian inputs and weights.
Theorem 3. Consider the algorithm of section 3 (MAML one-step), with ω0 = 0, and the data
generating model of section 4, where the input x and the weights w are not necessarily Gaussian,
and have zero mean and covariances, respectively, Σ = ExxT and Σw = EwwT. Let F be the
matrix of fourth order moments F = E xTΣx xxT. Let p > nvm. Let p(ξ) and nt(ξ) be any
function of order O(ξ) as ξ → ∞. Let Tr Σ2w be of order O ξ-1 , and let the variances of matrix
products of the rescaled inputs x/√p, UP to sixth order, be of order O (ξ-1) (see Eqs.134-136 in
the Appendix). Then the test loss of Eq.2, averaged over the entire data distribution (see Eq.27 in
the Appendix) is equal to
+ 1 nvmTMHrHt) {，,&H',: U nTE } + O ,K)
where we define the following matrices
2
Ht = Σ(I - at∑)2 + -t (F -
nt
2
Hr = Σ(I - -rΣ)2 + -r (F -
(14)
(15)
(16)
7
Published as a conference paper at ICLR 2021
Proof. The proof of this Theorem can be found in Appendix, section 7.4.
□
Note that this result reduces to Eqs.5, 6, 7 When Σ = I, ∑w = Iν2/p, F = I(P + 2), ω0 = 0,
w = 0. This expression for the loss is more difficult to analyze than those given in the previous
sections, because it involves traces of nonlinear functions of matrices, all elements of Which are free
hyperparameters. Nevertheless, it is possible to shoW that, as a function of the adaptation learning
rate αr, the loss in Eq.14 is still a quadratic function. As a function of the adaptation learning rate
αr, the loss in Eq.14 is the ratio of tWo fourth order polynomials, but it is difficult to draW any
conclusions since their coefficients do not appear to have simple relationships.
Even if the influence of the hyperparameters is not easy to predict, the expression in Eq.14 can still
be used to quickly probe the behavior of the loss empirically, by using example values for the Σ,
Σw , F, since computing the expression is very fast. Here We choose values of Σ, Σw by a single
random draW from a Wishart distribution
∑ 〜W (I,P)	Σw 〜ν2W (I,p)	(17)
p
Note that the number of degrees of freedom of the distribution is equal to the size of the ma-
trices, P, therefore this covariances display significant correlations. Furthermore, We choose
F = 2Σ3 + ΣTr Σ2 , Which is the value taken When x folloWs a Gaussian distribution. There-
fore, We effectively test the loss in Eq.14 for a Gaussian distribution, as in previous section, but We
stress that the expression is valid for any distribution of x Within the assumptions of Theorem 3. We
also run experiments of MAML, applied again to mixed linear regression, but noW using the covari-
ance matrices draWn in Eq.17. Figure 4 shoWs the loss in Eq.14 as a function of the learning rates,
during adaptation (panel a) and training (panel b). Qualitatively, We observe a similar behaviour as
in section 5.1: the adaptation learning rate has a unique minimum for a positive value of αr, While
the training learning rate shoWs better performance for negative values of αt . Again, there is a good
agreement betWeen theory and experiment, suggesting that Eq.14 is a good approximation.
Figure 4: Average test loss of MAML as a function of the learning rate, on overparameterized mixed
linear regression With Wishart covariances, as predicted by our theory and confirmed in experiments.
a) Effect of learning rate αr during adaptation. b) Effect of learning rate αt during training. The
optimal learning rate during adaptation is positive, While that during training appears to be negative.
Values of parameters: nt = 30, nv = 2, nr = 20, m = 3, P = 60, σ = 1., ν = 0.5, ω0 = 0,
w0 = 0. In panel a) We set αt = 0.2, in panel b) We set αr = 0.2. In the experiments, each run is
evaluated on 100 tasks of 50 data points each, and each point is an average over 100 runs (a) or 500
runs (b).
5.4 Nonlinear regression
To investigate Whether negative learning rates improve performance on non-linear regression in
practice, We studied the simple case of MAML With a neural netWork applied to a quadratic function.
Specifically, the target output is generated according to y = (wTx + b)2 + z, Where b is a bias term.
The data x, z and generating parameters w are sampled as described in section 4 (in addition, the
bias b Was draWn from a Gaussian distribution of zero mean and unit variance.). We use a 2-layer
8
Published as a conference paper at ICLR 2021
feed-forward neural network with ReLU activation functions. Weights are initialized following a
Gaussian distribution of zero mean and variance equal to the inverse number of inputs. We report
results with a network width of 400 in both layers; results were similar with larger network widths.
We use the square loss function and we train the neural network in the outer loop with stochastic
gradient descent with a learning rate of 0.001 for 5000 epochs (until convergence). We used most
parameters identical to section 5.1: nt = 30; nv = 2; nr = 20; m = 3; p = 60; σ = 1, ν =
0.5, w0 = 0. The learning rate for adaptation was set to αr = 0.01. Note that in section 5.1
the model was initialized at the ground truth of the generative model (ω0 = w0 ), while here the
neural network parameters are initialized at random. Figure 5 shows the test loss as a function of the
learning rate αt. The best performance is obtained for a negative learning rate of αt = -0.0075.
6 Discussion
We calculated algebraic expressions for the average
test loss of MAML applied to a simple family of lin-
ear models, as a function of the hyperparameters.
Surprisingly, we showed that the optimal value of
the learning rate of the inner loop during training is
negative. This finding seems to carry over to more
interesting nonlinear models in the overparameter-
ized case. However, additional work is necessary
to establish the conditions under which the optimal
learning rate may be positive, for example by prob-
ing more extensively Eq.14.
A negative optimal learning rate is surprising and
counter-intuitive, since negative learning rates push
parameters towards higher values of the loss. How-
ever, the meta-training loss is minimized by the outer
loop, therefore it is not immediately obvious whether
the learning rate of the inner loop should be positive,
and we show that in some circumstances it should
not. However, perhaps obviously, we also show that
the learning rate during adaptation at test time should
always be positive, otherwise the target task cannot
be learned.
In this work, we considered the case of nonlinear
models in the overparameterized case. However,
typical applications of MAML (and meta-learning in
Figure 5: Average test loss of MAML as a
function of the learning rate, on nonlinear
(quadratic) regression using a 2-layer feed-
forward neural network. Optimal learning
rate is negative, consistent with results on the
linear case. Each run is evaluated on 1000
test tasks, and each point is an average over
10 runs. Error bars show standard errors.
Note the qualitative similarity with Figures
2b and 4b.
general) implement relatively small models due to the heavy computational load of running bi-level
optimization, including both outer and inner loop. Our theory applies to regression problems, and
assumes a limited number of tasks where data is independently drawn in each task, while some ap-
plications use a large number of tasks with correlated draws (for example, images may be shared
across tasks in few-shot image classification, see Bertinetto et al. (2019)). Our theory is valid at
the exact optimum of the outer loop, which is equivalent to training the outer loop to convergence,
therefore overfitting may occur in the outer loop of our model. Another limitation of our theory
is represented by the assumptions on the input and task covariance, which have no correlations in
Theorems 1, 2, and are subject to some technical assumptions in Theorem 3.
To the best of our knowledge, nobody has considered training meta-learning models with negative
learning rates in the inner loop. Given that some studies advocate removing the inner loop altogether,
which is similar to setting the learning rate to zero, it would be interesting to try a negative one. On
the other hand, it is possible that a negative learning rate does not work in classification problems,
in nonlinear models, or using input or tasks with a complex structure, settings that are outside the
theory presented in this work.
We would like to thank Paolo Grazieschi for helping with formalizing the theorems, and Ritwik
Niyogi for helping with nonlinear regression experiments.
9
Published as a conference paper at ICLR 2021
References
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error
in neural networks. arXiv:1710.03667 [physics, q-bio, stat], October 2017. URL http:
//arxiv.org/abs/1710.03667. arXiv: 1710.03667.
Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason D. Lee, Sham Kakade, Huan Wang, and Caiming
Xiong. How Important is the Train-Validation Split in Meta-Learning? arXiv:2010.05843 [cs,
stat], February 2021. URL http://arxiv.org/abs/2010.05843. arXiv: 2010.05843.
LUca Bertinetto, Joao F. Henriques, Philip H. S. Torr, and Andrea Vedaldi. Meta-Iearning with
differentiable closed-form solvers. arXiv:1805.08136 [cs, stat], July 2019. URL http://
arxiv.org/abs/1805.08136. arXiv: 1805.08136.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A
Closer Look at Few-shot Classification. arXiv:1904.04232 [cs], January 2020a. URL http:
//arxiv.org/abs/1904.04232. arXiv: 1904.04232.
Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A New Meta-Baseline
for Few-Shot Learning. arXiv:2003.04390 [cs], March 2020b. URL http://arxiv.org/
abs/2003.04390. arXiv: 2003.04390.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Program-
ming. arXiv:1812.07956 [cs, math], January 2020. URL http://arxiv.org/abs/1812.
07956. arXiv: 1812.07956.
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Why Does MAML Outperform ERM? An
Optimization Perspective. arXiv:2010.14672 [cs, math, stat], December 2020. URL http:
//arxiv.org/abs/2010.14672. arXiv: 2010.14672.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning To Learn
Around A Common Mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
10169-10179. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8220-learning-to-learn-around-a-common-mean.pdf.
Guneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A Baseline
for Few-Shot Image Classification. arXiv:1909.02729 [cs, stat], March 2020. URL http:
//arxiv.org/abs/1909.02729. arXiv: 1909.02729.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Dar-
rell. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. ICML,
pp. 9, 2014.
Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-Shot Learning via Learning
the Representation, Provably. arXiv:2002.09434 [cs, math, stat], February 2020. URL http:
//arxiv.org/abs/2002.09434. arXiv: 2002.09434.
Chelsea Finn and Sergey Levine. Meta-Learning and Universality: Deep Representations and Gra-
dient Descent can Approximate any Learning Algorithm. arXiv:1710.11622 [cs], February 2018.
URL http://arxiv.org/abs/1710.11622. arXiv: 1710.11622.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adap-
tation of Deep Networks. arXiv:1703.03400 [cs], March 2017. URL http://arxiv.org/
abs/1703.03400. arXiv: 1703.03400.
Katelyn Gao and Ozan Sener. Modeling and Optimization Trade-off in Meta-learning.
arXiv:2010.12916 [cs, math, stat], October 2020. URL http://arxiv.org/abs/2010.
12916. arXiv: 2010.12916.
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Gold-
stein. Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks.
arXiv:2002.06753 [cs, stat], March 2020. URL http://arxiv.org/abs/2002.06753.
arXiv: 2002.06753.
10
Published as a conference paper at ICLR 2021
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. RECASTING
GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES. ICLR, pp. 13, 2018.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in High-
Dimensional Ridgeless Least Squares Interpolation. arXiv:1903.08560 [cs, math, stat], Novem-
ber 2019. URL http://arxiv.org/abs/1903.08560. arXiv: 1903.08560.
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-Learning in Neural
Networks: A Survey. arXiv:2004.05439 [cs, stat], April 2020. URL http://arxiv.org/
abs/2004.05439. arXiv: 2004.05439.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL http:
//arxiv.org/abs/1806.07572. arXiv: 1806.07572.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-Step Model-Agnostic Meta-Learning: Convergence
and Improved Algorithms. arXiv:2002.07836 [cs, math, stat], February 2020. URL http:
//arxiv.org/abs/2002.07836. arXiv: 2002.07836.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language
Models. arXiv:2001.08361 [cs, stat], January 2020. URL http://arxiv.org/abs/2001.
08361. arXiv: 2001.08361.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive Gradient-Based Meta-
Learning Methods. arXiv:1906.02717 [cs, stat], December 2019. URL http://arxiv.org/
abs/1906.02717. arXiv: 1906.02717.
Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, and Sewoong Oh. Meta-learning for
mixed linear regression. arXiv:2002.08936 [cs, stat], February 2020. URL http://arxiv.
org/abs/2002.08936. arXiv: 2002.08936.
Yann LeCun, YoshUa Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436—444,
May 2015. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature14539. URL http://www.
nature.com/articles/nature14539.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradi-
ent Descent. arXiv:1902.06720 [cs, stat], February 2019. URL http://arxiv.org/abs/
1902.06720. arXiv: 1902.06720.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: Learning to Learn Quickly for
Few-Shot Learning. arXiv:1707.09835 [cs], September 2017. URL http://arxiv.org/
abs/1707.09835. arXiv: 1707.09835.
Preetum Nakkiran. More Data Can Hurt for Linear Regression: Sample-wise Double Descent.
arXiv:1912.07242 [cs, math, stat], December 2019. URL http://arxiv.org/abs/1912.
07242. arXiv: 1912.07242.
Sinno Jialin Pan and Qiang Yang. A Survey on Transfer Learning. IEEE Transactions on Knowledge
andDataEngineering, 22(10):1345-1359, October 2010.ISSN 1041-4347. doi: 10.1109/TKDE.
2009.191. URL http://ieeexplore.ieee.org/document/5288526/.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid Learning or Feature
Reuse? Towards Understanding the Effectiveness of MAML. arXiv:1909.09157 [cs, stat], Febru-
ary 2020. URL http://arxiv.org/abs/1909.09157. arXiv: 1909.09157.
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A CONSTRUCTIVE
PREDICTION OF THE GENERALIZATION ERROR ACROSS SCALES. ICLR, pp. 30, 2020.
Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A Sample Complexity Separation
between Non-Convex and Convex Meta-Learning. arXiv:2002.11172 [cs, math, stat], February
2020. URL http://arxiv.org/abs/2002.11172. arXiv: 2002.11172.
11
Published as a conference paper at ICLR 2021
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. Rethinking
Few-Shot Image Classification: a Good Embedding Is All You Need? arXiv:2003.11539 [cs],
June 2020. URL http://arxiv.org/abs/2003.11539. arXiv: 2003.11539.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-
Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. arXiv:1903.03096 [cs,
stat], February 2020. URL http://arxiv.org/abs/1903.03096. arXiv: 1903.03096.
Nilesh Tripuraneni, Chi Jin, and Michael I. Jordan. Provable Meta-Learning of Linear Representa-
tions. arXiv:2002.11684 [cs, stat], February 2020. URL http://arxiv.org/abs/2002.
11684. arXiv: 2002.11684.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing Networks for One Shot Learning. arXiv:1606.04080 [cs, stat], December 2017. URL
http://arxiv.org/abs/1606.04080. arXiv: 1606.04080.
Haoxiang Wang, Ruoyu Sun, and Bo Li. Global Convergence and Generalization Bound of
Gradient-Based Meta-Learning with Deep Neural Nets. arXiv:2006.14606 [cs, stat], November
2020a. URL http://arxiv.org/abs/2006.14606. arXiv: 2006.14606.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the Global Optimality of Model-
Agnostic Meta-Learning. arXiv:2006.13182 [cs, stat], June 2020b. URL http://arxiv.
org/abs/2006.13182. arXiv: 2006.13182.
Greg Yang and Edward J. Hu. Feature Learning in Infinite-Width Neural Networks.
arXiv:2011.14522 [cond-mat], November 2020. URL http://arxiv.org/abs/2011.
14522. arXiv: 2011.14522.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? arXiv:1411.1792 [cs], November 2014. URL http://arxiv.org/abs/
1411.1792. arXiv: 1411.1792.
Yufan Zhou, Zhenyi Wang, Jiayi Xian, Changyou Chen, and Jinhui Xu. Meta-Learning with Neural
Tangent Kernels. arXiv:2102.03909 [cs], February 2021. URL http://arxiv.org/abs/
2102.03909. arXiv: 2102.03909.
12
Published as a conference paper at ICLR 2021
7 Appendix
Figure 6: Average test loss of MAML as a function of the learning rate αt (training) on mixed linear
regression, showing the transition from strongly overparameterized (a), to weakly overparameterized
(b), weakly underparameterized (c) and strongly underparameterized (d). As expected, predictions
of theory are accurate only in panels (a) and (d). The amount of validation data increases from panels
(a) to (d), with the following values: m = 1, nv = 2 (a), m = 5, nv = 5 (b), m = 10, nv = 10 (c),
m = 10, nv = 40. Other parameters are equal to: nt = 40, nr = 40,p = 50, σ = 0.5., ν = 0.5,
αr = 0.2, ω0 = 0, w0 = (0.1, 0.1, . . . , 0.1) (note that overfitting occurs since ω0 6= w0). In the
experiments, each run is evaluated on 100 test tasks of 50 data points each, and each point is an
average over 100 runs.
7.1	Definition of the loss function
We consider the problem of mixed linear regression y = Xw + z with squared loss, where X is
a n × p matrix of input data, each row is one of n data vectors of dimension p, z is a n × 1 noise
vector, w is a p × 1 vector of generating parameters and y is a n × 1 output vector. Data is collected
for m tasks, each with a different value of the parameters w and a different realization of the input
X and noise z. We denote by w(i) the parameters for task i, for i = 1, . . . , m. For a given task i, we
denote by Xt(i), Xv(i) the input data for, respectively, the training and validation sets, by zt(i), zv(i)
the corresponding noise vectors and by yt(i), yv(i) the output vectors. We denote by nt, nv the data
sample size for training and validations sets, respectively.
For a given task i, the training output is equal to
yt(i) = Xt(i)w(i) + zt(i)	(18)
Similarly, the validation output is equal to
yv(i) = Xv(i)w(i) + zv(i).	(19)
We consider MAML as a model for meta-learning (Finn et al 2017). The meta-training loss is equal
to
Lmeta _	1
2nvm
X yv(i) - Xv(i) θ(i) (ω)2	(20)
i=1
13
Published as a conference paper at ICLR 2021
where vertical brackets denote euclidean norm, and the estimated parameters θ(i) (ω) are equal to
the one-step gradient update on the single-task training loss L(i) = |yt(i) - Xt(i)θ(i) |2/2nt, with
initial condition given by the meta-parameter ω . The single gradient update is equal to
θ(i)(ω) = (Ip - αtXt(i)TXMi)) ω + αtXt(i)Tyt(i)	(21)
where Ip is the p × p identity matrix and αt is the learning rate. We seek to minimize the meta-
training loss with respect to the meta-parameter ω , namely
ω? = arg min Lmeta	(22)
ω
We evaluate the solution ω? by calculating the meta-test loss
Ltest = ɪ |ys - Xsθ*∣2	(23)
2ns
Note that the test loss is calculated over test data Xs, zs, and test parameters w0, namely
ys = Xsw0 + zs	(24)
Furthermore, the estimated parameters θ? are calculated on a separate set of target data Xr , zr,
namely
θ? = (Ip - αrXrTXr) ω? + αrXrTyr	(25)
nr	nr
yr = Xrw0 + zr	(26)
Note that the learning rate and sample size can be different at testing, denoted by αr, nr, ns. We are
interested in calculating the average test loss, that is the test loss of Eq.23 averaged over the entire
data distribution, equal to
Ltest = EEEEEEEEEE 1— |ys - X sθ*∣2	(27)
w zt Xt zv Xv w0 zs Xs zr Xr 2ns
7.2	Definition of probability distributions
We assume that all random variables are Gaussian. In particular, we assume that the rows of the
matrix X are independent, and each row, denoted by x, is distributed according to a multivariate
Gaussian with zero mean and unit covariance
X 〜N (0,Ip)	(28)
where Ip is the p × p identity matrix. Similarly, the noise is distributed following a multivariate
Gaussian with zero mean and variance equal to σ2, namely
Z 〜N (0,σ2In)	(29)
Finally, the generating parameters are also distributed according to a multivariate Gaussian of vari-
ance V2∕p, namely
W 〜N (wo, V-Ip)	(30)
The generating parameter w is drawn once and kept fixed within a task, and drawn independently
for different tasks. The values of X and Z are drawn independently in all tasks and datasets (train-
ing, validation, target, test). In order to perform the calculations in the next section, we need the
following results.
14
Published as a conference paper at ICLR 2021
Lemma 1. Let X be a Gaussian n × p random matrix with independent rows, and each row has
covariance equal to Ip, the p × p identity matrix. Then:
E XTX = nIp	(31)
E [(X T X )2] =	n	(n + P +1)Ip = n2 μ2Ip	(32)
E [(XTX)3] =	n	(n2	+ Pp	+ 3np + 3n + 3p + 4)Ip = n3μ3Ip	(33)
E [(XTX)4] =	n	(n3	+ p3	+ 6n2p + 6np2+	(34)
+6n2 + 6p2 + 17np + 21n + 21p + 20)Ip = n4μ4Ip	(35)
E [XTX Tr (XTX)] = (n2p + 2n) Ip = ρn2μ1,1Ip	(36)
E [(XTX)2 Tr(XTX)] = n (n2p +nρ2 + np + 4n + 4p + 4) Ip = pn3μ2,1Ip	(37)
E [xTXTr ((XTX)2) = = n (n2P + np2 + np + 4n + 4p + 4) Ip = pn3μ1,2Ip	(38)
E [(X TX )2 Tr ((X TX )2) ]= n (n3p +nP3 + 2n2P2 + 2n2P + 2nP2 +	(39)
+8n2 + 8p2 + 21np + 20n + 20p + 20) Ip = ρn4μ2,2Ip	(40)
where the last equality in each of these expressions defines the variables μ. Furthermore, for any
n × n symmetric matrix C and anyP × P symmetric matrix D, independent ofX:
E [XT CX] = Tr (C) Ip	(41)
E [XT XDXT X] = n(n+ 1)D+nTr(D)Ip	(42)
Proof. The Lemma follows by direct computations of the above expectations, using Isserlis’ theo-
rem. Particularly, for higher order exponents, combinatorics plays a crucial role in counting products
of different Gaussian variables in an effective way.
□
Lemma 2. Let Xv(i), X t(i) be Gaussian random matrices, of size respectively nv × P and nt × P,
with independent rows, and each row has covariance equal to Ip, the P × P identity matrix. Let P(ξ)
and nt(ξ) be any function of order O(ξ) as ξ → ∞. Then:
XVCi)XVCi)T = PInv + O (ξ1/2)	(43)
Xv(i)Xt(i)TXt(i)Xv(i)T = pnt Inv + O (g3/2)	(44)
XV(i)Xt(i)TXt(i)Xt(i)TXt(i)XV(i)T = pnt(nt + P + 1)Inv + O (g5/2)	(45)
Note that the order O (ξ ) applies to all elements of the matrix in each expression. For i 6= j
XV(i)XV(j)T = O ξ 1/2	(46)
XVCi)Xt(i)Txt(i)χv(j)T = o (ξ3∕2)	(47)
χv(i)Xt(i)TXt(i)Xt(j)TXt(j)Xv(j)T = o (ξ5∕2)	(48)
15
Published as a conference paper at ICLR 2021
Furthermore, for any positive real number δ and for any p × p symmetric matrix D independent of
X, where Tr(D) and Tr(D2) are both of order O(ξδ)
Xv(i)DXv(i)T = Tr(D)Inv + O (ξ"2)	(49)
Xv(i)Xt(i)TXt(i)DXv(i)T = Tr(D) ntInv + O (ξ1+“2)	(50)
Xv(i)Xt(i)TXt(i)DXt(i)TXt(i)Xv(i)T = Tr(D) nt(nt + P + 1)Inv + O (ξ2+“2)	(51)
XV⑴DXVj)T = O 6/2)	(52)
XV⑴Xt⑴TXt⑴DXVj)T = O (ξ1+"2)	(53)
XVei)XMi)TXMi)DXtCj)TXtCj)XVj)T = o (ξ2+δ∕2)	(54)
Proof. The Lemma follows by direct computations of the expectations and variances of each term.
□
Lemma 3. Let XV, Xt be Gaussian random matrices, of size respectively nV × p and nt × p, with
independent rows, and each row has covariance equal to Ip, the p × p identity matrix. Let nV (ξ)
and nt(ξ) be any function of order O(ξ) forξ → ∞. Then:
XVTXV = n Ip + O (ξ1∕2)	(55)
XtTXtXVTXV = ntnV Ip + O (g3/2)	(56)
XtTXtXVTXVXtTXt = nnt (nt + P +1)Ip + O (ξ5∕2)	(57)
Note that the order O (ξ) applies to all elements of the matrix in each expression.
Proof. The Lemma follows by direct computations of the expectations and variances of each term.
□
7.3	Proof of Theorems 1 and 2
We calculate the average test loss as a function of the hyperparameters nt, nV , nr, P, m, αt, αr, σ,
ν, w0. Using the expression in Eq.24 for the test output, we rewrite the test loss in Eq.27 as
Ltest = E ɪ |XS (w0- θ?)+ Zsl2
2ns
(58)
We start by averaging this expression with respect to Xs, Zs, noting that θ? does not depend on test
data. We further average with respect to w0, but note that θ? depends on test parameters, so we
average only terms that do not depend on θ?. Using Eq.31, the result is
Ltest = σ22 + ? + 孚 + E
lθ2——(wo + δw0)T θ?
(59)
where we define δw0 = w0 - w0 . The second term in the expectation is linear in θ? and can be
averaged over Xr , Zr, using Eq.25 and noting that ω? does not depend on target data. The result is
E E θ? = (1 - αr)ω? + αr (w0 + δw0)	(60)
Xr zr
Using Eq.60 we average over w0 the second term in the expectation of Eq.59 and find
Ltest = σ2- + (2 - αj (V2 + ∣W0∣2) - (1 - αr) wTE ω? + Elθ2l-
(61)
16
Published as a conference paper at ICLR 2021
We average the last term of this expression over zr , w0, using Eq.25 and noting that ω? does not
depend on target data and test parameters. The result is
22
EoElθ?1 = lω*l+ nr(ω?-w0)	(XrXr) (ω?-w0)-
-2αrXrTXrω*τ (ω? - W0) + α2σ2Tr [xrXrT] + α2V2Tr
nr	nr2	nr2p
(62)
(63)
We now average over Xr, again noting that ω? does not depend on target data. Using Eqs.31, 32,
we find
EEE ∣θ*∣2 = ∣ω*∣2 + «r fl + p-+-1) (ν2 + ∣ω? - wo∣2) - 2%ω*T (ω? - w0) + αrσ2p
X r w0 zr	r	nr	nr
(64)
We can now rewrite the average test loss 61 as
Ltest = στ (1 + αrp) + 1 ](1-αr)2 + αrp+1 _ (ν2+elω?- w0|2)	(65)
In order to average the last term, we need an expression for ω?. We note that the loss in Eq.20
is quadratic in ω, therefore the solution of Eq.22 can be found using standard linear algebra. In
particular, the loss in Eq.20 can be rewritten as
Lmeta = -i- ∣γ - Bω∣2
2nvm
(66)
where γ is a vector of shape nvm × 1, and B is a matrix of shape nvm × p. The vector γ is a stack
of m vectors
χ	XV⑴(Ip- αtXt(1)TXt(1))w⑴-atXV(I)Xt(1)TZt⑴+zv⑴ ʌ
γ
(67)
XV(m)
atχt(m)TχMm)) WIm)
nt
at X v(m)χ t(m) T 2t(m) + 2v(m)
nt	'	)
Similarly, the matrix B is a stack of m matrices
X xV(I) (Ip - atxt(I)Txt(I)) ∖
B
(68)
Xv(m) 11 - at X Mm)T X Mm))
p nt
We denote by Ip the p × p identity matrix. The expression for ω that minimizes Eq.66 depends on
whether the problem is overparameterized (p > nVm) or underparameterized (p < nVm), therefore
we distinguish these two cases in the following sections.
7.3.1	Overparameterized case (Theorem 1)
In the overparameterized case (p > nVm), under the assumption that the inverse of BBT exists, the
value of ω that minimizes Eq.66 is equal to
ω? = BT (BBT)-1 Y + [Ip - BT (BBT)-1 Bi 30	(69)
The vector ω0 is interpreted as the initial condition of the parameter optimization of the outer loop,
when optimized by gradient descent. Note that the matrix B does not depend on w, zt , zV , and
Ew Ezt Ezv γ = Bw0 . We denote by δγ the deviation from the average, and we have
ω? - wo = BT (BBT) — 1 δγ + [Ip - BT (BBT) — 1 Bi (ω0 - w0)	(70)
We square this expression and average over w, zt , zV . We use the cyclic property of the trace and
the fact that BT BBT —1 B is a projection. The result is
∣ω? - w0∣2 = Tr	[γ	(BBT) 1] +	(ω°	- wo)T	[Ip -	BT (BBT)	1	Bi	(ωQ	- wo)	(71)
17
Published as a conference paper at ICLR 2021
The matrix Γ is defined as
∕Γ(1)	0	0
Γ = EEE δγ δγT =	0	..	0
w zt zv
0	0	Γ(m)
(72)
Where matrix blocks are given by the following expression
Γ⑴=ν2XV⑴(Ip- aXt(i)TXMi)) Xv(i)T + σ2 (In
+ αt X v(i)χ t(i)T X t(i)χ v(i)TA
n2	J
(73)
v
It is convenient to rewrite the scalar product of Eq.71 in terms of the trace of outer products
∣ω? - w0∣2 = Tr [(BBt) 1 (γ - B (ωo - w°)(ω° - wo)T BT)] + ∣ω0 - wo∣2	(74)
In order to calculate E ∣ω? - w0∣2 in Eq.65 We need to average this expression over training and
validation data. These averages are hard to compute since they involve nonlinear functions of the
data. HoWever, We can approximate these terms by assuming that p and nt are large, both of order
O(ξ), where ξ is a large number. Furthermore, we assume that ∣ω0 - wo| is of order O(ξ-1/4).
Using Lemma 2, together With the expressions of B (Eq.68) and Γ (Eqs.72,73), We can prove that
1BBT = Γ(1 - a/2 + α2p+1 ]九悄 + O (ξ-")	(75)
p	nt	v
Γ =卜2 [(1 - at)2 + a2p+1 ] + σ2 (1 + 萼)^m + O (ξ-1/2)	(76)
B (ωo - w0)(ω0 - WO)T BT = ∣ω0 - wo|2 ](1 - at)2 + a2p+1 Invm + O (ξ-"2) (77)
Using Eq.75 and Taylor expansion, the inverse (BBT) 1 is equal to
(BBT)-1 = P (1 - at)2 + a2pn+1	1 Invm + O k-3/2) ,	(78)
Substituting the three expressions above in Eq.74, and ignoring terms of lower order, we find
E ∣ω? - w012 =(1
nvm ) ∣ω - W ∣2 + nm
P )	0	0 P
・	1 + 02ρ
V2 + σ2-------∖ rn
(1 - at) + atpnt~
Substituting this expression into in Eq.65, we find the value of average test loss
2
^Ltest	σ
=^2
(79)
(80)
+hr
,2	σ2nv m 1 + αnp
w012 + FP-hnt
(81)
where we define the following expressions
ht = (1 — at)2 + aP + ɪ and	hr = (1 — ar )2 + Or P + ɪ	(82)
t nt	r nr
7.3.2	Underparameterized case (Theorem 2)
In the underparameterized case (P < nv m), under the assumption that the inverse of BTB exists,
the value of ω that minimizes Eq.66 is equal to
ω? = (Bt B)-1 Bt Y	(83)
18
Published as a conference paper at ICLR 2021
Note that the matrix B does not depend on w, zt, zv, and Ew Ezt Ezv γ = Bw0. We denote by δγ
the deviation from the average, and we have
∣ω? — w0∣2 = Tr [(BTB)-1 BTδγ δγτB (BTB)-1]
(84)
We need to average this expression in order to calculate E ∣ω? - w0∣2 in Eq.65. We start by aver-
aging δγ δγT over w, zt , zv, since B does not depend on those variables. Note that w, zt , zv are
independent on each other and across tasks. As in previous section, we denote by Γ the result of this
operation, given by Eq.s72, 73. Finally, we need to average over the training and validation data
E ∣ω? — wo∣2 = EE Tr [(BτB)-1 BTΓB (BTB)-1]
(85)
It is hard to average this expression because it includes nonlinear functions of the data. However,
we can approximate these terms by assuming that either m or ξ (or both) is a large number, where
ξ is defined by assuming that both nt and nv are of order O(ξ). Using Lemma 3, together with the
expression of B (Eq.68), and noting that each factor in Eq.85 has a sum over m independent terms,
we can prove that
----BTB =(1 — 2αt + 02μ2) Ip + O ((mξ) 1/2)
nv m
(86)
The expression for μ2 is given in Eq.32. Using this result and a Taylor expansion, the inverse is
equal to
nvm (BTB) 1 =(1 - 2αt + α2μ2) 1 Ip + O ((mξ)-1∕2)
Similarly, the term BTΓB is equal to its average plus a term of smaller order
ɪBtΓB = ɪE (BTΓB) + O ((mξ)T2)
nvm	nvm
(87)
(88)
We substitute these expressions in Eq.85 and neglect lower orders. Here we show how to cal-
culate explicitly the expectation of BT ΓB. For ease of notation, we define the matrix At(i) =
I - αntXMi)TXt(i). Using the expressions of B (Eq.68) and Γ (Eqs.72,73), the expression for
BTΓB is given by
m
BTΓB = σ2 X AtCi)T XVCi)T XVCi)AtCi)
(i)TXv(i)TXv(i)At(i)2 +
i=1
22m
+ _t___'χ Atl(i')τ X VCi)T X Vei)X Mi)T Xt⑶ X VCi)T X VCi)AtCi)
n2 y
t i=1
We use Eqs.31, 32 to calculate the average of the first term in Eq.89
m
(89)
E E X At(i)T Xv(i)T Xv(i)At(i)
XtXv i=1
nvm(1 — 2αt + α2μ2) Ip
(90)
We use Eqs.31, 32, 33, 41, 36, 37, 38, 39 to calculate the average of the second term
E E	Xm	AtCi)T XVCi)T XVCi)AtCi)2 = E	Xm	hnV (nV	+ 1)AtCi)4	+nVAtCi)2TrAtCi)2i =
XtXv i=1	Xti=1
(91)
=mnv (nv + 1)(1 - 4at + 6a2μ2 — 4α3μ3 + α4μ4) Ip+
+ mnvP(1 — 4αt + 2a2μ2 + 4α2μι,ι — 4α3μ2,ι + α4μ2,2) Ip
Finally, we compute the average of the third term, using Eqs.31, 32, 33, 34, 41, 36, 37
m
E E X At(i)TXv(i)TXv(i)Xt(i)TXt(i)Xv(i)TXv(i)At(i) =
XtXv
i=1
m
(92)
(93)
E
Xt
X hnv (nv + 1) At(i)TXt(i)TXt(i)At(i) + nvAt(i)TAt(i)Tr Xt(i)TXt(i)i =
(94)
i=1
mnv (nv + 1) n(1 — 2αtμ2 + α2μ31 Ip + mnntp(1 — 2αtμι,ι + α2μ2,ι) Ip
(95)
19
Published as a conference paper at ICLR 2021
Putting everything together in Eq.85, and applying the trace operator, we find the following expres-
sion for the meta-parameter variance
E ∣ω? - wo|2 = p—(1 - 2at + α2μz) - < σ2(1 - 2at + α2μz) +
nv m
α2σ2
+---t— [(nv + I)(I - 2ɑtμ2 + α2μ3) + P (1 - 2αtμι,ι + α2μ2,ι)]
nt
ν2	2	3	4 4
+---(n + 1) (1 — 4at + 6at μ2 — 4αt μ3 + a μ ) +
+ p (1 - 4at + 2a2μ2 + 4a2μι,ι - 4a3μ2,1 + a4μ2,2} } + O ((mg厂3/2)	(96)
We rewrite this expression as
Elω?- wol2 =	h⅛ {σ2 [ht+at [(nv + 1)g1+pg2]
+ O ((mξ)T2)
+ νp [(nv + 1) g3 + Pg3]} +
(97)
where we defined the following expressions for gi
g1	=1 — 2a,tμ2 + atμ3	(98)
g2	11 - 2αtμι,ι + a2μ2,1	(99)
g3	=1 - 4at + 6at μ2 — 4at μ3 + at μ4	(100)
g4	=1 — 4at + 2a2μ2 + 4a2μι,ι — 4a3μ2,ι + a4μ2,2	(101)
and μi are equal to
μ2 = — (nt + P + 1)	(102)
nt
μ3 = 4 (n2 + p2 + 3ntP + 3nt + 3p + 4)	(103)
nt
μ4 = 3 (n3 + p3 + 6n2p + 6ntp2 + 6n2 + 6p2 + 17ntP + 21nt + 21p + 20)	(104)
nt
μ1,1
μ2,1
μ2,2
n1p (n2p+2nt)	(105)
n1p (n2p + ntP2 +	ntp + 4nt + 4p + 4)	(106)
—3— (n3p + nt p3 +	2n2p2 + 2n2p + 2n p2 +	8n2 +	8p2	+ 21ntp + 20nt	+ 20p + 20)
ntp	(107)
Substituting this expression back into Eq.65 returns the final expression for the average test loss,
equal to
hr	p 2 t	αt2
亦 nvm u V + nt [(nv + 1)g1 +pg2]
2
+ — [(nv + 1) g3 + pg4] } + O ((mξ)-3∕2)
(108)
7.4	Proof of Theorem 3
In this section, we release some assumption on the distributions of data and parameters. In particular,
we do not assume a specific distribution for input data vectors x and generating parameter vector
20
Published as a conference paper at ICLR 2021
w, besides that different data vectors are independent, and so are data and parameters for different
tasks. We further assume that those vectors have zero mean, and denote their covariance as
Σ = ExxT	(109)
Σw = EwwT	(110)
We will also use the following matrix, including fourth order moments
F = E (XT Σx) XXT
(111)
We do not make any assumption about the distribution of X, but we note that, if X is Gaussian, then
F = 2Σ3 + ΣTr Σ2 . We keep the assumption that the output noise is Gaussian and independent
for different data points and tasks, with variance σ2. Using the same notation as in previous sections,
we will also use the following expressions (for any p × p matrix A)
E XTX = nΣ	(112)
E Tr [∑XTXAXTX] = Tr {A [n2Σ3 + n (F - Σ3)]}	(113)
We proceed to derive the same formula under these less restrictive assumptions, in the overparam-
eterized case only, following is the same derivation of section 7.3. We further assume ω0 = 0,
w0 = 0. Again we start from the expression in Eq.24 for the test output, and we rewrite the test loss
in Eq.27 as
Ltest=E 贵IX S (w0- θ?)+zs12
(114)
We average this expression with respect to Xs , zs, noting that θ? does not depend on test data. We
further average with respect to w0 , but note that θ? depends on test parameters, so we average only
terms that do not depend on θ?. Using Eq.112, the result is
Ltest = σ2 + 2Tr(Σ∑w) + E 2θ*τΣ θ? - w0TΣ θ?	(115)
The second term in the expectation is linear in θ? and can be averaged over Xr , zr, using Eq.25 and
noting that ω? does not depend on target data. The result is
E E θ? = (I - αrΣ)ω? + αrΣw0	(116)
Xr zr
Furthermore, we show below (Eq.128) that the following average holds
EEE ω? =0	(117)
w zt zv
Combining Eqs.116, 117, we can calculate the second term in the expectation of Eq.115 and find
Ltest = σ2 + 1 Tr (Σ∑w) - arTr (Σ2∑w) + E1 θ*τΣ θ?	(118)
We start by averaging the third term of this expression over zr , w0, using Eq.25 and noting that ω?
does not depend on target data and test parameters. The result is
E E θ*τΣ θ? = Tr ∣Σ (I - αrXrTXr[ ω*ω*T (I - αrXrTXr∖] +	(119)
w0 zr	nr	nr
22	2
+ ar^Tr XrΣXrT + arTr ΣXrTXr∑wXrTXr	(120)
nr	nr
We now average over Xr, again noting that ω? does not depend on target data. Using Eqs.112, 113,
we find
EEE θ*TΣ θ? = Tr ω 3*3*τ
Xr w0 zr
+ 互Tr (∑2) + αrTr ] ∑w [∑3 + ɪ (F - Σ3)ll	(122)
nr	nr
∑(I - arΣ)2 + Or (F - Σ3) + +	(121)
21
Published as a conference paper at ICLR 2021
We can now rewrite the average test loss in Eq.118 as
Ltest = σ22 1 + αr2 Tr(∑2)] +∣ Tr h(∑w + E ω*ω*T) H [	(123)
where we define the following matrix
2
Hr = Σ(I - arΣ)2 + -r (F -
(124)
In order to average the last term, we need an expression for ω?. We note that the loss in Eq.20
is quadratic in ω, therefore the solution in Eq.22 can be found using standard linear algebra. In
particular, the loss in Eq.20 can be rewritten as
Lmeta
2n1mlγ- Bωl2
(125)
where γ is a vector of shape nvm × 1, and B is a matrix of shape nvm × p. The vector γ is a stack
of m vectors
(	XV(I) (i - atXt(I)TXt(I)) W(I) - atXV(I)Xt(I)TZt(I) + Zv(I)、
Y =	.
.
∖Xv(m) (i - at XMm)TXXm)) W(m) - at XMm)Xt(m)TZMm) + ZMm),
Similarly, the matrix B is a stack ofm matrices
(XV(I) (i - atXt(I)TXt(I))、
B =	.
.
Xv(m) Ii - atXt(m)TXt(m)∖
nt
(126)
(127)
In the overparameterized case (p > nVm), under the assumption that the inverse of BBT exists, the
value of ω that minimizes Eq.125, and that also has minimum norm, is equal to
ω? = BT (BBT) — 1 γ	(128)
Note that the matrix B does not depend on W, Zt, ZV, and Ew Ezt Ezv γ = 0, therefore Eq.117
holds. In order to finish calculating Eq.123, we need to average the following term
Tr (Hrω*ω*T) = Tr [(BBt) — 1 γγT (BBT) — 1 (BHrBT)]	(129)
where we used the cyclic property of the trace. We start by averaging γγT over W, Zt , ZV , since B
does not depend on those variables. Note that W, Zt , ZV are independent on each other and across
tasks. We denote by Γ the result of this operation, which is equal to a block diagonal matrix
∕Γ(1) 0	0
Γ = EEE γγ T = o .. o
wztzv
0	0	Γ(m)
Where matrix blocks are given by the following expression
Γ⑴=XVei)(I - -tXMi)TXMi)) Σw(I - -tXMi)TXMi)) XVci)T+
+ σ2 (Inv + a XV(i)Xt(i)TXt(i)XV(i)T)
Finally, we need to average over the training and validation data
E Tr (Hr3*3*τ) = EE Trh(BBT) — 1 Γ (BBt)-1 (BHrBτ)]
(130)
(131)
(132)
(133)
22
Published as a conference paper at ICLR 2021
These averages are hard to compute since they involve nonlinear functions of the data. However, we
can approximate these terms by assuming that p and nt are large, both of order O(ξ), where ξ is a
large number. Furthermore, we assume that Tr Σ2w is of order O ξ-1 , and that the variances of
matrix products of the rescaled inputs X/√p, UP to sixth order, are all of order O (ξ-1), in particular
Var (1 Xv(i)XVj)T) = O (ξ-1)	(134)
Var (42Xv(i)Xt(i)TXt(i)Xv(j)T) = O (ξ-1)	(135)
Var ( J3Xv(i)Xt(i)TXt(i)Xt(j)TXt(j)Xv(j)T) = O (ξ-1)	(136)
Then, using Eqs.112, 113 and the expressions ofB (Eq.127) and Γ (Eqs.130,131), we can prove that
BBT = Tr(Ht) Invm + O Q2)	(137)
Γ= I Tr (∑w Ht) + σ2 1 + Q Tr (∑2) J Invm + O (g1/2)	(138)
BHrBT = Tr(HrHt) Invm + +O (ξ1/2)	(139)
where, similar to Eq.124, we define
Ht = Σ(I - αtΣ)2 + α2 (F - Σ3)]	(140)
Note that all these terms are of order O (ξ). The inverse of BBT can be found by a Taylor expansion
(BBT)-1 = Tr(Ht)-1 Invm + O 伫3/2)	(M1)
Substituting these expressions in Eq.133, we find
E Tr
ω?ω?T ) = n m
Tr(HrHt) {Tr (ΣwHt) + σ2 [1 + α2Tr
Tr(Ht)2
(* ) } + O (ξ-3/2) (142)
Substituting this expression into in Eq.123, we find the value of average test loss
Ltest = 2 Tra H r) + σ2 [1 + α2Tr 则 +
I -Tr(HrHt) {Tr(∑wHt) + σ2 [1 + n-Tr(∑2)]}
+ 2 nv m	Trw
+ O (S-3/2)
(143)
(144)
23