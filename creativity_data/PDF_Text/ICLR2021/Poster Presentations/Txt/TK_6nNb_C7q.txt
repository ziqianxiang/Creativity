Published as a conference paper at ICLR 2021
Hierarchical Autoregressive Modeling
for Neural Video Compression
Ruihan Yang1 , Yibo Yang1 , Joseph Marino2 and Stephan Mandt1
Department of Computer Science, UC Irvine1
Computation & Neural Systems, California Institute of Technology2
{ruihan.yang,yibo.yang,mandt}@uci.edu, jmarino@caltech.edu
Ab stract
Recent work by Marino et al. (2020) showed improved performance in sequential
density estimation by combining masked autoregressive flows with hierarchical
latent variable models. We draw a connection between such autoregressive gener-
ative models and the task of lossy video compression. Specifically, we view recent
neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustsson
et al., 2020) as instances of a generalized stochastic temporal autoregressive trans-
form, and propose avenues for enhancement based on this insight. Comprehensive
evaluations on large-scale video data show improved rate-distortion performance
over both state-of-the-art neural and conventional video compression methods.
1	Introduction
Recent advances in deep generative modeling have seen a surge of applications, including learning-
based compression. Generative models have already demonstrated empirical improvements in image
compression, outperforming classical codecs (Minnen et al., 2018; Yang et al., 2020d), such as BPG
(Bellard, 2014). In contrast, the less developed area of neural video compression remains chal-
lenging due to complex temporal dependencies operating at multiple scales. Nevertheless, recent
neural video codecs have shown promising performance gains (Agustsson et al., 2020), in some
cases on par with current hand-designed, classical codecs, e.g., HEVC. Compared to hand-designed
codecs, learnable codecs are not limited to specific data modality, and offer a promising approach
for streaming specialized content, such as sports or video chats. Therefore, improving neural video
compression is vital for dealing with the ever-growing amount of video content being created.
Source compression fundamentally involves decorrelation, i.e., transforming input data into white
noise distributions that can be easily modeled and entropy-coded. Thus, improving a model’s ca-
pability to decorrelate data automatically improves its compression performance. Likewise, we can
improve the associated entropy model (i.e., the model’s prior) to capture any remaining dependen-
cies. Just as compression techniques attempt to remove structure, generative models attempt to
model structure. One family of models, autoregressive flows, maps between less structured distribu-
tions, e.g., uncorrelated noise, and more structured distributions, e.g., images or video (Dinh et al.,
2014; 2016). The inverse mapping can remove dependencies in the data, making it more amenable
for compression. Thus, a natural question to ask is how autoregressive flows can best be utilized in
compression, and if mechanisms in existing compression schemes can be interpreted as flows.
This paper draws on recent insights in hierarchical sequential latent variable models with autoregres-
sive flows (Marino et al., 2020). In particular, we identify connections between this family of models
and recent neural video codecs based on motion estimation (Lu et al., 2019; Agustsson et al., 2020).
By interpreting this technique as an instantiation of a more general autoregressive flow transform,
we propose various alternatives and improvements based on insights from generative modeling.
In more detail, our main contributions are as follows:
1.	A new framework. We interpret existing video compression methods through the more
general framework of generative modeling, variational inference, and autoregressive flows,
allowing us to readily investigate extensions and ablations. In particular, we compare fully
data-driven approaches with motion-estimation-based neural compression schemes, and
1
Published as a conference paper at ICLR 2021
consider a more expressive prior model for better entropy coding (described in the second
bullet point below). This framework also provides directions for future work.
2.	A new model. Following the predictive coding paradigm of video compression (Wiegand
et al., 2003), Scale-Space Flow (SSF)(Agustsson et al., 2020) uses motion estimation to
predict the frame being compressed, and further compresses the residual obtained by sub-
traction. Our proposed model extends the SSF model with a more flexible decoder and
prior, and improves over the state of the art in rate-distortion performance. Specifically, we
•	Incorporate a learnable scaling transform to allow for more expressive and accurate
reconstruction. Augmenting a shift transform by scale-then-shift is inspired by im-
provements from extending NICE (Dinh et al., 2014) to RealNVP (Dinh et al., 2016).
•	Introduce a structured prior over the two sets of latent variables in the generative
model of SSF, corresponding to jointly encoding the motion information and residual
information. As the two tend to be spatially correlated, encoding residual information
conditioned on motion information results in a more informed prior, and thus better
entropy model, for the residual information; this cuts down the bit-rate for the latter
that typically dominates the overall bit-rate.
3.	A new dataset. The neural video compression community currently lacks large, high-
resolution benchmark datasets. While we extensively experimented on the publicly avail-
able Vimeo-90k dataset (Xue et al., 2019), we also collected and utilized a larger dataset,
YouTube-NT1 , available through executable scripts. Since no training data was publicly
released for the previous state-of-the-art method (Agustsson et al., 2020), YouTube-NT
would be a useful resource for making and comparing further progress in this field.
2	Related Work
We divide related work into three categories: neural image compression, neural video compression,
and sequential generative models.
Neural Image Compression. Considerable progress has been made by applying neural networks
to image compression. Early works proposed by Toderici et al. (2017) and Johnston et al. (2018)
leveraged LSTMs to model spatial correlations of the pixels within an image. Theis et al. (2017) first
proposed an autoencoder architecture for image compression and used the straight-through estima-
tor (Bengio et al., 2013) for learning a discrete latent representation. The connection to probabilistic
generative models was drawn by Balle et al. (2017), Who firstly applied variational autoencoders
(VAEs) (Kingma & Welling, 2013) to image compression. In subsequent work, Balle et al. (2018)
encoded images with a two-level VAE architecture involving a scale hyper-prior, which can be fur-
ther improved by autoregressive structures (Minnen et al., 2018; Minnen & Singh, 2020) or by
optimization at encoding time (Yang et al., 2020d). Yang et al. (2020e) and Flamich et al. (2019)
demonstrated competitive image compression performance without a pre-defined quantization grid.
Neural Video Compression. Compared to image compression, video compression is a signifi-
cantly more challenging problem, as statistical redundancies exist not only within each video frame
(exploited by intra-frame compression) but also along the temporal dimension. Early works by Wu
et al. (2018); Djelouah et al. (2019) and Han et al. (2019) performed video compression by predict-
ing future frames using a recurrent neural network, whereas Chen et al. (2019) and Chen et al. (2017)
used convolutional architectures within a traditional block-based motion estimation approach. These
early approaches did not outperform the traditional H.264 codec and barely surpassed the MPEG-2
codec. Lu et al. (2019) adopted a hybrid architecture that combined a pre-trained Flownet (Dosovit-
skiy et al., 2015) and residual compression, which leads to an elaborate training scheme. Habibian
et al. (2019) and Liu et al. (2020) combined 3D convolutions for dimensionality reduction with ex-
pressive autoregressive priors for better entropy modeling at the expense of parallelism and runtime
efficiency. Our method extends a low-latency model proposed by Agustsson et al. (2020), which
allows for end-to-end training, efficient online encoding and decoding, and parallelism.
1https://github.com/privateyoung/Youtube-NT
2
Published as a conference paper at ICLR 2021
Sequential Deep Generative Models. We drew inspiration from a body of work on sequential
generative modeling. Early deep learning architectures for dynamics forecasting involved RNNs
(Chung et al., 2015). Denton & Fergus (2018) and Babaeizadeh et al. (2018) used VAE-based
stochastic models in conjunction with LSTMs to model dynamics. Li & Mandt (2018) introduced
both local and global latent variables for learning disentangled representations in videos. Other
video generation models used generative adversarial networks (GANs) (Vondrick et al., 2016; Lee
et al., 2018) or autoregressive models and normalizing flows (Rezende & Mohamed, 2015; Dinh
et al., 2014; 2016; Kingma & Dhariwal, 2018; Kingma et al., 2016; Papamakarios et al., 2017).
Recently, Marino et al. (2020) proposed to combine latent variable models with autoregressive flows
for modeling dynamics at different levels of abstraction, which inspired our models and viewpoints.
3	Video Compression through Deep Autoregressive Modeling
We identify commonalities between hierarchical autoregressive flow models (Marino et al., 2020)
and state-of-the-art neural video compression architectures (Agustsson et al., 2020), and will use
this viewpoint to propose improvements on existing models.
3.1	Background
We first review VAE-based compression schemes (Bane et al., 2017) and formulate existing low-
latency video codecs in this framework; we then review the related autoregressive flow model.
Generative Modeling and Source Compression. Given a a sequence of video frames x1:T , lossy
compression seeks a compact description of x1:T that simultaneously minimizes the description
length R and information loss D. The distortion D measures the reconstruction error caused by
encoding xi：T into a lossy representation Zi：T and subsequently decoding it back to Xi：T, while R
measures the bit rate (file size). In learned compression methods (Balle et al., 2017; Theis et al.,
2017), the above process is parameterized by flexible functions f (“encoder”) and g (“decoder”)
that map between the video and its latent representation 乞上T = f (xi：T). The goal is to minimize a
rate-distortion loss, with the tradeoff between the two controlled by a hyperparameter β > 0:
L = D(Xi：T, g(bzi：Te)) + eR(bzi：Te).
We adopt the end-to-end compression approach of Bane et al. (2017), which approximates the
rounding operations [•] (required for entropy coding) by uniform noise injection to enable gradient-
based optimization during training. With an appropriate choice of probability model p(zi：T), the
relaxed version of above R-D (rate-distortion) objective then corresponds to the VAE objective:
L = Eq(z1:T |x1:T)[- log p(xi：T |zi：T) - log p(zi：T)].	(1)
In this model, the likelihoodp(xi：T |zi：T) follows a Gaussian distribution with mean Xi：T = g(zi：T)
and diagonal covariance 万盟 I, and the approximate posterior q is chosen to be a unit-width uni-
form distribution (thus has zero differential entropy) whose mean 乞上T is predicted by an amortized
inference network f. The prior density p(zi：T) interpolates its discretized version, so that it mea-
sures the code length of discretized Zi：T after entropy-coding.
Low-Latency Sequential Compression We specialize Eq. 1 to make it suitable for low-latency
video compression, widely used in both conventional and recent neural codecs (Rippel et al., 2019;
Agustsson et al., 2020). To this end, we encode and decode individual frames xt in sequence.
Given the ground truth current frame Xt and the previously reconstructed frames X<t, the encoder
is restricted to be of the form Zt = f (xt, X<t), and similarly the decoder computes reconstruction
sequentially based on previous reconstructions and the current encoding, Xt = g(X<t, [Zt])). Ex-
isting codecs usually condition on a single reconstructed frame, substituting X<t by Xt-1 in favor
of efficiency. In the language of variational inference, the sequential encoder corresponds to a vari-
ational posterior of the form q(Zt|Xt, Z<t), i.e., filtering, and the sequential decoder corresponds to
the likelihoodp(Xt∣z≤t) = N(Xt, 2ɪθg^I); in both distributions, the probabilistic conditioning on
z<t is based on the observation that Xt-i is a deterministic function of z<t, if we identify [Zt] with
the random variable Zt and unroll the recurrence Xt = g(X<t, zt). As we show, all sequential com-
pression approaches considered in this work follow this paradigm, and the form of the reconstruction
transform X determines the lowest hierarchy of the corresponding generative process of video x.
3
Published as a conference paper at ICLR 2021
Masked Autoregressive Flow (MAF). As a final component in neural sequence modeling, we
discuss MAF (Papamakarios et al., 2017), which models the joint distribution of a sequence p(x1:T)
in terms of a simpler distribution of its underlying noise variables y1:T through the following au-
toregressive transform and its inverse:
Xt = hμ(x<t) + hσ (x<t) Θ yt; ⇔ yt = thσ(x[t<" ∙	(2)
The noise variable yt usually comes from a standard normal distribution. While the forward MAF
transforms a sequence of standard normal noises into a data sequence, the inverse flow “whitens”
the data sequence and removes temporal correlations. Due to its invertible nature, MAF allows for
exact likelihood computations, but as we will explain in Section 3.3, we will not exploit this aspect
in compression but rather draw on its expressiveness in modeling conditional likelihoods.
3.2	A General Framework for Generative Video Coding
We now describe a general framework that captures several existing low-latency neural compression
methods as specific instances and gives rise to the exploration of new models. To this end, we
combine latent variable models with autoregressive flows into a joint framework. We consider a
sequential decoding procedure of the following form:
Xt = hμ(Xt-1, Wt) + hσ (Xt-1, Wt) Θ gv (vt, wt).	(3)
Eq. 3 resembles the definition of the MAF in Eq. 2, but augments this transform with two sets of
latent variables Wt, Vt 〜p(wt, vt). Above, hμ and hσ are functions that transform the previous
reconstructed data frame X— along with Wt into a shift and scale parameter, respectively. The
function gv (vt, Wt) converts these latent variables into a noise variable that encodes residuals with
respect to the mean next-frame prediction h*(^t-ι, Wt).
This stochastic decoder model has several advantages over existing generative models for com-
pression, such as simpler flows or sequential VAEs. First, the stochastic autoregressive transform
hμ(Xt-ι, Wt) involves a latent variable Wt and is therefore more expressive than a deterministic
transform (Schmidt & Hofmann, 2018; Schmidt et al., 2019). Second, compared to MAF, the addi-
tional nonlinear transform gv (vt, Wt) enables more expressive residual noise, reducing the burden
on the entropy model. Finally, as visualized in Figure 2, the scale parameter hσ (Xt-ι, Wt) effec-
tively acts as a gating mechanism, determining how much variance is explained in terms of the
autoregressive transform and the residual noise distribution. This provides an added degree of flexi-
bility, in a similar fashion to how RealNVP improves over NICE (Dinh et al., 2014; 2016).
Our approach is inspired by Marino et al. (2020) who analyzed a restricted version of the model
in Eq. 3, aiming to hybridize autoregressive flows and sequential latent variable models for video
prediction. In contrast to Eq. 3, their model involved deterministic transforms as well as residual
noise that came from a sequential VAE.
3.3	Example Models and Extensions
Next, we will show that the general framework expressed by Eq. 3 captures a variety of state-of-the-
art neural video compression schemes and gives rise to extensions and new models.
Temporal Autoregressive Transform (TAT). The first special case among the class of models
that are captured by Eq. 3 is the autoregressive neural video compression model by Yang et al.
(2020b), which we refer to as temporal autoregressive transform (TAT). Shown in Figure 1(a), the
decoder g implements a deterministic scale-shift autoregressive transform of decoded noise yt,
Xt = g(zt, Xt-I) = hμ(Xt-l) + hσ(^t-l) Θ yt, yt = gz(zt).	(4)
The encoder f inverts the transform to decorrelate the input frame Xt into y and encodes the result
as Zt = f(Xt,Xt-ι) = fz(yt), where Vt = xt-(^xt1-I). The shift h“ and scale hσ transforms
are parameterized by neural networks, fz is a convolutional neural network (CNN), and gz is a
deconvolutional neural network (DNN) that approximately inverts fz .
The TAT decoder is a simple version of the more general stochastic autoregressive transform in Eq 3,
where hμ and hσ lack latent variables. Indeed, interpreting the probabilistic generative process of
4
Published as a conference paper at ICLR 2021
(a)
Figure 1: Model diagrams for the generative and inference procedures of the current frame xt , for
various neural video compression methods. Random variables are shown in circles, all other quan-
tities are deterministically computed; solid and dashed arrows describe computational dependency
during generation (decoding) and inference (encoding), respectively. Purple nodes correspond to
neural encoders (CNNs) and decoders (DNNs), and green nodes implement temporal autoregressive
transform. (a) TAT; (b) SSF; (c) STAT or STAT-SSF; the magenta box highlights the additional
proposed scale transform absent in SSF, and the red arrow from wt to vt highlights the proposed
(optional) structured prior. See Appendix Fig. 7 for computational diagrams of the structured prior.
x, TAT implements the model proposed by Marino et al. (2020), as the transform from y to X is
a MAF. However, the generative process corresponding to compression (reviewed in Section 3.1)
adds additional white noise to X, with X := ^ + G W 〜N(0, 2&21). Thus, the generative process
from y to x is no longer an autoregressive flow. Regardless, TAT was shown to better capture the
low-level dynamics of video frames than the autoencoder (fz, gz) alone, and the inverse transform
decorrelates raw video frames to simplify the input to the encoder fz (Yang et al., 2020b).
DVC (Lu et al., 2019) and Scale-Space Flow (SSF, Agustsson et al. (2020)). The second class
of models captured by Eq. 3 belong to the conventional video compression framework based on
predictive coding (Cutler, 1952; Wiegand et al., 2003; Sullivan et al., 2012); both models make use
of two sets of latent variables z1:T = {w1:T, v1:T} to capture different aspects of information being
compressed, where w captures estimated motion information used in warping prediction, and v
helps capture residual error not predicted by warping.
Like most classical approaches to video compression by predictive coding, the reconstruction trans-
form in the above models has the form of a prediction shifted by residual error (decoded noise), and
lacks the scaling factor hσ compared to the autoregressive transform in Eq. 3
Xt = hwarp(Xt-ι,gw (Wt)) + gv (vt, Wt).	(5)
Above, gw and gv are DNNs, ot := gw(wt) has the interpretation ofan estimated optical flow (mo-
tion) field, hwarp is the computer vision technique of warping, and the residual rt := gv (vt, Wt) =
Xt - hwarp(xt-ι, ot) represents the prediction error unaccounted for by warping. LU et al. (2019)
only makes use ofvt in the residual decoder gv, and performs simple 2D warping by bi-linear inter-
pretation; SSF (Agustsson et al., 2020) augments the optical flow (motion) field ot with an additional
scale field, and applies scale-space-warping to the progressively blurred versions of Xt-ι to allow
for uncertainty in the warping prediction. The encoding procedure in the above models compute
the variational mean parameters as Wt = fw(Xt, ^t-ι), Vt = fv(Xt - hwarp(^t-ι, gw(wt))), cor-
responding to a structured posterior q(zt|Xt, z<t) = q(Wt|Xt, z<t)q(vt|Wt, Xt, z<t). We illustrate
the above generative and inference procedures in Figure 1(b).
Proposed: models based on Stochastic Temporal Autoregressive Transform. Finally, we con-
sider the most general models as described by the stochastic autoregressive transform in Eq. 3,
shown in Figure 1(c). We study two main variants, categorized by how they implement hμ and hσ:
STAT uses DNNs for hμ and hσ as in (Yang et al., 2020b), but complements it with the latent
variable Wt that characterizes the transform. In principle, more flexible transforms should give better
compression performance, but we find the following variant more parameter efficient in practice:
STAT-SSF: a less data-driven variant of the above that still uses scale-space warping (Agustsson
5
Published as a conference paper at ICLR 2021
Previous reconstruction X — 1
by STAT-SSF-SP
SSF warping (mean) prediction
μt = hμ (Xt-1, b Wte)
Magnitude of the proposed scale Rate savings in residual encoding [Vte	Rate savings in residual encoding [Vte
parameter Gt = h。(Xt-1, I Wte)	relative to STAT-SSF (BPP=0.053)	relative to SSF (BPP=0.075)
Decoded noise y t = gv (b Vte, b W te)
Decoded residual rt = yt Θ σt
Current reconstruction Xt = μt + rt
Figure 2: Visualizing the proposed STAT-SSF-SP model on one frame of UVG video “Shake-
NDry”. Two methods in comparison, STAT-SSF (proposed) and SSF (Agustsson et al., 2020), have
comparable reconstruction quality to STAT-SSF-SP but higher bit-rate; the (BPP, PSNR) for STAT-
SSF-SP, STAT-SSF, and SSF are (0.046, 36.97), (0.053, 36.94), and (0.075, 36.97), respectively. In
this example, the warping prediction μt = hμ(^t-ι, [Wte) incurs large error around the dog's mov-
ing contour, but models the mostly static background well, with the residual 山tents [Vte taking up
an order of magnitude higher bit-rate than [W te in the three methods. The proposed scale parameter
^t gives the model extra flexibility when combining the noise ^ (decoded from ([Vt], [Wt])) with
the warping prediction μt (decoded from [W te only) to form the reconstruction Xt = μt + ^t Θ ^:
the scale σt downweights contribution from the noise yt in the foreground where it is very costly,
and reduces the residual bit-rate R( [Vt]) (and thus the overall bit-rate) compared to STAT-SSF and
SSF (with similar reconstruction quality), as illustrated in the third and fourth figures in the top row.
et al., 2020) in the shift transform, i.e., hμ(xt-ι, Wt) = hwarp(^t-ι,gw(Wt)). This can also be
seen as an extended version of the SSF model, whose shift transform hμ is preceded by a new
learned scale transform hσ .
Structured Prior (SP). Besides improving the autoregressive transform (affecting the likelihood
model for xt), one variant of our approach also improves the topmost generative hierarchy in the
form ofa more expressive latent prior p(z1:T), affecting the entropy model for compression. We ob-
serve that motion information encoded in Wt can often be informative of the residual error encoded
in vt. In other words, large residual errors Vt incurredby the mean prediction hμ(xt-ι, Wt) (e.g., the
result of warping the previous frame hμ(xt-ι)) are often spatially collocated with (unpredictable)
motion as encoded by Wt. The original SSF model’s prior factorizes as p(Wt, vt) = p(Wt)p(vt)
and does not capture such correlation. We therefore propose a structured prior by introducing condi-
tional dependence between Wt and vt, so thatp(Wt, vt) = p(Wt)p(vt|Wt). At a high level, this can
be implemented by introducing a new neural network that maps Wt to parameters of a parametric
distribution of p(vt |Wt) (e.g., mean and variance of a diagonal Gaussian distribution). This results
in variants of the above models, STAT-SP and STAT-SSF-SP, where the structured prior is applied
on top of the proposed STAT and STAT-SSF models.
4 Experiments
In this section, we train our models both on the existing dataset and our new YouTube-NT dataset.
Our model also improves over state-of-the-art neural and classical video compression methods
when evaluated on several publicly available benchmark datasets. Lower-level modules and training
scheme for our models largely follow Agustsson et al. (2020); we provide detailed model diagrams
and schematic implementation, including the proposed scaling transform and structured prior, in
Appendix A.4. We also implement a more computationally efficient version of scale-space warping
(Agustsson et al., 2020) based on Gaussian pyramid and interpolation (instead of naive Gaussian
blurring of Agustsson et al. (2020)); pseudocode is available in Appendix A.3.
4.1	Training Datasets
Vimeo-90k (Xue et al., 2019) consists of 90,000 clips of7 frames at 448x256 resolution collected
from vimeo.com, which has been used in previous works (Lu et al., 2019; Yang et al., 2020a;
Liu et al., 2020). While other publicly-available video datasets exist, they typically have lower
6
Published as a conference paper at ICLR 2021
resolution and/or specialized content. e.g., Kinetics (Carreira & Zisserman, 2017) only contains
human action videos, and previous methods that trained on Kinetics (Wu et al., 2018; Habibian
et al., 2019; Golinski et al., 2020) generally report worse rate-distortion performance on diverse
benchmarks (such as UVG, to be discussed below), compared to Agustsson et al. (2020) who trained
on a significantly larger and higher-resolution dataset collected from youtube.com.
YouTube-NT. This is our new dataset. We collected 8,000 nature videos and movie/video-game
trailers from youtube.com and processed them into 300k high-resolution (720p) clips, which we
refer to as YouTube-NT. We release YouTube-NT in the form of customizable scripts to facilitate
future compression research. Table 1 compares the current version of YouTube-NT with Vimeo-90k
(Xue et al., 2019) and with Google’s closed-access training dataset (Agustsson et al., 2020). Figure
5b shows the evaluation performance of the SSF model architecture after training on each dataset.
Table 1: Overview of Training Datasets.
Dataset name	Clip length	Resolution	# of clips	# of videos	Public	Configurable
Vimeo-90k	7 frames	448x256	90,000	5,000	✓	X
YOuTube-NT (ours)	6-10 frames	1280x720	300,000	-8000-	-✓-	✓
Agustsson 2020 et al.	60 frames	1280x720	700,000	700,000	X	X
4.2	Training and Evaluation Procedures
Training. All models are trained on three consecutive frames and batchsize 8, which are randomly
selected from each clip, then randomly cropped to 256x256. We trained on MSE loss, following
similar procedure to Agustsson et al. (2020) (see Appendix A.2 for details).
Evaluation. We evaluate compression performance on the widely used UVG (Mercat et al., 2020)
and MCLJCV (Wang et al., 2016) datasets, both consisting of raw videos in YUV420 format. UVG
is widely used for testing the HEVC codec and contains seven 1080p videos at 120fps with smooth
and mild motions or stable camera movements. MCLJCV contains thirty 1080p videos at 30fps,
which are generally more diverse, with a higher degree of motion and a more unstable camera.
We compute the bit rate (bits-per-pixel, BPP) and the reconstruction quality (measured in PSNR)
averaged across all frames. We note that PSNR is a more challenging metric than MS-SSIM (Wang
et al., 2003) for learned codecs (Lu et al., 2019; Agustsson et al., 2020; Habibian et al., 2019;
Yang et al., 2020a;c). Since existing neural compress methods assume video input in RGB format
(24bits/pixel), we follow this convention in our evaluations for meaningful comparisons. We note
that HEVC also has special support for YUV420 (12bits/pixel), allowing it to exploit this more
compact file format and effectively halve the input bitrate on our test videos (which were coded
in YUV420 by default), giving it an advantage over all neural methods. Regardless, we report the
performance of HEVC in YUV420 mode (in addition to the default RGB mode) for reference.
Table 2: Overview of compression methods and the datasets trained on (if applicable).
Model Name I Category Vimeo-90k Youtube-NT	Remark
-STAT-SSF-	Proposed	✓	✓	Proposed autoregressive transform (on top of our efficient scale-space flow implementation)
STArSSF-SP	Proposed	✓	X	Same as above (STAT-SSF) but combined with our proposed structured prior
SsF	Baseline	✓	✓	Agustsson et al. (2020) CVPR
DVc	Baseline	✓	X	LUetal.(2019) CVPR
VcII	Baseline	X	X	Wu et al. (2018) ECCV (trained on the Kinectics dataset)
DGVC	Baseline	✓	X	Han et al. (2019) NeurIPS; modified for low-latency compression setup (no future frames used)
TAT	Baseline	✓	X	Yang et al. (2020b) ICML workshop
HEVC	Baseline	N/A	N/A	State-of-the-art conventional codec with RGB 4:4:4 color space input
HEVC(YUV)	Baseline	N/A	N/A	State-of-the-art conventional codec with YUV 4:2:0 color space input
STAT	Ablation	✓	✓	Replace scale space flow in STAT-SSF with neural network
SSF-SP	Ablation	X	✓	Scale space flow model with structured prior
4.3	Baseline Analysis
We trained our models on Vimeo-90k to compare with the published results of baseline models listed
in Table 2. Figure 3a compares our proposed models (STAT-SSF, STAT-SSF-SP) with previous
state-of-the-art classical codec HEVC and neural codecs on the UVG test dataset. Our STAT-SSF-
SP model provides superior performance at bitrates ≥ 0.07 BPP, outperforming conventional HEVC
7
Published as a conference paper at ICLR 2021
MCL-JCV
0 9 8 7 6 5 4
4 3 3 3 3 3 3
(mp)dNsd
0.10	0.15	0.20	0.25	0.30	0.35	0.40	0.45
Bits/Pixel
0.05
-B- STAT-SSF-SP(Proposed)
-B- STAT-SSF(PropoSed)
一▲-- SSF(AgUstsson et al. 2020)
—t— DVC(Lu et al. 2019)
——HEVC(YUV420)
HEVC(RGB444)
-+- VCII(Wu et al. 2018)
-*- STAT(AbIation)
TAT(Yang et al. 2020b)
DGVC(Han et al. 2019)
(a)
0 9 8 7 6
4 3 3 3 3
(p)dNsd
33
0.00	0.05	0.10	0.15
-B- STAT-SSF-SP(Proposed)
-B- STAT-SSF(PropoSed)
——HEVC(YUV420)
HEVC(RGB444)
T- SSF(AgUstsson et al. 2020)
-→- STAT(Ablation)
T-- TAT(YangetaL 2020b)
DGVC(Han et al. 2019)
0.20	0.25	0.30	0.35	0.40	0.45	0.50
Bits/Pixel
(b)
Figure 3: Rate-Distortion Performance of various models and ablations. Results are evaluated on
(a) UVG and (b) MCLJCV datasets. All the learning-based models (except VCII (Wu et al., 2018))
are trained on Vimeo-90k. STAT-SSF-SP (proposed) achieves the best performance.
(a) HEVC;
BPP = 0.087,
PSNR = 38.099
(b) SSF;
BPP = 0.082,
PSNR = 37.440
(c) STAT-SSF (ours); (d) STAT-SSF-SP (ours);
BPP = 0.0768,	BPP = 0.0551,
PSNR = 38.108	PSNR = 38.097
Figure 4:	Qualitative comparisons of various methods on a frame from MCL-JCV video 30. Fig-
ures in the bottom row focus on the same image patch on top. Here, models with the proposed
scale transform (STAT-SSF and STAT-SSF-SP) outperform the ones without, yielding visually more
detailed reconstructions at lower rates; structured prior (STAT-SSF-SP) reduces the bit-rate further.
even in its favored YUV 420 mode and state-of-the-art neural method SSF (Agustsson et al., 2020),
as well as the established DVC (Lu et al., 2019) model, which leverages a more complicated model
and multi-stage training procedure. We also note that, as expected, our proposed STAT model
improves over TAT (Yang et al., 2020b), with the latter lacking stochasticity in the autoregressive
transform compared to our proposed STAT and its variants.
8
Published as a conference paper at ICLR 2021
Bits/PiXel	Bits/PiXel
(a)	(b)
Figure 5:	Ablations & Comparisons. (a) An ablation study on our proposed components. (b)
Performance of SSF (Agustsson et al., 2020) trained on different datasets. Both sets of results are
evaluated on UVG.
Figure 3a shows that the performance ranking on MCLJCV is similar to on UVG, despite
MCLJCV having more diverse and challenging (e.g., animated) content (AgUStSSon et al., 2020).
We provide qualitative results in Figure 2 and 4, offering insight into the behavior of the proposed
scaling transform and structured prior, as well as visual qualities of the top-performing methods.
4.4 Ablation Analysis
Using the baseline SSF (Agustsson et al., 2020) model, we study the performance contribution of
each of our proposed components, stochastic temporal autoregressive transform (STAT) and struc-
tured prior (SP), in isolation. We trained on YouTube-NT and evaluated on UVG. As shown in
Figure 5a, STAT improves performance to a greater degree than SP, while SP alone does not provide
noticeable improvement over vanilla SSF (however, note that when combined with STAT, SP offers
higher improvement over STAT alone, as shown by STAT-SSF-SP v.s. STAT-SSF in Figure 3a).
To quantify the effect of training data on performance, we compare the test performance (on UVG)
of the SSF model trained on Vimeo-90k (Xue et al., 2019) and YouTube-NT. We also provide the
reported results from Agustsson et al. (2020), which trained on a larger (and unreleased) dataset.
As seen from the R-D curves in Figure 5b, training on YouTube-NT improves rate-distortion per-
formance over Vimeo-90k, in many cases bridging the gap with the performance from the larger
closed-access training dataset of Agustsson et al. (2020). At a higher bitrate, the model trained on
Vimeo-90k(Xue et al., 2019) tends to have a similar performance to YouTube-NT. This is likely
because YouTube-NT currently only covers 8000 videos, limiting the diversity of the short clips.
5	Discussion
We provide a unifying perspective on sequential video compression and temporal autoregressive
flows (Marino et al., 2020), and elucidate the relationship between the two in terms of their un-
derlying generative hierarchy. From this perspective, we consider several video compression meth-
ods, particularly a state-of-the-art method Scale-Space-Flow (Agustsson et al., 2020), as sequential
variational autoencoders that implement a more general stochastic temporal autoregressive trans-
form, which allows us to naturally extend the Scale-Space-Flow model and obtain improved rate-
distortion performance on standard public benchmark datasets. Further, we provide scripts to gen-
erate a new high-resolution video dataset, YouTube-NT, which is substantially larger than current
publicly-available datasets. Together, we hope that this new perspective and dataset will drive further
progress in the nascent yet highly impactful field of learned video compression.
9
Published as a conference paper at ICLR 2021
6	Acknowledgements
We gratefully acknowledge extensive contributions from Yang Yang (Qualcomm), which were in-
dispensable to this work. This material is based upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Contract No. HR001120C0021. Any opinions, findings
and conclusions or recommendations expressed in this material are those of the author(s) and do not
necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA). Yibo
Yang acknowledges funding from the Hasso Plattner Foundation. Furthermore, this work was sup-
ported by the National Science Foundation under Grants 1928718, 2003237 and 2007719, as well
as Intel and Qualcomm.
References
Eirikur Agustsson, David Minnen, Nick Johnston, Johannes Balle, Sung Jin Hwang, and George
Toderici. Scale-space flow for end-to-end optimized video compression. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8503-8512, 2020.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.
Stochastic variational video prediction. In International Conference on Learning Representa-
tions, 2018.
Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.
In 5th International Conference on Learning Representations, ICLR 2017, 2017.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational im-
age compression with a scale hyperprior. International Conference on Learning Representations,
2018.
Fabrice Bellard. Bpg image format, 2014. URL https://bellard.org/bpg/bpg_spec.
txt.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
6299-6308, 2017.
T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma. Deepcoder: A deep neural network based
video compression. In 2017 IEEE Visual Communications and Image Processing (VCIP), pp.
1-4, 2017.
Zhibo Chen, Tianyu He, Xin Jin, and Feng Wu. Learning for video compression. IEEE Transactions
on Circuits and Systems for Video Technology, 30(2):566-576, 2019.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980-2988, 2015.
Cassius C Cutler. Differential quantization of communication signals, July 29 1952. US Patent
2,605,361.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In International
Conference on Machine Learning, pp. 1174-1183. PMLR, 2018.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
10
Published as a conference paper at ICLR 2021
A. Djelouah, J. Campos, S. Schaub-Meyer, and C. Schroers. Neural inter-frame compression for
video coding. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp.
6420-6428, 2019.
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov,
Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with
convolutional networks. In Proceedings of the IEEE international conference on computer vision,
pp. 2758-2766, 2015.
Gergely Flamich, Marton Havasi, and Jose MigUel Hernandez-Lobato. Compression without quan-
tization. In OpenReview, 2019.
Adam Golinski, Reza Pourreza, Yang Yang, Guillaume Sautiere, and Taco S Cohen. Feedback
recurrent autoencoder for video compression. arXiv preprint arXiv:2004.04342, 2020.
Amirhossein Habibian, Ties van Rozendaal, Jakub M Tomczak, and Taco S Cohen. Video compres-
sion with rate-distortion autoencoders. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 7033-7042, 2019.
Jun Han, Salvator Lombardo, Christopher Schroers, and Stephan Mandt. Deep generative video
ComPression. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019.
Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung
Jin Hwang, Joel Shor, and George Toderici. Improved lossy image compression with priming and
spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4385-4393, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.
Stochastic adversarial video prediction. arXiv preprint arXiv:1804.01523, 2018.
Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 5670-5679, Stockholmsmassan, Stockholm
Sweden, 10-15 Jul 2018. PMLR.
Haojie Liu, Han Shen, Lichao Huang, Ming Lu, Tong Chen, and Zhan Ma. Learned video compres-
sion via joint spatial-temporal correlation exploration. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 11580-11587, 2020.
Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. Dvc: An end-
to-end deep video compression framework. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 11006-11015, 2019.
Joseph Marino, Lei Chen, Jiawei He, and Stephan Mandt. Improving sequential latent variable
models with autoregressive flows. In Symposium on Advances in Approximate Bayesian Inference,
pp. 1-16, 2020.
11
Published as a conference paper at ICLR 2021
Alexandre Mercat, Marko Viitanen, and Jarno Vanne. Uvg dataset: 50/120fps 4k sequences for
video codec analysis and development. In Proceedings of the 11th ACM Multimedia Systems
Conference,pp. 297-302, 2020.
David Minnen and Saurabh Singh. Channel-wise autoregressive entropy models for learned image
compression. In 2020 IEEE International Conference on Image Processing (ICIP), pp. 3339-
3343. IEEE, 2020.
David Minnen, Johannes Balle, and George D Toderici. Joint autoregressive and hierarchical priors
for learned image compression. In Advances in Neural Information Processing Systems, pp.
10771-10780, 2018.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2338-2347, 2017.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows.
In Proceedings of the 32nd International Conference on International Conference on Machine
Learning-Volume 37, pp. 1530-1538, 2015.
Oren Rippel, Sanjay Nair, Carissa Lew, S. Branson, Alexander G. Anderson, and Lubomir D. Bour-
dev. Learned video compression. 2019 IEEE/CVF International Conference on Computer Vision
(ICCV), pp. 3453-3462, 2019.
Florian Schmidt and Thomas Hofmann. Deep state space models for unconditional word generation.
In Advances in Neural Information Processing Systems, pp. 6158-6168, 2018.
Florian Schmidt, Stephan Mandt, and Thomas Hofmann. Autoregressive text generation beyond
feedback loops. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 3391-3397, 2019.
Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high
efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video
technology, 22(12):1649-1668, 2012.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with
compressive autoencoders. International Conference on Learning Representations, 2017.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and
Michele Covell. Full resolution image compression with recurrent neural networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In Advances in neural information processing systems, pp. 613-621, 2016.
Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang,
Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. Mcl-jcv: a jnd-based h. 264/avc video
quality assessment dataset. In 2016 IEEE International Conference on Image Processing (ICIP),
pp. 1509-1513. IEEE, 2016.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Thomas Wiegand, Gary J Sullivan, Gisle Bjontegaard, and Ajay Luthra. Overview of the h. 264/avc
video coding standard. IEEE Transactions on circuits and systems for video technology, 13(7):
560-576, 2003.
Chao-Yuan Wu, Nayan Singhal, and Philipp Krahenbuhl. Video compression through image inter-
polation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 416-431,
2018.
12
Published as a conference paper at ICLR 2021
Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement
with task-oriented flow. International Journal of Computer Vision (IJCV), 127(8):1106-1125,
2019.
Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Timofte. Learning for video compression with
hierarchical quality and recurrent enhancement. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020a.
Ruihan Yang, Yibo Yang, Joseph Marino, Yang Yang, and Stephan Mandt. Deep generative video
compression with temporal autoregressive transforms. ICML 2020 Workshop on Invertible Neural
Networks, Normalizing Flows, and Explicit Likelihood Models, 2020b.
Yang Yang, GUillaUme Sautiere, J Jon Ryu, and Taco S Cohen. Feedback recurrent autoencoder. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 3347-3351. IEEE, 2020c.
Yibo Yang, Robert Bamler, and Stephan Mandt. Improving inference for neural image compression.
Advances in Neural Information Processing Systems, 33, 2020d.
Yibo Yang, Robert Bamler, and Stephan Mandt. Variational bayesian quantization. In International
Conference on Machine Learning, 2020e.
A Appendix
A.1 Command for HEVC codec
To avoid FFmpeg package taking the advantage of the input file color format (YUV420), we first
need to dump the video.yuv file to a sequence of lossless png files:
ffmpeg -i video.yuv -vsync 0 video/%d.png
Then we use the default low-latency setting in ffmpeg to compress the dumped png sequences:
ffmpeg -y -i video/%d.png -c:v libx265 -preset medium \
-x265-params bframes=0 -crf {crf} video.mkv
where crf is the parameter for quality control. The compressed video is encoded by HEVC with
RGB color space.
To get the result of HEVC (YUV420), we directly execute:
ffmpeg -pix_fmt yuv420p -s 1920x1080 -i video.yuv \
-c:v libx265 -crf {crf} -x265-params bframes=0 video.mkv
A.2 Training schedule
Training time is about four days on an NVIDIA Titan RTX. Similar to Agustsson et al. (2020), we
use the Adam optimizer (Kingma & Ba, 2015), training the models for 1,050,000 steps. The initial
learning rate of 1e-4 is decayed to 1e-5 after 900,000 steps, and we increase the crop size to 384x384
for the last 50,000 steps. All models are optimized using MSE loss.
A.3 Efficient scale-space-flow implementation
Agustsson et al. (2020) uses a simple implementation of scale-space flow by convolv-
ing the previous reconstructed frame Xt-ι with a sequence of Gaussian kernel σ2 =
{0, σ02, (2σ0)2, (4σ0)2, (8σ0)2, (16σ0)2}. However, this leads to a large kernel size when σ is large,
which can be computationally expensive. For example, a Gaussian kernel with σ2 = 256 usually
requires kernel size 97x97 to avoid artifact (usually kernel_Size = (6 * σ + 1)2). To alleviate the
problem, we leverage an efficient version of Gaussian scale-space by using Gaussian pyramid with
upsampling. In our implementation, we use σ2 = {0, σ02 , σ02 + (2σ0)2 , σ02 + (2σ0)2 + (4σ0)2 , σ02 +
13
Published as a conference paper at ICLR 2021
(2σ0)2 + (4σ0)2 + (8σ0)2,σ02 + (2σ0)2 + (4σ0)2 + (8σ0)2 + (16σ0)2}, because by using Gaussian
pyramid, we can always use Gaussian kernel with σ = σ0 to consecutively blur and downsample
the image to build a pyramid. At the final step, we only need to upsample all the downsampled
images to the original size to approximate a scale-space 3D tensor. Detailed algorithm is described
in Algorithm 1.
Algorithm 1: An efficient algorithm to build a scale-space 3D tensor
Result: ssv: Scale-space 3D tensor
Input: input input image; σ0 base scale; M scale depth;
ssv = [input];
kernel = Create_Gaussian-Kemel(σ0);
for i=0 to M-1 do
input = GaussianBlur(input, kernel);
if i == 0 then
I ssv.append(input);
else
tmp = input;
for j=0 to i-1 do
I tmp = UPSamPle2x(tmp); {step upsampling for smooth interpolation};
end
ssv.append(tmp);
end
input = DownSample2x(input);
end
return Concat(ssv)
A.4 Lower-level Architecture Diagrams
Figure 6 illustrates the low-level encoder, decoder and hyper-en/decoder modules used in our pro-
posed STAT-SSF and STAT-SSF-SP models, as well as in the baseline TAT and SSF models, based
on Agustsson et al. (2020). Figure 7 shows the encoder-decoder flowchart for wt and vt separately,
as well as their corresponding entropy models (priors), in the STAT-SSF-SP model.
Figure 6: Backbone module architectures, where “5x5/2, 128” means 5x5 convolution kernel with
stride 2; the number of filters is 128.
14
Published as a conference paper at ICLR 2021
En/Decoder for Wt En/Decoder for Vt
Priors for Wt
Priors for Vt
Figure 7: Computational flowchart for the proposed STAT-SSF-SP model. The left two subfig-
ures show the decoder and encoder flowcharts for wt and vt , respectively, with “AT” denoting
autoregressive transform. The right two subfigures show the prior distributions that are used for en-
tropy coding wt and vt, respectively, with p(wt, wth) = p(wth)p(wt|wth), and p(vt, vth|wt, wth) =
p(vth)p(vt |vth, wt , wth), with wth and vth denoting hyper latents (see (Agustsson et al., 2020) for
a description of hyper-priors); note that the priors in the SSF and STAT-SSF models (without
the proposed structured prior) correspond to the special case where the HyperDecoder for vt
does not receive wth and wt as inputs, so that the entropy model for vt is independent of wt :
p(vt,vth) =p(vth)p(vt|vth).
15