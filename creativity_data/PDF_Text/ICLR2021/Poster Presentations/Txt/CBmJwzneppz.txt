Published as a conference paper at ICLR 2021
Optimism in Reinforcement Learning with
Generalized Linear Function Approximation
Yining Wang
University of Florida
yining.wang@warrington.ufl.edu
Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu
Simon S. Du
University of Washington
ssdu@cs.washington.edu
Akshay Krishnamurthy
Microsoft Research
akshaykr@microsoft.com
Ab stract
We design a new provably efficient algorithm for episodic reinforcement learning
with generalized linear function approximation. We analyze the algorithm under
a new expressivity assumption that we call “optimistic closure,” which is strictly
weaker than assumptions from prior analyses for the linear setting. With opti-
mistic closure, we prove that our algorithm enjoys a regret bound of Oe (H √d3T)
where H is the horizon, d is the dimensionality of the state-action features andTis
the number of episodes. This is the first statistically and computationally efficient
algorithm for reinforcement learning with generalized linear functions.
1	Introduction
We study episodic reinforcement learning problems with infinitely large state spaces, where the
agent must use function approximation to generalize across states while simultaneously engaging in
strategic exploration. Such problems form the core of modern empirical/deep-RL, but relatively little
work focuses on exploration, and even fewer algorithms enjoy strong sample efficiency guarantees.
On the theoretical side, classical sample efficiency results from the early 00s focus on “tabular”
environments with small finite state spaces (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002;
Strehl et al., 2006), but as these methods scale with the number of states, they do not address prob-
lems with infinite or large state spaces. While this classical work has inspired practically effective
approaches for large state spaces (Bellemare et al., 2016; Osband et al., 2016; Tang et al., 2017),
these methods do not enjoy sample efficiency guarantees. More recent theoretical progress has
produced provably sample efficient algorithms for complex environments where function approxi-
mation is required, but these algorithms are relatively impractical (Krishnamurthy et al., 2016; Jiang
et al., 2017). In particular, these methods are computationally inefficient or rely crucially on strong
dynamics assumptions (Du et al., 2019b).
In this paper, with an eye toward practicality, we study a simple variation of Q-learning, where we
approximate the optimal Q-function with a generalized linear model. The algorithm is appealingly
simple: collect a trajectory by following the greedy policy corresponding to the current model,
perform a dynamic programming back-up to update the model, and repeat. The key difference over
traditional Q-learning-like algorithms is in the dynamic programming step. Here we ensure that the
updated model is optimistic in the sense that it always overestimates the optimal Q-function. This
optimism is essential for our guarantees.
Optimism in the face of uncertainty is a well-understood and powerful algorithmic principle in short-
horizon (e.g,. bandit) problems, as well as in tabular reinforcement learning (Azar et al., 2017; Dann
et al., 2017; Jin et al., 2018). With linear function approximation, Yang & Wang (2019) and Jin
et al. (2019) show that the optimism principle can also yield provably sample-efficient algorithms,
when the environment dynamics satisfy certain linearity properties. Their assumptions are always
satisfied in tabular problems, but are somewhat unnatural in settings where function approximation
1
Published as a conference paper at ICLR 2021
is required. Moreover as these assumptions are directly on the dynamics, it is unclear how their anal-
ysis can accommodate other forms of function approximation, including generalized linear models.
In the present paper, we replace explicit dynamics assumptions with expressivity assumptions on
the function approximator, and, by analyzing a similar algorithm to Jin et al. (2019), we show that
the optimism principle succeeds under these strictly weaker assumptions.1 More importantly, the re-
laxed assumption facilitates moving beyond linear models, and we demonstrate this by providing the
first practical and provably efficient RL algorithm with generalized linear function approximation.
The paper is organized as follows: In Section 2 we formalize our setting, introduce the optimistic
closure assumption, and discuss related assumptions in the literature. In Section 3 we study opti-
mistic closure in detail and verify that it is strictly weaker than the recently proposed Linear MDP
assumption. Our main algorithm and results are presented in Section 4, with the main proof in Sec-
tion A. We close with some final remarks and future directions in Section 5.
2	Preliminaries
We consider episodic reinforcement learning in a finite-horizon markov decision process (MDP)
with possibly infinitely large state space S, finite action space A, initial distribution μ ∈ ∆(S),
transition operator P : S × A → ∆(S), reward function R : S × A → ∆([0, 1]) and
horizon H. The agent interacts with the MDP in episodes and, in each episode, a trajectory
(si, aι, ri, s2, a2,r2,..., SH, aH, t，h) is generated where si ~ μ, for h > 1 we have Sh ~ P(∙ |
Sh-1, ah-i), rh 〜 R(Sh ah), and actions ai：H are chosen by the agent. For normalization, We
assume that PhH=i rh ∈ [0, 1] almost surely.
A (deterministic, nonstationary) policy π = (∏ι, ∙∙∙ ,∏h) consists of H mappings ∏h : S → A,
where πh(Sh) denotes the action to be taken at time point h if at state Sh ∈ S The value function
for a policy π is a collection of functions (Viπ , . . . , VHπ ) where Vhπ : S → R is the expected future
reward the policy collects if it starts in a particular state at time point h. Formally,
H
Vn(S)，E E rh | Sh = s, ah.H ~ π .
h0=h
The value for a policy π is simply Vπ，Es-* \V∏ (si)], and the optimal value is V?，max∏ Vπ,
where the maximization is over all nonstationary policies. The typical goal is to find an approxi-
mately optimal policy, and in this paper, we measure performance by the regret accumulated over T
episodes,
TH
Reg(T) , TV? - E XXrh,t .
t=i h=i
Here rh,t is the reward collected by the agent at time point h in the tth episode. We seek algorithms
with regret that is sublinear in T , which demonstrates the agent’s ability to act near-optimally over
the long run.
2.1	Q-values and function approximation
For any policy π, the state-action value function, or the Q-function is a sequence of mappings
Qπ = (Qiπ, . . . , QπH) where Qhπ : S × A → R is defined as
H
Qh(s, a)，E £ rh0 | Sh = S,ah = a,ah+i：H ~ ∏ .
h0=h
The optimal Q-function is Q?h , Qπh? where π? , argmaxπ Vπ is the optimal policy.
In the value-based function approximation setting, we use a function class G to model Q? . In this
paper, we always take G to be a class of generalized linear models (GLMs), defined as follows: Let
d ∈ N be a dimensionality parameter and let Bd , x ∈ Rd : kxk2 ≤ 1 be the `2 ball in Rd.
1This is also mentioned as a remark in Jin et al. (2019).
2
Published as a conference paper at ICLR 2021
Definition 1. For a known feature map φ : S × A → Bd and a known link function f : [-1, 1] 7→
[-1, 1] the class of generalized linear models is G , {(s, a) 7→ f (hφ(s, a), θi) : θ ∈ Bd}.
As is standard in the literature (Filippi et al., 2010; Li et al., 2017), we assume the link function
satisfies certain regularity conditions.
Assumption 1. f (∙) is either monotonically increasing or decreasing. Furthermore, there exist
absolute constants 0 < κ < K < ∞ and M < ∞ such that κ ≤ |f0(z)| ≤ K and |f00(z)| ≤ M for
all |z| ≤ 1.
For intuition, two example link functions are the identity map f(z) = z and the logistic map f(z) =
1/(1 + e-z ) with bounded z . It is easy to verify that both of these maps satisfy Assumption 1.
2.2	Expressivity assumptions: realizability and optimistic closure
To obtain sample complexity guarantees that scale polynomially with problem parameters in the
function approximation setting, it is necessary to posit expressivity assumptions on the function
class G (Krishnamurthy et al., 2016; Du et al., 2019a). The weakest such condition is realizability,
which posits that the optimal Q function is in G , or at least well-approximated by G . Realizability
alone suffices for provably efficient algorithms in the “contextual bandits” setting where H = 1 (Li
et al., 2017; Filippi et al., 2010; Abbasi-Yadkori et al., 2011), but it does not seem to be sufficient
when H > 1. Indeed in these settings itis common to make stronger expressivity assumptions (Chen
& Jiang, 2019; Yang & Wang, 2019; Jin et al., 2019).
Following these works, our main assumption is a closure property of the Bellman update operator
Th. This operator has type Th : (S × A → R) → (S × A → R) and is defined for all s ∈ S, a ∈ A
as
Th (Q)(s, a) , E [rh + VQ (sh+1 ) | sh = s, ah = a] ,
VQ(s) , max Q(s, a).
The Bellman update operator for time point H is simply TH (Q)(s, a) , E [rH | sH = s, aH = a],
which is degenerate. To state the assumption, we must first define the enlarged function class Gup.
For a d × d matrix A, A 0 denotes that A is positive semi-definite. For a positive semi-definite
matrix A, kAkop is the matrix operator norm, which is just the largest eigenvalue, and kxkA ,
√x>Ax is the matrix Mahalanobis seminorm. For a fixed constant Γ ∈ R+ that We will set to be
polynomial in d and log(T ), define
GUp，{(s,a) → 1 ∧ f(hΦ(s, a),θi + Y kΦ(s, a)ka ： θ ∈ Bd,A 占 OJA* ≤ 1},
Here we use a∧b , min{a, b}. The class Gup contains G in addition to all possible upper confidence
boUnds that arise from solving least sqUares regression problems Using the class G. We now state
oUr main expressivity assUmption, which we call optimistic closure.
Assumption 2 (Optimistic closUre). For any 1 ≤ h < H and g ∈ Gup, we have Th (g) ∈ G.
In words, when we perform a Bellman backUp on any Upper confidence boUnd fUnction for time
point h + 1, we obtain a generalized linear fUnction at time h. While this property seems qUite
strong, we note that a similar notion is mentioned informally in Jin et al. (2019) and that related
closUre-type assUmptions are common in the literatUre (see Section 2.3 for detailed discUssion).
More importantly, we will prove in Section 3 that optimistic closUre is actUally strictly weaker than
previoUs assUmptions Used in oUr RL setting where exploration is reqUired. Before tUrning to these
discUssions, we mention two basic properties of optimistic closUre.
Fact 1 (Optimistic closUre and realizability). Optimistic closure implies that Q? ∈ G (realizability).
Proof. We will solve for Q? via dynamic programming, starting from time point H. In this case,
the Bellman Update operator is degenerate, and we start by observing that TH (g) ≡ Q?H for all g.
ConseqUently we have Q?H ∈ G . Next, indUctively we assUme that we have Q?h+1 ∈ G, which
implies that Q?h+1 ∈ GUp as we may take the same parameter θ and set A ≡ 0. Then, by the standard
Bellman fixed-point characterization, we know that Q?h = Th(Q?h+1), at which point AssUmption 2
yields that Qh ∈G.	□
3
Published as a conference paper at ICLR 2021
Fact 2 (Optimistic closure in tabular settings). If S is finite and φ(s, a) = es,a is the standard-basis
feature map, then under Assumption 1 we have optimistic closure.
Proof. We simply verify that G contains all mappings from (s, a) 7→ [0, 1], at which point the
result is immediate. To see why, observe that via Assumption 1 we know that f is invertible (it is
monotonic with derivative bounded from above and below). Then, note that any function (s, a) 7→
[0,1] can be written as a vector V ∈ [0,1]lSl×lAl. For such a vector v, if We define θs,a，f T(Vs,a)
we have that f (hes,a, θi) = vs,a. Hence G contains all functions, so we trivially have optimistic
closure.	□
2.3	Related work
The majority of the theoretical results for reinforcement learning focus on the tabular setting where
the state space is finite and sample complexities scaling polynomially with |S | are tolerable (Kearns
& Singh, 2002; Brafman & Tennenholtz, 2002; Strehl et al., 2006). Indeed, by now there are a num-
ber of algorithms that achieve strong guarantees in this setting (Dann et al., 2017; Azar et al., 2017;
Jin et al., 2018; Simchowitz & Jamieson, 2019). Via Fact 2, our results apply to this setting, and in-
deed our algorithm can be viewed as a generalization of the canonical tabular algorithm (Azar et al.,
2017; Dann et al., 2017; Simchowitz & Jamieson, 2019) to the function approximation setting.2
Turning to the function approximation setting, several other results concern function approximation
in settings where exploration is not an issue, including the infinite-data regime (Munos, 2003; Farah-
mand et al., 2010) and the “batch RL” setting where the agent does not control the data-collection
process (Munos & Szepesvari, 2008; Antos et al., 2008; Chen & Jiang, 2019). While the details
differ, all of these results require that the function class satisfy some form of (approximate) closure
with respect to the Bellman operator. As an example, one assumption is that T(g) ∈ G for all g ∈ G,
with an appropriately defined approximate variant (Chen & Jiang, 2019). These results therefore
provide motivation for our optimistic closure assumption. While optimistic closure is stronger than
the assumptions in these works, we emphasize that we are also addressing exploration, so our setting
is also significantly more challenging.
A recent line of work studies function approximation in settings where the agent must explore the
environment (Krishnamurthy et al., 2016; Jiang et al., 2017; Du et al., 2019b). The algorithms
developed here can accommodate function classes beyond generalized linear models, but they are
still relatively impractical and the more practical ones require strong dynamics assumptions (Du
et al., 2019b). In contrast, our algorithm is straightforward to implement and does not require any
explicit dynamics assumption. As such, we view these results as complementary to our own.
Most closely related to our work are the recent results of Yang & Wang (2019) and Jin et al. (2019).
Both papers study MDPs with certain linear dynamics assumptions (what they call the Linear MDP
assumption) and use linear function approximation to obtain provably efficient algorithms. Jin et al.
(2019) hint at optimistic closure as a weakening of their Linear MDP assumption and remark that
their guarantees continues to hold under this weaker assumption. One of our contributions is to
formalize this remark. Indeed, our algorithm is almost identical to theirs. However we empha-
size that optimistic closure is strictly weaker than their Linear MDP assumption, which in turn is
strictly weaker than the assumption of Yang & Wang (2019). Further, and perhaps more impor-
tantly, by avoiding explicit dynamics assumptions, we enable approximation with GLMs, which are
incompatible with the Linear MDP structure. Hence, the present paper can be seen as a significant
generalization of these recent results.
Since the initial version of this paper appeared, several other works have studied linear function
approximation in reinforcement learning. A number of papers (Cai et al., 2019; Ayoub et al., 2020;
Modi et al., 2020; Zhou et al., 2020) study an incomparable class of dynamics models that permit
linear function approximation. Others study weakenings of the Linear MDP assumptions. In partic-
ular, Agarwal et al. (2020) only require small transfer error for linear regression, which formalizes
out-of-distribution generalization and is always zero in Linear MDPs. Zanette et al. (2020a) only
require that the Bellman operator is closed with respect to linear functions, which is considerably
2The description of the algorithm looks quite different from that of Azar et al. (2017), but via an equivalence
between model-free methods with experience replay and model-based methods (Fujimoto et al., 2018), they are
indeed quite similar.
4
Published as a conference paper at ICLR 2021
weaker than our optimistic closure assumption. However, their algorithm is not computationally
efficient. Computational efficiency is addressed in Zanette et al. (2020b) in the reward-free setting
with reachability assumptions. As we do not require reachability assumptions, this latter result is
incomparable to ours. None of these results considers generalized linear models.
3	On optimistic closure
For a more detailed comparison to the recent work of Yang & Wang (2019) and Jin et al. (2019), we
define the linear MDP model studied in the latter work.
Definition 2. An MDP is said to be a linear MDP if there exist known feature map ψ : S × A → Rd,
unknown signed measures μ : S → Rd, and an unknown vector η ∈ Rd such that (1) P(s0∣s, a)=
hψ(s, a), μ(s0)〉holds forall states s, s0 and actions a, and (2) E[r | s,a] = hψ(s, a), ηi.
Linear MDPs are studied by Jin et al. (2019), Who establish a √T-type regret bound for an optimistic
algorithm. This assumption already subsumes that of Yang & Wang (2019), and related assumptions
also appear elseWhere in the literature (Bradtke & Barto, 1996; Melo & Ribeiro, 2007; Zanette
et al., 2019). In this section, We shoW that optimistic closure (Assumption 2) is strictly Weaker than
assuming the environment is a linear MDP.
Proposition 1. If an MDP is linear then Assumption 2 holds with G = {(s, a) 7→ hw, ψ(s, a)i :
w ∈ Bd} .
Proof. The result is implicit in Jin et al. (2019), and We include the proof for completeness. For any
function g, observe that oWing to the linear MDP property
Th(g)(s, a) = E r + maxg(s0,a0) | s, a = hψ(s, a), ηi + / hψ(s,a),μ(s0)i maxg(s0, a0)ds0,
a0	a0
Which is clearly a linear function in ψ(s, a). Hence for any function g, Which trivially includes the
optimistic functions, we have Th(g) ∈ G.	□
Thus the linear MDP assumption is stronger than Assumption 2. Next, We shoW that it is strictly
stronger.
Proposition 2. There exists an MDP with H = 2, d = 2, |A| = 2 and |S | = ∞ such that Assump-
tion 2 is satisfied, but the MDP is not a linear MDP.
Thus we have that optimistic closure is strictly weaker than the linear MDP assumption from Jin
et al. (2019). Thus, our results strictly generalize theirs.
Proof. Fix the link function to be f (z) = z. We first construct the MDP. Set the action space
A = {a1,a2}. We use ei to denote the ith standard basis element, and let X = (0.1∕Γ, 0.1∕Γ) be
a fixed vector where Γ appears in the construction of Gup. Recall that s1 is the first state in each
trajectory. In our example, for all a ∈ A, φ(s1, a) is sampled uniformly at random from the set
{αe1 + (1 - α)e2 : α ∈ [0, 1]}. The transition rule is deterministic and given by:
∀a ∈ A : φ(s2, a) = αx if φ(s1, a) = αe1 + (1 - α)e2 .
Moreover, for the reward function, R(sι,a) = 0 and R(s2, a) = 0.1ɑ∕Γ.
We first show that the Linear MDP property does not hold for the constructed MDP and the given
feature map φ. Let s(11) be the state with φ(s(11), a) = e2 and s(12) be the state with φ(s(12), a) = e1.
Notice that we deterministically transition from s(11) to a state s(21) with φ(s(21), a) = 0, and we de-
terministically transition from s(12) to a state s(22) with φ(s(22), a) = x, which already fixes the whole
transition operator under the linear MDP assumption. Thus, under the linear MDP assumption, we
must therefore have a randomized transition for any state s1 with φ(s1, a) = αe1 + (1 - α)e2
where α ∈ (0, 1). This contradicts the fact that our constructed MDP has deterministic transitions
everywhere, so the linear MDP cannot hold.
We next show that Assumption 2 holds. Consider an arbitrary optimistic Q estimate of the form
g(z) = min{1, z>θ + γ√z>Az} ∈ Gup. Notice that for X = (0.1∕Γ, 0.1∕Γ), we always have that
5
Published as a conference paper at ICLR 2021
Algorithm 1 The LSVI-UCB algorithm with generalized linear function approximation.
1:	Initialize estimates Qh,0 ≡ 1 for all h ≤ H and QH+ι,t ≡ 0 for all 1 ≤ t ≤ T;
2:	Set Y = CKKTpI + M + K + d2 ln((1 + K + Γ)TH) for a universal constant C;
3:	for t = 1,2, ∙∙∙ ,T do
4:	Commit to policy ∏h,t(s)，argmaXa∈/ Qh,t-ι(s,a);
5:	Use policy ∏∙,t to collect one trajectory {(sh,t, ah,t, rh,t)}H=ι;
6:	for h = H,H 一 1,…，1 do
7:	Compute xh,τ , φ(sh,τ , ah,τ ) and yh,τ , rh,τ + maxa0∈A Qh+1,t (sh+1,τ , a ) for all
τ ≤ t;
8:	Compute ridge estimate
θh,t，argmin£(yh,T - f (hxh,τ,θi))2;
kθk2≤1 τ≤t
(1)
9:
10:
11:
12:
Compute Λh,t ,
Construct Qh,t(s, a)
end for
xh,τ xh>,τ + I;
minn1,f ms a)>θh,t)+γ ∣∣φ(s, a)kΛ-1 o；
end for
χ>θ + γVχ>Aχ ≤ 1 for any θ ∈ Bd and A with IlAkoP ≤ 1. Moreover, for all s2, i.e., the second
state in the trajectory, we always have φ(s2, a) = αx for some α ∈ [0, 1]. Hence we can ignore the
first term in the minimum, and, by direct calculation, we have that when φ(s, a) = αe1 + (1 一 α)e2:
T1(g)(s, a) = ɑx>θ + γλ∕ɑ2x>Ax
=α(x>θ + γλ∕ x>Ax) = αc0.
Hence We can write T!(g) = hφ(s, a), (co, 0)i, which verifies Assumption 2.	口
4 Algorithm and main result
We now turn to presenting our algorithm and main results. We study a least-squares dynamic pro-
gramming style algorithm that we call LSVI-UCB, with pseudocode presented in Algorithm 1. The
algorithm is nearly identical to the algorithm proposed by Jin et al. (2019) with the same name.
A similar algorithmic template has also been extensively studied in the tabular setting Azar et al.
(2017); Dann et al. (2017); Simchowitz & Jamieson (2019), albeit with slightly different confidence
bounds. As our algorithm applies to all of these settings, it should be considered as a generalization.
The algorithm uses dynamic programming to maintain optimistic Q function estimates
{Qh,t}h≤H,t≤τ for each time point h and each episode t. In the tth episode, we use the previ-
ously computed estimates to define the greedy policy ∏h,t (∙)，argmaxa∈/ Q!h,t-ι (∙, a), which we
use to take actions for the episode. Then, with all of the trajectories collected so far, we perform a
dynamic programming update, where the main per-step optimization problem is (1). Starting from
time point H, we update our Q function estimates by solving constrained least squares problems
using our class of GLMs. At time point H, the covariates are {φ(sH,τ, aH,τ)}τ ≤t, and the regres-
sion targets are simply the immediate rewards {rH,τ}τ≤t. For time points h < H, the covariates are
defined similarly as {φ(sh,τ, ah,τ)}τ≤t but the regression targets are defined by inflating the learned
Q function for time point h + 1 by an optimism bonus.
In detail, the least squares problem for time point h + 1 yields a parameter θh+1,t and
we also form the second moment matrix Λh+1,t of all the covariates at time h + 1 that
we have seen so far. Using these, we define the optimistic Q function Qh+ι,t(s, a)
min 1 1,f (hΦ(s, a), θh+ι,ti) + Y kΦ(s, a)∣Λ-ι 卜
h+1,t
In our analysis, we verify that Qh+ι,t is op-
timistic in the sense that it over-estimates Q? for every (s, a). Then, the regression targets for the
least squares problem at time point h are rhj^ + maXa0∈A Qh+1,t(sh+1,τ, a0), which is a natural
stochastic approximation to the Bellman backup of Qh+ι,t. Applying this update backward from
6
Published as a conference paper at ICLR 2021
time point H to 1, we obtain the Q-function estimates that we use to define the policy for the next
episode.
The main conceptual difference between Algorithm 1 and the algorithm of Jin et al. (2019) is that
we allow non-linear function approximation with GLMs, while they consider only linear models.
On a more technical level, we use constrained least squares for our dynamic programming backup
which we find easier to analyze, while they use the ridge regularized version.
On the computational side, the algorithm is straightforward to implement, and, depending on the link
function f, it can be easily shown to run in polynomial time. For example, if f is the identity map,
then (1) is equivalent to standard least square ridge regression, which can be solved in closed form.
Moreover, we can use the Sherman-Morrison formula to amortize matrix inversions, and, by doing
so, we obtain a running time of O d2|A|HT2 . The dominant cost in this calculation is evaluating
the optimism bonus when computing the regression targets. In practice, using an epoch schedule or
incremental optimization algorithms for updating Q would yield an even faster algorithm. Of course,
with modern machine learning libraries, it is also straightforward to implement the algorithm with a
non-trivial link function f, even though (1) may be non-convex.
4.1 Main result
Our main result is a regret bound for LSVI-UCB when the link function satisfies Assumption 1 and
the function class satisfies Assumption 2.
Theorem 1. For any episodic MDP, with Assumption 1 and Assumption 2, and for any T, the
cumulative regret of Algorithm 1 is3
O (HKKTp(M + K + d2 ln(KTH)) ∙ Tdln(T∕d)) = Oe (H√d3T),
with probability 1 - 1/(TH).
The result states that LSVI-UCB enjoys √T-regret for any episodic MDP problem and any GLM,
provided that the regularity conditions are satisfied and that optimistic closure holds. As we have
mentioned, these assumptions are relatively mild, encompassing the tabular setting and prior work
on linear function approximation. Importantly, no explicit dynamics assumptions are required.
Thus, Theorem 1 is one of the most general results we are aware of for provably efficient explo-
ration with function approximation.
Nevertheless, to develop further intuition for our bound, it is worth comparing to prior results. First,
in the linear MDP setting of Jin et al. (2019), we use the identity link function so that K = κ = 1 and
M = 1, and we also are guaranteed to satisfy Assumption 2. In this case, our bound differs from that
of Jin et al. (2019) only in the dependence on H, which arises due to a difference in normalization.
Our bound is essentially equivalent to theirs and can therefore be seen as a strict generalization.
To capture the tabular setting, we use the standard basis featurization as in Fact 2 and the identity
link function, which gives d = |S||A|, K = κ = 1, and M = 1. Thus, we obtain the following
corollary:
Corollary 2. For MDPs with finite state and action spaces, using feature map φ(s, a) , es,a ∈
RlSl×lAl ,for
any T, the cumulative regret of Algorithm 1 is Oe (Hp∣S∣3∣A∣3τ), with probability
1 - 1∕(TH).
Note that this bound is polynomially worse than the near-optimal O(HJSAT + H2S2Alog(T))
bound of Azar et al. (2017). However, a refined analysis specialized to the tabular setting can
be shown to obtain a better regret bound of O (Hp∣S∣2∣A∣2T). Of course, our algorithm and
analysis address problems with infinitely large state spaces and other settings that are significantly
more complex than tabular MDPs, which we believe is more important than recovering the optimal
guarantee for tabular MDPs.
3We use Oe (∙) to suppress factors of M,K,κ, Γ and any logarithmic dependencies on the arguments.
7
Published as a conference paper at ICLR 2021
5 Discussion
This paper presents a Provably efficient reinforcement learning algorithm that approximates the Q?
function with a generalized linear model. We prove that the algorithm obtains O(HVdiT) regret
under mild regularity conditions and a new expressivity condition that we call optimistic closure.
These assumptions generalize both the tabular setting, which is classical, and the linear MDP setting
studied in recent work. Further they represent the first statistically and computationally efficient al-
gorithms for reinforcement learning with generalized linear function approximation, without explicit
dynamics assumptions.
We close with some open problems. First, using the fact that Corollary 3 applies beyond GLMs, can
we develop algorithms that can employ general function classes? While such algorithms do exist for
the contextual bandit setting (Foster et al., 2018), it seems quite difficult to generalize this analysis to
multi-step reinforcement learning. More importantly, while optimistic closure is weaker than some
prior assumptions (and incomparable to others), it is still quite strong, and stronger than what is
required for the batch RL setting. An important direction is to investigate weaker assumptions that
enable provably efficient reinforcement learning with function approximation. We look forward to
studying these questions in future work.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, 2011.
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and
application to sparse stochastic bandits. In International Conference on Artificial Intelligence and
Statistics, 2012.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
2008.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin F Yang. Model-based reinforce-
ment learning with value-targeted regression. arXiv preprint arXiv:2006.01107, 2020.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, 2017.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, 2016.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine Learning, 1996.
Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for
near-optimal reinforcement learning. The Journal of Machine Learning Research, 2002.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy opti-
mization. arXiv preprint arXiv:1912.05830, 2019.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, 2019.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC
bounds for episodic reinforcement learning. In Advances in Neural Information Processing Sys-
tems, 2017.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient
for sample efficient reinforcement learning? arXiv:1910.03016, 2019a.
8
Published as a conference paper at ICLR 2021
Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Lang-
ford. Provably efficient RL with rich observations via latent state decoding. In International
Conference on Machine Learning, 2019b.
Amir-massoud Farahmand, Csaba Szepesvari, and Remi Munos. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, 2010.
Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, 2010.
Dylan J Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert E Schapire. Practical
contextual bandits with regression oracles. In International Conference on Machine Learning,
2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv:1812.02900, 2018.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low Bellman rank are PAC-learnable. In International Conference
on Machine Learning, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. arXiv:1907.05388, 2019.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-
chine learning, 2002.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, 2016.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-
tual bandits. In International Conference on Machine Learning, 2017.
Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In Confer-
ence on Learning Theory, 2007.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pp. 2010-2020, 2020.
Remi Munos. Error bounds for approximate policy iteration. In International Conference on Ma-
chine Learning, 2003.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. The Journal of
Machine Learning Research, 2008.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances in neural information processing systems, 2016.
Victor H Pena, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and
Statistical Applications. Springer Science & Business Media, 2008.
Max Simchowitz and Kevin Jamieson. Non-asymptotic gap-dependent regret bounds for tabular
mdps. arXiv:1905.03814, 2019.
Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC
model-free reinforcement learning. In International Conference on Machine Learning, 2006.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-
man, Filip DeTurck, and Pieter Abbeel. #Exploration: A study of count-based exploration for
deep reinforcement learning. In Advances in Neural Information Processing Systems, 2017.
9
Published as a conference paper at ICLR 2021
Lin F Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and
regret bound. arXiv:1905.10389, 2019.
Andrea Zanette, David Brandfonbrener, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret
bounds for randomized least-squares value iteration. arXiv preprint arXiv:1911.00567, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020a.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient
reward-agnostic navigation with linear value iteration. arXiv preprint arXiv:2008.07737, 2020b.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for dis-
counted mdps with feature mapping. arXiv preprint arXiv:2006.13165, 2020.
A Proof of Theorem 1
We now provide the proof of Theorem 1, deferring some technical details to later sections in this
appendix. The proof has three main components: a regret decomposition for optimistic Q learning,
a deviation analysis for least squares with GLMs to ensure optimism, and a potential argument to
obtain the final regret bound.
Regret decomposition. The first step of the proof is a regret decomposition that applies generi-
cally to optimistic algorithms.4 The lemma demonstrates concisely the value of optimism in rein-
forcement learning, and is the primary technical motivation for our interest in optimistic algorithms.
We state the lemma more generally, which requires some additional notation. Fix round t and
let {(Qh,t-ι}h≤H denote the current estimated Q functions. The precondition is that Qh,t-ι is
optimistic and has controlled overestimation. Precisely, we assume that there exists a function
cnfh,t-1 : S × A → R+ such that
Qh(s,a) ≤ Qh,t-1(s,a)	⑵
Qh,t-1 (s, a) ≤ Th(Qh+1,t-1 )(s, a) + cnf h,t-1 (s, a)	(3)
We will verify that our estimates Qh,. satisfy these properties subsequently. Before doing so, We
state the regret decomposition lemma and an immediate corollary.
Lemma 1. Fix episode t and let Ft-1 be the filtration of {(sh,τ, ah,τ , rh,τ )}τ<t. Assume that
Q h,t-ι satisfies (3) for some function Cnf h,t-ι. Then, if ∏t = argmax。*/ Qjh,t-ι (∙, a) is deployed
we have
HH
V?-E X rh,t | Ft-1 ≤ζt+Xcnfh,t-1(sh,t,ah,t),
h=1	h=1
where E [Zt | Ft-1] = 0 and |Zt| ≤ 2H almost surely.
Corollary 3. Assume thatfor all t, Qh,t-ι satisfies (3) and that ∏t is the greedy policy with respect
to Qh,t-ι. Then with probability at least 1 一 δ, we have
T H
Reg(T) ≤ XX
Cnf h,t-i(sh,t,ah,t)+ O(H PT log(1∕δ)).
t=1 h=1
Proof of Lemma 1. Observe that
V? = E[Q?(sι,∏*(sι))] ≤ E [Qι,t-ι(sι,π*(sι))]
≤ E [Q1,t-1 (sι,∏t(sι))]
≤ E[cnf ι,t-ι(sι,∏t(sι))] + E [T1(<Q2,t-1)(s1, ∏t(sι))]
= E [Cnf1,t-1(s1,πt(s1))] +E [r1 | s1, a1 = πt(s1)]
+ Es2~∏t [Q2,t-1(s2,∏t(s2))]
4Related results appear elsewhere in the literature focusing on the tabular setting, see e.g., Simchowitz &
Jamieson (2019).
10
Published as a conference paper at ICLR 2021
Throughout this calculation, si 〜μ. The first step here is by definition, the second uses the opti-
mism property for Q 1,t-1. The third uses that ∏t is the greedy policy with respect to Q 1,t-1 while
the fourth uses the upper bound on Q1,t-1. Finally We use the definition of the Bellman operator
and the fact that πt is the greedy policy yet again. Comparing this upper bound with the expected
reward collected by πt we observe that r1 cancels, and we get
H
V?-E X rh,t | Ft-1
h=1
H
≤ E∏t [cnf 1,t-i(s1,∏t(s1))] + Ent Q2,t-i(s2,∏t(s2)) - Erh,t I Ft-I
h=2
At this point, notice that Q2,t-1(s2,∏t(s2)) is precisely what we alreacy upper bounded at time
point h = 1 and we are always considering the state-action distribution induced by πt . Hence,
repeating the argument for all h, we obtain
HH	H
V?-E	rh,t I Ft-1	≤	Eπt [cnfh,t-1(sh,ah)] =	cnfh,t-1(sh,t, ah,t) + ζt,
h=1	h=1	h=1
where ζt , PhH=1 ζh,t and
ζh,t , Eπt [cnf h,t-1(sh, πt(sh))] - cnfh,t-1(sh,t, ah,t),
which is easily seen to have the required properties.
□
The lemma states that if Qh,t-i is optimistic and we deploy the greedy policy ∏t, then the per-
episode regret is controlled by the overestimation error of Qh,t-ι,uptoa stochastic term that enjoys
favorable concentration properties. Crucially, the errors are accumulated on the observed trajectory,
or, stated another way, the cnf h,t-1 is evaluated on the states and actions visited during the episode.
As these states and actions will be used to update Q, we can expect that the Cnf function will
decrease on these arguments. This can yield one of two outcomes: either we will incur lower regret
in the next episode, or we will explore the environment by visiting new states and actions. In this
sense, the lemma demonstrates how optimism navigates the exploration-exploitation tradeoff in the
multi-step RL setting, analogously to the bandit setting.
Note that Lemma 1 does not assume any form for Qh,t-i and does not require Assumption 2.
In particular, they are not specialized to GLMs. In our proof, we use the GLM representation
and Assumption 2 to ensure that (3) holds and to bound the confidence sum in Corollary 3. We
believe these technical results will be useful in designing RL algorithms for general function classes,
which is a natural direction for future work.
Deviation analysis. The next step of the proof is to design the cnf function and ensure that (3)
holds, with high probability. This is the contents of the next lemma.
Lemma 2. Under Assumption 1 and Assumption 2, with probability 1 - 1/(T H), we have that
∀t, h, s, a:
f (hφ(s, a), θh,ti) - Th(Qh+i,t)(s, a)∣ ≤ Y kφ(s, a)h-;
where γ, Λh,t are defined in Algorithm 1.
A simple induction argument then verifies that (3) holds, which we summarize in the next corollary.
Corollary 4. Under Assumption 1 and Assumption 2, with probability 1 - 1/(T H), we have that (3)
holds for all t, h with cnf h,t-1 (s, a) = min{2, 2γ kφ(s, a)kΛ-1 }.
h,t-1
As the proof of Lemma 2 is rather long and technical, we defer the details to the appendix and instead
explain the high-level argument here. The proof requires an intricate deviation analysis to account
for the dependency structure in the data sequence. The intuition is that, thanks to Assumption 2 and
the fact that (Qh+ι,t ∈ Gup, We know that there exists a parameter θht such that f (hφ(s, a),4h,ti)=
Th(Qh+ι,t)(s, a). It is easy to verify that θht is the Bayes optimal predictor for the square loss
problem in (1), and so with a uniform convergence argument we can expect that θθh,t is close to θh,t,
which is our desired conclusion.
11
Published as a conference paper at ICLR 2021
There are two subtleties with this argument. First, We want to show that θθh,t and θh,t are close in a
data-dependent sense, to obtain the dependence on the Λh-,1t-Mahalanobis norm in the bound. This
can be done using vector-valued self-normalized martingale inequalities (Pena et al., 2008), as in
prior work on linear stochastic bandits (Abbasi-Yadkori et al., 2012; Filippi et al., 2010; Abbasi-
Yadkori et al., 2011).
However, the process we are considering is not a martingale, since Qh+ι,t, which determines the
regression targets yh,τ , depends on all data collected so far. Hence yh,τ is not measurable with
respect to the filtration Fτ, which prevents us from directly applying a self-normalized martingale
concentration inequality. To circumvent this issue, we use a uniform convergence argument and
introduce a deterministic covering ofGup. Each element of the cover induces a different sequence of
regression targets yh,τ, but as the covering is deterministic, we do obtain martingale structure. Then,
we show that the error term for the random Qh+ι,t that we need to bound is close to a corresponding
term for one of the covering elements, and we finish the proof with a uniform convergence argument
over all covering elements.
The corollary is then obtained by a straightforward inductive argument. Assuming Qh+ι,t dominates
Q?, it is easy to show that Qh,t also dominates Q?, and the upper bound is immediate. Combin-
ing Corollary 4 with Corollary 3, all that remains is to upper bound the confidence sum.
Potential argument. To bound the confidence sum, we use a standard potential argument that
appears in a number of works on stochastic linear bandits. We summarize the conclusion with the
following lemma, which follows directly from Lemma 11 of Abbasi-Yadkori et al. (2012).
Lemma 3. For any h ≤ H we have that
T
Xkφ(sh,t,ah,t)k2Λ-1	≤ 2dln(1+T/d).
h,t-1
t=1
Wrapping up. Equipped with the above results, we are now prepared to prove Theorem 1.
Proof of Theorem 1. Assume that Corollary 4 holds for all 1 ≤ h ≤ H and 1 ≤ t ≤ T . Apply-
ing Lemma 1 and the definition of cnfh,t-1 implied by Corollary 4, the cumulative expected regret
is at most
TH
TV?-E XXrh,t
t=1 h=1
T	TH
≤ Xζt +XXmin 2, γ kφ(sh,t, ah,t)kΛ-1
t=1	t=1 h=1	,
TH	T
≤ X Zt + XPTY2 ∙tχ kφ(sh,t ,ah,t)kΛ-ι 1
h,t-1
t=1	h=1	t=1
T	H
≤ X ζt + X PTY2 ∙ p2 ln(1 + Tld.
t=1	h=1
Here, the second step follows from the Cauchy-Schwarz inequality, and the last step is an application
OfLemma3. The first term forms a martingale, and we know that | Zt | ≤ 2H. Therefore, by Azuma's
inequality, we have that with probability at least 1 - 1/TH
T
X Zt ≤ √8TH2 ln(TH).
t=1
Finally, using the definition of γ, the final regret is upper bounded by
Reg(T) ≤ O(H√Tln(TH) + HKKT × √(m + k + d2 in((κ + γ)th)) ∙ Tdln(1 + T/d))
≤ O(H√d3T),
which proves the result.
□
12
Published as a conference paper at ICLR 2021
B Proof of Lemma 2 and Corollary 4
To facilitate our analysis we define the following important intermediate quantity:
θh,t ∈ Bd ： f(hφ(s, a),θh,ti)，E rh + moιax(Qh+ι,t(s0,a0) | s, a ∙
In words, θht is the Bayes optimal predictor for the squared loss problem at time point h in the tth
episode. Since by inspection Qh+ι,t ∈ Gup, by Assumption 2 We know that θh,t exists for all h and
t.
Lemma 4. For any θ, θ0, x ∈ Rd satisfying kθk2, kθ0k2, kxk2 ≤ 1,
κ2∣hχ,θ0 -θi∣2 ≤ |f(hχ,θ0i) - f(hχ,θi)∣2 ≤ K2kθ0 -θk2.
Proof. By the mean-value theorem, there exists θ = θ + λ(θ0 - θ) for some λ ∈ (0,1) such that
f (hx, θ0i) - f((x,θ)) = Nθ f(hχ,石i), θ0 - θ). On the other hand, by the chain rule and Assump-
_________ .,. ~.. ..,. ~..
tion 1, Vθf (hx, θi) = f0(hx, θ)) ∙ x. Hence,
KVθf (x>θ),θ0 - θi∣2 ≤ f0(hx, θi)2 ∙∖hx,θ' - θi∣2 ≤ K2 kxk2 kθ0 - θk2 ≤ K2 kθ0 - θk2;
∣hVθ f (x>θ),θ0-θi∖2 ≥ κ2∖hx,θ0-θi∖2,
which are to be demonstrated.	□
Lemma 5. For any 0 < ε ≤ 1, there exists a finite subset Vε ⊂ Gup with ln ∖Vε ∖ ≤ 6d2 ln(2(1 +
K + Γ)∕ε), such that
sup min sup ∖g(φ(s, a)) - v(φ(s, a))∖ ≤ ε.	(4)
g∈Gup v∈Vε s,a
Proof. Recall that for every g ∈ Gup, there exists θ ∈ Bd, 0 ≤ γ ≤ Γ and kAkop ≤ 1 such that
g(x) = min{1, f (hx, θi) + γ kxkA}. Let Θε ⊆ Bd, Γε ⊆ [0, Γ] and Mε ⊆ {M ∈ Sd+ ： kM kop ≤
1} be finite subsets such that for any θ, γ, A, there exist θ0 ∈ Θε, γ0 ∈ Γε, A0 ∈ Mε such that
maxnkθ-θ0k2,∖γ-γ0∖,kA-A0kopo ≤ε0,
where ε0 ∈ (0, 1) will be specified later in the proof. For the function g ∈ Gup corresponding to the
parameters θ, γ, A the function g0 corresponding to parameters θ0 , γ0 , A0 satisfies
sup ∖g(φ(s, a)) - g0(φ(s, a))∖ ≤ sup ∖g(x) - g0(x)∖
s,a	x∈Bd
≤ sup ∖f (hx, θi) - f (hx, θ0i) +γkxkA -γ0 kxkA0 ∖
x∈Bd
≤ K kθ-θ0k2 + ∖γ - γ 0∖ +Γ∖kxkA-kx∣∣A0 ∖
≤ K kθ - θ0k2 + ∖γ - Y0∖ + Γ,∖x>(A - A0)x∖
≤ Kε0 + ε0 + Γ√Z7 ≤ (1 + K + Γ)√0.
In the last step we use ε0 ≤ 1. Therefore, if we define the class Vε , {(s, a) 7→
min{1, f (hφ(s, a), θ7i) + γ7 kφ(s, a)kA0 ： θ7 ∈ Θε, γ ∈ Γε, A ∈ Mε}, we know that the cov-
ering property is satisfied with parameter (1 + K + Γ)√ε7. Setting ε0 = ε2∕(1 + K + Γ)2 we have
the desired covering property.
Finally, we upper bound ln ∖Vε∖. By definition, we have that ln ∖Vε∖ ≤ ln ∖Θε∖ + ln ∖Γε∖ + ln ∖Mε∖.
Furthermore, standard covering number bounds reveals that ln ∖Θε∖ ≤ d ln(2∕ε0),ln ∖Γε∖ ≤ ln(l∕ε0)
and ln ∖Mε∖ ≤ d2 ln(2∕ε0). Plugging in the definition of ε0 yields the result.	□
For the next lemma, let Ft-1 , σ ({(sh,τ, ah,τ, rh,τ)}τ <t) be the filtration induced by all observed
trajectories up to but not including time t. Observe that Q∙,t-ι and our policy ∏h,t are Ft-ι mea-
surable.
13
Published as a conference paper at ICLR 2021
Lemma 6 (Restatement of Lemma 2). Fix any 1 ≤ t ≤ T and 1 ≤ h ≤ H. Then as long as πt is
Ft-1 measurable, with probability 1 - 1/(T H)2 it holds that
f (hφ(s, a),θh,ti) - f (hφ(s,a),θh,ti')∖ ≤ min {2,Y kΦ(s,a)kΛ-1 } ,	∀s,a∙
for Y ≥ CKKTPI + M + K + d2 ln((1 + K + Γ)TH) and 0 < C < ∞ isa universal Constant.
Note that this is precisely Lemma 2, as θh,t is defined as f (hφ(s, a), θhr,t) = Th(Qh+ι,t)(s, a).
Proof. The upper bound of 2 is obvious, since both terms are upper bounded by 1 in absolute
value. Therefore we focus on the second term in the minimum. To simplify notation we omit the
dependence on h in the subscripts and write x「, y「for xh,τ and yh,τ. We also abbreviate θ，θh1,t
and θ，θh,t.	，	，	，
Since IBll2 ≤ 1, the optimality of θ for (1) implies that
X ff(hXT,θi) -yj ≤ Xf ((XT⑻)-yτ)2.
τ≤t	τ≤t
Decomposing the squares and re-organizing the terms, we have that
Xf((xτ,θi) - f(hxτ,θi))2≤ 2 Xξτ(f((Xτ,θi) - f((Xτ⑻)),	(5)
T≤t	T≤t
where ξτ，y「一 f (〈/丁,8〉). By the fundamental theorem of calculus, We have
,. ʌ..
f((Xτ,θi)-
hxτ,θi
f ((xτ ,θi) = /	f 0(s)ds
JhxTei
1
(xτ,θ — θi / f ((xτ, sθ+(1 — s)θi)ds .
0
'--------------{--------------}
,Dτ
Using this identity on both sides of (5), we have that
XDT 回,θ -夕i)2 ≤ 2 XξτDt(Xt,θ - θi .	(6)
T≤t	T ≤t
Note also that, by Assumption 1, DT satisfies κ2 ≤ DT2 ≤ K2 almost surely for all τ.
The difficulty in controlling (6) is that G itself is a random variable that depends on {(xt,yτ)}τ≤t. In
particular, we want that E[ξT | DT (XT, φi , FT-1] = 0 for any fixed φ, but this is not immediate as
θG depends on XT . To proceed, we eliminate this dependence with a uniform convergence argument.
Let ε ∈ (0, 1) be a covering accuracy parameter to be determined later in this proof. Let Vε be
the pointwise covering for Gup that is implied by Lemma 5. Let gε ∈ Vε be the approximation for
QG h+1,t that satisfies (4). By Assumption 2, there exists some θ] ∈ Bd such that
∀s, a : f ((φ(s, a), θ]i) = E r + maxgε(s0, a0) | s, a .
a0∈A
Now, define yT] and ξT] as
yT] , rh,T + m0axgε(sh+1,T,a0),	ξT] , yT] - f ((Xh,T, θ]i).
The right-hand side of (6) can then be upper bounded as
2 EξτD"Xτ ,θ - θi ≤ 2 ]ΓξT Dt (Xτ ,θ - θ +∆,
T≤t	T≤t
(7)
where ∣∆∣ ≤ Kt X max「≤t ∣ξT — ξτ| almost surely.
14
Published as a conference paper at ICLR 2021
Upper bounding ∆ in (7). Fix τ ≤ t. By definition, we have that
∣ξT -ξτ∣ ≤ IyT -y∕ + ∣f(hχτ,4i) -f((χτ,θ]i)∣
≤ max ∣gε(sh+1,τ, a) - Qih+1,t(sh+1,τ, a) ∣ + K ∖∖θθ - θ] ∖∖2	⑻
a∈A
≤ + K ≤ (K + 1),	(9)
where (8) holds by Lemma 4 and (9) follows from Lemma 5. In particular, the bound on ∖∖8 - θ] 卜
can be verified by expanding the definitions and noting that gε is pointwise close to Q!h+ι,t. There-
fore, we have
∆∣ ≤ (K + 1)2te.	(10)
Upper bounding (7). Note that Dτ is a function of xτ , θ, and θ. For clarity, we define
Dτ (θ, θ0) := R01 f0(hxτ, sθ+ (1 - s)θ0)i)ds. As |f00(z)| ≤ M for all |z| ≤ 1 and kxτ k2 ≤ 1,
we have that for every θ, θ0 , θ, θ0 ∈ Bd
∣Dτ(θ, θ0) - Dτ(θ, θ0)∣ ≤ / 1 ∣f0(hxτ, sθ + (1 - s)θ0i) - f(hxτ, sθ + (1 - S泗i)∣ ds
... ~.. . ~...
≤ M(kθ - θk2 + ∣∣θ - θ ∣∣2).
Hence, for any (θ, θ0) and (θ, θ0) pairs, we have for every τ that
∣ξT DxT, Dτ(θ,θ∖(θ - θ0) - Dτ (θ, θ,)(θ -少 )E∣
≤ ∣Dτ(θ,θ0)- Dτ(θ,θ0)∣ × ∣θ - θ0∣2 + ∣Dτ(θ,θ0)∣ × (kθ - θ∣2 + ∣θ0 - θ0∣2)
... ~. ... ~.
≤ M(kθ - θk2 + kθ - θ ∣∣2) X 2 + K(kθ - θk2 + kθ - θ ∣∣2)
≤ (2M + K)(kθ -θ∣2 + kθ0 - θ0∣2).
Here We are using that ∣ξτ | ≤ 1.
We are now in a position to invoke Lemma 8. Consider a fixed function gε , which defines a fixed
θ]. We will bound ∣∣PT≤t ξT] hxT, DT (θ, θ0)(θ - θ0)i∣∣ uniformly over all pairs (θ, θ0). With gε, θ]
fixed and since πt is Ft-1 measurable, we have that {xT, ξT] }T≤t are random variables satisfying
E[ξT | x1：T,ξ"τ-1] =0. For φ = (θ,θ0) we define the function q(xτ, φ) = hx, Dτ(Φ)(θ - θ0)i,
which as We have just calculated satisfies ∣q(xτ, φ) - q(xτ, φ0)∣ ≤ (2M + K) ∣∣φ - φ0∣2. For
δ0 ∈ (0, 1/2) with probability 1 - δ0 we have ∀φ = (θ, θ0) ∈ B2d:
X ξT hxτ, Dτ(φ)(θ - θ0)i ≤ (2M + K) + 2(1 + pv(φ)) P2dln(4T) + ln(1∕δ0)
∣∣T ≤t	∣∣
≤ 4maχ nM + K + √2dln(4T) + ln(1∕δ0), PV(φ)，2dln(4T) +ln(1∕δ0)} ,	(11)
where V(φ) , PT≤t hxT, DT (φ)(θ - θ0)i2. The last inequality holds because a+ b ≤ 2 max{a, b}.
Next, take a union bound over all gε ∈ Vε so (11) holds for any gε and any subsequently induced
choice of ξT with probability at least 1 - ∣Vε]δ0. In particular, this union bound implies that (11)
holds for the choice of gε that approximates Qh+1,t. Therefore, combining (6), (7), (10) with (11)
for this choice of gε, we have that with probability at least 1 - ∣Vε∣δ0
EDThxτ,θ -夕i2 ≤ 2∆ + 2 ]TξThxτ,Dτ(θ -夕)i
T≤t	∣T≤t	∣
≤ 2(K + 1)2tε + 8max [M + K + √2dln(4T)+ln(∣Vε∣∕δ0), JV(θ, θ) ∙ √2dln(4T) + ln(∣Vε∣∕δ0)
15
Published as a conference paper at ICLR 2021
Observe that the left hand side is precisely V(θ, θ). Now, set ε = 1/(2(K + 1)2T) and δ0 =
1∕(∣Vε∣T2H2) and use the bound on ln M| from Lemma 5 to get
Pdln(4T)+ln(∣Vε∣∕δ0) ≤ √2dln(4T) + 12d2 ln(2(1 + K + Γ)∕ε) +2ln(TH)
≤ √4dln(2TH) + 24d2 ln(2(1 + K + Γ)T) ≤ √28d2 ln(2(1 + K + Γ)TH)
Therefore, we obtain
V(θ, θ) ≤ 1 + 8 max {m + K + √28d2 ln(2(1 + K + Γ)TH), JV(θ, θ) ∙ √28d2 ln(2(1 + K + Γ)TH)}
≤ 16max {1 + M + K + √28d2 ln(2(1 + K + Γ)TH), JV(θ, θ) ∙ √28d2 ln(2(1 + K + Γ)TH)
Subsequently,
V(θ, θ) = XDThxτ,θ - 夕)2
τ≤t
≤ 16 max {1 + M + K + √28d2 ln(2(1 + K + Γ)TH), 448d2 ln(2(1 + K + Γ)TH)}
≤ CV2 (1+M+K+d2ln((1+K+Γ)TH)),
where 0 < CV < ∞ is a universal constant.
Next, note that Dτ2 ≥ κ2, thanks to Assumption 1. We then have
J(θ -夕)>Λh,t(θ -夕)≤ KT∖JV(θ,θ) ≤ CVκ-1√1 + M + K + d2 ln((1 + K + Γ)TH),
where Λh,t = Pτ<t xτ, xτ>. Finally, for any (s, a) pair, invoking Lemma 4 and the Cauchy-Schwarz
inequality we have
f (hφ(s, a), GY)- f(hφ(s, a),矶 ≤ K∣hφ(s, a), θ - M
≤ K J(θ - θ)>Λh,t(θ -夕)× ,φ(s,a)>Λ-,tφ(s,a)
≤ CVKκ-1 √1 + M + K + d2 ln((1 + K + Γ)TH) × kφ(s,a)h-ι
h,t
which is to be demonstrated.	口
Corollary 5 (Restatement of Corollary 4). With probability 1 - 1∕(TH), Qh,t (s, a) ≥ Qh(s, a)
holds for all h, t, s, a.
Proof. Fix 1 ≤ t ≤ T. We use induction on h to prove this corollary. For h = H +1, Qa+ι,t(∙, ∙) ≥
QH+ι(∙, ∙) clearly holds because QH+ι,t ≡ QH+ι ≡ 0. Now assume that Qh+ι,t ≥ Qh+「and let
us prove that this is also true for time step h.
Since Qh+ι,t(s0, a0) ≥ Qh+ι(s0,a0) forall s0, a； Wehavethat f (hΦ(s, a), θh,ti) ≥ f (hφ(s,a),θ?))
for all (s, a) pairs. Then, by the definition of Qh,t and Lemma 6, with probability 1 - 1∕(TH)2
it holds uniformly for all (s, a) pairs that Qh,t(s, a) ≥ f (hφ(s, a),4h,t)). Hence, with the same
probability, we have Qh,t(s, a) ≥ Qh(s, a) for all (s, a) pairs. A union bound over all t ≤ T and
h ≤ H completes the proof.	口
C Tail inequalities
Lemma 7 (Azuma,s inequality). Suppose X0,X1,X2,…，Xn form a martingale (i.e.,
E[Xk+1∣X1,…,Xk] = Xk) and satisfy |Xk — Xk-ι∣ ≤ Ck almost surely. Thenfor any e > 0,
Pr [∣Xn - X01 ≥ e] ≤ 2exp {-2P 总}.
16
Published as a conference paper at ICLR 2021
Lemma 8. Fix t, D ∈ N.	Let {ξτ , uτ }τ≤t be random variables such that
E[ξτ∣uι, ξι,…，u—1, ξτ-ι, uτ] = 0 and ∣ξτ∣ ≤ 1 almost surely. Let q : (u,φ) → R be
an arbitrary deterministicfunction satisfying ∣q(u,φ) — q(u,φ0)∣ ≤ Ckφ 一 φ0∣∣2 forall u,φ and φ0,
where φ, φ0 ∈ RD. Then for any δ ∈ (0, 1) and R > 0,
t
Pr ∀φ ∈ BD(R) : Xξτq(uτ,φ)
τ=1
where BD (R) , {x ∈ RD : kxk2 ≤ R} and Vq (φ) , Pτ≤t q2(uτ, φ).
≤ C + 2(1 +，匕(φ)) PD ln(2tR) + ln(1∕δ) ≥ 1 — δ,
Proof. Let > 0 be a small precision parameter to be specified later. Let H ⊆ BD(R) be a finite -
covering of BD(R) such that supx∈BD (R) minz∈H kx — zk2 ≤ . Using standard covering number
arguments, such a covering exists with ln |H| ≤ D ln(2R∕).
For any φ ∈ BD(R) let φ0 , argminz∈H kφ — zk2. By definition, kφ — φ0k2 ≤ . This implies
IPT =ι ξτ[q(uτ,φ) — q(uτ, φ0)]∣ ≤ Cte because ∣ξτ∣ ≤ 1 almost surely. Subsequently, for any
∆>0,
Pr ∃φ ∈ BD(R)
t
Xξτq(uτ,φ)∣∣ > Cte+∆ ≤ Pr ∃φ0 ∈ H :
τ=1	∣
t
X ξτq(uτ, φ0)∣∣ > ∆
τ=1	∣
≤ Pr
φ0∈H
t
X ξτq(uτ, φ0)∣∣ > ∆
τ=1	∣
where the last inequality holds by the union bound.
For any fixed φ0 ∈ H, h(uτ, φ0) only depends on uτ, and therefore E[ξτ | q(uτ, φ0)] = 0 for all τ.
Invoking Lemma 7 with XT，PT，<「ξτ’q(u’, φ0) and Cτ0 = ∣q(uτ0, φ0)∣, We have
Pr Xξτq(UT,φO) > δ ≤ 2eχp[———^-- = = 2expʃ	ʌ 0 ∖
∣T=1	∣	2 T≤t q2(uT, φ0)	2Vq(φ0)
Equating the right-hand side of the above inequality with δ0 and combining with the union bound
application, we have
t	t	-
Pr ∃φ ∈ Bd(R) : Xξτh(uT,φ) >Cte + /2%(φ0)ln(2∕δ0) ≤ δ0∣H∣.	(12)
∣T=1	∣
Further equating δ0 = δ∕∣H∣ and using the fact that ln |H| ≤ D ln(2R∕e), we have
一	t	-
Pr ∃φ ∈ Bd(R) : X ξTq(ur,φ) > Cte +，2DVq(φ0) ln(2R∕e) + 2%(φ0) ln(1∕δ) ≤ δ.
∣T=1	∣
Finally, as |q(u「, φ0) — q(u, Φ)∣ ≤ e, we have Vq(φ0) ≤ 2Vq(φ) + 2te2 and so
Pr ∃φ ∈ BD (R) :
t
ξT q(uT, φ)
T=1
> Cte + 2ePDtln(2R∕eδ) + 2 JVq(φ)(Dln(2R∕e) + ln(1∕δ) ≤ δ.
Setting e = 1∕t in the above inequality completes the proof.
□
17