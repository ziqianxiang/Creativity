Published as a conference paper at ICLR 2021
Towards Resolving the Implicit Bias of
Gradient Descent for Matrix Factorization:
Greedy Low-Rank Learning
Zhiyuan Li* Yuping Luo*
Princeton University
{zhiyuanli,yupingl}@cs.princeton.edu
Kaifeng Lyu*
Tsinghua University
vfleaking@gmail.com
Ab stract
Matrix factorization is a simple and natural test-bed to investigate the implicit
regularization of gradient descent. Gunasekar et al. (2017) conjectured that Gradi-
ent Flow with infinitesimal initialization converges to the solution that minimizes
the nuclear norm, but a series of recent papers argued that the language of norm
minimization is not sufficient to give a full characterization for the implicit reg-
ularization. In this work, we provide theoretical and empirical evidence that for
depth-2 matrix factorization, gradient flow with infinitesimal initialization is math-
ematically equivalent to a simple heuristic rank minimization algorithm, Greedy
Low-Rank Learning, under some reasonable assumptions. This generalizes the
rank minimization view from previous works to a much broader setting and enables
us to construct counter-examples to refute the conjecture from Gunasekar et al.
(2017). We also extend the results to the case where depth ≥ 3, and we show
that the benefit of being deeper is that the above convergence has a much weaker
dependence over initialization magnitude so that this rank minimization is more
likely to take effect for initialization with practical scale.
1	Introduction
There are usually far more learnable parameters in deep neural nets than the number of training
data, but still deep learning works well on real-world tasks. Even with explicit regularization, the
model complexity of state-of-the-art neural nets is so large that they can fit randomly labeled data
easily (Zhang et al., 2017). Towards explaining the mystery of generalization, we must understand
what kind of implicit regularization does Gradient Descent (GD) impose during training. Ideally, we
are hoping for a nice mathematical characterization of how GD constrains the set of functions that
can be expressed by a trained neural net.
As a direct analysis for deep neural nets could be quite hard, a line of works turned to study the implicit
regularization on simpler problems to get inspirations, for example, low-rank matrix factorization,
a fundamental problem in machine learning and information process. Given a set of observations
about an unknown matrix W * ∈ Rd×d of rank r* d, one needs to find a low-rank solution W
that is compatible with the given observations. Examples include matrix sensing, matrix completion,
phase retrieval, robust principal component analysis, just to name a few (see Chi et al. 2019 for a
survey). When W * is symmetric and positive semidefinite, one way to solve all these problems
is to parameterize W as W = UU> for U ∈ Rd×r and optimize L(U):= 2f (UU>), where
f (∙) is some empirical risk function depending on the observations, and r is the rank constraint. In
theory, if the rank constraint is too loose, the solutions do not have to be low-rank and we may fail
to recover W*. However, even in the case where the rank is unconstrained (i.e., r = d), GD with
small initialization can still get good performance in practice. This empirical observation reveals
that the implicit regularization of GD exists even in this simple matrix factorization problem, but
its mechanism is still on debate. Gunasekar et al. (2017) proved that Gradient Flow (GD with
infinitesimal step size, a.k.a., GF) with infinitesimal initialization finds the minimum nuclear norm
solution in a special case of matrix sensing, and further conjectured this holds in general.
Conjecture 1.1 (Gunasekar et al. 2017, informal). With sufficiently small initialization, GF converges
to the minimum nuclear norm solution of matrix sensing.
* Alphabet ordering.
1
Published as a conference paper at ICLR 2021
Subsequently, Arora et al. (2019a) challenged this view by arguing that a simple mathematical norm
may not be a sufficient language for characterizing implicit regularization. One example illustrated in
Arora et al. (2019a) is regarding matrix sensing with a single observation. They showed that GD with
small initialization enhances the growth of large singular values of the solution and attenuates that of
smaller ones. This enhancement/attenuation effect encourages low-rank, and it is further intensified
with depth in deep matrix factorization (i.e., GD optimizes f (Ui …UL) for L ≥ 2). However,
these are not captured by the nuclear norm alone. Gidel et al. (2019); Gissin et al. (2020) further
exploited this idea and showed in the special case of full-observation matrix sensing that GF learns
solutions with gradually increasing rank. Razin and Cohen (2020) showed in a simple class of matrix
completion problems that GF decreases the rank along the trajectory while any norm grows towards
infinity. More aggressively, they conjectured that the implicit regularization can be explained by rank
minimization rather than norm minimization.
Our Contributions. In this paper, we move one further step towards resolving the implicit
regularization in the matrix factorization problem. Our theoretical results show that GD performs
rank minimization via a greedy process in a broader setting. Specifically, we provide theoretical
evidence that GF with infinitesimal initialization is in general mathematically equivalent to another
algorithm called Greedy Low-Rank Learning (GLRL). At a high level, GLRL is a greedy algorithm
that performs rank-constrained optimization and relaxes the rank constraint by 1 whenever it fails
to reach a global minimizer of f (∙) with the current rank constraint. As a by-product, we refute
Conjecture 1.1 by demonstrating an counterexample (Example 5.9).
We also extend our results to deep matrix factorization Section 6, where we prove that the trajectory
of GF with infinitesimal identity initialization converges to a deep version of GLRL, at least in the
early stage of the optimization. We also use this result to confirm the intuition achieved on toy models
(Gissin et al., 2020), that benefits of depth in matrix factorization is to encourage rank minimization
even for initialization with a relatively larger scale, and thus it is more likely to happen in practice.
This shows that describing the implicit regularization using GLRL is more expressive than using the
language of norm minimization. We validate all our results with experiments in Appendix E.
2	Related Works
Norm Minimization. The view of norm minimization, or the closely related view of margin
maximization, has been explored in different settings. Besides the nuclear norm minimization for
matrix factorization (Gunasekar et al., 2017) discussed in the introduction, previous works have
also studied the norm minimization/margin maximization for linear regression (Wilson et al., 2017;
Soudry et al., 2018a;b; Nacson et al., 2019b;c; Ji and Telgarsky, 2019b), deep linear neural nets (Ji
and Telgarsky, 2019a; Gunasekar et al., 2018), homogeneous neural nets (Nacson et al., 2019a; Lyu
and Li, 2020), ultra-wide neural nets (Jacot et al., 2018; Arora et al., 2019b; Chizat and Bach, 2020).
Small Initialization and Rank Minimization. The initialization scale can greatly influence the
implicit regularization. A sufficiently large initialization can make the training dynamics fall into
the lazy training regime defined by Chizat et al. (2019) and diminish test accuracy. Using small
initialization is particularly important to bias gradient descent to low-rank solutions for matrix
factorization, as empirically observed by Gunasekar et al. (2017). Arora et al. (2019a); Gidel
et al. (2019); Gissin et al. (2020); Razin and Cohen (2020) studied how gradient flow with small
initialization encourages low-rank in simple settings, as discussed in the introduction. Li et al.
(2018) proved recovery guarantees for gradient flow solving matrix sensing under Restricted Isometry
Property (RIP), but the proof cannot be generalized easily to the case without RIP. Belabbas (2020)
made attempts to prove that gradient flow is approximately rank-1 in the very early phase of training,
but it does not exclude the possibility that the approximation error explodes later and gradient flow is
not converging to low-rank solutions. Compared to these works, the current paper studies how GF
encourages low-rank in a much broader setting.
3	Background
Notations. For two matrices A, B, we define hA, Bi := Tr(AB>) as their inner product. We use
k AkF , k Ak* and k Ak2 to denote the Frobenius norm, nuclear norm and the largest singular value of
A respectively. For a matrix A ∈ Rd×d, we use λ1(A), . . . , λd(A) to denote the eigenvalues of A
in decreasing order (if they are all reals). We define Sd as the set of symmetric d × d matrices and
Sd+ ⊆ Sd as the set of positive semidefinite (PSD) matrices. We write A B or B A iff A - B
is PSD. We use Sd+,r, Sd+,≤r to denote the set of d × d PSD matrices with rank = r, ≤ r respectively.
2
Published as a conference paper at ICLR 2021
Matrix Factorization. Matrix factorization problem asks one to optimize L(U, V):= 1 f (UV>)
among U, V ∈ Rd×r, where f : Rd×d → R is a convex function and in this paper we assume f is
C3-smooth. A notable example is matrix sensing. There is an unknown rank-r* matrix W * ∈ Rd×d
with r*《d. Given m measurements Xi,..., Xm ∈ Rd×d, one can observe yi :=hX%, W*)
through each measurement. The goal of matrix sensing is to reconstruct W* via minimizing
f (w)：= 1 Pm=I (hw, Xii - yi)2. Matrix completion is a notable special case of matrix sensing
in which every measurement has the form Xi = epie>, where {ei,…，ed} stands for the standard
basis (i.e., exactly one entry is observed through each measurement).
Note that matrix factorization in the general case can be reduced to this symmetric case: let U0 =
[V ] ∈ R2d×r, f0 ([ AB]) = 2f (B) + 1 f (C),then f (UV>) = f0(U0U0>). So in this paper we
focus on the symmetric case as in previous works (Gunasekar et al., 2017), i.e., finding a low-rank
solution for the convex optimization problem: minW 0 f(W). For this, we parameterize W as
W = UU> for U ∈ Rd×r and optimize L(U):= 2 f (UU>). We assume WLOG throughout
this paper that f (W) = f (W>); otherwise, we can set f0(W) = 2 (f (W) + f (W>)) so that
f0(W) = f0(W>) while L(U) = ɪf0(UU>) is unaffected. This assumption makes Vf(W)
symmetric for every symmetric W.
Gradient Flow. In this paper, we analyze Gradient Flow (GF) for symmetric matrix factorization,
defined as the solution of the following ODE for U(t) ∈ Rd×r:
dU = -VL(U) = -Vf (UU >)U.	(1)
Let W(t) = U (t)U (t)> ∈ Rd×d. Then the following end-to-end dynamics holds for W (t):
dW
F = -WVf(W) - Vf(W)W =: g(W).	(2)
We use φ(W0, t) to denote the matrix W(t) in (2) when W(0) = W0	0. Throughout this paper,
we assume φ(W0, t) exists for all t ∈ R, W0	0. It is easy to prove that U is a stationary point of
L(∙) (i.e., VL(U) = 0) iff W = UU> is a critical point of (2) (i.e., g(W) = 0); see Lemma C.1
for a proof. If W is a minimizer of f (∙) in S；, then W is a critical point of (2), but the reverse may
not be true, e.g., g(0) = 0, but 0 is not necessarily a minimizer.
In this paper, we particularly focus on the overparameterized case, where r = d, to understand the
implicit regularization of GF when there is no rank constraint for the matrix W.
4	Warmup Examples
First, we illustrate how GD performs greedy learning using two warmup examples.
Linearization Around the Origin. In general, for a loss function L(U) = 1 f (UU>), we can
always apply Taylor expansion f(W) ≈ f(0) + hW, Vf (0)i around the origin to approximate
it with a linear function. This motivates us to study the linear case: f(W) := f0 - hW, Qi for
some symmetric matrix Q. In this case, the matrix U follows the ODE,净=QU, which can
be understood as a continuous version of the classical power iteration method for solving the top
eigenvector. Let Q := Pd=I μ%viv> be the eigendecomposition of Q, where μ1 ≥ μ2 ≥ ∙ ∙ ∙ ≥ μd
and v1, . . . , vd are orthogonal to each other. Then we can write the solution as:
U(t) = etQU(0) = (Xd=IeμitViV>) U(0).	⑶
When μ1 > μ2, the ratio between eμ1t and eμit for i = 1 increases exponentially fast. As t → +∞,
U(t) and W(t) become approximately rank-1 as long as vi>U(0) 6= 0, i.e.,
lim e-μ1tU(t) = v1v>U(0),	lim e-2μ1tW(t) = (v>W(0)v1)v1v>.	(4)
t→∞	1	t→∞	1	1
The analysis for the simple linear case reveals that GD encourages low-rank through a process similar
to power iteration. However, f(W) is non-linear in general, and the linear approximation is close to
f(W) only if W is very small. With sufficiently small initialization, we can imagine that GD still
resembles the above power iteration in the early phase of the optimization. But what if W(t) grows
to be so large that the linear approximation is far from the actual f(W)?
Full-observation Matrix Sensing. To understand the dynamics of GD when the linearization fails,
we now consider a well-studied special case (Gissin et al., 2020): L(U) = 1 f (UU>), f(W)=
2 k W 一 W * kF for some unknown PSD matrix W *. GF in this case can be written as:
3
Published as a conference paper at ICLR 2021
币=(W* - UU>)U,	-d^- = (W* - W)W + W(W* - W).	(5)
Let W * := Pd= ι μiViV› be the eigendeComPosition of W *. Our previous analysis shows that the
dynamics is approximately 笔 = W*U in the early phase and thus encourages low-rank.
To get a sense for the later phases, we simplify the setting by specifying U(0) = √αI for a small
number α. We can write W (0) and W * as diagonal matrices W (0)= diag(α, α,…，α), W * =
diag(μι, μ2,…，μd) with respect to the basis vι,..., vd. It is easy to see that W(t) is always
a diagonal matrix, since the time derivatives of non-diagonal coordinates stay 0 during training.
Let W(t) = diag(σ1(t),σ2(t),…，σd(t)), then σ,t) satisfies the dynamical equation 奈σi(L)=
2σi(t)(μi - σi(L)), and thus σi(L) = α+(μ^-0)e-2μit. ThiS shows that every σi(L) increases from a
to μi over time. Asα → 0, every σi(L) has a sharp transition from near 0 to near μi at time roughly
(2μ- + ο(1))lοg α, which can be seen from the following limit:
lim σi ((21- +	c)lοg(1∕ɑ))	= lim -------”i ——	= ∕θ	C ∈ ( 2μi, O),
ɑ→0	2μi	α→0 α +	(μi — α)α1+2cμi [μi	C ∈ (0, +∞).
This means for every q ∈ (击,2.1 十])for i = 1,..., d - 1 (or q ∈ (击,+∞) for i = d),
lim0→ο W(qlοg(1∕a)) = diag(μι, μ2,..., μi, 0,0, ∙∙∙ , 0). Therefore, when the initialization is
sufficiently small, GF learns each component of W* one by one, according to the relative order
of eigenvalues. At a high level, this shows a greedy nature of GD: GD starts learning with simple
models; whenever it underfits, it increases the model complexity (which is rank in our case). This is
also called sequential learning or incremental learning (Gidel et al., 2019; Gissin et al., 2020).
However, it is unclear how and why this sequential learning/incremental learning can occur in general.
Through the first warmup example, we may understand why GD learns a rank-1 matrix in the early
phase, but does GD always learn solutions with rank 2, 3, 4, . . . sequentially? If true, what is the
mechanism behind this? The current paper answers the questions by providing both theoretical and
empirical evidence that the greedy learning behavior does occur in general with a similar reason as
for the first warmup example.
5	Greedy Low-Rank Learning (GLRL)
In this section, we present a trajectory-based analysis for the implicit bias of GF on matrix factoriza-
tion. Our main result is that GF with infinitesimal initialization is generically the same as that of a
simple greedy algorithm, Greedy Low-Rank Learning (GLRL, Algorithm 1). See Appendix A for a
comparison with existing greedy algorithms for rank-constrained optimization.
The GLRL algorithm consists of several phases,
numbered from 1. In phase r, GLRL increases
the rank constraint to r and optimizes L(Ur) :=
Wf(UrU>) among Ur ∈ Rd×r via GD un-
til it reaches a stationary point Ur (∞), i.e.,
▽L(Ur(∞)) = 0. At convergence, Wr-:=
Ur(∞)Ur>(∞) is a critical point of (2), and we
call it the r-th critical point of GLRL. If Wr is
further a minimizer of f (∙) in Sj, or equivalently,
λ1 (-▽f (Wr )) ≤ 0 (see Lemma C.2), GLRL
returns Wr ; otherwise GLRL enters phase r + 1.
To set the initial point of GD in phase r, GLRL
appends a small column vector δr ∈ Rd to the
resulting stationary point Ur-1(∞) from the last
phase, i.e., Ur(0) - [Ur-ι(∞) δ" ∈ Rd×r (in
the case of r = 1, Uι(0) J [δι] ∈ Rd×1). In this
way, Ur (0)Ur>(0) = Wr-1 + δrδr> is perturbed
Algorithm 1: Greedy Low-Rank Learning
parameter : step size η > 0; small > 0
r J 0, W0 J 0 ∈ Rd×d, and
U0(∞) ∈ Rd×0
while λι(-Vf (Wr)) > 0 do
rJr+1
ur J unit top eigenvector of
-▽f (Wr-l)
Ur (0) J [Ur-l(∞) √6Ur] ∈ Rd×r
for L = 0, 1, . . . do
L Ur(L + 1) J Ur(L) - ηVL(Ur(L))
_ Wr J Ur (∞)U>(∞) 0
return Wr
a In practice, we approximate the infinite time limit
by running sufficiently many steps.
away from the (r - 1)-th critical point. In GLRL,
we set δr = √ur∙, where Ur is the top eigenvector of -Vf (Wr) with unit norm ∣∣Ur k2 = 1, and
> 0 is a parameter controlling the magnitude of perturbation (preferably very small). Note that it is
guaranteed that λι(-Vf (Wr-ι)) > 0; otherwise Wr--ι is a minimizer of the convex function f (∙)
in Sd+ and GLRL exits before phase r.
4
Published as a conference paper at ICLR 2021
Trajectory of GLRL. We define the (limiting) trajectory of GLRL by taking the learning rate
η → 0. The goal is to show that the trajectory of GLRL is close to that of GF with infinitesimal
initialization. Recall that φ(W0, t) stands for the solution W(t) in (2) when W (0) = W0.
Definition 5.1 (Trajectory of GLRL). Let Wo,e := 0 be the 0th critical point of GLRL. For every
r ≥ 1, if the (r - 1)-th critical point Wr-ι,e exists and is not a minimizer of f (∙) in S；, We define
WrGe (t) := Φ(Wr-1,e + eur,eu>e, t), where ur,e is a top eigenvector of Vf (Wr-ι,e) with unit
norm, kur,e∣∣2 = 1. We define Wr,e := limt→+∞ WrGe(t) to be the r-th critical point of GLRL if
the limit exists.
Throughout this paper, we always focus on the case where the top eigenvalue of every Vf (W r-ι,e)
is unique. In this case, the trajectory of GLRL is unique for every e > 0, since the normalized top
eigenvectors can only be ±ur,e, and both of them lead to the same WrG,e (t).
5.1	The Limiting Trajectory: A General Theorem for Dynamical System
To prove the equivalence between GF and GLRL, we first introduce our high-level idea by analyzing
the behavior of a more general dynamical system around its critical point, say 0. A specific example
is (2) if we set θ to be the vectorization of W .
dθ
币=g(θ),	where g(0) = 0.	(6)
We use φ(θ0, t) to denote the value of θ(t) in the case of θ(0) = θ0. We assume that g(θ) is
C2-smooth with J(θ) being the Jacobian matrix and φ(θ0, t) exists for all θ0 and t. For ease of
presentation, in the main text we assume J(0) is diagonalizable over R and defer the same result for
the general case into Appendix G.3. Let J(0) = V DV -1 be the eigendecomposition, where V is
an invertible matrix and D = diag(μι,..., μd) is the diagonal matrix consisting of the eigenvalues
…	…	…	ʃ-	，一	一、	- ʃ- -1	，一	-	、T 一	.	一
μι ≥ μ2 ≥ ∙∙∙ ≥ μd. Let V = (vι,..., Vd) and V T = (uι,..., U d)>, then Ui, Vi are left and
right eigenvectors associated with μi and U> Vj = δj. We can rewrite the eigendecomposition as
J(0) = pd=ι "iViU>. We also assume the top eigenvalue μι is positive and unique. Note μι > 0
means the critical point θ = 0 is unstable, and in matrix factorization it means 0 is a strict saddle
point of L(∙). The key observation is that if the initialization is infinitesimal, the trajectory is almost
uniquely determined. To be more precise, we need the following definition:
Definition 5.2. For any θ0 ∈ Rd and U ∈ Rd, we say that {θα}α∈(0,1) converges to θ0 with positive
θoand lim→nf (kθθαs⅛
U > 0.
alignment with U if lim θα
α→0
A special case is that the direction of θα - θo converges, i.e., θ := limα→o 波喘0心 exists. In this
case, {θa} has positive alignment with either U or -u except for a zero-measure subset of θ. This
means any convergent sequence generically falls into either of these two categories.
The following theorem shows that if the initial point θα converges to 0 with positive align-
ment with Ui as α → 0, the trajectory starting with θa converges to a unique trajectory
ZO := φ(αVι,t + -1 log £). By symmetry, there is another unique trajectory for sequences
{θα} with positive alignment to -Ui, which is z0(t) := φ(-αVι, t + -1 log £). This is somewhat
surprising: different initial points should lead to very different trajectories, but our analysis shows
that generically there are only two limiting trajectories for infinitesimal initialization. We will soon
see how this theorem helps in our analysis for matrix factorization in Sections 5.2 and 5.3.
Theorem 5.3. Let Zα(t) := φ(αVι,t + -1 log 1) for every α > 0, then z(t) := limα→o Zα(t) exists
and is also a solution of (6), i.e., z(t) = φ(z(0), t). If δɑ converges to 0 with positive alignment with
Ui as a → 0, then ∀t ∈ R, there is a constant C > 0 such that
∣∣φ (δα,t + ⅛ log (δα,Ul) ) - Z(t)∣∣2 ≤ C∙Mαk 产,	⑺
for ^very sufficiently small a, where γ := μι 一 μ2 > 0 is the eigenvalue gap.
Proof sketch. The main idea is to linearize the dynamics near origin as we have done for the
first warmup example. For sufficiently small θ, by Taylor expansion of g(θ), the dynamics
is approximately 需 ≈ J(0)θ, which can be understood as a continuous version of power it-
eration. If the linear approximation is exact, then θ(t) = etJ(0) θ(0). For large enough t0,
5
Published as a conference paper at ICLR 2021
et0J⑼ = Pd=ι eμit0Viu> = eμιt0VιU> + O(eμ2t0). Therefore, as long as the initial point
θ(0) has a positive inner product with U1, θ(to) should be very close to eVι for some e > 0, and the
rest of the trajectory after to should be close to the trajectory starting from eVι. However, here is a
tradeoff: we should choose t0 to be large enough so that the power iteration takes effect; but if t0
is so large that the norm of θ(t0) reaches a constant scale, then the linearization fails unavoidably.
Nevertheless, if the initialization scale is sufficiently small, we show via a careful error analysis that
there is always a suitable choice of to such that θ(to) is well approximated by eVι and the difference
between θ(to +1) and φ(eVι, t) is bounded as well. We defer the details to Appendix G. □
5.2	Equivalence Between GD and GLRL: Rank-One Case
Now we establish the equivalence between GF and GLRL in the first phase. The main idea is to apply
Theorem 5.3 on (2). For this, we need the following lemma on the eigenvalues and eigenvectors.
Lemma 5.4. Let g(W):= -WVf (W) 一 Vf (W) W and J(W) be its Jacobian. Then J(0) is
symmetric and thus diagonalizable. Let -Vf (0) = Pd=ι 小逸1阳〃>力 be the eigendecomposition of
the symmetric matrix -Vf (0), where μι ≥ μ2 ≥ ∙∙∙ ≥ μ√. Then J (0) has the form:
dd
J (O)[△] = ^X ^X(Mi + μj )(△，u1[i]u1[j])u1[i]u1[j],	⑻
where J (0)[∆] stands for the resulting matrix produced by left-multiplying J(0) to the vectorization
of ∆. For every pair of 1 ≤ i ≤ j ≤ d, μ% + μj is an eigenvalue of J(0) and %团〃>,]+ 町方〃>勾
is a corresponding eigenvector. All the other eigenvalues are 0.
We simplify the notation by letting u1 := u1[1] . A direct corollary of Lemma 5.4 is that u1u1> is the
top eigenvector of J (0). According to Theorem 5.3, now there are only two types of trajectories,
which correspond to infinitesimal initialization Wα → 0 with positive alignment with u1 u1>
or -u1u1> . As the initialization must be PSD, Wα → 0 cannot have positive alignment with
-u1u1>. For the former case, Theorem 5.6 below states that, for every fixed time t, the GF solution
φ(Wα,T(Wa) +1) after shifting by a time offset T(Wɑ)：=击 log(<Wɑ, uιu>i-1) converges
to the GLRL solution W1G (t) as Wα → 0. The only assumption for this result is that 0 is not a
minimizer of f (∙) in S； (which is equivalent to λι(-Vf (0)) > 0) and -Vf (0) has an eigenvalue
gap. In the full observation case, this assumption is satisfied easily if the ground-truth matrix has a
unique top eigenvalue. The proof for Theorem 5.6 is deferred to Appendix I.1.
Assumption 5.5. μι > max{μ2,0}, where μι := λι(-Vf (0)), μ2 ：= λ2(-Vf (0)).
Theorem 5.6. Under Assumption 5.5, the following limit W1G(t) exists and is a solution of (2).
WG (t) =JimO WGe ( 2⅛ log 1 + t) = limo φ (eu1u> , 2⅛ log 1 + t).	⑼
Let {Wα} ⊆ Sd+ be PSD matrices converging to 0 with positive alignment with u1u1> as α → 0,
that is, limα→o Wα = 0 and ∃αo, q > 0 such that Wα, u1u1> ≥ q kWαkF for all α < αo. Then
∀t ∈ R, there is a constant C > 0 such that
∣Φ (Wa，击loghwa⅛y +t) - WG(t)|F ≤ CkWakFμγ+"	(10)
for ^very sufficiently small a, where Y := 2μι — (μι + μ2) = μι 一 μ2.
It is worth to note that W1G(t) has rank ≤ 1 for any t ∈ R, since every W1G,e(t) has rank ≤ 1 and
the set Sd+,≤1 is closed. This matches with the first warmup example: GD does start learning with
rank-1 solutions. Interestingly, in the case where the limit W1 := limt→+∞ W-GG(t) happens to be a
minimizer of f (∙) in S；, GLRL should exit with the rank-1 solution W1 after the first phase, and
the following theorem shows that this is also the solution found by GF.
Assumption 5.7. f(W) is locally analytic at each point.
Theorem 5.8. Under Assumptions 5.5 and 5.7, if kW1G(t)kF is bounded for all t ≥ 0, then the
limit W1 := limt→+∞ WG (t) exists. Further, if W G is a minimizer of f (∙) in S+, then for PSD
matrices {Wa} ⊆ Sd； converging to 0 with positive alignment with u1u1> as α → 0, it holds that
limα→o limt→+∞ φ(Wa,t) = W1.
6
Published as a conference paper at ICLR 2021
Assumption 5.7 is a natural assumption, since f (∙) in most cases of matrix factorization is a quadratic
or polynomial function (e.g., matrix sensing, matrix completion). In general, it is unlikely for a
gradient-based optimization process to get stuck at saddle points (Lee et al., 2017; Panageas et al.,
2019). Thus, we should expect to see in general that GLRL finds the rank-1 solution if the problem
is feasible with rank-1 matrices. This means at least for this subclass of problems, the implicit
regularization of GD is rather unrelated to norm minimization. Below is a concrete example:
Example 5.9 (Counter-example of Conjecture 1.1, Gunasekar et al. 2017). Theorem 5.8 enables us
to construct counterexamples of the implicit nuclear norm regularization conjecture in (Gunasekar
et al., 2017). The idea is to construct a loss L : Rd×d → R where every rank-1 stationary point
of L(U) attains the global minimum but none of them is minimizing the nuclear norm. Below we
give a concrete matrix completion problem that meets the above requirement. Let M be a partially
observed matrix to be recovered, where the entries in Ω = {(1, 3), (1,4), (2,3), (3,1), (3, 2), (4,1)}
are observed and the others (marked with “?”) are unobserved. The optimization problem is defined
formally by L(U) = 1 f (UU>),f(W) = 2 P(”(Wij - Mij)2∙
	-?	?	1 R		-R	1	1 R		-1
	??R?		1RR1		R
M=	1R??	,^Mnorm =	1RR1	,Mrank =	1
	R???		R11R		R
- 一
RRRR
1R1R
R2R R2R
Here R > 1 is a large constant, e.g., R = 100. The minimum nuclear norm solution is the rank-2
matrix Mnorm, which has IlMnorm∣∣* = 4R (which is 400 when R = 100). Mrank is a rank-1
solution with much larger nuclear norm, k Mnorm k* = 2R2 + 2 (which is 20002 when R = 100). We
can verify that f (∙) satisfies Assumptions 5.5 and 5.7 and WG (t) converges to the rank-1 solution
Mrank. Therefore, GF with infinitesimal initialization converges to Mrank rather than Mnorm, which
refutes the conjecture in (Gunasekar et al., 2017). See Appendix D for a formal statement.
5.3	Equivalence between GD and GLRL: General Case
Theorem 5.6 shows that for any fixed time t, the trajectory of GLRL in the first phase approx-
imates GF with infinitesimal initialization, i.e., W1G (t) = limα→0 Wcα(t), where Wcα(t) :=
Φ(Wα,+ log(hWα, uιu>i-1) + t). However, Wf(∞) = limα→o Wα(∞) does not hold in
general, unless the prerequisite in Theorem 5.8 is satisfied, i.e., unless Wι = WG (∞) is a minimizer
of f (∙) in S+. This is because of the well-known result that GD converges to local minimizers (Lee
et al., 2016; 2017). We adapt Theorem 2 of Lee et al. (2017) to the setting of GF (Theorem I.5) and
obtain the following result (Theorem 5.10); see Appendix I.4 for the proof.
Theorem 5.10. Let f : Rd×d → R be a convex C 2 -smooth function. (1). All stationary points of
L : Rd×d → R, L(U) = 2f (UU>) are either strict saddles or global minimizers; (2). For any
random initialization, GF (1) converges to strict saddles of L(U) with probability 0.
Therefore, for convex f (∙) such as matrix sensing and completion, suppose f (∙) has no rank-1 PSD
minimizer, then no matter how small α
is, Wcα (∞)
(if exists) is a minimizer of f (∙) with a higher
rank and thus away from the rank-1 matrix Wι. In other words, WG (t) only describes the limiting
trajectory of GF in the first phase, i.e., when GF goes from near 0 to near Wι. After a sufficiently
long time (depending on α), GF escapes the critical point Wι, but this is not described by WG (t).
To understand how GF escapes W1, a priori, we need to know how GF approaches W1. Using
a similar argument for Theorem 5.3, Theorem 5.11 shows that generically GF only escapes in the
direction of vGv>, where vG is the (unique) top eigenvector of -Vf (Wι), and thus the limiting
trajectory exactly matches with that of GLRL in the second phase until GF gets close to another
critical point W2 ∈ S+≤2. If W2 is still not a minimizer of f (∙) in S； (but it is a local minimizer
in Sd+,≤2 generically), then GF escapes W2 and the above process repeats until WK is a minimizer
in Sd+ for some K . Here by “generically” we hide some technical assumptions and we elaborate
on them in Appendix J. See Figure 1 and Figure 2 for experimental verification of the equivalence
between GD and GLRL. We end this section with the following characterization of GF:
Theorem 5.11 (Theorem I.2, informal). Let W be a critical point of (2) satisfying that W_ is a
local minimizer of f (∙) in S+≤r for some r ≥ 1 but not a minimizer in S+. Let -Vf (W) =
7
Published as a conference paper at ICLR 2021
Figure 1: The trajectory of depth-2 GD, WGD(t), converges to the trajectory of GLRL, WGLRL(t), as the initialization scale goes to 0.
We plot dist(t) = mint0 ∈T kWGD (t) - WGLRL(t0)kF for different initialization scale kW (0)kF, where T is a discrete subset ofR that
δ-covers the entire trajectory of GLRL: maxt mint0 ∈T WGLRL(t) - WGLRL (t0)F ≤ δ for δ ≈ 0.00042. For each kW (0)kF, we
run 20 randomseeds and plotthemseparately. The ground truth W * ∈ R20×20 is a randomly generated rank-3 matrix with ∣∣ W *∣∣f = 20.
30% entries are observed. See more in Appendix E.1.
Pd=ι μiViV> be the eigendeComPositionof-Vf (W). If μι > μ2 and ifthere exists time Ta ∈ R
for every a so that φ(Wa, Tα) converges to W with positive alignment with the top principal
component vιv> as a → 0, thenfor every fixed t, lima→o φ(Wα, Ta + 21- log / J ->y +1)
μ1	hφ( Wα ,T α ) , v1 vι i
exists and is equal to WG (t) := lime→o φ(W + evιv>,击 log ɪ + t).
Characterization of the trajectory of GF. Generically, the trajectory of GF with small initialization
can be split into K phases by K +1 critical points of (2), { Wr}K=0 (W0 = 0), where in phase r GF
escapes from Wr-ι in the direction of the top principal component of -Vf (Wr-ι) and gets close
to Wr. Each Wr is a local minimizer of f (∙) in S，≤, but none of them is a minimizer of f (∙) in
S+ except WK. The smaller the initialization is, the longer GF stays around each Wr. Moreover,
{Wr }K=o corresponds to {Wr,e}K=° in Definition 5.1 with infinitesimal e > 0.
6	B enefits of Depth: A View from GLRL
In this section, we consider matrix factorization problems with depth L ≥ 3. Our goal is to
understand the effect of the depth-L parametrization W = U↑U2 •一UL on the implicit bias ——how
does depth encourage GF to find low rank solutions? We take the standard assumption in existing
analysis for the end-to-end dynamics that the weight matrices have a balanced initialization, i.e.
Ui>(0)Ui(0) = Ui+1(0)Ui>+1(0), ∀1 ≤ i ≤ L - 1. Arora et al. (2018) showed that if {Ui}iL=1 is
balanced at initialization, then we have the following end-to-end dynamics. Similar to the depth-2
case, we use φ(W (0), t) to denote W (t), where
dW = - X 二1(WW τ) Li Vf (W )(W τW )1-i+1
(11)
The lemma below is the foundation of our analysis for the deep case, which greatly simplifies (11).
Due to the space limit, we defer its derivations and applications into Appendix K.
Lemma 6.1. For M (t):= W (t)2/L, we have 需=-Vf (M L/2)ML/2 - M L/?Vf(M L/2).
Our main result, Theorem 6.2, gives a characterization of the limiting trajectory for deep matrix
factorization with infinitesimal identity initialization. Here W(t) := lima→0 WG(t) is the trajectory
of deep GLRL, where WG (t) := φ(αeιe>,1.((p-P) +1) (See Algorithm 2). The dynamics for
general initialization is more complicated. Please see discussions in Appendix L.
Theorem 6.2. Let P = L, L ≥ 3. Suppose ∣∣Vf (0)k2 = λι(-Vf (0)) > max{λz(-Vf (0)), 0},1
for every fixed t ∈ R,卜卜I,a“(；P-P) +1) - W (t)∣∣p = O(α P(P1+1)),	(12)
and for any 2 ≤ k ≤ d,
foreVeryfiXedt ∈ R,	λk (φ (aI, 1.((p—P； + t)) = O(α)∙	(13)
So how does depth encourage GF to find low-rank solutions? When the ground truth is low-rank,
say rank-k, our experiments (Figure 2) suggest that GF with small initialization finds solutions with
smaller k-low-rankness compared to the depth-2 case, thus achieving better generalization. At first
glance, this is contradictory to what Theorem 6.2 suggests, i.e., the convergence rate of deep GLRL
at a constant time gets slower as the depth increases. However, it turns out the uniform upper bound
for the distance between GF and GLRL is not the ideal metric for the eventual k-low-rankness of
learned solution. Below we will illustrate why the r-low-rankness of GF within each phase r is a
better metric and how they are different.
Definition 6.3 (r-low-rankness). For matrix M ∈ Rd×d,
we define the r-low-rankness of M as
Pid=r+1 σi2 (M), where σi(M) is the i-th largest singular value of M.
1We believe assumption ∣∣Vf (0)∣b = λι(-Vf (0)) could be removed with a more refined analysis.
8
Published as a conference paper at ICLR 2021
----distance -------- grad norm -------- r-low-rankness
Figure 2: GD passes by the same set of critical points as GLRL when the initialization scale is small, and gets much closer to the critical
points when L ≥ 3. Depth-2 GD requires a much smaller initialization scale to maintain small low-rankness. Here the ground truth matrix
W * ∈ R20×20 is of rank 3 as stated in Appendix E.1. In this case, GLRL has 3 phases and 4 critical points {W r "=0, where W o = 0
and W 3 = W *. For each depth L and initialization scale ∣∣ W (0) kF, we plot the distance between the current step of GD and the closest
CritiCal point of GLRL, k WGd (t) — W r ∣f, the norm of full gradient, I^u1：l L(Ull)IIf and the (r + 1)-low-rankness of WGd (t)
with r := argmin°≤i≤3 k WGD(t) — Wi∣F.
Suppose f (∙) admits a unique minimizer Wo in S；「and we run GF from αI for both depth-2 and
depth-L cases. Intuitively, the 1-low-rankness of the depth-2 solution is Ω(α1-μ2∕μι), which can be
seen from the second warmup example in Section 4. For the depth-L solution, though it may diverge
from the trajectory of deep GLRL more than the depth-2 solution does, its 1-low-rankness is only
O(α), as shown in Theorem 6.4. The key idea is to show that there is a basin in the manifold of
rank-1 matrices around W0 such that any GF starting within the basin converges to W0 . Based on
this, we can prove that starting from any matrix O(α)-close to the basin, GF converges to a solution
O(α)-close to W0. See Appendix M for more details.
Theorem 6.4. In the same settings as Theorem 6.2, if W (∞) exists and is a minimizer of f (∙) in
S+≤1, under regularity assumption M.1, we have inft∈R ∣∣φ (αI,t) 一 W(∞)∣∣f = O(α).
Interpretation for the advantage of depth with multiple phases. For depth-2 GLRL, the low-
rankness is raised to some power less then 1 per phase (depending on the eigengap). For deep GLRL,
we show the low-rankness is only multiplied by some constant for the first phase and speculate it to
be true for later phases. This conjecture is supported by our experiments; see Figure 2. Interestingly,
our theory and experiments (Figure 5) suggest that while being deep is good for generalization,
being much deeper may not be much better: once L ≥ 3, increasing the depth does not improve
the order of low-rankness significantly. While this theoretical result is only for identity initialization,
Theorem F.1 and Corollary F.2 further show that the dynamics of GF (11) with any initialization
pointwise converges as L → ∞, under a suitable time rescaling. See Figure 6 for experimental
verification.
7 Conclusion and Future Directions
In this work, we connect gradient descent to Greedy Low-Rank Learning (GLRL) to explain the
success of using gradient descent to find low-rank solutions in the matrix factorization problem. This
enables us to construct counterexamples to the implicit nuclear norm conjecture in (Gunasekar et al.,
2017). Taking the view of GLRL can also help us understand the benefits of depth.
Acknowledgments
The authors thank Sanjeev Arora and Jason D. Lee for helpful discussions. The authors also thank
Runzhe Wang for useful suggestions on writing. ZL and YL acknowledge support from NSF, ONR,
Simons Foundation, Schmidt Foundation, Mozilla Research, Amazon Research, DARPA and SRC.
ZL is also supported by Microsoft PhD Fellowship.
9
Published as a conference paper at ICLR 2021
References
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for
convex optimization problems. Journal ofControl and Decision, 5(1):42-60, 2018.
Sanjeev Arora, N Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In 35th International Conference on Machine Learning, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ AlChe-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7411-7422.
Curran Associates, Inc., 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d’ Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 8139-8148. Curran Associates, Inc., 2019b.
Mohamed Ali Belabbas. On implicit regularization: Morse functions and applications to matrix
factorization. arXiv preprint arXiv:2001.04264, 2020.
Jeff Bezanson, Stefan Karpinski, Viral B Shah, and Alan Edelman. Julia: A fast dynamic language
for technical computing. arXiv preprint arXiv:1209.5145, 2012.
Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization:
An overview. IEEE Transactions on Signal Processing, 67(20):5239-5269, 2019.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. volume 125 of Proceedings of Machine Learning Research, pages
1305-1338. PMLR, 09-12 Jul 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems 32, pages 2937-2947. Curran Associates, Inc.,
2019.
Francis H. Clarke, Yuri S. Ledyaev, Ronald J. Stern, and Peter R. Wolenski. Nonsmooth analysis and
control theory, volume 178. Springer Science & Business Media, 2008.
Frank H. Clarke. Generalized gradients and applications. Transactions of the American Mathematical
Society, 205:247-262, 1975.
Frank H Clarke. Optimization and Nonsmooth Analysis. Society for Industrial and Applied Mathe-
matics, 1990. doi: 10.1137/1.9781611971309.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages
1329-1338, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alche-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages
3196-3206. Curran Associates, Inc., 2019.
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental
learning drives generalization. In International Conference on Learning Representations, 2020.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems 30, pages 6151-6159. Curran Associates, Inc., 2017.
10
Published as a conference paper at ICLR 2021
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on
linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages
9482-9491. Curran Associates, Inc., 2018.
Benjamin D. Haeffele and Rene Vidal. Structured Low-Rank Matrix Factorization: Global Optimality,
Algorithms, and Applications. IEEE Transactions on Pattern Analysis and Machine Intelligence
(PAMI), 42(6):1468-1482, 2019.
Jean-Baptiste Hiriart-Urruty and A. S. Lewis. The clarke and michel-penot subdifferentials of the
eigenvalues of a symmetric matrix. Comput. Optim. Appl., 13(1-3):13-23, 1999. doi: 10.1023/A:
1008644520093.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages
8571-8580. Curran Associates, Inc., 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019a.
Ziwei Ji and Matus Telgarsky. A refined primal-dual analysis of the implicit bias. arXiv preprint
arXiv:1906.04540, 2019b.
Rajiv Khanna, Ethan Elenberg, Alexandros G Dimakis, and Sahand Negahban. On approximation
guarantees for greedy low rank optimization. arXiv preprint arXiv:1703.02721, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pages 1246-1257, 2016.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin
Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406,
2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Sebastien Bubeck, Vianney
Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory,
volume 75 of Proceedings of Machine Learning Research, pages 2-47. PMLR, 06-09 Jul 2018.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic
and depth-sensitive margins in homogeneous and non-homogeneous deep models. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4683-4692,
Long Beach, California, USA, 09-15 Jun 2019a. PMLR.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In Kamalika
Chaudhuri and Masashi Sugiyama, editors, Proceedings of Machine Learning Research, volume 89
of Proceedings of Machine Learning Research, pages 3420-3428. PMLR, 16-18 Apr 2019b.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In Kamalika Chaudhuri and Masashi Sugiyama,
editors, Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine
Learning Research, pages 3051-3059. PMLR, 16-18 Apr 2019c.
Ioannis Panageas, Georgios Piliouras, and Xiao Wang. First-order methods almost always avoid
saddle points: The case of vanishing step-sizes. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 6474-6483. Curran Associates, Inc., 2019.
11
Published as a conference paper at ICLR 2021
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035.
Curran Associates, Inc., 2019.
Lawrence Perko. Differential equations and dynamical systems, volume 7. Springer Science &
Business Media, 2013.
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by
norms. arXiv preprint arXiv:2005.06398, 2020.
Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separa-
bility: New relaxations and efficient boosting algorithms. Machine learning, 80(2-3):141-163,
2010.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):1-57,
2018a.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018b.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, and Jieping Ye. Rank-one
matrix pursuit for matrix completion. In International Conference on Machine Learning, pages
91-99, 2014.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 4148-4158. Curran Associates, Inc., 2017.
Quanming Yao and James Tin Yau Kwok. Greedy learning of generalized low-rank models. In IJCAI
International Joint Conference on Artificial Intelligence, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.
Stanislaw Eojasiewicz. Ensembles Semi-analytiques. IHES notes, 1965.
12
Published as a conference paper at ICLR 2021
A	Comparison to Existing Greedy Algorithms for
Rank-Constrained Optimization
The most related one to GLRL (Algorithm 1) is probably Rank-1 Matrix Pursuit (R1MP) proposed by
Wang et al. (2014) for matrix completion, which was later generalized to general convex loss in (Yao
and Kwok, 2016). R1MP maintains a set of rank-1 matrices as the basis, and in phase r, R1MP adds
the same urur> as defined in Algorithm 1 into its basis and solve minα f (Pir=1 αiuiui>) for rank-r
estimation. The main difference between R1MP and GLRL is that the optimization in each phase
of R1MP is performed on the coefficients α, while the entire Ur evolves with GD in each phase of
GLRL. In Figure 3, we provide empirical evidence that GLRL generalizes better than R1MP when
ground truth is low-rank, although GLRL may have a higher computational cost depending on η, .
Similar to R1MP, Greedy Efficient Component Optimization (GECO, Shalev-Shwartz and Singer
2010) also chooses the r-th component of its basis as the top eigenvector of -Vf (Wr), while
it solves minβ f (P1≤i,j≤r βijuiuj>) for the rank-r estimation. Khanna et al. (2017) provided
convergence guarantee for GECO assuming strong convexity. Haeffele and Vidal (2019) proposed a
local-descent meta algorithm, of which GLRL can be viewed as a specific realization.
B Deep GLRL Algorithm
Algorithm 2: Deep Greedy Low-Rank Learning (Deep GLRL)
parameter: step size η > 0; small e > 0
e0 一 e1/L, L(UI,…，UL)= f (Wι …Wl).
Wo J 0 ∈ Rd×d, and U0,1(∞),..., Uo,l(∞) ∈ Rd×0 are empty matrices
while λ1(-Vf (Wr)) > 0 do
rJr+1
let ur be a top (unit) eigenvector of -Vf (Wr-1)
Ur,1(0) J [Ur-1,1(∞) e0ur] ∈ Rd×r
Ur,k(0) J Ur-10,k(∞) e00 ∈ Rr×r for all 2 ≤ k ≤ L - 1
Ur,L(0) J Ur-e01u,L>(∞) ∈ Rr×d
for t = 0, 1, . . . do
L Ur,i(t + 1) J Ur,i(t) — ηVUi L (Ur,l(t),…，Ur,L(t)), ∀1 ≤ i ≤ L.
_ Wr J Ur,l(∞)…Ur,L(∞)
return Wr
C	Preliminary Lemmas
Lemma C.1. For U0 ∈ Rd×r and W0 := U0U0>, the following statements are equivalent:
(1)	. Uo is a stationary point of L(U) = 2 f (UU >);
(2)	. Vf(W0)W0 = 0;
(3)	. Wo := Uo Uo> is a critical point of (2).
Proof. (2) ⇒ (3) is trivial. We only prove (1) ⇒ (2), (3) ⇒ (1).
Proof for (1) ⇒ (2). If Uo is a stationary point, then 0 = VL(Uo) = Vf(Wo)Uo. So
Vf(Wo)Wo = (Vf(Wo)Uo)Uo> =0.
Proof for (3) ⇒ (1). If Wo is a critical point, then
0= hg(Wo),Vf(Wo)i = -2Tr(Vf(Wo)WoVf(Wo)) = -2kVf(Wo)Uok2F,
which implies VL(UO) = 0.	□
13
Published as a conference paper at ICLR 2021
Lemma C.2. For a stationary point Uo ∈ Rd×r of L(U) = 2f (UU>) where f (∙) is convex,
Wo := UoU> attains the global minimum of f (∙) in S+ := {W : W 占 0} iffVf (Wo)占 0.
Proof. Since f(W) is a convex function and Sd+ is convex, we know that W0 is a global minimizer
of f(W) in Sd+ iff
hVf(Wo),W-Woi ≥ 0,	∀W	0.	(14)
Note that hVf (Wo), Woi = Tr(Vf(Wo)Wo). By Lemma C.1, hVf(Wo), Woi = 0. Combining
this with (14), we know that Wo is a global minimizer iff
hVf(Wo),Wi ≥0,	∀W	0.	(15)
It is easy to check that this condition is equivalent to Vf (Wo)占 0.	口
D Proofs for Counter-example
Conjecture D.1 (Formal Statement, Gunasekar et al. 2017). Suppose f : Rd×d → R is a quadratic
function and min f (W) = 0. Then for any Winit * 0 if W1 = lim lim Φ(αW⅛it,t) exists
W0	α→o t→+∞
and f(W 1) =0,then ∣∣W4|* = min ∣∣Wk* s.t. f(W)=0.
W0
Propsition D.2 (Formal Statement for Example 5.9). For constant R > 1, let
	-?	?	1 R		-R	1	1 R		-1
	??R?		1RR1		R
M=	1R??	,^MnOrm =	1RR1	,and Mrank =	1
	R???		R11R		R
L(U) = 2 f (UU >),
f(W) = 1 X (Wij- Mij)2
(i,j)E。
- -
R2RR2R
1R1R
R2R R2R
and
where Ω = {(1, 3), (1,4), (2,3), (3,1), (3, 2), (4,1)}.
Then for any Winit	0, s.t. u1> Winitu1 > 0,
lim lim φ(αWinit, t) = Mrank.
α→o t→+∞
Moreover, we have
∣Mrankk*
2R2 +2 > 4R = kMnormk* =WHmi(W Jk-
Proof. We define W1G,(t), W1G(t) in the same way as in Definition 5.1, Theorem 5.6.
WG(t) := φ (euιu>,t),
WG (t) := l→o WGe( 2μ1 log 1 + t).
Below we will show
1.	Assumption 5.7 and Assumption 5.5 are satisfied.
2.	W1G (t)F bounded for t ≥ 0;
3.	limt→+∞ W1G (t) = Mrank;
4.	Mnorm = arg minW H0,f (W)=o ∣W ∣* .
14
Published as a conference paper at ICLR 2021
Thus Since Mrank is a global minimizer of f (∙), applying Theorem 5.8 finishes the proof.
Proof for Item 1. Let Mo := Vf (0), then
M0
0
0
1
R
01
0R
R0
00
R
0
0
0
Let A := [R R], then we have λι(A) = 1+√2+R2, λ2(A) = 1-√1+R2, thus λι(A) > ∣λ2(A)∣ >
0 > λ2(A). As a result, λ1(A) = kAk2. Let v1 ∈ R2 be the top eigenvector of A. We claim
that u1 = [ vv11 ] ∈ R4 is the top eigenvector of Vf (0). First by definition it is easy to check that
M0u1 = λ1(A)u1. Further noticing that M02 = A02 A02 , we know λi2(M0) ∈ {λ12(A), λ22(A)}
for all eigenvalues λi(M0). That is, λ1(M0) = λ1(A), λ2(M0) = -λ2(A), λ3(M0) = λ2(A),
and λ4(M0) = -λ1(A). Thus Assumption 5.5 is satisfied. Also note that f is quadratic, thus
analytic, i.e., Assumption 5.7 is also satisfied.
Proof for Item 2. Let (χe(t), ye(t)) ∈ R2 be the gradient flow of g(χ,y) = 1 (x2 一 1)2 + (xy — R)2
starting from (xe(0),ye(0)) = √vι.
dxdtt) = (1 一 x(t)2)x(t) — 2y(t)(x(t)y(t) — R)
dydtt) = -2x(t)(x(t)y(t) — R)
Let W(t) be the following matrix:
(16)
W(t) :
-Xe(t)-
Ve(t)
x(t)
y(t)
[x(t) y(t) x(t)	y(t)] .
Then it is easy to verify that W(0) = W1G,(0) and W(t) satisfies (2). Thus by the existence and
uniqueness theorem, we have W (t) = W1G,(t) for all t. Taking the limit → 0, we know that
W1G(t) can also be written in the following form:
-χ(t)-
wG(t) = X(t) [x⑴ y⑴ X⑴ y⑴],
y(t)
and (xe(t), ye(t)) ∈ R2 is a gradient flow of g(x, y) = 2(x2 一 1)2 + (Xy 一 R)2.
Since g(x(t), y(t)) is non-increasing overtime, and lim g(x(-t), y(-t)) = g(x(-∞), y(一∞))=
t→-∞
g(0, 0) = R2 + 0.5, we know |x(t)y(t)| ≤ 3R for all t. So whenever y2(t) 一 x2(t) ≥ 9R2, we have
X2(t) ≤ y9R2) ≤ y2(t9Rx2(t) ≤ 1. In this case, "「""『⑴)=2x2(t)(x2(t) 一 1) ≤ 0. Combining
this with yjg)2 — xjg)2 = 0 ≤ 9R2, we have y2(t) — x2(t) ≤ 9R2 for all t, which also
implies that y(t) is bounded. Noticing that 9R2 ≥ g(x(t), y(t)) ≥ (x2(t) 一 1)2, we know x2(t) is
also bounded. Therefore, W1G (t) is bounded.
Proof for Item 3. Note that (x(∞), y(∞)) is a stationary point of g(x, y). It is clear that g(x, y)
only has 3 stationary points — (0,0), (1, R) and (一1, 一尺).Thus Wι can only be 0 or Mrank.
However, since for all t, f (WG(t)) < f (0), Wι = limt→∞ WG (t) cannot be 0. So Wι must be
Mrank.
Proof for Item 4. Let mij be (i, j)th element of M. Suppose M	0, we have
(e1 一	e4)>M(e1	一 e4) ≥ 0	=⇒	m11	+ m44	≥ m14 + m41	= 2R
(e2 一	e3)>M(e2	一 e3) ≥ 0	=⇒	m22	+ m33	≥ m23 + m32	= 2R
15
Published as a conference paper at ICLR 2021
Depth (L)	Simulation method
2	ConstantLR, η = 10-3 for 106 iterations
3	Adaptive LR, η = 2 × 10-5 and ε = 10-4 for 106 iterations
4	Adaptive LR, η = 3 × 10-4 and ε = 10-3 for 106 iterations
Table 1: Choice of hyperparameters for simulating gradient flow. For L = 2, gradient descent escapes
saddles in O (log ɪ) time, where e is the distance between the initialization and the saddle.
Thus 4R = minw占o,f(w)=o k W∣∣*, where the equality is only attained at m^ = R, i = 1, 2, 3,4.
m11	m14	m22	m23
Otherwise, either	11	14	or	22	23	will have negative eigenvalues. Contradiction to
m41	m44	m32	m33
that M 0.
1 -1 0 0
Below we will show the rest unknown off-diagonal entries must be 1. Let V = 0	0	1 0
0	0	01
then
M 0 =⇒ V M V >	0 =⇒
0
m31 - m32
m41 - m42
m13 - m23
R
R
m14 - m24
R	0,
R
which implies m13 = m23, m14 = m24.
100
With the same argument for V = 0 1 0
001
note M is symmetric and m13 = 1, thus mij
arg minw”〃w)=o IIW∣∣*, which is unique.
0
0 , we have m13 = m14 , m23 = m24 . Also
-1
mji = 1, ∀i = 1, 2, j = 3, 4. Thus Mnorm =
□
E Experiments
E.1	General Setup
The code is written in Julia (Bezanson et al., 2012) and PyTorch (Paszke et al., 2019).
The ground-truth matrix W* is low-rank by construction: we sample a random orthogonal matrix
U, a diagonal matrix S with Frobenius norm ∣∣SIlF = 1 and set W* = USU>. Each measurement
X in X1 , . . . , Xm is generated by sampling two one-hot vectors u and v uniformly and setting
X = 1 uv> + 1 vu>.
In Figures 1, 2, 3 to 5 and 7, the ground truth matrix W* has shape 20 × 20 and rank 3, where
∣∣W*∣∣F = 20, λι(W*) = 17.41, λ2(W*) = 8.85, λ3(W*) = 4.31 and λι(-Vf(0))=
6.23, λ2(-Vf (0)) = 5.41. P = 0.3 is used for generating measurements, except P = 0.25 in
Figure 3, i.e., each pair of entries of Wi*j and Wj*i is observed with probability p.
Gradient Descent. Let e > 0 be the Frobenius norm of the target random initialization. For the
depth-2 case, we sample 2 orthogonal matrices V1, V2 and a diagonal matrix D with Frobenius
norm e, and we set U = V1D1/2V2>; for the depth-L case with L ≥ 3, we sample L orthogonal
matrices X,..., VL and a diagonal matrix D with Frobenius norm 七 and we set Ui := %D1∕L匕+1
(VL+ι = X). In this way, we can guarantee that the end-to-end matrix W = U1 ∙ ∙ ∙ UL is symmetric
and the initialization is balanced for L ≥ 3.
We discretize the time to simulate gradient flow. When L > 2, gradient flow stays around saddle
points for most of the time, therefore we use full-batch GD with adaptive learning rate %, inspired by
16
Published as a conference paper at ICLR 2021
RMSprop (Tieleman and Hinton, 2012), for faster convergence:
vt+ι = αvt +(I - α) ∣∣vL(θt) k2,
η
ηt =	I	—,
vt+t+ι + ε
1-αt+1
θt+ι = θt- ηtNL(θt),
where α = 0.99, η is the (unadjusted) learning rate. The choices of hyperparameters are summarized
in Table 1. The continuous time for θt is measured as Pt-1 ηi.
GLRL. In Figures 1, 2, 3 and 4, the GLRL’s trajectory is obtained by running Algorithm 1 with
= 10-7 and η = 10-3. The stopping criterion is that if the loop has been iterated for 107 times.
E.2 Experimental Equivalence between GLRL and Gradient Descent
Here we provide experimental evidence supporting our theoretical claims about the equivalence
between GLRL and GF for both cases, L = 2 and L ≥ 3.
In Figure 1, we show the distance from every point on GF (simulated by GD) from random initializa-
tion is close to the trajectory of GLRL. In Figure 2, We first run GLRL and obtain the critical points
{Wr }3=0 passed by GLRL. We also define the distance of a matrix W to the critical points to be
min0≤r≤3 k W — Wr∣f.
E.3 How well does GLRL work?
We compare GLRL With gradient descent (With not-so-small initialization), nuclear norm minimiza-
tion and R1MP (Wang et al., 2014). We use CVXPY (Diamond and Boyd, 2016; AgraWal et al.,
2018) for finding the nuclear norm solution. The results are shoWn in Figure 3. GLRL can fully
recover the ground truth, While others have difficulty doing so.
d = 20, ∣∣W(0)∣∣f = 10-2
ContinuousTime le4
——GLRL1 L = 2
GD, L = 2
----nuclear norm
----RlMP (rank 3)
----RlMP (rank 10)
Figure 3: GD With small initialization outperforms R1MP and minimal nuclear norm solution on
synthetic data With loW-rank ground truth. Solid (dotted) curves correspond to test (training) loss.
Here the loss f (W):= *∣∣ W — W*∣F and f(0) = 1. Werun 10 random seeds for GD and plot
them separately (most of them overlap).
E.4 How does initialization affect the convergence rate to the rank- 1 GLRL
trajectory?
We use the general setting in Appendix E.1. In these experiments, We use the constant learning rate
10-5 for 4 × 107 iterations. The reference matrix Wref is obtained by running the first stage of GLRL
With ∣W (0)∣F = 10-48 and We pick one matrix in the trajectory With ∣Wref ∣F about 0.6.
For every = 10i, i ∈ {—1, —2, —3, —4, —5}, We run both gradient descent and the first phase of
GLRL With ∣W (0)∣F = . For gradient descent, We use random initialization so ∣W (0)∣F is full
rank W.p. 1. The distance of a trajectory to Wref is defined as mint≥0 ∣W (t) — Wref∣F. In practice,
17
Published as a conference paper at ICLR 2021
as we discretized time to simulate gradient flow, we check every t during simulation to compute the
distance. As a result, the estimation might be inaccurate when a trajectory is really close to Wref.
The result is shown at Figure 4. We observe that GLRL trajectories are closer to the reference
matrix Wref by magnitudes. Thus the take home message here is that GLRL is in general a more
computational efficient method to simulate the trajectory of GF (GD) with infinitesimal initialization,
as one can start GLRL with a much larger initialization, while still maintaining high precision.
sso-t,s018-
0	12	3	4
Continuous Time le4
Figure 4: Using v1v1> (denoted by “rank 1”) as initialization makes GD much closer to GLRL
compared to using random initialization (denoted by “rank d”), where v1 is the top eigenvector of
-Vf (0). We take a fixed reference matrix on the trajectory of GLRL with constant norm and plot
the distance of GD with each initialization to it respectively..
0	2	4	6
Contin uous Time	1 e4
0	2	4	6
Continuous Time	le4
0	12	3
Continuous Time le5
Figure 5: Deep matrix factorization encourages GF to find low rank solutions at a much practical
initialization scale, e.g. 10-3. Here the ground truth is rank-3. For each setting, we run 5 different
random seeds. The solid curves are the mean and the shaded area indicates one standard deviation.
We observe that performance of GD is quite robust to its initialization. Note that for L > 2, the
shaded area with initialization scale 10-7 is large, as the sudden decrement of loss occurs at quite
different continuous times for different random seeds in this case.
E.5 Benefit of Depth: polynomial vs exponential dependence on initialization
To verify the our theory in Section 6, we run gradient descent with different depth and initialization.
The results are shown in Figure 5. We can see that as the initialization becomes smaller, the final
solution gets closer to the ground truth. However, a depth-2 model requires exponentially small
initialization, while deeper models require polynomial small initialization, though it takes much
longer to converge.
F	The marginal value of being deeper
Theorem F.1 shows that the end-to-end dynamics (17) converges point-wise while L → ∞ if the
product of learning rate and depth, ηL, is fixed as constant. Interestingly, (17) also allows us to
simulate the dynamics of W (t) for all depths L while the computation time is independent of L.
In Figure 6, we compare the effect of depth while fixing the initialization and ηL. We can see that
deeper models converge faster. The difference between L = 1, 2, and 4 is large, while difference
among L ≥ 16 is marginal.
18
Published as a conference paper at ICLR 2021
Figure 6: The marginal value of being deeper. The trajectory of GD converges when depth goes to
infinity. Solid (dotted) curves correspond to test (train) loss. The x-axis stands for the normalized
continuous time t (multiplied by L).
Theorem F.1. Suppose W = UΣV > is the SVD decomposition of W, where Σ =
diag(σ1, . . . , σd). The dynamics of L-layer linear net is the following, ◦ denotes the entry-wise
multiplication:
dWW = -LU ((U>Vf(W)V) ◦ K(L)) V>,	(17)
where KL) = σ2-2∕L, KL) = L.--；" for i = j.
,	, Lσ -Lσ
ij
Proof. We start from (11):
dW
dt
L-1
-X(WW>)LVf(W)(W>W)L-T
l=0
L-1
X2l	2(L-1-l)
U Σ F U >Vf (W )V Σ -L— V
l=0
L-1
=-LU LT X ΣL (U>Vf (W)V)ΣQT- V.
l=0
Note that Σ is diagonal, so
Σ L (U >vf (W )V)Σ 2%IT) = (U >vf (W )V) ◦ H(I),
2l 2(L ― 1T)
where Hij = σiL σj∙ L . Therefore,
L-1	L-1
LT X ΣLl(U>Vf (W)V)ΣQETI = LT X(U>Vf (W)V) ◦ H(I)
l=0	l=0
=(U > Vf (W )V) ◦ K(L),
where K(L) = L-1 PlL=-01 H(l). Hence,
dWW = -LU [(U>Vf(W)V) ◦ K(L)i
The entries of K(L) can be directly calculated by
〜
V.
L 1	2-2/L
(L)	_i L 2 2(LTT)	σi	,
Kij =L X σiL σj L =	-2--2
l=0	I L-2/L -L-2/L ,
ij
i = j,
i 6= j.
□
Corollary F.2. As L → ∞, K(L) converges to K*, where Ki,i = σ2 , Kij = ln -2--j-2
for i 6= j .
19
Published as a conference paper at ICLR 2021
Experiment details. We follow the general setting in Appendix E.1. The ground truth W * is
different but is generated in the same manner and has the same shape of 20 × 20 and p = 0.3 is used
for observation generation. We directly apply (17), in which we compute V and U through SVD, to
simulate the trajectory together with a constant learning rate of 10-- for depth L. W(0) is sampled
from 10-3 × N (0, Id).
G	Proofs for Dynamical System
In this section, we prove Theorem 5.3 in Section 5.1. In Appendix G.1, we show how to reduce
Theorem 5.3 to the case where J(0) is exactly a diagonal matrix, then we prove this diagonal case
in Appendix G.2. Finally, in Appendix G.3, we discuss how to extend it to the case where J(0) is
non-diagonalizable.
G.1 Reduction to the Diagonal Case
Theorem G.1. If J(0) = diag(μ1,..., μd) is diagonal, then the statement in Theorem 5.3 holds.
Prooffor Theorem 5.3. We show how to prove Theorem 5.3 based on Theorem G.1. Let 黑=g(θ)
be the dynamical system in Theorem 5.3. Let J(0) = V DV -1 be the eigendecomposition, where
V is an invertible matrix and D = diag(μι,..., μd). Now we define the following new dynamics
by changing the basis:
^ . . ~ -1 ..
θ(t) = V-1 θ(t).
Then dθdtt) = g(θ) for g(θ) ：= VTg(Vθ), and the associated Jacobian matrix is J(θ):=
VTJ(Vθ)V, and thus J(0) = diag(μι,...,μd).
Now we apply Theorem G.1 to θ(t). Then za(t) ：= V-1Za(t) converges to the limit Z(t):=
lim Zα(t). This shows that the limit z(t) = Vz(t) exists in Theorem 5.3. We can also verify that
α→0
z(t) is a solution of (6).
Given δa converging to 0 with positive alignment with Ut1 as a → 0, we can define δɑ ：= V-1δa,
then δa converges to 0 with positive alignment with eι, where eι is the first vector in the standard
basis and is also the top eigenvector of J(0). Therefore, for every t ∈ (-∞, +∞), there is a constant
C > 0 such that
V-1φ (δα,t + ɪlogɪɪ-- - z(t)	≤ C∙kδɑ∣∣μ1^	(18)
∖	μι	hδα, UI)	2
for every sufficiently small a. As V are invertible, this directly implies (7).	□
G.2 Proof for the Diagonal Case
Now we only need to prove Theorem G.1. Let eι,..., ed be the standard basis. Then Ui = Vi = eι
in this diagonal case. We only use ei to stand for Ui and Vi in the rest of our analysis.
Let R > 0. Since g(θ) is C2-smooth, there exists β > 0 such that
kJ(θ)-J(θ+h)k2≤βkhk2	(19)
for all kθk2, kθ + hk2 ≤ R. Then the following can be proved by integration:
g(θ+h)-g(θ)= Z iJ(θ+ξh)dξh,	(20)
kg(θ + h) - g(θ) -J(θ)hk2 ≤βkhk22.	(21)
By (21), we also have
kg(θ) - J(0)θk2 = kg(θ) - g(0) - J(0)θk2 ≤ βkθk22.	(22)
20
Published as a conference paper at ICLR 2021
Let K := β∕μι. We assume WLOG that R ≤ 1∕κ. Let F(x) = logX - log(1 + κx). It is
easy to see that F0(χ) = χ+Kx^ and F(x) is an increasing function with range (-∞, log(1∕κ)).
We use FT(y) to denote the inverse function of F(x). Define Tα(r):= 卷 (F(r) - F(α))=
⅛ (log r - log ⅛).
Our proof only relies on the following properties of J(0) (besides that μι, eι are the top eigenvalue
and eigenvector of J (0)):
Lemma G.2. For J(0) := diag(μι,..., μd), we have
1.	For any h ∈ Rd, h>J(0)h ≤ μι kh∣∣2;
2.	For any t ≥ 0, ∣∣etJ(0) — eμιteιe> ∣∣2 = eμ2t.
Proof. For Item 1, h>J(0)h = Pcd=1 口也彳 ≤ ai||h||2. FOrItem 2, ∣∣etJ(0) - eμιteιe>∣[=
∣∣diag(0,eμ2t,...,eμdt)∣∣2 = e∙2t.	□
Lemma G.3. For θ(t) = φ(θ0, t) with kθ0k2 ≤ α andt ≤ Tα(r),
kθ(t)k2 ≤
1 + κr
1 + κα
α ∙ eμιt
≤ r.
Proof. By (22) and Lemma G.2, we have
1 吗逃=hθ(t),g(θ(t))i ≤ hθ(t), J(0)θ(t)i + βkθ(t)k2 ≤ μιkθ(t)k2 + βkθ(t)k3.
This implies dkθdt)k2 ≤ μ1(∣∣θ(t)∣∣2 + κ∣∣θ(t)k2). Since F0(x) = χ+Kxτ, We further have
ddtF(kθ(t)k2) ≤ μι.
So F(∣∣θ(t)∣∣2) ≤ F(ɑ) + μιt. By definition of Tα(r), we then know that ∣∣θ(t)∣∣2 ≤ r for all
t ≤ Tα(r). So
log ∣∣θ(t)k2 ≤ F(∣∣θ(t)∣∣2) +log(1 + Kr) ≤ F(α) + μi + log(1 + κr).
Expending F(α) proves the lemma.	□
Lemma G.4. For θ(t) = φ(θ0, t) with ∣θ0∣2 ≤ α andt ≤ Tα(r), we have
θ(t) = etJ(0)θ0 + O(r2).
Proof. Let θ(t) = etJ(0)θo. Then we have
ι dd ∣θ(t) - θ(t)∣2 ≤ Dg(θ(t)) - J (o)θ(t), θ(t) - θ(t)E
=Dg(θ(t)) - J (o)θ(t), θ(t) - θ(t)E+(θ(t) - θ(t))>J (o)(θ(t) - θ(t))
^ . ... . . ^ . . . . √~1
≤ kg(θ(t)) - J(0)θ(t)k2 ∙kθ(t) - θ(t)k2 + μιkθ(t) - θ(t)k2,
where the last inequality is due to Lemma G.2. By (22) and Lemma G.3, we have
∣g(θ(t)) - J(0)θ(t)∣2 ≤ β∣θ(t)∣22 ≤ β
1 + Kr
-----α
1 + Kα
2
• e2μιt
Sowe have dt∣∣θ(t) - θ(t)k2 ≤ β (甘*α) • e2'1t + μι∣θ(t) - θ(t)k2. By Gronwall,s inequality,
kθ(t) - θ(t)∣2 ≤ ]： β (1+£r ɑj ∙ e2μ1τ eμil(t-τ )dτ.
Evaluating the integral gives
kθ(t) - θ(t)∣2 ≤ β (井竺a) e”ιt •上二
1 + Ka J	μι
≤ κ (2a • e“ιt)2≤ κr2,
1 + Ka
which proves the lemma.
□
21
Published as a conference paper at ICLR 2021
_	—	-，、	，，一	、公，、	.Z ^ X	，，，一,,	^	•	〜	一，、
Lemma G.5. Let θ(t) = φ(θo, t), θ(t) = φ(θo, t). If max{∣∣θ0k2, ∣∣θ0k2} ≤ α, thenfor t ≤ Tα(r),
kθ(t) - θ(t)k2 ≤ eμ1t+κrkθo- Θ0k2.
Proof. Fort ≤ Tα(r), by (20),
1 d no。一 θ(t)k2 = Dg®t))- g(θ(t)), θ(t) - θ(t)E
=(θ(t)—o(t))> u： J (θξ ⑴焕)(θ(t) - θ(t)),
where θξ(t) := ξθ(t) + (1 — ξ)θ(t). ByLemmaG.3, max{∣∣θ(t))2, ∣∣θ(t)k2} ≤ 1+Krα ∙ eμ1t for
all t ≤ Tɑ(r). So ∣∣θξ(t)k2 ≤ 也1 α ∙ eμ1t. Combining these with (19) and Lemma G.2, We have
h>J(θξ(t))h = h>J(0)h+h>(J(θξ(t)) - J(0))h ≤
1 + κr
1 + κα
α ∙ eμ1t
∣h∣22 ,
μι + 8 ∙
for all h ∈ Rd. Thus, ddt ∣∣θ(t) - θ(t)∣2 ≤ Ri + β ∙ 1+黑α ∙ e"1t) ∣∣θ(t) - θ(t)∣2. This implies
^
J∣θ(t)- θ(t)∣2
Iog	ʌ
kθ(0) - θ(0)k2
≤∕(μ1 + β ∙ 1≡α ^ eμ1τ 卜
≤ μιt + κ ∙ 1+^αeM
1 + κα
≤ μιt + κr.
Therefore, ∣θ(t) - θ(t)∣2 ≤ eμ1t+κr∣θ(0) - θ(0)∣L
□
Lemma G.6. For every t ∈ (-∞, +∞), z(t) exists and zα(t) converges to z(t) in the following
rate:
∣zα(t) - z(t)∣2 = O(α),
where O hides constants depending on g(θ) and t.
Proof. We prove the lemma in the cases of t ∈ (-∞, F(R)∕μι] and t > F(R)/μι respectively.
Case 1.	Fix t ∈ (-∞, F(R)∕μι]. Let a be the unique number such that ɪ+^ = α (i.e.,
F(α) = log α). Let ɑ0 be an arbitrary number less than a. Let to := 六 log Oa. Then
to =卷(F(α) 一 log α0) ≤ Ta' (α). By Lemma G.4, we have
kΦ (α0e1,to) - αeι∣2 = ∣∣Φ (α0e1,to) - et0J(0)a0ei( = O(α2).
Let r := F-1(μιt) ≤ R. Then t + -=- log 1 = Ta(r) if α < r.
ByLemmaG.3, ∣φ (α0e1,to)k2 ≤ α. Also, ∣αeι∣2 =]晨& ≤ α. By Lemma G.5,
∣zα(t) - zα0 (t)∣2
φ (α0e1,t + ɪ log J) - φ (αeι,t + ，log 1) ||
φ φφ(α0e1,tO),t + = log —1- φ α αe1,t + h log —
∖	μι
≤ O(α2 ∙ eμ1(t+ ⅛ log a)+κr)
μi α J ∣∣2
α
≤O
For a small enough, we have ɑ
α2 ∖
α ).
O(α), so for any α0 ∈ (0, α),
∣zα(t) - zα0 (t)∣2 = O(α).
This implies that {zα(t)} satisfies Cauchy’s criterion for every t, and thus the limit z(t) exists for
t ≤ F(R)/μι. The convergence rate can be deduced by taking limits for a0 → 0 on both sides.
22
Published as a conference paper at ICLR 2021
Case 2.	For t = F(R)∕∙ι + T with τ > 0, φ(θ,τ) is locally Lipschitz with respect to θ. So
∣Zα(t) — Na (t)∣2 = ∣Φ(Zα(F 3) / ST ) — 6— (F(R)/a1),丁 在
=O(kza(F (R)∕μι) — Na (F (R)∕μ1)k2)
= O(α),
which proves the lemma for t > F(R)∕μι∙
□
Prooffor Theorem G.1. The existence of z(t) := limα→o Zα(t) = limα→o Φ (αeι,t + 卷 log ：)
has already been proved in Lemma G.6, where we show kzα(t) - z(t)k2 = O(α).
By the continuity of φ(∙, t) for every t ∈ R,we have
z(t) =	lim φ ( αvι,t +	-1	log 1 )	= φ ( lim φ ( αvι,	ɪ log	1 ) ,t )	= φ(z(0),t).
α→0	∖	μι α)	∖α→0	∖	μι	α)	)
Now it is OnlylefttoProVe (7). WLOGwecanassumethat ∣∣δα∣∣2 is decreasing and 2 ≤ ∣∣δɑ∣∣2 ≤ α
(otherwise we can do reparameterization). Then our goal becomes proving
kθa(t) — z(t)∣2 = O α μι+γ
(23)
where θa(t) ：= Φ (δa, t + 上 log(© ；)) ∙ We prove (23) in the cases of t ∈ (-∞, F(R)/μι] and
t > F(R)∕μι respectively.
Case 1.	Fix t ∈ (-∞, (F(R) + log q)∕μι]∙ Let &i = α访+γ . Let αι := eF(a1) = ɪ+Oar∙ Let
to := 夫(F(αι) 一 log α) ≤ T∣∣δαk2 (αι). At time to, by LemmaG.2 we have
~	μ2
∣∣et0J(O)-eμ1t0e1e>∣∣2 = eμ2t0 = eμ2(F(OI)TOgO) = (α1 )μ .	(24)
Let qa ：=(等,eι)∙ By Definition 5.2, there exists q > 0 such that qα ≥ q for all sufficiently small
α∙ Then we have
kΦ (δα,to) — α1qae1k2 = ∣∣φ (δa,to) - et0J⑼δa∣∣2 + ∣∣ (et0J(0) — eμι1t0eιe>) δa(
=O(α2)+ (?) μ1 kδa∣2
=O(α2 + αf2/μ1 α1-12∕访)
= O(α12 ).
Let r := F-1(μιt + log A) ≤ R. Then t + -1 log a~q~ = Ta(r) if α < r. By Lemma G.3,
kΦ (δa,to)∣2 ≤ αι∙ Also, ∣∣αιqaeι∣∣2 ≤ αι = 1+⅛ ≤ &> By Lemma G.5,
kθa(t) - zaι(t)k2≤ φ(φ5,t+μlog0⅛
=O fɑ2 ∙ eμ1(t+ ⅛ log θ⅛ )+κ]
α1qae1, t + h log
,	μι
— φ
2
O(α1).
Combining this with the convergence rate for Na1 (t), we have
kθa(t) — Z(t)∣2 ≤ ∣θa(t) — Nai (t)∣2 + 1。](t) — z(t)∣∣2 = O(α∕
Case 2.	For t = (F(R) + log q)∕μι + T with τ > 0, φ(θ, T) is locally Lipschitz with respect to θ.
So
kθα(t) — z(t)∣2 = kΦ(θa((F (R) + log q)∕"ι),τ) — φ(z((F(R) + log q)∕μι),τ 此
=O(kθa((F(R)+log q)∕μi) — z((F(R)+log q)∕μi也)
= O(α1),
which proves (23) for t > (F(R) + log q)∕μι∙	□
23
Published as a conference paper at ICLR 2021
G.3 Extension to Non-Diagonalizable Case
The proof in Appendix G.2 can be generalized to the case where J(0). Now we state the theorem
formally and sketch the proof idea. We use the notations g(θ), φ(θ0, t), J(θ) as in Section 5.1, but
We do not assume that J(0) is diagonalizable. Instead, We use "ι, "2,..., "d ∈ C to denote the
eigenvalues of J (0), repeated according to algebraic multiplicity. We sort the eigenvalues in the
descending order of the real part of each eigenvalue, i.e., <(μι) ≥ <(μ2) ≥ ∙∙∙ ≥ <(ad), where
<(z) stands for the real part of a complex number z ∈ C. We call the eigenvalue With the largest real
part the top eigenvalue.
Theorem G.7. Assume that θ = 0 is a critical point and the following regularity conditions hold:
1.	g(θ) is C2-smooth;
2.	φ(θ0, t) exists for all θ0 and t;
3.	The top eigenvalue of J(0) is unique and is a positive real number, i.e.,
μι > max{<(μ2), 0}.
Let Vι, Ui be the left and right eigenvectors associated with μι, satisfying U>Vι = L Let zα(t):二
φ(αV1, t + 看 log α) for every α > 0, then ∀t ∈ R, z(t) := lim Za(t) exists and z(t) = φ(z(0), t).
If δα converges to 0 with positive alignment with U1 as a → 0, thenfor any t ∈ R and for any e > 0,
there is a constant C > 0 such that for every sufficiently small α,
I,	(δa, t + ⅛ log hδ⅛) - Z(t)ll2 ≤ C ∙Mak文:	(25)
where γ := μι 一 <(a2)is the eigenvalue gap.
Proof Sketch. Define the folloWing tWo types of matrices. For r ≥ 1, a, δ ∈ R, We define
aδ
a
δ
a
δ
∈ Rr×r.
.	..
..
aδ
a
For r ≥ 1, a, b, δ ∈ R, We define
-C δI
C δI
C δI
C
Where C = ab -ab ∈ R2×2.
By linear algebra, the real matrix J(0) can be Written in the real Jordan normal form, i.e., J(0) =
V diag(J[1], . . . , J[m])V -1, Where V ∈ Rd×d is an invertible matrix, and each J[j] is a real Jordan
block. Recall that there are tWo types of real Jordan blocks, Ja(r,1) or Ja(r,b),1. The former one is
associated With a real eigenvalue a, and the latter one is associated With a pair of complex eigenvalues
a ± bi. The sum of sizes of all Jordan blocks corresponding to a real eigenvalue a is its algebraic
multiplicity. The sum of sizes of all Jordan blocks corresponding to a pair of complex eigenvalues
a ± bi is tWo times the algebraic multiplicity of a + bi or a - bi (note that a ± bi have the same
multiplicity).
It is easy to see that Ja(r,δ) = DJa(,r1)D-1 for D = diag(δr, δr-1, . . . , δ) ∈ Rr×r and Ja(,rb),δ =
DJa(,rb),1D-1 for D = diag(δr, δr, δr-1, δr-1, . . . , δ, δ) ∈ R2r×2r. This means for every δ > 0
24
Published as a conference paper at ICLR 2021
there exists Vδ SUch that J(0) = Vδ Jδ 1V-1, where Jδ ：= diag( Jδ[i],…，Jδ[m]), Jδ[j] ：= JTr)
if J[j] := Ja(r,1), or Jδ[j] := Ja(r,b),δ if J[j] := Ja(r,b),1. Since the top eigenvalue of J(0) is positive
and unique, μ1 corresponds to only one block [μ1] ∈ R1×1. WLOG We let Ji = [.ι], and thus
Jδ[1] = [μi].
We only need to select a parameter δ > 0 and prove the theorem in the case of J(0) = Jδ since we
can change the basis in a similar way as we have done in Appendix G.1. By scrutinizing the proof for
Theorem G.1, we can find that we only need to reprove Lemma G.2. However, Lemma G.2 may not
be correct since J(0) is not diagonal anymore. Instead, we prove the following:
1.	If δ ∈ (0,7), then h> Jrh ≤ ”i||h||2 for all h ∈ Rd;
2.	For any μ2 ∈ (<(μ2), μι), if δ ∈ (0, μ2 - <(α2)), then ∣∣etJδ - eμ1teιe>∣∣2 ≤ eμ⅛t for
all t ≥ 0.
Proof for Item 1. Let K be the set of pairs (ki, k2) such that ki 6= k2 and the entry of Jr at the
ki-th row and the k2-th column is non-zero. Then we have
h>Jrh = h> Jr +2 J> h = XX <(μk )hk +	X	hkι hk2 δ
k=i	(k1,k2)∈K
≤ X<W)hk+	X	hk4h2δ.
k=i	(k1,k2)∈K
Note that <(μ7k) ≤ <(μ72) for k ≥ 2. Also note that there is no pair in K has ki = 1 or k2 = 1, and
for every k ≥ 2 there are at most two pairs in K has ki = k or k2 = k. Combining all these together
gives
d
h>Jrh ≤ μ7ihi2 + (<(μ72) + δ) X h2k ≤ μ7ikhk22,
k=2
which proves Item 1.
Proof for Item 2. Since Jr is block diagonal, we only need to prove that ∣∣etJδ[j] k2 ≤ eμ2t for
every j ≥ 2. If Jr[j] = Ja(r,r) = aI + δN, where N is the nilpotent matrix, then
etJδ[j] =eatI+rtN=eatIertN=eatertN,
where the second equality uses the fact that I and N are commutable. So we have
∣etJδ[j] ∣2 ≤ eat∣ertN ∣2 = eatertkNk2 ≤ e(a+r)t.
If Jr[j] = Ja(r,r) = D + δN2, where D = diag(C, C, . . . , C) and N is the nilpotent matrix, then
etJδ[j] = etD+rtN2 = etDertN2 ,
where the second equality uses the fact that D and N2 are commutable. Note that etC =
at cos(bt) - sin(bt)	hih ili	tD	tC	at S h
e	sin(bt) cos(bt) , which implies ∣e ∣2 = ∣e ∣2 = e . So we have
ketJδj] ∣∣2 ≤ IIetD∣∣2 ∙ IlertN2∣∣2 = eαtertkN2k2 ≤ e(a+r)t.
Since δ ∈ (0, μ702 - <(μ72)), we know that a + δ < μ702, which completes the proof.
Proof for a fixed δ. Since Item 1 continues to hold for δ ∈ (0, γ7), Lemmas G.3 to G.6 also hold.
This proves that z(t) exists and satisfies (6).
γ0	γ
It remains to prove (25) for any e > 0. Let 70 ∈ (0, Y) be a number such that μγpψ ≥ μγγ - e. Fix
μ2 = μι - γ0, δ = μ2 - <(μ2). By Item 2, we have ∣∣etJδ - eμ1teie> ∣[ ≤ eμ2t for all t ≥ 0. By
scrutinizing the proof for Theorem G.1, we can find that the only place we use Item 2 in Lemma G.2
25
Published as a conference paper at ICLR 2021
is in (24). For proving (25), We can repeat the proof while replacing all the occurrences of μ2 by μ2.
Then we know that for every t ∈ R, there is a constant C > 0 such that
..	.	Y0
IlVT φ (δα, t + ⅛ log hδ⅛) - VTz (t)ll2 ≤ C ∙k VTδαkμi+L
γ0	γ
for every sufficiently small a. By definition of 70, μγpψ ≥ μ~γ+γ - e∙ Since δa → 0 as α → 0, we
have k%Tδα∣∣2 < 1 for sufficiently small α. Then we have
∣∣φ(δα,t+ ⅛loghδa‰r) -z(t)∣∣2 ≤ kVδk2 ∙ IlVTφ(δα,t+ ⅛loghδa‰r) - VTz⑴Il2
0
≤kVδ k2 CkVTδαkμ1"0
0
」~	~γ ~_e
≤ C∙kVδk21VH ∙kδαkμι+γ .
0
〜	〜	~ γ ~
Absorbing ∣∣ Vδ∣∣2 ∙ kVτ∣∣μι"' into C proves (25).	□
H Eigenvalues of Jacob ians and Hessians
In this section we analyze the eigenvalues of the Jacobian J(W) at critical points of (2).
For notation simplicity, we write sz(A) := A + A> to denote the symmetric matrix produced by
adding up A and its transpose, and write ac{A, B} = AB + BA to denote the anticommutator of
two matrices A, B. Then g(W) can be written as g(W):= -ac{Vf (W), W}.
Let Uo ∈ Rd×r be a stationary point of the function L : Rd×r → R, L(U) = 2f (UU>), i.e.,
VL(U0) = Vf(U0U0>)U0 = 0. By Lemma C.1, this implies
Vf(W0)W0 = 0	(26)
for W0 := U0U0>, and thus W0 is a critical point of (2).
For a real-valued or vector-valued function F (θ), we use DF (θ)[δ], D2F (θ)[δ1, δ2] to denote the
first- and second-order directional derivatives of F(∙) at θ.
Let X be a linear space, which can be Rd×d or Rd×r. For a function F : X → X, we use DF (θ) to
denote the directional derivative of F at θ, represented by the linear operator
DF(θ)[∆] : X → X, ∆ → DF(θ)[∆] = lim F(θ + ⑸ - F(θ).
t→0	t
We also write DF (θ)[∆1, ∆2] := hDF (θ)[∆1], ∆2i.
For a function F : X → R, we use D2F (θ) = D(VF (θ)) to denote the second directional derivative
of F at θ, i.e., D2F(θ)[∆] = D(VF (θ))[∆], D2F (θ)[∆1, ∆2] = D(VF(θ))[∆1, ∆2].
Define J(W) := Dg(W). By simple calculus, we can compute the formula for J(W0):
J(W0)[∆] = -ac{Vf(W0),∆} - ac{D2f (W0)[∆], W0},
J(W0)[∆1,∆2] = - Vf (W0), sz(∆1∆2>) - D2f (W0)[∆1, sz(W0∆2>)],
where ∆, ∆1, ∆2 ∈ Rd×d.
We can also compute the formula for D2L(U0):
D2L(U0)[∆] = Vf(W0)∆+D2f(W0)[sz(∆U0>)]U0,
D2L(U0)[∆1, ∆2] = 2 ((Vf(Wo),sζ(∆ι∆>)> + D2f(W0)[sz(∆1U>),sz(∆2U>)]),
where ∆, ∆1, ∆2 ∈ Rd×r.
26
Published as a conference paper at ICLR 2021
H. 1 Eigenvalues at the Origin
The eigenvalues of J (0) is given in Lemma 5.4. Now we provide the proof.
Proof for Lemma 5.4. For W0 = 0, we have
J(0)[∆] = -Vf(0)∆ - ∆Vf(0)
J(0)[∆ι,∆2] = -(Vf (0), sz(∆ι∆>)>
It is easy to see from the second equation that J(0) is symmetric.
Let -Vf (0) = Pd=ι μiUi[i]U>i] be the eigendeComPosition of the symmetric matrix -Vf (0).
Then we have
d
J(O)[△] = X μi (ui[勾u>[i]A + AuEuk])
i=1
dd
=XXμi (ui[i]u>[i]Aui[j]u>j] + ui[j]u>j]Aui[i]u>[i])
i=1 j=1
dd
=EE(Mi + μj )ui[i]u>[iQui[j] u>[j]
i=1 j=1
dd
=^XX(Mi + μj )(△，u1[i]u1[j]) u1[i] u1[j],
i=1 j=1
which Proves (8).
For ∆ = u1[i]u1>[j] + u1[j]u1>[i], we have
J(O)[△] = (μi + μj )u1[i]u>[j] + (μi + μj )u1[j]u>[i] = (μi + μj )△.
So ui[i]u[j] + ui[j]u>[i] is an eigenvector of J(0) associated with eigenvalue μi + μj. Note that
{u1[i]u1>[j] +u1[j]u1>[i] : i, j ∈ [d]} sPans all the symmetric matrices, so these are all the eigenvectors
in the sPace of symmetric matrices.
For every antisymmetric matrix ∆ (i.e., ∆ = -∆>), we have
J(0)[∆]=J(0)[∆>]=J(0)[-∆].
So J (0)[∆] = 0 and every antisymmetric matrix is an eigenvector associated with eigenvalue 0.
Since every matrix can be exPressed as the sum of a symmetric matrix and an antisymmetric matrix,
we have found all the eigenvalues.	□
H.2 Eigenvalues at Second-Order S tationary Points
Now we study the eigenvalues of J(Wο) when Uo is a second-order stationary point of L( ∙), i.e.,
VL(U0) = 0,D2L(U0)[∆,∆] ≥ 0forall∆ ∈ Rd×r. We further assume that U0 is full-rank, i.e.,
rank(Uo) = r. This condition is meet if Wo := UoUT is a local minimizer of f (∙) in S] but not a
minimizer in Sd+ .
Lemma H.1. For r ≤ d, if Uo ∈ Rd×r is a second-order stationary point of L(∙), then either
rank(Uo) = rank(Wo) = r, or Wo is a minimizer of f (∙) in S+, where Wo = Uo U>.
Proof. Assume to the contrary that Uo has rank < r and Wo is a minimizer of f (∙) in S]. The
former one implies that there exists a unit vector q ∈ Rr such that Uoq = 0, and the latter one
implies that there exists v ∈ Rd such that v>Vf(Wo)v < 0 by Lemma C.2.
27
Published as a conference paper at ICLR 2021
Let ∆ = vq> . Then we have
D2L(Uo)[∆, ∆] = (Vf(Wo), vv>〉+ 2D2f(W0)[sz(v(U0q)>), SZ(V(U0q)>)]
=(Vf (Wo), VvD + 1D2 f (Wo)[0,0]
= Vf (W0), vv>.
So D2L(U0)[∆, ∆] < 0, which leads to a contradiction.	口
By (26), the symmetric matrices -Vf (W0) and W0 commute, so they can be simultaneously
diagonalizable. Since (26) also implies that they have different column spans, we can have the
following diagonalization:
d-r
-Vf(Wo) = X μiViV>,
i=1
d
Wo =	E	μwv>.
i=d-r+1
(27)
First we prove the following lemma on the eigenvalues and eigenvectors of the linear operator
-D2L(Uo):
Lemma H.2. For every ∆ ∈ Rd×d, if
Uo∆> + ∆Uo> = 0	(28)
then ∆ is an eigenvector of the linear operator -D2 L(Uo)[ ∙ ] : Rd×r → Rd×r associated with
eigenvalue 0. Moreover, the solutions of (28) spans a linear space ofdimension ".—'.
Proof. Suppose	Uo∆>	+	∆Uo>	= 0. Then we have	Uo∆> =	-∆Uo>,	and thus	∆>	=
-Uo+∆Uo>, where Uo+ is the pseudoinverse of the full-rank matrix Uo. This implies that there is a
matrix R ∈ Rr×r , such that ∆ = UoR. Then we have
-D2L(Uo)[∆] = -Vf(Wo)UoR-D2f(Wo)[Uo∆> +∆Uo>]Uo
= -(Vf(Wo)Uo)R-D2f(Wo)[0]Uo
= 0.
Replacing ∆ with UoR in (28) gives Uo(R+R>)Uo> = 0, which is equivalent to R = -R> since
Uo is full-rank. Since the dimension of r X r antisymmetric matrices is r(r-1), the span spanned by
the solutions of (28) also has dimension r(r-1).	口
Definition H.3 (Eigendecomposition of -D2L(Uo)). Let
rd
-D2L(Uo)[∆] = XξphEp,∆iEp
p=1
be the eigendecomposition of the symmetric linear operator —D2L(U0)[ ∙ ] : Rd×r → Rd×r, where
ξ1, . . . , ξrd ∈ R are eigenvalues, E1, . . . , Erd ∈ Rd×r are eigenvectors satisfying hEp, Eqi = δpq.
We enforce ξp to be 0 and Ep to be a solution of (28) for every rd — Nr-I) < p ≤ rd.
Lemma H.4. Let A ∈ Rd×d be a matrix. If {u^ 1,..., UK} is a set of linearly independent
left eigenvectors associated With eigenvalues λι,...,λκ and {vι,..., vd-k } IS a set of linearly
〜
〜
independent right eigenvectors associated with eigenvalues λι,...,λD-k, and gi, Vj = 0 for all
- K, then λ1 , . . . , λK, λ1
1 ≤ i ≤ K,1 ≤ j ≤ D
, . . . , λD-K are all the eigenvalues ofA.
Proof. Let U := (u^ 1,..., UK)> ∈ Rk×d and V := (Vι,..., Vd-k) ∈ Rd×(D-K). Then both
U and V are full-rank. Let U+ = U>(UU>)-1, V+ = (V>V)-1V> be the pseudoinverses of
^ 〜
U and V.
28
Published as a conference paper at ICLR 2021
Now we define
^ fτ ~
P= V+ , Q = [u+	v].
Then we have
^	^	,	^	~	~	,	^	,	. ~ -I- ~ . -ɪ . ^	~ . -I- , ^	^ -I- -	-,	~,~
Note that UU+ = IK, UV = 0, V +U+ = (V>V)-1(UV)>(UU>)-1 = 0, V +V = ID-K.
So PQ = ID, or equivalently Q = P-1. Then we have
P-1AP
diag(λ1, . . . ,λK)
0
*
diag(λι,..., Ad-k)
where * can be any K × (D - K) matrix. Since P-1AP is upper-triangular, we know that P-1AP
1 ∙	1 ʌ	ʌ T	T	11 A	I~I
has eigenvalues λι,..., λκ, λι,...,λD-κ, and so does A.	□
Theorem H.5. The eigenvalues of J(W0) can be fully classified into the following 3 types:
i	.	∙	∙	1 r	r∕'j.∕7	ι τ^τ	T	T ∙
1.	μi + μj is an eigenvalue for every 1 ≤ i ≤ j ≤ d 一 r, and Uij := ViV> + Vj v> is an
associated left eigenvector.
2.	ξp is an eigenvalue for every 1 ≤ P ≤ rd — Nr-I), and Vp := Ep U> + UoE> is an
associated right eigenvector.
3.	0 is an eigenvalue, and any antisymmetric matrix is an associated right eigenvector, which
d(d-1)
spans a linear space of dimension	—L.
Proof of Theorem H.5. We first prove each item respectively, and then prove that these are all the
eigenvalues of J(W0 ).
Proof for Item 1. For Uj = viv> + Vjv>, it is easy to check:
一 . .^ . ^
ac{-Vf (W0), U.ij} = (λi + λj)Uij
^
Uij Wo = 0
^
WoU^ij = 0
So we have
J (Wo)[∆, Uij] = (λi + λj)(△, U.ij E — D2f (Wo)[∆, 0] = (λi + λj)(△, UijE,
which shows that Uij is a left eigenvector associated with eigenvalue λi + λj .
Proof for Item 2. By definition of eigenvector, we have —D2L(U0)[Ep] = ξpEp, so
ξpEp = —Vf(W0)Ep —D2f(W0)[U0Ep>+EpU0>]U0.
Right-multiplying both sides by U0>, we get
ξp Ep U0> = —Vf(W0)EpU0> —D2f(W0)[VAp]W0
= —Vf(W0)(EpU0>+U0Ep>) —D2f(W0)[VAp]W0
= —Vf(W0)VAp—D2f(W0)[VAp]W0,
where the second equality uses the fact that Vf(W0)U0 = 0 since U0 is a critical point. Taking
both sides into sz(∙) gives
ξp VAp = —sz(Vf(W0)VAp) — sz(D2f(W0)[VAp]W0)
. 、一 ~ 一
= J(W0)[VAp],
which proves that Vp is a right eigenvector associated with eigenvalue ξp .
29
Published as a conference paper at ICLR 2021
Proof for Item 3. Since Vf (W) is symmetric, g(W) is also symmetric. For any ∆ = -∆>,
J(W0)[∆] = J(W0)[∆>] = J(W0)[-∆].
So J (W0)[∆] = 0 and ∆ is an eigenvector associated with eigenvalue 0.
No other eigenvalues. Let Sd be the space of symmetric matrices and Ad be the space of anti-
symmetric matrices. It is easy to see that Sd and Ad are orthogonal to each other, and Sd and Ad
are invariant subspaces of J(W0)[∆]. Let h : Sd → Sd, ∆ 7→ J (W0)[∆] be the linear operator
J (W0)[∆] restricted on symmetric matrices. We only need to prove that h is diagonalizable.
ɪ, ∙	,	,1 C TΓ~T 1	1 ∙	1 ∙ 1	1 , ,	1	,1	1 ,1	1	CC
It is easy to see that {Uij} are linearly independent to each other and thus spans a subspace of Sd
with dimension (d-r)(d-r+1). We can also prove that {Vp} spans a subspace of Sd with dimension
rd - r(r-1) by contradiction. Assume to the contrary that there exists scalars αp for 1 ≤ p ≤ rd -
r(r - 1)/2, not all zero, such that pp=-r(r-1)/2 apV^p = 0. Then pp=-r(r-1)/2 αpEp is a solution
of (28). However, this suggests that pp=-r(r-1)/2 αpEp lies in the span of {Ep}rd-r(r-i)∕2<p≤rd,
which contradicts to the linear independence of {Ep}1≤p≤rd.
Note that
(d — r)(d — r + 1)
'-F—i+
r(r - 1)
-2-
d(d + 1)
——2— = dιm(Sd).
—
PIj
Also note that
2vi>EpU0>vj + 2vj>EpU0>vi = 0. By Lemma H.4, Items 1 and 2 give
all the eigenvalues of h, and thus Items 1, 2, 3 give all the eigenvalues of J(W0).
□
I Proofs for the Depth-2 Case
I.1	Proof for Theorem 5.6
Proof for Theorem 5.6. Since W(t) is always symmetric, it suffices to study the dynamics of the
lower triangle of W(t). For any symmetric matrix W ∈ Sd, let VecLT(W) ∈ Rd(d2+1) be the vector
consisting of the d(d+1) entries of W in the lower triangle, permuted according to some fixed order.
Let g(W) be the function defined in (2), which always maps symmetric matrices to symmetric
matrices.
d(d+1)
Let g : R -2一
d(d+1)
→ R 2
be the function such that g(vecLτ( W)) = VecLT(g( W)) for
any W ∈ Sd. For W(t) evolving with (2), we view vecLT(W(t)) as a dynamical system.
-d VecLT(W (t)) = g(veɑτ(W (t))).
By Lemma 5.4, the spaces of symmetric matrices Sd and antisymmetric matrices Ad are invariant
subspaces of J(0), and
{(μi + μj，u1[i]u>[j] + u1[j]u>[i])} ι≤i≤j≤d
is the set of all the eigenvalues
and eigenvectors in the invariant subspace Sd. Thus, μι := 2μι and μ2 := μι + μ2 are the largest
and second largest eigenvalues of the Jacobian of g(∙) at VecLT(W) = 0, and Ui = Vι = uιu>
are the corresponding left and right eigenvectors of the top eigenvalue. Then it is easy to translate
Theorem 5.3 to Theorem 5.6.
□
I.2	Proof for Theorem 5.8
The proof for Theorem 5.8 relies on the following Lemma on the gradient flow around a local
minimizer:
Lemma I.1. If θ is a local minimizer of L(θ) and for all ∣∣θ — θ∣∣2 ≤ r, θ satisfies LGjasiewicz
inequality:
∣VL(θ)∣2 ≥ C (L(θ)-L(θ))μ
for some μ ∈ [1/2,1), then the gradient flow θ(t) = φ(θo, t) converges to a point θ∞ near θ if θo is
close enough to θ, and the distance Can be bounded by ∣∣θ∞ —创2 = O(∣θo — θk2(1-μ)).
30
Published as a conference paper at ICLR 2021
Proof. For every t ≥ 0,if ∣∣θ(t) 一 θ∣∣2 ≤ r,
d (L(θ(t)) - L(θ))1-μ = (1 — μ) (L(θ(t)) - L(θ))-μ ∙ (vL, dθ
dt	dt
=-(1- μ)(L(θ(t))-L(θ))-“∙kVL∣∣2 ∙
≤ -(I 一 μ)c
dθ
dt
dθ
dt
2
Therefore, kθ(t) - Θ0k2 ≤ R：愕 ∣∣2 dt ≤(1—“)cL(θ0)1-μ = O(kθ0 - M∣2(1-μ)). If we choose
kθ(t)-圳2 small enough, then ∣∣θ(t)-划2 ≤ ∣∣θ(t) -Θ0k2 + |优一θ∣∣2 = O(∣∣θ0-θk2(1-μ)) < r
and thus R0+∞ ∣∣ dθ ∣∣2 dt is convergent and finite. This implies that θ∞ := limt→+∞ θ(t) exists and
kθ∞ - θk2 = O(kθ0-圳 V-")).	口
Proof for Theorem 5.8. Since W1G(t) ∈ Sd+,≤1 satisfies (2), there exists u(t) ∈ Rd such that
u(t)u(t)> = WG (t) and u(t) satisfies (1), i.e.,普 =-VL(u), where L : Rd → R, u →
2 f (uu>). If WG(t) does not diverge to infinity, then so does u(t). This implies that there is a limit
point U of the set {u(t) : t ≥ 0}.
Let U := {u : L(U) ≥ L(U)}. Since L(u(t)) is non-increasing, we have u(t) ∈ U for all t. Note
that U is a local minimizer of L(∙) in U. By analyticity of f (∙), LOjaSieWiCz inequality holds
for L( ∙) around U (Lojasiewicz, 1965). Applying Lemma I.1 for L restricted on U, we know
that if u(to) is sufficiently close to U, the remaining length of the trajectory of u(t) (t ≥ to) is
finite and thus limt→+∞ U(t) exists. As U is a limit point, this limit can only be U. Therefore,
W ι := limt→+∞ WG (t) = UU> exists.
If Wι is a minimizer of f (∙), U = (U, 0, •一，0) ∈ Rd×d is also a minimizer of L : Rd×d →
R, U → 1 f (UU>). By analyticity of f (∙), Lojasiewicz inequality holds for L( ∙) around U. For
every e > 0, we can always find a time te such that ∣∣U(te) - U∣∣2 ≤ e/2. On the other hand,
by Theorem 5.6, there exists a number α such that for every α < α,
∣∣Φ(Wα,T (Wɑ)+ te)- Wf(te)∣∣2 ≤ e/2, where T (W) := jlog 八“ 1	>、.
2	2μι (W, Uiu/ )
Combining these together we have ∣∣φ(Wα, T (Wa) + te) - W J1 ≤ e.
It is easy to construct a factorization φ(Wα,T(Wa) + te) := Ua,eU>,e such that ∣∣Ua,e - U∣2 =
O(e), e.g., we can find an arbitrary factorization and then right-multiply an orthogonal matrix so that
the row vector with the largest norm aligns with the direction of U. Applying Lemma I.1, we know
that gradient flow starting with Ua,e converges to a point that is only O(e2(1-μ)) far from U. So we
have
∣∣ lim φ(Wα,T(Wa)+1) - W1	= O(e2(1-μ)).
∣∣t→+∞	2
Taking e → 0 complete the proof.	口
I.3 Proof for Theorem 5.11
Theorem I.2. Let W bea critical point of (2) satisfying that W is a local minimizer of f (∙) in S+ ≤
forsome r ≥ 1 but not a minimizer in S+. Let -Vf (W) = Pd=i μi Vivi betheeigendeCOmPOSitiOn
of-Vf (W). If μι > μ2,thefollowing limit exists and is a solution of (2).
WG(t) := lim φ (W + evιv>, —ɪ- log - + t ).
e→0	2μι	e )
For {Wa} ⊆ S+, if there exists time Ta ∈ R for every α so that φ( Wa, Ta) converges to W with
positive alignment with the top principal component v1 v1> as α → 0, then ∀t ∈ R,
lim φ ( Wa, Ta +	ɪ log	/ Qu」、------->y	+ t∣ = W G (t).
a→0	2μi	〈0( Wa,	Ta),	vivi )
31
Published as a conference paper at ICLR 2021
Moreover, there exists a constant C > 0 such that
1	1	∖	N
Φ Wα,Tα + 厂 l°g∕Ew t、~>V + t - WG(t)	≤ C k φ( Wα ,Ta) k Fμ1+γ
∖	2μi	(φ(Wɑ,Tα), V1V>> J	F
for ^very sufficiently small a, where γ := 2μι 一 max{μι + μ2, 0}.
Proof. Following Appendix I.1, we view vecLT(W(t)) as a dynamical system.
ddt VecLT(W (t)) = g(vecuτ(W (t))).
Let W = UU> be a factorization of W, where U ∈ Rd×r. Since W is a local minimizer of
f(∙) in S+≤r, U is also a local minimizer of L : Rd×r → R, U → ɪ f (UU >). Since W is not a
minimizer of f (∙) in S。by Lemma H.1, U is full-rank. By Theorem H.5, J(W) has eigenvalues
μi + μj, ξp, 0. By a similar argument as in Appendix I.1, the Jacobian of g at VecLT(W(t)) has
eigenvalues μi + μj, ξp.
Since U is a local minimizer, ξp ≤ 0 for all p. If μι > μ2, then 2μι is the unique largest eigenvalue,
and Theorem H.5 shows that VecLT(Vιv>) is a left eigenvector associated with 2μι. The eigenvalue
gap Y := 2μι — max{μι + μ2, max{ξp : 1 ≤ P ≤ rd — r(r—1)}} ≥ 2μι — max{μι + μ2,0}.
Also note that <φ(Wɑ,Tα) — W, v1v» = <φ(Wɑ,Tα), v1v7) because (W, v1v» = 0 by
(27). If φ(Wα, Tα) converges to W as α → 0, then it has positive alignment with v1v> iff
hφ(Wα,Tα),VlV>i
kφ(Wα ,Tɑ)-W kF
lim inf α→0
> 0. Then it is easy to translate Theorem 5.3 to Theorem I.2.
□
I.4 Gradient Flow only finds minimizers (Proof for Theorem 5.10)
The proof for Theorem 5.10 is based on the following two theorems from the literature.
Theorem I.3 (Theorem 3.1 in Du and Lee 2018). Let f : Rd×d → R be a C2 convex function. Then
L : Rd×k → R, L(U) = f(UU>), k ≥ d satisfies that (1). Every local minimizer of L is also a
global minimizer; (2). All saddles are strict. Here saddles denote those stationary points whose
hessian are not positive semi-definite (thus including local maximizers). 2
Theorem I.4 (Theorem 2 in Lee et al. 2017). Let g be a C1 mapping from X → X and det(Dg(x)) 6=
0 for all x ∈ X. Then the set of initial points that converge to an unstable fixed point has measure zero,
μ ({x0 ： limk→∞ gk(xo) ∈ Ag}) = 0, where Ag = {x : g(x) = x, maxi ∣λi(Dg(x))∣ > 1}.
Theorem I.5 (GF only finds minimizers, a continuous analog of Theorem I.4). Let f : Rd → Rd be
a C 1 -smooth function, and φ : Rd × R → Rd be the solution of the following differential equation,
>"f," = f(Φ(x, t)), Φ(x, 0) = x, ∀x ∈ Rd,t ∈ R.
Then the set of initial points that converge to a unstable critical point has measure zero,
μ({xo :	limt→∞ φ(x0,t)	∈Uf})	=	0,	where	Uf	= {x :	f (x) =	0,λ1(Df(x))	> 0} and
Df is the Jacobian matrix off.
Proofof Theorem I.5. By Theorem 1 in Section 2.3, Perko (2013), We know φ(∙, ∙) is C1-smooth for
both x, t. We let g(x) = φ(x, 1), then we know g-1(x) = φ(x, —1) and both g, g-1 are C1-smooth.
Note that Dg-1(x) is the inverse matrix of Dg(x). So both of the two matrices are invertible. Thus
We can apply Theorem I.4 and We know μ ({x0 : limk→∞ gk(x0) ∈ Ag}) = 0.
Note that if limt→∞ φ(x, t) exists, then limk→∞ gk(x) = limt→∞ φ(x, t). It remains to show that
Uf ⊆ Ag. For f (xo) = 0, we have φ(x0, t) = x0 and thus g(xo) = x0. Now it suffices to prove
2Though the original theorem is proven for convex functions of form Pn=ι '(xiUU>x>, yi), where '(∙, ∙)
is C2 convex for its first variable. By scrutinizing their proof, we can see the assumption can be relaxed to f is
C 2 convex.
32
Published as a conference paper at ICLR 2021
that λ1(Dg(x0)) > 1. For every t ∈ [0, 1], by Corollary of Theorem 1 in Section 2.3, Perko (2013),
We have ∂Dφ(x, t) = Df(φ(x, t))Dφ(x, t), ∀x, t. Thus,
∂
—Dφ(xo,t) = Df(φ(x0,t))Dφ(xo,t) = Df(x0)Dφ(x0,t).
∂t
Solving this ODE gives Dg(x0) = Dφ(x, 1) = eDf(x0)Dφ(x, 0) = eDf (x0), Where the last equality
is due to Dφ(x, 0) ≡ I, ∀x. Combining this With λ1(Df (x0)) > 0, We have λ1(Dg(x0)) > 1.
Thus we have Uf := {x° : f(x0) = 0, λ1(Df (xo)) > 0} ⊆ Ag, which implies that {x0 :
limt→∞ φ(xo,t) ∈ U*} ⊆ {xo : limk→∞ gk(xo) ∈ Ag}
□
Theorem 5.10. Let f : Rd×d → R be a convex C 2 -smooth function. (1). All stationary points of
L : Rd×d → R, L(U) = 1 f (UU>) are either strict saddles or global minimizers; (2). For any
random initialization, GF (1) converges to strict saddles of L(U) with probability 0.
Proof of Theorem 5.10. For (1), by Theorem I.3, we immediately know all the stationary points of
L( ∙) are either global minimizers or strict saddles. (2) is just a direct consequence of Theorem I.5 by
□
setting f in the above proof to -VL.
J Equivalence B etween GF and GLRL
In this section we elaborate on the theoretical evidence that GF and GLRL are equivalent generically,
including the case where GLRL does not end in the first phase. The word “generically” used when
we want to assume one of the following regularity conditions:
1.	We want to assume that GF converges to a local minimizer (i.e., GF does not get stuck on
saddle points);
2.	We want to assume that the top eigenvalue λ1 (-Vf (W)) is unique for a critical point W
of (2) that is not a minimizer of f (∙) in S；;
3.
We want to assume that a convergent sequence of PSD matrices Wa → W has positive
alignment with vv> for some fixed vector V withhW, vv>i = 0, i.e., for a convergent
sequence of PSD matrices Wa → W, it holds for sure that lim inf ( ,,,Wα-W , vv> )=
α	α→0	kWα-WkF
liminf ,；。-WF/ ≥ 0, and we further assume that the inequality is strict generically.
Theorem I.2 uncovers how GF with infinitesimal initialization generically behaves. Let Wo :=
0. For every r ≥ 1, if Wr-ι is a local minimizer in S+≤r-1 but not a minimizer in S；, then
λι(-Vf (Wr-1)) > 0 by Lemma C.2. Generically, the top eigenvalue λ1(-Vf (Wr-1)) should
be unique, i.e., λ1(-Vf (Wr-1)) > λ2(-Vf (Wr-1)). This enables us to apply Theorem I.2 and
deduce that the limiting trajectory
WG (t) := lim φ ( Wr-1 + eUru>,----------———log - +1 )
r	一。	r 1 r r 2λ1(-Vf(Wr-1)) Se
exists, where Ur is the top eigenvector of -Vf (Wr-1). This WG(∙) is exactly the trajectory of
GLRL in phase r as e → 0.
Note that WG (∙) corresponds to a trajectory of GF minimizing L(∙) in Rd×r, which should generi-
cally converge to a local minimizer of L(∙) in Rd×r. This means the limit Wr := limt→+∞WrG(t)
should generically be a local minimizer of f (∙) in §+<广 If W丁 is further a minimizer in S；, then
λι(-Vf (Wr)) ≤ 0 and GLRL exits with Wr; otherwise GLRL enters phase r + 1.
If GF aligns well with GLRL in the beginning of phase r (defined below), then by Theorem I.2,
as α → 0, the minimum distance from GF to WrG(t) converges to 0 for every t ∈ R. Therefore,
GF can get arbitrarily close to the r-th critical point Wr of GLRL, i.e., there exists a suitable
choice Tar) so that limα→o φ(Wa, 7、") = Wr. Note that〈 Wr, uru>〉= 0 by (27) and thus
33
Published as a conference paper at ICLR 2021
φ(Wα,T(T))-W r	>	h^(Wα^Tθθ'r'),Ur u>i 、
lim inf	α,-α	r-, Uru> = = lιmιnf	(「)、— J ≥ 0. Generically, there should
α→0 ∖kφ(Wα,Tαr))-W rkF , r r	α→0 kΦ(Wα,Tαr))-W r |山-
exist a suitable choice of Tar) so that φ(Wa, Tar)) not only converges to Wr but also has positive
alignment with urur>, that is, GF should generically align well with GLRL in the beginning of phase
r+1.
Definition J.1. We say that GF aligns well with GLRL in the beginning of phase r if there exists
Tar) for every α > 0 such that φ(Wa, Tar)) converges to Wr-ι with positive alignment with Uru>
as α → 0.
If the initialization satisfies that Wa converges to 0 with positive alignment with U1U1> as α → 0,
then GF aligns well with GLRL in the beginning of phase 1, which can be seen by taking Ta(1) = 0.
Now assume that GF aligns well with GLRL in the beginning of phase r - 1, then the above argument
shows that GF should generically align well with GLRL in the beginning of phase r, if GLRL does
not exit in phase r 一 1. In the other case, we can use a similar argument as in Theorem 5.8 to show
that GF converges to a solution near the minimizer Wr of f (∙) as t → ∞, and the distance between
the solution and Wr converges to 0 as α → 0. By this induction we prove that GF with infinitesimal
initialization is equivalent to GLRL generically.
K Proofs for Deep Matrix Factorization
K. 1 Preliminary Lemmas
Lemma K.1. If W(0)	0, then W(t)	0 and rank(W (t)) = rank(W (0)) for all t.
Proof. Note that we can always find a set of balanced Ui(t), such that U1(t) . . . UL(t) = W (t),
d，2 = ʤ = •一=dL = rank(W(t)) and write the dynamics of W(t) in the space of {Ui}L=ι, Thus
it is clear that for all t0, rank(W(t0)) ≤ rank(W (t)). We can apply the same argument for t0 and
we know rank(W (t)) ≤ rank(W(t0)). Thus rank(W (t)) is constant over time, and we denote it
by k. Since eigenvalues are continuous matrix functions, and ∀t, λi (W (t)), i ∈ [k] 6= 0. Thus they
cannot change their signs and it must hold that W(t)占 0.	□
Lemma K.2. ∀a,b,P ∈ R ,if a > b ≥ 0,P ≥ 1 ,then aPa-f∙ ≤ PaP-1.
Proof. Let f(x) = P(1 — x) — (1 — XP). Since f0(x) = -P + PxP-1 < 0 for all X ∈ [0,1),
f (x) ≥ f (0) = 0. Then substituting X by b completes the proof.	口
Recall we use DF (N)[M] to denote the directional derivative along M of F at N.
Lemma K.3. LetF : Sd+ → Sd+,M 7→ MP, where P ≥ 1 andP ∈ Q. Then∀M,N	0,
kDF(N)[M]kF ≤PkNk2P-1kMkF,
where DF(N)[M]:= lim-。F(N+tM)-F(N)is the directional derivative of F along M.
Proof. Let N = UΣU>, where UU> = I and Σ = diag(σι,…，σd), Note that F(UMU>)=
UF(M)U> for any M ∈ Sd+. Then we have
kDF(N)M]kF = li→m kF(N + 丁(N-
= lim IF(ς+tU >mu)- F (∑⅛
t—→0	t
= DF (Σ)[U >M U]F .
Therefore, it suffices to prove the lemma for the case where N is diagonal, i.e., N = Σ.
34
Published as a conference paper at ICLR 2021
Assume P = P, where p, q ∈ N and q ≥ p > 0. Define G(N) = N1. Then G(Σ)p = Σ. Taking
directional derivative on both sides along direction M, we have
P
G(Σ)i-1DG(Σ)[M]G(Σ)P-1 = M,
i=1
So we have
[DG(Σ)[M]]ij
mj
-k —1
PP=1 σj
Let H (G) = Gq. With the same argument, we know
q
p — k .
σjp
[DH(G(Σ))[M]]ij = mij	σi
k=1
p σjp
k — 1 q — k
Note that H (G(Σ)) = F (Σ). By chain rule, we have
DF (Σ)[M] = DH (G(Σ))[DG(Σ)[M]].
That is,
k-1
[DF (Σ)[M]]ij =mij
Pk=I σ/
k-1
PP=I σj
q - k
σ产
p — k .
j
q - p
When σi = σj, clearly [DF(Σ)[M]] j = mj P ∙ σi p = PmijσP- . Otherwise, We assume
WLOG that σi > σj, we multiply σi - σj to both numerator and denominator and we have
PP
|[DF(∑)M]]ijI = mjI -~j ≤ ∣mijI PσP-1 ≤ |mj| P k∑kp-1
σi - σj
where the first inequality is by Lemma K.2. Thus we conclude the proof.
Lemma K.4. For any A, B 0 and P ∈ R, P ≥ 1,
AP-BPF≤P∣A-B∣Fmaxn∣A∣2P-1,∣B∣2P-1o.
□
Proof. Since both sides are continuous in P and Q is dense in R, it suffices to prove the lemma
for P ∈ Q. Let ρ := max {kAk2 , kBk2} and F(M) = MP. Define N : [0, 1] → Sd+, N(t) =
(1 - t)A + tB, we have
1.	∣∣N(t)k2 ≤ ρ, since ∣∣∙k2 is convex.
2.	kDF(N(t))[B-A]kF ≤PkN(t)k2P-1 kB - AkF by Lemma K.3.
Therefore,
∣F(N(1)) - F(N(0))∣F ≤Z 1
0
=Z1
t=0
dF (N (t))
-dt
dt
F
kDF(N(t))[B-A]kFdt
≤ P kA - BkF ρP-1
which completes the proof.
□
For a locally Lipschitz function f (∙), the Clarke subdifferential (Clarke, 1975; 1990; Clarke et al.,
2008) of f at any point x is the following convex set
a f (X) := co < lim Vf (Xk) : Xk → x,f is differentiable at Xkl,
∂x	k→∞
where co denotes the convex hull.
Clarke subdifferential generalize the standard notion of gradients in the sense that, when f is smooth,
d fXx) = {Vf (x)}. Clarke subdifferential satisfies the chain rule:
35
Published as a conference paper at ICLR 2021
Theorem K.5 (Theorem 2.3.10, Clarke 1990). Let F : Rk → Rd be a differentiable function and
g : Rd → R Lipschitz around F (x). Then f = g ◦ F is Lipschitz around x and one has
∂F
∂°f(x) ⊆ ∂°g(F(x)) ◦ dF(x)
∂ x —	∂F °	dx	.
Let λm : Sd → R, M 7→ λm(M) be the m-th largest eigenvalue of a symmetric matrix M. The
following theorem gives the Clarke’s subdifferentials of the eigenvalue:
Theorem K.6 (Theorem 5.3, Hiriart-Urruty and Lewis 1999). The Clarke subdifferential of the
eigenvalue function λm is given below, where co denotes the convex hull:
d λmM) = co{vv> : Mv = λm(M)v, kv∣∣2 = 1}.
K.2 Proof of Lemma 6.1
The equation to be proved is:
ddM = -Vf(ML/2)ML/2 - ML/2Vf(ML/2).	(29)
Since W (t)	0 by Lemma K.1, (11) can be rewritten as the following:
d^W	y~2	2i	2i+2
---=-£ WLVf (W)W2-^L-.	(30)
dt	i=0
Proof for Lemma 6.1. Suppose W(t) is a symmetric solution of (11). By Lemma K.1, we know
W(t) also satisfies (30). Now we let R(t) be the solution of the following ODE with R(0) :=
(W(0)) 1. NoteWedon’t define R(t) by (w(t))L.
L-1
B = - X (-1YRVf(RL尔L-I.
i=0
The calculation beloW shoWs that RL (t) also satisfies (30).
L L-1	L-1 L-1
d— = X Rj竽RLτ-j = X XDiRi+jVfH笠R2—
dt	j=0	dt	j=0 i=0
2L-2	i
= X X(-1)j RiVf(RL)R2T-2-i
i=0	j=0
L-1
=X (RL) ɪ Vf (RL )(RL)2-岑2.
i=0
Since RL(0) = W (0), by existence and uniqueness theorem, RL(t) = W (t), ∀t ∈ R. So
dMM = RdR + dRR = -Vf (ML/2)ML/2 - ML/2Vf(ML/2),
Which completes the proof.
□
K.3 Proof for Theorem 6.2
NoW We turn to prove Theorem 6.2. Let P = L/2. Then (29) can be reWritten as
dM = - (Vf(MP)MP + MPVf (MP)).
The folloWing lemma about the groWth rate of λk(M) is used later in the proof.
(31)
(32)
36
Published as a conference paper at ICLR 2021
Lemma K.7. Suppose M(t) satisfies (32), we have for any T0 > T, and k ∈ [d],
and
T0
λk(M(T0)) - λk(M(T)) ≤
T
2λk (M (t))P ∣∣Vf(M P (t))k2dt.
(λk-P(M(T))- λk-P(M(T))) ≤
ZT0
T
2kVf(MP(t))k2dt.
(33)
(34)
1
P — 1
Proof. Since λk (M (t)) is locally Lipschitz in t, by Rademacher’s theorem, we know λk (M (t)) is
differentiable almost everywhere, and the following holds
T0
λk(MT))- λk(M(T))=	MJt)dt.
T	dt
When dλk(M㈤)exists, We have
dλk (M (t)) U ʃ/L dM ()∖‹Jλk (M)[
-dt- ∈t ∖G, -1Γ： : G ∈	∂M ∫
={2λk(MP(t))(G, -Vf(MP(t))): G ∈	∂MM) }
Note that 性|岛 ≤ ∣∣Gk* = 1. So ∣(G,-Vf (MP(t)))∣ ≤ ∣∣Vf(MP(t))∣∣2. We can prove (34)
with a similar argument.
To prove Theorem 6.2, it suffices to consider the case that M(0) = αI where α := a1/P. WLOG
we can assume -Vf (0) = diag(μι,..., μd) by choosing a suitable standard basis. By assumption
in Theorem 6.2, we have μι > max{μ2,0} and μι = ∣∣Vf (0) |卜.We use φm(Mo, t) to denote the
solution of M(t) when M(0) = M0.
Let R > 0. Since f(∙) is C 3-smooth, there exists β > 0 such that
∣Vf(W1) - Vf(W2)∣F ≤β∣W1-W2∣2
for all W1, W2 with ∣W1∣2 , ∣W2∣2 ≤ R.
a-(P T)	A
Let K = β∕μι. We assume WLOG that R ≤《(/-1). Let Fa(x) := JX-(P -i) —：壬/(「-i)
(P-I)X-P = P-1
1 + κxP	— (1 + κxP )xP .
Then Fa (x)
We will use this function to bound norm growth. Let
g^,c(t) = a-(p-1)-K(P-LI)C-2μ1(P-i)t. Define Ta(r) = & (P ⅛⅜⅜-rjP 1). Itis easy to
verify that ga,r(Ta(r)) = rP-1.
Lemma K.8. For any X ∈ [a, R] we have
(α-(PT) - X-(PT)) - Fa(x) ∈ [0, κ(P - 1)x].
Proof. On the one hand, we have
α-(P T)- X-(P T)
-Fa (x)=[
x
.a-(P-1)
1-
1
1 + κz-P∕(P-1)
dz ≥ 0.
On the other hand,
α-(P T)- X-(P T)
-Fa (x)=[
x
a-(P-1)
______K_____dz ≤
ZP∕(P-1) + K ≤
产 TP-1)
x-(P -1)
zP7⅛-l)dz
-1
17(P-1)
=κ(P - 1)——
z
≤ K(P - 1)X,
a-(P-1)
x-(P -1)
which completes the proof.
□
37
Published as a conference paper at ICLR 2021
LemmaK.9. Let Mo bea PSD matrix with ∣∣M0k2 ≤ L For M(t) := φm(αMo,t) and t ≤ Ta (c),
IlM(t)k2 = λι(M(t)) ≤ gα,c(t)P-1.
Proof. Since ∣∣Vf (MP)∣∣2 ≤ ∣∣Vf (0)k2 + β∣∣M∣∣P ≤ μι + β(λι(M))P, by Lemma K.7, we have
λι(M(t)) ≤ λι(M(0)) + Zt 2(μι + β(λι(M(τ)))P)(λι(M(τ)))pdτ
0
α + 2μι(P — 1) Z
o
dτ
Fa (λι(M (T))
So
Fa(λι(M(t))) ≤ 2μι(P - 1)t.
IfkM(t)∣2 < α, then ∣∣M(t)∣2 ≤ ga,c(t)P-I. If ∣M(t)∣2 ≥ α, then by Lemma K.8,
Fa(∣M(t)k2) ≤ 2μι(P - 1)Ta(c) = GPT)- K(P - 1)c - C-(PT) ≤ Fa(c),
so ∣∣M(t)∣2 ≤ c for all t ≤ T^(c). Applying Lemma K.8 again, we have
α-(pT) -∣M(t)∣-(P-1) ≤ F(∣M(t)∣2) + κ(P - 1)c ≤ 2μι(P - 1)t + κ(P - 1)c,
which implies ∣∣M(t)∣2 ≤ g^,c(t)P-I by definition.
□
Consider the following ODE:
dM = -(Vf(O)MP + MPVf(0)).
We use φm(Mo, t) to denote the solution of M(t) when M(0) = Mo. For diagonal matrix Mo,
M(t) is also diagonal for any t, and itis easy to show that
e>M(t)ei = ∖ ((^e>M°ei)-(PT)-2μi(P-1)t
∣0
1
P-1
~r 二〜二	,„
ei>Mcoei 6= 0,
~r 二〜二	„
ei>Moei = 0.
(35)
Remark K.10. Unlike depth-2 case, the closed form solution, M(t) is only tractable for diagonal
initialization, i.e., (35) (note that the identity matrix is diagonal). And this is the main barrier for
extending our two-phase analysis to the case of general initialization when L ≥ 3. In Appendix L, we
give a more detailed discussion on this barrier.
The following lemma shows that the trajectory of M(t) is close to M (t).
Lemma K.11. Let Mo be a diagonal PSD matrix with ∣∣M0∣∣2 ≤ 1. For M(t) := φm(αMo, t)
and M(t) := φm(αM0,t), we have
∣M (Ta (r)) — M(Ta (r))kF = O(rP +1).
Proof. We bound the difference D := M - M between M and M.
dD
IT
=2 ∣∣Vf(O)(MP - MP) + (Vf(MP) - Vf(O)) Mp∣∣
F
≤2(∣Vf(O)∣2∣MP-McP∣F+∣Vf(MP) -Vf(O)∣F∣MP∣2
≤ 2 (μiPmax^MIlP-1, IIMIIPTHDIf + β ∣M∣2P),
where the last step is by Lemma K.4. This implies that
∣D(t)∣F ≤ ∕[ J dDτ)|F dτ ≤ Lt 2 "Pga,r(τ) ∣D(τ)∣f + βg^,r(τ)PP1) dτ.
38
Published as a conference paper at ICLR 2021
So
T (r)	江 ,	产& (r)
2βga,r (t) P-I exp ( 2μιΡ J	ga,r (T)dτ I dt
/(r) 2βga,r(t产 exp f ɪ ln ^^) dt
P - 1	ga,r(t))
f
kD(Tα(r))kF ≤ /
0
= Z0
= Z0
7^ (r)	P	P
2βgα,r (t) P-I gα,r (Ta (r)) P-I dt
1	，一 ，、、	1	，一 ，、、 P
2β ・ bgα,r(Ta(r))P-I ∙ gα,r(Ta(r))P-I
2μι
P+1
κga,r (Ta (r)) P-I
κrP +1.
which proves the bound.	□
LemmaK.12. Let M(t) = φm(αM0,t), M(t) = φm(αM0,t). If max{∣∣M0k2, ∣∣M0k2} ≤ 1.
For t ≤ Ta (r), we have
rP P
kM(t) — M(t)kF ≤ (α^) e2κr kM(0)- M(0))f∙
_ ~ _ 一 一 ， 、 _________ 、 ，、一 .
Proof. Define D(t) = M(t) - M (t). Then we have
币 =2 KVf(MP) (MP - MP) + (Vf(MP) - Vf(MP)) MP)I
F
≤2kVf(MP)k2kMP-MfPkF+βkMP-MfPkFkMfPk2
≤ 2 (〃1 + βkMfkP + βkMkP) Pmax{kMkP-1, kMkP-1}kDkF,
where the last step is by Lemma K.4. So
IlD(Ta(r))kF ≤kD(0)h exp
2Pμι
/Tα (r)
0
(1 + 2κga,r(t) P-1) ga,r(t)dt
≤ kD(0)kF ∙ exp (PPT ln "'，(TaN)) + 2κg^,r(T^(r))p⅛)
∖P ― 1	ga,r(0)	)
≤ kD(0)kF (α)P e2κrP,
which proves the bound.
□
Let MG(t) := φm 卜eιe>, ^P」)	+1).	Let M(t) := ɪim	MG(t).
Lemma K.13. For every t ∈ (-∞,	+∞),	M(t) exists and	MG(t) converges	to M(t)	in the
following rate:	____
∣∣MG(t) - M (t)∣∣F = O(α).
Proof. Let C be a sufficiently small constant. Let T :
the cases of t ∈ (-∞,T] and t > T respectively.
-K(P-1)C-C-(P-I)
2μι(P-1)
. We prove this lemma in
-(P -1)
Case 1.	Fix t ∈ (-∞, T]. Then 宜(P_、+ t ≤ T^(c). Let a be the unique number such that
K(P - 1)α + α-(P-1) = α-(P-1). Let α0 < α be an arbitrarily small number. Let t0
T^o (a)
(a0)-(P T)-a-(P T)
2μ1(P-1)
. By Lemma K.11 and (35), we have
∣∣φm(α0e1e>,t0) - aeιe>∣∣F ≤ ∣∣φm(α0e1e>,t0) - φm(α0e1e>,t0)∣∣^ ≤ O(αP+1).
39
Published as a conference paper at ICLR 2021
ByLemmaK.9, kφm(α0e1e>,to)k2 ≤ α. Then by Lemma K.12, we have
∣∣φm(α0e1e>,to + t) - φ(αe1e>,t)∣∣F ≤ ( Ca) e2κcP ∙ O(αP+1) = O(α) = O(α).
This implies that {MG(t)} satisfies Cauchy,s criterion for every t, and thus the limit M(t) exists
for t ≤ T. The convergence rate can be deduced by taking limits for a0 → 0 on both sides.
Case 2.	For t = T + T with τ > 0, φm(M ,τ) is locally Lipschitz with respect to M. So
∣∣MG(t)- MGo (t)∣∣F =∣∣φm(MG(T),T) - φm(MGo (T),τ )∣[
=O(∣∣MG(T)- MGo(T)∣∣f)
=O(a),
which proves the lemma for t > T.
Theorem K.14. For every t ∈ (-∞, +∞), as α → 0, we have:
□
α-(P T)
φm αI1, 2μ1(P -1) +t) - M(t)
=O(α P+1),
F
(36)
and for any 2 ≤ k ≤ d,
λk
α一(P T)
2μι(P-1)
a-(P T),
2μ1(P -1) +t
O(a).
(37)
Proof. Let Ma(t) ：= φm (αI,
+ t . Again we let c be a sufficiently small constant and
T：
-K(P-1)C-C-(P T)
2μι (P-1)
.We prove in the cases of t ∈ (-∞,T] and t > T respectively.
Case 1. Fix t ∈ (-∞, T]. Let ɑ1 := αP+1. Let ɑ1 be the unique number such that K(P - I)α1 +
α-(P I) = α-(P 1). Let to := Ta (αj
α-(P τ)-α-(P T)
∣∣φm(aI,to) - α1e1e>∣∣F ≤ ∣∣φm(aI,to)
2μι(P-1)
. Then
-φm(αI, t0UF + ,dm(ai,tO)- α1e1
e1>F
^
=O(aP+1 + a)
=O(a).
ByLemmaK.9, kφm(a0l,to)k2 ≤ a> Then by Lemma K.12, we have
∣∣Ma(t) - MGL (t) ∣∣f = ∣∣φm(aI, tO + t) - φm(a1e1e>, t) ∣∣f
≤ (二)e2κcP ∙ O(a) = O(aP+τ).
Combining this with the convergence rate for MGL (t) proves the bound (36).
For (37), by Lemma K.7, we have
λk-P(Ma(T))- λk-P(Ma(to)) ≤ ZT 2(P - 1) ∣∣ Vf(MP(t))1 dt
t0
TT
≤ / 2(P - I)(μ1 + β∣∣Ma(t)kP))dt
t0
≤ -2(P -I) (μ1(t - TI) + 2 ∙ ga,c(t)P-T).
(38)
ByLemmaK.11, λ1(Mα(T)) = ∣∣Ma(T)∣[ = C + O(CP +1). For k ≥ 2,
λk(M^(T))-(PT) ≥ Ω(α-(PT))- 2(P - 1) "(T - T1) + K ∙ C
≥ Ω(α-(PT))-
α-P+1 - C-(PT)
2μι(P - I)
- O(c)
≥ Ω(α-(P-1)).
40
Published as a conference paper at ICLR 2021
Thus λk(Ma(T)) ≤ O(ɑ).
CaSe 2. Fort = T + T With τ > 0, φm(M, τ) is locally LiPsChitZ With respect to M. So
l∣M^(t) - MG (t)l∣F = ∣∣Φm(M^(T),τ) - φm(M91 (T),τ)∣∣F
=O (∣∣Mα(T)- mGi(T)∣∣f) = O(αP++1),
Which proves the bound (36).
For (37), again by Lemma K.7, We have
λk-P(Ma(T))- λk-P(Ma(T + T))
≤ ZTT+τ 2(P - 1)∣∣Vf(MP(t))∣∣2dt
flT+τ
≤Jt	2(P - 1) (β∣∣Mf (t) - (MG)p(t)∣∣2 + ∣∣Vf((MG)P(t))∣∣2) dt
产十τ
≤ Jt 2(p - i)(o(α 1+1P) + β∣∣MG(t)∣∣2 )dt
≤ O(1).

Thus λk-P(Ma(T + τ)) = Ω(α-(PT)), that is, λk(Ma(T + T)) = O(α), ∀k ≥ 2.	□
P ɪɪɪ,、 一
Proof of Theorem 6.2. Note that(M (t)) = W (t) and
α-(P T)	P	α-(P T)
^P^+1)) = φ(αI, ^P^+1).
By Theorem K.14, We have
α-(1-1∕p)
2μι(p - I)
+1 - W(t)
F
φ αI,
≤
α-(P T)	WP _______ p
2μ1(P - 1) + t)) -(M⑴)
F
(	C^-(P T)	、	__
≤ P φm αI1, 2μi(P---i)+ η- M(t)
11
=O(α p+1 )O(1) = O(α P(p+1)),
and for 2 ≤ k ≤ d,
a-(1-1/P)
λk "1, 2μ1(P - 1) + t λ
α-(P T)
2μι(P - 1) + t
max
F
φm (αI, 2：i(p -1)+1)∣∣2,∣∣M (t)∣∣2)
O(αP) = O(α).
□
L Escaping direction for deep matrix factorization
For deep matrix factoriZation, recall that We only prove that GF With infinitesimal identity initialiZation
escapes in the direction of the top eigenvector. The main burden for us to generaliZe this proof to
general initialiZation is that We don’t knoW hoW to analyZe the early phase dynamics of (29), i.e.,
the analytical solution of (39) is difficult to compute, When L ≥ 3. Intuitively, the direction that the
infinitesimal initialization escapes 0 is exactly M := limt→∞ 口黑缶 ,where M(t) is the solution
of (39). Showing M = v1v> is a critical step in our analysis towards convergence to GLRL.
—=-Vf(0)MLL/ - ML2Vf(O).
(39)
41
Published as a conference paper at ICLR 2021
However, unlike the depth-2 case, M can be different from vιv> even if v>M(0)vι > 0. We here
give an example for diagonal M(0) and Vf (0) at Appendix L.2. Nevertheless, we still conjecture
that except for a zero measure set of M(0), M = vιv>, based on the following theoretical and
experimental evidences:
•	If v>M(0)vι > 0 and rank(M(0)) = 1, we prove that M = vιv>. (See TheOremL.1)
•	For the counter-example, we show experimentally, even with perturbation of only magnitude
10-5, M = vιv>. The results are shown at Figure 7. The y-axis indicateshv1, uι(t)i
where u1(t) is the top eigenvector of M (t). As kW (t)kF becomes larger, u1(t) aligns
better with v1 , which means the noise helps M escaping from v1 . The larger the noise is,
the faster u1 (t) converges to v1.
L.1 Rank-one Case
Theorem L.1 (rank-1 initialization escapes along the top eigenvector). When rank(M (0)) = 1,
limt→∞ kMtjk∣F = vιv>, if v>M(0)vι > 0.
Proof. Let u(0) be the vector such that M(0) = u(0)u(0)> and u(t) ∈ Rd be the solution of
du(t)
dt
ku(t)k2L-2 Vf(0)u(t).
It is easy to check that M(t) = u(t)u(t)> is the solution of (39), because
M = duu> + Udu> = - Vf (0)M(t) ku(t)kL-2 - M(t)Vf (0) ku(t)kL-2
dt	dt	dt
= - Vf(0)ML/2 - ML/2Vf(0).
Let τ(t) = R0t ku(s)k2L-2 ds. Then
du	du dt
——=---------
dτ	dt dτ
— ɪ kukL-2Vf(0)u = -Vf(0)u.
dt
That is, under time rescaling t → τ (t), the trajectory of u(t) still follows the power iteration,
regardless of the depth L.	□
L.2 Counter-example for Escaping Direction
Let Vf (0) = diag(2, 0.9, 0.8, . . . , 0.1) ∈ R10×10 be diagonal. Let W (0) be also diagonal and
W(0)i,i 〜Unif[0.9,1.1] ∙ α for i ∈ [10] \ {2}, W(0)2,2 = 16α, where α = 10-16 is a small
constant. Let the depth be 4.
Lemma L.2. With Vf (0) and W(0) constructed above, vιM(0)v> > 0 and M = vιv>.
Proof. It is easy to check that vι = eι, so vιM(0)v> > 0. Now we prove that M(∞) = vιv>.
As both W(0) and Vf (0) are diagonal, W(t) is always diagonal and has dynamics
dMdtii = -2Vf (0)i,iM(*,	∀i ∈ [10],
therefore we have closed form of M (t):
M(t)i-,i1 =M(0)i-,i1 - 2Vf(0)i,it, ∀i∈ [10].
For i ∈ [10], the time for M(t)i,i going to infinity is (2M(0)i,iVf (0)i,i)-1. By simple calculation,
M(t)2,2 goes to infinity the fastest, thus M = e2e> = vιv>.	□
We remark that the scales of W(0) and Vf (0) do not matter as in gradient flow, as scaling Vf (0) is
equivalent to scaling time (by Lemma L.3 below). And for this reason, the x-axis is the chosen as
kW(0)kF , the relative growth rate.
42
Published as a conference paper at ICLR 2021
∣W(¾∣f
∣W(O)∣∣f
Figure 7: Dynamics of 需=-Vf(0)ML/2 - ML/2Vf (0) plotted, where L = 4, uι(t) is the
top eigenvector of W(t) and is the relative magnitude of noise. The initialization we use in this
experiment is WnOise(0)= W(0) + 等(Z + Z>), where W(0) is what we construct at Appendix L.2,
and Z is a matrix where entries are i.i.d. samples from the standard Gaussian distribution N(0, 1).
We run 5 fixed random seeds (the noise matrix) for each . The trajectory of W is calculated by
simulating gradient flow on M with small timestep and RMSprop (Tieleman and Hinton, 2012) for
faster convergence.
Lemma L.3. Suppose g : Rd → Rd is a P -homogeneous function, that is, g(αθ) = λPg(α) for
any α > 0, and „ = g(θ0(t)). Then αθ0(αPTt) is the solution of
嘤=g(θ(t)),	θ(0) = αθ0(0).	(40)
Proof. Simply plug in θ(t) = αθ0(αP-1t), then we have
曙=d*； Tt) = ɑP [J") = αPg(θ0(αP Tt)) = g(αθ0(αP Tt)) = g(θ(t)).
dt	dt	d(αP-1t)
□
M Proof of Linear Convergence to Minimizer
In this section, we will present the theorems that guarantee the linear convergence to a minimizer
Wo of f(∙) if the dynamics (41) is initialized sufficiently close to Wo, i.e., k W (0) - W°∣∣f is
sufficiently small. In Appendix M.3, we will apply this result to prove Theorem 6.4.
dW = - X W空Vf (W)W2-2i+2 =: g(W).
i=o
(41)
Throughout this section, we assume rank(Wo) = k and use m := λk(Wo) to denote the k-th
smallest non-zero eigenvalue of Wo . The tangent space of manifold of rank-k symmetric matrices at
W0 is T = {VW0r + W0V> : V ∈ Rd×k}. It can be shown that dim(T) = k(d - k) + k(k+1) =
k(2d-k+1)
2	'
Let J(W) be the Jacobian of g(W) in (41). For depth-2 case, we have shown that T is an invariant
subspace of J(W0) in Theorem H.5, property 2. This can be generalize to the deep case where
L ≥ 3. Therefore, we can use J(W0)|T : T → T to denote the linear operator J(W0) restricted on
T. We also define Π1d2 (W) as the projection of W ∈ Rd×d on T, and Π2d2 (W) := W - Π1d2 (W).
Towards showing the main convergence result in the section, we make the following assumption.
Assumption M.1. Suppose J(W0)|T diagonalizable and all eigenvalues are negative real numbers.
W0 is a minimizer, so it is clear that J(W0)|T has no eigenvalues with positive real parts (otherwise
there is a descending direction of f (∙) from Wo, since the loss f (∙) strictly decreases along
43
Published as a conference paper at ICLR 2021
the trajectory of (41)). If further Assumption M.1 holds, then we know J(W0)|T : T → T can
be diagonalized as J(Wo)∣τ[∙ ] = V(ΣV-1( ∙)), where ∑i = diag(-μι,..., -μdim(τ)), V :
Rdim(T) → τ, V(χ) = Pdim(T) Xi Vi, and Vi is the eigenvector associated with eigenvalue -μi.
As shown in Theorem M.3 below, this assumption implies that if W(0) is rank-k and is sufficiently
close to Wo, then ∣∣ W(t) - Wo∣∣f ≤ Ce-μ1t for some constant C. For depth-2 case, the above
assumption is equivalent to that L(U0) is “strongly convex” at U0, except those 0 eigenvalues due
to symmetry, by property 2 of Theorem H.5). For the case where L ≥ 3, because this dynamics is
not gradient flow, in general it does not correspond to a loss function and strongly convexity does
not make any sense. Nevertheless, in experiments we do observe linear convergence to W0 , so this
assumption is reasonable.
M.1 RANK-k INITIALIZATION
For convenience, we define for all W ∈ Sd,
∣W∣V:= V-1Π1d2(W)	,	∣W ∣F,1 := Π1d2 (W)	,	∣W∣F,2:= Π2d2 (W)	.
The reason for such definition of norms, as we will see later, is that the norm (or the difference) in the
tangent space of the manifold of symmetric rank-r matrices, ∣W - W0∣F,1, dominates that in the
orthogonal complement of the tangent space, ∣W - W0 ∣F,2, when both W, W0 get very close to
the W0 (see a more rigorous statement in Lemma M.2). WLOG, we can assume
k∙∣F,1
K
≤k∙∣V ≤k∙∣F,1
for some constant K, which may depend on f and Wo. This also implies that ∣∣∙∣∣v ≤ ∣∣∙∣∣f∙
Below we also assume for sufficiently small R, and any W such that ∣W - W0 ∣F ≤ R, we have
∣∣Vf (W )∣2 ≤ P and ∣∣ J (W )[Δ]∣∣f ≤ β∣∣∆∣∣F for any ∆. In the proof below, we assume such
properties hold as long as we can show the boundedness of W(t) - Wo.
Lemma M.2. Let max{∣ W — WOkF 1, k W0 — WOkF 1} = Ir, When r ≤ 学,we have
∣W - W0∣F,2 ≤ mr ∣W - W0∣F,1
As a special case, we have
∣W - WO ∣F,2 ≤
5 ∣W - W0kF,1
m
Proof. WLOG we can assume WO is only non-zero in the first k dimension, i.e., [WO]ij = 0, for all
i ≥ k + 1, j ≥ k + 1. We further denote W and W0 by
W =	A	B>	and	W0	=	A0	B0>
W =	B	C	and	W	=	B0 C0	,
where A, A0 ∈ Rk×k, B, B0 ∈ R(d-k)×k, C, C0 ∈ R(d-k)×(d-k). By definition, we have
∣A - A0 ∣F , ∣B - B0 ∣F ≤ ∣W - W0 ∣F,1 and ∣W - W0 ∣F,2 = ∣C - C0 ∣F . Moreover, we
have λmin(A) ≥ m -IIA - WOllF ≥ m -k W - Wo∣F,1 ≥ *
Since W,W0 is rank-k, we have C = BA-1B>, C0 = B0A0-1B0>. Thus
kW-W0kF,2
=kC-C0kF
= BA-1B> - B0A0-1B0>
≤ kB - B0kFkA-1B>kF + kBA-1kFkA0 - AkFkA0-1B0>kF + kB0A0-1kFkB> - B0>kF
≤ kW - W0∣F 1 2r + kW - W0∣F 1(2r) + kW - W0∣F1 2r
F,1 m	F,1 m	F,1 m
≤kW - W0kF,1 t
F,1 m
□
44
Published as a conference paper at ICLR 2021
Theorem M.3 (Linear convergence of rank-k matrices). Suppose that rank(W (0)) = rank(W0) =
k and
kw (0)- Wokv ≤ R := max ( ⅛,山，
V	2K K2(29β + 10ρ∕m) J
we have ∣∣ W(t) 一 Wokv ≤ Ce-μ1t k W(0) 一 Wokv forsome Constant C depending on Wo, where
W(t) satisfies (41).
Proof. For convenience, we define W1 (t) := Π1d2 (W (t) 一 Wo) , W2(t) := Π2d2 (W (t) 一 Wo) =
Πd2 (W(t)). We also use〈•，九―=(V-1 (∙), V-1 (∙)) for short.
dkWι(t)kV = dkW(t)- WokV
dt	dt
=2《di dɪ )m2 (W⑴-WQV-I
= 2 DΠd12 (g(W(t))), W1(t)E -1
≤2DΠ1d2(J(Wo)[W(t)-Wo]),W1(t)E -1
+ 2 kg(W(t) -Wo) -J(Wo)[W(t) - Wo]kv kW (t) -Wokv
= 2 DΠd12 (J(Wo)[W1(t)+W2(t)]),W1(t)E -1
+	2 kg(W(t) -Wo) -J(Wo)[W(t) - Wo]kv kW1(t)kv
=2DΠ1d2(J(Wo)[W1(t)]),W1(t)E -1+2kJ(Wo)[W2(t)]kvkW1(t)kv
+	2 kg(W(t) -Wo) -J(Wo)[W(t) - Wo]kv kW1(t)kv.
For the first term Π1d2 (J(Wo)[W1(t)]) , W1(t)	, we know W1(t) ∈ T, and T is an invariant
space of J(Wo). Recall J(Wo)∣τ[∙] = V (∑V-1 (∙)), We have
2 D∏d2 (J(Wo)[W1(t)]), Wi(t»y-1 = 2 (∑V-1 (Wι(t)), V-1 (W1(t))) ≤ -2μ1 k W1(t)∣∣F,1.
For the second term 2β kJ(Wo)[W2(t)]kv kW1(t)kv, We have
2kJ(Wo)[W2(t)]kv ≤2kJ(Wo)[W2(t)]kF ≤2kJ(Wo)k2kW2(t)kF=2ρkW2(t)kF.
For the third term 2 kg(W (t) - Wo) -J(Wo)[W(t) - Wo]kv kW1(t)kv,Wehave
2 kg(W(t) -Wo) -J(Wo)[W(t) -Wo]kv ≤ 2β kW (t) -Wok2F
≤ 4β(kW1(t)k2F + kW2(t)k2F)
≤ 4β(K2 kW1(t)k2v + kW2(t)k2F).
Thus We have shoWn the folloWing. Note so far We have not used the assumption that W is rank-k.
d kWt(t)kV ≤ -2μι k Wi(t)kV+2 k Wι(t)kv (P k W2(t)∣∣F + 2βK2 k Wι(t)kV + 2β k W2(t)∣∣F),
that is,
d⅛W< ≤-2”ι +4βK2 kW1(t)kv + ≡W⅛+∣≡>.	(42)
Let T := sup{t ≥ 0 ： k Wι(t)kv ≤ 2mK}. Setting W0 = Wo in Lemma M.2, We have for t < T,
r = kW(t)-丽∣∣f,i ≤ kW(t) - WOkF ≤ K kW(t) - Wokv ≤ 贤.Thus,
kW2(t)kF = kW2(t)kF,2 ≤ 5 kW(t： WOkFj ≤ 5K2 kW(m -WOkV ≤ 5m.
45
Published as a conference paper at ICLR 2021
Thus, from (42) we can derive that
dlog kW1(t)kV ≤ -2μι + K2(29β + I0ρ∕m)) ∣∣皿乂制卜 ≤ -μ1.
(43)
Since μι < 0, ∣∣Wι(t)∣∣v decreases for [0,T). Thus T must be ∞, otherwise ∣∣Wι(T)|卜
limt→T - kW1(t)kV < R1. Contradiction.
Therefore, for any t ∈ [0, ∞), we have ∣∣Wι(t)∣∣v ≤ ∣∣Wi(0)∣∣v e-号t. Thatis,
I ∞ ∣Wι(t)kv dt ≤ - ∣Wι(0)kv ≤ 2R.
Jo	μι	μι
Thus from (43), we have
∣W(t)kv = ∣Wι(t)kv ≤ ∣Wι(0)kvexp -μιt +
≤ l∣Wι(0)kv exp (—μιt +
=:CkW (0)∣v e-μιt,
K2	∞
ɪ(29β +10ρ∕m)/ ∣Wι(t)kv dt
KR (29β + 1
μι
which completes the proof.
□
M.2 ALMOST RANK-k INITIALIZATION
We use M(t) to denote the top-k components of W(t) in SVD, and N(t) to denote the rest part,
i.e., W(t) - M (t). One can think M(t) as the main part and N(t) as the negligible part.
Below we show that for deep overparametrized matrix factorization, where W(t) satisfies (41), if
the trajectory is initialized at some W(0) in a small neighborhood of the k-th critical point W0
of deep GLRL, and W(0) is approximately rank-k, in the sense that N (0) is very small, then
inf t≥0 kW (t) - W0kV is roughly at the same magnitude of N (0).
Theorem M.4 (Linear convergence of almost rank-k matrices, deep case). Suppose W0 is a critical
point of rank k and W0 satisfies Assumption M.1, there exists constants C0 and r, such that if
C0 kN (0)kF ≤ kW1 (0)kV ≤ r, then there exists a time T and constants C, C0, such that
(1)	. kW (t) - Wokv ≤ Ce-μιt/2 ∣W (0) - Wok V ,for t ≤ T
(2)	. kW(T)-W0kF ≤C0kN(0)kF.
Proof. When ∣∣ W (t) 一 WOkF ≤ λmin(W01), kN (t)kF ≤ λmin( W0) ,thus we have
kM(t) - WokF,ι ≤ kW(t) - WokF,ι + kN(t)kF,ι ≤ λmm(W0),
thus by Lemma M.2, we have
kW2(t)kF,2 ≤
≤
kM(t) - WokF,2 + kN (t)kF,2
5 kM(t) - WOkFJ
-λmin (Wo)-
+ kN (t)kF,2
≤
10 kW1(t)kF,1 + 10 kN(t)kF
λmin(Wθ)
+ kN (t)kF,2
≤
10K2kW1(t)kV + 10 kN(t)kF
λmin(Wθ)
+ kN (t)kF,2 .
Thus we can pick constant Co large enough and r small enough, such that for any t ≥ 0, if
Co kN (t)kF ≤ kW1(t)kV ≤ r, then it holds that:
46
Published as a conference paper at ICLR 2021
•	The “small terms” in the RHS of (42) satisfies that
4βK2 kWι(t)kv + 4β kW2% +IPk.("kF ≤ Ci kWι(t)kv + C2 kN(t)kF ≤ μι
kW1 (t)kV
for some C1 and C2 independent of t.
•	The spectral norm 2 ∣∣Vf (W(t))k2 ≤ ∣∣Vf (W0)k2 =: P for all t ≥ 0.
.._.2-1	L-2
•	∀x < r, ⅛-2κ > μ ln Crx, Where KL =1 - 0.5 F.
Note these conditions can alWays be satisfied by some C0 and r because We can first find 3 groups
(C0 , r) to satisfy each individual condition, and then take the maximal C0 and minimal r, it’s
easy to check these conditions are still verified. And We let TC0 ,r be the earliest time that such
condition, i.e., C0 kN (t)kF ≤ kW1(t)kV ≤ r fails. Thus by (42), for t ∈ [0, TC0 ,r), We have
kW(t)kv = kWi(t)kv ≤ kW1(0)kv e-μ21t = kW(0)kv e-μ1t. Thus(I) holdsforany T smaller
than TC0 ,r. If TC0 ,r = ∞, then clearly We can pick a sufficiently large T, such that (2) holds.
Therefore, beloW it suffices to consider the case Where TC0 ,r is finite. And We knoW the condition
that fails must be C0 kN (t)kF ≤ kW1(t)kV,i.e. C0kN(TC0,r)kF = kW1(TC0,r)kV.
By (34) in Lemma K.7, We have
∣kN (0)k22-1 -kN (t)k22-1∣ ≤ (L - 2)ρt.
2 -ι
Define T0 := KLkN-2)p——,we know for any t < T0, we have ∣kN(0)kLT -∣∣N(t)kL-1∣ ≤
2_ _i
KL ∣∣N(t)kL . That is,
2 _1
kN (t)kL 1 U
2 _i ∈
kN (0)kL 1
1 - κL,
1 - KL
L-2
0.5 ɪ, 0.5-
kN (t)k2
kN (0)k2
∈ [1/2, 2].
Now we claim it must hold that T0 ≥ TC0 ,r. Otherwise, we have
C20 kN(0)k2 ≤ Co kN(T0)If ≤ kWι(T0)kv ≤ e-μ1T0/2 kWι(0)kv ≤ e-μ1T0/2τ.
L ,、,y-1
Therefore, KLkN-2)p—— =T0 ≤ 言 ln CO ∣∣N(0)k , which contradicts to the definition of Co and r.
As a result, we have
2Co√d∣N(0)∣2 ≥ 2Co ∣N(0)∣∣f ≥ C ∣∣N(Tci肌=kWi(TCi)kv
≥ kWi(0)kv e"μιTcθ,r/2,
and therefore,
T ≤ 2ln	kWi(0)kv
c",r — μι	2√dC0 kN (0)∣F.
Thus by Lemma M.2, we know
kW(TC0,r)-WokF ≤ kW(TC0,r)-WokF,i+kW(TC0,r)-WokF,2
≤KkWi(TC0,r)kV+kM(TC0,r) -WokF,2+kN(TC0,r)kF,2
≤O(kN(0)kF)+O(kN(0)k2F)+O(kN(0)kF)
= O(kN(0)kF).
□
47
Published as a conference paper at ICLR 2021
M.3 Proof for Theorem 6.4
Prooffor Theorem 6.4. Let Co,r be the constants predicted by Theorem M.4 w.r.t. to W(∞).
We claim that we can pick large enough constant T , and α0 sufficiently small, such that for all
α ≤ α0, the initial condition in Theorem M.4 holds, i.e. C0 kN (0)kF ≤ kW1(0)kV ≤ r, where
W(0) := φ(αI, 2α-(pP-⅛ + t).
This is because we can first ensure ∣∣ W(T) - W(∞)/ is SUfficiently small, i.e., smaller than 2.
By Theorem 6.2, we know when ɑ → 0, ∣∣ W(T) - W(0)∣∣v ≤ K ∣∣ W(T) - W(0)∣∣f = o(1) and
kN(0)kF = O(α).
By Theorem M.4, we know there is a time T (either TC0,r or some sufficiently large number when
Tco,r = ∞), such that kW(T) - Wo∣∣F =O(IlN(0)IlF) = O(α).	□
48