Published as a conference paper at ICLR 2021
Understanding the failure modes of out-of-
DISTRIBUTION GENERALIZATION
Vaishnavh Nagarajan*
Carnegie Mellon University
vaishnavh@cs.cmu.edu
Anders Andreassen
Blueshift, Alphabet
ajandreassen@google.com
Behnam Neyshabur
Blueshift, Alphabet
neyshabur@google.com
Ab stract
Empirical studies suggest that machine learning models often rely on features,
such as the background, that may be spuriously correlated with the label only dur-
ing training time, resulting in poor accuracy during test-time. In this work, we
identify the fundamental factors that give rise to this behavior, by explaining why
models fail this way even in easy-to-learn tasks where one would expect these
models to succeed. In particular, through a theoretical study of gradient-descent-
trained linear classifiers on some easy-to-learn tasks, we uncover two complemen-
tary failure modes. These modes arise from how spurious correlations induce two
kinds of skews in the data: one geometric in nature, and another, statistical in na-
ture. Finally, we construct natural modifications of image classification datasets
to understand when these failure modes can arise in practice. We also design ex-
periments to isolate the two failure modes when training modern neural networks
on these datasets.* 1
1	Introduction
A machine learning model in the wild (e.g., a self-driving car) must be prepared to make sense of
its surroundings in rare conditions that may not have been well-represented in its training set. This
could range from conditions such as mild glitches in the camera to strange weather conditions. This
out-of-distribution (OoD) generalization problem has been extensively studied within the framework
of the domain generalization setting (Blanchard et al., 2011; Muandet et al., 2013). Here, the clas-
sifier has access to training data sourced from multiple “domains” or distributions, but no data from
test domains. By observing the various kinds of shifts exhibited by the training domains, we want
the classifier can learn to be robust to such shifts.
The simplest approach to domain generalization is based on the Empirical Risk Minimization (ERM)
principle (Vapnik, 1998): pool the data from all the training domains (ignoring the “domain label”
on each point) and train a classifier by gradient descent to minimize the average loss on this pooled
dataset. Alternatively, many recent studies (Ganin et al., 2016; Arjovsky et al., 2019; Sagawa et al.,
2020a) have focused on designing more sophisticated algorithms that do utilize the domain label on
the datapoints e.g., by enforcing certain representational invariances across domains.
A basic premise behind pursuing such sophisticated techniques, as emphasized by Arjovsky et al.
(2019), is the empirical observation that ERM-based gradient-descent-training (or for convenience,
just ERM) fails in a characteristic way. As a standard illustration, consider a cow-camel classi-
fication task (Beery et al., 2018) where the background happens to be spuriously correlated with
the label in a particular manner only during training — say, most cows are found against a grassy
background and most camels against a sandy one. Then, during test-time, if the correlation is com-
pletely flipped (i.e., all cows in deserts, and all camels in meadows), one would observe that the
* Work performed in part while Vaishnavh Nagarajan was interning at Blueshift, Alphabet.
1Code is available at https://github.com/google-research/OOD-failures
1
Published as a conference paper at ICLR 2021
Spurious

optimal
classifier fails.
(c) ResNet on CIFAR10 with
spuriously colored lines
(d) OoD accuracy drop
despite no color-label correlation
Invariant
SPuriOuS、，
Max-margin
Yet, classifiers
in practice fail.
(b) This work: Fully predictive
invariant feature
(a) Earlier work: Partially
predictive invariant feature
Figure 1: Unexplained OoD failure: Existing theory can explain why classifiers rely on the spurious
feature when the invariant feature is in itself not informative enough (Fig 1a). But when invariant
features are fully predictive of the label, these explanations fall apart. E.g., in the four-point-dataset
of Fig 1b, one would expect the max-margin classifier to easily ignore spurious correlations (also see
Sec 3). Yet, why do classifiers (including the max-margin) rely on the spurious feature, in so many
real-world settings where the shapes are perfectly informative of the object label (e.g., Fig 1c)? We
identify two fundamental factors behind this behavior. In doing so, we also identify and explain
other kinds of vulnerabilities such as the one in Fig 1d (see Sec 4).
accuracy of ERM drops drastically. Evidently, ERM, in its unrestrained attempt at fitting the data,
indiscriminately relies on all kinds of informative features, including unreliable spurious features
like the background. However, an algorithm that carefully uses domain label information can hope
to identify and rely purely on invariant features (or “core” features (Sagawa et al., 2020b)).
While the above narrative is an oft-stated motivation behind developing sophisticated OoD general-
ization algorithms, there is little formal explanation as to why ERM fails in this characteristic way.
Existing works (Sagawa et al., 2020b; Tsipras et al., 2019; Arjovsky et al., 2019; Shah et al., 2020)
provide valuable answers to this question through concrete theoretical examples; however, their ex-
amples critically rely on certain factors to make the task difficult enough for ERM to rely on the
spurious features. For instance, many of these examples have invariant features that are only par-
tially predictive of the label (see Fig 1a). Surprisingly though, ERM relies on spurious features even
in much easier-to-learn tasks where these complicating factors are absent — such as in tasks with
fully predictive invariant features e.g., Fig 1c or the Waterbirds/CelebA examples in Sagawa et al.
(2020a) or for that matter, in any real-world situation where the object shape perfectly determines
the label. This failure in easy-to-learn tasks, as we argue later, is not straightforward to explain
(see Fig 1b for brief idea). This evidently implies that there must exist factors more general and
fundamental than those known so far, that cause ERM to fail.
Our goal in this work is to uncover these fundamental factors behind the failure of ERM. The hope is
that this will provide a vital foundation for future work to reason about OoD generalization. Indeed,
recent empirical work (Gulrajani & Lopez-Paz, 2020) has questioned whether existing alternatives
necessarily outperform ERM on OoD tasks; however, due to a lack of theory, it is not clear how to
hypothesize about when/why one algorithm would outperform another here. Through our theoretical
study, future work can hope to be better positioned to precisely identify the key missing components
in these algorithms, and bridge these gaps to better solve the OoD generalization problem.
Our contributions. To identify the most fundamental factors causing OoD failure, our strategy is
to (a) study tasks that are “easy” to succeed at, and (b) to demonstrate that ERM relies on spurious
features despite how easy the tasks are. More concretely:
1.	We formulate a set of constraints on how our tasks must be designed so that they are easy to
succeed at (e.g., the invariant feature must be fully predictive of the label). Notably, this class of
easy-to-learn tasks provides both a theoretical test-bed for reasoning about OoD generalization
and also a simplified empirical test-bed. In particular, this class encompasses simplified MNIST
and CIFAR10-based classification tasks where we establish empirical failure of ERM.
2.	We identify two complementary mechanisms of failure of ERM that arise from how spurious
correlations induce two kinds of skews in the data: one that is geometric and the other statistical.
In particular, we theoretically isolate these failure modes by studying linear classifiers trained by
2
Published as a conference paper at ICLR 2021
gradient descent (on logistic/exponential loss) and its infinite-time-trained equivalent, the max-
margin classifier (Soudry et al., 2018; Ji & Telgarsky, 2018) on the easy-to-learn-tasks.
3.	We also show that in any easy-to-learn task that does not have these geometric or statistical skews,
these models do not rely on the spurious features. This suggests that these skews are not only a
sufficient but also a necessary factor for failure of these models in easy-to-learn tasks.
4.	To empirically demonstrate the generality of our theoretical insights, we (a) experimentally vali-
date these skews in a range of MNIST and CIFAR10-based tasks and (b) demonstrate their effects
on fully-connected networks (FNNs) and ResNets. We also identify and explain failure in scenar-
ios where standard notions of spurious correlations do not apply (see Fig 1d). We perform similar
experiments on a non-image classification task in App E.
2	Related Work
Spurious correlations. Empirical work has shown that deep networks find superficial ways to
predict the label, such as by relying on the background (Beery et al., 2018; Ribeiro et al., 2016) or
other kinds of shortcuts (McCoy et al., 2019; Geirhos et al., 2020). Such behavior is of practical
concern because accuracy can deteriorate under shifts in those features (Rosenfeld et al., 2018;
Hendrycks & Dietterich, 2019). It can also lead to unfair biases and poor performance on minority
groups (Dixon et al., 2018; Zhao et al., 2017; Sagawa et al., 2020b).
Understanding failure of ERM. While the fact that ERM relies on spurious correlations has be-
come empirical folk wisdom, only a few studies have made efforts to carefully model this. Broadly,
there are two kinds of existing models that explain this phenomenon. One existing model is to imag-
ine that both the invariant and the spurious features are only partially predictive of the label (Tsipras
et al., 2019; Sagawa et al., 2020b; Arjovsky et al., 2019; Ilyas et al., 2019; Khani & Liang, 2020),
as a result of which the classifier that maximizes accuracy cannot ignore the spurious feature (see
Fig 1a). The other existing model is based on the “simplicity bias” of gradient-descent based deep
network training (Rahaman et al., 2018; Neyshabur et al., 2015; Kalimeris et al., 2019; Arpit et al.,
2017; Xu et al., 2019; des Combes et al., 2018). In particular, this model typically assumes that
both the invariant and spurious features are fully predictive of the label, but crucially posits that the
spurious features are simpler to learn (e.g., more linear) than the invariant features, and therefore
gradient descent prefers to use them (Shah et al., 2020; Nar et al., 2019; Hermann & Lampinen,
2020).
While both these models offer simple-to-understand and useful explanations for why classifiers may
use spurious correlations, we provide a more fundamental explanation. In particular, we empirically
and theoretically demonstrate how ERM can rely on the spurious feature even in much easier tasks
where these explanations would fall apart: these are tasks where unlike in the first model (a) the
invariant feature is fully predictive and unlike in the second model, (b) the invariant feature corre-
sponds to a simple linear boundary and (c) the spurious feature is not fully predictive of the label.
Further, we go beyond the max-margin settings analyzed in these works to analyze the dynamics of
finite-time gradient-descent trained classifier on logistic loss. We would also like to point the reader
to concurrent work of Khani & Liang (2021) that has proposed a different model addressing the
above points. While their model sheds insight into the role of overparameterization in the context of
spurious features (and our results are agnostic to that), their model also requires the spurious feature
to be “dependent” on the invariant feature, an assumption we don’t require (see Sec 3).
Algorithms for OoD generalization. Due to the empirical shortcomings of ERM, a wide range of
sophisticated algorithms have been developed for domain generalization. The most popular strat-
egy is to learn useful features while constraining them to have similar distributions across domains
(Ganin et al., 2016; Li et al., 2018b; Albuquerque et al., 2020). Other works constrain these features
in away that one can learn a classifier that is simultaneously optimal across all domains (Peters et al.,
2016; Arjovsky et al., 2019; Krueger et al., 2020). As discussed in Gulrajani & Lopez-Paz (2020),
there are also many other existing non-ERM based methods, including that of meta-learning (Li
et al., 2018a), parameter-sharing (Sagawa et al., 2020a) and data augmentation (Zhang et al., 2018).
Through their extensive empirical survey of many of the above algorithms, Gulrajani & Lopez-Paz
(2020) suggest that ERM may be just as competitive as the state-of-the-art. But we must emphasize
that this doesn’t vindicate ERM of its failures but rather indicates that we may be yet to develop a
substantial improvement over ERM.
3
Published as a conference paper at ICLR 2021
3	Easy-to-learn domain generalization tasks
Below, we first set up the basic domain generalization setting and the idea of ERM. Then, in Sec-
tion 3.1, we formulate a class of domain-generalization tasks that are in many aspects “easy” for
the learner (such as fully informative invariant features) - What exactly makes a task “easy” will be
discussed in Section 3.1). This discussion sets the ground for the later sections to show how ERM
can fail even in these easy tasks, which will help uncover the fundamental factors behind its failure.
Notations. Consider an input (vector) space X and label space Y ∈ {-1, 1}. For any distribution
D over X X Y, let PD(∙) denote its probability density function (PDF). Let H denote a class of
classifiers h : X → R. Let the error of h on D be denoted as LD(h) := E(x,y)〜D [h(x) ∙ y < 0].
The domain generalization setting and ERM. In the domain generalization setting, one considers
an underlying class D of data distributions over X × Y corresponding to different possible domains.
The learner is given training data collected from multiple distributions from D. For an ERM-based
learner in particular, the training data will be pooled together, so we can model the data as com-
ing from a single (pooled) distribution Dtrain, which for simplicity, can be assumed to belong to D.
Given this data, the learner outputs a hypothesis h ∈ H that is tested on a new distribution Dtest
picked from D. This can be potentially modeled by assuming that all test and training distributions
are drawn from a common hyper-distribution over D. However, this assumption becomes pointless
in most practical settings where the training domains are not more than three to four in number
(e.g., PACS (Asadi et al., 2019), VLCS (Fang et al., 2013)), and therefore hardly representative
of any hyper-distribution. Here, the problem becomes as hard as ensuring good performance on
a worst-case test-distribution without any hyper-distribution assumption; this boils down to mini-
mizing maxD∈D LD(h). Indeed, most works have studied the worst-case setting, both theoretically
(Sagawa et al., 2020b) and empirically (Arjovsky et al., 2019; Sagawa et al., 2020a).
Similarly, for this work, we focus on the worst-case setting and define the optimal target function h?
to be h? = arg minh∈H maxD∈D LD(h). Then, we define the features that this “robust” classifier
uses as invariant features Xinv (e.g., the shape of the object), and the rest as spurious features Xsp
(e.g., the background). To formalize this, we assume that there exists a mapping Φ : Xinv × Xsp → X
such that each D ∈ D is induced by a distribution over Xinv × Xsp (so we can denote any x as
Φ(xinv, Xsp)). With an abuse of notation we will use PD (∙) to also denote the PDF of the distribution
over Xinv × Xsp. Then, the fact that Xsp are features that h? does not rely on, is mathematically stated
as: ∀Xinv and ∀XSP = XsP, h*(Φ(xmv, XSP)) = h*(Φ(xinv, xsp)). Finally, we note that, to make
this learning problem tractable, one has to impose further restrictions; we’ll provide more details on
those when we discuss the class of easy-to-learn domain generalization tasks in Sec 3.1.
Empirical failure of ERM. To guide us in constructing the easy-to-learn tasks, let us ground our
study in a concrete empirical setup where an ERM-based linear classifier shows OoD failure. Specif-
ically, consider the following Binary-MNIST based task, where the first five digits and the remaining
five digits form the two classes. First, we let Φ be the identity mapping, and so X = (Xinv, Xsp). Then,
we let Xinv be a random ReLU features representation of the MNIST digit i.e., if Xraw represents the
MNIST image, then Xinv = ReLU(WXraw) where W is a matrix with Gaussian entries. We make
this representation sufficiently high-dimensional so that the data becomes linearly separable. Next,
we let the spurious feature take values in {+B, -B} for some B > 0, imitating the two possible
background colors in the camel-cow dataset. Finally, on Dtrain, for any y, we pick the image Xinv
from the corresponding class and independently set the “background color” xsp so that there is some
spurious correlation i.e., PrDtrain [xsp ∙ y > 0] > 0.5. During test time however, we flip this correla-
tion around so that PrDteSJxSP ∙ y > 0] = 0.0. In this task, we observe in Fig 2a (shown later under
Sec 4) that as we vary the train-time spurious correlation from none (PrDtrain [xsp ∙ y > 0] = 0.5) to
its maximum (PrDtrain [xsp ∙ y > 0] = 1.0), the OoD accuracy of a max-margin classifier progres-
sively deteriorates. (We present similar results for a CIFAR10 setting, and all experiment details in
App C.1.) Our goal is now to theoretically demonstrate why ERM fails this way (or equivalently,
why it relies on the spurious feature) even in tasks as “easy-to-learn” as these.
3.1	Constraints defining easy-to-learn domain-generalization tasks.
To formulate a class of easy-to-learn tasks, we enumerate a set of constraints that the tasks must
satisfy; notably, this class of tasks will encompass the empirical example described above. The
4
Published as a conference paper at ICLR 2021
motivation behind this exercise is that restricting ourselves to the constrained set of tasks yields
stronger insights — it prevents us from designing complex examples where ERM is forced to rely
on spurious features due to a not-so-fundamental factor. Indeed, each constraint here forbids a
unique, less fundamental failure mode of ERM from occuring in the easy-to-learn tasks.
Constraint 1. (Fully predictive invariant features.) For all D ∈ D, LD (h?) = 0.
Arguably, our most important constraint is that the invariant features (which is what h? purely relies
on) are perfectly informative of the label. The motivation is that, when this is not the case (e.g.,
as in noisy invariant features like Fig 1a or in Sagawa et al. (2020b); Tsipras et al. (2019)), failure
can arise from the fact that the spurious features provide vital extra information that the invariant
features cannot provide (see App A for more formal argument). However this explanation quickly
falls apart when the invariant feature in itself is fully predictive of the label.
Constraint 2. (Identical invariant distribution.) Across all D ∈ D, pD (xinv) is identical.
This constraint demands that the (marginal) invariant feature distribution must remain stable across
domains (like in our binary-MNIST example). While this may appear to be unrealistic, (the exact
distribution of the different types of cows and camels could vary across domains), we must empha-
size that it is easier to make ERM fail when the invariant features are not stable (see example in
App A Fig 4a).
Constraint 3. (Conditional independence.) For all D ∈ D, xsp ⊥ xinv |y .
This constraint reflects the fact that in the MNIST example, we chose the class label and then picked
the color feature independent of the actual hand-written digit picked from that class. This prevents us
from designing complex relationships between the background and the object shape to show failure
(see example in App A Fig 4b or the failure mode in Khani & Liang (2021)).
Constraint 4. (Two-valued spurious features.) We set Xsp = R and the support of xsp in Dtrain is
{-B, +B}.
This constraint2 captures the simplicity of the cow-camel example where the background color is
limited to yellow/green. Notably, this excludes failure borne out of high-dimensional (Tsipras et al.,
2019) or carefully-constructed continuous-valued (Sagawa et al., 2020b; Shah et al., 2020) spurious
features.
Constraint 5. (Identity mapping.) Φ is the identity mapping i.e., x = (xinv, xsp).
This final constraint3, also implicitly made in Sagawa et al. (2020b); Tsipras et al. (2019), prevents
ERM from failing because of a hard-to-disentangle representation (see example in App A Fig 4c).
Before we connect these constraints to our main goal, it is worth mentioning their value beyond that
goal. First, as briefly discussed above and elaborated in App A, each of these constraints in itself
corresponds to a unique failure mode of ERM, one that is worth exploring in future work. Second,
the resulting class of easy-to-learn tasks provides a theoretical (and a simplified empirical) test-bed
that would help in broadly reasoning about OoD generalization. For example, any algorithm for
solving OoD generalization should at the least hope to solve these easy-to-learn tasks well.
Why is it hard to show that ERM relies on the spurious feature in easy-to-learn tasks? Consider
the simplest easy-to-learn 2D task. Specifically, during training we set xinv = y (and so Constraint 1
is satisfied) and xsp to be yB with probability p ∈ [0.5, 1) and -yB with probability 1 - p (hence
satisfying both Constraint 3 and 4). During test-time, the only shifts allowed are on the distribution
of xsp (to respect Constraint 2). Observe from Fig 1b that this distribution has a support of the four
points in {-1, +1} × {-B, +B} and is hence an abstract form of the cow-camel dataset which
also has four groups of points: (a majority of) cows/camels against grass/sand and (a minority of)
cows/camels against sand/grass. Fitting a max-margin classifier on Dtrain leads us to a simple yet
key observation: owing to the geometry of the four groups of points, the max-margin classifier has
no reliance on the spurious feature despite the spurious correlation. In other words, even though
this dataset distills what seem to be the core aspects of the cow/camel dataset, we are unable to
reproduce the corresponding behavior of ERM. In the next two sections, we will try to resolve this
apparent paradox.
2Note that the discrete-value restriction holds only during training. This is so that, in our experiments, we
can study two kinds of test-time shifts, one within the support {-B, +B} and one outside ofit (the vulnerability
to both of which boils down to the level of reliance on xsp).
3The rest of our discussion would hold even if Φ corresponds to any orthogonal transformation of (xinv, xsp),
since the algorithms we study are rotation-invariant. But for simplicity, we’ll work with the identity mapping.
5
Published as a conference paper at ICLR 2021
Spurious
Minority
(b) `2 norm of max-margin
∙∙∙∙∙
Majority
(a) OOD failure in Sec 3
Binary-MNIST example
；★
. Minority
Max-margin
classifier
Invariant
Majority
Training
distribution
Accuracy
83.7%
Test
distribution
Accuracy
57.6% ± 19.5%
(c)	Geometric failure
mechanism
(d)	CIFAR10 example with a spurious
line
Figure 2: Geometric skews: Fig 2a demonstrates failure of the max-margin classifier in the easy-
to-learn Binary-MNIST task of Sec 3. Fig 2b forms the basis for our theory in Sec 4 by showing
that the random-features-based max-margin has norms increasing with the size of the dataset. In
Fig 2c, we visualize the failure mechanism arising from the “geometric skew”: with respect to the
xinv-based classifier, the closest minority point is farther away than the closest majority point, which
crucially tilts the max-margin classifier towards wsp > 0. Fig 2d, as further discussed in Sec 4,
stands as an example for how a wide range of OoD failures can be explained geometrically.
4	Failure due to geometric skews
A pivotal piece in solving this puzzle is a particular geometry underlying how the invariant features
in real-world distributions are separated. To describe this, consider the random features representa-
tion of MNIST (i.e., xinv) and fit a max-margin classifier on it, i.e., the least-norm winv that achieves
a margin of y ∙ (Winv ∙ Xinv + b) ≥ 1 on all data (for some b). Then, we'd observe that as the number
of training points increase, the `2 norm of this max-margin classifier grows (see Figure 2b); similar
observations also hold for CIFAR10 (see App C.2). This observation (which stems from the geom-
etry of the dataset) builds on ones that were originally made in Neyshabur et al. (2017); Nagarajan
& Kolter (2017; 2019) for norms of overparameterized neural networks. Our contribution here is to
empirically establish this for a linear overparameterized model and then to theoretically relate this
to OoD failure. In the following discussion, we will take this “increasing norms” observation as a
given4), and use it to explain why the max-margin classifier trained on all the features (including
xsp) relies on the spurious feature.
Imagine every input Xinv to be concatenated with a feature xsp ∈ {-B, B} that is spuriously corre-
lated with the label i.e., Pr[xsp ∙y > 0] > 0.5. The underlying spurious correlation implicitly induces
two disjoint groups in the dataset S: a majority group Smaj where XSp ∙ y > 0 e.g., cows/camels with
green/yellow backgrounds, and a minority group Smin where XSp ∙ y < 0 e.g., cows/camels with
yellow/green backgrounds. Next, let Wall ∈ Xinv denote the least-norm vector that (a) lies in the
invariant space and (b) classifies all of S by a margin of at least 1. Similarly, let Wmin ∈ Xinv
denote a least-norm, purely-invariant vector that classifies all of Smin by a margin of at least 1. Cru-
cially, since Smin has much fewer points than S, by the “increasing-norm” property, we can say that
kWmin k	kWall k. We informally refer to the gap in these `2 norms as a geometric skew.
Given this skew, we explain why the max-margin classifier must use the spurious feature. One way to
classify the data is to use only the invariant feature, which would cost an `2 norm of kWallk. But there
is another alternative: use the spurious feature as a short-cut to classify the majority of the dataset
Smaj (by setting wsp > 0) and combine it with Wmin to classify the remaining minority Smin. Since
kWmin k	kWall k, the latter strategy requires lesser `2 norm, and is therefore the strategy opted by
the max-margin classifier. We illustrate this failure mechanism in a 2D dataset in Fig 2c. Here, we
have explicitly designed the data to capture the “increasing norms” property: the distance between
purely-invariant classifier boundary (i.e., a vertical separator through the origin) and the closest point
in Smaj is smaller than that of the closest point in Smin. In other words, a purely-invariant classifier
4To clarify, we take the “increasing norms” observation as a given in that we don’t provide an explanation
for why it holds. Intuitively, we suspect that norms increase because as we see more datapoints, we also sample
rarer/harder training datapoints. We hope future work can understand this explain better.
6
Published as a conference paper at ICLR 2021
would require much greater norm to classify the majority group by a margin of 1 than to classify the
minority group. We can then visually see that the max-margin classifier would take the orientation
of a diagonal separator that uses the spurious feature rather than a vertical, purely-invariant one.
The following result formalizes the above failure. In particular, for any arbitrary easy-to-learn task,
we provide lower and upper bounds on wsp that are larger for smaller values of kwmink/kwallk. For
readability, we state only an informal, version of our theorem below. In App B.1, we present the
full, precise result along with the proof.
Theorem 1. (informal) Let H be the set of linear classifiers, h(x) = winvxinv + wspxsp + b. Then
for any task satisfying all the constraints in Sec 3.1 with B = 1, the max-margin classifier satisfies:
1	- 2Pkikwallk ≤ wsp ≤ kwmink/kwaiik - L
A salient aspect of this result is that it explains the varying dynamics between the underlying spu-
rious correlation and spurious-feature-reliance in the classifier. First, as the correlation increases
(PrDtrain Ep ∙ y > 0] → 1.0), the size of the minority group decreases to zero. Then, We empirically
know that kwmin k/kwall k progressively shrinks all the way down to 0. Then, we can invoke the
loWer bound Which implies that wsp groWs to ≈ 1. This implies serious vulnerability to the test-time
shifts: any flip in the sign of the spurious feature can reduce the original margin of≈ 1 by a value of
2|wsp xsp | ≈ 2 (since B = 1 here) making the margin negative (implying misclassification). On the
other hand, when spurious correlations diminish (PrDtrain [xsp ∙ y > 0] → 0.5), the value of ∣∣Wmin∣∣
groWs comparable to kwallk, and our upper bound suggests that the spurious component must shrink
towards ≈ 0, thereby implying robustness to these shifts.
Broader empirical implications. While our theorem explains failure in linear, easy-to-learn set-
tings, the underlying geometric argument can be used to intuitively understand failure in more gen-
eral settings i.e., setting where the classifier is non-linear and/or the task is not easy-to-learn. For
illustration, we identify a few such unique non-linear tasks involving the failure of a neural network.
The first two tasks below can be informally thought of as easy-to-learn tasks5:
•	In Fig 1c, we consider a CIFAR10 task where we add a line to with its color spuriously correlated
with the class only during training. The & 10% OoD accuracy drop of a ResNet here, we argue (in
App C.3.1), arises from the fact that it takes greater norms for the ResNet to fit larger proportions
CIFAR10.
•	In Fig 1d, we consider a colored Cats vs. Dogs task (Elson et al., 2007), where a majority of the
datapoints are blue-ish and a minority are green-ish. During testing, we color all datapoints to
be green-ish. Crucially, even though there is no correlation between the label and the color of
the images, the OoD accuracy of an ResNet drops by & 20%. To explain this, in App. C.3.3,
we identify an “implicit”, non-visual kind of spurious correlation in this dataset, one between the
label and a particular component of the difference between the two channels.
Next, we enumerate two not-easy-to-learn tasks. Here, one of the easy-to-learn constraints is dis-
obeyed significantly enough to make the task hard, and this is essential in causing failure. In other
words, the failure modes here correspond to ones that were outlined in Sec 3.1. Nevertheless, we ar-
gue in App C.3 that even these modes can be reasoned geometrically as a special case of Theorem 1.
The two not-easy-to-learn tasks are as follows:
•	In Fig 2d, we add a line to the last channel of CIFAR10 images regardless of the label, and make
the line brighter during testing resembling a camera glitch, which results in a & 27% drop in a
ResNet’s accuracy. We geometrically argue how this failure arises from breaking Constraint 5.
•	In App C.3.5, we consider an MNIST setting inspired by Tsipras et al. (2019), where failure arises
(geometrically) due to high-dimensional spurious features (breaking Constraint 4).
We hope that these examples, described in greater detail in App C.3, provide (a) a broader way to
think about how spurious correlations manifest, and (b) how a variety of resulting failure modes can
be reasoned geometrically.
5 Note that although technically speaking these tasks do break Constraint 4 (as the spurious feature does not
take two discrete values) this is not essential to the failure.
7
Published as a conference paper at ICLR 2021
5	Failure due to statistical s kews
Having theoretically studied max-margin classifiers, let us now turn our attention to studying linear
classifiers trained by gradient descent on logistic/exponential loss. Under some conditions, on lin-
early separable datasets, these classifiers would converge to the max-margin classifier given infinite
time (Soudry et al., 2018; Ji & Telgarsky, 2018). So it is reasonable to say that even these classifiers
would suffer from the geometric skews, even if stopped in some finite time. However, are there any
other failure modes that would arise here?
To answer this, let us dial back to the easiest-to-learn task: the setting with four points {-1, +1} ×
{-B, +B}, where in the training distribution (say D2-dim), we have xinv = y and xsp to be yB with
probability p ∈ [0.5, 1) and -yB with probability 1 -p. Here, even though the max-margin does not
rely on xsp for any level of spurious correlation p ∈ [0.5, 1) — there are no geometric skews here
after all — the story is more complicated when we empirically evaluate via gradient descent stopped
in finite time t. Specifically, for various values of P We plot wsp∕√w2v+w2l vs. t (here, looking at WSP
alone does not make sense since the weight norm grows unbounded). We observe in Fig 3a that the
spurious component appears to stagnate around a value proportional top, even after sufficiently long
training, and even though it is supposed to converge to 0. Thus, even though max-margin doesn’t
fail in this dataset, finite-time-stopped gradient descent fails. Why does this happen?
To explain this behavior, a partial clue already exists in Soudry et al. (2018); Ji & Telgarsky (2018):
gradient descent can have a frustratingly sloW logarithmic rate of convergence to the max-margin
i.e,. the ratio |wsp/winv| could decay to zero as sloW as 1/ lnt . HoWever, this bound is a distribution-
independent one that does not explain Why the convergence varies With the spurious correlation. To
this end, We build on this result to derive a distribution-specific convergence bound in terms ofp, that
applies to any easy-to-learn task (Where xinv may be higher dimensional unlike in D2-dim). For con-
venience, We focus on continuous-time gradient descent under the exponential loss exp(-yh(x))
(the dynamics of Which is similar to that of logistic loss as noted in Soudry et al. (2018)). Then We
consider any easy-to-learn task and informally speaking, any corresponding dataset Without geomet-
ric skeWs, so the max-margin Wouldn’t rely on the spurious feature. We then study the convergence
rate of WsP(t>B/WinV(t)*inv to 0 i.e., the rate at which the ratio between the output of the spurious com-
ponent to that of the invariant component converges to its corresponding max-margin value. We
show that the convergence rate is Θ(1/ ln t), crucially scaled by an extra factor that monotonically
increases in [0, ∞) as a function of the spurious correlation, p ∈ [0.5, 1), thus capturing slower
convergence for larger spurious correlation. Another notable aspect of our result is that when there
is no spurious correlation (p = 0.5), both the upper and lower bound reduce to 0, indicating quick
convergence. We provide the full statement and proof of this bound in App B.2. For completeness,
we also provide a more precise analysis of the dynamics for a 2D setting under both exponential and
logistic loss in Theorem 5 and Theorem 6 in App B.2.
Theorem 2. (informal) Let H be the set of linear classifiers h(x) = WinV ∙ XinV + WSPxSp∙ Then,
for any easy-to-learn task, and for any dataset without geometric skews, continuous-time gradient
descent training of Winv(t) ∙ XinV + Wsp(t)xsp to minimize the exponential loss, satisfies:
ω (ln1+√+(P-p) ) ≤ ∣wWv(t))BinVI ≤ O (⅛⅜) , where P ：= PrDtoJxSP ∙ y> 0] ∈ [0∙5,1) ∙
The intuition behind this failure mode is that in the initial epochs, when the loss eχp(-yw ∙ x) on
all points are more or less the same, the updates 1∕∣s∣ ∙ P(X y)∈s yx exp(-yw ∙x) roughly push along
the direction, 1∕∣s∣ ∙ P(X 期h yx. This is the precise (mis)step where gradient descent “absorbs” the
spurious correlation, as this step pushes Wsp along PB - (1 - P)B = (2P - 1)B. While this update
would be near-zero when there is only little spurious correlation (P ≈ 0.5), it takes larger values
for larger levels of spurious correlation (P ≈ 1). Unfortunately, under exponential-type losses, the
gradients decay with time, and so the future gradients, even if they eventually get rid of this absorbed
spurious component, take an exponentially long time to do so.
Broader empirical implications. We now demonstrate the effect of statistical skews in more gen-
eral empirical settings consisting of a non-linear easy-to-learn task learned using a neural network. 6 *
6The overall accuracy on CIFAR10 is low because even though |Scon | = 50k and |Sexp | = 455k, the number
of unique samples here is just 5k. See App C.4.2 for more explanation.
8
Published as a conference paper at ICLR 2021
(a) Failure of gradient-descent
Spurious feature scale
(c) ResNet + CIFAR10
Figure 3: Statistical skews: In Fig 3a, We plot the slow convergence of Wsp/√W2v+w2p ∈ [0,1] under
logistic loss with learning rate 0.001 and a training set of 2048 from D2-dim with B = 1. In Fig 3b
and Fig 3c, we demonstrate the effect of statistical skews in neural networks. Here, during test-time
we shift both the scale of the spurious feature (from its original scale of 0.1 as marked by the red
triangle) and its correlation. Observe that the network trained on the statistically-skewed Sexp is
more vulnerable to these shifts compared to no-skew Scon.6
Isolating the statistical skew effect is however challenging in practice: any gradient-descent-trained
model is likely to be hurt by both statistical skews and geometric skews, and we’d have to some-
how disentangle the two effects. We handle this by designing the following experiment. We first
create a control dataset Scon where there are no geometric or statistical skews. For this, we take
a set of images Sinv (with no spurious features), and create two copies of it, Smaj and Smin where
we add spurious features, positively and negatively aligned with the label, respectively, and define
Scon = Smaj ∪ Smin . Next, we create an experimental dataset Sexp with a statistical skew in it. We
do this by taking Scon and duplicating Smaj in it so that the ratio |Smaj | : |Smin| becomes 10 : 1.
Importantly, this dataset has no geometric skews, since merely replicating points does not affect the
geometry. Then, if we were to observe that (stochastic) gradient descent on Sexp results in greater
spurious-feature-reliance than Scon, we would have isolated the effect of statistical skews.
Indeed, we demonstrate this in two easy-to-learn tasks. First, we consider a Binary-MNIST task
learned by an fully-connected network. Here, we concatenate a spurious channel where either all
the pixels are “on” or “off”. Second, we consider a (multiclass) CIFAR10 task (where we add a
spuriously colored line) learned using a ResNet (He et al., 2016). In Fig 3, we demonstrate that
training on Sexp leads to less robust models than on Scon in both these tasks. In other words, gradient
descent on Sexp leads to greater spurious-feature-reliance, thus validating the effect of statistical
skews in practice. More details are provided in App C.4.
6	Conclusions and future work
We identify that spurious correlations during training can induce two distinct skews in the training
set, one geometric and another statistical. These skews result in two complementary ways by which
empirical risk minimization (ERM) via gradient descent is guaranteed to rely on those spurious
correlations. At the same time, our theoretical results (in particular, the upper bounds on the spurious
component of the classifier) show that when these skews do disappear, there is no failure within the
considered tasks. This suggests that within the class of easy-to-learn tasks and for gradient-descent-
trained linear models, the above discussion likely captures all possible failure modes.
However, when we do venture into the real-world to face more complicated tasks and use non-linear,
deep models, many other kinds of failure modes would crop up (such as the ones we enumerate in
Sec 3.1, in addition to the fundamental ones mentioned above). Indeed, the central message of our
work is that there is no one unique mechanism by which classifiers fail under spurious correlations,
even in the simplest of tasks. This in turn has a key practical implication: in order to improve our
solutions to OoD generalization, it would be valuable to figure out whether or not a unified solution
approach is sufficient to tackle all these failure mechanisms. While we outline some solutions in
App D, we hope that the foundation we have laid in this study helps future work in better tackling
out-of-distribution challenges.
Acknowledgements. We thank Hanie Sedghi for providing useful feedback on the draft.
9
Published as a conference paper at ICLR 2021
References
Isabela Albuquerque, Joao Monteiro, Mohammad Darvishi, Tiago H. Falk, and Ioannis Mitliagkas.
Generalizing to unseen domains via distribution matching, 2020.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
2019. URL http://arxiv.org/abs/1907.02893.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Proceedings of Machine Learning
Research. PMLR, 2017.
Nader Asadi, Mehrdad Hosseinzadeh, and Mahdi Eftekhari. Towards shape biased unsupervised
representation learning for domain generalization. abs/1909.08245, 2019.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari,
Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 -
15th European Conference, Proceedings, Part XVI, 2018.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification
tasks to a new unlabeled sample. In Advances in Neural Information Processing Systems 24,
2011.
Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron C. Courville, and Yoshua
Bengio. On the learning dynamics of deep neural networks. 2018. URL http://arxiv.org/
abs/1809.06848.
Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and miti-
gating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference
on AI, Ethics, and Society, AIES 2018. ACM, 2018.
Jeremy Elson, John (JD) Douceur, Jon Howell, and Jared Saul. Asirra: A captcha that exploits
interest-aligned manual image categorization. In Proceedings of 14th ACM Conference on Com-
puter and Communications Security (CCS). Association for Computing Machinery, Inc., October
2007. URL https://www.microsoft.com/en-us/research/publication/
asirra- a- captcha- that- exploits- interest- aligned- manual- image- categorization/.
Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of mul-
tiple datasets and web images for softening bias. In IEEE International Conference on Computer
Vision, ICCV 2013, 2013.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural
networks. J. Mach. Learn. Res., 2016.
Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Bren-
del, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks.
abs/2004.07780, 2020. URL https://arxiv.org/abs/2004.07780.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. 2020. URL
https://arxiv.org/abs/2007.01434.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016.
Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In 7th International Conference on Learning Representations,
ICLR 2019. OpenReview.net, 2019.
Katherine L. Hermann and Andrew K. Lampinen. What shapes feature representations? exploring
datasets, architectures, and training. In Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.
10
Published as a conference paper at ICLR 2021
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, pp.
125-136, 2019.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression.
abs/1803.07300, 2018.
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin L. Edelman, Tristan Yang, Boaz
Barak, and Haofeng Zhang. SGD on neural networks learns functions of increasing complex-
ity. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, 2019.
Fereshte Khani and Percy Liang. Feature noise induces loss discrepancy across groups. In Pro-
ceedings of the 37th International Conference on Machine Learning, ICML 2020, Proceedings of
Machine Learning Research. PMLR, 2020.
Fereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups
disproportionately. In FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Trans-
parency. ACM, 2021.
David Krueger, Ethan Caballero, Jorn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex).
abs/2003.00688, 2020.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-
learning for domain generalization. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.),
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18). AAAI
Press, 2018a.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Computer Vision -
ECCV 2018 - 15th European Conference, 2018b.
Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In Anna Korhonen, David R. Traum, and Lluls MarqUez
(eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL
2019. Association for Computational Linguistics, 2019.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In Proceedings of the 30th International Conference on Machine Learning,
ICML 2013, 2013.
Vaishnavh Nagarajan and J. Zico Kolter. Generalization in deep networks: The role of distance from
initialization. 2017.
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. In Advances in Neural Information Processing Systems 32, 2019.
Kamil Nar, Orhan Ocal, S. Shankar Sastry, and Kannan Ramchandran. Cross-entropy loss and
low-rank features have responsibility for adversarial examples. abs/1901.08360, 2019. URL
http://arxiv.org/abs/1901.08360.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. In 3rd International Conference on Learning
Representations, ICLR 2015, 2015. URL http://arxiv.org/abs/1412.6614.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning. 2017.
F. M. Palechor and A. de la Hoz Manotas. Dataset for estimation of obesity levels based on eating
habits and physical condition in individuals from colombia, peru and mexico. 2019.
11
Published as a conference paper at ICLR 2021
Jonas Peters, Peter BUhlmann, and Nicolai Meinshausen. Causal inference by using invariant Pre-
diction: identification and confidence intervals. Journal of the Royal Statistical Society Series B,
2016.
Nasim Rahaman, Devansh ArPit, Aristide Baratin, Felix Draxler, Min Lin, Fred A. HamPrecht,
Yoshua Bengio, and Aaron C. Courville. On the sPectral bias of deeP neural networks. 2018.
URL http://arxiv.org/abs/1806.08734.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should I trust you?”： Explaining the
Predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 2016.
Amir Rosenfeld, Richard S. Zemel, and John K. Tsotsos. The elephant in the room. abs/1808.03305,
2018. URL http://arxiv.org/abs/1808.03305.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts： On the importance of regularization for worst-case generaliza-
tion. 2020a.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why
overparameterization exacerbates spurious correlations. 2020b.
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The
pitfalls of simplicity bias in neural networks. In Advances in Neural Information Processing
Systems, volume 33, 2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. J. Mach. Learn. Res., 19, 2018.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In 7th International Conference on Learning Repre-
sentations, ICLR 2019, 2019.
Vladimir Vapnik. Statistical learning theory. Wiley, 1998. ISBN 978-0-471-03003-4.
Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. In Tom Gedeon, Kok Wai Wong, and Minho Lee (eds.), Neural Information
Processing - 26th International Conference, ICONIP 2019, Lecture Notes in Computer Science.
Springer, 2019.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-
cal risk minimization. In 6th International Conference on Learning Representations, ICLR 2018,
2018.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like
shopping: Reducing gender bias amplification using corpus-level constraints. In Proceedings
of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017.
Association for Computational Linguistics, 2017.
A Constraints and other failure modes
Here, we elaborate on how each of the constraints we impose on the easy-to-learn tasks corresponds
to eliminating a particular complicated kind of failure of ERM from happening. In particular, for
each of these constraints, we’ll construct a task that betrays that constraint (but obeys all the others)
and that causes a unique kind of failure of ERM. We hope that laying these out concretely can
provide a useful starting point for future work to investigate these various failure modes. Finally,
it is worth noting that all the failure modes here can be explained via a geometric argument, and
furthermore the failure modes for Constraint 5 and Constraint 4 can be explained as a special case
of the argument in Theorem 1.
Failure due to weakly predictive invariant feature (breaking Constraint 1). We enforced in
Constraint 1 that the invariant feature be fully informative of the label. For a setting that breaks this
12
Published as a conference paper at ICLR 2021
constraint, consider a 2D task with noisy invariant features, where across all domains, we have a
Gaussian invariant feature of the form XinV 〜N(y,。2丫)——this sort of a noisy invariant feature
was critically used in Tsipras et al. (2019); Sagawa et al. (2020b); Arjovsky et al. (2019) to explain
failure of ERM. Now, assume that during training We have a spurious feature XSP 〜N(y, σ1) (Say
with relatively larger variance, while positively correlated with y). Then, observe that the Bayes
optimal classifier on Dtrain is sgn (XinV/σinv + Xsp∕σsp) i.e., it must rely on the spurious feature.
However, also observe that if one were to eliminate noise in the invariant feature by setting σinv → 0
(thus making the invariant feature perfectly informative of the label like in our MNIST example),
the Bayes optimal classifier approaches sgn (Xinv), thus succeeding after all.
Failure due to “unstable” invariant feature (breaking Constraint 2). If during test-time, we
push the invariant features closer to the decision boundary (e.g., partially occlude the shape of every
camel and cow), test accuracy will naturally deteriorate, and this embodies a unique failure mode.
Concretely, consider a domain generalization task where across all domains Xinv ≥ -0.5 determines
the true boundary (see Fig 4a). Now, assume that in the training domains, we see Xinv = 2y or
Xinv = 3y. This would result in learning a max-margin classifier of the form Xinv ≥ 0. Now,
during test-time, if one were to provide “harder” examples that are closer to the true boundary in
that Xinv = -0.5 + 0.1y, then all the positive examples would end up being misclassified.
Failure due to complex conditional dependencies (breaking Constraint 3). This constraint im-
posed Xinv ⊥ Xsp |y. Stated more intuitively, given that a particular image is that of a camel, this
constraint captures the fact that knowing the background color does not tell us too much about the
precise shape of the camel. An example where this constraint is broken is one where Xinv and Xsp
share a neat geometric relationship during training, but not during testing, which then results in
failure as illustrated below.
Consider the example in Fig 4b where for the positive class we set Xinv + Xsp = 1 and for the negative
class we set Xinv + Xsp = -1, thereby breaking this constraint. Now even though the invariant feature
in itself is fully informative of the label, while the spurious feature is not, the max-margin here is
parallel to the line Xinv + Xsp = c and is therefore reliant on the spurious feature.
Failure mode due to high-dimensional spurious features (lack of Constraint 4). Akin to the
setting in Tsipras et al. (2019), albeit without noise, consider a task where the spurious feature has
D different co-ordinates, Xsp = {-1, 1}D and the invariant feature just one Xinv = {-1, +1}.
Then, assume that the ith spurious feature Xsp,i independently the value y with probability pi and
-y with probability 1 - pi, where without loss of generality pi > 1/2. Here, with high probability,
all datapoints in S can be separated simply by summing up the spurious features (given D is large
enough). Then, we argue that the max-margin classifier would provide some non-zero weight to this
direction because it helps maximize its margin (see visualization in Fig 4d).
One way to see why this is true is by invoking a special case of Theorem 1. In particular, if we
define P Xsp,i to be a single dimensional spurious feature Xsp, this feature satisfies XSP ∙ y > 0 on all
training points. In other words, this is an extreme scenario with no minority group. Then, Theorem 1
would yield a positive lower bound on the weight given to Xsp, explaining why the classifier relies
on the spurious pixels. For the sake of completeness, we formalize all this discussion below:
Proposition 1. Let C be a constant such that for all i, Pj > 1 + C. Let D be sufficiently large so
that D ≥ 2C '2 ln m where m is the number oftraining datapoints in S. Then, w.h.p. of 1 一 δ over
the draws of S, the max-margin classifier corresponds is of the form winvXinv + wspxsp where:
Ilwspk、c√D
	≥ 	.
winv-------2
Proof. First, we,ll show that on S there exists a classifier that relies purely on the spurious features
to separate the data. In particular, consider w0p where the ith dimension is 1/√D if Pi > 1/2, and
-1/√D otherwise. By the Hoeffding,s inequality, we have that with high probability 1 一 δ, on all
the m training datapoints, yw0p ∙ XSP ≥ √D P(2pi - 1)-《D ln 等 ≥ C √D.
13
Published as a conference paper at ICLR 2021
Spurious
Max-margin
“…’classifier
.......★
test-time
Shift .
Invariant
Spurious
,☆«
“True”、
classifier
> Invariant
∖ ★
Max-margin
classifier
Spurious + Invariant
A ★
-ΔzL
'、 Invariant
t
^χmαr∙n
Max-margin
- classifier
∑SpuriouSi
Invariant
Max-margin
classifier
(a) Breaking Constraint 2:
Unstable invariant feature
(b)	Breaking Constraint 3:
Conditionally dependent features
(c)	Breaking Constraint 5:
Hard-to-disentangle features
(d) Breaking Constraint 4:
High-dimensional spurious
features
Figure 4: Other failure modes: We visualize the different (straightforward) ways in which a max-
margin classifier can be shown to fail in tasks where one of the Constraints in Sec 3.1 is disobeyed.
More discussion in Sec A.
Now, for the max-margin classifier, assume that wi2nv = α. Further, assume that the margin con-
tributed by Wspxsp + b equals √1 - a2m for some m. Observe that m must satisfy m ≥ C √D (as
otherwise, We can replace Wsp with √1 - a2 wsp to achieve a better margin). Now, for the resulting
margin to be maximized, a must satisfy √1-a2 = mm.
□
Failure mode due to hard-to-separate features (lack of Constraint 5) Here we assumed that the
feature space can be orthogonally decomposed into invariant and spurious features. Now let us
imagine a 2D task, visualized in Fig 4c where this is not respected in that each datapoint is written
as (xinv, xinv + xsp), assuming that xinv = y and xsp ∈ {-0.5, 0.5}. A practical example of this sort
of structure is the example in Fig 2d where we add a line to the last channel of CIFAR10, and then
vary the brightness of the line during test-time.
To understand why failure occurs in this 2D example, observe that, regardless of the correlation
between xsp and y, we,d have that (xi∩v + xsp) ∙ y > 0. In other words, the second co-ordinate
is fully informative of the label. The max-margin classifier, due to its bias, relies on both the first
and the second co-ordinate to maximize its margin i.e., the classifier would be of the form w1xinv +
w2(xinv + xsp) where w2 > 0. (Again, like in our discussion of the failure mode of Constraint 4, we
can argue this via Thm 1 by considering xinv + xsp itself as a spurious feature, and by observing that
there’s no minority group here.) Hence, by assigning a positive weight to the second co-ordinate it
inadvertently becomes susceptible to the spurious feature that may shift during testing.
B	Proofs
B.1	Proof of Theorem 1 on failure due geometric skews
Below we provide a proof for our result analyzing the failure mode arising from geometric skews in
the data.
Recall that given a dataset S, where the spurious feature can take only values in {-B, +B}, we
partitioned S into two subsets Smaj and Smin where in Smaj the points satisfy xsp ∙ y > 0 and in Smin
the points satisfy xsp ∙ y < 0.
Next, we define two key notations. First, for any dataset T ⊆ S, let v(T) ∈ Xinv denote a least-norm
vector (purely in the invariant space) that achieves a margin ofat least 1 on all datapoints in T (we’ll
define this formally a few paragraphs below). Similarly, let V(T) ∈ XinV denote a least-norm vector
that achieves a margin of at least 1 on T, and a margin ofat least 0 on S\T (again, full definition will
shortly follow). While by definition, kv(T)k ≤ Ilv(T)k, we can informally treat these quantities
as the same since empirically ∣∣v(T)∣ ≈ ∣∣v(T)∣. We show these plots in Sec C.1 for both MNIST
and CIFAR10. But importantly, both these quantities grow with the size of T. Then, by virtue of
the small size of the minority group Smin, we can say that ∣v(Smin)∣ is smaller than both ∣v(Smaj)∣
and ∣v(S)∣. We refer to this gap as a geometric skew. When this skew is prominent enough (e.g.,
14
Published as a conference paper at ICLR 2021
kv(Smin)k/kv(S)k ≈ 0), our result below argues that the spurious component in the overall max-
margin classifier must be sufficiently large (and positive). On the flip side, we also show that when
the skew is negligible enough (e.g., kv(Smin)k/kv(S)k ≈ 1), then the spurious component has to
be sufficiently small.
To be able to better visualize these bounds, we write these as bounds on |Bwsp| (i.e., |wspxsp| rather
than |wsp |). Then we can think of a lower bound of the form |Bwsp | & 1 as demonstrating serious
failure as a shift in the correlation can adversely reduce the original margin of ≈ 1.
Before We state the result, for clarity, We state the full mathematical definition of V and V as follows.
For any T ⊆ S :
V(T), b(T) = arg min
winv∈Xinv,b
s.t.
___~ _____
V(T),b(T) = arg min
winv ∈Xinv ,b
s.t.
kwinv k2
y (Winv ∙ xinv ) + b) ≥ 1
kWinv k2
y (Winv ∙ xinv ) + b) ≥ 1
y (Winv ∙ xinv ) + b) ≥ 0
∀ ((xinv, xsp ), y) ∈ T
∀ ((xinv, xsp ), y) ∈ T
∀ ((xinv, xsp), y) ∈ S \ T
Using these notations, We state our full theorem and provide its proof beloW.
Theorem 3. Let H be the set of linear classifiers, h(x) = Winvxinv + wspxsp + b. Let the geometric
skews in a dataset S be quantified through the terms κ1 = kv(Smin)k/kv(S)k, κ2 = kv(Smin)k/kv(Smaj)k
and Ki := kv(Smin)阴⑼但*,K2 := kv(Smin)k/kv(Smaj)k∙ ThenforanytasksatisjyingaUtheconstraints
in Sec 3.1 the max-margin classifier satisfies the inequalities (where for readability, we will use
Ci ：= 1∕(2kV(S)kB),C2 ：= 1∕(2kV(Smaj)kB))：
Bwsp ≥ max
(1 - 2+ c2,0)
if κK2 ≤	i/4 - c22 ,	and
|Bwsp| ≤ min (1∕κι - 1,BkV(S)II)
if κ2 ≤ 1.
For readability, it helps to think of ci and c2 as small constants here (also see remark beloW). Fur-
thermore, for readability, one can also imagine that all the κ terms are numerically similar to each
other.
We make a feW remarks beloW before providing the proof.
Remark 1. For the loWer bound on wsp to be positive, We need ci and c2 to be small. This Would
be true When either B or kV(S)k (or kVK(Smaj)k ) is sufficiently large. This is intuitive: after all, if
B is too small (say 0) there is no spurious feature effectively and therefore the max-margin has no
incentive to use it; similarly, if V(S) is too small (say 0), then the max-margin has no incentive to
use any feature besides the invariant feature, Which is already quite cheap.
Remark 2. The above result is not intended to be a numerically tight/upper loWer bound. In fact,
the proof can be tightened in numerous places Which We hoWever avoid, to keep the result and the
proof simple. The bound is rather meant to be instructive of the effect of the geometric skeW (i.e.,
the gap betWeen the max-margin norms on the minority and Whole/majority dataset) on the spurious
component.
Proof. We present the proof of loWer bound first, folloWed by the upper bound.
Proof of lower bound. First, We Will shoW that there exists a classifier of norm 1 that relies on
the spurious feature to create a sufficiently large margin. We’ll let this classifier be of the form
a k v(Smin) k ∙ xinv+ √1 - α2 xsp + αbmin. By the definition of VK(Smin), the margin of this classifier
on any datapoint in Smin is at least 归(义.)k - √Γ-^02B. Again, by the definition of V(Smin), the
margin on Smaj is at least √1 - α2B. Let us pick an α such that these two quantities are equal. Such
15
Published as a conference paper at ICLR 2021
an ɑ would satisfy √-02 = 2∣∣ V(Smin) ∣∣B. By plugging this back, We get that the resulting margin
of this classifier on the whole dataset S is at least / B . In other words, this also means
1+4kWSmin)k2B2
the least norm classifier w with a margin of at least 1 on S has its norm upper bounded as:
∣w∣ ≤
,1+4∣V(Smin )k2B2
B
(1)
Now, assume that w (i.e., the least norm classifier with a margin of 1 on S) is of the form winvxinv +
wspxsp + b. First, we derive a lower bound on |wsp |, and after that we’ll show that wsp > 0. For this
we,ll consider two cases, one where |wsp | ≥ B (and so we already have a lower bound) and another
case where | Wsp | < B. In the latter case, we will need the invariant part of the max-margin classifier,
namely winvxinv + b, to have to have a margin of at least 1 - |wsp |B on S; if it were any lesser,
the contribution from the spurious component which is at most |wsp xsp | will be unable to bump this
margin up to 1. Now, for the invariant part of the max-margin classifier to have a margin of at least
1 - |wsp|B (a non-negative quantity since |wsp| ≤ 1/B) on S, the norm of the (invariant part of the)
classifier must be at least (1 一 ∣Wsp∣B)∣v(S)k (which follows from the definition of V(S)). This
implies,
kwk≥ (1-∣Wsp∣B)∣v(S)∣.	(2)
Combining this with Eq 1, we get (1 一 ∣Wsp∣B)∣v(S)k ≤ ʌ/1+4'"(Bmin)产按.Rearranging this gives
us the bound that B∣Wsp∣ ≥ 1 一 2 JkvVSmin)22 + 4B2||，(5川2. Finally note that V(S) is the same as
V(S). Hence we can interchange these terms in the final result (which we,ve done for readability)
to arrive at |Bwsp| ≥ 1 一 2K +。2).
What remains now is to show that wsp > 0. For this we do the same argument as above but with a
slight modification. First, if wsp > 1/B, we are done. So, assume that wsp ≤ 1/B. Then, we can say
that the invariant part of the max-margin classifier i.e., WinVXinv+b, must achieve a margin of 1-WspB
(a non-negative quantity since wsp ≤ 1/B) specifically on Smaj. Then, by the definition of V(Smaj),
it follows that the norm of our overall max-margin classifier must be at least (1 一 wspB)∣V(Smaj)∣.
Again, for the overall classifier to be the max-margin classifier, we need (1 一 ∣Wsp∣B)∣∣v(Smaj)k ≤
√1+4kV(Smin)k2B2	√4B2kV(Smin)k2 + 1
X l-B--——,which when rearranged gives us BwSP ≥ 1 一 X-Bkv(S .4j . The R.H.S is
at least 0 when 4 (1 一 kv(s，)k2B2)≥ kv(Sman)j2 (ie, J4 - c2 ≥ 方2). In other words when
K ≤ J； — c2, we have wsp ≥ 0.
Proof of upper bound. The spurious component of the classifier WinvXinv + wspxsp + b posi-
tively contributes to the margin of one of the groups (i.e., one of Smin and Smaj), and negatively
contributes to the other group, depending on the sign of wsp. On whichever group the spurious
component negatively contributes to the margin, the invariant part of the classifier, WinvXinv + b,
must counter this and achieve a margin of 1 + |wsp |B. To manage this, we,d require ∣Winv ∣ ≥
(1 + |wsp|B) min(∣V(Smin)∣, ∣V(Smaj)∣). In other words, for the overall max-margin classifier W,
we have:
∣W∣ ≥ (1 + |wsp|B) min(∣V(Smin)∣, ∣V(Smaj)∣).	(3)
At the same time, we also know from the definition of V(S) that
∣W∣ ≤ ∣V(S)∣.	(4)
Combining the above two equations, we can say (1+|wsp|B) min(∣V(Smin)∣, ∣V(Smaj)∣) ≤ ∣V(S)∣.
Since we are given κ2 ≤ 1, it means that min(∣V(Smin)∣, ∣V(Smaj)∣) = ∣V(Smin)∣, this simplifies
to (1 + ∣wsp∣B)∣v(Smin)k ≤ IIV(S)k, which when rearranged reaches the result BwSP ≤ -1 一 1.
To get the other upper bound here, observe that for WinvXinv + wsp xsp + b to be the overall min-norm
classifier, its `2 norm, which is lower bounded by |wsp| must not be larger than the `2 norm ofV(S).
Hence |wsp | ≤ IIV(S) ∣∣.	□
16
Published as a conference paper at ICLR 2021
B.2	Proof of Theorem 2 on failure due to statistical skews
Below we state the full form of Theorem 2 and its proof demonstrating the effect of statistical skews
in easy-to-learn tasks. After that, we’ll present a more precise analysis of the same in a 2D setting
in Theorem 5 (for exponential loss) and in Theorem 6 (for logistic loss).
Our result below focuses on any easy-to-learn task and on a corresponding dataset where there are
no geometric skews. Specifically, we consider a dataset where the invariant features have the same
empirical distribution in both the majority subset (where XSP ∙ y > 0) and the minority subset (where
XSP ∙ y < 0). As a result, in this setting the max-margin classifier would not rely on the spurious
feature. This allows us to focus on a setting where we can isolate and study the effect of statistical
skews.
For the sake of convenience, we focus on the exponential loss and under infinitesimal learning rate,
and a classifier initialized to the origin.
Theorem 4. (full form of Theorem 2 ) LetH be the set of linear classifiers, h(x) = winvxinv +wspXsp.
Consider any task that satisfies all the constraints in Section 3.1. Consider a dataset S drawn from D
such that the empirical distribution of Xinv given XSP ∙ y > 0 is identical to the empirical distribution
of Xinv given XSP ∙ y < 0. Let winv(t)Xinv + WSP (t)xsp be initialized to the origin, and trained with
an infinitesimal rate to minimize the exponential loss on a dataset S. Then, for any (X, y) ∈ S, we
have:
ω (ln c+√p(L) ʌ ≤	WSPIt)B	≤ O ∣ ln 1⅞ !
M ln(t + 1)	∣winv(t) ∙ Xinvl	ln(t +1)y
where:
•	P denotes the empirical level ofspurious correlation, P = 看 ∑(χ y)∈s 1[xsp ∙y > 0] which
without generality is assumed to satisfy p ∈ [0.5, 1).
•	M denotes the maximum value of the margin of the max-margin classifier on S i.e., M =
maxχ∈s W ∙ x where W is the max-margin classifier on S.
2(2M-1)
•	C := B2
Proof. Throughout the discussion, we’ll denote winv(t) and WsP(t) as just winv and WsP for readabil-
ity.
Let Smin and Smaj denote the subset of datapoints in S where XSP ∙ y < 0 and XSP ∙ y > 0 respectively.
Let Dinv denote the uniform distribution over Xinv induced by drawing X uniformly from Smin. By
the assumPtion of the theorem, this distribution would be the same if X was drawn uniformly from
Smaj. Then, the loss function that is being minimized in this setting corresPonds to:
L(Winv,Wsp) =PExinV 〜DinVhe-(WiMxinV+wspB)i + (1 - P)ExinV 〜DinVhe-(XinV∙winv-wspB)i ,
where P ∈ [0.5, 1). Here, the first term is the loss on the majority dataset (where XsP = yB) and the
second term is the loss on the minority dataset (where Xsp = -yB).
The update on Wsp can be written as:
WSP= ExinV〜⅛Je-winvx叫∙ B ∙ (Pe-WSPB - (1 - P)ewspB)
To study the dynamics of this quantity, we first bound the value of winv(t)Xinv.
Bounds on winv(t)Xinv The result from Soudry et al. (2018) states that we can write w(t) =
Wln(1 + t) + ρ(t) where W is the max-margin classifier and P is a residual vector that is
bounded as ∣∣ρ(t)k2 = O(lnlnt). Since the max-margin classifier here is of the form W =
(Winv, 0) (i.e., it only relies on the invariant feature), We can infer from this that Winv(t) =
Winvln(1 + t) + ρt(t) where again kρt(t)∣∣2 = O(lnlnt). For a sufficiently large t, We can
17
Published as a conference paper at ICLR 2021
say that lnlnt《ln(1 + t). This would then imply that for all X ∈ S, ∣winv(t) ∙ Xinv| ∈
[0.5WinvXinv(t) ln(1 +1), 2WinvXinv(t)ln(1 +1)]. Since the max-margin classifier has a margin be-
tween 1 and M on the training data, this implies that, for a sufficiently large t and for all X ∈ S:
Winv(t) ∙ Xinv| ∈ [0.5ln(1 + t), 2M ln(1 + t)].
Next, we bound the dynamics of wsp.
Upper bound on wsp. To upper bound wsp, We first note that WSP = 0 only when WSP =点 ln ι-pp.
Furthermore, WSP is a decreasing function in Wsp. Hence, for any value of WSP that is less than
2B ln ι-p, Wsp ≥ 0 and for any that is greater than this value, WSP ≤ 0. So, we can conclude that
when the system is initialized at 0, it can never cross the point WSP = 克 ln I-P. In other words,
for all t, Wsp(t) ≤ 克 ln ɪ-p. Combining this with the lower bound on wi∩v(t), we get the desired
result.
Lower bound on Wsp. We lower bound WSP via the upper bound on WSP as:
WSP ≥ Exinv〜D^i1Je-winvxinv ] ∙ B ∙ (pe-wspB -(1- P)
=ExinV〜Diije-Winvx叫∙ B ∙ (Pe-WSPB - pp(1-p)
Next, since we have that for all X ∈ S, ∣Winv ∙ Xinv∣ ≤ 2M ln(t + 1):
WSP ≥ (t + 1)2M B ∙ (Pe-WSPB - PP(I - P)).
Rearranging this and integrating, we get:
Wsp
0	Pe-Wsp B
—
ɪ ,	--dWs∏ ≥ B B， Lrrrdt,
Pp(T-^ sp≥ J0 (1 + t)2M ,
(Since 2M ≥ 2, we can integrate the right hand side as below)
-	ln(P - ewspBpP(1 - P)) + ln(P — Pp(1 — P)) ≥ B (1
BpP(1 — P)	BpP(1 — P)	— 2M - 1 ∖	(I + t)2M-1
since for a sufficiently large t, the final paranthesis involving t will at least be half,
(J1⅛ - 1 ʌ ≥ 1 PP(Γ-^B2
[6 - ewspB 广 2	2M- 1
we can further lower bound the right hand side by applying the inequality x ≥ ln(x + 1) for positive
x,
≥ in(1+l PM-B
Taking exponents on both sides and rearranging,
/ P
V 1 - P
ɪ -1
eWsp B ≥
1 + 1 √p(1-P)B2
—
Wsp ≥
1ln _笙"+ P_
B *舁 + PpT-^.
Combining this with the upper bound on Winv(t), we get the lower bound on Wsp(t)/Winv(t).
□
18
Published as a conference paper at ICLR 2021
B.3	Precise analysis of statistical skews for a 2D setting under exponential
LOSS
We now consider the 2D dataset D2-dim considered in the main paper, with the spurious feature scale
set as B = 1, and provide a more precise analysis of the dynamics under exponential loss. This
analysis is provided for the sake of completeness as the proof is self-contained and does not rely on
the result of Soudry et al. (2018); Ji & Telgarsky (2018). In the next section, we perform a similar
analysis for logistic loss.
Theorem 5. Under the exponential loss with infinitesimal learning rate, a linear classifier
winv(t)xinv + wsp(t)xsp initialized to the origin and trained on D2-dim with B = 1 satisfies:
ln ((I+2P)/(3-2p)) ≤ Wsp(t) ≤ ln (p∕(i-p))
ln(1+ 3max(t, 1)) — winv(t) — ln(1 +2t)
where P ：= Pe. [xsp ∙ y > 0] ∈ [0.5,1]∙
Proof. Throughout the proof, we’ll drop the argument t from winv(t) and wsp(t) for convenience.
The loss function that is being minimized in this setting corresponds to:
L(winv,wsp) =pe-(winv+wsp) + (1 -p)e-(winv-wsp),
where p ≥ 0.5. Here, the first term is the loss on the majority dataset (where xsp = yB) and the
second term is the loss on the minority dataset (where xsp = -yB).
Now the updates on winv and wsp are given by:
W inv = Pe-(Winv+wsP) + (1 - p)e-(WinV-WsP)
Wsp = Pe-(Winv+wsP) - (1 - p)e-(Winv-WsP),
which means:
d(Winv + Wsp) = 2pe-(Winv + Wsp)
dt
d(Winv-Wsp) = 2(1 - p)e-(Winv+"
Thus, by rearranging and integrating we get:
Winv + WsP = ln(1 + 2Pt)
Winv - WsP = ln(1 + 2(1 - P)t)
Winv = 0.5(ln(1 + 2Pt) + ln(1 + 2(1 - P)t))
WsP = 0.5(ln(1 + 2Pt) - ln(1 + 2(1 - P)t)).
Now let us define β(t) = WsP/Winv:
β(t) ：= Wsp(t) = ln(1 + 2pt) - ln(1 + 2(1 - p)t)
.	Winv(t) ln(1 + 2pt) + ln(1 + 2(1 - p)t)
(5)
To bound this quantity, we’ll consider two cases, t ≥ 1 and t < 1. First let us consider t ≥ 1. We
begin by noting that the numerator WsP(t) is increasing with time t. This is because,
WsP (t) = ln 1 + 2pt
SpL 1 + 2(1 - p)t
19
Published as a conference paper at ICLR 2021
ln C IS ).
Here, the term i+p-1P)is increasing with t due to the fact that the numerator is non-negative
(p≥ 0.5) and the denominator is decreasing with t. So, given that wsp(t) is increasing, we can say
that for all t ≥ 1:
/1、	ln 1+2P	ln 1+2P
B(t) ≥ Wsp ⑴=_____________ln E_____________≥ ln E
一Winv(t)	ln(1 + 2(1 — p)t) + ln(1 + 2pt) — ln(1 + 3t).
Here we have used the fact that the denominator can be upper bounded as ln(1 +2(1-p)t)+ln(1 +
2pt) ≤ ln(1 + 2t + 4(1 — p)pt) ≤ ln(1 + 3t).
ln 1 + 2P	ln 1 + 2P
Now, for any t ≤ 1, we can show that β(t) ≥ β(1)= 皿3+4(--p2))≥	晨-2P . This follows if We
can show that β(t) is decreasing for t ≥ 0. Taking its derivative with respect to time, we get:
(In(I + 2pt) + ln(1 + 2(1 — p)t)) (l+‰ — (i+22(ι-P)t))
(ln(1 + 2pt) + ln(1 + 2(1 — p)t))2
(ln(1 + 2pt) — ln(1 + 2(1 - p)t)) ((⅛) + ⅛⅛)
(ln(1 + 2pt)+ln(1 + 2(1 — p)t))2
ln(1 + 2(1 - n)t) —2P_____ln(1 + 2nt) 2(I-P)
In(I + 2(I p)t) 1+2pt	In(I + 2Pt) 1+2(l-p)t
(ln(1 + 2pt) + ln(1 + 2(1 — p)t))2
ln(1 + 2(1 — p)t) 1 — ln(1 + 2pt)	1 -
___________________2P +t	2(i-p)+t
(ln(1 + 2pt) + ln(1 + 2(1 — p)t))2
The sign of the above quantity is equal to the sign of:
ln(1 + 2(1 — p)t)(2T-P)+1)— ln(1 + 2pt) G+1)
=ln (J)+1) (2⅛+1 )+ln(J))(2⅛+1)
、------------------{z------------------}
: = f ( 2(1-P) )
—inG+1) G+t)—ln2p G+1),
X{}
=f (泰)
Now, we show that f (x) = (x +1) ln(x +1) — (x +1) ln X = (x +1) ln(1 + t) is a non-increasing
function:
f 0 (X)=ln(1 + X∙)+f⅛
20
Published as a conference paper at ICLR 2021
i(1 + Xt)
≤ - - - ≤ 0.
t
—
x
Now since p ≥ 0.5, and f is non-increasing, f(2(1-P)) - f 岛)≤ 0. Subsequently, β ≤ 0.
Therefore, β(-) ≥ β(1) for any- ∈ [0, 1].
Upper bound. For an upper bound on β(-), we note that since wsp(-) is always increasing wsp(-) ≤
limt→∞ wsp(t) = ln (i-p).On the other hand winv(t) = ln(1 + 2t + 4p(1 — p)t2) ≥ ln(1 + 2t).
Combining these inequalities, we get:
β () ≤
ln (P-1)
ln(1 + 2t).
□
B.4 Analysis of statistical skews for a 2D setting under logistic loss
While the Theorem 2 and Theorem 5 were concerned with the exponential losses, as noted in Soudry
et al. (2018), the dynamics under logistic loss are similar (although harder to analyze). For the sake
of completeness, we show similar results for logistic loss in the same 2D setting as Theorem 5.
Theorem 6. Under the logistic loss with infinitesimal learning rate, a linear classifierwinv(-)xinv +
wsp(-)xsp initialized to the origin and trained on D2-dim with B = 1 satisfies for a sufficiently large -
(where P ：= PrD〜. Ep ∙ y > 0] ∈ [0.5,1])：
.A 1 ln (⅛) ʌ
min「ln(t +1))
≤ WSp ⑴ ≤	2ln 1-p
W Winv(t) — ln(0.5t + 1)
Proof. Here, the loss function is of the form:
L(Winv, Wsp) = p log(1 + e-(winv+wsp)) + (1 - p) log(1 + e-(winv-wsp))
where p ≥ 0.5. Now the updates on Winv and Wsp are:
e-(winv +wsp)
W inv = P
inv 1 + e-(winv+wsp)
e-(winv +wsp)
1W sp	P
sp 1 + e-(winv+wsp)
+ (1 -P)
- (1 - P)
e-(winv -wsp)
1 + e-(winv-Wsp)
e-(winv-wsp)
1 + e-(winv-Wsp)
which means:
d(Winv + Wsp )
d
e-(wi
nv+wsp)	1
p1 1 + e-(Winv+Wsp)	p1 1 + e(Winv +Wsp )
Mwinl-WSp) = 2(1 — P)	e-(Winv-WSPL = 2(1 — P)—J-T
d	1 + e-(winv -wsp )	1 + e(winv -wsp )
Solving for this, we get:
Winv + Wsp + eWinv +Wsp = 2P- + 1
(6)
21
Published as a conference paper at ICLR 2021
winv - wsp + ewinv-wsp = 2(1 - p)t + 1.
(7)
We first derive some useful inequalities.
First, we argue that forall t,
wsp(t) ≥ 0.	(8)
This is because at the point where wsp(t) = 0, Wsp(t) ≥ 1++-Wnv ≥ 0 (since P ≥ 0.5). Hence, the
system can never reach values of wsp < 0.
Next, we have for all t,
winv(t) ∈ [0, ln(t + 1)].	(9)
We can show this by summing up Eq 6 and 7
2winv+ewinv+wsp + ewinv-wsp = 2t+2
=⇒	2Winv + 2√ewinv+wsp ∙ ewinv-WSP ≤ 2t + 2
=⇒	2winv + 2ewinv ≤ 2t + 2
and since Winv ≥ 0, and Winv(t) = 0, Winv(t) ≥ 0,
2ewinv ≤ 2t + 2
Next, we show:
Wsp (t) ≤ 1ln
2(1 - p)t +1
2pt +1
(10)
≤」
To show this, we divide Eq 6 by Eq 7, to get:
WinV + WSP + ewinv+wsp = 2(1 - p)t + 1
Winv — WSP + ewinv-wsp	2pt + 1
=⇒	(2(2p	- 1)t)Winv	+ (2(2p	- 1)t)Wsp	+	(2pt + 1)ewinv+wsp	=	ewinv-(+(1-p)t+1)wsp
since by p ≥ 0.5, Eq 8 and Eq 9 the first two terms are positive,
=⇒	(2pt + 1)ewinv+wsp ≤ (2(1 -p)t+ 1)ewinv-wsp
=⇒	e+wsp ≤ 2(I-P)) +1.
2pt + 1
This proves the first inequality. The second inequality follows from the fact that '1-*%1 is in-
creasing with t so applying lim t → ∞ gives us an upper bound.
Finally, we rewrite Equation 6 and Equation 7 to get:
Winv + Wsp = ln(2Pt + 1 - (Winv + Wsp ))	(11)
Winv - Wsp = ln(2(1 - P)t + 1 - (Winv - Wsp ).	(12)
Adding and subtracting these, we get a different form for the dynamics of these quantities:
Winv = 0.5(ln(2Pt + 1 - (Winv	+ Wsp)) + ln(2(1 -P)t	+ 1 -	(Winv	-	Wsp))	(13)
Wsp = 0.5(ln(2Pt + 1 - (Winv	+ Wsp)) - ln(2(1 -P)t	+ 1 -	(Winv	-	Wsp)).	(14)
22
Published as a conference paper at ICLR 2021
Lower bound. To prove a lower bound on wsp(t)/winv(t), we’ll first lower bound wsp. Observe
that:
wsp(t) = 1 ln	22p + 1-(winv + wsP)
2	2(I - p)t + 1 - (Winv - Wsp)
=1 ln f1+	2中 - 1)t - 2wsp~-
2	2(1 - P)t + 1 - (Winv - Wsp)
Now, since Wsp is upper bounded by a constant (Eq 10), for sufficiently large t, the numerator of
the second term inside the ln will be positive, and can be lower bounded by (2p - 1)t. Then, let
us consider two scenarios. Either that Wsp > Winv, in which case we already have a lower bound on
β(t), or that Wsp ≤ Winv. In the latter case, we can lower bound the above as:
wsp (t) ≥ 2ln (1 +
(2p - 1)t
2(1 - p)t + 1J
Since the right hand side is an increasing in t, we can say that for sufficiently large t ≥ 1,
WsP(t) ≥ 1 ln f 1+ J2p-1)
spl，- 2	+ 2(1 -p) + 1
≥ 1ln (—2—).
一 2	3 — 2p
Combining this with Eq 9 we get for sufficiently large t, either
Wsp(I) ≥
Winv (t)
2ln (3⅜)
ln(t + 1).
or W⅛) ≥ 1.
Upper bound. To upper bound Wsp(t)/Winv(t), we’ll lower bound Winv(t):
Winv = 0.5(ln(2pt + 1 - (Winv + Wsp )) + ln(2(1 - p)t + 1 - (Winv - Wsp ))
Since by Eq 9 Winv(t) ∈ [0, ln(t +1)] and by Eq 10, Wsp(t) ∈ [0,1 ln 1-p], for a sufficiently large t,
the linear terms within the ln terms dominate and so, for large t
Winv ≥ (ln(0.5 ∙ 2pt +1) = ln(0.5t + 1)
Combining this with Eq 10, we get, for large t:
Wsp (t) ≤	1 ln 2-p
Winv (t) — ln(0.5t + 1)
□
C More on experiments
Common details: In all our MNIST-based experiments, we consider the Binary-MNIST classifica-
tion task (Arjovsky et al., 2019) where the first five digits (0 to 4) need to be separated from the rest
23
Published as a conference paper at ICLR 2021
(5 to 9). Unless specified otherwise, for this we train a fully-connected three-layered ReLU network
with a width of 400 and using SGD with learning rate 0.1 for 50 epochs. In all our CIFAR10-based
experiments, unless stated otherwise, we consider the 10-class classification problem, and train a
ResNetV1 with a depth of 20 for 200 epochs. 7 All values are averaged at least over fives runs.
Finally, when we describe our datasets, we’ll adopt the convention that all pixels lie between 0 and
1,
C.1 Random feature experiments from Section 3.1
For the random features based experiment on Binary MNIST in Section 3.1, we consider 50k ran-
dom ReLU features i.e., xinv = ReLU(Wxraw) where W is a 50k × 784 matrix (and so this is well
overparameterized for dataset sizes upto 6400). Each entry here is drawn from the normal distribu-
tion. We set the spurious feature support to be {-100, 100}, which is about 1/10th the magnitude
of kxinv k. We also conduct similar experiments on a two-class CIFAR10 and report similar OoD
accuracy drops in Fig 5a. Here, we use the first two classes of CIFAR10, as against grouping five
classes together into each of the classes. This is because on the latter dataset, the random features
representation has poor in-distribution accuracy to begin with.
C.2 Increasing norm experiments from Section 4
The main premise behind our geometric skews argument is that as we increase the number of data-
points, it requires greater norm for the model to fit those points. We verify this for random features
models on Binary-MNIST and two-class CIFAR10 in Figs 5b, 5c.
While the theory is solely focused on linear classifiers, we can verify this premise intuitively for
neural network classifiers. However, for neural networks, the notion of margin is not well-defined.
Nevertheless, as a proxy measure, we look at how much distance the weights travel from their
initialization in order to classify the dataset completely. Such plots have already been considered
in Neyshabur et al. (2017); Nagarajan & Kolter (2017; 2019) (although in the completely different
context of understanding why deep networks succeed at in-distribution generalization). We present
similar plots for completeness.
Fig 5d shows this for Binary-MNIST on an FNN. For CIFAR10, we conduct two experiments. Fig 5f
uses a ResNet with Adam and decaying learning rate. Here, we observe that the norms saturate after
a point, which is because of the learning rate decay. Since this does not make a fair comparison
between the geometries of larger datasets and smaller datasets, we also plot this for SGD with fixed
learning rate in Fig 5e to recover the increasing norms observation. Here, sometimes the model
sometimes saturates at an accuracy of 99% (rather than 100%); in those cases, we report the value
of the weight norm at the final epoch (namely, at the 200th epoch).
C.3 Broader examples of geometric failure
We elaborate on the multiple datasets we showcased in the paper as examples where ERM fails
due to a geometric skew. We first discuss the two CIFAR10 datasets, and then discuss the cats vs.
dogs example, and then discuss two Binary-MNIST datasets, one that is similar to the cats vs. dogs
example, and another corresponding to high-dimensional spurious features.
C.3.1 CIFAR 1 0 example with spuriously colored line
Here we provide more details about the CIFAR-10 dataset we presented in Sec 4 and in Fig 1c. This
dataset can be thought of as an equivalent of the cow-camel classification tasks but for 10 classes.
For this, we use ten different values of the spurious feature, one for each class. We argue that the
failure here arises from the fact that the ResNet requires greater norms to fit larger datapoints (see
Fig 5e), and so a similar argument as Theorem 1 should explain failure here.
Dataset details. To create the ten-valued spurious feature, we consider a vertical line passing
through the middle of each channel, and also additionally the horizontal line through the first chan-
nel. Next, we let each of these four lines take a constant value of either (0.5 ± 0.5B) where
7Borrowing the implementation in https://github.com/keras-team/keras/blob/master/
examples/cifar10_resnet.py, without data augmentation.
24
Published as a conference paper at ICLR 2021
Training set size
Training set size
(a) OoD accuracy drop of max-margin in (b) Random features max-margin on
two-class CIFAR10	Binary-MNIST
(d) FNN on Binary-MNIST
(c) Random features max-margin on
two-class CIFAR-10
(e) ResNet on CIFAR10 with SGD, fixed (f) ResNet on CIFAR10 with Adam, (plot
learning rate	saturates due to decaying learning rate)
Figure 5: Validating geometric skews in MNIST and CIFAR10: In Fig 5a, we show the OOD
accuracy drop of a random features based max-margin model trained to classify two classes in
CIFAR10. In the next few images, we demonstrate that MNIST and CIFAR10 datasets have the
property that the more the datapoints in the dataset, the larger the norm required to fit them. Specif-
ically, in Fig 5b and Fig 5c, we plot the max-margin norms of a random features representation (see
App B.1 for the definitions of two plotted lines). In Fig 5d, Fig 5e, Fig 5f, we plot the distance from
initialization of neural network models (presented for the sake of completeness).
B ∈ [-1, 1] denotes a “spurious feature scale”. Since each of these lines can take two configu-
rations, it allows us to instantiate 16 different configurations. We’ll however use only 10 of these
configurations, and arbitrarily fix a mapping from those configurations to the 10 classes. For con-
venience let us call these ten configurations xsp,1, . . . , xsp,10. Then, for any datapoint in class i, we
denote the probability of the spurious feature taking the value xsp,j , conditioned on y, as pi,j .
To induce a spurious correlation, we set pi,i > 0.1, and set all other pi,j := (1-pi,i)/10. Thus, every
value of the spurious feature xsp,j is most likely to occur with its corresponding class j. Finally,
note that to incorporate the spurious pixel, we zero out the original pixels in the image, and replace
them with the spurious pixels.
For the observation reported in Fig 1c, we use the value of B = 0.5 during training and testing. We
set pi,i = 0.5 for all classes. This means that on 50% of the data the spurious feature is aligned with
the class (we call this the ‘Majority’ group). On the remaining 50% data, the spurious feature takes
one of the other 9 values at random (we call this the ‘Minority’ group).
C.3.2 CIFAR 1 0 example with a line in the third channel
Here, we elaborate on the discussion regarding the dataset in Fig 2d. In this dataset, we add a line
to the last channel of CIFAR10 (regardless of the label), and vary its brightness during testing. We
argue that one way to understand the failure is via the fact that the “linear mapping” Constraint 5 is
broken. In particular, if we imagine that each channel contains the same invariant feature xinv (which
is almost the case as can be seen in Fig 7), then for simplicity, we can imagine this dataset to be of
the form (xinv, xinv + xsp) i.e., xinv and xsp are not orthogonal to each other. In this scenario, the
second co-ordinate can still be fully predictive of the label, and therefore the max-margin classifier
would rely on both the first and the second co-ordinate to maximize its margins e.g., wι ∙ Xinv +
w2(xinv + xsp)) + b where w1, w2 6= 0. Crucially, since the classifier has not quite disentangled xsp
from the invariant part of the second channel, this makes the classifier vulnerable to test-time shifts
on Xsp. In Sec A we detail this under the failure mode of Constraint 5, and visualize this failure in
Fig 4c, and also connect it to Theorem 1.
25
Published as a conference paper at ICLR 2021
(a) Images with lines whose colors are (b) Images with lines in the third channel
spuriously correlated with the label.	added regardless of label.
Figure 6: Our CIFAR10 examples: In Fig 6a we visualize the dataset discussed in App C.3.1.
Each row corresponds to a different value of the “scale” of the spurious feature. The left two images
correspond to datapoints where the spurious feature maps to the corresponding value for that label.
The right two images correspond to datapoints where the spurious feature maps to one of the 9 other
values. In Fig 6b, we visualize the dataset discussed in App C.3.2. The left two images correspond
to adding a horizontal line to the last channel, and the right corresponds to a vertical line.
(a) Channels in the non-orthogonal dataset
(b) Channels in the orthogonal dataset.
Figure 7: The channels in the CIFAR-10 dataset from Section C.3.2: In the top image, is our
main dataset, where one can see that the third channel has a faint copy of the original “invariant
feature” and an extra vertical line added onto it. In the bottom image, is the control dataset where
the last channel only contains the vertical line (thus making the spurious feature orthogonal to the
invariant features).
Dataset details. In Fig 6b, we visualize this dataset for multiple values of a spurious feature scale
parameter, B ∈ [-4, 4]. In particular, we take the last channel of CIFAR10 and add B to the middle
line of the channel. Since this can result in negative pixels, we add a value of 4 to all pixels in the
third channel, and then divide all those pixels by a value of 1 + 8 so that they lie in between 0 and 1.
(This normalization is the reason the color of the images differ from the original CIFAR10 dataset;
as such this normalization is not crucial to our discussion.)
More experimental results. We run two kinds of experiments: one where we add only a vertical
line to all images, and another where we add a horizontal line to 50% of the images (essentially
simulating data from two different domains). We also run experiments for two different values of B
during training, 0.2 and 1.0. As a “control” experiment, we completely fade out the original CIFAR
image in the third channel. Then, according to our explanation, the model should not fail in this
setting as data is of the form (xinv , xsp) i.e., the two features are orthogonally decomposed. We
summarize the key observations from these experiments (plotted in Fig 8) here:
26
Published as a conference paper at ICLR 2021
(c) Only vertical line, B = 2	(d) Vertical or horizontal lines,
B = 2.0 during training
during training
(b) Vertical or horizontal lines,
B = 0.2 during training
(a) Only vertical line, B = 0.2
during training
Figure 8: More experiments on CIFAR10 example from App C.3.2: Here the red triangle corre-
sponds to the value of the scale of spurious feature during training. ‘Non-orthogonal’ corresponds
to our main setting, and ‘Orthogonal’ corresponds to the control setting where the original image in
the third channel is zeroed out.
1.	As predicted, we observe that when trained on the “orthogonal” dataset (where the original CI-
FAR10 image is zerod out in the third channel), the OoD performance of the neural network is
unaffected. This provides evidence for our explanation of failure in the “non-orthogonal” dataset.
2.	We observe that training on ERM on the “multidomain dataset” (Figs 8b, 8d) that has both hor-
izontal and vertical lines, makes it more robust to test-time shifts compared to the dataset with
purely vertical lines (Figs 8a, 8c).
3.	Remarkably, even though the third channel has only a very faint copy of the image (Fig 7), the
classifier still learns a significant enough weight on the channel that makes it susceptible to shifts
in the middle line.
4.	We note that introducing a different kind of test-time shift such as a Gaussian shift, is not powerful
enough to cause failure since such a shift does not align with the weights learned, w2 . However,
shifts such as the line in this case, are more likely to be aligned with the classifier’s weights, and
hence cause a drop in the accuracy.
C.3.3 Cats vs. Dogs example with colors independent of label
Recall that the dataset from Fig 1d consists of a scenario where the images of cats vs. dogs (Elson
et al., 2007) are colored independently of the label. To generate this dataset, we set the first channel
to be zero. Then, for a particular color, we pick a value B ∈ [-1, 1], and then set the second channel
of every image to be 0.5 ∙ (1 -B) times the original first channel image, and the second channel to
be 0.5 ∙ (1 + B) times the original first channel image. For the blueish images, We set B = 0.90 and
for the greenish images, we set B = -0.90 (hence both these kinds of images have non-zero pixels
in both the green and blue channels). We visualize these images in Fig 9.
Then, on the training distribution, We randomly select p fraction of the data to be bluish and 1 - p
fraction to be greenish as described above. On the testing distribution, We force all datapoints to
be greenish. Finally, note that We randomly split the original cats vs. dogs dataset into 18000
points for training and use the remaining 5262 datapoints for testing/validation. In Fig 1d, We set
1 - p = 0.001, so about ≈ 20 greenish points must be seen during training. We shoW more detailed
results for 1 -p set to 0.001, 0.01 and 0.1 in Fig 10. We observe that the OoD failure does diminish
When 1 - p = 0.1.
Explaining this failure via an implicit spurious correlation. Peculiarly, even though there is
no explicit visual spurious correlation betWeen the color and the label here, We can still identify a
different kind of non-visual spurious correlation. To reason about this, first observe that, if the tWo
active channels (the blue and the green one) correspond to (x1, x2), then x1 +x2 is a constant across
all domains (and is fully informative of the label). Hence x1 + x2 can hence be thought of as an
invariant feature. On the other hand, consider the feature xdiff = x1 - x2 . During training time,
this feature Would correspond to a version of the original image scaled by a positive factor of 2B for
most datapoints (and scaled by a negative factor of -2B for a minority of datapoints). A classifier
that relies only on x1 + x2 to predict the label Will Work fine on our test distribution; but if it relies
on xdiff, it is susceptible to fail.
NoW, We informally argue Why an ERM-based classifier Would rely on xdiff. If We Were to think of
this in terms of a linear classifier, We can say that there must exist a Weight vector wdiff such that
27
Published as a conference paper at ICLR 2021
Class 1	Class 2
B = -0.9
B = 0.0
B = 0.9
Figure 9: Our cats vs. dogs examples: We present the dataset from App C.3.3 for various values
of B which determines how much of the image resides in the second channel vs. the third channel.
Spurious
=ChanI — Chan2
Figure 10: Experiments on Cat vs Dogs dataset from App C.3.3: Each of the first three plots above
corresponds to a different value of 1 -p i.e., the proportion of the minority, greenish datapoints. We
plot the OoD accuracy on various distributions for varying values of B ∈ [-1, 1]. The final plot
provides a visualization of these failure mode as explained in App C.3.3.
y ∙ (wdiff ∙ Xdiff) > 0 for the majority of the training datapoints. Then, We can imagine a Single-
dimensional spurious feature that corresponds to the component of the data along this direction i.e.,
Xsp := Wdiff ∙ Xdiff. Notably, for a majority of the datapoints in the training set we have XSP ∙ y > 0
and for a minority of the datapoints this feature does not necessarily correlate align With the label —
let,s Say XSP ∙ y < 0 for convenience. Then, this setting would effectively boil down to the geometric
skew setting considered by Theorem 1. In particular, we can say that when the minority group is
sufficiently small, the classifier would rely on the spurious feature Xsp to make its classification. We
visualize this in Fig 10d.
Thus, even though there is no color-label correlation in this dataset, there is still a spurious corre-
lation, although manifest in a way that does not visually stand out. While this is one such novel
kind of spurious correlation, the key message here is that we must not limit ourselves to thinking
of spurious correlations as straightforward co-occurences between the label and a spurious object in
the image.
Remark 3. It is worth distinguishing this spurious correlation with the one in the Colored MNIST
dataset (Arjovsky et al., 2019). In Colored MNIST, the color is correlated with the label, and one
can again think of Xdiff as giving rise to the spurious feature. However, the manner in which this
translates to a spurious feature is different. Here, the sign of each co-ordinate in Xdiff is indicative
of the true label (if it is positive, it means the image resides in the first channel, and is red, which is
correlated with the label). Mathematically, if we define Wdiff to be the vector of all ones, then we can
think of Wdiff ∙ Xdiff as a single-dimensional spurious feature here. In our case however, the vector
28
Published as a conference paper at ICLR 2021
wdiff that yields the single-dimensional spurious feature is different, and is given by the direction of
separation between the two classes.
Remark 4. We observe that in this non-standard spurious correlation setting, we require the minor-
ity group to be much smaller than in standard spurious correlation setting (like CMNIST) to create
similar levels of OoD failure. We argue that this is because the magnitude of the spurious feature
|xsp | is much smaller in this non-standard setting. Indeed, this effect of the spurious feature mag-
nitude is captured in Theorem 3: the lower bound on the spurious component holds only When the
minority group is sufficiently small to achieve K .，1/4 - 1/2%| i.e., when the spurious feature
magnitude |xsp | is smaller, we need the minority group to be smaller.
To see why |xsp | differs in magnitude between the standard and non-standard spurious correlation
settings, let us To see why |xsp | differs in magnitude between the standard and non-standard spurious
correlation settings, recall from the previous remark that in our setting, |xsp | = ∣wdiff ∙ Xdiff |, while in
standard spurious correlation settings |xsp| = Wdiff ∙ Xdiff. Intuitively, Wdiff ∙ Xdiff corresponds to sep-
arating all-positive-pixel images from all-negative-pixel images, which are well-separated classes.
On the other hand, Wdiff ∙ Xdiff corresponds to separating images of one real-world class from another,
which are harder to separate. Therefore, assuming that both weights vectors are scaled to unit norm,
we can see that ∣Wdiff ∙ XdiffI《|Wdiff ∙ Xdiff |
C.3.4 B inary-MNIS T example with colors independent of label
Similar to the cats vs. dogs dataset, we also consider a Binary-MNIST dataset. To construct this,
for each domain we pick a value B ∈ [-1, 1], and then set the first channel of every image to be
0.5 ∙ (1 + B) times the original MNIST image, and the second channel to be 0.5 ∙ (1 -B) times
the original MNIST image. To show greater drops in OoD accuracy, we consider a stronger kind of
test-time shift: during training time we set B to have different positive values so that the image is
concentrated more towards the first channel; during test-time, we flip the mass completely over to
the other channel by setting B = -1.
Here again, we can visualize the dataset in terms ofan invariant feature that corresponds to the sum
of the two channels, and a spurious feature that corresponds to Wdiff ∙ Xdiff. The exact visualization of
failure here is slightly different here since we’re considering a stronger kind of shift in this setting. In
particular, during training we,d have y ∙ (Wdiff ∙ Xdiff) > 0 for all the training datapoints in this setting.
Thus, we can think of this as a setting with no minority datapoints in the training set (see Fig 12a).
Then, as a special case of Theorem 1, we can derive a positive lower bound on the component of
the classifier along xsp. However, during training time, since Xdiff is no longer a positively scaled
version of the MNIST digits, the value of Wdiff ∙ Xdiff would no longer be informative of the label.
This leads to an overall drop in the accuracy.
Experimental results. We conduct two sets of experiments, one where we use only a single domain
to train and another with two domains (with two unique positive values of B). In both variations,
we also consider a control setting where the data is not skewed: in the single-domain experiment,
this means that we set B = 0 (both channels have the same mass); in the two-domain experiment
this means that we set B to be positive in one domain and negative in another. According to our
explanation above, in the single-domain control setting Fig 12, since Xinv = 0, the classifier is likely
to not rely on this direction and should be robust to test-time shifts in Xdiff. In the two-domain control
setting, since the classifier also sees negatively scaled images in Xdiff, it would be relatively robust
to such negative scaling during test-time. Indeed, we observe in Fig 12, that the performance on
the non-skewed, control datasets are robust. On the other hand, on the skewed datasets, we observe
greater drops in OoD accuracy when B is flipped, thus validating our hypothesis.
C.3.5 B inary-MNIS T example with high-dimensional spurious features
In this dataset, we consider a two-channel MNIST setting where the second channel is a set of
spurious pixels, each independently picked to be either 0 or 0.1 with probability 1 - p and p for
positively labeled points and p and 1 - p for negatively labeled points. During training we set
p = 0.55 and p = 0.60 corresponding to two training domains, and during testing, we flip this to
p = 0.0. We visualize this dataset in Fig 11b. In Fig 13, we observe that the classifier drops to 0
accuracy during testing. To explain this failure, we can think of the sum of the pixels in the second
channel as a spurious feature: since these features are independently picked, with high probability,
29
Published as a conference paper at ICLR 2021
Class 1	Class 2	Class 1	Class 2	Majority	Minority
B= -1.0	p=0.0	B= -1.0
B = 0.5	p = 0.7	B = 0.1
B=1.0
p= 1.0
(b) Each pixel in second channel
independently drawn correlated with label.
B=1.0
(a) Colored identically regardless of the label.
(c) All pixels in the second channel of the
same value, correlated with the label.
Figure 11:	Our Binary-MNIST examples: In Fig 11a we present the dataset from App C.3.4 for
various values of B which determines how much of the image resides in the first channel. Fig 11b
presents the dataset from App C.3.5 where the second channel pixels are individually picked to align
with the label with probability p (the difference is imperceptible because the pixels are either 0 or
0.1). Fig 11c presents the dataset for App C.4.1, where all pixels in the second channel are either 0
or B.
Spurious
=ChanI - Chan2
(a) Understanding failure
Spurious feature scale
(b) Single domain
Spurious feature scale
(c) Two domains
Figure 12:	Failure on the Binary-MNIST dataset in App C.3.4. In Fig 12a, we visualize the
failure observed on this dataset. Here we can think of the spurious feature as the difference between
the two channels (projected along a direction wdiff where they are informative of the label) and the
invariant feature as the sum of the two channels. Fig 12b and Fig 12c show the OoD performance
under different shifts in the scale of the spurious feature B. During training time this is set to the
value given by ‘Domain1’ and/or ‘Domain2’.
Pr[xspy>0]
{Level of spurious correlation)
(a) Single domain
Pr[xspy>0]
{Level of spurious correlation)
(b) Two domains
Figure 13:	Experiments for the high-dimensional spurious features dataset in App C.3.5. The
red triangles here denote the values of p used during training.
their sum becomes fully informative of the label. As discussed in Section C.3.4, this boils down
to the setting of Theorem 1 when there are no minority datapoints. In Appendix A, we make this
argument more formal (see under the discussion of Constraint 4 in that section).
30
Published as a conference paper at ICLR 2021
Spurious feature scale
(a) Statistically skewed, with
B=0.1
(b) No skew, with B = 0.1
(c) Statistically skewed, with
B=1.0
(d) No skew, with B = 1.0
Figure 14:	Experiments validating the effect of statistical skews on the MNIST dataset in
App C.4.1. The red triangle denotes the value of B during training.
C.4 Experiments from Sec 5 on statistical skews
Experiment on D2-dim. For the plot in Fig 3a, we train a linear model with no bias on the logistic
loss with a learning rate of 0.001, batch size of 32 and training set size of 2048.
C.4. 1 Binary-MNIST experiments validating the effect of statistical skews.
For this experiment, we consider a Binary-MNIST dataset where the second channel is set to be
either all-0 or all-constant pixels. We visualize this dataset in Fig 11c.
To generate our control and experimental datasets, we do the following. First, we sample a set of
Binary-MNIST images Sinv and their labels from the original MNIST distribution. Next we create
two datasets Smaj and Smin by taking each of these invariant images, and appending a spurious
feature to it. More precisely, we let Smin = {((xinv, xsp), y) where xsp = B(y + 1)|xinv ∈ Sinv} and
Smaj = {((xinv, xsp), y) where xsp = B(-y + 1)|xinv ∈ Sinv}.
We then define a “control” dataset Scon := Smaj ∪ Smin, which has a 1:1 split between the two groups
of points. Next, we create an experimental “duplicated” dataset Sexp := Smaj ∪ Smin ∪ Sdup where
Sdup is a large dataset consisting of datapoints randomly chosen from Smaj, thus creating a spurious
correlation between the label and the spurious feature. The motivation in creating datasets this way
is that neither Scon and Sexp have geometric skews; however Sexp does have statistical skews, and so
any difference in training on these datasets can be attributed to those skews.
Observations. In our experiments, we let Sinv be a set of 30k datapoints, and so Scon has 60k
datapoints. We duplicate Smin nine times so that Sexp has 330k datapoints and has a 10 : 1 ratio
between the two groups. We consider two different settings, one where B = 0.1 during training and
B = 1.0 during training. During testing, we report the accuracy under two kinds of shifts: shifting
the value of B, and also shifting the correlation completely to one direction (i.e., by concentrating
all mass on the minority/majority group). As reported in Fig 14, the statistically skewed dataset does
suffer more during test-time when compared to the unskewed dataset.
C.4.2 CIFAR 1 0 experiments validating the effect of statistical skews.
For this experiment, we consider the same CIFAR-10 dataset that we design in Section C.3.1 i.e.,
we introduce lines in the dataset, that can take 10 different color configurations each corresponding
to one of the 10 different classes. Here, the scale B of the spurious feature varies from [-1, 1] (see
Section C.3.1 for more details on this).
The way we construct the control and experimental dataset requires a bit more care here since we
have 10 classes. Specifically, we replicate Sinv ten times, and to each copy, we attach a spurious
feature of a particular configuration. This creates Scon which has no geometric or statistical skews.
Also, the has a size of |Scon| = 10|Sinv|. Then, we consider the 1/10th fraction of points in Scon
where the spurious feature has the correct configuration corresponding to the label. We create a large
duplicate copy of this subset that is 81 times larger than it (we do this by randomly sampling from
that subset). We add this large duplicate set to Scon to get Sexp. This gives rise to a dataset where for
31
Published as a conference paper at ICLR 2021
(a) Statistically skewed, with
B=0.1
(b) No skew, with B = 0.1
(c) Statistically skewed, with
B=1.0
(d) No skew, with B = 1.0
Figure 15: Experiments validating the effect of statistical skews on the MNIST dataset in
App C.4.1. The red triangle denotes the value of B during training.
any label, there is a 10 : 1 ratio8 between whether the spurious feature matches the label or where it
takes one of the other nine configurations.
Observations. We run experiments by setting |Sinv | = 5k and so |Scontrol | = 50k and Sexp = 455k .
During training we try two different values of B, 0.1 and 1 respectively. During testing, as in the
previous section, we vary both the scale of the spurious feature and also its correlation by evaluating
individually on the minority dataset (where the spurious features do not match the label) and majority
datasets (where the spurious features match the label). Here, again we observe that the model trained
on the non-skewed dataset is less robust, evidently due to the statistical skews.
It is worth noting that even though there is no statistical or geometric skew in the control dataset,
we observe in Fig 15 (b) that the classifier is not completely robust to shifts in the spurious feature.
We suspect that this may point to other causes of failure specific to how neural network models train
and learn representations.
D Solutions to the OoD problem
Our discussion regarding geometric and statistical skews tells us about why ERM fails. A natural
follow-up is to ask: how do we fix the effect of these skews to learn a good classifier? Below, we
outline some natural solutions inspired by our insights.
Solution for geometric skews. Our goal is to learn a margin-based classifier that is not biased by
geometric skews, and therefore avoids using the spurious feature. Recall that we have a majority
subset Smaj of the training set which corresponds to datapoints where XSP ∙ y > 0 and a minority
subset Smin which corresponds to datapoints where XSP ∙ y < 0. Our insight from the geometric
skews setting is that the max-margin classifier fails because it is much “harder” (in terms of `2
norm) to classify the majority dataset Smaj using xinv when compared to classifying Smin using xinv.
A natural way to counter this effect would be to somehow bring the difficulty levels of these datasets
closer. In particular, we propose a “balanced” max-margin classifier wbal that does the following:
/ ,
Wbal = arg max min	{wbal ∙ x|x ∈ Smaj},	{c ∙ Wbal ∙ x|x ∈ Smin}
k wbal k = 1	X-----------{Z---------} S--------------V-------------'
margin on majority group downscaled margin on minority group
where c is a sufficiently small constant. In words, we consider a classifier that maximizes a “bal-
anced” margin on the dataset. The balanced margin is computed by scaling down the margins on
the minority datapoints, thereby making that subset artificially harder, and thus diminishing the ge-
ometric skew. For an appropriately small choice of c, we can expect the balanced max-margin to
minimize its reliance on the spurious feature.
8Here’s the calculation: in Scontrol, we have a subset of size |Sinv| where the spurious feature is aligned with
the label and in the remaining 9|Sinv| datapoint, the spurious feature is not aligned. So, if we add 81|Sinv|
datapoints with matching spurious features, we’ll have a a dataset where 90|Sinv| datapoints have matching
spurious features while 9|Sinv | don’t, thus creating the desired 10 : 1 ratio.
32
Published as a conference paper at ICLR 2021
θ
3θ2θlθ
ULION S
16	64	256	1Θ24
Training set size
IuaUOdUJOU SnoTJndS
Epoch
(a) Evidence of geometric skews	(b) Evidence of statistical skews
Figure 16: Experiments validating geometric and statistical skews on the obesity dataset: In
Fig 16a, we show that the `2 norm required to fit the data in the invariant feature space grows with
dataset size. As discussed earlier, this would result in a geometric skew, which explains why a
max-margin classifier would rely on the spuriously correlated feature. In Fig 16b, we show that the
spurious component wsp/kwk of the gradient descent trained classifier converges to zero very slowly
depending on the level of statistical skew.9
Note that this sort of an algorithm is applicable only in settings like those in fairness literature where
we know which subset of the data corresponds to the minority group and which subset corresponds
to the majority group.
Solution for statistical skews. One natural way to minimize the effect of statistical skews is to use
`2 weight decay while learning the classifier: weight decay is known to exponentially speed up the
convergence rate of gradient descent, leading to the max-margin solution in polynomial time. Note
that this doesn’t require any information about which datapoint belongs to the minority group and
which to the majority.
In the setting where we do have such information, we can consider another solution: simply over-
sample the minority dataset while running gradient descent. This would result in a dataset with
no statistical skews, and should hence completely nullify the effect of statistical skews. Thus, our
argument provides a theoretical justification for importance sampling for OoD generalization.
E Demonstrating skews on a non-image-classification dataset
So far, we have demonstrated our insights in the context of image classification tasks. However, our
theoretical framework is abstract enough for us to apply these insights even in non-image classifi-
cation tasks. To demonstrate this, we consider an obesity estimation task based on the dataset from
Palechor & de la Hoz Manotas (2019).
Dataset details. The dataset consists of 16 features including height, weight, gender and habits of
a person. The label takes one of six different values corresponding to varying levels of obesity. We
convert this to a binary classification task by considering the first three levels as one class and the last
three levels as another; we ignore any datapoint from the middle level, resulting in a dataset of 1892
points. We randomly split the data to extract 729 test datapoints. The dataset also has a categorical
variable corresponding to the preferred mode of transport of the individual, which we convert to five
binary features (corresponding to automobile, motorbike, bike, public transport and walking). We
then scale all the resulting 20 features to lie between -1 and 1. For all our experiments, we will
consider fitting a linear classifier directly on these features.
Next, we construct a “biased” training set by sampling from the above training set in a way that the
public transport feature becomes spuriously correlated with the label. For convenience we denote
this feature as xsp and the remaining features as xinv. Note that in the original dataset there is not
much correlation between Xsp and y. In particular PrDtrain [x§p ∙ y > 0] ≈ 0.47 (a value close to 0.5
implies no spurious correlation). Indeed, a max-margin classifier w trained on the original dataset
does not rely on this feature. In particular, wsp/kwk is as small as 0.008. Furthermore, this classifier
achieves perfect accuracy on all of the test dataset. However, as we discuss below, the classifier
learned on the biased set does rely on the spurious feature.
33
Published as a conference paper at ICLR 2021
Geometric skews. We sample a biased training dataset that has 581 datapoints for which xsp = y
and 10 datapoints for which xsp = -y. We then train a max-margin classifier on this dataset,
and observe that its spurious component wsp/kwk is as large as 0.12 (about 15 times larger than
the component of the max-margin on the original dataset). But more importantly, the accuracy of
this model on the test dataset where xsp = y is 99.5% while the accuracy on a test dataset where
xsp = -y is only 57.20%. In other words, the classifier learned here relies on the spurious feature,
and suffers from poor accuracy on datapoints where the spurious feature does not align with the
label. Why does this happen? Our theory says that this must be because of the fact that as we
increase the number of training datapoints, it requires greater and greater `2 max-margin norm to fit
the data using only the invariant feature space (and ignoring the spurious feature). Indeed, we verify
that this increase does happen, in Fig 16a.
Statistical skews. To demonstrate the effect of statistical skews, we take a similar approach as in
earlier sections. We consider a dataset where we have the same number of unique data in the majority
group (where xsp = y) and in the minority group (where xsp = -y). However, the majority group
contains many duplicates of its unique points, outnumbering the minority group. In particular, we
consider two different datasets of 500 datapoints each, and in one dataset, the majority group forms
0.75 fraction of the data, and in the other it forms a 0.85 fraction of the data. Note that in both these
datasets, the max-margin classifier does not rely on the spurious feature since there’s no geometric
skew.
To demonstrate the effect of the statistical skew, we train a linear classifier to minimize the logistic
loss using SGD with a learning rate of 0.01 and batch size of 32 for as many as 10k epochs (which is
well beyond the number of epochs required to fit the dataset to zero error). We then verify in Fig 16b
that gradient descent takes a long time to let the spurious component of the classifier get close to its
final value which is close to zero. This convergence rate, as we saw in our earlier experiments, is
slower when the statistical skew is more prominent.
Important note. We must caution the reader that this demonstration is merely intended to showcase
that our theoretical insights can help understand the workings of a classifier in a practically impor-
tant, non-image classification dataset. However, we make no recommendations about how to deal
with such high-risk tasks in practice. Such tasks require the practitioner to make careful ethical and
social considerations which are beyond the scope of this paper. Indeed, empirical benchmarks for
evaluating OoD generalization (Gulrajani & Lopez-Paz, 2020) are largely based on low-risk image
classification tasks as it provides a safe yet reasonable test-bed for developing algorithms.
Remark on synthetic vs. natural spurious feature shifts. It would be a valuable exercise to
validate our insights on datasets with naturally-embedded spurious features. However, in order to
test our insights, it is necessary to have datasets where one can explicitly quantify and manipulate
the spurious features e.g., we need this power to be able to discard the spurious feature and examine
the `2 norms of the max-margin in the invariant feature space. Currently though, OoD research lacks
such datasets: we either have MNIST-like datasets with synthetic but quantifiable spurious features,
or realistic datasets like PACS (Asadi et al., 2019), VLCS (Fang et al., 2013) where it is not even
clear what the spurious feature is. The lack of datasets with natural yet quantifiable spurious features
is a gap that is beyond the scope of this paper, and is worth being bridged by the OoD community
in the future.
34