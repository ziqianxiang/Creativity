Published as a conference paper at ICLR 2021
Optimal Regularization Can Mitigate Double
Descent
Preetum Nakkiran
Harvard University
preetum@cs.harvard.edu
Prayaag Venkat
Harvard University
pvenkat@g.harvard.edu
Sham Kakade
Microsoft Research & University of Washington
sham@cs.washington.edu
Tengyu Ma
Stanford University
tengyuma@stanford.edu
Ab stract
Recent empirical and theoretical studies have shown that many learning algo-
rithms - from linear regression to neural networks - can have test performance that
is non-monotonic in quantities such the sample size and model size. This strik-
ing phenomenon, often referred to as “double descent”, has raised questions of if
we need to re-think our current understanding of generalization. In this work, we
study whether the double-descent phenomenon can be avoided by using optimal
regularization. Theoretically, we prove that for certain linear regression models
with isotropic data distribution, optimally-tuned `2 regularization achieves mono-
tonic test performance as we grow either the sample size or the model size. We
also demonstrate empirically that optimally-tuned `2 regularization can mitigate
double descent for more general models, including neural networks. Our results
suggest that it may also be informative to study the test risk scalings of various
algorithms in the context of appropriately tuned regularization.
1 Introduction
Recent works have demonstrated a ubiquitous “double descent” phenomenon present in a range
of machine learning models, including decision trees, random features, linear regression, and deep
neural networks (Opper, 1995; 2001; Advani & Saxe, 2017; Spigler et al., 2018; Belkin et al., 2018;
Geiger et al., 2019b; Nakkiran et al., 2020; Belkin et al., 2019; Hastie et al., 2019; Bartlett et al.,
2019; Muthukumar et al., 2019; Bibas et al., 2019; Mitra, 2019; Mei & Montanari, 2019; Liang &
Rakhlin, 2018; Liang et al., 2019; XU & Hsu, 2019; Derezinski et al., 2019; Lampinen & Ganguli,
2018; Deng et al., 2019; Nakkiran, 2019). The phenomenon is that models exhibit a peak of high
test risk when they are just barely able to fit the train set, that is, to interpolate. For example, as we
increase the size of models, test risk first decreases, then increases to a peak around when effective
model size is close to the training data size, and then decreases again in the overparameterized
regime. Also surprising is that Nakkiran et al. (2020) observe a double descent as we increase
sample size, i.e. for a fixed model, training the model with more data can hurt test performance.
These striking observations highlight a potential gap in our understanding of generalization and
an opportunity for improved methods. Ideally, we seek to use learning algorithms which robustly
improve performance as the data or model size grow and do not exhibit such unexpected non-
monotonic behaviors. In other words, we aim to improve the test performance in situations which
would otherwise exhibit high test risk due to double descent. Here, a natural strategy would be to
use a regularizer and tune its strength on a validation set. This motivates the central question of this
work:
When does optimally tuned regularization mitigate or remove the double-descent phenomenon?
Another motivation is the fact that double descent is largely observed for unregularized or under-
regularized models in practice. As an example, Figure 1 shows a simple linear ridge regression
1
Published as a conference paper at ICLR 2021
Figure 1: Test Risk vs. Num. Samples
for Isotropic Ridge Regression in d =
500 dimensions. Unregularized regression
is non-monotonic in samples, but optimally-
regularized regression (λ = λopt) is mono-
tonic. In this setting, the optimal regularizer
λopt does not depend on number of samples
n (Lemma 2), but this is not always true - see
Figure 2.
setting in which the unregularized estimator exhibits double descent, but an optimally-tuned regu-
larizer has monotonic test performance.
Our Contributions: We study this question from both a theoretical and empirical perspective.
Theoretically, we start with the setting of high-dimensional linear regression. Linear regression is
a sensible starting point to study these questions, since it already exhibits many of the qualitative
features of double descent in more complex models (e.g. Belkin et al. (2019); Hastie et al. (2019)
and further related works in Section 1.1). Our work shows that optimally-tuned ridge regression can
achieve both sample-wise monotonicity and model-size-wise monotonicity under certain assump-
tions. Concretely, we show
1.	Sample-wise monotonicity: In the setting of well-specified linear regression with isotropic
features/covariates (Figure 1), we prove that optimally-tuned ridge regression yields monotonic test
performance with increasing samples. That is, more data never hurts for optimally-tuned ridge
regression. (See Theorem 1).
2.	Model-wise monotonicity: We consider a setting where the input/covariate lives in a high-
dimensional ambient space with isotropic covariance. Given a fixed model size d (which might be
much smaller than ambient dimension), we consider the family of models which first project the
input to a random d-dimensional subspace, and then compute a linear function in this projected
“feature space.” (This is nearly identical to models of double-descent considered in Hastie et al.
(2019, Section 5.1)). We prove that in this setting, as we grow the model-size, optimally-tuned
ridge regression over the projected features has monotone test performance. That is, with optimal
regularization, bigger models are always better or the same. (See Theorem 3).
3.	Monotonicity in the real-world: We also demonstrate several richer empirical settings where
optimal `2 regularization induces monotonicity, including random feature classifiers and convolu-
tional neural networks. This suggests that the mitigating effect of optimal regularization may hold
more generally in broad machine learning contexts. (See Section 5).
A few remarks are in order:
Problem-specific vs Minimax and Bayesian. It is worth noting that our results hold for all linear
ground-truths, rather than holding for only the worst-case ground-truth or a random ground-truth.
Indeed, the minimax optimal estimator or the Bayes optimal estimator are both trivially sample-wise
and model-wise monotonic with respect to the minimax risk or the Bayes risk. However, they do not
guarantee monotonicity of the risk itself for a given fixed problem. In particular, there exist minimax
optimal estimators which are not sample-monotonic in the sense we desire.
Universal vs Asymptotic. We also remark that our analysis is not only non-asymptotic but also
works for all possible input dimensions, model sizes, and sample sizes. To our knowledge, the
results herein are the first non-asymptotic sample-wise and model-wise monotonicity results for
linear regression. (See discussion of related works Hastie et al. (2019); Mei & Montanari (2019)
for related results in the asymptotic setting). Our work reveals aspects of the problem that were not
2
Published as a conference paper at ICLR 2021
present in prior asymptotic works. For example, we empirically show that optimal regularization
can eliminate even “triple descent” in ridge regression (Figure 2). Moreover, we show that for non-
Gaussian covariates, optimally-tuned ridge regression is not always sample-monotonic: we give a
counterexample in Section 4.
Towards a more general characterization. Our theoretical results crucially rely on the covariance
of the data being isotropic. A natural next question is if and when the same results can hold more
generally. A full answer to this question is beyond the scope of this paper, though we give the
following results:
1.	Optimally-tuned ridge regression is not always sample-monotonic: we show a counterex-
ample for a certain non-Gaussian data distribution and heteroscedastic noise. We are not
aware of prior work pointing out this fact. (See Section 4 for the counterexample and
intuitions.)
2.	For non-isotropic Gaussian covariates, we can achieve sample-wise monotonicity with a
regularizer that depends on the population covariance matrix of data. This suggests unla-
beled data might also help mitigate double descent in some settings, because the population
covariance can be estimated from unlabeled data. (See Appendix B).
3.	For non-isotropic Gaussian covariates, we conjecture that optimally-tuned ridge regression
is sample-monotonic even with a standard `2 regularizer (as in Figure 2). We derive a
sufficient condition for this conjecture. Due to that current random matrix theory may be
insufficient to verify this conjecture, we verify it numerically on a wide variety of cases.
(See Appendix B for details).
The last two results above highlight the importance of the form of the regularizer, which leads to the
open question: “How do we design good regularizers which mitigate or remove double descent?”
We hope that our results can motivate future work on mitigating the double descent phenomenon,
and allow us to train high performance models which do not exhibit nonmonotonic behaviors.
1.1 Related Works
The study of nonmonotonicity in learning algorithms existed prior to double descent and has a long
history going back to (at least) Trunk (1979) and LeCun et al. (1991); Le Cun et al. (1991), where the
former was largely empirical observations and the latter studied the sample non-nonmonotonicity of
unregularized linear regression in terms of the eigenspectrum of the covariance matrix; the difference
to our works is that we study this in the context of optimal regularization. In fact, Duin (1995;
2000); Opper (2001); Loog & Duin (2012). Loog et al. (2019) introduces the same notion of risk
monotonicity which we consider, and studies several examples of monotonic and non-monotonic
procedures.
Double descent of test risk as a function of model size was considered recently in more generality
by Belkin et al. (2018). Similar behavior was observed empirically in earlier work in somewhat
more restricted settings Trunk (1979); Opper (1995; 2001); Skurichina & Duin (2002); Le Cun et al.
(1991); LeCun et al. (1991) and more recently in Advani & Saxe (2017); Geiger et al. (2019a);
Spigler et al. (2018); Neal et al. (2018). Recently Nakkiran et al. (2020) demonstrated a generalized
double descent phenomenon on modern deep networks, and highlighted “sample non-monotonicity”
as an aspect of double descent.
A recent stream of theoretical works consider model-wise double descent in simplified settings—
often via linear models for regression or classification. This also connects to works on high-
dimentional regression in the statistics literature. A partial list of works in these areas include
Belkin et al. (2019); Hastie et al. (2019); Bartlett et al. (2019); Muthukumar et al. (2019); Bibas
et al. (2019); Mitra (2019); Mei & Montanari (2019); Liang & Rakhlin (2018); Liang et al. (2019);
XU & HsU (2019); Derezinski et al. (2019); Lampinen & Ganguli (2018); Deng et al. (2019); Nakki-
ran (2019); Mahdaviyeh & Naulet (2019); Dobriban et al. (2018); Dobriban & Sheng (2019); Kobak
et al. (2018). Of these, most closely related to oUr work are Hastie et al. (2019); Dobriban et al.
(2018); Mei & Montanari (2019). Specifically, Hastie et al. (2019) considers the risk of UnregUlar-
ized and regUlarized linear regression in an asymptotic regime, where dimension d and nUmber of
samples n scale to infinity together, at a constant ratio d/n. In contrast, we show non-asymptotic
resUlts, and are able to consider increasing the nUmber of samples for a fixed model, withoUt scaling
3
Published as a conference paper at ICLR 2021
both together. Mei & Montanari (2019) derive similar results for unregularized and regularized ran-
dom features, also in an asymptotic limit. The non-asymptotic versions of the settings considered in
Hastie et al. (2019) are almost identical to ours— for example, our projection model in Section 3 is
nearly identical to the model in Hastie et al. (2019, Section 5.1). Finally, subsequent to our work,
d’Ascoli et al. (2020) identified triple descent in an asymptotic setting.
2 Sample Monotonicity in Ridge Ridgression
In this section, we prove that optimally-regularized ridge regression has test risk that is monotonic
in samples, for isotropic gaussian covariates and linear response. This confirms the behavior empir-
ically observed in Figure 1. We also show that this monotonicity is not “fragile”, and using larger
than larger regularization is still sample-monotonic (consistent with Figure 1).
Formally, we consider the following linear regression problem in d dimensions. The input/covariate
X ∈ Rd is generated from N(0, Id), and the output/response is generated by y =(x, β*〉 + ε with
ε 〜 N (0, σ2) for some unknown parameter β* ∈ Rd. We denote the joint distribution of (x, y) by
D. We are given n training examples {(xi, yi)}in=1 i.i.d sampled from D. We aim to learn a linear
model fβ(x) = (x, β with small population risk R(β) := E(x,y)〜D[((x, βi — y)2]. For simplicity,
let X ∈ Rn×d be the data matrix that contains xi> ’s as rows and let ~y ∈ Rn be column vector that
contains the responses yis as entries. For any estimator βn(X, y) as a function of n samples, define
the expected risk of the estimator as:
—.人. -.d . 一.-
RMn)=X,yEDn[R(en(X，~))]
(1)
We consider the regularized least-squares estimator, also known as the ridge regression estimator.
For a given λ > 0, define
βn,λ := argmin ∣Xβ - ~∣∣2 + λ∣∣β||2 = (XTX + λId)-1XT~	(2)
β
Here Id denotes the d dimensional identity matrix. Let λonpt be the optimal ridge parameter (that
achieves the minimum expected risk) given n samples: λnpt := argminλ0≥0 R(βn,λ)). Let βnpt
be the estimator that corresponds to the λnpt. That is, βnpt := argmin. ∣∣Xβ 一 ~||2 + λnpt∣∣β∣∣2.
Our main theorem in this section shows that the expected risk of βnpt monotonically decreases as n
increases.
Theorem 1.	In the setting above, the expected test risk of optimally-regularized well-specified
isotropic linear regression is monotonic in samples. That is, for all β* ∈ Rd and all d ∈ N, n ∈
N,σ > 0,
R(βn+ι) ≤ R(βnpt)
The above theorem shows a strong form of monotonicity, since it holds for every fixed ground-
truth β*, and does not require averaging over any prior on ground-truths. Moreover, it holds non-
asymptotically, for every fixed n, d ∈ N. Obtaining such non-asymptotic results is nontrivial, since
we cannot rely on concentration properties of the involved random variables.
In particular, evaluating R(βnpt) as a function of the problem parameters (n, σ, β*, and d) is techni-
cally challenging. In fact, we suspect that a simple closed form expression does not exist. The key
idea towards proving the theorem is to derive a “partial evaluation” — the following lemmas shows
that We can write R(βnpt) in the form of E[g(γ, σ, n, d, β*)] where Y ∈ Rd contains the singular val-
ues of X . We will then couple the randomness of data matrices obtained by adding a single sample,
and use singular value interlacing to compare their singular values.
Lemma 1. In the setting of Theorem 1, let γ = (γ1, . . . , γd) be the singular values of the data
matrix X ∈ Rn×d. (If n < d, we pad the γi = 0 for i > n.) Let Γn be the distribution of γ. Then,
the expected test risk is
R(βn,λ) =	E
(γi ,…Yd)〜Γn
d
X
i=1
∣∣β*∣∣2λ2∕d + σ2γ2
(Y2 + λ)2
+ σ2
4
Published as a conference paper at ICLR 2021
From Lemma 1, the below lemma follows directly by taking derivatives to find the optimal λ.
Lemma 2. In the setting of Theorem 1, the optimal ridge parameter is constant for all n: λonpt
dσ2
∣∣βσ∣∣2. Moreover, the optimal expected test risk can be written as
R(βnpt)=	E
(Yl ,…Yd)〜Γn
d
X
i=1
σ2
Y2 + dσ2∕∣∣β*∣∣2
+ σ2
(3)
Proofs of Lemma 1 and 2 are deferred to the Appendix, Section A.1. Now we are ready to prove
Theorem 1.
Proof of Theorem 1. Let Xe ∈ R(n+1)×d and X ∈ Rn×d be any two matrices which differ by
only the last row of X. By the Cauchy interlacing theorem Theorem 4.3.4 of Horn et al. (1990)
(c.f.,Lemma 3.4 of Marcus et al. (2014)), the singular values of X and X are interlaced: ∀i :
Yi-ι(X) ≥ Yi(X) ≥ Yi(X) where γi(∙) is the i-th singular value.
If we couple X and X, it will induce a coupling Π between the distributions Γn+1 and Γn , of the
singular values of the data matrix for n + 1 and n samples. This coupling satisfies that Yei ≥ Yi with
probability 1 for ({Yi}, {γi})〜 ∏. Now, expand the test risk using Lemma 2, and observe that each
term in the sum of Equation (4) below is monotone decreasing with Yi . Thus:
d
X
i=1
σ2
Y2 +dσ2∕∣∣β*∣∣2
+ σ2
ΓXX	σ2
N (Yl,…ed)〜Γn+1	i=i e2 +而2/||e*||2
=R(βn+1)
(4)
(5)
(6)
□
By similar techniques, we can also prove that overregularization —that is, using ridge parameters λ
larger than the optimal value— is still monotonic. This proves the behavior empirically observed in
Figure 1.
Theorem 2.	In the same setting as Theorem 1, over-regularized regression is also monotonic in
samples. That is, for all d ∈ N, n ∈ N,σ > 0,β* ∈ Rd, thefollowing holds
∀λ ≥ λ ：	R(βn+ι,λ) ≤ R(βn,λ)
where λ* = √⅛⅛2.
Proof. In Section A.1.
□
3 Model-wise Monotonicity in Ridge Regression
In this section, we show that for a certain family of linear models, optimal regularization prevents
model-wise double descent. That is, for a fixed number of samples, larger models are not worse than
smaller models.
We consider the following learning problem. Informally, covariates live in a p-dimensional am-
bient space, and we consider models which first linearly project down to a random d-dimensional
subspace, then perform ridge regression in that subspace for some d ≤ p. Formally, the covari-
ate x ∈ Rp is generated from N (0, Ip), and the response is generated by y = hx, θi + ε with
ε 〜 N(0,σ2) and for some unknown parameter θ ∈ Rp. Next, n examples {(xi,yi)}n=ι are
sampled i.i.d from this distribution. For a given model size d ≤ p, we first sample a random or-
thonormal matrix P ∈ Rd×p which specifies our model. We then consider models which operate
on (xei, yi) ∈ Rd × R, where xei = Pxi. We denote the joint distribution of (xe, y) by D. Here, we
emphasize that p is some large ambient dimension and d ≤ p is the size of the model we learn.
5
Published as a conference paper at ICLR 2021
1 -	r∙∙17-⅝	. . 1	1 •	II， ∕~∖	∕~ A∖ i'	. ∙	. ∙	∙ . Λ	11
For a fixed P, We want to learn a linear model fβ(χ) = hx, β for estimating y, With small mean
squared error on distribution: RP(β) := E(e,y)〜D[(〈x, βi - y)2]. For n samples (xi, yi), let X ∈
Rn×p be the data matrix, Xe = XPT ∈ Rn×d be the projected data matrix and ~y ∈ Rn be the
responses. For any estimator β(X, ~y) as a function of the observed samples, define the expected
risk of the estimator as:
--,ʌ ,	- - , ,	„ r
R(β):= E~ E	[Rp]β(X,y)]	⑺
PX,~〜Dn
We consider the regularized least-squares estimator. For a given λ > 0, define
βd,λ := argmin ||Xβ - ~||2 + λ∣∣β∣∣2 = (XTX + λId)-1XT~	(8)
β
Let λodpt be the optimal ridge parameter (that achieves the minimum expected risk) for a model of
size d, with n samples: λdpt := argminλ≥o R(βd,λ)). Let βopt be the estimator that corresponds to
the λdpt, that is βopt := argmi□β ||Xβ 一 ~||2 + λdpt∣∣β∣∣2. Now, our main theorem in this setting
shows that with optimal `2 regularization, test performance is monotonic in model size.
Theorem 3.	In the setting above, the expected test risk of the optimally-regularized model is mono-
tonic in the model size d. That is, for all p ∈ N, θ ∈ Rp , d ≤ p, n ∈ N, σ > 0, we have
R(βo+1) ≤ R(βopt)
The proof of Theorem 3 is in Appendix A.2, and follows closely the proof of Theorem 1.
4 Counterexamples to Monotonicity
In this section, we show that optimally-regularized ridge regression is not always monotonic in
samples. We give a numeric counterexample in d = 2 dimensions, with non-gaussian covariates and
heteroscedastic noise. This does not contradict our main theorem in Section 2, since this distribution
is not jointly Gaussian with isotropic marginals.
Counterexample. Here we give an example of a distribution (x, y) for which the expected error of
optimally-regularized ridge regression with n = 2 samples is worse than with n = 1 samples. This
counterexample is most intuitive to understand when the ridge parameter λ is allowed to depend on
the specific sample instance (X, ~y) as well as n1. We sketch the intuition for this below. Consider the
following distribution on (x, y) in d = 2 dimensions. This distribution has one “clean” coordinate
and one “noisy” coordinate. The distribution is: (x, y) = (~e1, 1) with probability 1/2, and (x, y) =
(~e2, ±10) w.p. 1/2. Where ±10 is uniformly random independent noise. This distribution is “well-
specified” in that the optimal predictor is linear in x: E[y|x] = hβ*,x〉for β* = [1,0]. However, the
noise is heteroscedastic.
For n = 1 samples, the estimator can decide whether to use small λ or large λ depending on if the
sampled coordinate is the “clean” or “noisy” one. Specifically, for the sample (x, y): Ifx = ~e1, then
the optimal ridge parameter is λ = 0. Ifx = ~e2, then the optimal parameter is λ = ∞.
For n = 2 samples, with probability 1/2 the two samples will hit both coordinates. In this case, the
estimator must chose a single value ofλ uniformly for both coordinates. This yields to a suboptimal
tradeoff, since the “noisy” coordinate demands large regularization, but this hurts estimation on the
“clean” coordinate.
It turns out that a slight modification to the above also serves as a counterexample to monotonicity
when the regularization parameter λ is chosen only depending on n (and not on the instance X, y).
The distribution is: (x, y) = (~e1, 1) w.p. 0.98 and (x, y) = (~e2, ±20) w.p. 0.02. This distribution
has the following property.
Theorem 4.	There exists a distribution D over (x, y) for x ∈ R2, y ∈ R with the following prop-
erties. Let βnpt be the optimally-regularized ridge regression solution for n samples (X, ~) from D.
Then:
1Recall, our model of optimal ridge regularization from Section 2 only allows λ to depend on n (not on
X, ~y).
6
Published as a conference paper at ICLR 2021
1.	D is “well-specified” in that ED[y|x] is a linear function of x,
2.	The expected test risk increases as a function of n, between n = 1 and n = 2. Specifically
R(βn= i)<R(βn=2)
Proof. For n = 1 samples, it can be confirmed analytically that the expected risk R(B；= 1) <
8.157. This is achieved with λ = 400/2401 ≈ 0.166597. For n = 2 samples, it can be confirmed
numerically (via Mathematica) that the expected risk R(βn= 2) > 8.179. ThiS is achieved with
λ = 0.642525.	一	□
5 Experiments
We now experimentally demonstrate that optimal `2 regularization can mitigate double descent, in
more general settings than Theorems 1 and 3.
5.1	Sample Monotonicity
Here we show various settings where optimal `2 regularization empirically induces sample-
monotonic performance.
Nonisotropic Regression. We first consider the setting of Theorem 1, but with non-isotropic co-
variantes x. That is, we perform ridge regression on samples (x, y), where the covariate x ∈ Rd
is generated from N(0, Σ) for Σ = Id. AS before, the response is generated by y =(x, β*〉+ ε
with ε 〜N(0, σ2) for some unknown parameter β* ∈ Rd. We consider the same ridge regression
estimator, βn,λ := argmine||xe — y ||2 + λ∣∣β ∣∣2.
Figure 2: Test Risk vs. Num. Samples
for Non-Isotropic Ridge Regression in
d = 30 dimensions. Unregularized re-
gression is non-monotonic in samples,
but optimally-regularized regression is
monotonic. Note the optimal regulariza-
tion λ depends on the number of samples
n.
Figure 2 shows one instance of this, for a particular choice of Σ and β* . The covariance Σ is
diagonal, with Σi,i = 10 for i ≤ 15 and Σi,i = 1 for i > 15. That is, the covariance has one “large”
eigenspace and one “small” eigenspace. The ground-truth β* = 0.1e~1 + e~30, which lies almost
entirely within the “small” eigenspace of Σ. The noise parameter is σ = 0.5.
We see that unregularized regression (λ = 0) actually undergoes “triple descent”2 in this setting,
with the first peak around n = 15 samples due to the 15-dimensional large eigenspace, and the sec-
ond peak at n = d. In this setting, optimally-regularized ridge regression is empirically monotonic
in samples (Figure 2). Unlike the isotropic setting of Section 2, the optimal ridge parameter λn is
no longer a constant, but varies with number of samples n.
2See also the “multiple descent” behavior of kernel interpolants in Liang et al. (2020).
7
Published as a conference paper at ICLR 2021
Random ReLU Features. We consider random ReLU features, in the random features framework
of Rahimi & Recht (2008). For a given number of features D , and number of samples n, the
random feature classifier is obtained by performing regularized linear regression on the embedding
X := ReLU(Wx), where W ∈ RDXd is a matrix with each entry sampled i.i.d N(0,1/Vd) and
ReLU applies pointwise. This is equivalent to a 2-layer fully-connected neural network with a
frozen (randomly-initialized) first layer, trained with `2 loss and weight decay. In Appendix A.4, we
apply random features to Fashion-MNIST Xiao et al. (2017). From Appendix Figure 4a, we see that
underregularized models are non-monotonic, but optimal `2 regularization is monotonic in samples.
Moreover, the optimal ridge parameter λ appears to be constant for all n, similar to our results from
the isotropic setting in Theorem 1.
5.2 Model-size Monotonicity
Here we empirically show that optimal `2 regularization can mitigate model-wise double descent.
Random ReLU Features. We consider the same experimental setup as in Section 5.1, but now fix
the number of samples n, and vary the number of random features D . This corresponds to varying
the width of the corresponding 2-layer neural network. Figure 4b in Appendix A.4 shows the test
error of the random features classifier, for n = 500 train samples and varying number of random
features. We see that underregularized models undergo model-wise double descent, but optimal `2
regularization prevents double descent.
Convolutional Neural Networks.
We follow the experimental setup of
Nakkiran et al. (2020) for model-
wise double descent, and add varying
amounts of `2 regularization (weight
decay). We chose the following set-
ting from Nakkiran et al. (2020), be-
cause it exhibits double descent even
with no added label noise. We con-
sider the same family of 5-layer con-
volutional neural networks (CNNs)
from Nakkiran et al. (2020), con-
sisting of 4 convolutional layers of
widths [k, 2k, 4k, 8k] for varying k ∈
N. We train and test on CIFAR-
100 (Krizhevsky et al., 2009), an im-
age classification problem with 100
Effect of Regularization: CNNs on CIFAR-100
10	20	30	40	50	60
CNN Model Size (width)
λ = 0.003
λ = 0.005
λ =0.01
Optimally Regularized
Figure 3: Test Error vs. Model Size for 5-layer CNNs on
CIFAR-100, with `2 regularization (weight decay). Note
that the optimal regularization λ varies with n.
classes. Inputs are normalized to
[-1, 1]d, and we use standard data-augmentation of random horizontal flip and random crop with 4-
pixel padding. All models are trained using Stochastic Gradient Descent (SGD) on the cross-entropy
loss, with step size 0.1/ ybT∕512j+l at step T. We train for 1e6 gradient steps, and use weight
decay λ for varying λ. Due to optimization instabilities for large λ, we use the model with the min-
imum train loss among the last 5K gradient steps. Figure 3 shows the test error of these models on
CIFAR-100. Although unregularized and under-reguarized models exhibit double descent, the test
error of optimally-regularized models is largely monotonic. Note that the optimal regularization λ
varies with the model size — no single regularization value is optimal for all models.
6 Discussion and Conclusion
In this work, we study the double descent phenomenon in the context of optimal regularization. We
show that, while unregularized or under-regularized models often have non-monotonic behavior,
appropriate regularization can eliminate this effect.
Theoretically, we prove that for certain linear regression models with isotropic covariates, optimally-
tuned `2 regularization achieves monotonic test performance as we grow either the sample size or
the model size. These are the first non-asymptotic monotonicity results we are aware of in linear
8
Published as a conference paper at ICLR 2021
regression. We also demonstrate empirically that optimally-tuned `2 regularization can mitigate
double descent for more general models, including neural networks. We hope that our results can
motivate future work on mitigating the double descent phenomenon, and allow us to train high
performance models which do not exhibit unexpected nonmonotonic behaviors.
Open Questions. Our work suggests a number of natural open questions. First, it is open to
prove (or disprove) that optimal ridge regression is sample-monotonic for non-isotropic Gaussian
covariates. We conjecture that it is, and outline a potential route to proving this (via Conjectures 1
and 2 in the Appendix). Second, more broadly, it is open to prove sample-wise or model-wise
monotonicity for more general (non-linear) models with appropriate regularizers. Finally, it is open
to understand why large neural networks in practice are often sample-monotonic in realistic regimes
of sample sizes, even without careful choice of regularization.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. arXiv preprint arXiv:1906.11300, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019.
Koby Bibas, Yaniv Fogel, and Meir Feder. A new look at an old problem: A universal learning
approach to linear regression. arXiv preprint arXiv:1905.04708, 2019.
Stephane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensional binary linear classification. arXiv preprint arXiv:1911.05822, 2019.
MichaI Derezinski, Feynman Liang, and Michael W. Mahoney. Exact expressions for double descent
and implicit regularization via surrogate random design, 2019.
Edgar Dobriban and Yue Sheng. Wonder: Weighted one-shot distributed ridge regression in high
dimensions. arXiv preprint arXiv:1903.09321, 2019.
Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals ofStatistics, 46(1):247-279, 2018.
Robert PW Duin. Small sample size generalization. In Proceedings of the Scandinavian Confer-
ence on Image Analysis, volume 2, pp. 957-964. PROCEEDINGS PUBLISHED BY VARIOUS
PUBLISHERS, 1995.
Robert PW Duin. Classifiers in almost empty spaces. In Proceedings 15th International Conference
on Pattern Recognition. ICPR-2000, volume 2, pp. 1-7. IEEE, 2000.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d'Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. arXiv preprint arXiv:1901.01608, 2019a.
Mario Geiger, Stefano Spigler, StePhane d'Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019b.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation, 2019.
9
Published as a conference paper at ICLR 2021
Roger A Horn, Roger A Horn, and Charles R Johnson. Matrix Analysis. Cambridge University
Press, 1990.
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. Optimal ridge penalty for real-world high-
dimensional data can be zero or negative due to the implicit ridge regularization. arXiv preprint
arXiv:1805.10939, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991.
Yann LeCun, Ido Kanter, and Sara A Solla. Second order properties of error surfaces: Learning time
and generalization. In Advances in neural information processing Systems, pp. 918-924, 1991.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel” ridgeless” regression can gener-
alize. arXiv preprint arXiv:1808.00387, 2018.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the risk of minimum-norm interpolants
and restricted lower isometry of kernels. arXiv preprint arXiv:1908.10292, 2019.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. 2020.
Marco Loog and Robert PW Duin. The dipping phenomenon. In Joint IAPR International Work-
shops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern
Recognition (SSPR), pp. 310-317. Springer, 2012.
Marco Loog, Tom Viering, and Alexander Mey. Minimizers of the empirical risk and risk mono-
tonicity. In Advances in Neural Information Processing Systems, pp. 7476-7485, 2019.
Yasaman Mahdaviyeh and Zacharie Naulet. Asymptotic risk of least squares minimum norm esti-
mator under the spike covariance model. arXiv preprint arXiv:1912.13421, 2019.
Adam W Marcus, Daniel A Spielman, and Nikhil Srivastava. Ramanujan graphs and the solution of
the kadison-singer problem. arXiv preprint arXiv:1408.4421, 2014.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Partha P. Mitra. Understanding overfitting peaks in generalization error: Analytical risk curves for
l2 and l1 penalized interpolation. ArXiv, abs/1906.03667, 2019.
Vidya Muthukumar, Kailas Vodrahalli, and Anant Sahai. Harmless interpolation of noisy data in
regression. arXiv preprint arXiv:1903.09139, 2019.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591, 2018.
Manfred Opper. Statistical mechanics of learning: Generalization. The Handbook of Brain Theory
and Neural Networks, 922-925., 1995.
Manfred Opper. Learning to generalize. Frontiers of Life, 3(part 2), pp.763-775., 2001.
10
Published as a conference paper at ICLR 2021
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing Systems, pp.1177-1184, 2008.
Marina Skurichina and Robert PW Duin. Bagging, boosting and the random subspace method for
linear classifiers. Pattern Analysis & Applications, 5(2):121-135, 2002.
Stefano Spigler, Mario Geiger, Stephane d'Ascoli, Levent Sagun, Giulio Biroli, and MatthieU Wyart.
A jamming transition from under-to over-parametrization affects loss landscape and generaliza-
tion. arXiv preprint arXiv:1810.09665, 2018.
Gerard V Trunk. A problem of dimensionality: A simple example. IEEE Transactions on pattern
analysis and machine intelligence, (3):306-307, 1979.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Ji Xu and Daniel J Hsu. On the number of variables to use in principal component regression. In
Advances in Neural Information Processing Systems, pp. 5095-5104, 2019.
11
Published as a conference paper at ICLR 2021
A Appendix
In Section A.1 and A.2 we provide the proofs for sample-monotonicity and model-size monotonic-
ity. In Section A.4 we include additional and omitted plots. In Section B we investigate whether
monotonicity provably holds in more general models, and present a monotonicity conjecture for
non-isotropic covariates.
A.1 Sample Monotonicity Proofs
First we prove Lemma 1.
Proof of Lemma 1. For isotropic x, the test risk is related to the parameter error as:
R(β) :=	E	[(hχ,βi-y)2]
(X,y)~D
=	E	[(hχ,β - β*i + η)2]
χ~N (0,id),η~N (0,σ2)
=Ilβ-β*ll2 + σ2
Plugging in the form of βn,λ and expanding:
_ ʃ ʌ	、	r 一 ʃ ʌ	、r
R闻I)=X,yEJR®N
E["βn,λ - B*||2]+ σ2
E [||(X T X + λI )-1X T y - β∣∣2]+ σ2
X,y
E	|||(XTX + λI)-1XT(Xβ* + η) - β*||2]+ σ2
x,η~N (0,σ2in)
E|1|(XT X + λI 尸 XT χβ* - β*"2] + XyI(XT X + λI 尸XT n112] + σ2
E[1|(XTX + λI 尸 XT Xβ* - βi2] + σ2 E[1|(XTX + λI 尸XT11F]+σ2
Now let X = U ΣV T be the full singular value decomposition of X, with U ∈ Rn×n , Σ ∈
Rn×d, V ∈ Rd×d. Let (γ1, . . . γd) denote the singular values, defining γi = 0 for i > min(n, d).
Then, continuing:
R(βn,λ)=段[∣∣diag({ γ2=λλ })V T β*∣∣2] + σ2 EX (Y 2 [λ)2 ]+ σ2	⑼
=	E	[∣∣diag({丁工})z∣∣2]+ σ2 E[X	? R ] + σ2	(10)
z~Unif(∣∣β*∣∣2Sd-1),∑	Y + λ	∑ 片(Y2 + λ)2
=乎 EX-J ]+ σ2 EX ^J ]+ σ2	(11)
d ∑ V (Yi + λ)2	∑ V (γ2 + λ)2
κrX llβ*ll2λ2" + σ2γ21 ,	2	门力
=E[ N —除F—]+σ	(12)
In Line (10) follows because by symmetry, the distribution of V is a uniformly random orthonormal
matrix, and Σ is independent of V. Thus, Z := VTβ* is distributed as a uniformly random point on
the unit sphere of radius ∣∣β* ∣∣2.
□
Next we prove Lemma 2.
12
Published as a conference paper at ICLR 2021
Proof of Lemma 2. First, we determine the optimal ridge parameter. Using Lemma 1, we have
∂i	、	∂
--R(yβn,λ) = ɪ E
dλ	dλ (γι,…Yd)~Γ
∣∣β∣∣2λ2/d + σ2γ2
(Y + λ)2
""Rd") (γι,.Ed)~r]X f
'----------V-----------}
>0
Thus, ∂dλR(βn,λ) = 0 =⇒ λ = y∣βσ2ι and We conclude that λnpt = ∣yβσ2∣
For this optimal parameter, the test risk follows from Lemma 1 as
XX σ
(Y1,…Yd)〜Γn	= Y +加2/||町|2
(13)
(14)
Proof of Theorem 2. We folloW a similar proof strategy as in Theorem 1: We invoke singular value
interlacing (γei ≥ γi) for the data matrix When adding a single sample. We then apply Lemma 1 to
argue that the test risk varies monotonically With the singular values.
We have
--,ʌ 、
R(βn,λ)
E [X llβ*“2λ2" + σ2γ2
(Y1,…Yd)~Γ J	(Y2 + λ)2
i X--------7--------'
S(γi)
and We compute hoW each term in the sum varies With γi:
Thus We have
∂⅛ X S(Yi) = ∂⅛S(Yi)
=(-2Yi )2llβ"2λ2 + " (Y - λ)
=(F)	(y2 + λ)3
dσ2	∂
λ ≥ 2W =⇒ 而S(Yi) ≤ 0
(15)
By the coupling argument in Theorem 1, this implies that the test risk is monotonic:
_ ʃ ʌ 、 ______ ʃ ʌ 、
R(βn+1,λ) - R(βn,λ)
d
(Yι,…YE 〜Xi Xi=1S(Yei)
(Yl,…Yd)~Γn
d
X S(Yi)
i=1
E
({Yi},{Yi})〜∏
d
XS(Yei)-S(Yi)
i=1
≤0
(16)
(17)
□
Where Π is the coupling. Line (17) folloWs from Equation (15), and the fact that the coupling obeys
Yi ≥ Yi.	□
13
Published as a conference paper at ICLR 2021
A.2 Projection Model Proofs
Lemma 3. For all θ ∈ Rp, d, n ∈ N, and λ > 0, let X ∈ Rn×p be a matrix with i.i.d. N (0, 1)
entries. Let P ∈ Rd×p be a random orthonormal matrix. Define X := XPT. Let (γ1, . . . , γm)
be the singular values of the data matrix X ∈ Rn×d, for m := max(n,d) (with Yi = 0 for
i > min(n, d)). Let Γd be the distribution of singular values (γ1, . . . , γm).
Then, the optimal ridge parameter is constant for all d: λdpt = 才血 §. where we define e2 :=
σ2 + p--d ∣∣θ∣∣2. Moreover, the optimal expected test risk can be written as
R(βOpt) = e2 + E
(YI ,…,Ym hrd
p	σe 2
2 +	e2p2-
.i=1 Yi + d∣W
Proof. This proof follows exactly analogously as the proof of Lemma 2 from Lemma 1, in Sec-
tionA.1.	□
Lemma 4. For all θ ∈ Rp, d, n ∈ N, and λ > 0, let X ∈ Rn×p be a matrix with i.i.d. N (0, 1)
entries. Let P ∈ Rd×p be a random orthonormal matrix. Define X := XPT and β := Pθ.
Let (γι,..., Ym) be the singular values of the data matrix X ∈ Rn×d, for m := max(n, d) (with
Yi = 0for i > min(n, d)). Let Γd be the distribution of singular values (Y1, . . . , Ym).
Then, the expected test risk is
, ʌ	,	- -	, △	, 3	“ r
R(βd,λ) := E ~ E	[Rp(βd,λ(X, y)]
P X ,y〜Dn
σ2 + (1 - d M2
+E
(YI,…,Ym)〜rd
p
X
i=1
(σ2 + 吩 I∣Θ∣∣2)Y2 + p2 I∣θ∣l2λ2
^FW
Proof of Lemma 4. We first define the parameter that minimizes the population risk. It follows
directly that:
βP := argmin RP(β) = Pθ
β∈Rd
First, we can expand the risk as
R(β) = E	[(hPχ,βi- y)2]	(18)
(e,y)〜D
=	E	[(hχ,PTβ - θ + η)2]	(19)
(e,y)~D,η~N (0,σ2)
=σ2 +	∣∣θ	-	PTβ∣∣2	(20)
=σ2 +	∣∣θ	-	Ptβ*∣∣2	+1|PTβ*	-	Ptβ∣∣2	(21)
+ 2h(θ - PTβ*),PTβ* - PTβi	(22)
=σ2 +	∣∣θ	-	Ptβ*∣∣2	+	∣∣ptβ*	-	Ptβ∣∣2	(23)
=σ2 +	∣∣θ	-	Ptpθ∣∣2	+	∣∣β* -	β∣∣2	(24)
The cross terms in Line (22) vanish because the first-order optimality condition for β * implies that
β* satisfies P(θ* - PTβ*) = 0. We now simplify each of the two remaining terms.
First, we have that:
Ellθ- p t pθll2 = (I-d )1 砒
(25)
14
Published as a conference paper at ICLR 2021
since PT P is an orthogonal projection onto a random d-dimensional subspace.
Now, recall We have ~ = Xθ + η where η 〜N(0, σ2In). Expand this as:
~y=Xθ+η	(26)
= XPTPθ + X(1 -PTP)θ+η	(27)
= Xeβ + ε + η	(28)
where ε := X(1 - PTP)θ. Note that conditioned on P, the three terms X, ε and η are conditionally
independent, since PTP and (I - PTP) project X onto orthogonal subspaces. And further, ε 〜
N(0,∣∣(1- P T P )θ∣∣2In).
..ʌ	... C E E ιιβ-β*ι∣2 P Xe ,y	2	(29)
=EE ||(XTX + λI)-1XTy - β*∣∣2 P Xe ,y	2	(30)
=E ~ E	||(XTX + λI)-1XT(Xβ* + ε + η) - β*∣∣2 P Xe ,y,ε,η	(31)
=E 〜E	[∣∣(XTX + λi)-1XTXβ* -β*∣∣2 P Xe ,y,ε,η	(32)
+ ll(XT X + λi )-1 XT ε∣∣2 + ∣∣(XT X + λi )-1XT η∣∣2 ]	(33)
	(34)
Now, since X is conditionally independent of ε conditioned on P,	
E~E	||(X T X + λI )-1X T ε∣∣2 px,y,ε∣P	(35)
=E E [∣I(XTX + λI)-1XTIlF] E [∣∣ε∣∣2] PX |P	ε∣P	(36)
=E[∣I(XTX + λI)-1XTIlF] E [∣∣ε∣∣2] Xe	P,ε	(37)
=E[∣∣(XTX + λi)-1XTIIF] ∙ E [∣∣X(1 - PTP)θ∣∣2] Xe	F P,X	2	(by definition of ε)
=e[∣∣(X T X + λi )-1X T ∣∣F ](j ∣∣θ∣∣2) ʌ-	/W	(38)
where Line (37) holds because the marginal distribution ofX does not depend on P.
Similarly,
E~E	||(X T X + λI )-1X T η∣∣2	(39)
PX ,y,η∣P
=σ2 E[||(XTX + λI)-1XT||F]	(40)
Xe
Now let X = U ΣV T be the full singular value decomposition of X, with U ∈ Rn×n , Σ ∈
Rn×d, V ∈ Rd×d. Let (γ1, . . . γm) denote the singular values, where m = max(n, d) and defining
γi = 0 for i > min(n, d).
Observe that by symmetry, X = XPT and P are independent, because the joint distribution (X, P)
is equivalent to the distribution (XQ, PQ) for a random orthonormal Q ∈ Rp×p. Thus X and
15
Published as a conference paper at ICLR 2021
β* = Pθ are also independent, and We have:
ee∣∣(XtX + λi)-1xtXβ* - β*∣∣2	(41)
PX
=EE [∣∣diag((丁工})VTβ*∣∣2]	(42)
β* V,Σ	Yi + λ
=EE [∣∣diag((丁)})VTβ*∣∣∣]	(43)
β* V,∑	γ∣ + λ
=E	E	[∣∣diag({√λ- })z∣∣∣]	(44)
β* Z〜Unif(I∣β*∣∣2Sd-1),∑	Yi + 入
=E [旭业• E[X 2 ∣λ∣ ^∣ ]]	(45)
β* P ∑ L (γ∣ + λ)∣
1	C 一	λ∣
=-E[∣∣PΘ∣∣∣]∙ E[£	]	(46)
p p	∑ V (YI+ λ)∣
d C	λ∣
=铲 ∣∣θ∣∣∣ EiX H + λ)∣ ]	(47)
Finally, continuing from Line (33), we can use Lines (38), (40), and (47) to write:
..ʌ	. . . rɪ E E ∣∣β -β*∣∣∣	(48)
PX ,y	
=(σ ∣ + p-d ∣∣θ∣∣∣ ) e[∣∣(X t X + λi )-1Xt∣∣F ] P	X	(49)
+ p>∣∣∣eX ⅛ ]	(50)
=(σ ∣+丁 ∣∣θ∣∣∣) E[X O⅛]	(51)
+ 4 l l θ l l ∣E[χ	] P∣	∑ U (γ∣ + λ)∣	(52)
U (σ∣ + p-dllθ l l ∣)γ∣ + 制∣θ l l ∣λ∣	
=E[>	p~τ^rrτ-p] ∑ J	(y∣ + λ)∣	(53)
NoW, We can continue from Line (24), and apply lines (25), to conclude:
E[R(β)] = σ ∣ + E[∣∣θ - P t Pθ∣∣∣ ]+ E[∣∣β* - β∣∣∣ ]
σ ∣+(1 - d )∣∣θ∣∣∣
+E
Σ
■ P
X
.i=1
(σ∣ + 宁 ∣∣θ∣∣∣ )y∣ + P l∣θ∣∣∣ λ∣ -
(γ∣ + λ)∣
□
ProofofTheorem 3. This follows analogously to the proof of Theorem 1, Let Xd and Xd+1 be
the observed data matrices for d and d + 1 model size. As in Theorem 1, there exists a coupling
Π between the distributions Γd and Γd+ι of the singular values of Xd and Xd+ι such that these
singular values are interlaced.
16
Published as a conference paper at ICLR 2021
Thus by Lemma 3,
R(βdpt) = σ + E
(YI ,…,Ym)〜rd
p
X
i=1
e2
2 + σ2p2
Yi + d∣∣θ∣l2
≥ σe 2 + E
(Y1 ,…,Ym)〜Γd + 1
p
X
i=1
σ
芯2 + e2P2
Yi + d∣∣θ∣∣2
r(^+ 1)
□
A.3 Nonisotropic Reduction
Here we observe that results on isotropic regression in Section 2 also imply that ridge regression
can be made sample-monotonic even for non-isotropic covariates, if an appropriate regularzier is
applied. Specifically, the regularizer depends on the covariance on the inputs. This follows from a
general equivalence between the non-isotropic and isotropic problems.
Lemma 5. For all n ∈ N, d ∈ N, λ ∈ R, σ∈ R, covariance Σ ∈ Rd×d, PSD matrix M ∈ Rd×d,
and ground-truth β* ∈ Rd,thefollowing holds.
Consider the following two problems:
1.	Regularized regression on isotropic covariates, and an M -regularizer. That is, suppose
n samples (x,y) are drawn with Covariates X 〜N(0, Id) and response y = (β*, x) +
N(0, σ2). Let X ∈ Rn×d be the matrix of covariates, and ~y the vector of responses.
Consider
βλ ：= argmin ∣∣Xβ 一 ~||2 + λ∣∣β∣∣M	(54)
β
Let R = Eχ,y [∣∣β — β*∣∣2] + σ2 be the expected test risk ofthe above estimator
2.	Regularized regression with Covariance Σ, and an (夕1/2MΣ1/2)-regularizer. That is,
suppose n samples (e, y) are drawn with covariates e 〜N(0, Σ) and response y 二
hz*,e) + N (0,σ2) ,for
z
Σ-"β*
Let Xe ∈ Rn×d be the matrix of covariates, and ~y the vector of responses. Consider
Zλ ：= argmin ||XZ — ~∣∣2 + λ∣∣z∣∣∑ι∕2Mςi∕2	(55)
z
Let R = Eχ y [||Z — z*∣∣∑ ] + σ2 be the expected test risk ofthe above estimator
Then, the expected test risks of the above two problems are identical:
— ~
R 二 R
1/2
ProofofLemma 5. The distribution of X m the Problem 2 is equivalent to XΣ1/2, where X is as m
Problem 1. Thus, the two settings are equivalent by the Change-of-variable β = Σ"z. Specifically,
Zλ ：= argmin ||XZ — ~∣∣2 + λ∣∣z∣∣∑ι∕2Mςi∕2	(56)
z
=argmin ||X£1/2Z — ~||2 + λzτ£1/2M£1/2Z	(57)
z
=argmin ||X£1/2Z — ~||2 + λzτ£1/2M£1/2Z	(58)
z
=Σ-1/2 argmin ∣∣Xβ — ~∣∣2 + λβτMβ	(59)
β=Σ1∕2z
17
Published as a conference paper at ICLR 2021
Further, the response〈z*,e)= hβ, x), and the test risk transforms identically:
R = E [∣∣Z-z*ll∑]+ σ2	(60)
Xe,y
=E [∣lβ - β*ll2]+ σ2	(61)
X,y
=R	(62)
□
This implies that if the covariance Σ is known, then ridge regression with a Σ-1 regularizer is
sample-monotonic.
Theorem 5. For all n ∈ N, d ∈ N, σ ∈ R ,covariance Σ ∈ Rd×d, and ground-truths β * ∈ Rd, the
following holds.
Suppose n samples (x, y) are drawn with Covariates X 〜N(0, Σ) and response y = hβ*,x) +
N(0, σ2). Let X ∈ Rn×d be the matrix of covariates, and ~y the vector of responses. For λ > 0,
consider the ridge regression estimator with Σ-1-regularizer:
βn,λ ：= argmin ∣Xβ 一 ~||2 + λ∣∣β∣∣∑-ι	(63)
β
Let R(βn,λ) := Ee ∣∣β 一 β*∣∣∑ + σ2 be the expected test risk Ofthe above estimator. Let λnt be the
optimal ridge parameter (that achieves the minimum expected risk) given n samples:
λnpt := argmin R(βn,λ))	(64)
λ
And let βnpt be the estimator that corresponds to the λopt. Then, the expected test risk of optimally-
regularized linear regression is monotonic in samples:
R(βnp+1)≤ R(βnpt)
Proof. This follows directly by applying the reduction in Lemma 5 for M = Id to reduce to the
isotropic case, and then applying the monotonicity of isotropic regression from Theorem 1.	□
A.4 Additional Plots
We apply random features to Fashion-MNIST Xiao et al. (2017), an image classification problem
with 10 classes. Input images x ∈ Rd are normalized and flattened to [-1, 1]d for d = 784. Class
labels are encoded as one-hot vectors y ∈ {e~1, . . . e~10} ⊂ R10.
B	Towards Monotonicity with General Covariates
Here we investigate whether monotonicity provably holds in more general models, inspired by the
experimental results. As a first step, we consider Gaussian (but not isotropic) covariances and
homeostatic noise. That is, we consider ridge regression in the setting of Section 2, but with
x 〜N(0, Σ), and y 〜(x, β*) + N(0, σ2). In this section, We observe that ridge regression can
be made sample-monotonic with a modified regularizer. We also conjecture that ridge regression
is sample-monotonic Without modifying the regularizer, and We outline a potential proof strategy
along With numerical evidence.
B.1	Adaptive Regularization
The results on isotropic regression in Section 2 imply that ridge regression can be made sample-
monotonic even for non-isotropic covariates, if an appropriate regularizer is applied. Specifically,
the appropriate regularizer depends on the covariance of the inputs. For X 〜N(0, Σ), the following
estimator is sample-monotonic for optimally-tuned λ: βn,λ := argming ∣∣Xβ 一 ~||2 + λ∣∣β∣∣∑-ι.
This follows directly from Theorem 1 by applying a change-of-variable; full details of this equiva-
lence are in Section A.3. Note that if the population covariance Σ is not known, it can potentially be
estimated from unlabeled data.
18
Published as a conference paper at ICLR 2021
(a) Test Classification Error vs. Number of Train-
ing Samples.
Figure 4: Double-descent for Random ReLU Features. Test classification error as a function of
model size and sample size for Random ReLU Features on Fashion-MNIST. Left: with D = 500
features. Right: with n = 500 samples. See Figures 7, 8 for the corresponding test Mean Squared
Error. See Appendix D of Nakkiran et al. (2020) for the performance of these unregularized models
plotted across Num. Samples × Model Size simultaneously.
(b) Test Classification Error vs. Model Size
(Number of Random Features).
CNN Model Size (width)
Figure 5: Train Error vs. Model Size for 5-layer CNNs on CIFAR-100, with `2 regularization
(weight decay).
B.2	Towards Proving Monotonicity
We conjecture that optimally-regularized ridge regression is sample-monotonic for non-isotropic
covariates, even without modifying the regularizer (as suggested by the experiment in Figure 2). We
derive a sufficient condition for monotonicity, which we have numerically verified in a variety of
instances. Specifically, we conjecture the following.
Conjecture 1. For all d ∈ N, and all PSD covariances Σ ∈ Rd×d, consider the distribution on
(x, y) where X 〜N(0, Σ), and y 〜〈x, β*〉+ N(0, σ2). Then, we conjecture that the expected test
risk of the ridge regression estimator: βn,λ := argming ∣∣Xβ — y ||2 + λ∣∣β ||2 for optimally-tuned
λ ≥ 0, is monotone non-increasing in number of samples n. That is, for all n ∈ N,
-Z O	、	-	, O 、
inf R(βn+1,λ) ≤ inf R(βn,λ)
λ≥0	λ≥0
where We define βn,o := limχ→o+ βn,λ = Xty.
(65)
19
Published as a conference paper at ICLR 2021
Figure 6: Train MSE vs. Num. Samples for Non-Isotropic Ridge Regression in d = 30 dimensions,
in the setting of Figure 2. Plotting train MSE: 1 ∣∣Xβ - ~||2.
Figure 7: Test Mean Squared Error vs. Num Train Samples for Random ReLU Features on Fashion-
MNIST, with D = 500 features.
In Appendix B.3 we present a technical conjecture in random matrix theory (Conjecture 2) which
suffices to prove Conjecture 1. Proving this Conjecture 2 presents a number of technical challenges,
but we have numerically verified it in a variety of cases. It can also be shown that Conjecture 2 is true
when Q = I , corresponding to isotropic covariates. We prove the reduction between Conjecture 2
and 1 in Appendix B.3.
B.3	Monotonicity Conjecture Proofs
In order to establish Conjecture 1, it is sufficient to prove the following technical conjecture.
20
Published as a conference paper at ICLR 2021
山 SN∙4-Js ①一 PeZPedXL
Figure 8: Test Mean Squared Error vs. Num Features for Random ReLU Features on Fashion-
MNIST, with n = 500 samples.
Conjecture 2. For all n ∈ N, d ≥ n, λ > 0, symmetric positive definite matrix Q ∈ Rd×d, the
following holds.
Define
Gλn := λ2E[(XTX + λQ)-2]
X
where X ∈ Rn×d is sampled with each entry i.i.d. N(0, 1). Similarly, define
Hn := EMXTX + λQ)-1 XT||F]
X
The expected test risk for n samples can be expressed as:
R(βn,λ) = (β * )T Gn β * + σ2 Hn + σ2	(66)
Then, we conjecture that the following two conditions hold.
1.
Gλn	Gλn+1	(67)
2.
(Gn - Gn+1) - (Hn- Hn+1) dGn∕dλ 占 0	(68)
λ
Lemma 6. Conjecture 2 implies Conjecture 1.
Proof. By the reduction in Section A.3, showing monotonicity for non-isotropic regression with
an isotropic regularizer is equivalent to showing monotonicity for isotropic regression with a non-
isotropic regularizer. Thus, we consider the latter. Specifically, Conjecture 1 is equivalent to showing
monotonicity for the estimator
βn,λ ：= argmin ∣∣Xβ 一 ~||2 + λ∣∣β∣∣∑-ι	(69)
β
= (XTX + λΣ-1)-1XTy	(70)
where X 〜N(0, I) is isotropic, and y 〜〈x, β*〉+ N(0, σ2).
21
Published as a conference paper at ICLR 2021
Now, letting Q := Σ-1, the expected test risk of this estimator for n samples is:
R(βn,λ)= E [∣∣βn,λ — β*∣∣2]+ σ2
X,y
=E [||(X T X + λQ)-1X T y - β*∣∣2]+ σ2
X,y
= E	[||(XTX + λQ)-1XT(Xβ* + η) - β*||2]+ σ2
X,η~N (0,σ2In)
=E[∣∣(X T X + λQ)-1X T Xβ* - β*)∣∣2] + σ2 E[∣∣(X T X + λQ)-1X T ||F ]+ σ2
XX
=E[∣∣(X T X + λQ)-1(X T X + λQ - λQ)β* - β *)∣∣2] + σ2 E[∣∣(X T X + λQ)-1X T ||F ]+ σ2
XX
=λ2 E[∣∣(X T X + λQ)-1Qβ* ||2]+ σ2 E[∣∣(X T X + λQ)-1X T ||F ]+ σ2
XX
=(β*)T Gλ β* + σ2Hλn + σ2
Consider the infimum
.---,ʌ 、
inf R(βn,λ)
λ≥0
(71)
We consider several cases below.
Case (1).	Suppose the infimum in Equation 71 is achieved in the limit λ → +∞. In this case,
monotonicity trivially holds, since
lιm R(βn,λ) = R(0) = lιm R(βn+ι,λ)
λ→∞	λ→∞
Case (2).	Suppose the infimum in Equation 71 is achieved by some λ = λonpt in the interior of the
set (0, ∞).
Because R(βn,λ) is continuous and differentiable in λ for all λ ∈ (0, ∞), We have that λnpt must
satisfy the following first-order optimality condition:
___ʃ ʌ 、
dR(βn,λ ) I =0
dλ lλ=λnt —
(72)
(73)
=⇒ (β*
β*+ σ2
=0
λ=λonpt
We Will later use this condition to shoW monotonicity.
Case (3).	Suppose the infimum in Equation 71 is achieved at λnpt = 0. Recall, we define βn,o :
limλ→0+ βn,λ . This means that,
__, ʌ , _ _ . _________________________
dR(βn,λ) I	=旧产经 β*+σ2 d∏n∣
dλ	Iλ=0	dλ σ dλ Iλ=0
≥0
(74)
dHn
Note that since -dλλ ≤ 0, both Equations (73) and (74) in Case (2) and Case (3) respectively imply
that
σ2 ≤-(β*)T(dGn)β* ∣
一	dHn∕dλ	∣λ=λopt
(75)
Now, assuming Conjecture 2, we will show that the choice of λonpt in Cases (2) and (3) has non-
increasing test risk for (n + 1) samples. That is,
__ʃ ʌ 、 _______ʃ ʌ 、
R(βn,λnt) ≥ R(βn+1,λnt)
This implies the desired monotonicity, since R(βn+1,λopt) ≥ R(βn+1,λopt ).
22
Published as a conference paper at ICLR 2021
We first consider the case when Hn - Hn+1卜=入著 ≥ 0. In this case, because Gn — G：+1 占 0 by
assumption, we have
R(βn^) - R(βn+χ) = (β *)T (Gn - G*ι)β*+σ"n -鸾+1)1=样	OS
≥ 0	(77)
Otherwise, assume. Hn - Hn+11λ=λopt ≤ 0. Then we have:
R(βn,λ nr) - R(βn+ι,λ nr) = (β *)T (Gn - Gn+ι)β*+σ2(Hn - Hn+i)∣λ=λ 警	W
≥ (β*)T(Gn - Gn+1)β* - (β*)τ(等)β* (HinHnHf) ∣ λ IOPt
0^∖	κ / / /α^∖	I λ—λ n
(by Equation (75), and Hn - Hn+1
(β *)T ((Gn - Gn+1) -(Hn-Hn+1) d§^) ∣	e*
∖	dH 》/dλ j I λ=λ n
'-------------------------------------'
≤ 0)
(79)
{^^^^^^^~
^0 by Conjecture 2
≥0
(80)
as desired.
□
23