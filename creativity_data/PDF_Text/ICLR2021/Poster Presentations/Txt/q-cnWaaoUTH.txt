Published as a conference paper at ICLR 2021
Conformation-Guided Molecular Representa-
tion with Hamiltonian Neural Networks
Ziyao Li1, ShuWen Yang2； Guojie Song2j Lingsheng Cai2
1 Center for Data Science, Peking University, Beijing, China
2Key Laboratory of Machine Perception and Intelligence (MOE), Peking University, Beijing, China
{leeeezy,swyang,gjsong,cailingsheng}@pku.edu.cn
Ab stract
Well-designed molecular representations (fingerprints) are vital to combine med-
ical chemistry and deep learning. Whereas incorporating 3D geometry of
molecules (i.e. conformations) in their representations seems beneficial, current
3D algorithms are still in infancy. In this paper, we propose a novel molecular
representation algorithm which preserves 3D conformations of molecules with a
Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and
momentums of atoms in a molecule interact in the Hamiltonian Engine following
the discretized Hamiltonian equations. These implicit coordinations are super-
vised with real conformations with translation- & rotation-invariant losses, and
further used as inputs to the Fingerprint Generator, a message-passing neural net-
work. Experiments show that the Hamiltonian Engine can well preserve molec-
ular conformations, and that the fingerprints generated by HamNet achieve state-
of-the-art performances on MoleculeNet, a standard molecular machine learning
benchmark.
1	Introduction
The past several years have seen a prevalence of the intersection between medical chemistry and
deep learning. Remarkable progress has been made in various applications on small molecules,
ranging from generation (Jin et al., 2018; You et al., 2018) and property prediction (Gilmer et al.,
2017; Cho & Choi, 2019; Klicpera et al., 2020) to protein-ligand interaction analysis (Lim et al.,
2019; Wang et al., 2020), yet all these tasks rely on well-designed numerical representations, or
fingerprints, of molecules. These fingerprints encode molecular structures and serve as the indicators
in downstream tasks. Early work of molecular fingerprints (Morgan, 1965; Rogers & Hahn, 2010)
started from encoding the two-dimensional (2D) structures of molecules, i.e. the chemical bonds
between atoms, often stored as atom-bond graphs. More recently, a trend of incorporating molecular
geometry into the representations arose (Axen et al., 2017; Cho & Choi, 2019).
Molecular geometry refers to the conformation (the three-dimensional (3D) coordinations of atoms)
of a molecule, which contains widely interested chemical information such as bond lengths and
angles, and thus stands vital for determining physical, chemical, and biomedical properties of the
molecule. Whereas incorporating 3D geometry of molecules seems indeed beneficial, 3D finger-
prints, especially in combination with deep learning, are still in infancy. The use of 3D fingerprints
is limited by pragmatic considerations including i) calculation costs, ii) translational & rotational
invariances, and iii) the availability of conformations, especially considering the generated ligand
candidates in drug discovery tasks. Furthermore, compared with current 3D algorithms, mature
2D fingerprints (Rogers & Hahn, 2010; Gilmer et al., 2017; Xiong et al., 2020) are generally more
popular with equivalent or even better performances in practice. For example, as a 2D approach,
Attentive Fingerprints (Attentive FP) (Xiong et al., 2020) have become the de facto state-of-the-art
approach.
To push the boundaries of leveraging 3D geometries in molecular fingerprints, we propose HamNet
(Molecular Hamiltonian NetWorks). HamNet simulates the process of molecular dynamics (MD)
* Equal Contribution.
,Corresponding Author.
1
Published as a conference paper at ICLR 2021
to model the conformations of small molecules, based on which final fingerprints are calculated
similarly to (Xiong et al., 2020). To address the potential lack of labeled conformations, HamNet
does not regard molecular conformations as all-time available inputs. Instead, A Hamiltonian engine
is designed to reconstruct known conformations and generalize for unknown ones. Encoded from
atom features, implicit positions and momentums of atoms interact in the engine following the dis-
cretized Hamiltonian Equations with learnable energy and dissipation functions. Final positions are
supervised with real conformations, and further used as inputs to a Message-Passing Neural Network
(MPNN) (Gilmer et al., 2017) to generate the fingerprints. Novel loss functions with translational &
rotational invariances are proposed to supervise the Hamiltonian Engine, and the architecture of the
Fingerprint Generator is elaborated to better incorporate the output quantities from the engine.
We show via our conformation-reconstructing experiments that the proposed Hamiltonian Engine
is eligible to better predict molecular conformations than conventional geometric approaches as
well as common neural structures (MPNNs). We also evaluate HamNet on several datasets with
different targets collected in a standard molecular machine learning benchmark, MoleculeNet (Wu
et al., 2017), all following the same experimental setups. HamNet demonstrates state-of-the-art
performances, outperforming baselines including both 2D and 3D approaches.
2	Preliminaries
Notations. Given a molecule with n atoms, we use vi to denote the features of atom i, and eij
that of the chemical bond between i and j (if exists). Bold, upper-case letters denote matrices, and
lower-case, vectors. All vectors in this paper are column vectors, and ∙> stands for the transpose
operation. We use ㊉ for the concatenation operation of vectors. The positions and momentums of
atom i are denoted as qi and pi, and the set of all positions of atoms in a molecule is denoted as
Q = (qi,…，qn)>. N(V) refers to the neighborhood of node V in some graph.
Graph Convolutional Networks (GCNs). Given an attributed graph G = (V, A, X), where V =
{vι, ∙∙∙ , Vn} is the set of vertices, A ∈ Rn×n the (weighted) adjacency matrix, and X ∈ Rn×d the
attribute matrix, GCNs (Kipf & Welling, 2017) calculate the hidden states of graph nodes as
GCN(L)(X) ≡ H(L), H(l+1) = σ (AH(I)W(l)) , H⑼=X, l = 0,1,…，L - 1. (1)
Here, H = (hvι,…,hvn )> are hidden representations of nodes, A = D- 1 AD-2 is the nor-
malized adjacency matrix, D with Dii = Pj Aij is the diagonal matrix of node degrees, and Ws
are network parameters.
Message-Passing Neural Networks (MPNNs). MPNN (Gilmer et al., 2017) introduced a general
framework of Graph Neural Networks (GNNs). In the t-th layer ofa typical MPNN, messages (mt)
are generated between two connected nodes (i, j) based on the hidden representations of both nodes
(ht) and the edge in-between. After that, nodes receive the messages and update their own hidden
representations. A readout function is then defined over final node representations (hT) to derive
graph-level representations. Denoted in formula, the calculation follows
mV+1 =	E Mt(htυ, hW, ev,w),	ht+1 = Ut(htv, mt+1),	y = R({hT|v ∈ V}),	⑵
w∈N (v)
where Mt , Ut , R are the message, update and readout functions.
Hamiltonian Equations. The Hamiltonian Equations depict Newton’s laws of motion in the form of
first-order PDEs. Considering a system of n particles with positions (qι,…，qn) and momentums
(pi, •…，pn), the dynamics of the system follow
dqi	∂H	dpi	∂H
π = --- = --- n：=-------=-------
dt	∂pi	dt	∂qi
(3)
where H is the Hamiltonian of the system, and equals to the total system energy. Generally, the
Hamiltonian is composed of the kinetic energy of all particles and the potential energy as
n
H =	Ti + U .
(4)
2
Published as a conference paper at ICLR 2021
əu-auw
U.2U0 三 UlPH
Potential Step-size	Increments I	(T Iterations)
ɛɪh f4 ʌ^o I*c+y4 √dι l*∙ ∙∙∙ →ΓrTη
」Oməuəo
Ju七 djəbn,s工
Hamiltonian Molecular Dynamics
Layers with Ieamable parameters Tnputs/Labels ∣ ] Interaction between two modules [ ] Hidden Representations
Figure 1: The overall structure of HamNet.
Meanwhile, if dissipation exists in the system, the Hamiltonian Equations shall be adapted as
∂H
qi = -, Pi =-
∂pi
+ mi
(5)
where mi is the mass of the particle and Φ is the dissipation function which describes how the
system energy is dissipated by the outer environment.
3	Method: HamNet
Figure 1 shows the overall architecture of HamNet. HamNet consists of two modules: i) a Hamil-
tonian Engine, where molecular conformations are reconstructed; and ii) a Fingerprint Generator,
where final fingerprints are generated from atom & bond features and outputs from the Hamiltonian
Engine.
3.1	Hamiltonian Engine
Discretized Hamiltonian Equations. The Hamiltonian Engine is designed to simulate the physical
interactions between atoms in a molecule. To correctly incorporate the laws of motion, we discretize
the Hamiltonian Equations with dissipation (Equation 5) and model the energy and dissipation with
learnable functions. At the t-th step (t = 0,1, ∙ ∙ ∙ , T) in the engine, for atom i,
(t+1)	(t)	d H ⑴
qi	= qi+ O,
(t+1)
pi
(t)
=pi
-η
∂ H(t)
d q(t)
+ mi
∂Φ(t)
(6)
Here, η is a hyperparameter of step size which controls the granularity of the discretization, mi is the
(normalized) mass of atom i, and H(t), Φ(t) are the learnable Hamiltonian and dissipation functions
of q(t) and p(t) : (superscripts skipped)
n
H = X Ti (Pi) + U (qi ,…，qn),
i=1
n
Φ = X φ(Pi).	(7)
i=1
It should be noted that in order to improve the expressional power of the network, we extend the
concept of positions and momentums into implicit ones in a generalized df -dimensional space, i.e.
q, P ∈ Rdf , df > 3. We will discuss how to supervise these implicit quantities, and will show the in-
fluences of the dimensionality in the experimental results. Nonetheless, we parameterize the energy
3
Published as a conference paper at ICLR 2021
and dissipation function in a physically inspired manner: for the atom-wise kinetic energy, we gen-
eralize the definition ofkinetic energy (T =薨)as the quadratic forms of the implicit momentums;
for the atom-wise dissipation, We adapt the Rayleigh's dissipation function (Φ = 1 Pnj=I Cijqiqj)
into the generalized space. Denoted in formula, we have
Ti(Pi) = PiWWTPi,	Φi(Pi) =片鲁.	(8)
2mi	2mi2
For the potential energy, we simplify the Lennard-Jones potential (U (r) =	r-12 - r-6 ) with
parameterized distances rs in the generalized space, that is,
U =	uij,	uij	=	ri-j4	- ri-j2,	ri2j	≡	r(qi,	qj)	=	(qi	- qj)>WU>WU (qi	-	qj).	(9)
i6=j
In both Equation 8 and Equation 9, Ws are network parameters. For the mass of the atoms, we
empirically normalize the relative atomic mass Mi of atom i with mi = Mi /50. One would also
note that the parameters in each step of the engine remain the same, and thus the scale of parameters
in the engine depends only on df and is irrelevant to the depth T .
Initial positions and momentums. Graph-based neural networks are used to initialize these spatial
quantities. Molecules are essentially graphs, while bonds between atoms can be of different types
(single, double, triple and aromatic). Instead of using channel-wise GCNs which assign different
parameters for different types of edges (Schlichtkrull et al., 2018), we calculate a bond-strength
adjacency for the molecules: the strength ofa bond depends on the atom-bond-atom tuple, i.e.
Aij = sigmoid (MLP ((Vi ㊉ ej ㊉ Vj))) if the bond exists, Aij = 0 otherwise.	(10)
With so-defined adjacency, we first encode the atoms with vanilla GCNs (Equation 1) and concate-
nate all hidden layers following the DenseNet scheme (Huang et al., 2017). Deep enough GCNs
do capture entire molecular structures, however, atoms with identical chemical environment cannot
be distinguished, for example, carbon atoms in the benzene ring. This may not be a problem in
conventional MPNNs, but atoms with coinciding positions are inacceptable in physics as well as the
Hamiltonian engine. Therefore, we conduct an LSTM over the GCN outputs to generate unique
positions for atoms. The order that the atoms appear in the LSTM conforms with the SMILES rep-
resentations (Weininger, 1988) of the molecule, which display atoms in the molecule in a specific
topological order. Denoted in formula, initial positions and momentums of atoms are calculated as
L
⅛ = M F q(0) = LSTMsi @1 &2,…，K)	(ii)
l=0
L
Pi = M gi(l,	P(O)= LSTMsi (Psi, Ps2 ,…，Psn)	(12)
l=0
where fi(l), gi(l) are hidden representations of atom i in the l-th GCN layer and sk is the atom at
k-th position in the SMILES. As unique orders are assigned for atoms, their initial positions are thus
unique.
Conformation preserving. After the dynamical process in the Hamiltonian Engine, positions in the
generalized Rdf space are transformed into real 3D space linearly, that is,
Q = QWtrans,	Q ∈ Rn×3, Q = (%,…，Qn) ∈ Rn×df ∙	(13)
Considering translational & rotational invariances, we do not require that Q approximates the la-
beled 3D coordinations of atoms. Instead, three translational- and rotational-invariant metrics are
proposed and used to supervise Q:
I)	Kabsch-RMSD (K-RMSD). We use the Kabsch Algorithm (Kabsch, 1976) to rotate and align the
approximated atom positions (Q) to the real ones (Qr), and then calculate the Root ofMean Squared
Deviations (RMSD) of two conformations using atom mass as weights:
QK = KabSch(Q; Qr), Lk-rmsd(Q, QR) = t
Pn=I mi × IIqK - qr俏
v-``n	∙	(14)
i=1 mi
4
Published as a conference paper at ICLR 2021
One should note that the alignment Kabsch(∙; ∙) is calculated with SVD and is thus differentiable.
II)	Distance Loss. Pair-wise distances between atoms are vital quantities in describing molecular
conformations and enjoy desired invariances. Therefore, we propose the metric Ldist as
n2
LLlistqQR) = n X (gi-qjk2- W - qRII2)	(15)
i,j=1
III)	ADJ-k Loss. A drawback of the naive distance loss is that distances between far atoms are
over-emphasized, leading to deformed local structures. Therefore, we further propose ADJ-k loss,
where only distances between k-hop connected atoms are preserved under weights calculated from
hop-distances. Denote the normalized, simple adjacency matrix as A,1 the ADJ-k loss is defined as
n2
L2adj-k(Q,qR) = n X A^kj (kqi-qjk2 - ||qR -qR∣l2)	(16)
i,j=1
In implementation, we use a linear combination of K-RMSD and ADJ-3 losses to supervise the
engine, i.e. LHE = Lk-rmsd + λLadj-3, where λ is a hyperparameter.
3.2	Fingerprint Generator
After the dynamical process in the Hamiltonian Engine, the molecular fingerprints are generated
with the outputs as well as atom & bond features. The architecture of the Fingerprint Generator can
be seen as an MPNN (Gilmer et al., 2017) instance. Analogous to that in Attentive FP (Xiong et al.,
2020), messages are generated with Graph Attention Layers (GAT) (Velickovic et al., 2018), and
hidden representations of nodes are updated with Gated Recurrent Units (GRU) (Cho et al., 2014).
Nonetheless, the architecture of HamNet is further adapted as conformation-aware: we modify the
calculation of messages and attentive energies to incorporate relative positions and momentums.
Denoted in formula, the atom-level calculation in the Fingerprint Generator follows
h0 = MLP(vi),	fij= MLP(ej),	rj	=	(qi	㊉ Pi)	-	(qj	㊉ Pj)；	(17)
ej = (wl)>(fij ㊉ rij),	Oij = Softmax({ejj ∈N(i)}),	(18)
mi+1=	E	αi+ 1WM	[hi	㊉ rij ㊉	hj]	,	h『=GRU(hi,	mi+1),	l = 0,1,…，L. (19)
j∈N(i)
Atom representations in the last layer (hiLs) then serve as inputs to a global attentive readout func-
tion. A virtual meta node (g) is established and connected to all atoms in order to conduct M layers
of attentive pooling. Similar to that in atom-level calculation, the positions and momentums of atoms
are incorporated in the calculation:
hg = n X hL；	ηim =	[hg ㊉	qi	㊉ Pi	㊉ hL]	,	βm	= SOftmax({ηm∣i	∈ V}),	(20)
i
Sm = X βmwsm [qi ㊉ Pi ㊉ hL],	hm+1 = GRu(hm, sm),	m=0,1,…，m.	(2i)
i
Here, sg is the global message, and V is the set of all atoms. The final output, hgM, is the desired
fingerprints of the target molecules. The same as current neural molecular fingerprints (Duvenaud
et al., 2015; Xiong et al., 2020), the generated fingerprints are then supervised with molecular prop-
erties, such as regression and classification tasks.
1The simple adjacency matrix refers to the indicator matrix of bond existences, regardless of bond types;
the normalization is conducted the same as A in GCNS (See Section 2).
5
Published as a conference paper at ICLR 2021
3.3	Discussion
From a physical perspective, the Hamiltonian Engine is essentially inspired by and highly related
to Molecular Dynamics (MD). The potential energy function in the engine can be regarded as a
generalized while simplified molecular force field. Force fields are widely used tools in molecular
simulation, where potential energies are modeled as a family of functions of conformations, whose
parameters are calculated with quantum chemistry or determined by experiments. After a force
field is established, conformations are optimized by minimizing the potential energy. Similarly, the
dissipation we introduced in the Hamiltonian Engine serves as an implicit optimization of potential
energy, as the system energy is continuously dissipated through Φ during the dynamical process. As
a result, after adequate steps, the molecular conformations always converge to a local minimum of
the potential energy, and the momentums of atoms converge to 0. From a deep learning perspective,
the Hamiltonian Engine can be seen as a pair of dual, residual MPNNs operating on fully-connected
graphs: at each step, messages calculated by q influence p and vice versa. Under this view, the most
essential difference of introducing physical laws is that the messages passed from q topis symmetric
between any two given atoms, following the Newton’s third law and conforming a conservative
field (the potential force field). Speaking more detailly, q-messages sent between a pair of atoms
(i.e. the forces, ∂H∕∂q) are implicitly guaranteed as symmetric, yet the actual P-update (i.e. the
accelerations, P∕m) may not be: they are also related to properties of the receiver (the atom mass)
and the outer environment (the dissipation).
4	Results
4.1	Experimental Setup
Datasets. Five molecular datasets are used to evaluate HamNet, including a Quantum Mechan-
ics dataset (QM9) and four biomedical datasets, namely Tox21, Lipop, FreeSolv, and ESOL.
QM9 (Ramakrishnan et al., 2014) contains calculated conformations and 12 quantitative quantum-
chemical properties of 133k molecules; Tox21 contains 12 binary toxicological indices of 7, 831
molecules; Lipop contains quantitative liposolubility lipophilicity of4, 200 molecules; FreeSolv
contains quantitative hydration free energy of 642 molecules, and ESOL contains quantitative sol-
ubility of 1, 128 molecules. All datasets are referred in MoleculeNet, and the same metrics 2, data
split ratios 3, and multi-task scheme (for QM9 and Tox21) 4 are used in our paper.
Featurization and Implementation. We use the identical featurization as Attentive FP (Xiong
et al., 2020). In total, 39-dimensional atom features (including atom types, atom degree, indicators of
aromaticity and chirality et al) and 10-dimensional bond features (including bond types, indicator of
conjugation et al) are derived from the molecules. One could refer to the Appendix for more details
of featurization. As a default setup, we use a 20-step (T = 20) Hamiltonian Engine with df = 32,
and L = 2, M = 2 with 200-dimensional hidden representations (dim(hi) = dim(hg) = 200) in
the Fingerprint Generator. For the training of HamNet, we first train the Hamiltonian Engine with
known conformations,5 and use the output to train the Fingerprint Generator, with mean-squared-
error losses for regression tasks with RMSE metric, mean-absolute-error losses for those with MAE
metric, and cross-entropy losses for classification tasks. Other implementation details, including the
choices of hyperparameters on different datasets and the training setup are available in the Appendix.
4.2	Conformation Prediction
We evaluate the ability of the Hamiltonian Engine in predicting molecular conformations on QM9,
where known conformations of molecules are available, and reported the Kabsch-RMSD Lk-rmsd
and distance loss Ldist losses. Two baselines are compared against the Hamiltonian Engine (Ham.
Eng.): i) an MPNN with the exact architecture proposed in (Gilmer et al., 2017), supervised in the
2We use MAE for QM9, ROC for Tox21, and RMSE for Lipop, FreeSolv & ESOL.
3Data are randomly split to 8 : 1 : 1 as training, validation and test sets.
4Models are trained to simultaneously preserve all targets after standard normalization, and averaged per-
formances are reported.
5On datasets without known conformations, we generate labeled conformations with RDKit to train the
engine
6
Published as a conference paper at ICLR 2021
Table 1: Quantitative results of conformation prediction on QM9.
Metric	I KabSCh-RMSD(A)	Distance Loss (10-2 A)
MPNN	1.708	8.620
RDKit	1.649	7.519
Ham. Eng. (w/o LSTM)	2.039	10.871
Ham. Eng. (w/o dyn.)	1.442	5.519
Ham. Eng. (w/o Φ)	1.389	5.227
Ham. Eng. (w/o ADJ-3)	1.084	7.746
Ham. Eng. (as proposed)	1.384	5.186
Real Coni. ∣ Step 0 Step 3 Step 6 Step 9 Step 12 Step 15 Step 20
Figure 2: Visualized conformations at different steps of the Hamiltonian Engine.
same way as we have introduced in Section 3.1; ii) a Distance Geometry (Blaney & Dixon, 2007)
method tuned with the Universal Force Field (UFF), implemented in the RDKit package (referred as
RDKit) 6. We also conduct an ablation analysis of the Hamiltonian Engine by testing: i) an engine
with the LSTM removed (w/o LSTM); ii) an engine with no Hamiltonian dynamics, i.e. T = 0 (w/o
dyn.); iii) an engine without the dissipation function (w/o Φ); and iv) an engine trained without the
ADJ-3 loss (w/o ADJ-3).
Table 1 shows the two losses on the test sets. Our approach outperforms the Distance Geometry
baseline (RDKit) by 16%, while the MPNN cannot. In the ablation analysis, effectiveness of differ-
ent components is also varified: improvements on both metrics are observed when LSTM, molecular
dynamics, and dissipation function exist. Although training the Hamiltonian Engine simply with
Kabsch-RMSD leads to better performances on the very metric, distance losses of these models are
unacceptably large. This indicates although atoms tend to appear closer to their labeled locations,
the relative structures inside the molecules are compromised, which is a particularly undesired result
in molecular science. One could refer to Figure 2 for a more intuitive understanding of the dynam-
ics in the Hamiltonian Engine: atoms in the initial conformations (Q(0)) tend to gather around the
molecular centers, and the repulsion forces derived from the potential energy stretch the molecules
into the correct conformations (QR). As dissipation exists in the system, conformations converge to
the real ones after adequate steps.
4.3	Molecular Property Prediction
We compare HamNet with five baselines on the molecular property prediction tasks. i) Molecu-
leNet (Wu et al., 2017) tested a collection of molecular representation approaches at the time, by
which we present the best performances achieved. ii) 3DGCN (Cho & Choi, 2019) augmented
conventional GCN-based methods with input bond directions. iii) DimeNet (Klicpera et al., 2020)
proposed directional message passing where messages instead of atoms are embedded. iv) Atten-
tive FP (Xiong et al., 2020) proposed an MPNN instance where local and global attentive layers
are used. v) CMPNN (Song et al., 2020) strengthened the message interactions between nodes and
edges through a communicative kernel. Two HamNet variants are also tested: i) a HamNet without
known conformation, where all q , p-related components in the Fingerprint Generator are removed;
6We use the 2020.03.1.0 version of the RDKit package. See http://www.rdkit.org/
7
Published as a conference paper at ICLR 2021
Table 2: Quantitative results on various datasets of baselines, HamNet, and its variants. Baselines
using 3D conformations of test molecules are marked italic. For different metrics, “↑" indicates
that the higher is better, "]" Contrarily. We directly take reported performances from corresponding
references, and leave unreported entries blank ("一").
DATASET Metric	QM9 MWti-MAEl	Tox21 Multi-ROC↑	Lipop RMSEl	FreeSolv RMSEl	ESOL RMSEl
MoleculeNet (2017)	2.350	0.829	0.655	1.150	0.580
3DGCN (2019)	—	—	—	0.824±0.014	0.558±0.069
DimeNet (2020)	1.920	—	—	一	—
Attentive FP (2020)	1.292	0.857	0.578	0.736	0.505
CMPNN (2020)	—	0.856± 0.006	—	0.808±0.129	0.547±0.011
HamNet (w/o conf.)	1.237±0.030	0.868±0.012	0.572±0.011	0.840±0.023	0.547±0.015
HamNet (real conf.)	1.199±0.017	0.864±0.006	0.566±0.015	0.811±0.048	0.584±0.012
HamNet (ours)	1.194±0.038	0.875± 0.006	0.557±0.014	0.731±0.024	0.504±0.016
(a) Engine depth (T).	(b) Dimensionality of q,p (df).	(c) Step size (η).
Figure 3: Effects on conformation prediction of hyperparameters in the Hamiltonian Engine. Dis-
tance losses and running time versus (a) the engine depth T ; (b) the dimensionality of the general-
ized space df; and (c) the step size η of discretization are plotted.
ii) a HamNet with real conformations, where the Hamiltonian Engine is removed and all q , ps are
transformed from real coordinations, with MLPs, to the corresponding dimensionality. Five replicas
of HamNet models are trained with means and standard deviations reported.
Table 2 shows the quantitative results of the fingerprints. As HamNet and all chosen baselines follow
the same evaluation scheme proposed in MoleculeNet, we directly present the reported performances
and leave unreported ones blank. We also italicize baselines which leverage true 3D conformations
of the test set. HamNet is able to significantly outperform all baselines on all datasets. Moreover,
compared with the HamNet variant using real conformations, HamNet with the Hamiltonian En-
gine still performs better. We believe the reason is that as the Hamiltonian Engine is trained with
translation- & rotation-invariant losses, the generalized space enjoys more robustness compared with
real coordinations of atoms used directly.
4.4 Parameter Analysis
We conduct further analysis of the effects of several important hyperparameters on the conformation
prediction performances in the Hamiltonian Engine, including the engine depth (L), the dimension-
ality of the generalized space (df), and the step size (η). Figure 3 demonstrate the distance loss and
/ or the running time. i) As the depth increases, the running time increases linearly, and the test
loss gradually decreases until convergence. Empirically, 25-30 steps suffice. ii) The running time of
the engine is hardly influenced by the dimensionality df, while the performance enjoys a significant
improvement by increasing the dimensionality when df ≤ 32. iii) An appropriate choice of the step
size is crucial. For example, with the fixed engine depth T = 20, an ideal choice of the step size
would be η ∈ [0.025, 0.050].
8
Published as a conference paper at ICLR 2021
5 Discussion & Future Work
In this paper, we proposed a novel molecular representation approach, Molecular Hamiltonian Net-
work (HamNet). Instead of directly using conformations as inputs, HamNet learns to predict real
conformations with a physically inspired module, the Hamiltonian Engine. Novel loss functions
with translational & rotational invariances are proposed to train the engine. Final representations
of molecules are generated with a Fingerprint Generator, whose architecture is based on MPNNs
and considerately modified to incorporate generated implicit conformations. We discussed the rela-
tionships between physics and deep learning inside the Hamiltonian Engine from both perspectives,
and we believe that the physics-based model enjoys better interpretability than general MPNNs. We
further demonstrated with our experiments that the proposed Hamiltonian Engine is eligible to learn
molecular conformations, and that HamNet achieves state-of-the-art performances on molecular
property prediction tasks in a standard benchmark (MoleculeNet).
It should be noted that a recent trend in incorporating machine learning, especially deep learn-
ing, into modeling molecular potentials emerged (Chmiela et al., 2017; 2018; Zhang et al., 2018).
Another related field of HamNet is neural physics engines (Sanchez-Gonzalez et al., 2018; 2019;
Greydanus et al., 2019), which learn to conduct simulations that conform to physical laws. The de-
sign of the Hamiltonian Engine in our paper is highly motivated by these works, while the ultimate
goal of HamNet is to derive well-designed molecular representations, instead of to accurately model
the molecular dynamics. Also, instead of using the structural optimization to derive stablized con-
formations after a force-field (potentials in HamNet) is established, HamNet uses the Hamiltonian
Engine to make the whole process differentiable.
For future work, a promising aspect would be to elaborate the learnable potential, kinetics and dis-
sipation functions used in the Hamiltonian Engine. Work in using HamNet in more straight-forward
applications would also be useful, such as virtual screening, protein-ligand binding prediction, etc.
In addition, an interesting attempt in further modifying HamNet would be to change the current
discretization approach of the Hamiltonian Equations to Neural ODEs (Chen et al., 2018; Sanchez-
Gonzalez et al., 2019), which may yield a finer-grained simulation of the molecular dynamics.
Acknowledgments
This work was supported by the National Natural Science Foundation of China (Grant No.
61876006). We would also like to thank Dr. Chenbo Wang for his help in the physics theories
of this paper.
References
S. D. Axen, X. P. Huang, E. L. Cceres, L. Gendelev, B. L. Roth, and M. J. Keiser. A simple
representation of three-dimensional molecular structure. Journal of medicinal chemistry, 60(17):
7393-7409, 2017.
Jeffrey M. Blaney and J. Scott Dixon. Distance Geometry in Molecular Modeling, pp. 299-335.
John Wiley & Sons, Ltd, 2007.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018 (NeurIPS 2018), pp. 6572-6583, 2018.
Stefan Chmiela, Alexandre Tkatchenko, HUziel E. Sauceda, Igor Poltavsky, Kristof T. Schutt, and
Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields.
Science advances, 3(5), 2017.
Stefan Chmiela, Huziel E. Sauceda, Klaus-Robert Mller, and Alexandre Tkatchenko. Towards exact
molecular dynamics simulations with machine-learned force fields. Nature communications, 9
(1), 2018.
H. Cho and I. S. Choi. Enhanced deep-learning prediction of molecular properties via augmentation
of bond topology. ChemMedChem, 14(17):1604-1609, 2019.
9
Published as a conference paper at ICLR 2021
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2014) ,pp.1724-1734, 2014.
David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Timo-
thy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learn-
ing molecular fingerprints. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015 (NeurIPS 2015), pp. 2224-2232,
2015.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning (ICML 2017), pp. 1263-1272, 2017.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019 (NeurIPS 2019), pp. 15353-15363, 2019.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In Proceedings of the 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR 2017), pp. 2261-2269, 2017.
Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In Proceedings of the 35th International Conference on Machine
Learning, (ICML 2018), pp. 2328-2337, 2018.
Wolfgang Kabsch. A solution of the best rotation to relate two sets of vectors. Acta Crystallograph-
ica A, 32:922-923, 1976.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the 5th International Conference on Learning Representations (ICLR
2017), 2017.
Johannes Klicpera, Janek Groβ, and Stephan Gunnemann. Directional message passing for molec-
ular graphs. In 8th International Conference on Learning Representations (ICLR 2020), 2020.
J. Lim, S. Ryu, K. Park, Y. J. Choe, J. Ham, and W. Y. Kim. Predicting drug-target interaction
using a novel graph neural network with 3d structure-embedded graph representation. Journal of
chemical information and modeling, 59(9):3981-3988, 2019.
H. L. Morgan. The generation of a unique machine description for chemical structures-a technique
developed at chemical abstracts service. Journal of chemical documentation, 5(2):63-112, 1965.
Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. Quan-
tum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1(140022), 2014.
D. Rogers and M. Hahn. Extended-connectivity fingerprints. Journal of chemical information and
modeling, 50(5):742-754, 2010.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin A. Ried-
miller, Raia Hadsell, and Peter W. Battaglia. Graph networks as learnable physics engines for
inference and control. In Proceedings of the 35th International Conference on Machine Learning
(ICML 2018), pp. 4467-4476, 2018.
Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter W. Battaglia. Hamiltonian graph
networks with ODE integrators. CoRR, abs/1909.12790, 2019.
Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convolutional networks. In Proceedings of the
15th Extended Semantic Web Conference (ESWC 2018), pp. 593-607, 2018.
10
Published as a conference paper at ICLR 2021
Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang.
Communicative representation learning on attributed molecular graphs. In Proceedings of the
Twenty-Ninth International Joint Conference on Artificial Intelligence, (IJCAI2020), pp. 2831-
2838, 2020.
Petar Velickovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In Proceedings of the 6th International Conference on Learn-
ing Representations (ICLR 2018), 2018.
Y. Wang, J. HU, J. Lai, Y. Li, H. Jin, L. Zhang, L. R. Zhang, and Z. M. LiU. Tf3p: Three-dimensional
force fields fingerprint learned by deep capsUlar network. Journal of chemical information and
modeling, 60(6):2754-2765, 2020.
D. Weininger. Smiles 1. introdUction and encoding rUles. Journal of chemical information and
computer sciences, 28(31), 1988.
Z. WU, B. RamsUndar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. PappU, K. Leswing, and
V. Pande. MolecUlenet: a benchmark for molecUlar machine learning. Chemical science, 9
(2):513-530, 2017.
Z. Xiong, D. Wang, X. LiU, F. Zhong, X. Wan, X. Li, Z. Li, X. LUo, K. Chen, H. Jiang, and M. Zheng.
PUshing the boUndaries of molecUlar representation for drUg discovery with the graph attention
mechanism. Journal of medicinal chemistry, 63(16):8749-8760, 2020.
JiaxUan YoU, Bowen LiU, Zhitao Ying, Vijay S. Pande, and JUre Leskovec. Graph convolUtional
policy network for goal-directed molecUlar graph generation. In Advances in Neural Informa-
tion Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018
(NeurIPS 2018), pp. 6412-6422, 2018.
Linfeng Zhang, JieqUn Han, Han Wang, Roberto Car, and E. Weinan. Deep potential molecUlar
dynamics: A scalable model with the accUracy of qUantUm mechanics. Physical review letters,
120(14), 2018.
11