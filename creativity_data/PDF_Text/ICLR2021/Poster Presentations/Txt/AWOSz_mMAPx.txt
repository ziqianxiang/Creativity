Published as a conference paper at ICLR 2021
Local Convergence Analysis of Gradient De-
scent Ascent with Finite Timescale Separation
Tanner Fiez & Lillian J. Ratliff
Department of Electrical and Computer Engineering
University Washington
Seattle, WA 98195
{fiezt, ratliffl}@uw.edu
Ab stract
We study the role that a finite timescale separation parameter τ has on gradient
descent-ascent in non-convex, non-concave zero-sum games where the learning
rate of player 1 is denoted by γ1 and the learning rate of player 2 is defined to be
γ2 = τγ1. We provide a non-asymptotic construction of the finite timescale sepa-
ration parameter T* such that gradient descent-ascent locally converges to x* for
all τ ∈ (T*, ∞) if and only if it is a strict local minmax equilibrium. Moreover,
we provide explicit local convergence rates given the finite timescale separation.
The convergence results we present are complemented by a non-convergence re-
sult: given a critical point x* that is not a strict local minmax equilibrium, we
present a non-asymptotic construction ofa finite timescale separation T0 such that
gradient descent-ascent with timescale separation T ∈ (T0, ∞) does not converge
to x* . Finally, we extend the results to gradient penalty regularization methods for
generative adversarial networks and empirically demonstrate on CIFAR-10 and
CelebA the significant impact timescale separation has on training performance.
1	Introduction
In this paper we study learning in zero-sum games of the form
min max f(x1, x2)
x1∈X1 x2∈X2
where the objective function of the game f is assumed to be sufficiently smooth and potentially non-
convex and non-concave in the strategy spaces X1 and X2 respectively with each Xi a precompact
subset of Rni. This general problem formulation has long been fundamental in game theory (BaSar
& Olsder, 1998) and recently it has become central to machine learning with applications in genera-
tive adversarial networks (Goodfellow et al., 2014), robust supervised learning (Madry et al., 2018;
Sinha et al., 2018), reinforcement and multi-agent reinforcement learning (Rajeswaran et al., 2020;
Zhang et al., 2019), imitation learning (Ho & Ermon, 2016), constrained optimization (Cherukuri
et al., 2017), and hyperparameter optimization (Lorraine et al., 2020; MacKay et al., 2019).
The gradient descent-ascent learning dynamics are widely studied as a potential method for effi-
ciently computing equilibria in game formulations. However, in zero-sum games, a number of past
works highlight problems with this learning dynamic including both non-convergence to meaning-
ful critical points as well as convergence to critical points devoid of game theoretic meaning, where
common notions of ‘meaningful’ equilibria include the local Nash and local minmax (Stackelberg)
concepts. For instance, in bilinear games, gradient descent-ascent avoids local Nash and Stackel-
berg equilibria due to the inherent instability of the update rule for this class. Fortunately, in this
class of games, regularization or gradient-based learning dynamics that employ different numerical
discretization schemes (as compared to forward Euler for gradient descent-ascent) are known to al-
leviate this issue (Daskalakis et al., 2018; Mertikopoulos et al., 2019; Zhang & Yu, 2020). For the
more general nonlinear nonconvex-nonconcave class of games, it has been shown gradient descent-
ascent with a shared learning rate is prone to reaching critical points that are neither local Nash
equilibria nor local Stackelberg equilibria (Daskalakis & Panageas, 2018; Jin et al., 2020; Mazum-
dar et al., 2020). While an important negative result, it does not rule out the prospect that gradient
1
Published as a conference paper at ICLR 2021
descent-ascent may be able to guarantee equilibrium convergence as it fails to account for a key
structural parameter of the dynamics, namely the ratio of learning rates between the players.
Motivated by the observation that the order of play between players is fundamental to the definition
of the game, the role of timescale separation in gradient descent-ascent has recently been explored
theoretically (Chasnov et al., 2019; Heusel et al., 2017; Jin et al., 2020). On the empirical side, it
has been widely demonstrated that timescale separation in gradient descent-ascent is crucial to im-
proving the solution quality when training generative adversarial networks (Arjovsky et al., 2017;
Goodfellow et al., 2014; Heusel et al., 2017). Denoting γ1 as the learning rate of the player 1, the
learning rate of player 2 can be redefined as γ2 = τγ1 where τ = γ2 /γ1 > 0 is the learning rate
ratio. Toward understanding the effect of timescale separation, Jin et al. (2020) show the locally sta-
ble critical points of gradient descent-ascent coincide with the set of strict local minmax/Stackelberg
equilibrium across the spectrum of sufficiently smooth zero-sum games as τ → ∞. In other words,
all ‘bad critical points’ (critical points lacking game-theoretic meaning) become unstable and all
‘good critical points’ (game-theoretically meaningful equilibria) remain or become locally expo-
nentially stable (cf. Definition 3) as τ → ∞. While a promising theoretical development, gradient
descent-ascent with a timescale separation approaching infinity does not lead to a practical learning
rule and the analysis of it does not necessarily provide insights into the common usage of a reason-
able finite timescale separation. An important observation is that choosing τ arbitrarily large with
the goal of ensuring local equilibrium convergence can lead to numerically ill-conditioned prob-
lems. This highlights the significance of understanding the exact range of learning rate ratios that
guarantee local stability. Moreover, our experiments in Section 5 (Dirac-GAN) and in Appendix K
show that modest values of τ are typically sufficient to guarantee stability of only equilibria which
allows for larger choices of γ1 and results in faster convergence to an equilibrium.
Contributions. We show gradient descent-ascent locally converges to a critical point for a range
of finite learning rate ratios if and only if the critical point is a strict local Stackelberg equilibria
(Theorem 1).1 This result is constructive in the sense that we explicitly characterize the exact range
of learning rate ratios for which the guarantee holds. Furthermore, we show all other critical points
are unstable for a range of finite learning rate ratios that we explicitly construct (Theorem 2). To
our knowledge, the aforementioned guarantees are the first of their kind in nonconvex-nonconcave
zero-sum games for an implementable first-order method. Moreover, the technical results in this
work rely on tools that have not appeared in the machine learning and optimization communities
analyzing games. Finally, we extend these results to gradient penalty regularization methods in gen-
erative adversarial networks (Theorem 3), thereby providing theoretical guarantees for a common
combination of heuristics used in practice, and empirically demonstrate the benefits and trade-offs
of regularization and timescale separation on the Dirac-GAN along with image datasets.
2	Preliminaries
A two-player zero-sum continuous game is defined by a collection of costs (fι, f2) where fι ≡ f
and f2 ≡ -f with f ∈ Cr (X, R) for some r ≥ 2 and where X = X1 × X2 with each Xi a
precompact subset of Rni for i ∈ {1, 2} and n = n1 + n2 . Each player i ∈ I seeks to minimize
their cost fi (xi, x-i) with respect to their choice variable xi where x-i is the vector of all other
actions xj with j 6= i. We denote Difi as the derivative of fi with respect to xi, Dijfi as the partial
derivative of Difi with respect to xj, and Di2fi as the partial derivative of Difi with respect to xi.
Mathematical Notation. Given a matrix A ∈ Rn1 ×n2, let vec(A) ∈ Rn1n2 be its vectorization such
that vec(A) takes rows ai of A, transposes them and stacks them vertically in order of their index.
Let 0 and ㊉ denote the Kronecker product and sum respectively, where A ㊉ B = A 0 I + 10 B.
Moreover,田 is an operator that generates an 2n(n +1) X ɪn(n +1) matrix from a matrix A ∈ Rn×n
such that A田 A = Hn (A㊉ A)Hn where Hn = (HnHn)THn is the (left) pseudo-inverse of Hn,
a full column rank duplication matrix. Let X«&乂(・)be the largest positive real root of its argument if
it exists and zero otherwise. See Lancaster & Tismenetsky (1985) and Appendix B for more detail.
Equilibrium. There are natural equilibrium concepts depending on the order of play: the (local)
Nash equilibrium concept in the case of simultaneous play and the (local) Stackelberg (equivalently
minmax in zero-sum games) equilibrium concept in the case of hierarchical play (BaSar & Olsder,
1Following Fiez et al. (2020), we refer to strict local Stackelberg as differential Stackelberg throughout.
2
Published as a conference paper at ICLR 2021
1998). Formal local equilibrium definitions are provided in Appendix B, while here we characterize
the different equilibrium notions in terms of sufficient conditions on player costs as is typical in the
machine learning and optimization literature (see, e.g., Berard et al. 2020; Daskalakis & Panageas
2018; Fiez et al. 2020; Goodfellow 2016; Jin et al. 2020; Mazumdar et al. 2020; Wang et al. 2020).
The following definition is characterized by sufficient conditions for a local Nash equilibrium.
Definition 1 (Differential Nash Equilibrium, Ratliff et al. 2013). The joint strategy x ∈ X is a
differential Nash equilibrium if D1f(x) = 0, -D2f (x) = 0, D12 f (x) > 0, and D22 f (x) < 0.
The Jacobian of the vector of individual gradients g(x) = (D1f(x), -D2f(x)) is defined by
J(x)
D12f(x)
-D1>2f (x)
D12f(x)
-D22f(x)
(1)
Let Sι(∙) denote the SchUr complement of (∙) with respect to the n X n block in (∙). The following
definition is characterized by sufficient conditions for a local Stackelberg equilibrium.
Definition 2 (Differential Stackelberg EqUilibriUm, Fiez et al. 2020). The joint strategy x ∈ X is a
differential Stackelberg equilibrium if D1f(x) = 0, -D2 f (x) = 0, S1 (J (x)) > 0, D22 f (x) < 0.
Learning Dynamics. We stUdy agents seeking eqUilibria of the game via a learning algorithm and
consider argUably the most natUral learning rUle in zero-sUm continUoUs games: gradient descent-
ascent (GDA). Moreover, we investigate this learning rUle with timescale separation between the
players. Let T = γ2/γι be the learning rate ratio and define Λτ = blockdiag(I^, τIn2) where Ini
is a ni × ni identity matrix. The τ -GDA dynamics with g(x) = (D1f(x), -D2f(x)) are given by
xk+1 = xk - γ1Λτg(xk).	(2)
3 Stability of Continuous Time GDA with Timescale Separation
To characterize the convergence of τ-GDA, we begin by stUdying its continUoUs time limiting system
X = -Λτ g(x).	(3)
The Jacobian of the system from (3) is given by Jτ (x) = ΛτJ(x) where J(x) is defined in (1).
Observe that critical points (x* such that g(χ*) = 0) are shared between T-GDA and (3). Thus,
by analyzing the stability of the continUoUs time system aroUnd critical points as a fUnction of
the timescale separation T using the Jacobian Jτ(x*), we can draw conclusions about the stability
and convergence of the discrete time system T -GDA. It well known that a critical point is locally
(exponentially) stable when the spectrum of -JT (x*) is in the open left-half complex plane C- (cf.
Theorem B.1, Appendix B). Throughout, we use the broader term “stable” to mean the following.
Definition 3. A critical point x* is locally exponentially Stablefor X = —Λτg(x) if an only if
spec(-Jτ(x*)) ⊂ C- (or, equivalently, SPec(Jτ(x*)) ⊂ C*) where C- and C* denote the open
left-half and right-half complex plane, respectively.
We now show differential Stackelberg equilibria are the only critical points that are stable for a range
of finite learning rate ratios,2 whereas the remainder of critical points are unstable for a range of finite
learning rate ratios. Importantly, we characterize the learning rate ratios for which the results hold.
3.1	Necessary and Sufficient Conditions for Stability
To motivate our main stability result, the following example shows the existence of a differential
Stackelberg which is unstable for T = 1, but is stable all for T ∈ (T*, ∞) where T* is finite.
Example 1. Consider the quadratic zero-sum game defined by the cost
f (χι, x2) = 2(—X21 + 2χ22 - 2χιιχ2i - 1 χ2ι + χ12χ22 - χ22)
where v > 0 and x1, x2 ∈ R2. The unique critical point x* = (0, 0) is a differential Stackelberg
equilibrium since g(x*) = 0, Sι(J(x*)) = diag(v, 4) > 0, and D2f (x*) = -diag(2,v) < 0.
2Note that differential Nash are a subset of differential Stackelberg (Fiez et al., 2020; Jin et al., 2020).
3
Published as a conference paper at ICLR 2021
Moreover, spec(-Jτ(x*)) = {-v(2τ+1±√4τ2 — 8τ +1), -V(T—2±√τ2 — 12τ + 4)}. Observe
that for any v > 0, x* is unstable for T = 1 since spec(-Jτ (x*)) ⊂ C-, but x* is Stablefor a
range of learning rates since spec(-Jτ (x*)) ⊂ C- for all T ∈ (2, ∞).
In other words, GDA fails to converge to the equilibrium but a finite timescale separation is sufficient
to remedy this problem. We now fully characterize this phenomenon. To provide some background,
We remark it is known that the spectrum of -JT(x*) asymptotically splits as T → ∞ such that nι
eigenvalues tend to fixed positions defined by the eigenvalues of —S1(J(x*)), while the remaining
n2 eigenvalues tend to infinity at a linear rate T along asymptotes defined by the eigenvalues of
D22f(x*). This result is known from Klimushchev & Krasovskii (1961) and further discussion can
be found in Appendix I as well as from Kokotovic et al. (1986, Chap. 2, Thm. 3.1). The previ-
ous fact is specialized from the class of singularly perturbed linear systems to T -GDA by Jin et al.
(2020) which directly results in the connection between critical points of ∞-GDA and differential
Stackelberg equilibrium. Specifically, the result of Jin et al. (2020) is showing that for the class of
all sufficiently smooth games the stable critical points of ∞-GDA are exactly the strict local min-
max. As a corollary of this fact, there exists a T1 < ∞, such that T -GDA is stable for all T > T1
(Kokotovic et al., 1986, Chap. 2, Cor. 3.1); this can be inferred from the proof of Theorem 28
in Jin et al. (2020) as well. Indeed, Jin et al. (2020) gives an asymptotic expansion showing that
nι eigenvalues of -JT(x*) are in spec(-Sι( J(x*))) + O(T-1) and the remaining n eigenvalues
are in T (spec(D22f (x*)) + O(T -1)). Using the limit definition for the asymptotic expansion, for
any fixed game and a strict local minmax x*, one can show that there exists a finite T such that
x* is stable. We provide a detailed discussion of the relationship between the results of Jin et al.
(2020) and Kokotovic et al. (1986) in Appendices A, I, and J. Unfortunately, the finite T1 obtainable
from the asymptotic expansion method can be arbitrarily large. From a practical perspective, this
poses significant problems for the implementation and performance of T -GDA. Indeed, the eigen-
value gap between spec(—S1(J (x*))) and spec(T D22f (x*)) has a linear dependence on T and, in
turn, the problem may become highly ill-conditioned from a numerical perspective as T becomes
large (Kokotovic, 1975). In contrast, we determine exactly the range of T such that the spectrum of
-JT(x) remains in C-, and hence, remedy this problem.
For the statement of the following theorem on the non-asymptotic construction ofT*, we define the
following matrices: for a critical point x*, let S1 = S1(-JT(x*)) = A11 - A12A2-21A1>2 and
-D12f(x*)	-D12f(x*)
TD1>2f(x*)	TD22f(x*)
A11	A12
-T A1>2 TA22
Theorem 1 (Non-Asymptotic Construction of Necessary and Sufficient Conditions for Stability).
Consider a zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr (X, R) for some r ≥ 2. Suppose
that x* is such that g(x*) = 0 and det(D22f2(x*)) 6= 0. There exists a T* ∈ [0, ∞) such that
spec(-Jτ(x*)) ⊂ C- forall T ∈ (T*, ∞) ifandonlyif x* is a differential Stackelberg equilibrium.
Moreover, T* = λ+max(Q) where
C — O Γ/ 4 GzI-I、订	(T GzI-I 4>、订 ] A22 Hn (A 12 X In 2)	_ ( -	-	d-八
Q = 2 [(A12 % A22 )Hn2	(InI % A22 A12)HnJ	-S-1H + (Sl 0 A12A-D	-(AII	区	A22 )
with A22 = A22 田 A22 and Si = Si 田 Si.
While at first glance Q may appear difficult to understand, it is efficiently computable and can be
used to understand the typical value for important classes of games. Indeed, many problems like
generative adversarial networks have specific structure for the individual Hessians of each player
and the interaction matrix Di2f (cf. Assumption 1, Section 3.3) and are in a sense subject to design
via network architecture and loss function selection. This result opens up an interesting future
direction of research on understanding and potentially designing the structure ofQ. To take a step in
this direction, we explore a number of games in Section 5 and Appendix K where we compute T * by
the construction and validate it is tight empirically. Along the way, we discover that T* is typically
a reasonable value that is amenable to practical implementations.
As a direct consequence of Theorem 1, T -GDA converges locally asymptotically for any sufficiently
small γ(T) and for all T ∈ (T*, ∞) if and only if x* is a differential Stackelberg equilibrium; for a
formal statement see Corollary C.1 in Appendix C.
Proof Sketch of Theorem 1. The full proof is contained in Appendix C. The key tools used in this
proof are a combination of Lyapunov stability and the notion of a guard map (Saydy et al., 1990),
4
Published as a conference paper at ICLR 2021
a new tool to the learning community. Recall that a matrix is exponentially stable if and only if
there exists a symmetric positive definite P = P> > 0 such that PJT(x*) + J>(x*)P > 0 (cf.
Theorem B.1, Appendix B). Hence, given a positive definite Q = Q> > 0, -JT(x*) is stable if and
only if there exists a unique solution P = P> to
((J>(x*) 0 I) + (I 0 J>(x*)))vec(P) = (J>(x*)㊉ J>(x*))vec(P) = Vec(Q)	(4)
where 0 and ㊉ denote the Kronecker product and Kronecker sum, respectively.3 The existence of
a unique solution P occurs if and only if JT> and -JT> have no eigenvalues in common. Hence,
using the fact that eigenvalues vary continuously, if we vary τ and examine the eigenvalues of the
map JT(x*)㊉ JT(x*), this tells US the range of T for which spec(-Jτ(x*)) remains in C-. This
method of varying parameters and determining when the roots of a polynomial (or correspondingly,
the eigenvalues of a map) cross the boundary of a domain uses a guard map; it provides a certificate
that the roots of a polynomial lie in a particular guarded domain for a range of parameter values.
Formally, let X be the set of all n × n real matrices or the set of all polynomials of degree n with
real coefficients. Consider S an open subset of X with closure S and boundary ∂S. The map
V : X → C is said to be a guardian map for S if for all X ∈ S, V(x) = 0 ^⇒ X ∈ ∂S. Elements
of S(C-) = {A ∈ Rn×n : SPec(A) ⊂ C-} are (Hurwitz) stable. Given a pathwise connected
set U ⊆ R, the parameterized family {A(τ) : τ ∈ U} is stable if and only if (i) it is nominally
stable—meaning A(τι) ∈ S(C-) for some τι ∈ U—and (ii) V(A(T)) = 0 for all T ∈ U (Saydy
et al., 1990, Prop. i). The map V(T) = det(2(-Jτ(x*) Θ I)) = det(-( J「(x*)㊉ Jτ(x*))) guards
S (C-) where Θ is the bialternate product and is defined by A Θ B = 2 (A ㊉ B) for matrices A and
B (Govaerts, 2000, Sec. 4.4.4). For intuition, consider the case where each x1, x2 ∈ R so that
JT(x*)= -aTb Tbd ∈ R2×2.
It is known that spec(-Jτ(x*)) ⊂ C- if det(-Jτ(x*)) > 0 and tr(-Jτ(x*)) < 0 so that V(τ)=
det(-Jτ (x* ))tr( - Jτ (x*)) isa guard map for the 2×2 stable matrices S (C-). Since the bialternate
product generalizes the trace operator and det(-JT (x*)) = Tn2 det(D22f(x*)) det(-S1(J(x*))) 6=
0 for T 6= 0 by the facts (det(S1(J(x*))) 6= 0 and det(D22f(x*)) 6= 0) for a differential Stackelberg
equilibrium x*, a guard map in the general n X n case is V (τ) = det(一(Jr (x*)㊉ JT (x*))).
This guard map in T is closely related to the vectorization in (4): for any symmetric positive definite
Q = Q> > 0, there will be a symmetric positive definite solution P = P> > 0 of -(JT(x*)㊉
J>(x*))vec(P) = vec(-Q) if and only if det(-( Jτ(x*)㊉ Jτ(x*))) = 0. Hence, to find the range
ofT for which, given any Q = QT > 0, the solution P = PT is no longer positive definite, we need
to find the value of T such that V(τ) = det(-(JT(x*)㊉ JT(x*))) = 0—that is, where it hits the
boundary ∂S(C-). Through algebraic manipulation, this problem reduces to an eigenvalue problem
in T, giving rise to an explicit construction of T*.	口
3.2	Sufficient Conditions for Instability
To motivate our main instability result, the following example shows a non-equilibrium critical point
that is stable for T = 1, but is unstable for all T ∈ (T0, ∞) where T0 is finite.
Example 2. Consider the quadratic zero-sum game defined by the cost
f (x1, x2) = 4 (x2l - 1 xl2 + 2x11x21 + 2x21 + 2x12x22 - x22)
where x1, x2 ∈ R2 and v > 0. The unique critical point x* = (0, 0) is not a differential Stackelberg
(norNash) equilibrium since D2f(x*) = diag(v∕2, -v/4)>0, D2f (x*) = diag(v∕4, -v/2)攵 0.
Moreover, spec(-JT(x*)) = {-v(2t — 1 土 √4t2 — i2t + 1), -V(2 — T ± √t2 — 12t + 4)}.
Observe that for any v > 0, x* is stable for T = 1 since spec(-Jr (x*)) ⊂ C-, but x* is unstable
for a range of learning rates since spec(-Jr (x*)) ⊂ C- for all T ∈ (2, ∞). This is not an artifact
of the quadratic example: games can be constructed in which stable critical points lacking game-
theoretic meaning become unstable for all T > T0 even in the presence of multiple equilibria.
3See Lancaster & Tismenetsky (1985); Magnus (1988) for more detail on the definition and properties of
these mathematical operators, and Appendix C for more detail directly related to their use in this paper.
5
Published as a conference paper at ICLR 2021
This example demonstrates a finite timescale separation can prevent convergence to critical points
lacking game-theoretic meaning. We now characterize this behavior generally. Note that Theorem 1
implies that for any critical point which is not a differential Stackelberg equilibrium, there is no
finite τ* such that spec(-Jτ(x*)) ⊂ C- for all T ∈ (T*, oc). In particular, there exists at least
one finite, positive value of T such that spec(-Jτ(x*)) ⊂ C-. We can extend this result to answer
the question of whether there exists a finite learning rate ratio T0 such that -Jτ(x*) has at least one
eigenvalue with strictly positive real part for all T ∈ (T0, o), thereby implying that x* is unstable.
Theorem 2 (Non-Asymptotic Construction of Sufficient Condition for Instability.). Consider a
zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr(X, R) for some r ≥ 2. Suppose that x*
is such that g(x*) = 0, det(D22f2(x*) 6= 0, and x* is not a differential Stackelberg equilibrium.
Then spec(-Jτ(x*)) ⊂ C- for all T ∈ (to, ∞) with
T0 =λ+max(Q2-1((P1D12f(x*)+S1(-J(x*))L0>P2)>Q1-1(P1D12f(x*)
+ S1 (-J(x*))L0>P2) - P2L0D12f(x*) - (P2L0D12f(x*))>)).
where P1, P2, Q1, Q2 are any non-singular Hermitian matrices such that (a) Qi > 0 for each
i = 1, 2, (b) S1(-J(x*))P1 + P1S1(-J(x*)) = Q1 and D22f(x*)P2 + P2D22f(x*) = Q2, and (c)
the following matrix pairs have the same inertia: (P1, S1 (-J (x*))) and (P2, D22 f (x*)).
Proof Sketch. The full proof is provided in Appendix D. The key idea is to leverage the Lyapunov
equation and Lemma B.3 to show that -Jτ (x*) has at least one eigenvalue with strictly positive real
part. Indeed, Lemma B.3 states that if S1(-J(x*)) has no zero eigenvalues, then there exists matri-
ces P1 = P1> and Q1 = Q1> > 0 such that P1S1(-J(x*)) + S1(-J(x*))P1 = Q1 where P1 and
S1(-J(x*)) have the same inertia—that is, the number of eigenvalues with positive, negative and
zero real parts, respectively, are the same. An analogous statement applies to -D22f(x*) with some
P2 and Q2. Since x* is a non-equilibrium critical point, without loss of generality, let S1(-J(x*))
have at least one strictly positive eigenvalue so that P1 does as well. Next, we construct a matrix P
that is congruent to blockdiag(P1, P2) and a matrix Qτ such that -PJτ(x*) - Jτ>(x*)P = Qτ.
Since P and blockdiag(P1, P2) are congruent, Sylvester’s law of inertia implies that they have the
same number of eigenvalues with positive, negative, and zero real parts, respectively. Hence, P
has at least one eigenvalue with strictly positive real part. We then construct T0 via an eigenvalue
problem such that for all T > T0, Qτ > 0. Applying Lemma B.3 again, for any T > T0, -Jτ(x*)
has at least one eigenvalue with strictly positive real part so that spec(- JT (x*)) ⊂ C-.	□
3.3 Regularization with Applications to Adversarial Learning
In this section, we focus on generative adversarial networks with regularization and using the theory
developed so far extend the results to provide a stability guarantee for a range of regularization
parameters and learning rate ratios. Consider the training objective
f(θ, ω)= Ep(Z) ['(D(G(z; θ); ω))] + EpD ⑺['(-D(x; ω))]	(5)
where Dω(x) and Gθ(z) are discriminator and generator networks, pD(x) is the data distribution
while p(z) is the latent distribution, and ` ∈ C2 (R) is some real-value function.4 Nagarajan &
Kolter (2017) show, under suitable assumptions, that gradient-based methods for training generative
adversarial networks are locally convergent assuming the data distributions are absolutely contin-
uous. However, as observed by Mescheder et al. (2018), such assumptions not only may not be
satisfied by many practical generative adversarial network training scenarios such as natural im-
ages, but often the data distribution is concentrated on a lower dimensional manifold. The latter
characteristic leads to highly ill-conditioned problems and nearly purely imaginary eigenvalues.
Gradient penalties ensure that the discriminator cannot create a non-zero gradient which is orthog-
onal to the data manifold without suffering a loss. Introduced by Roth et al. (2017) and refined in
Mescheder et al. (2018), we consider training generative adversarial networks with one of two fairly
natural gradient-penalties used to regularize the discriminator:
Rι(θ,ω) = μEpD(χ)[∣∣VχD(x; ω)k2] and R2(θ,ω) = 2Epθ(χ)[∣∣VχD(x; ω)k2],
4For example, '(x) = — log(1 + exp(-x)) gives the original formulation of Goodfellow et al. (2014).
6
Published as a conference paper at ICLR 2021
where, by a slight abuse of notation, Vχ(∙) denotes the partial gradient with respect to X of the
argument (∙) when the argument is the discriminator D(∙; ω) in order prevent any conflation between
the notation D(∙) elsewhere for derivatives. Let hι(θ) = Epg(χ)[VωD(x;ω)∣ω=ω*] and h2(ω)=
EpD(x)[|D(x; ω)∣2 + ∣∣VχD(x; ω)k2]. Define reparameterization manifolds MG = {θ : pθ = PD}
and MD = {ω : h2 (ω) = 0} and let Tθ* MG and Tω* MD denote their respective tangent spaces
at θ* and ω*. As in Mescheder et al. (2018), We make the following assumption.
Assumption 1. Consider a zero-sum game of the form given in (5) where f ∈ C2(Rn1 ×Rn2 , R) and
G(∙; θ) and D(∙; ω) are the generator and discriminator networks, respectively, and X = (θ, ω) ∈
Rn1 X Rn2. Suppose that x* = (θ*,ω*) is an equilibrium. Then, (a) at (θ*,ω*), pθ* = PD and
D(x; ω*) = 0 in some neighborhood of SuPP(PD) ,(b) the function ' ∈ C2 (R) satisfies '0(0) = 0
and 'oo(0) < 0, (c) there are E-balls Be(θ*) and Be(ω*) centered around θ* and ω*, respectively,
so that MG ∩ B(θ*) and MD ∩ B(ω*) define C 1 -manifolds. Moreover, (i) ifw ∈/ Tθ* MG, then
w>Vwh1(θ*)w 6= 0, and (ii) ifv ∈/ Tω* MD, then v>V2ωh2(ω*)v 6= 0.
We note that as explained by Mescheder et al. (2018), Assumption 1.c(i) implies that the discrim-
inator is capable of detecting deviations from the generator distribution in equilibrium, and As-
sumption 1.c(ii) implies that the manifold MD is sufficiently regular and, in particular, its (local)
geometry is captured by the second (directional) derivative of h2 .
Theorem 3. Consider training a generative adversarial network via a zero-sum game with genera-
tor network Gθ, discriminator network Dω, and loss f(θ, ω) with regularization Rj (θ, ω) (for either
j = 1 or j = 2) and any regularization parameter μ ∈ (0, ∞) such that Assumption 1 is satisfied
for an equilibrium X* = (θ*, ω*) of the regularized dynamics. Then, X* = (θ*, ω*) is a differential
Stackelberg equilibrium. Furthermore, for any T ∈ (0, ∞), spec(-J(τ,μ)(x*)) ⊂ C-.
4	Provab le Convergence of GDA with Timescale Separation
In this section, we characterize the asymptotic convergence rate for τ -GDA to differential Stackelberg
equilibria, and provide a finite time guarantee for convergence to an ε-approximate equilibrium. The
asymptotic convergence rate result uses Theorem 1 to construct a finite τ* ∈ (0, ∞) such that X*
is stable, meaning spec(-Jr(x*)) ⊂ C-, and then for any T ∈ (τ*, ∞), the two key lemmas一
namely, Lemmas F.1 and F.2 in Appendix F—imply a local asymptotic convergence rate.
Theorem 4. Consider a zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr (X, R) for r ≥
2 and let X* be a differential Stackelberg equilibrium of the game. There exists a T* ∈ (0, ∞)
such that for any T ∈ (T*, ∞) and α ∈ (0, γ), T -GDA with learning rate γ1 = γ - α converges
locally asymptotically at a rate of O((1 一 ɑ∕(4β))k/2) where Y = mi□λ∈spec(jτ(x*)) 2Re(λ)∕∣λ∣2,
λm = argminλ∈spec(jτ(χ*)) 2Re(λ)∕∣λ∣2, and β = (2Re(λm) — α∣λm∣2)-1. Moreover, if x* is a
differential Nash equilibrium, T* = 0 so that for any T ∈ (0, ∞) and α ∈ (0, γ), T -GDA with
Yi = γ — α converges with a rate O((1 — α∕(4β))k/2).
To build some intuition, consider a differential Stackelberg equilibrium x* and its corresponding T*
obtained via Theorem 1 so that for any fixed T ∈ (τ*, ∞), spec(-Jr (x*)) ⊂ C-. For the discrete
time system xk+1 = xk — Y1Λτg(xk), if Y1 is chosen such that the spectral radius of the local
linearization of the discrete time map is a contraction, then xk locally (exponentially) converges
to x* (cf. Proposition B.1). With this in mind, we formulate an optimization problem to find the
upper bound Y on the learning rate Y1 such that for all Y1 ∈ (0, Y), ρ(I — Y1Jτ(x*)) < 1; indeed,
let γ = minγ>o {γ : maxλ∈spec(jτ(x*)) |1 — γλ∣ ≤ 1}. The intuition is as follows. The inner
maximization problem is over a finite set sPec(Jτ (x*)) = {λ1, . . . , λn} where Jτ(x*) ∈ Rn×n. As
γ increases away from zero, each 11 一 γλi | shrinks in magnitude. The last λi such that 1 -γλ% hits the
boundary of the unit circle in the complex plane gives us the optimal Y and the λm ∈ sPec(Jτ (x*))
that achieves it. Examining the constraint, we have that for each λ%, Y(γ∣λ∕2 — 2Re(λi)) ≤ 0 for
any γ > 0. As noted this constraint will be tight for one of the λ, in which case Y = 2Re(λ)∕∣λ∣2
since γ > 0. Hence, by selecting Y = mi□λ∈spec(jτ(x*)) 2Re(λ)∕∣λ∣2, we have that |1 — γιλ∣ < 1
for all λ ∈ sPec(Jτ(x*)) and any Y1 ∈ (0, Y). From here, one can use standard arguments from
numerical analysis to show that for the choice of α and β, the claimed asymptotic rate holds.
Theorem 4 directly implies a finite time convergence guarantee for obtaining an ε-differential Stack-
elberg equilibrium, that is, a point with an ε-ball around a differential Stackelberg equilibrium x*.
7
Published as a conference paper at ICLR 2021
Corollary 1. Given ε > 0, under the assumptions of Theorem 4, T-GDA obtains an ε-differential
StaCkleberg equilibrium in d(4β∕α) log(∣∣x0 - x*∣∣∕ε)[ iterations for any x0 ∈ Bδ (x*) with δ =
α∕(4Lβ) where L is the local Lipschitz constant of I — γJτ(x*).
Moreover, the convergence rates and finite time guarantees extend to the gradient penalty regularized
generative adversarial network described in the preceeding section.
Corollary 2. Under the assumptions of Theorems 3 and 4, for any fixed μ ∈ (0, ∞) and T ∈
(0, ∞), τ-GDA converges locally asymptotically at a rate of O((1 — α∕(4β))k/2), and achieves an
ε-equilibrium in d(4β∕α) log(∣∣x0 — x*∣∕ε)] iterations for any x0 ∈ Bδ (x*).
In Appendix H, we extend the convergence analysis to the stochastic setting.
5	Experiments
We now present numerical experiments and Appendix K contains further simulations and details.
Dirac-GAN: Regularization, Timescale Separation, and Convergence Rate. The Dirac-
GAN (Mescheder et al., 2018) consists of a univariate generator distribution pθ = δθ and a linear
discriminator D(x; ω) = ωx, where the real data distribution pD is given by a Dirac-distribution
concentrated at zero. The resulting zero-sum game is defined by the cost f (θ,ω) = '(θω) + '(0)
and the unique critical point (θ*, ω*) = (0,0) is a local NaSh equilibrium. However, the eigenvalues
of the Jacobian are purely imaginary regardless of the choice of timescale separation so that T -GDA
oscillates and fails to converge. This behavior is expected since the equilibrium is not hyperbolic
and corresponds to neither a differential Nash equilibrium nor a differential Stackelberg equilibrium
but it is undesirable nonetheless. The zero-sum game corresponding to the Dirac-GAN with regular-
ization can be defined by the cost f (θ, ω) = '(θω) + '(0) — 2ω2. The unique critical point remains
unchanged, but for all T ∈ (0, ∞) and μ ∈ (0, ∞) the equilibrium of the unregularized game is
stable and corresponds to a differential Stackelberg equilibrium of the regularized game.
From Figures 1a and 1f, we observe that the impact of timescale separation with regularization
μ = 0.3 is that the trajectory is not as oscillatory since it moves faster to the zero line of -D2f (θ, ω)
and then follows along that line until reaching the equilibrium. We further see from Figure 1b that
with regularization μ = 0.3, T-GDA with T = 8 converges faster to the equilibrium than T-GDA with
T = 16, despite the fact that the former exhibits some cyclic behavior in the dynamics while the
(a) Trajectories of T-GDA (b) Distance to equilibrium (c) SPeC(JT), μ = 0.3	(d) SPeC(JT), μ = 1
---τ=l,μ = 0.3 --- T = 4, 〃 = 0.3 - τ = 8,μ = 0.3 - τ = 16,μ = 0.3 - τ=l,μ = l
(f) Trajectories of T-GDA overlayed on vector fields generated by choices of T and μ.
Figure 1: Experimental results for the Dirac-GAN game of Section 5.
8
Published as a conference paper at ICLR 2021
μ∖τ	1	2	4	8
10	:	37.0	27.1	26.3	29.1
1	26.7/21.6	21.6/18.1	20.1/17.6	20.7/18.6
Figure 2: CIFAR-10 FID
μ∖τ	1	2	4	8	16
10	:14.6	11.7	10.7	10.1	11.2
1	13.1/8.5	10.3/6.8	8.8/6.1	8.0/5.8	8.1/6.2
Figure 3: CelebA FID
latter does not. The eigenvalues of the Jacobian with regularization μ = 0.3 presented in Figure 1c
explains this behavior since the imaginary parts are non-zero with τ = 8 and zero with τ = 16,
while the eigenvalue with the minimum real part is greater at τ = 8 than at τ = 16. This highlights
that some oscillatory behavior in the dynamics is not always harmful for convergence. For μ = 1
and τ = 1, Figures 1a and 1b show that even though τ -GDA does not cycle since the eigenvalues
of the Jacobian are purely real, the trajectory converges slowly to the equilibrium. Indeed, for each
regularization parameter, the eigenvalues of JT (θ*, ω*) split after becoming purely real and then
converge toward the eigenvalues of S1 (J (θ*, ω *)) and -τD2 f(θ*, ω *). Since S1 (J (θ *, ω *)) α 1∕μ
and -τD2f (θ*, ω*) Y τμ, there is a trade-off between the choice of regularization μ and the
timescale separation τ on the conditioning of the Jacobian matrix that dictates the convergence rate.
Generative Adversarial Networks: Image Datasets. We build on the implementations
of Mescheder et al. (2018) and train with the non-saturating objective and the R1 gradient penalty.
The network architectures are both ResNet based. We fix the initial learning rate for the generator
to be γ1 = 0.0001 with CIFAR-10 and γ1 = 0.00005 for CelebA. The learning rates are decayed
so that γ1,k = γ1∕(1 + ν)k and γ2,k = τ γ1,k are the generator and discriminator learning rates at
update k where ν = 0.005. The batch size is 64, the latent data is drawn from a standard normal
of dimension 256, and the resolution of the images is 32 × 32 × 3. We run RMSprop with param-
eter α = 0.99 and retain an exponential moving average of the generator parameters for evaluation
with parameter β = 0.9999. We remark that RMSprop is an adaptive method that builds on GDA
and is commonly used in training for image datasets. It is adopted here to explore the interplay
with timescale separation and to determine if similar observations emerge compared to our exten-
sive experiments with τ -GDA (see Appendix K). The FID scores (Heusel et al., 2017) along the
learning path and in numeric form at 150k/300k mini-batch updates for CIFAR-10 and CelebA with
regularization parameters μ = 10 and μ = 1 are presented in Figures 2 and 3, respectively. The
experiments were each repeated with 3 random seeds which yielded similar results and the mean
scores are reported. The choices of τ = 4 and τ = 8 converge fastest with each regularization
parameter for CIFAR-10 and CelebA, respectively. The performance with regularization μ = 1 is
superior to that with μ = 10, which highlights the interplay between timescale separation and regu-
larization. Moreover, we see that timescale separation improves convergence until hitting a limiting
value. These conclusions agree with the insights from the simple Dirac-GAN experiment. Finally, it
is worth reiterating there is a coupling between τ and γ1 : τ must be selected so that the continuous-
time system is stable and then γ1 must be chosen so that the discrete-time update is both stable and
numerically well-conditioned for the choice of τ .
6	Conclusion
We prove gradient descent-ascent locally converges to a critical point for a range of finite learning
rate ratios if and only if the critical point is a differential Stackelberg equilibrium. This answers a
standing open question about the local convergence of first order methods to local minimax equilib-
ria. A key component of the proof is the construction of a (tight) finite lower bound on the learning
rate ratio τ for which stability is guaranteed, and hence local asymptotic convergence of τ-GDA.
9
Published as a conference paper at ICLR 2021
References
EH Abed, L Saydy, and AL Tits. Generalized stability of linear singularly perturbed systems includ-
ing calculation of maximal parameter range. In Robust Control of Linear Systems and Nonlinear
Control,pp.197-203. SPringer,1990.
Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In International Conference on Artificial Intel-
Iigence and Statistics, pp. 486T95, 2019.
John M Alongi and Gail Susan Nelson. Recurrence and topology, volume 85. American Mathemat-
ical Society, 2007.
I.	K. Argyros. A generalization of ostrowski’s theorem on fixed points. Applied Mathematics Letters,
12:77-79, 1999.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
James P. Bailey, Gauthier Gidel, and Georgios Piliouras. Finite regret and cycles with fixed step-size
via alternating gradient descent-ascent. In Conference on Learning Theory, pp. 391-407, 2020.
D Balduzzi, S Racaniere, J Martens, J Foerster, K Tuyls, and T Graepel. The mechanics of n-player
differentiable games. In International Conference on Machine Learning, volume 80, pp. 363-372,
2018.
Tamer BaSar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.
Michel Benaim. A Dynamical System Approach to Stochastic Approximations. SIAM Journal on
Control and Optimization, 34(2):437-472, 1996.
Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A
Closer Look at the Optimization Landscapes of Generative Adversarial Networks. In Interna-
tional Conference on Learning Representations, 2020.
Vivek Borkar. Stochastic Approximation: A Dynamical Systems Approach. Springer, 2008.
Benjamin Chasnov, Lillian J. Ratliff, Eric Mazumdar, and Samuel A. Burden. Convergence anal-
ysis of gradient-based learning in continuous games. Conference on Uncertainty in Artificial
Intelligence, 2019.
Ashish Cherukuri, Bahman Gharesifard, and Jorge Cortes. Saddle-point dynamics: conditions for
asymptotic stability of saddle points. SIAM Journal on Control and Optimization, 55(1):486-511,
2017.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236-9246,
2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. International Conference on Learning Representations, 2018.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization. In ACM Symposium on Theory of Computing, 2021.
Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? International Confer-
ence on Machine Learning, 2020.
Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. International Con-
ference on Machine Learning, 2020.
Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In International Conference on Au-
tonomous Agents and MultiAgent Systems, pp. 122-130, 2018.
10
Published as a conference paper at ICLR 2021
GaUthier GideL Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-JUlien. A varia-
tional inequality perspective on generative adversarial networks. In International Conference on
Learning Representations, 2019a.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Le Priol, Gabriel Huang, Si-
mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In International Conference on Artificial Intelligence and Statistics, pp. 1802-1811, 2019b.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Willy J. F. Govaerts. Numerical Methods for Bifurcations of Dynamical Equilibria. Society for
Industrial and Applied Mathematics, 2000.
Joao P Hespanha. Linear Systems Theory. Princeton University Press, 2nd edition, 2018.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Roger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, 1985.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. What is local optimality in nonconvex-
nonconcave minimax optimization? International Conference on Machine Learning, 2020.
Sameer Kamal. On the convergence, lock-in probability, and sample complexity of stochastic ap-
proximation. SIAM Journal on Control and Optimization, 48(8):5178-5192, 2010.
Hassan Khalil. Nonlinear Systems. Prentice Hall, 3rd edition, 2002.
AI Klimushchev and NN Krasovskii. Uniform asymptotic stability of systems of differential equa-
tions with a small parameter in the derivative terms. Journal of Applied Mathematics and me-
chanics, 25(4):1011-1025, 1961.
P. Kokotovic. A Riccati equation for block-diagonalization of ill-conditioned systems. IEEE Trans-
actions on Automatic Control, 20(6):812-814, 1975.
Peter V Kokotovic, John O’Reilly, and Hassan K Khalil. Singular Perturbation Methods in Control:
Analysis and Design. Academic Press, Inc., 1986.
NN Krasovskii. Stability of Motion: Application of Lyapunov’s Second Method to Differential
Systems and Equations with Time-Delay, 1963.
PA Lagerstrom and RG Casten. Basic concepts underlying singular perturbation techniques. Siam
Review, 14(1):63-120, 1972.
Peter Lancaster and Miron Tismenetsky. The theory of matrices: with applications. Elsevier, 1985.
Alistair Letcher, David Balduzzi, Sebastien Racaniere, James Martens, Jakob N Foerster, Karl Tuyls,
and Thore Graepel. Differentiable game mechanics. Journal of Machine Learning Research, 20:
84-1, 2019a.
Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktaschel, and Shimon Whiteson. Stable
opponent shaping in differentiable games. International Conference on Learning Representations,
2019b.
Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization.
Conference on Learning Theory, 2020a.
11
Published as a conference paper at ICLR 2021
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. International Conference on Machine Learning, 2020b.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pp.
1540-1552, 2020.
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive ap-
proximation for one-sided non-convex min-max problems: algorithms and applications. IEEE
Transactions on Signal Processing, 2020.
Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse. Self-tuning net-
works: Bilevel optimization of hyperparameters using structured best-response functions. Inter-
national Conference on Learning Representations, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Jan Magnus. Linear Structures. Griffin, 1988.
E. Mazumdar and L. J. Ratliff. Local Nash Equilibria are Isolated, Strict Local Nash Equilibria in
‘Almost All’ Zero-Sum Continuous Games. In IEEE Conference on Decision and Control, pp.
6899-6904, 2019.
Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On Gradient-Based Learning in Continuous
Games. SIAM Journal on Mathematics of Data Science, 2(1):103-131, 2020.
Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local nash equilibria (and
only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In International Conference on Learning Representations, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine learning, pp. 3481-3490, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. International Conference on Learning Representations, 2017.
D Mustafa and TN Davidson. Generalized integral controllability. In IEEE Conference on Decision
and Control, volume 1, pp. 898-903. IEEE, 1994.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative first order methods. In Advances in Neural
Information Processing Systems, pp. 14934-14942, 2019.
J.	M. Ortega and W. C. Rheinboldt. Iterative Solutions to Nonlinear Equations in Several Variables.
Academic Press, 1970.
Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-order nash
equilibria in nonconvex-concave smooth min-max problems. arXiv preprint arXiv:2002.07919,
2020.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060,
2018.
12
Published as a conference paper at ICLR 2021
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model
based reinforcement learning. International Conference on Machine Learning, 2020.
L. J. Ratliff, S. A. Burden, and S. S. Sastry. Characterization and computation of local Nash equi-
libria in continuous games. In Allerton Conference on Communication, Control, and Computing,
pp. 917-924, 2013.
L. J. Ratliff, S. A. Burden, and S. S. Sastry. Genericity and structural stability of non-degenerate
differential Nash equilibria. In American Control Conference, pp. 3990-3995, 2014.
Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. On the characterization of local Nash
equilibria in continuous games. IEEE Transactions on Automatic Control, 61(8):2301-2307,
2016.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Advances in Neural Information Pro-
cessing Systems, pp. 2018-2028, 2017.
S. Shankar Sastry. Nonlinear Systems Theory. Springer-Verlag, 1999.
Shankar Sastry and C Desoer. Jump behavior of circuits and systems. IEEE Transactions on Circuits
and Systems, 28(12):1109-1124, 1981.
Lahcen Saydy. New stability/performance results for singularly perturbed systems. Automatica, 32
(6):807 - 818, 1996.
Lahcen Saydy, Andre L Tits, and Eyad H Abed. Guardian maps and the generalized stability of
parametrized families of matrices and polynomials. Mathematics of Control, Signals and Systems,
3(4):345-371, 1990.
Florian Schafer and Anima Anandkumar. Competitive gradient descent. In Advances in Neural
Information Processing Systems, pp. 7625-7635, 2019.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. International Conference on Learning Representations, 2018.
Gerald Teschl. Ordinary differential equations and dynamical systems. Graduate Studies in Mathe-
matics, 140:08854-8019, 2000.
Gugan Thoppe and Vivek Borkar. A concentration bound for stochastic approximation via alekseev’s
formula. Stochastic Systems, 9(1):1-26, 2019.
Christiane Tretter. Spectral theory of block operator matrices and applications. World Scientific,
2008.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In Advances in Neural Information Processing Systems, pp. 10450-10461, 2019.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. International Conference on Learning Representations, 2020.
Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chan-
drasekhar. The unusual effectiveness of averaging in gan training. International Conference on
Learning Representations, 2019.
Chongjie Zhang and Victor R Lesser. Multi-agent learning with policy prediction. In AAAI Confer-
ence on Artificial Intelligence, volume 3, pp. 8, 2010.
Guojun Zhang and Yaoliang Yu. Convergence of gradient methods on bilinear zero-sum games. In
International Conference on Learning Representations, 2020.
Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and stability in non-convex-non-
concave min-max optimization. arXiv preprint arXiv:2002.11875, 2020a.
13
Published as a conference paper at ICLR 2021
Guojun Zhang, Kaiwen Wu, Pascal Poupart, and Yaoliang Yu. Newton-type methods for minimax
optimization. arXiv preprint arXiv:2006.14592, 2020b.
Kaiqing Zhang, ZhUoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
14
Published as a conference paper at ICLR 2021
Appendix
Below we provide a table of contents as a guide to the appendix.
Table of Contents
A Related Work	16
Section A contains an extensive discussion of related work at the intersection of machine learning
and game theory as well as historical connections to control theory and dynamical systems.
B Mathematical and Game Theoretic Preliminaries	20
Section B includes mathematical and game theoretic preliminaries not covered in the main text and
it contains proofs of short technical helper lemmas.
C Proof of Theorem 1: Stability of T-GDA	24
Section C contains the proof of our main stability result: a critical point x* of the continuous time
dynamical system X = -Λτg(χ) is stable for a range of T if and only if x* is a differential Stackel-
berg equilibrium. Moreover this range of timescale separation parameters is characterized.
D Proof of Theorem 2: Instability of T -GDA	29
Section D contains the proof of our second main result on the existence ofa finite range of values of
T such that non-equilibrium critical points which are stable for some T, become unstable for all T in
a range of T’s lower bounded by a finite value.
E Proof of Theorem 3: Stability of T-GDA in Regularized GANs	30
Section E contains the proof of Theorem 3 which is an application of the main results to the impor-
tant adversarial learning application of training generative adversarial networks. Within the section,
we also provide necessary conditions on the sizes of the network architectures for the discriminator
and generator network for stability.
F Proof of Helper Lemmas and Theorem 4 for T -GDA Convergence	32
Section F contains the statement and proofs of two core technical lemmas on the convergence rate
of T -GDA. We then show how to invoke them together with the stability results to obtain Theorem 4.
G Proof of Corollary 1: Finite Time Convergence of T-GDA	35
Section G contains the proof of a finite time convergence guarantee on achieving an ε-differential
Stackelberg equilibrium, including an estimate on the size of the ball and commentary on how to
improve that estimate with alternative techniques.
H Convergence of Stochastic GDA with Timescale Separation	35
Section H contains an extension of the deterministic convergence guarantees to the stochastic setting
in which agents have access only to an unbiased estimator of their gradient. We provide convergence
guarantees for T -GDA in this setting as well under the least restrictive, to our knowledge, assumptions
on stochastic approximation updates to date.
I Stability of ∞-GDA: A singular perturbation approach	38
Section I contains a proof of the result for the limiting case of T → ∞. While this result appeared
recently in Jin et al. (2020), an analogous result has been known for some time in the theory of
singularly perturbed dynamical systems. Since the proof techniques are particularly illuminating
15
Published as a conference paper at ICLR 2021
and expose a new set of tools to this community, we provide this alternative proof from the singular
perturbation theory perspective.
J Further Details on Related Work	42
Section J contains an extended discussion on the related work by Jin et al. (2020). Specifically, we
expand on the relationship between Proposition 27 in Jin et al. (2020), which provides examples
of games such that the stable critical points of τ -GDA are not a subset of local minmax and vice
versa, and our main results. We show that the examples in these results, while illustrative of the fact
intended to be shown by the proposition stated, do not conflict with out findings.
K Experiments Supplement	44
Section K contains extended experimental results including applications to illustrative games (such
as those examples included in the main body) as well as extended GAN experiments. Our results
show that we are able to leverage τ and improve training results. These results are promising as
they suggest that understanding the relationship between the three key hyperparameters—timescale
separation T, regularization μ, and exponential moving averaging weight β—can improve first order
training (and hence, scalable) algorithms.
L Alternative Proof of Theorem 3 via τ * Construction from Theorem 1	60
In order to highlight the utility of Theorem 1 along with future directions of obtaining values of
τ * for structured games, we revisit the proof of Theorem 3 and derive the result directly from the
construction. The purpose of this section is to illustrate that the structure of equilibria considered in
Theorem 3 can be exploited to obtain the value of τ* for the entire class of games using properties
of the Kronecker product and sum.
A Related Work
In this section, we provide a review of related work at the intersection machine learning and game
theory, as well as connections to dynamical systems theory and control.
A.1 Machine Learning and Learning in Games
Given the extensive work on the topic of learning in games in machine learning that has gone on
over the last several years, we cannot cover all of it and instead focus our attention on only the
most relevant to this paper. We begin by reviewing solution concepts developed for the class of
games under consideration and then discuss some learning dynamics studied in the literature beyond
gradient descent-ascent. Following this, we delineate the related work studying gradient descent-
ascent in non-convex, non-concave zero-sum games and finish by making note of the literature on
bilinear and non-convex, concave zero-sum games.
Solution Concepts. Owing to the numerous applications in machine learning, a significant portion
of the modern work on learning in games has focused on the zero-sum formulation with non-convex,
non-concave cost functions. Most recently, Daskalakis et al. (2021) tout the importance and signif-
icance of this class of games in a paper on the complexity of finding equilibria (in particular, in
the constrained setting) in such games. Consequently, local solution concepts have been broadly
adopted. Compared to the standard game-theoretic notions of equilibrium that characterize player’s
incentive to deviate given the game and information structure, local equilibrium concepts restrict
the deviation search space to a suitable local neighborhood. Following the standard game-theoretic
viewpoint, a vast number of works in machine learning study the local Nash equilibrium concept and
critical points satisfying gradient-based sufficient conditions for the equilibrium, which are often re-
ferred to as differential Nash equilibria (Ratliff et al., 2013; 2014; Ratliff et al., 2016). Based on the
observation that in non-convex, non-concave zero-sum games the order of play is fundamental in
the definition of the game, there has been a push toward considering local notions of the Stackelberg
equilibrium concept, which is the usual game-theoretic equilibrium when there is an explicit order
of play between players. In the zero-sum formulation, Stackelberg equilibrium are often referred to
16
Published as a conference paper at ICLR 2021
as minmax equilibria. Similar to as for the Nash equilibrium, gradient-based sufficient conditions
for local minmax/Stackelberg equilibrium have been given (Fiez et al., 2020; Jin et al., 2020) and
such critical points have been referred to as differential Stackelberg equilibria (Fiez et al., 2020). We
remark that it has been shown that local/differential Nash equilibria are a subset of local/differential
Stackelberg equilibria (Fiez et al., 2020; Jin et al., 2020). Following past works, we adopt the termi-
nology of differential Nash equilibrium and differential Stackelberg equilibrium in this paper as the
meaning of strict local Nash equilibrium and strict local minmax/Stackelberg equilibrium, respec-
tively. It is also worth mentioning the proximal equilibria proposed by Farnia & Ozdaglar (2020),
which we do not consider in this work, that depending on a regularization parameter can interpolate
between the local Nash and local Stackelberg equilibrium notions. Finally, Zhang et al. (2020a)
propose a local robust point as an equilibrium definition that contains both local minmax and local
maxmin equilibrium and Vlatakis-Gkaragkounis et al. (2019) consider an equilibrium concept in
what they deem hidden bilinear games.
Learning Dynamics. Given that the focus of this work is on gradient descent-ascent, we center
our coverage of related work on papers analyzing its behavior. Nonetheless, we mention that a sig-
nificant number of learning dynamics for zero-sum games have been developed in the past few years,
in some cases motivated by the shortcomings of gradient descent-ascent without timescale separa-
tion. The methods include optimistic and extra-gradient algorithms (Daskalakis et al., 2018; Gidel
et al., 2019a; Mertikopoulos et al., 2019), negative momentum (Gidel et al., 2019b), gradient adjust-
ments (Balduzzi et al., 2018; Letcher et al., 2019a; Mescheder et al., 2017), and opponent modeling
methods (Foerster et al., 2018; Letcher et al., 2019b; Metz et al., 2017; Schafer & Anandkumar,
2019; Zhang & Lesser, 2010), among others. While the aforementioned learning dynamics possess
some desirable characteristics, they cannot guarantee that the set of stable critical points coincide
with a set of local equilibria for the class of games under consideration. However, there have been
a select few learning dynamics proposed that can guarantee the stable critical points coincide with
either the set of differential Nash equilibria (Adolphs et al., 2019; Mazumdar et al., 2019) or the set
of differential Stackelberg equilibria (Fiez et al., 2020; Wang et al., 2020; Zhang et al., 2020b)—
effectively solving the problem of guaranteeing local convergence to only a class of local equilibria.
However, since each of the algorithms achieving the equilibrium stability guarantee require solving
a linear equation in each update step, they are not efficient and can potentially suffer from degenera-
cies along the learning path in applications such as generative adversarial networks. These practical
shortcomings motivate either proving that existing learning dynamics using only first-order gradi-
ent feedback achieve analogous theoretical guarantees or developing novel computationally efficient
learning dynamics that can match the theoretical guarantee of interest.
Gradient Descent-Ascent. Gradient descent-ascent has been studied extensively in non-convex,
non-concave zero-sum games since it is a natural analogue to gradient descent from optimization, is
computationally efficient, and has been shown to be effective in practice for applications of interest
when combined with common heuristics. A prevailing approach toward gaining understanding of
the convergence characteristics of gradient descent-ascent has been to analyze the local stability
around critical points of the continuous time limiting dynamical system. The majority of this work
has not considered the impact of timescale separation. Numerous papers have pointed out that
the stable critical points of gradient descent-ascent without timescale separation may not be game-
theoretically meaningful. In particular, it has been shown that there can exist stable critical points
that are not differential Nash equilibrium (Daskalakis & Panageas, 2018; Mazumdar et al., 2020).
Furthermore, it is known that there can exist stable critical points that are not differential Stackelberg
equilibria (Jin et al., 2020). The aforementioned results rule out the possibility that gradient descent-
ascent without timescale separation can guarantee equilibrium convergence. In terms of the stability
of equilibria, it is known that differential Nash equilibrium are stable for gradient descent-ascent
without timescale separation (Daskalakis & Panageas, 2018; Mazumdar et al., 2020), but that there
can exist differential Stackelberg equilibria which are not stable with respect to gradient descent-
ascent without timescale separation.
The work of Jin et al. (2020) is the most relevant exploring how the aforementioned stability prop-
erties of gradient descent-ascent change with timescale separation. In particular, Jin et al. (2020)
investigate whether the desirable stability characteristics (stability of differential Nash equilibria)
and undesirable stability characteristics (stability of non-equilibrium critical points and instability
of differential Stackelberg equilibria) of gradient descent without timescale separation are main-
17
Published as a conference paper at ICLR 2021
tained and remedied, respectively with timescale separation. In terms of the former query, extending
the examples shown in Mazumdar et al. (2020) and Daskalakis & Panageas (2018), Jin et al. (2020)
show that differential Nash equilibrium are stable for gradient descent-ascent with any amount of
timescale separation.
On the other hand, for the latter query, Jin et al. (2020) shows (in Proposition 27) two interesting
examples: (a) for an a priori fixed τ , there exists a game with a differential Stackelberg equilibrium
that is not stable and (b) for an a priori fixed τ , there exists a game with a stable critical point that
is not a differential Stackelberg equilibrium. However, (a) does not imply that for the constructed
game, there does not exist another (finite) τ —independent of the game parameters—such the differ-
ential Stackelebrg equilibrium is stable for all larger τ. In simple language, the result summarized in
(a) says the following: if a bad timescale separation is chosen, then convergence may not be guar-
anteed. Similarly, (b) does not imply that there is no τ such that for all larger τ for the constructed
game instance, the critical point becomes unstable. Again, in simple language, the result summa-
rized in (b) says the following: if a bad timescale separation is chosen, then non-game theoretically
meaningful equilibria may persist. While at first glance this set of results may appear to indicate
that the undesirable stability characteristics of gradient descent without timescale separation cannot
be averted by any finite timescale separation, it is important to emphasize that these results do not
answer the questions of whether there (a) exists a game with a critical point that is not a differential
Stackelberg equilibrium which is stable with respect to gradient descent-ascent without timescale
separation and remains stable for all finite timescale separation ratios or (b) exists a game with a
differential Stackelberg equilibrium that is not stable for all finite timescale separation ratios. The
preceding questions are left open from previous work and are exactly the focus of this paper. In
Appendix J, we go into greater detail on the comparison between Proposition 27 of Jin et al. (2020)
as we believe this to be an important point of distinction between Theorem 1 and 2 in this paper.
Finally, Jin et al. (2020) study GDA with a timescale separation approaching infinity and show that
the stable critical points of gradient descent-ascent coincide with the set of differential Stackelberg
equilibria in this regime. This result effectively shows that gradient descent-ascent can guarantee
only equilibrium convergence with timescale separation, albeit only when it becomes arbitrarily
large. We remark that an equivalent result in the context of general singularly perturbed systems
has been known in the literature (Kokotovic et al., 1986, Chap. 2) as we discuss further in Section I.
Finally, we point out that since a timescale separation approaching infinity does not result in an
implementable algorithm, fully understanding the behavior with a finite timescale separation is of
fundamental importance and the motivation for our work.
Beyond the work of Jin et al. (2020) considering timescale separation in gradient descent-ascent, itis
worth mentioning the work of Chasnov et al. (2019) and Heusel et al. (2017). Chasnov et al. (2019)
study the impact of timescale separation on gradient descent-ascent, but focus on the convergence
rate as a function of it given an initialization around a differential Nash equilibrium and do not
consider the stability questions examined in this paper. Heusel et al. (2017) study stochastic gradient
descent-ascent with timescale separation and invoke the results of Borkar (2008) for analysis. The
stochastic approximation results the claims rely on guarantee the convergence of the system locally
to a stable critical point. Consequently, the claim of convergence to differential Nash equilibria of
stochastic gradient descent-ascent given by Heusel et al. (2017) only holds given an initialization
in a local neighborhood around a differential Nash equilibrium. In this regard, the issue of the
local stability of the types of critical point is effectively assumed away and not considered. In
contrast, we are able to combine our stability results for gradient descent-ascent with timescale
separation together with the stochastic approximation theory of Borkar (2008) to guarantee local
convergence to a differential Stackelberg equilibrium in Section H. We remark that Heusel et al.
(2017) empirically demonstrate that timescale separation can significantly improve the performance
of gradient descent-ascent when training generative adversarial networks.
The final relevant line of work studying gradient descent-ascent is specific to generative adversarial
networks. The results from this literature develop assumptions relevant to generative adversarial
networks and then analyze the stability and convergence properties of gradient descent-ascent un-
der them (see, e.g., works by Daskalakis et al. (2018); Goodfellow et al. (2014); Mescheder et al.
(2018); Metz et al. (2017); Nagarajan & Kolter (2017)). Within this body of work, there has been
a significant amount of effort focusing on how the stability (and, hence, convergence properties)
of gradient descent-ascent in generative adversarial networks can be enhanced with regularization
methods. Nagarajan & Kolter (2017) show, under suitable assumptions, that gradient-based methods
18
Published as a conference paper at ICLR 2021
for training generative adversarial networks are locally convergent assuming the data distributions
are absolutely continuous. However, as observed by Mescheder et al. (2018), such assumptions not
only may not be satisfied by many practical generative adversarial network training scenarios such
as natural images, but it can often be the case that the data distribution is concentrated on a lower
dimensional manifold. The latter characteristic leads to nearly purely imaginary eigenvalues and
highly ill-condition problems. Mescheder et al. (2018) provide an explanation for observed insta-
bilities consequent of the true data distribution being concentrated on a lower dimensional manifold
using discriminator gradients orthogonal to the tangent space of the data manifold. Further, the au-
thors introduce regularization via gradient penalties that leads to convergence guarantees under less
restrictive assumptions than were previously known. In this paper, we further extend these results
to show that convergence to differential Stackelberg equilibria is guaranteed under a wide array of
hyperparameter configurations (i.e., learning rate ratio and regularization).
Bilinear Games. We would be remiss to not include a discussion of gradient methods applied
to an important class of zero-sum games: bilinear games. Bilinear games fall within a mea-
sure zero set of C2 games that do not possess the generic properties that at critical points x*,
det(D2f (x*)) = 0, det(S1( J(x*)) = 0 (see Fiez et al. 2020 for the genericity statements regarding
local minmax equilibria in zero-sum games). This means they need special treatment. For zero-sum
bilinear unconstrained games, however, the behavior of gradient descent-ascent is already known.
In particular, the zero point is always a center type equilibrium of the continuous time dynamics so
that the continuous time dynamics are recurrent. A forward Euler discretization of the continuous
time dynamics gives gradient descent-ascent and such dynamics will always diverge. The alternat-
ing gradient descent update introduced by Bailey et al. (2020) is one solution to this problem such
that the behavior of the continuous time dynamics is preserved under the discretization. This method
simply uses a different discretization scheme that respects the continuous time behavior. We also
note that due to the fact that zero-sum bilinear games do not admit equilibria with generic properties,
simply introducing regularization can remedy the problem. In particular, if the maximizing player’s
update is modified to include the derivative of -2 Ilyll2, then the local minmax (which is not strict)
becomes a strict local minmax for the regularized game. In this case, our results do apply.
Nonconvex-Concave Optimization. A final related line of work is on nonconvex-concave opti-
mization (Lin et al., 2020a;b; Lu et al., 2020; Nouiehed et al., 2019; Ostrovskii et al., 2020; Rafique
et al., 2018). The focus in this set of works (among many others on the topic) is on characterizing the
iteration complexity to stationary points, rather than stability and asymptotic convergence as in the
non-convex, non-concave zero-sum game setting. The primary relevance of work on this problem
is that a number of the algorithms rely on timescale separation and variations of gradient descent-
ascent. Moreover, the methods for obtaining fast convergence rates may be relevant to future work
attempting to characterize fast rates in the non-convex, non-concave setting after there is a more
fundamental understanding of the stability and asymptotic convergence.
A.2 Historical Perspective: Dynamical Systems and Control
The study of gradient descent-ascent dynamics with timescale separation between the minimizing
and maximizing players is closely related to that of singularly perturbed dynamical systems (Koko-
tovic et al., 1986). Such systems arise in classical control and dynamical systems in the context of
physical systems that either have multiple states which evolve on different timescales due to some
underlying immutable physical process or property, ora single dynamical system which evolves on a
sub-manifold of the larger state-space. For example, robot manipulators or end effectors often have
have slower mechanical dynamics than electrical dynamics. On the other hand, in electrical circuits
or mechanical systems, certain resistor-capacitor circuits or spring-mass systems have a state which
evolves subject to a constraint equation (Lagerstrom & Casten, 1972; Sastry & Desoer, 1981). Due
to their prevalence, singularly perturbed systems have been studied extensively with one of the out-
comes being a number of works on determining the range of perturbation parameters for which the
overall system is stable (Kokotovic et al., 1986; Saydy, 1996; Saydy et al., 1990). We exploit these
results and analysis techniques to develop novel results for learning in games. One of contributions
of this work is the introduction of the algebraic analysis techniques to the machine learning and
game theory communities. These tools open up new avenues for algorithm synthesis; we comment
on potential directions in the concluding discussion section.
19
Published as a conference paper at ICLR 2021
This being said, there are a couple key difference between the present setting and that of the classical
literature including the following:
1.	The perturbation parameter is no longer an immutable characteristic of the physical
system, but rather a hyperparameter subject to design. Indeed, in singular perturbation
theory, the typical dynamical system studied takes the form
X = gι (x,y) Ey = g2(x,y)	(6)
where is a small parameter that abstracts some physical characteristics of the state vari-
ables. On the other hand, in learning in games, the continuous time limiting dynamical
system of gradient descent-ascent for a zero-sum game defined by f ∈ C2(X × Y, R)
takes the form
X= -DIf(X,y) y = τD2f(x,y)	(7)
where the X-Player seeks to minimize f with respect to X and the y-player seeks to max-
imize f with respect to y, and τ is the ratio of learning rates (without loss of generality)
of the maximizing to the minimizing player. These learning rates—and hence the value of
τ —are hyperparameters subject to design in most machine learning and optimization ap-
plications. Another feature of (7) as compared to (6), is that the dynamics Di f are partial
derivatives of a function f, which leads to the second key difference.
2.	There is structure in the dynamical system that arises from gradient-play which re-
flects the underlying game theoretic interactions between players. This structure can
be exploited in obtaining convergence guarantees in machine learning and optimization ap-
plications of game theory. For instance, minmax optimization is analogous to a zero sum
game for which the local linearization of gradient descent-ascent dynamics has the structure
AB
J = -τB> -τC
where A = A> and C = C> and τ is the learning rate ratio or timescale separation
parameter. Such block matrices have very interesting properties. In particular, second order
optimality conditions for a minmax equilibrium correspond to positive definiteness of the
first Schur complement S1(J) = A - BC-1B> > 0, and of -C > 0 (Fiez et al., 2020).
This turns out to be keenly important for understanding convergence of gradient descent-
ascent. Furthermore, due to the structure of J, tools from the theory of block operators
(see, e.g., works by Lancaster & Tismenetsky (1985); Magnus (1988); Tretter (2008)) such
as the quadratic numerical range can be exploited (and combined with singular perturbation
theory) to understand the effects of hyperparameters such as τ (the learning rate ratio) and
regularization (which is common in applications such as generative adversarial networks)
on convergence.
B Mathematical and Game Theoretic Preliminaries
In this appendix section, we review mathematical and game theoretic preliminaries needed for the
technical details of the proofs. We also include some short technical lemmas from algebra that are
used in the proofs.
B.1	Game Theory
We now formally present the local equilibrium definitions (see BaSar & OlSder 1998) that We char-
acterize in the main body by gradient-based sufficient conditions as is typical in the literature.
Definition B.1 (Local Nash Equilibrium). The joint strategy X ∈ X is a local Nash equilibrium on
i∈I Ui ⊂ X, where Ui ⊆ Xi, if f(X1, X2) ≤ f(X01, X2), for all X01 ∈ U1 ⊂ X1 and f(X1, X2) ≥
f(X1, X02) for all X02 ∈ U2 ⊂ X2. Furthermore, if the inequalities are strict, we say X is a strict local
Nash equilibrium.
Definition B.2 (Local Stackelberg Equilibrium). Consider Ui ⊂ Xi fori = 1, 2 where, without loss
of generality, player 1 is the leader (minimizing player) and player 2 is the follower (maximizing
Player). The strategy x； ∈ Ui is a local Stackelberg solution for the leader if, ∀xi ∈ Ui,
SuPx2∈r% (χ* ) f (X1, X2) ≤ SUPx2∈ru2 (xι) f (X 1, x2 ),
20
Published as a conference paper at ICLR 2021
where rU2 (x1) = {y ∈ U2|f(x1, y) ≥ f(x1, x2), ∀x2 ∈ U2} is the reaction curve. Moreover, for
any x2i ∈ ru (x；), the joint strategy profile (x；, xg) ∈ Ui X U is a local Stackelberg equilibrium
on U1 × U2.
While characterizing existence of equilibria is outside the scope of this work, we remark that Nash
equilibria exist for convex costs on compact and convex strategy spaces and Stackelberg equilibria
exist on compact strategy spaces (BaSar & Olsder, 1998, Thm. 4.3, Thm. 4.8, & Sec. 4.9). Exis-
tence of local equilibria is guaranteed if the neighborhoods and cost functions restricted to those
neighborhoods satisfy the assumptions of the cited results. The differential characterization of local
Nash equilibria in continuous games was first reported in (Ratliff et al., 2013). Genericity and struc-
tural stability we studied in general-sum settings in (Ratliff et al., 2014) and in zero-sum settings in
(Mazumdar & Ratliff, 2019). Fiez et al. (2020); Jin et al. (2020) present sufficient conditions for a
local Stackelberg equilibria and Fiez et al. (2020) studied the genericity and structural stability.
B.2	Dynamical Systems Theory
Recall the following equivalent characterizations of stability for an equilibrium of X = -g(x) in
terms of the Jacobian matrix J(x) = Dg(x).
Theorem B.1 (Theorem 4.15, Khalil 2002). Consider a critical point x； of g(x). The follow-
ing are equivalent: (a) x； is a locally exponentially Stable equilibrium of X = -g(x); (b)
spec(-J(x；)) ⊂ C-; (c) there exists a Symmetric positive-definite matrix P = P> > 0 such
that P J(x；) + J(x；)>P > 0.
Leveraging Linearization to Infer Qualitative Properties. The Hartman-Grobman theorem as-
serts that it is possible to continuously deform all trajectories ofa nonlinear system onto trajectories
of the linearization at a fixed point of the nonlinear system. Informally, the theorem states that if the
linearization of the nonlinear dynamical system X = F(x) around a fixed point X——i.e., F(x) = 0——
has no zero or purely imaginary eigenvalues, then there exists a neighborhood U of X and a home-
omorphism h : U → Rn——i.e., h, h-i ∈ C (U, Rn)——taking trajectories of X = F (x) and mapping
them onto those of Z = DF(x)z. In particular, h(x) = 0.
Given a dynamical system X = F(x), the state or solution of the system at time t starting from X at
time t0 is called the flow and is denoted φt(x).
Theorem B.2 (Hartman-Grobman: Theorem 7.3, Sastry 1999; Theorem 9.9, Teschl 2000). Con-
sider the n-dimensional dynamical system X = F(x) with equilibrium point X. If DF(x) has no
zero or purely imaginary eigenvalues, there is a homeomorphism h defined on a neighborhood U
of X taking orbits of the flow φt to those of the linear flow etDF(x) of X = F (x) —that is, the flows
are topologically conjugate. The homeomorphism preserves the sense of the orbits and is chosen to
preserve parameterization by time.
The above theorem says that the qualitative properties of the nonlinear system X = F(x) in the
vicinity (which is determined by the neighborhood U) of an isolated equilibrium X are determined
by its linearization if the linearization has no eigenvalues on the imaginary axes in the complex
plane. We also remark that Hartman-Grobman can also be applied to discrete time maps (Sastry,
1999, Thm. 2.18) with the same qualitative outcome.
Limiting dynamical systems and connections to singular perturbation theory. The continu-
ous time dynamical system takes the form X = -A「g(x) due to the timescale separation τ. Such a
system is known as a singularly perturbed system or a multi-timescale system in the dynamical sys-
tems theory literature (Kokotovic et al., 1986), particularly where τ-1 is small. Singularly perturbed
systems are classically expressed as
X =	-Difi (x,z)
Ez =	-D2f2(x,z)
(8)
where E = τ-i is most often a physically meaningful quantity inherent to some dynamical system
that describes the evolution of some physical phenomena; e.g., in circuits itmaybea constant related
to device material properties, and in communication networks, it is often the speed at which data
flows through a physical medium such as cable. This brings up to one key point of separation in
21
Published as a conference paper at ICLR 2021
applying dynamical systems theory to the study of algorithms versus physical system dynamics:
is no longer necessarily is a physical quantity but is most often a hyper-parameter subject to design.
Internally Chain Transitivity. In proving results for stochastic gradient descent-ascent, we lever-
age what is known as the ordinary differential equation method in which the flow of the limiting
continuous time system starting at sample points from the stochastic updates of the players actions
is compared to asymptotic psuedo-trajectories—i.e., linear interpolations between sample points.
To understand stability in the stochastic case, we need the notion of internally chain transitive sets.
For more detail, the reader is referred to (Alongi & Nelson, 2007, Chap. 2-3).
A closed set U ⊂ Rm is an invariant set for a differential equation X = F(x) if any trajectory x(t)
with x(0) ∈ U satisfies x(t) ∈ U for all t ∈ R. Let φt be a flow on a metric space (X, d). Given
ε > 0, T > 0 and x, y ∈ X, an (ε, T)-chain from x to y with respect to φt and d is a pair of
finite sequences x = x0, x1, . . . , xk-1, xk = y in X and t0, . . . , tk-1 in [T, ∞), denoted together
by (x0, x1, . . . , xk-1, xk; t0, . . . , tk-1), such that d(φti (xi), xi+1) < ε for i = 0, 1, 2, . . . , k - 1.
A set U ⊆ X is (internally) chain transitive with respect to φt if U is a non-empty closed invariant
set with respect to φt such that for each x, y ∈ U, > 0 and T > 0 there exists an (ε, T )-chain
from x to y. A compact invariant set U is invariantly connected if it cannot be decomposed into two
disjoint closed nonempty invariant sets. It is easy to see that every internally chain transitive set is
invariantly connected.
B.3	Tools for Convergence Analysis
The following proposition is a well-known result in numerical analysis and can be found in a number
of books and papers on the subject. Essentially, it provides an asymptotic convergence guarantee for
a discrete time update process or dynamical system.
Proposition B.1 (Ostrowski’s Theorem Argyros 1999; Theorem 10.1.2, Ortega & Rheinboldt
1970). Let x* be a fixed point for the discrete dynamical system Xk+ι = F (Xk). Ifthe spectral
radius of the Jacobian satisfies P(DF(x*)) < 1, then F is a contraction at x* and hence, x* is
asymptotically stable.
We analyze the iteration complexity or local asymptotic rate of convergence of learning rules of
the form xk+1 = h(xk) in the neighborhood of an equilibrium. Given two real valued functions
F(k) and G(k), we write F(k) = O(G(k)) if there exists a positive constant c > 0 such that
|F (k)| ≤ c|G(k)|. For example, consider iterates generated by xk+1 = h(xk) with initial condition
x0 and critical point x*. Then, if F(k) = kxk+1 - x* k ≤ Mk kx0 - x* k, we write F(k) = O(Mk)
where c = kx0 - x* k.
B.4	Numerical and Quadratic Numerical Range.
The numerical range and quadratic numerical range of a block operator matrix are particularly use-
ful for proving results about the spectrum of a block operator matrix as they are supersets of the
spectrum (Tretter, 2008). Given a matrix A ∈ Rn×n, the numerical range is defined by
W(A)={z∈Cn: hAz,zi, kzk = 1},
and is a convex subset of C. Define spaces Wi = {z ∈ Cni : kzk = 1} for each i ∈ {1, 2}.
Consider a block operator
A A11 A12
A21 A22 ,
where Aii ∈ Rni×ni and Aij ∈ Rni×nj for each i, j ∈ {1, 2}. Given v ∈ W1 and w ∈ W2, let
Av,w ∈ C2×2 be defined by
Av,w = hA11v, vi	hA12w, vi
hA21v, wi hA22w, wi .
The quadratic numerical range of A is defined by
W2(A) =	[	spec(Av,w)
v∈W1,w∈W2
22
Published as a conference paper at ICLR 2021
where spec(∙) denotes the spectrum of its argument.
The quadratic numerical range can be described as the set of solutions of the characteristic polyno-
mial
λ2 - λ(hA11v, vi + hA22w, wi) + hA11v, vihA22w, wi - hA12v, wihA21w, vi = 0	(9)
for V ∈ Wi and W ∈ W2. We use the notation(Av, Wi = V>Aw to denote the inner product. Note
that W2 (A) is a (potentially non-convex) subset of W(A) and contains spec(A).
B.5	Technical Lemmas
In this appendix, we present a handful of technical lemmas and review some additional mathematical
preliminaries excluded from the main body but which are important in proving the results in the
paper.
The following technical lemma is used in proving an upper bound on the spectral radius of the
linearization of the discrete time update τ -GDA a requirement for obtaining the convergence rate
results.
Lemma B.1. Thefunction c(z) = (1 一 z)1/2 + 4 一 (1 一 ∣)1/2 satisfies c(x) ≤ 0 forall Z ∈ [0,1].
Proof. Since c(0) = 0 and c(1) = 4 一 √12 ≤ 0, we simply need to show that c0(z) ≤ 0 on (0,1)
to get that c(z) is a decreasing function on [0, 1], and hence negative on [0, 1]. Indeed, c0(z) =
1 + 2√4i-2z 一 2√-z ≤ 0 since (I 一 z)-1/2 一(4 一 2z)-1∕2 ≥ 1/2 for a∏ Z ∈ (0, 1).	□
The following technical lemma, due to Mustafa & Davidson (1994), is used in constructing the finite
learning rate ratio.
Lemma B.2 (Lemma 15, Mustafa & Davidson 1994). Let V, Z ∈ Rp×p, W ∈ Rp×q andY ∈ Rq×q.
IfV and Y 一 XV -1W are non-singular, then
det (VX Z W) = det(V) det(Y-XVTW) det(I+VT(I +W(Y-XVTW)-1XVT)Z)
For completeness (and because there is a typo in the original manuscript), we provide the proof here.
Proof. Suppose that V and Y 一 XV-1W are non-singular so that the partial Schur decomposition
V W V	0	I V-1W
X Y_| = [x Y 一 XV-1w] [0	I
holds, and
det	XV WY	= det(V) det(Y 一 XV-1W).
Further,
一 V W IT= ΓI -V TW ]『	V T	0
X Y_|	= [0	I	[—(Y - XVTW)-1XVT	(Y 一 XVTW)-i
(10)
Applying the determinant operator, we have that
det
V+Z W
det
VW
XY
det
I0
0I
-1
Z0
00
(11)
X
Y
+
V
X
W
Y
so that
det	I0
VW
XY
-1
Z0
00
det(V-1(I + W(Y 一 XV-1W)-1XV-1)Z + I).
0
I
+
(12)
Combining (10) with (12) in (11) gives exactly the claimed result.
□
23
Published as a conference paper at ICLR 2021
The following lemma is Theorem 2 Lancaster & Tismenetsky (1985, Chap. 13.1). We use this
lemma several times in the proofs of Theorem 1 and 2 so we include it here for ease of reference.
For a given matrix A, υ+(A), υ- (A), and ζ(A) are the number of eigenvalues of the argument that
have positive, negative and zero real parts, respectively.
Lemma B.3. Consider a matrix A ∈ Rn×n.
(a)	If P is a symmetric matrix such that AP + PA> = Q where Q = Q> > 0, then P is
nonsingular and P and A have the same inertia, meaning that
υ+(A)=υ+(P), υ-(A)=υ-(P), ζ(A)=ζ(P).	(13)
(b)	On the other hand, if ζ(A) = 0, then there exists a matrix P = P > and a matrix Q =
Q> > 0 such that AP + PA> = Q and P and A have the same inertia ((13) holds).
C Proof of Theorem 1： Stability of T-GDA
To prove Theorem 1 and Corollary C.1, we introduce some techniques that are arguably new to the
machine learning and artificial intelligence communities. The first is the notion of a guard map. A
guard map can be used to provide a certificate of a particular behavior for a dynamical system as a
parameter(s) varies. A critical point of a dynamical systems is known to be stable if the spectrum
of the Jacobian at the critical point lies in the open left-half complex plane, denoted C-. Hence, We
construct a guard map as a function of T and show that it guards C-. Specifically we show that the
existence of a T* ∈ (0, ∞) such that V(T*) = 0 and V(T) = 0 for all T ∈ (T*, ∞) is equivalent to
Sι( J(x*)) > 0 and -D2f (x*) > 0 where
S1(J(x*)) = S1(Jτ(x*)) =D12f(x*) - D12f(x*)(D22f(x*))-1D21f(x*).
Towards this end, we need to introduced some notation as well as formal definitions for important
concepts such as the guard map.
C.1 Notation and Preliminaries
Given a matrix A ∈ Rn1 ×n2, let vec(A) ∈ Rn1n2 be the vectorization of A. We use the convention
that rows are transposed and stacked in order. That is,
—aι —	a；
vec :	..	7→	..
..
— an1 —	an>1
Let 0 and ㊉ denote the Kronecker product and Kronecker sum respectively. Recall that A ㊉ B =
A 01 +10 B. A less common operator, we define 田 as an operator that generates an ；n(n + 1) X
2n(n + 1) matrix from a matrix A ∈ Rn×n such that
A 田 A = H+(A ㊉ A)Hn
where Hn+ = (Hn>Hn)-1Hn> is the (left) pseudo-inverse of Hn, a full column rank duplication
matrix. A duplication matrix Hn ∈ Rn ×n(n+1)/2 is a clever linear algebra tool for mapping a
n (n +1) vector to a n2 vector generated by applying vec(∙) toa symmetric matrix and it is designed
to respect the vectorization map vec(∙). In particular, if Vech(X) is the half-vectorization map of
any symmetric matrix X ∈ Rn×n, then vec(X) = Hn vech(X) and vech(X) = Hn+ vec(X).
Given a square matrix A, let λ+max(A) be the largest positive real eigenvalue ofA and if A does not
have a positive real eigenvalue then it is zero.
Guardian maps. The use of guardian maps for studying stability of parameterized families of
dynamical systems was arguably introduced by Saydy et al. (1990). Guardian or guard maps act as
a certificate for a performance criteria such as stability.
Formally, let X be the set of all n × n real matrices or the set of all polynomials of degree n with
real coefficients. Consider S an open subset of X with closure S and boundary ∂S.
24
Published as a conference paper at ICLR 2021
Definition C.1. The map V : X → C is said to be a guardian map for S if for all X ∈ S, V(X)=
0 ^⇒ x ∈ ∂S.
Consider an open subset Ω of the complex plane that is symmetric with respect to the real axis.
Then, elements of S(Ω) = {A ∈ Rn×n : SPec(A) ⊂ Ω} are said to be stable relative to Ω.
The following result gives a necessary and sufficient condition for stability of parameterized families
of matrices relative to some open subset of the complex plane.
Proposition C.1 (Proposition 1 (Saydy et al., 1990); Theorem 2 (Abed et al., 1990)). Consider U
to be a pathwise connected subset ofR and A(τ) ∈ Rn×n a matrix which depends continuously on
τ. Let S (Ω) be guarded by the map V. The family {A(τ) : T ∈ U} is Stable relative to Ω if and
only if (i) it is nominally stable一meaning A(τι) ∈ S(Ω) for some τι ∈ U一and (ii) V(A(T)) = 0
for all τ ∈ U.
In proving Theorem 1, we define a guard map for the space of n × n Hurwitz stable matrices which
is denoted by S(C-).
Lemma C.1. The map V : A 7→ det(A	A) guards the set of non-singular n × n Hurwitz stable
matrices S (C-).
Proof. This follows from the following observation: for A ∈ Rn×n ,
Vech(AX + XA>) = H+vec(AX + XA>) = H+(A ㊉ A)Vec(X) = H+(A ㊉ A)HnVech(X)
from which it can be shown that the eigenvalues of A A are λi + λj for 1 ≤ j ≤ i ≤ n where λi
for i = 1, . . . , n are the eigenvalues of A.
Indeed, let S be a non-singular matrix such that S-1AS = M where M is upper triangular with
λι,...,λn on its diagonal. Observe that for any n X n matrix P, HnHn (P 0 P)Hn, = (P 0 P)Hn
and H+(P 0 P )HnH+ = Hn (P0P). Hence, using properties of the Kronecker product (namely,
that (A1 0 A2)(B1 0 B2) = (A1B1 0 A2B2)), we have that
Hnn(S-1 0S-1)HnHnn(I0A+A0I)HnHnn(S0S)Hn = Hnn(I0M+M0I)Hn
so that the spectrum of Hnn(I 0 A + A 0 I)Hn and Hnn(I 0 M + M 0 I)Hn coincide. Now, since
M is upper triangular, Hnn(I 0 M + M 0 I)Hn is upper triangular with diagonal elements λi + λj
(1 ≤ j ≤ i ≤ n) which can be verified by direct computation and using the definition of Hn . This
implies that λ% + λj (1 ≤ j ≤ i ≤ n) are exactly the eigenvalues of Hn(10 A + A 0 I)Hn.	□
We note that there are several other guard maps for the space of Hurwtiz stable matrices including
v : A → det(A ㊉ A). To give some intuition for this map, it is fairly straightforward to see that
the Kronecker sum A ㊉ A = A 0 I + I 0 A has spectrum {λj + λi} where λ%, λj ∈ SPec(A).
The operator A 田 A is simply a more computationally efficient expression of A ㊉ A, and as such
the eigenvalues of A 田 A are those of A ㊉ A removing redundancies. We use A 田 A specifically
because of its computational advantages in computing T*.
C.2 Proof of Theorem 1
We first prove that if X* is a differential Stackelberg equilibrium (that is, S1(Jτ(X*)) > 0 and
-D22f(X*) > 0), then there exists a finite T* ∈ (0, ∞) such that for all T ∈ (T*, ∞), X* is locally
exponentially stable for X = -Λτg(χ) (that is, spec(-Jτ(x*)) ⊂ C-). Towards this end, We
construct a guard map for the space of n × n Hurwtiz stable matrices and explicitly construct the T *
using it.
Then we prove the other direction. That is, if there exists a finite T* ∈ (0, ∞) such that for all
τ ∈ (τ*, ∞), x* is exponentially stable for X = -Λτg(χ), then x* is a differential Stackelberg
equilibrium. We prove this by contradiction.
C.2.1 PROOF THAT IF x* IS A DIFFERENTIAL STACKELBERG THEN FINITE T* EXISTS
J ( *)= -D12f(x*)	-D12f(x*)
-Jτ (x ) = TD1>2f(x*)	TD22f(x*)
For a critical point x* , let
A11 A12
-T A1>2	TA22
25
Published as a conference paper at ICLR 2021
and define
Si = Sι(-Jτ(x*)) = A11 - A12A-21A>2.
Note that this is equivalent to the first SchUr complement of -J(x*) (i.e., when T = 1) since the
T and τ-1 cancel, and by assumption the first Schur complement of -J(x*) is positive definite.
Suppose that x* is a differential Stackelberg equilibrium so that -Si > 0 and -A-- > 0.
Polynomial guard map with family of matrices parameterized by T. By Lemma C.1, ν : A 7→
det( A田 A) is a guard map for S (C-). Indeed, using the fact that the determinant is the product of the
eigenvalues of a matrix and the fact that spec(AA) = {λi+λj, 1 ≤ i ≤ j ≤ n, λi, λj ∈ spec(A)},
we have that
det(A	A) = Y (λi+λj) = Y 2Re(λi)(4Re-(λi)+4Im-(λi))	Y (λi+λj).
i≤j ≤i≤n	i≤i≤n	1<i<j<n:
λi 6=λj
Hence, consider S(C-), det(A田 A) = 0 if and only if Affl A is singular if and only if A has a purely
imaginary eigenvalue——that is, if and only if A ∈ ∂ S (C-).5 Now, consider the parameterized family
of matrices -Jτ(x*), parameterized by T. By an abuse of notation, let ν(T) = det(-Jτ (x*) ffl
-JT (x*)). Ifwe consider the subset of this family of matrices that lies in S (C-) (this subset could a
priori be empty thought we show itis not), then for any T such that -Jτ (x*) is in this subset, we have
that V(T) = 0 if and only if - Jτ(x*)田(一Jτ(x*)) is singular if and only if - Jτ(x*) ∈ ∂S(C-).
Hence, V(T) = det(-Jτ(x*)田-J「(x*)) guards S(C-).
In particular, if we envision -Jτ (x*) as the input to ν : A 7→ det(A ffl A) and simply vary T
(holding all the entries of -Jτ(x*) otherwise fixed), then V : T 7→ det(-Jτ (x*) ffl (-Jτ (x*)))
can be thought of simply as a function of T which guards the set of Hurwitz stable matrices via the
reasoning describe above. Indeed, by slightly overloading the notation for V,
V(T) := V0 + ViT +-----+ Vp-iTPT + VpTP = V(-Jτ(x*))
Hence, for intuition, observe that as T decreases (towards zero) stability is first lost when at least
one eigenvalue of -Jτ(x*) reaches the imaginary axis, at which point V(T) = 0.
There are two cases to consider:
Case 1:	V(T) is an identically zero polynomial. In this case, -Jτ(x*) is in the interior of the
complement of the set of Hurwitz stable matrices for all values of T > 0——that is,
-JT(x*) ∈ int(Sc(C-)) for all T ∈ R+ = (0, ∞).
Case 2:	V(T) is not an identically zero polynomial. In this case, V(T) has finitely many zeros. If
V (t) has no positive real roots, then as T varies in R+, -JT (x*) does not cross ∂S(C-——
i.e., the boundary of the space of n × n Hurwitz stable matrices. Hence, {-JT(x*) :
t ∈ R+} ⊂ Sc(C-) or {-Jτ(x*) : t ∈ R+} ⊂ int(Sc(C-)). It suffices to check
-JT(x*) ∈ Sc(C-) or -JT(x*) ∈ int(Sc(C-)) for an arbitrary T ∈ R+.
On the other hand, if v(t) has ' ≥ 1 real positive zeros, say 0 < Ti < •…< t` = T*, then
by Proposition C.1, - JT(x*) ∈ S(C-) for all τ > τ* if and only if - J「(x*) ∈ S(C-)
for arbitrarily chosen τ > τ*. We choose the largest positive root t` because we are
guaranteed that V(T) stops changing sign for T > T*. Further, the largest neighborhood in
R+ for which -JT(x*) ∈ S(C-) is (t`, ∞).
Recall that we have assumed that x* is a differential Stackelberg equilibrium (that is, Si > 0 and
-A-- > 0). We will show next (by way of explicit construction ofT*) that we are always in case 2.
Construction of T*. We note that there are more elegant, simpler constructions, but to our knowl-
edge this construction gives the tightest bound on the range of T for which -JT (x*) is guarnateed
to be Hurwitz stable. Recall that
-JT (x* ) =
-Di-f(x*)
TDi>-f(x*)
-Di- f (x*)	Aii	Ai-
TD--f(x*)	-T Ai>- TA--
5Indeed, this holds since the only scenarios in which det(A ffl A) = 0 are such that the eigenvalues of A do
not lie in S(C-).
26
Published as a conference paper at ICLR 2021
and
S1 = A11 - A12A2-21A1>2.
Let Im denote the m × m identity matrix for some m.
Claim C.1. Thefinite learning rate ratio is T* =》m&x(Q) where
C — —	1 f Δ G 4-1、订	(-	G 4-1 4>、订 A A22 Hn (A 12 X In 2 )	_	(- 个 d-八
Q = 2	[(A12 % A22 )Hn2	(InI	% A22 A12)HnJ	-g-1 H + (SI % /修4-')	-(AII 脸 A22 )
n1	(14)
with A22 = A22 田 A22 and S1 = S1 田 S3
Proof. Recall that V(T) =det(-Jτ(x*)田(一J「(x*))) is a guard map for S(C-).
We apply basic properties of the Kronecker product and sum as well as Schur’s determinant formula
to obtain a reduced form of the guard map. To this end, we have that
「 Au 田 A”	2H+1(Inι 0 A12)	0	-
-JT(X*) 田(一 Jτ(x*)) = T(Ini 0 (-A>2))Hnι	A11 ㊉ τA22	(A12 0 In2)Hn2
0	2T Hn+2 (-A1>2 % In2 )	T (A22 A22)
Now, we apply Schur’s determinant formula to get that
V(T ) = τ n2(n2 + 1)/2 det(A22 田 A22)det (J, (In” 田A⅛)Hnι AHlb k +4MJ(15)
where
M1 = -2Hn+2(-A1>20In2)(A22A22)-1(A120In2)Hn2
From here, we apply Lemma B.2 to further reduce the guard map. First, note that
Au ㊉ τA22 = A11 0 In2 + Ini 0 τA22.
Let V =	In1	0 TA22, Z =	A11	0	In2	+ M1,	Y = A11	A11,	W = -T (In1	0 A1>2)Hn1, and
X = 2Hn+i (Ini 0 A12). Using the two properties of the Kronecker product (B1 0 B2)(B3 0 B4) =
(B1B3 0 B2B4) and (B1 0 B2)-1 = (B1-1 0 B2-1), we have that
Y -XV-1W =A11A11+ 2Hn+i(Ini 0A12)(Ini 0A22)-1(Ini 0A1>2)Hni	(16)
=A11A11+ 2Hn+i(Ini 0A12A2-21A1>2)Hni	(17)
=A11A11+ Hn+i((Ini 0A12A2-21A1>2)+ (A12A2-21A1>20Ini))Hni	(18)
= S1	S1	(19)
where (18) holds since Hn+i (Ini 0 A12A2-21A1>2)Hni = Hn+i (A12A2-21A1>2 0 Ini)Hni. Now, define
V-1 + V-1W(Y - XV -1W)-1XV -1 = T-1M2 where
M2 = Ini 0 A2-21 - 2(Ini 0 A2-21A1>2)Hni (S1	S1)-1Hn+i (Ini 0 A12A2-21)
so that applying Lemma B.2 we have
V(T) = Tn2(n2+1)/2det(A22A22)det(S1S1)det(Ini0A22)det(TInin2+M2(A110In2+M1))
(20)
The assumptions that S1 > 0 and -A22 > 0 together imply that det(S1 S1) 6= 0 and det(Ini 0
A22)	6=	0. Hence,	V(T)	= 0 if and only if det(TIni n2	+ M2(A11	0	In2	+ M1))	= 0 since
0 < T < ∞. The determinant expression is exactly an eigenvalue problem.
Since by assumption the Schur complement of J(x*) and the individual Hessian -D22f(x*) are
positive definite (that is, x* is a differential Stackelberg equilibrium), Thus, the largest positive real
root of V (T ) = 0 is
T* = λ+max(-M2(A11 0 In2 + M1))
where X«&乂(・)is the largest positive real eigenvalue of its argument if one exists and otherwise its
zero. Using properties of the Kronecker product and duplication matrices, it can easily be seen that
Q, as defined in (14), is equivalent to -M2(A11 0 In? + M，	□
The result of this claim concludes the proof that if x* is a differential Stackelberg, then there exists
a finite T* ∈ [0, ∞) such that for all T ∈ (τ*, ∞), spec(-Jτ(x*)) ⊂ C-.
27
Published as a conference paper at ICLR 2021
C.2.2 PROOF THAT EXISTENCE OF FINITE T* IMPLIES THAT x* IS A DIFFERENTIAL
Stackelberg
To begin, consider a critical point x* such that g(x*) = 0 and det(D22f(x*)) 6= 0. Then,
det(-Jτ (x*)) = τn2 det(D22 f (x*)) det(-S1 (J (x*))) so that det(-Jτ (x*)) = 0 if and only
if det(-S1(J(x*))) = 0 which implies -Jτ (x*) is unstable for all τ ∈ (0, ∞) when
det(-S1 (J(x*))) = 0. As a result, we are left to consider when det(S1 (J(x*))) 6= 0 for the
remainder of the proof.
We proceed by arguing a contradiction. Let -C ≡ -D22f(x*) and S1 ≡ S1(J(x*)) = D12f(x*) -
D12f(x*)(D22f(x*))-1D1>2f(x*) have no zero eigenvalues—that is, det(S1) 6= 0 and det(C) 6= 0.
Suppose that there exists a T* ∈ (0, ∞) such that for all T ∈ (T*, ∞), spec(-Jτ(x*)) ⊂ C-, yet
x* is not a differential Stackelberg equilibrium. That is, either -S1 or C have at least one positive
eigenvalue. Without loss of generality, let -S1 have at least one positive eigenvalue.
Since det(S1) 6= 0 and det(C) 6= 0, by Lemma B.3.b, there exists non-singular Hermitian matrices
P1, P2 and positive definite Hermitian matrices Q1, Q2 such that -S1P1 - P1S1 = Q1 and CP2 +
P2C = Q2. Further, -S1 and P1 have the same inertia, meaning
υ+(-S1) = υ+(P1), υ- (-S1) = υ- (P1), ζ(-S1) = ζ(P1)
where fora given matrix A, υ+(A), υ-(A), and ζ(A) are the number of eigenvalues of the argument
that have positive, negative and zero real parts, respectively. Similarly, C and P2 have the same
inertia:
υ+(C) = υ+(P2), υ-(C) = υ-(P2), ζ(C) = ζ(P2).
Since -S1 has at least one strictly positive eigenvalue, υ+(P1) = υ+(-S1) ≥ 1.
Define
P=	I	L0>	P1	0	I	0	(21)
P =	0 I	0	P2	L0	I	(21)
where L0 = (D22f(x*))-1D1>2f(x*) = CD1>2f(x*). Since P is congruent to blockdiag(P1, P2),
by Sylvester’s law of inertia (Horn & Johnson, 1985, Thm. 4.5.8), P and blockdiag(P1, P2) have the
same inertia, meaning that υ+(P) = υ+ (blockdiag(P1, P2)), υ- (P) = υ- (blockdiag(P1, P2)),
and ζ(P) = ζ (blockdiag(P1, P2)). Consider the matrix equation -PJτ(x*) - Jτ>(x*)P = Qτ for
-Jτ (x*) where
Qτ= I0 LI0> Bτ LI0 I0
with
B	Q1	P1D12f(x*) -S1L0>P2
Bτ = (P1D12f(x*) - S1L0>P2)> P2L0D12f(x*) + (P2L0D12f(x*))> + TQ2
which can be verified by straightforward calculations.
Observe that Qτ > 0 is equivalent to Bτ > 0 and both matrices are symmetric so that Bτ > 0 if
and only if Q1 > 0 and S2 (Bτ) > 0 where
S2(Bτ) = P2L0D12f(x*) + (P2L0D12f(x*))> + TQ2
- (P1D12f(x*) -S1L0>P2)>Q1-1(P1D12f(x*) - S1L0>P2).
Now, S2(Bτ) is also a real symmetric matrix, and hence, it is positive definite if and only if all its
eigenvalues are positive. To determine the range of T such that S2 (Bτ) is positive definite, we can
formulate an eigenvalue problem to determine the value of T such that the matrix S2 (Bτ) becomes
singular. This is analogous to the guard map approach used in the proof in the previous subsection
for the other direction of the proof, and in this case, we are varying T from zero to infinity and finding
the point such that for all larger T, S2 (Bτ) is positive definite. Intuitively, such an argument works
since T scales the positive definite matrix Q2 . Towards this end, consider the eigenvalue problem in
T given by
0 =det (τI - Q-I((PID12f(x*) - S1L>P2)>Q-1(P1D12f (x*) - SiL>P2)
-P2L0D12f(x*) -(P2L1D12f(x*))>.
28
Published as a conference paper at ICLR 2021
Let τ0 be the maximum positive eigenvalue, and zero otherwise. Then, since eigenvalues vary
continuously, for all τ ∈ (τ0 , ∞), Qτ > 0 so that by Lemma B.3.a we conclude that P and
-JT(x*) have the same inertia, but this contradicts the stability of -JT(x*) for all T ∈ (T*, ∞)
since υ+ (P ) ≥ 1.
Remark on the tightness of T*. The construction ofT* is tight in the following sense. While it is
possible to construct multiple guard maps for a domain, all guard maps have the same positive real
roots by definition (Saydy, 1996, Remark 2). Hence, independent of the guard map choice, we will
get the same value of T*. Moreover, T* tells us exactly when the eigenvalues move into the open
left-half complex plane C- and remain there. Hence, this gives us the precise, tight lower bound on
the value of T * .
C.3 T -GDA PROVABLE CONVERGES TO DIFFERENTIAL STACKELEBRG EQUILIBRIA
As a corollary to Theorem 1, we first show that the discrete time T -GDA update is locally asymptot-
ically stable for a range of learning rates γ1.
Corollary C.1 (Asymptotic convergence of T -GDA). Suppose the assumptions of Theorem 1 hold
so that x* is a critical points of g and S1 (J (x*)) and D22f2(x*) are non-singular. There exists a
τ* ∈ (0, ∞) such that T-GDA with γι ∈ (0,γ(τ)) where Y(T) = argmi□λ∈spec(jτ(χ*)) 2Re(λ)∕∣λ∣2
converges locally asymptotically for all T ∈ (T*, ∞) if and only if x* is a differential Stackelberg
equilibrium.
The proceeding corollary to Theorem 1 follows immediately from Lemma F.1 given in Appendix F.
Proof. Suppose that x* is a differential Stackelberg equilibrium so that by Theorem 1, there exists
a τ* ∈ (0, ∞) such that spec(-Jr(x*)) ⊂ C- for all T ∈ (T*, ∞). Now that We have a guarantee
that -JT (x*) is Hurwitz stable for any T ∈ (T*, ∞), we apply Hartman-Grobman to get that the
nonlinear system X = -Λτg(x) is stable in a neighborhood of x*. Fix any T ∈ (T*, ∞) and let
Y = argminλ∈spec(Jτ(x*)) 2Re(λ)∕∣λ∣2. Then, applying Lemma F.1, for any γι ∈ (0,γ), T-GDA
converges locally asymptotically to x*.
On the other hand, suppose that there exists a T* ∈ (0, ∞) such that spec(-Jr(x*)) ⊂ C- for all
T ∈ (T*, ∞). Then by Theorem 1, ifx* is a differential Stackelberg equilibrium. Furthermore, since
spec(-Jτ (x*)) ⊂ C- for all T ∈ (t*, ∞), if we let Y = argmi□λ∈spec(jτ(x*)) 2Re(λ)∕∣λ∣2, then
by Lemma F.1 T-GDA converges locally asymptotically to x* for any choice of γι ∈ (0, γ).	□
D	Proof of Theorem 2: Instability of T-gda
To begin, consider a critical point x* defined by g(x*) = 0 which is not a differ-
ential Stackelberg equilibria such that det(D22f(x*)) 6= 0. Then, det(-JT (x*)) =
Tn2 det(D22 f (x*)) det(-S1 (J (x*))) so that det(-JT (x*)) = 0 if and only if det(-S1 (J (x*))) =
0 which implies -JT(x*) is unstable for all T ∈ (T0, ∞) where T0 = 0 when det(-S1(J(x*))) = 0.
We now consider when det(S1(J(x*))) 6= 0 for the remainder of the proof.
Let x* be a stable critical point of 1-GDA (without loss of generality) which is not a differential
Stackelberg equilibrium. Without loss of generality, suppose that S1(-J(x*)) has at least one
strictly positive eigenvalue. Note that both S1 (-J(x*)) and -D22f(x*) are symmetric matrices
and hence, have purely real eigenvalues.
Since both S1(-J(x*)) andD22f(x*) have no zero valued eigenvalues, by Lemma B.3.b, there exists
non-singular Hermitian matrices P1, P2 and positive definite Hermitian matrices Q1, Q2 such that
S1(-J(x*))P1+P1S1(-J(x*)) = Q1andD22f(x*)P2+P2D22f(x*) = Q2.Further,S1(-J(x*))
and P1 have the same inertia, meaning
υ+(S1(-J(x*))) = υ+(P1), υ-(S1(-J(x*))) = υ-(P1), ζ(S1(-J(x*))) = ζ(P1)
where fora given matrix A, υ+(A), υ-(A), and ζ(A) are the number of eigenvalues of the argument
that have positive, negative and zero real parts, respectively. Similarly, D22f(x*) and P2 have the
same inertia:
υ+(D22f(x*)) =υ+(P2), υ-(D22f(x*))=υ-(P2), ζ(D22f(x*)) =ζ(P2).
29
Published as a conference paper at ICLR 2021
I0	LI0>
P
Recall that We assumed Sι(-J(x*)) has at least one eigenvalue with strictly positive real part.
Hence, υ+(Pι) = υ+(Sι(-J(x*))) ≥ 1.
Define
P1	0	I	0
0	P2	L0 I
where L0 = (D22f(x*))-1D1>2f(x*). Since P is congruent to blockdiag(P1, P2), by Sylvester’s
law of inertia (Horn & Johnson, 1985, Thm. 4.5.8), P and blockdiag(P1, P2) have the same inertia,
meaning that υ+ (P) = υ+ (blockdiag(P1, P2)), υ- (P) = υ- (blockdiag(P1, P2)), and ζ(P) =
ζ(blockdiag(P1, P2)). Consider now the Lyapunov equation -PJτ(x*) - Jτ> (x*)P = Qτ for
-Jτ (x* ) where
with
B	Q1	P1D12f(x*) +S1(-J(x*))L0>P2
Bτ = (P1D12f(x*) + S1(-J(x*))L0>P2)> P2L0D12f(x*) + (P2L0D12f(x*))> + τQ2
which can be verified by straightforward calculations.
Since υ+(P1) ≥ 1, we have that υ+(P) ≥ 1. Now, we find the value of τ0 such that for all τ > τ0,
QT > 0 so that, in turn, we can apply LemmaB.3.a, to conclude that spec(-Jr (x*)) ⊂ C-. Indeed,
observe that Qτ > 0 is equivalent to Bτ > 0 and both matrices are symmetric so that Bτ > 0 if and
only if Q1 > 0 and S2 (Bτ) > 0 where
S2(Bτ) = P2L1D12f(x*) + (P2L1D12f(x*))> + τQ2
-	(P1D12f(x*)+S1(-J(x*))L0>P2)>Q1-1(P1D12f(x*)+S1(-J(x*))L0>P2).
Now, S2 (Bτ) is also a real symmetric matrix, and hence, it is positive definite if and only if all its
eigenvalues are positive. To determine the range of τ for which Qτ > 0, we simply need to solve
the eigenvalue problem
0=det(τI-Q2-1((P1D12f(x*)+S1(-J(x*))L0>P2)>Q1-1(P1D12f(x*)+S1(-J(x*))L0>P2)
-	P2L0D12f(x*) - (P2L0D12f(x*))>)).
and extract the maximum eigenvalue, namely,
τ0 =λmax(Q2-1((P1D12f(x*)+S1(-J(x*))L0>P2)>Q1-1(P1D12f(x*)
+S1(-J(x*))L0>P2) - P2L0D12f(x*) - (P2L0D12f(x*))>)).
Hence, as noted previously, by Lemma B.3.a, we conclude that for all τ ∈	(τ0, ∞),
spec(-Jτ(x*)) ⊂ C-. This concludes the proof.
Additional context/intuition for the proof approach. To provide some context for the proof
approach, we remark that it follows the same idea as the proof of Theorem 1 in Appendix C.2.2.
Indeed, to determine the range of τ such that S2 (Bτ) is positive definite, we can formulate an
eigenvalue problem to determine the value of τ such that the matrix S2(Bτ) becomes singular. We
vary τ from zero to infinity in order to find the point such that for all larger τ, S2 (Bτ) is positive
definite. Intuitively, such an argument works since τ scales the positive definite matrix Q2 .
E	Proof of Theorem 3: Stability of T—gda in Regularized GANS
As in Mescheder et al. (2018), we only apply the regularization to the discriminator. In the following
proof, we use Vχ(∙) to denote the partial gradient with respect to X of the argument (∙) when the
argument is the discriminator D(∙; ω) in order prevent any confusion between the notation D(∙)
which we use elsewhere for derivatives.
To prove the first part of this result, we following similar arguments to Theorem 4.1 of (Mescheder
et al., 2018). To prove the second part, we leverage the concept of the quadratic numerical range.
30
Published as a conference paper at ICLR 2021
For both components of the proof, we will use the following form of the Jacobian of the regularized
game. Indeed, first observe that the structural form of J(τ,μ)(x*) is
J(TW)(X*)= -T(BT T (C + μR)	(22)
where B = D12f(χ*), C = -D2f (x*) and R = D^Rj(x*) where Rj is either gradient penalty
indexed by j = 1,2.6 This follows from Assumption 1-a., which implies that D(x; ω*) = 0 in some
neighborhood of SuPP(PD) and hence, VχD(x; ω*) = 0 and VXD(x; ω*) = 0 for X ∈ supp(pD).
In turn, we have that D12f(x*) = 0.
Proof that x* = (θ*,ω*) is a differential Stackelberg equilibrium. For any fixed μ ∈ (0, ∞),
then we first observe that x* is also a critical point of the unregularized dynamics. Indeed, by As-
sumption 1-a., D(x; ω*) = 0 in some neighborhood of suPP(pD) and hence, VxD(x; ω*) = 0 and
VχD(x; ω*) = 0 for x ∈ SuPP(PD). Further, D2Rj(θ, ω) = μEpi(χ) [D2(VχD(x; ω))VχD(x, ω)]
for j = 1, 2 where p1 (x) = pD (x) and p2 (x) = pθ (x). Thus, using the above observation that
VxD(x; ω*) = 0, we have that D2Ri(θ*, ω*) = 0 for i = 1, 2 meaning that the derivative of the
regularizer with respect to ω is zero at x* = (θ*, ω*) which in turn implies that D1f(x*) = 0 and
-D2f(x*) = 0. Hence, x* is a critical point of the unregularized dynamics as claimed. Further,
C + μR > 0 which follows from Lemma D.5 in (Mescheder et al., 2018). From Lemma D.6 in
(Mescheder et al., 2018), due to Assumption 1-c., if v = 0 and V ∈ T⅛*Mg, then Bv = 0 which im-
plies that B can only be rank deficient on T⅛* Mg . Using this fact along with the structure of the Ja-
cobian as in (22), we have that the Schur complement of J(τ,μ) (x*) is equal to B >(C+μR)-1B > 0
since C + μR > 0. Hence, x* = (θ*,ω*) is a differential Stackelberg equilibrium.
Proof of stability. Examining (22), it is straightforward to see that the quadratic numerical range
W2( J(τ,μ)) has eigenvalues of the form
λτ,μ = 2(τ(c + μr)) ± 2P(-τ(C + μr))2 — 4τ|b|2
where b = hD12f(x*)v, wi, c = h-D22f (x*)w, wi and r = hD22Ri(x*)w, wi for vectors v ∈
W1 ∩ (Tθ* MG)⊥ and w ∈ W2 ∩ (Tω* MD)⊥ where U⊥ denotes the orthogonal complement of U.
We claim that for any value of μ ∈ (0, ∞) and any T ∈ (0, ∞), Re(λτ,μ) > 0. Indeed, we argue
this by considering the two possible cases: (1) (τ(c + μr))2 ≤ 4∣b∣2τ or (2) (τ(c + μr))2 > 4τ|b|2.
•	Case 1: Suppose that (τ(c + μr))2 ≤ 4∣b∣2τ. Then, Re(λτ,μ) = 1 (τ(c + μr)) > 0 trivially
since C + μr > 0.
•	Case 2: Suppose that (τ(c + μr))2 > 4τ|b|2. In this case, we want to ensure that
Re(λτ) > 1 (τ(c + μr)) - 1 p(-τ(C + μr))2 - 4τ|b|2 > 0.
which holds since
(τ(c + μr))2 > (-τ(c + μr))2 — 4τ|b|2 Q⇒ 0 > -4τ|b|2
This concludes the proof.
E.1 Necessary Conditions on GAN Architecture
The following proposition provides necessary conditions on the sizes of the network architectures
for the discriminator and generator network for stability.
Theorem A.7 of Mescheder et al. (2018) shows that matrices of the form
0	-B
B>	-C
(23)
are stable if B is full rank and C > 0. The following proposition provides necessary conditions on
the sizes of the network architectures for the discriminator and generator network for stability.
6Mescheder et al. (2018) imply that their results hold for a convex combination of the two gradient penalties,
which would in turn imply our results will hold in this case. However, we have not included the details here.
31
Published as a conference paper at ICLR 2021
Proposition E.1. Consider training a generative adversarial network via a zero-sum game with
generator network Gθ, discriminator network Dω, and loss f(θ, ω) with regularization Rj(θ, ω) (for
some j ∈ {1, 2}) such thatAssumption 1 is SatisfiedfOr an equilibrium x* = (θ*, ω*). Independent
of the learning rate ratio and the regularization parameter μ, for x* to be stable it is necessary
that the dimension of the discriminator network parameter vector is at least half as large as the
corresponding generator network parameter vector: n2 ≥ n1/2 where θ ∈ Rn1 and ω ∈ Rn2.
The intuition for the why this proposition should hold follows immediately from observing the struc-
ture of the Jacobian: for any matrix of the form (23), at least one eigenvalue will be purely imaginary
if n2 < n1/2 where B ∈ Rn1 ×n2 and C ∈ Rn2 ×n2. This proposition follows immediately from
observing the structure of the Jacobian: for any matrix of the form
0	-B
-J = B>	-C
at least one eigenvalue will be purely imaginary ifn2 < n1/2 where B ∈ Rn1 ×n2 and C ∈ Rn2×n2.
Indeed, by Lyapunov’s stability theorem for linear systems (Hespanha, 2018, Theorem 8.2), a matrix
A is Hurwitz stable if and only if for every symmetric positive definite Q = Q> > 0, there exists
a>>
unique symmetric positive definite P = P > 0, such that A P + P A = -Q. Hence, -J is
Hurwitz stable if and only if there exists a P = P > > 0 such that
P1	P2	0	B
P2> P3	-B> C
0<Q= B0>	-CB	PP2>1	PP23 +
-BP2> - P2B>	-BP3+P1B+P2C
B>P1 +CP2> - P3B>	B>P2 +CP3 +P2>B+P3C
Since this is a symmetric positive definite matrix, the block diagonal components must also be
symmetric positive definite so that -BP2 - P2B> > 0.7 Recall that B ∈ Rn1 ×n2 and P2 ∈
Rn2×n1. Hence, a necessary condition for this matrix to be positive definite is that n2 ≥ n1/2 for
-BP2 - P2 B> to have full rank; of course this is not sufficient, but it is necessary. It is easy to see
this argument is independent of whether a learning rate ratio τ 6= 0 or regularization is incorporated.
F Proof of Helper Lemmas and Theorem 4 for T-GDA Convergence
In this appendix section, we prove the following two lemmas. The proofs build on one another so
we state them jointly here. We then invoke them to prove Theorem 4.
Lemma F.1. Consider a zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr (X, R) for
some r ≥ 2. Suppose that x* is a differential Stackelberg equilibrium and that given τ > 0,
spec(-Jτ(x*)) ⊂ C-. Let Y = mi□λ∈spec(jτ(x*)) 2Re(λ)∕∣λ∣2. For any γι ∈ (0,γ), τ-GDA
converges locally asymptotically.
Lemma F.2. Consider a zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr(X, R) for some
r ≥ 2. Suppose that x* is a differential Stackelberg equilibrium and that given τ, spec(-Jτ (x*)) ⊂
C-. Let γ = minλ∈spec(Jτ(x*)) 2Re(λ)∕∣λ∣2, and λm = argminλ∈spec(j,(x*)) 2Re(λ)∕∣λ∣2. For
any α ∈ (0, γ), τ -GDA with learning rate γ1 = γ - α converges locally asymptotically at a rate of
O((I - 4ɑβ )k/2 ) where β = (2Re(Xm) - a|1m|2厂1.
F.1 Proof of Lemma F.1
Suppose that x* is a differential Stackelberg or Nash equilibrium and that 0 < τ < ∞ is such that
spec(-Jτ(x*)) ⊂ C-. For the discrete time dynamical system Xk+ι = Xk — γιΛτg(xk), it is well
known that ifγ1 is chosen such that ρ(I -γ1Jτ(x*)) < 1, then xk locally (exponentially) converges
to x* (ortega & Rheinboldt, 1970). With this in mind, we formulate an optimization problem to
find the upper bound γ on the learning rate γ1 such that for all γ1 ∈ (0, γ), the spectral radius of the
local linearization of the discrete time map is a contraction which is precisely ρ(I - γ1Jτ(x*)) < 1.
The optimization problem is given by
γ = min < γ :	max |1 — γλ∣ ≤ 1 '.	(24)
γ>0	λ∈spec(Jτ (x*))
7If a block matrix Q with block entries Qij for i, j ∈ {1, 2} is positive definite symmetric, then Qii > 0
for i = 1, 2.
32
Published as a conference paper at ICLR 2021
The intuition is as follows. The inner maximization problem is over a finite set SPec(JT(x*))=
{λι,..., λn} where JT(x*) ∈ Rn×n. As Y increases away from zero, each |1 - γλ∕ shrinks in
magnitude. The last λi such that 1 - γλi hits the boundary of the unit circle in the complex plane
(that is, |1 - γλi∣ = 1) gives US the optimal value of Y and the element of SPec(JT(x*)) that
achieves it. Examining the constraint, We have that for each λi, γ(γ∣λ∕2 - 2Re(λi)) ≤ 0 for any
γ > 0. As noted this constraint will be tight for one of the λ, in which case Y = 2Re(λ)∕∣λ∣2 since
γ > 0. Hence, by selecting Y = mi□λ∈spec(jτ(χ*)) 2Re(λ)∕∣λ∣2, We have that 11 - γιλ∣ < 1 for all
λ ∈ SPec(JT(x*)) and any Y1 ∈ (0, Y).
To see this is the case, let
λ∈spec(Jτ (x* )) "”囚2
and
λm = arg min	2Re(λ)∕∣λ∣2.
λ∈spec(Jτ )
Using the expression for Y, we have that
修)6
1 - 2γRe(λ) + γ1 2 *(Re(λ)2 + Im(λ)2) = 1 - 22R⅛m)Re(λ) +
X2
Now, using the fact that Re(λ)∕∣λ∣2 > Re(λm)∕∣λιn|2, we have
1 - 4"Re(λ) + (") 2 ∣λ∣2 ≤ 1 - 2胃Re(λ) + (
|M2	∖ Dm|2 J	|%|2	∖
2Re(λm)∖2 ∣λm∣2Re(λ)
∣λm∣2
Re(λm)
1 - 4Re(λIm)Re(λ) + 4Re(λIm)Re(λ)
1	4 ∣λm∣2 Re(λ)+4 ∣λm∣2 Re(λ)
1
as claimed. From this argument, it is clear that for any γι ∈ (0, Y), |1 - γιλ∣ < 1 for all λ ∈
SPec(JT(x*)).
Now, consider any α ∈ (0, Y) and let β = (2Re(λm) - α∣λm∣2)-1. Observe that yi = Y - a so that
Y1 ∈ (0, Y). Hence,
|1 - (Y - α)λm∣2 =(1 - (2Reλm) - α) Re(λm))2 + (2R^ - O) Im(λln)2
I - 4Re(λm )2
+ 2αRe(λm) + 4Re(F-
—4αRe(λm) + α2 ∣λm∣2
1 — 2aRe(λιn) + α2 ∣λ1J2
α
β
1
—
so that
ρ(I - Y1JT(x*)) <
Hence, the ρ(I -Y1JT(x*)) < 1 so that an application of Proposition B.1 gives us the desired result.
F.2 Proof of Lemma F.2
To prove this lemma, we build directly on the conclusion of the proof of Lemma F.1. Indeed, since
ρ(I - Y1JT(x*)) <
given ε = 布 > 0 there exists a norm k ∙ k (cf. Lemma 5.6.10 in Horn & Johnson (1985))8 such that
kI-Y1JT(x*)k ≤
8The norm that exists can easily be constructed as essentially a weighted induced 1-norm. Note that the
norm construction is not unique. The proof in Horn & Johnson (1985) is by construction and the construction
of this norm can be found there.
33
Published as a conference paper at ICLR 2021
where the last inequality holds by Lemma B.1. Taking the Taylor expansion of I - γ1gτ(x) around
x*, We have
I - γιgτ(x) = (I - γιgτ(x*)) + (I - γJτ(x*))(x - x*) + R2(x - x*)
Where R2(x - x*) is the remainder term satisfying R2(x - x*) = o(kx - x* k) as x → x*.9 This
implies that there is a δ > 0 SUchthatkR2(x - x*)k ≤ 8β ∣∣x - x*k whenever ||x - x*k < δ. Hence,
kI - γ1gτ (x) - (I -
kx-x*k
kx-x*k
where the last ineqUality holds again by Lemma B.1. Hence,
α k/2
kxk-χ*k ≤(1- 4β)	kχo—χ*k
(25)
whenever kx0 - x* k < δ which verifies the claimed convergence rate.
F.3 Proof of Theorem 4
This resUlt follows directly from Theorem 1, Theorem B.2, Lemma F.2, and the following resUlt.
Proposition F.1 (Jin et al. 2020). Consider a zero-sum game (f1, f2) = (f, -f) defined by
f ∈ Cr (X, R) for some r ≥ 2. Suppose that x* is a differential Nash equilibrium. Then,
spec(-Jτ(x*)) ⊂ C- forall T ∈ (0, ∞).
Now to prove Theorem 4, we apply Theorem 1 to constrUct τ* via the gUard map ν(τ) =
det(-JT(x*)田-JT(X*)) SUch that for all T ∈ (τ*, ∞), SPec(Jτ(x*)) ⊂ C*. This guarantees
that spec(-Jτ(x*)) ⊂ C- for any T ∈ (τ*, ∞) and hence the nonlinear dynamical system
X = -Λτ g(χ)
is locally asymptotically (in fact, exponentially) stable by the Hartman-Grobman theorem (cf. The-
orem B.2). Therefore, for any T ∈ (T*, ∞), by Lemma F.2, T-GDA converges with a rate of
O((1 - 4β)k/2). Finally, T* = 0 by Proposition F.1 if x* is a differential Nash equilibrium. This
conclUdes the proof.
Intuition for Proposition F.1. While this result was shown in Jin et al. (2020), we give some
intuition for why it holds based on the quadratic numerical range tool. Indeed simple observation of
the eigenvalues of the quadratic numerical range show that SPec(Jr (x*)) ⊂ C* for any T ∈ (0, ∞).
Recall that
W2 (JT(x*)) =	SPec(JTv,w(x*))
v∈W1,w∈W2
where
JTv,w(x*)
hD12f (x*)v, vi	hD12f (x*)w, vi
h-T D1>2f (x*)v, wi h-T D22f (x*)w, wi
and Wi = {z ∈ Cni : kzk = 1} for each i = 1, 2. Fix v ∈ W1 and w ∈ W2 and consider
Jv,w O-Tb Td
Then, the elements ofW2(JT(x*)) are of the form
λτ = 2(a + Td) ± 1 p(α — Td)2 — 4t|b|2
9The notation R2(x — x*) = o(∣∣x — x* k) as X → x* means limχ→χ* ∣∣R2(x — x*)k∕∣∣x — x* k = 0.
34
Published as a conference paper at ICLR 2021
where a = (D2f (x*)v,v), b = hD12f (x*)w,v> and d =(—D2f (x*)w,wi for vectors V ∈ Wi
and W ∈ W2. Since D2f(x*) > 0 and —D2f(x*) > 0, Re(λ(Jv,w(x*))) > 0 since λ(Jv,w(x*))=
2(a + d) ± 1 -∖∕(a — d)2 — 4|b|2. The introduction of T ∈ (0, ∞) does not alter the sign. This is
obvious when Im(λτ) 6= 0. On the other hand, if Im(λτ) = 0 so that (a — τd)2 > 4τ |b|2, then
Re(λτ) > 1 (a + τd) — 1 p(a — Tdy — 4τ|b|2 > 0.
The last inequality is easily seen to be equivalent to —ad < |b|2, which holds for any pair of vectors
(v, w) such that v ∈ W1 and w ∈ W2 since a > 0 and d > 0. Hence, for any T ∈ (0, ∞),
SPec(Jr(x*)) ⊂ C+ since the spectrum of an operator is contained in its quadratic numerical range
and the above argument shows that W 2(Jτ (x*)) ⊂ Cj.
G Proof of Corollary 1： Finite Time Convergence of T—gda
Let k ∙ k be the norm that exists (via construction a la Horn & Johnson (1985, Lem. 5.6.10)) in
the proof of Lemma F.2 which is given in Appendix F. Following standard arguments, (25) in the
proof of Lemma F.2 implies a finite time convergence guarantee. indeed, let ε > 0 be given. Since
0 < 4αβ < 1 We have that (1 — α∕(4β))k < exp(—kα∕(4β)). Hence,
IIxk — x*k ≤ exp(—kα∕(4β))kxo — x*∣∣.
in turn, this implies that xk ∈ Bε(x*), meaning that xk is a ε-differential Stackelberg equilibrium
for all k ≥ d等 log(∣∣χo — x* ∣∣∕ε)] whenever ∣∣xo — x*k < δ.
Now, given that fi ∈ Cr(X, R) for r ≥ 2, I — γ1Jτ(x) is locally Lipschitz with constant L so that
we can find an explicit expression for δ in terms ofL. indeed, recall that R2(x — x*) = o(∣x — x* ∣)
as X → x* which means limχ→χ* ∣∣R2(χ — χ*)∣∕∣χ — χ*∣ = 0 so that
∣R2 (x — x* )∣ ≤
Z1
0
∣I — γ1Jτ(x* + η(x — x*)) — (I — γ1Jτ(x*))∣∣x
—x*k dη ≤ 2∣∣x — x*k2
observing that
kR2(X- x*)k ≤ L kx — x*k2 = L kx — x*kkx — x*k,
we have that the δ > 0 such that ∣R2(x — x*)∣ ≤ α∕(8β)∣x — x* ∣ is δ = α∕(4Lβ).
Comments on computing the neighborhood Bδ(x*). We note that we have essentially given
a proof that there exists a neighborhood on which T -GDA converges. of course, due to the non-
convexity of the problem in general, this neighborhood could be arbitrarily small. We provide
an estimate of the neighborhood size using the local Lipschitz constant of the local linearization
I — γ1Jτ(x*). one way to better understand the size of this neighborhood is to use Lyapunov
analysis, a tool which is well explored in the singular perturbation theory (Kokotovic et al., 1986).
in particular, Lyapunov methods can be applied directly to the nonlinear system if one can construct
Lyapunov functions for the fast and slow subsystems individually—also known as the boundary
layer model and reduced order model. With these Lyapunov functions in hand, one can “stitch”
the two together (via convex combination) and show under some reasonable assumptions that this
combined function is a Lyapunov function for the overall singularly perturbed system. The benefit
of this analysis is that the Lyapunov function gives one an estimate of the region of attraction (via,
e.g., the level sets); however, it is not easy to construct a Lyapunov function for a nonlinear system
in general. We leave expanding to such methods to future work.
H Convergence of Stochastic GDA with Timescale S eparation
in this section, we analyze convergence when players do not have oracle access to their gradients
but instead have an unbiased estimator in the presence of zero mean, finite variance noise. Specif-
ically, we show that the agents will converge locally asymptotically almost surely to a differential
Stackelberg equilibrium.
The key insight in this section is that due to Theorem 1 in the main body, we know that a critical
point x* is stable for X = —A「g(x) for a range of finite learning rates T ∈ (τ*, ∞) if and only
35
Published as a conference paper at ICLR 2021
if x* is a differential Stackelberg equilibrium. Hence, treating X = -Λτg(χ) as the continuous
time limiting differential equation in the so-called ordinary differential equation (ODE) method in
stochastic approximation (Borkar, 2008), we apply classical stochastic approximation analysis to
conclude that the stochastic gradient descent-ascent update with timescale separation converges.
H. 1 Asymptotic Convergence Guarantees via Stochastic Approximation
The stochastic form of the update is given by
xk+1 = xk - γk (Λτg(xk) + wk+1)
(26)
where wk+1 is a zero mean, finite variance random variable and {γk } is the learning rate sequence.
Assumption 2. The stochastic process {wk} is a martingale difference sequence with respect to the
increasing family of σ-fields defined by
Fk = σ(x', w`,' ≤ k), ∀k ≥ 0,
so that E[wk+1 | Fk] = 0 almost surely (a.s.) for all k ≥ 0. Moreover, wk is square-integrable so
that, for some constant C > 0,
E[kwk+1 k2 | Fk] ≤ C(1 + kxkk2) a.s., ∀k ≥ 0.
We note that this assumption has been relaxed in the literature (Thoppe & Borkar, 2019), however
simplicity, we state the theorem with the most accessible criteria. We remark below in the paragraph
on extensions to concentration bounds on the nature of the relaxed assumptions.
Theorem H.1. Consider a zero-sum game (f, -f) such that f ∈ Cr (X, R) for some r ≥ 2.
Suppose that Assumption 2 holds and that {γk} is square summable but not summable—i.e.,
k γk2 < ∞, yet k γk = ∞. For any τ ∈ (0, ∞), the sequence {xk} generated by (26) converges
to a, possibly sample path dependent, internally chain transitive invariant set of X = -Λτg(x).
Moreover, ifx* is a differential Stackelberg equilibrium, then there exists a finite τ * ∈ [0, ∞) such
that {Xk} almost surely converges locally asymptotically to X* for every τ ∈ (τ*, ∞).
Proof. The convergence of {Xk} to a, possibly sample path dependent, compact connected inter-
nally chain transitive invariant set of X= -Λτg(χ) follows from classical results in stochastic
approximation theory (Borkar (2008, Chap. 2); Benaim (1996)).
Suppose that X* is a differential Stackelberg equilibrium. By Theorem 1, there exists a finite
τ* ∈ [0, ∞) such that for all τ ∈ (τ*, ∞), X* is a locally exponentially stable equilibrium of
the continuous time dynamics X = -Λτg(x)—that is, spec(-Jτ(x*)) ⊂ C- for all T ∈ (T*, ∞).
Fix arbitrary T ∈ (τ*, ∞). Since spec(-Jτ(x*)) ⊂ C-, det(-JT(x*)) = 0 so that x* is an
isolated critical point. Furthermore, exponentially stability of X* implies that there exists a (local)
Lyapunov function defined on a neighborhood of X* by the converse Lyapunov theorem (Sastry
(1999, Thm. 5.17), Krasovskii (1963, Thm. 4.3)). Let U be the neighborhood of X* on which the
local Lyapunov function is defined, such that U contains no other critical points (which is possible
since X* is isolated). That is, let Φ : U → [0, ∞) be the local Lyapunov function defined on U
where x* ∈ U, Φ is positive definite on U, and for all X ∈ U, %Φ(x) ≤ 0 where equality holds
for z ∈ U if and only if Φ(z) = 0. By Corollary 3 (Borkar, 2008, Chap. 2), {Xk} converges to
an internally chain transitive invariant set contained in U almost surely. The only internally chain
transitive invariant set in U is x* .	口
The following corollary shows that if there is a finite T* such that x* is stable for X = -Λτg(x),
then by Theorem 1 X* must be a differential Stackelberg equilibrium and in turn, {Xk} almost surely
converges locally asymptotically to x* by the above theorem.
Corollary H.1. Consider a zero-sum game (f, -f) such that f ∈ C2 (X, R). Suppose that Assump-
tion 2 holds and that {γk} is square summable but not summable: k γk2 < ∞, yet k γk = ∞.
Ifthere exists a finite T * ∈ [0, ∞) such that spec(-Jτ (x*)) ⊂ C- for all T ∈ (T *, ∞), then x* is a
differential Stackelberg equilibrium and {xk} almost surely converges locally asymptotically to x*.
36
Published as a conference paper at ICLR 2021
While (local) almost sure convergence in gradient descent-ascent (Chasnov et al., 2019) to a critical
point10 in the stochastic setting, the result requires time varying learning rates with a sufficient
separation in timescale. Specifically, the players need to be using learning rate sequences {γi,k} for
each i ∈ {1, 2} such that (without loss of generality) not only is it assumed that γ1,k = o(γ2,k),
but also Pk γ12,k + γ22,k < ∞ and Pk γi,k = ∞ for each i ∈ {1, 2}. The challenge with these
assumptions on the learning rate sequences is that empirically the sequences that satisfy them result
in poor behavior along the learning path such as getting stuck at saddle points or making no progress.
This is, in essence, due to the fact that the faster player-that is, player 2 if γι,k = 0(γ2,k)—
equilibriates too quickly causing progress to stall. This can result in undesirable behavior such as
vanishing gradients (so that the discriminator does not provide enough information for the generator
to make progress), mode collapse, or failure to converge in practical applications such as generative
adversarial networks.
On the other hand, our convergence result gives a similar guarantee with less restrictive requirements
on the learning rate sequence. In particular, only a single learning rate sequence is required (so that
the algorithm can be viewed as a single timescale stochastic approximation update) as long as the
fast player (who, without loss of generality, is player 2 in this paper) scales their estimated gradient
by T ∈ (T*, ∞) where T* is as in Theorem 1.
H.2 Extensions to concentration bounds and relaxed assumptions on
STEPSIZES
It is possible to obtain concentration bounds and even finite time, high probability guarantees on
convergence leveraging recent advances in stochastic approximation (Borkar, 2008; Kamal, 2010;
Thoppe & Borkar, 2019). To our knowledge, the concentration bounds in (Thoppe & Borkar, 2019)
require the weakest assumptions on learning rates—e.g., the learning rate sequence {γk} needs only
to satisfy Pk γk = ∞, limk→∞ γk = 0, and Pk γk ≤ 1. Specifically, since it is assumed, for
the zero sum game (f, -f), that f ∈ C2 (X, R) and x* is a differential Stackelberg equilibrium,
Theorem 1 implies that x* is a locally asymptotically stable attractor of X = -Λτg(χ) for arbitrary
fixed T ∈ (T*, ∞), and hence, the concentration bounds in Theorem 1.1 and 1.2 of (Thoppe &
Borkar, 2019) directly apply.
Furthermore, we note that in applications such as generative adversarial networks, while it has been
observed that timescale separation heuristics such as unrolling or annealing the stepsize of the dis-
criminator work well, in the stochastic case, summmable/square-summable assumptions on stepsizes
are generally too restrictive in practice since they lead to a rapid decay in the stepsize which, in turn,
can stall progress. On the other hand, stepsize sequences such as γk = 1/(k + 1)β for β ∈ (0, 1]—
a sequence which satisfies the assumptions posed in (Thoppe & Borkar, 2019)—tend not to have
this issue of decaying too rapidly for appropriately chosen β, while also maintaining the guaran-
tees of the theoretical results. We state a convergence guarantee under these relaxed assumptions in
Proposition H.1 below.
Let X(t) be the asymptotic pseudo-trajectories of the stochastic approximation process {χk}. That
is, X(t) are linear interpolates between the sample points Xk generated by the stochastic T-GDA
process, and are defined by
X⑴=X(tk )+:(X(U)- x(tk))
where tk = tk + γk and t0 = 0.
Assumption 3. The stochastic process {wk} is a martingale difference sequence with respect to the
increasing family of σ-fields defined by
Fk = σ(X', w`,' ≤ k), ∀k ≥ 0,
so that E[wk+1| Fk] = 0 almost surely for all k ≥ 0. Furthermore, there exists c1, c2 ∈ C(Rd, R>0)
such that
Pr{kwk+1 k > v| Fk} ≤ c1(Xk) exp(-c2 (Xk)v), n ≥ 0
for all V ≥ V where V is some sufficiently large, fixed number
10To date it has not been shown that for a sufficient separation in timescale the only critical point attractors
are local minmax.
37
Published as a conference paper at ICLR 2021
Proposition H.1. Suppose that Assumption 3 holds and that x* is a differential Stackelberg equi-
librium. Let Yk = l/(k + 1)β where β ∈ (0,1]. There exists a T* ∈ [0, ∞) and an 6o ∈ (0, ∞)
such that for any fixed ∈ (0, 0], there exists functions h1 () = O(log(1/)) and h2() = O(1/)
so that when T ≥ hι(e) and ko ≥ KT where KT is such that 1 /γk ≥ h2(E) for all k ≥ KT, the
stochastic iterates ofτ-GDA with stepsize sequence γk and timescale separation τ ∈ (τ*, ∞) satisfy
Pr{kX(t) - x*k ≤ E ∀t ≥ tko + T + 1| X(tko) ∈ Be(x*)} = 1 - O(k0-e/2 exp(-Cke/2))
for some constant CT > 0.
The proof largely follows from the proofs of Theorem 1.1 and 1.2 in (Thoppe & Borkar, 2019),
combined with the existence of a finite timescale separation parameter obtained via Theorem 1.
Indeed, since x* is a differential StackeIberg equilibrium, by Theorem 1 there exists a range of T——
namely, (T*, ∞)——such that for any T ∈ (T*, ∞), x* is a locally asymptotically stable equillibrium
for X = -Λτg(x). Hence, fixing any T ∈ (T*, ∞), a converse Lyapunov theorem can be applied to
construct a local Lyapunov function. Let V : Rn → R be this Lyapunov function so that there exists
r, r0 , E0 > 0 such that r > r0, and
B(x*) ⊆ Vr0 ⊂ N0(Vr0) ⊆ Vr
for any E ∈ (0, E0] where, for a given q > 0, V q = {x ∈ dom(V ) : V (x) ≤ q} and N0 (V r0) is an
E0-neighborhood of Vr0 ——i.e., N^ (Vr0) = {χ ∈ Rn| ∃y ∈ Vr0, ||x - yk ≤ e0}. From here, the
result follows from an application of the results in the work by Thoppe & Borkar (2019).
The utility of this result is that it provides a guarantee in the stochastic setting for a more reasonable
and practically useful stepsize sequence. However, constructing the constants such as KT , CT and
E0 is highly non-trivial as can be seen in the work of Thoppe & Borkar (2019) and similar works in
the area of stochastic approximation (Borkar, 2008). One direction of future work is examining the
Lyapunov approach for directly analyzing the nonlinear singularly perturbed system; it is known,
however, that the stochastic singularly perturbed systems have much weaker guarantees in terms of
stability (Kokotovic et al., 1986, Chap. 4).
I Stability of ∞-gda: A singular perturbation approach
The examples included in Section 3 provide evidence that there exists a range of finite learning rate
ratios for which differential Stackelberg equilibrium are stable and a range of learning rate ratios for
which non-equilibrium critical points are unstable. Yet, until this paper, no result has appeared in the
literature on gradient descent-ascent with timescale separation confirming this behavior in general.
The closest existing result studies the limiting case T → ∞. As mentioned previously Jin et al.
(2020) show that as T → ∞, the set of stable critical points with respect to the dynamics X =
-ΛT g(x) coincide with the set of differential Stackelberg equilibrium. However, an equivalent result
in the context of general singularly perturbed systems has been known in the literature ( Kokotovic
et al. 1986, Chap. 2). We give a proof based on this type of analysis because it reveals a new set
of analysis tools to the study of game-theoretic formulations of machine learning and optimization
problems. The formal statement (which is a restatement of the result from Jin et al. 2020 in our
notation) is given below.
Proposition I.1. Consider a zero-sum game (f1, f2) = (f, -f) defined by f ∈ Cr(X, R) for some
r ≥ 2. Suppose that x* is such that g(x*) = 0 and det(D22f2(x*)) 6= 0. Then, as T → ∞,
spec(-Jt(x*)) ⊂ C- ifand only if x* is a differential Stackelberg equilibrium.
The structure of this proof is as follows. We begin by introducing general background for analyzing
general singularly perturbed systems. Following this, we consider the linearization of the singularly
perturbed system that approximates the simultaneous gradient dynamics and describe how insights
made about this system translate to the corresponding nonlinear system. Finally, we analyze the
stability of the linear system around a critical point to arrive at the stated result. The analysis is
primarily from Kokotovic et al. (1986).
Analysis of General Singularly Perturbed Systems. Let us begin by considering a general sin-
gularly perturbed system for x ∈ Rn, z ∈ Rm, and a sufficiently small parameter ε > 0 given by
38
Published as a conference paper at ICLR 2021
X = f (x,z,ε,t), x(to,ε) 二 xo, x ∈ Rn
εZ = g(x, z, ε, t), z(to, ε) = z0,z ∈ Rm
(27)
where f and g are assumed to be sufficiently many times continuously differential functions of the
arguments x, z, ε, and t. Observe that when ε = 0, the dimension of the system given in (27) drops
from n + m to n since Z degenerates into the equation
0 = g(x,西 0,t)
(28)
where the notation of X, W indicates that the variables belong to the system with ε = 0. We further
require the assumption that (28) has k ≥ 1 isolated roots, which for each i ∈ {1, . . . , k} are given
by
zW = φWi (xW, t).
We now define an n-dimensional manifold Mε for any ε > 0 characterized by the expression
z(t, ε) = φ(x(t, ε), ε),	(29)
where φ is sufficiently many times continuously differentiable function of x and ε. For Mε to be an
invariant manifold of the system given in (27), the expression in (29) must hold for all t > t* if it
holds for t = t*. Formally, if
z(t* , ε) = φ(x(t*, ε), ε) → z(t, ε) = φ(x(t, ε), ε) ∀t ≥ t*,	(30)
then Mε is an invariant manifold for (27). Differentiating the expression in (30) with respect to t,
we obtain
Z = dtφ(x(t, ε), ε) = ∂φX.	(31)
Now, multiplying the expression in (31) by ε and substituting in the forms of X, Z, and Z from (27)
and (29), the manifold condition becomes
g(x, φ(x, ε),ε,t) = ε ∂x f (x, φ(x, ε) , ε, t) ,	(32)
which φ(x, ε) must satisfy for all x of interest and all ε ∈ [0, ε*], where ε* is a positive constant.
We now define
η = Z - φ(x, ε).
Then, in terms of x and η, the system becomes
X = f (x, φ(x, ε) + η, ε, t)
∂φ
εη = g(x,φ(x,ε) + η,ε,t) - ε∂Xf (x,φ(x,ε) + η,ε,t).
Remark 1. One interesting observation is that the above system is exactly the continuous time limit-
ing system for the τ -Stackelberg learning update in Fiez et al. (2020) under a simple transformation
of coordinates.
Observe that the invariant manifold Mε is characterized by the fact that η = 0 implies η = 0 for all
x for which the manifold condition from (32) holds. This implies that if η(t0, ε) = 0, it is sufficient
to solve the system
X = f (x, φ(x, ε), ε, t), x(to, ε) = x0.
This system is often referred to as the exact slow model and is valid for all x, Z ∈ Mε and Mε known
as the slow manifold of (35).
Linearization of Simultaneous Gradient Descent Singularly Perturbed System. We now con-
sider the singularly perturbed system for simulataneous gradient descent given by
X = -Dlfι(x,z)
εZ = -D f2(x,z).
(33)
39
Published as a conference paper at ICLR 2021
Let Us linearize the system around a point (x*, z*). Then,11
D1f1(x,z) ≈ D1f1 (x*,z*) + D,2fι(x*,z*)(x - x*) + D12f1(x*,z*)(z - z*)
D2f2(x, z) ≈ D2f2(x*, z*) + D21f2(x*, z*)(x - x*) + D22f2(x*, z*)(z - z*).
(34)
Defining u = (x-x*) and v = (z-z*) and considering a point (x*, z*) such that D1f1(x*, z*) = 0
and D2f2(x*, z*) = 0, then linearized singularly perturbed system is given by
U = -Dlfι(x*,z*)u - D12f1(x*,z*)v
εV = -D21f2(x*, z*)u - D2f2(x*,z*)v.
(35)
To simplify notation, let us define Jτ as follows
J	D12f1(x*,z*)	D12f1(x*,z*)	A11 A12
τ	ε-1D21f2(x*, z*) ε-1D22f2(x*, z*)	ε-1A21 ε-1A22
along with
W = Ju ]	and W = R ].
Then, an equivalent form of (35) is given by
W
(36)
In what follows, we make insights about the behavior of the nonlinear system given in (33) around
a critical point (x*, z*) by analyzing the linear system given in (36). Recall that if (x*, z*) is
asymptotically stable with respect to the linear system in (36), then it is also asymptotically stable
with respect to the nonlinear system from (33). Moreover, to determine asymptotic stability, it is
sufficient to prove that SPec(JT((χ*,z*)) ⊂ C+. In What follows, we specialize the general analysis
of singularly perturbed systems to the singularly perturbed linear system given in (36).
Stability of Critical Points of Simutaneous Gradient Descent. The manifold condition
from (32) for the system in (36) is given by
∂φ
A21u + A22φ(u, ε) = ε—— (A11u + A12φ(u, ε)).	(37)
∂u
We claim that (37) can be satisfied by a function φ that is linear in u. Indeed, defining
v = φ(u, ε) = -L(ε)u
and then substituting back into (32), we get the simplified manifold condition of
A21 - A22L(ε) = -εL(ε)A11 + εL(ε)A12L(ε).	(38)
Before we prove that an L(ε) always exists to satisfy (38), consider the change of variables
η = v + L(ε)u.
The change of variables transforms the system from (36) into the equivalent representation
U A11 — A12L(ε)	A12	U
n =	R(L,ε)	A22 + εL(ε)A12	η
where
R(L, ε) = A21 - A22L(ε) + εL(ε)A11 - εL(ε)A12L(ε).
Consider that R(L, ε) = 0. Then, the system from (39) has the upper block-triangular form
X = A11 - A12L(ε)	A12	X
η —	0	A22 + εL(ε)A12 η
(39)
(40)
(41)
which has the effect of generating a replacement fast subsystem given by
εη = (A22 + εLA12)η.
We now proceed to show that an L(ε) such that R(L, ε) = 0 always exists.
11Here, the ≈ means, e.g., D1f1(x, Z) = D1f1(x*, z*) + D2fι(x*, z*)(x — x*) + D12f1(x*, z*)(z —
z*) + O(∣∣x — x* k2 + ∣∣z — z* k2), and similarly for D2f2(x, z).
40
Published as a conference paper at ICLR 2021
LemmaL1. If A22 is such that det(A22) = 0, there is an ε* Suchthatforall ε ∈ [0, ε*], there exists
a solution L(ε) to the matrix quadratic equation
R(L, ε) = A21 - A22L(ε) + εL(ε)A11 - εL(ε)A12L = 0	(42)
which is approximated according to
L(ε) = A2-21A21 + εA2-22A21A0 + O(ε2),	(43)
where
A0 = A11 - A12A2-21A21 .	(44)
Proof. To begin, observe that for ε = 0, the unique solution to (42) is given by L(0) = A2-21A21.
Now, differentiating R(L, ε) from (42) with respect to ε, we find
A22 + εL(ε)A12 dε - ε^dε (AII - A12L(E)) = L(ε)A11 - L(E)A12L(E)∙
The unique solution of this equation at ε is
-Γ- I	= a-2 L(O)(A11 - A12L(O)) = A222A21A0.
dE ε=0
Accordingly, (43) represents the first two terms of the MacLaUrin series for L(ε).	□
We remark that L(E) as defined in (43) is unique in the sense that even though R(L, ) as given
in (42) may have several real solUtions, only one is approximated by (43).
The characteristic eqUation of (41) is eqUivalent to that for the system from (36) owing to the sim-
ilarity transform between the systems. The block-triangUlar form of (36) admits a characteristic
eqUation given by
ψ(s,ε) = εm ψs(s,ε)ψf (p,ε)=0,	(45)
where
ψs(s, E) = det(sI - (A11 - A12L(E)))	(46)
is the characteristic polynomial of the slow sUbsystem, and
ψf (p, E) = det(pI - (A22 + EA12L(E)))	(47)
is the characteristic polynomial of the fast sUbsystem in the timescale p = sE. ConseqUently, n of
the eigenvalUes of (36) denoted by {λ1, . . . , λn} are the roots of the slow characteristic eqUation
ψs(s, ε) = 0 and the rest of the eigenvalues {λn+ι,..., λn+m} are denoted by λ% = Vj/ε for
i = n + j and j ∈ {1, . . . , m} where {ν1 , . . . , νm} are the roots of the fast characteristic eqUation
ψf (p, E) = 0.
The roots of ψs(s, E) at E = 0, given by the solution to
ψs (s, 0) = det(sI - (A11 - A12L(0))) = 0,	(48)
are the eigenvalues of the matrix A0 defined in (44) since L(0) = A2-21A21 as shown in Lemma I.1.
The roots of the fast characteristic equation at E = 0, given by the solution to
ψf (p, 0) = det(pI - A22 ) = 0	(49)
are the eigenvalues of the matrix A22. The roots of the systems correspond to the conditions for a
differential Stackelberg equilibrium, which thus gives the result.
We now proceed by characterizing how closely the eigenvalues of the system at E = 0 approximate
the eigenvalues of the system from (36) as E → 0.
If det(A22) 6= 0, then as E → 0, n eigenvalues of the system given in (36) tend toward the eigen-
values of the matrix A0 while the remaining m eigenvalues of the system from (36) tend to infinity
with the rate 1/E along asymptotes defined by the eigenvalues of A22 given as spec(A22)/E as a
result of the continuity of coefficients of the polynomials from (46) and (47) with respect to E.
41
Published as a conference paper at ICLR 2021
Now, consider the special (but generic) case in which the eigenvalues of A0 are distinct and the
eigenvalues of A22 are distinct, but A0 and A22 may have common eigenvalues. Then, taking the
total derivative of (45) with respect to ε we have that
∂ψs ds + ∂ψs = 0
∂s dε ∂ε
Now, observe that ∂ψs /∂s 6= 0 since the eigenvalues ofA0 = A11 - A12A2-21A21 are distinct.12 For
each i = 1,... ,n, this gives Us a well-defined derivative ds∕dε (by the implicit mapping theorem)
and hence, with s(0) = λi(A0), the O(ε) approximation of s(ε) follows directly. That is,
λi = λi(A0) + O(ε), i = 1, . . . , n1
Similarly, taking the total derivative of ψf (p, ε) = 0 and again applying the implicit fUnction theo-
rem, we have
λi+n1 = ε-1(λj(A22 + O(ε)), i = 1, . . . , n2
where we have Used the fact that p = sε.
J	Further Details on Related Work
In this section, we provide fUrther details on the discUssion from Section A regarding the resUlts
presented by Jin et al. (2020) on the local stability of gradient descent-ascent with a finite timescale
separation. The pUrpose of this discUssion is to make clear that Proposition 27 from the work of Jin
et al. (2020) does not disagree with the resUlts we provide in Theorem 1 and Theorem 2 and is
instead complementary. In what follows, we recall Proposition 27 of Jin et al. (2020) in separate
pieces in the terminology of this paper and delineate its meaning from oUr resUlts on the stability of
gradient descent-ascent with a finite timescale separation.
To begin, we consider the component of Proposition 27 from Jin et al. (2020) which says that given
any fixed and finite timescale separation τ > 0, a zero-sUm game can be constrUcted with a differ-
ential Stackelberg eqUilibriUm that is not stable with respect to the continUoUs time limiting system
of T-GDA given by the dynamics X = -Λτg(x).
Proposition J.1 (Rephrasing of Jin et al. 2020, Proposition 27(a)). For any fixed τ > 0, there
exists a zero-sum game G = (f, —f) such that SPec(Jr(x*)) ⊂ C+ for a differential Stackelberg
equilibrium x*.
We now explain the proof. Let Us consider any > 0 and the game
f(χ,y) = —x2 +2√exy - (e∕2)y2.	(50)
At the UniqUe critical point (x*, y*) = (0, 0), the Jacobian of the dynamics is given by
Jτ(χ*,y*)=]--√ι ¥ .
Moreover, observe that (x*, y*) is a differential Stackelberg eqUilibriUm and not a differential Nash
equilibrium since D2f (x*,y*) = -2>0, -D2f (x*,y*) = e > 0 and Sι( J(x*,y*)) = 2 > 0.
Finally, the spectrUm of the Jacobian is
ʃ -2 + Te ± √τ2e2 - 12τe + 4 ɪ
SPec(Jr(x ,y )) = I---------------2-------------b
Let us now fix T as any arbitrary positive value. Then, consider the game construction from (50)
with e = 1/T. For the fixed choice ofT and subsequent game construction, we get that
SPec(Jτ(x*,y*)) = {( - 1 ± i√7)∕2} ⊂ Ci.
This in turn means the differential Stackelberg equilibrium is not stable with respect to the dynamics
X = -Λτg(x) for the given choice of T. Since the choice of T was arbitrary, this is a valid procedure
12Recall that having distinct eigenvalues is a generic condition for a matrix an n1 × n1 matrix, though not
explicitly required for the asymptotic results; its only a condition for the big-O approximation λi = λi (A0) +
O(ε) for i = 1, . . . , n1 and λi = ε-1 (λj (A22) + O(ε)) where i = n1 + j for j = 1, . . . , n2.
42
Published as a conference paper at ICLR 2021
to generate a game with a differential Stackelberg equilibrium that is not stable with respect to
X = -Λτg(χ) given a choice of T beforehand.
This result contrasts with that of Theorem 1 in the following fundamental way. In the proof of
Proposition J.1, τ is fixed and then the game is constructed, whereas in Theorem 1 the game is
fixed and then the conditions on τ given. To illustrate this point, consider the game construction
from (50) with E fixed to be an arbitrary positive value. It can be verified that SPec(JT(x*, y*)) ⊂
C+ for all T > 2/e. This means that given the differential Stackelberg equilibria in this game
construction, there is indeed a finite T* such that the equilibrium is stable with respect to X =
-Λτ g(x) for all T ∈ (T*, ∞). Put concisely, Proposition J.1 is showing that there is exists a
continuum of games for which a differential Stackelberg equilibrium is unstable with an improper
choice of finite learning rate ratio T . On the other hand, Theorem 1 is proving that given a game
with a differential Stackelberg equilibrium, there exists a range of suitable finite learning rate ratios
such that the differential Stackelberg equilibrium is guaranteed to be stable.
We now move on to examining the portion of Proposition 27 from Jin et al. (2020) which says that
given any fixed and finite timescale separation T > 0, a zero-sum game can be constructed with
a critical point that is not a differential Stackelberg equilibrium which is stable with respect to the
continuous time limiting system of T-GDA given by X= -Λτg(x).
Proposition J.2 (Rephrasing of Jin et al. 2020, Proposition 27(b)). For any fixed T, there exists
a zero-sum game G = (f, —f) such that SPec(JT(x*)) ⊂ C* for a critical point x* satisfying
g(X* ) = 0 that is not a differential Stackelberg equilibrium.
In a similar manner as following Proposition J.1, we now explain the proof of Proposition J.2 and
then contrast the result with Theorem 2. Again, consider any E > 0, along with the game construc-
tion
f(x,y) = x1 + 2√Exιyι + (〃2)y2 - X2/2 + 2√Ex2y2 - ey2.	(51)
At the unique critical point (X*, y*) = (0, 0), the Jacobian of the dynamics is given by
	一 2	0	2 √E	0 一
JT(X*, y*) =	0 -2τ √e	-1 0	0 -Te	2√E 0
	0	-2τ √e	0	2Te
Observe that (X*, y*) is neither a differential Nash equilibrium nor a differential Stackelberg equi-
librium since D12f(X*, y*) = diag(2, -1) and -D22f(X*, y*) = diag(E, 2E) are both indefinite. The
spectrum of the Jacobian is
SPec(J ( * *)) n 2 — Te ± √τ2e2 - 12te + 4 -1 + 2TE ± √4τ2e2 - 12te + 1 o
Now, fix T as any arbitrary positive value, then consider the game construction from (51) with
E = 1/T. For the fixed choice of T and resulting game construction given the choice of E, we have
that
SPec(JT(x*,y*)) = {1 ± i√7,1 ± i√7} ⊂ C*.
This indicates that the non-equilibrium critical point is stable with respect to the dynamics Z =
-ΛT g(z) where z = (X, y) for the given choice of T. Similar to the proof of Proposition J.1, since
the choice of T was arbitrary, the procedure to generate a game with a non-equilibrium critical point
that is stable with respect to Z= -Λτg(z) is valid given a choice of T beforehand.
The key distinction between Proposition J.2 and Theorem 2 is analogous to that between Proposi-
tion J.1 and Theorem 1. Indeed, the proof and result of Proposition J.2 rely on T being fixed followed
by the game being constructed. On the other hand, in Theorem 2 the game is fixed and then the con-
ditions on T given. To make this clear, consider the game construction from (51) with E fixed to be
an arbitrary positive value. It turns out that SPec(JT(x*,y*)) ⊂ C* for all t > 2/e since
/2 — Te ± √t2e2 — 12te + 4、
Re(----------\----------------)< 0.
As a result, given the unique critical point of the game there is a finite T0 such that the non-
equilibrium critical point is not stable with respect to X = -Λτg(x) for all T ∈ (to, ∞). In
43
Published as a conference paper at ICLR 2021
summary, Proposition J.2 is showing that there is exists a continuum of games for which a non-
equilibrium critical point is stable given an unsuitable choice of finite learning rate ratio τ . In
contrast, Theorem 2 is showing that given a game with a non-equilibrium critical point, there exists
a range of finite learning rate ratios such that it is not stable.
To recap, the discussion in this section is meant to explicitly contrast Proposition 27 from the work
of Jin et al. (2020) with Theorem 1 and Theorem 2 since they may potentially appear contradictory
to each other without close inspection. The result of Jin et al. (2020) shows that (i) given a fixed
finite learning ratio, there exists a game for with a differential Stackelberg equilibria that is not stable
and (ii) given a fixed finite learning ratio, there exists a game with a non-equilibrium critical point
that is stable. From a different perspective, we show that (i) given a fixed game and differential
Stackelberg equilibrium, there exists a range of finite learning rate ratios for which the equilibrium
is stable (Theorem 1) and (ii) given a fixed game and a non-equilibrium critical point, there exists a
range of finite learning rate ratios for which the critical point is not stable (Theorem 2).
K Experiments Supplement
In this section we present several experiments not included in the body of the paper along with sup-
plemental simulation results and details for the experiments presented in Section 5. We numerically
investigate Example 1 in Section K.1 and a game similar to that from Example 2 in Section K.2. Af-
ter that, we investigate a polynomial game with multiple equilibria in Section K.3. We study a torus
game in Section K.4 and examine the connection between timescale separation and the region of
attraction. Then, in Section K.5, we return to the Dirac-GAN game and consider the non-saturating
objective function. In Section K.6, we explore a generative adversarial network formulation using
the Wasserstein cost function with a linear generator and quadratic discriminator for the problem of
learning a covariance matrix. We finish in Section K.7 by presenting further results and details on
our experiments training generative adversarial networks on image datasets along with additional
generative adversarial network experiments parameterized by neural networks with a mixture of
Gaussians. Code for the experiments is included in the supplemental material.
K. 1 Quadratic Game: Timescale Separation and Stackelberg Stability
We now revisit the game from Example 1 that demonstrated there exists differential Stackelberg
equilibrium that are unstable for choices of the timescale separation τ . To be clear, we repeat the
game construction and some characteristics of the game. Let us consider the quadratic zero-sum
game defined by the cost
f(xι,X2) = 2
>
x1
x2
-v 0	-v	0
0
-v
0
x1
x2
(52)
where x1 , x2 ∈ R2 andv > 0. The unique critical point of the game given by x* = (x↑,x^') = (0,0)
is a differential Stackelberg equilibrium. The spectrum of the Jacobian evaluated at the equilibrium
is given by
spec(Jτ (x*)) = n
v(2τ + 1 ± √4τ2 — 8τ + 1) v(τ — 2 ± √τ2 — 12τ + 4)
4	,	4
.
As mentioned in Example 1, it turns out that SPec(JT(x*) ⊂ C+ only when T ∈ (2, ∞). We remark
that we computed τ * using the theoretical construction from Theorem 1 and found that it recovered
the precise value of τ* = 2 such that the equilibrium is stable for all τ ∈ (τ*, ∞) with respect
to the dynamics X = —A「g(χ). In the experiments that follow, We consistently observe that the
construction of τ* from the theory is tight.
For this experiment, we select v = 4 and simulate τ-GDA from the initial condition (x01, x20) =
(5, 4, 3, 2) with γ1 = 0.0005 and τ ∈ {2, 2.5, 3, 5, 10}. In Figures 4a and 4b, we show the tra-
jectories of the players coordinate pairs (x11, x21) and (x21, x22), respectively. We observe that
τ -GDA cycles around the equilibrium with τ = 2 since it is marginally stable with respect to the
dynamics. For τ ∈ (2, ∞), the equilibrium is stable and τ -GDA ends up converging to it at a rate
44
Published as a conference paper at ICLR 2021
(a)
(b)
(c)
(e)	(f)	(g)
Figure 4: Experimental results for the quadratic game defined in (52) of Section K.1 and presented
in Example 1. Figures 4a and 4b show trajectories of the players coordinate pairs (x11, x21) and
(x21 , x22) for a range of learning rate ratios, respectively. Figures 4c shows the distance from the
equilibrium along the learning paths. Figures 4e, 4f, and 4g show the trajectories of the eigenvalues,
the real parts of the eigenvalues, and the imaginary parts of the eigenvalues for the JT (x*) as a
function of the τ , respectively.
that depends on the choice of τ . We demonstrate how the convergence rate depends on the choice
of τ in Figure 4c by showing the distance from the equilibrium along the learning path for each of
the trajectories. The primary observation is that the cyclic behavior of τ -GDA dissipates as τ grows
and as a result the dynamics then rapidly converge to the equilibrium.
The behavior of the learning dynamics as a function of the timescale separation τ can be further
explained by evaluating the eigenvalues of the game Jacobian at the equilibrium. We show the
eigenvalues of the Jacobian at the equilibrium in several forms in Figures 4e, 4f, and 4g. Analyzing
the spectrum, we are able to verify that for all τ ∈ (2, ∞) the equilibrium is indeed stable. Moreover,
we see that the imaginary parts of the conjugate pairs of eigenvalues decay after τ = 1 and τ = 6,
and then the eigenvalues of the conjugate pairs eventually become purely real at τ = 1.87 and
τ = 11.66, respectively. After the eigenvalues of a conjugate pair become purely real, they split
so that one of the eigenvalues asymptotically converges to an eigenvalue of S1 (J(x*)) by moving
back along the real line, while the other eigenvalue tends toward an eigenvalue of —τD2f (x*). This
occurrence is exactly what was described in Section 3 as an immediate implication of Proposition I.1
when the eigenvalues of S1 (J(x*)) and τD2f(x*) are distinct. The convergence rate is in fact
limited by the eigenvalues splitting since as τ grows, the spectrum of the Jacobian is limited by
the eigenvalues of the Schur complement which remain constant. A related open question centers
on finding the worst case convergence rate as a function of the spectral properties of S1 (J(x*))
and D2f (x*). Finally, the evolution of the eigenvalues as a function of the timescale separation T
demonstrates that the rotational dynamics in τ-GDA vanish as the ratio between the magnitude of the
real and imaginary parts of the eigenvalues grows.
K.2 Polynomial Game: Timescale Separation and Non-Equilibrium Stability
We now return to a game similar to that from Example 2 with a non-equilibrium critical point which
is stable without timescale separation and becomes unstable for a range of finite learning ratios with
45
Published as a conference paper at ICLR 2021
(a)
(b)
(c)
g*ds)6E_
(e)
(f)
Figure 5: Experimental results for the polynomial game defined in (53) of Section K.2 and presented
in Example 2. Figures 5a and 5b show trajectories of the players coordinate pairs (x11, x21) and
(x21, x22) for a range of learning rate ratios, respectively. Figures 5d, 5e, and 5f show the trajectories
of the eigenvalues, the real parts of the eigenvalues, and the imaginary parts of the eigenvalues for
JT (x*) as a function of the T, respectively where x* is the non-equilibrium critical point.
multiple equilibria in the vicinity. Consider a zero-sum game defined by the cost
f (xι, X2)= 5 (x1ι + 2xιιX21 + 2x2ι - 2x22 + 2x12X22 - x2z) (x11 - 1)2
+ x21 ( P2=1(XIi-I)2 -(X2i - I)2).
(53)
This game has critical points at (0, 0, 0, 0), (1, 1, 1, 1), and (-4.73, 0.28, -92.47, 0.53). Among the
critical points, only (1, 1, 1, 1) and (-4.73, 0.28, -92.47, 0.53) are game-theoretically meaningful
equilibrium. In fact, they are each differential Nash equilibrium and are locally stable for any choice
of τ ∈ (0, ∞) as a result of Proposition F.1. On the other hand, the critical point x* = (0, 0, 0, 0)
is neither a differential Nash equilibrium nor a differential Stackelberg equilibrium. However, x*
is stable for τ ∈ (0, 2) and it is marginally stable for τ = 2. In general, convergence to the non-
equilibrium critical point x* in the presence of multiple game-theoretically meaningful equilibrium
would be viewed as undesirable. In fact, this is precisely the type of critical point that sophisticated
schemes for converging to only differential Nash equilibria or only differential Stackelberg equilibria
seek to avoid (Adolphs et al., 2019; Fiez et al., 2020; Mazumdar et al., 2019; Wang et al., 2020). We
show in this example that the simple inclusion of timescale separation in gradient descent-ascent is
sufficient to avoid x* and instead converge to a differential Nash equilibrium.
Indeed, for all T ∈ (2, ∞) the non-equilibrium critical point x* is unstable with respect to X =
-Λτg(x). We simulate τ -GDA from the initial condition (x10, x02) = (-1.5, 2.5, 2.5, 3) with γ1 =
0.0005 and T ∈ {0.75, 2, 5, 12}, where we use the superscript to denote the time index so as not to
be confused with the multiple indexes for player choice variables. In Figures 5a and 5b, we show the
trajectories of the players coordinate pairs (x11, x21) and (x21, x22), respectively. We observe thatT-
GDA converges to the non-equilibrium critical point x* with T = 0.75 as expected and the dynamics
46
Published as a conference paper at ICLR 2021
(a)
(b)
((V}u① ds)6LLH
(c)
Figure 6: Experimental results for the polynomial game defined in (54) of Section K.3. Figures 6a
provides a 3d view of the cost function -f (x1 , x2) along with the cost contours and critical point
locations. Figure 6b shows trajectories of τ -GDA for a range of learning rate ratios given an initial-
ization around the differential StaCkelberg equilibrium (x；,x5) = (-11.03, -11.03). Figures 6c
and 6d show the evolution of the eigenvalues from JT (x*) as a function of T where x* is the differ-
ential StaCkelberg equilibrium (x1； , x2；) = (-11.03, -11.03).
125
IOO
(d)
Re⅛λι)
Re湍
----Irng(Rl)
Imga2)
move near it and then cycle around it with τ = 2 since the critical point becomes marginally stable.
However, for τ = 5 and τ = 12, τ -GDA avoids the non-equilibrium critical point since it becomes
unstable and instead the dynamics converge to the nearby differential Nash equilibrium. We show
the eigenvalues of the Jacobian at the non-equilibrium critical point x* = (0, 0, 0, 0) in several forms
in Figures 5d-5f. Again, We observe that the eigenvalues quickly become purely real as T grows and
then they split, and asymptotically converge toward the eigenvalues of S1(J(x*)) and -τD22f(x*).
Together, this example demonstrates that often there is a reasonable finite learning rate ratio such
that non-meaningful critical points become unstable for T-GDA.
K.3 Polynomial Game: Vector Field Warping and Region of Attraction
Consider a zero-sum game defined by the cost
f(xι,X2) = -e-(0.01x2 +0.01x2) ((0.3χι + X2)2 + (0.3x2 + x21 )2) .	(54)
The cost structure of this game is visualized in Figure 6a, where we present a three dimensional
view of -f(x1, x2) along with the cost contours and the locations of critical points. This game
has eleven critical points including one differential Nash equilibrium and two differential Stackel-
berg equilibria that are not a differential Nash equilibrium. The critical points that are neither a
47
Published as a conference paper at ICLR 2021
X (x^,x^) = (10.57, -8.95) x (x*,x*) = (-11.03, - 11.03) X (x*,x*) = (-1.62, - 1.62)
(b)
Figure 7:	Experimental results for the polynomial game defined in (54) of Section K.3. In Figure 7a,
we overlay the trajectories from Figure 6b produced by τ -GDA onto the vector field generated by
the choice of timescale separation selection τ . The shading of the vector field is dictated by its
magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds
to a lower magnitude. Figure 7b demonstrates the effect of timescale separation on the region of
attractions around critical points by coloring points in the strategy space according to the equilibrium
τ -GDA converges. We remark that areas without coloring indicate where τ -GDA did not converge in
the time horizon.
differential Nash equilibrium nor a differential Stackelberg equilibrium are unstable for any choice
of timescale separation τ. The differential Nash equilibrium is at (x1 , x2) = (10.57, -8.95) and
it is stable for all τ ∈ (0, ∞) by Proposition F.1. The differential Stackelberg equilibria are at
(x1, x2) = (-1.625, -1.625) and (x；, x3) = (-11.03, -11.03); each is stable for all T ∈ (1, ∞).
We computed T * for the pair of differential StaCkelberg equilibrium using the theoretical ConstruC-
tion from Theorem 1 and observed that it properly recovered τ ； = 1 for each equilibrium as the
timescale separation such that the continuous time system is stable for all T ∈ (T* , ∞). Finally,
we note that while the set of equilibrium follow a linear translation, this game is generic and the
equilibria are in fact isolated.
In Figure 6b, we show the trajectories of T -GDA with γ1 = 0.0001 and T ∈ {1, 2, 5, 20} given
the initialization (x10, x02) = (-9, -9) near the differential Stackelberg equilibrium at (x1* , x2*) =
(-11.03, -11.03). Moreover, in Figure 7a, we overlay the trajectories on the vector field gener-
ated by the respective timescale separation parameters. As expected, the choice of T = 1 results
in a trajectory that cycles around the equilibrium in a closed curve since it is marginally stable and
Jτ (x*) has purely imaginary eigenvalues. Notably, as T grows, the cyclic behavior dissipates as
the timescale separation reshapes the vector field until the trajectory moves near directly to the zero
derivative line of the maximizing player and then follows a path along that line toward the equilib-
rium and converges rapidly. The eigenvalues of Jτ(x*) as a function ofT are presented in Figures 6c
and 6d. As was the case for the previous experiments, we observe that after the eigenvalues be-
come purely real as T grows, they then split and asymptotically converge toward the eigenvalues of
S1(J(x*)) and -TD22f(x*). It is worth noting that much of the rotational behavior in the dynamics
and vector field disappears as a result of timescale separation well before the eigenvalues become
48
Published as a conference paper at ICLR 2021
purely real; this seems to occur after the timescale separation is such that the magnitude of the real
part of the eigenvalues is greater than that of the imaginary part.
Finally, in Figure 7b, we demonstrate how the choice of timescale separation τ not only warps
the vector field but also shapes the regions of attraction around critical points. The vector field is
again shown for each τ ∈ {1, 2, 5, 20}, but now zoomed out to include each of the equilibria. The
colors overlayed on the vector field indicate the equilibria that the dynamics converge to given an
initialization at that position. Positions in the strategy space without color did not converge to an
equilibrium in the fixed horizon of 75000 iterations with γ1 = 0.001. This is explained by the fact
that the dynamics are not guaranteed to be globally convergent and may get stuck in limit cycles or
may simply move slowly for a long time in flat regions of the optimization landscape. We produced
this experiment by running τ-GDA for a dense set of initial conditions chosen uniformly over the
space of interest. It is clear from the experiment that the choice of timescale separation determines
not only the stability of equilibria, but also has a fundamental impact on the equilibria the dynamics
converge to from a given initial condition as a result of the warping of the vector field. As a concrete
example, given an initialization of (x1, x2) = (-10, -2), the dynamics with τ = 1 converge to the
differential Nash equilibria at (x1, x2) = (10.57, -8.95). However, for any τ > 1, the dynamics
instead converge to the differential Stackelberg equilibrium at (x1, x2) = (-11.03, -11.03) that
is significantly closer to the initial condition. This example motivates future work on methods for
obtaining accurate estimates of the regions of attraction around critical points and techniques to
design τ in order to explicitly shape the region of attraction around an equilibrium of interest. We
refer to the end of Section G for further discussion on potentially relevant analysis methods in this
direction.
K.4 Location Game on the Torus
We use the example in this section to further study the role of timescale separation on the regions of
attraction around critical points. Consider the zero-sum game defined by the cost
f(x1, x2) = -0.15cos(x1) + cos(x1 - x2) + 0.15 cos(x2).	(55)
This game can be interpreted as a location game on the torus. Specifically, the first player seeks to
be far from the second player but near zero, while the second player seeks to be near the first player.
This is a non-convex game on a non-convex strategy space. The critical points are given by the set13:
{x : g(x) = 0} = {(0, 0), (π, π), (π, 0), (0, π), (-1.646, -1.496), (1.646, 1.496)}.
The critical points (0, 0) and (π, π) are the only differential Stackelberg equilibrium and neither is
a differential Nash equilibrium. The differential Stackelberg equilibrium at (0, 0) is stable for all
T ∈ (T*, ∞) where T* = 0.74 and the differential Stackelberg equilibrium (∏, ∏) is stable for all
T ∈ (T*, ∞) where T = 1.35. The rest of the critical points are unstable for any choice of T. We
remark that we computed T* for each differential Stackelberg equilibrium using the construction
from Theorem 1 in Section 3 and it again gave the exact value of T* such that the system is stable
for all T > T * .
In Figure 8a, we show the trajectories of T -GDA with γ1 = 0.001 and T ∈ {1, 2, 5, 10} given the
initializations (x01, x02) = (2, -1) and (x01, x02) = (1.9, -2.1) overlayed on the vector field gener-
ated by the respective timescale separation parameters. We observe that as the timescale separation
T grows, the rotational dynamics in the vector field dissipate and the directions of movement be-
come sharp. As we mentioned in previous examples, T -GDA moves directly to the zero line of
-D2f(x1, x2) and then along that line to an equilibrium given sufficient timescale separation. The
warping of the vector field that occurs as a result of timescale separation impacts the equilibrium
that the dynamics converge to from a fixed initial condition and the neighborhood on which T-GDA
converges to an equilibrium. In other words, the region of attraction around critical points depends
heavily on the timescale separation T .
To illustrate this fact, in Figure 8b we show the regions of attraction for each choice of timescale sep-
aration. The vector fields are again shown for each T ∈ {1, 2, 5, 10}, but now with colors overlayed
indicating the equilibria that the dynamics converge to given an initialization at that position. This
13Note that because the joint strategy space is a torus, (土π, ±π) = (干∏, ±∏), (∏, 0) = (-π, 0), and
(0, -π) = (0, π).
49
Published as a conference paper at ICLR 2021
(a)
X (XKX力= (0,0) X (X*,X*) = (±∏, ± ∏)
(b)
Figure 8:	Experimental results for the torus game defined in (55) of Appendix K.4. In Figure 8a,
we overlay multiple trajectories produced by τ -GDA onto the vector field generated by the choice
of timescale separation selection τ . The shading of the vector field is dictated by its magnitude so
that lighter shading corresponds to a higher magnitude and darker shading corresponds to a lower
magnitude. Figure 8b demonstrates the effect of timescale separation on the regions of attraction
around critical points by coloring points in the strategy space according to the equilibrium τ -GDA
converges. We remark that areas without coloring indicate where τ -GDA did not converge in the
time horizon.
experiment was generated by running τ -GDA with a dense set of initial conditions chosen uniformly
over the strategy space. Positions in the strategy space without color did not converge to an equilib-
rium in the fixed horizon of 20000 iterations with γ1 = 0.04. This happens when τ -GDA is not ini-
tialized in the local neighborhood of attraction around a stable equilibrium. For the choice of τ = 1,
(0, 0) is the only stable equilibrium. However, as demonstrated in Figure 8a, τ -GDA fails to con-
verge to the equilibrium from the initial conditions (x01, x02) = (2, -1) and (x10, x20) = (1.9, -2.1).
This behavior is further demonstrated over the strategy space in Figure 8b and highlights the local
nature of the guarantees since convergence is only assured given an initialization in a suitable local
neighborhood around a stable critical point. On the other hand, τ -GDA converges to an equilibrium
from any initial condition for τ ∈ {2, 5, 10} as can be seen by Figure 8b. Notably, the equilibrium to
which the learning dynamics converge depends on the timescale separation and initial condition. To
give a concrete example, consider the initial conditions shown in Figure 8a of (x10, x20) = (2, -1) and
(x10, x02) = (1.9, -2.1). For the initial condition (x10, x20) = (2, -1), τ -GDA converges to the equi-
librium at (0, 0) for each τ ∈ {2, 5, 10}. Yet, for the initial condition (x01, x02) = (1.9, -2.1), τ -GDA
converges to the equilibrium at {(0, 0), (π, π), (π, π)} for the respective choices of τ ∈ {2, 5, 10}.
In other words, the region of attraction around the critical points changes so that from a fixed ini-
tial condition τ -GDA may converge to distinct equilibrium depending on the initial condition. From
Figure 8b, we see that the region of attraction around (x01 , x02) = (1.9, -2.1) grows from τ = 1 to
τ = 2 and τ = 4, but then shrinks at τ = 10. This example highlights that timescale separation has
a fundamental impact on the region of attraction around critical points and as τ grows it is possible
for the region of attraction around an equilibrium to shrink. Collectively, this motivates explicit
methods for trying to shape the region of attraction around desirable equilibria.
50
Published as a conference paper at ICLR 2021
Figure 9: Experimental results for the Dirac-GAN game defined in (56) of Appendix K.5. Figure 9a
shows trajectories of T-GDA for T ∈ {1, 4, 8,16} with regularization μ = 0.3 and T = 1 with
regularization μ = 1. Figure 9b shows the distance from the equilibrium along the learning paths.
Figure 9d shows the trajectories of T -GDA overlayed on the vector field generated by the respective
timescale separation and regularization parameters. The shading of the vector field is dictated by its
magnitude so that lighter shading corresponds to a higher magnitude and darker shading corresponds
to a lower magnitude.
K.5 Dirac-GAN and Regularization: Non-Saturating Formulation
In Section 5, we presented experiments for the Dirac-GAN game studied by Mescheder et al. (2017)
using the original generative adversarial network formulation of Goodfellow et al. (2014). In this
section, we revisit the Dirac-GAN game using the non-saturating generative adversarial network
formulation also proposed by Goodfellow et al. (2014). Recall that the zero-sum game which arises
from the original objective with regularization μ > 0 is defined by the cost
f(θ,ω)= '(θω)+ '(0)- 2ω2.
AS discussed in Section 5, the unique critical point of the game is (θ*, ω*) = (0, 0) and it cor-
responds to the local Nash equilibrium of the unregularized game and a differential Stackelberg
equilibrium of the regularized game. Moreover, the equilibrium is stable with respect to the conti-
nous time dynamics for all τ > 0 and μ > 0 so that the discrete time update T-GDA converges with
a suitable learning rate γ1.
The non-saturating generative adversarial network formulation proposed by Goodfellow et al. (2014)
in the context of the Dirac-GAN game corresponds to player 1 maximizing '(—θω) instead of min-
imizing '(θω). This results in the general-sum game defined by the costs
(fι(θ,ω),f2(θ,ω)) = (-'(-θω) + '(0) - 2ω2,-'(θω) - '(0) + 2ω2).	(56)
As shown by Mescheder et al. (2018), the unique critical point of the game remains at (θ* , ω*) =
(0, 0). Moreover, it can be observed that Jτ (θ* , ω*) in this formulation is identical to the game
51
Published as a conference paper at ICLR 2021
Jacobian for the Dirac-GAN, which is given by
Jτ(θ*,ω*)
0
-τ'0 (0)
'0(0)
τμ
(57)
so this game is locally equivalent to the zero-sum game that arises from the original objective pro-
posed by Goodfellow et al. (2014). This is despite the fact that the non-saturating objective was
motivated by global concerns (vanishing gradients early in the training process) rather than local
considerations. In Figure 9 we present experiments with τ -GDA for the regularized Dirac-GAN
game with the non-saturating objective and `(t) = -`(1 + exp(-t)). We observe similar behavior
as the experiments with the standard objective and refer back to Section 5 for the insights we draw
from the simulation. This experiment is primarily included for completeness and to motivate our
use of the non-saturating objective in the generative adversarial networks experiments we perform
on image datasets in Section 5.
K. 6 Generative Adversarial Network: Learning a Covariance Matrix
We now consider a generative adversarial network formulation presented by Daskalakis et al. (2018)
for learning a covariance matrix. This is a simple example with degeneracies much like the Dirac-
GAN game, but it can be generalized to arbitrary dimensional strategy spaces and has served as a
benchmark for comparing convergence rates in a number of recent papers on learning in games. Of-
ten, the example is used to show that gradient descent-ascent cycles and converges slowly. However,
by and large, timescale separation is not considered. We show that gradient descent-ascent con-
verges fast in this game with suitable timescale separation and further explore the interplay between
timescale separation, regularization, and rate of convergence. We primarily follow the notation
of Daskalakis et al. (2018) when describing the problem.
The objective of this problem is to learn a covariance matrix using the Wasserstein GAN formula-
tion. The real data x is drawn from a mean-zero multivariate normal distribution with an unknown
covariance matrix Σ. The generator is restricted to be a linear function of the random input noise
Z 〜N(0,I) and is of the form GV (z) = Vz. The discriminator is restricted to the set of all
quadratic functions, which we represent by DW (x) = x>Wx. The parameters of the generator and
the discriminator are given by W ∈ Rd×d and V ∈ Rd×d, respectively. For the given generator and
discriminator classes the Wasserstein GAN game is defined by the cost
f (V,W) = Ex〜N(0,∑)[x>Wx] - Ez〜N(0,I)[z>V>WVz].
As shown by Daskalakis et al. (2018), the cost function can be simplified to be expressed as
dd	d
f(V,W)=XXWij Σij - X VikVjk .
i=1 j=1	k=1
With this cost, the individual gradients for gradient descent-ascent are given by
g(V,W) = (-(W + W >)V, -(Σ - VV>)).
From the individual gradients, it is clear that the critical points of the game are given by (V, W) such
that VV> = Σ and W + W> = 0. Moreover, given the form of g(V, W), the game Jacobian at any
critical point (V *, W *) is of the form
JT (V *,W *)
0
-τD1>2f(V*,W*)
D12f(V*,W*)
0
Consequently, the eigenvalues of the game Jacobian are purely imaginary and the critical points
are not stable. To fix this problem, Daskalakis et al. (2018) regularized both the generator and
discriminator. We only regularize the discriminator in this example. The cost function of the zero-
sum game with regularization μ > 0 is given by
dd	d
f(V,W) = XX Wij(∑ij - X VikVjk) - 2Tr(W>W).
The individual gradients for gradient descent-ascent in this regularized game are then
g(V, W) = (-(W + W>)V, -(∑ - VV>) + μW).
(58)
52
Published as a conference paper at ICLR 2021
(d)	(e)	(f)
∣∣∑ — VVt∖∖f — — ∣∣l∕l∕+ Wt∖∖f — τ=l τ = 5	τ=10 τ = 25
(j) Legend for Figures 10a, 10b, 10c, 10g, 10h, and 10i.
Figure 10: Experimental results for the generative adversarial network formulation for learning a
covariance matrix defined by the cost from (58) of Section K.6. Figures 10a, 10b, and 10c show
the distance from the equilibrium along the learning paths of τ -GDA with d = 1. Figures 10d, 10e,
and 10f show the trajectories of the eigenvalues of JT (x*) as a function of the T, respectively.
Figures 10g, 10h, and 10i show the distance from the equilibrium along the learning paths ofτ-GDA
withd= 5, 10, 20.
We begin by considering the simplest form of this problem, which is that d = 1. The critical
points with this restriction are (V *, W *) = (σ, 0) and (V *, W *) = (-σ, 0) and the game Jacobian
evaluated at them is
JT(V*,W*) = [2°σ :2σ .
Each critical point is a local Nash equilibrium of the unregularized game and a differential Stackel-
berg equilibrium of the regularized game since —D2 f (V *, W *) = μ > 0 and S1 (J (V *, W *))=
4σ2/μ > 0. Furthermore, SPeC(JT(V *, W *)) = {(τμ ± pτ2 μ2 - 16τσ2 )/2} so that each critical
point is stable for all T ∈ (0, ∞) and μ ∈ (0, ∞) since SPeC(JT(θ*,ω*)) ⊂ C*. Thus, given a
suitably chosen learning rate γ1, the discrete time update τ-GDA locally converges to an equilib-
rium. For this reason, we focus on studying the rate of convergence for the problem as a function
53
Published as a conference paper at ICLR 2021
(a) d = 1, μ = 0.5
(b) d = 1,μ = 0.75
(c) d = 1, μ = 1
Figure 11: Experimental results for learning a covariance matrix defined by the cost from (58) of
Section K.6. We overlay the trajectories produced by τ -GDA onto the vector field generated by the
choices of T and μ. The shading of the vector field is dictated by its magnitude so that lighter shading
corresponds to a higher magnitude and darker shading corresponds to a lower magnitude.
of timescale separation and regularization. Figures 10a, 10b, and 10c show the distance from an
equilibrium along the learning path of τ -GDA with τ ∈ {1, 5, 10, 25} given a fixed initial condition
with learning rate γ1 = 0.001 and regularization μ ∈ {0.5,0.75, l}, respectively. Moreover, Fig-
ures 10d, 10e, and 10f show the trajectories of the eigenvalues for JT (V *, W *) as a function of T
for the regularization parameters μ ∈ {0.5,0.75,1}. Finally, Figures 11a, 11b, and 11c show the
trajectories of T -GDA overlayed on the vector field generated by the respective timescale separation
and regularization parameters.
From the eigenvalue trajectories, We see that as μ grows, the eigenvalues become purely real at a
smaller value of T. Moreover, as μ increases, the magnitude of the real and imaginary parts of the
eigenvalues decreases. We observe the effect of this on the convergence, where the dynamics do not
cycles as much for larger μ. Again, We see the trade-off between timescale separation, regularization,
and convergence. For example, despite the eigenvalues being purely real with μ = 1 and T = 25
so that there is no rotational dynamics, the convergence is slower than for μ = 0.75 where there is
some non-zero imaginary piece of the eigenvalues.
Figures 10g, 10h, and 10i show the distance from a critical point along the learning path of T -GDA
with T ∈ {1, 5, 10, 25} given a fixed initial condition with learning rate γ1 = 0.001, regularization
μ = 1, and the dimension of the problem d among the set {5,10, 20}, respectively. The primary
purpose of showing this set of results is simply to be clear that the behavior for d = 1, which is easier
to explain and visualize, transfers over to higher dimensional formulations of this problem. This is
to be expected since the problem dimension is not necessarily fundamental to the convergence rate,
but rather it depends on the conditioning of Σ and each Σ was chosen so that the behavior was
comparable for each choice of dimension.
54
Published as a conference paper at ICLR 2021
K.7 Generative Adversarial Networks Parameterized by Neural Networks
In this section, we provide a much more detailed discussion of our experiments training generative
adversarial networks on image datasets than was included in Section 5 and also present more the
results at greater depth. We also present experiments training a generative adversarial network with
τ -GDA to learn a mixture of Gaussians.
Background. The empirical benefits of training with a timescale separation have been documented
previously. For example, Heusel et al. (2017) showed on a number of image datasets that a timescale
separation between the generator and discriminator improves generation performance as measured
by the Frechet Inception Distance (FID). Since then a significant number of papers have presented
results training generative adversarial networks with timescale separation. Moreover, it is common
in the literature for the discriminator to be updated multiple times between each update of the gen-
erator (Arjovsky et al., 2017). Indeed, it has been widely demonstrated that this heuristic improves
the stability and convergence of the training process and locally it has a similar effect as including a
timescale separation between the generator and discriminator. The disadvantage of this approach is
that the number of gradient calls per generator update increases and consequently the convergence
is then slower in terms of wall clock time when a similar effect could potentially be achieved by a
learning rate separation between the generator and discriminator. We remark that it appears to be
reasonably common for practitioners to fix a shared learning rate for the generator and discriminator
along with a pre-selected number of discriminator updates per generator update and not thoroughly
investigate the impact timescale separation has on the training process.
The goal of our generative adversarial network experiments is to reinforce the importance of the
timescale separation between the generator and the discriminator as a hyperparameter in the training
process, demonstrate how it changes the behavior along the learning path, and show that it is com-
patible with a number of common training heuristics. This is to say that our goal is not necessarily to
show state-of-the art performance, but rather to perform experiments that allow us to make insights
relevant to the theory in this paper. We remark that our empirical work on training generative adver-
sarial networks is distinct from and complimentary to that of Heusel et al. (2017) in several ways.
The theory given by Heusel et al. (2017) only applies to stochastic stepsizes, however in the exper-
iments they implemented constant step sizes. We train with mini-batches and decaying stepsizes in
our image dataset experiments, which does satisfy the theory we provide as detailed in Section H.
Moreover, by and large, the experiments by Heusel et al. (2017) compare a fixed learning rate ratio
between the generator and discriminator to multiple fixed shared learning rates for the generator and
discriminator. In contrast, we fix a learning rate for the generator and explore the behavior of the
training process as the timescale parameter τ is swept over a given range.
K.7.1 Generative Adversarial Networks: Mixture of Gaussians
We now provide the results from training generative adversarial networks to learn a mixture of
Gaussians. The underlying data distribution consists of Gaussian distributions configured in a circle
arrangement with means given by μ = [sin(ω), cos(ω)] for ω ∈ {kπ∕4}k=°, each with covariance
σ2I where σ2 = 0.05. Each sample of real data given to the discriminator is selected uniformly at
random from the set of Gaussian distributions. We train the generator using latent vectors z ∈ R16
sampled from a standard normal distribution in each training batch. The batch size for each player in
the game is 512. The network for the generator and discriminator contain two and one hidden layers
respectively, each which contain 32 neurons and ReLU activation functions. The training objective
is the non-saturating objective and we run experiments without and with the R1 gradient penalty
proposed by Mescheder et al. (2018) using parameter μ = 0.1. The generator learning rate is fixed
to be γ1 = 0.005 and the discriminator learning rate is fixed as γ2 = τγ1 where we experiment with
T ∈ {4, 8,16, 32, 64,100}. For each parameter choice (timescale separation T and regularization μ),
the experiment is repeated with 50 random seeds. The training does not rely on any adaptive gradient
methods (Adam, RMSprop, etc.) and is the ‘vanilla’ stochastic T -GDA dynamics. We evaluate the
performance along the learning path by computing the KL-divergence between the generated data
and the real data, where we sample 4096 data points from each.
The results of this experiment are presented in Figure 12. We show the mean of the KL-divergence
and the standard error of the means across the runs along the learning path without (μ = 0) and
with regularization (μ = 0.1) in Figures 12a and 12b, respectively. Moreover, Figures 12c and 12d
55
Published as a conference paper at ICLR 2021
(a) μ = 0: Mean KL Divergence	(b) μ = 0.1: Mean KL Divergence
IOOOO 20000	30000	40000	50000	60000	10000	20000	30000	40000	50000	60000
Number of Mini-Batch Updates	Number of Mini-Batch Updates
(c) μ = 0: Median KL Divergence	(d) μ = 0.1: Median KL Divergence
Figure 12: KL-divergence between generated and real data for a mixture of Gaussians.
show the medians of the KL-divergence across the runs without (μ = 0) and with regularization
(μ = 0.1), respectively. From Figure 12a, We observe that the choices of T = 4 and T = 100 do not
show on the plot since they perform poorly, which may be a result of equilibrium not being stable
for T = 4 and numerically conditioning for T = 100. Furthermore, we see that timescale separa-
tion improves the results up to a reasonable timescale parameter and after which the performance
degrades. Furthermore, Figure 12b reveals that the results are improved with regularization and we
see that T = 100 ends up performing well, potentially since the regularization can alleviate some of
the problems of numerical stability. In general, we draw similar conclusions from the median scores
as reported in Figures 12c and 12d.
The primary purpose of this experiment is to train generative adversarial networks parameterized by
neural networks using T -GDA without heuristics such as adaptive gradient methods or parameter av-
eraging as is employed on the image dataset experiments from Section 5 and expanded on further in
Appendix K.7.2. Notably, we see that consistent themes emerge that timescale separation improves
convergence until hitting a limiting value and regularization can improve the rate of convergence but
there is an interplay with the timescale separation.
K.7.2 Generative Adversarial Networks: Image Datasets
We now provide further details on the methods for the experiments training generative adversarial
networks with image datasets presented in Section 5 along with more in-depth results.
Methods. We again note that we built our experiments based on the methods and implementa-
tions of Mescheder et al. (2018) and used the publicly available code from the paper available
at https://github.com/LMescheder/GAN_stability. We effectively only changed
the learning rates, retained multiple exponential averages at once, and modified the updates to
be simultaneous in the code. In Figure 17 we provide the network architectures from our exper-
56
Published as a conference paper at ICLR 2021
iments and in Figure 18 we include the hyperparameters that were selected. The architectures
are analogous to that reported in Mescheder et al. (2018), but scaled down since we run exper-
iments with 32 × 32 × 3 images. For evaluation, we computed the Frechet Inception Distance
using 10k samples from the real and generated data. For both experiments and across the set
of hyperparameters we performed the evaluation using a fixed random noise vector to make for
an equal comparison and a fixed set of real images which were randomly selected. The evalua-
tion was done using the training data. We used the FID score implementation in pytorch available
at https://github.com/mseitzer/pytorch-fid.
We train the generative adversarial networks with the non-saturating objective function and the R1
gradient penalty proposed by Mescheder et al. (2018) with regularization parameters μ ∈ {1,10}.
We note that the non-saturating objective results in a game that is not zero-sum, however it is com-
monly used in practice and under the realizable assumptions it can be locally equivalent to the
zero-sum objective as discussed in Section K.5. The theory we provide does not apply to using
RMSprop, but it is ubiquitous in practice for training generative adversarial networks and we are
interested in exploring the interplay of timescale separation with common heuristics to understand
if similar conclusions hold when using them as from the previous experiments regarding timescale
separation with the ‘vanilla’ τ -GDA dynamics. Moreover, we note that similarly Heusel et al. (2017)
and Mescheder et al. (2018) also rely upon Adam or RMSprop in generative adversarial experiments.
A final heuristic and hyperparameter that we explore in conjunction with the timescale separation τ
is that of using an exponential moving average to produce the model that is evaluated. This means
that at each update k, given that the parameters of the generator are given by x1,k, the moving av-
erage Xk = χι,kβ + Xι,k-ι(1 - β) is kept where β ∈ (0,1). Experimental studies have shown
that this heuristic can yield a significant improvement in terms of both the inception score and the
FID (Gidel et al., 2019a; Yazici et al., 2019). The success of this method is thought to be a result of
dampening both rotational dynamics and the noise from the randomness in the mini-batches of data.
Experimental Results. We run the training algorithm with the learning rate ratio τ belonging to
the set {1, 2, 4, 8} for CIFAR-10 and {1, 2, 4, 8, 16} for CelebA along with the regularization pa-
rameter μ belonging to the set {1,10}. For each choice of T and μ, We retain exponential moving
averages of the generator parameters forβ ∈ {0.99, 0.999, 0.9999}. The training process is repeated
2 times for each hyperparameter configuration in the CIFAR-10 experiments and the experiments
with CelebA are simulated once. The performance is evaluated along the learning path at every
10,000 updates in terms of the FID score. We report the mean scores and the standard error of the
mean over the repeated experiments for each dataset. The FID score is such that a lower score beats
a higher score. The experiments are computationally intensive which limits the number of repeats
of experiments that can be simulated, however, we observed that the scores were quite consistent
between random seeds particularly with exponential averaging of the parameters. We run the ex-
periments with μ = 1 for 150k mini-batch updates and the experiments with μ = 10 for 300k
mini-batch updates.
The results for each dataset across the hyperparameter configurations are presented in numeric form
in Figure 15. Figure 16 shows some generated samples selected at random for each dataset with the
hyperparameter configuration that performed best in terms of the FID score at the end of the training
process. We now describe the key observations from the experiments for each dataset.
CIFAR-10. The FID scores along the learning path for CIFAR-10 with μ = 10 and μ = 1 are
presented in Figures 13a and 13b, respectively. The corresponding scores in numeric form are
given in Figures 15a, 15c, and 15e for μ = 10 at 150k iterations and μ = 1 at 150k and 300k
iterations, respectively. To begin, we observe that the exponential moving average significantly
improves performance, and of the parameters considered, β = 0.9999 performed best. This may
be a result of removing noise as mentioned previously or potentially it could be from dampening
oscillatory behavior in the dynamics. Moreover, we that timescale separation also has a significant
impact on the FID score of the training process. Indeed, even selecting τ = 2 versus τ = 1 can
yield an impressive performance gain. In this experiment for each regularization parameter, τ = 4
converges fastest and performs the best. We see that T = 2 outperforms T = 8 when μ = 10 and
the relationship is flipped when viewing the evaluation at 150k updates with μ = 1 and then returns
back when looking at the evaluation at 300k updates. The choice of T = 1 performs the worst
for each regularization parameter by a wide margin. Finally, observe that the performance with
57
Published as a conference paper at ICLR 2021
Number of Mini-Batch Updates X IOk
(a) μ = 10
(b) μ = 1
Figure 13: CIFAR-10 FID scores with regularization μ = 10 in Figure 13a and μ = 1 in Figure 13b.
regularization μ = 1 is much better than with regularization μ = 10 for each timescale separation
parameter and exponential averaging parameter.
CelebA. The FID scores along the learning path for CIFAR-10 with μ = 10 and μ = 1 are
presented in Figures 14a and 14b, respectively. The corresponding scores in numeric form are
given in Figures 15b, 15d, and 15f for μ = 10 at 150k iterations and μ = 1 at 150k and 300k
iterations, respectively. In this experiment we observe that while the exponential moving average
helps performance, the gain is not as drastic as it was for CIFAR-10. It is not entirely clear if this
is a consequence of the scores being lower or something fundamental to the optimization landscape
and dynamics for the dataset. The timescale separation in combination with the regularization again
has a major effect on the the FID score of the training process in this experiment. For regularization
μ = 10, the timescale parameters of T = 4 and T = 8 outperform T = 1, T = 2, and T = 16 by a
wide margin, again highlighting that timescale separation can speed up convergence until a certain
point where it can potentially slow it down owing to the effect on the conditioning of the problem
locally. A similar trend can be observed with regularization μ = 1, but with T = 16 performing
closer to T = 4 and T = 8. For each regularization parameter and timescale parameter, we see
that T = 8 performs the best. We again observe in this experiment that for all timescale separation
parameters, the performance is significantly improved with regularization μ = 1 as compared with
μ = 10. This once again highlights the importance of considering how this the hyperparameters of
regularization and timescale interact and dictate the local convergence rates.
Summary. In summary, we took a well-performing method and implementation for training gen-
erative adversarial networks and demonstrated that timescale separation is an extremely important,
and easy to implement, hyperparameter that is worth careful consideration since it can have a major
impact on the convergence speed and final performance of the training process. Interestingly, the
conclusions we draw are in line with the insights drawn from the simple Dirac-GAN experiment
in Section 5 and from the mixture of Gaussian experiments from Appendix K.7.1. In particular,
timescale separation only speeds up to convergence until hitting a limiting value and there is a key
interplay between timescale separation, regularization, and convergence rate.
58
Published as a conference paper at ICLR 2021
Number of Mini-Batch Updates X IOk
(a) μ = 10
(b) μ = 1
Figure 14: CelebA FID scores with regularizationμ = 10 in Figure 14a and μ = 1 in Figure 14b.
					T\8		0.99	—		0.999	—		0.9999 —
T\户	0.99	0.999	0.9999								
														
1 ^^	39.18 ± 1.23-	:37.55 ± 1.27"	:37.04 ± 1.46Z=		1		16.08 土 0.44		14.9 土 0.26		14.63 土 0.26
					2		12.46 ± 0.05		11.85 土 0.11		11.72 土 0.11
2	一 29.33 ± 0.65	一 27.84 ± 0.4	一 27.14 ± 0.34								
					4		11.24 土 0.13		10.9 土 0.12		10.72 土 0.13
4^^	27.91 ± 0.17	27.14 ± 0.22	26.26 ± 0.25								
					8		10.62 土 0.22		10.16 土 0.25		10.08 土 0.25
8	^ 30.99 ± 0.39-	一 29.86 ± 0.34-	一 29.07 ± 0.1								
					16		12.4 ± 0.28~~		11.64 土 0.05~		11.22 土 0.0厂
											
(a) CIFAR-10 FID at 150k updates With μ = 10					(b) CelebA FID at 150k updates with μ = 10						
					T\e		0.99		0.999		0.9999
T\「	0.99	0.999	0.9999								
					1		:13.98 土 0.5F=		13.38 土 0.3丁:		13.13 土 0.29=
1 ^一	28.87 ± 0.92=	27.47 ± 1.If=	26.69 ± 1.02"								
					2		10.51 土 0.28		10.29 土 0.32		10.33 土 0.36
2^一	24.18 ± 0.28	22.65 ± 0.15	21.63 ± 0.12								
					4		9.01 土 0.27		8.83 土 0.25		8.78 土 0.26
4^一	22.38 ± 0.36	21.05 ± 0.21	20.12 ± 0.13								
					8		-8.32 土 0.1		8.04 土 0.14		7.98 土 0.15
8 .	22.74 ± 0.15~	21.71 ± 0.1Γ^	20.72 ± 0.08-								
					16		8.47 土 0.0厂		8.19 土 0.07—		8.13 土 0.07—
											
(c) CIFAR-10 FID at 150k updates with μ = 1						(d) CelebA FID at 150k updates with μ = 1					
						T ∖β		0.99		0.999	0.9999
τ∖β	0.99	0.999	0.9999								
										ΣΣ		-				
1 ^一	24.46 ± 0.32=	23.14 ± 0.f	21.59 ± 0.f			1		9.16 士 0.29		8.// 土 0.25	8.52 土 0.22
						2		7.22 士 0.11		6.91 士 0.19	-6.82 士 0.18
2^一	21.37 ± 0.11	19.51 ± 0.07	18.08 ± 0.19								
						4		6.47 士 0.13		6.2 士 0.11	-6.07 士 0.10
4^一	20.75 ± 0.19	19.08 ± 0.08	17.61 ± 0.18								
						8		6.25 士 0.02		5.95 士 0.05	5.81 士 0.05
8 .	21.94 ± 0.16-	19.87 ± 0.1—	18.64 ± 0.08-								
						16		6.65 士 0.12—		6.35 士 0.11 一	-6.17 士 0.06-
											
(e) CIFAR-10 FID at 300k updates with μ = 1						(f) CelebA FID at 300k updates with μ = 1					
Figure 15:	FID Scores on CIFAR-10 and CelebA.
59
Published as a conference paper at ICLR 2021
(a)	CIFAR-10 generated sample images
(b)	CelebA generated sample images
Figure 16:	Generated sample images with τ = 4 and β = 0.9999
Layer	Output Size	Filter
Fully Connected Reshape	-512 ∙ 4∙4 512 × 4 × 4	256 → 512 ∙ 4 ∙ 4
Resnet-Block NN-Upsampling	256 × 4 × 4― 256 × 8 × 8	512 → 256 → 256
Resnet-Block NN-Upsampling	128 × 8 × 8― 128 × 16 × 16	256 → 128 → 128
Resnet-Block NN-Upsampling	64 × 16 × 16 64 × 32 × 32	128 → 64 → 64
Resnet-Block Conv2D	64 × 32 × 32 3 × 64 × 64	-64 → 64 → 64- 64 1 3
Layer	Output Size	Filter
Conv2D	64 × 32 × 32	3 — 64
Resnet-Block Avg-Pool2D	64 × 32 × 32 64 × 16 × 16	-64 → 64 → 64-
Resnet-Block Avg-Pool2D	128 × 16 × 16 128 × 8 × 8	64 → 64 → 128
Resnet-Block Avg-Pool2D	256 × 8 × 8- 256 × 4 × 4	128 → 128 → 256
Resnet-Block Fully Connected	512 × 4 × 4- 512 ∙ 4∙4	256 → 256 → 512 512 ∙ 4 ∙ 4 → 1
(b) Discriminator Network Architecture
(a) Generator Network Architecture
Figure 17: Network Architectures for GAN experiments on CIFAR-10 and CelebA
Hyperparameter	Value(s)
Objective	NSGAN
Batch Size	64
Latent Distribution	Z ∈ R256
Generator Learning Rate	CIFAR-10: 0.0001; CelebA: 0.00005
Timescale Separation T	CIFAR-10: {1, 2, 4, 8}; CelebA: {1, 2, 4, 8,16}
Learning Rate Decay	(1+ x)-0∙005
Optimizer	RMSprop
RMSprop Smoothing Constant a	0.99
RMSprop e	10-8
Regularization μ	{1,10}
EMA Parameter β		{0.99, 0.999, 0.9999}	
Figure 18: Hyperparameters for GAN experiments on CIFAR-10 and CelebA
L Alternative Proof of Theorem 3 via T* Construction FROM
Theorem 1
in order to highlight the utility of Theorem 1 along with future directions of obtaining values of
T* for structured games, we revisit the proof of Theorem 3 and derive the result directly from the
construction. The purpose of this section is to illustrate that the structure of equilibria considered in
Theorem 3 can be exploited to obtain the value of τ* for the entire class of games using properties
of the Kronecker product and sum.
Consider the generative adversarial network example under Assumption 1. Then, we can show
precise from the construction that for all games of this class that T* = 0 as long as μ > 0. Fix a
60
Published as a conference paper at ICLR 2021
0	-B
tB T T (μR + C)
-Jτ,*(x*)
regularization parameter μ > 0. Then,
A11	A12
-τA12 TA22
Note that A22 < 0 and A11 is the zero matrix and
S1 = B(μR + C)-1Bτ = A11 - A12A221A>2 < 0
by Assumption 1. Now, We can conclude for this entire class of games that T* = 0 precisely because
Q has no positive real roots. Indeed, to start recall from the proof that we need to find
T* = λmaχ(-M2(A11 0 In2 + MI))
'-----------------V--------'
Q
where
M1 = -2Hn+2 (-AT2 0 ln2 )(A22 田 A22) I(A12 0 ln2 )H∏2
and
M2 = Ini 0 A221 - 2(Inι 0 A22lAT2)Hn1 (S1 田 SI)THnI (Ini 0 A12A2⅛)
Plugging in the generative adversarial network parameters we see that we need
T * = λ1Lx(-M2 M1)
where
M1 =2H+2 (B 0 In2)((μR + C)田(μR + C))-1(Bτ 0 In2)Hn2 < 0
and
M2 = Ini 0 (μR + C)T - 2(Inι 0 (μR + C)-1Bτ)Hnι
•	(B(μR + C)-1BT 田 B(μR + C)TBT)THnI (In1 0 B(μR + C)T)
For later use, let us define
M3 = -2(Inι 0 (μR + C)-1Bτ)Hnι
•	(B(μR + C)-1Bτ 田 B(μR + C)TBT)TH+i (In1 0 B(μR + C)T)
=-2(Inι 0 (μR + C)-1Bτ)Hnι (H+1 (B(μR + C)TBτ 0 m
+ In1 0 B(μR + C)TBT)Hn1)TH+1 (In1 0 B(μR + C)-1).
Hence, as long as M2 ≥ 0 we know that T* = 0. Here is where we can exploit the properties
from Magnus (1988) on the 田 and 0 operators. Specifically, let us use the property that H+l (G 0
InI)HnI = H+1 (In1 0 G)Hn1 = 2H+1 (屈 0 G + G 0 InI)HnI for a matrix G which in turn
implies that
(HnI (In1 0 G)HnI)T= 2H+1 (In 0 G + G 0 InI)THnI
to get the following:
2Hn1 (B(μR + C)-1Bτ 田 B(μR + C)TBT)THnI = (H+ (瓯 0 B(μR + C)TBT)HnI)T
Hence,
M3 = -(InI 0 (μR + C)-1BT)(InI 0 B(μR + C)-1Bτ)-1 (In1 0 B(μR + C)-1)
=-(InI 0 (μR + C)-1BT)(InI 0 (B(μR + C)TBT)T)(InI 0 B(μR + C)-1)
=-(InI 0 (μR + C)-1BT(B(μR + C)TBT)T)(InI 0 B(μR + C)-1)
=-(InI 0 B+)(In1 0 B(μR + C)-1)
=-(InI 0 (μR + C)-I)
which clearly holds since B is full rank in general by Assumption 1, for which it is necessary that
n2 ≥ n1/2 by Proposition E.1.
61