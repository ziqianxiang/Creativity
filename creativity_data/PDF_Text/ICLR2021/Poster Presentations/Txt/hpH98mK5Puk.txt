Published as a conference paper at ICLR 2021
InfoBERT: Improving Robustness of Language
Models from An Information Theoretic
Perspective
*Boxin Wang1, Shuohang Wang2, Yu Cheng2, Zhe Gan2, Ruoxi Jia3, Bo Li1, Jingjing Liu2
1 University of Illinois at Urbana-Champaign 2 Microsoft Dynamics 365 AI Research 3 Virginia Tech
{boxinw2,lbo}@illinois.edu {shuohang.wang,yu.cheng,zhe.gan,jingjl}@microsoft.com
Ab stract
Large-scale pre-trained language models such as BERT and RoBERTa have
achieved state-of-the-art performance across a wide range of NLP tasks. Re-
cent studies, however, show that such BERT-based models are vulnerable fac-
ing the threats of textual adversarial attacks. We aim to address this problem
from an information-theoretic perspective, and propose InfoBERT, a novel learn-
ing framework for robust fine-tuning of pre-trained language models. InfoBERT
contains two mutual-information-based regularizers for model training: (i) an
Information Bottleneck regularizer, which suppresses noisy mutual information
between the input and the feature representation; and (ii) an Anchored Feature
regularizer, which increases the mutual information between local stable fea-
tures and global features. We provide a principled way to theoretically analyze
and improve the robustness of language models in both standard and adversar-
ial training. Extensive experiments demonstrate that InfoBERT achieves state-
of-the-art robust accuracy over several adversarial datasets on Natural Language
Inference (NLI) and Question Answering (QA) tasks. Our code is available at
https://github.com/AI-secure/InfoBERT.
1	Introduction
Self-supervised representation learning pre-trains good feature extractors from massive unlabeled
data, which show promising transferability to various downstream tasks. Recent success includes
large-scale pre-trained language models (e.g., BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;
Liu et al., 2019; Brown et al., 2020)), which have advanced state of the art over a wide range of
NLP tasks such as NLI and QA, even surpassing human performance. Specifically, in the computer
vision domain, many studies have shown that self-supervised representation learning is essentially
solving the problem of maximizing the mutual information (MI) I(X; T) between the input X and
the representation T (van den Oord et al., 2018; Belghazi et al., 2018; Hjelm et al., 2019; Chen
et al., 2020). Since MI is computationally intractable in high-dimensional feature space, many MI
estimators (Belghazi et al., 2018) have been proposed to serve as lower bounds (Barber & Agakov,
2003; van den Oord et al., 2018) or upper bounds (Cheng et al., 2020) of MI. Recently, Kong et al.
point out that the MI maximization principle of representation learning can be applied to not only
computer vision but also NLP domain, and propose a unified view that recent pre-trained language
models are maximizing a lower bound of MI among different segments of a word sequence.
On the other hand, deep neural networks are known to be prone to adversarial examples (Goodfel-
low et al., 2015; Papernot et al., 2016; Eykholt et al., 2017; Moosavi-Dezfooli et al., 2016), i.e., the
outputs of neural networks can be arbitrarily wrong when human-imperceptible adversarial pertur-
bations are added to the inputs. Textual adversarial attacks typically perform word-level substitution
(Ebrahimi et al., 2018; Alzantot et al., 2018; Ren et al., 2019) or sentence-level paraphrasing (Iyyer
et al., 2018; Zhang et al., 2019) to achieve semantic/utility preservation that seems innocuous to
human, while fools NLP models. Recent studies (Jin et al., 2020; Zang et al., 2020; Nie et al., 2020;
Wang et al., 2020) further show that even large-scale pre-trained language models (LM) such as
*Work was done during Boxin Wang's Summer internship in Microsoft Dynamics 365 AI Research.
1
Published as a conference paper at ICLR 2021
BERT are vulnerable to adversarial attacks, which raises the challenge of building robust real-world
LM applications against unknown adversarial attacks.
We investigate the robustness of language models from an information theoretic perspective, and
propose a novel learning framework InfoBERT, which focuses on improving the robustness of lan-
guage representations by fine-tuning both local features (word-level representation) and global fea-
tures (sentence-level representation) for robustness purpose. InfoBERT considers two MI-based
regularizers: (i) the Information Bottleneck regularizer manages to extract approximate minimal
sufficient statistics for downstream tasks, while removing excessive and noisy information that may
incur adversarial attacks; (ii) the Anchored Feature regularizer carefully selects useful local stable
features that are invulnerable to adversarial attacks, and maximizes the mutual information between
local stable features and global features to improve the robustness of the global representation. In
this paper, we provide a detailed theoretical analysis to explicate the effect of InfoBERT for robust-
ness improvement, along with extensive empirical adversarial evaluation to validate the theory.
Our contributions are summarized as follows. (i) We propose a novel learning framework InfoBERT
from the information theory perspective, aiming to effectively improve the robustness of language
models. (ii) We provide a principled theoretical analysis on model robustness, and propose two MI-
based regularizers to refine the local and global features, which can be applied to both standard and
adversarial training for different NLP tasks. (iii) Comprehensive experimental results demonstrate
that InfoBERT can substantially improve robust accuracy by a large margin without sacrificing the
benign accuracy, yielding the state-of-the-art performance across multiple adversarial datasets on
NLI and QA tasks.
2	Related Work
Textual Adversarial Attacks/Defenses Most existing textual adversarial attacks focus on word-
level adversarial manipulation. Ebrahimi et al. (2018) is the first to propose a whitebox gradient-
based attack to search for adversarial word/character substitution. Following work (Alzantot et al.,
2018; Ren et al., 2019; Zang et al., 2020; Jin et al., 2020) further constrains the perturbation search
space and adopts Part-of-Speech checking to make NLP adversarial examples look natural to human.
To defend against textual adversarial attacks, existing work can be classified into three categories:
(i) Adversarial Training is a practical method to defend against adversarial examples. Existing work
either uses PGD-based attacks to generate adversarial examples in the embedding space of NLP as
data augmentation (Zhu et al., 2020a), or regularizes the standard objective using virtual adversarial
training (Jiang et al., 2020; Liu et al., 2020; Gan et al., 2020). However, one drawback is that
the threat model is often unknown, which renders adversarial training less effective when facing
unseen attacks. (ii) Interval Bound Propagation (IBP) (Dvijotham et al., 2018) is proposed as a new
technique to consider the worst-case perturbation theoretically. Recent work (Huang et al., 2019; Jia
et al., 2019) has applied IBP in the NLP domain to certify the robustness of models. However, IBP-
based methods rely on strong assumptions of model architecture and are difficult to adapt to recent
transformer-based language models. (iii) Randomized Smoothing (Cohen et al., 2019) provides a
tight robustness guarantee in `2 norm by smoothing the classifier with Gaussian noise. Ye et al.
(2020) adapts the idea to the NLP domain, and replace the Gaussian noise with synonym words to
certify the robustness as long as adversarial word substitution falls into predefined synonym sets.
However, to guarantee the completeness of the synonym set is challenging.
Representation Learning MI maximization principle has been adopted by many studies on self-
supervised representation learning (van den Oord et al., 2018; Belghazi et al., 2018; Hjelm et al.,
2019; Chen et al., 2020). Specifically, InfoNCE (van den Oord et al., 2018) is used as the lower
bound of MI, forming the problem as contrastive learning (Saunshi et al., 2019; Yu et al., 2020).
However, Tian et al. (2020) suggests that the InfoMax (Linsker, 1988) principle may introduce ex-
cessive and noisy information, which could be adversarial. To generate robust representation, Zhu
et al. (2020b) formalizes the problem from a mutual-information perspective, which essentially per-
forms adversarial training for worst-case perturbation, while mainly considers the continuous space
in computer vision. In contrast, InfoBERT originates from an information-theoretic perspective
and is compatible with both standard and adversarial training for discrete input space of language
models.
2
Published as a conference paper at ICLR 2021
3	INFOBERT
Before diving into details, we first discuss the textual adversarial examples we consider in this paper.
We mainly focus on the dominant word-level attack as the main threat model, since it achieves higher
attack success and is less noticeable to human readers than other attacks. Due to the discrete nature
of text input space, it is difficult to measure adversarial distortion on token level. Instead, because
most word-level adversarial attacks (Li et al., 2019; Jin et al., 2020) constrain word perturbations via
the bounded magnitude in the semantic embedding space, by adapting from Jacobsen et al. (2019),
we define the adversarial text examples with distortions constrained in the embedding space.
Definition 3.1. (-bounded Textual Adversarial Examples). Given a sentence x = [x1; x2; ...; xn],
where xi is the word at the i-th position, the -bounded adversarial sentence x0 = [x01; x02; ...; x0n]
for a classifier F satisfies: (1) F(x) = o(χ) = o(χ0) but F(χ0) = o(χ0), where o(∙) is the oracle
(e.g., human decision-maker); (2) ||ti - t0i||2 ≤ for i = 1, 2, ..., n, where ≥ 0 and ti is the word
embedding of xi .
3.1	Information Bottleneck as a Regularizer
In this section, we first discuss the general IB implementation, and then explain how IB formulation
is adapted to InfoBERT as a regularizer along with theoretical analysis to support why IB regularizer
can help improve the robustness of language models. The IB principle formulates the goal of deep
learning as an information-theoretic trade-off between representation compression and predictive
power (Tishby & Zaslavsky, 2015). Given the input source X, a deep neural net learns the internal
representation T of some intermediate layer and maximizes the MI between T and label Y , so that
T subject to a constraint on its complexity contains sufficient information to infer the target label Y .
Finding an optimal representation T can be formulated as the maximization of the Lagrangian
LIB = I (Y; T) - βI (X; T),	(1)
where β > 0 is a hyper-parameter to control the tradeoff, and I(Y; T) is defined as:
I (Y; T) = P p(y, t)iog p(y2 dy dt.	(2)
p(y)p(t)
Since Eq. (2) is intractable, we instead use the lower bound from Barber & Agakov (2003):
I(Y; T) ≥	p(y,t)logqψ(y | t) dydt,	(3)
where qψ (y|t) is the variational approximation learned by a neural network parameterized by ψ for
the true distribution p(y|t). This indicates that maximizing the lower bound of the first term of IB
I(Y; T) is equivalent to minimizing the task cross-entropy loss `task = H(Y | T).
To derive a tractable lower bound of IB, we here use an upper bound (Cheng et al., 2020) of I(X; T)
p(x,t)log(p(t|x))dxdt-Zp(x)p(t)log(p(t|x))dxdt.	(4)
By combining Eq. (3) and (4), we can maximize the tractable lower bound LIB ofIB in practice by:
I(X; T) ≤ /
N	βN	N
LIB = N X [logqψ(y(i) | t(i))] - N X [log(p(t(i) |Xci)))- N Xlog(p(t(j) | x(i)))]⑸
i=1	i=1	j=1
with data samples {x(i), y(i)}iN=1, where qψ can represent any classification model (e.g., BERT), and
p(t | x) can be viewed as the feature extractor fθ : X → T, where X and T are the support of the
input source X and extracted feature T, respectively.
The above is a general implementation of IB objective function. In InfoBERT, we consider T as
the features consisting of the local word-level features after the BERT embedding layer fθ . The
following BERT self-attentive layers along with the linear classification head serve as qψ(y|t) that
predicts the target Y given representation T.
Formally, given random variables X = [X1; X2; ...; Xn] representing input sentences with Xi (word
token at i-th index), let T = [Ti;...; Tn] = fθ([Xι; X2;...; Xn]) = [fθ(Xi); fθ(X2);…;fθ(Xn)]
3
Published as a conference paper at ICLR 2021
denote the random variables representing the features generated from input X via the BERT em-
bedding layer fθ , where Ti ∈ Rd is the high-dimensional word-level local feature for word Xi .
Due to the high dimensionality d of each word feature (e.g., 1024 for BERT-large), when the sen-
tence length n increases, the dimensionality of features T becomes too large to compute I(X; T ) in
practice. Thus, we propose to maximize a localized formulation of IB LLIB defined as:
n
LLIB ：= I (Y; T) - nβ XI (Xi； Ti).	(6)
i=1
Theorem 3.1.	(Lower Bound of LIB) Given a sequence of random variables X = [X1; X2; ...; Xn]
and a deterministic feature extractor fθ, let T = [T1; ...; Tn] = [fθ (X1); fθ (X2); ...; fθ (Xn)]. Then
the localized formulation of IB LLIB is a lower bound of LIB (Eq. (1)), i.e.,
n
I (Y ； T) - βI (X； T) ≥ I (Y ； T) - nβ XI (Xi； Ti).	⑺
i=1
Theorem 3.1 indicates that we can maximize the localized formulation of LLIB as a lower bound of
IB LIB when I(X； T) is difficult to compute. In Eq. (6), if we regard the first term (I(Y； T)) as a
task-related objective, the second term (-nβ Pin=1 I(Xi； Ti)) can be considered as a regularization
term to constrain the complexity of representation T, thus named as Information Bottleneck regular-
izer. Next, we give a theoretical analysis for the adversarial robustness of IB and demonstrate why
localized IB objective function can help improve the robustness to adversarial attacks.
Following Definition 3.1, let T = [T1； T2； ...； Tn] and T0 = [T10； T20； ...； Tn0] denote the features for
the benign sentence X and adversarial sentence X0 . The distributions of X and X0 are denoted
by probability p(x) and q(x) with the support X and X0, respectively. We assume that the feature
representation T has finite support denoted by T considering the finite vocabulary size in NLP.
Theorem 3.2.	(Adversarial Robustness Bound) For random variables X = [X1； X2； ...； Xn] and
X0 = [X1 ； X2； ...； xn], let T = [Ti; T2； ...； Tn] = [fθ(Xi)； fθ(X2); ...； fθ(Xn)] and T0 =
[T10； T20； ...； Tn0 ] = [fθ (X10 )； fθ (X20 )； ...； fθ (Xn0 )] with finite support T, where fθ is a deterministic
feature extractor. The performance gap between benign and adversarial data |I(Y； T) - I(Y； T0)|
is bounded above by
nn
|I (Y ； T) - I (Y ； T 0)| ≤ Bo + Bi X Prn(I (Xi； Ti))1/2 + B2 X |T|3/4(I (Xi； Ti))1/4
i=i	i=i
nn
+ B3 X pm (I (X/； T/))1/2 + B4 X |T|3/4(I (X0； Ti，))1/4,	(8)
i=i	i=i
where B0, B1, B2, B3 and B4 are constants depending on the sequence length n, and p(x).
The sketch of the proof is to express the difference of |I(Y； T) - I(Y 0； T)| in terms of I(Xi； Ti).
Specifically, Eq. (25) factorizes the difference into two summands. The first summand, the con-
ditional entropy |H(T | Y) - H(T0 | Y)|, can be bound by Eq. (42) in terms of MI be-
tween benign/adversarial input and representation I(Xi； Ti) and I(Xi0； Ti0). The second summand
|H(T) - H(T0)| has a constant upper bound (Eq. (85)), since language models have bounded vo-
cabulary size and embedding space, and thus have bounded entropy.
The intuition of Theorem 3.2 is to bound the adversarial performance drop |I(Y； T) - I(Y； T0)|
by I(Xi； Ti). As explained in Eq. (3), I(Y； T) and I(Y； T0) can be regarded as the model perfor-
mance on benign and adversarial data. Thus, the LHS of the bound represents such a performance
gap. The adversarial robustness bound of Theorem 3.2 indicates that the performance gap becomes
closer when I(Xi； Ti) and I(Xi0； Ti0) decrease. Note that our IB regularizer in the objective function
Eq. (6) achieves the same goal of minimizing I(Xi； Ti) while learning the most efficient informa-
tion features, or approximate minimal sufficient statistics, for downstream tasks. Theorem 3.2 also
suggests that combining adversarial training with our IB regularizer can further minimize I(Xi0； Ti0),
leading to better robustness, which is verified in §4.
3.2 Anchored Feature Regularizer
In addition to the IB regularizer that suppresses noisy information that may incur adversarial at-
tacks, we propose a novel regularizer termed “Anchored Feature Regularizer”, which extracts local
4
Published as a conference paper at ICLR 2021
Algorithm 1 - Local Anchored Feature Extraction. This algorithm takes in the word local features
and returns the index of local anchored features._________________________________________________
1:	Input: Word local features t, upper and lower threshold ch and cl
2:	δ J 0 //Initialize the perturbation vector δ
3： g(δ) = Vδ'task(qψ (t + δ),y) //Perform adversarial attack on the embedding space
4:	Sort the magnitude of the gradient of the perturbation vector	from
l∣g(δ)ι∣∣2,∣∣g(δ)2∣∣2,…,∣∣g(δ)n∣∣2 into I∣g(δ)k1∣∣2, I∣g(δ)k21∣2,…,∣∣g(δ)kn∣∣2 in ascending
order, where zi corresponds to its original index.
5:	Return: ki, ki+1, ..., kj, where Cl ≤ ni ≤ n ≤ ch.
stable features and aligns them with sentence global representations, thus improving the stability
and robustness of language representations.
The goal of the local anchored feature extraction is to find features that carry useful and stable in-
formation for downstream tasks. Instead of directly searching for local anchored features, we start
with searching for nonrobust and unuseful features. To identify local nonrobust features, we perform
adversarial attacks to detect which words are prone to changes under adversarial word substitution.
We consider these vulnerable words as features nonrobust to adversarial threats. Therefore, global
robust sentence representations should rely less on these vulnerable statistical clues. On the other
hand, by examining the adversarial perturbation on each local word feature, we can also identify
words that are less useful for downstream tasks. For example, stopwords and punctuation usually
carry limited information, and tend to have smaller adversarial perturbations than words containing
more effective information. Although these unuseful features are barely changed under adversarial
attacks, they contain insufficient information and should be discarded. After identifying the non-
robust and unuseful features, we treat the remaining local features in the sentences as useful stable
features and align the global feature representation based on them.
During the local anchored feature extraction, we perform “virtual” adversarial attacks that generate
adversarial perturbation in the embedding space, as it abstracts the general idea for existing word-
level adversarial attacks. Formally, given an input sentence x = [x1; x2; ...; xn] with its correspond-
ing local embedding representation t = [t1; ...; tn], where x andt are the realization of random vari-
ables X and T , we generate adversarial perturbation δ in the embedding space so that the task loss
`task increases. The adversarial perturbation δ is initialized to zero, and the gradient of the loss with
respect to δ is calculated by g(δ) = Vδ'task(qψ (t + δ), y) to update δ J Q∣∣δ∣∣F≤e(ηg(δ)∕∣∣g(δ)∣∣F).
The above process is similar to one-step PGD with zero-initialized perturbation δ. Since we only
care about the ranking of perturbation to decide on robust features, in practice we skip the update of
δ to save computational cost, and simply examine the `2 norm of the gradient g(δ)i of the perturba-
tion on each word feature ti. A feasible plan is to choose the words whose perturbation is neither too
large (nonrobust features) nor too small (unuseful features), e.g., the words whose perturbation rank-
ings are among 50% 〜80% of all the words. The detailed procedures are provided in Algorithm 1.
After local anchored features are extracted, we propose to align sentence global representations Z
with our local anchored features Ti. In practice, we can use the final-layer [CLS] embedding to
represent global sentence-level feature Z. Specifically, we use the information theoretic tool to
increase the mutual information I(Ti; Z) between local anchored features Ti and sentence global
representations Z, so that the global representations can share more robust and useful information
with the local anchored features and focus less on the nonrobust and unuseful ones. By incorporating
the term I(Ti; Z) into the previous objective function Eq. (6), our final objective function becomes:
nM
max I(Y; T) - nβ XI(Xi; Ti)+ α XI(Tkj; Z),	(9)
i=1	j=1
where Tkj are the local anchored features selected by Algorithm 1 and M is the number of local
anchored features. An illustrative figure can be found in Appendix Figure 2.
5
Published as a conference paper at ICLR 2021
In addition, due to the intractability of computing MI, we use InfoNCE (van den Oord et al., 2018)
as the lower bound of MI to approximate the last term I(Tkj ; Z):
I(InfoNCE)(Ti； Z)= EP gω(ti,z) - E- hlogX egω(ti,z)i ,	(10)
t0i
where gω(∙, ∙) is a score function (or critic function) approximated by a neural network, ti are the
positive samples drawn from the joint distribution P of local anchored features and global repre-
sentations, and t0i are the negative samples drawn from the distribution of nonrobust and unuseful
features P.
4	Experiments
In this section, we demonstrate how effective InfoBERT improves the robustness of language models
over multiple NLP tasks such as NLI and QA. We evaluate InfoBERT against both strong adversarial
datasets and state-of-the-art adversarial attacks.
4.1	Experimental Setup
Adversarial Datasets The following adversarial datasets and adversarial attacks are used to eval-
uate the robustness of InfoBERT and baselines. (I) Adversarial NLI (ANLI) (Nie et al., 2020)
is a large-scale NLI benchmark, collected via an iterative, adversarial, human-and-model-in-the-
loop procedure to attack BERT and RoBERTa. ANLI dataset is a strong adversarial dataset which
can easily reduce the accuracy of BERTLarge to 0%. (II) Adversarial SQuAD (Jia & Liang, 2017)
dataset is an adversarial QA benchmark dataset generated by a set of handcrafted rules and refined
by crowdsourcing. Since adversarial training data is not provided, we fine-tune RoBERTaLarge on
benign SQuAD training data (Rajpurkar et al., 2016) only, and test the models on both benign and
adversarial test sets. (III) TextFooler (Jin et al., 2020) is the state-of-the-art word-level adversarial
attack method to generate adversarial examples. To create an adversarial evaluation dataset, we sam-
pled 1, 000 examples from the test sets of SNLI and MNLI respectively, and run TextFooler against
BERTLarge and RoBERTaLarge to obtain the adversarial text examples.
Baselines Since IBP-based methods (Huang et al., 2019; Jia et al., 2019) cannot be applied to large-
scale language models yet, and the randomized-smoothing-based method (Ye et al., 2020) achieves
limited certified robustness, we compare InfoBERT against three competitive baselines based on
adversarial training: (I) FreeLB (Zhu et al., 2020a) applies adversarial training to language models
during fine-tuning stage to improve generalization. In §4.2, we observe that FreeLB can boost the
robustness of language models by a large margin. (II) SMART (Jiang et al., 2020) uses adversarial
training as smoothness-inducing regularization and Bregman proximal point optimization during
fine-tuning, to improve the generalization and robustness of language models. (III) ALUM (Liu
et al., 2020) performs adversarial training in both pre-training and fine-tuning stages, which achieves
substantial performance gain on a wide range of NLP tasks. Due to the high computational cost of
adversarial training, we compare InfoBERT to ALUM and SMART with the best results reported in
the original papers.
Evaluation Metrics We use robust accuracy or robust F1 score to measure how robust the baseline
models and InfoBERT are when facing adversarial data. Specifically, robust accuracy is calculated
by: ACC = ∣d1^ Pχ0∈Dadv 1 [arg max qψ(fθ(x0)) ≡ y], where Dadv is the adversarial dataset, y is
the ground-truth label, arg max selects the class with the highest logits and 1(∙) is the indicator func-
tion. Similarly, robust F1 score is calculated by: F1 =	Pχ0∈pad v(argmaxqψ(fθ(x0)),a),
where v(∙, ∙) is theF1 score between the true answer a and the predicted answer arg max qψ (fθ(χ0)),
and arg max selects the answer with the highest probability (see Rajpurkar et al. (2016) for details).
Implementation Details To demonstrate InfoBERT is effective for different language models, we
apply InfoBERT to both pretrained RoBERTaLarge and BERTLarge. Since InfoBERT can be applied
to both standard training and adversarial training, we here use FreeLB as the adversarial training
implementation. InfoBERT is fine-tuned for 2 epochs for the QA task, and 3 epochs for the NLI
task. More implementation details such as α, β, ch, cl selection can be found in Appendix A.1.
6
Published as a conference paper at ICLR 2021
Training	Model	Method	Dev				Test			
			AI	A2	A3	ANLI	A1	A2	A3	ANLI
			 RoBERTa		 Vanilla	49.1	26.5	27.2	33.8	49.2	27.6	24.8	33.2
Standard Training		InfoBERT	47.8	31.2	31.8	36.6	47.3	31.2	31.1	36.2
	BERT	Vanilla	20.7	26.9	31.2	26.6	21.8	28.3	28.8	26.5
		InfoBERT	26.0	30.1	31.2	29.2	26.4	29.7	29.8	28.7
	RoBERTa	FreeLB	50.4	28.0	28.5	35.2	48.1	30.4	26.3	34.4
Adversarial Training		InfoBERT	48.4	29.3	31.3	36.0	50.0	30.6	29.3	36.2
	BERT	FreeLB	23.0	29.0	32.2	28.3	22.2	28.5	30.8	27.4
		InfoBERT	28.3	30.2	33.8	30.9	25.9	28.1	30.3	28.2
Table 1: Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI +
SNLI) only. ‘A1-A3’ refers to the rounds with increasing difficulty. ‘ANLI’ refers to A1+A2+A3.
Training	Model	Method	Dev				Test			
			AI	A2	A3	ANLI	A1	A2	A3	ANLI
			 RoBERTa		 Vanilla	74.1	50.8	43.9	55.5	73.8	48.9	44.4	53.7
Standard Training		InfoBERT	75.2	49.6	47.8	56.9	73.9	50.8	48.8	57.3
	BERT	Vanilla	58.5	46.1	45.5	49.8	57.4	48.3	43.5	49.3
		InfoBERT	59.3	48.9	45.5	50.9	60.0	46.9	44.8	50.2
		FreeLB	75.2	47.4	45.3	55.3	73.3	50.5	46.8	56.2
	RoBERTa	SMART	74.5	50.9	47.6	57.1	72.4	49.8	50.3	57.1
Adversarial Training		ALUM	73.3	53.4	48.2	57.7	72.3	52.1	48.4	57.0
		InfoBERT	76.4	51.7	48.6	58.3	75.5	51.4	49.8	58.3
		FreeLB	60.3	47.1	46.3	50.9	60.3	46.8	44.8	50.2
	BERT	ALUM	62.0	48.6	48.1	52.6	61.3	45.9	44.3	50.1
		InfoBERT	60.8	48.7	45.9	51.4	63.3	48.7	43.2	51.2
Table 2: Robust accuracy on the ANLI dataset. Models are trained on both adversarial and benign
datasets (ANLI (training) + FeverNLI + MNLI + SNLI).
4.2	Experimental Results
Evaluation on ANLI As ANLI provides an adversarial training dataset, we evaluate models in two
settings: 1) training models on benign data (MNLI (Williams et al., 2018) + SNLI (Bowman et al.,
2015)) only, which is the case when the adversarial threat model is unknown; 2) training models
on both benign and adversarial training data (SNLI+MNLI+ANLI+FeverNLI), which assumes the
threat model is known in advance.
Results of the first setting are summarized in Table 1. The vanilla RoBERTa and BERT models
perform poorly on the adversarial dataset. In particular, vanilla BERTLarge with standard training
achieves the lowest robust accuracy of 26.5% among all the models. We also evaluate the robust-
ness improvement by performing adversarial training during fine-tuning, and observe that adversar-
ial training for language models can improve not only generalization but also robustness. In contrast,
InfoBERT substantially improves robust accuracy in both standard and adversarial training. The ro-
bust accuracy of InfoBERT through standard training is even higher than the adversarial training
baseline FreeLB for both RoBERTa and BERT, while the training time of InfoBERT is 1/3 〜1/2
less than FreeLB. This is mainly because FreeLB requires multiple steps of PGD attacks to gener-
ate adversarial examples, while InfoBERT essentially needs only 1-step PGD attack for anchored
feature selection.
Results of the second setting are provided in Table 2, which shows InfoBERT can further improve
robust accuracy for both standard and adversarial training. Specifically, when combined with adver-
sarial training, InfoBERT achieves the state-of-the-art robust accuracy of 58.3%, outperforming all
existing baselines. Note that although ALUM achieves higher accuracy for BERT on the dev set, it
tends to overfit on the dev set, therefore performing worse than InfoBERT on the test set.
7
Published as a conference paper at ICLR 2021
Training	Model	Method	SNLI	MNLI (m/mm)	adv-SNLI (BERT)	adv-MNLI (BERT)	adv-SNLI (RoBERTa)	adv-MNLI (RoBERTa)
		Vanilla	92.6	90.8/90.6	-56.6~~	68.1/68.6	19.4	24.9/24.9
Standard	RoBERTa	InfoBERT	93.3	90.5/90.4	59.8	69.8/70.6	42.5	50.3/52.1
Training		Vanilla	91.3	86.7/86.4	0.0	0.0/0.0	44.9	57.0/57.5
	BERT	InfoBERT	91.7	86.2/86.0	36.7	43.5/46.6	45.4	57.2/58.6
		FreeLB	93.4	90.1/90.3	60.4	70.3/72.1	41.2	49.5/50.6
Adversarial	RoBERTa	InfoBERT	93.1	90.7/90.4	62.3	73.2/73.1	43.4	56.9/55.5
Training		FreeLB	92.4	86.9/86.5	46.6	60.0/60.7	50.5	64.0/62.9
	BERT	InfoBERT	92.2	87.2/87.2	50.8	61.3/62.7	52.6	65.6/67.3
Table 3: Robust accuracy on the adversarial SNLI and MNLI(-m/mm) datasets generated by
TextFooler based on blackbox BERT/RoBERTa (denoted in brackets of the header). Models are
trained on the benign datasets (MNLI+SNLI) only.
Training ∣ Method ∣ benign ∣ AddSent AddOneSent
Standard	Vanilla	93.5/86.9	72.9/66.6	80.6/74.3
Training	InfoBERT	93.5/87.0	78.5/72.9	84.6/78.3
Adversarial Training	FreeLB	93.8/87.3	76.3/70.3	82.3/76.2
	ALUM	-	75.5/69.4	81.4/75.9
	InfoBERT	93.7/87.0	78.0/71.8	83.6/77.1
Table 4: Robust F1/EM scores based on RoBERTaLarge on
the adversarial SQuAD datasets (AddSent and AddOne-
Sent). Models are trained on standard SQuAD 1.0 dataset.
0.08
0.06
0.04
0.02
0
MI Improvement after adding adv
examples in the training set
δ'r	MR	N'n	MN
Adversarial Test Data ■ Benign Test Data
Figure 1: Local anchored features con-
tribute more to MI improvement than
nonrobust/UnUsefUI features, unveiling
closer relation with robustness.
Evaluation against TextFooler InfoBERT can defend against not only human-crafted adversarial
examples (e.g., ANLI) but also those generated by adversarial attacks (e.g., TextFooler). Results are
summarized in Table 3. We can see that InfoBERT barely affects model performance on the benign
test data, and in the case of adversarial training, InfoBERT even boosts the benign test accuracy. Un-
der the TextFooler attack, the robust accuracy of the vanilla BERT drops to 0.0% on both MNLI and
SNLI datasets, while RoBERTa drops from 90% to around 20%. We observe that both adversarial
training and InfoBERT with standard training can improve robust accuracy by a comparable large
margin, while InfoBERT with adversarial training achieves the best performance among all models,
confirming the hypothesis in Theorem 3.2 that combining adversarial training with IB regularizer
can further minimize I(Xi0; Ti0), leading to better robustness than the vanilla one.
Evaluation on Adversarial SQuAD Previous experiments show that InfoBERT can improve model
robustness for NLI tasks. Now we demonstrate that InfoBERT can also be adapted to other NLP
tasks such as QA in Table 4. Similar to our observation on NLI dataset, we find that InfoBERT
barely hurts the performance on the benign test data, and even improves it in some cases. Moreover,
InfoBERT substantially improves model robustness when presented with adversarial QA test sets
(AddSent and AddOneSent). While adversarial training does help improve robustness, InfoBERT
can further boost the robust performance by a larger margin. In particular, InfoBERT through stan-
dard training achieves the state-of-the-art robust F1/EM score as 78.5/72.9 compared to existing
adversarial training baselines, and in the meantime requires only half the training time of adversarial-
training-based methods.
4.3	Analysis of Local Anchored Features
We conduct an ablation study to further validate that our anchored feature regularizer indeed fil-
ters out nonrobust/unuseful information. As shown in Table 1 and 2, adding adversarial data in
the training set can significantly improve model robustness. To find out what helps improve the
robustness from the MI perspective, we first calculate the MI between anchored features and global
features Mm PM=I I (Tkj ; Z) on the adversarial test data and benign test data, based on the model
trained without adversarial training data (denoted by IR0 and IR). We then calculate the MI between
nonrobust/unuseful features and global features MMo PM=I I(Tki ； Z) on the adversarial test data and
8
Published as a conference paper at ICLR 2021
benign data as well (denoted by IN0 and IN). After adding adversarial examples into the training set
and re-training the model, we find that the MI between the local features and the global features
substantially increases on the adversarial test data, which accounts for the robustness improvement.
We also observe that those local anchored features extracted by our anchored feature regularizer,
as expected, contribute more to the MI improvement. As shown in Figure 1, the MI improvement
of anchored features on adversarial test data ∆IR0 (red bar on the left) is higher than that of nonro-
bust/unuseful ∆IN0 (red bar on the right), thus confirming that local anchored features discovered by
our anchored feature regularizer have a stronger impact on robustness than nonrobust/unuseful ones.
We conduct more ablation studies in Appendix §A.2, including analyzing the individual impact of
two regularizers, the difference between global and local features for IB regularizer, hyper-parameter
selection strategy and so on.
5	Conclusion
In this paper, we propose a novel learning framework InfoBERT from an information theoretic per-
spective to perform robust fine-tuning over pre-trained language models. Specifically, InfoBERT
consists of two novel regularizers to improve the robustness of the learned representations: (a) In-
formation Bottleneck Regularizer, learning to extract the approximated minimal sufficient statistics
and denoise the excessive spurious features, and (b) Local Anchored Feature Regularizer, which
improves the robustness of global features by aligning them with local anchored features. Supported
by our theoretical analysis, InfoBERT provides a principled way to improve the robustness of BERT
and RoBERTa against strong adversarial attacks over a variety of NLP tasks, including NLI and
QA tasks. Comprehensive experiments demonstrate that InfoBERT outperforms existing baseline
methods and achieves new state of the art on different adversarial datasets. We believe this work will
shed light on future research directions towards improving the robustness of representation learning
for language models.
6	Acknowledgement
We gratefully thank the anonymous reviewers and meta-reviewers for their constructive feedback.
We also thank Julia Hockenmaier, Alexander Schwing, Sanmi Koyejo, Fan Wu, Wei Wang, Pengyu
Cheng, and many others for the helpful discussion. This work is partially supported by NSF grant
No.1910100, DARPA QED-RML-FP-003, and the Intel RSA 2020.
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. In Ellen Riloff, David Chiang, Julia
Hockenmaier, and Jun'ichi Tsujii (eds.), EMNLP, pp. 2890-2896. Association for Computational
Linguistics, 2018.
David Barber and Felix V. Agakov. The im algorithm: A variational approach to information maxi-
mization. In NeurIPS, 2003.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and An-
dreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-
Ume 80 of Proceedings of Machine Learning Research, pp. 531-540, Stockholmsmassan, Stock-
holm Sweden, 10-15 Jul 2018. PMLR.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large an-
notated corpus for learning natural language inference. In Lluis Marquez, Chris Callison-Burch,
Jian Su, Daniele Pighin, and Yuval Marton (eds.), EMNLP, pp. 632-642. The Association for
Computational Linguistics, 2015.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
9
Published as a conference paper at ICLR 2021
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. CoRR, abs/2002.05709, 2020.
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and L. Carin. Club: A contrastive
log-ratio upper bound of mutual information. ArXiv, abs/2006.12013, 2020.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via random-
ized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), ICML, volume 97 of
Proceedings ofMachine Learning Research, pp.1310-1320. PMLR, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
Thamar Solorio (eds.), NAACL-HLT, pp. 4171-4186. Association for Computational Linguistics,
2019.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers. CoRR, abs/1805.10265, 2018.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples
for text classification. In Iryna Gurevych and Yusuke Miyao (eds.), ACL, pp. 31-36. Association
for Computational Linguistics, 2018.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Xiaodong Song. Robust physical-world attacks on deep
learning models. 2017.
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial
training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2015.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In ICLR, 2019.
Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krish-
namurthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions
via interval bound propagation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),
EMNLP-IJCNLP, pp. 4081-4091. Association for Computational Linguistics, 2019.
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation
with syntactically controlled paraphrase networks. In Marilyn A. Walker, Heng Ji, and Amanda
Stent (eds.), NAACL-HLT, pp. 1875-1885. Association for Computational Linguistics, 2018.
Joern-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge. Excessive invariance
causes adversarial vulnerability. In ICLR, 2019.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In
Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), EMNLP, pp. 2021-2031. Association
for Computational Linguistics, 2017.
Robin Jia, Aditi RaghUnathan, Kerem GokseL and Percy Liang. Certified robustness to adversarial
word substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), EMNLP-
IJCNLP, pp. 4127-4140. Association for Computational Linguistics, 2019.
Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART:
robust and efficient fine-tuning for pre-trained natural language models through principled regu-
larized optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.),
ACL, pp. 2177-2190. Association for Computational Linguistics, 2020.
10
Published as a conference paper at ICLR 2021
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline
for natural language attack on text classification and entailment. In AAAI, pp. 8018-8025. AAAI
Press, 2020.
Lingpeng Kong, Cyprien de Masson d’Autume, Lei Yu, Wang Ling, Zihang Dai, and Dani Yo-
gatama. A mutual information maximization perspective of language representation learning. In
ICLR, 2020.
Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text
against real-world applications. In NDSS. The Internet Society, 2019.
Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng
Gao. Adversarial training for large neural language models. CoRR, abs/2004.08994, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining
approach. CoRR, abs/1907.11692, 2019.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. CVPR, pp. 2574-2582, 2016.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adver-
sarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai,
Natalie Schluter, and Joel R. Tetreault (eds.), ACL, pp. 4885-4901. Association for Computational
Linguistics, 2020.
Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597, 2016.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions
for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), EMNLP,
pp. 2383-2392. The Association for Computational Linguistics, 2016.
Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial
examples through probability weighted word saliency. In Anna Korhonen, David R. Traum, and
Lluls Marquez (eds.), ACL, pp. 1085-1097. Association for Computational Linguistics, 2019.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In Kamalika Chaud-
huri and Ruslan Salakhutdinov (eds.), ICML, volume 97 of Proceedings of Machine Learning
Research, pp. 5628-5637. PMLR, 2019.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. CoRR, abs/2005.10243, 2020.
N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE
Information Theory Workshop (ITW), pp. 1-5, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), NeurIPS, pp. 5998-6008, 2017.
Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, and Bo Li. T3: Tree-
autoencoder constrained adversarial text generation for targeted attack. In EMNLP, 2020.
Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent
(eds.), NAACL-HLT, pp. 1112-1122. Association for Computational Linguistics, 2018.
11
Published as a conference paper at ICLR 2021
Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for certified robustness
to adversarial word substitutions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
TetreaUlt (eds.), ACL, pp. 3465-3475. Association for Computational Linguistics, 2020.
Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. Fine-tuning pre-
trained language model with weak supervision: A contrastive-regularized self-training approach.
CoRR, abs/2010.07835, 2020.
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun.
Word-level textual adversarial attacking as combinatorial optimization. In Dan Jurafsky, Joyce
Chai, Natalie Schluter, and Joel R. Tetreault (eds.), ACL, pp. 6066-6080. Association for Com-
putational Linguistics, 2020.
Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: paraphrase adversaries from word scram-
bling. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), NAACL-HLT, pp. 1298-1308.
Association for Computational Linguistics, 2019.
Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced
adversarial training for natural language understanding. In ICLR. OpenReview.net, 2020a.
Sicheng Zhu, Xiao Zhang, and David Evans. Learning adversarially robust representations via
worst-case mutual information maximization. CoRR, abs/2002.11798, 2020b.
12
Published as a conference paper at ICLR 2021
A Appendix
Figure 2: The complete objective function of InfoBERT, which can be decomposed into (a) standard
task objective, (b) Information Bottleneck Regularizer, and (c) Local Anchored Feature Regularizer.
For (b), we both theoretically and empirically demonstrate that we can improve the adversarial
robustness by decreasing the mutual information of I(Xi; Ti) without affecting the benign accuracy
much. For (c), we propose to align the local anchored features Tkj (highlighted in Yellow) with the
global feature Z by maximizing their mutual information I(Tkj ; Z).
A. 1 Implementation Details
Model Details1 BERT is a transformer (Vaswani et al., 2017) based model, which is unsupervised
pretrained on large corpora. We use BERTLarge-uncased as the baseline model, which has 24 lay-
ers, 1024 hidden units, 16 self-attention heads, and 340M parameters. RoBERTaLarge shares the
same architecture as BERT, but modifies key hyperparameters, removes the next-sentence pretrain-
ing objective and trains with much larger mini-batches and learning rates, which results in higher
performance than BERT model on GLUE, RACE and SQuAD.
Standard Training Details For both standard and adversarial training, we fine-tune InfoBERT for
2 epochs on the QA task, and for 3 epochs on the NLI task. The best model is selected based on the
performance on the development set. All fine-tuning experiments are run on Nvidia V100 GPUs.
For NLI task, we set the batch size to 256, learning rate to 2 × 10-5, max sequence length to 128
and warm-up steps to 1000. For QA task, we set the batch size to 32, learning rate to 3 × 10-5 and
max sequence length to 384 without warm-up steps.
Adversarial Training Details2 Adversarial training introduces hyper-parameters including ad-
versarial learning rates, number of PGD steps, and adversarial norm. When combing adversarial
training with InfoBERT, we use FreeLB as the adversarial training implementation, and set adver-
Sarial learning rate to 10-1 or 4 * 10-2, adversarial steps to 3, maximal perturbation norm to 3 * 10-1
or 2 * 10-1 and initial random perturbation norm to 10-1 or 0.
Information Bottleneck Regularizer Details For information bottleneck, there are different ways
to model p(t | x):
1.	Assume that p(t | x) is unknown. We use a neural net parameterized by qθ (t | x) to learn
the conditional distribution p(t | x). We assume the distribution is a Gaussian distribution. The
1We use the huggingface implementation https://github.com/huggingface/transformers
for BERT and RoBERTa.
2We follow the FreeLB implementations in https://github.com/zhuchen03/FreeLB.
13
Published as a conference paper at ICLR 2021
neural net qθ will learn the mean and variance of the Gaussian given input x and representation t.
By reparameterization trick, the neural net can be backpropagated to approximate the distribution
given the training samples.
2.	p(t | x) is known. Since t is the representation encoded by BERT, we actually already know the
distribution ofp. We also denote it as qθ, where θ is the parameter of the BERT encoder fθ. Ifwe
assume the conditional distribution is a Gaussian N(ti, σ) for input xi whose mean is the BERT
representation ti and variance is a fixed constant σ, the Eq.6 becomes
1N	n
LLIB = NN X [logqψ(y(i) I t(i))] - βX [ - c(σ)||tk(i)
i=1	k=1
-t(k)∖∖2+n X c(σ)ιιtj - tk |同),
j=1	(11)
where c(σ ) is a positive constant related to σ. In practice, the sample t0i from the conditional
distribution Gaussian N (ti, σ) can be ti with some Gaussian noise, an adversarial examples of
ti, or ti itself (assume σ = 0).
We use the second way to model p(t | x) for InfoBERT finally, as it gives higher robustness im-
provement than the first way empirically (shown in the following §A.2). We suspect that the main
reason is because the first way needs to approximate the distribution p(t | x) via another neural net
which could present some difficulty in model training.
Information Bottleneck Regularizer also introduces another parameter β to tune the trad-off between
representation compression I(Xi; Ti) and predictive power I(Y ; T). We search for the optimal β
via grid search, and set β = 5 × 10-2 for RoBERTa, β = 10-3 for BERT on the NLI task. On the
QA task, we set β = 5 × 10-5, which is substantially lower than β on NLI tasks, thus containing
more word-level features. We think it is mainly because the QA task relies more on the word-level
representation to predict the exact answer spans. More ablation results can be found in the following
§A.2.
Anchored Feature Regularizer Details Anchored Feature Regularizer uses α to weigh the bal-
ance between predictive power and importance of anchored feature. We set α = 5 × 10-3 for both
NLI and QA tasks. Anchored Feature Regularizer also introduces upper and lower threshold cl and
ch for anchored feature extraction. We set ch = 0.9 and cl = 0.5 for the NLI task, and set ch = 0.95
and cl = 0.75 for the QA task. The neural MI estimator used by infoNCE uses two-layer fully
connected layer to estimate the MI with the intermediate layer hidden size set to 300.
A.2 Additional Experimental Results
A.2.1 Ablation S tudy on Information Bottleneck Regularizer
Modeling p(t | x) As discussed in §A.1, we have two ways to model p(t | x): (i) using an
auxiliary neural network to approximate the distribution; (ii) directly using the BERT encoder fθ
to calculate the p(t | x). Thus we implemented these two methods and compare the robustness
improvement in Table 5. To eliminate other factors such as Anchored Feature Regularizer and
adversarial training, we set α = 0, β = 5 × 10-2 and conduct the following ablation experiments
via standard training on standard datasets. We observe that although both modeling methods can
improve the model robustness, modeling as BERT encoder gives a larger margin than the Auxiliary
Net. Moreover, the second way barely sacrifices the performance on benign data, while the first
way can hurt the benign accuracy a little bit. Therefore, we use the BERT Encoder fθ to model the
p(t | x) in our main paper.
Local Features v.s. Global Features Information Bottlenck Regularizer improves model robust-
ness by reducing I(X; T). In the main paper, we use T as word-level local features. Here we
consider T as sentence-level global features, and compare the robustness improvement with T as
local features. To eliminate other factors such as Anchored Feature Regularizer and adversarial
training, we set α = 0, β = 5 × 10-2 and conduct the following ablation experiments via standard
training.
14
Published as a conference paper at ICLR 2021
Model	Datasets	Method	Adversarial Accuracy (ANLD	Benign Accuracy (MNLI/SNLI)
	Standard Datasets	Vanilla	26.5	86.7/91.3
BERT			 Auxiliary Net		 27.1 ―	83.1/90.7
				BERT Encoder fθ	27.7	85.9/91.7
Table 5: Robust accuracy on the ANLI dataset. Here we refer “Standard Datasets” as training on
the benign datasets (MNLI + SNLI) only. “Vanilla” refers to the vanilla BERT trained without
Information Bottleneck Regularizer.
The experimental results are summarized in Table 6. We can see that while both features can boost
the model robustness, using local features yield higher robust accuracy improvement than global
features, especially when adversarial training dataset is added.
Hyper-parameter Search We perform grid search to find out the optimal β so that the optimal
trade-off between representation compression (“minimality”) and predictive power (“sufficiency”)
is achieved. An example to search for the optimal β on QA dataset is shown in Fingure 3, which
illustrates how β affects the F1 score on benign and adversarial datasets. We can see that from a
very small β, both the robust and benign F1 scores increase, demonstrating InfoBERT can improve
both robustness and generalization to some extent. When we set β = 5 × 10-5 (log(β) = -9.9),
InfoBERT achieves the best benign and adversarial accuracy. When we set a larger β to further
minimize I(Xi ; Ti), we observe that the benign F1 score starts to drop, indicating the increasingly
compressed representation could start to hurt its predictive capability.
A.2.2 Ablation S tudy on Anchored Feature Regularizer
Visualization of Anchored Words To explore which local anchored features are extracted, we
conduct another ablation study to visualize the local anchored words. We follow the best hyper-
parameters of Anchored Feature Regularizer introduced in §A.1, use the best BERT model trained
on benign datasets (MNLI + SNLI) only and test on the ANLI dev set. We visualize the local
anchored words in Table 7 as follows. In the first example, we find that Anchored Features mainly
focus on the important features such as quantity number “Two”, the verb “playing” and objects
“card”/“poker” to make a robust prediction. In the second example, the matching robust features
between hypothesis and premise, such as “people”, “roller” v.s. “park”, “flipped upside” v.s. “ride”,
are aligned to infer the relationship of hypothesis and premise. These anchored feature examples
confirm that Anchored Feature Regularizer is able to find out useful and stable features to improve
the robustness of global representation.
Model	Datasets	Features	Adversarial Accuracy (ANLI)	Benign Accuracy (MNLI/SNLI)
	Standard	Vanilla	33.2	90.8/92.6
	Datasets	Global Feature	33.8	90.4/93.5
RoBERTa		Local Feature	33.9	90.6/93.7
	Standard and Adversarial	Vanilla	53.7	91.0/92.6
		Global Feature	55.1	90.8/93.3
	Datasets	Local Feature	562	90.5/93.3
Table 6: Robust accuracy on the ANLI dataset. Here we refer “Standard Datasets” as training on
the benign datasets (MNLI + SNLI) only, and “Standard and Adversarial Datasaets” as training on
the both benign and adversarial datasets (ANLI(trianing) + MNLI + SNLI + FeverNLI). “Vanilla”
refers to the vanilla RoBERTa trained without Information Bottleneck Regularizer.
15
Published as a conference paper at ICLR 2021

94.0-
93.5-
93.0-
92.5-
92.0-
Benign/Robust Fl on Benign/Adversarial SQuAD Dataset
94.5-	-80
Figure 3: Benign/robust F1 score on benign/adversarial QA datasets. Models are trained on the
benign SQuAD dataset with different β .
91.5 I	I	I	I	I
-12	-11	-10	-9	-8
IOgB
'	'	'	73
-7	-6	-5
QU 8spp<,rayesJ8>pv
7
7
Input (bold = local stable words for local anchored features.)
Premise: Two woman, both sitting near a pile of poker chips, are playing cards.
Hypothesis: Two woman playing poker.
Premise: People are flipped upside - down on a bright yellow roller coaster.
Hypothesis: People on on a ride at an amusement park.
Table 7: Local anchored features extracted by Anchored Feature Regularizer.
A.2.3 Ablation S tudy on Disentangling Two Regularizers
To understand how two regularizers contribute to the improvement of robustness separetely, we
apply two regularizers individually to both the standard training and adversarial training. We refer
InfoBERT trained with IB regularizer only as “InfoBERT (IBR only)” and InfoBERT trained with
Anchored Feature Regularizer only as “InfoBERT (AFR only)”. “InfoBERT (Both)” is the standard
setting for InfoBERT, where we incorporate both regularizers during training. For “InfoBERT (IBR
only)”, we set α = 0 and perform grid search to find the optimal β = 5 × 10-2. Similarly for
“InfoBERT (AFR only)”, we set β = 0 and find the optimal parameters as α = 5 × 10-3, ch = 0.9
and cl = 0.5.
The results are shown in Table 8. We can see that both regularizers improve the robust accuracy
on top of vanilla and FreeLB to a similar margin. Applying one of the regularizer can achieve
Similar performance of FreeLB, but the training time of InfoBERT is only 1/31/2 less than FreeLB.
Moreover, after combining both regularizers, we observe that InfoBERT achieves the best robust
accuracy.
A.2.4 Examples of Adversarial Datasets generated by TextFooler
We show some adversarial examples generated by TextFooler in Table 9. We can see most adver-
sarial examples are of high quality and look valid to human while attacking the NLP models, thus
confirming our adversarial dastasets created by TextFooler is a strong benchmark dataset to evaluate
model robustness. However, as also noted in Jin et al. (2020), we observe that some adversarial
examples look invalid to human For example, in the last example of Table 9, TextFooler replaces
“stand” with “position”, losing the critical information that “girls are standing instead of kneeling”
and fooling both human and NLP models. Therefore, we expect that InfoBERT should achieve
better robustness when we eliminate such invalid adversarial examples during evaluation.
16
Published as a conference paper at ICLR 2021
Model	Training		Method	Adversarial Accuracy (ANLI)	Benign Accuracy (MNLI/SNLI)
		Standard Training	Vanilla	26.5	86.7/91.3
			InfoBERT (IBR only)	27.7	85.9/91.7
BERT			InfoBERT (AFR only)	28.0	86.6/91.9
			InfoBERT (Both)	29.2	85.9/91.6
		Adversarial Training	FreeLB	27.7	86.7/92.3
			InfoBERT (IBR only)	29.3	87.0/92.3
			InfoBERT (AFR only)	30.3	86.9/92.3
			InfoBERT (Both)	309	87.2/92.2
Table 8: Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI +
SNLI). Here we refer “IBR only” as training with Information Bottleneck Regularizer only. “AFR
only” refers to InfoBERT trained with Anchored Feature Regularizer only. “Both” is the standard
InfoBERT that applies two regularizers together.
Input (red = Modified words, bold = original words.)
Valid Adversarial Examples
Premise: A young boy is playing in the sandy water.
Original Hypothesis: There is a boy in the water.
Adversarial Hypothesis: There is a man in the water.
Model Prediction: Entailment → Contradiction
Premise: A black and brown dog is playing with a brown and white dog .
Original Hypothesis: Two dogs play.
Adversarial Hypothesis: Two dogs gaming.
Model Prediction: Entailment → Neutral
Premise: Adults and children share in the looking at something, and some young ladies stand to the side.
Original Hypothesis: Some children are sleeping.
Adversarial Hypothesis: Some children are dreaming.
Model Prediction: Contradiction → Neutral
Premise: Families with strollers waiting in front of a carousel.
Original Hypothesis: Families have some dogs in front of a carousel.
Adversarial Hypothesis: Families have some doggie in front of a carousel.
Model Prediction: Contradiction → Entailment
Invalid Adversarial Examples
Premise: Two girls are kneeling on the ground.
Original Hypothesis: Two girls stand around the vending machines.
Adversarial Hypothesis: Two girls position around the vending machinery.
Model Prediction: Contradiction → Neutral
Table 9: Adversarial Examples Generated by TextFooler for BERTLarge on SNLI dataset.
17
Published as a conference paper at ICLR 2021
A.3 Proofs
A.3.1 Proof of Theorem 3.1
We first state two lemmas.
Lemma A.1. Given a sequence of random variables X1, X2, ..., Xn and a deterministic function f,
then ∀i,j = 1, 2, ..., n, we have
I (Xi； f (Xi)) ≥ I (Xj; f (Xi))	(12)
Proof. By the definition,
I(Xi;f(Xi)) = H(f(Xi)) - H(f(Xi) |Xi)	(13)
I(Xj; f(Xi)) = H(f(Xi)) - H(f(Xi) |Xj)	(14)
Since f is a deterministic function,
H(f(Xi) | Xi) = 0	(15)
H(f(Xi) | Xj) ≥ 0	(16)
Therefore,
I (Xi； f (Xi)) ≥ I (Xj; f (Xi))	(17)
□
Lemma A.2. Let X = [X1; X2; ...; Xn] be a sequence of random variables, and T =
[T1； T2； ...； Tn] = [f(X1)； f(X2)； ...； f(Xn)] be a sequence of random variables generated by a
deterministic function f . Then we have
n
I (X ； T) ≤ n XI (Xi； Ti)	(18)
i=1
Proof. Since X = [X1； X2； ...； Xn] and T = [T1； T2； ...； Tn] are language tokens with its corre-
sponding local representations, we have
n
I(X；T) =I(X；T1,T2,...,Tn) =X[H(Ti | T1,T2,...,Ti-1) -H(Ti | X, T1, T2,..., Ti-1)]
i=1
(19)
nn
≤	X[H(Ti) - H(Ti |X)] =XI(X；Ti)	(20)
nn	n
≤	XXI (Xj ； Ti) ≤ n XI (Xi； Ti),	(21)
where the first inequality follows because conditioning reduces entropy, and the last inequality is
because I (Xi； Ti) ≥ I (Xj; Ti) based on Lemma A.1.	□
Then we directly plug Lemma A.2 into Theorem 3.1, we have the lower bound of LIB as
n
I (Y ； T) - βI (X ； T) ≥ I (Y ； T) - nβ XI (Xi； Ti).	(22)
i=1
A.3.2 Proof of Theorem 3.2
We first state an easily proven lemma,
18
Published as a conference paper at ICLR 2021
Lemma A.3. For any a, b ∈ [0, 1],
Ia log(a) - b log(b)I ≤ φ(Ia - bI),
where φ(∙) : R+ → R+ is defined as
0	X=0
Φ(χ) = χ x log(1) 0 < x < 1.
〔ɪ	x> I
(23)
(24)
It is easy to verify that φ(x) is a continuous, monotonically increasing, concave and subadditive
function.
Now, we can proceed with the proof of Theorem 3.2.
Proof. We use the fact that
II(Y; T) -I(Y;T0)I ≤ IH(T I Y) - H(T0 I Y)I + IH(T) -H(T0)I
and bound each of the summands on the right separately.
We can bound the first summand as follows:
IH(TIY)-H(T0 IY)I ≤Xp(y)IH(TIY=y)-H(T0 IY=y)I
= Xp(y)I Xp(t I y)log(1/p(t I y)) - Xq(t I y) log(1/q(t I y))I
(25)
(26)
(27)
where
≤	p(y)	|p(t | y)logp(t	|	y) - q(t	|	y)	logq(t	|	y)|
≤ Ep(y) Eφ(Ip(t | y) - q(t | y)|)
Ep⑹Eφ(I Ep(t | x)[p(x | y) - q(x | y)]|),
tx
p(x I y)
q(x I y)
p(y I X)P(X)
Px p(y i X)P(X)
p(y i X)q(X)
Pxp(y i X)q(X).
Since	x∈X∪X0 p(x I y) - q(x I y) = 0 for any y ∈ Y, we have that for any scalar a,
I	p(t I X)[p(X I y) - q(X I y)])I
x
= I	(p(t I X) - a)(p(X I y) - q(X I y))I
x
≤ jX(P(t i χ) - a)2 jX(P(X i y) - q(X i y))2.
Setting a = ∣X⅛0∣ px∈x∪x0 P(t i x) We get
(28)
(29)
(30)
(31)
(32)
(33)
(34)
(35)
|H(T I Y) - H(T01 Y) ≤ Ep(y)E φ(VZV(p(t I X e X ∪X0) ∙ IIP(X I y) - q(χ I y)∣∣2),
(36)
y
y
y
y
t
t
t
t
y
t
19
Published as a conference paper at ICLR 2021
where for any real-value vector a = (a1, ..., an), V (a) is defined to be proportional to the variance
of elements of a:
nn
V (a) = X(ai-1 X aj )2，
i=1	j=1
(37)
p(t | x ∈ X ∪ X0) stands for the vector in which entries are p(t | x) with different values of
x ∈ X ∪ X0 for a fixed t, and p(x | y) and q(x | y) are the vectors in which entries are p(x | y) and
q(x | y), respectively, with different values of x ∈ X ∪ X0 for a fixed y.
Since
||p(x | y) - q(x | y)||2 ≤ ||p(x | y) - q(x | y)||1 ≤ 2,	(38)
it follows that
|H (T | Y) - H (T0 | Y )| ≤ X p(y) X φ(2,V(p(t | X ∈X∪X 0)))	(39)
Moreover, we have
√V(p(t |	X ∈ X ∪ X0) ≤	√V(p(t	| X ∈	X))	+ V(p(t | X ∈ X0))	(40)
≤	√V(p(t	| x ∈	X))	+ √V(p(t | X ∈ X0)),	(41)
where the first inequality is because sample mean is the minimizer of the sum of the squared dis-
tances to each sample and the second inequality is due to the subadditivity of the square root func-
tion. Using the fact that φ(∙) is monotonically increasing and subadditive, We get
|H(T I Y) - H(T01 Y)1 ≤ ∑p(y)∑ Φ(2VZV(p(t I X ∈ X)))
yt
+ ∑p(y) ∑ φ(2√v(p(t I X ∈ X0)))
(42)
Now we explicate the process for establishing the bound for Py p(y) Pt φ(2，V(p(t ∣ X ∈ X)))
and the one for Py p(y) Pt φ(2，V(p(t ∣ X ∈ X0))) can be similarly derived.
By definition of V(∙) and using Bayes’ theorem p(t ∣ x) = P(WxxIt) for X ∈ X, we have that
√V(p(t ∣ X ∈X)) = p(t)
ʌ/X
x∈X
P(X | t)
p(X)
1 X P(X0 | t) )2
IXi x0∈X P(XO)
(43)
20
Published as a conference paper at ICLR 2021
Denoting 1 二	二(1,..., 1), we have by the triangle inequality that ∕χ(P(X ∣t	1 X p(χ'∣ t^	(44) Vxfo p(χ)	∣X∣ 幺 P(X)J	() “ IlP(X ∣ t) ill , ∕v^ 1	1 ―∖ ' p(x' ∣ t) '2	∕z1<∖ ≤∣⅛T -1∣∣2 + √∑(1-闭 NX k)	(45) IIP(X ∣ t)	111	,	/3∣∕1	1 L p(X ∣ t))2 =∣∣k-1∣∣2+F∣(雨ZH2	() _ Il P(XIt) 1∣∣ , / 1 ” Xl_x p(X ∣t) )2	Z47X =∣ ∣ ʒ(Xr -1 112+y 团(∣ X ∣- x⅛ ^XT)	(47) UP(XIt)	1∣ι l 1	1 X ∕1	p(X ∣ t)M	”。、 = ∣∣k-1∣ ∣2+√m∣!⅛(	El∣	(48) ≤ ∣∣R2 -1 11 2 + √⅛∣ ∣ p⅜7 -1 11 1	(49) P(X)	√∣x∣	P(X) ≤(1+√⅛) ∣ ∣pP⅛2 -1 11 1	(50) 2 ≤ mmxeχ P(X) ∣ ∣P(XIt)-P(X) ∣ ∣ 1	(51)
From an inequality linking KL-divergence and the ∕1 norm, We have that
	∣ ∣ p(x I t) - p(x) I ∣ 1 ≤ √2log(2)DκL[p(X I t) 11 p(x)]	(52)
Plugging Eq.(52) into Eq. (51) and using Eq. (43), we have the following bound:
	. .		 B	r- VZV(p(t ∣ x ∈ X)) ≤ — p(t)√dt,	(53)
where B = ；£Xgs) and dt = dkl[p(x I t)∣∣p(x)].
We will first proceed the proof under the assumption that Bp(t)√dt ≤ ɪ for any t. We will later see
that this condition can be discarded. If Bp(t)√dt ≤ ɪ, then
£。(2,V(p(t I X ∈ X)))	(54)
t
≤ XBp(t)pdt(l°s(⅛) + los(-77rT))	(55)
L	B	p(t)dt
=B log(ɪ) Xp(t)pdt + B Xp(t)pdt log(ɪ)	(56)
B L	L	p(t)dt
≤ Blog(B1)∣∣p(t)√dt∣∣1 + B∣∣∕p(t)√dt∣∣ι,	(57)
where the last inequality is due to an easily proven fact that for any x > 0,x log(ɪ) ≤ √x. We p(t)
and d(t) are vectors comprising p(t) and dt with different values of t, respectively.
Using the following two inequalities:
∣∣p(t)√dt∣∣1 ≤ pτη∣∣p(t)√dt∣∣2 ≤ pη∣∣√Pm∣∣2	(58)
and
∣∣,P(t)√dt∣∣1 ≤ √m∣∣√P(t)√dt∣∣2	____________ (59)
=√T√∣∣p(t)√dt∣∣1 ≤ ∣T∣3∕4√∣∣√P(t⅛
(60)
21
Published as a conference paper at ICLR 2021
we have
Xφ(2√v(p(t | χ ∈ x))) ≤ Biog(∣)pτη∣ι√P(t)dtι∣2 + BrTT∕4q∣∣√P(丽Z ⑹)
t
Using the equality
|| √p⑴dt||2 = qE[DKL[p(x|t)||p(x)]] = √I(X；T),	(62)
we reach the following bound
X Φ(2√V(p(t | X ∈ X)))	(63)
t
≤ Blog(4)|T|1/2I(X;T)1/2 + B∣TW4I(X;T)1/4.	(64)
B
Plug Lemma A.2 into the equation above, we have
X φ(2√V(p(t | X ∈ X)))	(65)
t
nn
≤ B log( B∙)∣T∣1∕2(n X I (Xi； Ti))1/2 + BrTT/4(n X I (Xi； Ti)) 1/4	(66)
i=1	i=1
nn
≤√nB log( ⅛∣T∣1/2 X I (Xi； Ti) 1/2 + n1/4 B|T|3/4 X I(Xi；Ti)1/4	(67)
B	i=1	i=1
We now show the bound is trivial if the assumption that Bp(t)√dt ≤ e does not hold. If the
assumption does not hold, then there exists a t such that Bp(t)√dt > ɪ. Since
√I (X; T)
Xt
p(t)dt ≥ ɪ^p(t) VZdt ≥ P(t) VZdt
t
(68)
for any t, we get that，I(X; T) ≥ =. Since |T| ≥ 1 and C ≥ 0, we get that our bound in Eq. (63)
is at least
B log(4)|T|1/2I (X; T )1/2 + B|T|3/4 I (X; T )1/4	(69)
B
≥ √m(log≡ + 空M4)	(70)
e	e1/2
Let f (c) = log?/C) + C [1T2 / . It can be verifed that f 0(c) > 0 if c > 0. Since B > 4√2 log(2)
by the definition of B, we have f (B) > f (4√2 log(2)) > 0.746. Therefore, we have
B log(4)|T|1/2I (X; T )1/2 + B|T|3/4 I (X; T )1/4	(71)
B
≥ 0.746√m ≥ log(∣T∣)	(72)
Therefore, if indeed Bp(t)√dt > ɪ for some t, then the bound in Eq. (63) is trivially
true, since H(T | Y ) is within [0, log(|T |)]. Similarly, we can establish a bound for
Pt φ(2√V(p(t | X ∈ X0))) as follows:
nn
X Φ(2√V(p(t | X ∈ X0))) ≤√nB0 log(-)∣T∣1∕2 XI(X0; T∕)1/2 + n1∕4B0∣T∣3∕4 XI(X0; Ti11∕4,
t	i=1	i=1
(73)
where B0
4√2 log(2)
minx∈χo q(χ)
22
Published as a conference paper at ICLR 2021
Plugging Eq. (73) and Eq. (65) into Eq. (42), we get
nn
|H(T | γ) — H (T0 | Y )| ≤√nB log(-)|T|1/2 XI (Xi； Ti)1/2 + n1/4B|T|3/4 XI (Xi； Ti)1/4+
B	i=1	i=1
nn
√nB0log(B)|T|1/2 XMX*?1/ + n1/4B0|T|3/4 X I(Xi-； Ti)1/4
i=1	i=1
(74)
Now we turn to the third summand in Eq. (25), we have to bound |H(T) - H(T0)|.
Recall the definition of -bounded adversarial example. We denote the set of the benign data repre-
sentation t that are within the -ball of t0 by Q(t0). Then for any t ∈ Q(t0), we have
||t0i - ti|| ≤,	(75)
for i = 1, 2, ..., n. We also denote the number of the -bounded adversarial examples around the
benign representation t by c(t). Then we have the distribution of adversarial representation t0 as
follows:
P⑴
C⑴
q(t0)
Σ
t∈Q(t0)
(76)
|H(T) - H(T0)|	(77)
=| =| ≤ =| =|	p(t) log p(t) -	q(t0) log q(t0)|	(78) t	t0 Xp(t) logP(t) - X [( X PI)log( X P≡)] I	(79) t	t0	t∈Q(t0)	t∈Q(t0) XP⑴logP⑴-X X Pt)IOg(Pt))I	(80) t	t0 t∈Q(t0) XP⑴logP⑴一XC⑴ ρθ log( pθ )|	(81) t	t	c(t)	c(t) X P(t) log C(t)I,	(82) t
where the inequality is by log sum inequality. If we denote the C = maxt c(t) which is the maxi-
mum number of -bounded textual adversarial examples given a benign representation tof a word
sequence x, we have
|H(T) - H(T0)|	(83)
≤ | X p(t) log c(t)|	(84)
t
≤ | X p(t) log C| = log C.	(85)
t
Note that given a word sequence x of n with representation t, the number of -bounded textual
adversarial examples c(t) is finite given a finite vocabulary size. Therefore, if each word has at most
k candidate word perturbations, then log C ≤ nlog k can be viewed as some constants depending
only on n and .
Now, combining Eq. (25), Eq.(74) and Eq.(85), We prove the bound in Theorem 3.2.	□
23