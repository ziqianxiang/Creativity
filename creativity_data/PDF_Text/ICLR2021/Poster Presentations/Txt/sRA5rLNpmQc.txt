Published as a conference paper at ICLR 2021
Provably robust classification of
ADVERSARIAL EXAMPLES WITH DETECTION
Fatemeh Sheikholeslami
Bosch Center for Artificial Intelligence
Pittsburgh, PA
fatemeh.sheikholeslami@us.bosch.com
Ali Lotfi Rezaabad *
The University of Texas at Austin
Austin, TX
alotfi@utexas.edu
J. Zico Kolter
Bosch Center for Artificial Intelligence
Carnegie Mellon University
Pittsburgh, PA
zkolter@cs.cmu.edu
Ab stract
Adversarial attacks against deep networks can be defended against either by build-
ing robust classifiers or, by creating classifiers that can detect the presence of ad-
versarial perturbations. Although it may intuitively seem easier to simply detect
attacks rather than build a robust classifier, this has not bourne out in practice
even empirically, as most detection methods have subsequently been broken by
adaptive attacks, thus necessitating verifiable performance for detection mecha-
nisms. In this paper, we propose a new method for jointly training a provably
robust classifier and detector. Specifically, we show that by introducing an addi-
tional “abstain/detection” into a classifier, we can modify existing certified defense
mechanisms to allow the classifier to either robustly classify or detect adversarial
attacks. We extend the common interval bound propagation (IBP) method for cer-
tified robustness under '∞ perturbations to account for our new robust objective,
and show that the method outperforms traditional IBP used in isolation, especially
for large perturbation sizes. Specifically, tests on MNIST and CIFAR-10 datasets
exhibit promising results, for example with provable robust error less than 63.63%
and 67.92%, for 55.6% and 66.37% natural error, for = 8/255 and 16/255 on
the CIFAR-10 dataset, respectively.
1	Introduction
Despite popularity and success of deep neural networks in many applications, their performance
declines sharply in adversarial settings. Small adversarial perturbations are shown to greatly dete-
riorate the performance of neural network classifiers, which creates a growing concern for utilizing
them in safety critical application where robust performance is key. In adversarial training, different
methods with varying levels of computational complexity aim at robustifying the network by finding
such adversarial examples at each training steps and adding them to the training dataset. While such
methods exhibit empirical robustness, they lack verifiable guarantees as it is not provable that a more
rigorous adversary, e.g., one that does brute-force enumeration to compute adversarial perturbations,
will not be able to cause the classifier to misclassify.
It is thus desirable to provably verify the performance of robust classifiers without restricting the ad-
versarial perturbations by inexact solvers, while restraining perturbations to a class of admissible set,
e.g., within an '∞ norm-bounded ball. Progress has been made by ‘complete methods' that use Sat-
isfiability Modulo Theory (SMT) or Mixed-Integer Programming (MIP) to provide exact robustness
bounds, however, such approaches are expensive, and difficult to scale to large networks as exhaus-
tive enumeration in the worst case is required (Tjeng et al., 2017; Ehlers, 2017; Xiao et al., 2018).
* Work was done when the author was an intern at Bosch Center for Artificial Intelligence, Pittsburgh, PA.
1
Published as a conference paper at ICLR 2021
‘Incomplete methods’ on the other hand, proceed by computing a differential upper bound on the
worst-case adversarial loss, and similarly for the verification violations, with lower computational
complexity and improved scalability. Such upper bounds, if easy to compute, can be utilized during
the training, and yield provably robust networks with tight bounds. In particular, bound propaga-
tion via various methods such as differentiable geometric abstractions (Mirman et al., 2018), convex
polytope relaxation (Wong & Kolter, 2018), and more recently in (Salman et al., 2019; Balunovic &
Vechev, 2020; Gowal et al., 2018; Zhang et al., 2020), together with other techniques such as semi-
definite relaxation, (Fazlyab et al., 2019; Raghunathan et al., 2018), and dual solutions via additional
verifier networks (Dvijotham et al., 2018) fall within this category. In particular, recent successful
use of Interval Bound Propagation (IBP) as a simple layer-by-layer bound propagation mechanism
was shown to be very effective in Gowal et al. (2018), which despite its light computational com-
plexity exhibits SOTA robustness verification. Additionally, combining IBP in a forward bounding
pass with linear relaxation based backward bounding pass (CROWN) Zhang et al. (2020) leads to
improved robustness, although it can be up to 3-10 times slower.
Alternative to robust classification, detection of adversarial examples can also provide robust-
ness against adversarial attacks, where suspicious inputs will be flagged and the classifier “re-
jects/abstains” from assigning a label. There has been some work on detection of out-of-distribution
examples Bitterwolf et al. (2020), however the situation in the literature on the detection of adver-
sarial examples is quite different from above. Most techniques that attempt to detect adversarial
examples, either by training explicit classifiers to do so or by simply formulating “hand-tuned” de-
tectors, still largely look to identify and exploit statistical properties of adversarial examples that
appear in practice Smith & Gal (2018); Roth et al. (2019). However, to provide a fair evalua-
tion, a defense must be evaluated under attackers that attempt to fool both the classifier and the
detector, while addressing particular characteristics of a given defense, e.g., gradient obfuscation,
non-differentability, randomization, and simplifying the attacker’s objective for increased efficiency.
A non-exhaustive list of recent detection methods entails randomization and sparsity-based defenses
(Xiao et al., 2019; Roth et al., 2019; Pang et al., 2019b), confidence and uncertainty-based detection
(Smith & Gal, 2018; Stutz et al., 2020; Sheikholeslami et al., 2020), transformation-based defenses
(Bafna et al., 2018; Yang et al., 2019), ensemble methods (Verma & Swami, 2019; Pang et al.,
2019a), generative adversarial training Yin et al. (2020), and many more. Unfortunately, existing
defenses have largely proven to have poor performance against adaptive attacks (Athalye et al., 2018;
Tramer et al., 2020), necessitating provable guarantees on detectors as well. Recently Laidlaw &
Feizi (2019) have proposed joint training of classifier and detector, however it also does not provided
any provable guarantees.
Our contribution. In this work, we propose a new method for jointly training a provably robust
classifier and detector. Specifically, by introducing an additional “abstain/detection” into a classi-
fier, we show that the existing certified defense mechanisms can be modified, and by building on
the detection capability of the network, classifier can effectively choose to either robustly classify or
detect adversarial attacks. We extend the light-weight Interval Bound Propagation (IBP) method to
account for our new robust objective, enabling verification of the network for provable performance
guarantees. Our proposed robust training objective is also effectively upper bounded, enabling its
incorporation into the training procedure leading to tight provably robust performance. While tight-
ening of the bound propagation may be additionally possible for tighter verification, to the best of
our knowledge, our approach is the first method to extend certification techniques by considering
detection while providing provable verification. By stabilizing the training, as also used in simi-
lar IBP-based methods, experiments on MNIST and CIFAR-10 empirically show that the proposed
method can successfully leverage its detection capability, and improves traditional IBP used in iso-
lation, especially for large perturbation sizes.
2	Background and related work
Let us consider an L-layer feed-forward neural network, trained for a K-class classification task.
Given input x, it will pass through a sequential model, with hl denoting the mapping at layer l,
recursively parameterized by
Zl = hl (Zl-I)= σι(W>zi-i + bl), l = 1,…，L Wl ∈ Rnl-1×nl, b ∈ Rnl	(1)
where σl (.) is a monotonic activation function, z0 denotes the input, and zL ∈ RK is the pre-
activation unnormalized K-dimensional output vector (nL = K and σL(.) as identity operator),
2
Published as a conference paper at ICLR 2021
referred to as the logits. Robust classifiers can be obtained by minimizing the worst-case (adversar-
ial) classification loss, formally trained by the following min-max optimization Madry et al. (2017)
minimize
θ
,E C max '(fθ(X + δ),y)
(x,y)〜D L δ∈∆e
(2)
where θ denotes network parameters, vector fθ(X) = zL is the logit output for input X, `(.) is
the misclassfication loss, e.g., 'χent(∙) defined as the cross-entropy loss, and ∆e denotes the set
of permissible perturbations, e.g., for '∞-norm ball of radius e giving ∆e := {δ | ∣∣δk∞ ≤ e}.
Although augmenting the training set with adversarial inputs, obtained by approximately solving
the inner maximization in Eq. 2, empirically leads to improved adversarial robustness Madry et al.
(2017); Shafahi et al. (2019); Wong et al. (2019); Zhang et al. (2019), inexact solution of the inner
maximization prevents such methods from providing provable guarantees. In critical applications
however, provable verification of classification accuracy against a given threat model is crucial.
2.1	performance verification and network relaxation
Given input (X, y), a classification network is considered verifiably robust if all of its perturbed
variations, that is X + δ for ∀δ ∈ ∆, are correctly classified as class y. Such verification can be
effectively obtained by
p↑	= min	c>iZL	where ,	ZL	:=	{zl∣zi	=	hι(zι-ι), l = 1,…，L, zo = X +	δ,	∀δ	∈	∆e}
zL ∈ZL ,
where cy,i = ey - ei for i = 1, 2, .., K, i 6= y, and ei is the standard ith canonical basis vector.
If p* > 0 ∀i = y, then the classifier is verifiably robust at point (x, y) as this guarantees that
zi ≤ zy ∀i 6= y for all admissible perturbations δ ∈ ∆ .
The feasible set ZL is generally nonconvex, rendering obtaining p* intractable. Any convex relax-
ation of ZL however, will provide a lower bound on p*, and can be alternatively used for verification.
As outlined in Section 1, various relaxation techniques have been proposed in the literature. Specif-
ically, IBP in (Mirman et al., 2018; Gowal et al., 2018) proceeds by bounding the activation zl of
each layer by propagating an element-wise bounding box using interval arithmetic for networks with
monotonic activation functions. Despite its simplicity and relatively small computational complex-
ity (computational requirements for bound propagation for a given input using IBP is equal to two
forward passes of the input), it can provide tight bounds once the network is trained accordingly.
Specifically, starting from the input layer z0, it can be bounded for the perturbation class δ ∈ ∆ as
zo = x 一 el and zo = X + e1, and ZI for the following layers can be bounded as
Zl = σι(W> Zj + ZlT -|W>| j -ZlT ), Zl = σι(W> zl-1 + ZlT + |W>| Zj - ZlT ),
(3)
where | ∙ | is the element-wise absolute-value operator. The verification problem over the relaxed
feasible set ZL := {zl | zL,i ≤ zL,i ≤ zL,i}, where ZL ⊆ ZL is then easily solved as
P* = min c>,izL ≥ min c>,izL = ZL,y - %.	⑷
zL∈ZL	Zl∈Zl
2.2 Robust training of verifiable networks
It has been shown that convex relaxation ofZL can also provide a tractable upper bound on the inner
maximization in Eq. 2. While this holds for various relaxation techniques, focusing on the IBP let
us define
JIB,θP(x, y)	:=	[J1IBP,	J2IBP, ...,	JKIBP]	where	JiIBP	:= min	cy>,izL	(5)
zl∈Zl
with (θ, e) implicitly influencing ZL (dropped for brevity), and upperbound the inner-max in Eq. 2
max 'xent(fθ(x + δ),y) ≤
'xent(-Je^J(X,y),y),	'xent(Z, C) := 一 log
exp(zc)
Pi exP(Zi)
(6)
By using this tractable upper bound of the robust optimization, network can now be trained by
minimize £ (1 一 κ)'xent(-JBP(x, y), y) + κγ'xent(fθ(x),y),
(x,y)∈D
(7)
3
Published as a conference paper at ICLR 2021
where γ trades natural versus robust accuracy, and κ is scheduled through a ramp-down process to
stabilize the training and tightening of IBP Gowal et al. (2018) (where γ = 1 is selected therein).
3	Verifiable classification with detection
In this paper, we propose a new method for jointly training a provably robust classifier and detector.
Specifically, let us augment the classifier by introducing an additional “abstain/detection”. This can
be readily done by extending the K-class classification task to a (K + 1)-class classification, with
the (K + 1)-th class dedicated to the detection task, and the maximum weighted class is finally
chosen as the classification output. The classifier is then trained such that adversarial examples, or
ideally any other example that the network would misclassify, are classified in this abstain class,
denoted by a, thus preventing incorrect classification.
Formally, the classifier can be denoted as in Eq. 1, with the only difference that the final output is
K + 1 dimensional, i.e., zL ∈ RK+1; simply by substituting the last fully-connected weight matrix
WL of dimension nL × K with that of dimension nL × (K + 1), and similarly for bL.
3.1	Verification problem for classification with abstain/detection
It is desirable to provably verify performance of the joint classification/detection. In contrast to
existing robust classifiers, however, on a perturbed image x + δ, the classification/detection task is
considered successful if the input is classified either as the correct class y, or as the abstain class
a; as both cases prevent misclassification of the adversarially perturbed input as a wrong class. On
clean natural images however, classification/detection is considered successful only if it is classified
as the correct class y, and abstaining is considered misclassification.
In order to certify performance in adversarial settings, it is now sufficient to verify that the network
satisfies the following for a given input pair (x, y) and δ ∈ ∆ and i = 1, .., K, i 6= y:
max{cy>,iz, ca>,iz}	≥ 0	∀z ∈	ZL	:=	{zL|zl	=	hl(zl-1) l = 1, ..., L, z0 = x +	δ,	∀δ	∈ ∆}
,	,	(8)
where cy := ey - ei and ca := ea - ei, a denotes the “abstain” class, and the dependence ofZL on
(x, y, , θ) is omitted for brevity. Verification can be done effectively by seeking a counterexample
πi := min maχ{c>,iz, c>,iz}
z∈ZL
(9)
If ∏ ≥ 0 ∀i = y , the specification is then satisfied and the performance is verified.
Similar to previous verification methods, to overcome the non-convexity of the optimization in Eq.
C	1	1	1 .1	11	1	1∙	,1 i' ∙1 1	<	/	1	S .
9,	one can lower bound the problem by expanding the feasible set ZL ⊆ ZL , where ZL is convex,
as stated in Theorem 1, and proved in Appendix A.1.
Theorem 1:	For any convex ZL s.t. ZL ⊆ ZL, Eq. 9 can be bounded by the convex relaxation
0mη≤X1 W	S+(1 - η) cy,i)>z ≤ mzL max{c>,/，c>,㈤
(10)
Although Theorem 1 holds for any convex relaxation of ZL, for IBP relaxation in Gowal et al.
(2018) it can be further simplified by substituting z = WL>zL-1 + bL, thus not propagating the
intervals through the last layer for tighter bounding, and solved analytically as follows.
Theorem 2:	The optimization in Eq. 9 can be lower-bounded by the convex optimization
Ji(x, y) = max min	(ω1 +η ω2)>zL-1+η ω3+ω4
0≤η≤1ZL-1∈Zl-1
≤ min max{
z∈ZL
cy>,iz,ca>,iz} (11)
in which ω1 := WL>cy,i, ω2 := WL>(ca,i - cy,i), ω3 := bL>(ca,i - cy,i), ω4 := bL>cy,i and convex
set Zl-ι is a convex relaxation of Zl-i on the hidden values at L 一 L Furthermore, Ji(x,y) can
be analytically obtained as outlined in Alg. 1
Note that since η is the dual variable, any selection within the feasible set serves as a (looser but
valid) lower bound, while the maximization makes the bound tight; see Appendix A.2 and A.3 for
proof and a step-by-step algorithm description. Similar to other convex relaxation-based verification
methods, in order for a networks to provide verifiable performance, one needs to incorporate bound
propagation in training.
4
Published as a conference paper at ICLR 2021
Algorithm 1 Solution for Ji (x, y) in Theorem 2
1:	Input. Bounds on layer L - 1 : Zl-i, %l-i, and weight matrix WL
2:	ω1 = WLcy,i and ω2 = WL(ca,i - cy,i), ω3 := bL>(ca,i - cy,i), and ω4 := bL>cy,i
3:	Z = [Zι,…,ζnL] ：= -ω1∕ω2 and vector of indices S that sorts Z , i.e., Zsi ≤ ∙∙∙ ≤ ZsnL I
4： Ui = ∏s(ωι ◦ Zl-i) , Ui = ∏s(ωι ◦ Zl-i), U2 := ∏s(ω2 ◦ Zl-J , u2 := ∏s(ω2 ◦ Zl-i)
where operators ◦ and Πs(.) denote element-wise multiplication, and permutation according to
indices s, respectively.
5:	m = minζsj≥0 j and M = maxζsj≤1 j for j = 1, ..., nL-i
6:	for η = 0,Zs∙ ,Zsm + 1 ,∙∙∙,Zsm-i ,ZsM ,1 dθ
7:	Compute
nL-1
g(η) = X (1{ω1,j+ηω2,j≤0} (UIj + ηu2,j) + 1{ωι,j+ηω2,j≥o} (ui,j + ηu2,j)) + ηω + ω4
j=i
8:	return max g(η) over the computed values.
4 Training a verifiable robust clas sification with detection
In order to train a robust classifier with detection, let us start by formalizing the objective of an
adversarial attacker. Naturally, an adaptive attacker’s objective is to craft perturbation δ such that
it simultaneously evades detection and causes misclassification. Formally, this can be tackled by
seeking δ such that loss corresponding to the winner of the two classes y and a (higher logit leading
to smaller cross-entropy loss) is maximized, i.e.,
maxmin {'xent(fθ(X + δ), y),'xent(fθ(X + δ), a)}
(12)
where 'xent(z, C) denotes the cross-entropy loss for class C = y and C = a, and I = {1, 2,...,K,a}
denotes the class index set with K + 1 elements.
Let us now define
LabbUaitn(x, y； θ) ：= max min {'xe∏t∖a(fθ (X + δ), y), 45nt∖y(fθ (x + δ), a)}
(13)
in which the inner maximization is closely related to that of the adversarial objective in Eq. 12 with
a small difference: loss terms 'xent∖a and 'xent∖y are defined as
exp(zy)	exp(za)
'xent∖a(z,y) := -log ( P exp(zQ ,and 'xent∖y(X,a) := -log ( P exp(zQ.
i∈I∖{a}	i∈I∖{y}
This small alteration to the cost, while not changing the minimization “winner” between the true
class y and rejection class a in Eq. 12 and 13, i.e.,
[za ≤	Zy	⇒	'xent(fθ (X + δ),y) ≤	'xent(fθ (X + δ),a)	and	'xent∖a(fθ (X + δ),y) ≤	'xent∖y (fθ (X	+	δ),a)
Izy ≤	Za	⇒	'xent(fθ (X + δ),a) ≤	'xent(fθ (X + δ),y)	and	'xent∖y(fθ (X + δ),a) ≤	'xent∖a(fθ (X	+	δ),y)
favorably influences the training process. That is so since, for δ such that, for instance za < zy,
minimizing LaobUsin(x, y； θ) during training reduces to minimizing 'xent(fθ(x + δ), y) which in turn
leads to further increasing zy while decreasing the logit value za ; and similarly, increasing zy while
decreasing zy if zy < za . Intuitively however, the true objective of the classifier augmented with
detection on adversarial examples is to increase both zy and za while reducing zj , ∀j 6= a, y; thus
preventing any gap in between the boundary of the classes a and y, which can potentially lead to
successful adaptive attacks. Hence, minimizing Eq. 12 would be in contrast with the true underlying
objective, and Eq. 13 simply prevents the raised issue.
Upon defining LnatUral(x, y； θ) ：= 'xent(fθ(x), y) and LrObUSt(x, y; θ) ：= maxδ∈∆ 'xent(fθ(x + δ), y),
we then define the overall training loss as
L = Lrobust (x, y; θ) + λi Lraob4 suastn (x, y; θ) + λ2 Lnatural (x, y; θ),
(14)
5
Published as a conference paper at ICLR 2021
where Lnatural (x, y; θ) captures the misclassification loss of the natural (clean) examples, and
Lrobust(x, y; θ) denotes that of adversarial examples without considering the rejection class, i.e., sim-
ilar to that of Gowal et al. (2018), and parameters (λ1, λ2) trade-off clean and adversarial accuracy.
To train a robust classifier, we proceed by minimizing the overall loss Eq. 14, by first upperbounding
Lrobust(x, y; θ) and Lraobbsutasitn(x, y; θ).
4.1	Upperbounding the training loss
Using Theorem 2, and restricting 0 < η ≤ η ≤ η < 1, let us now define J,(x, y), where trivially
Jiη,η(x,y) := max	(ωι + ηω2)>ZL-i + η ω3 + ω4 ≤ Ji(X,y)	(15)
0≤n≤n≤n≤1
and can also be solved analytically similar to Theorem 2. By generalizing the findings in Wong &
Kolter (2018); Mirman et al. (2018), we can upper bound the robust optimization problem using our
dual problem in Eq. 15, according to the following Theorem, which we prove in Appendix A.4.
Theorem 3: For any data point (x, y), and e > 0, and for any 0 ≤ η ≤ η ≤ 1, the adversarial loss
Lraobbsutasitn(x, y; θ) in Eq. 13 can be upper bounded by
LrobuSt (χ,y; θ) ≤ LrobuSt (χ, y; θ) ：= 'xent∖a(-J3θ(χ,y),y) = 'xe∏t∖y(-Je,θ(χ,y),a)	(16)
where Je,θ(x, y) is a (K + 1)-dimensional VeCtOr valued at index i as [Je,θ (x, y)]i = JF(x, y).
Note that maximization over η for obtaining J;n,n (x, y) can be done either by bisection (concave
maximization) or by following Alg. 1 and substituting m = minζι ≥η V , and M = maxζsι^ <n V
Remark 1. Setting η = η = 0 forces η = 0 which reduces J:η (x, y) in Eq. 15 to that in Eq. 5, .i.e,
JIBP(x, y) = Jη,η(x, y)∣η=η=o, also bounding loss term LnaturaI(x, y; θ) as
Lrobust (x, y; θ) ≤ Lrobust(x, y; θ) := 'xent(-JiBP(X,y),y).	(17)
Remark 2. While setting η = 0 and η= 1 gives tighter bounds, (and is thus used for the verification
counterpart in Theorem 2), strictly setting 0 < η ≤ η < 1 empirically yields better generalization of
the network. This can be intuitively understood by rewriting ω1 + ηω2 = WL>(ηca,i + (1 - η)cy,i)
which is a convex combination of the verification constraints for the correct and the abstain class.
Thus η 6= 0 6= 1 will lead to minimizing a combination of both terms, preventing gaps in between
the two classes. Also, higher values of η increase the influence of the term corresponding to the
abstain case, and vice versa, whose tuning can promote abstaining by considering how desirable
such outcome is (or is not).
Utilizing upperbounds in Eq. 16 and Eq. 17, we can proceed to training the network by minimizing
the tractable upperbound on the overall loss
min L ≤ min'xent(-JeBP(x, y), y) + λι'xent∖y(-Je,θ(x,y),a) + λ2'natural(fθ(x),y)	(18)
Note that setting λ1 = 0 and γ = λ2 - and incorporation of a ramp-down process by parameter κ as
detailed in Section 5 - reduces the training in Eq. 18 to that of Gowal et al. (2018) without detection.
Complexity. Since given IBP bounds on zL-1, the solution to Eq. 16 is analytically available (that
is after sorting whose complexity is negligible in comparison with forward pass), computing Eq. 18
imposes the same computational complexity as in IBP, which is twice the normal training procedure,
as it requires propagating the upper and lower bounds via forward pass.
5 Experiments
Empirical performance of the proposed robust classification with detection on MNIST-10 and
CIFAR-10 datasets is reported in this section, and is compared with the state-of-the-art alternatives.
The training procedure is stabilized as detailed next 1.
1Code is available at https://github.com/boschresearch/robust_classification_
with_detection
6
Published as a conference paper at ICLR 2021
5.1	Stabilizing the training procedure
We incorporate the following mechanisms to stabilize the training procedure in our tests, where the
first two have been previously used in (Gowal et al., 2018) and (Zhang et al., 2020) as well.
Ramp down of κ: To stabilize the trade-off between nominal and verified accuracy, let us introduce
parameter κ in the overall loss by trading the natural and robust loss as
L =(1 - K) (Lrobust(x,y; θ) +,λιLaOSn(x,y; θ)
^^^^{^^^^
Robust loss
+κ λ2Lnatural(x, y; θ)
{}
Natural loss
(19)
Setting κ = 0.5 renders the optimization identical to that in Eq. 18. During the training however, we
incorporate a ramp down procedure where κ starts at value κstart = 1, thus training the model to fit
the natural data, and slowly decreasing it to value κend = 0.5, similar to that in Gowal et al. (2018).
Ramp up of : It is very important during the training process to start at = 0 and gradually
increase it to train, while also setting train larger than test can improve generalization.
Ramp down of η and n： Setting 0 < η and η < 1 helps with better generalization. Furthermore,
setting large η and η promotes the abstain class in loss term LaobUsin by increasing the weight of ω2
in Eq. 15. ThUs, We can further stabilize the training process through a ramp down procedure where
these parameters start at η = 么.t and η = ηstart, and are gradually reduced to η = /口4 and η = ηend,
with ηend < ηstart and ηend < ηstart.
Furthermore, although the term LrobUSt(x, y; θ) could in theory be excluded from the training process,
as the term Lnatural(x, y; θ) prevents the degenerate solution of always classifying all images in the
abstain class, it’s inclusion empirically helps the stability of the training process.
5.2	Empirical results on MNIST and CIFAR 1 0
The classification networks are identical to the large network in Gowal et al. (2018), also detailed
in Table 2, trained by minimizing the loss in Eq. 18 with the above stabilizing schemes. Selec-
tion of parameters for each datasets is detailed in Appendix B. Since most recent detector networks
have shown very low performance against adaptive attacks, and lack provable performance Tramer
et al. (2020), we only compare the performance with other provable robust classification methods,
while focusing on the different decomposition in the reported natural and robust accuracy among
these two. As numbers in Table 1 suggest, the proposed detection/classification network shows
improved robustness against other methods, including IBP in isolation (without the detection capa-
bility), specially against larger perturbations in the CIFAR-10 dataset, which intuitively is pleasing:
as larger perturbations are naturally more distinguishable, the detection capability of the network is
successfully leveraged for improving the adversarial robustness. Let us now take a closer look at the
performance by focusing on the detection capability.
Effectiveness of the detection class. By nature, the proposed classification “adaptively chooses”
between (robust) correct classification and detection of adversarial or difficult inputs during the
training. This gives rise to two phenomena:
(1)	In verifiably robust methods, natural image accuracy declines as robustness improves. In the pro-
posed approach however, a considerable number of misclassified natural inputs are in fact abstained
on, which in certain applications is more favorable than assigning them to a wrong class, as classi-
fiers without detection capability would: compare 30.5% abstain and 25.6% ‘wrong-class misclassi-
fication’ (other than abstain and the correct class) in IBP-with-detection, with that of 53.7% ‘wrong-
class’ misclassification in IBP on natural CIFAR-10 images in networks trained for = 8/255.
(2)	Regardless of the training procedure, the proposed classifier with detection can still be verified
using verification in Eq. 4 to obtain its guaranteed robustness with only considering the correct
class. Thus, comparing this verification percentage with that of Eq. 11 highlights the effectiveness
of the abstain class in detecting perturbed images and increasing robustness: for instance, using
our method 76.07% maximum robust error successfully decreases to 63.63% by considering the
detection capability, on CIFAR-10 trained for = 8/255, compared to 69.92% in IBP without
detection.
7
Published as a conference paper at ICLR 2021
dataset	attack	method	standard err	verified err	pgd-attack-success
		IBP	212	847	678
		IBP w/ detection	4.34	5.98		4.15
	test = 0.3	Best recorded in literature			
	train = 0.4	IBP (Gowal et al.,2018)	1.66	8.21	6.12
		IBP-CROWN (	get al.,202 )	1.82	7.02	6.05
		XiaOetal (201 )	2.67	19.32	7.95
MNIST		Mirman eta (	)	2.8	11.2	4.6
		Balunovic & VeChev ( 020)	2.7	14.3	-
		Wong&Kolter ( 01 )	14.87	43.10	-
		^TBP	274	14.80	ΓTΓT4
	Qest = 0.4	IBP w/ detection	4.79	11.29		7.55
	€ train = 0.4	Best recorded in literature			
		IBP (Gowal et al.,2018)	1.66	15.01	10.34
		IBP-CROWN (	get al.,202 )	2.17	12.06	9.47
		IBP	38.54	5521	4972
		IBP w/ detection	34.66	57.9		47.2
		Best recorded in literature			
	Qtest = 2/255	IBP (Gowal et al.,2018)	29.84	55.88	49.98
	€train = 2.2/255	IBP-CROWN (	get al.,202 )	28.48	46.03	40.28
		Balunovic & VeChev ( 020)	21.6	39.5	-
		Mirman eta (	)	38.0	47.8	-
		Wong&Kolter ( 01 )	31.72	46.11	-
		Xiao et al (201 )	38.88	54.07	50.08
		^TBP	53:69	69.92	6517
		IBP w/ detection	55.60	63.63		49.22
CIFAR-10		Best recorded in literature			
	€test = 8/255	IBP (Gowal et al.,2018)	50.51	68.44+	65.23
	€train = 8.8/255	IBP-CROWN (	get al.,202 )	54.02	66.94	65.42
		Mirman eta (	)	43.8	72.8	65.3
		Balunovic & VeChev ( 020)	48.3	72.5	-
		Xiao et al (201 )	59.55	79.73	73.22
		Wong&Kolter ( 01 )	71.33	78.22	-
		^TBP	68:97	78.12	76:66
	etest = 16/255	IBP w/ detection	66.37	67.92		58.20
	dain = 16.7/255	Best recorded in literature			
		IBP-CROWN(	getal.,202 )	66.06	76.80	75.23
Table 1: The verified, standard (clean), and PGD attack errors for models trained on MNIST and CIFAR-
10. IBP with detection is to be compared with IBP (without detection capability) to emphasize the suc-
cessful utilization of the detection capability of the network in increasing its verifiable as well as empirical
performance. For a more detailed decomposition of the standard and robust error terms see Fig. 1.
+ As reported in Zhang et al. (2020), achieving the 68.44% IBP verified error requires extra PGD adversarial
training loss, without which the verified error is 72.91% (LP/MIP verified) or 73.52% (IBP verified), thus
our result should be compared to 73.52%.
*	Best reported numbers for IBP are computed using mixed integer programming (MIP), which are strictly
smaller than IBP veified error rates, see table 3 and 4 in Gowal et al. (2018). For fair comparison, we report
IBP verified error rates from table 3 therein.
*	* Best reported results from the literature may use different network architectures, and empirical PGD
error rate may have been computed under different settings, e.g., number of steps and restarts.
*	** Number in the IBP rows in this table are the best between (Zhang et al., 2020) and our experiments,
while results from (Gowal et al., 2018) are reported under best literature record for IBP.
f It is important to note that unlike robust classification, the proposed joint ClassifiCation/detection does
successfully leverage the detection capability to decrease the verified error rate by rejecting some adversar-
ial examples, which makes direct comparison of these values difficult. However since there exists no other
verifiable detection scheme, such comparison is made here to show the effect of successful detection; see
Figure 1 for a detailed discussion on this.
See Fig. 1 for decomposition of the performance metrics of the proposed network over CIFAR-10
dataset, demonstrating the effectiveness of the abstain class in detecting “difficult” natural images
while also increasing the robustness certificate over adversarial inputs.
5.3 Natural versus adversarial error tradeoff
Reporting a single set point in the Pareto Frontier as reported in Table 1 gives limited understanding
on how different methods trade off natural versus robust error. To address this, a more detailed study
on this trade-off in IBP-based robust classification with and without detection is discissed here.
In order to get the best performance for IBP-based robust training without detection (that is λ1 = 0),
and since it is not known whether varying κend or λ2 will lead to a better performance, we have
8
Published as a conference paper at ICLR 2021
correct class. abstain misclassification
(a) Accuracy on natural (clean) images
robust class. robust class, w∖ detection non-verified
(b) Verified accuracy on adversarial images
0.45	0.50	0.55	0.60	0.65
natural error
• IBP (Λι = 0), varying λ2
Figure 1: Decomposition of accuracy and verified accuracy on CIFAR-10 dataste: the detection
capability of the network can increase robustness by adaptively abstaining on adversarial inputs
while also abstaining on some natural images rather than misclassifying them.
0.55	0.60	0.65	0.70
natural error
natural error
IBP (Λι = 0), varying Kend ∙ IBP w/ detection (Λι = 1), varying %
0.64	0.66	0.68
(a) CIFAR-10, = 8/255
(b) CIFAR-10, = 12/255	(c) CIFAR-10, = 16/255
Figure 2: Naural versus robust error tradeoff for IBP (λ1 = 0) and IBP-with-detection (λ1 > 0)
on CIFAR-10 dataset for various perturbation sizes = 8/255, 12/255, 16/255. Lower curve is
better. IBP-with-detection is effectively utilizing its detection capability to adaptively trade natural
and robust performance, leading to improved certified robustness against adversarial perturbations.
trained the classification networks in two ways: (1) setting κend = 0.5 and varying λ2 ∈ [0, 3], and
(2) setting λ2 = 1 and varying κend ∈ [0, 0.5], to get multiple set points along the Pareto Frontier.
Similarly, for IBP-with-detection-based classification, we have set κend = 0.5 , λ1 = 0.6, 0.8, 1.0 for
MNIST and λ1 = 1.0 for CIFAR-10, and varied λ2 ∈ [1 4] to get various points along the frontier.
The network is trained for various values, with other training parameters as stated in Appendix B.
Results are plotted in Fig. 2 and 3 (presented in the Appendix due to space limitation). As shown,
the classifier enhanced with detection capability is better able to trade natural and robust accuracy,
thus attaining higher robustness by trading small decrease in natural accuracy. This together with
the fact that the natural accuracy decrease is also partly handled by abstaining of such natural im-
ages that would have been misclassified (as one of the original K classes) otherwise, demonstrates
the effective utilization of the detection capability in the proposed method. It is important to note
that IBP w/detection allows us to obtain additional regions on this Pareto frontier that traditional-
robust-classifiers without detection cannot obtain, and could potentially provide additional gain to
what is achievable by other various improvement techniques such as tighter relaxation and bound
propagation methods.
6 Conclusion
We proposed a new method for jointly training a provably robust classifier and detector. By intro-
ducing an additional “abstain/detection” into a classifier, we have proposed a verification scheme for
classifiers with detection under adversarial settings, and shown that such networks can be efficiently
trained be extending the common IBP relaxation techniques. The effectiveness of the proposed de-
tection scheme with provable guarantees versus SOTA robust verifiable classification methods is
corroborated by empirical tests on MNIST and CIFAR-10, specially against large perturbations.
9
Published as a conference paper at ICLR 2021
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. Proceedings of Machine Learning
Research, pp. 274-283. PMLR, 10-15 JUl 2018.
Mitali Bafna, Jack Murtagh, and Nikhil Vyas. Thwarting adversarial examples: An l0-robust sparse
FoUrier transform. In Advances in Neural Information Processing Systems, pp. 10075-10085,
2018.
Mislav BalUnovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.
In International Conference on Learning Representations, 2020.
JUlian Bitterwolf, Alexander Meinke, and Matthias Hein. Certifiably adversarially robUst detection
of oUt-of-distribUtion data. In Advances in Neural Information Processing Systems, volUme 33,
pp. 16085-16095, 2020.
KrishnamUrthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and PUshmeet Kohli.
A dUal approach to scalable verification of deep networks. In UAI, volUme 1, pp. 550-559, 2018.
RUediger Ehlers. Formal verification of piece-wise linear feed-forward neUral networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer,
2017.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accUrate estimation of lipschitz constants for deep neUral networks. In Advances in Neural
Information Processing Systems 32, pp. 11427-11438. 2019.
Sven Gowal, KrishnamUrthy Dvijotham, Robert Stanforth, RUdy BUnel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and PUshmeet Kohli. On the effectiveness of interval
boUnd propagation for training verifiably robUst models. arXiv preprint arXiv:1810.12715, 2018.
Cassidy Laidlaw and Soheil Feizi. Playing it safe: Adversarial robUstness with an abstain option.
arXiv preprint arXiv:1911.11253, 2019.
Aleksander Madry, Aleksandar Makelov, LUdwig Schmidt, Dimitris Tsipras, and Adrian VladU.
Towards deep learning models resistant to adversarial attacks. ICLR, 2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robUst neUral networks. In International Conference on Machine Learning, pp. 3578-3586,
2018.
Matthew Mirman, Gagandeep Singh, and Martin Vechev. A provable defense for deep residUal
networks. arXiv preprint arXiv:1903.12519, 2019.
TianyU Pang, KUn XU, Chao DU, Ning Chen, and JUn ZhU. Improving adversarial robUstness via
promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970-4979,
2019a.
TianyU Pang, KUn XU, and JUn ZhU. MixUp inference: Better exploiting mixUp to defend adversarial
attacks. In International Conference on Learning Representations, 2019b.
Aditi RaghUnathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. ICLR, 2018.
Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting
adversarial examples. In International Conference on Machine Learning, pp. 5498-5507, 2019.
Hadi Salman, Greg Yang, HUan Zhang, Cho-JUi Hsieh, and PengchUan Zhang. A convex relaxation
barrier to tight robUstness verification of neUral networks. In Advances in Neural Information
Processing Systems 32, pp. 9835-9846. 2019.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng XU, John Dickerson, Christoph
StUder, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In
Advances in Neural Information Processing Systems, pp. 3358-3369, 2019.
10
Published as a conference paper at ICLR 2021
Fatemeh Sheikholeslami, Swayambhoo Jain, and Georgios B Giannakis. Minimum uncertainty
based detection of adversaries in deep neural networks. In Information Theory and Applications
Workshop (ITA). IEEE, 2020.
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detec-
tion. arXiv preprint arXiv:1803.08533, 2018.
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial training: Gener-
alizing to unseen attacks. In International Conference on Machine Learning, 2020.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. ICLR, 2017.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Gunjan Verma and Ananthram Swami. Error correcting output codes improve probability estimation
and adversarial robustness of deep neural networks. In Advances in Neural Information Process-
ing Systems,pp. 8643-8653, 20l9.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. volume 80 of Proceedings of Machine Learning Research, pp. 5286-5295.
PMLR, 10-15 Jul 2018.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.
In International Conference on Learning Representations, 2019.
Chang Xiao, Peilin Zhong, and Changxi Zheng. Resisting adversarial attacks by k-winners-take-all.
arXiv preprint arXiv:1905.10510, 2019.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Mahi Shafiullah, and Aleksander Madry. Training for
faster adversarial robustness verification via inducing relu stability. In International Conference
on Learning Representations, 2018.
Yuzhe Yang, Guo Zhang, Dina Katabi, and Zhi Xu. Me-net: Towards effective adversarial robustness
with matrix estimation. In International Conference on Machine Learning, pp. 7025-7034, 2019.
Xuwang Yin, Soheil Kolouri, and Gustavo K Rohde. Gat: Generative adversarial training for ad-
versarial example detection and robust classification. In International Conference on Learning
Representations, 2020.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In
International Conference on Learning Representations, 2020.
11
Published as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Theorem 1
Since ZL ∈ ZL it trivially holds that
min	max{c>,iz, c>,iz} ≤ zmZn	max{c>,iz, c>,iz}	QO)
The lower bound is now a convex minimization, which can be rewritten as
min	max{cy>,iz, ca>,iz} = min τ s. t. cy>,iz ≤ τ ,ca>,iz ≤ τ.
z∈ZL	τ,z∈ZL
Defining the slack variables ηa ≥ 0 and ηy ≥ 0 for the inequality constraints, the Lagrangian can be
written as
L(τ, z, ηa, ηy) = τ +ηa(ca>,iz - τ) +ηy(cy>,iz - τ)
and minimizing L(τ, z, ηa, ηy) with respect to the primal variable τ, yields ηa + ηy = 1. Defining
η := ηa = 1 - ηy , and using the fact that the dual maximization always serves as a lower bound on
the primal we get
max min η ca,i + (1 - η) cy,i>z ≤ min	max{cy>,iz,ca>,iz}.
0≤η≤1 z∈Zl	z	z∈Zl
A.2 Proof of Theorem 2
Following on the statement of Theorem 1 and by substituting z = WL>zL-1 + bL, we get
max min
0≤η≤1ZL-1∈Zl-1
η ca,i +(1-η)
≤ min
Z∈Zl
max{cy>,iz, ca>,iz} (21)
which can be reordered as
max min	(ω1 + ηω2)>zL-1 + η ω3 + ω4	(22)
0≤η≤1 Zl-i≤zl-i≤Zl-i
where ω1 := WLcy,i, ω2 := WL(ca,i - cy,i), ω3 := bL>(ca,i - cy,i), and ω4 := bL>cy,i, which
then equals
max	3 + ηω2)>ZL-i + η ω3 + ω4	(23)
0≤η≤1	-
where minimization w.r.t. zL-1 is solved by the (this is under the setting for most networks with
positive activations, and thus lower bound Zl is always non-negative)
[ZL-l]j
ΠzL-1]j
UZL-1]j
if	[ω1 + ηω2]j ≤ 0
if	[ω1 + ηω2]j ≥ 0
(24)
and can be rewritten as
max
0≤η≤1
X hω1 + ηω2i j (1{ωι,j+ηω2,j ≤0} [zL-1 ]j + 1{ωι,j +ηω2,j ≥0}[zL-lj) +η ω3 + ω4 (25)
and can be rewritten as
nL-1
Omnaxi〉： (1{ωι,j +ηω2,j≤0} [ω1 ◦ zL-1 + ηω2 ◦ zL-1]j + 1{ωι,j +ηω2,j≥0} [ω1 ◦ zL-1 + ω2 ° zL-1] j
+ η ω3 + ω4	(26)
where “◦” denotes the elementwise multiplication. Thus, due to the concavity of the dual, optimal
η can be found by evaluationg the objective in between the break points which are given by ζ :=
[ζ1, ..., ζnL-1] with its j-th element defined as ζj := -ω1,j /ω2,j.
12
Published as a conference paper at ICLR 2021
To do this, let us use s to denote the nL-ary tuple of indices that sorts ζ. That is
ζ = [ζi, ..., ζnL-1 ] := Πs(ζ) := [ζs1 , ..., ζsnL-1 ] s.t. ζs1 ≤ ... ≤ ζsnL-1
• .1	. TT ∕∖1	. ∙	. 1	.	Γ∙ ∙ .	.	1 ∙	.	1,1, Z	Λ∙ ∖ / ■
with operator Πs(.) denoting the permutation of its arguments according to s, such that ζi = ζsi ∀i,
and ζ is sorted in the increasing order .
We can also rewrite the problem by summing over the indices in the sorting set s instead, as
nL-1
max
0≤η≤1
〉：I 1{ωι,j +ηω2,j≤0} [ω1 ◦ zL-1 + ηω2 ◦ zL-1]Sj + 1{ωι,j +ηω2,j≥0} [ω1 ◦ zL-1 + ω2 ◦ z-L-1]Sj
j=1
+η ω3 +ω4.
(27)
Now let us define Ui := ∏s(ωι ◦ zL-ι) , Ui := ∏s(ωι ◦ Zl-i), U2 := ∏s(ω2 ◦ zL-ι) , U2 :
∏s(ω2 ◦ ZL-ι),we get
nL-1
max	1
0≤η≤i j=i
{η≤ζj and ω2,sj >0} or {η≥ζj and ω2,sj <0}
o (UIj + ηu2,j)
+1n
{η≥ζj and ω2,sj >0} or {η≤ζj and ω2,sj <0}
o (ui,j + ηu2,j)
~
~
~
~
+ η ω3 + ω4.
(28)
In order to break the objective of maximization into piece-wise linear programming subproblems,
let us first identify the (indices of) ζsi values that fall in the feasible set 0 ≤ η ≤ 1 by
m = min ν and M = max ν
ζsν ≥0	ζsν ≤1
The overall maximization can now be reduced to piece-wise subproblems over sets ζν ≤ η ≤ ζν+i
for m - 1 ≤ ν ≤ M as
max
max{0,ζν}≤η≤min{1,ζν+1 }
nL-1
Xj=i	1n
r∕j∙	ɪ	、c\ r∖j∙	ɪ	-c∖
{η≤ζj	and	ω2,sj >0}	or {η≥ζj	and ω2,sj	<0}
o (UIj + ηu2,j)
~
~
~
~
+1n
o (ui,j + ηu2,j)
~
~
r∖j∙	ɪ	、c\	r∕j∙	ɪ	-c∖
{η≥ζj and ω2,sj >0} or {η≤ζj and ω2,sj <0}
+ η ω3 +ω4.
(29)
Since each of these subproblems are maximized at the boundaries of the feasible sets, the overall
maximization essentially reduces to evaluation of the following objective function at (M - m + 3)
~ ~
~ ~
PointS η = 0,ζm,ζm+1, ∙∙∙ ,ZM-1,ζM, 1
g(η) = X (1{ωι,j +ηω2,j≤0} (u1,j + ηu2,j) + 1{ωι,j +ηω2,3-≥0}(Ul,j + gj + η ω3 + ω4
Values of g(η) can be efficiently computed by a forward cumulative sum and forward-backward
cumulative sum of Ui and 吗，uι and u2, thus imposing the overall complexity which is dominated
by the sorting at O(nL-1 log(nL-1)) in an efficient implementation.
.
A.3 Description of Algorithm 1
Here is a step-by-step walk-through for Algorithm 1, with insight on how these steps are performed.
1.	Form vectors ωi and ω2 , which are the last layer values as ωi = WLcy,i, ω2
WL(ca,i - cy,i) , ω3 := bL>(ca,i - cy,i), and ω4 := bL>cy,i.
13
Published as a conference paper at ICLR 2021
2.	Define Z = [Zι,…，Z”] := -ω1∕ω2 and get the vector of indices S that sorts it, i.e.,
Zsi ≤ …≤ ZsnL-1
3.	Form the element-wise product of (ωι, ω2) with (zl-i,乞l-i)), and sort them according
to the index set s.
Ui = ∏s(ωι ◦ Zl-i) , u 1 = ∏s(ωι ◦乞l-i), U2 ：= ∏s(ω2 ◦ Zl-i) , u2 ：= ∏s(ω2 ◦ Zl-i).
4.	Get the lowest and highest indexes (m, M) such that the sorted Z vector value at those
indices are in the feasible set, between 0 and 1.
5.	Iterate over the feasible values of η = 0, Zsm, Zsm+i,…，Zsm-i ,Zsm , 1 and compute the
corresponding objective values
nL-1
g(n) = X (1{ωι,j +ηω2,j≤0} (UIj + ηu2,j) + 1{ωι,j +ηω2,j ≥0}(Ul,j + g,j))
j=1
+ η ω3 + ω4
6.	Return the maximum value of g(η) over the evaluated points.
A.4 Proof of Theorem 3
Let us start by splitting the feasible set into disjoint sets of
ZL-1 ：= {zL-1 | zL-1,a ≥ zL-1,y }, and ZL-1 ：= {zL-1 | zL-1,a < zL-1,y }
where
Z^L-1 = ZL-1 ∪ ZL-i, and ZL-ι ∩ ZcL-I =%
Proof is carried out by considering Z ∈ ZL-ι and Z ∈ ZL-i, separately.
Restricting Z ∈ ZL-ι We have 'xent∖a(fθ(X + δ), y) ≤ 'xe∏t∖y(fθ(X + δ), a) which leads to
Lao黑itn(x, y； θ) = Iimaxmin {'xent∖a(fθ(x + δ),y),'xent∖y (fθ(x + δ),a)}	(30)
≤ max	'xent∖a(zL,y) St ZL = W> Zl-1 + 比	(31)
ZL-1 ∈Zy-1
Loss function 'xent∖a is the cross entropy loss defined on the K-dimensional vector [zl,i,…，zl,k]
and class y, and thus following Wong & Kolter (2018) given its transnational invariance equals
max	'xent∖a(ZL,y) = max	'xent∖a(ZL - ZL,y 1,y) s.t. ZL = W>Zl-1 + bL (32)
zL-1∈Zy — 1	zL-i∈Zy — 1
with 1 denoting the (K + 1)-dimensional vector of all ones. Given the invariance of 'xent∖a with
respect to zL,a, it can finally be upperbounded by taking the upperbound for all i indices where
i = 1, ..., K, i 6= a, y and lowerbound at index i = y. Note that for i = y, value [ZL - zL,y 1]i = 0,
and a lower bound on other entries i = 1, ..., K, i 6= a, y can be obtained by
zL,i - zL,y = - max{zL,y - zL,i , zL,a - zL,i } = - max{cy,iZ, ca,iZ}
≤- ZmiZLm-,㈤≤ -Ji(X,y) ≤ -Jη'η(χ,y)
(33)
(34)
where the first equality holds since ZL-ι ：= {zl-i | zL-i,a < zL-ι,y} for Z ∈ ZL-ι, second
inequality is due to Theorem 2, and third inequality is given by Eq. 15.
Thus, for Z ∈ ZL-i the loss term is now upperbounded by
Labbuaitn(χ,y; θ) ≤ 'xent∖a(-%(χ,y),y)
where
[Je,θ (χ,y)]i= 0 η,ηif	i = a.y .
Ji (x, y)	otherwise.
(35)
14
Published as a conference paper at ICLR 2021
Network layers
ConV 64 3 X 3+1
Conv 64 3 × 3 + 1
ConV 128 3 × 3+ 2
ConV 128 3 × 3+ 1
ConV 128 3 × 3+ 1
Fully Conn. 512
# hidden	230K
# params.	17M
Table 2: Network architecture. Similar to the Large network used in (Gowal et al., 2018)
Similarly, it can be shown that for Thus, for Z ∈ ZL-ι the loss term is now upperbounded by
Laobuaitn(x,y; θ) ≤ 'xent∖y(-Je,θ(x,"),α)∙
The equality of 'χen∖y(-Je,θ(x, y), a) = 'xent∖a(-J∈,θ(x, y), y) trivially follows from the fact that
[J,θ (x, y)]i = 0 for i = a, y.
Thus, since ZL-I = ZL-ι ∪ ZL-1, the proof is complete.
.
B	Appendix: Experiment set up
Training parameters and schedules are similar to (Gowal et al., 2018) and (Zhang et al., 2020), and
outlined in detail here. For training the classifier network with architecture given in Table 2, for
both datasets, Adam optimizer with learning rate of 5 × 10-4 is used. Unless stated differently, κ
is scheduled by a linear ramp-down process, starting at 1, which after a warm-up perio,d is ramped
down to value κend = 0.5. Value of during the training is also simultaneously scheduled by a linear
ramp-up, starting at 0, and ramped up to the final value of train, reported in Tabel 1, and networks
are trained with a single NVIDIA Tesla V100S GPU.
•	For MNIST, the network is trained in 100 epochs with batchsize of 100 (total of 60K steps).
A warm up period of 3 epochs (2K steps) is used (normal classification training with no
robust loss), followed up by a ramp-up duration of 18 epochs (10K steps), and the learning
rate is decayed ×10 at epochs 25 and 42. No data augmentation is used. Furthermore,
fixed selection of η = 0.9 and η = 0.1 during training is used for this dataset with no
ramp-down. Reported numbers in Table 1 corresponds to λι = 1 and λ2 = 2 for e = 0.3,
and λ1 = 0.6 and λ2 = 1 for = 0.4 respectively.
•	For CIFAR10, the network is trained in 3200 epochs with batchsize of 1600 (total of 100K
steps). A warm up period of 320 epochs (10K steps) is used (normal classification training
with no robust loss), followed up by a ramp-up duration of 1600 epochs (50K steps), and
the learning rate is decayed ×10 at epochs 2600 and 3040 (60k and 90K steps). Random
translations and flips, and normalization of each image channel (using the channel statistics
from the train set) is used during training. Furthermore, during training for all e values we
have selected ηstart = 1.0 and ηe∏d = 0.9. Additionally,1口4 = 0.1 is used during training,
with ηstart = 0.1 for e = 2/255 (no ramp down), ηstart = 0.3 for e = 8/255, ηstart = 0.4
for e = 12/255, and ηstart = 0.5 for e = 16/255. The intuition behind these parameters
selection lies in Remark 2, as large η values promote the abstain option more, so for large
e, we start with larger 么tart as well. Reported numbers in Tabel 1 correspond to λι = 1
for all e values, and λ2 = 3.0 for e = 2/255, λ2 = 2.9 for e = 8/255, and λ2 = 3.1 for
e = 16/255 to insure similar natural accuracy for fair comparison against other methods.
15
Published as a conference paper at ICLR 2021
Figure 3: Naural versus robust error tradeoff for IBP (λ1 = 0) and IBP-with-detection (λ1 > 0) on
MNIST dataset for various perturbation sizes = 0.3 and = 0.4. Points closer to the origin are
better. IBP-with-detection is effectively utilizing its detection capability to adaptively trade natural
and robust performance, leading to improved certified robustness against adversarial perturbations.
IBP, Varying Kend
IBP, Varying λz
Λι=0.6, Varying Λz
Xl=O.8, Varying 即
Λι = 1.0r Varying λ?
(b) MNIST = 0.4
B.1	Pareto Frontier for MNIST dataset
B.2	Empirical attack success rate using PGD attacks
In order to obtain empirical attack success on the trained networks, adversarial perturbations are
sought by solving
max max zL,i - max{zL,y, zL,a}	(36)
This attack is indeed an adaptive attack as it aims at circumventing the detection while trying to cause
misclassification (Tramer et al., 2020). Perturbations are sought by maximizing this objective using
PGD with 200-steps for mnist and 500-steps for CIFAR-10 Madry et al. (2017), with 10 random
restarts. It is interesting to note that the achieved attack success rate in Table 1 is well below the
verified robust error, further implying the effectiveness of incorporation of the detection mechanism
as the true robustness of the system against practical adaptive PGD attacks are considerably stronger
in comparison to robust classification without detection.
16