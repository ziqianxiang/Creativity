Published as a conference paper at ICLR 2021
Uncertainty in Gradient Boosting
via Ensembles
Andrey Malinin *
Yandex; HSE University
Moscow, Russia
am969@yandex-team.ru
Liudmila Prokhorenkova*
Yandex; HSE University;
Moscow Institute of Physics and Technology
Moscow, Russia
ostroumova-la@yandex-team.ru
Aleksei Ustimenko*
Yandex
Moscow, Russia
austimenko@yandex-team.ru
Ab stract
For many practical, high-risk applications, it is essential to quantify uncertainty
in a model’s predictions to avoid costly mistakes. While predictive uncertainty is
widely studied for neural networks, the topic seems to be under-explored for mod-
els based on gradient boosting. However, gradient boosting often achieves state-
of-the-art results on tabular data. This work examines a probabilistic ensemble-
based framework for deriving uncertainty estimates in the predictions of gradient
boosting classification and regression models. We conducted experiments on a
range of synthetic and real datasets and investigated the applicability of ensemble
approaches to gradient boosting models that are themselves ensembles of decision
trees. Our analysis shows that ensembles of gradient boosting models successfully
detect anomalous inputs while having limited ability to improve the predicted total
uncertainty. Importantly, we also propose a concept of a virtual ensemble to get
the benefits of an ensemble via only one gradient boosting model, which signifi-
cantly reduces complexity.
1	Introduction
Gradient boosting (Friedman, 2001) is a widely used machine learning algorithm that achieves state-
of-the-art results on tasks containing heterogeneous features, complex dependencies, and noisy data:
web search, recommendation systems, weather forecasting, and many others (Burges, 2010; Caru-
ana & Niculescu-Mizil, 2006; Richardson et al., 2007; Roe et al., 2005; Wu et al., 2010; Zhang
& Haghani, 2015). Gradient boosting based on decision trees (GBDT) underlies such well-known
libraries like XGBoost, LightGBM, and CatBoost. In this paper, we investigate the estimation of pre-
dictive uncertainty in GBDT models. Uncertainty estimation is crucial for avoiding costly mistakes
in high-risk applications, such as autonomous driving, medical diagnostics, and financial forecasting.
For example, in self-driving cars, it is necessary to know when the AI-pilot is confident in its ability
to drive and when it is not to avoid a fatal collision. In financial forecasting and medical diagnostics,
mistakes on the part of an AI forecasting or diagnostic system could either lead to large financial
or reputational loss or to the loss of life. Crucially, both financial and medical data are often repre-
sented in heterogeneous tabular form — data on which GBDTs are typically applied, highlighting
the relevance of our work on obtaining uncertainty estimates for GBDT models.
Approximate Bayesian approaches for uncertainty estimation have been extensively studied for neu-
ral network models (Gal, 2016; Malinin, 2019). Bayesian methods for tree-based models (Chipman
et al., 2010; Linero, 2017) have also been widely studied in the literature. However, this research did
not explicitly focus on studying uncertainty estimation and its applications. Some related work was
* All authors contributed equally and are listed in alphabetical order.
1
Published as a conference paper at ICLR 2021
done by Coulston et al. (2016); Shaker & Hullermeier (2020), Who examined quantifying Predic-
tive uncertainty for random forests. However, the area has been otherwise relatively under-explored,
esPecially for GBDT models that are Widely used in Practice and knoWn to outPerform other aP-
Proaches based on tree ensembles.
While for classification Problems GDBT models already return a distribution over class labels, for
regression tasks they tyPically yield only Point Predictions. Recently, this Problem Was addressed
in the NGBoost algorithm (Duan et al., 2020), Where a GBDT model is trained to return the mean
and variance of a normal distribution over the target variable y for a given feature vector. HoWever,
such models only caPture data uncertainty (Gal, 2016; Malinin, 2019), also knoWn as aleatoric un-
certainty, Which arises due to inherent class overlaP or noise in the data. HoWever, this does not
quantify uncertainty due to the model’s inherent lack of knoWledge about inPuts from regions either
far from the training data or sParsely covered by it, knoWn as knowledge uncertainty, or epistemic un-
certainty (Gal, 2016; Malinin, 2019). One class of aPProaches for caPturing knowledge uncertainty
are Bayesian ensemble methods, Which have recently become PoPular for estimating Predictive un-
certainty in neural netWorks (DePeWeg et al., 2017; Gal & Ghahramani, 2016; Kendall et al., 2018;
Lakshminarayanan et al., 2017; Maddox et al., 2019; Smith & Gal, 2018). A key feature of ensem-
ble aPProaches is that they alloW overall uncertainty to be decomPosed into data uncertainty and
knowledge uncertainty Within an interPretable Probabilistic frameWork (DePeWeg et al., 2017; Gal,
2016; Malinin, 2019). Ensembles are also knoWn to yield imProvements in Predictive Performance.
This Work examines ensemble-based uncertainty-estimation for GBDT models. The contributions
are as folloWs. First, We consider generating ensembles using both classical Stochastic Gra-
dient Boosting (SGB) as Well as the recently ProPosed Stochastic Gradient Langevin Boosting
(SGLB) (Ustimenko & Prokhorenkova, 2020). ImPortantly, SGLB alloWs us to guarantee that the
models are asymPtotically samPled from a true Bayesian Posterior. Second, We shoW that using
SGLB We can construct a virtual ensemble using only one gradient boosting model, significantly re-
ducing the comPutational comPlexity. Third, to understand the attributes of using ensembles-based
uncertainty estimation in GBDT models, We conduct extensive analysis on several synthetic datasets.
Finally, We evaluate the ProPosed aPProach on a range of real regression and classification datasets.
Our results shoW that this aPProach successfully enables the detection of anomalous out-of-domain
inPuts. ImPortantly, our solution is easy to combine With any imPlementation of GBDT. Our meth-
ods have been imPlemented Within the oPen-source CatBoost library. The code of our exPeriments is
Publicly available at https://github.com/yandex- research/GBDT- uncertainty.
2	Preliminaries
Uncertainty Estimation via Bayesian Ensembles In this Work We consider uncertainty estima-
tion Within the standard Bayesian ensemble-based frameWork (Gal, 2016; Malinin, 2019). Here,
model Parameters θ are considered random variables and a Prior p(θ) is Placed over them to com-
pute a posterior p(θ∣D) via Bayes' rule:
p( θD ) = X
(1)
Where D = {x(i) , y(i) }iN=1
is the training dataset. Each set of parameters can be considered a
hypothesis or explanation about hoW the World Works. Samples from the posterior should yield
explanations consistent With the observations of the World contained Within the training data D.
HoWever, on data far from D each set of parameters can yield different predictions. Therefore,
estimates of knowledge uncertainty can be obtained by examining the diversity of predictions.
Consider an ensemble of probabilistic models {P(y|x; θ(m))}mM=1 sampled from the posterior
p(θ∣D). Each model P(y∣x, θ(m)) yields a different estimate of data uncertainty, represented by
the entropy of its predictive distribution (Malinin, 2019). Uncertainty in predictions due to knowl-
edge uncertainty is expressed as the level of spread, or “disagreement”, of models in the ensem-
ble (Malinin, 2019). Note that exact Bayesian inference is often intractable, and it is common to
consider either an explicit or implicit approximation q(θ) to the true posterior p(θ∣D). While a
range of approximations has been explored for neural netWork models (Gal & Ghahramani, 2016;
Lakshminarayanan et al., 2017; Maddox et al., 2019)1, to the best of our knoWledge, limited Work
1A full overview is available in (Ashukha et al., 2020; Ovadia et al., 2019).
2
Published as a conference paper at ICLR 2021
has explored Bayesian inference for gradient-boosted trees. Given p(θ∣D), the predictive posterior
of the ensemble is obtained by taking the expectation with respect to the models in the ensemble:
1M
P(y∣x, D)= Ep(e⑼[P(y∣x; θ)] ≈ MM E P(y∣x; θ(m)), θ(mm 〜p(θ∣D).	⑵
m=1
The entropy of the predictive posterior estimates total uncertainty in predictions:
HP(y|x, D) = EP(y|x,D) - ln P(y|x, D) .	(3)
Total uncertainty is due to both data uncertainty and knowledge uncertainty. However, in applica-
tions like active learning (Kirsch et al., 2019) and out-of-domain detection it is desirable to estimate
knowledge uncertainty separately. The sources of uncertainty can be decomposed by considering
the mutual information between the parameters θ and the prediction y (Depeweg et al., 2017):
Iy,θ θ∣χ, D = H[P(y|x, D)] - Ep(00) H[P(y∣χ; θ)]]
|-----{z----}	|----{z-----} |-----------{---------}
Knowledge Uncertainty Total Uncertainty Expected Data Uncertainty
1 M	1 M	(4)
≈H岛 EP(y∣χ;θ(m))] - IM E H[P(y|x;θ(m))] .
m=1	m=1
This is expressed as the difference between the entropy of the predictive posterior, a measure of
total uncertainty, and the expected entropy of each model in the ensemble, a measure of expected
data uncertainty. Their difference is a measure of ensemble diversity and estimates knowledge
uncertainty.
Unfortunately, when considering ensembles of probabilistic regression models {p(y|x; θ(m))}mM=1
over continuous-valued target y ∈ R, it is no longer possible to obtain tractable estimates of the
(differential) entropy of the predictive posterior, and, by extension, mutual information. In this cases
uncertainty estimates can instead derived via the law of total variation:
Vp(y|x,D) [y]
|------{------}
Total Uncertainty
Vp(θ∣D) [Ep(y∖x,θ)[y]] + Ep(θ∣D) [Vp(y∖x,θ)[y]]
|-----------{------------} |------------{---------------}
Knowledge Uncertainty	Expected Data Uncertainty
(5)
This is conceptually similar to the decomposition (4) obtained via mutual information. For an en-
semble of probabilistic regression models which parameterize the normal distribution, and where
each models yields a mean and standard-deviation, the total variance can be computed as follows:
MM	2
TM 叱 0)[ n X m X [(X Mm)- μm +
T + 1 二 {z =. + -	m =1	m = 1
Total Uncertainty ∣	{z	，
Knowledge Uncertainty
{μm,σm} = f (x; θ(m)).
(6)
1M
-1 X σ 2
M乙m
m=1
Expected Data Uncertainty
However, while these measures are tractable, they are based on only first and second moments, and
may therefore miss high-order details in the uncertainty. They are also not scale-invariant, which
can cause issues is the scale of prediction on in-domain and out-of-domain data is very different.
Gradient boosting is a powerful machine learning technique especially useful on tasks containing
heterogeneous features. It iteratively combines weak models, such as decision trees, to obtain more
accurate predictions. Formally, given a dataset D and a loss function L : R2 → R, the gradient
boosting algorithm (Friedman, 2001) iteratively constructs a model F : X → R to minimize the
empirical risk L(F |D) = ED [L(F (x), y)]. At each iteration t the model is updated as:
F(t)(x) =F(t-1)(x)+h(t)(x),
(7)
where F (t-1) is a model constructed at the previous iteration, h(t)(x) ∈ H is a weak learner chosen
from some family of functionds H, and is learning rate. The weak learner h(t) is usually chosen to
approximate the negative gradient -g(t) (x, y) :
∂L(y,s) I	:
∂s	I s=F (t-1)( x),
h(t) = argminED( - g(t)(x, y) - h(x)2.	(8)
h∈H
3
Published as a conference paper at ICLR 2021
A weak learner h(t) is associated with parameters φ(t) ∈ Rd. We write h(t) (x, φ(t) ) to reflect this
dependence. The set of weak learners H often consists of shallow decision trees, which are models
that recursively partition the feature space into disjoint regions called leaves. Each leaf Rj of the
tree is assigned to a value, which is the estimated response y in the corresponding region. We can
write h(x, φ(t)) = Pjd=1 φ(jt) 1{x∈Rj}, so the decision tree is a linear function of φ(t). The final
GBDT model F is a sum of decision trees (7) and the parameters of the full model are denoted by θ.
For classification tasks, a model yields estimates data uncertainty if it is trained via negative log-
likelihood and provides a distribution over class labels. However, classic GBDT regression models
yield point predictions, and there has been little research devoted to estimating predictive uncertainty.
Recently, this issue was addressed by Duan et al. (2020) via an algorithm called NGBoost (Natural
Gradient Boosting), which allows estimating data uncertainty. NGBoost simultaneously estimates
the parameters of a conditional distribution p(y|x, θ) over the target y given the features x, by
optimizing a proper scoring rule. Typically, a normal distribution over y is assumed and negative
log-likelihood is taken as a scoring rule. Formally, given an input x, the model F predicts two
parameters of normal distribution - the mean μ and the logarithm of the standard deviation log σ.
The loss function is the expected negative log-likelihood:2
p(y∣x, θ(t))= N(y∣μ(t),σ(t)), {μ(11, logσ(11} = F(t)(x).	(9)
1N
L (θ∣D) = E D [ - log p( y∣χ, θ)] = - N EIog p( y (i) x (i), θ) .	(10)
i=1
Note that θ denotes the concatenation of two parameter vectors used to predict μ and log σ.
3 Generating ensembles of GDBT models
As discussed in Section 2, knowledge uncertainty can be estimated by considering an ensemble of
models { p( y∣x; θ (m)) }M=ι sampled from the posterior p( θ∣D). The level of diversity or “disagree-
ment” between the models is an estimate of knowledge uncertainty. In this section, we consider three
approaches to generating an ensemble of GBDT models. We emphasize that this section discusses
ensembles of GBDT models, where a each GBDT model is itself an ensemble of trees.
SGB ensembles One way to generate an ensemble is to consider several independent models gener-
ated via Stochastic Gradient Boosting (SGB). Stochasticity is added to GBDT models via random
subsampling of the data at every iteration (Friedman, 2002). Specifically, at each iteration of (8)
we select a subset of training objects D0 (via bootstrap or uniformly without replacement), which is
smaller than the original training dataset D, and use D0 to fit the next tree instead of D. The frac-
tion of chosen objects is called sample rate. This implicitly injects noise into the learning process,
effectively inducing a distribution q(θ) over such models. Thus, an SGB ensemble is an ensemble
of independent models {θ(m)}mM=1 built according to SGB with different random seeds for sub-
sampling data. Unfortunately, there are no guarantees on how well the distribution q(θ) estimates
the true posterior p(θ∣D).
SGLB ensembles Remarkably, there is a way to sample GBDT models from the true posterior
p(θ∣D) via a recently proposed Stochastic Gradient Langevin Boosting (SGLB) algorithm (US-
timenko & Prokhorenkova, 2020). SGLB combines gradient boosting with stochastic gradient
Langevin dynamics (Raginsky et al., 2017) in order to achieve convergence to the global optimum
even for non-convex loss functions. The algorithm has two differences compared with SGB. First,
Gaussian noise is explicitly injected into the gradients, so (8) is replaced by:
h(t) = arg minEd [(-g(t)(x,y) - h(x, φ) + V) ,ν 〜N (。标加),	(11)
where β is the inverse diffusion temperature and I|D| is an identity matrix. This random noise ν
helps to explore the solution space in order to find the global optimum and the diffusion temperature
controls the level of exploration. Second, the update (7) is modified as:
F(t) (x) = (1 -γ)F(t-1)(x) +h(t)(x,φ(t)),	(12)
2Since GBDT model is determined by θ, We use notation L(FD) and L(θ∣D) interchangeably.
4
Published as a conference paper at ICLR 2021
二…"
First model
Second model
Last model
Figure 1:	Virtual ensemble
where γ is regularization parameter. If the number of all possible trees is finite (a natural assumption
given that the training dataset is finite), then the SGLB parameters θ(t) at each iteration form a
Markov chain that weakly converges to the stationary distribution, also called the invariant measure:
pβ(θ) U exp(-βL(θ∣D) - βγkΓθ∣∣2),	(13)
where Γ = ΓT > 0 is an implicitly defined regularization matrix which depends on a particular tree
construction algorithm (Ustimenko & Prokhorenkova, 2020).
While Ustimenko & Prokhorenkova (2020) used the weak convergence to (13) to prove the global
convergence, we apply this to enable sampling from the true posterior. For this purpose, we set
β = ∣D∣ and Y = 2D. For the negative log-likelihood loss function (10) the invariant measure (13)
can be expressed as:
pβ(θ) Y exp (logp(D∖θ) - 2kΓθk2)Y p(D∣θ)p(θ),	(14)
which is proportional to the true posterior distribution p(θ∖D) under Gaussian prior p(θ) = N(0, Γ).
Thus, an SGLB ensemble is an ensemble of independent models {θ(m)}mM=1 generated according to
the SGLB algorithm using different random seeds. In this case, asymptotically, models are sampled
from the true posterior p(θ∖D).
Virtual SGLB ensembles While SGB and SGLB yield ensembles of independent models, their
time and space complexity is M times larger than that of a single model, which is a significant
overhead. Consequently, generating an ensemble requires either significantly increasing complexity
or sacrificing the quality by reducing the number of training iterations. To address this, we introduce
the concept of a virtual ensemble that enables generating an ensemble using only one model. This is
possible since a GBDT model is itself an ensemble of trees. However, in contrast to random forests
formed by independent trees (Shaker & Hullermeier, 2020), the sequential nature of GBDT models
implies that all trees are dependent and individual trees cannot be considered as separate models.
Hence, we use “truncated” sub-models of a single GBDT model as elements of an ensemble, as
illustrated in Figure 1. Notably, a virtual ensemble can be obtained using any already constructed
GBDT model. Below we formally describe this procedure applied to SGLB models since in this
case we can guarantee asymptotically sampling from the true posterior p(θ∖D).
Each “truncated” model is described by the vector of parameters θ(t). As the parameters θ(t) at each
iteration of the SGLB algorithm form a Markov chain that weakly convergences to the stationary
distribution (14), we can consider using them as an ensemble of models. However, unlike parameters
taken from different SGLB trajectories, these will have a high degree of correlation, which adversely
affects the ensemble’s quality. This problem can be overcome by retaining only every K-th set of
parameters. Formally, fix K ≥ 1 and consider a set of models ΘTK = {θ(Kt), [2K] ≤ t ≤ [-T]},
i.e., we add to ΘT,K every K-th model obtained while constructing one SGLB model using T
iterations of gradient boosting. Choosing larger values of K allows us to reduce the correlation
between samples from the SGLB Markov chain. Furthermore, we do not include to the ensemble
the models θ(t) with t <T/ 2 as (14) holds only asymptotically. The set of M = [2K] models
ΘT,K is called a virtual ensemble. Note that virtual ensembles behave similarly to true ensembles
in the limit (for large K and T).
Importantly, we can compute the prediction of ΘT,K with the same computation time as one θ(T).
Indeed, when computing the prediction of one model, we have to sum up the predictions made by
individual trees. To get the virtual ensemble, we only have to store the partial sums. For SGLB,
we also have to account for regularization (12). Formally, according to (12), for SGLB we have
5
Published as a conference paper at ICLR 2021
(a) True Data Uncertainty
-0.035
-0.030
-0.025
-0.020
0.015
'-0.010
01234567«
-0.005
(b) Estimated Data Uncertainty (c) Knowledge Uncertainty
Figure 2:	Uncertainty for synthetic regression dataset with two categorical features. Inside the heart
(white region on the first figure) there are no training examples.
θ(T) = PiT=1 (1 - γ)T -iφ(i), where (1 - γ)T-i appears due to shrinkage. While computing
θ(T) we store the partial sums θ≤(Tt) = Pit=1 (1 - γ )T -i φ(i) . Then, any model θ(t) from ΘT,K
can easily be obtained from the stored values:
t
θ(t) = X (1 - γ)t-iφ(i) = (1 - γ)t-T θ≤(Tt).	(15)
i=1
4	Analysis on Synthetic Data
In this section, we analyze how ensemble algorithms discussed in Section 3 perform on synthetic
data. The aim is to understand the attributes of ensembles of GBDT models for estimating data and
knowledge uncertainty in a controllable setting.
GBDT models are usually applied to tabular data, where features are often categorical. Hence,
we first generate a dataset with each example described by two categorical features x1, x2 with
9 values each, resulting in 81 possible combinations. The target depends on the features as
y = a(xι,x2) + ε(xι,x2), where ε(x 1 ,x2)〜 N(0, b(x 1, x2)) and a(x 1 ,x2), b(x 1 ,x2) are some
deterministic functions. The values for a(x1, x2) are randomly generated according to the uniform
distribution over [0, 1]. The values for b(x1, x2) are shown on Figure 2(a). We generate a heart-
shaped dataset with this distribution: inside the heart (white region on Figure 2(a)) there are no
training points, for the other cells we have 1000 examples per cell.
We train an ensemble of 10 SGLB models (each model consists of 1000 trees) and observe the
following effects. First, Figure 2(b) shows the total uncertainty estimated with SGLB ensemble and
we see that the models correctly capture this uncertainty in all cells containing training examples.
At the same time, arbitrary values can be predicted inside the heart, as no training data constrain the
models’ behavior there. Second, Figure 2(c) shows that estimates of knowledge uncertainty allow
us to detect regions that are out-of-domain and are not covered by the training data. Notably, the
separation is perfect, as there is no trace of the original heart border.
To further analyze ensembles of GBDT models, we apply them to a two-dimensional classification
task with continuous-valued features. We consider a 3-class spiral dataset shown on Figure 3(a);
and this setting is much harder for gradient boosted trees.3 Figure 3(b) shows the total uncertainty
estimated with SGLB ensemble, while Figure 3(c) demonstrates knowledge uncertainty. We observe
several effects. First, total uncertainty correctly detects class boundaries and ‘sectors’ of input space
outside the training dataset. Second, looking at these ‘sectors’ of high uncertainty, we can better
understand how GBDT ensembles work: as decision trees are discriminative functions (Bishop,
2006), if features have values outside the training domain, then the prediction is the same as for
the “closest” elements in the dataset. In other words, the models’ behavior on the boundary of
the dataset is further extended to the outer regions. Third, estimates of knowledge uncertainty allow
discrimination between out-of-domain regions and class boundaries. However, we still can see traces
of the class boundaries in Figure 3(c). A possible reason is the fact that for real-valued features, near
the class borders, the splitting values may vary across all models in the ensemble, resulting in non-
zero estimates of knowledge uncertainty due to decision-boundary ‘jitter’.
3To partially mitigate the difficulties, we use coordinates in rotated axes and radius as additional features.
6
Published as a conference paper at ICLR 2021
(a) Spiral Dataset
Figure 3: Uncertainty for synthetic classification dataset
■ 0.035
■ 0.030
■ 0.025
■ 0.020
■ 0.015
-0.010
-0.005
(a) Heart SGLB KU
■ 0.00035
■ 0.00030
■ 0.00025
■ 0.00020
0.00015
-0.00010
-0.00005
-012345678
(b) Heart VSGLB KU
(c) Spiral SGLB KU
(d) Spiral vSGLB KU
Figure 4: Comparison of SGLB and vSGLB knowledge uncertainty estimates
On both “heart” and “spiral” datasets, we observed that the absolute values of knowledge uncertainty
are much smaller than of data uncertainty and therefore contribute very little to total uncertainty.
Thus, we expect that while knowledge uncertainty is especially useful for detecting anomalous in-
puts, the proposed approaches will contribute little to error detection on top of estimates of data
uncertainty provided by single models.
Finally, on Figure 4, we compare the performance of ‘true’ SGLB ensembles with the virtual SGLB
ensembles (vSGBL) on both the “heart” and “spiral” datasets. The virtual ensemble is ten times
cheaper to train and infer, but the ensemble members are strongly correlated. We observe that on
the “heart” dataset, vSGLB perfectly detects regions not covered by training data. However, the
absolute values of knowledge uncertainty are much smaller than for SGLB, which can be explained
by the correlations. The “spiral” dataset is more challenging for both SGLB and vSGLB. While
having qualitatively similar behavior, virtual ensembles struggle to detect out-of-domain regions
and separate them from class boundaries. In all cases, the absolute values of knowledge uncertainty
are far lower than for ‘true’ SGLB ensembles. This shows that while vSGLB yields very cheap
estimates of knowledge uncertainty by exploiting the ‘ensemble of trees’ structure of GBDT models,
the quality of these estimates is inferior to ensembles of independent models.
5	Experiments on classification and regression datasets
In this section, we evaluate the performance of ensembles of GBDT models on a range of classifica-
tion and regression tasks, focusing on their ability to detect errors and out-of-domain inputs.
Experimental setup Our implementation of all GBDT models is based on the CatBoost library
that is known to achieve state-of-the-art results in a variety of tasks (Prokhorenkova et al., 2018).
Classification models yield a probability distribution over binary class labels, while regression mod-
els yield the mean and variance of the normal distribution, as discussed in Section 2. All models
are trained by optimizing the negative log-likelihood.4 We consider SGB and SGBL single models
as the baselines and examine all ensemble methods defined in Section 3. Ensembles of SGB and
SGLB models consist of 10 independent (with different seeds) models with 1000 trees each. The
virtual ensemble vSGLB is obtained from one model with 1000 trees, where each 50th model from
the interval [501, 1000] is added to the ensemble. Thus, vSGLB has the same computational and
4In Appendix A.1, we compare our implementation with the original NGBoost and Deep Ensembles in
terms of NLL (negative log-likelihood) and RMSE. Our implementation has comparable performance to the
existing methods.
7
Published as a conference paper at ICLR 2021
space complexity as just one SGB or SGLB model. Hyper-parameters are tuned by grid search, for
details see Appendix A.2.
We compare the algorithms on several classification and regression tasks (Gal & Ghahramani, 2016;
Prokhorenkova et al., 2018), the description of which is available in Appendix A.3.
While not being the focus of the current research, Random Forest (RF) models are naturally suitable
for ensemble approaches. Hence, we conduct additional experiments and analyze the performance
of ensemble approaches applied to RF models in Appendix C.
Detection of errors and anomalous inputs We analyze whether measures of total and knowledge
uncertainty can be used to detect errors and out-of-domain inputs. Error detection can be evaluated
via the Prediction-Rejection Ratio (PRR) (Malinin, 2019; Malinin et al., 2020), which measures
how well uncertainty estimates correlate with errors and rank-order them. The best value is 100,
random is 0. Out-of-domain (OOD) detection is assessed via area under the ROC curve (AUC-
ROC) (Hendrycks & Gimpel, 2016). For OOD detection, we need an OOD test-set. However,
obtaining ‘real’ OOD examples for the datasets considered in this work is challenging, so we instead
create synthetic OOD data as follows. For each dataset, we take its test set as the in-domain examples
and sample an OOD dataset of the same size from the Year MSD dataset to get out-of-domain (OOD)
data. The only exceptions are KDD datasets (Appetency, Churn, Upselling) and Year MSD, for
which we sample OOD data from the Relative location ofCT slices on axial axis Data Set (Graf et al.,
2011). All numerical features in OOD data are normalized by the per-column mean and variance
obtained on the in-domain training data. For categorical features, we sample a random category
uniformly at random from the set of all feature’s categories. Total and knowledge uncertainty are
estimated via entropy of the predictive posterior (3) and mutual information (4) for classification
models and via total variance and variance of the mean (5) for regression ones.
Test errors can occur due to both noise and lack of knowledge, so we expect that ranking elements by
total uncertainty would give better values of PRR. Table 1 shows that measures of total uncertainty
consistently yield better PPR results across all datasets. This is consistent with results obtained
for ensembles of neural network models (Lakshminarayanan et al., 2017; Malinin, 2019; Malinin
& Gales, 2019; Malinin et al., 2020). However, ensembles do not outperform single models. We
believe this occurs for two reasons. First, due to the additive nature of boosting, GDBT models
are already ensembles. Second, as we have discussed in Section 4, for GBDT models, estimates of
knowledge uncertainty obtained via the approaches considered here contribute little to estimates of
total uncertainty.
In contrast, Table 1 shows that measures of knowledge uncertainty yield superior OOD detection
performance compared to total uncertainty in terms of AUC-ROC, which is consistent with results
for non-GBDT models (Malinin, 2019; Malinin & Gales, 2019; Malinin et al., 2020).5 The results
also show that SGB and SGLB ensembles performed almost equally well. At the same time, virtual
ensembling (vSGLB) performed consistently worse (with one exception) than SGB/SGLB ensem-
bles, which is explained by the presence of strong correlations between the models in a virtual
ensemble. However, in classification tasks, estimates of knowledge uncertainty provided by vSGLB
nevertheless outperform uncertainty estimates derived from single SGB and SGLB models. This
shows that useful measures of knowledge uncertainty can be derived from a single SGLB model by
interpreting it as a virtual ensemble at no additional computational or memory cost. For vSGLB, the
difference between classification and regression tasks can be explained by the presence or absence
of categorical features. In our preliminary experiments on synthetic data, we noticed that categori-
cal features may have a noticeable effect on the diversity of vSGLB models, and our classification
datasets contain categorical features.
6	Conclusion
This work examined principled, ensemble-based uncertainty-estimation for GBDT models. Two
main approaches to generating ensembles of GDBT models, where each model is itself an ensemble
of trees, were considered — Stochastic Gradient Boosting (SGB) and Stochastic Gradient Langevin
Boosting (SGLB). Based on SGLB, we propose constructing a virtual ensemble (vSGLB) by ex-
5 Note that single models do not allow distinguishing between the types of uncertainty.
8
Published as a conference paper at ICLR 2021
Table 1: Detection of errors and OOD examples for regression and classification tasks
Dataset	∣		Single		SGB	Ensemble SGLB	vSGLB	Single		SGB	Ensemble SGLB	vSGLB
		SGB	SGLB				SGB	SGLB			
		ZZ	Classification		% PRR (T)		Classification % AUC-ROC (T)				
	 Adult		 TU	72	72	72	72		 72	53	50	52	51	51
	KU	—	—	49	49	38	—	—	89	89	85
Amazon	TU	71	69	70	68	68	86	87	86	86	86
	KU	—	—	64	61	40	—	—	88	74	67
Click	TU	43	44	43	44	44	61	67	64	64	68
	KU	—	—	22	22	11	—	—	91	92	90
Internet	TU	76	79	77	79	79	67	68	70	69	68
	KU	—	—	69	72	61	—	—	87	89	81
KDD-Appetency	TU	68	69	69	69	69	29	48	47	50	52
	KU			64	54	14			90	91	93
KDD-Churn	TU	47	45	48	46	46	81	57	82	75	60
	KU	—	—	33	35	28	—	—	99	98	92
KDD-Upselling	TU	56	56	57	57	56	53	51	62	60	47
	KU			45	49	33			97	97	78
Kick	TU	44	45	44	44	45	45	37	52	58	38
	KU	—	—	34	34	20	—	—	98	98	89
Dataset		Regression % PRR (T)					Regression % AUC-ROC (T)				
BostonH	TU	45	45	44	45	46	70	68	71	69	64
	KU	—	—	36	37	38	—	—	80	80	49
Concrete	TU	45	41	44	42	41	78	80	79	81	78
	KU	—	—	27	27	25	—	—	92	92	56
Energy	TU	58	56	58	56	62	67	69	89	89	69
	KU	—	—	36	31	54	—	—	100	100	32
Kin8nm	TU	59	59	59	59	58	43	43	43	43	42
	KU	—	—	18	19	35	—	—	45	45	45
Naval-p	TU	75	76	82	82	81	99	99	100	100	99
	KU	—	—	52	56	69	—	—	100	100	87
Power-p	TU	30	32	31	33	32	48	47	51	49	47
	KU			8	9	13			72	73	57
Protein	TU	49	48	52	50	48	82	84	92	91	86
	KU	—	—	30	29	12	—	—	99	99	94
Wine-qu	TU	33	32	33	32	32	60	56	60	56	56
	KU			25	19	9			74	72	49
Yacht	TU	89	88	88	88	88	57	57	58	58	55
	KU	—	—	74	78	66	—	—	62	60	40
Year	TU	61	62	62	63	62	59	58	60	60	57
	KU	—	—	30	30	25	—	—	67	57	52
ploiting the ‘ensemble-of-trees’ nature of GBDT models. Properties of the estimates of total, data,
and knowledge uncertainty derived from these ensembles were first analyzed on synthetic data. It
was shown that the proposed approach can successfully detect anomalous inputs and is especially
successful on tabular data. On continuous data, detecting knowledge uncertainty is still possible, but
it becomes harder to differentiate it with data uncertainty due to decision-boundary ‘jitter’. Further
experiments on a wide range of classification and regression datasets showed that while ensembles
of GDBT models do not offer much advantage in terms of error detection, as each model is already
an ensemble of trees, they do yield useful measures of knowledge uncertainty, which enables out-
of-domain detection in both regression and classification tasks. Notably, measures of knowledge un-
certainty, which can only be obtained via ensembles, achieve far better OOD detection performance
than measures of total uncertainty. It is also shown that while there is little practical difference be-
tween SGB and SGLB ensembles, vSGLB performs noticeably worse. However, for classification
tasks containing categorical features, vSGLB still yields useful measures of knowledge uncertainty
at the computational time and space complexity of a single SGLB model. Thus, vSGLB allows us
to derive the benefits of an ensemble at no additional computational and memory cost.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We would like to thank Ekaterina Ermishkina and Stanislav Kirillov for implementing the proposed
methods within the CatBoost library.
References
UCI datasets. https://github.com/yaringal/DropoutUncertaintyExps/tree/
master/UCI_Datasets.
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain
uncertainty estimation and ensembling in deep learning. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=BJxI5gHKDr.
Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song
dataset. In Proceedings of the 12th International Society for Music Information Retrieval Confer-
ence (ISMIR 2011), 2011.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Christopher JC Burges. From RankNet to LambdaRank to LambdaMART: An overview. Learning,
11(23-581):81, 2010.
Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning
algorithms. In Proceedings of the 23rd international conference on Machine learning, pp. 161-
168. ACM, 2006.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive regression
trees. The Annals of Applied Statistics, 4(1):266-298, 2010.
John W Coulston, Christine E Blinn, Valerie A Thomas, and Randolph H Wynne. Approximating
prediction uncertainty for random forest regression models. Photogrammetric Engineering &
Remote Sensing, 82(3):189-197, 2016.
Stefan Depeweg, Jose MigUeI Herndndez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty for active learning and reliable reinforcement learning in stochastic sys-
tems. stat, 1050:11, 2017.
Tony Duan, Anand Avati, Daisy Yi Ding, Sanjay Basu, Andrew Y Ng, and Alejandro Schuler. Ng-
boost: Natural gradient boosting for probabilistic prediction. Proc. 37th International Conference
on Machine Learning (ICML), 2020.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of
statistics, pp. 1189-1232, 2001.
Jerome H Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):
367-378, 2002.
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning. In Proc. 33rd International Conference on Machine Learning
(ICML-16), 2016.
Franz Graf, Hans-Peter Kriegel, Matthias Schubert, Sebastian Polsterl, and Alexander Cavallaro. 2d
image registration in ct images using radial image descriptors. In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pp. 607-614. Springer, 2011.
Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-
Distribution Examples in Neural Networks. http://arxiv.org/abs/1610.02136, 2016.
arXiv:1610.02136.
Kaggle. "Don’t Get Kicked!". https://www.kaggle.com/c/DontGetKicked, 2011.
10
Published as a conference paper at ICLR 2021
Kaggle. Amazon dataset. https://www.kaggle.com/bittlingmayer/
amazonreviews, 2017.
KDD. Kdd cup 2009: Customer relationship prediction. https://www.kdd.org/kdd-cup/
view/kdd-cup-2009/Data, 2009.
KDD. Kdd cup 2012 (track 2): Predict the click-through rate of ads given the query and user infor-
mation. https://www.kdd.org/kdd-cup/view/kdd-cup- 2012-track-2, 2012.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 7482-7491, 2018.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. BatchBALD: Efficient and diverse batch
acquisition for deep bayesian active learning. In Advances in Neural Information Processing
Systems 32 (NeurIPS 2019), 2019.
Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Proceed-
ings of the Second International Conference on Knowledge Discovery and Data Mining, 1996.
B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and Scalable Predictive Uncertainty Es-
timation using Deep Ensembles. In Proc. Conference on Neural Information Processing Systems
(NIPS), 2017.
Antonio R Linero. A review of tree-based bayesian methods. Communications for Statistical Appli-
cations and Methods, 24(6), 2017.
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning. arXiv preprint arXiv:1902.02476, 2019.
Andrey Malinin. Uncertainty Estimation in Deep Learning with application to Spoken Language
Assessment. PhD thesis, University of Cambridge, 2019.
Andrey Malinin and Mark JF Gales. Reverse kl-divergence training of prior networks: Improved
uncertainty and adversarial robustness. 2019.
Andrey Malinin, Bruno Mlodozeniec, and Mark JF Gales. Ensemble distribution distillation. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BygSP6Vtvr.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? Evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems,
2019.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning
Research, 12:2825-2830, 2011.
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey
Gulin. Catboost: unbiased boosting with categorical features. In Proceedings of the 32nd Interna-
tional Conference on Neural Information Processing Systems (NeurIPS), pp. 6638-6648, 2018.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. CoRR, abs/1702.03849, 2017.
Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-
through rate for new ads. In Proceedings of the 16th international conference on World Wide Web,
pp. 521-530. ACM, 2007.
Byron P Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. Boosted decision
trees as an alternative to artificial neural networks for particle identification. Nuclear Instruments
and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associ-
ated Equipment, 543(2):577-584, 2005.
11
Published as a conference paper at ICLR 2021
Mohammad Hossein Shaker and Eyke Hullermeier. Aleatoric and epistemic uncertainty with ran-
dom forests. In International Symposium on Intelligent Data Analysis, pp. 444-456. Springer,
2020.
Lewis Smith and Yarin Gal. Understanding Measures of Uncertainty for Adversarial Example De-
tection. In UAI, 2018.
UCI. KDD internet usage data. https://archive.ics.uci.edu/ml/datasets/
Internet+Usage+Data, 1997.
Aleksei Ustimenko and Liudmila Prokhorenkova. SGLB: Stochastic Gradient Langevin Boosting.
arXiv e-prints, art. arXiv:2001.07248, 2020.
Qiang Wu, Christopher JC Burges, Krysta M Svore, and Jianfeng Gao. Adapting boosting for
information retrieval measures. Information Retrieval, 13(3):254-270, 2010.
Yanru Zhang and Ali Haghani. A gradient boosting method to improve travel time prediction. Trans-
portation Research Part C: Emerging Technologies, 58:308-324, 2015.
12
Published as a conference paper at ICLR 2021
A Experimental setup
A. 1 Our implementation of data uncertainty
As discussed in Section 2.2 of the main text, for regression we simultaneously predict the parameters
μ and log σ of the Normal distribution. Similarly to NGBoost, We use the natural gradients. For our
loss and parameterization, the natural gradient is:
g(t) (x, y)
μ (T-% 1 -
1
2
(16)
At each step of the gradient boosting procedure, We construct one tree predicting both components
of g(t), similarly to the MultiRMSE regime of CatBoost.6
Recall that for classification We optimize the logistic loss.
In Table 2, We compare our implementation With NGBoots (Duan et al., 2020) and Deep Ensem-
bles (Lakshminarayanan et al., 2017) on regression datasets. For our implementation, We consider
SGB With fixed sample rate (0.5) and perform parameter tuning as described beloW. The best results
are highlighted.
Table 2: Comparison of our implementation With existing methods
Dataset	Deep. Ens.	RMSE NGBoost	CatBoost	Deep. Ens.	NLL NGBoost	CatBoost
Boston	3.28 ± 1.00	2.94 ± 0.53	3.06 ± 0.68	2.41 ± 0.25	2.43 ± 0.15	2.47 ± 0.20
Concrete	6.03 ± 0.58	5.06 ± 0.61	5.21 ± 0.53	3.06 ± 0.18	3.04 ± 0.17	3.06 ± 0.13
Energy	2.09 ± 0.29	0.46 ± 0.06	0.57 ± 0.06	1.38 ± 0.22	0.60 ± 0.45	1.24 ± 1.28
Kin8nm	0.09 ± 0.00	0.16 ± 0.00	0.14 ± 0.00	-1.20 ± 0.02	-0.49 ± 0.02	- 0.63 ± 0.02
Naval	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	-5.63 ± 0.05	-5.34 ± 0.04	-5.39 ± 0.04
PoWer	4.11 ± 0.17	3.79 ± 0.18	3.55 ± 0.27	2.79 ± 0.04	2.79 ± 0.11	2.72 ± 0.12
Protein	4.71 ± 0.06	4.33 ± 0.03	3.92 ± 0.08	2.83 ± 0.02	2.81 ± 0.03	2.73 ± 0.07
Wine	0.64 ± 0.04	0.63 ± 0.04	0.63 ± 0.04	0.94 ± 0.12	0.91 ± 0.06	0.93 ± 0.08
Yacht	1.58 ± 0.48	0.50 ± 0.20	0.82 ± 0.40	1.18 ± 0.21	0.20 ± 0.26	0.41 ± 0.39
Year MSD	8.89 ± NA	8.94 ± NA	8.99 ± NA	3.35 ± NA	3.43 ± NA	3.43 ± NA
A.2 Parameter tuning
For all approaches, We use grid search to tune learning-rate in {0.001, 0.01, 0.1}, tree depth in
{3, 4, 5, 6}. We fix subsample to 0.5 for SGB and to 1 for SGLB. This is done to avoid joint random-
ization effects of SGB sampling and SGLB noise in gradients. We also set diffusion-temperature
=N and model-shrink-rate =* for SGLB.
A.3 Datasets
The datasets are described in Table 3. For regression, We use standard train/validation/test
splits (UCI). For classification, We split the datasets into proportion 65/15/20 in train, validation,
and test sets. For more details, see our GitHub repository.7
A.4 Statistical significance
For regression, We perform cross-validation to estimate statistical significance With paired t-test. In
the corresponding tables, We highlight the approaches that are insignificantly different from the best
one (p-value > 0.05).
6https://catboost.ai/docs/concepts/loss- functions-multiregression.html
7https://github.com/yandex-research/GBDT-uncertainty
13
Published as a conference paper at ICLR 2021
Table 3: Datasets description
Dataset	# Examples	# Features
Classification		
Adult(Kohavi,1996)	48842	14
Amazon (Kaggle, 2017)	32769	9
Click (KDD, 2012)	399482	11
Internet (UCI, 1997)	10108	68
KDD-Appetency (KDD, 2009)	50000	419
KDD-Churn (KDD, 2009)	50000	419
KDD-Upselling (KDD, 2009)	50000	419
Kick (Kaggle, 2011)	72983	43
Regression		
Boston (UCI)	506	13
Concrete (UCI)	1030	8
Energy (UCI)	768	8
Kin8nm (UCI)	8192	8
Naval (UCI)	11934	16
Power (UCI)	9568	4
Protein (UCI)	45730	9
Wine (UCI)	1599	11
Yacht (UCI)	308	6
Year MSD (Bertin-Mahieux et al., 2011)	515345	90
Table 4: NLL and RMSE/Error rate for regression and classification
	Single		Ensemble			Single		Ensemble		
Dataset	SGB	SGLB	SGB	SGLB	vSGLB	SGB	SGLB	SGB	SGLB	vSGLB
	Classification NLL Q)						Classification		% Error (J)	
	 Adult	0.276	0.273	0.276	0.271	0.274	12.8	12.7	12.8	12.6	12.7
Amazon	0.141	0.142	0.140	0.142	0.143	4.7	4.6	4.6	4.5	4.5
Click	0.393	0.392	0.392	0.391	0.392	15.6	15.7	15.6	15.6	15.6
Internet	0.224	0.218	0.221	0.217	0.218	9.9	10.0	9.7	10.0	10.0
Appetency	0.073	0.073	0.073	0.073	0.073	1.8	1.8	1.8	1.8	1.8
Churn	0.235	0.236	0.233	0.234	0.235	7.3	7.2	7.3	7.2	7.2
Upselling	0.168	0.168	0.168	0.168	0.168	5.0	4.9	5.0	5.0	4.9
Kick	0.287	0.286	0.286	0.285	0.286	9.5	9.6	9.5	9.4	9.6
Dataset	Regression NLL (J)					Regression RMSE (J)				
BostonH	2.47	2.52	2.46	2.50	2.50	3.06	3.12	3.04	3.10	3.27
Concrete	3.06	3.06	3.05	3.05	3.06	5.21	5.11	5.21	5.10	5.37
Energy	1.24	1.70	1.13	1.52	0.70	0.57	0.54	0.57	0.54	0.64
Kin8nm	-0.63	-0.65	-0.63	-0.65	-0.60	0.14	0.14	0.14	0.14	0.15
Naval-p	-5.39	-5.42	-5.61	-5.65	-5.39	0.00	0.00	0.00	0.00	0.00
Power-p	2.72	2.71	2.66	2.66	2.69	3.55	3.56	3.52	3.54	3.64
Protein	2.73	2.73	2.61	2.64	2.70	3.92	3.96	3.90	3.93	4.02
Wine-qu	0.93	0.99	0.92	0.98	0.96	0.63	0.65	0.63	0.65	0.66
Yacht	0.41	0.38	0.27	0.32	0.51	0.82	0.84	0.83	0.84	0.97
Year	3.43	3.43	3.41	3.40	3.42	8.99	8.96	8.97	8.94	8.98
14
Published as a conference paper at ICLR 2021
For classification (and Year MSD), we measure statistical significance for NLL and error/RMSE on
the test set. In the corresponding tables, the approaches that are insignificantly different from the
best one are highlighted. For PRR and AUC-ROC (for classification and Year MSD), we highlight
the best value.
B	Additional experimental results
In Table 4, we compare ensemble approaches with single models in terms of NLL and error rate for
classification and in terms of NLL and RMSE for regression tasks. Results for NLL demonstrate
an advantage of ensembling approaches compared to single models. However, in some cases the
difference is not significant, which can be explained by the additive nature of boosting: averaging
several tree ensembles gives another (larger) tree ensemble. Thus, improved NLL can result from
the increased complexity of ensemble models. We can make a similar conclusion from the results
for RMSE and error rate.
C Comparison with Random Forest
Our paper specifically focuses on uncertainty estimation in Gradient Boosted Decision Trees
(GBDT) models. However, some related work was done for quantifying uncertainty in random
forests (CoUlston et al., 2016; Shaker & Hullermeier, 2020), which are also ensembles of decision
trees. Thus, for completeness, we also analyze how ensemble approaches perform in combination
with random forests.
In these experiments, we Use the scikit-learn implementation of random forests (Pedregosa et al.,
2011). We limit the maximUm depth to 10 and keep all other parameters defaUlt. For categorical
featUres, we Use leave-one-oUt encoding.
Unlike GBDT, where trees are added to correct the previoUs model’s mistakes, random forests (RF)
consist of decision trees that are independently trained on bootstrapped sUb-samples of the dataset.
Hence, for knowledge uncertainty we can divide RF into several independent parts, each consisting
of several trees. Drawing a parallel to virtUal SGLB, we call this approach vRF (virtUal RF) since
it allows estimating knowledge uncertainty Using only one trained random forest model. In oUr
experiments with vRF, we divide one RF model into 10 independent parts, each consisting of 100
trees. Similarly, one can also constrUct an ensemble of several independently trained random forest
models, which is expected to be a stronger baseline. However, we expect a small difference between
vRF and an ensemble of random forests, as there are, a priori, no correlations between trees both in
a single model and across mUltiple RF models.
In Tables 5 and 6, we compare the predictive performance of random forests (both individUal models
and explicit ensembles of mUltiple models) to SGLB individUal and ensemble models on classifica-
tion and regression tasks. The resUlts show that generally GBDT models oUtperform random forest
models in terms of classification error rate and NLL. Note that we cannot calcUlate NLL for RF
regression models as they are not natUrally probabilistic (do not yield a predicted variance). As a
resUlt, they are Unable to estimate data uncertainty, and therefore we can only obtain estimates of
knowledge uncertainty.
Table 7 compares SGLB and RF ensembles in terms of error detection (PRR) and oUt-of-domain
inpUt detection (ROC-AUC). One can see that SGLB UsUally oUtperforms RF, especially for OOD
detection. Notably, as we expected, for OOD detection vRF and RF give similar resUlts. ThUs, we
conclUde that for random forests, a virtUal ensemble is a good and cheap alternative to the trUe one.
15
Published as a conference paper at ICLR 2021
Table 5: Comparison with random forest: NLL and error rate for classification
	Single		Ensemble		Single		Ensemble	
Dataset	SGLB	RF	SGLB	RF	SGLB	RF	SGLB	RF
	Classification NLL			0)	Classification % Error (J)			
	 Adult	0.273	0.300	0.271	0.300	12.7	13.9	12.6	13.9
Amazon	0.142	0.183	0.142	0.183	4.6	5.6	4.5	5.6
Click	0.392	0.411	0.391	0.411	15.7	16.0	15.6	16.0
Internet	0.218	0.275	0.217	0.274	10.0	11.2	10.0	11.0
KDD-Appetency	0.073	0.083	0.073	0.083	1.8	1.8	1.8	1.8
KDD-Churn	0.236	0.249	0.234	0.249	7.2	7.3	7.2	7.3
KDD-Upselling	0.168	0.202	0.168	0.202	4.9	7.4	5.0	7.4
Kick	0.286	0.311	0.285	0.311	9.6	10.4	9.4	10.4
Table 6: Comparison with random forest: RMSE for regression
Dataset	Single		Ensemble	
	SGLB	RF	SGLB	RF
BostonH	3.12	2.98	3.10	2.98
Concrete	5.11	4.96	5.10	4.95
Energy	0.54	0.50	0.54	0.50
Kin8nm	0.14	0.15	0.14	0.15
Naval-p	0.00	0.00	0.00	0.00
Power-p	3.56	3.53	3.54	3.53
Protein	3.96	4.19	3.93	4.19
Wine-qu	0.65	0.58	0.65	0.58
Yacht	0.84	0.84	0.84	0.84
Year	8.96	9.43	8.94	9.43
16
Published as a conference paper at ICLR 2021
Table 7: Comparison with random forest: detection of errors and OOD examples for regression and
classification (virtual and true ensembles)
Dataset	∣		VSGLB	vRF	SGLB	RF	VSGLB	VRF	SGLB	RF
		Classification		% PRR (↑)		Classification % AUC-ROC (↑)			
	 Adult	— TU	72	70	72	70	51	58	51	58
	KU	38	20	49	22	85	86	89	87
Amazon	TU	68	63	68	64	86	48	86	49
	KU	40	45	61	52	67	55	74	53
Click	TU	44	36	44	37	68	71	64	71
	KU	11	20	22	21	90	81	92	80
Internet	TU	79	69	79	68	68	72	69	72
	KU	61	38	72	36	81	79	89	79
KDD-APPetenCy	TU	69	56	69	56	52	82	50	81
	KU	14	34	54	34	93	97	91	98
KDD-Chum	TU	46	39	46	39	60	75	75	78
	KU	28	13	35	9	92	93	98	94
KDD-Upselling	TU	56	66	57	66	47	73	60	72
	KU	33	45	49	44	78	90	97	92
Kick	TU	45	38	44	38	38	64	58	63
	KU	20	27	34	29	89	89	98	89
Dataset		Regression % PRR (↑)				Regression %		AUC-ROC	(↑)
BostonH	TU	46	—	45	—	64	—	69	—
	KU	38	53	37	55	49	75	80	76
Concrete	TU	41	—	42	—	78	—	81	—
	KU	25	43	27	42	56	80	92	80
Energy	TU	62	—	56	—	69	—	89	—
	KU	54	40	31	41	32	100	100	100
Kin8nm	TU	58	—	59	—	42	—	43	—
	KU	35	33	19	33	45	48	45	48
Power-p	TU	32	—	33	—	47	—	49	—
	KU	13	21	9	22	57	66	73	65
Protein	TU	48	—	50	—	86	—	91	—
	KU	12	40	29	40	94	91	99	92
Wine-qu	TU	32	—	32	—	56	—	56	—
	KU	9	35	19	32	49	74	72	74
Yacht	TU	88	—	88	—	55	—	58	—
	KU	66	79	78	81	40	52	60	52
Year	TU	62	—	63	—	57	—	60	—
	KU	25	28	30	27	52	74	57	74
17