Published as a conference paper at ICLR 2021
Multi-Prize Lottery Ticket Hypothesis :
Finding Accurate B inary Neural Networks by
Pruning A Randomly Weighted Network
James Diffenderfer & Bhavya Kailkhura
Center for Applied Scientific Computing
Lawrence Livermore National Laboratory
Livermore, CA 94550, USA
{diffenderfer2,kailkhura1}@llnl.gov
Ab stract
Recently, Frankle & Carbin (2019) demonstrated that randomly-initialized dense
networks contain subnetworks that once found can be trained to reach test accu-
racy comparable to the trained dense network. However, finding these high per-
forming trainable subnetworks is expensive, requiring iterative process of training
and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize
Lottery Ticket Hypothesis:
A sufficiently over-parameterized neural network with random weights contains
several subnetworks (winning tickets) that (a) have comparable accuracy to a
dense target network with learned weights (prize 1), (b) do not require any fur-
ther training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of
quantization (i.e., binary weights and/or activation) (prize 3).
This provides a new paradigm for learning compact yet highly accurate binary
neural networks simply by pruning and quantizing randomly weighted full pre-
cision neural networks. We also propose an algorithm for finding multi-prize
tickets (MPTs) and test it by performing a series of experiments on CIFAR-10
and ImageNet datasets. Empirical results indicate that as models grow deeper
and wider, multi-prize tickets start to reach similar (and sometimes even higher)
test accuracy compared to their significantly larger and full-precision counter-
parts that have been weight-trained. Without ever updating the weight values,
our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA)
Top-1 accuracy - 94.8% on CIFAR-10 and 74.03% on ImageNet - but also out-
perform their full-precision counterparts by 1.78% and 0.76%, respectively. Fur-
ther, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural net-
works on CIFAR-10. Code and pre-trained models are available at: https:
//github.com/chrundle/biprop.
1	Introduction
Deep learning (DL) has made a significant breakthroughs in a wide range of applications (Goodfel-
low et al., 2016). These performance improvements can be attributed to the significant growth in the
model size and the availability of massive computational resources to train such models. Therefore,
these gains have come at the cost of large memory consumption, high inference time, and increased
power consumption. This not only limits the potential applications where DL can make an impact
but also have some serious consequences, such as, (a) generating huge carbon footprint, and (b)
creating roadblocks to the democratization of AI. Note that significant parameter redundancy and a
large number of floating-point operations are key factors incurring the these costs. Thus, for discard-
ing the redundancy from DNNs, one can either (a) Prune: remove non-essential connections from
an existing dense network, or (b) Quantize: constrain the full-precision (FP) weight and activation
values to a set of discrete values which allows them to be represented using fewer bits. Further, one
can exploit the complementary nature of pruning and quantization to combine their strengths.
1
Published as a conference paper at ICLR 2021
Figure 1: Multi-Prize Ticket Performance: Multi-prize tickets, obtained only by pruning and
binarizing random networks, outperforms trained full precision and SOTA binary weight networks.
Although pruning and quantization1 are typical approaches used for compressing DNNs (Neill,
2020), it is not clear under what conditions and to what extent compression can be achieved with-
out sacrificing the accuracy. The most extreme form of quanitization is binarization, where weights
and/or activations can only have two possible values, namely -1(0) or +1 (the interest of this paper).
In addition to saving memory, binarization results in more power efficient networks with significant
computation acceleration since expensive multiply-accumulate operations (MACs) can be replaced
by cheap XNOR and bit-counting operations (Qin et al., 2020a). In light of these benefits, it is of in-
terest to question if conditions exists such that a binarized DNN can be pruned to achieve accuracy
comparable to the dense FP DNN. More importantly, even if these favourable conditions are met
then how do we find these extremely compressed (or compact) and highly accurate subnetworks?
Traditional pruning schemes have shown that a pretrained DNN can be pruned without a significant
loss in the performance. Recently, (Frankle & Carbin, 2019) made a breakthrough by showing that
dense network contain sparse subnetworks that can match the performance of the original network
when trained from scratch with weights being reset to their initialization (Lottery Ticket Hypothesis).
Although the original approach to find these subnetworks still required training the dense network,
some efforts (Wang et al., 2020b; You et al., 2019; Wang et al., 2020a) have been carried out to
overcome this limitation. Recently a more intriguing phenomenon has been reported - a dense
network with random initialization contains subnetworks that achieve high accuracy, without any
further training (Zhou et al., 2019; Ramanujan et al., 2020; Malach et al., 2020; Orseau et al., 2020).
These trends highlight good progress being made towards efficiently and accurately pruning DNNs.
In contrast to these positive developments for pruning, results on binarizing DNNs have been mostly
negative. To the best of our knowledge, post-training schemes have not been successful in binariz-
ing pretrained models without retraining. Even with training binary neural networks (BNNs) from
scratch (though inefficient), the community has not been able to make BNNs achieve compara-
ble results to their full precision counterparts. The main reason being that network structures and
weight optimization techniques are predominantly developed for full precision DNNs and may not
be suitable for training BNNs. Thus, closing the gap in accuracy between the full precision and
the binarized version may require a paradigm shift. Furthermore, this also makes one wonder if
efficiently and accurately binarizing DNNs similar to the recent trends in pruning is ever feasible.
In this paper, we show that a randomly initialized dense network contains extremely sparse binary
subnetworks that without any weight training (i.e., efficient) have comparable performance to their
trained dense and full-precision counterparts (i.e., accurate). Based on this, we state our hypothesis:
Multi-PriZe Lottery Ticket Hypothesis. A sufficiently over-parameterized neural network with
random weights contains several subnetworks (Winning tickets) that (a) have comparable ac-
curacy to a dense target network with learned weights (prize 1), (b) do not require any further
training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary
weights and/or activation) (prize 3).
Contributions. First, we propose the multi-prize lottery ticket hypothesis as a new perspective on
finding neural networks with drastically reduced memory size, much faster test-time inference and
1A detailed discussion on related work on pruning and quantization is provided in Appendix F.
2
Published as a conference paper at ICLR 2021
lower power consumption compared to their dense and full-precision counterparts. Next, we pro-
vide theoretical evidence of the existence of highly accurate binary subnetworks within a randomly
weighted DNN (i.e., proving the multi-prize lottery ticket hypothesis). Specifically, we mathemati-
cally prove that we can find an ε-approximation of a fully-connected ReLU DNN with width n and
depth ` using a sparse binary-weight DNN of sufficient width. Our proof indicates that this can be
accomplished by pruning and binarizing the weights of a randomly weighted neural network that is
a factor O(n3^'∕ε) wider and 2' deeper. To the best of our knowledge, this is the first theoretical
work proving the existence of highly accurate binary subnetworks within a sufficiently overparame-
terized randomly initialized neural network. Finally, we provide biprop (binarize-prune optimizer)
in Algorithm 1 to identify MPTs within randomly weighted DNNs and empirically test our hypoth-
esis. This provides a completely new way to learn BNNs without relying on weight-optimization.
Results. We explore two variants of multi-prize tickets - one with binary weights (MPT-1/32) and
other with binary weights and activation (MPT-1/1) where x/y denotes x and y bits to represent
weights and activation, respectively. MPTs we find have 60 - 80% fewer parameters than the
original network. We perform a series of experiments on on small and large scale datasets for image
recognition, namely CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). On
CIFAR-10, we test the performance of multi-prize tickets against the trend of making the model
deeper and wider. We found that as models grow deeper and wider, both variants of multi-prize
tickets start to reach similar (and sometimes even higher) test accuracy compared to the dense and
full precision original network with learned weights. In other words, the performance of multi-
prize tickets improves with the amount of redundancy in the original network. We also carry out
experiments with state-of-the-art (SOTA) architectures on CIFAR-10 and ImageNet datasets with
an aim to investigate their redundancy. We find that within most randomly weighted SOTA DNNs
reside extremely compact (i.e., sparse and binary) subnetworks which are smaller than, but match
the performance of trained target dense and full precision networks. Furthermore, with minimal
hyperparameter tuning, our MPTs achieve Top-1 accuracy comparable to (or higher than) SOTA
BNNs. The performance of MPTs is further improved by allowing the parameters in BatchNorm
layer to be learned. Finally, on both CIFAR-10 and ImageNet, MPT-1/32 subnetworks outperform
their significantly larger and full-precision counterparts that have been weight-trained.
2	Multi-Prize Lottery Tickets: Theory and Algorithms
We first prove the existence of MPTs in an overparameterized randomly weighted DNN. For ease
of presentation, we state an informal version of Theorem 2 which can be found in Appendix B. We
then explore two variants of tickets (MPT-1/32 and MPT-1/1) and provide an algorithm to find them.
2.1	Proving the Multi-Prize Lottery Tickets Hypothesis
In this section we seek to answer the following question: What is the required amount of over-
parameterization such that a randomly weighted neural network can be compressed to a sparse
binary subnetwork that approximates a dense trained target network?
Theorem 1. (Informal Statement of Theorem 2) Let ε, δ > 0. For every fully-connected (FC) target
network with ReLU activations of depth ` and width n with bounded weights, a random binary FC
network with ReLU activations of depth 2' and width O (('n3/2/£)+ 'n log('n∕δ)) contains with
probability (1 - δ) a binary subnetwork that approximates the target network with error at most ε.
Sketch OfProof Consider a FC ReLU network F(x) = W(')σ(W('-1) …σ(W⑴x)), where
σ(x) = max{0, x}, x ∈ Rd, W(i) ∈ Rki ×ki-1, k0 = d, and i ∈ [']. Additionally, consider a
FC network with binary weights given by G(X) = B('0)σ(B('0-1) …σ(B⑴x)), where B⑶ ∈
{-1, +1}k0i×ki0-1, k00 = d, and i ∈ ['0]. Our goal is to determine a lower bound on the depth, '0,
and the widths, {ki}'= i, such that with probability (1 - δ) the network G(X) contains a subnetwork
G(X) satisfying kG(X) - F (X)k ≤ ε, for any ε > 0 and δ ∈ (0, 1). We first establish lower
bounds on the width of a network of the form g(X) = B(2)σ(B(1)X) such that with probability
(1 - δ0) there exists a subnetwork g(x) of g(x) s.t. kg(x) — σ(Wx)k ≤ ε0, for any ε0 > 0 and
δ0 ∈ (0, 1). This process is carried out in detail in Lemmas 1, 2, and 3 in Appendix B. We have
3
Published as a conference paper at ICLR 2021
now approximated a single layer FC real-valued network using a subnetwork of a two-layer FC
binary network. Hence, We can take ' = 2' and Lemma 3 provides lower bounds on the width of
each intermediate layer such that with probability (1 一 δ) there exists a subnetwork G(X) of G(x)
satisfying IlG(X) — F(x)k ≤ ε. This is accomplished in Theorem 2 in Appendix B.
To the best of our knowledge this is the first theoretical result proving that a sparse binary-weight
DNN that can approximate a real-valued target DNN. As it has been established that real-valued
DNNs are universal approximators (Scarselli & Tsoi, 1998), our result carries the implication that
sparse binary-weight DNNs are also universal approximators. In relation to the first result establish-
ing the existence of real-valued subnetworks in a randomly weighted DNN approximating a real-
valued target DNN (Malach et al., 2020), the lower bound on the width established in Theorem 2 is
better than their lower bound of O ('2n2 log('n∕δ)∕ε2).
2.2 Finding Multi-Prize Winning Tickets
Given the existence of multi-prize winning tickets from Theorem 2, a natural question arises - How
should we find them? In this section, we answer this question by introducing an algorithm for finding
multi-prize tickets.2 Specifically, we explore two variants of multi-prize tickets in this paper - 1)
MPT-1/32 where weights are quantized to 1-bit with activations being real valued (i.e., 32-bits) and
2) MPT-1/1 where both weights and activations are quantized to 1-bit. We first outline a generic
process for identifying MPTs along with some theoretical motivation for our approach.
Given a neural network g(X; W) with weights W ∈ Rm, we can express a subnetwork of g using
a binary mask M ∈ {0, 1}m as g(X; M W), where denotes the Hadamard product. Hence, a
binary subnetwork can be expressed as g(X; M B), where B ∈ {一1, +1}m. Lemma 1 in Ap-
pendix B indicates that rescaling the binary weights to {-α,α} using a gain term α ∈ R is necessary
to achieve good performance of the resulting subnetwork. We note that the use of gain terms is com-
mon in binary neural networks (Qin et al., 2020a; Martinez et al., 2020; Bulat & Tzimiropoulos,
2019). Combining all this allows us to represent a binary subnetwork as g(X; α(M B)).
Now we focus on how to update M, B, and a. Suppose f (x; W*) is a target network with opti-
mized weights W * that we wish to approximate. Assuming g(x; ∙) is K-Lipschitz continuous yields
Ilg (x; α(M Θ B))- f (x; W*)k ≤ K IlM Θ (W - αB)k + ∣∣g(x; M Θ W) - f (x; W*)k.	(1)
-- y 、 一..- / --
MPT error
Binarization error
Subnetwork error
Hence, the MPT error is bounded above by the error of the subnetwork of g with the original weights
and the error from binarizing the current subnetwork. This informs our approach for identifying
MPTs: 1) Update a pruning mask M that reduces the subnetwork error (lines 7-9in Algorithm 1),
and 2) apply binarization with a gain term that minimizes the binarization error (lines 4 and 10).
We first discuss how to update M . While we could search for M by minimizing the subnetwork
error in (1), this would require the use of a pretrained target network (i.e., f(X; W*)). To avoid
requiring a target network in our method we instead aim to minimize the training loss w.r.t. M in the
current binary subnetwork. Directly optimizing over the pruning mask is a combinatorial problem.
So to update the pruning mask efficiently we optimize over a set of scores S ∈ Rm corresponding to
each randomly initialized weight in the network. In this approach, each component of the randomly
initialized weights is assigned a pruning score. The pruning scores are updated via backpropagation
by computing the gradient of the loss function over minibatches with respect to the pruning scores
(line 7). Then the magnitude of the scores in absolute value are used to identify the P percent
of weights in each layer that are least important to the success of the binary subnetwork (line 8).
The components of the pruning mask corresponding to these indices are set to 0 and the remaining
components are set to 1 (line 9). To avoid unintentionally pruning an entire layer of the network, we
use a pruning mask for each layer that prunes P percent of the weights in that layer. The choice to
use pruning scores to update the mask M was due to the fact that it is computationally efficient. The
use of pruning scores is a well-established optimization technique used in a range of applications
(Joshi & Boyd, 2009; Ramanujan et al., 2020).
2Although our results are derived under certain assumptions (e.g., fully-connected, ReLU neural network
approximated by a subnetwork with binary weights), our algorithm is not restricted by these assumptions.
4
Published as a conference paper at ICLR 2021
Algorithm 1 biprop: Finding multi-prize tickets in a randomly weighted neural network
1:	Input: Neural network g(x; ∙) With 1- or 32-bit activations; Network depth '; Layer widths
{kj}'=i； Loss function L; Training data {(x(i), y(i))}N=ι; Pruning percentage P.
2:	Randomly Initialize FP Parameters: Network weights { W(j)}'=ι; Pruning scores {S(j)}j=ι.
3:	Initialize Layerwise Pruning Masks: {M(j)}'=ι each to 1.
4:	Initialize BinarySubnetWorkWeights: {B(j)}j=ι - {sign(W(j))}'=ι.
5:	Initialize Layerwise Gain Terms: {α(j)}j=ι J {∣∣M(j) Θ W(j)kι∕∣∣M(j)kι}'=ι∙
6:	for k = 1 to Nepochs do
7:	SIjj J SIjj - nNs(j)L({α(j)(M⑶ Θ B(j))}j=ι)	Update pruning scores at layer j
8:	{τ(i)}k= ι J Sorting of indices {i}k= ι s.t.厚；：)| ≤ 厚*1)| Index sort over values |S(j)|
9:	Mij) J 1{τ(i)≥dkjp∕i0θ]}(i)	Update pruning mask at layer j
10:	α(jj J kM(jj Θ W(jj k1∕kM(jj k1	Update gain term at layer j
11:	Output: Return Binarized Subnetwork g(x; {α(jj(M(j) Θ B(jj)}j=ι).
We now consider how to update B and α. By keeping M fixed, we can derive the following
closed form expressions that minimize the binarization error in (1): B* = Sign(W) and a* =
kM Θ Wk1∕kMk1. These closed form expressions indicate that only the gain term needs to be
recomputed after each update to M. Hence, B = sign(W) throughout our entire approach (line
4). We update a gain term for each layer of the subnetwork in our approach based on the formula
for α* (line 10). More details on the derivation of B * and α* are provided in Appendix C.
Pseudocode for our method biprop (binarize-prune optimizer) is provided in Algorithm 1 and cross-
entropy loss is used in our experiments. Note that the process for identifying MPT-1/32 and MPT-1/1
differs only in computation of the gradient. Next, we explain how these gradients can be computed.
2.2	. 1 Updating Pruning Scores for Binary-Weight Tickets (MPT- 1/32)
As an example, for a FC network where the state at each layer is defined recursively by
U⑴=α(1j(B(1j	Θ M⑴)x	and	U(j)	=	α(j)(B(j)	Θ M(jj)σ(U(j-1))	we have	^Ly	=
∂Sp(j,q)
∂l ∂uqj) ∂Mpj)
∂uqj) ∂Mj ∂spq
We use the straight-through estimator (Bengio et al., 2013) for
∂Mp"
∂Sjq
which
yields ^dL) = ^dLj) a(j，Bj σ (UpjT)), where ^dLT is computed via backpropagation.
2.2	.2 Updating Pruning Scores for Binary-Activation Tickets (MPT-1/1)
Note that MPT-1/1 uses the sign activation function. From Section 2.2.1, it immediately follows that
^dLr = ^dLj) a(j)Bj sign (Upj-1j). However, updating ^dLj) via backpropagation requires a
gradient estimator for the sign activation function. To motivate our choice of estimator note that we
can approximate the sign function using a quadratic spline parameterized by some t > 0:
st(χ)={q[(X)
x < -t
x ∈ [-t, 0)
x ∈ [0, t)
x≥t
(2)
In (2), qi (x) = aix2 + bix + ci and suitable values for the coefficients are derived using the
following zero- and first-order constraints: q1(-t) = -1, q1 (0) = 0, q2(0) = 0, q2(t) = 1,
q10 (-t) = 0, q10 (0) = q20 (0), and q20 (t) = 0. This yields q1(x) = (x∕t)2 + 2(x∕t) and
q2(x) = -(x∕t)2 + 2(x∕t).	As st(x)	approximates	sign(x),	we can use s0t(x) as	our gra-
dient estimator. Since q1 (x)	= ∣ (1 +	x) and q2 (x) = ∣ (1	一 xχ) it follows that	s；(x) =
[t(1 一 号)]1{x∈[-t,t]}(χ). The choice to approximate sign using a quadratic spline instead
of a cubic spline results in a gradient estimator that can be implemented efficiently in PyTorch
5
Published as a conference paper at ICLR 2021
as torch.clamp(2*(1-torch.abs(x)/t)/t,min=0.0). We note that limt→0 st(x) =
sign(x), which suggests that smaller values of t yield more suitable approximations. Our experi-
ments use s01 (x) as the gradient estimator since we found it to work well in practice. Finally, we
note that taking t = 1 in our gradient estimator yields the same value as the gradient estimator in
(Liu et al., 2018a), however, our implementation in PyTorch is 6× more memory efficient.
3	Experimental Results
The primary goal of the experiments in Section 3.1 is to empirically verify our Multi-Prize Lot-
tery Ticket Hypothesis. As a secondary objective, we would like to determine tunable factors that
make randomly-initialized networks amenable to containing readily identifiable Multi-Prize Tickets
(MPTs). Thus, we test our hypothesis against the general trend of increasing the model size (depth
and width) and monitor the accuracy of the identified MPTs. After verifying our Multi-Prize Lottery
Ticket Hypothesis, we consider the performance of MPTs compared to state-of-the-arts in binary
neural networks and their dense counterparts on CIFAR-10 and ImageNet datasets in Section 3.2.
Building upon edge-popup (Ramanujan et al., 2020), we implement Algorithm 1 to identify MPTs.3
3.1	Where can we expect to find multi-prize tickets ?
In this section, we empirically test the effect of overparameterization on the performance of MPTs.
We overparameterize networks by making them (a) deeper (Sec. 3.1.1) and (b) wider (Sec. 3.1.2).
Conv-2	Conv-4	Conv-6	Conv-8
20 30 40 50 60 70 80 90 20 30 40 50 60 70 80 90 20 30 40 50 60 70 80 90 20 30 40 50 60 70 80 90
Percentage of Weights Pruned	Percentage of Weights Pruned	Percentage of Weights Pruned	Percentage of Weights Pruned
-H MPT-1/32 (Pruned) T・ MPT-1/1 (Pruned) ■•卜・Dense (Learned Weights)
Figure 2: Effect of Varying Depth and Pruning Rate: Comparing the Top-1 accuracy of small and
binary MPTs to a large, full-precision, and weight-optimized network on CIFAR-10.
We use VGG (Simonyan & Zisserman, 2014) variants as our network architectures for searching for
MPTs. In each randomly weighted network, we find winning tickets MPT-1/32 and MPT-1/1 for
different pruning rates using Algorithm 1. We choose our baselines as dense full-precision models
with learned weights. In all experiments, we use three independent initializations and report the
average of Top-1 accuracy with with error bars extending to the lowest and highest Top-1 accuracy.
Additional experiment configuration details are provided in Appendix A.
3.1.1	Do winning tickets exist in deep networks?
In this experiment, we empirically test the following hypothesis: As a network grows deeper, the per-
formance of multi-prize tickets in the randomly initialized network will approach the performance of
the same network with learned weights. We are further interested in exploring the required network
depth for our hypothesis to be true.
In Figure 2, we vary the depth of VGG architectures (d = 2 to 8) and compare the Top-1 accuracy
of MPTs (at different pruning rates) with weight-trained dense network. We notice that there exist
a range of pruning rates where the performance of MPTs are very similar, and beyond this range
the performance drops quickly. Interestingly, as the network depth increases, more parameters can
be pruned without hurting the performance of MPTs. For example, MPT-1/32 can match the per-
formance of trained ConV-8 while having only 〜20% of its parameter count. Interestingly, the
3A comparison of MPT-1/32 found using biprop and edgepopup is provided in Appendix E, which demon-
strates that biprop outperforms edgepopup.
6
Published as a conference paper at ICLR 2021
Conv-4
Conv-6
Conv-8
0.5	1.0	1.5	2.0	0.5	1.0	1.5	2.0	0.5	1.0	1.5	2.0
Layer Width Factor	Layer Width Factor	Layer Width Factor
-H MPT-1/32 (Prune 90%) T ■ MPT-1/32 (Prune 80%) T - MPT-1/32 (Prune 60%) ・・卜・ Dense (Learned Weights)
Figure 3: Effect of Varying Width on MPT-1/32: Comparing the Top-1 accuracy of sparse and
binary MPT-1/32 to dense, full-precision, and weight-optimized network on CIFAR-10.
performance gap between MPT-1/32 and MPT-1/1 does not change much with depth across differ-
ent pruning rates. We further note that the performance of MPTs improve when increasing the depth
and both start to approach the performance of the dense model with learned weights. This gain
starts to plateau beyond a certain depth, suggesting that the MPTs might be approaching the limit
of their achievable accuracy. Surprisingly, MPT-1/32 performs equally good (or better) than the
weight-trained model regardless of having 50 - 80% lesser parameters and weights being binarized.
3.1.2	Do winning tickets exist in wide networks
ConV-4_______________ ______________________COnV-6_______________ _____________________COnV ・8
0.5	1.0	1.5	2.0	0.5	1.0	1.5	2.0	0.5	1.0	1.5	2.0
Layer Width Factor	Layer Width Factor	Layer Width Factor
-I- MPT-1/1 (Prune 90%) T, MPT-1/1 (Prune 80%)T・ MPT-1/1 (Prune 60%) ・・卜・ Dense (Learned Weights)
Figure 4: Effect of Varying Width on MPT-1/1: Comparing the Top-1 accuracy of sparse and
binary MPT-1/1 to dense, full-precision, and weight-optimized network on CIFAR-10.
Similar to the previous experiment, in this experiment, we empirically test the following hypothesis:
As a network grows wider, the performance of multi-prize tickets in the randomly initialized network
will approach the performance of the same network with learned weights. We are further interested
in exploring the required layer width for our hypothesis to be true.
In Figures 3 and 4, we vary the width of different VGG architectures and compare the Top-1 accuracy
of MPT-1/32 and MPT-1/1 tickets (at different pruning rates) with weight-trained dense network. A
width multiplier of value 1 corresponds to the models in Figure 2. Performance of all the models
improves when increasing the width and the performance of both MPT-1/32 and MPT-1/1 start to
approach the performance of the dense model with learned weights. Although, this gain starts to
plateau beyond a certain width. For both MPT-1/32 and MPT-1/1, as the width and depth increase
the performance at different pruning rates approach the same value. This observed phenomenon
yields a more significant gain in the performance for MPTs with higher pruning rates. Similar to the
previous experiment, the performance of MPT-1/32 matches (or exceeds) the performance of dense
models for a large range of pruning rates. Furthermore, in the high width regime, a large number of
weights (〜90%) can be pruned without having a noticeable impact on the performance of MPTs.
We also notice that the performance gap between MPT-1/32 and MPT-1/1 decreases significantly
with an increase the width which is in sharp contrast with the with the depth experiments where the
performance gap between MPT-1/32 and MPT-1/1 appeared to be largely independent of the depth.
7
Published as a conference paper at ICLR 2021
Key Takeaways. Our experiments verify Multi-Prize Lottery Ticket Hypothesis and additionally
convey the significance of choosing appropriate network depth and layer width for a given pruning
rate. In particular, we find that a network with a large width can be pruned more aggressively without
sacrificing much accuracy, while the accuracy of a network with smaller widths suffers when pruning
a large percentage of the weights. Similar patterns hold for the depth of the networks as well. The
amount of overparametrization needed to approach the performance of dense networks seems to
differ for MPT variants - MPT-1/1 requires higher depth and width compared to MPT-1/32.
3.2	How Redundant Are State-of-the-Art Deep Neural Networks ?
Having shown that MPTs can perform equally good (or better) than overparameterized networks,
this experiment aims to answer: Are state-of-the-art weight-trained DNNs overparametrized enough
that significantly smaller multi-prize tickets can match (or beat) their performance?
Experimental Configuration. Instead of focusing on extremely large DNNs, we experiment with
small to moderate size DNNs. Specifically, we analyze the redundancy of following backbone
models: (1) VGG-Small and ResNet-18 on CIFAR-10, and (2) WideResNet-34 and WideResNet-
50 on ImageNet. As we will show later that even these models are highly redundant, thus, our
finding automatically extends to larger models. In this process, we also perform a comprehensive
comparison of the performance of our multi-prize winning tickets with state-of-the-art in binary
neural networks (BNNs). Details on the experimental configuration are provided in Appendix A.
This experiment uses Algorithm 1 to find MPTs within randomly initialized backbone networks.
We compare the Top-1 accuracy and number of non-zero parameters for our MPT-1/32 and MPT-
1/1 tickets with selected baselines in BNNs (Qin et al., 2020a). Results for CIFAR-10 and ImageNet
are shown in Tables 1, 2 and Tables 3, 4, respectively. Next to each MPT method we include the
percentage of weights pruned in parentheses. Motivated by (Frankle et al., 2020), we also include
models in which the BatchNorm parameters are learned when identifying the random subnetwork
using biprop, indicated by +BN. A more comprehensive comparison can be found in Appendix D.
Method	Model	Top-1	Params
BinaryConnect	VGG-Small	91.7	4.6M
ProxQuant	ResNet-56	92.3	0.85M
DSQ		ResNet-20	90.2	0.27M
IR-Net		ResNet-20	90.8	0.27M
Full-Precision	ResNet-18	93.02	11.2M
MPT (80)	-ResNet-18	94.66	-2.2M
MPT(80)+BN	ResNet-18	94.8	2.2M'
Table 1: Comparison of MPT-1/32 with
trained binary-1/32 networks on CIFAR-10.
Method	Model	Top-1	Params
BNN	VGG-Small	89.9	4.6M
XNOR-Net		VGG-Small	89.8	4.6M
DSQ		VGG-Small	91.7	4.6M
IR-Net		ResNet-18	91.5	11.2M
Full-Precision	VGG-Small	93.6	4.6 M
MPT (75)	VGG-Small	88.52	1.44M
MPT(75)+BN	VGG-Small	91.9	1.44M
Table 2: Comparison of MPT-1/1 with
trained binary-1/1 networks on CIFAR-10.
Method	Model	Top-1	Params
ABC-Net	ResNet-18	62.8	11.2M
BWN		ResNet-18	60.8	11.2M
IR-Net		ResNet-34	70.4	21.8M
Quant-Net	ResNet-50	72.8	25.6M'
Full-Precision	ResNet-34	73.27	21.8M
MPT (80)	-WRN-50	72.67	13.7M
MPT(80)+BN	WRN-50	74.03	13.7M'
Table 3: Comparison of MPT-1/32 with
trained binary-1/32 networks on ImageNet.
Method	Model	Top-1	Params
BNN	AlexNet	27.9	62.3M
XNOR-Net		AlexNet	一…44.2	62.3M
ABC-Net		ResNet-34	52.4 "	21.8M
IR-Net		ResNet-34	62.9	21.8M-
Full-Precision	ResNet-34	73.27	21.8M
MPT (60)	-WRN-34	45.06	19.3M
MpT(60)+BN	WRN-34	52.07	19.3M-
Table 4: Comparison of MPT-1/1 with
trained binary-1/1 networks on ImageNet.
Our results highlight that SOTA DNN models are extremely redundant. For similar parameter count,
our binary MPT-1/32 models outperform even full-precision models with learned weights. When
compared to state-of-the-art in BNNs, with minimal hyperparameter tuning our multi-prize tick-
ets achieve comparable (or higher) Top-1 accuracy. Specifically, our MPT-1/32 outperform trained
8
Published as a conference paper at ICLR 2021
binary weight networks on CIFAR-10 and ImageNet and our MPT-1/1 outperforms trained binary
weight and activation networks on CIFAR-10. Further, on CIFAR-10 and ImageNet, MPT-1/32 net-
works with significantly reduced parameter counts outperform dense and full precision networks
with learned weights. Searches for MPT-1/1 in BNN-specific architectures (Kim et al., 2020; Bulat
et al., 2020a) and adopting other commonly used tricks to improve model & representation capac-
ities (Bulat et al., 2020b; Yang et al., 2020; Lin et al., 2020; 2021) are likely to yield MPT-1/1
networks with improved performance. For example, up to a 7% gain in the MPT-1/1 accuracy
was achieved by simply allowing BatchNorm parameters to be updated. Additionally, alternative
approaches for updating the pruning mask in biprop could alleviate issues with back-propagating
gradients through binary activation networks.
4	Discussion and Implications
Existing compression approaches (e.g., pruning and binarization) typically rely on some form of
weight-training. This paper showed that a sufficiently overparametrized randomly weighted network
contains binary subnetworks that achieve high accuracy (comparable to dense and full precision
original network with learned weights) without any training. We referred to this finding as the
Multi-Prize Lottery Ticket Hypothesis. We also proved the existence of such winning tickets and
presented a generic procedure to find them. Our comparison with state-of-the-art neural networks
corroborated our hypothesis. With minimal hyperparameter tuning, our binary weight multi-prize
tickets outperformed current state-of-the-art in BNNs and proved its practical importance. Our work
has several important practical and theoretical implications.
Algorithmic. Our biprop framework enjoys certain advantages over traditional weight-
optimization. First, contemporary experience suggests that sparse BNN training from scratch
is challenging. Both sparseness and binarization bring their own challenges for gradient-based
weight training - getting stuck at bad local minima in the sparse regime, incompatibility of back-
propagation due to discontinuity in activation function, etc. Although we used gradient-based ap-
proaches in this paper, biprop is flexible to accommodate different class of algorithms that might
avoid the pitfalls of gradient-based weight training. Next, in contrast to weight-optimization that re-
quires large model size and massive compute resources to achieve high performance, our hypothesis
suggests that one can achieve similar performance without ever training the large model. Therefore,
strategies such as fast ticket search (You et al., 2019) or forward ticket selection (Ye et al., 2020) can
be developed to enable more efficient ways of finding-or even designing-MPTs. Finally, as opposed
to weight-optimization, biprop by design achieves compact yet accurate models.
Theoretical. MPTs achieve similar performance as the model with learned weights. First, this
observation notes the benefit of overparameterization in the neural network learning and reinforces
the idea that an important task of gradient descent (and learning in general) may be to effectively
compress overparametrized models to find multi-prize tickets. Next, our results highlight the expres-
sive power of MPTs - since we showed that compressed subnetworks can approximate any target
neural network who are known to be universal approximators, our MPTs are also universal approxi-
mators. Finally, the multi-prize lottery ticket hypothesis also uncovers the generalization properties
of DNNs. Generalization theory for DL is still in its infancy and its not clear what and how DNNs
learn (Neyshabur et al., 2017). Multi-prize lottery ticket hypothesis may serve as a valuable tool for
answering such questions as it indicates the dependence of generalization on the compressiblity.
Practical. Huge storage and heavy computation requirements of state-of-the-art deep neural net-
works inevitably limit their applications in practice. Multi-prize tickets are significantly lighter,
faster, and efficient while maintaining performance. This unlocks a range of potential applications
DL could be applied to (e.g., applications with resource-constrained devices such as mobile phones,
embedded devices, etc.). Our results also indicate that existing SOTA models might be spending far
more compute and power than is needed to achieve a certain performance. In other words, SOTA
DL models have terrible energy efficiency and significant carbon footprint (Strubell et al., 2019). In
this regard, MPTs have the potential to enable environmentally friendly artificial intelligence.
9
Published as a conference paper at ICLR 2021
References
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. In International Conference on Learning Representations, 2018.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks. arXiv
preprint arXiv:1909.13863, 2019.
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Bats: Binary architecture search. arXiv
preprint arXiv:2003.01711, 2020a.
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. High-capacity expert binary networks.
arXiv preprint arXiv:2010.03558, 2020b.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems,pp. 3123-3131, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2019. URL https://
openreview.net/forum?id=rJl-b3RcF7.
Jonathan Frankle, David J. Schwab, and Ari S. Morcos. Training batchnorm and only batchnorm:
On the expressive power of random features in cnns, 2020.
Adam Gaier and David Ha. Weight agnostic neural networks. In Advances in Neural Information
Processing Systems, pp. 5364-5378, 2019.
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and
Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 4852-4861, 2019.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David Doer-
mann. Projection convolutional neural networks for 1-bit cnns via discrete back propagation. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 8344-8351, 2019.
Masafumi Hagiwara. Removal of hidden units and weights for back propagation networks. In
Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan),
volume 1, pp. 351-354. IEEE, 1993.
Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu. Training binary neural
networks through learning with noisy supervision. arXiv preprint arXiv:2010.04871, 2020.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
10
Published as a conference paper at ICLR 2021
Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. arXiv
preprint arXiv:1611.01600, 2016.
Siddharth Joshi and Stephen Boyd. Sensor selection via convex optimization. Trans. Sig. Proc., 57
(2):451-462,FebrUary2009.ISSN 1053-587X. doi:10.1109/TSP.2008.2007095. URL https:
//doi.org/10.1109/TSP.2008.2007095.
DahyUn Kim, KUnal Pratap Singh, and JonghyUn Choi. Learning architectUres for binary networks.
In European Conference on Computer Vision, pp. 575-591. Springer, 2020.
Alex Krizhevsky et al. Learning mUltiple layers of featUres from tiny images. 2009.
Yann LeCUn, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network prUning
based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.
Mingbao Lin, Rongrong Ji, Zihan XU, Baochang Zhang, Yan Wang, Yongjian WU, FeiyUe HUang,
and Chia-Wen Lin. Rotated binary neUral network. arXiv preprint arXiv:2009.13055, 2020.
Mingbao Lin, Rongrong Ji, Zihan XU, Baochang Zhang, Fei Chao, Mingliang XU, Chia-
Wen Lin, and Ling Shao. Siman: Sign-to-magnitUde network binarization. arXiv preprint
arXiv:2102.07981, 2021.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accUrate binary convolUtional neUral network. In
Advances in Neural Information Processing Systems, pp. 345-353, 2017.
ZechUn LiU, BaoyUan WU, Wenhan LUo, Xin Yang, Wei LiU, and Kwang-Ting Cheng. Bi-real net:
Enhancing the performance of 1-bit cnns with improved representational capability and advanced
training algorithm. In Proceedings of the European conference on computer vision (ECCV), pp.
722-737, 2018a.
ZhUang LiU, Mingjie SUn, TinghUi ZhoU, Gao HUang, and Trevor Darrell. Rethinking the valUe of
network prUning. arXiv preprint arXiv:1810.05270, 2018b.
Eran Malach, Gilad YehUdai, Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket
hypothesis: PrUning is all yoU need. arXiv preprint arXiv:2002.00585, 2020.
Brais Martinez, Jing Yang, Adrian BUlat, and Georgios TzimiropoUlos. Training binary neUral
networks with real-to-binary convolUtions. arXiv preprint arXiv:2003.11535, 2020.
James O’ Neill. An overview of neUral network compression. arXiv preprint arXiv:2006.03669,
2020.
Behnam NeyshabUr, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in neural information processing systems, pp. 5947-5956,
2017.
LaUrent OrseaU, MarcUs HUtter, and Omar Rivasplata. Logarithmic prUning is all yoU need. Ad-
vances in Neural Information Processing Systems, 33, 2020.
Ankit Pensia, Shashank RajpUt, Alliot Nagle, Harit Vishwakarma, and Dimitris PapailiopoUlos.
Optimal lottery tickets via sUbsetsUm: Logarithmic over-parameterization is sUfficient, 2020.
Haotong Qin, RUihao Gong, Xianglong LiU, Xiao Bai, JingkUan Song, and NicU Sebe. Binary neUral
networks: A sUrvey. Pattern Recognition, pp. 107281, 2020a.
Haotong Qin, RUihao Gong, Xianglong LiU, MingzhU Shen, Ziran Wei, Fengwei YU, and JingkUan
Song. Forward and backward information retention for accUrate binary neUral networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250-
2259, 2020b.
11
Published as a conference paper at ICLR 2021
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Raste-
gari. What’s hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp.11893-11902, 2020.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European conference on computer
vision, pp. 525-542. Springer, 2016.
Franco Scarselli and Ah Chung Tsoi. Universal approximation using feedforward neural networks:
A survey of some existing methods, and some new results. Neural Netw., 11(1):15-37, January
1998. ISSN 0893-6080. doi: 10.1016/S0893-6080(97)00097-X. URL https://doi.org/
10.1016/S0893-6080(97)00097-X.
Mingzhu Shen, Xianglong Liu, Ruihao Gong, and Kai Han. Balanced binary neural networks with
gated residual. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 4197-4201. IEEE, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv 1409.1556, 09 2014.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep
learning in nlp. arXiv preprint arXiv:1906.02243, 2019.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. arXiv preprint arXiv:2002.07376, 2020a.
Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu. Pruning
from scratch. In AAAI, pp. 12273-12280, 2020b.
Andreas S Weigend, David E Rumelhart, and Bernardo A Huberman. Generalization by weight-
elimination with application to forecasting. In Advances in neural information processing systems,
pp. 875-882, 1991.
Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Discovering neural wirings. In Ad-
vances in Neural Information Processing Systems, pp. 2684-2694, 2019.
Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring randomly wired neu-
ral networks for image recognition. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1284-1293, 2019.
Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and Chang Xu. Search-
ing for low-bit weights in quantized neural networks. arXiv preprint arXiv:2009.08695, 2020.
Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnet-
works provably exist: Pruning via greedy forward selection. arXiv preprint arXiv:2003.01794,
2020.
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk,
Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training
of deep networks. arXiv preprint arXiv:1909.11957, 2019.
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for
highly accurate and compact deep neural networks. In Proceedings of the European conference
on computer vision (ECCV), pp. 365-382, 2018.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros,
signs, and the supermask. In Advances in Neural Information Processing Systems, pp. 3597-3607,
2019.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
12
Published as a conference paper at ICLR 2021
Acknowledgements
The authors would like to thank Shreya Chaganti for her valuable contributions to the biprop open
source code development and for her help on training MPT models for the final version of the paper.
This work was performed under the auspices of the U.S. Department of Energy by the Lawrence
Livermore National Laboratory under Contract No. DE-AC52-07NA27344, Lawrence Livermore
National Security, LLC. This document was prepared as an account of the work sponsored by an
agency of the United States Government. Neither the United States Government nor Lawrence
Livermore National Security, LLC, nor any of their employees makes any warranty, expressed or
implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness
of any information, apparatus, product, or process disclosed, or represents that its use would not
infringe privately owned rights. Reference herein to any specific commercial product, process, or
service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or
imply its endorsement, recommendation, or favoring by the United States Government or Lawrence
Livermore National Security, LLC. The views and opinions of the authors expressed herein do not
necessarily state or reflect those of the United States Government or Lawrence Livermore National
Security, LLC, and shall not be used for advertising or product endorsement purposes. This work
was supported by LLNL Laboratory Directed Research and Development project 20-ER-014 and
released with LLNL tracking number LLNL-CONF-815432.
13
Published as a conference paper at ICLR 2021
A Hyperparameter Configurations
A. 1 Hyperparameters for Section 3.1
Experimental Configuration. For MPT-1/32 tickets, the network structure is not modified from
the original. For MPT-1/1 tickets, the network structure is modified by moving the max-pooling
layer directly after the convolution layer and adding a batch-normalization layer before the binary
activation function, as is common in many BNN architectures (Rastegari et al., 2016). We choose our
baselines as dense full precision models with learned weights. The baselines were obtained by train-
ing backbone networks using the Adam optimizer with learning rate of 0.0003 for 100 epochs and
with a batch size of 60. In each randomly weighted backbone network, we find winning tickets MPT-
1/32 and MPT-1/1 for different pruning rates using Algorithm 1. For both the weight-optimized and
MPT networks, the weights are initialized using the Kaiming Normal distribution (He et al., 2015).
All training routines make use of a cosine decay learning rate policy.
Method	Model	Optimizer	LR	Momentum	Weight Decay	Batch	Epochs
MPT-1/32	Conv2/4/6/8	SGD	0.1	0.9	1e-4	128	250
MPT-1/1	Conv2/4/6/8	Adam	0.1	-		1e-4	…128		250
Table 5: Hyperparameter Configurations for CIFAR-10 Experiments
A.2 Hyperparameters for Section 3.2
In these experiments, the weights are initialized using the Kaiming Normal distribution (He et al.,
2015) for all the models except for MPT-1/32 on ImageNet where we use the Signed Constant
initialization (Ramanujan et al., 2020) as it yielded slightly better performance. All training routines
make use of a cosine decay learning rate policy. For ImageNet training we used a label smoothing
value of 0.1 and a learning rate warmup length of 5 epochs.
Method	Model	Opt.	LR	Momentum	Weight Decay	Batch Epochs
MPT-1/32	ResNet-18	SGD	0.1	0.9	5e-4	256	250
MPT1/32+BN....ResNet-18...SGD.....0.1........0.9.........5e-4............256.250
MPT-1/1.......VGG-Small	Adam	3.63e-3..........-.......17.335............128.600
MPT-1/1+BN....VGG-Small	Adam	3.63e-3..........-.........1e-4............128.600
Table 6: Hyperparameter Configurations for CIFAR-10 Experiments
Method	Model	Optimizer	LR	Momentum	Weight Decay	Batch	Epochs
MPT-1/32	WRN-50	SGD	0.256	0.875	3.051757812e-5	256	120
MPT-1/32+BN	WRN-50		SGD	0.256		0.875	3.051757812e-5	256		120
MPT-1/1		WRN-34	Adam	2.56e-4	-	3.051757812e-5	256		250
MPT-1/1+BN	WRN-34	Adam	2.56e-4	-	3.051757812e-5	256		250
Table 7: Hyperparameter Configurations for ImageNet Experiments
B Existence of B inary-Weight Subnetwork Approximating
Target Network
In the following analysis, note that we write Bin({-1, +1}m×n) to denote matrices of dimension
m × n whose components are independently sampled from a binomial distribution with elements
{-1, +1} and probability p = 1/2.
Lemma 1. Let S ∈ [d], a ∈ [—√1s,表],i ∈ [d], and ε,δ ≥ 0 be given. Let B ∈ { — 1, +1}k×d
be chosen randomly from Bin({-1, 1}k×d) and u ∈ {-1, +1}k be chosen randomly from
14
Published as a conference paper at ICLR 2021
Bin({-1,+1}k).If
k ≥ T + 16l0g(2)，	(3)
εs	δ
then with probability at least 1 一 δ there exist masks m ∈ {0,1}k and M ∈ {0,1}k×d such that
the function g : Rd → R defined by
g(x) = (m Θ u)| σ (ε(M Θ B)X),	(4)
satisfies
|g(x) — αxi∣ ≤ ε,	(5)
for all kxk∞ ≤ 1. Furthermore, krh∣∣o = ∣∣M∣∣o ≤ ε√s, and maxι≤j≤k ∣∣Mj,J∣o ≤ 1.
Proof. If ∣α∣ ≤ ε then taking M = 0 yields the desired result. Suppose that ∣ɑ∣ > ε. Then there
exists a ci ∈ N such that
Ciε ≤ |a| ≤ (Ci + 1)ε and 陵£ — ∣α∣∣ ≤ ε.	(6)
Hence, it follows that
∣ciεsign(α)xi — αx∕ = ∣Xi∣∣ciε — |a|| ≤ ε,
(7)
where the final inequality follows from (6) and the hypothesis that ∣x∣∞ ≤ 1. Our goal now is to
show that with probability 1 — δ the random initialization of U and B yield masks m and M such
that g(x) = Ciε sign(α)xi.
Now fix i ∈ [d] and take k0 = 2. First, We consider the probability
P (|{j ∈ [k0]: Uj = +1 and Bji = sign(α)}∣ < Ci).
(8)
As u and B:,i are each sampled from a binomial distribution with k0 trials, the distribution that the
pair (uj, Bj,i) is sampled from is a multinomial distribution with four possible events each having a
probability of 1/4. Since we are only interested in the event (uj, Bj,i) = (+1, sign(α)) occurring,
We can instead consider a binomial distribution where P((Uj,Bj,i) = (+1, sign(α)) = 4 and
P((Uj, Bj,i) = (+1, sign(α)) = 3. Hence, using HOeffding's inequality we have that
P (|{j ∈ [k0] : Uj = +1 and Bj,i
sign(α)}∣ < Ci) ≤ exp —2k0
eχp (-1 k0 + Ci- 210
< exp
- 8 k0+
(9)
(10)
(11)
where the final inequality follows since exp() is an increasing function and —2ci0 < 0. From (6)
and the fact that ∣α∣ ≤ √s, it follows that
Ci ≤ ε√s'	(12)
Combining our hypothesis in (3) with (12) yields that
一ɪk0+Ci=一ɪk+Ci ≤—ɪ (√=+16log O) +- -√=log O.	(13)
8	16	16 ε s	δ ε s	2
Substituting (13) into (11) yields
P(∣{j ∈ [k0]: Uj = +1 and Bj,i = sign(α)}∣ < Ci) < 2.	(14)
15
Published as a conference paper at ICLR 2021
Additionally, it follows from the same argument that
P (∣{k0 <j ≤ k : Uj = -1 and Bj,i = — sign(α)}∣ < Ci) <《.	(15)
From (14) and (15) it follows with probability at least 1 - δ that there exist sets S+ := {j :
uj = +1 and Bj,i = sign(α)} and S- := {j : uj = -1 and Bj,i = - sign(α)} satisfying
|S+1 = |S-| = Ci and S+ ∩ S- = 0. Using these sets, We define the components of the mask m
and M by
rm. = (1	: j ∈s+ ∪S-
mj =	0 : otherWise
and
M	1 : j ∈ S+ ∪S- and`= i
j,` =	0 : otherWise	.
Using the definition of g(x) in (4) We noW have that
g(x) =	σ (ε sign(α)xi) -	σ (-ε sign(α)xi)
= Ciσ (ε sign(α)xi) - Ciσ (-ε sign(α)xi)
= Ciε sign(α)xi,
(16)
(17)
(18)
(19)
(20)
Where the final equality folloWs from the identity σ(a) - σ(-a) = a, for all a ∈ R. This concludes
the proof of (5).
Lastly, by our choice of m in (16), M in (17), and (12), it follows that
2
kmko = kMko = 2ci ≤ —F
εs
and
max kMj,:k0 ≤ 1,
1≤j≤k
which concludes the proof.
(21)
(22)
□
The next step is to consider an analogue for Lemma A.2 from (Malach et al., 2020) which we provide
in Lemma 2.
Lemma 2. Let S ∈ [d], w* ∈ [— √1s,表] with ∣∣w*ko ≤ s, and ε,δ > 0 be given. Let B ∈
{-1, +1}k×d be chosen randomly from B in({-1, 1}k×d) andu ∈ {-1, +1}k be chosen randomly
from Bin({—1, +1}k). If
k ≥ s ∙ 16—f+ + 16log(2s) ,	(23)
then with probability at least 1 — δ there exist masks rh ∈ {0,1}k and M ∈ {0,1}k×d such that
the function g : Rd → R defined by
g(x) = (m Θ u)| σ (ε(M Θ B)X),	(24)
satisfies
|g(x) — hw*, xi| ≤ ε, for all ∣x∣∞ ≤ 1.	(25)
Furthermore, ∣rm∣∣o = ∣∣M∣∣o ≤ 2s√s and maχ1<7-≤k ||Mj,： ∣o ≤ 1.
Proof. Assume k
乎 + 16log (2S)]
s ∙ 116ε√s + 16log (2S)] and set k0
Note that if k > S ∙
then the excess neurons can be masked yielding the desired value for k. We
k
S
16
Published as a conference paper at ICLR 2021
decompose u, m, B, and M into S equal size submatrices by defining
u(i) :=	uk0(i-1) + 1	∙ ∙ ∙	uk0i| ∈ {一1, +1}k ×1	(26)
m⑻：=I	m k0(i-1) + 1	.一	m H1 ∈{0, i}k0×1	(27)
	b(k0(i-1)+1),1	• ∙	b(k0(i-1)+1),d	
B(i) :=	. . .	...	.	∈ {-1, +I}k0×d	(28)
	bk0i,1	• • •	bk0 i,d	
	m(k0(i-1)+1),1	• • •	m(k0(i-1)+1),d	
M(i) :=	. . .	...	.	∈ {0,1}k0×d,	(29)
	mk0i,1	• • •	mk0i,d	
for i ∈ [s]. Note that these submatrices satisfy
u=	〃⑴一 . . .	,m =	% (I)- . . .	,B =	-B(!)- . . .	,M =	-M (I)- . . .	.	(30)
	u(S)		m(S)		B(S)		M(S)	
Now let I := {i ∈ [d] : w* = 0}. By our hypothesis that ∣∣w*∣∣o ≤ s, it follows that |I| ≤ s.
WLOG, assume that I ⊆ [s]. Now fix i ∈ [s] and define gi : Rd → R by
gi(x) := (m(i) Θ u(i)) σ (ε(M(i) Θ B(i))x)	(31)
By (23), taking ε0 = S and δ0 = S yields that k0 ≥ ε0√6s + 16log (专).Hence, it follows from
Lemma 1 that with probability at least 1 一 δ0 there exist m⑶ ∈ {0, ι}k0 and M(i) ∈ {0, i}k0×d
such that
ε
|gi(x) - WiXiI ≤ ε = -,	(32)
s
for every x ∈ Rd with kxk∞ ≤ 1, and
km(i)ko = kM叫0 ≤ -√= = 2√s and maχ	kM^ko ≤ L (33)
ε0 s ε	k0(i-1)+1≤j≤k0i
By the definition of g(x) in (24), using (30) yields
SS
g(x) = (m Θ u)| σ (ε(M Θ B)X)= X (m⑺ Θ u(i))| σ (ε(M⑴ Θ B(i))x) = X gi(x).
i=1	i=1
(34)
Hence, combining (32) for all i ∈ [s], it follows that with probability at least 1 一 δ we have
S	S S
|g(x) 一 hw*,xi| = Xgi(x) 一 Xwi*xi ≤ X |gi(x) 一 wi*xi| ≤ ε.	(35)
i=1	i=1	i=1
Finally, it follows from (30) and (33) that
krnko = ∣∣M∣∣o ≤ 2s^s and max ∣∣Mj,J∣o ≤ 1,	(36)
ε	1≤j≤k
which concludes the proof.	□
We now state and prove an analogue to Lemma A.5 in (Malach et al., 2020) which is the last lemma
we will need to establish the desired result.
17
Published as a conference paper at ICLR 2021
n×d
[-√s, √s]	with kW*k0
Lemma 3. Let S ∈ [d], W * ∈
≤ s, F : Rd → Rn defined by
Fi (x) = σ(hwi*, xi), and ε, δ > 0 be given. Let B ∈ {-1, +1}k×d be chosen randomly from
B in({-1, 1}k×d) and U ∈ {-1, +1}k×n be chosen randomly from Bin({-1, +1}k×n). If
1	「16 √ns	1	2 2ns
k ≥ ns ∙ —ε------+ 16 log ( ~δ~ ) ，	(37)
then with probability at least 1 一 δ there exist masks MM ∈ {0,1}k×n and M ∈ {0,1}k×fd such that
the function G : Rd → Rn defined by
G(X)= σ ((M Θ U)Tσ (ε(M Θ B)X)) ,	(38)
satisfies
kG(X) 一 F (X)k2 ≤ ε, for all kXk∞ ≤ 1.	(39)
Furthermore, kMM∣∣o = ∣∣M∣∣o ≤ 2nsε√ns.
Proof. Assume k = ns ∙ [16√ns + 16log (2ns)] and set k0 = nk. Note that if k > ns ∙
[16√ns + 16log (孥)]
then excess neurons can be masked to yield the desired value for k .
As in the proof of Lemma 2, we can split U, M, B, and M into n submatrices, denoted
U⑴ ∈ {-1, +1}k0×n, M⑴ ∈ {-1, +1}k0×n, B⑴ ∈ {-1,+1}k0×d, and M⑺ ∈ {-1,+1}k0×d
for i ∈ [n], such that
-U⑴-
U =	.
.
U(n)
-M⑴一
M =	.
.
M⑺
-B0)-
B =	:
.
B(n)
-M⑴一
and M =	.
.
M(n)
(40)
To simplify notation in the following definition, We define the vectors rn(i) := M(ii) and u(i):
U'?. Now we define the functions gi : Rd → R by
gi(x) = (m(i) Θ u(i)) σ (β(M(i) Θ B(i))x),
(41)
for each i ∈ [n]. Taking ε0 = √εn and δ0 = δ, it follows from (37) that k0 ≥ S ∙
116ε√s + 16 log (δs)]. As the hypotheses of Lemma 2 are satisfied, with probability at least 1 一 δ
there exist masks m(i) and M(i) with
∣m(i)ko = ∣m(i)ko
V 2s√S	2s√nS
ε0
(42)
ε
such that
ε
∣gi(x) - hWi , xi∣ ≤ 7, forall ∣x∣∞ ≤ 1.
(43)
For each i ∈ [n], note that this results in choosing the columns of the mask M(i) by
M)={m (i)
' =i
otherwise
(44)
Combining this choice with (40) yields
g1(X)
(M Θ U)lσ (β(M Θ B)X)
(45)
gn(X)
18
Published as a conference paper at ICLR 2021
By the definition of G(x) in (38), it follows from (45) that
^σ(g1(x))^
G(X) =	.	.	(46)
σ(gn(x))
Combining (43) and (46), we have with probability at least 1 - δ that
nn
kG(x) - F(x)k2 = X (σ(gi(x))-σ(hwi, x〉))2 ≤ X (依⑺-(用*, Xli) ≤ ε2.	(47)
i=1	i=1
Finally, it follows from (42) and (44) that
kMMko = kMko ≤ 2ns√ns	(48)
ε
which concludes the proof.	□
We are now ready to prove the main result in Theorem 2.
Theorem 2. Let `, n, s ∈
and W(')* ∈ h-√, √i
hd×n	n×n
√, √i	, {W(i)*}'-1 ∈ 卜√n, √ni	,
1×n	(i)
.Assume that for each i ∈ ['] we have ∣∣W(i)*k) ≤ 1 and
maxj kWj'i)*ko ≤ S. Define F(x) := F(') ◦…。F⑴(x) where F(i)(x) = σ(W(i)*x) for
i ∈ [' — 1] and F(')(x) = W(')*x. Fix ε,δ ∈ (0,1).
Let B⑴ ∈ {-1,+1}k×d be sampled from Bin({-1,+1}k×d), {B(i)}'=) ∈ {-1,+1}k×n
be SamPled from Biη({-1, +1}k×n), {U(i)}'-1 ∈ { —1,+1}k×n be SamPled from
Bin({-1, +1}k×n) and U(') ∈ { —1, +1}k×1 Sampledfrom Bin({-1, +1}k×1). If
k ≥ ns ∙
二” + 16log(2ns'
(49)
then with probability at least 1 — δ there exist binary masks {M(i)}'=ι and {MM(i)}'=ι for
{B(i)}'=ι and {U(i)}'=ι, respectively, such that thefunction G : Rd → R defined by
G(x) := G(') ◦.・.◦ G(1)(x),	(50)
where
G(i)(x) := σ ((M(i) Θ U(i))lσ(ε(M⑻ Θ B(i))x)) , for i ∈ [' — 1]	(51)
G(')(x) := (M⑻ Θ U(i))lσ(ε(M⑻ Θ B⑴)x),	(52)
satisfies
|G(x) — F (x)| ≤ ε, for all ∣x∣2.	(53)
Additionally, ∣mM∣∣o = ∣∣M∣∣o ≤ 4ns'ε√ns.
Proof. Let i ∈ [' — 1]. Using Lemma 3 with ε0 =克 and δ0 = ', with probability at least 1 — '
there exist M(i) and M(i) such that
∣G(i)(x) — F(i)(x)∣2 ≤ 2', for all ∣x∣∞ ≤ 1	(54)
and
kM(i)∣o = ∣M(i)ko ≤ 2nS√ns = 4nS'√ns.	(55)
The remainder of the proof follows from applying the same argument as in the proof of Theorem A.6
from (Malach et al., 2020).	□
19
Published as a conference paper at ICLR 2021
C Motivation for Framework to Identify MPTs
Suppose that f (x; W *) with optimized weights W * is a target network that We wish to approx-
imate. Let g(x; W) denote the network in which we want to identify a MPT-1/32 that is an ε-
approximation of f(x; W*), for some ε > 0.
Now assume that g(x; ∙) is Lipschitz continuous with constant κ, B ∈ {-1, +1}m are binary
parameters for g , and α ∈ R is gain term. It follows that
kg (x; α(M	B)) - f(x;W*)k ≤ kg (x; α(M	B) -g(x;M B)k
+kg(x;MW)-f(x;W*)k
<κk(MW)-α(MB)k
+kg(x;MW)-f(x;W*)k.	(56)
If we take M to be a fixed binary mask, we can minimize the error of binarizing the subnetwork
parameters M W by solving the optimization problem
min k(MW) -α(M B)k2
α,B	(57)
s.t. α ∈ R, B ∈ {-1, 1}n
where M, W, and B are stacked into vectors of some length, say n. As the pruning mask M is
applied to both W and B, solving problem (57) is equivalent to solving problem (2) in (Rastegari
et al., 2016) with a different dimension. Hence, it immediately follows that one closed form solution
for B in problem (57) is
B* = sign(W).	(58)
Taking the derivative of the cost function in (57) with respect to α and setting it equal to zero yields
α(M	B*)|(M B*) - (MW)|(MB*) = 0.
Recalling that M ∈ {0, 1}n and using (58), we have
nn	n
(M	B*)|(M	B*) =X(MiBi*)2 =XMi2(sign(Wi))2 =XMi = kMk1
i=1	i=1	i=1
and
nn
(M	W)|(M	B*) = XMi2Wisign(Wi) = XMi|Wi| = kM	Wk1.
i=1	i=1
Substituting (60) and (61) into (59) and solving for α yields the closed form solution
*	kMWk1
α -----------
kM kι .
Hence, α* and B * minimize the right hand side of (56) and, consequently, reduce the approximation
error of the MPT-1/32. So when the binarization error, k(M W) - α(M sign(W))k, and the
subnetwork error, kg(x; MW)-f(x; W*)k, are sufficiently small then the binarized subnetwork
g (x; α(M sign(W))) serves as a good approximation to the target network.
These closed form expressions for the gain term and the binarized weights are the updates used for
the gain term and binary subnetwork weights in biprop after updating the binary pruning mask.
D Comparison of MPTs with binary neural network SOTA
Here We provide a more exhaustive comparison of MPT-1/32 and MPT-1/1 on CIFAR-10 and Im-
ageNet to SOTA methods - BinaryConnect (Courbariaux et al., 2015), BNN (Courbariaux et al.,
2016), DoReFa-Net (Zhou et al., 2016), LQ-Nets (Zhang et al., 2018), BWN and XNOR-Net (Raste-
gari et al., 2016), ABC-Net (Lin et al., 2017), IR-Net (Qin et al., 2020b), LAB (Hou et al., 2016),
ProxQuant (Bai et al., 2018), DSQ (Gong et al., 2019), and BBG (Shen et al., 2020). Results for
CIFAR-10 can be found in Tables 8 and9 and results for ImageNet can be found in Tables 10 and 11.
Next to the MPT method we include the percentage of weights pruned and the layer width multiplier
(if larger than 1) in parentheses.
(59)
(60)
(61)
(62)
20
Published as a conference paper at ICLR 2021
Method	Model	Top-1	Params
BinaryConnect	VGG-Small	91.7	4.6M
BWN		VGG-Small	90.1	4.6M'
DoReFa-Net	ResNet-20	90.0	0.27M'
LQ-Nets		ResNet-20	90.1	0.27M'
LAB		VGG-Small	89.5	4.6M
ProxQuant	ResNet-56	92.3	0.85M
DSQ		ResNet-20	90.2	0.27M'
IR-Net		ResNet-20	90.8	0.27M'
Full-Precision	ResNet-18	93.02	…11.2M
MPT-1/32 (95)	VGG-Small	91.48	0.23M
MPT(80)		ResNet-18	94.66	2.2M'
MPT(80)+BN	ResNet-18	94.8	2.2M'
Table 8: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on CIFAR-10
Method	Model	Top-1	Params
BNN	VGG-Small	89.9	4.6M
XNOR-Net		VGG-Small	89.8	4.6M
DoReFa-Net		ResNet-20	79.3	0.27M
BBG		ResNet-20	-85.3	0.27M
LAB		VGG-Small	…87.7	4.6M
DSQ		VGG-Small	91.7	4.6M
IR-Net		ResNet-18	91.5	4.6M
Full-Precision	VGG-Small	93.6	4.6M
MPT(75, 1.25x)	VGG-Small	88.49	1.44M
MPT(75,1.25x)+BN	VGG-Small	91.9	1.44M
Table 9: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on CIFAR-10
Method	Model	Top-1	Params
ABC-Net	ResNet-18	62.8	11.2M
BWN		ResNet-18	60.8	11.2M
BWNH		ResNet-18	64.3	11.2M
PACT		ResNet-18	65.8	11.2M
IR-Net		ResNet-34	70.4	21.8M
Quantization-Networks	ResNet-18	66.5	11.2M
Quantization-Networks	ResNet-50	72.8	25.6M
Full-Precision	ResNet-34	73.27	21.8M
MPT (80)	-WRN-50	72.67	13.7M
MPT(80)+BN		WRN-50	74.03	13.7M-
Table 10: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on ImageNet
E	Comparis on to edgepopup for MPT-1/32
Note that binarization step of biprop can be avoided while finding MPT-1/32 - by initializing (and
pruning) our backbone neural network with binary initialization (e.g., edgepopup with Signed Con-
stant initialization (Ramanujan et al., 2020)). In this specific instance, biprop boils down to edge-
popup with proper scaling. Next, we compare the performance of MPT-1/32 networks identified
using these two approaches. Both networks presented below use the same hyperparameter config-
urations and are trained for 250 epochs on the CIFAR-10 dataset. We initialize the networks iden-
tified with edgepopup using the Signed Constant initialization as it yielded their best performance.
MPT-1/32 networks identified using biprop are initialized using the Kaiming Normal initialization.
We plot the average over three experiments for each pruning percentage and bars extending to the
minimum and maximum accuracy for each pruning percentage. Additionally, for each network we
include the Top-1 accuracy of a dense model with learned weights. These plots can be found in
21
Published as a conference paper at ICLR 2021
Method	Model Top-1 Params
BNN XNOR-Net	 ABC-Net	 ABC-Net	 rΓ0Λλ		AlexNet AlexNet ResNet-18 …ResNet-34…	27.9 ……44.2 ……42.7 ……52.4…	62.3 M …62.3 M ……11.2 M' …21.8 M
TSQ WRPN		AlexNet …ResNet-34…	58.0 60.5	62.3 M …21.8 M
HWGQ		AlexNet	52.7	…62.3 M
IR-Net		ResNet-18	58.1	……11.2 M'
IR-Net		ResNet-34	62.9	21.8 M'
Full-Precision	…ResNet-34…	73.27	…21.8 M
MPT (60) MPT (60) +BN	-WRN-34 ………WRN-34…	45.06 52.07	19.3 M 19.3 M
Table 11: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on ImageNet
Figure 5. We find that the performance of MPT-1/32 identified with biprop outperforms networks
identified using edgepopup. This highlights the benefit of binarization (in conjunction with pruning)
as a learning strategy.
Conv-2	Conv-4	Conv-6	Conv-8
50 55 60 65 70 75 80 85 90	50 55 60 65 70 75 80 85 90	50 55 60 65 70 75 80 85 90	50 55 60 65 70 75 80 85 90
Percentage of Weights Pruned	Percentage of Weights Pruned	Percentage of Weights Pruned	Percentage of Weights Pruned
I MPT-1/32 (Untrained)	Edgepopup (Signed Constant) - -∣∙ - Dense (Learned Weights)
Figure 5: Comparing biprop and edgepopup: Comparing the Top-1 accuracy of MPT-1/32 to
binary weight networks of the same size identified using edgepopup on CIFAR-10.
F Related Work
F.1 Pruning
We categorize pruning methods based on whether a model is pruned either after the training or
before the training (see (Neill, 2020) for a comprehensive review).
Post-Training Pruning. The traditional pruning methods leverage a three-stage pipeline - Pre-
training (a large model), pruning, and fine-tuning. The main distinction lies among these approaches
is what type of criteria is used for pruning. One of the most popular approach is the magnitude-based
pruning where the weights with the magnitude below a certain threshold are discarded (Hagiwara,
1993). Further, certain penalty term (e.g., l1, l2 or lasso weight regularization) can be used during
training to encourage a model to learn certain smaller magnitude weights and removing them post-
training (Weigend et al., 1991). Models can also be pruned by measuring the importance of weights
by computing the sensitivity of the loss function when weights are removed and prune those which
cause the smallest change in the loss (LeCun et al., 1990).
Pruning Before Training. Thus far, we have have discussed methods for pruning pretrained
DNNs.
Recently, (Frankle & Carbin, 2019) proposed the Lottery Ticket Hypothesis and showed that
randomly-initialized neural networks contain sparse subnetworks that can be effectively trained
from scratch when reset to their initialization. Further, (Liu et al., 2018b) showed that the training
22
Published as a conference paper at ICLR 2021
an over-parameterized model is often not necessary to obtain an efficient final model and network
architecture itself is more important than the remaining weights after pruning pretrained networks.
These findings has revived interest in finding approaches for searching sparse and trainable subnet-
works. For example, (Lee et al., 2018; Wang et al., 2020b; You et al., 2019; Wang et al., 2020a)
explored efficient approaches to search for these sparse and trainable subnetworks. Along this line
of work, a striking finding was reported by (Zhou et al., 2019; Ramanujan et al., 2020) showing that
randomly-initialized neural networks contain sparse subnetworks that achieve good performance
without any training. (Malach et al., 2020; Pensia et al., 2020) provided theoretical evidences for
this phenomenon and showed that one can approximate any target neural network, by pruning a
sufficiently over-parameterized network of random weights.
F.2 B inarization
Similar to pruning, we categorize binarization methods based on whether a model is binarized either
after the training or during the training (see (Qin et al., 2020a) for a comprehensive review).
Post-Training Binarization. To the best of our knowledge, none of the post-training schemes have
been successful in binarizing pretrained models with or without retraining to achieve reasonable test
accuracy. Most existing works (Han et al., 2015; Zhou et al., 2017) are limited to ternary weight
quantization.
Training-Aware Binarization. There are several efforts to improve the performance of BNN
training. This is a challenging problem as binarization introduces discontinuities which makes dif-
ferentiation during backpropogation difficult. Binaryconnect (Courbariaux et al., 2015) established
how to train networks with binary weights within the familiar back-propagation paradigm. Bina-
ryNet (Courbariaux et al., 2016) further quantize both the weights and the activations to 1-bit values.
Unfortunately, these early schemes resulted in a staggering drop in the accuracy compared to their
full precision counterparts. In an attempt to improve the performance, XNOR-Net (Rastegari et al.,
2016) proposed to add a real-valued channel-wise scaling factor. Dorefa-Net (Zhou et al., 2016) ex-
tends XNOR-Net to accelerate the training process using quantized gradients. ABC-Net (Lin et al.,
2017) improved the performance by using more weight bases and activation bases at the cost of
increase in memory and computation. There have also been efforts in making modifications to the
network architectures to make them amenable for the binary neural network training. For example,
Bireal-Net (Liu et al., 2018a) added layer-wise identity short-cut, and AutoBNN (Shen et al., 2020)
proposed to widen or squeeze the channels in an automatic manner. (Han et al., 2020) proposed
to learn to binarize neurons with noisy supervision. Some efforts also have been carried out to de-
signing gradient estimators extending straight-through estimator (STE) (Bengio et al., 2013) for
accurate gradient back-propagation. DSQ (Gong et al., 2019) used differentiable soft quantization
to have accurate gradients in backward propagation. On the other hand, PCNN Gu et al. (2019)
proposed a new discrete back-propagation via projection algorithm to build BNNs.
F.3 Other Related Directions
Gaier & Ha (2019) proposed a search method for neural network architectures that can already per-
form a task without any explicit weight training, i.e., each weight in the network has the same shared
value. Recent work in randomly wired neural networks (Xie et al., 2019) showed that constructing
neural networks with random graph algorithms often outperforms a manually engineered architec-
ture. As opposed to fixed wirings in (Xie et al., 2019), (Wortsman et al., 2019) learned the network
parameters as well as the structure. This show that finding a good architecture is akin to finding a
sparse subnetwork of the complete graph.
23