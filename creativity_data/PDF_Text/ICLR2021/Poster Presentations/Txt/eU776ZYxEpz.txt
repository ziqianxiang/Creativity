Published as a conference paper at ICLR 2021
Learning to live with Dale’s principle: ANNs
WITH SEPARATE EXCITATORY AND INHIBITORY UNITS
Jonathan Cornford1,2, Damjan Kalajdzievski2,3, Marco Leite4, Amelie Lamarquette5
Dimitri M. Kullmann4, Blake Richards1,2,t
1 McGill University，2MILA, 3Universite de Montreal, 4UCL, 5University of Cambridge
Ab stract
The units in artificial neural networks (ANNs) can be thought of as abstractions
of biological neurons, and ANNs are increasingly used in neuroscience research.
However, there are many important differences between ANN units and real neu-
rons. One of the most notable is the absence of Dale’s principle, which ensures that
biological neurons are either exclusively excitatory or inhibitory. Dale’s principle
is typically left out of ANNs because its inclusion impairs learning. This is prob-
lematic, because one of the great advantages of ANNs for neuroscience research
is their ability to learn complicated, realistic tasks. Here, by taking inspiration
from feedforward inhibitory interneurons in the brain we show that we can develop
ANNs with separate populations of excitatory and inhibitory units that learn just
as well as standard ANNs. We call these networks Dale’s ANNs (DANNs). We
present two insights that enable DANNs to learn well: (1) DANNs are related to
normalization schemes, and can be initialized such that the inhibition centres and
standardizes the excitatory activity, (2) updates to inhibitory neuron parameters
should be scaled using corrections based on the Fisher Information matrix. These
results demonstrate how ANNs that respect Dale’s principle can be built without
sacrificing learning performance, which is important for future work using ANNs
as models of the brain. The results may also have interesting implications for how
inhibitory plasticity in the real brain operates.
1	Introduction
In recent years, artificial neural networks (ANNs) have been increasingly used in neuroscience
research for modelling the brain at the algorithmic and computational level (Richards et al., 2019;
Kietzmann et al., 2018; Yamins & DiCarlo, 2016). They have been used for exploring the structure
of representations in the brain, the learning algorithms of the brain, and the behavioral patterns of
humans and non-human animals (Bartunov et al., 2018; Donhauser & Baillet, 2020; Michaels et al.,
2019; Schrimpf et al., 2018; Yamins et al., 2014; Kell et al., 2018). Evidence shows that the ability
of ANNs to match real neural data depends critically on two factors. First, there is a consistent
correlation between the ability of an ANN to learn well on a task (e.g. image recognition, audio
perception, or motor control) and the extent to which its behavior and learned representations match
real data (Donhauser & Baillet, 2020; Michaels et al., 2019; Schrimpf et al., 2018; Yamins et al.,
2014; Kell et al., 2018). Second, the architecture of an ANN also helps to determine how well it can
match real brain data, and generally, the more realistic the architecture the better the match (Schrimpf
et al., 2018; Kubilius et al., 2019; Nayebi et al., 2018). Given these two factors, it is important for
neuroscientific applications to use ANNs that have as realistic an architecture as possible, but which
also learn well (Richards et al., 2019; Kietzmann et al., 2018; Yamins & DiCarlo, 2016).
Although there are numerous disconnects between ANNs and the architecture of biological neural
circuits, one of the most notable is the lack of adherence to Dale’s principle, which states that a
neuron releases the same fast neurotransmitter at all of its presynaptic terminals (Eccles, 1976).
Though there are some interesting exceptions (Tritsch et al., 2016), for the vast majority of neurons in
,Corresponding author: blake.richards@mcgill.ca
1
Published as a conference paper at ICLR 2021
adult vertebrate brains, Dale’s principle means that presynaptic neurons can only have an exclusively
excitatory or inhibitory impact on their postsynaptic partners. For ANNs, this would mean that
units cannot have a mixture of positive and negative output weights, and furthermore, that weights
cannot change their sign after initialisation. In other words, a unit can only be excitatory or inhibitory.
However, most ANNs do not incorporate Dale’s principle.
Why is Dale’s principle rarely incorporated into ANNs? The reason is that this architectural constraint
impairs the ability to learn—a fact that is known to many researchers who have tried to train such
ANNs, but one that is rarely discussed in the literature. However, when we seek to compare ANNs
to real brains, or use them to explore biologically inspired learning rules (Bartunov et al., 2018;
Whittington & Bogacz, 2019; Lillicrap et al., 2020), ideally we would use a biologically plausible
architecture with distinct populations of excitatory and inhibitory neurons, and at the same time, we
would still be able to match the learning performance of standard ANNs without such constraints.
Some previous computational neuroscience studies have used ANNs with separate excitatory and
inhibitory units (Song et al., 2016; Ingrosso & Abbott, 2019; Miconi, 2017; Minni et al., 2019;
Behnke, 2003), but these studies addressed questions other than matching the learning performance of
standard ANNs, e.g. they focused on typical neuroscience tasks (Song et al., 2016), dynamic balance
(Ingrosso & Abbott, 2019), biologically plausible learning algorithms (Miconi, 2017), or the learned
structure of networks (Minni et al., 2019). Importantly, what these papers did not do is develop
means by which networks that obey Dale’s principle can match the performance of standard ANNs
on machine learning benchmarks, which has become an important feature of many computational
neuroscience studies using ANNs (Bartunov et al., 2018; Donhauser & Baillet, 2020; Michaels et al.,
2019; Schrimpf et al., 2018; Yamins et al., 2014; Kell et al., 2018).
Here, we develop ANN models with separate excitatory and inhibitory units that are able to learn as
well as standard ANNs. Specifically, we develop a novel form of ANN, which we call a “Dale’s ANN”
(DANN), based on feed-forward inhibition in the brain (Pouille et al., 2009). Our novel approach is
different from the standard solution, which is to create ANNs with separate excitatory and inhibitory
units by constraining whole columns of the weight matrix to be all positive or negative (Song et al.,
2016). Throughout this manuscript, we refer to this standard approach as “ColumnEi” models. We
have departed from the ColumnEI approach in our work because it has three undesirable attributes.
First, constrained weight matrix columns impair learning because they limit the potential solution
space (Amit et al., 1989; Parisien et al., 2008). Second, modelling excitatory and inhibitory units
with the same connectivity patterns is biologically misleading, because inhibitory neurons in the
brain tend to have very distinct connectivity patterns from excitatory neurons (Tremblay et al., 2016).
Third, real inhibition can act in both a subtractive and a divisive manner (Atallah et al., 2012; Wilson
et al., 2012; Seybold et al., 2015; Pouille et al., 2013), which may provide important functionality.
Given these considerations, in DANNs, we utilize a separate pool of inhibitory neurons with a distinct,
more biologically realistic connectivity pattern, and a mixture of subtractive and divisive inhibition
(Fig. 1). This loosely mimics the fast feedforward subtractive and divisive inhibition provided by
fast-spiking interneurons in the cortical regions of the brain (Atallah et al., 2012; Hu et al., 2014;
LoUrengo et al., 2020). In order to get DANNs to learn as well as standard ANNs We also employ
two key insights:
1.	It is possible to view this architectUre as being akin to normalisation schemes applied to the
excitatory inpUt of a layer (Ba et al., 2016; Ioffe & Szegedy, 2015; WU & He, 2018), and we
Use this perspective to motivate DANN parameter initialisation.
2.	It is important to scale the inhibitory parameter Updates based on the Fisher information
matrix, in order to balance the impact of excitatory and inhibitory parameter Updates, similar
in spirit to natUral gradient approaches (Martens, 2014).
Altogether, oUr principle contribUtion is a novel architectUre that obey’s Dale’s principle, and that
we show can learn as well as standard ANNs on machine learning benchmark tasks. This provides
the research commUnity with a new modelling tool that will allow for more direct comparisons with
real neUral data than traditional ANNs allow, bUt which does not sUffer from learning impairments.
Moreover, oUr resUlts have interesting implications for inhibitory plasticity, and provide a means for
fUtUre research into how excitatory and inhibitory neUrons in the brain interact at the algorithmic
level.
2
Published as a conference paper at ICLR 2021
2	Biologically inspired networks that obey Dale’s principle
2.1	Model definition
Our design for DANNs takes inspiration from the physiology of feedforward inhibitory microcircuits
in the neocortex and hippocampus. Based on these circuits, and an interpretation of layers in ANNs
as corresponding to brain regions, we construct DANNs with the following architectural constraints:
1.	Each layer of the network contains two distinct populations of units, an excitatory and an
inhibitory population.
2.	There are far fewer inhibitory units than excitatory units in each layer, just as there are far
more excitatory neurons than inhibitory neurons (〜5-10 times) in cortical regions of the
brain (Tremblay et al., 2016; Hu et al., 2014).
3.	As in real neural circuits where only the excitatory populations project between regions,
here only excitatory neurons project between layers, and both the excitatory and inhibitory
populations of a layer receive excitatory projections from the layer below.
4.	All of the synaptic weights are strictly non-negative, and inhibition is enforced via the
activation rules for the units (eq. 1).
5.	The inhibitory population inhibits the excitatory population through a mixture of subtractive
and divisive inhibition.
This constrained architecture is illustrated in Figure 1.
Figure 1: Illustration of DANN architecture. Lines with arrow ends indicate excitatory projections.
Lines with bar ends indicate inhibitory projections, which can be both subtractive and divisive.
Formally, we define the network as follows. Input to the network is received as a vector of positive
scalar values x ∈ Rd+ , which we consider to be the first excitatory population. Each hidden layer,
',is comprised of a vector of excitatory units h` ∈ R[e and inhibitory units h` ∈ R[i, in-line with
constraint (1) above. (We will drop the layer index when it is unnecessary for clarity.) Note, for the
first layer (` = 1), we have h` = x and ne = d. Next, based on constraint (2) we set ne >> ni,
and use 10% inhibitory units as default. Following constraint (3), both the excitatory and inhibitory
units receive inputs from the excitatory units in the layer below (h'-1), but the inhibitory units do not
project between layers. Instead, excitatory units receive inputs from the inhibitory units of the same
layer. In-line with constraint (4), we have three sets of strictly non-negative synaptic weights, one for
the excitatory connections between layers, WEE ∈ R+e Xne, one for the excitatory projection to the
inhibitory units WIE ∈ R1i ×ne, and one for the inhibitory projections within layer WEI ∈ R+eXni.
Finally, per constraint (5), we define the impact of the inhibitory units on the excitatory units as
comprising both a subtractive and a divisive component:
h' = f (z`)	z` =生 Θ (ZE - WEIh') + β'	(1)
Y'
where ZE = WEE h`-1	h' = f i (z') = f i (W'e h`-1)
Y' = WEi (eα' Θ h')
3
Published as a conference paper at ICLR 2021
where for each layer ', β' ∈ Rne is a bias, g` ∈ Rje controls the gain, γ' is the divisive inhibitory
term, and ɑ` ∈ Rni is a parameter that controls the strength of this divisive inhibition. Here Θ
denotes elementwise multiplication (Hadamard product) and the exponential function and division
are applied elementwise. In the rest of this manuscript we set f to be the rectified linear function
(ReLU). Though a ReLU function is not a perfect match to the input-output properties of real neurons,
it captures the essential rectification operation performed by neurons in physiologically realistic
low activity regimes (Salinas & Sejnowski, 2000). In this paper, we model the inhibitory units as
linear (i.e. fI(zI) = zI) since they receive only positive inputs and have no bias, and therefore their
activation would always be in the linear part of the ReLU function. Although we make make this
modelling choice mainly for mathematical simplicity, there is some biological justification, as the
resting membrane potential of the class of fast-spiking interneurons most related to our model is
relatively depolarised and their spike outputs can follow single inputs one-to-one (Hu et al., 2014;
Galarreta & Hestrin, 2001). In future work, for example in which inhibitory connections are included
between inhibitory units, we expect that the use of nonlinear functions for inhibitory units will be
important.
3 Parameter initialisation for Dale’s ANNs
In biology, excitation and inhibition are balanced (Isaacson & Scanziani, 2011), and we use this
biological property to derive appropriate weight initialisation for DANNs. First we initialise excitatory
parameters from an exponential distribution with rate parameter λE, WEE iiid Exp(λE), and then
inhibitory parameters are initialised such that excitation and subtractive inhibition are balanced, i.e.
E[zkE] = E[(WEIzI)k], ∀k. This can be achieved in a number of ways (see appendix C.2). In line
with biology, we choose to treat excitatory weights onto inhibitory and excitatory units the same,
and sample WIE iid Exp(λE) and set WEI J 1/%. We note that for a DANN layer with a single
inhibitory neuron, e.g. at an output layer with 10 excitatory neurons, the noise inherent in sampling a
single weight vector may result in a poor match between the excitatory and inhibitory inputs, so in
this case we initialise WIE as n- Pn= 1 WEE explicitly (where WEE is the jth row of WEE).
Next, we consider the relationship between this initialisation approach and normalisation schemes (Ba
et al., 2016; Ioffe & Szegedy, 2015). Normalisation acts to both center and scale the unit activities in
a network such that they have mean zero and variance one. The weight initialisation given above will
produce centered activities at the start of training. We can also draw a connection between the divisive
inhibition and standardisation ifwe assume that the elements ofx are sampled from a rectified normal
distribution, X iid max(0, N(0, σ2-1)). Under this assumption, the mean and standard deviation of
the excitatory input are proportional (see Appendix D). For example, if We consider the relationship
C ∙ Ε[zE] = Var(ZE)1/2 for each unit k, we get the scalar proportionality constant C = √2π — 1 /√d,
as:
Ε[zE] = d ∙ E[wEE]E[x]
=d ∙ E[wEE]勺
√2∏
Var(ZE) = d ∙ Var(WEE)(E[x2] + Var(X))
= d ∙ Var(WEE)σ2-ι -----
2π
(2)
with expectation over the data and the parameters, and where WEE, x refer to any element of WEE, X.
Therefore, since E[WEE]2 = Var(WEE) for weights drawn from an exponential distribution, we
have
Var(ZE)2	√2π - 1
e[zE]	= ^^√d-
(3)
This proportionality means that you can perform centering and standardisation operations using the
same neurons. For DANNs, eα will dictate the expected standard deviation of the layer’s activation
z, as it controls the proportionality between subtractive and divisive inhibition for each inhibitory
unit. If eα is set to C, then the divisive inhibition approximates dividing zE by its standard deviation,
as E[zE] ∙ c = E[wEI(eα Θ zjI)] = E[γk]. We note that due to the proportionality between the
mean and standard deviation of zE, other values of eα will also control the layer,s variance with
depth. However, given these considerations, we initialise eɑ J √2∏ — 1 /√d, thereby achieving
standardisation at initialisation. We find that these initialisation schemes enable DANNs to learn well.
We next turn to the question of how to perform parameter updates in DANNs in order to learn well.
4
Published as a conference paper at ICLR 2021
4 Parameter updates for Dale’s ANNs
Unlike a layer in a column constrained network, whose affine function is restricted by sign constrained
columns, a layer in a DANN is not restricted in its potential function space. This is because excitatory
inputs to a layer can still have an inhibitory impact via feedforward inhibition. However, the inhibitory
interneuron architecture of DANN layers introduces disparities in the degree to which updates to
different parameters affect the layer’s output distribution. This can be seen intuitively, for example if
a single element of WIE is updated, this has an effect on each element of z. Similarly, an update
to wiEjI will change zi depending on the alignment of x and all of the jth inhibitory unit’s weights.
Therefore, instead of using the euclidean metric to measure distance between parameter settings, we
employ an alternative approach. Similar to natural gradient methods, we use an approximation of
the Kullback-Leibler divergence (KL divergence) of the layer’s output distribution for our metric. In
order to help ensure that both excitatory and inhibitory parameter updates have similar impacts on the
KL divergence, we scale the updates using correction terms derived below. We provide an extended
derivation of these scaling factors in the Appendix E.
Given a probability distribution parameterized by some vector θ, a second order approximation to the
KL divergence for a change to the parameters θ is
DKL [P(y∣x; θ) k P(y∣x; θ + δ)] ≈ 2δτF(θ)δ
F(θ) =	E
X〜P (X),y〜P卬反;②
∂ log P(y∣x; θ) ∂ log P(y|x; θ) T
∂θ	∂θ
(4)
(5)
Where F(θ) is the Fisher Information matrix (or just the Fisher). In order to calculate the Fisher for
the parameters of a neural network, we must interpret the network’s outputs in a probabilistic manner.
One approach is to view a layer’s activation as parameterising a conditional distribution from the
natural exponential family P (y|x; θ) = P (y|z), independent in each coordinate of y|z (similar to a
GLM, and as done in Ba et al. (2016)). The log likelihood of such a distribution can be written as1
logP(y1x; θ) = ylz-ηz) + c(y,φ)
E[y|x; θ] = f(z) = η0(z)	Cov(y|x; θ) = diag(φf0(z))
(6)
(7)
where f(z) is the activation function of the layer, and ,η, c define the particular distribution in the
exponential family. Note that here We are taking η0(z) and f 0(z) to denote 祭 and df ,respectively.
In our networks, we have used softmax activation functions at the output and ReLU activation
functions in the hidden layers. In this setting, the log likelihood of the output softmax probability
layer Would only be defined for a one-hot vector y and Would correspond to φ= 1, c(y, φ) =
0, and η(z) = log(Pi ezi). For the ReLU activation functions, the probabilistic model corre-
sponds to a tobit regression model, in Which y is a censored observation of a latent variable
y 〜 N(z, diag(φf 0(z))). In this case, one could consider either the censored or pre-censored
latent random variable, depending on modelling preference. As it fits Well With the above frameWork
we analyze the pre-censored random variable y, i.e. f (z) = Z in equation 6. Returning to the
general case, Where We consider layer’s activation as parameterising a conditional distribution from
the natural exponential family, the fisher ofa layer is:
F(θ) =	E
X〜P (X),y〜P卬反;②
∂z (y - η0(z)) (y - η0(z)) T ∂z T
∂θ φ	φ ∂θ
E	∂z diag(f0(Z)) ∂ZT
X〜P (x) ∂θ φ ∂θ
(8)
(9)
1Note the general form of the exponential family is log P(y|z) = z.T(yφ-η(z) + c(y, φ), but here we only
consider distributions from the natural exponential family, where T (y) = y, as this includes distributions of
interest for us, such as Normal and Categorical, and also common distributions including Exponential, Poisson,
Gamma, etc.
5
Published as a conference paper at ICLR 2021
To estimate the approximate KL divergence resulting from the simple case of perturbing an individual
parameter θ ∈ θ of a single-layer DANN, we only need to consider the diagonal entries of the Fisher:
DKL[PθkPθ+δg] ≈ 2-X E	f0(Zk)(-Zk)2
2φ Y x~P(X)L	dθ .
(10)
where δ^ represents a 1-hot vector corresponding to θ multiplied by a scalar δ. We now consider the
approximate KL divergence after updates to a single element of WEE , WIE , WEI and α:
δ2	g
DKL [pθ k Pθ+δwEE ] ≈ 2φE f (zi)( Yxj)	(II)
δ2 ne	g
DKL [Pθ k Pθ+δwiE ] ≈ 2φ EE f (zk)( χxj) (Wki aki)	(12)
δ2 d	g
DKL [Pθ k Pθ+δWEI ] ≈ 2φ EEf(Zi)( Yxn))(Wjnaij )2]	(13)
δ2	d	g
+ 2φ EE f (Zi)(L )2xnxmwjEWjm (aij )2
n6=m	i
δ2 ne d	g
DKL [Pθ k Pθ+δɑi ] = 2φ XX E f (Zk )(上 Xj )2wkiwif(aki - 1)2	(14)
2φ k	j	γk
2 ne	d
+ 2φ XX E f0(Zk )* )2XnXmWIEWIm (WEiI)2(aki - 1)2
eαj
Where akj∙= ——(ZE - (WeiZ1)k) + 1, and expectations are over the data, Ex〜P(X).
γk
Therefore, as a result of the feedforward inhibitory architecture of DANNs, for a parameter update
δ, the effect on the model’s distribution will be different depending on the updated parameter-type.
While the exact effect depends on the degree of co-variance between terms, the most prevalent
differences between and within the excitatory and inhibitory parameter-types are the sums over layer
input and output dimensions. For example, an inhibitory weight update of δ to WiIjE is expected to
change the model distribution approximately ne times more than an excitatory weight update of δ
to WiEjE . In order to balance the impact of updating different parameter-types, we update DANN
parameters after correcting for these terms: updates to WIE were scaled by √ne-1, WEI by d-1
and a by (d√ne)-1. As a result, inhibitory unit parameters updates are scaled down relative to
excitatory parameter updates. This leads to an interesting connection to biology, because while
inhibitory neuron plasticity is well established, the rules and mechanisms governing synaptic updates
are different from excitatory cells (Kullmann & Lamsa, 2007; Kullmann et al., 2012), and historically
interneuron synapses were thought to be resistant to long-term weight changes (McBain et al., 1999).
Next, we empirically verified that our heuristic correction factors captured the key differences between
parameter-types in their impact on the KL divergence. To do this we compared parameter gradients
before and after correction, to parameter gradients multiplied by an approximation of the diagonal of
the Fisher inverse for each layer (which we refer to as Fisher corrected gradients), see Appendix F.3.
The model was trained for 50 epochs on MNIST, and updated using the Fisher corrected gradients.
Throughout training, we observed that the heuristic corrected gradients were more aligned to the
Fisher corrected gradients than the uncorrected gradients were (Fig. 2). Thus, our derived correction
factors help to balance the impact of excitatory and inhibitory updates on the network’s behaviour.
Below, we demonstrate that these corrections are key to getting DANNs to learn well.
6
Published as a conference paper at ICLR 2021
8 6
0.Ci
AɪB=Eω
φ⊂~ωoo
Uncorrected
Corrected
O	10	20	30	40	50
Epoch #
Figure 2: Empirical verification of update correction terms: Cosine of the angle between gradients
multiplied by an approximation of the diagonal of the Fisher inverse for each layer, and either
uncorrected gradients (black) or corrected gradients (orange) over 50 epochs. Plot displays a moving
average over 500 updates
5	Experimental Results
Having derived appropriate parameter initialisation and updates for DANNs, we now explore how
they compare to traditional ANNs and ColumnEi models on simple benchmark datasets. In brief, we
find that column constrained models perform poorly, failing even to achieve zero training-set error,
whereas DANNs perform equivalently to traditional ANNs.
5.1	Implementation Details
All models were composed of 4 layers: in general 3 hidden layers of dimension 500 with a ReLU
activation function followed by a softmax output with 10 units, and all experiments were run for 50
epochs with batch size 32. Unless stated, for DANNs and ColumnEi models, 50 inhibitory units were
included per hidden layer. For DANN models, the softmax output layer was constructed with one
inhibitory unit. For ColumnEi models, each hidden layer’s activation is z = Wx where 500 columns
of W were constrained to be positive and 50 negative (therefore for ColumnEi models h` was of
dimension 550). ColumnEi layer weights were initialised so that variance did not scale with depth
and that activations were centered (see Appendix C.1 for further details). All benchmark datasets
(MNIST, Kuzushiji MNIST and Fashion MNIST) were pre-processed so that pixel values were in
[0, 1]. Learning rates were selected according to validation error averaged over 3 random seeds, after
a random search (Orion; Bouthillier et al. (2019), log uniform [10, 1e-5], 100 trials, 10k validation
split). Selected models were then trained on test data with 6 random seeds. Plots show mean training
error per epoch, and mean test set error every 200 updates over random seeds. Tables show final error
mean ± standard deviation. For further implementation details and a link to the accompanying code
see Appendix F.
Note that because our goal in this paper is not to achieve state-of-the-art performance, we did not
apply regularisation techniques, such as dropout and weight decay, or common modifications to
stochastic gradient descent (SGD). Instead the goal of the experiments presented here was simply to
determine whether, in the simplest test case scenario, DANNs can learn better than ColumnEi models
and as well as traditional ANNs.
5.2	Comparison of DANNs to column-ei models and MLPS
Epoch #
■ DANN ■ DANN
Figure 3: Model comparison on MNIST dataset. nc - no update corrections, 1i - one inhibitory unit
Epoch #
nc ■ DANN 1i ■ CoIumnEi
7
Published as a conference paper at ICLR 2021
We first compared model performance on the MNIST dataset (Fig 3). We observed that ColumnEi
models generalised poorly, and failed to achieve 0 % training error within the 50 epochs. This confirms
the fact that such models cannot learn as well as traditional ANNs. In contrast, we observed that
DANNs performed equivalently to multi-layer perceptrons (MLPs), and even generalised marginally
better. This was also the case for ColumnEi and DANN models constructed with more inhibitory
units (Supp. Fig. 6, 100 inhibitory units per layer). In addition, performance was only slightly
worse for DANNs with one inhibitory unit per layer. These results show that DANN performance
generalizes to different ratios of excitatory-to-inhibitory units. We also found that not correcting
parameter updates using the corrections derived from the Fisher significantly impaired optimization,
further verifying the correction factors (Fig 3).
Next, we compared DANN performance to MLPs trained with batch and layer normalization on more
challenging benchmark datasets (Fig 4). Again we found that DANNs performed equivalently to
these standard architectures, whereas ColumnEi models struggled to achieve acceptable performance.
Figure 4: Model comparison on Fashion MNIST and Kuzushiji MNIST datasets.
We also explored methods for improving DANN performance (Appendix F.4). First, in order to
maintain the positive DANN weight constraint, if after a parameter update a weight was negative, we
reset it to zero, i.e. θ - max(0, θ), and as a result the actual update is no longer that suggested by
SGD. We therefore experimented with temporarily reducing the learning rate whenever this parameter
clipping would reduce the cosine of the angle made between the gradient and actual updates below a
certain constraint (see Appendix F.4). Second, we note that the divisive inhibition term, γ, appears
in the denominator of the weight gradients (Appendix E.2) and, therefore, if γ becomes small, the
gradients will become large, potentially resulting in inappropriate parameter updates. We therefore
wondered if constraining the gradient norm would be particularly effective for DANNs. We tested
both of these modifications to DANNs trained on Fashion MNIST (Supp. Fig. 5). However, we found
that they provided no observable improvement, indicating that the loss landscape and gradients were
well behaved over optimization.
Finally, we provide an analysis and preliminary experiments detailing how the DANN architecture
described above may be extended to recurrent and convolutional neural networks in future work
(Appendix B). In brief, we unroll recurrent networks over time and place inhibition between both
network layers and timesteps, corresponding to fast feedforward and local recurrent inhibition,
respectively. For convolutional architectures, we can directly apply the DANN formulation to
activation maps if inhibitory and excitatory filters are of the same size and stride. Supporting this, we
found that a DANN version of VGG16 (Simonyan & Zisserman, 2014) converged equivalently to a
standard VGG16 architecture (Supp.Fig.7).
Altogether, our results demonstrate that: (1) the obvious approach to creating ANNs that obey Dale’s
principle (ColumnEi models) do not learn as well as traditional ANNs, (2) DANNs learn better than
ColumnEi models and as well as traditional ANNs, (3) DANN learning is significantly improved by
taking appropriate steps to scale updates in excitatory and inhibitory units appropriately.
8
Published as a conference paper at ICLR 2021
6	Discussion
Here we presented DANNs, a novel ANN architecture with separate inhibitory and excitatory units.
We derived appropriate parameter initialisation and update rules and showed experimentally that,
unlike ANNs where some columns are simply constrained to be positive or negative, DANNs perform
equivalently to traditional ANNs on benchmark datasets. These results are important as they are, as
far as we know, the first example of an ANN architecture that fully adheres to Dale’s law without
sacrificing learning performance. However, our results also raise an interesting question: why does
nature employ Dale’s principle? After all, we did not see any improvement over normal ANNs in our
experiments. There are two possible hypotheses. First, it is possible that Dale’s principle represents
an evolutionary local minima, whereby early phylogenetic choices led to constraints on the system
that were difficult to escape via natural selection. Alternatively, Dale’s principle may provide some
computational benefit that we were unable to uncover given the specific tasks and architectures we
used here. For example, it has been hypothesized that inhibition may help to prevent catastrophic
forgetting (Barron et al., 2017). We consider exploring these questions an important avenue for future
research.
There are a number of additional avenues for future work building upon DANNs, the most obvious
of which are to further extend and generalize DANNs to recurrent and convolution neural networks
(see Appendix B). It would also be interesting to explore the relative roles of subtractive and divisive
inhibition. While subtractive inhibition is required for the unconstrained functional space of DANN
layers, divisive inhibition may confer some of the same optimisation benefits as normalisation
schemes. A related issue would be to explore the continued balance of excitation and inhibition
during optimization, because while DANNs are initialised such that these are balanced, and inhibition
approximates normalisation schemes, the inhibitory parameters are updated during training, and the
model is free to diverge from this initialisation. As a result, the distribution of layer activations may be
unstable over successive parameter updates, potentially harming optimization. In the brain, a variety
of homeostatic plasticity mechanisms stabilize neuronal activity. For example, reducing excitatory
input naturally results in a reduction in inhibition in real neural circuits (Tien & Kerschensteiner, 2018).
It would therefore be interesting to test the inclusion of a homeostatic loss to encourage inhibition
to track excitation throughout training. Finally, we note that while fast feedforward inhibition in
the mammalian cortex was the main source of inspiration for this work, future investigations may
benefit from drawing on a broader range of neurobiology, for example by incorporating principles of
invertebrate neural circuits, such as the mushroom bodies of insects (Serrano et al., 2013).
In summary, DANNs sit at the intersection of a number of programs of research. First, they are a new
architecture that obeys Dale’s principle, but which can still learn well, allowing researchers to more
directly compare trained ANNs to real neural data (Schrimpf et al., 2018; Yamins et al., 2014). Second,
DANNs contribute towards computational neuroscience and machine learning work on inhibitory
interneurons in ANNs, and in general towards the role of inhibitory circuits and plasticity in neural
computation (Song et al., 2016; Sacramento et al., 2018; Costa et al., 2017; Payeur et al., 2020;
Atallah et al., 2012; Barron et al., 2017). Finally, the inhibition in DANNs also has an interesting
connection to normalisation methods used to improving learning in deep networks (Ioffe & Szegedy,
2015; Wu & He, 2018; Ba et al., 2016). As DANNs tie these distinct programs of research together
into a single model, we hope they can serve as a basis for future research at the intersection of deep
learning and neuroscience.
Acknowledgements
We would like to thank Shahab Bakhtiari, Luke Prince, and Arna Ghosh for their helpful comments
on this work. This work was supported by grants to BAR, including a NSERC Discovery Grant
(RGPIN-2020-05105), an Ontario Early Career Researcher Award (ER17-13-242), a Healthy Brains,
Healthy Lives New Investigator Start-up (2b-NISU-8), and funding from CIFAR (Learning in
Machines and Brains Program, Canada CIFAR AI Chair), the Wellcome Trust and by the Medical
Research Council (UK). Additionally, DK was supported by the FRQNT Strategic Clusters Program
(2020-RS4-265502-UNIQUE).
9
Published as a conference paper at ICLR 2021
References
Daniel J Amit, C Campbell, and KYM Wong. The interaction space of neural networks with
sign-constrained synapses. Journal of Physics A: Mathematical and General, 22(21):4687, 1989.
Bassam V Atallah, William Bruns, Matteo Carandini, and Massimo Scanziani. Parvalbumin-
expressing interneurons linearly transform cortical responses to visual stimuli. Neuron, 73(1):
159-170, 2012.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Helen C Barron, Tim P Vogels, Timothy E Behrens, and Mani Ramaswami. Inhibitory engrams in
perception and memory. Proceedings of the National Academy of Sciences, 114(26):6666-6674,
2017.
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E Hinton, and Timothy Lilli-
crap. Assessing the scalability of biologically-motivated deep learning algorithms and architectures.
In Advances in Neural Information Processing Systems, pp. 9368-9378, 2018.
Sven Behnke. Hierarchical neural networks for image interpretation, volume 2766. Springer, 2003.
Xavier Bouthillier, Christos Tsirigotis, Frangois Corneau-Tremblay, Pierre Delaunay, Reyhane Askari,
Dendi Suhubdy, Michael Noukhovitch, Dmitriy Serdyuk, Arnaud Bergeron, Peter Henderson,
Pascal Lamblin, Mirko Bronzi, and Christopher Beckham. Orion - asynchronous distributed
hyperparameter optimization, October 2019. URL https://doi.org/10.5281/zenodo.
3478592.
Rui Costa, Ioannis Alexandros Assael, Brendan Shillingford, Nando de Freitas, and Tim Vogels.
Cortical microcircuits as gated-recurrent neural networks. In Advances in neural information
processing systems, pp. 272-283, 2017.
Peter W Donhauser and Sylvain Baillet. Two distinct neural timescales for predictive speech
processing. Neuron, 105(2):385-393, 2020.
John Carew Eccles. From electrical to chemical transmission in the central nervous system: the
closing address of the sir henry dale centennial symposium cambridge, 19 september 1975. Notes
and records of the Royal Society of London, 30(2):219-230, 1976.
Mario Galarreta and Shaul Hestrin. Spike transmission and synchrony detection in networks of
gabaergic interneurons. Science, 292(5525):2295-2299, 2001.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Hua Hu, Jian Gan, and Peter Jonas. Fast-spiking, parvalbumin+ gabaergic interneurons: From cellular
design to microcircuit function. Science, 345(6196):1255263, 2014.
Alessandro Ingrosso and LF Abbott. Training dynamically balanced excitatory-inhibitory networks.
PloS one, 14(8):e0220547, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Jeffry S Isaacson and Massimo Scanziani. How inhibition shapes cortical activity. Neuron, 72(2):
231-243, 2011.
Alexander JE Kell, Daniel LK Yamins, Erica N Shook, Sam V Norman-Haignere, and Josh H
McDermott. A task-optimized neural network replicates human auditory behavior, predicts brain
responses, and reveals a cortical processing hierarchy. Neuron, 98(3):630-644, 2018.
Tim Christian Kietzmann, Patrick McClure, and Nikolaus Kriegeskorte. Deep neural networks in
computational neuroscience. BioRxiv, pp. 133504, 2018.
10
Published as a conference paper at ICLR 2021
Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha Hong, Najib Majaj, Elias Issa,
Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, et al. Brain-like object recognition with
high-performing shallow recurrent anns. In Advances in Neural Information Processing Systems,
pp.12805-12816, 2019.
Dimitri M Kullmann and Karri P Lamsa. Long-term synaptic plasticity in hippocampal interneurons.
Nature Reviews Neuroscience, 8(9):687-699, 2007.
Dimitri M Kullmann, Alexandre W Moreau, Yamina Bakiri, and Elizabeth Nicholson. Plasticity of
inhibition. Neuron, 75(6):951-962, 2012.
Qianli Liao and Tomaso Poggio. Bridging the gaps between residual learning, recurrent neural
networks and visual cortex. arXiv preprint arXiv:1604.03640, 2016.
Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backprop-
agation and the brain. Nature Reviews Neuroscience, pp. 1-12, 2020.
Joana LoUrengo, Angela Michela De Stasi, Charlotte Deleuze, Mathilde Bigot, Antonio Pazienti,
Andrea Aguirre, Michele Giugliano, Srdjan Ostojic, and Alberto Bacci. Modulation of coordinated
activity across cortical layers by plasticity of inhibitory synapses. Cell reports, 30(3):630-641,
2020.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Chris J McBain, Tamas F Freund, and Istvan Mody. Glutamatergic synapses onto hippocampal
interneurons: precision timing without lasting plasticity. Trends in neurosciences, 22(5):228-235,
1999.
Jonathan A Michaels, Stefan Schaffelhofer, Andres AgUdelo-Toro, and HansjOrg Scherberger. A
neural network model of flexible grasp movement generation. bioRxiv, pp. 742189, 2019.
Thomas Miconi. Biologically plausible learning in recurrent neural networks reproduces neural
dynamics observed during cognitive tasks. Elife, 6:e20899, 2017.
Sun Minni, Li Ji-An, Theodore Moskovitz, Grace Lindsay, Kenneth Miller, Mario Dipoppa, and
Guangyu Robert Yang. Understanding the functional and structural differences across excitatory
and inhibitory neurons. 2019.
Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J
DiCarlo, and Daniel L Yamins. Task-driven convolutional recurrent models of the visual system.
In Advances in Neural Information Processing Systems, pp. 5290-5301, 2018.
Christopher Parisien, Charles H Anderson, and Chris Eliasmith. Solving the problem of negative
synaptic weights in cortical models. Neural computation, 20(6):1473-1494, 2008.
Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake Richards, and Richard Naud. Burst-
dependent synaptic plasticity can coordinate learning in hierarchical circuits. bioRxiv, 2020.
Fr6d6ric Pouille, Antonia Marin-Burgin, Hillel Adesnik, Bassam V Atallah, and Massimo Scanziani.
Input normalization by global feedforward inhibition expands cortical dynamic range. Nature
neuroscience, 12(12):1577, 2009.
Frederic Pouille, Oliver Watkinson, Massimo Scanziani, and Andrew J Trevelyan. The contribution
of synaptic location to inhibitory gain control in pyramidal cells. Physiological reports, 1(5), 2013.
Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia
Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep
learning framework for neuroscience. Nature neuroscience, 22(11):1761-1770, 2019.
11
Published as a conference paper at ICLR 2021
Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits
approximate the backpropagation algorithm. In Advances in Neural Information Processing
Systems,pp. 8721-8732, 2018.
Emilio Salinas and Terrence J Sejnowski. Impact of correlated synaptic input on output firing rate
and variability in simple neuronal models. Journal of neuroscience, 20(16):6193-6209, 2000.
Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa, Kohitij
Kar, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, et al. Brain-score: Which artificial
neural network for object recognition is most brain-like? BioRxiv, pp. 407007, 2018.
Eduardo Serrano, Thomas Nowotny, Rafael Levi, Brian H Smith, and Ram6n Huerta. Gain control
network conditions in early sensory coding. PLoS Comput Biol, 9(7):e1003133, 2013.
Bryan A Seybold, Elizabeth AK Phillips, Christoph E Schreiner, and Andrea R Hasenstaub. Inhibitory
actions unified by network integration. Neuron, 87(6):1181-1192, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
H Francis Song, Guangyu R Yang, and Xiao-Jing Wang. Training excitatory-inhibitory recurrent
neural networks for cognitive tasks: a simple and flexible framework. PLoS computational biology,
12(2):e1004792, 2016.
Nai-Wen Tien and Daniel Kerschensteiner. Homeostatic plasticity in neural development. Neural
development, 13(1):1-7, 2018.
Robin Tremblay, Soohyun Lee, and Bernardo Rudy. Gabaergic interneurons in the neocortex: from
cellular properties to circuits. Neuron, 91(2):260-292, 2016.
Nicolas X Tritsch, Adam J Granger, and Bernardo L Sabatini. Mechanisms and functions of gaba
co-release. Nature Reviews Neuroscience, 17(3):139-145, 2016.
James CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in
cognitive sciences, 2019.
Nathan R Wilson, Caroline A Runyan, Forea L Wang, and Mriganka Sur. Division and subtraction by
distinct cortical inhibitory networks in vivo. Nature, 488(7411):343-348, 2012.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand
sensory cortex. Nature neuroscience, 19(3):356, 2016.
Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J
DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual
cortex. Proceedings of the National Academy of Sciences, 111(23):8619-8624, 2014.
12
Published as a conference paper at ICLR 2021
Supplementary Material
A Supplementary results
Figure 5: DANNs trained on Fashion MNIST with gradient normalisation and learning rate scaling.
Figure 6: Model comparison on MNIST dataset as in Fig. 3 but including models with 100 inhibitory
units. nc - no update corrections, #i - no. inhibitory units, ColumnEi #e #i.
ColUmn日 500 50
CoIumnEi 500 100
Table 1: MNISTresults
Model	# inhib	Learning rate	Train error	Test error
MLP	0	0.2976	0.0 ± 0.0	1.44 ± 0.034
DANN	1	0.05327	0.0 ± 0.0	1.56 ± 0.041
DANN	50	0.1107	0.0 ± 0.0	1.325 ± 0.066
DANN	100	0.3576	0.0 ± 0.0	1.244 ± 0.067
DANN nc	50	0.003981	0.293 ± 0.039	2.167 ± 0.113
ColumnEi	50	0.05273	0.662 ± 0.071	3.035 ± 0.231
ColumnEi	100	0.06533	0.574 ± 0.092	2.857 ± 0.08
13
Published as a conference paper at ICLR 2021
Table 2: K-MNIST results
Model	# inhib	Learning rate	Train error	Test error
MLP	0	0.213	0.0 ± 0.0	6.842 ± 0.119
LayerNorm	0	0.1166	0.0 ± 0.0	7.171 ± 0.134
BatchNorm	0	0.7235	0.123 ± 0.017	6.747 ± 0.129
DANN	50	0.156	0.0 ± 0.0	6.728 ± 0.23
ColumnEi	50	0.05426	1.886 ± 0.191	12.763 ± 0.44
Table 3: FaShion MNIST results
Model	# inhib	Learning rate	Train error	Test error
MLP	0	0.08403	1.793 ± 0.119	10.626 ± 0.466
LayerNorm	0	0.04743	2.136 ± 0.076	10.445 ± 0.455
BatchNorm	0	0.1098	1.104 ± 0.044	9.992 ± 0.218
DANN	50	0.01973	3.832 ± 0.031	10.962 ± 0.365
ColumnEi	50	0.02265	12.365 ± 0.217	14.986 ± 0.674
B	Extension of DANNs to other architectures
Here we discuss how our results and analysis of fully-connected feedforward Dale’s ANNs may be
applied to convolutional and recurrent neural networks.
B.1	Extension to Convolutional Neural Networks
Consider the response of a standard convolutional layer of n output channels with filters of size k × k
at a single position j over m input channels:
zj = Wxj + b	(15)
Here, W is a n × k2m matrix whose rows correspond to the kernel weights of each output channel,
and the vector xj of length k2m contains the values over the n input channels for the spatial location i.
Concatenating each input location xj as the columns of a matrix X, the full output of the convolutional
layer over all input locations can be expressed as Z = WX + b, where b is broadcast over the
columns of Z. We can readily make an equivalent DANN formulation for a convolution layer by
assuming the same kernel size and stride for excitatory and inhibitory filter-sets WEE and WEI :
Zj = g Θ (WEEXj- WEWEXj) + β,
γ = WEI(eα	WIExj)
(16)
Here the inhibitory channels are mapped to each excitatory output channel by WIE for subtractive
inhibition, and are first scaled by eα for divisive inhibition. For parameter initialisation, by following
the approach of He et al. (2015) and considering the response of the layer at a single location, we use
the same initialisations as those derived in section 3, but where the input dimension d is the product
of kernel size and input channels, k2m. Next, the correction factors to parameters updates apply
as in section 4 as the KL divergence is summed over each valid input location j, which results in
approximately the same multiplicative factor for each parameter, but does not change the approximate
relative differences between parameter types:
δ2 n
DKL [Pθk Pθ+δ∕≈ X ∣2φ X X〜E
ji
f 0(Zi,j)( M )2
(17)
where we consider the full response Z of the layer over all valid kernel locations.
14
Published as a conference paper at ICLR 2021
In order to confirm our extension to convolutional neural networks we conducted preliminary
experiments with DANN versions of convolutional neural networks as described above. Below,
we show results of training a standard VGG16 architecture, and a DANN version of the VGG16
architecture (Supp. Fig. 7) on CIFAR-10. As can be seen, the DANN network trains approximately
as well as the standard VGG16 model.
Figure 7: Convolutional network results on CIFAR-10 with control and DANN VGG16 models. Plots
show mean training and test set error over 6 random seeds.
Both control and DANN VGG16 architectures were trained on CIFAR-10 with stochastic gradient
descent with batch size 128, without dataset augmentation, dropout, or batch normalisation layers.
Best model learning rates (control - 0.089 , DANN - 0.03458) were selected after a random search
according to average final validation error over random seeds, and conditional on all seeds beginning
to converge within 5 epochs (convergence defined as validation error < 90%). The random search
was performed with learning rates sampled from a log-uniform [1e-4,1] distribution, 3 seeds per trial,
60 trials, 150 epochs, and with a 10k validation split. Final epoch test error over 6 random seeds
was 21.08 ± 0.811 for the control VGG16 model, and 21.178 ± 0.348 for the DANN-VGG16 model
with constrained weights. VGG16 models were adapted from code here2, and for the DANN VGG16
model we used 10 inhibitory filters per 64 excitatory filters, and 10% inhibitory units in the fully
connected layers.
B.2	Extension to Recurrent Neural Networks
We can readily make a connection between the fully-connected Dales ANNs described in Section
2.1 and recurrent neural networks (RNNs) by considering the similarities between depth and time.
As has been previously noted, a shallow RNN unrolled over time can be expressed as a deep neural
network with weight sharing (Liao & Poggio, 2016).
ht = f(zt)	Zt = gt Θ Wht-1 + β
(18)
where W = Wee — WeiWei,	Yt = Wei(eα Θ WEIht-I)
where in this simple case, recurrent processing steps over time are applied to the input x = h0 . In
this view, layer depth corresponds to time, and inhibition between layers corresponds to fast feedback
inhibition.
We note that if there are a sequence of inputs coming at each time-step, xt , then this formulation can
still hold, but with a simple modification to incorporate the time-varying inputs. Specifically, we need
to add additional input weights, U:
ht = f (zt)	Zt = g Θ Wht-1 + gx Θ UJXt + β
γt	γx
2https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py
(19)
15
Published as a conference paper at ICLR 2021
where WW = WEE — WEIWEI, Yt = WEI(eαt Θ WEIht-I)
UJ = UEE — UEIUEI
γx = UEI(eαx ΘUEIxt)
All of the existing DANN approaches developed above can be applied to this case.
C Parameter initialisations
In this section we provide further details regarding parameter initialisations.
Throughout we assume that the elements of the input, x, to a layer ` are iid and also the output of a
layer ' 一 1 whose pre-activations were distributed N(0, σ2-ι). Therefore X will follow a rectified
normal distribution:
E[x]	=I- ∞ X ∙ 0	e-χ2∕2σ'-1 	d^dx 二 σ'-i √2π	σ'-i =—— √2∏
E[x2]	=Z∞X2 0	e-x2∕2σ2-ι •	^^dX σ'-i √2π	2 σ'-1 =	 2
Var(x)	= E[X2] 一	E[x]2 = σ2-iπ	一 1 2π~
Var(x) + E[x]2	2 2π 一 1 =σ'-ι H		
(20)
where here, and throughout the text, non-indexed non-bold to refers to any element of a vector
or matrix, e.g E[x] refers to the expectation of any element of x, Var(x) refers to the variance of
any element of x, etc. In addition, for all models we draw positively constrained weights iid from
exponential distributions, and make use of the following properties for W 〜Exp(λ)
E[w] ==
λ
Var(W) = ɪ = E[w]2
2!
E[w2 ]=率=2Var(w) = 2E[w]
(21)
C.1 Column constrained EI models and weight initialisation
Here we provide detail on the parameter initialisation of column constrained models. Layer activations
are z = Wx where columns of W are constrained to be positive or negative. Therefore, for
convenience, let us denote W = [W+, W-], and x = [xE, xI], and we assume xiE, xjI are iid
∀i, j . Note for this model, ne + ni = d, the input dimensionality. As for DANN models, throughout
training we preserve the sign constraints of the weights by resetting weights using rectification around
zero, i.e. W+ J max(0, W+), W- J min(0, W-).
At initialisation for the column constrained model for each layer we require E[zk] = 0,
Var(Zk ) = σ'-1.
E[zk] = neE[W+]E[x] 一 niE[W-]E[x]
neE[W+]E[x] = niE[W-]E[x]
n
E[w-] = E[w+]—
ni
(22)
16
Published as a conference paper at ICLR 2021
Where w+, w- refer to any element of W+, W-.
ne	ni
Var(zk) =	Var(wk+ixiE) +	Var(wk-j xjI)
ij
= neVar(w+x) + niVar(w-x)	(23)
=ne (E[w+]2 Var(x) + Var(w+)E[x]2 + Var(w+)Var(x))
+ n (E[w-]2Var(x) + Var(W-)E[x]2 + Var(W-)Var(x))
As weights are drawn from an exponential distribution, Var(w+ ) = E[w+]2, we have
Var(zk) = neE[W+]2 (2Var(x) + E[x]2) + niE[W-]2 (2Var(x) + E[x]2)
= neE[W+]2 (E[x2] + Var(x)) + niE[W-]2(E[x2] + Var(x))
= (E[x2] + Var(x))(neE[W+]2 + niE[W-]2)	(24)
=σ2-i(2∏-1 )E[w+]2(ne + 逞)
2π	ni
2
Therefore E[w+] = 1/( 2∏-1 )(ne + ne)
Note that as the input to the network is all positive, the first weight matrix has no negative columns.
We therefore use the bias vector to center the activations of the first layer (in other layers it is
initialised to zeros).
E[zk] = neE[W+]E[x] + βk
(25)
Therefore we initialise all elements of β to -ne E[W +]E[x]
C.2 Initialisation of DANN inhibitory weights for balanced excitation and
SUBTRACTIVE INHIBITION
Here provide details of inhibitory parameter initialisation such that E[zkE] = E[(WEIzI)k], for
WEE iid Exp(λE).
d1
E[zE] = e[£ WEExi] = dλEE[x]
i
ni	d
E[(WEIzI)k] = E[X WkEjI X WjEiIxi] = niE[WEI]dE[WIE]E[x]
ji
(26)
These expectaions are equal when both sets of excitatory weights are drawn from the same distribution,
WIE
R Exp(λE) and WEI J l/ng. Or alternatively, inhibitory weights can both drawn from
the same distribution, WIE, WEI iid Exp(^λ^Eni). Note, that although the above always holds
in expectation, in the case of a multiple inhibitory units we can apply the law of large numbers to
conclude that the subtractive inhibition and excitatory input will be approximately equal.
Note that while this initialisation is general to different settings of λE, we initialise
λE J pd(2π - 1)/√2π (see section D.1).
17
Published as a conference paper at ICLR 2021
D	Proportional relationship between excitatory input mean and
S TANDARD DEVIATION
Here we provide further details regarding the proportionality between zE’s mean and standard
deviation. This proportionality constant depends on which statistic or distribution that is of interest
for activations (e.g. layer-statistics or unit batch-statistics as in layer and batch normalisation).
D. 1 Unit statistics over data and parameter distributions
As discussed in the main text, if We consider C ∙ E[zE] = Var(ZE)1/2 for a Unit k, With expectation
over the data and parameters, C = √2∏ - 1 / √d:
E[zE] = d ∙ E[wEE]E[x]
=d ∙ E[wEE] √-1
2π
d
Var(zkE) = Var(XwkEiExi)
i
=d ∙ Var(WEEx)
=d ∙ Var(WEE)E[x2] + d ∙ Var(x)E[wEEK
=d ∙ Var(WEE)(E[x2] + Var(x))
= d ∙ Var(WEE)σ2-1 -----
2π
Where E[WEE]2 = Var(WEE) for Weights draWn from an exponential distribution. Therefore
E[zE] ∙ C =，Var(ZE)
√2π - 1
C =-----——
√d
(27)
(28)
Additionally, We see that for Var(ZE) = σj-1 the variance of the distribution that elements of WEE
are draWn from should be
LL	2Tr
Var(WEE) =  ------------ (29)
d ∙ (2π — 1)
and so we can set λE J dd(2π - 1)/√2π, for Var(ZE) = σ'2-1.
D.2 Unit statistics over the data distribution
If instead We consider a unit k, With excitatory Weights wkEE and expectation and variance taken only
over the data We have the approximation:
d
E[ZkE] = E[x] X WkEiE
i
≈ d ∙ E[x]E[WEE]
=d ∙ √-1 E[wEE]
2π
LikeWise the variance over the data can be approximated as
d
Var(ZkE) = Var(x) X(WkEiE)2
i
≈ d ∙ Var(x) ∙ E[(wee)2]
=d ∙ σ2-1π-1 ∙ 2 ∙ e[wEEF
2π
(30)
(31)
18
Published as a conference paper at ICLR 2021
Therefore
Ex〜P(X)Izk ] ∙ C = JVarx〜P(X)Izk]
c≈
√2π - 2
√d
(32)
D.3 Layer statistics over the data and parameter distributions
Alternatively We can consider the mean and standard deviation of the layer statistics μ%E, σ%E as
calculated if one was to apply layer normalisation to zE. Here again, these statistics are proportionally
related, but with the constant √π/√d.
If we were to apply layer normalisation to zE , the layer statistics would be as follows:
ne	ne	ne
Z =	σ~ (ZE - μzE )+β	μzE	= n X ZE	=	n1	X Wa	σZE	=	n4-1 X(zj-μzE )2
σ E	ne	ne	ne -
z	ej	ej	e	j
(33)
We now derive the relationship that the expectation of layer statistics are proportionally related by
E[μzE] ∙ √π∕√d = E(g；e)1/2. The expectation of E[μzE] is straightforward:
E[μzE] = d ∙ E[wee] ∙ E[x]	(34)
Turning to the derivation of EIσz2E ]:
ne E[σZE ]= E[——T X(zE - μzE )2] ne - 1 i ne	ne =n--1 X E[(wEEX - - X WEEX)2] 1 ne =T X E[(^i)2] ne - 1 i	(35) (36) (37)
where we have defined ^ = WEEX - n1- Pne WEEx. We can obtain E[(^i)2] by deriving E[zi] and
Var(zi). As
ne
Wj = WjE - - X WEE
ne
k
ne
-X (WjE -WEjE)
e k=1,k6=i
(38)
we see that E[Wj] = 0, and therefore E[^i] = 0. For the variance of Var(zi) we start with Var(Wj).
ne
Var(Wj) = nVar( X (WEE - WEjE))	(39)
ne	k=1,k6=i
ne	ne
=n( X Var(WjE - WEE) + X	CoV(WjE - WEE,WEE - WEOE)) (4O)
e k=1,k6=i	k=1,k0=1
k,k0 6=i, k6=k0
=3((n - 1)2Var(WEE) + (n - 1)(ne - 2)Var(WEE))	(41)
ne2
=(Ue - I) Var(WEE)	(42)
ne
19
Published as a conference paper at ICLR 2021
For i ≤ ne We calculate Var(^i), keeping in mind that for i ≤ ne,j ≤ d, Xj are iid, and equation
(38) shows that Wij are iid in the j,th coordinate, so We see that
d
Var(Zi) = Var(^X Wij Xj) = dVar(Wx)	(43)
j
Remembering the values of E[W], Var(W), that E[X2] = Var(X) + E[X]2, and for independent
X,Y, Var(XY) = Var(X)Var(Y) +Var(X)E[Y]2 +Var(Y)E[X]2, we have
Var(Zi) = d(Var(W)Var(x) + Var(W)E[x]2 + Var(X)E[W]2)	(44)
=d(Var(W)E[x2] + Var(X)E[W]2) = dVar(W)E[x2]	(45)
="ne------ Var(WEE)E[x2]	(46)
Now putting these terms together we can derive E[σz2E].
ne
E[σZE]=——T ∑E[(Zi)2]
ne - 1
i
ne
=「X Var(Zi)
ne -
i
=d ∙ Var(WEE) ∙ E[x2]
(47)
(48)
(49)
Therefore returning to E[μzE] ∙ C = E(σZE )1/2 and keeping in mind that the variance of an exponential
random variable is it’s mean squared,
(d ∙ Var(WEE)E[x2])1∕2	pE[x2]
c ----------------------------------
d ∙ E[wee]∙ E[x]	√d ∙ E[x]
(50)
We have assumed that X follows a rectified normal distribution. Therefore, E[x] = σ√-∏1, E[x2]
σ2
-j2-1. Resulting in:
(51)
We note that for a DANN layer with a single inhibitory unit, μ%E = ZI as W IE - n1e Pne wjE,E: ,
and WEI - 1. Therefore DANN divisive inhibition, γ, can be made equivalent to layer standard
deviation at initialisation in expectation if eα — c. However, these calculations apply for the case of
multiple interneuron if one makes the approximation μ%E ? ≈ (WEIWIEx)i for any i.
20
Published as a conference paper at ICLR 2021
E Parameter updates and Fisher information matrix
E.1 Layer Fisher information matrix
We view a layer’s activation as parameterising a conditional distribution from the exponential family
P (y|x; θ) = P (y|z), independent in each coordinate of y|z.
logP(y 因θ)=ylz-ηz)+c(y,φ)
(52)
E[y|x; θ] = f(z) = η0(z)	Cov(y|x; θ) = diag(φf0(z))	(53)
where f(z) is the activation function of the layer, and φ,η, c define the particular distribution in the
exponential family. Note We take η0(z), f 0(z) to denote the 鸡,df.
F(θ) is defined as:
As
We have
F(θ) =	E X〜P (X),y~P (y|X⑻	∂ log P(y|x； θ) ∂ log P(y|x;		θ)T
	-	∂θ	∂θ	-
∂	∂z ∂ 而 log P(y|x; θ)=而 Tr ∂θ	∂θ ∂z		[+ c(y,φ)l φ	
1 ∂z =φ∂θ(y		-电) ∂z	
F(θ) =	E
X〜P (X),y〜P卬区内
∂z (y -η0(z)) (y - η0(z)) T ∂zT
∂θ	φ	φ	∂θ
E
X〜P (x)
∂ z
∂θ
E
y〜P (y|x;6)
(y - W(Z)Ny — η0(Z))T∣ ,θ∣ ∂zT
φ	φ I ;	∂θ
E	∂Z Cov [y∣x; θ] ∂Z T
X〜P (x) ∂θ	φ2	∂θ
E
X〜P (x)
∂z diag(f0(Z)) ∂zT
∂θ	φ ∂θ
(54)
(55)
(56)
(57)
(58)
(59)
Where We recognise the covariance matrix is diagonal:
Cov yIx; θ = diag(Var(y1|x; θ), ..., Var(yne |x; θ))
To analyse the approximate KL divergence resulting from the simple case of perturbing individual
parameters of a single-layer DANN, We only need to consider the diagonal entries of the Fisher.
DKL [Pθ k Pθ+δj ≈ 1 δθ E,)
2 X〜P (x)
Q E
2φ X〜P (x)
Q E
2φ X〜P (x)
∂z diag(f0(Z)) ∂zT δ
∂θ	φ ∂θ θ
∂z 0	∂z T
而diag(f (Z))而
∂θ	∂θ
0 T	∂z ∂zT
(f (z) Θ --)-W
∂θ ∂θ
2 ne
一X E
2φ	-X X〜P(x)
k
(60)
(61)
(62)
(63)
2
21
Published as a conference paper at ICLR 2021
where δ^ represents a 1-hot vector corresponding to θ, multiplied by a scalar δ.
E.2 Derivatives
Here we provide derivatives for DANN layer activations with respect to the different parameter
groups. The equations for the layer activation can be written
Z = g Θ (ZE - WEIZI) + β
γ	(64)
where ZE = WEEx ZI = WIEx γ = WEI (eα Θ ZI)
∂zk	∂zk
∂WEE =研=0for k = i.
ij	ij
∂zi ∂wEE ij	:∂⅛(Y(ZE -(WEIZI)i)+ βi)	(65)
	=g ɪ (ZE) YidWjE (Zi )	(66)
	gi 二—xj γi	(67)
		
∂zi _ ∂wEj ij	=∂⅛( gi (ZE-(WEIZI)i) + βi)	(68)
	=_g_ dγi (ZE _ (WEIZI).) _ giZI	(69)
	「2 ∂wEI(Zi(W	)i)	Yi j	
	=-g2eɑjZj(ZE -(WEIZI)i)- gZj Yi2	j i	Yi j	(70)
	--更Zj (吃(ZE -(WEIZI)i) + 1) Yi	Yi	(71)
	=-X gi WjE xk (- - (ZE -(WEIZI)i) + 1) k Yi	Yi	(72)
In contrast
∂zk ∂zk
∂WEI ,∂αj = 0for k = i.
ij
∂Zk _ ∂wiE ij	=∂⅛E (gk(ZE -(WEIZI)k) + βk) wij Yk		(73)
	T∂⅜(ZE -(WEIZI)k) - Yk	wEIx wki xj	(74)
	=-g2 eαi WEiIxj(ZE -(WEIZI)k)- Yk	gk EI 丁 WkiX Yk	(75)
	二一gWEiIxj(	(ZE -(WEIZI)k) + 1)		(76)
	Yk	Yk		
22
Published as a conference paper at ICLR 2021
∂zi
daj
U (gi (zE -(WEIzI),) + 仪)
αj γi
gi ∂γi
Y2 daj
(ziE - (WEIzI)i)
W WEIeaj zj (ZE-(WEIz))
-X g WEIwjE Xk"— (ZE τwEIzIκ
k γi	γi
g∙ = Y (ZE-(WEIz^
∂Zi
(77)
(78)
(79)
(80)
(81)
(82)
(83)
E.3 Approximate KL divergence for weight updates
If we consider an update to an element ij of WEE the approximate KL divergence is
δ2 ne	∂Z
DKL Wθ k Pθ+δwEE ]≈ 2φ X XjE (X) f0(Zk )( ∂WEE )
k	ij
=δφ XJE (JO(Zi)( Y Xj )2_
(84)
as ∂WzkE = 0 for k = i.
ij
In contrast, for an update to an element ij of WIE We sum over n terms, as = 0 for k = i.
∂wij
δ2 ne
DKL [Pθ k Pθ+δwIE ] ≈ 2φ £
k
_ δ2 X
=2φ Z
_ δ2X
=2φ v
E
XjP (X)
E
XjP (X)
f(Zk)(篝)2
∂WiIjE
f O(Zk )( - kw WEiIXj aki)2
Yk
(85)
E
XjP (X)
f0(Zk)* Xj n⅛i产
eαj
where a* =——(ZE -(WEIZI)k) + 1.
Yk
23
Published as a conference paper at ICLR 2021
∂zk	I
For an update δwEi, while C EI = 0 for k = i, the derivative contains a z1 term, so there is instead
ij	∂wiEjI	j
a squared sum over d terms.
DKLWe k Pθ+δwEjJ≈ 2Φ X XjE (x) f0(Zk)( ∂⅛ )2
k	ij
M E	f 0(zi)( 7⅛ )2
2φ XJP (x)	'∂wj”
2φXJE(X) f0(zi)( - giZjaj)2]
2φXJE(X) f0(Zi)(Zj)2(gi)2(aj)2_
2d
2φ XJE (X) f0(Zi)(E wjEnxn)2( γi )2(aj )2
δ2	d	d	g
2φ X 〜E x) f0(Zi)( E(WjE)2(Xn )2 + E WjnWjmxnxm)( Y )2(aij )2
δ2 d
2φ X XJE(x) f (Zi)(Wjn) (Xn) (；) (aij)
δ2 d	g
+ 2φ E X 〜E X) f (Zi)WjnWjmxnxm( Y )2(aij ¥
n6=m
(86)
Finally, for alpha
δ2 ne	∂
DKL[Pθ k Pθ+δɑi ] ≈ 2φ E XJP (X) f '(Zk )(西)
k
2 ne
E	E,.
2φ	XJP (X)
k
f0(Zk)( - E gkWEiIWIExj--(ZE-(WEIzI)k)2
j γk	γk
2 ne
77Γ E E,,
2φ	XJP (X)
k
2 ne
777 E E,,
2φ	XJP (X)
k
d
f 0(Zk )( - E "WEiIWIExj (aki - 1))
j	γk
d
f0(Zk )(E( gk WEiIWIExj (aki T))2
j γk
d
+ E ((生WEiI)2WiExnWIExm(aki - 1)2)
γk
n6=m
δ2 ne d
——E E e
2φ k j XJP(X)
Q E E
+ 2φ ⅜n=rnXJP (x)
f O(Zk )( gk WEiIWIExj (aki - 1))2
γk
γk
ki - 1)2
(87)
24
Published as a conference paper at ICLR 2021
F Algorithms
Here we provide pseudo-code for implementation details. Please see the following link for code:
https://github.com/linclab/ltlwdp
F.1 Parameter initialization
Algorithm 1 Parameter initialization for DANNs
for layer L do
require ne, ni, d
WEE 〜exp(λE)
if ni = 1
WIE J n1e Pn=I WEE
WEI J 1
else:
WIE 〜 exp(λE)
WEI J 1/ni
end if __________
α j ɪ ∙ log(^^2√-)
g, β Jl
end for
Where number of excitatory output units is n。number of inhibitory units n%, and input dimensionality
d and λE = Pd(2π — 1)/√2π.
F.2 Parameter updates
For DANN parameter updates we used the algorithms detailed below. Note that gradients were
corrected as detailed in Section 4 and see Algorithm 3.
All below algorithms are computed using the loss gradients Vθ of parameter θ, in a given model
computed on a minibatch sample.
Algorithm 2 Parameter updates
Require learning rate η, updates ∆θ
for each layer l do
WEE — WEE - η∆wEE
WIE — WIE - η∆wIE
WEI — WEI - η∆WEI
α - α — η∆α
g J g 一 η∆g
一β - η∆β
EE J max(WEE, 0)
IE J max(WIE, 0)
EI J max(WEI, 0)
g J max(g, 0)
end for
βWW
F.3 DANN gradient correction algorithms
For the majority of experiments we scaled gradients using the heuristic correction terms derived in
Section 4 (and see Appendix E). In this case we applied the following algorithm before Algorithm 2.
25
Published as a conference paper at ICLR 2021
Algorithm 3 DANN gradient correction
for each layer l do
require ne, ni, d
△WEE J VWEE
△WIE J -ɪ VW
√ne
△WEI J d VWEI
△a J . 1——Vα
d ne
△g J Vg
∆β JVe
end for
IE
We also tested that our heuristic correction factors approximated gradient multiplication by the
diagonal of Ft-1 for each layer (see Figure 2).
Algorithm 4 Gradient correction by approximation of diag(F-1 )
Require learning rate η, fisher momentum k, fisher learning rate λ
for each batch (xt , yt) do
Compute p = softmax(z)
Compute cross entropy loss L(p, yt)
for each layer l do
vθ1 J 翁
end for
Sample y 〜Categorical(P)
Compute cross entropy loss L(p, y)
for each layer l do
F _	1 PIbatch|( ∂L )2
F = λ∙∖batch∖ 乙i	( ∂θl )i
Ft = kF+(1 - k)Ft-ι
Ft-1 = 1/Ft
Ft = F—1 ∙ 1/||FWEE|| where FWEE is the elements of F corresponding to WEE
∆θl J Ft*Vθl
end for
end for
Here we note this update can be considered very rough diagonal approximation to natural gradient
descent. In addition, various efficient approximations to natural gradient descent that have been
utilized such as KFAC Martens & Grosse (2015) could not be applied due to the structure of
DANNs, as the mathematical assumptions of KFAC, which were made for feedforward networks
with activations as matrix multiplications, do not apply.
F.4 Learning rate scaling and gradient normalisation
We also tested whether constraining the gradient norm and scaling the learning rate based on parameter
clipping improved DANN performance. For these experiments we applied the following algorithms.
Algorithm 5 Gradient normalisation
for each layer ' do
Require Vθ', M
if ∣∣VΘ'∣∣2 > M:
△* J M ∙ ∣W⅛
else:
△e' J vθ'
end if
end for
26
Published as a conference paper at ICLR 2021
Algorithm 6 Learning rate scaling
for each layer ' do
Require Vθ', M, ξ
i — 1
C J 0
while c < M:
η J ξi
c J CosineSimilarity(max(0,θ' - ηVθ'), θ' 一 ηVθ'))
iJi+1
end for
The learning rate scaling method temporarily reduces the learning rate whenever parameter clipping
would reduce the cosine of the angle, made between the gradient and actual updates, below a certain
constraint. For any optimization problems caused by actual clipped updates not following the gradient,
learning rate scaling is a principled way of following the direction of the gradient.
We also note, this technique can be generally applied to any other model which is constrained so that
it cannot have updates freely follow gradient descent. If the constrained parameter space is an open
subset of euclidean space, and we allow the learning rate to be arbitrarily small (Algorithm 6 with
limi→∞ ξi = 0), updates will always follow the direction of the gradient.
27