Published as a conference paper at ICLR 2021
Learning "What-if" Explanations
for Sequential Decision-Making
Ioana Bica
University of Oxford, Oxford, UK
The Alan Turing Institute, London, UK
ioana.bica@eng.ox.ac.uk
Daniel Jarrett
University of Cambridge, Cambridge, UK
daniel.jarrett@maths.cam.ac.uk
Alihan Huyuk
University of Cambridge, Cambridge, UK
ah2075@cam.ac.uk
Mihaela van der Schaar
University of Cambridge, Cambridge, UK
Cambridge Center for AI in Medicine, UK
University of California, Los Angeles, USA
The Alan Turing Institute, London, UK
mv472@cam.ac.uk
Ab stract
Building interpretable parameterizations of real-world decision-making on the basis
of demonstrated behavior—i.e. trajectories of observations and actions made by an
expert maximizing some unknown reward function—is essential for introspecting
and auditing policies in different institutions. In this paper, we propose learning
explanations of expert decisions by modeling their reward function in terms of
preferences with respect to “what if” outcomes: Given the current history of
observations, what would happen ifwe took a particular action? To learn these cost-
benefit tradeoffs associated with the expert’s actions, we integrate counterfactual
reasoning into batch inverse reinforcement learning. This offers a principled way
of defining reward functions and explaining expert behavior, and also satisfies
the constraints of real-world decision-making—where active experimentation is
often impossible (e.g. in healthcare). Additionally, by estimating the effects of
different actions, counterfactuals readily tackle the off-policy nature of policy
evaluation in the batch setting, and can naturally accommodate settings where the
expert policies depend on histories of observations rather than just current states.
Through illustrative experiments in both real and simulated medical environments,
we highlight the effectiveness of our batch, counterfactual inverse reinforcement
learning approach in recovering accurate and interpretable descriptions of behavior.
1 Introduction
Consider the problem of explaining sequential decision-making on the basis of demonstrated behavior.
In healthcare, an important goal lies in being able to obtain an interpretable parameterization of the ex-
perts’ behavior (e.g in terms of how they assign treatments) such that we can quantify and inspect poli-
cies in different institutions and uncover the trade-offs and preferences associated with expert actions
(James & Hammond, 2000; Westert et al., 2018; Van Parys & Skinner, 2016; Jarrett & van der Schaar,
2020). Moreover, modeling the reward function of different clinical practitioners can be revealing as
to their tendencies to treat various diseases more/less aggressively (Rysavy et al., 2015), which —in
combination with patient outcomes—has the potential to inform and update clinical guidelines.
In many settings, such as medicine, decision-makers can be modeled as reasoning about "what-if"
patient outcomes: Given the available information about the patient, what would happen if we took
a particular action? (Djulbegovic et al., 2018; McGrath, 2009). As treatments often affect several
patient covariates, by having both benefits and side-effects, decision-makers often make choices
based on their preferences over these counterfactual outcomes. Thus, in our case, an interpretable
explanation of a policy is one where the reward signal for (sequential) actions is parameterized on the
basis of preferences over (sequential) counterfactuals (i.e. "what-if" patient outcomes).
1
Published as a conference paper at ICLR 2021
U 10^01
UOlSSəjsojd OSEOSIa
History ht
E[Ut+ι[q] | ht]
E[Ut+ι[。] | ht]
Counterfactuals I	Z
A=axsosmoq∞∙0
sɔəjjə 9p∞
Counterfactuals
E[Zt+ι[ ] | ht]
E[Zt+ι[ ] | ht]
t
Q Action (Treatment) Q No Action ( No Treatment)
Reward function:	------------------J ----------------------------
R(ht, θ)=
R(ht, ∙)=
WUE[Ut+1 [∙] | ht] + WzE[Zt+ι[∙] | ht]
WUE[Ut+1 [ ] I ht]+ME[Zt+ι[ ] | ht]
Our goal: recover preferences over Counterfactual outcomes.
Figure 1: Explaining decision-making behaviour in terms of preferences over What if outcomes.
Consider the evolution of tumour volume (U) and side effects (Z) under a binary action. E [Ut+1 [at] |
ht] and E[Zt+1 [at] | ht] are the counterfactuals for the patient features under action at given
history ht of prior actions and covariates. Parameterizing the reWard as the Weighted sum of these
counterfactuals: R(ht, at) = wuE[Ut+1 [at] | ht] + wz E[Zt+1 [at] | ht], naturally alloWs us to model
the preferences of experts: e.g. finding that |wu| > |wz| indicates that the expert is treating more
aggressively, by placing more Weight on reducing tumour volume than on minimizing side effects.
Given the observations and actions made by an expert, inverse reinforcement learning (IRL) offers
a principled Way for modeling their behavior by recovering the (unknoWn) reWard function being
maximized (Ng et al., 2000; Abbeel & Ng, 2004; Choi & Kim, 2011). Standard solutions operate
by iterating on candidate reWard functions, solving the associated (forWard) reinforcement learning
problem at each step. In many real-World problems, hoWever, We are specifically interested in the
challenge of offline learning—that is, Where further experimentation is not possible—such as in
medicine. In this batch setting, We only have access to trajectories sampled from the expert policy in
the form of an observational dataset—such as in electronic health records.
Batch IRL. By their nature, classic IRL algorithms require interactive access to the environment, or
full knoWledge of the environment’s dynamics (Ng et al., 2000; Abbeel & Ng, 2004; Choi & Kim,
2011). While batch IRL solutions have been proposed by Way of off-policy evaluation (Klein et al.,
2011; 2012; Lee et al., 2019), they suffer from tWo disadvantages. First, they are limited by the
assumption that state dynamics are fully-observable and Markovian. This is hardly true in medicine:
treatment assignments generally depend on hoW patient covariates have evolved over time (Futoma
et al., 2020). Second, reWards are often parameterized as uninterpretable representations of neural
netWork hidden states and consequently cannot be used to explain sequential decision making.
"What-if" Explanations. To address these shortcomings and to obtain a parameterizable inter-
pretation of the expert’s behavior, We propose explicitly incorporating counterfactual reasoning
into batch IRL. In particular, We focus on “What if” explanations for modeling decision-making,
While simultaneously accounting for the partially-observable nature of patient histories. Under the
max-margin apprenticeship frameWork (Abbeel & Ng, 2004; Klein et al., 2011; Lee et al., 2019),
We learn a parameterized reWard function R(ht , at) that is defined as a Weighted sum over potential
outcomes (Rubin, 2005) for taking action at given history ht .
As highlighted in Figure 1, consider the decision making process of assigning a binary action given
the tumour volume (U) and side effects (Z). Let E[Ut+1 [at] | ht] and E[Zt+1 [at] | ht] be the
counterfactual outcomes for the tWo covariates When action at is taken given the history ht of
covariates and previous actions. We define the reWard as the Weighted sum of these counterfactuals:
R(ht, at) = wuE[Ut+1[at] | ht] + wz E[Zt+1 [at] | ht], to take into account the effect of actions and
to directly model the preferences of the expert. The ideal scenario is When both the tumour volume
and the side effects are zero, so the reWard Weights of a doctor aiming for this are both negative.
HoWever, recovering that |wu| > |wz |, it means that the doctor is treating more aggressively, as
they are focusing more on reducing the tumour volume rather than on the side effects of treatments.
Alternatively, |wu | < |wz | indicates that the side effects are more important and the expert is treating
less aggressively. Our motivation for using counterfactuals to define the reWard comes from the idea
that rational decision making considers the potential effects of actions (Djulbegovic et al., 2018).
Contributions. Exploring the synergy betWeen counterfactual reasoning and batch IRL for un-
derstanding sequential decision making confers multiple advantages. First, it offers a principled
approach for parameterizing reWard functions in terms of preferences over what-if patient outcomes,
Which enables us to explain the cost-benefit tradeoffs associated With an expert’s actions. Second,
by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of
2
Published as a conference paper at ICLR 2021
Method	Environment Batch Feature map for reward	Policy Feat. expectations
Abbeel & Ng (2004) Model-based		No	φ(st) = basis functions for state st	π(at	| st)	Model roll-outs
Choi & Kim (2011)	Model-based	No	Ps bt(s)φ(s, at) = basis for belief bt	π(at	| bt)	Model roll-outs
Klein et al. (2011)	Model-free	Yes	φ(xt) = basis functions for state xt	π(at	| xt)	LSTD-Q
Lee et al. (2019)	Model-free	Yes	φ(xt , at) = concat(φ(xt), at)	π(at	| xt)	DSFN
Ours	Model-free	Yes	φ(ht, at) = E[Yt+1[at]|ht]	π(at | ht)		Counterfactual μ-learning
Table 1: Comparison of our proposed method (batch, counterfactual IRL) with related works in IRL.
policy evaluation in the batch setting. Furthermore, we demonstrate that not only does this alleviate
the cold-start problem typical of conventional batch IRL solutions, but also accommodates settings
where the usual assumption of full observability fails to hold. Through experiments in both real and
simulated medical environments, we illustrate the effectiveness of our batch, counterfactual inverse
reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.
2	Related works
In our work, the aim is to explain decision-making by recovering the preferences of experts with
respect to the effects of their actions, denoted by the counterfactual outcomes. This goal is fundamen-
tally different from the goal of IRL methods which generally aim to match the performance of experts.
We operate under the standard max-margin apprenticeship framework (Ng et al., 2000; Abbeel & Ng,
2004), which searches for a reward function that minimizes the margin between feature expectations
of the expert and candidate policies. However, our approach to recovering and understanding decision
policies is uniquely characterized by incorporating counterfactuals to obtain explainable reward
functions. To tackle the challenges posed by real-world decision making, our method also operates in
an offline and model-free manner, and accommodates partially-observable environments.
Explainability. By using basis functions (Klein et al., 2012) or hidden layers of a deep network
(Lee et al., 2019) to define the feature map, the learned rewards of either approach are inherently
uninterpretable, and cannot be used to explain differences in expert behavior. An alternative approach
for recovering the expert policy (without reward functions) is imitation learning (Hussein et al., 2017;
Osa et al., 2018; Torabi et al., 2019; Jarrett et al., 2020). However, these methods do not allow us to
fully model the decision-making process of experts and to uncover the trade-offs behind their actions.
Batch Learning. Klein et al. (2011) propose an off-policy evaluation method based on least squares
temporal difference (LSTD-Q) (Lagoudakis & Parr, 2003) for estimating feature expectations, and
Klein et al. (2012) use a linear score-based classifier to directly approximate the Q-function offline.
However, both methods require the constraining assumptions that rewards are direct, linear functions
of fully-observable states—assumptions we cannot afford to make in realistic settings such as
medicine. Lee et al. (2019) propose a deep successor feature network (DSFN) based on Q-learning
to estimate feature expectations. But their approach similarly assumes fully-observable states, and
additionally suffers from the “cold-start” problem where off- policy evaluations are heavily biased
unless the initial candidate policy is (already) close to the expert.
Partial Observability. No existing batch IRL method accommodates modeling expert policies that
depend on patient histories. While Choi & Kim (2011) and Makino & Takeuchi (2012) extend the
apprenticeship learning paradigm to partially observable environments by considering policies on
beliefs over states, both need to interact with the environment (or a perfect simulator) during learning.
To the best of our knowledge, we are the first to propose explaining sequential decisions through
counterfactual reasoning and to tackle the batch IRL problem in partially-observable environments.
Our use of the estimated counterfactuals yields inherently interpretable rewards and simultaneously
addresses the cold-start problem in Lee et al. (2019). Table 1 highlights the main differences between
our method and the relevant related works. See Appendix A for additional related works.
3	Problem formulation
Preliminaries. At timestep t, let random variable Xt ∈ X denote the observed patient features and let
At ∈ A denote the action (e.g. treatment) taken, where Ais a finite set of actions. Let xt and at denote
realizations of these random variables. Let ht = (x0, a0, . . . , xt-1, at-1, xt) = (x0:t, a0:t-1) ∈ H
be a realization of the history Ht ∈ H of patient observations and actions until timestep t.
3
Published as a conference paper at ICLR 2021
A stationary stochastic policy represents a mapping: π : H × A → [0, 1], where π(a | h) indicates the
probability of choosing action a ∈ A given history h ∈ H and Pa∈A π(a | h) = 1. Taking action at
under history ht results in observing xt+1 and obtaining ht+1. The reward function is R : H×A → R
where R(h, a) represents the reward for taking action a ∈ A given history h ∈ H. The value function
of a policy π, V : H → R is defined as: V π (h) = E[Pt∞=0 γtR(Ht, At) | π, H0 = h], where
Y ∈ [0,1) is the discount factor and At 〜∏(∙ | Ht) for t ≥ 0. The action-value function
Q : H × A → R of a policy is defined as Qπ (h, a) = E[Pt∞=0 γtR(Ht, At) | π, H0 = h, A0 = a]
where At 〜∏(∙ | Ht) for t ≥ 0. A higher Q-value indicates that action a will yield better long
term returns if taken for history h. We assume we know the discount factor γ which indicates the
importance of future rewards for the current history and action pair.
Batch IRL. Let D = {ζi}iN=1 be a batch observational dataset consisting of N patient trajectories:
ζi = (xi0, ai0, . . . xiTi-1, aiTi-1, xiTi). The trajectory ζi for patient i consists of covariates xti and
actions ati observed for Ti timesteps. For simplicity, we drop the superscript i unless explicitly needed.
The actions at ∈ D are assigned according to some expert policy ∏e such that at 〜∏e(∙ | ht).
We work in the apprenticeship learning set-up (Abbeel & Ng, 2004) and we consider a linear
reward function R(ht,at) = W ∙ φ(ht,αQ, where the weights W ∈ Rd satisfy kwk1 ≤ 1. The
feature map φ : HX A → Rd also satisfies ∣∣φ(∙)∣∣2 ≤ 1 such that the reward is bounded. We
assume that the expert policy πE is attempting to optimize, without necessarily succeeding, some
unknown reward function R*(ht, a/ = w* ∙ φ(ht, aj, where w* are the 'true' reward weights. Given
R(ht, at), the value of policy π can be re-written as: E[Vπ (Ho)] = E[P∞=o γtw ∙ φ(H, At) | π]=
w ∙ E[P∞=o γtφ(Ht, At) | ∏], where the expectation is taken with respect to the sequence of histories
and action pairs (Ht, At)t≥0 obtained by acting according to π. The feature expectation of policy
π, defined as the expected discounted cumulative feature vector obtained when choosing actions
according to policy π is μπ = E[P∞=o γtφ(Ht, At) | π] ∈ Rd such that: E[Vπ(Ho)] = W ∙ μπ.
Our aim is to recover the expert weights w* as well as find a policy π that is close to the policy of
the expert πE . We take the max-margin IRL approach and we measure the similarity between the
feature expectations of the expert’s policy and the feature expectations of a candidate policy using
∣∣μπE 一 μπ k2. In this batch IRL setting, we do not have knowledge of transition dynamics and we
cannot sample more trajectories from the environment. Note that in this context, we are the first to
model expert policies that depend on patient histories and not just current observations.
Counterfactual reasoning. To explain the expert’s behaviour in terms of their trade-off associated
with "what if" outcomes, we use counterfactual reasoning to define the feature map φ(ht, at) part
of the reward R(ht,at) = W ∙ φ(ht, at). We adopt the potential outcomes framework (Neyman,
1923; Rubin, 1978; Robins & Herndn, 2008). Let Y [a] be the potential outcome, either factual or
counterfactual, for treatment a ∈ A. Using the dataset D we learn feature map φ(ht, at) such that:
φ(ht, at) = E[Yt+1 [at] | ht],	(1)
where E[Yt+1[at] | ht] is the potential outcome for taking action at at time t given the history ht.
For the factual action at, assigned under policy ∏(∙ | ht), the factual outcome is xt+1 and this is the
same as the potential outcome E[Yt+1 [at] | ht]. The potential outcomes for the other actions at ∈ A
are the counterfactual ones and they allow us to understand what would happen to the patient if they
receive a different treatment at . To identify the potential outcomes from the batch data we make the
standard assumptions of consistency, positivity and no hidden confounders as described in Appendix
B. No hidden confounders means that we observe all variables affecting the action assignment and
potential outcomes. Overlap means that at each timestep, every action has a non-zero probability and
can be satisfied in this setting by having a stochastic expert policy. These assumptions are standard
across methods for estimating counterfactual outcome (Robins et al., 2000; Schulam & Saria, 2017;
Bica et al., 2020a). Note that these assumptions are needed to be able to reliably perform causal
inference using observational data. However, they do not constrain the batch IRL set-up.
Estimating the potential outcomes from batch data poses additional challenges that need to be
considered. The fact that the expert follows policies that consider the history of patient observations
when deciding new actions, gives rise to time-dependent confounding bias. Standard supervised
learning methods for learning E[Yt+1 [at] | ht] from D will be biased by the expert policy used in the
observational dataset and will not be able to correctly estimate the counterfactual outcomes under
alternative policies (Schulam & Saria, 2017). Methods for adjusting for the confounding bias involve
4
Published as a conference paper at ICLR 2021
using either inverse probability of treatment weighting (Robins et al., 2000; Lim et al., 2018) or
building balancing representations (Bica et al., 2020a). Refer to Appendix B for more details.
In the sequel, we consider the model for estimating counterfactuals as a black box such that the
feature map φ(ht, at) represents the effect of taking action at for history ht. The reward is then:
R(ht,at) = W ∙ φ(ht,at) = W ∙ E[Yt+ι[at] [ ht]	(2)
Defining the reward function using counterfactuals gives an interpretable parameterization of doctor
behavior: It allows us to interpret their behavior with respect to the importance weights implicitly
assigned to the effects of their actions. This enables describing the relative trade-offs in treatment
decisions. Note that we are not assuming that the experts themselves actually compute these quantities
(nor that they explicitly adopt the same causal inference assumptions); rather, we are simply providing
a way to understand how decision-makers are effectively behaving (i.e. in terms of counterfactuals).
4	Batch inverse reinforcement learning using counterfactuals
Max-margin IRL (Abbeel & Ng, 2004) starts with an initial random policy π and iteratively performs
the following three steps to recover the expert policy and its reward weighs: (1) estimate feature
expectations μπ of candidate policy π, (2) compute new reward weights W and (3) find new candidate
policy π that is optimal for reward function R(ht, at) = W ∙ φ(ht, at). This approach finds a policy
π that satisfies ∣∣μπe - μπ∣∣2 < C such that π has an expected value function close the expert policy.
The expert feature expectations can be estimated
empirically from the dataset D using:
1	N τi
μπE = N XXYtΦ(ht,at).	⑶
i=1 t=0
In the batch setting, we cannot estimate the fea-
ture expectations of candidate policies by taking
the sample mean of on-policy roll-outs: μπ =
N Pi=ι PT=0 γtφ(ht, Mht)). To address thisoff-
policy nature of estimating feature expectations, we
introduce a new method that leverages the estimated
counterfactuals. We also make use of the counterfac-
tuals to learn optimal policies for different reward
weights. Figure 2 illustrates how we integrate "what
if" reasoning into batch IRL.
Figure 2: Counterfactual inverse reinforcement
learning (CIRL). Counterfactuals are used to
define φ(h, a), to estimate feature expectations
μπ of candidate policy π in batch setting and
to learn optimal policy for reward weights W.
4.1	Counterfactual "-learning
Similar to the approach proposed by Klein et al. (2012); Lee et al. (2019), we consider a history-action
feature expectation defined as follows μπ(h, a) = E[£∞=0 Ytφ(Ht, At)∣π, H0 = h, A0 = a], where
the first action a can be chosen randomly and for t ≥ 1, At ~ π(∙ | Ht). This can be re-written as:
∞
μπ (h,a) = φ(h,a) + Eh/w ~π(∙∣h∕) [£ Y t Φ(Ht ,At) | ∏,Hι = h', Ai = a ]	(4)
t=1
=Φ(h,α) + γEho,a0~∏(.∣ho)[μπ(h', a')],	(5)
where h0 is the next history. Notice the analogy between μπ (h, a) and the action-value function:
∞
Qπ(h, a) = R(h, a) + Eh0,a0~∏(∙∣h0)[£ YtR(Ht, At) | ∏, Hi = h', Ai = a']	(6)
t=1
=R(h, a) + γEh/,。，~∏(.∣ho)[Qπ(h', a')],	(7)
that allows us to use temporal difference learning to estimate feature expectations (Sutton et al., 1998).
Existing methods for estimating feature expectations fall into two extremes: (1) model-based (online)
IRL approaches learn a model of the world and then use the model as a simulator to obtain on-policy
roll-outs (Abbeel & Ng, 2004) and (2) batch IRL approaches use Q-learning (or alternative methods)
5
Published as a conference paper at ICLR 2021
Algorithm 1 (Batch, Max-Margin) CIRL
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
Input: Batch dataset D, max iterations n, convergence threshold ,
feature map φ(ht, at) = E[Yt+1[at]|ht]
μπE J compute ∏e's feature expectations (Equation 3)
wo J random initial reward weights, ∏o J compute optimal policy for Ro = wo ∙ φ
μπ0 J compute no,s feature expectations	(COUnterfaCtUaI μ-learning)
∏ = {no}, ∆ = {μπ0 },μo = μπ0
for k = 1 to n do
Wk = μπE - μk-ι, ∏k J compute optimal policy for Rk = Wk ∙ φ
μπk J compute ∏k's feature expectations
Π = Π ∪{∏k}, ∆ = ∆ ∪{μπ4
Orthogonally project μπE onto line through μk-ι, μπk:
μk
(counterfacual μ-learning)
11:
12:
13:
14:
(μπ - μk-I)T(μπE - μk-1)
(μπk - μk-i)T(μπk - μk-i)
if t < then break
(μπk - μk-1) + μk-1,
t = kμπE — μk k2
end for
∙-v
K = argminkw∏k∈∆ ∣∣μπE — μπk∣∣2, R(h,a) = WK ∙ φ(h,a)
Output: R, ∆, Π
for off-policy evaluation (Lee et al., 2019), and can only be used to evaluate policies similar to the
expert policy and require warm start. In our case, the counterfactual model allows us to compute
h0 = (h, a, E[Y (a)|h]) for any h ∈ D and any arbitrary action a. Thus, we propose counterfactual
μ-learning, a novel method for estimating feature expectations that uses these COUnterfaCtUaIS as part
of temporal difference learning with 1-step bootstrapping. This approach falls in-between (1) and (2)
and allows us to estimate feature expectations for any candidate policy n in the batch IRL setting.
The counterfactual μ-learning algorithm learns the μ-values for policy n iteratively by updating the
current estimates of the μ-values with the feature map plus the μ-values obtained by following policy
n in the new counterfactual history h0 = (h, a, E[Y [a]|h]):
μπ(h, a)	J	μπ(h, a)	+	α(φ(h,	a)	+ YE°o〜冗(.々)[μπ(h0, a0)]	—	μπ(h, a)),	(8)
where α is the learning rate. We use a recurrent network with parameters θ to approximate μπ (h, a | θ)
and we train it by minimizing the sequence of loss functions Li which changes for every iteration i:
Li(θi) = Eh〜DMyi- μπ(h,a | θi)∣∣2]	Θi+1 — θi + αV(Li(θi)),	(9)
where the action a can be chosen randomly from A and yi = φ(h, a)+γEa0 〜∏(.∣h√)[μπ (h0,a0 | θi-ι)]
is the target for iteration i. The parameters for the previous iteration θi-1 are held fixed when optimiz-
ing Li(θi). Refer to Appendix C for full details of the counterfactual μ-learning algorithm. The feature
expectations for the policy n are given by μπ = Eh0 ,a0〜∏(∙∣ h0)[μπ (Ho, Ao)], which can be estimated
empirically from the observational dataset D using μπ = 得 PN=I Pa∈A μπ(h⅛, a)n(a | ho).
4.2	Finding optimal policy for given reward weights
During each iteration of max-margin IRL, we obtain a candidate policy to evaluate by finding the
optimal policy for a given vector of reward weights. We use deep recurrent Q-learning (Hausknecht &
Stone, 2015), a model-free approach for learning Q-values for reward function R(h, a) = W ∙ φ(h, a).
The counterfactuals are used to compute φ(h, a), and to estimate the next history for the temporal
difference updates. See Appendix D for details. After estimating the Q—values, Q(h, a) for history
h and action a, a new candidate policy is obtained using: n(a | h) = 10=a唔 maxct0 Q(h,a').
4.3	Counterfactual inverse reinforcement learning algorithm (CIRL)
Algorithm 1 describes our proposed counterfactual inverse reinforcement learning (CIRL) method for
the batch setting. CIRL is based on the projection algorithm proposed by Abbeel & Ng (2004) and
iteratively updates the reward weights to minimize the margin between the expert’s feature expecta-
tions and the feature expectations of intermediate policies. CIRL uses our proposed counterfactual
μ-learning algorithm for estimating the feature expectations of intermediate policies in an off-policy
manner that is suitable for the batch setting. Compared to the algorithm for batch IRL proposed by
6
Published as a conference paper at ICLR 2021
Lee et al. (2019) that requires the initial policy π0 to already be similar to the expert policy, CIRL
works for any initial policy π that is optimal for the randomly initialized reward weights w0 .
∙-v
Similarly to Choi & Kim (2011), the CIRL algorithm returns the reward function R(h, a) that
results in a policy with feature expectations closest to the ones of the expert policy. We show
experimentally that the reward that yields the closest feature expectations will be similar to the true
underlying reward function of the expert. CIRL returns the set of policies tried Π and their feature
expectations ∆, which allows us to compute a mixing policy that would yield similar performance to
the expert policy (Abbeel & Ng, 2004). Let μ be the closest point to μπE in the convex closure of
∆ = {μπ0, μπ1 ... μπk}, which can be computed by solving the quadratic programming problem:
min ∣∣μπE - μ∣∣2 s.t. μ = X λiμπi ,λ ≥ 0, X λi = 1.	(10)
From the termination criteria of Algorithm 1, μπE is separated from the points μπi by a margin of
at most e. Thus, the solution μ will satisfy ∣∣μπE - μk2 ≤ e. To obtain a policy that is close to
the performance of the expert policy, we mix together the policies Π = {π0, . . . πk} returned by
Algorithm 1, where the probability of selecting πi is λi .
5	Experiments
We evaluate the ability of CIRL to recover the preferences of experts over the "what if" outcomes
of actions. These preferences are denoted by the magnitude of the recovered reward weights. Since
we do not have access to the underlying reward weights of experts in real data, we first validate the
method in a simulated environment. To show the applicability of CIRL in healthcare, we also perform
a case study on an ICU dataset from the MIMIC III database (Johnson et al., 2016).
Benchmarks. For our proposed CIRL method, we uses the Counterfactual Recurrent Network, a
state-of-the art model for estimating counterfactual outcomes in a temporal setting (Bica et al., 2020a).
Note that other models for this task are also applicable (Lim et al., 2018). Refer to Appendix F
for details. We benchmark CIRL against MB-IRL: model-based IRL—i.e. inverse reinforcement
learning with model-based policy evaluation (e.g. Yin et al. (2016); Nagabandi et al. (2018); Kaiser
et al. (2019); Buesing et al. (2018)). We consider two versions of this benchmark: MB(h)-IRL which
uses the patient history and MB(x)-IRL which only uses the current observations to define the policy.
For both MB(x)-IRL and MB(h)-IRL we define the reward as a weighted sum of counterfactual
outcomes, but these methods use instead standard supervised methods to estimate the next history h0
needed for the counterfactual μ-learning algorithm. These benchmarks are aimed to highlight the
need of handling the bias from the time-dependent confounders and using a suitable counterfactual
model, but also the importance of handling the patient history. We also compare against the deep
successor feature networks (DSFN) proposed by Lee et al. (2019), which currently represents the
state-of-the-art Batch IRL for the MDP setting. To show that their approach for estimating feature
expectations in the batch setting is suboptimal, we extend their method to also incorporate histories
in the DSFN(h) benchmark. Implementation details of the benchmarks can be found in Appendix G.
5.1	Extracting different types of expert behaviour
Simulated environment. We propose an environment that uses a general data simulation involving
p-order auto-regressive processes. To analyze different types of expert behaviour (e.g. treating
more/less aggressively) we simulate data for patient features representing disease progression (x),
e.g. tumour volume and side effects (z) and action (a) indicating the binary application of treatment.
For time t, we model the evolution of patient covariates according to the treatments as follows:
xt
1p	p
P ∑^xt-i - 2.5 工at-i + 0.5p +
1p	p
Zt = P 工 zt-i + 0.5 工 at-i - p + η
(11)
whereP = 5 and E 〜N(0,0.12), η 〜N(0,0.12) are noise terms. The initial values for the features
are sampled as follows: x。〜N(30, 5) and z0 〜N(2,1). We set Xmax = 50 and Zmax = 15. The
trajectory of the patient terminates when either xt = 0, xt ≥ xmax, zt ≥ zmax or t ≥ 20. The
tumour volume xt , denoting the disease progression, decreases when we give treatment and increases
otherwise. Conversely, the side effects zt , increase when we give treatment and decrease otherwise.
We define a linear reward for taking action at given history ht = (x0:t, z0:t, a0:t-1) as follows:
xt+1	zt+1
R(ht ,at) = wι------------H W2-----------
xmax - xmin	zmax - zmin
(12)
7
Published as a conference paper at ICLR 2021
where w = [w1, w2], ||w||1 ≤ 1 and xt+1 and zt+1 are simulated according to equations 11 to take
into account the effect of action at for history ht. The features are normalized to [0, 1]. The best
scenario for a patient is when both the side effects and tumour volume are zero and a doctor attempting
to achieve this will have negative reward weights. However, different settings of the reward weights
will result in different expert behaviours. For instance, for w1 = -0.7 and w2 = -0.3, the expert
policy will focus more on the disease progression and will consequently treat more aggressively,
while this behaviour will be reversed for reward weights set to w1 = -0.3 and w2 = -0.7. We
used deep recurrent Q-learning (Hausknecht & Stone, 2015) to find a stochastic expert policy that
optimizes the reward function for different settings of the weights. The batch dataset D consists of
10000 trajectories sampled from the expert policy. Refer to Appendix E for more details.
Recovering decision making preferences of experts.
We first evaluate the benchmarks on their ability to re-
cover the weights of the reward function optimized by
the expert for the experimental setting with γ = 0.99,
w1 = -0.3 and w2 = -0.7. Note that DSFN does not
provide interpretable reward weights, and thus cannot
be used for understanding the trade-offs in the expert
behavior. We train each benchmark 10 times and we plot
in Figure 3 the reward weights obtained for the different
iterations. We show that our proposed CIRL method
performs best at recovering the preferences of the expert,
which in this case is to treat less aggressively. While the
CIRL: Wj CIRL: WZ MB(ħ)-IRL: Wl MBφ>IRL: IV2 MBOo∙IRL: WI MBgIRL: W2
Reward weights
Figure 3: Reward weights recovered by
benchmarks over 10 runs. The weights of
the expert are w1 = -0.3 and w2 = -0.7.
MB(h)-IRL method also recovers the correct trade-offs in the expert behavior, the computed weights
have a much higher variance. Conversely, the MB(x)-IRL method, which does not consider the
patient history fails to recover the underlying weights of the expert policy.
Matching the expert policy. We evaluate the benchmarks’ ability to recover policies that match the
performance on the expert policy for two settings of the discount factor γ ∈ {0.99, 0.5}. A lower γ
indicates that the expert is optimizing for the immediate effect of actions, while a higher γ means they
considered the long term effect of actions. For each γ we learn expert policies for different reward
weights and we use the expert policies to generate the batch datasets. We evaluate the policies learned
by the benchmarks using two metrics: cumulative reward for running the policy in the simulated
environment and accuracy on matching the expert policy (computed as described in Appendix H). We
report in Tables 2 and 3 the average results and their standard error over 1000 sampled trajectories
from the environment. CIRL recovers a policy that has the closest cumulative reward to the expert
policy and that can best match the treatments assigned by the expert.
Table 2: Mean cumulative reward and standard deviation for running learnt policy in the environment.
Y = 0.99	I	Y = 0.5
Reward	wι = -0.3	w1 = -0.7	w1 = -0.5	wι = -0.3	wι = -0.7	w1 = -0.5
weights	W2 = -0.7	w2 = -0.3	W2 = -0.5	W2 = -0.7	W2 = -0.3	W2 = -0.5
MB(x)-IRL	-3.78 ± 0.02	-4.42 ± 0.05	-4.90 ± 0.05	-4.51 ± 0.05	-4.53 ± 0.05	-4.54 ± 0.04
MB(h)-IRL	-3.23 ± 0.02	-4.10 ± 0.02	-4.63 ± 0.04	-4.43 ± 0.04	-3.54 ± 0.05	-4.35 ± 0.03
DSFN	-3.56 ± 0.06	-4.32 ± 0.04	-3.77 ± 0.05	-4.11 ± 0.06	-3.07 ± 0.03	-4.67 ± 0.07
DSFN(h)	-3.31 ± 0.07	-4.33 ± 0.07	-3.60 ± 0.07	-3.95 ± 0.07	-3.05 ± 0.05	-4.61 ± 0.05
CIRL	-2.89 ± 0.02	-3.92 ± 0.03	-3.41 ± 0.05	-2.79 ± 0.02	-2.91 ± 0.02	-4.27 ± 0.03
Expert	-2.72 ± 0.02	-3.61 ± 0.02	-2.81 ± 0.01	-2.65 ± 0.02	-2.36 ± 0.01	-3.97 ± 0.03
Table 3: Average accuracy and standard deviation for matching the actions in the expert policy.
γ = 0.99	I	γ = 0.5
Reward weights	w1 = -0.3 w2 = -0.7	wι = -0.7 W2 = -0.3	w1 = -0.5 W2 = -0.5	wι = -0.3 W2 = -0.7	wι = -0.7 w2 = -0.3	w1 = -0.5 W2 = -0.5
MB(X)-IRL	62.5 ± 0.41%	61.4 ± 0.81%	54.6 ± 0.56%	52.4 ± 0.63%	60.1 ± 0.39%	71.8 ± 0.72%
MB(h)-IRL	77.8 ± 0.31%	70.2 ± 0.45%	71.4 ± 0.69%	66.3 ± 0.58%	70.2 ± 0.71%	75.6 ± 0.52%
DSFN	75.4 ± 0.32%	68.4 ± 0.21%	73.4 ± 0.45%	80.2 ± 0.37%	70.8 ± 0.24%	69.8 ± 0.44%
DSFN(h)	76.3 ± 0.37%	67.5 ± 0.32%	80.6 ± 0.54%	80.4 ± 0.56%	71.0 ± 0.35%	70.2 ± 0.47%
CIRL	81.8 ± 0.42%	75.5 ± 0.51%	83.7 ± 0.76%	89.5 ± 0.37%	73.2 ± 0.43%	80.4 ± 0.42%
8
Published as a conference paper at ICLR 2021
5.2 Case study on real-world dataset: MIMIC III
Suppose we want to explain the decision-making process of doctors assigning antibiotics to patients
in the ICU. For this purpose, we consider a dataset with 6631 patients that have received antibiotics
during their ICU stay, extracted from the Medical Information Mart for Intensive Care (MIMIC III)
database (Johnson et al., 2016).
We used CIRL, with γ = 0.99, to recover the policy and the re-
ward weights of doctors administering antibiotics to understand
their preferences over the effect of antibiotics on the patient fea-
tures. The relative magnitude of the reward weights for the
counterfactual outcomes of the patient features considered are
illustrated in Figure 4. CIRL found that reducing temperature
had the highest weight in the reward function of the expert, fol-
lowed by WBC. This corresponds to known medical guidelines
(Marik, 2000; Palmer et al., 2008). Sepsis is a leading cause
of morbidity and mortality in the ICU, and several studies have
shown that early administratio of antibiotics is crucial to decrease
the risk of adverse outcomes (Zahar et al., 2011). Fever and elevated
WBC are among a small subset of variables that indicate a systemic
inflammatory response, concerning for the development of sepsis
(Neviere et al., 2017). While these findings are not specific to bacte-
rial infection, the risk of failing to treat a potentially serious infection
often outweighs the risk of inappropriate antibiotic administration,
thereby driving clinicians to prescribe antibiotics in the setting of
these abnormal findings. Similarly, the decision to discontinue an-
tibiotics is complex, but it is often supported by signs of a resolution
Figure 4: Radar plot of reward
weights magnitude for assign-
ing antibiotics.
		Accuracy
MB(x)-IRL	70.1 ± 0.11%
MB(h)-IRL	77.5 ± 0.15%
DSFN	73.5 ± 0.25%
DSFN(h)	75.3 ± 0.19%
CIRL	83.4 ± 0.17%
Table 4: Accuracy on match-
ing expert actions.
of infection which included normalization of body temperature and a downtrending of the WBC. As
such, our finding that the two highest reward weights for the administration of antibiotics in the ICU
is temperature and WBC is consistent with clinical practice. Moreover, note that the model is simply
identifying the factors that are driving the decision-making of the clinician represented in this dataset.
In Table 4, to verify that explainability does not come at the cost of accuracy we also evaluate the
benchmarks on matching the expert actions.
6 Discussion
In this paper, we propose building interpretable parametrizations of sequential decision-making by
explaining an expert’s behaviour in terms of their preferences over "what-if" outcomes. To achieve
this, we introduce CIRL, a new method that incorporates counterfactual reasoning into batch IRL:
counterfactuals are used to define the feature map part of the reward function, but also to tackle the
off-policy nature of estimating feature expectations in the batch setting. The reward weights recovered
by CIRL indicate the relative preferences of the expert over the counterfactual outcomes of their
actions. Our aim is to provide a description of behavior, i.e. how the expert is effectively behaving
under our interpretable parameterization of the reward based on counterfactuals. We are not assuming
that the experts actually operate under this specific (linear, in our case) model or that they compute
the exact counterfactuals. Instead, our purpose is to show that we can explain an agent’s behavior
on the basis of counterfactuals, which is useful in that it allows us to audit them, sanity-check their
policies and find variation in practice. Further discussion can be found in Appendix I.
There are several limitations of your method and directions for future work. While our method
considers reward functions that are linear in the features, one way of extending it to handle more
complex reward functions is to use domain knowledge to define the feature map as a non-linear
function over the counterfactual outcomes. Nevertheless, these functions should be defined in a way
that still allows us to obtain interpretable explanations of the expert’s behaviour. Moreover, although
time-invariant reward weights w are standard in the IRL literature (Abbeel & Ng, 2004; Choi & Kim,
2011; Lee et al., 2019), time-variant rewards/policies have been considered in the dynamic treatment
regimes (reinforcement learning) literature (Chakraborty, 2013; Zhang & Bareinboim, 2019). Thus
another direction for future work would be to extend our method to consider non-stationary policies
and reward weights that can change over time.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We would like to thank the reviewers for their valuable feedback. The research presented in this paper
was supported by The Alan Turing Institute, under the EPSRC grant EP/N510129/1, by Alzheimer’s
Research UK (ARUK), by the US Office of Naval Research (ONR), and by the National Science
Foundation (NSF) under grant numbers 1407712, 1462245, 1524417, 1533983, and 1722516. The
authors are also grateful to Brent Ershoff for the insightful discussions and help with interpreting the
medical results on MIMIC III.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Ioana Bica, Ahmed Alaa, James Jordon, and Mihaela van der Schaar. Estimating counterfactual
treatment outcomes over time through adversarially balanced representations. In International
Conference on Learning Representations, 2020a.
Ioana Bica, Ahmed M Alaa, Craig Lambert, and Mihaela van der Schaar. From real-world patient
data to individualized treatment effects using machine learning: Current and future methods to
address underlying challenges. Clinical Pharmacology & Therapeutics, 2020b.
Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp.
182-189, 2011.
Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste
Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search.
International Conference on Representation Learning (ICLR), 2018.
Benjamin Burchfiel, Carlo Tomasi, and Ronald Parr. Distance minimization for reward learning from
scored trajectories. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Bibhas Chakraborty. Statistical methods for dynamic treatment regimes. Springer, 2013.
Jaedeug Choi and Kee-Eung Kim. Inverse reinforcement learning in partially observable environments.
Journal of Machine Learning Research, 12(Mar):691-730, 2011.
Benjamin Djulbegovic, Shira Elqayam, and William Dale. Rational decision making in medicine:
implications for overuse and underuse. Journal of evaluation in clinical practice, 24(3):655-665,
2018.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-
ment learning. arXiv preprint arXiv:1710.11248, 2017.
Joseph Futoma, Michael C Hughes, and Finale Doshi-Velez. Popcorn: Partially observed predic-
tion constrained reinforcement learning. International Conference on Artificial Intelligence and
Statistics (AISTATS), 2020.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 AAAI Fall Symposium Series, 2015.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A
survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1-35, 2017.
Brent C James and M Elizabeth H Hammond. The challenge of variation in medical Practice. Archives
of pathology & laboratory medicine, 124(7):1001-1003, 2000.
10
Published as a conference paper at ICLR 2021
Daniel Jarrett and Mihaela van der Schaar. Inverse active sensing: Modeling and understanding
timely decision-making. International Conference on Machine Learning (ICML), 2020.
Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-
based distribution matching. Advances in Neural Information Processing Systems, 2020.
Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a
freely accessible critical care database. Scientific data, 3:160035, 2016.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Edouard Klein, Matthieu Geist, and Olivier Pietquin. Batch, off-policy and model-free apprenticeship
learning. In European Workshop on Reinforcement Learning, pp. 285-296. Springer, 2011.
Edouard Klein, Matthieu Geist, Bilal Piot, and Olivier Pietquin. Inverse reinforcement learning
through structured classification. In Advances in neural information processing systems, pp.
1007-1015, 2012.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Donghun Lee, Srivatsan Srinivasan, and Finale Doshi-Velez. Truly batch apprenticeship learning
with deep successor features. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI-19, pp. 5909-5915. International Joint Conferences on Artificial
Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/819. URL https://doi.org/
10.24963/ijcai.2019/819.
Bryan Lim, Ahmed Alaa, and Mihaela van der Schaar. Forecasting treatment responses over time
using recurrent marginal structural networks. In Advances in Neural Information Processing
Systems, pp. 7493-7503, 2018.
Takaki Makino and Johane Takeuchi. Apprenticeship learning for model parameters of partially
observable environments. arXiv preprint arXiv:1206.6484, 2012.
Mohammad Ali Mansournia, Goodarz Danaei, Mohammad Hossein Forouzanfar, Mahmood Mah-
moodi, Mohsen Jamali, Nasrin Mansournia, and Kazem Mohammad. Effect of physical activity
on functional performance and knee pain in patients with osteoarthritis: analysis with marginal
structural models. Epidemiology, pp. 631-640, 2012.
Paul E Marik. Fever in the icu. Chest, 117(3):855-869, 2000.
Brent M McGrath. How doctors think, 2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynam-
ics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Remi Neviere, Polly E Parsons, and Geraldine Finlay. Sepsis syndromes in adults: Epidemiology,
definitions, clinical presentation, diagnosis, and prognosis. Monografla en Internet]. Wolters
Kluwer: UpToDate, 2017.
Jersey Neyman. SUr les applications de la th6orie des ProbabiliteS aux experiences agricoles: Essai
des principes. Roczniki Nauk Rolniczych, 10:1-51, 1923.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 663-670, 2000.
11
Published as a conference paper at ICLR 2021
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al.
An algorithmic perspective on imitation learning. Foundations and TrendsR in Robotics, 7(1-2):
1-179, 2018.
Lucy B Palmer, Gerald C Smaldone, John J Chen, Daniel Baram, Tao Duan, Melinda Monteforte,
Marie Varela, Ann K Tempone, Thomas O’Riordan, Feroza Daroowalla, et al. Aerosolized
antibiotics and ventilator-associated tracheobronchitis in the intensive care unit. Critical care
medicine, 36(7), 2008.
Robert W Platt, Enrique F Schisterman, and Stephen R Cole. Time-modified confounding. American
journal of epidemiology, 170(6):687-694, 2009.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7,
pp. 2586-2591, 2007.
James M Robins and MigUel A Herndn. Estimation of the causal effects of time-varying exposures.
In Longitudinal data analysis, pp. 547-593. Chapman and Hall/CRC, 2008.
James M Robins, Miguel Angel Hernan, and Babette Brumback. Marginal structural models and
causal inference in epidemiology, 2000.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41-55, 1983.
Donald B Rubin. Bayesian inference for causal effects: The role of randomization. The Annals of
statistics, pp. 34-58, 1978.
Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469):322-331, 2005.
Matthew A Rysavy, Lei Li, Edward F Bell, Abhik Das, Susan R Hintz, Barbara J Stoll, Betty R
Vohr, Waldemar A Carlo, Seetha Shankaran, Michele C Walsh, et al. Between-hospital variation in
treatment and outcomes in extremely preterm infants. N Engl J Med, 372:1801-1811, 2015.
Peter Schulam and Suchi Saria. Reliable decision support using counterfactual models. In Advances
in Neural Information Processing Systems, pp. 1697-1708, 2017.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from observa-
tion. arXiv preprint arXiv:1905.13566, 2019.
Jessica Van Parys and Jonathan Skinner. Physician practice style variation—implications for policy.
JAMA internal medicine, 176(10):1549-1550, 2016.
Gert P Westert, Stef Groenewoud, John E Wennberg, Catherine Gerard, Phil DaSilva, Femke Atsma,
and David C Goodman. Medical practice variation: public reporting a first necessary step to spark
change. International Journal for Quality in Health Care, 30(9):731-735, 2018.
Hang Yin, PatriCia Alves-Oliveira, Francisco S Melo, Aude Billard, and Ana Paiva. Synthesizing
robotic handwriting motion by learning from human demonstrations. In Proceedings of the 25th
International Joint Conference on Artificial Intelligence, number CONF, 2016.
Jean-Ralph Zahar, Jean-Francois Timsit, Maite Garrouste-Orgeas, Adrien Francais, AUrelien Vesim,
Adrien Descorps-Declere, Yohann Dubois, Bertrand Souweine, Hakim Haouache, Dany Goldgran-
Toledano, et al. Outcomes in severe sepsis and patients with septic shock: pathogen species and
infection sites are not associated with mortality. Critical care medicine, 39(8):1886-1895, 2011.
Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment
regimes. In Advances in Neural Information Processing Systems, pp. 13401-13411, 2019.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. 2008.
12
Published as a conference paper at ICLR 2021
A	Additional related works
Alternative approaches for IRL require a known MDP or POMDP model. In this context, several
Bayesian approaches to IRL have also been proposed which make use of the know transition dynamics
provided by the model of the environment (Ziebart et al., 2008; Ramachandran & Amir, 2007). We
do not assume access to a MDP or POMDP model. Makino & Takeuchi (2012) propose a method
for estimating the unknown parameters of a POMDP model of the environment; however their
method is only applicable to discrete observation spaces. We do not make any assumptions about
the observation space. While several model-free IRL methods have also been developed, these are
also not applicable in the batch setting where we only have access to a fixed set of expert trajectories.
Relative Entropy IRL (Boularias et al., 2011) requires non-expert trajectories following arbitrary
policy; such experimentation to obtain non-expert trajectories is not possible in the healthcare setting.
Distance Minimization IRL (DM-IRL) (Burchfiel et al., 2016) learns reward functions from scores
assigned by experts to sub-optimal demonstrations. Moreover, guided cost learning proposed by Finn
et al. (2016) which performs model-free maximum entropy optimization also requires the ability to
sample trajectories from the environment. Similarly, adversarial IRL, proposed by Fu et al. (2017)
builds upon the maximum entropy framework in Ziebart et al. (2008), but also requires the ability to
execute trajectories from candidate policies.
B Counterfactual estimation and time-dependent confounding
Observational patient data (e.g. electronic health records) contain information about how actions,
such as treatments are performed by doctors and how they affect the patient’s trajectory. Such data
can be used by causal inference methods to estimate Counterfactual outcomes-what would happen to
the patient if the expert takes a particular action given a history of observations?
To identify the counterfactual outcomes from observational data we make the standard assumptions
of consistency, positivity and no hidden confounders as described in Assumption 1 (Rosenbaum &
Rubin, 1983; Robins et al., 2000). Under Assumption 1, E[Yt+1 [at] | ht] = E[Xt+1 | at, ht] and this
can be estimated by training a regression model on the batch observational data.
Assumption 1 (Consistency, Ignorability and Overlap). For any individual i, if action at is
taken at time t, we observe Xt+1 = Yt+1 [at]. Moreover, we have sequential strong ignorabil-
ity {Yt+1[a]}a∈A ⊥⊥ at | ht, ∀t and sequential overlap Pr(At = a | ht) > 0, ∀t, ∀a ∈ A.
The sequential strong ignorability assumption, also known as no hidden confounders, means that
we observe all variables affecting the action assignment and potential outcomes. Sequential overlap
means that at each timestep, every action has a non-zero probability, which can be satisfied by having
a stochastic expert policy. These assumptions are standard across causal inference methods (Robins
et al., 2000; Schulam & Saria, 2017; Lim et al., 2018; Bica et al., 2020a). We emphasize that these
assumptions are needed to be able to reliably perform causal inference using observational data.
Nevertheless, they do not constrain the batch IRL set-up.
Using observational data to estimate the counterfactual outcomes poses additional challenges in this
set-up that need to be considered. In particular, direct estimation of the treatment effects is biased by
the presence of time-varying confounders (Mansournia et al., 2012), which are patient covariates that
influence the assignment of treatments and which, in turn, are affected by past actions.
For instance, consider that the doctor’s policy of assigning treatments takes into account whether the
patients’ covariates denoting disease progression, such as tumor volume, has been increasing above
normal thresholds for several timesteps. If these patients are also more likely to have severe side
effects, without adjusting for the bias introduced by time-varying confounders, we may incorrectly
conclude that the treatment is harmful to the patients (Platt et al., 2009). Using standard supervised
learning methods to estimate the patients’ response to treatments in this setting will be biased by
the doctor’s policy and will not be able to estimate the effect of treatments under different policies.
This represents an issue for our batch IRL setting where we need to be able to evaluate candidate
policies that are different from the expert policy. Thus, being able to correctly estimate counterfactual
treatment outcomes is crucial for our method. Methods for adjusting for the confounding bias use
either inverse probability of treatment weighting (IPTW) or balancing representations. The first
approach, IPTW, involves creating a pseudo-population where the probability of taking action at
13
Published as a conference paper at ICLR 2021
does not depend on x0:t (Robins et al., 2000; Lim et al., 2018), while the second approach builds a
balancing representation of the history that is invariant to the treatment (Bica et al., 2020a). Using
either approach will result in an unbiased estimation of the counterfactual outcomes. See Robins et al.
(2000); Lim et al. (2018); Bica et al. (2020b) for more details about time-dependent confounding as
well as for a more in-depth review of alternative methods for removing the bias from time-dependent
confounders and estimating counterfactual outcomes.
C COUNTERFACTUAL μ-LEARNING
Our CoUnterfactUal μ-learning algorithm involves learning the μ-values for policy π iteratively by
updating the current estimates of the μ-values with the feature map plus the μ-values obtained by
following policy π in the new coUnterfactUal history h0 = (h, a, E[Y [a]|h]):
μπ(h, a) - μπ(h, a) + α(φ(h, a) + YE°o〜冗(.々)[μπ(h0, a0)] - μπ(h, a)),	(13)
where α is the learning rate and γ is the discount factor.
We approximate μπ (h, a | θf) using an RNN with parameters θf. The RNN consists of an LSTM
unit with linear output. To stabilize training We use a target network μπ with parameters θ- that
provide updates for the main network. Note that using a target network is standard in Q-learning
(Mnih et al., 2013; Hausknecht & Stone, 2015). The target network μπ has the same architecture as
the main network μπ and its parameters θ- are updated to match θf every M- iterations.
The loss function for updating the μ network for estimating feature expectations at iteration i is:
Lf,i(θi) = Eh〜D[llyf,i-μπ(h,a | θf,i)ll2]	(14)
θf,i+1 J θf,i + αV(Li(θf,i))	(15)
where α is the learning rate, D is the batch observational dataset and
φ(h, a),	if patient trajectory terminates at history h
yf,i = (φ(h, a) + γE°0〜∏(∙∣hθ)[μπ(h0,a0 | θ-)], otherwise
(16)
is the target for iteration i obtained using the target network μπ. The action a is chosen by following
an ε-greedy policy by selecting action a 〜 π(∙ | h) with probability 1 - ε and a random action with
probability . We perform M training iterations. Algorithm 2 describes the full training procedure
used.
Table 5 indicates the search ranges used for the various hyperparameters involved in the counter-
factual μ-learning algorithm. The hyperparameters were selected to ensure stability of training and
convergence of the algorithm.
Hyperparameter	Hyperparameter search range	Hyperparameter value
LSTM size	64,128, 256	128
Batch size	128, 256, 512	256
Learning rate	0.00001, 0.0001, 0.001	0.001
Target network update M -	100, 200, 500	100
Min ε	-	0.0
Max ε	-	0.9
ε decay	-	0.00001
Number of training iterations M	-	20000
Optimizer	-	Adam
Table 5: Hyperparameters for training μ-network for estimating feature expectations.
14
Published as a conference paper at ICLR 2021
Algorithm 2 Counterfactual μ-learning
1:	Input: Batch dataset D = {(xi0, ai0, . . . xiTi-1, aiTi-1, xiT i)}iN=1, policy to evaluate π, counter-
factual model φ(h, a)
2:	Initialize feature expectations network μπ with random weights θf
3:	Initialize target feature expectations network μπ with random weights θ- = θf
4:	for i = 1 to M do
5:	Sample random minibatch B = {hb}bB=1 of histories h from D
6:	for b = 1 to B do
7:	With probability E select random action ab, otherwise select ab 〜π(∙ | hb)
8:	Compute counterfactual histories hb0 = (hb, ab, E[Y [ab]|hb]) using the counterfactual
model: E[Y[ab]|hb] = φ(hb,ab).
9:	Set targets:
φ(hb, ab),	if trajectory terminates at history hb
φ(hb,ab) + γEao~∏(∙∣hb0)[μπ(hb0, a0 | θ-)], otherwise
(17)
10:	end for
11:	Perform gradient descent step on PB=ι ||yf i - μπ (hb, ab | θf i)∣∣2 with respect to parameters
θf
12:	if i mod M- = 0 then θf- = θf,i
13:	end for
14:	A" = N Pi=ι Pa∈Aμπ(h0,a)∏(a | h0)
15:	Output: μπ * is
D Finding optimal policy for given reward weights
We use deep recurrent Q-learning (Hausknecht & Stone, 2015) to find the optimal policy for each
setting of the reward weights R(h, a) = W ∙ φ(h, a).
For history h, let the next history for taking action a be h0 = (h, a, E[Y (a) | h]), where E[Y (a) | h]
is estimated by the counterfactual model. Using the batch dataset D, we learn the Q-values iteratively
by updating the current estimate of the Q-values towards the reward plus the maximum Q-value
over all possible actions in the new history h0 :
Q(h, a) . Q(h, a) + α(R(h, a) + Y mi ax Q(h0, a) — Q(h, a)),	(18)
where α is the learning rate and γ is the discount factor.
We approximate Q(h, a∣θq) using an RNN parameterized by θq. The RNN consists of an LSTM
unit with linear output. We use the standard practices for training Q—networks (Mnih et al., 2013;
Hausknecht & Stone, 2015) and we employ a target network Q with parameters θ- to provide the
updates for the main network and to stabilize training. The target network Q is the same architecture
as the main network Q and its parameters θq- are updated to match θq every M - iterations.
We use the following loss function for the Q—learning update at iteration i:
Lq,i(θi) = Eh~D [(yq,i — Q(h,a∣θq,i))2]	(19)
θq,i+1 J θq,i + α5Lq,i (θq,i ))	(20)
where α is the learning rate and
R(h, a),
yq,i = ∣R(h, a) + γ maxa0 Q(h0, a0∣θ-),
if patient trajectory terminates at history h
otherwise
(21)
is the stale update target obtained from the target network Q. The action a is chosen by following an
ε-greedy policy by selecting action a = arg max。，Q(h, a0∣θq) with probability 1 — ε and a random
action with probability E. We perform M training iterations. Table 6 indicates the search ranges
used for the various hyperparameters involved in the deep recurrent Q—learnign algorithm. The
hyperparameters were selected to ensure stability of training and convergence of the algorithm.
15
Published as a conference paper at ICLR 2021
Hyperparameter	Hyperparameter search range	Hyperparameter value
LSTM size	64,128, 256	128
Batch size	128, 256, 512	256
Learning rate	0.00001, 0.0001, 0.001	0.001
Target network update M -	100, 200, 500	100
Min ε	-	0.0
Max ε	-	0.9
ε decay	-	0.00001
Number of training iterations M	-	20000
Optimizer	-	Adam
Table 6: Hyperparameters used for training Q-network to find optimal policy for a setting of the
reward weights.
E Data simulation
Using the dynamics of the simulated environment described in Equation 22, we trained a deep
recurrent Q-learning agent (Hausknecht & Stone, 2015) to find a stochastic optimal policy that
optimizes the reward function for different settings of the reward weights w1 and w2 in Equation 23.
Let ht = (x0:t, z0:t, a0:t-1) contain the history of the simulated patient covariates representing
disease progression x and side effects z. Let ht+1 = (x0:t+1, z0:t+1, a0:t) be the new history after
taking action at given ht and let rt = R(ht, at) be the corresponding reward.
We again learn the Q-values iteratively by updating the current estimate of the Q-values for history
ht towards the reward plus the maximum Q-value over all possible actions in the next history ht+1.
Q(ht ,at) — Q(ht, at) + α(R(ht, at) + Y max Q(ht+ι, a0) — Q(ht, at))	(22)
a0
where α is the learning rate and γ is the given discount factor.
We approximate the action-value function Q(ht,at∣θe) using an RNN parameterized by θe. We use
an LSTM (Hochreiter & Schmidhuber, 1997) unit as part of the RNN with a liner output activation
for estimating the Q-values. We use the standard practices for training Q—networks (Mnih et al.,
2013; Hausknecht & Stone, 2015).
Let E = {(ht, at, rt, ht+1)}tE=0 be the experience replay memory of maximum capacity E obtained
by simulating patient trajectories from the environment. During training, the Q—learning agent
selects and executes actions in the environment using an ε-greedy policy that follows the greedy
policy a = arg max。，Q(h, a0∣θe) with probability 1 一 ε and selects random action with probability
. The trajectories obtained from the agent’s behaviour are added to the experience replay memory
which is then used to train the Q—network. At each training iteration, we sample a batch B of
examples from E .
Moreover, we use a target network Q with parameters θ- to provide the updates to the main network.
The target network Q is the same architecture as the main network Q and its parameters θ- are
updated to match θe every M- iterations. The purpose of the target network is to stabilize training.
We use the following loss function for the Q—learning update at iteration i:
Le,i(θe,i) = E(ht,at,rt,ht+ι)〜E [Qe,i - Q(M atlθe,i))2]
θe,i+1 一 θe,i + αV(Le,i(θe,i))
where α is the learning rate and
rt,	if patient trajectory terminates at timestep t
ye,i = Vt + Y max。，Q(ht+1, a0∣θ-),	otherwise
(23)
(24)
(25)
is the stale update target obtained from the target network Q. We perform M training iterations.
Table 7 indicates the search ranges used for the various hyperparameters involved in the deep recurrent
Q—learnign algorithm. We selected hyperparameters based on the cummulative reward obtained
from the learnt optimal greedy policy in the simulated environment.
16
Published as a conference paper at ICLR 2021
Hyperparameter	Hyperparameter search range	Hyperparameter value
LSTM size	64,128, 256	128
Experience replay memory capacity E	10000, 50000	10000
Batch size	128, 256, 512	256
Learning rate	0.00001, 0.0001, 0.001	0.001
Target network update M -	100, 200, 500	200
Min ε	-	0.0
Max ε	-	0.9
ε decay	-	0.00005
Number of training iterations M	-	40000
Optimizer	-	Adam
Table 7: Hyperparameters used for training Q-network to solve the simulated environment.
After training the Q-network we create a batch dataset D containing 10000 trajectories from the
following stochastic expert policy:
π(at | ht) = Bernoulli(sigmoid(κQ(ht, at)).	(26)
where κ is a parameter that introduces time-dependent confounding bias in the expert policy.
F	Implementation details CIRL
The CIRL algorithm replies on using a counterfactual model to estimate the potential outcomes
E[Y [a] | h] which are used to define the reward and to estimate the feature expectations. For this,
we use the Counterfactual Recurrent Network (Bica et al., 2020a) which removes the bias from the
time-dependend counfounders by building a balancing representation that is invariant to the treatment
received by the patient at each timestep. Refer to (Bica et al., 2020a) for details about the model
architecture. Note that alternative counterfactual models can be used for this purpose (Lim et al.,
2018).
For each simulated batch observational dataset, we use 9000 samples for training the Counterfactual
Recurrent Network and 1000 for validation (hyperparameter optimization). We perform hyperpa-
rameter optimization for the counterfactual model using the hyperparameter search ranges described
in Table 8. We select the model that has the lowest error in estimating the factual outcomes on the
validation dataset.
Table 8: Hyperparameter search range for Counterfactual Recurrent Network. C is the size of the
input and R is the size of the balancing representation built by the Counterfactual Recurrent Network.
HyPerParameter	Hyperparameter search range
Iterations of Hyperparameter Search	20
Learning rate	0.01, 0.001, 0.0001
Minibatch size	64, 128, 256
RNN hidden units	0.5C, 1C, 2C, 3C, 4C
Balancing representation size	0.5C, 1C, 2C, 3C, 4C
FC hidden units	0.5R, 1R, 2R, 3R, 4R
RNN dropout probability	0.1, 0.2, 0.3, 0.4, 0.5
In the CIRL algorithm, we set the maximum number of iterations to 50 and the convergence threshold
to 0.001.
The experiments were run on a system with 2 NVIDIA K80 Tesla GPUs, 12CPUs, and 112GB of
RAM.
17
Published as a conference paper at ICLR 2021
G Implementation details benchmarks
For benchmarks, we integrate as part of the proposed batch inverse reinforcement learning algorithm
model-based policy evaluation reinforcement learning methods that use standard supervised learning
approaches to estimate the next history (Nagabandi et al., 2018). These methods use standard
supervised learning to estimate the next history h0 needed for the Counterfactual μ-learning algorithm
and for finding the optimal policy given a vector of reward weights. Our MB(h) benchmark uses
a standard RNN to estimate the next history and define the policy, while MB(x) uses a multi-
layer perceptron (MLP) that only considers the current observations for this purpose. The aim of
these benchmarks is to highlight the importance of handling the the bias from the time-depdendent
confounders and using a suitable counterfactual model, but also the importance of handling the patient
history.
For each simulated batch observational datasets, we use 9000 of the samples for trainign and 1000
for validation. We chose hyperparameters according to the error of the models in estimating the next
patient history on the validation set.
The MB(h) benchmark receives as input the patient history ht and current action at and estimates the
next observations xt+1 which are used to form the next history ht+1. For this purpose, the MB(h)
benchmark uses an LSTM unit, with a fully connected layer (FC) on top with ELU activation. The
hyperparameter search ranges used for this model are described in Table 9.
Table 9: Hyperparameter search range for the RNN. C is the size of the input.
Hyperparameter	Hyperparameter search range
Iterations of Hyperparameter Search	20
Learning rate	0.01, 0.001, 0.0001
Minibatch size	64, 128, 256
RNN hidden units	1C, 2C, 3C, 4C, 5C
FC hidden units	1C, 2C, 3C, 4C, 5C
Conversely, the MB(x) benchmark receives as input the current patient observation xt and current
action at and is trained to estimate the next observation xt+1. The MB(x) uses a multi-layer
perceptron with two fully connected layers and ELU activation. The hyperparameter search ranges
used for this model are described in Table 10.
Table 10: Hyperparameter search range for MLP. C is the size of the input.
Hyperparameter	Hyperparameter search range
Iterations of Hyperparameter Search	20
Learning rate	0.01, 0.001, 0.0001
Minibatch size	64, 128, 256
FC hidden units	2C, 3C, 4C, 5C
H	Evaluating accuracy on matching expert actions
The accuracy on matching the expert policy is computed as follows. Consider running the expert
policy πe in the environment and obtaining a test dataset with N trajectories: Dt = {ζi}iN=1, where
each Zi = (χ0,a0,... XT「，air」，XTJ has length Ti. Let ^t be the action selected by the policy
π recovered by CIRL for history ht = (x0：t, a0：t—i). The accuracy for matching the expert policy is
equalto N P3(T(PT 11[at == at])).
18
Published as a conference paper at ICLR 2021
I Limitations and interaction between counterfactual module
and CIRL
We illustrate in Figure 5 how the different components in our method interact and which are their
limitations.
ht = (xo,ao,... ,xt-1,at-1,xt)
(a) Counterfactual model
Possible limitations:
-	handling large actions spaces
-	handling high dimensional
patient covariates
φ(ht,at) = E[Yt+ι[at] | ht]
Limitations:
-linear rewards
-time-invariant W
(b) CIRL
opmμloβplu
əɔuəjəjəjd
Figure 5: The two different components part of our method for obtaining interpretable parametriza-
tions of decision making: (a) Counterfactual model which estimates the potential outcomes for taking
action at given patient history ht . Note that we do not propose a new counterfactual estimation
algorithm, but instead use already existing methods which could have limitations in terms of how
they handle large action spaces and large patient covariates. (b) CIRL algorithm which represents the
novelty of this work. We propose incorporating the estimated counterfactuals as part of a method for
batch IRL that can recover the preferences of the expert over the "what if" patient outcomes. As part
of the limitations of CIRL are the use of linear rewards and time-invariant reward weights w.
19