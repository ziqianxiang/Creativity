Published as a conference paper at ICLR 2021
Empirical or Invariant Risk Minimization?
A Sample Complexity Perspective
Kartik Ahuja*, Jun Wang*, Amit Dhurandhar*, Karthikeyan Shanmugam*, Kush R. Varshneyt
t IBM Research, TJ Watson Research Center, NY, * Rensselaer PolytechniqUe Institute
Ab stract
Recently, invariant risk minimization (IRM) was proposed as a promising solu-
tion to address out-of-distribution (OOD) generalization. However, it is unclear
when IRM should be preferred over the widely-employed empirical risk mini-
mization (ERM) framework. In this work, we analyze both these frameworks
from the perspective of sample complexity, thus taking a firm step towards an-
swering this important question. We find that depending on the type of data gen-
eration mechanism, the two approaches might have very different finite sample
and asymptotic behavior. For example, in the covariate shift setting we see that
the two approaches not only arrive at the same asymptotic solution, but also have
similar finite sample behavior with no clear winner. For other distribution shifts
such as those involving confounders or anti-causal variables, however, the two ap-
proaches arrive at different asymptotic solutions where IRM is guaranteed to be
close to the desired OOD solutions in the finite sample regime for polynomial gen-
erative models, while ERM is biased even asymptotically. We further investigate
how different factors — the number of environments, complexity of the model,
and IRM penalty weight — impact the sample complexity of IRM in relation to
its distance from the OOD solutions.
1	Introduction
A recent study shows that models trained to detect COVID-19 from chest radiographs rely on spuri-
ous factors such as the source of the data rather than the lung pathology (DeGrave et al., 2020). This
is just one of many alarming examples of spurious correlations failing to hold outside a specific train-
ing distribution. In one commonly cited example, Beery et al. (2018) trained a convolutional neural
network (CNN) to classify camels from cows. In the training data, most pictures of the cows had
green pastures, while most pictures of camels were in the desert. The CNN picked up the spurious
correlation and associated green pastures with cows thus failing to classify cows on beaches.
Recently, Arjovsky et al. (2019) proposed a framework called invariant risk minimization (IRM)
to address the problem of models inheriting spurious correlations. They showed that when data is
gathered from multiple environments, one can learn to exploit invariant causal relationships, rather
than relying on varying spurious relationships, thus learning robust predictors. More recent work
suggests that empirical risk minimization (ERM) is still state-of-the-art on many problems requiring
OOD generalization (Gulrajani & Lopez-Paz, 2020). This gives rise to a fundamental question:
when is IRM better than ERM (and vice versa)? In this work, we seek to answer this question
through a systematic comparison of the sample complexity of the two approaches under different
types of train and test distributional mismatches.
The distribution shifts (Ptrain (X, Y) = Ptest(X, Y)) that We consider informally stated satisfy an in-
variance condition - there exists a representation Φ* of the covariates such that Ptrain(Y ∣Φ*(X))=
PteSt(Y∣Φ*(X)) = P(Y∣Φ*(X)). A special case of this occurs when Φ* is identity — Ptrain(X)=
Ptest(X) but Ptrain(Y|X) = Ptest(Y|X) - such a shift is known as a Covariate-Shif (Gretton et al.,
2009). In many other settings Φ* may not be identity (denoted as I), examples include settings
with confounders or anti-causal variables (Pearl, 2009) where covariates appear spuriously corre-
lated with the label and Ptrain(Y|X) 6= Ptest(Y|X). We use causal Bayesian networks to illustrate
these shifts in Figure 1. Suppose Xe = [X1e , X2e] represents the image, where X1e is the shape of
the animal and X2e is the background color, Y e is the label of the animal, and e is the index of the
environment/domain. In Figure 1a) X2e is independent of (Ye, X1e), it represents the covariate shift
1
Published as a conference paper at ICLR 2021
a) Environment for	b) Environment for Confounder-	ɑ) Environment for
covariate shift: No clear winner	based shift: IRM wins	anti-causal shift: IRM wins
(Proposition 4)	(Proposition 5 and 6)	(Proposition 5 and 6)
Figure 1: Causal Bayesian networks for different distribution shifts.
case (Φ* = I). In Figure 1b) Xe is spuriously correlated With Ye through the Confounder εe. In
Figure 1c) X2e is spuriously correlated with Ye as it is anti-causally related to Ye . In both Figure
1b) and c) Φ* 6= I; Φ* is a block diagonal matrix that selects X1e.
Our setup assumes We are given data from multiple training environments satisfying the invariance
condition, i.e., P(Y∣Φ*(X)) is the same across all of them. Ideally, we want to learn and predict
using E[Y∣Φ*(X)]; this predictor has a desirable OOD behavior as we show later where we prove
min-max optimality with respect to (w.r.t.) unseen test distributions satisfying the invariance condi-
tion. Our goal is to analyze and compare ERM and IRM's ability to learn E[Y∣Φ*(X)] from finite
training data acquired from a fixed number of training environments. Our analysis has two parts.
1) Covariate shift case (Φ* =
I):	ERM and IRM achieve
the same asymptotic solution
E[Y |X].	We prove (Proposi-
tion 4) that the sample complex-
ity for both the methods is sim-
ilar thus there is no clear win-
ner between the two in the finite
sample regime. For the setup in
Figure 1a), both ERM and IRM
learn a model that only uses X1e .
2	) Confounder/Anti-causal variable case (Φ* 6= I): We consider a family of structural equation
models (linear and polynomial) that may contain confounders and/or anti-causal variables. For
the class of models we consider, the asymptotic solution of ERM is biased and not equal to the
desired E[Y∣Φ*(X)]. We prove that IRM can learn a solution that is within Ο(√e) distance from
E[Y∣Φ*(X)] with a sample complexity that increases as 0(9)and increases polynomially in the
complexity of the model class (Proposition 5, 6); (defined later) is the slack in IRM constraints.
For the setup in Figure 1b) and c), IRM gets close to only using X1e , while ERM even with infinite
data (Proposition 17 in the supplement) continues to use X2e . We summarize the results in Table 1.
Arjovsky et al. (2019) proposed the colored MNIST (CMNIST) dataset; comparisons on it showed
how ERM-based models exploit spurious factors (background color). The CMNIST dataset relied on
anti-causal variables. Many supervised learning datasets may not contain anti-causal variables (e.g.
human labeled images). Therefore, we propose and analyze three new variants of CMNIST in ad-
dition to the original one that map to different real-world settings: i) covariate shift based CMNIST
(CS-CMNIST): relies on selection bias to induce spurious correlations, ii) confounded CMNIST
(CF-CMNIST): relies on confounders to induce spurious correlations, iii) anti-causal CMNIST (AC-
CMNIST): this is the original CMNIST proposed by Arjovsky et al. (2019), and iv) anti-causal and
confounded (hybrid) CMNIST (HB-CMNIST): relies on confounders and anti-causal variables to
induce spurious correlations. On the latter three datasets, which belong to the Φ* 6= I class de-
scribed above, IRM has a much better OOD behavior than ERM, which performs poorly regardless
of the data size. However, IRM and ERM have a similar performance on CS-CMNIST with no clear
winner. These results are consistent with our theory and are also validated in regression experiments.
2 Related Works
IRM based works. Following the original work IRM from Arjovsky et al. (2019), there have been
several interesting works — (Teney et al., 2020; Krueger et al., 2020; Ahuja et al., 2020; Chang
et al., 2020; Mahajan et al., 2020) is an incomplete representative list — that build new methods
inpired from IRM to address the OOD generalization problem. Arjovsky et al. (2019) prove OOD
guarantees for linear models with access to infinite data from finite environments. We generalize
these results in several ways. We provide a first finite sample analysis of IRM. We characterize
the impact of hypothesis class complexity, number of environments, weight of IRM penalty on the
sample complexity and its distance from the OOD solution for linear and polynomial models.
Theory of domain generalization and domain adaption. Following the seminal works (Ben-
David et al., 2007; 2010), there have been many interesting works — (Muandet et al., 2013; Ajakan
et al., 2014; Zhao et al., 2019; Albuquerque et al., 2019; Li et al., 2017; Piratla et al., 2020; Mat-
suura & Harada, 2020; Deng et al., 2020; David et al., 2010; Pagnoni et al., 2018) is an incomplete
representative list (see Redko et al. (2019) for further references) — that build the theory of do-
main adaptation and generalization and construct new methods based on it. While many of these
works develop bounds on loss over the target domain using train data and unlabeled target data,
2
Published as a conference paper at ICLR 2021
Table 1: Summary of (empirical) IRM vs. ERM for finite hypothesis class HΦ. : slack in IRM con-
straints, ν: approximation w.r.t optimal risk, δ: failure probability, Etr: set of training environments,
n: data dimension, p: degree of the generative polynomial, L, L0 : bound on loss & its gradients.
Assumptions	Method	Sample complexity	OOD
Covariate shift case: Ee[Ye∣Xe] is invariant (Proposition 4)	ERM IRM	8L2 log (中|) max {8L2 log (空抖),IF log (2)}	YeS Yes
Confounder/Anti-causal variable case: Ee[Ye∣Φ*(Xe)] is invariant, Linear, Polynomial models, |Etr| = O(np) (Proposition 5, 6,17)	ERM IRM	8L2 log (中) Ir log( F)	No Yes
some (Ben-David & Urner, 2012; David et al., 2010; Pagnoni et al., 2018) analyze the finite sample
(PAC) guarantees for domain adaptation under covariate shifts. These works (Ben-David & Urner,
2012; David et al., 2010; Pagnoni et al., 2018) access unlabeled data from a target domain, which
we do not. Instead, we have data from multiple training domains (as in domain generalization).
In these works, the guarantees are w.r.t. a specific target domain, while we provide (for linear and
polynomial models) worst-case guarantees w.r.t. all the unseen domains satisfying the invariance
condition. Also, we consider a larger family of distribution shifts including covariate shifts. The
above two categories are not exhaustive - e.g., there are some recent works that characterize how
some inductive biases favor extrapolation Xu et al. (2021) and can be better for OOD generalization.
3	Sample Complexity of Invariant Risk Minimization
3.1	Invariant Risk Minimization
We start with some background on IRM (Arjovsky et al., 2019). Consider a dataset D = {De}e∈Etr,
which is a collection of datasets De = {(xie, yie, e)}in=e1} obtained from a set of training environ-
ments Etr , where e is the index of the environment, i is the index of the data point in the en-
vironment, ne is the number of points from environment, xie ∈ X ⊆ Rn is the feature value
and yie ∈ Y ⊆ R is the corresponding label. Define a probability distribution {πe}e∈Etr , πe is
the probability that a training data point is from environment e. Define a probability distribution
of points conditional on environment e as Pe, (Xe, Ye) 〜 Pe. Define the joint distribution P,
(Xe, Ye, e)〜P, dP(Xe, Ye, e) = πedPe(Xe, Ye). D is a collection of i.i.d. samples from P. De-
fine a predictor f : X → R and the space F of all the possible maps from X → R. Define the risk
achieved by f in environment e as Re(f) = Ee ['(f (Xe), Ye)], where ' is the loss, f (Xe) is the
predicted value, Ye is the corresponding label and Ee is the expectation conditional on environment
e. The overall expected risk across the training environments is R(f) = Pe∈E πeRe(f). We are
interested in two settings: regression (square loss) and binary-classification (cross-entropy loss). In
the main body, our focus is regression (square loss) and we mention wherever the results extend to
binary-classification (cross-entropy). We discuss these extensions in the supplement.
OOD generalization problem. We want to construct a predictor f that performs well across many
unseen environments Eall, where Eall ⊇ Etr . For o ∈ Eall \Etr, the distribution Po can be very
different from the train environments. Next we state the OOD problem.
min max Re(f)	(1)
f∈F e∈Eall
The above problem is very challenging to solve since we only have access to data from training
environments Etr but are required to find the robust solution over all environments Eall . Next, we
make assumptions on Eall and characterize the optimal solution to equation 1.
Assumption 1. Invariance condition. There exists a representation Φ* that transforms Xe to
Ze = Φ*(Xe) and ∀e, o ∈ Eaii,∀z ∈ Φ*(X) satisfies Ee[Ye∣Ze = z] = Eo[Yo∣Zo = z]. Also,
∀e ∈ Eaii, ∀z ∈ Φ*(X), Vare[Ye∣Ze = z] = ξ2, where Vare is the conditional variance.
The above assumption is inspired from causality (Pearl, 2009). Φ* acts as the causal feature extractor
and from the definition of causal features, it follows that Ee[Ye|Ze = z] does not vary across
3
Published as a conference paper at ICLR 2021
environments. When a human labels a CoW she uses Φ* to extract causal features from the pixels to
identify cow while ignoring the background. The first part of the above assumption encompasses a
large class of distribution shifts including standard covariate shifts (Gretton et al., 2009). Covariate
shift assumes ∀e, o ∈ Eall, ∀x ∈ X, P(Y e|Xe = x) and P(Y o|Xo = x) are equal thus implying
Ee[Ye∣Xe = x] = Eo[Yo∣Xo = x]. Therefore, for covariate shifts, Φ* is identity in Assumption 1.
A simple instance illustrating Assumption 1 with Φ* = I is when Ye J g(Xe)+ εe, where Ee [εe]=
0, Ee[(εe)2] = σ2, εe ⊥ Xe. Using Assumption 1, we define the invariant map m : Φ*(X) → R as
follows
∀z ∈ Φ*(X), m(z) = Ee[Ye∣Ze = z], where Ze = Φ*(Xe)	(2)
Assumption 2. Existence of an environment where the invariant representation is sufficient. ∃
an environment e ∈ Eall such that Ye ⊥ Xe |Ze
Assumption 2 states there exists an environment where the information that X e has about Ye is also
contained in Ze. Define a composition m ◦ Φ*, ∀x ∈ X, m ◦ Φ*(x) = Ee[Ye∣Ze = Φ*(x)].
Proposition 1. If' is the square loss, and Assumptions 1 and 2 hold, then m ◦ Φ* solves the OOD
problem (equation 1).
The proofs of all the propositions are in the supplement. A similar result holds for the cross-entropy
loss (discussion in supplement). For the rest of the paper, we focus on learning m ◦ Φ* as it solves
the OOD problem. For covariate shifts Φ* = I, m(x) = Ee[Ye∣Xe = x] is the OOD solution. In
Arjovsky et al. (2019), a proof connecting m ◦ Φ* and OOD was not stated. Recently, in KOyama
& Yamaguchi (2020), a result similar to Proposition 1 was shown but with a few differences. The
authors assume conditional probabilities are invariant unlike our assumption that only requires con-
ditional expectations and variances to be invariant. However, their result applies to more losses.
m ◦ Φ* is the target we want to learn. Arjovsky et al. (2019) proposed IRM since standard min-max
optimization over the training environments Etr and ERM fail to learn m ◦ Φ* in many cases. The
authors in Arjovsky et al. (2019) identify a crucial property of m ◦ Φ* and use it to define an object
called invariant predictor that we define next.
Invariant predictor and IRM optimization. Define a representation map Φ : X → Z from feature
space to representation space Z ⊆ Rq . Define a classifier map, w : Z → R from representation
space to real values. Define HΦ and Hw as the spaces of representations and classifiers respectively.
A data representation Φ elicits an invariant predictor w ◦ Φ across environments e ∈ Etr if there
is a classifier w that achieves the minimum risk simultaneously for all the environments, i.e., ∀e ∈
Etr, w ∈ argminw∈Hw Re(W ◦ Φ). Observe that if we we transform the data with representation
Φ* then m will achieve the minimum risk simultaneously in all the environments. If Φ* ∈ Hφ and
m∈Hw , then m ◦ Φ* is an invariant predictor. IRM selects the invariant predictor with least sum
risk across environments (results presented later can be adapted if invariant predictor was selected
based on the worst-case risk over the environments as well) as follows:
min	R(w ◦ Φ) =	πeRe (w ◦ Φ)
Φ∈HΦ,w∈Hw
e∈Etr
s.t. w ∈ arg min Re(W ◦ Φ), ∀e ∈ Etr
w∈Hw
(3)
From the above discussion we know m ◦ Φ* is a feasible solution to equation 3. It is also the ideal
solution we want IRM to find since it solves equation 1. Later in Propositions 4, 5, and 6, we show
that IRM actually solves equation equation 1. For the setups in Proposition 5, and 6, conventional
ERM based approaches fail thus justifying the need for above formulation.
3.2	Sample Complexity of Gradient Constraint Formulation of IRM
In Arjovsky et al. (2019), a gradient constrained alternate (derived below in equation 4) to equation 3
was proposed, which focuses on linear and scalar classifiers (Z = R, Φ : X → R, Hw = R). In
this case, the composite predictor W ◦ Φ is a multiplication of W and Φ written as W ∙ Φ. (For binary-
classification predictor,s output w∙Φ(x) represents logits.) From the definition of invariant predictors
and HW = R it follows that if ∀W ∈ R, Re(1 ∙ Φ) ≤ Re(W ∙ Φ), then Φ is an invariant predictor.
For square and cross-entropy losses, Re(W ∙ Φ) is convex in w. Therefore, a gradient constraint
[1▽w|w=LORe(W ∙Φ)∣∣ = 0 is equivalent to the condition that ∀W ∈ R, Re(1∙Φ) ≤ Re(W ∙Φ), which
implies Φ is an invariant predictor. Recall that IRM aims is to search among invariant predictors and
find one that minimizes the risk. We state this as a gradient constrained optimization as follows
4
Published as a conference paper at ICLR 2021
min R(Φ)
Φ∈HΦ
s.t. „Vw∣w=ι.oRe(W ∙Φ)∣∣ =0, ∀e ∈Etr
(4)
We propose an approximation of the above with slack in the constraint. Define R0 (Φ) =
Pe∈Etr ∏e∣∣Vw∣w=1.0Re(w ∙ Φ)k2 and a set SIV(E) = {Φ | R0(Φ) ≤ e, Φ ∈ Hφ}. Note that R0 is
very similar to the penalty defined in Arjovsky et al. (2019). The approximation of equation 4 is
min R(Φ)	(5)
Φ∈SIV()
If E = 0, then equation 4 and equation 5 are equivalent. In all the optimizations so far, the ex-
pectations are computed w.r.t. the distributions Pe, which are unknown. Therefore, we develop an
empirical version of equation 5 below (in equation 6) and call it empirical IRM (EIRM). We replace
R and R with empirical estimators R and R respectively. For R We use a simple plugin estimator
(sample mean of loss across all the samples in D). For R0 we construct a new estimator that enables
the use of standard concentration inequalities. Define a set SIV(E) = {Φ | RR (Φ) ≤ e, Φ ∈ Hφ}.
min R(Φ)	(6)
φ∈S∣v9
If we replaced SIV (E) with Hφ in equation 6, then we get the standard ERM. ERM aims to solve
minφ∈Hφ R(Φ). The sample complexity analysis ofERM aims to understand the distance between
the empirical solutions and the expected solutions as a function of the number of samples. Similarly,
we seek to understand the relationship between solutions of equation 6 and equation 5.
Assumption 3. Bounded loss and bounded gradient of the loss. ∃ L < ∞, L0 < ∞ such that
∀Φ ∈ Hφ,∀χ ∈ X,∀y ∈ Y, ∣'(Φ(χ),y)∣ ≤ L, |必若产m ∣w=ι.o∣ ≤ L.
If every Φ in the hypothesis class HΦ is bounded by M and the label space Y is bounded, then
for both square and cross-entropy loss, '(Φ(∙), ∙) and d"waW，") ∣w=1.0 are bounded. Define K =
minΦ∈HΦ |R0 (Φ) - E|; κ measures how close any penalty can get to the boundary E. κ quantifies
how good the finite sample approximation R need to be in order to get SIV(E) = Siv(e). Define V
to quantify the approximation w.r.t. optimal risk.
Proposition 2. For every ν > 0, E > 0 and δ ∈ (0, 1), ifHΦ is a finite hypothesis class, Assumption
3 holds, κ > 0, and if the number of samples |D| is greater than max {16L-, 8L2 } log (4|Hφ1 ),
then with a probability at least 1 - δ, every solution Φ to EIRM (equation 6) is a ν approximation
ofIRM, i.e. Φ ∈ SIV(E), R(Φ*) ≤ R(Φ) ≤ R(Φ*) + V, where Φ* is a solution to IRM(equation5).
Proof Sketch. The standard analysis in learning theory on ERM or regularized/constrained ERM
typically relies on linearly separable loss functions. In such cases, we can use standard plug-
in estimators and analyze their behavior using concentration inequalities. In our setting, R0 is
a weighted sum of squares of expectation and thus it is not linearly separable. We develop a
new way of expressing R0 that allows us to make it linearly separable. Next, in order to ensure
R(Φ*) ≤ R(Φ) ≤ R(Φ*) + v we first need to guarantee that the set of invariant predictors is ex-
actly recovered, i.e., SIV (E) = SIV(E) (exact recovery is typically not required in existing constrained
analysis such as Woodworth et al. (2017) Agarwal et al. (2018)). We show that if the number of sam-
ples grow as 今 even the closest points on either side of the boundary of the set Siv(e) are correctly
discriminated, which guarantees exact recovery of SIV(E). Once the exact set is recovered, beyond
this we use standard learning theory tools to ensure R(Φ*) ≤ R(Φ) ≤ R(Φ*) + v.
The above result holds for both square and cross-entropy loss. For ease of exposition, we use the
standard setting of finite hypothesis class and extend all the results to infinite hypothesis classes
in the supplement (summary of insights from the extension are in Section 3.3.2). Next, we state a
standard result on ERM’s sample complexity. Define a Φ+ such that Φ+ ∈ arg minΦ∈HΦ R(Φ)
Proposition 3. (Shalev-Shwartz & Ben-David, 2014) For every v > 0 and δ ∈ (0, 1), if HΦ is
a finite hypothesis class, Assumption 3 holds, and if the number of samples |D| is greater than
8V2- log (2|Hφ1 ), then With a probability at least 1 一 δ, every solution Φ* to ERM is an V approxi-
mation ofexpected risk minimization, i.e., R(Φ+) ≤ R(Φ*) ≤ R(Φ+) + V.
5
Published as a conference paper at ICLR 2021
Proposition 2 vs. 3 Since K ≤ e, the sample complexity ofEIRM grows at least as O(max{ *,今}).
Let us look at the two terms inside max- i)击 growth term is similar to ERM, it ensures V approx-
imate optimality in the overall risk R, ii) £ growth ensures the IRM penalty R is less than e. A
direct comparison of sample complexities in Propositions 2 and 3 suggests that the sample complex-
ity of EIRM is higher than ERM, which is not the complete picture. The two approaches may not
converge to the same solutions and IRM may converge to a solution with better OOD behavior than
one achieved by ERM. Therefore, a fair comparison is only possible when we also study the OOD
properties of the solutions achieved by the two approaches, which is the subject of the next section.
3.3 OOD PERFORMANCE: ERM VS. IRM
We divide the comparisons based on distributional shift assumptions that decide whether ERM and
IRM arrive at the same asymptotic solutions or not.
3	.3.1	Covariate shift
Assumption 4. Invariance w.r.t. all the features. ∀e, o ∈ Eall and ∀x ∈ X, E[Y e|Xe = x] =
E[Yo∣Xo = x]. ∀e ∈ Eall, Xe 〜PXe and the support of PXe is equal to X.
As stated earlier, the first part of the above assumption follows from standard covariate shift assump-
tions (Gretton et al., 2009) and is a special case of the first part of the Assumption 1 with Φ* set
to I. If Φ* = I, then m (equation 2), which simplifies to m(x) = E[Ye∣Xe = x], solves the OOD
problem equation 1. A generative model that satisfies the above Assumption 4 is given as
Ye 一 g(Xe) + εe, E[εe] = 0, εe ⊥ Xe, E[(εe)2] = σ2	(7)
In the above model Xe is the cause, Ye is the effect, and g a general non-linear function (it satisfies
Assumption 4 with m = g). Next, we compare ERM and IRM’s ability to learn m under covariate
shifts. In Figure 1 a), we show Define K = minφ1,φ2∈Hφ,Φ1=Φ2 ∣R(Φ1) - R(Φ2)∣, which measures
the minimum separation between the risks of any two distinct hypothesis in HΦ .
Proposition 4. Let ` be the square loss. For every ν > 0, > 0 and δ ∈ (0, 1), ifHΦ is a finite
hypothesis class, m ∈ HΦ, Assumptions 3, 4 hold, and
•	if the number of samples |D| is greater than max { 8L2 log( 4|H φ1 ), 16L- log( 2)}, then with a
probability at least 1 - δ, every solution Φ to EIRM (equation 6) satisfies R(m) ≤ R(Φ) ≤ R(m) +
v. Ifalso v < κ, then Φ = m.
•	ifthe number ofsamples |D| is greater than 8⅞2 log( 2|Hφ1 ), then with a probability at least 1 — δ,
every solution Φ* to ERM satisfies R(m) ≤ R(Φ*) ≤ R(m) + V. Ifalso v < K, then Φ* = m.
Implications of Proposition 4. ERM and EIRM both asymptotically achieve the ideal OOD solu-
tion; the above proposition helps compare them in a finite sample regime. The second term inside
the max for EIRM, 16⅛- log(2), does not depend on the size of the hypothesis class. Hence, for
large hypothesis classes, the sample complexity of EIRM equals 8L2 log( 4|Hφ1 ). Consequently, the
sample complexity ofEIRM and ERM differs by a constant 8VL2 log(2). Thus we conclude, for large
hypothesis classes, both ERM and EIRM have similar sample complexity. Next, we contrast the
sample complexity of EIRM in Proposition 4, O(V2), to Proposition 2, O(max{*,ν2}); the addi-
tional covariate shift assumption in Proposition 4 helps get to a lower sample complexity of O(V2).
In Proposition 4, we assumed square loss, but a similar result extends to cross-entropy loss as well.
3	.3.2 Distributional Shift with Confounders and (or) Anti-Causal Variables
In this section, we consider more general models than equation 7, which only contained cause Xe
and effect Ye. We also allow confounders and anti-causal variables. However, we restrict g to
polynomials. We start with linear models from Arjovsky et al. (2019).
Assumption 5.
e 〜Categorical({πo}o∈Etr), ∀o ∈ Etr,∏o > 0
Ye —	YT(Ze) +	εe,	εe	⊥	Ze,	E[εe]	=	0,	E[(εe)2]	= σ2,	∣εe∣	≤	εsup	(8)
xe J s(ze,ze)
6
Published as a conference paper at ICLR 2021
We assume that Z1 component of S is invertible, i.e. ∃ S such that S(S(Z1, Z2)) = Z1, and γ 6= 0.
∀e ∈ Etr, ∏e ≥ ∏min∖ > 0∙ Define Σe = EXeXe,T]. ∀e ∈ Etr, ∑e is positive definite. The support
|Etr |
of distribution of Ze = (Z1e, Z2e), PeZe, is bounded and the norm of S, kSk = σmax (S) (maximum
singular value of S), is also bounded.
In the above model, Z1e is the cause of Xe and Y e but may not be directly observed. Z2e may
be arbitrarily correlated with Z1e and e. We observe a scrambled transformation of (Z1e, Z2e) in
Xe. If Ze is an effect of Ye (Ze J Ye + Ne), then Ze is an anti-causal variable. If He causes
both εe (εe J He + Ne) and Ze (Ze J He + Ne), then He is a ConfoUnder. In both these
cases, Z2e is spuriously correlated with the label Ye . Consequently, the standard ERM based models
estimate Z2e from Xe and end up being biased w.r.t the desired OOD model, which does not use
Ze. If Assumptions 5, 2 hold, then a linear model STY (Xe --→ YTSXe = YTZe) solves the
OOD problem in equation 1 (Φ* = S, m = YT in Proposition 1) and it relies only on Ze. In the
supplement (Proposition 17), We prove that ERM based models do not recover STγ.
Assumption 6. Linear general position. A set of training environments Etr is said to lie in a linear
general position of degree r for some r ∈ N if |Etr | > n - r + n/r and for all non-zero x ∈ Rn
dim span Ee [XeXe,T]x - Ee[Xeεe]	> n- r
(9)
where span is the linear span, dim is the dimension, and recall n is dimension of Xe. This assump-
tion checks for diversity in the environments and holds almost everywhere (Arjovsky et al., 2019).
Assumption 7. Inductive bias. HΦ is a finite set of linear models parametrized by Φ ∈ Rn (output
ΦtXe). STY ∈Hφ. ∃ ω > 0, Ω > 0, s.t. ∀Φ ∈Hφ, ω ≤ ∣∣Φk2 ≤ Ω & 2ω ≤ ∣∣STγ∣∣2 ≤ 3+2√2Ω.
T ɪ'	11	. . 1 . 1	Λ	. ∙	∙	.1	z ʌ z ʌ l ʌ	. ∙	1	Λ ∙	P*T . 1 ∙ ∙ .t
Informally stated, the above assumption requires the OOD optimal predictor STY to lie in the
interior of the search space and not on the boundary. If Assumptions 5, 7 hold, then Assump-
tion 3 holds. Hence, we can use the bounds L and L0 on ' and d'(wdW(，” ∣w=1.0 respectively
in our next result. Define the minimum eigenvalue across all Σe as λmin = mine∈Etr λmin (Σe),
Eth = I24-；6* ∣∏mn∣(ωλmin)2 and T = 2ωλ~ J用EH . Next, we analyze how EIRM learns STY.
Proposition 5. Let ` be the square loss. For every ∈ (0, th) and δ ∈ (0, 1), if Assumptions 5, 6
(with r = 1), 7hold and ifthe number ofdatapoints |D| is greater than 16L log (2|Hφ1 ), then with
a probability at least 1 - δ, every solution Φ to EIRM (equation 6) satisfies Φ = (STY α, where
α ∈ [ 1+1√, 1-T√].
Proof Sketch. In learning theory it is common to analyze the concentration of empirical risks
around the expected risks. In our case, we have a target ideal solution to equation 1 (STY) and
we want our empirical solutions to concentrate around that. A direct finite sample approximation
of equation 4 is hard to analyze. Therefore, we introduce an intermediate problem in equation 5
and then develop a finite sample approximation of it in equation 6. We first show that solving
equation 5 leads to solutions in the neighborhood of the target. To show this we use the linear
general position assumption. Next, we connect equation 6 and equation 5 using our new estimator
for R0 and Hoeffding’s inequality.
Implications of Proposition 5. 1. Convergence rate of ERM vs. EIRM: Recall that E is the slack
on IRM penalty R. If E is sufficiently small and the data grows as O(*),every solution Φ to EIRM
(equation 6) is in √e radius of the OOD solution, i.e., ∣Φ - S TY ∣∣ = 0( √e). We contrast these rates
to ones in the covariate shift setting (Section 3.3.1). LetE[Ye|Xe = x] = ΨTx. If the data grows as
0(S), then both ERM and EIRM solution converge to Ψ as ∣Φ - Ψ∣ = Ο(√ν) (from Proposition
4). This shows that EIRM works in more settings (Proposition 4, 5) than ERM while matching the
convergence rate of ERM.
2	. Sample complexity grows polynomially in data dimension to ensure OOD generalization:
Next, we set E = μeth with μ ∈ [0,1), and |Etr| = 2n (satisfies Assumption 6 for r = 1). A simple
manipulation of terms in Proposition 5 shows that sample complexity with quadratic growth in data
dimension n, 0 (n2 log(2Hl)), ensures φ = (STY)α with α ∈ [ 1+√μ√2-l), l-√μ1√2-l)].
7
Published as a conference paper at ICLR 2021
3	. Comparison with Proposition 2: Lastly, we contrast sample complexity of EIRM in Propo-
sition 5, O(*), to Proposition 2, O(max{*, £}); the additional distributional assumptions in
Proposition 5 help arrive at a lower sample complexity of O(*).The bound in Proposition 2,
O(max{*, ν2}), is larger than the one in Proposition 4, O(£), and Proposition 5, O(*), but is
more general as it is agnostic to the distributional assumptions.
A simple illustration summarizing Propositions 4, 5: Set S to identity in Assumption 5. Recall
Z1e and Z2e from Assumption 5. Since S is identity Xe can be written as [X1e, X2e], where X1e = Z1e
and X2e = Z2e . If X2e ⊥ εe, then E[Y e |Xe] is invariant and Assumption 4 holds. This corresponds
to the setup in Figure 1a). We can now use Proposition 4 and deduce that ERM and IRM have same
sample complexity and end up learning the ideal model that only uses the causal features X1e . If
Xe J εe + Ne, then this corresponds to the setup Figure 1b), Xe is the cause and Xe is spuriously
correlated with label Ye through the confounder εe. If Xe J Ye + Ne, then this corresponds to the
setup in Figure 1c), X1e is the cause and X2e is anti-causally related to the label Ye . In both these
cases, the ideal OOD solution that solves equation 1 will only exploit Xe to make predictions. From
Propositions 5, it follows that IRM when fed with O(£) samples, it is in √∕e radius of the target
OOD solution, while ERM is asymptotically biased and exploits X2e (Proposition 17).
We define a polynomial version of the model in Assumption 5. We only need to change Ye J
γT(Z1e) + εe to Ye J γtζp(Z1e) + εe. ζp is a polynomial feature map of degree p defined as
Zp : Rc → Rc , where C is the dimension of the input Zf, Zp(W) = (W, W 0 W,..., (W 0
W...p times 0 W)) = ((W0i)p=1) and 0 is the Kronecker product. Also, c = Pp=ι ci. Can
we directly use the analysis from the linear case by transforming Xe appropriately? No, we first
need to find an appropriate transformation for the scrambling matrix S that satisfies the conditions
(invertibiltiy) in Assumption 5 while maintaining a linear relationship between transformations of
Xe and Ze . We present the main result informally below (details are in the supplement).
Proposition 6. (Informal statement) For sufficiently small andδ ∈ (0, 1), if Assumptions similar to
04	2|H |
Proposition 5 hold and |D| ≥ ,L log( 1 δ φ ) ,then With a probability at least 1 一 δ, every solution
Φ to EIRM (equation 6) satisfies Φ = STγ(α), where STY is the OOD optimal model (defined in
the supplement), α ∈ [ *T√, J√].
Insights from the polynomial case and infinite hypothesis case. In the polynomial case, we
adapt the linear general position Assumption 6, the number of environments |Etr | are now required
to grow as O(np). As a result, in the sample complexity analysis we discussed, we replace n
with np to obtain that a sample complexity of O (富 log (2Hφ1 )) ensures Φ = STγ(α) with
1
1
α∈[
1+√μ(√2-1), 1-√μ(√2-1)
]. In the infinite hypothesis case, the main change in the results is
that we replace ∣Hφ∣ with an appropriate model complexity metric (Shalev-Shwartz & Ben-David,
2014). Consider Proposition 5, a sample complexity of O(μ log (n)) ensures Φ = (STY)α with
α∈[
1 + √μ(√2-1) , 1-√μ(√2-1)
]in contrast to O (μ2
in the finite hypothesis case. We
showed the benefits of IRM for polynomial models and other extensions (non-linear S) are future
work. In the supplement, we provide a dialogue explaining how our work fits in the big picture.
1
1
4 Experiments
In this section, we discuss classification experiments (regression experiments with similar qualitative
findings are in the supplement). We introduce three new variants of the colored MNIST (CMNIST)
dataset in (Arjovsky et al., 2019). We divide the training data in MNIST digits into two environments
(e = 1, 2) equally and the testing data in MNIST digits is assigned to another environment (e = 3).
Xge: gray scale image of the digit, Yge: label of the gray scale digit (digits ≥ 5 have Yge = 1 and digits
< 5 have Yge = 0). Xe : final colored image and Ye : final label are generated as follows. Define
Bernoulli variables G, N, Ne that take a value 1 with probability θ, β and βe and 0 otherwise.
Define a color variable Ce, where Ce = 0 is red and Ce = 1 is green. Let ㊉ denotes xor operation.
Yge J L(Xge), L : Human labeling,
Ye J L(Xe)㊉ N, Corrupt the original labels with noise
Ce J G(Ye ㊉ Ne) + (1 ― G)(N ㊉ Ne), Use G to select b/w anti-causal or confounded
Xe J T(Xg , Ce), T : transformation to color the image
(10)
8
Published as a conference paper at ICLR 2021
If the probability θ = 1, then G = 1 and that gives us back the original CMNIST in (Arjovsky
et al., 2019), which we call anti-causal CMNIST (AC-CMNIST). If θ = 0, then we get confounded
colored MNIST (CF-CMNIST). If0 < θ < 1, we get a hybrid dataset (HB-CMNIST). The above
model in equation 10 has features of the model in Assumption 5, where Xge, Ce, L, T take the
role of Z1e, Z2e, γ, S. We set the noise N parameter β = 0.25, and the parameter for Ne in
the three environments [β1, β2, β3] = [0.1, 0.2, 0.9]. Color is spuriously correlated with the label;
P(Ce = 1|Ye = 1) varies drastically in the three environments ([0.9, 0.8, 0.1] for AC-CMNIST). In
the variants of CMNIST we discussed, P(Y e |Xe) varies across the environments. We now define
a covariate shift based CMNIST (CS-CMNIST) (P(Y e |Xe) is invariant). We use selection bias to
induce spurious correlations. Generate a color Ce uniformly at random. Select the pair (Xge , Ce)
with probability 1 - ψe if the label Yge and Ce are the same, else select them with a probability ψe .
If the pair is selected, then color the image Xe — T(Xg, Ce) and Ye — Yge. Selection probability
ψe for the three environments are [ψ1, ψ2, ψ3] = [0.1, 0.2, 0.9]. Due to the selection bias, color is
spuriously correlated, P(Ce = 1|Ye = 1) varies drastically [0.9, 0.8, 0.1]. We provide the graphical
models for the CMNIST variants and computations of P(Ce|Ye), P(Ye |Xe) in the supplement.
10000	20000	30000	40000	50000	60000
a) Number of samples
IOOOO 20∞0 3∞∞ 4∞∞ 5∞∞ COOOO
b) Number of samples
100∞ 200∞ 300∞ 40∞0 50∞0 COOOO
c) Number of samples
0.2-
O.I-
0.0 J-,----,------,-------,------,-------,------1-
0	10000	20∞0	3∞∞	4∞∞	5∞∞	COOOO
d) Number Of samples
Figure 2: Comparisons: a) CS-CMNIST, b) CF-CMNIST, c) AC-CMNIST and d) HB-CMNIST.
4.1	Results
We use the first two environments (e = 1, 2) to train and third environment (e = 3) to test. Other
details of the training (models, hyperparameters, etc.) are in the supplement. For each of the above
datasets, we run the experiments for different amounts of training data from 1000 to up to 60000
samples (10 trials for each data size). In Figure 2, we compare the models trained using IRM and
ERM in terms of the classification error on the test environment e = 3 (a poor performance indicates
model exploits the color) for varying number of train samples. We also provide the performance
of the ideal hypothetical optimal invariant model. Observe that except for in the covariate shift
setting where IRM and ERM are similar as seen in Figure 2a (as predicted from Proposition 4), IRM
outperforms ERM in the remaining three datasets (as predicted from Proposition 5) as seen in Figure
2b-d. We further validate this claim through the regression experiments provided in the supplement.
In CF-CMNIST, IRM achievs an error of 0.45, which is much better than error of ERM (0.7) but is
marginally better than a random guess. This suggests that confounder induced spurious correlations
are harder to mitigate and may need more samples than needed in anti-causal case (AC-CMNIST).
5	Conclusion
We presented a sample complexity analysis of IRM to answer the question: when is IRM better than
ERM (and vice-versa)? For distribution shifts such as the covariate shifts, we proved that both IRM
and ERM have similar sample complexity and arrive at the desired OOD solution asymptotically. For
distribution shifts involving confounders and (or) anti-causal variables and polynomial generative
models, we proved that IRM is guaranteed to achieve the desired OOD solution while ERM can be
asymptotically biased. We proposed new variants of original colored MNIST dataset from Arjovsky
et al. (2019), which are more comprehensive and better capture how spurious correlations occur in
reality. To the best of our knowledge, we believe this to be the first work that provides a rigorous
characterization of impact of factors such as model complexity, number of environments on the
sample complexity and its distance from the OOD solution in distribution shifts that go beyond
covariate shifts.
9
Published as a conference paper at ICLR 2021
6	Acknowledgements
This work was supported in part by the Rensselaer-IBM AI Research Collaboration (part of the IBM
AI Horizons Network).
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reduc-
tions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk min-
imization game. In International Conference on Machine Learning, 2020.
Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, and Mario Marchand.
Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.
Isabela Albuquerque, Joao Monteiro, Tiago H Falk, and Ioannis Mitliagkas. Adversarial target-
invariant representation learning for domain generalization. arXiv preprint arXiv:1911.00804,
2019.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Robert B Ash, B Robert, Catherine A Doleans-Dade, and A Catherine. Probability and measure
theory. Academic Press, 2000.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of
the European Conference on Computer Vision, pp. 456-473, 2018.
Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unla-
beled target samples. In International Conference on Algorithmic Learning Theory, pp. 139-153.
Springer, 2012.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola. Invariant rationalization. In International
Conference on Machine Learning, 2020.
Shai Ben David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adaptation.
In Proceedings ofthe Thirteenth International Conference on Artificial Intelligence and Statistics,
pp. 129-136, 2010.
Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects
shortcuts over signal. medRxiv, 2020.
Zhun Deng, Frances Ding, Cynthia Dwork, Rachel Hong, Giovanni Parmigiani, Prasad Patil, and
Pragya Sur. Representation via representations: Domain generalization via adversarially learned
invariant representations. arXiv preprint arXiv:2006.11478, 2020.
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Scholkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5,
2009.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
Masanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal in-
variant predictor. arXiv preprint arXiv:2008.01883, 2020.
10
Published as a conference paper at ICLR 2021
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). arXiv
preprint arXiv:2003.00688, 2020.
Alan J Laub. Matrix analysis for scientists and engineers, volume 91. Siam, 2005.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542-5550, 2017.
Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching.
arXiv preprint arXiv:2006.07500, 2020.
Toshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent
domains. In AAAI, pp. 11749-11756, 2020.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Artidoro Pagnoni, Stefan Gramatovici, and Samuel Liu. Pac learning guarantees under covariate
shift. arXiv preprint arXiv:1812.06393, 2018.
Judea Pearl. Causality. Cambridge university press, 2009.
Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi. Efficient domain generalization via
common-specific low-rank decomposition. In International Conference on Machine Learning,
2020.
Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younes Bennani. Advances in
Domain Adaptation Theory. Elsevier, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved
generalization. arXiv preprint arXiv:2002.11894, 2020.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.
Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie
Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=UH-cmocLJC.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019.
7	supplement
7.1	Dialogue on IRM Continued
[In the original IRM paper by Arjovsky et al. (2019), we left two graduate students ERIC and IRMA
strolling along the Parisian streets on a warm summer evening. A lot has transpired since then.
ERIC is now holed UP inside his room at Cite Universitaire and IRMA has returned to her parents"
home in Provence. The two are talking through a Zoom call.]
•	ERIC: After reading Arjovsky et al. (2019) and Gulrajani & Lopez-Paz (2020), I was not
sure of how to understand the settings where IRM is beneficial over ERM and vice-versa?
From Gulrajani & Lopez-Paz (2020), I understood that ERM continues to be the state-of-
the-art if sufficient care is taken in performing model selection.
11
Published as a conference paper at ICLR 2021
•	IRMA: But after reading this manuscript I understand that there are settings where no matter
how one selects the model, ERM is bound to be at a disadvantage.
•	ERIC: What you say seems to contradict my understanding of Gulrajani & Lopez-Paz
(2020).
•	IRMA: Actually, they ...
[IRMA’s audio and video freeze for a minute, leaving ERIC to ponder this conundrum for a little bit
on his own. IRMA stops her video and continues only with audio and the conversation is able to
resume.]
•	IRMA: Sorry, how much did you hear?
•	ERIC: You were just starting to explain why ERM can be at a disadvantage, but I didn’t
hear anything after that.
•	IRMA: Okay, let me start my explanation again. There is no contradiction. The current
manuscript says that in covariate shift settings, there maybe no clear winner. Many of
the datasets considered in Gulrajani & Lopez-Paz (2020) are perhaps similar to covariate
shift settings. Also, there could be another reason why IRM did not outperform ERM in
Gulrajani & Lopez-Paz (2020) and I will come to it in a bit.
•	ERIC: Yes, you are right! The datasets are human labeled images if I recall correctly. In
these datasets it’s safe to assume P(Y e |Xe) is close to invariant across domains (as long as
the cohort of humans in the different domains are not very different). What happens when
P(Y e |Xe) varies a lot?
•	IRMA: Yes, when P(Y e|Xe) varies a lot, IRM can be at an advantage over ERM. In this
manuscript, I learned that when data generation involves confounders or anti-causal vari-
ables, it is possible to show that IRM methods converge to desirable OOD solutions with
good convergence rates.
•	ERIC: Interesting! I should read that. The experiment on colored MNIST by Arjovsky
et al. (2019) contained anti-causal variables right? Is that the reason why IRM performed
better than ERM?
•	IRMA: Yes, you are right. In the current manuscript, the authors also develop other variants
of colored MNIST (covariate shift based, confounder based, anti-causal based, and hybrid
of confounder and anti-causal). IRM retains its advantage on all but the first dataset. I liked
that IRM has advantage over ERM in confounded datasets as I believe many real datasets
can have confounders generating spurious correlations.
•	ERIC: Wow! These different variants of CMNIST sound exciting. Going back to the
covariate shift case, do you think that we can perhaps come up with a finer criterion to say
when IRM is better than ERM and vice-versa?
•	IRMA: Yes, actually I have been thinking about this problem and would be happy to share
my initial thoughts. Identifying a representation Φ* that leads to invariant conditional dis-
tributions is crucial to the success of IRM. A subtle factor that is implicitly assumed is
representations obtained from multiple domains should overlap.
•	ERIC: Can you clarify what you mean by overlap?
•	IRMA: Sure! Consider the dataset from domain 1, say photographs of birds, and domain 2,
say sketches Ofbirds. Imagine I have access to the oracle representation Φ*. Now I pass the
data from domain 1 and 2 through Φ* to get the representations. If these representations
from the two domains live in very different parts of the representation space, then we cannot
hope that IRM will offer any advantage. This is the other explanation, which I was going
to come to, why IRM did not outperform ERM in Gulrajani & Lopez-Paz (2020).
•	ERIC: How about when there is a complete overlap?
•	IRMA: Yes, if there is a strong or complete overlap in the representations from the two
domains, then it is possible that IRM can help. In fact I believe in such settings, if the
ERM models trained on the two domains disagree a lot, then that can be a strong indication
towards the models heavily exploiting spurious correlations. In such cases, I believe IRM
can again offer some advantage.
12
Published as a conference paper at ICLR 2021
•	ERIC: I will try to put these ideas down on paper and meet you again for a discussion.
•	Irma: Great! I truly hope that it can be in person at the Cafe in Palais-Royal where We first
started this conversation. So long for now!
[The students end their call.]
7.2	Supplementary Materials for Experiments
In this section, we cover the supplementary materials for the experiments. The code to reproduce
the results presented in this work can be found at https://github.com/IBM/OoD.
7.2.1	Classification
We first describe the model and other training details.
Choice of HΦ and other training details We use the same architecture for both ERM and IRM.
We choose the architecture that was used in Arjovsky et al. (2019): 2 layer MLP. The first two layer
consists of 390 hidden nodes, and the output layer has two nodes (for the two classes). We use
ReLU activation in each layer, a regularization weight of 0.0011 is used for each layer. We use a
learning rate of 4.9e-4, batch size of 512 for both ERM and IRM. We use 1000 gradient steps for
IRM. As was done in the original IRM work (Arjovsky et al., 2019), we use a threshold on steps
(190) after which a large penalty is imposed for violating the IRM constraint. We use the train
domain validation set procedure described in Gulrajani & Lopez-Paz (2020) to select the penalty
value from the set {1e4, 3.3e4, 6.6e4, 1e5} (with 4:1 train-validation split). With the same learning
rate, we observed ERM was slower at learning than IRM. To ensure ERM always converges we set
the number of epochs to a very high value 100 (118k steps).
A. Covariate shift based for CMNIST
We provide the generative model for CS-CMNIST below.
Y7 J L(Xe), Ce J Uniform({0,1})
Ue J BernoUlli((Ce ㊉ γe)ψe +(1 — (Ce ㊉ Ye))(I — ψe)))	(11)
Xe J T(Xg,Ce)|Ue = 1,Ye J Yge|Ue = 1
A.1. ComputeP(Ce|Ye) andP(Ye|Xe).
We compute P(Ce|Y e) and P(Y e|Xe) for the covariate shift based CMNIST described by equa-
tion 11. P(Ce |Y e ) helps us understand how the spurious correlations vary across the environments.
P(Y e|Xe) helps us understand if the covariate shift condition is satisfied or not. Compute the prob-
abilityP(Ce|Ye = 1) = P(Ce|Yge = 1, Ue = 1) as follows.
P(Ce = 1|Yge = 1,Ue
P(Ce = 1, Yge = 1|Ue
P(Ce = 0, Yge = 1|Ue
P(Ce = 1|Yge = 1,Ue
”___________P(Ce = ι,γe = ι∣ue = 1)__
) = P(Ce = 1,γe = 1|Ue = 1) + P(Ce = 0,γe = 1|Ue = 1)
1) =	P(C e=1,Ye = 1,U e = 1)	=1(1 - ψe)
)	Pa,bP(Ce = a,Yge = b,Ue = 1)	2( ψ )	(12)
p(c e = 0,γe = 1,u e = 1)	=1
) = PTPCeFYFɪueɪ^ = 2ψ
1)=(1-ψe)
From the above simplification we gather that P(Ce = 1|Ye = 1) is 0.9, 0.8 and 0.1 in the three
environments. Define pl = P (Yge = 1|Xge). In MNIST data itis reasonable to assume deterministic
labeling, i.e. pl = 1 or pl = 0. From the way the data is constructed we can assume that T is
invertible, i.e. from colored image Xe we can get back the grayscale image and the color Xge, Ce.
We now move to computing P(Ye|Xe). We assume Yge = 1 in the simplifcation below.
13
Published as a conference paper at ICLR 2021
grayscale image
Ue selection variable
Yg digit label
e environment index
Ce color index
Xe colored image
Ye final label
Figure 3: Graphical model for CS-CMNIST
P(Yge = 1,Ce = 0,Ue = 1|Xge) = P(Yge = 1|Xge)P(Ce = 0)P(U e = 1|Yge = 1,Ce = 0)
= 0.5plψe
P(Yge = 0,Ce = 0,Ue = 1|Xge) = P(Yge = 0|Xge)P(Ce = 0)P(U e = 1|Yge = 1,Ce = 0)
= 0.5(1 - pl)(1 - ψe)
P(Ye = 1|Xe) = P(Yge = 1|Xge,Ce = 0,Ue = 1) =	(13)
P(Yge = 1,Ce = 0, Ue = 1|Xe)
P(Yge = 1,Ce = 0, Ue = 1|Xe) + P(Yge = 0,Ce = 0, Ue = 1|Xe)
P(Y e = 1|X e) = —■~plψe-----------
2pi ψe + 1 - Pl - ψe
Observe that under standard assumption of deterministic labeling pl = 1, P(Ye = 1|Xe) remains
invariant and for high values of Pi it is relatively stable across the environments.
A.2 Graphical model for covariate shift based CMNIST. In Figure 3, we provide the graphical
model for covariate shift based CMNIST described in equation 11.
A.3. Results with numerical values and standard errors. In Table 2, we provide the numerical
values for the results showed in the Figure 2 a) along with the standard errors.
B.	Colored MNIST with anti-causal variables (AC-CMNIST)
B.1.	Compute P(Ce|Ye) and P(Ye|Xe). We first compute P(Ce|Ye). P(Ce = 1|Ye = 1) =
P(Ye ㊉ Ne = 1|Ye = l) = P(Ne = 0|Ye = 1) = βe. Therefore, P(Ce = 1|Ye = 1) is 0.9, 0.8
and 0.1 in environments 1, 2, and 3 respectively. Next, we compute P(Y e|Xe). We assume Yge = 1
in the simplfication below. We also assume deteministic labeling. In the simplfication that follows
we use β = 0.25.
14
Published as a conference paper at ICLR 2021
Method	Number of samPles	Test error
ERM	1000	47.27 ± 0.72
IRM	1000	42.84 ± 0.94
ERM	5000	23.91 ± 1.23
IRM	5000	28.85 ± 5.21
ERM	10000	15.80 ± 0.63
IRM	10000	14.80 ± 0.30
ERM	30000	9.86 ± 0.32
IRM	30000	11.20 ± 0.64
ERM	60000	8.15 ± 0.37
IRM	60000	9.62 ± 0.67
Table 2: Comparison of ERM vs IRM: CS-CMNIST
P(Ye = 1, Ce = P(Ye = 1, Ce = P(Ye = 0, Ce = P(Ye = 1, Ce =	0|Xge) = 0|Xge) = P(Ye = 1|Yge = 1)P(Ce = 0|Ye = 1) = 0.75βe 0|Xge) = 0|Xge) = P(Ye = 0|Yge = 1)P(Ce = 0|Ye = 0) = 0.25(1 -βe)
P(Ye = 1|Xe) P(Ye = 1|Xe) P(Ye = 1|Xe) P(Ye = 1|Xe)	3βe	(14) =P(Ye = 1|Xe, Ce = 0) = —β— I	Ig,	'	2βe + 1 = 0.25 (For environment 1) = 0.428 (For environment 2) = 0.964 (For environment 2)
B.2	Graphical model for anti-causal CMNIST In Figure 4, we provide the graphical model for
AC-CMNIST described in equation 10 (for G = 1).
B.3.	Results with numerical values and standard errors. In Table 3, we provide the numerical
values for the results showed in the Figure 2 c) along with the standard errors.
C.	Colored MNIST with confounded variables (CF-CMNIST)
C.1. Compute P(Ce |Y e) and P(Y e |Xe). We start by computing P(Ce|Y e). Recall that Ce =
(N ㊉ Ne). In the SimPlfication that follows We use β = 0.25.
P(N = 0|Ye = 1)
P(N = 0,Ye = 1)
P(N = 0,Ye = 1) + P(N = 1,Ye = 1)
0.75 * 0.5
-----------------=0.75
0.75*0.5+0.25*0.5
(15)
We use the above to comPute P(Ce = 0|Ye = 1) = P(N = 0, Ne = 0|Ye = 1) + P(N = 1, Ne =
1|Ye = 1) = (1 - βe)0.75 + 0.25βe = 0.75 - 0.5βe. For environment 1, 2, 3 the above Probability
P(Ce = 0|Ye = 1) is 0.7, 0.65 and 0.30 resPectively. Next, we comPute the Probability P(Ye|Xe).
SuPPose Yge = 1 for the calculation below. We also assume deterministic labeling.
15
Published as a conference paper at ICLR 2021
Method	Number of samples	Test error
ERM	1000	78.04 ± 0.70
IRM	1000	77.12 ± 1.00
ERM	5000	75.23 ± 1.04
IRM	5000	74.89 ± 1.08
ERM	10000	74.68 ± 1.23
IRM	10000	60.42 ± 1.72
ERM	30000	69.87 ± 0.39
IRM	30000	49.61 ± 2.57
ERM	60000	71.96 ± 0.55
IRM	60000	32.78 ± 2.70
Table 3: Comparison of ERM vs IRM: AC-CMNIST
P(Ye	1,Ce=	1|Xge) =
P(Ye	1,Ce=	1|Xge) = P(Ye = 1|Yge = 1)P(Ce =
P(Ye	0, Ce =	1|Xge) =
P(Ye	1,Ce=	1|Xge) = P(Ye = 0|Yge = 1)P(Ce =
P(Ye	1|Xe)	3βe =P(Ye = 1|X e,Ce = 1) = —β— I	Ig,	7	2βe + 1
P(Ye	1|Xe)	= 0.25 (For environment 1)
P(Ye	1|Xe)	= 0.428 (For environment 2)
P(Ye	1|Xe)	= 0.964 (For environment 2)
1|N =0) = 0.75βe
1|N = 1) = 0.25(1 - βe)
(16)
C.2 Graphical model for confounded CMNIST. In Figure 4, we provide the graphical model for
confounded CMNIST described in equation 10 (for G = 0).
C.3. Results with numerical values and standard errors. In Table 4, we provide the numerical
values for the results showed in the Figure 2 b) along with the standard errors.
D.	Colored MNIST with anti-causal variables and confounded variables (HB-CMNIST)
D.1. ComputeP(Ce|Ye) andP(Ye|Xe).
We start by computing P(C e∣Ye). Recall that Ce = G(Ye ㊉ Ne) + (1 - G)(N ㊉ Ne). G = 1 with
probability θ and 0 otherwise. P(Ce = 1|Ye =	1) = P(C e = ImYe = 1)	(17)
P(Ce = 1,Ye = 1) =θ(1 -βe) +(1 - θ)(0.25 + 0.5βe)	(18)
P(Ce = 0,Ye = 1) = θ(βe) + (1 - θ)(0.75 - 0.25βe)	(19)
We used θ = 0.8 in the experiments. P(Ye |Xe) can be computed on the same lines as was shown
for anti-causal and confounded model and it varies significantly across the environments.
D.2 Graphical model for confounded CMNIST.
In Figure 4, we provide the graphical model for confounded CMNIST described in equation 10 (for
0<θ< 1).
D.3. Results with numerical values and standard errors. In Table 5, we provide the numerical
values for the results showed in the Figure 2 d) along with the standard errors.
16
Published as a conference paper at ICLR 2021
Method	Number of samples	Test error
ERM	1000	65.21 ± 0.64
IRM	1000	65.60 ± 0.53
ERM	5000	67.91 ± 0.40
IRM	5000	67.59 ± 1.14
ERM	10000	66.92 ± 0.30
IRM	10000	59.26 ± 1.62
ERM	30000	67.32 ± 0.28
IRM	30000	47.01 ± 1.37
ERM	60000	67.62 ± 0.31
IRM	60000	44.92 ± 0.84
Table 4: Comparison of ERM vs IRM: CF-CMNIST
Method	Number of samples	Test error
ERM	1000	65.38 ± 0.56
IRM	1000	66.98 ± 0.61
ERM	5000	62.92 ± 0.99
IRM	5000	61.09 ± 1.74
ERM	10000	62.08 ± 1.19
IRM	10000	51.46 ± 1.22
ERM	30000	60.48 ± 0.42
IRM	30000	34.96 ± 1.47
ERM	60000	59.70 ± 0.72
IRM	60000	32.80 ± 0.55
Table 5: Comparison of ERM vs IRM: HB-CMNIST
7.2.2	Regression
We use the same structure for the generative model as described by Arjovsky et al. (2019). We work
with the four different variants on the same lines as CMNIST (covariate shift based, confounded,
anti-causal, hybrid). The comparisons in Arjovsky et al. (2019) were for anti-causal and hybrid
models. The general model is written as
He 一 N(0,σ2ls)
Xe 一 N(0,σ2ls)+ Wh→ιHe
Ye 一 We→yXe + N(0, σ2) + Wh→yHe
Xe 一 Wy→2Ye + N(0, Is) + Wh-2He
(20)
He is the hidden confounder, Xe = [X1e, X2e] is the observed covariate vector, Ye is the label. Dif-
ferent W’s correspond to the weight vectors that multiply with the covariates and the confounders.
The four datasets differ in the weight W vectors and we describe them below. σe is environment
17
Published as a conference paper at ICLR 2021
Ce color index Xe colored image	Q select b/w anti-causal
confounded
Figure 4: Graphical model for CF-CMNIST, AC-CMNIST, HB-CMNIST
CS-RegreSSion
Figure 5: Graphical models for the different regression datasets
dependent standard deviation, We use two training environments with σ1 = 0.2 and σ2 = 2.0
respectively and both environments .
•	Covariate shift case (CS-regresion): In this case, we fix Wh→2, Wh→y, and Wy→2 to zero
and we draw each entry of W1→y from SN(0,1) and set Wh→1 to identity.
•	Confounded variable case (CF-regression): Set Wy→2 to zero and we draw each entry in
Wι→y, Wh→1,Wh→2, Wh→y from SN(0,1).
•	Anti-causal variable case (AC-regression): Set Wh→y ,Wh→1 and Wh→2 to zero and draw
each entry of W1→y and Wy→2 from SN(0,1).
•	Hybrid confounded and anti-causal variable case (HB-regression): Draw each
Wh→y,Wh→ι, Wh→2, Wι→y and Wy→2 from SN(0,1).
We also present the graphical models for the four types of models used in Figure 5.
7.2.3	CHOICE OF HΦ AND OTHER TRAINING DETAILS
We use a linear model that takes as input Xe . For ERM we use standard linear regression from
sklearn. For IRM, we use 50k gradient steps with learning rate 1e-3, batch size is equal to the size
of the training data. We use the train domain validation set procedure described by Gulrajani &
Lopez-Paz (2020) to select the penalty value from the set {0, 1e - 5, 1e - 4, 1e - 3, 1e - 2, 1e - 1}
(with 4:1 train-validation split). We average the results over 25 trials.
18
Published as a conference paper at ICLR 2021
7.2.4	Results
We discuss results for the case when the length of the covariate vector X e is 10. The desired
optimal invariant predictor is W * = [Wι→y, 0]. We will compare ERM and IRM in terms of the
model estimation error, i.e., the distance between the model estimated by the method W and the
true model given as kW - W* k2. In Figures 6, 7, 8, 9, we compare the model estimation error
vs. the number of samples when the model. In these comparisons, we see that consistent with the
classification experiments and predictions from Proposition 4 in the covariate shift case (See Figure
6) there is no clear winner between the two approaches. There are gains from using IRM in the other
cases (Figures 7, 8, 9). However, in the confounder case in Figure 7, the gains from IRM appear in
the low sample regime but are not there in the high sample regime. This is because for this setup
the asymptotic bias of ERM is also very small. In addition to the Figures 6, 7, 8, 9, we provide
the tables (see Table 6, 7, 8, 9) with the numerical values for the mean model estimation (and the
standard error) error shown in the figures.
Figure 6: Comparisons: n = 10 CS-regression
l. 1.75-
O
⅛ 1∙5°-
O 1∙25 -
E 1.00-
⅛ 0.75 -
Φ
"φ 0.50-
P
: 0.25-
o.oo-
0	500	1000 1500 2000 2500 3000 3500 4000
Number of samples
Figure 7: Comparisons: n = 10 CF-regression
Figure 8: Comparisons: n = 10 AC-regression
0.25-
0.00-U------1--------1---1------1------1-----1------1----->—
0	500	1000	1500	2000	2500	3000	3500	4000
Number of samples
Figure 9: Comparisons: n = 10 HB-regression
19
Published as a conference paper at ICLR 2021
Method	Number of samples	Model estimation error
ERM	50	0.65 ± 0.06
IRM	50	0.57 ± 0.10
ERM	200	0.18 ± 0.02
IRM	200	0.13 ± 0.02
ERM	500	0.08 ± 0.005
IRM	500	0.08 ± 0.014
ERM	1000	0.037 ± 0.004
IRM	1000	0.051 ± 0.007
ERM	1500	0.021 ± 0.002
IRM	1500	0.045 ± 0.007
ERM	2000	0.018 ± 0.002
IRM	2000	0.035 ± 0.006
Table 6: Comparison of ERM vs IRM: n = 10 CS-regression
Method	Number of samples	Model estimation error
ERM	50	1.48 ± 0.15
IRM	50	0.59 ± 0.10
ERM	200	0.30 ± 0.03
IRM	200	0.20 ± 0.03
ERM	500	0.14 ± 0.01
IRM	500	0.15 ± 0.02
ERM	1000	0.10 ± 0.02
IRM	1000	0.09 ± 0.01
ERM	1500	0.07 ± 0.01
IRM	1500	0.10 ± 0.01
ERM	2000	0.07 ± 0.01
IRM	2000	0.07 ± 0.01
Table 7: Comparison of ERM vs IRM: n = 10 CF-regression
7.3 Proofs for the Propositions
In the results to follow, we will rely on Hoeffding’s inequality. We restate the inequality below for
convenience.
Lemma 1. (Hoeffding’s inequality). Let θ1 , . . . θm be a sequence of i.i.d. random variables and
assume thatforαll i, E[θi] = μ and P[a ≤ θi ≤ b] = 1. Then,forany e > 0
m2
1	m
P m lX θi-* >e ≤ 2exp(-2(b - a)2 )
i=1
We restate the propositions from the main body. Next, we prove Proposition 1 from the main body
of the manuscript.
20
Published as a conference paper at ICLR 2021
Method	Number of samples	Model estimation error
ERM	50	1.57 ± 0.18
IRM	50	0.82 ± 0.20
ERM	200	0.90 ± 0.08
IRM	200	0.63 ± 0.08
ERM	500	0.81 ± 0.08
IRM	500	0.61 ± 0.05
ERM	1000	0.75 ± 0.07
IRM	1000	0.53 ± 0.04
ERM	1500	0.79 ± 0.07
IRM	1500	0.53 ± 0.05
ERM	2000	0.77 ± 0.08
IRM	2000	0.54 ± 0.04
Table 8: Comparison of ERM vs IRM: n = 10 AC-regression
Method	Number of samples	Model estimation error
ERM	50	1.81 ± 0.19
IRM	50	1.13 ± 0.18
ERM	200	0.87 ± 0.09
IRM	200	0.68 ± 0.10
ERM	500	0.83 ± 0.09
IRM	500	0.67 ± 0.07
ERM	1000	0.76 ± 0.07
IRM	1000	0.61 ± 0.05
ERM	1500	0.77 ± 0.07
IRM	1500	0.63 ± 0.05
ERM	2000	0.74 ± 0.07
IRM	2000	0.58 ± 0.05
Table 9: Comparison of ERM vs IRM: n = 10 HB-regression
Proposition 7. If' is square loss, andAssumptions 1,2 hold, then m ◦ Φ* solves the OOD problem
(equation 1).
Proof. Define a predictor W ◦ Φ*, where Φ* is defined in Assumption 1. Let Us simplify the expres-
sion for the risk for this predictor Re(W ◦ Φ*) using square loss for '. Recall Ze = Φ*(Xe)
21
Published as a conference paper at ICLR 2021
Re(woφ*) = Ee (ye -Ee[ye∖ze] + Ee[ye∣ze] - (W◦ φ*)(xe))2
=Ee (Ye - Ee[ye ∖ Ze])2 + Ee (Ee[ye ∖ Ze] - (w o Φ*)(Xe))2 +
2Ee (Ye - Ee [Ye ∖ Ze])(Ee[Ye ∖ Ze] - (w o Φ*)(Xe))
=Ee	(Ye	- m(Ze))2	+ Ee	(m(Ze) -	w(Ze))2	+ 2Ee	(Ye	- m(Ze))(m(Ze)	- w(Ze))
=Ee	(Ye	- m(Ze))2	+ Ee	(m(Ze) -	w(Ze))2
=ξ2 + Ee (m(Ze) - w(Ze))2
(21)
In the above simplification in equation 21, we use the following equation 22 and equation 23, which
rely on the law of total expectation.
Ee (Ye - m(Ze))(m(Ze) - w(Ze))] = Ee Ee [(Ye - m(Ze))(m(Ze) - w(Ze)) ∖ Ze]
=Ee(Ee[Ye ∖ Ze] - m(Ze))(m(Ze) - w(Ze))"∣ = 0
(22)
Ee(Ye - m(Ze))2] = Ee Ee [(Ye - m(Ze))[ Ze] = Ee Vare [Ye ∖ Ze] = ξ2	(23)
In the last equality in equation 23, We use the Assumption 1 and obtain ξ2. Therefore, substituting
W = m in equation 21 achieves a risk of ξ2 for all the environments.
∀e ∈ %”，Re(m o Φ*) = ξ2	(24)
Consider the environment q that satisfies Assumption 2. Let us simplify the expression for the risk
achieved by every predictor f ∈ F in the environment q following the steps similar to equation 21.
Rq (f) = Eq
-Eq [Yq|Zq])2] +
Eq(Eq[Yq|Zq] - f (Xq))2 +2Eq (Yq - Eq[Yq|Zq])(Eq[Yq|Zq] - f (Xq)
Eq
(Yq - m(Zq))2"∣ + Eq (m(Zq) - f (Xq)
2 + 2Eq
(Yq - m(Zq)) (m(Zq) - f (Xq)
Eq
=ξ2 + Eq (m(Zq) - f (Xq)
(Yq - m(Zq))2] + Eq
)-f (Xq))2
(25)
In the above simplification in equation 25, We use the following equation 26
Eq
(yq - m(Zq))(m(Zq) - f (Xq)))	= Eq EeKYe - m(Zq))(m(Zq) - f (Xq)) ∖ Zq]
Eq
(Eq [Yq ∖ Zq]-
m(Zq)) (m(Zq) - f (Xq)
0 (Last equality follows from the Assumption 2)
(26)
22
Published as a conference paper at ICLR 2021
Therefore, for environment q satisfying Assumption 2 from equation 25, it follows that for all f ∈ F,
Rq (f) ≥ ξ2 . Therefore, we can write that
∀f ∈ F, max Re (f) ≥ Rq(f) ≥ ξ2	(27)
e∈Eall
Therefore, from equation 27 it directly follows that
min max Re(f) ≥ ξ2	(28)
f∈F e∈Eall
We showed in equation 24, Re(m ◦ Φ*) = ξ2 for all the environments.
Hence, f = m ◦ Φ* achieves the RHS of equation 28. This completes the proof.	□
Some of the proofs that we describe next take a few intermediate steps to build. Here we give a brief
preview of the key ingredients that we developed to build these propositions.
•	In Proposition 2, our goal is to carry outa sample complexity analysis of EIRM in the same
spirit as ERM. However, there are two key challenges that we are faced with - i) the IRM
penalty R0 is not separable (as it is composed of terms involving squares of expectations)
and ii) unlike ERM, IRM is a constrained optimization problem. To deal with i), we develop
an estimator in the next section that allows us to re-express IRM penalty in a separable
fashion. To deal with ii), we define a parameter κ that measures the minimum separation
between IRM penalty and . We show that as long as this separation for all the predictors in
the hypothesis class is positive, then we can rely on κ-representative property from Shalev-
Shwartz & Ben-David (2014) applied to the new estimator that we build to show that the set
of empirical invariant predictors SIV(E) are the same as exact invariant predictors SIV(e).
•	In Proposition 5, our goal is to show that approximate OOD can be achieved by IRM in the
finite sample regime. This result builds on the infinite sample result from Arjovsky et al.
(2019). In Theorem 9 in Arjovsky et al. (2019), it was shown that for linear models (defined
in Assumption 5) obeying linear general position 6, if the gradient constraints in the exact
gradient constraint IRM equation 4 are satisfied, then the OOD solution is achieved. We
extend this result to show that if the constraints in the approximate E penalty based IRM
in equation 5 are satisfied, then We are guaranteed to be in the √E neighborhood of the
OOD solution. Note that this result is again in the infinite sample regime as it proves the
approximation for solutions of the problem equation 5, which involves expectations w.r.t
true distributions. Next, we exploit similar tools that we introduced to prove Proposition 2
to also prove the finite sample extension.
•	In later sections, we show the generalizations to infinite hypothesis classes. In particu-
lar, we focus on parametric model families that are Lipschitz continuous. The extension
to infinite hypothesis classes is based on carefully exploiting the covering number based
techniques Shalev-Shwartz & Ben-David (2014) for the IRM penalty estimator that we in-
troduced. We also provide generalizations of the results for linear models to polynomial
models. To arrive at these results, we exploit some standard properties of tensor products.
7.3.1	EMPIRICAL ESTIMATOR OF R0
Next, we define an estimator for R0. We first simplify R0 as follows.
Observe that
Vw∣w=1.0Re(w ∙ Φ)
∂Ee ['(w ∙ Φ(Xe),Ye) I	= Ee Γ∂'(w ∙ Φ(Xe),Ye) I	-
∂w	w=1.0	∂w	w=1.0
and
kVw∣w=ι.oRe(w∙Φ)k2 =
∂Ee ['(w ∙ Φ(Xe),Ye)] I	y
∂w	Iw=1.0
. Φ(X e ),Y e)
∂w
w=1.0
(29)
2
23
Published as a conference paper at ICLR 2021
In the above simplification, we used Leibniz integral rule and take the derivative inside the expecta-
tion.
Also we can write E[X]2 = E[AB], where A and B are independent and identical random variables
with same distribution as X. Therefore, We consider two independent data points (Xe, Ye)〜 Pe
and (Xe,Ye)〜Pe.
Mlw=LORe (W ^ Φ)k2 = E' ](d'(w<≡ LJ( d'(w<m L=ij](30)
In the above the expectation Ee is taken over the joint distribution over pairs of distributions of pairs
(Xe, Ye), (Xe, Ye) from the same environment e.
We write
R (Φ)= E ∏ekvw∣w=1.0Re(w ∙ Φ)k2
e∈EtrπeEe
• Φ(Xe),Ye) I
∂w	w=1.0
,~ ~
• Φ(Xe ),Ye) I
∂w	Iw=1.0
(31)
In the above simplification, we used equation 30. Define a joint distribution P over the tuple
(e, (Xe, Ye), (Xe, Ye)), where e 〜{∏o}o∈Etr, (Xe, Ye)〜Pe and (Xe, Ye)〜Pe. Also,
P (e, (Xe, Ye), (Xe, Ye)) = ∏ePe(Xe, Ye)Pe(Xe, Ye)
(32)
We rewrite the above expression equation 31 in terms of an expectation w.r.t P, which we represent
as E follows
0
R (Φ) = E
. Φ(X e),Y e)
∂w
,~ ~
Φ(X e),Ye) I
∂w	Iw=1.0
(33)
Define
`0 (h, ((X e,Y e),(Xe,Ye)))=(
∂'(w . Φ(Xe),Ye)
∂w
,~ ~
勺Xq ∣	)o’)
w=1.0	∂w	w=1.0
Substitute equation 34 in equation 33 to obtain
R0(Φ) = E '(h, ((Xe,Ye),(Xe,Ye)))
(35)
We construct a simple estimator R (Φ) by pairing the data points in each environment. For SimPlic-
ity assume that each environment has even number of points. In environment e, which has ne points
we construct n2e pairs. Define a set of such pairs as
D = {{(Xei-1,yei-I), (X2i
ne
yei)}4 l}e∈Etr
(36)
ne
2	2	/	、
R0(Φ) =而 E ∑'0 (h, ((Xei-ι,Ye-ι), (Xei, Yei)))	(37)
e∈Etr i=1
There can be other estimators of R0 (Φ) where we separately estimate each term
∣∣Vw∣w=1.0Re(w.Φ)k2 and πe in the summation. We rely on the above estimator equation 37 as its
separability allows us to use standard concentration inequalities, e.g., Hoeffding’s inequality.
24
Published as a conference paper at ICLR 2021
7.3.2	-REPRESENTATIVE TRAINING SET FOR R AND R0
We use the definition of -representative sample from Shalev-Shwartz & Ben-David (2014) and state
it appropriately for both R and R0 .
Definition 1. A training set S is called -representative (w.r.t. domain Z, hypothesis H, loss ` and
distribution D) if
.ʌ , ..
∀h ∈ H, ∣R^(h) - R(h)∣ ≤ E	(38)
where R(h) = ED['(h(X), Y] and (X, Y)〜D.
Following the above definition, we apply it to the set of points D defined above in equation 36. D
0
is called E-representative w.r.t. domain X , hypothesis HΦ, loss ` (equation 33) and distribution P
(equation 32) if
∀Φ ∈ Hφ, ∣R0(Φ) — R0(Φ)∣ ≤ E	(39)
where	R0 (Φ)	= E '0 (h, ((Xe, Ye), (Xe, Ye)))	(from equation 34) and
,, ,,~ ~ .. ~
(e,(Xe,Ye),(Xe,Ye))〜P.
Recall the definition of κ, K = minφ∈Hφ |R0 (Φ) 一 e|. Next, We show that if DD is 2-representative
w.r.t X, Hφ, loss '0, distribution P then the set of invariant predictors in equation 6 (SIV (E)) and the
set of invariant predictors in equation 5 (SIV(E)) are equal.
0
Lemma 2. If κ > 0 and D is 2-representative w.r.t X, Hφ, loss ' and distribution P , then
SIV (E)= S IV(E).
Proof. First we show SIV(E) ⊆ SIV(E). From the definition of κ, K = minφ∈Hφ |R0 (Φ) 一 e| it
follows that ∀Φ ∈ HΦ
|R0(Φ) 一 e| ≥ κ =⇒ R (Φ) ≥ e + κ or R (Φ) ≤ E 一 K	(40)
Consider any Φ in SIV(E).
R0(Φ) ≤ E	(41)
Given the definition of K and equation 40, we obtain
R0 (Φ) ≤ E =⇒ R0 (Φ) ≤ E 一 K	(42)
Therefore, SIV(E) ⊆ SIV(E 一 K). Also, it follows from the definition of the set SIV(E) that SIV(E 一
K) ⊆ SIV(E). Hence,
SIV(E) =SIV(E 一 K)	(43)
Consider any Φ in SIV(E)
R0 (Φ) ≤ E 一 K (From equation 43)
R (Φ) 一 R0(Φ) + R0(Φ) ≤ E 一 K	(44)
R0(Φ) ≤ e - κ + |R0(Φ) - R0(Φ)∣
From the definition of 2-representativeness it follows that |R0 (Φ) 一 R0(Φ)∣ ≤ K and substituting
this in equation 44 we get
R0(Φ) ≤ E 一 k/2 =⇒ R0(Φ) ≤ E =⇒ SIV(e) ⊆ SIV(E)	(45)
Next we show SIV(E) ⊆ SIV(e).
Consider Φ ∈ SIV(e)
25
Published as a conference paper at ICLR 2021
R0(Φ) ≤ e
R0 (Φ) - R (Φ) + R (Φ0) ≤ e	(46)
R (Φ) ≤ e + ∣R0(Φ) - R0(Φ)∣
From the definition of 2-representativeness it follows that |R0(Φ) - R(Φ)∣ ≤ K and substituting
this in equation 46 we get
R0 (Φ) ≤ C + 2	(47)
From equation 40, it follows that R0(Φ) ≤ C + K =⇒ R'(Φ) ≤ c. Therefore, Φ ∈ SIV(c). This
proves the second part SIV(C) ⊆ SIV(C) and completes the proof.
□
Lemma 3. If κ > 0, D is ν-representative w,r,t X, Hφ, loss ' and distribution P (joint distribution
over (e, Xe, Ye) defined in Section 3.1) and and D is κ-representative w,r,t X, Hφ, loss ' and
distribution P, then every solution Φ to EIRM (equation 6) satisfies Φ is in SIV(c) and R(Φ*) ≤
R(Φ) ≤ R(Φ*) + c, where Φ* is the solution of IRM in equation 5.
Proof. Given the condition in the above lemma, we are able to use the previous Lemma 2 to deduce
that SIV (c) = S IV(c). This makes the set of predictors satisfying the constraints in EIRM equation 6
and IRM equation 5 the same.
Φ* solves equation 5 and Φ solves equation 6. From ν-representativeness We know that R(Φ) - 2 ≤
R(Φ). From the OPtlmality of Φ we know that R(Φ) ≤ R(Φ*) (Φ* ∈ SIV(C) = SIV(C)). Moreover,
from 2-representativeness we know that R(Φ*) ≤ R(Φ*) + 2. We combine these conditions as
follows.
R(Φ) - 2 ≤ R(Φ) ≤ R(Φ*) ≤ R(Φ*) + 2	(48)
Comparing the first and third inequality in the above equations we get R(Φ) ≤ R(Φ*) + V. From the
optimality of Φ* over the set SIV(c) and since Φ ∈ SIV(c) it follows that R(Φ*) ≤ R(Φ). Hence,
R(Φ*) ≤ R(Φ) ≤ R(Φ*) + V. This completes the proof.
□
Next, we prove Proposition 2 from the main body of the manuscript.
Proposition 8. For every V > 0, C > 0 and δ ∈ (0, 1), ifHΦ is a finite hypothesis class, Assumption
3 holds, κ > 0, and if the number of samples |D| is greater than max {16L-, ^L~ } log (4|Hφ1 ),
then with a probability at least 1 - δ, every solution Φ of EIRM (equation 6) is a V approximation
ofIRM, i.e. Φ ∈ SIV(c), R(Φ*) ≤ R(Φ) ≤ R(Φ*) + V, where Φ* is a solution to IRM(equation5).
Proof. From Lemma 3, we know that if D is ν-representative w.r.t X, Hφ, loss ' and distribution
0
P and if D is K -representative X, Hφ, loss ' and distribution P, then the the claim in the above
Proposition is true.
Define an event A: D is 2-representative w.r.t X, Hφ, loss ' and distribution P.
0
Define an event B: D is K-representative X, Hφ, loss ' and distribution P.
Define success as A ∩ B. Next, we show that if |D| is greater than max {16L4,雪} log (4|Hφ1 ),
then P(A∩B) occurs with a probability at least 1 - δ. P(A∩ B) = 1 - P(Ac ∪Bc) ≥ 1 - P(Ac) -
26
Published as a conference paper at ICLR 2021
P(Bc). If We bound P(Ac) ≤ 2 and P(BC) ≤ 2, then We know the probability of success is at
least 1 - δ.
We Write
P(A) = P({D : Vh ∈ Hφ, ∖Rl(h)-R(h)∖ ≤ V}) = 1-P({D : ∃h ∈ Hφ, ∖Rl(h)-R(h)∖ > ν})
P({d : ∃h ∈Hφ,∖R(h)- R(h)∖ >2})= P( U {A∖R(h)- R(h)∖ >V})
h∈Hφ
≤ X p({d : ∖R(h) - R(h)∖ >2})
h∈Hφ
(49)
The loss function is bounded ∖'(Φ(∙), ∙)∖ ≤ L. From Hoeffding,s inequality in Lemma 1 it follows
that
p({d : ∖R(h) - R(h)∖ > V}) ≤ 2exp (一零)
Using this expression equation 50 in equation 49, we get
WEP (-零)≤ I =⇒∖D≥ 专 kg (4^
(50)
(51)
P(B) = P({力:Vh ∈ Hφ, ∖ R0(h)-Ro(h) ∖ ≤ ∙∣}) = 1-P({力:∃h ∈ Hφ, ∖ RR(h)-R (h)∖ > ")
P({Dτh ∈Hφ,∖R'(h)- R (h)∖ >K})= P( U {D: ∖ R0(h)- R0(h) ∖ >2})
h∈Hφ
≤ X p({D: ∖R0 (h) - R (h) ∖ >2 })
h∈Hφ
(52)
The gradient of loss function is bounded 产"",)∖ w=1.0 ∖ ≤ L0. From the definition of
20(h(∙),∙) in equation 34, we can infer that ∖ 20(h(∙),∙) ∖ ≤ L02. Recall that R0(h)=
E 卜(h, ((X e,γ e),(X e,Y e))J
From Hoeffding,s inequality in Lemma 1 it follows that
p({d : ∖ R0(h) - R (h)∖ > κ}) ≤ 2eχp ( - ⅛-) =2eχP ( - HL⅛)	(53)
Using the above equation 53 in equation 52 we get
2 ∖ Hφ ∖ exp (-
∖D ∖ κ2 )
16L04J
16L04 /4 ∖Hφ 八
T log(—)
(54)
≤ 2 =⇒ ∖ D ∖ ≥
Combining the two conditions in equation 51 and equation 51 we get that if
∖ D ∖ ≥ max
116L04 8L2u T ∖ Hφ ∖
[τ F }log( 丁
then with probability at least 1 - δ event A ∩ B occurs.
□
27
Published as a conference paper at ICLR 2021
7.3.3 Property of least squares optimal solutions
We first remind ourselves of a simple property of least squares minimization. Consider the least
squares minimization setting, where R(h) = E[(Y - h(X))2].
E[(Y - h(X))2] = Eh(Y - E[Y∣X] + E[Y∣X] - h(X))2]
=EX [e[(Y - E[Y∣X])2∣X]] + Eχ h(E[Y∣X] — h(X))2]	(55)
=EX [Var[Y∣X]] + EX h(E[Y∣X] - h(X))2]
In the above simplification, we use the law of total expectation. Both the terms in the above equations
are always greater than or equal to zero. The first term does not depend on h, which implies the
minimization can focus on second term only.
minR(h) = EXhVar[Y∣X]] + minEX [(E[Y∣X] - h(X))2]	(56)
Assume that P has full support over X. Define ∀x ∈ X,h*(x) = E[Y|X = x]. ∀h, R(h) ≥
EX [Var[Y∣X]]. Since R(h*) = EX [Var[Y∣X]]. Therefore,
h ∈ argminEX [(E[Y∣X] - h(X))2]	(57)
Moreover, We conclude that h is the unique minimizer. Observe that EX [(E[Y∣X] - h(X))2] is
zero for h = h*. From Theorem 1.6.6 in (ASh et al., 2000), it follows that any other minimizer is
same as h except over a set of measure zero.
Recall the definition of m(x) = E[Ye |Xe = x]
Lemma 4. Let ` be the square loss. If Assumption 4 holds and m ∈ HΦ, then m uniquely solves
expected risk minimization m ∈ arg minΦ∈HΦ R(Φ) and also uniquely solves IRM (equation 5).
Proof. R(Φ) = Pe∈E πeRe(Φ). From Assumption 4 and the observation in equation 57, it fol-
lows that the unique optimal solution to expected risk minimization for each Re is m. Therefore, m
also minimizes the weighted combination R.
To show the latter part of the Lemma, if we can show that m ∈ SIV(), then the rest of the proof
follows from the previous part as we already showed m is a minimizer among all the functions in
HΦ and SIV() ⊆ HΦ.
Suppose m ∈ SIV (e). This implies there exists at least one environment for which ∣∣Vw∣w=1.0Re(W ∙
m) ∣∣2 > 0 =⇒ Vw∣w=1.0Re(w ∙ m) = 0. As a result ∃ W in the neighborhood of W = 1.0 where
Re(w ∙ m) < Re(m) (if such a point does not exist and all the points in the neigbohood of W = 1.0
are greater than or equal to Re(m) that would make Vw∣w=1.0Re(w ∙ m) = 0, which would be
a contradiction). Therefore, Re(W ∙ m) < Re(m). However, this is a contradiction as we know
that m is the unique optimizer for each environment. Hence, m 6∈ SIV() cannot be true and thus
m ∈ SIV(). This completes the proof.
□
Next, we prove Proposition 4 from the main body of the manuscript.
Proposition 9. Let ` be the square loss. For every ν > 0, > 0 and δ ∈ (0, 1), if HΦ is a finite
hypothesis class, m ∈ HΦ, Assumptions 3, 4 hold, and
• if the number of samples |D| is greater than max { 8L2- log( 4|H φ1 ), 16L4 log( ∣)}, then with a
probability at least 1 - δ, every solution Φ to EIRM (equation 6) satisfies R(m) ≤ R(Φ) ≤ R(m) +
V. Ifalso ν < κ ,then Φ* = m.
• ifthe number ofsamples |D| is greater than 笔2 log( 2|Hφ1 ), then with a probability at least 1 一 δ,
every solution Φ* to ERM satisfies R(m) ≤ R(Φ*) ≤ R(m) + V. Ifalso ν < 方,then Φ* = m.
28
Published as a conference paper at ICLR 2021
Proof. We first cover the second part of the Proposition. From Proposition 3, we know that the
output of ERM will satisfy R(Φ+) ≤ R(Φ*)≤ R(Φ+) + ν. In this case from Lemma 4, it follows
that Φ+ = m. From the definition of K and the fact that ν < κ implies that Φ* = m. We now move
to the first part of the Proposition.
For EIRM we will derive a tighter bound on sample complexity than the one in Proposition 2 since
we can now use the Assumption 4. Observe that ∀e ∈ Etr, Vw∣w=1.0Re(w ∙ m) = 0 (see the proof
of Lemma 4). Therefore, R0 (m) = 0.
Define an event A: D is 2-representative w.r.t X, Hφ, loss ' and distribution P.
Define an event B: D is such that |R0 (m) 一 R0(m)∣ ≤ 2. Since R0(m) = 0, ∣R0(m) — R0(m)∣ ≤
2 =⇒ ∣R0(m)∣ ≤ N =⇒ R0(m) ≤ f. Therefore, m ∈ SIV(e).
If A ∩ B occurs, then R(m) ≤ R(Φ) ≤ R(m) + ν; we justify claim next. Suppose Φ solves
equation 6. If event A occurs, then from 2 -representative condition we know that R(Φ) — 2 ≤ R(Φ).
From optimality of Φ it follows that R(Φ) ≤ R(m) (event B =⇒ m ∈ S IV ()). Moreover, from
2-representative property, we conclude that R(m) ≤ R(m) + 2. We combine these conditions as
follows.
R(Φ) — 2 ≤ R(Φ) ≤ R(m) ≤ R(m) + 2	(58)
1 -	.1	1	1	I 1 /	∖	T-I	11	.1	1 f' ∙ . ∙	i' ~	1	-〜
From the above we have R(m) ≤ R(Φ) ≤ R(m) + ν. Recall the definition of κK and since ν < κK
=⇒ Φ = m.
Next, we bound the probability of success. P(A ∩ B) = 1 — P(Ac ∪ Bc) ≥ 1 — P (Ac) — P(Bc).
If we can bound P(Ac) ≤ 2 and P(Bc) ≤ 2, then we know the probability of success is at least
1 — δ.
We write
P(A) = P({D : ∀h ∈ Hφ, |R(h)—R(h)∣ ≤ V}) = 1—P({D : ∃h ∈ Hφ, |R(h)—R(h)∣ > ν})
From equation 51 if the condition
∣D∣≥ 8L2 log (苧
ν2	δ
(59)
is true, then is true, then event Ac occurs with probability at most δ.
We write P(B) = P(DD :, ∣R0(m) — R0(m)∣ ≤ N} = = 1 — P(DD : ∣R0(m) — R0(m)∣ > j}).
The gradient of loss function is bounded | d'(；W),∙) ∣w=1.0∣ ≤ L0. From Hoeffding,s inequality in
Lemma 1 it follows that
p({D : ∣R0(h) — R (h)| > I}) ≤ 2exp( — IDLJ)	(60)
We bound the above equation 60 by 2 to get
2exp(—处2) ≤ δ =⇒ |D| ≥ 16L04log(4)	(61)
p 16L04-2 I I - i2 g'δj	7
Combining the two conditions equation 59 and equation 61,
∕8L2	4∣Hφ∣ 16L04	4 .
lDl≥ maxI Flog(—)，log( δ))
This ensures P(A ∩ B) ≥ 1 — δ. This completes the proof.
□
29
Published as a conference paper at ICLR 2021
Before stating the proof of Proposition 5, we will prove an intermediate proposition. For clarity, we
will restate the result (Theorem 9 from Arjovsky et al. (2019)) next.
Proposition 10. (Theorem 9 Arjovsky et al. (2019)) If Assumptions 5 and 6 (with r = 1) hold and
let Φ ∈ Rn×1 (Φ 6= 0), then
ΦTEe[XeXe,T]Φ = ΦTEe[XeY e]	(62)
holdsfor all e ∈ Etr iff Φ = STY∙
Next, we propose an -approximation of Proposition 10.
Define e° = ∣Et∏(ωλmin)2(12 — 8√2).
Proposition 11. Let ` be the square loss. If Assumptions 5, 6 (with r = 1) and 7 hold , then for all
0 < < 0, the solution Φ
R0(Φ) ≤	(63)
Saaisfies φ = STYm), Where α ∈ [ 1+	1 1 q,Jr | , 1~~1 1 qv∣Etr | ]
+ 2ωλmm V ∏min	2ωλmin V πmin
Proof. Let Us start by simplifying Vw∣w=ι.oRe(W ∙ Φ), using square loss for ' and linear represen-
tation Φ ∈ Rn×1.
VwIw=10Re(w ∙ Φ)= dEe [(ye -W ∙ φTXe) ] 1	。=2Ee [(ΦTXe)2] - 2Ee [ΦTXeYe]
= 2ΦTEe XeXe,TΦ - 2ΦTEe YeXe
(64)
Plug the above equation 64 in the condition R0 (Φ) ≤ to get
R0(Φ) = X ∏ekVw∣w=1.0Re(w ∙ Φ)k2 =4 X πe(φTEe[X eX e,T]Φ - ΦTEe[X eY e])2 ≤ e
e	e	(65)
From the bound on πe in Assumption 5 it follows that for each e ∈ Etr
ΦTEe[x ex e,T]Φ - ΦTEe[x eγ e])2 ≤ e|Ed
4πm'n	(66)
ΦTEe[xexe,T]Φ - ΦTEe[xeYe] I ≤ e∣eEmrn
Note that if the condition above equation 66 is not true, then the preceeding condition in equation 65
cannot be true as contribution from one term in the summation itself will exceed e. In the above
equation 66, we are using the positive square root since RHS has to be greater than or equal to zero.
We compute the second derivative of loss w.r.t W
VwRe(w ∙ Φ)
∂2Ee [(Ye - ΦTXew)]
∂w2
∂ 2WEe (ΦTxe)2 - 2Ee ΦTxeY e
∂w
2ΦTEe[xexe,T]Φ
2ΦTΣeΦ
(67)
Since Σe is symmetric, we can use the eigenvalue decomposition of Σe = UΛUT in equation 67
to get ΦTΣeΦ = ΦtUΛUtΦ. Substitute Φ = UtΦ to get ΦTΣeΦ = ΦtΛΦ ≥ λmin(∑e)∣∣Φ∣∣2 =
λmin(Σe)ΦTUUTΦ = λmin(Σe)kΦk2. From Assumption 5, we have λmin(Σe) ≥ λmin and from
Assumption 7 we have kΦk2 ≥ ω. Therefore, we can deduce that 2ΦTΣeΦ ≥ 2λminω. Therefore,
the second derivative defined in equation 67 is always greater than or equal to 2λminω.
VwRe(W ∙ Φ) ≥ 2λminω > 0	(68)
30
Published as a conference paper at ICLR 2021
Let J = 2√ ⅛Et⅛ = √ ^Etrɪ. We rewrite equation 66 in terms of Vw∣w=ι.oRe(W ∙ Φ) and / to get
∣Vw∣w=ι.oRe(W ∙ Φ)∣≤ /
—≤Vw∣w=1.0Re(w ∙ Φ) ≤ /
(69)
Since the second derivative (equation 68) is strictly positive and larger than or equal to λminω =⇒
∃ we in the neighborhood of W = 1.0 at which Vw∣w=weRe(W ∙ Φ) = 0. This holds for all the
environments in Etr . Define
C(W) = VwRe(W ∙ Φ)
Also, define c (w) = dC(w). Also, c (w) = VwRe(W∙Φ). Suppose Vw∣w=1.0Re(W∙Φ) = c(1) < 0.
Since for all W, c0(W) > 0 (from equation 68), ∃ We > 1, where c(We) = 0. Using fundamental
theorem of calculus, we write
c(W) - c(1) =	c0 (u)du ≥ 2λminω(W - 1)
Substituting W = We in the above
c(We) - c(1) ≥ 2λminω(We - 1)
-c(1) ≥ 2λminω(We - 1)
0
J
∣
We ≤ 1 ——C(1- ≤ 1 +
2λminω
—.....=1 +	—
2λmin ω	2λminω
(70)
Suppose Vw,w=1.0Re(W.Φ) = c(1) > 0. Using fundamental theorem of calculus we can write
c(1) - c(W) =	c (u)du ≥ λminω(1 - W)
w
Substituting W = We in the above
c(1) - c(We) ≥ 2λminω(1 - We)
c(1) ≥ 2λminω(1 - We)
0
J
∣
We ≥
1 c(1)>1
1 —	≥ 1 —
2λminω
—.....=1 -	—
2λmin ω	2λminω
(71)
1	∕e∣Et"
Define η =	2ωλmin V 求 ,η
1_____1	/ e IEtr |
2ωλmin V "in
1
2ωλmin
ι+T—
2ωλmin
|
| . Combining equation 70 and equation 71 and
using the definition of η1 and η2, we can conclude that We ∈ [ι⅛，1+η2]. If we reparamterize
We
ɪ+^, then ηe ∈ [η2, ηι]. We expand this condition Vw∣w=weRe(W ∙ Φ) = 0.

∂2
d-Ee[(Ye - ΦtXeW)2] =2WEe[(ΦTXe)2] - 2Ee [ΦtXeYe]
= 2ΦThEeXeXe,TΦWe - EeXeY ei
=2Φt 忸e [XeXe，T] ΦWe - Ee [XeXe，T] STY - Ee [Xeεe]]
(72)
=2Φt [Ee [XeXe，T]®We - STY) - Ee [Xeεe]]
=2Φt ∣Ee[XeXe,T](φWe - STY) - Ee [Xeεe]] = 0
=2Φt ∣Ee[XeXe，T] (Φγq1~^ - STY) - Ee [Xeεe]] = 0
31
Published as a conference paper at ICLR 2021
Assume that Φι+1ηe = STγ, for all e ∈ Etr. If this assumption is not true and Φι+1ηe = STY for
some e ∈ Etr , then it already establishes the claim we set out to prove in this Proposition (since
ηe ∈ [η2 , η1]).
[Xeεe]
Define qe = ∣Ee [XeXe,T] (φ 1+^ — STY) — Ee
From the Assumption 6 and since
Φι+1ηe = STγ, We know dim (span{qe}) > n _ 1.
From rank-nullity theorem we know dimension of kernel space of Φ (rank ofΦ is 1) is n - 1. From
equation 72 it follows that qe is in kernel space of Φ. Therefore, dim(Ker(Φ)) = n - 1 =⇒
dim span qe ≤ n - 1 which leads to a contradiction.
Therefore, Φ ι+ηe = STY at least for one environment.
If φ = STY(1 + ηe), then ∣∣Φ∣∣2 = kSTγk2(i + ηe)2 ≥ kSTγk2(i + η2)2 ≥ kSTγk21 ≥
ω. In this simplification, we use (1 + η2)2 ≥ 7----------1	、2 ≥ -7----------1	、2- =
/ 1+ 1 q / e lEtr | )	/ 1+ 1 q/^0∖Etr | )
∖ 1 2ωλmin V ∏min )	Cl 2ωλmin V ∏min )
1----- 1	、2 = 2 and IISTYk2 ≥ 2ω (from Assumption 7). This ensures that for any solution of
(1+√3-2√2)
the form Φ1+^ the assumption ∣∣Φ∣2 ≥ ω is automatically satisfied.
If Φ = Sty(1+ηe), then ∣Φ∣2 = kSTγk2(1+ηe)2 ≤ kSTγk2(1+η1)2 ≤ ISTYk2(3+2√2) ≤ ω.
In the above simplification, we use (1 + η1)2 ≤ -7-------1------ʌɪ ≤ -7----------1-------ʌɪ =
/1— 1	/ e∖Etr∖ )	1\_ 1 J e0∖Etr∖ )
< 2ωλmin V ∏min )	k 2ωλmin V ∏min J
-7----1	、2 = 3+2^2 and ∣StYk2 ≤ 2√ΩΩ (from Assumption 7). This ensures that for any
(1-√3-2√2)
solution of the form Φ 1+1^ the assumption ∣∣Φ∣2 ≤ Ω is automatically satisfied.
The entire proof so far has characterized the property of Φ that satisfies R0 (Φ) ≤ . But how do we
know such a Φ exists. Recall StY ∈ Hφ. For each environment e ∈ Etr
▽w|w=1.0Re(W ∙ Φ) = ΦTΕe[XeXe,T]Φ - ΦTEe[XeYe]
=YTSEe [XeXe，T] STY - YTSEe [XeYe]
= YTEe[Z1eZ1e,T]Y - YTEe [Z1eYe]	(73)
= YTEe[Z1eZ1e,T]Y -YTEe[Z1eZ1e,T]Y - YTEe [Z1eεe]
=0
We use Assumption 5 in the simplification above in equation 73 Therefore, R0 (StY) = 0 and as a
result the existence ofa Φ that satisfies R0 (Φ) ≤ is guaranteed. This completes the proof.
□
Before proving Proposition 5, we first establish that Assumptions 5 and 7 are sufficient to ensure
that Assumption 3 holds and we can thus use bounds L and L0 defined in Assumption 3.
Proving Assumption 3 for square loss ` from Assumptions 5, 7. From Assumption 5, we have
Ze = (Z1e,Z2e)
Xe = SZe
kXek ≤ kSkkZek
(74)
From Assumption 5, kS k is bounded and kZe k is bounded =⇒ kXe k is bounded as well (from
equation 74). Therefore, there exists Xsup < ∞, s.t. kXe k ≤ Xsup.
32
Published as a conference paper at ICLR 2021
Ye = (STY)TXe + εe
|YeI ≤kSTYkkXek + ∣εe∣ =⇒
|YeI ≤ J —2-^QXsup + εsup
y 3 + 2√2
(75)
In the last step of equation 75, We use IISTYk2 ≤ ^2√Ω (Assumption 7), ∣∣Xek ≤ XSup derived
above (equation 74), and Iεe I ≤ εsup (Assumption 5). Therefore, Ye is bounded and there exists a
K such that ∣Ye∣ ≤ K ≤ √ΩXSup + εsup. Therefore, ∀Φ ∈ Hφ and for all Xe, Ye sampled from
the model in Assumption 5 We have
'(Φ(Xe), Ye) = (Ye — ΦtXe)2 ≤ (K + √ΩXsup)2	(76)
∂'(w ∙ ΦtX, Y)
∂w
I I = ∣ΦtX(ΦtX - Y)1 ≤ (√ΩXsup)(√ΩXSuP + K)	(77)
w=1.0
From equation 76, we conclude that '(Φ(∙), ∙) is bounded and there exists an L such that
∣'(φ(∙), ∙)∣ ≤ L ≤ (K + √ΩXsup)2. From equation 77, we conclude that d'(W察乂丫) ∣
∂w	w=1.0
is bounded and there exists an L0 such that ∣ '侬然)，)∣ ɪj ≤ L ≤ (√ΩX sup)(√ΩX SuP + K).
Define th
24-36√2 -∏Emr∣ (ωλmin)2 and T
1	3 3 lEtr |
2ωλmmV 2πmin
Next, we prove Proposition 5 from the main body of the manuscript.
Proposition 12. Let ` be the square loss. For every ∈ (0, th) and δ ∈ (0, 1), if Assumptions 5, 6
(with r = 1), 7hold and ifthe number ofdatapoints ∣D∣ is greater than 16L log (2|Hφl), then with
a probability at least 1 - δ, every solution Φ to EIRM (equation 6) satisfies Φ = (STY α, where
α ∈ [ 1+1√, 1-T√] ∙
Proof. Define an event A: {D : ∀Φ ∈ Hφ, |R0 (Φ) — R0 (Φ) ∣ ≤ 2}. If event A happens, then
R' (Φ) ≤ e
R0 (Φ) - R (Φ) + R (Φ) ≤ e
R (Φ) ≤ e + ∣R0(Φ) — R0(Φ)∣
ro (Φ) ≤ 3e
(78)
If event A happens, then every solution Φ to EIRM (equation 6) satisfies equation 78. From equa-
tion 78, we see that we should substitute e with 学 and use Proposition 11. If the Assumptions 5,
6 (with r = 1), 7 hold and event A happens, then for all 0 < 3 e < e° the output of EIRM equa-
tion 6 Φ = St(α), where α ∈ [--------------1 ”U ∣ ,---------1 /U ∣ ] = [Z-,1 1 广].Note that
∖ 1	L 1∣	1	∕3e∣Etr∣ , [— 1	/匹LEtU」	∣-1+τ√e , 1-T√e-1
+ 2ωλmin V 2πmin	2ωλmin V 2πmin
3e < eo =⇒ e < eg. Hence, all that remains to be shown is that event A occurs with a probability
at least 1 — δ. Next, we show that if ∣D∣ ≥ 16L4 log(2|Hφl), then with probability 1 — δ event A
happens.
Now we need to bound the probability P(A). We will find an upper bound on the failure probability
using Hoeffding’s inequality (Lemma 1) and the bound L0 (derived in equation 77) as follows. We
redo the same analysis as was done in equation 54 for reader’s convenience.
33
Published as a conference paper at ICLR 2021
P(A) = 1-P({DTΦ ∈Hφ,∣R0(Φ)-R0(Φ)∣ > ∣})= P( U {D: ∣R0(Φ)-R0(Φ)∣ > ∣})
Φ∈HΦ
P( U {D"R0(Φ)- R (Φ)∣ > I}) ≤ X P({D:|R0(Φ)- R (Φ)∣ > I}) ≤ 2∣Hφ∣e- 16D
Φ∈HΦ	Φ∈HΦ
,	, ”|D|	〜，ɪ
2∣Hφ∣e- 16L04 ≤ δ =⇒ P(Ac) ≤ δ
|D| ≥ 16L04 log (2Hl) =⇒ P(Ac) ≤ δ
(79)
Hence, we know that if |D| ≥ 16L4 log (2|Hφ1 ), then with probability 1 - δ event A happens.
0
The proof characterized the property of Φ that satisfies R (Φ) ≤ I. But how do we know such a Φ
exists; we show that a Φ always exists. Consider a Φ that satisfiess the following.
R (Φ) ≤ I
R0(Φ)- R0(Φ) + R (Φ) ≤ I
R0(φ) ≤ I + ∣R0(φ)- R (Φ)l
R (Φ) ≤ ∣ (follows from event A)
(80)
From equation 73 in the proof of Proposition 11, We know that R0 (STγ) = 0. From equation 80,
STY ∈ Hφ satisfies R0(Φ) ≤ ∣.
□
Since equation 6 is a constrained optimization, a penalty based version (IRMv1) was proposed by
0
Arjovsky et al. (2019) that minimizes R(Φ) + λR (Φ). Both Propositions 4 and Proposition 5 can
be extended to IRMv1. Below we show the sample complexity analysis for IRMv1 applied to the
setting assumed in Proposition 5. Sample complexity analysis OfIRMvI below shows that distance
to the OOD solution decays as O(vz1∕λ). Define λth = max{5σ2∕3∈th, 1}
Corollary 1. For every δ ∈ (0, 1), λ > λth, if Assumptions 5, 6 (with r = 1), 7 hold, and |D| ≥
max { 64L；j2 log (4|Hφ1 ), 32L：442 log(4)}, then with a probability at least 1 一 δ every solution Φ
ofIRMv1 satisfies Φ = (STγ)α, where α ∈ [ ——1 尸,ɪ 1 尸].
Proof. The empirical version of IRMv1 minimizes
0
R(Φ) + λR (Φ)
Let us compute the risk achieved by the ideal invariant predictor <STγ.
R(STY) = X ∏eEe[(Ye-YTSXe)[ = X ∏eEe[(Ye-γTZe)： = X ∏eEe[(εe)2] = σ2
e∈Etr	e∈Etr	e∈Etr
(81)
Define event A: {d : |R(STY) - R(STY)I ≤ N }∙
If event A holds, then
R(STY) ≤ σ2 + I	(82)
Define event B: {D : ∀ Φ ∈ Hφ, ∣R0(Φ) - R0(Φ)∣ ≤ ∣}.
34
Published as a conference paper at ICLR 2021
If event B holds, then ∣R' (STY)-宜(STY) | ≤ W and if We plug in 宜(STY) = 0 (from equa-
tion 73), then
R (STY) ≤ I
(83)
Define success as A ∩ B. If event A ∩ B occurs, then from equation 82 and equation 83 the following
is true for any solution Φ of IRMv1
R(Φ) + λR’ (Φ) ≤ R(STY) + λR' (STY) ≤ σ2 +1 + λI	(84)
Since R(Φ) ≥ 0 it follows that
R0(φ) ≤ ⅛i	+1 =⇒ R0(φ)- R (φ) + R (φ) ≤ σ2+2 +1	=⇒	R (φ) ≤ ⅛i	+1
λ	2	λ	2	λ
(85)
In the last implication in the above equation 85, we use the condition that event B occurs. Let
2
I = y and substitute in the above equation 85 to get
o 2σ2 σ2	5σ2 .	σ2 σ2
R &)≤ ɪ + 2λ2 ≤ ^2Λ (since λ ≥ 1, 2λ2 ≤ 2λ)
(86)
CllC	∙ .∙	<<	1	5c2 . 1	.1	IC	TC 5c2 /	^	、 、 5c2 J
Recall Proposition 11 and see	罢^	takes the role of	∣.	If % ≤	∣0	=⇒	λ ≥	II^,	then
1+τσ√lλ, 1-τσ
5b2|Etr|
5仃2包"
the condition in the Proposition 11 is true, then every solution of IRMv1 is STY(α), where
α ∈ [----------1/	,----------1/	] = [---1 E,---------]
Il 1 J 5b2|Etr| I _ 1 J 5b2|Et"	1 + τ σ ∖/ ɜʌ 1-τ σ ∖ ɜʌ
丁 ωλmin V 2λπmin	ωλmin V '
2λπmin
We arrived at the above result assuming A ∩ B occurs. We will now show that if ∣D∣ ≥
max( 16L4λ2 log(4|Hφ|), 8*2 log(4)}, then with a probability at least 1 - δ, A ∩ B occurs.
We write P(A) = P({d : ∣R(STY) - R(STY)I ≤ f }) = 1 - P({d : ∣R(STY) - R(STY)∣ >
W }). From HOefding’s inequality in Lemma 1 and 3 it follows that
p({d : IR(STY)- R(STY)I > I}) ≤ 2exp (-患)
(∣D∣e2∖	δ
2eXP (- -8L^) ≤ 2
=⇒ P(AC) ≤ 2
∣D∣≥ 当 log (4) =⇒ P(AC) ≤ δ
I2	δ	2
(87)
Next, we will show next that if ∣D∣ ≥ 16L4 log(4|Hφ1 ), then with probability at least 1 - δ event B
happens.
Now we need to bound the probability P(B). We will find an upper bound on the failure probability
using Hoeffding,s inequality and 3 as follows
P(B) = 1-p({D : ∃Φ ∈Hφ,∣R'(Φ)-R0(Φ)∣ > I}) = p( U {D"R'(Φ)-R0(Φ)∣ > I})
Φ∈Hφ
p( U {D"R'(Φ) - R0(Φ)∣ > I}) ≤ X p({D"R'(Φ) - R0(Φ)∣ > I}) ≤ 2∣Hφ∣e- 16D
Φ∈Hφ	Φ∈Hφ
e2|。	δ	δ
2∣Hφ∣e-印 ≤ 2 =⇒ P(Bc) ≤ 2
∣D∣ ≥ ɪ log (乎)=⇒ P(Bc) ≤
δ
2
(88)
35
Published as a conference paper at ICLR 2021
Hence, We know that if |D| ≥ 16L4 log(4|Hφ1 ), then with a probability at least 1 - δ event B
happens.
Combining equation 87 and equation 88 we get if
∕16L∖	M∣Hφ∣λ 8L21 M∏
DI ≥ max!TIog (-ry Flog ⑴)
52
then A ∩ B occurs with at least 1 - δ probability. Substitute E = 52λ and this completes the proof.
□
7.4 Extensions: Polynomial models, infinite hypothesis classes,
binary-classification
7.4.1	Polynomial model
In the main body of the work, we did not cover the polynomial model in detail due to space limita-
tions. In this section, we study the polynomial model and show how the results for the linear model
can be generalized to this case. We first begin by stating the model itself.
Assumption 8.
e 〜Categorical(πe), πe > 0∀e ∈ Etr
Ye = YTZp(Ze) + εe,εe ⊥ Ze, E[εe] = 0, E[(εe)2] = σ2, ∣εe∣ ≤ εsup	(89)
Xe = S(Z1e,Z2e)
Assume that Z1e component of S is invertible, i.e. ∃S such that S(S(Z1e, Z2e)) = Z1e and also
StY = 0. In the above Za is a polynomial feature map of degree P defined as Za : Ra → Ra , where
a denotes the dimension Ofthe input to the map Zp, Za(W) = [W, W 0 W,..., (W 0 W...P times0
W)] = [(W0i)P=ι] and 0 is the Kroneckerproduct. Also, a = PP=ι ai. ∀e ∈ Etr,∏e ≥ -E-∣.
The support of distribution ofZe = [Z1e, Z2e], PeZe, is bounded and the operator norm ofS, kS k =
σmax(S) (σmax(S) is maximum singular value ofS), is also bounded.
Define Ze = (Z1e, Z2e) and say Z1e ∈ Rc and Z2e ∈ Rd.
Can we directly use the analysis from the linear case? We cannot directly use the polynomial map for
the features as we also need to find an appropriate transformation of the matrix S, which preserves
the linear relationship between the transformed features and the transformed variables Z. We carry
out this exercise below.
Define S = diag [(S0i)P=∕∣, where S is a block diagonal matrix with diagonal matrices defining it
given as {s,S182...,S 18p0.
Define Xe = ZP (Xe), where Zn(Xe) is the polynomial feature map of degree P of n dimensional
input Xe. Similarly, define Ze = Zp(Z；), where Zp(Ze) is the polynomial feature map of degree P
of C dimensional input Ze and define Ze = Zp+d(Ze), where Zp+d(Ze) is the polynomial feature
map of degree P of C + d dimensional input Ze = (Ze, Ze). Observe that each component of Ze is
also in Z.
From the model we know that Xe = SZe. We would like to remind the reader of the mixed product
property of tensors. Consider matrices A ∈ Ri×j, B ∈ Rk×l C ∈ Rj×p, D ∈ Rl×q.
(A0B)(C0D) = (AC) 0 (BD)	(90)
In the expressions that follow, we exploit the mixed-product property of Kronecker product stated
above.
SZe = diag[(S^i)p=ι] [(Ze,0i)f=ι] = [(SZe)0i)f=ι] = [(Xe@)P=i]
ZP(Xe)= Xe (91)
36
Published as a conference paper at ICLR 2021
Define S = diag[(S18i)P=1].
SXe = SSZe = diag[(S18i)p=1idiagh(S 18i)p=1i[(Z e,18i )p=1]
diag[((SS 泞)p=i ih(z e@)p=i i = h((SSze 产)p=J = ["e 产)p=J = Zp(Ze) = Ze
(92)
The dimensionality of Xe is n = Pp=1 ni = nP+--n.
Assumption 9. Inductve bias. HΦ is a finite set of linear models (bounded) parametrized by Φ ∈
Rn0. STY ∈Hφ . ∃ ω > 0, Ω > 0, ∀Φ ∈Hφ,ω ≤ ∣∣Φk2 ≤ Ω and 2ω ≤ ∣∣Sτγk2 ≤ 3+2√2 Ω,
We next compute the norm of S in terms of the norm of S. Recall We are using operator
norm defined as ∣∣S∣ = σmaχ(S). ∣S∣ = Idiagh(S室i)P=J ∣. Since S is a diagonal ma-
trix ∣S∣ = maxi=ι,..,p{∣S0i∣}. Also, note that∣S0i∣ = ISki (Laub, 2005). Therefore,
∣S∣ = maxi=ι.…,p{∣S∣∣i}. Hence, if ∣∣S∣ isbounded, ∣S∣ is also bounded.
Also, ∣7ek2 = PikZe,0ik2. Observe that ∣∣Ze,0ik = ∣∣Ze∣i. Hence, if ∣Ze∣ is bounded, 17e∣ is
also bounded. Since Xe = SZe We can conclude that ∣∣Xe∣∣ is also bounded. We can now follow
the same line of reasoning as in equation 74, equation 75,equation 76, equation 77 to conclude that
the loss and the gradient of the loss are bounded.
We rewrite the above model in Assumption 8 as a linear model in terms of the transformed features.
e 〜Categorical(πe), πe > 0∀e ∈ E
Ye = γtZi + εe, εe ⊥ Zιe, E[εe] = 0, E[(εe)2] = σ2, ∣εe∣ ≤ εsup	(93)
Xe = SZe
We showed above in equation 92 that Ze defined in equation 91 component of Ze is invertible,
SSZe = Ze. We have also shown above that support of Ze is bounded and the norm of S is
bounded and as a result ∣∣Xe∣∣ and the loss and the gradient of the loss (conditions in Assumption 3
are satisfied) are bounded. We adapt the linear general position assumption (Assumption 6) for the
polynomial case below.
Assumption 10. Linear general position of training environments. A set of training environments
Etr is said to lie in a linear general position of degree r for some r ∈ N if |Etr | > n0 - r + n0/r and
for non-zero x ∈ Rn and
dim
span
{Ee[XeXe,T]x - Ee[Xe,Tεe]}
> n0 - r
(94)
We denote E[X eX e，T] = Σ e
Assumption 11. For all the environments e ∈ Etr, ∑e is positive definite.
Define the minimum eigenvalue over all the matrices Σe as λmin = mine∈Etr λ(Σe). Define &h =
24-16√2 ɪm! (ωλ . )2
3 |Etr| (ωλmin) .
From the analysis in this section, we see that we have been able to construct a linear model identical
to 5, where the role of Xe, Ze, S, S, Σe, λmin, Eth is taken by Xe, Z。, S, S, Σe,入m^,瓦h. Now we
are ready to use the result already proven for linear model and state the next Proposition in terms of
the parameters for the polynomial model.
Proposition 13. Let ' be the square loss. For every E ∈ (0, eth), δ ∈ (0,1), if Assumptions 8, 9,
10 (with r = 1), 11 hold and if the number of data points |D| is greater than 16L log (2|H φ1 ),
then with a probability at least 1 一 δ, every solution Φ to EIRM (equation 6) satisfies Φ = (STγ)α,
where α ∈ [I+⅛, I-⅛].
37
Published as a conference paper at ICLR 2021
7.4.2 Infinite Hypothesis Classes
In the work so far we have assumed that the hypothesis class HΦ is finite. In this section, we discuss
infinite hypothesis class extensions. Before we do that we state an important result on covering
numbers that we will use soon.
Lemma 5. (Shalev-Shwartz & Ben-David, 2014) Define a set A = {a ∈ Rk, kak2 ≤ Asup}.
Covering number for η-cover of A given as Nη (A) is bounded as
Nn(A) ≤ (卓)k
(95)
A. Infinite Hypothesis Class: Confounders and Anti-causal variables
In this section, we seek to extend Proposition 5 to infinite hypothesis classes.
We restate the Assumption 7 for linear models.
Assumption 12. Inductve bias. HΦ is a set of linear models (bounded) parametrized by Φ ∈ Rn
Hφ = {Φ ∈ Rn, 0 < ω ≤ kΦk2 ≤ Ω}. STY ∈ Hφ. 2ω ≤ ∣∣STγk2 ≤ 3+f√2Ω,
Note that the only difference between Assumption 7 and Assumption 12 is that the hypothesis class
is not required to be finite anymore.
We already established in equation 75, equation 76, equation 77 that from Assumptions 5 and 7, we
can show that the conditions in Assumption 3 hold, i.e., loss and the gradient of the loss are bounded,
and also Xe , Y e are bounded . The same conclusion follows from Assumptions 5 and Assumption
12. Hence, for the rest of this section, we can state that kXe k ≤ Xsup, |Y e | ≤ K, the square loss
'(Φ(∙), ∙) is bounded by L and 加叱Φ⑺# I	is bounded by L0. In the next lemma, We aim to
∂w	w=1.0
shoW that if Assumption 5 and 12 hold, then R (Φ) is Lipschitz continuous.
Lemma 6. If Assumption 5 and 12 hold, then R0 (Φ) is Lipschitz continuous in Φ.
Proof. The output of the model | ΦtX | ≤ ∣∣Φ∣∣∣∣X k ≤ √ΩXsup (FromCauchy-Schwarz).
R (Φ) = XπeEe∣ΦτXe(ΦTXe - Ye)]2
e
|R0(Φι) - R (Φ2)∣ = IXπe(Ee[φTxe(φTχe - Ye)i2 - Ee[φ[Xe(ΦjXe - Ye)]2)∣
e
≤ X∏e∣Ee[φTxe(Φ[xe - Ye)i2 - Ee[φTXe(ΦjXe - Ye)i2∣
e
(96)
We bound each term in the summation in the equation 96 above
38
Published as a conference paper at ICLR 2021
∣Ee[φ[Xe(Φ[Xe - Ye)『-Ee[φJXe(ΦjXe - Ye)]2∣
=∣(Ee[φ[Xe(φ[Xe - Ye)] - Ee[φJXe(ΦjXe - Ye)])(Ee∣ΦTxe(Φ[Xe - Ye)] +
Ee[φJXe(φJXe - Ye)])∣
<	∣ Ee [φTχe(φTχe - Ye)] - Ee [φTχe(φjxe - Ye)] ∣ 2√ΩXsup(√ΩXsup + K)
<	∣ Ee [φ[Xe(φ[Xe - Ye)] - Ee [φJXe(Φ[Xe - Ye)] + Ee [φJXe(φ[Xe - Ye)]-
Ee [φjXe(φjXe - Ye)] ∣ 2√ΩXsup(√ΩXsup + K)
=	∣ Ee [(Φ[Xe - ΦJXe)(φ[Xe - Ye)] + Ee [φjXe(φ[Xe - ΦJXe)] ∣ 2√ΩXsup(√ΩXsup + K)
<
Ee [(Φ[Xe - ΦJXe)(φ[Xe - Ye)]∣ 2√ΩXsup(√ΩXsι
Ee [φjXe(φ[Xe - ΦJXe)] ∣ 2√ΩXsup(√ΩXsup + K)
MP + K)十
We bound each term in the last line in equation 97
Ee [闽Xe - ΦJXe)(φ[Xe - Ye)] ∣
<	Ee [∣ (φ[xe - φJxe) I ∣ (φ[xe - Ye) ∣ ]
<	(√ΩXsup + K)Ee [ ∣ (Φ1 - Φ2)τXe) ∣ ]
<	(√ΩXsup + K)Ee [∣∣Φ1 - Φ2 ∣∣Xsup] (Cauchy-Scwarz)
<	(√ΩXsup + K)XSUPkΦ1 - Φ2∣
(97)
(98)
∣ Ee [φTXe(Φ^Xe - ΦTXe)] ∣
<	e[∣φTx e ∣ ∣(Φ^x e - φTx e) ∣ ]
<	√ΩXsupEe[ ∣ (Φ1 - Φ2)txe) ∣ ] (Cauchy-Scwarz)	(99)
<	√ΩxsupEe[∣Φ1 - Φ2∣Xsup]
<	√Ω(Xsup)2∣Φι - Φ2k
Substituting equation 98 and equation 99 in equation 97 to get
∣R0(Φι) - R (Φ2)∣ < 2√Ω(Xsup)2(√ΩXsup + K)(2√ΩXsup + K)∣∣Φ1 - Φ2k	(100)
Therefore, R0 is Lipschitz with a constant C0 < 2√Ω(Xsup)2(√ΩXsup + K)(2√ΩXsup + K).
□
We just showed that R0 is Lipschitz continuous and We set its Lipschitz constant as C0.
Proposition 14. Let' be the square loss. For every E ∈ (0, Eth) and δ ∈ (0,1), if Assumptions 5, 6
(with r
1), 12 hold and if the number of samples ∣D∣ is greater than 32L—
Inlog (
+
log (δ)], then with a probability at least 1 一 δ every solution Φ to EIRM (equation 6) satisfies
φ = STγ(α), where α ∈ [ 1+τ√1, I-TTi].
39
Published as a conference paper at ICLR 2021
Proof. Following the proof of Proposition 5, our goal is to compute the probability of event A:
{∀Φ ∈ Hφ,∣R0(Φ) - R0(Φ)∣ ≤ §}. We construct a minimum cover of size Nn(Hφ) (SeeLemma
5) with points C = {Φj}jb=1.
Compute the probability of failure at one point Φj in the cover
PhnD : ∣R0(Φj) - R (Φj )| > : Oi < 2e-32D	(101)
We use union bound to bound the probability of failure over the cover C as follows
PhnD : maX ∣R0(Φj) - R0(Φj)| > 4Oi < 2Nn(Hφ)e-12图	(102)
Now consider any Φ ∈ HΦ and suppose Φj is nearest point to it in the cover.
∣R0(Φ) - R0(Φ)∣ = ∣R0(Φ) - R (Φj) + R (Φj) - R(Φj) + R(Φj) - R0(Φ)∣
≤ ∣R0(Φ) - R(Φj)1 + ∣R0(Φj) - R(Φj)1 + ∣R0(Φj) - R0(Φ)I ≤ ∣R0(Φj) - R(Φj)1 + 2ηC
(103)
In the above simplification, we exploited the Lipschitz continuity ofR0. Therefore, for each Φ ∈ HΦ
∣R0 (Φ)- R (Φ)l ≤ max |R0(Φj) - R(Φj)| + 2ηC0
φmaχ |r0(φ) - R(φ)1 ≤ m∈χ |r0(φj) - R0(φj)| + 2ηC0
(104)
e2|。
Set η =旨 in equation 104 and from equation 102 with probability at least 1 - Nn (Hφ)2e-32L04
^ax ∣R0(Φ) — R0(Φ)∣ ≤ e/2 (since max ∣R0(Φj) — R (Φj)| ≤ e/4 )	(105)
…	……-ɪ2≡  ................. .
We bound Nn(Hφ)2e 32L04 ≤ δ and solve for bound on |D| to get
∣D∣≥ 8L4 log (2Nn(HΦ))(USeLemma 5)
τ	(106)
∣D∣≥ 亨 hn log (…)+log (2 )i
Therefore, if condition in equation 106 holds, then event A occurs and following the same argument
as in the proof of Proposition 5 the proof is complete.
□
B.	Infinite hypothesis class: Lipschitz continuous functions
In this section, we seek to extend Proposition 2 and 4 to infinite hypothesis class of Lipschitz con-
tinuous functions that we formally define next. Define a map Φ : P × X → R from the parameter
space P and the feature space X to reals. Each p ∈ P is a possible choice for the representation
Φ(p, ∙). Consider neural networks as an example, P represents the set of the values the weights of
the network can take.
Assumption 13. Φ : P × X → R is a a Lipschitz continuous function (with Lipschitz constant say
Q).
P ⊂ Rk is closed and bounded, thus there exists a P < ∞ such that ∀p ∈ P, kpk2 ≤ P.
X ⊂ Rn is closed and bounded, thus there exists a Xsup < ∞ such that ∀x ∈ X, kxk ≤ Xsup.
Y ⊂ R is closed and bounded, thus there exists a K < ∞ such that ∀y ∈ Y, |y| ≤ K
40
Published as a conference paper at ICLR 2021
Assumption 14. Lipschitz loss and gradient of loss. R(Φ) is Lipschitz with a constant C, R0 (Φ) is
Lipchitz with a constant C0.
From Assumption 13 derive the conditions in Assumption 3 and Assumption 14 Φ : P ×X → R
is a continuous function defined over closed and bounded domain P × X (domain is compact) and
as a result Φ is bounded say by M .
Consider square loss '(Φ(p,X),Y) = (Y - Φ(p, X))2 ≤ (M + K)2. Hence, there exists an L such
that ∣'(Φ(∙),∙)∣ ≤ L ≤ (M + K)2.
d'(ww∙ML=L0isbounded:
Iy),1 I = l(Y - Φ(p,x ))Φ(p,x )1 ≤ (K + M)M.
w	w=1.0
Hence, there exists an L0 SUCh that |d'(Wdw(),')∣	ɪj ≤ L0 ≤ (K + M )M
Lemma 7. IfAssumption 13 holds, then R(Φ(p, ∙)) and R0(Φ(p, ∙)) are Lipschitz continuous in P.
R is Lipschitz:
∣R(Φ(p, ∙) - R(Φ(q, ∙))∣ = ∣X ∏e (Ee[(Φ(p, Xe) - Y e)2] - Ee[(Φ(q, Xe) - Y e)2]) ∣
e
≤ X∏e∣Ee [(Φ(p, Xe) - Ye)2i - Ee [(Φ(q, Xe) - Ye)2i ∣
e
=Xπe∣∣∣EehΦ(p,Xe) - Φ(q, Xe)iEehΦ(p, Xe) + Φ(q, Xe) - 2Y i∣∣∣
e
≤XπeEeh∣∣∣Φ(p,Xe) - Φ(q, Xe)∣∣∣iEeh∣∣∣Φ(p, Xe) + Φ(q, Xe) - 2Y ∣∣∣i
e
≤XπeEeh∣∣∣Φ(p,Xe) -Φ(q,Xe)∣∣∣i2(M+K)
e
≤ kp-qk2(M+K)Q
(107)
Therefore, R is Lipschitz with a constant C ≤ 2(M + K)Q
R0 is Lipschitz:
∣R0 (Φ(p,∙))- R (Φ(q, ∙))l = ∣X ∏e(Ee [φ(p, X e)(Φ(p, Xe) - Y e)i 2 - E [φ(q, X e)(Φ(q, X e) - Y e)i 2) ∣
e
≤ X πe∣∣∣EeΦ(p, Xe)(Φ(p, Xe) - Y e)]2 - E[Φ(q, Xe)(Φ(q, Xe) - Y e)2∣∣∣
e
(108)
We bound each term in the summation in the equation 96 above
41
Published as a conference paper at ICLR 2021
|(Ee [φ(p, X e)(Φ(p, X e) - Y e)i - Ee[Φ(q, X e)(Φ(q, X e) - Ye)i)(Ee [φ(p, X e)(Φ(p, X e) - Ye)] +
Ee[φ(q,X e)(Φ(q,X e) - Y e)i)∣
≤	(Ee[φ(p,Xe)(Φ(p,Xe)	- Ye)i - Ee∣Φ(q,Xe)(Φ(q,Xe)	-	Ye)])∣2M(M + K)
≤	(Ee	[φ(p, Xe) (Φ(p, Xe)	- Ye)i - Ee [φ(q, Xe) (Φ(p, Xe)	-	Ye)i + Ee [φ(q, Xe) (Φ(p, Xe)	- Ye)i -
Ee ∣Φ(q, Xe)(Φ(q, Xe) - Ye)i) ∣2M(M + K)
=(Ee [(Φ(p, Xe) - Φ(q, Xe) (Φ(p, Xe) - Ye)i) + Ee [φ(q, Xe)(Φ(p, Xe) - Φ(q, Xe))] ∣2M(M + K)
≤ (Ee[(Φ(p,Xe) - Φ(q,Xe))(Φ(p,Xe) - Ye)]) ∣2M(M + K)+
∣Ee [φ(q, Xe)(Φ(p, Xe) - Φ(q, Xe))]∣2M(M + K)
(109)
We bound each term in the last line in equation 109
∣ (Ee[(Φ(p,Xe) - Φ(q,Xe))(Φ(p,Xe) - Ye)]) ∣
≤Ee∣∣(Φ(p,Xe)-Φ(q,Xe))∣∣∣∣(Φ(p,Xe)-Ye)∣∣	(110)
≤(M+K)Qkp-qk
∣∣E[Φ(q, Xe)(Φ(p, Xe) - Φ(q, Xe)]∣∣
≤ E[∣Φ(q,Xe)ll(Φ(p,Xe) - Φ(q,Xe)∣]	(111)
≤MQkp-qk
Substituting equation 110 and equation 111 in equation 109 to get
|R0(Φ(p, ∙)) - R0(Φ(q, ∙))l ≤ 2M(M + K)(2M + K)Qkp - q∣∣	(112)
Therefore, R0 is Lipschitz with a constant C0 ≤ 2M(M + K)(2M + K)Q.
C.	EIRM: Sample complexity with no distributional assumptions
In this section, we discuss the extension of Proposition 2 to the infinite hypothesis class case. Con-
sider the problem in equation 6 and replace the with + κ. Define distance between the two sets
SIV() and its approximation SIV( + κ) as follows
dis(κ)
max min d(g, h)
g∈SIV(+κ) h∈SIV()
(113)
where d(g, h) is some metric that measures the distance between functions g and h. Observe that if
a ≤ b, dis(a) ≤ dis(b).
Assumption 15. limk→0 dis(κ) = 0
Define
D* = max n 3VL2 hk log (16CrPk) +log (2)], 8L04 hk log (8COKpk) +log (2 )iθ
Proposition 15. For every ν > 0 and δ ∈ (0, 1), if Assumption 13, 15 hold, then ∃ κ > 0 such
that if the number of samples |D| is greater than D*, then with a probability at least 1 — δ, ^very
solution Φ to EIRM (replace E with E + K inequation 6) in SIV(e + 2κ) and ∣R(Φ) — R(Φ*)∣ ≤ V,
where Φ* is a solution of IRM in equation 5.
42
Published as a conference paper at ICLR 2021
Proof. We divide the proof in two parts.
Define an event A: {D : ∀p ∈ P, ∣R0(Φ(p, ∙)) - R0(Φ(p, ∙))∣ ≤ κ}.
In the first half, We will show that if event A occurs, then SIV(E) ⊆ SIV(E + K) ⊆ SIV(E + 2κ) and
then bound the probability of A not occuring.
R0(Φ(p,∙)) ≤ E =⇒ R (Φ(p, ∙)) - R0(Φ(p, ∙))+ R0(Φ(p, ∙)) ≤ E =⇒
R0(Φ(p, ∙)) ≤ E + ∣R0(Φ(p, ∙)) - R0(Φ(p, ∙))∣ =⇒ R0(Φ(p, ∙)) ≤ E + K
Therefore, SIV(E) ⊆ SIV(e + K)
R0(Φ(p, ∙)) ≤ E + K =⇒ R0(Φ(p, ∙)) - R0(Φ(p, •))+ R0(Φ(p, ∙)) ≤ E + K =⇒
R0(Φ(p, ∙)) ≤ E + K + ∣R0(Φ(p, •))- R0(Φ(p, ∙))∣ =⇒ R0(Φ(p, ∙)) ≤ E + 2k
(114)
(115)
Therefore, SIV(E + K) ⊆ Siv(e + 2k)
We bound the probability of event A not occurring. Using the covering number (from Lemma 5) we
construct a minimum cover of size b = Nη (P) with points C1 = {pj }jb=1.
Compute the probability of failure at one point pj in the cover
p[{D : ∣R0(Φ(pj, ∙)) - R0(Φ(pj, ∙))∣ >2Oi < 2e-κ2D	(116)
We use union bound to bound the probability of the failure over the entire cover C1 as
P[{D : ^^ |R0(Φ(pj, ∙)) - R0(Φ(pj, ∙))∣ >2Oi < Nn(P)2e-KLD
(117)
Now consider any p ∈ P and suppose pj is nearest point to it in the cover.
|R0(Φ(p, ∙)) - R0(Φ(P, ∙))l =
|R0(Φ(p, ∙)) - R0(Φ(Pj, ∙)) + R0(Φ(Pj, ∙)) - R0(Φ(Pj, ∙)) + R0(Φ(Pj, ∙)) - R0(Φ(p, ∙))l (118)
≤ |R0(Φ(pj, ∙)) - R0(Φ(Pj, ∙))l + 2ηC0
In the above simplficiation, we exploit the Lipschitz continuity of R0 .
Therefore
∀p e P |R0(Φ(p, ∙)) - R0(Φ(p, ∙))l ≤ max |R0(Φ(Pj, ∙)) - R0(Φ(Pj, ∙))l + 2ηC0
0	0	pj∈C1	0	0	0 (119)
max |R (Φ(p, ∙)) - R (Φ(p, ∙))∣ ≤ max |R (Φ(pj, ∙)) - R (Φ(pj, ∙))∣ + 2〃C
p∈P	pj ∈C1
K2 |D|
Set η = 言 in equation 119 and from equation 117 with probability at least 1 - Nn (P)2e- 8L04
max |R (Φ(p, ∙)) — R (Φ(p, ∙))∣ ≤ K (since max |R (Φ(pj, ∙)) — R (Φ(pj, ∙))∣ ≤ k/2 ) (120)
p∈P	pj ∈C1
κ2∣D∣
We bound Nn(P)2e- 8L04 ≤ δ and solve for bound on |D| to get
当 log( F)
K2	δ
iγλi	8L04∣∙, ,	∕8C0√Pk∖	,	∕2∖1
lDl≥ 下[kl°g()+l°g (δ)]
(121)
43
Published as a conference paper at ICLR 2021
Therefore, if condition in equation 121 holds, then event A occurs. If event A occurs, then SIV () ⊆
SIV(E + K) ⊆ SIV(E + 2κ).
Φ(p*, ∙) is a solution to IRM equation 5 and it satisfies ∀Φ(p, ∙) ∈ SIV(E) R(Φ(p*)) ≤ R(Φ(p, ∙).
Define an event B: {D : ∀p ∈ P, ∣R(Φ(p, ∙)) - R(Φ(p, ∙))∣ ≤ 2}.
If event B occurs, then for a solution Φ(p, ∙) of equation 6 (where E is replaced with E + K) satisfies
R(φ(p, •)) - 2 ≤ R(Φ(P, ∙) ≤ R(Φ(p*, ∙)) ≤ R(Φ(p*, ∙)) + ν
R(Φ(P,∙)) ≤ R(Φ(p*, ∙)) + V
(122)
Using the covering number (from Lemma 5) we construct a minimum cover of size b = Nη(P) with
points C1 = {pj}jb=1.
Let us bound the probability of failure at one point pj in the cover
p[∣R(Φ(pj, ∙)) - R(Φ(pj, ∙))∣ >4i < 2e-ν32D	(123)
We use union bound to bound the probability defined as
ν	ν2 |D|
p[{d : max ∣R(Φ(pj, ∙)) - R(Φ(pj, ∙))∣ > 4}] < Nn(P)2e-B	(124)
Now consider any p ∈ P and suppose pj is nearest point to it in the cover.
... ʌ , ...
∣R(Φ(p, •))- R(Φ(p, ∙))l =
. . . . . . . ʌ , . . ʌ , . . ʌ , ...	. . _
∣R(Φ(p, ∙))	- R(Φ(Pj, ∙))	+ R(Φ(Pj, ∙))	- R(Φ(Pj, ∙)) + R(Φ(Pj, ∙))	- R(Φ(p, ∙))l	(125)
ʌ , ..
≤ ∣R(Φj) - R(Φj)1 + 2ηC
Therefore, for each p ∈ P
. . ʌ , ... . . ʌ , ...
∣R(Φ(p, ∙)) - R(Φ(Pj, ∙))l ≤ max ∣R(Φ(pj, ∙)) - R(Φ(pj, ∙))∣ + 2ηC
pj ∈C1
., , 一 ʌ , , 一, . , , 一 ʌ , ,	一, 一
max ∣R(Φ(p, ∙)) - R(Φ(p, ∙))∣ ≤ max ∣R(Φ(pj, ∙)) - R(Φ(pj, ∙))∣ + 2〃C
p∈P	pj ∈C1
(126)
ν2∖D∖
Set η = 8C in equation 124 and from equation 126 with probability at least 1 - Nn(P)2e-32L04
max ∣R(Φ(p, ∙)) — R'(Φ(p, ∙))∣ ≤ v/2 (since max ∣R'(Φ(pj, ∙)) — R(Φ(pj, ∙))∣ ≤ v/4 ) (127)
p∈P	pj ∈C1
ν2∖D∖
We bound Nn(P)2e- 32L2 ≤ δ and solve for bound on |D| to get
Dl≥吗log (竺詈)
V v δ j	(128)
DI ≥ 亭 hk log (16C√Pk )+iog( 2)i
Therefore, if condition in equation 128 holds, then with probability at least 1 - 2 event A occurs.
Also, if
DI ≥ max n 3νL2 hk log( *√pk)+log( 2)i, 8L04 hk log( 8C0√pk)+log( 2)io(129)
44
Published as a conference paper at ICLR 2021
then the event A ∩ B occurs with at least 1 - δ probability.
Project Φ(p, ∙) ∈ SIV(C + K) on SIV(e), i.e., find the closest function in terms of the metric dis, to
obtain Φ(p, ∙). If event A occurs, then P ∈ SIV(E + 2κ). The distance ∣∣p^ - Pk ≤ dis(2κ).
,~ ... ~.. 一 .. . .
∣R(Φ(p, ∙)) - R(Φ(P, ∙))∣ ≤ C∣p -p∣∣ ≤ Cdis(2κ)	(130)
We choose κ0 such that κ < κ0 (use Assumption 15) such that Cdis(2κ) ≤ ν.
,~ .. .....
R(Φ(p*, ∙) ≤ R(Φ(p, ∙)) ≤ R(Φ(p, ∙)) + V	(131)
Therefore, by combining equation 122 amd equation 131, we can conclude that if event A ∩ B
occurs, then Φ(p, ∙) ∈ SIV(C + 2κ) and ∣R(Φ(p*, ∙)) - R(Φ(p, ∙))∣ ≤ V. From the conditions on
|D|, We know that A ∩ B occurs with probability 1 - δ. We substitute Φ(p*, ∙) as Φ* and Φ(p, ∙) as
Φ and this completes the proof.	口
D.	OOD Performance: Covariate shift case
In this section, we discuss the extension of Proposition 4 to the infinite hypothesis class case.
Define Di = {笔[klog (32c√Pk) + log (4)],喳4 log(4)}.
Define Di = 3VL2 [k log (16C√Pk) + log (2)].
Proposition 16. If Assumptions 4, 13 hold, m ∈ HΦ and if the number of samples |D| is greater
than D1i, then with a probability at least 1 - δ, every solution Φ to EIRM satisfies R(m) ≤ R(Φ) ≤
R(m) + V.
If Assumptions 4, 13 hold, m ∈ HΦ and if the number of samples |D| is greater than D2i, then with
a probability at least 1 — δ every solution Φ* of ERM satisfies R(m) ≤ R(Φ*) ≤ R(m) + V.
Proof. We begin with the first part. Following the proof of Proposition 5, our goal is to compute
the probability of event A: {∀p ∈ P, ∣R(Φ(p, ∙)) - R(Φ(p, ∙))∣ ≤ 2}. Using the covering number
(from Lemma 5) we construct a minimum cover of size b = Nη(P) with points C = {pj}jb=1.
Compute the probability of failure at one point pj in the cover
「r	ʌ	V Λ~∖	v2∣d∣
P[{D : ∣R(Φ(pj, ∙)) - R(Φ(pj, ∙))∣ > 4}] < 2e-B	(132)
We use union bound to bound the probability of failure over the cover C
V	ν2 |D|
PHD : max ∣R(Φ(pj, ∙)) - R(Φ(pj,∙))∣ > 4}J < Nn(P)2e-B	(133)
Now consider any p ∈ P and suppose pj is nearest point to it in the cover.
... ʌ , ...
∣R(Φ(p, •))- R(Φ(p, ∙))l =
. . . . . . . ʌ , . . ʌ , . . ʌ , ...	.....
∣R(Φ(p, ∙)) - R(Φ(Pj, ∙)) + R(Φ(Pj, ∙)) - R(Φ(Pj, ∙)) + R(Φ(Pj, ∙)) - R(Φ(p, ∙))l	(134)
ʌ , ...
≤ ∣R(Φ(Pj, ∙) - R(Φ(Pj,∙))l + 2ηC
In the above simplification, we used the Lipschitz continuity of R. Therefore
. . ʌ , ... . . ʌ , ...
∀P ∈ P, ∣R(Φ(p, ∙)) - R(Φ(p, ∙))∣ ≤ maχ ∣R(Φ(Pj, F- R(Φ(Pj, ∙))∣ + 2ηC
pj∈C
., , 一 ʌ , ,	一, . , ,	一 ʌ , , 一, 一
max ∣R(Φ(p, ∙)) - R(Φ(p,	∙))∣	≤ maχ ∣R(Φ(Pj,	∙))	- R(Φ(Pj, ∙))∣	+ 2ηC
p∈P	pj ∈C
(135)
45
Published as a conference paper at ICLR 2021
C .	V ∙	J YCL 1 C	.∙ YCC ...	1 …一 .1	' r	"T Es --ν2n
Set η = 8C in equation 135 and from equation 133 With probability at least 1 - Nn(P)2e 32L04
max |R (Φ(p, ∙)) — R (Φ(p, ∙))∣ ≤ v/2 (since max |R (Φ(pj, ∙)) — R (Φ(pj, ∙))∣ ≤ ν/4 ) (136)
p∈P	pj∈C
ν2∣D∣
We bound Nn(P)2e- 32L2 ≤ δ and solve for bound on |D| to get
∣D∣≥ 8L2 log (Nr)
4
(Use Lemma 5)
ICl 32L2 γ7 1 ∕16C√Pk∖ 2 ∕2∖^∣
lDl≥ L [k log(	)+log (δ)]
(137)
Therefore, if condition in equation 137 holds, then event A occurs. Observe that the optimal solution
p* for expected risk minimization p* ∈ argminp∈p R(Φ(p, ∙)) satisfies Φ(p*,∙) = m(∙) (From
Lemma 4 and m ∈ HΦ). If event A occurs, then from same argument used in equation 48, a
solutionp+ ∈ P of ERM satisfies R(Φ(p*, ∙)) ≤ R(Φ(p+, ∙)) ≤ R(Φ(p*, ∙)) + V is true.
We noW move to the second part. From the first part of the proof, We conclude that When
32L2 γ7 1	∕16C√Pk∖	1 ，4j
|D| ≥ -V^ [klog (——V——)+log(δ)]	(138)
with a probability at least 1 - 2 event A occurs.
Define an event B: D is such that |R0(m) — R0(m)∣ ≤ 2. Since R0(m) = 0, ∣R0(m) — R0(m)∣ ≤
2 =⇒ ∣R0(m)∣ ≤ N =⇒ R0(m) ≤ f. Therefore, m ∈ SIV(e).
We write P(B) = P({D : |R0(m) — R0(m)∣ ≤ j}) = 1 — P({D : ∣R0(m) — R0(m)∣ > j}). The
gradient of loss function is bounded | d'(φW)# | w=1.01 ≤ L. From Hoeffding,s inequality in Lemma
1 it follows that
P(D : R0(h) — R (h)| > I) ≤ 2exp ( — IDL4)
2 exp —
∣D∣∣2)
16L04J
δ
≤ —
2
∣D∣≥ 亭 log (δ)	(139)
Combining the two conditions equation 138 and equation 139,
,l	3 32L2 γ ∕32Cy∕Pk∖	∕4∖^∣ 16L04	∕4∖1
|D| ≥ max {L [k log ()+ log (δ)], Tlog (δ)}
This ensures P(A ∩ B) ≥ 1 — δ. If event A ∩ B occurs, then we follow the same justification as in
the proof of Proposition 4 to claim that a solution P ∈ P to EIRM equation 6 satisfies R(Φ(p*, ∙)) ≤
R(Φ(p, ∙)) ≤ R(Φ(p*, •)) + v
This completes the proof.
□
7.4.3	Extensions to binary classification (cross-entropy)
In the main body of the manuscript, we focused on regression (square-loss). In this section, we
discuss the results that can be extended to binary classification (cross-entropy) loss. We will not go
in the order in which the results were introduced in the manuscript but in an order that makes for
easier exposition for the classification case.
46
Published as a conference paper at ICLR 2021
We begin by showing how to extend Proposition 4 to binary classification (cross-entropy). Recall
that the entropy of a distribution PX is H(P) = -EP log(dP) . Recall that the cross entropy of
Q relative to P is H(P, Q) = -EP log(dQ) = H(P) + KL(dPkdQ). The cross entropy loss `
for binary classification when using a predictor f : X → [0, 1] (f(Xe) is the probability of label
1 conditional on Xe) is given as 'f(Xe),Ye) = Yelog (f(Xe)) + (1 - Ye)log(1 - f(Xe)).
For the discussion below Q(Ye|Xe) is defined in terms of f as follows Q(Ye = 1|Xe) = f(Xe)
(Q(Ye = 0|Xe) = 1 - f(Xe)).
Re(f ) = Ee['(Y e,f (X e))]
=EehYe log (f (Xe)) + (1- Ye) log (1 - f (Xe))i
=EehEe[Ye∣Xe] log (f (Xe)) + (1 - E[Ye∣Xe]) log(1 - f (Xe))]
=EehP(Ye = 1|Xe)log (f (Xe)) + P(Ye =0|Xe)log (1 - f (Xe))]	(140)
=EehH (P(Y e |X e), Q(Y e∣X e))i
=Ee [H (P(Y e∣X e)) + KL(P(Y e∣X e)kQ(Y e∣X e)]
=EehH (P(Y e |X e ))i + Ee ∣KL(P(Y e∣X e)kQ(Y e∣X e)]
From the above it is clear that Q(Ye|Xe = P(Ye |Xe minimizes the risk in an individual environ-
ment.
Assumption 16. Invariance w.r.t all the features. For all e, o ∈ Eall and for all x ∈ X, E[Ye |Xe =
x] = E[Yo∣Xo = x]. Xe 〜PXe and∀e ∈ Eall support of PXe is equal to X.
Observe that in the binary-classification setting the above assumption amounts to equating the con-
ditional probabilities P(Ye |Xe) and P(Yo|Xo).
Recall that map m (from equation 2) simplifies to ∀x ∈ X
m(x) = Ee[Ye|Xe = x] = P(Ye = 1|Xe = x)	(141)
If Assumption 16 holds, then from cross-entropy decomposition in equation 140 it is clear that m
solves the OOD problem (as it is optimal w.r.t each environment). It is also the unique minimizer.
We can justify it based on the same argument presented in equation 56. Suppose there was another
optimizer which was different from m over a set with a non-zero measure. Over such a set the the
KL divergence term inside equation 140 will be greater than zero, thus making the second term in
equation 140 positive thus contradicting the optimality. This shows m is the unique optimizer. The
rest of the arguments presented in the proof of Proposition 4 carry over to this case. Therefore,
Proposition 4 extends to the cross-entropy loss.
Note that Proposition 2’s proof was agnostic to loss type and only used boundedness, which holds
for both cross-entropy as long as the probability output are in the strict interior of [0, 1] defined by
[pmin, pmax] ⊂ [0, 1]. We could not generalize Proposition 5 to cross-entropy loss and that is left as
future work.
Next, we move to showing how Proposition 1 can be generalized to binary classification.
Assumption 17. Existence of an invariant representation. ∃ Φ* : X → Z such that ∀e,o ∈ Eall
and∀x ∈ X, E[Ye∣Φ*(x)] = E[Yo∣Φ*(x)].
Recall m defined in equation 2, ∀z ∈ Φ*(X)
m(z) = Ee[Ye|Ze = z] = P(Ye = 1|Xe = z)	(142)
Define a composite predictor W ◦ Φ*. Substituting f = W ◦ Φ* in equation 140 We get the following.
For the discussion below, a distribution R(Ye∣Xe) is defined in terms of W ◦ Φ* as follows R(Ye =
1|Xe) = W ◦ Φ*(Xe) (R(Ye = 0|Xe) = 1 - W ◦ Φ*(Xe)).
Re(W ◦ Φ*) = EehH(P(Ye∣Ze))i + EehKL(P(Ye∣Ze)∣∣R(Ye∣ (Ze))]	(143)
47
Published as a conference paper at ICLR 2021
If all the data is transformed by Φ*, then from the above decomposition equation 143 it is clear that
R(Ye∣Ze) = P(Ye∣Ze) is the optimal predictor for each environment. Hence, w*(Ze) = P(Ye =
1|Ze) is the best choice for w.
Assumption 18. Existence of an environment where the invariant representation is sufficient. ∃
an environment e ∈ Eall such that Ye ⊥ Xe∣Ze, where Ze = Φ*(Xe).
We derive a relationship as follows for the environment q satisfying Assumption 18.
P(Yq|Zq, Xq) = P(Yq |Zq) (follows from conditional independence in Assumption 18) (144)
Also, note that since Zq = Φ*(Xq) We have
P(Yq |Zq, Xq) = P(Yq|Xq)	(145)
From equation 144 and equation 145 We have
P(Yq|Xq) = P(Yq|Zq)	(146)
We use equation 146 in the cross entropy decomposition from equation 140
Rq(f) = Eq [H(P(Yq|Zq))] + Eq [KL(P(Yq|Zq)kQ(Yq|Xq)]	(147)
Recall Q(Yq = 1|Xq) = f (Xq) (Q(Yq = 0|Xq) = 1 - f (Xq)). Also, recall w*(Zq) = P(Yq =
1 |Zq). From the above it is clear that f = w* ◦ Φ* is the optimal predictor for environment q.
Rq(w* ◦ Φ*) = Eq [H(P(Yq|Xq))]
(148)
The expected conditional entropy for environment e is defined as He
Ee [H (P(Y e∣Z e))
is the
risk achieved by w* ◦ Φ*. Also, He measures the amount of noise in the environment. This is much
like the variance that remains in the least squares minimization. In the next assumption, We state that
the noise in all the environments is bounded above. We also assume that one of the environments
Which achieves the maximum noise level is environment q, Which satisfies Assumption 18.
Assumption 19. ∀e ∈ Eall,He ≤ Hsup,Hq = HSup
Therefore,
Rq(w* ◦ Φ*) = HSUP
From equation 143 for all the environments
Re(w* ◦ Φ*) = He
(149)
(150)
Observe that maXe∈Ean Re(W* ◦ Φ*) = Hsup. From the above assumption it is clear that for all
predictors f : X → [0, 1].
∀f, max Re(f) ≥ Rq (f) ≥ HSUp
e∈Eall
min max Re(f) ≥ HSUp
f e∈Eall
(151)
Since maXe∈Eaπ Re(w* ◦ Φ*) = Hsup, we conclude that w* ◦ Φ* is the predictor that solves the
OOD problem in equation 1. This completes the extension of Proposition 1 to cross-entropy.
7.4.4	On the Biasedness of ERM
Consider the model in Assumption 5. For each environment e ∈ Etr , define a vector ρe =
Ee[εeXe]. Define a matrix P with Pe as column vectors P = [ρ1,..., ρlEtr|]. Define a vector
∏ = [∏1 ,...,∏|Etr | ], where recall from Assumption 5 ∏o is probability a point comes from environ-
ment o.
Proposition 17. If Assumption 5 holds, HΦ is a linear hypothesis class with parameter Φ and if the
rank of P is at least one, then ERM is asymptotically biased, i.e., even with infinite data ERM will
not achieve the desired solution St γ, except over a set of measure zero of probability distributions
π.
48
Published as a conference paper at ICLR 2021
Proof. Consider the case when ERM has access to infinite data, i.e., we are solving the expected risk
minimization problem stated as minΦ∈HΦ R(Φ). We will consider the linear model in Assumption
5 and assume Hφ linear hypothesis class parametrized by Φ ∈ Rn. We simplify the ▽①R(Φ) for
the square loss below
VφR(Φ) = X πeEe[(Ye - ΦTXe)Xe]
e∈Etr
We compute the gradient for Φ = STY as
Vφiφ=STYR(Φ)= X ∏eEe [(Ye - YTSXe)Xe] (Use Assumption 5)
e∈Etr
= X πeEe εeXe
e∈Etr
(152)
(153)
Recall Pe = Ee [εeXe] and P = [ρ1,... ,ρlEtr|]. Recall ∏ = [∏1,..., ∏lEtr|]. Setting the gradient
defined in equation 153 to zero and using the above matrix notation we get
ρ∏ = 0, 1Tn= 1,π ≥ 0	(154)
If ∏ satisfies equation 154, then ERM is unbiased, else it is not. Consider the set of vectors in the
probability simplex {∏ | ITn = 1, ∏ ≥ 0} and define a uniform probability distribution over it.
Since rank of P > 0 at least one of the columns of P is non-zero. As a result a uniform random
draw from this set of probablity distributions would have zero probability of satisfying Pn = 0.
Therefore, STY is not the optimal solution to ERM and thus the solution of ERM would be biased
away from STγ.
□
In Proposition 5, We had assumed Assumptions 5, 6, 7 hold. If We also assume that rank of P is at
least one, the Proposition 5 continues to hold. If for at least one e ∈ Etr , Ee εeXe is non-zero,
then the rank of P is at least one. From proof of Theorem 10 in Arjovsky et al. (2019), linear general
position continues to hold except over a set of covariance matrices with measure zero even when
one of the Ee εeX e is non-zero.
Also, in the above Proposition 17, We only required that rank of P is at least one. However, if
we make the additional assumptions 5, 6, 7, the result of the above Proposition continues to hold.
Therefore, if Assumption 5, 6, 7 hold and rank of P is at least one, then ERM is asymptotically
biased and IRM can be within √ neighborhood of the ideal solution with the sample complexity
shown in Proposition 5.
49