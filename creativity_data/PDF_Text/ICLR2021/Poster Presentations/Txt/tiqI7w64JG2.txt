Published as a conference paper at ICLR 2021
On Graph Neural Networks versus Graph-
Augmented MLPs
Lei Chen*, Zhengdao Chen*
Joan Bruna
Courant Institute of Mathematical Sciences Courant Institute of Mathematical Sciences
New York University, New York, NY
{lc3909, zc1216}@nyu.edu
Center for Data Science
New York University, New York, NY
bruna@cims.nyu.edu
Ab stract
From the perspectives of expressive power and learning, this work compares
multi-layer Graph Neural Networks (GNNs) with a simplified alternative that
we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first aug-
ments node features with certain multi-hop operators on the graph and then applies
learnable node-wise functions. From the perspective of graph isomorphism test-
ing, we show both theoretically and numerically that GA-MLPs with suitable op-
erators can distinguish almost all non-isomorphic graphs, just like the Weisfeiler-
Lehman (WL) test and GNNs. However, by viewing them as node-level functions
and examining the equivalence classes they induce on rooted graphs, we prove a
separation in expressive power between GA-MLPs and GNNs that grows expo-
nentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the
number of attributed walks. We also demonstrate via community detection exper-
iments that GA-MLPs can be limited by their choice of operator family, whereas
GNNs have higher flexibility in learning.
1	Introduction
While multi-layer Graph Neural Networks (GNNs) have gained popularity for their applications in
various fields, recently authors have started to investigate what their true advantages over baselines
are, and whether they can be simplified. On one hand, GNNs based on neighborhood-aggregation
allows the combination of information present at different nodes, and by increasing the depth of
such GNNs, we increase the size of the receptive field. On the other hand, it has been pointed out
that deep GNNs can suffer from issues including over-smoothing, exploding or vanishing gradients
in training as well as bottleneck effects (Kipf & Welling, 2016; Li et al., 2018; Luan et al., 2019;
Oono & Suzuki, 2020; Rossi et al., 2020; Alon & Yahav, 2020).
Recently, a series of models have attempted at relieving these issues of deep GNNs while retaining
their benefit of combining information across nodes, using the approach of firstly augmenting the
node features by propagating the original node features through powers of graph operators such as
the (normalized) adjacency matrix, and secondly applying a node-wise function to the augmented
node features, usually realized by a Multi-Layer Perceptron (MLP) (Wu et al., 2019; NT & Maehara,
2019; Chen et al., 2019a; Rossi et al., 2020). Because of the usage of graph operators for augment-
ing the node features, we will refer to such models as Graph-Augmented MLPs (GA-MLPs). These
models have achieved competitive performances on various tasks, and moreover enjoy better scala-
bility since the augmented node features can be computed during preprocessing (Rossi et al., 2020).
Thus, it becomes natural to ask what advantages GNNs have over GA-MLPs.
In this work, we ask whether GA-MLPs sacrifice expressive power compared to GNNs while gaining
these advantages. A popular measure of the expressive power of GNNs is their ability to distinguish
non-isomorphic graphs (Hamilton et al., 2017; Xu et al., 2019; Morris et al., 2019). In our work,
besides studying the expressive power of GA-MLPs from the viewpoint of graph isomorphism tests,
we propose a new perspective that better suits the setting of node-prediction tasks: we analyze the
* Equal contributions. Code available at https://github.com/leichen2018/GNN_vs_GAMLP.
1
Published as a conference paper at ICLR 2021
expressive power of models including GNNs and GA-MLPs as node-level functions, or equiva-
lently, as functions on rooted graphs. Under this perspective, we prove an exponential-in-depth gap
between the expressive powers of GNNs and GA-MLPs. We illustrate this gap by finding a broad
family of user-friendly functions that can be provably approximated by GNNs but not GA-MLPs,
based on counting attributed walks on the graph. Moreover, via the task of community detection,
we show a lack of flexibility of GA-MLPs, compared to GNNs, to learn the best operators to use.
In summary, our main contributions are:
•	Finding graph pairs that several GA-MLPs cannot distinguish while GNNs can, but also
proving there exist simple GA-MLPs that distinguish almost all non-isomorphic graphs.
•	From the perspective of approximating node-level functions, proving an exponential gap
between the expressive power of GNNs and GA-MLPs in terms of the equivalence classes
on rooted graphs that they induce.
•	Showing that the functions that count a particular type of attributed walk among nodes can
be approximated by GNNs but not GA-MLPs both in theory and numerically.
•	Through community detection tasks, demonstrating that GNNs have higher flexibility in
learning than GA-MLPs due to the fixed choice of the operator family in the latter.
2	Related Works
Depth in GNNs Kipf & Welling (2016) observe that the performance of Graph Convolutional
Networks (GCNs) degrade as the depth grows too large, and the best performance is achieved with
2 or 3 layers. Along the spectral perspective on GNNs (Bruna et al., 2013; Defferrard et al., 2016;
Bronstein et al., 2017; NT & Maehara, 2019), Li et al. (2018) and Wu et al. (2019) explain the
failure of deep GCNs by the over-smoothing of the node features. Oono & Suzuki (2020) show an
exponential loss of expressive power as the depth in GCNs increases in the sense that the hidden
node states tend to converge to Laplacian sub-eigenspaces as the depth increases to infinity. Alon &
Yahav (2020) show an over-squashing effect of deep GNNs, in the sense that the width of the hidden
states needs to grow exponentially in the depth in order to retain all information about long-range
interactions. In comparison, our work focuses on more general GNNs based on neighborhood-
aggregation that are not limited in the hidden state widths, and demonstrates the their advantage in
expressive power compared to GA-MLP models at finite depth, in terms of distinguishing rooted
graphs for node-prediction tasks. On the other hand, there exist examples of useful deep GNNs.
Chen et al. (2019b) apply 30-layer GNNs for community detection problems, which uses a family
of multi-scale operators as well as normalization steps (Ioffe & Szegedy, 2015; Ulyanov et al.,
2016). Recently, Li et al. (2019; 2020a) and Chen et al. (2020a) build deeper GCN architectures
with the help of various residual connections (He et al., 2016) and normalization steps to achieve
impressive results in standard datasets, which further highlights the need to study the role of depth
in GNNs. Gong et al. (2020) propose geometrically principled connections, which improve upon
vanilla residual connections on graph- and mesh-based tasks.
Existing GA-MLP-type models Motivated by better understanding GNNs as well as enhancing
computational efficiency, several models of the GA-MLP type have been proposed and they achieve
competitive performances on various datasets. Wu et al. (2019) propose the Simple Graph Con-
volution (SGC), which removes the intermediary weights and nonlinearities in GCNs. Chen et al.
(2019a) propose the Graph Feature Network (GFN), which further adds intermediary powers of the
normalized adjacency matrix to the operator family and is applied to graph-prediction tasks. NT &
Maehara (2019) propose the Graph Filter Neural Networks (gfNN), which enhances the SGC in the
final MLP step. Rossi et al. (2020) propose Scalable Inception Graph Neural Networks (SIGNs),
which augments the operator family with Personalized-PageRank-based (Klicpera et al., 2018; 2019)
and triangle-based (Monti et al., 2018; Chen et al., 2019b) adjacency matrices.
Expressive Power of GNNs Xu et al. (2019) and Morris et al. (2019) show that GNNs based on
neighborhood-aggregation are no more powerful than the Weisfeiler-Lehman (WL) test for graph
isomorphism (Weisfeiler & Leman, 1968), in the sense that these GNNs cannot distinguish between
any pair of non-isomorphic graphs that the WL test cannot distinguish. They also propose models
that match the expressive power of the WL test. Since then, many attempts have been made to build
2
Published as a conference paper at ICLR 2021
GNN models whose expressive power are not limited by WL (Morris et al., 2019; Maron et al.,
2019a; Chen et al., 2019c; Morris & Mutzel, 2019; You et al., 2019; Bouritsas et al., 2020; Li et al.,
2020b; Flam-Shepherd et al., 2020; Sato et al., 2019; 2020). Other perspectives for understanding
the expressive power of GNNs include function approximation (Maron et al., 2019b; Chen et al.,
2019c; Keriven & Peyre, 2019), substructure counting (Chen et al., 2020b), Turing universality
(Loukas, 2020) and the determination of graph properties (Garg et al., 2020). Sato (2020) provides
a survey on these topics. In this paper, besides studying the expressive power of GA-MLPs along
the line of graph isomorphism tests, we propose a new perspective of approximating functions on
rooted graphs, which is motivated by node-prediction tasks, and show a gap between GA-MLPs and
GNNs that grows exponentially in the size of the receptive field in terms of the equivalence classes
that they induce on rooted graphs.
3	Background
3.1	Notations
Let G = (V, E) denote a graph, with V being the vertex set and E being the edge set. Let n denote
the number of nodes in G, A ∈ Rn×n denote the adjacency matrix, D ∈ Rn×n denote the diagonal
degree matrix with Dii = di being the degree of node i. We call D-2 AD-2 the (symmetrically)
normalized adjacency matrix, and D-αAD-β a generalized normalized adjacency matrix for any
α, β ∈ R. Let X ∈ Rn×d denote the matrix of node features, where Xi denotes the d-dimensional
feature that node i possesses. For a node i ∈ V , let N (i) denote the set of neighbors ofi. We assume
that the edges do not possess features. In a node prediction task, the labels are given by Y ∈ Rn .
For a positive integer K, we let [K] = {1, ..., K}. We use {...}m to denote a multiset, which
allows repeated elements. We say a function f(K) is doubly-exponential in K if log log f(K) is
polynomial in K, and poly-exponential in K if log f(K) is polynomial in K, as K tends to infinity.
3.2	Graph Neural Networks (GNNs)
Following the notations in Xu et al. (2019), we consider K-layer GNNs defined generically as
follows. For k ∈ [K], we compute the hidden node states H ∈ Rn×d(k) iteratively as
Mi(k) = AGGREGATE(k)({Hj(k-1) : j ∈ N (i)}) , Hi(k) = COMBINE(k) (Hi(k-1), Mi(k)) , (1)
where we set H(0) = X to be the node features. Ifa graph-level output is desired, we finally let
ZG = READOUT({Hi(K) : i ∈ V }) ,	(2)
Different choices of the trainable COMBINE, AGGREGATE and READOUT functions result in
different GNN models, though usually AGGREGATE and READOUT are chosen to be permutation-
invariant. As graph-level functions, it is shown in Xu et al. (2019) and Morris et al. (2019) that
the maximal expressive power of models of this type coincides with running K iterations of the
WL test for graph isomorphism, in the sense that any two non-isomorphic graphs that cannot be
distinguished by the latter cannot be distinguished by the K-layer GNNs, either. For this reason, we
will not distinguish between GNN and WL in discussions on expressive powers.
3.3	Graph-Augmented Multi-Layer Peceptrons (GA-MLPs)
GA-MLPs are models that consist of two steps - first augmenting the node features with some
operators based on the graph topology, and then applying a node-wise learnable function. Below
we focus on using linear graph operators to augment the node features, while an extension of the
definition as well as some of the theoretical results in Section 5 to GA-MLPs using general graph
operators is given in Appendix A. Let Ω = {ωι(A),…，ωκ(A)} ⊆ Rn×n be a set of (usually
multi-hop) linear operators that are functions of the adjacency matrix, A. Common choices of the
operators are powers of the (normalized) adjacency matrix, and several particular choices of Ω that
give rise to existing GA-MLP models are listed in Appendix B. In its general form, a GA-MLP first
computes a series of augmented features via
Xk = ωk(A) ∙以X) ∈ Rn×d ,	(3)
3
Published as a conference paper at ICLR 2021
Figure 1: A pair of graphs that can be distinguished by 2 iterations of the WL test but not by GA-MLPs with
Ω ⊆ {Ak : k ∈ N},as proved in Appendix K.
了
With 夕：Rd → Rd being a learnable function acting as a feature transformation applied to each
node separately. It can be realized by an MLP, e.g.夕(X) = σ(XW1)W2, where σ is a nonlinear
activation function and W1 , W2 are trainable Weight matrices of suitable dimensions. Next, the
1
model concatenates X1, ..., XK into X = [X1, ..., XK] ∈ Rn×(Kd), and computes
Z = P(X) ∈ Rn×d0 ,	(4)
where ρ : RKd → Rd0 is also a learnable node-wise function, again usually realized by an MLP. If
a graph-level output is desired, we can also add a READOUT function as in (2).
A simplified version of the model sets 夕 to be the identity function, in which case (3) and (4) can be
written together as
Z = P(E(A) ∙ X,...,ωκ(A) ∙ X])	(5)
Such a simplification improves computational efficiency since the matrix products ωk (A) ∙ X can be
pre-computed before training (Rossi et al., 2020). Since we are mostly interested in an upper bounds
on the expressive power of GA-MLPs, we will work with the more general update rule (3) in this
paper, but the lower-bound result in Proposition 2 remains valid even when we restrict to the subset
of models where 夕 is taken to be the identity function.
4	Expressive Power as Graph Isomorphism Tests
We first study the expressive power of GA-MLPs via their ability to distinguish non-isomorphic
graphs. It is not hard to see that when Ω = {I,A,...,Ak}, where A = D-αAD-β for any
α, β ∈ R generalizes the normalized adjacency matrix, this is upper-bounded by the power ofK+ 1
iterations of WL. We next ask whether it can fall strictly below. Indeed, for two common choices of
Ω, we can find concrete examples: 1) If Ω consists of integer powers of any normalized adjacency
matrix of the form D-αAD-(1-α) for some α ∈ [0, 1], then it is apparent that the GA-MLP cannot
distinguish any pair of regular graphs with the same size but different node degrees; 2) If Ω consists
of integer powers of the adjacency matrix, A, then the model cannot distinguish between the pair of
graphs shown in Figure 1, which can be distinguished by 2 iterations of the WL test. The proof of
the latter result is given in Appendix K. Together, we summarize the results as:
Proposition 1. If Ω ⊆ {Ak : k ∈ N}, with either A = A or A = D-αAD-(1-α) for some
α ∈ [0, 1], there exists a pair of graphs which can be distinguished by GNNs but not this GA-MLP.
Nonetheless, if we focus on not particular counterexamples but rather the average performance in
distinguishing random graphs, it is not hard for GA-MLPs to reach the same level as WL, which is
known to distinguish almost all pairs of random graphs under a uniform distribution (Babai et al.,
1980). Specifically, building on the results in Babai et al. (1980), we prove in Appendix F that:
Proposition 2. For all n ∈ N+, ∃αn > 0 such that any GA-MLP that has {D,AD-αn} ⊆ Ω
can distinguish almost all pairs of non-isomorphic graphs of at most n nodes, in the sense that the
fraction of graphs on which such a GA-MLP fails to test isomorphism is 1 - o(1) as n → ∞.
The hypothesis that distinguishing non-isomorphic graphs is not difficult on average for either GNNs
or GA-MLPs is further supported by the numerical results provided in Appendix C, in which we
count the number of equivalence classes that either of them induce on graphs that occur in real-
world datasets. This further raises the question of whether graph isomorphism tests along suffice as a
4
Published as a conference paper at ICLR 2021
criterion for comparing the expressive power of models on graphs, which leads us to the explorations
in the next section.
Lastly, We remark that with suitable choices of operators in Ω, it is possible for GA-MLPs to go
beyond the power of WL. For example, if Ω contains the power graph adjacency matrix introduced
in Chen et al. (2019b), min(A2, 1), then the GA-MLP can distinguish between a hexagon and a pair
of triangles, which WL cannot distinguish.
5	Expressive Power as Functions on Rooted Graphs
To study the expressive power beyond graph isomorphism tests, we consider the setting of node-
wise prediction tasks, for which the final readout step (2) is dropped in both GNNs and GA-MLPs.
Whether the learning setup is transductive or inductive, we can consider the models as functions on
rooted graphs, or egonets (Preciado & Jadbabaie, 2010), which are graphs with one node designated
as the root {i1, ..., in} is a set of nodes in the graphs {G1, ..., Gn} (not necessarily distinct) and with
node-level labels {Yi1 , ..., Yin} known during training, respectively, then the goal is to fit a function
to the input-output pairs (G[nin], Yin), where we use G[i] to denote the rooted graph with G being the
graph and the node i in G being the root. Thus, we can evaluate the expressive power of GNNs and
GA-MLPs by their ability to approximate functions on the space of rooted graphs, which we call E .
To do so, we introduce a notion of induced equivalence relations on E, analogous to the equivalence
relations on G introduced in Appendix C. Given a family of functions F on E , we can define an
equivalence relation 'E;F among all rooted graphs such that ∀G[i] , G0[i0] ∈ E, G[i] 'E;F G0[i0]
if and only if ∀f ∈ F, f(G[i]) = f (G0[i0]). By examining the number and sizes of the induced
equivalence classes of rooted graphs, we can evaluate the relative expressive power of different
families of functions on E in a quantitative way.
In the rest of this section, we assume that the node features belong to a finite alphabet X ⊆ N and all
nodes have degree at most m ∈ N+ . Firstly, GNNs are known to distinguish neighborhoods up to
the rooted aggregation tree, which can be obtained by unrolling the neighborhood aggregation steps
in the GNNs as well as the WL test (Xu et al., 2019; Morris et al., 2019; Garg et al., 2020). The
depth-K rooted aggregation tree of a rooted graph G[i] is a depth-K rooted tree with a (possibly
many-to-one) mapping from every node in the tree to some node in G[i] , where (i) the root of the
tree is mapped to node i, and (ii) the children of every node j in the tree are mapped to the neighbors
of the node in G[i] to which j is mapped. An illustration of rooted graphs and rooted aggregation
trees is given in Figure 4. Hence, each equivalence class in E induced by the family of all depth-K
GNNs consists of all rooted graphs that share the same rooted aggregation tree of depth-K. Thus, to
estimate the number of equivalence classes on E induced by GNNs, we need to estimate the number
of possible rooted aggregation trees, which is given by Lemma 3 in Appendix G. Thus, we derive
the following lower bound on the number of equivalence classes in E that depth-K GNNs induce:
Proposition 3. Assume that |X | ≥ 2 andm ≥ 3. The total number of equivalence classes of rooted
graphs induced by GNNs of depth K grows at least doubly-exponentially in K.
In comparison, we next demonstrate that the equivalence classes induced by GA-MLPs are more
coarsened. To see this, let's first consider the example where we take Ω = {I, A, A2,…，AK}, in
which A = D-αAD-β with any α,β ∈ R is a generalization of the normalized adjacency matrix.
From formula (3), by expanding the matrix product, we have
(AfW))i=	X	d-αdSβ…d-kMβd-β以Xik) ,	(6)
(i1,...,ik)∈Wk(G[i] )
where we define Wk(G[i]) = {(i1, ..., ik) ⊆ V : Ai,i1, Ai1,i2, ...,Aik-1,ik > 0} to be set of all
walks of length k in the rooted graph G[i] starting from node i (an illustration is given in Figure 2).
Thus, the kth augmented feature of node i, (Ak夕(X))i, is completely determined by the number
of each “type” of walks in G[i] of length k, where the type of a walk, (i1, ..., ik), is determined
jointly by the degree multiset, {di1 , ..., dik-1 } as well as the degree and the node feature of the end
node, dik and Xik . Hence, to prove an upper bound on the total number of equivalence classes on
E induced by such a GA-MLP, it is sufficient to upper-bound the total number of possibilities of
5
Published as a conference paper at ICLR 2021
Figure 2: A pair of rooted graphs, G[1] (left) and G0[1] (right), in which blue nodes have node feature 0
and green nodes have node feature 1. They belong to the same equivalence class induced by any GA-MLP
with operators that only depend on the graph structure, but different equivalence classes induced by GNNs.
In particular, G[1] and G0[1] ∈ T2,2,(1,1,3) (defined in Appendix I), and |W2(G[1] ; (1, 1))| = 1 whereas
|W2(G0[1];(1,1))| =0.
assigning the counts of all types of walks in a rooted graph. This allows us to derive the following
result, which we prove in Appendix H.
Proposition 4. FiX Ω = {I, A, A2,…，AK}, where A = D-αAD-β for some α,β ∈ R. Then the
total number of equivalence classes in E induced by such GA-MLPs is poly-exponential in K.
Compared with Proposition 3, this shows that the number of equivalence classes on E induced by
such GA-MLPs is exponentially smaller than that by GNNs. In addition, as the other side of the same
coin, these results also indicate the complexity of these hypothesis classes. Building on the results
in Chen et al. (2019c; 2020b) on the equivalence between distinguishing non-isomorphic graphs and
approximating arbitrary permutation-invariant functions on graphs by neural networks, and by the
definition of VC dimension (Vapnik & Chervonenkis, 1971; Mohri et al., 2018), we conclude that
Corollary 1. The VC dimension of all GNNs of K layers as functions on rooted graphs grows at
least doubly-exponentially in K; Fixing α,β ∈ R, the VC dimension of all GA-MLPs with Ω =
{I,A,A2,..., AK } asfunctions on rooted graphs is at most poly-exponential in K.
Meanwhile, for more general operators, we can show that the equivalence classes induced by GA-
MLPs are coarser than those induced by GNNs at least under some measurements. For instance,
the pair of rooted graphs in Figure 2 belong to the same equivalence class induced by any GA-MLP
(as we prove in Appendix I) but different equivalence classes induced by GNNs. Rigorously, we
characterize such a gap in expressive power by finding certain equivalence classes in E induced by
GA-MLPs that intersect with many equivalence classes induced by GNNs. In particular, we have
the following general result, which we prove in Appendix I:
Proposition 5. If Ω is any family of equivariant linear operators on the graph that only depend
on the graph topology of at most K hops, then there exist exponentially-in-K many equivalence
classes in E induced by the GA-MLPs with Ω, each of which intersects with doubly-exponentially-
in-K many equivalence classes in E induced by depth-K GNNs, assuming that |X | ≥ 2 andm ≥ 3.
Conversely, in constrast, if Ω = {I,A,A2,..., AK}, in which A = D-αAD-β with any α, β ∈ R,
then each equivalence class in E induced by depth-(K + 1) GNNs is contained in one equivalence
class induced by the GA-MLPs with Ω.
In essence, this result establishes that GA-MLP circuits can express fewer (exponentially fewer)
functions than GNNs with equivalent receptive field. Taking a step further, we can find explicit
functions on rooted graphs that can be approximated by GNNs but not GA-MLPs. In the framework
that we have developed so far, this occurs when the image of each equivalence class in E induced
by GNNs under this function contains a single value, whereas the image of some equivalence class
in E induced by GA-MLPs contains multiple values. Inspired by the proofs of the results above, a
natural candidate is the family of functions that count the number of walks of a particular type in the
rooted graph. We can establish the following result, which we prove in Appendix J:
Proposition 6. For any sequence of node features {xk}k∈N+ ⊆ X, consider the sequence of func-
tions fk(G[i]) := |Wk(G[i]; (x1, ..., xk))| on E. For all k ∈ N+, the image under fk of every
equivalence class in E induced by depth-k GNNs contains a single value, while for any GA-MLP
using equivariant linear operators that only depend on the graph topology, there exist exponentially-
in-k many equivalence classes in E induced by this GA-MLP whose image under fk contains
exponentially-in-k many values.
In other words, there exist graph instances where the attributed-walk-counting-function fk takes
different values, yet no GA-MLP model can predict them apart - and there are exponentially many
6
Published as a conference paper at ICLR 2021
		Cora	Citeseer		Pubmed			Cora		RRG	
# Nodes		2708		3327	19717		Model	Train	Test	Train	Test
K	GNN	GA-MLP	GNN	GA-MLP	GNN	GA-MLP					
							GIN	3.98E-6	9.72E-7	3.39E-5	2.61E-4
											
											
1	37	37	31	31	82	82	GA-MLP-A	1.23E-1	1.56E-1	1.75E-2	2.13E-2
2	1589	756	984	506	8059	3762	GA-MLP-A+	1.87E-2	6.44E-2	1.69E-2	2.13E-2
3	2301	2158	1855	1550	12814	12014	〜 GA-MLP-A(I) GA-MLP-A⑴ +	4.22E-1	5.79E-1	1.02E-1	1.58E-1
4 5	2363 2365	2359 2365	2074 2122	2019 2115	12990 12998	12979 12998		4.00E-1	5.79E-1	1.12E-1	1.52E-1
Table 2: MSE loss divided by label variance
for counting attributed walks on the Cora graph
and RRG. The models denoted as “+” contain
twice as many powers of the operator.
Table 1: The number of equivalence classes of rooted
graphs induced by GNN and GA-MLP on node classification
datasets with node features removed.
of these instances as the number of hops increases. This suggests the possibility of lower-bounding
the average approximation error for certain functions by GA-MLPs under various random graph
families, which we leave for future work.
6	Experiments
The baseline GA-MLP models We consider has operator family Ω = {I,A,…,AK} for a certain
K, and we call it GA-MLP-A. In Section 6.2 and 6.3, we also consider GA-MLPS with Ω =
{I, A(1), ..., A(K1)} (A() is defined in Appendix B), denoted as GA-MLP-A(1). For the experiments
in Section 6.3, due to the large K as well as the analogy with spectral methods (Chen et al., 2019b),
we use instance normalization (Ulyanov et al., 2016). Further details are described in Appendix L.
6.1	Number of equivalence classes of rooted graphs
Motivated by Propositions 3 and 4, we numerically count the number of equivalence classes in-
duced by GNNs and GA-MLPs among the rooted graphs found in actual graphs with node features
removed. For depth-K GNNs, we implement a WL-like process with hash functions to map the
depth-K egonet associated with each node to a string before comparing across nodes. For GA-
MLP-A, we compare the augmented features of each egonet computed via (3). From the results in
Table 1, we see that indeed, the number of equivalence classes induced by GA-MLP-A is smaller
than that by GNNs, with the highest relative difference occurring at K = 2. The contrast is much
more visible than their difference in the number of graph equivalence classes given in Appendix C.
6.2	Counting attributed walks
Motivated by Proposition 6, we test the ability of GNNs and GA-MLPs to count the number of walks
of a particular type in synthetic data. We take graphs from the Cora dataset (with node features
removed) as well as generate a random regular graph (RRG) with 1000 nodes and the node degree
being 6. We assign node feature blue to all nodes with even index and node feature red to all nodes
with odd index, due to which the node feature is given by 2-dimensional one-hot encoding. On the
Cora graph, a node i’s label is given by the number of walks of the type blue-→blue-→blue starting
from i. On the RRG, the label is given by the number of walks of the type blue-→blue-→blue-→blue
starting from i. The number of nodes for training and testing is split as 1000/1708 for the Cora graph
and 300/700 for the random regular graph. We test four GA-MLP models, two with as many powers
of the operator as the walk length and the other two with twice as many operators, and compare their
performances against that of the Graph Isomorphism Network (GIN), a GNN model that achieves
the expressive power of the WL test (Xu et al., 2019). From Table 2, we see that GIN significantly
outperforms GA-MLPs in both training and testing on both graphs, consistent with the theoretical
result in Proposition 6 that GNNs can count attributed walks while GA-MLPs cannot. Thus, this
points out an intuitive task that lies in the gap of expressive power between GNNs and GA-MLPs.
6.3	Community detection on Stochastic Block Models (SBM)
We use the task of community detection to illustrate another limitation of GA-MLP models: a lack
of flexibility to learn the family of operators. SBM is a random graph model in which nodes are
partitioned into underlying communities and each edge is drawn independently with a probability
7
Published as a conference paper at ICLR 2021
that only depends on whether the pair of nodes belong to the same community or not. The task of
community detection is then to recover the community assignments from the connectivity pattern.
We focus on binary (that is, having two underlying communities) SBM in the sparse regime, where
it is known that the hardness of detecting communities is characterized by a signal-to-noise ratio
(SNR) that is a function of the in-group and out-group connectivity (Abbe, 2017). We select 5 pairs
of in-group and out-group connectivity, resulting in 5 different hardness levels of the task.
Among all different approaches to community detection, spectral methods are particularly worth
mentioning here, which usually aim at finding a certain eigenvector of a certain operator that is
correlated with the community assignment, such as the second largest eigenvector of the adjacency
matrix or the second smallest eigenvector of the Laplacian matrix or the Bethe Hessian matrix (Krza-
kala et al., 2013). In particular, the Bethe Hessian matrix is known to be asymptotically optimal in
the hard regime, provided that a data-dependent parameter is known. Note that spectral methods
bear close resemblance to GA-MLPs and GNNs. In particular, Chen et al. (2019b) propose a spec-
tral GNN (sGNN) model for community detection that can be viewed as a learnable generalization
of power iterations on a collection of operators. Further details on Bethe Hessian and sGNN are
provided in Appendix L.
We first compare two variants of GA-
MLP models: GA-MLP-A with K =
120, and GA-MLP-H with Ω gen-
erated from the Bethe Hessian ma-
trix with the oracle data-dependent
parameter also up to K = 120. From
Figure 3, we see that the latter con-
sistently outperforms the former, in-
dicating the importance of the choice
of the operators for GA-MLPs. As re-
ported in Appendix L, replacing A by
A(1) yields no improvement in per-
formance. Meanwhile, we also test
a variant of sGNN that is only based
on powers of the A and has the same
Figure 3: Community detection on binary SBM with 5 choices of
in- and out-group connectivities, each yielding to a different SNR.
Higher overlap means better performance.
receptive field as GA-MLP-A (fur-
ther details given in Appendix L). We
see that its performance is compara-
ble to that of GA-MLP-H. Thus, this
demonstrates a scenario in which GA-MLP with common choices of Ω do not work well, but there
exists some choice of Ω that is a priori unknown, with which GA-MLP can achieve good perfor-
mance. In contrast, a GNN model does not need to rely on the knowledge of such an oracle set of
operators, demonstrating its superior capability of learning.
7	Conclusions
We have studied the separation in terms of representation power between GNNs and a popular alter-
native that we coined GA-MLPs. This latter family is appealing due to its computational scalability
and its conceptual simplicity, whereby the role of topology is reduced to creating ‘augmented’ node
features then fed into a generic MLP. Our results show that while GA-MLPs can distinguish almost
all non-isomorphic graphs, in terms of approximating node-level functions, there exists a gap grow-
ing exponentially-in-depth between GA-MLPs and GNNs in terms of the number of equivalence
classes of nodes (or rooted graphs) they induce. Furthermore, we find a concrete class of functions
that lie in this gap given by the counting of attributed walks. Moreover, through community detec-
tion, we demonstrate the lack of GA-MLP’s ability to go beyond the fixed family of operators as
compared to GNNs. In other words, GNNs possess an inherent ability to discover topological fea-
tures through learnt diffusion operators, while GA-MLPs are limited to a fixed family of diffusions.
While we do not attempt to provide a decisive answer of whether GNNs or GA-MLPs should be
preferred in practice, our theoretical framework and concrete examples help to understand their
differences in expressive power and indicate the types of tasks in which a gap is more likely to
8
Published as a conference paper at ICLR 2021
be seen - those exploiting stronger structures among nodes like counting attributed walks, or those
involving the learning of graph operators. That said, our results are purely on the representation side,
and disregard optimization considerations; integrating the possible optimization counterparts is an
important direction of future improvement. Finally, another open question is to better understand the
links between GA-MLPs and spectral methods, and how this can help learning diffusion operators.
Acknowledgements
We are grateful to Jiaxuan You for initiating the discussion on GA-MLP-type models, as well as
Mufei Li, Minjie Wang, Xiang Song, Lingfan Yu, Michael M. Bronstein and Shunwang Gong for
helpful conversations. This work is partially supported by the Alfred P. Sloan Foundation, NSF
RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091, Capital One, Samsung Electronics,
and the Institute for Advanced Study.
References
Emmanuel Abbe. Community detection and stochastic block models: recent developments. The
Journal of Machine Learning Research, 18(1):6446-6531, 2017.
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
arXiv preprint arXiv:2006.05205, 2020.
Laszlo Babai, Paul Erdos, and Stanley M Selkow. Random graph isomorphism. SIaM Journal on
computing, 9(3):628-635, 1980.
Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improv-
ing graph neural network expressivity via subgraph isomorphism counting. arXiv preprint
arXiv:2006.09252, 2020.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:
Going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42, 2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. arXiv preprint arXiv:2007.02133, 2020a.
Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on
graph classification. arXiv preprint arXiv:1905.04579, 2019a.
Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural
networks. Internation Conference on Learning Representations, 2019b.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In Advances in Neural Information
Processing Systems, pp. 15868-15876, 2019c.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? arXiv preprint arXiv:2002.04025, 2020b.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Daniel Flam-Shepherd, Tony Wu, Pascal Friederich, and Alan Aspuru-Guzik. Neural message pass-
ing on high order paths. arXiv preprint arXiv:2002.10413, 2020.
Vikas K. Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks, 2020.
9
Published as a conference paper at ICLR 2021
Shunwang Gong, Mehdi Bahri, Michael M Bronstein, and Stefanos Zafeiriou. Geometrically prin-
cipled connections in graph neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 11415-11424, 2020.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Sergei Ivanov, Sergei Sviridov, and Evgeny Burnaev. Understanding isomorphism bias in graph data
sets. arXiv preprint arXiv:1910.12091, 2019.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems, pp. 7092-7101, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2018.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph learn-
ing. In Advances in Neural Information Processing Systems, pp. 13354-13366, 2019.
Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborova,
and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National
Academy of Sciences, 110(52):20935-20940, 2013.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as
cnns? In Proceedings of the IEEE International Conference on Computer Vision, pp. 9267-9276,
2019.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020a.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding-design provably
more powerful gnns for structural representation learning. arXiv preprint arXiv:2009.00142,
2020b.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial In-
telligence (AAAI-18), pp. 3538-3545. Association for the Advancement of Artificial Intelligence,
2018.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
B1l2bp4YwS.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. In Advances in neural information processing systems,
pp. 10945-10955, 2019.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems, pp. 2153-2164, 6 2019a.
10
Published as a conference paper at ICLR 2021
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. volume 97 of Proceedings ofMachine Learning Research, pp. 4363-4371, Long Beach,
California, USA, 09-15 JUn 2019b. PMLR. URL http://Proceedings.mlr.ρress/
v97/maron19a.html.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolu-
tional network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225-228.
IEEE, 2018.
Christopher Morris and Petra Mutzel. Towards a practical k-dimensional weisfeiler-leman algo-
rithm. arXiv preprint arXiv:1904.01543, 2019.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
Association for the Advancement of Artificial Intelligence, 2019.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020.
Victor M Preciado and Ali Jadbabaie. From local measurements to network spectral properties:
Beyond degree distributions. In 49th IEEE Conference on Decision and Control (CDC), pp.
2686-2691. IEEE, 2010.
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Fed-
erico Monti. Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198,
2020.
Alaa Saade, Florent Krzakala, and Lenka Zdeborova. Spectral clustering of graphs with the bethe
hessian. In Advances in Neural Information Processing Systems, pp. 406-414, 2014.
Ryoma Sato. A survey on the expressive power of graph neural networks. arXiv preprint
arXiv:2003.04078, 2020.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation ratios of graph neural networks
for combinatorial problems. In Advances in Neural Information Processing Systems, pp. 4081-
4090, 2019.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. arXiv preprint arXiv:2002.03155, 2020.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability & Its Applications, 16(2):264-280, 1971. doi:
10.1137/1116025. URL https://doi.org/10.1137/1116025.
B Weisfeiler and A Leman. The reduction of a graph to canonical form and the algebra which
appears therein. Nauchno-Technicheskaya Informatsia, 2(9):12-16, 1968.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph convolutional networks. volume 97 of Proceedings of Machine Learning Re-
search, pp. 6861-6871, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/wu19e.html.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453-5462, 2018.
11
Published as a conference paper at ICLR 2021
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International
Conference on Machine Learning, pp. 7134-7143, 2019.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp. 3391-
3401, 2017.
12
Published as a conference paper at ICLR 2021
A GA-MLP with general equivariant graph operators for node
FEATURE AUGMENTATION
For a graph G = (V, E) with n nodes, assume without loss of generality that V = [n]. Let Sn
denote the set of permutations of n, and ∀π ∈ Sn, it maps a node i ∈ [n] to π(i) ∈ [n]. For π ∈ Sn
and a matrix M ∈ Rn×n, we use π ? M ∈ Rn×n to denote the π-permuted version of M, that is,
(π ? M)i,j = Mπ(i),π(j). For π ∈ Sn and a matrix Z ∈ Rn×d, we use π ? Z ∈ Rn×d to denote the
π-permuted version of M, that is, (π ? Z)i,p = Zπ(i),p.
Below, we define a more general form of GA-MLP models that extend the use of equivariant linear
operators for node feature propagation to that of general equivariant graph operators. We first define
a map ω : Rn×n × Rn×d → Rn×d0 , whose first input argument is always the adjacency matrix of a
graph, A, and second input argument is a node feature matrix. We say the map satisfies equivariance
to node permutations if ∀π ∈ Sn, ∀Z ∈ Rn×d, there is ω(π*A,π?Z) = π*ω(A, Z). With a slight
abuse of notations, we also use ω[A](Z) to denote ω(A, Z), thereby considering ω[A] : Rn×d →
Rn×d0 as an operator on node features. If ω satisfies equivariance to node permutations as defined
above, we then call ω [A] an equivariant graph operator. We can then define a general (nonlinear)
GA-MLP model as
~ -...
X = ω[A](X)
.~ .
Z = P(X)
(7)
where ω is an equivariant graph operator, and ρ is a node-wise function.
It is easy to see that
Proposition 7. If ω[A](X) = m(A) ∙ X, where m(∙) = Rn×n → Rn×n is an entry-wise function
or matrix product or compositions thereof, then ω[A] is an equivariant graph operator.
A.1 Extending the proof of Proposition 5 and 6 to general GA-MLPs
An extension of the first half of Proposition 5 is
Proposition 8. If ω[A] is an equivariant graph operator, then there exist exponentially-in-K many
equivalence classes in E induced by the general GA-MLPs with ω [A], each of which intersects with
doubly-exponentially-in-K many equivalence classes in E induced by depth-K GNNs, assuming
that |X | ≥ 2 and m ≥ 3.
Proof: Similar to the proof of Proposition 5 given in Appendix I, we consider the set of full m-ary
rooted trees of depth K, Tm,K,X, that is all rooted trees of depth K in which the nodes have features
belonging to the discrete set X ⊆ N and all non-leaf nodes have m children. Tm,K,X is a subset
of E, the space of all rooted graphs. Suppose f is a function represented by a general GA-MLP
defined in (7) with an equivariant graph operator ω[A]. Let Vk denote the set of nodes at depth k
of T. Notice the following symmetry among nodes in each Vk: if π is the permutation of a pair of
nodes in some Vk for 1 ≤ k ≤ K, then π ? A= A. By the equivariance property ofω, this implies
that
ω[A](π ? Z) =ω[π ? A](π ? Z)
=π ? ω[A](Z)
(8)
Let X denote the node feature matrix associated with T, and ∏?T denote the rooted tree in TmKX
with the same topology (i.e., also a full m-ary rooted tree) but node feature matrix π ? X . Then,
since the root node is not permuted under π, we know that
f(T) =ρ (ω[A](X)1,:)
=ρ (π ? ω[A](X))1,:	(9)
=P (ω[A](π? X)1,:)
=f(π ?T)
This implies that for two trees T and T0 ∈ TmKX, if ∀0 ≤ k ≤ K, ∀x ∈ X, they satisfy
|Wk(T;x)| = |Wk(T0;x)|, then f(T) = f(T0) for all such f's, and hence T and T0 belong to
13
Published as a conference paper at ICLR 2021
the same equivalence class in E induced by GA-MLPs. Therefore, by the rest of the argument given
in Proposition 5, Proposition 8 can be proven analogously for GA-MLPs with general equivariant
graph operators.
Similarly, Proposition 6 can also be extended to
Proposition 9. For any sequence of node features {xk}k∈N+ ⊆ X, consider the sequence of func-
tions fk(G[i]) := |Wk(G[i]; (x1, ..., xk))| on E. For all k ∈ N+, the image under fk of every
equivalence class in E induced by depth-k GNNs contains a single value, while for any GA-MLP
using equivariant graph operators, there exist exponentially-in-k many equivalence classes in E
induced by this GA-MLP whose image under fk contains exponentially-in-k many values.
The proof replies on the same extension as described above in the proof of Proposition 8.
B Examples of existing GA-MLP models
For E ∈ R, let A© = A + EI, D(e)be the diagonal matrix with D(e)= Pj Aij + e, and
4e)= D -TA©/-F.
•	Simple Graph Convolution (Wu et al., 2019):
Ω(A) = {(A(i))K} for some K > 0. In addition,夕 is the identity function and P(H)=
sof tmax(H W) for some trainable weight matrix W.
•	Graph Feature Network (Chen et al., 2019a):
Ω(A) = {I, D, A©,…，(A(e))K } for some K > 0 and e > 0. In addition,夕 is the identity
function and ρ is an MLP.
•	Scalable Inception Graph Networks (Rossi et al., 2020):
Ω(A) = {I} ∪ Ω1(A) ∪ Ω2(A) ∪ Ω3(A), where Ω1 (A) is a family of simple / normal-
ized adjacency matrices, Ω2 (A) is a family of PerSOnalized-PageRank-based adjacency
matrices, and Ω3(A) is a family of triangle-based adjacency matrices. In addition, writing
X = [Xi,…，XK], there is Z = P(X) = σ1(σ2([Xf1W1,..., XKWK])Wout), With σ1 and
σ2 being nonlinear activation functions and W1 , ..., WK and Wout being trainable weight
matrices of suitable dimensions.
C Equivalence classes induced by GNNs and GA-MLPs among
REAL GRAPHS
# Graphs K	IMDBBINARY		IMDBMULTI		REDDITBINARY		REDDITMULTI5K		COLLAB	
	GNN	1000 GA-MLP	GNN	1500 GA-MLP	GNN	2000 GA-MLP	GNN	5000 GA-MLP	GNN	5000 GA-MLP
1	51	51	49	49	781	781	1365	1365	294	294
2	537	537	387	387	1998	1998	4999	4999	4080	4080
3	537	537	387	387	1998	1998	4999	4999	4080	4080
ground truth		537		387		1998		4999		4080
Table 3: The number of equivalence classes of graphs induced by GNN and GA-MLP on real
datasets with node features removed. The last row gives the ground-truth number of isomorphism
classes of graphs computed from the implementation of Ivanov et al. (2019).
Given a space of graphs, G, and a family F of functions mapping G to R, F induces an equivalence
relation that we denote by 'G;F among graphs in G such that for Gi, G2 ∈ G, Gi 'G;F G2
if and only if ∀f ∈ F, f(Gi) = f(G2). For example, if F is powerful enough to distinguish all
pairs of non-isomorphic graphs, then each equivalence class under 'G,F contains exactly one graph.
Thus, by examining the number or sizes of the equivalence classes induced by different families of
functions on G, we can evaluate their relative expressive power in a quantitative way.
Hence, we supplement the theoretical result of Proposition 2 with the following numerical results
on five real-world datasets for graph-predictions. For graphs in each of the two real datasets, we
14
Published as a conference paper at ICLR 2021
remove their node features and count the total number of equivalence classes among them induced
by depth-K GNNs (equivalent to K-iterations of the WL test, as discussed in Section 3.2) as well
as GA-MLPs with Ω = {I, A,..., AK} for different K’s. We see from the results in Table 3 that as
soon as K ≥ 2, the number of equivalence classes induced by GNNs and the GA-MLPs are both
close to the total number of graphs up to isomorphism, implying that they are indeed both able to
distinguish almost all pairs of non-isomorphic graphs among the ones occurring in these datasets.
D Additional notations
For any k ∈ N+ and any rooted graph G[i] = (V, E, i) ∈ E, define
Wk(G[i]) ={(i1,...,ik) ⊆V : Ai,i1,Ai1,i2,...,Aik-1,ik >0}	(10)
Wk(G[i]) = {(i1,…,ik) ∈	Wk(G[i])	: i =	i2,i1 =	i3,…,ik-3	6=	ik-1 ,	ik-2	6= ik }	(11)
as the sets of walks and non-backtracking walks of length k in G[i] starting from the root node,
respectively. Note that when G[i] is a rooted tree, a non-backtracking walk of length k is a path from
the root node to a node at depth k. In addition, for 0 ≤ d1, ..., dk ≤ m and x1, ..., xk ∈ X, define
the following subsets ofWk (G[i] ):
Wk	G[i]; (d1,	..., dk),	xk	= {(i1,	..., ik)	∈	Wk(G[i])	: {di1,	..., dik}m = {d1, ...,dk}m,Xik	= xk}
(12)
Wk	G[i]; (x1,	..., xk)	=	{(i1,	...,	ik)	∈ Wk(G[i])	:	(Xi1,	...,	Xik)	= (x1, ...,	xk)}	(13)
Wk(G[i];xk) = {(i1,..., ik) ∈Wk(G[i]) : Xik =xk}	(14)
We also define Wk (G[i]; (di,..., dk),xk), Wk(G[i];(xi,…,xk)) and Wk(G[i]; Xk) similarly.
E Illustration of rooted graphs and rooted aggregation trees
Figure 4: An illustration of rooted graphs and rooted aggregation trees. Left: a pair of graphs, G and
G0. Center: the rooted graphs of 1 in G and G0, G[1] and G0[1] . Right: the rooted aggregation tree
that both G[1] and G0[1] correspond to.
F Proof of Proposition 2
With node features being identical in the random graphs, we take X ∈ Rn×1 to be the all-1 vector.
Thus,
(DX)i = di ,	(15)
15
Published as a conference paper at ICLR 2021
and
(AD-αX)i =	dj-α .	(16)
j∈N(i)
Since (4) and (2) together can approximate arbitrary permutation-invariant functions on multisets
(Zaheer et al., 2017), if two graphs G = (V, E) and G0 = (V 0, E0) cannot be distinguished by the
GA-MLP with an operator family Ω that includes {D, AD-ɑ} under any choice of its parameters, it
means that the two multisets {(di, Pj ∈N (i) dj-α) : i ∈ V }m = {(di0, Pj0∈N(i0) dj-0α) : i0 ∈ V 0}m,
and therefore both of the following hold:
{di : i ∈ V }m ={di0 : i0 ∈ V 0}m	(17)
{ X j : i ∈ V}m ={ X j : i0 ∈ V0}m	(18)
j∈N(i)	j0∈N (i0)
To see what this means, we need the two following lemmas.
Lemma 1. Let Sn be the set of all multisets consisting of at most n elements, all of which are
integers between 0 and n. Consider the function hα(S) := u∈S u-α defined for multisets S. If
a > a n-0gg(n-i), ha is an injective function on Sn.
Proof of Lemma 1: For hα to be injective on Sn, it suffices to require that ∀l ≤
there is l-α > n(l + 1)-α, for which it is sufficient to require that (n - 1)-α > n
α>
log n
n - 1,
-α+1, or
log n—log(n-1) .
Lemma 2 (Babai et al. (1980), Theorem 1). Consider the space of graphs with n vertices, Gn. There
isa subset Kn ⊆ Gn that contains almost all such graphs (i.e. the fraction converges to 1 asn → ∞)
such that the following algorithm yields a unique identifier for every graph G = (V, E) ∈ Kn:
Algorithm 1: Set r = [3 log n/ log 2], and let d(G) be the degree of the node in V with the rth
largest degree; For each node i in G, define the multiset Yi = {dj : j ∈ N(i),dj > d(G)}m;
Finally define a multiset associated with G, F(G) = {γi : i ∈ V}m, which is the output of the
algorithm.
In other words, ∀G, G0 ∈ Kn, G and G0 are isomorphic if and only ifF (G) = F (G0) as multisets.
In particular, we can choose Kn such that the top r node degrees of every graph in Kn are distinct.
Based on these lemmas, we will show that when a > logn-0on(n-i) and for G, G ∈ Kn (17) and
(18) together imply that G is isomorphic to G0. To see this, suppose that (17) and (18) hold. Because
of (17), we know that G and G0 share the same degree sequence, and hence d(G) = d(G0). Because
of (18), we know that there is a bijective map σ from V to V0 such that ∀i ∈ V,
X j = X d-0ɑ,	(19)
j∈N (i)	j0 ∈N (i0)
which, by Lemma 1, implies that {dj: j ∈ N (i)}m = {dj0 : j0 ∈ N(i0)}m. We then have γi =
{dj : j ∈ N(i)}m = {dj : j ∈ N(i)}m ∩ (N(G), ∞) = {dj0 : j0 ∈ N(i0)}m ∩ (d(G0), ∞)= Y,
and therefore F (G) = F(G0), which implies that G and G0 are isomorphic by Lemma 2. This
shows a contradiction. Therefore, if G, G0 ∈ Kn are not isomorphic, then it cannot be the case
that both (17) and (18) hold, and hence there exists a choice of parameters for the GA-MLP with
{D, AD-a} ⊆ Ω that makes it return different outputs when applied to G and G0. This proves
Proposition 2.
G	Proof of Proposition 3
As argued in the main text, to estimate the number of equivalence classes on E induced by GNNs,
we need to estimate the number of possible rooted aggregation trees. In particular, to lower-bound
the number of equivalence classes on E induced by GNNs, we only need to focus on a subset of all
possible rooted aggregation trees, namely those in which every node has exactly m children. Letting
TmA,K,X denote the set of all rooted aggregation trees of depth K in which each non-leaf node has
degree exactly m and the node features belong to X , we will first prove the following lemma:
16
Published as a conference paper at ICLR 2021
Lemma 3. If |X | ≥ 2, then |TmA,K,X | ≥ (m - 1)(2K-1).
Note that a rooted aggregation tree needs to satisfy the constraint that each of its node must have
its parent’s feature equal to one of its children’s feature, and so this lower bound is not as straight-
forward to prove as lower-bounding the total number of rooted subtrees. As argued above, this will
allow us to derive Proposition 3.
Proof of Lemma 3: Define B := {0, 1}. Since |X | ≥ 2, we assume without loss of generality that
B ⊆ X. To prove a lower-bound on the cardinality of TmA,K,X, it suffices to restrict our attention to
its subset, TmA,K := TmA,K,B, where all nodes have feature either 0 or 1. Furthermore, it is sufficient
to restrict our attention to the subset of TmA,K which contain all 2K possible types of paths of length
K from the root to the leaves. Formally, with Wk defined as in Appendix D, We let
TmA,K = {T ∈ TAK : ∀X1,…,Xk ∈ B, W K (T ;(xi,...,xk )) ≥ 1} ,	(20)
and it is sufficient to prove a lower bound on the cardinality of TmK. Define Pk = {(χι,..., Xk):
x1, ..., xk ∈ B} to be the set of all binary k-tuples. By the definition of (20), we know that ∀τ ∈
PK, |Wk (T ； T )| ≥ 1. This means that ∀τ ∈ PK, there exists at least one leaf node in T such that
the path from the root node to this node consists of a sequence of nodes with features exactly as
given by τ . We call any such node a node under τ .
We show such a lower bound on the cardinality of TmA,K inductively. For the base case, we know that
TmA,ι consists of all binary-featured depth-1 rooted trees with at least 1 leaf node of feature 0 and 1
leaf node of feature 1, and hence Tm,ι = 2(m - 1). Next, we consider the inductive step. For every
K ≥ 1 and every T∈TmA,κ,we can generate rooted aggregation trees belonging to T ∈ TmK+1 by
assigning children of feature 0 or 1 to the leaf nodes ofT. First note that, from two non-isomorphic
rooted aggregation trees T and T0∈TA,k ,we obtain non-isomorphic rooted aggregation trees in
TmK+ι in this way. Moreover, as we will show next, for every T ∈ TmK, we can lower-bound the
number of distinct rooted aggregation trees belonging to Tm,κ+ι obtained from T in this way.
There are many choices to assign the children. To get a lower-bound on the cardinality of TmK+「
we only need to consider a subset of these choices of assignments, namely, those that assign the
same number of children with feature 0 to every node under the same T ∈ PK. Thus, we let qκ+ι,τ
denote the number of children of feature 0 assigned to every node in τ. Due to the constraint that
each node in the rooted aggregation tree must have its parent’s feature equal to one of its children’s
feature, not all choices of {(fκ+ι,τ}τ∈Pk lead to legitimate rooted aggregation trees. Nonetheless,
when restricting to the choices where ∀τ ∈ PK, 1 ≤ qκ+ι,τ ≤ m - 1, we see that every leaf node
of T gets assigned at least one child of feature 0 and another child of feature 1, thereby satisfying
the constraint above whether its parent has feature 0 or 1. Moreover, for such choices, the rooted
aggregation tree of depth K+1 obtained in this way contains all 2K+1 possible paths of length K+1,
and therefore belongs to TmK+「Hence, it remains to show a lower bound on how many distinct
trees in TmK+ι can be obtained in this way from each T. Since for τ, T0 ∈ PK such that T = T0,
a node under T is distinguishable from a node under T0, we see that every legitimate choice of the
tuple of 2k integers, (7κ+1,τ )τ∈Pk , leads to a distinct rooted aggregation tree of depth K + 1, and
there are (m - 1)2K of these choices. Hence, we have derived that ∣Tm,κ+ι I ≥ (m - 1)2K 1Tm,κ ∣,
and therefore 1Tm,κ I ≥ (m - 1)PK=I 2k = (m - 1)2K-1.
17
Published as a conference paper at ICLR 2021
H Proof of Proposition 4
According to the formula (3), by expanding the matrix product, we have
(Ak 奴 X ))i =	X	d-α d-(α+β) ...dik-1+β)dikβ 以 Xik)
(i1,...,ik)∈Wk (G[i] )
=d-α	X	X	(dι...dk-i)-(α+β)dk φ(x)
{d1,…上dt-1}m,	("ii,…,ik )∈	_
dk ,x	Wk(G[i];{di,…,dk-ι}m,dk,x)
=d-α	X	((^d1...d-1)-(α+β)dkψ(x:)^ ∣Wk (G[i]; {d1,...,dt-1 }m,dk,x)∖ ,
{ di ,…,dk - 1 } m ,
dk ,X
(21)
With Wk(G[i]; {d1,...,dt-1}m,dk,x) defined in Appendix D. Hence, for two different nodes
i in G and i0 in G0 (G and G0 can be the same graph), the node-wise outputs of
the GA-MLP at i and i0 will be identical if the rooted graphs Gi and G0 [i0] satisfy
Wk(G[i]; {(dι, ...,(dk-ι}m,,dk,x) = Wk(G0[i ]；{di,...,dk-i}m,dk,x) for every combination of
choices on the multiset {dι,..., dk-ι}m, the integer dk and the node feature x, under the constraints
of di,..., dk ≤ m and X ∈ X. Note that there are at most (km--2) ≤ (k + m - 2)m-1 possible
choices of the multiset {d1,...,dk-1}m, m choices of dk and |X| choices of x, thereby allowing at
most |X |m(k + m - 2)m-1 possible choices. Because of the constraint
X	∣Wk (GK; {di,...,dt-i}m,dk ,x)∣ = |Wk (G[i])∣≤ mk ,	(22)
{d1,…,dk-1}m,
dk,x
We see that the total number of equivalence classes on E induced by such a GA-MLP is upper-
bounded by (m +XXmmkk+--^- -1), which is on the order of O(mkm) with k growing and m
bounded. Finally, since the total number of equivalence classes induced by multiple operators can
be upper-bounded by the product of the number of equivalence classes induced by each operator
separately, we derive the proposition as desired.
I Proof of Proposition 5
Consider the set of full m-ary rooted trees of depth K, Tm,K,X , that is all rooted trees of depth K
in which the nodes have features belonging to the discrete set X ⊆ N and all non-leaf nodes have m
children. Tm,K,X is a subset of E, the space of all rooted graphs. If f is a function represented by a
GA-MLP using operators of at most K-hop, then for T ∈ Tm,K,X , we can write
f(T)=ρ(XajXj),	(23)
j∈V
where we denote the node set of T by V and the vectors aj ’s depend only on the topological re-
lationship between j and the root node. Let Vk denote the set of nodes at depth k of T . By the
assumption that the operators depend only on the graph topology, and thanks to the topological
symmetry of such full m-ary trees among all nodes on the same depth, we have that ∀1 ≤ k ≤ K
and ∀j, j0 ∈ Vk, there is aj = a0j =: a[k] . Thus, we can write
f(T) =ρ(	a[k]φ(Xj))
0≤k≤K j∈Vk
=PI X X «[k],x|Wk(T;X)I)
0≤k≤K x∈X
(24)
for some other set of coefficients avfc,χ,s,	and where	Wk(T;x)	is defined	in Appendix D. In
other words, for two trees T and T0 ∈	Tm,K,X , if	∀0 ≤	k	≤ K, ∀x	∈ X, they satisfy
18
Published as a conference paper at ICLR 2021
|Wk(T;x)| = |Wk(T0;x)|, then f(T) = f(T0) for all such f's, and hence T and T0 belong to
the same equivalence class in E induced by GA-MLPs. Thus, for a certain subset of these equiv-
alence classes, we can lower-bound the number of equivalence classes in E induced by GNNs that
they intersect by lower-bounding the number of distinct trees in Tm,K,X that they contain, because
GNNs are able to distinguish non-isomorphic rooted subtrees. In particular, as a lower-bound is
sufficient, we restrict attention to the subset of those trees with node features either 0 or 1, that is,
trees belonging to Tm,K := Tm,K,B, with B := {0, 1}.
In a rooted tree T, Wk (T; x) gives the total number of nodes with feature X at depth k. For integers
q0, q1, ..., qK such that 0 ≤ qk ≤ mk, ∀k ≤ K, define
Tm,K,(qo,qι,..,qκ) = {T ∈ Tm,K ： Nk ≤ K, |W k (T ；0)| = qk } ,	(25)
that is, the subset of trees whose per-level-node-Counts, {|Wk(T[0)∣}k≤κ (and therefore
{|Wk(T; χ)∣}k≤κ,χ∈B) are given by the tuple (qo, qι,..., qk). From the argument above, all trees
in the same Tm,K,(q0,q1,...,qK) belong to the same equivalence class in E induced by GA-MLPs. On
the other hand, every pair of non-isomorphic trees belong to different equivalence class in E in-
duced by GNNs. Thus, to show Proposition 5, it is sufficient to find sufficiently many choices of
(q0, q1, ..., qK) such that Tm,K,(q0,q1,...,qK) contains sufficiently many non-isomorphic trees. Specif-
ically, we will show the following:
Lemma 4. For all integers q0, q1, ..., qK such that ∀2 ≤ k ≤ K,
2k - 2k-2 ≤ qk ≤ 1 mk ,	(26)
there is
K-1
,...,qK) | ≥ 2
-1
(27)
Proof of Lemma 4: To prove such a lower bound on the cardinality of Tm,K,(q0,q1,...,qK), it is suffi-
cient to prove a lower bound on the cardinality of its subset,
Tm,K,(qo,qι ,...,qκ) = {T ∈ Tm,K,(qo,qι ,...,qκ) : ∀χ1,…，xK ∈ B, W K (T ； (XI,…，xK )) ≥ 1} .
(28)
A similar construction is involved in the proof of Lemma 3 in Appendix G. Then, we will prove this
lemma by induction on K. For the base cases, it is obvious that |Tm,0,(0) | = |Tm,0,(1) | = 1, and
|Tm,1,(0,0) | = |Tm,1,(0,1) | = |Tm,1,(0,2) | = |Tm,1,(1,0) | = |Tm,1,(1,1) | = |Tm,1,(1,2) | = 1. We next
prove the inductive hypothesis that, for K ≥ 2 and when q0, q1, ..., qK satisfying (26), there is
K-2
|Tm,K,(q0,q1,...,qK)| ≥ 2	∙ lTm,K-1,(qo,qι ,...,qK-1) | .	(29)
To see this, we will next show that ∀T ∈ Tm,K-1,(q0,q1,...,qK-1), we can generate enough number
of depth-K trees in Tm,K,(q0,q1,...,qK) by appending children to the leaf nodes of T. Since any
two depth-K trees generated from two non-isomorphic depth-(K - 1) trees in this way are non-
isomorphic, this will allow us to lower-bound the total number of trees in Tm,K,(q0,q1,...,qK).
Consider the set of binary k-tuples, Pk = {(x1, ..., xk) : x1, ..., xk ∈ B}, of cardinality 2k. As
T ∈ Tm,κ-1,(q0,qι,...,qκ-ι), we know that ∀τ ∈ Pκ-ι, |Wk-i(T；τ)| ≥ 1. This means that
∀τ ∈ Pk, there exists at least one leaf node in T such that the path from the root node to this node
consists of a sequence of nodes with features given by τ. Wecall any such node a node under T. The
total number of the children of all nodes under T is thus m ∙ | W κ-ι (T ； T )| ≥ m. Thus, the total num-
ber of children with feature 0 of all nodes under T is bounded between 0 and m ∙ |W κ-ι (T ； T )|. Con-
versely, for any 2κ-1 -tuple of non-negative integers, (qκ,τ)τ∈Pk-i , which satisfy ∀t ∈ Pκ-ι, 1 ≤
qκ,τ ≤ m ∙ |W κ-ι(T ； t ) |- 1 can be “realized” by at least some depth-K tree T 0 obtained by ap-
pending children to the leaf nodes of T, in the sense that ∀τ = (xi,…，χκ-ι) ∈ Pκ-ι, there is
W κ (T0;(xi,…,xκ-ι,0)) = qκ,τ and WK(T0; (xi, ...,xκ-ι, 1)) = m ∙ |Wκ-ι(T;τ)| - qκ,τ,
and hence T0 ∈ Tm,κ,(qo,...,qκ-ι,qκ), with qκ = ∑τ∈Pk-1 qκ,τ. Because of the requirement that
1 ≤ qκ,τ ≤ m TWκ-i(T; τ)|-1, we further have that ∀τ0 ∈ PK, WK(T0,τ0) ≥ 1, which implies
that T0 ∈ 7m,κ,(qo,..,qκ-ι,qκ). Therefore, for some fixed qκ, in order to lower-bound the cardinal-
ity ofTm,κ,(q0,...,qK) by that of Tm,κ-i,(q0,...,qK-1), itis sufficient to show a lower bound (which
19
Published as a conference paper at ICLR 2021
is uniform for all T ∈ "Tm,κ-ι,(qo,qι,...,qκ-ι)) on the number of 2K-1-tuples, (qκ,τ )τ ∈Pk-i ,which
satisfy
qK =	qK,τ
τ∈PK-1	(30)
∀τ ∈ Pk-1, 1 ≤qκ,τ ≤ m ∙ |Wκ-i(T；τ)| - 1
A simple bound can be obtained in the following way. For every such T, we sort the
2K-1-tuples in Pκ-ι in ascending order of |Wk-i(T; ∙)∣, and define PK-IT to be the
subset of the first 2K-2 of these elements according to this order. Thus, for example,
∀τ ∈ PK-1,T,∀T0 ∈ Pk-1 \ P0K-IT1 there is |WK-I(T; τ)| ≤ |WK-D; τ0)|. As
a consequence, we have PT∈p^- "Wk-i(T; τ)| ≤ PT∈Pκ-1∖Pk7 "Wk-i(T；τ)|,
and so	PT∈PK-1TlWK-I(T；τ)|	≤	1 PT∈pk-1	|wK-I(T；τ)|	=	1 mK-1	≤
Pt∈Pκ-ι∖PK-ι,τ∣WK-I(T； T)|.
Lemma 5. Let K ≥ 2 and qK satisfy (26). Then for all choices of the 2K-2-tuple of integers,
(qK,τ)τ∈P0<τ τ, such that ∀τ ∈ PK-i,t, 7k,t = 1 or 2, we can complete it into at least one
2k-1 -tuple Ofintegers, (qy )τ∈Pk-i, which satisfy (30).
ProofofLemma 5: For any such 2K-2-tuple,(阪,丁)τ∈p^.] t, in order to satisfy the constraints of
(30), it is sufficient to find another 2k-2 integers, (^k,t)t∈Pκ-ι∖Pk.] t, which satisfy
E	<1k,t =qK- E	<ik,t	(31)
T ∈PK-1 \P0K-1,T	T ∈P0K -1,T
∀τ ∈ PK-1 \ pK-1,T, 1 ≤qK,τ ≤ m ∙ 1WK-I(T; τ)| - 1	(32)
On one hand, since qk,t = 1 or 2, ∀τ ∈ PK-i,t, there is qK - 2KT ≤ qK - PT∈p^ ɪ T qk,t ≤
qK - 2K-2 . On the other hand, with the only other constraint being (32), it is possible
to find (OK,τ)τ∈Pκ-1∖PK-1,τ such that PT∈pk- \P0	equals any integer between 2K-2
and m ∙ ∑T∈Pk-i'p'k ɪ T |Wk-i(T；τ)| - 2k-2, and hence any integer between 2k-2 and
m ∙ 1 mK-1 - 2k-2 = 2mκ - 2k-2. Hence, as long as 2k - 2k-2 ≤ qκ ≤ 1 mκ, which is the
assumption of (26), Lemma 5 holds true.
Lemma 5 implies that ∀T ∈ Tm,κ-1,(q0,q1,…,qκ-ι), there are at least 22K 2 distinct choices
of 2K-1-tuples (qK,T)T ∈PK-1 that satisfy the constraint of (30), and hence at least 22K-2 non-
isomorphic trees in Tm,K,(q0,q1,...,qK-1,qK) obtained by appending children to the leaf nodes of T.
This proves the inductive hypothesis. Hence, we have
K
[Tm,K,(q0,q1,...,qκ )| ≥ Y	= 2/-1-1 ,	(33)
k=2
which implies Lemma 4.

Since m ≥ 2 by assumption, 2mκ - (2κ - 2κ-2) grows exponentially in K. This proves Propo-
sition 5.
J	Proof of Proposition 6
Since the number of walks of a particular type that has length at most k is completely determined
by the rooted aggregation tree structure of depth k, it is straightforward to see that all egonets in the
same equivalence class induced by k iterations ofWL (and therefore GNNs of depth k), which yield
the same rooted aggregation tree, will get mapped to the same value by fk .
20
Published as a conference paper at ICLR 2021
Figure 5: A pair of graphs with identical node features, G (left) and G0 (right), which can be distin-
guished by 2 iterations of the WL testbut not by the GA-MLP with Ω ⊆ {Ak }k∈N.
For the second part of the claim pertaining to GA-MLPs, we assume for simplicity that X = B =
{0, 1}, as the extension to the general case is straightforward but demanding heavier notations.
Following the strategy in the proof of Proposition 5, it is sufficient to find exponentially-in-k many
choices of the tuple (q0,q1, ..., qk), with 0 ≤ qk ≤ mk, such that the image of Tm,k,(q0,q1,...,qk) (as
defined in (25)) under fk contains exponentially-in-k many values.
To make it simpler to refer to different nodes in the tree, we index each node in a rooted tree by a
tuple of natural numbers: for example, the index-tuple [1, 3, 2] refers to the node at depth 3 that is
the second children of the third children of the first children of the root. Since there is no intrinsic
ordering to different children of the same node, there exist multiple ways of consistently indexing
the nodes in a rooted tree. However, to specify a tree, it suffices to specify the node features of all
nodes under one such way of indexing.
Given x1, ..., xk ∈ B, we consider a set of depth-k full m-ary trees that satisfy the following:
∀k0 ≤ k - 1 and l1, ..., lk0 ∈ [m], x[l1,l2,...,lk0] = xk0 if l1 = 1 and xk0 if l1 > 1. Note that
these trees satisfy, for k0 ≤ k - 1, qk0 = mk0-1 if xk0 = 0 and qk0 = (m - 1)mk0-1 if xk0 = 1.
Thus, ∀l2, ..., lk ∈ [m], the node [1, l2, ..., lk] is under the path τ = (x1, ..., xk) if and only if
x[1,l2,...,lk] = xk, whereas for l1 > 1, the node [l1, l2, ..., lk] is not under the path τ regardless of the
feature of [1, l2, ..., lk]. Therefore, fk(G[i]) = |Wk(G[i]; (x1, ..., xk))| equals the number of node of
feature xk among the set of mk-1 nodes, {[1, l2, ..., lk]}l2,...,lk∈[m]. Hence, if for k0 ≤ k - 1, we
set qk0 = mk0-1 if xk0 = 0 and qk0 = (m - 1)mk0-1 if xk0 = 1, then choosing any qk between
mk-1 and (m - 1)mk-1, we have that for every integer between 0 and mk-1, there exists a tree T
in Tm,k,(q0,...,qk) such that fk (T) equals this integer. Since there are (m - 2)mk-1 choices of qk
(and therefore the tuple (q0, ..., qk)) and mk-1 + 1 values in the image of Tm,k,(q0,...,qk) under fk,
this proves the proposition.
K	Proof of Proposition 1
We will first prove that the pair of graphs cannot be distinguished by any GA-MLP with Ω ⊆
{Ak }k∈N . Let X and A, X 0 and A0 be the node feature vector and adjacency matrix of the two
graphs, G and G0, respectively. As these two graphs both contain 14 nodes that have identical
features, we have X, X0 ∈ R14×1 both being the all-1 vector. Moreover, ∀i ∈ [14],
(AkX)i =wk(i) , ((A0)k(X0))i = wk0 (i)	(34)
where we use wk(i) and wk0 (i) to denote the numbers of walks (allowing backtracking) of length
k starting from node i in graphs G and G0 , respectively. Thus, to show that any GA-MLP with
Ω ⊆ {Ak}k∈N necessarily returns the same output on G and G0, it is sufficient to show that ∀k ∈
N, AkX = (A0)k(X0), and therefore sufficient to show that ∀k ∈ N and ∀i ∈ [14], there is wk (i) =
wk0 (i). In fact, we will prove the following lemma:
21
Published as a conference paper at ICLR 2021
Lemma 6. ∀k ∈ N,
wk(i) =wk0 (i), ∀i ∈ [14]
wk(1) =wk(2)
wk(3) +wk(9) =wk(6) +wk(8)
wk(5) +wk(7) =wk(4) + wk (10)
(35)
(36)
(37)
(38)
Proof of Lemma 6: We prove this lemma by induction. For the base case, we have that w0 (i) =
w00 (i), ∀i ∈ [14]. Next, we assume that (35) - (38) hold for some k ∈ N and prove it for k + 1. A
first property to note is that ∀k ∈ N, wk+1(i) = Pj ∈N (i) wk(j) and wk0 +1 (i) = Pj∈N0(i) wk0 (j),
where we use N(i) and N0(i) to denote the neighborhood of i in G and G0, respectively.
To show (35) for k + 1, we look at each node separately:
•i=1	wk+1 (1) =wk (3) + wk (5) + wk(7) + wk(9) =wk (5) + wk (6) + wk(7) + wk(8) =wk0 (5) +wk0 (6) +wk0 (7) +wk0 (8)	() =wk+1 (1)
•i=2	wk+1(2) =wk(4) +wk(6) +wk(8) +wk(10) =wk(3) + wk(4) + wk(9) + wk(10) =wk0 (3) + wk0 (4) +wk0 (9) +wk0 (10)	() =wk+1 (2)
•i=3	wk+1 (3) =wk(1) + wk(11) +wk(12) =wk(2) + wk(11) + wk(12) =wk0 (2) +wk0 (11) +wk0 (12)	() =wk+1 (3)
•i=4	wk+1 (4) =wk(2) +wk(13) +wk(14) =wk0 (2) + wk0 (13) + wk0 (14)	(42) =wk0 +1 (4)
• i=5	wk+1(5) =wk(1) +wk(13) =wk0 (1) + wk0 (13)	(43) =wk0 +1 (5)
•i=6	wk+1(6) =wk(2) +wk(11) =wk(1) +wk(11) =wk0 (1) +wk0 (11) =wk0 +1 (6)
•i=7	wk+1(7) =wk(1) +wk(13) =wk0 (1) +wk0 (13)	(45)
wk0 +1 (7)
22
Published as a conference paper at ICLR 2021
•i=8		
	wk+1(8) =wk(2) +wk(12)	
	=wk(1) +wk(12)	
	=wk0 (1) +wk0 (12)	(46)
	=wk0 +1 (8)	
•i=9		
	wk+1 (9) =wk(1)	
	=wk (2)	
	=wk0 (2)	(47)
	=wk0 +1 (9)	
• i=10		
	wk+1 (10) =wk (2)	
	=wk0 (2)	(48)
	=wk0 +1 (10)	
• i ∈ {11,..., 14}		
For each of these i’s, N (i)	N0(i). Therefore,	
wk+1(i) =	wk(j)
j∈N(i)
= X wk0 (j)	(49)
j∈N0(i)
=wk0 +1 (i)
Next, for (36) - (38) at k + 1,
wk+1 (1) =wk (3) +wk(5) +wk(7) +wk(9)
=wk(4) + wk(6) + wk(8) + wk(10)	(50)
=wk+1 (2)
wk+1 (3) + wk+1(9) =2wk(1) + wk(11) +wk(12)
=2wk(2) + wk(11) + wk(12)	(51)
=wk+1 (6) + wk+1 (8)
wk+1 (5) + wk+1(7) =2wk(1) +wk(13) +wk(14)
=2wk(2) + wk(13) + wk(14)	(52)
=wk+1 (4) + wk+1(10)
This proves the inductive hypothethis for k + 1.
We next argue that these two graphs can be distinguished by WL in 2 iterations. This is because 2
iterations of WL distinguish neighborhoods up to the depth-2 rooted aggregation trees (as will be
defined in Section 5), and it is not hard to see that the multiset of depth-2 rooted aggregation trees are
different for the two graphs. Note that a depth-2 rooted subtree can be represented by the multiset
of the degrees of the depth-1 children. Then for example, the depth-2 rooted aggregation trees of
1 and 2 in G are both {3, 2, 2, 1}m, while their rooted aggregation trees in G0 are {2, 2, 2, 2}m and
{3, 3, 1, 1}m, respectively.
L	Experiment Details
L.1 Specific Architectures
In Section 6, we show experiments on several tasks to confirm our theoretical results with several
related architectures. Here are some explanations for them:
23
Published as a conference paper at ICLR 2021
•	GIN: Graph Isomorphism Networks proposed by Xu et al. (2019). In our experiment
of counting attributed walks, we take the depth of GIN as same as the depth of target
walks. The number of hidden dimensions is searched in {8, 16, 32, 64, 256}. The model
is trained with the Adam optimizer (Kingma & Ba, 2014) with learning rate selected from
{0.1, 0.02, 0.01, 0.005, 0.001}. We also train a variant with Jumping Knowledge (Xu et al.,
2018).
•	sGNN: Spectral GNN proposed by Chen et al. (2019b), which can be viewed as a learnable
generalization of power iterations on a collection of operators. While the best performing
variant utilizes the non-backtracking operator on the line graph, for a fairer comparison
with GA-MLPs, we choose a variant with the base collection of operators being {I, A, A2 }
on each layer and depth 60, which then has the same receptive field as the chosen GA-MLP
models. The model is trained with the Adam optimizer with learning rate selected from
{0.001, 0.002, 0.004}.
•	GA-MLP: a multilayer perceptron following graph augmented features. For counting at-
tributed walks, We choose the operators from {I, Ak8 }. The number of hidden dimensions
is searched in {8, 32, 64, 256}. We take the highest order of operators as the twice depth of
target walks at most. For comminity detection, we choose the operators from {I, AQ, Hk }
where H is induced from the Bethe Hessian matrix H . The highest order of operators is
searched in {30, 60, 120}. The number of hidden dimensions is searched in {10, 20}. On
both tasks, the model is trained with the Adam optimizer (Kingma & Ba, 2014) with learn-
ing rate selected from {0.1, 0.02, 0.01, 0.005, 0.001, 0.0001}. Additionally, we use Batch
Normalization (Ioffe & Szegedy, 2015) in community detection after propagating through
each operator, following the normalization strategy from Chen et al. (2019b). We choose
夕 to be the identity function.
L.2 Bethe Hessian
The Bethe Hessian matrix is defined as
H (r) := (r2 - 1)I - rA + D.
with r being a flexible parameter. In SBM, an optimal choice is rc = √c, where C is the average
degree. Spectral clustering (Saade et al., 2014) can be performed by computing the eigenvectors
associated with the negative eigenvalues ofH (rc) to get clustering information in assortative binary
stochastic block model, which is the scenario we consider. In order to utilize power iterations for
eigenvector extraction, we induce a new matrix H as
~
H = KI - H(rc),
so that the smallest eigenvalues of H become the largest eigenvalues of H. We choose K = 8 in our
experiments. For GA-MLP-H, we then let Ω = {I,H,…，HK }.
L.3 RESULTS FOR GA-MLP-A(1) IN COMMUNITY DETECTION
Table 4: Results for community detection on binary SBM by GA-MLP-A(1)
Rank of hardness	1	2	3	4	5
Overlap	0.128	0.164	0.262	0.707	0.563
24