Published as a conference paper at ICLR 2021
Robust Pruning at Initialization
Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet & Yee Whye Teh
Department of Statistics
University of Oxford
United Kingdom
{soufiane.hayou, ton, doucet, teh}@stats.ox.ac.uk
Ab stract
Overparameterized Neural Networks (NN) display state-of-the-art performance.
However, there is a growing need for smaller, energy-efficient, neural networks to
be able to use machine learning applications on devices with limited computational
resources. A popular approach consists of using pruning techniques. While these
techniques have traditionally focused on pruning pre-trained NN (LeCun et al.,
1990; Hassibi et al., 1993), recent work by Lee et al. (2018) has shown promising
results when pruning at initialization. However, for Deep NNs, such procedures
remain unsatisfactory as the resulting pruned networks can be difficult to train
and, for instance, they do not prevent one layer from being fully pruned. In this
paper, we provide a comprehensive theoretical analysis of Magnitude and Gradient
based pruning at initialization and training of sparse architectures. This allows
us to propose novel principled approaches which we validate experimentally on a
variety of NN architectures.
1 Introduction
Overparameterized deep NNs have achieved state of the art (SOTA) performance in many tasks
(Nguyen and Hein, 2018; Du et al., 2019; Zhang et al., 2016; Neyshabur et al., 2019). However,
it is impractical to implement such models on small devices such as mobile phones. To address
this problem, network pruning is widely used to reduce the time and space requirements both at
training and test time. The main idea is to identify weights that do not contribute significantly to
the model performance based on some criterion, and remove them from the NN. However, most
pruning procedures currently available can only be applied after having trained the full NN (LeCun
et al., 1990; Hassibi et al., 1993; Mozer and Smolensky, 1989; Dong et al., 2017) although methods
that consider pruning the NN during training have become available. For example, Louizos et al.
(2018) propose an algorithm which adds a L0 regularization on the weights to enforce sparsity while
Carreira-Perpinan and IdelbayeV (2018); Alvarez and Salzmann (2017); Li et al. (2020) propose the
inclusion of compression inside training steps. Other pruning variants consider training a secondary
network that learns a pruning mask for a given architecture (Li et al. (2020); Liu et al. (2019)).
Recently, Frankle and Carbin (2019) have introduced and validated experimentally the Lottery
Ticket Hypothesis which conjectures the existence of a sparse subnetwork that achieves similar
performance to the original NN. These empirical findings have motivated the development of pruning
at initialization such as SNIP (Lee et al. (2018)) which demonstrated similar performance to classical
pruning methods of pruning-after-training. Importantly, pruning at initialization never requires
training the complete NN and is thus more memory efficient, allowing to train deep NN using limited
computational resources. However, such techniques may suffer from different problems. In particular,
nothing prevents such methods from pruning one whole layer of the NN, making it untrainable. More
generally, it is typically difficult to train the resulting pruned NN (Li et al., 2018). To solve this
situation, Lee et al. (2020) try to tackle this issue by enforcing dynamical isometry using orthogonal
weights, while Wang et al. (2020) (GraSP) uses Hessian based pruning to preserve gradient flow.
Other work by Tanaka et al. (2020) considers a data-agnostic iterative approach using the concept of
synaptic flow in order to avoid the layer-collapse phenomenon (pruning a whole layer). In our work,
we use principled scaling and re-parameterization to solve this issue, and show numerically that our
algorithm achieves SOTA performance on CIFAR10, CIFAR100, TinyImageNet and ImageNet in
some scenarios and remains competitive in others.
1
Published as a conference paper at ICLR 2021
Table 1: Classification accuracies on CIFAR10 for Resnet with varying depths and sparsities using SNIP (Lee et al. (2018)) and our algorithm SBP-SR						
	ALGORITHM	90%	95%	98%	99.5%	99.9%
ResNet32	SNIP	92.26 ± 0.32	91.18 ± 0.17	87.78 ± 0.16	77.56±0.36	9.98±0.08
	SBP-SR	92.56 ± 0.06	91.21 ± 0.30	88.25 ± 0.35	79.54±1.12	51.56±1.12
ResNet50	SNIP	91.95 ± 0.13	92.12 ± 0.34	89.26 ± 0.23	80.49±2.41	19.98±14.12
	SBP-SR	92.05 ± 0.06	92.74± 0.32	89.57 ± 0.21	82.68±0.52	58.76±1.82
ResNet 104	SNIP	93.25 ± 0.53	92.98 ± 0.12	91.58 ± 0.19	33.63±33.27	10.11±0.09
	SBP-SR	94.69 ± 0.13	93.88 ± 0.17	92.08 ± 0.14	87.47±0.23	72.70±0.48
In this paper, we provide novel algorithms for Sensitivity-Based Pruning (SBP), i.e. pruning schemes
that prune a weight W based on the magnitude of |W募 | at initialization where L is the loss.
Experimentally, compared to other available one-shot pruning schemes, these algorithms provide
state-of the-art results (this might not be true in some regimes). Our work is motivated by a new
theoretical analysis of gradient back-propagation relying on the mean-field approximation of deep
NN (Hayou et al., 2019; Schoenholz et al., 2017; Poole et al., 2016; Yang and Schoenholz, 2017;
Xiao et al., 2018; Lee et al., 2018; Matthews et al., 2018). Our contribution is threefold:
•	For deep fully connected FeedForward NN (FFNN) and Convolutional NN (CNN), it has been
previously shown that only an initialization on the so-called Edge of Chaos (EOC) make models
trainable; see e.g. (Schoenholz et al., 2017; Hayou et al., 2019). For such models, we show that an
EOC initialization is also necessary for SBP to be efficient. Outside this regime, one layer can be
fully pruned.
•	For these models, pruning pushes the NN out of the EOC making the resulting pruned model
difficult to train. We introduce a simple rescaling trick to bring the pruned model back in the EOC
regime, making the pruned NN easily trainable.
•	Unlike FFNN and CNN, we show that Resnets are better suited for pruning at initialization
since they ‘live’ on the EOC by default (Yang and Schoenholz, 2017). However, they can suffer
from exploding gradients, which we resolve by introducing a re-parameterization, called ‘Stable
Resnet’ (SR). The performance of the resulting SBP-SR pruning algorithm is illustrated in Table
1: SBP-SR allows for pruning up to 99.5% of ResNet104 on CIFAR10 while still retaining around
87% test accuracy.
The precise statements and proofs of the theoretical results are given in the Supplementary. Appendix
H also includes the proof of a weak version of the Lottery Ticket Hypothesis (Frankle and Carbin,
2019) showing that, starting from a randomly initialized NN, there exists a subnetwork initialized on
the EOC.
2 Sensitivity Pruning for FFNN/CNN and the Rescaling Trick
2.1	Setup and notations
Let x be an input in Rd . A NN of depth L is defined by
yl (x) = Fl(W l, yl-1(x)) + Bl,	1 ≤l ≤L,	(1)
where yl (x) is the vector of pre-activations, Wl and Bl are respectively the weights and bias of
the lth layer and Fl is a mapping that defines the nature of the layer. The weights and bias are
initialized with Wl 尢 N(0, σW/vι), where Vl is a scaling factor used to control the variance of yl,
and Bl iid N(0, σ2). Hereafter, Ml denotes the number of weights in the lth layer, φ the activation
function and [m : n] := {m, m + 1, ..., n} for m ≤ n. Two examples of such architectures are:
•	Fully connected FFNN. For a FFNN of depth L and widths (Nl)0≤l≤L, we have vl = Nl-1,
Ml = Nl-1Nl and
d	Nl-1
yi1(x)=XWi1jxj+Bi1,	yil(x) = X Wiljφ(yjl-1(x))+Bil forl≥2.	(2)
j=1	j=1
2
Published as a conference paper at ICLR 2021
•	CNN. For a 1D CNN of depth L, number of channels (nl)l≤L, and number of neurons per channel
(Nl)l≤L, we have
nl-1	nl-1
yi1,α(x) = X X Wi1,j,βxj,α+β + bi1, yil,α(x) = X X Wil,j,βφ(yjl-,α1+β(x))+bli, forl ≥ 2,
j=1 β∈kerl	j=1 β∈kerl
(3)
where i ∈ [1 : nl] is the channel index, α ∈ [0 : Nl -1] is the neuron location, kerl = [-kl : kl] is the
filter range, and 2kl + 1 is the filter size. To simplify the analysis, we assume hereafter that Nl = N
and kl = k for all l. Here, we have vl = nl-1 (2k + 1) and Ml = nl-1nl(2k + 1). We assume
periodic boundary conditions; so yil,α = yil,α+N = yil,α-N. Generalization to multidimensional
convolutions is straightforward.
When no specific architecture is mentioned, (Wil)1≤i≤Ml denotes the weights of the lth layer. In
practice, a pruning algorithm creates a binary mask δ over the weights to force the pruned weights to
be zero. The neural network after pruning is given by
yl(x) = Fl(δl ◦ W l, yl-1(x)) + Bl,	(4)
where ◦ is the Hadamard (i.e. element-wise) product. In this paper, we focus on pruning at
initialization. The mask is typically created by using a vector gl of the same dimension as Wl using
a mapping of choice (see below), we then prune the network by keeping the weights that correspond
to the top k values in the sequence (gil)i,l where k is fixed by the sparsity that we want to achieve.
There are three popular types of criteria in the literature :
•	Magnitude based pruning (MBP): We prune weights based on the magnitude |W|.
•	Sensitivity based pruning (SBP): We prune the weights based on the values of | W 募 | where L
is the loss. This is motivated by LW ≈ LW =0 + W 繇 used in SNIP (Lee et al. (2018)).
•	Hessian based pruning (HBP): We prune the weights based on some function that uses the Hessian
of the loss function as in GraSP (Wang et al., 2020).
In the remainder of the paper, we focus exclusively on SBP while our analysis of MBP is given in
Appendix E. We leave HBP for future work. However, we include empirical results with GraSP
(Wang et al., 2020) in Section 4.
Hereafter, we denote by s the sparsity, i.e. the fraction of weights we want to prune. Let Al be the set
of indices of the weights in the lth layer that are pruned, i.e. Al = {i ∈ [1 : Ml], s.t. δil = 0}. We
define the critical sparsity scr by
scr = min{s ∈ (0, 1), s.t. ∃l, |Al| = Ml},
where |Al| is the cardinality of Al. Intuitively, scr represents the maximal sparsity we are allowed to
choose without fully pruning at least one layer. scr is random as the weights are initialized randomly.
Thus, we study the behaviour of the expected value E[scr] where, hereafter, all expectations are
taken w.r.t. to the random initial weights. This provides theoretical guidelines for pruning at
initialization.
For all l ∈ [1 : L], we define αl by vl = αlN where N > 0, and ζl > 0 such that Ml = ζlN 2, where
we recall that vl is a scaling factor controlling the variance of yl and Ml is the number of weights
in the lth layer. This notation assumes that, in each layer, the number of weights is quadratic in the
number of neurons, which is satisfied by classical FFNN and CNN architectures.
2.2	Sensitivity-Based Pruning (SBP)
SBP is a data-dependent pruning method that uses the data to compute the gradient with backpropa-
gation at initialization (one-shot pruning).We randomly sample a batch and compute the gradients
of the loss with respect to each weight. The mask is then defined by δi = I(∣Wl ∂w | ≥ ts), where
i
ts = |W ∂∣L |(ks) and k§ = (1 - S) Pl Ml and |W IWL |(ks) is the kth order statistics of the sequence
(IWldWLl l)i≤l≤L,1≤i≤Mι.
i
However, this simple approach suffers from the well-known exploding/vanishing gradients problem
which renders the first/last few layers respectively susceptible to be completely pruned. We give a
formal definition to this problem.
3
Published as a conference paper at ICLR 2021
Definition 1 (Well-conditioned & ill-conditioned NN). Let mi = E[∣Wl∂∂Lι|2] for l ∈ [1 : L]. We
say that the NN is well-conditioned if there exist A, B > 0 such that for all L ≥ 1 and l ∈ [1 : L] we
have A ≤ mi/mL ≤ B, and it is ill-conditioned otherwise.
Understanding the behaviour of gradients at initialization is thus crucial for SBP to be efficient. Using
a mean-field approach, such analysis has been carried out in (Schoenholz et al., 2017; Hayou et al.,
2019; Xiao et al., 2018; Poole et al., 2016; Yang, 2019) where it has been shown that an initialization
known as the EOC is beneficial for DNN training. The mean-field analysis of DNNs relies on two
standard approximations that we will also use here.
Approximation 1 (Mean-Field Approximation). When Ni 1 for FFNN or ni 1 for CNN, we
use the approximation of infinitely wide NN. This means infinite number of neurons per layer for fully
connected layers and infinite number of channels per layer for convolutional layers.
Approximation 2 (Gradient Independence). The weights used for forward propagation are indepen-
dent from those used for back-propagation.
These two approximations are ubiquitous in literature on the mean-field analysis of neural networks.
They have been used to derive theoretical results on signal propagation (Schoenholz et al., 2017;
Hayou et al., 2019; Poole et al., 2016; Yang, 2019; Yang and Schoenholz, 2017; Yang et al., 2019)
and are also key tools in the derivation of the Neural Tangent Kernel (Jacot et al., 2018; Arora et al.,
2019; Hayou et al., 2020). Approximation 1 simplifies the analysis of the forward propagation as it
allows the derivation of closed-form formulas for covariance propagation. Approximation 2 does
the same for back-propagation. See Appendix A for a detailed discussion of these approximations.
Throughout the paper, we provide numerical results that substantiate the theoretical results that we
derive using these two approximations. We show that these approximations lead to excellent match
between theoretical results and numerical experiments.
Edge of Chaos (EOC): For inputs x, x0, let ci (x, x0) be the correlation between yi (x) and yi (x0).
From (Schoenholz et al., 2017; Hayou et al., 2019), there exists a so-called correlation function f
that depends on (σw, σb) such that ci+1(x, x0) = f(ci(x, x0)). Let χ(σb, σw) = f0(1). The EOC
is the set of hyperparameters (σw, σb) satisfying χ(σb, σw) = 1. When χ(σb, σw) > 1, we are in
the Chaotic phase, the gradient explodes and ci(x, x0) converges exponentially to some c < 1 for
x 6= x0 and the resulting output function is discontinuous everywhere. When χ(σb, σw) < 1, we are
in the Ordered phase where ci(x, x0) converges exponentially fast to 1 and the NN outputs constant
functions. Initialization on the EOC allows for better information propagation (see Supplementary
for more details).
Hence, by leveraging the above results, we show that an initialization outside the EOC will lead to an
ill-conditioned NN.
Theorem 1 (EOC Initialization is crucial for SBP). Consider a NN of type (2) or (3) (FFNN or CNN).
Assume (σw, σb) are chosen on the ordered phase, i.e. χ(σb, σw) < 1, then the NN is ill-conditioned.
Moreover, we have
≤ L(1 + —)+ O (▲),
where K = | logχ(σb,σw)|/8. If (σw,σb) are on the EOC, i.e. χ(σb,σw) = 1, then the NN is
well-conditioned. In this case, κ = 0 and the above upper bound no longer holds.
The proof of Theorem 1 relies on the behaviour of the gradient norm at initialization. On the ordered
phase, the gradient norm vanishes exponentially quickly as it back-propagates, thus resulting in an
ill-conditioned network. We use another approximation for the sake of simplification of the proof
(Approximation 3 in the Supplementary) but the result holds without this approximation although
the resulting constants would be a bit different. Theorem 1 shows that the upper bound decreases
the farther χ(σb, σw) is from 1, i.e. the farther the initialization is from the EOC. For constant width
FFNN with L = 100, N = 100 andκ = 0.2, the theoretical upper bound is E[scr] / 27% while
we obtain E[scr] ≈ 22% based on 10 simulations. A similar result can be obtained when the NN is
initialized on the chaotic phase; in this case too, the NN is ill-conditioned. To illustrate these results,
Figure 1 shows the impact of the initialization with sparsity s = 70%. The dark area in Figure 1(b)
corresponds to layers that are fully pruned in the chaotic phase due to exploding gradients. Using an
EOC initialization, Figure 1(a) shows that pruned weights are well distributed in the NN, ensuring
that no layer is fully pruned.
4
Published as a conference paper at ICLR 2021
(a) Edge of Chaos	(b) Chaotic phase
Figure 1: Percentage of weights kept after SBP applied to a randomly initialized FFNN with depth 100 and
width 100 for 70% sparsity on MNIST. Each pixel (i, j) corresponds to a neuron and shows the proportion of
connections to neuron (i, j) that have not been pruned. The EOC (a) allows us to preserve a uniform spread of
the weights, whereas the Chaotic phase (b), due to exploding gradients, prunes entire layers.
2.3	Training Pruned Networks Using the Rescaling Trick
We have shown previously that an initialization on the EOC is crucial for SBP. However, we have not
yet addressed the key problem of training the resulting pruned NN. This can be very challenging in
practice (Li et al., 2018), especially for deep NN.
Consider as an example a FFNN architecture. After pruning, we have for an input x
yi(x) = PN-IWijδjφ(yj-1(x)) + Bi, for l ≥2,	(5)
where δ is the pruning mask. While the original NN initialized on the EOC was satisfy-
ing cl+1(x,x0) = f(cl(x,x0)) for f0(1) = χ(σb, σw ) = 1, the pruned architecture leads to
^l+1(x, x0) = fpruned(cl(x, x0)) With f；runed(1) = 1, hence pruning destroys the EOC. Consequently,
the pruned NN will be difficult to train (Schoenholz et al., 2017; Hayou et al., 2019) especially if
it is deep. Hence, We propose to bring the pruned NN back on the EOC. This approach consists
of rescaling the Weights obtained after SBP in each layer by factors that depend on the pruned
architecture itself.
Proposition 1 (Rescaling Trick). Consider a NN of type (2) or (3) (FFNN or CNN) initialized on
the EOC. Then, after pruning, the pruned NN is not initialized on the EOC anymore. However, the
rescaled pruned NN
yl(x) = F(ρl ◦ δl ◦ Wl,yl-1(x)) + Bl, forl ≥ 1,	(6)
where
Plij = (E[Ni-i(Wiι)2δiι])-2 for FFNN, PIi,j,β = (E[ni-i(Wl,ι,β)2%,产])-1 for CNN,	(7)
is initialized on the EOC. (The scaling is constant across j).
The scaling factors in equation 7 are easily approximated using the Weights kept after pruning.
Algorithm 1 (see Appendix I) details a practical implementation of this rescaling technique for FFNN.
We illustrate experimentally the benefits of this approach in Section 4.
3	Sensitivity-Based Pruning for Stable Residual Networks
Resnets and their variants (He et al., 2015; Huang et al., 2017) are currently the best performing
models on various classification tasks (CIFAR10, CIFAR100, ImageNet etc (Kolesnikov et al., 2019)).
Thus, understanding Resnet pruning at initialization is of crucial interest. Yang and Schoenholz
(2017) shoWed that Resnets naturally ‘live’ on the EOC. Using this result, We shoW that Resnets
are actually better suited to SBP than FFNN and CNN. HoWever, Resnets suffer from an exploding
gradient problem (Yang and Schoenholz, 2017) Which might affect the performance of SBP. We
5
Published as a conference paper at ICLR 2021
Figure 2: Percentage of non-pruned weights per layer in a ResNet32 for our Stable ResNet32 and standard
Resnet32 with Kaiming initialization on CIFAR10. With Stable Resnet, we prune less aggressively weights in
the deeper layers than for standard Resnet.
address this issue by introducing a new Resnet parameterization. Let a standard Resnet architecture
be given by
y1(x) = F(W1, x), yl (x) = yl-1(x) + F(Wl, yl-1), for l ≥ 2,	(8)
where F defines the blocks of the Resnet. Hereafter, we assume that F is either of the form (2) or (3)
(FFNN or CNN).
The next theorem shows that Resnets are well-conditioned independently from the initialization and
are thus well suited for pruning at initialization.
Theorem 2 (Resnet are Well-Conditioned). Consider a Resnet with either Fully Connected or
Convolutional layers and ReLU activation function. Then for all σw > 0, the Resnet is well-
conditioned. Moreover, for all l ∈ {1,..., L}, we have ml = Θ((1 + σw)L).
The above theorem proves that Resnets are always well-conditioned. However, taking a closer look
at ml, which represents the variance of the pruning criterion (Definition 1), we see that it grows
exponentially in the number of layers L. Therefore, this could lead to a ‘higher variance of pruned
networks’ and hence high variance test accuracy. To this end, we propose a Resnet parameterization
which we call Stable Resnet. Stable Resnets prevent the second moment from growing exponentially
as shown below.
Proposition 2 (Stable Resnet). Consider the following Resnet parameterization
yl (χ) = yl-1(χ) + √L F (W l,yl-1), for l ≥ 2,	(9)
then the NN is well-conditioned for all σw > 0. Moreover, for all l ≤ L we have ml = Θ(L-1).
In Proposition 2, L is not the number of layers but the number of blocks. For example, ResNet32
has 15 blocks and 32 layers, hence L = 15. Figure 2 shows the percentage of weights in each layer
kept after pruning ResNet32 and Stable ResNet32 at initialization. The jumps correspond to limits
between sections in ResNet32 and are caused by max-pooling. Within each section, Stable Resnet
tends to have a more uniform distribution of percentages of weights kept after pruning compared
to standard Resnet. In Section 4 we show that this leads to better performance of Stable Resnet
compared to standard Resnet. Further theoretical and experimental results for Stable Resnets are
presented in (Hayou et al., 2021).
In the next proposition, we establish that, unlike FFNN or CNN, we do not need to rescale the pruned
Resnet for it to be trainable as it lives naturally on the EOC before and after pruning.
Proposition 3 (Resnet live on the EOC even after pruning). Consider a Residual NN with blocks of
type FFNN or CNN. Then, after pruning, the pruned Residual NN is initialized on the EOC.
4	Experiments
In this section, we illustrate empirically the theoretical results obtained in the previous sections. We
validate the results on MNIST, CIFAR10, CIFAR100 and Tiny ImageNet.
4.1	Initialization and rescaling
According to Theorem 1, an EOC initialization is necessary for the network to be well-conditioned.
We train FFNN with tanh activation on MNIST, varying depth L ∈ {2, 20, 40, 60, 80, 100} and
6
Published as a conference paper at ICLR 2021
(a) EOC Init & Rescaling
(b) EOC Init
(c) Ordered phase Init
Figure 3: Accuracy on MNIST with different initialization schemes including EOC with rescaling, EOC without
rescaling, Ordered phase, with varying depth and sparsity. This shows that rescaling to be on the EOC allows us
to train not only much deeper but also sparser models.
sparsity s ∈ {10%, 20%, .., 90%}. We use SGD with batchsize 100 and learning rate 10-3, which
we found to be optimal using a grid search with an exponential scale of 10. Figure 3 shows the test
accuracy after 10k iterations for 3 different initialization schemes: Rescaled EOC, EOC, Ordered.
On the Ordered phase, the model is untrainable when we choose sparsity s > 40% and depth L > 60
as one layer being fully pruned. For an EOC initialization, the set (s, L) for which NN are trainable
becomes larger. However, the model is still untrainable for highly sparse deep networks as the sparse
NN is no longer initialized on the EOC (see Proposition 1). As predicted by Proposition 1, after
application of the rescaling trick to bring back the pruned NN on the EOC, the pruned NN can be
trained appropriately.
Table 2: Classification accuracies for CIFAR10 and CIFAR100 after pruning
Sparsity	CIFAR10				CIFAR100	
	90%	95%	98%	90%	95%	98%
ResNet32 (NO PRUNING)	94.80	-	-	74.64	-	-
OBD LeCun et al. (1990)	93.74	93.58	93.49	73.83	71.98	67.79
Random Pruning	89.95±0.23	89.68±0.15	86.13±0.25	63. 13±2.94	64.55±0.32	19.83±3.21
MBP	90.21±0.55	88.35±0.75	86.83±0.27	67.07±0.31	64.92±0.77	59.53±2.19
SNIP LEE ET AL. (2018)	92.26 ± 0.32	91.18 ± 0.17	87.78 ± 0.16	69.31 ± 0.52	65.63 ± 0.15	55.70 ± 1.13
GRASP WANG ET AL. (2020)	92.20±0.31	91.39±0.25	88.70±0.42	69.24 ± 0.24	66.50 ± 0.11	58.43 ± 0.43
GRASP-SR	92.30±0.19	91.16±0.13	87.8 ± 0.32	69.12 ± 0.15	65.49 ± 0.21	58.63 ± 0.23
SynFlow Tanaka et al. (2020)	92.01±0.22	91.67±0.17	88.10 ± 0.25	69.03 ± 0.20	65.23 ± 0.31	58.73 ± 0.30
SBP-SR (STABLE RESNET)	92.56 ± 0.06	91.21 ± 0.30	88.25 ± 0.35	69.51 ± 0.21	66.72 ± 0.12	59.51 ± 0.15
ResNet50 (NO PRUNING)	94.90	-	-	74.9	-	-
Random Pruning	85.11±4.51	88.76±0.21	85.32±0.47	65.67±0.57	60.23±2.21	28.32±10.35
MBP	90.11 ± 0.32	89.06 ± 0.09	87.32 ± 0.16	68.51 ± 0.21	63.32 ± 1.32	55.21 ± 0.35
SNIP	91.95 ± 0.13	92.12 ± 0.34	89.26 ± 0.23	70.43 ± 0.43	67.85 ± 1.02	60.38 ± 0.78
GRASP	92.10 ± 0.21	91.74 ± 0.35	89.97± 0.25	70.53±0.32	67.84±0.25	63.88±0.45
SynFlow	92.05 ± 0.20	91.83 ± 0.23	89.61± 0.17	70.43±0.30	67.95±0.22	63.95±0. 1 1
SBP-SR	92.05 ± 0.06	92.74± 0.32	89.57 ± 0.21	71.79 ± 0.13	68.98 ± 0.15	64.45 ± 0.34
ResNet104 (NO PRUNING)	94.92	-	-	75.24	-	-
Random Pruning	89.80±0.33	87.86±1.22	85.52±2. 12	66.73±1.32	64.98±0. 1 1	30.31±4.51
MBP	90.05 ± 1.23	88.95±0.65	87.83±1.21	69.57±0.35	64.31±0.78	60.21±2.41
SNIP	93.25 ± 0.53	92.98 ± 0.12	91.58 ± 0.19	71.94 ± 0.22	68.73±0.09	63.31 ± 0.41
GRASP	93.08 ± 0.17	92.93 ± 0.09	91.19±0.35	73.33±0.21	70.95 ± 1.12	66.91±0.33
SynFlow	93.43 ± 0.10	92.85 ± 0.18	91.03±0.25	72.85±0.20	70.33 ± 0.15	67.02±0. 10
SBP-SR	94.69 ± 0.13	93.88 ± 0.17	92.08 ± 0.14	74.17 ± 0.11	71.84 ± 0.13	67.73 ± 0.28
4.2	Resnet and Stable Resnet
Although Resnets are adapted to SBP (i.e. they are always well-conditioned for all σw > 0), Theorem
2 shows that the magnitude of the pruning criterion grows exponentially w.r.t. the depth L. To resolve
this problem we introduced Stable Resnet. We call our pruning algorithm for ResNet SBP-SR (SBP
with Stable Resnet). Theoretically, we expect SBP-SR to perform better than other methods for
deep Resnets according to Proposition 2. Table 2 shows test accuracies for ResNet32, ResNet50 and
ResNet104 with varying sparsities s ∈ {90%, 95%, 98%} on CIFAR10 and CIFAR100. For all our
experiments, we use a setup similar to (Wang et al., 2020), i.e. we use SGD for 160 and 250 epochs
for CIFAR10 and CIFAR100, respectively. We use an initial learning rate of 0.1 and decay it by 0.1
7
Published as a conference paper at ICLR 2021
at 1/2 and 3/4 of the number of total epoch. In addition, we run all our experiments 3 times to obtain
more stable and reliable test accuracies. As in (Wang et al., 2020), we adopt Resnet architectures
where we doubled the number of filters in each convolutional layer. As a baseline, we include pruning
results with the classical OBD pruning algorithm (LeCun et al., 1990) for ResNet32 (train → prune
→ repeat). We compare our results against other algorithms that prune at initialization, such as
SNIP (Lee et al., 2018), which is a SBP algorithm, GraSP (Wang et al., 2020) which is a Hessian
based pruning algorithm, and SynFlow (Tanaka et al., 2020), which is an iterative data-agnostic
pruning algorithm. As we increase the depth, SBP-SR starts to outperform other algorithms that
prune at initialization (SBP-SR outperforms all other algorithms with ResNet104 on CIFAR10 and
CIFAR100). Furthermore, using GraSP on Stable Resnet did not improve the result of GraSP on
standard Resnet, as our proposed Stable Resnet analysis only applies to gradient based pruning. The
analysis of Hessian based pruning could lead to similar techniques for improving trainability, which
we leave for future work.
Table 3: Classification accuracies on Tiny ImageNet for Resnet with varying depths
	ALGORITHM	85%	90%	95%
ResNet32	SBP-SR	57.25 ± 0.09	55.67 ± 0.21	50.63±0.21
	SNIP	56.92± 0.33	54.99±0.37	49.48±0.48
	GRASP	57.25±0.11	55.53±0.11	51.34±0.29
	S ynFlow	56.75±0.09	55.60±0.07	51.50±0.21
ResNet50	SBP-SR	59.8±0.18	57.74±0.06	53.97±0.27
	SNIP	58.91±0.23	56.15±0.31	51.19±0.47
	GRASP	58.46±0.29	57.48±0.35	52.5±0.41
	S ynFlow	59.31±0.17	57.67±0.15	53.14±0.31
ResNet104	SBP-SR	62.84±0.13	61.96±0.11	57.9±0.31
	SNIP	59.94±0.34	58.14±0.28	54.9±0.42
	GRASP	61.1±0.41	60.14±0.38	56.36±0.51
	S ynFlow	61.71±0.08	60.81±0.14	55.91±0.43
To confirm these results, we also test SBP-SR against other pruning algorithms on Tiny ImageNet.
We train the models for 300 training epochs to make sure all algorithms converge. Table 3 shows test
accuracies for SBP-SR, SNIP, GraSP, and SynFlow for s ∈ {85%, 90%, 95%}. Although SynFlow
competes or outperforms GraSP in many cases, SBP-SR has a clear advantage over SynFlow and
other algorithms, especially for deep networks as illustrated on ResNet104.
Additional results with ImageNet dataset are provided in Appendix F.
4.3	ReScaling Trick and CNNs
The theoretical analysis of Section 2 is valid for Vanilla CNN i.e. CNN without pooling layers. With
pooling layers, the theory of signal propagation applies to sections between successive pooling layers;
each of those section can be seen as Vanilla CNN. This applies to standard CNN architectures such
as VGG. As a toy example, we show in Table 4 the test accuracy of a pruned V-CNN with sparsity
s = 50% on MNIST dataset. Similar to FFNN results in Figure 3, the combination of the EOC Init
and the ReScaling trick allows for pruning deep V-CNN (depth 100) while ensuring their trainability.
Table 4: Test accuracy on MNIST with V-CNN for different depths with sparsity 50% using SBP(SNIP)
	L = 10	L = 50	L=100
Ordered phase Init	98.12±0.13	10.00±0.0	10.00±0.0
EOC INIT	98.20±0.17	98.75±0.1 1	10.00±0.0
EOC + ReScaling	98.18±0.21	98.90±0.07	99.15±0.08
However, V-CNN is a toy example that is generally not used in practice. Standard CNN architectures
such as VGG are popular among practitioners since they achieve SOTA accuracy on many tasks.
Table 5 shows test accuracies for SNIP, SynFlow, and our EOC+ReScaling trick for VGG16 on
CIFAR10. Our results are close to the results presented by Frankle et al. (2020). These three
8
Published as a conference paper at ICLR 2021
algorithms perform similarly. From a theoretical point of view, our ReScaling trick applies to vanilla
CNNs without pooling layers, hence, adding pooling layers might cause a deterioration. However,
we know that the signal propagation theory applies to vanilla blocks inside VGG (i.e. the sequence
of convolutional layers between two successive pooling layers). The larger those vanilla blocks are,
the better our ReScaling trick performs. We leverage this observation by training a modified version
of VGG, called 3xVGG16, which has the same number of pooling layers as VGG16, and 3 times
the number of convolutional layers inside each vanilla block. Numerical results in Table 5 show that
the EOC initialization with the ReScaling trick outperforms other algorithms, which confirms our
hypothesis. However, the architecture 3xVGG16 is not a standard architecture and it does not seem
to improve much the test accuracy of VGG16. An adaptation of the ReScaling trick to standard VGG
architectures would be of great value and is left for future work.
Table 5: Classification accuracy on CIFAR10 for VGG16 and 3xVGG16 with varying sparsities
	ALGORITHM	85%	90%	95%
VGG16	SNIP	93.09±0.11	92.97±0.08	92.61±0.10
	SYNFLOW	93.21±0.13	93.05±0.11	92. 19±0. 12
	EOC + RESCALING	93.15±0.12	92.90±0.15	92.70±0.06
3XVGG16	SNIP	93.30±0.10	93.12±0.20	92.85±0. 15
	SYNFLOW	92.95±0.13	92.91±0.21	92.70±0.20
	EOC + RESCALING	93.97±0.17	93.75±0.15	93.40±0.16
Summary of numerical results. We summarize in Table 6 our numerical results. The letter ‘C’
refers to ‘Competition’ between algorithms in that setting, and indicates no clear winner is found,
while the dash means no experiment has been run with this setting. We observe that our algorithm
SBP-SR consistently outperforms other algorithms in a variety of settings.
Table 6: Which algorithm performs better? (according to our results)
DATASET	ARCHITECTURE	85%	90%	95%	98%
CIFAR1 0	RESNET32	-	C	C	GRASP
	RESNET50	-	C	SBP-SR	GRASP
	RESNET104	-	SBP-SR	SBP-SR	SBP-SR
	VGG16	C	C	C	-
	3XVGG16	EOC+RESC	EOC+RESC	EOC+RESC	-
CIFAR1 00	RESNET32	-	SBP-SR	SBP-SR	SBP-SR
	RESNET50	-	SBP-SR	SBP-SR	SBP-SR
	RESNET104	-	SBP-SR	SBP-SR	SBP-SR
TINY IMAGENET	RESNET32	C	C	SYNFLOW	-
	RESNET50	SBP-SR	C	SBP-SR	-
	RESNET104	SBP-SR	SBP-SR	SBP-SR	-
5	Conclusion
In this paper, we have formulated principled guidelines for SBP at initialization. For FNNN and CNN,
we have shown that an initialization on the EOC is necessary followed by the application of a simple
rescaling trick to train the pruned network. For Resnets, the situation is markedly different. There
is no need for a specific initialization but Resnets in their original form suffer from an exploding
gradient problem. We propose an alternative Resnet parameterization called Stable Resnet, which
allows for more stable pruning. Our theoretical results have been validated by extensive experiments
on MNIST, CIFAR10, CIFAR100, Tiny ImageNet and ImageNet. Compared to other available
one-shot pruning algorithms, we achieve state-of the-art results in many scenarios.
9
Published as a conference paper at ICLR 2021
References
Alvarez, J. M. and M. Salzmann (2017). Compression-aware training of deep networks. In 31st
Conference in Neural Information Processing Systems, pp. 856-867.
Arora, S., S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang (2019). On exact computation with an
infinitely wide neural net. In 33rd Conference on Neural Information Processing Systems.
Carreira-Perpiiian, M. and Y. Idelbayev (2018, June). Learning-compression algorithms for neural
net pruning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Dong, X., S. Chen, and S. Pan (2017). Learning to prune deep neural networks via layer-wise optimal
brain surgeon. In 31st Conference on Neural Information Processing Systems, pp. 4860-4874.
Du, S., X. Zhai, B. Poczos, and A. Singh (2019). Gradient descent provably optimizes over-
parameterized neural networks. In 7th International Conference on Learning Representations.
Frankle, J. and M. Carbin (2019). The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In 7th International Conference on Learning Representations.
Frankle, J., G. Dziugaite, D. Roy, and M. Carbin (2020). Pruning neural networks at initialization:
Why are we missing the mark? arXiv preprint arXiv:2009.08576.
Hardy, G., J. Littlewood, and G. Polya (1952). Inequalities, Volume 2. Cambridge Mathematical
Library.
Hassibi, B., D. Stork, and W. Gregory (1993). Optimal brain surgeon and general network pruning.
In IEEE International Conference on Neural Networks, pp. 293 - 299 vol.1.
Hayou, S., E. Clerico, B. He, G. Deligiannidis, A. Doucet, and J. Rousseau (2021). Stable resnet. In
24th International Conference on Artificial Intelligence and Statistics.
Hayou, S., A. Doucet, and J. Rousseau (2019). On the impact of the activation function on deep
neural networks training. In 36th International Conference on Machine Learning.
Hayou, S., A. Doucet, and J. Rousseau (2020). Mean-field behaviour of neural tangent kernel for
deep neural networks. arXiv preprint arXiv:1905.13654.
He, K., X. Zhang, S. Ren, and J. Sun (2015). Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778.
Huang, G., Z. Liu, L. Maaten, and K. Weinberger (2017). Densely connected convolutional networks.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261-2269.
Jacot, A., F. Gabriel, and C. Hongler (2018). Neural tangent kernel: Convergence and generalization
in neural networks. In 32nd Conference on Neural Information Processing Systems.
Kolesnikov, A., L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby (2019). Large
scale learning of general visual representations for transfer. arXiv preprint arXiv:1912.11370.
LeCun, Y., J. Denker, and S. Solla (1990). Optimal brain damage. In Advances in Neural Information
Processing Sstems, pp. 598-605.
Lee, J., Y. Bahri, R. Novak, S. Schoenholz, J. Pennington, and J. Sohl-Dickstein (2018). Deep neural
networks as Gaussian processes. In 6th International Conference on Learning Representations.
Lee, J., L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington (2019).
Wide neural networks of any depth evolve as linear models under gradient descent. In 33rd
Conference on Neural Information Processing Systems.
Lee, N., T. Ajanthan, S. Gould, and P. H. S. Torr (2020). A signal propagation perspective for pruning
neural networks at initialization. In 8th International Conference on Learning Representations.
Lee, N., T. Ajanthan, and P. H. Torr (2018). Snip: Single-shot network pruning based on connection
sensitivity. In 6th International Conference on Learning Representations.
10
Published as a conference paper at ICLR 2021
Li, H., A. Kadav, I. Durdanovic, H. Samet, and H. Graf (2018). Pruning filters for efficient convnets.
In 6th International Conference on Learning Representations.
Li, Y., S. Gu, C. Mayer, L. V. Gool, and R. Timofte (2020). Group sparsity: The hinge between filter
pruning and decomposition for network compression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition,pp. 8018-8027.
Li, Y., S. Gu, K. Zhang, L. Van Gool, and R. Timofte (2020). Dhp: Differentiable meta pruning via
hypernetworks. arXiv preprint arXiv:2003.13683.
Lillicrap, T., D. Cownden, D. Tweed, and C. Akerman (2016). Random synaptic feedback weights
support error backpropagation for deep learning. Nature Communications 7(13276).
Liu, Z., H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and J. Sun (2019). Metapruning: Meta
learning for automatic neural network channel pruning. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 3296-3305.
Louizos, C., M. Welling, and D. Kingma (2018). Learning sparse neural networks through l0
regularization. In 6th International Conference on Learning Representations.
Matthews, A., J. Hron, M. Rowland, R. Turner, and Z. Ghahramani (2018). Gaussian process
behaviour in wide deep neural networks. In 6th International Conference on Learning Representa-
tions.
Mozer, M. and P. Smolensky (1989). Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In Advances in Neural Information Processing Systems, pp.
107-115.
Neal, R. (1995). Bayesian Learning for Neural Networks, Volume 118. Springer Science & Business
Media.
Neyshabur, B., Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro (2019). The role of over-
parametrization in generalization of neural networks. In 7th International Conference on Learning
Representations.
Nguyen, Q. and M. Hein (2018). Optimization landscape and expressivity of deep CNNs. In 35th
International Conference on Machine Learning.
Pecaric, J., F. Proschan, and Y. Tong (1992). Convex Functions, Partial Orderings, and Statistical
Applications. Academic Press.
Poole, B., S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli (2016). Exponential expressivity
in deep neural networks through transient chaos. In 30th Conference on Neural Information
Processing Systems.
Puri, M. and S. Ralescu (1986). Limit theorems for random central order statistics. Lecture Notes-
Monograph Series Vol. 8, Adaptive Statistical Procedures and Related Topics.
Schoenholz, S., J. Gilmer, S. Ganguli, and J. Sohl-Dickstein (2017). Deep information propagation.
In 5th International Conference on Learning Representations.
Tanaka, H., D. Kunin, D. L. Yamins, and S. Ganguli (2020). Pruning neural networks without any
data by iteratively conserving synaptic flow. In 34th Conference on Neural Information Processing
Systems.
Van Handel, R. (2016). Probability in High Dimension. Princeton University. APC 550 Lecture
Notes.
Von Mises, R. (1936). La distribution de la plus grande de n valeurs. Selected Papers Volumen II,
American Mathematical Society, 271-294.
Wang, C., G. Zhang, and R. Grosse (2020). Picking winning tickets before training by preserving
gradient flow. In 8th International Conference on Learning Representations.
11
Published as a conference paper at ICLR 2021
Xiao, L., Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and P. Pennington (2018). Dynamical
isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural
networks. In 35th International Conference on Machine Learning.
Yang, G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian process behav-
ior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760.
Yang, G., J. Pennington, V. Rao, J. Sohl-Dickstein, and S. S. Schoenholz (2019). A mean field theory
of batch normalization. In 7th International Conference on Learning Representations.
Yang, G. and S. Schoenholz (2017). Mean field residual networks: On the edge of chaos. In 31st
Conference in Neural Information Processing Systems, Volume 30,pp. 2869-2869.
Zhang, C., S. Bengio, M. Hardt, B. Recht, and O. Vinyals (2016). Understanding deep learning
requires rethinking generalization. In 5th International Conference on Learning Representations.
12
Published as a conference paper at ICLR 2021
A	Discussion about Approximations 1 and 2
A.1 Approximation 1: Infinite Width Approximation
FeedForward Neural Network
Consider a randomly initialized FFNN of depth L, widths (Nl )i≤i≤l, weights Wij iid N(0, Nw)
and bias Bi iiid N (0, σ2), where N (μ, σ2) denotes the normal distribution of mean μ and variance
σ2. For some input x ∈ Rd, the propagation of this input through the network is given by
d
yi1(x) = X Wi1j xj + Bi1,	(10)
j=1
Nl-1
yil(x) = X Wiljφ(yjl-1(x))+Bil,	forl ≥2.	(11)
j=1
Where φ : R → R is the activation function. When we take the limit Nl-1 → ∞, the Central
Limit Theorem implies that yil (x) is a Gaussian variable for any input x. This approximation by
infinite width solution results in an error of order O(1/y∕N-) (standard Monte Carlo error). More
generally, an approximation of the random process yil(.) by a Gaussian process was first proposed
by Neal (1995) in the single layer case and has been recently extended to the multiple layer case by
Lee et al. (2018) and Matthews et al. (2018). We recall here the expressions of the limiting Gaussian
process kernels. For any input x ∈ Rd, E[yil(x)] = 0 so that for any inputs x, x0 ∈ Rd
κl (x, x0) = E[yil (x)yil (x0)]
= σb2 + σw2 E[φ(yil-1(x))φ(yil-1(x0))]
= σb2 + σw2 Fφ(κl-1(x, x), κl-1(x, x0), κl-1(x0, x0)),
where Fφ is a function that only depends on φ. This provides a simple recursive formula for the
computation of the kernel κl; see, e.g., Lee et al. (2018) for more details.
Convolutional Neural Networks
Similar to the FFNN case, the infinite width approximation with 1D CNN (introduced in the main
paper) yields a recursion for the kernel. However, the infinite width here means infinite number of
channels, and results in an error O(1/√nl-ι). The kernel in this case depends on the choice of the
neurons in the channel and is given by
2
Kaa(X,x0) = E[yi,α(χ)yi,ɑo(X0)] = σ2 + 2k + 1 X E[φ(y1-1+β(X))。(91-1，+产(X0))]
β∈ker
so that
σ2
Ka,a (X, XZ) = σb + 2k + 1 ^X Fφ (κO-+β,a0+β (X，X)，κa+β,a0 + β (X，XZ )，κa+β,a0 + β (X，XZ)).
β∈ker
The convolutional kernel κlα,α0 has the ‘self-averaging’ property; i.e. it is an average over the
kernels corresponding to different combination of neurons in the previous layer. However, it is
easy to simplify the analysis in this case by studying the average kernel per channel defined by
KI = N Pa ao Ka a0 ∙ Indeed, by summing terms in the previous equation and using the fact that we
use circular padding, we obtain
Kl (x, X) = σb + σW N2 X Fφ(Kaa (x, x), K-J (x, X0),K- (x0,x0)).
a,a0
This expression is similar in nature to that of FFNN. We will use this observation in the proofs.
Note that our analysis only requires the approximation that, in the infinite width limit, for any two
inputs X, X0, the variables yil(X) and yil(X0) are Gaussian with covariance Kl(X, X0) for FFNN, and
13
Published as a conference paper at ICLR 2021
yil,α(x) and yil,α0 (x0) are Gaussian with covariance κlα,α0 (x, x0) for CNN. We do not need the much
stronger approximation that the process yil(x) (yil,α(x) for CNN) is a Gaussian process.
Residual Neural Networks
The infinite width limit approximation for ResNet yields similar results with an additional residual
terms. It is straighforward to see that, in the case a ResNet with FFNN-type layers, we have that
κl(x, x0) = κl-1(x, x0) + σb2 + σw2 Fφ(κl-1(x, x), κl-1 (x, x0), κl-1(x0, x0)),
whereas for ResNet with CNN-type layers, we have that
κlα,α0 (x, x0)
l-1
κα,α0
2
σw2
2k + 1
Fφ(κα-+β,α0+β (x, x),
κα+β,α0 +β(x,x0),
κα+β,α0+β (x , x )).
β ∈ker
+
A.2 Approximation 2:
Gradient Independence
For gradient back-propagation, an essential assumption in prior literature in Mean-Field analysis
of DNNs is that of the gradient independence which is similar in nature to the practice of feedback
alignment (Lillicrap et al., 2016). This approximation allows for derivation of recursive formulas for
gradient back-propagation, and it has been extensively used in literature and verified empirically; see
references below.
Gradient Covariance back-propagation: this approximation was used to derive analytical formulas
for gradient covariance back-propagation in (Hayou et al., 2019; Schoenholz et al., 2017; Yang and
Schoenholz, 2017; Lee et al., 2018; Poole et al., 2016; Xiao et al., 2018; Yang, 2019). It was shown
empirically through simulations that it is an excellent approximation for FFNN in Schoenholz et al.
(2017), for Resnets in Yang and Schoenholz (2017) and for CNN in Xiao et al. (2018).
Neural Tangent Kernel (NTK): this approximation was implicitly used by Jacot et al. (2018) to
derive the recursive formula of the infinite width Neural Tangent Kernel (See Jacot et al. (2018),
Appendix A.1). Authors have found that this approximation yields excellent match with exact NTK.
It was also exploited later in (Arora et al., 2019; Hayou et al., 2020) to derive the infinite NTK for
different architectures. The difference between the infinite width NTK Θ and the empirical (exact)
NTK ΘΘ was studied in Lee et al. (2019) where authors have shown that ∣∣Θ - ΘΘ∣∣f = O(N-1) where
N is the width of the NN.
More precisely, we use the approximation that, for wide neural networks, the weights used for forward
propagation are independent from those used for back-propagation. When used for the computation
of gradient covariance and Neural Tangent Kernel, this approximation was proven to give the exact
computation for standard architectures such as FFNN, CNN and ResNets, without BatchNorm in
Yang (2019) (section D.5). Even with BatchNorm, in Yang et al. (2019), authors have found that the
Gradient Independence approximation matches empirical results.
This approximation can be alternatively formulated as an assumption instead of an approximation as
in Yang and Schoenholz (2017).
Assumption 1 (Gradient Independence): The gradients are computed using an i.i.d. version of the
weights used for forward propagation.
B	Preliminary results
Let x be an input in Rd . In its general form, a neural network of depth L is given by the following set
of forward propagation equations
yl(x) = Fl(W l, yl-1(x)) + Bl,	1 ≤l ≤L,	(12)
where yl(x) is the vector of pre-activations and Wl and Bl are respectively the weights and bias of the
lth layer. Fl is a mapping that defines the nature of the layer. The weights and bias are initialized with
iid	σ2	iid
Wl 〜N(0, -w-) where VliS a scaling factor used to control the variance of yl, and Bl 〜N(0, σ2).
Hereafter, we denote by Ml the number of weights in the lth layer, φ the activation function and
[n : m] the set of integers {n, n + 1, ..., m} for n ≤ m. Two examples of such architectures are:
14
Published as a conference paper at ICLR 2021
•	Fully-connected FeedForward Neural Network (FFNN)
For a fully connected feedforward neural network of depth L and widths (Nl)0≤l≤L, the
forward propagation of the input through the network is given by
d
yi1(x) = XWi1jxj +Bi1,
j=1
Nl-1
yil(x) = X Wiljφ(yjl-1(x))+Bil,	forl≥2.
j=1
(13)
Here, we have vl = Nl-1 and Ml = Nl-1Nl.
•	Convolutional Neural Network (CNN/ConvNet)
For a 1D convolutional neural network of depth L, number of channels (nl)l≤L and number
of neurons per channel (Nl)l≤L. we have
nl-1
yi1,α (x) =	Wi1,j,βxj,α+β + bi1 ,
j=1 β∈kerl
nl-1
yil,α(x) = X X Wil,j,βφ(yjl-,α1+β(x)) + bli, forl≥2,
j=1 β∈kerl
(14)
where i ∈ [1 : nl] is the channel index, α ∈ [0 : Nl - 1] is the neuron location,
kerl = [-kl : kl] is the filter range and 2kl + 1 is the filter size. To simplify the
analysis, we assume hereafter that Nl = N and kl = k for all l. Here, we have
vl = nl-1(2k + 1) and Ml = nl-1nl(2k + 1). We assume periodic boundary conditions,
so yil,α = yil,α+N = yil,α-N. Generalization to multidimensional convolutions is
straighforward.
Notation: Hereafter, for FFNN layers, we denote by ql (x) the variance of y1l (x) (the choice of the
index 1 is not crucial since, by the mean-field approximation, the random variables (yi(χ))i∈[i:Nl]
are iid Gaussian variables). We denote by ql(x, x0) the covariance between y1l (x) and y1l (x0), and
cl1 (x, x0 ) the corresponding correlation. For gradient back-propagation, for some loss function L,
We denote by ql (x, χ0) the gradient covariance defined by ql (x, χ0) = E
[∂∂Lr(X)嬴(XO)] ∙ Similarly,
ql (x) denotes the gradient variance at point x.
For CNN layers, We use similar notation accross channels. More precisely, We denote by qαl (x)
the variance of y1l ,α, (x) (the choice of the index 1 is not crucial here either since, by the mean-
field approximation, the random variables (y!i，a(x))iE[i：Ni] are iid Gaussian variables). We denote
by qαl ,α0 (x, x0) the covariance betWeen y1l ,α(x) and y1l ,α0 (x0), and clα,α0 (x, x0) the corresponding
correlation.
As in the FFNN case, we define the gradient covariance by Ga α, (x, x0) = E
dLf- (x) " (x0).
∂y1l,α	∂y1l,α0
B.1	Warmup : Some results from the Mean-Field theory of DNNs
We start by recalling some results from the mean-field theory of deep NNs.
B.1.1	Covariance propagation
Covariance propagation for FFNN:
In Section A.1, we presented the recursive formula for covariance propagation in a FFNN, which we
derive using the Central Limit Theorem. More precisely, for two inputs x, x0 ∈ Rd , we have
ql(x, x0) = σb2 + σw2E[φ(yil-1(x))φ(yil-1(x0))].
This can be rewritten as
ql(x,x0) = σ2 + σWE φ (JqI(x)ZI) φ (yql(x)(clτZι + J1 - (CIT)2Z2
15
Published as a conference paper at ICLR 2021
where cl-1 := cl-1 (x, x0).
With a ReLU activation function, we have
ql(χ,χ0)=蟾+σw q q(χ)q qιχ0)f (ClT),
where f is the ReLU correlation function given by (Hayou et al. (2019))
f (c) = ɪ(earcsinc + pl - c2) + ɪe.
Covariance propagation for CNN:
Similar to the FFNN case, it is straightforward to derive recusive formula for the covariance. However,
in this case, the independence is across channels and not neurons. Simple calculus yields
2
qa,ao(χ,χ0) = E[yi,α(x)yi,ɑo(x0)] = σ2 + 2k+ι X E[φ(y1-1+β(χ))φ(y1-1o+β(x0))]
β ∈ker
Using a ReLU activation function, this becomes
2	,-------,------------
qa,ao (χ,χ0) = σ2 + 2k+ι X Jqa+β(x) ∖∣qao+β(X0)f (Ca+^β,α0 + β(X，x0)).
β ∈ker
Covariance propagation for ResNet with ReLU :
This case is similar to the non residual case. However, an added residual term shows up in the
recursive formula. For ResNet with FFNN layers, we have
and for ResNet with CNN layers, we have
2	,--------,-------------
qal,a0(X,X0) = qal-,a10 (X, X0) +σb2 + 2k+l X Jqa+β(X) 区+ β (X0)f (Ca+β,a0+β (X，x0)).
β∈ker
B.1.2	Gradient Covariance back-propagation
Gradiant Covariance back-propagation for FFNN:
Let L be the loss function. Let x be an input. The back-propagation of the gradient is given by the set
of equations
∂yl=φ0(yi) X1 / Wl+1
Using the approximation that the weights used for forward propagation are independent from those
used in backpropagation, we have as in Schoenholz et al. (2017)
ql(χ) = ql+1(χ) N+ χ(ql(χ)),
Nl
where χ(ql(x)) = *j E[φ( Vzql (x) Z)2].
Gradient Covariance back-propagation for CNN:
Similar to the FFNN case, we have that
∂ L
dWjβ
XX dyLa φ(yl-α+β)
and
∂LL = XX X
Ma j=1 Iiker
∂L
∂√+^ Wil+,β φ0(yi,α).
∂yj,α-β
16
Published as a conference paper at ICLR 2021
Using the approximation of Gradient independence and averaging over the number of channels (using
CLT) we have that
E[2] = σwE[φ0(Pqa(X)Z月 X E[-d+L-2].
∂yil,α	2k+ 1	β∈ker ∂yil,+α1-β
We can get similar recursion to that of the FFNN case by summing over α and using the periodic
boundary condition, this yields
∂L 2	∂L 2
X E[7T^i- ]= Xga(X)) X E[ W 1 + 1 ].
α	∂yil,α	α α	∂yil,+α1
B.1.3	EDGE OF CHAOS (EOC)
Let X ∈ Rd be an input. The convergence of ql (X) as l increases has been studied by Schoenholz
et al. (2017) and Hayou et al. (2019). In particular, under weak regularity conditions, it is proven
that ql (X) converges to a point q(σb , σw ) > 0 independent of X as l → ∞. The asymptotic
behaviour of the correlations cl (X, X0) between yl (X) and yl (X0) for any two inputs X and X0 is
also driven by (σb, σw): the dynamics of cl is controlled by a function f i.e. cl+1 = f(cl) called
the correlation function. The authors define the EOC as the set of parameters (σb, σw ) such that
σw2 E[φ0( ∙√q(σ∖, σw)Z)2] = 1 where Z 〜 N(0,1). Similarly the Ordered, resp. Chaotic, phase
is defined by σWE[φ0(pq(σfc, σw)Z)2] < 1, resp. σWE[φ0(pq(σb, σ.)Z)2] > 1. On the Ordered
phase, the gradient will vanish as it backpropagates through the network, and the correlation cl (X, X0)
converges exponentially to 1. Hence the output function becomes constant (hence the name ’Ordered
phase’). On the Chaotic phase, the gradient explodes and the correlation converges exponentially
to some limiting value c < 1 which results in the output function being discontinuous everywhere
(hence the ’Chaotic’ phase name). On the EOC, the second moment of the gradient remains constant
throughout the backpropagation and the correlation converges to 1 at a sub-exponential rate, which
allows deeper information propagation. Hereafter, f will always refer to the correlation function.
B.1.4	Some results from the Mean-Field theory of Deep FFNNs
Let ∈ (0, 1) and B = {(X, X0)Rd : c1(X, X0) < 1 - } (For now B is defined only for FFNN).
Using Approximation 1, the following results have been derived by Schoenholz et al. (2017) and
Hayou et al. (2019):
•	There exist q, λ > 0 such that supx∈Rd |ql - q| ≤ e-λl.
•	On the Ordered phase, there exists γ > 0 such that supx,x0∈Rd |cl(X, X0) - 1| ≤ e-γl.
•	On the Chaotic phase, For all ∈ (0, 1) there exist γ > 0 and c < 1 such that
sup(x,x0)∈B |cl(X,X0) -c| ≤ e-γl.
•	For ReLU network on the EOC, we have
f (x) _= x + 23∏2(1 - x)3/2 + O((1 - x)5∕2)∙
•	In general, we have
σ2 + σW E的yqZn^yqZ(X))]
f (x)=--------------------
q
where Z(x) = xZ1 + √1 - x2Z2 and Z1, Z2 are iid standard Gaussian variables.
•	On the EOC, we have f0(1) = 1
•	On the Ordered, resp. Chaotic, phase we have that f0(1) < 1, resp. f0(1) > 1.
•	For non-linear activation functions, f is strictly convex and f(1) = 1.
•	f is increasing on [-1, 1].
(15)
17
Published as a conference paper at ICLR 2021
•	On the Ordered phase and EOC, f has one fixed point which is 1. On the chaotic phase, f
has two fixed points: 1 which is unstable, and c ∈ (0, 1) which is a stable fixed point.
•	On the Ordered/Chaotic phase, the correlation between gradients computed with different
inputs converges exponentially to 0 as we back-progapagate the gradients.
Similar results exist for CNN. Xiao et al. (2018) show that, similarly to the FFNN case, there exists
q such that qαl (x) converges exponentially to q for all x, α, and studied the limiting behaviour of
correlation between neurons at the same channel clα,α0 (x, x) (same input x). These correlations
describe how features are correlated for the same input. However, they do not capture the behaviour
of these features for different inputs (i.e. clα,α0 (x, x0) where x 6= x0). We establish this result in the
next section.
B.2 Correlation behaviour in CNN in the limit of large depth
Appendix Lemma 1 (Asymptotic behaviour of the correlation in CNN with smooth activation
functions). We consider a 1D CNN. Let (σb, σw) ∈ (R+)2 and x 6= x0 be two inputs ∈ Rd. If
(σb , σw ) are either on the Ordered or Chaotic phase, then there exists β > 0 such that
sup lcα,α0 (X,xO)-CI = O(e-βl ),
α,α0
where c = 1 if (σb, σw) is in the Ordered phase, and c ∈ (0, 1) if (σb, σw) is in the Chaotic phase.
Proof. Let x 6= x0 be two inputs and α, α0 two nodes in the same channel i. From Section B.1, we
have that
2
qla,α∕(X,χO) = E[yi,α(X)yi,ɑo(XO)] = 2k； 1 X E[φ(y1-1+β(X))φ3l-1,+β(XO))] + σ2
β∈ker
This yields
clα,α0(X,XO)
1
2k +1
f (cα+β,α0 +β (X, X )),
β∈ker
where f is the correlation function.
We prove the result in the Ordered phase, the proof in the Chaotic phase is similar. Let (σb, σw) be in
the Ordered phase and clm = minα,α0 clα,a0 (X, XO). Using the fact that f is non-decreasing (section
B.1), we have that《a，(x,xo) ≥ 2k+ι Pβ∈ker ,13+产(x,xo)) ≥ f (C1m1). Taking the min again
over α, αO, we have clm ≥ f(clm-1), therefore clm is non-decreasing and converges to a stable fixed
point of f . By the convexity of f, the limit is 1 (in the Chaotic phase, f has two fixed point, a stable
point C1 < 1 and C2 = 1 unstable). Moreover, the convergence is exponential using the fact that
0 < f (1) < 1. We conclude using the fact that suPa,a，∣c~ a (x, xo) - 1I = 1 - Cm.	□
C Proofs for Section 2 : SBP for FFNN/CNN and the Rescaling
Trick
In this section, we prove Theorem 1 and Proposition 1. Before proving Theorem 1, we state the
degeneracy approximation.
Approximation 3 (Degeneracy on the Ordered phase). On the Ordered phase, the correlation Cl
and the variance ql converge exponentially quickly to their limiting values 1 and q respectively. The
degeneracy approximation for FFNN states that
•	∀X 6= XO, Cl (X, XO) ≈ 1
•	∀X, ql (X) ≈ q
For CNN,
•	∀X 6= XO, α, αO, Clα,α0 (X, XO) ≈ 1
18
Published as a conference paper at ICLR 2021
•	∀x, qαl (x) ≈ q
The degeneracy approximation is essential in the proof of Theorem 1 as it allows us to avoid many
unnecessary complications. However, the results holds without this approximation although the
constants may be a bit different.
Theorem 1 (Initialization is crucial for SBP). We consider a FFNN (2) or a CNN (3). Assume
(σw, σb) are chosen on the ordered, i.e. χ(σb, σw) < 1, then the NN is ill-conditioned. Moreover, we
have
Elscr] ≤ k
L
1 + Iog(KLN2)
κ
+ O (K2√LN2
where K = | log χ(σb,σw )|/8. If (σw,σb) are on the EOC, i.e. χ(σb,σw) = 1 ,then the NN is
well-conditioned. In this case, κ = 0 and the above upper bound no longer holds.
Proof. We prove the result using Approximation 3.
1.	Case 1 : Fully connected Feedforward Neural Networks
To simplify the notation, we assume that Nl = N and Ml = N2 (i.e. αl = 1 and ζl = 1) for
all l. We prove the result for the Ordered phase, the proof for the Chaotic phase is similar.
Let Lo》1, e ∈ (0, 1 — 六),L ≥ L0 and X ∈ (L + e, 1). With sparsity x, We keep
kx = b(1 - x)LN2c weights. We have
∂L
P(Scr ≤ x) ≥ P(maX |Wij l∣∂WT I <t(kx))
where t(kx) is the kχh order statistic of the sequence {∣Wj∣∣ ∂WLτ ∣,l > 0, (i,j) ∈ [1 : N ]2}.
∂Wij
We have
∂ L	1	∂ L ∂yi (x)
∂Wj = ∣D∣ x∈D ∂y∏x) ^∂Wj
=∣D∣ X 忌 φ(yjτ(χ))∙
x∈
On the Ordered phase, the variance ql (x) and the correlation cl(x, x0) converge exponentially
to their limiting values q, 1 (Section B.1). Under the degeneracy Approximation 3, we have
•	∀x 6= x0, cl (x, x0) ≈ 1
•	∀x, ql(x) ≈ q
Let ql(x) = E[∂∂LX)2] (the choice of i is not important since (yi(χ)) are iid). Using these
approximations, we have that yil(x) = yil(x0) almost surely for all x, x0. Thus
∂L 2
E[dWτ ] = Elφ(√qz)2]ql(χ),
where x is an input. The choice of x is not important in our approximation.
From Section B.1.2, we have
碓=旗+1 NNi χ.
Then we obtain
qx = NL qXχL- = qXχJ,
Nl
where X = σWE[φ(√qZ)2] as we have assumed Nl = N. Using this result, we have
E[jLr 2] = AXJ
LdWil j」 X ,
ij
19
Published as a conference paper at ICLR 2021
where A = E[φ(√qZ)2历L for an input x. Recall that by definition, one has χ < 1 on the
Ordered phase.
In the general case, i.e. without the degeneracy approximation on cl andql, we can prove
that
which suffices for the rest of the proof. However, the proof of this result requires many
unnecessary complications that do not add any intuitive value to the proof.
In the general case where the widths are different, ql will also scale as XLT up to a different
constant.
Now we want to lower bound the probability
∂L
以max WjIIdWII <t(kx)).
i,j	∂Wij
Let tekx) be the kth order statistic of the sequence {∣Wj∣I ∂vLr∖, l > 1 + "，(i,j) ∈ ]：
∂Wij
N]2}. It is clear thatt(kx) > t(kx), therefore
dL	dL
以max WjIIdWI i <t(kx)) ≥ 叫maχ WjIIdWI i <t!kχ)).
Using Markov’s inequality, we have that
∣ ∂L ∣
P(IdWI ∣≥α) ≤
ij
E[∣∂Wj^]
ij
α2
(16)
Note that Var(χ L-L ∣ ∂W∏ ∣) = A. In general, the random variables X L-L ∣ ∂L ∣ have a
Wij	Wij
density filj for all l > 1 + L, (i,j) ∈ [1 : N]2, such that filj (0) 6= 0. Therefore, there exists
a constant λ such that for x small enough,
l-L ∖ ∂ L i
P(X HIdWr l≥ x) ≥ 1 - λx.
ij
(1-e∕2)L-1
By selecting X = X 2	, We obtain
l — L	(1 + eL) — L (1 — e∕2)L — 1	τ In
X F X X ≤ X-2- X	2	= X /	.
Therefore, for L large enough, and all l > 1 + L,	(i, j)	∈ [1 : Nl]	×	[1 : NlT1],	we have
P(IdWrI ≥ X (1-/2L-) ≥ 1 - λX l-^+11 ≥ 1 - λX^L∕2.
(1 —e∕4)L-1
Now choosing a = X 2 in inequality (16) yields
P(IdW1∣≥ X (1-1/4L- ) ≥ 1 - AXeL/4.
ij
Since we do not know the exact distribution of the gradients, the trick is to bound them using
the previous concentration inequalities. We define the event B := {∀(i, j) ∈ [1 : N] × [1 :
d],∣∂∂⅛l ≤ X(1-/4L-1 }∩ {∀l> 1 + eL, (i,j) ∈ [1: N]2, ∣∂WH ≥ X(1-/2L-I}.
ij	ij
We have
∂L	∂L
P(max WjII	∣ <"kX)) ≥ P(max WjII	∣ < "k 叫 B)P(B).
,	ij	,	ij
20
Published as a conference paper at ICLR 2021
But, by conditioning on the event B, we also have
∂L
P(max WjIl	l <t∕x)∣B) ≥ P(max |Wj| < χ-eL∕8t;(kx)),
i,j j ∂Wi1j	i,j j
where t0(kx) is the kxth order statistic of the sequence {|Wilj |, l > 1 + L, (i, j) ∈ [1 : N]2}.
Now, as in the proof of Proposition 4 in Appendix E (MBP section), define xζ,γL = min{y ∈
(0,1) : ∀x > y, YLQx > Q	2-ζ}, where YL = χ-eL/8. Since lim(→2 xζ,γL = 0,
1-(1-x) L
then there exists Ze < 2 such that x^c,yl = e + L.
As L grows, t0e(kχ) converges to the quantile of order ɪ-f. Therefore,
P(maχ |Wj| < x-eL/8te(kx)) ≥ P(maχ |Wj| < Q1-(1-x-父) + O(√=)
≥1 - N 2( W『+ O(√N).
Using the above concentration inequalities on the gradient, we obtain
P(B) ≥ (1 - AχeL∕4)N2(1 - XxeL/2)LN2.
Therefore there exists a constant η > 0 independent of e such that
P(B) ≥ 1 - ηLN2χeL∕4.
Hence, we obtain
P(Scr ≥ x) ≤ N2(F)γL-ζ' + ηLN2xeL/4 + 0(√=1=).
1 - e	LN2
Integration of the previous inequality yields
1	N2	1
E[scr] ≤e + L + 1 + γ2-ζe + ηLNX	+ O(√LN).
Now let K = | log(χ)1 and set e ="虱KLN ). By the definition of xζe, we have
YLQxZ-YL= Q-γL-ζ∖
For the left hand side, we have
C	log(κLN2)
YLQxZ-YL 〜αγL -KL-
where α > 0 is the derivative at 0 of the function x → Qx . Since YL = κLN2, we have
YLQxZmL 〜αN2 log(κLN2)
Which diverges as L goes to infinity. In particular this proves that the right hand side
diverges and therefore we have that (1 - xζ,γL )γL2-Z converges to 0 as L goes to infinity.
Using the asymptotic equivalent of the right hand side as L → ∞, we have
Qι-(1-x< U)YL-Z。〜q-2lθg((I - χZ°YL )YLY ) = YL-"2√-2lθg(1 - xZe,YL ).
Therefore, we obtain
Q	2-Z。
1-(1-xZe,YL )YL
YLY/2
∕2log(KLN2)
V KL
〜
Combining this result to the fact that YLQxZ ,Y Z 〜 aYL log(KLN ) we obtain
Y-Z。〜β
log(KLN 2)
KL
21
Published as a conference paper at ICLR 2021
where β is a positive constant. This yields
E[scr] ≤
log(κLN2)	I
+ L + κLN2 log(κLN2)
κL
1(1 + -
log(KLN2))+ O(
K2√LN),
(I + O(I)) + ηκLN2 + O( √N)
μ
1
κ
where K = | log(χ)1 and μ is a constant.
2.	Case 2 : Convolutional Neural Networks
The proof for CNNs in similar to that of FFNN once we prove that
E[ -dL- 2] = AXLT
LdWilj,β」X
where A is a constant. We have that
∂L
dWUβ
X ∂⅛ φ(yj-+β)
and
M = X X
∂yl
i,α j=1β ∈ker
∂L
L- Wil+,β φ0(yi,a).
∂yj,α-β
Using the approximation of Gradient independence and averaging over the number of
channels (using CLT) we have that
e[洋2] = σw≡≡π X e[∂^2]∙
∂yi,α	2k+ 1 β ∈ker	∂yi,α-β
Summing over α and using the periodic boundary condition, this yields
∂L 2	∂L 2
X E[ ∂⅛] = X X E[ ∂<].
Here also, on the Ordered phase, the variance ql and the correlation cl converge exponen-
tially to their limiting values q and 1 respectively. As for FFNN, we use the degeneracy
approximation that states
•	∀x 6= x0, α, α0, clα,α0 (x, x0) ≈ 1,
•	∀x, qαl (x) ≈ q.
Using these approximations, we have
∂L 2
E[∂WW- ] = E[φ(√qz)2]ql(x),
∂Wi,j,β
where ql(x) = Pa E[ ∂jLχ ] for an input x. The choice of X is not important in our
approximation.
From the analysis above, we have
ql(χ) = qL(χ)χL-l,
so we conclude that
E
2
∂L
dWjβ
= A XL-l
where A = E[φ(√qZ)2]⅞L(χ).
22
Published as a conference paper at ICLR 2021
□
After pruning, the network is usually ‘deep’ in the Ordered phase in the sense thatχ=f0(1)1.
To re-place it on the Edge of Chaos, we use the Rescaling Trick.
Proposition 1 (Rescaling Trick). Consider a NN of the form (2) or (3) (FFNN or CNN) initialized
on the EOC. Then, after pruning, the sparse network is not initialized on the EOC. However, the
rescaled sparse network
yl(x) = F(Plo δl ◦ Wl,yl-1(x)) + Bl, for l ≥ 1,	(17)
where
•	Pij = √E[Nι-ι(wiι1)2δl1] QrFFNNOfhefimm (22,
•	pl - = = /	1	— for CNN oftheform (3),
i,j,β	qE[nι-ι(Wt,β )2δ∣,ι,β]
is initialized on the EOC.
Proof. For two inputs x, x0, the forward propagation of the covariance is given by
ql(χ, x0) = E[yi(x)yi(XO)]
Nl-1
=E[X WijWilkδijδikφ(yj-1(χ))φ(yj-1(χ0))] + 式.
j,k
We have
∂L	1	∂L ∂yi(x)
∂Wj = |d| X∈D ∂y∏xy IWj
=|D| XD ∂⅛) φ(yjT(x)).
x∈
Under the assumption that the weights used for forward propagation are independent from the weights
used for back-propagation, Wj and ∂∂L) are independent for all X ∈D.We also have that Wj and
φ(yj-1(x)) are independent for all X ∈ D. Therefore, Wj and ∂L are independent for all l, i,j.
∂ Wij
This yields
^ (x,x0) = σW αι E[φ(y1-1(x))φ(yi-1(x0))] + σ2,
where αl = E[Nl-1(W1l1)2δ1l 1] (the choice of i, j does not matter because they are iid). Unless we
do not prune any weights from the lth layer, we have that αl < 1.
These dynamics are the same as a FFNN with the variance of the weights given by σW = σW αι.
Since the EOC equation is given by σWE[φ0(√qZ)2] = 1, with the new variance, it is clear that
σWE[φ0(√qZ)2] = 1 in general. Hence, the network is no longer on the EOC and this could be
problematic for training.
With the rescaling, this becomes
ql(x,x0) = σW p2αιE[φ(y1-1 (x))φ(y1-1(x0))] + σ2
=σW E[φ(y1-1(x))φ(y1-1(x0))] + 潞.
Therefore, the new variance after re-scaling is σW = σW, and the limiting variance q = q remains also
unchanged since the dynamics are the same. Therefore σWE[φ0(√qZ)2] = σWE[φ0(√qZ)2] = 1.
Thus, the re-scaled network is initialized on the EOC. The proof is similar for CNNs.
□
23
Published as a conference paper at ICLR 2021
D	Proof for section 3 : SBP for Stable Residual Networks
Theorem 2 (Resnet is well-conditioned). Consider a Resnet with either Fully Connected or Convo-
lutional layers and ReLU activation function. Then for all σw > 0, the Resnet is well-conditioned.
Moreover, for all l ∈ {1,…,L},ml = Θ((1 + σw)L).
Proof. Let us start with the case of a Resnet with Fully Connected layers. we have that
∂ L	1	∂L ∂yi(x)
∂Wj = |D1 χ∈D 砧而 ∂Wj
=iDi XD 枭 φ(yj-1(x))
and the backpropagation of the gradient is given by the set of equations
∂L	∂L	0 l Nl+1 ∂L l+1
觎=F + φ (yi) X 可Wji .
Recall that ql(x) = E[yi(x)2] and ql (x, χ0) = E[d：L)已慝,)]for some inputs x, χ0. We have that
2
ql(χ) = E[yi-1(χ)2]+ σW E[Φ(y1-1)2] = (1 + 芋)ql-1(χ),
and
ql (X,xO) = (I + σW E[φ0(yi(χ))φ0(yi(χ0 ))])ql+1(χ,x0).
We also have
where tlxx0
15).	,
∂ L 2 _ 1 X l
[∂Wj ] = |D12 M te
ql(x, x0) Jq(x)ql(x0)f (ClT(x, x0)) and f is defined in the preliminary results (Eq
Let k ∈ {1, 2, ..., L} be fixed. We compare the terms tlx,x0 for l = k and l = L. The ratio between
the two terms is given by (after simplification)
tX,” = QL-1(1 + 竽f0(cl(χ,χ0))) f(ck-1(χ,χ'))
tLH —	(1 + 除)Li	f (cL-1(χ,χ0))
We have that f0(cl (x, x)) = f0(1) = 1. A Taylor expansion of f near 1 yields f0(cl (x, x0)) =
1 - l-1 + o(l-1) and f (cl (x, x)) = 1 - sl-2 + o(l-2) (see Hayou et al. (2019) for more details).
2
Therefore, there exist two constants A,B > 0 such that A < "』k (1+工 f (C (x,x ))) < B for all L
(1+缪 )L-k
and k ∈ {1, 2, ..., L}. This yields
A≤
E[ ∂⅜ 2]
E[ ∂∂⅛ 2]
ij
≤ B,
which concludes the proof.
For Resnet with convolutional layers, we have
∂ L
dWijβ
24
Published as a conference paper at ICLR 2021
and
∂l = ∂∂L1+XX X RWjφ0(yi,a).
yi,a	∂yi,a	j=1 β∈ker ∂yj,a-β
Recall the notation Ga. (x, XD = E[
∂L	∂L
dyl,α(X) dyl,a0(XO)
]. Using the hypothesis of independence of
forward and backward weights and averaging over the number of channels (using CLT), we have
dα,α (x, x''0) = ⅛+α, (x,χ0) +
σw2f0(clα,α0(x,x0))
2(2k + 1)
qGa++β,a0 +β (x, x0).
β
Let Ki = ((qa ɑ+β(x,x0))a∈[θ:N-i])β∈[0:N-1] be a vector in RN2. Writing this previous equation
in matrix form, we obtain
Kl = (I+
σw2 f0(clα,α0 (x, x0))
2(2k + 1)
U)Kl+1
and
where tla,a0 (x, x0)
∂L 2	1
E[∂Wl .	] = ∣D∣2 X X XOaOa (X,x ),
i,j,β	X,X0∈D a,a0
qa,ao(χ,χ0) Jqa+β(χ)qOo+β(χ0)f(cO-+β,a0+β(χ,χ0))∙ Since we have
f 0(cla,a0(x, x0)) → 1, then by fixing l and letting L goes to infinity, it follows that
2
Kl 〜L→∞ (1 + σ2w )L-leιeT Kl
and, from Lemma 2, we know that
2
VZqa+β (χ)qla o+β (χ0) = (1 + -2w )l-1√q0,χ q0,χ0.
Therefore, for a fixed k < L, we have :士,：，(x,x0)〜(1 + σ2w)L-1f(cka-+1β,a0+β(x,x0))(e1TKL)
Θ(taL,a0 (x, x0)). This concludes the proof.
□
Proposition 2 (Stable Resnet). Consider the following Resnet parameterization
yl(χ) = yl-1(χ) + √lF(Wl,yl-1), for l ≥ 2,
(18)
then the network is well-conditioned for all choices of σw > 0. Moreover, for all l ∈ {1, ..., L} we
have ml = Θ(L-1).
Proof. The proof is similar to that of Theorem 2 with minor differences. Let us start with the case of
a Resnet with fully connected layers, we have
∂L	1	∂L ∂yi(x)
∂Wj = ∣D∣√L x∈D ∂yl(X) IWij
=∣D∣√L XXD ∂yL) MTIX))
and the backpropagation of the gradient is given by
∂L ∂L	1 0 l Nl+1 ∂L l+1
羽=膏1 + √Lφ (yi)X 可Wji .
25
Published as a conference paper at ICLR 2021
Recall that ql(χ) = E[yi(χ)2] and ql(x, χ0) = E[∂∂(X) ∂ydLX0)] for some inputs x, χ0. We have
and
22
ql (X) = E[yi-1(χ)2] + -w E[Φ(y1-1(χ))2] = (1 + 声)ql-1(χ)
L	2L
2
ql(x,x0) = (1 + wE E[φ0(yi(χ))φ0(yi(χ0 ))])ql+1(χ,χ0).
L
We also have
∂ L 2	1 X ι
E[∂Wij ] = LiDF KBx,
where tlχxo = ql(x, x0) Jq(x)ql(x0)f (ClT(x, x0)) and f is defined in the preliminary results (Eq.
15).	,
Let k ∈ {1, 2, ..., L} be fixed. We compare the terms tlx,x0 for l = k and l = L. The ratio between
the two terms is given after simplification by
tX,χ0 = QL-1(1 + σ⅜f0(cl(x,x0))) f(ck-1(x,x'))
tL,χo —	(1+⅛)L-k	f(cLτ(x,XO))
As in the proof of Theorem 2, we have that f0(cl(x, x)) = 1, f0(cl(x, x0)) = 1 - l-1 + o(l-1)
and f(cl(x, x)) = 1 - sl-2 + o(l-2). Therefore, there exist two constants A, B > 0 such that
2
A < QL-1(i+¾Lf0(cl(χ,χ0)))
(1+σL )L-k
< B for all L and k ∈ {1, 2, ..., L}. This yields
A≤
E[册 2]
___i
Ewi
ij
≤ B.
2
Moreover, since (1 + 2Lw)L
→ eσw/2,then ml = Θ(1) for all l ∈ {1,…,L}. This concludes the proof.
For Resnet with convolutional layers, the proof is similar. With the scaling, we have
∂L	1	∂L	l-1
dWjβ=√l∣d∣ X∈D ? dyoxy φ(yj,a+β ())
and
∂L	∂L
dyi
Let KaO (x,x0) = E
—=--------+
Q	∂yi+1
∂L	∂L
√ X X d4L-W,+,βφ0(yi,a).
L j=1 β∈ker ∂yj,α-β
]. Using the hypothesis of independence of forward and
backward weights and averaging over the number of channels (using CLT) we have
~l	( 八 _ ~l+1 / 八	-Ef (Cla,a，(x,x'))	l+1	0
qa,a0 (x, x ) = qa,a0 (x,x ) +	2(2k + 1)L	) / qa+β,a0 + β (x,x ).
Let Kl = ((⅞a a+β(x,x0))a∈[θ:N-i])β∈[0:N-i] isavector in RN2. Writing this previous equation in
matrix form, we have
-E f (ClaH (X，XO))〃
Kl = (I +	2(2k +1)L	U)Kl+1,
and
∂L	2	1
E[∂wl.	] = L|d|2 X Xta，a0(x,x),
i,j,β	χ,χ0∈D α,α0
26
Published as a conference paper at ICLR 2021
where tα,α0 (X,xO)	= KaO (X,χO) yqO+β(Xqo+β(x)f (Co+β,α0 + β(X,xO)) ∙ Since We have
f 0 (clα,α0 (x, x0)) → 1, then by fixing l and letting L goes to infinity, we obtain
Kl 〜L→∞
2
(I + 2W)
L-le1e1TKL
σ2
and we know from Appendix Lemma 2 (using ɑβ = 2w for all β) that
,------------------ 4 2
Jqa+β (X)qθ o+β (XO) = (I + 2W )l-1√qo,χ q0,χ0.
Therefore, for a fixed k < L, we have tθ,ɑo (x,xo)〜(1 + σt )L-1f(cka-+1β,a0+β(X,XO))(e1TKL)=
Θ(taL,a0 (X, XO)) which proves that the stable Resnet is well conditioned. Moreover, since (1 +
σw)L-1 → eσW/2, then ml = Θ(L-1) for all l.
□
In the next Lemma, we study the asymptotic behaviour of the variance qal . We show that, as l → ∞,
a phenomenon of self averaging shows that qal becomes independent of α.
Appendix Lemma 2. Let X ∈ Rd. Assume the sequence (al,a)l,a is given by the recursive formula
al,a = al-1,a +	λβ al-1,a+β
β∈ker
where λβ > 0 for all β. Then, there exists ζ > 0 such that for all X ∈ Rd and α,
al,a(X) = (1 + X αβ)la0 + O((1 + X αβ)le-ζl)),
where a0 is a constant and the O is uniform in α.
Proof. Recall that
al,a = al-1,a +	λβ al-1,a+β.
β∈ker
We rewrite this expression in a matrix form
Al = UAl-1,
where Al = (al,a)a is a vector in RN and U is the is the convolution matrix. As an example, for
k = 1, U given by
	1 + λ0	λ1	0	...	0	λ-1
	λ-ι	1 + λo	λι	0	...	0
U=	0	λ-ι	1 + λo	λι	...	0
	0	0	λ-1	1 + λo ..	0 .	.	..
	.	.	.. λ1	0	. . .	0	λ-1	1 + λ0
U is a circulant symmetric matrix with eigenvalues b1 > b2 ≥ b3... ≥ bN. The largest eigenvalue
of U is given by b1 = 1 + Pβ λβ and its equivalent eigenspace is generated by the vector e1 =
√N (1,1,…，1) ∈ RN. This yields
b1-lUl = e1e1T + O(e-ζl),
where Z = log(b1). Using this result, we obtain
b1-lAl = (b1-lUl)A0 = e1e1TA0+O(e-ζl).
This concludes the proof.
□
27
Published as a conference paper at ICLR 2021
Unlike FFNN or CNN, we do not need to rescale the pruned network. The next proposition establishes
that a Resnet lives on the EOC in the sense that the correlation between yil(x) and yil(x0) converges
to 1 at a sub-exponential O(l-2) rate.
Proposition 3 (Resnet live on the EOC even after pruning). Let x 6= x0 be two inputs. The following
statments hold
1.	For ReSnet with Fully Connected layers, let ^ (x, x0) be the COrreIatiOn between yl(x) and
yi(x0) after pruning the network. Then we have
1 - C (X,x')〜IK,
where κ > 0 is a constant.
2.	ForResnetwith Convolutional layers, let ^ (x,x0) = ^α E[yι,α(χ)yια(χ2] bean 'average'
α,α0 qαl (x) qαl 0(x0 )
correlation after pruning the network. Then we have
1 - CI (X,X0) & I-2.
Proof. 1. Let X and x0 be two inputs. The covariance of yi (x) and yi(x0) is given by
ql(χ,χ0) = qlT(χ,χ0) + αE(Zι,Z2)〜N(0,Ql-I) [φ(Zι)φ(Z2)]
Where QlT= qiql!(XxXo)(-1；Xx)) and α = E[Nl-1W1ι2δ11].
Consequently, we have ^ (x) = (1 + 2)ql-1(χ). Therefore, we obtain
^ (x, x0)
vɪr yτ(χ,χ0) +
1+λ
EyfCT(χ, x0)),
1+λ
where λ = 2 and f (x) = 2E[φ(Zι)φ(χZι + √1 - x2Z2)] and Zi and Z2 are iid standard
normal variables.
Using the fact that f is increasing (Section B.1), it is easy to see that ^l(χ,χ0) → 1.
Let ζι = 1 - ^l(χ,χ0). Moreover, using a Taylor expansion of f near 1 (Section B.1)
f(x) = x + β(1 - x)3/2 + O((1 - x)5/2), it follows that
Zι = Zι-ι- ηZ3-1 + θ(Z5-2),
where η = ^^ ∙ Now using the asymptotic expansion of ZlT/2 given by
J = Zl-1/ + 2 + 0(Zl-1),
this yields ZlT/2 〜ηl. We conclude that 1 - ^l(χ, χ0)〜^4p∙.
l→∞ 2	η l
2. Let x be an input. Recall the forward propagation of a pruned 1D CNN
c
yil,α (x) =yil,-α1(x) + X X δil,j.βWil,j,βφ(yjl-,α1+β(x)) + bli.
j=1 β∈ker
Unlike FFNN, neurons in the same channel are correlated since we use the same filters for
all of them. Let x, x0 be two inputs and α, α0 two nodes in the same channel i. Using the
Central Limit Theorem in the limit of large nl (number of channels), we have
E[yi,α(χ)yi,ɑ0(X0)] = E[yi-1(χ)yi-i(χ0)]+2k1+1 X。产E[φ(y1-1+β(χ))φ(yl^+β(χ0))],
β∈ker
where αβ = E[δil,1.βWil,1,β2nl-1].
28
Published as a conference paper at ICLR 2021
Let qαl (x) = E[y1l ,α(x)2]. The choice of the channel is not important since for a given α,
neurons (yil,α (x))i∈[c] are iid. Using the previous formula, we have
qα(X) = ql-1 (X) + 2k+ 1
qα 1(x)+2k+ι
αβE[φ(y1l-,α1+β(X))2]
β∈ker
qα-+β (X)
αβ αβ -2—.
∈ker
Therefore, letting ql(x) = -N Pα∈[N] dα(x) and σ = ¾0β, We obtain
ql(x) = qlT(x) + 2⅛ X αβ X B
β∈ker	α∈[n]
=(I+ 2MT(X) = (I + 2)l-1q1(X),
Where We have used the periodicity qαl-1 = qαl--1N = qαl-+1N . Moreover, We have
mina qlα(x) ≥ (1 + 2)mi□α ql-1(x) ≥ (1 + 2)l-1 mi□α q](x).
The convolutional structure makes it hard to analyse the correlation betWeen the values of
a neurons for tWo different inputs. Xiao et al. (2018) studied the correlation betWeen the
values of tWo neurons in the same channel for the same input. Although this could capture
the propagation of the input structure (say hoW different pixels propagate together) inside
the netWork, it does not provide any information on hoW different structures from different
inputs propagate. To resolve this situation, We study the ’average’ correlation per channel
defined as
cl (X, X0)
Paa E[y1,a(x)y1,“ 3)]
Pα,α0 Pqa (X) pq]O(XO)
for any two inputs X = xo. We also define cl (x, xo) by
cl (x, xo)
N Pa,a，E[y1,a(X)y1,a，3)]
qN Pa qla (X) q寺 Pa qa (XO)
Using the concavity of the square root function, we have
\：N X qa(X) ʌIN X qa(XO) = SN X qa(X)qa(XO)
a	a	a,a，
≥ N Xqqa(X) qqa(XO)
a,a，
≥ N X |E[y1,a(X)y1,a，(xo)]∣.
a,a，
This yields cl(x, xo) ≤ cl(x, xo) ≤ 1. Using Appendix Lemma 2 twice with aι,a = qla(x),
aι,a = qa(xo), and λβ = 2^+1), there exists Z > 0 such that
cl(x, xo ) = cl (x, xo)(1 + O(e-ζl)).	(19)
This result shows that the limiting behaviour of cl (x, xo) is equivalent to that of cl (x, xo) up
to an exponentially small factor. We study hereafter the behaviour of cl(x, xo) and use this
result to conclude. Recall that
E[yi,a(X)yi,a，(x')] = E[yi-1(X)yi-1 3)1+2k1+1 X αβE[φ(y1-1+β(X))φ(y1-1o+β(XO))].
β ∈ker
29
Published as a conference paper at ICLR 2021
Therefore,
X E[y1l ,α(x)y1l ,α0 (x0)]
α,α0
=X E[y1-α1(χ)y1-α10(χ0)] + 2k1+1 X X。产E[Φ(y1-0l+β(χ))Φ(y^+β3川
α,α0	α,α0 β∈ker
= X E[y1l-,α1(x)y1l-,α10 (x0)] + σ X E[φ(y1l-,α1(x))φ(y1l-,α10 (x0))]
α,α0	α,α0
=X E[y1-01(X)y1-10(XO)] + 2 X q(ιl-1^q(ιl-1'^'')f (COiO(X,χO)),
α,α0	α,α0
where f is the correlation function of ReLU.
Let Us first prove that Cl (x, xo) converges to 1. Using the fact that f (Z) ≥ Z for all Z ∈ (0,1)
(Section B.1), we have that
X E[y1,α(χ)y1,αo(χ0)] ≥ X E[y1-α1(χ)y1-α1o(χ0)] + 2 X VZqaT(X) JqaTO(X0)4-!,(χ,χ0)
α,α0	α,α0	α,α0
=X E[y1-a1(X)y1-a10 (xo)] + 2 X E[y1-a1(X)y1-a10 3)]
a,a0	a,a0
=(1 + 2)E[y1-a1(X)y1-a10 (X0)].
Combining this result with the fact that Pa q：(x) = (1 + 2) Pa 9『3, We have
cCl(X, XO) ≥ cCl-1(X, XO). Therefore cCl(X, XO) is non-decreasing and converges to a limit-
ing point c.
Let us prove that c = 1. By contradiction, assume the limit c < 1. Using equation (19),
we have that 3(：，：0) converge to 1 as l goes to infinity. This yields C(x, xo) → c. There-
fore, there exists α0, αO0 and a constant δ < 1 such that for all l, cla ,a0 (X, XO) ≤ δ < 1.
Knowing that f is strongly convex and that fO(1) = 1, we have that f (cla ,a0 (X, XO)) ≥
cla0,a0 (X, XO) + f(δ) - δ. Therefore,
cCl (X, XO) ≥ cCl-1 (X, XO) +
2∖ Iqa 1(X)qlc- 1(xo)
V 1(f(δ) - δ)
N 2pql (x) p ql(X0)
≥ cCl-1 (X, XO ) +
2 Vzmina q：(x) min：，q：，(xo)
(f(δ) -δ).
By taking the limit l → ∞,we find that C ≥ C + 2 VZmma qa(xlmin：0 qα0(x ) (f (δ) - δ). This
N2 q1 (x) q1 (x0 )
cannot be true since f(δ) > δ. Thus we conclude that C = 1.
Now we study the asymptotic convergence rate. From Section B.1, we have that
f(x) =_ X + 23∏2(1 - x)3/2 + o((1 - x)5/2).
Therefore, there exists κ > 0 such that, close to 1- we have that
f(X) ≤ X + κ(1 - X)3/2.
Using this result, we can upper bound Cl (X, XO)
30
Published as a conference paper at ICLR 2021
cl(x,x0) ≤ L(X,x0)+κ X N P-Ipq:(1 -心(x,x0))3/2.
To get a polynomial convergence rate, We should have an upper bound of the form cl ≤
cl~1 + Z(1 - cl-1)1+e (see below). However, the function x3/2 is convex, so the sum cannot
be upper-bounded directly using Jensen,s inequality. We use here instead (PeCariC et al.,
1992, Theorem 1) which states that for any x1, x2, ...xn > 0 and s > r > 0, we have
(X xs)1/s < (X xr)1"	(20)
ii
Let zαl ,α0 =
, we have
EzaR(1 -CaR (χ,χ0))3/2 ≤ ZI eMr(1-4,〃 (x,x0))]3/2,
α,α0	α,α0
where Zl = maxαoo ι 1 仍.Using the inequality (20) with S = 3/2 and r = 1, we have
zα,α0
X[zal ,a0(1 - cla,a0(x,x0))]3/2 ≤ (Xzal ,a0(1 - cla,a0(x,x0)))3/2
a,a0	a,a0
= (Xzal ,a0 - ccl (x, x0)))3/2 .
a,a0
Moreover, using the concavity of the square root function, we have Pa,a0 zal ,a0 ≤ 1. This
yields
ccl (x, x0) ≤ ccl-1 (x, x0) + Z(1 - ccl-1 (x, x0))3/2,
where Z is constant. Letting γl = 1 - ccl (x, x0), we can conclude using the following
inequality (we had an equality in the case of FFNN)
Yl ≥ Yl-1 - ζY3-1
which leads to
Y-1/2 ≤ Y-T(I- ZYl1-2)T∕2=Y-1ι/2 + 2 + o(1).
Hence we have
Yl & l-2 .
Using this result combined with (19) again, we conclude that
1 - cl (x, x0) & l-2 .
□
E Theoretical analysis of Magnitude Based Pruning (MBP)
In this section, we provide a theoretical analysis of MBP. The two approximations from Appendix A
are not used here.
MBP is a data independent pruning algorithm (zero-shot pruning). The mask is given by
l = 1	if |Wil| ≥ts,
i = 0	if |Wil| <ts,
31
Published as a conference paper at ICLR 2021
where ts is a threshold that depends on the sparsity s. By defining ks = (1 - s) l Ml, ts is given
by ts = |W ∣(ks) where |W ∣(ks) is the kth order statistic of the network weights (|W/ ∣)ι≤ι≤L,ι≤i≤Mι
(|W |(1) > |W |(2) > ...).
With MBP, changing σw does not impact the distribution of the resulting sparse architecture since
it is a common factor for all the weights. However, in the case of different scaling factors vl, the
variances σw Used to initialize the weights vary across layers. This gives potentially the erroneous
intuition that the layer with the smallest variance will be highly likely fully pruned before others as
we increase the sparsity s. This is wrong in general since layers with small variances might have
more weights compared to other layers. However, we can prove a similar result by considering the
limit of large depth with fixed widths.
Proposition 4 (MBP in the large depth limit). Assume N is fixed and there exists l0 ∈ [|1, L|]
SUCh that aio > α for all l = lo. Let Qx be the Xth quantile of X | where X iid N(0,1) and
Y = minι=ι0 翳.For e ∈ (0, 2), define xe,γ = inf{y ∈ (0,1) : ∀x > y, γQx > Q1-(1-x)γ2- }
and x,γ = ∞ for the null set. Then, for all ∈ (0, 2), x,γ is finite and there exists a Constant ν > 0
suCh that
E[scr] ≤ e∈inf2){xe,γ + l+0 γ2-e (I - xe,γ )1+Y	} + O( √LN2 ).
Proposition 4 gives an upper bound on E[scr] in the large depth limit. The upper bound is easy
to approximate numerically. Table 7 compares the theoretical upper bound in Proposition 4 to the
empirical value of E[scr] over 10 simulations for a FFNN with depth L = 100, N = 100, α1 = γ
and α2 = α3 = … = αL = 1. Our experiments reveal that this bound can be tight.
Table 7: Theoretical upper bound of Proposition 4 and empirical observations for a FFNN with
N= 100andL= 100
Gamma	γ=2	γ=5	γ= 10
Upper b ound	5.77	0.81	0.72
Empirical ob s ervation	≈1	0.79	0.69
Proof. Let x ∈ (0, 1) and kx = (1 - x)ΓLN2, where ΓL = Pi6=i ζi. We have
P(scr ≤ x) ≥ P(max |Wii0 | < |W|(kx)),
where |W |(kx) is the kxth order statistic of the sequence {|Wii |, l 6= l0, i ∈ [1 : Mi]}; i.e
|W |(1) > |W |(2) > ... > |W|(kx).
Let (Xi)i∈[i:MlO] and (Zi)i∈[iτLN2] be two sequences of iid standard normal variables. It is easy to
see that
P(max ∣Wl01 < |W∣(kχ)) ≥ P(max |Xi| < γ∣Z∣(kχ))
i,j ij	i
where Y = minι=ι口 答∙
Moreover, we have the following result from the theory of order statistics, which is a weak version of
Theorem 3.1. in Puri and Ralescu (1986)
Appendix Lemma 3. Let X1, X2, ..., Xn be iid random variables with a Cdf F. Assume F is
differentiable and let p ∈ (0, 1) and let Qp be the order p quantile of the distribution F, i.e.
F(Qp) = p. Then we have
√n(X(Pn)- Qp)F0(Qp)σ-1 → N(0,1),
where the ConvergenCe is in distribution and σp = p(1 - p).
32
Published as a conference paper at ICLR 2021
Using this result, we obtain
P(max Xi| < Y|Z|(kx)) = P(max Xi| < YQx) + O(√==),
i	i	LN2
where Qx is the x quantile of the folded standard normal distribution.
The next result shows that x,γ is finite for all ∈ (0, 2).
Appendix Lemma 4. Let γ > 1. For all ∈ (0, 2), there exists x ∈ (0, 1) such that, for all x > x,
γQx > Q1-(1-x)γ2-.
Proof. Let > 0, and recall the asymptotic equivalent of Q1-x given by
Qι-x 〜x→0 √-2log(x)
Therefore, Q YQx C 一 〜x→1 √Y > 1. Hence Xe exists and is finite.	□
Q1-(1-x)γ2-
Let > 0. Using Appendix Lemma 4, there exists xe > 0 such that
P(max |Xi| < γQx) ≥ P(max |Xi| < Q1-(1-x)γ2-)
= (1 - (1 - x)γ2-)ζl0N2
≥ 1 - ζl0N2(1 - x)γ-,
where we have used the inequality (1 -t)z ≥ 1 -zt for all (t, z) ∈ [0, 1] × (1, ∞) and β = αl0αl0+1.
Using the last result, we have
P(Scr ≥ X) ≤ βN 2(1 — X)γ2-e + O( √Lp ).
Now we have
E[Scr] =	P(Scr ≥ X)dX
0
≤ Xe +	P(Scr ≥ X)dX
x
≤ Xe + ι + γ2-e (1 — Xe )Y	+1+ O( √LN2).
This is true for all e ∈ (0,2), and the additional term O(√=2) does not depend on e. Therefore
there exists a constant ν ∈ R such that for all
E[Scr] ≤ Xe + 1 + γ2-e (1 — Xe)Y +1 + √LN2 .
We conclude by taking the infimum over .
□
Another interesting aspect of MBP is when the depth is fixed and the width goes to infinity. The next
result gives a lower bound on the probability of pruning at least one full layer.
Proposition 5 (MBP in the large width limit). Assume there exists l0 ∈ [1 : L] such that αl0 > αl
(i.e. vio > Vl)for all l, and let so = PMM工.For some sparsity s, let PR* (s) be the event that layer
l0 is fully pruned before other layers, i.e.
PRIO (S) = {|Alo | = Mlo } ∩l∈[1:LNIAl | < Mlh
and let PRl0 = ∪s∈(s0,smax)PRl0 (s) be the event where there exists a sparsity s such that layer l0 is
fully pruned before other layers. Then, we have
Lπ2	1
(l0) ≥	— 4(γ - 1)2 log(N)2 十 °(log(N)2),
Where Y = mink=l0 签.
αk
33
Published as a conference paper at ICLR 2021
Proposition 5 shows that when the width is not the same for all layers, MBP will result in one layer
being fully pruned with a probability that converges to 1 as the width goes to infinity. The larger the
ratio γ (ratio of widths between the largest and the second largest layers), the faster this probability
goes to 1.
The intuition behind Proposition 5 comes from a result in Extreme Value Theory stated in Appendix
Lemma 6. Indeed, the problem of pruning one whole layer before the others is essentially a problem
of maxima: we prune one whole layer l0 before the others if and only if for all maxi |Wil0 | <
minl6=l0 maxi |Wil |. The expected value of n iid standard Gaussian variables is known to scale as
√logn for large n; see e.g. Van Handel (2016).
The proof of Proposition 5 relies on the following two auxiliary results.
Appendix Lemma 5 (Rearrangement inequality (Hardy et al., 1952)). Let f,g : R → R+ be
functions which are either both non-decreasing or non-increasing and let X be a random variable.
Then
E[f(X)g(X)] ≥ E[f(X)]E[g(X)].
Appendix Lemma 6 (Von Mises (1936)). Let (Xi)1≤i≤n be iid random variables with common
density f and cumulative distribution function F. Assume limχ→F-i(i)( ~d (IfF(X))) = 0, then
limn→∞ P(maxi Xi ≤ anx + bn) = G(x) where G is the Gumbel cumulative distribution function
and SerieS a。and bn are given by bn = F-1(1 一 1) and a。=η会).
We are now in a position to prove Proposition 5.
Proof. Assume there exists l0 ∈ [1 : L] such that αl0 > αl for all l. The trick is to see that
P Rl0 = {∀k 6= l0 , max |Wil0 | < max |Wik |}.
0	i i	ij i
Let us prove that
P(P Rl0) ≥ Y P(max |Wil0| < max |Wik|).
ij
k6=l0
Let X = maxi |Wil0 |. We have that
P(P Rl0) = E[	P(X < max |Wik||X)]
k6=l0
using the rearrangement inequality presented in Appendix Lemma 5 with functions fi(x) = P(X <
maxi |Wik ||X = x) which are all non-increasing, we obtain
P(P Rl0) ≥	E[P(X < max|Wik||X)] =	P(max |Wil0 | < max|Wik|).
0	ii	ii	ii
k6=l0
k6=l0
In order to deal with the probability P(maxi |Wil0 | < maxi |Wik|), we use Appendix Lemma 6 which
is a result from Extreme Value Theory which provides a comprehensive description of the law of
maxi Xi needed in our analysis. In our case, we want to characterise the behaviour of maxi |Xi |
where Xi are iid Gaussian random variables.
Let Ψ and ψ be the cdf and density of a standard Gaussian variable X . The cdf of |X | is given by
F = 2Ψ — 1 and its density is given by f = 2ψ on the positive real line. Thus, 1-F = 1-ψ and it is
sufficient to verify the conditions of Appendix Lemma 6 for the standard Gaussian distribution. We
have limχ→F-i(i)於 1ψψ(χ) = limχ→F-i(i)X(1ψψ(X)) - 1 = x/x - 1 = 0, where We have used
the fact that χ(1 — Ψ(x))〜φ(x) in the large X limit.
Let us now find the values of an and bn . In the large x limit, we have
t2
∞ e-ɪ 一
1 — F(x) = 2 /	____dt
Jx	√2∏
π 一χ2,1 .	1	.	, 1、、
2e 2 (X + xs + o(χ3)).
34
Published as a conference paper at ICLR 2021
Therefore, one has
log(1- F(x))〜一χ22.
This yields
bn = F-1(1 - 1)〜p2 log n.
n
Using the same asymptotic expansion of 1 - F (x), we can obtain a more precise approximation of bn
∣	/o——∕1	log(log n)	2 log(4)	log(log n)	Jog(log n)
bn = Vz2百(1- -4iog^ + W/ -9布2 + o(EF)).
Now let us find an approximation for an . We have
,,,、√2 「、——
ψ(bn) 〜∏ k.
Therefore, it follows that
π
an 〜—’	.
√2 log n
We use these results to lower bound the probability P(maxi |Wil0 | < maxi |Wik|). We have
P(max |Wil0 | ≥ max |Wik |) = P(max |Xi| ≥ γk max |Yi|),
ii	i	i	i
where Yk = 00 and (Xi) and (Yi) are standard Gaussian random variables. Note that Yk > 1. Let
AN = maxi |Xi | and BN = maxi |Yi |. We have that
P(AN ≥ YkBN) = P(AN - E[AN] ≥ Yk(BN - E[BN]) + YkE[BN] - E[AN])
≤ El__________IAN - E[AN])2____________]〜 __________π________
≤ L(Yk (BN - E[Bn ])+ YkE[BN ] - E[An ]))2j N →∞ 4(Yk - 1)2 Iog(N )2'
We conclude that for large N
Lπ2	1
P(PRl0)≥1
- 4(γ - 1)2log(N)2 +o(
Iog(N)2 ),
where Y = min®=1。00 ∙	□
F	ImageNet Experiments
To validate our results on large scale datasets, we prune ResNet50 using SNIP, GraSP, SynFlow and
our algorithm SBP-SR, and train the pruned network on ImageNet. We train the pruned model for
90 epochs with SGD. The training starts with a learning rate 0.1 and it drops by a factor of 10 at
epochs 30, 60, 80. We report in table 8 Top-1 test accuracy for different sparsities. Our algorithm
SBP-SR has a clear advantage over other algorithms. We are currently running extensive simulations
on ImageNet to confirm these results.
Table 8: Classification accuracy on ImageNet (Top-1) for ResNet50 with varying sparsities (TODO: These
results will be updated to include confidence intervals)
Algorithm	85%	90%	95%
SNIP	69.05	64.25	44.90
GRASP	69.45	66.41	62.10
SynFlow	69.50	66.20	62.05
SBP-SR	69.75	67.02	62.66
35
Published as a conference paper at ICLR 2021
Table 9: Test accuracy of pruned neural network on CIFAR10 with different activation functions
Resnet32	Algo	90	98	99.5	99.9
Relu	SBP-SR	92.56(0.06)	88.25(0.35)	79.54(1.12)	51.56(1.12)
	SNIP	92.24(0.25)	87.63(0.16)	77.56(0.36)	10(0)
Tanh	SBP-SR	90.97(0.2)	86.62(0.38)	75.04(0.49)	51.88(0.56)
	SNIP	90.69(0.28)	85.47(008)	10(0)	10(0)
Resnet50 RelU	SBP-SR	92.05(0.06)	89.57(0.21)	82.68(0.52)	58.76(1.82)
	SNIP	91.64(0.14)	89.20(0.54)	80.49(2.41)	19.98(14.12)
Tanh	SBP-SR	90.43(0.32)	88.18(0.10)	80.09(0.0.55)	58.21(1.61)
	SNIP	89.55(0.10)	10(0)	10(0)	10(0)
G	Additional Experiments
In Table 10, we present additional experiments with varying Resnet Architectures (Resnet32/50), and
sparsities (up to 99.9%) with Relu and Tanh activation functions on Cifar10. We see that overall,
using our proposed Stable Resnet performs overall better that standard Resnets.
In addition, we also plot the remaining weights for each layer to get a better understanding on the
different pruning strategies and well as understand why some of the Resnets with Tanh activation
functions are untrainable. Furthermore, we added additional MNIST experiments with different
activation function (ELU, Tanh) and note that our rescaled version allows us to prune significantly
more for deeper networks.
Figure 4: Percentage of pruned weights per layer in a ResNet32 for our scaled ResNet32 and standard
Resnet32 with Kaiming initialization
36
Published as a conference paper at ICLR 2021
(b) ELU with EOC Init
(c) ELU with Ordered phase
Init
(a) ELU with EOC Init &
Rescaling
(e) Tanh with EOC Init
(d) Tanh with EOC Init &
Rescaling
Figure 5: Accuracy on MNIST with different initialization schemes including EOC with rescaling,
EOC without rescaling, Ordered phase, with varying depth and sparsity. This figure clearly illustrates
the benefits of rescaling very deep and sparse FFNN.
(f) Tanh with Ordered phase
Init
Table 10: Test accuracy of pruned vanilla-CNN on CIFAR10 with different depth/sparsity levels
Resnet32	Algo	90	98	99.5	99.9
Relu	SBP-SR	92.56(0.06)	88.25(0.35)	79.54(1.12)	51.56(1.12)
	SNIP	92.24(0.25)	87.63(0.16)	77.56(0.36)	10(0)
Tanh	SBP-SR	90.97(0.2)	86.62(0.38)	75.04(0.49)	51.88(0.56)
	SNIP	90.69(0.28)	85.47(0.18)	10(0)	10(0)
Resnet50 RelU	SBP-SR	92.05(0.06)	89.57(0.21)	82.68(0.52)	58.76(1.82)
	SNIP	91.64(0.14)	89.20(0.54)	80.49(2.41)	19.98(14.12)
Tanh	SBP-SR	90.43(0.32)	88.18(0.10)	80.09(0.0.55)	58.21(1.61)
	SNIP	89.55(0.10)	10(0)	10(0)	10(0)
37
Published as a conference paper at ICLR 2021
H	On The Lottery Ticket Hypothesis
The Lottery Ticket Hypothesis (LTH) (Frankle and Carbin, 2019) states that “randomly initialized
networks contain subnetworks that when trained in isolation reach test accuracy comparable to the
original network”. We have shown so far that pruning a NN initialized on the EOC will output
sparse NNs that can be trained after rescaling. Conversely, if we initialize a random NN with any
hyperparameters (σw, σb), then intuitively, we can prune this network in a way that ensures that
the pruned NN is on the EOC. This would theoretically make the sparse architecture trainable. We
formalize this intuition as follows.
Weak Lottery Ticket Hypothesis (WLTH): For any randomly initialized network, there exists a
subnetwork that is initialized on the Edge of Chaos.
In the next theorem, we prove that the WLTH is true for FFNN and CNN architectures that are
initialized with Gaussian distribution.
Theorem 3. Consider a FFNN or CNN with layers initialized with variances σw2 > 0 for weights
and variance σb2 for bias. Let σw,EOC be the value of σw such that (σw,EOC, σb) ∈ EOC. Then, for
all σw > σw,EOC, there exists a subnetwork that is initialized on the EOC. Therefore WLTH is true.
The idea behind the proof of Theorem 3 is that by removing a fraction of weights from each layer, we
are changing the covariance structure in the next layer. By doing so in a precise way, we can find a
subnetwork that is initialized on the EOC.
We prove a slightly more general result than the one stated.
Theorem 4 (Winning Tickets on the Edge of Chaos). Consider a neural network with layers ini-
tialized with variances σw,l ∈ R+ for each layer and variance σb > 0 for bias. We define σw,EOC
to be the value of σw such that (σw,EOC, σb) ∈ EOC. Then, for all sequences (σw,l)l such that
σw,l > σw,EOC for all l, there exists a distribution of subnetworks initialized on the Edge of Chaos.
Proof. We prove the result for FFNN. The proof for CNN is similar. Let x, x0 be two inputs. For all
l, let (δl)ij be a collection of Bernoulli variables with probability pl. The forward propagation of the
covariance is given by
ql(x, χ0) = E[yi(χ)yi(χ0)]
Nl-1
=E[X WjWilkδjδikφ(yjT(x))φ(yjT(x0))] + σb
j,k
This yields
ql(χ,χ0) = σW ,lplE[φ(y1-1(χ))φ(y1-1(χ0))] + σ2.
σ2
By choosing Pl = w,E , this becomes
σ
w,l
ql(X, χ0) = σW,EOCE[φ(y1-1(χ))φ(y1-1(χ0))] + σ2.
Therefore, the new variance after pruning with the Bernoulli mask δ is σW = σW EOC. Thus, the
subnetwork defined by δ is initialized on the EOC. The distribution of these subnetworks is directly
linked to the distribution of δ. We can see this result as layer-wise pruning, i.e. pruning each layer
aside. The proof is similar for CNNs.	□
Theorem 3 is a special case of the previous result where the variances σw,l are the same for all layers.
38
Published as a conference paper at ICLR 2021
I Algorithm for section 2.3
Algorithm 1 Rescaling trick for FFNN
Input: Pruned network, size m
for L = 1 to L do
for i = 1 to Nl do
αi - PN=-I(Wij)2δij
Pij - 1∕pαi for all j
end for
end for
39