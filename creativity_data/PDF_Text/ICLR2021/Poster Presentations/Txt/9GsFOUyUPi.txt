Published as a conference paper at ICLR 2021
Progressive Skeletonization: Trimming more
FAT FROM A NETWORK AT INITIALIZATION
Pau de Jorge* *
University of Oxford &
NAVER LABS EUroPet
Amartya Sanyal	Harkirat S. Behl
University of Oxford &	University of Oxford
The Alan TUring InstitUte, London, UK
Philip H. S. Torr
University of Oxford
Gregory Rogez
NAVER LABS EUroPe
Puneet K. Dokania
University of Oxford &
Five AI Limited
Ab stract
Recent stUdies have shown that skeletonization (PrUning Parameters) of networks
at initialization Provides all the Practical benefits of sParsity both at inference and
training time, while only marginally degrading their Performance. However, we
observe that beyond a certain level of sParsity (aPProx 95%), these aPProaches
fail to Preserve the network Performance, and to oUr sUrPrise, in many cases
Perform even worse than trivial random PrUning. To this end, we ProPose an
objective to find a skeletonized network with maximUm foresight connection
sensitivity (FORCE) whereby the trainability, in terms of connection sensitivity, of
a PrUned network is taken into consideration. We then ProPose two aPProximate
ProcedUres to maximize oUr objective (1) Iterative SNIP: allows Parameters that
were UnimPortant at earlier stages of skeletonization to become imPortant at later
stages; and (2) FORCE: iterative Process that allows exPloration by allowing
already PrUned Parameters to resUrrect at later stages of skeletonization. EmPirical
analysis on a large sUite of exPeriments show that oUr aPProach, while Providing at
least as good a Performance as other recent aPProaches on moderate PrUning levels,
Provide remarkably imProved Performance on higher PrUning levels (coUld remove
UP to 99.5% Parameters while keePing the networks trainable).
1	Introduction
The majority of PrUning algorithms for DeeP NeUral Networks reqUire training dense models and
often fine-tUning sParse sUb-networks in order to obtain their PrUned coUnterParts. In Frankle &
Carbin (2019), the aUthors Provide emPirical evidence to sUPPort the hyPothesis that there exist
sParse sUb-networks that can be trained from scratch to achieve similar Performance as the dense
ones. However, their method to find sUch sUb-networks reqUires training the fUll-sized model and
intermediate sUb-networks, making the Process mUch more exPensive.
Recently, Lee et al. (2019) Presented SNIP. BUilding UPon almost a three decades old saliency criterion
for PrUning trained models (Mozer & Smolensky, 1989), they are able to Predict, at initialization,
the imPortance each weight will have later in training. PrUning at initialization methods are mUch
cheaPer than conventional PrUning methods. Moreover, while traditional PrUning methods can helP
accelerate inference tasks, PrUning at initialization may go one steP fUrther and Provide the same
benefits at train time Elsen et al. (2020).
Wang et al. (2020) (GRASP) noted that after aPPlying the PrUning mask, gradients are modified dUe to
non-trivial interactions between weights. ThUs, maximizing SNIP criterion before PrUning might be
sUb-oPtimal. They Present an aPProximation to maximize the gradient norm after PrUning, where they
treat PrUning as a PertUrbation on the weight matrix and Use the first order Taylor’s aPProximation.
While they show imProved Performance, their aPProximation involves comPUting a Hessian-vector
ProdUct which is exPensive both in terms of memory and comPUtation.
* Correspondence to pau@robots.ox.ac.uk
*www.europe.naverlabs.com
1
Published as a conference paper at ICLR 2021
Figure 1: Test accuracies on CIFAR-10 (ResNet50) for different pruning methods. Each point is the
average over 3 runs of prune-train-test. The shaded areas denote the standard deviation of the runs
(too small to be visible in some cases). Random corresponds to removing connections uniformly.
We argue that both SNIP and GRASP approximations of the gradients after pruning do not hold
for high pruning levels, where a large portion of the weights are removed at once. In this work,
while we rely on the saliency criteria introduced by Mozer & Smolensky (1989), we optimize what
this saliency would be after pruning, rather than before. Hence, we name our criteria Foresight
Connection sEnsitivity (FORCE). We introduce two approximate procedures to progressively optimize
our objective. The first, which turns out to be equivalent to applying SNIP iteratively, removes a
small fraction of weights at each step and re-computes the gradients after each pruning round. This
allows to take into account the intricate interactions between weights, re-adjusting the importance of
connections at each step. The second procedure, which we name FORCE, is also iterative in nature,
but contrary to the first, it allows pruned parameters to resurrect. Hence, it supports exploration,
which otherwise is not possible in the case of iterative SNIP. Moreover, one-shot SNIP can be viewed
as a particular case of using only one iteration. Empirically, we find that both SNIP and GRASP have
a sharp drop in performance when targeting higher pruning levels. Surprisingly, they perform even
worse than random pruning as can be seen in Fig 1. In contrast, our proposed pruning procedures
prove to be significantly more robust on a wide range of pruning levels.
2	Related work
Pruning trained models Most of the pruning works follow the train - prune - fine-tune cycle
(Mozer & Smolensky, 1989; LeCun et al., 1990; Hassibi et al., 1993; Han et al., 2015; Molchanov
et al., 2017; Guo et al., 2016), which requires training the dense network until convergence, followed
by multiple iterations of pruning and fine-tuning until a target sparsity is reached. Particularly,
Molchanov et al. (2017) present a criterion very similar to Mozer & Smolensky (1989) and therefore
similar to Lee et al. (2019) and our FORCE, but they focus on pruning whole neurons, and involve
training rounds while pruning. Frankle & Carbin (2019) and Frankle et al. (2020) showed that it was
possible to find sparse sub-networks that, when trained from scratch or an early training iteration,
were able to match or even surpass the performance of their dense counterparts. Nevertheless, to find
them they use a costly procedure based on Han et al. (2015). All these methods rely on having a
trained network, thus, they are not applicable before training. In contrast, our algorithm is able to find
a trainable sub-network with randomly initialized weights. Making the overall pruning cost much
cheaper and presenting an opportunity to leverage the sparsity during training as well.
Induce sparsity during training Another popular approach has been to induce sparsity during
training. This can be achieved by modifying the loss function to consider sparsity as part of the
optimization (Chauvin, 1989; Carreira-Perpindn & Idelbayev, 2018; Louizos et al., 2018) or by
dynamically pruning during training (Bellec et al., 2018; Mocanu et al., 2018; Mostafa & Wang,
2019; Dai et al., 2019; Dettmers & Zettlemoyer, 2020; Lin et al., 2020; Kusupati et al., 2020; Evci
et al., 2019). These methods are usually cheaper than pruning after training, but they still need to
train the network to select the final sparse sub-network. We focus on finding sparse sub-networks
before any weight update, which is not directly comparable.
Pruning at initialization These methods present a significant leap with respect to other pruning
methods. While traditional pruning mechanisms focused on bringing speed-up and memory reduction
at inference time, pruning at initialization methods bring the same gains both at training and inference
time. Moreover, they can be seen as a form of Neural Architecture Search (Zoph & Le, 2016) to
find more efficient network topologies. Thus, they have both a theoretical and practical interest.
2
Published as a conference paper at ICLR 2021
Lee et al. (2019) presented SNIP, a method to estimate, at initialization, the importance that each
weight could have later during training. SNIP analyses the effect of each weight on the loss function
when perturbed at initialization. In Lee et al. (2020), the authors studied pruning at initialization
from a signal propagation perspective, focusing on the initialization scheme. Recently, Wang et al.
(2020) proposed GRASP, a different method based on the gradient norm after pruning and showed a
significant improvement for higher levels of sparsity. However, neither SNIP nor GRASP perform
sufficiently well when larger compressions and speed-ups are required and a larger fraction of the
weights need to be pruned. In this paper, we analyse the approximations made by SNIP and GRASP,
and present a more suitable solution to maximize the saliency after pruning.
3	Problem Formulation: Pruning at Initialization
Given a dataset D = {(xi, yi)}in=1, the training of a neural network f parameterized by θ ∈ Rm can
be written as minimizing the following empirical risk:
arg min 1 X L((fM; θ)), Yi st θ ∈ C,	(1)
θn
i
where L and C denote the loss function and the constraint set, respectively. Unconstrained (standard)
training corresponds to C = Rm . Assuming we have access to the gradients (batch-wise) of the
empirical risk, an optimization algorithm (e.g. SGD) is generally used to optimize the above objective,
that, during the optimization process, produces a sequence of iterates {θi }iT=0, where θ0 and θT
denote the initial and the final (optimal) parameters, respectively. Given a target sparsity level of
k < m, the general parameter pruning problem involves C with a constraint kθT k0 ≤ k, i.e., the final
optimal iterate must have a maximum of k non-zero elements. Note that there is no such constraint
with the intermediate iterates.
Pruning at initialization, the main focus of this work, adds further restrictions to the above mentioned
formulation by constraining all the iterates to lie in a fixed subspace ofC. Precisely, the constraints are
to find an initialization θo such that kθoko ≤ k 1, and the intermediate iterates are θi ∈C ⊂ C, ∀i ∈
{1,..., T}, where C is the subspace of Rm spanned by the natural basis vectors {ej}j∈supp(θo).
Here, supp(θ0) denotes the support of θ0, i.e., the set of indices with non-zero entries. The first
condition defines the sub-network at initialization with k parameters, and the second fixes its topology
throughout the training process. Since there are mk such possible sub-spaces, exhaustive search
to find the optimal sub-space to optimize (1) is impractical as it would require training mk neural
networks. Below we discuss two recent approaches that circumvent this problem by maximizing a
hand-designed data-dependent objective function. These objectives are tailored to preserve some
relationships between the parameters, the loss, and the dataset, that might be sufficient to obtain a
reliable θ0. For the ease of notation, we will use θ to denote the dense initialization.
SNIP Lee et al. (2019) present a method based on the saliency criterion from Mozer & Smolensky
(1989). They add a key insight and show this criteria works surprisingly well to predict, at initializa-
tion, the importance each connection will have during training. The idea is to preserve the parameters
that will have maximum impact on the loss when perturbed. Let c ∈ {0, 1}m be a binary vector, and
the Hadamard product. Then, the connection sensitivity in SNIP is computed as:
g(θ)：=也产 =喊Q θ.	⑵
∂c	c=1	∂θ
Once g(θ) is obtained, the parameters corresponding to the top-k values of ∣g(θ)i∣ are then kept.
Intuitively, SNIP favors those weights that are far from the origin and provide high gradients
(irrespective of the direction). We note that SNIP objective can be written as the following problem:
maxS(θ, c) := X	∣θi VL(θ)i∣ s.t. c ∈ {0,1}m, ∣∣c∣∣o = k.	(3)
c0
i∈supp(c)
It is trivial to note that the optimal solution to the above problem can be obtained by selecting the
indices corresponding to the top-k values of ∣θi VL(θ)/.
1In practice, as will be done in this work as well, a subset of a given dense initialization is found using some
saliency criterion (will be discussed soon), however, note that our problem statement is more general than that.
3
Published as a conference paper at ICLR 2021
GRASP Wang et al. (2020) note that the SNIP saliency is measuring the connection sensitivity of
the weights before pruning, however, it is likely to change after pruning. Moreover, they argue that, at
initialization, it is more important to preserve the gradient signal than the loss itself. They propose to
use as Saliency the gradient norm of the loss ∆L(θ) = VL(θ)T VL(θ), but measured after pruning.
To maximize it, Wang et al. (2020) adopt the same approximation introduced in LeCun et al. (1990)
and treat pruning as a perturbation on the initial weights. Their method is equivalent to solving:
max G(θ, c) :=	X -θi [Hg]i s.t. c ∈ {0, 1}m, kck0 = k.	(4)
{i: ci=0}
Where H and g denote the Hessian and the gradient of the loss respectively.
4 Foresight Connection Sensitivity
Since removing connections of a neural network will have significant impact on its forward and
backward signals, we are interested in obtaining a pruned network that is easy to train. We use
connection sensitivity of the loss function as a proxy for the so-called trainability of a network. To
this end, we first define connection sensitivity after pruning which we name Foresight Connection
sEnsitivity (FORCE), and then propose two procedures to optimize it in order to obtain the desired
pruned network. Let θ = θ Θ C denotes the pruned parameters once a binary mask C with ∣∣cko =
k ≤ m is applied. The FORCE at θ for a given mask C is then obtained as:
g(θ) :
∂L(θ) I
dc lc=c
∂L(θ)	. ∂θ
∂θ	ʌ ∂ ∂c	一
c=c	c=c
∂L(θ)∣
∂ θ Ic=^
Θ θ.
(5)
The last equality is obtained by rewriting θ as diag(θ)c, where diag(θ) is a diagonal matrix with θ
as its elements, and then differentiating w.r.t. c. Note, when k < m, the sub-network θ is obtained by
removing connections corresponding to all the weights for which the binary variable is zero. Therefore,
only the weights corresponding to the indices for which C(i) = 1 contribute in equation (5), all other
weights do not participate in forward and backward propagation and are to be ignored. We now
discuss the crucial differences between our formulation (5), SNIP (2) and GRASP (4).
•	When C= 1, the formulation is exactly the same as the connection sensitivity used in SNIP.
However, C= 1 is too restrictive in the sense that it assumes that all the parameters are
active in the network and they are removed one by one with replacement, therefore, it fails
to capture the impact of removing a group of parameters.
•	Our formulation uses weights and gradients corresponding to θ thus, compared to SNIP,
provides a better indication of the training dynamics of the pruned network. However,
GRASP formulation is based on the assumption that pruning is a small perturbation on the
Gradient Norm which, also shown experimentally, is not always a reliable assumption.
•	When ∣∣Cko《∣∣1∣∣0, i.e., extreme pruning, the gradients before and after pruning will
have very different values as ∣∣θ Θ C∣b《∣∣θ∣∣2, making SNIP and GRASP unreliable
(empirically we find SNIP and GRASP fail in the case of high sparsity).
FORCE saliency Note FORCE (5) is defined for a given sub-network which is unknown a priori, as
our objective itself is to find the sub-network with maximum connection sensitivity. Similar to the
reformulation of SNIP in (3), the objective to find such sub-network corresponding to the foresight
connection sensitivity can be written as:
max S(θ, c) :=	^X	∣θi VL(θ Θ c)/ s.t. c ∈ {0,1}m, ∣∣c∣∣o = k.	(6)
i∈supp(c)
Here VL(θ Θ c) represents the i-th index of d∂θ) ∣ . As opposed to (3), finding the optimal solution
of (6) is non trivial as it requires computing the gradients of all possible mk sub-networks in order
to find the one with maximum sensitivity. To this end, we present two approximate solutions to
the above problem that primarily involve (i) progressively increasing the degree of pruning, and (ii)
solving an approximation of (6) at each stage of pruning.
Progressive Pruning (Iterative SNIP) Let k be the number of parameters to be kept after pruning.
Let us assume that we know a schedule (will be discussed later) to divide k into a set of natural
4
Published as a conference paper at ICLR 2021
numbers {kt }tT=1 such that kt > kt+1 and kT = k. Now, given the mask ct corresponding to kt,
pruning from kt to kt+1 can be formulated using the connection sensitivity (5) as:
Ct+1 = arg max S(θ, C) s.t. C ∈ {0,1}m, ∣∣c∣∣o = kt+ι, C Θ Ct = c,	(7)
c
where θ = θΘct. The additional constraint cΘct = C ensures that no parameter that had been pruned
earlier is activated again. Assuming that the Pruning schedule ensures a smooth transition from one
topology to another (k Ct k 0 ≈ ∣∣Ct+1k0)suchthatthe gradient approximation dL(θ^ I ≈ dLθ) I
ct	ct+1
is valid, (7) can be approximated as solving (3) at θ. Thus, for a given schedule over k, our first
approximate solution to (6) involves solving (3) iteratively. This allows re-assessing the importance
of connections after changing the sub-network. For a schedule with T = 1, we recover SNIP where a
crude gradient approximation between the dense network C0 = 1 and the final mask C is being used.
This approach of ours turns out to be algorithmically similar to a concurrent work (Verdenius et al.,
2020). However, our motivation comes from a novel objective function (5) which also gives place to
our second approach (FORCE). Tanaka et al. (2020) also concurrently study the effect of iterative
pruning and report, similar to our findings, pruning progressively is needed for high sparsities.
Progressive Sparsification (FORCE) The constraint C Θ Ct = C in (7) (Iterative SNIP) might be
restrictive in the sense that while re-assessing the importance of unpruned parameters, it does not
allow previously pruned parameters to resurrect (even if they could become important). This hinders
exploration which can be unfavourable in finding a suitable sub-network. Here we remove this
constraint, meaning, the weights for which C(i) = 0 are not removed from the network, rather they
are assigned a value of zero. Therefore, while not contributing to the forward signal, they might
have a non-zero gradient. This relaxation modifies the saliency in (5) whereby the gradient is now
computed at a sparsified network instead of a pruned network. Similar to the above approach, we
sparsify the network progressively and once the desired sparsity is reached, all connections with
C(i) = 0 are pruned. Note, the step of removing zero weights is valid if removing such connections
does not adversely impact the gradient flow of the unpruned parameters. We, in fact, found it to be
true in our experiments shown in Fig 7 (Appendix). However, this assumption might not hold always.
An overview of Iterative SNIP and FORCE is presented in Algorithm 1.
Sparsity schedule Both the above discussed iterative procedures approximately optimize (5), how-
ever, they depend on a sparsity/pruning schedule favouring small steps to be able to reliably apply the
mentioned gradient approximation. One such valid schedule would be where the portion of newly
removed weights with respect to the remaining weights is small. We find a simple exponential decay
schedule, defined below, to work very well on all our experiments:
ExPmode: kt = exp {a log k +(1 — α) log m} , α = ɪ.
(8)
In section 5.3 we empirically show that these methods are very robust to the hyperparameter T .
Some theoretical insights When pruning weights gradually, we are looking for the best possible
sub-network in a neighbourhood defined by the previous mask and the amount of weights removed at
that step. The problem being non-convex and non-smooth makes it challenging to prove if the mask
Algorithm 1 FORCE/Iter SNIP algorithms to find a pruning mask
1:	Inputs: Training set D, final sparsity k, number of steps T , weights θ0 ∈ Rm.
2:	Obtain {kt}t=1:T using the chosen schedule (refer to Eq (8))
3:	Define intial mask C0 = 1
4:	for t = 0, . . . , T — 1 do
5:	Sample mini-batch {zi}in=1 from D
6:	Define θθ = θ Θ Ct (as sparsified (FORCE) vs pruned (Iter SNIP) network)
7:	Compute g(θθ) (refer to Eq (5) )
8:	I = {i1, . . . , ikt+1} are top-kt+1 values of |gi|
9:	Build Ct+1 by setting to 0 all indices not included in I.
10:	end for
11:	Return: CT .
5
Published as a conference paper at ICLR 2021
obtained by our method is globally optimal. However, in Appendix D we prove that each intermediate
mask obtained with Iterative SNIP is indeed an approximate local minima, where the degree of
sub-optimality increases with the pruning step size. This gives some validation on why SNIP fails
on higher sparsity. We can not provide the same guarantees for FORCE (there is no obvious link
between the step size and the distance between masks), nevertheless, we empirically observe that
FORCE is quite robust and more often than not improves over the performance of Iterative SNIP,
which is not able to recover weights once pruned. We present further analysis in Appendix C.4.
5	Experiments
In the following we evaluate the efficacy of our approaches accross different architectures and datasets.
Training settings, architecture descriptions, and implementation details are provided in Appendix A.
5.1	RESULTS ON CIFAR- 1 0
Fig 2 compares the accuracy of the described iterative approaches with both SNIP and GRASP. We
also report the performance of a dense and a random pruning baseline. Both SNIP and GRASP
consider a single batch to approximate the saliencies, while we employ a different batch of data at
each stage of our gradual skeletonization process. For a fair comparison, and to understand how the
number of batches impacts performance, we also run these methods averaging the saliencies over T
batches, where T is the number of iterations. SNIP-MB and GRASP-MB respectively refer to these
multi-batch (MB) counterparts. In these experiments, we use T = 300. We study the hyper-parameter
robustness regarding T later in section 5.3.
(a) Resnet50	(b) VGG19
Figure 2: Test accuracies on CIFAR-10 for different pruning methods. With increased number of
batches (-MB) one-shot methods are more robust at higher sparsity levels, but our gradual pruning
approaches can go even further. Moreover, FORCE consistently reaches higher accuracy than other
methods across most sparsity levels. Each point is an average of ≥ 3 runs of prune-train-test. The
shaded areas denote the standard deviation of the runs (too small to be visible in some cases). Note
that in (b), GRASP and GRASP-MB overlap.
We observe that for moderate sparsity levels, one batch is sufficient for both SNIP and GRASP as
reported in Lee et al. (2019); Wang et al. (2020). However, as we increase the level of sparsity, the
performance of SNIP and GRASP degrades dramatically. For example, at 99.0% sparsity, SNIP
drops down to 10% accuracy for both ResNet50 and VGG19, which is equivalent to random guessing
as there are 10 classes. Note, in the case of randomly pruned networks, accuracy is nearly 75% and
82% for ResNet50 and VGG19, respectively, which is significantly better than the performance of
SNIP. However, to our surprise, just using multiple batches to compute the connection sensitivity
used in SNIP improves it from 10% to almost 90%. This clearly indicates that a better approximation
of the connection sensitivity is necessary for good performance in the case of high sparsity regime.
Similar trends, although not this extreme, can be observed in the case of GRASP as well. On the other
hand, gradual pruning approaches are much more robust in terms of sparsity for example, in the case
of 99.9% pruning, while one-shot approaches perform as good as a random classifier (nearly 10%
accuracy), both FORCE and Iterative SNIP obtain more than 80% accuracy. While the accuracies
obtained at higher sparsities might have degraded too much for some use cases, we argue this is an
encouraging result, as no approach before has pruned a network at initialization to such extremes
while keeping the network trainable and these results might encourage the community to improve the
performance further. Finally, gradual pruning methods consistently improve over other methods even
at moderate sparsity levels (refer to Fig 5), this motivates the use of FORCE or Iterative SNIP instead
of other methods by default at any sparsity regime. Moreover, the additional cost of using iterative
6
Published as a conference paper at ICLR 2021
pruning instead of SNIP-MB is negligible compared to the cost of training and is significantly cheaper
than GRASP-MB, further discussed in section 5.3.
Percentage of remaining weights (log scale)
(b) CIFAR 100 - VGG19
(a) CIFAR 100 - Resnet50
Percentage of remaining weights (log scale)
(c) Tiny Imagenet - Resnet50
(d) Tiny Imagenet - VGG19
Figure 3: Test accuracies on CIFAR-100 and Tiny Imagenet for different pruning methods. Each
point is the average over 3 runs of prune-train-test. The shaded areas denote the standard deviation of
the runs (too small to be visible in some cases).
5.2	Results on larger datasets
We now present experiments on large datasets. Wang et al. (2020) and Lee et al. (2019) suggest using
a batch of size 〜10 times the number of classes, which is Very large in these experiments. Instead,
for memory efficiency, we average the saliencies over several mini-batches. For CIFAR100 and
Tiny-ImageNet, we aVerage 10 and 20 batches per iteration respectiVely, with 128 examples per batch.
As we increase the number of batches per iteration, computing the pruning mask becomes more
expensiVe. From Fig 4, we obserVe that the accuracy conVerges after just a few iterations. Thus, for
the following experiments we used 60 iterations. For a fair comparison, we run SNIP and GRASP
with T × B batches, where T is the number of iterations and B the number of batches per iteration
in our method. We find the results, presented in Fig 3, consistent with trends in CIFAR-10.
In the case of Imagenet, we use a batch size of 256 examples and 40 batches per iteration. We use
the official implementation of VGG19 with batch norm and Resnet50 from Paszke et al. (2017).
As presented in Table 1, gradual pruning methods are consistently better than SNIP, with a larger
gap as we increase sparsity. We would like to emphasize that FORCE is able to prune 90% of the
weights of VGG while losing less than 3% of the accuracy, we find this remarkable for a method
that prunes before any weight update. Interestingly, GRASP performs better than other methods at
95% sparsity (VGG), moreoVer, it also slightly surpasses FORCE for Resnet50 at 90%, howeVer, it
under-performs random pruning at 95%. In fact, we find all other methods to perform worse than
random pruning for Resnet50. We hypothesize that, for a much more challenging task (Imagenet
with 1000 classes), Resnet50 architecture might not be extremely oVerparametrized. For instance,
Table 1: Test accuracies on Imagenet for different pruning methods and sparsities.
Network	VGG19				Resnet50			
Sparsity percentage Accuracy	90%		95%		90%		95%	
	Top-1	Top-5	Top-1	Top-5	Top-1	Top-5	Top-1	Top-5
(Baseline)	73.1	91.3			75.6	92.8		
FORCE (Ours)	70.2	89.5	65.8	86.8	64.9	86.5	59.0	82.3
Iter SNIP (Ours)	69.8	89.5	65.9	86.9	63.7	85.5	54.7	78.9
GRASP Wang et al. (2020)	69.5	89.2	67.6	87.8	65.4	86.7	46.2	66.0
SNIP Lee et al. (2019)	68.5	88.8	63.8	86.0	61.5	83.9	44.3	69.6
Random	64.2	86.0	56.6	81.0	64.6	86.0	57.2	80.8
7
Published as a conference paper at ICLR 2021
—90.0%
,8	98.0%
1—99.0%
6 — 99.5%
Algorithm	Time	ACC
SNIP (1 b)	-T4S~	10.0
GRASP (1 b)	47 s	19.2
FORCE (20 it)	57 s	87.9
Iter SNIP (20 it)	57 s	88.9
SNIP (300 b)	4.8 m	76.1
FORCE (300 it)	11.6m	90.0
Iter SNIP (300 it)	11.6m	89.0
GRASP (300 b)	50.7 m	88.5
Figure 4: Left: FORCE saliency (6) obtained with (7) when varying T normalized by the saliency
with one-shot SNIP (T = 1). Pruning iteratively brings more gains for higher sparsity levels. Error
bars not shown for better visualization. Middle: Test acc pruning with FORCE and Iter SNIP at
99.5% sparsity for different T . Both methods are extremely robust to the choice of T . Right: Wall
time to compute pruning masks for CIFAR10/Resnet50/TeslaK40m vs acc at 99.5% sparsity; (x b)
means we used x batches to compute the gradients while (x it) denotes we used x pruning iterations,
with one batch per iteration. Numbers in red indicate performance below random pruning.
VGG19 has 143.68M parameters while Resnet50 uses 25.56M (refer to Table 2). On the other hand,
the fact that random pruning can yield relatively trainable architectures for these sparsity levels is
somewhat surprising and might indicate that there still is room for improvement in this direction.
Results seem to indicate that the FORCE saliency is a step in the right direction and we hypothesize
further improvements on its optimization might lead to even better performance. In Appendix C.6,
we show superior performance of our approach on the Mobilenet-v2 architecture (Sandler et al.,
2018) as well, which is much more "slim" than Resnet and VGG 2.
5.3	Analysis
Saliency optimization To experimentally validate our approach (7), we conduct an ablation study
where we compute the FORCE saliency after pruning (5) while varying the number of iterations T
for different sparsity levels. In Fig 4 (left) we present the relative change in saliency as we vary the
number of iterations T, note when T = 1 we recover one-shot SNIP. As expected, for moderate levels
of sparsity, using multiple iterations does not have a significant impact on the saliency. Nevertheless,
as we target higher sparsity levels, we can see that the saliency can be better optimized when pruning
iteratively. In Appendix C.1 we include results for FORCE where we observe similar trends.
Hyperparameter robustness As shown in Figures 2 and 3, for low sparsity levels, all methods are
comparable, but as we move to higher sparsity levels, the gap becomes larger. In Fig 4 (middle)
we fix sparsity at 99.5% and study the accuracy as we vary the number of iterations T. Each point
is averaged over 3 trials. SNIP (T = 1) yields sub-networks unable to train (10% acc), but as we
move to iterative pruning (T > 1) accuracy increases up to 90% for FORCE and 89% for Iter SNIP.
Moreover, accuracy is remarkably robust to the choice of T, the best performance for both FORCE
and Iter SNIP is with more iterations, however a small number of iterations already brings a huge
boost. This suggests these methods might be used by default by a user without worrying too much
about hyper-parameter tuning, easily adapting the amount of iterations to their budget.
Pruning cost As shown in Fig 2, SNIP performance quickly degrades beyond 95% sparsity. Wang
et al. (2020) suggested GRASP as a more robust alternative, however, it needs to compute a Hessian
vector product which is significantly more expensive in terms of memory and time. In Fig 4
(right), we compare the time cost of different methods to obtain the pruning masks along with the
corresponding accuracy. We observe that both SNIP and GRASP are fragile when using only one
batch (red accuracy indicates performance below random baseline). When using multiple batches
their robustness increases, but so does the pruning cost. Moreover, we find that gradual pruning based
on the FORCE saliency is much cheaper than GRASP-MB when using equal amount of batches, this
is because GRASP involves an expensive Hessian vector product. Thus, FORCE (or Iterative SNIP)
would be preferable over GRASP-MB even when they have comparable accuracies.
FORCE vs Iterative SNIP Empirically, we find that FORCE tends to outperform Iter SNIP more
often than not, suggesting that allowing weights to recover is indeed beneficial despite having less
theoretical guarantees (see gradient approximation in Section 4). Thus, we would make FORCE
2Mobilenet has 2.3M params compared to 20.03M and 23.5M of VGG and Resnet, respectively.
8
Published as a conference paper at ICLR 2021
algorithm our default choice, especially for Resnet architectures. In Appendix C.4 we empirically
observe two distinct phases when pruning with FORCE. The first one involves exploration (early
phase) when the amount of pruned and recovered weights seem to increase, indicating exploration
of masks that are quite different from each other. The second, however, shows rapid decrease in
weight recovery, indicating a phase where the algorithm converges to a more constrained topology.
As opposed to Iter SNIP, the possibility of the exploration of many possible sub-networks before
converging to a final topology might be the reason behind the slightly improved performance of
FORCE. But this exploration comes at a price, in Fig 4 (middle and left) we observe how, despite
FORCE reaching a higher accuracy when using enough steps, if we are under a highly constrained
computational budget and can only afford a few pruning iterations, Iter SNIP is more likely to obtain
a better pruning mask. This is indeed expected as FORCE might need more iterations to converge
to a good sub-space, while Iter SNIP will be forced to converge by construction. A combination of
FORCE and Iter SNIP might lead to an even better approach, we leave this for future work.
Early pruning as an additional baseline Our gradual pruning approaches (SNIP-MB and GRASP-
MB as well) use multiple batches to obtain a pruned mask, considering that pruning can be regarded
as a form of training (Mallya et al., 2018), we create another baseline for the sake of completeness.
We train a network for one epoch, a similar number of iterations as used by our approach, and then
use magnitude pruning to obtain the final mask, we call this approach early pruning (more details in
Appendix C.5). Interestingly, we find that early pruning tends to perform worse than SNIP-MB (and
gradual pruning) for Resnet, and shows competitive performance at low sparsity level for VGG but
with a sharp drop in the performance as the sparsity level increases. Even though these experiments
support the superiority of our approach, we would like to emphasize that they do not conclude that
any early pruning strategy would be suboptimal compared to pruning at initialization as an effective
approach in this direction might require devising a well thought objective function.
Iterative pruning to maximize the Gradient Norm In Sec 4, we have seen Iterative SNIP can
be used to optimize the FORCE saliency. We also tried to use GRASP iteratively, however, after
a few iterations the resulting networks were not trainable. Interestingly, if we apply the gradient
approximation to GRASP saliency (instead of Taylor), we can come up with a different iterative
approximation to maximize the gradient norm after pruning. We empirically observe this method is
more robust than GRASP to high sparsity levels. This suggests that 1) Iterative pruning, although
beneficial, can not be trivially applied to any method. 2) The gradient approximation is more general
than in the context of FORCE/SNIP sensitivity. We present further details and results in Appendix E.
6	Discussion
Pruning at initialization has become an active area of research both for its practical and theoretical
interests. In this work, we discovered that existing methods mostly perform below random pruning at
extreme sparsity regime. We presented FORCE, a new saliency to compute the connection sensitivity
after pruning, and two approximations to progressively optimize FORCE in order to prune networks
at initialization. We showed that our methods are significantly better than the existing approaches
for pruning at extreme sparsity levels, and are at least as good as the existing ones for pruning at
moderate sparsity levels. We also provided theoretical insights on why progressive skeletonization
is beneficial at initialization, and showed that the cost of iterative methods is reasonable compared
to the existing ones. Although pruning iteratively has been ubiquitous in the pruning community,
it was not evident that pruning at initialization might benefit from this scheme. Particularly, not
every approximation could be used for gradual pruning as we have shown with GRASP. However,
the gradient approximation allowed us to gradually prune while maximizing either the gradient
norm or FORCE. We consider our results might encourage future work to further investigate the
exploration/exploitation trade-off in pruning and find more efficient pruning schedules, not limited to
the pruning at initialization.
Acknowledgments
This work was supported by the Royal Academy of Engineering under the Research Chair and Senior
Research Fellowships scheme, EPSRC/MURI grant EP/N019474/1 and Five AI Limited. Pau de
Jorge was fully funded by NAVER LABS Europe. Amartya Sanyal acknowledges support from
The Alan Turing Institute under the Turing Doctoral Studentship grant TU/C/000023. Harkirat was
supported using a Tencent studentship through the University of Oxford.
9
Published as a conference paper at ICLR 2021
References
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=BJ_wN01C-.
MigUel A. Carreira-Perpindn and Yerlan Idelbayev. “learning-compression" algorithms for neural
net pruning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. In Advances in
neural information processing Systems, pp. 519-526, 1989.
Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based on a
grow-and-prune paradigm. IEEE Transactions on Computers, 68(10):1487-1497, 2019.
Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance, 2020. URL https://openreview.net/forum?id=ByeSYa4KPS.
Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. Fast sparse convnets. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14629-14638,
2020.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. arXiv preprint arXiv:1911.11134, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJl-b3RcF7.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Stabilizing the
lottery ticket hypothesis, 2020.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Advances
in neural information processing systems, pp. 1379-1387, 2016.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In IEEE international conference on neural networks, pp. 293-299. IEEE, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images, 2009.
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham M.
Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. ArXiv,
abs/2002.03231, 2020.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.
Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip H. S. Torr. A signal propagation
perspective for pruning neural networks at initialization. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=HJeTo2VFwH.
10
Published as a conference paper at ICLR 2021
Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model
pruning with feedback. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SJem8lSFwB.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In International Conference on Learning Representations, 2018.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through
l0 regularization. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=H1Y8hhg0b.
Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple
tasks by learning to mask weights. In Proceedings of the European Conference on Computer
Vision (ECCV),pp. 67-82, 2018.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity
inspired by network science. Nature communications, 9(1):1-12, 2018.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. In International Conference on Learning Repre-
sentations, 2017.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization, 2019. URL https://openreview.net/forum?
id=S1xBioR5KX.
Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In Advances in Neural Information Processing Systems, 1989.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen. Mobilenetv2: Inverted residuals and
linear bottlenecks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 4510-4520, 2018. doi: 10.1109/CVPR.2018.00474.
Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 6377-6389. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf.
Stijn Verdenius, Maarten Stol, and Patrick Forra Pruning via iterative ranking of sensitivity statistics.
arXiv preprint arXiv:2006.00896, 2020.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkgsACVKPH.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
11
Published as a conference paper at ICLR 2021
A Pruning implementation details
We present experiments on CIFAR-10/100 (Krizhevsky et al., 2009), which consists of 60k 32×32
colour images divided into 10/100 classes, and also on Imagenet challenge ILSVRC-2012 (Rus-
sakovsky et al., 2015) and its smaller version Tiny-ImageNet, which respectively consist of 1.2M/1k
and 100k/200 images/classes. Networks are initialized using the Kaiming normal initialization (He
et al., 2015). For CIFAR datasets, we train Resnet503 and VGG194 architectures during 350 epochs
with a batch size of 128. We start with a learning rate of 0.1 and divide itby 10 at 150 and 250 epochs.
As optimizer we use SGD with momentum 0.9 and weight decay 5 × 10-4. We separate 10% of the
training data for validation and report results on the test set. We perform mean and std normalization
and augment the data with random crops and horizontal flips. For Tiny-Imagenet, we use the same
architectures. We train during 300 epochs and divide the learning rate by 10 at 1/2 and 3/4 of the
training. Other hyper-parameters remain the same. For ImageNet training, we adapt the official code5
of Paszke et al. (2017) and we use the default settings. In this case, we use the Resnet50 and VGG19
with batch normalization architectures as implemented in Paszke et al. (2017).
In the case of FORCE and Iter SNIP, we adapt the same public implementation6 of SNIP as Wang
et al. (2020). Instead of defining an auxiliary mask to compute the saliencies, we compute the product
of the weight times the gradient, which was shown to be equivalent in Lee et al. (2020). As for
GRASP, we use their public code.7 After pruning, we implement pruned connections by setting the
corresponding weight to 0 and forcing the gradient to be 0. This way, a pruned weight will remain 0
during training.
An important difference between SNIP and GRASP implementations is in the way they select the
mini-batch to compute the saliency. SNIP implementation simply loads a batch from the dataloader.
In contrast, in GRASP implementation they keep loading batches of data until they obtain exactly
10 examples of each class, discarding redundant samples. In order to compare the methods in equal
conditions, we decided to use the way SNIP collects the data since it is simpler to implement and
does not require extra memory. This might cause small discrepancies between our results and the
ones reported in Wang et al. (2020).
Table 2: Percentage of weights per layer for each network and dataset.
Layer type	Conv	Fully connected	BatChNorm	Bias	Prunable	Total
CIFAR10 Resnet50	99.69	0.09	0.11	0.11	99.78	23.52M
VGG19	99.92	0.03	0.03	0.03	99.95	20.04M
CIFAR100 Resnet50	98.91	0.86	0.11	0.11	99.78	23.71M
VGG19	99.69	0.25	0.03	0.03	99.94	20.08M
TinyImagenet Resnet50	98.06	1.71	0.11	0.11	99.78	23.91M
VGG19	99.44	0.51	0.03	0.03	99.94	20.13M
Imagenet Resnet50	91.77	8.01	0.10	0.11	99.79	25.56M
VGG19	13.93	86.05	0.01	0.01	99.98	143.68M
A meaningful design choice regarding SNIP and GRASP implementations is that they only prune
convolutional and fully connected layers. These layers constitute the vast majority of parameters
in most networks, however, as we move to high sparsity regimes, batch norm layers constitute a
non-negligible amount. For CIFAR10, batch norm plus biases constitute 0.2% and 0.05% of the
parameters of Resnet50 and VGG19 networks respectively. For consistency, we have as well restricted
pruning to convolutional and fully connected layers and reported percentage sparsity with respect
to the prunable parameters, as is also done in Lee et al. (2019) and Wang et al. (2020) to the best
of our knowledge. In Table 2 we show the percentage of prunable weights for each network and
3https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py
4https://github.com/alecwangcq/GraSP/blob/master/models/base/vgg.py
5https://github.com/pytorch/examples/tree/master/imagenet
6https://github.com/mi-lad/snip
7https://github.com/alecwangcq/GraSP
12
Published as a conference paper at ICLR 2021
5432109876543210
∙∙o^o^∙o^09∙∙09c9∙09c9∙∙
^^^^^^^^
OI3>UE3UU<
(a) Resnet50 CIFAR-10
(b) VGG19 CIFAR-10
—FORCE
—IterSNIP
—GRASP-MB
—SNIP-MB
—•— Random
Dense Baseline
Oo 且-u>uro⅛uu<
0.5%	1%	5%	10%
Percentage of remaining weights (log scale)
(c) Resnet50 CIFAR-100
ooi≈wd>UE3UW<
Percentage of remaining weights (log scale)
(e) Resnet50 Tiny Imagenet
(d) VGG19 CIFAR-100
(f) VGG19 Tiny Imagenet
Figure 5: Test accuracies on CIFAR-10/100 and Tiny Imagenet for different pruning methods. Each
point is the average over 3 runs of prune-train-test. The shaded areas denote the standard deviation of
the runs (too small to be visible in some cases).
dataset we use. In future experiments we will explore the performance of pruning at initialization
when including batch norm layers and biases as well.
B	Additional accuracy-sparsity plots
In the main text we show the complete range of the accuracy-sparsity curves for the different methods
so it is clear why more robust methods are needed. However, it makes it more difficult to appreciate
the smaller differences at lower sparsities. In Fig 5 we show the accuracy-sparsity curves where we
cut the y axis to show only the higher accuracies.
C Further analysis of pruning at initialization
C.1 Saliency vs T
In Fig 4 (left) we have seen that for higher sparsity levels, FORCE obtains a higher saliency when
we increase the number of iterations. In Fig 6 we compare the relative saliencies as we increase the
number of iterations for FORCE and Iterative SNIP. As can be seen, both have a similar behaviour.
C.2 Pruning vs Sparsification
FORCE algorithm is able to recover pruned weights in later iterations of pruning. In order to do
that, we do not consider the intermediate masks as pruning masks but rather as sparsification masks,
where connections are set to 0 but not their gradients. In order to understand how does computing
the FORCE (5) on a sparsified vs pruned network affect the saliency, we prune several masks with
FORCE algorithm at varying sparsity levels. For each mask, we then compute their FORCE saliency
either considering the pruned network (gradients of pruned connections will be set to 0 during the
backward pass) or the sparsified network (we only set to 0 the connections, but let the gradient signal
13
Published as a conference paper at ICLR 2021
1	2 3 4 5	10	20	40 60 100 200300
Num iterative steps (log scale)
(a) Saliency vs T across sparsity levels (FORCE)
Ooo
6 4 2
(Llo 口 eySJeds) ω⅛on-
1	2 3 4 5	10	20	40 60 100 200300
Num iterative steps (log scale)
(b) Saliency vs T across sparsity levels (Iter SNIP)
Figure 6: FORCE saliency (6) obtained with iterative pruning normalized by the saliency obtained
with one-shot SNIP, T = 1. (a) Applying the FORCE algorithm (b) Using Iterative SNIP. Note how
both methods have similar behaviour.
0.2
A-SJeds
8 6 - 4
40
FORCE (pruning)
60
Figure 7: FORCE saliency computed for masks as we vary sparsity. FORCE (sparsification) refers to
measuring FORCE when we allow the gradients of zeroed connections to be non-zero, while FORCE
(pruning) cuts all backward signal of any removed connection. As can be seen on the plot, they are
strongly correlated.
flow through all the connections). Results are presented in Fig 7. We observe that the two methods
to compute the saliency are strongly correlated, thus, we can assume that when we use the FORCE
algorithm that maximizes the saliency of sparsified networks we will also maximize the saliency of
the corresponding pruned networks.
C.3 Network structure after pruning
In Fig 8 we visualize the structure of the networks after pruning 99.9% of the parameters. We show
the fraction of remaining weights and the total number of remaining weights per layer after pruning.
As seen in (a) and (d), all analysed methods show a tendency to preserve the initial and final layers
and to prune more heavily the deep convolutional layers, this is consistent with results reported
in Wang et al. (2020). In (b) and (e), we note that FORCE has a structure that stands out compared
to other methods that are more similar. This is reasonable since, it is the only method that allows
pruned weights to recover. In the zoomed plots (c) and (f) we would like to point out that FORCE
and Iterative SNIP preserve more weights on the deeper layers than GRASP and SNIP for VGG19
while we observe the opposite behaviour for Resnet50.
In Fig 2, we observe that gradual pruning is able to prune the Resnet50 network up to 99.99% sparsity
without falling to random accuracy. In contrast, with VGG19 we observe Iterative SNIP is not able
to prune more than 99.9%. In Fig 8 we observe that for Resnet50, all methods prune some layers
completely. However, in the case of ResNets, even if a convolutional layer is entirely pruned, skip
connections still allow the flow of forward and backward signal. On the other hand, architectures
without skip connections, such as VGG, require non-empty layers to keep the flow of information.
Interestingly, in (c) we observe how FORCE and Iter SNIP have a larger amount of completely
pruned layers than GRASP, however, there are a few deep layers with a significantly larger amount of
unpruned weights. This seems to indicate that when a high sparsity is required, it is more efficient to
have fewer layers with more weights than several extremely sparse layers.
14
Published as a conference paper at ICLR 2021
8 6 4 2 0
(OOOIX)」生-更-ll--BM
Layer index
(a) Resnet50 Layer density
(b) Resnet50 total weights
6 5 4 3 2 1 0
(OOOIX)」生-更-ll--BM
(c) Resnet50 total weights (zoom)
8	16 O	8	16	O	8	16
Layer index	Layer index	Layer index
(d) VGG19 Layer density	(e) VGG19 total weights (f) VGG19 total weights (zoom)
Figure 8: Visualization of remaining weights after pruning 99.9% of the weights of Resnet-50 and
VGG-19 for CIFAR-10. (a) and (d) show the fraction of remaining weights for each prunable layer.
(b) and (e) show the actual number of remaining weights and (c) and (f) zoom to the bottom part of
the plot. Observe in (c) and (f) that some layers have exactly 0 weights left, so they are removed
entirely.
15
Published as a conference paper at ICLR 2021
(a) Pruned and recovered weights (Iter SNIP)
(b) Pruned and recovered weights (FORCE)
Figure 9: Normalized amount of globally pruned and recovered weights at each pruning iteration for
Resnet50, CIFAR-10 when pruned with (a) Iterative SNIP and (b) FORCE. As expected, the amount
of recovered weights for Iterative SNIP is constantly zero, since this is by design. Moreover, the
amount of pruned weights decays exponentially as expected from our pruning schedule. On the other
hand, we see the amount of recovered weights is non-zero with FORCE, interestingly the amount of
pruned/recovered weights does not decay monotonically but has a clear peak, indicating there is an
"exploration" and a "convergence" phase during pruning.
C.4 Evolution of pruning masks
As discussed in the main text, FORCE allows weights that have been pruned at earlier iterations to
become non-zero again, we argue this might be beneficial compared to Iterative SNIP which will not
be able to correct any possible "mistakes" made in earlier iterations. In particular, it seems to give
certain advantage to prune VGG to high sparsities without breaking the flow of information (pruning
a layer entirely) as can be seen in Fig 2 (b). In order to gain a better intuition of how does the amount
of pruned/recovered weights ratio evolve during pruning, in Fig 9 we plot the normalized amount of
pruned and recovered weights (globally on the whole network) at each iteration of FORCE and also
for Iter SNIP as a sanity check. Note that Iterative SNIP does not recover weights and the amount
of weights pruned at each step decays exponentially (this is expected since we derived Iterative
SNIP as a constrained optimization of FORCE where each network needs to be a sub-network of
the previous iteration. On the other hand, FORCE does recover weights. Moreover, the amount of
pruned/recovered weights does not decay monotonically but has a clear peak, indicating there are
two phases during pruning: While the amount of pruned weights increases, the algorithm explores
masks which are quite far away from each other, although this might be harmful for the gradient
approximation (refer to section 4), we argue that during the initial pruning iterations the network is
still quite over-parametrized. After reaching a peak, both the pruning and recovery rapidly decay,
thus the masks converge to a more constrained subset.
C.5 Comparison with early pruning
For fair comparison, we provided the same amount of data to SNIP and GRASP as was used by our
approach and call this variant SNIP-MB and GRASP-MB. Similarly, under this new baseline which
we call early pruning, we train the network on 1 epoch of data of CIFAR-10, that is slightly more
examples than our pruning at initialization methods which use 128 ∙ 300 = 38400 examples (See
section 5). After training for 1 epoch we perform magnitude pruning which requires no extra cost
(a) Resnet50
(b) VGG19
Figure 10: Test accuracies on CIFAR-10 for different pruning methods. Each point is the average
over 3 runs of prune-train-test. The shaded areas denote the standard deviation of the runs (too small
to be visible in some cases). Early pruning is at most on par with gradual pruning at initialization
methods and has a strong drop in performance as we go to higher sparsities.
16
Published as a conference paper at ICLR 2021
(results presented in Fig 10). Although early pruning yields competitive results for VGG at moderate
sparsity levels, it soon degrades its performance as we prune more weights. On the other hand, for
Resnet architecture it is sub-optimal at all evaluated sparsity levels. Note, this result does not mean
that any early pruning strategy would be sub-optimal compared to pruning at initialization, however
exploring this further is out of the scope of this work.
C.6 Mobilenet experiments
All our experiments were on overparameterized architectures such as Resnet and VGG. To test the
wider usability of our methods, in this section we prune the Mobilenet-v2 architecture8 (Sandler
et al., 2018) which is much more "slim" than Resnet and VGG (Mobilenet has 2.3M params compared
to 20.03M and 23.5M of VGG and Resnet respectively). Results are provided in Fig 11. Similarly
to Resnet and VGG architectures, we see that gradual pruning tends to better preserve accuracy at
higher sparsity levels than one-shot methods. Moreover, both FORCE and Iter SNIP improve over
SNIP at moderate sparsity levels as well. FORCE and Iter SNIP have comparable accuracies except
for high sparsity where Iter SNIP surpasses FORCE. We hypothesize for such a slim architecture
(≈ 10 × fewer parameters than Resnet and VGG) the gradient approximation becomes even more
sensitive to the distance between iterative masks and perhaps the exploration of FORCE is harmful
in this case. As discussed in the main paper, we believe that further research to understand the
exploration/exploitation trade-off when pruning might yield to even more efficient pruning schemes,
especially for very high sparsity levels. We train using the same settings as described in Appendix A
except for the weight decay which is set to 4 × 10-5, following the settings of the original Mobilenet
paper.
Figure 11: Test accuracies on CIFAR-10 for different pruning methods on Mobilenet architecture.
Each point is the average over 3 runs of prune-train-test. The shaded areas denote the standard
deviation of the runs (too small to be visible in some cases). When pruning Mobilenet-v2 architecture,
which has roughly 10× less parameters than Resnet or VGG, we observe a similar pattern: Gradual
pruning tends to be able to preserve accuracy for higher sparsities better than one-shot methods.
Moreover, it tends to improve slightly over SNIP at moderate sparsities as well. Although GRASP
does not show an acute drop in performance like SNIP, it seems to be sub-optimal with respect to
other methods across all sparsities.
D Local optimal masks
Definition 1 ((p, e)-local optimal mask). Consider any two sets9 Ct ⊆ {1, ∙∙∙ ,m} and ct+ι ⊂ c'
For any > 0 and 0 ≤ p ≤ |ct \ ct+1|, ct+1 is a (p, ) local optimal with respect to ct if the
following holds
S (θ, ct+ι) ≥ S (θ, (ct+ι \ S-) ∪ S+) - E	(9)
for all S- ⊂ ct+1, |S-| = p and S+ ⊂ (ct \ ct+1) , |S+| = p.
Definition 2 (CRS; Coordinate-Restricted-Smoothness). Given a function L : Rm → R (which
encodes both the network architecture and the dataset), L is said to be λc-Coordinated Restricted
8https://github.com/kuangliu/pytorch-cifar/blob/master/models/
mobilenetv2.py
9For ease of notation, we will use this representation interchangeably with its binary encoding i.e. a
m-dimensional binary vector with its support equal to c
17
Published as a conference paper at ICLR 2021
Smooth with respect to C ⊆ {1, ∙ ∙ ∙ ,m} if there exists a real number λc such that
∣∣c Θ VL (w Θ c) - c Θ VL (w Θ b)∣∣∞ ≤ λc IlW Θ C - W Θ b%	(10)
for all w ∈ Rm and c U c. When S = |c \ c|, an application of Holder,s inequality shows
XC Ilw θ C - W θ c∣1 ≤ λc ∣∣w∣∞ ∣∣c - Ck1 = λcs ∣∣w∣∞
We define L to be Λ-total CRS if there exists a function Λ : {0,1}m → R such that for all C ∈ {0,1}m
L is Λ (C)-Coordinate-Restricted-Smooth with respect to C (for ease of notation we use Λ (c) = ʌɑ).
Theorem 1 (Informal). The mask ct+ι produced from Ct by FORCE is (p, 2λp ∣θ∣∞ ∣ct∣^ -local
optimal if the L is Λ-CRS..
Proof. Consider the masks Ct and ct+ι where the latter is obtained by one step of FORCE on the
former. Let S- and S+ be any set of size p such that S- U ct+ι and S+ U (Ct \ ct+ι). Finally, for
ease of notation we define Z = (ct+ι \ S_) U S+
S (θ,ζ) - S (θ, Ct+1) = E∣θi ∙VL (θ Θ Z)i∣- E ∣θi ∙VL (θ Θ Ct+ι)i∣
i∈ζ	i∈ct+ι
=X ∣θi ∙VL (θ Θ QiI- X ∣θi ∙VL (θ Θ Ct+ι)i∣
i∈ct十ι	i∈ct+ι
'---------------------------V-----------------------------'
Γ1
+ E ∣θi ∙VL (θ Θ GiI-E ∣θi ∙ VL (θ Θ Z)i∣
i∈S+	i∈S-
'------V-------'、------V--------'
Γ2	Γ3
(11)
Let US look at the three terms individually. We assume that L is Λ-CRS.
Γ1 = E ∣θi ∙VL (θ Θ Z)i∣-∣θi ∙VL (θ Θ Ct+ι)i∣
i∈ct+ι
≤ ∣ct+ι Θ θ ΘVL (θ Θ ct+ι) - ct+ι Θ θ Θ VL (θ Θ Z)∣ι
≤ l∣ct+ι θ θ∣lι l∣ct+ι θ vl (θ θ ct+1)- ct+ι θ vl (θ θ gii∞
≤ 2λct+ι ∣Ct+1∣P ∣θ∣∞
By Triangle Inequality
By Holder,s Inequality
∙.∙ L is Λ-CRS (12)
Γ2 = E ∣θi ∙ VL (θ Θ ζ)i∣-∣θi ∙ VL (θ Θ Ct)i∣ + ∣θi ∙ VL (θ Θ c∕∣
i∈S+
≤ X ∣θi ∙VL (θ θ ct)i∣ + λctp ∣∣θk∞ (∣ct∣ - ∣ct+11)	V ∣ct \ Z∣ = ∣ct∣ - ∣ct+1∣,
i∈S+
(13)
Γ3 = - X ∣θi ∙ VL (θ Θ Z)i∣ + ∣θi ∙ VL (θ Θ Ct)i∣-∣θi ∙ VL (θ Θ c//
i∈S-
≤- X ∣θi ∙ VL (θ Θ Ct)i∣ + λct P ∣θ∣∞ (∣Ct∣-∣Ct+1∣),	(14)
i∈S-
Adding eqs. (13) and (14), we get
Γ2 +Γ3 ≤ X ∣θi ∙ VL (θ Θ Ct)i∣- X ∣θi ∙VL (θ Θ Ct)i∣ +2λctP ∣θ∣∞ (∣ct∣ - ∣Ct+1∣)
i∈S +	i∈S-
≤ 2λctP I∣θ∣∣∞ (ICt∣-∣ct+1∣)-γp	(15)
where Y = mi∏i∈ct+ι,j∈(ct∖ct+ι) ∣θivL (θ θ Ct)i∣ - ∣θjVL (θ θ Ct)J ≥ 0
18
Published as a conference paper at ICLR 2021
Substituting eqs. (12) and (15) into (11) we get
S(θ,ζ) - S (θ, ct+1) ≤ 2λct+1 |ct+1| p kθk2∞ + 2λct p kθk2∞ (|ct| - |ct+1|) - γp
= 2λct+1 p kθk2∞ (|ct+1| + (|ct| - |ct+1|)) -γp
S (θ,ct+ι) ≥ S (θ,Z) — 2λct+ιP kθk∞ |ct| + Yp
S (θ, ct+1) ≥ S (θ, ζ) - 2λct+1p kθk2∞ |ct|
□
E Iterative pruning to maximize the gradient norm
0.9
o 0.8
§0.7
≤0.6
⅛0∙5
^0.4
§0.3
W 0.2
0.1
0.0
Percentage of remaining weights (log scale)
Percentage of remaining weights (log scale)
(a) CIFAR 10 - Resnet50	(b) CIFAR 10 - VGG19
ooτα<tu >US3UU<
≡
3%^
10%^
Percentage of remaining weights (log scale)

FORCE
IterSNIP
Iter GRASP
GRASP-MB
SNIP-MB
Random
Dense Baseline
0.7
S 0.6
$0.5
□ 0.4
⅛0.3
u 0.2
⅛0.l
0.0
Percentage of remaining weights (log scale)
(c) CIFAR 100 - Resnet50	(d) CIFAR 100 - VGG19
Percentage of remaining weights (log scale)
(e) Tiny Imagenet - Resnet50
(f) Tiny Imagenet - VGG19
Figure 12: Test accuracies for different datasets and networks when pruned with different methods.
Each point is the average over 3 runs of prune-train-test. The shaded areas denote the standard
deviation of the runs (sometimes too small to be visible).
E.1 Maximizing the gradient norm using the gradient approximation
In order to maximize the gradient norm after pruning, the authors in Wang et al. (2020) use the
first order Taylor’s approximation. While this seems to be better suited than SNIP for higher levels
of sparsity, it assumes that pruning is a small perturbation on the weight matrix. We argue that
this approximation will not be valid as we push towards extreme sparsity values. Our gradient
approximation (refer to section 4) can also be applied to maximize the gradient norm after pruning.
In this case, we have
G(θ, c)=∆L(θ Θ C) — ∆L(θ) ≈ X	-[VL(θ)i]2,	(16)
{i: ci=0}
where we assume pruned connections have null gradients (this is equivalent to the restriction used
for Iterative SNIP) and we assume gradients remain unchanged for unpruned weights (gradient
19
Published as a conference paper at ICLR 2021
approximation). Combining this approximation with Eq. (7), we obtain a new pruning method we
name Iterative GRASP, although it is not the same as applying GRASP iteratively. Unlike FORCE,
Iterative GRASP does not recover GRASP when T = 1.
In Fig 12 we compare Iterative GRASP to other pruning methods. We use the same settings as
described in section 5. We observe that Iterative GRASP outperforms GRASP in the high sparsity
region. Moreover, for VGG19 architecture Iterative GRASP achieves comparable performance to
Iterative SNIP. Nevertheless, for Resnet50 we see that Iterative GRASP performance falls below
that of FORCE and Iterative SNIP as we prune more weights. FORCE saliency takes into account
both the gradient and the magnitude of the weights when computing the saliency, on the other hand,
the Gradient Norm only takes gradients into account, therefore it is using less information. We
hypothesize this might the reason why Iterative GRASP does not match Iterative SNIP.
Figure 13: (a) - (c) Portion of remaining weights for each layer. Each point is an average of 3 runs
and the error bars (hardly visible) denote standard deviation. (b) - (d) Portion of remaining weights
for each intermediate mask that is computed during iterative pruning with global portion of remaining
weights of 0.01.
As explained in the main text, we tried to apply GRASP iteratively with the Taylor approximation
described in Wang et al. (2020). Unfortunately, we found that all resulting masks yield networks
unable to train. In light of this result, we performed some analysis of the behaviour of GRASP
compared to that of SNIP and, in the following, we provide some insights as to why we can not use
GRASP’s approximation iteratively.
In Liu et al. (2018), the authors show that applying a pruning method to the same architecture with
different random initializations would yield consistent pruning masks. Specifically, they find that the
percentage of pruned weights in each layer had very low variance. We reproduce the same experiment
and additionally explore another dimension, the (global) sparsity level. Given an architecture, we
prune it at varying levels of sparsity and extract the percentage of remaining weights at each layer.
For each level of sparsity, we average the results over 3 trials of initialize-prune. As shown in Fig 2,
both SNIP and GRASP have very low variance across initializations, on the other hand, as we vary
the global sparsity with GRASP, the percentage of remaining weights for each layer is inconsistent.
The layers that are most preserved at high levels of sparsity, such as the initial and last layers, are the
most heavily pruned at low sparsity levels.
The authors in Liu et al. (2018), reason that manually designed networks have layers which are more
redundant than others. Therefore, pruning methods even this redundancies by pruning layers with
different percentages. We extend this reasoning, and hypothesize that pruning algorithms should
always have preference for pruning the same (redundant) layers across all levels of sparsity. We
denote this as pruning consistency. We observe that when applying iterative pruning to GRASP,
the resulting masks tend to prune almost all of the weights at the initial and final layers, producing
20
Published as a conference paper at ICLR 2021
networks that are unable to converge. When using iterative pruning, we prune a small portion of
remaining weights at each step. Thus, we are always in the low sparsity regime, where the GRASP
behaviour is reversed. Conversely, when we use SNIP the behaviour changes completely. In this
case, the preserved layers are consistent across sparsity levels, and when we use iterative pruning we
obtain networks that reach high accuracy values.
21