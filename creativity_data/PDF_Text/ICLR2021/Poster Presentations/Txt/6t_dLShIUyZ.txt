Published as a conference paper at ICLR 2021
Greedy-GQ with Variance Reduction: Finite-
time Analysis and Improved Complexity
Shaocong Ma, Ziyi Chen & Yi Zhou
Department of ECE
University of Utah
Salt Lake City, UT 84112
{s.ma,u1276972,yi.zhou}@utah.edu
Shaofeng Zou
Department of EE
University at Buffalo
Buffalo, NY 14260
szou3@buffalo.edu
Ab stract
Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal
control. Recently, the finite-time analysis of Greedy-GQ has been developed un-
der linear function approximation and Markovian sampling, and the algorithm is
shown to achieve an -stationary point with a sample complexity in the order of
O(-3). Such a high sample complexity is due to the large variance induced by
the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ
(VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algo-
rithm applies the SVRG-based variance reduction scheme to reduce the stochas-
tic variance of the two time-scale updates. We study the finite-time convergence
of VR-Greedy-GQ under linear function approximation and Markovian sampling
and show that the algorithm achieves a much smaller bias and variance error than
the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an
improved sample complexity that is in the order of O(-2). We further compare
the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL exper-
iments to corroborate our theoretical findings.
1	Introduction
In reinforcement learning (RL), an agent interacts with a stochastic environment following a certain
policy and receives some reward, and it aims to learn an optimal policy that yields the maximum ac-
cumulated reward Sutton & Barto (2018). In particular, many RL algorithms have been developed to
learn the optimal control policy, and they have been widely applied to various practical applications
such as finance, robotics, computer games and recommendation systems Mnih et al. (2015; 2016);
Silver et al. (2016); Kober et al. (2013).
Conventional RL algorithms such as Q-learning Watkins & Dayan (1992) and SARSA Rummery &
Niranjan (1994) have been well studied and their convergence is guaranteed in the tabular setting.
However, it is known that these algorithms may diverge in the popular off-policy setting under lin-
ear function approximation Baird (1995); Gordon (1996). To address this issue, the two time-scale
Greedy-GQ algorithm was developed in Maei et al. (2010) for learning the optimal policy. This
algorithm extends the efficient gradient temporal difference (GTD) algorithms for policy evaluation
Sutton et al. (2009b) to policy optimization. In particular, the asymptotic convergence of Greedy-
GQ to a stationary point has been established in Maei et al. (2010). More recently, Wang & Zou
(2020) studied the finite-time convergence of Greedy-GQ under linear function approximation and
Markovian sampling, and it is shown that the algorithm achieves an -stationary point of the objec-
tive function with a sample complexity in the order of O(-3). Such an undesirable high sample
complexity is caused by the large variance induced by the Markovian samples queried from the
dynamic environment. Therefore, we want to ask the following question.
•	Q1: Can we develop a variance reduction scheme for the two time-scale Greedy-GQ algorithm?
1
Published as a conference paper at ICLR 2021
In fact, in the existing literature, many recent work proposed to apply the variance reduction tech-
niques developed in the stochastic optimization literature to reduce the variance of various TD learn-
ing algorithms for policy evaluation, e.g., Du et al. (2017); Peng et al. (2019); Korda & La (2015);
Xu et al. (2020). Some other work applied variance reduction techniques to Q-learning algorithms,
e.g., Wainwright (2019); Jia et al. (2020). Hence, it is much desired to develop a variance-reduced
Greedy-GQ algorithm for optimal control. In particular, as many of the existing variance-reduced
RL algorithms have been shown to achieve an improved sample complexity under variance reduc-
tion, it is natural to ask the following fundamental question.
•	Q2: Can variance-reduced Greedy-GQ achieve an improved sample complexity under Markovian
sampling?
In this paper, we provide affirmative answers to these fundamental questions. Specifically, we
develop a two time-scale variance reduction scheme for the Greedy-GQ algorithm by leveraging
the SVRG scheme Johnson & Zhang (2013). Moreover, under linear function approximation and
Markovian sampling, we prove that the proposed variance-reduced Greedy-GQ algorithm achieves
an -stationary point with an improved sample complexity O(-2). We summarize our technical
contributions as follows.
1.1	Our Contributions
We develop a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for optimal control in re-
inforcement learning. Specifically, the algorithm leverages the SVRG variance reduction scheme
Johnson & Zhang (2013) to construct variance-reduced stochastic updates for updating the parame-
ters in both time-scales.
We study the finite-time convergence of VR-Greedy-GQ under linear function approximation and
Markovian sampling in the off-policy setting. Specifically, we show that VR-Greedy-GQ achieves
an e-stationary point of the objective function J (i.e., ∣Rj(θ)k2 ≤ e) with a sample complexity
in the order of O(-2 ). Such a complexity result improves that of the original Greedy-GQ by a
significant factor of O(e-1) Wang & Zou (2020). In particular, our analysis shows that the bias
error caused by the Markovian sampling and the variance error of the stochastic updates are in the
order of O(M -1), O(ηθM-1), respectively, where ηθ is the learning rate and M corresponds to the
batch size of the SVRG reference batch update. This shows that the proposed variance reduction
scheme can significantly reduce the bias and variance errors of the original Greedy-GQ update (by
a factor of M) and lead to an improved overall sample complexity.
The analysis logic of VR-Greedy-GQ partly follows that of the conventional SVRG, but requires
substantial new technical developments. Specifically, we must address the following challenges.
First, VR-Greedy-GQ involves two time-scale variance-reduced updates that are correlated with
each other. Such an extension of the SVRG scheme to the two time-scale updates is novel and re-
quires new technical developments. Specifically, we need to develop tight variance bounds for the
two time-scale updates under Markovian sampling. Second, unlike the convex objective functions
of the conventional GTD type of algorithms, the objective function of VR-Greedy-GQ is generally
non-convex due to the non-stationary target policy. Hence, we need to develop new techniques to
characterize the per-iteration optimization progress towards a stationary point under nonconvexity.
In particular, to analyze the two time-scale variance reduction updates of the algorithm, we intro-
duce a ‘fine-tuned’ Lyapunov function of the form Rtm = J(θt(m)) + ctkθt(m) - θe(m)k2, where the
parameter ct is fine-tuned to cancel other additional quadratic terms kθt(m) - θe(m) k2 that are implic-
itly involved in the tracking error terms. The design of this special Lyapunov function is critical to
establish the formal convergence of the algorithm. With these technical developments, we are able
to establish an improved finite-time convergence rate and sample complexity for VR-Greedy-GQ.
1.2	Related Work
Q-learning and SARSA with function approximation. The asymptotic convergence of Q-learning
and SARSA under linear function approximation were established in Melo et al. (2008); Perkins &
Precup (2003), and their finite-time analysis were developed in Zou et al. (2019); Chen et al. (2019).
However, these algorithms may diverge in off-policy training Baird (1995). Also, recent works
focused on the Markovian setting. Various analysis techniques have been developed to analyze
2
Published as a conference paper at ICLR 2021
the finite-time convergence of TD/Q-learning under Markovian samples. Specifically, Wang et al.
(2020) developed a multi-step Lyapunov analysis for addressing the biasedness of the stochastic
approximation in Q-learning. Srikant & Ying (2019) developed a drift analysis to the linear stochas-
tic approximation problem. Besides the linear function approximation, the finite-time analysis of
Q-learning under neural network function approximation is developed in Xu & Gu (2019).
GTD algorithms. The GTD2 and TDC algorithms were developed for off-policy TD learning.
Their asymptotic convergence was proved in Sutton et al. (2009a;b); Yu (2017), and their finite-time
analysis were developed recently in Dalal et al. (2018); Wang et al. (2017); Liu et al. (2015); Gupta
et al. (2019); Xu et al. (2019). The Greedy-GQ algorithm is an extension of these algorithms to
optimal control and involves nonlinear updates.
RL with variance reduction: Variance reduction techniques have been applied to various RL al-
gorithms. In TD learning, Du et al. (2017) reformulate the MSPBE problem as a convex-concave
saddle-point optimization problem and applied SVRG Johnson & Zhang (2013) and SAGA Defazio
et al. (2014) to primal-dual batch gradient algorithm. In Korda & La (2015), the variance-reduced
TD algorithm was introduced for solving the MSPBE problem, and later Xu et al. (2020) provided
a correct non-asymptotic analysis for this algorithm over Markovian samples. Recently, some other
works applied the SVRG , SARAH Nguyen et al. (2017) and SPIDER Fang et al. (2018) variance
reduction techniques to develop variance-reduced Q-learning algorithms, e.g., Wainwright (2019);
Jia et al. (2020). In these works, TD or TDC algorithms are in the form of linear stochastic approx-
imation, and Q-learning has only a single time-scale update. As a comparison, our VR-Greedy-GQ
takes nonlinear two time-scale updates to optimization a nonconvex MSPBE.
2	Preliminaries: Policy Optimization and Greedy-GQ
In this section, we review some preliminaries of reinforcement learning and recap the Greedy-GQ
algorithm under linear function approximation.
2.1	Policy Optimization in Reinforcement Learning
In reinforcement learning, an agent takes actions to interact with the environment via a Markov
Decision Process (MDP). Specifically, an MDP is specified by the tuple (S, A, P, r, γ), where S and
A respectively correspond to the state and action spaces that include finite elements, r : S ×A×S →
[0, +∞) denotes a reward function and γ ∈ (0, 1) is the associated reward discount factor.
At any time t, assume that the agent is in the state st ∈ S and takes a certain action at ∈ A
following a stationary policy ∏, i.e., at 〜∏(∙∣st). Then, at the subsequent time t + 1, the current
state of the agent transfers to a new state st+ι according to the transition kernel P(∙∣st, at). At the
same time, the agent receives a reward rt = r(st, at, st+1) from the environment for this action-state
transition. To evaluate the quality of a given policy π, we often use the action-state value function
Qπ : S × A → R that accumulates the discounted rewards as follows:
Qπ(s, a) = Es,〜p(∙∣s,α) [r(s, a, s) + γVπ(s0)],
where Vπ(s) is the state value function defined as Vπ(s)
e[ P∞=0 Ytrtlso = s]
In particu-
lar, define the Bellman operator Tπ such that TπQ(s, a) = Es0,a0 [r(s, a, s0) + γQ(s0, a0)] for any
Q(s, a), where a0 〜π(∙∣s0). Then, Qn (s, a) is a fixed point of Tπ, i.e.,
TπQπ(s,a) = Qπ(s,a), ∀s, a.
(1)
The goal of policy optimization is to learn the optimal policy ∏* that maximizes the expected total
reward E[P∞=0 γtrt∣so = s] for any initial state S ∈ S, and this is equivalent to learn the optimal
value function Q* (s, a) = sup∏ Qn(s, a), ∀s, a. In particular, Q* is a fixed point of the Bellman
operator T that is defined as TQ(s, a) = Es，〜p(∙∣s,a)[r(s, a, s0) + Y maxb∈A Q(s0, b)].
2.2	Greedy-GQ with Linear Function Approximation
The Greedy-GQ algorithm is inspired by the fixed point characterization in eq. (1), and in the tabular
setting it aims to minimize the Bellman error ∣∣TπQπ - Qπk,s a. Here, k ∙ |丘 Q is induced by
3
Published as a conference paper at ICLR 2021
the state-action stationary distribution μs,a (induced by the behavior policy ∏b), and is defined as
kQ%,a = E(s,a)~μs,a[Q(S,a/ ].
In practice, the state and action spaces may include a large number of elements that makes tabular
approach infeasible. To address this issue, function approximation technique is widely applied. In
this paper, we consider approximating the state-action value function Q(S, a) by a linear function.
Specifically, consider a set of basis functions {φ(i) : S × A → R, i = 1, 2, . . . , d}, each of which
maps a given state-action pair to a certain value. Define φs,a = [φ(1) (S, a); ...; φ(d) (S, a)] as the
feature vector for (S, a). Then, under linear function approximation, the value function Q(S, a) is
approximated by Qθ (S, a) = φs>,aθ, where θ ∈ Rd denotes the parameter of the linear approxi-
mation. Consequently, Greedy-GQ aims to find the optimal θ* that minimizes the following mean
squared projected Bellman error (MSPBE).
(MSPBE): J(θ) := 2k∏TπθQθ- Qθ|卮,。，	⑵
where μs,a is the stationary distribution induced by the behavior policy ∏b, Π is a projection operator
that maps an action-value function Q to the space Q spanned by the feature vectors, i.e., ΠQ =
argminu ∈q ∣∣U - Qkμs,a. Moreover, the policy ∏θ is parameterized by θ. In this paper, we consider
the class of Lipschitz and smooth policies (see Assumption 4.2).
Next, We introduce the Greedy-GQ algorithm. Define Vso(θ) = Pa0∈/∏θ(a0∣s0)Φ>,aoθ,
δs,a,s0(θ) = r(s,a, s0) + YVs，(0) - φ>aθ and denote φs(θ) = VVS(θ). Then, the gradient of
the objective function J(θ) in eq. (2) is expressed as
VJ(θ) = -E[δs,α,s0 (θ)φs,a] + YE[bs0 ^φ"ω" ⑹，
where ω* (θ) = E[Φs,aΦ>a]-1E[δs,a,s0 (θ)Φs,a]. To address the double-sampling issue when esti-
mating the product of expectations involved in E®，(θ)φ;a]ω*(θ), Sutton et al. (2009a) applies a
weight doubling trick and constructs the following two time-scale update rule for the Greedy-GQ
algorithm: for every t = 0, 1, 2, ..., sample (St, at, rt, St+1) using the behavior policy πb and do
(θt+ι = θt - ηθ( - δt+ι(θt)φt + γ(ω>φt)φt+ι(θt)),
(Greedy-GQ): ( ωt+ι = ωt - & (φ>ωt - δt+ι(θt))φt,	(3)
Iπθt+1 = P (φ> θt+1).
where ηθ,ηω > 0 are the learning rates and we denote δt+1 (θ) := δst,at,st+1 (θ), φt := φst,at,
φt+1(θt) := φst+1 (θt) for simplicity. To elaborate, the first two steps correspond to the two time-
scale updates for updating the value function Qθ , whereas the last step is a policy improvement
operation that exploits the updated value function to improve the target policy, e.g., greedy, -greedy,
softmax and mellowmax Asadi & Littman (2017).
The above Greedy-GQ algorithm uses a single Markovian sample to perform the two time-scale
updates in each iteration. Such a stochastic Markovian sampling often induces a large variance that
significantly slows down the overall convergence. This motivates us to develop variance reduction
schemes for the two time-scale Greedy-GQ in the next section.
3	Greedy- GQ with Variance Reduction
In this section, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm under
Markovian sampling by leveraging the SVRG variance reduction scheme Johnson & Zhang (2013).
To simplify notations, we define the stochastic updates regarding a sample xt = (St, at, rt, St+1)
used in the Greedy-GQ as follows:
Gxt (θ, ω) := -δt+1(θ)φt + γ(ω> φt)φbt+1(θ),
Hxt(θ,ω) := (φ>ω - δt+1(θ))φt.
Next, consider a single MDP trajectory {xt}t≥0 obtained by the behavior policy πb. In partic-
ular, we divide the entire trajectory into multiple batches of samples {Bm }m≥1 so that Bm =
4
Published as a conference paper at ICLR 2021
{x(m-1)M, ..., xmM-1}, and our proposed VR-Greedy-GQ uses one batch of samples in every
epoch. To elaborate, in the m-th epoch, we first initialize this epoch with a pair of reference points
θ0(m) = θe(m), ω0(m) = ωe(m), where θe(m), ωe(m) are set to be the output points θM(m-1), ωM(m-1) of
the previous epoch, respectively. Then, we compute a pair of reference batch updates using the
reference points and the batch of samples as follows
mM-1	mM-1
Gg) = M X	Gxk(e(m),ω(m)),	H(m) = M X	Hxk(e(m),ω(m)).	(4)
k=(m-1)M	k=(m-1)M
In the t-th iteration of the m-th epoch, we first query a random sample xξm from the batch
Bm uniformly with replacement (i.e., sample ξtm from {(m - 1)M, ..., mM - 1} uniformly).
Then, we use this sample to compute the stochastic updates Gxξm , Hxξm at both of the points
(θt(m), ωt(m)), (θe(m), ωe(m)). After that, we use these stochastic updates and the reference batch
updates to construct the variance-reduced updates in Algorithm 1 via the SVRG scheme, where for
simplicity we denote the stochastic updates Gxξm , Hxξm respectively as G(tm), Ht(m). In particular,
we project the two time-scale updates onto the Euclidean ball with radius R to stabilize the algo-
rithm updates, and we assume that R is large enough to include at least one stationary point of J.
Lastly, we further update the policy via the policy improvement operation P .
Algorithm 1: Variance-Reduced Greedy-GQ
Input: learning rates η, &, batch size M.
Initialize: e(1) = θo, ω(1) = ω°,开在⑴P(φ>θ⑴).
for m = 1, 2, . . . do
θ0(m) = θe(m), ω0(m) = ωe(m). Compute Ge(m), He (m) according to eq. (4).
for t = 0, 1, . . . , M - 1 do
Query a sample from Bm with replacement.
θ(m) = Πr Wm)- ηθ(G(m)(θ(m),ω(m)) - G(m)(e(m)Q(m)) + G(m))i.
ω(m1 =ΠR∣ω(m) - ηω (Htm)(θt(m,ω(m)) - Htrr"θm),ω(m)) + H(m)]
Policy improvement: πθ(m) 一 P(φ>θ(mml).
end
Set θe(m+1) = θM(m), ωe(m+1) = ωM(m).
end
Output: parameter θ chosen among {θt(m)}t,m uniformly at random.
The above VR-Greedy-GQ algorithm has several advantages and uniqueness. First, it takes incre-
mental updates that use a single Markovian sample per-iteration. This makes the algorithm sample
efficient. Second, VR-Greedy-GQ applies variance reduction to both of the two time-scale updates.
As we show later in the analysis, such a two time-scale variance reduction scheme significantly
reduces the variance error of both of the stochastic updates.
We want to further clarify the incrementalism and online property of VR-Greedy-GQ. Our VR-
Greedy-GQ is based on the online-SVRG and can be viewed as an incremental algorithm with regard
to the batches of samples used in the outer-loops, i.e., in every outer-loop the algorithm samples a
new batch of samples and use them to perform variance reduction in the corresponding inner-loops.
Therefore, VR-Greedy-GQ can be viewed as an online batch-incremental algorithm. In general,
there is a trade-off between incrementalism and variance reduction for SVRG-type algorithms: a
larger batch size in the outer-loops enhances the effect of variance reduction, while a smaller batch
size makes the algorithm more incremental.
4	Finite-Time Analysis of VR-Greedy-GQ
In this section, we analyze the finite-time convergence rate of VR-Greedy-GQ. We adopt the follow-
ing standard technical assumptions from Wang & Zou (2020); Xu et al. (2020).
5
Published as a conference paper at ICLR 2021
Assumption 4.1 (Feature boundedness). The feature vectors are uniformly bounded, i.e., kφs,ak ≤
1 for all (s, a) ∈ S × A.
Assumption 4.2 (Policy smoothness). The mapping θ 7→ πθ is k1-Lipschitz and k2-smooth.
We note that the above class of smooth policies covers a variety of practical policies, including
softmax and mellowmax policies Asadi & Littman (2017); Wang & Zou (2020).
Assumption 4.3 (Problem solvability). The matrix C := E[φs,aφs>,a] is non-singular.
Assumption 4.4 (Geometric uniform ergodicity). There exists Λ > 0 and ρ ∈ (0, 1) such that
supdtv(P(StIs0 = s),μ) ≤ Λρt,
s∈S
for any t > 0, where dTV is the total-variation distance.
Based on the above assumptions, we obtain the following finite-time convergence rate result.
Theorem 4.5 (Finite-time convergence). Let Assumptions 4.1- 4.4 hold and consider the VR-
Greedy-GQ algorithm. Choose learning rates ηθ, ηω and the batch size M that satisfy the conditions
specified in eqs. (15) to (19). Then, after T epochs, the output of the algorithm satisfies
EkVJ (θ(Z))k2 ≤ O( ηθτ1M+τ ®+ηθ)+(/+ηω)2+M )，
where ξ, ζ are random indexes that are sampled from {0, ..., M - 1} and {1, ..., T } uniformly at
random, respectively.
Theorem 4.5 shows that VR-Greedy-GQ asymptotically converges to a neighborhood ofa stationary
point at a sublinear rate. In particular, the size of the neighborhood is in the order of O(M-1 +
ηθ4ηω-4 + ηω2 ), which can be driven arbitrarily close to zero by choosing a large batch size and
sufficiently small learning rates that satisfy the two time-scale condition ηe/& → 0. Moreover, the
convergence error terms implicitly include a bias error O(M) caused by the Markovian sampling
and a variance error O( M) caused by the stochastic updates, both of which are substantially reduced
by the large batch size M. This shows that the SVRG scheme can effectively reduce the bias and
variance error of the two time-scale stochastic updates.
By further optimizing the choice of hyper-parameters, we obtain the following characterization of
sample complexity of VR-Greedy-GQ.
Corollary 4.6 (Sample complexity). Under the same conditions as those of Theorem 4.5, choose
learning rates so that ηθ = O(吉),ηω = O(η2/3), and set T,M = O(e-1)). Then, the required
sample complexity for achieving EkVJ (θξ(ζ))k2 ≤ is in the order ofT M = O(-2).
Such a complexity result is orderwise lower than the complexity O(-3) of the original Greedy-GQ
Wang & Zou (2020). Therefore, this demonstrates the advantage of applying variance reduction to
the two time-scale updates of VR-Greedy-GQ. We also note that for online stochastic non-convex
optimization, the sample complexity of the SVRG algorithm is in the order of O(-5/3) Li & Li
(2018), which is slightly better than our result. This is reasonable as the SVRG in stochastic opti-
mization is unbiased due to the i.i.d. sampling. In comparison, VR-Greedy-GQ works on a single
MDP trajectory that induces Markovian noise, and the two-timescale updates of the algorithm also
introduces additional tracking error.
5	S ketch of the Technical Proof
In this section, we provide an outline of the technical proof of the main Theorem 4.5 and highlight
the main technical contributions. The details of the proof can be found in the appendix.
We note that our proof logic partly follows the that of the conventional SVRG, i.e., exploiting the
objective function smoothness and introducing a Lyapunov function. However, our analysis requires
substantial new developments to address the challenges of off-policy control, two time-scale updates
of VR-Greedy-GQ and correlation of Markovian samples.
6
Published as a conference paper at ICLR 2021
The key step of the proof is to develop a proper Lyapunov function that drives the parameter to a
stationary point along the iterations. In addition, we also need to develop tight bounds for the bias
error, variance error and tracking error. We elaborate the key steps of the proof below.
Step 1: We first define the following Lyapunov function with certain ct > 0 to be determined later.
Rtm := J(θt(m)) +ctkθt(m) - θe(m)k2.	(5)
To explain the motivation, note that unlike the analysis of variance-reduced TD learning Xu et al.
(2020) where the term ∣∣θ(m) - θ(m)k2 can be decomposed into ∣∣θ(m) - θ*∣∣2 + ∣∣e(m) - θ*∣∣2 to
get the desired upper bound, here we do not have θ* due to the non-convexity of J(θ). Hence, we
need to properly merge this term into the Lyapunov function Rtm . By leveraging the smoothness of
J(θ) and the algorithm update rule, we obtain the following bound for the Lyapunov function Rtm
(see eq. (10) in the appendix for the details).
E[Rm+1] ≤ E[J(θ(m))] + θ(E[∣θ(m) - θ(m)∖∖2] + E[∣VJ(θ(m))]∣2 + MT)
+ θ(E[∣ω(m) - ω*(e(m))k2] + E[∣ω(m) - ω*(θ(m))∣2]).	(6)
In particular, the error term 焉 is due to the noise of Markovian sampling and the variance of the
stochastic updates, and the last two terms correspond to tracking errors.
Step 2: To telescope the Lyapunov function over t based on eq. (6), one may want to define
J(θ(m))] + θ(E[∣θ(m) - θ(m)k2]) = Rm by choosing a proper Ct of R}. However, note that
eq. (6) involves the last two tracking error terms, which also implicitly depend on E∣θt(m) - θe(m) ∣2
as we show later in the Step 3. Therefore, we need to carefully define the Ct of Rtm so that after
applying the tracking error bounds developed in the Step 3, the right hand side of eq. (6) can yield
an Rtm without involving the term E∣θt(m) - θe(m) ∣2 . It turns out that we need to define Ct via the
recursion specified in eq. (11) in the appendix. We rigorously show that the sequence {Ct}t is uni-
formly bounded by a small constant b = ɪ. Then, plugging these bounds into eq. (6) and summing
over one epoch, we obtain the following bound (see eq. (13) in the appendix for the details).
M -1
ηθ X E[∣VJ(θ(m))k2] ≤ E[Rm] - E[RMM] + O(ηθ + ηθME[∣ω(m) - ω*(e(*k2])
t=0
M -1	2 M -1
+ O(ηθ X E∣ω(m) -ω*(θ(m))k2 -ηθb(ηω + 与 X E[∣θ(m) - e(m)k2])
t=0	ηω t=0
Step 3: We derive bounds for the tracking error terms PM=-I E∣ω(m) - ω*(θ(m))∣2 and E∣ω(m) -
ω* (eO)) ∣2 in the above bound in Lemma D.7 and Lemma D.8.
Step 4: Lastly, by substituting the tracking error bounds obtained in Step 3 into the bound obtained
in Step 2, the resulting bound does not involve the term PtM=-0 1 E∣θt(m) - θe(m) ∣2. Then, summing
this bound over the epochs m = 1, ..., T , we obtain the desired finite-time convergence rate result.
6 Experiments
In this section, we conduct two reinforcement learning experiments, namely, Garnet problem
Archibald et al. (1995) and Frozen Lake game Brockman et al. (2016), to test the performance of
VR-Greedy-GQ in the off-policy setting, and compare it with Greedy-GQ in the Markovian setting.
6.1	Garnet Problem
For the Garnet problem, we refer to Appendix F for the details of the problem setup. In Figure 1
(left), we plot the minimum gradient norm v.s. the number of pseudo stochastic gradient com-
putations for both algorithms using 40 Garnet MDP trajectories, and each trajectory contains 10k
samples. The upper and lower envelopes of the curves correspond to the 95% and 5% percentiles
of the 40 curves, respectively. It can be seen that VR-Greedy-GQ outperforms Greedy-GQ and
achieves a significantly smaller asymptotic gradient norm.
7
Published as a conference paper at ICLR 2021
# of gradient computations
一。」」山 əuuəpbauou
Figure 1: Comparison of Greedy-GQ and VR-Greedy-GQ in solving the Garnet problem.
In Figure 1 (middle), we track the estimated variance of the stochastic update for both algorithms
along the iterations. Specifically, we query 500 Monte Carlo samples per iteration to estimate the
pseudo gradient variance EkGtm) (θ(m), ω(m)) - VJ(θ(m))k2. It can be seen from the figure that
the stochastic updates of VR-Greedy-GQ induce a much smaller variance than Greedy-GQ. This
demonstrates the effectiveness of the two time-scale variance reduction scheme of VR-Greedy-GQ.
We further study the asymptotic convergence error of VR-Greedy-GQ under different batch sizes M.
We use the default learning rate setting that is mentioned previously and run 100k iterations for one
Garnet trajectories. We use the mean of the convergence error of the last 10k iterations as an estimate
of the asymptotic convergence error (the training curves are already saturated and flattened). Figure
1 (right) shows the asymptotic convergence error of VR-Greedy-GQ under different batch sizes M .
It can be seen that VR-Greedy-GQ achieves a smaller asymptotic convergence error with a larger
batch size, which matches our theoretical result.
Figure 2: Comparison of MSPBE and reward obtained by Greedy-GQ, VR-Greedy-GQ and PG.
In Figure 2 (Left), we plot the MSPBE J(θ) v.s. number of gradient computations for both Greedy-
GQ and VR-Greedy-GQ, where one can see that VR-Greedy-GQ achieves a much smaller MSPBE
than Greedy-GQ. In Figure 2 (Middle), we plot the estimated expected maximum reward (see Ap-
pendix F for details) v.s. number of gradient computations for Greedy-GQ, VR-Greedy-GQ and
actor-critic, where for actor-critic we set learning rate ηθ = 0.02 for the actor update and ηω = 0.01
for the critic update. One can see that VR-Greedy-GQ achieves a higher reward than the other two
algorithms, demonstrating the high quality of its learned policy. In addition, we also plot the es-
timated expected maximum reward v.s. number of iterations for Greedy-GQ, VR-Greedy-GQ and
policy gradient in Figure 2 (Right). For the policy gradient, we apply the standard off-policy policy
gradient algorithm. For each update, we sample 30 independent trajectories with a fixed length 60
to estimate the expected discounted return. The learning rate of policy gradient is set as ηθ . We note
that each iteration of policy gradient consumes 1800 samples and hence it is very sample inefficient.
Hence we set the x-axis to be number of iterations for a clear presentation (otherwise it becomes
a flat curve). One can see that VR-Greedy-GQ achieves a much higher expected reward than both
Greedy-GQ and policy gradient.
6.2	Frozen Lake Game
We further test these algorithms in solving the more complex frozen lake game. we refer to Ap-
pendix F for the details of the problem setup. Figure 3 shows the comparison between VR-Greedy-
8
Published as a conference paper at ICLR 2021
GQ and Greedy-GQ, and one can make consistent observations with those made in the Garnet ex-
periment. Specifically, Figure 3 (left) shows that VR-Greedy-GQ achieves a much more stationary
policy than Greedy-GQ. Figure 3 (middle) shows that the stochastic updates of VR-Greedy-GQ
induce a much smaller variance than those of Greedy-GQ. Moreover, Figure 3 (right) verifies our
theoretical result that VR-Greedy achieves a smaller asymptotic convergence error with a larger
batch size.
70
60
Oooo
5 4 3 2
⅛∕a=we
100
.5,0.5Q.5,0.5.0
3.3.NNLLCiCi
① UUeμe> 4U ①一 pe」0P91BE4S山
Figure 3: Comparison of Greedy-GQ and VR-Greedy-GQ in solving the Frozen Lake problem.
We further plot the MSPBE v.s. number of gradient computations for both Greedy-GQ and VR-
Greedy-GQ in Figure 4 (Left), where one can see that VR-Greedy-GQ outperforms Greedy-GQ.
In Figure 2 (Middle), we plot the estimated expected maximum reward v.s. number of gradient
computations for Greedy-GQ, VR-Greedy-GQ and actor-critic, where for actor-critic we set learning
rate ηθ = 0.2 for the actor update and ηω = 0.1 for the critic update. It can be seen that VR-
Greedy-GQ achieves a higher reward than the other two algorithms. In Figure 2 (Right), we plot the
estimated expected maximum reward v.s. number of iterations for Greedy-GQ, VR-Greedy-GQ and
policy gradient. For policy gradient, we use the same parameter settings as before. One can see that
VR-Greedy-GQ achieves a much higher expected reward than both Greedy-GQ and policy gradient.
Figure 4: Comparison of MSPBE and reward obtained by Greedy-GQ, VR-Greedy-GQ and PG.
7	Conclusion
In this paper, we develop a variance-reduced two time-scale Greedy-GQ algorithm for optimal con-
trol by leveraging the SVRG variance reduction scheme. Under linear function approximation and
Markovian sampling, we establish the sublinear finite-time convergence rate of the algorithm to a
stationary point and prove an improved sample complexity bound over that of the original Greedy-
GQ. The RL experiments well demonstrated the effectiveness of the proposed two time-scale vari-
ance reduction scheme. Our algorithm design may inspire new developments of variance reduction
for two time-scale RL algorithms. In the future, we will explore Greedy-GQ with other nonconvex
variance reduction schemes to possibly further improve the sample complexity.
ACKNOWLEDGEMENT
The work of S. Zou was supported by the National Science Foundation under Grant CCF-2007783.
9
Published as a conference paper at ICLR 2021
References
TW Archibald, KIM McKinnon, and LC Thomas. On the generation of Markov decision processes.
Journal of the OperationaIResearch Society, 46(3):354-361,1995.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 243-
252. JMLR. org, 2017.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym, 2016.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
mance of Q-learning with linear function approximation: Stability and finite-time analysis. arXiv
preprint arXiv:1905.11425, 2019.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. Proceedings of
Machine Learning Research, 75:1-35, 2018.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 1646-1654, 2014.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In Proc. International Conference on Machine Learning (ICML),
pp. 1049-1058, 2017.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 689-699, 2018.
Geoffrey J. Gordon. Chattering in SARSA (λ)-a CMU learning lab internal report. Citeseer, 1996.
Harsh Gupta, R Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), pp. 4706-4715, 2019.
Haonan Jia, Xiao Zhang, Jun Xu, Wei Zeng, Hao Jiang, Xiaohui Yan, and Ji-Rong Wen. Variance
reduction for deep q-learning using stochastic recursive gradient. arXiv:2007.12817, 07 2020.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Nathaniel Korda and Prashanth La. On td (0) with function approximation: Concentration bounds
and a centered variant with exponential convergence. In International Conference on Machine
Learning, pp. 626-634, 2015.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in neural information processing systems, pp. 5564-5574, 2018.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In Proc. International Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 504-513. Citeseer, 2015.
Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy
learning control with function approximation. In Proc. International Conference on Machine
Learning (ICML), 2010.
10
Published as a conference paper at ICLR 2021
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with
function approximation. In Proc. International Conference on Machine Learning (ICML), pp.
664-671. ACM, 2008.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, and G. Ostrovski. Human-level control through deep reinforcement learn-
ing. Nature, 518:529-533, 2015.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proc. International Conference on
Machine Learning (ICML), pp. 1928-1937, 2016.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 2613-2621. JMLR. org, 2017.
Zilun Peng, Ahmed Touati, Pascal Vincent, and Doina Precup. SVRG for policy evaluation with
fewer gradient evaluations. arXiv:1906.03704, 2019.
Theodore J Perkins and Doina Precup. A convergent form of approximate policy iteration. In Proc.
Advances in Neural Information Processing Systems (NeurIPS), pp. 1627-1634, 2003.
G. A. Rummery and M. Niranjan. Online Q-learning using connectionist systems. Technical Report,
Cambridge University Engineering Department, September 1994.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of Go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pp. 2803-2830. PMLR, 2019.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction, Second Edition.
The MIT Press, Cambridge, Massachusetts, 2018.
Richard S Sutton, Hamid R Maei, and Csaba Szepesvari. A convergent O(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in neural in-
formation processing systems, pp. 1609-1616, 2009a.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learn-
ing with linear function approximation. In Proc. International Conference on Machine Learning
(ICML), pp. 993-1000, 2009b.
Martin Wainwright. Variance-reduced q-learning is minimax optimal. arXiv:1906.04697, 06 2019.
Gang Wang, Bingcong Li, and Georgios B. Giannakis. A multistep lyapunov approach for finite-
time analysis of biased stochastic approximation. arXiv:1909.04299, 2020.
Yue Wang and Shaofeng Zou. Finite-sample analysis of greedy-gq with linear function approxima-
tion under markovian noise. In Proc. Machine Learning Research, volume 124, pp. 11-20, Aug
2020.
Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Finite sample analysis of the
gtd policy evaluation algorithms in markov setting. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), pp. 5504-5513, 2017.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor
critic methods. arXiv preprint arXiv:2005.01350, 2020.
Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function ap-
proximation. arXiv preprint arXiv:1912.04511, 2019.
11
Published as a conference paper at ICLR 2021
Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale off-policy TD learning: Non-
asymptotic analysis over Markovian samples. In Proc. Advances in Neural Information Pro-
cessing Systems (NeurIPS),pp. 10633-1θ643, 2019.
Tengyu Xu, Zhe Wang, Yi Zhou, and Yingbin Liang. Reanalysis of variance reduced temporal
difference learning. In Proc. International Conference on Learning Representations (ICLR), 2020.
Huizhen Yu. On convergence of some gradient-based temporal-differences algorithms for off-policy
learning. arXiv preprint arXiv:1712.09652, 2017.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. In Advances in Neural Information Processing Systems, pp. 8665-8675,
2019.
12
Published as a conference paper at ICLR 2021
Appendix
Table of Contents
A	Filtration and List of Constants	13
B	Proof of Theorem 4.5	13
C	Proof of Corollary 4.6	20
D	Technical Lemmas	20
E	Other Supporting Lemmas	27
F	Details of Experiments	29
A	Filtration and List of Constants
Filtration We follow the definition of filtration in VRTD (Appendix D, Xu et al. (2020)). Recall
that Bm denotes the set of Markovian samples used in the m-th epoch, and we also abuse the notation
here by letting x(tm) be the sample picked in the t-th iteration of the m-th epoch. Then, we define
the filtration for Markovian samples as follows
Fι,o = σ(Bo ∪ σ(θ⑼,W⑼)),F1,1 = σ(F1,0 ∪ σ(x01))),..., Fi,m = σ(Fι,M-i ∪ σ(xM)-ι))
F2,0 = σ(B1 ∪ Fi,m ∪ σ(θ(1),W⑴)),F2,1 = σ(F2,0 ∪ σ(x02))),. . .,F2,M = σ(F2,M—1 ∪ σ(xM∖))
Fm,0 = σ(Bm-i ∪ Fm-1,M ∪ σ @m-1) ,W(mT) )),Fm,ι = σ(Fm,0 ∪ σ(xOm))),...,
Fm,M = σ(Fm,M-1 ∪ σ(x(Mm-) 1)).
Moreover, we define Et,m as the conditional expectation with respect to the σ-field Ft,m
List of Constants We summarize all the constants that are used in the proof as follows.
•	G = rmax + (I + Y)R + Y(IAIRkI + 1)R.
•	H = (2 +γ)R+ rmax.
•	Ci = (1 +2Λ ɪ )(G + Cvj )2.
•	C2 = H 2(1+Λ ɪ).
•	C3 = 8RC2 (i + 1-P).
•	C4 = λ2C (R(2 + Y) + rmax)2(I + 1-p ).
B Proof of Theorem 4.5
We first define the following Lyapunov function
Rtm := J(θt(m)) +ctkθt(m) - θe(m)k2,
13
Published as a conference paper at ICLR 2021
where ct > 0 is to be determined later. Our strategy is to characterize the per-iteration progress of
Rtm. In particular, we use Lemma E.9 to bound the first term of Rtm and use Lemma E.10 to bound
the second term of Rtm . Note that Lemma E.9 implies that
J(θ(m)) ≤ J(θ(m)) - ηθBJ(θtm),gtm - VJ(θ(m))i - ηkVJ(θ(m))k2 + 2ηθkg(m)k2.
Let ξ(m) := hVJ(θ(m)), g(m) -VJ(θ(m))i. Then, we obtain that
J(θ(m)) ≤ J(θ(m)) -ηθξm-ηkVJ(θ(m))k2 + 2η2kg(m)k2.	⑺
Substituting eq. (7) and eq. (30) into the definition of Rtm+1, we obtain that
Rm+ι ：= J (θ(m))+ct+ι kθ(m) - e(m)k2
≤ J(θ(m)) - ηθξ" -ηθkVJ(θ(m))k2 + 2η2kg(m)k2
+ct+1hηθ2kgt(m)k2+kθt(m) -θe(m)k2 -2ηθζt(m)
+ η[&kVJ(θ(m))k2 + βtkθ(m) - e(m)k2]i
=J(θt(m))+ct+1(ηθβt+1)kθt(m) -θe(m)k2
+( - 加+,+I ηθθ)kVJ (Oim))k2+( 2 η2+ct+in2)kg(m)k2
-ηθξt(m) -2ct+1ηθζt(m).
Next, we bound the two inner product terms ξt(m) and ζt(m) .
Bounding the term ξt(m) :
ξt(m) := hVJ(θt(m)),gt(m) -VJ(θt(m))i.
Recall the variance-reduced stochastic update
gt(m) =G(tm)(θt(m),ωt(m)) - Gt(m)(θe(m), ωe(m)) + Ge(m).
Then, the term ξt(m) can be decomposed as
ξt(m) = hVJ(θt(m)),gt(m) -VJ(θt(m))i
=hVJ(θ(m)),G(m)(θ(m),ω*(θ(m))) -VJ(θ(m))i
+ hVJ(θ(m)),G(m)(θ(m),ω(m))- G(m)(θ”,ω*(θ(m)))i
+ hVJ(θt(m)),-Gt(m)(θe(m),ωe(m)) + Ge(m)i
In the last equality, the first inner product term is the bias caused by Markovian samples, and by
Lemma D.3 we have that
EhVJ(O(m)),Gtm)(θ(m),ω*(θ(m))) -VJ(θ(m))i
=EhVJ(θ(m)),G(m)(θ(m),ω*(θ(m))) -VJ(θ(m))i
=1 EkVJ(θ(m))k2 + EkG(m)(θ(m),ω*(θ(m))) - VJ(θ(m))ik2
≤ 1 EkVJ (θ(m))k2 + M.
The second inner product term is the bias caused by tracking error, and we further obtain that
NJ(θ(m)),G(m)(θ(m),ω(m)) - G(m)(θ"ω*(θ(m)))i
≤ IkVJ (θ(m))k2 +Lιkω(m)-ω*(θ(m))k2.
14
Published as a conference paper at ICLR 2021
The third inner product term is unbiased. Combining all of these bounds, we finally obtain that
∣Eξ(m)l ≤ M + 2 kVJ (θ(m))k2 + L1 kω” - ω*(θ")∣∣2.	(8)
Bounding the term ζt(m) :
ζt(m) := hgt(m) -VJ(θt(m)),θt(m)-θe(m)i.
Similar to the previous proof for bounding ξt(m), we can decompose ζt(m) as
ζt(m) = hθt(m) - θe(m), gt(m) -VJ(θt(m))i
=hθ(m) - e(m),G(m)(θ(m),ω*(θ(m))) - VJ(θ(m))i
+ hθ(m) - e(m),G(m)(θ(m),ω(m)) - Gtm)(θ"ω*(θ(m)))i
+ hθt(m) -θe(m),-Gt(m)(θe(m),ωe(m)) + Ge(m)i
In the last equality, the first inner product term is the bias caused by Markovian samples. We obtain
that
E(θ(m) - e(m), Gtm)(θ”,ω*(θ"))-VJ(θ(m))i
≤ 1 Ekθ(m) - e(m)k2 + 1 EkGtm)(θ(m),ω*(θ(m))) - VJ(θ(m))ik2
≤ 2 Ekθ(m) - e(m)k2+2 M.
The second inner product term is the bias caused by tracking error. We obtain that
hθ(m) - e(m),Gtm)(θ(m),ω(m)) - G(m)(θ”,ω*(θ(m)))i
≤2 kθ(m) - e(m)k2 + L kω"-ω*(θ”)k2.
The third inner product term is unbiased. Combining all of these bounds, we finally obtain that
∣EZ" ≤ 2M + kθ(m) - e(m)k2 + L kω(m) - ω*(θ严)k2.	(9)
Next, we continue to bound the Lyapunov function. Recall we have shown that
Rtm+1 ≤J(θt(m))+ct+1(ηθβt+1)kθt(m) -θe(m)k2
+ ( - ηθ + ct+1 βθ)kVJ(θ(m))k2 + (2η2 + ct+1η2)kg(m)k2
- ηθξt(m) -2ct+1ηθζt(m).
Taking expectation on both sides of the above inequality and applying eq. (8), eq. (9), and Lemma
D.1, we obtain that
E[Rtm+1] ≤EJ(θt(m))+ct+1(ηθβt+1)kθt(m) -θe(m)k2
+ (- ηθ + ct+ι ηθ )e∣∣vj (Mm))I∣2
βt
+ (Lηθ + ct+ιηθ) [6LιE∣∣ω" - ω*(θ")k2 + 9LiE|0m) - s*(e(m))∣2
+ 9L2E∣∣θ(m) - e(m)k2 + M ∙ 9Cι + 9E∣VJ(θ(m))k2]
+ ηθ[ M + 1 kVJ (θ(m))k2 +Lιkω(m)-ω*(θ(m))k2]
+ 2ct+ιηθ [ 1M + kθ(m) - e(m)k2 + L1 ∣ω(m) - ω*(θ(m))k2].	(10)
15
Published as a conference paper at ICLR 2021
Wenote that the tracking error term ∣∣ω(m) - ω*(θ(m))k2 has dependence on ∣∣θ(m) - θ(m)k2. Here
we use a trick to merge this dependence to the coefficient ct+1. Specifically, we add and subtract the
same term in the above bound and obtain that
E[Rm+ι] ≤ EhJ(θ(m)) + [ct+ι(ηθβt + 1 + 2ηθ) + 9Lι(Lηθ + ct+ιηθ)]∣θ(m) - e(m)k2i
+	h- 2η + ct+ι βθ + 9(—η2 + ct+ιηθ)iE∣NJ (θ(m))k2
+	hηθ + 2ct+ιηθ + 9(—η2 + ct+ιη2)] M
+	9Lι(L2η2 + ct+ιηθ)E∣∣ω(m) - ω*(e(m))∣∣2
+	h6Ll( —η2 + ct+1η2) + ηθL1 + ηθL1ct+l] Ekω(m) - ω*(θ(m))k2
-	h6Ll( —η2 +	ct+1η2)	+ ηθ L1 + ηθ L1ct+l] ∙又	h12L2ηω	+ (又	+ 2L3)9L2 ηθ iEkθ(m)	-	θ(m)k2
+ h6Ll( —η2 +	ct+1η2)	+ ηθ L1 + ηθ L1ct+l] ∙	λ^	h12L2ηω	+ (λ^	+ 2L3)9L2 ηθ iEkθ(m)	-	θ(m)k2.
Then, we define Rtm := J(θt(m)) + ct ∣θt(m) - θe(m) ∣2 with ct being specified via the following
recursion.
ct = ct+ι(ηθ Bt + 1 + 2ηθ) + 9Lι(—ηθ + ct+ιη2)
+ h6Ll( — ηθ + ct+1η2) + ηθL1 + ηθL1ct+l] ∙ λ^ h12Liηω + (λ^ + 2L3)9L2 ηθi . (II)
Based on this definition, the previous inequality reduces to
ER+ι] ≤ ERm]+ h - 2ηθ + ct+1 ηθ + 9(Lη2 + ct+ιηθ)]Ek^J(θ(m))k2
+ [ηe + 2ct+ιηθ + 9(—ηθ + ct+ιη2)] C1
+ 9Lι(Lη2 + ct+ιηθ)E∣ω(m) - ω*(e(m))k2
+ h6Ll( —η2 + ct+1η2) + ηLI + ηL1ct+l] Ekω(m) - ω*(θ(m))k2
-h6Ll( —η2 + ct+1η2) + η L1 + η L1ct+l] •无 h12L2ηω + ( λ^ + 2L3)9L2 ηθ ]Ekθ(m) - θ(m)k2.
ω(12)
Assume that ct ≤ bc for some universal constant cb > 0 (we will formally prove it later). Then, we
sum the above inequality over one epoch and obtain that
M-1
h 1 ηθ -瑞-9(Lηθ + bη2)] X EkVJ(θ(m))k2
2	βt	2	t=0
≤ E[Rm] - E[RM] + [ηe + 2bηθ + 9(—ηθ + bη2)]cι + 9Lι(—n + 啕ME∣ω(m) - ω*(e(m))k2
M-1
+ [6Ll( —η2 + bηθ) + ηθL1 + ηθLIbl X Ekω(m) - ω*(θ(m))k2
2	t=0
2 M-1
-h6Ll( —η2	+	bηθ)	+ ηθ L1 +	ηθ LIbI	•又 h12Li ηω	+ (λ^7	+ 2L3)9L2 ηθ ]	X Ekθ(m) - θ(m)k2.
(13)
16
Published as a conference paper at ICLR 2021
By Lemma D.7, we have that
M-1
X Ekω(m) - ω*(θ(m))k2
t=0
≤ e h;+ M [(±+2L3)9L2 η2 + 18L2ηω iiEkω(m) - ω"m))k2
+ λC ( λ9C + 2L3) ηl ∙ 9C1 + λC ηω ∙ 12C2 + ( λ9C + 2L2)专 λ36 X1 EkVJ (Nk2
2 M-1
+ λ4- [12L5ηω + (W + 2L3)9L2 η∣i X Ekθ(m) - e(m)k2
8
+「(C3 + C4).
λC
For simplicity, We define D := 6L1 (L + b + Li + LIb Substituting the above bound into the
previous inequality and simplifying, we obtain that
M-1
h 1 ηθ - bβθ - 9(2ηθ + bη∣)i X EkVJ(θ(m))k2
2 βt 2	t=0
≤ ERm - ERM + [ηθ	+ 2bηθ	+ 9(2η∣	+ bη∣)] Ci +	9Li (ILη∣	+ bη∣)ME|@(m)- ω*(e(m))k2
+ Dηθ
λ⅛ hη1 + M[(A + 2L∣)9L2ηl + 18L4ηω]]Ekω(m) - ω*(e(m))k2
+λ4c(λ9c+2L2) η∣ ∙ 9Ci+λ4cηω ∙ 12C2+(λ9c+2L3) η∣ 3c XEkVJ (θ(m))k2+。(C3+C4).
One can see that the above bound is independent of PtM=-0 i Ekθt(m+i) - θe(m)k2, and this is What
We desire. After simplification, the above inequality further implies that
hlηθ-bηθ - 9(fη2+bηθ)- D(τ^ + 2L3)v-ηθi X EkVJ(θ(m))k2
2	βt	2	λC	λC ηω2 t=0
≤E[R0m] - E[RmM]
+ [ηθ + 2bηθ + 9(2η∣ + 流)]。1 + A(C3 + C4)Dηθ + Dηθ [. (A + 2L∣)	∙ 9Ci + .ηω ∙ 12C∣]
L
+ 9L1 (Iηθ+bη2)M+Dηθ λC hηω
2
+ M h(λ9; + 2L3)9L1 ηl + 18L∣ηωiJ E∣Q(m) - ω*(e(m))k2.
(14)
Choose optimal learning rates: Here, We provide the omitted proof of our earlier claim made
after eq. (12), that is, the upper bound of {ct} is a small constant. We first present the folloWing
fundamental simple lemma, and the proof is omitted.
Lemma B.1. Let {ci}i=0,...,M be a finite sequence with cM = 0 and satisfies the following relation
for certain a > 1:
Ct ≤ a ∙ ct+i + b.
Then, {ci}i=0,...,M is a deceasing sequence and
co ≤ ab ∙
aM - 1
a-1
17
Published as a conference paper at ICLR 2021
Next, we derive the upper bound cbof ct. Set βt = 1 for all t. Then we have that
Ct ≤ ct+ι(ηθ + 1 + 2ηθ) + 9Lι(2η2 + ct+ιη2)
+ [6Ll( —η2 + ct+1ηθ ) + ηθ LI + ηθ LI ct+l] ∙ λ^ h12L2ηω + ( λ^ + 2L2)9L2
:=a ∙ Ct+ι + b
where
a = 1 + (3 + 16L1)ηθ
and
b = '— L1Lη2 + Lme ∙又 h12L∣ηω + (~λ^ + 2L3)9L2 ηθi .
Note that here we require
又[12L2ηω + (— + 2L2)9L2 η2i ≤ 1	(15)
and
max{ηω , ηθ} ≤ 1.	(16)
Moreover, let
(3 + 16Lι)ηe ≤ M.	(17)
Based on the above conditions, we obtain that
co ≤ ∖-^-LILne + ʒ-1 h12L2ηω + (ɪ + 2L2)9L2ηθii • : + ：：：1 ∙ h(1 + (3+ 16Lι)ηe)M - 1i
2	λC	5	λC	3	2 ηω2	3 + 16L1
≤ ∖ 125LL ^C ∖ι2L5nω + (λc + 2L3)9L2nθii ∙ 3Γ⅛L7 ∙(e -1).
Lastly, we choose
∖"LiLne + 4L1 h12L2nω	+ (2	+ 2L2)9L2吗ii	∙ 4±^L1	∙ (e	- 1) ≤ 1.	(18)
L 2 1 n + λC L 5 ω	+ vλC	+	37	2nω	3+16L1	(	) ≤ 8	()
Therefore co ≤ 8. Since {ct}t is decreasing, We obtain that b = 1. Now, substituting βt = 1 and
b = 1 into the coefficient of the term PM=-I EkVJ (θ(m))k2 in eq. (14), the coefficient reduces to
the following, and we choose an appropriate (ne, &) such that the coefficient is greater than 4 ne.
3ne - 9(—n2+bn2)- D( λC+2L2)λ∣ n2 ≥ 4ne.	(19)
Deriving the final bound: Exploiting the above conditions on the learning rates, eq. (14) further
implies that
M-1
Wne X EkVJ(θ(m))k2
4 t=o
≤E[J(θe(m))] - E[J(θe(m+1))]
+ ∖ne + 2bne + 9(—n2 + bn2)iCi + A(C3 + C4)Dne + Dnehλ-(A + 2L3)n2 ∙ 9Ci + λLnω ∙ 12C2i
9Li (2 n2 + bn2)M + Dne
+
A ∖n~ + M ∖( A + 2L3)9L2n2 + 18L2nωiij Ekω(m) - ω*(e(m))k2.
(20)
18
Published as a conference paper at ICLR 2021
On the other hand, by Lemma D.8 we have that
Ekω(m) - ω*(θe(m))k2 ≤ (1 - 1 λcηω)mMEkω(O)- ω*(e⑼)∣∣2
+ τ~ (C3 + C Kr + τ-H 2ηω + τ- (2L3G2 + τ~G2) ηθ^ ∙
λC	M λC	λC	λC ηω2
Substituting the above bound into eq. (20) and summing over m, we obtain that
T M-1
ZηθTM X X EW(θ(m))k2
m=1 t=0
≤ TM E[J (e(0))]
+ 而{[ηθ + 2bηθ + 9(万ηθ2 + bη2)i CI + ʒ-(C3 + C4)Dηθ + Dηθ [ʒ-(ʒ—+ 2L2)当• 9Ci + 丁ηω • 12C∣iO
M	2	λC	λC λC	ηω2	λC
+ TM [9Lι( 2 ηθ + bη2)M
+ Dηθ h I h:+ M [(A + 2L3)9L2 ηl + 18L4ηω]iii ∙ Ekω ⑼一ω*(ee⑼)『∙
+ M [9L1 (2 ηθ+bη2)M+Dηθ h λc h ηω+M [(A+2L3)9L2 ηθ + 18L4ηω iii
• hɪ (C3 + C4)⅛ + ɪH2ηω + ɪ (2L∣G2 + ɪG2)图].
C	M C	C 3	C	ηωl
1
1 -(I - 1 λCηω )M
Rearranging the above inequality, we obtain the following final bound, where ξ, ζ are random in-
dexes that are sampled from {0, ..., M - 1} and {1, ..., T} uniformly at random, respectively.
EkVJ (θ(ζ))k2
≤-ɪ • 4E[J0°))]
ηθ TM
+M {hι+2b+9( L ηθ+bηθ)]Ci+λc(C3+C4)D+Dh ʌe( A+2L3) η∣ • 9C1+XC ηω • 12C2io
1
+——
+TM
9Li (2 ηθ + bηθ )M + D
Xc h ηω+M h(三+2L3)9Liη∣+18L4ηω ]] J
. E∣∣ω(O)- ω*(θe(0))k2 .
1
1 - (1 - 1 λCηω )M
+ 9L1(Lηθ+bηθ)+D 士 hη⅛+h(XC+2L2)9L2η∣+18L2ηω]]
.[止(C3 +	C4) E +	止H2ηω	+ }^^ (2LlG2 +	FG2)	ηl]
XC	M	XC	XC	XC	ηω∣
Next, we simplify the above inequality into an asymptotic form. Note that the first term is in the order
2
of O(n@TM). The second term is of order O( M). The third term is of order O(. TM + T (ηω + nθ)),
and the last term is the product of a term of order O(+ ηω + n~M) and another term of order
O(MM + n + ηω), which leads to the overall order O((患 + ηω)2 + M). Combining these asymptotic
orders togωether, we obtain the following asymptotic conωvergence rate result.
EkVJ (皆)k2 = o( dM +(I+ηω)2 + M∙ + T (ηω +	)).
19
Published as a conference paper at ICLR 2021
C Proof of Corollary 4.6
Regarding the convergence rate result of Theorem 4.5, we choose the optimized learning rates such
that ηθ = O(n」/2), and We obtain that
EkVJ (θξζ))k2 = o( ηθTM + ηω + O + ηω).
Then, we set η = O(M) such that eq. (17) is satisfied, and moreover & = O(^2/3). Under
this learning rate setting, the learning rate conditions in eq. (15), eq. (16), eq. (18), eq. (19) are
all satisfied for a sufficiently large constant-level M. Then, the overall convergence rate further
becomes
EkVJ (θ(ζ))k2 = θ(T + -M).	(21)
By choosing T, M = O(-1), we conclude that the sample complexity for achieving
EkVJ(θξ(ζ))k2 ≤ isintheorderof TM= O(-2).
D	Technical Lemmas
In this section, we present all the technical lemmas that are used in the proof of the main theorem.
Bounding Ekgt(m) k2 and Ekh(tm) k2 :
Lemma D.1. Under the same assumptions as those of Theorem 4.5, the square norm of the one-step
update of θt(m) in Algorithm 1 is bounded as
Ekg(m)k2 ≤ 6L2E∣∣ω(m) - ω*(θ(m))k2 + 9LlE|@(m) — ω*(ee(m))k2 +9L∕Ekθ(m) - e(m)『
+ MM9Cι + 9EkVJ (θ(m))k2
where the constant C1 is specified in Lemma D.3.
Proof. For convenience, define
Ttm := G"(θ”,ω”)-G(m)(e(m),ω(m)),
and
Stm) := G(m) (θ(m), ω* (θ(m))) - Gtm) 0m), ω* 0m))).
Then, we obtain that
kgt(m)k2 = kGt(m)(θt(m),ωt(m)) -Gt(m)(θe(m),ωe (m)) + Ge(m)k2
=∣∣τt(m) + Gg)- Gg)(e(m),“*(e(m))) + G(m)(e(m), ω*0m4)
- St(m) + St(m)k2
≤ 3∣Tt(m) -S(m)k2 + 3||G(m) - G(m)@m),“*(e(m)))『
+ 3kS(m) + G(m)(e(m) ,ω*(ee(m)))k2
≤ 6Ll∣ω(m) - ω*(θ(m))k2 +9L2|@(m) - ω*(e(m))k2
+ 3kS(m) + G(m)(e(m) ,ω*(ee(m)))k2,
where G(m')(θ(m'),ω* (θ(m))) is obtained by substituting the arguments θ(m),ω* (θ(m)) into the def-
inition in eq. (4). Moreover, we have that
kS(m) + G(m)(e(m) ,ω*(ee(m)))k2
=kS(m) + G(m)(e(m),"*(e(m))) - VJ (θ(m)) + VJ (θ(m))k2
≤ 3kS(m) - Em,tS(m)『 +3k<5(m)(θ(m),ω*(θ(m))) -VJ(θ(m))k2
+ 3kVJ(θt(m))k2,
20
Published as a conference paper at ICLR 2021
which further implies that
EkSt(m) + Ge(m)(e(m),ω*(e(m)))k2
≤ 3EkS(m)『+ 3Ek<5(m)(θ(m),ω*(θ(m))) - VJ(θ(m))k2 + 3E∣∣VJ(Mm))k2
≤ 3L2E∣Wm) - e(m)k2 + ɪ ∙ 3C1 + 3EkVJ(Mm))『.
Combining all the above bounds, we finally obtain that
Ekg(m)k2 ≤ 6L2E∣∣ω(m) - ω*(θ(m))k2 +9L2E∣∣ω(m) - ω*(e(m))k2
+ 9L2Ekθ(m) - e(m)k2 + ɪ ∙ 9C1 +9EkVJ(Mm))k2.
□
Lemma D.2. Under the same assumptions as those of Theorem 4.5, we have that
∣∣h(m)k2 ≤ 6L2kω(m) - ω*(θ(m))k2 +9L2|@(m) - ω*(e(m))k2 +6L2∣∣θ(m) - e(m)∖∖2 + 6C2.
Proof. For convenience, define
Vt(m) := Ht(m)(et(m),ωt(m)) -Ht(m)(ee(m),ωe(m)),
and
Ut(m) ：= Htm) (e(m) ,ω* (e(m))) - Htm) (e(m) ,ω* (e(m))).
Then, we obtain that
kht(m)k2 = kHt(m)(et(m),ωt(m)) -Ht(m)(ee(m),ωe(m)) +He(m)k2
=|%(m) + H(m) - H(m) (e(m),ω*(e(m))) + H(m)(θ(m),ω*(e(m))) - Ut(m) + Ut(m)k2
≤ 3∣Vt(m) - Ut(m)∣2 + 3∣H(m) - H(m)(e(m),ω*(e(m)))k2
+ 3∣Ut(m) + H (m)(e(m),ω*(e(m)))k2
≤ 6L4kω(m) - ω*(θ(m))k2 + 9L4∣ω(m) - ω*(e(m))k2
+ 3∣Ut(m) + H (m)(e(m),ω*(e(m)))k2.
Moreover, note that
∣∣Ut(m) + H(m)(e(m),ω*(e(m)))k2 ≤ 2kUt(m)『 + 2∣H(m)(e(m),ω*(e(m)))k2
≤ 2L2∣ Wm)- e(m)k2 + 2M2.
Combining the above bounds, we finally obtain that
∣∣h(m)k2 ≤ 6L4kω(m) - ω*(θ(m))∣2 + 9L2∣Q(m) - ω*(θe(m))k2
+ 6L2kθ(m) - e(m)k2 + 6M2.
□
Bounding pseudo-gradient variance:
Lemma D.3. Under the same assumptions as those of Theorem 4.5, we have that
EkG(m)(e(m),ω*(e(m))) -vj(e(m))k2
≤ MM.
21
Published as a conference paper at ICLR 2021
Proof. Note that the variance can be expanded as
EkGe(m)(θ(m), ω*(θ(m)))- VJ(θ(m))k2
=ME[ X kGSm)(θ(m),ω*(θ")) -VJ(θ(m))k2
M	s=0
+ XhGim) (θ(m), ω* (θ(m)))- VJ(θ(m)), Gjm) (θ(m), ω* (θ(m)))- VJ(θ")》]
i6=j
≤ M2 [ X (G + CyJ)2 + X λPIi-j1 (G + CVJ)2]
s=0	i6=j
≤ 0(1 + 丛1-^)(G + CVJ )2.
Then, We define the constant Ci := (1 + 2Λɪ-^)(G + CNJ)2.	□
Lemma D.4. Under the same assumptions as those of Theorem 4.5, we have that
EkH(m)(e(m),ω*(e(m)))k2 ≤
CI
M.
Proof. Note that this second moment term can be expanded as
M-1
EkH(m)(e(m),ω*(e(m)))k2 = __ X EkHi(m)(e(m),ω*(e(m)))k
M	i=0
十 击 X EhHi(m) (e(m) ,ω* (e(m))), Hjm) (e(m) ,ω* (e(m)))〉
i6=j
≤ H + +HA X PTI
i6=j
≤ H2(1 + λE)MM
Lastly, we define the constant C_ := H2(1 + AI-P).
□
Bounding Markovian Noise:
Lemma D.5. Let the same assumptions as those of Theorem 4.5 hold and define
ς(m) := hω" -ω*(θ(m)), (φ(m)(φ(m))> - C)(ω(m) -ω*(θ(m)))i
Then, it holds that
E[ς(m)] ≤ 1 λckω(m)-ω*(θ(m))k2 + M,
where C3 = 8RC2 (1 + 篝)∙
Proof∙ By definition of ςt(m), we obtain that
Ehω(m) -ω*(θ(m)), (φ(m)(φ(m))> - C)(ω" -ω*(θ"))>
=E(ω(m) -ω*(θ"), Em,t-i(φ(m)(φ(m))> - C)(ω" - ω*(θ"))>
2	M-1
≤5∙*Eks" - ω*(θ")k2 + - ∙ -- Ek X Wm(Φim))> - C)k2.
2	4	2 λCM
i=0
22
Published as a conference paper at ICLR 2021
For the last term, note that
M-1
Ek X (Φ(m)(Φ(m))>-C)k2
i=0
M-1
=E X kφi(m)(φi(m))> -Ck2+EXhφi(m)(φi(m))> -C,φj(m)(φ(jm))> -Ci
i=0	i6=j
≤ 4M + 4 X Λρli-jl
i6=j
≤ 4M + 4M-^λ~.
Combining the above bounds, we finally obtain that
E(ω" - ω*(θ"),(φ(m)(φtm))> - C)(ω(m) - ω*(θ(m)))i
≤λCEkω(m) - ω*(θ")k2 + 8R2(1 + a /
8	λC	1 - ρ M
Wedefine C3 := 8R2(1 + £).	□
Lemma D.6. Let the same assumptions as those of Theorem 4.5 hold and define
K":=(ω" -ω*(θ”),[(φ ”)>ω*(θ")-碑)(。(叫 φ”).
Then, we obtain that
Eκ(m) ≤ 1 λckω(m)-ω*(θ(m))k2 + M.
Proof. Similar to the proof of Lemma D.6, we have that
Ehω(m) -ω*(θ”), [(φ”)>ω*(θ(m))--]。")
≤ 1 ∙ λCEkω(m)-ω*(θ(m))k2
M-1
+ 1 ∙ Γ⅛2Ek X ([(φim))>ω*(θ")- δ(mI)(θ")]φ")k2.
2 λcM2	i=0
For the last term, we can bound it as
M-1
Ek X ([(φ*)>ω*(θ")- δ(mI)(θ(m))]φ(m))k2
i=0
≤(R(2 + Y) + rmaX)2M + (R(2 + Y) + rmaX)2 J PM.
Combining all the above bounds, we finally obtain that
Ehω严-ω*(θ(m ),[(φ严)>ω*(θ(?-端(。(叫 φ”
≤λCEkω" - ω*(θ”k2 + 9(R(2 + γ) + 小)2(1 + /^)ɪ.
8	λc	1 - ρ M
We then define C4 :=3(R(2 + Y) + rmax)2(1 + £).	□
Bounding Tracking Error:
23
Published as a conference paper at ICLR 2021
Lemma D.7. Under the same assumptions as those of Theorem 4.5, the tracking error can be
bounded as
M-1
X Ekω(m)-ω*(θ(m))k2
t=0
≤ I h}+ M [([+2L3)9L2 η2 + 18L2ηω]]Ekω(m) - ω"m))k2
+ λC ( λ9C + 2L3)4∙ 9C1 + λC ηω ∙ 12C2 + ( λ9C + 2L2)专 λ36 X1 EkVJ 仍)k2
2 M-1
+ λ- [12L5ηω + (W + 2L3)9L2 2ηi X Ekθ(m) - e(m)k2
8
+=(C3 + C4).
λC
Proof. Recall the one-step update at ωt(+m1):
ω(m) =Πr (ω(mjω h(m)).
Then, We obtain the following upper bound of the tracking error ∣∣ω(m) 一 ω * (θ(m)) k2,
kω(m) -ω*(θ(m))k2 ≤ kω(m) -ω*(θ(m)) 一 &h" + ω*(θ(m)) 一 ω*(θ(m))∣2
≤ kωt(m) - ω*(θt(m))k2 - 2ηωhωt(m) - ω*(θt(m)), ht(m)i
+ 2hωt(m) - ω*(θt(m)), ω*(θt(m)) - ω*(θt(+m1))i
+ 2ηω2 kht(m)k2 + 2kω*(θt(m)) -ω*(θt(+m1))k2.
Substituting the bound of Lemma D.2 into the above bound, we obtain that
kωt(+m1) -ω*(θt(+m1))k2
≤ kωt(m) -ω*(θt(m))k2 -2ηωhωt(m) -ω*(θt(m)),ht(m)i
+λCηωkωt(m) -ω*(θt(m))k2
+ (ɪ + 2L2) η2 ∣6L2Ekω(m) - ω*(θ(m))∣2 +9L1E∣ω(m) - ω* (θe(m')')f
+ 9L2E∣θ(m) - e(m)k2 + M ∙ 9Cι + 9E∣VJ(θ(m))k2]
+ 2宿 ∣6L4Ekω(m) - ω*(θ(m))∣2 + 9L4E∣ω(m) - ω*(e(m))k2 +6L2E∣θ(m) - θ(m)∣2 + 6C2].
Taking expectation on both sides of the above inequality and simplifying, we obtain that
Ekωt(+m1) - ω*(θt(+m1))k2
≤ (1 - λCηω + (τ	+ 2L2)6L1 ηθ + 12L4η2)Ekω(m) - ω*(θ(m))k2
λC	ηω
+ h(/ + 2L2)9L2啥 + 18L4*]E|0m) - ω*(e(m))k2
+( λ9c+2L2) η2 ∙ M+* •若+( λ9c+2L2) ηω9EkVJ 就叫『
+ [12L5* + (' + 2L3)9L2 η2 ]Ekθ(m) - e(m)k2
- 2ηωEκt(m) - 2ηωEςt(m),
(22)
24
Published as a conference paper at ICLR 2021
where
and
ς(m) := hω(m) -ω*(θ(m)), (φ(m)(φ(m))> - C)(ω(m) -ω*(θ(m)))i,
K":=(ω" -ω*(θ”), [(φ”)>ω*(θ")-碑)就叫 φ”).
Applying Lemma D.5, and Lemma D.6 to (22), we obtain that
Ekω(mi - ω*(θ(mι)k2 ≤ (1 - XλCηω + K + 2L2)6L2ηθ + 1”祝)Ekω(m) - ω*(θ(m))k2
2	λC	ηω
+ [(λ9- + 2L2)9L1 η + 18L4*]E|@(m) - s*(e(m))『
+ (F +2L2)喷.8 9 *C + 危.哈 + (三 +2L2)喷9EkVJ(θ(m))k2
λC	ηω	M ω M λC	ηω
+ [i2L5ηω + (λ9- + 2L3)9L2 η2 iEkθ(m) - e(m) k2
+ 2ηω (C3 + C4) M∙
Telescoping the above inequality over one epoch, we obtain that
2	M-1
(5 λC ηω -	( ʌ	+ 2L2)6L2 -	- 12L4ηω)	X	Ekω(m)	- ω*(θ(m) )k2
2	λC	ηω	t=0
≤ Ekω(m) - ω*(θ(m))k2
+ M h(ɪ + 2L3)9L2η2 + 18L2*]E|Q(m) - ω*(e(m))k2
2	2 M-1
+ (ɪ + 2L3) ηθ ∙ 9Cι + ηω ∙ 12C2 + (ɪ + 2L3) ηθ9 X EkVJ—
λC	3 ηω	ω	λC	3 ηω	t=0	t
2 M-1
+ [i2L2ηω + (工 + 2L3)9L2ηθ] X Elwm)- e(m)k2
+ 2ηω (C3 + C4).
Choosing an appropriate (ηθ, ηω ) such that
5λCηω - (τ----+ 2L3)6L2 ηθ 一 12L4ηω ≥ 彳λCηω,
2	λC 3	1 ηω	4 ω 4
and we finally obtain that
M-1
(23)
(24)
X Ekωt(m)
t=0
—
≤ λ- h η~+ M [( <+2L2)9L2 η2 + 18L2ηω iiE∣ω(m) - ω*(e(m))k2
+ λC (三 + 2L3)?∙ 9C1 + 口 ω ∙ 12C2 + ( λC + 2L2) I λC X1 EkVJ (θ”k2
2 M-1
+ 5 [l2L2ηω + (三 + 2L2)9L2ηθi X Ekθ(m) - e(m)k2
8
+ τ-(C3 + C4).
λC
□
25
Published as a conference paper at ICLR 2021
Lemma D.8. Under the same assumptions as those of Theorem 4.5, the tracking error can be
bounded as
Ekω(m) - ω*(θe(m))k2 ≤ (1 - 1 λcηω)mME∣∣ω(O)- ω*(e⑼)k2
+士(C3+C4)1+士 H 2ηω+W (2L3G2+λC G2)⅛.
Proof. Recall the one-step update at ωt(+m1):
ω(m) =Πr (ω(m) - ηωh(m))
Then, We obtain the following upper bound of the tracking error ∣∣ω(m) - ω*(θ")k2.
kω(m) -ω*(θ(m))k2 ≤ ∣ω(m) -ω*(θ(m)) - ηh" + ω*^m)) - 9*(。圈)||2
≤ ∣ω” -ω*(θ")k2- 2&hω(m) -ω*(θ(m)),h(m)i
+ 2(ω”-ω*(θ"),ω*(θ ")-ω*(θ(m) ))
+ 2* H2 +2kω*(θ(m))-ω*(W))k2.
Then above inequality can be further bounded as
Is* -ω*(璐)∣2 ≤ ∣ω(m) -ω*(θ(m))k2 - 2&(ω(m) - ω*(θ(m)), h(m)i
+ λc ηω kω(m)-ω*(碎 m))k2
+ ("+2L3)喷 G2
λC	ηω
+ 2ηω2 H2 .
Taking conditional expectation on both sides of the above inequality, we obtain that
Em,okω(m)-ω*(*)k2
≤ Em,°kω"-ω*(θ")k2- 2ηωEm,0hSm -3*(θ(T[,Hm(θ浮,3浮»
-2ηωEm,0hω(m) - 3*(θ"), -Ht(m)(e(m),3(m)) + HI(m)i
+ λcηω k3(m)-3*(θ(m))k2 + 兽G2
λCηω
+ 2H2ηω2 + 2L23G2ηθ2
=Em,0kω(m) -3*(θ”)k2 - 2ηω Em,0 <3"-3* (θ(m) ) ,H(m) (θ(m) ,3")〉
+ λcηωkω(m) - ω*(θ(m))k2
+ 2H 2ηω + (2L3G2 + ɪ G2)喷	(25)
λC	ηω
To further bound the inequality above, we first consider the following explicit form of the pseudo-
gradient term:
Ht(m)(θ,3)= (φt(m))>3 - δt(+m1)(θ)φt(m)
=φtm)(φ(m))>(ω - ω*(θ)) + [(φ(m))>3*(θ(m)) - δ(m)(θ)]φtm)
=(φtm)(φ(m))> - C)(ω - 3*(θ)) + C(3 - 3*(θ))
+ (φt(m))>3*(θ) - δt(+m1)(θ)φt(m).	(26)
By Assumption 4.3, we have
-2ηωEm,ohω(m) - 3*(θ(m)),C(3(m) -3*(θ(m))〉≤ *。∣ω” - 3*(θ")12.	(27)
26
Published as a conference paper at ICLR 2021
Substituting eq. (27) and eq. (26) into eq. (25) yields that
Ekω(m) - ω*(θ(m))k2 ≤ (1-λc狐旧心”-ω*(θ")k2 - 2&EK(m) - 2/旧,尸
+ 2H 2ηω + (2L3G2 + A G2)!，	(28)
where
and
ς(m := (ω(m) -ω*(θ"), (φ(m)(φ(m))> - C)(ω" - ω*(θ"))》,
Ktm ：= hω" -ω*(θ(m)), [(φ")>ω*(θ")-碑)(成叫。”).
Applying Lemma D.5, and Lemma D.6 to the above inequality, we obtain that
Ekω(m) -ω*(θ(m))k2 ≤ (1 - 2λc国网9"-ω*(θ")k2
+ 2ηω (C3 + C4)M
+ 2H2ηω + (2L3G2 + ɪ G2)哈
ω 3	λC	ηω
Telescoping the above inequality over one epoch, we obtain that
Ekω(m) - ω*(θ(m))k2 ≤ (1 - 2λc加)ME∣∣ω0m) - ω*(θ0m))∣∣2
+ 2ηω (C3 + C4) ɪ ∙ -—-
M	2 λcηω
+ 2H2ηω ∙ 1-(11- 1 λCηω)M + (2L2G2 + ɪG2)喷∙ 1-(11- 1 λCηω)M
2 λCηω	λC	ηω	2 λC&
By definition, ωe (m) = ωM(m) and θe(m) = θM(m) , and the initial parameter for the current inner loop is
chosen as the reference parameter, ω0(m) = ωe (m) and θ0(m) = θe(m). Then we have
E|@(m) - ω*(e(m))k2 ≤ (1 -1 λcηω严E|@(m) - ω*(e(m))∣∣2
(	)1	1 -(I- 2 入 C ηω 产
+ 2ηω(°3 + °4)M------鼻嬴一
+ 2H2ηω ∙ I-(II-2λCηω)M + (2L3G2 + ɪG2)喷
2 λcηω	λC	ηω
1 - (1 - 1 λCηω 严
2 λC ηω
Then, we unroll the inequality above and yield that
E∣∣ωtm) - ω*(e(m))k2 ≤ (1 - 1 λcηω)mME∣∣ω⑼-ω*(e(0))k2
+ τ~(C3 + C4)T7 + JH2ηω + ɪ (2L2G2 + ^V~G2) ηθ.
λC	M λC	λC	λC ηω2
□
E Other S upporting Lemmas
Constant Bounds:
Lemma E.1. Within the set {θ : kθ∣∣ ≤ R} ,there exists a constant CyJ such that
sup ∣∣VJ(θ)k ≤ CyJ.	(29)
θ
Proof. By Lemma E.9, VJ(θ) is smooth. Hence, by the compactness of {θ : ∣∣θ∣ ≤ R}, We
conclude that ∣ VJ(θ)∣ is bounded by a certain constant CyJ.	□
27
Published as a conference paper at ICLR 2021
Lemma E.2. Let G :=『m&x +(1+ Y)R + γ(∣A∣Rkι + 1)R be a constant unrelated to m and t.
Then kGt(m) k ≤ G for all m and t.
Proof. By its definition, we obtain that
kGtm)k = k ( - δt+ι(θt)φt + γ(ω>φt)φt+ι(θt)) k
≤k(- δt+ι(θt)kkΦtk + γk(ω>φt)kkbt+ι(θt))k
≤ rmax + (1 + γ )R + γ (|A|Rk1 + 1)R.
□
Lemma E.3. Let H = (2 + γ)R + rmax be a constant unrelated to m and t. Then kHt(m) k ≤ H
for all m and t.
Proof. The result follows from the definition:
kHt(m)k = kφtTωt-δt+1(θt)φtk
≤ (2 +γ)R+ rm
ax.
□
Lipschitz Continuity:
Lemma E.4. The mapping ω 7→ G(t+m1)(θ, ω) is L1-Lipschitz in ω for all θ.
Proof. See Lemma 3 of Wang & Zou (2020).	□
Lemma E.5. The mapping θ → Gtm) (θ, ω*(θ)) is L2-LiPschitz in θ.
Proof. See Lemma 3 of Wang & Zou (2020).	□
Lemma E.6. The mapping ω*(∙) is L3-Lipschitz.
Proof. See eq.(56) of Wang & Zou (2020).	□
Lemma E.7. The mapping ω 7→ Ht(+m1)(θ, ω) is L4-Lipschitz.
Proof. It follows that
kH(mI)(θ,ω2) - H(mI)(θ,ω2)k = k(δ(m)(θ) - [φ(m)]Tωι)φ(m) - (δ(m)(θ) - [φ(m)]Tω2)φ(m)k
≤ kφ(tm)k2kω1 - ω2k
≤ kω1 - ω2k.
Hence, L4 = 1.	□
Lemma E.8. The mapping θ → HtmI(θ, ω*(θ)) is L5-Lipschitz.
Proof. By definition, we have
kH(m)(θι,ω*(θι))- H(m)(θ2,ω*(θ2))k
=k(δ(m)(θι) - [φ(m)]Tω*(θι))φ(m) - (δ(m)(θ2) - [φ(m)]Tω*(θ2))φ(m)k
≤kδ(m)(θι)- δ(m")k + ∣∣ω*(θ1)-ω*(θ2)k
≤ ((γ∣A∣kιR +1) + 1 + L3)kθ1 -θ2k.
Hence, L5 = (γ∣A∣kιR +1) + 1 + L3.	□
Bounding Lyapunov function:
28
Published as a conference paper at ICLR 2021
Lemma E.9 (L-smoothness of J). For any θ1 and θ2, it holds that
| J(θι) - J(θ2) - hVJ(θ2), θι - Θ2i∣ ≤ ]∣∣θι - θ2k2.
Proof. See Lemma 2 of Wang & Zou (2020).
Lemma E.10. It holds that
kθt(+m1) - θe(m)k2 = ηθ2kgt(m)k2 + kθt(m) -θe(m)k2-2ηθζt(m)
+ ηθhIkVJ(θ(m))k2 + βtkθ(m) - e(m)k2i，
where ζt(m) := hgt(m) - VJ(θt(m)),θt(m) - θe(m)i.
□
(30)
Proof. Note that
kθt(+m1) -θe(m)k2 = kθt(+m1) - θt(m)k2 + kθt(m) - θe(m)k2 + 2hθt(+m1) -θt(m),θt(m) -θe(m)i
= ηθ2kgt(m)k2 + kθt(m) -θe(m)k2 -2ηθhgt(m),θt(m) -θe(m)i
= ηθ2kgt(m)k2 + kθt(m) -θe(m)k2 -2ηθhgt(m) -VJ(θt(m)),θt(m) -θe(m)i
- 2ηθhVJ(θt(m)),θt(m) - θe(m)i
= ηθ2kgt(m)k2 + kθt(m) -θe(m)k2 -2ηθhgt(m) -VJ(θt(m)),θt(m) -θe(m)i
+ ηθ hβkVJ(θ(m))k2 + βtkθ(m) - e(m)k2i.
□
F Details of Experiments
Garnet problem: The Garnet problem Archibald et al. (1995) is specified as G(nS, nA, b, d), where
nS and nA denote the cardinality of the state and action spaces, respectively, b is referred to as the
branching factor-the number of states that have strictly positive probability to be visited after an
action is taken, and d denotes the dimension of the features. In our experiment, we set nS = 5,
nA = 3, b = 2, d = 4 and generate the features Φ ∈ RnS ×d via the uniform distribution on [0, 1].
We then normalize its rows to have unit norm. Then, we randomly generate a state-action transition
kernel P ∈ RnS ×nA ×nS via the uniform distribution on [0, 1] (with proper normalization). We set
the behavior policy as the uniform policy, i.e., ∏b(a∣s) = n[1 for any S and a. The discount factor
is set to be γ = 0.95. As the transition kernel and the features are known, we compute kVJ (θ)k2
to evaluate the performance of all the algorithms. We set the default learning rates as ηθ = 0.02
and ηω = 0.01 for both VR-Greedy-GQ and Greedy-GQ algorithm. For VR-Greedy-GQ, we set the
default batch size as M = 3000.
Frozen Lake: We generate a Gaussian feature matrix with dimension 8 to linearly approximate the
value function and we aim to evaluate a target policy based on a behavior policy. The target policy
is generated via the uniform distribution on [0, 1] with proper normalization and the behavior policy
is the uniform policy. We set the learning rates as ηθ = 0.2 and ηω = 0.1 for both algorithms and
set the batch size as M = 3000 for the VR-Greedy-GQ. We run 200k iterations for each of the 10
trajectories.
Estimated maximum Reward: In the experiments, we compute the maximum reward as follows:
When the policy parameter θt is updated to θt+1 , we estimate the corresponding reward by sam-
pling a Markov decision process {s1, a1, s2, a2, . . . , sN, aN, sN+1} using πθ. Then we estimate the
expected reward using
1N
rt = Nfr(Si,ai, si+1).
i=1
29
Published as a conference paper at ICLR 2021
Under the ergodicity assumption, this average reward will tend to the expected reward with respected
the stationary distribution induced by πθ (Wu et al. (2020)). Then the maximum reward is defined
as the maximum estimated expected reward along the training trajectory; that is,
Maximum Reward = max ^.
t
In the experiments, we set N = 100 when estimating the expected reward.
30