Published as a conference paper at ICLR 2021
Training GANs with Stronger Augmentations
via Contrastive Discriminator
Jongheon Jeong1 & Jinwoo Shin2,1
1 School of Electrical Engineering 2Graduate School of AI
Korea Advanced Institute of Science and Technology (KAIST)
Daejeon 34141, South Korea
{jongheonj,jinwoos}@kaist.ac.kr
Ab stract
Recent works in Generative Adversarial Networks (GANs) are actively revisiting
various data augmentation techniques as an effective way to prevent discriminator
overfitting. It is still unclear, however, that which augmentations could actually
improve GANs, and in particular, how to apply a wider range of augmentations
in training. In this paper, we propose a novel way to address these questions by
incorporating a recent contrastive representation learning scheme into the GAN
discriminator, coined ContraD. This “fusion” enables the discriminators to work
with much stronger augmentations without increasing their training instability,
thereby preventing the discriminator overfitting issue in GANs more effectively.
Even better, we observe that the contrastive learning itself also benefits from our
GAN training, i.e., by maintaining discriminative features between real and fake
samples, suggesting a strong coherence between the two worlds: good contrastive
representations are also good for GAN discriminators, and vice versa. Our ex-
perimental results show that GANs with ContraD consistently improve FID and
IS compared to other recent techniques incorporating data augmentations, still
maintaining highly discriminative features in the discriminator in terms of the lin-
ear evaluation. Finally, as a byproduct, we also show that our GANs trained in
an unsupervised manner (without labels) can induce many conditional generative
models via a simple latent sampling, leveraging the learned features of ContraD.
Code is available at https://github.com/jh-jeong/ContraD.
1 Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have become one of the most
prominent approaches for generative modeling with a wide range of applications (Ho & Ermon,
2016; Zhu et al., 2017; Karras et al., 2019; Rott Shaham et al., 2019). In general, a GAN is defined by
a minimax game between two neural networks: a generator network that maps a random vector into
the data domain, and a discriminator network that classifies whether a given sample is real (from the
training dataset) or fake (from the generator). Provided that both generator and discriminator attain
their optima at each minimax objective alternatively, it is theoretically guaranteed that the generator
implicitly converges to model the data generating distribution (Goodfellow et al., 2014).
Due to the non-convex/stationary nature of the minimax game, however, training GANs in practice
is often very unstable with an extreme sensitivity to many hyperparameters (Salimans et al., 2016;
Lucic et al., 2018; Kurach et al., 2019). Stabilizing the GAN dynamics has been extensively studied
in the literature (Arjovsky et al., 2017; Gulrajani et al., 2017; Miyato et al., 2018; Wei et al., 2018;
Jolicoeur-Martineau, 2019; Chen et al., 2019; Schonfeld et al., 2020), and the idea of incorporat-
ing data augmentation techniques has recently gained a particular attention on this line of research:
more specifically, Zhang et al. (2020) have shown that consistency regularization between discrimi-
nator outputs of clean and augmented samples could greatly stabilize GAN training, and Zhao et al.
(2020c) further improved this idea. The question of which augmentations are good for GANs has
been investigated very recently in several works (Zhao et al., 2020d; Tran et al., 2021; Karras et al.,
2020a; Zhao et al., 2020a), while they unanimously conclude only a limited range of augmentations
(e.g., flipping and spatial translation) were actually helpful for the current form of training GANs.
1
Published as a conference paper at ICLR 2021
Meanwhile, not only for GANs, data augmentation has also been played a key role in the literature of
self-supervised representation learning (Doersch et al., 2015; Gidaris et al., 2018; Wu et al., 2018),
especially with the recent advances in contrastive learning (Bachman et al., 2019; Oord et al., 2018;
Chen et al., 2020a;b; Grill et al., 2020): e.g., Chen et al. (2020a) have shown that the performance
gap between supervised- and unsupervised learning can be significantly closed with large-scale con-
trastive learning over strong data augmentations. In this case, contrastive learning aims to extract
the mutual information shared across augmentations, so good augmentations for contrastive learn-
ing should keep information relevant to downstream tasks (e.g., classification), while discarding
nuisances for generalization. Finding such augmentations is still challenging, yet in some sense,
it is more tangible than the case of GANs, as there are some known ways to formulate the goal
rigourously, e.g., InfoMax (Linsker, 1988) or InfoMin principles (Tian et al., 2020).
Contribution. In this paper, we propose Contrastive Discriminator (ContraD), a new way of train-
ing discriminators of GAN that incorporates the principle of contrastive learning. Specifically, in-
stead of directly optimizing the discriminator network for the GAN loss, ContraD uses the network
mainly to extract a contrastive representation from a given set of data augmentations and (real or
generated) samples. The actual discriminator that minimizes the GAN loss is defined independently
upon the contrastive representation, which turns out that a simple 2-layer network is sufficient to
work as a complete GAN. By design, ContraD can be naturally trained with augmentations used in
the literature of contrastive learning, e.g., those proposed by SimCLR (Chen et al., 2020a), which are
in fact much stronger than typical practices in the context of GAN training (Zhang et al., 2020; Zhao
et al., 2020c;a; Karras et al., 2020a). Our key observation here is that, the task of contrastive learning
(to discriminate each of independent real samples) and that of GAN discriminator (to discriminate
fake samples from the reals) benefit each other when jointly trained with a shared representation.
Self-supervised learning, including contrastive learning, have been recently applied in GAN as an
auxiliary task upon the GAN loss (Chen et al., 2019; Tran et al., 2019; Lee et al., 2021; Zhao et al.,
2020d), mainly in attempt to alleviate catastopic forgetting in discriminators (Chen et al., 2019).
For conditional GANs, Kang & Park (2020) have proposed a contrastive form of loss to efficiently
incorporate a given conditional information into discriminators. Our work can be differentiated to
these prior works in a sense that, to the best of our knowledge, it is the first method that success-
fully leverage contrastive learning alone to incorporate a wide range of data augmentations in GAN
training. Indeed, for example, Zhao et al. (2020d) recently reported that simply regularizing auxil-
iary SimCLR loss (Chen et al., 2020a) improves GAN training, but could not outperform existing
methods based on simple data augmentations, e.g., bCR (Zhao et al., 2020c).
2	Background
Generative adversarial networks. We consider a problem of learning a generative model pg from
a given dataset {xi}N=ι, where Xi 〜Pdata and Xi ∈ X .To this end, generative adversarial network
(GAN) (Goodfellow et al., 2014) considers two neural networks: (a) a generator network G : Z →
X that maps a latent variable Z 〜 p(z) into X, where p(z) is a specific prior distribution, and (b)
a discriminator network D : X → [0, 1] that discriminates samples from pdata and those from the
implicit distribution pg derived from G(z). The primitive form of training G and D is the following:
min max V(G,D) ：= Ex〜PdatjlOg(D(X))] + EZ〜p(z)[log(1 - D(G(Z)))].	⑴
For a fixed G, the inner maximization objective (1) with respect to D leads to the following optimal
discriminator DG, and consequently the outer minimization objective with respect to G becomes to
minimize the Jensen-Shannon divergence between Pdata and Pg: DG := max。V(G, D)=.广+P .
Although this formulation (1) theoretically guarantees Pg = Pdata as the global optimum, the non-
saturating loss (Goodfellow et al., 2014) is more favored in practice for better optimization stability:
max L(D) := V(G, D), and min L(G) := -EZ[log(D(G(Z)))].	(2)
Here, compared to (1), G is now optimized to let D to classify G(Z) as 1, i.e., the “real”.
Contrastive representation learning. Consider two random variables v(1) and v(2), which are
often referred as views. Generally speaking, contrastive learning aims to extract a useful repre-
sentation of v(1) and v(2) from learning a function that identifies whether a given sample is from
2
Published as a conference paper at ICLR 2021
p(v(1))p(v(2) |v(1)) or p(v(1))p(v(2)), i.e., whether two views are dependent or not. More specif-
ically, the function estimates the mutual information I(v(1); v(2)) between the two views. To this
end, Oord et al. (2018) proposed to minimize InfoNCE loss, which turns out to maximize a lower
bound of I(v(1); v(2)). Formally, for a given V(I)〜 p(v(1)) and vi(2) 〜p(v(2) ∣v(1)) while assum-
ing vj2) 〜p(v(2)) for j = 1,…，K, the InfoNCE loss is defined by:
LNCE(vi(1); v(2)
s) := - log
eχp(S(V(1), Vy)))
PK=I eχp(S(V(1), Vjl)))
(3)
where s(∙, ∙) is the score function that models the log-density ratio of p(v(2)∣v(1)) to p(v⑵),PoSSi-
bly including some parametrized encoders for v(1) and v(2).
Many of recent unsuPervised rePresentation learning methods are based on this general framework of
contrastive learning (WU et al., 2018; Bachman et al., 2019; Henaffet al., 2020; He et al., 2020; Chen
et al., 2020a). In this PaPer, we focus on the one called SimCLR (Chen et al., 2020a), that adoPts a
wide range of indePendent data augmentations to define views: sPecifically, for a given set of data
samPles x = [xi]iN=1, SimCLR aPPlys two indePendent augmentations, namely t1 and t2, to the
given data to obtain v(1) and v(2), i.e., (v(1), v(2)) := (t1(x), t2(x)). The actual loss of SimCLR
is slightly different to InfoNCE, mainly due to Practical considerations for samPle efficiency:
1N
LSimCLR(V(1), V(2)) := 2N ^X (LNCE(V(ZiV(2); V-i,bsSimCLR) + LNCE(V(4 ; [V(1); V-)], sSimCLR
2N i=1
(4)
where V-i := V \ {Vi}. For SSimCLR, SimCLR sPecifies to use (a) an encoder network f : X →
Rde , (b) a small neural network called projection head h : Rde → Rdp , and (c) the normalized
temperature-scaled cross entropy (NT-Xent). Putting altogether, SSimCLR is defined by:
SSimCLR(v(1), v(2); f, h) :
h(f(v(1))) ∙ h(f (v(2)))
τ ∙∣∣h(f (v(1)))l∣2∣∣h(f (V(2)))∣∣2 ,
(5)
where τ is a temPerature hyPerParameter. Once the training is done with resPect to LSimCLR, the
Projection head h is discarded and f is served as the learned rePresentation for downstream tasks.
3	C ontraD: Contrastive Discriminator for GANs
We aim to develoP a training scheme for GANs that is caPable to extract more useful information
under stronger data augmentation beyond the existing yet limited Practices, e.g., random transla-
tions uP to few Pixels (Zhang et al., 2020; Zhao et al., 2020c;a). For examPles, one may consider to
aPPly augmentations introduced in SimCLR (Chen et al., 2020a) for training GANs, i.e., a stochas-
tic comPosition of random resizing, croPPing, horizontal fliPPing, color distortion, and Gaussian
blurring. Indeed, existing aPProaches that handle data augmentation in GANs are not guranteed to
work best in this harsh case (as also observed emPirically in Table 5), and some of them are even
exPected to harm the Performance: e.g., aPPlying consistency regularization (Zhang et al., 2020;
Zhao et al., 2020c), which regularizes a discriminator to be invariant to these augmentations, could
overly restrict the exPressivity of the discriminator to learn meaningful features.
Our ProPosed architecture of Contrastive Discriminator (ContraD) is designed to alleviate such
Potential risks from using stronger data augmentations by incorPorating the contrastive learning
scheme of SimCLR (Chen et al., 2020a), which arguably works with those augmentations, and its
design Practices into the discriminator. More concretely, the main objective of ContraD is not to
minimize the discriminator loss of GAN, but to learn a contrastive rePresentation that is compatible
to GAN. This means that the objective does not break the contrastive learning, while the rePresenta-
tion still contains sufficient information to discriminate real and fake samPles, so that a small neural
network discriminator is enough to Perform its task uPon the rePresentation. We describe how to
learn such a rePresentation of ContraD in Section 3.1, and how to incorPorate this in the actual GAN
training in Section 3.2. Finally, Section 3.3 introduces a natural aPPlication of ContraD for deriving
conditional generative models from an unconditionally trained GAN. Figure 1 illustrates ContraD.
3
Published as a conference paper at ICLR 2021
Figure 1: An overview of Contrastive Discriminator (ContraD). Overall, the representation of Con-
traD is not learned from the discriminator loss (Ldis), but from two contrastive losses Lc+on and Lc-on,
each is for the real and fake samples, respectively. Here, sg(∙) denotes the stop-gradient operation.
contrastive
3.1	Contrastive representation for GANs
Recall the standard training of GANs, i.e., we train G : Z → X and D : X → [0, 1] via optimizing
(2) from a given dataset {xi}iN=1, and let T be a family of possible transforms for data augmentation,
which we assume in this paper to be those used in SimCLR (Chen et al., 2020a). In order to define
the training objective of ContraD, we start by re-defining the scalar-valued D to have vector-valued
outputs, i.e., D : X → Rde, to represent an encoder network of contrastive learning. Overall, the
encoder network D of ContraD is trained by minimizing two different contrastive losses: (a) the
SimCLR loss (4) on the real samples, and (b) the supervised contrastive loss (Khosla et al., 2020)
on the fake samples, which will be explained one-by-one in what follows.
Loss for real samples. By default, ContraD is built upon the SimCLR training: in fact, the training
becomes equivalent to SimCLR if the loss (b) is missing, i.e., without considering the fake sam-
ples. Here, we attempt to simply follow the contrastive training scheme for real samples x, to keep
open the possibility to improve the method by adopting other self-supervised learning methods with
data augmentations (Chen et al., 2020b; Grill et al., 2020). More concretely, we first compute two
independent views VrI), vf2) := t1 (x), t2(x) for real samples using t1, t2 〜T, and minimize:
Lc+on(D,hr) := LSimCLR(vr(1),vr(2);D,hr),	(6)
where hr : Rde → Rdp is a projection head for this loss.
Loss for fake samples. Although the representation learned via Lc+on (6) may be enough, e.g., for
discriminating two independent real samples, it does not give any guarantee that this representation
would still work for discriminating real and fake samples, which is needed as a GAN discrimina-
tor. We compensate this by considering an auxiliary loss Lc-on that regularizes the encoder to keep
necessary information to discriminate real and fake samples. Here, several design choices can be
possible: nevertheless, we observe that one should choose the loss deliberately, otherwise the con-
trastive representation from Lc+on could be significantly affected. For example, we found using the
original GAN loss (2) for Lc-on could completely negate the effectiveness of ContraD.
In this respect, we propose to use the supervised contrastive loss (Khosla et al., 2020) over fake sam-
ples to this end, an extended version of contrastive loss to support supervised learning by allowing
more than one view to be positive, so that views of the same label can be attracted to each other in
the embedding space. In our case, we assume all the views from fake samples have the same label
against those from real samples. Formally, for each vi(1), let Vi(+2) be a subset of v(2) that represent
the positive views for vi(1). Then, the supervised contrastive loss is defined by:
LSupCon(vi(1),v(2),Vi(+2))
log
vi(+2)∈Vi(+2)
exp(ssimCLR(v(1), v(+)))
Pj exp(ssimCLR(v(1), Vj)))
(7)
1
4
Published as a conference paper at ICLR 2021
Using the notation, we define the ContraD loss for fake samples as follows:
1 N	(1)	(2)
Lcon(D, hf ) := N〉: LSuPCon(Vf ,i, [vf,-i; Vr ; Vr ], [vf,-i]; D,hf ),	⑻
i=1
where again Vf := t3(G(z)) is a random view of fake samples, and V-i := V \ {Vi}. Remark that
we use an independent projection header hf : Rde → Rdp instead of hr in (6) for this loss.
There can be other variants for Lc-on, as long as (a) it leads D to discriminate real and fake samples,
while (b) not compromising the representation from Lc+on . For example, one can incorporate another
view of fake samples to additionally perform SimCLR similarly to Lc+on (6). Nevertheless, we found
the current design (8) is favorable among such variants considered, both in terms of performance
under practice of GANs, and computational efficiency from using only a single view.
To sum up, ContraD learns its contrastive representation by minimizing the following loss:
Lcon(D, hr, hf) := Lc+on(D, hr) + λconLc-on(D, hf),	(9)
where λcon > 0 is a hyperparameter. Nevertheless, we simply use λcon = 1 in our experiments.
3.2	Training GANs with ContraD
The contrastive loss defined in (9) is jointly trained under the standard framework of GAN, e.g., we
present in this paper the case of non-saturating loss (2), for training the generator network G. To
obtain a (scalar) discriminator score needed to define the GAN loss, we simply use an additional
discriminator head hd : Rde → R upon the contrastive representation of D. Here, a key difference
of ContraD training from other GANs is that ContraD only optimizes the parameters of hd for mini-
mizing the GAN loss: in practical situations of using stochastic gradient descent for the training, this
can be implemented by stopping gradient before feeding inputs to hd . Therefore, the discriminator
loss of ContraD is defined as follows:
Ldis(hd) := -Err [log(σ(hd(rr)))] - Erf [log(1 - σ(hd(rf)))],	(10)
where r1= Sg(D(Vr)) and rf = Sg(D(Vf)). Here, σ(∙) denotes the sigmoid function, Vr and Vf
are random views of real and fake samples augmented via T, respectively, and sg(∙) is the StoP-
gradient operation. Combined with the contrastive training loss (9), the joint training loss LD for a
single discriminator update is the following:
LD := Lcon + λdis Ldis = Lc+on + λcon Lc-on + λdisLdis,	(11)
where λdis > 0 is a hyperparameter. Again, as like λcon, we simply use λdis = 1 in our experiments.
Finally, the loss for the generator G can be defined simply like other standard GANs using the
discriminator score, except that we also augment the generated samples G(z) to Vf, in a similar way
to DiffAug (Zhao et al., 2020a) or ADA (Karras et al., 2020a): namely, in case of the non-saturating
loss (Goodfellow et al., 2014), we have:
LG := -Evf [log(σ(hd(D(Vf))))].	(12)
Note that the form (12) is equivalent to the original non-saturating loss (2) if We regard σ(hd(D(∙))):
X → [0, 1] as a single discriminator. Algorithm 1 in Appendix A describes a concrete training
procedure of GANs with ContraD using Adam optimizer (Kingma & Ba, 2014).
3.3	Self-conditional sampling with ContraD
Apart from the improved training of GANs, the learned contrastive representation of ContraD could
offer some additional benefits in practical scenarios compared to the standard GANs. In this section,
we present an example of how one could further utilize this additional information to derive a con-
ditional generative model from an unconditionally-trained GAN with ContraD. More specifically,
here we consider a variant of discriminator-driven latent sampling (DDLS) (Che et al., 2020) to
incorporate a given representation vector from the ContraD encoder as a conditional information.
Originally, DDLS attempts to improve the sample quality of a pre-trained GAN via a Langevin
sampling (Welling & Teh, 2011) on the latent space of the following form:
ε _
Zt+1 ：= Zt- 2VztE(Zt) + √ε∏t, nt 〜N(0,1),	(13)
5
Published as a conference paper at ICLR 2021
Table 1: Comparison of the best FID score and IS on unconditional image generation of CIFAR-10
and CIFAR-100. Values in the rows marked by * are from those reported in its reference.
Architecture	Method	Augment.	CIFAR-10		CIFAR-100	
			FID ；	IS ↑	FID ；	IS ↑
	-	-	26.6	7.38	28.5	7.25
G: SNDCGAN :	CR (Zhang et al., 2020)	HFlip, Trans	19.5	7.87	22.2	7.91
D: SNDCGAN	bCR (Zhao et al., 2020c)	HFlip, Trans	14.0	8.35	19.2	8.46
	DiffAug (Zhao et al., 2020a)	Trans, CutOut	22.9	7.64	27.0	7.47
	ContraD (ours)	SimCLR	10.9	8.78	15.2	9.09
	-	-	41.3	6.33	52.3	5.24
G: SNDCGAN :	CR (Zhang et al., 2020)	HFlip, Trans	32.1	7.08	36.5	6.55
D: SNResNet-18	bCR (Zhao et al., 2020c)	HFlip, Trans	22.8	7.29	28.2	7.30
	DiffAug (Zhao et al., 2020a)	Trans, CutOut	59.5	5.62	58.7	5.39
	ContraD (ours)	SimCLR	9.86	9.09	15.0	9.56
G: StyleGAN2 D: StyleGAN2	- DiffAug* (Zhao et al., 2020a)	- Trans, CutOut	11.1 9.89	9.18 9.40	16.5 15.2	9.51 10.0
	ContraD (ours)	SimCLR	9.80	9.47	14.1	10.0
Table 2: Comparison of the best FID score and IS on unconditional image generation of CelebA-
HQ-128 with SNDCGAN. We report the mean and standard deviation of best scores across 3 trials.
CelebA-HQ	W/O	Hinge	CR	bCR	ContraD (ours)
FID ；	24.8±1.69	26.4±1.32	20.9±0.37	19.5±0.08	17.1±1.38
IS ↑	2.06±0.06	2.05±0.06	2.11±0.09	2.21±0.10	2.44±0.06
where E(Z) := - logp(z) - d(G(z)) and d(∙) denotes the logit of D. In this manner, for our
ContraD, we propose to use the following “conditional” version of energy function E(z, v) for an
arbitrary vector v ∈ Rde in the ContraD representation space, called conditional DDLS (cDDLS):
E(z,v) ：= - logp(z) - hd(D(G(z))) - λ(v ∙ D(G(Z))).	(14)
In our experiments, for instance but not limited to, we show that using the learned weights obtained
from linear evaluation as conditional vectors successfully recovers class-conditional generations
from an unconditional ContraD model.
4	Experiments
We verify the effectiveness of our ContraD in three different aspects: (a) its performance on image
generation compared to other techniques for data augmentations in GANs, (b) the quality of repre-
sentations learned from ContraD in terms of linear evaluation, and (c) its ability of self-conditional
sampling via cDDLS. Overall, we constantly observe integrating contrastive learning into GANs and
vice versa have positive effects to each other: (a) for image generation, ContraD enables a simple
SNDCGAN to outperform a state-of-the-art architecture of StyleGAN2 on CIFAR-10/100; (b) in
case of linear evaluation, on the other hand, we observe the representation from ContraD could out-
perform those learned from SimCLR. Finally, we perform an ablation study to further understand
each of the components we propose. We provide the detailed specification on the experimental
setups, e.g., architectures, training configurations and hyperparameters in Appendix F.
We consider a variety of datasets including CIFAR-10/100 (Krizhevsky, 2009), CelebA-HQ-128
(Lee et al., 2020), AFHQ (Choi et al., 2020) and ImageNet (Russakovsky et al., 2015) in our ex-
periments, mainly with three well-known GAN architectures: SNDCGAN (Miyato et al., 2018),
StyleGAN2 (Karras et al., 2020b) and BigGAN (Brock et al., 2019).1 We measure Frechet Incep-
tion distance (FID) (Heusel et al., 2017) and Inception score (IS) (Salimans et al., 2016) as quanti-
tative metrics to evaluate generation quality.2 We compute FIDs between the test set and generated
samples of the same size. We follow the best hyperparameter practices explored by Kurach et al.
(2019) for SNDCGAN models on CIFAR and CelebA-HQ-128 datasets. For StyleGAN2, on the
1Results on AFHQ and ImageNet can be found in Appendix B and C, respectively.
2We use the official InceptionV3 model in TensorFlow to compute both FID and IS in our experiments.
6
Published as a conference paper at ICLR 2021
Table 3: Comparison of classification accuracy under linear evaluation protocol on CIFAR-10 and
CIFAR-100. We report the mean and standard deviation across 3 runs of the evaluation.
Dataset	Training	SNDCGAN	SNResNet-18	StyleGAN2
CIFAR10	SimCLR (λcon = λdis = 0)	72.9±o.02	80.3±0.05	86.2±0.06
-	ContraD (ours)	77.5±0.20	85.7±0.10	88.6±0.06
CIFAR100 CIFAR-100	SimCLR (λcon = λdis = 0)	30.8±o.ii	41.2±0.06	61.1±0.06
	ContraD (ours)	37.4±0.06	51.1±0.18	68.1±0.07
other hand, we follow the training details of Zhao et al. (2020a) in their CIFAR experiments. All the
training details are shared across methods per experiment, but the choice of batch size for training
ContraD: we generally observe that ContraD greatly benefits from using larger batch (Appendix E),
similarly to SimCLR (Chen et al., 2020a), but in contrast to the common practice of GAN training.
Therefore, in our experiments, we consider to use 2× to 8× larger batch sizes for training ContraD,
where the details are specified in Appendix F.
4.1	Results
Image generation on CIFAR-10/100. CIFAR-10 and CIFAR-100 (Krizhevsky, 2009) consist of
60K images of size 32 × 32 in 10 and 100 classes, respectively, 50K for training and 10K for testing.
We start by comparing ContraD on CIFAR-10/100 with three recent methods that applies data aug-
mentation in GANs: (a) Consistency Regularization (CR) (Zhang et al., 2020), (b) the “balanced”
Consistency Regularization (bCR) (Zhao et al., 2020c), and (c) Differentiable Augmentation (Dif-
fAug) (Zhao et al., 2020a) as baselines.3 We follow their best pratices on which data augmentations
to use: specifically, we use a combination of horizontal flipping (HFlip) and random translation up
to 4 pixels (Trans) for CR and bCR, and Trans + CutOut (DeVries & Taylor, 2017) for DiffAug
for CIFAR datasets. We mainly consider SNDCGAN and StyleGAN2 for training, but in addition,
we further consider a scenario that the discriminator architecture of SNDCGAN is scaled up into
SNResNet-18, which is identical to ResNet-18 (He et al., 2016) without batch normalization (Ioffe &
Szegedy, 2015) but with spectral normalization (Miyato et al., 2018), in order to see that how each
training method behaves when the capacity of generator and discriminator are significantly different.
Table 1 shows the results: overall, we observe our ContraD uniformly improves the baseline GAN
training by a large margin, significantly outperforming other baselines as well. Indeed, our results on
SNDCGAN already achieve better FID than those from the baseline training of StyleGAN2. More-
over, when the discriminator of SNDCGAN is increased to SNResNet-18, ContraD could further
improve its results, without even modifying any of the hyperparameters. It is particularly remark-
able that all the other baselines could not improve their results in this setup: A further ablation study
presented in Section 4.2 reveals that stopping gradients before the discriminator head in our design
is an important factor to benefit from SNResNet-18. Finally, our ContraD can also be applied to a
larger, sophisticated StyleGAN2, further improving both FID and IS compared to DiffAug.
Image generation on CelebA-HQ-128. We also perform experiments on CelebA-HQ-128 (Lee
et al., 2020), which consist of 30K human faces of size 128 × 128, to see that our design of ContraD
still applies well for high-resolution datasets. We split 3K images out of CelebA-HQ-128 for testing,
and use the remaining for training. We use SNDCGAN for this experiment, and compare FID and
IS to (a) the default non-saturating loss (W/O) (Goodfellow et al., 2014), (b) the hinge loss (Hinge)
(Lim & Ye, 2017; Tran et al., 2017), (c) CR and (d) bCR. Similarly to the CIFAR experiments,
we use the “HFlip+Trans” augmentation for both CR and bCR, but for CelebA-HQ-128 we allow
them to translate up to 16 pixels, following (Zhang et al., 2020). Again, as shown in Table 2, we
confirm that ContraD could still improve training of GANs compared to other methods, even with
this high-resolution, yet with limited samples and a straightforward choice of architecture.
Linear evaluation. Recall that the training objective of ContraD we propose in (11) can be simply
reduced into the SimCLR loss (Chen et al., 2020a) without considering the fake samples, i.e., by
letting λcon = λdis = 0. A natural question from here is whether those fake samples could help to
3Here, we notice that ADA (Karras et al., 2020a), a concurrent work to DiffAug, also offers a highly similar
method to DiffAug, but with an additional heuristic of tuning augmentations on-the-fly with a validation dataset.
We consider this addition is orthogonal to ours, and focus on comparing DiffAug for a clearer comparison in
the CIFAR experiments. A more direct comparison with ADA, nevertheless, can be found in Appendix B.
7
Published as a conference paper at ICLR 2021
Table 4: Comparison of FID (lower is better) from the class-wise subsets of CIFAR-10 test set (1K
images per class). “Random” indicates unconditional generation, and “Train-1K” indicates a 1K
random subsamples from the CIFAR-10 train set of the same class.
D Arch. Sampling ∣ Plane Car Bird Cat Deer Dog Frog Horse ShiP Truck ∣ Mean
SNDCGAN	Random	110.6	125.9	103.8	104.4	107.0	121.0	134.1	120.1	128.8	130.0	118.6
	+ cDDLS	65.3	72.1	63.9	69.4	57.5	69.0	107.0	61.5	58.2	53.6	67.8
SNResNet-18	Random	117.6	136.3	100.5	99.6	98.6	115.0	128.2	111.7	140.9	140.3	118.9
	+ cDDLS	67.1	65.8	59.8	63.8	59.7	61.9	68.8	59.9	58.9	51.6	61.7
Train-IK ∣ 38.9	28.4	41.4	49.7	36.4	40.2	41.7	33.6	30.7	24.6 ∣ 36.6
Table 5: Comparison of the best
FID and IS on CIFAR-10 with SND-
CGAN when stronger (bCR and
DiffAug) or weaker augmentations
(ContraD) are used for each method.
Method	Augment.	FID J	IS ↑
bCR	HFlip, Trans	14.0	8.35
+ Aug.	SimCLR	20.6	7.44
DiffAug	Trans, CutOut	22.9	7.64
+ Aug.	SimCLR	21.9	7.42
ContraD	SimCLR	10.9	8.78
-Aug.	HFlip, Trans	13.7	8.54
Table 6: Comparison of the best FID score and IS on
CIFAR-10 for ablations of our proposed components. All
the models are trained with batch size 256. For the ablation
of “MLP hd”, We replace hd with a linear model. "sg(∙)”
indicates the use of stop-gradient operation.
D: SNDCGAN D: SNResNet-18
MLP hd	sg(∙	Lcon	Lcon	FID J	IS ↑	FID J	IS ↑
✓	✓	✓	✓	11.1	8.62	10.6	8.99
X	✓	✓	✓	185	3.43	274	2.09
✓	X	✓	✓	11.6	8.61	28.0	7.52
✓	✓	X	✓	11.9	8.45	182	2.02
✓	✓	✓	X	210	1.93	232	2.56
improve the representation of SimCLR: we verify this by comparing the linear evaluation perfor-
mance (Kolesnikov et al., 2019; Chen et al., 2020a; Grill et al., 2020) of the learned representation
of ContraD to those from SimCLR: specifically, linear evaluation measures the test accuracy that
a linear classifier on the top of given (frozen) representation could maximally achieve for a given
(downstream) labeled data. Following this protocol, we compare the linear evaluation of a Con-
traD model with its ablation that λcon and λdis are set to 0 during its training. We consider three
discriminator architectures and two datasets for this experiments: namely, we train SNDCGAN,
SNResNet-18, and StyleGAN2 models on CIFAR-10 and CIFAR-100. In this experiment, we use
batch size 256 for training all the SNDCGAN and SNResNet-18 models, while in case of Style-
GAN2 we use batch size 64 instead. Table 3 summarizes the results, and somewhat interestingly,
it shows that ContraD significantly improves the SimCLR counterpart in terms of linear evaluation
for all the models tested: in other words, we observe that keeping discriminative features between
real and fake samples in their representation, which is what ContraD does in addition to SimCLR,
can be beneficial to improve the contrastive learning itself. Again, our observation supports a great
potential of ContraD not only in the context of GANs, but also in the contrastive learning.
Conditional DDLS with ContraD. We further evaluate the performance of conditional DDLS (cD-
DLS) (14) proposed for ContraD, with a task of deriving class-conditional generative models for a
given (unconditionally-trained) GAN model. Here, we measure the performance of this conditional
generation by computing the class-wise FIDs to the test set, i.e., FIDs computed for each subsets of
the same class in the test set. We test cDDLS on SNDCGAN and SNResNet-18 ContraD models
trained on CIFAR-10. To perform a conditional generation for a specific class, we simply take a
learned linear weight obtained from the linear evaluation protocol for that class. The results summa-
rized in Table 4 show that applying cDDLS upon unconditional generation significantly improves
class-wise FIDs for all the classes in CIFAR-10. Some of the actual samples conditionally generated
from an SNResNet-18 ContraD model can be found in Figure 5 of Appendix D.
4.2	Ablation study
We conduct an ablation study for a more detailed analysis on the proposed method. For this section,
unless otherwise specified, we perform experiments on CIFAR-10 with SNDCGAN architecture.
Stronger augmentations for other methods. One of the key characteristics of ContraD compared
to the prior works is the use of stronger data augmentations for training GANs, e.g., we use the
augmentation pipeline of SimCLR (Chen et al., 2020a) in our experiments. In Table 5, we examine
the performance bCR (Zhao et al., 2020c) and DiffAug (Zhao et al., 2020a) when this stronger
8
Published as a conference paper at ICLR 2021
augmentation is applied, and shows that the SimCLR augmentation could not meaningfully improve
FID or IS for both methods. Unlike these methods, our ContraD effectively handles the SimCLR
augmentations without overly regularizing the discriminator, possibly leveraging many components
of contrastive learning: e.g., the normalized loss, or the use of separate projection heads.
Weaker augmentations for ContraD. On the other side, we also consider a case when ContraD is
rather trained with a weaker data augmentation, here we consider “HFlip, Trans” instead of “Sim-
CLR”, in Table 5: even in this case, we observe ContraD is still as good as (or better in terms of
IS) a strong baseline of bCR. The degradation in FID (and IS) compared to “ContraD + SimCLR”
is possibly due to that SimCLR requires strong augmentations to learn a good representation (Chen
et al., 2020a), e.g., we also observe there is a degradation in the linear evaluation performances,
77.5 → 72.9, when “HFlip, Trans” is used.
MLP discriminator head. In our experiments, we use a 2-layer network for the discriminator head
hd . As shown in Table 6, this choice can be minimal: replacing hd with a linear model severely
breaks the training. Conversely, although not presented here, we have also observed that MLPs of
more than two layers neither give significant gains on the performance in our experimental setup.
Stopping gradients. We use the stop-gradient operation sg(∙) before the discriminator head hd in
attempt to decouple the effect of GAN loss to the ContraD representation. Table 6 also reports the
ablation of this component: overall, not using sg(∙) does not completely break the training, but it
does degrade the performance especially when a deeper discriminator is used, e.g., SNResNet-18.
Contrastive losses. Recall that the representation of ContraD is trained by two losses, namely Lc+on
and Lc-on (Section 3.1). From an ablation study on each loss, as presented in Table 6, we observe that
both of the losses are indispensable for our training to work in a larger discriminator architecture.
Separate projection headers. As mentioned in Section 3.1, we use two independent projection
headers, namely hr and hf , to define Lc+on and Lc-on, respectively. We have observed this choice is
empirically more stable than sharing them: e.g., under hr = hf, we could not obtain a reasonable
performance when only D is increased to SNResNet-18 on CIFAR-10 with SNDCGAN. Intuitively,
an optimal embedding (after projection) from Lc-on can be harmful to SimCLR, as it encourages the
embedding of real samples to degenerate into a single point, i.e., the opposite direction of SimCLR.
5	Conclusion and future work
In this paper, we explore a novel combination of GAN and contrastive learning, two important,
yet seemingly different paradigms in unsupervised representation learning. They both have been
independently observed the crucial importance of maintaining consistency to data augmentations
in their representations (Zhang et al., 2020; Tian et al., 2020), and here we further observe that
these two representations complement each other when combined upon the shared principle of view-
invariant representations: although we have put more efforts in this work to verify the effectiveness
of contrastive learning on GANs, we do observe the opposite direction through our experiments, and
scaling up our method to compete with state-of-the-art self-supervised learning benchmarks (Chen
et al., 2020b; Grill et al., 2020; Caron et al., 2020) would be an important future work.
We believe our work also suggests several interesting ideas to explore in future research in both
sides of GAN and contrastive learning: for example, our new design of introducing a small header
to minimize the GAN loss upon other (e.g., contrastive) representation is a promising yet unexplored
way of designing a new GAN architecture. For the SimCLR side, on the other hand, we suggest a
new idea of incorporating “fake” samples for contrastive learning, which is also an interesting direc-
tion along with other recent attempts to improve the efficiency of negative sampling in contrastive
learning, e.g., via hard negative mining (Kalantidis et al., 2020; Robinson et al., 2021).
Acknowledgments
This work was supported by Samsung Advanced Institute of Technology (SAIT). This work was also
partly supported by Institute of Information & Communications Technology Planning & Evaluation
(IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence
Graduate School Program (KAIST)). The authors would like to thank Minkyu Kim for helping addi-
tional experiments in preparation of the camera-ready revision, and thank the anonymous reviewers
for their valuable comments to improve our paper.
9
Published as a conference paper at ICLR 2021
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://
proceedings.mlr.press/v70/arjovsky17a.html.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In Advances in Neural Information Processing Systems 32, pp.
15535-15545. Curran Associates, Inc., 2019.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - Mining discriminative com-
ponents with random forests. In Proceedings of the European Conference on Computer Vision,
pp. 446-461. Springer, 2014.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural
Information Processing Systems, 33, 2020.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and
Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator
driven latent sampling. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 12275-12287. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
90525e70b7842930586545c6f1c9310c-Paper.pdf.
Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised GANs via
auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 12154-12163, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. StarGAN v2: Diverse image synthesis
for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, June 2020.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3606-3613, 2014.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with Cutout. arXiv preprint arXiv:1708.04552, 2017.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings ofthe IEEE International Conference on Computer Vision, pp.
1422-1430, 2015.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=S1v4N2l0-.
10
Published as a conference paper at ICLR 2021
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.
nips.cc/paper/5423-generative-adversarial-nets.pdf.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -
a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21271-21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems 30, pp. 5767-5777. Curran Associates, Inc., 2017. URL http://papers.nips.cc/
paper/7159- improved- training- of- wasserstein- gans.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, June 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and
Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In
Hal Daume In and Aarti Singh (eds.), Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4182-4192.
PMLR, 13-18 Jul 2020. URL http://proceedings.mlr.press/v119/henaff20a.
html.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems 30, pp. 6626-6637. Curran Associates, Inc., 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. InD. Lee, M. Sugiyama,
U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 29, pp. 4565-4573. Curran Associates, Inc., 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from stan-
dard GAN. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=S1erHoR5t7.
Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard
negative mixing for contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21798-21809. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf.
Minguk Kang and Jaesik Park. ContraGAN: Contrastive learning for conditional image gen-
eration. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21357-21369. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f490c742cd8318b8ee6dca10af2a163f- Paper.pdf.
11
Published as a conference paper at ICLR 2021
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, June 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12104-12114. Curran Associates, Inc., 2020a. URL https://Proceedings.neurips.
cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,
Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In Ad-
vances in Neural Information Processing Systems, volume 33, pp. 18661-18673. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
d89a66c7c80a29b1bdbab0f2a1a94af8- Paper.pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2014.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recog-
nition, pp. 1920-1929, 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Depart-
ment of Computer Science, University of Toronto, 2009.
Karol Kurach, Mario LuCiC, XiaohUa Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study
on regularization and normalization in GANs. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 3581-3590, Long Beach, California, USA, 09-15
Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/kurach19a.html.
Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGAN: Towards diverse and interactive
facial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, June 2020.
Kwot Sin Lee, Ngoc-Trung Tran, and Ngai-Man Cheung. InfoMax-GAN: Improved adversarial
image generation via information maximization and contrastive learning. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3942-3952, January 2021.
Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv preprint arXiv:1705.02894, 2017.
R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs
created equal? a large-scale study. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems
31, pp. 700-709. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7350-are-gans-created-equal-a-large-scale-study.pdf.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 3481-3490, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/mescheder18a.html.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ.
12
Published as a conference paper at ICLR 2021
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In The Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
pp. 722-729. IEEE, 2008.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8026-8037, 2019.
Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning
with hard negative samples. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=CR1XOQ0UTh-.
Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. SinGAN: Learning a generative model from
a single natural image. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, October 2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision,
115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training GANs. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 2234-2242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6125-improved-techniques-for-training-gans.pdf.
Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A U-Net based discriminator for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 8207-8216, 2020.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
6827-6839. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf.
Dustin Tran, Rajesh Ranganath, and David Blei. Hierarchical implicit models and likelihood-free
variational inference. In Advances in Neural Information Processing Systems, volume 30. Cur-
ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/
file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf.
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, and Ngai-Man Cheung. An improved self-
supervised GAN via adversarial training, 2019.
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Che-
ung. On data augmentation for GAN training. IEEE Transactions on Image Processing, 30:
1882-1897, 2021. doi: 10.1109/TIP.2021.3049346.
Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing Gong. Improving the improved training of
Wasserstein GANs. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=SJx9GQb0-.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning, pp. 681-688, 2011.
13
Published as a conference paper at ICLR 2021
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3733-3742, 2018.
Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. SUN database:
Large-scale scene recognition from abbey to zoo. In IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 3485-3492. IEEE, 2010.
Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for
generative adversarial networks. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1lxKlSKPH.
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for
data-efficient GAN training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 7559-7570. Cur-
ran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/
file/55479c55ebd1efd3ff125f1337100388-Paper.pdf.
Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization im-
proves GAN training. In Proceedings of the 37th International Conference on Machine Learning,
Proceedings of Machine Learning Research. PMLR, 2020b.
Zhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, and Han Zhang. Im-
proved consistency regularization for GANs, 2020c.
Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for
GAN training. arXiv preprint arXiv:2006.02595, 2020d.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
on Computer Vision, Oct 2017.
14
Published as a conference paper at ICLR 2021
A Training procedure of ContraD
Algorithm 1 GANs with Contrastive Discriminator (ContraD)
Require: generator G, discriminator D, Adam hyperparameters α, β1,β2, number of D updates
per G update ND, family of data augmentations T, λ > 0.
1:	for # training iterations do
2:	for t = 1 to ND do
3:	Sample X 〜Pdata(x) and Z 〜p(z)
4:	Sample t1,t2, t3 〜T
5:	vr1), v[2), Vf - tι(x), t2(x), t3(G(z))
6:	L+on J LSimCLR(VrD, vC ; D,hr)
7:	Lcon J N Pi=1 LSupCon(vf,i, [vf,-i; Vr ,; Vr )],[vf,-i]; D,hf)
8:	rr, rf J sg(D(Vr(2))), sg(D(Vf))
9:	Ldis J -Err [log(σ(hd(rr)))] - Erf [log(1 - σ(hd(rf)))]
10:	LD J Lc+on + λconLc-on + λdisLdis
11:	D, hr, hf, hd J Adam([D, hr, hf, hd], LD; α, β1, β2)
12:	end for
13:	LG J -Evf [log(σ(hd(D(Vf))))]
14:	G J Adam(G, LG; α, β1 , β2 )
15:	end for
B	Results on AFHQ datasets
We evaluate our training method on the Animal Faces-HQ (AFHQ) dataset (Choi et al., 2020), which
consists of 〜15,000 samples of animal-face images at 512×512 resolution, to further verify the ef-
fectiveness of ContraD on higher-resolution, yet limited-sized datasets. We partition the dataset by
class labels into three sub-datasets, namely AFHQ-Dog (4,739 samples), AFHQ-Cat (5,153 sam-
ples) and AFHQ-Wild (4,738 samples), and evaluate FIDs on these datasets. In this experiment,
we compare our method with ADA (Karras et al., 2020a), another recent data augmentation scheme
for GANs (concurrently to DiffAug (Zhao et al., 2020a) considered in Section 4), which includes a
dynamic adaptation of augmentation pipeline during training. We follow the training details of ADA
particularly for this experiment: specifically, we train StyleGAN2 with batch size 64 for 25M train-
ing samples, using Adam with (α, β1, β2) = (0.0025, 0.0, 0.99), R1 regularization with γ = 0.5,
and exponential moving average on the generator weights with half-life of 20K samples. For Con-
traD training on AFHQ datasets, we consider to additionally use CutOut (DeVries & Taylor, 2017)
of p = 0.5 upon the SimCLR augmentation, which we observe an additional gain in FID. We also
follow the way of computing FIDs as per Karras et al. (2020a) here for a fair comparison, i.e., by
comparing 50K of generated samples and all the training samples (〜5K per dataset).
Table 7 compares our ContraD models with the results reported by Karras et al. (2020b) where
the models are available at https://github.com/NVlabs/stylegan2-ada. Overall, we
consistently observe that ContraD improves the baseline StyleGAN2, even achieving significantly
better FIDs than ADA on AFHQ-Dog and AFHQ-Wild. These results confirms the effectiveness
of ContraD on higher-resolution datasets, especially under the regime of limited data. Qualitative
comparisons can be found in Figure 6, 7, and 8 of Appendix D.
Table 7: Comparison of the best FID score (lower is better) on unconditional image generation of
AFHQ datasets (Choi et al., 2020) with StyleGAN2. Values in the rows marked by * are from those
reported in its reference. We set our results bold-faced whenever the value improves the StyleGAN2
baseline (“Baseline”). Underlined indicates the best score among tested for each dataset.
Architecture	Method	DOG	Cat	Wild
	Baseline* (Karras et al., 2020b)	19.4	5.13	3.48
StyleGAN2	ADA* (Karras et al., 2020a)	7.40	3.55	3.05
	ContraD (ours)	7.16	3.82	2.54
15
Published as a conference paper at ICLR 2021
C Results on ImageNet dataset
Table 8: Comparison of the best FID score and IS on conditional generation of ImageNet (64×64)
with BigGAN architecture. Values in the rows marked by * are from those reported in its reference.
ImageNet	FID ；	IS ↑
BigGAN* (Brock et al., 2019)	10.6	25.43±0.15
FQ-BigGAN* (Zhao et al., 2020b)	9.67	25.96±0.24
ContraD-BigGAN (ours)	8.32	26.94±0.40
Table 9: Comparison linear evaluation and transfer learning performance across 6 natural image
classification datasets for BigGAN discriminators pretrained on ImageNet (64 × 64). We report the
top-1 accuracy except for ImageNet and SUN397, which we instead report the top-5 accuracy.
Training (BigGAN)	ImageNet	CIFAR10	CIFAR100	DTD	SUN397	Flowers	Food
Supervised (ImageNet)	63.5	76.1	55.2	45.4	31.7	78.1	44.5
SimCLR (λcon = λdis = 0)	43.4	81.2	55.3	43.9	37.6	69.8	38.8
ContraD (ours)	51.5	84.5	61.1	50.6	44.4	78.6	44.5
We also apply ContraD to the state-of-the-art BigGAN (Brock et al., 2019) architecture on ImageNet
(Russakovsky et al., 2015), to examine the effectiveness of our method on this large-scale, class-
conditional GAN model. For this experiment, we exactly follow the training setups done in FQ-
BigGAN (Zhao et al., 2020b), a recent work showing that feature quantization in discriminator
could improve state-of-the-art GANs, which is based on the official PyTorch (Paszke et al., 2019)
implementation of BigGAN.4 Following Zhao et al. (2020b), we down-scale the ImageNet dataset
into 64 × 64, and reduce the default channel width of 96 into 64. We train the model with batch size
512 for 100 epochs (〜250K generator steps). Unlike other experiments, We compute FIDs between
the training set and 50K of generated samples for a fair comparison to the baseline scores reported
by Zhao et al. (2020b). In Table 8, we show that ContraD further improves the baseline BigGAN
training and FQ-BigGAN (Zhao et al., 2020b) both in FID and IS under the same training setups.
Next, we evaluate the representation of the learned BigGAN encoder D from ContraD5 under lin-
ear evaluation (on ImageNet) and several transfer learning benchmarks of natural image classifica-
tion: namely, we consider CIFAR-10 and CIFAR-100 (Krizhevsky, 2009), the Describable Textures
Dataset (DTD) (Cimpoi et al., 2014), SUN397 (Xiao et al., 2010), Oxford 102 Flowers (Flowers)
(Nilsback & Zisserman, 2008), and Food-101 (Bossard et al., 2014). As done in Table 3, we com-
pare ContraD with an ablation when λcon = λdis = 0, which is equivalent to the SimCLR (Chen
et al., 2020a) training. In addition, we also compare against the supervised baseline, where the rep-
resentation is pre-trained on ImageNet with standard cross-entropy loss using the same architecture
of BigGAN discriminator. Similarly to the ContraD training, this baseline is also trained with the
SimCLR augmentation, using Adam optimizer (Kingma & Ba, 2014) for 120 epochs.
Table 9 summarizes the results. We first note that the supervised training achieves 63.5% top-5
accuracy on ImageNet, while the SimCLR baseline achieves 43.4% (in linear evaluation). Again,
as also observed in Table 3, ContraD significantly improves the linear evaluation performance from
the baseline SimCLR, namely to 51.5%. Although the results are somewhat far behind compared to
those reported in the original SimCLR (Chen et al., 2020a), this is possibly due to the use of model
with much limited capacity, i.e., the BigGAN discriminator: its essential depth is only 〜10, and it
also uses spectral normalization (Miyato et al., 2018) for every layer unlike the ResNet-50 architec-
ture considered by Chen et al. (2020a). Nevertheless, we still observe a similar trend in the accuracy
gap between supervised and (unsupervised) contrastive learning. On the other hand, the remaining
results in Table 9, those from the transfer learning benchmarks, confirm a clear advantage of Con-
traD, compared to both the SimCLR and supervised baselines: we found the two baselines perform
similarly on transfer learning, while our ContraD training further improves SimCLR consistently.
4https://github.com/ajbrock/BigGAN-PyTorch
5We notice here that, although BigGAN is a class-conditional architecture, i.e., it uses the label information
in training, the encoder part D in ContraD is still trained in an unsupervised manner, as we put the labels only
after stopping gradients, i.e., in the discriminator header hdis.
16
Published as a conference paper at ICLR 2021
D Qualitative results
bCR (FID: 14.0)
ContraD (FID: 10.9)
Real Images
CR (FID: 19.5)
丸 5龙∙t ZID F 一
U 就EirBib
B2白老3.1
 
m趣 UB≡0R5
E⅛ >1^---
国盲■呈
幽陶典4ħ≡o√,
W/O (FID: 26.6)
DiffAug (FID: 22.9)
工XU ,」〃
■aE冰 空■■幽

i
■■■■案
: <
W-薄 X⅜F"≡"一 ■
-气O整器O阖苴
■□4£。可早至
Figure 2: Qualitative comparison of unconditionally generated samples from GANs with different
training methods. All the models are trained on CIFAR-10 with an SNDCGAN architecture.
ContraD (FID: 14.8)
Real Images
9邪■短H
Γ kr ⅛ SJ型■痣EI
W&岸”■品1
卷■超量靠”
y. .
bCR (FID: 19.2)
Figure 3: Qualitative comparison of unconditionally generated samples from GANs with different
training methods. All the models are trained on CIFAR-100 with an SNDCGAN architecture.
CR (FID: 22.2)
DiffAug (FID: 27.0)
W/O (FID: 28.5)
17
Published as a conference paper at ICLR 2021
W/0 (FID: 24.8)
CR (FID: 20.9)
Figure 4: Qualitative comparison of unconditionally generated samples from GANs with different
training methods. All the models are trained on CelebA-HQ-128 with an SNDCGAN architecture.
E9史必以&EIIW
工≡Mκm∕a也
E2EIEE∙fll① R 产
ΠM%元①厘H*
Dog
Figure 5: Visualization of conditionally generated samples via conditional DDLS (Section 3.3) with
ContraD. We use an SNDCGAN generator trained with an SNResNet-18 ContraD for the generation.
Cat
Horse
Truck
18
Published as a conference paper at ICLR 2021
Figure 6: Qualitative comparison of unconditionally generated samples from GANs with different
training methods, along with real images from the training set. All the models are trained on AFHQ-
Dog (4,739 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) with
ψ = 0.7 to produce the images at the bottom row. We use the pre-trained models officially released
by the authors to obtain the results of the baseline StyleGAN2 and ADA.
19
Published as a conference paper at ICLR 2021
Figure 7: Qualitative comparison of unconditionally generated samples from GANs with different
training methods, along with real images from the training set. All the models are trained on AFHQ-
Cat (5,153 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) with ψ =
0.7 to produce the images at the bottom row. We use the pre-trained models officially released by
the authors to obtain the results of the baseline StyleGAN2 and ADA.
StyleGAN2 / Untruncated (FID: 5.13)
Real Images
ADA / Untruncated (FID: 3.55)
ContraD / Truncated (ψ = 0.7)
ADA / Truncated (ip = 0.7)
ContraD / Untruncated (FID: 3.82)
20
Published as a conference paper at ICLR 2021
Real Images	StyleGAN2 / Untruncated (FID: 3.48)
Figure 8: Qualitative comparison of unconditionally generated samples from GANs with different
training methods, along with real images from the training set. All the models are trained on AFHQ-
Wild (4,738 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) with
ψ = 0.7 to produce the images at the bottom row. We use the pre-trained models officially released
by the authors to obtain the results of the baseline StyleGAN2 and ADA.
21
Published as a conference paper at ICLR 2021
E Effect of using larger batch sizes
Figure 9: Comparion of the FID distribution of the top 25% of trained models (Kurach et al., 2019)
on CIFAR-10 (SNDCGAN) for different batch sizes.
Contrary to existing practices for training GANs, we observe in our experiments that ContraD typ-
ically needs a larger batch size in training to perform best. We further confirm this effect of batch
sizes in Figure 9: overall, we observe the stanard training (“W/O”) is not significantly affected by
a particular batch size, while ContraD offers much better FIDs on larger batch sizes. We also ob-
serve bCR (Zhao et al., 2020c) slightly benefits from larger batches, but not as much as ContraD
does. This observation is consistent with Chen et al. (2020a) that contrastive learning benefits from
using larger batch sizes, and it also supports our key hypothesis on the close relationship between
contrastive and discriminator representations.
F Experimental details
F.1 Architecture
Overall, we consider four architectures in our experiments: SNDCGAN (Miyato et al., 2018),
StyleGAN-2 (Karras et al., 2020b), and BigGAN (Brock et al., 2019) for GAN architectures, and
additionally SNResNet-18 for a discriminator architecture only. For all the architectures considered,
we have modified the last linear layer into a multi-layer perceptron (MLP) having the same input
and output size, for a fair comparison to ContraD that requires such an MLP for the discriminator
header hd . When a GAN model is trained via ContraD, we additionally introduce two 2-layer MLP
projection heads of the output size 128 upon the penutimate representation in the discriminator to
stand hr and hf. All the models are implemented in PyTorch (Paszke et al., 2019) framework.
SNDCGAN. We adopt a modified version of SNDCGAN architecture by Kurach et al. (2019) fol-
lowing the experimental setups of other baselines (Zhang et al., 2020; Zhao et al., 2020c). The
official implementation can be found in Compare_gan codebase,6 and We have re-implemented
this TensorFlow implementation into PyTorch framework. The detailed structures of the generator
and discriminator of SNDCGAN are summarized in Table 10 and 11, respectively.
SNResNet-18. We modify the ResNet-18 (He et al., 2016) architecuture, Where a PyTorch imple-
mentation is available from torchvision7 library, to not include batch normalization (Ioffe &
Szegedy, 2015) layers inside the netWork. Instead, We apply spectral normalization (Miyato et al.,
2018) for all the convolutional and linear layers. We made such a modification to adapt this model
as a stable GAN discriminator, as batch normalization layers often harm the GAN dynamics in
discriminator in practice.
StyleGAN2. We folloW the StyleGAN2 architecture used in the DiffAug baseline (Zhao et al.,
2020a). Specifically, We generally folloW the official StyleGAN2, but the number of channels at
32×32 resolution is reduced to 128 and doubled at each coarser level With a maximum of 512
channels, in order to optimize the model for CIFAR datasets, as suggested by Zhao et al. (2020a).
6https://www.github.com/google/compare_gan
7https://github.com/pytorch/vision/blob/master/torchvision
22
Published as a conference paper at ICLR 2021
Table 10: SNDCGAN generator__________Table 11: SNDCGANdiscriminator
Layer	Kernel	Output			Layer	Kernel	Output		
Latent z	-			128	Conv, LeakyReLU	[3, 3,1]	h	×w ×	64
Linear, BN, ReLU	-	h/8	× w/8 ×	512	Conv, LeakyReLU	[4, 4, 2]	h/2	× w/2 ×	128
DeConv, BN, ReLU	[4, 4, 2]	h/4	× w/4 ×	256	Conv, LeakyReLU	[3, 3, 1]	h/2	× w/2 ×	128
DeConv, BN, ReLU	[4, 4, 2]	h/2	× w/2 ×	128	Conv, LeakyReLU	[4, 4, 2]	h/4	× w/4 ×	256
DeConv, BN, ReLU	[4, 4, 2]	h	×w ×	64	Conv, LeakyReLU	[3, 3, 1]	h/4	× w/4 ×	256
DeConv, Tanh	[3, 3,1]	h	×w ×	3	Conv, LeakyReLU Conv, LeakyReLU	[4, 4, 2] [3, 3,1]	h/8 h/8	× w/8 × × w/8 ×	512 512
									
BigGAN. We use the official PyToch implementation of BigGAN (https://github.com/
ajbrock/BigGAN-PyTorch). In order to adapt ContraD into the class-conditional BigGAN
architecture, we apply the stop-gradient operation not only on the penultimate representation before
computing hd , but also before applying the class-conditional projection (Miyato & Koyama, 2018).
F.2 Training details
SNDCGAN. Overall, we follow the best hyperparameter practices explored by Kurach et al. (2019)
for SNDCGAN models on CIFAR and CelebA-HQ-128 datasets: We use Adam (Kingma & Ba,
2014) with (α, β1, β2) = (0.0002, 0.5, 0.999) for optimization with batch size of 64, while in case
of ContraD on CIFAR datasets the batch size of 512 is used instead. For CelebA-HQ-128, we keep
the default batch size of 64 even for ContraD. All models but the “Hinge” baselines in Table 2 are
trained with the non-saturating loss (Goodfellow et al., 2014). We stop training after 200K generator
updates for CIFAR-10/100 and 100K for CelebA-HQ-128. When CR or bCR is used, we set their
regularization strengths λ = 10 for both real and fake images.
StyleGAN2. We follow the training details of DiffAug (Zhao et al., 2020a) in their CIFAR ex-
periments: we use Adam with (α, β1 , β2) = (0.002, 0.0, 0.99) for optimization with batch size of
32, but 64 for ContraD models. We use non-saturating loss for training, and use R1 regularization
(Mescheder et al., 2018) with γ = 0.1. We do not use, however, the path length regularization and
the lazy regularization (Karras et al., 2020b) in training. We take exponential moving average on the
generator weights with half-life of 106 . We stop training after 800K generator updates. When CR
or bCR is used, we set their regularization strengths λ = 10 for both real and fake images.
Linear evaluation and transfer learning. For a single linear evaluation (and transfer learning) run,
we train a linear classifier upon a given (frozen) representation. The results in Table 3 are trained via
stochastic gradient descent for 100 epochs: the initial learning rate of 0.1, and it is decayed by 0.1 at
60, 75, and 90-th epoch. In case of Table 9, on the other hand, we use Adam optimizer (Kingma &
Ba, 2014) for 90 epochs: the initial learning rate if 0.0002, and it is decayed by 0.3 at 60 and 75-th
epoch. We augment every training dataset via random crop, resizing and horizontal flipping.
Conditional DDLS. We run Langevin sampling of step size ε = 0.1 for 1,000 iterations. We
follow all the practical considerations proposed by Che et al. (2020) to improve the sample quality
of DDLS: specifically, (a) we separately set the standard deviation of the Gaussian noise (13) to 0.1,
and (b) to alleviate mode dropping issues, instead of running DDLS on G itself, we run the sampling
with G*(z, z0) := G(Z) + εz0, where z0 is also jointly updated via Langevin dynamics.
Hyperparameters in ContraD. Unless otherwise specified, we use λcon = λdis = 1, and τ = 0.1
in our experiments, where τ is the temperature hyperparameter defined in (5). We use hidden layer
of size 512 for hr, hf, hd in SNDCGAN and StyleGAN2, and 1024 in SNResNet-18 and BigGAN.
For ContraD models, we use linear warm-up strategy on learning rate up to 3K steps of training for
both generator and discriminator, following practices in contrastive learning (Chen et al., 2020a).
SimCLR augmentations. We use the augmentation pipeline proposed in SimCLR (Chen et al.,
2020a) for training ContraD. Specifically, for CIFAR datasets, we sequentially transform a given
image by: (a) random crop and resizing, (b) horizontal flipping, (c) color jittering with probability
of p = 0.8, and (d) graying the image with probability of p = 0.2. We additionally apply (f)
Gaussian blurring with probability ofp = 0.5 for higher resolution images such as CelebA-HQ-128
and ImageNet. We ensure that every transformation in this pipelines are differentiably implemented,
so that the augmentation still can be used for training the generator of GAN.
23