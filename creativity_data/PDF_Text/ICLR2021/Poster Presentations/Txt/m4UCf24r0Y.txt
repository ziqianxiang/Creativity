Published as a conference paper at ICLR 2021
Knowledge Distillation as Semiparametric
Inference
Tri Dao1, Govinda M. Kamath2, Vasilis Syrgkanis2, Lester Mackey2
1	Department of Computer Science, Stanford University
2	Microsoft Research, New England
trid@stanford.edu, {govinda.kamath,vasy,lmackey}@microsoft.com
Ab stract
A popular approach to model compression is to train an inexpensive student model
to mimic the class probabilities of a highly accurate but cumbersome teacher model.
Surprisingly, this two-step knowledge distillation process often leads to higher ac-
curacy than training the student directly on labeled data. To explain and enhance this
phenomenon, we cast knowledge distillation as a semiparametric inference problem
with the optimal student model as the target, the unknown Bayes class probabili-
ties as nuisance, and the teacher probabilities as a plug-in nuisance estimate. By
adapting modern semiparametric tools, we derive new guarantees for the prediction
error of standard distillation and develop two enhancements—cross-fitting and loss
correction—to mitigate the impact of teacher overfitting and underfitting on student
performance. We validate our findings empirically on both tabular and image data
and observe consistent improvements from our knowledge distillation enhancements.
1	Introduction
Knowledge distillation (KD) (Craven & Shavlik, 1996; Breiman & Shang, 1996; Bucila et al., 2006;
Li et al., 2014; Ba & Caruana, 2014; Hinton et al., 2015) is a widely used model compression technique
that enables the deployment of highly accurate predictive models on devices such as phones, watches,
and virtual assistants (Stock et al., 2020). KD operates by training a compressed student model to
mimic the predicted class probabilities ofan expensive, high-quality teacher model. Remarkably and
across a wide variety of domains (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2019; Liu et al.,
2018; Tan et al., 2018; Fakoor et al., 2020), this two-step process often leads to higher accuracy than
training the student directly on the raw labeled dataset.
While the practice ofKD is now well developed, a general theoretical understanding of its successes
and failures is still lacking. As we detail below, a number of authors have argued that the success ofKD
lies in the more precise “soft labels” provided by the teacher’s predicted class probabilities. Recently,
Menon et al. (2020) observed that these teacher probabilities can serve as a proxy for the Bayes
probabilities (i.e., the true class probabilities) and that the closer the teacher and Bayes probabilities,
the better the student’s performance should be.
Building on this observation, we cast KD as a plug-in approach to semiparametric inference (Kosorok,
2007): that is, we view KD as fitting a student model f in the presence of nuisance (the Bayes
probabilities po) with the teacher's probabilities P asa plug-in estimate of p°. This insight allows Us
to adapt modern tools from semiparametric inference to analyze the error of a distilled student in Sec. 3.
Our analysis also reveals two distinct failure modes of KD: one due to teacher overfitting and data
reuse and the other due to teacher underfitting from model misspecification or insufficient training.
In Sec. 4, we introduce and analyze two complementary KD enhancements that correct for these
failures: cross-fitting—a popular technique from semiparametric inference (see, e.g., Chernozhukov
et al., 2018)—mitigates teacher overfitting through data partitioning while loss correction mitigates
teacher underfitting by reducing the bias of the plug-in estimate p. The latter enhancement was inspired
by the orthogonal machine learning (Chernozhukov et al., 2018; Foster & Syrgkanis, 2019) approach
to semiparametric inference which suggests a particular adjustment for the teacher’s log probabilities.
We argue in Sec. 4 that this orthogonal correction minimizes the teacher bias but often at the cost of
1
Published as a conference paper at ICLR 2021
unacceptably large variance. Our proposed correction avoids this variance explosion by balancing
the bias and variance terms in our generalization bounds.
In Sec. 5, we complement our theoretical analysis with a pair of experiments demonstrating the value
of our enhancements on six real classification problems. On five real tabular datasets, cross-fitting
and loss correction improve student performance by up to 4% AUC over vanilla KD. Furthermore, on
CIFAR-10 (Krizhevsky & Hinton, 2009), a benchmark image classification dataset, our enhancements
improve vanilla KD accuracy by up to 1.5% when the teacher model overfits.
Related work. Since we cannot review the vast literature on KD in its entirety, we point the interested
reader to Gou et al. (2020) for a recent overview of the field. We devote this section to reviewing
theoretical advances in the understanding of KD and summarize complementary empirical studies
and applications of in the extended literature review in App. A.
A number of papers have argued that the availability of soft class probabilities from the teacher rather
than hard labels enables us to improve training of the student model. This was hypothesized in Hinton
et al. (2015) with empirical justification. Phuong & Lampert (2019) consider the case in which the
teacher is a fixed linear classifier and the student is either a linear model or a deep linear network.
They show that the student can learn the teacher perfectly if the number of training examples exceeds
the ambient dimension. Vapnik & Izmailov (2015) discuss the setting of learning with privileged
information where one has additional information at training time which is not available at test time.
Lopez-Paz et al. (2015) draw a connection between this and KD, arguing that KD is effective because
the teacher learns a better representation allowing the student to learn at a faster rate. They hypothesize
that a teacher’s class probabilities enable student improvement by indicating how difficult each point
is to classify. Tang et al. (2020) argue using empirical evidence that label smoothing and reweighting
of training examples using the teacher’s predictions are key to the success of KD. Mobahi et al. (2020)
analyzed the case of self-distillation in which the student and teacher function classes are identical.
Focusing on kernel ridge regression models, they proved that self-distillation can act as increased
regularization strength. Bu et al. (2020) considers more generic model compression in a rate-distortion
framework, where the rate is the size of the student model and distortion is the difference in excess
risk between the teacher and the student. Menon et al. (2020) consider the case of losses such that
the population risk is linear in the Bayes class probabilities. They consider distilled empirical risk
and Bayes distilled empirical risk which are the risk computed using the teacher class probabilities
and Bayes class probabilities respectively rather than the observed label. They show that the variance
of the Bayes distilled empirical risk is lower than the empirical risk. Then using analysis from Maurer
& Pontil (2009); Bennett (1962), they derive the excess risk of the distilled empirical risk as a function
of the `2 distance between the teacher’s class probabilities and the Bayes class probabilities. We
significantly depart from Menon et al. (2020) in multiple ways: i) our Thm. 1 allows for the common
practice of data re-use, ii) our results cover the standard KD losses SEL and ACE which are non-linear
in p0 , iii) we use localized Rademacher analysis to achieve tight fast rates for standard KD losses,
and iv) we use techniques from semiparametric inference to improve upon vanilla KD.
2	Knowledge Distillation Background
We consider a multiclass classification problem with k classes andn training datapoints zi = (xi,yi)
sampled independently from some distribution P. Each feature vector x belongs to a set X, each label
vector y ∈ {e1,...,ek} ⊂ {0,1}k is a one-hot encoding of the class label, and the conditional probability
of observing each label is the Bayes class probability function p0 (x) =E[Y | X = x]. Our aim is to
identify a scoring rule f : X →Rk that minimizes a prediction loss on average under the distribution P.
Knowledge distillation. Knowledge distillation (KD) is a two-step training process where one first uses
a labeled dataset to train a teacher model and then trains a student model to predict the teacher’s predicted
class probabilities. Typically the teacher model is larger and more cumbersome, while the student is
smaller and more efficient. Knowledge distillation was first motivated by model compression (Bucila
et al., 2006), to find compact yet high-performing models to be deployed (such as on mobile devices).
In training the student to match the teacher’s prediction probability, there are several types of loss
functions that are commonly used. Let P(X) ∈ Rk be the teacher's vector of predicted class probabilities,
f(x) ∈ Rk be the student model’s output, and [k] , {1,2,...,k}. The most popular distillation loss
2
Published as a conference paper at ICLR 2021
functions1 '(z;f (χ),p(χ)) include the squared error logit (SEL) loss (Ba & Caruana, 2014)
'se(Zf(X),p(X)) , Pj∈[k]2 (fj (X)-IOg(Pj (X)))2	(SEL)
and the annealed cross-entropy (ACE) loss (Hinton et al., 2015)
心(zf (x) ^(x)) = -P r 1 —Pj(X) 3 loσ∙ (_eχp(βfj(x))_ ʌ	(ACE)
'β(Zf(X),p(X))一乙j∈[k] Pι∈[k]Pι(χ)β log1Pι∈[k]exp(βfι(x))	(ACE)
for an inverse temperature β >0. These loss functions measure the divergence between the probabilities
predicted by the teacher and the student.
A student model trained with knowledge distillation often performs better than the same model trained
from scratch (Bucila et al., 2006; Hinton et al., 2015). In Secs. 3 and 4, we will adapt modern tools
from semiparametric inference to understand and enhance this phenomenon.
3	Distillation as S emiparametric Inference
In semiparametric inference (Kosorok, 2007), one aims to estimate a target parameter or function f0, but
that estimation depends on an auxiliary nuisance function p0 that is unknown and not of primary interest.
We cast the knowledge distillation process as a semiparametric inference problem, by treating the
unknown Bayes class probabilities p0 as nuisance and the teacher’s predicted probabilities as a plug-in
estimate of that nuisance. This perspective allows us bound the generalization of the student in terms
of the mean squared error (MSE) between the teacher and the Bayes probabilities. In the next section
(Sec. 4) we use techniques from semiparametric inference to enhance the performance of the student.
The interested reader could consult Tsiatis (2007) for more details on semiparametric inference.
Our analysis starts from taking the following perspective on distillation. For a given pointwise loss
function '(z;f (x),po(x)), We view the goal of the student as minimizing an oracle population loss
over a function class F,
LD (f,Po)= E['(Z f(X ),Po(X))] with fo，argminf ∈f Ld (f,Po).
The main hurdle is that this is objective depends on the unknown Bayes probabilities p0. We view
the teacher,s model P as an approximate version of po and bound the distillation error of the student
as a function of the teacher’s estimation error.
Typical semiparametric inference considers cases where f0 is a finite dimensional parameter; however
recent work of Foster & Syrgkanis (2019) extends this framework to infinite dimensional models
f0 and to develop statistical learning theory with a nuisance component framework. The distillation
problem fits exactly into this setup.
Bounds on vanilla KD As a first step we derive a vanilla bound on the error of the distilled student
model without any further modifications of the distillation process, i.e., we assume that the student
is trained on the same data as the teacher and is trained by running empirical risk minimization (ERM)
on the plug-in loss, plugging in the teacher’s model instead of P0, i.e.,
f = argminf ∈f Ln(f,p) for Ln (f,p)，En ['(Z ;f (X ),p(X))]	(VanillaKD)
where En [X] = n Pn=ι Xi denotes the empirical expectation of a random variable.
Technical definitions Before presenting our main theorem we introduce some technical notation.
For a vector valued function f that takes as input a random variable X, we use the shorthand notation
kf kp,q，k Ilf(X)kPllLq= E[kf(X)kp] 1/q. Let Vφ and V∏ denotethepartial derivatives of '(z;0,n),
with respect to its second and third input correspondingly and Vφ∏ the Jacobian of cross partial
derivatives, i.e., [Vφ∏'(z[φ,∏)]ij = ∂φ‰'3Φ,∏). Finally, let
qf,P(X)= E[Vφ∏'(Z;f (X),p(X)) |X = x] and Yf,p3= EU〜unif([o,i])[qf,up+(i-u)po (x)]∙
Critical radius Finally, we need to define the notion of the critical radius (see, e.g., Wainwright
(2019, 14.1.1)) of a function class, which typically provides tight learning rates for statistical learning
theory tasks. For any function class F we define the localized Rademacher complexity as:
R(δ;F)= EXi：n,ei：n [supf∈F:kf ∣∣2≤δnPn=Ieif(Xi)]
where i are i.i.d. random variables taking values equiprobably in {-1,1}. The critical radius ofa
class F, taking values in [-H,H], is the smallest positive solution δn to the inequality R(δ;F) ≤ H.
1These loss functions do not depend on the ground-truth label y, but we use the augmented notation
'(z;f (x) ,p(x)) to accommodate the enhanced distillation losses presented in Sec. 4.
3
Published as a conference paper at ICLR 2021
Theorem 1 (Vanilla KD analysis). Suppose f0 belongs to a convex set F satisfying the `2 /`4 ratio
condition SuPf ∈f ∣∣f 一 f0k2,4∕kf — f0∣∣2,2 ≤ C and that the teacher estimates P ∈ P from the Same
dataset used to train the student. Let δn,ζ
=δn + C0 产妥)
for universal constants c0,c1 and δn
an upper bound on the critical radius of the function class
G，{z → r('(zf(x),p(x)) -'(zfo(x),p(x))): f ∈F ,P ∈P, r ∈ [0,1]}.
Let μ(z) = suPφ∣Vφ'(zιφ,p(x))∣2, and assume that the loss '(z∖φ,π) is σ -strongly convex in φ for
each Z and that each g ∈G IS uniformly bounded In [—H,H ]. Then the Vanilla KD f satisfies
II/ —fok2,2 = σ2 O(δ2,ζ C 2H 2kμk2 + kγ>0,p(p-P0)k2,2) Withprobabilityat least 1 -Z.
Thm. 1, proved in App. C, shows that vanilla distillation yields an accurate student whenever the
teacher generalizes well(i.e., kp-po∣2,2 is small) and the student and teacher model classes F and P
are not too complex. The '2 /'4 ratio requirement can be removed at the expense of replacing ∣ μ 14 by
kμk ∞ = SuPz ∣μ(z)∣ inthe final bound. Moreover, we highlight that the strong convexity requirement
for'is satisfied by all standard distillation objectives including SEL and ACE, as itis strong convexity
with respect to the output of f and not the parameters off. Even this requirement could be removed,
but this would yield slow rate bounds of the form: ∣ f - fo ∣∣2,2 = O(δn,ζ + ∣∣γ>,p(p-po)∣∣2,2).
Failure modes of vanilla KD Thm. 1 also hints at two distinct ways in which vanilla distillation
could fail. First, since the student only learns from the teacher and does not have access to the original
labels, we would expect the student to be erroneous when the teacher probabilities are inaccurate due
to model misspecification, an overly restrictive teacher function class, or insufficient training. Prop. 2,
proved in App. D, confirms that, in the worst case, student error suffers from inaccuracy due to this
teacher underfitting even when both the student and teacher belong to low complexity model classes.
Proposition 2 (Impact of teacher underfitting on vanilla KD). There exists a classification problem
in which the following properties all hold simultaneously with high probability for f0 = log(p0):
•	The teacher learns p(x)=7]].)Pn=Iyi forall X ∈X via ridge regression with λ = Θ(1∕n1/4).
•	Vanilla KD with SEL loss and constant f satisfies IIf-fo∣∣ 2,2 ≥ kγ> pcι(p-Po)k2,2 = Ω( √n),
matching the dependence of the Thm. 1 upper bound up to a constant factor.
•	EnhancedKD with SEL loss,夕⑴=diag(击),and constant f satisfies ∣∣f - fo ∣2,2 = O( 1).
Second, the critical radius in Thm. 1 depends on the complexity of the teacher model class P. If P
has a large critical radius, then the student error bound suffers due to potential teacher overfitting even
ifthe teacher generalizes well. Prop. 3, proved in App. E, shows that, in the worst case, this teacher
overfitting penalty is unavoidable and does in fact lead to increased student error. This occurs as the
student only has access to the teacher’s training set probabilities which, due to overfitting, need not
reflect its test set probabilities.
Proposition 3 (Impact of teacher overfitting on vanilla KD). There exists a classification problem in
which the following properties all hold simultaneously with high probability for f0 = E[log(p0(X))]:
•	The critical radius δn of the teacher-student function class G in Thm. 1 is a non-vanishing
constant, due to the complexity of the teacher’s function class.
•	The Vanilla KD error ∣ f - fo 12,2 forconstant f with SEL loss is lower bounded by a non-vanishing
constant, matching the δn dependence ofthe Thm. 1 upper bound up to a constant factor.
•	Enhanced KD with SEL loss,夕⑴=0, and constant f satisfies ∣∣∕-fo∣2,2 = O(n-4/(4+d)).
These examples serve to lower bound student performance in the worst case by the teacher’s critical
radius and class probability MSE, matching the upper bounds given in Thm. 1. However, we note that
in other better-case scenarios vanilla distillation can perform better than the upper-bounding Thm. 1
would imply. In the next section, we adapt and generalize techniques from semiparametric inference
to mitigate the effects of teacher overfitting and underfitting in all cases.
4 Enhancing Knowledge Distillation
To address the two distinct inefficiencies of vanilla distillation revealed in Sec. 3, we will adapt and gen-
eralize two distinct techniques from semiparametric inference: orthogonal correction and cross-fitting.
4
Published as a conference paper at ICLR 2021
4.1	Combating teacher underfitting with loss correction
We can view the plug-in distillation loss '(z;f (x),p(x)) as a zeroth order Taylor approximation to
the ideal loss '(z;f (χ),po(χ)) aroundp. An ideal first-order approximation would take the form
'(zf(x),p(x)) + hpo(x)-p(x),V∏ '(zf(x),p(x))i.
However, its computation also requires knowledge ofp0. Nevertheless, sincep0(x) =E[Y | X =x],
we can always construct an unbiased estimate of the ideal first order term by replacing p0 (x) with y:
'ortho(z;f (χ),j^(χ))= '(zf(χ),P(χ))+hy - j^(χ),E[V∏ '(zf(χ),P(χ)) | χ]i∙	(1)
For standard distillation base losses like SEL and ACE, the orthogonal loss (1) has an especially simple
form, as V∏'(z;f (χ),p(χ)) is linear in f. Indeed, this is true more generally for the following class
of Bregman divergence losses.
Definition 1 (Bregman divergence losses). Any Bregman divergence loss function of the form
'(zf(x),p(x)), Ψ(f(x))-Ψ(g(p(x)))-hVg Ψ(g(p(x))),f (x)-g(p(x))i has
'ortho(z；f(x),p(x)) = '(z；f(x),p(x)) + (y-p(x))>Vpg(P(X))>V2gΨ(g(p(x)))f(x)+const (2)
with the second term bilinear in f (x) and y — p(x). Forthe SEL loss, Ψ(s) = 1 ∣∣sk2, g(p) =log(p),
and the correction matrix Vpg(P(X))> VggΨ(g(p(x))) = diag(p(X)). Similarly, theACE loss falls
into the class of Bregman divergence losses.
We will show that orthogonal correction (1) can significantly improve student bias due to teacher
underfitting; however, for our standard distillation losses (SEL and ACE), the same orthogonal
correction term often introduces unreasonably large variance due to division by small probabilities
appearing in the correction matrix (see Definition 1). To grant ourselves more flexibility in balancing
bias and variance, we propose and analyze a family of γ-corrected losses, parameterized by a matrix
valued function γ : X →Rk ×Rk:
'γ (z;f (χ),p(χ))，'(z;f (χ),p(χ))+(y-P(X))>γ(χ)f(χ)
to mimic the bilinear structure of Bregman orthogonal losses (2). Note that we can always recover
the vanilla distillation loss by taking γ≡0. We denote the associated population and empirical risks by
LD(f,p,γ),E['γ(Z；f (X),p(X))] and Ln(f,p,γ)，En['γ(Z;f (X),p(X))].
Observe that atp0 the correction term is mean-zero and hence LD(f,p0,γ) is independent of γ
LD (f,po)，E['(Z ；f (X ),po(X ))] = LD (f,po,γ) forall γ.
The γ-corrected loss has strong connections to the literature on Neyman orthogonality (Chernozhukov
et al., 2018; Chernozhukov et al., 2016; Nekipelov et al., 2018; Chernozhukov et al., 2018; Foster
& Syrgkanis, 2019). In particular, if the function γ is set appropriately, then one can show that the
γ-corrected loss function satisfies the condition of a Neyman orthogonal loss defined by Foster &
Syrgkanis (2019). We begin our analysis by showing a general lemma for any estimator f, which
adapts the main theorem of Foster & Syrgkanis (2019) to account for approximate orthogonality; the
proof can be found in App. F.
Lemma 4 (Algorithm-agnostic analysis). Consider any estimation algorithm that produces an
estimate f with small plug-in excess risk, i.e.,
_	, O .	_	....	, O .
LD(f,p,γ)-LD(fo,p,γ) ≤e(f,p,γ).
If the loss LD is σ-strongly convex with respect to f andF isa convex set, then
4ll∕-fok2,2 ≤ e(∕,p,γ)+σk(Yfo,p-γ)>(P-PO)k2,2∙
If, in addition, suPz,φ,∏,i∈[d] ∣∣Vφi∏∏'(zgπ)kop ≤ M, then
k(γfo,p-γ)>(p-po)k2,2 ≤ 2(k(qf0,p-γ)>(p-P0)k2,2+M 2kkp-P0k4,4).
Connection to Neyman orthogonality Remarkably, if we set Y = q/。,p, then the γ-corrected loss
is Neyman orthogonal (Foster & Syrgkanis, 2019), and the student MSE bound depends only on the
squared MSE of the teacher. Moreover, q/。,p is an observable quantity for any Bregman divergence loss
(Definition 1) as q/。,p is independent of fo. However, we note that this setting of the Y can lead to larger
variance, i.e., the achievable excess risk can be much larger than the excess risk without the correction.
For instance, in the case of the SEL loss q/。,p (x) = ^X), which can be excessively large when P is
close to 0, leading to a large increase in the variance of our loss. Thus, in a departure from the standard
approach in semiparametric inference, we will be choosing Y in practice to balance bias and variance.
5
Published as a conference paper at ICLR 2021
Example instantiation of student’s estimation algorithm If we use plug-in empirical risk
minimization, ι.e., f = argminf∈τLn(f,p,γ), to estimate fo with P estimated on an independent
sample, then the results of Maurer & Pontil (2009) directly imply that as long as the loss function
'(z;0,n) is uniformly bounded in [-H,H], then, with probability at least 1 -δ,
e(/ P Y)= O ( q∕suPf ∈FVaK'γ (Zf(X),p(X/Iog(Tg/\ + Hlog(T(n)∕δ))
where T (n) = N∞(1 /n,F ,2n) and N∞ (e,F ,m) is the '∞ empirical covering number of function class
F in the worst-case over all realizations of m data points and at approximation level e. This result has two
drawbacks: it is a slow rate result that scales as 1/√n for parametric or bounded Vapnik-Chervonenkis
(VC)-dimension classes, and it requires the student to be fiton a completely separate dataset from the
teacher’s. In the next theorem, we address both of these drawbacks: i) we invoke localized Rademacher
complexity analysis to provide a fast rate result which would be of the order of 1/n for VC or parametric
function classes, and ii) we use a more sophisticated data-partitioning technique called cross-fitting,
which allows the student to be trained using all of the available teacher data.
4.2	Combating teacher overfitting with cross-fitting
We now describe a more sophisticated version of data partitioning to make use of all data points in our
student estimation, while at the same time not suffering from the sample complexity of the teacher’s
function space. This approach is referred to as cross-fitting (CF) in the semiparametric inference
literature (see, e.g., Chernozhukov et al. (2018)):
1.	Partition the dataset into B equally sized folds P1,...,PB.
2.	For each fold t ∈ [B] estimate p(t) and Yo) using all the out-of-fold data points.
3.	Estimate f by minimizing the empirical loss:
∕ = argminf∈f 1 PB=IPi∈pt'γ⑸(Zif(Xi),P⑴(Xi)).	(EnhancedKD)
In other words, the nuisance estimates (^(t) ,p(t)) that are evaluated on the data points in fold t when
fitting the student in step 3, are estimated only using data points outside ofPt.
Theorem 5 (Enhanced KD analysis). Suppose fo belongs to a convex Set F. Let
δn∕B,ζ∕B = δn∕B + coʌ/Blog，；1"/Z) for UniVersal constants co, ci and δn∕B an upper bound
on the critical radius of the class
G(PdYw) = {z→r('^(t) (z;f (x),p(t)(x)) -'γ(t) (z;fo(x),p(t)(x))) :f ∈F,r ∈ [0,1]}
for each t ∈ [B]. Let μ(z) = SuPf ∈f ,t∈[B]∣∣Vφ '^(t) (z；f (X ),p>(t)(x))^2 ,and assume that, with
probability 1 for each t ∈ [B], the loss '^(t) (z;φ,p(t)(x)) is Q-strongly convex in φ for each Z and
each g ∈ G(p(t),Y(t)) is uniformly bounded in [-H,H]. Moreover, suppose that thefunction class
kf-f k
F satisfies the '2/'4 ratio condition: SuPf ∈F⅛f-f⅛4 ≤ C. If f is the output of Enhanced KD, then,
with probability at least 1 - ζ,
8 kf-fok2,2=1MB,ζB 2 * *H 2(kμk2+B Pt=IqE[k(Y-P⑴(X ))>f(x )k2]))
+ σ。( B PB=Ik(Yf0,p(t) -γ(t) )>(P(t) -p0)k2,2).
The proof is found in App. G. Observe that, unlike Thm. 1, the function classes G(P(t),Y(t)) in the
Thm. 5 do not vary the teacher’s model over P but rather evaluate p at the specific out-of-fold estimates
p(t) and only vary f ∈F. Since in practice the teacher,s model can be quite complex, removing this
dependence on the sample complexity of the teacher’s function space can bring immense improvement
with the critical radius of G(p(t) ,γ(t)) significantly smaller than that of G from Thm. 1.
For instance, suppose that the loss function '夕⑴(z;f ,p(t)) is L-Lipschitz with respect to f and that F is
a VC-subgraph class with VC dimension dF. Then the critical radius of the function class G(p(t),Y(t))
is of order 7&予log(n)/n for any choice of (p(t),Y(t)) (see, e.g., Foster & Syrgkanis, 2019, Sec. 4.2).2
However, under the same conditions, the critical radius of the teacher-student function class G in
2In fact, under the Lipschitz condition alone and using contraction lemma arguments as in Foster & Syrgkanis
(2019, Lem. 11), one can derive a version of Thm. 5 in which the upper bound depends only on the critical radius
of the function class {r(f -f0) : f ∈F,r ∈ [0,1]}, which solely depends on the function space of the student.
6
Published as a conference paper at ICLR 2021
Thm. 1 will still depend on the teacher's function space. If P is also a VC-SUbgraPh class with VC
dimension dp》dF, then the critical radius of G will be of the much larger order y/dplog(n)/n.
We can also see in the bound of Thm. 5 the interplay between bias and variance introduced by γ. In
particular, the part of the bound that depends on γ(t) can be further simplified as
,E[δn ,ζ C 4∣∣(Y -P(X ))>γ㈤(X )∣∣2 + k(γp,0(X Ttt(X ))>(P(X )-P0(X ))k2],⑶
where the terms respectively encode the increase in variance and decrease in bias from employing
loss correction. Notably, Thm. 5 implies that CF without Y-correction (i.e., γ(t) (x)=0) is sufficient
to reduce student error due to teacher overfitting but may still be susceptible to excessive student error
due to teacher underfitting. These qualitative predictions accord with our experimental observations
in Sec. 5 and Fig. 5.
4.3 Biased stochastic gradient descent analysis
When the set of candidate prediction rules fθ is parameterized by a vector θ ∈Rd, we may alternatively
fitθ via stochastic gradient descent (SGD) (Robbins & Monro, 1951; Bottou & Bousquet, 2008) on
the γ-corrected objective LD (fθ ,p,Y). With a minibatch size of 1 and a starting point θ0, the parameter
updates take the form
θt+ι = θt-ηtVθ f (Xt)>Vφ'γ(Wt;f (Xt),p(Xt)) for t+1 ∈ [n].	(4)
Ideally, these updates would converge to a minimizer of the ideal risk L(θ; po) = LD (fθ ,po). Our
next result shows that, if the teacher P is independent of (Wt)f, then the SGD updates (4) have
excess ideal risk governed by a bias term Z(γ) and a variance term σ(γ)2∕n. Here, σ0 (θ) represents
the baseline stochastic gradient variance that would be incurred if SGD were run directly on the ideal
risk L(9;po) rather than our surrogate risk. Our proof in App. H builds upon the biased SGD bounds
of Ajalloeian & Stich (2020).
Theorem 6 (Biased SGD analysis). Suppose that the loss L(9；po) is λ-strongly smooth in θ. Define
the bias and root-variance parameters
Z (Y) ,suPθ∈Rd I∣vθ f> (Yf θ ,p - Y)T(P-PO) 112,2
σ(Y) ,suPθ∈Rdσ0(θ) + qE[kvθ fθ(X)>y(x)>(y-PO(X))k2] + ∣∣vθf>(Yfθ,P-γ)>(p-Po)ll2,2
for σO(θ) ， Pi∈[d] Var[Vθi'(W； fθ(X), Po(X))] the unbiased SGD variance. If
Fo = L(θo;po)-minθ∈Rd L(0；Po),then the iterates {θt}n=ι ofthe loss correctedSGDalgorithm satisfy
mint∈[n]E[∣∣VθL(θt-)k2] = O( σ≡√λF0 + Z 2(Y))∙
If, in addition, L(°；po) is μ-strongly convex in θ, then the iterates satisfy
Similar to Thm. 5, the bound in Thm. 6, portrays the interplay ofbias and variance as ^ ranges from
0 to qfθ,p (recall that qfθ ,p is independent of fθ for any Bregman loss). In particular, the part of the
bound for strongly convex losses that depends on ^ can be further simplified to:
e[( λk*(xtk2μY-p0(Xtk2 + k(γfθ,p(x)-Y(X ))>(P(X) -PO (X ))k2)kVθ fθ(x )k2i	(5)
This has a very intuitive form: the first term is the impact of γ(X) on the variance, which is also
related to the square of the noise ofy, divided by the standard error scaling. The second controls how
Y improves the bias introduced by the error in the teacher,s P.
5 Experiments
We complement our theoretical analysis with a pair of experiments demonstrating the practical benefits
of cross-fitting and loss correction on six real-world classification tasks. Throughout, we use the SEL
loss and report mean performance ± 1 standard error across 5 independent runs. Code to replicate
all experiments can be found at
https://github.com/microsoft/semiparametric-distillation,
and supplementary experimental details and results can be found in App. I.
7
Published as a conference paper at ICLR 2021
Selecting the loss correction matrix Y Motivated by the analyses in Sec. 4, for each training point
(χ,y), We will select our correction matrix γ(χ) to balance bias and variance by minimizing a pointwise
upper bound on the loss correction error (5) (ideally with a closed-form solution to avoid excessive
computational overhead).3 To eliminate dependence on the unobserved p0, we observe that the
bias term k(Yf0,p(x) - ^(x))> (P(X) -Po(x))k2 =。(|%°,p(x)-^(x)k2p) UP to additive terms
independent of γ. We introduce a tunable hyperparameter α> 0 to trade offbetween this bias bound
and the variance term in (5) and select ^(x) = diag(v(x)) to minimize:
E[kγ,(χ)(y -P(X)) k2 |χ]+αkqfθ ,p(X)-^(X) kθp=E[kv(x)(y -P(X)) k2 | x]+αk 志一V(X) k2.
Since the conditional expectation involves the unknown quantity p0, we estimate
E[kv(X)(y - P(X))k2 I x] with its sample Ilv(X)(y - P(X))Il2∙4 ThiS objective is quadratic in
V(X) and thus has a closed-form solution. Given ^(x), the student,s loss-corrected objective is
equivalent to a square loss with labels log(P(X))+γ(X)>(y - p(x)).
Tabular data. We first validate our KD enhancements on five real-world tabular datasets—FICO (FIC),
StumbleUpon (Eve; Liu et al., 2017), and Adult, Higgs, and MAGIC from Dheeru & Karra Taniskidou
(2017)—with random forest (Breiman, 2001) students and teachers. In Fig. 1a, we examine the impact
of varying student model capacity for a fixed high-capacity teacher with 500 trees on FICO. This
setting lends itself to teacher overfitting, and we find that cross-fitting consistently improves upon
vanilla KD by up to 4 AUC percentage points. In Fig. 1b we explore the impact of teacher underfitting
by limiting the teacher’s maximum tree depth on Adult. Here we observe consistent gains from loss
correction with student performance exceeding even that of the teacher for smaller maximum tree
depths. Analogous results for the remaining datasets can be found in App. I.1.
Student's number of trees
0.920 -
0.915-
0.910-
0.905-
0.900 -
0.895-
0.890 -
0.885-
0.880-
1 2 3	5	10	15	20
Teacher's max tree depth
(a) FICO dataset, when teacher overfits	(b) Adult dataset, when teacher underfits
Figure 1: For random forest students and teachers, cross-fitting improves student performance when the teacher
overfits, while loss correction improves student performance when the teacher underfits.
Image data. We next validate our KD enhancements on the image classification dataset CIFAR-10
(Krizhevsky & Hinton, 2009). We pair a residual network (ResNet-8) student with teacher networks
of varying depths (ResNet-14/20/32/44/56) (He et al., 2016). It has been observed that larger and
deeper teachers need not yield better students, as the teacher might overfit to the training set (Cho &
Hariharan, 2019; Muller et al., 2019). To induce this overfitting, we turn off data augmentation (random
horizontal flipping and cropping). We compare students trained with Vanilla KD and Enhanced KD
with and without loss correction in Fig. 2. We find that cross-fitting consistently reduces the effect
of teacher overfitting with largest impact realized for the deepest models. This effect is most evident
in the cross-entropy test loss, where the Vanilla KD student incurs significantly larger loss than the
cross-fitted student. For both accuracy and test loss, employing loss correction on top of cross-fitting
provides an additional small performance boost.
Effect of the loss correction hyperparameter α. Our hyperparameter α controls the tradeoff between
bias and variance in loss correction. When α is very small, the objective is close to the vanilla KD
objective. When α is large, the objective is closer to the Neyman-orthogonal loss. In Figure 3, we show
the effect of varying α, with ResNet-8 as the student and ResNet-20 as the teacher, on the CIFAR-10
dataset. Large values of α lead to high variance and thus lower test accuracy. Intermediate values ofα
improves on both the Vanilla KD objective, which corresponds to α = 0 and on the orthogonal objective
3Balancing the bias and variance terms (3) of Thm. 5 yields a similar objective.
4An alternative estimate that performs slightly worse is ∣∣v(x)p3(x)(1-p(x))k2.
8
Published as a conference paper at ICLR 2021
Figure 2: On CIFAR-10 with ResNet students and teachers, cross-fitting reduces the effect of teacher overfitting,
and loss correction yields an additional small performance boost. Here, the test loss is cross-entropy.
(α = ∞). The test accuracy drops sharply beyond some threshold ofα as the variance becomes too
high (due to the terms qp(x) = diag (-^ɪ),…,-.[))),causing training to become unstable.
Enhanced KD Student
432109876
888887777
Ae,lne①
0.85
0 5 0 5 0 5 0
-87 7∙6∙65 5
Ooooooo
Wwo- ⅛ΦH
10^3	10^2 IO-I
alpha
Figure 3: On CIFAR-10 with ResNet students and teachers, large values of the loss correction hyperparameter α
(corresponding to the orthogonal loss correction) lead to large variance and training instability, while intermediate
values improve upon cross-fit KD without loss correction (α = 0). Here, the test loss is cross-entropy.
6 Conclusion
We developed anew analysis of knowledge distillation under the lens of semiparametric inference. By
framing the KD process as learning with plug-in estimation in the presence of nuisance, we obtained
new generalization bounds for distillation and new lower bounds highlighting the susceptibility
of KD to teacher overfitting and underfitting. To address these failure modes, we introduced two
complementary KD enhancements—cross-fitting and loss correction—which improve student
performance both in theory and in practice. Past work has shown that augmenting the student training
set with synthetic data from a generative model (e.g., a generative adversarial network (Liu et al., 2018)
or MUNGE (Bucila et al., 2006)) often leads to improved student performance. A natural next step is to
prove an analogue of Thm. 5 for synthetic augmentation to understand when this strategy successfully
mitigates the impact of teacher overfitting. In addition, two tantalizing open questions are, first, whether
other techniques from semiparametric inference, such as targeted maximum likelihood (Van Der Laan
& Rubin, 2006), can be used to improve KD performance and, second, whether a semiparametric
perspective can explain the surprising success of self-distillation (Furlanello et al., 2018) and noisy
student training (Xie et al., 2020) through which students routinely outperform their teachers.
References
Stumbleupon evergreen dataset. https://www.kaggle.com/c/stumbleupon.
9
Published as a conference paper at ICLR 2021
FICO: Explanable machine learning challenge. https://community.fico.com/s/
explainable-machine-learning-challenge.
Ahmad Ajalloeian and Sebastian U Stich. Analysis of sgd with biased gradient estimators. arXiv
preprint arXiv:2008.00051, 2020.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information
processing Systems ,pp. 2654-2662,2014.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict
statistical optimality? In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1611-1619. PMLR, 2019.
George Bennett. Probability inequalities for the sum of independent random variables. Journal of
the American Statistical Association, 57(297):33-45, 1962.
Sergei Bernstein. The theory of probabilities. Gastehizdat Publishing House, 1946.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161-168, 2008.
Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
Leo Breiman and Nong Shang. Born again trees. University of California, Berkeley, Berkeley, CA,
Technical Report, 1:2, 1996.
Yuheng Bu, Weihao Gao, Shaofeng Zou, and Venugopal V Veeravalli. Information-theoretic under-
standing of population risk improvement with model compression. In AAAI, pp. 3300-3307, 2020.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings
of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
Philadelphia, PA, USA, August 20-23, 2006, pp. 535-541, 2006. doi: 10.1145/1150402.1150464.
Yevgen Chebotar and Austin Waters. Distilling knowledge from ensembles of neural networks for
speech recognition. In Interspeech, pp. 3439-3443, 2016.
Wei-Chun Chen, Chia-Che Chang, and Che-Rung Lee. Knowledge distillation with feature maps
for image classification. In Asian Conference on Computer Vision, pp. 200-215. Springer, 2018.
Xu Cheng, Zhefan Rao, Yilan Chen, and Quanshi Zhang. Explaining knowledge distillation by
quantifying the knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 12925-12935, 2020.
Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K. Newey, and James M.
Robins. Locally Robust Semiparametric Estimation. arXiv e-prints, art. arXiv:1608.00033, July
2016.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney
Newey, and James Robins. Double/debiased machine learning for treatment and structural
parameters. The Econometrics Journal, 21(1):C1-C68, 2018. doi: 10.1111/ectj.12097. URL
https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097.
Victor Chernozhukov, Whitney Newey, and Rahul Singh. De-Biased Machine Learning of Global
and Local Parameters Using Regularized Riesz Representers. arXiv e-prints, art. arXiv:1802.08667,
February 2018.
Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 4794-4802, 2019.
Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks.
In Advances in neural information processing systems, pp. 24-30, 1996.
Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL
http://archive.ics.uci.edu/ml.
10
Published as a conference paper at ICLR 2021
Rasool Fakoor, Jonas Mueller, Nick Erickson, Pratik Chaudhari, and Alexander J Smola. Fast, accurate,
and simple models for tabular data via augmented distillation. arXiv preprint arXiv:2006.14284,
2020.
Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036,
2019.
Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran. Ensemble distillation for neural machine
translation. arXiv preprint arXiv:1702.01802, 2017.
Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. arXiv preprint arXiv:1805.04770, 2018.
Yotam Gil, Yoav Chai, Or Gorodissky, and Jonathan Berant. White-to-black: Efficient distillation
of black-box adversarial attacks. arXiv preprint arXiv:1904.02405, 2019.
Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. Adversarially robust distillation. In
Proceedings ofthe AAAI Conference on Artificial Intelligence, volume 34,pp. 3996-4003,2020.
Jianping Gou, Baosheng Yu, Stephen John Maybank, and Dacheng Tao. Knowledge distillation: A
survey. arXiv preprint arXiv:2006.05525, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, and Ming Zhou.
Attention-guided answer distillation for machine reading comprehension. arXiv preprint
arXiv:1808.07644, 2018.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.
Michael R Kosorok. Introduction to empirical processes and semiparametric inference. Springer
Science & Business Media, 2007.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Jinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan Gong. Learning small-size dnn with output-distribution-
based criteria. In Fifteenth annual conference of the international speech communication
association, 2014.
Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In
Proceedings of the ieee conference on computer vision and pattern recognition, pp. 6356-6364, 2017.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
Ruishan Liu, Nicolo Fusi, and Lester Mackey. Teacher-student compression with generative
adversarial networks. arXiv preprint arXiv:1812.02271, 2018.
Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, and Ce Zhang. MLBench: How good are machine
learning clouds for binary classification tasks on structured data. ArXiv e-prints, 2017.
Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner. Data-free knowledge distillation for deep
neural networks. arXiv preprint arXiv:1710.07535, 2017.
David Lopez-Paz, Leon Bottou, Bernhard Scholkopf, and Vladimir Vapnik. Unifying distillation and
privileged information. arXiv preprint arXiv:1511.03643, 2015.
11
Published as a conference paper at ICLR 2021
Liang Lu, Michelle Guo, and Steve Renals. Knowledge distillation for small-footprint highway
networks. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP),pp. 4820-4824. IEEE, 2017.
Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance
penalization. arXiv preprint arXiv:0907.3740, 2009.
Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, Seungyeon Kim, and Sanjiv Kumar.
Why distillation helps: a statistical perspective. arXiv preprint arXiv:2005.10419, 2020.
Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization
in hilbert space. arXiv preprint arXiv:2002.05715, 2020.
Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, and Zhi Jin. Distilling word embeddings: An encoding
approach. In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management, pp. 1977-1980, 2016.
Rafael Muller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems, pp. 4694-4703, 2019.
Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):
141-142, 1964.
Ndapandula Nakashole and Raphael Flauger. Knowledge distillation for bilingual dictionary induction.
In Proceedings of the 2017 conference on empirical methods in natural language processing, pp.
2497-2506, 2017.
Denis Nekipelov, Vira Semenova, and Vasilis Syrgkanis. Regularized Orthogonal Machine Learning
for Nonlinear Semiparametric Models. arXiv e-prints, art. arXiv:1806.04823, June 2018.
Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George
Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast high-fidelity
speech synthesis. In International conference on machine learning, pp. 3918-3926. PMLR, 2018.
Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised
knowledge transfer for deep learning from private training data. arXiv preprint arXiv:1610.05755,
2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016b.
Mary Phuong and Christoph Lampert. Towards understanding knowledge distillation. In International
Conference on Machine Learning, pp. 5142-5151, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann.
Math. Statist., 22(3):400-407, 09 1951. doi: 10.1214/aoms/1177729586. URL
https://doi.org/10.1214/aoms/1177729586.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability
of deep neural networks by regularizing their input gradients. arXiv preprint arXiv:1711.09404, 2017.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Peng Shen, Xugang Lu, Sheng Li, and Hisashi Kawai. Feature representation of short utterances based
on knowledge distillation for spoken language identification. In Interspeech, pp. 1813-1817, 2018.
Pierre Stock, Armand Joulin, Remi Gribonval, Benjamin Graham, and HerVe Jegou. And the bit goes
down: Revisiting the quantization of neural networks. 2020.
Sarah Tan, Rich Caruana, Giles Hooker, Paul Koch, and Albert Gordo. Learning global additive
explanations for neural nets using model distillation. arXiv preprint arXiv:1801.08640, 2018.
12
Published as a conference paper at ICLR 2021
Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain.
Understanding and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.
Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media, 2007.
Mark J Van Der Laan and Daniel Rubin. Targeted maximum likelihood learning. The international
journal of biostatistics, 2(1), 2006.
Vladimir Vapnik and Rauf Izmailov. Learning using privileged information: similarity control and
knowledge transfer. J. Mach. Learn. Res.,16(1):2023-2049,2015.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi:
10.1017/9781108627771.
Chong Wang, Xipeng Lan, and Yangang Zhang. Model distillation with knowledge transfer from
face classification to alignment and verification. arXiv preprint arXiv:1709.02929, 2017.
Ji Wang, Weidong Bao, Lichao Sun, Xiaomin Zhu, Bokai Cao, and S Yu Philip. Private model
compression via knowledge distillation. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 1190-1197, 2019.
Shinji Watanabe, Takaaki Hori, Jonathan Le Roux, and John R Hershey. Student-teacher network
learning with enhanced features. In 2017 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 5275-5279. IEEE, 2017.
Geoffrey S Watson. Smooth regression analysis. Sankhya： The Indian Journal ofStatistics, Series
A, pp. 359-372, 1964.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 10687-10698, 2020.
13
Published as a conference paper at ICLR 2021
A	Extended literature review
We point the interested reader to Gou et al. (2020) for a sweeping survey of the many developments in knowledge distillation over
the past half decade. In addition to the references discussing theoretical aspects of knowledge distillation provided in Sec. 1, we
highlight here a number of empirical investigations of why distillation works. Cho & Hariharan (2019) show that larger teacher
models do not necessarily improve the performance of student models as parsimonious student models are not able to mimic
the teacher model. They suggest early stopping in training large teacher neural networks as means of regularizing. Cheng et al.
(2020) demonstrate that when applied to image data, distillation allows the student neural net to learn multiple visual concepts
simultaneously, while, when learning from raw data, neural networks learn concepts sequentially.
Knowledge distillation has also been used for adversarial attacks (Papernot et al., 2016b; Ross & Doshi-Velez, 2017; Gil et al.,
2019; Goldblum et al., 2020), data security (Papernot et al., 2016a; Lopes et al., 2017; Wang et al., 2019), image processing (Li
& Hoiem, 2017; Wang et al., 2017; Chen et al., 2018; Li et al., 2017), natural language processing (Nakashole & Flauger, 2017;
Mou et al., 2016; Hu et al., 2018; Freitag et al., 2017), and speech processing (Chebotar & Waters, 2016; Lu et al., 2017; Watanabe
et al., 2017; Oord et al., 2018; Shen et al., 2018).
B Glossary
Table 1: Glossary of notation
Notation	Definition
'(Z;f(X),P0(X)) Population risk LD(f,P) Empirical risk Ln(f,P) Population optimal student model f0 Empirical optimal student model f kfkp,q Vφ V∏ Vφ∏ qf,p(x) γf,p(x) R(δ;F) δn γ-corrected loss 'γ (z;f(x),P(x)) Population γ-risk LD (f,P,γ) Empirical γ-risk Ln(f,P,γ)	Loss function on a random data point E['(Z;f(X),P(X))] En['(Z;f(X),P(X))] argminf ∈F LD (f,P0) argminf ∈f Ln(f,P) kkf(X)kpkLq=Ekf(X)kqp1/q Partial derivative of '(z;φ,∏) with respect to the second input Partial derivative of '(z;φ,∏) with respect to the third input [vΦ∏ '(zφ,π)]i,j = ∂φ‰ '(zφ,π) E[Vφ∏'(Z ;f(X ),p(X ))1 X=x] EU〜Unif([0,1]) [qf,Up+(1-U)po (x)] Localized Rademacher complexity of function class F Critical radius '(Zf(X),p(X))+(y -P(X))T Y(X) f (X) E['γ (Z；f (X ),P(X))] En['γ (Z f(X ),P(X ))]	
C Proof of Thm. 1: Vanilla distillation analysis
Introduce the shorthand 'f,p(z)= '(z；f (χ),p^(χ)). Since δn upper bounds the critical radius of the function class G, the localized
Rademacher analysis of Foster & Syrgkanis (2019, Lem. 11) implies5
[Ln(f ,p)-Ln(f0,P)-(LD (Ap)-LD (f0,P))I ≤ O(Hδn< k'j^,p-'fo ,pk2,2 +Hδ2,Z )
with probability at least 1 - ζ . Moreover, by Cauchy-Scwharz,
...	.	..	.....o ...
k'f,p-'fo,pk2,2 ≤ kμk4kf-f0k2,4.
By the assumed '2 /'4 ratio condition we therefore have
e(f,P,Y) ≤ θ(δn,ζCHkμk4kf-f0k2,2 +Hδn,ζ).
Plugging this bound into Lemma 4 (which holds irrespective of whether data re-use, sample splitting, or cross-fitting is employed)
and applying the arithmetic-geometric mean inequality yields
8 ιι∕-f0k2,2 ≤ σO(δn<C 2H 2kμk2+kγ>0(P-Po)k2,2)
5We apply Foster & SyrgkaniS (2019, Lem. 11) with Lg = g for g ∈ G with g* = 0. Then we instantiate the
concentration inequality for the choice g='^ ^-'fo,p ∈G.
14
Published as a conference paper at ICLR 2021
D Proof of Prop. 2: Impact of teacher underfitting on vanilla distillation
Suppose that p0 does not vary with x and, for known > 0, belongs to the set
P={p : pj (x) ∈ [,1],∀x∈X,j∈ [k]}.
As all quantities in this proof are independent of x, we will omit the dependence on x whenever convenient.
Consider the constant teacher estimate P = 1+^ ∨ e obtained Via ridge regression with regularization strength λ ≤ 1 and y，nPn=Iyi.
A constant student prediction rule in
F={f : fj (x) ∈ [log(),0],∀x∈X,j∈ [k]}
trained via Vanilla KD with SEL loss yields f(χ) =log(p).
Suppose that, unbeknownst to the teacher and student, the true p0 satisfies the more stringent condition p0,j ≥ 2 for all j ∈ [k].
Then the student satisfies
f0-∕ = lOg(PO)-log(P) ≥ diag(p10)(P0 -P)= γf0,p0 (P0 -P)= γf0,p0 (λp0(⅞p0) +min(O,ι+λ -e))
_c/> ∕λp0-(y-P0) I rn∙n(C y-P0+P0-(I+ λ"∖∖、> λP0-ly-P0l
=γf0,p0(—(1+λ) +min(0,	1+λ	)) ≥ γf0,P0 -(1+λ)一
by the concavity of the logarithm and the choice λ ≤ 1. Since
P(Iyj-Po,jι≥Mi)≤2Z for θ≥q2⅛j)log(Z)+3就log(k)
by Bernstein’s inequality (Bernstein, 1946), we have
P(kf0- fk2,2 ≥ kγ;0,p0 (P0-P)k2,2) ≥ P(f0,j-fj ≥ 0, Vj ∈ [k]) ≥ P(λP0,j ≥ |yj - P0j∣, Vj ∈ [k]) ≥ 1-2ζ
whenever	_______
q n log( k)+4 n⅛ log( k) ≤λ ≤ L
Moreover, since limsup /	VnIyj-p0jl	― = 1 With probability 1 by the law of the iterated logarithm,
n→∞	√2P0,j (1-P0,j )loglog(n)	,
kY> PO(P0 -P)k2,2 =Ω(min(1,λ2)) with probability 1 whenever 1 ≥ λ ≥ “，悭应.The choice
λ=min(1,maχ( n14 ,q 2lognog(n) ,q n . Z)+4 log( Z)))=。(人)
now yields the first two advertised claims.
The final claim follows directly from Thm. 5 with B = O(1) as ^(t) = γf0,p and the critical radius of G(P(t),Y(t)) satisfies
δn∕B =O(PBkIn by Wainwright (2019, Ex. 13.8).
E Proof of Prop. 3: Impact of teacher overfitting on vanilla distillation
Suppose that P0 has Lipschitz gradient and, for known >0, belongs to the set
P={P : Pj (x) ∈ [,1], Vx∈X,j ∈ [k]}.
Suppose moreover that X ∈ Rd has Lebesgue density bounded away from 0 and ∞ and that e < 4 E [,jX)(X-P/j (XX)] for each
j. Consider the teacher estimates Pj (x) = max(e,Pj (x)) forP the Nadaraya-Watson kernel smoothing estimator (Nadaraya, 1964;
Watson, 1964)
(x), yi	if x=xi
P	Pin=1yiK((x-xi)/h)/Pin=1K((x-xi)/h) otherwise
with kernel K(x) = kxk2-aI[kxk2 ≤ 1], a ∈ (0,d/2), and h = n-1/(4+d). By Belkin et al. (2019, Thm. 1), the teacher satisfies
E[kP0-Pk2,2] = O(n-4∕(4+d)).
Now instantiate the notation of Thm. 1, and consider a student prediction rule trained to learn a constant prediction rule via Vanilla
KD with the SEL loss and
F = {f : f(x) =f(x0) ∈ [log(e),0]k for all x,x0 ∈ X}.	(6)
Since P exactly interpolates the observed labels (i.e., P(Xi) = yn), the critical radius of the teacher-student function class
G satisfies δn = Ω(1). Moreover, since the student only has access to the teacher,s training set probabilities, its estimate
f(x) = 1 Pn=JOg(max(yn,e)) is inconsistent for the optimal constant rule f0 (x)= E[log(P0 (X))] as
f0,j(x)-Efj(x)= E[log(P0,j(X))-log(max(Yj,e))] ≥E[p0j(X)0-1(Xx(YY) + (max(YY)-P0j(X))2]
=e[p0,j(X)(i-p0,j(X))2+(i-p0,j(X))(po,j(X)-e)2 ] -eE[ i-p0 j(X)] ≥ e[p0j(X)(i-p0,j(X))2 ]
15
Published as a conference paper at ICLR 2021
by Taylor’s theorem with Lagrange remainder. This non-vanishing student error reflects the non-vanishing critical radius δn of the
composite student-teacher function class G defined in Thm. 1; since the student function class F has low complexity, the complexity
of G is driven by the highly flexible interpolating teacher.
Next, instantiate the notation of Thm. 5, and consider a student prediction rule f trained via Enhanced KD with SEL loss, γ(t) =0,
B = O(1), and F (6). The critical radius of G(p^(t),γ(t)) satisfies δn∕B =O(PBkln) by Wainwright (2019, Ex. 13.8). Moreover,
each cross-fitted teacher satisfies E[∣∣po -p^(t) ∣∣2"= O(n-4/(4+d)) by Belkin et al. (2019, Thm, 1), so, by Chebyshev's andJensen's
inequalities, with probability at least 1 - Z/2,
kP0-p(t)k2,2 ≤ E[kP0-p㈤ ∣∣2,2] + P2BVar(MO-p(t)k2,2)/Z
≤ (1 + P2BTZ ),E[kP0-P⑴k2,2] = O(n-2/(4+d)) forall t.
Therefore, Thm. 5 implies that
kf-fok2,2=O( n+B PB=1 k(Yfo,p(t) )>(P(t) -P0)k2,2)
=O( n+B PB=Ikmiag(^p1))(P(t) -PO) k23
=O( 1+⅛2 P3kP(t)- P0k2,2)=O(n-4/(4+d))
with probability at least 1 - ζ.
F	Proof of Lemma 4: Algorithm-agnostic analysis
First we define for any functional L(f) the Frechet derivative as:
∂
DfLf)[v] = ∂tL(f+tν) |t=0
When L is an operator of the form: E[g(f (X))], then: Df L(f)[ν]= E[Vg(f (X ))>ν (X)].
By the σ-strong convexity of LD,6 we have that
LD (f,P,Y ) ≥ LD (f0,P,Y ) + Df LD (f0,P,Y )[∕-f0] + 2 kf-f0k2,2∙
Furthermore, our excess risk assumption and the optimality off0 give us
2 Il/-f0k2,2 ≤ LD (f,P,Y); LD (f0,P,Y), -Df LD (f0,P,Y)[∕-f0]
excess risk of f
(a)
≤ e(f,P,Y)-DfLD(f0,P0,γ)[f-f0]+Df(Ld(f0,P0,γ)-Ld(f0,P,Y))[f-f0]∙
X---------{---------}
≥0 by optimality of f0
By Taylor's theorem with integral remainder,
. . .. . . ^. ... 一
E[hVφ'(W f0(x),P0(x))-Vφ'(W f0(x),P(x)),f(x)-f0(x)i∣ X = x]	⑺
= (PO(X)- P(X))I Yf0,p(X)(J(X)- f0(X))
whenever Vφ∏' is well-defined. We can now invoke the expansion (7) and Cauchy-Schwarz to obtain the bound
_	, _	, .	.	_	. . . O .,
Df (LD (f0,P0,γ) -Ld (f0,P,γ))[f-f0]
. . . . . . ^ . ..,
=E[hVφ'(W f0(X ),P0(X)) -Vφ'(W f0(X ),p(X )),f(X)-f0(X )〉]
-E[(p0(X )-P(X ))>γ (X )(f(X )-f0(X))]
=E[(P0(X )-P(X ))>(Yf0,p(X )-γ (X ))(f(X )-f0 (X))]
≤ E[k(p0(X)-P(X))>(Yf0,p(X)-γ(X))k2kf(X)-f0(X)k2]
≤ k(P0 - p5)>(Yfo,p-Y)k2,2k∕-f0k2,2
Thus combining all the above inequalities:
σ2 kf-f0k2,2 ≤ e(f,p,Y ) + k(P-PO)T(Yf0,P-Y )k2,2k∕-f0k2,2
6Notably this strong convexity assumption can be relaxed to E [Vφ'(W ;f0(X ),p0(X ))(f(X) — f0 (X)] ≥ 0.
16
Published as a conference paper at ICLR 2021
By an AM-GM inequality, for all a,b ≥ 0: a ∙ b ≤ 1 (2 a2 + σ b2). Applying this to the product of norms on the RHS and re-arranging
yields
4ll∕-∕ok2,2 ≤e(f,f>,Y) +Ik(P-PO)T(Yf0,P-Y)k2,2.
To get the final inequality, observe that:
k(∙p-P0 )>(Yfo,p-γ)k2,2 ≤ 2k(p-po)>(qfo,p-γ)k2,2+2k(p-po)>(γfo,p-qfo,p)k2,2
Moreover, by the boundedness of the third derivative, we have:
k(P-P0)>(γfo,p-qfo,p)k2,2≤ E[kP(x)-P0(X)k2∣Yfo,p(x)-q∕0,p(x)k2]
≤ E[kP(X)-P0(X)k2M2kk^(X)-P0(X)k2]
≤ M2kkP-P0k4,4
Combining all the above yields the final bound.
G	Proof of Thm. 5: Cross-fitted ERM analysis
Let Ln,t denote the empirical loss over the samples in the t-th fold andP(t),Y(t) the nuisance functions used on the samples in
the k-th fold. For any t ∈ [K ] and conditional on P^(t),γ(t), suppose that δn upper bounds the critical radius of the function class
G(P(t) ,Y(t) ),then by Lemma 11 ofFoster & Syrgkanis (2019),7 if We denote with 'tf(z) = 'γ(t)(zf(χ),P(t) (x)), w.p. 1-Z:
∖Ln,t(f,PRY⑶)-Ln,t(f0,^(t ,Y⑶)-(LD (f,r>w 2、-LD (f。,P(t),Y(t))) | ≤ θ(Hδn∕B,Z k't,^-'t,fo ∣2,2 + H δn/bJ
Moreover, we have that by the definition of cross-fitted ERM:
17 B
B XLn,t(f 伊),Y⑴)-Ln,t(f0,P()种)≤ 0
t=1
Thus we have that w.p. 1 -ζB:
B X LD (f,^t,Y ⑴)-Ld (f0,P(t),Y(t)) ≤ θ[Hδn∕B,Z B X k't,f-'t,fo ∣2,2 +Hδnl∕B,ζ∖
t=1	t=1
Moreover, if we let μ(z) = sup@,J| ^'(之涉“)(x)) ∣∣ 2,then we have by Cauchy-Schwarz inequality:
k't,f-'t,fok2,2≤ kμ∣4kf-f0k2,4+JEh((Y-P(t)(X))>γ(t)(X)(f(X)-f0(X)))2i
≤ kμ∣4kf-f0k2,4+e II(Y-P⑻(X))>^(t)(X)∣∣2kf(X)-f0(X))k2
≤ (kμ∣4+E ∣∣(Y-P(t)(X))>^(t)(X)∣∣4]")kf-f0k2,4
If we further assume that the function class F satisfies an '2 /'4 condition that:
Sup kf-f0k2,4 ≤c
f ∈F kf-f0k2,2 -
then w.p. 1 -ζ:
B X e(f,P(t),Y(t)) ≤ O^Hδn∕B,Z∕B B X c(kμp∣∣4 + e[∣∣(Y -P(X ))>Y(X 升 4]1∕4)kf-f0k2,2 +Hδn∕B,ζ∕B)
Applying Lemma 4 for any P(D ,^(t) and averaging the final inequality we get:
4kf-f0k2,2 ≤ BX(e(f,*,γ(t))+1 k(γfo,p(t) -Y⑴)τ(P⑴-P0)k2,21
t=1	σ
7We apply the lemma with Lg = g and g ∈ G(p(t) ,γ(t)) and g* = 0. Then we instantiate the concentration
inequality with g = 't f-lt,fo ∈ G(p(t) ,γ(t)).
17
Published as a conference paper at ICLR 2021
Plugging in the bound above to Lemma 4 and applying the AM-GM inequality and Jensen’s inequality, yields:
8 kf-fok2,2 ≤ "1凤/与 c 2h 2 (k〃k2+BB XrEhlI(Y -P ⑴(X ))>^ ㈤(X )∣∣4]))
+10(⅛ X k(Yfo,p(t)-Y(t))>(P(t)-P0)k2j.
H	Proof of Thm. 6: Biased SGD analysis
Below, for any integer s, we define the operator norm of any vector v ∈Rs and any tensor T operating on Rs as
kvkop, kvk2 and kTkop, sup kT [v]kop.
v:kvk2=1
Recall the definition
V(W W)=Vθ fθ (X )>Vφ'γ (W fθ (X ),p(X))
=Vθ fθ (X )>(Vφ'(W fθ (X ),P(X ))+γ(X )>(Y -P(X))).
Observe that since E[Y | X=x] =p0(x), we can write for anyγ:
L(θ∙,P0)= E['(W fθ (X ),P0(X )) + (Y - P0(X ))>γ(X )fθ (X )]= E['γ (W fθ (X ),P0 (X))]
Thus we also have that:
∀θ,γ: Vθ L(θg)= E[V(W ∙,θ,P0,γ)]
Given this observation, we can decompose the gradient that is used in our SGD algorithm into a bias and variance component,
when viewed from the perspective of a biased SGD algorithm for the population oracle loss:
V(W ∙,p,γ)= VθL(θg)
+E[V(W-,p,γ)]- E[V(W W,P0,γ)]+V(W ∙,θ,p,γ)-E[V(W ∙,θ,p,γ)]
x---------------{--------------} X------------{------------}
b(θ,P,γ)	n(W iθ,P,Y)
The following two lemmas bound the gradient bias and noise terms.
Lemma 7 (Gradient bias). If supχ,φ,∏ ∣∣E[V∏∏φ '(W ;0,n) | X = x] k0p ≤ M, thenfor any parameter vector θ andfunCtiOnS P and
γ, we have:
b(θ,P,γ)=E[Vθfθ(X)>(γfθ,p(X)-γ(X))>(P(X)-P0(X))],
∣b(θ,P,γ)∣2 ≤ llVθfθ>(γfθ,p-γ)>(P-P0)ll2,2, and
Ilb(θ,p,γ)k2 ≤ ∣∣vΘ f> (qfθ,p-γ)>(p -p0)ll2,2 + ^2 IIvΘ fθ l∣F,2kp-p0k2,4∙
Proof By Taylor’s theorem with integral remainder and Lagrange remainder respectively the SGD bias for each parameter i
takes the form
bi(θ,P,γ)= E[Vi(W ∙,θ,p,γ)] -E[Vi(W -P0,γ)]
=E[hVφ'γ (W fθ (X ),p(X ))-Vφ'γ (W ;fθ(X ),P0(X )),Vθifθ (X )〉]
= E[(P(X)-P0(X))>(γfθ,p(X)-γ(X))Vθifθ(X)]
= E[(P(X)-P0(X))>(qfθ,p(X) -γ(X))Vθifθ(X)]
+ 2 E[V∏∏φ'(W f (X ),p(X ))[V%fθ (X ),p(X) -P0(X ),p(X )-p0(X)]].
Furthermore, our operator norm assumption and Cauchy-Schwarz imply
∣bi(θ,p,γ)∣≤ ∣E[(p(X)-p0(X))>(qfθ,p(X)-γ(X))V%fθ(X)]| + ME[kV/fθ(X)∣2∣p(X)-P0(X)k2]
M2
≤ |E[(P(X)-p0(X)) (qfθ,p(X)-γ(X))vθifθ(X)]l+^2^l∣vθifθk2,2kp-p0ll2,4.
Thus, by the triangle inequality and Jensen’s inequality we find that
Ib(θ,P,γ)I2 ≤ llVθfθ>(γfθ,p-γ)>(P-P0)ll2,2 and
llb(θ,p,γ)k2 ≤ ∣∣vΘ f> (qfθ,p-γ)>(p -p0)∣l2,2 + ^2 llvθ fθ l∣F,2kp-p0k2,4.
□
Lemma 8 (Gradient Variance). Define For any parameter θ andfunctions P and γ,
q/EUIn(W∕,p,γ)∣∣2] ≤σ0(θ) + q∕E[∣∣vθfθ(X)>Y(X)>(Y — P0(X))∣∣2]+llvθf>(Yfθ,p- Y)>(P-P0)12,2.
18
Published as a conference paper at ICLR 2021
Proof For each i ∈ [d], define the shorthand
∆i = Vθi '(W fθ (X ),p(X ))+Vθifθ (X )>γ(X )>(Y-P(X ))-Vθi '(W f (X ),po(X)) and
Zi = E[∆i | X]
=Vθifθ(X)>(γ(X)-γfθ,p(X))>(p0(X)-p(X))
=Vθifθ(X)>(γ(X)-qfθ,p(X))>(p0(X)-p(X))
+ 2 E[V∏∏φ'(W fθ (X ),p(X ))[Vf (X ),po(X)- p(X ),p(X)- po(X)]]
for some convex combination P(X) of p(X) andpo(X).
We begin by bounding the target expectation using Cauchy-Schwarz
E[kn(W ；4p,Y)k2]
=X Var[Vθi '(W fθ (X ),P(X ))+V%fθ (X )>γ (X )>(Y-p(X))]
i∈[d]
=X Var[Vθi '(W fθ (X ),P0(X )) + △/
i∈[d]
=σo(θ,po)2 + X Var[∆i] + 2Cov(Vθi '(W fθ(X ),Po(X )),∆i)
i∈[d]
≤ σo(θ,po)2 + XVar[∆i] + 2PVar[Vθi'(Wfθ(X),Po(X))]Var[∆i]
i∈[d]
≤ σo(θ,po)2 + (XVar[∆i])+2 IXVar[Vθi'(Wfθ(X),Po(X))]XVar[∆i]
i∈[d]	i∈[d]	i∈[d]
=(σ0(θ,P0)+ IX Var[∆i])2∙
i∈[d]
We next employ the law of total variance to rewrite the variance terms:
XVar[∆i] = XVar[Zi+Vθifθ(X)>γ(X)>(Y-P0(X))]
i∈[d]
i∈[d]
E[∣∣Vθfθ(X)>γ(X)>(Y-P0(X))∣∣22]+XVar[Zi]∙
i∈[d]
Finally, we control Var[Zi] using Cauchy-Schwarz
JXVariZi≤ ∣∣vθf>(Yfθ,p-γ)>(P-PO)I∣2,2∙
i∈[d]
□
The two claims of Thm. 6 now follow from Theorems 2 and 3 of Ajalloeian & Stich (2020) respectively, with the parameters σ2
and ζ instantiated with quantities σ2 (γ) andζ(γ) of Lemmas 7 and 8.
I	Experiment Details and Additional Results
I.1 Tabular data
We use cross-fitting with 10 folds. The student is trained using the SEL loss with clipped teacher class probabilities max(p(x) ,e)
for = 10-3. The α hyperparameter of the loss correction was chosen by cross-validation with 5 folds. We repeat the experiments
5 times to measure the mean and standard deviation.
For the overfitting experiment, we use a random forest with 500 trees as the teacher and a random forest with 1-40 trees as the student.
We also evaluate the impact of teacher underfitting by limiting the teacher’s maximum tree depth (from 1 to 20). Lower depth
corresponds to greater underfitting. The teacher has 100 trees, and the student has 10 trees. For all of the datasets, loss correction
successfully mitigates the teacher’s underfitting and thus improves the student’s performance. The effect is most pronounced when
the teacher underfits more heavily (has lower tree depth).
19
Published as a conference paper at ICLR 2021
We show the full results for all 5 of the datasets in Figs. 4 and 5.
0.900-
0.875 ■
g 0.850
2 0.825 -
黄 O-BOO-
0.775
0.750
0.72
0.70
›
u
Ia
⅛ 0.68 -
IO66
芭
I- 0.64 ∙
0.62 ∙
123 5	10	15	20	30	40	123 5	10	15	20	30	40	123 5	10	15	20	30	40
Student's number of trees	Student's number of trees	Students number of trees
(a) Adult dataset	(b) FICO dataset	(c) Higgs dataset
0.94
0.92
123 5	10	15	20	30	40
Student's number of trees
(d)	MAGIC dataset
123 5	10	15	20	30	40
Students number of trees
(e)	StumbleUpon dataset
ADeJnUSVa"l
Tabular random forest distillation with varying student complexity.
Figure 4:
0⊃< ttωH
0.885
0.β80
1 2 3	5	10	15	20
Teacher's max tree depth
1 2 3	5	10	15	20
Teacher's max tree depth
A3e」n8±s"l
1 2 3	5	10	15	20
Teacher's max tree depth
(a)	Adult dataset
0.94
(b)	FICO dataset
(c)	Higgs dataset
0.90
o
< 0.β8
⅛
® 0.86
0.84
0.82
1 2 3	5	10	15	20
Teacher's max tree depth
0.830-
0.825-
0.820 -
0.815-
0.810-
0.805
0.800
Λ3e∙lnuu4"31
----KD Stixlent: 10 trees, max depth=S
----Cross-fit KD Student
---- Enhanced KD Student
----Teacher： 100 trees
1 2 3	5	10	15	20
Teacher's max tree depth
(d) MAGIC dataset
(e) StumbleUpon dataset
Figure 5: Tabular random forest distillation with varying teacher complexity.
I.2 Image data (CIFAR- 1 0)
We use SGD with initial learning rate 0.1, momentum 0.9, and batch size 128 to train for 200 epochs. We use the standard learning
rate decay schedule, where the learning rate is divided by 5 at epoch 60, 120, and 160. For loss correction, we select the value
of the hyperparameter α that yields the highest accuracy on a held-out validation set. For cross-fitting, we use 10 folds.
20