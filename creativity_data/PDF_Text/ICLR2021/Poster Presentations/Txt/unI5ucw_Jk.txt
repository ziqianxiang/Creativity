Published as a conference paper at ICLR 2021
Explaining by Imitating:
Understanding Decisions by
Interpretable Policy Learning
Alihan Huyuk*
University of Cambridge, UK
ah2075@cam.ac.uk
Daniel Jarrett*
University of Cambridge, UK
daniel.jarrett@maths.cam.ac.uk
Cem Tekin
Bilkent University, Ankara, Turkey
cemtekin@ee.bilkent.edu.tr
Mihaela van der Schaar
University of Cambridge, UK
Cambridge Centre for AI in Medicine, UK
The Alan Turing Institute, UK; UCLA, USA
mv472@cam.ac.uk
Ab stract
Understanding human behavior from observed data is critical for transparency and
accountability in decision-making. Consider real-world settings such as healthcare,
in which modeling a decision-maker’s policy is challenging—with no access to
underlying states, no knowledge of environment dynamics, and no allowance for
live experimentation. We desire learning a data-driven representation of decision-
making behavior that (1) inheres transparency by design, (2) accommodates partial
observability, and (3) operates completely offline. To satisfy these key criteria, we
propose a novel model-based Bayesian method for interpretable policy learning
(“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update
process together with their (possibly suboptimal) belief-action mapping. Through
experiments on both simulated and real-world data for the problem of Alzheimer’s
disease diagnosis, we illustrate the potential of our approach as an investigative de-
vice for auditing, quantifying, and understanding human decision-making behavior.
1	Introduction
A principal challenge in modeling human behavior is in obtaining a transparent understanding of
decision-making. In medical diagnosis, for instance, there is often significant regional and institutional
variation in clinical practice [1], much ofit the leading cause of rising healthcare costs [2]. The ability
to quantify different decision processes is the first step towards a more systematic understanding of
medical practice. Purely by observing demonstrated behavior, our principal objective is to answer the
question: Under any given state of affairs, what actions are (more/less) likely to be taken, and why?
We address this challenge by setting our sights on three key criteria. First, we desire a method that is
transparent by design. Specifically, a transparent description of behavior should locate the factors that
contribute to individual decisions, in a language readily understood by domain experts [3, 4]. This will
be clearer per our subsequent formalism, but we can already note some contrasts: Classical imitation
learning—popularly by reduction to supervised classification—does not fit the bill, since black-box
hidden states of RNNs are rarely amenable to meaningful interpretation. Similarly, apprenticeship
learning algorithms—popularly through inverse reinforcement learning—do not satisfy either, since
the high-level nature of reward mappings is not informative as to individual actions observed in the
data. Rather than focusing purely on replicating actions (imitation learning) or on matching expert
performance (apprenticeship learning), our chief pursuit lies in understanding demonstrated behavior.
Second, real-world environments such as healthcare are often partially observable in nature. This
requires modeling the accumulation of information from entire sequences of past observations—an
endeavor that is prima facie at odds with the goal of transparency. For instance, in a fully-observable
setting, (model-free) behavioral cloning is arguably ‘transparent’ in providing simple mappings of
states to actions; however, coping with partial observability using any form of recurrent function
* Authors contributed equally
1
Published as a conference paper at ICLR 2021
Table 1: Comparison with Related Work. INTERPOLE satisfies our key criteria of (1) transparency by design,
(2) partial observability, and (3) offline learning, and makes no assumptions w.r.t. unbiasedness of beliefs or
optimality of policies. Observations, beliefs, (optimal) q-values, actions, and policies are denoted z, b, q*, a, and
π; bold denotes learned quantities, italics are known (or queryable), and “f” denotes jointly-learned quantities.
Approach	Prototype	Overview	(1) (2) (3) Beliefs	Policy
gninraeL gninraeL
noitatimI pihsecitnerpp
BC	[5]	Z		n(a|Z)	„ a	X Y X		✓	(N/A)	No assumption
			(supervised learning)					
GAIL	[6]			π(alZ)	„ a	X Y X		X	(N/A)	No assumption
		Z Z	(informed by dynamics)					
MB-IL	[7]	Z		π(alZ)	> a	✓	X	✓	(N/A)	No assumption
		Z	(informed by environment model)					
IRL	[8]	Z	rewards, dynamics	X	X	X	(N/A)	Optimal
		Z	(reinforcement learning)	×-** (argmax) ×					
PO-IRL	[9]	Z . Z	dynamics	⅛ rewards, dynamics	X	✓	X	Unbiased	Optimal
			(inference) '`-× (reinforcement learning)	(argmax)					
Off. PO-IRL	[10]	Z . Z	dynamics 十 ⅛ rewards j dynamics1 -⅛	⅛	X	✓	✓	Unbiased	Optimal
			(inference) '`-× (reinforcement learning)	(argmax)					
(Ours)
decision	decision
z- dynamicsτ _π(αIb) ~ boundaries1	—χ
Z (inference)* b	* a
✓ ✓ ✓ No assumption No assumption
Interpole
approximation immediately lands in black-box territory. Likewise, while (model-based) methods have
been developed for robotic control, their transparency crucially hinges on fully-observable kinematics.
Finally, in realistic settings it is often impossible to experiment online—especially in high-stakes envi-
ronments with real products and patients. The vast majority of recent work in (inverse) reinforcement
learning has focused on games, simulations, and gym environments where access to live interaction
is unrestricted. By contrast, in healthcare settings the environment dynamics are neither known a
priori, nor estimable by repeated exploration. We want a data-driven representation of behavior that is
learnable in a completely offline fashion, yet does not rely on knowing/modeling any true dynamics.
Contributions Our contributions are three-fold. First, we propose a model for interpretable policy
learning (“Interpole”)—where sequential observations are aggregated through a decision agent’s
decision dynamics (viz. subjective belief-update process), and sequential actions are determined by
the agent’s decision boundaries (viz. probabilistic belief-action mapping). Second, we suggest a
Bayesian learning algorithm for estimating the model, simultaneously satisfying the key criteria of
transparency, partial observability, and offline learning. Third, through experiments on both simulated
and real-world data for Alzheimer’s disease diagnosis, we illustrate the potential of our method as an
investigative device for auditing, quantifying, and understanding human decision-making behavior.
2	Related Work
We seek to learn an interpretable parameterization of observed behavior to understand an agent’s
actions. Fundamentally, this contrasts with imitation learning (which seeks to best replicate dem-
onstrated policies) and apprenticeship learning (which seeks to match some notion of performance).
Imitation Learning In fully-observable settings, behavior cloning (BC) readily reduces the imitation
problem to one of supervised classification [5, 11-13]; i.e. actions are simply regressed on obser-
vations. While this can be extended to account for partial observability by parameterizing policies
via recurrent function approximation [14], it immediately gives up on ease of interpretability per the
black-box nature of RNN hidden states. A plethora of model-free techniques have recently been devel-
oped, which account for information in the rollout dynamics of the environment during policy learning
(see e.g. [15-20])—most famously, generative adversarial imitation learning (GAIL) based on state-
distribution matching [6, 21]. However, such methods require repeated online rollouts of intermediate
policies during training, and also face the same black-box problem as BC in partially observable set-
tings. Clearly in model-free imitation, itis difficult to admit both transparency and partial observability.
Specifically with an eye on explainability, Info-GAIL [22, 23] proposes an orthogonal notion of
“interpretability” that hinges on clustering similar demonstrations to explain variations in behavior.
However, as with GAIL it suffers from the need for live interaction for learning. Finally, several
model-based techniques for imitation learning (MB-IL) have been studied in the domain of robotics.
[24] consider kinematic models designed for robot dynamics, while [25] and [7] consider (non-)linear
autoregressive exogenous models. However, such approaches invariably operate in fully-observable
settings, and are restricted models hand-crafted for specific robotic applications under consideration.
2
Published as a conference paper at ICLR 2021
Apprenticeship Learning In subtle distinction to imitation learning, methods in apprenticeship
learning assume the observed behavior is optimal with respect to some underlying reward function.
Apprenticeship thus proceeds indirectly—often through inverse reinforcement learning (IRL) in order
to infer a reward function, which (with appropriate optimization) generates learned behavior that
matches the performance of the original—as measured by the rewards (See e.g. [8, 26-29]). These
approaches have been variously extended to cope with partial observability (PO-IRL) [9, 30], to
offline settings through off-policy evaluation [31-33], as well as to learned environment models [10].
However, a shortcoming of such methods is the requirement that the demonstrated policy in fact be
optimal with respect to a true reward function that lies within an (often limited) hypothesis class under
consideration—or is otherwise black-box in nature. Further, learning the true environment dynamics
[10] corresponds to the requirement that policies be restricted to the class of functions that map from
unbiased beliefs (cf. exact inference) into actions. Notably though, [34] considers both a form of
suboptimality caused by time-inconsistent agents as well as biased beliefs. However, perhaps most
importantly, due to the indirect, task-level nature of reward functions, inverse reinforcement learning
is essentially opposed to our central goal of transparency—that is, in providing direct, action-level
descriptions of behavior. In Section 5, we provide empirical evidence of this notion of interpretability.
Towards Interpole In contrast, we avoid making any assumptions as to either unbiasedness of
beliefs or optimality of policies. After all, the former requires estimating (externally) “true” envi-
ronment dynamics, and the latter requires specifying (objectively) “true” classes of reward functions—
neither of which are necessary per our goal of transparently describing individual actions. Instead,
INTERPOLE simply seeks the most plausible explanation in terms of (internal) decision dynamics and
(subjective) decision boundaries. To the best of our knowledge, our work is the first to tackle all three
key criteria—while making no assumptions on the generative process behind behaviors. Table 1 con-
textualizes our work, showing typical incarnations of related approaches and their graphical models.
Before continuing, we note that the separation between the internal dynamics of an agent and the
external dynamics of the environment has been considered in several other works, though often for
entirely different problem formulations. Most notably, [35] tackles the same policy learning problem
as we do in online, fully-observable environments but for agent’s with internal states that cannot be
observed. They propose agent Markov models (AMMs) to model such environment-agent interactions.
For problems other than policy learning, [36-38] also consider the subproblem of inferring an agent’s
internal dynamics; however, none of these works satisfy all three key criteria simultaneously as we do.
3	Interpretable Policy Learning
We first introduce Interpole’s model of behavior, formalizing notions of decision dynamics and de-
cision boundaries. In the next section, we suggest a Bayesian algorithm for model-learning from data.
Problem Setup Consider a partially-observable decision-making environment in discrete time. At
each step t, the agent takes action at ∈ A and observes outcome zt ∈ Z.1 We have at our disposal an
observed dataset of demonstrations D={(ai1, z1i , . . . , aiτi, zτii)}in=1 by an agent, τi being the length
of the i-th trajectory (we shall omit indices i unless required). Denote by ht =. (a1 , z1, . . . , at-1, zt-1)
the observed history at the beginning of step t, where hi = 0. Analogously, let Ht = (A X Z)t-1
indicate the set of all possible histories at the start of step t, where Hi = {0}, and let H = ∪∞=ιHt.
A proper policy π is a mapping π ∈ ∆(A)H from observed histories to action distributions, where
π(a∣h) is the probability of taking action a given h. We assume that D is generated by an agent
acting according to some behavioral policy πb. The problem we wish to tackle, then, is precisely
how to obtain an interpretable parameterization of πb. We proceed in two steps: First, we describe a
parsimonious belief-update process for accumulating histories—which we term decision dynamics.
Then, we take beliefs to actions via a probabilistic mapping—which gives rise to decision boundaries.
Decision Dynamics We model belief-updates by way of an input-output hidden Markov model
(IOHMM) identified by the tuple (S, A, Z, T, O, bi), with S being the finite set of underlying states.
T ∈ ∆(S)S×A denotes the transition function such that T (st+i |st, at) gives the probability of
transitioning into state st+i upon action at in state st, and O ∈ ∆(Z)A×S denotes the observation
function such that O(zt|at, st+i) gives the probability of observing zt after taking action at and
transitioning into state st+i. Finally, let beliefs bt ∈ ∆(S) indicate the probability bt(s) that the
1While we take it here that Z is finite, our method can easily be generalized to allow continuous observations.
3
Published as a conference paper at ICLR 2021
environment exists in any state s ∈ S at time t, and let b1 give the initial state distribution. Note
that—unlike in existing uses of the IOHMM formalism—these “probabilities” are for representing
the thought process of the human, and may freely diverge from the actual mechanics of the world. To
aggregate observed histories as beliefs, we identify bt(s) with P(st = s|ht)—an interpretation that
leads to the recursive belief-update process (where in our problem, quantities T, O, b1 are unknown):
bt+ι(s0) H Ps∈s bt(s)T(s0∣s, at)O(zt∣at, s0)	(1)
A key distinction bears emphasis: We do not require that this latter set of quantities correspond
to (external) environment dynamics—and we do not obligate ourselves to recover any such notion
of “true” parameters. To do so would imply the assumption that the agent in fact performs exactly
unbiased inference on a perfectly known model of the environment, which is restrictive. It is also
unnecessary, since our mandate is simply to model the (internal) mechanics of decision-making—
which could well be generated from possibly biased beliefs or imperfectly known models of the world.
In other words, our objective (see Equation 3) of simultaneously determining the most likely beliefs
(cf. decision dynamics) and policies (cf. decision boundaries) is fundamentally more parsimonious.
Decision Boundaries Given decision dynamics,
a policy is then equivalently a map π ∈ ∆(A)∆(S).
Now, what is an interpretable parameterization?
Consider the three-state example in Figure 1. We
argue that a probabilistic parameterization that di-
rectly induces “decision regions” (cf. panel 1b)
over the belief simplex is uniquely interpretable.
For instance, strong beliefs that a patient has under-
lying mild cognitive impairment may map to the
region where a specific follow-up test is promptly
prescribed; this parameterization allows clearly
locating such regions—as well as their boundaries.
Figure 1: The INTERPOLE Model. Here, S= {K, L,
M} and A={1, 2, 3, 4}. (a) Beliefs are updated rec-
ursively (Equation 1). (b) Actions are chosen with res-
pect to relative locations of mean vectors (Equation 2).
Precisely, we parameterize policies in terms of |A|-many “mean” vectors that correspond to actions:
π(a∣b) = e-ηkb-μa k2/ Pao∈a e-ηkb-μa0 k2 , J 〃a(s) = 1	(2)
where η ≥ 0 is the inverse temperature, k」the '2-norm, and 仙。∈ R|S| the mean vector CorresPond-
ing to action a ∈ A. Intuitively, mean vectors induce decision boundaries (and decision regions) over
the belief space ∆(S): At any time, the action whose corresponding mean is closest to the current
belief is most likely to be chosen. In particular, lines that are equidistant to the means of any pair
of actions form decision boundaries between them. The inverse temperature controls the transitions
between such boundaries: A larger η captures more deterministic behavior (i.e. more “abrupt” transi-
tions), whereas a smaller η captures more stochastic behavior (i.e. “smoother” transitions). Note that
the case of η = 0 recovers policies that are uniformly random, and η → ∞ recovers argmax policies.
A second distinction is due: The exponentiated form of Equation 2 should not be confused with typical
Boltzmann [27] or MaxEnt [39] policies common in RL: These are indirect parameterizations via op-
timal/soft q-values, which themselves require approximate solutions to optimization problems; as we
shall see in our experiments, the quality of learned policies suffers as a result. Further, using q-values
would imply the assumption that the agent in fact behaves optimally w.r.t. an (objectively) “true” class
of reward functions—e.g. linear—which is restrictive. It is also unnecessary, as our mandate is simply
to capture their (subjective) tendencies toward different actions—which are generated from possibly
suboptimal policies. In contrast, by directly partitioning the belief simplex into probabilistic “decision
regions”, Interpole’s mean-vector representation can be immediately explained and understood.
Learning Objective In a nutshell, our objective
is to identify the most likely parameterizations T,
O, bi for decision dynamics as well as η, {μa}a∈A
for decision boundaries, given the observed data:
Given: D, S, A, Z	(3)
Determine: T, O,bι,η, {μa}a∈A	()
Next, we propose a Bayesian algorithm that finds
the maximum a posteriori (MAP) estimate of these
quantities. Figure 2 illustrates the problem setup.
Decision Dynamics	Decision Boundaries
Figure 2: The INTERPOLE Objective. Inputs (demon-
stration data) are fed into Interpole through dashed
lines, and outputs (estimates) are issued in bold lines.
(Beliefs bt can then be computed via a forward pass).
4
Published as a conference paper at ICLR 2021
4 Bayesian Interpretable Policy Learning
Denote with θ = (T, O, bi, η, {μa}a∈A) the set of parameters to be determined, and let θ be drawn
from some prior P(θ). In addition, denote with DD = {⑸，...，ST+ι)}n=ι the set of underlying
(unobserved) state trajectories, such that D∪D gives the complete (fully-observed) dataset. Then the
complete likelihood—of the unknown parameters θ with respect to D∪ DD—is given by the following:
nτ
n
τ
P(D, D∣θ)= πππ(at∣bt[T,O, bi, ht-i]) × ɪɪ bι(sι) ɪɪ T(st+ι∣st, at)O(zt∣at, St+i)
(4)
i=1 t=1
{^^^^^^^^β
action likelihoods
i=i
} <^_
t=i
{^^^^^^^^^^^
observation likelihoods
where ∏(∙∣∙) is described by η and {μa}a∈A1 and each bt[∙] is a function of T, O, bi, and ht-i
(Equation 1). Since we do not have access to DD, we propose an expectation-maximization (EM)-like
algorithm for maximizing the posterior P(θ∣D) = P(D∣θ)P(θ)/ R P(P∣θ)dP(θ) overthe parameters:
Bayesian Learning Given an initial estimate
θ0, we iteratively improve the estimate by perf-
orming the following steps at each iteration k:
•	“E-step”: Compute the expected log-likeli-
hood of the model parameters θ given the pre-
vious parameter estimate θk-i, as follows:
Q(θ∕-i)= Ed∣d,θ- [logP(D,D ∣θ)]
=PD logP(D,D∣θ)P(D∣D,θkT) ,
where we compute the necessary marginal-
izations of joint distribution P(D∣D,θk-i)
by way of a forward-backward procedure
(detailed procedure given in Appendix A.1).
•	“M-step”: Compute a new
Algorithm 1 Bayesian INTERPOLE
1: Parameters: learning rate w ∈ R+
2: Input: dataset D = {hiτi+i}in=i, prior P(θ)
3: Sample θ0 from P(θ)
4: for k = 1, 2, . . . do
5: Compute P(D∣D,θk-i)	.Appendix A.1
6: Compute %QMθk-i) at θk-i .Appendix A.2
7: θk — θk-i
+ w[VθQ(θ; θk-i) + Vθ log P(θ)]θ=θk-ι
8: while (6)
9: θ  θk-i
10: Output: MAP estim. θ = (T, O, bi,^, {μa}a∈A)
estimate θk that improves the expected log-posterior-that is, such that:
Q(θk; θk-i) + logP(θk) > Q(θk-i; θk-i) + logP(θk-i)	(6)
subject to appropriate non-negativity and normalization constraints on parameters, which can be
achieved via gradient-based methods (detailed procedure given in Appendix A.2). We stop when it
becomes no longer possible to obtain a new estimate further improving the expected log-posterior
—that is, when the “M-step” cannot be performed. Algorithm 1 summarizes this learning procedure.
Explaining by Imitating Recall our original mandate—to give the most plausible explanation for
behavior. Two questions can be asked about our proposal to “explain by imitating”—to which we now
have precise answers: One concerns explainability, and the other concerns directness of explanations.
First, what constitutes the “most plausible explanation” of behavior? Now, Interpole identifies
this as the most likely parameterization of that behavior using a state-based model for beliefs and
policies—but otherwise with no further assumptions. In particular, we are only positing that modeling
beliefs over states helps provide an interpretable description of how an agent reasons (which we do
have ample evidence for2)—but we are not assuming that the environment itself takes the form of a
state-based model (which is an entirely different claim). Mathematically, the complete likelihood
(Equation 4) highlights the difference between decision dynamics (which help explain the agent’s
behavior) and “true” environment dynamics (which we do not care about). The latter are independent
of the agent, and learning them would have involved just the observation likelihoods alone. In
contrast, by jointly estimating T, O, bi with η, {μa}a∈A according to both the observation- and
action-likelihoods, we are learning the decision dynamics—which in general need not coincide with
the environment, but which offer the most plausible explanation of how the agent effectively reasons.
The second question is about directness: Given the popularity of the IRL paradigm, could we have sim-
ply used an (indirect) reward parameterization, instead of our (direct) mean-vector parameterization?
As it turns out, in addition to the “immediate” interpretability of direct, action-level representations,
2In healthcare, diseases are often modeled in terms of states, and beliefs over disease states are eminently
transparent factors that medical practitioners (i.e. domain experts) readily comprehend and reason about [40, 41].
5
Published as a conference paper at ICLR 2021
it comes with an extra perk w.r.t. computability: While it is (mathematically) possible to formulate a
similar learning problem swapping out μ for rewards, in practice it is (computationally) intractable to
perform in our setting. The precise difficulty lies in differentiating through quantities π(at |bt)—which
in turn depend on beliefs and dynamics—in the action-likelihoods (proofs located in Appendix B):
Proposition 1 (Differentiability with q-Parameterizations) Consider softmax
by q-values from a reward function, such that π(a∣b) = eq*(b,a)/ P。，eq*(b,a
policies parameterized
) in lieu of Equation 2.
Then differentiating through logπ(at∣bt) terms with respect to unknown parameters θ is intractable.
In contrast, Interpole avoids ever needing to solve any “forward” problem at all (and therefore
does not require resorting to costly—and approximate—sampling-based workarounds) for learning:
Proposition 2 (Differentiability with μ-Parameterizations) Consider the mean-vector policy parame-
terization proposed in Equation 2. Differentiation through the log π(at |bt) terms with respect to the
unknown parameters θ is easily and automatically performed using backpropagation through time.
5 Illustrative Examples
Three aspects of Interpole deserve empirical demonstration, and we shall highlight them in turn:
•	Interpretability: First, we illustrate the usefulness of our method in providing transparent
explanations of behavior. This is our primary objective here--of explaining by imitating.
•	Accuracy: Second, we demonstrate that the faithfulness of learned policies is not given up
for transparency. This shows that accuracy and interpretability are not necessarily opposed.
•	Subjectivity: Third, we show INTERPOLE correctly recovers underlying explanations for
behavior——even if the agent is biased. This sets us apart from other state-based algorithms.
In order to do so, we show archetypical examples to exercise our framework, using both simulated
and real-world experiments in the context of disease diagnosis. State-based reasoning is prevalent
in research and practice: three states in progressive clinical dementia [42, 43], preterminal cancer
screening [44, 45], or even—as recently shown—for cystic fibrosis [46] and pulmonary disease [47].
Decision Environments For our real-world setting, we consider the diagnostic patterns for 1,737
patients during sequences of 6-monthly visits in the Alzheimer’s Disease Neuroimaging Initiative [48]
database (ADNI). The state space consists of normal functioning (“NL”), mild cognitive impairment
(“MCI”), and dementia. For the action space, we consider the decision problem of ordering vs.
not ordering an MRI test, which—while often informative of Alzheimer’s—is financially costly.
MRI outcomes are categorized according to hippocampal volume: {“avg”, “above avg”, “below
avg”, “not ordered”}; separately, the cognitive dementia rating-sum of boxes (“CDR-SB”) result—
which is always measured—is categorized as: {“normal”, “questionable impairment”, “mild/severe
dementia”} [42]. In total, the observation space therefore consists of the 12 combinations of outcomes.
We also consider a simulated setting to better validate performance. For this we employ a diagnostic
environment (DIAG) in the form of an IOHMM with certain (true) parameters Ttrue, Otrue , bt1rue .
Patients fall within diseased (s+) and healthy (s-) states, and vital-sign measurements available
at every step are classified within positive (z+) and negative (z-) outcomes. For the action space,
we consider the decision of continuing to monitor a patient (a=), or stopping and declaring a final
diagnosis—and if so, a diseased (a+) or healthy (a-) declaration. If we assume agents have perfect
knowledge of the true environment, then this setup is similar to the classic “tiger problem” for optimal
stopping [49]. Lastly, we also consider a (more realistic) variant of DIAG where the agent’s behavior
is instead generated by biased beliefs due to incorrect knowledge T, O, b1 6= T true , Otrue , bt1rue of
the environment (BIAS). Importantly, this generates a testable version of real-life settings where
decision-makers’ (subjective) beliefs often fail to coincide with (objective) probabilities in the world.
Benchmark Algorithms Where appropriate, we compare INTERPOLE against the following bench-
marks: imitation by behavioral cloning [5] using RNNs for partial observability (R-BC); Bayesian
IRL on POMDPs [9] equipped with a learned environment model (PO-IRL); a fully-offline coun-
terpart [10] of Bayesian IRL (Off. PO-IRL); and an adaptation of model-based imitation learning
[7] to partially-observable settings, with a learned IOHMM as the model (PO-MB-IL). Algorithms
requiring learned models for interaction are given IOHMMs estimated using conventional methods
[50]. Further information on environments and benchmark implementations is found in Appendix C.
6
Published as a conference paper at ICLR 2021
□ An MRIis more likely to be ordered. ∙ An MRIis ordered.	♦ Final beliefs △ Belief simplex
I I An MRIis less likely to be ordered. A An MRIis not ordered. → Belief updates --- Decision boundary
Figure 3: Decision Trajectories. Examples of real patients, including: (a) A typical normally-functioning patient,
where the decision-maker’s beliefs remain mostly on the decision boundary. (b) A typical patient who is believed
to be deteriorating towards dementia. (c) A patient who—apparently—could have been diagnosed much earlier
than they actually were. (d) A patient with a (seemingly redundant) MRI test that is actually highly informative.
Interpretability First, we direct attention to the potential utility of INTERPOLE as an investigative
device for auditing and quantifying individual decisions. Specifically, modeling the evolution of an
agent’s beliefs provides a concrete basis for analyzing the corresponding sequence of actions taken:
•	Explaining Trajectories. Figure 3 shows examples of such decision trajectories for four real ADNI
patients. Each vertex of the belief simplex corresponds to one of the three stable diagnoses, and
each point in the simplex corresponds to a unique belief (i.e. probability distribution). The closer
the point is to a vertex (i.e. state), the higher the probability assigned to that state. For instance, if
the belief is located exactly in the middle of the simplex (i.e. equidistant from all vertices), then all
states are believed to be equally likely. If the belief is located exactly on a vertex (e.g. directly on
top of MCI), then this corresponds to an absolutely certainty of MCI being the underlying state.
Patients (a) and (b) are “typical” patients who fit well to the overall learned policy. The former is a
normally-functioning patient believed to remain around the decision boundary in all visits except
the first; appropriately, they are ordered an MRI during approximately half of their visits. The latter
is believed to be deteriorating from MCI towards dementia, hence prescribed an MRI in all visits.
•	Identifying Belated Diagnoses. In many diseases, early diagnosis is paramount [51]. INTERPOLE
allows detecting patients who appear to have been diagnosed significantly later than they should
have. Patient (c) was ordered an MRI in neither of their first two visits—despite the fact that the
“typical” policy would have strongly recommended one. At a third visit, the MRI that was finally
ordered led to near-certainty of cognitive impairment—but this could have been known 12 months
earlier! In fact, among all ADNI patients in the database, 6.5% were subject to this apparent pattern
of “belatedness”, where a late MRI is immediately followed by a jump to near-certain deterioration.
•	Quantifying Value of Information. Patient (d) highlights how INTERPOLE can be used to quantify
the value of a test in terms of its information gain. While the patient was ordered an MRI in all
of their visits, it may appear (on the surface) that the third and final MRIs were redundant—since
they had little apparent affect on beliefs. However, this is only true for the factual belief update
that occurred according to the MRI outcome that was actually observed. Having access to an
estimated model of how beliefs are updated in the form of decision dynamics, we can also compute
counterfactual belief updates—that is belief updates that could have occurred if the MRI outcome
in question were to be different. In the particular case of patient (d), the tests were in fact highly
informative, since (as it happened) the patient’s CDR-SB scores were suggestive of impairment, and
(in the counterfactual) the doctor’s beliefs could have potentially leapt drastically towards MCI. On
the other hand, among all MRIs ordered for ADNI patients, 19% may indeed have been unnecessary
(i.e. triggering apparently insignificant belief-updates both factually as well as counterfactually).
Evaluating Interpretability through Clinician Surveys
To cement the argument for interpretability, We evaluated INTERPOLE by consulting nine clini-
cians from four different countries (United States, United Kingdom, the Netherlands, and China)
for feedback. We focused on evaluating two aspects of interpretability regarding our method:
7
Published as a conference paper at ICLR 2021
•	Decision Dynamics: Whether the proposed representation of (possibly subjective) belief
trajectories are preferable to raw action-observation trajectories—that is, whether decision
dynamics are a transparent way of modeling how information is aggregated by decision-makers.
•	Decision Boundaries: Whether the proposed representation of (possibly suboptimal) decision
boudnaries are a more transparent way of describing policies, compared with the representation
of reward functions (which is the conventional approach in the policy learning literature).
For the first aspect, we presented to the participating clinicians the medical history of an example
patient from ADNI represented in three ways using: only the most recent action-observation, the
complete action-observation trajectory, as well as the belief trajectory as recovered by Interpole.
Result: All nine clinicians preferred the belief trajectories over action-observation trajectories.
For the second aspect, we showed them the policies learned from ADNI by both Off. PO-IRL and
Interpole, which parameterize policies in terms of reward functions and decision boundaries
respectively. Result: Seven out of nine clinicians preferred the representation in terms of decision
boundaries over that offered by reward functions. Further details can be found in Appendix D.
Accuracy Now, a reasonable question is whether
such explainability comes at a cost: By learning an
interpretable policy, do we sacrifice any accuracy?
To be precise, we can ask the following questions:
•	Is the belief-update process the same? For this,
the appropriate metric is the discrepancy with
respect to the sequence of beliefs—which we
take to be Pt DκL(bt∣∣bt) (Belief Mismatch).
•	Is the belief-action mapping the same? Our
metric is the discrepancy with respect to the
policy distribution itself—which we take to be
PtDkl(∏b(∙∣bt)k∏(∙∣bt)) (Policy Mismatch).
Table 2: Performance Comparison in ADNI. INTER- pole is best/second-best for action-matching metrics.			
Algorithm	Calibration (Brier Score)	Area under ROC Curve	Area under PR Curve
R-BC	0.18 ± 0.05	0.61 ± 0.05	0.81 ± 0.08
pO-MB-IL	0.19 ± 0.07	0.54 ± 0.07	0.79 ± 0.11
pO-IRL	0.23 ± 0.01	0.51 ± 0.07	0.78 ± 0.09
Off. pO-IRL	0.24 ± 0.01	0.54 ± 0.05	0.79 ± 0.09
Interpole	0.17 ± 0.05	0.60 ± 0.04	0.81 ± 0.09
•	Is the effective behavior the same? Here, the
metrics are those measuring the discrepancy
with respect to ground-truth actions observed
(Action-Matching) for ADNI, and differences
in stopping (Stopping Time Error) for DIAG.
Table 3: Performance Comparison in DIAG. INTER- POLE is best. Belief mismatch is n/ato R-BCj ×10-3			
Algorithm	Belief MiSmatCht	Policy Mismatcht	Stopping Time Error
R-BC	-	6.7± 1.4	5.91 ± 1.29
PO-MB-IL	42.7 ± 34.0	47.9 ± 18.9	6.34 ± 1.48
PO-IRL	42.7 ± 34.0	62.1 ± 29.1	6.41 ± 1.80
Off. PO-IRL	26.1 ± 5.6	2.1 ± 0.2	5.42 ± 0.69
Note that the action-matching and stopping time	Interpole	0.6± 0.1	0.8± 0.2 5.38 ± 1.14
errors evaluate the quality of learned models in
imitating per se, whereas belief mismatch and policy mismatch evaluate their quality in explaining.3
The results are revealing, if not necessarily surprising. To begin, we observe for the ADNI setting
in Table 2 that Interpole performs first- or second-best across all three action-matching based
metrics; where it comes second, it does so only by a small margin to R-BC (bearing in mind that
R-BC is specifically optimized for nothing but action-matching). Similarly for the DIAG setting, we
observe in Table 3 that Interpole performs the best in terms of stopping-time error. In other words,
it appears that little—if any—imitation accuracy is lost by using Interpole as the model.
perhaps more interestingly, we also see in Table 3 that the quality of internal explanations is superior—
in terms of both belief mismatch and policy mismatch. In particular, even though the comparators
pO-MB-IL, pO-IRL, and Off. pO-IRL are able to map decisions through beliefs, they inherit the
conventional approach of attempting to estimate true environment dynamics, which is unnecessary—
and possibly detrimental—if the goal is simply to find the most likely explanation of behavior.
Notably, while the difference in imitation quality among the various benchmarks is not tremendous,
with respect to explanation quality the gap is significant—where Interpole has great advantage.
Subjectivity Most significantly, we now show that INTERpOLE correctly recovers the underlying
explanations, even if—or perhaps especially if—the agent is driven by subjective reasoning (i.e. with
biased beliefs). This aspect sets Interpole firmly apart from the alternative state-based techniques.
3Belief/policy mismatch are not applicable to ADNI since we have no access to ground-truth beliefs/policies.
8
Published as a conference paper at ICLR 2021
Consider the BIAS environment: Here the true
environment dynamics are unchanged from DIAG,
but no longer coincide with the agent’s decision
dynamics. Specifically, we let the behavioral
policy be generated using erroneous parameters
O(z- |a=, s+) < Otrue (z- |a=, s+); that is, the
doctor now incorrectly believes the test to have a
smaller false-negative rate than it does in reality
—thus biasing their beliefs regarding patient states.
Now suppose we wish to recover the decision
boundary from the demonstrations—that is, at
what confidence threshold does a doctor commit
to a healthy diagnosis? Figure 4 shows that both
Interpole and PO-MB-IL appear to recover the
correct effective behavior: Starting from a neutral
prior, doctors tend to stop and issue a “healthy”
diagnosis if the first two observations return neg-
ative signals. Importantly, Interpole also cor-
rectly locates the confidence target 〜90%. On the
other hand, PO-MB-IL—which first attempts to
estimate the environment’s true parameters—ends
up learning a policy on the basis of miscalibrated
beliefs, thereby incorrectly explaining the same
effective behavior with a lower confidence target
〜70%. Finally, through similarly benchmarked
metrics, Table 4 further confirms Interpole’s
advantage in providing the (correct) explanations.
6 Discussion
ɪ Ground-Truth ʌ PO-MB-ILjβ∙ Interpole
b(s- ) Decision Boundaries
(as in Fig. 1b)
Figure 4: Explaining Subjective Behavior. Markers
show the evolution of beliefs that explain ground-truth
and learned policies in BIAS, for an example sce-
nario where two consecutive negative (z- ) observa-
tions are made. While all policies display similar ef-
fective behavior, only Interpole correctly identifies
the ground-truth decision boundary. This underscores
the significance of distinguishing between decision
dynamics (which help explain an agent’s behavior)
and “true” dynamics (which we do not care about).
Table 4: Performance Comparison in BIAS. INTER-
POLE is best. Belief mismatch is n/a to R-BC. *× 10-3
Algorithm	Belief MiSmatcht	Policy MiSmatcht	Stopping Time Error
R-BC	-	191.8 ± 78.8	2.22 ± 0.45
PO-MB-IL	83.2 ± 3.03	125.4 ± 18.4	3.80 ± 0.19
PO-IRL	83.2 ± 3.03	144.0 ± 29.4	3.46 ± 0.22
Off. PO-IRL	84.9 ± 8.28	62.8 ± 7.5	3.12 ± 0.41
Interpole	3.8 ± 0.34	1.8 ± 0.6	2.14 ± 0.51
Three points deserve brief comment in closing.
First, while We gave prominence to several exam-
ples of how Interpole may be used to audit and improve decision-making behavior, the potential
applications are not limited to these use cases. Broadly, the chief proposition is that having quantita-
tive descriptions of belief trajectories provides a concrete language that enables investigating observed
actions, including outliers and variations in behavior: On that basis, different statistics and post-hoc
analyses can be performed on top of Interpole’s explanations (see Appendix C for examples).
Second, a reasonable question is whether or not it is reasonable to assume access to the state space. For
this, allow us to reiterate a subtle distinction. There may be some “ground-truth” external state space
that is arbitrarily complex or even impossible to discover, but—as explained—we are not interested
in modeling this. Then, there is the internal state space that an agent uses to reason about decisions,
which is what we are interested in. In this sense, it is certainly reasonable to assume access to the
state space, which is often very clear from medical literature [42-47]. Since our goal is to obtain inter-
pretable representations of decision, it is therefore reasonable to cater precisely to these accepted state
spaces that doctors can most readily reason with. Describing behavior in terms of beliefs over these
(already well-understood) states is one of the main contributors to the interpretability of our method.
Finally, it is crucial to keep in mind that INTERPOLE does not claim to identify the real intentions of
an agent: humans are complex, and rationality is—of course—bounded. What it does do, is to provide
an interpretable explanation of how an agent is effectively behaving, which-as we have seen for
diagnosis of ADNI patients--offers a yardstick by which to assess and compare trajectories and
subgroups. In particular, Interpole achieves this while adhering to our key criteria for healthcare
settings, and without imposing assumptions of unbiasedness or optimality on behavioral policies.
Acknowledgments
This work was supported by the US Office of Naval Research (ONR) and Alzheimer’s Research UK
(ARUK). We thank the clinicians who participated in our survey, the reviewers for their valuable
feedback, and the Alzheimer’s Disease Neuroimaging Initiative for providing the ADNI dataset.
9
Published as a conference paper at ICLR 2021
References
[1]	J. W. O’Sullivan, C. Heneghan, R. Perera, J. Oke, J. K. Aronson, B. Shine, and B. Goldacre,
“Variation in diagnostic test requests and outcomes: a preliminary metric for OpenPathology.net,”
Sci. Reports, vol. 8, no.1, pp. 1-6, 2018.
[2]	M. Allen, “Unnecessary tests and treatment explain why health care costs so much,” Sci. Amer.,
2017.
[3]	A. Holzinger, C. Biemann, C. S. Pattichis, and D. B. Constantinos, “What do we need to build
explainable AI systems for the medical domain?” arXiv preprint arXiv:1712.09923, 2017.
[4]	G. Montavon, W. Samek, and K.-R. Muller, “Methods for interpreting and understanding deep
neural networks,” Digit. Signal Process., vol. 73, pp. 1-15, 2018.
[5]	B. Piot, M. Geist, and O. Pietquin, “Boosted and reward-regularized classification for ap-
prenticeship learning,” in Proc. 13th Int. Conf. Auton. Agents and Multiagent Syst., 2014, pp.
1249-1256.
[6]	W. Jeon, S. Seo, and K.-E. Kim, “A Bayesian approach to generative adversarial imitation
learning,” in Adv. Neural Inf. Process. Syst. 32, 2018.
[7]	P. Englert, A. Paraschos, J. Peters, and M. P. Deisenroth, “Probabilistic model-based imitation
learning,” Adaptive Behav., vol. 21, pp. 388-403, 2013.
[8]	J. Choi and K.-E. Kim, “MAP inference for Bayesian inverse reinforcement learning,” in Adv.
Neural Inf. Process. Syst. 24, 2011, pp. 1989-1997.
[9]	D. Jarrett and M. van der Schaar, “Inverse active sensing: modeling and understanding timely
decision-making,” in Proc. 37th Int. Conf. Mach. Learn., 2020.
[10]	T. Makino and J. Takeuchi, “Apprenticeship learning for model parameters of partially observ-
able environments,” in Proc. 29th Int. Conf. Mach. Learn., 2012, pp. 891-898.
[11]	M. Bain and C. Sammut, “A framework for behavioural cloning,” Mach. Intell., vol. 15, pp.
103-129, 1996.
[12]	S. Ross and D. Bagnell, “Efficient reductions for imitation learning,” in Proc. 13th Int. Conf.
Artif. Intell. Statist., 2010, pp. 661-668.
[13]	D. Jarrett, I. Bica, and M. van der Schaar, “Strictly batch imitation learning by energy-based
distribution matching,” arXiv preprint arXiv:2006.14154, 2020.
[14]	W. Sun, A. Venkatraman, G. J. Gordon, B. Boots, and J. A. Bagnell, “Deeply AggreVaTeD:
differentiable imitation learning for sequential prediction,” in Proc. 34th Int. Conf. Mach. Learn.,
2017.
[15]	L. Blonde and A. Kalousis, “Sample-efficient imitation learning via generative adversarial nets,”
in Proc. 22nd Int. Conf. Artif. Intell. Statist., 2019.
[16]	I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson, “Discriminator-actor-critic:
addressing sample inefficiency and reward bias in adversarial imitation learning,” in Proc. 7th
Int. Conf. Learn. Representations, 2019.
[17]	A. H. Qureshi, B. Boots, and M. C. Yip, “Adversarial imitation via variational inverse reinforce-
ment learning,” in Proc. 7th Int. Conf. Learn. Representations, 2019.
[18]	F. Sasaki, T. Yohira, and A. Kawaguchi, “Sample efficient imitation learning for continuous
control,” in Proc. 7th Int. Conf. Learn. Representations, 2019.
[19]	K. Brantley, W. Sun, and M. Henaff, “Diagreement-regularized imitation learning,” in Proc. 8th
Int. Conf. Learn. Representations, 2020.
[20]	S. Reddy, A. D. Dragan, and S. Levine, “SQIL: imitation learning via regularized behavioral
cloning,” in Proc. 8th Int. Conf. Learn. Represantations, 2020.
10
Published as a conference paper at ICLR 2021
[21]	J. Ho and S. Ermon, “Generative adversarial imitation learning,” in Adv. Neural Inf. Process.
Syst 29, 2016,pp. 4565-4573.
[22]	Y. Li, J. Song, and S. Ermon, “InfoGAIL: interpretable imitation learning from visual demon-
strations,” in Adv. Neural Inf. Process. Syst. 30, 2017, pp. 3812-3822.
[23]	M. Sharma, A. Sharma, N. Rhinehart, and K. M. Kitani, “Directed-Info GAIL: learning
hierarchical policies from unsegmented demonstrations using directed information,” in Proc.
7th Int. Conf. Learn. Representations, 2019.
[24]	A. Ude, C. G. Atkeson, and M. Riley, “Programming full-body movements for humanoid robots
by observation,” Robot. Auton. Syst., vol. 47, no. 2-3, pp. 93-108, 2004.
[25]	J. van den Berg, S. Miller, D. Duckworth, H. Hu, A. Wan, X. Fu, K. Goldberg, and P. Abbeel,
“Superhuman performance of surgical tasks by robots using iterative learning from human-guided
demonstrations,” in Proc. IEEE Int. Conf. Robot. Automat., 2010, pp. 2074-2081.
[26]	P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in Proc.
21st Int. Conf. Mach. Learn., 2004.
[27]	D. Ramachandran and E. Amir, “Bayesian inverse reinforcement learning,” in Proc. 20th Int.
Joint Conf. Artif. Intell., 2007, pp. 2586-2591.
[28]	B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy inverse reinforcement
learning,” in Proc. 23rd AAAI Conf. Artif. Intell., 2008, pp. 1433-1438.
[29]	J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adversarial inverse reinforcement
learning,” in Proc. 6th Int. Conf. Learn. Representations, 2018.
[30]	J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially observable environments,” J.
Mach. Learn. Res., vol. 12, pp. 691-730, 2011.
[31]	E. Klein, M. Geist, B. Piot, and O. Pietquin, “Inverse reinforcement learning through structured
classification,” in Adv. in Neural Inf. Process. Syst. 25, 2012, pp. 1007-1015.
[32]	A. C. Y. Tossou and C. Dimitrakakis, “Probabilistic inverse reinforcement learning in unknown
environments,” in Proc. 29th Conf. Uncertainty Artif. Intell., 2013, p. 635-643.
[33]	V. Jain, P. Doshi, and B. Banerjee, “Model-free IRL using maximum likelihood estimation,” in
Proc. 33rd AAAI Conf. Artif. Intell., 2019, pp. 3951-3958.
[34]	O. Evans, A. Stuhlmuller, and N. D. Goodman, “Learning the preferences of ignorant, inconsis-
tent agents,” in Proc. 13th AAAI Conf. Artif. Intell., 2016, pp. 323-329.
[35]	V. V. Unhelkar and J. A. Shah, “Learning models of sequantial decision-making with partial
specification of agent behavior,” in Proc. 33rd AAAI Conf. Artif. Intell., 2019, pp. 2522-2530.
[36]	F. Schmitt, H. J. Bieg, M. Herman, and C. A. Rothkopf, “I see what you see: inferring sensor
and policy models of human real-world motor behavior,” in Proc. 31st AAAI Conf. Artif. Intell.,
2017, pp. 3797-3803.
[37]	M. Kwon, S. Daptardar, P. Schrater, and X. Pitkow, “Inverse rational control with partially
observable continuous nonlinear dynamics,” in Adv. Neural Inf. Process. Syst. 34, 2020.
[38]	S. Reddy, S. Levine, and A. D. Dragan, “Assisted perception: optimizing observations to
communicate state,” arXiv preprint arXiv:2008.02840, 2020.
[39]	C. Finn, P. Christiano, P. Abbeel, and S. Levine, “A connection between generative adver-
sarial networks, inverse reinforcement learning, and energy-based models,” NIPS Workshop
Adversarial Training, 2016.
[40]	F. A. Sonnenberg and J. R. Beck, “Markov models in medical decision making: a practical
guide,” Health Econ., vol. 13, pp. 322-338, 1983.
11
Published as a conference paper at ICLR 2021
[41]	C. H. Jackson, L. D. Sharples, S. G. Thompson, S. W. Duffy, and E. Couto, “Multistate Markov
models for disease progression with classification error,” Statistician, vol. 52, pp. 192-209,
2003.
[42]	S. E. O’Bryant, S. C. Waring, C. M. Cullum, J. Hall, L. Lacritz, P. J. Massman, P.J. Lupo, J. S.
Reisch, and R. Doody, “Staging dementia using Clinical Dementia Rating Scale Sum of Boxes
scores: a Texas Alzheimer’s research consortium study,” Arch. of Neurology, vol. 65, no. 8, pp.
1091-1095, 2008.
[43]	D. Jarrett, J. Yoon, and M. van der Schaar, “Dynamic prediction in clinical survival analysis
using temporal convolutional networks,” IEEE J. Biomed. Health Inform., vol. 24, no. 2, pp.
424-436, 2019.
[44]	P. Petousis, A. Winter, W. Speier, D. R. Aberle, W. Hsu, and A. A. T. Bui, “Using sequential
decision making to improve lung cancer screening performance,” IEEE Access, vol. 7, pp.
119 403-119 419, 2019.
[45]	F. Cardoso, S. Kyriakides, S. Ohno, F. Penault-Llorca, P. Poortmans, I. T. Rubio, S. Zackrisson,
and E. Senkus, “Early breast cancer: ESMO Clinical Practice Guidelines for diagnosis, treatment
and follow-up,” Anna. Oncology, vol. 30, no. 8, pp. 1194-1220, 2019.
[46]	A. M. Alaa and M. van der Schaar, “Attentive state-space modeling of disease progression,” in
Adv. Neural Inf. Process. Syst. 33, 2019, pp. 11 338-11 348.
[47]	X. Wang, D. Sontag, and F. Wang, “Unsupervised learning of disease progression models,” in
Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 85-94.
[48]	R. V. Marinescu, N. P. Oxtoby, A. L. Young, E. E. Bron, A. W. Toga, M. W. Winer et al.,
“Tadpole challenge: prediction of longitudinal evolution in Alzheimer’s disease,” arXiv preprint
arXiv:1805.03909, 2018.
[49]	L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and acting in partially observable
stochastic domains,” Artif. Intell., vol. 101, no. 1-2, pp. 99-134, 1998.
[50]	Y. Bengio and P. Frasconi, “An input output HMM architecture,” in Adv. Neural Inf. Process.
Syst. 7, 1995, pp. 427-434.
[51]	C. Salvatore, P. Battista, and I. Castiglioni, “Frontiers for the early diagnosis of AD by means
of MRI brain imaging and support vector machines,” Current Alzheimer Res., vol. 13, no. 5, pp.
509-533, 2016.
12
Published as a conference paper at ICLR 2021
A	Details of the Algorithm
A. 1 Forward-Backward Procedure
We compute the necessary marginalizations of the joint distribution P(D|D, θ) using the forward-
backward algorithm. Letting xt:t0 = {xt, xt+1 , . . . , xt0 } for any time-indexed quantity xt, the
forward messages are defined as ɑt(s) = P(St = s, a±t-ι, z±t-ι∣θ), which can be computed
dynamically as
αt+1(S ) = P(St+1 = S , a1:t, z1:t lθ)
X0
P(St = s, a1:t —1, z1:t-1 lθ)P(st+1 = S , at, zt |st = s, a1:t—1, z1:t-1, θ)
s∈S
X
αt(S)∏(at∣bt)T(S |s, at)。®%, S )
s∈S
H Eat(S)T(S0∣S, at)O(zt∣at, s0)
s∈S
with initial case α1(S) = P(S1 = S) = b1(S). The backward messages are defined as βt(S) =
P(at：T, zt：T∣St = s, ai：t—ι, zi：t—i, θ), which can also be computed dynamically as
βt(s) = P(at:T, zt：T |St = s, a1:t—1, z1:t—1, θ)
X00
P(St+1 = S , at, zt |St = s, a1:t—1, z1:t—1, θ)P(at+1:T, zt+1：T | st+1 = S , a1:t, z1:t, θ)
s0∈S
=X ∏(at∣bt)T(s0∣s, at)O(zt|at, so)βt+ι(s0)
s0∈S
H X T(SlS,at)O(zt∣at,s0)βt+ι(s0)
s0∈S
• . ) ♦ ∙ . ∙ i	r> / ∖ πτ,∕dl	zi∖	r
with initial case βτ+ι(s) = P(0|s「+ι = s, ai：「zi：T, θ) = 1.
Then, the marginal probability of being in state S at time t given the dataset D and the estimate θ can
be computed as
,、	_ ,	. _ O.
γt(s) = P(st = s∣D,θ)
= P(St = S|a1:T, z1:T, θ)
H P(St = s, a1：T, z1:Tlθ)
= αt(S)β(S)
and similarly, the marginal probability of transitioning from state S to state S0 at the end of time t
given the dataset D and the estimate θ can be computed as
ξt(S, s0) = P(St = s, st+1 = s0lD, θ)
H P(st = s, st+1 = S , a1:T, z1:Tlθ)
0
=P(st = s, a1:t — 1, z1:t—1 lθ)P(st+1 = S ,at,zt|st = s, a1:t—1, z1:t —1, θ)
0
× P(at+1:T, zt+1:T |st+1 = s , a1:t, z1:t, θ)
=at(s)Π(at∣bt)T(s0∣s, a)O(z∣a, s0)βt+1(s0)
0	00
H at (s)T (s0 |s, a)O(z|a, s0)βt+1(s0) .
A.2 Gradient-Ascent Procedure
Taking the gradient of the expected log-likelihood Q(θ; θ) in (5) with respect to the unknown
parameters θ = (T, O, b1, η, μ°∈A) first requires computing the Jacobian matrix V⑴bt，for 1 ≤
13
Published as a conference paper at ICLR 2021
t < t0 ≤ T, where (Vbbz)ij = ∂bf(i)∕∂b(j) for i,j ∈ S. This can be achieved dynamically as
▽bt bt，= Vbt+1 bto Vbt bt+ι with initial case Vb^bt，= I, where
∂bt+ι(i)
~εtjT
∂ 一
∂bj
(▽bt bt+1)ij
Exes bt(x)T(i∖x,at)O(zt∖at,i)]
∑x∈s ∑xo∈s bt(x)T(x0∖x, at)O(zt∖at, x0) J
T(i∖j, at)O(zt∖at, i)
Ex∈s ∑x∕∈s bt(X)T(x，\x,出)。(ZtM x')
∑x∕∈s T(x'∖j, at)o(zt∖at, X)
(∑x∈s ∑xo∈s bt(x)T(x0∖x, at)O(zt∖at, x0))2
A.2.1 PARTIAL DERIVATIVES
EI i ♦	. ∙	/` ʌʌ / ι∖ A∖ ∙ . λ	. . m /	! I ∖ ∙
The derivative of Q(θ; θ) with respect to T(s'∖s, a) is
患B) = ∂T(s'∖s,a) X 忙I(at = a} EE &(X,x')l°gT(Xi
v 1 , 7	v 1 , i i=1 Lt=1	x∈Sx0∈S	T
+ £log π(at∖bt)
t=2	.
∂ log π(at∖bt)
∂T(s'∖s, a)
n
E
i=1
E I{at = a}；
T
+ E
t=2
n =E i=1	E I(at = a} 3⅛A * { t	}T(s'∖s,a) T t-1	- +	Vbt logπ(at∖bt)Vbt0+1 btVt(s，|s,a)bt，+1 , t=2 t，= 1	_
where (Vbt log∏(at∖bt))ij =	∂ log(at∖bt) ∂bt(j) ∂bj (-ηkbt - μatk2 - log E e-ηkbt-μak2) χ.一,	e-rη∖[bt -μak2 -2η(btj — μat(j)) + 2η £ P	-ηkbt-μ°，k2 (bt(j) — μa (j)) a∈A 乙a，eA e	a
	=-2η(bt(j) - μat(j))+ 2ηEπ(a∖bt)(bt(j) - μa(j)) a∈A
and (VT (s，|s,a)bt，+ 1)i1 二	_ dbt，+1(i) ∂T(s!∖s, a) _	∂	(	Pxes b (X)T (i∖x,aγ ∖O(zt∕ % M ∂T (s[s, a) VPx∈s Px，es bt，(x)T (x/∖x, at^ )O(zt^ ^tt，,x')
=" I{i = s '}b (s∙)O(zt ∖a,s')
={t0= } k∑x∈s Px，es bt，(x)T(x'∖x,a)O(R ∖a,x')
b(s)O(zt，∖a, s')
-- -------=-----T ； .   , ：---： ：~；-：---、 、 C
(Pxes Px，es bt0 (x)T(x,x^ a)O(zt' V,-,x'))2
The derivative of Q(θ; θ) with respect to O(z∖a, s') is
dQ(e；θ)	d G「Gur	】/ '、、-，、/ i 八
∂O(z∖a,sθ) = ∂O(z∖a,s') ⅛	=	} J	(X ) g O(H X )
14
Published as a conference paper at ICLR 2021
T
+ £log π(at∖bt)
t=2	.
XX "X I{αt = a,zt = z} zγt+1(g,), + XX d^g π**
± [±	，。(Ha,s,)	士 ∂O(z∖a,s,)
X X I{αt = α,zt = z} "I)
τ t—1	-
+	Vbt logπ(at∖bt)Vbt,+1 btVo(z|a,s，)bt，+i ,
t=2 t0 = 1	_
where
(VO(Z∣a,s0)bt0 +1)il
"1⑴
∂O(z∖a, s,)
∂	(	ExES 跖(X)T (i∖x,at"O(如 ％，，i)	λ
∂O(z∖a, s，) IPxES Px，ES 如(X)T(x'∖x, at0 )O(Zf ∖at0 ,x，) J
=	=}( 叩=s,} PxES bt，(X)T (S∖χ,a)
，	IPxES Px0ES bt，(x)T(x[x, a)O(z∖a, X ,)
____________PxES bt，(X)T(six, a)	、
(PxES Px，ES bt，(X)T(x'∖x, a)O(z∖a, X ,))Q
EI 1	♦	. ∙	∕' ʌʌ / lΛ A∖ ∙ . Λ	. .	1	/ ∖ ∙
The derivative of Q(θ; θ) with respect to bι(s) is
_ ʌ.
∂Q(θ; θ)
∂bι(s)
∂ n	T
∂b^)∑^ EYI(X)logbi(X) + ∑Slogπ(at∖bt)
n
X
i=1
≡⅛+Xvbtlog*"Vb，⑸瓦,
where (VbI(S)bt)ii = (VbIbt)i
EI 1	. ∙	∕' ʌʌ / lΛ A∖	∙ . Λ	. .	♦
The derivative of Q(θ; θ) with respect to η is
_ ʌ. ∂Q(θ; θ)二 ∂η	nτ W XX log π(at∖bt) η i=1t=1 「X X ⅛ Jkbtiatk2- log X e-『) "ʃ (	L	e—nkbt—“0 ∣2 =XX (-kbt - μat k2 + XP“，eAe—nkbt—“"∣2 kbt - μak2 =X X (-kbt-μatk2 + Xπ(a∖bt)kbt - μak2).
τ-,∙	11	.1	1	∙	i` /ʌ / /k A∖ ∙ . Λ	. .	/	∖ ∙
Finally, the derivative of Q(θ; θ) with respect to μα(s) is
_ ʌ.	_
∂Q(θ; θ) _	∂
∂μa(s)	∂μa(s)
n τ
=X X
nτ
nτ
XX
log π(at∖bt)
E 卜ηkbt-μatk2- logXe—nkbt-小2
ΣΣ(^2ηI{at = a}(bt(s) - μa(s)) - 2η
e—nkbt-μa Il2
P, 〃e—nkbt—“J2 (bt(S) - μa(S))
Ja' EA
15
Published as a conference paper at ICLR 2021
nτ
XX
(2ηI{at = a}(bt(s) - μο(s)) - 2η∏(a∣bt)(bt(s) - μ0(s))
nτ
XX
2η(I{at = a}- ∏(a∣bt))(bt(s) - μa(s)).
B Proofs of Propositions
B.1 Proof of Proposition 1
First, denote with qR ∈ Rδ(S) ×A the optimal (belief-state) q-value function with respect to the
underlying (state-space) reward function R ∈ Rs×a, and denote with v* ∈ Rδ(S) the corresponding
optimal value function VR(b) = SoftmaXa,∈A qR(b, a0). Now, fix some component i of parameters θ;
we wish to compute the derivative of log ∏(a∣b) with respect to θi:
∂∂
海 logn(a|b) =辐(qR(b, a) - VR(b))
∂θi	∂θi
=di：卜 R (b, a)- log X eqR (b,a0)!
∂	(	eqR (b,aO)	d	∖
=∂∣iqR Ma- X (P 〃	eqR(b,a，，) ∙ ∂θiqR(b,a'))
i	a0∈A	a00∈A e	i
∂∂
=丽qR(b，a) - X π(a |b)∂∣7qR(b,a )
∂	*
∂θqR (b, a) - Ea0 〜π(∙∣b)
∂
∂θi qR (b,a)
where we make explicit here the dependence on R, but note that it is itself a parameter; that is, R = θj
for some j. We see that this in turn requires computing the partial derivative ∂qR(b, a)∕∂θi. Let Y be
some appropriate discount rate, and denote with ρR ∈ R∆(S)×A the effective (belief-state) reward
ρR (b, a) =. Ps∈S b(s)R(s, a) corresponding to R. Further, let
P(b0|b,a)
= X P(z |b, a)P(b0 |b, a, z)
z∈Z
_ X X X X K、τ/ 01	\n( I oΛ X Λo___________Σ2s∈S b(S)T (Is,a)O(Z|a, ∙) ʌ
—Z∈Z(S!SSiS S S s,α	za,s J P	Ps∈s Ps0∈s b(s)τ (SlS,a)O(z|a,s0J
denote the (belief-state) transition probabilities induced by T and O, where δ is the Dirac delta
function such that δ(b0) integrates to one if and only if b0 = 0 is included in the integration region.
Then the partial ∂qR(b, a)∕∂θi is given as follows:
焉 qR (b，a)=卷(ρR(b, a) + γ ∕o∈∆(S) P(b01b，a)VR (b0)db0
∂
=∂θ^ρR(b, a) + γ
∂θi	b0∈∆(S)
'-----------------------
∂
vR(b )^∂θ~∙P(b |b, a)db
{
ρR,i(b,a)
+ Y [	P(b0∣b,a)Ea，〜∏(∙∣b0)
b0∈∆(S)
∂
瓯qR小小b0
from which We observe that ∂qR(b, a)∕∂θi is a fixed point of a certain Bellman-like operator.
Specifically, fix any function f ∈ Rδ(S)×A; then ∂qR(b, a)∕∂θi is the fixed point of the operator
16
Published as a conference paper at ICLR 2021
TRπ,i : R∆(S)×A → R∆(S)×A defined as follows:
(TRπ,if)(b, a) = PR,i(b, a) + Y [	PMAa) Xn(a0|bO)f(b0,a0)db0
b0∈∆(S)	a0
which takes the form of a “generalized” Bellman operator on q-functions for POMDPs,
where for brevity here We have written ρR,i(b, a) to denote the expression 晶PR(b, a) +
γ Jb0∈∆(s) VRC)∂∂θ~P(b0∣b, a)db0. Mathematically, this means that a recursive procedure can in
theory be defined——cf. “Vq-iteration”, analogous to q-iteration; see e.g. [52]——that may converge on
the gradient under appropriate conditions. Computationally, however, this also means that taking a
single gradient is at least as hard as solving POMDPs in general.
Further, note that while typical POMDP solvers operate by taking advantage of the convexity property
ofρR (b, a)——see e.g. [53]——here there is no such property to make use of: In general, it is not the case
that ρR,i(b, a) is convex. To see this, consider the following counterexample: Let S =. {s-, s+}, A =.
{a=}, Z =. {z-, z+}, T(s- |s-, a=) = T (s+ |s+, a=) = p = 1, O(z- |a=, s-) = O(z+ |a=, s+) =
1/4, b1(s+) = 1/2, R(s-, a=) = 0, R(s+, a=) = 1/2, and γ = 1/2. For simplicity, we will simply
write b instead of b(s+). Note that:
qR (b,a=) = b£ γ tR(s+,a=) + (1 - b) £ YtR(S-, a= = b
t=0	t=0
VR (b) = log E eqR (b，a) =log eqR (b，a=) = qR (b,a=) = b
a∈{a= }
1	3	13
P(z+lb, a=) = 4(bp + (1 — b)(1 — P)) + 4(b(1 — p) +(I — b)p) = 4b + 4(1 —b)
1	3	13
P(Z-|b,a=) = 4(b(1 — p) + (1 — b)p) + 4(bp + (1 — b)(1 — P)) = 4(1 —b) + 4 b
b0Ib, a=, Z+	_ P(s0 = s+,z+∣b,a=) = 4 bp + 4(I - b)(1- p) =	4 b P(Z+|b,a=)	P(Z+|b,a=)	4b + 4(1 - b)
b0Ib, a=, Z-	_ P(s0 = s+,Z-∣b, a=) = 4(1 - b)(1 - p) + fbp =	Ib P(Z-∣b, a=)	P(Z-∣b, a=)	4(1 - b) + 4b (P(z+ ∣b, a=)	if b0 = b0∣b, a=, z+
P(b0Ib,a=)	二 < P(z- ∣b, a=)	if b0 = b0∣b, a=, Z- I 0	otherwise
Now, let the elements of θ be ordered such that p is the i-th element, and consider ρR,i(b, a)——
evaluated at p = 1:
∂∂
PR,i(b, a=) = -χ~PR(b, a=) + γ	Vr(b )歹P(b ∣b,a=)db
∂p	b0∈∆(s)	∂p
=2 (R(b0|b,a=,z+)∂dpP(Z+|b,a=) + VR(b0∣b,a=,z-)∂dpP(Z-Ib,a=))
=1 (—4b—仕 b -1(1 - b) - 3 b + 3(1 - b)l
2 <4b + 3(1 - b) <4	4<	/	4 八 7
+1——4b~~ɜ- (- 1b +1(1 - b) + 3b - 3(1 - b)))
+ 4(1-b) + 4b V 4 +4(	)+4	4(	)川
Clearly ρR,i(b, a=) cannot be convex since ρR,i(1∕2,a=) = 0 and ρR,i(1, a=) = 0 but
ρR,i (3/4, a= ) > 0.
B.2 Proof of Proposition 2
In contrast, unlike the indirect q-value parameterization above (which by itself requires approximate
solutions to optimization problems), the mean-vector parameterization of Interpole maps beliefs
17
Published as a conference paper at ICLR 2021
directly to distributions over actions. Now, the derivatives of log ∏(a∣b) are given as closed-form
expressions in Appendices A.1 and A.2.
In particular, note that each bt is computed through a feed-forward structure, and therefore can easily
be differentiated with respect to the unknown parameters θ through backpropagation through time:
Each time step leading up to an action corresponds to a “hidden layer” in a neural network, and the
initial belief corresponds to the “features” that are fed into the network; the transition and observation
functions correspond to the weights between layers, the beliefs at each time step correspond to the
activations between layers, the actions themselves correspond to class labels, and the action likelihood
corresponds to the loss function (see Appendices A.1 and A.2).
Finally, note that computing all of the forward-backward messages αt and βt in Appendix A.1 has
complexity O(nτS 2), computing all of the Jacobian matricies RbtbtO in Appendix A.2 has complexity
O(nτ 2S3), and computing all of the partial derivatives given in Appendix A.2 has complexity at
most O(nτ2S2AZ). Hence, fully differentiating the expected log-likelihood Q(θ; θ) with respect to
the unknown parameters θ has an overall (polynomial) complexity O(nτ2S2 max{S, AZ}).
C Experiment Particulars
C.1 Details of Decision Environments
ADNI We have filtered out visits without a CDR-SB measurement, which is almost always taken,
and visits that do not occur immediately after the six-month period following the previous visit
but instead occur after 12 months or later. This filtering leaves 1,626 patients with typically three
consecutive visits each. For MRI outcomes, average is considered to be within half a standard
deviation of the population mean. Since there are only two actions in this scenario, we have set η = 1
and relied on the distance between the two means to adjust for the stochasticity of the estimated
policy—closer means being somewhat equivalent to a smaller η.
DIAG We set Ttrue(s-∣s-, ∙) = Ttrue(s+∣s+, ∙) = 1, meaning patients do not heal or contract
the diseases as the diagnosis progresses, Otrue(z- |a=, s+) = Otrue(z+ |a=, s-) = 0.4, meaning
measurements as a test have a false-negative and false-positive rates of 40%, and bt1rue(s+) = 0.5.
Moreover, the behavior policy is given by T = Ttrue, O =OtrUe, bi = bfue, η = 1θ, μo= (s+)=
0.5, and μ0- (s-) = μ0+ (s+) = 1.3. Intuitively, doctors continue monitoring the patient until they
are 90% confident in declaring a final diagnosis. In this scenario, T and η are assumed to be known.
The behavior dataset is generated as 100 demonstration trajectories.
BIAS We set all parameters exactly the same way we did in DIAG with one important exception:
now O(s- |a=, z+) = 0.2 while it is still the case that Otrue (z- |a=, s+) = 0.4, meaning O 6= Otrue
anymore. In this scenario, b1 is also assumed to be known (in addition to T and η) to avoid any
invariances between b1 and O that we have encountered during training. The behavioral dataset is
generated as 1000 demonstration trajectories.
C.2 Details of Benchmark Algorithms
R-BC We train an RNN whose inputs are the observed histories ht and whose outputs are the
predicted probabilities ∏(a∣ht) of taking action a given the observed history ht. The network consists
of an LSTM unit of size 64 and a fully-connected hidden layer of size 64. We minimize the cross-
entropy loss L = 一 Pn=i PT=ι Pa∈A I{at = a} log ∏(a∣ht) using Adam optimizer with learning
rate 0.001 until convergence, that is when the cross-enropy loss does not improve for 100 consecutive
iterations.
PO-IRL The IOHMM parameters T, O, and b1 are initialized by sampling them uniformly at
random. Then, they are estimated and fixed using conventional IOHMM methods. The reward
parameter R is initialized as R0(s, a) = £§,a where £§,0 〜N(0,0.0012). Then, it is estimated via
Markov chain Monte Carlo (MCMC) sampling, during which new candidate samples are generated
by adding Gaussian noise with standard deviation 0.001 to the last sample. A final estimate is formed
by averaging every tenth sample among the second set of 500 samples, ignoring the first 500 samples.
In order to compute optimal q-values, we have used an off-the-shelf POMDP solver available at
https://www.pomdp.org/code/index.html.
18
Published as a conference paper at ICLR 2021
Off. PO-IRL All parameters are initialized exactly the same way as in PO-IRL. Then, both the
IOHMM parameters T, O, and b1, and the reward parameter R are estimated jointly via MCMC
sampling. When generating new candidate samples, with equal probabilities, we have either sampled
new T, O, and b1 from IOHMM posterior (without changing R) or obtained a new R the same way
we did in PO-IRL (without changing T, O, and b1). A final estimate is formed the same way as in
PO-IRL.
PO-MB-IL The IOHMM parameters T , O, and b1 are initialized by sampling them uniformly at
random. Then, they are estimated and fixed using conventional IOHMM methods. Given the IOHMM
parameters, we parameterized policies the same way we did in Interpole, that is as described in (2).
The policy parameters {μ°}a∈A are initialized as μa(s) = (1∕∣S∣ + ε°,s)/pso∈s(1∕∣S∣ + G)
where 已&,5，〜N(0,0.0012). Then, they are estimated according solely to the action likelihoods in
(4) using the EM algorithm. The expected log-posterior is maximized using Adam optimizer with
learning rate 0.001 until convergence, that is when the expected log-posterior does not improve for
100 consecutive iterations.
Interpole All parameters are initialized exactly the same way as in PO-MB-IL. Then, the IOHMM
parameters T, O, and b`, and the policy parameters {μa}a∈A are estimated jointly according to both
the action likelihoods and the observation likelihoods in (4). The expected log-posterior is again
maximized using Adam optimizer with learning rate 0.001 until convergence.
C.3 Further Example: Post-hoc Analyses
Policy representations learned by Intepole provide users with means to derive concrete criteria
that describe observed behavior in objective terms. These criteria, in turn, enable the quantitative
analyses of the behavior using conventional statistical methods. For ADNI, we have considered two
such criteria: belatedness of individual diagnoses and informativeness of individual tests. Both of
these criteria are relevant to the discussion of early diagnosis, which is paramount for Alzheimer’s
disease [51] as we have already mentioned during the illustrative examples.
Formally, we consider the final diagnoses of a patient to be belated if (i) the patient was not ordered
an MRI in one of their visits despite the fact that an MRI being ordered was the most likely outcome
according to the policy estimated by Interpole and (ii) the patient was ordered an MRI in a later
visit that led to a near-certain diagnosis with at least 90% confidence according to the underlying
beliefs estimated by Interpole. We consider An MRI to be uninformative if it neither (factually)
caused nor could have (counterfactually) caused a significant change in the underlying belief-state
of the patient, where an insignificant change is half a standard deviation less than the mean factual
change in beliefs estimated by Interpole.
Having defined belatedness and informativeness, one can investigate the frequency of belated diag-
noses and uninformative MRIs in different cohorts of patients to see how practice varies between
one cohort to another. In Table 5, we do so for six cohorts: all of the patients, patients who are over
75 years old, patients with apoE4 risk factor for dementia, patients with signs of MCI or dementia
since their very first visit, female patients, and male patients. Note that increasing age, apoE4 allele,
and female gender are known to be associated with increased risk of Alzheimer,s disease [54-57].
For instance, we see that uninformative MRIs are much more prevalent among patients with signs of
MCI or dementia since their first visit. This could potentially be because these patients are monitored
much more closely than usual given their condition.
Table 5: Frequency of belated diagnoses and uninformative MRIs in various patient cohorts.
Cohort	Frequency of belated diagnoses	Frequency. of uninformative MRIs
All patients	6.52%	18.8%
Patients over 75 years old	9.29%	18.1%
Patients with apoE4 risk factor	8.75%	19.3%
Patients with signs of MCI/dementia	9.26%	26.1%
Female patients	7.19%	17.6%
Male patients	5.97%	19.8%
19
Published as a conference paper at ICLR 2021
Alternatively, one can divide patients into cohorts based on whether they have a belated diagnoses
or an uninformative MRI to see which features these criteria correlate with more. We do so in
Table 6. For instance, we see that a considerable percentage of belated diagnoses are seen among
male patients.
Table 6: Features of patients with belated diagnoses and uninformative MRIs.
Feature	All patients	Patients with belated diagnoses	Patients with uninformative MRIs
Mean age	73.9 ± 7.1	75.8 ± 7.5	73.0 ± 7.3
Freq. of apoE4	45.7%	54.2%	49.0%
Freq. of MCI/dementia signs	68.4%	95.8%	98.1%
Perc. of female patients	45.4%	39.0%	43.5%
Perc. of male patients	54.6%	61.0%	56.5%
C.4 Further Example: Decision Trees
Clinical practice guidelines are often given in the form of decision trees, which usually have vague
elements that require the judgement of the practitioner [58, 59]. For example, the guideline could
ask the practitioner to quantify risks, side effects, or improvements in subjective terms such as being
significant, serious, or potential. Using direct policy learning, how vague elements like these are
commonly resolved in practice can be learned in objective terms.
Formulating policies in terms of IOHMMs and decision boundaries is expressive enough to model
decision trees. An IOHMM with deterministic observations, that is O(z|a, s0) = 1 for some z ∈ Z
and for all a ∈ A, s ∈ S, essentially describes a finite-state machine, inputs of which are equivalent
to the observations. Similarly, a deterministic decision tree can be defined as a finite-state machine
with no looping sequence of transitions. The case where the observations are probabilistic rather than
deterministic correspond to the case where the decision tree is traversed in a probabilistic way so that
each path down the tree has a probability associated with it at each step of the traversal.
Figure 5: Two Different Descriptions of the Same Policy: (a) in the form of a decision tree and (b) in
terms of an equivalent IOHMM. For the IOHMM in (b), arrows denote possible transitions, where
the probability of a transition is proportional to the quantity written above the corresponding arrow.
Using direct policy learning, we can infer the risk of disease, b2 (DIS), and the probability of testing
for the sub-type based on the risk, ∏(τsτ-TYP∣b2), which are left vague in (a).
As a concrete example of modeling decision trees in terms of IOHMMs, consider the scenario of
diagnosing a disease with two sub-types: Disease-A and Disease-B. Figure 5a depicts the policy of
the doctors in the form of a decision tree. Each newly-arriving patient is first tested for the disease in
a general sense without any distinction between the two sub-types it has. The patient is then tested
20
Published as a conference paper at ICLR 2021
for a specific sub-type of the disease only if the doctors deem there is a significant risk that the patient
is diseased. Note that which exact level of confidence constitutes as a significant risk is left vague in
the decision tree. By modeling this scenario using our framework, we can learn: (i) how the risk is
determined based on initial test results and (ii) what amount of risk is considered significant enough
to require a subsequent test for the sub-type.
Let S = {INI, HLT, DIS, DSA, DSB}, where INI denotes that the patient has newly arrived, HLT
denotes that the patient is healthy, dis denotes that the patient is diseased, dsa denotes that the
patient has Disease-A, and dsb denotes that the patient has Disease-B. Figure 5b depicts the state
space S with all possible transitions. Note that the initial belief b1 is such that b1 (INI) = 1. Let
A = {TST-DIS, TST-TYP, STP-HLT, STP-DSA, STP-DSB}, where TST-DIS denotes testing for the
disease, tst-typ denotes testing for the sub-type of the disease, and the remaining actions denote
stopping and diagnosing the patient with one of the terminal states, namely states hlt, dsa, and
dsb.
After taking action a1 = TST-DIS and observing some initial test result z1 ∈ Z, the risk of disease,
which is the probability that the patient is diseased, can be calculated with a simple belief update:
b2(DIS) H y^bι(s)T(DIS∣s, TST-DIS)O(zι∣TST-DIS, DIS)
s∈S
= T (DIS|INI, TST-DIS)O(z1 |TST-DIS, DIS) .
Moreover, we can say that the doctors are more likely to test for the sub-type of the disease as opposed
to stopping and diagnosing the patient as healthy, that is ∏b(τsτ-TYP∣b2) > ∏b(sτP-HLT∣b2), when
b2(DIS) >
μTST-TYP(DIS) + μSTP-HLT(DIS)
2
assuming Mtst-typ(dis) > Mstp-hlt(dis). Note that there are only two possible actions at the second
time step: actions tst-typ and stp-hlt.
D	Details of the Clinician Surveys
Each participant was provided a short presentation explaining (1) the ADNI dataset and the decision-
making problem we consider, (2) what rewards and reward functions are, (3) what beliefs and belief
simplices are, and (4) how policies can be represented in terms of reward functions as well as
decision boundaries. Then, they were asked two multiple-choice questions, one that is strictly about
representing histories, and one that is strictly about representing policies. Importantly, the survey
was conducted blindly—i.e. they were given no context whatsoever as pertains this paper and our
proposed method. The question slides can be found in Figures 6 and 7. Essentially, each question
first states a hypothesis and shows two/three representations relevant to the hypothesis stated. Then,
the participant is asked which of the representations shown most readily expresses the hypothesis.
Here are the full responses that we have received, which includes some additional feedback:
•	Clinician 1
Question 1: C > B > A
Question 2: B
Additional Feedback: The triangle was initially more confusing than not, but the first example
(100% uncertainty) was helpful. It isn’t clear how the dots in the triangle are computed. Are these
probabilities based on statistics? Diagram is always better than no diagram.
•	Clinician 2
Question 1: C > B > A
Question 2: B
Additional Feedback: I always prefer pictures to tables, they are much easier to understand.
•	Clinician 3
Question 1: C > B > A
Question 2: B
Additional Feedback: Of course the triangle is more concise and easier to look at. But how is the
decision boundary obtained? Does the decision boundary always have to be parallel to one of the
sides of the triangle?
21
Published as a conference paper at ICLR 2021
•	Clinician 4
Question 1: C > B > A
Question 2: B
Additional Feedback: [Regarding Question 1,] representation A and B do not show any interpreta-
tion of the diagnostic test results, whereas representation C does. I think doctors are most familiar
with representation B, as it more closely resembles the EHR. Although representation C is visually
pleasing, I’m not sure how the scale of the sides of the triangle should be interpreted. [Regarding
Question 2,] again I like the triangle, but it’s hard to interpret what the scale of the sides of the
triangle mean. I think option A is again what doctors are more familiar with.
•	Clinician 5
Question 1: C > B > A
Question 2: A
•	Clinician 6
Question 1: C > B > A
Question 2: A
•	Clinician 7
Question 1: C
Question 2: B
Additional Feedback: I thought I’d share with you my thoughts on the medical aspects in your
scenario first (although I realise you didn’t ask me for them). [...] The Cochrane review concludes
that MRI provides low sensitivity and specificity and does not qualify it as an add on test for
the early diagnosis due to dementia (Lombardi G et. al. Cochrane database 2020). The reason
for MRI imaging is (according to the international guidelines) to exclude non-degenerative or
surgical causes of cognitive impairment. [...] In your example the condition became apparent when
the CDR-SB score at Month 24 hit 3.0 (supported by the sequence of measurements over time
showing worsening CDR-SB score). I imagine the MRI was triggered by slight worsening in the
CDR-SB score (to exclude an alternative diagnosis). To answer your specific questions: Q1. The
representation C describes your (false) hypothesis that it was the MRI that made the diagnosis
of MCI more likely/apparent the best—I really like the triangles. Q2. I really like the decision
boundary.
•	Clinician 8
Question 1: C > B > A
Question 2: B
•	Clinician 9
Question 1: C
Question 2: B
Additional Feedback: Q1. Representation C gives the clearest illustration of the diagnostic change
following MRI. However, the representation of beliefs on a continuous spectrum around discrete
cognitive states could be potentially confusing given that cognitive function is itself a continuum
(and ‘MCI’, ‘Dementia’ and ‘NL’ are stations on a spectrum rather than discrete states). Also, while
representation C is the clearest illustration, it is the representation that conveys the least actual data
and it isn’t clear from the visualisation exactly what each shift in 2D space represents. Also, the
triangulation in ‘C’ draws a direct connection between NL and Dementia, implying that this is a
potential alternative route for disease progression, although this is more intuitively considered as a
linear progression from NL to MCI to Dementia. Q2. For me, the decision boundary representation
best expresses the concept of the likelihood of ordering and MRI with the same caveats described
above. Option B does best convey the likelihood of ordering an MRI, but doesn’t convey the
information value provided by that investigation. However, my understanding is that this is not
what you are aiming to convey here.
References
[52]	M. Herman, T. Gindele, J. Wagner, F. Schmitt, and W. Burgard, “Inverse reinforcement learning
with simultaneous estimation of rewards and dynamics,” in Proc. 19th Int. Conf. Artif. Intell.
Statist. ,2016,pp.102-110.
22
Published as a conference paper at ICLR 2021
•	Hypothesis: This patient's condition (MCI) only became apparent after the first MRI
was ordered.
•	Question #1: Which of the following representations of the patient's medical history
most readily expresses this hypothesis? (Please rank them from most to least
accessible.)
Most recent CDR-SB:
3.0 (very mild dementia)
Most recent hippocampal volume:
5980 mm3 (below avg.)
Most recent ADAS11:
13.00
Most recent ADAS13:
21.00
Most recent whole brain volume:
1040240 mm3
Biomarker	Baseline	Month 6	Month 12	Month 18	Month 24
CDR-SB	1.0	1.5	2.0	1.5	3.0
CDR-SB Category	Question -able im -pairment	Question -able im -pairment	Question -able im -pairment	Question -able im -pairment	Question -able im -pairment
MRI	Not ordered	Not ordered	Ordered	Ordered	Ordered
Hippocampal Volume			6004 mm3 (below avg.)	5630 mm3 (below avg.)	5980 mm3 (below avg.)
ADAS11	12.00	11.00	10.67	11.00	13.00
ADAS13	21.00	18.00	19.67	15.00	21.00
Whole Brain Volume			1060310 mm3	1019500 mm3	1040240 mm3
Representation A:
Most Recent Measurement
Representation B:
Sequence of Measurements
Representation C:
Sequence of Beliefs
Figure 6: Slide 5 out of 9, which contains the first question regarding histories.
23
Published as a conference paper at ICLR 2021
[53]	M. Araya, O. Buffet, V. Thomas, and F. Charpillet, “A pomdp extension with belief-dependent
rewards,” in Adv. Neural Inf Process. Syst. 24, 2010,pp. 64-72.
[54]	S. Gao, H. C. Hendrie, K. S. Hall, and S. Hui, “The relationships between age, sex, and the
incidence of dementia and Alzheimer disease: a meta analysis,” Arch. General Psychiatry,
vol. 55, no. 9, pp. 809-815, 1998.
[55]	L. J. Launer, K. Andersen, M. E. Dewey, L. Letenneur, A. Ott, L. A. Amaducci, C. Brayne,
J. R. M. Copeland, J.-F. Dartigues, P. Kragh-Sorensen, A. Lobo, J. M. Martinez-Lage, T. Stijnen,
and A. Hofman, “Rates and risk factors for dementia and Alzheimer’s disease,” Neurology,
vol. 52, no. 1, pp. 78-78, 1999.
[56]	J. Lindsay, D. Laurin, R. Verreault, R. Hebert, B. Helliwell, G. B. Hill, and I. McDowell, “Risk
factors for Alzheimer’s disease: a prospective analysis from the Canadian Study of Health and
Aging,” Amer. J. Epidemiology, vol. 156, no. 5, pp. 445-453, 2002.
[57]	J.-H. Chen, K.-P. Lin, and Y.-C. Chen, “Risk factors for dementia,” J. Formosan Med. Assoc.,
vol. 108, no. 10, pp. 754-764.
[58]	R. Chou, A. Qaseem, V. Snow, D. Casey, J. T. Cross, P. Shekelle, and D. K. Owens, “Diagnosis
and treatment of low back pain: a joint clinical practice guideline from the American College
of Physicians and the American Pain Society,” Ann. of Internal Medicine, vol. 147, no. 7, pp.
478-491, 2007.
[59]	A. Qaseem, S. D. Fihn, P. Dallas, S. Williams, D. K. Owens, and P. Shekelle, “Manage-
ment of stable ischemic heart disease: summary of a clinical practice guideline form the
American College of Physicians/American College of Cardiology Foundation/American Heart
Association/American Association for Thoracic Surgery/Preventive Cardiovascular Nurses
Association/Society of Thoracic Surgeons,” Ann. of Internal Medicine, vol. 157, no. 10, pp.
735-743, 2012.
24