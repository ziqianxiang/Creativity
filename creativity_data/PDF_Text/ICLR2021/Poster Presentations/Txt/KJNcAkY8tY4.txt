Published as a conference paper at ICLR 2021
Do Wide and Deep Networks Learn the Same
Things ? Uncovering How Neural Network
Representations Vary with Width and Depth
Thao Nguyen； Maithra Raghu, & Simon Kornblith
Google Research
{thaotn,maithra,skornblith}@google.com
Ab stract
A key factor in the success of deep neural networks is the ability to scale models
to improve performance by varying the architecture depth and width. This simple
property of neural network design has resulted in highly effective architectures for
a variety of tasks. Nevertheless, there is limited understanding of effects of depth
and width on the learned representations. In this paper, we study this fundamental
question. We begin by investigating how varying depth and width affects model
hidden representations, finding a characteristic block structure in the hidden rep-
resentations of larger capacity (wider or deeper) models. We demonstrate that
this block structure arises when model capacity is large relative to the size of the
training set, and is indicative of the underlying layers preserving and propagating
the dominant principal component of their representations. This discovery has
important ramifications for features learned by different models, namely, repre-
sentations outside the block structure are often similar across architectures with
varying widths and depths, but the block structure is unique to each model. We
analyze the output predictions of different model architectures, finding that even
when the overall accuracy is similar, wide and deep models exhibit distinctive
error patterns and variations across classes.
1	Introduction
Deep neural network architectures are typically tailored to available computational resources by
scaling their width and/or depth. Remarkably, this simple approach to model scaling can result
in state-of-the-art networks for both high- and low-resource regimes (Tan & Le, 2019). However,
despite the ubiquity of varying depth and width, there is limited understanding of how varying these
properties affects the final model beyond its performance. Investigating this fundamental question
is critical, especially with the continually increasing compute resources devoted to designing and
training new network architectures.
More concretely, we can ask, how do depth and width affect the final learned representations? Do
these different model architectures also learn different intermediate (hidden layer) features? Are
there discernible differences in the outputs? In this paper, we study these core questions, through
detailed analysis of a family of ResNet models with varying depths and widths trained on CIFAR-10
(Krizhevsky et al., 2009), CIFAR-100 and ImageNet (Deng et al., 2009).
We show that depth/width variations result in distinctive characteristics in the model internal rep-
resentations, with resulting consequences for representations and outputs across different model
initializations and architectures. Specifically, our contributions are as follows:
•	We develop a method based on centered kernel alignment (CKA) to efficiently measure the simi-
larity of the hidden representations of wide and deep neural networks using minibatches.
•	We apply this method to different network architectures, finding that representations in wide or
deep models exhibit a characteristic structure, which we term the block structure. We study how
the block structure varies across different training runs, and uncover a connection between block
* Work done as a member of the Google AI Residency program.
1
Published as a conference paper at ICLR 2021
structure and model overparametrization — block structure primarily appears in overparameter-
ized models.
•	Through further analysis, we find that the block structure corresponds to hidden representations
having a single principal component that explains the majority of the variance in the representa-
tion, which is preserved and propagated through the corresponding layers. We show that some
hidden layers exhibiting the block structure can be pruned with minimal impact on performance.
•	With this insight on the representational structures within a single network, we turn to compar-
ing representations across different architectures, finding that models without the block structure
show reasonable representation similarity in corresponding layers, but block structure representa-
tions are unique to each model.
•	Finally, we look at how different depths and widths affect model outputs. We find that wide and
deep models make systematically different mistakes at the level of individual examples. Specif-
ically, on ImageNet, even when these networks achieve similar overall accuracy, wide networks
perform slightly better on classes reflecting scenes, whereas deep networks are slightly more ac-
curate on consumer goods.
2	Related Work
Neural network models of different depth and width have been studied through the lens of univer-
sal approximation theorems (Cybenko, 1989; Hornik, 1991; Pinkus, 1999; Lu et al., 2017; Hanin
& Sellke, 2017; Lin & Jegelka, 2018) and functional expressivity (Telgarsky, 2015; Raghu et al.,
2017b). However, this line of work only shows that such networks can be constructed, and provides
neither a guarantee of learnability nor a characterization of their performance when trained on finite
datasets. Other work has studied the behavior of neural networks in the infinite width limit by relat-
ing architectures to their corresponding kernels (Matthews et al., 2018; Lee et al., 2018; Jacot et al.,
2018), but substantial differences exist between behavior in this infinite width limit and the behavior
of finite-width networks (Novak et al., 2018; Wei et al., 2019; Chizat et al., 2019; Lewkowycz et al.,
2020). In contrast to this theoretical work, we attempt to develop empirical understanding of the
behavior of practical, finite-width neural network architectures after training on real-world data.
Previous empirical work has studied the effects of width and depth upon model accuracy in the con-
text of convolutional neural network architecture design, finding that optimal accuracy is typically
achieved by balancing width and depth (Zagoruyko & Komodakis, 2016; Tan & Le, 2019). Further
study of accuracy and error sets have been conducted in (Hacohen & Weinshall, 2020) (error sets
over training), and (Hooker et al., 2019) (error after pruning). Other work has demonstrated that it
is often possible for narrower or shallower neural networks to attain similar accuracy to larger net-
works when the smaller networks are trained to mimic the larger networks’ predictions (Ba & Caru-
ana, 2014; Romero et al., 2015). We instead seek to study the impact of width and depth on network
internal representations and (per-example) outputs, by applying techniques for measuring similarity
of neural network hidden representations (Kornblith et al., 2019; Raghu et al., 2017a; Morcos et al.,
2018). These techniques have been very successful in analyzing deep learning, from properties of
neural network training (Gotmare et al., 2018; Neyshabur et al., 2020), objectives (Resnick et al.,
2019; Thompson et al., 2019; Hermann & Lampinen, 2020), and dynamics (Maheswaranathan et al.,
2019) to revealing hidden linguistic structure in large language models (Bau et al., 2019; Kudugunta
et al., 2019; Wu et al., 2019; 2020) and applications in neuroscience (Shi et al., 2019; Li et al., 2019;
Merel et al., 2019; Zhang & Bellec, 2020) and medicine (Raghu et al., 2019).
3	Experimental Setup and Background
Our goal is to understand the effects of depth and width on the function learned by the underlying
neural network, in a setting representative of the high performance models used in practice. Re-
flecting this, our experimental setup consists of a family of ResNets (He et al., 2016; Zagoruyko
& Komodakis, 2016) trained on standard image classification datasets CIFAR-10, CIFAR-100 and
ImageNet.
For standard CIFAR ResNet architectures, the network’s layers are evenly divided between three
stages (feature map sizes), with numbers of channels increasing by a factor of two from one stage to
the next. We adjust the network’s width and depth by increasing the number of channels and layers
respectively in each stage, following Zagoruyko & Komodakis (2016). For ImageNet ResNets,
2
Published as a conference paper at ICLR 2021
ResNet-50 and ResNet-101 architectures differ only by the number of layers in the third (14 × 14)
stage. Thus, for experiments on ImageNet, we scale only the width or depth of layers in this stage.
More details on training parameters, as well as the accuracies of all investigated models, can be
found in Appendix B.
We observe that increasing depth and/or width indeed yields better-performing models. However,
we will show in the following sections how they exhibit characteristic differences in internal repre-
sentations and outputs, beyond their comparable accuracies.
3.1	Measuring Representational Similarity Using Minibatch CKA
Neural network hidden representations are challenging to analyze for several reasons including (i)
their large size; (ii) their distributed nature, where important features in a layer may rely on multiple
neurons; and (iii) lack of alignment between neurons in different layers. Centered kernel alignment
(CKA) (Kornblith et al., 2019; Cortes et al., 2012) addresses these challenges, providing a robust way
to quantitatively study neural network representations by computing the similarity between pairs of
activation matrices. Specifically, we use linear CKA, which Kornblith et al. (2019) have previously
validated for this purpose, and adapt it so that it can be efficiently estimated using minibatches. We
describe both the conventional and minibatch estimators of CKA below.
Let X ∈ Rm×p1 and Y ∈ Rm×p2 contain representations of two layers, one with p1 neurons and
another p2 neurons, to the same set of m examples. Each element of the m × m Gram matrices
K = XXT and L = Y Y T reflects the similarities between a pair of examples according to
the representations contained in X or Y. Let H = In - 111T be the centering matrix. Then
K0 = HKH and L0 = HLH reflect the similarity matrices with their column and row means
subtracted. HSIC measures the similarity of these centered similarity matrices by reshaping them to
vectors and taking the dot product between these vectors, HSIC°(K, L) = Vec(K0) ∙ vec(L0)∕(m -
1)2 . HSIC is invariant to orthogonal transformations of the representations and, by extension, to
permutation of neurons, but it is not invariant to scaling of the original representations. CKA further
normalizes HSIC to produce a similarity index between 0 and 1 that is invariant to isotropic scaling,
CKA(K, L)
HSIC0(K,L)
√HSICo(K, K)HSICo(L, L)
(1)
Kornblith et al. (2019) show that, when measured between layers of architecturally identical net-
works trained from different random initializations, linear CKA reliably identifies architecturally
corresponding layers, whereas several other proposed representational similarity measures do not.
However, naive computation of linear CKA requires maintaining the activations across the entire
dataset in memory, which is challenging for wide and deep networks. To reduce memory consump-
tion, we propose to compute linear CKA by averaging HSIC scores over k minibatches:
CKAminibatch
k Pk=I HSICI(XiXT, YiYT)
1P Pk=1 HSIC1 (XiXT, XiXT) ,1 Pk=I HSICI(YiYT, YiYT)
(2)
where Xi ∈ Rn×p1 and Yi ∈ Rn×p2 are now matrices containing activations of the ith minibatch of
n examples sampled without replacement. In place of HSIC0, which is a biased estimator of HSIC,
we use an unbiased estimator of HSIC (Song et al., 2012) so that the value of CKA is independent
of the batch size:
1
HSICI(K，L) = n(n-3)	tr(KL) +
ITKIITiLI
(n - 1)(n - 2) n - 2
1TKLiL1
(3)
2
where K and L are obtained by setting the diagonal entries of similarity matrices K and L to zero.
This approach of estimating HSIC based on minibatches is equivalent to the bagging block HSIC
approach of Yamada et al. (2018), and converges to the same value as if the entire dataset were
considered as a single minibatch, as proven in Appendix A. We use minibatches of size n = 256
obtained by iterating over the test dataset 10 times, sampling without replacement within each time.
4	Depth, Width and Model Internal Representations
We begin our study by investigating how the depth and width of a model architecture affects its in-
ternal representation structure. How do representations evolve through the hidden layers in different
3
Published as a conference paper at ICLR 2021
architectures? How similar are different hidden layer representations to each other? To answer these
questions, we use the CKA representation similarity measure outlined in Section 3.1.
We find that as networks become wider and/or deeper, their representations show a characteristic
block structure: many (almost) consecutive hidden layers that have highly similar representations.
By training with reduced dataset size, we pinpoint a connection between block structure and model
overparametrization — block structure emerges in models that have large capacity relative to the
training dataset.
4.1	Internal Representations and the Block Structure
CIFAR-10
Wider--------------------------------------------------------------------------------------------------->
ResNet-38 4× ResNet-38 8× ResNet-38 10×
ResNet-381 ×
ResNet-382×
O 50	100 O
oυ
50	100
Layer
O
50	100
O
50	100
1/16 ImageNet
CIFAR-100
O 50	100 O 50	100
Layer
Layer
0.5
CKA
1.0
100
50
ReSNet-50
O 50	100
Layer
ReSNet-50
O 50	100
Layer
O
Figure 1: Emergence of the block structure with increasing width or depth. As we increase the depth or
width of neural networks, we see the emergence of a large, contiguous set of layers with very similar represen-
tations — the block structure. Each of the panes of the figure computes the CKA similarity between all pairs of
layers in a single neural network and plots this as a heatmap, with x and y axes indexing layers. See Appendix
Figure C.1 for block structure in wide networks without residual connections.
In Figure 1, we show the results of training ResNets of varying depths (top row) and widths (bottom
row) on CIFAR-10. For each ResNet, we use CKA to compute the representation similarity of all
pairs of layers within the same model. Note that the total number of layers is much greater than the
stated depth of the ResNet, as the latter only accounts for the convolutional layers in the network
but we include all intermediate representations. We can visualize the result as a heatmap, with the x
and y axes representing the layers of the network, going from the input layer to the output layer.
The heatmaps start off as showing a checkerboard-like representation similarity structure, which
arises because representations after residual connections are more similar to other post-residual rep-
resentations than representations inside ResNet blocks. As the model gets wider or deeper, we see
the emergence of a distinctive block structure — a considerable range of hidden layers that have
very high representation similarity (seen as a yellow square on the heatmap). This block structure
mostly appears in the later layers (the last two stages) of the network. We observe similar results in
networks without residual connections (Appendix Figure C.1).
4
Published as a conference paper at ICLR 2021
All Training Data
1/4 Training Data
1/16 Training Data
Layer
Figure 2: Block structure emerges in narrower networks when trained on less data. We plot CKA Simi-
larity heatmaps as we increase network width (going right along each row) and also decrease the dataset size
(down each column). AS a reSult of the increaSed model capacity (with reSpect to the taSk) from Smaller dataSet
Size, Smaller (narrower) modelS now alSo exhibit the block Structure.
Block structure across random seeds: In Appendix Figure D.1, we plot CKA heatmapS acroSS
multiple random SeedS of a deep network and a wide network. We obServe that while the exact Size
and poSition of the block Structure can vary, it iS preSent acroSS all training runS.
4.2	The Block Structure and Model Overparametrization
Having obServed that the block Structure emergeS aS modelS get deeper and/or wider (Figure 1), we
next Study whether block Structure iS a reSult of thiS increaSe in model capacity — namely, iS block
Structure connected to the absolute model Size, or to the Size of the model relative to the Size of the
training data?
Commonly uSed neural networkS have many more parameterS than there are exampleS in their train-
ing SetS. However, even within thiS overparameterized regime, larger networkS frequently achieve
higher performance on held out data (Zagoruyko & KomodakiS, 2016; Tan & Le, 2019). ThuS, to
explore the connection between relative model capacity and the block Structure, we fix a model ar-
chitecture, but decrease the training dataSet Size, which ServeS to inflate the relative model capacity.
The reSultS of thiS experiment with varying network widthS are Shown in Figure 2, while the cor-
reSponding plot with varying network depthS (which SupportS the Same concluSionS) can be found
in Appendix Figure D.2. Each column of Figure 2 ShowS the internal repreSentation Structure of a
fixed architecture aS the amount of training data iS reduced, and we can clearly See the emergence
of the block Structure in narrower (lower capacity) networkS aS leSS training data iS uSed. Refer to
FigureS D.3 and D.4 in the Appendix for a Similar Set of experimentS on CIFAR-100. Together,
theSe obServationS indicate that block Structure in the internal repreSentationS ariSeS in modelS that
are heavily overparameterized relative to the training dataSet.
5	Probing the Block Structure
In the previouS Section, we Show that wide and/or deep neural networkS exhibit a block Structure in
the CKA heatmapS of their internal repreSentationS, and that thiS block Structure ariSeS from the large
5
Published as a conference paper at ICLR 2021
ResNet-110(1×)
Layer
Layer
Figure 3: Block structure arises from preserving and propagating the (dominant) first principal compo-
nent of the layer representations. Above are two sets of four plots, for layers of a deep network (left) and
a wide network (right). CKA of the representations (top right), shows block structure in both networks. By
comparing this to the variance explained by the top principal component of each layer representation (bottom
left), we see that layers in the block structure have a highly dominant first principal component. This principal
component is also preserved throughout the block structure, seen by comparing the squared cosine similarity of
the first principal component across pairs of layers (top left), to the CKA representation similarity (top right).
Compared to the latter, after removing the first principal component from the representations (bottom right), the
block structure is highly reduced — the block structure arises from propagating the first principal component.
capacity of the models in relation to the learned task. While this latter result provides some insight
into the block structure, there remains a key open question, which this section seeks to answer: what
is happening to the neural network representations as they propagate through the block structure?
Through further analysis, we show that the block structure arises from the preservation and propaga-
tion of the first principal component of its constituent layer representations. Additional experiments
with linear probes (Alain & Bengio, 2016) further support this conclusion and show that some layers
that make up the block structure can be removed with minimal performance loss.
5.1	The Block Structure and The First Principal Component
For centered matrices of activations X ∈ Rn×p1 , Y ∈ Rn×p2 , linear CKA may be written as:
CKA(XX t,YYt) = Pp=1 Pp=1XX λYh UXj2
PPP=ι(λχ)2 JPp=ι(λγ )2
(4)
where uiX ∈ Rn and uiY ∈ Rn are the ith normalized principal components of X and Y and λiX
and λiY are the corresponding squared singular values (Kornblith et al., 2019). As the fraction of
the variance explained by the first principal components approaches 1, CKA reflects the squared
alignment between these components hu1X, u1Y i2. We find that, in networks with a visible block
structure, the first principal component explains a large fraction of the variance, whereas in networks
with no visible block structure, it does not (Appendix Figure D.5), suggesting that the block structure
reflects the behavior of the first principal component of the representations.
Figure 3 explores this relationship between the block structure and the first principal components
of the corresponding layer representations, demonstrated on a deep network (left group) and a wide
network (right group). By comparing the variance explained by the first principal component (bot-
tom left) to the location of the block structure (top right) we observe that layers belonging to the
block structure have a highly dominant first principal component. Cosine similarity of the first prin-
cipal components across all pairs of layers (top left) also shows a similarity structure resembling the
block structure (top right), further demonstrating that the principal component is preserved through-
out the block structure. Finally, removing the first principal component from the representations
6
Published as a conference paper at ICLR 2021
ResNet-26 (2×)
50
0
100
75
j5°
25
0 25 50 75
Layer
ResNet-26 (2×)
1.0
0.8
0.6 g
0.4 0
0.2
0.0
Figure 4: Linear probe accu-
racy. Top: CKA between lay-
ers of individual ResNet mod-
els, for different architectures
and initializations. Bottom:
Accuracy of linear probes for
each of the layers before (or-
ange) and after (blue) the resid-
ual connections.
0 25 50 75
Layer
Figure 5: Effect of deleting
blocks on accuracy for mod-
els with and without block
structure. Blue lines show
the effect of deleting blocks
backwards one-by-one within
each ResNet stage. (Note the
plateau at the block structure.)
Vertical green lines reflect
boundaries between ResNet
stages. Horizontal gray line
reflects accuracy of the full
model.
nearly eliminates the block structure from the CKA heatmaps (bottom right). A full picture of how
this process impacts models of increasing depth and width can be found in Appendix Figure D.6.
In contrast, for models that do not contain the block structure, we find that cosine similarity of the
first principal components across all pairs of layers bears little resemblance to the representation
similarity structure measured by CKA, and the fractions of variance explained by the first principal
components across all layers are relatively small (see Appendix Figure D.7). Together these re-
sults demonstrate that the block structure arises from preserving and propagating the first principal
component across its constituent layers.
Although layers inside the block structure have representations with high CKA and similar first
principal components, each layer nonetheless computes a nonlinear transformation of its input. Ap-
pendix Figure D.8 shows that the sparsity of ReLU activations inside and outside of the block struc-
ture is similar. In particular, ReLU activations in the block structure are sometimes in the linear
regime and sometimes in the saturating regime, just like activations elsewhere in the network.
5.2	Linear Probes and Collapsing the Block Structure
With the insight that the block structure is preserving key components of the representations, we next
investigate how these preserved representations impact task performance throughout the network,
and whether the block structure can be collapsed in a way that minimally affects performance.
In Figure 4, we train a linear probe (Alain & Bengio, 2016) for each layer of the network, which
maps from the layer representation to the output classes. In models without the block structure
(first 2 panes), we see a monotonic increase in accuracy throughout the network, but in models with
the block structure (last 2 panes), linear probe accuracy shows little improvement inside the block
structure. Comparing the accuracies of probes for layers pre- and post-residual connections, we find
that these connections play an important role in preserving representations in the block structure.
Informed by these results, we proceed to pruning blocks one-by-one from the end of each residual
stage, while keeping the residual connections intact, and find that there is little impact on test ac-
curacy when blocks are dropped from the middle stage (Figure 5), unlike what happens in models
without block structure. When compared across different seeds, the magnitude of the drop in accu-
racy appears to be connected to the size and the clarity of the block structure present. This result
suggests that block structure could be an indication of redundant modules in model design, and that
the similarity of its constituent layer representations could be leveraged for model compression.
7
Published as a conference paper at ICLR 2021
Layer Layer Layer
Layer Layer Layer
Layer Layer Layer
Figure 6: Representations within “block structure” differ across initializations. Each group of plots shows
CKA between layers of models with the same architecture but different initializations (off the diagonal) or
within a single model (on the diagonal). For narrow, shallow models such as ResNet-38 (1×), there is no block
structure, and CKA across initializations closely resembles CKA within a single model. For wider (middle)
and deeper (right) models, representations within the block structure are unique to each model.
6	Depth and Width Effects on Representations Across Models
The results of the previous sections help characterize effects of varying depth and width on a (single)
model’s internal representations, specifically, the emergence of the block structure with increased
capacity, and its impacts on how representations are propagated through the network. With these
insights, we next look at how depth and width affect the hidden representations across models.
Concretely, are learned representations similar across models of different architectures and different
random initializations? How is this affected as model capacity is changed?
We begin by studying the variations in representations across different training runs of the same
model architecture. Figure 6 illustrates CKA heatmaps for a smaller model (left), wide model (mid-
dle) and deep model (right), trained from random initializations. The smaller model does not have
the block structure, and representations across seeds (off diagonal plots) exhibit the same grid-like
similarity structure as within a single model. The wide and deep models show block structure in
all their seeds (as seen in plots along the diagonal), and comparisons across seeds (off-diagonal
plots) show that while layers not in the block structure exhibit some similarity, the block structure
representations are highly dissimilar across models.
Appendix Figure E.1 shows results of comparing CKA across different architectures, controlled for
accuracy. Wide and deep models without the block structure do exhibit representation similarity
with each other, with corresponding layers broadly being of the same proportional depth in the
model. However, similar to what we observe in Figure 6, the block structure representations remain
unique to each model.
7	Depth, Width and Effects on Model Predictions
To conclude our investigation on the effects of depth and width, we turn to understanding how the
characteristic properties of internal representations discussed in the previous sections influence the
outputs of the model. How diverse are the predictions of different architectures? Are there examples
that wide networks are more likely to do well on compared to deep networks, and vice versa?
By training populations of networks on CIFAR-10 and ImageNet, we find that there is considerable
diversity in output predictions at the individual example level, and broadly, architectures that are
more similar in structure have more similar output predictions. On ImageNet we also find that there
are statistically significant differences in class-level error rates between wide and deep models, with
the former exhibiting a small advantage in identifying classes corresponding to scenes over objects.
Figure 7a compares per-example accuracy for groups of 100 architecturally identical deep models
(ResNet-62) and wide models (ResNet-14 (2×)), all trained from different random initializations on
CIFAR-10. Although the average accuracy of these groups is statistically indistinguishable, they
tend to make different errors, and differences between groups are substantially larger than expected
8
Published as a conference paper at ICLR 2021
0	50	100
ResNet-62 (1 ×) Accuracy (%)
6 4 2 O-2
C (60ual6-α Aoalnoov
honeycomb
ResNet-83 - ResNet-83 (Control)
ResNet-50 (2.8×) - ResNet-83
mud turtle	Labrador retriever
. ,	black and gold
trash caπ	garden spider
ram
20	40	60	80	100
ResNet-83 Accuracy (%)
Figure 7: Systematic per-example and per-class performance differences between wide and deep models.
a: Comparison of accuracy on individual examples for 100 ResNet-62 (1×) and ResNet-14 (2×) models, which
have statistically indistinguishable accuracy on the CIFAR-10 test set. b: Same as (a), for disjoint sets of 100
architecturally identical ResNet-62 models trained from different initializations. See Figure F.1 for a similar
plot for ResNet-14 (2×) models. c: Accuracy differences on ImageNet classes for ResNets between models
with increased width (y-axis) or depth (x-axis) in the third stage. Orange dots reflect difference between two
sets of 50 architecturally identical deep models (i.e., different random initializations of ResNet-83).
by chance (Figure 7b). Examples of images with large accuracy differences are shown in Appendix
Figure F.1, while Appendix Figures F.2 and F.3 further explore patterns of example accuracy for
networks of different depths and widths, respectively. As the architecture becomes wider or deeper,
accuracy on many examples increases, and the effect is most pronounced for examples where smaller
networks were often but not always correct. At the same time, there are examples that larger net-
works are less likely to get right than smaller networks. We show similar results for ImageNet
networks in Appendix Figure F.4.
We next ask whether wide and deep ImageNet models have systematic differences in accuracy at
the class level. As shown in Figure 7c, there are small but statistically significant differences in
accuracy for 419/1000 classes (p < 0.05, Welch’s t-test), accounting for 11% of the variance in
the differences in example-level accuracy (see Appendix F.3). Three of the top 5 classes that are
more likely to be correctly classified by wide models reflect scenes rather than objects (seashore,
library, bookshop). Indeed, the wide architecture is significantly more accurate on the 68 ImageNet
classes descending from “structure” or “geological formation” (74.9% ± 0.05 vs. 74.6% ± 0.06,
p = 6 × 10-5, Welch’s t-test). Looking at synsets containing > 50 ImageNet classes, the deep
architecture is significantly more accurate on the 62 classes descending from “consumer goods”
(72.4% ± 0.07 vs. 72.1% ± 0.06, p = 0.001; Table F.2). In other parts of the hierarchy, differences
are smaller; for instance, both models achieve 81.6% accuracy on the 118 dog classes (p = 0.48).
8	Conclusion
In this work, we study the effects of width and depth on neural network representations. Through
experiments on CIFAR-10, CIFAR-100 and ImageNet, we have demonstrated that as either width
or depth increases relative to the size of the dataset, analysis of hidden representations reveals the
emergence of a characteristic block structure that reflects the similarity of a dominant first principal
component, propagated across many network hidden layers. Further analysis finds that while the
block structure is unique to each model, other learned features are shared across different initializa-
tions and architectures, particularly across relative depths of the network. Despite these similarities
in representational properties and performance of wide and deep networks, we nonetheless observe
that width and depth have different effects on network predictions at the example and class levels.
There remain interesting open questions on how the block structure arises through training, and
using the insights on network depth and width to inform optimal task-specific model design.
9
Published as a conference paper at ICLR 2021
Acknowledgements
We thank Gamaleldin Elsayed for helpful feedback on the manuscript.
References
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. arXiv preprint arXiv:1610.01644, 2016.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural informa-
tion processing Systems,pp. 2654-2662, 2014.
Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.
Identifying and controlling important neurons in neural machine translation. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=H1z-PsR5KX.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based
on centered alignment. The Journal of Machine Learning Research, 13(1):795-828, 2012.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Bradley Efron. Regression and anova with zero-one data: Measures of residual variation. Journal
of the American Statistical Association, 73(361):113-121, 1978.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint
arXiv:1810.13243, 2018.
Guy Hacohen and Daphna Weinshall. Let’s agree to agree: Neural networks share classification
order on real datasets. In ICML, 2020.
Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Katherine L Hermann and Andrew K Lampinen. What shapes feature representations? exploring
datasets, architectures, and training. arXiv preprint arXiv:2006.12433, 2020.
Sara Hooker, Aaron Courville, Yann Dauphin, and Andrea Frome. Selective brain damage: Mea-
suring the disparate impact of model pruning. arXiv preprint arXiv:1911.05248, 2019.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In ICML, 2019.
10
Published as a conference paper at ICLR 2021
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Sneha Reddy Kudugunta, Ankur Bapna, Isaac Caswell, Naveen Arivazhagan, and Orhan Firat. In-
vestigating multilingual nmt representations at scale. arXiv preprint arXiv:1909.02197, 2019.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.
Zhe Li, Wieland Brendel, Edgar Walker, Erick Cobos, Taliah Muhammad, Jacob Reimer, Matthias
Bethge, Fabian Sinz, Zachary Pitkow, and Andreas Tolias. Learning from brains how to regularize
machines. In Advances in Neural Information Processing Systems, pp. 9529-9539, 2019.
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approxi-
mator. In Advances in neural information processing systems, pp. 6169-6178, 2018.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in neural information processing systems,
pp. 6231-6239, 2017.
Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Uni-
versality and individuality in neural dynamics across large populations of recurrent networks. In
Advances in neural information processing systems, pp. 15629-15641, 2019.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, and Bence Olveczky. Deep
neuroethology of a virtual rodent. arXiv preprint arXiv:1911.09451, 2019.
Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. In Advances in Neural Information Processing Systems, pp.
5727-5736, 2018.
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learn-
ing? arXiv preprint arXiv:2008.11687, 2020.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8(1):
143-195, 1999.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Advances in
Neural Information Processing Systems, pp. 6076-6085, 2017a.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neural networks. In international conference on machine learning, pp.
2847-2854. PMLR, 2017b.
Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning for medical imaging. In Advances in neural information processing systems, pp.
3347-3357, 2019.
11
Published as a conference paper at ICLR 2021
Cinjon Resnick, Zeping Zhan, and Joan Bruna. Probing the state of the art: A critical look at visual
representation evaluation. arXiv preprint arXiv:1912.00215, 2019.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.
Jianghong Shi, Eric Shea-Brown, and Michael Buice. Comparison against task driven artificial
neural networks reveals functional properties in mouse visual cortex. In Advances in Neural
Information Processing Systems,pp. 5764-5774, 2019.
Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via
dependence maximization. The Journal of Machine Learning Research, 13(1):1393-1434, 2012.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105-6114, 2019.
Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint
arXiv:1509.08101, 2015.
Jessica AF Thompson, Yoshua Bengio, and Marc Schoenwiesner. The effect of task and training on
intermediate representations in convolutional neural networks revealed with modified rv similarity
analysis. arXiv preprint arXiv:1912.02260, 2019.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems, pp. 9712-9724, 2019.
John M Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Simi-
larity analysis of contextual word representation models. arXiv preprint arXiv:2005.01172, 2020.
Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging cross-
lingual structure in pretrained language models. arXiv preprint arXiv:1911.01464, 2019.
Makoto Yamada, Yuta Umezu, Kenji Fukumizu, and Ichiro Takeuchi. Post selection inference with
kernels. In International Conference on Artificial Intelligence and Statistics, pp. 152-160. PMLR,
2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference (BMVC), pp. 87.1-87.12, September 2016.
Yu Zhang and Pierre Bellec. Transferability of brain decoding using graph convolutional networks.
bioRxiv, 2020.
12
Published as a conference paper at ICLR 2021
Appendix
A Convergence of Minibatch HSIC
Proposition 1. Let K ∈ Rm×m and L ∈ Rm×m be two kernel matrices constructed by apply-
ing kernel functions k and l respectively to all pairs of examples in a dataset D. Form c random
partitionings P of D into m/n minibatches b of size n, and let KKb,p ∈ Rn×n and Lb,p ∈ Rn×n
be kernel matrices constructed by applying kernels k and l to all pairs of examples within each
minibatch. Define U0 = HSIC1(K , L), the value of HSIC1 applied to the full dataset, and
Up = mn P m/n HSICI(K b,p, Lb,p) ,the average value of HSIC1 over the minibatches in parti-
tioning (epoch) p. Then C Pp=1 Up -→ Uo as C → ∞.
Proof. Let i4m be the set of all 4-tuples of indices between 1 and m where each index occurs exactly
once. As proven in Theorem 3 of Song et al. (2012), U0 is a U-statistic:
Uo = HSICι(K, L) = (m -4)! X h(KS, LS),	⑸
m!
S∈i4m
where K(i,j,q,r) = (Ki,j, Ki,q, Ki,r, Kj,q, Kj,r, Kq,r) and the kernel of the U-statistic h is defined
in Song et al. (2012). Let δSb be 1 if the 4-tuple of dataset indices S is selected in minibatch b and 0
otherwise. Then:
(n - 4)!
n!
m/n
m X X δSh(Ks,Ls).
b=1 S∈i4m
(6)
Taking the expectation with respect to δ, and noting that δ is independent of h(KS, LS),
Eδ [Up] = (n-4! m X X Eδ [δSh(Ks,Ls)]
n m b=1 S∈i4m
J1 - Xn X Eδ [δS] h(Ks,Ls).
n! m
b=1 S∈i4m
(7)
(8)
By symmetry, Eδ δSb is the same for all example and batch indices. Specifically, there are n!/(n -
4)! 4-tuples that can be formed from each batch and m!/(m - 4)! 4-tuples that can be formed
from the entire dataset, so the probability that a given 4-tuple is in a given batch is Eδ δSb =
(n!/(n - 4)!)/(m!/(m - 4)!). Thus:
Eδ[Up] = (m -,4)! X h(Ks, LS) = Uo.	(9)
m!
S∈i4m
The minibatch indicators δSb are either 0 or 1, so their variances and covariances are bounded, and
the weighted sum in Eq. 6 has finite variance. Thus, by the law of large numbers, C Pp=1 Up -→ Uo
as p → ∞.	□
B Training Details
Our CIFAR-10 and CIFAR-100 networks follow the same architecture as He et al. (2016);
Zagoruyko & Komodakis (2016). We train a set of models where we fix the width multiplier of
deep networks to 1 and experiment with models of depths 32, 44, 56, 110, 164. On CIFAR-100,
the block structure only appears at a greater depth so we also include depths 218 and 224 in our
investigation. For wide networks, we examine width multipliers of 1, 2, 4, 8 and 10 and depths
of 14, 20, 26, and 38. We use SGD with momentum of 0.9, together with a cosine decay learning
rate schedule and batch size of 128, to train each model for 300 epochs. Models are trained with
standard CIFAR-10 data augmentation comprising random flips and translations of up to 4 pixels.
13
Published as a conference paper at ICLR 2021
Each depth and width configuration is trained with 10 different seeds for CKA analysis, and 200
seeds for model predictions comparison.
On ImageNet, we start with the ResNet-50 architecture and increase depth or width in the third stage
only, following the scaling approach of (He et al., 2016). We train for 120 epochs using SGD with
momentum of 0.9 and a cosine decay learning rate schedule at a batch size of 256. We use 100 seeds
for model prediction comparison.
For experiments with reduced dataset size, we subsample the training data from the original CIFAR
training set by the corresponding proportion, keeping the number of samples for each class the same.
All CKA results are then computed based on the full CIFAR test set.
Table B.1: Accuracy of examined neural networks on CIFAR-10 and CIFAR-100.
3244561064
Depth Width CIFAR-10 Test CIFAR-100 Test
Accuracy (%)	Accuracy (%)
1	93.5	712
1	94.0	72.0
1	94.2	73.3
1	94.3	74.0
1	94.4	73.9
14	1	92.0	67.8
14	2	94.1	72.9
14	4	95.4	77.0
14	8	95.9	80.0
14	10	96.0	80.2
20	1	92.8	69.4
20	2	94.6	74.4
20	4	95.4	77.6
20	8	96.0	80.2
20	10	95.8	80.8
26	1	93.3	70.5
26	2	94.9	75.8
26	4	95.6	79.3
26	8	95.9	80.9
26	10	95.8	81.0
38	1	93.8	72.3
38	2	95.1	75.9
38	4	95.5	78.6
38	8	95.7	79.8
38	10	95.7	80.5
14
Published as a conference paper at ICLR 2021
C Block Structure in a Different Architecture
Figure C.1: Block structure also appears in models without residual connections. We remove residual
connections from existing CIFAR-10 ResNets and plot CKA heatmaps for layers in the resulting architecture
after training. Since the lack of residual connections prevents deep networks from performing well on the task,
here we only show the representational similarity for models of increasing width. As previously observed in
Figure 1, the block structure emerges in higher capacity models.
D Probing the Block Structure
ResNet 110 1 ×
1.0
300
0.8
0.6
罪200
100
0.4
100
0.2
0.0
0
0 100 200 300	0 100 200 300	0 100 200 300	0 100 200 300	0 100 200 300
ResNet-38 10×
Figure D.1: Block structure varies across random initializations. We plot CKA heatmaps as in Figure 1 for
5 random seeds of a deep model (top row) and a wide model (bottom row) trained on CIFAR-10. While the
size and position vary, the block structure is clearly visible in all seeds.
15
Published as a conference paper at ICLR 2021
All Training Data
1/4 Training Data
1/16 Training Data
Layer
^~IIQ
Figure D.2: Block structure emerges in shallower networks when trained on less data (CIFAR-10). We
plot CKA similarity heatmaps as we increase network depth (going right along each row) and also decrease
the size (down each column) of training data. Similar to the observation made in Figure 2, as a result of the
increased model capacity (with respect to the task) from smaller dataset size, smaller (shallower) models now
also exhibit the block structure.
All Training Data
ResNet-218 1 ×
0.8
1/2 Training Data
ResNet-218 1 ×
ResNet-224 1 ×
0.6
o
0.4
0.2
1/4 Training Data
Layer
0 250 500 750
0 250 500 750
Figure D.3: Block structure emerges in shallower networks when trained on less data (CIFAR-100). We
plot CKA similarity heatmaps as We increase network depth (going right along each row) and also decrease
the size of training data (down each column). Similar to the observation made in Figure 2, as a result of the
increased model capacity (with respect to the task) from smaller dataset size, smaller (shallower) models now
also exhibit the block structure.
16
Published as a conference paper at ICLR 2021
ResNet-38 1 ×
ResNet-38 8×
ResNet-38 10×
All Training Data
ResNet-38 4×
50	100
100
φ
宙
-j 50
1.0
ResNet-38 10×
50	100
50	100
50	100
50	100
ResNet-38 2×
0.8
ResNet-38 1 ×
ResNet-38 2×
1/2 Training Data
ResNet-38 4×
ResNet-38 8×

0.6
3
O
0.4
O
100
I
-j 50
0.2
0.0
Layer
Figure D.4: Block structure emerges in narrower networks when trained on less data (CIFAR-100). We
plot CKA similarity heatmaps as we increase network width (going right along each row) and also decrease
the size (down each column) of training data. Similar to the observation made in Figure 2, as a result of the
increased model capacity (with respect to the task) from smaller dataset size, smaller (narrower) models now
also exhibit the block structure.
17
Published as a conference paper at ICLR 2021
1.0
0.8
0.6
0.4
0.2
0.0
100
50
100
50
0.54
0.49
0.36
0.27
0.22
0.58
0.55
0.40
0.40
100
100
100
100
100
尢/四 1 二 0∙40
Seed 1
Seed 2
Seed 3 Seed 4	Seed 5
Figure D.5: Top principal component explains a large fraction of variance in the activations of models
with block structure. Each row shows a different model configuration that is trained on CIFAR-10, with the
first 5 rows showing models of increasing depth, and the last 5 rows models of increasing width. Columns
correspond to different seeds. Each heatmap is labeled with the fraction of variance explained by the top
principal component of activations combined from the last 2 stages of the model (where block structure is often
found). Rows (seeds belonging to the same architecture) are sorted by decreasing value of the proportion of
variance explained. We observe that this variance measure is significantly higher in model seeds where the
block structure is present.
18
Published as a conference paper at ICLR 2021
1-1.0
0.8
0.6
0.4
0.2
0.0
Figure D.6: How the representational structure evolves with increasing depth and width when the first
principal component is removed. We plot CKA similarity heatmaps as models become deeper (top row) and
wider (bottom row), with the top principal component of their internal representations removed. Compared to
Figure 1, we observe that while this process significantly eliminates the block structure in large capacity models
(as also shown in Figure 3), it has negligible impact on the representational structures of smaller models (where
no block structure is present). The latter is not surprising, since the first principal component doesn’t account
for a large fraction of the variance in representations of these models, as demonstrated in Appendix Figure D.5
above.
ResNet-44 (1 ×)
20	40	20	40
Layer	Layer
ResNet-38 (2×)
Similarity of First PCs	CKA
10	20	30
10	20	30
Layer
Layer
30
120
S
10
1.00-
φ
o
I 0.75-
>
右 0.50-
⊂
⅞ 0.25-
co
亡
0.00-
Figure D.7: Relationship between the representation similarity structure and the first principal compo-
nent in networks without block structure. Above are two sets of four plots, for layers of a deep network
(left) and a wide network (right), that don’t contain block structure. In contrast to Figure 3, the first principal
component of each layer representation only accounts for a small fraction of variance in the representation
(bottom left). Comparing the squared cosine similarity of the first principal component across pairs of layers
(top left), to the CKA representation similarity (top right), we find that these two structures don’t resemble each
other. Last but not least, the representational structure of each model remains mostly unchanged after the first
principal component is removed (bottom right).
19
Published as a conference paper at ICLR 2021
ResNet-38 1 × ResNet-38 2× ResNet-38 4× ResNet-38 8× ResNet-38 10×
∏1.OO
0.75
。5噂
0.25
0.00
φ
SUO-W≥104 0」©Z 次
30-
20-
10-
100
80
60
40
20
0
10 20 30
10 20 30
Figure D.8: ReLU activations inside and outside the block structure are similarly sparse. To rule out
the possibility that the block structure arises because layers inside it behave linearly, we measured the sparsity
of the ReLU activations. We observe that a significant proportion of activations are always non-zero, and the
proportion is similar inside and outside the block structure. Thus, although layers inside the block structure
have similar representations, each layer still applies a nonlinear transformation to its input.
20
Published as a conference paper at ICLR 2021
E Representations Across Models
Z
CC l
200-
∣150-
CD
l?
N
CC
I 40
20
100
50
0
25
50 0 50 100150200
RN-14 (2×) Layer RN-164 Layer
RN-14 (2×) Layer RN-56 Layer
Figure E.1: Representations align between models of different widths and depths when no block struc-
ture is present. In each group of heatmaps, top left and bottom right show CKA within a single model trained
on CIFAR-10. Bottom left shows CKA for all pairs of layers between these (non-architecturally-identical)
models, which have similar test performance. In the absence of block structure (left group), representations at
the same relative depths are similar across models. But when comparison involves models with block structure
(right group), representations within the block structure are dissimilar to those of the other model.
F Example- and Class-Level Accuracy Differences
F.1 Effect of Varying Width and Depth on CIFAR- 1 0 Predictions
O 25	50	75	100
ResNet-62 (1 ×) Example Acc. (%)
O 25	50	75	100
ResNet-62 (1 ×) Example Acc. (%)
(s) .oo<qdIUBX山大里寸LJoNsθH
O 25	50	75	100
ResNet-14(2×) Example Acc. (%)
b ResNet-62 (1 ×): 87%	74%	64%	97%	98%	96%	86%	74%
ResNet-14 (2×): 22%	6%	8%	44%	44%	36%	43%	35%
Easier for
ResNet-62 (1×)
71%	60%
11%	13%
50%	55%
Easier for
ResNet-14 (2×)
ResNet-62 (1 ×): 20%
ResNet-14 (2×): 90%
23%	32%	39%	19%
Figure F.1: Systematic per-example performance differences between wide and deep models on CIFAR-
10. Comparison of predictions of 200 ResNet-62 (1×) and ResNet-14 (2×) models, which have statistically
indistinguishable accuracy on the CIFAR-10 test set (mean ± SEM 94.09 ± 0.01 vs. 94.08 ± 0.01, t(199) =
0.73, p = 0.47). a Scatter plots of per-example accuracy for 100 ResNet-14 (2×) models vs. 100 ResNet-
62 (1×) models (left) show substantially higher dispersion than corresponding plots for disjoint sets of 100
architecturally identical models trained from different initializations (middle and right). b: Examples with the
highest accuracy differences between the two types of models. Accuracies are reported on a subset of models
that is disjoint from those used to select the examples.
21
Published as a conference paper at ICLR 2021
Res Net-14 ResNet-20 ResNet-26
(91.69%)	(92.61 %)	(93.09%)
ResNet-32
(93.44%)
ResNet-38
(93.67%)
ResNet-56 ResNet-110 ResNet-164
(94.02%)	(94.33%)	(94.37%)
0	1000	1000	1000	1000	1000	1000	1000	100
ResNet-14 ResNet-20 ResNet-26 ResNet-32 ResNet-38 ResNet-56 ResNet-110 ResNet-164
Acc. (%) Acc. (%) Acc. (%) Acc. (%) Acc. (%) Acc. (%) Acc. (%) Acc. (%)
Figure F.2: Effect of depth on example accuracy. Scatter plots of per-example accuracies of ResNet models
with different depths on CIFAR-10. Blue dots indicate per-example accuracies of two groups of 100 networks
each with different architectures indicated by axes labels. Orange dots show the distribution for groups of
architecturally identical models, copied from the plot on the diagonal above. Accuracy of each model is shown
at the top.
22
Published as a conference paper at ICLR 2021
RN-26 1.0×
(93.09%)
RN-26 1.4×
(94.10%)
RN-26 1.7×
(94.53%)
RN-26 2.2× RN-26 2.4× RN-26 3.0× RN-26 4.0× RN-26 8.0×
(94.94%)	(95.05%)	(95.30%)	(95.53%)	(95.81%)
O 1000	1000	1000	1000	1000	1000	1000	100
RN-26 1.0×	RN-26 1.4×	RN-26 1.7×	RN-26 2.2×	RN-26 2.4×	RN-26 3.0×	RN-26 4.0×	RN-26 8.0×
Acc. (%)	Acc. (%) Acc. (%)	Acc. (%) Acc. (%)	Acc. (%) Acc. (%) Acc. (%)
Figure F.3: Effect of width on example accuracy. Scatter plots of per-example accuracies of ResNet models
with different widths on CIFAR-10. Blue dots indicate per-example accuracies of two groups of 100 networks
each with different architectures indicated by axes labels. Orange dots show the expected distribution for
groups of architecturally identical models, copied from the plot on the diagonal above. Accuracy of each model
is shown at the top.
23
Published as a conference paper at ICLR 2021
F.2 Effect of Varying Width and Depth on ImageNet Predictions
Res Net-5 O
(76.97%)
ResNet-83
(78.00%)
ResNet-50 (2.8×)
(77.97%)
(％) ∙84 (％) ∙004
OgJeNSB (x∞e) Ogj① NS ①Cc
(％) g<
Co0?一① NS ①B
Figure F.4: Systematic per-example performance differences between wide and deep models on Ima-
geNet. Scatter plots of per-example accuracy averaged across 50 vanilla ResNet-50 (1×) models versus that
for groups of 50 models with increased depth (6 → 17 blocks, “ResNet-83”) or width (2.8× wider) in the
3rd stage. Orange dots in plots show the expected distribution for two groups of 50 architecturally identical
models, copied from the plot on the diagonal above. The deeper and wider models have very similar but statis-
tically distinguishable accuracy (mean ± SEM for deeper model: 78.00 ± 0.01, wider model: 77.97 ± 0.01,
t(99) = 2.0,p= 0.047).
F.3 Effect Sizes for Class-Level Effects
We measure how much of the difference between the example-level predictions of the wide
and deep ImageNet ResNets in Section 7 could be explained by the classes to which they
belonged by fitting a set of three models. Model A attempts to model whether each pre-
diction was correct or incorrect as a linear combination factors corresponding to the exam-
ple ID and whether the prediction came from a wide or deep model (statsmodels for-
mula y ~ C(example_id) + C(Wide_or_deep)). Model B includes a factor Corre-
sponding to the example ID as well as factors corresponding to the interaction between
the class ID and the type of model the prediction came from (statsmodels formula
y ~ C(example_id) + C(Wide_or_deep) * C(Class_id)). Model C includes the
interaction between the example ID and the type of model, and corresponds to simply mea-
suring the average accuracy separately for both types of models (statsmodels formula
y ~ C(example_id) * C(Wide_or_deep)). Note that model C is nested inside model
B, which is nested inside model A.
We seek to measure how much of the variability in predictions that can be explained by model C but
not model A can be explained by model B. We fit these models with logistic regression and measure
the residual variance VarQ = En=1(yi - πi)2∕n, where n is the number of examples × the number
of models, yi is either 1 or 0 depending on whether the CNN’s prediction was correct or incorrect,
and πi is the output probability from logistic regression model Q. We then compute the squared
differences between y and the predictions of the logistic regression model:
2	VarA - VarB
V =VarA- VarC "皿
(10)
This approach is analogous to the pseudo-R2 of Efron (1978).
24
Published as a conference paper at ICLR 2021
Finally, we can compare the AIC values of the logistic regression models, shown in the table below.
Because the models are nested GLMs, we can also test for statistical significance using a χ2 test,
which is highly significant for each pair of nested models. We do not report p-values because they
are 0 to within machine precision.
Table F.1: AIC for models A, B, and C, described above
Model	AIC
A:	Example + Model	3011367
B:	Example + Class * Model 3006889
C:	Example * Model	2969410
F.4 Performance Differences Among ImageNet Synsets
Table F.2: Differeces between wide and deep architectures on ImageNet synsets with many classes. Com-
parison of accuracy of wide (ResNet-50 with 2.8× width in 3rd stage) and deep (ResNet-83) ImageNet models
on synsets with >50 classes. Note that some synsets are descendants (hyponyms) of others. p-values are com-
puted using a t-test with multiple testing (Holm-Sidak) correction. Results are for the sets of models used to
generate blue dots in Figures F.4 and 7. Post-selection effect sizes and testing in the main text use a disjoint set
of models.
Class	# Classes	Wide Acc.	Deep Acc.	Diff.	p-value
entity	1000	78.0 ± 0.01	78.0 ± 0.01	-0.03	0.89
physical entity	997	78.0 ± 0.01	78.0 ± 0.01	-0.03	0.89
object	958	78.1 ± 0.01	78.1 ± 0.01	-0.04	0.76
whole	949	78.2 ± 0.02	78.2 ± 0.01	-0.05	0.48
artifact	522	73.8 ± 0.02	73.8 ± 0.02	-0.01	1
living thing	410	83.5 ± 0.02	83.6 ± 0.02	-0.10	0.023
organism	410	83.5 ± 0.02	83.6 ± 0.02	-0.10	0.023
animal	398	83.3 ± 0.02	83.4 ± 0.02	-0.09	0.032
instrumentality	358	73.9 ± 0.03	74.0 ± 0.02	-0.02	1
vertebrate	337	83.3 ± 0.02	83.3 ± 0.02	-0.08	0.22
chordate	337	83.3 ± 0.02	83.3 ± 0.02	-0.08	0.22
mammal	218	82.0 ± 0.03	82.1 ± 0.03	-0.09	0.47
placental	212	81.9 ± 0.03	81.9 ± 0.03	-0.08	0.66
carnivore	158	81.1 ± 0.03	81.2 ± 0.03	-0.09	0.73
device	130	72.9 ± 0.05	72.9 ± 0.04	-0.00	1
canine	130	81.3 ± 0.03	81.4 ± 0.04	-0.04	1
domestic animal	123	81.0 ± 0.04	81.0 ± 0.04	-0.00	1
dog	118	81.6 ± 0.04	81.6 ± 0.04	-0.01	1
container	100	72.7 ± 0.05	72.7 ± 0.04	0.00	1
covering	90	72.0 ± 0.05	72.2 ± 0.05	-0.19	0.13
conveyance	72	83.5 ± 0.04	83.4 ± 0.05	0.13	0.65
vehicle	67	83.2 ± 0.04	83.1 ± 0.05	0.11	0.76
hunting dog	63	81.2 ± 0.05	81.2 ± 0.05	0.01	1
commodity	63	72.2 ± 0.06	72.6 ± 0.07	-0.42	5.1 × 10-5
consumer goods	62	72.3 ± 0.06	72.7 ± 0.07	-0.41	6.7 × 10-5
invertebrate	61	83.6 ± 0.05	83.8 ± 0.04	-0.16	0.37
bird	59	92.5 ± 0.04	92.7 ± 0.05	-0.21	0.0018
structure	58	75.9 ± 0.06	75.5 ± 0.07	0.42	5.7 × 10-5
matter	50	77.6 ± 0.05	77.4 ± 0.05	0.17	0.74
25