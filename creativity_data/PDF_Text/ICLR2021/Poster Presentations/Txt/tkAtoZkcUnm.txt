Published as a conference paper at ICLR 2021
Neural Thompson Sampling
Weitong Zhang
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA, USA, 90095
wt.zhang@ucla.edu
Lihong Li
Google Research
USA
lihong@google.com
Dongruo Zhou
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA, USA, 90095
drzhou@cs.ucla.edu
Quanquan Gu
Department of Computer Science
University of California, Los Angeles
Los Angeles, CA, USA, 90095
qgu@cs.ucla.edu
Ab stract
Thompson Sampling (TS) is one of the most effective algorithms for solving con-
textual multi-armed bandit problems. In this paper, we propose a new algorithm,
called Neural Thompson Sampling, which adapts deep neural networks for both
exploration and exploitation. At the core of our algorithm is a novel posterior dis-
tribution of the reward, where its mean is the neural network approximator, and
its variance is built upon the neural tangent features of the corresponding neu-
ral network. We prove that, provided the underlying reward function is bounded,
the proposed algorithm is guaranteed to achieve a cumulative regret of O(T 1/2),
which matches the regret of other contextual bandit algorithms in terms of total
round number T . Experimental comparisons with other benchmark bandit algo-
rithms on various data sets corroborate our theory.
1	Introduction
The stochastic multi-armed bandit (BUbeck & Cesa-Bianchi, 2012; Lattimore & Szepesvari, 2020)
has been extensively studied, as an important model to optimize the trade-off between exploration
and exploitation in sequential decision making. Among its many variants, the contextual bandit is
widely used in real-world applications such as recommendation (Li et al., 2010), advertising (Grae-
pel et al., 2010), robotic control (Mahler et al., 2016), and healthcare (Greenewald et al., 2017).
In each round of a contextual bandit, the agent observes a feature vector (the “context”) for each
of the K arms, pulls one of them, and in return receives a scalar reward. The goal is to maximize
the cumulative reward, or minimize the regret (to be defined later), in a total of T rounds. To do
so, the agent must find a trade-off between exploration and exploitation. One of the most effective
and widely used techniques is Thompson Sampling, or TS (Thompson, 1933). The basic idea is
to compute the posterior distribution of each arm being optimal for the present context, and sam-
ple an arm from this distribution. TS is often easy to implement, and has found great success in
practice (Chapelle & Li, 2011; Graepel et al., 2010; Kawale et al., 2015; Russo et al., 2017).
Recently, a series of work has applied TS or its variants to explore in contextual bandits with neu-
ral network models (Blundell et al., 2015; Kveton et al., 2020; Lu & Van Roy, 2017; Riquelme
et al., 2018). Riquelme et al. (2018) proposed NeuralLinear, which maintains a neural network and
chooses the best arm in each round according to a Bayesian linear regression on top of the last
network layer. Kveton et al. (2020) proposed DeepFPL, which trains a neural network based on
perturbed training data and chooses the best arm in each round based on the neural network out-
put. Similar approaches have also been used in more general reinforcement learning problem (e.g.,
Azizzadenesheli et al., 2018; Fortunato et al., 2018; Lipton et al., 2018; Osband et al., 2016a). De-
spite the reported empirical success, strong regret guarantees for TS remain limited to relatively
simple models, under fairly restrictive assumptions on the reward function. Examples are linear
1
Published as a conference paper at ICLR 2021
functions (Abeille & Lazaric, 2017; Agrawal & Goyal, 2013; Kocak et al., 2014; RUsso & Van Roy,
2014), generalized linear functions (Kveton et al., 2020; Russo & Van Roy, 2014), or functions with
small RKHS norm induced by a properly selected kernel (Chowdhury & Gopalan, 2017).
In this paper, we provide, to the best of our knowledge, the first near-optimal regret bound for neural
network-based Thompson Sampling. Our contributions are threefold. First, we propose a new algo-
rithm, Neural Thompson Sampling (NeuralTS), to incorporate TS exploration with neural networks.
It differs from NeuralLinear (Riquelme et al., 2018) by considering weight uncertainty in all layers,
and from other neural network-based TS implementations (Blundell et al., 2015; Kveton et al., 2020)
by sampling the estimated reward from the posterior (as opposed to sampling parameters).
Second, we give a regret analysis for the algorithm, and obtain an
O(d√T) regret,
where d is the
effective dimension and T is the number of rounds. This result is comparable to previous bounds
when specialized to the simpler, linear setting where the effective dimension coincides with the
feature dimension (Agrawal & Goyal, 2013; Chowdhury & Gopalan, 2017).
Finally, we corroborate the analysis with an empirical evaluation of the algorithm on several bench-
marks. Experiments show that NeuralTS yields competitive performance, in comparison with state-
of-the-art baselines, thus suggest its practical value in addition to strong theoretical guarantees.
Notation: Scalars and constants are denoted by lower and upper case letters, respectively. Vectors
are denoted by lower case bold face letters x, and matrices by upper case bold face letters A. We de-
note by [k] the set {1,2, ∙∙∙ ,k} for positive integers k. For two non-negative sequence {an}, {bn},
an = O (bn means that there exists a positive constant C such that anι ≤ Cbn and we use O(∙) to
hide the log factor in O(∙). We denote by ∣∣ ∙ k2 the Euclidean norm of vectors and the spectral norm
of matrices, and by ∣∣T∣f the Frobenius norm of a matrix.
2	Problem Setting and Proposed Algorithm
In this work, we consider contextual K-armed bandits, where the total number of rounds Tis known.
At round t ∈ [T], the agent observes K contextual vectors {xt,k ∈ Rd | k ∈ [K]}. Then the agent
selects an arm at and receives a reward rt,at. Our goal is to minimize the following pseudo regret:
RT = E]χcrtq - rt,at) ,	(2.I)
where aJ= is the optimal arm at round t that has the maximum expected reward: aJ= =
argmaxa∈[K] E[rt,a]. To estimate the unknown reward given a contextual vector x, we use a fully
connected neural network f(x; θ) of depth L ≥ 2, defined recursively by
f1 = W1 x,
fl = Wl ReLU(fl-1),	2≤l ≤L,
f(x; θ) = √mfL,	(2.2)
where ReLU(x) := max{x, 0}, mis the width of neural network, W1 ∈ Rm×d, Wl ∈ Rm×m, 2 ≤
l < L, WL ∈ R1×m, θ = (Vec(W1);…；Vec(WL)) ∈ Rp is the collection of parameters of the
neural network, P = dm + m2(L 一 2) + m, and g(x; θ) = Vθf (x; θ) is the gradient of f (x; θ)
w.r.t. θ.
Our Neural Thompson Sampling is given in Algorithm 1. It maintains a Gaussian distribution for
each arm’s reward. When selecting an arm, it samples the reward of each arm from the reward’s
posterior distribution, and then pulls the greedy arm (lines 4-8). Once the reward is observed, it
updates the posterior (lines 9 & 10). The mean of the posterior distribution is set to the output of the
neural network, whose parameter is the solution to the following minimization problem:
J
minL(θ) = XfXg； θ) - %」2/2 + mλ∣θ - θo∣2∕2.	(2.3)
i=1
We can see that (2.3) is an '2-regularized square loss minimization problem, where the regularization
term centers at the randomly initialized network parameter θ0 . We adapt gradient descent to solve
(2.3) with step size η and total number of iterations J.
2
Published as a conference paper at ICLR 2021
Algorithm 1 Neural Thompson Sampling (NeuralTS)
Input: Number of rounds T, exploration variance V, network width m, regularization parameter λ.
1:	Set U0 = λI
2:	Initialize θo = (Vec(W1);…；Vec(WL)) ∈ Rp, where for each 1 ≤ l ≤ L 一 1,
Wl = (W, 0; 0, W), each entry of W is generated independently from N(0, 4/m); WL =
(w>, -w>), each entry of W is generated independently f⅛om N(0, 2/m).
3:	for t = 1,…，T do
4:	for k = 1, ∙∙∙ ,K do
5:	σ2,k = λ g>(xt,k; θt-i) Ui-II g(xt,k； θt-i)/m
6:	Sample estimated reward et^ 〜N(f (xt,k； θt-ι),ν2◎：卜)
7:	end for
8:	Pull arm at and receive reward rt,at, where at = argmaxa ret,a
9:	Set θt to be the output of gradient descent for solving (2.3)
10:	Ut = Ut-1 + g(xt,at ； θt)g(xt,at ； θt)>∕m
11:	end for
A few observations about our algorithm are in place. First, compared to typical ways of implement-
ing Thompson Sampling with neural networks, NeuralTS samples from the posterior distribution of
the scalar reward, instead of the network parameters. It is therefore simpler and more efficient, as
the number of parameters in practice can be large.
Second, the algorithm maintains the posterior distributions related to parameters of all layers of the
network, as opposed to the last layer only (Riquelme et al., 2018). This difference is crucial in our
regret analysis. It allows us to build a connection between Algorithm 1 and recent work about deep
learning theory (Allen-Zhu et al., 2018; Cao & Gu, 2019), in order to obtain theoretical guarantees
as will be shown in the next section.
Third, different from linear or kernelized TS (Agrawal & Goyal, 2013; Chowdhury & Gopalan,
2017), whose posterior can be computed in closed forms, NeuralTS solves a non-convex optimiza-
tion problem (2.3) by gradient descent. This difference requires additional techniques in the regret
analysis. Moreover, stochastic gradient descent can be used to solve the optimization problem with
a similar theoretical guarantee (Allen-Zhu et al., 2018; Du et al., 2018; Zou et al., 2019). For sim-
plicity of exposition, we will focus on the exact gradient descent approach.
3	Regret Analysis
In this section, we provide a regret analysis of NeuralTS. We assume that there exists an unknown
reward function h such that for any 1 ≤ t ≤ T and 1 ≤ k ≤ K,
rt,k = h(xt,k) + ξt,k, with |h(xt,k)| ≤ 1
where {ξt,k} forms an R-sub-Gaussian martingale difference sequence with constant R > 0, i.e.,
E[exp(λξt,k)∣ξLt-ι,k, xi：t,k] ≤ exp(λ2R2) for all λ ∈ R. Such an assumption on the noise se-
quence is widely adapted in contextual bandit literature (Agrawal & Goyal, 2013; Bubeck & Cesa-
Bianchi, 2012; Chowdhury & Gopalan, 2017; Chu et al., 2011; Lattimore & Szepesvari, 2020; Valko
et al., 2013).
Next, we provide necessary background on the neural tangent kernel (NTK) theory (Jacot et al.,
2018), which plays a crucial role in our analysis. In the analysis, we denote by {xi }iT=K1 the set of
observed contexts of all arms and all rounds: {xt,k}1≤t≤T,1≤k≤K where i = K(t 一 1) + k.
Definition 3.1 (Jacot et al. (2018)). Define
Ml+I) = 2E(u,v)〜N(0,A(1)) max{u, 0} max{v, 0},
Hil+1) = 2H(IjE(u,v)〜N(0,Ailj) 1(u ≥ 0) 1(v ≥ 0) + 理+1).
Then, H = (H(L) + Σ(L) )/2 is called the neural tangent kernel matrix on the context set.
3
Published as a conference paper at ICLR 2021
The NTK technique builds a connection between deep neural networks and kernel methods. It
enables us to adapt some complexity measures for kernel methods to describe the complexity of the
neural network, as given by the following definition.
Definition 3.2. The effective dimension d of matrix H with regularization parameter λ is defined as
e logdet(I + H∕λ)
=log(1 + TK∕λ).
Remark 3.3. The effective dimension is a metric to describe the actual underlying dimension in the
set of observed contexts, and has been used by Valko et al. (2013) for the analysis of kernel UCB. Our
definition here is adapted from Yang & Wang (2019), which also considers UCB-based exploration.
Compared with the maximum information gain γt used in Chowdhury & Gopalan (2017), one can
verify that their Lemma 3 shows that γt ≥ log det(I + H∕λ)∕2. Therefore, γt and d are of the same
order up to a ratio of 1∕(2 log(1 + T K∕λ)). Furthermore, d can be upper bounded if all contexts xi
are nearly on some low-dimensional subspace of the RKHS space spanned by NTK (Appendix D).
We will make a regularity assumption on the contexts and the corresponding NTK matrix H.
Assumption 3.4. Let H be defined in Definition 3.1. There exists λ0 > 0, such that H	λ0I. In
addition, for any t ∈ [T], k ∈ [K], kxt,k k2 = 1 and [xt,k]j = [xt,k]j+d/2.
The assumption that the NTK matrix is positive definite has been considered in prior work on NTK
(Arora et al., 2019; Du et al., 2018). The assumption on context xt,a ensures that the initial output
of neural network f(x; θ0) is 0 with the random initialization suggested in Algorithm 1. The con-
dition on X is easy to satisfy, since for any context x, one can always construct a new context x as
[X∕(√2冈2), X∕(√2kxk2)]>.
We are now ready to present the main result of the paper:
Theorem 3.5. Under Assumption 3.4, set the parameters in Algorithm 1 as λ = 1 + 1∕T, ν =
B + RJdlog(1 + TK∕λ) +2 + 2log(1∕δ) where B = max {l∕(22e√∏), √2h>H-1h} with
h = (h(X1), . . . , h(XTK))>, and R is the sub-Gaussian parameter. In line 9 of Algorithm 1, set
η = C1(mλ + mLT)-1 and J = (1 + LT ∕λ)(C2 + log(T 3 Lλ-1 log(1∕δ)))∕C1 for some positive
constants C1, C2. If the network width m satisfies:
m≥polyλ,T,K,L,log(1∕δ),λ0-1,
then, with probability at least 1 - δ, the regret of Algorithm 1 is bounded as
RT ≤ C3(l + CT)ν J2λL(dl0g(l + TK) + 1)T +(4 + C4(l + CT)νL)p2log(3∕δ)T + 5,
where C3, C4 are some positive absolute constants, and CT = √4log T + 2log K.
Remark 3.6. The definition B in Theorem 3.5 is inspired by the RKHS norm of the reward function
defined in Chowdhury & Gopalan (2017). It can be verified that when the reward function h belongs
to the function space induced by NTK, i.e., k h∣∣H < ∞, we have √h>H-1h ≤ IlhkH according to
Zhou et al. (2019), which suggests that B ≤ max{1∕(22e√π), √2kh∣H}.
1/2
Remark 3.7. Theorem 3.5 implies the regret of NeuralTS is on the order of O(dT 1/2). This re-
sult matches the state-of-the-art regret bound in Chowdhury & Gopalan (2017); Agrawal & Goyal
(2013); Zhou et al. (2019); Kveton et al. (2020).
Remark 3.8. In Theorem 3.5, the requirement of m is specified in Condition 4.1 and the proof
of Theorem 3.5, which is a high-degree polynomial in the time horizon T, number of layers L
and number of actions K. However, in our experiments, we can choose reasonably small m (e.g.,
m = 100) to obtain good performance of NeuralTS. See Appendix A.1 for more details. This
discrepancy between theory and practice is due to the limitation of current NTK theory (Du et al.,
2018; Allen-Zhu et al., 2018; Zou et al., 2019). Closing the gap is a venue for future work.
Remark 3.9. Theorem 3.5 suggests that we need to know T before we run the algorithm in order
to set m. When T is unknown, we can use the standard doubling trick (See e.g., Cesa-Bianchi &
Lugosi (2006)) to set m adaptively. In detail, we decompose the time interval (0, +∞) as a union
of non-overlapping intervals [2s, 2s+1). When 2s ≤ t < 2s+1, we restart NeuralTS with the input
T = 2s+1. It can be verified that similar O)(d√T) regret still holds.
4
Published as a conference paper at ICLR 2021
4	Proof of the Main Theorem
This section sketches the proof of Theorem 3.5, with supporting lemmas and technical details pro-
vided in Appendix B. While the proof roadmap is similar to previous work on Thompson Sam-
Pling (e.g., AgraWal & Goyal, 2013; Chowdhury & Gopalan, 2017; Kocak et al., 2014; Kveton
et al., 2020), our proof needs to carefully track the approximation error of neural networks for ap-
proximating the reward function. To control the approximation error, the following condition on the
neural network width is required in several technical lemmas.
Condition 4.1. The network width m satisfies
m ≥ C max {√λL-3∕2[log(TKL2∕δ)]3∕2,T6K6L6 log(TKL∕δ) max{λ-4,1}, }
m[logm]-3 ≥ CTLV2λ- + CT7λ-8L18(λ + LT)6 + CL21T7λ-7(1 + √T∕λ)6,
where C is a positive absolute constant.
For any t, we define an event Etσ as follows
Et =	{ω	∈	Ft+ι	: ∀k	∈	[K],阮常-f(xt,k；	θt-i)∖	≤	ctνσt,k},	(4.1)
where Ct = √4logt + 2log K. Under event Et, the difference between the sampled reward 亍t,k
and the estimated mean reward f(xt,k; θt-1) can be controlled by the reward’s posterior variance.
We also define an event Et as follows
Et = {ω ∈	Ft	: ∀k	∈	[K],	∖f (xt,k；	θt-i)	— h(xt,k)∖ ≤ νσt,k +	e(m)},	(4.2)
where (m) is defined as
e(m) = €p(m) + Ce,ι(1 — ηmλ)J √TL∕λ
6p(m) = Ce2T 2/3m-1/6A-2/3L3Plogm + "m-1/6 PogmL4T 5∕3λ-5∕3(1 + √T∕λ)
+ Ce,4 (B + R√log det(I + H∕λ) + 2 + 2 log(1∕δ)) √log mT7/6m-1/6A-2/3L9/2,
(4.3)
and {Ce,i}4=ι are some positive absolute constants. Under event E%, the estimated mean reward
f (xt,k; θt-1) based on the neural network is similar to the true expected reward h(xt,k). Note that
the additional term (m) is the approximate error of the neural networks for approximating the true
reward function. This is a key difference in our proof from previous regret analysis of Thompson
Sampling Agrawal & Goyal (2013); Chowdhury & Gopalan (2017), where there is no approximation
error.
The following two lemmas show that both events Et and Et happen with high probability.
Lemma 4.2. For any t ∈ [T], Pr (Et IFt) ≥ 1 — t-2.
Lemma 4.3. Suppose the width of the neural network m satisfies Condition 4.1. Set η = C(mλ +
mLT)-1, then we have Pr (∀t ∈ [T], E∕) ≥ 1 — δ, where C is an positive absolute constant.
The next lemma gives a lower bound of the probability that the sampled reward reis larger than true
reward up to the approximation error (m).
Lemma 4.4. For any t ∈ [T], k ∈ [K], we have Pr (et,k + e(m) > h(xt,k)∣Ft, Etl) ≥ (4e√π)-1.
Following Agrawal & Goyal (2013), for any time t, we divide the arms into two groups: saturated
and unsaturated arms, based on whether the standard deviation of the estimates for an arm is smaller
than the standard deviation for the optimal arm or not. Note that the optimal arm is included in the
group of unsaturated arms. More specifically, we define the set of saturated arms St as follows
St = {k∣k ∈ [K], h(xt,at) — h(xt,k) ≥ (1 + Ct)Vσt,k + 2e(m)}.	(4.4)
Note that we have taken the approximate error (m) into consideration when defining saturated
arms, which differs from the Thompson Sampling literature (Agrawal & Goyal, 2013; Chowdhury
& Gopalan, 2017). It is now easy to show that the immediate regret of playing an unsaturated arm
can be bounded by the standard deviation plus the approximation error (m).
The following lemma shows that the probability of pulling a saturated arm is small in Algorithm 1.
5
Published as a conference paper at ICLR 2021
Lemma 4.5. Let at be the arm pulled at round t ∈ [T]. Then, Pr (at ∈ St∣Ft, Ef) ≥ 4e√∏ -表.
The next lemma bounds the expectation of the regret at each round conditioned on Etμ.
Lemma 4.6. Suppose the width of the neural network m satisfies Condition 4.1. Set η = C1 (mλ +
mLT)-1 , then with probability at least 1 - δ, we have for all t ∈ [T] that
E[h(xtq) - h(xt,at)∣Ft, Et] ≤ C2(l + ct)ν√LE[min{σt,at, 1}∣Ft, Ef] + 4e(m) + 2t-2,
where C1 , C2 are some positive absolute constants.
Based on Lemma 4.6, we define At := (h(xt川)-h(xt,aj) 1(Eμ), and
t
Xt := At - (C∆(1 + ct)ν√Lmin{σt,at, 1} + 4e(m) + 2t-2), Yt = XXi,	(4.5)
i=1
where C∆ is the same with constant C in Lemma 4.6. By Lemma 4.6, we can verify that with
probability at least 1 - δ, {Yt} forms a super martingale sequence since E(Yt - Yt-1 ) = EXt ≤ 0.
By Azuma-Hoeffding inequality (Hoeffding, 1963), we can prove the following lemma.
Lemma 4.7. Suppose the width of the neural network m satisfies Condition 4.1. Then set η =
C1 (mλ + mLT)-1 , we have, with probability at least 1 - δ, that
T	T
X Ai ≤ 4Te(m) + π2/3 + C2(l + CT)ν√L X min{σtg, 1}
i=1	i=1
+ (4 + C3(l + CT )νL + 4e(m))p2 log(1∕δ)T,
where C1 , C2 , C3 are some positive absolute constants.
The last lemma is used to control PiT=1 min{σt,at, 1} in Lemma 4.7.
Lemma 4.8. Suppose the width of the neural network m satisfies Condition 4.1. Then set η =
C1 (mλ + mLT)-1 , we have, with probability at least 1 - δ, it holds that
Xmin{σt,at,1} ≤ ,2λT(elog(1 + TK) + 1) + C2T13/6PlOgmm-1/6λ-2∕3L9∕2,
i=1
where C1 , C2 are some positive absolute constants.
With all the above lemmas, we are ready to prove Theorem 3.5.
Proofof Theorem 3.5. By Lemma 4.3, Eμ holds for all t ∈ [T] with probability at least 1 - δ.
Therefore, with probability at least 1 - δ, we have
T
Rt = X(h(xt川)-h(xt,at)) ME/)
i=1
2	T
≤ 4Tf(m) + — + C1(1+ CT )ν √l X min{σt,at, 1}
3	i=1
+ (4 + C2(1 + CT )νL + 4e(rn))√2 log(1∕δ)T
≤ C1(1 + ct)ν√L(,2λT(elog(1 + TK) + 1) + C3T13/6Plogmmτ∕6λ-2∕3L9∕2
+ ∏3- + 4Te(m) + 4e(m),2 log(1∕δ)T + (4 + C2(1 + CT )νL),2 log(1∕δ)T,
=C1(1 + ct )ν√L( ,2λT (elog(1 + TK ) + 1) + C3T 13∕6 Plog mmτ∕6λ-2∕3L9∕2
+ ∏2 + ep(m)(4T + P2log(1∕δ)T) + (4 + C2(1 + CT )νL)P2log(1∕δ)T
+ Ce,ι(1 - ηmλ)J√TL∕λ(4T + √2log(1∕δ)T),
6
Published as a conference paper at ICLR 2021
where Ci, G, C3 are some positive absolute constants, the first inequality is due to Lemma 4.7,
and the second inequality is due to Lemma 4.8. The third equation is from (4.3). By setting η =
C4(mλ + mLT)-1 and J =(1 + LT∕λ)(log(24Ce,ι) + log(T3Lλ-i l0g(l∕δ)))∕C4, We have
Gj(I - ηmλ)JPTL∕λ(4T + P2log(1∕δ)T) ≤ 1,
Then choosing m such that
GC3(1 + CT)νT 13∕6Pl0gmm-1∕6λ-2∕3L5 ≤ 1,	Ep(m)(4T + P2log(1∕δ)T) ≤ 1.
RT can be further bounded by
RT ≤ G(1 + CT)ν J2λL(dlog(l + TK) + 1)T +(4 + G(1 + CT)νL)P2 log(1∕δ)T + 5.
Taking union bound over Lemmas 4.3, 4.7 and 4.8, the above inequality holds with probability
1 - 3δ. By replacing δ with δ∕3 and rearranging terms, we complete the proof.	□
5	Experiments
This section gives an empirical evaluation of our algorithm in several public benchmark datasets,
including adult, covertype, magic telescope, mushroom and shuttle, all from
UCI (Dua & Graff, 2017), as well as MNIST (LeCun et al., 2010). The algorithm is compared
to several typical baselines: linear and kernelized Thompson Sampling (Agrawal & Goyal, 2013;
Chowdhury & Gopalan, 2017), linear and kernelized UCB (Chu et al., 2011; Valko et al., 2013),
BootstrapNN (Osband et al., 2016b; Riquelme et al., 2018), and -greedy for neural networks. Boot-
strapNN trains multiple neural networks with subsampled data, and at each step pulls the greedy ac-
tion based on a randomly selected network. It has been proposed as away to approximate Thompson
Sampling (Osband & Van Roy, 2015; Osband et al., 2016b).
5.1	Experiment setup
To transform these classification problems into multi-armed bandits, we adapt the disjoint models (Li
et al., 2010) to build a context feature vector for each arm: given an input feature x ∈ Rd of a
k-class classification problem, we build the context feature vector with dimension kd as: x1 =
(x; 0; •…；0), x2 =(0; x; •…；0),…,Xk =(0; 0; •…;x). Then, the algorithm generates a set of
predicted reward following Algorithm 1 and pulls the greedy arm. For these classification problems,
if the algorithm selects a correct class by pulling the corresponding arm, it will receive a reward as
1, otherwise 0. The cumulative regret over time horizon T is measured by the total mistakes made
by the algorithm. All experiments are repeated 8 times with reshuffled data.
We set the time horizon of our algorithm to 10 000 for all data sets, except for mushroom which
contains only 8 124 data. In order to speed up training for the NeuralUCB and Neural Thompson
Sampling, we use the inverse of the diagonal elements of U as an approximation of U-1. Also,
since calculating the kernel matrix is expensive, we stop training at t = 1000 and keep evaluating
the performance for the rest of the time, similar to previous work (Riquelme et al., 2018; Zhou et al.,
2019). Due to space limit, we defer the results on adult, covertype and magic telescope,
as well as further experiment details, to Appendix A. In this section, we only show the results on
mushroom, shuttle and MNIST.
5.2	Experiment I: Performance of Neural Thompson Sampling
The experiment results of Neural Thompson Sampling and other benchmark algorithms are shown
in Figure 1. A few observations are in place. First, Neural Thompson Sampling’s performance
is among the best in 6 datasets and is significantly better than all other baselines in 2 of them.
Second, the function class used by an algorithm is important. Those with linear representations
tend to perform worse due to the nonlinearity of rewards in the data. Third, Thompson Sampling
is competitive with, and sometimes better than, other exploration strategies with the same function
class, in particular when neural networks are used.
7
Published as a conference paper at ICLR 2021
-BOF
O 2000	4000	6000
#of round
(a) MNIST
8000 IOOOO
-BOF
O IOOO 2000 3000 4000 5000 6000 7000 8000
# of round
(b) Mushroom
-BOF
0	2000	4000	6000
# of round
(c) Shuttle
8000	10000
Figure 1: Comparison of Neural Thompson Sampling and baselines on UCI datasets and MNIST
dataset. The total regret measures cumulative classification errors made by an algorithm. Results
are averaged over 8 runs with standard errors shown as shaded areas.
5.3	Experiment II: Robustness to Reward Delay
This experiment is inspired by practical scenarios where reward signals are delayed, due to various
constraints, as described by Chapelle & Li (2011). We study how robust the two most competitive
methods from Experiment I, Neural UCB and Neural Thompson Sampling, are when rewards are
delayed. More specifically, the reward after taking an action is not revealed immediately, but arrive
in batches when the algorithms will update their models. The experiment setup is otherwise identical
to Experiment I. Here, we vary the batch size (i.e., the amount of reward delay), and Figure 2 shows
the corresponding total regret. Clearly, we recover the result in Experiment I when the delay is 0.
Consistent with previous findings (Chapelle & Li, 2011), Neural TS degrades much more gracefully
than Neural UCB when the reward delay increases. The benefit may be explained by the algorithm’s
randomized exploration nature that encourages exploration between batches. We, therefore, expect
wider applicability of Neural TS in practical applications.
O 200	400	600	800 IOOO
Reward Delay
(a) MNIST
O-,	，	，	，	，
O	200	400	600	800	IOOO
Reward Delay
(b) Mushroom
O 200	400	600	800	1000
Reward Delay
(c) Shuttle
Figure 2: Comparison of Neural Thompson Sampling and Neural UCB on UCI datasets and MNIST
dataset under different scale of delay. The total regret measures cumulative classification errors made
by an algorithm. Results are averaged over 8 runs with standard errors shown as error bar.
6	Related Work
Thompson Sampling was proposed as an exploration heuristic almost nine decades ago (Thompson,
1933), and has received significant interest in the last decade. Previous works related to the present
paper are discussed in the introduction, and are not repeated here.
Upper confidence bound or UCB (Agrawal, 1995; Auer et al., 2002; Lai & Robbins, 1985) is a
widely used alternative to Thompson Sampling for exploration. This strategy is shown to achieve
near-optimal regrets in a range of settings, such as linear bandits (Abbasi-Yadkori et al., 2011; Auer,
2002; Chu et al., 2011), generalized linear bandits (Filippi et al., 2010; Jun et al., 2017; Li et al.,
2017), and kernelized contextual bandits (Valko et al., 2013).
Neural networks are increasingly used in contextual bandits. In addition to those mentioned ear-
lier (Blundell et al., 2015; Kveton et al., 2020; Lu & Van Roy, 2017; Riquelme et al., 2018), Zahavy
8
Published as a conference paper at ICLR 2021
& Mannor (2019) used a deep neural network to provide a feature mapping and explored only at the
last layer. Schwenk & Bengio (2000) proposed an algorithm by boosting the estimation of multiple
deep neural networks. While these methods all show promise empirically, no regret guarantees are
known. Recently, Foster & Rakhlin (2020) proposed a special regression oracle and randomized
exploration for contextual bandits with a general function class (including neural networks) along
with theoretical analysis. Zhou et al. (2019) proposed a neural UCB algorithm with near-optimal
regret based on UCB exploration, while this paper focuses on Thompson Sampling.
7	Conclusions
In this paper, we adapt Thompson Sampling to neural networks. Building on recent advances in
1/2
deep learning theory, we are able to show that the proposed algorithm, NeuralTS, enjoys a O(dT 1/2)
regret bound. We also show the algorithm works well empirically on benchmark problems, in com-
parison with multiple strong baselines.
The promising results suggest a few interesting directions for future research. First, our analysis
needs NeuralTS to perform multiple gradient descent steps to train the neural network in each round.
It is interesting to analyze the case where NeuralTS only performs one gradient descent step in
each round, and in particular, the trade-off between optimization precision and regret minimization.
Second, when the number of arms is finite, Oe(√dT) regret has been established for parametric
bandits with linear and generalized linear reward functions. It is an open problem how to adapt
NeuralTS to achieve the same rate. Third, Allen-Zhu & Li (2019) suggested that neural networks
may behave differently from a neural tangent kernel under some parameter regimes. It is interesting
to investigate whether similar results hold for neural contextual bandit algorithms like NeuralTS.
Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. WZ, DZ and QG are
partially supported by the National Science Foundation CAREER Award 1906169 and IIS-1904183.
The views and conclusions contained in this paper are those of the authors and should not be inter-
preted as representing any funding agencies.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the
20th International Conference on Artificial Intelligence and Statistics, pp. 176-184, 2017.
Rajeev Agrawal. Sample mean based index policies by o(log n) regret for the multi-armed bandit
problem. Advances in Applied Probability, 27(4):1054-1078, 1995.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
In International Conference on Machine Learning, pp. 127-135, 2013.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? In
Advances in Neural Information Processing Systems, pp. 9015-9025, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47(2-3):235-256, 2002.
9
Published as a conference paper at ICLR 2021
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through Bayesian deep Q-networks. In Proceedings of the 2018 Information Theory and Appli-
cations Workshop,pp.1-9, 2018.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems, pp. 12893-12904, 2019.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In Proceedings of the 32nd International Conference on Machine Learning, pp.
1613-1622, 2015.
Sebastien BUbeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.
YUan Cao and QUanqUan GU. Generalization boUnds of stochastic gradient descent for wide and
deep neUral networks. In Advances in Neural Information Processing Systems, pp. 10835-10845,
2019.
YUan Cao, Zhiying Fang, YUe WU, Ding-XUan ZhoU, and QUanqUan GU. Towards Understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proceedings of
the 34th International Conference on Machine Learning, pp. 844-853, 2017.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff func-
tions. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,
pp. 208-214, 2011.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586-594,
2010.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg. Noisy networks for exploration. In Proceedings of the 6th Interna-
tional Conference on Learning Representations, 2018.
Dylan J Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with
regression oracles. arXiv preprint arXiv:2002.04926, 2020.
Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. Web-scale
Bayesian click-through rate prediction for sponsored search advertising in Microsoft’s Bing
search engine. In Proceedings of the 27th International Conference on Machine Learning, pp.
13-20, 2010.
Kristjan Greenewald, Ambuj Tewari, Susan Murphy, and Predag Klasnja. Action centered contextual
bandits. In Advances in Neural Information Processing Systems 30, pp. 5977-5985, 2017.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Matthew W Hoffman, Bobak Shahriari, and Nando de Freitas. Exploiting correlation and budget
constraints in bayesian multi-armed bandit optimization. arXiv preprint arXiv:1303.6746, 2013.
10
Published as a conference paper at ICLR 2021
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, pp. 8571-
8580, 2018.
Kwang-Sung Jun, Aniruddha Bhargava, Robert D. Nowak, and Rebecca Willett. Scalable gen-
eralized linear bandits: Online computation and hashing. In Advances in Neural Information
Processing Systems 30, pp. 99-109, 2017.
Jaya Kawale, Hung Hai Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla. Efficient
Thompson sampling for online matrix-factorization recommendation. In Advances in Neural
Information Processing Systems 28, pp. 1297-1305, 2015.
Tomas Kocak, Michal Valko, Remi Munos, and Shipra Agrawal. Spectral Thompson sampling. In
28th AAAI Conference on Artificial Intelligence, 2014.
Tomas Kocak, Michal Valko, Remi Munos, and Shipra Agrawal. Spectral Thompson sampling. In
Proceedings of the 28th AAAI Conference on Artificial Intelligence, pp. 1911-1917, 2014.
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In Proceedings of the
22nd International Conference on Artificial Intelligence and Statistics, 2020.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
in Applied Mathematics, 6(1):4-22, 1985.
Tor Lattimore and Csaba Szepesvari. BanditAlgorithmS. Cambridge University Press, 2020.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, pp. 661-670, 2010.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contex-
tual bandits. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2071-2080. JMLR. org, 2017.
Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. BBQ-
networks: Efficient exploration in deep reinforcement learning for task-oriented dialogue systems.
In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp. 5237-5244, 2018.
Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. In Advances in Neural Information
Processing Systems 30, pp. 3258-3266, 2017.
Jeffrey Mahler, Florian T Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry,
Kai Kohlhoff, Torsten Kroger, James Kuffner, and Ken Goldberg. Dex-net 1.0: A cloud-based
network of3d objects for robust grasp planning using a multi-armed bandit model with correlated
rewards. In 2016 IEEE international conference on robotics and automation (ICRA), pp. 1957-
1964. IEEE, 2016.
Ian Osband and Benjamin Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv
preprint arXiv:1507.00300, 2015.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances in Neural Information Processing Systems 29, pp. 4026-4034,
2016a.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026-4034, 2016b.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An
empirical comparison of Bayesian deep networks for Thompson sampling. arXiv preprint
arXiv:1802.09127, 2018.
11
Published as a conference paper at ICLR 2021
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. arXiv preprint arXiv:1707.02038, 2017.
Holger Schwenk and Yoshua Bengio. Boosting neural networks. Neural computation, 12(8):1869-
1887, 2000.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
Lin F Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and
regret bound. arXiv preprint arXiv:1905.10389, 2019.
Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with UCB-based explo-
ration. arXiv preprint arXiv:1911.04462, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, pp. 1-26, 2019.
12
Published as a conference paper at ICLR 2021
A Further Detail of the Experiments in Section 5
A.1 Parameter Tuning
In the experiments, we shuffle all datasets randomly, and normalize the features so that their `2 -
norm is unity. One-hidden-layer neural networks with 100 neurons are used. Note that we do not
choose m as suggested by theory, and such a disconnection has its root in the current deep learning
theory based on neural tangent kernel, which is not specific in this work. During posterior updating,
gradient descent is run for 100 iterations with learning rate 0.001. For BootstrapNN, we use 10
identical networks, and to train each network, data point at each round has probability 0.8 to be
included for training (p = 10, q = 0.8 in the original paper (Schwenk & Bengio, 2000)) For -
Greedy, we tune with a grid search on {0.01, 0.05, 0.1}. For (λ, ν) used in linear and kernel
UCB / Thompson Sampling, we set λ = 1 following previous works (Agrawal & Goyal, 2013;
Chowdhury & Gopalan, 2017), and do a grid search of ν ∈ {1, 0.1, 0.01} to select the parameter
with best performance. For the Neural UCB / Thompson Sampling methods, we use a grid search on
λ ∈ {1, 10-1, 10-2, 10-3} and ν ∈ {10-1, 10-2, 10-3, 10-4, 10-5}. All experiments are repeated
20 times, and the average and standard error are reported.
A.2 Detailed Results
Table 1 summarizes the total regrets measured at the last round on different data sets, with mean
and standard deviation error computed based on 20 independent runs. The Bold Faced data is the
top performance over 8 experiments. Table 2 shows the number of times the algorithm in that row
significantly outperforms, ties, or significantly underperforms, compared with other algorithm with
t-test at 90% significance level. Figure 3 shows the performance of Neural Thompson Sampling
compared with other baseline method. Figure 4 shows the comparison between Neural Thompson
Sampling and Neural UCB in delay reward settings.
Table 1: Total regrets get at the last step with standard deviation attached
	Adult	Covertype	Magic1	MNIST	Mushroom	Shuttle
Round#	10 000	10 000	10 000	10 000	8124	10 000
Input Dim2	2×15	2 × 55	2×12	10 × 784	2 × 23	7×9
Random3	5000	5000	5000	9000	4062	8571
Linear UCB	2097.5	3222.7	2604.4	2544.0	562.7	966.6
	±50.3	±67.2	±34.6	±235.4	±23.1	±39.0
Linear TS	2154.7	4297.3	2700.5	2781.4	643.3	1020.9
	±40.5	±328.7	±46.7	±338.3	±30.4	±42.8
Kernel UCB	2080.1	3546.2	2406.5	3595.8	199.0	166.5
	±44.8	±175.7	±79.4	±580.1	±41.0	±39.4
Kernel TS	2111.5	3659.9	2442.6	3406.0	291.2	283.3
	±87.4	±113.8	±64.5	±411.7	±40.0	±180.5
BooststrapNN	2097.3	3067.0	2269.4	1765.6	132.3	211.7
	±39.3	±56.1	±27.9	±321.1	±8.6	±20.9
eps-greedy	2328.5	3334.2	2381.8	1893.2	323.2	682.0
	±50.4	±72.6	±37.3	±93.7	±32.5	±79.8
NeuralUCB	2061.8	3012.1	2033.0	2071.6	160.4	338.6
	±42.8	±87.0	±48.6	±922.2	±95.3	±386.4
NeuralTS	2092.5	2999.1	2037.4	1583.4	115.0	232.0
(ours)	±48.0	±74.3	±61.3	±198.5	±35.8	±149.5
1Magic is short for data set MagicTelescope
2Using disjoint encoding thus is NumofClass × NumofFeatures
3Random pulling an arm at each round
4Magic is short for data set MagicTelescope
13
Published as a conference paper at ICLR 2021
Table 2: Performance on total regret comparing with other methods on all datasets. Tuple (w/t/l)
indicates the times of the algorithm at that row wins, ties with or loses, compared to all other 7
algorithms with t-test at 90% significant level.
	Adult	Covertype	Magic4	MNIST	Mushroom	Shuttle
Linear UCB	2/3/2	4/0/3	1/0/6	2/2/3	1/0/6	1/0/6
Linear TS	1/0/6	0/0/7	0/0/7	2/1/4	0/0/7	0/0/7
Kernel UCB	4/3/0	2/0/5	3/1/3	0/0/7	4/0/3	7/0/0
Kernel TS	2/3/2	1/0/6	2/0/5	1/0/6	3/0/4	3/3/1
BooststrapNN	2/4/1	5/0/2	5/0/2	4/3/0	5/2/0	3/3/1
eps-greedy	0/0/7	3/0/4	3/1/3	4/2/1	2/0/5	2/0/5
NeuralUCB	6/1/0	6/1/0	6/1/0	3/3/1	5/1/1	3/3/1
NeuralTS (ours)	2/4/1	6/1/0	6/1/0	6/1/0	6/1/0	3/3/1
n⅞πvκ -BOF
0	2000	4000	6000	8000	10000
# ofrαund
n⅞πvκ -BOF
0	2000	4000	6000	8000	10000
# ofround
n⅞πvκ -BOF
0	2000	4000	6000	8000	10000
# ofround
(a) Adult	(b) Covertype	(c) Magic Telescope
Figure 3:	Comparison of Neural Thompson Sampling and baselines on UCI datasets and MNIST
dataset. The total regret measures cumulative classification errors made by an algorithm. Results
are averaged over multiple runs with standard errors shown as shaded areas.
3000-
2800
2000
O 200	400	600	800 IOOO
Reward Delay
(a) Adult
200	400	600	800 IOOO
Reward Delay
(b) Covertype
Reward Delay
(c) Magic Telescope
Figure 4:	Comparison of Neural Thompson Sampling and Neural UCB on UCI datasets and MNIST
dataset under different scale of delay. The total regret measures cumulative classification errors made
by an algorithm. Results are averaged over multiple runs with standard errors shown as error bar.
14
Published as a conference paper at ICLR 2021
—Ms) ∙EH Bu-Uun-
BootstrapNN	e ρs-greedy	NeuraIUCB Neura ∏⅛ (ours)
(a) Adult
6000
5000
∣4∞°
^∣3000
≡ 2000
IOOO
BootstrapNN eps-greedy NeuraIUCB Neure∏⅛ (ours)
5000
-.40∞
I
E 3000
c 2000
1000
(b) Covertype
O-
BootstrapNN e ρs-greedy NeuraIUCB Neura∏⅛ (ours)
(c) Magic Telescope
™ I	a I	soo° I
二■ 一.J■ 广 I
＜：： ■ ■■产■ . . ■ ■ ■
rH≡*M:H≡≡M ι[H≡*M
O-
BootstrapNN	e ρs-greedy	NeuraIUCB Neura ∏⅛ (ours)
O-
BootstrapNN eps-greedy NeuraIUCB Neure∏⅛ (ours)
O-
BootstrapNN eρs-greedy NeuraIUCB Neura∏⅛ (ours)
(d) MNIST
(e) mushroom
(f) shuttle
Figure 5:	Comparison of the running time for Neural TS, Neural UCB and -greedy for neural
networks on UCI datasets and MNIST dataset.
A.3 Run time analysis
We compare the run time of the four algorithms based on neural networks: BootstrapNN, -greedy
for neural networks, NeuralUCB, and NeuralTS. The comparison is shown in Figure 5. We can see
that NeuralTS and NeuralUCB are about 2 to 3 times slower than -greedy, which is due to the extra
calculation of the neural network gradient for each input context. BootstrapNN is often more than 5
times slower than -greedy because it has to train several neural networks at each round.
B Proof of Lemmas in Section 4
Under Condition 4.1, we can show that the following inequalities hold.
2P1∕λ ≥ CmJmTL-3/2[log(TKL2∕δ)]3R
2√T∕λ ≤ Cm,2 min {m1/2L-6[logm]-3/2, mv8((λη)2L-6T-1(log m)-1)3/8},
m1/6 ≥ Cm,3p0gmL7/2T7/6A-7/6(1 + √T∕λ)
m ≥ Cm,4T 6K6L6 log(T KL∕δ) max{λ0-4, 1},
where {Cm,1, Cm,2, . . . , Cm,4} are some positive absolute constants.
B.1	Proof of Lemma 4.2
The following concentration bound on Gaussian distributions will be useful in our proof.
Lemma B.1 (Hoffman et al. (2013)). Consider a normally distributed random variable X 〜
N(μ, σ2) and β ≥ 0. The probability that X is within a radius of βσ from its mean can then
be written as
Pr (|X - μ∣≤ βσ) ≥ 1 - exp(-β2∕2).
Proof of Lemma 4.2. Since the estimated reward ret,k is sampled from N(f(xt,k; θt-1), ν2σt2,k) if
given filtration Ft, Lemma B.1 implies that, conditioned on Ft and given t, k,
Pr (∣et,k - f(xt,k； θt-i)∣ ≤ qνσt,k∣Ft) ≥ 1 - exp(-c2∕2).
Taking a union bound over K arms, we have that for any t
Pr (∀k, ∣et,k - f(xt,k θt-i)∖ ≤ ctνσt,k∣Ft) ≥ 1 - Kexp(-c2∕2).
15
Published as a conference paper at ICLR 2021
Finally, choose Ct = √4 logt + 2log K as defined in (4.1), We get the bound that
Pr (Eσ∣Ft) = Pr (∀k, ∣et,k - f (xt,k； θt-i)| ≤ ctνσt,k∣Ft) ≥ 1 -".
□
B.2	Proof of Lemma 4.3
Before going into the proof, some notation is needed about linear and kernelized models.
Definition B.2. Define Ujt = λI + Pt=ι g(xig ； θo)g(xt,at ； θo)>∕m and based on Ujt, we further
define 行；k= λg>(xt,k; θo)UJ —⅛(xt,k; θo)/m. Furthermore, for convenience we define
Jt = (g(x1,aι ； θt)…g(xt,at ； θt)),
Jt = (g(xi,aι ； θθ)…g(Xt,at ； θθ))
ht = (h(χι,aI)…h(χt,at ))> ,
Tt = (r1 …rt),
Q = (h(χι,aI) - ri … h(χt,at ) - rt)>,
where et is the reward noise. We can verify that Ut = λI + JtJ>∕m, Ut = λI + JtJ>/m . We
further define Kt = J>Jt/m.
The first lemma shows that the target function is well-approximated by the linearized neural network
if the network width m is large enough.
Lemma B.3 (Lemma 5.1, Zhou et al. (2019)). There exists some constant C > 0 such that for any
δ∈ (0, 1), if
m ≥ CT4K4L6log(T2K2L∕δ)∕λ0,
then with probability at least 1 - δ over the random initialization of θ0, there exists a θ* ∈ Rp such
that
h(xi) = hg(xi; θo), θ* - θoi,	√m∣∣θ*- Θ0k2 ≤ √2h>H-1h ≤ B,	(B.1)
for all i ∈ [TK], where B is defined in Theorem 3.5.
From Lemma B.3, it is easy to show that under this initialization parameter θ0, we have that ht =
J> (θ* - θo)
The next lemma bounds the difference between the σJt,k from the linearized model and the σt,k
actually used in the algorithm. Its proof, together with other technical lemmas’, will be given in the
next section.
Lemma B.4. Suppose the network size m satisfies Condition 4.1. Set η = C1(mλ + mLT)-1,
then with probability at least 1 - δ,
∣σt,k - σt,k| ≤ C2Plogmt7/6mT/6X-2/3L9/2,
where C1 , C2 are two positive constants.
We next bound the difference between the outputs of the neural network and the linearized model.
Lemma B.5. Suppose the network width m satisfies Condition 4.1.
Then, set η = C1(mλ + mLT)-1, with probability at least 1 - δ over the random initialization of
θ0, we have
∣f(xt,k ； θt-i) -〈g(xt,k； θo), U -LIJt-ιrt-i∕m)∣ ≤ Cf/m-/k-2/L Plog m
+ C3(1 - ηmλ)J PtL∕λ
+ C4m-l26piogmL4t523λ-523(1 + pt∕λ),
where {Ci }i4=1 are positive constants.
16
Published as a conference paper at ICLR 2021
The next lemma, due to Chowdhury & Gopalan (2017), controls the quadratic value generated by
an R-SUb-GaUSSian random vector e:
Lemma B.6 (Theorem 1, Chowdhury & Gopalan (2017)). Let {j}∞U be a real-valued stochastic
process such that for some R ≥ 0 and for all t ≥ 1, Q is Ft-measurable and R-sub-Gaussian
conditioned on Ft, Recall Kt defined in Definition B.2. With probability 0 < δ < 1 and for a given
η > 0, with probability 1 - δ, the following holds for all t,
e>t((Kt + ηI)-1 + I)TeI：t ≤ R log det((1 + η)I + Kt) + 2R2 log(1∕δ).
Finally, the following lemma shows the linearized kernel and the neural tangent kernel are closed:
Lemma B.7. For all t ∈ [T], there exists a positive constants C such that the following holds: if the
network width m satisfies
m ≥ CT6L6K6 log(TKL∕δ),
then with probability at least 1 - δ,
logdet(I + λ-1Kt) ≤ logdet(I + λ-1H) + 1.
We are now ready to prove Lemma 4.3.
ProofofLemma 4.3. First of all, since m satisfies Condition 4.1, then with the choice of η ,the
condition required in Lemmas B.3-B.7 are satisfied. Thus, taking a union bound, we have with
probability at least 1 - 5δ, that the bounds provided by these lemmas hold. Then for any
t ∈ [T], we will first provide the difference between the target function and the linear function
(虱xt,k； θ0), U= J-ιrt-ι∕m> as:
Ih(Xt,k) - <g(xt,k； θ0), U-11 Jt-Irt-i/m)|
≤ ∖h(xt,k) - <g(xt,k； θ0), U--1ιJt-iht-i∕m) I + Kg(xt,k； θ0), U-11Jt-Iq-1/m)∣
=Kg(Xt,k； θ0), θ* - θ0 - U=J-1J3(Θ* - θ0)/m〉| + ∣ g(xt,k； θ0)τU∕ιJt-iet-i∕m∣
=Kg(Xt,k； θ0), (I - U-31 (Ut-1 - λI))(θ* - 00)∣ + ∣ g(xt,k； θ0)τU=1Jt-u∕m∣
=λ g(xt,k； θ0)τU-11(θ* - θ0)∣ + ]g(xt,k; θ0)τU-11Jt-Iet-1∕m ∣
≤ λ∕g(Xt,k ； 00)τU 二 1g(Xt,k ； 00) J(θ*- 00)τU -ʌ(θ* - 00)
+ yg(Xt,k5 00)TU-11g(Xt,k； 00) JeT-IJT-IU-11Jt-Iet-1∕m
≤ √m∣∣0* - 00∣∣2σt,k + σt,kλ-1/2，e3JLU-11 Jt-Iet-1 ∕m	(B.2)
where the first inequality uses triangle inequality and the fact that rt-1 = ht-1 + et-1； the first
equality is from Lemma B.3 and the second equality uses the fact that Jt-1J3 = m(Ut-1 - λI)
which can be verified using Definition B.2; the second inequality is from the fact that IaTAeI ≤
√ατAα√βτAβ. Since U-LI W 11 and σt,k defined in Definition B.2, we obtain the last in-
equality.
Furthermore, by obtaining
Jl1U-31Jt-1∕m = JL1(λI + Jt-IJT-1∕m)TJt-I
=JT-1(λ-1I - λ-2Jt-1(I + λ-1jT-1Jt-1∕m)-1JT-1∕m)Jt-1∕m
=λ-1JT-1Jt-1∕m - λ-1JT-1Jt-1(λI + JT-1Jt-1∕m)-1JT-1Jt-1∕m2
=λ-1Kt-1(I - (λI + Kt-1)-1Kt-1) = Kt-1(λI + Kt-1)-1,
where the first equality is from the Sherman-Morrison formula, and the second equality uses Defi-
nition B.2 and the fact that (λI + Kt-I)TKt-I = I - λ(λI + Kt-I)T which could be verified
by multiplying the LHS and RHS together, we have that
etT-1JtT-1U-Jt-1et-1∕m ≤ J
etT-1Kt-1(λI + Kt-1)-1 et-1
≤ 'etL1(Kt-1 + (λ - I)I)(λI + Kt-I)-1et-1
=JeT-1(I + (K- + (λ - 1)I)T)Tet-1	(B.3)
17
Published as a conference paper at ICLR 2021
where the second inequality is because λ = 1 + 1/T ≥ 1 set in Theorem 3.5.
Based on (B.2) and (B.3), by utilizing the bound on ∣∣θ* - θ∣∣2 provided in Lemma B.3, as well as
the bound given in Lemma B.6, and λ ≥ 1, we have
Ih(Xt,k) - <g(xt,k； θo), ^C-1ιJt-Irt-im)| ≤ (B + Rplogdet(λI + Kt-i) + 2log(1∕δ))σt,k,
since it is obvious that
log det(λI + Kt-1) =logdet(I+λ-1Kt-1)+(t-1)logλ
≤ log det(I + λ-1Kt-1) + t(λ - 1)
≤ log det(I + λ-1H) + 2,
where the first equality moves the λ outside the log det, the first inequality is due to log λ ≤ λ - 1,
and the second inequality is from Lemma B.7 and the fact that λ = 1 + 1∕T (as set in Theorem 3.5).
Thus, we have
Ih(Xt,k) - <g(xt,k； θo), ID-11Jt-irt-im)∣ ≤ νσt,k,
where we set V = B + R,logdet(I + H∕λ) +2 + 2log(1∕δ). Then, by combining this bound
with Lemma B.5, We conclude that there exist positive constants C71, C2, G so that
|f (xt,k； θt-i) - h(xt,k)∣ ≤ νσt,k + C^/m--/k-2/Lplogm + C2(1 - ηmλ)JptL∕λ
+ C/ m-126PlθgmL4t523λ-523(1 + pt∕λ),
≤ νσt,k + Cιt2∕3m-1∕6λ-2∕3L3 plog m + C- (1 — ηmλ)JptL∕λ
+ C3mT∕6pogmL4t5∕3λ-5∕3(1 + pt∕λ)
+ (B + Rplogdet(I + ~H∣λ) +2 + 2log(1∕δD (σt,k - σt,k).
Finally, by utilizing the bound of ∣σt,k - σt,k | provided in Lemma B.4, We conclude that
|f (Xt,k; θt-1) - h(Xt,k)| ≤ νσt,k + (m),
where (m) is defined by adding all of the additional terms and taking t= T:
e(m) = CiT2∕3mT∕6λ-2∕3L3plogm + C-(1 - ηmλ)JpTL∕λ+
+ C3mT∕6piθgmL4T 5∕3λ-5∕3(l + pT∕λ)
+ C4(B + Rplogdet(I + H∕λ) + 2 + 2log(1∕δ)) plog mT726m-126λ-223L9/2,
where is exactly the same form defined in (4.3). By setting δ to δ∕5 (required by the union bound
discussed at the beginning of the proof), We get the result presented in Lemma 4.3.	□
B.3	Proof of Lemma 4.4
Our proof requires an anti-concentration bound for Gaussian distribution, as stated below:
Lemma B.8 (Gaussian anti-concentration). For a Gaussian random variable X with mean μ and
standard deviation σ, for any β > 0,
p/ J >β)≥「.
σ
18
Published as a conference paper at ICLR 2021
ProofofLemma 4.4. Since et,k 〜N(f (xt,k; θt-ι), ν2σ2,k) conditioned on Ft, We have
Pr (et,k + e(m) > h(xt,k)∣Ft,Eμ)
Pr (亍t,k - f(xt,k； θt-i) + c(m) > h(xt,k) - f(xt,k； θt-i)
νσt,k	νσt,k	∣
≥ pr (正,k - f(xt,k； θt-i) + e(m) > ∣h(xt,k) - f(xt,k； θt-i)∣
Ft, Eμ
νσt,k	νσt,k
Pr (K,k - f(xt,k； θt-i) > ∣h(xt,k) - f (xt,k； θt-i)∣ - Hm)
νσt,k
≥ Pr (et,k - f(xt,k; θtT) > 1 Ft, Eμ
νσt,k
νσt,k
1
≥ 4e√π,
Ft, Eμ
Ft, Eμ
where the first inequality is due to |x| ≥ x, and the second inequality follows from event Etμ, i.e.,
∀k ∈ [K],	|f (xt,k; θt-1) - h(xt,k)| ≤ νσt,k + (m).
□
B.4	Proof of Lemma 4.5
Proof of Lemma 4.5. Consider the following two events at round t:
A = {∀k ∈ St万t,k <7t,a↑ |Ft, Eμ},
B = {at ∈St∣Ft, Ef}.
Clearly, A implies B, since at = argmaxk ret,k. Therefore,
Pr (at ∈St∣Ft, Eμ) ≥ Pr (∀k ∈ St" <Ftq∣Ft, E〉
Suppose Eμ also holds, then it is easy to show that ∀k ∈ [K],
∣h(xt,k) - rt,k | ≤ ∣h(xt,k) - f (xt,k ； θt)l + ∣f(xt,k ； θt) - et,k | ≤ e(m) + (1 + Vt)Vtσt,k. (B.4)
Hence, for all k ∈ St, we have that
h(XtH ) - et,k ≥ h(XtH ) - h(xt,k) - |h(xt,k) - et,k | ≥ e(m),
where we used the definitions of saturated arms in Definition 4.4, and of Etμ and Et in (4.1).
Consider the following event
C = {h(xt川)一e(m) < Ft,a： |Ft, Eμ}.
Since Et implies h(xtq)-e(m) ≥ et,k, We have that if C, Etσ holds, then A holds, i.e. Et ∩C ⊆ A.
Taking union with Etσ we have that C = Et ∪ Et ∩C⊆A∪Etσ, which implies
Pr(A) + Pr(Et) ≥ Pr(C).	(B.5)
Then, (B.5) implies that
Pr (∀k ∈ St,rt,k <Ftq∣Ft, Ef) ≥ Pr 伍川 + e(m) > h(xtq )∣Ft, Ef) - Pr(Et Ft, Ef)
11
≥ ——-——,
4e√π	t2
where the first inequality is from aJ= is a special case of ∀k ∈ [K], the second inequality is
from Lemmas 4.2 and 4.4.	口
19
Published as a conference paper at ICLR 2021
B.5	Proof of Lemma 4.6
To prove Lemma 4.6, we will need an upper bound bound on δt,k .
Lemma B.9. For any time t ∈ [T], k ∈ [K], and δ ∈ (0, 1), if the network width m satisfies
Condition 4.1, we have, with probability at least 1 - δ, that
σt,k ≤ C√L,
where C is a positive constant.
ProofofLemma 4.6. Recall that given Ft and E%, the only randomness comes from sampling rt^
for k ∈ [K]. Let" be the unsaturated arm with the smallest σt,∙, i.e.
kt = argmin σt,k,
k∈St
then we have that
E[σt,at |Ft, Eμ] ≥ E[σt,at∣Ft, Et ,at ∈ St] Pr(at ∈ St∣Ft, E%)
≥ σtkt (4e√π -12),	(B.6)
where the first inequality ignores the case when at ∈ St, and the second inequality is from
Lemma 4.5 and the definition of kkt mentioned above.
If both Et and Et hold, then
∀k ∈ [K], |h(xt,k) - ret,k| ≤ (m) + (1 + ct)νσt,k,	(B.7)
as proved in equation (B.4). Thus,
h(xt,at ) - h(xt,at ) = h(xt,at ) - h(xt,鼠)+ h(xt,鼠)-h(xt,at )
≤ (1 + ct)νσt,kt + 2e(m) + h(xt,kt) - ret,fet - h(xt,at)
+ ret,at + ret,fet - ret,at
≤ (1 + Ct)V(2σt,kt + σt,at) + 4e(m),	(B.8)
where the first inequality is from Definition 4.4 and kkt ∈/ St, and the second inequality comes from
equation (B.7). Since a trivial bound on h(xt,a*) - h(xt,at) could be get by h(xt,a*) - h(xt,at) ≤
∣h(xt川 )| + ∣h(xt,at)| ≤ 2, then we have
E[h(xt川) - h(xt,at )|Ft, Eμ] = E[h(xt川) - h(xt,at )|Ft, Etμ, Et]Pr(Ef)
+ E[h(xtq) - h(xt,at )|Ft, Eμ,Et]Pr(Etσ)
2
≤ (1 + ct)ν(2σt,鼠 + E[σt,at lFt, Er]) + 4e(m) + t2
≤ (1 + ct)ν 22EWat lFt1Eμ] + E[σt,at |Ft, Eμ][ + 4e(m) + V
∖	4e√π- t2	t	t
≤ 44e√π(1 + q)νE[σt,at |Ft, Eμ] + 4e(m) + 2t-2,
where the inequality on the second line uses the bound provide in (B.8) and the trivial bound of
h(xt,a*) - h(xt,at) for the second term plus Lemma 4.2, the inequality on the third line uses the
bound of σt,kt provide in (B.6), inequality on the forth line is directly calculated by 1 ≤ 4e√π and
一ɪ ≤ 20e√π,
4e√π	t2
which trivially holds since LHS is negative when t ≤ 4 and when t = 5, the LHS reach its maximum
as ≈ 84.11 < 96.36 ≈ RHS.
Noticing that |h(x)| ≤ 1, it is trivial to further extend the bound as
E[h(xt川) - h(xt,at )|Ft, Eμ] ≤ min{44e√π(1 +。加旧⑸用 |Ft, Eμ], 2} + 4e(m) +2t-2,
20
Published as a conference paper at ICLR 2021
and since We have 1 + Ct ≥ 1 and V = B + R,logdet(I + H∕λ) +2 + 2log(1∕δ) ≥ B, recall
22e√∏B ≥ 1, it is easy to verify the following inequality also holds:
E[h(Xt,a" - h(xt,at )lFt, Eμ]
≤ 44e√∏(1 + Ct)Vmin{E[σt,at∣Ft, Eμ], 1} + 4e(m) + 2t-2
≤ 44e√∏(1 + Ct)VCι√LE[min{σt,at, 1}∣Ft,Eμ] +4e(m) + 2t-2,
where we use the fact that there exists a constant Ci such that σtg is bounded by Ci √L with
probability 1 - δ provided by Lemma B.9. Merging the positive constant Ci with 44e√π, we get
the statement in Lemma 4.6.	□
B.6 Proof of Lemma 4.7
We start with introducing the Azuma-Hoeffding inequality for super-martingale:
Lemma B.10 (Azuma-Hoeffding Inequality for Super Martingale). Ifa super-martingale Yt, corre-
sponding to filtration Ft satisfies that |Yt - Yt-i | ≤ Bt, then for any δ ∈ (0, 1), w.p. 1 - δ, we
have
Yt - Y0 ≤ t
t
2 log(1∕δ) X Bi2.
i=i
Proof of Lemma 4.7. From Lemma B.9, we have that there exists a positive constant Ci such that
Xt defined in (4.5) is bounded with probability 1 - δ by
|Xt| ≤ ∣∆t| + Ci(1 + ct)ν√Lmin{σt,at, 1} + 4e(m) + 2t-2
≤ 2+ 2t-2 + CiC2(1 +Ct)VL+4(m)
≤ 4 + CiC2(1 + Ct)VL+ 4(m)
where the first inequality uses the fact that |a - b| ≤ |a| + |b|; the second inequality is from
Lemma B.9 and the fact that h ≤ 1, where C2 is a positive constant used in Lemma B.9; the
third inequality uses the fact that t-2 ≤ 1. Noticing the fact that Ct ≤ CT, and from Lemma 4.6, we
know that with probability at least 1 - δ, Yt is a super martingale. From Lemma B.10, we have
YT - Yo ≤ (4 + C1C2(1 + CT)νL + 4e(m))P2log(1∕δ)T.	(B.9)
Considering the definition of YT in (4.5), (B.9) is equivalent to
TT
X Ai ≤ 4Te(m) + 2 X t-2
i=i	i=i
T
+ C1(1 + CT )ν√Lɪ2 min{σt,at, 1}
i=i
+ (4 + C1C2 (1 + CT )νL + 4e (m)) √2 log(1∕δ)T,
then by utilizing P∞=ι t-2 = π2∕6, and merge the constant Ci with 44e√π, taking union bound of
the probability bound of Lemma 4.6, B.10, B.9, we have the inequality above hold with probability
at least 1 - 3δ. Re-scaling δ to δ∕3 and merging the product of CiC2 as a new positive constant
leads to the desired result.	□
B.7 Proof of Lemma 4.8
We first state a technical lemma that will be useful:
Lemma B.11 (Lemma 11, Abbasi-Yadkori et al. (2011)). Let {vt}t∞=i be a sequence in Rd, and
define Vt = λI + Pit=i vivi>. If λ≥ 1, then
Tt
min{vt>Vt--iivt-i, 1} ≤ 2 log det I+λ-i	vivi>
i=i	i=i
21
Published as a conference paper at ICLR 2021
ProofofLemma 4.8. First, recall σt,k defined in Definition B.2 and the bound of σt,k -σt,k provided
in Lemma B.4. We have that there exists a positive constants C1 such that
T
min{σt,at
i=1
TT
1} = Emin{^,at,1} + J2(σt,at - 内,aJ
i=1	i=1
≤t
T
T ^X min{σ2at, 1} + CiT13/6 PiogmmT/6λ-2∕3L9/2,
i=1
where the first term in the inequality on the second line is from Cauchy-Schwartz inequality, and the
second term is from Lemma B.4.
From Definition B.2, we have
TT
X min{σ2,at, 1} ≤ λ X min{g(xt,at, θθ)>U=lg(xt,at, θ0)/m, 1}
i=1	i=1
≤ 2λ log det (l + λ-1 X g(xt,at ； θo)g(xt,at ； θo)>∕m)
i=1
=2λ logdet(I + λ-1JT J>∕m)
=2λ log det(I + λ-1J> JT ∕m)
= 2λ log det(I + λ-1KT)
where the first inequality moves the positive parameter λ outside the min operator and uses the
definition of σt,k in Definition B.2, then the second inequality utilizes Lemma B.11, the first equality
use the definition of Jt in Definition B.2, the second equality is from the fact that det(I + AA>)=
det(I + A>A), and the last equality uses the definition of Kt in Definition B.2. From Lemma B.7,
we have that
log det(I + λ-1KT) ≤ log det(I + λ-1H) + 1
under condition on m and η presented in Theorem 3.5. By taking a union bound we have, with
probability 1 - 2δ, that
XXmin{σt,at, 1} ≤ ,2λT(elog(1 + TK) + 1) + CiT13/6Plog^m-1/6λ-2/3L9/2,
i=1
where We use the definition of d in Definition 3.2. Replacing δ with δ∕2 completes the proof. □
C Proof of Auxiliary Lemmas in Appendix B
In this section, we are about to show the proof of the Lemmas used in Appendix B, we will start
with the following NTK Lemmas. Among them, the first is to control the difference between the
parameter learned via Gradient Descent and the theoretical optimal solution to linearized network.
Lemma C.1 (Lemma B.2, Zhou et al. (2019)). There exist constants {Ci}i5=1 > 0 such that for any
δ > 0, if η, m satisfy that for all t ∈ [T],
2Pt∕λ ≥ CImTL-3∕2[log(TKL2∕δ)]3∕2,
2pt∕λ ≤ C2 min {m1∕2L-6[logm]-3/2, m7∕8((λη)2L-6t-1(logm)-1)3/8},
η ≤ C3 (mλ + tmL)-1 ,
m1/6 ≥ C4pogmL7/2t7/6X-7/6(1 + pt∕λ),
then with probability at least 1 - δ over the random initialization of θ0, for any t ∈ [T], we have
that kθt-i - θo∣∣2 ≤ 2/t∕(mλ) and
kθt-i- θo - U=1Jt-irt-i∕m∣∣2
≤ (1 — ηmλ)Jpt∕(mλ) + C5mΓ-2/PlogmL7∕2t5∕3λ-5∕3(1 + pt∕λ).
22
Published as a conference paper at ICLR 2021
And the next lemma, controls the difference between the function value of neural network and the
linearized model:
Lemma C.2 (Lemma 4.1, Cao & Gu (2019)). There exist constants {Ci }i3=1 > 0 such that for any
δ > 0, if τ satisfies that
Cim-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ C2L-6[logm]-3/2,
then with probability at least 1 - δ over the random initialization of θ0, for all θ, θ satisfying
kθe- θ0k2 ≤ τ, kθb- θ0k2 ≤ τ andj ∈ [TK] we have
f(xj;e) - f(xj;b) -hg(xj; b), e - bi ≤ C3T4/3L3Pmlog m.
Furthermore, to continue with, next lemma is proposed to control the difference between the gradient
and the gradient on the initial point.
Lemma C.3 (Theorem 5, Allen-Zhu et al. (2018)). There exist constants {Ci}i3=1 > 0 such that for
any δ ∈ (0, 1), if τ satisfies that
Cim-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ C2L-6[logm]-3/2,
then with probability at least 1 - δ over the random initialization of θ0 , for all kθ - θ0 k2 ≤ τ and
j ∈ [TK] we have
kg(xj; θ) - g(xj; Θ0)k2 ≤ C3Plog mτ 1/3L3kg(xj; Θ0)k2.
Also, we need the next lemma to control the gradient norm of the neural network with the help of
NTK.
Lemma C.4 (Lemma B.3, Cao & Gu (2019)). There exist constants {Ci}i3=1 > 0 such that for any
δ > 0, if T satisfies that
Cim-3/2L-3/2[log(TKL2/6)]3/2 ≤ T ≤ C2L-6[logm]-3/2,
then with probability at least 1 - δ over the random initialization of θo, for any ∣∣θ - Θ0k2 ≤ T and
j ∈ [TK] We have Ilg(Xj; Θ)∣f ≤ C3√mL.
Finally, as literally shows, we can also provide bounds on the kernel provided by the linearized
model and the NTK kernel if the network is width enough.
Lemma C.5 (Lemma B.1, ZhoUetaL (2019)). Set K = PT=I PK=I g(xt,k； θo)g(xt,k； θo)/m,
recall the definition of H in Definition 3.1,then there exists a constant C1 such that
m ≥ CiL6log(TKL/b)c-4,
we coUld get that ∣K - H∣F ≤ TK.
EqUipped with these lemmas, we coUld continUe for oUr proof.
C.1 Proof of Lemma B.4
ProofofLemma B.4. Firstly, set T = 2ʌ/t/(mʌ), then we have the condition on the network m
and learning rate η satisfy all of the condition need from Lemma C.1 to Lemma C.5. ThUs from
Lemma C.1, we have that there exists ∣θt-1 - θ0∣2 ≤ T, thUs from Lemma C.4, we have that there
exists positive constant Ci such that ∣∣g(x; θt-1)k2 ≤ Ci√mL, ∣∣g(x; Θ0)k2 ≤ Ci√mL, consider
the fUnction defined as
ψ(a, ai,…
it is then easy to verify that
/ g g(Xt,k ； θt-i) g(Xi,aι ； θi)
ψ V—√m —, 一√m —, •
/ ( g(xt,k ； θo) g(xi,aι ； θo)
ψv-√m-, -√m-
g(Xt-i,at-1; θt-i)
√m	J=σtk
g(Xt-i,at-1; θ0)
•,	√m) = σt,k
23
Published as a conference paper at ICLR 2021
then we obtain that the function ψ is defined under the domain ∣∣a∣2 ≤ C1 √L, ∣∣a∕∣2 ≤ C1 √L then
by taking the derivation w.r.t. ψ2,we have that
2ψ∂ψ = (∂a)>	λI + a^aɪ ) a + a>	λI + a^aj )	∂a
+ aτ
-1 t-1
λI + aiaτ ) X ((∂ai)aτ + ai∂a
λI + aiaɪ I a,
i=1
by taking trace with both side and utilizing tr(AB) = tr(BA) and tr(α>β) = tr(ɑβ>), We have
that
t-1
+ 2 X(∂ a,)
j=1
2tr(ψ∂ψ) = tr I 2(∂a)τ
λI + a^aj I a
λI + a^a[ I aaτ
λI + a^a[ I	a,
thus by setting C = ( Pt-I λI + a^aj) for simplicity and decompose C = QτDQ, b = Qa
where D = diag(01, ∙ ∙ ∙ ,ρp) as the eigen-value of C, we have that
L , Ca UL … ∕aτC2a ∕bτD2b
vaψ=#无 JVaψ∣2 = V MCa = V b^Db
PL吐%
≤ 1∕√λ
∖ P=1 b %
where the last inequality is from the fact that C W 1∕λI, which indicates that all eigen-value % ≤
1∕λ, for the same reason, we have
IlVaiψ∣2 = kCaaTCai∣2 ≤ M∣2∣√aTCh =忖∣2kCa∣2√⅛ ≤ 同1同/6
√a 1 Ca	√a 1 Ca	√a 1 Ca
Thus under the domain that ∣a∣2 ≤ C√L, ∣a∕∣2 ≤ C√L, we have that
∣Vaψ∣2 ≤ 1∕√λ, ∣Vaiψ∣2 ≤ CL/瓜
Then, Lipschitz continuity implies
∖σt,k - σt,k I
g(xt,k；仇-1) g(xi,αι ； θ1)
√m
√m
g(xt,k； θo) g(xi@ ； θo)
√m
√m
g(Xt-Is-1 ； θt-1)
,	√m
g(Xt-1,αt-1 ； θo) ∖∖
√m	/
≤ SUp{∣Vaψ∣∣2}
g(xt,k； θt-1) _ g(xt,k； θo)
√m
√m
t-1
+ X SUp{∣Vaiψ∣2}
i=1
g(xi,a ； θi)	g(xi,a ； θo)
----=-----------=----
√m
1	g(xt,k； θt) - g(xt,k; θo)
√m
√m
+ CL X
2	√λ 士
g(xi,a； θi) - g(xi,a； θo)
√m
2
(C.1)
≤
ψ
-ψ
2
2
By Lemma C.3 with T = 2y∕t∕mλ, there exist positive constants C and C so that each gradient
difference in (C.1) is bounded by
∣∣g(x; θ) - g(x; θo)∣∣2 ≤ CPIOgmτ 1/3L3∣∣g(x; θo)k2∕√m
≤ Cplog mt1/6m-1/6X-1/6L7/2.
24
Published as a conference paper at ICLR 2021
Thus, since we obtain that there exists constant C5 such that
∣σt,k - σt,k| ≤ CιPiogmt7∕6m-1∕6λ-2/3L9/2,
where We use the fact that Ci = max{C3, C3C2} and L ≥ 1 to merge the first term into the
summation. This inequality is based on Lemma C.1, Lemma C.3 and Lemma C.4, thus it holds with
probability at least 1 - 3δ. Replacing δ with δ∕3 completes the proof.	口
C.2 Proof of Lemma B.5
ProofofLemma B.5. Setting T = 2y∕t∕mλ, we have the condition on the network m and learning
rate η satisfy all of the condition needed by Lemmas C.1 to C.5. From Lemma C.1 we have kθt-1 -
θ0 k2≤ τ. Then, by Lemma C.2, there exists a constant C1 such that
∣f(xt,k; θt-i) -hg(xt,k θo), θt-i - θoi∣ ≤ Cιt2∕3m-1∕6λ-2∕3L3piog m,	(C.2)
Using the bound on θt-ι - θ0 - U-1i Jt-Irt-1 / m provided in Lemma C.1 and the norm of gradient
bound given in Lemma C.4, we have that there exist positive constants Ci, C2 such that
Kg(Xt,k; θo), θt-i - θoi - hg(xt,k; θo), U=Jt-irt-i/mil
≤ kg(xt,k)k2kθt-i - θo - U-ΛJt-irt-i∕m∣∣2
≤ Ci√mL((1 — ηmλ)Jpt∕(mλ) 十 C2m-2∕3plog mL7∕2t5∕3λ-5∕3(l + pt∕λ))
=C2(l - ηmλ)JPtL∕λ + C3m-i/6plogmL4t5∕3λ-5∕3(1 + Pt∕λ),	(C.3)
where C? = Ci, C3 = CiC2. Combining (C.2) and (C.3), we have
f(xt,k； θt-i) -〈g(xt,k； θo),U--iiJt-Irt-i/m)I ≤ Cit2∕3m-i∕6λ-2∕3L3piog m
+ C2 (1 - ηmλ)J PtL∕λ
+ C3m-i∕6Plog mL4t5∕3λ-5∕3(1 + pt∕λ),
which holds with probability 1 -3δ with a union bound (Lemma C.4, Lemma C.1, and Lemma C.2).
Replacing δ with δ∕3 completes the proof.	口
C.3 Proof of Lemma B.7
Proof of Lemma B.7. From the definition ofKt, we have that
logdet(I+λ-iKt) = log det I + X g(xi,ai; θ0)g(xi,ai; θ0)>∕(mλ)
i=i
TK
≤ log det I +ΣΣg(xi,ai; θ0)g(xi,ai; θ0)>∕(mλ))
t=i k=i
= log det(I + K∕λ)
≤ logdet(I+H∕λ+(H-K)λ) +T(λ- 1)
≤ log det(I + H∕λ) + h(I + H∕λ)-i, (K - H)∕λi
≤ log det(I + H∕λ) + k(I + H∕λ)-ikF k(K - H)kF ∕λ
≤ logdet(I + H∕λ) + √TKk(K - H)IlF
≤ log det(I + H∕λ) +1
where the the first inequality is because the double summation on the second line contains more
elements than the summation on the first line. The second inequality utilizes the definition of K in
LemmaC.5 and H in Definition 3.1, the third inequality is from the convexity of log det(∙) function,
and the forth inequality is from the fact that hA, Bi ≤ IAIF IBIF. Then the fifth inequality is from
the fact that k AkF ≤ √TKk A∣∣2 if A ∈ Rtk×tk and λ ≥ 0. Finally, the sixth inequality utilizes
Lemma C.5 by setting E = (TK)-3∕2 with m ≥ CiL6T6K6 log(TKL∕δ), where we conclude our
proof.	口
25
Published as a conference paper at ICLR 2021
C.4 Proof of Lemma B.9
ProofofLemma B.9. Set T in Lemma C.4 as 2,t∕(mλ). Then the network width m and learning
rate η satisfy all of the condition needed by Lemma C.1 to C.5. Hence, there exists Ci such that
kg(x; θ)k2 ≤ kg(x; θ)∣∣F ≤ Cι√mL for all x, since it is easy to verify that U-1 W λ-1I. Thus
we have that for all t ∈ [T], k ∈ [K],
σt2,k = λg>(xt,k; θt-1)Ut--11g(xt,k; θt-1)∕m ≤ kg(xt,k; θt-1)k22∕m ≤ C52L.
Therefore, We could get that σt,k ≤ Ci√L, with probability 1 一 2δ by taking a union bound
(Lemmas C.1 and C.4). Replacing δ with δ∕2 completes the proof.	□
ɪʌ	ɪ ɪɪ	ɪʌ	L	ɪʌ	T
D AN UPPER B OUND OF EFFECTIVE DIMENSION d
We now provide an example, showing when all contexts xi concentrate on a d0-dimensional non-
linear subspace of the RKHS space spanned by NTK, the effective dimension d is bounded by d0 .
We consider the case when λ= 1, L = 2. Suppose that there exists a constant d0 such that for any
i > d0, 0 < λi(H) ≤ 1/(T K). Then the effective dimension d can be bounded as
e	log det(I + H)
=log(1 + TK)
TK
TK
d0
TK
i=i
log(1 + λi(H)) ≤	λi(H) =	λi(H)+	λi(H).
i=i
i=i
1-----{------'
I1
i=d0+i
|
-- J
{z^^^^^
I2
≤
For Ii and I2 we have
Ii
≤
d0
X kHk2 = Θ(d0), I ≤ TK ∙ 1/(TK) = 1,
i=i
Therefore, the effective dimension satisfies that d ≤ d0 + 1. To show how to satisfy the requirement,
we first give a charcterization of the RKHS space spanned by NTK. By Bietti & Mairal (2019); Cao
et al. (2019) we know that each entry of H has the following formula:
∞	N(d,k)
Hi,s = ∑>k E Yk,j(Xi)Ykj(xs),
k=0 j=i
where Yk,j for j = 1, . . . , N(d, k) are linearly independent spherical harmonics of degree k in d
variables, d is the input dimension, N(d, k) = (2k + d 一 2)∕k ∙ Cd-d-3, μk = Θ(max{k-d, (d 一
1)-k+1}). In that case, the feature mapping (√μkYkj(x))k,j maps any context X from Rd to a
RKHS space R corresponding to H. Let yi ∈ R denote the mapping for xi. Then if there exists a
d0-dimension subspace R0 such that for all i, kyi 一 zi k 1 where zi is the projection of yi onto
Rd0, the requirement for λi(H) holds.
26