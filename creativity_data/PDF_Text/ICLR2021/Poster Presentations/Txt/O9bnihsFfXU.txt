Published as a conference paper at ICLR 2021
Implicit Under-Parameterization Inhibits
Data-Efficient Deep Reinforcement Learning
Aviral Kumar* 1,2, Rishabh Agarwal* 2,3, Dibya Ghosh1, Sergey LeVine1,2
1UC Berkeley, 2Google Research, 3MILA, Universite de Montreal
Ab stract
We identify an implicit under-parameterization phenomenon in value-based deep
RL methods that use bootstrapping: when value functions, approximated using
deep neural networks, are trained with gradient descent using iterated regression
onto target values generated by previous instances of the value network, more
gradient updates decrease the expressivity of the current value network. We char-
acterize this loss of expressivity via a drop in the rank of the learned value net-
work features, and show that this typically corresponds to a performance drop.
We demonstrate this phenomenon on Atari and Gym benchmarks, in both offline
and online RL settings. We formally analyze this phenomenon and show that it
results from a pathological interaction between bootstrapping and gradient-based
optimization. We further show that mitigating implicit under-parameterization by
controlling rank collapse can improve performance.
1	Introduction
Many pervasive deep reinforcement learning (RL) algorithms estimate value functions using boot-
strapping, that is, by sequentially fitting value functions to target value estimates generated from
the value function learned in the previous iteration. Despite high-profile achievements (Silver et al.,
2017), these algorithms are highly unreliable due to poorly understood optimization issues. Al-
though a number of hypotheses have been proposed to explain these issues (Achiam et al., 2019;
Bengio et al., 2020; Fu et al., 2019; Igl et al., 2020; Liu et al., 2018; Kumar et al., 2020a), a com-
plete understanding remains elusive.
We identify an “implicit under-parameterization” phenomenon that emerges when value networks
are trained using gradient descent combined with bootstrapping. This phenomenon manifests as
an excessive aliasing of features learned by the value network across states, which is exacer-
bated with more gradient updates. While the supervised deep learning literature suggests that
some feature aliasing is desirable for generalization (e.g., Gunasekar et al., 2017; Arora et al.,
2019), implicit under-parameterization exhibits more pronounced aliasing than in supervised learn-
ing. This over-aliasing causes an otherwise expressive value network to implicitly behave as an
under-parameterized network, often resulting in poor performance.
Implicit under-parameterization becomes aggravated when the rate of data re-use is increased, re-
stricting the sample efficiency of deep RL methods. In online RL, increasing the number of gradient
steps in between data collection steps for data-efficient RL (Fu et al., 2019; Fedus et al., 2020b)
causes the problem to emerge more frequently. In the extreme case when no additional data is
collected, referred to as offline RL (Lange et al., 2012; Agarwal et al., 2020; Levine et al., 2020),
implicit under-parameterization manifests consistently, limiting the viability of offline methods.
We demonstrate the existence of implicit under-parameterization in common value-based deep RL
methods, including Q-learning (Mnih et al., 2015; Hessel et al., 2018) and actor-critic (Haarnoja
et al., 2018), as well as neural fitted-Q iteration (Riedmiller, 2005; Ernst et al., 2005). To isolate the
issue, we study the effective rank of the features in the penultimate layer of the value network (Sec-
tion 3). We observe that after an initial learning period, the rank of the learned features drops steeply.
As the rank decreases, the ability of the features to fit subsequent target values and the optimal value
function generally deteriorates and results in a sharp decrease in performance (Section 3.1).
* Equal Contribution. Correspondence to Aviral Kumar < aviralk@berkeley.edu > and Rishabh
Agarwal < rishabhagarwal@google.com >.
1
Published as a conference paper at ICLR 2021
Figure 1: Implicit under-parameterization. Schematic diagram depicting the emergence of an effective rank
collapse in deep Q-learning. Minimizing TD errors using gradient descent with deep neural network Q-function
leads to a collapse in the effective rank of the learned features Φ, which is exacerbated with further training.
To better understand the emergence of implicit under-parameterization, we formally study the dy-
namics of Q-learning under two distinct models of neural net behavior (Section 4): kernel re-
gression (Jacot et al., 2018; Mobahi et al., 2020) and deep linear networks (Arora et al., 2018).
We corroborate the existence of this phenomenon in both models, and show that implicit under-
parameterization stems from a pathological interaction between bootstrapping and the implicit regu-
larization of gradient descent. Since value networks are trained to regress towards targets generated
by a previous version of the same model, this leads to a sequence of value networks of potentially
decreasing expressivity, which can result in degenerate behavior and a drop in performance.
The main contribution of this work is the identification of implicit under-parameterization in deep
RL methods that use bootstrapping. Empirically, we demonstrate a collapse in the rank of the
learned features during training, and show it typically corresponds to a drop in performance in the
Atari (Bellemare et al., 2013) and continuous control Gym (Brockman et al., 2016) benchmarks in
both the offline and data-efficient online RL settings. We verify the emergence of this phenomenon
theoretically and characterize settings where implicit under-parameterization can emerge. We then
show that mitigating this phenomenon via a simple penalty on the singular values of the learned
features improves performance of value-based RL methods in the offline setting on Atari.
2	Preliminaries
The goal in RL is to maximize long-term discounted reward in a Markov decision pro-
cess (MDP), defined as a tuple (S, A, R, P, γ) (Puterman, 1994), with state space S, ac-
tion space A, a reward function R(s, a), transition dynamics P(s0|s, a) and a discount fac-
tor Y ∈ [0,1). The Q-function Qn(s, a) for a policy π(a∣s), is the expected long-term
discounted reward obtained by executing action a at state S and following ∏(a∣s) thereafter,
Qπ(s, a) := E [Pt∞=0 γtR(st, at)]. Qπ(s, a) is the fixed point of the Bellman operator Tπ,
∀s,a: TπQ(s, a) := R(s,a) + γEso〜P(.|s,a),a，〜∏(∙w) [Q(s0,a0)], which can be written in vector
form as: Qn = R + YPπQπ. The optimal Q-function, Q*(s, a), is the fixed point of the Bellman
optimality operator T: TQ(s, a) := R(s, a) + γEso〜P(小㈤[maxa，Q(s0, a0)].
Practical Q-learning methods (e.g., Mnih et al., 2015; Hessel et al., 2018; Haarnoja et al., 2018)
convert the Bellman equation into an bootstrapping-based objective for training a Q-network, Qθ,
via gradient descent. This objective, known as mean-squared temporal difference (TD) error, is
given by: L(θ) = Ps a (R(s, a) + γQθ(s0, a0) - Q(s, a))2, where Qθ is a delayed copy of the
Q-function, typically referred to as the target network. These methods train Q-networks via gradient
descent and slowly update the target network via Polyak averaging on its parameters. We refer the
output of the penultimate layer of the deep Q-network as the learned feature matrix Φ, such that
Q(s,a) = WTΦ(s, a), where W ∈ Cd and Φ ∈ RlSllAl×d.
For simplicity of analysis, we abstract deep Q-learning
methods into a generic fitted Q-iteration (FQI) frame-
work (Ernst et al., 2005). We refer to FQI with neural
nets as neural FQI (Riedmiller, 2005). In the k-th fitting
iteration, FQI trains the Q-function, Qk, to match the tar-
get values, yk = R+YPπ Qk-1 generated using previous
Q-function, Qk-1 (Algorithm 1). Practical methods can
be instantiated as variants of FQI, with different target
update styles, different optimizers, etc.
Algorithm 1 Fitted Q-Iteration (FQI)
1:	Initialize Q-network Qθ, buffer μ.
2:	for fitting iteration k in {1, . . . , N} do
3:	Compute Qθ (s, a) and target values
yk(s, a) = r + γ maxa0 Qk-1 (s0, a0)
on {(s, a)} 〜μ for training
4:	Minimize TD error for Qθ via t =
1,… ,T gradient descent updates,
minθ (Qθ(s, a) - yk)2
5:	end for
2
Published as a conference paper at ICLR 2021
Ooooo
6 5 4 3 2
IO.0Ns ∙(φ)⅛fuβjs
—Supervised
---T-10
——T-200
10
100 200 300 400 500
Fitting iterations
Figure 2: Offline RL. srankδ (Φ) and performance of neural FQI on gridworld, DQN on Atari and SAC on
Gym environments in the offline RL setting. Note that low rank (top row) generally corresponds to worse policy
performance (bottom row). Rank collapse is worse with more gradient steps per fitting iteration (T= 10 VS.
200 on gridworld). Even when a larger, high coverage dataset is used, marked as DQN (4x data), rank collapse
occurs (for Asterix also see Figure A.2 for a complete figure with a larger number of gradient updates).
3	IMPLICIT UNDER-PARAMETERIZATION IN DEEP Q-LEARNING
In this section, we empirically demonstrate the existence of implicit under-parameterization in deep
RL methods that use bootstrapping. We characterize implicit under-parameterization in terms of
the effective rank (Yang et al., 2019) of the features learned by a Q-network. The effective rank
of the feature matrix Φ, for a threshold δ (we choose δ = 0.01), denoted as srankδ(Φ), is given
by srankδ(Φ) = min {k : PdT ；i(；) ≥ 1 - δ}, where {σi(Φ)} are the singular values of Φ in de-
creasing order, i.e., σι ≥ ∙∙∙ ≥ ◎& ≥ 0. Intuitively, srankδ(Φ) represents the number of “effective”
unique components of the feature matrix Φ that form the basis for linearly approximating the Q-
values. When the network maps different states to orthogonal feature vectors, then srankδ (Φ) has
high values close to d. When the network “aliases” state-action pairs by mapping them to a smaller
subspace, Φ has only a few active singular directions, and srankδ (Φ) takes on a small value.
Definition 1. Implicit under-parameterization refers to a reduction in the effective rank ofthefea-
tureS, Srankδ (Φ), that occurS implicitly aS a by-product of learning deep neural network Q-functionS.
While rank decrease also occurs in supervised learning, it is usually beneficial for obtaining gener-
alizable solutions (Gunasekar et al., 2017; Arora et al., 2019). However, we will show that in deep
Q-learning, an interaction between bootstrapping and gradient descent can lead to more aggressive
rank reduction (or rank collapse), which can hurt performance.
Experimental setup. To study implicit under-parameterization empirically, we compute srankδ (Φ)
on a minibatch of state-action pairs sampled i.i.d. from the training data (i.e., the dataset in the offline
setting, and the replay buffer in the online setting). We investigate offline and online RL settings on
benchmarks including Atari games (Bellemare et al., 2013) and Gym environments (Brockman et al.,
2016). We also utilize gridworlds described by Fu et al. (2019) to compare the learned Q-function
against the oracle solution computed using tabular value iteration. We evaluate DQN (Mnih et al.,
2015) on gridworld and Atari and SAC (Haarnoja et al., 2018) on Gym domains.
Data-efficient offline RL. In offline RL, our goal is to learn effective policies by performing Q-
learning on a fixed dataset of transitions. We investigate the presence of rank collapse when deep
Q-learning is used with broad state coverage offline datasets from Agarwal et al. (2020). In the
top row of Figure 2, we show that after an initial learning period, srankδ (Φ) decreases in all do-
mains (Atari, Gym and the gridworld). The final value of srankδ (Φ) is often quite small 一 e.g., in
Atari, only 20-100 singular components are active for 512-dimensional features, implying signif-
icant underutilization of network capacity. Since under-parameterization is implicitly induced by
the learning process, even high-capacity value networks behave as low-capacity networks as more
training is performed with a bootstrapped objective (e.g., mean squared TD error).
On the gridworld environment, regressing to Q* using supervised regression results in amuch higher
srankδ(Φ) (black dashed line in Figure 2(left)) than when using neural FQI. On Atari, even when a
4x larger offline dataset with much broader coverage is used (blue line in Figure 2), rank collapse
still persists, indicating that implicit under-parameterization is not due to limited offline dataset size.
3
Published as a conference paper at ICLR 2021
Figure 2 (2nd row) illustrates that policy performance generally deteriorates as srank(Φ) drops, and
eventually collapses simultaneously with the rank collapse. While we do not claim that implicit
under-parameterization is the only issue in deep Q-learning, the results in Figure 2 show that the
emergence of this under-parameterization is strongly associated with poor performance.
To prevent confounding from the distribution mismatch between the learned policy and the offline
dataset, which often affects the performance of Q-learning methods, we also study CQL (Kumar
et al., 2020b), an offline RL algorithm designed to handle distribution mismatch. We find a sim-
ilar degradation in effective rank and performance for CQL (Figure A.3), implying that under-
parameterization does not stem from distribution mismatch and arises even when the resulting policy
is within the behavior distribution (though the policy may not be exactly pick actions observed in
the dataset). We provide more evidence in Atari and Gym domains in Appendix A.1.
Ooooo
Ooooo
5 4 3 2 1
IOOZ (le)⅛luw
Seaquest
--n=4
—n=16
20	40	60
Figure 3: Data Efficient Online RL. srankδ (Φ) and performance of neural FQI on gridworld, DQN on Atari
and SAC on Gym domains in the online RL setting, with varying numbers of gradient steps per environment
step (n). Rank collapse happens earlier with more gradient steps, and the corresponding performance is poor.
Data-efficient online RL. Deep Q-learning methods typically use very few gradient updates (n)
per environment step (e.g., DQN takes 1 update every 4 steps on Atari, n = 0.25). Improving the
sample efficiency of these methods requires increasing n to utilize the replay data more effectively.
However, We find that using larger values of n results in higher levels of rank collapse as well as
performance degradation. In the top row of Figure 3, we show that larger values of n lead to a
more aggressive drop in srankδ (Φ) (red VS. blue/orange lines), and that rank continues to decrease
with more training. Furthermore, the bottom row illustrates that larger values of n result in worse
performance, corroborating Fu et al. (2019); Fedus et al. (2020b). We find similar results with the
Rainbow algorithm (Hessel et al., 2018) (Appendix A.2). As in the offline setting, directly regressing
to Q* via supervised learning does not cause rank collapse (black line in Figure 3).
3.1	Understanding Implicit Under-parameterization and its Implications
How does implicit under-parameterization degrade performance? Having established the pres-
ence of rank collapse in data-efficient RL, we now discuss how it can adversely affect performance.
As the effective rank of the network features Φ decreases, so does the network,s ability to fit the
subsequent target values, and eventually results in inability to fit Q* . in the gridworld domain, we
measure this loss of expressivity by measuring the error in fitting oracle-computed Q* values via a
linear transformation ofΦ. When rank collapse occurs, the error in fitting Q* steadily increases dur-
ing training, and the consequent network is not able to predict Q* at all by the end of training (Fig-
ure 4a) 一 this entails a drop in performance. In Atari domains, we do not have access to Q*, and so
we instead measure TD error, that is, the error in fitting the target value estimates, R + γPπQk . in
Seaquest, as rank decreases, the TD error increases (Figure 4b) and the value function is unable to
fit the target values, culminating in a performance plateau (Figure 3). This observation is consistent
across other environments; we present further supporting evidence in Appendix A.4.
Does bootstrapping cause implicit under-parameterization? We perform a number of controlled
experiments in the gridworld and Atari environments to isolate the connection between rank collapse
and bootstrapping. We first remove confounding issues of poor network initialization (Fedus et al.,
2020a) and non-stationarity (igl et al., 2020) by showing that rank collapse occurs even when the
Q-network is re-initialized from scratch at the start of each fitting iteration (Figure 4c). To show that
4
Published as a conference paper at ICLR 2021
(a) Q* Fitting Error
(b) TD Error
(c) Q-REINITIALIZATION
Gradient Updates (x 62.5k)
(d) FQE vs. MC
Figure 4: (a) Fitting error for Q* prediction for n = 10 Vs n = 200 steps in Figure 3 (left). Observe that rank
collapse inhibits fitting Q* as the fitting error rises over training while rank collapses. (b) TD error for varying
values of n for Seaquest in Figure 3 (middle). TD error increases with rank degradation. (c) Q-network
re-initialization in each fitting iteration on gridworld. (d) Trend of srankδ (Φ) for policy evaluation based on
bootstrapped updates (FQE) vs Monte-Carlo returns (no bootstrapping). Note that rank-collapse still persists
with reinitialization and FQE, but goes away in the absence ofbootstrapping.
the problem is not isolated to the control setting, We show evidence of rank collapse in the policy
evaluation setting as well. We trained a value network using fitted Q-evaluation for a fixed policy
∏ (i.e., using the Bellman operator Tπ instead of T), and found that rank drop still occurs (FQE in
Figure 4d). Finally, we show that by removing bootstrapped updates and instead regressing directly
to Monte-Carlo (MC) estimates of the value, the effective rank does not collapse (MC Returns in
Figure 4d). These results, along with similar findings on other Atari environments (Appendix A.3),
our analysis indicates that bootstrapping is at the core of implicit under-parameterization.
4	Theoretical Analysis of Implicit Under-Parameterization
In this section, we formally analyze implicit under-parameterization and prove that training neural
networks with bootstrapping reduces the effective rank of the Q-network, corroborating the empiri-
cal observations in the previous section. We focus on policy evaluation (Figure 4d and Figure A.9),
where we aim to learn a Q-function that satisfies Q = R+γPπQ for a fixed π, for ease of analysis.
We also presume a fixed dataset of transitions, D, to learn the Q-function.
4.1	Analysis via Kernel Regression
We first study bootstrapping with neural networks through a mathematical abstraction that treats the
Q-network as a kernel machine, following the neural tangent kernel (NTK) formalism (Jacot et al.,
2018). Building on prior analysis of self-distillation (Mobahi et al., 2020), we assume that each
iteration of bootstrapping, the Q-function optimizes the squared TD error to target labels yk with a
kernel regularizer. This regularizer captures the inductive bias from gradient-based optimization of
TD error and resembles the regularization imposed by gradient descent under NTK (Mobahi et al.,
2020). The error is computed on (si, ai) ∈ D whereas the regularization imposed by a universal
kernel u with a coefficient of c ≥ 0 is applied to the Q-values at all state-action pairs as shown
in Equation 1. We consider a setting c > 0 for all rounds of bootstrapping, which corresponds to
the solution obtained by performing gradient descent on TD error for a small number of iterations
with early stopping in each round (Suggala et al., 2018) and thus, resembles how the updates in
Algorithm 1 are typically implemented in practice.
Qk+1 — arg Qm∈Q E (Q(si, ai) — yk(si, ai))2 + CE EU((s, a),(s0, a0))Q(s, a)Q(s0, a0). (1)
si ,ai∈D	(s,a) (s0 ,a0)
The solution to Equation 1 can be expressed as Qk+1 (s, a) = g(Ts,a) (cI + G)-1yk, where G is
the Gram matrix for a special positive-definite kernel (Duffy, 2015) and g(s,a) denotes the row
of G corresponding to the input (s, a) (Mobahi et al., 2020, Proposition 1). A detailed proof is
in Appendix C. When combined with the fitted Q-iteration recursion, setting labels yk = R +
γP πQk-1, we recover a recurrence that relates subsequent value function iterates
Qk+ι = G(CI + G)Tyk = G(CI + G)TR + γPπQk] = A (Pk=I Yi(PπA)I) R := AM*R. (2)
A
Equation 2 follows from unrolling the recurrence and setting the algorithm-agnostic initial Q-value,
Q0 , to be 0. We now show that the sparsity of singular values of the matrix Mk generally increases
over fitting iterations, implying that the effective rank of Mk diminishes with more iterations. For
this result, we assume that the matrix S is normal, i.e., the norm of the (complex) eigenvalues of S
is equal to its singular values. We will discuss how this assumption can be relaxed in Appendix A.7.
5
Published as a conference paper at ICLR 2021
Theorem 4.1. Let S be a Shorthandfor S = YPπ A and assume S is a normal matrix. Then
there exists an infinite, strictly increasing Sequence of fitting iterations, (kι )= Startingfrom
kι = 0, Such that, for any two singular-values σi(S) and σ, (S) of S with σ. (S) < σj (S),
∀ l ∈ N and l0 ≥ l,
σi(Mkιo) , σi(MkJ / %(S)
<	≤
σj (Mkio)	σj (MM - % (S)
(3)
Hence, srank δ (Mk〃) ≤ srank δ (Mkl). Moreover, if S is positive semi-definite, then (kι)∞=ι
N, i.e., srank ContinuouSly decreases in each fitting iteration.
We provide a proof of the theorem above as well as present a stronger variant that shows a gradual
decrease in the effective rank for fitting iterations outside this infinite sequence in Appendix C. As
k increases along the sequence of iterations given by k = (kl)l∞=1, the effective rank of the matrix
Mk drops, leading to low expressivity of this matrix. Since Mk linearly maps rewards to the Q-
function (Equation 2), drop in expressivity results of Mk in the inability to model the actual Qπ .
Summary of our analysis. Our analysis of bootstrapping and gradient descent from the view of
regularized kernel regression suggests that rank drop happens with more training (i.e., with more
rounds of bootstrapping). In contrast to self-distillation (Mobahi et al., 2020), rank drop may not
happen in every iteration (and rank may increase between two consecutive iterations occasionally),
but srankδ exhibits a generally decreasing trend.
4.2 Analysis with Deep Linear Networks under Gradient Descent
While Section 4.1 demonstrates rank collapse will occur in a kernel-regression model of Q-learning,
it does not illustrate when the rank collapse occurs. To better specify a point in training when
rank collapse emerges, we present a complementary derivation for the case when the Q-function is
represented as a deep linear neural network (Arora et al., 2019), which is a widely-studied setting
for analyzing implicit regularization of gradient descent in supervised learning (Gunasekar et al.,
2017; 2018; Arora et al., 2018; 2019). Our analysis will show that rank collapse can emerge as
the generated target values begin to approach the previous value estimate, in particular, when in the
vicinity of the optimal Q-function.
Proof strategy. Our proof consists of two steps: (1) We show that the effective rank of the feature
matrix decreases within one fitting iteration (for a given target value) due to the low-rank affinity, (2)
We show that this effective rank drop is “compounded” as we train using a bootstrapped objective.
Proposition 4.1 explains (1) and Proposition 4.2, Theorem 4.2 and Appendix D.2 discuss (2).
Additional notation and assumptions. We represent the Q-function as a deep linear network
with at ≥ 3 layers, such that Q(s, a) = WN Wφ [s; a], where N ≥ 3, WN ∈ R1×dN-1 and
Wφ = WN-iWn-2 …Wi with Wi ∈ Rdi×di-ι for i = 1,..., N - 1. Wφ maps an input
[s; a] to corresponding penultimate layer’ features Φ(s, a). Let Wj(k, t) denotes the weight ma-
trix Wj at the t-th step of gradient descent during the k-th fitting iteration (Algorithm 1). We define
Wk,t = WN(k, t)Wφ(k, t) and LN,k+1 (Wk,t) as the TD error objective in the k-th fitting itera-
tion. We study srankδ(Wφ(k, t)) since the rank of features Φ = Wφ(k, t)[S, A] is equal to rank of
Wφ(k, t) provided the state-action inputs have high rank.
We assume that the evolution of the weights is governed by a continuous-time differential equa-
tion (Arora et al., 2018) within each fitting iteration k. To simplify analysis, we also assume that
all except the last-layer weights follow a “balancedness” property (Equation D.4), which suggests
that the weight matrices in the consecutive layers in the deep linear network share the same singular
values (but with different permutations). However, note that we do not assume balancedness for
the last layer which trivially leads to rank-1 features, making our analysis strictly more general than
conventionally studied deep linear networks. In this model, we can characterize the evolution of the
singular values of the feature matrix Wφ(k, t), using techniques analogous to Arora et al. (2019):
Proposition 4.1. The singular values of the feature matrix Wφ (k, t) evolve according to:
σ r(k,t) = -N ∙ er(k,t))i-N-T ∙ BN (k,t)T dLMk+WWxt) , Ur(k,t)Vr(k,t)T : , (4)
for r = 1, •一，minN-i di, where Ur (k, t) and Vr (k, t) denote the left and right singular vectors of
the feature matrix, Wφ (k, t), respectively.
6
Published as a conference paper at ICLR 2021
Solving the differential equation (4) indicates that larger singular val-
ues will evolve at a exponentially faster rate than smaller singular values
(as we also formally show in Appendix D.1) and the difference in their
magnitudes disproportionately increase with increasing t. This behavior
also occurs empirically, illustrated in the figure on the right (also see Fig-
ure D.1), where larger singular values are orders of magnitude larger than
smaller singular values. Hence, the effective rank, srankδ (Wφ (k, t)),
will decrease with more gradient steps within a fitting iteration k.
Evolution of singular val-
ues of Wφ on SEAQUEST
Abstract optimization problem for the low-rank solution. Building on Proposition 4.1, we note
that the final solution obtained in a bootstrapping round (i.e., fitting iteration) can be equivalently
expressed as the solution that minimizes a weighted sum of the TD error and a data-dependent
implicit regularizer hD (Wφ, WN) that encourages disproportionate singular values of Wφ, and
hence, a low effective rank ofWφ. While the actual form for h is unknown, to facilitate our analysis
of bootstrapping, we make a simplification and express this solution as the minimum of Equation 5.
min AJWNWφ[s; a] — Yk(s, a)||2 + λksrankδ(Wφ)∙	(5)
Wφ,WN ∈M
Note that the entire optimization path may not correspond to the objective in Equation 5, but the
Equation 5 represents the final solution of a given fitting iteration. M denotes the set of constraints
that WN obtained via gradient optimization of TD error must satisfy, however we do not need to ex-
plicitly quantify M in our analysis. λk is a constant that denotes the strength of rank regularization.
Since srankδ is always regularized, our analysis assumes that λk > 0 (see Appendix D.1).
Rank drop within a fitting iteration “compounds” due to bootstrapping. In the RL setting, the
target values are given by yk (s, a) = r(s, a) + γP πQk-1 (s, a). First note that when r(s, a) = 0
and Pπ = I, i.e., when the bootstrapping update resembles self-regression, we first note that just
“copying over weights” from iteration k — 1 to iteration k is a feasible point for solving Equation 5,
which attains zero TD error with no increase in srankδ . A better solution to Equation 5 can thus be
obtained by incurring non-zero TD error at the benefit of a decreased srank, indicating that in this
setting, srankδ(Wφ) drops in each fitting iteration, leading to a compounding rank drop effect.
We next extend this analysis to the full bootstrapping setting. Unlike the self-training setting,
yk(s, a) is not directly expressible as a function of the previous Wφ(k, T) due to additional reward
and dynamics transformations. Assuming closure of the function class (Assumption D.1) under the
Bellman update (Munos & Szepesvari, 2008; Chen & Jiang, 2019), We reason about the compound-
ing effect of rank drop across iterations in Proposition 4.2 (proof in Appendix D.2). Specifically,
srankδ can increase in each fitting iteration due to R and Pπ transformations, but Will decrease due
to loW rank preference of gradient descent. This change in rank then compounds as shoWn beloW.
Proposition 4.2. Assume that the Q-function is initialized to Wφ(0) and WN (0). Let the
Q-function class be closed under the backup, i.e., ∃ WN, WP, s.t. (R + YPπQk-I)T =
WN(k)Wp(k)[S; A]t, and assume that the change in Srank due to dynamics and reward
transformations is bounded: srankδ (Wp(k)) ≤ srankδ(Wφ(k - 1)) + ck. Then,
srankδ(Wφ(k)) ≤ srankδ(Wφ(0)) + ^X Cj- ^X "Qj	yj||.
j=1	j=ι	j
Proposition 4.2 provides a bound on the value of srank after k rounds of bootstrapping. srank de-
creases in each iteration due to non-zero TD errors, but potentially increases due to reWard and
bootstrapping transformations. To instantiate a concrete case Where rank clearly collapses, We in-
vestigate Ck as the value function gets closer to the Bellman fixed point, Which is a favourable
initialization for the Q-function in Theorem 4.2. In this case, the learning dynamics begins to re-
semble the self-training regime, as the target values approach the previous value iterate yk ≈ Qk-1,
and thus, as We shoW next, the potential increase in srank (Ck in Proposition 4.2) converges to 0.
Theorem 4.2. Suppose target values yk = R+γPπ Qk-ι are close to the previous value esti-
mate Qk-ι, i.e. ∀s, a, y (s, a) = Qk-ι(s, a)+ ε(s, a), with ∣ε(s, a)|《∣Qk-ι(s, a)|. Then,
there is a constant e° depending upon WN and Wφ, such thatfor all kε∣∣ < ε0, Ck = 0. Thus,
srank decreases in iteration k: srankδ(Wφ(k)) ≤ srankδ(Wφ(k — 1)) — ||Qk — yk ∣∣∕λk.
7
Published as a conference paper at ICLR 2021
We provide a complete form, including the expres-
sion for 0 and a proof in Appendix D.3. To em-
pirically show the consequence of Theorem 4.2
that a decrease in srankδ (Wφ) values can lead to
an increase in the distance to the fixed point in
a neighborhood around the fixed point, we per-
formed a controlled experiment on a deep linear
net shown in Figure 5 that measures the relation-
ship between of srankδ(Φ) and the error to the pro-
jected TD fixed point |Q - Q*|. Note that a drop
in srankδ (Φ) corresponds to a increased value of
| Q — Q* | indicating that rank drop when Q get close to a fixed point can affect convergence to it.
Figure 5: Trend of srankδ (Φ) V.s. error on log scale
to the projected TD fixed point. A drop in srankδ (Φ)
(shown as blue and yellow circles) corresponds to a
corresponding increase in distance to the fixed point.
5 Mitigating UNDER-PARAMETRIZATION Improves Deep Q-LEARNING
We now show that mitigating implicit under-parameterization by preventing rank collapse can im-
prove performance. We place special emphasis on the offline RL setting in this section, since it is
particularly vulnerable to the adverse effects of rank collapse.
We devise a penalty (or a regularizer) Lp (Φ)
that encourages higher effective rank of
the learned features, srankδ (Φ), to prevent
rank collapse. The effective rank function
srankδ (Φ) is non-differentiable, so we choose
a simple surrogate that can be optimized over
deep networks. Since effective rank is max-
imized when the magnitude of the singular
values is roughly balanced, one way to in-
crease effective rank is to minimize the largest
singular value of Φ, σmax(Φ), while simul-
taneously maximizing the smallest singular
value, σmin(Φ). We construct a simple penalty
Lp (Φ) derived from this intuition, given by:
Figure 6: (a): srankδ (Φ) (top) and performance (bot-
tom) of FQI on gridworld in the offline setting with
200 gradient updates per fitting iteration. Note reduced
rank collapse and higher performance with the regular-
izer Lp (Φ). (b): Lp (Φ) mitigates the rank collapse in
DQN and CQL in the offline RL setting on Atari.
(a)
0	100	200	300	400	500
Gradient Updates
Lp(Φ) = σm2 ax(Φ) - σm2 in(Φ).	(6)
Lp(Φ) can be computed by invoking the
singular value decomposition subroutines
in standard automatic differentiation frame-
works (Abadi et al., 2016; Paszke et al., 2019).
We estimate the singular values over the feature matrix computed over a minibatch, and add the
resulting value of Lp as a penalty to the TD error objective, with a tradeoff factor α = 0.001.
Does Lp (Φ) address rank collapse? We first verify whether controlling the minimum and maxi-
mum singular values using Lp (Φ) actually prevents rank collapse. When using this penalty on the
gridworld problem (Figure 6a), the effective rank does not collapse, instead gradually decreasing
at the onset and then plateauing, akin to the evolution of effective rank in supervised learning. In
Figure 6b, we plot the evolution of effective rank on two Atari games in the offline setting (all games
in Appendix A.5), and observe that using Lp also generally leads to increasing rank values.
Does mitigating rank collapse improve performance? We now evaluate the performance of the
penalty using DQN (Mnih et al., 2015) and CQL (Kumar et al., 2020b) on Atari dataset from Agar-
wal et al. (2020) (5% replay data), used in Section 3. Figure 7 summarizes the relative improvement
from using the penalty for 16 Atari games. Adding the penalty to DQN improves performance on
all 16/16 games with a median improvement of 74.5%; adding it to CQL, a state-of-the-art offline
algorithm, improves performance on 11/16 games with median improvement of 14.1%. Prior work
has discussed that standard Q-learning methods designed for the online setting, such as DQN, are
generally ineffective with small offline datasets (Kumar et al., 2020b; Agarwal et al., 2020). Our
results show that mitigating rank collapse makes even such simple methods substantially more ef-
fective in this setting, suggesting that rank collapse and the resulting implicit under-parameterization
may be an crucial piece of the puzzle in explaining the challenges of offline RL.
8
Published as a conference paper at ICLR 2021
We also evaluated the reg-
ularizer Lp(Φ) in the data-
efficient online RL setting,
with results in Appendix A.6.
This variant achieved median
improvement of 20.6% per-
formance with Rainbow (Hes-
sel et al., 2018), however
performed poorly with DQN,
where it reduced median per-
formance by 11.5%. Thus,
O3∙bol0o涓O1O2
IIIl-1-1-1
(SleUS 60=1aUaEaAo-IdE- /
.enpl!3 一
p
while our proposed penalty is effective in many cases in offline and online settings, it does not
solve the problem fully, i.e., it does not address the root cause of implicit under-parameterization
and only addresses a symptom, and a more sophisticated solution may better prevent the issues
with implicit under-parameterization. Nevertheless, our results suggest that mitigation of implicit
under-parameterization can improve performance of data-efficient RL.
6	Related Work
Prior work has extensively studied the learning dynamics of Q-learning with tabular and linear func-
tion approximation, to study error propagation (Munos, 2003; Farahmand et al., 2010) and to prevent
divergence (De Farias, 2002; Maei et al., 2009; Sutton et al., 2009; Dai et al., 2018), as opposed to
deep Q-learning analyzed in this work. Q-learning has been shown to have favorable optimization
properties with certain classes of features (Ghosh & Bellemare, 2020), but our work shows that
the features learned by a neural net when minimizing TD error do not enjoy such guarantees, and
instead suffer from rank collapse. Recent theoretical analyses of deep Q-learning have shown con-
vergence under restrictive assumptions (Yang et al., 2020; Cai et al., 2019; Zhang et al., 2020; Xu &
Gu, 2019), but Theorem 4.2 shows that implicit under-parameterization appears when the estimates
of the value function approach the optimum, potentially preventing convergence. Xu et al. (2005;
2007) present variants of LSTD (Boyan, 1999), which model the Q-function as a kernel-machine
but do not take into account the regularization from gradient descent, as done in Equation 1, which
is essential for implicit under-parameterization. Igl et al. (2020); Fedus et al. (2020a) argue that
non-stationarity arising from distribution shift hinders generalization and recommend periodic net-
work re-initialization. Under-parameterization is not caused by this distribution shift, and we find
that network re-initialization does little to prevent rank collapse (Figure 4c). Luo et al. (2020) pro-
poses a regularization similar to ours, but in a different setting, finding that more expressive features
increases performance of on-policy RL methods. Finally, Yang et al. (2019) study the effective rank
of the Q *-values when expressed as a |S| X |A| matrix in online RL and find that low ranks for this
Q *-matrix are preferable. We analyze a fundamentally different object: the learned features (and
illustrate that a rank-collapse of features can hurt), not the Q*-matrix, whose rank is upper-bounded
by the number of actions (e.g., 24 for Atari).
7	Discussion
We identified an implicit under-parameterization phenomenon in deep RL algorithms that use boot-
strapping, where gradient-based optimization of a bootstrapped objective can lead to a reduction in
the expressive power of the value network. This effect manifests as a collapse of the rank of the
features learned by the value network, causing aliasing across states and often leading to poor per-
formance. Our analysis reveals that this phenomenon is caused by the implicit regularization due to
gradient descent on bootstrapped objectives. We observed that mitigating this problem by means of
a simple regularization scheme improves performance of deep Q-learning methods.
While our proposed regularization provides some improvement, devising better mitigation strate-
gies for implicit under-parameterization remains an exciting direction for future work. Our method
explicitly attempts to prevent rank collapse, but relies on the emergence of useful features solely
through the bootstrapped signal. An alternative path may be to develop new auxiliary losses (e.g.,
Jaderberg et al., 2016) that learn useful features while passively preventing underparameterization.
More broadly, understanding the effects of neural nets and associated factors such as initialization,
choice of optimizer, etc. on the learning dynamics of deep RL algorithms, using tools from deep
learning theory, is likely to be key towards developing robust and data-efficient deep RL algorithms.
9
Published as a conference paper at ICLR 2021
Acknowledgements
We thank Lihong Li, Aaron Courville, Aurick Zhou, Abhishek Gupta, George Tucker, Ofir Nachum,
Wesley Chung, Emmanuel Bengio, Zafarali Ahmed, and Jacob Buckman for feedback on an earlier
version of this paper. We thank Hossein Mobahi for insightful discussions about self-distillation
and Hanie Sedghi for insightful discussions about implicit regularization and generalization in deep
networks. We additionally thank Michael Janner, Aaron Courville, Dale Schuurmans and Marc
Bellemare for helpful discussions. AK was partly funded by the DARPA Assured Autonomy pro-
gram, and DG was supported by a NSF graduate fellowship and compute support from Amazon.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan,
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 16), pp. 265-283, 2016. URL https://www.usenix.org/system/
files/conference/osdi16/osdi16-abadi.pdf.
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep q-
learning. ArXiv, abs/1903.08894, 2019.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning (ICML), 2020.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7413-7424, 2019.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. J. Artif. Int. Res., 47(1):253-279, May 2013.
ISSN 1076-9757.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 449-458. JMLR. org, 2017.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal
difference learning. arXiv preprint arXiv:2003.06350, 2020.
Justin A Boyan. Least-squares temporal difference learning. In ICML, pp. 49-56. Citeseer, 1999.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference and q-learning
provably converge to global optima. arXiv preprint arXiv:1905.10027, 2019.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
ICML, 2019.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1133-1142, 2018.
Daniela Pucci De Farias. The linear programming approach to approximate dynamic programming:
Theory and application. PhD thesis, 2002.
Dean G Duffy. Green’s functions with applications. CRC Press, 2015.
10
Published as a conference paper at ICLR 2021
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(Apr):503-556, 2005.
Amir-massoud Farahmand, Csaba Szepesvari, and Remi Munos. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems (NIPS), 2010.
William Fedus, Dibya Ghosh, John D Martin, Marc G Bellemare, Yoshua Bengio, and Hugo
Larochelle. On catastrophic interference in atari 2600 games. arXiv preprint arXiv:2002.12499,
2020a.
William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark
Rowland, and Will Dabney. Revisiting fundamentals of experience replay. arXiv preprint
arXiv:2007.06700, 2020b.
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-
learning algorithms. In Proceedings of the 36th International Conference on Machine Learning.
PMLR, 2019.
Dibya Ghosh and Marc G Bellemare. Representations for stable off-policy reinforcement learning.
arXiv preprint arXiv:2007.05520, 2020.
Caglar Gulcehre, ZiyU Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Kon-
rad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl un-
plugged: Benchmarks for offline reinforcement learning. 2020.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,
2018. URL http://arxiv.org/abs/1801.01290.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Edmund Hlawka, Rudolf Taschner, and Johannes SchoiBengeier. The Dirichlet Approximation
Theorem, pp. 1-18. Springer Berlin Heidelberg, Berlin, Heidelberg, 1991. ISBN 978-3-
642-75306-0. doi: 10.1007/978-3-642-75306-0」. URL https://doi.org/1O.10O7/
978-3-642-75306-0_1.
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson.
The impact of non-stationarity on generalisation in deep reinforcement learning. arXiv preprint
arXiv:2006.05826, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems 31. 2018.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Robert Johnson. Approximate irrational numbers by rational numbers, 2016. URL https://
math.stackexchange.com/questions/1829743/.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020a.
11
Published as a conference paper at ICLR 2021
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020b.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Vincent Liu, Raksha Kumaraswamy, Lei Le, and Martha White. The utility of sparse representations
for control in reinforcement learning. CoRR, abs/1811.06626, 2018. URL http://arxiv.
org/abs/1811.06626.
Xufang Luo, Qi Meng, Di He, Wei Chen, and Yunhong Wang. I4r: Promoting deep reinforcement
learning by the indicator for expressive representations. In Christian Bessiere (ed.), Proceed-
ings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp.
2669-2675. International Joint Conferences on Artificial Intelligence Organization, 7 2020. doi:
10.24963/ijcai.2020/370. URL https://doi.org/10.24963/ijcai.2020/370. Main
track.
Hamid R. Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard S.
Sutton. Convergent temporal-difference learning with arbitrary smooth function approximation.
In Proceedings of the 22nd International Conference on Neural Information Processing Systems,
2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, feb 2015. ISSN 0028-0836.
Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization
in hilbert space. arXiv preprint arXiv:2002.05715, 2020.
Remi Munos. Error bounds for approximate policy iteration. In Proceedings ofthe Twentieth Inter-
national Conference on International Conference on Machine Learning, ICML’03, pp. 560-567.
AAAI Press, 2003. ISBN 1577351894.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8026-8037. Curran
Associates, Inc., 2019.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317-328. Springer,
2005.
Axel Ruhe. On the closeness of eigenvalues and singular values for almost normal matrices. Linear
Algebra and its Applications, 11(1):87-93, 1975.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
12
Published as a conference paper at ICLR 2021
Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regulariza-
tion paths. In Advances in Neural Information Processing Systems,pp. 10608-10619, 2018.
Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In International Conference on Machine Learning (ICML),
2009.
James Townsend. Differentiating the singular value decomposition. Technical report, Technical
Report 2016, https://j-towns. github. io/papers/svd-derivative . . . , 2016.
Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function ap-
proximation. arXiv preprint arXiv:1912.04511, 2019.
Xin Xu, Tao Xie, Dewen Hu, and Xicheng Lu. Kernel least-squares temporal difference learning.
International Journal of Information Technology, 11(9):54-63, 2005.
Xin Xu, Dewen Hu, and Xicheng Lu. Kernel-based least squares policy iteration for reinforcement
learning. IEEE Transactions on Neural Networks, 18(4):973-992, 2007.
Yuzhe Yang, Guo Zhang, Zhi Xu, and Dina Katabi. Harnessing structures for value-based planning
and reinforcement learning. arXiv preprint arXiv:1909.12255, 2019.
Zhuoran Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep q-learning. In
Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Yufeng Zhang, Qi Cai, Zhuoran Yang, Yongxin Chen, and Zhaoran Wang. Can temporal-difference
and q-learning learn representation? a mean-field theory. arXiv preprint arXiv:2006.04761, 2020.
13
Published as a conference paper at ICLR 2021
Appendices
A Additional Evidence for Implicit Under-Parameterization
In this section, we present additional evidence that demonstrates the existence of the implicit under-
parameterization phenomenon from Section 3. In all cases, we plot the values of srankδ (Φ) com-
puted on a batch size of 2048 i.i.d. sampled transitions from the dataset.
A.1 Offline RL
Figure A.1: Offline DQN on Atari. srankδ (Φ) and performance of DQN on five Atari games in the offline
RL setting using the 5% DQN Replay dataset (Agarwal et al., 2020) (marked as DQN). Note that low srank
(top row) generally corresponds to worse policy performance (bottom row). Also note that rank collapse begins
to generally occur close to the position of peak return. Average across 5 runs is showed for each game with
individual runs.
Gradient Updates (xβ2.5k)
Figure A.2: Offline DQN on Atari. srankδ (Φ) and performance of DQN on five Atari games in the offline RL
setting using the 20% DQN Replay dataset (Agarwal et al., 2020) (marked as DQN) trained for 1000 iterations.
Note that low srank (top row) generally corresponds to worse policy performance (bottom row). Average across
5 runs is showed for each game with individual runs.
Asterlx
UJn$H OBeJOAV
s 400
& 300
。200
» 100
0
0	200 400 6∞ 800 1000
0	200 400 600 800 1000
Gradient Updates (x62.5k)
—CQL
0	200 400 600 800 1000
Gradient Updates (xβ2.5k)
0	200 400 600 800 1000
Gradient Updates (x62.5k)
200 400 600 800 1000
Gradient Updates (xβ2.5k)
200 400 6∞
Gradient Updates (x62.5k)
IOOT
Figure A.3: Offline CQL on Atari. srankδ (Φ) and performance of CQL on five Atari games in the offline
RL setting using the 5% DQN Replay. Note that low srank (top row) generally corresponds to worse policy
performance (bottom row). Average across 5 runs for each game trained for 1000 iterations.
14
Published as a conference paper at ICLR 2021
O 200	400
Gradient updates (x 8k)
O 200	400
Gradient Updates (x8k)
O 200	400
Gradient Updates (x8k)
Figure A.4: Offline Control on MuJoCo. srankδ (Φ) and performance of SAC on three Gym environments
the offline RL setting. Implicit Under-parameterization is conspicuous from the rank reduction, which highly
correlates with performance degradation. We use 20% uniformly sampled data from the entire replay experience
of an online SAC agent, similar to the 20% setting from Agarwal et al. (2020).
1
IMn-eV eβea*v
soπ⅛-⅞⅞s
Figure A.5: Offline Control on MuJoCo. srankδ (Φ) and performance of CQL on three Gym environments
the offline RL setting. Implicit Under-parameterization is conspicuous from the rank reduction, which highly
correlates with performance degradation. We use 20% uniformly sampled data from the entire replay experience
of an online SAC agent, similar to the 20% setting from Agarwal et al. (2020).
A.2 Data Efficient Online RL
In the data-efficient online RL setting, we verify the presence of implicit under-parameterization on
both DQN and Rainbow (Hessel et al., 2018) algorithms when larger number of gradient updates are
made per environment step. In these settings we find that more gradient updates per environment
step lead to a larger decrease in effective rank, whereas effective rank can increase when the amount
of data re-use is reduced by taking fewer gradient steps.
15000
IOOOO
O 50 IOO 150	200 O 50	100	150	200 O 50 IOO 150	200 O 50	100	150	200 O 50 IOO 150	200
Environment Steps	Environment 5tepβ	Environment Steps	Environment 5tepβ	Environment Steps
Figure A.6: Online DQN on Atari. srankδ (Φ) and performance of DQN on 5 Atari games in the online RL set-
ting, with varying numbers of gradient steps per environment step (n). Rank collapse happens earlier with more
gradient steps, and the corresponding performance is poor. This indicates that implicit under-parameterization
aggravates as the rate of data re-use is increased.
15
Published as a conference paper at ICLR 2021
180170160150
20	40	60	80 100
Environment Steps (XlOk)
160
140
0	20	40	60	80 100
Environment Steps (xlθk)
20	40	60	80 100
Environment Steps (XlOk)
Ant-v2
5000
4000
3000
2000
1000
0	20	40	60	80 100
Environment Steps (XlOk)
Figure A.7: Online SAC on MuJoCo. srankδ (Φ) and performance of SAC on three Gym environments the
online RL setting, with varying numbers of gradient steps per environment step (n). While in the simpler
environments, HalfCheetah-v2, Hopper-v2 and Walker2d-v2 we actually observe an increase in the values of
effective rank, which also corresponds to good performance with large n values in these cases, on the more
complex Ant-v2 environment rank decreases with larger n, and the corresponding performance is worse with
more gradient updates.
Double Dunk	Endura
Environment Steps
Environment Steps
Figure A.8: Online Rainbow on Atari. srankδ (Φ) Rainbow on 16 Atari games in the data-efficient online
setting, with varying numbers of gradient steps per environment step (n). Rank collapse happens earlier with
more gradient steps, and the corresponding performance is poor. This plot indicates the multi-step returns,
prioritized replay or distributional C51 does not address the implicit under-parameterization issue.
A.3 Does Bootstrapping Cause Implicit Under-Parameterization ?
In this section, we provide additional evidence to support our claim from Section 3 that suggests
that bootstrapping-based updates are a key component behind the existence of implicit under-
parameterization. To do so, we empirically demonstrate the following points empirically:
16
Published as a conference paper at ICLR 2021
• Implicit under-parameterization occurs even when the form of the bootstrapping update
is changed from Q-learning that utilizes a maxa0 backup operator to a policy evaluation
(fitted Q-evaluation) backup operator, that computes an expectation of the target Q-values
under the distributions specified by a different policy. Thus, with different bootstrapped
updates, the phenomenon still appears.
Gradient Updates
Gradient Updates
Gradient Updates
Figure A.9: Offline Policy Evaluation on Atari. srankδ (Φ) and performance of offline policy evalua-
tion (FQE) on 5 Atari games in the offline RL setting using the 5% and 20% DQN Replay dataset (Agarwal
et al., 2020). The rank degradation shows that under-parameterization is not specific to the Bellman optimal-
ity operator but happens even when other bootstrapping-based backup operators are combined with gradient
descent. Furthermore, the rank degradation also happens when we increase the dataset size.
•	Implicit under-parameterization does not occur when Monte-Carlo regression targets - that
compute regression targets for the Q-function by computing a non-parameteric estimate the
future trajectory return, i.e., , yk(st, at) = Pt∞0=t γtr(st0, at0) and do not use bootstrap-
ping. In this setting, we find that the values of effective rank actually increase over time
and stabilize, unlike the corresponding case for bootstrapped updates. Thus, other fac-
tors kept identically the same, implicit under-parameterization happens only when
bootstrapped updates are used.
Figure A.10: Monte Carlo Offline Policy Evaluation. srankδ (Φ) on 5 Atari games in when using Monte
Carlo returns for targets and thus removing bootstrapping updates. Rank collapse does not happen in this
setting implying that is bootstrapping was essential for under-parameterization. We perform the experiments
using 5% and 20% DQN replay dataset from Agarwal et al. (2020).
•	For the final point in this section, we verify if the non-stationarity of the policy in the Q-
learning (control) setting, i.e., when the Bellman optimality operator is used is not a reason
behind the emergence of implicit under-parameterization. The non-stationary policy in a
control setting causes the targets to change and, as a consequence, leads to non-zero er-
rors. However, rank drop is primarily caused by bootstrapping rather than non-stationarity
of the control objective. To illustrate this, we ran an experiment in the control setting
on Gridworld, regressing to the target computed using the true value function Qπ for the
current policy π (computed using tabular Q-evaluation) instead of using the bootstrap TD
estimate. The results, shown in figure A.11a, show that the srankδ doesn’t decrease signif-
icantly when regressing to true control values and infact increases with more iterations as
compared to Figure 6a where rank drops with bootstrapping. This experiment, alongside
with experiments discussed above, ablating bootstrapping in the stationary policy evalua-
tion setting shows that rank-deficiency is caused due to bootstrapping.
A.4 How Does Implicit Regularization Inhibit Data-Efficient RL?
Implicit under-parameterization leads to a trade-off between minimizing the TD error vs. encourag-
ing low rank features as shown in Figure 4b. This trade-off often results in decrease in effective rank,
at the expense of increase in TD error, resulting in lower performance. Here we present additional
evidence to support this.
Figure A.11b shows a gridworld problem with one-hot features, which naturally leads to reduced
state-aliasing. In this setting, we find that the amount of rank drop with respect to the supervised
17
Published as a conference paper at ICLR 2021
Gridworld
Gradient Updates
(a) Using oracle access to exact Qn for
computing the target values does not sig-
nificantly decrease in rank, and training
for longer can increase feature rank. This
experiment uses a policy iteration style
setup but the essential trend of rank drop
as we train more does not occur.
°
15.0
12.5
I ιo∙o
o)
Ξ 7.5
E 5.0
O
2.5
00O
CGrldWOrld (One-Hot Features)
40^∣ ........................
3 2 1
I0du9-(e)⅛IUaJB
				
				
		Σ3∑:		
				
				
0	100 200 300 400
---Supervised (Oracle)
----n=10
---n=200
100 200 300 400 500
Environment Steps
(b) Q* fitting error and Srank in a one-hot
variant of the gridworld environment.
Figure A.11: Gridword: (a) Effective rank change due to non-stationarity (b) Q* fitting error vs. srankδ .
projection of oracle computed Q* values is quite small and the regression error to Q* actually
decreases unlike the case in Figure 4a, where it remains same or even increases. The method is able
to learn policies that attain good performance as well. Hence, this justifies that when there’s very
little rank drop, for example, 5 rank units in the example on the right, FQI methods are generally
able to learn Φ that is able to fit Q* . This provides evidence showing that typical Q-networks learn
Φ that can fit the optimal Q-function when rank collapse does not occur.
In Atari, we do not have access to Q*, and so we instead measure the error in fitting the target value
estimates, R + γPπQk. As rank decreases, the TD error increases (Figure A.12) and the value
function is unable to fit the target values, culminating in a performance plateau (Figure A.6).
Figure A.12: TD error vs. Effective rank on Atari. We observe that Huber-loss TD error is often higher when
there is a larger implicit under-parameterization, measured in terms of drop in effective rank. The results are
shown for the data-efficient online RL setting.
A.5 Trends in Values of Effective Rank With Penalty.
In this section, we present the trend in the values of the effective rank when the penalty Lp (Φ) is
added. In each plot below, we present the value of srankδ (Φ) with and without penalty respectively.
18
Published as a conference paper at ICLR 2021
A.5.1 Offline RL: DQN
sonp 宜√USJW
O 20	40	60	80 IOO
Gradient Updates
O 20	40	€0	80 IOO
Gradient Updates
O 20	40	€0	80 IOO
Gradient Updates
O 20	40	60	80 IOO O 20	40	60	80 IOO
Gradient Updates	Gradient Updates
Figure A.13: Effective rank values with the penalty on DQN. Trends in effective rank and performance for
offline DQN. Note that the performance of DQN with penalty is generally better than DQN and that the penalty
(blue) is effective in increasing the values of effective rank. We report performance at the end of 100 epochs,
as per the protocol set by Agarwal et al. (2020) in Figure 7.
Io. OnPfnUSJW
E3%s &EO>4
20-
10-
O-
-10-
-20-
3QQ
Asterlx

Figure A.14: Effective rank values with the penalty on DQN on a 4x larger dataset. Trends in effective
rank and performance for offline DQN with a 4x larger dataset, where distribution shift effects are generally
removed. Note that the performance of DQN with penalty is generally better than DQN and that the penalty
(blue) is effective in increasing the values of effective rank in most cases. Infact in Pong, where the penalty is
not effective in increasing rank, we observe suboptimal performance (blue vs. red).
3 2 1 O Oo 1 2
Ooo O O O O
111 1 1 1 1
- - -
(9-8s 601) 1UΘEΘ>O.Idlu- %
3 2 1 O Do 1 2
Ooo O O O Λ
Illl-I-lr
(əraus 601rUΘEΘ>QJdE-
J-ums<COES
，」OMJopeNM
，* Una3≡noα
,tt3nbeas
，23peΛUα5ueds
'A∞POH3U-
-OUoqS3E»
,.13p≡Ee3B
• UeEDedSW
'x∙E3asV
TeUUnHPeOH
,36u3A3HSJeA
'io
'UoxXeZ
,QJnpuUJ
Uod
J-vms<COEΦ□
⅛≡ttEmφm
'UoxXeZ
，&J3pe>uα>ueds
JSSnbeSS
,x∙Easv
,AaJPOHa
'UeUJuedSW
-36U3>3HsJeA
，6UOd
-」SUUnHPeOH
'⅛≥copmN_5
-OJnPUIu
10
,XUnMqnOa
-PUOqSSUJe-
U
-ɪ
%
Game
Game
(a)	Offline DQN with Lp (Φ).
(b)	Offline CQL with Lp (Φ).
Figure A.15: Performance improvement of (a) offline DQN and (b) offline CQL with the Lp(Φ) penalty on
20% Atari dataset, i.e., the dataset referred to as 4x large in Figure 2.
19
Published as a conference paper at ICLR 2021
A.5.2 OFFLINE RL: CQL WITH Lp(Φ) PENALTY
Breakout
Gradient updates
Peng
Figure A.16: Effective rank values with the penalty Lp(Φ) on CQL. Trends in effective rank and perfor-
mance for offline DQN. Note that the performance of CQL with penalty is generally better than vanilla CQL
and that the penalty (blue) is effective in increasing the values of effective rank. We report performance at the
end of 100 epochs, as per the protocol set by Agarwal et al. (2020) in Figure 7.
A.6 Data-Efficient Online RL: Rainbow
A.6.1 RAINBOW WITH Lp(Φ) PENALTY: RANK PLOTS
Demon Attack
400
350
300-
250
---Rainbow (n=4)
----Rainbow (n=4) w/ Penalty
20	40	60	B0-
Environment Steps
500
450
Double Dunk
250
200
150
IQOUP.(S∙J2∙UE8
Yβιs's Revenge
200
150
0
100
Rainbow (n=4)
Rainbow (n=4) w/ Penalty
^20	40	60	B0-
Environment Steps
100-
100
----Ralnbew (n=4)
--- Rainbow (n=4) w/ Penalty
S 40	60	80-
Environment Steps
Wizard Of Wor
----Ralnbew (n=4)
---- Rainbow (n=4) w/ Penalty
Environment Steps
100
Environment Steps
50π⅛-εσ-CB⅛
100
Environment Steps

Figure A.17: Effective rank values with the penalty on Rainbow in the data-efficient online RL setting.
Trends in effective rank and performance for online Rainbow, where distribution shift effects are generally
removed. Note that the performance of DQN with penalty is generally better than DQN and that the penalty
(blue) is effective in increasing the values of effective rank in most cases. Infact in Pong, where the penalty is
not effective in increasing rank, we observe suboptimal performance (blue vs. red).
20
Published as a conference paper at ICLR 2021
A.6.2 RAINBOW WITH Lp(Φ) PENALTY: PERFORMANCE
In this section, we present additional re-
sults for supporting the hypothesis that pre-
venting rank-collapse leads to better per-
formance. In the first set of experiments,
we apply the proposed Lp penalty to Rain-
bow in the data-efficient online RL setting
(n = 4). In the second set of experiments,
we present evidence for prevention of rank
collapse by comparing rank values for dif-
ferent runs.
As we will show in the next section,
the state-of-the-art Rainbow (Hessel et al.,
2018) algorithm also suffers form rank col-
lapse in the data-efficient online RL setting
Game
Figure A.18: Performance of Rainbow (n = 4) and Rain-
bow (n = 4) with the Lp(Φ) penalty (Equation 6. Note that
the penalty improves on the base Rainbow in 12/16 games.
when more updates are performed per gradient step. In this section, we applied our penalty Lp to
Rainbow with n = 4, and obtained a median 20.66% improvement on top of the base method. This
result is summarized below.
Beam Rider
Demon Attack
Double Dunk
0	25	50	75	100
Environment Steps
Ms. Pac-Man
2500
2000
1500
1000
500
25	50	75	100
Environment Steps
Pong
0	25	50	75	100
Environment Steps
Q*Bert
0	25	50	75	100
Environment Steps
James Bond
0	25	50	75	100
Environment Steps
Ooooo
2 1 1 2
- -
Enl<υα ωmrab><
0	25	50	75	100
Environment Steps
Road Runner
3000
2500
2000
1500
1000
500
Seaquest
0	25	50	75	100
Environment Steps	Environment Steps
0	25	50	75	100
Environment Steps
Figure A.19: Learning curves with n = 4 gradient updates per environment step for Rainbow(Blue) and
Rainbow with the Lp(Φ) penalty (Equation 6) (Red) on 16 games , corresponding to the bar plot above. One
unit on the x-axis is equivalent to 1M environment steps.
21
Published as a conference paper at ICLR 2021
A.7 Relaxing the Normality Assumption in Theorem 4.1
We can relax the normality assumption on S in Theorem 4.1. An analogous statement holds for
non-normal matrices S for a slightly different notion of effective rank, denoted as srankδ,λ(Mk),
that utilizes eigenvalue norms instead of singular values. Formally, let λι(Mk),…，λ2(Mk),…
be the (complex) eigenvalues of Mk arranged in decreasing order of their norms, i.e.,, ∣λι(Mk )| ≥
∣λ2(Mk)| ≥ …，then,
Srankδ λ(Mk) = min /k : Pi=I lλi(Mk)I ≥ 1 - Λ .
'	)	k Pd=ι ∣λi(Mk)∣ ≥	∫
A statement essentially analogous to Theorem 4.1 suggests that in this general case, srankδ,λ(Mk)
decreases for all (complex) diagonalizable matrices S, which is the set of almost all matrices of size
dim(S). Now, if S is approximately normal, i.e., when ∣σi(S) - ∣λi(S)∣∣ is small, then the result in
Theorem 4.1 also holds approximately as we discuss at the end of Appendix C.
We now provide empirical evidence showing that the trend in the values of effective rank computed
using singular values, srankδ(Φ) is almost identical to the trend in the effective rank computed using
normalized eigenvalues, srankδ,λ (Φ). Since eigenvalues are only defined for a square matrix Φ, in
practice, we use a batch of d = dim(φ(s, a)) state-action pairs for computing the eigenvalue rank
and compare to the corresponding singular value rank in Figures A.20 and A.21.
Asterlx
sranl⅛,λ(Φ)
sranl⅛(Φ)
100 200 300 400 500	100 200 300 400 500	0	100 200 300 400 500	100 200 300 400 500	100 200 300 400 500
Gradient UPdateS (xβ2.5k}	Gradient Updates (xβ2.5k}	Gradient UPdateS (xβ2.5k}	Gradient Updates (x62.5k}	Gradient UPdateS (xβ2.5k)
Figure A.20: Comparing different measures of effective rank on a run of offline DQN in the 5% replay
setting, previously studied in Figure A.3.
Connection to Theorem 4.1. We computed the effective rank of Φ instead of S, since S is a theo-
retical abstraction that cannot be computed in practice as it depends on the Green’s kernel (Duffy,
2015) obtained by assuming that the neural network behaves as a kernel regressor. Instead, we com-
pare the different notions of ranks of Φ since Φ is the practical counterpart for the matrix, S, when
using neural networks (as also indicated by the analysis in Section 4.2). In fact, on the gridworld
(Figure A.21), we experiment with a feature Φ with dimension equal to the number of state-action
pairs, i.e., dim(φ(s, a)) = |S||A|, with the same number of parameters as a kernel parameterization
of the Q-function: Q(s, a) = Ps0,a0 w(s0, a0)k(s, a, s0, a0). This can also be considered as perform-
ing gradient descent on a “wide” linear network , and we measure the feature rank while observing
similar rank trends.
Since we do not require the assumption that S is normal in Theorem 4.1 to obtain a decreasing
trend in srankδ,λ(Φ), and we find that in practical scenarios (Figures A.20 and A.21), srankδ(Φ) ≈
srankδ,λ (Φ) with an extremely similar qualitative trend we believe that Theorem 4.1 still explains
the rank-collapse practically observed in deep Q-learning and is not vacuous.
WO=P-⅛IUIMW
5 0 5 0 5。
171512107 5
WO=P-(⅛γj≡rUIM8
GridWoricI
0	100 200 300 400 500
Fitting Iterations
Figure A.21: Comparison of srankδ,λ (Φ) and srankδ(Φ) on the gridworld in the offline setting (left) and the
online setting (right), when using a deep linear network with dim(φ(s, a)) = |S||A|. Note that both notions
of effective rank exhibit a similar decreasing trend and are closely related to each other.
22
Published as a conference paper at ICLR 2021
A.8 Normalized Plots for Figure 3/ Figure A.6
In this section, we provide a set of normalized srank and performance trends for Atari games (the
corresponding unnormalized plots are found in Figure A.6). In these plots, each unit on the x-axis
is equivalent to one gradient update, and so since n = 8 prescribes 8× many updates as compared
to n = 1, it it runs for 8× as long as n = 1. These plots are in Figure A.22.
Seaquest	Q*Bert	Pong	Seaquest
Asterix
Number of gradient updates
Number of gradient updates	Number of gradient updates
Number of gradient updates	Number of gradient updates
Figure A.22: Rank collapse in DQN as a function of gradient updates on the x-axis for five Atari games in
the data-efficient online RL setting. This setting was previously studied in Figure A.6. Note that lesser number
of updates per unit amount data, i.e., smaller values of n possess larger srankδ values.
Note that the trend that effective rank decreases with larger n values also persists when rescaling
the x-axis to account for the number of gradient steps, in all but one game. This is expected since
it tells us that performing bootstrapping based updates in the data-efficient setting (larger n values)
still leads to more aggressive rank drop as updates are being performed on a relatively more static
dataset for larger values of n.
B Hyperparameters & Experiment Details
B.1 Atari Experiments
We follow the experiment protocol from Agarwal et al. (2020) for all our experiments including
hyperparameters and agent architectures provided in Dopamine and report them for completeness
and ease of reproducibility in Table B.1. We only use hyperparameter selection over the regular-
ization experiment αp based on results from 5 Atari games (Asterix, Seaquest, Pong, Breakout and
Seaquest). We will also open source our code to further aid in reproducing our results.
Evaluation Protocol. Following Agarwal et al. (2020), the Atari environments used in our ex-
periments are stochastic due to sticky actions, i.e., there is 25% chance at every time step that
the environment will execute the agent’s previous action again, instead of the agent’s new action.
All agents (online or offline) are compared using the best evaluation score (averaged over 5 runs)
achieved during training where the evaluation is done online every training iteration using a -greedy
policy with = 0.001. We report offline training results with same hyperparameters over 5 random
seeds of the DQN replay data collection, game simulator and network initialization.
Offline Dataset. As suggested by Agarwal et al. (2020), we randomly subsample the DQN Replay
dataset containing 50 millions transitions to create smaller offline datasets with the same data distri-
bution as the original dataset. We use the 5% DQN replay dataset for most of our experiments. We
also report results using the 20% dataset setting (4x larger) to show that our claims are also valid
even when we have higher coverage over the state space.
Optimizer related hyperparameters. For existing off-policy agents, step size and optimizer were
taken as published. We used the DQN (Adam) algorithm for all our experiments, given its superior
performance over the DQN (Nature) which uses RMSProp, as reported by Agarwal et al. (2020).
Atari 2600 games used. For all our experiments in Section 3, we used the same set of 5 games
as utilized by Agarwal et al. (2020); Bellemare et al. (2017) to present analytical results. For our
23
Published as a conference paper at ICLR 2021
Table B.1: Hyperparameters used by the offline and online RL agents in our experiments.
Hyperparameter	Setting (for both variations)	
Sticky actions Sticky action probability Grey-scaling Observation down-sampling Frames stacked Frame skip (Action repetitions) Reward clipping Terminal condition Max frames per episode Discount factor Mini-batch size Target network update period Training environment steps per iteration Update period every Evaluation Evaluation steps per iteration Q-network: channels Q-network: filter size Q-network: stride Q-network: hidden units Hardware		Yes 0.25 True (84, 84) 4 4 [-1, 1] Game Over 108K 0.99 32 every 2000 updates 250K 4 environment steps 0.001 125K 32, 64, 64 8 × 8, 4 × 4, 3 × 3 4, 2, 1 512 Tesla P100 GPU
Hyperparameter	Online	Offline
Min replay size for sampling	20,000	-
Training (for -greedy exploration)	0.01	-
-decay schedule	250K steps	-
Fixed Replay Memory	No	Yes
Replay Memory size (Online)	1,000,000 steps	—
Fixed Replay size (5%)	—	2,500,000 steps
Fixed Replay size (20%)	—	10,000,000 steps
Replay Scheme	Uniform	Uniform
Training Iterations	200	500
empirical evaluation in Appendix A.5, we use the set of games employed by Fedus et al. (2020b)
which are deemed suitable for offline RL by Gulcehre et al. (2020). Similar in spirit to Gulcehre
et al. (2020), we use the set of 5 games used for analysis for hyperparameter tuning for offline RL
methods.
5 games subset: ASTERIX, QBERT, PONG, SEAQUEST, B REAKOUT
16 game subset: In addition to 5 games above, the following 11 games: DOUBLE DUNK, JAMES
Bond, Ms. Pacman, Space Invaders, Zaxxon, Wizard of Wor, Yars ’ Revenge, En-
duro, Road Runner, BeamRider, Demon Attack
B.2 Gridworld Experiments
We use the gridworld suite from Fu et al. (2019) to obtain gridworlds for our experiments. All of
our gridworld results are computed using the 16 × 16 GRID 1 6SMOOTHOBS environment, which
consists of a 256-cell grid, with walls arising randomly with a probability of 0.2. Each state allows 5
different actions (subject to hitting the boundary of the grid): move left, move right, move up, move
down and no op. The goal in this environment is to minimize the cumulative discounted distance to
a fixed goal location where the discount factor is given by γ = 0.95. The features for this Q-function
are given by randomly chosen vectors which are smoothened spatially in a local neighborhood of a
grid cell (x, y).
We use a deep Q-network with two hidden layers of size (64, 64), and train it using soft Q-learning
with entropy coefficient of 0.1, following the code provided by authors of Fu et al. (2019). We use
a first-in-first out replay buffer of size 10000 to store past transitions.
24
Published as a conference paper at ICLR 2021
C Proofs for Section 4.1
In this section, we provide the technical proofs from Section 4.1. We first derive a solution to
optimization problem Equation 1 and show that it indeed comes out to have the form described in
Equation 2. We first introduce some notation, including definition of the kernel G which was used
for this proof. This proof closely follows the proof from Mobahi et al. (2020).
Definitions. For any universal kernel u, the Green’s function (Duffy, 2015) of the linear kernel
operator L given by: [LQ] (s, a) := P(s0,a0) u((s, a), (s0, a0))Q(s0, a0) is given by the function
g((s, a), (s0, a0)) that satisfies:
X U((S, a), (s0, a0)) g((s0, a0), (S a)) = δ((s, a) - (S, a)),	(C.1)
(s,a)
where δ is the Dirac-delta function. Thus, Green’s function can be understood as a kernel that
“inverts” the universal kernel u to the identity (Dirac-delta) matrix. We can then define the matrix G
as the matrix of vectors g(s,a) evaluated on the training dataset, D, however note that the functional
g(s,a) can be evaluated for other state-action tuples, not present in D.
G((Si, ai),	(Sj,aj)) := g((Si, ai),	(Sj,aj))	and	g(s,a)[i]	= g((S,	a), (Si,ai))	∀(Si,ai)	∈ D.
(C.2)
Lemma C.0.1. The solution to Equation 1 is given by Equation 2.
Proof. This proof closely follows the proof of Proposition 1 from (Mobahi et al., 2020). We revisit
key aspects the key parts of this proof here.
We restate the optimization problem below, and solve for the optimum Qk to this equation by ap-
plying the functional derivative principle.
min J(Q) :
Q∈Q
(Q(Si, ai) - yk(Si, ai))2 + c Σ Σ u((S, a), (S0, a0))Q(S, a)Q(S0
si,ai∈D	(s,a) (s0,a0)
a0).
The functional derivative principle would say that the optimal Qk to this problem would satisfy, for
any other function f and for a small enough ε → 0,
Vf ∈ Q : dJ(Qk + εf) I = 0.	(C.3)
∂ε ε=0
By setting the gradient of the above expression to 0, we obtain the following stationarity conditions
on Qk (also denoting (Si, ai) := xi) for brevity:
X δ(x - xi) (Qk(xi) - yk(xi)) + cXu(x,x0)Qk(x0) = 0.	(C.4)
xi ∈D	x
Now, we invoke the definition of the Green’s function discussed above and utilize the fact that
the Dirac-delta function can be expressed in terms of the Green’s function, we obtain a simplified
version of the above relation:
u(x, x0)	(Qk(xi) - yk (xi))g(x0, xi) = -c	u(x, x0)Qk(x0).	(C.5)
x	xi ∈D	x
Since the kernel u(x, x0) is universal and positive definite, the optimal solution Qk(x) is given by:
Qk(s, a) =	-1	X	(Qk(Si, ai)	— Vk(Si, ai))	∙	g((s,	a),(si,	ai)).	(C.6)
(si,ai)∈D
Finally we can replace the expression for residual error, Qk (Si, ai) - yk (Si, ai) using the green’s
kernel on the training data by solving for it in closed form, which gives us the solution in Equation 2.
Qk(S, a) = - 1 gTS,a)(Qk- Vk ) = gTS,a)(Cl + G)Tyk∙	(C.7)
□
25
Published as a conference paper at ICLR 2021
Next, we now state and prove a slightly stronger version of Theorem 4.1 that immediately implies
the original theorem.
Theorem C.1. Let S be a shorthand for S = γPπA and assume S is a normal matrix. Then there
exists an infinite, strictly increasing sequence of fitting iterations, (kl)l∞=1 starting from k1 = 0, such
that, for any two singular-values σi(S) and σj (S) of S with σi(S) ≤ σj (S),
∀l ∈ N and l0 ≥ l,
σi (Mkl0) σi (Mkl) σi(S)
---；----T < ---；----- ≤ --, 一,、
σj (MklO)	σj (Mkl) — σj (S)
(C.8)
Therefore, the effective rank of Mk satisfies: srankδ (Mkl0 ) ≤ srankδ (Mkl ). Furthermore,
∀ l ∈ N and t ≥ kl
σi(Mt) ; P(Mkl) + O ((空Yl
σj (Mt)	σj (Mkl)	σj (S)J
(C.9)
Therefore, the effective rank of Mt, srankδ (Mt), outside the chosen subsequence is also controlled
above by the effective rank on the subsequence (srankδ (Mkl ))l∞=1.
To prove this theorem, we first show that for any two fitting iterations, t < t0, if St and St are
positive semi-definite, the ratio of singular values and the effective rank decreases from t to t0 .
As an immediate consequence, this shows that when S is positive semi-definite, the effective rank
decreases at every iteration, i.e., by setting kl = l (Corollary C.1.1).
To extend the proof to arbitrary normal matrices, we show that for any S, a sequence of fitting iter-
ations (kl)l∞=1 can be chosen such that Skl is (approximately) positive semi-definite. For this subse-
quence of fitting iterations, the ratio of singular values and effective rank also decreases. Finally, to
control the ratio and effective rank on fitting iterations t outside this subsequence, we construct an
upper bound on the ratio f (t): ；i(Mt) < f (t), and relate this bound to the ratio of singular values
on the chosen subsequence.
Lemma C.1.1 (srankδ(Mk) decreases when Sk is PSD.). Let S be a shorthand for S = γPπA
and assume S is a normal matrix. Choose any t, t0 ∈ N such that t < t0. If St and St0 are positive
semi-definite, then for any two singular-values σi(S) andσj(S) of S, such that 0 < σi(S) < σj (S):
σi(Mt0)	σi(Mt)	σi(S)
—：-----< —：  ≤ —：--
σj(Mt，)	σj(Mt) ≤ %• (S)
(C.10)
Hence, the effective rank of Mk decreases from t to t0: srankδ (Mt0) ≤ srankδ (Mt).
Proof. First note that Mk is given by:
kk
Mk := XYI(PπA)I = X Sl.
i=1	i=1
(C.11)
From hereon, we omit the leading γ term since itis a constant scaling factor that does not affect ratio
or effective rank. Almost every matrix S admits a complex orthogonal eigendecomposition. Thus,
we can write S := Uλ(S)UH. And any power of S, i.e., , Si can be expressed as: Si = Uλ(S)i UH,
and hence, we can express Mk as:
Mk := U (X λ(S)i) UH = U ∙ diag (1 - 黑:)∙ UH
i=0	1- λ(S)
(C.12)
Since S is normal, its eigenvalues and singular values are further related as σk(S) = ∣λk(S)|. And
this also means that Mk is normal, indicating that σi(Mk) = ∣λi(Mk)|. Thus, the singular values
of Mk can be expressed as
σi(Mk) ：= 1- R(S): ,	(C.13)
1 - λi(S)
When Sk is positive semi-definite, λi(S)k = σi(S)k, enabling the following simplification:
σi(Mk)
|1- σi(S)k|
|1 - λi(S)∣ .
(C.14)
26
Published as a conference paper at ICLR 2021
To show that the ratio of singular values decreases from t to t0, we need to show that f (σ)
∣1-σt0∣
l1-σt |
is an increasing function of σ when t0 > t. It can be seen that this is the case, which implies the
desired result.
To further show that srankδ(Mt) ≥ srankδ(Mt，)，We can simply show that ∀i ∈ [1, ∙∙∙ ,n],
Pi σ (M )
hk(i) := Pn 仃(mJ increases with k, and this would imply that the srankδ(Mk) cannot increase
from k = t to k = t0. We can decompose hk (i) as:
i
hk(i) = X
j=1
σ (Mk)
Pl σι(Mk)
1
1	Pn=i+1 σj(Mk).
1+ Pi = I σj(Mk)
(C.15)
Since σj(Mk)∕σι (Mi) decreases over time k for all j, l if σj(S) ≤ σι(S), the ratio in the denomi-
nator of hk(i) decreases with increasing k implying that hk(i) increases from t to t0.	□
Corollary C.1.1 (srankδ(Mk) decreases for PSD S matrices.). Let S be a shorthand for S =
γP πA. Assuming that S is positive semi-definite, for any k, t ∈ N, such that t > k and that
for any two singular-values σi(S) and σj (S) of S, such that σi(S) < σj (S),
σi(Mt)	σi(Mk) < σi(S)
σj(Mt)	σj(Mk) ≤ σj(S)
(C.16)
Hence, the effective rank of Mk decreases with more fitting iterations: srankδ (Mt) ≤ srankδ (Mk).
In order to now extend the result to arbitrary normal matrices, we must construct a subsequence of
fitting iterations (kl)l∞=1 where Skl is (approximately) positive semi-definite. To do so, we first prove
a technical lemma that shows that rational numbers, i.e., numbers that can be expressed as r = p,
for integers p, q ∈ Z are “dense” in the space of real numbers.
Lemma C.1.2 (Rational numbers are dense in the real space.). For any real number α, there exist
infinitely many rational numbers P such that a can be approximated by p upto q2 accuracy.
p
α----
q
≤ 3.
q2
(C.17)
Proof. We first use Dirichlet’s approximation theorem (see Hlawka et al. (1991) for a proof of this
result using a pigeonhole argument and extensions) to obtain that for any real numbers α and N ≥ 1,
there exist integers p and q such that 1 ≤ q ≤ N and,
lqα-pl≤ ∣N1+T <Nn.	(C.18)
Now, since q ≥ 1 > 0, we can divide both sides by q, to obtain:
α -P ≤ N ≤ 1.	(C.19)
To obtain infinitely many choices for p, we observe that Dirichlet,s lemma is valid only for all
values of N that satisfy N ≤ ∣qj~-p∣. Thus if we choose an N0 such that N0 ≥ Nmax where Nmax
is defined as:
Nmax = max { .ɑ-p0∣ | P0, d ∈ Z, 1 ≤ q0 ≤ q} .	(C.20)
Equation C.20 essentially finds a new value of N, such that the current choices of p and q, which
were valid for the first value ofN do not satisfy the approximation error bound. Applying Dirichlet’s
lemma to this new value of N0 hence gives us a new set of p0 and q0 which satisfy the q⅛ approx-
imation error bound. Repeating this process gives us countably many choices of (p, q) pairs that
satisfy the approximation error bound. As a result, rational numbers are dense in the space of real
numbers, since for any arbitrarily chosen approximation accuracy given by q2, we can obtain atleast
one rational number, P which is closer to α than q2. This proof is based on Johnson (2016).	□
Now we utilize Lemmas C.1.1 and C.1.2 to prove Proposition 4.1.
27
Published as a conference paper at ICLR 2021
Proof of Proposition 4.1 and Theorem C.1 Recall from the proof of Lemma C.1.1 that the sin-
gular values of Mk are given by:
σi(Mk) :
1 - λi(S)k
1 - λi(S)
(C.21)
Bound on Singular Value Ratio: The ratio between σi(Mk) and σj(Mk) can be expressed as
σi(Mk) = 1 - %(S)k	1 - λj(S)
σj(Mk) = 1 - λj(S)k 1 - λi(S).
For a normal matrix S, σi(S) = ∣λi(S)∣, so this ratio can be bounded above as
σi(Mk) < 1+ σi(S)k 1 - λj(S)
σj(Mk) ≤ |1 -同(S)k | 1 - λi(S)
(C.22)
(C.23)
Defining f(k) to be the right hand side of the equation, we can verify that f is a monotonically
decreasing function in k when σi < σj . This shows that this ratio of singular values in bounded
above and in general, must decrease towards some limit limk→∞ f (k).
Construction of Subsequence: We now show that there exists a subsequence (kl)l∞=1 for which Skl
is approximately positive semi-definite. For ease of notation, let’s represent the i-th eigenvalue as
λi(S) = ∣λi(S)∣ ∙ eiθi, where θ% > 0 is the polar angle of the complex value λi(s) and ∣λi(S)∣ is
its magnitude (norm). Now, using Lemma C.1.2, we can approximate any polar angle, θi using a
rational approximation, i.e.,, We apply lemma C.1.2 on 务
∃ pi,qi ∈ N, s.t. T - ≤ -2 .	(C.24)
2π	qi	qi2
Since the choice of qi is within our control we can estimate θi for all eigenvalues λi(S) to infinites-
imal accuracy. Hence, we can approximate θi ≈ 2∏ pi. We will now use this approximation to
construct an infinite sequence (kl)l∞=1, shown below:
k = l ∙LCM(qι,…，qn) ∀ j ∈ N,	(C.25)
where LCM is the least-common-multiple of natural numbers qι,…q%
In the absence of any approximation error in θi , we note that for any i and for any l ∈ N as defined
above, λi(S)kl = ∣λi(S)∣kl ∙ exp(2in ∙ pi ∙ k，= ∣λi(S)∣kl, since the polar angle for any k is
going to be a multiple of 2π, and hence it would fall on the real line. As a result, Skl will be positive
semi-definite, since all eigenvalues are positive and real. Now by using the proof for Lemma C.1.1,
we obtain the ratio of i and j singular values are increasing over the sequence of iterations (kj)j∞=1.
Since the approximation error in θi can be controlled to be infinitesimally small to prevent any
increase in the value of srankδ due to it (this can be done given the discrete form of srankδ), we
note that the above argument applies even with the approximation, proving the required result on the
subsequence.
Controlling All Fitting Iterations using Subsequence:
We now relate the ratio of singular values within this chosen subsequence to the ratio of singular
values elsewhere. Choose t, l ∈ N such that t > kl. Earlier in this proof, we showed that the ratio
between singular values is bounded above by a monotonically decreasing function f (t), so
σi(Mt) / 介八 / ,∣,、
σjwt) ≤f(t) <f(kl).
Now, we show that that f(kl) is in fact very close to the ratio of singular values:
f(kl)
|1- σi(S)kl |
∣1-σj(S)kl |
1 -	λj (S)	<	C 2σi(S)kl	1 - λj (S)
1 -	X(S)	≤	σj(Mt)	+ |1 - σj(S)/	1 - %(S)
(C.26)
(C.27)
The second term goes to zero as kl increases; algebraic manipulation shows that this gap be bounded
by
fk ≤ σjw⅛+
kl
2σj(S)
1 - λj(S)
jIS))	|1 - σj(S)| 1 - λi(S).
(C.28)
I
}
z
constant
28
Published as a conference paper at ICLR 2021
Putting these inequalities together proves the final statement,
σi(Mt) ≤ /(MJ + O ((山 丫)
σj(Mt) ~ σj(Mkl)	σj(S)J
(C.29)
Extension to approximately-normal S. We can extend the result in Theorem C.1 (and hence also
Theorem 4.1) to approximately-normal S. Note that the main requirement for normality of S (i.e.,
σi(S) = ∣λi(s)∣) is because it is straightforward to relate the eigenvalue of S to M as shown below.
∣λi(Mk)∣ :
1 - λi(S)k
1 - %(S)
(C.30)
Now, since the matrix S is approximately normal, we can express it using its Schur’s triangular
form as, S = U ∙ (Λ + N) ∙ UH, where A is a diagonal matrix and N is an “offset" matrix. The
departure from normality of S is defined as: ∆(S) := infN ||N||2, where the infimum is computed
over all matrices N that can appear in the Schur triangular form for S. For a normal S only a
single value of N = 0 satisfies the Schur’s triangular form. For an approximately normal matrix S,
||N||2 ≤ ∆(S) ≤ ε, for a small ε.
Furthermore note that from Equation 6 in Ruhe (1975), we obtain that
∣σi(S) -∣λi(S)∣∣ ≤ ∆(S) ≤ ε,	(C.31)
implying that singular values and norm-eigenvalues are close to each other for S.
Next, let US evaluate the departure from normality of Mk. First note that, Sj = U ∙ (A + N)j ∙ UH,
and so, Mk = U ∙ (Pk=ι(A + N)j) ∙ UH and if ∣∣N∣∣2 ≤ ε, for a small epsilon (i.e., considering
only terms that are linear in N for (A + N)j), we note that:
k1
lσi(Mk) - lλi(Mk)1| ≤ X j ∙ lλ1(S)IjT△⑸ ≤ (T-KW ∙ J	(C.32)
Thus, the matrix Mk is also approximately normal provided that the max eigenvalue norm of S is
less than 1. This is true, since S = γPπA (see Theorem 4.1, where both Pπ and A have eigenvalues
less than 1, and γ < 1.
Given that we have shown that Mk is approximately normal, we can show that srankδ (Mk) only
differs from srankδ,λ(Mk), i.e., , the effective rank of eigenvalues, in a bounded amount. If the value
of ε is then small enough, we still retain the conclusion that srankδ(Mk) generally decreases with
more training by following the proof of Theorem C.1.
D	Proofs for Section 4.2
In this section, we provide technical proofs from Section 4.2. We start by deriving properties of opti-
mization trajectories of the weight matrices of the deep linear network similar to Arora et al. (2018)
but customized to our set of assumptions, then prove Proposition 4.1, and finally discuss how to ex-
tend these results to the fitted Q-iteration setting and some extensions not discussed in the main pa-
per. Similar to Section 4.1, we assume access to a dataset of transitions, D = {(si, ai, r(si, ai), s0i}
in this section, and assume that the same data is used to re-train the function.
Notation and Definitions. The Q-function is represented using a deep linear network with at least
3 layers, such that
Q(s, a)= WN WN-1 …Wι[s; a], where N ≥ 3, WN ∈ R1×dN-1,	(D.1)
and Wi ∈ Rdi×di-1 for i = 1, . . . , N - 1. We index the weight matrices by a tuple (k, t): Wj(k, t)
denotes the weight matrix Wj at the t-th step of gradient descent during the k-th fitting itera-
tion (Algorithm 1). Let the end-to-end weight matrix WNWN-ι ∙∙∙ W1 be denoted shorthand as
WN:1, and let the features of the penultimate layer of the network, be denoted as Wφ(k, t) :=
29
Published as a conference paper at ICLR 2021
WN-ι(k, t) ∙∙∙ Wι(k,t). Wφ(k,t) is the matrix that maps an input [s; a] to corresponding fea-
tures Φ(s, a). In our analysis, it is sufficient to consider the effective rank of Wφ(k, t) since the
features Φ are given by: Φ(k, t) = Wφ(k, t)[S; A], which indicates that:
rank(Φ(k, t)) = rank(Wφ(k, t)[S; A]) ≤ min (rank(Wφ(k, t)), rank([S; A])) .
Assuming the state-action space has full rank, we are only concerned about rank(Wφ(k, t)) which
justifies our choice for analyzing srankδ (Wφ(k, t)).
Let Lk+1 (WN:1(k, t)) denote the mean squared Bellman error optimization objective in the k-th
fitting iteration.
|D|
Lk+1 (WN:1 (k, t)) = X (WN(k,t)Wφ(k,t)[si; ai] - yk(si, ai))2 , where yk = R + γPπQk.
i=1
When gradient descent is used to update the weight matrix, the updates to Wi (k, t) are given by:
∂Lk+ι(WMi(k,t))
∂ Wj (k,t)
Wj(k,t + 1) J Wj(k,t) - η
If the learning rate η is small, we can approximate this discrete time process with a continuous-time
differential equation, which we will use for our analysis. We use W(k, t) to denote the derivative of
W(k, t) with respect to t, for a given k.
, , ,
Wj(k,t)
- dLk+i(WN：i(k,t))
-η	∂Wj (k,t)
(D.2)
In order to quantify the evolution of singular values of the weight matrix, Wφ(k, t), we start by
quantifying the evolution of the weight matrix Wφ(k, t) using a more interpretable differential
equation. In order to do so, we make an assumption similar to but not identical as Arora et al. (2018),
that assumes that all except the last weight matrix are “balanced” at initialization t = 0, k = 0. i.e.
∀i ∈ [0,…，N — 2] ： WT+ι(0,0)Wi+ι(0,0) = Wi(0,0)Wi(0,0)T	(D.3)
Note the distinction from Arora et al. (2018), the last layer is not assumed to be balanced. As a result,
we may not be able to comment about the learning dynamics of the end-to-end weight matrix, but
we prevent the vacuous case where all the weight matrices are rank 1. Now we are ready to derive
the evolution of the feature matrix, Wφ(k, t).
Lemma D.0.1 (Adaptation of balanced weights (Arora et al., 2018) across FQI iterations). Assume
the weight matrices evolve according to Equation D.2, with respect to Lk for all fitting iterations
k. Assume balanced initialization only for the first N - 1 layers, i.e., Wj+1 (0, 0)T Wj+1(0, 0) =
Wj (0, 0)Wj (0, 0)T,∀ j ∈ 1, ∙∙∙ ,N 一 2. Then the weights remain balanced throughout, i.e.
∀ k,t Wj+1(k,t)τ Wj+1(k,t) = Wj (k, t)Wj (k,t)τ, ∀ j ∈ 1,…，N 一 2.	(D.4)
Proof. First consider the special case of k = 0. To beign with, in order to show that weights
remain balanced throughout training in k = 0 iteration, we will follow the proof technique in Arora
et al. (2018), with some modifications. First note that the expression for 江飞WWNAty)) Can be
expressed as:
∂Lk+ι(WN ：i(k,t))
∂Wj (k,t)
mWT)∙ dLWrι ∙ (YWT!
(WTHWT+2 …WN) ∙ dLWN^ ∙( J WT
Now, since the weight matrices evolve as per Equation D.2, by multiplying the similar differential
equation for Wj with WjT(k, t) on the right and multiplying evolution of Wj+1 with WjT+1(k, t)
from the left, and adding the two equations, we obtain:
∀j ∈ [0,…，N — 2] : WTH(0,t)Wj+ι(0,t)= Wj(0,t)WT(0,t).	(D.5)
30
Published as a conference paper at ICLR 2021
We can then take transpose of the equation above, and add it to itself, to obtain an easily integrable
expression:
d wj+1(0,tdWj+1(0,t)T = WT+1(0,t)W j+ι(0,t)+ W j+ι(0,t)WT+ι(0,t)=
Wj (0,t)WT(0,t) + Wj (0,t)WT(0,t) = d WT(O,dtWj (0,t). (D.6)
Since we have assumed the balancedness condition at the initial timestep 0, and the derivatives of
the two quantities are equal, their integral will also be the same, hence we obtain:
WjT+1(0,t)WjT+1(0,t) = Wj(0,t)Wj(0,t)T.	(D.7)
Now, since the weights after T iterations in fitting iteration k = 0 are still balanced, the initialization
for k = 1 is balanced. Note that since the balancedness property does not depend on which objective
gradient is used to optimize the weights, as long as Wj and Wj+1 utilize the same gradient of the
loss function. Formalizing this, we can show inductively that the weights will remain balanced
across all fitting iterations k and at all steps t within each fitting iteration. Thus, we have shown the
result in Equation D.4.	□
Our next result aims at deriving the evolution of the feature-matrix that under the balancedness
condition. We will show that the feature matrix, Wφ(k, t) evolves according to a similar, but distinct
differential equation as the end-to-end weight matrix, WN:1 (k, t), which still allows us to appeal to
techniques and results from Arora et al. (2019) to study properties of the singular value evolution
and hence, discuss properties related to the effective rank of the matrix, Wφ(k, t).
Lemma D.0.2 ((Extension of Theorem 1 from Arora et al. (2018)). Under conditions specified in
Lemma D.0.1, the feature matrix, Wφ(k, t) evolves as per the following continuous-time differential
equation, for all fitting iterations k:
Wφ(k,t) = -ηPNII [Wφ(k,t)Wφ(k,t)T] N-j ∙ WN(k,t)TdLk(WN*i ∙ [Wφ(k,t)TWφ(k,t)] N-1.
Proof. In order to prove this statement, we utilize the fact that the weights upto layer N - 2 are
balanced throughout training. Now consider the singular value decomposition of any weight wj
(unless otherwise states, we use Wj to refer to Wj (k, t) in this section, for ease of notation. Wj =
Uj Σj VjT . The belancedness condition re-written using SVD of the weight matrices is equivalent to
Vj+1ΣjT+1Σj+1VjT+1 = UjΣjΣjTUjT.	(D.8)
Thus for all j on which the balancedness condition is valid, it must hold that ΣjT+1Σj+1 = ΣjΣjT,
since these are both the eigendecompositions of the same matrix (as they are equal). As a result,
the weight matrices Wj and Wj+1 share the same singular value space which can be written as
ρι ≥ ρ2 ≥ …≥ Pm. The ordering of eigenvalues can be different, and the matrices U and V can
also be different (and be rotations of one other) but the unique values that the singular values would
take are the same. Note the distinction from Arora et al. (2018), where they apply balancedness on
all matrices, and that in our case would trivially give a rank-1 matrix.
Now this implies, that we can express the feature matrix, also in terms of the common singular
values, (ρι, ρ2, ∙ ∙ ∙ , Pm), for example, as Wj(k, t) = Uj+1Diag (√ρT, ∙ ∙∙ , √ρm) VT, where
U)j = Vj+1Oj, where Oj is an orthonormal matrix. Using this relationship, we can say the
following:
N-1	N-1	N-j
∏ Wi(k,t) ∏ Wi(k,t)τ = [Wφ(k,t)WT(k,t)]N-T
i=j	i=j
j	j	j
∏Wi(k,t)τ∏Wi(k,t) = [Wφ(k,t)τWφ(k,t)] N-1.
i=1	i=1
31
Published as a conference paper at ICLR 2021
Now, we can use these expressions to obtain the desired result, by taking the differential equations
governing the evolution of Wi(k,t), for i ∈ [1,…，N - 1], multiplying the i-th equation by
QiN+-11 Wj (k, t) from the left, and Qi1-1 Wj (k, t) to the right, and then summing over i.
N-1	N-1	i-1
Wφ(k,t) = X Y Wj(k,t) Wi(k,t)	Y Wj(k,t)
i=1	j=i+1	j=1
N —1 (N-1	N	∕i-1	i-1	∖
=-η X Y Wj(k,t)Y Wj(k,t)T	dW N:1 I Y Wj(k,t)T Y Wj(k,t)l
i=1	i+1	i+1	N :1 j=1	j=1
The above equation simplifies to the desired result by taking out WN (k, t) from the first summation,
and using the identities above for each of the terms.	口
Comparing the previous result with Theorem 1 in Arora et al. (2018), we note that the resulting
differential equation for weights holds true for arbitrary representations or features in the network
provided that the layers from the input to the feature layer are balanced. A direct application of Arora
et al. (2018) restricts the model class to only fully balanced networks for convergence analysis and
the resulting solutions to the feature matrix will then only have one active singular value, leading to
less-expressive neural network configurations.
Proof of Proposition 4.1. Finally, we are ready to use Lemma D.0.2 to prove the relationship with
evolution on singular values. This proof can be shown via a direct application of Theorem 3 in Arora
et al. (2019). Given that the feature matrix, Wφ(k, t) satisfies a very similar differential equation as
the end-to-end matrix, with the exception that the gradient of the loss with respect to the end-to-end
matrix is pre-multiplied by WN(k, t)T. As a result, we can directly invoke Arora et al. (2019)’s
result and hence, We have ∀r ∈ [1,…，dim(W)] that:
σr (k,t) = -N ∙ (σr2 (k,t))1-N-I ∙ (Wn(k,t)T dLNd(WK)), Ur (k,t"r (k,t)τ). (D.9)
Further, as another consequence of the result describing the evolution of Weight matrices, We can
also obtain a result similar to Arora et al. (2019) that suggests that the goal of the gradient update on
the singular vectors U(k, t) and V(k, t) of the features Wφ(k, t) = U(k, t)S(k, t)V(K, t)T, is to
align these spaces with WN(k,t)T "NdWWKt).
D.1 Explaining Rank Decrease Based on Singular Value Evolution
grow at a disproportionately higher rate than other smaller singular values as described by equation 4 .
In this section, we discuss why the evolution of singular values discussed in Equation 4 indi-
cates a decrease in the rank of the feature matrix within a fitting iteration k. To see this, let’s
consider the case when gradient descent has been run long enough (i.e., the data-efficient RL
case) for the singular vectors to stabilize, and consider the evolution of singular values post
timestep t ≥ t0 in training. First of all, when the singular vectors stabilize, we obtain that
Ur (k,t)TWN(k,tTdLNdWWKt) Vr(k,t) is diagonal (extending result of Corollary 1 from Arora
et al. (2019)). Thus, we can assume that
UT (k,t) WN (k,t)T "1NkkWKt Vr (k,t) = f (k,t) ∙ er ∙ d『,
where er is given by the unit basis vector for the standard orthonormal basis, f is a shorthand for the
gradient norm of the loss function pre-multiplied by the transpose of the last layer weight matrix,
32
Published as a conference paper at ICLR 2021
and dr denotes the singular values of the state-action input matrix, [S, A]. In this case, we can
re-write Equation 4 as:
σr (k, t) = -N Sr(k,t))1 -IN ∙ f (k,t) ∙ er ∙ dr.	(D.10)
Note again that unlike Arora et al. (2019), the expression for f(k, t) is different from the gradient of
the loss, since the weight matrix WN(k, t) is multiplied to the expression of the gradient in our case.
By using the fact that the expression for f(k, t) is shared across all singular values, we can obtain
differential equations that are equal across different σr(k, t) and σr0 (k, t). Integrating, we can show
that depending upon the ratio eer[r,, and the value of N, the singular values σr∙(k,t) and σw(k, t)
will take on different magnitude values, in particular, they will grow at disproportionate rates. By
then appealing to the result from Proposition 4.1 in Section 4.1 (kernel regression argument), we can
say that disproportionately growing singular values would imply that the value of srankδ(Wφ(k, t))
decreases within each iteration k as we will discuss next.
Interpretation of the abstract rank-regularized objective. We now discuss the form for the ab-
stract rank-regularized objective in Equation 5 that we utilize in Section 4.2. Intuitively, the justifica-
tion for Equation 5 is that larger singular values grow much faster than small singular values, which
dictates how the ratio of singular values evolves through time, thereby reducing effective rank.
The discussion in the previous subsection shows that the effective rank, srankδ(Wφ(k, t)) decreases
within each fitting iteration (i.e., decreases over the variable t). Now, we will argue that for a given
value of effective rank att = 0 at a fitting iteration k (denoted srankδ(Wφ(k, 0)), the effective rank
after T steps of gradient descent, srankδ (Wφ(k, T )), is constrained to be equal to some constant
εk < srankδ(Wφ(k, 0)) due to the implicit regularizaton effect of gradient descent. To see this,
note that if σi(k, 0) > σj(k, 0), then σi(k, t) increases at a much faster rate than σj (k, t) with
increasing t (see Equation D.10 and Arora et al. (2019), Equation 11) and hence, the value of hk,t(i)
is increasing over t:
i
hk,t(i) = X
j=1
σ (Wφ(At))
Pl σι(Wφ(k,t))
1 , Pn=i+1 <σ (wφ(k,t))
1+ Pj = I σj (Wφ(k,t))
≥ ι + Pn	σj(Wφ(k,t)), (D.1I)
1 + 乙 j=i+1 σι(Wφ(k,t))
1
as the ratio between larger and smaller singular values increases. Using Equation D.11, we note that
the value of hk,t(i) approaches 1 as the ratio σj /σ1 decreases (in the limit, the ratio is zero). An
increase in the value of hk(i) implies a decrease in srankδ, which can be expressed as:
srankδ(Wφ(k, t)) = min{ hk(j) ≥ 1 - δ } .
j
Thus, by running gradient descent for T0 = T(εk) (a function of εk) steps within a fitting iteration,
k, the effective rank of the resulting Wφ(k, T0) satisfies srankδ(Wφ(k, T0)) = εk. Now, since
the solution Wφ(k, T0) satisfies the constraint srankδ(Wφ(k, T0)) = εk < srankδ (Wφ(k, 0)),
we express this solution using the solution of a penalized optimization problem (akin to penalty
methods) that penalizes srankδ (Wφ) for a suitable coefficient λk, which is a function of εk. We
assume λk > 0 since when running gradient descent long enough (i.e., for T0 steps in the above
discussion), we can reduce srankδ(Wφ) from its initial value (note that εk < srankδ(Wφ(k, 0))),
indicating a non-zero value of regularization.
D.2 Proof of Proposition 4.2: Compounding Rank Drop in Section 4.2
In this section, we illustrate that the rank drop effect compounds due to bootstrapping and prove
Proposition 4.2 formally. Our goal is to demonstrate the compounding effect: a change in the rank
at a given iteration gets propagated into the next iteration. We first illustrate this compounding effect
and then discuss Theorem 4.2 as a special case of this phenomenon.
Assumptions. We require two assumptions pertaining to closure of the function class and the change
in effective rank due to reward and dynamics transformations, to be able for this analysis. We assume
that the following hold for any fitting iteration k of the algorithm.
Assumption D.1 (Closure). The Chosenfunction class W-W2 …WN is closed under the Bellman
evaluation backup for policy π. That is, if QkT-1 = WN(k - 1)Wφ(k - 1)[S; A]T, then there exists
an assignment of weights to the deep linear net such that the corresponding target value R+γP πQk
can be written as (R + γP πQk--)T = WNP (k)WφP (k)[S; A]T. This assumption is commonly
used in analyses of approximate dynamic programming with function approximation.
33
Published as a conference paper at ICLR 2021
Assumption D.2 (Change in srankδ). For every fitting iteration k, we assume that the the difference
between srankδ (WφP (k)), equal to the effective rank to feature matrix obtained when R+γP πQk-1
is expressed in the function class, and srankδ(Wφ(k - 1)) by an iteration specific threshold ck, i.e.,
srankδ (WφP (k)) ≤ srankδ (Wφ (k - 1)) + ck.
We will characterize the properties of ck in special cases by showing how Theorem 4.2 is a special
case of our next argument, where we can analyze the trend in ck .
To first see how a change in rank indicated by ck in Assumption D.2 propagates through bootstrap-
ping, note that we can use Assumption D.2 and D.1 to observe the following relation:
srankδ(Wφ(k)) ≤ srankδ(WP(k)) - llQk- Yk|| (since We can copy over the weights)
≤ srankδ (Wφ(k - 1)) + ck -
≤ srankδ (Wφ(k - 2)) + ck-1
llQk	yk|| (using AssumptionD.2)
λk
llQk-1 - yk-1ll	llQk - ykll
λk-1 — + ck	λk—
srankδ(Wφ(k)) ≤ srankδ(Wφ(0))+ XCj- X llQj∖ yjl1	(D.12)
λj
j=1	j=ι	j
Derivation of the steps: The first inequality holds via the weight-copying argument: since the
weights for the target value R+γPπQk-1 can be directly copied to obtain a zero TD error feasible
point with an equal srank, we will obtain a better solution at the optimum with a smaller srank
in Equation 5. We then use Assumption D.2 to relate srankδ (WφP (k)) to the effective rank of the
previous Q-function, srankδ(Wφ(k - 1)), which gives us a recursive inequality. Finally, the last two
steps follow from a repeated application of the recursive inequality in the second step. This proves
the result shown in Proposition 4.2.
Now if the decrease in effective rank due to accumulating TD errors is larger than the possible
cumulative increase in rank ck, then we observe a rank drop. Also note that ck need not be positive,
ck can be negative, in which case both terms contribute to a drop in rank. But in the most general
case, ck, can also be positive for some iterations. This equation indicates how a change in in rank in
one iteration gets compounded on the next iteration due to bootstrapping.
What can we comment about the general value of ck? In
the general case, ck can be positive, for example when the ad-
dition of the reward and dynamics projection increases srank.
Our current set of assumptions are insufficient to guarantee
something about ck in the general case and more assumptions
are needed. For instance, as shown in Theorem 4.2, under
Assumption D.1, we obtain that ifwe are in a small neighbor-
hood around the fixed point, we will find that ck ≤ 0.
Empirical verification: While our analysis does not dictate
the value of ck , we performed an experiment on Seaquest
to approximately visualize srankδ (WφP (k)) by computing
the effective rank for the feature matrix Φ obtained when
P π Qk-1 is expressed in the Q-function class. We do not in-
clude the reward function in this experiment, but we believe
that a “simple” reward function, i.e., it takes only three possi-
ble values in Atari, and hence it may not contribute too much
to srankδ (WφP (k)). As shown in Figure D.2, the value of the
target value feature effective rank decreases, indicating that
the coefficient ck is expected to be bounded in practice.
Figure D.2: Two pairs of runs depict-
ing the trends of target feature (approx-
imately equal to srankδ(Wp (k))) (de-
noted as “Target”) and srankδ (Wφ (k -
1)) (denoted as “Main”) on Seaquest
with in the data-efficient online RL set-
ting. Note that the contribution of dy-
namics transformation in the value of Ck
is generally negative for the Pair 1 and
is small/positive for Pair 2.
D.3 PROOF FOR THEOREM 4.2: RANK COLLAPSE NEAR A FIXED POINT
Now, we will prove Theorem 4.2 by showing that when the current Q-function iterate Qk is close to
a fixed point but have not converged yet, i.e., , when llQk - (R+γPπQk)ll ≤ ε, then rank-decrease
34
Published as a conference paper at ICLR 2021
happens. We will prove this as a special case of the discussion in Section D.2, by showing that for
any iteration k, when the current Q-function iterate Qk is close to a fixed point, the value of ck can
be made close to 0. While the compounding argument (Equation D.12) may not directly hold here
since we cannot consistently comment on whether making an update on the Q-function pushes it out
of the ε-ball near the fixed point, so we instead prove a “one-step” result here.
To prove Theorem 4.2, we evaluate the (infinitesimal) change in the singular values of the features
of Wφ(k, t) as a result of an addition in the value of ε. In this case, the change (or differential) in
the singular value matrix, S(k, T), (Wφ (k, T) = U(k, T)S(k, T )V(k, T)T) is given by:
dS(k, T) = Id ◦ [u(k, T)T ∙ dWφ(k,t) ∙ V(k, T)],	(D.13)
using results on computing the derivative of the singular value decomposition (Townsend, 2016).
Proof strategy: Our high-level strategy in this proof is to design a form for ε such that the value
of effective rank for feature matrix obtained when the resulting target value yk = Qk-1 + ε is
expressed in the deep linear network function class, let’s say with feature matrix, Wφ0 (k, T) and the
last layer weights, WN (k, T) = WN(k - 1, T), then the value of srankδ(Wφ0 (k, T)) can be larger
than srankδ(Wφ(k - 1, T)) by only a bounded limited amount α that depends on ε. More formally,
we desire a result of the form
srankδ(W0φ(k, T)) ≤ srankδ (Wφ(k - 1, T)) + α,
where ||||	||Qk-1||, and yk (s, a) = WN(k - 1, T)Wφ0 (k, T)[s; a].
(D.14)
Once we obtain such a parameterization of yk in the linear network function class, using the argu-
ment in Section 4.2 about self-training, we can say that just copying over the weights Wφ0 (k, T) is
sufficient to obtain a bounded increase in srankδ(Wφ(k, T)) at the cost of incurring zero TD error
(since the targets can be exactly fit). As a result, the optimal solution found by Equation 5 will attain
lower srankδ(Wφ(k, T)) value if the TD error is non-zero. Specifically,
srankδ(Wφ(k,T))+ llQkλ-yk|| ≤ srankδ(Wφ(k - 1,T))+ α	(D.15)
As a result We need to examine the factors that affect srankδ(Wφ(k,T)) - (1) an increase in
srankδ (Wφ(k, T)) due to an addition of α to the rank, and (2) a decrease in srankδ (Wφ(k, T))
due to the implicit behavior of gradient descent.
Proof: In order to obtain the desired result (Equation D.14), We can express ε(s, a) as a function of
the last layer Weight matrix of the current Q-function, WN(k - 1, T) and the state-action inputs,
[s; a]. Formally, We assume that ε(s, a) = WN(k - 1, T)ζ[s; a], Where ζ is a matrix of the same
size as Wφ such that ||Z ∣∣∞«T = O(∣∣Wφ(k, T )∣∣∞), i.e., Z has entries with ignorable magnitude
compared the actual feature matrix, Wφ(k, T). This can be computed out of the closure assumption
(Assumption D.1) from Section D.2.
Using this form of ε(s, a), we note that the targets yk used for the backup can be written as
Vk = Qk-1 + ε = WN(k - 1, T)Wφ(-1, T)[S; A] + WN(k - 1, T)Z[S； A]
=WN(k - 1, T) ∙ (Wφ(k - 1, T) + Z)[S； A]
X--------------------------{--------}
Wφ0 (k,T)
Using the equation for sensitivity of singular values due to a change in matrix entries, we obtain the
maximum change in the singular values of the resulting “effective” feature matrix of the targets yk
in the linear function class, denoted as Wφ0 (k, T), is bounded:
dS0(k,T )= Id ◦ [u(k,T) ∙ dWφ(k,T) ∙ V(k,T )t]	=⇒ ∣∣dS0(k,T )∣∣∞ ≤ Z. (D.16)
35
Published as a conference paper at ICLR 2021
Now, let’s use this inequality to find out the maximum change in the function, hk(i) used to compute
srankδ(Wφ): hk(i)	=	Ij	；曜：(黑))	(Srankδ(Wφ(k,T))	= min	{j	:	hk(j) ≥ 1 -	δ}).
j=1 σj(Wφ ( ,T))
|dhk(i)|
= Pi=I dσ∙(W0) - PPi=I σ(W。)! PPN=I dσ(W0)! I
=Pj=Iσ(Wφ) - IPN=1 σ(Wφ)) ^P[UW))\
≤ iZ + PPi=I σ (Wφ) !	NZ
-pj=ι σ(Wφ)	∖PN=ι σ(Wφ)J Pj= σ(Wφ)
X------V----}
≤1
≤	(i + N )Z
-pi=ι σ (Wφ)
The above equation implies that the maximal change in the effective rank of the feature matrix
generated by the targets, yk (denoted as Wφ0 (k, T)) and the effective rank of the features of the
current Q-function Qk (denoted as Wφ(k, T)) are given by:
srankδ(W0φ(k,T)) - srankδ(Wφ(k, T)) ≤ α
where α can be formally written by the cardinality of the set:
α =\[j ： hk(j) - Jj + N£、≥ (1 — δ), hk(j) ≤ (1 - δ)l∖.	(D.17)
\	iN=1 σi (Wφ)	\
Note that by choosing ε(s, a) and thus ζ to be small enough, we can obtain Wφ0 (k, T) such that
α = 0. Now, the self-training argument discussed above applies and gradient descent in the next
iteration will give us solutions that reduce rank.
Assuming r > 1 and srankδ(Wφ(k, T)) = r, we know that hk(r - 1) < 1 -δ, while hk (r) ≥ 1 - δ.
Thus, srankδ(Wφ0 (k, T)) to be equal to r, it is sufficient to show that hk (r) - |dhk(r)| ≥ 1 - δ
since both hk(i) and dhk(i) are increasing for i = 0,1,…，r. Thus, srankδ(Wφ(k, T)) = r∀Z,
whenever
ζ≤
PiN=1 σi (Wφ)
r+N
hk (r)
-1-δ
Pi=I σ(Wφ)-(1- δ) Pjj=I σj(Wφ)
r + N
Pir=1σi(Wφ) - (1 -δ)PiN=1σi(Wφ)
This implies that ε ≤ ||Wn(k,T)∣∣∞  ------------------------- .
r+N
Proof summary: We have thus shown that there exists a neighborhood around the optimal fixed
point of the Bellman equation, parameterized by ε(s, a) where bootstrapping behaves like self-
training. In this case, it is possible to reduce srank while the TD error is non-zero. And of course,
this would give rise to a rank reduction close to the optimal solution.
36