Published as a conference paper at ICLR 2021
The Inductive Bias of ReLU Networks
on Orthogonally Separable Data
Mary Phuong & Christoph H. Lampert
IST Austria
Am Campus 1, 3400 Klosterneuburg, Austria
{bphuong,chl}@ist.ac.at
We study the inductive bias of two-layer ReLU networks trained by gradient flow. We
identify a class of easy-to-learn (‘orthogonally separable’) datasets, and characterise the
solution that ReLU networks trained on such datasets converge to. Irrespective of network
width, the solution turns out to be a combination of two max-margin classifiers: one corre-
sponding to the positive data subset and one corresponding to the negative data subset.
The proof is based on the little-known concept of extremal sectors, for which we prove
a number of properties in the context of orthogonal separability. In particular, we prove
stationarity of activation patterns from some time T onwards, which enables a reduction of
the ReLU network to an ensemble of linear subnetworks.
1	Introduction
This paper is motivated by the problem of understanding the inductive bias of ReLU networks, or
to put it plainly, understanding what it is that neural networks learn. This is a fundamental open
question in neural network theory; it is also a crucial part of understanding how neural networks
behave on previously unseen data (generalisation) and it could ultimately lead to rigorous a priori
guarantees on neural nets’ behaviour.
For a long time, the dominant way of thinking about machine learning systems was as minimisers
of the empirical risk (Vapnik, 1998; Shalev-Shwartz & Ben-David, 2014). However, this paradigm
has turned out to be insufficient for understanding deep learning, where many empirical risk min-
imisers exist, often with vastly different generalisation properties. To understand deep networks, we
therefore need a more fine-grained notion of ‘what the model learns’.
This has motivated the study of the implicit bias of the training procedure - the Ways in which the
training algorithm influences which of the empirical risk minimisers is attained. This is a productive
research area, and the implicit bias has already been worked out for many linear models.1 Notably,
Soudry et al. (2018) consider a logistic regression classifier trained on linearly separable data, and
show that the normalised weight vector converges to the max-margin direction. Building on their
work, Ji & Telgarsky (2019a) consider deep linear networks, also trained on linearly separable data,
and show that the normalised end-to-end weight vector converges to the max-margin direction. They
in fact show that all first-layer neurons converge to the same ‘canonical neuron’ (which points in the
max-margin direction). Although such impressive progress on linear models has spurred attempts at
nonlinear extensions, the problem is much harder and analogous nonlinear results have been elusive.
In this work, we provide the first such inductive-bias result for ReLU networks trained on ‘easy’
datasets. Specifically, we
•	propose orthogonal separability of datasets as a stronger form of linear separability that
facilitates the study of ReLU network training,
•	prove that a two-layer ReLU network trained on an orthogonally separable dataset learns a
function with two distinct groups of neurons, where all neurons in each group converge to
the same ‘canonical neuron’,
•	characterise the directions of the canonical neurons, which turn out to be the max-margin
directions for the positive and the negative data subset.
1A more thorough overview of related work can be found in Section 6.
1
Published as a conference paper at ICLR 2021
The proof is based on the recently introduced concept of extremal sectors (Maennel et al., 2018)
which govern the early phase of training. Our main technical contributions are a precise charac-
terisation of extremal sectors for orthogonally separable datasets, and an invariance property which
ensures that the network’s activation pattern becomes fixed at some point during training. The latter
allows us to treat ReLU networks late in training as ensembles of linear networks, which are much
better understood. We hope that a similar proof strategy could be useful in other contexts as well.
2	Setting and assumptions
In this section, we introduce the learning scenario including the assumptions we make about the
dataset, the model, and the training procedure. We consider binary classification. Denote the training
data {(xi, yi)}in=1 with xi ∈ Rd and yi ∈ {±1} for all i ∈ [n]. We denote by X ∈ Rd×n the matrix
with {xi} as columns and by y ∈ Rn the vector with {yi} as entries.
Orthogonally separable data. A binary classification dataset (X, y) is called orthogonally sepa-
rable if for all i, j ∈ [n],
xi|xj > 0,
xi|xj ≤ 0,
if yi = yj ,
if yi 6= yj .
(1)
In other words, a dataset is orthogonally separable iff it is linearly separable, and any training exam-
ple xi can serve as a linear separator. Geometrically, this means that examples with yi = 1 (‘positive
examples’) and examples with yi = -1 (‘negative examples’) lie in opposite orthants.
Two-layer ReLU networks. We define two-layer width-p fully-connected ReLU networks, pa-
rameterised by θ , {W, a}, as
fθ : Rd → R,	(2)
fθ(x) , alρ(Wx),
where W , [w1, . . . wp]| ∈ Rp×d and a , [a1, . . . , ap]| ∈ Rp are the first- and second-layer
weights of the network, and ρ is the element-wise ReLU function, ρ(z)i = max {0, zi}. We will
often view the network as a collection of neurons, {(aj, wj)}jp=1.
Cross-entropy loss. We assume a training loss of the form
n
'(θ) , X'i(fθ(Xi)),	'i(u) , log(1+exp(-yiu));	⑶
i=1
this is the standard empirical cross-entropy loss. More generally, our results hold when the loss is
differentiable, 'i is bounded and LiPschitz continuous, and satisfies -yi'i(u) > 0 for all U ∈ R.
Gradient flow training. We assume the loss is optimised by gradient descent with infinitesimally
small steP size, also known as gradient flow. Under the gradient flow dynamics, the Parameter
trajectory is an absolutely continuous curve {θ(t) | t ≥ 0} satisfying the differential inclusion
-)∈- ∈ -∂'(θ(t)),	for almost all t ∈ [0, ∞),	(4)
where ∂' denotes the Clarke subdifferential (Clarke,1975; Clarke et al., 2008) of ', an extension of
the gradient to not-everywhere differentiable functions,
∂'(θ)，Conv J lim V'(θk) I θk → θ∣.	(5)
k→∞
θ(t) is the value of the Parameters at time t, and we will use the suffix (t) more generally to denote
the value of some function of θ at time t.
2
Published as a conference paper at ICLR 2021
Near-zero balanced initialisation. We assume that the neurons {wj } are initialised iid from the
Gaussian distribution and then rescaled such that kwj k ≤ λ, where λ > 0 is a small constant. That
is, Wj = λjVj/∣∣vjk for Vj 肥 N(0,I) and arbitrary λj satisfying λj ∈ (0,λ]. We also assume
that aj ∈ {±λj}. These technical conditions ensure that the neurons are balanced and small in size,
kWj k = |aj | ≈ 0, which simplifies the calculations involved in the analysis of gradient flow.
Support examples span the full space. We assume that the support examples of the positive data
subset {xi | yi = 1} span the entire Rd, and similarly that the support examples of the negative data
subset {xi | yi = -1} span Rd. (We formally define support examples after introducing some more
notation below.)
3 Main res ult
Under the assumptions of Section 2, the network converges to a linear combination of two max-
margin neurons. Specifically, given a dataset (X, y), define the positive and the negative max-
margin vectors W+ , W- ∈ Rd as
W+ = arg min kWk2	subject to	W|xi	≥ 1 for i :	yi	= 1,	(6)
w
W- = arg min kWk2	subject to	W|xi	≥ 1 for i :	yi	= -1.	(7)
w
We call examples which attain equality in eqs. (6) and (7) positive support examples and negative
support examples respectively. We now state the main result.
Theorem 1. Let fθ be a two-layer width-p ReLU network trained by gradient flow with the cross-
entropy loss, initialised near-zero and balanced. Consider an orthogonally separable dataset (X, y)
such that its positive support examples span Rd, and its negative support examples also span Rd.
For almost all such datasets2 and with probability 1 - 1/2p over the random initialisation,
W(t)
kW(t)kF
- uW+| + zW-|	→ 0,
F
as t→ ∞,
(8)
for some u, z ∈ Rp+ such that either ui = 0 or zi = 0 for all i ∈ [p]. Also,
篇-(UkW+k- zkw-k)∣ → 0,
as t→ ∞.
(9)
The theorem says that each neuron (row of W), properly normalised, converges either to a scalar
multiple of the positive max-margin direction, uiW+, or to a scalar multiple of the negative max-
margin direction, ziW-. In other words, there are asymptotically only two distinct types of neurons,
and the network could in principle be pruned down to a width of just two. These two ‘canonical
neurons’ moreover have an explicit characterisation, given by eqs. (6) and (7).
As for the second-layer weights, the magnitude of each aj equals the norm of the respective Wj, and
the sign of aj is +1 if Wj approaches W+ and -1 ifWj approaches W-.
The following corollary summarises the above in terms of the function learnt by the network.
Corollary 1. Under the conditions of Theorem 1, there exist constants u, z ≥ 0 such that
fθ(t)(X)
kθ(t)k2
→ uρ(w+ x) — zρ(w-x),
as t→ ∞.
(10)
3.1	Discussion of assumptions
Many of our assumptions are technical, serving to simplify the analysis while detracting little from
the result’s relevance3. These include infinitesimal step size (gradient flow), balancedness at ini-
tialisation and the condition on support span. The first two could potentially be relaxed to their
2Formally, this means that if {xi } are sampled from any distribution with a density wrt. the Lebesgue
measure, then the theorem (treated as an implication) holds with probability one wrt. the data.
3We verify experimentally in Section 5.1 that these assumptions are indeed not crucial.
3
Published as a conference paper at ICLR 2021
approximate counterparts, i.e. gradient descent with a small constant step size and approximate bal-
ancedness (Arora et al., 2019). The assumption that support vectors span Rd comes from Ji &
Telgarsky (2019a); Soudry et al. (2018). It seems to us that it could be lifted, though we have not
investigated this possibility in depth.
Two assumptions that deserve more attention are near-zero initialisation and orthogonal separabil-
ity; both are crucial for the result to hold. Near-zero initialisation grants neurons high directional
mobility early in training, allowing them to cluster close to the canonical directions. Orthogonal
separability ensures that the canonical directions are ‘easy to find’ by local descent. In prior work,
which considered linear networks, this role is fulfilled by linear separability. The reason we need a
stronger condition is that ReLU updates are more local compared to linear updates: a linear neuron
takes into account all examples in the training set, whereas a ReLU neuron updates only on exam-
ples in its positive half-plane (its active examples). ReLU neurons therefore easily get stuck in a
variety of directions, unless the data is highly structured.
4 Proof sketch
In the analysis, we distinguish between two phases of training. The first phase takes place close to
the origin, kθk ≈ 0. In this phase, while neurons move little in the absolute sense, they converge in
direction to certain regions of the weight space called extremal sectors.
4.1	Convergence to extremal sectors
(All definitions and results in this subsection are by Maennel et al. (2018). We need them later on.)
Sectors are regions in weight space corresponding to different activation patterns. They are impor-
tant for understanding neuron dynamics: roughly speaking, neurons in the same sector move in the
same direction.
Definition 1 (Sector). The sector associated to a sequence of signs σ ∈ {-1, 0, 1}n is the region in
input space defined as
Sσ ,	{w	∈	Rd |	sign w|xi	=	σi,	i ∈	[n]}.	(11)
We may also refer to the sign sequence σ itself as a sector.
Some sectors are attractors early in training, i.e. neurons tend to converge to them. Such attracting
sectors are called extremal sectors. To give a formal definition, we first introduce the function
G : Sd-1 →R,
n
G(W)，— X'i(0) ∙ ρ(wlXi).	(12)
i=1
Intuitively, (normalised) neurons early in training behave as if they were locally optimising G, they
therefore tend to cluster around the local optima of G. We formally define extremal sectors as sectors
containing these local optima.
Definition 2 (Extremal directions and sectors). We say that W ∈ Sd-1 is a positive extremal di-
rection, if it is a strict local maximum of G. We say that W is a negative extremal direction if it is
a strict local minimum of G. A sector is called (positive/negative) extremal, if it contains a (posi-
tive/negative) extremal direction.
The following lemma (Maennel et al., 2018, Lemma 5) shows that all neurons either turn off, i.e.
become deactivated for all training examples and stop updating, or converge to extremal sectors.
Lemma 1. Let a two-layer ReLU network fθ be balanced at initialisation and trained by gradient
flow. Assume that the loss derivative 'i is Lipschitz continuous. Then, for almost all datasets and
almost all initialisations with λ small enough, there exists a time T such that each neuron satisfies
one of these three conditions:
•	Wj(T) ∈ Sσ where σ ≤ 0 and so Wj remains constant for t ≥ T, or
•	aj(T) > 0 and Wj(T) ∈ Sσ where σ is a positive extremal sector, or
•	aj(T) < 0 and Wj(T) ∈ Sσ where σ is a negative extremal sector.
4
Published as a conference paper at ICLR 2021
4.2	Orthogonal separability: Two absorbing extremal sectors
Lemma 1 shows that by the end of the early phase of training, neurons have converged to extremal
sectors. Although eq. (12) shows that the number of extremal sectors depends only on the data (i.e.
is independent of model expressivity), it is a priori unclear how many extremal sectors there are for
a given dataset, or what happens once neurons have converged to extremal sectors. We now answer
both of these questions for orthogonally separable datasets.
First, we claim that for orthogonally separable datasets, there are only two extremal sectors, one
corresponding to the positive data subset and one corresponding to the negative data subset. That is,
by converging to an extremal sector, neurons ‘choose’ whether to activate for positive examples or
for negative examples. They thus naturally form two groups of similar neurons.
Lemma 2. In the setting of Theorem 1, there is exactly one positive extremal direction and exactly
one negative extremal direction. The positive extremal sector σ+ is given by
σ+ {0,
if yj = 1,
if yj = -1 and xj|xi < 0 for some i with yi = 1,
if yj = -1 and xj|xi = 0 for all i with yi = 1,
and the negative extremal sector σ- is given by
σ-=(-0,
if yj = -1,
if yj = 1 and xj|xi < 0 for some i with yi = -1,
if yj = 1 and xj|xi = 0 for all i with yi = -1.
(13)
(14)
Second, we show that once a neuron reaches an extremal sector, it remains in the sector forever, i.e.
its activation pattern remains fixed for the rest of training.
Lemma 3. Assume the setting of Theorem 1. If at time T the neuron (aj, wj) satisfies aj (T) > 0
and wj(T) ∈ Sσ, where σ is the positive extremal sector (eq. (13)), then for t ≥ T, wj (t) ∈ Sσ.
The same holds if aj (T) < 0 and σ is the negative extremal sector (eq. (14)).
4.3	Proof of Theorem 1
Once neurons enter their respective absorbing sectors, the second phase of training begins. In this
phase, the network’s activation patterns are fixed: some neurons are active and update on the posi-
tive examples, while the others are active and update on the negative examples. The network thus
behaves like an ensemble of independent linear subnetworks trained on subsets of the data. Once
this happens, it becomes possible to apply existing results for linear networks; in particular, each
subnetwork converges to its respective max-margin classifier.
We give more details in the proof below.
Proof of Theorem 1. By Lemmas 1 and 2, there exists a time T such that each neuron satisfies either
•	wj (T) ∈ Sσ where σ ≤ 0 and wj remains constant for t ≥ T, or
•	aj (T) > 0 and wj (T) ∈ Sσ+, or
•	aj (T) < 0 and wj (T) ∈ Sσ- ,
where σ+ , σ- are the unique positive and negative extremal sectors given by eqs. (13) and (14).
Denote by J0 , J+ , J- , the sets of neurons satisfying the first, the second, and the third condition
respectively. By Lemma 3, if j ∈ J+ then wj (t) ∈ Sσ+ for all t ≥ T , and if j ∈ J- then
wj (t) ∈ Sσ- for t ≥ T. Hence, for t ≥ T, if xi is such that yi = 1 then
fθ(Xi) , E ajP(WjXi) = E ajWjxi∙	(15)
j∈[p]	j∈J+
5
Published as a conference paper at ICLR 2021
Combined with Lemma A.3, this implies that for k ∈ J+,
¾k = -iXι(X+ Mx) ∙wlxi,
∂wk
∂t
- E 'i( Σ aj wlxi
i:yi=1	j∈J+
• ak xi,
(16)
(where we have used that Pwxi = xi for i with yi = 1 due to positive extremality). From eq. (16) it
follows that the evolution of neurons in J+ depends only on positive examples and other neurons in
J+. The neurons behave linearly on the positive data subset, while ignoring the negative subset. The
same argument shows that the evolution of neurons in J- depends only on other neurons in J- and
the negative data subset, on which the neurons act linearly. In other words, from time T onwards the
ReLU network decomposes into a constant part and two independent linear networks, one trained
on the positive data subset and the other trained on the negative data subset.
We can therefore apply existing max-margin convergence results for linear networks to each of the
linear subnetworks. Denote by W| = [W0|, W+| , W|-] the three parts of the weight matrix. Then
by (Ji & Telgarsky, 2019a, Theorems 2.2 and 2.8) and (Ji & Telgarsky, 2020, Theorem 3.1), there
exist vectors u, z, such that
W+(t)
kW+(t)kF
—U w+
→ 0,
F
as t → ∞,
W-(t)
kW- (t)kF
一Zw|
→ 0,
F
as t → ∞.
(17)
(18)
(We allow U, Z ∈ R0 to account for the fact that J+, J- may be empty). We now need to relate
kW+ kF and kW-kF to kWkF. In particular, it will be useful to show that kW+(t)k2F/ logt has
a limit as t → ∞; the same is true for kW-(t)k2F/ logt (by the same argument). If J+ or J-
is empty, this is trivially true and the limit is 0. Otherwise, consider the learning of the positive
linear subnetwork, whose objective is effectively '+(θ) := Pi：ya=1 'i(fθ(xi)). By (Ji & TeIgarsky,
2019a, Theorem 2.2), we know that'+ (θ(t)) → 0 as t → ∞. Following (LyU & Li, 2020, Definition
A.3), define
ʌ gQog1"+(B))
γ(θ) = --------2--
2∣W+kF
(19)
where g(q) := - log (exp(exp(-q)) - 1) for the cross-entropy loss. Then
kW+(t)kF = g(log1∕'+(θ(t))) = — log(exp('+(θ(t)))- 1)
log t	2γ(t) log t	2γ(t) log t	.
Using the Taylor expansion exp(u) = 1 + Θ(u) for u → 0 and (Lyu & Li, 2020, Corollary A.11),
we obtain
kW+(t)kF = - logθ('+(θ(t))) = logΘ(t log t) =	1	(Θ(1) + loglog t + 1
log t	25(t) log t	2γ(t) log t	25(t) V	log t
(21)
By (LyU & Li, 2020, Theorem A.7:1), Y is increasing in t and hence converges; it follows that
kW+(t)k2F ∕ logt has a limit. By (Lyu & Li, 2020, Corollary A.11), kW+(t)k2F = Θ(log t),
implying that the limit is finite and strictly positive. We will denote it by ν+ and the analogous
quantity for W-by ν-.
We now return to the main thread of the proof. We analyse the convergence of W(t)∕ kW(t)kF by
analysing W0∕ kW(t)kF, W+(t)∕ kW(t)kF and W-(t)∕ kW(t)kF in turn. Since kW(t)k2F =
kW0k2F + kW+(t)k2F + kW-(t)k2F,
lim
t→∞
∣W(t)kF
log t
ν+ + ν-.
(22)
Now observe that with probability at least 1 — 1∕2p over the random initialisation, ν+ + ν-> 0 (or
equivalently, J+ ∪ J- = 0). To prove this, let xi+ be any training example with yi+ = 1 and let Xi-
6
Published as a conference paper at ICLR 2021
be any training example with yi- = -1. Then by Lemma B.1, if a neuron (aj, wj) is initialised
such that aj(0) > 0 and wj(0)|xi+ > 0 then for t ≥ 0, wj(t)|xi+ > 0. This holds in particular
at time T. The neuron j thus cannot be in J0 nor J-, implying j ∈ J+. Similarly, if the neuron is
initialised such that aj(0) < 0 and wj(0)|xi- > 0, then j ∈ J-. The probability that one of the
two initialisations occurs for a single neuron j is 1/2, as Pwj [ wj (0)|x > 0 ] = 1/2 for any fixed
x. Hence, the probability that j ∈ J0 is at most 1 - 1/2 = 1/2, and the probability that [p] ⊆ J0 is
at most 1/2p.
It follows that with probability at least 1 - 1/2p,
	W0 ∣W(t)∣∣F →0,	as →∞.	(23)
Also, by eqs. (17) and (22),
W+(t)= kW(t)kF = and similarly	W+ ⑴	kw+(t)kF /√logɪ →	√ν+	uw∣	(24) kW+(t)kF ∙ kW(t)kF/√0gi	√ν++ν-	+,	( ) W≡F →√√+ν=zw-.	(25)
For j ∈ J+ and t ≥ T we moreover know that if yi = 1 then wj(t)|xi > 0 because wj (t) ∈ Sσ+ .
As the same property holds for w+, it follows that Uj ≥ 0. By a similar argument, Zj ≥ 0.
Combining the last three equations then proves eq. (8).
As for eq. (9), we know by Lemma A.4 that aj (t) = sj kwj (t)k for some sj ∈ {±1}, implying
ka(t)k = kW(t)kF. Hence, forj ∈ J+,
	aj(t) = SjkWj(t)k → s.∣∣u.w∣ Il	(26) ka(t)k = ∣w(t)∣∣F → SjIIUjw+∣∣	(26)
by eq. (24), where uj ≥ 0. For j ∈ J+ we also know that aj (t) ≥ 0, so sj = 1 and
jk→ UjkW+k.
By a similar argument, we obtain that for j ∈ J- ,
j⅛→-zjkw-k.
Finally, for j ∈ J0, aj (t) is constant and so
aj ⑴ → 0
ka(t)k	.
(27)
(28)
(29)
□
5	Experiments
In this section, we first verify that the theoretical result (Theorem 1) is predictive of experimental
outcomes, even when some technical assumptions are violated. Second, we present evidence that a
similar result may hold for deeper networks as well, although this goes beyond Theorem 1.
5.1	Two-layer networks
To see how well the theory holds up, we train a two-layer ReLU network with 100 neurons on a syn-
thetic orthogonally separable dataset consisting of 500 examples in R20 . The dataset is constructed
from an iid Gaussian dataset by filtering, to ensure orthogonal separability and w+ 6≈ -w- (for
visualisation purposes). Specifically, let z := [1, -1, . . . , 1, -1]. A Gaussian-sampled point x is
included with label +1 if it lies in the first orthant and x|z ≥ 0, included with label -1 if it lies in
the orthant opposite to the first and x|z ≥ 0, and discarded otherwise.
7
Published as a conference paper at ICLR 2021
We train by stochastic gradient descent with batch size 50 and a learning rate of 0.1 for 500 epochs.
At initialisation, we multiply all weights by 0.05. This reflects a setting where both key assumptions
of Theorem 1 - orthogonal separability and small initialisation - hold, while the other assumptions
are relaxed to approach real-life practice.
Figure 1 shows the results. Figure 1a shows the top 10 singular values of the first-layer weight
matrix W ∈ R100×20 after training. We see that despite its size, the matrix has rank only two: all
singular values except the first two are effectively zero. This is exactly as predicted by the theorem.
Furthermore, when we project the neurons on the positive-variance dimensions (Figure 1b), we see
that they align along two main directions. To see how well these directions align with the predicted
max-margin directions, we compute the correlation (normalised inner product) of each neuron with
its respective max-margin direction. Figure 1c shows the histogram of these correlations. We see that
the correlation is generally high, above 0.9 for most neurons. Overall we find very good agreement
with theory.
a) Top 10 sing.values of W
b) Projected neurons
c) Correlation with max-margin
Figure 1: a) The 10 largest singular values of the first-layer weight matrix W after training. Each dot
represents one singular value. b) Neurons (rows ofW) projected on the top two singular dimensions.
Orange (or blue) dots represent neurons with aj > 0 (or aj < 0). c) Histogram of correlations
between each neuron and its respective max-margin direction. (There are 100 neurons in total).
5.2	Deeper networks
We now explore the behaviour of deeper networks on orthogonally separable data. We train a resid-
ual network rather than a fully-connected one. The reason for this is that fully-connected networks
with small initialisation are hard to train: early in training, the gradients are vanishingly small but
then grow very quickly. We therefore found setting a numerically stable learning rate rather delicate.
We consider a residual network fθ : Rd → R parameterised by θ , {W1, . . . , WL}, of the form
fθ1 (x) = W1x,
fθl (x) = fθl-1(x) + Wlρ(fθl-1(x)),	forl ∈ [2,L-1],	(30)
fθ(x) =WLρ(fθL-1(x)),
where p is the network’s width, and W1 ∈ Rp×d, Wl ∈ Rp×p and WL ∈ R1×p are its weights.
We train such a four-layer residual net with width 100 on the same dataset and using the same
optimiser and hyper-parameters as in Section 5.1. Figure 2 shows the results. The results are very
similar to what we observe for two-layer nets: the weight matrices are all rank two (Figure 2a-c),
and the weight matrices’ rows align in two main directions (Figure 2d-f). It is unclear what these
directions are for the intermediate layers of the network, but for the first layer, we conjecture it is
again the max-margin directions, as suggested by Figure 2g.
6	Related work
There is a lot of prior work on the implicit bias of gradient descent for various linear models. For
logistic regression, Soudry et al. (2018) show that assuming an exponentially-tailed loss and lin-
early separable data, the normalised weight vector converges to the max-margin direction. Ji &
Telgarsky (2019b) extend this result to non-separable data, Nacson et al. (2019) extend it to super-
polynomially-tailed losses, and Gunasekar et al. (2018a) considers different optimisation algorithms.
For deep linear networks, Ji & Telgarsky (2019a) show that the end-to-end weight matrix converges
8
Published as a conference paper at ICLR 2021
1
a) Top 10 sing.values OfWI
b) Top 10 sing.values OfW2
c) Top 10 sing.values OfW3
f) Projected rows of W3
*∙ “•
V
O
d) Projected rows OfWI
e) Projected rows of W2
g) Correlation. with, max-margin
Figure 2: a-c) The 10 largest singular values of the first-, second- and third-layer weight matrix
Wl after training. Each dot represents one singular value. d-f) Neurons (rows of Wl) projected on
the respective top two singular dimensions. g) Histogram of correlations between each first-layer
neuron and the closest max-margin direction. (There are 100 neurons in total).
to the max-margin solution and consecutive weight matrices align. Gunasekar et al. (2018b) consider
linear convolutional nets and prove convergence to a predictor related to the '2〃 bridge penalty.
A few papers have started addressing the implicit bias problem for nonlinear (homogeneous or
ReLU) networks. The problem is much harder and hence requires stronger assumptions. Lyu &
Li (2020) and Ji & Telgarsky (2020) assume that at some point during training, the network attains
perfect classification accuracy. Training from this point onward, Ji & Telgarsky (2020) show that the
network parameters converge in direction. Lyu & Li (2020) show that this direction is a critical KKT
point of the (nonlinear) max-margin problem. A complementary approach is taken by Maennel et al.
(2018) who analyse the very early phase of training, when the weights are close to the origin. For
two-layer networks, they show convergence of neurons to extremal sectors. Our work can be seen
as a first step towards bridging the very early and the very late phase of training.
Zooming out a bit, there is also work motivated by similar questions, but taking a different approach.
For example, Li & Liang (2018) show that two-layer ReLU nets trained on structured data converge
to a solution that generalises well. Like ours, their analysis requires that the network’s activation pat-
terns change little, but they achieve it by containing training in the neighbourhood of the (relatively
large) initialisation (this is the standard lazy training argument Chizat et al. (2019)). In contrast, we
initialise much closer to zero, allowing the neurons to move more. Another related paper is Chizat
& Bach (2020). Using a mean-field analysis, the authors show that infinite-width two-layer ReLU
nets converge to max-margin classifiers in a certain non-Hilbertian function space.
7	Conclusion
In this work, we prove that two-layer ReLU nets trained by gradient flow on orthogonally separa-
ble data converge to a combination of the positive and the negative max-margin classifier. To our
knowledge, this is the first result characterising the inductive bias of training neural networks with
ReLU nonlinearities, that does not require infinite width or huge overparameterisation.
The proof rests on a distinction between two phases of learning: an early phase, in which neurons
specialise, and a late phase, in which the network’s activation pattern is fixed and hence it behaves
like an ensemble of linear subnetworks. This approach enables us to understand nonlinear ReLU
networks in terms of the much better understood linear networks. Our hope is that a similar strategy
will prove fruitful in the context of deeper networks and more complicated datasets as well.
9
Published as a conference paper at ICLR 2021
References
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations
(ICLR), 2019.
Lenalc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Computational Learning Theory (COLT), 2020.
Lenalc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in supervised differentiable
programming. In Conference on Neural Information Processing Systems (NeurIPS), 2019.
Frank H Clarke. Generalized gradients and applications. Transactions of the American Mathemati-
cal Society, 205:247-262,1975.
Frank H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and
control theory, volume 178. Springer Science & Business Media, 2008.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient
method converges on tame functions. Foundations of computational mathematics, 20(1):119-
154, 2020.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learing (ICML), 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient de-
scent on linear convolutional networks. In Conference on Neural Information Processing Systems
(NeurIPS), 2018b.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning.
Springer, 2008.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations (ICLR), 2019a.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Computational Learning Theory (COLT), 2019b.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In
arXiv:2006.06657, 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Conference on Neural Information Processing Systems (NeurIPS),
2018.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations (ICLR), 2020.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes ReLU network
features. In arXiv:1803.08367, 2018.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In Conference on
Uncertainty in Artificial Intelligence (AISTATS), 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research (JMLR),
19(1):2822-2878, 2018.
Vladimir Naumovich Vapnik. Statistical learning theory. John Wiley & Sons, 1998.
10
Published as a conference paper at ICLR 2021
A Basic lemmas
This section collects a few lemmas useful for proofs. We assume the same setting and no-
tation as Sections 2 and 4. In addition, we denote by Pw the orthogonal projection onto
span {xi| w|xi = 0}⊥, and by g : Rd → Rd,
n
g(w)，- X 'i(0) ∙ 1{wlXi > 0} PwXi.	(31)
i=1
A.1 Lemmas about sectors
The following lemma gives a necessary condition for a vector to be an extremal direction.
Lemma A.1. If w ∈ Sd-1 is an extremal direction, then g(w) = Cw for some constant C.
Proof. Let W ∈ Sd-1 be a positive extremal direction (the negative case is analogous), and let
W ∈ S&. A sector is called open if σi = 0 for all i ∈ [n]. Denote by A(<σ) the set of all open sectors
adjacent to σ^,
A(S) := < σ ∈ {士1}n max ⑸一σ/ ≤ 1 >.	(32)
i∈[n]
Since W is a local maximum of G and G is sector-wise linear, W maximises G when constrained to
(the closure of) any adjacent sector, i.e. for any σ ∈ A(S),
W = arg max G(w), subject to ∣∣wk2 = 1,
w	(33)
σiWlXi ≥ 0 for all i ∈ [n].
For w in the feasible region, G can be treated as a linear function with VG(w) = g(wσ) where wσ
is any vector such that Wσ ∈ Sσ . Hence, the necessary first-order KKT conditions for the problem
(33) are
n
g(Wσ) = CW - X λiOiXi,	(34)
i=1
where λ% ≥ 0 for all i, but λi = 0 requires that the corresponding constraint is tight, σ%WTXi = 0.
It follows that PWλiσiXi = 0. Multiplying eq. (34) from the left by PW therefore yields
n
CW = PW g(wσ) = - X'i(0) ∙ 1{σi = 1} PWXi.	(35)
i=1
By adjacency, σi = σ% whenever σ% ∈ {±1}, so they can differ only when d% = 0, i.e. when
PWXi = 0. It follows that
n
CW = - X 'i(0) ∙ 1{σi = 1} PWXi= g(W).	(36)
i=1
□
The following lemma describes the local behaviour of the function G (defined in eq. (12)).
Lemma A.2. For W ∈ Sd-1 and v ∈ Rd, there exists max > 0 such that for ∈ [0, max],
G(P^¾) = P^⅛ (G(W) - e X 'i(0)1{(w + "X。> 0} v|X) .	(37)
Proof. Let g be defined as in eq. (31); then
W + v	1
Gl τ--~~n ) = τ—-~~n(w + ev)Tg(W + ev).	(38)
∣W + v∣	∣W + v∣
11
Published as a conference paper at ICLR 2021
We now analyse w|g(w + v) and v|g(w + v) separately, starting with the former. Denote
Ii := 1{(w + v)|xi > 0}. Then g(w + v) can be written as
nn	n
g(w+ev) = - χ 'i(0)∙I∕I0 PwXi-X'i(0)∙I∏1-I0) PwXi-X 'i(0)∙If (Pw+~-Pw)xi∙
i=1	i=1	i=1
(39)
Define
EmaX ：= m max e, subject to: sign {(w + Ev)lXi} sign {wlXi} ≥ 0 ∀i.	(40)
For ∈ [0, max], Ii0 = 1 implies Ii = 1, so the first term in eq. (47) equals g(w). Regarding the
second term, Ii(1 - Ii0) is nonzero only if Ii = 1, Ii0 = 0. For E ∈ [0, EmaX], this can only happen
if (w + Ev)|Xi > 0 and w|Xi = 0. In this scenario however, PwXi = 0, so the second term
in eq. (47) is zero. Regarding the third term, as long as E ∈ [0, EmaX], (w + Ev)|Xi = 0 implies
w|Xi = 0, so
w ∈ span {Xj | w|Xj = 0}⊥ ⊆ span {Xj | (w + Ev)| Xj = 0}⊥	(41)
and w| (Pw+v - Pw) = w| - w| = 0|. It follows that
w|g(w + Ev) = w|g(w) = G(w).	(42)
Turning to Ev|g(w + Ev), we have that
n
EvTg(w + Ev) = -EvT X'i(0) ∙ 1{(w + Ev)TXi > 0} Pw+~Xi,	(43)
i=1
where
EvTPw+v = (w + Ev)TPw+v - wTPw+v = (w + Ev)T - wT = EvT .	(44)
Plugging eq. (42) and eq. (43) into eq. (38) yields the result.	口
A.2 Training dynamics
In the following lemma, we prove a formula for the evolution of the parameters of a two-layer
network trained by gradient flow (eq. (4)). The formula has appeared in Maennel et al. (2018)
before (but without a proof).
Lemma A.3. Assume that the training inputs with the zero vector {Xi}i ∪ {0} are in general po-
sition.4 Then a two-layer ReLU network trained by gradient flow on (X, y) satisfies for all j ∈ [p]
and almost all t ≥ 0,
da
∂t
dwj
∂t
n
-X 'i⑴∙ P(WIXi),
i=1
n
-X'i(t) l{w∣Xi > 0} ajPwjXi.
i=1
(45)
(46)
Proof. Fix θ, and denote by Σθ ∈ {-1, 0, 1}p×n the activation matrix for fθ, Σθ[j, i] , sign wjT Xi.
Then for any sequence θk → θ such that {V'(θk)} exists and has a limit,
lim	Σθk ∈ Σ∈ {±1}p×n Σ[j, i] =Σθ[j,i]ifΣθ[j,i] 6=0 .
(47)
Conversely, for any Σ in the set above, there exists a sequence θk → θ such that limk→∞ Σθk = Σ.
To see this, observe that each wj can be approached separately. Let A be the matrix whose rows
are formed by those Xi for which wjTXi = 0. Then by the general position of inputs, A is a wide
full-rank matrix and Awj = 0. It follows that for any , Aw = has a solution, which can be
chosen convergent to wj as → 0.
4That is, no k of these points lie on a (k - 2)-dimensional hyperplane, for all k ≥ 2.
12
Published as a conference paper at ICLR 2021
We deduce that
∂'(θ(t)) = Conv {g(Σ) | Σ[j,i] ∈ {±1}, Σ[j,i] = ∑θ(t) [j,i] if ∑θ(t)[j,i] = 0},
where we define g(Σ) coordinate-wise as
n
g(£)[aj] ：= X'i⑴ 1{£[j,i] = 1} w∣χi,
i=1
n
g(£)[wj]：= X'i⑴ 1{£[j,i] = 1} ajχi.
i=1
(48)
(49)
Since the value ofg(Σ)[aj] is independent of Σ, this proves eq. (45).
The proof of eq. (46) is slightly more complicated, as we need to pin down a single member of
∂'(θ(t)). To do that, We recall a result by Davis et al. (2020), Who show that for a large class of
deep learning scenarios (which includes ours), the objective ` admits a chain rule, i.e.
'0(t)
for almost all t ≥ 0,
(50)
Where the right-hand side above should be interpreted as the only element of the set
{hl∂θ∕∂t | h ∈ ∂'(θ(t))}. For t such that both eq. (4) and eq. (50) hold,
0= (∂'(θ(t))- ∂'(θ(t)谓)	(51)
implying that
∂θ
∈S ∈ span {∂'(θ(t)) — ∂'(θ(t)} .	(52)
Suppose hi, h2 satisfy both eq. (4) and eq. (52) (taking the role of ∂θ∕∂t). Then
hi — h2 ∈ (∂'(θ(t)) — ∂'(θ(t))) ∩ span {∂'(θ(t)) — ∂'(θ(t)}⊥ = {0}.	(53)
It follows that ∂θ∕∂t is the unique member of both span {∂'(θ(t)) — ∂'(θ(t)}⊥ and —∂'(θ(t)).
By eqs. (48) and (49),
span {∂'(θ(t)) — ∂'(θ(t)} ⊇ span {ξj | WjXi = 0},	(54)
where ξij [wj] = χi and all other elements of ξij are zero. Since
∂θ	⊥
∂ ∈ span {∂'(θ(t)) — ∂'(θ(t)产 ⊆ span {ξj | WjXi = 0},
we obtain that for all (i,j) with w∣Xi = 0, ∂Wj∕∂t ⊥ Xi. In other words,
∂W	∂W	n
~∂Γ = PWj ~∂Γ ∈ Conv 1 — X 'i⑴ 1{£[j,i] = 1} ajpwjxi 〉,
∂t	∂t Σ
i=i
(55)
(56)
where the inclusion follows from ∂θ∕∂t ∈ —∂'(θ(t)) and eqs. (48) and (49). Now observe that by
definition, PWj Xi = 0 for all (i, j ) with Σθ [j, i] = 0, hence the set in eq. (56) is a singleton whose
only element equals eq. (46).	□
The following lemma shows that a balanced two-layer network remains balanced and neurons keep
their signs.
Lemma A.4. Ifa two-layer neural network is balanced at initialisation and trained by gradient flow
with a loss whose derivative is bounded, then for t ≥ 0,
aj⑴=Signaj(O) ∙ kwj⑴k.	(57)
13
Published as a conference paper at ICLR 2021
Proof. By Lemma A.3, for almost all t ≥ 0,
dkwjk = - X'i(t) 1{w∣χi > 0} ajWjXi = daj,	(58)
i=1
i.e. aj and wj grow equally fast. Since |aj(0)| = kwj(0)k at initialisation, |aj (t)| = kwj (t)k
throughout training.
Next denote by B, V › 0 some scalars such that ∣'i(u)∣ ≤ B for all i ∈ [n] and U ∈ R, and
IlXiIl ≤ V for all i ∈ [n]. Then ∣∂a2∕∂t∣ ≤ nBaj2V, or equivalently Idlogaj∕∂t∖ ≤ nBV. It
follows that aj2 (t) lies between aj2(0) exp(-nBVt) and aj2 (0) exp(nBVt), and hence aj cannot
cross zero in finite time, proving eq. (57).
B Proofs of main results
Lemma 2. In the setting of Theorem 1, there is exactly one positive extremal direction and exactly
one negative extremal direction. The positive extremal sector σ+ is given by
if yj = 1,
if yj = -1 and Xj|Xi < 0 for some i with yi = 1,
if yj = -1 and Xj|Xi = 0 for all i with yi = 1,
and the negative extremal sector σ- is given by
σ-=(-01
if yj = -1,
if yj = 1 and Xj|Xi < 0 for some i with yi = -1,
if yj = 1 and Xj|Xi = 0 for all i with yi = -1.
(13)
(14)
Proof. We will prove the positive case; the negative case follows by inverting all labels. Because
G is a continuous function on a compact domain, it has a maximum. At least one maximum must
moreover be strict, or otherwise G would have to be constant. This shows that a positive extremal
direction exists; we now show there is no more than one such direction.
By Lemma A.1, there cannot be more than one extremal direction per sector; it therefore suffices
to show that no sector except one, σ+, admits a positive extremal direction. We will show that if
w ∈ Sd-1 lies in any sector other than σ+, then w is not positive extremal; in particular we show
that G(w) can be locally increased.
Let σ 6= σ+ and let w ∈ Sσ ∩ Sd-1 . By Lemma A.2, for any v ∈ Rd there exists max
that for ∈ (0, max],
(w + Ev、_ G(w) + Ea
IkW + ev" — kw + Evk ,
where
n
a := — ^X'i(0) 1{(w + EV)TXi > 0}vτXi.
i=1
> 0 such
(59)
(60)
We now analyse the different possible realisations of σ, and for each we find v ∈ Rd such that
(G(w) + Ea)/Iw + EvI > G(w) for small E.
Suppose first that σj = —1 for some example with yj = 1, or that σj = 1 for some example with
yj = —1. Then set v := yj Xj /IXj I. By orthogonal separability, we have that a ≥ 0. Also,
σj , sign wτXj = —yj implies wτv < 0, therefore Iw + EvI < IwI = 1 for E small enough. It
follows that (G(w) + Ea)/Iw + EvI > G(w).
Next suppose that σj = 0 for some example with yj = 1, and set v := Xj /IXj I. Then a > 0,
because each term in eq. (60) is non-negative by orthogonal separability, and the term corresponding
to i = j is strictly positive:
-'j(O) 1{(W + EV)TXj > 0} VTXj = -'j(O) 1{EkXjk > 0}kxjk > 0.
(61)
14
Published as a conference paper at ICLR 2021
From σj ，signWTXj = 0 it further follows that ∣∣w + Evk = √Γ+^e2. Hence, (G(W) +
ea)∕kw + Evk = (G(w) + eα)∕(1 + O(e2)), which strictly exceeds G(w) for E small enough.
We have thus shown that if σ is positive extremal, then necessarily σi = 1 for all examples with
yi = 1, and σi ∈ {0, -1} for examples with yi = -1. Suppose now that σj = 0 for an example
with yj = -1 that satisfies Xj|Xk < 0 for some k with yk = 1. Taking v := -Xj /kXj k will
make α strictly positive, as the term corresponding to i = k in eq. (60) will be strictly positive (this
term’s indicator equals 1, as we know from the above that σk = 1). Like in the previous paragraph,
kw + Evk = 1 + O(E2), which suffices to show G(w) is locally submaximal.
Finally, let Xj be such that yj = -1 and Xj|Xi = 0 for all i with yi = 1, and suppose that σj = -1.
With v := PwXj/kPwXj k (we know that PwXj 6= 0 because w|Xj 6= 0 by σj = -1), we have
α
—
kPwXj k
n
X 'i(0) 1{(w + Ev)TXi > 0} x|PwXi.
i=1
(62)
1
We claim that each term in eq. (62) is zero: For terms with σi = -1, the indicator
1{(w + Ev)TXi > 0} is zero. For terms with σi = 0, PwXi = 0. (Also notice that such terms
necessarily satisfy yi = -1 and XiTXk = 0 for all k with yk = 1, which we will need shortly.)
Lastly, for terms with σi = 1, we know yi = 1, and hence XiTXl = 0 for all l with σl = 0. In other
words, Xi ⊥ span {Xl | wTXl = 0}, implying PwXi = Xi. In the context of eq. (62), we obtain
XjTPwXi = XjTXi = 0, concluding the proof that α = 0. Since σj = -1, kw + Evk < kwk = 1 for
small enough e, and (G(w) + Eα)∕∣w + Evk > G(w). We have thus ruled out all sectors except
σ+, proving that for orthogonally separable datasets there is a unique positive extremal sector. □
Lemma 3. Assume the setting of Theorem 1. If at time T the neuron (aj, wj) satisfies aj(T) > 0
and wj(T) ∈ Sσ, where σ is the positive extremal sector (eq. (13)), then for t ≥ T, wj (t) ∈ Sσ.
The same holds if aj(T) < 0 and σ is the negative extremal sector (eq. (14)).
Proof. We omit the neuron index, and only prove the positive case; the negative case is analogous.
Denote σ(t) := sign (XTw(t)). We proceed by contradiction. Suppose there exists a time T1 > T
such that σ(T1) 6= σ(T). Wlog, take T1 such that σ(t) is constant on (T, T1) and denote this
constant sector σ; by continuity σk = σk(T) if σk(T) = 0.
Now consider σk (T) = 0. By the gradient flow differential inclusion, for almost all t ∈ (T, T1),
-wQxk- ∈ coσnV--X `i(t) 1{σi = 1} axixk ∣,
(63)
where each σ0 in the definition of the convex hull satisfies σi0 = σi (T) if σi (T) 6= 0, implying
{i |	σi	= 1} ⊆	{i	|	σi(T)=	1}	∪	{i	|	σi(T)=o}	(64)
= {i | yi = 1} ∪ {i | yi = -1 and XiTXj = 0 for all j with yj = 1}.
Denote the two sets in the last expression I+ and I0, and consider the gradient corresponding to
some σ0 in eq. (63). The gradient terms corresponding to i ∈ I+ are zero (because k ∈ I0 and so
XiTX- = 0) and the terms corresponding to i ∈ I0 (if there are any) are negative. The total gradient
for σ0 is therefore non-positive, which is preserved under taking convex hulls, and so we obtain
∂∂t w|x- ≤ 0. It follows that σ- = 1.
By Lemma A.3, for almost all t∈ (T, T1) and any k ∈ [n],
Tn
vQtx - = - X `i(t) 1{σi = 1} ax|pw Xk,
i=1
(65)
where 1{σi = 1} = 1{yi = 1} as we have shown above. Observe that for Xi with y% = 1, we have
PwXi = Xi . This is because Pw projects onto
span {Xi | σi = 0}⊥ ⊇ span {Xi | σ%(T) = 0}⊥	(66)
and Xi lies in the right-hand side by the definition of positive extremal sector (eq. (13)). Therefore
Tn
-t k = - E'i⑴ 1{yi = 1} aX|Xk.	(67)
i=1
15
Published as a conference paper at ICLR 2021
One can easily check that if σk (T) = 1 then ∂ wlXk > 0, if σk (T) = -1 then ∂ wlXk < 0, and
if σk (T) = 0 then ∂w|Xk = 0. It follows that σ(Tι) = σ(T), which is a contradiction. □
Lemma B.1. Assume the setting of Theorem 1. Ifat time T the neuron (aj, wj) satisfies aj(T) > 0
and wj (T)|Xk > 0 for some k ∈ [n] with yk = 1, then for t ≥ T, wj (t)| Xk > 0. The same holds
if instead aj(T) < 0 and yk = -1.
Proof. We omit the neuron index, and only prove the positive case; the negative case is analogous.
We will show that for almost all t ∈ (T, ∞), ∂wlXk/∂t ≥ 0. By the gradient flow differential
inclusion, for almost all t ∈ (T, ∞),
∂wlXk
∂t
∈ coσn v{-X `i⑴ 1{σi = 1} aχiχk ∣.
(68)
Fix any σ0 and consider the summand corresponding to example i. If y% = 1, then -'i(t) > 0 and
x|xk > 0, so the summand is non-negative. If yi = -1, then -'i(t) < 0 and x|xk ≤ 0, so the
summand is again non-negative. It follows that the sum is non-negative irrespective of σ0, hence
∂wlXk/∂t ≥ 0.	□
Corollary 1. Under the conditions of Theorem 1, there exist constants u, z ≥ 0 such that
fθ(t)(X)
kθ(t)k2
→ uρ(w+ x) — zρ(w-x),
as t → ∞.
(10)
Proof. By Lemma A.4, kθk2
∣a∣2 + ∣W∣2F = 2∣a∣2 = 2 ∣W∣2F. Then for any x ∈ Rd,
2fθ(t)(x) = a(t)τ	( W(t)x )
1θwF =所 PI ∣w(t)∣F)'
(69)
Denote a := limt→∞ a(t)/ka(t)k. Then by Theorem 1, as t → ∞,
2∕θ(t) (χ) → aιρ(uw∣χ + ZWT χ)
kθ(t)k2	,+	)
=alp(uw+ x) + alp(zw-x)
=a|u p(w+x) + a|z p(w-x),
where in eq. (71) we used the fact that for all i ∈ [p], either ui = 0 or zi
used u, z ≥ 0. Finally, since a = u∣∣w+k - z∣∣w-k, we have
a|u = ∣∣w+kulu ≥ 0,
a|z = -∣w-∣zlz ≤ 0,
(70)
(71)
(72)
0, and in eq. (72) we
which completes the proof.
(73)
(74)
□
C Relationship to nonlinear max-margin
Lemma C.1. Let (X, y) be an orthogonally separable dataset, let w+, w- be defined as in eqs. (6)
and (7) and let
||
W := uw+ + zw- ,
a := l∣w+ku - kw-kz,
(75)
(76)
for some u, z ∈ Rp+ such that ui = 0 or zi = 0 for all i ∈ [p]. Also let u, z be normalised such
that ∣∣u∣ = ∣∣w+k-1/2 and ∣∣z∣ = ∣∣w-k-1/2. Then θ，{W, a} is a KKT point of the following
constrained optimisation problem:
min1 ∣∣θ∣2,	St	yifθ(Xi) ≥ 1, i ∈ [n].	(77)
16
Published as a conference paper at ICLR 2021
Proof. By standard linear max-margin considerations (e.g. Hastie et al. (2008, Section 4.5.2)), we
know that
w+ =	αixi,
i:yi=1
w- =	αixi,
i:yi=-1
(78)
for some αi ≥ 0 such that αi = 0 if xi is a non-support vector. It follows by orthogonal separability
that for i with yi = 1,
w+| xi > 0,	yiw-| xi ≤ 0,	(79)
and for i with yi = -1,
w+| xi ≤ 0,	yiw-| xi > 0;	(80)
we will need these properties shortly.
Let us now turn to checking the KKT status of θ wrt. eq. (77). We start by showing that θ is feasible.
Let xi be such that yi = 1; then
			
yifθ(Xi)=	=a| ρ(Wχi)	(81)
	=a| ρ(uw+ Xi + zw-Xi)	(82)
	=a| u w[ xi	(83)
	kw+kkuk2w+|Xi	(84)
	w+| Xi ≥ 1,	(85)
where the last inequality follows from the definition of w+, eq. (6). Similarly, if xi is such that
yi = -1, then
yifθ(χi) = -alzw-χi
= kw-kkzk2w|-xi
= w-| xi ≥ 1.
(86)
(87)
(88)
This shows that θ is feasible.
Next, we show that θ is a KKT point, i.e. we show that there exist λ1, . . . , λn ≥ 0 such that
1.	for all i ∈ [n], λ% (yifj(xi) - 1)=0, and
2.	θ ∈ Pn=ι λiyi∂θfθ(x∕
where ∂θfθ(x) denotes the Clarke subdifferential of fθ (x) wrt. θ, evaluated at θ. Specifically, We
show that the choice
λi = ["kw+k, if yi = 1,ι	(89)
[αi∕∣∣w-∣∣, if y = -1,
satisfies both conditions above.
As for the first condition, observe that if i is a non-support example, then λi = αi = 0 and the
condition holds. If i is a support example, then by eqs. (85) and (88), yifθ(xi) = 1 and the condition
holds as well.
As for the second condition, denote
gi(θ) ：= [Iθ(xi)ax|； P(WXi)],	(90)
17
Published as a conference paper at ICLR 2021
where Iθ (x) = diag [1{Wxi > 0}] ∈ Rp×p is the diagonal matrix whose (i, i)-element is one if
|
Wix > 0 and zero otherwise. It holds that gi(θ) ∈ ∂θfg(xi) and
n
X λiyigi (θ) = X η-^i-π- [diag [1{u > 0}] axl;
i=1	i:yi=1 kw+k
ρ(uw+xi + ZW-Xi)]
αi
U---U diag [1{z > 0}] axi; ρ(uw+Xi + zw-Xi)
kw- k
-
i:yi=-1
X T≡⅛ [kw+kuX|； uw+xi] - X ^α⅛ [-kw-kZx|； ZW-χi]
i:yi=1 kW+k	i:yi=-1 kW-k
π-[7]kw+kuw+ ； uw+w+] - 77—-π-HIW-kzw-; zw-W-]
kw+k	kw- k
uw+| + zw-| ; ukw+k - zkw- k
This proves the second condition, and shows that θ is a KKT point of eq. (77).	□
D Experiments on real data
In this section we explore the applicability of our result to real-world datasets and architectures
(which lie outside the scope formally covered by our assumptions). We experiment on the MNIST
dataset subsetted to two classes, the digit 0 and the digit 1.
We train a network consisting of six convolutional layers followed by two fully-connected layers.
We view the six convolutional layers as a ‘feature extractor’ and the two fully-connected layers
as a two-layer fully-connected network of the kind we analyse in this paper. The details of the
architecture are given in Table 1. We train the network by Adam with the binary cross-entropy loss
and a batch size of 128. We train for 50 epochs. Prior to training, we multiply the weights of the
fully-connected layers by 0.05, to approximate the small-norm initialisation assumed by theory.
Layer		Type	
-1-	conv(32, 3, 1, 1)
2	conv(32, 3, 1, 1)
3	conv(32, 5, 2, 2)
4	conv(64, 3, 1, 1)
5	conv(64, 3, 1, 1)
6	conv(64, 5, 2, 2)
7	fc(3136, 128)
8	fc(128, 1)
Table 1: Architecture of the studied network. By conv(n, k, s, p) we denote a convolutional
layer with n kernels of size k × k with stride s, where the input to the layer is padded by p rows or
columns on each margin. By fc(n, o) we denote a fully-connected layer whose input dimension
is n and whose output dimension is o.
We conduct two sub-studies. First, we demonstrate that the network learns orthogonally separable
representations all by itself, in the course of training. This is shown in Figure 3. The first subplot
shows three distributions: The blue distribution is the distribution of Xi|Xj where Xi is sampled
from class 0 and Xj is sampled from class 1. The orange (or green) distribution is the distribution of
Xi|Xj where both Xi, Xj are sampled from class 0 (or class 1). The other subplots show analogous
inner-product distributions for the intermediate representations or learned features of the data, i.e.
fθ(Xi)Tfθ(xj) instead of xiXj.
What we see is that the network learns representations such that examples of the same class are
more similar to each other than examples of different classes - the orange and green distributions are
generally more to the right compared to the blue distribution. Moreover, higher-layer representations
18
Published as a conference paper at ICLR 2021
Original data
1st layer features
2nd layer features
3rd layer features
4th layer features
5th layer features
6th layer features
7th layer features
Figure 3: Distributions of feature similarity, where examples are sampled from the specified classes.
Specifically, The l-th subplot shows the distribution of fθ(Xi)Tfθ(Xj) for Xi, Xj sampled from dif-
ferent classes (blue) or both from class 0 (orange) or both from class 1 (green).
are generally more strongly separated - as We move UP the layer hierarchy, the orange and green
distributions keep shifting rightward, whereas the blue distribution shifts leftward. Remarkably, the
7th layer representations are orthogonally separable.
In the second sub-study, we explore properties of the weight matrix learnt by the first linear layer of
the network, in analogy to the first-layer weight matrix in a two-layer net. Figure 4a shows the top
ten singular values of the weight matrix W7 ∈ R128×3136 . We see that despite its size, it has very
few (perhaps five or ten) significantly non-zero singular values. This is similar to what we observed
for synthetic data in Section 5, though the separation between small and large singular values is
less crisp and there are more than two non-zero values. Figure 4b shows the rows (neurons) of
W7 projected onto the top two singular dimensions (note that unlike in Section 5, the projection
is lossy). The neurons roughly form three clusters: a mixed cluster close to the origin and two
clusters corresponding to positive and negative outer-layer weights. Compared to our observations
from Section 5, there is less variation in the neurons’ norms, leading to them forming clusters rather
than rays. This deviation from the theoretical prediction could be due to a number reasons, e.g. the
use of biases, convolutional layers, or the large dimensionality of the layer. We leave a detailed
investigation of this question to future work.
a) Top 10 sing.values OfW7 CY b) Projected neurons
Figure 4: a) The 10 largest singular values of the first linear layer’s weight matrix W7 after training.
Each dot represents one singular value. b) Neurons (rows of W7) projected on the top two singular
dimensions. Orange (or blue) dots represent neurons with W8[j] > 0 (or W8[j] < 0).
19