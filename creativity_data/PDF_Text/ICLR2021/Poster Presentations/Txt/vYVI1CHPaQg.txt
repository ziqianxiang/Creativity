Published as a conference paper at ICLR 2021
A Better Alternative to Error Feedback for
Communication-Efficient Distributed Learn-
ING
Samuel Horvath and Peter Richtarik
KAUST
Thuwal, Saudi Arabia
{samuel.horvath, peter.richtarik}@kaust.edu.sa
Ab stract
Modern large-scale machine learning applications require stochastic optimization
algorithms to be implemented on distributed compute systems. A key bottleneck
of such systems is the communication overhead for exchanging information (e.g.,
stochastic gradients) across the workers. Among the many techniques proposed
to remedy this issue, one of the most successful is the framework of compressed
communication with error feedback (EF). EF remains the only known technique
that can deal with the error induced by contractive compressors which are not
unbiased, such as Top-K or PowerSGD. In this paper, we propose a new and
theoretically and practically better alternative to EF for dealing with contractive
compressors. In particular, we propose a construction which can transform any
contractive compressor into an induced unbiased compressor. Following this
transformation, existing methods able to work with unbiased compressors can be
applied. We show that our approach leads to vast improvements over EF, including
reduced memory requirements, better communication complexity guarantees and
fewer assumptions. We further extend our results to federated learning with partial
participation following an arbitrary distribution over the nodes, and demonstrate
the benefits thereof. We perform several numerical experiments which validate our
theoretical findings.
1 Introduction
We consider distributed optimization problems of the form
n
mRd f (X)= 1 Pfi(X),
x∈	i=1
(1)
where X ∈ Rd represents the weights of a statistical model we wish to train, n is the number of nodes,
and fi : Rd → R is a smooth differentiable loss function composed of data stored on worker i. In
a classical distributed machine learning scenario, fi(x) := EZ〜Di [fζ (x)] is the expected loss of
model X with respect to the local data distribution Di of the form, and fζ : Rd → R is the loss on the
single data point ζ. This definition allows for different distributions D1, . . . , Dn on each node, which
means that the functions f1, . . . , fn can have different minimizers. This framework covers Stochastic
Optimization when either n = 1 or all Di are identical, Empirical Risk Minimization (ERM), when
fi(x) can be expressed as a finite average, i.e, fi(x) = m1- pm=i1 fij (x) for some fj : Rd → R, and
Federated Learning (FL) (Kairouz et al., 2019) where each node represents a client.
Communication Bottleneck. In distributed training, model updates (or gradient vectors) have to be
exchanged in each iteration. Due to the size of the communicated messages for commonly considered
deep models (Alistarh et al., 2016), this represents significant bottleneck of the whole optimization
procedure. To reduce the amount of data that has to be transmitted, several strategies were proposed.
One of the most popular strategies is to incorporate local steps and communicated updates every
few iterations only (Stich, 2019a; Lin et al., 2018a; Stich & Karimireddy, 2020; Karimireddy et al.,
1
Published as a conference paper at ICLR 2021
2019a; Khaled et al., 2020). Unfortunately, despite their practical success, local methods are poorly
understood and their theoretical foundations are currently lacking. Almost all existing error guarantees
are dominated by a simple baseline, minibatch SGD (Woodworth et al., 2020).
In this work, we focus on another popular approach: gradient compression. In this approach, instead
of transmitting the full dimensional (gradient) vector g ∈ Rd , one transmits a compressed vector
C(g), where C : Rd → Rd is a (possibly random) operator chosen such that C(g) can be represented
using fewer bits, for instance by using limited bit representation (quantization) or by enforcing
sparsity. A particularly popular class of quantization operators is based on random dithering (Goodall,
1951; Roberts, 1962); see (Alistarh et al., 2016; Wen et al., 2017; Zhang et al., 2017; Horvgth
et al., 2019a; Ramezani-Kebrya et al., 2019). Much sparser vectors can be obtained by random
sparsification techniques that randomly mask the input vectors and only preserve a constant number
of coordinates (Wangni et al., 2018; KOnecny & Richtarik, 2018; Stich et al., 2018; Mishchenko et al.,
2019b; Vogels et al., 2019). There is also a line of work (Horvgth et al., 2019a; Basu et al., 2019) in
which a combination of sparsification and quantization was proposed to obtain a more aggressive
effect. We will not further distinguish between sparsification and quantization approaches, and refer
to all of them as compression operators hereafter.
Considering both practice and theory, compression operators can be split into two groups: biased and
unbiased. For the unbiased compressors, C(g) is required to be an unbiased estimator of the update
g. Once this requirement is lifted, extra tricks are necessary for Distributed Compressed Stochastic
Gradient Descent (DCSGD) (Alistarh et al., 2016; 2018; Khirirat et al., 2018) employing such a
compressor to work, even if the full gradient is computed by each node. Indeed, the naive approach
can lead to exponential divergence (Beznosikov et al., 2020), and Error Feedback (EF) (Seide et al.,
2014; Karimireddy et al., 2019b) is the only known mechanism able to remedy the situation.
Contributions. Our contributions can be summarized as follows:
•	Induced Compressor. When used within the stabilizing EF framework, biased compressors (e.g.,
Top-K) can often achieve superior performance when compared to their unbiased counterparts (e.g.,
Rand-K). This is often attributed to their low variance. However, despite ample research in this area,
EF remains the only known mechanism that allows the use of these powerful biased compressors.
Our key contribution is the development of a simple but remarkably effective alternative—and this is
the only alternative we know of—which we argue leads to better and more versatile methods both in
theory and practice. In particular, we propose a general construction that can transform any biased
compressor, such as Top-K, into an unbiased one for which we coin the name induced compressor
(Section 3). Instead of using the desired biased compressor within EF, our proposal is to instead
use the induced compressor within an appropriately chosen existing method designed for unbiased
compressors, such as distributed compressed SGD (DCSGD) (Khirirat et al., 2018), variance reduced
DCSGD (DIANA) (Mishchenko et al., 2019a) or accelerated DIANA (ADIANA) (Li et al., 2020).
While EF can bee seen as a version of DCSGD which can work with biased compressors, variance
reduced nor accelerated variants of EF were not known at the time of writing this paper.
•	Better Theory for DCSGD. As a secondary contribution, we provide a new and tighter theoretical
analysis of DCSGD under weaker assumptions. If f is μ-quasi convex (not necessarily convex)
and local functions fi are (L, σ2)-smooth (weaker version of L-smoothness with strong growth
condition), We obtain the rate O QnLr0 exp [-4μTL] + (δn-ιμD+δσ /n) , where δn = 1 + δ-1 and
δ ≥ 1 is the parameter which bounds the second moment of the compression operator, and T is the
number of iterations. This rate has linearly decreasing dependence on the number of nodes n, which
is strictly better than the best-known rate for DCSGD with EF, whose convergence does not improve
as the number of nodes increases, which is one of the main disadvantages of using EF. Moreover,
EF requires extra assumptions. In addition, while the best-known rates for EF (Karimireddy et al.,
2019b; Beznosikov et al., 2020) are expressed in terms of functional values, our theory guarantees
convergence in both iterates and functional values. Another practical implication of our findings is
the reduction of the memory requirements by half; this is because in DCSGD one does not need to
store the error vector.
•	Partial Participation. We further extend our results to obtain the first convergence guarantee for
partial participation with arbitrary distributions over nodes, which plays a key role in Federated
Learning (FL).
2
Published as a conference paper at ICLR 2021
Algorithm 1 DCSGD	Algorithm 2 DCSGD with Error Feedback
1	: Input: {ηk}kT=0 > 0, X0	1	Input: {ηk}T=o > 0, X0, e0 = 0 ∀i ∈ [n]
2	: for k = 0, 1, . . . T do	2	for k = 0, 1, . . . T do
3	: Parallel: Worker side	3	Parallel: Worker side
4	: for i = 1, . . . , n do	4	for i = 1, . . . , n do
5	:	obtain gik	5	obtain gik
6	:	send ∆ik = Ck (gik) to master	6	send ∆ik = Ck (ηkgik + eik) to master
7	:	[no need to keep track of errors]	7	eik+1 = ηkgik + eik -∆ik
8	: end for	8	end for
9	: Master side	9	Master side
10	aggregate ∆k = 1 Pn=I ∆k	10	aggregate ∆k = 1 Pi=I ∆k
11	: broadcast ∆k to each worker	11	broadcast ∆k to each worker
12	: Parallel: Worker side	12	Parallel: Worker side
13	: for i = 1, . . . , n do	13	for i = 1, . . . , n do
14	:	Xk+1 = Xk - ηk∆k	14	,, Xk+1 = Xk - ∆k
15	: end for	15	end for
16	: end for	16	end for
•	Experimental Validation. Finally, we provide an experimental evaluation on an array of classifi-
cation tasks with CIFAR10 dataset corroborating our theoretical findings.
2 Error Feedback is not a Good Idea when Using Unbiased
Compressors
In this section we first introduce the notions of unbiased and general compression operators, and
then compare Distributed Compressed SGD (DCSGD) without (Algorithm 1) and with (Algorithm 2)
Error Feedback.
Unbiased vs General Compression Operators. We start with the definition of unbiased and general
compression operators (Cordonnier, 2018; Stich et al., 2018; Koloskova et al., 2019).
Definition 1 (Unbiased Compression Operator). A randomized mapping C : Rd → Rd is an unbiased
compression operator (unbiased compressor) if there exists δ ≥ 1 such that
E [C(x)] = x, E kC(x)k2 ≤ δ kxk2 ,	∀x ∈ Rd.	(2)
If this holds, we will for simplicity write C ∈ U(δ).
Definition 2 (General Compression Operator). A (possibly) randomized mapping C : Rd → Rd is a
general compression operator (general compressor) if there exists λ > 0 and δ ≥ 1 such that
E [∣∣λC(X)-Xk2i≤ (1- 1) kxk2,	∀x ∈ Rd.	⑶
If this holds, we will for simplicity write C ∈ C(δ).
The following lemma provides a link between these notions (see, e.g. Beznosikov et al. (2020)).
Lemma 1. If C ∈ U(δ) ,then (3) holds with λ = 1, i.e., C ∈ C(δ). That is, U(δ) ⊂ C(δ).
Note that the opposite inclusion to that established in the above lemma does not hold. For instance,
the Top-K operator belongs to C(δ), but does not belong to U(δ). In the next section we develop a
procedure for transforming any mapping C : Rd → Rd (and in particular, any general compressor)
into a closely related induced unbiased compressor.
Distributed SGD with vs without Error Feedback. In the rest of this section, we compare the
convergence rates for DCSGD (Algorithm 1) and DCSGD with EF (Algorithm 2). We do this
comparison under standard assumptions (Karimi et al., 2016; Bottou et al., 2018; Necoara et al., 2019;
Gower et al., 2019; Stich, 2019b; Stich & Karimireddy, 2020), listed next.
First, we assume throughout that f has a unique minimizer X?, and let f? = f(X?) > -∞.
3
Published as a conference paper at ICLR 2021
Assumption 1 (μ-quasi convexity). f is μ-quasi convex, i.e.,
f? ≥ f(x) + Nf(X) x? — Xi + μμ ∣∣x? — xk2,	NX ∈ Rd.	(4)
Assumption 2 (unbiased gradient oracle). The stochastic gradient used in Algorithms 1 and 2 satisfies
E [gk | xk] = Nfi(Xk)	∀i,k.	(5)
Note that this assumption implies E [§ pn=1 gf | χk] = Nf(Xk).
Assumption 3 ((L, σ2)-expected smoothness). Function f is (L, σ2)-smooth if there exist constants
L > 0 and σ2 ≥ 0 such that ∀i ∈ [n] and ∀Xk ∈ Rd
Ehgik2i ≤ 2L(fi(Xk) — fi?) + σ2,	(6)
E h∣∣n1 Pi=Igk∣∣2i ≤ 2L(f(Xk) — f?) + σ2∕n,	⑺
where fi? is the minimum functional value of fi and [n] = {1, 2, . . . , n}.
This assumption generalizes standard smoothness and boundedness of variance assumptions. For
more details and discussion, see the works of Gower et al. (2019); Stich (2019b). Equipped with
these assumptions, we are ready to proceed with the convergence theory.
Theorem 2 (Convergence of DCSGD). Consider the DCSGD algorithm with n ≥ 1 nodes. Let
Assumptions 1-3 hold and C ∈ U(δ), where δn = δ-1 + 1. Let D := 2L Pn=I (fi(X?) — f?). Then
there exist stepsizes ηk ≤ 2δ1L and weights wk ≥ 0 such that for all T ≥ 1 we have
E [f(XT) — f?] + μE h∣∣XT — X*∣∣2] ≤ 64δnLr0exp [—毋]+ 36两-1『”,
where r0 = ∣∣x0 — x*∣∣2, WT = PT=O wk, and Prob(XT = Xk) = wk∕wT.
If δ = 1 (no compression), Theorem 2 recovers the optimal rate of Distributed SGD (Stich, 2019b). If
δ > 1, there is an extra term (δn — 1)D in the convergence rate, which appears due to heterogenity of
data (Pin=1Nfi(X?) = 0, but Pin=1C(Nfi(X?)) 6= 0 in general). In addition, the rate is negatively
affected by extra variance due to presence of compression which leads to L → δj and σ2∕n → δσ2∕n.
Next we compare our rate to the best-known result for Error Feedback (Stich & Karimireddy,
2020) (n = 1), (Beznosikov et al., 2020) (n ≥ 1) used with C ∈ U(δ) ⊂ C(δ)
E [f (XT) — f?] = O (δLr0 exp [— μT] + δD产)
One can note several disadvantages of Error Feedback (Alg. 2) with respect to plain DCSGD (Alg. 1).
The first major drawback is that the effect of compression δ is not reduced with an increasing number
of nodes. Another disadvantage is that Theorem 2 implies convergence for both the functional values
and the last iterate, rather than for functional values only as it is the case for EF. On top of that, our rate
of DCSGD as captured by Theorem 2 does not contain any hidden polylogarithmic factor comparing
to EF. Another practical supremacy of DCSGD is that there is no need to store an extra vector for the
error, which reduces the storage costs by a factor of two, making Algorithm 1 a viable choice for
Deep Learning models with millions of parameters. Finally, one does not need to assume standard
L-smoothness in order to prove convergence in Theorem 2, while, one the other hand, L-smoothness
is an important building block for proving convergence for general compressors due to the presence
of bias (Stich & Karimireddy, 2020; Beznosikov et al., 2020). The only term in which EF might
outperform plain DCSGD is O(σ2∕μT) for which the corresponding term is O(δσ2∕nμT). This is due
to the fact that EF compensates for the error, while standard compression introduces extra variance.
Note that this is not major issue as it is reasonable to assume δ∕n = O(1) or, in addition, σ2 = 0 if
weak growth condition holds (Vaswani et al., 2019), which is quite standard assumption, or one can
remove effect of σ2 by either computing full gradient locally or by incorporating variance reduction
such as SVRG (Johnson & Zhang, 2013). In Section 4, we also discuss the way how to remove the
effect of D in Theorem 2. Putting all together, this suggests that standard DCSGD (Algorithm 1) is
strongly preferable, in theory, to DCSGD with Error Feedback (Algorithm 2) for C ∈ U(δ).
4
Published as a conference paper at ICLR 2021
3	Induced Compressor: Fixing Bias with Error-Compression
In the previous section, we showed that compressed DCSGD is theoretically preferable to DCSGD
with Error Feedback for C ∈ U(δ). Unfortunately, C(δ) 6⊂ U(δ), an example being the Top-K
compressor (Alistarh et al., 2018; Stich et al., 2018). This compressors belongs to C(Kd), but does
not belong to U(δ) for any δ. On the other hand, multiple unbiased alternatives to Top-K have been
proposed in the literature, including gradient sparsification (Wangni et al., 2018) and adaptive random
sparsification (Beznosikov et al., 2020).
Induced Compressor. We now propose a general mechanism for constructing an unbiased compres-
sor C ∈ U from any biased compressor C1 ∈ C. We shall argue that it is preferable to use this induced
compressor within DCSGD, in both theory and practice, to using the original biased compressor C1
within DCSGD + Error Feedback.
Theorem 3. For C1 ∈ C(δ1) with λ = 1, choose C2 ∈ U(δ2) and define the induced compressor via
C(x) := C1(x) + C2(x - C1(x)).
The induced compression operator satisfies C ∈ U(δ) with δ = δ2 (1 一 1∕δι) + 1∕δ1.
To get some intuition about this procedure, recall the structure used in Error Feedback. The gradient
estimator is first compressed with C1 (g) and the error e = g 一 C1 (g) is stored in memory and used to
modify the gradient in the next iteration. In our proposed approach, instead of storing the error e, we
compress it with an unbiased compressor C2 (which can be seen as a parameter allowing flexibility in
the design of the induced compressor) and communicate both of these compressed vectors. Note that
this procedure results in extra variance as we do not work with the exact error, but with its unbiased
estimate only. On the other hand, there is no bias and error accumulation that one needs to correct for.
In addition, due to our construction, at least the same amount of information is sent to the master
as in the case of plain C1(g): indeed, we send both C1(g) and C2(e). The drawback of this is the
necessity to send more bits. However, Theorem 3 provides the freedom in generating the induced
compressor through the choice of the unbiased compressor C2. In theory, it makes sense to choose
C2 with similar compression factor to the compressor C1 we are transforming as this way the total
number of communicated bits per iteration is preserved, up to the factor of two.
Remark: The rtopk ,k (x, y) operator proposed by Elibol et al. (2020) can be seen as a special case
of our induced compressor with x = y, C1 = Top-k1 and C2 = Rand-k2 .
Benefits of Induced Compressor. In the light of the results in Section 2, we argue that one should
always prefer unbiased compressors to biased ones as long as their variances δ and communication
complexities are the same, e.g., Rand-K over Top-K. In practice, biased/greedy compressors are in
some settings observed to perform better due to their lower empirical variance (Beznosikov et al.,
2020). These considerations give a practical significance to Theorem 3 as we demonstrate on the
following example. Let us consider two compressors: one biased C1 ∈ C(δ1) and one unbiased
C2 ∈ U(δ2 ), such that δ1 = δ2 = δ, having identical communication complexity, e.g., Top-K
and Rand-K. The induced compressor C(x) := C1(x) + C2(x 一 C1(x)) belongs to U(δ3), where
δ3 = δ 一(1 一 δ) <δ. While the size of the transmitted message is doubled, one can use Algorithm 1
since C is unbiased, which provides better convergence guarantees than Algorithm 2. Based on the
construction of the induced compressor, one might expect that we need extra memory as “the error”
e = g 一 C1(g) needs to be stored, but during computation only. This is not an issue as compressors
for DNNs are always applied layer-wise (Dutta et al., 2019), and hence the size of the extra memory
is negligible. It does not help EF, as the error needs to be stored at any time for each layer.
4	Extensions
We now develop several extensions of Algorithm 1 relevant to distributed optimization in general, and
to Federated Learning in particular. This is all possible due to the simplicity of our approach. Note
that in the case of Error Feedback, these extensions have either not been obtained yet, or similarly to
Section 2, the results are worse when compared to our derived bounds for unbiased compressors.
Partial Participation with Arbitrary Distribution over Nodes. In this section, we extend our
results to a variant of DCSGD utilizing partial participation, which is of key relevance to Federated
5
Published as a conference paper at ICLR 2021
Figure 1: Comparison of Top-1 (+ EF) and NU Rand-1 on Example 1 from Beznosikov et al. (2020).
Learning. In this framework, only a subset of all nodes communicates to the master node in each
communication round. Such framework was analyzed before, but only for the case of uniform
subsampling (Sattler et al., 2019; Reisizadeh et al., 2020). In our work, we consider a more general
partial participation framework: we assume that the subset of participating clients is determined
by a fixed but otherwise arbitrary random set-valued mapping S (a “sampling”) with values in 2[n] ,
where [n] = {1, 2, . . . , n}. To the best of our knowledge, this is the first partial participation result
for FL where an arbitrary distribution over the nodes is considered. On the other hand, this is not
the first work which makes use of the arbitrary sampling paradigm; this was used before in other
contexts, e.g., for obtaining importance sampling guarantees for coordinate descent (Qu et al., 2015),
primal-dual methods (Chambolle et al., 2018), and variance reduction (Horvgth & Richt^rik, 2019).
Note that the sampling S is uniquely defined by assigning probabilities to all 2n subsets of [n]. With
each sampling S we associate a probability matrix P ∈ Rn×n defined by Pij := Prob({i, j} ⊆ S).
The probability vector associated with S is the vector composed of the diagonal entries of P:
p = (p1, . . . ,pn) ∈ Rn, where pi := Prob(i ∈ S). We say that S is proper if pi > 0 for all i. It
is easy to show that b := E [|S|] = Trace (P) = Pin=1 pi, and hence b can be seen as the expected
number of clients participating in each communication round.
There are two algorithmic changes due to this extension: line 4 of Algorithm 1 does not iterate over
every node, only over nodes i ∈ Sk, where Sk 〜S, and the aggregation step in line 9 is adjusted to
lead to an unbiased estimator of the gradient, which gives ∆k = Pi∈sk np∆kk.
To prove convergence, we exploit the following lemma.
Lemma 4 (Lemma 1, Horvgth & Richtgrik (2019)). Let ζ1, ζ2, . . . , ζn be vectors in Rd and let
《：=1 En=I ζi be their average. Let S be a proper SampIing. Then there exists V ∈ Rn such
P - pp>	Diag (p1v1,p2v2, . . . ,pnvn) .	(8)
Moreover, if S 〜S, then
E
i∈S
Zi
nPi
n
≤ n2 P Pi kζik2
(9)
Σ
The following theorem establishes the convergence rate for Algorithm 1 with partial participation.
Theorem 5. Let Assumptions 1-3 hold and C ∈ U(δ), then there exist stepsizes ηk ≤ ^^ and
weights wk ≥ 0 such that
E f(xτ) - f?] + μE h∣∣xτ - x?||2i ≤ 64δsLr0exp [-篇]+ 36@-1)勺尸二” ,
where r0, WT, XT, and D are defined in Theorem 2, as = maxi∈[n]{vi∕pi}, and δs = δaS +nδ-1) + 1.
For the case S = [n] with probability 1, one can show that Lemma 4 holds with v = 0, and hence
we exactly recover the results of Theorem 2. In addition, we can quantify the slowdown factor with
respect to full participation regime (Theorem 2), which is δ maxi∈[n] pi. While in our framework we
assume the distribution S to be fixed, it can be easily extended to several proper distributions Sj ’s or
we can even handle a block-cyclic structure with each block having an arbitrary proper distribution
Sj over the given block j combining our analysis with the results of Eichner et al. (2019).
6
Published as a conference paper at ICLR 2021
ss°u-e」l Sso- U-£1
100:
0	10	20	30	40	50
epochs
epochs
Figure 2: Algorithm 1 vs. Algorithm 2 on CIFAR10 with ResNet18 (bottom), VGG11 (top) and
TernGrad as a compression.
0	10	20	30	40	50
epochs
Obtaining Linear Convergence. Note that in all the previous theorems, we can only guarantee a
sublinear O(1/T) convergence rate. Linear rate is obtained in the special case when D = 0 and
σ2 = 0. The first condition is satisfied, when fi? = fi (x?) for all i ∈ [n], thus when x? is also
minimizer of every local function fi. Furthermore, the effect od D can be removed using compression
of gradient differences, as pioneered in the DIANA algorithm (Mishchenko et al., 2019a). Note
that σ2 = 0 if weak growth condition holds (Vaswani et al., 2019). Moreover, one can remove
effect of σ2 by either computing full gradients locally or by incorporating variance reduction such as
SVRG (Johnson & Zhang, 2013). It was shown by Horvath et al. (2019b) that both σ2 and D can be
removed for the setting of Theorem 2. These results can be easily extended to partial participation
using our proof technique for Theorem 5. Note that this reduction is not possible for Error Feedback
as the analysis of the DIANA algorithm is heavily dependent on the unbiasedness property. This
points to another advantage of the induced compressor framework introduced in Section 3.
Acceleration. We now comment on the combination of compression and acceleration/momentum.
This setting is very important to consider as essentially all state-of-the-art methods for training
deep learning models, including Adam (Kingma & Ba, 2015; Reddi et al., 2018), rely on the use of
momentum in one form or another. One can treat the unbiased compressed gradient as a stochastic
gradient (Gorbunov et al., 2020) and the theory for momentum SGD (Yang et al., 2016; Gadat
et al., 2018; Loizou & Richtarik, 2017) would be applicable with an extra smoothness assumption.
Moreover, it is possible to remove the variance caused by stochasticity and obtain linear convergence
with an accelerated rate, which leads to the Accelerated DIANA method (Li et al., 2020). Similarly to
our previous discussion, both of these techniques are heavily dependent on the unbiasedness property.
It is an intriguing question, but out of the scope of the paper, to investigate the combined effect of
momentum and Error Feedback and see whether these techniques are compatible theoretically.
5	Experiments
In this section, we compare Algorithms 1 and 2 for several compression operators. If the method
contains “ + EF ”, it means that EF is applied, thus Algorithm 2 is applied. Otherwise, Algorithm 1 is
displayed. To be fair, we always compare methods with the same communication complexity per
iteration. All experimental details can be found in the Appendix.
Failure of DCSGD with biased Top-1. In this experiment, we present example considered
in Beznosikov et al. (2020), which was used as a counterexample to show that some form of
error correction is needed in order for biased compressors to work/provably converge. In addition, we
run experiments on their construction and show that while Error Feedback fixes divergence, it is still
significantly dominated by unbiased non-uniform sparsification(NU Rand-1), which works by only
7
Published as a conference paper at ICLR 2021
SSO-U-e」J_
80-
>70-
∣60-
5 50-
H-
40-
10	20	30	40	50
epochs
SSo-U-
epochs
80-
70-
&60-
§50-
⅛40-
α)
l^ 30-
20-
epochs
Figure 3: Comparison of different sparsification techniques with and without usage of Error Feedback
on CIFAR10 with Resnet18 (top) and VGG11 (bottom). K = 5% * d, for Induced compressor Ci is
Top-K/2 and C2 is Rand-K/2 (Wangni et al.).
keeping one non-zero coordinate sampled with probability equal to |x|/Pid=1 |x|i, where |x| denotes
element-wise absolute value, as can be seen in Figure 1. The details can be found in the Appendix.
Error Feedback for Unbiased Compression Operators. In our second experiment, we compare
the effect of Error Feedback in the case when an unbiased compressor is used. Note that unbiased
compressors are theoretically guaranteed to work both with Algorithm 1 and 2. We can see from
Figure 2 that adding Error Feedback can hurt the performance; we use TernGrad (Wen et al., 2017)
(coincides with QSGD (Alistarh et al., 2016) and natural dithering (Horvgth et al., 2019a) with the
infinity norm and one level) as compressors. This agrees with our theoretical findings. In addition,
for sparsification techniques such as Random Sparsification or Gradient Sparsification (Wangni et al.,
2018), we observed that when sparsity is set to be 10 %, Algorithm 1 converges for all the selected
values of step-sizes, but Algorithm 2 diverges and a smaller step-size needs to be used. This is an
important observation as many practical works (Li et al., 2014; Wei et al., 2015; Aji & Heafield, 2017;
Hsieh et al., 2017; Lin et al., 2018b; Lim et al., 2018) use sparsification techniques mentioned in this
section, but proposed to use EF, while our work shows that using unbiasedness property leads not
only to better convergence but also to memory savings.
Unbiased Alternatives to Biased Compression. In this section, we investigate candidates for
unbiased compressors than can compete with Top-K, one of the most frequently used compressors.
Theoretically, Top-K is not guaranteed to work by itself and might lead to divergence (Beznosikov
et al., 2020) unless Error Feedback is applied. One would usually compare the performance of
Top-K with EF to Rand-K, which keeps K randomly selected coordinates and then scales the output
by d/K to preserve unbiasedness. Rather than naively comparing to Rand-K, we propose to use
more nuanced unbiased approaches. The first one is Gradient Sparsification proposed by Wagni et
al. (Wangni et al., 2018), which we refer to here as Rand-K (Wangni et al.), where the probability
of keeping each coordinate scales with its magnitude and communication budget. As the second
alternative, we propose to use our induced compressor, where C1 is Top-a and unbiased part C2 is
Rand-(K - a) (Wangni et al.) with communication budget K - a. It should be noted that a can be
considered as a hyperparameter to tune. For our experiment, we chose it to be K/2 for simplicity.
Figure 3 suggests that our induced compressor outperforms all of its competitors as can be seen
for both VGG11 and Resnet18. Moreover, induced compressor as well as Rand-K do not require
extra memory to store the error vector. Finally, Top-K without EF suffers a significant decrease in
performance, which stresses the necessity of error correction.
8
Published as a conference paper at ICLR 2021
6 Conclusion
In this paper, we argue that if compressed communication is required for distributed training due to
communication overhead, it is better to use unbiased compressors. We show that this leads to strictly
better convergence guarantees with fewer assumptions. In addition, we propose a new construction
for transforming any compressor into an unbiased one using a compressed EF-like approach. Besides
theoretical superiority, usage of unbiased compressors enjoys lower memory requirements. Our
theoretical findings are corroborated with empirical evaluation.
As a future work we plan to investigate the question of the appropriate choice of the inducing
compressor C. Our preliminary studies show that there is much to be discovered here, both in theory
and in terms of developing further practical guidelines to those already contained in this work. The
question of (theoretically) optimizing for C1 and C2 is difficult, as it necessitates a deeper theoretical
understanding of biased compressors, which is currently missing. An alternative is to impose some
assumptions on the structure of gradients encountered during the iterative process, or to perform an
extensive experimental evaluation on desired tasks to provide guidelines for practitioners.
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent.
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017.
Dan Alistarh, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized quantization for
communication-optimal stochastic gradient descent. arXiv preprint arXiv:1610.02132, 2016.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information
Processing Systems,pp. 5973-5983, 2018.
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed
SGD with quantization, sparsification and local computations. In Advances in Neural Information
Processing Systems, pp. 14668-14679, 2019.
Aleksandr Beznosikov, Samuel Horvath, Peter Richtarik, and Mher Safaryan. On biased compression
for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Antonin Chambolle, Matthias J Ehrhardt, Peter Richtarik, and Carola-Bibiane Schonlieb. Stochastic
primal-dual hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM
Journal on Optimization, 28(4):2783-2808, 2018.
Jean-Baptiste Cordonnier. Convex optimization using sparsified stochastic gradient descent with
memory. Technical report, 2018.
Aritra Dutta, El Houcine Bergou, Ahmed M Abdelmoniem, Chen-Yu Ho, Atal Narayan Sahu, Marco
Canini, and Panos Kalnis. On the discrepancy between the theoretical analysis and practical
implementations of compressed communication for distributed deep learning. arXiv preprint
arXiv:1911.08250, 2019.
Hubert Eichner, Tomer Koren, H Brendan McMahan, Nathan Srebro, and Kunal Talwar. Semi-cyclic
stochastic gradient descent. arXiv preprint arXiv:1904.10120, 2019.
Melih Elibol, Lihua Lei, and Michael I Jordan. Variance reduction with sparse gradients. arXiv
preprint arXiv:2001.09623, 2020.
Sebastien Gadat, Fabien Panloup, Sofiane Saadane, et al. Stochastic heavy ball. Electronic Journal
of Statistics, 12(1):461-529, 2018.
WM Goodall. Television by pulse code modulation. Bell System Technical Journal, 30(1):33-49,
1951.
9
Published as a conference paper at ICLR 2021
EdUard Gorbunov, FiliP Hanzely, and Peter Richt疝ik. A unified theory of sgd: Variance reduction,
sampling, quantization and coordinate descent. In The 23rd International Conference on Artificial
Intelligence and Statistics, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtdrik. SGD: General analysis and improved rates. Proceedings of the 36th International
Conference on Machine Learning, Long Beach, California, 2019.
Benjamin Grimmer. Convergence rates for deterministic and stochastic subgradient methods without
Lipschitz continuity. SIAM Journal on Optimization, 29(2):1350-1365, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Samuel Horvdth and Peter Richtdrik. Nonconvex variance reduced optimization with arbitrary
sampling. Proceedings of the 36th International Conference on Machine Learning, 2019.
Samuel Horvdth, Chen-Yu Ho, L’udovit Horvdth, Atal Narayan Sahu, Marco Canini, and Peter
Richtdrik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988,
2019a.
Samuel Horvdth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtdrik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019b.
Kevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R Ganger, Phillip B
Gibbons, and Onur Mutlu. Gaia: Geo-distributed machine learning approaching LAN speeds. In
14th Symposium on Networked Systems Design and Implementation, pp. 629-647, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojaSieWiCZ condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on-device federated
learning. arXiv preprint arXiv:1910.06378, 2019a.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U Stich, and Martin Jaggi. Error feedback
fixes signSGD and other gradient compression schemes. arXiv preprint arXiv:1901.09847, 2019b.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtdrik. Tighter theory for local SGD on
identical and heterogeneous data. In The 23rd International Conference on Artificial Intelligence
and Statistics (AISTATS 2020), 2020.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning With
compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Published as a
conference paper at the 3rd International Conference for Learning Representations, San Diego,
2015.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms With compressed communication. arXiv preprint arXiv:1902.00340, 2019.
Jakub Konecny and Peter Richtdrik. Randomized distributed mean estimation: Accuracy vs. commu-
nication. Frontiers in Applied Mathematics and Statistics, 4:62, 2018.
10
Published as a conference paper at ICLR 2021
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t)
convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002,
2012.
Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James
Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter
server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}
14),pp. 583-598,2014.
Zhize Li, Dmitry Kovalev, XUn Qian, and Peter Richtdrik. Acceleration for compressed gradient
descent in distributed and federated optimization. arXiv preprint arXiv:2002.11364, 2020.
Hyeontaek Lim, David G Andersen, and Michael Kaminsky. 3LC: Lightweight and effective traffic
compression for distribUted machine learning. arXiv preprint arXiv:1802.07389, 2018.
Tao Lin, Sebastian U Stich, KUmar Kshitij Patel, and Martin Jaggi. Don’t Use large mini-batches, Use
local SGD. arXiv preprint arXiv:1808.07217, 2018a.
YUjUn Lin, Song Han, HUizi Mao, YU Wang, and William J Dally. Deep gradient compression:
RedUcing the commUnication bandwidth for distribUted training. ICLR 2018 - International
Conference on Learning Representations, 2018b.
Nicolas LoizoU and Peter Richtdrik. MomentUm and stochastic momentUm for stochastic gradient,
Newton, proximal point and sUbspace descent methods. arXiv preprint arXiv:1712.09677, 2017.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019a.
Konstantin Mishchenko, Filip Hanzely, and Peter Richtdrik. 99% of parallel optimization is inevitably
a waste of time. arXiv preprint arXiv:1901.09437, 2019b.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, 175(1-2):69-107, 2019.
Zheng Qu, Peter Richtdrik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with
arbitrary sampling. In Advances in Neural Information Processing Systems, pp. 865-873, 2015.
Ali Ramezani-Kebrya, Fartash Faghri, and Daniel M Roy. NUQSGD: Improved communication
efficiency for data-parallel SGD via nonuniform quantization. arXiv preprint arXiv:1908.06077,
2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. ICLR
2018 - International Conference on Learning Representations, 2018.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quan-
tization. In International Conference on Artificial Intelligence and Statistics, pp. 2021-2031,
2020.
Peter Richtdrik and Martin Takdc. Parallel coordinate descent methods for big data optimization.
Mathematical Programming, 156(1-2):433-484, 2016.
Lawrence Roberts. Picture coding using pseudo-random noise. IRE Transactions on Information
Theory, 8(2):145-154, 1962.
Felix Sattler, Simon Wiedemann, Klaus-Robert Muller, and Wojciech Samek. Robust and
communication-efficient federated learning from non-iid data. IEEE transactions on neural
networks and learning systems, 2019.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of
the International Speech Communication Association, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. ICLR 2015 - International Conference on Learning Representations, 2015.
11
Published as a conference paper at ICLR 2021
Sebastian U Stich. Local SGD converges fast and communicates little. ICLR 2019 - International
Conference on Learning Representations, 2019a.
Sebastian U Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint
arXiv:1907.04232, 2019b.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for SGD
with delayed gradients and compressed communication. ICLR 2020 - International Conference on
Learning Representations, 2020.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Advances in Neural Information Processing Systems,pp. 4447-4458, 2018.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 1195-1204, 2019.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient
compression for distributed optimization. In Advances in Neural Information Processing Systems,
pp. 14236-14245, 2019.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1299-1309, 2018.
Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R Ganger, Phillip B Gibbons,
Garth A Gibson, and Eric P Xing. Managed communication and consistency for fast data-parallel
iterative analytics. In Proceedings of the Sixth ACM Symposium on Cloud Computing, pp. 381-394,
2015.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pp. 1509-1519, 2017.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? arXiv
preprint arXiv:2002.07839, 2020.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. Zipml: Training linear
models with end-to-end low precision, and a little bit of deep learning. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 4035-4043. JMLR. org, 2017.
12
Published as a conference paper at ICLR 2021
SSO-U-e」J_
0	10	20	30	40	50
epochs
Sso- u-e」J_
0	10	20	30	40	50
epochs
80-
70-
⅛ 60-
50 50-
出40-
30-
20-
0	10	20	30	40	50
epochs
80-
⅛ 70-
⅛60-
(υ
H
50-
0	10	20	30	40	50
epochs
Figure 4: Algorithm 1 vs. Algorithm 2 on CIFAR10 with ResNet18 (bottom), VGG11 (top) and
TernGrad as a compression.
Appendix
A	Experimental Details
To be fair, we always compare methods with the same communication complexity per iteration.
We report the number of epochs (passes over the dataset) with respect to training loss and testing
accuracy. The test accuracy is obtained by evaluating the best model in terms of validation accuracy.
A validation accuracy is computed based on 10 % randomly selected training data. We tune the
step-size using based on the training loss. For every experiment, we randomly distributed the training
dataset among 8 workers; each worker computes its local gradient-based on its own dataset. We
used a local batch size of 32. All the provided figures display the mean performance with one
standard error over 5 independent runs. For a fair comparison, we use the same random seed for
the compared methods. Our experimental results are based on a Python implementation of all the
methods running in PyTorch. All reported quantities are independent of the system architecture and
network bandwidth.
Dataset and Models. We do an evaluation on CIFAR10 dataset. We consider VGG11 (Simonyan &
Zisserman, 2015) and ResNet18 (He et al., 2016) models and step-sizes 0.1, 0.05 and 0.01.
A.1 Extra Experiments
Momentum. In this extra experiment, we look at the effect of momentum on Algorithm 1 and 2. We
set momentum to 0.9. Similarly to Figure 2, we work with the unbiased compressor, concretely Tern-
Grad (Wen et al., 2017) (coincides with QSGD (Alistarh et al., 2016) and natural dithering (Horvgth
et al., 2019a) with the infinity norm and one level), to see the effect of adding Error Feedback. We
can see from Figure 4 that adding Error Feedback can hurt the performance, which agrees with our
theoretical findings.
B Example 1, Beznosikov et al. (2020)
In this section, we present example considered in Beznosikov et al. (2020), which was used as a
counterexample to show that some form of error correction is needed in order for biased compressors
to work/provably converge. In addition, we run experiments on their construction and show that
while Error Feedback fixes divergence, it is still significantly dominated by unbiased non-uniform
sparsification as can be seen in Figure 1. The construction follows.
13
Published as a conference paper at ICLR 2021
Consider n = d = 3 and define the following smooth and strongly convex quadratic functions
f1 (X) = ha,xi2 + 4 kxk2 ,	f2 (X) = hb,xi2 + 4 kxk2 ,	f3(X) = hc,xi2 + 1 kxk2 ,
where a = (-3, 2, 2), b = (2, -3, 2), c = (2, 2, -3). Then, with the initial point x0 = (t, t, t), t > 0
Vfι(x0) = t (-11, 9, 9),	Vf2(x0) = t (9,-11, 9),	Vf3(x0) = t (9, 9,-11).
Using the Top-4 compressor, we get
C(VfI(X0)) = t (-11,0,0), C(Vf2(x0)) = t (0,-11,0),	C(Vf3(x0)) = t (0,0,-11).
The next iterate of DCGD is
3
x1 = χ0 - 3 XC(Vfi(X0))
i=1
Repeated application gives Xk = (1 + 16η)k x0, which diverges exponentially fast to +∞ since
η > 0.
As a initial point, We use (1,1,1)> in our experiments and We choose step size L, where L is
smoothness parameter of f = 3(fι + f2 + f3). Note that zero vector is the unique minimizer of f.
C Proofs
C.1 Proof of Lemma 1
We follow (2), which holds for C ∈ U(δ).
E	1Ck(X)-X口 = δ.E [∣∣Ck(x)∣∣[ - 21〈E[Ck(x)] ,x)+ |团|2
≤ (δ- 2 + 1)kXk2
=(1-1 )kXk2,
which concludes the proof.
14
Published as a conference paper at ICLR 2021
C.2 Proof of Theorem 2
We use the update of Algorithm 1 to bound the following quantity
E xk+1 - x?2 |xk
=1
∖2
⑺ E XX Ck 心)I
n	i 1
=1
xk
(2)+(5)
≤
∖∖xk -X*∖∖2
-ηk (Vf (Xk),xk - x?)+
(	k)2	n	2 n
号E XIlCk(gk)-gk∖∖ + Xgk	।
xk
(2)
≤
∖xk - x? ∖
(ηk )2 E
n2
=1
- ηk (Vf(xk), xk -x
n∖
=1
?) +
n
=1
=1
k
2

n
n
n


)+( ) ∖∖xk - x*∖∖2 - ηk (Vf (Xk),xk - X?) +
1 n	δσ2
δn(f (Xk) - f?) + (δn - 1)-X(fi(x?) - fi?) + (ηk)2
nn
i=1
(4)	2
≤	(1 - μηk) WXk- x*∖∖2 - 2ηk (- - ηkδnL) (f(xk) - f?) +
(ηk )2 ((δn - I)D + ^n-).
Taking full expectation and ηk ≤ 2jtl , We obtain
E [∖∖Xk+1 - x*∖∖2] ≤ (1 - μηk)E [∖∖Xk - x*∖∖[ - ηkE [f(Xk) - f?] + (ηk)2 f(δn - 1)D + 与
The rest of the analysis is closely related to the one of Stich (2019b). We Would like to point out that
similar results to Stich (2019b) Were also present in (Lacoste-Julien et al., 2012; Stich et al., 2018;
Grimmer, 2019).
We first reWrite the previous inequality to the form
rk+1 ≤ (1 -aηk)rk - ηksk + (ηk)2c,
(10)
where rk = E [∖∖Xk — X*J∖2], sk = E [f (Xk) — f?], a = μ, C = (δn — 1)D + δη2.
We proceed with lemmas that establish a convergence guarantee for every recursion of type (10).
Lemma 6. Let {rk }k≥o, {sk }k≥o be as in (10) for a > 0 and for constant stepsizes ηk ≡ η := d,
∀k ≥ 0. Then it holds for all T ≥ 0:
rT ≤ r0 exp
aT	c
d	ad
Proof. This follows by relaxing (10) using E [f(Xk) - f?] ≥ 0,and unrolling the recursion
T-1
rT ≤ (1 - aη)rT -1 + cγ2 ≤ (1 - aη)T r0 + cη2 X (1 - aη)k ≤ (1 - aη)Tr0 + —.
k=0	a
(11)
□
15
Published as a conference paper at ICLR 2021
Lemma 7. Let {rk }k≥o, {sk }k≥o as in (10) for a > 0 and for decreasing stepsizes ηk := θ(汽+k)
∀k ≥ 0, with parameter K := 2d, and weights wk := (K + k). Then
1T
WT X skwk + arT+1 ≤
k=0
2aκ2r0
T2
2c
+----
+ aT,
where W T := PkT=0 wk.
Proof. We start by re-arranging (10) and multiplying both sides with wk
skwk ≤ w
k(1 - aηk)rk	wkrk+1
ηk
+ cηk wk
—
η
k
k
a(K + k)(K + k - 2)r
—a(κ + k)2rk+1 + C
a
≤ a(κ + k — 1)2rk — a(κ + k)2rk+1 + C ,
a
where the equality follows from the definition of ηk and wk and the inequality from (K + k)(K + k -
2)	= (K + k - 1)2 - 1 ≤ (K + k - 1)2. Again we have a telescoping sum:
1	X SkWk + a(κ + T)2rT +1 ≤ aκ2r0 + C(T + I)
W k=0	W	W	aW
with
•	WT = PT=0 Wk = PT=o(κ + k) = (2κ≡T≡ ≥ 一 ≥ T2,
•	and WT = (2K+?T+1) ≤ 2(K+?1+T) ≤(κ + T)2 for K = 2 ≥ 1.
22	a
By applying these two estimates we conclude the proof.
□
The convergence can be obtained as the combination of these two lemmas.
Lemma 8. Let {rk}k≥o, {sk}k≥o as in (10), a > 0. Then there exists stepsizes ηk ≤ 1 and weighs
Wk ≥ 0, WT := PkT=0 Wk, such that
T
1 k k T+1	aT	36C
WTESW + ar ≤32dr0eχp -互 + aT.
k=0
Proof of Lemma 8. For integer T ≥ 0, we choose stepsizes and weights as follows
ifT ≤ d,
a
if T > - and k < to,
ηk
ηk
a
if T > - and k ≥ to, ηk
a
1
d,
1
d,
2
Wk
Wk
a(K + k - t0)
Wk
(1 - aηk )-(k+l)=(1-d )-(k+1)
0,
= (K + k - t0 )2 ,
for κ = 2d and to = ∣"T2"∣. We will now show that these choices imply the claimed result.
We start with the case T ≤ a. For this case, the choice η = ɪ gives
T
Tj^T ^X sk wk + arT+1 ≤ (1 — aη)( + )----------+ cη
k=o	η
≤ r0 exp [-aη(T + 1)] + Cn
≤ dr0 eχp
aT
~d
C
+ aT.
—
16
Published as a conference paper at ICLR 2021
If T > a, then We obtain from Lemma 6 that
rt0 ≤ r0 exp--------
一 P	2d
c
+ ad.
From Lemma 7 We have for the second half of the iterates:
1T	1T
WT X SkWk + arT +1 = WT X SkWk + arT +1
k=0	k=t0
8aκ2rt0	4c
T2	+ ατ.
NoW We observe that the restart condition rt0 satisfies:
≤
aκ2 rt0
-T2-
ακ2r0 exp (一盼 + κc ≤ 4ar0 exp
T2	+ dT2 - P
aT
2d^
4c
+ ατ,
because T > d. These conclude the proof.
□
Having these general convergence lemmas for the recursion of the form (10), the proof of the theorem
follows directly from Lemmas 6 and 8 with a = μ, C = σ2, d = 2δnL . It is easy to check that
condition ηk ≤ ɪ = 2δ1L is satisfied.
C.3 Proof of Theorem 3
We have to show that our new compression is unbiased and has bounded variance. We start with the
first property with λ = 1.
E [C1(x) + C2(x -	C1(x))] =EC1 [EC2 [C1(x) + C2(x -	C1(x))|C1(x)]]
= EC1 [C1(x) + x-	C1(x)] = x,
where the first equality follows from tower property and the second from unbiasedness of C2. For the
second property, we also use tower property
E hkC1(x)-	x + C2(x-	C1(x))k2i = EC1 hEC2 hkC1(x)-	x + C2(x-	C1(x))k2 |C1(x)ii
≤ (δ2-	1)EC1 hkC1(x)-	xk2i
≤ (δ2 - 1) (1 一 .)kxk2,
where the first and second inequalities follow directly from (2) and (3).
C.4 PROOF OF LEMMA 4 (HORVATH & RICHTARIK, 2019)
For the first part of the claim, it was shown that P - pp> is positive semidefinite (Richtarik & Takac,
2016), thus we can bound P- pp> nDiag P- pp> = Diag (p ◦ v), where vi = n(1- pi),
which implies that (8) holds for this choice of v.
For the second part of the claim, let 1i∈S = 1 if i ∈ S and 1i∈S = 0 otherwise. Likewise, let
1i,j∈S = 1 if i, j ∈ S and 1i,j∈S = 0 otherwise. Note that E [1i∈S] = pi and E [1i,j∈S] = pij. Next,
let US compute the mean of X := Pi∈s npi-:
E[X]
E
E
nζ
X npi 1i∈S
nn
X -L~E[1i∈S] = - X Zi = Z
npi	n
i=1 i	i=1
(12)
17
Published as a conference paper at ICLR 2021
Let A = [aι,...,an] ∈ Rd×n, where ai = Pi, and let e be the vector of all ones in Rn. We now
write the variance of X in a form which will be convenient to establish a bound:
E kX-E[X]k2
E kXk2 -kE[X]k2
E
X npi H
E X n^,j 1ij∈S -1∣^∣∣2
i,j npi npj
j
p Ipi)
npi npj
i,j
=n12 X(Pj- pip) )a>a)
= e>eτ ((P -pp>) ◦ A>A) e.
n2
Since by assumption we have P - pp>	Diag (p ◦ v), we can further bound
n
e> ((P — pp>) ◦ A>A) e ≤ e> (Diag (p ◦ V) ◦ A>A) e = ^XpiVi∣∣ai∣∣2.
i=1
To obtain (9), it remains to combine this with (13).
(13)
C.5 Proof of Theorem 5
Similarly to the proof of Theorem 2, we use the update of Algorithm 1 to bound the following quantity
E ∣∣xk+1 - x? ∣∣2 |xk
(2)+(5)
≤
∣∣-∣∣2-ηk X E [*∑ ɪ Ck(gk ),χk-x*+lχk
∣	∣2
+
E
Σ
i∈Sk
xk
k∣
npiCk(gk)	1
(2)+(5)+(9)
≤
(ηk)2 E
∖
1	1n
X npi Ck (gk)-n X C k (gk)
i∈Sk i	i=1
∣	∣2
¾LEX(F + δ - ι)∣∣gk ∣∣+Xgk	Ixk
n i=1	pi	∣ i=1	∣
2
|xk
+E
n
-X Ck (gk)	∣
i=1
xk
n
2
(4)+(6)+(7)
≤
(1 - μηk) ∣∣χk - χ*∣∣2 - 2ηk (1 - ηkδsL) (f(χk) - f?) +
(ηk)2 ((δs - 1)D + (1 + as) ~n~).
Taking full expectation and ηk ≤ 最L, We obtain
E h∣∣χk+1 - χ*∣∣2i ≤ (1 - μηk)E h∣∣χk - χ*∣∣2i - ηke f (χk) - f?] + (ηk)2 (侬 - 1)D + (1 + as)”
The rest of the analysis is identical to the proof of Theorem 2 with only difference c = (δs - 1)D +
(1+as) δn-.
18