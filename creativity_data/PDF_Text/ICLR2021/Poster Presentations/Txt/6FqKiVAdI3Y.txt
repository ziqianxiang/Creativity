Published as a conference paper at ICLR 2021
DOP: Off-Policy Multi-Agent Decomposed
Policy Gradients
Yihan Wang*, Beining Han*, Tonghan Wang*, Heng Dong, Chongjie Zhang
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
{memoryslices,bouldinghan,tonghanwang1996,drdhii}@gmail.com
chongjie@tsinghua.edu.cn
Ab stract
Multi-agent policy gradient (MAPG) methods recently witness vigorous progress.
However, there is a significant performance discrepancy between MAPG meth-
ods and state-of-the-art multi-agent value-based approaches. In this paper, we
investigate causes that hinder the performance of MAPG algorithms and present a
multi-agent decomposed policy gradient method (DOP). This method introduces the
idea of value function decomposition into the multi-agent actor-critic framework.
Based on this idea, DOP supports efficient off-policy learning and addresses the
issue of centralized-decentralized mismatch and credit assignment in both discrete
and continuous action spaces. We formally show that DOP critics have sufficient
representational capability to guarantee convergence. In addition, empirical evalu-
ations on the StarCraft II micromanagement benchmark and multi-agent particle
environments demonstrate that DOP outperforms both state-of-the-art value-based
and policy-based multi-agent reinforcement learning algorithms. Demonstrative
videos are available at https:// sites.google.com/ view/ dop- mapg/ .
1	Introduction
Cooperative multi-agent reinforcement learning (MARL) has achieved great progress in recent
years (Hughes et al., 2018; Jaques et al., 2019; Vinyals et al., 2019; Zhang et al., 2019; Baker et al.,
2020; Wang et al., 2020c). Advances in valued-based MARL (Sunehag et al., 2018; Rashid et al.,
2018; Son et al., 2019; Wang et al., 2020e) contribute significantly to the progress, achieving state-of-
the-art performance on challenging tasks, such as StarCraft II micromanagement (Samvelyan et al.,
2019). However, these value-based methods present a major challenge for stability and convergence
in multi-agent settings (Wang et al., 2020a), which is further exacerbated in continuous action spaces.
Policy gradient methods hold great promise to resolve these challenges. MADDPG (Lowe et al.,
2017) and COMA (Foerster et al., 2018) are two representative methods that adopt the paradigm
of centralized critic with decentralized actors (CCDA), which not only deals with the issue of non-
stationarity (Foerster et al., 2017; Hernandez-Leal et al., 2017) by conditioning the centralized critic
on global history and actions but also maintains scalable decentralized execution via conditioning
policies on local history. Several subsequent works make improvements to the CCDA framework by
introducing the mechanism of recursive reasoning (Wen et al., 2019) or attention (Iqbal & Sha, 2019).
Despite the progress, most of the multi-agent policy gradient (MAPG) methods do not provide
satisfying performance, e.g., significantly underperforming value-based methods on benchmark
tasks (Samvelyan et al., 2019). In this paper, we analyze this discrepancy and pinpoint three major
issues that hinder the performance of MAPG methods. (1) Current stochastic MAPG methods
do not support off-policy learning, partly because using common off-policy learning techniques is
computationally expensive in multi-agent settings. (2) In the CCDA paradigm, the suboptimality of
one agent’s policy can propagate through the centralized joint critic and negatively affect policy learn-
ing of other agents, causing catastrophic miscoordination, which we call centralized-decentralized
mismatch (CDM). (3) For deterministic MAPG methods, realizing efficient credit assignment (Tumer
et al., 2002; Agogino & Tumer, 2004) with a single global reward signal largely remains challenging.
* Equal Contribution. Listing order is random.
1
Published as a conference paper at ICLR 2021
In this paper, we find that these problems can be addressed by introducing the idea of value decompo-
sition into the multi-agent actor-critic framework and learning a centralized but factorized critic. This
framework decomposes the centralized critic as a weighted linear summation of individual critics that
condition on local actions. This decomposition structure not only enables scalable learning on the
critic, but also brings several benefits. It enables tractable off-policy evaluations of stochastic policies,
attenuates the CDM issues, and also implicitly learns an efficient multi-agent credit assignment.
Based on this decomposition, we develop efficient off-policy multi-agent decomposed policy gradient
methods for both discrete and continuous action spaces.
A drawback of an linearly decomposed critic is its limited representational capacity (Wang et al.,
2020b), which may induce bias in value estimations. However, we show that this bias does not violate
the policy improvement guarantee of policy gradient methods and that using decomposed critics
can largely reduce the variance in policy updates. In this way, a decomposed critic achieves a great
bias-variance trade-off.
We evaluate our methods on both the StarCraft II micromanagement benchmark (Samvelyan et al.,
2019) (discrete action spaces) and multi-agent particle environments (Lowe et al., 2017; Mordatch
& Abbeel, 2018) (continuous action spaces). Empirical results show that DOP is very stable across
different runs and outperforms other MAPG algorithms by a wide margin. Moreover, to our best
knowledge, stochastic DOP provides the first MAPG method that outperforms state-of-the-art valued-
based methods in discrete-action benchmark tasks.
Related works on value decomposition methods. In value-based MARL, value decomposi-
tion (Guestrin et al., 2002b; Castellini et al., 2019) is widely used. These methods learn local
Q-value functions for each agent, which are combined with a learnable mixing function to produce
global action values. In VDN (Sunehag et al., 2018), the mixing function is an arithmetic summa-
tion. QMIX (Rashid et al., 2018; 2020) proposes a non-linear monotonic factorization structure.
QTRAN (Son et al., 2019) and QPLEX (Wang et al., 2020b) further extend the class of value func-
tions that can be represented. NDQ (Wang et al., 2020e) addresses the miscoordination problem by
learning nearly decomposable architectures. A concurrent work (de Witt et al., 2020) finds that a
decomposed centralized critic in QMIX style can improve the performance of MADDPG for learning
in continuous action spaces. In this paper, we study how and why linear value decomposition can
enable efficient policy-based learning in both discrete and continuous action spaces. In Appendix F,
we discuss how DOP is related to recent progress in multi-agent reinforcement learning and provide
detailed comparisons with existing multi-agent policy gradient methods.
2	Background
We consider fully cooperative multi-agent tasks that can be modelled as a Dec-POMDP (Oliehoek
et al., 2016) G=I, S, A, P, R,Ω, O, n, γ), where I is the finite set of agents, Y ∈ [0,1) is the
discount factor, and s ∈ S is the true state of the environment. At each timestep, each agent i
receives an observation o% ∈ Ω drawn according to the observation function O(s,i) and selects
an action ai ∈ A, forming a joint action a ∈ An, leading to a next state s0 according to the
transition function P (s0 |s, a) and a reward r = R(s, a) shared by all agents. Each agent learns a
policy ∏i(ai∖τi; θi), which is parameterized by θ% and conditioned on the local history Ti ∈ T ≡
(Ω X A)*. The joint policy π, with parameters θ = hfi`,…，。丁), induces a joint action-value
function: Q∏ot(τ,a)=Es03,a0g [P∞=0 YtR(st, at)| so=s, ao=a, ∏]. We consider both discrete and
continuous action spaces, for which stochastic and deterministic policies are learned, respectively. To
distinguish deterministic policies, We denote them by μ = hμι,…，μQ.
Multi-Agent Policy Gradients The centralized training with decentralized execution (CTDE)
paradigm (Foerster et al., 2016; Wang et al., 2020d) has recently attracted attention for its abil-
ity to address non-stationarity while maintaining decentralized execution. Learning a centralized
critic with decentralized actors (CCDA) is an efficient approach that exploits the CTDE paradigm.
MADDPG and COMA are two representative examples. MADDPG (Lowe et al., 2017) learns
deterministic policies in continuous action spaces and uses the following gradients to update policies:
g = ET ,a~D〉： R θ μi(Ti)VaiQtOt(T, aXaa=μi(τi) ,	(I)
i
2
Published as a conference paper at ICLR 2021
and D is a replay buffer. COMA (Foerster et al., 2018) updates stochastic policies using the gradients:
g = Eπ
£ Vθjog∏i(ai∣Ti)Aπ(T, a)
(2)
where Aiπ(τ, a) = Qtπot(τ, a) - a0 Qtπot(τ, (a-i, a0i)) is a counterfactual advantage (a-i is the
ai
joint action other than agent i) that deals with the issue of credit assignment and reduces variance.
3	Analysis
In this section, we investigate challenges that limit the performance of state-of-the-art multi-agent
policy gradient methods.
3.1	Off-Policy Learning for Multi-Agent Stochastic Policy Gradients
Efficient stochastic policy learning in single-agent settings relies heavily on using off-policy data (Lil-
licrap et al., 2015; Wang et al., 2016; Fujimoto et al., 2018; Haarnoja et al., 2018), which is not
supported by existing stochastic MAPG methods (Foerster et al., 2018). In the CCDA frame-
work, off-policy policy evaluation—estimating Qtπot from data drawn from behavior policies
β = hβι,...,βni —encounters major challenges. Importance sampling (MeUleaU et al., 2000;
Jie & Abbeel, 2010; Levine & Koltun, 2013) is a simple way to correct for the discrepancy between π
and β, but, it requires computing Qi ；*口), whose variance grows exponentially with the number
of agents in mUlti-agent settings. An alternative is to extend the tree backUp techniqUe (PrecUp et al.,
2000; Munos et al., 2016) to multi-agent settings and use the k-step tree backup update target for
training the critic:
k-1
yTB = Qnrt(T, a) + EYt ∏λπ(aιlτι)	[rt + YEn[Qπot(τt+ι, ∙)] — Qnrt(Tt, at)],⑶
t=0	l=1
where To = T, ao = a. However, the complexity of computing En [Q∏0t(τt+ι, ∙)] is O(∣A∣n), which
becomes intractable when the number of agents is large. Therefore, it is challenging to develop
off-policy stochastic MAPG methods.
3.2	The Centralized-Decentralized Mismatch Issue
In the centralized critic with decentralized actors (CCDA) framework, agents learn individual policies,
∏i(ai∣τi; θi), conditioned on the local observation-action history. However, the gradients for updating
these policies are dependent on the centralized joint critic, Qtnot(T, a) (see Eq. 1 and 2), which
introduces the influence of actions of other agents. Intuitively, gradient updates will move an agent in
the direction that can increase the global Q value, but the presence of other agents’ actions incurs
large variance in the estimates of such directions.
Formally, the variance of policy gradients for agent i at (τi, ai) is dependent on other agents’ actions:
Vara-〜∏-i [QJ0t(T, (ai, a-i))Vθi log∏i(a∕τi)]
=Vara-a〜∏-i [QJ0t(T, (ai, a-i))] (Vθi log∏i(a∕τi))(Vθi log∏i(ai∣Ti))τ,
where Vara-i [Qtnot (T, (ai, a-i))] can be very large due to the exploration or suboptimality of other
agents’ policies, which may cause suboptimality in individual policies. For example, suppose that the
optimal joint action under T is a*={a；,..., a/〉. When Ea-i 〜n-JQJO/T, (a*, a-i))] < 0 , ∏i(a*∣τi)
will decrease, possibly resulting in a suboptimal πi . This becomes problematic because a negative
feedback loop is created, in which the joint critic is affected by the suboptimality of agent i, which
disturbs policy updates of other agents. We call this issue centralized-decentralized mismatch (CDM).
Does CDM occur in practice for state-of-the-art algorithms? To answer this question, we carry
out a case study in Sec. 5.1. We can see that the variance of DOP gradients is significantly smaller
than COMA and MADDPG (Fig. 2 left). This smaller variance enables DOP to outperform other
algorithms (Fig. 2 middle). We will explain this didactic example in detail in Sec. 5.1. In Sec. 5.2
and 5.3, we further show that CDM is exacerbated in sequential decision-making settings, causing
divergence even after a near-optimal strategy has been learned.
3
Published as a conference paper at ICLR 2021
3.3	Credit Assignment for Multi-Agent Deterministic Policy Gradients
MADDPG (Lowe et al., 2017) and MAAC (Iqbal & Sha, 2019) extend deterministic policy gradient
algorithms (Silver et al., 2014; Lillicrap et al., 2015) to multi-agent settings, enabling efficient off-
policy learning in continuous action spaces. However, they leave the issue of credit assignment (Tumer
et al., 2002; Agogino & Tumer, 2004) largely untouched in fully cooperative settings, where agents
learn policies from a single global reward signal. In stochastic cases, COMA assigns credits by
designing a counterfactual baseline (Eq. 2). However, it is not straightforward to extend COMA to
deterministic policies, since the output of polices is no longer a probability distribution. As a result, it
remains challenging to realize efficient credit assignment in deterministic cases.
4	Decomposed Off-Policy Policy Gradients
To address the limitations of existing MAPG methods discussed in Sec. 3, we introduce the idea
of value decomposition into the multi-agent actor-critic framework and propose a Decomposed
Off-Policy policy gradient (DOP) method. We factor the centralized critic as a weighted summation
of individual critics across agents:
Qtφot(τ,a) = Piki(τ)Qiφi(τ,ai) +b(τ),	(5)
where φ and φi are parameters of the global and local Q functions, respectively, and ki ≥ 0 and
b are generated by learnable networks whose inputs are global observation-action histories. In the
following sections, we show that this linear decomposition helps address existing limitations of
previous methods. A concern is the limited expressivity of linear decomposition (Wang et al., 2020b),
which may introduce bias in value estimations. We will show that this limitation does not violate the
policy improvement guarantee of DOP.
Fig. 1 shows the architecture for learning decomposed critics.
We learn individual critics Qiφi by backpropagating gradients
from global TD updates dependent on the joint global reward,
i.e., Qiφi is learned implicitly rather than from any reward
specific to agent i. We enforce ki ≥ 0 by applying an abso-
lute activation function at the last layer of the network. The
network structure is described in detail in Appendix H.
Based on the critic decomposition learning, the following sec-
tions will introduce decomposed off-policy policy gradients for
learning stochastic policies and deterministic policies, respec-
tively. Similar to other actor-critic methods, DOP alternates
between policy evaluation—estimating the value function for
a policy—and policy improvement—using the value function
to update the policy (Barto et al., 1983).
Figure 1: A Decomposed critic.
4.1	Stochastic Decomposed Off-Policy Policy Gradients
For learning stochastic policies, the linearly decomposed critic plays an essential role in enabling
tractable multi-agent tree backup for off-policy policy evaluation and attenuating the CDM issue
while maintaining provable policy improvement.
4.1.1	Off-Policy Learning
Policy Evaluation: Train the Critic As discussed in Sec. 3.1, using tree backup (Eq. 3) to carry out
multi-agent off-policy policy evaluation requires calculating En[Qφot(τt+1, •)], which needs O(∣A∣n)
steps of summation when a joint critic is used. Fortunately, using the linearly decomposed critic,
DOP reduces the complexity of computing this expectation to O(n|A|):
En[QΦot(τ, •)] = Piki(τ)E∏i[Qφi(τ, •)]+ b(τ),	⑹
making the tree backup technique tractable (detailed proof can be found in Appendix A.1). Another
challenge of using multi-agent tree backup (Eq. 3) is that the coefficient Ct = Qt=1 λπ (Ql ∣τl) decays
4
Published as a conference paper at ICLR 2021
as t gets larger, which may lead to relatively lower training efficiency. To solve this issue, we propose
to mix off-policy tree backup updates with on-policy T D(λ) updates to trade off sample efficiency
and training efficiency. Formally, DOP minimizes the following loss for training the critic:
L(φ) = κLDβOP-TB(φ) + (1 - κ)LOπn(φ)	(7)
where κ is a scaling factor, β is the joint behavior policy, and φ is the parameters of the critic. The
first loss item is LDβOP-TB (φ) = Eβ [(yDOP-TB - Qtφot (τ , a))2], where yDOP-TB is the update target of
the proposed k-step decomposed multi-agent tree backup algorithm:
k-1
yDOP-TB = Qφ0t(τ, a) + X Ytct Tt + Y X ki(Tt+I)Eni[Qφi(Tt+1, ∙力+ b(Tt+1)- QφOt(Tt, at)
t=0	i
(8)
Here, φ0 is the parameters of a target critic, and at 〜β(∙∣τt). The second loss item is L∏n(φ)=
Eπ[(yOn - Qtφot (T, a))2], where yOn is the on-policy update target as in TD(λ):
∞
yOn = Qtφot(T,a) + X(Yλ)t rt +YQtφot(Tt+1,at+1) - Qtφot(Tt,at) .	(9)
t=0
In practice, we use two buffers, an on-policy buffer for computing LOπn(φ) and an off-policy buffer
for estimating LDβOP-TB (φ).
Policy Improvement: Train Actors Using the linearly decomposed critic architecture, we can derive
the following on-policy policy gradients for learning stochastic policies:
g = EnhPiki(τ)Vθi log∏i(a∕τi; θi)Qφi(τ, aj]	(10)
In Appendix A.2, we provide the detailed derivation and an off-policy version of stochastic policy
gradients. This update rule reveals two important insights. (1) With a linearly decomposed critic,
each agent’s policy update only depends on the individual critic Qiφi . (2) Learning the decomposed
critic implicitly realizes multi-agent credit assignment, because the individual critic provides credit
information for each agent to improve its policy in the direction of increasing the global expected
return. Moreover, Eq. 10 is also the policy gradients when assigning credits via the aristocrat
utility (Wolpert & Tumer, 2002) (Appendix A.2). Eq. 7 and 10 form the core of our DOP algorithm
for learning stochastic policies, which we call stochastic DOP and is described in detail in Appendix E.
The CDM Issue occurs when decentralized policies’ suboptimality exacerbates each other through
the joint critic. As an agent’s stochastic DOP gradients do not rely on the actions of other agents, they
attenuate the effect of CDM. We empirically show that DOP can reduce variance in policy gradients
in Sec. 5.1 and can attenuate the CDM issue in complex tasks in Sec. 5.2.1.
4.1.2	Stochastic DOP Policy Improvement Theorem
In this section, we theoretically demonstrate that stochastic DOP can converge to local optimal despite
the fact that a linearly decomposed critic has limited representational capability. Since an accurate
analysis for a complex function approximator (e.g., neural network) is difficult, we adopt several mild
assumptions used in previous work (Feinberg et al., 2018; Degris et al., 2012).
We first show that the linearly decomposed structure ensures that the learned local value functions
Qφi(τ, ai) preserve the order of Qn(τ, a# = Pa^ ∏-i(a-i∣τ-i)Q∏ot(τ, a) for a wide range of
function class.
Fact 1. Under mild assumptions, when value evaluation converges, ∀π, Qiφi satisfies that
Qn (τ ,ai) ≥ Qn (τ, ai) ^⇒ Qφi (τ, ai) ≥ Qφi (τ, ai), ∀τ ,0i,Oi.
Detailed proof of Fact 1 can be found in Appendix C.1 as well as more detailed discussion of its
implications. Furthermore, we prove the following proposition to show that policy improvement can
be guaranteed as long as the function class expressed by Qiφi is sufficiently large and the loss of critic
training is minimized.
5
Published as a conference paper at ICLR 2021
Proposition 1. Suppose the function class expressed by Qiφi (τ, ai) is sufficiently large (e.g. neural
networks) and the following loss L(φ) is minimized
L(φ) = XP(T)π(alτ)(Qnrt(T, a) - Qtot(T, a))2,
a,τ
where Qtφot(T, a) ≡ Piki(T)Qiφi(T, ai) + b(T). Then, we have
g = EnPi Ji log∏i(ai∣Ti; θi)Qπ(τ, a)]
=En hpiki(τ )Vθi log ∏i(ai∣τi; θi )Qφi (τ ,a/ ,
which means stochastic DOP policy gradients are the same as those calculated using centralized
critics (Eq. 2). Therefore, policy improvement is guaranteed.
The proof can be found in Appendix C.2, which is inspired by Wang et al. (2020a).
4.2	Deterministic Decomposed Off-Policy Policy Gradients
4.2.1	Off-Policy Learning
To enable efficient learning with continuous actions, we propose deterministic DOP. As in single-
agent settings, because deterministic policy gradient methods avoid the integral over actions, it greatly
eases the cost of off-policy learning (Silver et al., 2014). For policy evaluation, we train the critic by
minimizing the following TD loss:
L(φ) = E(τt,rt,at,Tt+ι)~D [ }t 十 YQTot(Tt+1, 〃(Tt+1； θ0)) - Qtot(Tt, at)) ] , (II)
where D is a replay buffer, and φ0 , θ0 are the parameters of the target critic and actors, respectively.
For policy improvement, we derive the following deterministic DOP policy gradients:
g = Eτ~D [Piki(T)Vθiμi(τi; θi)VaiQφi(t, ai)|ai = μi(τi∖θi) .	(12)
Detailed proof can be found in Appendix B.1. Similar to the stochastic case, This result reveals
that updates of individual deterministic policies depend on local critics when a linearly decomposed
critic is used. Based on Eq. 11 and Eq. 12, we develop the DOP algorithm for learning deterministic
policies in continuous action spaces, which is described in Appendix E and called deterministic DOP.
4.2.2	Representation Capacity of Deterministic DOP Critics
In continuous and smooth environments, we first show that a DOP critic has sufficient expressive
capability to represent Q values in the proximity of μ(T), ∀t with a bounded error. For simplicity,
we denote Oδ(T) = {a| k a 一 μ(T) ∣∣2≤ δ}.
Fact2. Assume that∀t, a, a0 ∈ Oδ(T), ∣ VaQμ(T, a) — RaQμ0t(T, a0) ∣∣2≤ L ∣∣ a - a0 ∣2∙ The
estimation error of a DOP critic can be bounded by O(Lδ2) for a ∈ Oδ(T), ∀T.
Detailed proof can be found in Appendix D. Here we assume that the gradients of Q-values with re-
spect to actions are Lipschitz smooth under the deterministic policy μ. This assumption is reasonable
given that Q-values of most continuous environments with continuous policies are rather smooth.
We further show that when Q-values in the proximity of μ(T), ∀t are well estimated with a bounded
error, deterministic DOP policy gradients are good approximation to the true gradients (Eq. 1).
APPrOXimately, ∣VaiQt0t(T, a) - Vai岛(丁HaiQφ(T, ai)| ~ O(Lδ), ∀i when δ《1. For detailed
proof, we refer readers to Appendix D.
5	Experiments
We design experiments to answer the following questions: (1) Does the CDM issue commonly
exist and can decomposed critics attenuate it? (Sec. 5.1, 5.2.1, and 5.3) (2) Can our decomposed
multi-agent tree backup algorithm improve the efficiency of off-policy learning? (Sec. 5.2.1) (3)
Can deterministic DOP learn reasonable credit assignment? (Sec. 5.3) (4) Can DOP outperform
state-of-the-art MARL algorithms? For evaluation, all the results are averaged over 12 different
random seeds and are shown with 95% confidence intervals.
6
Published as a conference paper at ICLR 2021
Figure 2: Bias-variance trade-off of DOP on the didactic example. Left: gradient variance; Middle:
Performance; Right: Average bias in Q estimations; Right-bottom: the element in ith row and jth
column is the local Q value learned by DOP for agent i taking action j .
——Stochastic DOP(OurS)	——QPLEX	——ROMA	——MAVEN	——NDQ
——QTRAN	——VDN	——QMIX	——COMA
MMM2	2s3z	MMM
东 U 一 ΛΛE1
0	2	4	6	8
T (mil)
10mvs11m
0	2	4	6	8	10
T (mil)
Somanybaneling
0	1	2	3	4	5
T (mil)
3svs3z
50
% U一λλ 4s①,L
012345	02468	10	02468	10
T (mil)	T (mil)	T (mil)
Figure 3: Comparisons with baselines on the SMAC benchmark.
5.1	Didactic Example: The CDM Issue and Bias-Variance Trade-Off
We use a didactic example to demonstrate how DOP attenuates CDM and achieves bias-variance
trade-off. In a state-less game with 3 agents and 14 actions, if agents take action1, 5, 9, respectively,
they get a team reward of 10; otherwise -10. We train stochastic DOP, COMA, and MADDPG for
10K timesteps and show the gradient variance, value estimation bias, and learning curves in Fig. 2.
Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017) is used to enable MADDPG to learn
in discrete action spaces.
Fig. 2-right shows the average bias in the estimations of all Q values. We see that linear decomposition
introduces extra estimation errors. However, the variance of DOP policy gradients is much smaller
than other algorithms (Fig. 2-left). As discussed in Sec. 3.2, large variance of other algorithms is due
to the CDM issue that undecomposed joint critics are affected by actions of all agents. Free from
the influence of other agents, DOP preserves the order of local Q-values (bottom of Fig. 2-right) and
effectively reduces the variance of policy gradients. In this way, DOP sacrifices value estimation
accuracy for accurate and low-variance policy gradients, which explains why it can outperform other
algorithms (Fig. 2-middle).
5.2	Discrete Action Spaces: The StarCraft II Micromanagement Benchmark
We evaluate stochastic DOP on the challenging SMAC benchmark (Samvelyan et al., 2019) for its
high control complexity. We compare our method with the state-of-the-art multi-agent stochastic
policy gradient method (COMA), value-based methods (VDN, QMIX, QTRAN (Son et al., 2019),
NDQ (Wang et al., 2020e), and QPLEX (Wang et al., 2020b)), exploration method (MAVEN, Mahajan
et al. (2019)), and role-based method (ROMA, Wang et al. (2020c)). For stochastic DOP, we fix the
hyperparameter setting and network structure in all experiments which are described in Appendix H.
For baselines, we use their default hyperparameter settings that have been fine-tuned on the SMAC
benchmark. Results are shown in Fig. 3. Stochastic DOP significantly outperforms all the baselines
by a wide margin. To our best knowledge, this is the first time that a MAPG method has significantly
better performance than state-of-the-art value-based methods.
7
Published as a conference paper at ICLR 2021
----Stochastic DOP (OUrS) ----------On-Pohcy DOP ----------Off-PoIicy DOP (K=1)	---- DOP With Common Tree Backup
MMM2
50
<⅛ U一M 4SE
o
0	2	4
T (mil)
10mvs11m
________________________2s3z_____________________ MMM
100
50
U一M 4S01
T (mil)
SOmanybanelina
0	1	2	3	4	5
T (mil)
3svs3z
50
<⅛ U一M 4SE
2	3
T (mil)
sU一M 4S01
O O
O 5
U一M 4S01
Figure 4: Comparisons with ablations on the SMAC benchmark.
5.2.1	Ablations
Stochastic DOP has three main components: (a) off-policy policy evaluations, (b) the decomposed
critic, and (c) decomposed multi-agent tree backup. By design, component (a) improves sample
efficiency, component (b) can attenuate the CDM issue, and component (c) makes off-policy policy
evaluations tractable. We test the contribution of each component by carrying out the following
ablation studies.
Off-Policy Learning In our method, κ controls the "off-policyness" of training. For DOP, we set κ
to 0.5. To demonstrate the effect of off-policy learning, we change κ to 0 and 1 and compare the
performance. In Fig. 4, we can see that both DOP and off-policy DOP perform much better than
the on-policy version (κ=0), highlighting the importance of using off-policy data. Moreover, purely
off-policy learning generally needs more samples to achieve similar performance to DOP. Mixing
with on-policy data can largely improve training efficiency.
The CDM Issue On-Policy DOP uses the same decomposed critic structure as DOP, but is trained
only with on-policy data and does not use tree backup. The only difference between On-Policy
DOP and COMA is that the former one uses a decomposed joint critic. Therefore, given that a
COMA critic has a more powerful expression capacity than a DOP critic, the outperformance of
On-Policy DOP against COMA shows the effect of CDM. COMA is not stable and may diverge after
a near-optimal policy has been learned. For example, on map so_many_baneling, COMA policies
degenerate after 2M steps. In contrast, On-Policy DOP can converge with efficiency and stability.
Decomposed Multi-Agent Tree Backup DOP with Common Tree Backup (DOP without component
(C)) is the same as DOP except that En[Qφot(τ, •)] is estimated by sampling 200 joint actions from
π. Here, we estimate this expectation by sampling because direct computation is intractable (for
example, 2010 summations are needed on the map MMM). Fig. 4 shows that when the number of
agents increases, sampling becomes less efficient, and common tree backup performs even worse
than On-Policy DOP. In contrast, DOP with decomposed tree backup can quickly and stably converge
using a similar number of summations.
5.3	Continuous Action Spaces: Multi-Agent Particle Environments
We evaluate deterministic DOP on multi-agent particle environments (MPE, (Mordatch & Abbeel,
2018)), where agents take continuous actions in continuous spaces. We compare our method with
MADDPG (Lowe et al., 2017) and MAAC (Iqbal & Sha, 2019). Hyperparameters and the network
structure are fixed for deterministic DOP across experiments, which are described in Appendix H.
The CDM Issue We use task Aggregation as an example to show that deterministic DOP attenuates
the CDM issue. In this task, 5 agents navigate to one landmark. Only when all agents reach the
landmark will they get a team reward of 10 and successfully end the episode; otherwise, an episode
ends after 25 timesteps and agents get a reward of -10. Aggregation is a typical example where
other agents’ actions can influence an agent’s local policy through an undecomposed joint critic.
Intuitively, as long as one agent does not reach the landmark, the centralized Q value is negative,
confusing other agents who get to the landmark. This intuition is supported by the empirical results
8
Published as a conference paper at ICLR 2021
----Deterministic DOP (Ours)
Aggregation
ueew IUnsα
0.50 0.75 1.00 1.25 1.50 1.75 2.00
T (mil)
T (mil)
——MADDPG ——MAAC
Push Clockwise ------ Push Counterclockwise
Local Q-Values Learned by DOP on Task Mill
S
§20
-≡
>
σio∙
∙≡
2 o∙
0.00	0.05	0.10	0.15	0.20	0.25
T (mil)
Figure 5: Left and middle: performance comparisons with COMA and MAAC on MPE. Right: The
learned credit assignment mechanism on task Mill by deterministic DOP.
shown in Fig. 5-left - methods with undecomposed critics can find rewarding configurations but then
quickly diverge, while DOP converges with stability.
Credit Assignment We use task Mill to show that DOP can learn effective credit assignment
mechanisms. In this task, 10 agents need to rotate a millstone clockwise. They can push the millstone
clockwise or counterclockwise with force between 0 and 1. If the millstone’s angular velocity, ω,
gets greater than 30, agents are rewarded 3 per step. If ω exceeds 100 in 10 steps, the agents win
the episode and get a reward of 10; otherwise, they lose and get a punishment of -10. Fig. 5-right
shows that deterministic DOP can gradually learn a reasonable credit assignment during training,
where rotating the millstone clockwise has much larger Q-values. This explains why deterministic
DOP outperforms previous state-of-the-art deterministic MAPG methods, as shown in Fig. 5-middle.
6 Closing Remarks
This paper pinpointed drawbacks that hinder the performance of state-of-the-art MAPG algorithms:
on-policy learning of stochastic policy gradient methods, the centralized-decentralized mismatch
problem, and the credit assignment issue in deterministic policy learning. We proposed decomposed
actor-critic methods (DOP) to address these problems. Theoretical analyses and empirical evaluations
demonstrate that DOP can achieve stable and efficient multi-agent off-policy learning.
Acknowledgments
We would like to thank the anonymous reviewers for their insightful comments and helpful sugges-
tions. This work is supported in part by Science and Technology Innovation 2030 - "New Generation
Artificial Intelligence” Major Project (No. 2018AAA0100904), and a grant from the Institute of Guo
Qiang, Tsinghua University.
References
Adrian K. Agogino and Kagan Tumer. Unifying temporal and structural credit assignment problems.
In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent
Systems - Volume 2, AAMAS '04, pp. 980-987, USA, 2004. IEEE Computer Society. ISBN
1581138644.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. In Proceedings of the International
Conference on Learning Representations (ICLR), 2020.
A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5):
834-846, 1983.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Lucas Cassano, Kun Yuan, and Ali H Sayed. Multi-agent fully decentralized value function learning
with linear convergence rates. arXiv preprint arXiv:1810.07792, 2018.
9
Published as a conference paper at ICLR 2021
Jacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson. The representational
capacity of action-value networks for multi-agent reinforcement learning. In Proceedings of the
18th International Conference on Autonomous Agents and MultiAgent Systems, pp.1862-1864.
International Foundation for Autonomous Agents and Multiagent Systems, 2019.
Abhishek Das, TheoPhile GerveL Joshua Romoff, Dhruv Batra, Devi Parikh, Mike RabbaL and Joelle
Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine
Learning, pp. 1538-1546, 2019.
Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Bohmer,
and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous
cooperative control. arXiv preprint arXiv:2003.06709, 2020.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of the
29th International Coference on International Conference on Machine Learning, pp. 179-186,
2012.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 2137-2145, 2016.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1146-1155. JMLR. org, 2017.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596, 2018.
Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In
Advances in neural information processing systems, pp. 1523-1530, 2002a.
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In
ICML, volume 2, pp. 227-234. Citeseer, 2002b.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1861-1870, Stockholmsmassan, Stockholm
Sweden, 10-15 Jul 2018. PMLR. URL http:// proceedings.mlr.press/ v80/ haarnoja18b.html.
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of
learning in multiagent environments: Dealing with non-stationarity. ArXiv, abs/1707.09183, 2017.
Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Duenez-Guzman, Antonio GarCia
Castaneda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, et al. Inequity aversion
improves cooperation in intertemporal social dilemmas. In Advances in Neural Information
Processing Systems, pp. 3330-3340, 2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 2961-2970, 2019.
10
Published as a conference paper at ICLR 2021
Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-
level performance in 3d multiplayer games with population-based reinforcement learning. Science,
364(6443):859-865, 2019.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
Proceedings of the International Conference on Learning Representations (ICLR), 2017.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep
reinforcement learning. In International Conference on Machine Learning, pp. 3040-3049, 2019.
Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood ratio
policy gradient. In Advances in Neural Information Processing Systems, pp. 1000-1008, 2010.
Jelle R Kok and Nikos Vlassis. Collaborative multiagent reinforcement learning by payoff propagation.
Journal of Machine Learning Research, 7(Sep):1789-1828, 2006.
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. In Proceedings of the International Conference on Learning
Representations (ICLR), 2017.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, pp. 1-9, 2013.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
Proceedings of the International Conference on Learning Representations (ICLR), 2015.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
Sergio Valcarcel Macua, Aleksi Tukiainen, Daniel Garcia-OcaEa Herndndez, David Baldazo, En-
rique Munoz de Cote, and Santiago Zazo. Diff-dac: Distributed actor-critic for multitask deep
reinforcement learning. arXiv preprint arXiv:1710.10363, 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In Proceedings of the International Conference on
Learning Representations (ICLR), 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. In Advances in Neural Information Processing Systems, pp. 7611-7622,
2019.
Nicolas Meuleau, Leonid Peshkin, Leslie P Kaelbling, and Kee-Eung Kim. Off-policy policy search.
MIT Articical Intelligence Laboratory, 2000.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
R6mi Munos, Tom StePIeton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062,
2016.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep
decentralized multi-task multi-agent reinforcement learning under partial observability. In Pro-
ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2681-2690.
JMLR. org, 2017.
11
Published as a conference paper at ICLR 2021
Doina Precup, Richard S Sutton, and Satinder Singh. Eligibility traces for off-policy policy evaluation.
In ICML’00 Proceedings of the Seventeenth International Conference on Machine Learning, 2000.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, pp. 4292-4301, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement
learning. arXiv preprint arXiv:2003.08839, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
International Conference on Machine Learning-Volume 32, pp. I-387, 2014.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896, 2019.
Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation.
In Advances in Neural Information Processing Systems, pp. 2244-2252, 2016.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the
17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085-2087.
International Foundation for Autonomous Agents and Multiagent Systems, 2018.
Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang, Tamer Basar, and Ji Liu. A multi-
agent off-policy actor-critic algorithm for distributed reinforcement learning. arXiv preprint
arXiv:1903.06372, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in
Neural Information Processing Systems, pp. 4496-4506, 2017.
Kagan Tumer, Adrian K. Agogino, and David H. Wolpert. Learning sequences of actions in collectives
of autonomous agents. In Proceedings of the First International Joint Conference on Autonomous
Agents and Multiagent Systems: Part 1, AAMAS ’02, pp. 378-385, New York, NY, USA, 2002.
Association for Computing Machinery. ISBN 1581134800. doi: 10.1145/544741.544832. URL
https:// doi.org/ 10.1145/ 544741.544832.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JunyOung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Jianhao Wang, Zhizhou Ren, Beining Han, and Chongjie Zhang. Towards understanding linear value
decomposition in cooperative multi-agent q-learning, 2020a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b.
Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement
learning with emergent roles. In Proceedings of the 37th International Conference on Machine
Learning, 2020c.
Tonghan Wang, Jianhao Wang, Wu Yi, and Chongjie Zhang. Influence-based multi-agent exploration.
In Proceedings of the International Conference on Learning Representations (ICLR), 2020d.
12
Published as a conference paper at ICLR 2021
Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable
value functions with communication minimization. In Proceedings of the International Conference
on Learning Representations (ICLR), 2020e.
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode:
Learning roles to decompose multi-agent tasks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2021.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. In Proceedings of the
International Conference on Learning Representations (ICLR), 2016.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. In Proceedings of the International Conference on Learning
Representations (ICLR), 2019.
David H Wolpert and Kagan Tumer. Optimal payoff functions for members of collectives. In
Modeling CompIexity in economic and social Systems, pp. 355-369. World Scientific, 2002.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580,
2018.
Chongjie Zhang and Victor Lesser. Coordinated multi-agent reinforcement learning in networked
distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning, pp. 5872-5881, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer BaSar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Yan Zhang and Michael M Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. arXiv preprint arXiv:1903.09255, 2019.
13
Published as a conference paper at ICLR 2021
A Mathematical details for stochastic DOP
A.1 Decomposed critics enable tractable multi-agent tree backup
In Sec. 4.1.1, we propose to use tree backup (Precup et al., 2000; Munos et al., 2016) to carry out
multi-agent off-policy policy evaluation. When a joint critic is used,
calculating En 眼",•)]
requires O(|A|n) steps of summation. To solve this problem, DOP uses a linearly decomposed critic,
and it follows that:
En[Qφot(τ, a)] = En(HT)Qφot(τ, a) = En(HT) Eki(T)Qφi(T,ai) + b(T)
a	ai
=X n(HT) X ki(T )Qφi(T ,ai) + X n(a|T )b(T)
a	i	a	(13)
=XX
∏i(αi∣Ti)ki(T)Qφi(t, ai) Xn-i(a-i|T-i) + b(T)
i ai	a-i
=X ki(T)E∏i[Qφi(T, ∙)] + b(T),
i
which means the complexity of calculating this expectation is reduced to O(n|A|).
A.2 Stochastic DOP policy gradients
A.2.1 On-policy version
In Sec. 4.1.1, we give the on-policy stochastic DOP policy gradients:
g = En hPiki(T)Vθi log∏i(ai∣τi; θi)Qφi (t,a/ .	(14)
We now derive it in detail.
Proof. We use the aristocrat utility (Wolpert & Tumer, 2002) to perform credit assignment:
Ui(T ,ai) = Bot(T, a) - Eni(X%)Qφot(T, (x, a-i))
x
=Xkj(T)Qjφj(T,aj) -X ∏i(x∣Ti) I X kj (τ)Qφj (τ,aj) + ki(τ)Qφi (τ,x) I
j	x	j 6=i
=ki(τ)Qφi(τ,ai) - ki(τ) X∏i(x∣τi)Qφi(τ,x)
x
=ki(τ) Qφi (τ,ai) - X ∏i(x∣τi)Qφi (τ,x) ,
x
It is worth noting that Ui is independent of other agents’ actions. Then, for the policy gradients, we
have:
g=En[	Vθlog ∏i(αi∣Ti)Ui(τ ,ai)]
i
£ Vθlog πi (αi∣Ti)ki (τ)
i
En £ Vθ log∏i(ai∣Ti)ki(τ)Qφi(τ, ai)
i
□
14
Published as a conference paper at ICLR 2021
A.2.2 Off-policy version
In Appendix A.2, we derive the on-policy policy gradients for updating stochastic multi-agent policies.
Similar to policy evaluation, using off-policy data can improve the sample efficiency with regard to
policy improvement.
Using the linearly decomposed critic architecture, the off-policy policy gradients for learning stochas-
tic policies are:
g = Ee [丁, a) Piki(T)Vθ log∏i(ai∣Ti; θi)Qφi(T,ai) .	(15)
β(τ, a)
Proof. The objective function is:
J(θ) =Eβ[Vtπot(T)].
Similar to Degris et al. (2012), we have:
VθJ(θ) = Eβ
n(a|T) X Vθ log∏i(ai∣Ti)Ui(τ,ai)
β(a∣τ) V
π( alτ) 一一 , . ......
β⅛U EVθ log∏i(ai∣Ti)ki(τ)Ai(τ,aj
β(HT) V
n(a|T)X Vθ log ∏i(ai∣τi)ki(τ)Qφi (τ,ai) .
β(a∣τ) V
□
B	Mathematical details for deterministic DOP
B.1	Deterministic DOP policy gradient theorem
In Sec. 4.2.1, we give the following deterministic DOP policy gradients:
vJ(θ) = ET〜DhPiki(T)vθiμi(τi; θi)vaiQφi (τ,ai)|ai=*i(Ti；6i)i .	(16)
Now we present the derivation of this update rule.
Proof. Drawing inspirations from single-agent cases (Silver et al., 2014), we have:
VJ(θ)= E-d [VθQcUT, a)]
=Eτ~D [^X vθki (T)Qi i (τ, ai)∣ai=μi(τi[θi)]
i
=ET 〜D [£ vθ μi(Ti； θi)vai ki(τ )Qφi(τ ,ai)|ai =μi(%θi)].
i
□
C Theoretical justification for stochastic DOP policy
IMPROVEMENT
In order to understand how DOP works despite the biased Qπ (τ, a) estimation, we provide some
theoretical justification for the policy update. Unfortunately, a thorough analysis on deep neural
network and TD-learning is too complex to be carried out. Thus, we make some assumptions for the
mathematical proof. The following two subsections provide two different view points of theoretical
understanding.
15
Published as a conference paper at ICLR 2021
1.	In the first view (Sec. C.1), we assume some mild assumptions on value evaluation which
holds on a wide range of function class. In this way, we can prove a policy improvement
theorem similar to Degris et al. (2012).
2.	In the second view, we remove the Monotone condition from a practical point of view.
We then prove that when the loss of value evaluation is minimized (individual critics output
Qiφi (τ, ai) to be a good estimate of Qiπ(τ, ai)), the DOP gradients in Eq. 12 equal to Eq. 2
which is the standard gradient form.
C.1 Proof of stochastic DOP policy improvement theorem
Inspired by previous work (Degris et al., 2012), we relax the requirement that Qtφot is a good estimate
of Qtπot and show that stochastic DOP still guarantees policy improvement.
First, we define
Qn (T ,ai) = X ∏-i(a-i ∣T-i)Qfot(τ, a), An (T ,a，)= X π(a-i∣T-i)Aπ (T, a).
a-i	a-i
Directly analyzing the minimization of TD-error is challenging. To make it tractable, some
works (Feinberg et al., 2018) simplify this analysis to an MSE problem. For the analysis of stochastic
DOP, we adopt the same technique and formalize the critic’s learning as the following problem:
L(O) = XP(T)n(a|T) (Qπ)t(T, a)- QLt(T, a0 ,	(17)
a,τ
where Qtπot (T, a) are the true values, which are fixed during optimization. In the following analysis,
we assume distinct parameters for different T. We first show that Fact 1 holds for a wide range of
function class of Qiφi. To this end, we first prove the following lemma.
Lemma 1. Without loss of generality, we consider the following optimization problem:
LT(φ) = X∏(a∣T) (Qπ(t, a) - f (Qφ(t, a)))2.	(18)
a
Here, f(Qφ(T, a)) : Rn → R, and Qφ (T, a) is a vector whose ith entry is Qiφi (T, ai). In DOP, f
satisfies that -科---- > 0 for any i, ai.
∂Qi i (τ,ai)
If VφiQφi (t, ai) = 0,∀φi,ai it holds that:
Qn(t,ai) ≥ Qn(t,ai) ^⇒ Qφi(T, ai) ≥ Qφi(T,ai),	∀αi,αi.
Proof. When the optimization converges, φi reaches a stationary point where Vφi Lτ (φ) = 0, ∀i.
πi(ailτi) XY πj (aj |Tj ) (Qπ)t(τ, a) - fgφ(τ, a))) (— dQφ∂T α ) )vΦi Qφi (T ,ai) = 0, ∀ai.
Since Vφi Qiφi (T, ai) 6= 0, this implies that ∀i, ai, we have
XYπj(aj|Tj)(Qnot(T, a)- f (Qφ(T, a))) = 0
a-i j6=i
⇒ X ∏-i(a-i∣T-i)f(Qφ(τ, a)) = Qn (τ ,αi)
a-i
We consider the function q(τ, αi) = Pa , π-i(a-i∣τ-i)f (Qφ(τ, a))), which is a function of Qφ.
Its partial derivative w.r.t Qiφi (T, αi) is:
∂q(τ,ai)	L	∂f(Qφ(τ, a))
---7	= π ∏-i(a-i ∣T-i)-t--------
dQφi(T,ai)-------------------------------a-i	dQφi(T,ai)
>0
16
Published as a conference paper at ICLR 2021
Therefore, if Qiπ (τ , ai) ≥ Qiπ(τ, a0i), then any local minimal of Lτ (φ) satisfies Qiφi (τ, ai) ≥
QΦi (T ,ai).	□
We argue that VφiQφi (τ, ai) = 0 is a rather mild assumption and holds for a large range of function
class of Qiφi .
Fact 3. [Formal Statement of Fact 1] For the following choices of Qiφi:
1.	Tabular expression of Qφi which requires Q(n∣A∣∣τ|) space;
2.	Linear function class where ai are one-hot coded:
Qφi(τ,ai) = φi ∙ hτ,aii;
3.	2-layer neural networks (φi 6= 0) with strictly monotonic increasing activation functions
(e.g. tanh, leaky-relu).
4.	Arbitrary k-layer neural networks whose activation function at the (k - 1)th layer is sigmoid.
when value evaluation converges, ∀π, Qiφi satisfies that
Qn(τ,ai) ≥ Qn(τ,ai) ^⇒ Qφi(τ,ai) ≥ Qφi(T,ai), ∀τ,a%,*.
Proof. We need to prove that Vφi Qiφi (τ, ai) 6= 0. For brevity, we use aik to denote the kth element
of the one-hot coding, and use φitai to denote the weight connecting the tth element of the upper layer
and the aik element.
(1 & 2) For tabular expression and linear functions, ∀ai = k we have
∂Qφi(τ,a%) _ 1
ιak	=1
∂φi1ai
(3)	The 2-layer neural network can be written as Qiφi (T, ai) = W2σ(W1(T, ai)). Besides, we denote
the hidden layer as h. Since φi 6= 0, we consider some nonzero element φ1Wt,2i . For the kth action, the
gradient of the parameter φtWk,1i is
dQd：(WIai) = ΦW2iσ0(ht) = 0, ∀k
∂φtk,i
(4)	Without loss of generality, we consider the last layer φ1Wt,ki :
dQφi (T ,ai) b5kf`n
FWr = σ(ht ) >0
These are the cases where Vφi Qiφi 6= 0. Even when ∃φi, Vφi Qiφi = 0 , such φi usually occupy
only a small parameter space and happen with a small probability. As a result, we conclude that
VφiQφi (τ, ai) = 0 is a rather mild assumption.	□
Based on Fact 1, we are able to prove the policy improvement theorem for stochastic DOP. We will
show that even without an accurate estimate of Qtπot, the stochastic DOP policy updates can still
improve the objective function J(π) = Eπ[Pt γtrt]. We first prove the following lemma.
Lemma 2. For two sequences {ai}, {bi}, i ∈ [n] listed in an increasing order. If	i bi = 0, then
Piaibi≥0.
17
Published as a conference paper at ICLR 2021
Proof. We denote G= 1 Pi a%, then Pi aib = a(Pi bi) + Pi Gibi where Pi Ai = 0. Without
loss of generality, We assume that Gi = 0, ∀i. j and k which aj ≤ 0, aj+ι ≥ 0 and bk ≤ 0, bk+ι ≥ 0.
Since a, b are symmetric, we assume j ≤ k. Then we have
aibi =	aibi +	aibi +	aibi
i∈[n]	i∈[1,j]	i∈[j+1,k]	i∈[k+1,n]
≥	aibi +	aibi
i∈[j+1,k]	i∈[k+1,n]
≥ ak	bi + ak+1	bi
i∈[i+1,k]	i∈[k+1,n]
As Pi∈[j+1,n] bi ≥ 0, we have
- Pi∈[j+1,k] bi ≤ Pi∈[k+1,n] bi.
Thus, Pi∈[n] aibi ≥ (ak+1 - ak) Pi∈[k+1,n] bi ≥ 0∙	□
Based on Fact 1 and Lemma 2, we prove the following proposition.
Proposition 2. [Stochastic DOP policy improvement theorem] Under mild assumptions, for any
pre-update policy πo which is updated by Eq. 10 to π, denote ∏i(ai∣τi) = ∏0(ai∣τi) + 8@名,丁 δ, where
δ > 0 is a sufficiently small number. If it holds that ∀τ,ai,ai,Qφi (τ ,ai) ≥ Qφi (τ ,ai) ^⇒
βai,τ ≥ βa0,τ (MONOTONE condition, and φi is the parameters before update.), then we have
J(π) ≥ J(πo),	(19)
i	.e., the joint policy is improved by the update.
Proof. Under Fact 1, it follows that
Qno (τ,ai) > Qno (τ,ai) 0 βai,τ ≥ βai,τ .	(20)
Since J(π) = Pτ0p(τ0)Vtπot(τ0), it suffices to prove that ∀τt, Vtπot(τt) ≥ Vtπoto (τt). We have:
X π(atiτt)Qπt(τt, at)=X (Y πi(atiτit)) Qnot(Tt, at)
at	at	i=1
=X (Y(∏0(at∣Tit)+ βai,τtδ)) QEtE, at)
at	i=1
=Vt∏to (Tt) + δ X X βat,τt I Y ∏0(aj∣Tjt) I Q∏0t(τt, at) + o(δ)
i=1 at	j 6=i
n
=Vtnoto(Tt)+δXXβati,τtQino(Tt,ait)+o(δ).	(21)
i=1 ait
Since δ is sufficiently small, in the following analysis we omit o(δ). Observing that Eaa ∏i(ai∣τi)=
1, ∀i, we get Pa βai,τ = 0. Thus, by Lemma 2 and Eq. 21, we have
X n(at|Tt)Qn；(Tt, at) ≥ VnO(Tt).	(22)
at
Similar to the policy improvement theorem for tabular MDPs (Sutton & Barto, 2018) , we have
VnO(Tt) ≤ X π(at ITt)Qπ"τt, at)
at
=En(at ITt)卜 S, at)+YEp(Tt+in at)vnO(Tt+i) I
at	τt+1
18
Published as a conference paper at ICLR 2021
≤Enmt|Tt) I r(τt,at)+YEp(Tt+4τt,at) I En(at+i|Tt+I)Qπ"τt+ι,at+1)
at	τt+1	at+1
≤∙∙∙
≤ Vtπot (τt).
This implies J(π) ≥ J(πo) for each update.
Moreover, We verify that ∀τ, ai, a®, Qφi(τ, ai) > Qφi(τ, ai) ^⇒ 8&i,丁 ≥ βa0,τ (the MONOTONE
condition) holds for any π with a tabular expression. For these π, let ∏i(αi∣Ti) =。&%丁, then it holds
that Pa θai,τ = 1. Since the gradient of policy update can be Written as:
Vθ J(∏θ) = Ed(T) X ki(τ)Vθ log∏i(ai∣Ti; θi)Qφi (τ, aj
i
=X d(τ) X ki(τNθi∏(aigQcφ(τ,a^
=X d(τ) X ki(τ)Vθi∏(a∕τi)Aφi(τ,a^
where dπ(τ) is the occupancy measure w.r.t our algorithm. With a tabular expression, the update of
each θai,τ is proportion to βai,τ
β°i,τ (X dθ(πθ ) = d(T )Aφi (T ,ai)
dθai,τ
Clearly, βai,τ ≥ βa.τ ^⇒ Qφi(T,ai) ≥ Qφi(τ,αQ .
□
C.2 Analysis without Monotone condition
For practical implementation of policy ∏i(α∕τi), the MONOTONE condition is too strong to be
satisfied for all πi . Analyzing the policy update when the condition is violated is difficult with only
Fact 1 at hand. Therefore, it is beneficial to understand policy improvement without the Monotone
condition.
To bypass the MONOTONE condition, we require a stronger property of the learnt Qiφi (T, ai) in
addition to order preserving (Fact 1). Theorem 1 in Wang et al. (2020a) offers a closed form solution
of additive decomposition and we restate it as the following lemma
Lemma 3 (Restatement of Theorem 1 in Wang et al. (2020a)). If we consider the solution of
arg min E	π(a∣τ) ( y(τ, a) — EQi(T,a，)),
Q	(s,a)∈S×A	i=1
∀i ∈ [n], ∀T, a the individual action-value function Qi (T, ai) =
E	[y	(τ,	ai,	a-i)]	—	n∙-1	E	[y(τ,	a)]	+	Wi(S)	(23)
a-i 〜∏-i(∙∣τ-i)	n a 〜π(∙∣τ)
The residual term w is an arbitrary vector satisfying ∀s,	in=1 wi (s) = 0.
Based on this lemma, we can derive another proposition to theoretically justify the DOP architecture.
Proposition 1. Suppose the function class expressed by Qiφi (T, ai) is sufficiently large (e.g. neural
networks) and the following loss L(φ) is minimized
L(φ) = XP(T)n(a|T)(QnOt(T, a) - QfOt(T, a))2,
a,τ
19
Published as a conference paper at ICLR 2021
where Qtφot(τ, a) ≡ Piki(τ)Qiφi(τ,ai) + b(τ). Then, we have
g = En PiVθi log∏i(ai∣Ti; θi)Qπ(τ, a)]
=EnhPiki(T )Vθi log ∏i(ai∣τi; θi )Qφi (T ,aj ,
which means stochastic DOP policy gradients are the same as those calculated using centralized
critics (Eq. 2). Therefore, policy improvement is guaranteed.
Proof. For brevity, we denote Qkiφi (T, ai) = k(T)Qiφi(T, ai). Then L(φ) can be written as
L(O) = XP(T)π(alτ)(QnJt(T, a) - PiQkφi(T, ai) - b(T))2
a,τ
According to Lemma 3, when L(φ) is minimized, we have
Qkφi(t, ai) = Qn(t, ai) - n-1 Vn(T) + Wi(S)- 1b*(T)
nn
= Qin(T,ai) - wi0(T)
Then
g = En hpiki (t )Vθi log ∏i(ai∣τi; θi)Qφi (T, aj
=En [PiVθi log πi(ai ∣τi; θi)Qkφi (τ, aj
=En [PiVθi log∏i(ai∣τi; θi)Qn(τ, a)]
□
Therefore, in expectation, stochastic DOP gradients are the same as those calculated using centralized
critics (Eq. 2). We no longer require the Monotone condition to guarantee improvement of the
policy update. Proposition 1 is another point of view to explain the performance guarantee of
DOP despite its constrained critics.
D Representational capability of deterministic DOP critics
In Sec. 4.2.2, we present the following facts about deterministic DOP:
Fact2. Assume that∀τ, a, a0 ∈ Oδ(τ), k VaQμ(τ, a) 一 RaQμ0t(τ, a0) ∣∣2≤ L ∣∣ a 一 a0 k2. The
estimation error of a DOP critic can be bounded by O(Lδ2) for a ∈ Oδ(T), ∀T.
We consider the Taylor expansion with Lagrange remainder of Q&(t, a). Namely,
Qμt(τ, a) = Qμt(τ, μ(τ)) + VaQμt(τ, a)∣a=μ(τ) ∙ (a 一 μ(τ)) + 1 V2Qμt(τ, a4) ∣∣ a 一 π(τ) ∣∣2
Since ∀a ∈ Oδ (π (T)), we have
|Qtot(T, a) - Qtot (t , N(T)) - VaQtot(T, a) la=μ(τ) ∙ (a - N(T ))| ≤ 1 Lδ2
Noticing that the first order Taylor expansion of Qμ has the form P[n] ki (T)Qiφ(T, ai) + b(T).
Therefore, the optimal solution of the MSE problem in Eq. 17 under DOP critics has an error term
less than O(Lδ2) for arbitrary sampling distribution p(τ, a) of a ∈ Oδ(μ(τ)).
When Q values in the proximity of μ(τ), ∀τ is well estimated within a bounded error and δ《1,
approximately, we have
∣∂Qμt(τ, a)	_ ∂Qφot(τ,	a)	〜	Qμt(τ, a-i,	ai +	δ)	― Qμt(τ, a) _	Qφot(τ, a—i, ai	+ δ)	― Qφot(τ, a)
| —而i	∂ai — | ≈ |	δ	δ
Qtot(T, a-i , ai + δ) 一 Qtot(T, a-i , ai + δ)	Qtot(T, a) 一 Qtot(T, a)
=|	δ	δ
〜O(Lδ)
20
Published as a conference paper at ICLR 2021
E	Algorithms
In this section, we describe the details of our algorithms, as shown in Algorithm 1 and 2.
Algorithm 1 Stochastic DOP
Initialize a critic network Qφ, actor networks πθi, and a mixer network Mψ with random parameters
φ,θi, ψ.
Initialize target networks: φ0 = φ, θ0 = θ, ψ0 = ψ
Initialize an off-policy replay buffer Doff and an on-policy replay buffer Don.
for t = 1 to T do
Generate a trajectory and store it in Doff and Don
Sample a batch consisting of N1 trajectories from Don
Update decentralized policies using the gradients described in Eq. 10
Calculate LOn(φ)
Sample a batch consisting of N2 trajectories from Doff
Calculate LDOP-TB (φ)
Update critics using LOn(φ) and LDOP-TB(φ)
if t mod d = 0 then
Update target networks: φ0 = φ, θ0 = θ, ψ0 = ψ
end if
end for
Algorithm 2 Deterministic DOP
Initialize a critic network Qφ, actor networks μei and a mixer network Mψ with random parameters
θ, φ, ψ
Initialize target networks: φ0 = φ, θ0 = θ, ψ0 = ψ
Initialize replay buffer D
for t = 1 to T do
Select action with exploration noise a 〜μ(τ) + e, generate a transition and store the transition
tuple in D
Sample N transitions from D
Update the critic using the loss function described in Eq. 11
Update decentralized policies using the gradients described in Eq. 12
if t mod d = 0 then
Update target networks: φ0 = αφ+ (1 - α)φ0, θ0 = αθ + (1 - α)θ0, ψ0 = αψ + (1 - α)ψ0
end if
end for
F DOP with Communication
Although DOP can solve many coordination problems, as shown by the comparison against IQL
in Fig.6, its complete decomposition critic raises the concern that DOP can not deal with the
miscoordination problem induced by highly uncertain and partial observable environments.
We use an example to illustrate the causes of miscoordination problems and argue that introducing
communication into DOP can help address these problems. In hallway (Fig. 7(a)), two agents
randomly start at states a1 to am and b1 to bn , respectively. Agents can observe their position and
choose to move left, move right, or keep still at each timestep. Agents win and are rewarded
10 if they arrive at state g simultaneously. Otherwise, if any agent arrives at g earlier than the other,
the team gets no reward, and the next episode begins. The horizon is set to max(m, n) + 10 to avoid
an infinite loop.
Without communication, one agent cannot know the position of its teammates, so it is difficult to
coordinate actions. This explains why on hallway with m=n=4, the team can win only 25% of the
games (Fig. 7(b)). Equipping DOP with communication can largely solve the problem - agents learn
to communicate their positions and move left at a1 or b1 simultaneously. For communication, we
21
Published as a conference paper at ICLR 2021
----Stochastic DOP (Ours) ---------- IQL
MMM2
2s3z
0	2	4	6	8	0	2	4	6	8	012345
T (mil)	T (mil)	T (mil)
Figure 6: A decomposed critic can solve many coordination problems which can not be solved by
IQL.
(a) Task hallway
(b) Performance comparison
Figure 7: A highly partial observable task. (a) Task hallway; (b) Performance of DOP with and
without communication on hallway with m=n=4.
use the technique introduced by Wang et al. (2020e). Agents share a communication module, and
messages are passed both between actors and individual Q-functions.
Such miscoordination problems are common in complex multi-agent tasks (Wang et al., 2020e). We
believe introducing communication into DOP can help it solve a larger range of problems.
G Baseline by Sampling
One problem of existing MAPG methods is the CDM issue, which describes the large variance in
policy gradients caused by the influence of other agents’ actions introduced through the joint critic.
Another technique that is frequently used to reduce the variance in policy gradients in the single-agent
RL literature is by using baselines (Sutton & Barto, 2018). In this section, we investigate whether
using baselines can effectively reduce variance in multi-agent settings.
We start from centralized critics. COMA uses a baseline where local actions are marginalized. Since
the variance and performance of COMA have been discussed in Sec. 5, we omit it here and study the
baseline where actions of all agents are marginalized. In multi-agent settings, the calculation of this
baseline requires computing an expectation over the joint action space, which is generally intractable.
To solve this problem, we estimate the expectation by sampling.
We compare stochastic DOP, COMA, and On-Policy DOP against this method, which we call Regular
Critics with Baseline. Results are shown in Fig. 8. We can see that Regular Critics with Baseline
performs better than COMA. However, Regular Critics with Baseline performs worse than On-Policy
DOP. These results indicate that a linearly decomposed critic can reduce variance in policy gradients
more efficiently.
H	Infrastructure, architecture, and hyperparameters
Experiments are carried out on NVIDIA P100 GPUs and with fixed hyper-parameter settings, which
are described in the following sections.
22
Published as a conference paper at ICLR 2021
----Stochastic DOP (OUrS) ----------On-PoIicy DOP --------- COMA ---------- Regular Critics With Baseline
100
50
<⅛ U一M 4SE
<⅛ U一M 4SE
105
0	2	4	6	8	10
T (mil)
^0	1	2	3	4	5
T (mil)
3svs3z
2	4	6	8
T (mil)
IOmvsllm
0	1	2	3	4	5
T (mil)
O O
O 5
U一M 4S01
0	1	2	3	4	5
T (mil)
O O
O 5
U一M 4S01
Figure 8: Using baselines where actions of all other agents are marginalized within a centralized
critic is more efficient than COMA, but less efficient than a decomposed critic.
H.1 Stochastic DOP
In stochastic DOP, each agent has a neural network to approximate its local utility. The local utility
network consists of two 256-dimensional fully-connected layers with ReLU activation. Since the
critic is not used when execution, we condition local Q networks on the global state s. The output of
the local utility networks is Qφi (T, ∙) for each possible local action, which are then linearly combined
to get an estimate of the global Q value. The weights and bias of the linear combination, ki and b, are
generated by linear networks conditioned on the global state s. ki is enforced to be non-negative by
applying absolute activation at the last layer. We then divide ki by Pi ki to scale ki to [0, 1].
The local policy network consists of three layers, a fully-connected layer, followed by a 64 bit GRU,
and followed by another fully-connected layer that outputs a probability distribution over local actions.
We use ReLU activation after the first fully-connected layer.
For all experiments, we set κ = 0.5 and use an off-policy replay buffer storing the latest 5000
episodes and an on-policy buffer with a size of 32. We run 4 parallel environments to collect data.
The optimization of both the critic and actors is conducted using RMSprop with a learning rate of
5 × 10-4 , α of 0.99, and with no momentum or weight decay. For exploration, we use -greedy
with annealed linearly from 1.0 to 0.05 over 500k time steps and kept constant for the rest of the
training. Mixed batches consisting of 32 episodes sampled from the off-policy replay buffer and 16
episodes sampled from the on-policy buffer are used to train the critic. For training actors, we sample
16 episodes from the on-policy buffer each time. The framework is trained on fully unrolled episodes.
The learning rates for the critic and actors are set to 1 × 10-4 and 5 × 10-4, respectively. And we use
5-step decomposed multi-agent tree backup. All experiments on StarCraft II use the default reward
and observation settings of the SMAC benchmark.
H.2 Deterministic DOP
The critic network structure of deterministic DOP is similar to that of stochastic DOP, except that
local actions are part of the input in deterministic DOP. For actors, we use a fully-connected forward
network with two 64-dimensional hidden layers with ReLU activation, and the output of actors is
a local action. We use an off-policy replay buffer storing the latest 10000 transitions, from which
1250 transitions are sampled each time to train the critic and actors. The learning rates of both the
critic and actors are set to 5 × 10-3. To reduce variance in the updates of actors, we update the actors
and target networks only after 2 updates to the critic, as proposed by Fujimoto et al. (2018). We also
use this technique of delaying policy update in all the baselines. For all the algorithms, we run a
single environment to collect data, because we empirically find it more sample efficient than parallel
environments in the MPE benchmark. RMSprop with a learning rate of 5 × 10-4, α of 0.99, and
with no momentum or weight decay is used to optimize the critic and actors, which is the same as in
stochastic DOP.
23
Published as a conference paper at ICLR 2021
I Related works
Cooperative multi-agent reinforcement learning provides a scalable approach to learning collaborative
strategies for many challenging tasks (Vinyals et al., 2019; Berner et al., 2019; Samvelyan et al.,
2019; Jaderberg et al., 2019) and a computational framework to study many problems, including
the emergence of tool usage (Baker et al., 2020), communication (Foerster et al., 2016; Sukhbaatar
et al., 2016; Lazaridou et al., 2017; Das et al., 2019), social influence (Jaques et al., 2019), and
inequity aversion (Hughes et al., 2018). Recent work on role-based learning (Wang et al., 2020c;
2021) introduces the concept of division of labor into multi-agent learning and grounds MARL into
more realistic applications.
Centralized learning of joint actions can handle coordination problems and avoid non-stationarity.
However, the major concern of centralized training is scalability, as the joint action space grows
exponentially with the number of agents. The coordination graph (Guestrin et al., 2002b;a) is a
promising approach to achieve scalable centralized learning, which exploits coordination indepen-
dencies between agents and decomposes a global reward function into a sum of local terms. Zhang
& Lesser (2011) employ the distributed constraint optimization technique to coordinate distributed
learning of joint action-value functions. Sparse cooperative Q-learning (Kok & Vlassis, 2006) learns
to coordinate the actions of a group of cooperative agents only in the states where such coordination
is necessary. These methods require the dependencies between agents to be pre-supplied. To avoid
this assumption, value function decomposition methods directly learn centralized but factorized
global Q-functions. They implicitly represent the coordination dependencies among agents by the
decomposable structure (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Wang et al.,
2020e). The stability of multi-agent off-policy learning is a long-standing problem. Foerster et al.
(2017); Wang et al. (2020a) study this problem in value-based methods. In this paper, we focus on how
to achieve efficient off-policy policy-based learning. Our work is complementary to previous work
based on multi-agent policy gradients, such as those regarding multi-agent multi-task learning (Teh
et al., 2017; Omidshafiei et al., 2017) and multi-agent exploration (Wang et al., 2020d).
Multi-agent policy gradient algorithms enjoy stable convergence properties compared to value-based
methods (Gupta et al., 2017; Wang et al., 2020a) and can extend MARL to continuous control
problems. COMA (Foerster et al., 2018) and MADDPG (Lowe et al., 2017) propose the paradigm of
centralized critic with decentralized actors to deal with the non-stationarity issue while maintaining
decentralized execution. PR2 (Wen et al., 2019) and MAAC (Iqbal & Sha, 2019) extend the CCDA
paradigm by introducing the mechanism of recursive reasoning and attention, respectively. Another
line of research focuses on fully decentralized actor-critic learning (Macua et al., 2017; Zhang et al.,
2018; Yang et al., 2018; Cassano et al., 2018; Suttle et al., 2019; Zhang & Zavlanos, 2019). Different
from the setting of this paper, agents have local reward functions and full observation of the true state
in these works.
24