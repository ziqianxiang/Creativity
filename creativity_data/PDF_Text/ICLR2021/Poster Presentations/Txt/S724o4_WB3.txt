Published as a conference paper at ICLR 2021
When Does Preconditioning Help or Hurt Gen-
eralization?
*Shun-ichi Amari1, Jimmy Ba2,3, Roger Grosse2,3, XueChenLi4, Atsushi Nitanda5,6,
Taiji Suzuki5,6, Denny Wu2,3, Ji Xu7
1RIKEN CBS, 2University of Toronto, 3Vector Institute, 4Google Research, Brain Team,
5University of Tokyo, 6RIKEN AIP, 7ColumbiaUniversity
amari@brain.riken.jp, {jba,rgrosse,lxuechen,dennywu}@cs.toronto.edu,
{nitanda,taiji}@mist.i.u-tokyo.ac.jp, jixu@cs.columbia.edu
Ab stract
While second order optimizers such as natural gradient descent (NGD) often speed
up optimization, their effect on generalization has been called into question. This
work presents a more nuanced view on how the implicit bias of optimizers affects
the comparison of generalization properties. We provide an exact asymptotic bias-
variance decomposition of the generalization error of preconditioned ridgeless re-
gression in the overparameterized regime, and consider the inverse population
Fisher information matrix (used in NGD) as a particular example. We determine
the optimal preconditioner P for both the bias and variance, and find that the rela-
tive generalization performance of different optimizers depends on label noise and
“shape” of the signal (true parameters): when the labels are noisy, the model is
misspecified, or the signal is misaligned with the features, NGD can achieve lower
risk; conversely, GD generalizes better under clean labels, a well-specified model,
or aligned signal. Based on this analysis, we discuss several approaches to man-
age the bias-variance tradeoff, and the potential benefit of interpolating between
first- and second-order updates. We then extend our analysis to regression in the
reproducing kernel Hilbert space and demonstrate that preconditioning can lead
to more efficient decrease in the population risk. Lastly, we empirically compare
the generalization error of first- and second-order optimizers in neural network
experiments, and observe robust trends matching our theoretical analysis.
1	Introduction
We study the generalization property of an estimator θ obtained by minimizing the empirical risk
(or the training error) L(fθ) via a preconditioned gradient update with preconditioner P:
θt+ι = θt- ηP (t)Vθt L(fθt), t = 0,1,...	(1.1)
Setting P = I recovers gradient descent (GD). Choices of P which exploit second-order informa-
tion include the inverse Fisher information matrix, which gives the natural gradient descent (NGD)
(Amari, 1998); the inverse Hessian, which leads to Newton’s method (LeCun et al., 2012); and
diagonal matrices estimated from past gradients, which include various adaptive gradient meth-
ods (Duchi et al., 2011; Kingma & Ba, 2014). These preconditioners often alleviate the effect
of pathological curvature and speed up optimization, but their impact on generalization has been
under debate: Wilson et al. (2017) reported that in neural network optimization, adaptive or second-
order methods generalize worse compared to gradient descent (GD), whereas other empirical studies
showed that second-order methods achieve comparable, if not better generalization (Xu et al., 2020).
The generalization property of optimizers relates to the discussion of implicit bias (Gunasekar et al.,
2018a), i.e. preconditioning may lead to a different converged solution (with potentially the same
training loss), as illustrated in Figure 1. While many explanations have been proposed, our starting
point is the well-known observation that GD often implicitly regularizes the parameter `2 norm. For
instance in overparameterized least squares regression, GD and many first-order methods find the
minimum `2 norm solution from zero initialization (without explicit regularization), but precondi-
tioned updates may not. This being said, while the minimum `2 norm solution can generalize well
* Alphabetical ordering. Correspondence to: Denny Wu (dennywu@cs.toronto.edu).
1
Published as a conference paper at ICLR 2021
in the overparameterized regime (Bartlett et al., 2019), it is unclear whether preconditioning leads
to inferior solutions - even in the simple setting of overparameterized linear regression, quantitative
understanding of how preconditioning affects generalization is largely lacking.
Motivated by the observations above, in Section 3 we start with
overparameterized least squares regression (unregularized) and
analyze the stationary solution (t → ∞) of update (1.1) under
time-invariant preconditioner. Extending previous analysis in
the proportional limit (Hastie et al., 2019), we consider a more
general random design setting and derive the exact population
risk in its bias-variance decomposition. We characterize the op-
timal P within a general class of preconditioners for both the
bias and variance, and focus on the comparison between GD,
for which P is identity, and NGD, for which P is the inverse
population Fisher information matrix1. We find that the com-
parison of generalization is affected by the following factors:
Figure 1: 1D illustration of different
implicit biases: two-layer sigmoid net-
work trained with preconditioned GD.
1.	Label Noise: Additive noise in the labels leads to the variance term in the risk. We prove that
NGD achieves the optimal variance among a general class of preconditioned updates.
2.	Model Misspecification: Under misspecification, there does not exist a perfect fθ that recovers
the true function (target). We argue that this factor is similar to additional label noise, and thus
NGD may also be beneficial when the model is misspecified.
3.	Data-Signal-Alignment: Alignment describes how the target signal distributes among the in-
put features. We show that GD achieves lower bias when signal is isotropic, whereas NGD is
preferred under “misalignment” — when the target function focuses on small feature directions.
Beyond the decomposition of stationary risk, our findings in Section 4 and 5 are summarized as:
•	In Section 4.1 and 4.2 we discuss how the bias-variance tradeoff can be realized by different
choices of preconditioner P (e.g. interpolating between GD and NGD) or early stopping.
•	In Section 4.3 we extend our analysis to regression in the RKHS and show that under early stop-
ping, a preconditioned update interpolating between GD and NGD achieves minimax optimal
convergence rate in much fewer steps, and thus reduces the population risk faster than GD.
•	In Section 5 we empirically test how well our findings in linear model carry over to neural net-
works: under a student-teacher setup, we compare the generalization of GD with preconditioned
updates and illustrate the influence of all aforementioned factors. The performance of neural net-
works under a variety of manipulations results in trends that align with our theoretical analysis.
2	Background and Related Works
Natural Gradient Descent. NGD is a second-order method originally proposed in Amari (1997).
Consider a data distribution p(x) on the space X, a function fθ : X → Z parameterized by θ, and a
loss function L(X, fθ) = 1 Pn=1 l(yi, fθ(xi)), where l : Y ×Z → R. Also suppose a probability
distribution p(y∣z) = p(yfθ(X)) is defined on the space of labels. Then, the natural gradient is
defined as: VθL(X,fθ) = F-1VθL(X,f), where F = E[Vθ logp(x,y∣θ)Vθ logp(x,y∣θ)>]
is the Fisher information matrix, or simply the (population) Fisher. Note that expectations in the
Fisher are under the joint distribution of the model p(x, y∣θ) = p(x)p(y∣fθ (x)). In the literature,
the Fisher is sometimes defined under the empirical data distribution {xi}in=1 (Amari et al., 2000).
We instead refer to this quantity as the sample Fisher, the properties of which influence optimization
and have been studied in Karakida et al. (2018); Kunstner et al. (2019); Thomas et al. (2020). We
remark that in linear and kernel regression under squared loss, sample Fisher-based updates give the
same stationary solution as GD (see Section 3), whereas population Fisher-based update may not.
While the population Fisher is typically difficult to obtain, extra unlabeled data can be used in its
estimation, which empirically improves generalization (Pascanu & Bengio, 2013). Moreover, under
structural assumptions, parametric approaches to estimate F can be more sample-efficient (Martens
& Grosse, 2015; Ollivier, 2015), and thus closing the gap between sample and population Fisher.
1From now on we use NGD to denote the population Fisher-based update, and we write “sample NGD”
when P is the inverse or pseudo-inverse of the sample Fisher; see Section 2 for discussion.
2
Published as a conference paper at ICLR 2021
When the per-instance loss is the negative log-probability ofan exponential family, the sample Fisher
coincides with the generalized Gauss-Newton matrix (Martens, 2014). In least squares regression,
which is the focus of this work, the quantity also coincides with the Hessian. We thus take NGD as
a representative example of preconditioned update, and we expect our findings to also translate to
other second-order methods (not including adaptive gradient methods) in regression problems.
Analysis of Preconditioned Gradient Descent. While Wilson et al. (2017) outlined one example
under fixed training data where GD generalizes better than adaptive methods, in the online learning
setting, for which optimization speed relates to generalization, several works have shown the advan-
tage of preconditioning (Levy & Duchi, 2019; Zhang et al., 2019a). In addition, Zhang et al. (2019b);
Cai et al. (2019) established convergence and generalization guarantees of sample Fisher-based up-
dates for neural networks in the kernel regime. Lastly, the generalization of different optimizers
relates to the notion of “sharpness” (Keskar et al., 2016; Dinh et al., 2017), and it has been argued
that second-order updates tend to find sharper minima (Wu et al., 2018).
We note that two concurrent works also discussed the generalization performance of preconditioned
updates. Wadia et al. (2020) connected second-order methods with data whitening in linear models,
and qualitatively showed that whitening (thus second-order update) harms generalization in certain
cases. Vaswani et al. (2020) analyzed the complexity of the maximum P -margin solution in linear
classification problems. We emphasize that instead of upper bounding the risk (e.g. Rademacher
complexity), which may not decide the optimal P for generalization error, we compute the exact
risk for least squares regression, which allows us to precisely compare different preconditioners.
3	Asymptotic Risk of Ridgeles s Interpolants
In this section we consider the following setup: given n training samples {xi}in=1 labeled by a
teacher model (target function) f *: Rd →R with additive noise: yi = f * (Xi) + 已％, We learn a linear
student model fθ by minimizing the squared loss: L(X, fθ) = Pn=1(yi - x>θ)2. We assume a
random design: Xi = ∑X2Zi, where Zi ∈ Rd is an i.i.d. vector with zero-mean, unit-variance, and
finite 12th moment, and ε is i.i.d. noise independent to z with mean 0 and variance σ2. Our goal is
to compute the population risk R(f) = Ex [(f* (X) - f(X))2] in the proportional asymptotic limit:
•	(A1) Overparameterized Proportional Limit: n, d → ∞, d/n → γ ∈ (1, ∞).
(A1) entails that the number of features (or parameters) is larger than the number of samples, and
there exist multiple empirical risk minimizers with potentially different generalization properties.
Denote X = [X1>, ..., Xn>]> ∈ Rn×d the data matrix and y ∈ Rn the corresponding label vector.
We optimize the parameters θ via a preconditioned gradient flow with preconditioner P (t) ∈ Rd×d,
dθ⑴一PwdL(Wt)) — 1 P(n X>㈤ xθ	θ0 — 0
~∂Γ = -p(t)-iθ(tr = np( )x (y -xθ(t)), θ(0) = 0.
(3.1)
In this linear setup, many common choices of preconditioner do not
change through time: under Gaussian likelihood, the sample Fisher
(and also Hessian) corresponds to the sample covariance X>X/n
up to variance scaling, whereas the population Fisher corresponds
to the population covariance F = Σχ. We thus limit our analysis
to fixed preconditioner of the form P(t) =: P.
Write parameters at time t under update (3.1) with fixed P as
Θp(t). For positive definite P, the stationary solution is given as:
0p =limt→∞ θp(t) = PXτ(XPX>)-1y. One may check that
discrete time gradient descent update (with appropriate step size)
and other variants that do not alter the span of gradient (e.g. stochas-
tic gradient or momentum) converge to the same solution as well.
Figure 2: Geometric illustration
(2D) of how the interpolating θp
depends on the preconditioner.
Intuitively speaking, if the data distribution (blue contour in Figure 2) is not isotropic, then certain
directions will be more “important” than others. In this case uniform '2 shrinkage (which GD
implicitly provides) may not be most desirable, and certain P that takes data geometry into account
may lead to better generalization instead. The above intuition will be made rigorous in this section.
3
Published as a conference paper at ICLR 2021
τr*   .ι A ∙ ,1	∙	Il Zk Il	∙,	j , A	♦ Il Zkll	i F7 Λ	C
Remark. θP is the minimum kθkP-1 norm interpolant: θP = arg minθ kθkP -1 , s.t. Xθ = y for
positive definite P. For GD this translates to the parameter `2 norm, whereas for NGD (P = F-1),
the implicit bias is the kθkΣ norm. Since E[f (x)2] = kθk2Σ , NGD finds an interpolating function
with smallest norm under the data distribution. We empirically observe this divide between small
parameter norm and function norm in neural networks as well (see Figure 1 and Appendix A.1).
We highlight the following choices of P and the corresponding stationary solution θp as t → ∞.
•	Identity: P=Id recovers GD that converges to the min `2 norm
interpolant (also true for momentum GD and SGD), which we
write as θɪ := X> (XX>)-1y and refer to as the GD solution.
•	Population Fisher: P = F -1 = Σ-X1 leads to the estimator
θF-1, which we refer to as the NGD solution.
•	Sample Fisher: since the sample Fisher is rank-deficient, we
may add a damping term P = (X>X + λId)-1 or take the
pseudo-inverse P = (X>X)L In both cases, the gradient is
still spanned by X, and thus the update finds the same min `2-
norm solution 0i (also true for full-matrix Adagrad (Agarwal
et al., 2018)), although the trajectory differs (see Figure 3).
Remark. The above choices reveal a gap between sample- and
population-based P: while the sample Fisher accelerates opti-
mization (Zhang et al., 2019b), the following sections demonstrate
generalization properties only possessed by the population Fisher.
0.75
0.70
⅛
■e 0.65
U
∙∣ 0.60
京0.55
a.
0.50
0.45
Figure 3: Population risk of
preconditioned linear regression
vs. time with the following P : I
(red), Σx1 (blue) and (X>X)t
(cyan). Time is rescaled differ-
ently for each curve (convergence
speed is not comparable). Ob-
serve that GD and sample NGD
give the same stationary risk.
We compare the population risk of the GD solution θI and NGD solution θF-1
in its bias-variance
decomposition w.r.t. label noise (Hastie et al., 2019) and discuss the two components separately,
R(θ) = Eχ[(f *(x)-hx, Eε[θ]i)2] + tr(Cov(θ)∑x).
|
} '------------{---------}
V (θ), variance
B(θ), bias
Note that the bias does not depend on label noise ε, and the variance does not depend on the teacher
model f *. Additionally, given that f * can be independently decomposed into a linear component
on features X and a residual: f *(x) =(x, θ*i + fC(x), We can separate the bias term into a Well-
specified component kθ * - Eθ k2Σ , which captures the difficulty in learning θ* , and a misspecified
component, which corresponds to the error due to fitting fc* (beyond what the student can represent).
3.1	The Variance Term: NGD is Optimal
We first characterize the stationary variance which is independent to the teacher model f*. We
restrict ourselves to preconditioners satisfying the following assumption on the spectral distribution:
• (A2) Converging Eigenvalues: P is positive definite and as n, d → ∞, the spectral distribution
of Σχp := P1/2 ΣχP1/2 converges weakly to HXP supported on [c, C] for c,C > 0.
The following theorem characterizes the asymptotic variance and the corresponding optimal P.
Theorem 1. Given (A1-2), the asymptotic variance is given as
V(θp) → σ2 ( lim m0(-λ)m-2(-λ) — 1),	(3.2)
λ→0+
where m(z) > 0 is the Stieltjes transform of the limiting distribution of eigenvalues of nXPX>
satisfying the self-consistent equation m-1(z) = -z + γ τ(1 + τ m(z))-1dH XP (τ).
-1
Furthermore, under (A1-2), V(θ P) ≥ σ2(γ - 1)-1, and the equality is achieved by P = F-1.
Formula (3.2) is a direct extension of (Hastie et al., 2019, Thorem 4), which can be obtained from
the general results of Ledoit & PeChe (2011); Dobriban et al. (2018). Theorem 1 implies that precon-
ditioning with the inverse population Fisher F results in the optimal stationary variance, which is
supported by Figure 5(a). In other words, when the labels are noisy so that the risk is dominated by
4
Published as a conference paper at ICLR 2021
(a) variance.
(b) well-specified bias (isotropic).
(c) well-specified bias (misaligned).
Figure 5: We set eigenvalues of ΣX as two point masses with κX = 20 and kΣX k2F = d; empirical values
(dots) are computed with n = 300. (a) NGD (blue) achieves minimum variance. (b) GD (red) achieves lower
bias under isotropic signal: Σθ = Id. (c) NGD achieves lower bias under “misalignment”: ΣX = Σθ-1.
the variance, we expect NGD to generalize better upon convergence. We emphasize that this advan-
tage is only present when the population Fisher is used, but not its sample-based counterpart (which
gives θI). In Appendix A.3 we discuss the sample complexity of estimating F from unlabeled data.
Misspecification ≈ Label Noise. Under model misspecification, there does not exist a linear stu-
dent that perfectly recovers the teacher f *, which we may decompose as: f *(x) = x>θ* + fC(x).
In the simple case where f* is an independent linear function on unobserved features (Hastie et al.,
2019, Section 5): yi = xi>θ*+xc>,iθc+εi, where xc,i ∈ Rdc is independent to xi, we can show that
the additional error in the bias term due to misspecification is analogous to the variance term above:
Corollary 2. Under (A1)(A2), for the above unobserved features
model with E[xcxc>] = ΣcX and E[θcθc>] = dc-1 Σcθ, the addi-
tional error due to misspecification can be written as Bc(θP ) =
d-1tr(∑x∑θ)(V(θp) + 1), where V(θp) is the variance in (3.2).
In this case, misspecification can be interpreted as additional label
noise, for which NGD is optimal by Theorem 1. While Corollary 2
describes one specific example of misspecification, we may expect
similar outcome under broader settings. In particular, (Mei & Mon-
tanari, 2019, Remark 5) indicates that for many nonlinear fc* , the
misspecified bias is same as variance due to label noise. We empir-
ically observe similar findings under general covariance in Figure 4,
in which fc* is a quadratic function. Observe that NGD leads to lower
bias compared to GD as we further misspecify the teacher model.
2.0
1.5
≡ 1.0
0.5
Figure 4: Misspecified bias
with Σθ =Id (favors GD) and
fc (x) = α(x>X -tr(Σχ)),
where α controls the extent
of nonlinearity. Predictions
are generated by matching σ2
with second moment of f *.
3.2 The Bias Term: Alignment and “Difficulty” of Learning
We now analyze the bias term when the teacher model is linear on the input features (hence well-
specified): f* (x) = x>θ*. Extending the random effects hypothesis in Dobriban et al. (2018), we
consider a more general prior on θ*: E[θ* θ*>] = d-1Σθ, and assume the following joint relations2:
• (A3) Joint Convergence: ΣX and P share the same eigenvector matrix U. The empirical distri-
butions of elements of (ex, eθ, exp) jointly converge to random variables (υx, υθ, υxp) supported
on [c0, C0] for c0,C0 >0, where ex, exp are eigenvalues of ΣX, ΣXP, and eθ = diag (U>ΣθU).
We remark that when P =Id, previous works (Hastie et al., 2019; Xu & Hsu, 2019) considered the
special case of isotropic prior Σθ = Id. Our assumption thus allows for analysis of the bias term
under much more general Σθ3, which gives rise to interesting phenomena that are not captured by
simplified settings, such as non-monotonic bias and variance for γ > 1 (see Figure 15), and the
epoch-wise double descent phenomenon (see Appendix A.5). Under this general setup, we have the
following characterization of the asymptotic bias and the corresponding optimal preconditioner:
Theorem 3. Under (A1)(A3), the expected bias B(θp):= Ee* [B(θp)] IsgIvenas
B(θp) →→ lim m0(-λ)m-2(-λ)E[υχUθ(1 + Uχpm(-λ))-2],	(3.3)
2Note that (A2)(A3) covers many common choices of preconditioner, such as the population Fisher and
variants of the sample Fisher (which is degenerate but leads to the same minimum `2 norm solution as GD).
3Two concurrent works (Wu & Xu, 2020; Richards et al., 2020) also considered similar relaxation of Σθ in
the context of ridge regression.
5
Published as a conference paper at ICLR 2021
where expectation is taken over υ and m(z) is the Stieltjes transform defined in Theorem 1.
Furthermore, among all P satisfying (A3), the optimal bias is achieved by P = U diag (eθ)U>.
Note that the optimal P depends on the “orientation" of the teacher
model ∑θ , which is usually not known in practice. This result can thus
be interpreted as a no-free-lUnch characterization in choosing an opti-
mal preconditioner for the bias term a priori. As a consequence of the
theorem, when the true parameters θ* have roughly equal magnitude
(isotropic), GD achieves lower bias (see Figure 5(b) where ∑θ = Id).
On the other hand, NGD leads to lower bias when Σχ is “misaligned”
with ∑θ, i.e. when θ* focus on the least varying directions of input
features (see Figure 5(c) where ∑θ = Σχ1), in which case learning is
intuitively difficult since the features are not useful.
Figure 6: Illustration of
isotropic and misaligned θ*.
Connection to Source Condition. The “difficulty” of learning above relates to the source condi-
tion in RKHS literature (Cucker & Smale, 2002) (i.e., E[∑X2θ] < ∞, see (A4) in Section 4.3), in
which the coefficient r can be interpreted as a measure of “misalignment”. To elaborate this con-
nection, we consider the setting of ∑θ = ∑X: note that as r decreases, the teacher θ* focuses more
on input features with small magnitude, thus the learning problem becomes harder, and vice versa.
In this case we can show a clear transition in r for the comparison between GD and NGD.
Proposition 4 (Informal). When ∑θ = ∑X ,there exists a transition point r* ∈ ( — 1,0) such that
GD achieves lower (higher) stationary bias than NGD when r > (<) r*.
The above proposition confirms that for the stationary bias (well-specified), NGD outperforms GD
in the misaligned setting (i.e., small r), whereas GD has an advantage when the signal is aligned
(large r). For formal statement and more discussion on the transition point r* see Appendix A.2.
4 B ias -variance Tradeoff
Our characterization of stationary risk suggests that preconditioners that achieve the optimal bias and
variance are generally different. This section discusses how bias-variance tradeoff can be realized
by interpolating between preconditioners or by early stopping. Additionally, we analyze the non-
parametric least squares setting and show that by balancing the bias and variance, a preconditioned
update that interpolates between GD and NGD decreases the population risk faster than GD.
4.1	Interpolating between Preconditioners
Depending on the orientation of the teacher model, we may ex-
pect a bias-variance tradeoff in choosing P . Intuitively, given P 1
that minimizes the bias and P 2 that minimizes the variance, it is
possible that a preconditioner interpolating between P 1 and P2
can balance the bias and variance and thus generalize better. The
following proposition confirms this intuition in a setup of general
ΣX and isotropic Σθ4, for which GD (P = Id) achieves optimal
stationary bias and NGD (P=F-1) achieves optimal variance.
Figure 7: Bias-variance tradeoff
with κX = 25, Σθ = Id and
SNR=32/5. As we additively (ii)
or geometrically (iii) interpolate
from GD to NGD (left to right),
the stationary bias (blue) increases
and the stationary variance (or-
ange) decreases.
Proposition 5 (Informal). Let ΣX 6= Id and Σθ = Id. Consider
the following choices of interpolation scheme: (i) Pα = αΣ-X1 +
(1-α)I d, (ii) P α = (α∑χ + (1-α)I d)-1, (iii) P α = ∑χα .The
stationary variance monotonically decreases with α ∈ [0, 1] for all
three choices. For (i), the stationary bias monotonically increases
with α ∈ [0, 1], whereas for (ii) and (iii), the bias monotonically
increases with α in certain range that depends on ΣX.
In other words, as the signal-to-noise ratio (SNR) decreases, one can increase α, which makes the
update closer to NGD, to improve generalization, and vice versa. Indeed as shown in Figure 7
and 16(c), at a certain SNR, interpolating between Σ-X1 and Σθ can improve the stationary risk.
4Note that this reduces to the random effects model studied in Dobriban et al. (2018).
6
Published as a conference paper at ICLR 2021
Remark. Two of the aforementioned interpolation schemes reflect common practical choices: addi-
tive interpolation (ii) corresponds to the damping term to stably invert the Fisher, whereas geometric
interpolation (iii) resembles the “conservative” square-root scaling in adaptive gradient methods.
4.2	The Role of Early Stopping
Thus far we considered the stationary solution of the unregularized objective. It is known that the
bias-variance tradeoff can also be controlled by either explicit or algorithmic regularization. We
briefly comment on the effect of early stopping, starting from the monotonicity of the variance term.
Proposition 6. For all P satisfying (A2), the variance V (θP (t)) monotonically increases with time.
The proposition confirms the intuition that early stopping reduces overfitting. Variance reduction can
benefit GD in its comparison to NGD, which achieves lowest stationary variance: indeed, Figure 3
and 19 show that under early stopping, GD may be favored even if NGD has lower stationary risk.
On the other hand, early stopping may not improve the bias term. While a complete analysis is
difficult partially due to the potential non-monotonicity of the bias term (see Appendix A.5), we
speculate that previous findings for the stationary bias also translate to early stopping. As a con-
crete example, we consider well-specified settings in which either GD or NGD achieves the optimal
stationary bias, and demonstrate that such optimality is also preserved under early stopping:
Proposition 7. Given (A1) and denote the optimal early stopping bias as Bopt(θ) = inf t≥0B(θ(t)).
When Σθ = Σ-X1, we have Bopt(θP) ≥ Bopt (θF -1) for all P satisfying (A3). Whereas when
Σθ = Id, we have Bopt (θF -1 ) ≥ Bopt(θI).
Figure 19 illustrates that the observed trend in the stationary bias (well-specified) is indeed preserved
under optimal early stopping: GD or NGD achieves lower early stopping bias under isotropic or
misaligned teacher model, respectively. We leave a more precise characterization as future work.
4.3	Fast Decay of Population Risk
Our previous analysis suggests that certain preconditioners can achieve lower population risk, but
does not address which method decreases the risk more efficiently. Knowing that preconditioned
updates may accelerate optimization, one natural question to ask is, is this speedup also present for
generalization under fixed dataset? We answer this question in the affirmative in a slightly different
model: we study least squares regression in the RKHS, and show that a preconditioned update that
interpolates between GD and NGD achieves the minimax optimal rate in much fewer steps than GD.
We provide a brief outline and defer the details to Appendix D.9.1. Let H be an RKHS included in
L2 (PX ) equipped with a bounded kernel function k, and Kx ∈ H be the Riesz representation of
the kernel function. Define S as the canonical operator from H to L2(Pχ), and write Σ = S*S and
L = SS*. We aim to learn the teacher model f *, under the following standard regularity conditions:
•	(A4) Source Condition: ∃r∈(0,∞), M>0s.t.f*=Lrh* forh*∈L2(PX) and kf*k∞≤M.
•	(A5) Capacity Condition: ∃s > 1 s.t. tr(∑^S) < ∞ and 2r + s-1 > 1.
•	(A6) Regularity of RKHS: ∃μ ∈ [s-1,1],Cμ > 0 s.t. supχ∈suPP(PX)∣∣∑"-1"Kχ ∣∣H ≤ Cμ.
Note that in the source condition (A4), the coefficient r controls the complexity of the teacher model
and relates to the notions of model misalignment in Section 3.2: large r indicates a smoother teacher
model which is “easier” to learn, and vice versa5 (Steinwart et al., 2009). Given training points
{(xi, yi)}in=1, we consider the following preconditioned update on the student model ft ∈ H:
. . -1 . ^ ^ ,
ft = ft-1- η(∑ + αI)-1 (Σft-1- S*Y), fo = 0,	(4.1)
where Σ = 1 Pn=1 Kxi 0 Kxi, S* Y = 1 Pn=I yιKχi. In this setup, the population Fisher cor-
responds to the covariance operator Σ, and thus (4.1) can be interpreted as additive interpolation
between GD and NGD: update with large α behaves like GD, and small α like NGD. Related to our
update is the FALKON algorithm (Rudi et al., 2017) — a preconditioned gradient method for kernel
ridge regression. The key difference is that we consider optimizing the original objective (instead of
5We remark that most previous works considered the case where r ≥ 1/2 which implies f * ∈H.
7
Published as a conference paper at ICLR 2021
a regularized version as in FALKON) under early stopping. Importantly, since we aim to understand
how preconditioning affects generalization, explicit regularization should not be taken into account.
The following theorem shows that with appropriately chosen α, the preconditioned update (4.1)
leads to more efficient decrease in the population risk, due to faster decay of the bias term.
Theorem 8 (Informal). Under (A4-6), the population risk of ft can be written as R(ft) =
kSft - f *kL2(PX)≤ B(t)+V (t) defined inAppendix D.9. Given r ≥ 1/2 or μ ≤ 2r ,preconditioned
2s	2rs
update (4.1) with α = n- 2rs+1 achieves minimax optimal convergence rate R(ft) = Oln- 2rs+1 )
2rs
n 2rs+1 ) steps.
We comment that the optimal interpolation coefficient α and stopping time t are chosen to balance
the bias B(t) and variance V (t). Note that α depends on the teacher model in the following way: for
n > 1, α decreases as r becomes smaller, which corresponds to non-smooth and “difficult" f *, and
vice versa. This agrees with our previous observation that NGD is advantageous when the teacher
model is difficult to learn. We defer empirical verification of this result to Appendix C.
5	Neural Network Experiments
Protocol. We compare the generalization performance of GD and NGD in neural network settings
and illustrate the influence of the following three factors: (i) label noise; (ii) misspecification; (iii)
signal misalignment. We also verify the potential advantage of interpolating between GD and NGD.
We consider the MNIST and CIFAR-10 datasets. To create a student-teacher setup, we split the
training set into two halves, one of which (pretrain split) along with the original labels is used to
pretrain the teacher, and the other (distill split) along with the teacher’s labels is used to distill (Hin-
ton et al., 2015) the student. We take the teacher to be either a two-layer fully-connected ReLU
network or a ResNet (He et al., 2016), and the student is a two-layer ReLU network. We normalize
the teacher’s labels logits following Ba & Caruana (2014) before potentially adding label noise, and
fit the student by minimizing the L2 loss. Student models are trained on a subset of the distill split
with full-batch updates. We implement NGD using Hessian-free optimization (Martens, 2010). We
use 100k unlabeled data (possibly applying data augmentation) to estimate the population Fisher.
We report the test error when the training error is below 0.2% of its initial value as a proxy for the
stationary risk. We defer detailed setup to Appendix E and additional results to Appendix C.
5.1 Empirical Findings
GD	f	―⅛— GD	-- GD
ZCC/ICCCCC、	/	H ZCC/ICCCCc、	ʃɜ	in-2 : H ZCr
NGD (60000)
12
10-3
IO-4
0.00 0.25 0.50 0.75 1.001.25 1.501.75 2.00
Noise Standard Deviation
.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2
Γ
Teacher Pretrain Iters.
(a) label noise (MNIST). (b) misspecification (CIFAR-10).	(c) misalignment (MNIST).
Figure 8: Comparison between NGD and GD. Error bar is one standard deviation away from mean over five
independent runs. Numbers in parentheses denote amount of unlabeled examples for estimating the Fisher.
Label Noise. We pretrain the teacher with the full pretrain split and use 1024 examples from the
distill split to fit the student. For both the student and teacher, we use a two-layer ReLU net with 80
hidden units. We corrupt the labels with isotropic Gaussian noise of varying magnitude. Figure 8(a)
shows that as the noise level increases (the variance term begins to dominate), the stationary risk of
both NGD and GD worsen, with GD worsening faster, which aligns with our observation in Figure 5.
Misspecification. We use a ResNet-20 teacher and the same two-layer ReLU student from the
label noise experiment. We control the misspecification level by varying amount of pretraining
8
Published as a conference paper at ICLR 2021
of the teacher. Intuitively, large teacher models that are trained longer should be more complex
and thus likely to be outside of functions that a small two-layer student can represent (hence the
problem becomes misspecified). Indeed, Figure 8(b) shows that NGD eventually achieves better
generalization as the number of training steps for the teacher increases. In Appendix A.4 we report a
heuristic measure of model misspecification that relates to the NTK (Jacot et al., 2018), and confirm
that the quantity increases as more label noise is added or as the teacher model is trained longer.
Misalignment. We set the student and teacher to be the same two-layer ReLU network. We con-
struct the teacher model by perturbing the student’s initialization, the direction of which is given by
Fr, where F is the Fisher of the student model and r ∈ [-1, 0]. Intuitively, as r decreases, the impor-
tant parameters of the teacher (i.e. larger update directions) becomes misaligned with the student’s
gradient, and thus learning becomes more “difficult”. While this analogy is rather superficial due
to the non-convexity of neural network optimization, Figure 8(c) shows that as r becomes smaller
(setup is more misaligned), NGD begins to generalize better than GD (in terms of stationary risk).
Interpolating between Preconditioners. We validate our observations in Section 3 and 4 on the
difference between the sample Fisher and population Fisher, and the potential benefit of interpolating
between GD and NGD, in neural network experiments. Figure 9(a) shows that as we decrease the
number of unlabeled data in estimating the Fisher, which renders the preconditioner closer to the
sample Fisher, the stationary risk becomes more akin to that of GD, especially in the large noise
setting. This agrees with our remark on sample vs. population Fisher in Section 3 and Appendix A.1.
Figure 9(b)(c) supports the bias-variance tradeoff discussed in Section 4.1 in neural network settings.
In particular, we interpret the left end of the figure to correspond to the bias-dominant regime (due
to the same architecture for the student and teacher), and the right end to be the variance-dominant
regime (due to large label noise). Observe that at certain SNR, a preconditioner that interpolates
(additively or geometrically) between GD and NGD can achieve lower stationary risk.
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Noise Standard Deviation
(b) additive interpolation (ii)
between GD and NGD (MNIST).
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Noise Standard Deviation
(c) geometric interpolation (iii)
between GD and NGD (MNIST).
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Noise Standard Deviation
(a) interpolation between sample
and population Fisher (CIFAR-10).
Figure 9: (a) numbers in parentheses indicate the amount of unlabeled data used in estimating the Fisher F ;
we expect the estimated Fisher to be closer to the sample Fisher when the number of unlabeled data is small. (a)
additive interpolation P = (F + αId)-1; larger damping parameter yields update closer to GD. (b) geometric
interpolation P = F-α; larger α parameter yields update closer to that of NGD (blue).
6	Discussion and Conclusion
We analyzed the generalization properties of a general class of preconditioned gradient descent in
overparameterized least squares regression, with particular emphasis on natural gradient descent.
We identified three factors that affect the comparison of generalization performance of different
optimizers, the influence of which we also empirically observed in neural network6. We then de-
termined the optimal preconditioner for each factor. While the optimal P is usually not known in
practice, we provided justification for common algorithmic choices by discussing the bias-variance
tradeoff. Note that our current theoretical setup is limited to fixed preconditioners or those that do
not alter the span of gradient, and thus does not cover many adaptive gradient methods; understand-
ing these optimizers in similar setting would be an interesting future direction. Another important
problem is to further characterize the interplay between preconditioning and explicit (e.g. weight
decay7) or algorithmic regularization (e.g. large step size and gradient noise).
6We however note that observations in linear model may not translate to neural network: many works have
illustrated such a gap (e.g., Ghorbani et al. (2019); Allen-Zhu & Li (2019); Suzuki (2020); Yang & Hu (2020)).
7In a companion work (Wu & Xu, 2020) we characterized the impact of `2 regularization in similar settings.
9
Published as a conference paper at ICLR 2021
Acknowledgement
The authors would like to thank Murat A. Erdogdu, Fartash Faghri, Ryo Karakida, Yiping Lu,
Jiaxin Shi, Shengyang Sun, Yusuke Tsuzuku, Guodong Zhang, Michael Zhang, Tianzong Zhang,
and anonymous ICLR reviewers for helpful feedback. The authors are also grateful to Tomoya
Murata for his contribution to preliminary studies on the nonparametric least squares setting.
JB and RG were supported by the CIFAR AI Chairs program. JB and DW were partially supported
by LG Electronics and NSERC. AN was partially supported by JSPS Kakenhi (19K20337) and
JST-PRESTO. TS was partially supported by JSPS Kakenhi (26280009, 15H05707 and 18H03201),
Japan Digital Design and JST-CREST. JX was supported by a Cheung-Kong Graduate School of
Business Fellowship. Resources used in preparing this research were provided, in part, by the
Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the
Vector Institute.
References
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variance decomposition. arXiv preprint arXiv:2011.03321, 2020.
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
The case for full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares. In International Conference on Artificial Intelligence and Statistics, volume 22, 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Shun-ichi Amari. Neural learning in structured parameter spaces-natural riemannian gradient. In
Advances in neural information processing Systems, pp. 127-133, 1997.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Shun-Ichi Amari, Hyeyoung Park, and Kenji Fukumizu. Adaptive method of realizing natural gra-
dient learning for multilayer perceptrons. Neural computation, 12(6):1399-1409, 2000.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7411-7422, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized
nonlinear models: Convergence, implicit regularization, and generalization. arXiv preprint
arXiv:1906.03830, 2019.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural informa-
tion processing systems, pp. 2654-2662, 2014.
Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of
two-layer neural networks: An asymptotic viewpoint. International Conference on Learning
Representations, 2020.
Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108-127. World Scientific, 2008.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. arXiv preprint arXiv:1906.11300, 2019.
10
Published as a conference paper at ICLR 2021
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018a.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. In Advances in neural information processing
systems,pp. 2300-2311, 2018b.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict
statistical optimality? arXiv preprint arXiv:1806.09471, 2018c.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems, pp. 12873-12884, 2019.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
A gram-gauss-newton method learning overparameterized deep neural networks for regression
problems. arXiv preprint arXiv:1905.11675, 2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Nello Cristianini, John ShaWe-taylor, Andre Elisseeff, and Jaz Kandola. On kernel-target alignment.
In Advances in Neural Information Processing Systems 14. Citeseer, 2001.
Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the
American mathematical society, 39(1):1-49, 2002.
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni. Can implicit bias explain generalization?
stochastic convex optimization as a case study. arXiv preprint arXiv:2003.06152, 2020.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensional binary linear classification. arXiv preprint arXiv:1911.05822, 2019.
Guillaume Desjardins, Razvan Pascanu, Aaron Courville, and Yoshua Bengio. Metric-free natural
gradient for joint-training of boltzmann machines. arXiv preprint arXiv:1301.3545, 2013.
Oussama Dhifallah and Yue M Lu. A precise performance analysis of learning With random features.
arXiv preprint arXiv:2008.11904, 2020.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247-279, 2018.
Bin Dong, Jikai Hou, Yiping Lu, and Zhihua Zhang. Distillation ≈ early stopping? harvesting
dark knoWledge utilizing anisotropic information retrieval for overparameterized neural netWork.
arXiv preprint arXiv:1910.01255, 2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(Jul):2121-2159, 2011.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Gener-
alisation error in learning With random features and the hidden manifold model. arXiv preprint
arXiv:2002.09339, 2020.
Behrooz Ghorbani, Song Mei, Theodor MisiakieWicz, and Andrea Montanari. Limitations of lazy
training of tWo-layers neural netWorks. arXiv preprint arXiv:1906.08899, 2019.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in deep linear neural netWorks. arXiv preprint arXiv:1904.13262, 2019.
11
Published as a conference paper at ICLR 2021
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems,pp. 6151-6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471, 2018b.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Magnus Rudolph Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear
systems, volume 49. 1952.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero
training loss after achieving zero training error? arXiv preprint arXiv:2002.08709, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798, 2019.
Ryo Karakida and Kazuki Osawa. Understanding approximate fisher information for fast conver-
gence of natural gradient descent in wide neural networks. Advances in Neural Information Pro-
cessing Systems, 33, 2020.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in
deep neural networks: Mean field approach. arXiv preprint arXiv:1806.01316, 2018.
Noureddine El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-
dimensional robust regression estimators: rigorous results. arXiv preprint arXiv:1311.2445, 2013.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical fisher approxi-
mation. arXiv preprint arXiv:1905.12558, 2019.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Olivier Ledoit and Sandrine Peche. Eigenvectors of some large sample covariance matrix ensembles.
Probability Theory and Related Fields, 151(1-2):233-264, 2011.
12
Published as a conference paper at ICLR 2021
Daniel Levy and John C Duchi. Necessary and sufficient geometries for gradient methods. In
Advances in Neural Information Processing Systems, pp. 11491-11501, 2019.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stop-
ping is provably robust to label noise for overparameterized neural networks. arXiv preprint
arXiv:1903.11680, 2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203,
2017.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel” ridgeless” regression can gener-
alize. arXiv preprint arXiv:1808.00387, 2018.
Zhenyu Liao and Romain Couillet. The dynamics of learning: a random matrix approach. arXiv
preprint arXiv:1805.11917, 2018.
Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. The
Journal of Machine Learning Research, 18(1):3375-3421, 2017.
Cosme Louart, Zhenyu Liao, Romain Couillet, et al. A random matrix approach to neural networks.
The Annals of Applied Probability, 28(2):1190-1248, 2018.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free opti-
mization. In Neural networks: Tricks of the trade, pp. 479-535. Springer, 2012.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Stanislav Minsker. On some extensions of Bernstein’s inequality for self-adjoint operators. Statistics
& Probability Letters, 127:111-119, 2017.
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-
margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. arXiv
preprint arXiv:1911.01544, 2019.
Tomoya Murata and Taiji Suzuki. Gradient descent in rkhs with importance labeling. arXiv preprint
arXiv:2006.10925, 2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019.
Yann Ollivier. Riemannian metrics for neural networks i: feedforward networks. Information and
Inference: A Journal of the IMA, 4(2):108-153, 2015.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gra-
dient descent on hard learning problems through multiple passes. In Advances in Neural Infor-
mation Processing Systems, pp. 8114-8124, 2018.
13
Published as a conference paper at ICLR 2021
Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable data. In Advances in
Neural Information Processing Systems,pp. 7759-7767, 2019.
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by
norms. arXiv preprint arXiv:2005.06398, 2020.
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less) regression
under general source condition. arXiv preprint arXiv:2006.06386, 2020.
Francisco Rubio and Xavier Mestre. Spectral convergence for a general class of random matrices.
Statistics & probability letters, 81(5):592-602, 2011.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.
In Advances in Neural Information Processing Systems, pp. 3215-3225, 2017.
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel
method. In Advances in neural information processing SyStemS, pp. 3888-3898, 2017.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Ingo Steinwart, Don R Hush, and Clint Scovel. Optimal rates for regularized least squares regres-
sion. In COLT., pp. 79-93, 2009.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approx-
imation perspective. In Advances in Neural Information Processing Systems, pp. 2637-2646,
2019.
Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regulariza-
tion paths. In Advances in Neural Information Processing Systems, pp. 10608-10619, 2018.
Taiji Suzuki. Generalization bound of globally optimal non-convex neural network training:
Transportation map estimation by infinite dimensional langevin dynamics. arXiv preprint
arXiv:2007.05824, 2020.
Valentin Thomas, Fabian Pedregosa, Bart Merrienboer, Pierre-Antoine Manzagol, YoshUa Bengio,
and Nicolas Le Roux. On the interplay between noise and curvature and its effect on optimization
and generalization. In International Conference on Artificial Intelligence and Statistics, pp. 3503-
3513, 2020.
Sharan Vaswani, Reza Babanezhad, Jose Gallego, Aaron Mishkin, Simon Lacoste-JUlien, and
Nicolas Le RoUx. To each optimizer a norm, to each norm its generalization. arXiv preprint
arXiv:2006.06821, 2020.
Neha S Wadia, Daniel DUckworth, SamUel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein.
Whitening and second order optimization both destroy information aboUt the dataset, and can
make generalization impossible. arXiv preprint arXiv:2008.07545, 2020.
Per-AkeWedin. Perturbation theory for pseudo-inverses. BIT Numerical Mathematics, 13(2):217-
232, 1973.
Francis Williams, Matthew Trager, Claudio Silva, Daniele Panozzo, Denis Zorin, and Joan Bruna.
Gradient dynamics of shallow univariate relu networks. arXiv preprint arXiv:1906.07842, 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148T158, 2017.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. arXiv
preprint arXiv:2002.09277, 2020.
14
Published as a conference paper at ICLR 2021
Denny Wu and Ji Xu. On the optimal weighted `2 regularization in overparameterized linear regres-
sion. Advances in Neural Information Processing Systems, 33, 2020.
Lei Wu, Chao Ma, and E Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv
preprint arXiv:1611.03131, 2016.
Ji Xu and Daniel Hsu. How many variables should be entered in a principal component regression
equation? arXiv preprint arXiv:1906.01139, 2019.
Peng Xu, Fred Roosta, and Michael W Mahoney. Second-order optimization for non-convex ma-
chine learning: An empirical study. In Proceedings of the 2020 SIAM International Conference
on Data Mining, pp. 199-207. SIAM, 2020.
Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. In Advances in Neural Information Processing Systems, pp. 8194-
8205, 2019a.
Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. In Advances in Neural Information Processing Systems,
pp. 8080-8091, 2019b.
15
Published as a conference paper at ICLR 2021
Table of Contents
A Discussion on Additional Results	17
A.1 Implicit Bias of GD vs. NGD ................................................. 17
A.2 Bias Term Under Specific Source Condition ................................... 18
A.3 Estimating the Population Fisher ............................................ 19
A.4 Interpretation of VZyTKTy/n.................................................. 19
A.5 Non-monotonicity of Bias Term w.r.t. Time ................................... 20
B	Additional Related Works	20
C	Additional Figures	21
C.1 Overparameterized Linear Regression ......................................... 21
C.2 RKHS Setting ................................................................ 22
C.3 Neural Network .............................................................. 23
D	Proofs and Derivations	24
D.1 Missing Derivations in Section 3 ............................................ 24
D.2 Proof of Theorem 1 .......................................................... 24
D.3 Proof of Corollary 2 ........................................................ 25
D.4 Proof of Theorem 3 .......................................................... 25
D.5	Proof of Proposition	4 ..................................................... 27
D.6	Proof of Proposition	5 ..................................................... 28
D.7	Proof of Proposition	6 ..................................................... 30
D.8	Proof of Proposition	7 ..................................................... 31
D.9 Proof of Theorem 8 .......................................................... 32
D.10 Proof of Corollary 9 ....................................................... 39
D.11 Proof of Proposition	10 .................................................... 39
E Experiment Setup	40
E.1 Processing the Datasets ..................................................... 40
E.2 Setup and Implementation for Optimizers ..................................... 40
E.3 Other Details ............................................................... 40
16
Published as a conference paper at ICLR 2021
A Discussion on Additional Results
A.1 Implicit Bias of GD vs. NGD
It is known that gradient descent is the steepest descent with respect to the `2 norm, i.e. the update
direction is constructed to decrease the loss under small changes in the parameters measured by the
`2 norm (Gunasekar et al., 2018a). Following this analogy, NGD is the steepest descent with respect
to the KL divergence on the predictive distributions (Martens, 2014); this can be interpreted as a
proximal update which penalizes how much the predictions change on the data distribution.
Intuitively, the above discussion suggests GD tend to find solution that is close to the initialization in
the Euclidean distance between parameters, whereas NGD prefers solution close to the initialization
in terms of function outputs on the data distribution. This observation turns out to be exact in the case
of ridgeless interpolant under the squared loss, as remarked in Section 3. Moreover, Figure 1 and 10
confirms the same trend in the optimization of overparameterized neural network. In particular,
•	GD results in small changes in parameters, whereas NGD results in small changes in the function.
•	Preconditioning with the pseudo-inverse of the sample Fisher, i.e., P = (J > J) LleadstoimPlicit
bias similar to that of GD (also noted in (Zhang et al., 2019b)), but different than NGD with the
population Fisher.
•	“Interpolating” between GD and NGD (green) results in properties in between GD and NGD.
Remark. Qualitatively speaking, the small change in the function output is the essential reason
that NGD performs well under noisy labels in the interpolation setting: NGD seeks to interpolate
the training data by changing the function only “locally”, so that memorizing the noisy labels has
small impact on the “global” shape of the learned function (see Figure 1).
(a) Difference in Parameters.
(b) Difference in Function Values.
Figure 10: Illustration of implicit bias of GD and NGD. We set n = 100, d = 50, and regress a two-layer
ReLU network with 50 hidden units towards a teacher model of the same architecture on Gaussian input. The
x-axis is rescaled for each optimizer such that the final training error is below 10-3. GD finds solution with
small changes in the parameters, whereas NGD finds solution with small changes in the function. Note that the
sample Fisher (cyan) has implicit bias similar to GD and does not resemble NGD (population Fisher).
We note that the above observation also implies that wide neural networks trained with NGD (popu-
lation Fisher) is less likely to stay in the kernel regime: the distance traveled from initialization can
be large (see Figure 10(a)) and thus the Taylor expansion around the initialization is no longer ac-
curate. In other words, the analogy between wide neural net and its linearized kernel model (which
we partially employed in Section 5) may not be valid in models trained by NGD8.
Implicit Bias of Interpolating Preconditioners. We also expect that as we interpolate from GD
to NGD, the distance traveled in the parameter space would gradually increase, and distance traveled
in the function space would decrease. Figure 11 demonstrate that this is indeed the case for neural
networks: we use the same two-layer MLP setup on MNIST as in Section 5. Observe that updates
closer to GD result in smaller change in the parameters, and ones close to NGD lead to smaller
change in the function outputs.
8Note that this gap is only present when the population Fisher is used; previous works have shown NTK-type
global convergence for sample Fisher-related update (Zhang et al., 2019b; Cai et al., 2019).
17
Published as a conference paper at ICLR 2021
0 5 0 5 0 5 0
4 3 3 2 2 1 1
-≡slps-
ι∂^* id-5 ι∂-s	16T	idq ι∂l
Damping
1 1■ — J J >1
~=X5I(XH≡
10-4 io-5 IoT IOT ιo0 ιol
Damping
■ ■ ■ ,，，，
-osIQs-
0：0	0'2	。：4	。：6	。：8
(a) additive interp.; (b) additive interp.; (c) geometric interp.; (d) geometric interp.;
difference in parameters. difference in functions. difference in parameters. difference in functions.
Figure 11: Illustration of the implicit bias of preconditioned gradient descent that interpolates between GD
and NGD on MNIST. Note that as the update becomes more similar to NGD (smaller damping or larger α), the
distance traveled in the parameter space increases, where as the distance traveled on the output space decreases.
Two Kinds of Second-order Optimizer. Note that our optimal preconditioner derived in Section 3
requires knowledge of population second-order statistics, which we empirically approximate using
extra unlabeled data. Consequently, our results suggest that different “types” of second-order infor-
mation (sample vs. population) may affect generalization differently. Broadly speaking, there are
two types of practical approximate second-order optimizers for neural networks. Some algorithms,
such as Hessian-free optimization (Martens, 2010; Martens & Sutskever, 2012; Desjardins et al.,
2013), approximate second-order matrices (typically the Hessian or Fisher) using the exact matrix
on finite training examples. In high-dimensional problems, this sample-based approximation can be
very different from the population quantity (e.g. it is degenerate in the overparameterized regime).
Other algorithms fit a parametric approximation to the Fisher, such as diagonal (Duchi et al., 2011;
Kingma & Ba, 2014), quasi-diagonal (Ollivier, 2015), or Kronecker-factored (Martens & Grosse,
2015). If the parametric assumption is accurate, these approximations are more statistically efficient
and thus may lead to better approximation to the population Fisher. Our analysis reveals a difference
(in generalization properties) between sample- and population-based preconditioned updates, which
may also suggest a separation between the two kinds of approximate second-order optimizers. As
future work, we intend to investigate this discrepancy in real-world problems.
A.2 Bias Term Under Specific Source Condition
Motivated by the connection between the notion of “alignment” and the source condition in Sec-
tion 3.2, We consider a specific case of θ*: ∑θ = ∑X, where r controls the extent of misalignment,
and Theorem 3 implies that the optimal preconditioner for the bias term (well-specified) is P = ΣrX .
Note that smaller r corresponds to more misaligned and thus “difficult” problem, and vice versa. In
this setup we have the following comparison between GD and NGD.
Proposition (Formal Statement of Proposition 4). Consider the setting of Theorem 3 and Σθ =
r
ΣrX, then for all r ≤ -1 we have B(θF-1) ≤ B(θI), whereas for all r ≥ 0, we have B(θF-1) ≥
B(θI); the equality is achieved when features are isotropic (i.e., ΣX = cId).
The proposition confirms the intuition that when parameters
of the teacher model θ* are more “aligned” with features X
than the isotropic setting (r ≥ 0), then GD achieves lower bias
than NGD; on the other hand, when Σθ is more “misaligned”
than the Σ-X1 case (r ≤ -1), then NGD is guaranteed to be
advantageous for the bias term. We therefore expect a tran-
sition from the NGD-dominated to GD-dominated regime for
some r ∈ (-1, 0). The exact value of r for such transition
depends on the spectral distribution of ΣX and varies case-
by-case (one would need to specifically evaluate the equality
(D.9)). To give a concrete example, when ΣX has a simple
block structure, we can explicitly determine the the transition
point r* ∈ (-1,0), as shown by the following corollary.
Figure 12: We set Σθ = ΣrX, γ =
2, κ = 20, and plot the stationary bias
(well-specified) under varying r.
Corollary 9. Assume Σθ = ΣrX, and eigenvalues of ΣX come from two equally-weighted point
masses with K，λmaχ(∑χ"λma(Σχ). WLOG we take tr(Σχ)/d = L Then given r* =
- ln cκ,γ / lnκ (see Appendix D.10 for definition), we have B(θI) R B(θF-1 ) if and only ifr Q r*.
18
Published as a conference paper at ICLR 2021
Remark. When Y = 2, the transition happens at r* = —1/2 which is independent of the condition
number K, as indicated by the dashed line in Figure 12. However for other γ > 1, r* generally
relates to both γ and κ.
Our characterization above is supported by Figure 12, in which we plot the bias term under varying
extent of misalignment (controlled by r) in the setting of Corollary 9. Observe that as we construct
the teacher model to be more “misaligned” (and thus difficult to learn) by decreasing r, NGD (blue)
achieves lower bias compared to GD (red), and vice versa.
A.3 Estimating the Population Fisher
Our analysis on linear model considers the idealized setup with access to the exact population Fisher,
which can be estimated using additional unlabeled data. In this section we discuss how our result
in Section 3 and Section 4 are affected when the population covariance is approximated from N
i.i.d. (unlabeled) samples Xu ∈ RN×d. For the ridgeless interpolant we have the following result
on the substitution error in replacing the true population covariance with the sample estimate.
Proposition 10. Given (A1)(A3) and N/d → ψ > 1 as d → ∞, let ΣX = XUXu/N, we have
(a)	k∑X — ςX k2 =O(ψ-1/2) almost surely.
(b)	Denote the stationary bias and variance of NGD (with the exact population Fisher) as B * and
V *, respectively, and the bias and variance of the preconditioned update using the approximate
Fisher F = ΣX as B and V , respectively. Let 0 <	< 1 be the desired accuracy. Then
ψ = Θ(e-2) suffices to achieve |B* — B| < e and |V* — V| < e.
Proposition 10 entails that when the preconditioner is a sample estimate of the Fisher F (based
on unlabeled data), we can approach the stationary bias and variance of the population Fisher at a
rate of ψ-1/2 as We increase the number of unlabeled data N linearly in the dimensionality d. In
other words, any non-vanishing accuracy e can be achieved with finite ψ (to push e → 0, additional
logarithmic factor is required, e.g. N = O(d logd), Which is beyond the proportional limit).
On the other hand, for our result in Section 4.3, (Murata & Suzuki, 2020, Lemma A.5) directly im-
plies that setting N = Θ(ns log n) is sufficient to approximate the population covariance operator
(i.e.,sothat k∑"∑N1λ2k = O(1)). Finally, We remark that our analysis above does not impose any
structural assumptions on the estimated matrix. When the Fisher exhibits certain structures (e.g. Kro-
necker factorization (Martens & Grosse, 2015)), then estimation can be more sample-efficient. For
analysis on such structured approximations of the Fisher see Karakida & OsaWa (2020).
A.4 INTERPRETATION OF √yτK-1y∕n
As a heuristic measure of model misspecification, in Figure 13
we report ∖fyτK-y∕n studied in Arora et al.(2019b), where
y is the label vector and K is the NTK matrix (Jacot et al.,
2018) of the student model. This quantity relates to kernel-
based alignment measures (Cristianini et al., 2001), and in the
context of neural network optimization, it can be interpreted
as a proxy for measuring how much signal and noise are dis-
tributed along the eigendirections of the NTK (e.g., see Li
et al. (2019); Dong et al/2019); Su & Yang (2019)). Roughly
speaking, large √yτK-1y∕n implies that the problem is “dif-
ficult” to learn by the student model via GD, and vice versa.
Figure 13: yzyτK-τy∕n on two-
layer neural network (CIFAR-10).
Here we give a heuristic argument on how this quantity relates to label noise and misspecification.
For the ridgeless regression model considered in Section 3, write yi = f*(xi) + fc(xi) + εi, where
f*(x) = x>θ*, fc is the misspecified component, and εi is the label noise, then we have
Ey>K-1y =E	(XX>)-1/2(f*(X) + fc(X) + ε)
2
2
(≈i)trθ*θ*>X>(XX>)-1X + (σ2 +σc2)tr(XX>)-1,
(A.1)
19
Published as a conference paper at ICLR 2021
where we heuristically replaced the misspecified component with i.i.d. noise of the same variance σc2.
The first term of (A.1) resembles an RKHS norm of the target θ*, whereas the second term is small
when the feature matrix is well-conditioned or when the level of label noise σ and misspecification
σc2 is small (note that these are conditions under which GD achieves good generalization). We may
expect similar behavior for neural networks close to the kernel regime. This provides a non-rigorous
explanation of the trend observed in Figure 13: as we increase the level of label noise or model
misspecification (by pretraining the teacher for more steps), the quantity of interest becomes larger.
A.5 Non-monotonicity of Bias Term w.r.t. Time
Many previous works on the high-dimensional characteri-
zation of linear regression assumed a random effects model
with an isotropic prior on the true parameters (Dobriban
et al., 2018; Hastie et al., 2019), which may not hold in
practice. As an example of the limitation of this assump-
tion, note that when Σθ = Id, it can be shown that the ex-
pected bias B(θ(t)) monotonically decreases through time
(see proof of Proposition 7). In contrast, when the target pa-
rameters do not follow an isotropic prior, the bias ofGD can
exhibit non-monotonicity, which gives rise to the “epoch-
wise double descent” phenomenon also observed in deep
learning (Nakkiran et al., 2019; Ishida et al., 2020).
1.0
0.8
0噂
⅛
0.4'c
0.2
Figure 14: Epoch-wise double descent.
Note that non-monotonicity of the bias
w.r.t. time is present in GD but not NGD.
We empirically demonstrate this non-monotonicity when the model is close to the interpolation
threshold in Figure 14. We set eigenvalues ofΣX tobe two point masses with κX = 32, Σθ = Σ-X1
and γ = 16/15. Note that the GD trajectory (red) exhibits non-monotonicity in the bias term,
whereas for NGD the bias is monotonically decreasing through time (which we confirm in the proof
of Proposition 7). We remark that this mechanism of epoch-wise double descent may not relate the
empirical findings in deep neural networks (the robustness of which is also largely unknown), in
which it is typically speculated that the variance term exhibits non-monotonicity.
B Additional Related Works
Implicit Regularization in Optimization. In overparameterized linear models, GD finds the min-
imum `2 norm solution under many loss functions. For the more general mirror descent, the implicit
bias is determined by the Bregman divergence of the update (Gunasekar et al., 2018b; Suggala et al.,
2018; Azizan et al., 2019). Under the exponential or logistic loss, recent works showed that GD
finds the max-margin direction in various models (Ji & Telgarsky, 2018; 2019; Soudry et al., 2018;
Lyu & Li, 2019; Chizat & Bach, 2020). The implicit bias of Adagrad has been analyzed under
similar setting (Qian & Qian, 2019). Implicit regularization also relates to the model architecture;
examples include matrix factorization (Gunasekar et al., 2017; Saxe et al., 2013; Gidel et al., 2019;
Arora et al., 2019a) and certain stylized neural networks (Li et al., 2017; Gunasekar et al., 2018b;
Williams et al., 2019; Woodworth et al., 2020). For wide networks in the kernel regime (Jacot et al.,
2018), the implicit bias of GD relates to properties of the neural tangent kernel (NTK) (Xie et al.,
2016; Arora et al., 2019b; Bietti & Mairal, 2019). Finally, we note that the implicit bias of GD is
not always explained by the minimum norm property (Razin & Cohen, 2020; Dauber et al., 2020).
Asymptotics of Interpolating Estimators. In Section 3 we analyzed overparameterized estima-
tors that interpolate the training data. Recent works have shown that interpolation may not lead to
overfitting (Liang & Rakhlin, 2018; Belkin et al., 2018c;b; Bartlett et al., 2019), and the optimal risk
may be achieved under no regularization and extreme overparameterization (Belkin et al., 2018a; Xu
& Hsu, 2019). The asymptotic risk of overparameterized models has been characterized in various
settings, such as linear regression (Karoui, 2013; Dobriban et al., 2018; Hastie et al., 2019), random
features regression (Mei & Montanari, 2019; Gerace et al., 2020; Dhifallah & Lu, 2020; Adlam &
Pennington, 2020), max-margin classification (Montanari et al., 2019; Deng et al., 2019), and certain
neural networks (Louart et al., 2018; Ba et al., 2020). Our analysis is based on random matrix theory
results developed in RUbio & Mestre (2011); Ledoit & Peche (2011). Similar tools can also be used
to study the gradient descent dynamics of linear regression (Liao & Couillet, 2018; Ali et al., 2019).
20
Published as a conference paper at ICLR 2021
C Additional Figures
C.1 Overparameterized Linear Regression
Non-monotonicity of the Risk. Under our generalized (anisotropic) assumption on the covariance
of the features and the target, both the bias and the variance term can exhibit non-monotonicity
w.r.t. the overparameterization level γ > 1: in Figure 15 we observe two peaks in the bias term and
three peaks in the variance term. In contrast, it is known that when ΣX = Id, both the bias and
variance are monotone in the overparameterized regime (e.g., Hastie et al. (2019)).
°,uu-,IB>
1.5	2.0	2.5	3.0	3.5
γ= d∣n
(a) variance.
Figure 15: Illustration of the “multiple-descent” curve of the risk for γ > 1. We take n = 300, eigenvalues of
ΣX as three equally-spaced point masses with κX = 5000 and kΣX k2F = d, and Σθ = Σ-X1 (misaligned).
Note that for GD, both the bias and the variance are highly non-monotonic for γ > 1.
0 5 0 5 0 5
3 2 2 1 1 0
Sqq
1.5	2.0	2.5	3.0	3.5
γ = d∕n
(b) bias (well-specified).
Additional Figures for Section 3 and 4. We include additional figures on (a) well-specified bias
when Σθ = Id (GD is optimal); (b) misspecified bias under unobserved features (predicted by
Corollary 2); (c) bias-variance tradeoff by interpolating between preconditioners (SNR=5). As
shown in Figure 16 and 17, in all cases the experimental values match the theoretical predictions.
(a) well-specified bias (aligned). (b) misspecified bias
(unobserved features).
(c) bias-variance tradeoff.
Figure 16: We set eigenvalues of ΣX as a uniform distribution with κX = 20 and kΣX k2F = d.
(c) bias-variance tradeoff.
(a) well-specified bias (aligned).	(b) misspecified bias
(unobserved features).
Figure 17:	We construct eigenvalues of ΣX with a polynomial decay: λi(ΣX) = i-1 and then rescale the
eigenvalues such that κX = 500 and kΣX k2F = d.
21
Published as a conference paper at ICLR 2021
Early Stopping Risk. Figure 18 compares the stationary risk with the optimal early stopping risk
under varying misalignment level. We set Σθ = ΣrX and vary r from 0 to -1. As discussed in
Section 3.2 smaller α entails more “misaligned” teacher, and vice versa. Note that as the problem
becomes more misaligned, NGD achieves lower stationary and early stopping risk.
Figure 19 reports the optimal early stopping risk under misspecification (same trend can be obtained
when the x-axis is label noise). In contrast to the stationary risk (Figure 4), GD can be advantageous
under early stopping even with large extent of misspecification (for isotropic teacher). This aligns
with our finding in Section 4.2 that early stopping reduces the variance and the misspecified bias.
-8-64
Ooo
sq AJeUos
0.2	0.4	0.6	0.8
extent of misalignment (-r)
(a) stationary risk.
∙7∙6∙54∙3
Ooooo
Se6uadoAμe
.0	0.2	0.4	0.6	0.8	1.0
extent of misalignment (-r)
(b) optimal early stopping risk.
Figure 18:	Well-specified bias against different extent of “alignment”. We set n = 300, eigenvalues of ΣX
as two point masses with κX = 20, and take Σθ = ΣrX and vary r from -1 to 0. (a) GD achieves lower bias
when ∑θ is isotropic, NGD dominates when Σχ = ∑-1, whereas P = ∑χ1/2 (interpolates between GD
and NGD) is advantageous in between. (b) optimal early stopping bias follows similar trend as stationary bias.
Q-8∙64
Iooo
、」6u-ddoAμe3
o.:
extent of misspecification
(a) optimal early stopping risk
(aligned & misspecified).
2 Q-8-6
Iloo
*sμ 6u-ddoAμe3
0.-
extent of misspecification
(b) optimal early stopping risk
(misaligned & misspecified).
Figure 19:	Optimal early stopping risk vs. increasing model misspecification. We follow the same setup as
Figure 5(c). (a) Σθ = Id (favors GD); unlike Figure 5(c), GD has lower early stopping risk even under large
extent of misspecification. (b) Σθ = Σ-X1 (favors NGD); NGD is also advantageous under early stopping.
C.2 RKHS Setting
We simulate the optimization in the coordinates of RKHS via a finite-dimensional approximation
(using extra unlabeled data). In particular, We consider the teacher model in the form of f *(x)=
PN=I hiμrΦi(x) for square summable {hi}N=ι, in which r controls the “difficulty" of the learning
problem. We find {μi}N=ι and {φi}N=ι by solving the eigenfunction problem for some kernel k.
The student model takes the form of f (x) = Pi= ι -ai-φi(x) and We optimize the coefficients
i ― 1 μ μi
{ai}iN=1 via the preconditioned update (4.1). We set n = 1000, d = 5, N = 2500 and consider the
inverse multiquadratic (IMQ) kernel: k(x, y) = l 1	=.
1+kx-yk22
Recall that Theorem 8 suggests that for small r, i.e. “difficult” problem, the damping coefficient λ
would need to be small (which makes the update NGD-like), and vice versa. This result is (quali-
tatively) supported by Figure 20, from which we can see that small λ is beneficial when r is small,
and vice versa. We remark that this observed trend is rather fragile and sensitive to various hyperpa-
rameters, and we leave a comprehensive characterization of this observation as future work.
22
Published as a conference paper at ICLR 2021
*SLI UO -l-ndod
*sμ uo=-ndod
steps	steps
(a) r = 3/4.	(b) r = 1/4.
Figure 20:	Population risk of the preconditioned update in RKHS that interpolates between GD and NGD. We
use the IMQ kernel and set n = 1000, d = 5, N = 2500, σ2 = 5 × 10-4. The x-axis has been rescaled for
each curve and thus convergence speed is not directly comparable. Note that (a) large λ (i.e., GD-like update)
is beneficial when r is large, and (b) small λ (i.e., NGD-like update) is beneficial when r is small.
C.3 Neural Network
Label Noise. In Figure 21, (a) we observe the same phenomenon on CIFAR-10 that NGD gener-
alizes better as more label noise is added to the labels, and vice versa. Figure 21 (b) shows that in all
cases with varying amounts of label noise, the early stopping risk is however worse than that of GD.
This agrees with the observation in Section 4 and Figure 19(a) that early stopping can potentially
favor GD due to the reduced variance.
Misalignment. In Figure 21(c)(d) we confirm the finding in Proposition 7 and Figure 18(b) in
neural networks under synthetic data: we consider 50-dimensional Gaussian input, and both the
teacher and the student model are two-layer ReLU networks with 50 hidden units. We construct the
teacher by perturbing the initialization of the student as described in Section 5. As r approaches -1
(problem more “misaligned”), NGD achieves lower early stopping risk (Figure 21(d)), whereas GD
dominates the early stopping risk in less misaligned setting ( 21(c)). We note that this phenomenon
is difficult to observe in practical neural network training on real-world data, which may be partially
due to the fragility of the analogy between neural nets and linear models, especially under NGD
(discussed in Appendix A.1).
-^s≡≈tmco
0.∞ 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Noise Standard Deviation
(a) stationary risk
(CIFAR).
) I ɪ ɪ ɪ ɪ ♦♦
XS≡6wddoffiAμeuj
0.2	0.4	0.6	0.8	1.0
Noise Standard Deviation
(b) optimal early stopping
risk (CIFAR).
steps
(c) r = -1/2
(synthetic).
steps
(d) r = -3/4
(synthetic).
Figure 21:	(a)(b) Additional label noise experiment on CIFAR-10. (c)(d) Population risk of two-layer neural
networks in the misalignment setup with synthetic Gaussian data. We set n = 200, d = 50, the damping
coefficient λ = 10-6, and both the student and the teacher are two-layer ReLU networks with 50 hidden units.
The x-axis and the learning rate have been rescaled for each curve (i.e., optimization speed not comparable).
When r is sufficiently small, NGD achieves lower early stopping risk, whereas GD is beneficial for larger r.
23
Published as a conference paper at ICLR 2021
D Proofs and Derivations
D. 1 Missing Derivations in Section 3
Gradient Flow of Preconditioned Updates. Given positive definite P and γ > 1, one may check
that the gradient flow solution at time t can be written as
θp(t) = PX> In - exp(--XPX>) (XPX>)-1y.
Taking t → ∞ yields the stationary solution θp = PX>(XPX>)-1y. We remark that the
damped inverse of the sample Fisher P = (XX> + λId)-1 leads to the same minimum-norm
solution as GD 芬=X>(XX>)-1y since PX> and X share the same eigenvectors. On the
other hand, when P is the pseudo-inverse of the sample FiSher (XX>)* which is not full-rank, the
trajectory can be obtained via the variation of constants formula:
θ()
:X (k⅛ (-:X>(XXT)TX)k
X>(XX>)-1y,
for which taking the large limit also yields the minimum-norm solution X>(XX>)-1y.
Minimum kθkP -1 Norm Interpolant. For positive definite P and the corresponding stationary
>>	0
solution θP = PX> (XP X>)-1y, note that given any other interpolant θ , we have (θP -
θ )P-1^p = 0 because both θp and θ achieves zero empirical risk. Hence ∣∣0 ∣∣p-ι-∣∣0PkP-ι =
II。- Opkp-ι ≥ 0. This confirms that θp is the unique minimum ∣∣θ∣∣p-ι norm solution.
D.2 Proof of Theorem 1
IT* ∕* ɪʌ ,1 FC ∙ , ∙	1' , 1	∙	,	1,1	A
Proof. By the definition of the variance term and the stationary θ,
V (θ) = tr(cov(0)∑χ ) = σ2tr(PX t(XPX t)-2XP Σχ ).
Write X = XP1/2. Similarly, We define Σχp = P"2∑χP1/2. It is clear that the equation
above simplifies to
V(θP) = σ2tr(XT(XX>)-2XΣxp).
The analytic expression of the variance term follows from a direct application of (Hastie et al., 2019,
Thorem 4), in which conditions on the population covariance are satisfied by (A2).
Taking the derivative of m(-λ) yields
m0(-λ) = (m⅛-YZ(1+Tm(-λ))2dHXP(T)).
Plugging the quantity into the expression of the variance (omitting the scaling σ2 and constant shift),
m2(一λ) = (1 - Ym2(-λ) Z + τ(	2dHXP(T)).
m2 (-λ)	(1 + T m(-λ))2
From the monotonicity of ɪ+Xx on x > 0 or the Jensen,s inequality we know that
1
-γ
/ ( τm(-λ) ∖
J 1+ + τm(-λ)J
2
dHXP(T) ≤
T m(-λ)
1 + τm(-λ)
2
Taking λ → 0 and omitting the scalar σ2, the RHS evaluates to 1 - 1∕γ. We thus arrive at the
lower bound V ≥ (γ - 1)-1. Note that the equality is only achieved when HXP is a point mass,
i.e. P = Σ-X1. In other words, the minimum variance is achieved by NGD. As a verification, the
variance of the NGD solution 0尸-ι agrees with the calculation for the case were the features are
isotropic (Hastie et al., 2019, A.3).	□
24
Published as a conference paper at ICLR 2021
D.3 Proof of Corollary 2
Proof. Via calculation similar to (Hastie et al., 2019, Section 5), the bias can be decomposed as
EhBMP)] =Eχ,χ,θ* J(XTPXT(XPXT)T(Xθ* + Xcθc) - (x>θ* + Xτθc))
=)Eχ,e* ] (xτPXT (XPXτ)-1Xθ* - xτθ*)	+ Exc,θæ [(Xτθc)2]
+ Ex,θc ] (XTPXT(XPXτ) TXcθc俨TXCT(XPXτ) TXPX)
驾 Bθ (Θp ) + -1tr(∑X ∑θ )(1 + V (Θp )),
dc
where We used the independence of x, X and θ*, θc in (i), and (A1-3) as well as the definition of
the well-specified bias Be(θp) and variance V(θp) in (ii).	口
D.4 Proof of Theorem 3
Proof. By the definition of the bias term (note that Σχ, ∑e, P are all positive semi-definite),
„ z ^	一
B(θp) = Ee*
PXτ(XPXT)TXθ* - θ*
2
∑X
dtr (∑e(Id - PXτ(XPXτ)-1X)£(Id - PXτ(XPXT)T
t∑xp(Id - XT(XXT)
1XτX + λid)	∑XP (1XτX + λid
I \?-1 ʌ yl-I/2 yl	yl-I/2 I
十 aςθ∕P	ςθ∕P NXPςθ∕P	,
(i) 1
=万tr
d
网	λ2 +
=lιm tr
λ—→0+ d
(iii)	λ2 +
=lιm tr
λ—→0+ d
—X
where we utilized (A3) and defined X = XP1/2, ∑xp = P1/2ΣχP1/2, ∑θ∕p =
P-1∕2∑θP-1/2 in (i), applied the equality (AAτ)tA = lim.→0(ATA + λI)-1A in (ii),
and defined X = XP1/2N-1/2 in (iii). To proceed, we first assume that ∑θ∕p is invertible
(i.e. λmin(∑e∕p) is bounded away from 0) and observe the following relation via a leave-one-out
argument similar to that in Xu & Hsu (2019),
d tr(nX TX (nX τX+λ%1p)-)	01)
妇 X	1Xτ( 1XTx + λ∑e∕1p)-：Xi
d i=1 (1 + ɪXT(IXtx + λ∑θ∕1p)：Xi)2
1 (( 1 XT X I λ∑-1 ) 2∑-1∕2∑	∑-1/2)
-..)dtr n X X + λ∑θ∕P	∑Θ∕P ∑xp ∑Θ∕P
→一----------------------Z--------一,	(D.2)
(1 + 1 tr( (⅛X>X + λld)	∑Xp))
where (i) is due to the Woodbury identity and we defined (1XTX + λ∑-1P) = 1XTX -
1XiXT + λ∑//P which is independent to Xi (see (XU & Hsu, 2019, Eq. 58) for details), and in
25
Published as a conference paper at ICLR 2021
(ii) We used (A3), the convergence to trace (Ledoit & Peche, 2011, Lemma 2.1) and its stability
under low-rank perturbation (e.g., see (Ledoit & PeCha 2011, Eq. 18)) which we elaborate below.
>1
In particular, denote Σ = 1X X + λ∑-∣p, for the denominator we have
sup
i
n tr(∑-1ς-∕1P2ςxp Σ
λ -1/2	-1/2
≤ n 惮 θ∕P ςXP "/P L SuP
-1/2	λ -1 -1/2	-1/2
'θ∕P - - ntr(£-i ςθ∕p ςXPςθ∕p
∣tr(∑-1(∑ - ∑-J∑ -i1 )|
λ 1 2 1 2 -1 -1	(i)
≤nBςθ∕1P2ςxpς-1Pl2∣Iς	ILSup2"Ltr(ς-ς-J →→Op
where (i) is due to the definition of Σ — and (AI)(A3). The result on the numerator can be obtained
via a similar calculation, the details of which we omit.
We first note that the denominator can be evaluated by previous results (e.g. (Dobriban et al., 2018,
Theorem 2.1)) as follows,
ntr ((nX>x+λId) ςXj →.λm⅛ -1.
(D.3)
On the other hand, following the same derivation as Dobriban et al. (2018); Hastie et al. (2019),
(D.1) can be decomposed as
d tr(1X >X (nX >玄》%>)
=dtr((nX>X+λId)通P)- dtr((nX>X+λId)	eθ/p)
=dtr((nX>X+λId)	4/P)+dddλtr((nX>X+λId)通P
(D.4)
We employ (Rubio & Mestre, 2011, Theorem 1) to characterize (D.4). In particular, For any deter-
ministic sequence of matrices Θn ∈ Rd×d with finite trace norm, as n, d → ∞ we have
tr (θn( 1X >X - ZI d)	- Θn(Cn(z)ΣχP -ZI d)-j as 0,
in which cn (Z) → -Zm(Z) for Z ∈ C\R+ and m(Z) is defined in Theorem 1 due to the dominated
convergence theorem. By (A3) we are allowed to take Θn = ɪ ∑θ∕p . Thus we have
λ
dtr
1X >X + λI d
n
T dtr ^ςθ∕p UmJREXP + λI d) 1^
(=i)E
υχυθ υ-1
1 + m(一λ)υχp
∀λ > -cl,
(D.5)
in which (i) is due to (A3), the fact that the LHS is almost surely bounded for λ > -cl, where cl
is the lowest non-zero eigenvalue of n X > X, and the application of the dominated convergence
theorem. Differentiating (D.5) (note that the derivative is also bounded on λ > -cl) yields
ddλtr((1X>X + λI)ςθ∕p) → E
υχυθ U-I_______m0(一λ)υχUθ	(D6)
λ(l + m(-λ)Uχp)	(1 + m(-λ)Vχp)	.
Note that the numerator of (D.2) is the quantity of interest. Combining (D.1) (D.2) (D.3) (D.4)
(D.5) (D.6) and taking λ → 0 yields the formula of the bias term. Finally, when Σθ∕P is not in-
vertible, observe that if we increment all eigenvalues by some small > 0 to ensure invertibility
26
Published as a conference paper at ICLR 2021
∑Θ∕p = ∑θ∕p + eId, (3.3) is bounded and also decreasing w.r.t. e. Thus by the dominated Con-
vergence theorem we take → 0 and obtain the desired result. We remark that similar (but less
general) characterization can also be derived based on (Ledoit & Peche, 2011, Theorem 1.2) when
the eigenvalues of Σχp and ∑θ∕p exhibit certain relations.
To show that P = U diag (eθ)U> achieves the lowest bias, first note that under the definition of
random variables in (A3), our claimed optimal preconditioner is equivalent to υxp a=.s. υxυθ. We
therefore define an interpolation Ua = αυxυe + (1 - α)υ for some U and write the corresponding
Stieltjes transform as mα(-λ) and the bias term as Bα. We aim to show that argminα∈[0,1] Bα = 1.
For notational convenience define gα , mα(0)UxUθ and hα , mα(0)Uα. One can check that
UχUθ	"∣E∣"	ha	]-1
(1 + ha)2]	[(1 + ha)」
dma(-λ) I	= ma⑼E[(h+hg)2]
dα	k，(1 - a)Eh(⅛Pi
We now verify that the derivative of Ba w.r.t. α is non-positive for α ∈ [0, 1]. A standard simplifi-
cation of the derivative yields
2
E
dBa (X- 2E
dα
+ 4E
(ga - ha)2
_ (1 + ha)3 .
ha(ga - ha)
_ (1 + ha)3
ha
.(1 + ha )2 .
ga - ha
2 - 2E
ga - ha
.(1 + ha)2.
r ha ]
.(1 + ha)3-
[(1 + ha)2[E [(1 + ha)2 J
(i)
≤-4 E
(ga - ha)2
_(1 + ha)3.
r ha ]
一 (1 + ha)3.
ga - ha
_(1 + ha)2.
2E
ha
_(1 + ha)2.
+ 4E
ha(ga - ha)
_ (1 + ha)3
E 匹.....-E -----a——
E [(1 + ha)2」E [(1 + ha)2」
(ii)
≤ 0,
E
E
E
E
2
where (i) is due to AM-GM and (ii) due to Cauchy-Schwarz on the first term. Note that the two
equalities hold when ga = ha, from which one can easily deduce that the optimum is achieved
when Uxp a=.s. UxUθ, and thus we know that the proposed P is the optimal preconditioner that is
COdiagOnazabIe with Σχ.	口
D.5 Proof of Proposition 4
Proof. Since Σθ = ΣrX, we can simplify the expressions by defining Ux , h and thus Uθ = hr .
From Theorem 3 we have the following derivation of the GD bias under (A1)(A3),
ml	h ∙ hr
B(OI) → mE(1 + h ∙ mι)2
h1+r
(1+h∙mι )2
1 - YE (⅛≡⅛
(D.7)
E
where m1 = limλ→0+ m(-λ), and m satisfies
m⅛ = λ + YE
Similarly, for NGD (P = Σ-X1) we have
1 + h ∙ m(-λ)
h
B(θF-1) → m2E
F	m22
h ∙ hr
h1+r
(i+m2)2
(1 + m2)2 = 1-YE(1+m2 )2
Eh1+r
(1 + m2 )2 - Ym22 ,
(D.8)
E
where standard calculation yields m2 = (Y - 1)-1, and thus B(Θf-1) → (1 - Y-1)Eh1+r.
To compare the magnitude of (D.7) and (D.8), observe the following equivalence.
.^ . ^ .
B(^I)空 B(^f-1)
27
Published as a conference paper at ICLR 2021
⇔ E J+r )2 ∙ ɪ 分(l - YE JhZmI)F Eh1+r.
(1 + h ∙ mi)2 γ — 1	\	(1 + h ∙ mi)2 )
(i)	ζ1+r	ζ	ζ	1
⇔ E (⅛FET⅛ 袋 E (Γ⅛EZ EF.	(D.9)
where (i) follows from the definition of mi and We defined Z，h ∙ mi. Note that when r ≤ —1 and
h is not a point mass, we have
Zi+r	Z	Z Zi+r	Z	1
E U E 士 > E 小 E ⅛ > E 小 ez i+rE E.
On the other hand, when r ≥ 0, following the exact same procedure we get
Zi+r	Z	Z	1
E (⅛F E ⅛< E (1⅛ ez i+rE E.
Combining the two cases completes the proof.
□
D.6 Proof of Proposition 5
Proof. We first outline a more general setup where Pα = f(ΣX; α) for continuous and differen-
tiable function of α and f applied to eigenvalues of Σx. For any interval I ⊆ [0, 1], we claim
(a)	Suppose all four functions Xf(Xo)，f(x； α), df∂αr)/f (x； α) and Xdf∂αr) are decreasing
functions of X on the support of Vx for all α ∈ I. In addition, d∂θ° ≥ 0 on the support
of vX for all α ∈ I. Then the stationary bias is an increasing function of α on I.
(b)	For all α ∈ I, suppose Xf (X; α) is a monotonic function of X on the support of vx and
df∂θα)/f (x； α) is a decreasing function of X on the support of vx. Then the stationary
variance is a decreasing function of α on I.
Let us verify the three choices of P α in Proposition 5 one by one.
•	When Pα = (1 — α)Id +α(ΣX)-i, the corresponding f(x； α) is (1 — α) +αx. This satisfies all
conditions in (a) and (b) for all α ∈ [0, 1]. Hence, the stationary variance is a decreasing function
and the stationary bias is an increasing function of α ∈ [0, 1].
•	When Pα = (ΣX)-α, the corresponding f(x； α) is x-α. It is clear that it satisfies all conditions
in (a) and (b) for all α ∈ [0,1] except for the condition that X ”尸 = —χi-α ln X is a decreasing
function of x. Note that X d∂θ° = —Xi-α ln X is a decreasing function of X on the support of
Vx only for a ≥ 嚅K-I where K = sup vx/ inf Vx. Hence, the stationary variance is a decreasing
function of α ∈ [0,1] and the stationary bias is an increasing function of α ∈ [maχ(0,嚅K-I), 1].
•	When Pα = (α∑χ + (1-α)Id)-i, the corresponding f (x; α) is 1∕(αx +(1 — α)), which
satisfies all conditions in (a) and (b) for all α ∈ [0,1] except for the condition that X d∂θ0 =
(ax+(i-⅞)2 is a decreasing function of χ. Note that X "箔)=(03(1-0))2 is a decreasing
function of X on the support of Vx only for ɑ ≥ K-I. Hence, the stationary variance is a decreasing
function of α ∈ [0,1] and the stationary bias is an increasing function of α ∈ [max(0, K-I), 1].
To show (a) and (b), note that under the conditions on Σx and Σθ assumed in Proposition 5, the
, ,∙	1 ∙	τ~> / A ∖	1 ,1	,	∙	τ r / A ∖	ι ∙ ι∙ r∙ ι ,
stationary bias B(θPα) and the stationary variance V (θPα) can be simplified to
.^ .
B(^Pa )
ma(0) E_________vx_________
mα(0) (1 + Vxf (vx； α)mα(0))2
.ʌ	C
and V (θp α) = σ2
ma(0)
mα(o)
—1
where mα(z) and m0α(z) satisfy
1
-Zm (Z) + 7E Vxf(Vx； α)mα(z)
a( ) + Y 1 + Vxf (Vx； α)mα(z)
(D.10)
28
Published as a conference paper at ICLR 2021
m0α (z)	1
------：~~~ - ------------------------TT
mα(Z)	1 _ ~E / f (Vxia)mα(Z)、
Y	1+f (vx;a)ma(z) J
(D.11)
For notation convenience, let fα := vxf(vx; α). From (D.11), we have the following equivalences.
.^ .
B(ΘPa)
E Vx
E (1+fαmα(0))2
1 - YE(白吗、)
1+fαmα(0)
(D.12)
V (θp ɑ ) = σ2
1-
famɑ(0) ) 2
1 + famα(0) I
-1
(D.13)
(
1
We first show that (b) holds. Note that from (D.13), we have
2
E
∂V(θpa )	“2
F
/
1
1-、E( fαmα(0) )2
∖1 YE [l+fama(0))
2fama(0)
(1 + fama(0))3
‘∂ma(z)
fa ∂α
z=0+
(D.14)
To calculate dɪ(Z) I , We take derivatives with respect to a on both sides of (D.10),
z=0
_______1_________( f ∂ma(z)∣	+ ∂fa
(1 + fa ma (0))2 Ia ∂α ∣z=0 ∂α rn'
(D.15)
Therefore, plugging (D.15) into (D.14) yields
2
(E (1 + Ima (0))2
ʌ /
∂V (θp a)	Q 2	ma(0)
---- --=2γσ -----------------------2
dα	1 - ,er fɑmα(0) Y
∖1	YE [l + famα(0) J
f	f ∂fa	f	f2	∂fa
× (E (1 + f：m：(0))3 E (1+ f.ma(0))2 - E (1 + fa；a(0))3 E Wf mW
El i ∙ τ r / A ∖ ∙	1	♦ c , ∙ i`	∙ ι , , ι ∙	, ι
Thus showing V(θPα) is a decreasing function of α is equivalent to showing that
f 2	∂fa	f ∂fa	f
E (1 + fl(0))3 EEomW ≥ E (1 + f∕(0))3 E (1 + ffma(0))2 . (D.16)
Let μχ be the probability measure of Vχ. We define a new measure μχ = (1+/*;。))？, and let
Vx follow the new measure. Since/f (x; α) is a decreasing function of X and Xf (x; α) is a
monotonic function of x,
~ dvx . dv	dVxf(Vx；a)	dvx f(Vx；a)
E________xf (Vx; α)_____E______∂a____ ≥ E____________∂a___________
1	+ Vxf(Vx α)ma(0) Vxf(Vx； α) ~ 1 + Vxf(Vx； α)ma(0).
Changing Vx back to Vx, we arrive at (D.16) and thus (b).
TΓ∏ . 1	1 ∙	,	1-1 / A ∖	,	,1	, l'	/ɪʌ f z∖∖	1 /-rʌ T z-»\	1
For the bias term B(θPα), note that from (D.10) and (D.12), we have
∂B(Θpα ) =1 f 1 - E ( fθma(0) Y) -2
∂α Y ∖ Y 1 + fa ma (0)√
XtE 2(1+fama(0))3 ∙fa d⅛z2L+f me)E (fmm，
+	(1 + fama(0))2
E
2	fa ma(0)
(1 + fama(0))3
fa FI	+
∂α	Z=0
(D.17)
29
Published as a conference paper at ICLR 2021
CΛ∙ ∙1 1	1 ∙	/-ɪ-ʌ t L、	1 /-ɪ-ʌ ~i r、	1	1 ∙ 1' , 1	∙	E	∙ 1' 1-1 / A ∖ ∙
Similarly, we combine (D.15) and (D.17) and simplify the expression. To verify B(θPα) is an
increasing function of α, we need to show that
0≤ E
f
∂a
Vxfama(0)	E
(1 + fɑlα(0))3 (1 + fala(0))
2-E
E	fama (O)
(1 + fala(0))3 (1 + fala(0))
∂fα
Vx志
E	fama (O)
1	(1 + fama(0))2
-E
f
∂a
(1 + fαmα(0))2
fama(0))2 E
(1 + fɑmα(0))3 (1 + fαrmα(0))
 _ E fama(O) f E	fama(O)
2	(1 + fama(0))3 (1 + fama(0))2
(D.18)
E
Let ha , fama(0) = Vxf(Vx; α)ma(0) and ga，f = Vxfoa. Then (D.18) can be further
simplified to the following equation
0≤E
gα
(1 + ha)3 E (1+gha)3 E (1+ ha)
-E
3
^^{^^^
part 1
vx
gα
(1 + ha)3 E (1 + ha)3 E (1 + ha)
3
_}
+E
J
vx
(1 + ha )3 E (If hi)3 E (1+ I。)
-E
3
^z`'-/'^^
part 2
vxgα
(1 + ha)3 E (1+ la)3 E (1+ la)
Vxha	gaha	ha
+ 2E EJ E (TE E EA-2E
|----------------------V--
part 3
vxgα
(1 + ha)3 E (1+ ha)3 E (1+ ha)
3
_}
+ E - E M E ∕⅛ - E
|-----------------V-
part 4
E ——ha—— E ——ha—
(1 + ha)3 (1 + ha)3 (1 + ha)
vxgα
. (D.19)
3
，
|
3
Note that under condition of (a), we know that both hα and vx /hα are increasing functions of vx ;
and both gα∕hα and gα are decreasing functions of vx. Hence, with calculation similar to (D.16),
we know part 1,2,3,4 in (D.19) are all non-negative, and therefore (D.19) holds.
□
Remark. The above characterization provides sufficient but not necessary conditions for the mono-
tonicity of the bias term. In general, the expression of the bias is rather opaque, and determining
the sign of its derivative can be tedious, except for certain special cases (e.g., γ = 2 and the eigen-
values of ΣX are two equally weighted point masses). We conjecture that the bias is monotone for
α ∈ [0, 1] for a much wider class of ΣX, as shown in Figure 22.
0.4
s03
ra
S
0.2
0.1
0：0 0：2 0：4 0：6 0：8 l：0
GD interpolation coefficient NGD
⑶ Y = 2.	(b) Y = 5.
Figure 22: Illustration of the monotonicity of the bias term under Σθ = Id . We consider two distributions of
eigenvalues for ΣX: two equally weighted point masses (circle) and a uniform distribution (star), and vary the
condition number κX and overparameterization level Y. In all cases the bias in monotone in α ∈ [0, 1].
D.7 Proof of Proposition 6
Proof. Taking the derivative of V (θP (t)) w.r.t. time yields (omitting the scalar σ2),
dV(θP(t))	d
dt
dt
∑X2PX>(In - exp(--XPX>
PX>-1
2
F
30
Published as a conference paper at ICLR 2021
/
Σ
∖
=) 1tr
n
0,
where We defined X = XP1/2 and SP = XPXT in (i), and (ii) is due to (A2-3) the inequality
tr(AB) ≥ λmin( A)tr(B) for positive semi-definite A and B.	□
D.8 Proof OF Proposition 7
IT* Cc 11 , 1	IC ♦ J	, 1 1 ∙ Z 11	∙ r' 1∖ A / , ∖
Proof. Recall the definition of the bias (well-specified) of θp(t),
B(θp(t))= ；tr(∑e(Id - PXtWp(t)Sp1X)
=)dtr(∑θ∕p(Id - XTWp(t)Sp1X)
T
ςx
rd - PXTWP(t)Sp1x))
(Id - XtWp ⑴ Sp1X))
T
∑XP
(iii) 1
≥ -tr
一 d
(D.20)
where we defined SP = XPXt, WP⑴=In - exp(-《SP) in (i), X = XP1/2 in (ii), and
(iii) is due to
When Σχ
the inequality tr (AT A) ≥ tr (A2).
Σ-1, i.e. NGD achieves lowest stationary bias, (D.20) simplifies to
B(θP (t)) ≥ dtr((1 d - X TW P (t)Sp1X) ) = (1 - Y ) + d ^X exp(-n"j , (D.21)
where λ is the eigenvalue of SP. On the other hand, since F = Σχ, for the NGD iterate Θf-i (t),
B(θF-1 (t)) = dtr((Id - X WF-1 (t)SF-iX) ) = (1 - Y) + d ^Xeχp(-nʌi) (D.22)
where X = XΣχ"2 and λ is the eigenvalue of SF-i = XX . Comparing (D.21)(D.22), we
W
see that given θP(t) at a fixed t, if we run NGD for time T > r^maxt (note that T/t = O(1) by
入min
(A2-3)), then we have B(θP(t)) ≥ B(θp-i (T)) for any P satisfying (A3). This thus implies that
BOPt (θP) ≥ Bopt(θF-i).
On the other hand, when Σθ = Id, we can show that the bias term of GD is monotonically decreas-
ing through time by taking its derivative,
AB(θι(t)) = IAtr((Id - XtWi(t)SI1X)>Σχ(Id - XTWI(t)SI1
Ut	d Ut
/ ∖
1
-ndtr
Σχ XtSexp(--S)S-1X(Id - XtWi(t)SI1X)
'------------乙------------------------J
∖	p.s.d.	)
< 0.
(D.23)
Similarly, one can verify that the expected bias of NGD is monotonically decreasing for all choices
of Σχ and ∑θ satisfying (A2-4),
d tr(∑θ(I d - F-1X tW f-i (t)S--i X )>Σχ(I d - FT X tW F-i (t)SF-I
dtr (∑χθ(Id - XT Wf-i (t)S--iX)ɪ(Id - XTWf-i (t)SF-IX)) < 0,
31
Published as a conference paper at ICLR 2021
where (i) follows from calculation similar to (D.23). Since the expected bias is decreasing through
time for both GD and NGD when Σθ = Id, and from Theorem 3 we know that B(θI) ≤ B(θF-1),
we conclude that Bopt(θι) ≤ Bopt(Θf-i).	□
D.9 Proof of Theorem 8
D.9.1 Setup and Main Result
We restate the setting and assumptions. H is an RKHS included in L2(PX) equipped with a bounded
kernel function k satisfying supsupp(P ) k(x, x) ≤ 1. Kx ∈ H is the Riesz representation of the
kernel function k(χ, ∙), that is, k(x, y) = (Kχ, Ky〉h. S is the canonical embedding operator from
H to L2(Pχ). We write Σ = S *S : H → H and L = SS*. Note that the boundedness of the kernel
gives kSfkL2(PX) ≤ supx |f(x)| = supx |hKx,fi| ≤ kKxkHkfkH ≤ kfkH. Hence we know
kΣk ≤1andkLk ≤ 1. Our analysis will be made under the following regularity assumptions.
•	(A4). There exist r∈ (0, ∞) and M > 0 s.t. f* =Lrh* for some h* ∈ L2(PX) and kf* k∞ ≤M.
•	(A5). There exists s > 1 s.t. tr(∑^S) < ∞ and 2r + s-1 > 1.
•	(A6). There exist μ ∈ [s-1,1] and C” > 0 s.t. supχ∈suPP(PX)|用1/2-1/"(||丸 ≤ Cμ.
(A5)(A6) are standard regularity assumptions in the literature that provide capacity control of the
RKHS (e.g., see Caponnetto & De Vito (2007); Pillaud-Vivien et al. (2018)). For (A4), it is worth
noting that previous works mostly consider r ≥ 1/2 which implies that f* ∈H.
The training data is generated as yi = f *(xi) + 号i, where ε is an i.i.d. noise satisfying ∣ε∕ ≤ σ
almost surely. Let y ∈ Rn be the label vector. We identify Rn with L2(Pn) and define
nn
∑ = nXKxi 乳 Kxi ： h→h,	s*y = nXYiKxi, (y ∈ 乙2(2)).
i=1
i=1
We consider the following preconditioned update on ft ∈ H:
. . -1 . ^ ^ ,
ft = ft-1- η(∑ + λI)-1(Σft-1- S* Y),	fo = 0.
We briefly comment on how our analysis differs from Rudi et al. (2017), which showed that a precon-
ditioned update (the FALKON algorithm) for kernel ridge regression can also achieve accelerated
convergence in the population risk. We emphasize the following differences.
•	The two algorithms optimize different objectives, as highlighted by the different role of the
“ridge” coefficient λ. In FALKON, λ turns the objective into kernel ridge regression; whereas
in our (4.1), λ controls the interpolation between GD and NGD. As we aim to study how the
preconditioner affects generalization, it is important that we look at the objective in its original
(instead of regularized) form.
•	To elaborate on the first point, since FALKON minimizes a regularized objective, it would not
overfit even after large number of gradient steps, but it is unclear how preconditioning impacts
generalization (i.e., any preconditioner may generalize well with proper regularization). In con-
trast, we consider the unregularized objective, and thus early stopping plays a crucial role in
avoiding overfitting - this differs from most standard analysis of GD.
•	Algorithm-wise, the two updates employ different preconditioners. FALKON involves inverting
the kernel matrix K defined on the training points, whereas we consider the population covariance
operator Σ, which is consistent with our earlier discussion on the population Fisher in Section 3.
•	In terms of the theoretical setup, our analysis allows for r < 1/2, whereas Rudi et al. (2017) and
many other previous works assumed r ∈ [1/2, 1], as commented in Section 4.3.
We aim to show the following theorem:
Theorem (Formal Statement of Theorem 8). Given (A4-6), if the sample size n is sufficiently large
so that 1∕(nλ)《1,thenfor η < ∣∣∑k with ηt ≥ 1 and 0 < δ < 1 and 0 < λ < 1, it holds that
kSft-f*k2L2(PX) ≤C(B(t)+V(t)),
32
Published as a conference paper at ICLR 2021
with probability 1 - 3δ, where C is a constant and
λ 2r
B (t) ：= exp(-ηt) ∨ η~t],
V(t) := Vi(t) + (1 + ηt)(λ-1 B(t)+ 0士1 )λ-1 + λ-1(σ + M + (1+ tηk(2-r)+ 产]iog(i∕δ)2,
n	n2
in which
…、J /	、	( λ ∖2r	22( β0(1 ∨ x2r-")tr(£1 )λ-S	β02(i + λ-μ(i ∨ λ2r-μ) ∖]	2
匕⑴ := eχp(一ηt) ∨ 膝)+(tη)	I-----------n------------+-----------n-----------)1(1 + tη)	,
for β = log( 28Cμ(2_- ∨λ "+ )tr(£ /	)λ_/_).	When r ≥	1/2, if we Set λ	= n-2rs+1	=: λ* and
t = Θ(log(n)), then the overall convergence rate becomes
kSgt - f*k2L2 (PX)
n
2rs
2rs + 1

~
which is the minimax optimal rate (Op(∙) hides a Poly-Iog(n) factor). On the other hand, when
2r_
r < 1/2, the bound is also Op (n- 2rs+1 ) except the term Vi(t). In this case, if 2r ≥ μ holds
2r_
additionally, we have Vt(t) = Op (n- 2rs+1 ), which again recovers the optimal rate.
Note that if the GD (with iterates ft) is employed, from previous work Lin & Rosasco (2017) we
know that the bias term (优) is replaced by (2) ,and therefore the upper bound translates to
kSft- f *kL2(pχ)≤ C{ M-+1 (trbs)w产+ηt)卜+G )2r+M2+(nr⑵-1)},
2r_
with high probability. In other words, by the condition n =O(1), we need t = Θ(n2rs+1) steps to
sufficiently diminish the bias term. In contrast, the preconditioned update that interpolates between
GD and NGD (4.1) only require t = O(log(n)) steps to make the bias term negligible. This is
because the NGD amplifies the high frequency component and rapidly captures the detailed “shape”
of the target function f* .
D.9.2 Proof of Main Result
Proof. We follow the proof strategy of Lin & Rosasco (2017). First we define a reference optimiza-
tion problem with iterates f that directly minimize the population risk:
ft = ft-ι - n(∑ + λi)-1(∑ft-ι - S*f *), f = 0.
Note that E[ft] = ft. In addition, we define the degrees of freedom and its related quantity as
N∞(λ) := Eχ[hKχ, ∑-1KχiH] =tr(∑∑-i),	F∞(λ) := SUP	|同-1/2/我.
x∈supp(PX )
We can see that the risk admits the following bias-variance decomposition
kSft - f*kL2(Pχ)≤ 2(kSft - SRkL2(Pχ) + kft — f*kL2(Pχ)).
'---------{z--------} '---------{z-------}
V (t), variance	B(t), bias
We upper bound the bias and variance separately.
33
Published as a conference paper at ICLR 2021
Bounding the bias term B(t): Note that by the update rule (4.1), it holds that
Sft - f * = Sft-1 - f * - ηS(Σ + λI)-1(∑ft-1 - S*f *)
⇔ Sft - f * = (I - ηS(Σ + λI)-1S*)(Sft-1 - f *).
Hence, unrolling the recursion gives Sft - f * = (I-ηS(∑ + λI)-1S*)t(Sf -f *) = (I-ηS(∑ +
λI)-1S*)t(-f*) = -(I - ηS(Σ + λI)-1S*)tLrh*. Write the spectral decomposition of L as
L= Pj∞=1σjφjφj*forφj ∈ L2(PX)forσj ≥ 0. We have k(I-ηS(Σ+λI)-1S*)tLrh*kL2(PX) =
P∞=1(l - ησσjλ)2tσ2rhj, where h = P∞=1 hjφj. We then apply Lemma 11to obtain
2r λ 2r
B(t) ≤ exp(-ηt) E hj + ( —— ʌ E hj ≤ C exp(-ηt) ∨
j∙σj≥λ	` , η ,	j∙∙σj<λ	-
where C is a constant depending only on r.
λ 2r
ηt)
kh*kL2(PX),
Bounding the variance term V (t): We now handle the variance term V (t). For notational con-
venience, we write Aλ := A + λI for a linear operator A from a Hilbert space H to H . By the
definition of ft , we know
ft = (I - η(∑ + λI )-1Σ)ft-ι + η(∑ + λI )-1S*Y
t-1
=X(I - η(Σ + λI)-1Σ)jη(Σ + λI)-1S*Y
j=0
∑-1/2 η
t-1
X(I-ηΣ-1∕2∑Σ-1∕2
)j Σ-1∕2S*Y =: Σ-1∕2Gt∑-1∕2S*Y,
j=0
where We defined Gt := η[pj=0(I - ηΣ-1∕2ΣΣ-1∕2)j]. Accordingly, We decompose V(t) as
kSft- S力kL2(Pχ) ≤2(kS(ft- ∑,
-1/2Gtς-"ς ft)kL2(pχ)
|	{Z-
(a)
+ kS(∑-1∕2Gt∑-1∕2
I------------------
ς ft - ft)kL2 (Px )).
^Z
(b)
We bound (a) and (b) separately.
Step 1. Bounding (a). Decompose (a) as
kS(ft- Σ-"2Gt∑-∕2Σ¾(PX)
kS Σλ-1∕2GtΣλ-1∕2
,，、* '、 —...
(S*Y - Σft)k
^_____ ʌ
<"ll QV-1/2 ∣∣2	-1/2 ^ y-1/2 ∣∣2 1/2 ^ — 1y1/2 ∣∣2 -1/2( ^*γ ^F∖II
≤kS Σλ k kGtΣλ ΣλΣλ k kΣλ Σλ Σλ k kΣλ (S Y- Σft)k
We bound the terms in the RHS individually.
(i)	kSΣ-"k2 = k∑-1∕2∑∑-1∕2k ≤ 1.
2
L2(PX)
2
H.
(ii)	Note that Σ-1∕2Σλ∑-1∕2 = I - ∑-1∕2(∑ - ∑)∑-1/2 占(1 - k∑-1∕2(∑ - Σ)Σ-1∕2k)I.
Proposition 6 of Rudi & Rosasco (2017) and its proof implies that for λ ≤ kΣk and 0 < δ < 1,
k∑-1∕2(Σ - ∑)∑-1∕2k≤ J
2βF∞ (λ)	2β(1 + F∞ (λ))
I	0	=: Ξn,
with probability 1 - δ, where β = log 4tr
n
(ςς-1)
δ
3n
(D.24)
4N∞(I)). By Lemma 14, β ≤
}
}

log Cr(ς δ)λ----) and F∞(λ) ≤ λ-1. Therefore, if λ = o(n-1 log(n)) and λ = Ω(n-1∕s),
the RHS can be smaller than 1/2 for sufficiently large n, i.e. Ξn = O(/lοg(n)∕(nλ)) ≤ 1/2; thus,
Σ-“Σλ∑-1∕2 占 11.
34
Published as a conference paper at ICLR 2021
We denote this event as E1 .
(iii) Note that
t-1
Gt∑-"Σλ∑-1/2 = η X(I - η∑-v2Σ∑-")j Σ-"Σλ∑-1/2
j=0
t-1
=η X(I- η∑-∙2∑∑-* (∑-"∑∑-□ + λΣ-1).
j=0
Thus, by Lemma 12 we have
kGt∑-"2Σ λ∑-1∕2k
t-1
≤ η X(I — η∑-"∑∑-")j ∑-1/2∑∑-1/2 + η
j=0
、------------------{z------------------}
≤1 (due to Lemma 12)
t-1
≤1+ ηX k(I - η∑-1/2∑∑-1∕2)jkkλΣ-1k ≤ 1+ ηt.
j=0
t-1
X(I-ηΣ-F-")j λΣ-1
j=0
(iv) Note that
k∑32(S*γ - Σft)kH ≤ 2(k∑-/[(S*γ - Σft) - (S*f* - ∑ft)]kH + k∑-1∕2(s*f* - ∑ft)kH).
First We bound the first term of the RHS. Let ξ% = Σ-1∕2[Kχayi - Kxift(Xi) - (S* f * - Σft)].
Then, {ξi}in=1 is an i.i.d. sequence of zero-centered random variables taking value in H and thus
k∑-1∕2[(S*γ - ∑ft) - (S*f* - ∑ft)]kH
i=1
2
H
The RHS can be bounded by using Bernstein’s inequality in Hilbert space Caponnetto & De Vito
(2007). To apply the inequality, We need to bound the variance and sup-norm of the random variable.
The variance can be bounded as
E[kξikH] ≤ E(x,y) [k∑-1∕2(Kx(f*(x) - ft(x)) + Kξe)kH]
≤ 2{E(x,y) [k∑-1∕2(Kx(f*(x) - ft(x))kH + k∑-"(Kxe)kH]}
≤ 2 sup
x∈supp(PX)
k∑-1∕2Kxk2kf* - SftkL2(Pχ) + σ2tr(∑-1 ∑)}
≤ 2{F∞(λ)B(t)+ σ2tr(∑-1∑)}
≤ 2{λ-1B(t) + σ2tr(∑-1∑)},
The sup-norm can be bounded as follows. Observe that kftk∞ ≤ Ilftkh, and thus by Lemma 13,
kξikH ≤ 2 sup	k∑-1∕2KxkH(σ + kf*k∞ + kftk∞)
x∈supp(PX )
.F∞∕2(λ)(σ + M + (1+ tη)λ-(^T)+)
.λ-=2(σ + M + (1+ tη)λ-("T)+).
Hence, for 0 < δ < 1, Bernstein’s inequality (Proposition 2 of Caponnetto & De Vito (2007)) gives
1n
-X ξi
n i=1
2
≤C
H
卜
-------------------- ∖ 2
λ-1B(t)+ σ2tr(∑-1∑) + λ-"(σ + M +(1+ tη)λ-(V2T)+)	麻(1©2
n
n
35
Published as a conference paper at ICLR 2021
with probability 1 - δ where C is a universal constant. We define this event as E2 .
For the second term ∣∣Σ-"(S* f - Σft)kH we have
k∑-1∕2(s*f* - ∑ft)kH ≤ k∑Y2(f* - sft)kH = kf* - SftkL2(Pχ) ≤ B(t).
Combining these evaluations, on the event E2 where P(E2) ≥ 1 - δ for 0 < δ < 1 we have
k∑-"(S*γ - Σ ft)kH
------------- ∖ 2
λ-1B⑴+ σ⅛∑2 +—+M+(1+附λ-(1/"-	iogq∕δ)2 + B(t).
nn
where we used Lemma 14 in (i).
Step 2. Bounding (b). On the event E1, the term (b) can be evaluated as
kS(Σ-"Gt∑-"2Σft - ft)kL2(Pχ)
≤k∑"(Σ-"2Gt∑-"Σft - ft)kH
≤k∑"∑-"(Gt∑-"ΣΣ-" - I )∑fftkH
≤k∑1.∑32kk(Gt∑-"2∑∑32 -1W2ftkH
≤k(Gt∑-"2ΣΣ-"2 - I)∑FftkH.	(D.25)
(Gt∑-1∕2ΣΣ-1/2 - I)∑λ/2
where We used Lemma 13 in the last inequality. The term k(Gt∑-1∕2ΣΣ-1/2 - I)∑λ,2ft∣∣H can
be bounded as follows. First, note that
t-1
—1/2 ^—1/2 ∖j — —1/2^—1/2
η (I- ηΣλ	ΣΣλ )j Σλ	ΣΣλ	-I
j=0
(i-η∑-F—")t∑?2.
Therefore, the RHS of (D.25) can be further bounded by
k(I- η∑;1∕2Σ∑;1∕2)t∑[2ftkH
=k(I -庐—1∕2∑∑;1/2 + η∑λ12(∑ - Σ)∑—1/2)t∑λ/2ft\\H
t—1
=k X(I - η∑;1∕2∑∑;1/2)k(η∑;1∕2(∑ - ∑)∑;1/2)(I- n2—电厂kτ∑λ∕2ft - (I - η∑I1∑)t∑FftkH
k=0
(i)
≤k(I- n。—1∑)t∑λ∕2ftkH
t—1
+ n X k(I - n。—1/2。。-1/2)k。—1/2(。-。)2—1/2+r (I - n。—电厂-。；/2-ftkH
k=0
≤k(I- n。—1∑)t∑λ∕2ftkH + tnk。—1/2(。-。)。-1/2+rkk£；/2-rftkH
=k(I-n。—1∑)t∑λkk∑λ∕2f ftkH + tnk。—1/2(。-。)。—1/2+1|||。；/2—ftkH
.k(I-n。—1。)'。；1 + tnk。—1/2(。-。)。—1/2+rk(i + tn)kh*kL2(Pχ),	(D.26)
where (i) is due to exchangeability of。入 and。. By Lemma 11, for the RHS we have
k(I-n。-1BtMk ≤ exp(-nt∕2)∨
1 n)
Next, as in the (D.24), by applying the Bernstein inequality for asymmetric operators (Corollary 3.1
of Minsker (2017) with the argument in its Section 3.2), it holds that
k。—1/2(。-。)。-1/2+rk
36
Published as a conference paper at ICLR 2021
≤C，（r
β0cμ(22r-μ V λ2r-μ)N∞(λ)十 β0((i + λ)r + cμλ-μ%22r-μ v λr-/2))	_
n
n
with probability 1 - δ,	where
log (28C2(22r-μVλ-μ+2r)tr(∑1∕s)λ-1∕s ^
and the second order moments:
C'	is a universal constant and β'	≤
We also used the following bounds on the sup-norm
(sup-norm)
∣∣Σ-1∕2(KxKx
-ER-1」
(2nd order moment 1)
(2nd order moment 2)
≤k∑-1/2KxKx ∑-1∕2+rk + k∑λk
≤ k∑-"∕2∑λ∕2τ∕2KxKx∑-"十42∑λ-42k + k∑λk
≤ °μλ-V2(2r-/2 V λr-/2) + (1 + λ)r (a.s.),
∣∣Ex[∑U2(KxKx - ∑)∑-1+2r(KxKx - ∑)∑-1∕2]∣∣
≤ k∑-1∕2∑∑-1∕2k	sup	[Kx∑-^2+42∑-μ+2r∑-V2+∕2Kx]
x∈supp(Pχ )
≤ cμ(22r-μ v λ2r-μ),
∣∣Ex[∑-"+r(KxKx - ∑)∑-"2∑-"(KxKx- ∑)∑-1∕2+r]∣∣
≤∣∣Ex[∑-1∕2+r Kx Kx ∑-1KxKx ∑-1∕2+r]∣∣
≤ 盘(22-μ V λ2r-μ)Ex[Kx∑-1Kx]
=cμ(22r-μ V λ2r-μ)tr(∑∑-1)
=cμ(22r-μ V λ2r-μ)N∞(λ).
We define this event as £3. Therefore, the RHS of (D.26) can be further bounded by
[∣∣(I - η∑-1∑)t∑λk + ctηk∑-"(∑ - ∑)∑-1∕2+rk](i+ S)M*K(PX)
「	.	. .	∕1 λ ∖r _1	____
≤ }xp(-ηt∕2) V (ea)+ 切二nj (1+ 切)kh l∣L2(Pχ ).
Finally, note that when λ = λ* and 2r ≥ μ,
λ*2r-μ-1∕s	χ*2(r-μ)
二n = O (	+	2
n	n2
~ , _ s(4r —μ)	_ s(4r —2μ)+2	~ , _ 2rs 、
≤ Ο(n	2rs+1 + n 2rs+ι ) ≤ Ο(n 2rs+1).
Step 3. Combining the calculations in Step 1 and 2 leads to the desired result.
D.9.3 Auxiliary lemmas
Lemma 11. Fort ∈ N, 0 <η< 1, 0 < σ ≤
1 and 0 ≤ λ, it holds that
□
1 σ
1 - ησ+λ
Vr	fexP(-ηt∕2)
)σ≤ U 狞)r
(σ ≥ λ)
(σ<λ).
Proof. When σ ≥ λ, we have
1 - nɪ) = ≤ (1-斤)1r
σ + λ)	∖	2σ∕
(1 - η∕2)tσr≤
exp(-tη∕2)σr ≤ exp(-tη∕2)
due to σ ≤ 1. On the other hand, note that
1
∖ t
σr
-E σ
≤ exp( -ηt-σ-
σ+λ
σηt
σ + λ
×
)r (密)
r
37
Published as a conference paper at ICLR 2021
≤ 既exp(-x)xr (&)≤ ((σ⅛λk)
where we used supx>0 exp(-x)xr = (r/e)r.
□
Lemma 12. For t = N, 0 < η and 0 ≤ σ such that ησ < 1, it holds that η Pj=0(1 — ησ∖)σ ≤ 1.
Proof. If σ = 0, then the statement is obvious. Assume that σ > 0, then
t-1
X(1 -ησ)j σ
j=0
1 — (1 — ησ)t
1 — (1 — ησ)
l[1 - (1 - ησ)t]≤ η-1∙
This yields the desired claim.
□
Lemma 13. Under (A4-6), for any 0 < λ < 1 and q ≤ r, it holds that
k∑-sftkH < (1 + λ-(1∕2 + (q-r))+ + λtηλ-(3∕2 + (q-r))+ )k⅛*kL2(Pχ).
Proof. Recall that
t-1
ft = (I — η(∑ + λI)-1∑)ft-i + η(∑ + λI)-1S*f* = £(/ - η(∑ + ʌʃ)-1∑)jη(∑ + ʌʃ)-1S*f*.
j=0
Therefore, we obtain the following
t-1
k∑-ft∣∣H = ηk X(I- η∑-1∑)j∑-1-qSFh*g
j=0
t-1
=η∣∣ X(I - η∑-1∑)jΣ-1(Σ + λI)Σ-q-1S*Lrh*g
j=0
t-1	t-1
≤η∣∣ X(I - η∑-1∑)jΣ-1ΣΣ-q-1SFh* 旧 + λη∣∣ X(I- η∑-1∑)jΣ-1Σ-q-1S*Lrh*g
j=0	j=0
t-1	t-1
≤η∣∣ X(I - η∑-1∑)jΣ-1∑kk∑-q-1S*Lrh*g + λη∣∣ £(I - η∑-1∑)j∑-1∑-q-1S*Lrh*g
j=0	j=0
≤k∑-q-1S *Lr h*∣∣H + λtη∣∣Σ-1Σ-q-1S *Lr h*∣∣H
≤∣∣S *L-q-1+r h*E + λtη∣∣S *L-q-2+r h*g
≤ J(h*, L-q-1+rSS*L-q-1+rh%2(Pχ) + λtη尸工KrSSZL-1不向工晟
=√(h%L-q-1+r LL-q-1+r h*iL2(Pχ) + λtη J(h*,L-q-2+rLL-q-2+r h*)L2(PX)
≤(λ-1∕2-(q-r) + Mηλ-3∕2-(q-r))∣∣h*∣∣L2(Pχ) ≤ (1+ S"-1/2-ST)Ilh*∣∣L2(Pχ).
□
Lemma 14. Given (A4-6) and λ ∈ (0,1), we have N∞(λ) ≤ tr(∑1∕s)λ-1∕s, and F∞(λ) ≤ 1∕λ.
Proof. For the first inequality, We have
N∞(λ) =tr(∑∑-1) =tr(∑1∕sΣ1-1∕sΣ-(1-1∕s)∑-1∕s)
≤tr(∑VsΣ1-VsΣ-(I-1∕s))λ-Vs ≤ tr(∑Vs)λ-^s.
As for the second inequality, note that
F∞(λ) = SUPhKx, Σ-1 Kx)H ≤ sup λ-1(Kχ, Kx)H ≤ λ-1 sup k(x, x) ≤ λ-1.
x	x	x
□
38
Published as a conference paper at ICLR 2021
D.10 Proof of Corollary 9
Proof. Note that in this setting Ux takes value of 1++^ and 1+κ With probability 1/2 each. From
(D.8) in the proof of Proposition 4 one can easily verify that for NGD,
B(0f-i) →
2r (1 + κ1+r )(ι 1、
(1 + κ)1+r 1 - YJ
For GD, the bias formula (D.7) can be simplified as
1
B(θɪ) →- •
(TIK)	K (1+κ)	)(	mi	κmι 、
(1 + κ + 2mi)2 + (1 + κ + 2κmι)2 )	(1 + K + 2mi)2 + (1 + K + 2κmι)2 ,
Where m1 is the Stieltjes transform defined after Equation (D.7). From standard numerical calcula-
tion one can shoW that When γ > 1, K ≥ 1,
m1
-i
(K + 1),γ2(κ +1)2 +4(1 — Y)(K — 1)2 + (2 - Y)(K + 1)2
8(γ — 1)k
Setting B(θI) = B(θF-1 ) and solve for r, We have
r = — ln Cκ,γ∕ ln K,
c4 - c2
Cl - C3
(D.27)
Where We defined
1	(1 + K + 2 Kmi)
Y m1(1 + K + 2Km1)2 + Km1(1 + K + 2m1)2
1	k(1 + K + 2mi)2
Y mi(1 + K + 2Kmi)2 + Kmi(1 + K + 2mi)2
Hence, Proposition 4 (from which we know r* ∈ ( —1,0)) and the uniqueness of (D.27) implies that
when r ≥ r*, B(θι) ≤ B(Θf-i ), and Vice versa. Finally, observe that in the special case of Y = 2,
mi = κ√1. Therefore, one can check that constants in (D.27) simplify to
1 — √K	√K(√K — 1)
Ci — C3	,	C2 — C4	,
1	3	2(k + 1),	2	4	2(k + 1),
which implies that r* = —1/2.
□
D.11 Proof of Proposition 10
Proof. Part (a) is a simple combination of (Bai & Yin, 2008, Theorem 2) and assumption (A3),
which implies kΣX k2 and kΣ-Xi k2 are both finite. For part (b), the substitution error for the variance
term (ignoring the scalar σ2) can be bounded as
|V* — V| =卜r(FTX>(XFTX>)-2XF-1Σχ) — tr(FTX>(XFTX>)-2XF-1Σχ) ∣
≤O(1)BF-1—F-1∣∣2 (tr(X >S-2XFςx )+√d∣∣χ>S-2XUςXFTlIF)
+ tr(XF-1∑xF-1X>)∣∣S-2 — S-I? (=) O(e)∙
39
Published as a conference paper at ICLR 2021
-1
where We defined S = XF 1X > and S = XF X > m ⑴ and applied tr(AB) ≤ λmaχ(A +
A> )tr(B) for positive semi-definite B, as well as tr(AB) ≤ √dkAk2kB∣∣F, and (ii) is due to
(A3), ψ > 1, (Wedin, 1973, Theorem 4.1) and the following estimate,
nUk-2- S-j∣2 ≤∣nuS-1 - nuSTknuIISTIl2 + n“||STID
= O(1)∣∣nuS-1∣∣2∣∣nuS m,/"0 - S/nu( (=) O(e),
where (i) again follows from (Wedin, 1973, Theorem 4.1), and (ii) is due to (A1)(A3) and ψ > 1
1	-1
(which implies knuS-1k2 and knuS k2 are bounded a.s.). Finally, from part (a) we know that
ψ = Θ(e-2 ) suffices to achieve e-accurate approximation of F in spectral norm. The substitu-
tion error for the bias term can be derived from similar calculation, the details of which we omit. □
E Experiment Setup
E.1 Processing the Datasets
To obtain extra unlabeled data to estimate the Fisher, we zero pad pixels on the boarders of
each image before randomly cropping; a random horizontal flip is also applied for CIFAR10 im-
ages (Krizhevsky et al., 2009). We preprocess all images by dividing pixel values by 255 before
centering them to be located within [-0.5, 0.5] with the subtraction by 1/2. For CIFAR10, we
downsample the original images using a max pooling layer with kernel size 2 and stride 2.
E.2 Setup and Implementation for Optimizers
In all settings, GD uses a learning rate of 0.01 that is exponentially decayed every 1k updates
with the parameter value 0.999. For NGD, we use a fixed learning rate of 0.03. Since inverting
a parameter-by-parameter-sized Fisher estimate per iteration would be costly, we adopt the Hessian
free approach (Martens, 2010) which computes approximate matrix-inverse-vector products using
the conjugate gradient (CG) method (Hestenes et al., 1952). For each approximate inversion, we
run CG for 200 iterations starting from the solution returned by the previous CG run. The pre-
cise number of CG iterations and the initialization heuristic roughly follow Martens & Sutskever
(2012). For the first run of CG, we initialize the vector from a standard Gaussian, and run CG for 5k
iterations. To ensure invertibility, we apply a very small amount of damping (0.00001) in most sce-
narios. For geometric interpolation experiments between GD and NGD, we use the singular value
decomposition to compute the minus α power of the Fisher, as CG is not applicable in this scenario.
E.3 Other Details
For experiments in the label noise and misspecification sections, we pretrain the teacher using the
Adam optimizer (Kingma & Ba, 2014) with its default hyperparameters and a learning rate of 0.001.
For experiments in the misalignment section, we downsample all images twice using max pooling
with kernel size 2 and stride 2. Moreover, only for experiments in this section, we implement natural
gradient descent by exactly computing the Fisher on a large batch of unlabeled data and inverting
the matrix by calling PyTorch’s torch.inverse before right multiplying the gradient.
40