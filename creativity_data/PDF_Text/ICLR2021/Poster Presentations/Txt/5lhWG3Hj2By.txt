Published as a conference paper at ICLR 2021
Enforcing robust control guarantees within
NEURAL NETWORK POLICIES
Priya L. Donti1, Melrose Roderick1, Mahyar Fazlyab2, J. Zico Kolter1,3
1 Carnegie Mellon University, 2Johns Hopkins University, 3Bosch Center for AI
{pdonti, mroderick}@cmu.edu, mahyarfazlyab@jhu.edu, zkolter@cs.cmu.edu
Ab stract
When designing controllers for safety-critical systems, practitioners often face a
challenging tradeoff between robustness and performance. While robust control
methods provide rigorous guarantees on system stability under certain worst-case
disturbances, they often yield simple controllers that perform poorly in the aver-
age (non-worst) case. In contrast, nonlinear control methods trained using deep
learning have achieved state-of-the-art performance on many control tasks, but
often lack robustness guarantees. In this paper, we propose a technique that com-
bines the strengths of these two approaches: constructing a generic nonlinear con-
trol policy class, parameterized by neural networks, that nonetheless enforces the
same provable robustness criteria as robust control. Specifically, our approach en-
tails integrating custom convex-optimization-based projection layers into a neural
network-based policy. We demonstrate the power of this approach on several do-
mains, improving in average-case performance over existing robust control meth-
ods and in worst-case stability over (non-robust) deep RL methods.
1	Introduction
The field of robust control, dating back many decades, has been able to provide rigorous guarantees
on when controllers will succeed or fail in controlling a system of interest. In particular, if the uncer-
tainties in the underlying dynamics can be bounded in specific ways, these techniques can produce
controllers that are provably robust even under worst-case conditions. However, as the resulting
policies tend to be simple (i.e., often linear), this can limit their performance in typical (rather than
worst-case) scenarios. In contrast, recent high-profile advances in deep reinforcement learning have
yielded state-of-the-art performance on many control tasks, due to their ability to capture complex,
nonlinear policies. However, due to a lack of robustness guarantees, these techniques have still
found limited application in safety-critical domains where an incorrect action (either during training
or at runtime) can substantially impact the controlled system.
In this paper, we propose a method that combines the guarantees of robust control with the flexibility
of deep reinforcement learning (RL). Specifically, we consider the setting of nonlinear, time-varying
systems with unknown dynamics, but where (as common in robust control) the uncertainty on these
dynamics can be bounded in ways amenable to obtaining provable performance guarantees. Building
upon specifications provided by traditional robust control methods in these settings, we construct a
new class of nonlinear policies that are parameterized by neural networks, but that are nonetheless
provably robust. In particular, we project the outputs of a nominal (deep neural network-based)
controller onto a space of stabilizing actions characterized by the robust control specifications. The
resulting nonlinear control policies are trainable using standard approaches in deep RL, yet are
guaranteed to be stable under the same worst-case conditions as the original robust controller.
We describe our proposed deep nonlinear control policy class and derive efficient, differentiable
projections for this class under various models of system uncertainty common in robust control.
We demonstrate our approach on several different domains, including synthetic linear differential
inclusion (LDI) settings, the cart-pole task, a quadrotor domain, and a microgrid domain. Although
these domains are simple by modern RL standards, we show that purely RL-based methods often
produce unstable policies in the presence of system disturbances, both during and after training. In
contrast, we show that our method remains stable even when worst-case disturbances are present,
while improving upon the performance of traditional robust control methods.
1
Published as a conference paper at ICLR 2021
2	Related work
We employ techniques from robust control, (deep) RL, and differentiable optimization to learn prov-
ably robust nonlinear controllers. We discuss these areas of work in connection to our approach.
Robust control. Robust control is concerned with the design of feedback controllers for dynamical
systems with modeling uncertainties and/or external disturbances (Zhou and Doyle, 1998; BaSar and
Bernhard, 2008), specifically controllers with guaranteed performance under worst-case conditions.
Many classes of robust control problems in both the time and frequency domains can be formulated
using linear matrix inequalities (LMIs) (Boyd et al., 1994; Kothare et al., 1996); for reasonably-sized
problems, these LMIs can be solved using off-the-shelf numerical solvers based on interior-point or
first-order (gradient-based) methods. However, providing stability guarantees often requires the use
of simple (linear) controllers, which greatly limits average-case performance. Our work seeks to
improve performance via nonlinear controllers that nonetheless retain the same stability guarantees.
Reinforcement learning (RL). In contrast, RL (and specifically, deep RL) is not restricted to simple
controllers or problems with uncertainty bounds on the dynamics. Instead, deep RL seeks to learn
an optimal control policy, represented by a neural network, by directly interacting with an unknown
environment. These methods have shown impressive results in a variety of complex control tasks
(e.g., Mnih et al. (2015); Akkaya et al. (2019)); see BusSoniu et al. (2018) for a survey. However, due
to its lack of safety guarantees, deep RL has been predominantly applied to simulated environments
or highly-controlled real-world problems, where system failures are either not costly or not possible.
Efforts to address the lack of safety and stability in RL fall into several main categories. The first tries
to combine control-theoretic ideas, predominantly robust control, with the nonlinear control policy
benefits of RL (e.g., Morimoto and Doya (2005); Abu-Khalaf et al. (2006); Feng et al. (2009); Liu
et al. (2013); Wu and Luo (2013); Luo et al. (2014); Friedrich and Buss (2017); Pinto et al. (2017);
Jin and Lavaei (2018); Chang et al. (2019); Han et al. (2019); Zhang et al. (2020)). For example,
RL has been used to address stochastic stability in H∞ control synthesis settings by jointly learning
Lyapunov functions and policies in these settings (Han et al., 2019). As another example, RL has
been used to address H∞ control for continuous-time systems via min-max differential games, in
which the controller and disturbance are the “minimizer” and “maximizer” (Morimoto and Doya,
2005). We view our approach as thematically aligned with this previous work, though our method
is able to capture not only H∞ settings, but also a much broader class of robust control settings.
Another category of methods addressing this challenge is safe RL, which aims to learn control poli-
cies while maintaining some notion of safety during or after learning. Typically, these methods
attempt to restrict the RL algorithm to a safe region of the state space by making strong assump-
tions about the smoothness of the underlying dynamics, e.g., that the dynamics can be modeled as
a Gaussian process (GP) (Turchetta et al., 2016; Akametalu et al., 2014) or are Lipschitz continu-
ous (Berkenkamp et al., 2017; Wachi et al., 2018). This framework is in theory more general than our
approach, which requires using stringent uncertainty bounds (e.g. state-control norm bounds) from
robust control. However, there are two key benefits to our approach. First, norm bounds or polytopic
uncertainty can accommodate sharp discontinuities in the continuous-time dynamics. Second, con-
vex projections (as used in our method) scale polynomially with the state-action size, whereas GPs
in particular scale exponentially (and are therefore difficult to extend to high-dimensional problems).
A third category of methods uses Constrained Markov Decision Processes (C-MDPs). These meth-
ods seek to maximize a discounted reward while bounding some discounted cost function (Altman,
1999; Achiam et al., 2017; Taleghan and Dietterich, 2018; Yang et al., 2020). While these methods
do not require knowledge of the cost functions a-priori, they only guarantee the cost constraints
hold during test time. Additionally, using C-MDPs can yield other complications, such as optimal
policies being stochastic and the constraints only holding for a subset of states.
Differentiable optimization layers. A great deal of recent work has studied differentiable op-
timization layers for neural networks: e.g., layers for quadratic programming (Amos and Kolter,
2017), SAT solving (Wang et al., 2019), submodular optimization (Djolonga and Krause, 2017;
Tschiatschek et al., 2018), cone programs (Agrawal et al., 2019), and other classes of optimization
problems (Gould et al., 2019). These layers can be used to construct neural networks with useful
inductive bias for particular domains or to enforce that networks obey hard constraints dictated by
the settings in which they are used. We create fast, custom differentiable optimization layers for the
latter purpose, namely, to project neural network outputs into a set of certifiably stabilizing actions.
2
Published as a conference paper at ICLR 2021
3	Background on LQR and robust control specifications
In this paper, our aim is to control nonlinear (continuous-time) dynamical systems of the form
X(t) ∈ A(t)x(t) + B(t)u(t) + G(t)w(t),	(1)
where x(t) ∈ Rs denotes the state at time t; u(t) ∈ Ra is the control input; w(t) ∈ Rd captures
both external (possibly stochastic) disturbances and any modeling discrepancies; X(t) denotes the
time derivative of the state x at time t; and A(t) ∈ Rs×s, B(t) ∈ Rs×a, G(t) ∈ Rs×d. This class
of models is referred to as linear differential inclusions (LDIs); however, we note that despite the
name, this class does indeed characterize nonlinear systems, as, e.g., w(t) can depend arbitrarily
on x(t) and u(t) (though we omit this dependence in the notation for brevity). Within this class
of models, it is often possible to construct robust control specifications certifying system stability.
Given such specifications, our proposal is to learn nonlinear (deep neural network-based) policies
that provably satisfy these specifications while optimizing some objective of interest. We start by
giving background on the robust control specifications and objectives considered in this work.
3.1	Robust control specifications
In the continuous-time, infinite-horizon settings we consider here, the goal of robust control is often
to construct a time-invariant control policy u(t) = π(x(t)), alongside some certification that guar-
antees that the controlled system will be stable (i.e., that trajectories of the system will converge
to an equilibrium state, usually x = 0 by convention; see Haddad and Chellaboina (2011) for a
more formal definition). For many classes of systems,1 this certification is typically in the form of a
positive definite Lyapunov function V : Rs → R, with V (0) = 0 and V (x) > 0 for all x 6= 0, such
that the function is decreasing along trajectories - for instance,
, ,.. ,..
V(x(t)) ≤ -αV(x(t))	(2)
for some design parameter α > 0. (This particular condition implies exponential stability with a rate
of convergence α.2) For certain classes of bounded dynamical systems, time-invariant linear control
policies u(t) = Kx(t), and quadratic Lyapunov functions V (x) = xTP x, it is possible to construct
such guarantees using semidefinite programming. For instance, consider the class of norm-bounded
LDIs (NLDIs)
X = Ax(t) + Bu(t) + Gw(t), ∣∣w(t)k2 ≤ ∣∣Cx(t) + Du(t)k2,	(3)
where A ∈ Rs×s, B ∈ Rs×a, G ∈ Rs×d, C ∈ Rk×s, and D ∈ Rk×a are time-invariant and known,
and the disturbance w(t) is arbitrary (and unknown) but obeys the norm bounds above.3 For these
systems, it is possible to specify a set of stabilizing policies via a set of linear matrix inequalities
(LMIs, Boyd et al. (1994)):
[AS + SAT + μGGT + BY + Y T BT + aS SC T + Y T DTl	.n
CS + DY	-μI ]W 0，S A 0，μ> 0，	(4)
where S ∈ Rs×s and Y ∈ Ra×s. For matrices S and Y satisfying (4), K = YS-1 and P = S-1
are then a stabilizing linear controller gain and Lyapunov matrix, respectively. While the LMI above
is specific to NLDI systems, this general paradigm of constructing stability specifications using
LMIs applies to many settings commonly considered in robust control (e.g., settings with norm-
bounded disturbances or polytopic uncertainty, or H∞ control settings). More details about these
types of formulations are given in, e.g., Boyd et al. (1994); in addition, we provide the relevant LMI
constraints for the settings we consider in this work in Appendix A.
1In this work, we consider sub-classes of system (1) that may indeed be stochastic (e.g., due to a stochastic
external disturbance w(t)), but that can be bounded so as to be amenable to deterministic stability analysis.
However, other settings may require stochastic stability analysis; please see Astrom (1971).
2See, e.g., Haddad and Chellaboina (2011) for a more rigorous definition of (local and global) exponential
stability. Condition (2) comes from Lyapunov’s Theorem, which characterizes various notions of stability using
Lyapunov functions.
3A slightly more complex formulation involves an additional term in the norm bound, i.e., C x(t) + Du(t) +
Hw(t), which creates a quadratic inequality in w. The mechanics of obtaining robustness specifications in this
setting are largely the same as presented here, though with some additional terms in the equations. As such, as
is often done, we assume that H = 0 for simplicity.
3
Published as a conference paper at ICLR 2021
3.2	LQR control objectives
In addition to designing for stability, it is often desirable to optimize some objective characterizing
controller performance. While our method can optimize performance with respect to any arbitrary
cost or reward function, to make comparisons with existing methods easier, for this paper we con-
sider the well-known infinite-horizon “linear-quadratic regulator” (LQR) cost, defined as
∞
0
(x(t)TQx(t) + u(t)TRu(t)) dt,
(5)
for some Q ∈ Ss×s 0 and R ∈ Sa×a 0. If the control policy is assumed to be time-invariant
and linear as described above (i.e., u(t) = Kx(t)), minimizing the LQR cost subject to stability con-
straints can be cast as an SDP (see, e.g., Yao et al. (2001)) and solved using off-the-shelf numerical
solvers - a fact that We exploit in our work. For example, to obtain an optimal linear time-invariant
controller for the NLDI systems described above, we can solve
minimize tr(QS) + tr(R1/2Y S-1Y TR1/2) s. t. Equation (4) holds.
(6)
4	Enforcing robust control guarantees within neural networks
We now present the main contribution of our paper: A class of nonlinear control policies, poten-
tially parameterized by deep neural networks, that is guaranteed to obey the same stability condi-
tions enforced by the robustness specifications described above. The key insight of our approach is
as follows: While it is difficult to derive specifications that globally characterize the stability of a
generic nonlinear controller, if we are given known robustness specifications, we can create a suf-
ficient condition for stability by simply enforcing that our policy satisfies these specifications at all
t. For instance, given a known Lyapunov function, we can enforce exponential stability by ensuring
that our policy sufficiently decreases this function (e.g., satisfies Equation (2)) at any given x(t).
In the following sections, we present our nonlinear policy class, as well as our general framework
for learning provably robust policies using this policy class. We then derive the instantiation of
this framework for various settings of interest. In particular, this involves constructing (custom)
differentiable projections that can be used to adjust the output of a nominal neural network to satisfy
desired robustness criteria. For simplicity of notation, we will often suppress the t-dependence of x,
u, and w, but we note that these are continuous-time quantities as before.
4.1	A provab ly robust nonlinear policy class
Given a dynamical system of the form (1) and a quadratic Lyapunov function V (x) = xTPx, let
C(x) := {u ∈ Ra | V(X) ≤ -αV(x) ∀x ∈ A(t)x + B(t)u + G(t)w}	(7)
denote a set of actions that, for a fixed state x ∈ Rs, are guaranteed to satisfy the exponential
stability condition (2) (even under worst-case realizations of the disturbance w). We note that this
“safe” set is non-empty if P satisfies the relevant LMI constraints (e.g., system (4) for NLDIs)
characterizing robust linear time-invariant controllers, as there is then some K corresponding to P
such that Kx ∈ C(x) for all states x.
Using this set of actions, we then construct a robust nonlinear policy class that projects the output of
some neural network onto this set. More formally, consider an arbitrary nonlinear (neural network-
based) policy class ∏θ : Rs → Ra parameterized by θ, and let P(.)denote the projection operator
for some set (∙). We then define our robust policy class as ∏θ : Rs → Ra, where
∏θ (x) = Pc(x)(∏θ (x)).	(8)
We note that this policy class is differentiable if the projections can be implemented in a differen-
tiable manner (e.g., using convex optimization layers (Agrawal et al., 2019), though we construct
efficient custom solvers for our purposes). Importantly, as all policies in this class satisfy the stabil-
ity condition (2) for all states x and at all times t, these policies are certifiably robust under the same
conditions as the original (linear) controller for which the Lyapunov function V(x) was constructed.
Given this policy class and some performance objective ` (e.g., LQR cost), our goal is to then find
parameters θ such that the corresponding policy optimizes this objective - i.e., to solve
∞
0
minimize
θ
` ( x, πθ (x) ) dt
s. t. X ∈ A(t)x + B(t)∏θ(x) + G(t)w.
(9)
4
Published as a conference paper at ICLR 2021
Algorithm 1 Learning provably robust controllers with deep RL
1:	input performance objective `
2:	input stability requirement
3:	input policy optimizer A
4:	compute P , K satisfying LMI constraints
5:	construct specifications C(x) using P
6:	construct robust policy class πθ using C
7:	train πθ via A to optimize Equation (9)
8:	return πθ
// e.g., LQR cost
//e.g., V(X) ≤ -αV(x)
// e.g., a planning orRL algorithm
// e.g., by optimizing (6)
// as defined in Equation (7)
// as defined in Equation (8)
Since πθ is differentiable, we can solve this problem via a variety of approaches, e.g., a model-
based planning algorithm if the true dynamics are known, or virtually any (deep) RL algorithm if
the dynamics are unknown. 4
This general procedure for constructing stabilizing controllers is summarized in Algorithm 1. While
seemingly simple, this formulation presents a powerful paradigm: by simply transforming the output
of a neural network, we can employ an expressive policy class to optimize an objective of interest
while ensuring the resultant policy will stabilize the system during both training and testing.
We instantiate our framework by constructing “safe” sets C(x) and their associated (differentiable)
projections PC(x) for three settings of interest: NLDIs, polytopic linear differential inclusions
(PLDIs), and H∞ control settings. As an example, we describe this procedure below for NLDIs, and
refer readers to Appendix B for corresponding formulations for the additional settings we consider.
4.2 Example: NLDIs
In order to apply our framework to the NLDI setting (3), we first compute a quadratic Lyapunov
function V(x) = xTPx by solving the optimization problem (6) for the given system via semidef-
inite programming. We then use the resultant Lyapunov function to compute the system-specific
“safe” set C(x), and then create a fast, custom differentiable solver to project onto this set.
4.2.1	Computing sets of stabilizing actions
Given P, we compute CNLDI(x) as the set of actions u ∈ Ra that, for each state x ∈ Rs, satisfy the
stability condition (2) at that state under even a worst-case realization of the dynamics (i.e., in this
case, even under a worst-case disturbance w). The form of the resultant set is given below.
Theorem 1. Consider the NLDI system (3), some stability parameter α > 0, and a Lyapunov
function V(x) = xTPx with P satisfying Equation (4). Assuming P exists, define
CNLDI(x) := u ∈ Ra | kCx + Duk2 ≤
-xTPB	xT(2PA+αP)x
_______U _ ____________ \
IlGT Pχ∣∣2	2∣∣Gt Px∣∣2	/
for all states x ∈ Rs. For all x, CNLDI(x) is a non-empty set of actions that satisfy the exponential
stability condition (2). Further, CNLDI(x) is a convex set in u.
Proof. We seek to find a set of actions such that the condition (2) is satisfied along all possible
trajectories of (3). A set of actions satisfying this condition at a given x is given by
CNLDI (x) :
u ∈ Ra |
sup
w"∣wk2≤kCx+Du∣∣2
V(x) ≤ —αV(x)}.
Let S := {w : IwI2 ≤ ICx + DuI2}. We can then rewrite the left side of the above inequality as
SuP V(x) = SuP XTPx + xτPx = 2xτP(Ax + BU) + SuP 2xτPGw
w∈S	w∈S	w∈S
= 2xT P (Ax + Bu) + 2IGT P xI2 ICx + DuI2,
by the definition of the NLDI dynamics and the closed-form minimization of a linear term over an
L2 ball. Rearranging yields an inequality of the desired form. We note that by definition of the
4While this problem is infinite-horizon and continuous in time, in practice, one would optimize it in discrete
time over a large finite time horizon.
5
Published as a conference paper at ICLR 2021
specifications (4), there is some K corresponding to P such that the policy u = Kx satisfies the
exponential stability condition (2); thus, Kx ∈ CNLDI, and CNLDI is non-empty. Further, as the above
inequality represents a second-order cone constraint in u, this set is convex in u.	□
We further consider the special case where D = 0, i.e., the norm bound on w does not depend on
the control action. This form of NLDI arises in many common settings (e.g., where w characterizes
linearization error in a nonlinear system but the dynamics depend only linearly on the action), and
is one for which we can compute the relevant projection in closed form (as described shortly).
Corollary 1.1. Consider the NLDI system (3) with D = 0, some stability parameter α > 0, and
Lyapunov function V (x) = xTPx with P satisfying Equation (4). Assuming P exists, define
CNLDI-O(x) := {u ∈ Ra | 2xTPBu ≤ -XT(2PA + aP)x - 2∣∣GtPx∣∣2∣∣Cx∣∣2}
for all states x ∈ Rs. For all x, CNLDI-0(x) is a non-empty set of actions that satisfy the exponential
stability condition (2). Further, CNLDI-0(x) is a convex set in u.
Proof. The result follows by setting D = 0 in Theorem 1 and rearranging terms. As the above
inequality represents a linear constraint in u, this set is convex in u.	□
4.2.2	Deriving efficient, differentiable projections
For the general NLDI setting (3), we note that the relevant projection PCNLDI(x) (see Theorem 1)
represents a projection onto a second-order cone constraint. As this projection does not necessarily
have a closed form, we must implement it using a differentiable optimization solver (e.g., Agrawal
et al. (2019)). For computational efficiency purposes, we implement a custom solver that employs an
accelerated projected dual gradient method for the forward pass, and employs implicit differentiation
through the fixed point equations of this solution method to compute relevant gradients for the
backward pass. Derivations and additional details are provided in Appendix C.
In the case where D = 0 (see Corollary 1.1), we note that the projection operation PCNLDI-0(x) does
have a closed form, and can in fact be implemented via a single ReLU operation. Specifically,
defining ηT := 2xTPB and ζ := -xT (2P A + αP)x - 2kGT P xk2 kCxk2, we see that
π	,,( π	∕π(X)	if ητπ(x) ≤ Z	p τττ ηTτΠ(χ) - ζ∖	/1G
PCNLDI-0(x) (π(X)) = ∖ ʌ/、 ητ∏(x)-ζ	4	.	=π(X) - ReLU { -----τ----- η. (IO)
[∏(x)——/ ηT; Z η otherwise	∖ ητ η )
5 Experiments
Having instantiated our general framework, we demonstrate the power of our approach on a variety
of simulated control domains.5 In particular, we evaluate performance on the following metrics:
•	Average-case performance: How well does the method optimize the performance objec-
tive (i.e., LQR cost) under average (non-worst case) dynamics?
•	Worst-case stability: Does the method remain stable even when subjected to adversarial
(worst-case) dynamics?
In all cases, we show that our method is able to improve performance over traditional robust con-
trollers under average conditions, while still guaranteeing stability under worst-case conditions.
5.1	Description of dynamics settings
We evaluate our approach on five NLDI settings: two synthetic NLDI domains, the cart-pole task,
a quadrotor domain, and a microgrid domain. (Additional experiments for PLDI and H∞ control
settings are described in Appendix I.) For each setting, we choose a time discretization based on the
speed at which the system evolves, and run each episode for 200 steps over this discretization. In all
cases except the microgrid setting, we use a randomly generated LQR objective where the matrices
Q1/2 and R1/2 are drawn i.i.d. from a standard normal distribution.
5Code	for all experiments is available at https://github.com/locuslab/
robust-nn-control
6
Published as a conference paper at ICLR 2021
tnu ko^okŋmso PUSOJyW
Robust Methods
~l*^~*~≠∙w	工.l. l. I .l
0	250	500	750	1000
Training epochs
MBP PPO - RARL — Robust MBP* - Robust PPO*
Setting: -------Original ----------Adversarial
Figure 1: Test performance over training epochs for all learning methods employed in our experi-
ments. For each training epoch (10 updates for the MBP model and 18 for PPO), we report average
quadratic loss over 50 episodes, and use “X” to indicate cases where the relevant method became
unstable. (Lower loss is better.) Our robust methods (denoted by *), unlike the non-robust methods
and RARL, remain stable under adversarial dynamics throughout training.
Synthetic NLDI settings. We generate NLDIs of the form (3) with s = 5, a = 3, and d =
k = 2 by generating matrices A, B, G, C and D i.i.d. from normal distributions, and producing the
disturbance w(t) using a randomly-initialized neural network (with its output scaled to satisfy the
norm-bound on the disturbance). We investigate settings both where D 6= 0 and where D = 0. In
both cases, episodes are run for 2 seconds at a discretization of 0.01 seconds.
Cart-pole. In the cart-pole task, our goal is to balance an inverted pendulum resting on top of a cart
by exerting horizontal forces on the cart. For our experiments, we linearize this system as an NLDI
with D 6= 0 (see Appendix D), and add a small additional randomized disturbance satisfying the
NLDI bounds. Episodes are run for 10 seconds at a discretization of 0.05 seconds.
Planar quadrotor. In this setting, our goal is to stabilize a quadcopter in the two-dimensional plane
by controlling the amount of force provided by the quadcopter’s right and left thrusters. We linearize
this system as an NLDI with D = 0 (see Appendix E), and add a small disturbance as in the cart-pole
setting. Episodes are run for 4 seconds at a discretization of 0.02 seconds.
Microgrid. In this final setting, we aim to stabilize a microgrid by controlling a storage device and
a solar inverter. We augment the system given in Lam et al. (2016) with LQR matrices and NLDI
bounds (see Appendix F). Episodes are run for 2 seconds at a discretization of 0.01 seconds.
7
Published as a conference paper at ICLR 2021
5.2	Experimental setup
We demonstrate our approach by constructing a robust policy class (8) for each of these settings, and
optimizing this policy class via different approaches. Specifically, we construct a nominal nonlinear
control policy class as ∏θ (x) = Kx + ∏θ (x), where K is obtained via robust LQR optimization (6),
and where ∏θ (x) is a feedforward neural network. To construct the projections PC, We employ the
value of P obtained when solving for K. For the purposes of demonstration, we then optimize our
robust policy class ∏θ(x) = PC(∏θ(x)) using two different methods:
•	Robust MBP (ours): A model-based planner that assumes the true dynamics are known.
•	Robust PPO (ours): An RL approach based on PPO (Schulman et al., 2017) that does not
assume known dynamics (beyond the bounds used to construct the robust policy class).
Robust MBP is optimized using gradient descent for 1,000 updates, where each update samples 20
roll-outs. Robust PPO is trained for 50,000 updates, where each update samples 8 roll-outs; we
choose the model that performs best on a hold-out set of initial conditions during training. We note
that while we use PPO for our demonstration, our approach is agnostic to the particular method of
training, and can be deployed with many different (deep) RL paradigms.
We compare our robust neural network-based method against the following baselines:
•	Robust LQR: Robust (linear) LQR controller obtained via Equation (6).
•	Robust MPC: A robust model-predictive control algorithm (Kothare et al., 1996) based
on state-dependent LMIs. (As the relevant LMIs are not always guaranteed to solve, our
implementation temporarily reverts to the Robust LQR policy when that occurs.)
•	RARL: The robust adversarial reinforcement learning algorithm (Pinto et al., 2017), which
trains an RL agent in the presence of an adversary. (We note that unlike the other robust
methods considered here, this method is not provably robust.)
•	LQR: A standard non-robust (linear) LQR controller.
•	MBP and PPO: The non-robust neural network policy class ∏θ (x) optimized via a model-
based planner and the PPO algorithm, respectively.
In order to evaluate performance, we train all methods on the dynamical settings described in Sec-
tion 5.1, and evaluate them on two different variations of the dynamics:
•	Original dynamics: The dynamical settings described above (“average case”).
•	Adversarial dynamics: Modified dynamics with an adversarial test-time disturbance w(t)
generated to maximize loss (“worst case”). We generate this disturbance separately for
each method described above (see Appendix G for more details).
Initialization states are randomly generated for all experiments. For the synthetic NLDI and mi-
crogrid settings, these are generated from a standard normal distribution. For both cart-pole and
quadrotor, because our NLDI bounds model linearization error, we must generate initial points
within a region where this linearization holds. In particular, the linearization bounds only hold
for a specified L∞ ball, BNLDI, around the equilibrium. We use a simple heuristic to construct this
ball and jointly find a smaller L∞ ball, Binit , such that there exists a level set L of the Robust LQR
Lyapunov function with Binit ⊆ L ⊆ BNLDI (details in Appendix H). Since Robust LQR (and by
extension our methods) are guaranteed to decrease the relevant Lyapunov function, this guarantees
that these methods will never leave BNLDI when initialized starting from any point inside Binit — i.e.,
that our NLDI bounds will always hold throughout the trajectories produced by these methods.
5.3	Results
Table 1 shows the performance of the above methods. We report the integral of the quadratic loss
over the prescribed time horizon on a test set of states, or indicate cases where the relevant method
became unstable (i.e., the loss became orders of magnitude larger than for other approaches). (Sam-
ple trajectories for these methods are also provided in Appendix H.)
These results illustrate the basic advantage of our approach. In particular, both our Robust MBP
and Robust PPO methods show improved “average-case” performance over the other provably
robust methods (namely, Robust LQR and Robust MPC). As expected, however, the non-robust
8
Published as a conference paper at ICLR 2021
Environment		LQR	MBP	PPO	Robust LQR	Robust MPC	RARL	Robust MBP*	Robust PPO*
Generic NLDI	O	-373-	16	21	-^253^^	253	27	69	33
(D = 0)	A	—	unstable	——	1009	873	unstable	1111	2321
Generic NLDI	O	-278-	15	82	-^199^^	199	147	69	80
(D 6= 0)	A	—	unstable	——	1900	1667	unstable	1855	1669
Cart-pole	O	-363^^	3.6	7.2	-^10.2^^	10.2	8.3	9.7	8.4
	A	—unstable —		172.1	42.2	47.8	41.2	50.0	16.3
Quadrotor	O	5.4	2.5	7.7	-^13.8^^	13.8	12.2	11.0	8.3
	A	unstable	545.7	137.6	64.8	unstable *	63.1	25.7	26.5
Microgrid	O	-459-	0.60	0.61	-^073^^	0.73	0.67	0.61	0.61
	A	—	unstable	——	0.99	0.92	2.17	7.68	8.91
Table 1: Performance of various approaches, both robust (right) and non-robust (left). We report
average quadratic loss over 50 episodes under the original dynamics (O) and under an adversarial
disturbance (A). For the original dynamics (O), the best performance for both non-robust methods
and robust methods is in bold (lower loss is better). Under the adversarial dynamics (A), we seek
to observe whether or not methods remain stable; we use “unstable” to indicate cases where the
relevant method becomes unstable (and * to denote any instabilities due to numerical, rather than
theoretical, issues). Our robust methods (denoted by *) improve performance over Robust LQR and
Robust MPC in the average case while remaining stable under adversarial dynamics, whereas the
non-robust methods and RARL either go unstable or receive much larger losses.
LQR, MBP, and PPO methods often perform better within the original nominal dynamics, as they
are optimizing for expected performance but do not need to consider robustness under worst-case
scenarios. However, when we apply allowable adversarial perturbations (that still respect our dis-
turbance bounds), the non-robust LQR, MBP, and PPO approaches diverge or perform very poorly.
Similarly, the RARL agent performs well under the original dynamics, but diverges under adversar-
ial perturbations in the generic NLDI settings. In contrast, both of our provably robust approaches
(as well as Robust LQR) remain stable under even “worst-case” adversarial dynamics. (We note
that the baseline Robust MPC method goes unstable in one instance, though this is due to numerical
instability issues, rather than issues with theoretical guarantees.)
Figure 1 additionally shows the performance of all neural network-based methods on the test set
over training epochs. While the robust and non-robust MBP and PPO approaches both converge
quickly to their final performance levels, both non-robust versions become unstable under the ad-
versarial dynamics very early in the process. The RARL method also frequently destabilizes during
training. Our Robust MBP and PPO policies, on the other hand, remain stable throughout the
entire optimization process, i.e., do not destabilize during either training or testing. Overall, these
results show that our method is able to learn policies that are more expressive than traditional robust
methods, while guaranteeing these policies will be stable under the same conditions as Robust LQR.
6 Conclusion
In this paper, we have presented a class of nonlinear control policies that combines the expressive-
ness of neural networks with the provable stability guarantees of traditional robust control. This
policy class entails projecting the output of a neural network onto a set of stabilizing actions, param-
eterized via robustness specifications from the robust control literature, and can be optimized using
a model-based planning algorithm if the dynamics are known or virtually any RL algorithm if the
dynamics are unknown. We instantiate our general framework for dynamical systems characterized
by several classes of linear differential inclusions that capture many common robust control settings.
In particular, this entails deriving efficient, differentiable projections for each setting, via implicit
differentiation techniques. We show over a variety of simulated domains that our method improves
upon traditional robust LQR techniques while, unlike non-robust LQR and neural network methods,
remaining stable even under worst-case allowable perturbations of the underlying dynamics.
We believe that our approach highlights the possible connections between traditional control meth-
ods and (deep) RL methods. Specifically, by enforcing more structure in the classes of deep net-
works we consider, it is possible to produce networks that provably satisfy many of the constraints
that have typically been thought of as outside the realm of RL. We hope that this work paves the way
for future approaches that can combine more structured uncertainty or robustness guarantees with
RL, in order to improve performance in settings traditionally dominated by classical robust control.
9
Published as a conference paper at ICLR 2021
Acknowledgments
This work was supported by the Department of Energy Computational Science Graduate Fellowship
(DE-FG02-97ER25308), the Center for Climate and Energy Decision Making through a cooper-
ative agreement between the National Science Foundation and Carnegie Mellon University (SES-
00949710), the Computational Sustainability Network, and the Bosch Center for AI. This material
is based upon work supported by the National Science Foundation Graduate Research Fellowship
Program under Grant No. DGE1745016. Any opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do not necessarily reflect the views of
the National Science Foundation.
We thank Vaishnavh Nagarajan, Filipe de Avila Belbute Peres, Anit Sahu, Asher Trockman, Eric
Wong, and anonymous reviewers for their feedback on this work.
References
Kemin Zhou and John Comstock Doyle. Essentials of Robust Control, volume 104. Prentice hall
Upper Saddle River, NJ, 1998.
Tamer BaSar and Pierre Bernhard. H∞-Optimal Control and Related Minimax Design Problems: A
Dynamic Game Approach. Springer Science & Business Media, 2008.
Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear Matrix
Inequalities in System and Control Theory, volume 15. Siam, 1994.
Mayuresh V Kothare, Venkataramanan Balakrishnan, and Manfred Morari. Robust constrained
model predictive control using linear matrix inequalities. Automatica, 32(10):1361-1379, 1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving Rubik’s Cube with a
Robot Hand. arXiv preprint arXiv:1910.07113, 2019.
LUcian Buyoniu, Tim de Bruin, Domagoj Tolic, Jens Kober, and Ivana Palunko. Reinforcement
learning for control: Performance, stability, and deep approximators. Annual Reviews in Control,
46:8-28, 2018.
Jun Morimoto and Kenji Doya. Robust Reinforcement Learning. Neural Computation, 17(2):335-
359, 2005.
Murad Abu-Khalaf, Frank L Lewis, and Jie Huang. Policy Iterations on the Hamilton-Jacobi-Isaacs
Equation for H∞ State Feedback Control With Input Saturation. IEEE Transactions on Automatic
Control, 51(12):1989-1995, 2006.
Yantao Feng, Brian DO Anderson, and Michael Rotkowitz. A game theoretic algorithm to compute
local stabilizing solutions to HJBI equations in nonlinear H∞ control. Automatica, 45(4):881-
888, 2009.
Derong Liu, Hongliang Li, and Ding Wang. Neural-network-based zero-sum game for discrete-time
nonlinear systems via iterative adaptive dynamic programming algorithm. Neurocomputing, 110:
92-100, 2013.
Huai-Ning Wu and Biao Luo. Simultaneous policy update algorithms for learning the solution of
linear continuous-time H∞ state feedback control. Information Sciences, 222:472-485, 2013.
Biao Luo, Huai-Ning Wu, and Tingwen Huang. Off-Policy Reinforcement Learning for H∞ Control
Design. IEEE Transactions on Cybernetics, 45(1):65-76, 2014.
Stefan R Friedrich and Martin Buss. A robust stability approach to robot reinforcement learning
based on a parameterization of stabilizing controllers. In 2017 IEEE International Conference on
Robotics and Automation (ICRA), pages 3365-3372. IEEE, 2017.
10
Published as a conference paper at ICLR 2021
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust Adversarial Rein-
forcement Learning. In Proceedings of the 34th International Conference on Machine Learning,
pages 2817-2826. JMLR. org, 2017.
Ming Jin and Javad Lavaei. Stability-certified reinforcement learning: A control-theoretic perspec-
tive. arXiv preprint arXiv:1810.11505, 2018.
Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural Lyapunov Control. In Advances in Neural
Information Processing Systems, pages 3245-3254, 2019.
Minghao Han, Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. H∞ Model-free Reinforcement
Learning with Robust Stability Guarantee. CoRR, 2019.
Kaiqing Zhang, Bin Hu, and Tamer Basar. Policy Optimization for H2 Linear Control with H∞ Ro-
bustness Guarantee: Implicit Regularization and Global Convergence. In Learning for Dynamics
and Control, pages 179-190. PMLR, 2020.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe Exploration in Finite Markov
Decision Processes with Gaussian Processes. In Advances in Neural Information Processing
Systems, 2016.
Anayo K. Akametalu, Shahab Kaynama, Jaime F. Fisac, Melanie Nicole Zeilinger, Jeremy H.
Gillula, and Claire J. Tomlin. Reachability-based safe learning with Gaussian processes. In
53rd IEEE Conference on Decision and Control, CDC 2014, 2014.
Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe Model-based
Reinforcement Learning with Stability Guarantees. In Advances in Neural Information Process-
ing Systems, 2017.
Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe Exploration and Optimization
of Constrained MDPs Using Gaussian Processes. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 32, 2018.
Eitan Altman. Constrained Markov Decision Processes, volume 7. CRC Press, 1999.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained Policy Optimization. In
Proceedings of the 34th International Conference on Machine Learning, 2017.
Majid Alkaee Taleghan and Thomas G. Dietterich. Efficient Exploration for Constrained MDPs. In
2018 AAAI Spring Symposia, 2018.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-Based
Constrained Policy Optimization. In International Conference on Learning Representations,
2020.
Brandon Amos and J Zico Kolter. OptNet: Differentiable Optimization as a Layer in Neural Net-
works. In Proceedings of the 34th International Conference on Machine Learning, pages 136-
145, 2017.
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. SATNet: Bridging deep learning and
logical reasoning using a differentiable satisfiability solver. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, pages 6545-6554, 2019.
Josip Djolonga and Andreas Krause. Differentiable Learning of Submodular Models. In Advances
in Neural Information Processing Systems, pages 1013-1023, 2017.
Sebastian Tschiatschek, Aytunc Sahin, and Andreas Krause. Differentiable Submodular Maximiza-
tion. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages
2731-2738, 2018.
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, andJ Zico Kolter.
Differentiable Convex Optimization Layers. In Advances in Neural Information Processing Sys-
tems, pages 9558-9570, 2019.
11
Published as a conference paper at ICLR 2021
Stephen Gould, Richard Hartley, and Dylan Campbell. Deep Declarative Networks: A New Hope.
arXiv preprint arXiv:1909.04866, 2019.
Wassim M Haddad and VijaySekhar Chellaboina. Nonlinear Dynamical Systems and Control: A
Lyapunov-Based Approach. Princeton University Press, 2011.
Karl J Astrom. Introduction to Stochastic Control Theory. Elsevier, 1971.
David D Yao, Shuzhong Zhang, and Xun Yu Zhou. A primal-dual semi-definite programming
approach to linear quadratic control. IEEE Transactions on Automatic Control, 46(9):1442-1447,
2001.
Quang Linh Lam, Antoneta Iuliana Bratcu, and Delphine Riu. Frequency Robust Control in Stand-
alone Microgrids with PV Sources: Design and Sensitivity Analysis. 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017.
Hassan K Khalil and Jessy W Grizzle. Nonlinear Systems, volume 3. Prentice Hall Upper Saddle
River, NJ, 2002.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87.
Springer Science & Business Media, 2013.
Heinz H Bauschke. Projection algorithms and monotone operators. PhD thesis, Dept. of Mathe-
matics and Statistics, Simon Fraser University, 1996.
Russ Tedrake. Underactuated robotics: Learning, planning, and control for efficient and agile ma-
chines course notes for MIT 6.832. Working draft edition, 3, 2009.
Sumeet Singh, Spencer M Richards, Vikas Sindhwani, Jean-Jacques E Slotine, and Marco Pavone.
Learning stabilizable nonlinear dynamics with contraction-based regularization. The Interna-
tional Journal of Robotics Research, page 0278364920949931, 2020.
Roman Kuiava, Rodrigo A Ramos, and Hemanshu R Pota. A New Method to Design Robust Power
Oscillation Dampers for Distributed Synchronous Generation Systems. Journal of Dynamic Sys-
tems, Measurement, and Control, 135(3), 2013.
12
Published as a conference paper at ICLR 2021
A Details on robust control specifications
As described in Section 3.1, for many dynamical systems of the form (1), it is possible to specify
a set of linear, time-invariant policies guaranteeing infinite-horizon exponential stability via a set
of LMIs. Here, we derive the LMI (4) provided in the main text for the NLDI system (3), and
additionally describe relevant LMI systems for systems characterized by polytopic linear differential
inclusions (PLDIs) and for H∞ control settings.
A.1 Exponential stability in NLDIs
Consider the general NLDI system (3). We seek to design a time-invariant control policy u(t) =
Kx(t) and a quadratic Lyapunov function V (x) = xTPx with P 0 for this system that satisfy
the exponential stability criterion V (x) ≤ -αV (x), ∀t. We derive an LMI characterizing such a
controller and Lyapunov function, closely following and expanding upon the derivation provided
in Boyd et al. (1994).
Specifically, consider the NLDI system (3), reproduced below:
X = Ax + Bu + Gw, kw∣∣2 ≤ ∣∣Cχ + Du∣∣2.	(A.1)
The time derivative of this Lyapunov function along the trajectories of the closed-loop system is
V (x) = X T Px + XT PX
= (Ax + Bu + Gw)TPx + xTP(Ax + Bu + Gw)
= ((A + BK)X + Gw)TPX + XTP((A + BK)X + Gw)
XT (A+BK)TP+P(A+BK) PG	X
= w	GTP	0	w	.
(A.2)
EI	. ∙ 1 . 1 ∙ 1 ∙ .	∙>♦,♦	T½-	T 7- / ∖ ∙ . <	♦	1 ∙ 11 ♦	1 ∙.
The exponential stability condition V (X) ≤ -αV (X) is thus implied by inequality
X T X X T (A+BK)TP+P(A+BK)+αP
w M1 w := w	GTP
PG
0
≤ 0.
(A.3)
X
w
Additionally, the norm bound on w can be equivalently expressed as
T
M2
X XT (C + DK)T(C + DK)
w:	w	0
≥ 0.
(A.4)
X
w
Using the S-procedure, it follows that for some λ ≥ 0, the following matrix inequality is a sufficient
condition for exponential stability:
M1 + λM2	0.	(A.5)
Using Schur Complements, this matrix inequality is equivalent to
(A + BK)tP + P(A + BK) + αP + λ(C + DK)t(C + DK) + 1 PGGTP W 0.	(a.6)
λ
Left- and right-multiplying both sides by P-1, and making the change of variables S = P-1,
Y = KS, and μ = 1∕λ, We obtain
SAT + AS + YTBt + BY + αS + 1 (SCT + YTDT) (CS + DY) + μGGτ W 0.	(A.7)
Using Schur Complements again on this inequality, We obtain our final system of linear matrix
inequalities as
AS + SAt + μGGτ + BY + YT Bt + aS SCT + YT DT
CS + DY
W 0, S A 0, μ > 0, (A.8)
Where then K = YS-1 and P = S-1. Note that the first matrix inequality is homogeneous; We can
therefore assume μ = 1 (and therefore, λ = 1), without loss of generality.
13
Published as a conference paper at ICLR 2021
A.2 Exponential stability in PLDIs
Consider the setting of polytopic linear differential inclusions (PLDIs), where the dynamics are of
the form
x(t) = A(t)x(t) + B(t)u(t), (A(t), B(t)) ∈ Conv{(Aι,Bι),..., (Al, BL)}.	(A.9)
Here, A(t) ∈ Rs×s and B(t) ∈ Rs×a can vary arbitrarily over time, as long as they lie in the convex
hull (denoted Conv) of the set of points above, where Ai ∈ Rs×s, Bi ∈ Rs×a for i = 1, . . . , L.
We seek to design a time-invariant control policy u(t) = K x(t) and quadratic Lyapunov function
V (x) = xTP x with P 0 for this system that satisfy the exponential stability criterion V(X) ≤
-αV (x), ∀t. Such a controller and Lyapunov function exist if there exist S ∈ Rs×s 0 and
Y ∈ Ra×s such that
AiS + BiY + SAiT +YTBiT +αS	0, ∀i = 1, . . .,L,	(A.10)
where then K = Y S-1 and P = S-1. The derivation of this LMI follows similarly to that for
exponential stability in NLDIs, and is well-described in Boyd et al. (1994).
A.3 H∞ CONTROL
Consider the following H∞ control setting with linear time-invariant dynamics
X(t) = Ax(t) + Bu(t) + Gw(t), W ∈ L2,	(A.11)
where A, B, and G are time-invariant as for the NLDI case, and where we define L2 as the set of
time-dependent signals with finite L2 norm.6
In cases such as these with larger or more unstructured disturbances, it may not be possible to
guarantee asymptotic convergence to an equilibrium. In these cases, our goal is to construct a
robust controller with bounds on the extent to which disturbances affect some performance output
(e.g., LQR cost), as characterized by the L2 gain of the disturbance-to-output map. Specifically,
we consider the stability requirement that this L2 gain be bounded by some parameter γ > 0 when
disturbances are present, and that the system be exponentially stable in the disturbance-free case.
This requirement can be characterized via the condition that for all t and some σ ≥ 0,
E(x, X, u) := V(x) + αV(x) + σ (XTQx + UTRu — γ2kwk22 ≤ 0.	(A.12)
We note that when E(x(t),x(t), u(t)) ≤ 0 for all t, both of our stability criteria are met. To see this,
note that integrating both sides of (A.12) from 0 to ∞ and ignoring the non-negative terms on the
left hand side after integration yields
∞
0
∞
0
(x(t)T Qx(t)+u(t)T Ru(t))dt ≤ γ2
kw(t)k2dt + (1∕σ)V (x(0)).
(A.13)
This is precisely the desired bound on the L2 gain of the disturbance-to-output map (see Khalil and
Grizzle (2002)). We also note that in the disturbance-free case, substituting w = 0 into (A.12) yields
V(x) ≤ —αV(x) — σ (XTQx + uTRu) ≤ —αV(x),	(A.14)
where the last inequality follows from the non-negativity of the LQR cost; this is precisely our
condition for exponential stability.
We now seek to design a time-invariant control policy u(t) = Kx(t) and quadratic Lyapunov func-
tion V(x) = xTPx with P 0 that satisfies the above condition. In particular, we can write
E (x(t), (A + BK)x(t) + Gw(t), Kx(t))= wx((tt)) TM1 wx((tt)) ,	(A.15)
where
(A+BK)TP+P(A+BK)+αP+σ(Q+KTRK) PG
MI=	GTP	-γ2σI . (A.16)
6The L2 norm of a time-dependent signal w(t) : [0, ∞) → Rd is defined as
√R∞ kw(t)k2dt.
14
Published as a conference paper at ICLR 2021
Therefore, we seek to find a P ∈ Rs×s 0 and K ∈ Rs×a that satisfy M1	0, for some design
parameters α > 0 and σ > 0. Using Schur complements, the matrix inequality M1	0 is equivalent
to
(A + BK )T P + P (A + BK) + αP + σ(Q + K T RK) + PGGT P∕(γ2σ) W 0.	(A.17)
As in Appendix A.1, we left- and right-multiply both sides by P-1, and make the change of variables
S = P-1, Y = KS, and μ = 1∕σ to obtain
SAT + AS + YTBT + BY + αS +1 ((SQ1/2)(Q1/2S) + (YTR1/2)(R1/2Y)) + μGGT∕γ2 W 0.
Using Schur Complements again, we obtain the LMI
-SAT + AS + YT BT + BY + aS + μGGT ∕γ2	[SQ1/2 YT R1/2「
∣^Q"Sl	W 0, S > 0, μ> 0,
[r1/2 y]	-μI
(A.18)
where then K = YS-1, P = S-1, and σ = 1∕μ.
B	Derivation of sets of stabilizing policies and associated
PROJECTIONS
We describe the construction of the set of actions C(x), defined in Equation (7), for PLDI sys-
tems (A.9) and H∞ control settings (A.11). (The relevant formulations for the NLDI system (3) are
described in the main text.)
B.1 Exponential stability in PLDIs
For the general PLDI system (A.9), relevant sets of exponentially stabilizing actions CPLDI are given
by the following theorem.
Theorem B.1. Consider the PLDI system (A.9), some stability parameter α > 0, and a Lyapunov
function V (x) = xTPx with P satisfying (A.10). Assuming P exists, define
CPLDI (X) :=	《u ∈ Ra |	2XTPB1 2xt PB2 . .	u ≤ -	-XT(αP + 2PA1)X xt (αP + 2PA2)X
	I	. 2XTPBL		XT (αP + 2PAL)X
for all states x ∈ Rs. For all x, CPLDI(x) is a non-empty set of actions that satisfy the exponential
stability condition (2). Further, CPLDI(x) is a convex set in u.
Proof. We seek to find a set of actions such that the condition (2) is satisfied along all possible
trajectories of (A.9), i.e., for any allowable instantiation of (A(t), B(t)). A set of actions satisfying
this condition at a given x is given by
- _ ∙ .,. ... ,,. .-
CPLDI(X) = {u ∈ Ra | V(X) ≤ -αV (x) ∀(A(t), B (t)) ∈ C0nv{(A1, B1),..., (AL, Bl)}.
Expanding the left side of the inequality above, we see that for some coefficients γi ∈ R ≥ 0, i =
1, . . . ,L satisfying PiL=1 γi(t) = 1,
V(X) = XTPx + xtPX = 2xTP (A(t)x + B(t)u)
=2XTP (X γi(t)AiX + γi(t)BiU) = X Yi(2XTP(AiX + BiU))
i=1	i=1
by definition of the PLDI dynamics and of the convex hull. Thus, if we can ensure
2XTP (AiX + BiU) ≤ -αV (X) = -αXT P X, ∀i = 1, . . . , L,
then we can ensure that exponential stability holds. Rearranging this condition and writing it in
matrix form yields an inequality of the desired form. We note that by definition of the specifica-
tions (A.10), there is some K corresponding to P such that the policy U = KX satisfies all of
the above inequalities; thus, KX ∈ CPLDI(X), and CPLDI(X) is non-empty. Further, as the above
inequality represents a linear constraint in u, this set is convex in u.	□
15
Published as a conference paper at ICLR 2021
We note that the relevant projection PCPLDI(x) represents a projection onto an intersection of halfs-
paces, and can thus be implemented via differentiable quadratic programming (Amos and Kolter,
2017).
B.2 H∞ CONTROL
For the H∞ control system (A.11), relevant sets of actions satisfying the condition (A.12) are given
by the following theorem.
Theorem B.2. Consider the system (A.11), some stability parameter α > 0, and a Lyapunov func-
tion V (x) = xTPx with P satisfying Equation (A.18). Assuming P exists, define
Ch∞(x):= {u ∈ Ra | UT Ru + (2Bt Px)T U + XT (PA+AT P+αP+Q +γ-2PGGT P) X ≤ 0}
for all states x ∈ Rs. For all x, CH∞ (x) is a non-empty set of actions that guarantee condi-
tion (A.12), i.e., that the L2 gain of the disturbance-to-output map is bounded by γ and that the
system is exponentially stable in the disturbance-free case. Further, CH∞ (X) is convex in U.
Proof. We seek to find a set of actions such that the condition E(x,X,u) ≤ 0 is satisfied along
all possible trajectories of (A.11), where E is defined as in (A.12). A set of actions satisfying this
condition at a given X is given by
Ch∞(x) ：= {u ∈ Ra | sup E(x, X,u) ≤ 0, X = Ax + Bu + Gw}.
w∈L2
To begin, we note that
E(X, AX + Bu + Gw, u) = XTP(AX + Bu + Gw) + (AX + Bu + Gw)TPX + αXTPX
+ σ (XTQx + UTRu — γ2 ∣∣w∣∣2)
We then maximize E over w:
w? = arg max E(x, AX + Bu + Gw, u) = GTPX∕(σγ2).	(B.1)
w
Therefore,
CH∞ (X) = {u | E(X, AX + Bu + Gw?, u, w?) ≤ 0}.	(B.2)
Expanding and rearranging terms, this becomes
CH∞ (x) = {u | uT(σR)u + (2BTPx)tU + xt (PA+AtP+αP+σQ+PGGTP∕(σγ2)) X ≤ 0}.
(B.3)
We note that by definition of the specifications (A.18), there is some K corresponding to P such
that the policy u = KX satisifies the conditions above (see (A.17)); thus, KX ∈ CH∞, and CH∞ is
non-empty. We note further that CH∞ is an ellipsoid in the control action space, and is thus convex
in u.	□
We rewrite the set CH∞ (X) such that the projection PCH (x) can be viewed as a second-order cone
projection, in order to leverage our fast custom solver (Appendix C). In particular, defining P = σR,
q = BTPx, and r = xt (PA+ATP+αP+σQ +PgGtP∕(σγ2)) x, We can rewrite the ellipsoid
above as
Ch∞(x) = {u | u>Pu + 2q>u + r ≤ 0}.	(B.4)
We note that as P * 0 and r 一 GTPT - < o, this ellipsoid is non-empty (see, e.g., section B.1 in
Boyd and Vandenberghe (2004)). We can then rewrite the ellipsoid as
CH∞ (x) = {u | kAGu + Gbk2 ≤ 1}	(B.5)
where A= J于>PPI于-r and b = Jq>pPι -P-1q. The constraint ∣∣Au + b∣∣2 ≤ 1 is then a
second-order cone constraint in u.
16
Published as a conference paper at ICLR 2021
C A fast, differentiable solver for second-order cone
PROJECTION
In order to construct the robust policy class described in Section 4 for the general NLDI system (3)
and the H∞ setting (A.11), we must project a nominal (neural network-based) policy onto the
second-order cone constraints described in Theorem 1 and Appendix B.2, respectively. As this
projection operation does not necessarily have a closed form, we implement it via a custom differ-
entiable optimization solver.
More generally, consider a set of the form
C = {x ∈ Rn | kAx + bk2 ≤ cTx + d}
(C.1)
for some A ∈ Rm×n, b ∈ Rm, c ∈ Rn, and d ∈ R. Given some input y ∈ Rn, we seek to compute
the second-order cone projection PC (y) by solving the problem
minimize -kx — yk2
X∈Rn	2l1 yl 12
subject to kAx + bk2 ≤ cTx + d.
(C.2)
Let F denote the `2 norm cone, i.e., F := {(w, t) | kwk2 ≤ t}. Introducing the auxiliary variable
z ∈ Rm+1, we can then rewrite the above optimization problem equivalently as
minimize
x∈Rn, z∈Rm+1
2 kχ — yk2 + 1F(Z)
subject to
where for brevity we define G
for membership in the set F .
A
cT
Ax + b
cT x + d
: Gx + h,
(C.3)
and h = db , and where 1F denotes the indicator function
We describe our fast solution technique for computing this projection, as well as our method for
obtaining gradients through the solution.
C.1 Computing the projection
We construct a fast solver for problem (C.3) using an accelerated projected dual gradient method.
Specifically, define μ = Rm+1 as the dual variable on the equality constraint in Equation (C.3). The
Lagrangian for this problem can then be written as
L (χ,z,μ) = 1 l∣χ — yk2 + 1f (Z) + μτ (Z — Gx — h),	(C.4)
and the dual problem is given by max* minχ,z L(x, z, μ). To form the dual problem, We minimize
the Lagrangian with respect to x and Z as
inf L(x, z, μ) = inf - {∣x — y∣2 — μτGx} + inf{μτZ + 1f(z)} — μτh.	(C.5)
x,z	x 2	z
We note that the first term on the right side is minimized at x?(μ) = y + Gτμ. Thus, we see that
inf 2{kx — y∣∣2 — μτGx} = — 2μτGGTμ — μτGy.	(Cs)
For the second term, denote μ = (μ, S) and Z = (Z, t). We can then rewrite this term as
inf{μτZ + 1f(z)}} = inf inf {t ∙ S + μτZ | ∣∣Z∣∣2 ≤ t}.	(C.7)
Z	t≥0 2	一
For a fixed t ≥ 0, the above objective is minimized at Z = —t/z∕kμk2. (The problem is infeasible
for t < 0.) Substituting this minimizer into (C.7) and minimizing the result over t ≥ 0 yields
inf {μτ Z+IF(Z)}=in0t(s—问2)=—匕(〃)
(C.8)
17
Published as a conference paper at ICLR 2021
where the last identity follows from definition of the second-order cone F . Hence the negative dual
problem becomes
minimize ^μτGGTμ + μτ(Gy + h) + 1f(μ).	(C.9)
μ 2
We now solve this problem via Nesterov’s accelerated projected dual gradient method (Nesterov,
2013). For notational brevity, define f (μ) := 2μτGGTμ + μτ(Gy + h). Then, starting from
arbitrary μ(-1),μ(0) ∈ Rm+1 We perform the iterative updates
VIk= μ(k) + β(k)(μ(k) - μ(k-D)
μ(k+1)
-L Vf（V（k））），
(C.10)
Where Lf = λmax(GGτ) is the Lipschitz constant of f, and PF is the projection operator onto F
(Which has a closed form solution; see Bauschke (1996)). Letting mf = λmin(GGτ) denote the
strong convexity constant of f, the momentum parameter is then scheduled as (Nesterov, 2013)
βk
k — 1
k + 2
Pf-√mf
√Lf+√m
if mf= 0
if mf > 0.
(C.11)
After computing the optimal dual variable μ?, i.e., the fixed point of (C.10), the optimal primal
variable can be recovered via the equation x? = y + GTμ? (as can be observed from the first-order
conditions of the Lagrangian (C.4)).
C.2 Obtaining gradients
In order to incorporate the above projection into our neural netWork, We need to compute the gradi-
ents of all problem variables (i.e., G, h, and y) through the solution x?. In particular, We note that x?
has a direct dependence on both G and y, and an indirect dependence on all of G, h, and y through
μ?.
To compute the relevant gradients through μ?, we apply the implicit function theorem to the fixed
point of the update equations (C.10). Specifically, as these updates imply that μ? = V?, their fixed
point can be written as
μ? = PF fμ?-
L Vfa)).
(C.12)
Define M :
∂Pf(∙) I
d(∙)	∣(∙)=μ?-Lf Vf (μ?),
and note that Vf (μ?) = GGTμ? + Gy + h. The differential
of the above fixed-point equation is then given by
dμ? = M X
(dGGT μ? + GdGT μ? + GGT dμ? + dGy + Gdy +
(C.13)
Rearranging terms to separate the differentials of problem outputs from problem variables, we see
that
(I — M + ɪMGGT) dμ? = — ɪM (dGGTμ? + GdGTμ? + dGy + Gdy + dh) , (C.14)
where I is the identity matrix of appropriate size.
As described in e.g. Amos and Kolter (2017), we can then use these equations to form the Jacobian
of μ? with respect to any of the problem variables by setting the differential of the relevant problem
variable to I and of all other problem variables to 0; solving the resulting equation for dμ? then
yields the value of the desired Jacobian. However, as these Jacobians can be large depending on
problem size, we rarely want to form them explicitly. Instead, given some backward pass vector
∂∂μ? ∈ R1×(m+1) with respect to the optimal dual variable, we want to directly compute the gradient
18
Published as a conference paper at ICLR 2021
of the loss with respect to the problem variables: e.g., for y, we want to directly form the result of
the product 养 瑞 ∈ R1×n. We do this via a similar method as presented in Amos and Kolter
(2017), and refer the reader there for a more in-depth explanation of the method described below.
Define J := I 一 M + 吉 MGGT to represent the coefficient of dμ? on the left side of Equa-
tion (C.14). Given 急,We then compute the intermediate term
dμ := -J-T (∂μ?) .	(C.15)
We can then form the relevant gradient terms directly as
(∂μ? ∂Gg ) = L M(d"(GT 〃?)T+μ?(GT dμ)T+dμyT)
(∂μ?鉴)T = L M dμ	(C.16)
(养 ∂μ? )t=力 GT M d..
In these computations, We note that as our solver returns x?, the backWard pass vector We are given
is actually ∂X? ∈ R1 ×n; thus, we compute 券 =∂∂'?法 =∂X?GT for USe in Equation (C.15).
Accounting additionally for the direct dependence of some of the problem variables on x? (recalling
that x? = y + GTu?), the desired gradients are then given by
∂' ∂x? ∂u*∖T	? ∂'	1	，.T ?、T ?”,T, T T,
∂X?∂U?∂G) = μ ∂X? + LfM (d"(G μ ) + μ (G dμ) +dμy)
8£	T	(∂'	∂x?	+
∂G	=	V∂X?	∂G	
	T	∂ ∂'	Jdx?	，0
Sh	=		^∂h	+
'∂Q	T	∂ ∂'	∂X?	
∂' ∂x? ∂u? \	1
∂X?而而 I = Lf M d”
___	∂' ∂x* ∂u*∖T	( ∂' Tτ 1 CT …
∂X?西 + ∂X? ∂U?西)=(∂X?) + LfGM dμ.
(C.17)
D	Writing the cart-pole problem as an NLDI
In the cart-pole task, our goal is to balance an inverted pendulum resting on top ofa cart by exerting
T
horizontal forces on the cart. Specifically, the state of this system is defined as X = [px,px,中,夕],
where PX is the cart position and 夕 is the angular displacement of the pendulum from its vertical
position; we seek to stabilize the system at x = ~0 by exerting horizontal forces u ∈ R on the cart.
For a pendulum of length ` and mass mp, and for a cart of mass mc, the dynamics of the system are
(as described in Tedrake (2009)):
px
u+mp Sin 0(2S2 -g CoS φ)
mc +mp sin2 S
S
(mc+mp )g sin S-U cos S-mp'S2 CoS S Sin S
l(mc +mp Sin2 S)
(D.1)
where g = 9.81 m/s2 is the acceleration due to gravity. We rewrite this system as an NLDI by
defining X = f (x, U) and then linearizing the system about its equilibrium point as
X = Jf(0, 0) O + Inw,
kwk ≤ kCX+Duk,
(D.2)
19
Published as a conference paper at ICLR 2021
where Jf is the Jacobian of the dynamics, w = f(x, u) - Jf (0, 0) [x u]T is the linearization error,
and In is the n × n identity matrix. We bound this linearization error by numerically obtaining the
matrices C and D, assuming that x and u are within a neighborhood of the origin. We describe
this process in more detail below. As a note, while we employ an NLDI here to characterize the
linearization error, it is also possible to characterize this error via polytopic uncertainty (see Ap-
pendix J); we choose to use an NLDI here as it yields a much smaller problem description than a
PLDI in this case.
D.1 DERIVING Jf (0, 0)
For X = f (x, u), We see that
	01	0	0	0	
	0	0 ∂p'x∕∂ψ ∂pχ∕∂ S ∂pχ∕∂u	
Jf (X, u) =	^J	^J	u	^J U 00	0	1	0	,	(D.3)
	_0 0 dcP∣∂φ	d^∕∂0 dCp/du,_	
Where		
∂pX	mp cos 中(°21 — g cos 夕)+ gmp sin2 g	2mp sin g cos g(mp sin 中(°2l — g cos g)+ U)
dΨ	mc + mp sin2 °	(m° + mp sin2 °)2
∂pX	2°Lmp sin °
——=------------—
∂°	mc + mp sin2 °
∂p'χ = __________1________
∂u	mc + mp sin2 °
∂°
∂°
g(mc + mp) CoS ° + °2lmp sin2 °
一 °2 lmp cos2 ° + u sin °
l mc + mp sin2 °
2mp sin °cos °(g(mc + mp) sin°
一°2 ImP sin ° cos ° — U cos °
l (mc + mp sin2 °)2
∂°
∂°
—2°mp sin ° cos °
mc + mp sin2 °
∂°
∂u
— cos °
l(mc + mp sin2 °) .
We thus see that
	0	1	0	0	0	
Jf(0,0)=	0 0	0 0	-mp g∕mc 0	0 1	1∕mc 0	.	(D.4)
	0	0	g(mc+mp)∕lmc	0	-1∕mc	
D.2 OBTAINING C AND D
We then seek to construct matrices C and D that bound the linearization error w betWeen the true
dynamics X and our first-order linear approximation Jf (0,0) [j[. To do so, We bound the error of
this approximation entry-Wise: that is, for each entry i = 1, . . . , s, We Want to find Fi such that for
all x in some region X ≤ X ≤ X, and all U in some region u ≤ U ≤ u,
wi2
(Vfi(O)
(D.5)
Then, given the matrix
M = hF1T/2 F2T/2 F3T/2 F4T/2 F5T/2 F6T/2iT	(D.6)
We can then obtain C = M1:s and D = Ms:s+m (Where the subscripts indicate column-Wise index-
ing).
20
Published as a conference paper at ICLR 2021
We solve separately for each Fi to minimize the difference between the right and left sides of Equa-
tion (D.5) (while enforcing that the right side is larger than the left side) over a discrete grid of points
within X ≤ x ≤ X and U ≤ u ≤ u. By assuming that Fi is symmetric, We are able to cast this as a
linear program in the upper triangular entries of Fi .
To obtain the matrices C and D used for the cart-pole experiments in the main paper, we let
X = [1.5 2 0.2 1.5]T, u = 10, χ = -X, and U = —u. As each entry-wise difference in
Equation (D.5) contained exactly three variables (i.e., a total of three entries from x and u), we
solved each entry-wise linear program over a mesh grid of 50 points per variable.
E Writing quadrotor as an NLDI
In the planar quadrotor setting, our goal is to stabilize a quadcopter in the two-dimensional plane by
controlling the amount of force provided by the quadcopter’s right and left thrusters. Specifically,
T
the state of this system is defined as X = [pχ Pz 夕 Px Pz 钊 ,where (pχ,pz) is the position
of the quadcopter in the vertical plane and 夕 is its roll (i.e., angle from the horizontal position); we
seek to stabilize the system at X = ~0 by controlling the amount of force u = [ur, ul]T from right
and left thrusters. We assume that our action u is additional to a baseline force of [mg/2 mg/2]T
provided by the thrusters by default to prevent the quadcopter from falling. For a quadrotor with
mass m, moment-arm ` for the thrusters, and moment of inertia J about the roll axis, the dynamics
of this system are then given by (as modified from Singh et al. (2020)):
px cos φ — Pz Sin φ		0	0
Px sin 夕 + Pz cos g		0	0
夕 PZ S — g sin 夕	+	0 0	0 0
—px。— g cos s + g		1/m	1/m
0		'/J	—'/J
(E.1)
where g = 9.81 m/s2. We linearize this system via a similar method as for the cart-pole setting,
i.e., as in Equation (D.2). We describe this process in more detail below. We note that since the
dependence of the dynamics on u is linear, we have that D = 0 for our resultant NLDI. As for
cart-pole, while we employ an NLDI here to characterize the linearization error, it is also possible to
characterize this error via polytopic uncertainty (see Appendix J); we choose to use an NLDI here
as it yields a much smaller problem description than a PLDI in this case.
E.1 DERIVING Jf (0, 0)
For X = f (x, u), we see that
	Jf (X, u) =	0 0 0 0 0 0	0 0 0 0 0 0	-Px sin s - Pz cos S Px cos s — Pz sin S 0 —g cos S g sin S 0				cos S sin S 0 0 -S 0		— sin S cos S 0 S 0 0		0 0 1 Pz -Px 0	0 0 0 0 0 0	,	(E.2)
and thus					-0	0	0	10	0	0				
					0	0	0	01		0	0				
			Jf(0,0)=		0 0	0 0	0	00 —g 0 0		1 0	0 0	.			(E.3)
					0	0	0	00		0	0				
					0	0	0	00		0	0				
E.2 OBTAINING C AND D
We obtain the matrices C and D via a similar method as described in Appendix D, though in practice
we only consider the linearization error with respect to X (i.e., since the dynamics are linear with
respect to u, we have D = 0). We let X = [1 1 0.15 0.6 0.6 1.3] and X = —X. As for
cart-pole, each entry wise difference in the equivalent of Equation (D.5) contained exactly three
variables (i.e., a total of three entries from X and u), and each entry-wise linear program was solved
over a mesh grid of50 points per variable.
21
Published as a conference paper at ICLR 2021
F Details on the microgrid setting
For our experiments, we build upon the microgrid setting given in Lam et al. (2016). In this system,
the state x ∈ R3 captures voltage deviations, frequency deviations, and the amount of power gener-
ated by a diesel generator connected to the grid; the action u ∈ R2 describes the current associated
with a storage device and a solar PV inverter; and the disturbance w ∈ R describes the difference
between the amount of power demanded and the amount of power produced by solar panels on the
grid. The authors also define a performance index y ∈ R2 which captures voltage and frequency
deviations (i.e., two of the entries of the state x).
To construct an NLDI of the form (3) for this system, we directly use the A, B, and G matrices
given in Lam et al. (2016). We generate C i.i.d. from a normal distribution and let D = 0, to
represent the fact that the disturbance w and the entries of the state x are correlated, but that w is
likely not correlated with the actions u. Finally, we let Q and R be diagonal matrices with 1 in the
entries corresponding to quantities represented in the performance index y, and with 0.1 in the rest
of the diagonal entries, to emphasize that the variables in y are the most important in describing the
performance of the system.
G	Generating an adversarial disturbance
In the NLDI settings explored in our experiments, we seek to construct an “adversarial” disturbance
w(t) that obeys the relevant norm bounds kw(t)k2 ≤ kCx(t) +Du(t)k2 while maximizing the loss.
To do this, we use a model predictive control method where the actions taken are w(t). Specifically,
for each policy π, we model w(t) as a neural network specific to that policy. Every 10 steps of a
roll-out, we optimize w(t) through gradient descent to maximize the loss over a horizon of40 steps,
subject to the constraint kw(t)k2 ≤ kCx(t) + Du(t)k2.
H	Additional experimental details
Initial states. To pick initial states in our experiments, for the synthetic settings, we sample each
attribute of the state i.i.d. from a standard Gaussian distribution. For cart-pole and planar quadrotor,
we sample uniformly from bounds chosen such that the non-robust LQR algorithm (under the orig-
inal dynamics) did not go unstable. For cart-pole, these bounds were chosen to be px ∈ [-1, 1],
夕 ∈ [-0.1,0.1], Px = 。 = 0. For planar quadrotor, these bounds were Pχ,Pz ∈ [-1, l],
夕 ∈ [-0.05,0.05], px = Pz = S = 0.
Constructing NLDI bounds. Given these initial states, for the cart-pole and quadrotor settings,
we needed to construct our NLDI disturbance bounds such that they would hold over the entire
trajectory of the robust policy; if not, the robustness specification (A.8) would not hold, and our
agent might in fact increase the Lyapunov function. To ensure this approximately, we used a simple
heuristic: we ran the (non-robust) LQR agent for a full episode with 50 different starting conditions,
and constructed an L∞ ball around all states reached in any of these trajectories. We then used
these L∞ balls on the states to construct the matrices C and D for our disturbance bounds, using
the procedure described in Appendices D and E.
Computing infrastructure and runtime. All experiments were run on an XPS 13 laptop with an
Intel i7 processor. The planar quadrotor and synthetic NLDI experiment with D = 0 took about 1
day to run (since the projections were simple half-space projections), while all the other synthetic
domains and cart-pole took about 3 days to run. The majority of the run-time was in computing the
adversarial disturbances for test-time evaluations.
Hyperparameter selection. For our experiments, we did not perform large parameter searches.
The learning rate we chose for our model-based planner, (both robust and non-robust) remained
constant for the different domains; we tried learning rates of 1 × 10-3, 1 × 10-4, 1 × 10-5 and
found 1 × 10-3 worked best for the non-robust version and 1 × 10-4 worked best for the robust
version. For our PPO hyperparameters, we simply used those used in the original PPO paper.
One parameter we had to tune for each environment was the time step. In particular, we had to pick a
time step high enough that we could run episodes for a reasonable total length of time (within which
the non-robust agents would go unstable), but low enough to reasonably approximate a continuous-
time setting (since, for our robustness guarantees, we assume the agent’s actions evolve in continu-
ous time). Our search space was small, however, consisting of 0.05, 0.02, 0.01, and 0.005 seconds.
22
Published as a conference paper at ICLR 2021
(a) LQR
(b) Robust LQR
(c) MBP
(d) Robust MBP
(e) PPO
(f) Robust PPO
Figure H.1: Trajectories of 6 different methods on the cart-pole domain under adversarial dynamics.
Trajectory plots. Figure H.1 shows sample trajectories of different methods in the cart-pole do-
main under adversarial dynamics. The non-robust LQR and model-based planning approaches both
diverge and the non-robust PPO doesn’t diverge, but doesn’t clearly converge after 10 seconds. The
robust methods, on the other hand, all clearly converge after 10 seconds.
Runtime comparison. Tables H.1 and H.2 show the evaluation and training time of our methods
and the baselines over 50 episodes run in parallel. In the NLDI cases where D = 0, i.e., Generic
NLDI (D = 0) and Quadrotor, our projection adds only a very small computational cost. In the
other cases, the additional computational cost is more significant, but our method is still far less
expensive than the Robust MPC method.
23
Published as a conference paper at ICLR 2021
Environment	LQR	MBP	PPO	Robust LQR	Robust MPC	RARL	Robust MBP*	Robust PPO*
Generic NLDI (D = 0)	0.63	0.61	0.84	057^	718.06	0.71	0.73	0.94
Generic NLDI (D 6= 0)	0.64	0.62	0.83	038^^	824.86	0.81	15.13	25.38
Cart-pole	0.55	0.67	0.84	053^^	646.90	0.84	10.12	13.37
Quadrotor	0.95	0.98	1.19	088^^	3348.68	1.14	1.15	1.30
Microgrid	0.58	0.61	0.79	057^	601.90	0.74	8.14	10.25
Generic PLDI	0.57	0.54	0.76	051^	819.24	0.73	69.35	64.03
Generic H∞	0.84	0.80	1.03	0.76	N/A	1.00	47.81	63.67
Table H.1: Time (in seconds) taken to run each method on the test set of every environment for 50
episodes run in parallel.
Environment	MBP	PPO	RARL	Robust MBP*	Robust PPO*
Generic NLDI (D = 0)	26.36	101.77	102.37	30.78	114.60
Generic NLDI (D 6= 0)	26.46	100.79	^^82.53	221.35	1158.28
Cart-pole	25.49	^^87.04	^^98.90	146.34	689.93
Quadrotor	41.24	131.48	112.95	46.13	159.06
Microgrid	23.03	112.52	87.71	113.61	436.64
Table H.2: Time (in minutes) taken to train each method in every environment.
I	EXPERIMENTS FOR PLDIS AND H∞ CONTROL SETTINGS
In addition to the NLDI settings explored in the main text, we test the performance of our method
on PLDI and H∞ control settings. As for the experiments in the main text, we choose a time
discretization based on the speed at which the system evolves, and run each episode for 200 steps
over this discretization. In both cases, we use a randomly generated LQR objective where the
matrices Q1/2 and R1/2 are drawn i.i.d. from a standard normal distribution.
Synthetic PLDI setting. We generate PLDI instances (A.9) with s = 5, a = 3, and L = 3. Specifi-
cally, we generate convex hull matrices (A1, B1), . . . , (A3, B3) i.i.d. from normal distributions, and
generate (A(t), B(t)) by using a randomly-initialized neural network with softmax output to weight
the convex hull matrices. Episodes were run for 2 seconds at a discretization of 0.01 seconds.
Synthetic H∞ setting. We generate H∞ control instances (A.11) with s = 5, a = 3, and d = 2 by
generating matrices A, B and G i.i.d. from normal distributions. The disturbance w(t) was produced
using a randomly-initialized neural network, with its output scaled to satisfy the L2 bound on the
disturbance. Specifically, we scaled the output of the neural network to satisfy an attenuating norm-
bound on the disturbance; at time t, the norm-bound was given by 20 × f (2 × t/T ), where T is the
time horizon and f is the standard normal PDF function. Episodes were run for T = 2 seconds at a
discretization of 0.01 seconds.
Results are given in Figure I.1 and Table I.1.
Environment		LQR	MBP	PPO	Robust LQR	Robust MPC	RARL	Robust MBP*	Robust PPO*
Generic PLDI	O	96.3	3.3	80	-^192^^	19.2	15.8	18.6	10.2
	A	———	unstable	—	43.3	44.1	unstable	21.9	16.1
Generic H∞	O^	181	88	^314^^	-^165^^	N/A	115	116	125
	A	219	112	143	206	N/A	145	147	158
Table I.1: Performance of various approaches, both robust (right) and non-robust (left), on domains
of interest. We report average quadratic loss over 50 episodes under the original dynamics (O)
and under an adversarial disturbance (A). For the original dynamics (O), the best performance for
both non-robust methods and robust methods is in bold (lower loss is better). We use “unstable”
to indicate cases where the relevant method became unstable. Our robust methods (denoted by ")
improve performance over Robust LQR in the average case, while remaining stable under adversarial
dynamics, whereas the non-robust methods either went unstable or received much larger losses.
24
Published as a conference paper at ICLR 2021
Training epochs	Training epochs
MBP PPO - RARL - Robust MBP* - Robust PPO*
Setting: -------Original ----------Adversarial
Figure I.1: Representative results for our experimental settings. For each training epoch (10 updates
for the MBP model and 18 for PPO), we report average quadratic loss over 50 episodes, and use
“X” to indicate cases where the relevant method became unstable. (Lower loss is better.) Our robust
methods (denoted by *) improve performance over Robust LQR in the average case, while (Un-
like the non-robust methods) remaining stable under adversarial dynamics throughout the training
process.
J Notes on linearization via PLDIs and NLDIs
While we linearize the cart-pole and quadrotor dynamics via NLDIs in our experiments, we note that
these dynamics can also be characterized via PLDIs. More generally, in this section, we show how
we can use the framework of PLDIs to model linearization errors arising in the analysis of nonlinear
systems.
Consider the nonlinear dynamical system
X = f (x, U) with f (0,0) = 0.	(J.1)
for x ∈ Rs and u ∈ Ra. Define ξ = (x, u). We would like to represent the above system as a PLDI
in the region R := {ξ | ξ ≤ ξ ≤ ξ} including the origin. The mean value theorem states that for
each component of f, We can write
fi(ξ) = fi(0) + Nfi(Z)T ξ,	(J.2)
for some z = tξ, where t ∈ [0, 1]. Now, letp = s + a. Defining the Jacobian of f as
'Vfι (Z)T 一
Jf (Z)=	.	,	(J.3)
Vfp(Z)T
and recalling that f(0) = 0, we can rewrite (J.2) as
f(ξ) =Jf(Z)ξ.	(J.4)
Now, suppose we can find component-wise bounds on the matrix Jf(Z) over R, i.e,
M ≤ Jf (z) ≤ M for all Z ∈R.	(J.5)
We can then write
Jf(z) = E mij(t)Ej with mi7-(t) ∈ [mj,mj],	(J.6)
1≤i,j ≤p
where Eij = eiejT and ei is the i-th unit vector in Rp.
25
Published as a conference paper at ICLR 2021
We now seek to bound the Jacobian using polytopic bounds. To do this, note that we can write
2p2
Jf (z) = XγκAκ γκ ≥ 0, Xγκ = 1,	(J.7)
κ=1	κ
where Aκ's are the vertices of the polytope in (J.6), i.e.,
AK ∈ V = ∖ E mjEjI mj ∈ {mj, mij} . .	(J.8)
[ι≤i,j≤p
Together, Equations (J.2), (J.4), (J.7), and (J.8) characterize the original nonlinear dynamics as a
PLDI.
We note that this PLDI description is potentially very large; in particular, the size of V is exponential
in the square of the number of non-constant entries in the Jacobian Jf (z), which could be as large
as 2p2 = 2(s+a)2. This problem size may therefore become intractable for larger control settings.
We note, however, that we can in fact express this PLDI more concisely as an NLDI. More precisely,
we would like to find matrices A, B, C parameterizing the form of NLDI below, which is equivalent
to that presented in Equation (3) (see Chapter 4 of Boyd et al. (1994)):
Df(z) ∈ {A+B∆C I k∆k2 ≤ 1}	for all z ∈ R.
It can shown that the solution to the SDP
minimize tr(V + W)
subject to W 0
(J.9)
(J.10)
V
Aκ - A
(Aκ - A)T	0, ∀Aκ ∈ V
yields the matrices A, B, and C with V = CTC and W = BBT, which can be used to construct
NLDI (J.9). While the NLDI here is more concise than the PLDI, the trade-off is that the NLDI
norm bounds obtained via this method may be rather loose. As such, for our settings, we obtain
NLDI bounds numerically (see Appendices D and E), as these are tighter than NLDI specifications
obtained via the above method (though they are potentially slightly inexact). An alternative approach
would be to examine how to tighten the conversion from PLDIs to NLDIs, which has been explored
in other work (e.g. Kuiava et al. (2013)).
26