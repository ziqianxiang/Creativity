Published as a conference paper at ICLR 2021
Vulnerability-Aware Poisoning Mechanism
for Online RL with Unknown Dynamics
Yanchao Sun1	Da Huo2	Furong Huang3
1,3 Department of Computer Science, University of Maryland, College Park, MD 20742, USA
2 Shanghai Jiao Tong University, China
1 ycs@umd.edu, 2sjtuhuoda@sjtu.edu.cn, 3furongh@umd.edu
Ab stract
Poisoning attacks on Reinforcement Learning (RL) systems could take advantage
ofRL algorithm’s vulnerabilities and cause failure of the learning. However, prior
works on poisoning RL usually either unrealistically assume the attacker knows
the underlying Markov Decision Process (MDP), or directly apply the poisoning
methods in supervised learning to RL. In this work, we build a generic poison-
ing framework for online RL via a comprehensive investigation of heterogeneous
poisoning models in RL. Without any prior knowledge of the MDP, we propose a
strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poi-
son (VA2C-P), which works for on-policy deep RL agents, closing the gap that
no poisoning method exists for policy-based RL agents. VA2C-P uses a novel
metric, stability radius in RL, that measures the vulnerability of RL algorithms.
Experiments on multiple deep RL agents and multiple environments show that our
poisoning algorithm successfully prevents agents from learning a good policy or
teaches the agents to converge to a target policy, with a limited attacking budget.
1	Introduction
Although reinforcement learning (RL), especially deep RL, has been successfully applied in various
fields, the security of RL techniques against adversarial attacks is not yet well understood. In real-
world scenarios, including high-stakes ones such as autonomous driving vehicles and healthcare
systems, a bad decision may lead to a tragic outcome. Should we trust the decision made by an
RL agent? How easy is it for an adversary to mislead the agent? These questions are crucial to ask
before deploying RL techniques in many applications.
In this paper, we focus on poisoning attacks, which occur during the training and influence the
learned policy. Since training RL is known to be very sample-consuming, one might have to con-
stantly interact with the environment to collect data, which opens up a lot of opportunities for an
attacker to poison the training samples collected. Therefore, understanding poisoning mechanisms
and studying the vulnerabilities in RL are crucial to provide guidance for defense methods. How-
ever, existing works on adversarial attacks in RL mainly study the test-time evasion attacks (Chen
et al., 2019) where the attacker crafts adversarial inputs to fool a well-trained policy, but does not
cause any change to the policy itself. Motivated by the importance of understanding RL security in
the training process and the scarcity of relevant literature, in this paper, we investigate how to poison
RL agents and how to characterize the vulnerability of deep RL algorithms.
In general, RL is an “online” process: an agent rolls out experience from the environment with
its current policy, and uses the experience to improve its policy, then uses the new policy to roll
out new experience, etc. Poisoning in online RL is significantly different from poisoning in classic
supervised learning (SL), even online SL, and is more difficult due to the following challenges.
ChallengeI-Future Data Unavailable in Online RL. Poisoning approaches in SL (MUnoz-Gonzalez
et al., 2017; Wang & Chaudhuri, 2018) usually require the access to the whole training dataset, so
the attacker can decide the optimal poisoning strategy before the learning starts. However, in online
RL, the training data (trajectories) are generated by the agent while it is learning. AlthoUgh the
optimal poison shoUld work in the long rUn, the attacker can only access and change the data in the
cUrrent iteration, since the fUtUre data is not yet generated.
1
Published as a conference paper at ICLR 2021
Challenge II - Data Samples No Longer i.i.d.. It
is well-known that in RL, data samples (state-action
transitions) are no longer i.i.d., which makes learn-
ing challenging, since we should consider the long-
term reward rather than the immediate result. How-
(G#二工匚#‘二。S;
& = 1 V./#0： & = 1<—√α0: &=1
Figure 1: An example of difficult poisoning.
ever, we notice that data samples being not i.i.d. also makes poisoning attacks challenging. For
example, an attacker wants to reduce the agent,s total reward in a task shown as Figure 1; at state
si, the attacker finds that aι is less rewarding than a°; if the attacker only looks at the immediate
reward, he will lure the agent into choosing ai. However, following ai finally leads the agent to sio
which has a much higher reward.
Challenge III - Unknown Dynamics of Environment. Although Challenge I and II can be partially
addressed by predicting the future trajectories or steps, it requires prior knowledge on the dynam-
ics of the underlying MDP. Many existing poisoning RL works (Rakhsha et al., 2020; Ma et al.,
2019) assume the attacker has perfect knowledge of the MDP, then compute the optimal poison-
ing. However, in many real-world environments, knowing the dynamics of the MDP is difficult.
Although the attacker could potentially interact with the environment to build an estimate of the en-
vironment model, the cost of interacting with the environment could be unrealistically high, market
making (Spooner et al., 2018) for instance. In this paper, we study a more realistic scenario where
the attacker does not know the underlying dynamics of MDP, and can not directly interact with the
environment, either. Thus, the attacker learns the environment only based on the agent’s experience.
In this paper, we systematically investigate poisoning in RL by considering all the aforementioned
RL-specific challenges. Previous works either do not address any of the challenges or only address
some of them. Behzadan & Munir (2017) achieve policy induction attacks for deep Q networks
(DQN). However, they treat output actions of DQN similarly to labels in SL, and do not consider
Challenge II that the current action will influence future interactions. Ma et al. (2019) propose a
poisoning attack for model-based RL, but they suppose the agent learns from a batch of given data,
not considering Challenge I. Rakhsha et al. (2020) study poisoning for online RL, but they require
perfect knowledge of the MDP dynamics, which is unrealistic as stated in Challenge III.
Summary of Contributions. (1) We propose a practical poisoning algorithm called Vulnerability-
Aware Adversarial Critic Poison (VA2C-P) that works for deep policy gradient learners without any
prior knowledge of the environment. To the best of our knowledge, VA2C-P is the first practical
algorithm that poisons policy-based deep RL methods. (2) We introduce a novel metric, called
stability radius, to characterize the stability of RL algorithms, measuring and comparing the vulner-
abilities of RL algorithms in different scenarios. (3) We conduct a series of experiments for various
environments and state-of-the-art deep policy-based RL algorithms, which demonstrates RL agents’
vulnerabilities to even weaker attackers with limited knowledge and attack budget.
2	Related Work
The main focus of this paper is on poisoning RL, an emerging area in the past few years. We survey
related works of adversarial attacks in SL and evasion attacks in RL in Appendix A, as they are out
of the scope of this paper.
Targeted Poisoning Attacks for RL. Most RL poisoning researches work on targeted poisoning,
also called policy teaching, where the attacker leads the agent to learn a pre-defined target policy.
Policy teaching can be achieved by manipulating the rewards (Zhang & Parkes, 2008; Zhang et al.,
2009) or dynamics (Rakhsha et al., 2020) of the MDP. However, they require the attackers to not
only have prior knowledge of the environments (e.g., the dynamics of the MDP), but also have the
ability to alter the environment (e.g. change the transition probabilities), which are often unrealistic
or difficult in practice.
Poisoning RL with Omniscient Attackers. Most guaranteed poisoning RL literature (Rakhsha
et al., 2020; Ma et al., 2019) assume omniscient attackers, who not only know the learner’s model,
but also know the underlying MDP. However, as motivated in the introduction, the underlying MDP
is usually either unknown or too complex in practice. Some works poison RL learners by changing
the reward signals sent from the environment to the agent. For example, Ma et al. (2019) introduce
a policy teaching framework for batch-learning model-based agents; Huang & Zhu (2019) propose
a reward-poisoning attack model, and provide convergence analysis for Q-learning; Zhang et al.
2
Published as a conference paper at ICLR 2021
(2020b) present an adaptive reward poisoning method for Q-learning (while it also extends to DQN)
and analyze the safety thresholds of RL; these papers all assume the attacker knows not only the
models of the agent, but also the parameters of the underlying MDP, which could be possible in a
tabular MDP, but hard to realize in large environments and modern deep RL systems.
On the contrary, we consider non-omniscient attackers who do not know the underlying MDP or
environment in this paper. The non-omniscient attackers can be further divided into two categories:
white-box attackers, who know the learner’s model/parameters, and black-box attackers, who do not
know the learner’s model/parameters. They both tap the interactions between the learner and the
environment.
Black-box Poisoning for Value-based Learners. Although there are many successful black-box
evasion approaches (Xinghua et al., 2020; Inkawhich et al., 2020), black-box poisoning in RL is
rare. There is a black-box attacking method for a value-based learner (DQN) proposed by Behzadan
& Munir (2017), which does not require the attacker to know the learner’s model or the underlying
MDP. In this work, the attacker induces the DQN agent to output the target action by perturbing the
state with Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) in every step. However, the
data-correlation problem of RL (Challenge II) is not considered, and FGSM attack does not work
for policy-based methods due to their high stochasticity, as we show in experiments.
In this paper, we propose a new poisoning algorithm for policy-based deep RL agents, which can
achieve both non-targeted and targeted attacks. We do not require any prior knowledge of the
environment. And our algorithm works not only when the attacker knows the learner’s model (white-
box), but also when the leaner’s model is hidden (black-box).
3	Problem Formulation for Poisoning Online RL
3.1	Notations and Preliminaries
In RL, an agent interacts with the environment by taking actions, observing states and receiving
rewards. The environment is modeled by a Markov Decision Process (MDP), which is denoted
by a tuple M =(S, A, P, R, γ, μ), where S is the state space, A is the action space, P is the
transition kernel, R is the reward function, Y ∈ (0,1) is the discount factor, and μ is the initial
state distribution. A trajectory T 〜π generated by policy π is a sequence s1,a1,r1,s2,a2, •一，
where si 〜 μ, at 〜 π(a∣st), st+1 〜 P(s|st, at) and r = R(st, at). The goal of an RL agent
is to find an optimal policy π* that maximizes the expected total rewards η, which is defined as
η(π) = ET 〜π[r(T )] = Esι,aι,…〜μ,π,P,R [P∞=i YtTrt].
We use an overhead check sign - ona variable to denote that the variable is poisoned. For example, if
the attacker perturbs a reward rt, then the poisoned reward is denoted as rt. If a policy ∏ is updated
with poisoned observation, then it is denoted as ∏.
3.2	THE PROCEDURE OF ONLINE LEARNING AND POISONING
Procedure of Online Learning. We consider a
classical online policy-based RL setting, where the
learner iteratively updates its policy ∏ parametrized
by θ, through K iterations with the environment. For
notation simplicity, we omit θ and use ∏k to denote
∏θk, the learner,s policy at iteration k. The online
learning process is described as below.
Figure 2: Online PoiSoning-Iearning process.
At iteration k = 1,…，K,
⑴ The agent uses the current policy ∏k to roll out observation Ok = (OS, Of, Or) from envi-
ronment Mk, where Os = [si, s2,…],Oa = [a1,a2,…],Or = [r1,r2,…]are respectively the
sequence of states, actions and rewards generated at iteration k.
(2)	The agent updates its policy parameters θ with its algorithm f . Most policy-based algorithms
perform on-policy updating, i.e., update policy only by the current observation Ok. The on-policy
update can be then formalized as πk+i = f(πk, Ok) ≈ argmaxπJ(π, πk, Ok), where J is an
objective function defined by algorithm f, e.g., the expected total reward η(π).
3
Published as a conference paper at ICLR 2021
Procedure of Online Poisoning. A poisoning attacker influences the learner in the training process
by perturbing the training data. In SL, the training data consists of features and labels, and the
attacker poisons the training data before the learning starts. However, the training data in RL is
the trajectories a learner rolls out from the environment, i.e., observation O = (Os , Oa, Or). At
iteration k, the attacker eavesdrops on the interaction between the learner and the environment,
obtains the observation Ok, and may poison it into Ok, then send Ok to the learner before policy
updating. 1 Procedure 2 in Appendix B illustrates how the online game goes between the learner
and the attacker. Figure 2 visualizes this online learning-poisoning procedure, where we can see that
learning and poisoning are convoluted and inter-dependent.
3.3	A Unified Formulation for Poisoning Online Policy-based RL.
3.3.1	Attacker’s Poison Aim
As defined in Section 3.1, the observation is a collection of trajectories, consisting of the observed
states Os, the executed actions Oa or the received rewards Or. When poisoning the observation, the
attacker may only focus on the states, oron the actions, oron the rewards. We call the quantity being
altered as the poison aim of the attacker, denoted by D ∈ {Os, Oa, Or}. For example, D = Os
means the attacker chooses to attack the states.
Distinguishing different poison aims is important, since in real-world applications different poison
aims correspond to different behaviors of the attacker. For example, in an RL-based recommender
system, the RL agent recommends an item (i.e., an action) for a user (i.e., a state), and the user
may or may not choose to click on the recommended item (i.e., a reward). An adversary might
manipulate the reward (poisoning D = Or), e.g., blocking the user’s click from the RL agent or
creating a fake click. An adversary might also alter the state (poisoning D = Os), e.g., raising a
teenager user’s age which could result in inappropriate recommendations. An adversary might also
change the action (poisoning D = Oa), e.g., inserting a fake recommendation into the agent’s list of
recommendations. Under different scenarios, the feasibility of poisoning different aims may vary.
Most existing works on poisoning RL only solve one type of poison aim. Zhang et al. (2020b);
Huang & Zhu (2019) propose to poison rewards, and Behzadan & Munir (2017) assume the attacker
poison the states. However, in our paper, we provide a general solution for any of the poison aims to
satisfy the needs in different scenarios. Our proposed method also supports a ”hybrid” poison aim,
where the attacker could switch aims at different iterations, as discussed in Section 5.
3.3.2	A Poisoning Framework for RL
We focus on proposing a poisoning mechanism for the above challenging online learning scenario.
We first formalize the poisoning attacking at iteration k as a sequential bilevel optimization problem
in Problem (Q), and explain the details of the problem in the remaining of this section.
argmin
Dk,…,Dk
s.t.	∏j+ι = argmax∏ J(∏,∏j,Oj |Dj), ∀k ≤ j ≤ K ((b) imitate the learner)
K
j= I 1{Dj = Dj} ≤ C	((c) limited-budget)
U(Dj, Dj) ≤ e, ∀1 ≤ j ≤ K	((d) limited-power)
(a)	Attacker飞 Weighted Loss. La(Π) measures the attacker,s loss w.r.t. a poisoned policy ∏. As the
definition of poisoning implies, the attacker influences or misleads the learner,s policy. λk:K are the
weights of future attacker losses, controlling how much the attacker value the poisoning results in
different iterations. The goal of the attacker is either (1) non-targeted poisoning, which minimizes
the expected total rewards of the learner, i.e., LA = η(∏), or (2) targeted poisoning, which induces
the learner to learn a pre-defined target policy, i.e., LA = distance(∏, ∏f), where distance(∏, π*)
can be any distance measure between a learned policy ∏ and a target policy ∏*. Note that the targeted
1In this paper, we assume the attacker poisons the observation(trajectories), which is the most universal
setting in practice. Appendix B extends our problem formulation to a more general case where the attacker can
change the underlying MDP.
K
Xj=k λj LA(πj+ι)
((a) attacker,s weighted loss) (Q)
4
Published as a conference paper at ICLR 2021
poisoning objective can usually be directly computed with a prior target policy, while non-targeted
poisoning has a “reward-minimizing” objective, which is the reverse of the learner’s objective. With-
out any prior knowledge of the environment, non-targeted poisoning is usually more difficult, as the
attacker needs to first learn “what is the worst way” (which is as difficult as a learning problem
by a RL agent) and then lead the learner to that way (which is as difficult as a targeted poisoning
problem, assuming leading an agent to different policies is roughly equally challenging). However,
most existing poisoning work focus on targeted poisoning, which requires the attacker to know a
pre-defined target policy. Thus in this paper, we make more efforts to solve the reward-minimizing
poisoning problem, which may deprave the policy without any prior knowledge.
(b)	Imitate the Policy-Based Learner. To confidently mislead a learner, the attacker needs to predict
how the learner will behave under the poison, which can be achieved by imitating the learner using
the learner’s observation. More specifically, at the j-th iteration, the attacker estimates the learner’s
policy to be ∏j, called imitating policy. Then, the attacker predicts the next-policy ∏j+ι under
poisoned observation, based on the learner,s update rule argmax∏ J(∏,∏j, Oj|Dj), where D ∈
{Os , Oa , Or } stands for the poison aim of the poisoning, O|D denotes that O is poisoned into O
given that poison aim D is poisoned into DD. However, the imitating policy π may or may not be
the same as the actual learner’s policy, depending on the attacker’s knowledge. As introduced in
Section 2, we deal with both white-box and black-box attackers, and both of them do not know the
environment M. A white-box attacker knows the current and past observations O1:k, the learner’s
algorithm f and policy ∏, so it can directly copy the policy ∏j = ∏j, ∀j. A black-box attacker
knows the current and past observations O1:k, but does not know the learner’s policy π. In this case,
the attacker has to estimate π at every iteration. Section 4.3 states how to guess π.
(c,d) Limited-budget and Limited-power. In practice, the ability of an attacker is usually restricted
by some constraints. For the online poisoning problem, we consider attacker’s constraints in two
forms: (1) (attack budget C) the total number of iterations that the attacker could poison does not
exceed C; (2) (attack power E) in one iteration, the total change2 U(Dk, Dk) between Dk and Dk
can not be larger than . Attack power controls the amount of perturbation, as commonly used in the
adversarial learning literature. Attack budget considers the frequency of attack, which is similar to
the constraint studied by Wang & Chaudhuri (2018).
Problem (Q) is a generic formulation, covering a variety of poisoning models, and specifies the best
poison an attacker can execute. However, directly solving Problem (Q) is prohibitive, as (1) the
future observations Ok+1:K are unknown when poisoning the k-th iteration, as the attacker has no
knowledge of the underlying MDP. (2) the limited-budget constraint is analogous to `0 -norm reg-
ularization, which is generally NP-hard (Nguyen et al., 2019); and (3) minimizing attacker’s loss
while maximizing learner’s gain is non-convex minimax optimization, which is a complex prob-
lem (Perolat et al., 2015).
In spite of the above difficulties, we introduce a practical method to approximately and effectively
solve Problem (Q) in Section 4.
4	VA2C-P: Poison Policy Gradient Learners
In this section, we propose a practical and efficient poisoning algorithm called Vulnerability-Aware
Adversarial Critic Poison (VA2C-P) for policy gradient learners. Without loss of generality, we
assume the loss weights λj = 1 for all j = 1,…，K.
Main Idea. As discussed in Section 3.3, Problem (Q) is difficult mainly because of the unknown
future observations and the limited budget constraint. In other words, it is hard to exactly determine
(1) what kind of attack benefits the future the most, and (2) which iterations are worth attacking the
most. Thus, we propose to break Problem (Q) into two decisions: when to attack, and how to attack.
The “when to attack” decision allocates the limited budget to iterations which are more likely to be
influenced by the attacker, and the “how to attack” decision utilizes limited power to minimize the
attacker’s loss. We introduce two mechanisms of VA2C-P, vulnerability-awareness and adversarial
critic, to make these two decisions respectively.
2There are many choices of U(∙, ∙). For example, the total effort w.r.t. Os-poisoning can be the average
'p-distance between any poisoned and unpoisoned state in Os and Os.
5
Published as a conference paper at ICLR 2021
4.1	DECISION 1： WHEN TO ATTACK - VULNERABILITY-AWARE
To answer the question of when to attack, we identify the iterations under which the learner’s policy
gets more depraved by the same level of attack power. Inspired by the notion of stability in learning
theory, which measures how a machine learning algorithm changes due to a small perturbation of
the input data, we formally investigate the stability of an RL algorithm, which is the first attempt in
the existing literature to the best of our knowledge.
Stability of RL Algorithms. We first focus on a single update process ofan algorithm f. An update
π0 = f (π, O) is stable if a limited-power poisoning attack does not cause any difference on the
output policy π0 . That is, the learning algorithm produces the same result regardless of the presence
of the poison. More formally, we define the concept of stability radius of one update in Definition 1.
Definition 1 (Stability Radius of One Update). 3 For the update of an RL algorithm π0 = f(π, O),
with any poison aim D, the δ-stability radius of the update is defined as the minimum poison power
needed to cause δ change in policy (called δ-policy-discrepancy)
φδ,D(f,∏, O) = inf{∃DD s.t. U(D, DD) ≤ E and dmax[∏0∣∣∏0] > δ, where π0 = f(π, O∣D)}, (1)
Policy discrepancy dmax [∏1∣∣∏2] = maxs d[∏1(∙∣s)∣∣∏2(∙∣s)], where d[∙∣∣∙] could be any measure of
distribution distance.
Remarks. (1) The one-update stability radius is w.r.t. the algorithm f, the old policy π, the clean
observation O and the poison aim D. (2) Poison with power under φδ,D (f, π, O) will not cause
the policy distributions to change more than δ. (3) As shown by Proposition 2 in Appendix D.1,
poison with power under φδ,D (f, π, O) will not make the policy value drop more than O δ2γ(1 -
Y)-2 maxs,a ∣A∏o(s, a)|), where A is the advantage function, i.e., An(s, a) = Qn(s, a) - V∏(s).
Stability radius measures the minimal effort needed to make the poisoned next-policy ∏0 notably
different from the clean next-policy π0 which the learner will get if no poison is applied. Assuming
maxs,a |An0 (s, a)| does not drastically change for the learner’s policy during training, then an attack
that causes higher policy discrepancy between ∏0 and ∏ could cause more drop of the policy value.
Therefore, the idea of vulnerability-aware attack is to estimate the vulnerability of each update and
attack the most vulnerable ones. More specifically, if the attacker finds an E-powered attack results
in a policy discrepancy larger than some threshold δ, then it can conclude φδ (f, π, O) ≤ E, and
the current update is relatively vulnerable. In practice, we have the fixed budget C instead of δ, so
we perform the vulnerability check in an adaptive way： attacking the C iterations where E-powered
attacks can trigger the highest policy discrepancies. Section 4.3 introduces an algorithm to realize
this idea.
4.2	Decision 2: How to Attack — Adversarial Critic
Since “when to attack” decision tackles the limited-budget constraint, now we turn our attention
to minimizing the attacker’s loss while satisfying the limited-power constraint. if the attacker has
already decided to poison the k-th iteration due to its high vulnerability, then the decision of how
to attack should be made before the learner uses the observation to update the policy. We relax the
original Problem (Q) into Problem (P) as below.
argminDk LA(ττk+ι)	(P)
St	∏k+ι = argmax∏ J(π,∏k, Ok|Dk)
U (Dk, DDk) ≤ E
Compared with Problem (Q), Problem (P) does not consider future losses, which require the unavail-
able future observations. Instead, Problem (P) finds a greedy attack Dk to minimize the loss of the
immediate next iteration. The solution to Problem (P) is always feasible to Problem (Q), although
might not be optimal. Appendix F.2 provides details about the optimality of Problem (Q).
In practice, itis also challenging to estimate La(∏). For targeted attacking, LA = distance(∏, ∏*) is
directly computable with a properly defined distance metric, but for non-targeted attacking, the loss
3in order to characterize the stability and robustness of RL algorithms in a principled way, we provide more
measures for the vulnerability of RL in both training time and test time. See Appendix D.2) for details.
6
Published as a conference paper at ICLR 2021
LA = η(∏) can not be directly computed, since ∏ is not the behavior policy. Although one can use
importance sampling to evaluate η(∏) with the current trajectories generated by the learner,s policy,
i.e., ET〜∏k-ι [∏∏k(TT)r(τ)], it may suffer from a high variance (Schulman et al., 2015b) when there
are few trajectories. To solve this challenge, we introduce another mechanism, adversarial critic.
Adversarial Critic. Under poisoning, the value network (if any) held by the learner usually fails
to fit the correct value of its policy, since it does not observe the real trajectories generated by its
policy. However, the attacker observes the real trajectories before poisoning, and is able to learn the
real values to make the attacking stronger. Inspired by the Actor-Critic method, we propose to let
the attacker learn a value function (network) Vω with observations of the learner, i.e., the attacker
learns a critic of the learner,s current policy ∏. Then the attacker can use Vω to design poisoned
observations, directing the learner to a decreasing-value direction, which is called Adversarial Critic.
Then, using importance sampling, the attacker,s loss becomes Es,a〜∏k[S⅛ (G(st,at)-Vω (st))],
where G is the discounted future reward PiT=t γi-trt .
4.3	Poisoning Algorithm VA2C-P.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Algorithm 1: Vulnerability-Aware Adversarial Critic Poison
Input: total iterations of learning K; poisoning power e; poisoning budget C;
Initialize a list of policy discrepancies Ψ = 0, the number of poisoned iterations C = 0
Initialize an adversarial critic network Vω and an imitating policy network π
for k = 1,…，K do
if c > C then
I Break
Obtain the observation (trajectories) Ok obtained by the learner
Update the adversarial critic Vω with observation Ok
if White-box then
I Copy the learner,s policy parameters to the imitating policy of attacker: ∏ J ∏k
Compute the clean next-policy ∏0 J f (∏, Ok)
Solve Problem (P) with power and critic Vω, poison Ok to Ok
Compute the poisoned next-policy ∏ J f (∏, Ok)
Estimate the policy discrepancy ψk between ∏0 and ∏0
Add ψk to the list of policy discrepancies Ψ
if ψk ≥ b KC-ckC -th largest element in Ψ then
Send the learner the poisoned observation Ok
if Black-box then
I Update the imitating policy as the clean next-policy: ∏ J ∏0
else
Send the clean observation Ok
if Black-box then
I Update the imitating policy as the poisoned next-policy: ∏ J ∏0
Algorithm 1 illustrates how an attacker can poison an online RL learner with our proposed VA2C-
P, which is corresponding to Line 4 in Procedure 2 shown in Appendix B . More implementation
details are in Algorithm 3 in Appendix E.
In an iteration, an attacker first finds a good perturbation for observation O with the current attack
power , then estimates how much the output policy will change by computing the policy discrep-
ancy between the clean next-policy and the poisoned next-policy. Then the attacker poisons if the
policy discrepancy ranks high in the historical policy discrepancies. In Line 11, we use projected
gradient descent to solve Problem P. Computation details are illustrated in Appendix E.
Algorithm 1 covers both white-box and black-box settings, both of which maintain an imitating
policy ∏ to keep track of the learner,s potential status. As mentioned in Section 3.3, a white-box
attacker knows the learner,s current policy, thus he can directly copy the learner,s parameters to
7
Published as a conference paper at ICLR 2021
his imitating policy (Line 9), then predict the next-policy the learner would get under different
observations. In contrast, a black-box attacker does not know the learner’s policy, but he can update
its imitating policy using the same observation as the learner uses (Line 18, 22). As claimed and
verified by many black-box attacking methods (Behzadan & Munir, 2017), adversarial attacks are
usually transferable, i.e., if the attack works on the imitating learner, then the attack is also likely to
work for the real learner. Note that as assumed by Behzadan & Munir (2017), the black-box attacker
knows what RL algorithm the learner is using (e.g., PPO, A2C, etc), so that the black-box attacker
computes its estimation for the next-policy in Line 10 and Line 12.
5	Experiments
In this section, we evaluate the performance of VA2C-P by poisoning multiple algorithms on various
environments. We demonstrate that VA2C-P can effectively reduce the total reward of a training
agent, or force the agent to choose a specific policy with limited power and budget. Moreover,
VA2C-P works for heterogeneous poison aims, and works in both white-box and black-box settings.
Experiment Setup. We choose 4 policy-gradient learning algorithms, including Vanilla Policy Gra-
dient (Sutton et al., 2000), A2C (Mnih et al., 2016), ACKTR (Wu et al., 2017) and PPO (Schulman
et al., 2017). And we choose 5 Gym (Brockman et al., 2016) environments with increasing difficulty
levels: CartPole, LunarLander, Hopper, Walker and HalfCheetah. All results are averaged over 10
random seeds. The total effort U is calculated by the normalized '2-distance. See Appendix G.1 for
the expression of U, as well as more hyper-parameter settings.
(a) CartPole, A2C, D = Os, =0.5	(b) LunarLander, A2C, D = Oa, =0.1 (c) Walker2d, A2C, D
50 00 50
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
(e) CartPole, VPG, D = Os, =0.1
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
(f) Hopper, VPG, D = Oa, =0.1
No Poison
Random Poison
AC-P
VA2C-P
No Poison
Random Poison
AC-P
VA2C-P
(d) Ha4lfCheetah, A2C, D = Or, =0.1
draweR edosipe-reP naeM
No Poison
Random Poison
AC-P
VA2C-P
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K	C/K
(g) Hopper, PPO, D = Oa, =0.1 (h) Walker2d, ACKTR, D = Or, =0.3
00 50
draweR edosipe-reP nae
No Poison
Random Poison
AC-P
VA2C-P
50 00 50
draweR edosipe-reP naeM
draweR edosipe-reP naeM
200 100
draweR edosipe-reP naeM

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K	C/K	C/K	C/K
Figure 3:	Comparison of mean per-episode reward gained by VPG, PPO, A2C, ACKTR on various environ-
ments, under no poisoning, random poisoning, AC-P and VA2C-P.
Reward-minimizing Poisoning. Baselines. To the best of our knowledge, there is no exist-
ing poisoning algorithm against deep policy-gradient algorithms. Although some evasion meth-
ods (Inkawhich et al., 2020) also work for policy-gradient algorithms, evasion is substantially dif-
ferent from poisoning since it does not influence the policy. Therefore, to show the effectiveness
of VA2C-P, we compare it with 3 baselines: (1) the normal learning with no poison; (2) a random
attacker which randomly chooses C iterations, and perturbs the reward to an arbitrary direction by
; and (3) a simplified version of our algorithm, called Adversarial Critic Poison (AC-P), which
decides “how to attack” in the same way as VA2C-P does, but chooses “when to attack” randomly.
Performance. We first show the reward-minimizing performance of VA2C-P on all three types of
poison aims, assuming the attacker knows the learner’s model (white-box attack). Figure 3 shows
the rewards of various learners under different kinds of poisoning methods, with different ratios
of budget C to the total number of iterations K . Compared with random poisoning, our proposed
VA2C-P and the simplified version AC-P make the reward drop more significantly, demonstrating
the effectiveness of our “how to attack” decisions made by the Adversarial Critic. VA2C-P further
outperforms AC-P in almost all cases, which implies that our “when to attack” decisions based on
Vulnerability-Awareness work well in practice. An interesting observation is random poisoning not
only does not work well in many cases, but sometimes also facilitates the learner (Figure 3h). This
8
Published as a conference paper at ICLR 2021
phenomenon is mainly due to the uncertainty of the environment, as pointed out by Challenge I and
Challenge II in Section 1. Thus, a good poisoning strategy is important.
(a) LunarLander, A2C, =0.1
0
100 200 300
---
draweR edosipe-reP naeM
(b) CartPole, VPG, C/K=0.5	(c) CartPole, A2C, C/K=1.0	(d) LunarLander, A2C, C/K=1.0
1.0
00
draweR evitalumuC
noitcA tegraT fo noitroporP
VA2C-P Oa, f = 0.1
VA2C-P Or, f = 0.1
VA2C-P Os, f = 0.1
FGSM, f = 0.1
FGSM, f = 1O.0
0.1 0.2 0.3	0.4	0.5 0.6 0.7 0.8 0.9	0	200	400	600	800	1,000	0	1,000	2,000	3,000	4,000	5,000	0	1,000	2,000	3,000	4,000	5,000
C/K	Episodes	Iterations	Iterations
Figure 4:	(a) VA2C-P also works under black-box setting; (b) hybrid-aim poisoning could be better than
single-aim poisoning; (c)(d) VA2C-P successfully forces the agent to choose the target policy.
Black-box Poisoning. Figure 3 demonstrates the performance of VA2C-P when the attacker knows
the learner’s model. But when the learner’s model is not available (black-box attack), VA2C-P
could also work as shown in Figure 4a, where one can see that black-box poisoning is still effective,
although worse than white-box poisoning due to the lack of knowledge.
Hybrid-aim Poisoning. The experiments shown in Figure 3 assume that the attacker poisons one
of the 3 poison aims, Os , Oa or Or . But if the attacker happens to have access to all of these
poison aims, it can do better by performing a “hybrid” poisoning, i.e., at each iteration, evaluate
the vulnerability of the algorithm w.r.t. each poison aim with their corresponding attacker power,
then choose the one with the highest vulnerability. Figure 4b shows that adaptive attacking using a
hybrid poison aim could significantly outperform attacking using a fixed single poison aim.
Targeted Poisoning. Baselines. Although targeted RL poisoning is studied by many works (Huang
& Zhu, 2019; Rakhsha et al., 2020), few of them work for deep RL. Despite that no existing work
focuses on deep policy-based methods, we transfer the FGSM-based targeted poisoning method
proposed for DQN by Behzadan & Munir (2017) to attack policy-based learners as our baseline.
The attacking method is, for each new state, adding a perturbation to it such that the perturbed state
is pushed across the decision boundary and towards the target action. See Appendix G.2 for details.
Performance. Figure 4c and 4d respectively show the targeted-attack performance comparison on
CartPole and LunarLander, where the target policies are both “always going to the right”. The y-axes
show the proportion of target actions among all actions taken by the learner. VA2C-P successfully
leads the learner to learn the target policy using all types of poison aims with relatively small attack
power. In contrast, FGSM fails to let the learner select the target action in most cases, even with
much larger attack power. The main reason why FGSM works for DQN Behzadan & Munir (2017)
but not for policy-gradient methods is, policy-based agents generally learn stochastic policies, and
even though FGSM could perturb the state such that the output probability for the target action is
0.51, the agent will still choose other actions with probability 0.49. Therefore, FGSM cannot easily
perform targeted poisoning against policy-based learners.
More experiment results are in Appendix G.3. Note that our proposed poisoning method can also be
extended to off-policy learners, as discussed in Appendix G.4.
6	Conclusion and Discussion
In this paper, we propose VA2C-P, the first generic poisoning algorithm for deep policy-gradient
online RL methods, which incorporates heterogeneous poisoning models and does not require any
prior knowledge of the environment. Although the effectiveness of VA2C-P is verified in a wide
range of RL algorithms and environments, we acknowledge that poisoning RL in practice is still
challenging, due to the relatively high computational burden and the uncertainty of the environments.
These challenges are also exciting opportunities for future work.
Acknowledgments
This work is supported by National Science Foundation IIS-1850220 CRII Award 030742-00001
and DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness
against Deception (GARD), and Adobe, Capital One and JP Morgan faculty fellowships.
9
Published as a conference paper at ICLR 2021
References
Xiaoxuan Bai, Wenjia Niu, Jiqiang Liu, Xu Gao, Yingxiao Xiang, and Jingjing Liu. Adversarial
examples construction towards white-box q table variation in dqn pathfinding training. In 2018
IEEE Third International Conference on Data Science in Cyberspace (DSC),pp. 781-787. IEEE,
2018.
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recogni-
tion, pp. 262-275. Springer, 2017.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines. arXiv preprint arXiv:1206.6389, 2012.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong Tong, and Zhen Han. Adversarial
attack and defense in reinforcement learning-from ai security view. Cybersecurity, 2(1):11, 2019.
Adam Gleave, Michael Dennis, Neel Kant, Cody Wild, Sergey Levine, and Stuart Russell. Adver-
sarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Ad-
versarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial
intelligence, pp. 43-58, 2011.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217-237. Springer, 2019.
Matthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning. In
AAMAS, 2020.
Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tac-
tics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017.
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement
learning and control. In Advances in Neural Information Processing Systems, pp. 14543-14553,
2019.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on
machine learners. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
10
Published as a conference paper at ICLR 2021
LUis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,
pp. 27-38,2017.
T. T. Nguyen, C. Soussen, J. Idier, and E. Djermoune. Np-hardness of l0 minimization problems:
revision and extension to the non-negative setting. In 2019 13th International conference on
Sampling Theory and Applications (SampTA), pp. 1-4, 2019.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, pp. 2040-2042, Rich-
land, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum markov games. volume 37 of Proceedings of Machine Learning Re-
search, pp. 1321-1329, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/perolat15.html.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2817-2826. JMLR. org, 2017.
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via
environment poisoning: Training-time adversarial attacks against reinforcement learning. arXiv
preprint arXiv:2003.12909, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438,
2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In
Advances in Neural Information Processing Systems, pp. 6103-6113, 2018.
Thomas Spooner, John Fearnley, Rahul Savani, and Andreas Koukorinis. Market making via rein-
forcement learning. CoRR, abs/1804.04216, 2018.
Jacob Steinhardt, Pang WeiW Koh, and Percy S Liang. Certified defenses for data poisoning attacks.
In Advances in neural information processing systems, pp. 3517-3529, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Yizhen Wang and Kamalika Chaudhuri. Data poisoning attacks against online learning. arXiv
preprint arXiv:1808.08994, 2018.
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors
to adversarial examples. arXiv preprint arXiv:1706.03922, 2017.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5279-5288, 2017.
Yingxiao Xiang, Wenjia Niu, Jiqiang Liu, Tong Chen, and Zhen Han. A pca-based model to predict
adversarial examples on q-learning of path finding. In 2018 IEEE Third International Conference
on Data Science in Cyberspace (DSC), pp. 773-780. IEEE, 2018.
11
Published as a conference paper at ICLR 2021
Qu Xinghua, Zhu Sun, Yew Ong, Abhishek Gupta, and Pengfei Wei. Minimalistic attacks: How
little it takes to fool deep reinforcement learning policies. IEEE Transactions on Cognitive and
Developmental Systems, PP:1-1, 02 2020. doi:10.1109/TCDS.2020.2974509.
Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural
networks. arXiv preprint arXiv:1703.01340, 2017.
Haoqi Zhang and David C Parkes. Value-based policy teaching with active indirect elicitation. In
AAAI,, volume 8,pp. 208-214, 2008.
Haoqi Zhang, David C Parkes, and Yiling Chen. Policy teaching through reward function learning.
In Proceedings of the 10th ACM conference on Electronic commerce, pp. 295-304, 2009.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust
deep reinforcement learning against adversarial perturbations on observations. arXiv preprint
arXiv:2003.08938, 2020a.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. arXiv preprint arXiv:2003.12613, 2020b.
12
Published as a conference paper at ICLR 2021
Appendix: Vulnerability-Aware Poisoning Mechanism for Online RL
with Unknown Dynamics
A	Additional Related Works
Adversarial Attacks in SL. There have been many works studying adversarial attacking and de-
fending in the past decade (Huang et al., 2011; Steinhardt et al., 2017). Poisoning, as an important
type of adversarial attacking, has been well-studied in the field of machine learning (Biggio et al.,
2012; Mei & Zhu, 2015), including deep learning (Munoz-Gonzalez et al., 2017; Shafahi et al.,
2018). Wang & Chaudhuri (2018) consider online learning, which is similar to ours. However, they
focus on supervised learning, where the attacker knows all the data stream, while in our RL setting,
the whole training data stream is not available to the attacker (as Challenge I states).
Evasion Attacks in RL. Recently, adversarial RL is attracting more and more attention, especially
evasion attacks at test-time, as summarized by Chen et al. (2019). Most works consider adversarial
perturbations on observations, similar to adversarial examples in supervised learning. Huang et al.
(2017) first show that neural network policies are vulnerable to evasion attacks on states, by gener-
ating state perturbation with FGSM. Lin et al. (2017) consider the data-correlation problem in RL
and an attacker with limited ability, and propose a strategical attack method, which perturbs input
states only under certain conditions. In addition, Gleave et al. (2019) show that in multi-agent RL,
choosing an adversarial policy could also negatively affect the victim agent, which is similar to per-
turbing a state into a novel one. There are plenty of following works showing the vulnerability of
deep RL policies, even against a limited-power black-box attacker Xinghua et al. (2020); Inkawhich
et al. (2020), Adversarial attacks can be utilized to train robust policies Pinto et al. (2017); Pattanaik
et al. (2018). There is also a line of work focusing on adversarial examples in path-finding prob-
lems Xiang et al. (2018); Bai et al. (2018). These evasion attacks are created to fool a well-trained
policy, but they can not change the policy itself in training time as poisoning does.
B A Poisoning Framework for RL
In this section, we establish a generic framework of poisoning in online RL, systematically charac-
terizing its challenges and difficulties from multiple perspectives - objective of poisoning, various
poison aims, and attacker’s knowledge. Our in-depth comparison with the SL allows a thorough un-
derstanding of the additional vulnerability of online RL systems compared with the well-understood
SL systems. Our framework also provides a clear context to correctly position prior works in the
literature as well as to compare our work with existing works. Compared to the poisoning frame-
work described by Huang & Zhu (2019), we provide a solution for unifying these poison aims in
one attack model in this section.
We consider the online learning scenario, where the RL agent (the learner) does not know the dynam-
ics or rewards of the underlying MDP M with state space S, action space A, transition dynamics
P , rewards R and discount factor γ .
Settings and Notations In online RL, the learner interacts with the environment and collects obser-
vations. The learner’s algorithm, denoted by f, iteratively searches for a policy π parametrized by θ,
through K interactions with the environment. Before learning starts, the learner initializes a policy
π1 . At each iteration k, the learner uses its previous policy πk-1 to roll out observations Ok from
the MDP M. Ok is a concatenation of multiple trajectories, denoted as Ok = (sk, ak, rk), where
Sk = [s1,s2,…],ak = [a1,a2,…],rk = [r1,r2,…]]are respectively the sequence of states,
actions, and rewards in iteration k. Then, with the observations Ok, the learner updates its policy
by attempting to solve argmaxπ J (π, πk-1, Ok), where J is the objective function. The generated
policy by the learner’s algorithm πk = f (πk-1, Ok)4 does not necessarily achieve the maximization
of the objective function.
In this paper, an overhead check sign " on a variable always denotes that the variable is poisoned.
For example, if the attacker changes a reward rt, then the poisoned reward is denoted as r>
4For algorithms with experience replay, the update can be extended as πk+1 = f(πk , O1:k).
13
Published as a conference paper at ICLR 2021
Procedure 2: Flow of Online RL Poisoning
1	Learner initializes its initial policy πθ0
2	for k = 1,…，K do
3	Learner rollouts observation (trajectories) Ok in environment M with current policy πk
4	Attacker may poison the observation Ok to Ok
5	Learner updates its policy: πk+1 = f(πk, Ok) ≈ argmaxπ J (π, πk, Ok)
Poison Objective. We use LA to denote loss function of the poisoning attack, which the attacker
attempts to minimize. The form of LA is determined by its goal, which falls into one of the two
categories, non-targeted and targeted poisoning.
In non-targeted poisoning, the attack poisons a policy π to π to minimize the learner,s expected
rewards. Therefore the poison objective LA is to minimize the learner,s value η(∏).
In targeted poisoning, the attack “teaches” the agent to learn a pre-defined target policy ∏L There-
fore the poison objective LA is defined as distance5(∏, ∏1).
Most existing poison RL researches focus on targeted poisoning (Ma et al., 2019; Rakhsha et al.,
2020), and non-targeted poisoning, although discussed by Huang & Zhu (2019), remains relatively
untouched.
poison aims. To influence the behaviors of the learner, an attacker could inject poison at multiple
locations of the learner,s learning process as detailed in Figure 5. Part of the reason why poisoning
in RL is more challenging than in SL is that it involves more poison aims of poisoning, some of
which adapt with the environment, increasing the uncertainty.
Poison Training Data
(a) supervised learning
Poison MDP
(b) reinforcement learning
MP- poison
MR- poison
OS- poison	Poison Observation
State St	Or- poison
Reward Rt
Poison Executor
ʌ Eo- poison
Action Zt
Figure 5: Different poison aims of poisoning in supervised learning and reinforcement learning.
Poison Aim I - Poison Observation (Or, Os). The attacker could manipulate the observation of
the learner, i.e., change O into O. This may happen when the attacker is able to intercept the
communication between the learner and the environment, similar to the man-in-the-middle attack in
cryptography. The attacker could target the rewards, called Or -poisoning, studied by Huang & Zhu
(2019); or the states, called Os-poisoning, investigated by Behzadan & Munir (2017).
Poison Aim II- Poison MDP (MR, MP). An attack could directly change the MDP (environment)
that the learner is interacting with, i.e., change M into M. For example, a seller could influence the
behaviors of customers by changing the prices of products. The poison of MDP could be injected at
the reward model R or the transition dynamics P, respectively denoted as MR-poisoning (studied
by Ma et al. (2019)) and MP -poisoning (studied by Rakhsha et al. (2020)) . The analogy of poison
MDP in SL is to manipulate the underlying data distribution of the training data.
Poison Aim III- Poison Executor Ea. The executor of the learner could be poisoned. For example,
an attacker applies a force to the agent, so that the intended action “north” becomes “northeast”.
Pinto et al. (2017) train a robust RL agent against the executor poisoner. Denote this type of poi-
soning as Ea -poisoning. We show in the Appendix C that Ea-poisoning is equivalent to directly
changing a stored in the observation O = (s, a, r, d).
5There are many ways to define the distance between two policies, for instance KL-divergence for stochastic
policies (Schulman et al., 2015a), and average mismatch for deterministic policies (Rakhsha et al., 2020).
14
Published as a conference paper at ICLR 2021
Attacker’s Knowledge At the k-th iteration, what an attacker can do depends on its current knowl-
edge set, denoted by Kk. Kk could contain the underlying MDP M, the learner’s algorithm f, the
learner,s previous policy models θ±k-ι as well as the previous and current observations Oi：k.
An omniscient attacker knows everything , i.e, KkO)={M, f, θi:k-i, Oi：k }. Most guaranteed pol-
icy teaching literature (Rakhsha et al., 2020; Ma et al., 2019) assume omniscient attacker. However
as motivated in the introduction, it is often unrealistic to exactly know the underlying environment.
We discuss two more realistic setting where the attacker only has limited knowledge as follows.
A monitoring attacker has some information but does not assume knowledge of the underlying
MDP M, i.e., KkM) = {f, θ±k-ι, Oi：k}. This is especially relevant in applications where learner's
information is not secure (or even open), or an attacker hacks to steal information from the learner.
Monitoring attacker is similar to the white-box attacker in supervised learning.
(T)
A tapping attacker has very limited knowledge and knows the observations only, i.e., Kk =
{O1:k }. This is widely applicable since the tapping the communication between the learner and
the environment is easy. Tapping attacker is analogous to the black-box attacker in supervised learn-
ing. Behzadan & Munir (2017) consider a tapping attacker, which observes and manipulates the
states but does not know the learner’s parameters.
C Target Types of Attacking： MORE Detailed Explanation
Figure 6: The online poisoning vs learning. Blue solid lines denote the learning processes, while red
dashed lines denote the poisoning processes. in iteration k, the learner uses its previous policy πk-1 to roll
out observations Ok from current MDP Mk, then updates its model and policy by πk = f (πk-1, Ok) =
argmaxπ J (π, πk-1 , O). The attacker may (a) poison observations after they are generated, (b) poison MDP
before the learner generates observations, or (c) poison the policy when it is used to generate observations.
Among these three scenarios, Ok is always influenced by the attack, thus denoted as Ok.
C.1 Poison Observation
Consider a deep reinforcement learning algorithm, which uses its old policy to generate a series of
observations and updates its policy with the collected observations at every iteration. The observa-
tions are stored in a temporary buffer in the form of (s, a, r, d), where s, r, d are returned by the
environment, and a is produced by the policy itself.
An attacker could stay in the middle between the learner and the environment and falsify s, r re-
turned by the environment before the learner receives them. (it is also possible to alter the terminal
state flags d, but it is relatively easier for the learner to detect, and its influence to the learner is not
as large as s and r, so we do not discuss this attack type.) On the other hand, the attacker may also
hack the observation buffer to change s, r. in both cases, poisoning s and r are called Os-poisoning
and Or-poisoning respectively.
Note that in the case of hacking observation buffer, the attacker also has the option to manipulate
a in O. But we do not include this kind of attack in observation poisoning, because not like states
and rewards, the actions are taken by the learner, so it is easy for the learner to detect the change
of action sequence stored in the buffer. And we will show in Section C.3 that changing a in O is
similar to the case of executor poisoning.
15
Published as a conference paper at ICLR 2021
C.2 POISON MDP
MDP poisoning can be considered as “changing the reality” for the learner. For example, an attacker
decides to increase the reward for state-action pair (s, a) by ∆ at iteration t, it then changes the
parameter Rk (s, a) of the environment to Rk (s, a) = Rk (s, a) + ∆. As a result, whenever the
learner visits (s, a) in the current iteration, it receives Rk (s, a) as its reward. A more intuitive
example is depicted in Figure 7, where we want to train a mouse to find the cheese in a maze.
However, if some bad man adds another piece of cheese in the training environment, the mouse is
likely to be misled. Then, in the test time, the “malicious cheese” is removed, but the mouse still
wants to find the malicious cheese instead of the original one.
(b) poisoned environment
Figure 7: An intuitive example of MDP poisoning.
Compared with observation poisoning and executor poisoning, MDP Poisoning is more powerful
and more difficult to defend against. An example of MDP poisoning is given by Rakhsha et al.
(2020), where the attacker can force the learner to learn a target policy with guarantees.
C.3 Poison Executor
In the online RL process, the learner learns by taking actions and getting feedbacks of the actions. If
observation poisoning is viewed as changing the feedbacks, then executor poisoning can be viewed
as perturbing the actions taken by the learner. For example, for an auto-driving agent, one can
slightly add some force to the steering wheel, so when the agent takes “steer left 90 degrees”, what
actually happens is “steer left 100 degrees”. In this way, the trained policy is biased.
We now show that the aforementioned attack of changing a in O could be converted to executor
poisoning. We describe the following two scenarios respectively for these two poisoning attacks and
show they have equal effects. For simplicity, we only consider one step of the interaction, which can
be simply extended to multiple steps.
(1)	Poisoning a in O. For one-step experience, the learner observes state s, takes action ai, receives
reward r = R(s, αi) and observes a new state Si 〜P(∙∣s, a。then the tuple (s, ai, ri, Si) will be
stored in the observation buffer. Now the attacker may change the action ai to aj , and the tuple
becomes (S, aj, ri, S0i). Finally, the learner regard ri, S0i and the following observations as caused by
aj , instead of ai .
(2)	Poisoning Executor. In the same environment, suppose the learner observes state S, and takes
action aj , then the attacker conducts executor poisoning and changes aj to ai , then the reward and
the next state will also change to r = R(s, aj and Si 〜P(∙∣s, aj. The learner does not know the
falsification of action and stores (S, aj, ri, S0i) to the buffer. Finally, the attacker will take ri, S0i and
the future observations as caused by aj , and updates its policy accordingly.
Based on the descriptions, we can see changing ai to aj in the buffer is equivalent to changing aj
to ai on the executor. If we assume the policy always has non-zero probability on all actions, then
for any manipulation on a in O, it is possible to achieve the same effects by attacking the executor.
Hence, given the fact that the former attack can be trivially defended, we only discuss executor
poisoning in our poisoning framework.
16
Published as a conference paper at ICLR 2021
D	Robustness and S tab ility of RL Algorithms
D.1 Measuring Vulnerability of RL Algorithms
Definition 1 in Section 4 measures the stability of one update based on policy discrepancy. But it
is not obvious whether the rewards change drastically due to the attacks. Proposition 2 provides a
guarantee on the performance of the poisoned policy ∏0, compared with the Un-Poisoned new policy
π0.
Proposition 2. For an update of stochastic policy π0 = f(π, O), if its δ-stability radius is ε with the
total variance measure, then any poisoning effort smaller than ε on D will cause the expected total
reward drop (η(∏0) — η(∏0)) by no more than
4δ2γmaxs,a ∣A∏o(s,a)∣	, ʌ , ,, ʌ,	...
Zj E	+ 2δ / gπ 0(S) max An 0I(S, a) |	(2)
(1 — γ)2	s∈S	a
where Y is the discountfactor, g∏ (s) := E∞=0 YtP(St = s∣∏) is the discounted visitationfrequency,
and A is the advantage function, i.e., Aπ (S, a) = Qπ (S, a) — Vπ (S).
Proposition 2 shows that if the attack power is within the stability radius, the reward of the poisoned
policy will not be influenced too much. which also explains the motivation behind our proposed
vulnerability-aware attack. The proof is in Appendix D.4.
With Definition 1, the one-update stability measure, we are able to formally define the stability
radius of an RL algorithm w.r.t. an MDP.
Definition 3 (Stability Radius w.r.t an MDP). The δ-stability radius ofan algorithm f in anMDPM
is defined as the minimal stability radius of all observations drawn from the MDP, and all possible
policies in policy space Π. If f is on-policy, then φ(f, M) = min∏∈∏,o〜∏ φ(f,π, O); if f is
Of-Policy, then φ(f, M) = min∏∈∏,o〜∏ φ(f, π, O), where ∏ is the behavior policy;
D.2 Robustness Radius
Different with poisoning, test-time evasion (adversarial examples) misleads the agent by manipu-
lating the states only, since the agent no longer learns from interactions and feedbacks. Note that
although Gleave et al. (2019) propose an attack called “adversarial policy”, the perturbation does
not happen in the policy, but still happens in the input states (observations) of the agent.
To study how robust a trained policy is, we define the robustness radius with regard to both a single
state and the whole environment.
Definition 4 (Robustness Radius of Policy w.r.t. a State). The robustness radius ofa deterministic
policy π on a state S is defined as the minimal perturbation of S which changes the output action,
i.e.,
ρ(π, s) = inf{∃s ∈S∩Bε(s) s.t. π(s) = π(S)}	(3)
ε
Similarly, for any 0 < δ < 1, the δ-robustness radius of a stochastic policy π on a state S is
defined as the minimal perturbation of S which makes the output action distribution disagrees with
the original π(S) with probability more than δ, i.e.,
ρ(π, s)δ = inf{∃s ∈ S ∩ Bε(s) s.t. d(π(∙∣s)∣∣π(∙∣S)) > δ}	(4)
where d(∙∣∙) could be any distance measure between two distributions.
Remark. If we regard the policy as a classifier which “classifies” a state to an action, then the
robustness radius of policy defined above is analogous to the robustness radius of classifiers defined
by Wang et al. (2017), with an extension to stochastic predictions.
Definition 5 (Robustness Radius w.r.t an MDP). The (δ-)robustness radius of a policy π in an MDP
M is defined as the maximal robustness radius of all states, i.e., ρ(π, M) = mins∈S ρ(π, S)
Remarks. (1) A deterministic policy is robust against any state perturbation smaller than ρ(π, M).
17
Published as a conference paper at ICLR 2021
(2)	For a stochastic policy, if its δ-robustness radius in an M is ε, then any state perturbation within
ε will cause the expected total reward drop by no more than
(Z1 2δγ 2, +2δ)max |R(s, a)|,	(5)
(1 - γ)2	s,a
which is proven by Theorem 5 in Zhang et al. (2020a).
D.3 Vulnerability Comparison: Difference B etween SL and RL
To shed some light on understanding adversarial attacks in RL, we compare SL and RL in terms of
their vulnerability to poisoning and adversarial examples.
At test time, a policy network receives states as input, and returns probabilities of choosing each
actions as output; a value network receives states (or state-action pairs) as input, and returns the
corresponding value as the output. Thus, test-time RL systems are very similar to SL systems,
as one can view the policy networks as classification networks, and value networks as regression
networks. However, the key difference between evasion in RL and evasion in SL is, data samples
are not independent in RL. A single adversarial example in SL test dataset may cause at most one
misclassification instance, whereas an adversarial example in RL may case a drastic change of the
gained rewards (e.g., by leading the agent to a ”devastating” or ”absorbing” state).
At training time, SL systems and RL systems are significantly different, as Figure 5 shows. Even
when the supervised learner also learns from data streams in an online manner, the training data
are independent with the learner’s classifier. In contrast, the distribution of training data samples
changes as the learner updates its policy. Poisoning attacks against an SL system could alter the
decision boundary, so that the learner makes wrong decisions for certain data samples. For an RL
system, poisoning attacks could (1) alter the decision boundary so that the learner chooses bad
actions for certain states, and also (2) change the following observations and interactions due to a
different selection of action.
In summary, an adversarial attacker may cause higher damages on RL systems than on SL systems,
with the same power and budget. But it does not suggests attacking RL systems is easier than
attacking SL systems. As every coin has two sides, the high uncertainty of the environment may
help an attack reduce the learner’s reward, but may also lead the learner to gain higher reward in the
future (as shown in Section 5). Therefore, it is more challenging to successfully attack RL systems
than SL systems with a specific goal.
D.4 Proof of Proposition 2
Proof. According to the definition of δ-stability radius, for any poisoning effort within ε, the poi-
Soned policy satisfies DmVx [∏0ll∏0] ≤ δ (assume total variance DTV is the distance measure between
policy distributions). We are interested in the difference of expected rewards η(∏0) - η(∏0).
Define L∏ (∏0) = η(∏0) + Ps∈s gπ(S) Pa∈A ∏0(a∣s)A∏o(s,a), where g is the discounted state
visitation frequencies, i.e.,
g∏0(s):= P(S0 = s∣∏0) + YP (si = s∣∏0) + YP(S2 = s∣∏0) +----------.
Since DmVx[∏0∣∣Π0] ≤ δ, follow Theorem 1 in paper (Schulman et al., 2015a), We can get
∣η(∏0) - l∏o (∏0)∣≤ 4δ2γ max;A∏0 Ga)I.
So we have
lη(π0) - η(π0) - EgnO(S) E π0(als)A∏0(S, a)| ≤
s∈S	a∈A
4δ2γmaXs,a ∣A∏0(s,a)l
(1-^)2
which can be transformed to
η(∏0) - η(∏0) ≤
4δ2γmaXs,a ∣A∏0(s,a)l
(1-^)2
-fgπ(s) E∏0(a∣s)A∏o(s,a).
s∈S	a∈A
(6)
(7)
(8)
18
Published as a conference paper at ICLR 2021
We upper bound the term - 52s∈s gπ (S) ∑2a∈∕ ∏0(a∣s)A∏o (s, a) as below.
-fgπ(S) E ∏0(a∣s)A∏o (s, a)
s∈S	a∈A
=X g∏0(s)(- X π0(als)A∏0 (s, a))
s∈S	a∈A
=y^g∏0 (s)( -	∏0(a∣s)A∏0 (s, a) +	∏0(a∣s)A∏o (s,α) - E∏0(a∣s)A∏o (s,a))
s∈S	a∈A	a∈A	a∈A
=X g∏0(s)(XA∏o(s, a)(π0(a∣s)一 ∏0(a∣s))) - X ∏(a∣s)A∏o (s, a))
s∈S	a∈A	a∈A
≤ EgnO(S)(2δmax ∣A∏o(s, a)| - £ π0(a∣s)A∏o(s, a))
s∈S	a∈A
=2δ 1 g∏o(s) max |A∏o(s, a) |
a∈A
s∈S
(9)
Combining the above results, we obtain
η(∏0) - η(∏0) ≤ 4δ Yma：,。IAnO(S,a)| + 2δXg∏0(s)max ∣A∏0(s,a)∣	(10)
(1- γ)2	s∈S	a
□
E Algorithm
Assume the learner’s policy π is parametrized by θ.
HoW to solve the projected gradient descent. To solve (P), assume 条 exists, one can use Pro-
jected gradient descent to update r by using the chain rule:
dη∏θk 1 Ek)_ dη∏θk-1 (πθk)∂θk
∂r	∂θk	∂r .
For Vanilla Policy Gradient (VPG) whose update rule is, θk = θk-ι + αVθfc-ιη(∏θfc-ι, r), where
NT	T
vθk-ι η(πθk-ι, r) = N X((X vθk-ι log πθk-ι (a(i)|s(i)))(X r(i))),	(II)
i=1 t=1	t=1
we can derive
vθη∏θk-1 (πθfc) ≈ NX (πT=ι-------/ (M ⑴、(X vθk logπθk(at)|St'))(Xγ	rt )).
N i=1	πθk-1 (at |St ) t=1	t=1
(12)
and
t
[Vr θk]t = X vθk-ι log πθk-ι (aj |Sj Nt-"	(13)
j=1
Although 舞 has a closed-form expression for simple learners like VPG, analytically computing
how the poisoned reward influences the model is challenging for more complicated learners like
PPO, whose update rule is an argmax function. Therefore, we use the Direct Gradient Method
proposed by Yang et al. (2017) to approximate the gradient by
dη∏θk 1 Ek) _ η∏θk 1 (f(πθk-ι, r + δ))- η∏θk-1 (f(πθk-ι, r))
∂r	∆
(14)
19
Published as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
Algorithm 3: Non-targeted White-box VA2C-P with Or -Poisoning
Input: total iterations of learning K; poisoning power e; poisoning budget C; attacker's
learning rate β; maximum computing iterations J; distribution distance measure d
Initialize policy discrepancies as an empty list Ψ = 0
Initialize the number of already poisoned iterations c = 0
Initialize value network Vω
for k = 1,…，K do
if c > C then
I Break
Get the current observation Ok and the learner’s policy model θk-1
Fit the value function: ω —argminω Pτi∈Ok PT=I(Vω (Sf)) — PT=t γ'-tr(i) 产
Imitate learner,s update with the clean rewards θk,η J Update (θk-ι, r)
Initialize r as the original r in Ok
Set η0 = ηc
for j = 1,…，J do
for i = 1,…,N and t = 1,…，T do
Copy r0 J r, and add a small value ∆ to [r0](i)
Imitate learner’s update with poisoned rewards θ0, η0 J Update(θk-1, r0)
Compute the direct gradient: -¾∙ J η -RT
∂r,t !	δ
Update the poisoned reward: r J 口后式乃(r 一 β 祭)
Imitate learner,s update with poisoned rewards θk, η J Update (θk-1, r0)
if (ηj 一 ηj-1) converges then
I Break
Compute ψk = NT Ps(i)d(∏θk ∣∣∏θk) and add ψk to Ψ
if ψk is larger than the b(C 一 c)/(K 一 k)c-th largest element in Ψ then
Attack: replace r with r in Ok and send it back to the learner
C J C +1
Procedure Update(θ, r)
Perform an update with the learner,s algorithm θ0 J f(θ, r)
Compute the attacker,s objective
j j___L P PT (πθ0(ati) |Sti)) )(PT √z-tr(i) _ v (Si))
η J NT 乙 τi∈Ok 乙 t=1(∏θ (a(i)∣s(i)) )(乙 t0 =t Y rt Vω (St))
return θ0, η
To show the concrete poisoning process, we assume D = Or . Then Algorithm 3 shows the detailed
procedure of VA2C-P with a non-targeted goal and a white-box attacker.
For Targeted Poisoning. In line 16, instead of computing —¾), we compute
∂dist(θ0,θt)
-∂r(i)-
For Black-box Poisoning. In line 7, instead of getting the learner,s policy model θk-1, we train a
policy with the same algorithm of the learner θk-1 J f (θk-2, Ok-1).
F Theoretical Interpretation of the Bi-level Optimization
Problem
In this section, we discuss the problem relaxation made in Section 4.2. For notation simplicity, we
focus on the case D = Or .
F.1 Problem Forms
Suppose the attacker has budget C = K. Then following the format of Problem equation Q, we
could define the original Or -poisoning optimization problem for the poisoning at the k-th iteration
as Problem equation P *. Note that for notation simplicity, we use the policy parameter θ to denote
20
Published as a conference paper at ICLR 2021
the policy πθ, so that η(θ) := η(πθ).
K
argmin	η(θj)	(P *)
rk,…，rκ j=k
s.t.	θj = f(θj-ι, r)
l∣rj - rj k ≤ e, ∀j = k,…，K
Note that We only optimize on rk to rκ, since previous k 一 1 decisions have already been made at
the k-th iteration.
Although an omniscient attacker is able to predict all the observations and solve Problem equa-
tion P * directly, for the non-omniscient attacker that We focus on, Problem equation P* is not
solvable because of the unknoWn observations in iteration k + 1 to K. Hence, as discussed in Sec-
tion 4, We relax equation P*, a multi-variable optimization problem, to (K - k + 1) sequential
single-variable optimization problems
argmin η(θk)	(Pk)
rk
s.t.	θk = f(θk-i, rk)
∣∣r⅛ 一 rkk ≤ e
argmin η(θk+1)	(Pk+1)
rk + 1
s.t.	θk+ι = f (θk, rk+ι)
I∣rk+1 — rk+ι∣ ≤ E
and all the Way to
argmin η(θK)	(PK)
rκ
s.t.	θκ = f (θκ-ι, rκ)
∣∣rκ — rκk ≤ E
where rj 〜θj-ι,∀j = k,…，K.
Note that rk is already generated and knoWn for all the above problems. When solving Problem
(Pj), rj and θj are also known (determined by θj-1).
F.2 Tightness of Problem Relaxation
Problem equation P* is a (K 一 k + 1)-variable optimization problem with (K 一 k + 1) equality
constraints and (K — k + 1) inequality constraints. And Problem equation Pk, •…，equation PK
are (K 一 k + 1) single-variable optimization problems respectively with 1 equality constraints and
1 inequality constraints. The two sets of problems are naturally equivalent if θk,…，θκ, as well as
the constraints are independent. However, due to the online learning process (Figure 6), θk+1 and
rk+ι are all dependent on θk and ιrk, which makes the relaxation not necessarily optimal.
We call the optimal solution to Problem equation P * as(rk，…，rK),and the optimal solutions to
Problem equation Pk to equation PK asr#，…,rK.
For simplicity, we assume the environment and the policies are all deterministic. So that the value
of observed reward rj is a deterministic function of θj-1.
We claim that the relaxation does not change the feasibility as stated in Proposition 6.
Proposition 6. (r#,…，r#) is afeasible solution to Problem equation P*.
Proof. Note that rk is known and the same for both equation P* and equation Pk.
(1)	r# is feasible to equation P * because it satisfies ||r# — rk∣ ≤ e.
21
Published as a conference paper at ICLR 2021
(2)	For Problem equation P*, with θk = f (θk-ι, r#), rk+ι is the same with the rk+ι in Prob-
lem Pk+ι. Since the optimal solution to equation Pk+ι, r#+「satisfies ||r#+i -入+11∣ ≤ e, r#+i is
also feasible to equation P*.
(3)	By induction, {r#}j=k all satisfy the constraints in equation P * at the same time, so
(r#,…，r#) is a feasible solution to Problem equation P*.	□
Note that if the environment or the policy is stochastic, then rj is a random variable sampled from
some distribution defined by θj-1. In this case, the constraints for Problem equation P* should be
Pr(krj - rj| ≤ E) ≥ t,∀j = k,…，K, where t ∈ [0,1] is a threshold probability. Then, with
appropriate t, Proposition 6 also holds.
So far we have shown that the relaxation is feasible, so that the attacker will not plan for a non-
feasible poisoning with VA2C-P. Next, we discuss the optimality of the relaxation.
We first make the following assumptions.
Assumption 1: the learner,s update function f (θ, r) is differentiable w.r.t. r and θ.
Assumption 2: the environment and the policies are all deterministic, and the reward r generated by
a policy πθ is differentiable w.r.t. θ (i.e. R(s, πθ(s) is differentiable w.r.t. θ).
Proposition 7. If Assumption 1 and 2 hold, the necessary condition for (r#,…，r#) being an
optimal solution to Problem equation P * is, for all j = k, •…，K,
∂η(θj) ∂θj I	= XX
帆叫'jL j=+ι
∂η(θj0) ∂θj0
∂θj0	∂θj0-1
∂%+ι ∂θj ।
∂θj ∂rj |rj =r#
(15)
Proof. Consider the case K = 2 for simplicity, and the results naturally extend to a larger K.
At iteration 1, Problem equation P * becomes
argmin	η(θ1) + η(θ2)	(P0)
r1,r2
s.t.	θι = f (θo, ri)
θ2 = f (θl, r2)
∣∣rι - rik ≤ E
I∣r2 - r2k ≤ E
where θ0 and r1 are known.
The relaxed problems are
argmin	η(θ1 )	(P 1)
ri
s.t.	θi = f (θo, ri)
∣∣rι - rik ≤ E
where θ0 and ri are known, and
argmin	η(θ2 )	(P2)
r2
s.t.	θ2 = f (θi, r2)
I∣r2 - r2k ≤ E
where θi and r are determined by ri, the solution to equation P1.
Suppose r# and r# are the optimal solutions to equation P1 and equation P2. And we discuss
the necessary condition for rri# and rr2# being optimal to equation P0.
We can rewrite equation P 1 as
argmin	η(f (θ0, rri))
ri
s.t. ∣rri - ri∣2 - E2 +yi2 = 0
22
Published as a conference paper at ICLR 2021
And the Lagrange function of the above problem is
L(r1,λ1,y1) = η(f(θo, rι)) + λ1(kr1 - r4|2 -e2 + y2)	(16)
The necessary conditions for ri being optimal are
dη(f(θ0, rS + dλi(IlrI - rik2 - e2 + y2) = 0	(17)
ɑ -	+	ɑ -	U	(J_/)
∂rι	∂ri
l∣rι - rik2 — e2 + y2 =0	(18)
λiyi =0	(19)
for some λi and yi.
Similarly, for Problem equation P2, the necessary conditions of optimality are
	dη(f(θi, r2))	+	dλ2(kr2	- r2k2	- e2 +	y2) —	0	(20) ∂r2	+	∂ r	=	( ) krr2 —r2k2 —e2+y22 =0	(21) λ2y2 = 0	(22)
for some λ2 and y2 .
Since r# and r# are the optimal solutions to equation P1 and equation P2, r# and r# satisfy
Equation equation 17 〜 equation 22.
Expanding equation 17 and equation 20, we get
	∂θi ∂ri |ri=r# +2λi(rf - ri)=0	(23) ∂θ2∂ll|r2=r# +2λ2(r^^ -r2)=0	(24)
For Problem equation P0, the Lagrange is
L(rι, r2λ1,λ2,y1 ,y2) = η(f (θo, rI))+Mkri—riF—e2+© )2)+η(f (θ2,电力+讨怩—司户—j+侬 )2)
(25)
And the necessary conditions for ri, r being optimal are
dη(f(θo,ri)) + dη(f(θi, r2)) + dλi(kri - rik2 - e2 + (yi)2) + dλ2(kr2 - r2k2 一 a + (y)2) = 0				
∂rri	∂rri	∂rri		∂rri		
			(26)	
dη(f(θi,r2)) + dλ2(kr2 -r2k2 -e2 + (y2)2) =0			(27)	
∂rr2	∂rr2				
krri — rik2 — e2 + (yi0)2 = 0			(28)	
krr2 — r2k2 — e2 + (y20)2 = 0			(29)	
λ0i yi0 = 0			(30)	
λ02y20 = 0			(31)	
for some λ0i, λ02, yi0 , y20 (not the same with λi, λ2, yi, y2).				
Expanding equation 26 and equation 27, we get				
∂η ∂θi	∂η ∂θ2 ∂θi	∂θ2 ∂r ∂θi 西 ∂ri |ri=r# + 丽(西 ∂ri + ∂r^ "θT) ∂r^ |ri=r#	+ 2λi(rf -	ri ) + 2λ02 (rr2# —	) ∂r2 ∂θi |	_ r2) ∂θi ∂r^ |ri=r# 一	0
			(32)	
∂dη dr!扃=玻 +2M(r# — r2)=0			(33)	
23
Published as a conference paper at ICLR 2021
Combining equation 23, equation 24, equation 32 and equation 33, we obtain
∂η ∂θ2 ∂θι	∂η ∂θι
∂θ2 布 ∂r1 |ri=r# = ζ∂θ1 ∂r1 |ri=r#
(34)
for some Z, which is the necessary condition for r# and r# being optimal to Problem equation P0.
Intuitively, this condition implies that the gradient of η(θι) w.r.t. r# (the RHS) should be aligned
with the gradient of η(θ2) w.r.t. r# (the LHS), without considering the influence of ri to τ⅛. That
is, although ri influence θi, θi influences both θ and τ⅛ (because τ⅛ is generated by ∏θ) LHS
does not include 留蔻薪第,which makes equation 34 computable in many cases.
However, for the setting of our VA2C-P (monitoring attacker for online RL), the attacker at iteration
k = 1 does not know the observed reward r2 of iteration k = 2, which prevent the agent from
conducting the optimal attack. But equation 34 could help the attacker verify whether a past poison
is likely to be optimal for the iterations so far. For example, if the learner is using VPG, then at
iteration k = 2, the attacker can test whether its previous poison ri did a good job in minimizing
η(θι) + η(θ2) by evaluating whether (I + V2ιη(θ1))Vθ2η(θ2) is equal to R§、η(θι).
□
G Experiment Settings and Additional Results
G.1 Detailed Experiment Settings
Network Architecture. For all the learners, we use a two-layer policy network with T anh as the
activation function, where each layer has 64 nodes. PPO, A2C and ACKTR also have an additional
same-sized critic network. We implement VPG and PPO with PyTorch, and the implementation of
A2C and ACKTR are modified from the project by Kostrikov (2018).
Hyper-parameters. In all experiments, the discount factor γ is set to be 0.99. We run VPG and
PPO for 1000 episodes on every environment, and update the policy after every episode. For A2C
and ACKTR, we use 16 processes to collect observations simultaneously, and update policy every 5
steps (i.e., each observation O has 80 (s, a, r) tuples); learning last for 80000 steps in total.
Distance Measure for Perturbation The definition of total effort function U(∙) plays an important
role of understanding the attack power. Since states, actions and rewards are in different forms and
scales, we define U differently for various poison aims. Also, note that O is a concatenation of
state-action-reward tuples, and its length could vary in different iterations, so we normalize over the
length of observation.
For D = Os, we define the total effort as
U (Os，OS)= pR SXsks - sk2.
For D = Oa, if the action space is continuous, we define the total effort as
U (OaQa)=prn aXaka-ak2，
and if the action space is discrete, we define the total effort as
U(Oa, Oa) =	1___ X l(a = a).
POI a∈0a
For example, in CartPole, the action is either 0 or 1, then the attacker with = 0.1 can flip up to
10% of the actions in one iteration.
For D = Or , we define the total effort as
1
u (Or, Or ) = ^= Ilr - r∣∣.
Pm
24
Published as a conference paper at ICLR 2021
In the supplementary materials we provide the code and instructions, as well as demo videos of
poisoning A2C in the Hopper environment, where one can see under the same budget constraints,
random poisoning has nearly no influence the agent’s behaviors, while our proposed VA2C-P suc-
cessfully prevents the agent from hopping forward.
G.2 FGSM-based Poisoning
The procedure of the FGSM-based targeted poisoning is as follows. We transfer the method pro-
posed by Behzadan & Munir (2017) from attacking DQN to attacking policy-based algorithms.
Although Behzadan & Munir (2017) assume a black-box setting, we hereby use white-box attack
(attacker knows learner’s policy parameters) in order to let FGSM be a stronger baseline.
For step t,
Step 1: the learner observes st takes action at, the environment returns reward rt, state st+1;
Step 2: the attacker queries the target policy and gets a&&v = ∏*(st+ι);
Step 3: the attacker poisons st+1 by
St+1 = St+1 + C X sign(Vst+ι (∏(°adv ∣st+ι)));
Step 4: the attacker sends St+ι as st+ι to the learner.
G.3 Additional Experiment Results
Figure 8 shows additional results on various environments and RL algorithms, including both non-
targeted poisoning and targeted poisoning.
25
Published as a conference paper at ICLR 2021
CartPole, PPO, D = Os, =0.5 Hopper, ACLKTR, D = Or, =0.1
CartPole, A2C, D = Oa, =0.1
0000
8642
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
(e) CartPole, VPG, D = Os, =0.1
CartPole, A2C, D = Or, =0.1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
CartPole, VPG, D = Or, =0.4
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
CartPole, VPG, D = Or, =0.6
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
(d) Ha4lfCheetah, A2C, D = Or, =0.1
00 50
draweR edosipe-reP naeM
No Poison
Random Poison
AC-P
VA2C-P
000000
42 0 864
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
Hopper, VPG, D = Os, =0.1
draweR edosipe-reP naeM

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
Hopper, A2C, D = Or, =0.1
draweR edosipereP nae
-103
-104
AC-P
VA2C-P
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
CartPole, VPG, D = Oa, =0.1
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
HalfCheetah, ACKTR, D = Or, =0.1

500
-∑
400
C/K
150
00 50
draweR edosipe-reP naeM
No Poison
Random Poison
AC-P ∖s.
VA2C-P
Black-box
C/K
CartPole, A2C, D = Oa, =0.1
C/K
CartPole, A2C, D = Or, =0.1
C/K
(Targeted) LunarLander, VPG, C/K=1.0
00000
8642 0
draweR edosipe-reP naeM
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
50 00 50
draweR edosipe-reP nae
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
C/K
noitcA tegraT fo noitroporP
(Targeted) CartPole, VPG, C/K=1.0
0	200	400	600	800	1,000
Iterations
.8 .6 .4
000
noitcA tegraT fo noitroporP
-VA2C-P Or,f = 0.8
V - VA2C-P Os,f = 0.1
—FGSM,f = 0.1
- FGSM,f = 1.0
0	200	400	600	800	1,000
Iterations
Figure 8:	Additional Experimental Results
G.4 Extension to Off-policy Learners
Although we focus on on-policy policy gradient learners in this paper, our poisoning method is
also applicable to off-policy learners which update their policies using sampled mini-batches from
all historical observation (trajectories). If the adversary can manipulate the mini-batch the learner
samples at every step, our proposed poisoning process works as usual. We implement this idea and
test it for one of the state-of-the-art off-policy learning method SAC (Haarnoja et al., 2018), and the
results are shown in Figure 9, where VA2C-P significantly reduces the reward gained by the learner.
In the other case, if the adversary doesn’t see which mini-batch the learner samples but has access
to the buffer, he can still alter or insert some samples to influence learning.
26
Published as a conference paper at ICLR 2021
- - - - -
0000
0000
43 2 1
draweR evitalumuC
0	50	100	150	200
Episodes
Figure 9:	Poisoning off-policy algorithm SAC with VA2C-P. D = Or, C/K = 1, = 0.6.
27