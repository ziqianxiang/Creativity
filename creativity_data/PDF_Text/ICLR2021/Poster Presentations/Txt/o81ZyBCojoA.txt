Published as a conference paper at ICLR 2021
On Fast Adversarial Robustness Adaptation
in Model-Agnostic Meta-Learning
Ren Wang1,4 * Kaidi Xu2 Sijia Liu3,5 t Pin-Yu Chen3 Tsui-Wei Weng3 Chuang Gan3
Meng Wang1
1Rensselaer Polytechnic Institute, USA
2Northeastern University, USA
3MIT-IBM Watson AI Lab, IBM Research, USA
4University of Michigan, USA
5Michigan State University, USA
Ab stract
Model-agnostic meta-learning (MAML) has emerged as one of the most suc-
cessful meta-learning techniques in few-shot learning. It enables us to learn
a meta-initialization of model parameters (that we call meta-model) to rapidly
adapt to new tasks using a small amount of labeled training data. Despite the
generalization power of the meta-model, it remains elusive that how adversar-
ial robustness can be maintained by MAML in few-shot learning. In addition
to generalization, robustness is also desired for a meta-model to defend adver-
sarial examples (attacks). Toward promoting adversarial robustness in MAML,
we first study when a robustness-promoting regularization should be incorpo-
rated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update)
learning procedure. We show that robustifying the meta-update stage is suffi-
cient to make robustness adapted to the task-specific fine-tuning stage even if
the latter uses a standard training protocol. We also make additional justifi-
cation on the acquired robustness adaptation by peering into the interpretabil-
ity of neurons’ activation maps. Furthermore, we investigate how robust reg-
ularization can efficiently be designed in MAML. We propose a general but
easily-optimized robustness-regularized meta-learning framework, which allows
the use of unlabeled data augmentation, fast adversarial attack generation, and
computationally-light fine-tuning. In particular, we for the first time show that
the auxiliary contrastive learning task can enhance the adversarial robustness of
MAML. Finally, extensive experiments are conducted to demonstrate the effec-
tiveness of our proposed methods in robust few-shot learning. Codes are available
at https://github.com/wangren09/MetaAdv.
1 Introduction
Meta-learning, which can offer fast generalization adaptation to unseen tasks (Thrun & Pratt, 2012;
Novak & Gowin, 1984), has widely been studied from model- and metric-based methods (San-
toro et al., 2016; Munkhdalai & Yu, 2017; Koch et al., 2015; Snell et al., 2017) to optimization-
based methods (Ravi & Larochelle, 2016; Finn et al., 2017; Nichol et al., 2018). In particular,
model-agnostic meta-learning (MAML) (Finn et al., 2017) is one of the most intriguing bi-level
optimization-based meta-learning methods designed for fast-adapted few-shot learning. That is, the
learnt meta-model can rapidly be generalized to unforeseen tasks with only a small amount of data.
It has successfully been applied to use cases such as object detection (Wang et al., 2020), medical
image analysis (Maicas et al., 2018), and language modeling (Huang et al., 2018).
In addition to generalization-ability, recent works (Yin et al., 2018; Goldblum et al., 2019; Xu et al.,
2020) investigated MAML from another fundamental perspective, adversarial robustness, given by
the capabilities of a model defending against adversarially perturbed inputs (known as adversarial
* Corresponding: RenWang (Wangren348117609@gmail.com, renwang@umich.edu).
^ Work is done at the MIT-IBM Watson AI Lab
1
Published as a conference paper at ICLR 2021
examples/attacks) (Goodfellow et al., 2014; Xu et al., 2019b). The challenge of lacking robustness of
deep learning (DL) models has gained increasing interest and attention. And there exists a proactive
arm race between adversarial attack and defense; see overview in (Carlini et al., 2019; Hao-Chen
et al., 2020).
There have existed many defensive methods in the context of standard model training, e.g., (Madry
et al., 2017; Zhang et al., 2019b; Wong et al., 2020; Carmon et al., 2019; Stanforth et al., 2019; Xu
et al., 2019a), however, few work studied robust MAML except (Yin et al., 2018; Goldblum et al.,
2019) to the best of our knowledge. And tackling such a problem is more challenging than robustify-
ing the standard model training, since MAML contains a bi-leveled learning procedure in which the
meta-update step (outer loop) optimizes a task-agnostic initialization of model parameters while the
fine-tuning step (inner loop) learns a task-specific model instantization updated from the common
initialization. Thus, it remains elusive when (namely, at which learning stage) and how robust reg-
ularization should be promoted to strike a graceful balance between generalization/robustness and
computation efficiency. Note that neither the standard MAML (Finn et al., 2017) nor the standard
robust training (Madry et al., 2017; Zhang et al., 2019b) is as easy as normal training. Besides the
algorithmic design in robust MAML, it is also important to draw in-depth explanation and analysis
on why adversarial robustness can efficiently be gained in MAML. In this work, we aim to re-visit
the problem of adversarial robustness in MAML (Yin et al., 2018; Goldblum et al., 2019) and make
affirmative answers to the above questions on when, how and why.
Contributions Compared to the existing works (Yin et al., 2018; Goldblum et al., 2019), we make
the following contributions:
•	Given the fact that MAML is formed as a bi-level learning procedure, we show and explain why
regularizing adversarial robustness at the meta-update level is sufficient to offer fast and effective
robustness adaptation on few-shot test tasks.
•	Given the fact that either MAML or robust training alone is computationally intensive, we propose
a general but efficient robustness-regularized meta-learning framework, which allows the use of
unlabeled data augmentation, fast (one-step) adversarial example generation during meta-updating,
and partial model training during fine-tuning (only fine-tuning the classifier’s head).
•	We for the first time show that the use of unlabeled data augmentation, particularly introducing
an auxiliary contrastive learning task, can provide additional benefits on adversarial robustness of
MAML in the low data regime, 2% robust accuracy improvement and 9% clean accuracy improve-
ment over the state-of-the-art robust MAML method (named as adversarial querying) in (Goldblum
et al., 2019).
Related work To train a standard model (instead of a meta-model), the most effective robust train-
ing methods include adversarial training (Madry et al., 2017), TRADES that places a theoretically-
grounded trade-off between accuracy and robustness (Zhang et al., 2019b), and their many variants
such as fast adversarial training methods (Shafahi et al., 2019; Zhang et al., 2019a; Wong et al.,
2020; Andriushchenko & Flammarion, 2020), semi-supervised robust training (Carmon et al., 2019;
Stanforth et al., 2019), adversarial transfer learning and certifiably robust training (Wong & Kolter,
2017; Dvijotham et al., 2018). Moreover, recent works (Hendrycks et al., 2019; Chen et al., 2020a;
Shafahi et al., 2020; Chan et al., 2020; Utrera et al., 2020; Salman et al., 2020) studied the transfer-
ability of robustness in in the context of transfer learning and representation learning. However, the
aforementioned standard robust training methods are not directly applicable to MAML in few-shot
learning considering MAML’s bi-leveled optimization nature.
A few recent works studied the problem of adversarial training in the context of MAML (Goldblum
et al., 2019; Yin et al., 2018). Yin et al. (2018) considered the robust training in both fine-tuning and
meta-update steps, which is unavoidably computationally expensive and difficult in optimization.
The most relevant work to ours is (Goldblum et al., 2019), which proposed adversarial querying
(AQ) by integrating adversarial training with MAML. Similar to ours, AQ attempted to robustify
meta-update only to gain sufficient robustness. However, it lacks explanation for the rationale behind
that. We will show that AQ can also be regarded as a special case of our proposed robustness-
promoting MAML framework. Most important, we make a more in-depth study with novelties
summarized in Contributions.
2
Published as a conference paper at ICLR 2021
Another line of research relevant to ours is efficient MAML, e.g., (Raghu et al., 2019; Song et al.,
2019; Su et al., 2019), where the goal is to improve the computation efficiency and/or the generaliza-
tion of MAML. In (Song et al., 2019), gradient-free optimization was leveraged to alleviate the need
of second-order derivative information during meta-update. In (Raghu et al., 2019), MAML was
simplified by removing the fine-tuning step over the representation block of a meta-model. It was
shown that such a simplification is surprisingly effective without losing generalization-ability. In
(Su et al., 2019), a self-supervised representation learning task was augmented to the meta-updating
objective and resulted in a meta-model with improved generalization. Although useful insights were
gained from MAML in the aforementioned works, none of them took adversarial robustness into
account.
2 Preliminaries and Problem Statement
In this section, we first review model-agnostic meta learning (MAML) (Finn et al., 2017) and adver-
sarial training (Madry et al., 2017), respectively. We then motivate the setup of robustness-promoting
MAML and demonstrate its challenges in design when integrating MAML with robust regulariza-
tion.
MAML MAML attempts to learn an initialization of model parameters (namely, a meta-model)
so that a new few-shot task can quickly and easily be tackled by fine-tuning this meta-model over a
small amount of labeled data. The characteristic signature of MAML is its bi-level learning proce-
dure, where the fine-tuning stage forms a task-specific inner loop while the meta-model is updated
at the outer loop by minimizing the validation error of fine-tuned models over cumulative tasks.
Formally, consider N few-shot learning tasks {Ti}iN=1, each of which has a fine-tuning data set Di
and a validation set Di0 , where Di is used in the fine-tuning stage and Di0 is used in the meta-update
stage. Here the superscript (0) is preserved to indicate operations/parameters at the meta-upate stage.
MAML is then formulated as the following bi-level optimization problem (Finn et al., 2017):
minimize N PN=ι 'i(w0； Di)
w
subject to wi0 = arg minwi `i (wi; Di, w), ∀i ∈ [N]
(1)
where W denotes the meta-model to be designed, Wi is the Ti-specific fine-tuned model, 'i(wi; Di)
represents the validation error using the fine-tuned model, `i (wi; Di, w) denotes the training error
when fine-tuning the task-specific model parameters Wi using the task-agnostic initialization W,
and for ease of notation, [K] represents the integer set {1, 2, . . . , K}. In (1), the objective function
and the constraint correspond to the meta-update stage and fine-tuning stage, respectively. The bi-
level optimization problem is challenging because each constraint calls an inner optimization oracle,
which is typically instantiated into a K-step gradient descent (GD) based solver:
w(k) = w(k-1) - αVwi'i(w(k-1); Di, w), k ∈ [K], With W(O) = w.
We note that even with the above simplified fine-tuning step, updating the meta-model W still re-
quires the second-order derivatives of the objective function of (1) with respect to (w.r.t.) w.
Adversarial training The min-max optimization based adversarial training (AT) is known as one
of the most powerful defense methods to obtain a robust model against adversarial attacks (Madry
et al., 2017). We summarize AT and its variants through the following robustness-regularized opti-
mization problem:
miniwmize λE(x,y)∈D [`(w; x, y)] + E(x,y)∈D[maximize g(w; x + δ, y)],
|
^{l^^^-
R(w; D)
(2)
}
where `(w; x, y) denotes the prediction loss evaluated at the point x with label y, λ ≥ 0 is a
regularization parameter, δ denotes the input perturbation variable within the '∞-norm ball of radius
, g represents the robust loss evaluated at the model w at the perturbed example x + δ given the
true label y, and for ease of notation, let R(w; D) denote the robust regularization function for
model w under the data set D. In the rest of the paper, we consider two specifications of R: (a)
AT regularization (Madry et al., 2017), where we set g = ` and λ = 0; (b) TRADES regularization
(Zhang et al., 2019b), where we define g as the cross-entropy between the distribution of prediction
probabilities at the perturbed example (x + δ) and that at the original sample x.
3
Published as a conference paper at ICLR 2021
Robustness-promoting MAML Integrating MAML with AT is a natural solution to enhance ad-
versarial robustness of a meta-model in few-shot learning. However, this seemingly simple scheme
is in fact far from trivial, and there exist three critical roadblocks as elaborated below.
First, it remains elusive at which stage (fine-tuning or meta-update) robustness can most effectively
be gained for MAML. Based on (1) and (2), we can cast this problem as a unified optimization
problem that augments the MAML loss with the robust regularization under two degrees of freedom
characterized by two hyper-parameters γout ≥ 0 and γin ≥ 0:
minimize Nn PN=ι['i(WO； Di) + YoUtRi(W0； Di)]
subject to Wi = argminwi['i(wi； Di, w) + YinRi(Wi； Di)], ∀i ∈ [N].
(3)
Here Ri denotes the task-specific robustness regularizer, and the choice of (γin, γout) determines the
specific scenario of robustness-promoting MAML. Clearly, the direct application is to set γin > 0
and γout > 0, that is, both fine-tuning and meta-update steps would be carried out using robust
training, which calls additional loops to generate adversarial examples. Thus, this would make
computation most intensive. Spurred by that, we ask: Is it possible to achieve a robust meta-model by
incorporating robust regularization into only either meta-update or fine-tuning step (corresponding
to γin = 0 or γout = 0)?
Second, both MAML in (1) and AT in (2) are challenging bi-level optimization problems which need
to call inner optimization routines for fine-tuning and attack generation, respectively. Thus, we ask
whether or not the computationally-light alternatives of inner solvers, e.g., partial fine-tuning (Raghu
et al., 2019) and fast attack generation (Wong et al., 2020), can promise adversarial robustness in
few-shot learning.
Third, it has been shown that adversarial robustness can benefit from semi-supervised learning by
leveraging (unlabeled) data augmentation (Carmon et al., 2019; Stanforth et al., 2019). Spurred
by that, we further ask: Is it possible to generalize robustness-promoting MAML to the setup of
semi-supervised learning for improved accuracy-robustness tradeoff?
3 When to Incorporate Robust Regularization in MAML?
In this section, We evaluate at which stage adver-
sarial robustness can be gained during meta-training.
We will provide insights and step-by-step investiga-
tions to show when to incorporate robust training in
MAML and why it works. Based on (3), we focus
on two robustness-promoting meta-training protocols.
(a) R-MAMLboth, where robustness regularization ap-
plied to both fine-tuning and meta-update steps with
Yin, YoUt > 0; (b) R-MAMLout, where robust regular-
ization applied to meta-update only, i.e., Yin = 0 and
YoUt > 0. COmParedtOR-MAMLboth, R-MAMLoUt
is more user-friendly since it allows the use of stan-
dard fine-tuning over the learnt robust meta-model
when tackling unseen few-shot test tasks (known as
meta-testing). In what follows, we will show that even
if R-MAMLoUt does not use robust regularization in
fine-tuning, it is sufficient to warrant the transferability
of meta-model’s robustness to downstream fine-tuning
tasks.
Attack power e
Figure 1: RA of meta-models trained by stan-
dard MAML, R-MAMLboth and R-MAMLout
versus PGD attacks of different perturbation
sizes during meta-testing. Results show that ro-
bustness regularized meta-update with standard
fine-tuning (namely, R-MAMLout) has already
been effective in promotion of robustness.
All you need is robust meta-update during meta-
training To study this claim, we solve problem (3) using R-MAMLboth and R-MAMLoUt re-
spectively in the 5-way 1-shot learning setup, where 1 data sample at each of 5 randomly selected
MiniImagenet classes (Ravi & Larochelle, 2016) constructs a learning task. Throughout this sec-
tion, we specify Ri in (3) as the AT regularization, which calls a 10-step projected gradient descent
(PGD) attack generation method with = 2/255 in its inner maximization subroutine given by (2).
We refer readers to Section 6 for more implementation details.
4
Published as a conference paper at ICLR 2021
We find that the meta-model acquired by R-MAMLout yields nearly the same robust accuracy (RA)
as R-MAMLboth against various PGD attacks generated at the testing phase using different pertur-
bation sizes = {0, 2, . . . , 10}/255 as shown in Figure 1. Unless specified otherwise, we evaluate
the performance of the meta-learning schemes over 2400 random unseen 5-way 1-shot test tasks. We
also note that RA under = 0 becomes the standard accuracy (SA) evaluated using benign (unper-
turbed) test examples. It is clear from Figure 1 that both R-MAMLout and R-MAMLboth can yield
significantly better RA than MAML with slightly worse SA. It is also expected that RA decreases
as the attack power increases.
Spurred by experiment results in Figure 1, we hypothesize
that the promotion of robustness in meta-update alone (i.e.
R-MAMLout ) is already sufficient to offer robust represen-
tation, over which fine-tuned models can preserve robust-
ness to downstream tasks. In what follows, we justify the
above hypothesis from two perspectives: (i) explanation of
learned neuron’s representation and (ii) resilience of learnt ro-
bust meta-model to different fine-tuning schemes at the meta-
testing phase.
(i)	Learned signature of neuron’s representation It is re-
cently shown in (Engstrom et al., 2019) that a robust model
exhibits perceptually-aligned neuron activation maps, which
are not present if the model lacks adversarial robustness. To
uncover such a signature of robustness, a feature inversion
technique (Engstrom et al., 2019) is applied to finding an in-
Figure 2: Visualization of a randomly
selected neuron’s inverted input attri-
bution maps (IAMs) under different
meta-models. The first row shows the
seed images. The second-fourth rows
show IAMs corresponding to models
trained by MAML, R-MAMLboth ,
and R-MAMLout , respectively. Ex-
cept MAML, R-MAMLboth and
R-MAMLout all catch high-level
features from the data.
verted input attribution map (IAM) that maximizes neuron’s
activation. Based on that, we examine if R-MAMLboth and
R-MAMLout can similarly generate explainable inverted im-
ages from the learned neuron’s representation. We refer read-
ers to Appendix 2 for more details on feature inversion from
neuron’s activation.
In our experiment, we indeed find that both R-MAMLboth
and R-MAMLout yield similar IAMs inverted from neuron’s
activation at different input examples, as plotted in Figure 2.
More intriguingly, the learnt IAMs characterize the contour
of objects existed in input images, and accompanied by the
learnt high-level features, e.g., colors. In contrast, the IAMs of MAML lack such an interpretability.
The observations from the interpretability of neurons’ representation justify why R-MAMLout is as
effective as R-MAMLboth and why MAML does not preserve robustness.
(ii)	Robust meta-update provides robustness adap-
tation without additional adversarial fine-tuning at
meta-testing Meta-testing includes only the fine-tuning
stage. Therefore, we need to explore if standard fine-
tuning is enough to maintain the robustness. Suppose that
R-MAMLout is adopted as the meta-training method to
solve problem (3), we then ask if robustness-regularized
meta-testing strategy can improve the robustness of fine-
tuned model at downstream tasks. Surprisingly, we find
Table 1: Comparison of different strate-
gies in meta-testing on R-MAMLout : (a)
standard fine-tuning (S-FT), (b) adversarial
fine-tuning (A-FT).
	S-FT	A-FT
~^SA	40.9%	39.6%
RA	22.9%	23.5%
that making an additional effort to adversarially fine-tune the meta-model (trained by R-MAMLout)
during testing does not provide an obvious robustness improvement over the standard fine-tuning
scheme during testing (Table 1). This consistently implies that robust meta-update (R-MAMLout) is
sufficient to render intrinsic robustness in its learnt meta-model regardless of fine-tuning strategies
used at meta-testing. Figure S1 in Appendix 3 provides evidence that the visualization difference is
small between before standard fine-tuning and after standard fine-tuning.
Adversarial querying (AQ) (Goldblum et al., 2019): A special case of R-MAMLout The recent
work (Goldblum et al., 2019) developed AQ to improve adversarial robustness in few-shot learning.
5
Published as a conference paper at ICLR 2021
AQ can be regarded as a special case of R-MAMLout with γin = 0 but setting γout = ∞ in (3).
That is, the meta-update is overridden by the AT regularization. We find that AQ yields about 2%
RA improvement over R-MAMLout, which uses γout = 0.2 in (3). However, AQ leads to 11%
degradation in SA, and thus makes a much poorer robustness-accuracy tradeoff than our proposed
R-MAMLout . We refer readers to Table 2 for comparison of the proposed R-MAMLout with other
training baselines. Most importantly, different from (Goldblum et al., 2019), we provide insights on
why R-MAMLout is effective in promoting adversarial robustness from meta-update to fine-tuning.
4 Computationally-Efficient Robustness-Regularized MAML
In this section, we study if the proposed R-MAMLout can further be improved to ease of optimiza-
tion given the two computation difficulties in (3): (a) bi-leveled meta-learning, and (b) the need
of inner maximization to find the worst-case robust regularization. To tackle either problem alone,
there have been efficient solution methods proposed recently. In (Raghu et al., 2019), an almost-
no-inner-loop (ANIL) fine-tuning strategy was proposed, where fine-tuning is only applied to the
task-specific classification head following a frozen representation network inherited from the meta-
model. Moreover, in (Wong et al., 2020), a fast gradient sign method (FGSM) based attack generator
was leveraged to improve the efficiency of AT without losing its adversarial robustness. Motivated
by (Raghu et al., 2019; Wong et al., 2020), we ask if integrating R-MAMLout with ANIL and/or
FGSM can improve the training efficiency but preserves the robustness and generalization-ability of
a meta-model learnt from R-MAMLout .
R-MAMLout meets ANIL and FGSM We decompose the meta-model w = [wr , wc] into two
parts: representation encoding network wr and classification head wc . In R-MAMLout , namely, (3)
with γin = 0, ANIL suggests to only fine-tune wc over a specific task Ti . This leads to
wC,i = arg min '(Wc,i, w Di, w), With w" = W	(ANIL)
wc,i
In ANIL, the initialized representation netWork wr keeps intact during task-specific fine-tuning,
Which thus saves the computation cost. Furthermore, if FGSM is used in R-MAMLout , then the
robustness regularizer R defined in (2) reduces to
R(w； D) = E(χ,y)∈D[g(w； X + δ*(x),y)],	δ*(x) = δo + eVχg(w; x,y),	(FGSM)
Where δ0 is an initial point randomly draWn from a uniform distribution over the interval [-, ].
Note that in the original implementation of robust regularization R, a multi-step projected gradi-
ent ascent (PGA) is typically used to optimize the sample-Wise adversarial perturbation δ(w). By
contrast, FGSM only uses one-step PGA in attack generation and thus improves the computation
efficiency.
In Table 2, We study tWo
computationally-light alternatives of Table 2: Performance of computation-efficient alternatives of
R-MAMLout , R-MAMLout With ANIL R-MAMLout in SA, RA and computation time per epoch (in
(R-MAMLOUt-ANIL) and R-MAMLOUt minutes).______________________________________________________
With FGSM (R-MAMLOUt-FGSM). Compared to R-MAMLOUt , We find that although R-MAMLOUt -FGSM takes less computation time, it yields even better RA With slightly Worse SA. By contrast, R-MAMLOUt -ANIL yields the		-SA-	-RA-	Time
	MAML	43.6%	3.17%	42min
	AQ(GoldblUmetaL,2019)	29.6%	24.9%	52min
	R-MAMLOUt	40.9%	22.9%	54min
	R-MAMLOUt -ANIL	37.46%	22.7%	36min
	R-MAMLOUt-FGSM	一	40.82%	23.04%	44min
least computation cost but the Worst SA
and RA. For comparison, We also present the performance of the adversarial meta-learning baseline
AQ (Goldblum et al., 2019). As We can see, AQ promotes the adversarial robustness at the cost of a
significant SA drop, e.g., 7.56% Worse than R-MAMLOUt-ANIL. Overall, the application of FGSM
to R-MAMLOUt provides the most graceful tradeoff betWeen the computation cost and the standard
and robust accuracies. In the rest of the paper, unless specified otherWise We Will use FGSM in
R-MAMLOUt.
6
Published as a conference paper at ICLR 2021
5 Semi-Supervised Robustness-Promoting MAML
Given our previous solutions to when (Sec. 3) and how (Sec. 4) a robust regularization could ef-
fectively be promoted in few-shot learning, we next ask: Is it possible to further improve our pro-
posal R-MAMLout by leveraging unlabeled data? Such a question is motivated from two aspects.
First, the use of unlabeled data augmentation could be a key momentum to improve the robustness-
accuracy tradeoff (Carmon et al., 2019; Stanforth et al., 2019). Second, the recent success in self-
supervised contrastive representation learning (Chen et al., 2020b; He et al., 2020) demonstrates
the power of multi-view (unlabeled) data augmentation to acquire discriminative and generalizable
visual representations, which can guide down-stream supervised learning. In what follows, we
propose an extension of R-MAMLout applicable to semi-supervised learning with unlabeled data
augmentation.
R-MAMLout with TRADES regularization. We recall from (2) that the robust regularization
R can also be specified by TRADES (Zhang et al., 2019b), which relies only on the prediction
logits of benign and adversarial examples (rather than the training label), and thus lends itself to
the application of unlabeled data. Spurred by that, we propose R-MAMLout -TRADES, which is a
variant of R-MAMLout using the unlabeled data augmented TRADES regularization. To perform
data augmentation in experiments, we follow (Carmon et al., 2019) to mine additional (unlabeled)
data with the same amount of MiniImagenet data from the original ImageNet data set. For clarity, we
call R-MAMLout using TRADES or AT regularization (but without unlabeled data augmentation)
R-MAMLout(TRADES) or R-MAMLout(AT).
We find that with the help of unlabeled data,
R-MAMLout-TRADES improves the accuracy-
robustness tradeoff over its supervised counterpart
R-MAMLout using either AT or TRADES regu-
larization (Figure 3). Compared to R-MAMLout,
R-MAMLout-TRADES yields consistently better RA
against different attack strength e ∈ {2,..., 10}∕255
during testing. Interestingly, the improvement be-
comes more significant as e increases. As e = 0,
RA is equivalent to SA, and We observe that the
superior performance of R-MAMLout -TRADES
in RA bears a slight degradation in SA compared
to R-MAMLout(TRADES) and R-MAMLout(AT),
which indicates the robustness-accuracy tradeoff.
Figure S2 in Appendix 5 provides an additional evi-
dence that R-MAMLout -TRADES has the ability to
defend stronger attacks than R-MAMLout , and proper
unlabeled data augmentation can further improve the
accuracy-robustness tradeoff in MAML.
PGD attacks at different values of perturba-
tion strength . Here the robust models are
trained by different variants of R-MAMLout ,
including R-MAMLout -TRADES (with unla-
beled data augmentation), R-MAMLout us-
ing AT regularization but no data augmenta-
tion (R-MAMLout (AT)), and R-MAMLout us-
ing TRADES regularization but no data aug-
mentation (R-MAMLout (TRADES)).
R-MAMLout with contrastive learning (CL). To
improve adversarial robustness, many works, e.g.,
(Pang et al., 2019; Sankaranarayanan et al., 2017), also
suggest that it is important to encourage robust seman-
tic features that locally cluster according to class, namely, ensuring that features of samples in the
same class will lie close to each other and away from those of different classes. The above sugges-
tion aligns with the goals of contrastive learning (CL) (Chen et al., 2020b; Wang & Isola, 2020),
which promotes (a) alignment (closeness) of features from positive data pairs, and (b) uniformity of
feature distribution. Thus, we develop R-MAMLout -CL by integrating R-MAMLout with CL.
Prior to defining R-MAMLout-CL, we first introduce CL and refer readers to (Chen et al., 2020b)
for details. Given a data sample x, CL utilizes its positive counterpart x+ given by a certain data
transformation t, e.g., cropping and resizing, cut-out, and rotation, x+ = t(x). The data pair
(x, t(x0)) is then positive ifx = x0, and negative otherwise. The contrastive loss is defined by
'CL (Wc； p+) = E(x,x+)〜p+
r(x; Wc)T r (X+; Wc)/t
—log -------,-------------------------------F-----、丁 / _---
er(X：；Wc)T r(x+;Wc)/t + p	er(x；Wc)T r(X-;Wc
,	e	x 乙 X-〜p,(x,x-)∕p+ Le
7
Published as a conference paper at ICLR 2021
where X 〜P denotes the data distribution, p+(∙, ∙) is the distribution of positive pairs, r(x; Wc)
is the encoded representation of x extracted from the representation network wc , and τ > 0 is a
temperature parameter. The contrastive loss minimizes the distance of a positive pair among many
negative pairs, namely, learns network representation with instance-wise discriminative power.
According to CL, we then augment the data used to train R-MAMLout with their transformed coun-
terparts. In addition, the adversarial examples generated during robust regularization can also be
used as additional views of the original data, which in turn advance CL. Formally, we modify
R-MAMLout , given by (3) with γin = 0, as
minimize N P3 ['i (WO ； Di )+ YoUtRi(W0 ； Di) + Ycl'cl(Wc ,i； P+ ∪ padv)]	⑷
subject to	W0i	= arg minw	`i	(Wi	;	Di	, W),	∀i	∈ [N],
where γCL > 0 is a regularization parameter associated with the contrastive loss, Pi+ ∪ Piadv repre-
sents the distribution of positive data pairs constructed by the standard and adversarial views of D0,
and W0c,i denotes the representation block of the model W0i .
In Table 3, we compare the SA/RA
performance
with that of
variants of
the versions
of	R-MAMLout -CL
previously-suggested 3
R-MAMLout	including
R-MAMLout (AT)	and
Table 3: SA/RA performance of R-MAMLout -CL versus
other variants of proposed R-MAMLout and baselines.
—	-SA-	-RA-
MAML	43.6%	3.17%
AQ (GOldblUm et al., 2019)	29.6%	24.9%
R-MAMLout (AT)(OUrS)	40.82%	23.04%
R-MAMLoUt(TRADES)(oUrS)	39.06%	23.56%
R-MAMLout-TRADES (oUTST~	37.1%	25.51%
R-MAMLout -CL (ours)	38.60%	26.81%
R-MAMLout (TRADES) without using
unlabeled data, and the version with
unlabeled data R-MAMLout -TRADES,
as well as 2 baseline methods including
standard MAML and adversarial querying
(AQ) in few-shot learning (Goldblum
et al., 2019). Note that we specify Ri in
(4) as TRADES regularization for R-MAMLout -CL. We find that R-MAMLout -CL yields the best
RA among all meta-learning methods, and improves SA over R-MAMLout-TRADES. In particular,
the comparison with AQ shows that R-MAMLout-CL leads to 9% improvement in SA and 1.9%
improvement in RA.
6 Additional Experiments
Key facts of our implementation.
In the previous analysis, we con-
sider 1-shot 5-way image classi-
fication tasks over MiniImageNet
(Vinyals et al., 2016). And we use a
four-layer convolutional neural net-
work for few-shot learning (FSL).
By default, we set the training attack
strength = 2, γCL = 0.1, and set
Table 4: Summary of baseline performance in SA and TA.
—	-SA-	-RA^^
MAML (FSL)	43.6%	3.17%
AQ (FSL)(GOldblUm et al., 2019)	一	29.6%	24.9%
Supervised standard training (non-FSL)	29.74%	3.51%
Supervised AT (non-FSL)	28.22%	19.02%
γoUt = 5 (TRADES), γoUt = 0.2 (AT) viaa grid search. During meta-testing, a 10-step PGD attack
with attack strength = 2 is used to evaluate RA of the learnt meta-model over 2400 few-shot test
tasks. We provide experiment details in Appendix 4.
Summary of baselines. We remark that in addition to MAML and AQ baselines, we also consider
the other two baseline methods, supervised standard training over the entire dataset (non-FSL set-
ting), and supervised AT over the entire dataset (non-FSL setting); see a summary in Table 4. The
additional baselines demonstrate that robust adaptation in FSL is non-trivial as neither the supervised
full AT or the full standard training can achieve satisfactory SA and RA.
Experiments on Additional model architecture, datasets and FSL setups. In Table S1 of Ap-
pendix 5, we provide additional experiments using ResNet18. In particular, R-MAMLoUt-CL leads
to 13.94% SA improvement and 1.42% RA improvement over AQ. We also test our methods on
CIFAR-FS (Bertinetto et al., 2018) and Omniglot (Lake et al., 2015), and provide the results in Ta-
ble5 and Figure S3, respectively (more details can be viewed in Appendix 6 and Appendix 7). The
8
Published as a conference paper at ICLR 2021
Table 5: SA/RA performance of our proposed methods on CIFAR-FS (Bertinetto et al., 2018).
	I-Shot 5-Way-		5-Shot 5-Way	
	SA	RA	SA	RA
MAML	51.07%	0.235%	67.2%	0.225%
AQ(Goldblum et al.,2019)	31.25%	26.34%	52.32%	33.96%
R-MAMLout(AT)(ours)	39.76%	26.15%	57.18%	32.62%
R-MAMLout(TRADES)(ours)	40.23%	27.45%	57.46%	34.72%
R-MAMLout-TRADES (ours)	40.59%	28.06%	57.62%	34.76%
R-MAMLout-CL (OUrs)	41.25%	29.33%	57.95%	35.30%
results show that our methods perform well on various datasets and outperform the baseline meth-
ods. On CIFAR-FS, we study 1-Shot 5-Way and 5-Shot 5-Way settings. As shown in Table 5, the
use of unlabeled data augmentation (R-MAMLout-CL) on CIFAR-FS can provide 10% (or 5.6%)
SA improvement and 3% (or 1.3%) RA improvement over AQ under the 1-Shot 5-Way (or 5-Shot 5-
Way) setting. Furthermore, we conduct experiments in other FSL setups. On Omniglot, we compare
R-MAMLout(TRADES) to AQ (Goldblum et al., 2019) in the 1-shot (5, 10, 15, 20)-Way settings.
Figure S3 shows that R-MAMLout (TRADES) can always obtain better performance than AQ when
the number of classes in each task varies.
7 Conclusion
In this paper, we study the problem of adversarial robustness in MAML. Beyond directly integrating
MAML with robust training, we show and explain when a robust regularization should be promoted
in MAML. We find that robustifying the meta-update stage via fast attack generation method is suffi-
cient to achieve fast robustness adaptation without losing generalization and computation efficiency
in general. To further improve our proposal, we for the first time study how unlabeled data help ro-
bust MAML. In particular, we propose using contrastive representation learning to acquire improved
generalization and robustness simultaneously. Extensive experiments are provided to demonstrate
the effectiveness of our approach and justify our insights on the adversarial robustness of MAML.
In the future, we plan to establish the convergence rate analysis of robustness-aware MAML by
leveraging bi-level and min-max optimization theories.
Acknowledgement
This work was supported by the Rensselaer-IBM AI Research Collaboration (http://airc.
rpi.edu), part of the IBM AI Horizons Network (http://ibm.biz/AIHorizons).
References
Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial
training. arXiv preprint arXiv:2007.02617, 2020.
Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. In International Conference on Learning Representations, 2018.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. arXiv preprint arXiv:1902.06705, 2019.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11190-11201,2019.
Alvin Chan, Yi Tay, and Yew-Soon Ong. What it thinks is important is important: Robustness
transfers through input gradients. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 332-341, 2020.
9
Published as a conference paper at ICLR 2021
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to fine-tuning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 699-708, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020b.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers. arXiv preprint arXiv:1805.10265, 2018.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. arXiv preprint arXiv:1703.03400, 2017.
Micah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A meta-
learning approach. arXiv, pp. arXiv-1910, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Han Xu Yao Ma Hao-Chen, Liu Debayan Deb, Hui Liu Ji-Liang Tang Anil, and K Jain. Adversarial
attacks and defenses in images, graphs and text: A review. International Journal of Automation
and Computing, 17(2):151-178, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning, pp. 2712-2721, 2019.
Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. Natural language
to structured query generation via meta-learning. arXiv preprint arXiv:1803.02400, 2018.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Gabriel Maicas, Andrew P Bradley, Jacinto C Nascimento, Ian Reid, and Gustavo Carneiro. Training
medical image analysis systems like radiologists. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, pp. 546-554. Springer, 2018.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. Proceedings of machine learning research,
70:2554, 2017.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Joseph D Novak and D Bob Gowin. Learning how to learn. cambridge University press, 1984.
10
Published as a conference paper at ICLR 2021
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via
promoting ensemble diversity. arXiv preprint arXiv:1901.08846, 2019.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adver-
sarially robust imagenet models transfer better? arXiv preprint arXiv:2007.08489, 2020.
Swami Sankaranarayanan, Arpit Jain, Rama Chellappa, and Ser Nam Lim. Regularizing deep net-
works using efficient layerwise adversarial training. arXiv preprint arXiv:1705.07819, 2017.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learn-
ing,pp.1842-1850, 2016.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In
Advances in Neural Information Processing Systems, pp. 3353-3364, 2019.
Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom
Goldstein. Adversarially robust transfer learning. In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/forum?id=ryebG04YvB.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pp. 4077-4087, 2017.
Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao
Tang. Es-maml: Simple hessian-free meta learning. arXiv preprint arXiv:1910.01215, 2019.
Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al. Are labels required for improving adver-
sarial robustness? arXiv preprint arXiv:1905.13725, 2019.
Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan. When does self-supervision improve few-
shot learning? arXiv preprint arXiv:1910.03560, 2019.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Francisco Utrera, Evan Kravitz, N Benjamin Erichson, Rajiv Khanna, and Michael W Mahoney.
Adversarially-trained deep nets transfer better. arXiv preprint arXiv:2007.05869, 2020.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong, and Wenjun Zeng. Tracking by instance
detection: A meta-learning approach. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6288-6297, 2020.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. arXiv preprint arXiv:2005.10242, 2020.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BJx040EFvH.
Han Xu, Yaxin Li, Xiaorui Liu, Hui Liu, and Jiliang Tang. Yet meta learning can adapt fast, it can
also break easily. arXiv preprint arXiv:2009.01672, 2020.
11
Published as a conference paper at ICLR 2021
Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin.
Topology attack and defense for graph neural networks: An optimization perspective. In Interna-
tional Joint Conference on Artificial Intelligence (IJCAI), 2019a.
Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi
Wang, and Xue Lin. Structured adversarial attack: Towards general implementation and better
interpretability. In International Conference on Learning Representations, 2019b.
Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning, 2018.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. arXiv preprint arXiv:1905.00877,
2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. International Conference on
Machine Learning, 2019b.
12
Published as a conference paper at ICLR 2021
Supplementary Material
1 Framework of R-MAMLout
Algorithm S1 shows the framework of R-MAMLout . The initial inputs include model weights w,
distribution of the training tasks p(T), and the step sizes α, β1, β2, which correspond to fine-tuning,
clean meta-update, adversarial meta-update. Each batch contains multiple tasks that are sampled
from the p(T). K is the number of gradient updates in fine-tuning. The adapted parameter wi(K)
is used to generate adversarial validation data Di0 from the clean validation data Di0 and to compute
(K)
the loss value Ri(wi ; Di0). The attack generator can be selected from Projected Gradient Descent
(Madry et al., 2017), Fast Gradient Sign Method (Goodfellow et al., 2014), etc. Here is used to
control the attack strength in the training.
Algorithm S1 R-MAMLout
Input: The initialization weights w; Distribution over tasks p(T); Step size parameters α, β1, β2.
1	while not done do
2	Sample batch of tasks Ti 〜p(T) and separate data in Ti into (Di, Di)
3	for each Ti do
4	wi(0) := w
5	for k = 1, 2,…，K do
6	w(k) = w(k-1) — αVwi 'i(w(kT) ； Di, W)
7	end for
8	Using attack generator to generate adversarial validation data Di0 by maximizing adversar-
ial loss Ri(W(K); Di) with the ConstraintkD0 - Dik∞ ≤ E
9	end for
10	W := W - βivw PTi 〜P(T) 'i(W(K)； Di, w) - β2Youtvw PTi 〜P(T) Ri (W(K)； Di )
11	end while
12	Return: W
2	Details of Learned S ignature of Neuron’ s Activation
By maximizing a single coordinate of the neuron activation vector r (the output before the fully-
connected layer) with a perturbation in the input, the perturbation will show different behaviors
between a robust model and a standard model (Engstrom et al., 2019). To be more specific, the
feature pattern is revealed in the input under a robust model, while a standard model does not have
such behavior. The optimization problem can be mathematically written in the following form
maximize
δ
subject to
ri(x+ δ)
-xj ≤ δj ≤ 255 - xj ,
(S1)
where ri denotes the i-th coordinate of neuron activation vector. δ is the perturbation in the input.
xj is the j -th pixel of the image vector x.
3	Visualization of IAMs B efore and After Fine-Tuning in
Meta-Testing
Once obtain a model using R-MAMLout , we can test the impact of the standard fine-tuning on its
robustness. Figure S1 shows a randomly selected neuron’s inverted input attribution maps (IAMs)
before standard fine-tuning and after standard fine-tuning in the meta-testing phase. The second
row shows IAMs of the model before fine-tuning. The third row shows IAMs of the model after
fine-tuning. One can find that the difference is small between the IAMs before fine-tuning and after
fine-tuning, suggests that robust meta-update itself can provide the robustness adaptation without
additional adversarial training.
13
Published as a conference paper at ICLR 2021
Figure S1:	Visualization of a randomly selected neuron’s inverted input attribution maps (IAMs) before fine-
tuning and after fine-tuning in meta-testing. The model is obtained by R-MAMLout . The second row shows
IAMs of the model before fine-tuning. The third row shows IAMs of the model after fine-tuning. One can
find that the difference between the IAMs before fine-tuning and after fine-tuning is small, suggests that robust
meta-update itself can provide the robustness adaptation without additional adversarial training.
4 Details of Experiments
To test the effectiveness of our methods, we employ the MiniImageNet dataset Vinyals et al. (2016),
which is the benchmark for few-shot learning. MiniImageNet contains 100 classes with 600 samples
in each class. We use the training set with 64 classes and test set with 20 classes. In our experiments,
we downsize each image to 84 × 84 × 3.
we consider the 1-shot 5-way image classification task, i.e., the inner-gradient update (fine-tuning)
is implemented using five classes and one fine-tuning image for each class in one single task. In
meta-training, Each batch contains four tasks. We set the number of gradient update steps K = 5
in meta-training. For the meta-update, we use 15 validation images for each class. We set the
gradient step size in the fine-tuning as α = 0.01, and the gradient step sizes in the meta-update as
β1 = 0.001, β2 = 0.001 for clean validation data and adversarial validation data, respectively.
50
(卓)AoElnOUE Isnqox
40
2	4	6
Attack power e
-冬 R-MALMLouJe = 2)
-0-R-MAMLout-TRADES (e = 2)
.E R-MALMLout (e = 4)
-θ-R-MAMLout-TRADES (e = 4)
10
Figure S2:	RA versus (testing-phase) PGD attacks at different values of perturbation strength . Here the
robust models are trained by R-MAMLout-TRADES and R-MAMLout . Each method trains two models under
the training attack strength of = 2, 4, respectively. Results show that R-MAMLout -TRADES has the ability
to defend stronger attacks than R-MAMLout .
14
Published as a conference paper at ICLR 2021
5	Additional Comparisons on MiniImageNet
Figure S2 shows robust accuracy (RA) performance of models trained using our methods. One can
see that R-MAMLout -TRADES has the ability to defend stronger attacks than R-MAMLout .
In Table S1, we compare the SA/RA performance of variants of R-MAMLout including
R-MAMLout (AT), the TRADES regularization with unlabeled data R-MAMLout -TRADES, the
version with contrastive learning R-MAMLout -CL. One can see that R-MAMLout-CL yields the
best SA and RA among all meta-learning methods.
Table S1:	SA/RA performance of different variants of proposed R-MAMLout under the 1-shot
5-way scenario on ResNet18.
—	-SA-	-RA-
MAML	43.1%	5.347%
AQ (GoldblUm et al., 2019)	30.04%	20.05%
R-MAMLout(AT) (ours)	38.94%	19.94%
R-MAMLout-TRADES (ours)	41.94%	20.19%
R-MAMLout-CL (ours)	43.98%	21.47%
6 Experiments on CIFAR-FS
We also test our proposed methods on CIFAR-FS (Bertinetto et al., 2018), which is an image classifi-
cation dataset containing 64 classes of training data and 20 classes of evaluation data. The compared
methods are the same as in Table 3. We keep the settings to be the same as in the test on MiniIma-
genet except we set = 8. To perform data augmentation in experiments, we mine 500 additional
unlabeled data for each training class from the STL-10 dataset (Coates et al., 2011).
Table S2 and Table S3 show the comparisons in 1-Shot 5-Way and 5-Shot 5-Way learning scenarios,
respectively. One can see that our methods outperform the baseline methods MAML and AQ (Gold-
blum et al., 2019). The results also indicate that semi-supervised learning (in terms of TRADES
and contrastive learning) can further boost the performance. In particular, as shown by Table S2 and
Table S3, R-MAMLout-CL leads to 10% SA improvement and 3% RA improvement compared to
AQ under the MAML 1-Shot 5-Way setting, and 5.6% SA improvement and 1.3% RA improvement
under the 5-Shot 5-Way setting.
Table S2:	SA/RA performance of our proposed methods on CIFAR-FS (Bertinetto et al., 2018)
(1-Shot 5-Way).
—	-SA-	-Ra-
MAML	51.07%	0.235%
AQ (GoldblUm et al., 2019)	31.25%	26.34%
R-MAMLoUt(AT) (ours)	39.76%	26.15%
R-MAMLoUt(TRADES) (ours)	40.23%	27.45%
R-MAMLoUt-TRADES (ours)	40.59%	28.06%
R-MAMLout-CL (ours)	41.25%	29.33%
7 Experiments on Omniglot
We then conduct experiments on Omniglot (Lake et al., 2015), which includes handwritten charac-
ters from 50 different alphabets. There are 1028 classes of training data and 423 classes of evaluation
15
Published as a conference paper at ICLR 2021
Table S3:	SA/RA performance of our proposed methods on CIFAR-FS (Bertinetto et al., 2018)
(5-Shot 5-Way).
—	-SA-	-RA-
MAML	67.2%	0.225%
AQ (GoldblUm et al., 2019)	52.32%	33.96%
R-MAMLout(AT) (ours)	57.18%	32.62%
R-MAMLout(TRADES) (ours)	57.46%	34.72%
R-MAMLout-TRADES SUrS)	57.62%	34.76%
R-MAMLout-CL (oUrS)	57.95%	35.30%
data. Due to the hardness of finding the unlabeled data with similar patterns, we only test our su-
pervised learning methods on Omniglot. We compare R-MAMLout (TRADES) to AQ (Goldblum
et al., 2019) in the 1-shot (5, 10, 15, 20)-Way settings. Figure. S3 shows the results of RA/SA under
= 10. The results show that R-MAMLout(TRADES) can obtain better performance than AQ.
95
-B.R-MALMLout(TRADES)
a-
90 -
BR-MALMLout(TRADES)
-⅜ AQ
65 -
60 I---------------1----------------1---------------
5	10	15	20
Number of classes in each task
9
185
⅛
8 80
P
⅛
p5
6
70
65 L
5
10
-S-
15
Number of classes in each task
■**«
20

(a) RA
(b) SA
Figure S3: Performance of R-MAMLout(TRADES) and AQ (Goldblum et al., 2019) on Omniglot
versus number of classes in each task (from 5 to 20 ways): (a) RA. (b) SA.
16