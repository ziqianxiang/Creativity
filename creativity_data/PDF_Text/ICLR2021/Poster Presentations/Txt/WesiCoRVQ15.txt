Published as a conference paper at ICLR 2021
WHEN OPTIMIZING f -DIVERGENCE IS ROBUST
with Label Noise
Jiaheng Wei and Yang Liu*
Department of Computer Science and Engineering
University of California, Santa Cruz
Santa Cruz, CA 95060, USA
{jiahengwei, yangliu}@ucsc.edu
Ab stract
We show when maximizing a properly defined f -divergence measure with re-
spect to a classifier’s predictions and the supervised labels is robust with label
noise. Leveraging its variational form, we derive a nice decoupling property
for a family of f -divergence measures when label noise presents, where the di-
vergence is shown to be a linear combination of the variational difference de-
fined on the clean distribution and a bias term introduced due to the noise. The
above derivation helps us analyze the robustness of different f -divergence func-
tions. With established robustness, this family of f -divergence functions arises
as useful metrics for the problem of learning with noisy labels, which do not
require the specification of the labels’ noise rate. When they are possibly not
robust, we propose fixes to make them so. In addition to the analytical results,
we present thorough experimental evidence. Our code is available at https:
//github.com/UCSC-REAL/Robust-f-divergence-measures.
1 Introduction
A machine learning system continuously observes noisy training annotations and it remains a chal-
lenge to perform robust training in such scenarios. Earlier and classical approaches rely on estima-
tion processes to understand the noise rate of the labels and then leverage this knowledge to perform
label correction (Patrini et al., 2017; Lukasik et al., 2020), or loss correction (Natarajan et al., 2013;
Liu & Tao, 2015; Patrini et al., 2017), or both, among many other more carefully designed ap-
proaches (please refer to our related work section for more detailed coverage). Recent works have
started to propose robust loss functions or metrics that do not require the above estimation (Charoen-
phakdee et al., 2019; Xu et al., 2019; Liu & Guo, 2020; Cheng et al., 2021). Clear advantages of
the latter approaches include their easiness in implementation, as well as their robustness to noisy
estimates of the parameters. This work mainly contributes to the second line of studies and aimed
to propose relevant loss functions and measures that are inherently robust with label noise.
We start with formulating the problem of maximizing an f -divergence defined between a classifier’s
prediction and the labels:
h* = argmaxDf (Ph×γ∣∣Qh×γ),
(1)
h
where in above Df is an f -divergence function, P and Q are the joint and product (marginal)
distribution of the classifier h’s predictions on a feature space X and label Y . Though optimizing
the f -divergence measure is in general not the same as finding the Bayes optimal classifiers, we show
these measures encourage a classifier that maximizes an extended definition of f -mutual information
between the classifier’s prediction and the true label distribution. We will also provide analysis for
when the maximizer of this f -divergence coincides with the Bayes optimal classifier.
Building on a careful treatment of its variational form, we then reveal a nice property that helps
establish the robustness of the f -divergence specified in Eqn. (1): the variational difference term
defined with noisy labels is an affine transformation of the clean variational difference, subject to
* Corresponding author. yangliu@ucsc.edu.
1
Published as a conference paper at ICLR 2021
an addition of a bias term. Using this result, we analyze under which conditions maximizing an f-
divergence measure would be robust to label noise. In particular, we demonstrate strong robustness
results for Total Variation divergence, identify conditions under which several other divergences,
including Jenson-Shannon divergence and Pearson X2 divergence, are robust. The resultant f-
divergence functions offer ways to learn with noisy labels, without estimating the noise parameters.
As mentioned above, this distinguishes our solutions from a major line of previous studies that would
require such estimates. When the f -divergence functions are possibly not robust with label noise,
our analysis also offers a new way to perform “loss correction". We’d like to emphasize that instead
of offering one method/loss/measure, our results effectively offer a family of functions that can be
used to perform this noisy training task. Our contributions summarize as follows:
•	We show a certain set of f -divergence measures that are robust with label noise (some under cer-
tain conditions). The corresponding f -divergence functions provide the community with robust
learning measures that do not require the knowledge of the noise rates.
•	When the f -divergence measures are possibly not robust with label noise, our analysis provides
ways to correct the f -divergence functions to offer robustness. This process would require the
estimation of the noise rates and our results contribute new ways to leverage existing estimation
techniques to make the training more robust.
•	We empirically verified the effectiveness of optimizing f -divergences when noisy la-
bels present. We opensource our solutions at https://github.com/UCSC-REAL/
Robust-f-divergence-measures.
1.1	Related works
The now most popular approach of dealing with label noise is to first estimate the noise transition
matrix and then use this knowledge to perform loss or sample correction (Scott et al., 2013; Natarajan
et al., 2013; Patrini et al., 2017; Lu et al., 2018; Han et al., 2018; Tanaka et al., 2018; Yao et al., 2020;
Zhu et al., 2021). In particular, the surrogate loss (Scott et al., 2013; Natarajan et al., 2013; Scott,
2015; Van Rooyen et al., 2015; Menon et al., 2015) uses the transition matrix to define unbiased
estimates of the true losses. Other works include (Sukhbaatar & Fergus, 2014; Xiao et al., 2015),
which consider building a neural network to facilitate the learning of noise rates or noise transition
matrix. Symmetric loss has been studied and conditions have been identified for when there is
no need to estimate noise rate (Manwani & Sastry, 2013; Ghosh et al., 2015; 2017; Van Rooyen
et al., 2015; Charoenphakdee et al., 2019). Nonetheless, it remains a challenge to develop training
approaches without requiring knowing the noise rates for more generic settings.
More recently, (Zhang & Sabuncu, 2018; Amid et al., 2019) proposed robust losses for neural net-
works. When noise rates are asymmetric (label class-dependent), (Xu et al., 2019) proposed an
information-theoretic loss that is also robust to asymmetric noise rates. There are also some trials on
modifying the regularization term to improve generalization ability with the existence of label noise
(Jenni & Favaro, 2018; Yi & Wu, 2019), and on providing complementary negative labels (Kim
et al., 2019). Peer loss (Liu & Guo, 2020) is a recently proposed loss function that does not require
knowing noise rates.
f -divergence is a popular information theoretical measure, and has been widely used and studied.
Most relevant to us, f -GAN was proposed in (Nowozin et al., 2016) to study f -divergence in training
generative neural samplers. To our best knowledge, ours is the first to study the robustness of f-
divergence measures in the context of improving the robustness of training with noisy labels.
2	LEARNING WITH NOISY LABELS USING f -DIVERGENCE
Our solution ties to the definition of f -divergence. The f -divergence between two distributions P
and Q with probability density function p and q being measures for Z ∈ Z 1 is defined as:
Df(PkQ)=	q(Z)f
(2)
Z
1We use Z instead of X as conventionally done for a good reason - we will be reserving X to explicitly
denote the features.
2
Published as a conference paper at ICLR 2021
f (∙) is a convex function such that f ⑴=0. Examples include KL-divergence when f (V) = V log V
and Total Variation (TV) divergence with f (V) = 2 |v - 1|. Other examples can be found in Table
1. Following from Fenchel’s convex duality, f -divergence admits the following variational form:
Df(PkQ) =	SUp	EZ〜P [g(Z)] — EZ〜Q [f*(g(Z))],
g i Z→dom(f *)
where f * is the Fenchel duality of the function f (∙), which is defined as f *(u) = supv∈R{uv -
f (v)}. We use dom(f *) to denote the domain of f *.
We consider the classification problem of learning a classifier h : X → Y that maps features X ∈ X
to labels Y ∈ Y := {1, 2, ..., K}, where in above X × Y denote the random variables for features
and labels. X × Y jointly draw from a distribution D. For a clear presentation, we will often focus
on presenting the binary classification setting Y = {-1, +1}, but most of our core results extend to
multi-class classification problems, and we shall provide corresponding justifications.
Instead of having access to sampled training data from X ×Y , we consider a setting with noisy labels
where the noisy label Y generates according to a transition matrix T defined between Y and the true
label Y. The (i,j) element of T is defined as Tij = P(Y = j|Y = i) where i,j ∈ {1,…,K}.
For the ease of presentation, when we present for the binary case, we adopt the following notation:
e+ := P(Y = -1|Y = +1), e- ：= P(Y = +1∣Y = -1) ,e+ + e- < 1. Suppose We have access
to a noisy training dataset {xn, yn}N=ι, where ijn generates according to Y.
2.1	LEARNING USING Df
We will start with presenting our idea of training a classifier using Df with the clean training data.
Then we will proceed to the case with noisy labels. For an arbitrary classifier h, let’s denote by
Ph×Y the joint distribution of h(X) and Y:
Joint distribution: Ph×Y := P(h(X) = y,Y = y0), y, y0 ∈ Y.
And we use Qh×Y to denote the product (marginal) distribution of h(X) and Y:
Product distribution: Qh×γ ：= P(h(X) = y) ∙ P(Y = y0), y,y0 ∈ Y.
When it is clear from context we will also shorthand the above two distributions as P and Q. We
formulate the problem of learning using f -divergence as follows: the goal of the learner is to find a
classifier h that maximizes the following divergence measure between P and Q:
Learning using Df： hf = argmaxDf(Ph×γ∣∣Qh×γ)	(3)
h
Effectively the goal is to find a classifier that maximizes the divergence between the joint dis-
tribution and the product distribution. Define a f -mutual information based on f -divergence:
Mf(h(X); Y) = Df (Ph×γ∣∣Qh×γ) , equivalently the maximization in Eqn. (3) tries to find the
classifier that maximizes the f-mutual information between a classifier,s output distribution and
the true label distribution. A notable example is when f(V) = V logV, the corresponding Df and
Mf become the famous KL divergence and the mutual information. It is important to note in gen-
eral maximizing (f-) mutual information between the classifier’s predictions and labels does not
promise the Bayes optimal classifier h* = argmaxh P(h(X) = Y). Nonetheless, maximizing it of-
ten returns a quality one. We provide further analysis in Section 2.2.
Variational representation As we mentioned earlier, f -divergence admits a variational form
which further allows us to focus on maximizing the following variational difference:
hf = argmax SUp Ez^F>h×γ [g(Z)] - EZ〜Qh×γ [f *(g(Z))],
hg
where we use Z to shorthand the tuple [h(X), Y]. Denote the variational difference as follows:
VDf (h,g) := EZ〜Ph×γ[g(Z)]- EZ〜Qh×γ[f*(g(Z))].	(4)
Let g* be the corresponding optimal variational function g for VDf (h, g). This variational form
allows us to use a training dataset {(xn, yn)}nN=1 to perform the above maximization problem listed
in Eqn. (3) (Nowozin et al., 2016). A list of f -divergence functions together with the optimal
variational/conjugate functions g/f* is summarized in Table 1.
3
Published as a conference paper at ICLR 2021
Name	Df (P||Q)			g*	domf *	f*(u)
Total Variation	R 2Ip(Z) - q(Z)Idz	2q(Z) + q(z) log  	- dz	1 sin P(Z)	1 2 slgn q(Z)一 2P(Z)	11 U ∈ [-2,2]	u
Jenson-Shannon	1 R p(z)log	2p(Z) 2	p(Z) + q(Z)		log 		u < log 2	- log (2 - eu)
		p(Z) + q(Z)	P(Z) + q(Z)		
Pearson X2	(q(Z) - p(Z))2 I 	dz		2 © - 1!	R	Uu2 + U
	P(Z)		k q(Z))		4
KL	R P(z) log p(z) dx q(Z)		P(Z) 1 + log -yτ q(Z)	R	eu-1
Table 1: Df s, optimal variational g (g*), conjugate functions (f *). A more complete table, including
Jeffrey, Squared Hellinger, Neyman X2, Reverse KL, is provided in the Appendix.
2.2	HOW GOOD IS hf* ?
As we mentioned earlier, maximizing our defined f -divergence measures (or maximizing the f-
mutual information) between the classifier’s predictions and labels is not always returning the Bayes
optimal classifier. However, for a binary classification problem, we prove below that with balanced
dataset, maximizing Total Variation (TV) divergence returns the Bayes optimal classifier:
Theorem 1. For TV, when P(Y = +1) = P(Y = -1) (balanced), hf* is the Bayes optimal classifier.
Remark 2. The above theorem extends to the multi-class setting when we restrict attentions to
confident classifiers. See Appendix for details.
The above observation is not easily true for other f -divergence. Nonetheless, denote by Y *(X = x)
the Bayes optimal label for an instance x: Y * (X = x) = argmaxy P(Y = y|X = x). Denote by
Ph×γ*, Qh× γ* thejoint and product distribution P, Q defined w.r.t. h(X) and Y*. We prove:
Theorem 3. When P(Y* = +1) = P(Y* = —1) (balanced), maximizing Df(Ph×γ* ∣∣Qh×γ*)
returns the Bayes optimal classifier, if f(v) is monotonically increasing in |v - 1| on dom(f).
For example, Pearson X2 (f (v) = (v — 1)2) satisfies the monotonicity condition. In practice, when
the label distribution P(Y|X = x) has small uncertainties, the ground truth labels are approxi-
mately equivalent to the Bayes optimal label. Therefore, the above theorem implies that maximizing
Df (Ph×γ ∣Qh×γ) is also likely to return a high-quality classifier for other f -divergences.
2.3	Learning with noisy labels
Consider an arbitrary classifier h. Denote by Ph×γ the joint distribution of h(X) and Y:
.. . .. .. . ~ _ , _   	..	. _.
Joint noisy distribution: Ph×γ := P(h(X) = y,Y = y0), y,y ∈ Y.
Similarly, we use Qh× γ to denote the product (marginal) distribution of h(X) and Y:
Product noisy distribution: Qh×γ := P(h(X) = y) ∙ P(Y = y0), y,y0 ∈ Y.
When it is clear from context, we shorthand using P, Q. We are interested in understanding the
robustness in maximizing Df(Ph×γ||Qh×γ). Using training samples {χn,yn}N=ι, there exists
algorithms to compute the gradient of Df leveraging its variational form (Nowozin et al., 2016),
such that one can apply gradient descent or ascent to optimize it. We provide details in Section 5.
3	Variational difference with noisy labels
For an arbitrary g, we define the variational difference term w.r.t. the noisy label as follows:
VDf(h,g) := Ez~Ph×Yhg(Z)i - Ez~Qh×Y [f*(g(Z))]	(5)
where we use Z to denote [h(X),Y]. Denote by g* the corresponding optimal variational func-
tion g for VDf (h, g). In this section, we show that the variational difference term under noisy
labels is closely related to the variational difference term defined on the clean distributions P, Q.
Define the following quantity: ∆fy (h, g) := EX [g(h(X), y)] - EX [f* (g(h(X), y))] , for example
4
Published as a conference paper at ICLR 2021
∆f1(h,g) := EX[g(h(X),+1)] - EX [f* (g(h(X),+1))]. For a binary classification problem,
further denote by BiaSf(h,g) := e+ ∙ ∆-1(h,g) + e- ∙ ∆f+1(h,g). We derive the following fact:
Theorem 4.	For binary classification, the variational difference between the noisy distributions P
and Q relates to the one defined on the clean distributions in the following way:
VfDf (h, g) = (1 - e+ - e-)VDf(h,g) + Biasf (h, g)	(6)
The above decoupling result is inspiring: Biasf (h, g) can be viewed as the additional bias term
introduced by label noise. If this term has negligible effect in the maximization problem, maximizing
the noisy variational difference term will be equivalent to maximizing (1 - e+ - e-) ∙ VDf (h, g),
and therefore the clean variational difference term. If the above is true, we have established the
robustness of the corresponding f -divergence. This result also points out that when the effects from
the bias term are non-negligible, finding ways to counter the additional bias term will help us retain
the robustness of Df measures. Next we show that Theorem 4 extends to the multi-class setting
under two broad families of noise rate models, both covering the binary setting as a special case.
Multi-class extension of Theorem 4: uniform off-diagonal case We first consider the following
transition matrix: uniform off-diagonal transition matrix, where ej = Ti,j , ∀i 6= j, that is any other
classes i 6= j has the same chance of being flipped to class j. The diagonal entry Ti,i (chance of a
correct label) becomes 1 - Pj6=i ej. We further require that Pj ej < 1. Note that the binary noise
rate model is easily a uniform off-diagonal transition matrix.
Theorem 5.	[Multi-class] For uniform off-diagonal noise transition model, the noisy variational
difference term relates to the clean one in the following way:
KK
f f(h,g) = (1- Xej) ∙ VDf(h,g)+ Xej∙ ∆f (h,g)	(7)
If we define BiaSf (h, g) := PK=I ej ∙ ∆f (h, g), we reproduced the results in Theorem 4: for binary
一一 一 一 一 一，二- _______________________________________ .. 一，二-
case, relabel class 1 → +1, 2 → -1. Then eι := P(Y = +1∣Y = -1) = e-,e? := P(Y =
-1|Y = +1) = e+ . Another case of noise model we consider is sparse noise. Mathematically,
assume K is an even number, sparse noise model specifies KK disjoint pairs of classes (ic,jc) where
C ∈ [K] and ic < jc . The labels flip between each pair. We provide details in the Appendix.
4 WHEN Df IS ROBUST WITH LABEL NOISE
Denote by H an arbitrary hypothesis space for training a candidate classifier h. We will focus on H
throughout this section, and with abusing notation a bit, let hf = argmaXh∈H Df (Ph×γ∣∣Qh× γ).
We first define formally what we mean by robustness of Df (Ph×Y kQh×Y).
Definition 1. Df (Ph×γ∣∣Qh×γ) Is H-robust if hf = argmaxh∈H Df (Ph×Y∣∣Qh×γ).
The above definition is stating that the label noise does not disrupt the optimality of hf* when maxi-
mizing Df(Ph×γ∣∣Qh×γ) instead of Df (Ph×γ∣∣Qh×γ).
4.1	IMPACT OF THE Bias TERMS
In this section, we take a closer look at the BiaS terms and argue that they have diminishing effects as
compared to the VD terms when label noise increases. Recall g*,g* are the corresponding optimal
variational functions for VDf (h, g) and VDf (h, g).
Total variation (TV) For TY since f(v) = 2 |v - 1|, f*(u) = u, we immediately
have ∀y0 g(h = y0,y) - f *(g(h = y0, y)) = 0 and therefore ∆y(h,g) = EX[g(h(X),y)]-
EX [f * (g(h(X), y))] ≡ 0, ∀y, and further BiaSf (h, g) ≡ 0. This fact helps establish the robustness
of TV divergence measure (Theorem 7).
5
Published as a conference paper at ICLR 2021
Other divergences The above nice property generally does not hold for other f -divergence func-
tions. Next we focus on the binary classification setting and prove the following lemma:
Lemma 1. For f -divergence listed in Table 6 (Appendix), Biasf (h, g*) = O ((1 一 e+ 一 e-)2).
Note the variational form will be used when optimizing Df(Ph×γ∣∣Q%×γ) (and therefore We will
一 . 一.一.一 . _ ....................................... ,, 一. 、一
be using g*). This lemma simplifies Eqn. 4 to VDf (h, g) 8 VDf (h, g) + O(1 一 e+ 一 e-). Since
0 < 1 一 e+ 一 e- ≤ 1, when the noise rate e+ + e- is high, the effect of Bias term diminishes.
When the BiaS term becomes negligible, we will have VDf(h, g) 8 VDf(h, g) if e+ + e- → 1,
establishing the fact that optimizing VDf (h, g) is approximately the same as optimizing VDf (h, g).
4.2	HOW ROBUST ARE DfS?
We first prove the following result:
Theorem 6.	Df is H-robust when Biasf (h, g) satisfies either of the following conditions: (I) ∀h ∈
H, Biasf (h,g) ≡ const.; (II) ∀h ∈H,h = hf, Biasf(h,g*) ≤ Biasf (hf ,g*).
Theorem 6 gives sufficient conditions when the BiaS term does not get in the way of reaching the
optimality hf. Intuitively, when BiaSf(hf ,g*) is an upper bound of BiaSf (h,g*), the BiaS term
will not interfere with the convergence of the VD term. Next we provide specific examples of f-
divergence functions that would satisfy these conditions.
Total Variation (TV) is robust For TV, the fact that ∆fy (h, g) ≡ 0 allows us to prove:
Theorem 7.	For TV divergence, Biasf (h, g) ≡ const. and Df (Ph×Y kQh×Y) is H-robust with
label noise for any arbitrary hypothesis space H.
This result establishes TV as a strong measure that does not require specifying the noise rates.
Divergences that are conditionally robust Other divergences functions do not enjoy the above
nice property as TV has. The robustness of these functions need more careful analysis. Define the
following measures that capture the degree a classifier fits to a particular label distribution:
Definition 2. The fitness of h to R ∈ {Y, Y } is defined as FIT(h = y, R = y0)
P(h(X) = y∣R=y0)
P(h(X)=y)
FIT measures capture the degree of fit of the classifier to the corresponding label distribution. A
high FIT(h = y, Y = y) (same label) indicates a potential overfit to the noisy label. Denote by
_ _______________, ~ ________________________, - - .-
H* :={h ∈ H : min FIT(h = y,Y = y) ≥ max FIT(hf = y, Y = y) ≥ 1}∪ {hf}
yy
The 1 in the "≥ 1" above corresponds to the FIT for a random classifier. H* contains the classifiers
that are likely to overfit to the noisy labels. We argue, and also as observed in training, that H* is
the set of classifiers the training should avoid converging to, especially when the training only sees
noisy labels. Suppose P(Y = +1) = P(Y = 一1) (balanced clean labels) and e+ = e- (symmetric
noise rate), we have the following theorem for binary classification:
Theorem 8.	f -divergences listed in Table 6 (Appendix, except for Jeffrey) are H* -robust.
4.3 MAKING Df MEASURES ROBUST TO LABEL NOISE
For the general case, to further improve robustness of Df measures, we will need to estimate the
noise rates (e.g., e+, e-) and then subtract BiaSf (g, h) from the noisy variational difference term to
correct the bias introduced by the noisy labels. As a corollary of Theorem 4 we have:
Corollary 1. Maximizing the following bias-corrected VDf (h, g) defined over P and Q leads to h*f
hf = argmax SUp Ez~%工 Jg(Z)] - Ez~Q^XJf*(g(Z))] - Biasf (h,g).
By removing the BiaSf term, maximizing Ez~p [g(Z)] 一 Ez~q [f * (g(Z))] becomes the same with
maximizing the divergence defined on the clean distribution (1 一 PK=I ej) ∙ VDf (h, g). The Corol-
lary follows trivially from this fact. The calculation of the BiaS terms will require the inputs of
6
Published as a conference paper at ICLR 2021
noise rates. Our work does not intend to particularly focus on noise rate estimation. But rather,
we can leverage the existing results in performing efficient noise rate estimation. There are existing
literature on estimating noise rates (noise transition matrix) which can be implemented without the
need of ground truth labels. For interested readers, please refer to (Liu & Tao, 2015; Menon et al.,
2015; Harish et al., 2016; Patrini et al., 2017; Arazo et al., 2019; Yao et al., 2020; Zhu et al., 2021).
We will test the effectiveness of this bias correction step in Section 5.
5	Experiments
In this section, we validate our analysis ofDf measures’ robustness via a set of empirical evaluations
on 5 datasets: MNIST (LeCun et al. (1998)), Fashion-MNIST (Xiao et al. (2017)), CIFAR-10 and
CIFAR-100 (Krizhevsky et al. (2009)), and Clothing1M (Xiao et al. (2015)). Omitted experiment
details are available in the appendix.
Baselines We compare our approach with five baseline methods: Cross-Entropy (CE), Back-
ward (BLC) and Forward Loss Correction (FLC) methods as introduced in (Patrini et al., 2017),
the determinant-based mutual information (DMI) method introduced in (Xu et al., 2019) and
Peer-Loss (PL) functions in (Liu & Guo, 2020). BLC and FLC methods require estimating the
noise transition matrix. DMI and PL are approaches that do not require such estimation.
Noise model We test three types of noise transition models: uniform noise, sparse noise, and ran-
dom noise. All details of the noise are in the Appendix. Here we briefly overview them. The uniform
and sparse noise are as specified at the end of Section 3 for which our theoretical analyses mainly
focus on. The noise rates of low-level uniform noise and sparse noise are both approximately 0.2
(the average probability ofa label being wrong). The high-levels are about 0.55 and 0.4 respectively.
In the random noise setting, each class randomly flips to one of 10 classes with probability p (Ran-
dom p). For CIFAR-100, the noise rate of uniform noise is about 0.25. The sparse label noise is
generated by randomly dividing 100 classes into 50 pairs, and the noise rate is about 0.4.
Optimizing Df(Ph×γ∣∣Q%×γ) using noisy samples With the noisy training dataset
{xn, yn}N=ι, We optimize Df(Ph×γ ||Q h×γ) using gradient ascent of its variational form. Sketch
is given in Algorithm 1. For the bias correction version of our algorithm, the gradient will simply
include the VBiaSf (h, g). The variational function g* can be updated progressively or can be fixed
beforehand using an approximate activation function for each f (see e.g., (NoWozin et al., 2016)).
Algorithm 1 Maximizing Df measures: one step gradient
1:	Inputs: Training data {(xn, yn)}N=ι, f, variational function g*, conjugate f *, classifier ht.
2:	Randomly sample three mini-batches {(χn,yn)}B=ι, {(χn,yn)}B=ι, {(χn,yn)}B=1 from
{(xn, yn)}N=ι. {(xn, M)}b=i： simulate samples 〜P; {(χn, yn)}B=ι to simulate Q.
3：	Use ht,xn[yn] to denote model prediction on Xn for label 期n, E{(xn,yn)}^ 1, E{(x;/)}B- to
denote the empirical sample mean calculated using the mini-batch data.
4:	At step t, update ht by ascending its stochastic gradient With learning rate ηt :
ht+1 := ht +	ηt	∙	Vht hE{(Xn,yn)}B=1	5	(ht,xn [yn])] -	E {禺醯)}乙[f*	]))]]	∙
Tips: In practice, we suggest (also implemented in our experiments) using the fixed form of g*
Which appears as gf (v ) in Table 6 (appendix).
5.1	HOW GOOD IS hf* ON CLEAN DATA
As a supplementary of Section 2.2, we validate the quality of hf* on clean dataset of MNIST, Fashion
MNIST, CIFAR-10 and CIFAR-100. In experiments, since the estimation of product noisy distri-
bution are unstable when trained on CIFAR-100 training dataset, we use CE as a warm-up (120
epochs) and then switch to train with Df measures. For other datasets, we train with Df measures
without the warm-up stage. Results in Table 2 demonstrate that optimizing f -divergence on clean
dataset returns a high-quality hf* by referring to the performance of CE. Even though Df measures
7
Published as a conference paper at ICLR 2021
can’t outperform CE on clean dataset, we do observe that the gap between CE and Df measures are
negligible, for example, the largest gap of Total-Variation (TV) is only 0.81% among four datasets.
Dataset	CE	TV	Gap	J-S	Gap	KL	Gap
MNIST	99.39(99.38±0.01)	99.37(99.34±0.02)	-0.04	99.35(99.31±0.04)	-0.07	99.31(99.21±0.06)	-0.17
Fashion MNIST	90.44(90.34±0.12)	89.98(89.94±0.06)	-0.40	90.40(90.17±0.24)	-0.17	90.19(89.96±0.14)	-0.38
CIFAR-10	93.58(93.47±0.08)	92.80(92.66±0.13)	-0.81	92.35(92.23±0.07)	-1.24	90.55(90.38±0.15)	-3.09
CIFAR-100	73.47(73.39±0.05)	73.43(73.39±0.06)	0.00	73.47(73.26±0.17)	-0.13	73.33(73.16±0.10)	-0.23
Table 2: Experiment results comparison on clean datasets: We report the maximum accuracy of CE
and each Df measures along with (mean ± standard deviation); Gap: mean performance comparison
w.r.t. CE. Numbers highlighted in blue indicate the gap is less than 1%.
5.2	ROBUSTNESS OF Df MEASURES
∙0∙8∙64∙2
Ioooo
n-E>up> 一 PC
o'o o'l 0'2	03 OA 0'5 θ'β 0'7 θ'δ
Noise rate
Figure 1: Robustness of TV, JS, PS divergences.
----TV_acc
----JS_acc
----PS_3CC
(％) >U23U< ⅛ωH
101 } } }
Ooooo
1 8 6 4 2
O
As a demonstration, we apply the uniform noise model to CIFAR-10 dataset to test the robustness
of three Df measures: Total-Variation (TV), Jenson-Shannon (JS) and Pearson (PS). We trained
models with Df measures using Algorithm 1 on 10 noise settings with an increasing noise rate
from 0% to approximately 81%. The visualization of the Df values and accuracy w.r.t. noise rates
are shown in Figure 1. Both the Df values and test accuracy are calculated on the reserved clean
test data. We observe that almost all Df measures are robust to noisy labels, especially when the
percentage of noisy labels is not overwhelmingly large, e.g., ≤ 70%. Note that the curves for other
f -divergences are almost the same as the curve of total variation (TV), which is proved to be robust
theoretically. This partially validates the analytical evidences we provided for the robustness of other
f -divergences in Section 4.1 and 4.2.
5.3	Performance Evaluation and Comparison
From Table 3, several Df measures arise as competitive solutions in a variety of noise scenarios.
Among the proposed f -divergences, Total Variation (TV) has been consistently ranked as one of the
top performing method. This aligns also with our analyses that TV is inherently robust. For most
settings, the presented f -divergences outperformed the baselines we compare to, while they fell short
to DMI (once) and Peer Loss (5 times) on several cases, particularly when the noise is sparse and
high. The sparse high noise setting tends to be a challenging setting for all methods. We conjecture
this is because sparse high noise setting creates a highly imbalanced dataset, model training is more
likely to converge to a “sub-optimal" early in the training process. It is also possible that with sparse
noise, the impact of Bias terms becomes non-negligible. We do observe better performances with
very careful and intensive hyper-parameter tuning, but the results are not confident and we chose to
not report it. Fully understanding the limitation of our approach in this setting remains an interesting
on-going investigation.
In Table 4 (full details on MNIST and Fashion MNIST can be found in Appendix), we use noise
transition estimation method in (Patrini et al. (2017)) to estimate the noise rate. The estimates help
8
Published as a conference paper at ICLR 2021
us define the bias term and perform bias correction for Df measures. We observe that while adding
bias correction can further improve the performance of several divergence functions (Gap being
positive), the improvement or difference is not significant. This partially justified our analysis of the
bias term, especially when the noise is dense and high (uniform and random high).
Dataset	Noise	CE	BLC	FLC	DMI	PL	TV	J-S	KL
MNIST	Sparse, Low	97.21	95.23	97.37	97.76	98.59	99.23(99.11±0.08)	99.15(99.03±0.09)	99.21(99.15±0.05)
	Sparse, High	48.55	55.86	49.67	49.61	60.27	58.27(54.72±4.36)	58.93(55.80±1.93)	49.24(49.17±0.06)
	Uniform, Low	97.14	94.27	95.51	97.72	99.06	99.23(99.17±0.05)	99.1(99.08±0.04)	99.13(99.06±0.07)
	Uniform, High	93.25	85.92	87.75	95.50	97.77	98.09(97.96±0.13)	97.86(97.71±0.10)	98.14(97.88±0.18)
	-RandOm (0.2)-	98.26	97.46	97.61	98.82	99.25	99.26(99.19±0.05)	99.29(99.27±0.02)	99.26(99.19±0.06)
	-Random (0.7)-	97.00	93.52	87.74	95.47	98.52	98.81(98.73±0.06)	98.72(98.63±0.08)	98.76(98.65±0.10)
Fashion MNIST	Sparse, Low	84.36	86.02	88.15	85.65	88.32	89.74(89.34±0.33)	88.80(88.79±0.01)	89.77(89.42±0.34)
	Sparse, High	43.33	46.97	47.63	47.16	51.92	45.66(45.22±0.26)	47.46(46.39±0.70)	38.96(38.90±0.06)
	Uniform, Low	82.98	84.48	86.58	83.69	89.31	89.00(88.75±0.16)	88.58(88.46±0.18)	88.32(88.16±0.11)
	Uniform, High	79.52	78.10	82.41	77.94	84.69	85.58(85.07±0.31)	85.62(85.39± 0.33)	85.69(85.43±0.30)
	-Random (0.2)-	85.47	83.40	77.61	86.21	89.78	90.22(90.09±0.19)	89.73(89.43±0.24)	89.24(89.05±0.14)
	-Random (0.7)-	82.05	78.41	73.42	80.89	87.22	86.69(86.49±0.16)	87.79(87.33±0.29)	87.06(87.00±0.06)
CIFAR-10	Sparse, Low	87.20	72.96	76.17	92.32	91.35	91.81(91.56±0.16)	91.49 (91.43±0.08)	91.62(91.32±0.31)
	-Sparse, High-	61.81	56.30	66.12	27.94	69.70	63.96(62.25±1.00)	67.33(65.27±1.34)	46.55(46.43±0.08)
	Uniform, Low	85.68	72.73	77.12	90.39	91.70	92.10(92.01±0.09)	91.52(91.47±0.08)	92.26(92.08±0.12)
	-Uniform, High-	71.38	54.41	64.22	82.68	83.42	85.56(85.44±0.08)	84.49(84.35±0.13)	84.36(84.19±0.13)
	-Random (0.5)-	78.40	59.31	68.97	85.06	86.47	87.28(87.03±0.17)	86.92 (86.80±0.10)	86.93(86.85±0.11)
	-Random (0.7)-	68.26	38.59	54.39	77.91	57.81	80.59(80.45±0.10)	80.50(80.27±0.15)	78.93(78.59±0.30)
CIFAR-100	Uniform	63.87	51.40	60.04	64.39	67.94	69.15(68.90±0.17)	69.13(68.80±0.21)	68.79(68.60±0.11)
	Sparse	40.45	36.57	43.39	40.53	44.25	42.45(38.06±2.82)	38.09(38.00±0.08)	37.74(37.63±0.08)
	-Random (0.2)-	65.84	61.21	61.52	66.23	62.92	70.43(70.22±0.13)	70.40(70.12±0.21)	70.28(70.06±0.14)
	-Random (0.5)-	56.92	22.21	55.88	56.06	49.62	62.14(61.89±0.18)	61.58(61.15±0.27)	61.68(61.49±0.13)
Table 3: Experiment results comparison (w/o bias correction): The best performance in each setting
is highlighted in blue. We report the maximum accuracy of each Df measures along with (mean
± standard deviation). All f -divergences will be highlighted if their mean performances are better
(or no worse) than all baselines we compare to. A supplementary table including Pearson X2 and
Jeffrey (JF) is attached in Table 7 (Appendix).
Noise	J-S	Gap	PS	Gap	KL	Gap	JF	Gap
Sparse, Low	91.23(90.93±0.34)	-0.26	91.48(91.12±0.42)	+0.08	91.73(91.57±0.18)	+0.11	91.45(91.18±0.21)	-0.10
Sparse, High	46.45(46.31 ±0.14)	-20.88	46.31(45.90±0.44)	-0.05	46.59(46.52±0.05)	+0.04	46.25(45.77±0.50)	+0.04
Uniform, Low	92.16(92.09±0.09)	+0.64	92.25(92.13±0.09)	-0.12	90.92(90.84±0.10)	-1.34	92.19(92.10±0.08)	+0.02
Uniform, High	84.31(84.13±0.10)	-0.18	83.79(83.61±0.12)	+0.18	83.98(83.79±0.12)	-0.38	83.93(83.62±0.22)	+0.13
Table 4: Df measures with bias correction on CIFAR-10: Numbers highlighted in blue indicate
better than all baseline methods; Gap: relative performance w.r.t. their version w/o bias correction
(Table 3); those in red indicate better than w/o bias correction.
Clothing1M Clothing1M is a large-scale clothes dataset with comprehensive annotations and can
be categorized as a feature-dependent human-level noise dataset. Although this noise setting does
not exactly follow our assumption, we are interested in testing the robustness of our f -divergence
approaches. Experiment results in Table 5 demonstrate the robustness of the Df measures. TV and
KL divergences have outperformed other baseline methods.
Dataset ∣ Noise	∣ CE ∣ BLC ∣ FLC ∣ DMI ∣ PL ∣ T-V ∣ J-S ∣ Pear ∣ KL ∣ Jeffrey
ClothingIM ∣ HUmanNoise | 68.94 | 69.13 | 69.84 | 72.46 | 72.60 | 73.09 | 72.32 | 72.22 | 72.65 | 72.46
Table 5: Experiment results comparison on Clothing1M dataset.
6 Conclusion
In this paper, we explore the robustness of a properly defined f -divergence measure when used to
train a classifier in the presence of label noise. We identified a set of nice robustness properties
for a family of f -divergence functions. We also experimentally verified our findings. Our work
primarily contributed to the problem of learning with noisy labels without requiring the knowledge
of noise rate. Beyond this noisy learning problem, the derivation and analysis might be useful for
understanding the robustness of f -divergences for other learning tasks.
Acknowledgement This work is partially supported by the National Science Foundation (NSF)
under grant IIS-2007951 and the Office of Naval Research under grant N00014-20-1-22.
9
Published as a conference paper at ICLR 2021
References
Ehsan Amid, Manfred KK Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic
loss based on bregman divergences. In Advances in Neural Information Processing Systems, pp.
14987-14996, 2019.
Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Nontawat Charoenphakdee, Jongyeong Lee, and Masashi Sugiyama. On symmetric losses for learn-
ing from corrupted labels. arXiv preprint arXiv:1901.09314, 2019.
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-
dependent label noise: A sample sieve approach. In International Conference on Learning Rep-
resentations, 2021. URL https://openreview.net/forum?id=2VXyy9mIyU3.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. COLT 2010 - The 23rd Conference on Learning Theory, 12:257-269,
2010. ISSN 15324435.
Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise.
Neurocomputing, 160:93-107, 2015.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Ramaswamy Harish, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel
embeddings of distributions. In International conference on machine learning, pp. 2052-2060,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Simon Jenni and Paolo Favaro. Deep bilevel learning. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 618-633, 2018.
Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy
labels. In Proceedings of the IEEE International Conference on Computer Vision, pp. 101-110,
2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.	CoRR,
abs/1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38:447-461, 2015.
Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise
rates. ICML, 2020.
Nan Lu, Gang Niu, Aditya K Menon, and Masashi Sugiyama. On the minimal supervision for
training any binary classifier from only unlabeled data. arXiv preprint arXiv:1808.10585, 2018.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020.
10
Published as a conference paper at ICLR 2021
Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on
cybernetics, 43(3):1146-1151, 2013.
Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from cor-
rupted binary labels via class-probability estimation. In International Conference on Machine
Learning, pp. 125-134, 2015.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning
from noisy labels. In AISTATS, 2015.
Clayton Scott, Gilles Blanchard, Gregory Handy, Sara Pozzi, and Marek Flaska. Classification with
asymmetric label noise: Consistency and maximal denoising. In COLT, pp. 489-511, 2013.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2(3):4, 2014.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5552-5560, 2018.
Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In Advances in Neural Information Processing Systems,
pp. 10-18, 2015.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2691-2699, 2015.
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: An information-theoretic noise-
robust loss function. NeurIPS, arXiv:1909.03388, 2019.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi
Sugiyama. Dual t: Reducing estimation error for transition matrix in label-noise learning. arXiv
preprint arXiv:2006.07805, 2020.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels, 2018.
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when
learning with noisy labels. arXiv preprint arXiv:2102.05291, 2021.
11