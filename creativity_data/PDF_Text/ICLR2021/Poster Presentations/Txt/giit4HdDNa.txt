Published as a conference paper at ICLR 2021
Go with the flow:
Adaptive Control for Neural ODEs
Mathieu Chalvidal1,2,3, Matthew Ricci4, Rufin VanRullen1,3, Thomas Serre1,2
1Artificial and Natural Intelligence Toulouse Institute, Universite de Toulouse, France
2Carney Institute for Brain Science, Dpt. of Cognitive Linguistic & Psychological Sciences
Brown University, Providence, RI 02912
3Centre de Recherche Cerveau & Cognition CNRS, Universite de ToUloUse
4Data Science Initiative, Brown University, Providence, RI 02912
{mathieu-chalvid, mgr, thomas_serre}@brown.edu
rufin.vanrullen@cnrs.fr
Ab stract
Despite their elegant formUlation and lightweight memory cost, neUral ordinary
differential eqUations (NODEs) sUffer from known representational limitations.
In particUlar, the single flow learned by NODEs cannot express all homeomor-
phisms from a given data space to itself, and their static weight parameterization
restricts the type of fUnctions they can learn compared to discrete architectUres
with layer-dependent weights. Here, we describe a new modUle called neUrally-
controlled ODE (N-CODE) designed to improve the expressivity of NODEs. The
parameters of N-CODE modUles are dynamic variables governed by a trainable
map from initial or cUrrent activation state, resUlting in forms of open-loop and
closed-loop control, respectively. A single modUle is sUfficient for learning a dis-
tribUtion on non-aUtonomoUs flows that adaptively drive neUral representations.
We provide theoretical and empirical evidence that N-CODE circUmvents limi-
tations of previoUs NODEs models and show how increased model expressivity
manifests in several sUpervised and UnsUpervised learning problems. These favor-
able empirical resUlts indicate the potential of Using data- and activity-dependent
plasticity in neUral networks across nUmeroUs domains.
1 Introduction
The interpretation of artificial neUral networks as continUoUs-time dynamical systems has led to both
theoretical and practical advances in representation learning. According to this interpretation, the
separate layers of a deep neUral network are Understood to be a discretization of a continUoUs-time
operator so that, in effect, the net is infinitely deep. One important class of continUoUs-time models,
neUral ordinary differential eqUations (NODEs) (Chen et al., 2018), have foUnd natUral applications
in generative variational inference (GrathWohl et al., 2019) and physical modeling (Kohler et al.,
2019; RUthotto et al., 2020) becaUse of their ability to take advantage of black-box differential
eqUation solvers and correspondence to dynamical systems in natUre.
Nevertheless, NODEs sUffer from knoWn representational limitations, Which researchers have tried
to alleviate either by lifting the NODE activation space to higher dimensions or by alloWing the
transition operator to change in time, making the system non-aUtonomoUs (DUpont et al., 2019). For
example, Zhang et al. (2020) shoWed that NODEs can arbitrarily approximate maps from Rd to R
if NODE dynamics operate With an additional time dimension in Rd+1 and the system is affixed
With an additional linear layer. The same aUthors shoWed that NODEs coUld approximate homeo-
morphisms from Rd to itself if the dynamics Were lifted to R2d. Yet, the set of homeomorphisms
from Rd to itself is in fact qUite a conservative fUnction space from the perspective of represen-
tation learning, since these mappings preserve topological invariants of the data space, preventing
them from “disentangling” data classes like those of the annUlUs data in Fig. 1 (loWer left panel).
In general, mUch remains to be Understood aboUt the continUoUs-time frameWork and its expressive
capabilities.
1
Published as a conference paper at ICLR 2021
In this paper, we propose a new approach that we
call neurally-controlled ODEs (N-CODE) designed
to increase the expressivity of continuous-time neu-
ral nets by using tools from control theory. Whereas
previous continuous-time methods learn a single,
time-varying vector field for the whole input space,
our system learns a family of vector fields parameter-
ized by data. We do so by mapping the input space
to a collection of control weights which interact with
neural activity to optimally steer model dynamics.
The implications of this new formulation are critical
for model expressivity.
In particular, the transformation of the input space is
no longer constrained to be a homeomorphism, since
the flows associated with each datum are specifi-
cally adapted to that point. Consequently, our sys-
tem can easily “tear” apart the two annulus classes
in Fig. 1 (lower right panel) without directly lift-
ing the data space to a higher dimension. Moreover,
when control weights are allowed to vary in time,
they can play the role of fast, plastic synapses which
can adapt to dynamic model states and inputs.
The rest of the paper proceeds as follows. First,
we will lay out the background for N-CODE and
its technical formulation. Then, we will demon-
strate its efficacy for supervised and unsupervised
learning. In the supervised case, we show how N-
CODE can classify data by learning to bifurcate its
dynamics along class boundaries as well as memo-
rize high-dimensional patterns in real-time using fast
synapses. Then, we show how the flows learned by
N-CODE can be used as latent representations in an
unsupervised autoencoder, improving image genera-
tion over a base model.
Figure 1: Vector fields for continuous-time neu-
ral networks. Integral curves with arrows show
the trajectories of data points under the influence
of the network. Top left: Standard NODEs learn a
single time-independent flow (in black) that must
account for the whole data space. Top right: N-
CODE learns a family of vector fields (red vs yel-
low vs blue), enabling the system to flexibly ad-
just the trajectory for every data point. Bottom
left: Trained NODE trajectories of initial values
in a data set of concentric annuli, colored green
and yellow. The NODE transformation is a home-
omorphism on the data space and cannot separate
the classes as a result. Colored points are the ini-
tial state of the dynamics, and black points are the
final state. Bottom right: Corresponding flows
for N-CODE which easily separate the classes.
Transiting from the inner to outer annulus effects
a bifurcation which linearly separates the data.
2	Background
Neural ODEs (NODEs) (Chen et al., 2018) are dynamical systems of the form
dx
dx = f(x, θ,t),	⑴
where X is a space of features, θ ∈ Θ is a collection of learnable parameters, and f : X ×Θ×R 7→ X
is an equation of motion which we take to be differentiable on its whole domain. f defines a flow,
i.e. a triple (X, R, Φθ) with Φθ : X × R 7→ X defined by
T
Φθ (x(0), T) =x(0)+	f (x(t), θ, t)dt	(2)
0
which relates an initial point x(0) to an orbit of points {x(t) = Φθ(x(0), t), t∈ R}. For a fixed T,
the map x 7→ Φθ (x, T ) is a homeomorphism from X to itelf parametrized by θ.
Several properties of such flows make them appealing for machine learning. For example, the ODEs
that govern such such flows can be solved with off-the shelf solvers and they can potentially model
data irregularly sampled in time. Moreover, such flows are reversible maps by construction whose
inverse is just the system integrated backward in time, Φθ (., t)-1 = Φθ (., -t). This property en-
ables depth-constant memory cost of training thanks to the adjoint sensitivity method (Pontryagin
Lev Semyonovich ; Boltyanskii V G & F, 1962) and the modeling of continuous-time generative
normalizing flow algorithms (?).
2
Published as a conference paper at ICLR 2021
Interestingly, discretizing Eq. 2 yields the recursive formulation of a residual network (He et al.,
2015) with a single residual operator fθ :
T
ΦResNet(x(0),T) = x(0) + X fθ(xt-1)	(3)
t=1
In this sense, NODEs with a time-independent (autonomous) equation of motion, f, are the
infinitely-deep limit of weight-tied residual networks. Relying on the fact that every non-
autonomous dynamical system with state x ∈ Rd is equivalent to an autonomous system on the
extended state (x, t) ∈ Rd+1, Eq. 2 can also be used to model general, weight-untied residual net-
works. However it remains unclear how dependence of f in time should be modeled in practice and
how their dynamics relate to their discrete counterparts with weights evolving freely across blocks
through gradient descent.
3 N-CODE: Learning to control data-dependent flows
General formulation - The main idea of N-CODE is to consider the parameters, θ(t), in Eq. 1
as control variables for the dynamical state, x(t). Model dynamics are then governed by a coupled
system of equations on the extended state z(t) = (x(t), θ(t)). The initial value of the control
weights, θ(0), is given by a mapping γ : X → Θ. Throughout, we assume that the initial time
point is t = 0. The full trajectory of control weights, θ(t), is then output by a controller, g, given by
another differentiable equation of motion g : Θ × X × R 7→ Θ with initial condition γ(x0). Given
an initial point, x(0), we can solve the initial value problem (IVP)
ddt = Mzt)=((，*)= (f(x, θ,t),g(θ, x,t))
Z(O) = z0	{(x(0), θ(0)) = (xo,γ(xo))
(4)
where h = (f, g). We may think of g and γ as
a controller of the dynamical system with equa-
tion of motion, f . We model g and γ as neural
networks parameterized by μ ∈ Rnμ and use
gradient descent techniques where the gradient
can be computed by solving an adjoint prob-
lem (Pontryagin Lev Semyonovich ; Boltyan-
skii V G & F, 1962) that we describe in the
next section. Our goal here is to use the meta-
parameterization in the space Θ to create richer
dynamic behavior for a given f than directly
optimizing fixed weights θ .
Well-posedness - If f and g are continuously
differentiable with respect tox andθ and con-
tinuous with respect to t, then, for all initial
conditions (x(0), θ (0)), there exists a unique
solution z for Eq. 4 by the Cauchy-Lipschitz
theorem. This result leads to the existence
and uniqueness of the augmented flow (X ×
Θ, R, Φμ) with Φμ :(XX Θ) X R→X × Θ.
Moreover, considering the restriction of such a
flow on X, we are now endowed with a univer-
sal approximator for at least the set of homeo-
morphisms on X given that this restriction con-
stitutes a non-autonomous system. We discuss
Figure 2: Diagram of a general N-CODE module:
Given a initial state x(0), the module consists of an
augmented dynamical system that couples activity state
x and weights θ over time (red arrows). A mapping γ
infers initial control weights θ0 defining an initial flow
(open-loop control). This flow can potentially evolve in
time as θ might be driven by a feedback signal from x
(closed-loop, dotted line). This meta-parameterization
of f can be trained with gradient descent by solving an
augmented adjoint sensitivity system (blue arrows).
now how g and γ affect the evolution of the variable x(t), exhibiting two forms of control and noting
how they relate to previous extensions of NODEs.
3
Published as a conference paper at ICLR 2021
3.1	open and closed-loop controllers
If the controller outputs control weights as a function of the current state, x(t), then we say it is a
closed-loop controller. Otherwise, it is an open-loop controller.
Open-loop control: First, we consider the effect of using only the mapping γ in Eq. 4 as a con-
troller. Here, γ maps the input space X to Θ so that f is conditioned on x(0) but not necessarily
on x(t) for t > 0. In other words, each initial value x(0) evolves according to its own learned flow
(X , R, Φγ(x(0))). This allows for trajectories to evolve more freely than within a single flow that
must account for the whole data distribution and resolves the problem of non-intersecting orbits (see
Figure 4). Recently, (Massaroli et al., 2020b) proposed a similar form of data-conditioned open-
loop control with extended state (x(t), x(0))). This is a version of our method in which γ is of the
form γ(x) = θ(0) = [C : id] with C a constant vector, id is the identity function, and : denotes
concatenation. Our open-loop formulation makes an architectural distinction between controller and
dynamics and is consequently generalizable to the following closed-loop formulation.
d2x	2 dx
西=μ(I-x2)m-X
Jx = μ(x — 1 x3 — θ)
iθ=μ x
Figure 3: Example of dynamical control augmenta-
tion: The Van der Pol oscillator: As a second order
differential equation, the dynamics cannot be approx-
imated by a single 1-dimensional NODE with a con-
stant control θ (degenerate solution in red). However, if
the dynamics are decomposed into a planar system with
a dynamic control variable θ(t), then the parameter μ
can be adapted to fit a particular oscillatory regime (in
black). This is a particular form of augmentation dis-
cussed in (Dupont et al. (2019); Norcliffe et al. (2020))
that showcases the benefit of using additional variables
as evolving parameters of the dynamical system.
Closed-loop control: Defining a differentiable
mapping, g, which outputs the time-dependent
control weights θ(t) given the state of the vari-
able x(t) yields a non-autonomous system on
X (see Fig. 3). This can be seen as a specific
dimension augmentation technique where addi-
tional variables correspond to the parameters θ.
However, contrary to appending extra dimen-
sions which does not change the autonomous
property of the system, this augmentation re-
sults in a module describing a time-varying
transformation Φθ(t) (x0,t). Note that this
formulation generalizes the functional parame-
terization proposed in recent non-autonomous
NODES systems (Choromanski et al., 2020;
Massaroli et al., 2020b; ?), since the evolution
of θ(t) depends on x(t). Much like in the
case of classical control theory, we hypothe-
sized that the use of dynamic control weights
would be of particular use in reacting to a non-
stationary stimulus. We evaluate this hypothe-
sis in section. 5.3.
The expressivity of N-CODE compared to other
continuous-time neural networks is encapsu-
lated in the following proposition. The re-
sult, proven in appendix, shows that both open-
loop and closed-loop control systems overcome
NODEs’ expressivity constraint with two dis-
tinct strategies, data-conditioning and state-
space augmentation.
^⇒
Proposition 1 - There exists a transformation
φ : Rd → Rd which can be expressed by N-CODE but not by NODEs. In particular, φ is not a
homeomorphism.
3.2	Training
Loss function: Dynamics are evaluated according to a generalized loss function that integrates a
cost over some interval [0, T]:
l(z)
T
`(z(t), t)dt
0
T
/ '(x(t), θ(t),t)dt
0
(5)
4
Published as a conference paper at ICLR 2021
Figure 4: Trajectories over time for three types of continuous-time neural networks learning the 1-dimensional
reflection map φ(x) = -x. Left: (NODE) Irrespective of the form of f, a NODE module cannot learn SUCh
a function as trajectories cannot intersect (Dupont et al., 2019). Middle: (Open-loop N-CODE). The model
is able to learn a family of vector fields by controlling a single parameter of f conditioned on x(0). Right:
(Closed-loop N-CODE) With a fixed initialization, x(0), the controller model still learns a deformation of the
vector field to learn φ. Note that the vector field is time-varying in this case, contrary to the two others versions.
The loss in Eq. 5 is more general than in Chen et al. (2018) since ` can be any Lebesgue-measurable
function of both states, x, and control parameters, θ . In particular, this includes penalties at discrete
time points or over the entire trajectory (Massaroli et al., 2020b) but also regularizations on the
weights or activations over the entire trajectory rather than the final state z(T) of the system.
In order to estimate a control function θ(t) that is optimal with respect to Eq. 5, we invoke Pon-
tryagin’s maximum principle (PMP) (Pontryagin Lev Semyonovich ; Boltyanskii V G & F, 1962),
which only requires mild assumptions on the functional control space Θ and applies to functions f
that are non-smooth in θ. The PMP gives necessary conditions on θ(t) at optimality via the aug-
mented adjoint variable a(t). This quantity is the Jacobian of ` with respect to both x(t) and θ(t).
In the case of θ(t) being differentiable with respect to the meta-parameters μ, solving for the aug-
mented adjoint state a(t) as in Chen et al. (2018) allows us to compute the gradient of the loss with
respect to μ thanks to Theorem 1.
Theorem 1 - Augmented adjoint method: Given the IVP of equation 4 and for ` defined in equa-
tion 5, we have:
T
/ a(t)T鼠dt, such that a satisfies
0
'da	T ∂h ∂'
dt a . ∂ Z	∂ Z
a(T) = 0
(6)
where 理 is the Jacobian of h with respect to z:
∂Z
∂h
∂Z
In practice, we compute the Jacobian for this augmented dynamics with open source automatic
differentiation libraries using Pytorch (Paszke et al., 2019), enabling seamless integration of N-
CODE modules in bigger architectures. We show an example of such modules in section B of
Appendix.
4	Experiments
4.1	Reflection and Concentric Annuli
We introduce our approach with the 1- and 2-dimensional problems discussed in (Dupont et al.,
2019; Massaroli et al., 2020b), consisting of learning either the reflection map g(x) = -X or a
linear classification boundary on a data-set of concentric annuli. Earlier results have shown that
vanilla NODEs cannot learn these functions since NODEs preserve the topology of the data space,
notably its linking number, leading to unstable and complex flows for entangled classes. However,
both the open and closed loop formulations of N-CODE easily fit these functions, as shown in
Figs 4 and 5. The adaptive parameterization allows N-CODE to learn separate vector fields for
each input, allowing for simpler paths that ease model convergence. Informally, we may think of
the classification decisions as being mediated by a bifurcation parameterized by the data space.
Transiting across the true classification boundary switches the vector field from one pushing points
to one side of the classification boundary to the other. Moreover, the construction of γ and g as a
neural network guarantees a smooth variation of the flow with respect to the system state, which can
potentially provide interesting regularization properties on the family learned by the model.
5
Published as a conference paper at ICLR 2021
Figure 5: The concentric annuli problem. A classifier must separate two classes in R2, a central disk and an
annulus that encircles it. Left: Soft decision boundaries (blue to white) for NODE (First), N-CODE open-loop
(Second) and closed-loop (First) models. Right: Training curve for the three models.
4.2	Supervised image classification
We now explore different data-dependent controls for continuous-time models on an image
classification task. We tested the open-loop version of N-CODE against augmented NODEs
(Dupont et al., 2019) and the data-control model of (Massaroli et al., 2020b). We per-
form this experiment on the MNIST and CIFAR-10 image datasets. Our model consists of
the dynamical system with equation of motion f expressed as a single convolutional block
(1 × 1 7→ 3 × 3 7→ 1 × 1) with 50 channels. The control weights θ ∈ RK
are a real-valued vector of K concatenated parameters instantiating the convolution kernels.
Model	MnIst	CIFAR-10
ANODE	98.2%	61.2%
NODE DC	98.5%	62.0%
N-CODE (open)	99.2%	74.1%
Table 1: Classification accuracy on MNIST and
CIFAR10 test sets for NODE, NODE with data-
dependant flow (NODE DC) and open-loop N-
CODE (ours)
The 10-class prediction vector is obtained as a linear
transformation of the final state x(T ) of the dynam-
ics with a softmax activation. We aimed at simplicity
by defining the mapping γ as a two-layer perceptron
with 10 hidden units. We varied how γ affects θ by
defining different linear heads for each convolution
in the block. (see C.2 of Appendix). We did not
perform any fine-tuning, although we did augment
the state dimension, akin to ANODE, by increasing
the channel dimension by 10, which helped stabilize
training. We show that the open-loop control outperforms competing models. Results are shown in
Table. 1 with further details in Section C.2 of Appendix.
4.3	Real-time pattern memorization
To validate our intuition that closed-loop control is particularly valuable on problems involving flex-
ible adaptation to non-stationary stimuli and to distinguish our system from related data-conditioned
methods, we trained N-CODE on a dynamic, few-shot memorization task (Miconi et al., 2018).
Here, the model is cued to quickly memorize sets of sequentially presented n-dimensional binary
patterns and to reconstruct one of these patterns when exposed to a degraded version. For each
presentation episode, we exposed the model sequentially to m = 2, 5, or 10 randomly generated
n-length (n = 100, 1000) bit patterns with {-1, 1}-valued bits. Then, we presented a degraded ver-
sion of one of the previous patterns in which half of the bits were zeroed, and we tasked the system
with reconstructing this degraded pattern via the dynamic state, x(t). This is a difficult task since it
requires the memorization of several high-dimensional data in real-time. Miconi et al. (2018) have
shown that, although recurrent networks are theoretically able to solve this task, they struggle dur-
ing learning, while models storing transient information in dynamic synapses tend to perform better.
Following this result, we evaluated the performance of a closed-loop N-CODE model in which the
output of the control function takes the form of dynamic synapses (Eq. 7). Here, the control pa-
rameters are a matrix of dynamic weights, θ(t) ∈ Rm×m, which are governed by the outer product
μ Θ XxT,
(dχ∙,dθ=(P (θx) ,μ θ xxτ)
(x(0), θ(0)) =	(xstim,θ0)
(7)
where μ is matrix of learned parameters, θo is a learned initial condition, P is an element-wise
sigmoidal non-linearity, Θ is element-wise multiplication, and xstim is the presented stimulus. We
6
Published as a conference paper at ICLR 2021
Figure 6: Left: Diagram of an episode of the few-shot memorization task Miconi et al. (2018). Three, 5-length
-1, 1-valued bit patterns (n = 5 here for visualization) are presented to the system at regular intervals. At query
time, tq, a degraded version of one of the patterns is presented and the task of the model is to complete this
pattern. Right: Performance of a closed-loop N-CODE, an LSTM, a continuous-time ODE-RNN Rubanova
et al. (2019) and the data-conditioned NODE of Massaroli et al. (2020b) averaged over 3 runs. All models
besides N-CODE plateau at high values (chance is .25). N-CODE, on the other hand, learns immediately and
with extremely high accuracy across all n. (see Appendix)
have suppressed t for concision. We ran this experiment on the closed-loop version of N-CODE
as well as an LSTM, continuous-time ODE-RNN (Rubanova et al., 2019) and the data-controlled
model of Massaroli et al. (2020b). All systems had the same number of learnable parameters, except
for the LSTM which was much larger. We found that all considered models, except closed-loop N-
CODE, struggled to learn this task (Fig. 6). Note that chance is .25 since half the bits are degraded.
A discrete LSTM endowed with a much larger hidden state of 5000 neurons learns marginally better
than chance level. ODE-RNN and data control versions plateau at 17.5% error rate. For both n
tested, N-CODE learned the problem not only much better, but strikingly faster.
4.4	Unsupervised learning : Image autoencoding with controlled flow
Finally, we use our control approach in an autoencoding model for low-dimensional representation
learning in order to see if merely adding N-CODE to a simpler model can improve performance. Our
idea is to use the solution to a system of linear ODEs to encode the model’s latent representations.
First, images from U ∈ Rn are mapped by an encoder to a space of couplings, θ, by
Yμ ： U → Yμ(u)= θ ∈ Rm×m.	(8)
The latent code is taken as the vector x(T ) acquired by solving the autonomous differential system
presented in Eq. 9 from 0 to T starting from the initial condition (xo, γμ(u)) With xo 〜 N(0, I):
(ddx dθ∖	,八八、
［。正)=(θx, 0)	(9)
l(x(0), θ(0)) = (x0,Yμ(u))
This model, Which We call autoN-CODE, can be interpreted as a hybrid version of autoencoders and
generative normalizing floWs, Where the latent generative floW is data-dependent and parameterized
by the encoder output.
We found that a simple linear system Was already more expressive than a discrete linear layer in
shaping the latent representation. Since the output dimension of the encoder groWs quadratically
With the desired latent code dimension of the bottleneck, We adopted a sparse prediction strategy,
Where as feW as tWo elements of each roW of θ Were non-zero, making our model match exactly the
number of parameters of a VAE With same architecture. We believe that the increase in representa-
tional power over a vanilla autoencoder, as shown by improvements in Frechet Inception Distance
(FID; Heusel et al. (2017)) and more disentangled latent representations (see A.7), despite these
simplifying assumptions, shows the potential of this approach for representation learning (Fig. 2).
Training details can be found in Sec. C.4.
Here we measure the marginal improvement acquired by adding latent flows to a baseline VAE. In
order to endow our deterministic model with the ability to generate image samples, we perform,
7
Published as a conference paper at ICLR 2021
similarly to (Ghosh et al., 2020), a post hoc density estimation of the distribution of the latent tra-
jectories’ final state. Namely, we employ a gaussian mixture model fit by expectation-maximization
and explore the effect of increasing the number of components in the mixture. We report in Table
2, contrary to Ghosh et al. (2020), a substantial decrease in FID of our sampled images to the image
test set with an increasing number of components in the mixture (see Fig. 7 for latent evolution and
Fig. 8 for sampled examples).
4.5 IMAGE GENERATION
Considering the identical ar-
chitecture of the encoder and
decoder to a vanilla model,
these results suggest that the
optimal encoding control for-
mulation produced a structural
change in the latent manifold or-
ganization. (See section C.7 in
Appendix). Finally, we note that
-FIDW	CIFAR-10	CelebA-(64)
VAEt(Kingma & Welling, 2013)-	105.45	68.07
Auto-encoder^ (100 components)	73.24	63.11
Auto-encodert (1000 components)	55.55	55.60
AUtONCODE (100 components)	34.90	55.27
AutoNCODE (i000 components)	24.19	52.47
Table 2: Frechet Inception Distance (FID) for several recent architec-
tures (lower is better). ”Components” are the numbers of components
used in the gaussian mixture estimation of the latent distribution.
this model offers several original possibilities to explore, such as sampling from the control coef-
ficients, or adding a regularization on the dynamic evolution as in (Finlay et al., 2020), which we
leave for future work.
Figure 7: Left to right: Temporal evolutions of an image reconstruction along controlled orbits of AutoN-
CODE for representative CIFAR-10 and CelabA test images. Last column: ground truth image.
5	Related work
Neural ODEs - This work builds on the recent development of Neural ODEs, which have shown
great promise for building generative normalizing flows (Grathwohl et al., 2019) used for modeling
molecular interactions (Kohler et al., 2019) or in mean field games (Ruthotto et al., 2019). More
recent work has focused on the topological properties of these models (Dupont et al., 2019; Zhang
et al., 2020), introducing time-dependent parameterization (Zhang et al., 2019b; Massaroli et al.,
2020b; Choromanski et al., 2020), developing novel regularization methods using optimal trans-
port or stochastic perturbations (Finlay et al., 2020; Oganesyan et al., 2020) and adapting them to
stochastic differential equations (Tzen & Raginsky, 2019).
Optimal Control - Several authors have interpreted deep learning optimization as an optimal con-
trol problem (Li et al., 2017; Benning et al., 2019; Liu & Theodorou, 2019; Zhang et al., 2019a;
Seidman et al., 2020) providing strong error control and yielding convergence results for alternative
algorithms to standard gradient-based learning methods. Of particular interest is the mean-field opti-
mal control formulation of (E et al., 2018), which notes the dependence of a unique control variable
governing the network dynamics on the whole data population.
8
Published as a conference paper at ICLR 2021
Hypernetworks - Finally, our system is related to network architectures with adaptive weights such
as hypernetworks (Ha et al., 2017), dynamic filters (Jia et al., 2016) and spatial transformers (Jader-
berg et al., 2015). Though not explicitly formulated as neural optimal control, these approaches
effectively implement a form of modulation of neural network activity as a function of input data
and activation state, resulting in compact and expressive models. These methods demonstrate, in
our opinion, the significant untapped potential value in developing dynamically controlled modules
for deep learning.
Figure 8: Left Reconstructions of random test images from CelebA for different autoencoding
models. Right: Random samples from the latent space. Deterministic models are fit with 100-
components gaussian mixture model.
6	Conclusion
In this work, we have presented an original control formulation for continuous-time neural feature
transformations. We have shown that it is possible to dynamically shape the trajectories of the trans-
formation module applied to the data by augmenting the network with a trained control mechanism,
and further demonstrated that this can be applied in the context of supervised and unsupervised
representation learning. In future work, we would like to investigate the robustness and generaliza-
tion properties of such controlled models as well as their similarities with fast-synaptic modulation
systems observed in neuroscience, and test this on natural applications such as recurrent neural net-
works and robotics. An additional avenue for further research is the connection between our system
and the theory of bifurcations in dynamical systems and neuroscience.
Acknowledgments and Disclosure of Funding
This work was funded by the ANR-3IA Artificial and Natural Intelligence Toulouse Institute (ANR-
19-PI3A-0004).
Additional support provided by ONR (N00014-19-1-2029), NSF (IIS-1912280, OSCI-DEEP ANR
grant (ANR-19-NEUC-0004) and the Center for Computation and Visualization (CCV). We ac-
knowledge the Cloud TPU hardware resources that Google made available via the TensorFlow Re-
search Cloud (TFRC) program as well as computing hardware supported by NIH Office of the
Director (S10OD025181).
9
Published as a conference paper at ICLR 2021
References
Martin Benning, Elena Celledoni, Matthias J. Ehrhardt, Brynjulf Owren, and Carola-Bibiane
Schonlieb. Deep learning as optimal control problems: models and numerical methods, 2019.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In European Conference on Computer Vision, 2018.
J. Chang, L. Wang, G. Meng, S. Xiang, and C. Pan. Deep adaptive image clustering. In 2017 IEEE
International Conference on Computer Vision (ICCV), 2017.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems, 2018.
Krzysztof Choromanski, Jared Quincy Davis, Valerii Likhosherstov, Xingyou Song, Jean-Jacques
Slotine, Jacob Varley, Honglak Lee, Adrian Weller, and Vikas Sindhwani. An ode to an ode, 2020.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 1977.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Advances in
Neural Information Processing Systems, 2019.
Weinan E, Jiequn Han, and Qianxiao Li. A mean-field optimal control formulation of deep learning,
2018.
Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. HoW to train your
neural ode: the world of jacobian and kinetic regularization, 2020.
P. Ghosh, M. S. M. Sajjadi*, A. Vergari, M. J. Black, and B. Scholkopf. From variational to de-
terministic autoencoders. In 8th International Conference on Learning Representations (ICLR),
2020.
Will GrathWohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible
generative models With free-form continuous dynamics. In International Conference on Learning
Representations, 2019.
David Ha, AndreW Dai, and Quoc V. Le. HypernetWorks. 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2015.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a tWo time-scale update rule converge to a local nash equilibrium, 2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Max Jaderberg, Karen Simonyan, AndreW Zisserman, and koray kavukcuoglu. Spatial transformer
netWorks. In Advances in Neural Information Processing Systems, 2015.
Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classification and segmentation. In Proceedings of the IEEE International Conference on
Computer Vision, 2019.
Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter netWorks. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2013.
Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: sampling configurations for multi-
body systems With symmetric energies, 2019.
10
Published as a conference paper at ICLR 2021
Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algorithms for deep
learning, 2017.
Guan-Horng Liu and Evangelos A. Theodorou. Deep learning theory review: An optimal control
and dynamical systems perspective, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Stuart P. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory,
1982.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 2008.
Stefano Massaroli, Michael Poli, Michelangelo Bin, Jinkyoo Park, Atsushi Yamashita, and Hajime
Asama. Stable neural flows, 2020a.
Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting
neural odes, 2020b.
Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural
networks with backpropagation. In Proceedings of the 35th International Conference on Machine
Learning, 2018.
Andrew Ng. Sparse autoencoders, 2011.
Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Lio. On second
order behaviour in augmented neural odes, 2020.
Viktor Oganesyan, Alexandra Volokhova, and Dmitry Vetrov. Stochasticity in neural odes: An
empirical study, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, Alban Desmaison, Andreas Kopf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
LU Fang, JUnjie Bai, and SoUmith Chintala. Pytorch: An imperative style, high-performance deep
learning library, 2019.
Gamkrelidze R V Pontryagin Lev Semyonovich ; Boltyanskii V G and Mishchenko E F. The math-
ematical theory of optimal processes. 1962.
Alec Radford, LUke Metz, and SoUmith Chintala. UnsUpervised representation learning with deep
convolUtional generative adversarial networks, 2015.
YUlia RUbanova, Ricky T. Q. Chen, and David DUvenaUd. Latent odes for irregUlarly-sampled time
series, 2019.
Lars RUthotto, Stanley Osher, WUchen Li, Levon NUrbekyan, and Samy WU FUng. A machine learn-
ing framework for solving high-dimensional mean field game and mean field control problems,
2019.
Lars RUthotto, Stanley J. Osher, WUchen Li, Levon NUrbekyan, and Samy WU FUng. A machine
learning framework for solving high-dimensional mean field game and mean field control prob-
lems. Proceedings of the National Academy of Sciences, 2020.
Jacob H. Seidman, Mahyar Fazlyab, Victor M. Preciado, and George J. Pappas. RobUst deep learn-
ing as optimal control: Insights and convergence gUarantees. Proceedings of Machine Learning
Research, 2020.
Jianbo Shi and Jitendra Malik. Normalized cUts and image segmentation. Technical report, 2000.
Ilya Tolstikhin, Olivier BoUsqUet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein aUto-
encoders, 2017.
11
Published as a conference paper at ICLR 2021
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian
models in the diffusion limit, 2019.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. Journal in Machine Learning Res., 2010.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. In Advances in Neural Information
Processing Systems 32. 2019a.
Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural odes
and invertible residual networks, 2020.
Tianjun Zhang, Zhewei Yao, Amir Gholami, Joseph E Gonzalez, Kurt Keutzer, Michael W Ma-
honey, and George Biros. Anodev2: A coupled neural ode framework. In Advances in Neural
Information Processing Systems. 2019b.
12
Published as a conference paper at ICLR 2021
Appendix
A Theoretical results
A.1 Proof of Proposition 1
In order to show that the set of functions defined by N-CODE are not necessarily homeomorphisms
on X , we show that they are not generally injective.
Open-loop: Consider the one-dimensional N-CODE system Φ : (X , R) 7→ X with equation of
motion f(x, θ) = -θ where θ = γ(x(0)) = x(0). This system has solution
Φ(x(0), T) = x(0) +	f(x, θ, t) dt
= x(0) - x(0)T.
(10)
However, this implies
Φ(0,1)=Φ(1,1)=0,
(11)
giving the result.
Closed-loop: Similarly, we can show that the following 2D oscillatory system:
x( ) = ( )	=⇒	x(t)	= α cos(t)	+ β sin(t)	=⇒	x(t)	=	x(0) cos(t)	if θ(0) = 0 (12)
θ(t) = -x(t)
But Φ(x, π∕2) = 0, so x → Φ(x,t) = x(t) is not injective.
This shows that the two control forms, open and closed, are not restricted to learn homemorphisms
on the space X .
A.2 Proof of Theorem 1
Proof. The proof is inspired from Massaroli et al. (2020a). We place our analysis in the Banach
space Rn . Since ` and h are continuously differentiable, let us form the Lagrangian functional L
where a is an element of the dual space of z = xθ :
T
∂z
L(Z, a, t) := [ [' - ha, ∂t - hi]dt
0
(13)
∂z
The constraint ha(t), — — h(z, t)i is always active on the admissible set by construction of Z in
∂L
equation 4, such that We have ∀z, a, —— (z, a, t)
∂μ
part of the integral in equation 13 gives:
T
∂l
—(z, a,t). Moreover, integrating the left
T
∂ z	T ∂ a
J ha, ∂tiidt = ha, zi∣o -J h~∂t,Zidt
00
(14)
Now, given that' _h华, Zi is differentiable in μ for any t ∈ [0, T], Leibniz integral rule allows to
dt
write
∂l
∂μ
T
Z ∂μ[' + h ~dt,Zi + ha, hi]dt+ha(T),
0
-ha(0),
∂l
∂μ
T
/ ∂' ∂z
J ∂z ∂μ
0
+，获 i+ha,∂μ i]dt+ha(T)，富 i
(15)
(16)
13
Published as a conference paper at ICLR 2021
T
∂l ∂' ∂Z ,da ∂z,	/ ∂h ∂h ∂^ ∂h ∂z
∂μ = J[ ∂z ∂μ + h K 厢i + ha, ∂μ + 湍 + ∂Z ∂μ ]dt + ha(T),
0
(17)
The last equation can be reordered as:
Posing that a(T)
∂l
∂μ
(18)
da	∂h
θlXl+lθland h加-a∂Z -
∂' ∂z
∂z, ∂μ
0, the result follows.
□
Remark 1: This result allows to compute gradient estimate assuming that μ directly parametrizes
h SUch that the gradient -- are straightforward to compute. However, in the case of optimizing
∂μ
the initial mapping γ that infer the variable θ , this result can be combined with usual chain rule to
estimate the gradient:
∂l ∂θ ∂l
——=——--
∂μγ ∂γ ∂θ
(19)
B Implementation
Generic N-CODE module - We provide here a commented generic PyTorch implementation for
the N-CODE module in the open-loop setting.
class NCODE_func(torch.nn.Module):
def __init__(self, *args):
super(NCODE_func, self).__init__()
#Definition of the ODE system
self.f = ...
self.g = ...
def forward(self, t, z):
#Unpack variables (theta_t is potentially a tuple of tensors)
x_t, *theta_t = z
#Compute evolution od the system
d_x_t = self.f(x_t,theta_t)
d_theta_t = self.g(theta_t,d_x_t = f(x_t,theta_t)
return (d_x_t, *d_theta_t)
class NCODE_block(torch.nn.Module):
def __init__(self, *args):
super(NCODE_block, self).__init__()
#Definition of module and controller
self.gamma = ...
self.NCODE_func = ...
def forward(self, x_0):
#Set initial conditions
self.theta_0 = self.gamma(x_0)
14
Published as a conference paper at ICLR 2021
z_0 = (x_0,*self.theta_0)
#Run the system forward with defined settings
x_t, *theta_t = odeint_adjoint(self.odefunc, z_0, integration_time, rtol, at
return (x_t, *theta_t)
C Experimental results
Resources - Our experiments were run on a 12GB NVIDIA® Titan Xp GPUs cluster equipped
with CUDA 10.1 driver. Neural ODEs were trained using the torchdiffeq (Chen et al., 2018)
PyTorch package.
C.1 Flows for annuli dataset

Figure A.1: Time-varying flows for two points in different classes (inner: top row; outer: bottom
row) of the annuli dataset. Time advances from left to right.
C.2 Supervised image classification
For MNIST and CIFAR-10 classification, we defined the equation of motion f as sequence of con-
volutional filters replicating a ResNet block architecture with padding to conserve dimensionality
and non-linear ReLU activation. The base architecture takes one of the forms
•	1 × 1, k filters, 1 stride, 0 padding
•	3 × 3, k filters, 1 stride, 1 padding
•	1 × 1, c + a filters, 1 stride, 0 padding
with k the number of filters, c the number of channels (1 for MNIST and 3 for CIFAR10) and a
the augmentation channel (Dupont et al., 2019). The open-loop controller is a very simple linear
transformation of the data into 10 hidden units followed by multiple heads outputting the vector of
weights for each convolution kernel. In order to examine the effect of the size of dimensionality of
the equation of motion on learnability, we tried different parameterizations for f :
•	NCODE : The three kernels are conditioned by the controller γ .
•	NCODE 3 × 3: The inner 3x3 kernels are dynamically conditioned.
•	NODE DC : This corresponds to the form of data-control proposed by (Massaroli et al.,
2020b) where the image data is concatenated with state x(t) at each evaluation of the first
1x1 convolution kernel.
All parameters not adaptively controlled are simply learned as fixed parameters. For all models we
used the adaptative solver Dormund-Prince with tolerance of 1e-3.
15
Published as a conference paper at ICLR 2021
Figure A.2: Accuracy, Train and test losses over training interations and Number of function evaluations
(NFE) vs loss for several versions of NCODE on CIFAR-10.
The results show that N-CODE models achieve higher accuracy in fewer epochs and overall lower
loss (Fig. A.2, top panels). However, the simplicity of the mapping γ in this experiment and the
absence of regularization of the induced map Φγ (Finlay et al., 2020) yields a complex flow requiring
a higher total number of function evaluations (Fig. A.2, lower panels). This potentially prevents
NCODE from achieving even better accuracy. Interestingly, the results for the partially controlled
systems suggest that gradient descent accommodates the learning of hybrid modules f with both
static and dynamic convolution parameters. These results advocate for the exploration of specific
closed-loop formulations and regularization of control for convolution layers which we leave for
future work.
C.3 Pattern Memorization
This experiment is an update of Miconi et al. (2018), which showed that using a simple Hebbian
update rule between weights endowed recurrent networks with fast memory properties. The base
model is a single-cell recurrent network with all-to-all coupling, θ(t). The considered models are
Open-loop control: We adopt the implementation of (Massaroli et al., 2020b) by appending the pre-
sented pattern xstim to the dynamic variable x(t) such that θ(t) = θ is linear layer L(R2×N, RN).
Closed-loop control: The weights are fully dynamic and the influence of the plastic evolution is
tuned by learning the components μ that apply an element-wise gain for every weight velocity,
according to Eq. 7.
ODE-RNN: Here, a static-weight NODE models the evolution of a continuous-time single-cell recur-
rent network whose hidden state is carried over from one presentation to the next. A supplementary
linear read-out L(RN , RN) outputs the model’s guess and balances the number of parameters with
other models.
LSTM: A vanilla LSTM cell (Hochreiter & Schmidhuber, 1997). In coherence with (Miconi et al.,
2018) results, the hidden state dimension need to be greatly increased for the module to start mem-
orizing. We tested a 5000-dimensional hidden state for the reported results.
For each model, we learn the model weights with gradient descent using an Adam optimizer with a
learning rate λ = 3e - 4 and use the L2 distance between the reconstruction and objective pattern
as our objective function. For continuous models, each episode consists of a sequential presentation
of each pattern for 0.5 sec followed by a query time of 0.5 sec. For the LSTM, we adopted the
setting of (Miconi et al., 2018) where the sequence of presentation is discretized into 5 time-steps
of presentations. We tested for episodes of 3,5 and 10 patterns with 2 presentations in random order
and different proportions of degradation (0.5,0.7 and 0.9). Static-weight model performance rapidly
deteriorated in more challenging settings, whereas N-CODE reconstruction converged to a residual
error below 1% in all cases.
16
Published as a conference paper at ICLR 2021
Figure A.3: Error rate of N-CODE on the 1000-bit reconstruction task as a function of episodes for (Left)
different proportions of degraded bits and (Right) different number of patterns presented in one episode. In all
cases, the model learns in several hundreds episodes.
	MNIST	CIFAR-10	CelebA
	x ∈ R3×28×28	X ∈ R3×32×32	X ∈ R3×64×64
Encoder:	Flatten FC400 7→ Relu FC25 7→ Norm	Conv256 → BN → Relu Conv512 7→ BN 7→ Relu Conv1024 7→ BN 7→ Relu FC128*2 → Norm	Conv256 7→ BN 7→ Relu Conv512 7→ BN 7→ Relu Conv1024 7→ BN 7→ Relu FC64*io → Norm
Latent dynamics:	ODE[0,10]	ODE[0,10]	ODE[0,10]
Decoder:	FC400 7→ Relu FC784 7→ Sigmoid	FC1024*8*8 → Norm ConvT512 7→BN7→ Relu ConvT256 7→ BN 7→ Relu ConvT3	FC1024*8*8 → Norm ConvT512 7→BN7→ Relu ConvT256 7→ BN 7→ Relu ConvT3
Table A.1: Model architectures for the different data sets tested. FCn and Convn represent, respec-
tively, fully connected and convolutional layers with n output/filters. We apply a component-wise
normalisation of the control components which proved crucial for good performance of the model.
The dynamic is run on the time segment [0,10] which empirically yields good results.
C.4 AutoN-CODE architectures
Adapting the models used by Tolstikhin et al. (2017) and Ghosh et al. (2020), we use a latent space
dimension of 25 for MNIST, 128 for CIFAR-10 and 64 for CelebA. All convolutions and transposed
convolutions have a filter size of 4×4 for MNIST and CIFAR-10 and 5×5 for CELEBA. We apply
batch normalization to all layers. They all have a stride of size 2 except for the last convolutional
layer in the decoder. We use Relu non-linear activation and batch normalisation at the end of every
convolution filter. Official train and test splits are used for the three datasets. For training, we use
a mini-batch size of 64 in MNIST and CIFAR and 16 for CelebA in AutoN-CODE. (64 for control
models.) All models are trained for a maximum of 50 epochs on MNIST and CIFAR and 40 epochs
on CelebA. We make no additional change to the decoder. We train the parameters of the encoder
and decoder module for minimizing the mean-squared error (MSE) on CIFAR-10 and CelebA (Liu
et al., 2015) or alternatively the Kullback-Leibler divergence between the data distribution and the
output of the decoder for MNIST (formulas in Appendix.) Gradient descent is performed for 50
epochs with the Adam optimizer (Kingma & Ba, 2014) with learning rate λ = 1e-3 reduced by
half every time the loss plateaus. All experiments are run on a single GPU GeForce Titan X with 12
GB of memory.
C.5 Visualization of latent code dynamical evolution
C.6 Exploring sampling distributions
For random sampling, we train the VAE with aN (0, I) prior. For the deterministic models, samples
are drawn from a mixture of multivariate gaussian distributions fit using the testing set embeddings.
The distribution is obtained through expectation-maximization Dempster et al. (1977) with one sin-
17
Published as a conference paper at ICLR 2021
F
W
F
T
X-
F
于
I-
I-
决
千
-F野发歹矛歹ʃ* ʃ* 5~
堂》总》DDD
I- r X / z / z / /
* * Jl4 Jl' * Λ' Λ' 1 Z
壬壬壬壬XXsMM
7牙彳*々？。々““
了旷
ODD
/ / /
ZZN
3 3 3
于
女

7
79
7彳
7f
Figure A.4: Reconstructions of the image along the controlled orbits of AutoN-CODE for MNIST.
The temporal dimension reads from left to right. Last column: ground truth image
Figure A.5: Reconstructions of the image along the controlled orbits of AutoN-CODE for CIFAR-
10. The temporal dimension reads from left to right. Last column: ground truth image.
gle k-means initialization, tolerance 1e-3 and run for at most 100 iterations. We compute the FID
using 10k generated samples evaluated against the whole test set for all FID evaluations, using the
standard 2048-dimensional final feature vector of an Inception V3 following Heusel et al. (2017)
implementation.
The results show that 10K components (matching the number of data points in the test sets) natu-
rally overfits on the data distribution. Generated images display marginal changes compared to test
images. However, 1000 components does not, showing that our AutoN-CODE sampling strategy
mediates a trade-off between sample quality and generalization of images. We alternatively tested
non-parametric kernel density estimation with varying kernel variance to replace our initial sampling
strategy. We report similar results to the gaussian mixture experiment with an overall lower FID of
AutoN-CODE for small variance kernels. As the fitting distribution becomes very rough (σ ≈ 5),
the generated image quality is highly deteriorated.(see Figure A6).
18
Published as a conference paper at ICLR 2021
Figure A.6: Reconstructions of the image along the controlled orbits of AutoN-CODE for CelebA.
The temporal dimension reads from left to right. Last column: ground truth image
Figure A.7: Left: Evolution of FID as a function of the number of components in the Gaussian mixture used
to fit the latent distribution. We also tested a vanilla autoencoder with twice the capacity (236-dimensional),
exactly matching the number of parameters of our model. Interestingly, this model performs worst in a low
component regime. We interpret this a as a manifestation of the “curse of dimensionality”, as the latent ex-
ample population becomes less dense in this augmented space, making the naive gaussian mixture fitting less
accurate for fitting the latent distribution. Right: Samples of generated images for different number of mixture
components. The shape and details becomes clearer with increasing components.
19
Published as a conference paper at ICLR 2021
Figure A.8: Nearest neighbors search of random samples from the gaussian mixture with (Left)
1000 components (Left) and (Right) 10K components in the testing set of CIFAR-10. The sampled
latents show overfiting on the test set in the highest case.
Figure A.9: (Left) Evolution of FID between test set and generated samples as a function of the gaus-
sian kernel variance used to perform kernel estimation of the the latent code distribution. (Right)
Nearest neighbor search of random samples from the latent distribution fit by gaussian kernel esti-
mation with variance σ = 2 in the testing set.
20
Published as a conference paper at ICLR 2021
C.7 AutoN-CODE Latent Clustering
We further investigate the representations learned by our model by measuring its clustering accu-
racy against several other techniques in Fig. A.10. Similarly to the image generation experiment,
we consider the final state of the dynamical system as the latent representation that we cluster using
a 10 component gaussian mixture with different initializations. We did not perform any supplemen-
tary fine-tuning or data augmentation to train our model, but we also tested a version with further
dimensionality reduction using t-SNE (Maaten & Hinton, 2008). The results, although inferior to
recent deep clustering techniques, show better clustering accuracy than other autoencoding models,
suggesting a different organization of the latent code compared to the vanilla linear projection of the
autoencoder.
Clustering accuracy		MNIST
Model	CIFAR-10	
K-means (Lloyd, 1982)	22.9	57.2
Spectral clustering (Shi & Malik, 2000)	24.7	69.6
Variational Bayes AE* (Kingma & Welling, 2013)	29.1	83.2
Sparse AE (Ng, 2011)	29.7	82.7
Denoising AE (Vincent et al., 2010)	29.7	83.2
AE (GMM)*	31.4	81.2
GAN (2015) (Radford et al., 2015)	31.5	82.8
DeepCluster (Caron et al., 2018)	37.4	65.6
DAC (Chang et al., 2017)	52.2	97.8
UC (Ji et al., 2019)	61.7	99.2
AUtoeN-CODE (GMM)才	33.31	86.02
AUtoeN-CODE (t-SNE + GMM)*	27.00	97.26
Figure A.10: Unsupervised image clustering accuracy on CIFAR-10 and MNIST against recent
models. Results obtained with the authors original code are noted with ↑.
Figure A.11: tSNE embeddings of the latent code at t = t1 for MNIST test set colored with a 10
component gaussian mixture model.
21
Published as a conference paper at ICLR 2021
C.8 Latent code interpolation
Figure A.12: Interpolation: We further explore the latent space of AutoN-CODE by interpolating
between the latent vectors of the CelebA test set. (Upper panel) Linear interpolation between ran-
dom samples reconstructed with AutoN-CODE. (Middle panel) Interpolation comparison between
AutoN-CODE and a vanilla autoencoder for a single pair of vectors. (Lower panel) 2d interpolation
with three anchor reconstructions corresponding to the three corners of the square (A:up-left,B:up-
right and C:down-left. Left square corresponds to AutoN-CODE reconstructions and right to a
vanilla autoencoder.
22