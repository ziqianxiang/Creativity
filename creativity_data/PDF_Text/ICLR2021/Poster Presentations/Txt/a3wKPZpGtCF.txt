Published as a conference paper at ICLR 2021
Chaos of Learning Beyond Zero-sum and
Coordination via Game Decompositions
Yun Kuen Cheung
Department of Computer Science
Royal Holloway University of London
Egham, UK
yunkuen.cheung@rhul.ac.uk
Yixin Tao
Department of Mathematics
London School of Economics
London, UK
Y.Tao16@lse.ac.uk
Ab stract
It is of primary interest for Machine Learning to understand how agents learn
and interact dynamically in competitive environments and games (e.g. GANs).
But this has been a difficult task, as irregular behaviors are commonly observed
in such systems. This can be explained theoretically, for instance, by the works
of Cheung & Piliouras (2019; 2020), which showed that in two-person zero-sum
games, if agents employ one of the most well-known learning algorithms, Multi-
plicative Weights Update (MWU), then Lyapunov chaos occurs everywhere in the
cumulative payoff space. In this paper, we study how persistent chaos can occur
in the general normal-form game settings, where the agents might have the mo-
tivation to coordinate (which is not true for zero-sum games) and the number of
agents can be arbitrary.
We characterize bimatrix games where MWU, its optimistic variant (OMWU) or
Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost
everywhere in the cumulative payoff space. Our characterizations are derived by
extending the volume-expansion argument of Cheung & Piliouras via the canon-
ical game decomposition into zero-sum and coordination components. Interest-
ingly, the two components induce opposite volume-changing behaviors, so the
overall behavior can be analyzed by comparing the strengths of the components
against each other. The comparison is done via our new notion of “matrix domi-
nation” or via a linear program. For multi-player games, we present a local equiv-
alence of volume change between general games and graphical games, which is
used for volume and chaos analyses of MWU and OMWU in potential games.
1	Introduction
In Machine Learning (ML), it is of primary interest to understand how agents learn in competitive
environments. This is more strongly propelled recently due to the success of Generative Adversarial
Networks (GANs), which can be viewed as two neural-networks playing a zero-sum game. As such,
Evolutionary Game Theory (EGT) (Hofbauer & Sigmund (1998); Sandholm (2010)), a decades-old
area devoted to the study of adaptive (learning) behaviors of agents in competitive environments
arising from Economics, Biology, and Physics, has drawn attention from the ML community. In
contrast with the typical optimization (or no-regret) approach in ML, EGT provides a dynamical-
systemic perspective to understand ML processes, which has already provided new insights into a
number of ML-related problems. This perspective is particularly helpful in studying “learning in
games”, where irregular behaviors are commonly observed, but the ML community currently lacks
of a rigorous method to analyze such systems. In this paper, we study Lyapunov chaos, a central
notion that captures instability and unpredictability in dynamical systems. We characterize general
normal games where popular learning algorithms exhibit chaotic behaviors.
Lyapunov chaos captures the butterfly effect: when the starting point of a dynamical system is
slightly perturbed, the resulting trajectories and final outcomes diverge quickly; see Definition 1 for
a formal definition. The perturbations correspond to round-off errors of numerical algorithms in
ML (and Computer Science in general).1 While significant efforts have been spent in analyzing and
1In games, such perturbations can also occur due to errors in measuring payoffs.
1
Published as a conference paper at ICLR 2021
minimizing round-off effects of floating-point computations (Demmel (1997)), they are unavoidable
in general2 . As round-offs are inevitable, and the round-off schemes can vary from machine to
machine due to various hardware and software factors, we surely want to avoid chaotic learning that
does not fulfill our primary goals in building predictable and reproducible learning systems. Such
issues are exemplified by a quote from Ali Rahimi’s NIPS’2017 test-of-time award speech:
“Someone on another team changed the default rounding mode of some Tensor-
flow internals from ‘truncate toward zero’ to ‘round to even’. Our training broke,
our error rate Wentfrom less than 25% error to 〜99.97% error”
To avoid chaotic learning, we first need to understand how it can arise. This is the main mo-
tivation of our work. Recently, in the context of “learning in games”, Cheung & Piliouras (2019;
2020) presented interesting theoretical analyses to show that in two-person zero-sum and graphical
constant-sum games, if the agents employ Multiplicative Weights Update (MWU) or Follow-the-
Regularized-Leader (FTRL) algorithms, then Lyapunov chaos occurs everywhere in the cumulative
payoff space; the same result holds for the optimistic variant of MWU (OMWU) in coordination
games.3 While zero-sum and coordination games are interesting in their own rights, they are rather
small subspaces within the family of general normal games. In this paper, we present techniques
and tools for characterizing general games where MWU, FTRL, or OMWU are Lyapunov chaotic
almost everywhere. Next, we give an overview of our contributions and a discussion of related work.
Our Contributions. To show the results about chaos mentioned above, Cheung & Piliouras (2019;
2020) used a classical technique in the study of dynamical systems called volume analysis. Volume
analysis considers a set of starting points of positive Lebesgue measure, e.g. a ball centred at a point;
volume is an alternative name for Lebesgue measure in this context. When this set of starting points
evolves according to the rule of dynamical system, it becomes a new set with a different volume.
Intuitively, volume is a measure of the range of possible outcomes, so the larger it is, the more un-
predictable the system is. If the set’s volume increases exponentially with time, then its diameter
increases exponentially too, which implies Lyapunov chaos. This indicates that when players re-
peatedly play the games by employing the respective learning algorithms, a slight perturbation on
the initiating condition can lead to a wide range of possible cumulative payoffs in the long run. This
can be shown to imply instability in the mixed strategy space.
The technical starting point of Cheung & Piliouras is to show that when all agents in a bimatrix
game G use MWU with step-size , and when a set S in the cumulative payoff space evolves for one
time step, its volume change can be expressed as 2 S CG (s) ds + O(4), where CG is a function
that depends on the game G, which we will define in Section 2. Clearly, the sign of CG dictates the
volume change behaviors for all small enough . Cheung & Piliouras showed that CG is a positive
function when G is a two-person zero-sum game. For a large region in the cumulative payoff space,
this implies the volume change per time step is Ω(e2) ∙volume(S), i.e. volume expands exponentially.
They also showed that CG is a negative function if G is a two-person coordination game.
To extend their volume expansion results to general bimatrix games, we first discover that CG
admits a clean decoupling w.r.t. the canonical decomposition of such games into zero-sum and coor-
dination components (Basar & Ho (1974); Kalai & Kalai). Precisely, given any two-person general
game (A, B), it can be written uniquely as a direct sum of a zero-sum game (Z, -Z) and a coor-
dination game (C, C), where Z = (A - B)/2 and C = (A + B)/2. Interestingly, we find that
Cg(∙) = C(z,-z)(∙) + C(c,c)(∙) in Lemma 6. Recall from the last paragraph that C(z,-z)(∙) is
always positive, while C(c,c)(∙) is always negative. Thus, to see whether volume expansion occurs,
it boils down to comparing the strengths of C(z,-z)(∙) and -C(c,c)(∙).
We also discover that the function CG is invariant upon additions of trivial matrices to the game
G; see Definition 7 and Lemma 8. An immediate application of trivial matrices is for bimatrix
potential games (Monderer & Shapley (1996)), for which we show it can be transformed to a coor-
dination game via additions of trivial matrices. With the result in Cheung & Piliouras (2020), this
immediately implies that OMWU in any bimatrix potential game is Lyapunov chaotic everywhere
in the cumulative payoff space (Observation 10).
Based on the above discoveries, we identify two characterizations of bimatrix games where
MWU and FTRL are Lyapunov chaotic almost everywhere (Theorems 15 and 17). As said before,
2For instance, in learning algorithms we often evaluate the function ex , while e is an irrational number.
3They also showed some other chaos results for various combinations of learning algorithms and games.
2
Published as a conference paper at ICLR 2021
the key is to compare the strengths of the C-functions of the zero-sum and the coordination com-
ponents. The comparison is done via our new notion of matrix domination (see Definition 11), and
also a linear program (Eqn. (7)) which is designed to prune out the trivial-matrix projection and keep
the remaining part minimal. Such family of games has positive Lebesgue measure in the bimatrix
game space, so it is not confined to any proper game subspace4. This justifies the claim that the
occurrences of chaos are not only circumstantial, but a rather substantial issue in learning in games.
Analogous result holds for OMWU.
For games with any number of players (multi-player games), we use an observation in Cheung &
Piliouras (2019), coupled with our new findings about bimatrix games discussed above, to present a
new family of graphical games where MWU is Lyapunov chaotic almost everywhere (Theorem 18);
the new family strictly includes all graphical constant-sum games. To facilitate volume analyses of
learning in multi-player games, we establish their local equivalence of volume change with graphical
games. Briefly, we show that CG(p) for a general game G is the same as CH(p) for some graphical
game H; H will depend on the point p, that’s why we say the equivalence is local (Theorem 19).
This provides an intuitive procedure for understanding volume changes, as the volume change of
learning in a graphical game is easier to compute. This is used to show that the volume-changing
behaviors of MWU and OMWU are opposite to each other. We use these to analyze MWU and
OMWU in multi-player potential games; in particular, we show that CG (p) of a multi-player po-
tential game G is identical to CC(p) of a corresponding multi-player graphical coordination game,
while CC(p) ≤ 0 for any p (Proposition 21).5 6
Related Work. MWU and its variant, such as FTRL and Optimistic MWU, play important roles in
online learning. We recommend the texts of Cesa-Bianchi & Lugosi (2006) and Hart & Mas-Colell
(2013) for a modern overview of online learning from Machine Learning or Economics perspectives.
Recently, there is a stream of works that examine how learning algorithms behave in games
or min-max optimization from a dynamical-systemic perspective. This provides new insights into
learning systems which could hardly be obtained with classical tools in ML. For instance, some
learning systems are shown to be nearly-periodic under the notion of Poincare recurrence (PilioUras
& Shamma (2014); Mertikopoulos et al. (2018)). Using potential functions first proposed in EGT
and other tools from mathematics, surprising behaviors of first-order methods in zero-sum games
and min-max optimization were discovered (Daskalakis & Panageas (2018; 2019); Bailey & Pil-
iouras (2018); Cheung (2018)). In Appendix A, we give an account on some further related work.
Empirical evidences of Lyapunov chaos of learning in games were reported by Sato et al. (2002)
and Galla & Farmer (2013). Li-Yorke chaos, another classical chaos notion, was proved to occur in
several learning-in-game systems (Palaiopanos et al. (2017); Chotibut et al. (2020)).
Volume analysis has long been a technique of interest in the study of population and game
dynamics. It was discussed in a number of famous texts; see (Hofbauer & Sigmund, 1998, Section
11), (Fudenberg & Levine, 1998, Section 3) and (Sandholm, 2010, Chapter 9).
We use game decomposition in this paper, which is a natural and generic approach to extend
compelling results for a specific family of games to more general games. Let H denote the specific
family of games with some compelling properties. Given a general game, we seek to decompose it
into a sum of its projection on H, plus one or more residue components. If the residues are small,
then it is plausible that those compelling properties extend (approximately). In seeking of games
where learning is stable, game decompositions were used (Candogan et al. (2011; 2013a;b); Letcher
et al. (2019)) with H being potential games.
2	Preliminary
In this paper, every bold lower-case alphabet denotes a vector, every bold upper-case alphabet de-
notes a matrix or a game. When we say a “game”, we always mean a normal-form game. Given n,
let ∆n denote the mixed strategy space of dimension n - 16, i.e. {(zι, z2, ∙∙∙ ,Zn) | P；=i Zj = 1}.
Normal-Form Games. Let N denote the number of players in a game. Let Si denote the strategy
set of Player i, and S := Si ×∙∙∙× SN. Let n = |Si|. S = (si, ∙∙∙ ,sn) ∈ S denotes a strategy
4The families of zero-sum and coordination games are proper subspaces of the bimatrix game space. Any
proper subspace has Lebesgue measure zero.
5The theorem and propositions mentioned in this paragraph will be stated formally in Appendix C.
6The probability simplex over n strategies is (n - 1)-dimensional.
3
Published as a conference paper at ICLR 2021
profile of all players, and ui (s) denotes the payoff to Player i when each player picks si. A mixed
strategy profile is denoted by X = (xι, ∙∙∙ , XN) ∈ ∆n1 × ∙∙∙ × ∆nN, and Ui is extended to take
mixed strategies as inputs via Ui(X) = Es〜X [ui(s)]. We let -(iι,…，ig) denote the player set
other than players iι, ∙∙∙ , ig. We also let
UjIj2=jg (X)= Es-(ii,…,ig)~x-(ii,…,ig) [uiι (SiI = j1,…，sig = jg，S — (ii,…,ig))]，	⑴
which is the expected payoff to Player i1 when: for 1 ≤ f ≤ g, Player if picks strategy jf, while
for each player i ∈ {i1,…ig}, she picks a strategy randomly following Xi. We also use UiIj2二jg
ifX is clear from the context. We say a game is a zero-sum game if Pi Ui(s) = 0 for all s ∈ S, and
we say a game is a coordination game if Ui(s) = Uk(s) for all Players i and k and for all s ∈ S.
When N = 2, such games are called bimatrix games, for which we adopt the notations below.
Let (A, B) denote a bimatrix game, where for any j ∈ S1, k ∈ S2, Ajk := U1(j, k), Bjk :=
U2(j, k). X and y denote mixed strategies of Players 1 and 2 respectively. A bimatrix game is
a zero-sum game if A = -B; it is a coordination game if A = B. Note that Uj1 = [Ay]j,
Uk2 = [BTX]k, which we denote by Aj , Bk respectively when X, y are clear from context; Bj , Ak
are defined analogously.
MWU, FTRL and OMWU in Games. All three algorithms have a step-size , and can be im-
plemented as updating in the cumulative payoff (dual) space. In each round, the players’ actions
(mixed strategies) in the strategy space are functions of the cumulative payoff vectors to be defined
below, and these actions are then used to determine the payoffs in the next round. For a player with
d strategies, let pt ∈ Rd denote her cumulative payoff vector at time t, and let p0 ∈ Rd denote the
starting point chosen by the player. For MWU in a game, the update rule for Player i is
pj+1 = pj + 〜Uj(Xt),	⑵
where Uji is the function defined in Eqn. (1), and Xt is the mixed strategy as below:
Xj = Xj (Pt) = eχp(Pj)/(P'∈sieXp(PZ))	⑶
For OMWU in a game, the update rule for Player i starts with p1 = p0 , and for t ≥ 2, ptj+1 =
pj + e ∙ [2Ui(Xt) - Ui(Xt-1)], where Xt is determined by Eqn. (3).
For FTRL in a game, the update rule for Player i is same as Eqn. (2), but Xt is determined as
below using a convex regularizer function hi : ∆d → R: Xt = arg maxx∈∆d {h pt , X i - hi(X)}.
As all the results for MWU can be directly generalized to FTRL as discussed in (Cheung & Piliouras,
2019, Appendix D), to keep our exposition simple, in the rest of this paper, we focus on MWU and
OMWU, and their comparisons. For bimatrix game, we use p, q to denote the cumulative payoff
vectors of Players 1 and 2 respectively.
Dynamical Systems, Lyapunov Chaos and Volume Analysis. A learning-in-game system can be
viewed as a discrete-time dynamical system, for which we present a simplified definition which suits
our need. A discrete-time dynamical system in Rd is determined by a starting point r(0) ∈ Rd and an
update rule r(t+1) = f (r(t)),where f : Rd → Rd isafunction.7 The sequence r(0), r(1), r(2),…
is called a trajectory of the dynamical system. When f is clear from the context, we let Φ : (N ∪
{0}) ×Rd → Rd denote the function such that Φ(t, r) is the value of r(t) generated by the dynamical
system with starting point set to r. Given a set U ⊂ Rd, we let Φ(t, U) = {Φ(t, r)|r ∈ U}. Let
B (r, z ) denote the open ball with center r and radius z .
There are several similar but not identical definitions of Lyapunov chaos, all capturing the butter-
fly effect: when the starting point is slightly perturbed, the resulting trajectories diverge quickly. We
use the following definition, which was also used by Cheung & Piliouras (2019; 2020) implicitly.
Intuitively, a system is Lyapunov chaotic in an open set O ⊂ Rd if for any r ∈ O and any open ball
B around r, as long as Φ(t, B) remains inside O, there exists r0 ∈ B such that kΦ(t, r0) - Φ(t, r)k
grows exponentially with t. Lyapunov exponent in the definition below is a measure of how fast the
exponential growth is; the larger it is, the more unpredictable the dynamical system is.
7Rigorously, OMWU in game is not a dynamical system, as the update to p(t + 1) depends on both
p(t), p(t - 1). But there is a function f such that p(t + 1) ≈ f (p(t)), while the volume-changing behavior
is not really affected (Cheung & Piliouras (2020)).
4
Published as a conference paper at ICLR 2021
Definition 1. A dynamical system is Lyapunov chaotic8 in an open set O ⊂ Rd if there exists a
constant λ > 0 and a Lyapunov exponent γ ≡ γ(O) > 0, such that for any r ∈ O, for any
sufficiently small δ > 0 and for all t satisfying 0 ≤ t < min{τ | τ ≥ 0, Φ(τ, B(r, δ)) 6⊂ O},
sup ∣∣Φ(t, r0) — Φ(t, r)k ≥ λ ∙ δ ∙ exp(γt).
r0∈B(r,δ)
Definition 2. A dynamical system is Lyapunov chaotic everywhere if it is Lyapunov chaotic in any
bounded open subset of Rd.
In the above definitions, all norms and radii are Euclidean norms. For capturing round-off errors
in computer algorithms and ML systems, it is more natural to use '∞-norm with δ be the maximum
round-off error per step, say 〜10-16 when IEEE 754 binary64 (standard double) is used.
When O is a small set, it is easy to determine whether a dynamical system is Lyapunov chaotic
in O, since the dynamic can be locally approximated by a linear dynamical system, where the
eigenvalues of the local Jacobian characterizes chaotic behaviors. But when O is large, determining
whether Lyapunov chaos occurs is difficult in general. Cheung & Piliouras (2019) found that volume
analysis can be useful in this regard, based on the following simple observation.
Proposition 3. In Rd, if a set U has volume at least v, then its radius w.r.t. any point r ∈ U is
at least v1/d/2. Thus, if the volume of Φ(t, U) of some dynamical system is Ω(exp(γt)) for some
γ > 0, then the radius of Φ(t, U) w.r.t. any point r ∈ Φ(t, U) is Ω(exp( γd ∙ t)).
Cheung and Piliouras showed Lemma 4 below, which, for bimatrix games, reduces volume anal-
ysis to analyzing the sign of the function C(A,B) (p, q) defined in Eqn. (4) below; the sign also
determines the local volume-changing behavior around the point (p, q) when MWU is used. 9
Based on Proposition 3 that converts volume expansion to radius expansion, the sign can be used to
determine if the dynamical system is Lyapunov chaotic.
Let Aj = PkO Ajkoyko = Vχ∕xTAy] and Ak = pj, xj，Aj，k = VyJxTAy]; and, similarly,
Bj= PkO BjkOyk，= Vxj [xTBy] and Bk = P xj∙,Bj，k = Vy% [xTBy]. Then,
C(A,B)(P, q) = — Ex,y [(Ajk — Aj- Ak ) (Bjk — Bj- Bk )] + Ex,y [Ajk] ∙ Ex,y [Bjk] .	(4)
Note that here x and y are the shorthand for x(p) and y(q), which are the mixed strate-
gies (i.e. probability distributions over strategies) of Players 1 and 2 respectively, as computed via
Eqn. (3). Also, Eχ,y [f (j, k)] = E(j,k)〜(x(p),y(q))[f (j, k)] is the expected value of f(j, k) when the
strategies j and k are randomly chosen according to the distrubutions x(P) and y(q) respectively.
For multi-player game G, the analogous function Cg(∙) is given below; the U quantities were
defined in Eqn. (1). Lemma 4 is adapted from Cheung & Piliouras (2019) for games with any
number of players. Derivation of Eqn. (5) uses the Jacobian of the corresponding dynamical system
and integration by substitution; see Appendix D.
Cg(Pi,…，Pn)= — X X	Xijxk'(哨-U)Mk- Uj).	(5)
i∈[N ],j∈Si k>i,'∈Sk
Lemma 4. Suppose O is a set in the cumulative PayOffSPace Rd where d = nι + •… + nN, and
cg(O) := inf	Cg(pi, ∙∙∙ , PN) > 0.	(6)
(pι,…，PN)∈O
Then for the dynamical system in which MWU with any sufficiently small step-size is employed to
play the game G, it is Lyapunov chaotic in O with Lyapunov exponent ^G(O)∙ e2∕2d.
If MWU is replaced by OMWU, then the same result holds by replacing the condition (6) with
S(O)：=inf(pi,…,PN )∈oJCg(pi,…，pn )] > 0.
8We note that many linear dynamical systems admits simple closed-form solutions (e.g. dx/dt = x has
solution x(t) = x(0) ∙ et), but they are considered Lyapunov chaotic under this definition. This might not
match with the intuitive meaning of “chaos” to many people. However, for non-linear dynamical systems which
mostly do not admit closed-form solutions (including all systems we study), Lyapunov chaos is well-received
as a notion that captures unpredictability.
9They showed that, in the cumulative payoff space, when the set St is evolved to St+1 after one MWU step,
then volume(St+1) = volume(St) +2 R(p,q)∈St C(A,B) (p, q) dp dq + O(4). Thus, if C(A,B)(p, q) >
0, then the volume is increasing, which indicates diverging trajectories, i.e. chaos.
5
Published as a conference paper at ICLR 2021
Note that if we start from a Nash equilibrium in the strategy space, MWU and OMWU will stay at
the equilibrium. However, if this equilibrium (x*, y*) satisfies the conditions in Corollary 5 below,
there are points arbitrarily close to the equilibrium that keep moving away from the equilibrium (if
the region {(x0, y0) = (x(p), y(q))|(p, q) ∈ O} is large in the strategy simplex ∆n1 × ∆n2).
Corollary 5 (Adapted from (Cheung & Piliouras, 2020, Theorem 5)). Let (x*, y*) be a point in
the interior of the strategy space. Suppose that there exists (p, q) in the cumulative payoff space,
such that x* = x(p*) and y* = y(q*). Furthermore, suppose C(a,b)(p*, q*) > 0, and (p, q) ∈
O where O is the set described in Lemma 4. Then there are strategy points arbitrarily close to
(x*, y*) such that MWU in the game (A, B) eventually leaves the corresponding strategy set of O,
i.e. {(x0, y0) = (x(p), y(q))|(p, q) ∈ O}.
We give the intuitions behind the proof of Corollary 5. Suppose the contrary, i.e. for any open
neighbourhood of (p* , q* ), its flow never escapes from O. Then there are two contradicting facts.
First, the volume of the flow expands at least exponentially with time. Second, by Eqn. (2), each ptj
grows at most linearly with t (since |Uji| ≤ maxj,k{|Ajk|, |Bjk|}), and thus the volume of the flow
can only expand at most polynomially with time.
When the game is zero-sum, i.e., B = -A, hence C(A,B)(p, q) = Ex,y (Ajk - Aj - Ak)2 -
Ex,y[(Ajk)]2. Since Ex,y [Ajk] = Ex,y[Aj] = Ex,y [Ak] and hence Ex,y [Ajk - Aj - Ak] =
-Ex,y [Ajk], C(A,B)(p, q) is indeed the variance of the random variable Ajk - Aj - Ak, and thus
is non-negative. By Eqn. (4), we have C(A,B)(p, q) = -C(A,-B)(p, q). Thus, for any coordination
game (A, A), we have C(A,A)(p, q) = -C(A,-A)(p, q) ≤ 0.
3	B imatrix Games
In this section, we focus on general bimatrix games (A, B). In Section 3.1, we present two tools for
analyzing C(a,b)(∙), and then We provide an example to show how to use these tools. In Section 3.2,
we present two characterizations such that the dynamics are Lyapunov chaotic almost everywhere.
3.1	Tools for Analyzing B imatrix Game
First Tool: Canonical Decomposition for Bimatrix Games. For every bimatrix game (A, B), it
admits a canonical decomposition (Basar & Ho (1974); Kalai & Kalai) into the sum of a zero-sum
game (N, -Z) and a coordination game (C, C), where Z = 2 (A - B) and C = 1 (A + B), i.e.
(A,B)=(Z,-Z)+(C,C).
We call (Z, -Z) the zero-sum part of the game (A, B), and (C, C) the coordination part of the
game. Our first result shows that the function C(∙) can be decomposed neatly into the two parts too.
Lemma 6. For any bimatrix game (A, B),
C(A,B)(p, q) ≡ C(Z,-Z)(p, q) + C(C,C)(p, q).
Proof. We use Eqn. (4) to expand the following:
4	∙ c(z-Z)(p, q) + 4 ∙ c(c,c)(p, q)
=E (Ajk - Bjk - Aj + Bj - Ak + Bk)2 - E [Ajk - Bjk]
- E (Ajk + Bjk - Aj - Bj - Ak - Bk)2 + E [Ajk + Bjk]
=E (Ajk - Bjk - Aj + Bj - Ak + Bk)2 - (Ajk + Bjk - Aj - Bj - Ak - Bk)2
-(E [Ajk] - E [Bjk])2 + (E [Ajk]+ E [Bjk])2
=E [4(-Bjk + Bj + Bk)(Ajk - Aj- Ak)] + 4 ∙ E [Ajk]∙ EBjk] = 4 ∙C(Α,B)(p, q). □
By the end of Section 2, we discussed that C(Z,-Z) (p, q) is always non-negative and
C(C,C)(p, q) is always non-positive. By the above lemma, we can analyze the volume-changing be-
havior of a bimatrix game (A, B) by looking at its zero-sum and coordination parts independently.
One simple intuition is that if the coordination (resp. zero-sum) part is small, then the volume-
changing behavior of (A, B) is closer to the behavior of the zero-sum (resp. coordination) part. We
realize this intuition quantitatively in the next subsection.
6
Published as a conference paper at ICLR 2021
Second Tool: Trivial matrix. Trivial matrices are matrices which do not affect the volume-changing
behavior, as depicted in Lemma 8 below.
Definition 7 (Trivial Matrix). T ∈ Rn×m is a trivial matrix if there exists real numbers
uι,u2, ∙∙∙ ,un and v1,v2,…,Vm such that Tjk = Uj + Vk for all j ∈ [n],k ∈ [m].
Lemma 8. For any two trivial matrices T1, T2, for any two matrices A, B ∈ Rn×m,
C(A,B) (p, q) ≡ C(A+T1,B+T2)(p, q).
One immediate application of this lemma is for two player potential games.
Definition 9. A game G is a potential game if there exists a potential function P : S → R such that
for any Player i and any strategy profile s ∈ S, P(si, s-i) -P(s0i, s-i) = ui(si, s-i) -ui(si0, s-i).
For the potential game, we have the following observation:
Observation 10. For any bimatrix potential game (A, B), there is a coordination game (P, P) such
that A - P, B - P are trivial matrices. P is the matrix representation of the potential function P.
This observation immediately implies that the volume-changing behavior of a potential game is
equivalent to that of a corresponding coordination game.
We give a concrete example to show how these tools help Us to analyze the C(a,b) (∙).
A Simple Example. We will show how to use our tools to demonstrate C(∙) ≥ 0 everywhere
for the following game. In the example, each player has three strategies. The payoff bimatrix
(A, B) is given below. The first number gives the payoff of the row player, who chooses a strategy
from {a, b, c}; the second number gives the payoff of the column player, who chooses a strategy
from {1, 2, 3}. We first use our first tool to decompose this game into zero-sum part (Z, -Z) and
	Strategy 1	Strategy 2	Strategy 3
Strategy a	(4,4)	(12, -4)	(-6,10)
Strategy b	(-8, 8)	(0, 0)	(12,-4)
Strategy C	(14,-2)	(-8, 8)	(4, 4)
coordination part (C, C), where Z = -08 80 -88 and C = 40 04 24 . At this point, we still cannot
easily figure out which one is larger between C(z,-z)(∙) and C(c,-c)(∙). However, we can further
42 4	0	2 -2
decompose the coordination part by the second tool: C = 2 0 2 + -2 0 2 , where the first
matrix on the RHS is a trivial matrix (using notations in Definition 7, u = VT = [2, 0, 2]). It’s easy
to see the second matrix on the RHS is 1Z. Then by Lemmas 6 and 8, and the definition of the
function C, for any point (p, q) in the cumulative payoff space,
c(a,B)(P, Q) = c(z,-Z)(P, q) + C(IZ, 1 Z)(P, Q) = (I-(I/4产)∙ c(z,-Z)(P, q) ≥ 0.
3.2 Results for Bimatrix Games
In this subsection, we identify several characterizations for general bimatrix games in which we
have chaotic behavior with MWU dynamic in a following set Sδ in the cumulative payoff space
Rn1+n2 . Note that when δ is tiny, its strategy correspondence covers almost the entirety of the
strategy simplex ∆n1 × ∆n2, thus we informally say that if the dynamical system is Lyapunov
chaotic in Sδ for a tiny δ, then it is Lyapunov chaotic almost everywhere.
Sδ = {(p, q) ∣∀j ∈ Sι,k ∈ S2, Xj(p) ≥ δ ∧ yk(q) ≥ δ}.
In order to show chaotic behavior of MWU in a specific bimatrix game (A, B), it is sufficient to
show C(A,B) (P, q) is strictly positive in the region Sδ, due to Lemma 4. In the previous subsection,
we show that for each game (A, B), it can be decomposed into a zero-sum part (Z, -Z) and a
coordination part (C, C). Furthermore, C(A,B) (P, q) = C(Z,-Z) (P, q) + C(C,C) (P, q). We also
raise an intuition that if the zero-sum part is small, then the volume behavior in the game (A, B)
will be similar that in the coordination part; conversely, if the coordination part is small, then the
volume behavior will be similar to the zero-sum part. However, we have not yet presented a way to
compare the largeness of the two parts. This is what we do here.
7
Published as a conference paper at ICLR 2021
3.2.1	First Characterization: Matrix Domination
The first characterization we identify is matrix domination. In this part, we show that under certain
conditions, the zero-sum part is always no less than the coordination part, i.e. C(Z,-Z) (p, q) ≥
-C(C,C)(p, q) for all (p, q). This directly implies C(A,B) (p, q) will be non-negative in the whole
cumulative payoff space. Interestingly, the condition we identify is both necessary and sufficient.
Similar result can also be achieved in the case that coordination part is always no less than the
zero-sum part. We first introduce the definition of the matrix domination.
Definition 11. We say matrix K dominates matrix L if they are of the same dimension, and for any
row indices j, j0 and column indices k, k0,
|Kjk + Kj0k0 - Kjk0 - Kj0k | ≥ |Ljk + Lj0k0 - Ljk0 - Lj0k | .
Note that the domination induces a partial order on all matrices: if K dominates L and L domi-
nates M, then K dominates M. The theorem below gives the necessary and sufficient condition.
Theorem 12. C(A,B) (p, q) is non-negative for all (p, q) if and only if matrix of the zero-sum part
Z dominates the matrix of the coordination part C.
The above theorem is based on the following crucial observation.
Observation 13. For any matrix Z,
C(z,-z)(p, q) = 1 X	Xj(P)	∙	yk(q)	∙	χj,(p)	∙ y®，(q) ∙	(Zjk	+	Zj，k	-	Zjk，- Zj，k)2.
j,j0 ∈S1
k,k0∈S2
Matrix domination only implies C(A,B)(P, q) is non-negative. In order to have C(A,B)(P, q) to
be strictly positive in the set S, we need θ-domination.
Definition 14. We say matrix K θ-dominates (θ > 0) matrix L if K dominates L, and there exist j,
j0, k, k0 such that |Kjk + Kj0k0 - Kjk0 - Kj0k | ≥ |Ljk + Lj0k0 - Ljk0 - Lj0k | + θ.
The following theorem holds due to Lemma 4.
Theorem 15. For any general bimatrix game (A, B) which is decomposed into zero-sum part
(Z, -Z) and coordination part (C, C), if Z θ-dominates C, then MWU with any sufficiently small
step-size E in the game (A, B) is Lyapunov chaotic in Sδ with Lyapunov exponent 2(2：^) e2∙
Note that in Definition 14, K θ-dominates L if a finite number of inequalities are satisfied. In
the context of Theorem 15, it is easy to see that there are quite many games (A, B), such that Z θ-
dominates C with all those inequalities strictly satisfied. Thus, there exists an open neighbourhood
around these games such that every game in the neighbourhood has its zero-sum part θ-dominates
its coordination part. This shows that such family of games has positive Lebesgue measure.
3.2.2	Second Characterization: Linear Program
Note that matrix domination is not always true. In some scenarios, the zero-sum matrix might not
dominate the coordination matrix. Yet, it is still possible that C(A,B)(P, q) is strictly positive in the
region Sδ, when every entry in the coordination matrix is small.
Precisely, for a general bimatrix game (A, B), if its coordination part (C, C) is small in the
sense that the absolute values of all entries in C are smaller than some constant r, then we can
bound C(c,-c)(∙) by O(r2). This is not the only case where We can bound C(c,-c)(∙) by a small
term. Even if the entries in matrix C are large, we can use trivial matrices to reduce them without
affecting C(c,-c)(∙). This is done via a linear programming approach described below.
Given a matrix K, let r(K) be the optimal value of following linear program:
min r such that ∀j, k, - r ≤ Kjk - gj - hk ≤ r.	(7)
r≥0,g,h
Note that {gj- + hk }j,k is a trivial matrix. Let K0 = K-{gj + hk }j,k. By Lemma 8, C(k,-k)(∙)=
C(K0,-K0)(∙). The following lemma shows that the value of C(k,-k)(∙) is closely related to r(K).
Lemma 16. For any (P, q) in Sδ = {(p, q) ∣∀j,k, Xj(p) ≥ δ and yk(q) ≥ δ}, (r(K) ∙ δ)2 ≤
C(K,-K) (P, q) ≤ r(K)2.
8
Published as a conference paper at ICLR 2021
Then the theorem below follows by applying Lemma 16 with Lemma 4.
Theorem 17. For any general bimatrix game (A, B) which is decomposed into zero-sum part
(Z, -Z) and coordination part (C, C), if there exists θ > 0 such that (r(Z) ∙ δ)2 ≥ (r(C))2 +
(θδ2)2, then MWU with any sufficiently small step-size in the game (A, B) is Lyapunov chaotic in
Sδ with Lyapunov exponent 2，]+蜂)e2.
Intuitively, r(Z) is a distance measure from the zero-sum game (Z, -Z) to the trivial game
space; analogously, r(C) is a distance measure from the coordination game (C, C) to the trivial
game space. Theorem 17 shows that if the coordination part is much closer to the trivial game space
than the zero-sum part, then MWU in this game is Lyapunov chaotic in Sδ .
4 Experiment
To illuminate that volume expansion occurs when MWU is
employed in game with small coordination part, we simulate
MWU in the reduced payoff space10 in a game which is the
sum of zero-sum game [ 10 10 ] , -01 -01	and coordination game
-0.05 0.03	-0.05 0.03
0.03 -0.05 ,	0.03 -0.05	.
In the strategy space, x* = y* = (0.5,0.5) is the unique Nash
equilibrium of the game. In the reduced dual space, the origin cor-
responds to the equilibrium. We pick a square of side length 0.004
around the origin as the set of starting points (the small red square
in the middle of Figure 1). As these starting points are evolved via
MWU with step-size 0.02, we take snapshots after every 1900 time
steps, which are shown with colors blue, pink, lime, purple, orange
and green (then the colors repeat) respectively. As shown in the fig-
ure, the volume (which is area in two-dimensional space) increases,
and its shape changes from a square to a parallelogram.
Figure 1: Volume expansion
of MWU in the bimatrix game
(∖ 0.95 0.03 Γ-1.05 0.03 ]、
[ 0.03 0.95] , 0.03 -1.05
5 Conclusion and Future Works
In this paper, we analyze the volume-changing behavior of several well-known learning algorithms
(MWU, OMWU, FTRL) on general bimatrix games and multi-player games, which leads to a Lya-
punov chaos analysis. For bimatrix games, we do this by decomposing a game into the zero-sum
part and the coordination part. This decomposition turns the volume analysis into comparing the
strengths of volume-expansion (zero-sum part) and volume-contraction (coordination part) of the
MWU dynamics. The comparison of strengths is made via the notion of matrix domination and
the use of a linear program. For multi-player games, by the local equivalence, we show that the
volume-changing behavior of MWU and OMWU are opposite to each other even in multi-player
games. We also show, for a general multi-player potential game, the key function CG is equal to a
corresponding multi-player coordination game, which implies it is not positive.
Studying learning in matrix (normal-form) games, which are among one of the most classical
game models, is a good theoretical starting point. Matrix games admit mathematically amenable
analyses, as demonstrated in our work and many previous works. For future works, we are im-
mensely interested in chaos analyses on settings that are more relevant to applications in ML,
e.g. general GANs and differential games. We believe the techniques we use (volume analysis,
game decomposition, etc.) can be applicable.
Acknowledgments
We thank several anonymous reviewers for their suggestions, which help to improve the readability
of this paper from its earlier version. Yixin Tao acknowledges NSF grant CCF-1527568, CCF-
1909538 and ERC Starting Grant ScaleOpt-757481. Yun Kuen Cheung acknowledges Singapore
NRF 2018 Fellowship NRF-NRFF2018-07.
10Reduced payoff space is a projection of the original payoff space. In the reduced payoff space, the horizonal
axis is p1 -p2, while the vertical axis is q1 - q2. The original cumulative payoff space is four-dimensional, for
which it is difficult to give graphical illumination. This is why we use the reduced space instead.
9
Published as a conference paper at ICLR 2021
References
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC,
pp. 321-338, 2018.
Luis Barreira. Poincare recurrence: old and new. In XIVth International Congress on Mathematical
Physics. World Scientific., pp. 415-422, 2006.
Tamer Basar and Yu-Chi Ho. Informational properties of the nash solutions of two stochastic
nonzero-sum games. Journal of Economic Theory, 7(4):370-387, 1974.
Victor Boone and Georgios Piliouras. From darwin to poincare and Von neumann: Recurrence and
cycles in evolutionary and algorithmic game theory. In International Conference on Web and
Internet Economics (WINE), pp. 85-99. Springer, 2019.
Ozan Candogan, Ishai Menache, Asuman E. Ozdaglar, and Pablo A. Parrilo. Flows and decompo-
sitions of games: Harmonic and potential games. Math. Oper. Res., 36(3):474-503, 2011. doi:
10.1287/moor.1110.0500. URL https://doi.org/10.1287/moor.1110.0500.
Ozan Candogan, Asuman E. Ozdaglar, and Pablo A. Parrilo. Dynamics in near-potential games.
Games Econ. Behav., 82:66-90, 2013a. doi: 10.1016/j.geb.2013.07.001. URL https://doi.
org/10.1016/j.geb.2013.07.001.
Ozan Candogan, Asuman E. Ozdaglar, and Pablo A. Parrilo. Near-potential games: Geometry and
dynamics. ACM Trans. Economics and Comput., 1(2):11:1-11:32, 2013b. doi: 10.1145/2465769.
2465776. URL https://doi.org/10.1145/2465769.2465776.
Nikolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006.
Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum
games. In NeurIPS, pp. 3532-3542, 2018.
Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning
Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, pp. 807-834, 2019. URL http:
//proceedings.mlr.press/v99/cheung19a.html.
Yun Kuen Cheung and Georgios Piliouras. Chaos, extremism and optimism: Volume analysis of
learning in games. In NeurIPS, 2020.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. In COLT 2012 - The 25th Annual
Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland, pp. 6.1-6.20, 2012.
URL http://proceedings.mlr.press/v23/chiang12/chiang12.pdf.
Thiparat Chotibut, Fryderyk Falniowski, MichaI Misiurewicz, and Georgios Piliouras. Family of
chaotic maps from game theory. Dynamical Systems, pp. 1-16, 2020. doi: 10.1080/14689367.
2020.1795624. Published online. Volume and number not yet assigned.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9256-9266,
2018.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. ITCS, 2019.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms
for zero-sum games. Games and Economic Behavior, 92:327-348, 2015.
James W. Demmel. Applied Numerical Linear Algebra.	SIAM, 1997. doi: 10.1137/1.
9781611971446.
Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. In EuroCOLT, pp. 23-37, 1995.
10
Published as a conference paper at ICLR 2021
Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In COLT, pp.
325-332,1996.
Drew Fudenberg and David K. Levine. The Theory of Learning in Games. MIT Press Books. The
MIT Press, 1998.
Tobias Galla and J. Doyne Farmer. Complex dynamics in learning complicated games. PNAS, 110
(4):1232-1236, 2013.
Sergiu Hart and Andreu Mas-Colell. Simple Adaptive Strategies:From Regret-Matching to Un-
coupled Dynamics. Number 8408 in World Scientific Books. World Scientific Publishing Co.
Pte. Ltd., June 2013. ISBN ARRAY(0x3d2ca060). URL https://ideas.repec.org/b/
wsi/wsbook/8408.html.
Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: regret bounded by variation in
costs. Mach. Learn., 80(2-3):165-188, 2010. doi: 10.1007/s10994-010-5175-x. URL https:
//doi.org/10.1007/s10994-010-5175-x.
Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge
University Press, 1998. doi: 10.1017/CBO9781139173179.
Adam Tauman Kalai and Ehud Kalai. Engineering cooperation in two-player games.
http://www.robots.ox.ac.uk/-sjrob/Outgoing/GT_talks/kalai.pdf.
Michael J. Kearns, Michael L. Littman, and Satinder P. Singh. Graphical models for game the-
ory. In UAI ’01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelli-
gence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pp. 253-260,
2001. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=
1&smnu=2&article_id=107&proceeding_id=17.
Alistair Letcher, David Balduzzi, Sebastien Racaniere, James Martens, Jakob N. Foerster, Karl
Tuyls, and Thore Graepel. Differentiable game mechanics. J. Mach. Learn. Res., 20:84:1-84:40,
2019. URL http://jmlr.org/papers/v20/19- 008.html.
Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and
computation, 108(2):212-261, 1994.
Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial
regularized learning. In SODA, pp. 2703-2717, 2018.
D. Monderer and L. S. Shapley. Potential games. Games and Economic Behavior, pp. 124-143,
1996.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. In NIPS, pp.
5874-5884, 2017.
Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro
Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From Poincare re-
currence to convergence in imperfect information games: Finding equilibrium via regularization.
arXiv preprint arXiv:2002.08456, 2020.
Georgios Piliouras and Jeff S. Shamma. Optimization despite chaos: Convex relaxations to complex
limit sets via POinCare recurrence. In SODA, pp. 861-873, 2014.
H. Poincare. Sur le probleme des trois corps et les equations de la dynamique. Acta Math, 13:1-270,
1890.
Ali Rahimi.	NIPS 2017 test-of-time award presentation.
https://www.youtube.com/watch?v=ORHFOnaEzPc.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-
quences. In NIPS, pp. 3066-3074, 2013a.
11
Published as a conference paper at ICLR 2021
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT
2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University,
NJ, USA, pp. 993-1019, 2013b. URL http://jmlr.org/proceedings/papers/v30/
Rakhlin13.html.
William H. Sandholm. Population Games and Evolutionary Dynamics. MIT Press, 2010.
Yuzuru Sato, Eizo Akiyama, and J. Doyne Farmer. Chaos in learning a simple two-person game.
PNAS, 99(7):4748-4751, 2002.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of
regularized learning in games. In Proceedings of the 28th International Conference on Neural
Information Processing Systems, NIPS’15, pp. 2989-2997, 2015.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In Advances in Neural Information Processing Systems, pp. 10450-10461, 2019.
12
Published as a conference paper at ICLR 2021
A	Further Related Work
In the study of no-regret learning (e.g. Littlestone & Warmuth (1994); Freund & Schapire (1995)),
a vast literature concerns general or even adversarial settings, in which the online arrivals of payoff
values come with no pattern or even from an adversary. More recently, settings where the online
payoffs are more well-behaved, under the term of “predictable sequence” coined by Rakhlin & Srid-
haran (2013b), have been studied. These settings include game dynamics, as the online payoffs are
determined by the mixed strategy choices of the players, while these choices are updated gradually
and somewhat predictably. For these settings, online learning algorithms that perform particularly
well, e.g. achieving regret bound below the canonical O(√T) limit, are designed and studied (Hazan
& Kale (2010); Chiang et al. (2012); Syrgkanis et al. (2015)). For instance, Nesterov’s excessive gap
technique and optimistic mirror descent are found to achieve near-optimal regret O(log T) in zero-
sum games (Daskalakis et al. (2015); Rakhlin & Sridharan (2013a)), and thus the empirical average
of the learning sequence converges to Nash equilibrium of the game (see Freund & Schapire (1996)
for an explanation). OMWU (with time-varying step-sizes), and more generally optimistic vari-
ant of FTRL (Rakhlin & Sridharan (2013b)), are some canonical examples of such online learning
algorithms.
Recently, there is a stream of work that examines how learning algorithms behave in games
or min-max optimization from a dynamical-systemic perspective. Replicator dynamics (RD; the
continuous-time analogue of MWU) and continuous-time FTRL are found to achieve optimal regret
in general settings (Mertikopoulos et al. (2018)). Furthermore, RD in zero-sum games or graph-
ical constant-sum games admits a constant of motion and preserves volume; these two properties
are used to show that such dynamical systems are near-periodic (Piliouras & Shamma (2014); Mer-
tikopoulos et al. (2018); Boone & Piliouras (2019); Vlatakis-Gkaragkounis et al. (2019); Perolat
et al. (2020)), captured rigorously under the notion of Poincare recurrence (Poincare (1890); Bar-
reira (2006)). However, when MWU, the forward Euler discretization of RD, is used in discrete-time
setting in zero-sum games, the near-periodicity is destroyed totally; indeed, the system will never
visit the same point (or its tiny neighbourhood) twice, converge to the boundary of the strategy
simplex, and fluctuate there irregularly (Bailey & Piliouras (2018); Cheung (2018)). In contrast,
(discrete-time) OMWU in zero-sum game is shown to converge to Nash equilibrium (Daskalakis &
Panageas (2019)); yet, in the more general setting of min-max optimization, it was found that Opti-
mistic Gradient Descent Ascent (OGDA) can have limit points other than (local) min-max solutions
(Daskalakis & Panageas (2018)).
B Proofs in Section 3
Proof of Lemma 8. First, observe that it suffices to prove that the lemma holds when T1 is a trivial
matrix and T2 is the zero matrix. Then the lemma holds for any trivial matrices T1 , T2 due to
symmetry: C(A,B) (p, q) = C(A+T1,B) (p, q) = C(A+T1,B+T2)(p, q).
Due to the definition of trivial matrix, we can write Tj1k = uj + vk. Then
C(A+T1,B)(p, q) - C(A,B) (p, q)
= -E I	Ajk +	Uj +	Vk	-	Aj	- Uj	- ɪ2	v'y'	- Ak	- Vk -	u'x'	1	(Bjk	-	Bj	- Bk)
L ∖	'∈S2	'∈Sι	)	_
+ E [Ajk +	uj +	Vk] ∙ E [Bjk]	+ E [(Ajk - Aj- Ak ) (Bjk - Bj- Bk )] - E	[Ajk]	∙ E [Bjk]
=-E I- ɪ2	v'y'	- ɪs u'x'	) (Bjk - Bj - Bk) + E [uj + vk] ∙ E [Bjk]
L ∖ '∈S2	'∈Sι	)	_
=E [Vk + Uj] ∙ E	[Bjk	— Bj - Bk]	+ E [Uj + Vk] ∙ E [Bjk] ∙
By recalling that E	[Bjk	- Bj - Bk]	= -E [Bjk], we have C(A+T1,B) (p, q) - C(A,B) (p, q)	=
0.	□
Proof of Observation 10. Let Pjk be the potential value of a potential game when Player 1 plays
strategy j and Player 2 plays strategy k. Then according to the definition the potential function, for
any j1, j2 and k,
Aj1k - Aj2k = Pj1k - Pj2k.
13
Published as a conference paper at ICLR 2021
In particular, for any j, k, Ajk = Pjk + A1k - P1k . This implies that there exists v such that
Ajk = Pjk + vk for any j and k.
Similarly, there exists u such that Bjk = Pjk + uj for any j and k. This implies that any
two-player potential games are coordination games plus trivial matrices.	□
Proof of Theorem 12. We first prove that if Z dominates C, then C(A,B) (p, q) is always non-
negative. By Observation 13,
C(A,B) (p, q) = C(Z,-Z) (p, q) + C(C,C) (p, q)
= C(Z,-Z) (p, q) - C(C,-C) (p, q)
=4 X Xj(p)yk(q)xj0(p)yk0(q)∙
j,j0,k,k0
(Zjk + Zj0k0	-	Zjk0	-	Zj0k)	- (Cjk	+ Cj0k0	- Cjk0	-	Cj0k)
≥ 0.
τ	.	.	" i` r-r i	. ι ∙	x~ι . ι	. ι	∙	. ʌ ʌ / τ~ ι' ! IC-C	ι .ι
In contrast, if Z does not dominate C, then there exist j, j0, k, k0 and δ > 0 such that
(Cjk + Cj0^0 - Cj^0 - Cj0^)	≥ (Zj^ + Zj0^0 - Zj^0 - Zjo k) + δ∙
For each η > 0, we construct P and q such that Xj(P) = xj∙o (P) = y^ (q) = y^o (q) = 1-η.
Furthermore, we let Υ denote the maximum absolute value of all entries in matrices A and B.
Then, for all j and k, |Zjk| ≤ Υ and |Cjk| ≤ Υ. Therefore,
C(A,B) (P, q) = C(Z,-Z) (P, q) + C(C,C) (P, q)
= C(Z,-Z) (P, q) - C(C,-C) (P, q)
=4 X Xj (P)yk (Q)XjO (PyykO (Q)∙
j,j0,k,k0
(Zjk + ZjOkO	- ZjkO	-	ZjOk)	-	(Cjk +	CjOkO -	CjkO	-	CjOk)
≤-δ (1-η) + |S1|2 ∙∣S2∣2 ∙ η∙16Υ2.
The last inequality holds as (Cjk + CjOkO - CjkO - CjOk)2 - (Zjk + ZjOkO - ZjkO - ZjOk)2 ≤
16Υ2. The value of -δ (1-η )4 + ∣S1∣2 ∙∣S2∣2 ∙ η ∙ 16Υ2 will be negative if we pick a small enough
η.	□
Proof of Observation 13. Consider a random process, wherej,j0 ∈ S1 are randomly picked accord-
ing to distribution x(P), and k, k0 ∈ S2 are randomly picked according to distribution y(q). Then
the RHS of Observation 13 can be expressed as 1 ∙ E [(Zjk + ZjOkO — ZjkO — ZjOk )2].
Then we expand the squared term in the expectation. Observing the symmetries within the
expansion, we immediately have
1
4
• E [(Zjk + ZjOkO
- ZjkO - ZjOk)2
E [(Zjk)2 - E [ZjkZjkO] - E [ZjkZjOk] + E [ZjkZjOkO] .
Let Zj := [Zy]j and Zk = [ZTx]k. Then we have
E [ZjkZjkO] =	XjykZjk	ykOZjkO	=	Xj[Zy]j	ykZjk	=	Xj(Zj)2=E[(Zj)2 .
j,k	kO	j	k	j
Similarly, E [ZjkZjOk] = E [(Zk)2. Lastly, E [ZjkZjOkO] = E[Zjk]2. Thus, the RHS of Observa-
tion 13 is simplified to
E[(Zjk)2 -E[(Zj)2 -E[(Zk)2 +E[Zjk]2.
14
Published as a conference paper at ICLR 2021
We complete the proof by noting that from the definition of C(z,-z)(∙) in Eqn. (4), C(z,-z)(∙)
can be rewritten as
E (Zjk)2 - E [ZjZjk] - E [ZkZjk] + E [Zjk] ,
while E [ZjZjk] = Pj xjZj Pk ykZjk = Pj xjZjZj = E (Zj)2, and similarly E [ZjZjk] =
E [(Zk)2].	□
Proofof Theorem 15. By Lemma 4, it suffices to prove c(a,b)(Sδ) = inf(p,q)∈sδ C(a,b)(p, q) ≥
θ2δ4.
This holds because the matrix Z θ-dominates C, which implies there exist j, j0, k, and k0 such
that
(Zjk + Zj0k0 - Zjk0 - Zj0k)2 ≥ (Cjk + Cj0k0 - Cjk0 - Cj0k)2 + θ2.
By applying Observation 13, C(Z,-Z) (p, q) ≥ C(C,-C) (p, q) + θ2δ4, because for (p, q) ∈ Sδ,
every Xj(p), yk(q), xι∕(p),yko(q) is at least δ. By noting that C(c,c)(p, q) = —C(c,—c)(p, q)
and C(a,b) (p, q) = C(z,-z) (p, q) + C(c,c) (p, q), the result follows.	□
Proof of Lemma 16. A key observation is the following equality:
C(κ,-κ)(P, q) = minF(g, h), where F(g, h) = Exj(P) ∙ y (q) ∙ (Kjk - gj - hk)2.	(8)
g,h
j,k
Recall the notations Kj = Pk ykKjk and Kk = Pj xjKjk, and we let e := Pj,k xjykKjk ≡
E [Kjk]. The equality (8) holds due to the following observations: (i) F(g, h) is a smooth convex
function of its variables, thus all minimum points have the same function value; (ii) if 舞 and IhF
are all zero at some point (g, h), then the point is a minimal point of F; (iii) C(K,-K)(P, q) is the
variance of the random variable Kjk - Kj - Kk (see the end of Section 2), and thus by a definition
of variance,
C(K,-K) (P, q) = E [(Kjk - Kj - Kk - E [Kjk - Kj - Kk])2]
= E	[(Kjk	-Kj	-Kk+e)2]	(since E [Kjk -Kj	-Kk]	=	-E[Kjk] =	-e)
= X xjyk(Kjk - (Kj - e) - Kk)2 = F(g#, h#),
j,k
where gj# = Kj - e and hk# = Kk; and (iv) at (g#, h#), the partial derivatives stated in (ii) are all
zero.
With this observation and comparing this with the definition of r(Z), it’s easy to figure out that
C(K,-K) (P, q) ≤ r(K)2.
To see C(k,-k)(p, q) ≥ (r(K)) ∙ δ)2, We first let g* and h* to be the optimal choice of g and
h in C(k,-k)(p, q) = ming,h Pj∙,k xj(p) ∙ y (q) ∙ (Kjk - gj - hk)2. DUe to the specification of
the linear program (7), we have 11
2 ∙ r(K) ≤ maχ{ Kjk- gj- h*}- min{Kjk- g*- hk}.
j,k	j,k
Therefore,
max
mj,akx{Kjk - gj*
mj ikn{Kjk - gj*
—
This immediately implies that C(K,-K) (P, q) ≥ (r(K) ∙ δ)2.	□
11If this is not true, we can let g and h in r(Z) to be gj = g* — maxj,k{Kjk-gj -hk}+mmj,k{κjk-gj -hk}
and hk = hk. Then we can achieve r(K) = maxj,k{κjk-gj -hk}-minj,k{Kjk-gj -hk} which make r(K)
smaller.
15
Published as a conference paper at ICLR 2021
C Multi-Player Games
Computing volume change of learning algorithm in multi-player game is slightly more involved
than the two-player case. We present a local equivalence formula of volume change between
normal-form and graphical games. This provides an intuitive procedure for understanding volume
changes. Proposition 20 shows that in multi-player game, the volume-changing behaviors of MWU
and OMWU are again opposite to each other (which was shown for bimatrix game in Cheung &
Piliouras (2020)).
Graphical Games. A graphical game Kearns et al. (2001) is a special type of N -player game where
the payoffs can be compactly represented. In a graphical game H, for each pair of players i, k, there
is an edge-game which is a bimatrix game between the two players, denoted by (Hi,k, (Hk,i)T),
where Hi,k ∈ Rni ×nk is the payoff matrix that denotes the payoffs to Player i. Then the payoff to
Player i at strategy profile S = (si, s2,… ,sn) is the sum of payoffs to Player i in all her edge-
games, i.e. ui(s) = Pk6=i Hsi,ik,sk. As is standard, this payoff function is extended via expectation
when the inputs are mixed strategies.
Here, we first use an observation from Cheung & Piliouras (2019) to construct a family of multi-
Player graphical games where MWU is Lyapunov chaotic in SN,δ := {(pi,…，PN)∣∀i ∈ [N],j ∈
Si, xij (pi) ≥ δ}. It was observed that the function CH (p) defined in Eqn. (5) is the sum of
C(Hi,k,(Hk,i)T) (pi, pk) of all pairs of Players i < k. This observation yields Theorem 18.
Theorem 18. Let G↑ denote the family of bimatrix games which satisfy the condition either in
Theorem 15 or in Theorem 17. In an N -player graphical game where each edge-game is drawn
from G↑, if all players are employing MWU with a sufficiently small step-size , then the dynamical
system is Lyapunov chaotic in SN,δ with Lyapunov exponent N (N 一 1)θ2δ4e2∕4 PN=I n.
Local Equivalence of General Games and Graphical Games. Next, we present a theorem which
connects the value of CG(p) ofa general game to CH(p), where H is a graphical game.
Theorem 19. Given an N -player normal-form game G and any point p in the cumulative payoff
space, the value of CG(p) is the same as CH(p), where H is a graphical game specified as follows:
for each pair of Players i, k andj ∈ Si, ` ∈ Sk, the payoff to Player i in her edge-game with Player
k when PIayer i picks j and PIayer k picks ' is Hjk := Uj', where Uj' is defined in Eqn. (1).
This theorem shows that for any game G, the value of CG(p) is the same as in a particular
graphical game, where each pair of players, (i, k) play a bimatrix game whose utility is exactly the
utility of the original game G, but taking the expectation on the randomness of the other players’
strategies. If the original game G is a graphical game, then in the graphical game Hjk = Ujk +
c-i,-', where c-i,-' is a parameter which does not depend on Players i and k.
Theorem 19 will be used in Appendix D to show the following proposition, which shows that the
volume-changing behaviors of MWU and OMWU are opposite to each other in multi-player game,
generalizing a prior result in Cheung & Piliouras (2020).
Proposition 20. The volume integrands of MWU and OMWU in a multi-player game G are respec-
tively 1 + CG(P) ∙ e2 + O(e3) and 1 — CG(P) ∙ e2 + O(e3). Thus, volume expands locally around
a cumulative payoff point p for MWU (resp. OMWU) if CG(p) is positive (resp. negative).
Multiplayer Potential Game. By Observation 10, we know that the volume behavior ofa potential
game is equivalent to a corresponding coordination game in bimatrix game. In this section, we want
to show, this holds even in the multi player setting.
Proposition 21. Suppose P is the potential function of a potential game U. Let UP be a game that
all players will receive P(s) when players play strategies s. Then CU(P) = CUP (P) ≤ 0.
In Appendix E, we will discuss some situations where CU(P) is strictly less than 0, thus OMWU
is Lyapunov chaotic therein.
D	Local Equivalence of Volume Change between Normal-form
and Graphical Games
Here, we concern the volume change of a learning algorithm in multi-player game. We first recap
from Cheung & Piliouras (2020) on how the volume change is computed for dynamical systems
16
Published as a conference paper at ICLR 2021
which are gradual (i.e. those governed by a small step-size), followed by a continuous-time analogue
of OMWU in games, which are crucial for analyzing the volume change of discrete-time OMWU.
Then we compute the volume changes of MWU and OMWU in multi-player graphical games and
normal-form games respectively. Once these are done, the proofs of Proposition 20 and Theorem 17
become apparent.
D.1 Discrete-Time Dynamical Systems and Volume of Flow
We consider discrete-time dynamical systems in Rd . Such a dynamical system is determined recur-
sively by a starting point s(0) ∈ Rd and an update rule of the form s(t + 1) = G(s(t)), for some
function G : Rd → Rd. Here, we focus on the special case when the update rule is gradual, i.e. it is
in the form of
s(t + 1)= s(t) + 〜F(s(t)),
where F : Rd → Rd is a smooth function and step-size > 0. When F and are given, the
flow of the starting point s(0) at time t, denoted by Φ(t, s(0)), is simply the point s(t) generated
by the above recursive update rule. Then the flow of a set S ⊂ Rd at time t, denoted by Φ(t, S),
is the set {Φ(t, s) | s ∈ S}. Since F does not depend on time t, we have the following equality:
Φ(t1 + t2, S) = Φ(t2, Φ(t1, S)).
By equipping Rd with the standard Lebesgue measure, the volume of a measurable set S, denoted
by vol(S), is simply its measure. Given a bounded and measurable set S ⊂ Rd, if the discrete flow
in one time step maps S to S0 = Φ(1, S) injectively, then by integration by substitution for multi-
variables,
vol(S0) =
s∈S
det (I + 〜J(S)) dV,
(9)
where I is the identity matrix, and J(s) is the Jacobian matrix defined below:
Γ ∂∂1 FI(S)	∂∂2 FI(S)…∂∂d Fι(s)]
福 F2 (S)段 F2 (S)…然 F2 (S)
-悬 Fd(S)袅 Fd(S)…	悬 Fd(S).
Clearly, analyzing the determinant in the integrand in Eqn. (9) is crucial in volume analysis;
we call it the volume integrand. When the determinant is expanded using the Leibniz formula, it
becomes a polynomial of e, in the form of 1 + C(s) ∙ Eh + O(eh+1) for some integer h ≥ 1. Thus,
when the step-size is sufficiently small, the sign of C(S) dictates on whether the volume expands
or contracts.
D.2 Continuous-Time Analogue of OMWU
OMWU does not fall into the category of dynamical systems defined above, since its update rule
is in the form of S(t + 1) = G(S(t), S(t - 1)). Fortunately, Cheung and Piliouras Cheung &
Piliouras (2020) showed that OMWU can be well-approximated by the online Euler discretization
of a system of ordinary differential equations (ODE), and thus it can be well-approximated by a
dynamical system.
The ODE system is given below. p is a dual (cumulative payoff) vector variable, u : R+ → Rd
is the function such that u(t) gives the instantaneous payoff vector at time t. We assume that u is
twice differentiable with bounded second-derivatives, and UI denotes the time-derivative of u.
D = U + e ∙ U,	(11)
Online Euler discretization (OED) of Eqn. (11) refers to the following time-discretization of the
ODE system. In applications, U might not be explicitly given, and the sequence u(0), u(1), u(2),…
are available online (i.e., at time t We only have access of u(τ) for T = 0,1, ∙∙∙ ,t). As the dis-
cretization step is e, we approximate u(t) by (u(t) - u(t - 1))∕e. By using this approximation,
OED of Eqn. (11) yields
p(t + 1) = p(t) + E ∙ u(t) + E ∙ Utu(t—
E
P⑴ + E ' [2 ∙ U⑴-u(t - 1)],
17
Published as a conference paper at ICLR 2021
which is exactly the OMWU update rule in general context.
When compared the OED with the standard Euler discretization
p(t +1) = p(t) + J [u(t) + J u(t)],
OED incurs a local error that appears due to the approximation of U(t). The local error can be
bounded by O(3). Cheung and Piliouras Cheung & Piliouras (2020) showed that eventually the
determinant of the volume integrand is a of the form 1 + C(s) ∙ e2 + O(e3), the local error does not
affect the first and second highest-order terms, and hence can be ignored henceforth.
D.3 MWU in Graphical Games
Let H be a graphical game of N players, where between every pair of Players i and k, the payoff
bimatrices are (Hik, (Hki)T). In the cumulative PayOff space, let P = (pi,…，PN) denote the cu-
mulative payoff profile, and let X = (xi,…，XN) denote the corresponding mixed strategy profile,
where Xi is a function of Pi. We will write Xi and Xi(Pi) interchangeably. The expected payoff to
strategy j of Player i is
uij(p)
k∈[N]
k6=i
E Hik ∙ Xk(Pk)]j,
which will be used to compute the Jacobian matrices of MWU and OMWU.
For MWU, the Jacobian matrix J is a squared matrix with each row and each column indexed
by (i, j), where i is a Player and j ∈ Si. The precise values of its entries are given below:
∀j1,j2 ∈ Si,
(12)
and
∀i = k, j ∈ Si, ' ∈ Sk, eJ(i,j),(k,') = e ∙ -----ij = Cxk` ∙ (Hjk - [Hik ∙ χk]j) .	(13)
pk`
Then by expansion using Leibniz formula, the determinant of (I + c ∙ J) is
1-
i∈[N]
j∈Si
E (CJ(SG,'))(CJ(k,`),(i,j)) + O(C3)
k>i
'∈Sk
1 - C2 ∙ £ E xijxk' (Hji - [Hki ∙ Xi]') (Hjk- [Hik ∙ Xk]j)
i∈[N] k>i
j∈Si '∈Sk
+ O(C3).
(14)
By noting the similarity of the double summation to C(a,b)(∙) in Eqn. (4), We can immediately
rewrite the above expression as
1 + C2 ∙ X	C(Hik,(Hki)T)(Pi, pk) + O(C3).	(15)
i,k∙.1≤i<k≤N
D.4 OMWU in Graphical Games
For OMWU, as We pointed out already, We Will first consider its continuous analogue first. Thus,
we need to compute U in the continuous-time setting. By chain rule, we have
- g _	X d[Hik ∙ Xk (Pk )]j	dPk'	_	X ʃ (H ik rHik	γ 1 ) dPk'
uij (P)=	上----------而'---------dT	=	T xk' ∙ (Hj'	- [H	∙Xk j	∙k，
k∈[N]	k'	k∈[N]
k6=i	k6=i
'∈Sk	'∈Sk
and hence
⅛j = X [Hik ∙ Xk ]j + C ∙ X xk' ∙ (Hjk - [Hik ∙ Xk ]j) ∙ ⅛' ∙
k∈[N]	k∈[N]
k6=i	k6=i
'∈Sk
18
Published as a conference paper at ICLR 2021
Note that this is a recurrence formulae for dp. By iterating it12, We have
dptj = X [Hik ∙ Xkj + 〜X Xk' ∙ (H∣ - [Hik ∙ Xkj) ∙ X [Hkr∙ Xr]'	+ O(e2).
k∈[N]	k∈[N]	r∈[N]
k6=i	k6=i	r6=k
'∈Sk
Hence, its standard Euler discretization, Which approximates the OED With local error O(3), can
be Written as beloW (Where We ignore the O(3) error terms):
Pj(t+1) =	Pj(t)	+ e X Hik∙Xk]j	+ e2	X	Xk'∙(H∣	—	[Hik	∙ Xk j)∙ X Hkr	∙	Xr]`	.
k∈[N]	k∈[N]	r∈[N]
k6=i	k6=i	r6=k
'∈Sk
With this, We are ready to compute the Jacobian matrix J for OMWU. For all j1 , j2 ∈ Si ,
J(ijι),(ij2)	=	e2	X	xk'	∙	(Hik'	-	[Hik ∙ Xk jι) ∙	Xj2	∙	(H&	-	[Hk	∙	Xi]')	(16)
k∈[N]
k6=i
'∈Sk
and for all i 6= k, j ∈ Si, ` ∈ Sk,
J(ij),(k,') = ex®' (Hjk -Hik ∙ Xkj) + O(e2)	(17)
Then by expansion using Leibniz formula, the determinant of (I + e ∙ J) is
/ ∖
1+
i∈[N]
j∈Si
∖
eJ(i,j),(i,j)
-	(eJ(i,j),(k,'))(eJ(k,'),(i,j))
+ O(e3).
z
i∈[N] k>i
j∈Si '∈Sk
X---------------------
T2
}
/
By a direct expansions on T1 and T2, it is easy to see that T1 = 2T2 (after ignoring O(e3) terms). On
the other hand, the coefficient of e2 in T2 is exactly the same as the double summation in Eqn. (14),
thus it equals to 一 Pi kτ≤i<k≤N C(Hik,(Hki)T)(pi, Pk). Overall, we show that the determinant
equals to
1 - e2 ∙ X	C(Hik,(Hki)T)(Pi, pk) + O(E3).	(18)
i,k = 1≤i<k≤N
Observation 22. The coefficient of e2 in Eqn. (18) is the exact negation of the coefficient of e2 in
Eqn. (15).
D.5 Completing the Local Equivalence Proof
In a multiplayer normal-form game G, recall that notation Eqn. (1). We point out the following
formulae:
if i ∈ {i1,i2, ∙∙∙ ,ig}；
if i ∈ {i1, i2,…，ig}.
MWU. Here, MWU update rule is Pj(t + 1) = Pj(t) + e ∙ Uj. When computing the Jacobian
matrix for this update rule using the formulae above, and comparing it with the Jacobian matrix
computed in Eqn. (12) and Eqn. (13), it is immediate that they are the same by setting Hji'k = Uji'k.
This derives Eqn. (5), and completes the proof of Theorem 19.
12For the formality on why we can do iterations when is sufficiently small, see Cheung & Piliouras (2020).
19
Published as a conference paper at ICLR 2021
OMWU. As before, We use the continuous analogue and compute U. By the chain rule and the
above formulae, we have
and hence
U ij (P)
dpk`
dt
E Xk' ∙ (Uj - Uj)
k∈[N]
k6=i
'∈Sk
dpk`
dt
X ∂UL
k∈[N] dpk'
k6=i
'∈Sk
k∈[N]
k6=i
'∈Sk
dpk`
dt
Iterating the above recurrence yields
等=Uj + 〜X Xk' ∙ Mk- Uj ∙ U' + O(e2).
k∈[N]
k6=i
'∈Sk
Its standard Euler discretization is
Pij (t + I)= Pij ⑴ + C ∙ Uj +
C2 ∙ ∑χk' ∙ Mk
k∈[N]
k6=i
'∈Sk
—
NoW We compute the Jacobian matrix for this standard Euler discretization. For j1,j2 ∈ Si,
CJ(ijl),(ij2) = C2 E Xk' ∙ (U；k' - UjI) ∙ Xij2 ∙ (Uk2 - U')
k∈[N]
k6=i
'∈Sk
and for all i 6= k, j ∈ Si, ` ∈ Sk,
cJ(i,j),(k,') = cxk' (Uik - Uji) + O(C2).
By comparing this computed Jacobian matrix With the Jacobian matrix computed in Eqn. (16) and
Eqn. (17), itis immediate to see that their determinants are the same (after ignoring all O(C3) terms)
by setting Hji'k = Uji'k . With the result We just derived, together With Observation 22 and Theo-
rem 19, Proposition 20 folloWs.
E Multi-player Potential Game
Proof of Proposition 21. We knoW that the potential game satisfies the folloWing condition:
P(si, s-i) - P(s0i, s-i) = ui(si, s-i) - ui(si0, s-i).
Therefore, ui(si, s-i) = P(si, s-i) + vi(s-i). Note that vi(s-i) does not depend on si, the strategy
of player i.
By Theorem 19, let H(U) be the induced graphical game of U and H(UP) be the induced
graphical game of UP . Then,
CU(P) = CH(U)(P)
C(H(U)ik,(H(U)ki)T)(Pi, Pk)
i,k
C(H(UP)ik,(H(UP)ki)T)(Pi, Pk)
i,k
CH(UP)(P)
CUP (P).
(Theorem 19)
(By equation 15)
(see explanation beloW)
(By equation 15)
(Theorem 19)
20
Published as a conference paper at ICLR 2021
The third equality holds as the difference between H(U)ik and H(UP)ik is a trivial matrix:
H(U)jk = Ujk = (UP)jk + E-(i,k) [vi(s-i)] = H(UP)jk + E-(i,k) [vi(s-i)];
where 13 E-(i,k) vi(s-i) doesn’t depend on j, the strategy of player i, and only depends on l, the
strategy of player k. The same argument applies for (H(U)ki)T and (H(UP)ki)T.
To see CU (p) ≤ 0, observe that the induced graphical game of UP between player
i and k, (H(UP)ik, (H(UP)ki)T), is also a bimatrix coordination game, which implies
C(H(UP )ik,(H(UP )ki)T) (∙) ≤ 0. As CU(P) = ∑2i,k C(H(UP )ik,(H(UP )ki)T)(Pi, Pk ), the result fol-
lows.	□
Next, we identify several cases such that CU (P) is strictly negative in the region
Sδ = {x∣∀i,j Xij > δ}.
The conditions we pose are on the corresponding potential function P. Note that H(UP)ik,
the induced edge-game between player i and k, is also a coordination game, i.e. H(UP)ik =
(H(UP)ki)T.
•	Case 1:
min
x,g,h
Σ
1≤i<k≤N
X	(Pjik - gk -h'k )2 ≥ θ,
j∈Si,'∈Sk
where Pk = Es-(i,k) [P(Si = j, Sk = ', s-(i,k))]. With this condition, we can prove that
CU(P) ≤ -θδ2 for any P in Sδ. One key observation for this is true is that
CU(P) =	C(H(UP)ik,(H(UP)ki)T) (Pi, Pk)
i,k
=-XXXij(Pi)Xk`(Pk) (Pjk- gjk - hj'),
as H(UP)ik = UPik = Pik.
•	Case 2:
If U is a graphical game, then if there exists a pair of player i1 and i2, such that the game
between i1 and i2 is a non-trivial game, then CU will be strictly negative in Sδ .
•	Case 3:
Consider the payoff matrix of UP , the coordination game, between players i1 and i2 given
a strategy profile of the other players. There are total Qi6=i ,i ni such matrices, one for
each strategy profile of the other players, and each matrix is of dimension ni1 × ni2 . We
call these matrices the projected matrices for players i1, i2.
Let M denote the matrix space of ni1 × ni2 . On the other hand, trivial matrices form a
subspace of dimension ni1 + ni2 - 1.14 Let’s call this the trivial space, denoted by T.
We consider the direct decomposition M = T ㊉ V. Let a set of bases of M be
Bi, B2, B3, ∙∙∙ , Bnilni2, where the first niι + ni2 - 1 bases form a basis of T, and the
remaining bases form a basis of V . Without loss of generality, we assume that all bases are
of L2 norm 1.15
13E-(i,k) vi(s-i) is the expectation over all the strategies taken by the players other than i and k and
vi (s-i) does not depend on the strategy taken by player i.
14Recall that a trivial matrix T can be represented as {uj + vk}j,k. Consider the natural linear map L
such that L(υ1,υ2, •…,uniɪ ,v1,v2, •…,Vni2) maps to the trivial matrix T. Note that the kernel of L is of
dimension 1, since if L(uι,u2, •…,Uniγ ,v1,v2, •…,Vnill) is the zero matrix, then we must have Vk = -Uj
for all j, k, and hence the kernel of L must be the span of the vector (1,1,… ,1, —1, —1,… ,—1). Thus, the
V---{z---} V-----{z-----}
the u part	the v part
dimension of all trivial matrices is the dimension of the domain of L, which is ni1 + ni2 , minus the dimension
of the kernel of L.
15Here, the norm is defined w.r.t. the standard Frobenius matrix inner product.
21
Published as a conference paper at ICLR 2021
Given the above-mentioned bases of M, each of the projected matrices can be written
into a unique linear combination of these bases. Now, suppose there is a base b` for
l ≥ ni1 +ni2 (i.e. this base is in the set of bases for V), such that all projected matrices have
non-positive (or non-negative) coefficients of this base, and at least one of these projected
matrices (which we call a special projected matrix) has strictly negative (or strictly positive)
coefficient of the base. Then we claim that C(H(UP)i1 i2,(H(UP)i2 i1)T) (pi1 , pi2) will be
strictly negative in Sδ. This is because H(UP)i1 i2 is a convex combination of all those
projected matrices, and by our assumption above, when H(UP)i1 i2 is expressed as the
linear combinations of the bases of M, the coefficient of b` is strictly negative (or strictly
positive), thus H(UP)i1 i2 cannot be a trivial matrix.
Suppose further that there exists θ > 0 such that a special projected matrix has negative (or
positive) coefficient for b` which is smaller (or bigger) than -θ (or θ), then we are guar-
anteed that H(UP)i1 i2 is bounded away from T for a distance of θδN-2,16 and hence as
the calculations below show, C(H(UP)i1 i2,(H(UP)i2 i1)T) (pi1 , pi2) ≤ -θ2δ2N-2. If there
exists a pair of player i1 and i2 such that this condition holds, then CU ≤ -θ2δ2N-2.
C(H(UP)i1 i2,(H(UP)i2 i1)T) (pi1, pi2)
= -minXxi1j(pi1)xi2,l(pi2)(H(UP)i1i2 -gj - hk)2
g, jl
≤ - min δ2 X(H(UP)i1i2 -gj -hk)2
g, jl
=-δ2 X(H(UP)i1i2 - gj-h)2
jl
≤ - δ2(θδN-2)2,
where {gjj + hjk}jk is projection of H(UP)i1 i2 on the trivial space. The first inequality
follows as p ∈ Sδ ; the second equality holds as the projection minimizing the distance to
the trivial space, and the final inequality comes from the distance from H(UP)i1 i2 to the
trivial space.
For all these cases, we can have OMWU is CU to be strictly negative in domain Sδ , which
implies OMWU is Lyapunov chaotic in Sδ .
16To see why, when the coefficient for B' is bounded away from zero, we are guaranteed that the special
projected matrix has a strictly positive distance from T, and this distance is at least θ. Then H(UP)i1 i2 ,
which is a convex combination of all projected matrices where each projected matrix (in particular, the special
projected matrix) has a weight at least δN-2, has a strictly positive distance from T too, which is at least
22