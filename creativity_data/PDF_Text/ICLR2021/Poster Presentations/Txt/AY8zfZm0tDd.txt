Published as a conference paper at ICLR 2021
Randomized Ensembled Double Q-Learning:
Learning Fast Without a Model
Xinyue Chen1*
Che Wang1,2*
Zijian Zhou1*
Keith Ross1,2 *
1	New York University Shanghai
2	New York University
Ab stract
Using a high Update-To-Data (UTD) ratio, model-based methods have recently
achieved much higher sample efficiency than previous model-free methods for
continuous-action DRL benchmarks. In this paper, we introduce a simple model-
free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show
that its performance is just as good as, if not better than, a state-of-the-art model-
based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this
performance using fewer parameters than the model-based method, and with less
wall-clock run time. REDQ has three carefully integrated ingredients which allow
it to achieve its high performance: (i) a UTD ratio 1; (ii) an ensemble of Q
functions; (iii) in-target minimization across a random subset of Q functions from
the ensemble. Through carefully designed experiments, we provide a detailed
analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is
the first successful model-free DRL algorithm for continuous-action spaces using
a UTD ratio 1.
1	Introduction
Recently, model-based methods in continuous action space domains have achieved much higher
sample efficiency than previous model-free methods. Model-based methods often attain higher sam-
ple efficiency by using a high Update-To-Data (UTD) ratio, which is the number of updates taken
by the agent compared to the number of actual interactions with the environment. For example,
Model-Based Policy Optimization (MBPO) (Janner et al., 2019), is a state-of-the-art model-based
algorithm which updates the agent with a mix of real data from the environment and “fake” data
from its model, and uses a large UTD ratio of 20-40. Compared to Soft-Actor-Critic (SAC), which
is model-free and uses a UTD of 1, MBPO achieves much higher sample efficiency in the Ope-
nAI MuJoCo benchmark (Todorov et al., 2012; Brockman et al., 2016). This raises the question of
whether it is also possible to achieve such high performance without a model?
In this paper, we introduce a simple model-free algorithm called Randomized Ensemble Double Q
learning (REDQ), and show that its performance is just as good as, if not better than, MBPO. The
result indicates, that at least for the MuJoCo benchmark, simple model-free algorithms can attain
the performance of current state-of-the-art model-based algorithms. Moreover, REDQ can achieve
this performance using fewer parameters than MBPO, and with less wall-clock run time.
Like MBPO, REDQ employs a UTD ratio that is 1, but unlike MBPO it is model-free, has no
roll outs, and performs all updates with real data. In addition to using a UTD ratio that is 1, it has
two other carefully integrated ingredients: an ensemble of Q functions; and in-target minimization
across a random subset of Q functions from the ensemble.
Through carefully designed experiments, we provide a detailed analysis of REDQ. We introduce the
metrics of average Q-function bias and standard deviation (std) of Q-function bias. Our results show
that using ensembles with in-target minimization reduces the std of the Q-function bias to close to
* Equal contribution, in alphabetical order.
^Correspondence to: KeithRoss <keithwross@nyu.edu>.
1
Published as a conference paper at ICLR 2021
zero for most of training, even when the UTD is very high. Furthermore, by adjusting the number of
randomly selected Q-functions for in-target minimization, REDQ can control the average Q-function
bias. In comparison with standard ensemble averaging and with SAC with a higher UTD, REDQ
has much lower std of Q-function bias while maintaining an average bias that is negative but close to
zero throughout most of training, resulting in significantly better learning performance. We perform
an ablation study, and show that REDQ is very robust to choices of hyperparameters, and can work
well with a small ensemble and a small number of Q functions in the in-target minimization. We
also provide a theoretical analysis, providing additional insights into REDQ. Finally, we consider
combining the REDQ algorithm with an online feature extractor network (OFENet) (Ota et al.,
2020) to further improve performance, particularly for the more challenging environments Ant and
Humanoid. We achieve more than 7x the sample efficiency of SAC to reach a score of 5000 for both
Ant and Humanoid. In Humanoid, REDQ-OFE also greatly outperforms MBPO, reaching a score
of 5000 at 150K interactions, which is 3x MBPO’s score at that point.
To ensure our comparisons are fair, and to ensure our results are reproducible (Henderson et al.,
2018; Islam et al., 2017; Duan et al., 2016), we provide open source code1. For all algorithmic
comparisons, we use the same codebase (except for MBPO, for which we use the authors’ code).
2	Randomized Ensembled Double Q-learning (REDQ)
Janner et al. (2019) proposed Model-Based Policy Optimization (MBPO), which was shown to
be much more sample efficient than popular model-free algorithms such as SAC and PPO for the
MuJoCo environments. MBPO learns a model, and generates “fake data” from its model as well
as “real data” through environment interactions. It then performs parameter updates using both the
fake and the real data. One of the distinguishing features of MBPO is that it has a UTD ratio 1
for updating its Q functions, enabling MBPO to achieve high sample efficiency.
We propose Randomized Ensembled Double Q-learning (REDQ), a novel model-free algorithm
whose sample-efficiency performance is just as good as, if not better than, the state-of-the-art model-
based algorithm for the MuJoCo benchmark. The pseudocode for REDQ is shown in Algorithm
1. REDQ can be used with any standard off-policy model-free algorithm, such as SAC (Haarnoja
et al., 2018b), SOP (Wang et al., 2019), TD3 (Fujimoto et al., 2018), or DDPG (Lillicrap et al.,
2015). For the sake of concreteness, we use SAC in Algorithm 1. REDQ has the following key
components: (i) To improve sample efficiency, the UTD ratio G is much greater than one; (ii) To
reduce the variance in the Q-function estimate, REDQ uses an ensemble of N Q-functions, with
each Q-function randomly and independently initialized but updated with the same target; (iii) To
reduce over-estimation bias, the target for the Q-function includes a minimization over a random
subset M of the N Q-functions. The size of the subset M is kept fixed, and is denoted as M , and is
referred to as the in-target minimization parameter. Since our default choice for M is M = 2, we
refer to the algorithm as Randomized Ensembled Double Q-learning (REDQ).
REDQ shares some similarities with Maxmin Q-learning (Lan et al., 2020), which also uses ensem-
bles and also minimizes over multiple Q-functions in the target. However, Maxmin Q-learning and
REDQ have many differences, e.g., Maxmin Q-learning minimizes over the full ensemble in the
target, whereas REDQ minimizes over a random subset of Q-functions. Unlike Maxmin Q-learning,
REDQ controls over-estimation bias and variance of the Q estimate by separately setting M and N .
REDQ has many possible variations, some of which are discussed in the ablation section.
REDQ has three key hyperparameters, G, N, and M. When N = M = 2 and G = 1, then
REDQ simply becomes the underlying off-policy algorithm such as SAC. When N = M > 2 and
G = 1, then REDQ is similar to, but not equivalent to, Maxmin Q-learning (Lan et al., 2020). In
practice, we find M = 2 works well for REDQ, and that a wide range of values around N = 10 and
G = 20 work well. To our knowledge, REDQ is the first successful model-free DRL algorithm for
continuous-action spaces using a UTD ratio G 1.
1Code and implementation tutorial can be found at: https://github.com/watchernyu/REDQ
2
Published as a conference paper at ICLR 2021
Algorithm 1 Randomized Ensembled Double Q-learning (REDQ)
1:	Initialize policy parameters θ, N Q-function parameters φi, i = 1, . . . , N, empty replay buffer
D. Set target parameters φtarg,i J φi, for i = 1, 2,...,N
2:	repeat
3:	Take one action at 〜 ∏θ(∙∣st). Observe reward rt, new state st+ι.
4:	Add data to buffer: D J D ∪ {(st, at, rt, st+1)}
5:	for G updates do
6:	Sample a mini-batch B = {(s, a, r, s0)} from D
7:	Sample a set M of M distinct indices from {1, 2, . . . , N}
8:	Compute the Q target y (same for all of the N Q-functions):
y = r + Y (min Qφtarg,i (s∖a") — α log ∏ (a0 ∣ s0)J , W 〜∏θ (∙ ∣ s0)
9:
10:
for i = 1, . . . , N do
Update φi with gradient descent using
▽° ∣B∣	x	(QΦi (s, a)—y)2
(s,a,r,s0 )∈B
11:	Update target networks with φtarg,i J ρφtarg,i + (1 - ρ)φi
12:	Update policy parameters θ with gradient ascent using
s)) — αlogπθ (aWθ(s)∣s)
aθ(S)〜∏θ(∙ ∣ S)
2.1 Experimental Results for REDQ
We now provide experimental results for REDQ and MBPO for the four most challenging MuJoCo
environments, namely, Hopper, Walker2d, Ant, and Humanoid. We have taken great care to make
a fair comparison of REDQ and MBPO. The MBPO results are reproduced using the author’s open
source code, and we use the hyperparameters suggested in the MBPO paper, including G = 20. We
obtain MBPO results similar to those reported in the MBPO paper. For REDQ, we use G = 20,
N = 10, and M = 2 for all environments. We use the evaluation protocol proposed in the MBPO
paper. Specifically, after every epoch we run one test episode with the current policy and record the
performance as the undiscounted sum of all the rewards in the episode. A more detailed discussion
on hyperparameters and implementation details is given in the Appendix.
Figure 1 shows the training curves for REDQ, MBPO, and SAC. For each algorithm, we plot the
average return of 5 independent trials as the solid curve, and plot the standard deviation across 5
seeds as the transparent shaded region. For each environment, we train each algorithm for exactly
the same number of environment interactions as done in the MBPO paper. Figure 1 shows that
both REDQ and MBPO learn much faster than SAC, with REDQ performing somewhat better than
MBPO on the whole. In particular, REDQ learns significantly faster for Hopper, and has somewhat
better asymptotic performance for Hopper, Walker2d, and Humanoid. A more detailed performance
comparison is given in the Appendix, where it is shown that, averaging across the environments,
REDQ performs 1.4x better than MBPO half-way through training and 1.1x better at the end of
training. These results taken together are perhaps counter-intuitive. They show that a simple model-
free algorithm can achieve as good or better sample-efficiency performance as the state-of-the-art
model-based algorithm for the MuJoCo environments.
Does REDQ achieve its sample efficiency using more computational resources than MBPO? We
now compare the number of parameters used in REDQ and MBPO. With REDQ, for each Q net-
work and the policy network, we use a multi-layer perceptron with two hidden layers, each with 256
units. For MBPO, we use the default network architectures for the Q networks, policy network, and
model ensembles (Janner et al., 2019). The Appendix provides a table comparing the number of pa-
rameters: REDQ uses fewer parameters than MBPO for all four environments, specifically, between
3
Published as a conference paper at ICLR 2021
(a) Hopper	(b) Walker2d
(c) Ant	(d) Humanoid
Figure 1: REDQ compared to MBPO and SAC. Both REDQ and MBPO use G = 20.
26% and 70% as many parameters depending on the environment. Additionally, we measured the
runtime on a 2080-Ti GPU and found that MBPO roughly takes 75% longer. In summary, the results
in this section show that the model-free algorithm REDQ is not only at least as sample efficient as
MBPO, but also has fewer parameters and is significantly faster in terms of wall-clock time.
3 Why does REDQ succeed whereas others fail?
REDQ is a simple model-free algorithm that matches the performance of a state-of-the-art model-
based algorithm. Key to REDQ’s sample efficiency is using a UTD 1. Why is it that SAC and
ordinary ensemble averaging (AVG) cannot do as well as REDQ by simply increasing the UTD?
To address these questions, let Qπ (s, a) be the action-value function for policy π using the standard
infinite-horizon discounted return definition. Let Qφ (s, a) be an estimate of Qπ (s, a), which is
defined as the average of Qφi (s, a), i = 1, . . . , N, when using an ensemble. We define the bias
of an estimate at state-action pair (s, a) to be Qφ (s, a) - Qπ (s, a). We are primarily interested in
the accuracy of Qφ(s, a) over the state-action distribution of the current policy π. To quantitatively
analyze how estimation error accumulates in the training process, we perform an analysis that is
similar to previous work (Van Hasselt et al., 2016; Fujimoto et al., 2018), but not exactly the same.
We run a number of analysis episodes from different random initial states using the current policy
π . For each state-action pair visited, we obtain both the discounted Monte Carlo return and the
estimated Q value using Qφ , and then compute the difference to obtain an estimate of the bias for
that state-action pair. We then calculate the average and std of these bias values. The average gives
us an idea of whether Qφ is in general overestimating or underestimating, and the std measures how
uniform the bias is across different state-action pairs. We argue that the std is just as important as
the average of the bias. As discussed in Van Hasselt et al. (2016), a uniform bias is not necessarily
harmful as it does not change the action selection. Thus near-uniform bias can be preferable to a
highly non-uniform bias with a small average value. Although average bias has been analyzed in
several previous works (Van Hasselt et al., 2016; Fujimoto et al., 2018; Anschel et al., 2017), the std
does not seem to have received much attention.
Since the MC return values can change significantly throughout training, to make comparisons
more meaningful, we define the normalized bias of the estimate Qφ (s, a) to be (Qφ (s, a) -
Qn(s,a))/∖Es,azπ [Qπ(s, α)] |, which is simply the bias divided by the absolute value of the ex-
pected discounted MC return for state-action pairs sampled from the current policy. We focus on the
normalized bias in our analysis since it helps show how large the bias is, compared to the scale of
the current MC return.
4
Published as a conference paper at ICLR 2021
In this and the subsequent section, we compare REDQ with several algorithms and variants. We
emphasize that all of the algorithms and variants use the same code base as used in the REDQ
experiments (including using SAC as the underlying off-policy algorithm). The only difference is
how the targets are calculated in lines 7 and 8 of Algorithm 1.
We first compare REDQ with two natural algorithms, which we call SAC-20 and ensemble averaging
(AVG). SAC-20 is SAC but with G increased from 1 (as in standard SAC) to 20. For AVG, we use
an ensemble of Q functions, and when computing the Q target, we take the average of all Q values
without any in-target minimization. In these comparisons, all three algorithms use a UTD of G = 20.
In the later ablation section we also have a detailed discussion on experimental results with Maxmin,
and explain why it does not work well for the MuJoCo benchmark when using a large ensemble.
Figure 2 presents the results for Ant; the results for the other three environments are consistent with
those for Ant and are shown in the Appendix. For each experiment we use 5 random seeds. We first
note REDQ learns significantly faster than both SAC-20 and AVG. Strikingly, relative to the other
two algorithms, REDQ has a very low normalized std of bias for most of training, indicating the bias
across different in-distribution state-action pairs is about the same. Furthermore, throughout most
of training, REDQ has a small and near-constant under-estimation bias. The shaded areas for mean
and std of bias are also smaller, indicating that REDQ is robust to random initial conditions.
(a) Performance, Ant
(b) Average bias, Ant
(c) Std of bias, Ant
Figure 2: Performance, mean and std of normalized Q bias for REDQ, AVG, and SAC for Ant.
Figures for the other three environments have similar trends and are shown in the Appendix.
SAC with a UTD ratio of 20 performs poorly for the most challenging environments Ant and Hu-
manoid. For SAC-20, the high UTD ratio leads to an average bias that fluctuates during training.
We also see a high normalized std of bias, indicating that the bias is highly non-uniform, which can
be detrimental. The bias values also have large variance across random initial seeds, as indicated by
the large shaded area, showing that the bias in SAC-20 is sensitive to initial conditions. Comparing
AVG and SAC-20, we see AVG performs significantly better than SAC-20 in Ant and Humanoid.
This can be explained again by the bias: due to ensemble averaging, AVG can achieve a lower std
of bias; and when it does, its performance improves significantly faster than SAC-20.
REDQ has two critical components that allow it to maintain stable and near-uniform bias under
high UTD ratios: an ensemble and in-target minimization. AVG and SAC-20 each has one of these
components but neither has both. Thus the success of REDQ is largely due to a careful integration of
both of these critical components. Additionally, as shown in the ablation study, the random selection
of Q functions in the in-target minimization can give REDQ a further performance boost.
3.1 Theoretical Analysis
We now characterize the relation between the estimation error, the in-target minimization parameter
M and the size of the ensemble N . We use the theoretical framework introduced in Thrun &
Schwartz (1993) and extended in Lan et al. (2020). We do this for the tabular version of REDQ, for
which the target for Qi(s, a) for each i = 1, . . . , N is:
r + γmax min Qj(s0, a0)	(1)
a0∈Aj∈M
where A is the finite action space, (s, a, r, s0) is a transition, and M is again a uniformly random
subset from {1, . . . , N} with |M| = M. The complete pseudocode for tabular REDQ is provided
in the Appendix.
5
Published as a conference paper at ICLR 2021
Let Qi (s, a) - Qπ (s, a) be the pre-update estimation bias for the ith Q-function, where Qπ (s, a)
is once again the ground-truth Q-value for the current policy π . We are interested in how the bias
changes after an update, and how this change is effected by M and N. Similar to Thrun & Schwartz
(1993) and Lan et al. (2020), define the post-update estimation bias as the difference between the
target (1) and the target when using the ground-truth:
ZM N , r + γ max min Qj(s0, a0) - (r + γ max Qπ(s0, a0))
,	a0∈Aj∈M	a0∈A
= γ (max min Qj(s0, a0) - max Qπ(s0, a0))
a0∈A j∈M	a0∈A
Here we write ZM,N to emphasize its dependence on both M and N. Following Thrun & Schwartz
(1993) and Lan et al. (2020), fix s and assume each Qi(s, a) has a random approximation error eisa:
Qi (s, a) = Qπ (s, a) + eisa
where for each fixed s, {eisa} are zero-mean independent random variables such that {eisa } are
identically distributed across i for each fixed (s, a) pair. Note that due to the zero-mean assumption,
the expected pre-update estimation bias is E[Qi(s, a) - Qπ(s, a)] = 0. Thus if E ZM,N > 0, then
the expected post-update bias is positive and there is a tendency for over-estimation accumulation;
and if E ZM,N < 0, then there is a tendency for under-estimation accumulation.
Theorem 1.	1. For any fixed M, E ZM,N does not depend on N.
2.	EZ1,N ≥ 0forallN ≥ 1.
3.	EZM+1,N ≤EZM,N for any M < N.
4.	Suppose that eisa ≤ c for some c > 0 for all s, a and i. Then there exists an M such that
for all N ≥ M, EZM,N < 0.
Points 2-4 of the Theorem indicate that we can control the expected post-update bias E ZM,N ,
bringing it from above zero (over estimation) to under zero (under estimation) by increasing M .
Moreover, from the first point, the expected bias only depends on M , which implies that increasing
N can reduce the variance of the ensemble average but does not change the expected post-update
bias. Thus, we can control the post-update bias with M and separately control the variance of the
average of the ensemble with N . Note that this is not possible for Maxmin Q-learning, for which
M = N , and thus increasing the ensemble size N will also decrease the post-update bias, potentially
making it more negative, which can be detrimental. Note that this also cannot be done for standard
ensemble averaging, for which there is no in-target minimization parameter M .
Note we make very weak assumptions on the distribution of the error term. Thrun & Schwartz (1993)
and Lan et al. (2020) make a strong assumption, namely, the error term is uniformly distributed.
Because our assumption is much weaker, our proof methodology in the Appendix is very different.
We also consider a variant of REDQ where instead of choosing a random set of size M in the target,
we calculate the target by taking the expected value over all possible subsets of size M. In this case,
the target in the tabular version becomes
Σ
B⊂N
|B|=M
YM,N = r(S, a) + Y JN^
M
max min Qj(s0, a0)
a0 j∈B
We write the target here as YM,N to emphasize its dependence on both M and N. We refer to this
variant of REDQ as “Weighted” since we can efficiently calculate the target as a weighted sum of a
re-ordering of the N Q-functions, as described in the Appendix.
The following theorem shows that the variance of this target goes to zero as N → ∞. We note,
however, that in practice, some variance in the target may be beneficial in reducing overfitting or
help exploration. We can retain some variance by keeping N finite or using the unweighted REDQ
scheme.
Let vM := Var(maxa0 minj∈B Qj(s0, a0)) for any subset B ⊂ N where |B| = M. (It is easily
seen that vM only depends on M and not only the specific elements of B.)
6
Published as a conference paper at ICLR 2021
Theorem 2.
Var(YM,N) ≤ GM (N)
for some function GM (N) satisfying
lim GM (N)
N→∞ M2vM/N
=1
Consequently,
lim Var(YM N) = 0
N→∞	,
Also in the Appendix we show that the tabular version of REDQ convergences to the optimal Q
function with probability one.
4 REDQ variants and ablations
In this section, we use ablations to provide further insight into REDQ. We focus on the Ant envi-
ronment. We first look at how the ensemble size N affects REDQ. The top row in Figure 3 shows
REDQ with N = 2, 3, 5, 10, 15. We can see that when we increase the ensemble size, we generally
get a more stable average bias, a lower std of bias, and stronger performance. The result shows that
even a small ensemble (e.g., N = 5) can greatly help in stabilizing bias accumulation when training
under high UTD.
(b) Average bias, Ant
REDQ-Ml.5
REDQ-M2
REDQ-M2.5
——REDQ-M3
——REDQ-M5
(a) Performance, Ant
(c) std of bias, Ant
(d) Performance, Ant
(e) Average Q bias, Ant
(f) std of Q Bias, Ant
m①」①6(0」①><
(g) Performance, Ant	(h) Average bias, Ant	(i) std of bias, Ant
Figure 3: REDQ ablation results for Ant. The top row shows the effect of the ensemble size N . The
middle row shows the effect of the in-target minimization parameter M . The bottom row compares
REDQ to several variants.
The middle row of Figure 3 shows how M , the in-target minimization parameter, can affect perfor-
mance. When M is not an integer, e.g., M = 1.5, for each update, with probability 0.5 only one
7
Published as a conference paper at ICLR 2021
randomly-chosen Q function is used in the target, and with probability 0.5, two randomly-chosen
functions are used. Similarly, for M = 2.5, for each update either two or three Q functions are used.
Consistent with the theoretical result in Theorem 1, by increasing M we lower the average bias.
When M gets too large, the Q estimate becomes too conservative and the large negative bias makes
learning difficult.
M = 2, which has the overall best performance, strikes a good balance between average bias (small
underestimation during most of training) and std of the bias (consistently small).
The bottom row of Figure 3 shows the results for different target computation methods. The Maxmin
curve in the figures is a variant based on Maxmin Q-learning, where the min of all the Q networks in
the ensemble is taken to compute the Q target. As the ensemble size increases, Maxmin Q-learning
shifts from overestimation to underestimation (Lan et al., 2020); Figure 3 shows Maxmin with N =
3 instead of N = 10, since a large N value will cause even more divergence of the Q values. When
varying the ensemble size of Maxmin, we see the same problem as shown in the middle row of
Figure 3. When we increase the ensemble size to be larger than 3, Maxmin starts to reduce the bias
so much that we get a highly negative Q bias, which accumulates quickly, leading to instability in the
Q networks and poor performance. In the Maxmin paper, it was mainly tested on Atari environments
with a small finite action space, in which case it provides good performance. Our results show that
when using environments with high-dimensional continuous action spaces, such as MuJoCo, the
rapid accumulation of (negative) bias becomes a problem. This result parallels some recent research
in offline (i.e., batch) DRL. In Agarwal et al. (2020), it is shown that with small finite action spaces,
naive offline training with deep Q-networks (DQN) only slightly reduces performance. However,
continuous action Q-learning based methods such as Deep Deterministic Policy Gradient and SAC
suffer much more from Q bias accumulation compared to discrete action methods. Recent work
shows that offline training with these methods often lead to poor performance, and can even entirely
diverge (Fujimoto et al., 2019; Kumar et al., 2019).
Random ensemble mixture (REM) is a method originally proposed to boost performance of DQN in
the discrete-action setting. REM uses the random convex combination of Q values to compute the
target: it is similar to ensemble average (AVG), but with more randomization (Agarwal et al., 2020).
For Weighted, the target is computed as the expectation of all the REDQ targets, where the expec-
tation is taken over all N -choose-2 pairs of Q-functions. This leads to a formula that is a weighted
sum of the ordered Q-functions, where the ordering is from the lowest to the highest Q value in the
ensemble, as described in the Appendix. Our baseline REDQ in Algorithm 1 can be considered as a
random-sample version of Weighted. For the MinPair REDQ variant, we divide the 10 Q networks
into 5 fixed pairs, and during an update we sample a pair of Q networks from these 5 fixed pairs.
From Figure 3 we see that REDQ and MinPair are the best and their performance is similar. For
Ant, the performance of Weighted is much lower than REDQ. However, as shown in the Appendix,
Weighted and REDQ have similar performance for the other three environments. The randomization
might help alleviate overfitting in the early stage, or improve exploration. REM has performance
similar to AVG, studied in Section 3. In terms of the Q bias, REM has a positive average bias,
while REDQ, MinPair, and Weighted all have a small negative average bias. Overall these results
indicate that the REDQ algorithm is robust across different mechanisms for choosing the functions,
and that randomly choosing the Q functions can sometimes boost performance. Additional results
and discussions are provided in the appendix.
4.1 Improving REDQ with auxiliary feature learning
We now investigate whether we can further improve the performance of REDQ by incorporating
better representation learning? Ota et al. (2020) recently proposed the online feature extractor net-
work (OFENet), which learns representation vectors from environment data, and provides them to
the agent as additional input, giving significant performance improvement.
Is it possible to further improve the performance of REDQ with OFENet? We trained an OFENet to-
gether with REDQ to provide extra input, giving the algorithm REDQ-OFE. We found that OFENet
did not help much for Hopper and Walker2d, which may be because REDQ already learns very
fast, leaving little room for improvement. But as shown in Figure 4, online feature extraction can
further improve REDQ performance for the more challenging environments Ant and Humanoid.
8
Published as a conference paper at ICLR 2021
REDQ-OFE achieves 7x the sample efficiency of SAC to reach 5000 on Ant and Humanoid, and
outperforms MBPO with 3.12x and 1.26x the performance of MBPO at 150K and 300K data, re-
spectively. A more detailed performance comparison table can be found in the Appendix.
(a) Performance, Ant	(b) Performance, Humanoid
Figure 4: Performance of REDQ, REDQ with OFE, and SAC.
5	Related Work
It has long been recognized that maximization bias in Q-learning can significantly impede learning.
Thrun & Schwartz (1993) first highlighted the existence of maximization bias. Van Hasselt (2010)
proposed Double Q-Learning to address maximization bias for the tabular case, and showed that in
general it leads to an under-estimation bias. Van Hasselt et al. (2016) showed that adding Double
Q-learning to deep Q networks (DQN) (Mnih et al., 2013; 2015) gives a major performance boost
for the Atari games benchmark. For continuous-action spaces, Fujimoto et al. (2018) introduced
clipped-double Q-learning (CDQ), which further reduces maximization bias and brings significant
improvements over the deep deterministic policy gradient (DDPG) algorithm (Lillicrap et al., 2015).
CDQ was later combined with entropy maximization in SAC to achieve even stronger performance
(Haarnoja et al., 2018a;b). Other bias reduction techniques include using bias-correction terms
(Lee et al., 2013), using weighted Q estimates (Zhang et al., 2017; Li & Hou, 2019), penalizing
deterministic policies at early stage of training (Fox et al., 2015), using multi-step methods (Meng
et al., 2020), performing weighted Bellman updates to mitigate error propagation (Lee et al., 2020),
and truncating sampled Q estimates with distributional networks (Kuznetsov et al., 2020).
It has also long been recognized that using ensembles can improve the performance of DRL algo-
rithms (FauBer & SChWenker, 2015; OSband et al., 2016). For Q-learning based methods, AnSChel
et al. (2017) use the average of multiple Q estimates to reduce variance. Agarwal et al. (2020) intro-
duCed Random Ensemble Mixture (REM), WhiCh enforCes optimal Bellman ConsistenCy on random
Convex Combinations of multiple Q estimates. Lan et al. (2020) introduCed Maxmin Q-learning, as
disCussed in SeCtions 2-4. Although in disCrete aCtion domains it has been found that fine-tuning
DQN variants, inCluding the UTD, Can boost performanCe, little experimental or theoretiCal analysis
is given to explain hoW this improvement is obtained (Kielak, 2020; van Hasselt et al., 2019).
To address some of the CritiCal issues in model-based learning (Langlois et al., 2019), reCent methods
suCh as MBPO Combine a model ensemble With a Carefully Controlled rollout horizon to obtain better
performanCe (Janner et al., 2019; BuCkman et al., 2018). These model-based methods Can also be
enhanCed With advanCed sampling (Zhang et al., 2020), bidireCtional models (Lai et al., 2020),
or baCkprop through the model (Clavera et al., 2020), and be analyzed through neW theoretiCal
frameWorks (RajesWaran et al., 2020; Dong et al., 2020).
6	Conclusion
The Contributions of this paper are as folloWs. (1) We propose a simple model-free algorithm that
attains sample effiCienCy that is as good as or better than state-of-the-art model-based algorithms for
the MuJoCo benChmark. This result indiCates that, at least for the MuJoCo benChmark, models may
not be neCessary for aChieving high sample effiCienCy. (2) Using Carefully designed experiments,
We explain Why REDQ suCCeeds When other model-free algorithms With high UTD ratios fail. (3)
Finally, We Combine REDQ With OFE, and shoW that REDQ-OFE Can learn extremely fast for the
Challenging environments Ant and Humanoid.
9
Published as a conference paper at ICLR 2021
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning (ICML), 2020.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization
for deep reinforcement learning. In International Conference on Machine Learning, pp. 176-185,
2017.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scien-
tific Belmont, MA, 1996.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems, pp. 8224-8234, 2018.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In Advances in Neural Information Processing Systems, pp. 1787-1798, 2019.
Ignasi Clavera, Violet Fu, and Pieter Abbeel. Model-augmented actor-critic: Backpropagating
through paths. arXiv preprint arXiv:2005.08068, 2020.
Kefan Dong, Yuping Luo, Tianhe Yu, Chelsea Finn, and Tengyu Ma. On the expressivity of neu-
ral networks for deep reinforcement learning. International Conference on Machine Learning
(ICML), 2020.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Stefan Fauβer and Friedhelm Schwenker. Neural network ensembles in reinforcement learning.
Neural Processing Letters, 41(1):55-69, 2015.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
ICLR, 2018.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. arXiv preprint arXiv:1512.08562, 2015.
Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018b.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artificial Intel-
ligence, 2018.
10
Published as a conference paper at ICLR 2021
Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of
benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint
arXiv:1708.04133, 2017.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems, pp. 12519-
12530, 2019.
Kacper Kielak. Do recent advancements in model-based deep reinforcement learning really improve
data efficiency? arXiv preprint arXiv:2003.10181, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, pp. 11784-11794, 2019.
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overes-
timation bias with truncated mixture of continuous distributional quantile critics. arXiv preprint
arXiv:2005.04269, 2020.
Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization.
arXiv preprint arXiv:2007.01995, 2020.
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling
the estimation bias of q-learning. arXiv preprint arXiv:2002.06487, 2020.
Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking
model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.
Donghun Lee, Boris Defourny, and Warren B Powell. Bias-corrected q-learning to control max-
operator bias in q-learning. In 2013 IEEE Symposium on Adaptive Dynamic Programming and
Reinforcement Learning (ADPRL), pp. 93-99. IEEE, 2013.
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified frame-
work for ensemble learning in deep reinforcement learning. arXiv preprint arXiv:2007.04938,
2020.
Zhunan Li and Xinwen Hou. Mixing update q-value for deep reinforcement learning. In 2019
International Joint Conference on Neural Networks (IJCNN), pp. 1-6. IEEE, 2019.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Lingheng Meng, Rob GorbeL and Dana KuliC. The effect of multi-step methods on overestimation
in deep reinforcement learning. arXiv preprint arXiv:2006.12692, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026-4034, 2016.
Kei Ota, Tomoaki Oiki, Devesh K Jha, Toshisada Mariyama, and Daniel Nikovski. Can increasing
input dimensionality improve deep reinforcement learning? arXiv preprint arXiv:2003.01629,
2020.
11
Published as a conference paper at ICLR 2021
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model
based reinforcement learning. arXiv preprint arXiv:2004.07804, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement
learning. In Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ.
Lawrence Erlbaum, 1993.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In InteUigentRobots and Systems (IROS), 2012IEEE/RSJ International Conference on,pp. 5026-
5033. IEEE, 2012.
John N Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16
(3):185-202, 1994.
Hado Van Hasselt. Double q-learning. In Advances in neural information processing systems, pp.
2613-2621, 2010.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, volume 2, pp. 5. Phoenix, AZ, 2016.
Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in rein-
forcement learning? In Advances in Neural Information Processing Systems, pp. 14322-14333,
2019.
Che Wang, Yanqiu Wu, Quan Vuong, and Keith Ross. Striving for simplicity and performance in
off-policy drl: Output normalization and non-uniform sampling. arXiv, pp. arXiv-1910, 2019.
Chi Zhang, Sanmukh Rao Kuppannagari, and Viktor K Prasanna. Maximum entropy model
rollouts: Fast model based policy optimization without compounding errors. arXiv preprint
arXiv:2006.04802, 2020.
Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double q-learning. In IJCAI,
pp. 3455-3461, 2017.
12
Published as a conference paper at ICLR 2021
A Theoretical Results
A.1 Tabular version of REDQ
In the tabular algorithm below, for clarity we use G = 1.
Algorithm 2 Tabular REDQ
1:	Initialize {Qi(s, a),s ∈ S,a ∈ A}N=j,
2:	repeat
3:	Choose a ∈ A based on Qi(s, a)}iN=1, observe r, s0
4:	Randomly choose a subset M of size M from 1, .., N
5:	y = r + γ maxa0∈A minj∈M Qj(s0, a0)
6:	for i = 1, . . . , N do:
7:	Qi(s, a) - Qi(s, a) + α(y - Qi(s, a))
8:	S J s0
9:	until end
Alternatively in Algorithm 2, at each iteration we could just update one of the Qi functions.
A.2 Proof of Theorem 1
We first prove the following lemma:
Lemma 1. Let X1, X2, . . . be an infinite sequence of i.i.d. random variables. Let F(x) be the cdf
of Xm and let τ = inf {x : F(x) > 0}. Also let Ym = min{X1, X2, . . . , Xm}. Then Y1, Y2, . . .
converges to τ almost surely.
Proof:
Let Fm(x) be the cdf of Ym. Since X1, ..., Xm are independent,
Fm(x) =1-[1-F(x)]m
For x < τ, Fm(x) = 0 since F (x) = 0. For x > τ, Fm(x) -m--→-∞→ 1. Therefore, Ym weakly
converges to τ .
Moreover, for each ω ∈ Ω, {Ym(ω)} is a decreasing sequence. So {Ym(ω)} either converges to a
real number or -∞. Therefore Ym → Y almost surely for some random variable Y . Combined
with the result that Ym -→d τ , we can conclude that
a.s.
Ym --→ τ
Proof of Theorem 1:
1.	Let B1, B2 be two subsets ofN = 1, ..., N} of size M. First of all, since {Qj (s, a)}jN=1 are
i.i.d for any a ∈ A, minj∈B1 Qj (s, a) and minj∈B2 Qj (s, a) are identically distributed. Further-
more, since Qj(s, a) are independent for all a ∈ A and 1 ≤ j ≤ N, minj∈B Qj (s, a) a∈A are
independent for any B ⊂ N. Denote the distribution function of maxa minj∈B1 Qj (s, a) as F1(x)
and the distribution function of maxa minj∈B2 Qj (s, a) as F2(x). Then for any x ∈ R,
Fι(x) = P( max min Qj(s, a) ≤ x) = P( ∩a∈A {min Qj(s, a) ≤ x})
a j∈B1	i∈B1
= Y P( min Qj(s, a) ≤ x) = Y P( min Qj(s, a) ≤ x)
j∈B1	j∈B2
a∈A	a∈A
= P max min Qj (s, a) ≤ x = F2(x)
a j∈B2
13
Published as a conference paper at ICLR 2021
Therefore, we have proved that maxa minj∈B1 Qj(s, a) and maxa minj∈B2 Qj (s, a) are identically
distributed. Then
E ZM,N = γE (max min Qj (s, a) - max Qπ (s, a))
1
γE[τNy E
M B⊂N
|B|=M
a
max min Qj (s, a) - γ maxQπ(s, a)
j∈B	a
γE
a
max min Qj (s, a) - max Qπ (s, a)
which does not depend on N .
2.	It follows from 1 that
E Z1,N = γ E max Q1 (s, a) - maxQπ(s, a)
Since maxa Q1 (s, a) ≥ Q1 (s, a0) for all a0 ∈ A, we have
EmaxQ1(s, a) ≥ EQ1(s, a0)
for all a0 ∈ A. Consequently,
E maxQ1(s, a) ≥ maxE Q1(s, a) = max Qπ (s, a)
Hence
EZ1,N = γ E max Q1 (s, a) - max Qπ (s, a) ≥ 0
3.	Since maxa min1≤j≤M Qj (s, a) ≥ maxa min1≤j ≤M +1 Qj (s, a),
E ZM N = γ E max min Qj (s, a) - max Qπ (s, a)
,	a 1≤j≤M	a
≥ γ E max min	Qj (s, a) - max Qπ (s, a) = E ZM+1 N
a 1≤j ≤M +1	a	,
4.	Let Fa(x) be the cdf of Qj (s, a) and let τa = inf{x : Fa(x) > 0}. Here we assume the
approximation error eisa is non-trivial, which implies τa < Qπ (s, a). Note that τa can be equal to
-∞. Let
min Qj (s, a)
1≤i≤M
From Lemma 1 we have YaM converges to τa almost surely for each a. Because the action space is
finite, it therefore follows that
YM = YM
= max a
a
converges almost surely to τ = maxa τa. Furthermore, for each a we have
YM = min Qj (s, a) ≥ min Qj (s, a) = YM+1
a	1≤j≤M	1≤j≤M+1	a
from which it follows that Y M ≥ Y M+1. Thus {Y M} is a monotonically decreasing sequence.
We also note that due to the assumption eisa ≤ c for all a and i, and because Qπ (s, a) is finite
for all s and a, it follows that Y M ≤ d for all M for a finite d. Thus {Y M} is a bounded-above,
monotonically-decreasing sequence of random variables which converges almost surely to τ . We
can therefore apply the monotone convergence theorem, giving
E ZMN = γ E max min Qj (s, a) - max Qπ (s, a)
,	a 1≤j≤M	a
γ E[max YaM] - max Qπ (s, a) -M--→-∞→ γ max τa - max Qπ (s, a)
a	a	aa
< 0,
where the last inequality follows from τa < Qπ (s, a) for all actions a.
14
Published as a conference paper at ICLR 2021
A.3 Proof of Theorem 2
For convenience, define YB = maxa0 minj∈B Qj(s0, a0). Suppose N > 2M.
γ2
Var(YM,N) = -N-2Var( E YB )
(M)	∣B∣⊂M
=——Y2(M!)2~J [ X Var(YB) + 2 ∙
(∏M-1(N -i))2 ∣B⅛∙
Cov(YB1,YB2)
B1,B2⊂N
B1 6=B2
Let A =	B1,B2⊂N Cov(YB1 , YB2), which consists of
B1 6=B2
1
N!
N!
2 ^ (N - M)!M! ∙ ((N - M)!M! - 1
2W 5N ”
N!
2 ∙ M!(N - M)!
terms. ((M)) can be seen as a polynomial function of N with degree 2M. The coefficient for the
term N2M is 2p^. The coefficient for the term N2M-1 is 2(My ∙ (-2 PM=-I i).
Note that YB1 and YB2 are independent if Bi ∩B2 = 0. The total number of different pairs (B1,B2)
such that Bi ∩ B2 = 0 is
N
2M
1
N!
1
1
2	2(M !)2	(N-2M)!	2(M !)2
• ∏2=0-1(N -i)
This is again a polynomial function of N with degree 2M . The coefficient of the term N2M is
2(，!)2. The coefficient of the term N2MT is 苏木∙ (-P2=0-1 i). So the number of non-zero
terms in A is at most
1
2(M !)2
M2
• ΠiM=-0 1(N - i)2 -
N!
-----------———:---~~r
2 ∙ M!(N - M)!	2(M!)2
• Πi2=M0-1(N - i)
2(M !)2
• N2M -1 + O(N2M -2)
1
Moreover, by Cauchy-Schwarz inequality, for any B1, B2 ⊂ N
C0v(YB1 ,YB2) ≤ PVar(YB J ∙ Var(YB2) = VM
Therefore,
A≤[
M2
2(M !)2
• N2M -1 + O(N2M -2)]vM
which implies
Var(YM,N)
(∏iM≡⅛ [ BXNVar(YB )+2 ∙ BiB Cov(γBι ,YB2)
B1 6=B2
Y 2(M !)2
(∏M-1(N -i))2
Var(YB) + 2A
B⊂N
N2M-i
≤ Y2 [M2 ∙ ^N-----+ O(
一 L	∏M-1(N - i)2	1
N→∞
vM --→ 0
Moreover,
lim ________M2vmIN__________= iim π=-1(N i2 = 1
N→∞ [M2 ∙ ∏MN2M-1 i)2 +O( N )]VM	N→∞	N2M
Πi=0 (N -i)
15
Published as a conference paper at ICLR 2021
A.4 Proof of convergence of tabular REDQ
Assuming that the step size satisfies the standard Robbins-Monro conditions, it is easily seen that
the tabular version of REDQ converges with probability 1 to the optimal Q function. In fact, for our
Weighted scheme, where we take the expectation over all sets of size M, the convergence conditions
in Lan et al. (2020) are fully satisfied.
For the randomized case, only very minor changes are needed in the proof in Lan et al. (2020). Note
that in the case of REDQ, the underlying deterministic target is:
F (q1,q2,...,qn)(s,a) = r(s,a)+YEp(S0 | s,a)
s0
1
Σ
B⊂N
|B|=M
max min Qj(S0, a0)
a0 j∈B
Let T be the operator that concatenates N identical copies of F, so that T: RS×A×N → RS ×A×N
where S and A are the cardinalities of the state and action spaces, respectively. It is easy to show
that the operator T is a contraction with the l∞ norm. The stochastic approximation noise term is
given by
ω(S, a) = R - r(S, a) + γ
maxmin Qj(s0,a0) - ^Xp (s0 | s,a) ^X —ɪ- maxmin Qj(s0,a0)]
j∈B	s0	B⊂N M	a0 j∈B
a0
|B|=M
It is straightforward to show
E 卜2(s,a) | Fpast] ≤ Var(R | s,a)+ max max (Qi(S0,a0))2
(2)
As in Lan et al. (2020), it follows from the contraction property and (2) that REDQ converges with
probability 1 to the optimal Q function (Tsitsiklis, 1994; Bertsekas & Tsitsiklis, 1996).
16
Published as a conference paper at ICLR 2021
B	Hyperparameters and implementation details
Since MBPO builds on top ofa SAC agent, to make our comparisons fair, meaningful, and consistent
with previous work, we make all SAC related hyperparameters exactly the same as used in the
MBPO paper (Janner et al., 2019). Table 1 gives a list of hyperparameter used in the experiments.
For all the REDQ curves reported in the results section, we use a Q network ensemble size N of
10. We use a UTD ratio G of 20 on the four MuJoCo environments, which is the same value that
was used in the MBPO paper. Thus most of the hyperparameters are made to be the same as in the
MBPO paper to ensure fairness and consistency in comparisons.
For all the algorithms and variants, we also first obtain 5000 data points by randomly sampling
actions from the action space without making parameter updates. In our experiments we found that
using a high UTD from the very beginning with a very small amount of data can easily lead to
complete divergence on SAC-20. Sampling a number of random datapoints at the start of training is
also a common technique that has been used in previous model-free as well as model-based works
(Haarnoja et al., 2018a; Fujimoto et al., 2018; Janner et al., 2019).
Table 1: REDQ hyperparameters
Parameter	Value
Shared	
optimizer	Adam (Kingma & Ba, 2014)
learning rate	3∙10-4
discount (Y)	0.99
target smoothing coefficient (ρ)	0.005
replay buffer size	106
number of hidden layers for all networks	2
number of hidden units per layer	256
mini-batch size	256
nonlinearity	ReLU
random starting data	5000
REDQ	
ensemble size N	10
in-target minimization parameter M	2
UTD ratio G	20
OFENet	
random starting data	20,000
OFENet number of pretraining updates	100,000
OFENet UTDratiO		4
For the REDQ-OFE experiments, we implemented a minimal version of the OFENet in the original
paper, with no batchnorm layers. We use the recommended hyperparameters as described in the
original paper (Ota et al., 2020). Compared to REDQ without OFENet, the main difference is we
now first collect 20,000 random data points (which is accounted for in the training curves), and then
pre-train the OFENet for 100,000 updates, with the same learning rate and batch size. We then train
OFENet together with REDQ agent, and the OFENet uses a UTD ratio of 4. We tried a simple
hyperparameter search on Ant with 200,000, 100,000 and 50,000 pre-train updates, and learning
rates of 1e-4, 3e-4, 5e-4, and a OFENet UTD of 1, 4 and 20. However, the results are not very
different. It is possible that better results can be obtained through a more extensive hyperparameter
search or other modifications.
B.1 Efficient calculation of target for Weighted version of REDQ
In section 4 we provided experimental results for the Weighted version of REDQ. Recall that in
this version, instead of sampling a random subset M in the target, we average over all subsets B in
17
Published as a conference paper at ICLR 2021
{1, . . . ,N} of size M:
y =r + Y PNJ X (mBn QΦtarg,i(S0,5
In practice, however, we do not need to sum over all N choose M subsets. Instead we can re-order
the indices so that
Qφtaig,i(s0, a0) ≤ Qφtaig,i+1 (s,,a,)
for i = 1, . . . , N - 1. After the re-ordering, we can use the identity:
1	1	N	-M+1	N	i
7Nj	X	( mW QΦtarg,i	(S0，aO))	=	7Nj	X	(M	- J	)	Q0targ,i	(S0,	aO)
M	B	M	i=1	-
18
Published as a conference paper at ICLR 2021
C S ample efficiency comparison for REDQ, SAC and MBPO
The sample efficiency claims made in the main paper are based on Table 2 and Table 3. Table 2
shows that compared to naive SAC, REDQ is much more sample efficient. REDQ reaches 3500
on Hopper with 8x sample efficiency, and reaches 5000 for Ant and Humanoid with 5x and 3.7x
sample efficiency. After adding OFE, this becomes more than 7x on Ant and Humanoid. If we
average all the numbers for the four environments, then REDQ is 5.0x as sample efficient, and 6.4x
after including OFE results.
Table 3 compares REDQ to SAC and MBPO. As in the MBPO paper, we train for 125K for Hopper,
and 300K for the other three environments (Janner et al., 2019). The numbers in Table 3 show the
performance when trained to half and to the full length of the MBPO training limits. When averaging
the numbers, we see that REDQ reaches 4.5x and 2.1x the performance of SAC at 150K and 300K.
REDQ is also stronger than MBPO, with 1.4x and 1.1x the performance of MBPO at 150K and
300K. If we include the results of REDQ-OFE, then the numbers become 5.5x and 2.3x the SAC
performance at 150K and 300K, and 1.8x and 1.2x the MBPO performance at 150 and 300K.
Table 2: Sample efficiency comparison of SAC and REDQ. The numbers show the amount of data
collected when the specified performance level is reached. The last two columns show how many
times REDQ and REDQ-OFE are more sample efficient than SAC in reaching that performance.
Score	SAC	REDQ	REDQ-OFE	REDQ faster	REDQ-OFE faster
Hopper at 3500	933K	116K	-	8.04	-
Walker2d at 3500	440K	141K	-	3.12	-
Ant at 5000	771K	153K	106K	5.04	7.27
Humanoid at 5000	945K	255K	134K	3.71	7.05
Table 3: Performance comparison of REDQ, REDQ-OFE, MBPO and SAC. The numbers show the
performance achieved when the specific amount of data is collected. The last two columns show the
ratio of REDQ or REDQ-OFE performance compared to SAC and MBPO performance.
Amount of data	SAC	MBPO	REDQ	REDQ/SAC	REDQ/MBPO
Hopper at 62K	594	1919	^3278	5.52	1.71
Hopper at 125K	2404	3131	3517	1.46	1.12
Walker2d at 150K	760	3308	3544	4.66	1.07
Walker2d at 300K	2556	3537	4589	1.80	1.30
Ant at 150K	1245	4388	4803	3.86	1.09
Ant at 300K	2485	5774	5369	2.16	0.93
Humanoid at 150K	674	1604	2641	3.92	1.65
Humanoid at 300K	1633	4199	4674	2.86	1.11
Amount of data	SAC	MBPO	REDQ-OFE	OFE/SAC	OFE/MBPO
Ant at 150K	1245	4388	^3524	4.44	1.26
Ant at 300K	2485	5774	6578	2.65	1.14
Humanoid at 150K	674	1604	5011	7.43	3.12
Humanoid at 300K	1633	4199	5309	3.25	1.26
19
Published as a conference paper at ICLR 2021
D Number of parameters comparis on
Table 4 gives the number of parameters for MBPO, REDQ and REDQ-OFE, for all four environ-
ments. As discussed in the main paper, REDQ uses fewer parameters than MBPO for all four
environments: between 26% and 70% as many parameters depending on the environment. After
adding OFENet, REDQ still uses fewer parameters than MBPO, with 80% and 35% as many pa-
rameters on Ant and Humanoid. In particular, it is surprising that REDQ-OFE can achieve a much
stronger result on Humanoid with much fewer parameters.
Table 4: Number of parameters in millions. REDQ uses the same network structure and ensemble
size for all four environments. The difference in the number of parameters comes from the fact that
the environments have very different observation and action dimensions, which will affect the size
of the input and output layers of the networks.
Algorithm
Hopper Walker2d Ant
Humanoid
MBPO	1.106M	1.144M
REDQ N = 10	0.769M 0.795M
REDQ-OFE N = 10 -	-
1.617M
1.066M
1.294M
7.087M
1.840M
2.460M
20
Published as a conference paper at ICLR 2021
E Additional results for REDQ, SAC-20, and AVG
Due to lack of space, Figure 2 in Section 3 only compared REDQ with SAC-20 and AVG for the
Ant environment. Figure 5 presents the results for all four environments. We can see that in all
four environments, REDQ has much stronger performance and much lower std of bias compared to
SAC-20 and AVG. Note in terms of average normalized bias, AVG is slightly closer to zero in Ant
compared to REDQ, and SAC-20 is a bit closer to zero in Humanoid compared to REDQ; however,
their std of normalized bias is consistently higher. This shows the importance of having a low std of
the bias in addition to a close-to-zero average bias.
(c) Std of bias, Hopper
(a) Performance, Hopper
(b) Average bias, Hopper
(d)	Performance, Walker2d
(e)	Average bias, Walker2d
(f)	Std of bias, Walker2d
(j) Performance, Humanoid
(k) Average bias, Humanoid
(l) Std of bias, Humanoid
Figure 5: Performance, mean and std of normalized Q bias for REDQ, AVG, and SAC. All three
algorithms have a UTD ratio of 20.
21
Published as a conference paper at ICLR 2021
F REDQ and SAC with and without policy delay
Note that in the REDQ pseudocode, the number of policy updates is always one for each data point
collected. We set the UTD ratio for the policy update to always be one in order to isolate the effect
of additional policy updates from Q updates. Note in this way, REDQ, SAC-20 and SAC-1 all take
the same number of policy updates. This helps show that the performance gain mainly comes from
the additional Q updates.
Having a lower number of policy updates can also be seen as a delayed policy update, or policy
delay, and is a method that has been used in previous works to improve learning stability (Fujimoto
et al., 2018). In this section we discuss how delayed policy update, or policy delay, impact the
performance of REDQ and SAC (with UTD of 20). Figure 6 compares REDQ and SAC-20 with and
without policy delay (NPD for no policy delay). We can see that having the policy delay consistently
makes the bias and std of bias lower and more stable, although they have a smaller effect on REDQ
than on SAC. Performance-wise SAC always gets a performance boost with policy delay, while
REDQ sees improvement in Hopper and Humanoid, and becomes slightly worse in Walker2d and
Ant. The results show that policy delay can be important under high UTD when the variance is
not properly controlled. However, with enough variance reduction, the effect of policy delay is
diminished, and in some cases having more policy update can give better performance.
22
Published as a conference paper at ICLR 2021
(a) Performance, Hopper
P①Z=BE-。U①69」①><
environment interactions
(d) Performance, Walker2d
environment interactions
(g) Performance, Ant
environment interactions
sra-q P ①Z=BE」。U ① 6(0」① ><
(e) Average bias, Walker2d
(h) Average bias, Ant
(k) Average bias, Humanoid
□
environment interactions
(j) Performance, Humanoid
Figure 6: Performance, mean
policy delay.
s(D-q P ①Z--(UEJOU φct2φ><
(c) Std of Bias, Hopper
(f) Std of Bias, Walker2d
(i) Std of bias, Ant
(l) Std of bias, Humanoid
std of normalized Q bias of REDQ
and SAC, with and without
23
Published as a conference paper at ICLR 2021
G REDQ and SAC with different UTD ratios
How do different UTD ratio values G impact the performance of REDQ and SAC? Figure 7 com-
pares the two algorithms under UTD ratio values of 1, 5, 10 and 20 for the Ant environment. The
results show that in the Ant environment, REDQ greatly benefits from larger UTD values, with UTD
of 20 giving the best result. For SAC, performance improves slightly for UTD ratios of 5 and 10,
but becomes much worse at 20. Looking at the normalized bias and the std of the bias, we see that
changing the UTD ratio does not change the values very much for REDQ, while for SAC, we see
that as the UTD ratio increases, both the mean and the std of the bias becomes larger and more
unstable.
(c) Std of bias, Ant
(a) Performance, Ant
environment interactions
(d) Performance, Ant
(b) Average bias, Ant
(e) Average bias, Ant
environment interactions
(f) Std of bias, Ant
Figure 7: Performance, mean and std of normalized Q bias for REDQ, and SAC, with different UTD
ratios, in Ant environment.
24
Published as a conference paper at ICLR 2021
H Additional results for Weighted variant
In this section we provide additional results for the Weighted variant. Figure 8 shows the perfor-
mance and bias comparison on all four environments. Results show that Weighted and REDQ have
similar average bias and std of bias. In terms of performance, Weighed is worse in Ant and Hop-
per, similar in Humanoid and slightly stronger in Walker2d. Overall REDQ seems to have stronger
performance and is more robust. Randomness in the networks might help alleviate overfitting in the
early stage, or improve exploration, as shown in previous studies (Osband et al., 2016; Fortunato
et al., 2018). This can be important since positive bias in Q learning-based methods can sometimes
help exploration. This is commonly referred to as optimistic initial values, or optimism in the face of
uncertainty (Sutton & Barto, 2018; Brafman & Tennenholtz, 2002). Thus conservative Q estimates
in recent algorithms can lead to the problem of pessimistic underexploration (Ciosek et al., 2019).
An interesting future work direction is to study how robust and effective exploration can be achieved
without relying on optimistic estimates.
(f) Std of bias, Walker2d
(d) Performance, Walker2d
(e) Average bias, Walker2d
(j) Performance, Humanoid
(k) Average bias, Humanoid
(l) Std of bias, Humanoid
Figure 8: Performance, mean and std of normalized Q bias for REDQ and Weighted.
25