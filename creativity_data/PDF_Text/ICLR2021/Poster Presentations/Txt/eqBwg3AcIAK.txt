Off-Dynamics Reinforcement Learning:
Training for Transfer with Domain Classifiers
Benjamin Eysenbach*
CMU, Google Brain
beysenba@cs.cmu.edu
Shreyas Chaudhari*	Swapnil Asawa*
CMU	University of Pittsburgh
shreyaschaudhari@cmu.edu swa12@pitt.edu
Sergey Levine
UC Berkeley, Google Brain
Ruslan Salakhutinov
CMU
Abstract
We propose a simple, practical, and intuitive approach for domain adaptation in
reinforcement learning. Our approach stems from the idea that the agent’s ex-
perience in the source domain should look similar to its experience in the target
domain. Building off of a probabilistic view of RL, we achieve this goal by com-
pensating for the difference in dynamics by modifying the reward function. This
modified reward function is simple to estimate by learning auxiliary classifiers that
distinguish source-domain transitions from target-domain transitions. Intuitively,
the agent is penalized for transitions that would indicate that the agent is interact-
ing with the source domain, rather than the target domain. Formally, we prove that
applying our method in the source domain is guaranteed to obtain a near-optimal
policy for the target domain, provided that the source and target domains satisfy a
lightweight assumption. Our approach is applicable to domains with continuous
states and actions and does not require learning an explicit model of the dynam-
ics. On discrete and continuous control tasks, we illustrate the mechanics of our
approach and demonstrate its scalability to high-dimensional tasks.
1 Introduction
Reinforcement learning (RL) can automate the acquisition of complex behavioral policies through
real-world trial-and-error experimentation. However, many domains where we would like to learn
policies are not amenable to such trial-and-error learning, because the errors are too costly: from
autonomous driving to flying airplanes to devising medical treatment plans, safety-critical RL prob-
lems necessitate some type of transfer learning, where a safer source domain, such as a simulator, is
used to train a policy that can then function effectively in a target domain. In this paper, we examine
a specific transfer learning scenario that we call domain adaptation, by analogy to domain adaptation
problems in computer vision (Csurka, 2017), where the training process in a source domain can be
modified so that the resulting policy is effective in a given target domain.
RL algorithms today require a large amount of
experience in the target domain. However, for
many tasks we may have access to a different
but structurally similar source domain. While
the source domain has different dynamics than
the target domain, experience in the source do-
main is much cheaper to collect. However,
transferring policies from one domain to an-
other is challenging because strategies which
are effective in the source domain may not be
effective in the target domain. For example, ag-
gressive driving may work well on a dry race-
track but fail catastrophically on an icy road.
Figure 1: Our method acquires a policy for the tar-
get domain by practicing in the source domain using a
(learned) modified reward function.
* Equal contribution.
1
While prior work has studied the domain adaptation of observations in RL (Bousmalis et al., 2018;
Ganin et al., 2016; Higgins et al., 2017), it ignores the domain adaptation of the dynamics.
This paper presents a simple approach for domain adaptation in RL, illustrated in Fig. 1. Our main
idea is that the agent’s experience in the source domain should look similar to its experience in the
target domain. Building off of a probabilistic view of RL, we formally show that we can achieve this
goal by compensating for the difference in dynamics by modifying the reward function. This mod-
ified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-
domain transitions from target-domain transitions. Because our method learns a classifier, rather
than a dynamics model, we expect it to handle high-dimensional tasks better than model-based
methods, a conjecture supported by experiments on the 111-dimensional Ant task. Unlike prior
work based on similar intuition (Koos et al., 2012; Wulfmeier et al., 2017b), a key contribution of
our work is a formal guarantee that our method yields a near-optimal policy for the target domain.
The main contribution of this work is an algorithm for domain adaptation to dynamics changes
in RL, based on the idea of compensating for differences in dynamics by modifying the reward
function. We call this algorithm Domain Adaptation with Rewards from Classifiers, or DARC for
short. DARC does not estimate transition probabilities, but rather modifies the reward function using
a pair of classifiers. We formally analyze the conditions under which our method produces near-
optimal policies for the target domain. On a range of discrete and continuous control tasks, we both
illustrate the mechanics of our approach and demonstrate its scalability to higher-dimensional tasks.
2	Related Work
While our work will focus on domain adaptation applied to RL, we start by reviewing more general
ideas in domain adaptation, and defer to Kouw & Loog (2019) for a recent review of the field. Two
common approaches to domain adaptation are importance weighting and domain-agnostic features.
Importance-weighting methods (e.g., (Zadrozny, 2004; Cortes & Mohri, 2014; Lipton et al., 2018))
estimate the likelihood ratio of examples under the target domain versus the source domain, and
use this ratio to re-weight examples sampled from the source domain. Similar to prior work on
importance weighting (Bickel et al., 2007; S0nderby et al., 2016; Mohamed & Lakshminarayanan,
2016; Uehara et al., 2016), our method will use a classifier to estimate a probability ratio. Since we
will need to estimate the density ratio of conditional distributions (transition probabilities), we will
learn two classifiers. Importantly, we will use the logarithm of the density ratio to modify the reward
function instead of weighting samples by the density ratio, which is often numerically unstable (see,
e.g., Schulman et al. (2017, §3)) and led to poor performance in our experiments.
Prior methods for applying domain adaptation to RL include approaches based on system iden-
tification, domain randomization, and observation adaptation. Perhaps the most established ap-
proach, system identification (Ljung, 1999), uses observed data to tune the parameters of a simu-
lator (Feldbaum, 1960; Werbos, 1989; Wittenmark, 1995; Ross & Bagnell, 2012; Tan et al., 2016;
Zhu et al., 2017b; Farchy et al., 2013) More recent work has successfully used this strategy to
bridge the sim2real gap (Chebotar et al., 2019; Rajeswaran et al., 2016). Closely related is work
on online system identification and meta-learning, which directly uses the inferred system param-
eters to update the policy (Yu et al., 2017; Clavera et al., 2018; Tanaskovic et al., 2013; Sastry &
Isidori, 1989). However, these approaches typically require either a model of the environment or a
manually-specified distribution over potential test-time dynamics, requirements that our method will
lift. Another approach, domain randomization, randomly samples the parameters of the source do-
main and then finds the best policy for this randomized environment (Sadeghi & Levine, 2016; Tobin
et al., 2017; Peng et al., 2018; Cutler et al., 2014). While often effective, this method is sensitive
to the choice of which parameters are randomized, and the distributions from which these simulator
parameters are sampled. A third approach, observation adaptation, modifies the observations of
the source domain to appear similar to those in the target domain (Fernando et al., 2013; Hoffman
et al., 2016; Wulfmeier et al., 2017a). While this approach has been successfully applied to video
games (Gamrian & Goldberg, 2018) and robot manipulation (Bousmalis et al., 2018), it ignores the
fact that the source and target domains may have differing dynamics.
Finally, our work is similar to prior work on transfer learning (Taylor & Stone, 2009) and meta-
learning in RL, but makes less strict assumptions than most prior work. For example, most work
on meta-RL (Killian et al., 2017; Duan et al., 2016; Mishra et al., 2017; Rakelly et al., 2019) and
2
some work on transfer learning (Perkins et al., 1999; Tanaka & Yamamura, 2003; Sunmola & Wyatt,
2006) assume that the agent has access to many source tasks, all drawn from the same distribution
as the target task. Selfridge et al. (1985); Madden & Howley (2004) assume a manually-specified
curriculum of tasks, Ravindran & Barto (2004) assume that the source and target domains have
the same dynamics locally, and Sherstov & Stone (2005) assume that the set of actions that are
useful in the source domain is the same as the set of actions that will be useful in the target do-
main. Our method does not require these assumptions, allowing it to successfully learn in settings
where these prior works would fail. For example, the assumption of Sherstov & Stone (2005) is
violated in our experiments with broken robots: actions which move a joint are useful in the source
domain (where the robot is fully-function) but not useful in the target domain (where that joint is
disabled). Our method will significantly outperform an importance weighting baseline (Lazaric,
2008). Unlike Vemula et al. (2020), our method does not require learning a dynamics model and is
applicable to stochastic environments and those with continuous states and actions. Our algorithm
bears a resemblance to that in Wulfmeier et al. (2017b), but a crucial algorithmic difference allows
us to prove that our method acquires a near-optimal policy in the target domain, and also leads to
improved performance empirically.
The theoretical derivation of our method is inspired by prior work which formulates control as a
problem of probabilistic inference (e.g., (Toussaint, 2009; Rawlik et al., 2013; Levine et al., 2018)).
Algorithms for model-based RL (e.g., (Deisenroth & Rasmussen, 2011; Hafner et al., 2018; Jan-
ner et al., 2019)) and off-policy RL (e.g., (Munos et al., 2016; Fujimoto et al., 2018; Dann et al.,
2014; Dudlk et al., 2011) similarly aim to improve the sample efficiency of RL, but do use the
source domain to accelerate learning. Our method is applicable to any maximum entropy RL algo-
rithm, including on-policy (Song et al., 2019), off-policy (Abdolmaleki et al., 2018; Haarnoja et al.,
2018), and model-based (Janner et al., 2019; Williams et al., 2015) algorithms. We will use the
SAC (Haarnoja et al., 2018) in our experiments and compare against model-based baselines.
3	Preliminaries
In this section, we introduce notation and formally define domain adaptation for RL. Our problem
setting will consider two MDPs: Msource represents the source domain (e.g., a practice facility,
simulator, or learned approximate model of the target domain) while Mtarget represents a the target
domain. We assume that the two domains have the same state space S, action space A, reward
function r, and initially state distribution p1 (s1); the only difference between the domains is the
dynamics,psource(st+1 | st, at) andptarget(st+1 | st, at). We will learn a Markovian policy πθ(a | s),
parametrized by θ. Our objective is to learn a policy π that maximizes the expected discounted sum
of rewards on Mtarget, Eπ,Mtarget [Pt γtr(st, at)]. We now formally define our problem setting:
Definition 1. Domain Adaptation for RL is the problem of using interactions in the source MDP
Msource together with a small number of interactions in the target MDP Mtarget to acquire a policy
that achieves high reward in the target MDP, Mtarget.
We will assume every transition with non-zero probability in the target domain will have non-zero
probability in the source domain:
ptarget(st+1	| st , at)	> 0	=⇒	psource (st+1	|	st ,	at)	> 0	for all st , st+1 ∈	S, at	∈	A.	(1)
This assumption is common in work on importance sampling (Koller & Friedman, 2009, §12.2.2),
and the converse need not hold: transitions that are possible in the source domain need not be
possible in the target domain. If this assumption did not hold, then the optimal policy for the target
domain might involve behaviors that are not possible in the source domain, so it is unclear how one
could learn a near-optimal policy by practicing in the source domain.
4	A Variational Perspective on Domain Adaptation in RL
The probabilistic inference interpretation of RL (Kappen, 2005; Todorov, 2007; Toussaint, 2009;
Ziebart, 2010; Rawlik et al., 2013; Levine, 2018) treats the reward function as defining a desired
distribution over trajectories. The agent’s task is to sample from this distribution by picking trajec-
tories with probability proportional to their exponentiated reward. This section will reinterpret this
model in the context of domain transfer, showing that domain adaptation of dynamics can be done
3
by modifying the rewards. To apply this model to domain adaptation, define p(τ) as the desired
distribution over trajectories in the target domain,
P(T) H PI(SI)
Yptarget(st+1 | st, at)
exp
and q(τ) as our agent’s distribution over trajectories in the source domain,
q(τ) = P1(s1)	Psource(st+1 | st, at)πθ(at | st).
t
As noted in Section 3, we assume both trajectory distributions have the same initial state distribution.
Our aim is to learn a policy whose behavior in the source domain both receives high reward and has
high likelihood under the target domain dynamics. We codify this objective by minimizing the
reverse KL divergence between these two distributions:
min	DKL(q	k P) = -Epsource	r(st, at)	+Hπ[at	|	st]	+ ∆r(st+1,	st,at)	+ c, (2)
WaIs)	L 丁	_
where
∆r(st+1, st ,at ) , log P(st+1 | st , at ) - log q(st+1 | st ,at ).
The constant c is the partition function of P(τ), which is independent of the policy and dynamics.
While ∆r is defined in terms of transition probabilities, in Sec. 5 we show how to estimate ∆r by
learning a classifier. We therefore call our method domain adaptation with rewards from classifiers
(DARC), and will use ∏Darc to refer to the policy that maximizes the objective in Eq. 2.
Where the source and target dynamics are equal, the correction term ∆r is zero and we recover
maximum entropy RL (Ziebart, 2010; Todorov, 2007). The reward correction is different from prior
work that adds logβ(a | s) to the reward to regularize the policy to be close to the behavior policy β
(e.g., Jaques et al. (2017); Abdolmaleki et al. (2018)). In the case where the source dynamics are not
equal to the true dynamics, this objective is not the same as maximum entropy RL on trajectories
sampled from the source domain. Instead, this objective suggests a corrective term ∆r that should be
added to the reward function to account for the discrepancy between the source and target dynamics.
The correction term, ∆r, is quite intuitive. If a transition (st , at , st+1) has equal probability in the
source and target domains, then ∆r(st , at ) = 0 so no correction is applied. For transitions that
are likely in the source but are unlikely in the target domain, ∆r < 0, the agent is penalized for
“exploiting” inaccuracies or discrepancies in the source domain by taking these transitions. For
the example environment in Figure 1, transitions through the center of the environment are blocked
in the target domain but not in the source domain. For these transitions, ∆r would serve as a
large penalty, discouraging the agent from taking these transitions and instead learning to navigate
around the wall. Appendix A presents additional interpretations of ∆r in terms of coding theory,
mutual information, and a constraint on the discrepancy between the source and target dynamics.
Appendix C discusses how prior work on domain agnostic feature learning can be viewed as a special
case of our framework.
4.1	Theoretical Guarantees
We now analyze when maximizing the modified reward r + ∆r in the source domain yields a near-
optimal policy for the target domain. Our proof relies on the following lightweight assumption:
Assumption 1. Let π* = arg max∏ Ep [£ r(st, at)] be the reward-maximizing policy in the target
domain. Then the expected reward in the source and target domains differs by at most 2R语，〃2:
IEpni [Xr(st,at)] — E∏*,pSget [Xr(st,at)]∣ ≤ 2RmαxW
The variable Rmax refers to the maximum entropy-regularized return of any trajectory. This assump-
tion says that the optimal policy in the target domain is still a good policy for the source domain,
and its expected reward is similar in both domains. We do not require that the opposite be true: the
optimal policy in the source domain does not need to receive high reward in the target domain. If
there are multiple optimal policies, we only require that this assumption hold for one of them. We
now state our main result:
4
Algorithm 1 Domain Adaptation with Rewards from Classifiers [DARC]
1:	Input: source MDP Msource and target Mtarget; ratio r of experience from source vs. target.
2:	Initialize: replay buffers for source and target transitions, Dsource , Dtarget; policy π; parameters
θ = (θSAS, θSA) for classifiers qθSAS (target | st,at, st+1) and qθSAS (target | st, at).
for t = 1,…,num iterations do
Dsource4-Dsource ∪ ROLLOUT(n, MSOUrce)
if t mod r = 0 then
Dtarget《-Dtarget ∪ ROLLOUT(n, ^Mtarget)
θ 一 θ - ηVθ '(θ)
r(st, at, st+ι) - r(st, at) + ∆r(st, at, st+ι)
∏ J MAXENT RL(∏, Dsource,彳)
3:
4:
5:
6:
7:
8:
9:
. Collect source data.
. Periodically, collect target data.
. Update both classifiers.
. ∆r is computed with Eq. 3.
10
return π
Theorem 4.1. Let πDARC be the policy that maximizes the modified (entropy-regularized) reward in
the source domain, let π* be the policy that maximizes the (unmodified, entropy-regularized) reward
in the target domain, and assume that π* satisfies Assumption 1. Then thefollowing holds:
Eptarget,∏Darc [X r(St, a/ + Hiatl stf| ≥ Eptarget,∏* [X r(St, at ) + HIatI st]] - 4Rmax p/e/2.
See Appendix B for the proof and definition of e. This result says that ∏Darc attains near-optimal
(entropy-regularized) reward on the target domain. Thus, we can expect that modifying the reward
function should allow us to adapt to different dynamics. The next section will present a practical
algorithm for acquiring ∏Darc by estimating and effectively maximizing the modified reward in the
source domain.
5	Domain Adaptation in RL with a Learned Reward
The variational perspective on model-based RL
in the previous section suggests that We should
modify the reward in the source domain by
adding ∆r. In this section we develop a practi-
cal algorithm for off-dynamics RL by showing
how ∆r can be estimated without learning an
explicit dynamics model.
To estimate ∆r, we will use a pair of (learned)
binary classifiers, which will infer whether
transitions came from the source or target do-
main. The key idea is that the transition proba-
bilities are related to the classifier probabilities via Bayes’ rule:
Source
Experience from Source Domain
(Limited) Experience
from Target Domain
Reward Correction,
∆r(s, a, sɔ
<⅛Cgent
Two Classifi
2. Update classifiers to
distinguish source vs.
target transitions.
3. Train agent on
experience from source
with modified rewards.
1. Collect experience from source and (limited) target.
Figure 2: Block diagram of DARC (Alg. 1)
p(target l St, at, St+1) = p(St+1 l St,at,target)p(St, at l target)p(target)/p(St, at, St+1).
、	一.「一	J
=ptarget(st+1 |st,at)
We estimate the term p(St, at l target) on the RHS via another classifier, p(target l St, at):
p(St, at l target)
p(target l St, at)p(St, at)
p(target)
Substituting these expression into our definition for ∆r and simplifying, we obtain an estimate for
∆r that depends solely on the predictions of these two classifiers:
∆r(st, at, st+ι) = logp(target ∣ st, at, st+ι) — logp(target ∣ st, at)
-logp(source ∣ st,at, st+ι) + logp(source ∣ st,at)	(3)
The .o.r.a. n. .g.e. terms are the difference in logits from the classifier conditioned on st, at, st+1, while
the blue terms are the difference in logits from the classifier conditioned on just st,at. Intuitively,
5
∆r answers the following question: for the task of predicting whether a transition came from the
source or target domain, how much better can you perform after observing st+1? We make this
connection precise in Appendix A.2 by relating ∆r to mutual information. Ablation experiments
(Fig. 7) confirm that both classifiers are important to the success of our method. The use of transition
classifiers makes our method look somewhat similar to adversarial imitation learning (Ho & Ermon,
2016; Fu et al., 2017). While our method is not solving an imitation learning problem (we do not
assume access to any expert experience), our method can be interpreted as learning a policy such that
the dynamics observed by that policy in the source domain imitate the dynamics of the target domain.
Algorithm Summary Our algorithm modifies an existing MaxEnt RL algorithm to additionally
learn two classifiers, qθSAS (target | st, at, st+1) and qθSA (target | st, at), parametrized by θSAS and
θSA respectively, to minimize the standard cross-entropy loss.
'SAS (θSAS ) , -EDtarget [log qθsAS (target 1 St ,at ,st+1)] - EDsource [lθg OθsA (SOUrCe 1 St ,at ,st+1)]
'SA (θSA) , -EDtarget [log qθsA (target 1 St ,at)] — EDsource [log -Θsa (SOurCe 1 St ,at )].
Our algorithm, Domain Adaptation with Rewards from Classifiers (DARC), is presented in Alg. 1
and illustrated in Fig. 2. To simplify notation, We define θ , (θsAs, θSA) and '(θ) , 'SAS (θsAs) +
'SA(θSA). At each iteration, We collect transitions from the source and (less frequently) target do-
main, storing the transitions in separate replay buffers. We then sample a batCh of experienCe from
both buffers to update the classifiers. We use the classifiers to modify the reWards from the source
domain, and apply MaxEnt RL to this experience. We use sAC (Haarnoja et al., 2018) as our
MaxEnt RL algorithm, but emphasize that DARC is applicable to any MaxEnt RL algorithm (e.g.,
on-policy, off-policy, and model-based). When training the classifiers, We add Gaussian input noise
to prevent overfitting to the small number of target-domain transitions (see Fig. 7 for an ablation).
6	Experiments
We start With a didactic experiment to build intuition for the mechanics of our method, and then
evaluate on more complex tasks. Our experiments Will shoW that DARC outperforms alternative ap-
proaches, such as directly applying RL to the target domain or learning importance Weights. We Will
also shoW that our method can account for domain shift in the termination condition, and confirm
the importance of learning tWo classifiers.
Illustrative example. We start With a simple
gridWorld example, shoWn on the right, Where
We can apply our method Without function ap-
proximation. The goal is to navigate from the
top left to the bottom left. The real environment
contains an obstacle (shoWn in red), Which is
not present in the source domain. If We sim-
ply apply RL on the source domain, We obtain
Figure 3: Tabular example of off-dynamics RL
a policy that navigates directly to the goal (blue arroWs), and Will fail When used in the target do-
main. We then apply our method: We collect trajectories from the source domain and real World to
fit the tWo tabular classifiers. These classifiers give us a modified reWard, Which We use to learn a
policy in the source domain. The modified reWard causes our learned policy to navigate around the
obstacle, Which succeeds in the target environment.
Visualizing the reward modification in
stochastic domains. In our next experiment,
We use an “archery” task to visualize hoW the
modified reWard accounts for differences in dy-
namics. The task, shoWn in Fig. 4, requires
choosing an angle at Which to shoot an arroW.
The practice range (i.e., the source domain) is
outdoors, With Wind that usually bloWs from
left to right. The competition range (i.e., the
target domain) is indoors With no Wind. The re-
Ward is the negative distance to the target. We
Figure 4: Visualizing the modified reward
6
---- DARC (ours)	RL on Target (le6 steps) ------ Finetuning	---- Importance Weighting --------- PETS
RL on Target —— RL on Source	---- RL on Target (10x steps) ------- MBPO	—— MATL
Figure 6: DARC compensates for crippled robots and obstacles: We apply DARC to four continuous
control tasks: three tasks (broken reacher, half cheetah, and ant) which are crippled in the target domain but
not the source domain, and one task (half cheetah obstacle) where the source domain omits the obstacle from
the target domain. Note that naively ignoring the shift in dynamics (green dashed line) performs quite poorly,
while directly learning on the crippled robot requires an order of magnitude more experience than our method.
plot the reward as a function of the angle in both domains in Fig. 4. The optimal strategy for the
outdoor range is to compensate for the wind by shooting slightly to the left (θ = -0.8), while the
optimal strategy for the indoor range is to shoot straight ahead (θ = 0). We estimate the modified
reward function with DARC, and plot the modified reward in the windy outdoor range and indoor
range. We aggregate across episodes using J(θ) = logEp(so∣θ) [exp(r(s0))]; see Appendix E.4 for
details. We observe that maximizing the modified reward in the windy range does not yield high
reward in the windy range, but does yield a policy that performs well in the indoor range.
Figure 5: Environments: broken reacher, broken half
cheetah, broken ant, and half cheetah obstacle.
Scaling to more complex tasks. We now ap-
ply DARC to the more complex tasks shown in
Fig. 5. We define three tasks by crippling one of
the joints of each robot in the target domain, but
using the fully-functional robot in the source
domain. We use three simulated robots taken
from OpenAI Gym (Brockman et al., 2016): 7
DOF reacher, half cheetah, and ant. The broken reacher is based on the task described by Vemula
et al. (2020). We also include a task where the shift in dynamics is external to the robot, by modify-
ing the cheetah task to reward the agent for running both forward and backwards. It is easier to learn
to run backwards, an obstacle in the target domain prevents the agent from running backwards. This
“half cheetah obstacle” task does not entirely satisfy the assumption in Eq. 1 because transitions
such as bouncing off the obstacle are only possible in the target domain, not the source domain.
Nonetheless, the success of our method on this task illustrates that DARC can excel even in settings
that do not satisfy the assumption in Eq. 1.
We compare our method to eight baselines. RL on Source and RL on Target directly perform RL
on the source and target domains, respectively. The Finetuning baseline takes the result of running
RL on the source domain, and further finetunes the agent on the target domain. The Importance
Weighting baseline performs RL on importance-weighted samples from the source domain; the
importance weights are exp(∆r). Recall that DARC collects many (r = 10) transitions in the
source domain and performs many gradient updates for each single transition collected in the target
domain (Alg. 1 Line 5). We therefore compared against a RL on Target (10x) baseline that likewise
performs many (r = 10) gradient updates per transition in the target domain. Next, we compared
against two recent model-based RL methods: MBPO (Janner et al., 2019) and PETS (Chua et al.,
2018). Finally, we also compared against MATL (Wulfmeier et al., 2017b), which is similar in spirit
to our method but uses a different modified reward.
We show the results of this experiment in Fig. 6, plotting the reward on the target domain as a
function of the number of transitions in the target domain. In this figure, the transparent lines
correspond to different random seeds, and the darker lines are the average of these random seeds. On
all tasks, the RL on source baseline (shown as a dashed line because it observes no target transitions)
performs considerably worse than the optimal policy from RL on the target domain, suggesting that
good policies for the source domain are suboptimal for the target domain. Nonetheless, on three
of the four tasks our method matches (or even surpasses) the asymptotic performance of doing RL
on the target domain, despite never doing RL on experience from the target domain, and despite
observing 5 - 10× less experience from the target domain. On the broken reacher and broken half
cheetah tasks, finetuning on the target domain performs on par with our method. On the simpler
7
Figure 7: Ablation experiments (Left) DARC performs worse when only one classifier is used. (Right)
Using input noise to regularize the classifiers boosts performance. Both plots show results for broken reacher;
see Appendix D for results on all environments.
broken reacher task, just doing RL on the target domain with a large number of gradient steps works
quite well (we did not tune this parameter for our method). While the model-based baselines (PETS
and MBPO) also performed well on for low-dimensional tasks (broken reacher, broken half cheetah),
they perform quite poorly on more challenging tasks like broken ant, supporting our intuition that
classification is easier than learning a dynamics model in high dimensional tasks. Finally, DARC
outperforms MATL on all tasks.
Ablation Experiments. Our next experiment examines the importance of using two classifiers to
estimate ∆r. We compared our method to an ablation that does not learn the SA classifier, effectively
ignoring the blue terms in Eq. 3. As shown in Fig. 7 (left), this ablation performs considerably
worse than our method. Intuitively, this makes sense: We might predict that a transition came from
the source domain not because the next state had higher likelihood under the source dynamics, but
rather because the state or action was visited more frequently in the source domain. The second
classifier used in our method corrects for this distribution shift.
Next, we examine the importance of input noise regularization in classifiers. As we observe only
a handful of transitions from the target domain, we hypothesized that regularization would be im-
portant to prevent overfitting. We test this hypothesis in Fig. 7 (right) by training our method on
the broken reacher environment with varying amounts of input noise. With no noise or little noise
our method performs poorly (likely due to overfitting); too much noise also performs poorly (likely
due to underfitting). We used a value of 1 in all our experiments, and did not tune this value. See
Appendix D for more plots of both ablation experiments.
To gain more intuition for our method, we
recorded the reward correction ∆r throughout
training on the broken reacher environment. In
this experiment, we ran RL on the source do-
main for 100k steps before switching to our
method. Said another way, we ignored ∆r for
the first 100k steps of training. As shown in
Fig. 8, ∆r steadily decreases during these first
100k steps, suggesting that the agent is learn-
ing a strategy that takes transitions where the
source domain and target domain have different
dynamics: the agent is making use of its broken
joint. After 100k steps, when we maximize the
combination of task reward and ∆r, we observe
that ∆r increases, so the agent’s transitions in
the source domain are increasingly consistent
with target domain dynamics. After around 1e6
training steps ∆r is zero: the agent has learned a
able between the source and target domains.
Training
without ∆r Traininq with ∆r
(」<UO 一tj①」Joo PJQM ①比
0.0	0.2	0.4	0.6	0.8	1.0	1.2
Training Steps	le6
Figure 8: Without the reward correction, the agent
takes transitions where the source domain and target do-
mains are dissimilar; after adding the reward correction,
the agent’s transitions in the source domain are increas-
ingly likely under the target domain.
strategy that uses transitions that are indistinguish-
Safety emerges from domain adaptation to the termination condition. In many safety-critical
applications, the real-world and simulator have different safeguards, which kick in to stop the agent
and terminate the episode. For an agent to effectively transfer from the simulator to the real world, it
cannot rely on safeguards which are present in one domain but not the other. Since this termination
condition is part of the dynamics (White, 2017), we can readily apply DARC to this setting.
8
We use the humanoid shown in Fig. 9 for
this experiment and set the task reward to 0.
In the source domain episodes have a fixed
length of 300 steps; in the target domain the
episode terminates when the robot falls. The
scenario mimics the real-world setting where
robots have freedom to practice in a safe, cush-
ioned, practice facility, but are preemptively
stopped when they try to take unsafe actions in
the real world. Our aim is for the agent to learn
to avoid unsafe transitions in the source domain
Figure 9: Our method accounts for domain shift in the
termination condition, causing the agent to avoid tran-
sitions that cause termination in the target domain.
that would result in episode termination in the target domain. As shown in Fig. 9, our method learns
to remain standing for nearly the entire episode. As expected, baselines that maximize the zero
reward on the source and target domains fall immediately. While DARC was not designed as a
method for safe RL (Tamar et al., 2013; Achiam et al., 2017; Eysenbach et al., 2017; Berkenkamp
et al., 2017), this experiment suggests that safety may emerge automatically from DARC, without
any manual reward function design.
Comparison with Prior Transfer Learning
Methods. We are not the first work that mod-
ifies the reward function to perform trans-
fer in RL (Koos et al., 2012), nor the first
work to learn how to modify the reward func-
tion (Wulfmeier et al., 2017a). However,
these prior works lack theoretical justification.
In contrast, our approach maximizes a well-
defined variational objective and our analysis
guarantees that agents learned with our method
will achieve similar rewards in the source and
target domains. Our formal guarantees (Sec. 4)
Figure 10: Comparison with MATL
do not apply to MATL (Wulfmeier et al., 2017b) because their classifier is not conditioned on the
action. Indeed, our results on the four tasks in Fig. 6 indicate that DARC ourperforms MATL on all
tasks. To highlight this difference, we compared DARC and MATL on a gridworld (right), where
the source and target domains differed by assigning opposite effects to the “up” and “down” in the
purple state in the source and target domains. We collected data from a uniform random policy, so
the marginal distribution p(st+1 | st) was the same in the source and target domains, even though
the dynamics p(st+1 | st , at ) where different. In this domain, MATL fails to recognize that the
source and target domains are different. DARC succeeds in this task for 80% of trials while MATL
succeeds for 0% of trials. We conclude that the conditioning on the action, as suggested by our
analysis, is especially important when using experience collected from stochastic policies.
7	Discussion
In this paper, we proposed a simple, practical, and intuitive approach for domain adaptation to
changing dynamics in RL. We motivate this method from a novel variational perspective on domain
adaptation in RL, which suggests that we can compensate for differences in dynamics via the reward
function. Moreover, we formally prove that, subject to a lightweight assumption, our method is
guaranteed to yield a near-optimal policy for the target domain. Experiments on a range of control
tasks show that our method can leverage the source domain to learn policies that will work well in
the target domain, despite observing only a handful of transitions from the target domain.
Limitations The main limitation of our method is that the source dynamics must be sufficiently
stochastic, an assumption that can usually be satisfied by adding noise to the dynamics, or ensem-
bling a collection of sources. Empirically, we found that our method worked best on tasks that could
be completed in many ways in the source domain, but some of these strategies were not compatible
with the target dynamics. The main takeaway of this work is that inaccuracies in dynamics can be
compensated for via the reward function. In future work we aim to use the variation perspective on
domain adaptation (Sec. 4) to learn the dynamics for the source domain.
9
Acknowledgements. We thank Anirudh Vemula for early discussions; we thank Karol Hausman, Vincent
Vanhoucke and anonymous reviews for feedback on drafts of this work. We thank Barry Moore for providing
containers with MuJoCo and Dr. Paul Munro granting access to compute at CRC. This work is supported by
the Fannie and John Hertz Foundation, University of Pittsburgh Center for Research Computing (CRC), NSF
(DGE1745016, IIS1763562), ONR (N000141812861), and US Army. Any opinions, findings and conclusions
or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views
of the National Science Foundation.
Contributions. BE proposed the idea of using rewards to correct for dynamics, designed and ran many
of the experiments in the paper, and wrote much of the paper. SA did the initial literature review, wrote
and designed some of the DARC experiments and environments, developed visualizations of the modified
reward function, and ran the MBPO experiments. SC designed some of the initial environments, helped with
the implementation of DARC, and ran the PETS experiments. RS and SL provided guidance throughout the
project, and contributed to the structure and writing of the paper.
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Ried-
miller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31. JMLR. org, 2017.
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement
learning with stability guarantees. In Advances in neural information processing systems, pp. 908-918,
2017.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training and test
distributions. In Proceedings of the 24th international conference on Machine learning, pp. 81-88, 2007.
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura
Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve
efficiency of deep robotic grasping. In 2018 IEEE International Conference on Robotics and Automation
(ICRA), pp. 4243-4250. IEEE, 2018.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter
Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019
International Conference on Robotics and Automation (ICRA), pp. 8973-8979. IEEE, 2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in
a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing
Systems, pp. 4754-4765, 2018.
Ignasi Clavera, Anusha Nagabandi, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learn-
ing to adapt: Meta-learning for model-based control. arXiv preprint arXiv:1803.11347, 3, 2018.
Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for
regression. Theoretical Computer Science, 519:103-126, 2014.
Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint
arXiv:1702.05374, 2017.
Mark Cutler, Thomas J Walsh, and Jonathan P How. Reinforcement learning with multi-fidelity simulators. In
2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 3888-3895. IEEE, 2014.
Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences: A survey
and comparison. Journal of Machine Learning Research, 15:809-883, 2014.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In
Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465-472, 2011.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2 Fast reinforce-
ment learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
10
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv preprint
arXiv:1103.4601, 2011.
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to reset for safe
and autonomous reinforcement learning. arXiv preprint arXiv:1711.06782, 2017.
Alon Farchy, Samuel Barrett, Patrick MacAlpine, and Peter Stone. Humanoid robots learning to walk faster:
From the real world to simulation and back. In Proceedings of the 2013 international conference on Au-
tonomous agents and multi-agent systems, pp. 39-46, 2013.
AA Feldbaum. Dual control theory. i. Avtomatika i Telemekhanika, 21(9):1240-1249, 1960.
Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual domain adapta-
tion using subspace alignment. In Proceedings of the IEEE international conference on computer vision, pp.
2960-2967, 2013.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement
learning. arXiv preprint arXiv:1710.11248, 2017.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.
arXiv preprint arXiv:1812.02900, 2018.
Shani Gamrian and Yoav Goldberg. Transfer learning for related reinforcement learning tasks via image-to-
image translation. arXiv preprint arXiv:1806.07377, 2018.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of
Machine Learning Research, 17(1):2096-2030, 2016.
Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang,
Ekaterina Gonina, Chris Harris, Vincent Vanhoucke, et al. Tf-agents: A library for reinforcement learning
in tensorflow, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.
Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
1480-1490. JMLR. org, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural information
processing systems, pp. 4565-4573, 2016.
Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversarial and
constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy
optimization. In Advances in Neural Information Processing Systems, pp. 12498-12509, 2019.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez-Lobato, Richard E Turner, and
Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1645-1654. JMLR.
org, 2017.
Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of statistical
mechanics: theory and experiment, 2005(11):P11011, 2005.
Taylor W Killian, Samuel Daulton, George Konidaris, and Finale Doshi-Velez. Robust and efficient transfer
learning with hidden parameter markov decision processes. In Advances in neural information processing
systems, pp. 6250-6261, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
11
Sylvain Koos, Jean-Baptiste Mouret, and StePhane Doncieux. The transferability approach: Crossing the reality
gap in evolutionary robotics. IEEE Transactions on Evolutionary Computation, 17(1):122-145, 2012.
Wouter Marco Kouw and Marco Loog. A review of domain adaptation without target labels. IEEE transactions
on pattern analysis and machine intelligence, 2019.
Alessandro Lazaric. Knowledge transfer in reinforcement learning. PhD thesis, PhD thesis, Politecnico di
Milano, 2008.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv
preprint arXiv:1805.00909, 2018.
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordi-
nation for robotic grasping with deep learning and large-scale data collection. The International Journal of
Robotics Research, 37(4-5):421-436, 2018.
Zachary C Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and correcting for label shift with black box
predictors. arXiv preprint arXiv:1802.03916, 2018.
Lennart Ljung. System identification. Wiley encyclopedia of electrical and electronics engineering, pp. 1-19,
1999.
Michael G Madden and Tom Howley. Transfer of experience between reinforcement learning environments
with progressive difficulty. Artificial Intelligence Review, 21(3-4):375-398, 2004.
Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. Machine learning, 49(2-3):267-
290, 2002.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner.
arXiv preprint arXiv:1707.03141, 2017.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint
arXiv:1610.03483, 2016.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient Off-POliCy reinforce-
ment learning. In Advances in Neural Information Processing Systems, pp. 1054-1062, 2016.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic
control with dynamics randomization. In 2018 IEEE international conference on robotics and automation
(ICRA), pp. 1-8. IEEE, 2018.
Theodore J Perkins, Doina Precup, et al. Using options for knowledge transfer in reinforcement learning.
University of Massachusetts, Amherst, MA, USA, Tech. Rep, 1999.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning robust neural
network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-
reinforcement learning via probabilistic context variables. In International conference on machine learning,
pp. 5331-5340, 2019.
Balaraman Ravindran and Andrew G Barto. An algebraic approach to abstraction in reinforcement learning.
PhD thesis, University of Massachusetts at Amherst, 2004.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement
learning by approximate inference. In Twenty-Third International Joint Conference on Artificial Intelligence,
2013.
Stephane Ross and J Andrew Bagnell. Agnostic system identification for model-based reinforcement learning.
arXiv preprint arXiv:1203.1007, 2012.
Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image. arXiv
preprint arXiv:1611.04201, 2016.
Sosale Shankara Sastry and Alberto Isidori. Adaptive control of linearizable systems. IEEE Transactions on
Automatic Control, 34(11):1123-1131, 1989.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
12
Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics. In IJCAI, pp.
670-672,1985.
Alexander A Sherstov and Peter Stone. Improving action selection in mdp’s via knowledge transfer. In AAAI,
volume 5, pp. 1024-1029, 2005.
CasPer Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and FerenC HuszaE Amortised map inference
for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.
H Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W Rae, Seb
Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, et al. V-mpo: On-policy maximum a posteriori policy
optimization for discrete and continuous control. arXiv preprint arXiv:1909.12238, 2019.
Funlade T Sunmola and Jeremy L Wyatt. Model transfer for markov decision tasks via parameter matching.
In Proceedings of the 25th Workshop of the UK Planning and Scheduling Special Interest Group (PlanSIG
2006), pp. 246-252, 2006.
Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. arXiv preprint
arXiv:1306.6189, 2013.
Jie Tan, Zhaoming Xie, Byron Boots, and C Karen Liu. Simulation-based design of dynamic controllers for
humanoid balancing. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 2729-2736. IEEE, 2016.
Fumihide Tanaka and Masayuki Yamamura. Multitask reinforcement learning on the distribution of mdps. In
Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automa-
tion. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No. 03EX694),
volume 3, pp. 1108-1113. IEEE, 2003.
Marko Tanaskovic, Lorenzo Fagiano, Roy Smith, Paul Goulart, and Manfred Morari. Adaptive model predictive
control for constrained linear systems. In 2013 European Control Conference (ECC), pp. 382-387. IEEE,
2013.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal
of Machine Learning Research, 10(7), 2009.
Louis C Tiao, Edwin V Bonilla, and Fabio Ramos. Cycle-consistent adversarial learning as approximate
bayesian inference. arXiv preprint arXiv:1806.01771, 2018.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain ran-
domization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ
international conference on intelligent robots and systems (IROS), pp. 23-30. IEEE, 2017.
Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing
systems, pp. 1369-1376, 2007.
Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th annual
international conference on machine learning, pp. 1049-1056, 2009.
Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Generative adversarial
nets from a density ratio estimation perspective. arXiv preprint arXiv:1610.02920, 2016.
Anirudh Vemula, Yash Oza, J Andrew Bagnell, and Maxim Likhachev. Planning and execution using inaccurate
models with provable guarantees. arXiv preprint arXiv:2003.04394, 2020.
Paul J Werbos. Neural networks for control and system identification. In Proceedings of the 28th IEEE
Conference on Decision and Control,, pp. 260-265. IEEE, 1989.
Martha White. Unifying task specification in reinforcement learning. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3742-3750. JMLR. org, 2017.
Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model predictive path integral control using
covariance variable importance sampling. arXiv preprint arXiv:1509.01149, 2015.
Bjorn Wittenmark. Adaptive dual control methods: An overview. In Adaptive Systems in Control and Signal
Processing 1995, pp. 67-72. Elsevier, 1995.
Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Addressing appearance change in outdoor robotics
with adversarial domain adaptation. In 2017 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pp. 1551-1558. IEEE, 2017a.
13
Markus Wulfmeier, Ingmar Posner, and Pieter Abbeel. Mutual alignment transfer learning. arXiv preprint
arXiv:1707.07907, 2017b.
Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with
online system identification. arXiv preprint arXiv:1702.02453, 2017.
Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of the
twenty-first international conference on Machine learning, pp. 114, 2004.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer
vision,pp. 2223-2232, 2017a.
Shaojun Zhu, Andrew Kimmel, Kostas E Bekris, and Abdeslam Boularias. Fast model identification via physics
engines for data-efficient policy search. arXiv preprint arXiv:1710.08893, 2017b.
Brian D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.
PhD thesis, Carnegie Mellon University, 2010.
14
A	Additional Interpretations of the Reward Correction
This section presents four additional interpretations of the reward correction, ∆r.
A.1 Coding Theory
The reward correction ∆r can also be understood from the perspective of coding theory. Suppose
that we use a data-efficient replay buffer that exploits that fact that the next state st+1 is highly
redundant with the current state and action, st , at . If we assume that the replay buffer compression
has been optimized to store transitions from the target environment, (negative) ∆r is the number
of additional bits (per transition) needed for our source replay buffer, as compared with our target
replay buffer. Thus, an agent which maximizes ∆r will seek those transitions that can be encoded
most efficiently, minimizing the size of the source replay buffer.
A.2 Mutual Information
We can gain more intuition in the modified reward by writing the expected value of ∆r from Eq. 3
in terms of mutual information:
E[∆r(st, at, st+1)] = I(st+1;target | st,at) - I(st+1; source | st, at).
The mutual information I(st+1 ; target | st, at) reflects how much better you can predict the next
state if you know that you are interacting with the target domain, instead of the source domain. Our
approach does exactly this, rewarding the agent for taking transitions that provide information about
the target domain while penalizing transitions that hint to the agent that it is interacting with a source
domain rather than the target domain: we don’t want our are agent to find bugs in the Matrix.
A.3 Lower bound on the risk-sensitive reward objective.
While we derived DARC by minimizing a reverse KL divergence (Eq. 2), we can also show that
DARC maximizes a lower bound on a risk-sensitive reward objective (Mihatsch & Neuneier, 2002):
logEs0 〜Ptarget(s01 s,a),
a〜π(a∣ S)
exp
X r(st, at)
=log Es0~psource(s0∣s,a),
a〜π(a | s)
∏ Ptarget(St+1 | st, at) ʌ
t PSourCe (st+1 | st,at) J eXP
r(st, at)
log Es0-Psource(s0∣s,a),
a〜π(a | S)
exp
I
X r(st
t
,at) + log Ptarget(St+1 | st, at) - log PSourCe(St+1 | st, at)
∆r(st,at,st+1)
(4)
|
✓
≥ Es0 〜PsoUrCe(Sls,a), Er(St ,at + ^r(st ,at ,st + 1)
a 〜π(a∣s)	t
(5)
The inequality on the laSt line iS an appliCation of JenSen’S inequality. One intereSting queStion
iS when it would be preferable to maximize Eq. 4 rather than Eq. 5. While Eq. 5 provideS a loSer
bound on the riSk SenSitive objeCtive, empiriCally it may avoid the riSk-Seeking behavior that Can be
induCed by riSk-SenSitive objeCtiveS. We leave the inveStigation of thiS trade-off aS future work.
A.4 A Constraint on Dynamics Discrepancy
Our method regularizeS the poliCy to viSit StateS where the tranSition dynamiCS are Similar between
the SourCe domain and target domain:
max E a〜π(a∣s)	r(st,at) + logPtarget(St+1 | st, at) - logpsοurCe(St+1 | st,at)+ Hπ [at | st]
π	s0-p(s0∣s,a)	t	、	' {	/
-DKL (pSo
urCe k ptarget )
15
This objective can equivalently be expressed as applying MaxEnt RL to only those policies which
avoid exploiting the dynamics discrepancy. More precisely, the KKT conditions guarantee that there
exists a positive constant > 0 such that our objective is equivalent to the following constrained
objective:
max E a〜π(a∣s)	r r r(st, at) + Hπ [at | st] ,
π∈πdarc S0〜P(SIs,a) L t	-
where ΠDARC denotes the set of policies that do not exploit the dynamics discrepancy:
ΠDARC ,	πE a 〜π(a∣s)	X	DKL (psource(st+1 |	st,	at)	k	ptarget(st+1 |	st ,	at ))
I	s0~p(s0∣s,a) L t
≤	.
(6)
One potential benefit of considering our method as the unconstrained objective is that it provides
a principled method for increasing or decreasing the weight on the ∆r term, depending on how
much the policy is currently exploiting the dynamics discrepancy. We leave this investigation as
future work.
B Proofs of Theoretical Guarantees
In this seCtion we present our analysis showing that maximizing the modified reward r + ∆r in the
sourCe domain yields a near-optimal poliCy for the target domain, subjeCt to Assumption 1. To start,
we show that maximizing the modified reward in the sourCe domain is equivalent to maximizing the
unmodified reward, subjeCt to the Constraint that the poliCy not exploit the dynamiCs:
Lemma B.1. Let a reward function r(S, a), source dynamics Psource(S0 | S, a), and target dynamics
Ptarget(S0 | S, a) be given. Then there exists > 0 such the optimization problem in Eq. 2 is equivalent
to
max Epsource,π X r(St, at) +Hπ[at | St] ,
π∈Πno exploit
where Πno exploit denotes the set of policies that do not exploit the dynamics:
πno exploit ,〈 E a^∏(α∣s)
I s0 〜p(s0∣s,a)
DKL (psource (st+1 | st , at ) k ptarget (st+1 | st , at )) ≤
t
.
The proof is a straightforward appliCation of the KKT Conditions. This lemma says that maximizing
the modified reward Can be equivalently viewed as restriCting the set of poliCies to those that do not
exploit the dynamiCs. Next, we will show that poliCies that do not exploit the dynamiCs have an
expeCted (entropy-regularized) reward that is similar in the sourCe and target domains:
Lemma B.2. Let policy π ∈ Πno exploit be given, and let Rmax be the maximum (entropy-regularized)
return of any trajectory. Then the following inequality holds:
IEpsource hX r(st, at ) + Hn [at 1 st]] - Eptarget hX TiSt at + Hn [at 1 s"] ] ≤ 2Rmαx P€/2.
This Lemma proves that all policies in Πno exploit satisfy the same condition as the optimal policy
(Assumption 1).
Proof. To simplify notation, define r(s, a) = r(s,a) - logπ(a | s). We then apply Holder's
inequality and Pinsker’s inequality to obtain the desired result:
EpsourcJX r(st,at)] - EptargetX r(st,at)] = X(PSOUrCe(T) - Ptarget(T)) (X r(St, at))
τ
≤ Il Er(St,at)k∞ ∙ IIpsource(τ) - Ptarget(T)k1
≤ (max X r(st,at)) ∙ 2 j；DKL(PSOurCe(T) k Ptarget(T))
≤ 2RmaxPT∑
□
16
We restate our main result:
Theorem 4.1 (Repeated from main text). Let πDARC be the policy that maximizes the modified
(entropy-regularized) reward in the source domain, let π* be the policy that maximizes the (unmod-
ified, entropy-regularized) reward in the target domain, and assume that π* satisfies Assumption 1.
Then πDarc receives near-optimal (entropy-regularized) reward on the target domain:
Eptarget,∏DARC [^χ r(St ,a/	+ Hlat	1	st]]	≥	Eptarget,∏* [X	r(St, at )	+ Hlat	1	st]]	- 4Rmax p/"2.
Proof. Assumption 1 guarantees that the optimal policy for the target domain, ∏*, lies within
∏no exploit. Among all policies in Π∏o exploit, ∏Darc is (by definition) the one that receives highest
reward on the source dynamics, so
EpSOUrCe,∏Darc [X r(st, at) + H[at | stf| ≥ EpSOUrCe,∏* [X r(st,at) + H[at । st]] .
Since the both ∏Darc and ∏* lie inside the constraint set, Lemma B.2 dictates that their rewards on
the target domain differ by at most 2Rmax y∕~ep2 from their source domain rewards. In the worst case,
the reward for ∏Darc decreases by this amount and the reward for ∏ increases by this amount:
EpSOUrCe,∏Darc	[X r(st,	at)	+ H[at	।	st]i	≤	Eptarget,∏Darc [X r(st,at) + H[at । st]]	+2RmaX/^T2
Epsource,∏*	[X Ir(St,	at)	+ H[at	।	st]]	≥	Eptarget ,∏* [X Ir(St,at) + H[at । st]] -	2Rmax，〃2
Substituting these inequalities on the LHS and RHS of Eq. B and rearranging terms, we obtain the
desired result.	□
c The Special Case of an Observation Model
To highlight the relationship between domain adaptation of dynamics versus observations, we now
consider a special case. In this subsection, we will assume that the state st , (zt, ot) is a combi-
nation of the system latent state zt (e.g., the poses of all objects in a scene) and an observation ot
(e.g., a camera observation). We will define q(ot | zt) and p(ot | zt) as the observation models for
the source and target domains. In this special case, we can decompose the KL objective (Eq. 2) into
three terms:
DKL(q k p)	= -Eq	r(st,	at)	+ Hπlat	|	st]	+logptarget(ot	|	zt)	- log psource (ot	|	zt)
LV1-----------{z------------} 、-------------------{------------------}
MaxEnt RL objective	Observation Adaptation
+ log ptarget (zt+1 | zt, at) - logpsource(zt+1 | zt, at) .
|------------------------{----------------------------}_
Dynamics Adaptation
Prior methods that perform observation adaptation (Bousmalis et al., 2018; Gamrian & Goldberg,
2018) effectively minimize the observation adaptation term,1 but ignore the effect of dynamics. In
contrast, the ∆r reward correction in our method provides one method to address both dynamics
and observations. These approaches could be combined; we leave this as future work.
1Tiao et al. (2018) show that observation adaptation using cycleGan (Zhu et al., 2017a) minimizes a Jensen-
Shannon divergence. Assuming sufficiently expressive models, the Jensen-Shannon divergence and the reverse
KL divergence above have the same optimum.
17
Results of the ablation experiment from Fig. 7
Figure 11: Importance of using two classifiers:
(left) on all environments.
Figure 12: Importance of regularizing the classifiers: Results of the ablation experiment from
Fig. 7 (right) on all environments.
D Additional Experiments
Figures 11 and 12 show the results of the abla-
tion experiment from Fig. 7 run on all environ-
ments. The results support our conclusion in
the main text regarding the importance of using
two classifiers and using input noise. Figure 13
is a copy of Fig. 8 from the main text, modified
to also show the agent’s reward on the target do-
main. We observe that the reward does not start
increasing until we start using DARC.
Figure 13: Copy of Fig. 8 overlaid with the target
domain reward.
E Experiment Details and
Hyperparameters
Our implementation of DARC is built on top of the implementation of SAC from Guadarrama
et al. (2018). Unless otherwise specified, all hyperparameters are taken from Guadarrama et al.
(2018). All neural networks (actor, critics, and classifiers) have two hidden layers with 256-units
each and ReLU activations. Since we ultimately will use the difference in the predictions of the
two classifiers, we use a residual parametrization for the SAS classifier q(target | st, at, st+1). Us-
ing fSAS(St, at,st+1),fSA(St, at) ∈ R2 to denote the outputs of the two classifier networks, We
compute the classifier predictions as follows:
qθsA(∙ | St,at) = SOFTMAx(fsA(St,at))
qθsAs(∙ I St,at,st+ι) = SOFTMAX(fSAS(St,at,st+ι) + ∕sa(St,at))
For the SAS classifier we propagate gradients back through both networks parameters, θSAS and θSA.
Both classifiers use Gaussian input noise with σ = 1. Optimization of all networks is done with
Adam (Kingma & Ba, 2014) with a learning rate of 3e-4 and batch size of 128. Most experiments
with DARC collected 1 step in the target domain every 10 steps in the source domain (i.e., r = 10).
The one exception is the half cheetah obstacle domain, where we tried increasing r beyond 10 to 30,
100, 300, and 1000. We found a large benefit from increasing r to 30 and 100, but did not run the
other experiments long enough to draw any conclusions. Fig. 6 uses r = 30 for half cheetah obstacle.
We did not tune this parameter, and expect that tuning it would result in significant improvements in
sample efficiency.
18
We found that DARC was slightly more stable if we warm-started the method by applying RL on
the source task without ∆r for the first twarmup iterations. We used twarmup = 1e5 for all tasks except
the broken reacher, where we used twarmup = 2e5. This discrepancy was caused by a typo in an
experiment, and subsequent experiments found that DARC is relatively robust to different values
of twarmup; we did not tune this parameter.
E.1 Baselines
The RL on Source and RL on Target baselines are implemented identically to our method, with
the exception that ∆r is not added to the reward function. The RL on Target (10x) is identi-
cal to RL on Target, with the exception that we take 10 gradient steps per environment inter-
action (instead of 1). The Importance Weighting baseline estimates the importance weights as
ptarget(st+1 | st , at)/psource(st+1 | st, at) ≈ exp(∆r). The importance weight is used to weight
transitions in the SAC actor and critic losses.
PETS (Chua et al., 2018) The PETS baseline is implemented using the default configurations
used by (Chua et al., 2018) for the environments evaluated. The broken-half-cheetah en-
vironment uses the hyperparameters as used by the half-cheetah environment in (Chua et al.,
2018). The broken-ant environment uses the same set of hyperparameters, namely: task hori-
zon = 1000, number of training iterations = 300, number of planning (real) steps per iteration = 30,
number of particles to be used in particle propagation methods = 20. The PETS codebase can be
found at https://github.com/kchua/handful- of-trials.
MBPO (Janner et al., 2019) We used the authors implementation with the default hyper-
parameters: https://github.com/JannerM/mbpo. We kept the environment config-
urations the same as their default unmodified MuJoCo environments, except for the domain
and task name. We added our custom environment xmls in mbpo/env/assets/ folder,
and their corresponding environment python files in the mbpo/env/ folder. Their static
files were added under mbpo/static/. These environments can be registered as gym en-
Vironments in the init file under mbpo_odrl/mbpo/env/ or can be initialized directly in
softlearning/environments/adapters/gym adapter.py. We set the time limit to
max_episode_steps=10 0 0 for the Broken Half Cheetah, Broken Ant and Half Cheetah Obsta-
cle environments and to 100 for the Broken Reacher environment.
E.2 Environments
Broken Reacher This environment uses the 7DOF robot arm from the Pusher environment in
OpenAI Gym. The observation space is the position and velocities of all joints and the goal. The
reward function is
r(s, a) = - 2 11 send effector - sgoalk2 -m kak2,
and episodes are 100 steps long. In the target domain the 2nd joint (0-indexed) is broken: zero
torque is applied to this joint, regardless of the commanded torque.
Broken Half Cheetah This environment is based on the HalfCheetah environment in OpenAI
Gym. We modified the observation to include the agent’s global X coordinate so the agent can infer
its relative position to the obstacle. Episodes are 1000 steps long. In the target domain the 0th joint
(0-indexed) is broken: zero torque is applied to this joint, regardless of the commanded torque.
Broken Ant This environment is based on the Ant environment in OpenAI Gym. We use the
standard termination condition and cap the maximum episode length at 1000 steps. In the target
domain the 3rd joint (0-indexed) is broken: zero torque is applied to this joint, regardless of the
commanded torque.
In all the broken joint environments, we choose which joint to break to computing which joint caused
the “RL on Source” baseline to perform worst on the target domain, as compared with the “RL on
Target” baseline.
19
Half Cheetah Obstacle This environment is based on the HalfCheetah environment in OpenAI
Gym. Episodes are 1000 steps long. We modified the standard reward function to use the absolute
value in place of the velocity, resulting the following reward function:
r(s,a) = Sxve1 ∙ δt - ∣∣ak2,
where sx vel is the velocity of the agent along the forward-aft axis and ∆t = 0.01 is the time step
of the simulator. In the target domain, we added a wall at x = -3m, roughly 3 meters behind the
agent.
Humanoid Used for the experiment in Fig. 9, we used a modified version of Humanoid from
OpenAI Gym. The source domain modified this environment to ignore the default termination
condition and instead terminate after exactly 300 time steps. The target domain uses the unmodified
environment, which terminates when the agent falls.
E.3 Figures
Unless otherwise noted, all experiments were run with three random seeds. Figures showing learning
curves (Figures 6, 7, 8, 11, and 12) plot the mean over the three random seeds, and also plot the
results for each individual random seed with semi-transparent lines.
E.4 Archery Experiment
We used a simple physics model for the archery experiment. The target was located 70m North of
the agent, and wind was applied along the East-West axis. The system dynamics:
LC . /八、 一 小2	f f 〜N(μ	=1,σ =	1)	in the source domain
st+1 =	70 Sin⑹	+ f/Cos⑻2	If 〜N(μ	= 0,σ =	0.3)	in the target domain
We trained the classifier by sampling θ 〜U[-2, 2] (measured in degrees) for 10k episodes in
the source domain and 10k episodes in the target domain. The classifier was a neural network
with 1 hidden layer with 32 hidden units and ReLU activation. We optimized the classifier using
the Adam optimizer with a learning rate of 3e-3 and a batch size of 1024. We trained until the
validation loss increased for 3 consecutive epochs, which took 16 epochs in our experiment. We
generated Fig. 4 by sampling 10k episodes for each value of θ and aggregating the rewards using
J(θ) = logEp(so∣θ)[exp(r(s0))]. We found that aggregating rewards by taking the mean did not
yield meaningful results, perhaps because the mean corresponds to a (possibly loose) lower bound
on J (see Appendix A.3).
20