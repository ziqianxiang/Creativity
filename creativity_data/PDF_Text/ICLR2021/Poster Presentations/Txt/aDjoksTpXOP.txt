Published as a conference paper at ICLR 2021
Deep Equals Shallow for ReLU Networks
in Kernel Regimes
Alberto Bietti*
NYUt
alberto.bietti@nyu.edu
Francis Bach
Inria 去
francis.bach@inria.fr
Abstract
Deep networks are often considered to be more expressive than shallow ones
in terms of approximation. Indeed, certain functions can be approximated
by deep networks provably more efficiently than by shallow ones, however,
no tractable algorithms are known for learning such deep models. Sepa-
rately, a recent line of work has shown that deep networks trained with
gradient descent may behave like (tractable) kernel methods in a certain
over-parameterized regime, where the kernel is determined by the architec-
ture and initialization, and this paper focuses on approximation for such
kernels. We show that for ReLU activations, the kernels derived from deep
fully-connected networks have essentially the same approximation proper-
ties as their “shallow” two-layer counterpart, namely the same eigenvalue
decay for the corresponding integral operator. This highlights the limita-
tions of the kernel framework for understanding the benefits of such deep
architectures. Our main theoretical result relies on characterizing such
eigenvalue decays through differentiability properties of the kernel function,
which also easily applies to the study of other kernels defined on the sphere.
1	Introduction
The question of which functions can be well approximated by neural networks is crucial for
understanding when these models are successful, and has always been at the heart of the
theoretical study of neural networks (e.g., Hornik et al., 1989; Pinkus, 1999). While early
works have mostly focused on shallow networks with only two layers, more recent works
have shown benefits of deep networks for approximating certain classes of functions (Eldan
& Shamir, 2016; Mhaskar & Poggio, 2016; Telgarsky, 2016; Daniely, 2017; Yarotsky, 2017;
Schmidt-Hieber et al., 2020). Unfortunately, many of these approaches rely on constructions
that are not currently known to be learnable using efficient algorithms.
A separate line of work has considered over-parameterized networks with random neu-
rons (Neal, 1996), which also display universal approximation properties while additionally
providing efficient algorithms based on kernel methods or their approximations such as ran-
dom features (Rahimi & Recht, 2007; Bach, 2017b). Many recent results on gradient-based
optimization of certain over-parameterized networks have been shown to be equivalent to
kernel methods with an architecture-specific kernel called the neural tangent kernel (NTK)
and thus also fall in this category (e.g., Jacot et al., 2018; Li & Liang, 2018; Allen-Zhu et al.,
2019b; Du et al., 2019a;b; Zou et al., 2019). This regime has been coined lazy (Chizat et al.,
2019), as it does not capture the common phenomenon where weights move significantly
away from random initialization and thus may not provide a satisfying model for learn-
ing adaptive representations, in contrast to other settings such as the mean field or active
regime, which captures complex training dynamics where weights may move in a non-trivial
manner and adapt to the data (e.g., Chizat & Bach, 2018; Mei et al., 2018). Nevertheless,
* Work done while at Inria.
,Center for Data Science, New York University. New York, USA.
"lnria - DePartement d'Informatique de l'Ecole Normale Superieure. PSL Research University.
Paris, France.
1
Published as a conference paper at ICLR 2021
one benefit compared to the mean field regime is that the kernel approach easily extends
to deep architectures, leading to compositional kernels similar to the ones of Cho & Saul
(2009); Daniely et al. (2016). Our goal in this paper is to study the role of depth in de-
termining approximation properties for such kernels, with a focus on fully-connected deep
ReLU networks.
Our approximation results rely on the study of eigenvalue decays of integral operators asso-
ciated to the obtained dot-product kernels on the sphere, which are diagonalized in the basis
of spherical harmonics. This provides a characterization of the functions in the correspond-
ing reproducing kernel Hilbert space (RKHS) in terms of their smoothness, and leads to
convergence rates for non-parametric regression when the data are uniformly distributed on
the sphere. We show that for ReLU networks, the eigenvalue decays for the corresponding
deep kernels remain the same regardless of the depth of the network. Our key result is that
the decay for a certain class of kernels is characterized by a property related to differentiabil-
ity of the kernel function around the point where the two inputs are aligned. In particular,
the property is preserved when adding layers with ReLU activations, showing that depth
plays essentially no role for such networks in kernel regimes. This highlights the limitations
of the kernel regime for understanding the power of depth in fully-connected networks, and
calls for new models of deep networks beyond kernels (see, e.g., Allen-Zhu & Li, 2020; Chen
et al., 2020, for recent works in this direction). We also provide applications of our result
to other kernels and architectures, and illustrate our results with numerical experiments on
synthetic and real datasets.
Related work. Kernels for deep learning were originally derived by Neal (1996) for shal-
low networks, and later for deep networks (Cho & Saul, 2009; Daniely et al., 2016; Lee
et al., 2018; Matthews et al., 2018). Smola et al. (2001); Minh et al. (2006) study regular-
ization properties of dot-product kernels on the sphere using spherical harmonics, and Bach
(2017a) derives eigenvalue decays for such dot-product kernels arising from shallow networks
with positively homogeneous activations including the ReLU. Extensions to shallow NTK
or Laplace kernels are studied by Basri et al. (2019); Bietti & Mairal (2019b); Geifman
et al. (2020). The observation that depth does not change the decay of the NTK was pre-
viously made by Basri et al. (2020) empirically, and Geifman et al. (2020) provide a lower
bound on the eigenvalues for deep networks; our work makes this observation rigorous by
providing tight asymptotic decays. Spectral properties of wide neural networks were also
considered in (Cao et al., 2019; Fan & Wang, 2020; Ghorbani et al., 2019; Xie et al., 2017;
Yang & Salman, 2019). Azevedo & Menegatto (2014); Scetbon & Harchaoui (2020) also
study eigenvalue decays for dot-product kernels but focus on kernels with geometric decays,
while our main focus is on polynomial decays. Additional works on over-parameterized
or infinite-width networks in lazy regimes include (Allen-Zhu et al., 2019a;b; Arora et al.,
2019a;b; Brand et al., 2020; Lee et al., 2020; Song & Yang, 2019).
Concurrently to our work, Chen & Xu (2021) also studied the RKHS of the NTK for deep
ReLU networks, showing that it is the same as for the Laplace kernel on the sphere. They
achieve this by studying asymptotic decays of Taylor coefficients of the kernel function at
zero using complex-analytic extensions of the kernel functions, and leveraging this to ob-
tain both inclusions between the two RKHSs. In contrast, we obtain precise descriptions
of the RKHS and regularization properties in the basis of spherical harmonics for vari-
ous dot-product kernels through spectral decompositions of integral operators, using (real)
asymptotic expansions of the kernel function around endpoints. The equality between the
RKHS of the deep NTK and Laplace kernel then easily follows from our results by the fact
that the two kernels have the same spectral decay.
2	Review of Approximation with Dot-Product Kernels
In this section, we provide a brief review of the kernels that arise from neural networks and
their approximation properties.
2
Published as a conference paper at ICLR 2021
2.1	Kernels for wide neural networks
Wide neural networks with random weights or weights close to random initialization nat-
urally lead to certain dot-product kernels that depend on the architecture and activation
function, which we now present, with a focus on fully-connected architectures.
Random feature kernels. We first consider a two-layer (shallow) network of the form
f (x) = √1m Zm=I vjσ(Wjrx), for some activation function σ. When Wj 〜N(0,I) ∈ Rd are
fixed and only Vj ∈ R are trained with £2 regularization, this corresponds to using a random
feature approximation Rahimi & Recht (2007) of the kernel
k (x,x/) = E W 〜N (o,i)[ σ (w Tx) σ (W Tx，.	⑴
If x, x/ are on the sphere, then by spherical symmetry of the Gaussian distribution, one may
show that k is invariant to unitary transformations and takes the form k(x, xf) = K(xTxf)
for a certain function κ. More precisely, if σ(u) = i≥0 aihi (u) is the decomposition of σ
in the basis of Hermite polynomials hi, which are orthogonal w.r.t. the Gaussian measure,
then we have (Daniely et al., 2016):
κ(u) =	ai2ui.
i≥0
(2)
Conversely, given a kernel function of the form ab ove with κ(u) =	i≥0 biui with bi ≥ 0,
one may construct corresponding activations using Hermite polynomials by taking
σ(U) = EJaihi(U), ai ∈ {±b/b.}.
i
(3)
In the case where σ is s-positively homogeneous, such as the ReLU σ(U) = max(U, 0)
(with s = 1), or more generally σs(U) = max(U, 0)s, then the kernel (1) takes the
form k(x, xz) = ∣∣xIlSHX[∣SK(八力般勺)for any x, x!. This leads to RKHS functions of the
form f (x) = HxHSg(Jx^), with g in the RKHS of the kernel restricted to the sphere (Bietti
& Mairal, 2019b, Prop. 8). In particular, for the step and ReLU activations σ0 and σ1 , the
functions K are given by the following arc-cosine kernels (Cho & Saul, 2009):1
K0(U)
ɪ (π 一 arccos(U)),
κι(u) = — (u ∙ (π 一 arccos(U)) + ʌ/1 - U2).
(4)
Note that given a kernel function K, the corresponding activations (3) will generally not be
homogeneous, thus the inputs to a random network with such activations need to lie on the
sphere (or be appropriately normalized) in order to yield the kernel K.
Extension to deep networks. When considering a deep network with more than two
layers and fixed random weights before the last layer, the connection to random features is
less direct since the features are correlated through intermediate layers. Nevertheless, when
the hidden layers are wide enough, one still approaches a kernel obtained by letting the
widths go to infinity (see, e.g., Daniely et al., 2016; Lee et al., 2018; Matthews et al., 2018),
which takes a similar form to the multi-layer kernels of Cho & Saul (2009):
kL (x, x) = KL (xTxI := K ◦…。K(xTx),
^S
L-1 times
for x, xX on the sphere, where K is obtained as described above for a given activation σ, and L
is the number of layers. We still refer to this kernel as the random features (RF) kernel in
this paper, noting that it is sometimes known as the “conjugate kernel” or NNGP kernel
(for neural network Gaussian process). It is usually good to normalize K such that K(1) = 1,
so that we also have KL(1) = 1, avoiding exploding or vanishing behavior for deep networks.
In practice, this corresponds to using an activation-dependent scaling in the random weight
initialization, which is commonly used by practitioners (He et al., 2015).
1Here We assume a scaling，2/m instead of ʌ/l/m in the definition of f, which yields K(1) = 1,
a useful normalization for deep networks, as explained below.
3
Published as a conference paper at ICLR 2021
Neural tangent kernels. When intermediate layers are trained along with the last layer
using gradient methods, the resulting problem is non-convex and the statistical properties of
such approaches are not well understood in general, particularly for deep networks. However,
in a specific over-parameterized regime, it may be shown that gradient descent can reach a
global minimum while keeping weights very close to random initialization. More precisely,
for a network f (x; θ) parameterized by θ with large width m, the model remains close
to its linearization around random initialization θ0 throughout training, that is, f (x; θ) ≈
f (x; θ0) +(θ — θo, Vθf (x; θ0)). This is also known as the lazy training regime (Chizat et al.,
2019). Learning is then equivalent to a kernel method with another architecture-specific
kernel known as the neural tangent kernel (NTK, Jacot et al., 2018), given by
kNTK(x,x/)= lim〈Vf (x; θo), Vf (xz; θo)).
m→∞
(5)
For a simple two-layer network with activation σ , it is then given by
kNτκ(x, xf) = (xTxf) EW[σz(WTx)σ7(WTx')] + EW[σ(WTx)σ(WTx/)].	(6)
For a ReLU network with L layers with inputs on the sphere, taking appropriate limits on
the widths, one can show (Jacot et al., 2018): kntk(x, xf)=厘NTK(XTXz), With KNtk(U)=
K1 (u) = u and for £ = 2,..., L,
K (u) = κ ι( K -1( u))
KNTK (u) = KNTK (u)K0 (K	(u)) + K (u),	(7)
where K0 and K1 are given in (4).
2.2 Approximation and harmonic analysis with dot-product kernels
In this section, we recall approximation properties of dot-product kernels on the sphere,
through spectral decompositions of integral operators in the basis of spherical harmonics.
Further background is provided in Appendix A.
Spherical harmonics and description of the RKHS. A standard approach to study
the RKHS of a kernel is through the spectral decomposition of an integral operator T
given by T f(x) = k(x, y)f (y)dτ (y) for some measure τ, leading to Mercer’s theorem (e.g.,
Cucker & Smale, 2002). When inputs lie on the sphere Sd-1 in d dimensions, dot-product
kernels of the form k(x, xf) = K(xTxf) are rotationally-invariant, depending only on the
angle between x and x'. Similarly to how translation-invariant kernels are diagonalized
in the Fourier basis, rotation-invariant kernels are diagonalized in the basis of spherical
harmonics (Smola et al., 2001; Bach, 2017a), which lead to connections between eigenvalue
decays and regularity as in the Fourier setting. In particular, if τ denotes the uniform
measure on Sd-1, then TYk,j = μkYkj, where Ykj is the j-th spherical harmonic polynomial
of degree k, where k plays the role of a frequency as in the Fourier case, and the number of
such orthogonal polynomials of degree k is given by N(d, k) = 2k +'-2 (k+--3), which grows
as kd-2 for large k. The eigenvalues μk only depend on the frequency k and are given by
L
μk
3d-2
3d-1
K(t)Pk(t)(1 — t2)(d-3)/2dt,
(8)
where Pk is the Legendre polynomial of degree k in d dimensions (also known as Gegenbauer
polynomial when using a different scaling), and 3d-1 denotes the surface of the sphere Sd-1.
Mercer’s theorem then states that the RKHS H associated to the kernel is given by
{N (d,k)	N (d,k) 2	∣
f =	∑	∑ ak,jYk,j(∙)	s.t. Hf∣∣H:=工工于 < ∞	.	(9)
k ≥0,μk = 0 j =1	k ≥0,μk = 0 j =1 μk
In particular, if μk has a fast decay, then the coefficients ak,j of f must also decay quickly
with k in order for f to be in H, which means f must have a certain level of regularity.
Similarly to the Fourier case, an exponential decay of μk implies that the functions in H are
4
Published as a conference paper at ICLR 2021
infinitely differentiable, while for polynomial decay H contains all functions whose deriva-
tives only up to a certain order are bounded, as in Sobolev spaces. If two kernels lead to the
same asymptotic decay of μk up to a constant, then by ⑼ their RKHS norms are equivalent
up to a constant, and thus they have the same RKHS. For the specific case of random fea-
ture kernels arising from S-positively homogeneous activations, Bach (2017a) shows that μk
decays as k-d-2s for k of the opposite parity of s, and is zero for large enough k of opposite
parity, which results in a RKHS that contains even or odd functions (depending on the
parity of s) defined on the sphere with bounded derivatives up to order β := d/2 + s (note
that β must be greater than (d - 1)/2 in order for the eigenvalues of T to be summable
and thus lead to a well-defined RKHS). Bietti & Mairal (2019b) show that the same decay
holds for the NTK of two-layer ReLU networks, with s = 0 and a change of parity. Basri
et al. (2019) show that the parity constraints may be removed by adding a zero-initialized
additive bias term when deriving the NTK. We note that one can also obtain rates of ap-
proximation for Lipschitz functions from such decay estimates (Bach, 2017a). Our goal in
this paper is to extend this to more general dot-product kernels such as those arising from
multi-layer networks, by providing a more general approach for obtaining decay estimates
from differentiability properties of the function κ.
Non-parametric regression. When the data are uniformly distributed on the sphere,
we may also obtain convergence rates for non-parametric regression, which typically depend
on the eigenvalue decay of the integral operator associated to the marginal distribution
on inputs and on the decomposition of the regression function f *(x) = E[y |x] on the same
basis (e.g., Caponnetto & De Vito, 2007).2 Then one may achieve optimal rates that depend
mainly on the regularity of f * when using various algorithms with tuned hyperparameters,
but the choice of kernel and its decay may have an impact on the rates in some regimes, as
well as on the difficulty of the optimization problem (see, e.g., Bach, 2013, Section 4.3).
3	Main Result and Applications to Deep Networks
In this section, we present our main results concerning approximation properties of dot-
product kernels on the sphere, and applications to the kernels arising from wide random
neural networks. We begin by stating our main theorem, which provides eigenvalue decays
for dot-product kernels from differentiability properties of the kernel function κ at the end-
points ±1. We then present applications of this result to various kernels, including those
coming from deep networks, showing in particular that the RKHSs associated to deep and
shallow ReLU networks are the same (up to parity constraints).
3.1	Statement of our main theorem
We now state our main result regarding the asymptotic eigenvalue decay of dot-product
kernels. Recall that We consider a kernel of the form k(x,y) = K(xTy) for x,y ∈ Sd-1,
and seek to obtain decay estimates on the eigenvalues μk defined in (8). We now state our
main theorem, which derives the asymptotic decay of μk with k in terms of differentiability
properties of κ around {±1}, assuming that κ is infinitely differentiable on (-1, 1). This
latter condition is always verified when κ takes the form of a power series (2) with κ(1) = 1,
since the radius of convergence is at least 1. We also require a technical condition, namely
the ability to “differentiate asymptotic expansions” of κ at ±1, which holds for the kernels
considered in this work.
Theorem 1 (Decay from regularity of κ at endpoints, simplified). Let κ : [-1, 1] → R be a
function that is C∞ on (-1, 1) and has the fol lowing asymptotic expansions around ±1:
κ(1 -t) =p1(t)+c1tν+o(tν)	(10)
κ(-1 + t) = p-1 (t) + c-1tν + o(tν),	(11)
for t ≥ 0, where p1 , p-1 are polynomials and ν > 0 is not an integer. Also, assume that the
derivatives of κ admit similar expansions obtained by differentiating the above ones. Then,
there is an absolute constant C(d, ν) depending on d and ν such that:
2 The rates easily extend to distributions with a density w.r.t. the uniform distribution on the
sphere, although the eigenbasis on which regularity is measured is then different.
5
Published as a conference paper at ICLR 2021
•	For k even, if c 1 = — c-ι: μk 〜(c 1 + c-ι)C(d,ν)k-d-2ν +1;
•	For k odd, if c 1 = c-1 ： μk 〜(C 1 — C-1)C(d,ν)k-d-2ν+1.
In the case |c 1| = |c-1|, then We have μk = o(k-d-2V +1) for one of the two parities (or both
if c 1 = c-1 = 0). If K is infinitely differentiable on [—1, 1] so that no such V exists, then μk
decays faster than any polynomial.
The full theorem is given in Appendix B along with its proof, and requires an additional mild
technical condition on the expansion which is verified for all kernels considered in this paper,
namely, a finite number of terms in the expansions with exponents between ν and ν + 1.
The proof relies on integration by parts using properties of Legendre polynomials, in a
way reminiscent of fast decays of Fourier series for differentiable functions, and on precise
computations of the decay for simple functions of the form t → (1 — t2 )ν. This allows us to
obtain the asymptotic decay for general kernel functions κ as long as the behavior around
the endpoints is known, in contrast to previous approaches which rely on the precise form
of κ, or of the corresponding activation in the case of arc-cosine kernels (Bach, 2017a; Basri
et al., 2019; Bietti & Mairal, 2019b; Geifman et al., 2020). This enables the study of more
general and complex kernels, such as those arising from deep networks, as discussed below.
When κ is of the form κ(t) = k bktk, the exponent ν in Theorem 1 is also related to the
decay of coefficients bk . Such coefficients provide a dimension-free description of the kernel
which may be useful for instance in the study of kernel methods in certain high-dimensional
regimes (see, e.g., El Karoui, 2010; Ghorbani et al., 2019; Liang et al., 2020). We show
in Appendix B.1 that the bk may be recovered from the μk by taking high-dimensional
limits d → ∞, and that they decay as k-ν-1 .
3.2	Consequences for ReLU networks
When considering neural networks with ReLU activations, the corresponding random fea-
tures and neural tangent kernels depend on the arc-cosine functions κ1 and κ0 defined in (4).
These have the following expansions (with generalized exponents) near +1:
K o(1 — t ) = 1 — C1112 + O (t3 3 2)
π
K 1(1 — t ) = 1 — t + 2√21332 + O (t5 5 2).
3π
(12)
(13)
Indeed, the first follows from integrating the expansion of the derivative using the relation
ddt arccos(1 — t) = √=-1==== and the second follows from the first using the expression of K 1
in (4). Near —1, We have by symmetry Ko(—1 + t) = 1 — Ko(1 — t) = W1132 + O(t332), and
We have K 1(—1 + t) = 23√21332 + O(t533) by using Kf1 = Ko and K 1(—1) = 0. The ability to
differentiate the expansions follows from (Flajolet & Sedgewick, 2009, Theorem VI.8, p.419),
together With a complex-analytic property knoWn as ∆-analyticity, Which Was shoWn to hold
for RF and NTK kernels by Chen & Xu (2021). By Theorem 1, We immediately obtain a
decay of k-d-2 for even coefficients for K1, and k-d for odd coefficients for Ko, recovering
results of Bach (2017a). For the tWo-layer ReLU NTK, We have K2NTK(u) = uKo(u) + K1(u),
leading to a similar expansion to Ko and thus decay, up to a change of parity due to the
factor u Which changes signs in the expansion around —1; this recovers Bietti & Mairal
(2019b). We note that for these specific kernels, Bach (2017a); Bietti & Mairal (2019b)
shoW in addition that coefficients of the opposite parity are exactly zero for large enough k,
Which imposes parity constraints on functions in the RKHS, although such a constraint
may be removed in the NTK case by adding a zero-initialized bias term (Basri et al., 2019),
leading to a kernel KNTK,b(u) = (u + 1)Ko(u) + K1 (u).
Deep networks. Recall from Section 2.1 that the RF and NTK kernels for deep ReLU
netWorks may be obtained through compositions and products using the functions K1 and Ko .
Since asymptotic expansions can be composed and multiplied, We can then obtain expansions
for the deep RF and NTK kernels. The folloWing results shoW that such kernels have the
same eigenvalue decay as the ones for the corresponding shalloW (tWo-layer) netWorks.
6
Published as a conference paper at ICLR 2021
Corollary 2 (Deep RF decay.). For the random neuron kernel κLRF of an L-layer ReLU
network with L ≥ 3, we have μk 〜C(d,L)k-d-2, where C(d,L) is different depending on
the parity of k and grows linearly with L.
Corollary 3 (Deep NTK decay.). For the neural tangent kernel κLNTK of an L-layer ReLU
network with L ≥ 3, We have μk 〜C(d, L)k-d, where C(d, L) is different depending on the
parity of k and grows quadratical ly with L (it grows linearly with L when considering the
normalized NTK kNtk/L, which satisfies KNTK(1)/L = 1).
The proofs, given in Appendix C, use the fact that κ1 ◦ κ1 and κ1 have the same non-integer
exponent factors in their expansions, and similarly for κ0 ◦ κ1 and κ0. One benefit compared
to the shallow case is that the odd and even coefficients are both non-zero with the same
decay, which removes the parity constraints, but as mentioned before, simple modifications
of the shallow kernels can yield the same effect.
The finite neuron case. For two-layer networks with a finite number of neurons, the ob-
tained models correspond to random feature approximations of the limiting kernels (Rahimi
& Recht, 2007). Then, one may approximate RKHS functions and achieve optimal rates
in non-parametric regression as long as the number of random features exceeds a certain
degrees-of-freedom quantity (Bach, 2017b; Rudi & Rosasco, 2017), which is similar to stan-
dard such quantities in the analysis of ridge regression (Caponnetto & De Vito, 2007),
at least when the data are uniformly distributed on the sphere (otherwise the quantity
involved may be larger unless features are sampled non-uniformly). Such a number of
random features is optimal for a given eigenvalue decay of the integral operator (Bach,
2017b), which implies that the shallow random feature architectures provides optimal ap-
proximation for the multi-layer ReLU kernels as well, since the shallow and deep kernels
have the same decay, up to the parity constraint. In order to overcome this constraint
for shallow kernels while preserving decay, one may consider vector-valued random fea-
tures of the form (σ(WTx), xισ(WTx),..., xdσ(WTx)) with W 〜 N(0, I), leading to a ker-
nel κσ,b(u) = (1 + u)κσ (u), where κσ is the random feature kernel corresponding to σ.
With σ(U) = max(0, U), Kσ,b has the same decay as kRf，and when σ(U) = 1{U ≥ 0} it has
the same decay as κLNTK .
3.3	Extensions to other kernels
We now provide other examples of kernels for which Theorem 1 provides approximation
properties thanks to its generality.
Laplace kernel and generalizations. The Laplace kernel kc(x, y) = e-c口”-y口 has been
found to provide similar empirical behavior to neural networks when fitting randomly la-
beled data with gradient descent (Belkin et al., 2018). Recently, Geifman et al. (2020)
have shown that when inputs are on the sphere, the Laplace kernel has the same decay
as the NTK, which may suggest a similar conditioning of the optimization problem as
for fully-connected networks, as discussed in Section 2.2. Denoting Kc(U) = e-Cʌʌ-U so
that kc(x,y) = Kc√2(xTy), We may easily recover this result using Theorem 1 by noticing
that Kc is infinitely differentiable around -1 and satisfies
κc (1 — t) = e- c √t = 1 — c √t + O (t),
which yields the same decay k-d as the NTK. Geifman et al. (2020) also consider a heuristic
generalization of the Laplace kernel with different exponents, Kc,γ (U) = e-c(1-u)γ . The-
orem 1 allows us to obtain a precise decay for this kernel as well using Kc,γ (1 - t) =
1 - ctγ + O(t2γ), which is of the form k-d-2γ+1 for non-integer γ > 0, and in particu-
lar approaches the limiting order of smoothness (d - 1)/2 when γ → 0.3
Deep kernels with step activations. We saw in Section 3.2 that for ReLU activations,
depth does not change the decay of the corresponding kernels. In contrast, when considering
3For κc and κc,γ , the ability to differentiate expansions is straightforward since we have the
exact expansion κc,γ(u) =	kck(1 —U)γk/k!, which may be differentiated term-by-term.
7
Published as a conference paper at ICLR 2021
step activations σ(U) = 1{U ≥ 0}, We show in Appendix C.3 that approximation properties
of the corresponding random neuron kernels (of the form K0 ◦•••◦ K0) improve with depth,
leading to a decay k-d-2ν+1 with ν = 1/2L-1 for L layers. This also leads to an RKHS
which becomes as large as allowed (order of smoothness close to (d - 1)/2) when L → ∞.
While this may suggest a benefit of depth, note that step activations make optimization
hard for anything beyond a linear regime with random weights, since the gradients with
respect to inner neurons vanish. Theorem 1 may also be applied to deep kernels with other
positively homogeneous activations σs (U) = max(0, U)4 s with s ≥ 2, for which endpoint
expansions easily follow from those of K0 or K1 through integration.
Infinitely differentiable kernels. Finally, we note that Theorem 1 shows that kernels as-
sociated to infinitely differentiable activations (which are themselves infinitely differentiable,
see Daniely et al. (2016)4), as well as Gaussian kernels on the sphere of the form e-c(I-* y),
have faster decays than any polynomial. This results in a “small” RKHS that only contains
smooth functions. See Azevedo & Menegatto (2014); Minh et al. (2006) for a more precise
study of the decay for Gaussian kernels on the sphere.
4	Numerical experiments
We now present numerical experiments on synthetic and real data to illustrate our theory.
Our code is available at https://github.com/albietz/deep_shallow_kernel.
Synthetic experiments. We consider randomly sampled inputs on the sphere S3 in 4 di-
mensions, and outputs generated according to the following target models, for an arbi-
trary W ∈ S3: fj(x) = 1{WTx ≥ 0.7} and f2(x) = e-(1-wɪ⑼33/ + e-(1+WT⑼332. Note
that f is discontinuous and thus not in the RKHS in general, while fS is in the RKHS
of K1 (since it is even and has the same decay as K1 as discussed in Section 3.3). In Figure 1
we compare the quality of approximation for different kernels by examining generalization
performance of ridge regression with exact kernels or random features. The regularization
parameter λ is optimized on 10 000 test datapoints on a logarithmic grid. In order to il-
lustrate the difficulty of optimization due to a small optimal λ, which would also indicate
slower convergence with gradient methods, we consider grids with λ ≥ λmin , for two different
choices of λmin . We see that all kernels provide a similar rate of approximation for a large
enough grid, but when fixing a smaller optimization budget by taking a larger λmin, the
NTK and Laplace kernels can achieve better performance for large sample size n, thanks
to a slower eigenvalue decay of the covariance operator. Figure 1(right) shows that when
using m = ʌ/n random features (which can achieve optimal rates in some settings, see Rudi
& Rosasco, 2017), the “shallow” ReLU network performs better than a three-layer version,
despite having fewer weights. This suggests that in addition to providing no improvements
to approximation in the infinite-width case, the kernel regimes for deep ReLU networks may
even be worse than their two-layer counterparts in the finite-width setting.
MNIST and Fashion-MNIST. In Table 1, we consider the image classification datasets
MNIST and Fashion-MNIST, which both consist of 60k training and 10k test images of size
28x28 with 10 output classes. We evaluate one-versus-all classifiers obtained by using kernel
ridge regression by setting y = 0.9 for the correct label and y = -0.1 otherwise. We train
on random subsets of 50k examples and use the remaining 10k examples for validation. We
find that test accuracy is comparable for different numbers of layers in RF or NTK kernels,
with a slightly poorer performance for the two-layer case likely due to parity constraints, in
agreement with our theoretical result that the decay is the same for different L. There is a
small decrease in accuracy for growing L, which may reflect changes in the decay constants or
numerical errors when composing kernels. The slightly better performance of RF compared
to NTK may suggest that these problems are relatively easy (e.g., the regression function
is smooth), so that a faster decay is preferable due to better adaptivity to smoothness.
4This requires the mild additional condition that each derivative of the activation is in L2
w.r.t. the Gaussian measure.
8
Published as a conference paper at ICLR 2021

KRR"min = le-9
rf_b 2-layer
rf 3-layer
rf 4-layer
ntk_b 2-layer
ntk 3-layer
ntk 4-layer
lap c=l
IO1	IO2	IO3
n
IO-2
4× 10-2-
3×10^2-
2×10^2-
KRR"min = le-5
6×10-2
IO1	IO2	IO3
n
----rf_b 2-layer
----rf 3-layer
—rf 4-layer
ntk_b 2-layer
ntk 3-layer
ntk 4-layer
Iapc=I
ReLU random features
-----2-layer, m=sqrt(π)
3-layer (Sqrt(n), sqrt(n))
3-layer (10, sqrt(n))
3-layer (sqrt(n), 10)
101 IO2 103
n

Figure 1: (left, middle) expected squared error vs sample size n for kernel ridge regression
estimators with different kernels on f； and with two different budgets on optimization
difficulty λmin (the minimum regularization parameter allowed). (right) ridge regression
with one or two layers of random ReLU features on f2, with different scalings of the number
of “neurons” at each layer in terms of n.
MNIST
L	RF	NTK
ɪ	98.60 ± 0.03	98.49 ± 0.02
3	98.67 ± 0.03	98.53 ± 0.02
4	98.66 ± 0.02	98.49 ± 0.01
5	98.65 ± 0.04	98.46 ± 0.02
Table 1: Test accuracies on MNIST (left) and Fashion-MNIST (right) for RF and NTK
kernels with varying numbers of layers L. We use kernel ridge regression on 50k samples,
with λ optimized on a validation set of size 10k, and report mean and standard errors
across 5 such random splits of the 60k training samples. For comparison, the Laplace kernel
with c = 1 yields accuracies 98.39 ± 0.02 on MNIST and 90.38 ± 0.06 on F-MNIST.
F-MNIST
L	RF	NTK
ɪ	90.75 ± 0.11	90.65 ± 0.07
3	90.87 ± 0.16	90.62 ± 0.08
4	90.89 ± 0.13	90.55 ± 0.07
5	90.88 ± 0.08	90.50 ± 0.05
5	Discussion
In this paper, we have analyzed the approximation properties of deep networks in kernel
regimes, by studying eigenvalue decays of integral operators through differentiability proper-
ties of the kernel function. In particular, the decay is governed by the form of the function’s
(generalized) power series expansion around ±1, which remains the same for kernels aris-
ing from fully-connected ReLU networks of varying depths. This result suggests that the
kernel approach is unsatisfactory for understanding the power of depth in fully-connected
networks. In particular, it highlights the need to incorporate other regimes in the study of
deep networks, such as the mean field regime (Chizat & Bach, 2018; Mei et al., 2018), and
other settings with hierarchical structure (see, e.g., Allen-Zhu & Li, 2020; Chen et al., 2020).
We note that our results do not rule out benefits of depth for other network architectures
in kernel regimes; for instance, depth may improve stability properties of convolutional ker-
nels (Bietti & Mairal, 2019a;b), and a precise study of approximation for such kernels and
its dependence on depth would also be of interest.
Acknowledgments
The authors would like to thank David Holzmuller for finding an error in an earlier version
of the paper, which led us to include the new assumption on differentiation of asymptotic
expansions in Theorem 1. This work was funded in part by the French government under
management of Agence Nationale de la Recherche as part of the “Investissements d’avenir”
program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). We also acknowledge
support of the European Research Council (grant SEQUOIA 724063).
References
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning per-
forms deep learning. arXiv preprint arXiv:2001.04413, 2020.
9
Published as a conference paper at ICLR 2021
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overpa-
rameterized neural networks, going beyond two layers. In Advances in Neural Information
Processing Systems (NeurIPS), 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning
via over-parameterization. In Proceedings of the International Conference on Machine
Learning (ICML), 2019b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong
Wang. On exact computation with an infinitely wide neural net. In Advances in Neural
Information Processing Systems (NeurIPS), 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In
Proceedings of the International Conference on Machine Learning (ICML), 2019b.
Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit
sphere: an introduction, volume 2044. Springer Science & Business Media, 2012.
Douglas Azevedo and Valdir Antonio Menegatto. Sharp estimates for eigenvalues of integral
operators generated by dot product kernels on the sphere. Journal of Approximation
Theory, 177:57-68, 2014.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on
Learning Theory (COLT), 2013.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal
of Machine Learning Research (JMLR), 18(1):629-681, 2017a.
Francis Bach. On the equivalence between kernel quadrature rules and random feature
expansions. Journal of Machine Learning Research (JMLR), 18(1):714-751, 2017b.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of
neural networks for learned functions of different frequencies. In Advances in Neural
Information Processing Systems (NeurIPS), 2019.
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritch-
man. Frequency bias in neural networks for input of non-uniform density. In Proceedings
of the International Conference on Machine Learning (ICML), 2020.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to
understand kernel learning. In Proceedings of the International Conference on Machine
Learning (ICML), 2018.
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complex-
ity of deep convolutional representations. Journal of Machine Learning Research (JMLR),
20(25):1-49, 2019a.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In
Advances in Neural Information Processing Systems (NeurIPS), 2019b.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-
parametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
2020.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards under-
standing the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares
algorithm. Foundations of Computational Mathematics, 7(3):331-368, 2007.
Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same rkhs.
In Proceedings of the International Conference on Learning Representations (ICLR), 2021.
10
Published as a conference paper at ICLR 2021
Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard
Socher. Towards understanding hierarchical learning: Benefits of neural representations.
In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable pro-
gramming. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in
Neural Information Processing Systems (NIPS), 2009.
Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bul letin of
the American mathematical society, 39(1):1-49, 2002.
Amit Daniely. Depth separation for neural networks. In Conference on Learning Theory
(COLT), 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural
networks: The power of initialization and a dual view on expressivity. In Advances in
Neural Information Processing Systems (NIPS), 2016.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In Proceedings of the International Conference
on Machine Learning (ICML), 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably opti-
mizes over-parameterized neural networks. In Proceedings of the International Conference
on Learning Representations (ICLR), 2019b.
Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World
Scientific, 2014.
Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics,
38(1):1-50, 2010.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In
Conference on Learning Theory (COLT), 2016.
Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel
for linear-width neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Philippe Flajolet and Robert Sedgewick. Analytic combinatorics. Cambridge University
press, 2009.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen
Basri. On the similarity between the laplace and neural tangent kernels. In Advances in
Neural Information Processing Systems (NeurIPS), 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized
two-layers neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks
are universal approximators. Neural networks, 2(5):359-366, 1989.
Mourad Ismail. Classical and quantum orthogonal polynomials in one variable, volume 13.
Cambridge university press, 2005.
11
Published as a conference paper at ICLR 2021
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in Neural Information Processing
Systems (NIPS), 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and
Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In Proceedings of the
International Conference on Learning Representations (ICLR), 2018.
Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et al. Generalized leverage score
sampling for neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochas-
tic gradient descent on structured data. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the risk of minimum-norm in-
terpolants and restricted lower isometry of kernels. In Conference on Learning Theory
(COLT), 2020.
Alexander Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. arXiv preprint
arXiv:1804.11271, 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):
E7665-E7671, 2018.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation
theory perspective. Analysis and Applications,14(06):829—848, 2016.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and
smoothing. In Conference on Learning Theory (COLT), 2006.
Radford M Neal. Bayesian learning for neural networks. Springer, 1996.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica,
8:143-195, 1999.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In
Advances in Neural Information Processing Systems (NIPS), 2007.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems, pp. 3215-3225, 2017.
Meyer Scetbon and Zaid Harchaoui. Risk bounds for multi-layer perceptrons through spectra
of integral operators. arXiv preprint arXiv:2002.12640, 2020.
Johannes Schmidt-Hieber et al. Nonparametric regression using deep neural networks with
relu activation function. Annals of Statistics, 48(4):1875-1897, 2020.
Alex J Smola, Zoltan L Ovari, and Robert C Williamson. Regularization with dot-product
kernels. In Advances in Neural Information Processing Systems (NIPS), 2001.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff
bound. arXiv preprint arXiv:1906.03593, 2019.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory
(COLT), 2016.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions.
In Proceedings of the International Conference on Artificial Intel ligence and Statistics
(AISTATS), 2017.
12
Published as a conference paper at ICLR 2021
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv
preprint arXiv:1907.10599, 2019.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Net-
works, 94:103-114, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent
optimizes over-parameterized deep relu networks. Machine Learning, 2019.
A Background on Spherical Harmonics
In this section, we provide some background on spherical harmonics needed for our study
of approximation. See (Efthimiou & Frye, 2014; Atkinson & Han, 2012; Ismail, 2005)
for references, as well as (Bach, 2017a, Appendix D). We consider inputs on the d - 1
sphere SdT = {x ∈ Rd, ∣∣x∣∣ = 1}.
We recall some properties of the spherical harmonics Yk,j introduced in Section 2.2.
For j = 1, ...,N (d, k), where N (d, k) = 2 k +dd - (k +--3), the spherical harmonics Ykj are
homogeneous harmonic polynomials of degree k that are orthonormal with respect to the
uniform distribution τ on the d-1 sphere. The degree k plays the role of an integer fre-
quency, as in Fourier series, and the collection {Yk,j, k ≥ 0, j = 1, . . . , N(d, k)} forms an
orthonormal basis of L2(Sd-1 ,dτ). As with Fourier series, there are tight connections be-
tween decay of coefficients in this basis w.r.t. k, and regularity/differentiability of functions,
in this case differentiability on the sphere. This follows from the fact that spherical harmon-
ics are eigenfunctions of the Laplace-Beltrami operator on the sphere ∆Sd-1 (see Efthimiou
& Frye, 2014, Proposition 4.5):
∆Sd-1 Yk,j = -k(k + d - 2)Yk,j.	(14)
For a given frequency k, we have the following addition formula:
N (d,k)
底 Ykj (X)Ykj (y) = N(d,k)Pk (XTy),	(15)
j=1
where Pk is the k-th Legendre polynomial in dimension d (also known as Gegenbauer poly-
nomial when using a different scaling), given by the Rodrigues formula:
Pk(t) = (-1 /2)kr(k(W) (1 - 12)(3-d)/2 (dt j (1 - t2)k +(d-3)/2.	(16)
Note that these may also be expressed using the hypergeometric function 2F1 (see, e.g.,
Ismail, 2005, Section 4.5), an expression we will use in proof of Theorem 1 (see the proof of
Lemma 6).
The polynomials Pk are orthogonal in L2 ([-1, 1], dν) where the measure dν is given by the
weight function dν (t) = (1 -t2)(d-3)/2dt, and we have
「Ρk(t)(1 - 12)(d-3)/2dt = 3 vɪ,	(17)
-1	ωd-2 N(d, k)
where ωp-ι = Γ∏p∕2) denotes the surface of the sphere SP-1 in P dimensions. Using the
addition formula (15) and orthogonality of spherical harmonics, we can show
/ Pj(W t x ) Pk(W T y)dτ (W) = Nδjk) Pk(x t y)	(18)
We will use two other properties of Legendre polynomials, namely the following recurrence
relation (Efthimiou & Frye, 2014, Eq. 4.36)
k	k+d 2
tPk(t) = ^d-2 Pk -1( t) + 2k⅛ Pk +1( t),	(19)
13
Published as a conference paper at ICLR 2021
for k ≥ 1, and for k = 0 we simply have tP0(t) = P1 (t), as well as the differential equation
(see, e.g., Efthimiou & Frye, 2014, Proposition 4.20):
(1 - t2)PKt) + (1 - d)tPk(t) + k(k + d - 2)Pk(t) = 0.	(20)
The Funk-Hecke formula is helpful for computing Fourier coefficients in the basis of spherical
harmonics in terms of Legendre polynomials: for any j = 1, . . . , N (d, k), we have
/ f (XTy)Ykj(y)dτ(y) =	Ykj(X) L1 f (t)Pk(t)(1 - 12)(d-3)/2dt.	(21)
For example, we may use this to obtain decompositions of dot-product kernels by computing
Fourier coefficients of functions K(〈x, •〉). Indeed, denoting
μk
ωd-2
3d-1
∕11
κ(t)Pk (t)(1 - t2)(d-3)/2dt,
writing the decomposition of K(〈x, •〉) using (21) leads to the following Mercer decomposition
of the kernel:
∞	N (d,k)	∞
K(χty) =工 μk 工 Ykj(X)γk,j Iy) =工 μkN(d,k)Pk(χty).	(22)
k=0	j=	k=0
B Proof of Theorem 1
The proof of Theorem 1, stated below in full as Theorem 7, proceeds as follows. We first
derive an upper bound on the decay of K of the form k-d-2ν+3 (Lemma 5), which is weaker
than the desired k-d-2ν+1 , by exploiting regularity properties of K through integration by
parts. The goal is then to apply this result on a function K = K—ψ, where ψ is a function that
allows us to “cancel” the leading terms in the expansions of K, while being simple enough
that it allows a precise estimate of its decay. In the proof of Theorem 7, we follow this
strategy by considering ψ as a sum of functions of the form t → (1 - t2 )ν and t → t(1 - t2 )ν,
for which we provide a precise computation of the decay in Lemma 6.
Decay upper bound through regularity. We begin by establishing a weak upper bound
on the decay of K (Lemma 5) by leveraging its regularity up to the terms of order (1 - t2)ν.
This is achieved by iteratively applying the following integration by parts lemma, which
is conceptually similar to integrating by parts on the sphere by leveraging the spherical
Laplacian relation (14) in Appendix A, but directly uses properties of K and of Legendre
polynomials instead (namely, the differential equation (20)). We note that the final state-
ment in Theorem 1 on infinitely differentiable K directly follows from Lemma 5.
Lemma 4 (Integration by parts lemma). Let K : [-1, 1] → R be a function that is C∞
on (-1, 1) and such that Kz(t)(1 - 12)1+ d-3 = O(1). We have
[1 K(t)Pk(t)(1 - t2)d-3dt =	1	( - K(t)(1 - 12)1+d-3Pk(t)∣1	(23)
-1	k(k + d - 2) -1
+ KK(t)(1 - 12)1+中Pk(t)∣1ι +
(24)
with K(t) = — k"(t)(1 — t2) + (d — 1)tKk(t).
/ K(t)Pk(t)(1 - 12)(d-3)/2dt),
Proof. In order to perform integration by parts, we use the following differential equation
satisfied by Legendre polynomials (see, e.g., Efthimiou & Frye, 2014, Proposition 4.20):
(1 - t2)PKt) + (1 - d)tPk(t) + k(k + d - 2)Pk(t) = 0.	(25)
14
Published as a conference paper at ICLR 2021
Using this equation, we may write for k ≥ 1,
I K(t)Pk(t)(1 - 12)(d-3)/2dt =电二 %((d - 1) I tκ(t)Pk(t)(1 - 12)⅛3dt	(26)
-1	k(k + d - 2)
- / K (t) Pk (t )(1 - 12)1+ 与3 dt).	(27)
We may integrate the second term by parts using
—(κ(t)(1 - 12)1+d-3) = κ'(t)(1 - 12)1+ d-3 - 21(1 + (d - 3)/2)κ(t)(1 - t2)d-3
dt
=K/(t)(1 - 12)1+ 片-(d - 1)tκ(t)(1 - t2)d-3.	(28)
Noting that the first term in (26) cancels out with the integral resulting from the second
term in (28), we then obtain
Lκ (t) Pk(t )(1 -12 产3)/2dt=k (k +1d - 2) GK (t )(1 -12)1+宁Pk (t)L
L
+
K'(t)(1 - 12)1+d-3Pk(t)dt).
Integrating by parts once more, the second term becomes
11	.
J IK'(t)(1 - 12)1+ɪPk(t)dt = KK(t)(1 - 12)1+ ɪPk(t)|
1
-1
—/ (k"(t)(1 — 12) — (d — 1)tK/(t))Pk(t)(1 — 12)(d-3)/2dt.
-1	(29)
The desired result follows.
□
Lemma 5 (Weak upper bound on the decay). Let K : [-1, 1] → R be a function that is C∞
on (-1, 1) and has the fol lowing expansions around ±1 on its derivatives:
K(j)(t) = pj,1(1 - t) + O((1 - t)ν-j)	(30)
K(j)(t) = pj,-1(1 + t) + O((1 + t)ν-j),	(31)
for t ∈ [-1, 1] and j ≥ 0, where pj,1, pj,-1 are polynomials and ν may be non-integer. Then
the Legendre coefficients μk(K) of K given in (8) satisfy
μk (K) = O (k - d-2 V+3).	(32)
Proof. Let f0 := K and for j ≥ 1
fj(t) ：= -fj-ι(t)(1 - 12) + (d - 1)fj-ι(t).	(33)
Then fj is C∞ on (-1, 1) and has similar expansions to K of the form
fj(t) = qj,1(1 - t) + O((1 - t)ν-j)	(34)
fj(t) = qj,-1(1 + t) + O((1 + t)ν-j),	(35)
for some polynomials qj,±1. We may apply Lemma 4 repeatedly as long as the terms in
brackets vanish, until We obtain, for j = ∖V + d-3 ^∣ — 1,
L
K(t)Pk(t)(1 - t2)(d-3)/2dt
(k(k + J- 2))j+ι (fj(t)(1 - t2)1+ +d"Pk(tXi + J(t)Pk(t)(1 - t2)(d-3)/2d J
Given our choice for j, We have fj(t)(1 — 12)1+d-3 = O(1), and fj +i(t)(1 — 12)(d-3)/2 =
O((1 — 12)-1+e) for some c > 0. Since Pk(t) ∈ [—1, 1] for any t ∈ [—1, 1], we obtain
μk (κ) = O (k-2(j+1)) = O (k - d-2 ν+3).	□
15
Published as a conference paper at ICLR 2021
Precise decay for simple function. We now provide precise decay estimates for func-
tions of the form t → (1 - t2)ν and t → t(1 - t2)ν , which will lead to the dominant terms
in the decomposition of κ in the main theorem.
Lemma 6 (Decay for simple functions φν and φν). Let φν (t) = (1 - t2)ν, with ν > 0 non-
integer, and let μk (φν) denote its Legendre coefficients in d dimensions given by ωd-2 J-11(1-
12)ν +(d-3)/2Pk(t)dt. We have
•	μk (φν) = 0 if k is odd
•	μk (φν)〜C(d, V)k-d-2ν-1 for k even, k → ∞, with C(d, V) a constant.
Analogously, let φV(t) := t(1 — 12)ν. We have
•	μk (φν) = 0 if k is even
•	μk (φ V) 〜C(d, v)k-d-2ν-1 for k odd, k → ∞, with C(d, V) a constant.
Proof. We recall the following representation of Legendre polynomials based on the hyper-
geometric function (e.g., Ismail, 2005, Section 4.5):5
Pk (t) = 2F1(—k, k + d — 2; (d — 1)/2; (1 — t)/2),	(36)
where the hypergeometric function is given in its generalized form by
∞ ∞	Cd	h	s∞ (aI)S …(aP)S xs	Gn
pFq (a 1,.. .,ap; b 1 ,...,bq; x) —〉J (b )	(b ) S! ,	(37)
where (a)S = Γ(a + S)/Γ(a) is the rising factorial or Pochhammer symbol.
Using the above definitions and the integral representation of Beta functions, we then have
1
J (1 — 12)ν + ɪ Pk (t) dt = 22 V+d-3
V + d-3
Pk(t)dt
k
22V+d-3 工
S=0
k
22V+d-2 工
S=0
k
22V+d-2 工
S=0
d-3
(—k) S (d ——2 + k) S / (1 ——t ʌ 2	(1 + t ʌ
(dΞ1pJ-( F)	r)
Lk)S(d- 2 + k)S [1 (1 — X)v+⅛3+SxV+⅛3dx
(⅛1)S S!	Jo
(—k)S(d — 2 + k)S Γ(v + S + d-1 )Γ(v + d21)
V + d-3
dt
(⅛1) S S!
Γ( v + d-1 )2
Γ(2 V + d — 1)
Γ( v + d-1 )2
Γ(2 v + d — 1)
k
Σ
S=0
3F2
Γ(2V + S + d — 1)
(—k ) S ( d - 2 + k) S ( v + d--1 ) S
(d-1 ) S (2v + d — I)SS!
(—k,k + d — 2 ,v + (d — 1) / 2 八
I	(d — 1) / 2, 2 V + d — 1	I1J .
Now, we use Watson’s theorem (e.g., Ismail, 2005, Eq. (1.4.12)), which states that
F ( a,b,c I 八=γ( 1 )r(c + 1 )γ( + )r(c + +)
3 2 Va + b + 1)/2, 2C1 )	Γ(a+1 )Γ(b++1 )Γ(C + 1-a)	.
(38)
We remark that with a = —k, b = k + d — 2, C = v + (d — 1)/2, our expression above is of the
form of Watson's theorem, and We may thus evaluate μk in closed form. Indeed, We have
———k, k + d — 2, v + (d — 1)/21 ʌ
3 F 2(	(d - 1) / 2, 2 V + d — 1 l1J
Γ( 2 )Γ(v + d )Γ( ⅜1 )Γ(v +1)
). (39)
Γ( 1-k )Γ( d+-1 )Γ( V + 2 + d )Γ( V + 1 — 2
5Here we normalize such that Pk (1) = 1 as is standard for Legendre polynomials, in contrast
to (Ismail, 2005) where the standard Jacobi/Gegenbauer normalization is used.
16
Published as a conference paper at ICLR 2021
When k is odd, then (1 - k)/2 is a non-positive integer so that the denominator is infinite
and thus μk vanishes. We assume from now on that k is even, making the denominator is
finite. Using the following relation, for e ∈ Z and an integer n:
Γ(1 + E)	/	1、 n n n n ∖n-1 Γ(n + 1 — E)
Γ(7-n) =E ( e - 1)…( e - n ) = (-1)n 1 γ(-e )
(40)
we may then rewrite
，-k,k + d - 2 ,ν + (d - 1) / 2∣Λ
3 F 2(	(d - 1) / 2, 2 V + d - 1	∣1J
Γ(ν + 2)Γ(d-1 )Γ(v + 1)	Γ(k++1 )Γ(k - v)
Γ(-2)Γ(V + 2)Γ(-V - 1) Γ(d+-1 )Γ(v + 22 + d)'
(41)
When k → ∞, Stirling’s formula Γ(x)〜xx- 1 e-x√2∏ yields the equivalent
Γ(k+1 )Γ(2 - V)
Γ( d+2-1 )Γ( v + k + d)
(2)
-d-2ν +1
This yields
with
μk 〜C(d, V)k-d-2V +1,
C(d, V) = 22ν+d-2
ωd-2 γ( V + d-1 )2 γ( V + d )γ( d-1 )γ( V + I)
ωd-1 Γ(2V + d - 1) Γ(-2)Γ(V + 2)Γ(-V - 1)
(1/2)
-d-2ν +1
(42)
(43)
(44)
Decay for φν . The decay for φν follows from the decay of φν and the recurrence rela-
tion (Efthimiou & Frye, 2014, Eq. (4.36))
k	k+d 2
tPk(t )= 2kT^ Pk-1(t) + 2k+d-l Pk+1(t)，	(45)
which ensures the same decay with a change parity.	□
Final theorem. We are now ready to prove our main theorem, which differs from the
simplified statement of Theorem 1 by the technical assumption that only a finite number r
of terms of order between V and V + 1 are present in the series expansions around ±1.
Theorem 7 (Main theorem, full version). Let κ : [-1, 1] → R be a function that is C∞
on (-1, 1) and has the fol lowing expansions around ±1:
r
K (t )= m(1 - t) + 工叼,1(1 - t 产 + O ((1 - t)ν l+1++)	(46)
j=1
r
K (t) = P-1(1 + t) + 工 Cj,-1(1 + t )νj + O ((1 + t)ν 1+1++),	(47)
j=1
for t ∈ [-1, 1], where p1, p-1 are polynomials and 0 < V1 < . . . < Vr are not integers and
0 < e < V2 一 V1. We also assume that the derivatives K(s) of K have the following expansions:
K(s)(t) = Ps, 1(1 - t) + (-1)s £Cj,1	IVj +1)、(1 - t)"j-s + O((1 - t)νl + 1++ -)	(48)
j=1	Γ(Vj + 1 - s)
K (s)(t) = ps,-1(1 + t)+ £ cj,-1 二(IVj I? (1 + t )νj-s + O ((1 + t)ν l + 1++ -),	(49)
j=1	Γ(Vj + 1 - s)
for some polynomials Ps,±1 . Then we have, for an absolute constant C(d, V1) depending only
on d and V1 ,
•	For k even, if Cν 1,1 = -C1,-1: μ2 〜(C1,1 + C1,-1)C(d,V 1)k-d-2ν 1 + 1;
•	For k even, if c 1,1 = — c 1,-1: μ2 = o(k-d-2V1 + 1);
17
Published as a conference paper at ICLR 2021
•	For k odd, if c 1,1	=	C1 ,-1：	μk	〜(C1,1 — C1,-i)C(d, V1)k-d-2IV1+1.
•	For k odd, if c 1,1	=	c 1,-1:	μk	=	o(k-d-2ν 1+1).
Proof. Define the functions
/小	φVj (t) + φVj (t)	φVj (t) - φVj (t)
ψj(t) = cj, 1	2 Vj + 1	+ cj,-1	2 Vj+1	(50)
=j2+ji φνj (t) + j2jF φνj (t),	(51)
for j = 1, . . . , r, where φV ,φV are defined in Lemma 6. We have the asymptotic expansions:6
ψ 1(t) = c 1,1(1 - t)ν 1 - (1 + VI)C 1,1 + c1 T (1 - t)ν 1 + 1 + O((1 - t)ν 1+1+e)
ψ 1(t) = C1,-1 (1 + t)ν 1 + C1,1 - (1+ V)c1 T (1 + t)ν 1 + 1 + O((1 + t)ν 1+1++),
and for j ≥ 2,
ψj(t) = Cj, 1(1 - t)Vj + O((1 - t)ν 1 + 1++)
ψj (t ) = Cj,-1(1+ t)Vj + O ((1+ t)ν 1 + 1++).
Define additionally ψr+1 the same way as the other ψj , with Vr+1 = V1 + 1, Cr+1,1 =
((1 + V1)C1,1 + C1,-1)/2, and Cr+1,-1 = -(C1,1 - (1 + V)C1,-1)/2, which satisfies a similar
asymptotic expansion as the above ones for j ≥ 2. One can check that the derivatives
of the ψj can be expanded with the derivatives of the expansions above. Then, defining
K = K — E； +1 ψj, We have for S ≥ 0,
κ(s)(t )= Ps, 1(1 - t) + O ((1 - t)ν 1 + 1+一)	(52)
K( s)(t) = ps,-1(1 + t) + O ((1 + t)ν 1 + 1+e - s).	(53)
The functions ψj satisfy
ʃ j +c1-1 μk(φVj),	if k even，
μkM) = jc⅛c户μk(φVj),	if k odd.	(54)
By Lemma 5, We have
r
μk(K) = μk(K) +	μk(ψj)	(55)
j=1
r
=工 μk (Ψj)+ O (k - d-2(ν 1+1++)+3)	(56)
j=1
r
=工 μk (ψj)+ o (k - d-2 ν 1 + 1).	(57)
j=1
The result then folloWs from Lemma 6, With a constant C(d, V1)/2V1+1, Where C(d, V1) is
given by the proof of Lemma 6.	□
B.1	Dimension-free description
While our above description of the RKHS depends on the dimension d, in some cases a
dimension-free description given by Taylor coefficients of the kernel K at 0 may be useful, for
instance for the study of kernel methods in certain high-dimensional regimes (e.g., El Karoui,
2010; Ghorbani et al., 2019; Liang et al., 2020). Here We remark that such coefficients and
6These are obtained by writing ψj (t) = (a + bt)(1 + t)ν (1 - t)ν and computing, e.g., the first
two terms in the analytic expansion of t → (a + bt)(1 + t)ν around 1.
18
Published as a conference paper at ICLR 2021
their decay may be recovered from the Legendre coefficients in d dimensions,
dimensional limits d → ∞. We illustrate this on the functions φν (t) = (1 -
by taking high-
2ν
t ) , for which
Lemma 6 provides precise estimates of the Legendre coefficients μk,d(φν) in d dimensions
(this only serves as an instructive illustration, since in this case Taylor coefficients may be
computed directly through a power series expansion of φν using the Binomial formula).
Lemma 8 (Recovering Taylor coefficients of φν through high-dimensional limits).
(k)
Let bk(φν) :=	for some non-integer ν > 0. For k even, we have
bk (Φ ) = °” 2 k 与：，
for a constant CV depending only on ν. This leads to an equivalent bk 〜 CVk-ν-1 for k → ∞
with k even.
Proof. Assume throughout that k is even. Recall the expression of the Legendre coeffi-
cients μk,d(φν) of φν in d dimensions (We include d as a subscript for more clarity here)
from the proof of Lemma 6:
μk,d (φν )
1
%2	K(t)Pk,d(t)(1 - t2)ɪdt
ωd-1	-1
(58)
+d—2 ωd-2「(V + d-1 )2「(V + d)「(d-1 )「(V +1)	「(^⅛1 )「(2 - V)
ω∑Γ Γ(2V + d - 1) Γ(-2)Γ(V + 2)Γ(-V - 1) Γ(d+2-1 )Γ(ν + 2 + 2)'
(59)
NoW, note that When d is large enough compared to k, We may use the Rodrigues formula (16)
and integration by parts to obtain the folloWing alternative expression:
μkd(φ)=2-k ωd-2 方』φν)(t )(1 -12) k+宁dt
FolloWing similar arguments to Ghorbani et al. (2019), We may then use dominated conver-
gence to shoW:
√¾ Lφ νk)(t )(1-12)k+号心φ (k)(0)
as d → ∞.
Indeed, 二((d-“ (1 - 12)(d—3)/2 is
v π r( 2 )
When d → ∞. This yields
a probability density that approaches a Dirac mass at 0
bk(φV)
尊=lim 2kωd-1	,芈(k+ 吟)	μkd(φν).
k!	d→∞ 3d-2 √πΓ(d-1 )Γ(d-1 )Γ(k + 1)” ,出’
Plugging (59) and using Stirling’s formula to take limits d → ∞ yields
b (φ ) = Cv 2 k	，
Where CV only depends on V. Using Stirling’s formula once again yields the desired equiva-
lent bk(φν)〜 CVk—ν-1 for k → ∞, k even, with a different constant CV.	□
We note that a similar asymptotic equivalent holds for bk (φV) for k odd. The next re-
sult leverages this to derive asymptotic decays of bk (κ) for any κ of the form κ(u) =
k≥0 bk (κ)uk satisfying similar conditions as in Theorem 7.
Corollary 9 (Taylor coefficients of κ). Let κ : [-1, 1] → R be a function admitting a power
series expansion κ(u) =	k≥0 bkuk, with the fol lowing expansions around ±1:
r
K (t )= P 1(1 - t)+ 工 Cj,ι(1 - t 产 + O ((1 - t)「VI1+1)	(60)
j=1
r
κ (t )= P -1 (1+ t)+ 工 c∕,-ι(1+ t )νj + O ((1+ t )「V 11+1),	(61)
j=1
19
Published as a conference paper at ICLR 2021
for t ∈ [-1, 1], where p1 , p-1 are polynomials and 0 < ν1 < . . . < νr are not integers and
0 < e < ν2 — Vι. Then We have, for an absolute constant C(Vι) depending only on Vι,
•	For	k	even, if Cνι, 1 = — Cνι,-1:	bk 〜(CV 1,1 + CVι,-i)C(Vι)k-V1-1 ;
•	For	k	even, if Cνι, 1 = — Cν 1,-1:	bk = o(k-V1-1);
•	For	k	odd, if Cν 1,1 = Cν 1,-1: bk	〜(CV 1,1 — CVι,-1)C(V1)k-V1-1.
•	For	k	odd, if CV1,1 = CV1 ,-1 : bk	= o(k-V1-1).
Proof. As in the proof of Theorem 7, We may construct a function ψ = Ej αj φ% + αj @ν§,
with α 1 = 5g++11-1, (α1 = 01 g-1+11-1 for j = 1, the other terms being of higher orders Vj >
V1, such that K := K — ψ (which is also a power series with convergence radius ≥ 1) satisfies
κ( t) = P 1(1 — t)+ O ((1 — t AV 1l + 1)	(62)
K( t) = p-1(1 + t)+ O ((1 + t )「V 1l + 1),	(63)
It follows that K(「V 1〕+I) (1) is bounded, so that the Taylor coefficients of κK, denoted bk (κK),
satisfy
bk (K) = o (k TV 1]T) = o (k - v 1-1).
The result then follows from Lemma 8 by using the decays of bk(Φv) and bk(φV).
□
C Other Proofs
In this section, we provide the proofs for results in Section 3.3 related to obtaining power
series expansions (with generalized exponents) of kernels arising from deep networks, which
leads to the corresponding decays by Theorem 1. We note that for the kernels considered in
this section, we can differentiate the expansions since the kernel function is ∆-analytic (see
Chen & Xu, 2021, Theorem 7), so that the technical assumption in Theorem 1 is verified.
C.1 Proof of Corollary 2
Proof. Let K := K1 ◦…。K1 = κRp. We have
,^^—^^^1^^-^―^
I -1 times
K 1(1 — t) = 1 — t + ct3/2 + o(t3/2), c :=，2.
3π
We now show by induction that K(1 — t) = 1 — t + agt3/2 + o(t3/2), with ag = (£ — 1)c. This
is obviously true for £ = 2 since K = K1, and for £ ≥ 3 we have
K (1 — t) = k 1 (K-1(1 — t))
=κ 1 (1 — t + ag-113 / 2 + o (t3 / 2))
=1 — (t — a,-113 / 2 + o (t3 / 2))+ c (t + O (t3 / 2))3 / 2 + o (O (t )3 / 2)
=1 — t + a,-13 / 2 + ct3 / 2(1 + O (t1 / 2))3 / 2 + o (t3 / 2)
=1 — t + a,-113 / 2 + ct3 / 2(1 + O (t1 / 2)) + o (t3 / 2)
=1 — t + a,t3 / 2 + o (t3 / 2),
which proves the result.
Around —1, we know that
K1(—1 + t) = ct3/2 + o(t3/2).
We then have κ,(—1 + t) = b, + c,t3/2 + o(t3/2), with 0 ≤ b, < 1 and 0 < c, ≤ C (and
the upper bound is strict for £ ≥ 3). Indeed, this is true for £ = 2, and for £ ≥ 3 we have,
20
Published as a conference paper at ICLR 2021
for t > 0,
K (-1 + t) = K 1( κ”1(-1 + t))
=K 1®-1 + cg-113 / 2 + o( t3 / 2))
=K 1也-1) + κ 1( b"i) c£ -113 3 2 + o( t3 3 2).
Now, note that Kι and K1 are both positive and strictly increasing on [0, 1], with Kι(1)=
κ 1(1) = 1. Thus, we have b = Kι(b£-ι) ∈ (0, 1), and “ = K 1(a-ι)“-ι < “-ι, thus
completing the proof.
Since CE is bounded while a grows linearly with £, the constants in front of the asymptotic
decay k-d-2 grow linearly with £.
□
C.2 Proof of Corollary 3
Proof. We show by induction that KNTK as defined in (7) satisfies
KNTK(I- t) = £ -(2 S) Ct132 + o(t132),	C := √2.
For £ = 2 we have kNtk(U) = UK0(u) + K 1(u), so that
KNTK(I - t) = (1 - t)(1 - ct132 + o(t132)) + 1+ O(t) = 2 - ct132 + o(t132).
By induction, for £ ≥ 3, we have k^tk(U) = KNTK(U)K0(K-1(U)) + K(U), with K as in the
proof of Corollary 2, which hence satisfies K(1 — t) = 1 — t + o(t) for all £ ≥ 2. We then
have
κ0(K-1(1 - t)) = κ0(1 - t + o(t))
= 1 - C(t + o(t))132 + o(t132)
=1 - ct132(1 + o(t132))+ o(t132)
= 1 - Ct132 + o(t132).
This yields
E-2
KNTK(I - t) = (£ - 1 -(2 S)ct132 + o(t132))(1 - ct132 + o(t132)) + 1+ O(t)
S = 1
=£-£ S) ct13 2 + o (t13 2)
s =1
=£ - £(£-ct132 + o(t132),
which proves the claim for the expansion around +1.
Around -1, recall the expansion from the proof of Corollary 2, KE(-1 + t) = bg + O(t332),
with 0 ≤ b)E < 1. For £ = 2, we have
K2NTK (-1 + t) = (-1 + t)(ct132 + o(t132 )) + b2 + o(t132) = b2 - ct132 + o(t132).
Note also that for £ ≥ 2,
K 0( K (-1 + t)) = K 0( bE + O (t33 2)) = K 0( bE) + O (t33 2),
since K0(bg) is finite for bg < 1. We also have K0(bg) ∈ (0, 1) since K0 is positive and strictly
increasing on [0, 1] with K0(1) = 1. Then, by an easy induction, we obtain
κNTK (-1 + t) = aE - cEt 132 + o(1132),
21
Published as a conference paper at ICLR 2021
with ag ≤ £ and 0 < c2 < c.
Similar to the case of the RF kernel, the constant in front of t1 /2 grows with £2 for the
expansion around +1 but is bounded for the expansion around -1, so that the final constants
in front of the asymptotic decay k-d grow quadratically with £. However, they grow linearly
with £ when considering the NTK normalized by £, KM = KNTK /£, which then satisfies
K m ⑴=1.
□
C.3 Deep networks with step activations
In this section, we study the decay of the random weight kernel arising from deep networks
with step activations, as presented in Section 3.3. For an L-layer network, this kernel is of
the form KL := Ko ◦ ∙∙∙ ◦ K0.
L-1 times
Corollary 10. KsL has a decay k-d-2νL+1 with νL = 1/2L-1 for L layers.
Proof. We show by induction that we have, for £ ≥ 2,
Ks (1 - t ) = 1 - c E j=0 2-j t1 /21-1 + o (t1 /21-1),
with C :=递. This is true for £ = 2 due to the expansion for K°. Now assume it holds
for £ ≥ 2. We have
IKS +1(1 - t) = K0(kS (1 - t))
=κ0(1 - cEj=1 2-jt1 /2 j + o(t1 /2 j))
/ Xɔg —1 —j	n ∖ 1 /2	_
=1 - c 卜Ej=0 2-j t1 /22-1 + o(t1 /22 ))	+ o(o(t1 /22-1 )1 /2)
=1 — cEj=0 2 t1 /22 (1 + o(1)) + o(t1 /22)
=1 - CE2=0 2-j 11 /22 + o(11 /22 ),
proving the desired claim.
Around -1, we have K0(-1 + t) = ct1 /2 + o(t1 /2), and for £ ≥ 3, Ks(-1 + t) = as + O(t1 /2),
by an easy induction using the fact that K0([0, 1)) ⊂ (0, 1) and K0 is smooth on [0, 1). Thus
the behavior around -1 does not affect the decay of KS for £ ≥ 3, and Theorem 1 leads to
E 2-12-j
the desired decay, with a constant that only depends on £ through C乙j=0 , which lies in
the interval [c2, c] for any £.	□
22