Published as a conference paper at ICLR 2021
Robust Curriculum Learning: from clean la-
bel detection to noisy label self-correction
Tianyi Zhou*, Shengjie Wang*,JeffA. Bilmes
University of Washington, Seattle
{tianyizh,wangsj,bilmes}@uw.edu
Ab stract
Neural network training can easily overfit noisy labels resulting in poor gener-
alization performance. Existing methods address this problem by (1) filtering
out the noisy data and only using the clean data for training or (2) relabeling the
noisy data by the model during training or by another model trained only on a
clean dataset. However, the former does not leverage the features’ information
of wrongly-labeled data, while the latter may produce wrong pseudo-labels for
some data and introduce extra noises. In this paper, we propose a smooth transition
and interplay between these two strategies as a curriculum that selects training
samples dynamically. In particular, we start with learning from clean data and
then gradually move to learn noisy-labeled data with pseudo labels produced
by a time-ensemble of the model and data augmentations. Instead of using the
instantaneous loss computed at the current step, our data selection is based on the
dynamics of both the loss and output consistency for each sample across historical
steps and different data augmentations, resulting in more precise detection of both
clean labels and correct pseudo labels. On multiple benchmarks of noisy labels,
we show that our curriculum learning strategy can significantly improve the test
accuracy without any auxiliary model or extra clean data.
1	Introduction
The expressive power and high capacity of deep neural networks (DNNs) result in accurate modeling
and promising generalization if provided with sufficient data and clean(correct) labels. However,
recent studies show that the training process is fragile and can easily overfit on noisy labels (Zhang
et al., 2017), which commonly appear in real-world data since precise annotation is not always
available or affordable. Hence, it is important to study the training dynamics affected by imperfect
labels and develop robust learning strategies that ideally eliminate the negative impact of noisy labels
while fully exploiting the information from all the available data.
Numerous approaches have been developed to address this challenge from various perspectives, e.g.,
loss correction (Xiao et al., 2015; Vahdat, 2017; Lee et al., 2018; Veit et al., 2017; Li et al., 2017b),
robust loss functions (Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019; Ma et al., 2020)
with provable noise tolerance, sample re-weighting (Patrini et al., 2017), curriculum learning (Kumar
et al., 2010; Jiang et al., 2018; Guo et al., 2018), model co-teaching (Han et al., 2018), etc. A principal
methodology behind a variety of methods is to detect clean labels while discard/downweigh the
data with wrong labels, so the model mainly learns from correct labels. A broadly applied criterion
is to select the samples with small losses and treat them as clean data. It is inspired by empirical
observations that DNNs learn simple patterns first before overfitting on the noisy labels (Zhang et al.,
2017; Arpit et al., 2017). Several curriculum learning methods utilize this criterion (Kumar et al., 2010;
Jiang et al., 2014), and in each step, select/upweigh samples with small losses. Robust loss functions
also suppress the large losses associated with the possibly wrong labels. More recent approaches use
mixture models (Arazo et al., 2019) to estimate the distribution of losses for clean and noisy data.
However, the instantaneous loss (i.e., the loss evaluated at the current step) of an individual sample is
an unstable signal that can rapidly fluctuate due to DNN training’s randomness. The error generated
by such an unstable metric accumulates when the selected samples are used to train the model
producing the losses. Co-teaching methods alleviate this problem by training two DNNs and using
the loss computed on one model to guild the other. Also, as the model changes during training, each
sample’s loss needs to be re-evaluated even when it is not selected, which requires extra inference
1
Published as a conference paper at ICLR 2021
cost. MentorNet (Jiang et al., 2018) and Data Parameters (Saxena et al., 2019) train an extra model
to produce the sample weights or selection results without computing the loss. Furthermore, it may
not be efficient to repeatedly train the model only on clean data that consistently have small losses,
since the model have already learned, well memorized or overfitted to them.
A primary drawback of training only on clean labels detected is that discarding the whole data
pairs (x, y) with wrong labels y removes potentially useful information about the data distribution
p(x) (Arazo et al., 2019). Hence, there has been growing interest in leveraging noisy data. Loss
correction methods aim to correct the predicted class probabilities based on an estimated mislabeling
probability between classes. Some other methods seek to relabel them by using the model itself
(e.g., bootstrapping loss (Reed et al., 2014)) or another model/mechanism (e.g., directed graphical
models, conditional random fields, or CNNs) trained on an additional set of clean data, which,
however, is not always available. Self-training and unsupervised learning techniques (Rasmus et al.,
2015; Berthelot et al., 2019) have also been employed to generate pseudo labels to replace noisy
labels (Arazo et al., 2019). The pseudo labels are optimized together with the model or generated
by the model with data augmentations to encourage the output consistency on the same sample’s
augmentations. Unfortunately, the pseudo labels’ quality may vary across different samples and
significantly degenerate when the noise ratio is high, or the model fails to produce stable and correct
predictions. In such a case, the relabeling error on some samples can be accumulated during training.
In this paper, we address the aforementioned problems of noise-label learning by developing a
curriculum learning strategy called Robust Curriculum Learning (RoCL) that smoothly transitions
between two phases: (1) detection and supervised training on clean data; and (2) relabeling and
self-supervision on noisy data. Specifically, we train the model for multiple episodes, each starting
from phase(1) and gradually moving to phase(2). Unlike existing approaches, we only select samples
with accurate given/pseudo labels that are most informative to the current model training. Our
data selection criterion takes both the dynamics of per-sample loss and output consistency (across
multiple data augmentations) into account. Using an exponential moving average of the loss and
consistency over training history, it overcomes the instability of instantaneous losses and does
not incur any additional inference cost. In addition, by adjusting a temperature parameter, the
criterion can interpolate between the two phases and keep the training focusing on the data that
the model mostly needs to improve on, e.g., clean data with unsatisfying output consistency or
wrongly-labeled data with accurate pseudo labels. Thus, we can fully exploit both clean and noisy
data more efficiently with less risk of introducing extra noise or error accumulation. We further show
that our data selection can be derived from a novel optimization formulation for robust curriculum
learning. We evaluate our method on multiple noisy learning benchmarks and show that our method
outperforms a diverse set of recent noisy-label learning approaches.
1.1	Related Work
Early curriculum learning (CL) (Khan et al., 2011; Basu & Christensen, 2013; Spitkovsky et al., 2009;
Zhou et al., 2021) seeks an optimized sequence of training samples (i.e., a curriculum, which can be
designed by human experts) to improve model performance. Self-paced learning (SPL)(Kumar et al.,
2010; Tang et al., 2012a; Supancic III & Ramanan, 2013; Tang et al., 2012b) selects easy samples
with smaller losses. It starts with selecting a few samples of small loss and gradually increases
the selection size to cover all the training data. Self-paced curriculum learning (Jiang et al., 2015)
combines the human expert in CL and loss-adaptation in SPL. SPL with diversity (SPLD) (Jiang
et al., 2014) applies a negative group sparse regularization to SPL to promote the diversity of selected
samples. Minimax curriculum learning Zhou & Bilmes (2018) promotes the diversity of samples
during early learning to encourage exploration and focus on hard samples in later stages.
In the context of robust learning with noisy labels, label correction methods aim to identify the wrong
labels and possibly correct them to get more consistent labels for training. Previous work often apply
an extra noise model (directed graphical model (Xiao et al., 2015), conditional random fields (Vahdat,
2017), neural network (Lee et al., 2018; Veit et al., 2017), knowledge graph (Li et al., 2017b)) to cor-
rect the noisy labels, which often require extra clean data and as well as training/inference of the noise
model. Another line of research focuses on loss correction, which modifies the loss or prediction prob-
abilities during training to correct the misinformation from the noisy labels. Patrini et al. (2017) uses
two noise transition (backward and forward) matrices to correct the prediction probabilities. Label
Smoothing Regularization (Szegedy et al., 2016; Pereyra et al., 2017) alleviates the overfitting to noisy
labels by using soft labels instead of one-hot labels. Reed et al. (2014) augments the loss with a notion
2
Published as a conference paper at ICLR 2021
of perceptual consistency. Jiang et al. (2018) trains a mentor network to reweigh samples duri‘ng the
training of a student network. Guo et al. (2018) designs a curriculum by ranking the complexity of
data using its distribution density in a feature space. Ren et al. (2018) proposes a meta-learning algo-
rithm that learns to assign weights to samples based on their gradients in training compared to those
of validation data, which requires extra clean data. Co-teaching (Han et al., 2018) feeds in the network
with the most confident samples of another network to reduce confirmation bias. Amid et al. (2019)
generalizes the logistic loss and the exponents in the softmax by applying a temperature to each of
them and makes the training more robust to noise. Hu et al. (2019) trains a network on noisy labels in
the weakly supervised setting and uses it as a regularization term to improve the training on clean data.
Some approaches focus on designing loss functions that have robust behaviors and provable tolerance
to label noise. Ghosh et al. (2017) theoretically proves that the Mean Absolute Error(MAE) is a
robust loss. The Generalized Cross Entropy (Zhang & Sabuncu, 2018) uses a negative Box-Cox
transformation to obtain a loss function that generalizes MAE and Cross Entropy loss. Wang et al.
(2019) proposes a Symmetric Cross Entropy that combines Cross Entropy loss and Reverse Cross
Entropy loss. Ma et al. (2020) proposes a loss normalization method and shows that any loss can
be made robust to noisy labels.
RoCL shares similar ideas with some CL methods in that RoCL starts with learning easy and clean
samples and gradually moves to hard and noisy ones. RoCL is more related to the loss correction
approach in noisy-label learning literature as RoCL generates a curriculum dynamically assigning
weight (probability) to each sample. RoCL differs from existing methods in: (1) it only selects a
subset of informative and reliable labels for training in each epoch; (2) it is a smooth transition not
only from clean data to noisy data but also from supervised learning to self-supervision; (3) it runs
multiple episodes of the curriculum to avoid getting in a local minimum dominated by a small set
of clean/noisy data or a specific type of loss; (4) it does not assume the availability of an extra set
of clean data; (5) it does not require extra computation or any modification to the model.
2	Dynamic Patterns of Clean/Noisy Labels in Training
2.1 Loss Dynamics and Clean Label Detection
A key challenge for most noise-label
learning methods is to design a reliable
criterion to select/reweigh clean data and
distinguish them from the noisy data, so
all the clean data can be fully exploited
while most noisy labels are filtered out of
the training process. Loss computed at
an instantaneous step have been widely
used for this purpose according to the
observation that the loss on clean data
is usually smaller than noisy data. One
important reason is that the clean labels
are mutually consistent with each other in
producing gradient updates, and therefore,
the model can fit them better and faster.
On the other hand, the noisy labels may
contain mutually inconsistent information,
creating a form of long-lasting “tug of war”
amongst themselves. For example, it can
be hard for the model to find consistent vi-
sual patterns from images with noisy labels
to make the desired predictions. However,
instantaneous loss suffers from high vari-
ance across training epochs (as shown in
the first plot of Figure 1) and is inaccurate
for clean data detection under high noise
ratio (i.e., the proportion of wrong labels is
high) and the randomness of DNN training,
instantaneous consistency or data wltħ ∞rrect pseudo labels
Instantaneous consistent of data with wrong pseudo labels
0.62	——EMA consistency of data with ∞rrert pseudo labels
0.60	EMA consistent of data with wrong pseudo labels
Figure 1: Dynamic patterns (mean±std) of instantaneous
metrics (top) and exponential moving average (EMA) metrics
(bottom) when applying Alg. 2 that alternates between super-
vised learning on given labels and self-supervision on pseudo
labels. Larger gap between curves in each plot is better.
symmetric noise is defined in the beginning of section 4. We
use cross entropy for supervised loss '(∙, ∙) in Eq. (1) and 0-1
loss for '(∙, ∙) in consistency loss Eq. (2) with m = 7.

e.g., random initialization, random data augmentation, etc. Moreover, it needs to evaluate the
instantaneous loss for all samples in each step, resulting in extra inference cost on unselected samples.
3
Published as a conference paper at ICLR 2021
The dynamic patterns of losses (Zhou et al., 2020b) over the course of training give us a new insight
for better clean data detection even when the noise ratio (proportion of wrong labels) is high. In
particular, we hypothesize that a sample’s label is more likely to be correct if its losses persistently
retain low values over training steps. Given a sample (xi, yi) with xi being the features and yi being
the label, we describe its loss dynamics using a simple exponential moving average (EMA) of the
instantaneous loss `(f (xi; θt), yi) (where f(xi ; θt) denotes the model output and θt is the model
parameters at step t * * along the training history, which is defined and computed recursively as
lt+1(i)
γ × `(f (xi; θt), yi) + (1 - γ) × lt(i)
lt(i)
if i ∈ St
else ,
(1)
Where γ ∈ [0, 1] is a discounting factor, V is the set of all n training samples, and St ⊆ V is the
set of samples selected (by a certain curriculum) for training at epoch t. We only update the EMA
loss for selected samples using the byproduct lt(i) of training without requiring extra inference.
In the first and the third plots of Figure 1, we show how the losses and EMA losses associated with
clean/noisy data change throughout the training process. Specifically, we train a ResNet34 (He
et al., 2016) model on CIFAR10 with 60% of the original labels randomly changed to a wrong class.
To avoid quick overfitting to the noise, we train the model for multiple episodes (each composed
of several epochs over all data with a cosine annealing learning rate) and alternate between the
supervised learning episode that minimizes the cross-entropy loss against the given noisy labels and
the self-supervision episode that minimizes the consistency loss Eq.(2) against the pseudo labels.
Comparing the shaded areas (std) of instantaneous loss and EMA loss and the gap between curves in
the two plots, we see that EMA leads to smaller variance within each group and larger gap between
the clean and noisy groups, demonstrating the effectiveness of EMA loss for clean data detection.
2.2 Consistency Dynamics and Pseudo Label Selection
Simply removing noisy data (x, y) with wrong label y discards important information about the data
distribution p(x) (Arazo et al., 2019). Including all the noisy data for self-supervision, bootstrapping
or relabeling can also harm the training since the pseudo labels’ quality is not equal across samples
and much depends on the model generating them, where the model’s predictions may contain errors
that can accumulate if adopted for training. Hence, a careful selection of noisy data is necessary.
However, without access to a purely clean dataset or a reliable pre-trained model, it is nontrivial
to evaluate pseudo labels’ correctness. By analyzing the training dynamics of model outputs in the
above experiment, we discover that the model output for a sample tends to be an accurate pseudo label
if the output remains consistent over training steps and across different augmentations of the sample.
We first define the instantaneous consistency loss of a sample xi at step t as the discrepancy of the
model output f(xi; θ) between step t and t - 1 on xi and its m data augmentations {xi(j)}jm=1, where
the discrepancy can be measured by any loss function '(∙, ∙). However, the discrepancy can be small
if the models of epoch t and t - 1 are too similar and make the same errors. Therefore, we use
an exponential moving average of the model parameters (according to mean teacher (Tarvainen &
Valpola, 2017)) and compute the prediction at step t-1 by averaging over multiple data augmentations
(according to MixMatch (Berthelot et al., 2019)): ft(xi)，1/mPm=I f (χj;θt), θt，γθt-ι +
(1 - Y)θt-ι. Computing pseudo labels on augmented data and a time averaging ensemble of models
is commonly-adopted for semi-supervised learning (Sajjadi et al., 2016; Laine & Aila, 2016; Zhou
et al., 2020a). The instantaneous consistency loss* Zt(i) of sample Xi at step t is then defined as
m
Zt(i) , mm X'(f(Xi θt),ft(Xj))),	⑵
m j=1
Which can also be minimized as a consistency loss for Xi in self-supervised learning when no label is
given. Similar to the EMA loss in Eq. (1), we define the EMA consistency loss over training history
ct+1(i)
γ × ζt(i) + (1 - γ) × ct(i)
ct(i)
if i ∈ St
else.
(3)
Note we use the same γ value as in Eq. (1). The EMA consistency loss ct (i) measures both the
time-consistency (Zhou et al., 2020a) over multiple training steps and the spatial consistency over
different augmentations of sample Xi. If the output prediction is wrong and contradicts other samples’
*Each step does not strictly refer to a mini-batch: when the subset St for step t is larger than the mini-batch
size (which is common in practice), we run SGD steps over all mini-batches of the subset for one pass.
*This name maybe misleading since it actually measures the “inconsistency”, but it is commonly called
“consistency” in previous works (Zhu et al., 2017; Berthelot et al., 2019), so we decide to use the same name.
4
Published as a conference paper at ICLR 2021
labels, it will be inconsistent over time and across augmentations since it can easily change or flip
after the next training step. In the last plot of Figure 1, we report the mean and standard deviation
(the middle line and the shaded area) of the EMA consistency loss for two groups of data at each
epoch, i.e., the ones with correct pseudo labels and the ones with incorrect pseudo labels. Comparing
to the instantaneous consistency loss in the second plot, EMA consistency loss is a more reliable
criterion for allocating correct pseudo labels. Thus, we can safely learn the noisy data by using their
pseudo labels as training targets and avoid introducing harmful noises.
3	Robust Curriculum Learning
In this section, we first introduce the selection criterion for both the clean label detection and pseudo
label selection. By adjusting two temperature parameters τ1 and τ2, it can smoothly interpolate
between the two criteria and control their trade-off. We then show that the criterion is derived from a
novel optimization formulation for robust curriculum learning. We finally present the RoCL algorithm.
3.1	Data Selection Criterion
To combine of clean label detection and pseudo label selection, we use the two criteria from the
previous sections and apply a temperature parameter to control the preference for small/large loss
or consistency loss and their trade-off. We sample xi at training step t with probability:
Pt(i) = λ × pt(i) + (1 - λ) × qt(i),	(4)
where pt (i) and qt (i) are defined as softmax probabilities, i.e.,
pt(i)
exp[τιlt(i)]	,	expgct(i)]
j exp[τιltj)], qt(i) = Pn=ι exp[τ2ct(j)],
(5)
and λ ∈ [0, 1] controls trade-off between them. Here, pt(i) is a softmax probability computed from
EMA losses for clean data detection: samples with smaller (larger) EMA losses and thus clean
(noisy) labels tend to have high probability pt(i) when τ1 is negative (positive). Similarly, qt(i) is
computed from EMA consistency loss for pseudo label selection: samples with smaller (larger) EMA
consistency loss and thus correct (wrong) pseudo labels tend to have high probability qt (i) when
τ2 is negative (positive). When τ1 , τ2 = 0, the probabilities are uniform, and when τ1 , τ2 → +∞/
-∞, the probabilities approximate the max (min) operator.
Either pt(i) or qt (i) can be independently employed to select or reweigh samples for noise-label
learning. However, selecting samples with high probabilities pt(i) when τ1 0 tends to make the
training focus on clean data that the model has already learned, which carries limited new information
and prevents exploration. Similarly, selecting samples with high probabilities qt (i) when τ2 0 is
not informative since the model outputs are consistently correct for those data, and little progress can
be made.We can encourage exploration by manipulating the temperature parameters. By setting τ1
and τ2 close to zero, we move towards uniform exploration of all data. A more effective strategy is to
couple the values of τ1 and τ2 . For example, a negative τ1 and a positive τ2 strengthen the preference
for clean data that have not been fully exploited and learned by the model. Alternatively, a positive τ1
with a negative τ2 emphasizes the noisy data with correct pseudo labels, so relabeling them provides
new information in addition to the clean data.
We apply the data selection criterion of Eq. (4) in each step and gradually change its parameters
(τ1, τ2 , λ) over the course of training according to the properties discussed above. We start from
a negative τ1 associated with a positive τ2 and a large λ (pt(i) dominates), then gradually increase
τ1 while decrease τ2 and λ, and end with a positive τ1 , a negative τ2 and a small λ. In this way, we
get a curriculum with a smooth transition between supervised learning of clean data using correct
given labels and self-supervision of noisy data using reliable pseudo labels (i.e., minimizing Eq. (2).
Moreover, the coupling strategy on τ1 and τ2 encourages selecting informative samples that the
model mostly needs to improve on, i.e., clean data with inconsistent model outputs or noisy data
with correct pseudo labels. In our experiments, we can further reduce the hyperparameters: (1) given
the sequence for τι in the curriculum as 丁上T, We can reverse it as the sequence for τ2, i.e., ττ：i；
(2) we can make λ monotone increase with τ1, e.g., setting the initial value λ1 = aτ1 + b and ending
value λτ = aττ + b, solving this linear system for a and b, which generate λ±T = aτ±T + b.
3.2	Robust Curriculum Learning as an Optimization
The data selection criterion is derived from an optimization formulation of our robust curricu-
lum learning (RoCL), in which we aim to minimize a combination of supervised loss and
5
Published as a conference paper at ICLR 2021
consistency(self-supervised) loss in the following form.
λ 1n	1 λ 1n
min F (θ)，— log ( n 2exp[τι'(f(xi; θ),yi)]J + log (n ^expgZ ⑺]),(6)
where the consistency loss ζ(i) is defined in Eq. (2) (with t removed). For the first term of Eq. (6),
unlike the conventional empirical risk minimization (ERM) with arithmetic average loss, i.e.,
1/n Pn=ι '(f (xi； θ), yi), We use LogSUmExP loss with an additional temperature parameter (i.e.,
T11- log (1 Pn=ι exp[τι'(f (xi； θ), yi)])), so it can approximate both the min-loss (when τι → 一∞),
max-loss (when τ1 → +∞), and any interpolation between them, where the min-loss focuses on
the easiest samples with the smallest losses and the max-loss focuses on the hardest ones. Note it
reduces to the arithmetic average loss when τ → 0. It is called “tilted loss” in a recent work (Li
et al., 2020), which shows several intriguing properties in different learning settings. The second
term of Eq. (6) focuses on the consistency loss ζ(i). We use λ to control the trade-off between the
supervised loss and the consistency loss. By simple algebra, the gradient of F(θ) at step t is
nn
VθF(θt) = λ Xpt(i)Vθ'(f (Xi； θt), y) + (1 - λ) X q0⑺VθZt(i)
i=1	i=1
=X P0(i) [λp⅛(⅞Vθ'(f(Xi θt),yi)+ (1 -λ)q0(i) VθZt(i)l = Ei〜p0(i)Gt(i), (7)
i=1 t	Pt0(i)	Pt0(i)	t
where
Gt(i)，λp⅛(⅛Vθ'(f (xi； θt), yi) + (I -))q0⑶ VθZt(i).	(8)
Pt (i)	Pt (i)
Here, p0t(i) and qt0 (i) are similar to pt(i) and qt(i) defined in Eq. (5) except that the EMA loss and
EMA consistency loss are replaced by their instantaneous counterparts respectively, i.e., p0t(i) ，
Pn=Xp甯f'f jθ⅞,)], q0(i)，Pn3⅞¾)t]j)]. Similarly, We also denote P0(i) = λ × P,0⑶ +
(1 - λ) × qt0(i). Note in Section 2, we already discussed that the EMA metrics are better alternatives
to the instantaneous metrics when used for data selection in noisy-label learning. In our experiments,
we use the EMA metrics {pt(i), qt(i), Pt(i)} instead of the instantaneous ones {p0t(i), qt0 (i), Pt0(i)}.
For every training step, an unbiased estimator of the gradient in Eq. (7) can be achieved by drawing
a subset of samples St according to Pt(i) and averaging their gradients Gt(i) in Eq. (8).
3.3	Robust Curriculum Learning Algorithm
We describe our curriculum learning
algorithm RoCL in Alg. 1 based on
the data selection criterion in Sec-
Algorithm 1 Robust Curriculum Learning (RoCL)
1:
tion 3.1 and the optimization formu-
lation in Section 3.2. We denote the 2:
update of θt produced by the adopted 3:
optimizer as h(VθF(θt), η), where η 4:
contains all hyperparameters of the op- 5:
timizer at step t, e.g., the learning rate. 6:
We denote `t(i) as a shorthand nota- 7:
tion for '(f (xi； θt-ι), yi). We apply a	8
warm starting episode of a few epochs 9:
(e.g., 5-10) over all the data and given 10:
labels with label smoothing to obtain 11:
stable EMA metrics. After that, we
apply multiple episodes of curriculum
learning, each including a sequence of
steps following the curriculum at the
end of Section 3.1 for data selection
per step (Line 6-14). We repeat the
transition between clean data learning
to noisy data learning for K episodes
to avoid getting trapped in a local
12:
13:
14:
15:
16:
17:	end for
input： {(xi,yi)}i=ι, h(∙ η), '(∙, ∙), f (∙； θ), To：K; τι <
0, τT > 0; λ1, λT ∈ [0, 1]; γ, γb ∈ [0, 1]
initialize: θ0, b0 ∈ (0, n), l0 (i) = c0(i) = 0 ∀i ∈ [n]
for k ∈ {0,…，K} do
Schedule ti：Tk and )上Tk by Eq. (9)-(10);
for t0 ∈ {1, ∙∙∙ ,Tk} do
t J t0 + Tk-I;
if k = 0 then
St J [n];
θt J θt-ι + h (vθ⅛ Pn=o't⑴;办
else
Draw a subset St ⊆ [n] of bk samples according
to probability Pt in Eq. (4) with τ1 = τt0 , τ2 =
τTk -t0 , λ = λt0 ;
θt J θt-ι + h (bk Pi∈St Gt⑴;η) (Eq.⑻)；
end if
Update lt+1(i) and ct+1(i) by Eq. (1) and Eq. (3);
end for
bk+1 J (1 + γb ) × bk ;
minimum dominated by a small set of clean/noisy data or a specific type of loss. It also reinforces
6
Published as a conference paper at ICLR 2021
TT - Tm
Tt = --------×
σ
the memorization of clean labels and correct pseudo labels learned in previous episodes. Moreover,
under the coupling strategy of τ1 and τ2, each episode is encouraged to explore the clean/noisy data
that the previous episode fails to learn. Considering the undertrained model (producing inaccurate
pseudo labels) and the relatively high variance of the EMA metrics at the earlier episodes, we start
from a small budget for the selected subset size and gradually increase in later episodes.
To generate the whole schedule of 丁上T in each episode, We can apply any monotone interpolation
between τ1 and τT whose values are predefined. Let g : R 7→ [-σ, σ] be an invertible monotone
continuous function. We define the interpolation between τ1 and τT as follows, ∀t ∈ [T],
g(σt) - g(	σ)2+g(σ)	+τm,	σt	= g-1 (-σ) + 2tg-1 (σ),	Tm	=	T1	+ TT	.⑼
Note the g(σt) produces interpolation values between [-σ, σ] that correspond to T evenly spaced
input σt ∈ [g-1(-σ), g-1(σ)]. In our curriculum for each episode, we need to keep a high quality of
the selected clean (pseudo) labels in earlier (later) stages and make the exploration stages in between
shorter since their selected labels contain more noise. Therefore, we choose “s”-shaped functions
such as tanh or the logistic function for the interpolation. In this paper, we use g(∙) = tanh(∙) and
pick σ = 0.95. We illustrate Eq. (9) and visualize our choice of g(∙) and the resulting Tt in Figure 6
(Appendix). The corresponding schedule for λ can then be defined as an affine transformation of 丁上T:
λ -λ
∀t ∈ [T], λt = aλ(τt - TI) + λ1, aλ =--------.	(IO)
TT - T1
4 Experiments
We evaluate RoCL with other approaches for noisy-
label learning on three widely used benchmarks, i.e.,
CIFAR10/100 with two types of synthetic noises (i.e.,
symmetric and asymmetric), and mini-WebVision (Li
et al., 2017a) (the first 50 classes) containing unknown
noises from web labels. Symmetric noise flips each
label randomly to an incorrect class with probabil-
ity ρ (i.e., noise rate), and our experiments cover
ρ = {0.4, 0.6, 0.8}. Asymmetric noise flips the labels
within a specific set of classes. For CIFAR10, flipping
TRUCK→AUTOMOBILE, BIRD→AIRPLANE,
DEER→HORSE, CAT→DOG. In CIFAR100, the 100
classes are grouped into 20 super-classes with each has
5 sub-classes, we then flip each class within the same
super-class to the next in a circular fashion with prob-
ability ρ. Our experiments cover ρ = {0.2, 0.3, 0.4}.
Table 1: Accuracy (%) evaluated on WebVision
and ILSVRC2012 validation sets for DNNs
trained by noisy-label learning methods on
mini-WebVision training set (first 50 classes),
which contains real-world web-label noises.
Val. set	WebVision	ILSVRC2012
Accuracy	Top-1 Top-5 I Top-1 Top-5	
F-correct +?	61.12 82.68	57.36 82.36
Decoupling *?	62.54 84.74	58.26 82.26
Co-teaching *	63.58 85.20	61.48 84.70
MentorNet *?	63.00 81.40	57.80 79.92
MentorMix 埒？	76.00 90.20	72.90 91.10
D2L ?	62.68 84.00	57.80 81.36
INCV ?	65.24 85.34	61.60 84.98
RoCL (ours)力气	80.04 92.68	75.81 92.28
Practical Modifications In the experiments, we follow previous work and apply the techniques
below. We will present an ablation study of their effectiveness in Table 5.
•	We apply the class-balance regularization used in (Tanaka et al., 2018), which prevents the
model from predicting the same class for all the samples within a mini-batch B . We add
(1∕b) Pi∈B '(f (xi； θ),1/C ∙ 1) (where C is the number of classes) to the objective for a mini-
batch B with regularization weight of 1.
•	We apply label smoothing whose effectiveness in noisy-label learning has been studied in (Lukasik
et al., 2020). We modify each one-hot label yi to be yi — (1 - Oyyi + α∕c (e.g., we use α = 0.5).
•	We apply Mix-Up (Zhang et al., 2018) to all the selected data. However, Mix-Up of two (soft)
pseudo labels can significantly increase the entropy of the mixed label if both pseudo labels are
under-confident. Hence, we apply a curriculum to the beta distribution’s parameter α of Mix-Up
and gradually reduce it (e.g., from 8.0 to 0.2 in our experiments) within each episode.
Hyperparameter Setting We apply RoCL to train ResNet34 on CIFAR10/100 and ResNet50 on
WebVision, which are the most widely used models in other baseline papers. We apply SGD with
momentum of 0.9, weight decay of 10-4 and cosine annealing learning rate in each training episode.
The initial learning rate is set to 0.1 for CIFAR10/100 and 1.0 for WebVision. In all RoCL experi-
ments, we apply T0 = 10 warm starting epochs followed by K = 10 episodes of curriculum learning,
7
Published as a conference paper at ICLR 2021
whose lengths start from T1 = 10 and increase by 10 for every episode afterwards. We initialize the
subset size b0 = 0.2n and set γ = γb = 0.1, which are common choices for discounting/augmenting
factors. We use Cubuk et al. (2020) for data augmentations. We did not heavily tune λ1 , λT and
τ1 , τT and followed a principle that the resulting curriculum should have a transition from supervised
learning on clean data to self-supervised learning on noisy data with correct pseudo labels.
•	For λ, we start from λ1 close to 1 and end with λT close to 0 because our curriculum is a transition
from supervised learning (λ = 1) to self-supervised learning (λ = 0).
•	As explained in Section 3.1, our curriculum requires τ1 forpt(i) changing from negative to positive
values and an inverse sequence for τ2 in qt (i) to gain the above transition and encourage learning
on more informative samples. Hence, we set the starting value τ1 to be negative and τT to be
positive for the sequence 丁上T.
•	We set their exact values based on observations in Figure 1: clean data detection is easier but the
detection of correct pseudo-labels is harder. So we can be more confident on the former than the
latter and set the starting τ1 larger than τT in magnitude. For the same reason, we set the starting
value λ1 to be closer to 1 than the ending value λT to 0. In experiments, we tried τ1 = {-4, -3}
and τT = {1, 2} and finally chose τ1 = -4 and τT = 1 since this choice performs consistently
well on all experiments, though it might not be the best choice for all; we set λ1 = 0.9 and did not
try other values; we tried λT = {0.1, 0.2, 0.3} and on some experiments the first two choices lead
to slightly worse performance, so we chose λT = 0.3.
We compare RoCL to the following baselines: F-
correct (Patrini et al., 2017), Decoupling (Malach &
Shalev-Shwartz, 2017), Co-teaching (Han et al., 2018),
D2L (Ma et al., 2018), INCV (Chen et al., 2019), MD-
DYR-SH (Arazo et al., 2019), MentorNet (Jiang et al.,
2018), MentorMix (Jiang et al., 2020), O2U-net (Huang
et al., 2019), RoG+D2L (Lee et al., 2019), PENCIL (Yi &
Wu, 2019), GCE (Zhang & Sabuncu, 2018), SCE (Wang
et al., 2019), NFL/NCE variants (Ma et al., 2020),
and Bootstrap (Reed et al., 2014). To better compare
and categorize different baseline methods, we use the
following symbols to denote the techniques used: + for
Table 2: Test accuracy (%) of RoCL
applied with different loss functions on
CIFAR10 corrupted by {60%, 80%} sym-
metric(uniform) noises (CE-cross entropy).
Noise Rate	60%	80%
CE	90.22 ± 0.24	77.47 ± 0.67
GCE	89.30 ± 0.68	79.84 ± 1.12
SCE	92.06 ± 0.23	74.25 ± 0.86
NFL+MAE	88.73 ± 0.47	85.76 ± 0.26
NFL+RCE	87.68 ± 0.35	80.09 ± 0.41
NCE+MAE	90.37 ± 0.43	82.16 ± 0.93
NCE+RCE	88.03 ± 0.39	80.33 ± 0.80
additional clean training data; * for training additional auxiliary models;去 for using mixup; ? for
using data augmentations; f for class-balance regularization; o for label-smoothing. We report the
results and comparisons to baselines in three tables: real-world noise in Table. 1, symmetric noise
in Table. 3 and asymmetric noise in Table. 4. RoCL achieves the best performance in every setting,
and for most of the cases, improves upon the existing methods by large margins. The closest rival
to RoCL is MentorMix, which utilizes MentorNet and Mix-Up to assign weights to each sample.
We note that MentorMix requires training of an extra mentor network to generate the sample weights,
while RoCL is more flexible and only makes changes to the training process without modifying
the model. Table. 2 reports RoCL’s performance when applied with different loss functions on
Table 3: Test accuracy (%) of noisy-label learning methods on CIFAR10/100 corrupted by symmetric(uniform)
label noises of different levels. All the baselines’ results are from the original papers or the following-up works.
There are two formats of these reported results: “mean±variance” of 5 trials and single-trial accuracy.
Dataset		CIFAR10		CIFAR100		
Noise Rate	40%	60%	80%	40%	60%	80%
MD-DYR-SH *?料	92.3	86.1	74.1	70.1	59.5	39.5
MentorNet *?	91.2	74.2	60.0	68.5	61.2	35.5
MentorMix 埒？	94.2	91.3	81.0	71.3	64.6	41.2
O2U-net ?	90.3	-	43.4	69.2	-	39.4
RoG+D2L *?	87.0	78.0	-	64.9	40.6	-
PENCIL ?	-	-	-	69.12 ± 0.62	57.79 ± 3.86	fail
GCE ?	87.62 ± 0.26	82.70 ± 0.23	67.92 ± 0.60	62.64 ± 0.33	54.04 ± 0.56	29.60 ± 0.51
SCE ?	85.34 ± 0.07	80.07 ± 0.02	53.81 ± 0.27	53.69 ± 0.07	41.47 ± 0.04	15.00 ± 0.04
NFL+MAE ?	83.81 ± 0.06	76.36 ± 0.31	45.23 ± 0.52	58.18 ± 0.08	46.10 ± 0.50	24.78 ± 0.82
NCE+RCE ?	86.02 ± 0.09	79.78 ± 0.50	52.71 ± 1.90	59.48 ± 0.56	47.12 ± 0.62	25.80 ± 1.12
RoCL (OUrS尸？*o	94.55 ± 0.12 92.06 ± 0.23 85.76 ± 0.26			74.64 ± 0.43 66.79 ± 0.58 53.89 ± 0.62		
8
Published as a conference paper at ICLR 2021
Table 4: Test accuracy (%) of noisy-label learning methods on CIFAR10/100 corrupted by asymmetric(class- dependent) noises of 3 levels. All the baselines’ results are from the original papers or the following-up works.						
Dataset		CIFAR10		CIFAR100		
Noise Rate	20%	30%	40%	20%	30%	40%
PENCIL ?	92.43	91.84	91.01	74.70 ± 0.56	72.52 ± 0.38	63.61 ± 0.23
Bootstrap ?	86.57 ± 0.08	84.86 ± 0.05	79.76 ± 0.07	63.44 ± 0.35	63.18 ± 0.35	62.08 ± 0.22
F-correct +?	89.09 ± 0.47	86.79 ± 0.36	83.55 ± 0.58	42.46 ± 2.16	38.13 ± 2.97	34.44 ± 1.93
GCE ?	86.07 ± 0.31	80.78 ± 0.21	74.98 ± 0.32	59.99 ± 0.83	53.99 ± 0.29	41.49 ± 0.79
SCE ?	83.92 ± 0.07	79.70 ± 0.27	78.20 ± 0.03	58.22 ± 0.47	49.85 ± 0.91	42.19 ± 0.19
NFL+MAE ?	86.81 ± 0.32	83.91 ± 0.34	77.16 ± 0.10	63.10 ± 0.22	56.19 ± 0.61	43.51 ± 0.42
NCE+RCE ?	88.56 ± 0.17	85.58 ± 0.44	79.59 ± 0.40	62.68 ± 0.79	57.82 ± 0.41	46.79 ± 0.96
RoCL (ours) ɪMo	95.38 ± 0.21	94.19 ± 0.28 92.31 ± 0.35		80.03 ± 0.34 77.59 ± 0.45 73.28 ± 0.83		
CIFAR10 under high noise rates, i.e., 60% and 80%. We observe significant improvements over
their performance without using RoCL in Table. 3. It indicates that RoCL is compatible with any
loss function and can further enhance their performance.
Table 5: Ablation study: Test accuracy (%) of RoCL
variants with one part removed/changed when applied to
CIFAR10/100 corrupted by Symmetric(Uniform) label noise.
Dataset	CIFAR10 CIFAR100
To analyze the effect of each component
in RoCL, we conduct a thorough ablation
study of 10 variants of RoCL, each
removing/changing one component of
the original RoCL. In Table 5, we report
their test accuracies on CIFAR10/100 with
noise rates of {60%, 80%}. In Figure 7-10
in Appendix, we report how their test
accuracies change during the training to
study their learning efficiency and conver-
gence. Among them, “no ClassBalance”
removes the class-balance regularization;
“no RandAugment” replaces the strong
data augmentation RandAugment Cubuk
et al. (2020) with random crop and
random horizontal flip; “no RandSampling”
replaces the weighted sampling in Line
11 of Algorithm 1 by selecting the top-bk
samples with the largest Pt (i); “no EMA
metrics” replaces EMA loss and EMA
Noise Rate	60%	80%	60%	80%
RoCL: no MixUp	92.98	88.18	69.72	58.72
RoCL: no LabelSmooth	91.94	85.05	62.92	42.95
RoCL: no ClassBalance	93.08	74.91	62.66	43.94
RoCL: no RandAugment	86.59	72.35	64.84	44.06
RoCL: no RandSampling	92.31	85.99	64.09	57.00
RoCL: no EMA metrics	92.84	87.79	65.99	53.10
RoCL: pt (i) = 1/n	92.42	86.05	62.69	44.35
RoCL: qt (i) = 1/n	92.59	86.93	64.71	50.79
RoCL: pt (i) = qt (i) = 1/n	92.07	85.77	64.18	47.88
RoCLBase : no curriculum	87.83	66.93	61.84	41.92
RoCL: original version	92.82	88.00	66.79	54.22
MentorMix: +RandAugment	85.45	20.68	52.70	8.02
MentorMix: +RandAugment-MixUp	84.31	38.21	58.31	8.18
MentorMix: original version	91.30	81.00	64.60	41.20
consistency loss with their instantaneous counterparts; “pt(i) = 1/n” samples the clean data using
uniform probabilities; “qt(i) = 1/n” samples the correct pseudo-labels using uniform probabilities;
“pt(i) = qt(i) = 1/n” uses uniform probabilities for both. Note for the final three variants, we still
have the curriculum of λ. We keep the same hyperparameter settings as the original RoCL. We
give brief conclusions here and leave a detailed analysis to Appendix: (1) Except RoCLBase in
Algorithm 2, “no RandAugment” and “no ClassBalance”, most variants perform similarly as the
original RoCL and outperform the previous SoTA achieved by MentorMix. The removed components
are more important under higher noise rates. (2) RoCLBase removes our proposed curriculum and
preserves all other techniques but shows significant degradation on accuracies, indicating that the
curriculum is essential to RoCL’s appealing performance. (3) A strong data augmentation is critical
to effective self-supervision and accurate EMA consistency loss estimation in RoCL, while a weak
one may lead to error accumulation. However, applying RandAugment in MentorMix degrades its
original performance. (4) Class-balance regularization is only important under very high noise rates.
(5) Removing Mix-Up can improve RoCL’s performance since it damages information when mixing
soft pseudo labels. (6) Compared to other variants, “no RandSampling” or “no EMA metrics” causes
less degeneration on the final accuracies but can slow down the convergence and learning speed in the
early stages when exploration is insufficient. (7) Changingpt(i), qt(i) or both to uniform probabilities
reduces the final accuracies in all cases and significantly slows down the learning process.
5 Conclusion
We propose a novel curriculum learning method RoCL for robust learning under label noises. RoCL
features a smooth transition from learning with clean data to noisy data, and from learning with
supervised loss to self-supervised loss. Based on observations of training dynamics, RoCL can select
samples with reliable labels/pseudo labels and most informative to training. RoCL does not require
availability of extra clean data or training of extra auxiliary models. On multiple benchmarks of
noisy label learning, RoCL significantly improves upon existing baselines.
9
Published as a conference paper at ICLR 2021
Acknowledgments
This research is based upon work supported by the National Science Foundation under Grant No. IIS-
1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft,
and an Intel research award. It is also supported by the CONIX Research Center, one of six centers in
JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. Some GPUs
used to produce the experimental results are donated by NVIDIA. We would like to thank ICLR
area chairs and anonymous reviewers for their efforts in reviewing this paper and their constructive
comments! We also thank all the MELODI lab members for their helpful discussions and feedback.
References
Ehsan Amid, Manfred KK Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic
loss based on bregman divergences. In Advances in Neural Information Processing Systems, pp.
15013-15022, 2019.
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, MaXinder S.
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien.
A closer look at memorization in deep netWorks. In International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 233-242, 2017.
Sumit Basu and Janara Christensen. Teaching classification boundaries to humans. In AAAI, pp.
109-115, 2013.
David Berthelot, Nicholas Carlini, Ian GoodfelloW, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. MiXmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems 32 (NeurIPS), pp. 5050-5060. 2019.
Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
deep neural netWorks trained With noisy labels. arXiv preprint arXiv:1905.05040, 2019.
Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated
data augmentation With a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural netWorks. arXiv preprint arXiv:1712.09482, 2017.
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, MattheW R Scott, and
Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale Web images. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 135-150, 2018.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Mengying Hu, Hu Han, Shiguang Shan, and Xilin Chen. Weakly supervised image classification
through noise regularization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 11517-11525, 2019.
Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao. O2u-net: A simple noisy label detection
approach for deep neural netWorks. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 3326-3334, 2019.
Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and AleXander G. Hauptmann.
Self-paced learning With diversity. In NeurIPS, pp. 2078-2086, 2014.
10
Published as a conference paper at ICLR 2021
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G. Hauptmann. Self-paced
curriculum learning. In AAAI,pp. 2694-2700, 2015.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning, pp. 2304-2313, 2018.
Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on
controlled noisy labels. ICML, 2020.
Faisal Khan, Xiaojin (Jerry) Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning
and teaching dimension. In NeurIPS, pp. 1449-1457, 2011.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In NeurIPS, pp. 1189-1197, 2010.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
generative classifiers for handling noisy labels. arXiv preprint arXiv:1901.11300, 2019.
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable
image classifier training with label noise. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5447-5456, 2018.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization.
arXiv preprint arXiv:2007.01162, 2020.
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual
learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017a.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1910-1918, 2017b.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. arXiv preprint
arXiv:1806.02612, 2018.
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-
malized loss functions for deep learning with noisy labels. arXiv preprint arXiv:2006.13554,
2020.
Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In
Advances in Neural Information Processing Systems, pp. 960-970, 2017.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Eukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548,
2017.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised
learning with ladder networks. In Advances in Neural Information Processing Systems 28, pp.
3546-3554. 2015.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
11
Published as a conference paper at ICLR 2021
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. Advances in neural information
processing Systems, 29:1163-1171, 2016.
Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste. Data parameters: A new family of parameters
for learning a differentiable curriculum. In Advances in Neural Information Processing Systems,
pp. 11095-11105, 2019.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby Steps: How “Less is More” in unsu-
pervised dependency parsing. In NeurIPS 2009 Workshop on Grammar Induction, Representation
of Language and Language Learning, 2009.
James Steven Supancic III and Deva Ramanan. Self-paced learning for long-term tracking. In CVPR,
pp. 2379-2386, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework
for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5552-5560, 2018.
Kevin Tang, Vignesh Ramanathan, Li Fei-fei, and Daphne Koller. Shifting weights: Adapting object
detectors from image to video. In NeurIPS, pp. 638-646, 2012a.
Ye Tang, Yu-Bin Yang, and Yang Gao. Self-paced dictionary learning for image classification. In
MM, pp. 833-836, 2012b.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. In Advances in Neural Information
Processing Systems 30 (NeurIPS), pp. 1195-1204. 2017.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In Advances in Neural Information Processing Systems, pp. 5596-5605, 2017.
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 839-847, 2017.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 322-330, 2019.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2691-2699, 2015.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017-7025,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018.
12
Published as a conference paper at ICLR 2021
Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable
difficulties and scheduled diversity. In ICLR, 2018.
Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Time-consistent self-supervision for semi-supervised
learning. In International Conference on Machine Learning (ICML), 2020a.
Tianyi Zhou, Shengjie Wang, and Jeff A. Bilmes. Curriculum learning by dynamic instance hardness.
In NeurIPS, 2020b.
Tianyi Zhou, Shengjie Wang, and Jeff A. Bilmes. Curriculum learning by optimizing learning
dynamics. In AISTATS, 2021.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International
Conference on, 2017.
13
Published as a conference paper at ICLR 2021
A	Appendix
A.1 RoCLBase (no curriculum) in Section 2 and Figure 2-10
Algorithm 2 RoCLBase (no curriculum)
1:	input: {(χi,yi)}n=ι, h(∙; η), '(∙, ∙),f(∙; θ), To：K； Y ∈ [0,1]
2:	initialize: θ0, l0(i) = c0 (i) = 0 ∀i ∈ [n], T-1 = 0
3:	for k ∈ {0,…，K} do
4:	fort0 ∈ {1,…，Tk} do
5： t - t0 + Tk-1；
6：	St - [n]；
7:	if k%2 = 0 then
8：	θt J θt-ι + h (Vθ1 pn=o 't(i); η); {supervised learning using given labels}
9:	Update lt+1(i) by Eq. (1)； {update EMA loss}
10:	else
11:	θt J θt-ι + h (Vθ, pn=o ζt(i); η); {self-supervised learning using pseudo labels}
12:	Update ct+1(i) by Eq. (3)； {update EMA consistency loss}
13:	end if
14:	end for
15:	end for
Note the EMA metrics in line 9 and line 12 are not used for training in RoCLBase . They have been
updated and recorded for the purpose of empirical study presented in Section 2.
A.2 Additional Experiments
Figure 2: RoCL (Algorithm 1) vs. RoCLBase without any curriculum (Algorithm 2 in Appendix) during the
training of ResNet34 on CIFAR10 containing 60% symmetric noises on labels.
Figure 3: RoCL (Algorithm 1) vs. RoCLBase without any curriculum (Algorithm 2 in Appendix) during the
training of ResNet34 on CIFAR10 containing 80% symmetric noises on labels.
14
Published as a conference paper at ICLR 2021
Figure 4: RoCL (Algorithm 1) vs. RoCLBase without any curriculum (Algorithm 2 in Appendix) during the
training of ResNet34 on CIFAR100 containing 60% symmetric noises on labels.
number Oftraining batches (batch size = 128)
Figure 5: RoCL (Algorithm 1) vs. RoCLBase without any curriculum (Algorithm 2 in Appendix) during the
training of ResNet34 on CIFAR100 containing 80% symmetric noises on labels.
Table 6: Extended version of Table 3 with two more baselines: NFL+RCE and NCE+MAE.
Dataset		CIFAR10		CIFAR100		
Noise Rate	40%	60%	80%	40%	60%	80%
MD-DYR-SH	92.3	86.1	74.1	70.1	59.5	39.5
MentorNet	91.2	74.2	60.0	68.5	61.2	35.5
MentorMix	94.2	91.3	81.0	71.3	64.6	41.2
O2U-net	90.3	-	43.4	69.2	-	39.4
RoG+D2L	87.0	78.0	-	64.9	40.6	-
PENCIL	-	-	-	69.12 ± 0.62	57.79 ± 3.86	fail
GCE	87.62 ± 0.26	82.70 ± 0.23	67.92 ± 0.60	62.64 ± 0.33	54.04 ± 0.56	29.60 ± 0.51
SCE	85.34 ± 0.07	80.07 ± 0.02	53.81 ± 0.27	53.69 ± 0.07	41.47 ± 0.04	15.00 ± 0.04
NFL+MAE	83.81 ± 0.06	76.36 ± 0.31	45.23 ± 0.52	58.18 ± 0.08	46.10 ± 0.50	24.78 ± 0.82
NFL+RCE	86.05 ± 0.12	79.78 ± 0.13	55.06 ± 1.08	58.20 ± 0.31	46.30 ± 0.45	25.16 ± 0.55
NCE+MAE	84.19 ± 0.43	77.61 ± 0.05	49.62 ± 0.72	59.22 ± 0.36	48.06 ± 0.34	25.50 ± 0.76
NCE+RCE	86.02 ± 0.09	79.78 ± 0.50	52.71 ± 1.90	59.48 ± 0.56	47.12 ± 0.62	25.80 ± 1.12
RoCL (ours) ^?^0	94.55 ± 0.12	92.06 ± 0.23	85.76 ± 0.26	74.64 ± 0.43	66.79 ± 0.58	53.89 ± 0.62
Table 7: Extended version of Table 4 with two more baselines: NFL+RCE and NCE+MAE.
Dataset		CIFAR10		CIFAR100		
Noise Rate	20%	30%	40%	20%	30%	40%
PENCIL	92.43	91.84	91.01	74.70 ± 0.56	72.52 ± 0.38	63.61 ± 0.23
Bootstrap	86.57 ± 0.08	84.86 ± 0.05	79.76 ± 0.07	63.44 ± 0.35	63.18 ± 0.35	62.08 ± 0.22
F-correct	89.09 ± 0.47	86.79 ± 0.36	83.55 ± 0.58	42.46 ± 2.16	38.13 ± 2.97	34.44 ± 1.93
GCE	86.07 ± 0.31	80.78 ± 0.21	74.98 ± 0.32	59.99 ± 0.83	53.99 ± 0.29	41.49 ± 0.79
SCE	83.92 ± 0.07	79.70 ± 0.27	78.20 ± 0.03	58.22 ± 0.47	49.85 ± 0.91	42.19 ± 0.19
NFL+MAE	86.81 ± 0.32	83.91 ± 0.34	77.16 ± 0.10	63.10 ± 0.22	56.19 ± 0.61	43.51 ± 0.42
NFL+RCE	88.73 ± 0.29	85.74 ± 0.22	79.27 ± 0.43	63.12 ± 0.41	54.72 ± 0.38	42.97 ± 1.03
NCE+MAE	86.44 ± 0.23	83.98 ± 0.52	78.23 ± 0.42	62.38 ± 0.60	58.02 ± 0.48	47.22 ± 0.30
NCE+RCE	88.56 ± 0.17	85.58 ± 0.44	79.59 ± 0.40	62.68 ± 0.79	57.82 ± 0.41	46.79 ± 0.96
RoCL (ours)	95.38 ± 0.21	94.19 ± 0.28	92.31 ± 0.35	80.03 ± 0.34	77.59 ± 0.45	73.28 ± 0.83
15
Published as a conference paper at ICLR 2021
Figure 6: Illustration of Eq. (9) and visualization of our choice for g(∙) and the resulted Tt when T = 50. We
use g(∙) = tanh(∙) (which can be other "S”-ShaPe functions) and σ = 0.95 in our experiments. Here, We map
the points on the black curve in the left plot to the points on the red curve in the right plot. Each gray point on
the bottom of the left plot is from the T evenly spaced x-coordinates between the x-interval [g-1 (-σ), g-1 (σ)].
We scale them to the T t-coordinates in the bottom of the right plot (i.e., t = 1, 2,… ,50), which associates
with T τt values represented by the red points between [τ1 , τT] on the red curve.
CIFAR10 60% symmetric noise
80
75
70
65
10000
20000
30000
40000
50000
60000
number Oftraining batches (batch size = 128)
85
test accuracy (%)	一	test accuracy (%)
70000
Ablation study: RoCL vs. its variants during the training of ResNet34 on CIFAR10 containing
symmetric noises on labels.
CIFAR10 80% symmetric noise
80
70
60
50
40
30
20
RoCL: original
RoCL:
RoCL:
RoCL:
RoCL:
RoCL:
RoCL:
RoCL:
RoCL:
no MixUp
no label smoothing
no class balancing regularization
no RandAugment
no random sampling (using top-k selection)
no EMA metrics (using instantaneous metrics)
no clean data detection pt(i) = Ifn
no noisy label correction qt(i) = 1/n
RoCL: uniform curriculum pt{i) = qt[i) = 1/n
RoCLsase: no curriculum
10000
20000	30000	40000
50000	60000	70000
number Oftraining batches (batch size = 128)
Figure 8: Ablation study: RoCL vs. its variants during the training of ResNet34 on CIFAR10 containing
80% symmetric noises on labels.
16
Published as a conference paper at ICLR 2021
test accuracy (%)	一	test accuracy (%)
CIFAR100 60% symmetric noise
70
60
50
40
30
20
10
IOOOO
20000
30000
40000
50000
60000
number Oftraining batches (batch size = 128)
70000
re 9: Ablation study: RoCL vs. its variants during the training of ResNet34 on CIFAR100 containing
symmetric noises on labels.
CIFAR100 80% SymmetriC noise
40
10
50
30
20
IOOOO
20000
30000
40000
50000
60000
70000
number Oftraining batches (batch size = 128)
----RoCL: original
RoCL: no MixUp
RoCL: no label smoothing
RoCL: no class balancing regularization
----RoCL: no RandAugment
一 RoCL: no random sampling (using top-k selection)
一 RoCL: no EMA metrics (using instantaneous metrics)
---- RoCL: no clean data detection pt(i) = Ifn
---- RoCL: no noisy label correction qt(i) = Ifn
---- RoCL: uniform curriculum pt(i) = qt(i) = 1/n
-RoCLsase: no curriculum
Figure 10: Ablation study: RoCL vs. its variants during the training of ResNet34 on CIFAR100 containing
80% symmetric noises on labels.
We present a more detailed analysis of the ablation study results with explanations of the observed
phenomenons below.
•	Most variants (except RoCLBase , no RandAugment, and no ClassBalance) have similar perfor-
mance as the original RoCL and perform better than or competitive with the SoTA results achieved
by MentorMix. The differences compared to original RoCL become smaller under the lower
noise rate setting (60%). RoCLBase uses all data for training in each step without applying any
curriculum, showing that our proposed curriculum is the most critical component of RoCL in
achieving the appealing improvements. Note RoCLBase already outperforms most methods in
Table 3, which verifies the effectiveness of multi-episode training that alternates between supervised
learning with the given labels and self-supervision with the pseudo labels.
•	Removing RandAugment degrades the performance, especially when the noise rate is very high
(e.g., 80%) because strong data augmentations are required by the self-supervision and the EMA
consistency loss in RoCL, while trivial data augmentations can result in error accumulation or
over-confidence in pseudo labels and inaccurate EMA consistency loss. The self-supervision
aims to encourage the model output consistency over different augmentations of the same sample.
Without augmentations with sufficient variations, self-supervision reduces to reinforcing the same
outputs on similar samples and thus carries little information and can even magnify/accumulate
errors (if any) in the original outputs. Also, the EMA consistency loss cannot generate meaningful
consistency measures if computed on the same data or its trivial augmentations. Note a strong
data augmentation is not always beneficial in all noisy label learning methods since it can increase
the uncertainty in the presence of wrong labels, making the detection of clean data and noise
correction more challenging. For example, we tried applying RandAugment to MentorMix (using
17
Published as a conference paper at ICLR 2021
the official implementations of both) but observed inferior performance compared to the results
using its original data augmentations.
•	Class balance regularization is useful for the very high noise rate setting (80%), in which a wrong
label may dominate the learning on a mini-batch by a large chance. However, when the noise rate
is not that high (e.g., 60% on CIFAR10), removing it results in better performance.
•	Although Mix-Up has been proved effective in previous methods, and for this reason, we followed
MentorMix by starting with a relatively strong Mix-Up (alpha = 8.0) and then gradually reducing
it to α = 0.2. In the ablation study, we find that completely removing Mix-Up significantly
improves performance. Mix-Up is helpful when applied to mix a clean label with a noisy label
since the latter can be mediated with the former and thus softened. However, this is rarely the case
for RoCL since RoCL either mainly learns from clean data or wrongly-labeled data with correct
pseudo labels, and the transition between the two phases is short. When applied to two correct
labels/pseudo labels, Mix-Up weakens each label’s confidence, and we may lose information from
the inter-class probabilities in the soft pseudo labels.
•	Replacing weighted sampling with top-k selection (“no RandSampling”) or replacing EMA metrics
with instantaneous metrics (“no EMA metrics”) causes less degeneration on the final test accuracies.
However, they are important to the early-stage exploration and accurate estimation of EMA metrics
on less-visited samples. In Figure 7-10, these two variants usually suffer from low accuracy and
convergence speed during early stages. The only exception is “no RandSampling” in Figure 10,
which performs better than the original RoCL. A possible reason is that the randomness brought by
high uniform label noises already bring sufficient randomness for exploration.
•	Replacing pt (i), qt(i) or both with uniform probabilities over all samples reduces the final test
accuracies in all cases, e.g., the degradation is significant on CIFAR100 with 80% noise. In
Figure 7-10, we can see that by setting qt(i) = 1/n results in less degradation than the other two.
This is due to the more accurate pseudo labels generated for more data (even the ones with larger
EMA consistency loss) as training proceeds. Moreover, since we are conservative in setting λT and
τT , the performance is not very sensitive to wrong pseudo labels.
18