Published as a conference paper at ICLR 2021
On the Curse of Memory in Recurrent Neu-
ral Networks: Approximation and Optimiza-
tion Analysis
Zhong Lit
School of Mathematical Science
Peking University
li_zhong@pku.edu.cn
Jiequn Hant
Department of Mathematics
Princeton University
jiequnh@princeton.edu
Weinan E
Department of Mathematics and PACM
Princeton Univeristy
weinan@math.princeton.edu
Qianxiao L-
Department of Mathematics
National University of Singapore
IHPC, A*STAR, Singapore
qianxiao@nus.edu.sg
Ab stract
We study the approximation properties and optimization dynamics of recurrent
neural networks (RNNs) when applied to learn input-output relationships in tem-
poral data. We consider the simple but representative setting of using continuous-
time linear RNNs to learn from data generated by linear relationships. Mathemat-
ically, the latter can be understood as a sequence of linear functionals. We prove
a universal approximation theorem of such linear functionals and characterize the
approximation rate. Moreover, we perform a fine-grained dynamical analysis of
training linear RNNs by gradient methods. A unifying theme uncovered is the
non-trivial effect of memory, a notion that can be made precise in our framework,
on both approximation and optimization: when there is long-term memory in the
target, it takes a large number of neurons to approximate it. Moreover, the training
process will suffer from slow downs. In particular, both of these effects become
exponentially more pronounced with increasing memory - a phenomenon we call
the “curse of memory”. These analyses represent a basic step towards a concrete
mathematical understanding of new phenomenons that may arise in learning tem-
poral relationships using recurrent architectures.
1	Introduction
Recurrent neural networks (RNNs) (Rumelhart et al., 1986) are among the most frequently em-
ployed methods to build machine learning models on temporal data. Despite its ubiquitous applica-
tion (Baldi et al., 1999; Graves & Schmidhuber, 2009; Graves, 2013; Graves et al., 2013; Graves &
Jaitly, 2014; Gregor et al., 2015), some fundamental theoretical questions remain to be answered.
These come in several flavors. First, one may pose the approximation problem, which asks what
kind of temporal input-output relationships can RNNs model to an arbitrary precision. Second, one
may also consider the optimization problem, which concerns the dynamics of training (say, by gra-
dient descent) the RNN. While such questions can be posed for any machine learning model, the
crux of the problem for RNNs is how the recurrent structure of the model and the dynamical nature
of the data shape the answers to these problems. For example, it is often observed that when there
are long-term dependencies in the data (Bengio et al., 1994; Hochreiter et al., 2001), RNNs may
encounter problems in learning, but such statements have rarely been put on precise footing.
In this paper, we make a step in this direction by studying the approximation and optimization
properties of RNNs. Compared with the static feed-forward setting, the key distinguishing feature
* Equal contribution
^ Corresponding author
1
Published as a conference paper at ICLR 2021
here is the presence of temporal dynamics in terms of both the recurrent architectures in the model
and the dynamical structures in the data. Hence, to understand the influence of dynamics on learning
is of fundamental importance. As is often the case, the key effects of dynamics can already be
revealed in the simplest linear setting. For this reason, we will focus our analysis on linear RNNs,
i.e. those with linear activations. Further, we will employ a continuous-time analysis initially studied
in the context of feed-forward architectures (E, 2017; Haber & Ruthotto, 2017; Li et al., 2017)
and recently in recurrent settings (Ceni et al., 2019; Chang et al., 2019; Lim, 2020; Sherstinsky,
2018; Niu et al., 2019; Herrera et al., 2020; Rubanova et al., 2019) and idealize the RNN as a
continuous-time dynamical system. This allows us to phrase the problems under investigation in
convenient analytical settings that accentuates the effect of dynamics. In this case, the RNNs serve
to approximate relationships represented by sequences of linear functionals. On first look the setting
appears to be simple, but we show that it yields representative results that underlie key differences
in the dynamical setting as opposed to static supervised learning problems. In fact, we show that
memory, which can be made precise by the decay rates of the target linear functionals, can affect
both approximation rates and optimization dynamics in a non-trivial way.
Our main results are:
1.	We give a systematic analysis of the approximation of linear functionals by continuous-
time linear RNNs, including a precise characterization of the approximation rates in terms
of regularity and memory of the target functional.
2.	We give a fine-grained analysis of the optimization dynamics when training linear RNNs,
and show that the training efficiency is adversely affected by the presence of long-term
memory.
These results together paint a comprehensive picture of the interaction of learning and dynamics,
and makes concrete the heuristic observations that the presence of long-term memory affects RNN
learning in a negative manner (Bengio et al., 1994; Hochreiter et al., 2001). In particular, mirroring
the classical curse of dimensionality (Bellman, 1957), we introduce the concept of the curse of mem-
ory that captures the new phenomena that arises from learning temporal relationships: when there is
long-term memory in the data, one requires an exponentially large number of neurons for approxi-
mation, and the learning dynamics suffers from exponential slow downs. These results form a basic
step towards a mathematical understanding of the recurrent structure and its effects on learning from
temporal data.
2	Related Work
We will discuss related work on RNNs on three fronts concerning the central results in this paper,
namely approximation theory, optimization analysis and the role of memory in learning. A num-
ber of universal approximation results for RNNs have been obtained in discrete (Matthews, 1993;
Doya, 1993; Schafer & Zimmermann, 2006; 2007) and continuous time (Funahashi & Nakamura,
1993; Chow & Xiao-Dong Li, 2000; Li et al., 2005; Maass et al., 2007; Nakamura & Nakagawa,
2009). Most of these focus on the case where the target relationship is generated from a hidden
dynamical system in the form of difference or differential equations. The formulation of functional
approximation here is more general, albeit our results are currently limited to the linear setting. Nev-
ertheless, this is already sufficient to reveal new phenomena involving the interaction of learning and
dynamics. This will be especially apparent when we discuss approximation rates and optimization
dynamics. We also note that the functional/operator approximation using neural networks has been
explored in Chen & Chen (1993); Tianping Chen & Hong Chen (1995); Lu et al. (2019) for non-
recurrent structures and reservoir systems for which approximation results similar to random feature
models are derived (Gonon et al., 2020). The main difference here is that we explicitly study the
effect of memory in target functionals on learning using recurrent structures.
On the optimization side, there are a number of recent results concerning the training of RNNs
using gradient methods, and they are mostly positive in the sense that trainability is proved under
specific settings. These include recovering linear dynamics (Hardt et al., 2018) or training in over-
parameterized settings (Allen-Zhu et al., 2019). Here, our result concerns the general setting of
learning linear functionals that need not come from some underlying differential/difference equa-
tions, and is also away from the over-parameterized regime. In our case, we discover on the contrary
2
Published as a conference paper at ICLR 2021
that training can become very difficult even in the linear case, and this can be understood in a quan-
titative way, in relation to long-term memory in the target functionals.
This points to the practical literature in relation to memory and learning. The dynamical analysis
here puts the ubiquitous but heuristic observations - that long-term memory negatively impacts train-
ing efficiency (Bengio et al., 1994; Hochreiter et al., 2001) - on concrete theoretical footing, at least
in idealized settings. This may serve to justify or improve current heuristic methods (Tseng et al.,
2016; Dieng et al., 2017; Trinh et al., 2018) developed in applications to deal with the difficulty in
training with long-term memory. At the same time, we also complement general results on “van-
ishing and explosion of gradients” (Pascanu et al., 2013; Hanin & Rolnick, 2018; Hanin, 2018) that
are typically restricted to initialization settings with more precise characterizations in the dynamical
regime during training.
The long range dependency within temporal data has been studied for a long time in the time series
literature, although its effect on learning input-output relationships is rarely covered. For example,
the Hurst exponent (Hurst, 1951) is often used as a measure of long-term memory in time series,
e.g. fractional Brownian motion (Mandelbrot & Ness, 1968). In contrast with the setting in this
paper where memory involves the dependence of the output time series on the input, the Hurst
exponent measures temporal variations and dependence within the input time series itself. Much of
the time series literature investigates statistical properties and estimation methods of data with long
range dependence (Samorodnitsky, 2006; Taqqu et al., 1995; Beran, 1992; Doukhan et al., 2003).
One can also combine these classic statistical methodologies with the RNN-like architectures to
design hybrid models with various applications (Loukas & Oke, 2007; Diaconescu, 2008; Mohan &
Gaitonde, 2018; Bukhari et al., 2020).
3	Problem Formulation
The basic problem of supervised learning on time series data is to learn a mapping from an input
temporal sequence to an output sequence. Formally, one can think of the output at each time as
being produced from the input via an unknown function that depends on the entire input sequence,
or at least up to the time at which the prediction is made. In the discrete-time case, one can write the
data generation process
yk = Hk(x0, . . . , xk-1),	k = 0, 1, . . .	(1)
where xk , yk denote respectively the input data and output response, and {Hk : k = 0, 1, . . . } is a
sequence of ground truth functions of increasing input dimension accounting for temporal evolution.
The goal of supervised learning is to learn an approximation of the sequence of functions {Hk} given
observation data.
Recurrent neural networks (RNN) (Rumelhart et al., 1986) gives a natural way to parameterize such
a sequence of functions. In the simplest case, the one-layer RNN is given by
hk+1 = σ(Whk + Uxk),	yk = CVhk.	(2)
Here, {hk} are the hidden/latent states and its evolution is governed by a recursive application of a
feed-forward layer with activation σ, and yk is called the observation or readout. We omit the bias
term here and only consider a linear readout or output layer. For each time step k, the mapping
{xo,... ,xk-ι} → ^^k parameterizes a function Hk(∙) through adjustable parameters (c,W, U).
Hence, for a particular choice of these parameters, a sequence of functions {Hk} is constructed at
the same time. To understand the working principles of RNNs, We need to characterize how {Hk}
approximates {Hk }.
The model (2) is not easy to analyze due to its discrete iterative nature. Hence, here we employ a
continuous-time idealization that replaces the time-step index k by a continuous time parameter t.
This allows us to employ a large variety of continuum analysis tools to gain insights to the learning
problem. Let us now introduce this framework.
Continuous-time formulation. Consider a sequence of inputs indexed by a real-valued variable
t ∈ R instead of a discrete variable k considered previously. Concretely, we consider the input space
X = C0(R,Rd),	(3)
3
Published as a conference paper at ICLR 2021
which is the linear space of continuous functions from R (time) to Rd that vanishes at infinity. Here
d is the dimension of each point in the time series. We denote an element in X by x := {xt ∈ Rd :
t ∈ R} and equip X with the supremum norm kxkX := supt∈R kxt k∞. For the space of outputs we
will take a scalar time series, i.e. the space of bounded continuous functions from R to R:
Y = Cb(R, R).	(4)
This is due to the fact that vector-valued outputs can be handled by considering each output sepa-
rately. In continuous time, the target relationship (ground truth) to be learned is
yt = Ht(x),	t ∈ R	(5)
where for each t ∈ R, Ht is a functional Ht : X → R. Correspondingly, we define a continuous
version of (2) as a hypothesis space to model continuous-time functionals
dtht = σ(Wht + Uxt),	yt = CT ht,	(6)
whose Euler discretization corresponds to a discrete-time residual RNN. The dynamics then natu-
rally defines a sequences of functionals {HHt(x) = yt : t ∈ R}, which can be used to approximate
the target functionals {Ht} via adjusting (c, W, U).
Linear RNNs in continuous time. In this paper we mainly investigate the approximation and
optimization property of linear RNNs, which already reveals the essential effect of dynamics. The
linear RNN obeys (6) with σ being the identity map. Notice that in the theoretical setup, the initial
time of the system goes back to -∞ with limt→-∞ xt = 0, ∀x ∈ X, thus by linearity (Ht(0) = 0)
we specify the initial condition of the hidden state h-∞ = 0 for consistency. In this case, (6) has
the following solution
∞
0
yt
c> eWs U xt-s ds.
(7)
Since we will investigate uniform approximations over large time intervals, we will consider stable
RNNs, where W ∈ Wm with
Wm = {W ∈ Rm×m : eigenvalues of W have negative real parts}.	(8)
Owing to the representation of solutions in (7), the linear RNN defines a family of functionals
H ：= ∪m≥lHm,
H ：=
m :=
x),t ∈ R} : Ht(x) = ∞ cveWsUxt-sds,W ∈Wm,U ∈ Rm×d,c ∈ Rm
0
(9)
Here, m is the width of the network and controls the complexity of the hypothesis space. Clearly,
the family of functionals the RNN can represent is not arbitrary, and must possess some structure.
Let us now introduce some definitions of functionals that makes these structures precise.
Definition 3.1. Let {Ht :t∈ R} be a sequence of functionals.
1.	Ht is causal if it does not depend on future values of the input: for every pair of x, x0 ∈ X
such that xs = x0s for all s ≤t, we have Ht (x) = Ht (x0).
2.	Ht is linear and continuous if Ht (λx + λ0x0) = λHt(x) + λ0Ht (x0) for any x, x0 ∈ X
and λ, λ0 ∈ R, and supx∈X,kxk ≤1 |Ht(x)| < ∞, in which case the induced norm can be
defined as kHtk := supx∈X,kxkX ≤1 |Ht(x)|.
3.	Ht is regular if for any sequence {x(n) ∈ X : n ∈ N} such that x(n)t → 0 for Lebesgue
almost every t ∈ R, limn→∞ Ht(x(n)) = 0.
4.	{Ht : t ∈ R} is time-homogeneous if Ht(x) = Ht+τ (x(τ)) for any t, τ ∈ R, where
x(τ)s = xs-τ for all s ∈ R, i.e. x(τ) is x whose time index is shifted to the right by τ.
Linear, continuous and causal functionals are common definitions. One can think of regular func-
tionals as those that are not determined by values of the inputs on an arbitrarily small time interval,
e.g. an infinitely thin spike input. Time-homogeneous functionals, on the other hand, are those
where there is no special reference point in time: if the time index of both the input sequence and
the functional are shifted in coordination, the output value remains the same. Given these definitions,
the following observation can be verified directly and its proof is immediate and hence omitted.
4
Published as a conference paper at ICLR 2021
G -	T . C TT ɪ — τm T ι	r r . " ι ∙ .ι ɪ-ɪ λ tλ τ ι	. ι ∙	n't
Proposition 3.1. Let {Ht : t ∈ R} be a sequence of functionals in the RNN hypothesis space H
(see (9)). Thenfor each t ∈ R, Ht is a causal, continuous, linear and regular functional. Moreover,
the sequence OffUnCtionalS {H : t ∈ R} is time-homogeneous.
4 Approximation Theory
The most basic approximation problem for RNN is as follows: given some sequence of target func-
tionals {Ht : t ∈ R} satisfying appropriate conditions, does there always exist a sequence of RNN
functionals {Ht : t ∈ R} in H such that Ht ≈ Ht for all t ∈ R?
We now make an important remark with respect to the current problem formulation that differs from
previous investigations in the RNN approximation: we are not assuming that the target functionals
{Ht : t ∈ R} are themselves generated from an underlying dynamical system of the form
Ht (x) =	yt	where	ht	=	f (ht , xt),	yt	= g(ht)	(10)
for any linear or nonlinear functions f, g. This differs from previous work where it is assumed
that the sequence of target functionals are indeed generated from such a system. In that case, the
approximation problem reduces to that of the functions f, g, and the obtained results often resemble
those in feed-forward networks.
In our case, however, we consider general input-output relationships related by temporal sequences
of functionals, with no necessary recourse to the mechanism from which these relationships are
generated. This is more flexible and natural, since in applications it is often not clear how one can
describe the data generation process. Moreover, notice that in the linear case, if the target functionals
{Ht } are generated from a linear ODE system, then the approximation question is trivial: as long
as the dimension of ht in the approximating RNN is greater than or equal to that which generates
the data, we must have perfect approximation. However, we will see that in the more general case
here, this question becomes much more interesting, even in the linear regime. In fact, we now prove
precise approximation theories and characterize approximation rates that reveal intricate connections
with memory effects, which may be otherwise obscured if one considers more limited settings.
Our first main result is a converse of Proposition 3.1 in the form of an universal approximation
theorem for certain classes of linear functionals. The proof is found in Appendix A.
Theorem 4.1 (Universal approximation of linear functionals). Let {Ht : t ∈ R} be a family of
continuous, linear, causal, regular and time-homogeneous functionals on X. Then, for any > 0
there exists {Ht : t ∈ R} ∈ H such that
sup IlHt- Htk ≡ sup sup ∣Ht(x) - Ht(x)∣ ≤ e.	(11)
t∈R	t∈R kxkX ≤1
The proof relies on the classical Riesz-Markov-Kakutani representation theorem, which says that
each linear functional Ht can be uniquely associated with a signed measure μt such that Ht(X)=
JR x>dμt(s). Owing to the assumptions of Theorem 4.1, We can further show that the sequence of
representations {μt} are related to an integrable function P : [0, ∞) → Rd such that {Ht} admits
the common representation
Ht(x)
∞
xt>-sρ(s)ds,
0
t ∈ R,	x ∈ X.
(12)
Comparing this representation with the solution (7) of the continuous RNN, we find that the approx-
imation property of the linear RNNs is closely related to how well ρ(t) can be approximated by the
exponential sums of the form (c>eWtU)>. Intuitively, (12) says that each output yt = Ht(x) is
simply a convolution between the input signal and the kernel ρ. Thus, the smoothness and decay of
the input-output relationship is characterized by the convolution kernel ρ. Due to this observation,
we will hereafter refer to {Ht } and ρ interchangeably.
Approximation rates. While the previous result establishes the universal approximation property
of linear RNNs for suitable classes of linear functionals, it does not reveal to us which functionals
can be efficiently approximated. In the practical literature, it is often observed that when there is
5
Published as a conference paper at ICLR 2021
some long-term memory in the inputs and outputs, the RNN becomes quite ill-behaved (Bengio
et al., 1994; Hochreiter et al., 2001). It is the purpose of this section to establish results which
make these heuristics statements precise. In particular, we will show that the rate at which linear
functionals can be approximated by linear RNNs depends on the former’s smoothness and memory
properties. We note that this is a much less explored area in the approximation theory of RNNs.
To characterize smoothness and memory of linear functionals, we may pass to investigating the
properties of their actions on constant input signals. Concretely, let us denote by ei (i = 1, . . . , d)
the standard basis vector in Rd, and ei denotes a constant signal with ei,t = ei1{t≥0}. Then,
smoothness and memory is characterized by the regularity and decay rate of the maps t 7→ Ht(ei),
i = 1, . . . , d, respectively. Our second main result shows that these two properties are intimately
tied with the approximation rate. The proof is found in Appendix B.
Theorem 4.2 (Approximation rates of linear RNN). Assume the same conditions as in Theorem 4.1.
Consider the output of constant signals yi(t) := Ht(ei), i = 1, . . . , d. Suppose there exist constants
α ∈ N+, β, γ > 0 such that yi(t) ∈ C (α+1) (R), i = 1, . . . , d, and
eβty(k)(t)=。⑴ as t → +∞, and SuPβ-k ∣eβty(k)(t)∣ ≤ γ, k = 1,..., α + 1, (13)
t≥0
where yi(k)(t) denotes the kth derivative of yi(t). Then, there exists a universal constant C(α) only
depending on α, such that for any m ∈ N+, there exists a sequence of width-m RNN functionals
C -r'τ . τπ,、	r-t't	ɪ ɪ
{Ht : t ∈ R} ∈ Hm such that
C(α)γd
SuP IlHt — Htk ≡ suP suP ∣Ht(x) — Ht(x)∣ ≤ Q a .	(14)
t∈R	t∈R kxkX ≤1	βmα
The curse of memory in approximation. For approximation of non-linear functions using linear
combinations of basis functions, one often suffers from the “curse of dimensionality” (Bellman,
1957), in that the number of basis functions required to achieve a certain approximation accuracy
increases exponentially when the dimension of input space d increases. In the case of Theorem 4.2,
the bound scales linearly with d. This is because the target functional possesses a linear structure,
and hence each dimension can be approximated independently of others, resulting in an additive
error estimate. Nevertheless, due to the presence of the temporal dimension, there enters another
type of challenge, which we coin the curse of memory. Let us now discuss this point in detail.
The key observation is that the rate result requires exponential decay of derivatives ofHt(ei), but the
density result (Theorem 4.1) makes no such assumption. The natural question is thus, what happens
when no exponential decay is present? We assume d = 1 and consider an example in which the
target functional,s representation satisfies ρ(t) ∈ C⑴(R) and ρ(t)〜 t-(1+ω)
as t → +∞. Here
ω > 0 indicates the decay rate of the memory effects in our target functional family. The smaller its
value, the slower the decay and the longer the system memory. For any ω > 0, the system’s memory
vanishes more slowly than any exponential decay. Notice that y(1)(t) = ρ(t) and in this case there
exists no β > 0 making (13) true, and no rate estimate can be deduced from it.
A natural way to circumvent this obstacle is to introduce a truncation in time. With T (	1) we
can define p(t) ∈ C⑴(R) such that p(t) ≡ ρ(t) for t ≤ T, p(t) ≡ 0 for t ≥ T + 1, and p(t)
is monotonically decreasing for T ≤ t ≤ T + 1. With the auxiliary linear functional Ht(x) :=
R0 χt-sp(s)ds, we can have an error estimate (with technical details provided in Appendix B)
SuP ∣∣Ht - Htk ≤ SuP IlHt- Htk + SuP IlHt- Htk ≤ C (T ω +-------T1 ω) .	(15)
t∈R	t∈R	t∈R	m
In order to achieve an error tolerance e, according to the first term above We require T ~ E-1, and
then according to the second term we have
m = O (ωT1-ωIa = O (ωe-νω) .	(16)
This estimate gives us a quantitative relationship between the degree of freedom needed and the
decay speed. With ρ(t)〜 t-(1+ω), the system has long memory when ω is small. Denote the
minimum number of terms needed to achieve an L1 error E as m(ω, E). The above estimate shows
6
Published as a conference paper at ICLR 2021
an upper-bound of m(ω, ) goes to infinity exponentially fast as ω → 0+ with fixed . This is akin
to the curse of dimensionality, but this time on memory, which manifests itself even in the simplest
linear settings. A stronger result would be that the lower bound of m(ω, ) → ∞ exponentially fast
as ω → 0+ with fixed , and this is a point of future work. Note that this kind of estimates differ
from the previous results in the literature (Kammler, 1979b; Braess & Hackbusch, 2005) regarding
the order of m(ω, E)〜log(1∕e) as E → 0 with fixed ω = 1 or 2 in the L∞ or L1 sense. Note that
the L1 result has not been proved.
5 Fine-grained Analysis of Optimization Dynamics
According to Section 4, memory plays an important role in determining the approximation rates.
The result therein only depends on the model architecture, and does not concern the actual training
dynamics. In this section, we perform a fine-grained analysis on the latter, which again reveals an
interesting interaction between memory and learning.
The loss function (for training) is defined as
ExJT(x; c, W, U):= Eχ|HT(X)- HT(X)|2 = Ex
ZT
0
[c>eWtU - ρ(t)>]xT-tdt
(17)
2
Without loss of generality, here the input time series x is assumed to be finitely cut off at zero, i.e.
xt = 0 for any t ≤ 0 almost surely. Training the RNN amounts to optimizing Ex JT with respect
to the parameters (c, W, U). The most commonly applied method is gradient descent (GD) or its
stochastic variants (say SGD), which updates the parameters in the steepest descent direction.
We first show that the training dynamics ofExJT exhibits very different behaviors depending on the
form of target functionals. Take d = 1 and consider learning different target functionals with white
noise X. We first investigate two choices for ρ: a simple decaying exponential sum and a scaled
Airy function. The Airy function target is defined as ρ(t) = Ai(s0[t - t0]), where Ai(t) is the Airy
function of the first kind, given by the improper integral Ai(t) = ∏1 limξ→∞ R0ξ cos (u3 + tu) du.
Note that the effective rate of decay is controlled by the parameter t0 : for t ≤ t0, the Airy function
is oscillatory. Hence for large t0, a large amount of memory is present in the target.
Observe from Figure 1 that the training proceeds efficiently for the exponential sum case. How-
ever, in the second Airy function case, there are interesting “plateauing” behaviors in the training
loss, where the loss decrement slows down significantly after some time in training. The plateau is
sustained for a long time before an eventual reduction is observed.
As a further demonstration of that this behavior may be generic, we also consider a nonlinear forced
dynamical system, the Lorenz 96 system (Lorenz, 1996), where the similar plateauing behavior is
observed even for a non-linear RNN model trained with the Adam optimizer (Kingma & Ba, 2015).
All experimental details are found in Appendix C.3.1.
The results in Figure 1 hint at the fact that there are certainly some functionals that are much harder
to learn than others, and it is the purpose of the remaining analyses to understand precisely when and
why such difficulties occur. In particular, we will again relate this to the memory effects in the target
functional, which shows yet another facet of the curse of memory when it comes to optimization.
Dynamical analysis. To make analysis amenable, we will make a series of simplifications to the
loss function (17), by assuming that X is white noise, d = 1, T → ∞, and the recurrent kernel W is
diagonal. This allows us to write (see Appendix C.1 for details) the optimization problem as
min	J(a, w) :=	X ai e-wi t - ρ(t)	dt.	(18)
a∈Rm ,w∈R+m	0	i=1
We will subsequently see that these simplifications do not lose the key features of the training dy-
namics, such as the plateauing behavior. We start with some informal discussion on a probable
reason behind the plateauing. A straightforward computation shows that, for k = 1, 2, . . . , m,
∂w (a, w) = 2a/k Z	(—t)e-Wkt (X aie-wit — ρ(t)) dt.
(19)
7
Published as a conference paper at ICLR 2021
(a) Exponential sum target
O IOOOO 20000 30000 40000	O IOOOO 20000 30000 40000
epoch	epoch
(b) Airy function target
(c) Lorenz 96 dynamics target
Figure 1: Comparison of training dynamics on different types of functionals. (a) and (b): using
the linear RNN model with the GD optimizer; (c): using the nonlinear RNN model (with tanh
activation) with the Adam optimizer. The shaded region depicts the mean ± the standard deviation
in 10 independent runs using randomized initialization. Observe that learning complex functionals
(Airy, Lorenz) suffers from slow-downs in the form of long plateaus.
A similar expression holds for J. Write the (simplified) linear functional representation for linear
∂ak
RNNS as ρ(t; a, W) := Pm=I aie-wit, which serves to learn the target ρ. Observe that plateauing
under the GD dynamics occurs if the gradient VJ is small but the loss J is large. A sufficient
condition is that the residual p(t; a, w) - ρ(t) is large only for large t (meaning the exponential
multiplier to the residual is small). That is, the learned functional differs from the target functional
only at large times. This again relates to the long-term memory in the target.
Based on this observation, we build this memory effect explicitly into the target functional by con-
sidering ρ of the parametrized form
Pω (t) := p(t) + P0,ω (t),	(20)
where P is the function which can be well-approximated by the model p, e.g. the exponential sum
*	*
P(t) = ∑j=ι aje j t (with wj > 0, j = 1,…，m*). On the other hand, p0,ω (t) := po(t — 1∕ω)
controls the target memory, with P0 as any bounded template function in L2(R) ∩ C2 (R) with sub-
Gaussian tails. As ω → 0+, the support of p0,ω shifts towards large times, modelling the dominance
of long-term memories. In this case, if the initialization satisfies P ≈ p, the sufficient condition
informally discussed above is satisfied as ω → 0+, which heuristically leads to the plateauing.
A simple example of (20) can be Pω(t)
aje-w*t
_ (t - 1 /ω)2
+ coe	2σ2-, where aj,co,σ = 0 and wj
>0
are fixed constants. This corresponds to the simple case that mj = 1 and P0 is the Gaussian density.
Observe that as ω → 0+, the memory of this sequence of functionals represented by Pω increases.
It can be numerically verified that this simple target functional gives rise to the plateauing behavior,
which gets worse significantly as ω → 0+ (see Figure 2 in Appendix C.3.2).
Our main result on training dynamics quantifies the plateauing behavior theoretically for general
functionals possessing the decomposition (20). For rigorous statements and detailed proofs, see
Theorem C.1 in Appendix C.2.
Theorem 5.1. Define the loss function Jω as in (18) with the target P = Pω as defined in (20).
Consider the gradient flow training dynamics
-d-θω (T ) = -VJω (θω (T)),	θω (0)=。0,	(21)
dτ
where θω (T) := (aω (T), wω (T)) ∈ R2m for any T ≥ 0, and θ0 := (a0, w0). For any ω > 0,
m ∈ N+ and θ0 ∈ Rm × R+m , 0 < δ 1, define the hitting time
To = τo(δ; ω, m, θo):= inf {τ ≥ 0: | Jω(θω(T)) - Jω(θo)∣ > δ} .	(22)
Assume that m > mj, and the initialization is bounded and satisfies p(t; θ0) ≈ p(t). Then
τo(δ; ω,m,θo) & ω2ec"ω min {√m,ln(1 + δ)}	(23)
for any ω > 0 sufficiently small, where co and & hide universal positive constants independent of
ω, m and θo.
8
Published as a conference paper at ICLR 2021
Let us sketch the intuition behind Theorem 5.1. Suppose that we currently have a good approxima-
tion ρ of the short-term memory part ρ, then We can show that the loss is large (J = O(1)) since the
long-term memory part ρo,ω is not accounted for. However, the gradient now is small (VJ = o(1)),
since the gradient corresponding to the long-term memory part is concentrated at large t, and thus
modulated by exponentially decayed multipliers (see (19)). This implies slowdowns in the training
dynamics in the region P ≈ ρ. It remains to estimate a lower bound on the timescale of escaping
from this region, which depends on the curvature of the loss function. In particular, we show that
V2J is positive semi-definite when ω = 0, but has O(1) positive eigenvalues and multiple o(1)
(can be exponentially small) eigenvalues for any 0 < ω 1. Hence, a local linearization analysis
implies an exponentially increasing escape timescale, as indicated in (23).
While the target form (20) may appear restrictive, we emphasize that some restrictions on the type
of functionals is necessary, since plateauing does not always occur (see Figure 1). In fact, a goal
of the preceding analysis is to establish a family of functionals for which exponential slowdowns in
training provably occurs, and this can be related to memories of target functionals in a precise way.
The curse of memory in optimization. The timescale proved in Theorem 5.1 is verified numeri-
cally in Figure 3 in Appendix C.3.3, where we also show that the analytical setting here is represen-
tative of general cases, where plateauing occurs even for non-linear RNNs trained with accelerated
optimizers, as long as the target functional has the memory structure imposed in (20).
Theorem 5.1 reveals another aspect of the curse of memory, this time in optimization. When ω →
0+, the influence of target functional Ht does not decay, much like the case considered in the curse of
memory in approximation. However, different from the approximation case where an exponentially
large number of hidden states is required to achieve approximation tolerance, here in optimization
the adverse effect of memory comes with the exponentially pronounced slowdowns of the gradient
flow training dynamics. While this is theoretically proved under sensible but restrictive settings, we
show numerically in Appendix C.3.3 (Figure 4) that this is representative of general cases.
In the literature, a number of results have been obtained pertaining to the analysis of training dy-
namics of RNNs. A positive result for training by GD is established in Hardt et al. (2018), but this is
in the setting of identifying hidden systems, i.e. the target functional comes from a linear dynamical
system, hence it must possess good decay properties provided stablity. On the other hand, conver-
gence can also be ensured if the RNN is sufficiently over-parameterized (large m; Allen-Zhu et al.
(2019)). However, both of these settings may not be sufficient in reality. Here we provide an alter-
native analysis of a setting that is representative of the difficulties one may encounter in practice.
In particular, the curse of memory that we established here is consistent with the difficulty in RNN
training often observed in applications, where heuristic attributions to memory are often alluded to
Hu et al. (2018); Campos et al. (2017); Talathi & Vartak (2015); Li et al. (2018). The analysis here
makes the connection between memories and optimization difficulties precise, and may form a basis
for future developments to overcome such difficulties in applications.
6 Conclusion
In this paper, we analyzed the basic approximation and optimization aspects of using RNNs to learn
input-output relationships involving temporal sequences in the linear, continuous-time setting. In
particular, we coined the concept curse of memory, and revealed two of its facets. That is, when
the target relationship has the long-term memory, both approximation and optimization become
exceedingly difficult. These analyses make concrete heuristic observations of the adverse effect
of memory on learning RNNs. Moreover, it quantifies the interaction between the structure of the
model (RNN functionals) and the structure of the data (target functionals). The latter is a much less
studied topic. Here, we adopt a continuous-time approach in order gain access to more quantitative
tools, including classical results in approximation theory and stochastic analysis, which help us
derive precise results in approximation rates and optimization dynamics. The extension of these
results to discrete time may be performed via numerical analysis in subsequent work. More broadly,
this approach may be a basic starting point for understanding learning from partially observed time
series data in general, including gated RNN variants (Hochreiter & Schmidhuber, 1997; Cho et al.,
2014) and other methods such as transformers and convolution-based approaches (Vaswani et al.,
2017; Oord et al., 2016). These are certainly worthy of future exploration.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in Neural Information Processing Systems 32, 2019.
Pierre Baldi, S0ren Brunak, Paolo Frasconi, Giovanni Soda, and Gianluca Pollastri. Exploiting the
past and the future in protein secondary structure prediction. Bioinformatics, 15(11):937-946,
1999.
Richard Ernest Bellman. Dynamic Programming. Princeton University Press, 1957.
Yoshua. Bengio, Patrice. Simard, and Paolo. Frasconi. Learning long-term dependencies with gra-
dient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.
J. Beran. Statistical methods for data with long-range dependence. Statistical Science, 7:404-416,
1992.
Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Dietrich Braess. Approximation by exponential sums. In Nonlinear Approximation Theory, pp.
168-180. Springer, 1986.
Dietrich Braess and Wolfgang Hackbusch. Approximation of 1/x by exponential sums in [1, ∞).
IMA journal of numerical analysis, 25(4):685-697, 2005.
Ayaz Hussain Bukhari, Muhammad Asif Zahoor Raja, M. Sulaiman, S. Islam, Muhammad Shoaib,
and P. Kumam. Fractional neuro-sequential ARFIMA-LSTM for financial market forecasting.
IEEE Access, 8:71326-71338, 2020.
Victor Campos, Brendan Jou, Xavier Gir6-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN:
Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834,
2017.
Andrea Ceni, P. Ashwin, and L. Livi. Interpreting recurrent neural networks behaviour via excitable
network attractors. Cognitive Computation, 12:330-356, 2019.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A dynamical system
view on recurrent neural networks. In International Conference on Learning Representations,
2019.
Tianping Chen and Hong Chen. Approximations of continuous functionals by neural networks with
application to dynamical systems. IEEE Transactions on Neural Networks, 4:910-918, 1993.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Tommy W. S. Chow and Xiao-Dong Li. Modeling of continuous time dynamical systems with input
by recurrent neural networks. IEEE Transactions on Circuits and Systems I: Fundamental Theory
and Applications, 47(4):575-578, 2000.
Eugen Diaconescu. The use of narx neural networks to predict chaotic time series. WSEAS Trans-
actions on Computers archive, 3:182-191, 2008.
Adji B. Dieng, Chong Wang, Jianfeng Gao, and John W. Paisley. TopicRNN: A recurrent neural
network with long-range semantic dependency. In 5th International Conference on Learning
Representations, ICLR 2017, 2017.
P. Doukhan, G. Oppenheim, and M. Taqqu. Theory and applications of long-range dependence.
2003.
Kenji Doya. Universality of fully connected recurrent neural networks. Dept. of Biology, UCSD,
Tech. Rep, 1993.
10
Published as a conference paper at ICLR 2021
Weinan E. A Proposal on Machine Learning via Dynamical Systems. Communications in Mathe-
matics and Statistics, 5(1):1-11, 2017. ISSN 2194-6701.
Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous
time recurrent neural networks. Neural Networks, 6(6):801 - 806, 1993. ISSN 0893-6080.
Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega. Approximation bounds for random
neural networks and reservoir systems. arXiv preprint arXiv:2002.05933, 2020.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural
networks. In International Conference on Machine Learning, pp. 1764-1772, 2014.
Alex Graves and Jurgen Schmidhuber. Offline handwriting recognition with multidimensional re-
current neural networks. In Advances in Neural Information Processing Systems, pp. 545-552,
2009.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 6645-6649. IEEE, 2013.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. DRAW: A recurrent
neural network for image generation. volume 37 of Proceedings of Machine Learning Research,
pp. 1462-1471. PMLR, 2015.
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems,
34(1):014004, 2017.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In
Advances in Neural Information Processing Systems 31, pp. 582-591. Curran Associates, Inc.,
2018.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In Advances in Neural Information Processing Systems 31, pp. 571-581. Curran Associates, Inc.,
2018.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
Journal of Machine Learning Research, 19(29):1-44, 2018.
Qi-Ming He and Hanqin Zhang. On matrix exponential distributions. Advances in Applied Proba-
bility, 39:271-292, 2007.
Calypso Herrera, Florian Krach, and Josef Teichmann. Theoretical guarantees for learning condi-
tional expectation using controlled ODE-RNN. arXiv preprint arXiv:2006.04727, 2020.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JUrgen Schmidhuber. Gradient flow in re-
current nets: the difficulty of learning long-term dePendencies. In S. C. Kremer and J. F. Kolen
(eds.), A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.
Yuhuang Hu, Adrian Huber, Jithendar Anumula, and Shih-Chii Liu. Overcoming the vanishing
gradient problem in plain recurrent networks. arXiv preprint arXiv:1801.06105, 2018.
Harold Edwin Hurst. Long-term storage capacity of reservoirs. Transactions of the American Society
of Civil Engineers, 116:770-799, 1951.
Dunham Jackson. The theory of approximation, volume 11. American Mathematical Soc., 1930.
David W Kammler. Approximation with sums of exponentials in Lp[0, ∞). Journal of Approxima-
tion Theory, 16(4):384-408, 1976.
11
Published as a conference paper at ICLR 2021
David W. Kammler. Least squares approximation of completely monotonic functions by sums of
exponentials. SIAM Journal on Numerical Analysis,16(5):801-818, 1979a.
David W Kammler. L1-approximation of completely monotonic functions by sums of exponentials.
SIAM Journal on Numerical Analysis, 16(1):30-45, 1979b.
Diederik Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations (ICLR), 2015.
Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algorithms for deep
learning. The Journal of Machine Learning Research, 18(1):5998-6026, 2017. ISSN 1532-4435.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(IndRNN): Building a longer and deeper RNN. 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 5457-5466, 2018.
Xiao-Dong Li, John K. L. Ho, and Tommy W. S. Chow. Approximation of dynamical time-variant
systems by continuous-time recurrent neural networks. IEEE Transactions on Circuits and Sys-
tems II Analog and Digital Signal Processing, 52:656-660, 10 2005.
Soon Hoe Lim. Understanding recurrent neural networks using nonequilibrium response theory.
arXiv preprint arXiv:2006.11052, 2020.
Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on predictability,
volume 1, 1996.
G. Loukas and Gulay Oke. Likelihood ratios and recurrent random neural networks in detection of
denial of service attacks. 2007.
Lu Lu, Pengzhan Jin, and G. Karniadakis. DeepONet: Learning nonlinear operators for identifying
differential equations based on the universal approximation theorem of operators. arXiv preprint
arXiv:1910.03193, 2019.
Wolfgang Maass, Prashant Joshi, and Eduardo D Sontag. Computational aspects of feedback in
neural circuits. PLOS Computational Biology, 3(1):e165, 2007.
B.	Mandelbrot and J. V. Ness. Fractional Brownian motions, fractional noises and applications.
Siam Review, 10:422-437, 1968.
Michael B. Matthews. Approximating nonlinear fading-memory operators using neural network
models. Circuits, Systems and Signal Processing, 12:279-307, 1993.
A. Mohan and D. Gaitonde. A deep learning based approach to reduced order modeling for turbulent
flow control using lstm neural networks. arXiv: Computational Physics, 2018.
Ch H Muntz. Uber den approximationssatz Von Weierstrass. In Mathematische Abhandlungen
Hermann Amandus Schwarz, pp. 303-312. Springer, 1914.
Yuichi Nakamura and Masahiro NakagaWa. Approximation capability of continuous time recurrent
neural networks for non-autonomous dynamical systems. In Artificial Neural Networks - ICANN
2009, pp. 593-602, 2009.
Murphy Yuezhen Niu, Lior Horesh, and Isaac Chuang. Recurrent neural networks in the eye of
differential equations. arXiv preprint arXiv:1904.12933, 2019.
C.	A. O’Cinneide. Characterization of phase-type distributions. Stochastic Models, 6:1-57, 1990.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. volume 28 of Proceedings of Machine Learning Research, pp. 1310-1318, 2013.
12
Published as a conference paper at ICLR 2021
Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. Latent ordinary differential equations
for irregularly-sampled time series. In Advances in Neural Information Processing Systems 32,
pp. 5320-533θ. Curran Associates, Inc., 2019.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. Nature, 323(6088):533-536,1986.
G. Samorodnitsky. Long range dependence. Found. Trends Stoch. Syst., 1:163-257, 2006.
Anton Maximilian Schafer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In Artificial Neural Networks - ICANN 2006, pp. 632-640, 2006.
Anton Maximilian Schafer and Hans-Georg Zimmermann. Recurrent neural networks are universal
approximators. International journal of neural systems, 17(04):253-263, 2007.
Alex Sherstinsky. Fundamentals of recurrent neural network (RNN) and long short-term memory
(LSTM) network. arXiv preprint arXiv:1808.03314, 2018.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, pp. 2510-2518, 2014.
Otto Szasz. Uber die approximation stetiger funktionen durch lineare aggregate von potenzen. Math-
ematische Annalen, 77(4):482-496, 1916.
Sachin S Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu
nonlinearity. arXiv preprint arXiv:1511.03771, 2015.
M. Taqqu, Vadim Teverovsky, and W. Willinger. Estimators for long-range dependence: An empir-
ical study. Fractals, 03:785-798, 1995.
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. IEEE Transactions
on Neural Networks, 6(4):911-917, 1995.
Trieu H. Trinh, Andrew M. Dai, Thang Luong, and Quoc V. Le. Learning longer-term dependencies
in RNNs with auxiliary losses. In ICML, 2018.
Tzu-Hsuan Tseng, T. Yang, and Chia-Ping Chen. Verifying the long-range dependency of rnn lan-
guage models. 2016 International Conference on Asian Language Processing (IALP), pp. 75-78,
2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
13
Published as a conference paper at ICLR 2021
A Universal Approximation Theorem of Linear Functionals by
Linear RNNs
A key simplification of considering linear functionals is due to the classical representation result
below, which allows us to pass from the approximation of functionals to the approximation of func-
tions. In short, this theorem Says that while for each measure μ, x → JR x>dμ(s) defines a linear
functional, this is in fact the only way to define a linear functional: for any linear functional H there
exists a unique measure μ such that H(x) = JR x>dμ(s).
Theorem A.1 (Riesz-Markov-Kakutani representation theorem). Let H : X → R be a continuous
linear functional. Then, there exists a unique, vector-valued, regular, countably additive signed
measure μ on R such that
H(X)= / x>dμ(s)=E / Xs,idμi(s).
R	i=1 R
Moreover, we have
kHk := kχSUp≤ιH(X)I = kμk1(R)= ^"ig
(24)
(25)
Proof. Well-known, see e.g. Bogachev (2007), CH 7.10.4.	口
We will use the representation theorem to prove Theorem 4.1. First, we prove some lemmas. The
first shows that there is in fact a common representation ofa sequence of linear functionals satisfying
the assumptions of Theorem 4.1.
Lemma A.1. Let {Ht } be a family of continuous, linear, regular, causal and time homogeneous
functionals on X. Then, there exists a measurable function ρ : [0, ∞) → Rd that is integrable, i.e.
d∞
l∣ρkL1([o,∞)) := L	IPi(S)Ids < ∞
and
∞
Ht(X) =	Xt>-sρ(S)dS,	t ∈ R.
0
(26)
(27)
In particular, {Ht} is uniformly bounded with supt kHtk = kρkL1 ([0,∞)) andt 7→ Ht(X) is contin-
uous for all X ∈ X.
Proof. By the Riesz-Markov-Kakutani representation theorem (Theorem A.1), for each t there is a
unique regular signed Borel measure μt such that
Ht(X) = / Xsdμt(S),
R
(28)
and Pi ∣μt,i∣(R) = IlHtl∣. Since {Ht} is causal, We must have J∞ x>dμt(s) = 0 for any X and
thus
Ht(X) = J xs dμt(s).
Now, by time homogeneity we have
Z Xs dμt(S) = Ht(X) = Ht+τ(X (T) ) = Z	xs-τ dμt+τ (S).
-∞	-∞
Take T = -t and set μ = -μo to get
(29)
(30)
Ht(X)= [ xt-sdμ(s).
0
(31)
14
Published as a conference paper at ICLR 2021
Note that we have ∣∣μ∣∣ι([0, ∞)) = ∣∣μo∣∣ι([0, ∞)) = ∣∣Hok = kHtk, and continuity follows from
the fact that
∣Ht+δ(X)- Ht(x)∣
≤ XX J	kxt+δ-s - xt—s k ∞dlμi | (S),
(32)
which converges to 0 as δ → 0 by dominated convergence theorem. Finally, we will show that each
μi is absolutely continuous with respect to λ (Lebesgue measure). Take a measurable E ⊂ [0, ∞)
such that λ(E) = 0 and set E0 = [0, ∞) \ E. For each n ≥ 0 set Kn ⊂ E, Kn0 ⊂ E0 where Kn, Kn0
are closed and μi(E\ Kn) ≤ 1/n, μi(E0∖ Kn) ≤ 1/n. For a fixed i ∈ {1,..., d}, define x(n) to be
such that xt(n—)s,j = 0 for all j 6= i and all s. For j = i, we set xt(n—)s,i = 1 if s ∈ Kn and 0 if s ∈ Kn0 ,
which can then be continuously extended to [0, ∞). Observe that by construction, xt(n—)s → 0 for
λ-a.e. s, thus by dominated convergence theorem
0 = lim Ht(Xg) = μi(E).
(33)
n→∞
This shows that μ% is absolutely continuous with respect to λ, and by the Radon-Nikodym theorem
there exists a measurable function ρi : [0, ∞) → R such that for any measurable A ⊂ R we have
/ dμi(s) = / Pi(s)ds,
for i = 1, . . . , d. Hence, we have
(34)
∞
Ht(X)= /	x>-sP(s)ds
0
with kρkLi([o,∞)) = PiR0∞ |Pi(S)Ids = kμkιU0,∞)) < ∞.
(35)
Lemma A.2. Let ρ : [0, ∞) → R a Lebesgue integrable function, i.e. ∣ρ∣L1([0,∞)) < ∞. Then, for
any > 0, there exists a polynomial p with p(0) = 0 such that
UP-P(e-∙)IIL1([0,∞)) = L	|p⑴-P(e-t)|dt ≤ e.
(36)
Proof. The approach here is similar to that of the approximation of functions using exponential
sums (Kammler, 1976; Braess, 1986). Alternatively, one may also appeal to the density of phase
type distributions (He & Zhang, 2007; O’Cinneide, 1990) in the space of positive distributions, and
generalizing them to signed measures.
Fix > 0. Define
R(u)
1ρ(-logu), U ∈ (0,1],
0,
u = 0.
(37)
□
Then, we can check that
∣R∣L1([0,1]) = ∣ρ∣L1([0,∞)) < ∞.
(38)
〜
〜
By density of continuous functions in L1 there exists a continuous function R on [0, 1] with R(0)
0 such that
∣R — R∣li([o,i])≤ 〃2.	(39)
By MUntz-SzaSz theorem (Muntz, 1914; Szasz, 1916), there exists a polynomial P with p(0) = 0
such that
kq - RkL1([0,1]) ≤ e/2,
and q(u) := p(u)/u is also a polynomial. Therefore, we have
|1P — P(L)11L1([0,∞)) =/ IR(U)-PU)IudU
11
≤	|R(U) - R(U)|dU +	|R(U) - P(U)/U|dU ≤ .
(40)
(41)
□
15
Published as a conference paper at ICLR 2021
We are now ready to present the proof of Theorem 4.1.
Proof of Theorem 4.1. By (7), for each {Ht} ∈ H we can write
By Lemma A.1, we can write
ʌ , .
Ht (x)
(42)
∞
Ht(x) =	xt>-sρ(S)dS,
0
(43)
where ρ is integrable. Thus, we can apply Lemma A.2 to conclude that there exists polynomials
pi, i = 1, . . . , d with pi(0) = 0 such that
EkPi-Pi(e-∙)kL1([0,∞)) ≤ e.
i
(44)
Notice that we can write eachpi(u) = Pjm=1 αij uj for some m equaling the maximal order of {pi}.
Taking W = diag(-1, . . . , -m), c = (1, . . . , 1) and Uij = αji, we have
(U> [eWs]>c)i = pi(e-s),	i = 1, . . . , d.
Consequently, we have for any x with kxk∞ ≤ 1,
(45)
. , . ʌ , ..
∣Ht(x) - Ht(x)∣
∞∞
xt>-sρ(s)ds -	xt>-sp(e-s)ds
0
≤ XL	|xt-s,i| ∣Pi(S)- Pi(e-s)∖ds ≤ X ∣∣Pi - Pi(e-∙)∣L1([0,∞))	(46)
≤.
B Approximation Rates and the Curse of Memory in
Approximation
We first give the proof of Theorem 4.2.
Proof of Theorem 4.2. We fix i ∈ {1, . . . , d} below until the last part of the proof. By Lemma A.1,
there exists ρi (t) ∈ Cα [0, ∞) such that
yi(t) = Ht(ei) = Z ρi(r)dr,	t ≥ 0.
0
(47)
By the assumption,
ρi(k)(t) = o(e-βt) as t → ∞,
k = 0, . . . , α.
(48)
□
Consider the transform
0
qi(S) = ρ Pi(-(α+β)logS)
= 0,
∈ (0, 1].
(49)
s
s
For k = 0, . . . , α, one can prove by induction that
k	/	「∖ j Jj) --(a+1)log S
q(k)(s) = (-i)k X c(j,k) (α+1)	Je
(50)
16
Published as a conference paper at ICLR 2021
where c(j, k) are some integer constants. Together with the assumption, we have
k	+ 1 j	(j)(t)	k
Iqi (C-α+1 )1 =Xc(j,k) (--τ~)々Ji)；J ≤ XKj,k)|(a +I)j∣ Y ≤ Cg)Y, (51)
I	Ij=O β p e e-^α+^t I	j=0
where C(α) is a universal constant only depending on α. Note that for j = 0, . . . , α,
Cj) (-(α+1) log S λ
s→m+ p⅛≡1
lim
t→∞
PCj)⑴
(k + 1)β t
e	α+1 t
1. PCj) (t) - (α-k)β t C
lim ——-r-ʌ e α+1 = 0,
t→∞ e-βt
(52)
hence qi(s) ∈ Cα[0,1] with qi(0) = qCI)(O)=…=qCa)(O) = 0. By Jackson's theorem (Jackson,
1930), for m = 1, 2, . . . , there exists a polynomial Qi,m of degree m - 1 such that
kqi - Qi,mkL∞C[0,1]) ≤
Cg)Y
mα
(53)
Denote the polynomial Qi,m as
m-1
Qi,m(s) =	αi,j sj,
j=0
and define
φi,m(t) = e-α+1 tQi,m(e- α+1 t).
Then we have
φi,m(t) = c>eWtui,
where
c=(1,1,...,1),
β
a+1
2β
a+1
mβ
a+1
ui = (αi,0, αi,1, . . . , αi,m-1 ).
……	，	，一	一一屋t	，	..	...
With a change of variable S = e α+1 ,we have the estimate
kPi -
∞
φi,mkL1C[0,∞)) =
0
∣Pi(t) - Φi,m(t)∣dt
/ 1∣Pi (-(α +β1)lθgS) -SQim(S)
α⅛1 [1 ∣qi(s) - Qi,m(s)∣ds
β0
≤ C(a)Y
_ βma
Finally we define U = [u1, . . . , ud] ∈ Rm×d and have
c>eWtU = (φ1,m(t), . . . , φd,m(t)).
Recall the dynamical system (6)
dtht = σ(Wht + Uxt)	yt = c> ht,
α+1
----ds
βs
(54)
(55)
(56)
(57)
(58)
(59)
(60)
(61)
(62)
W
which is together determined by the parameters c, W, U. Similar to the argument in the proof of
Theorem 4.1, for any x with kxk∞ ≤ 1 and t, we have
∣Ht(x) - Ht(x)∣ ≤ E ∣∣Pi - Φi,mkLiC[0,∞)) ≤
i
C (α)γd
βma
(63)
□
17
Published as a conference paper at ICLR 2021
The curse of memory in approximation. Now we explain more technical details of why Theo-
rem 4.2 implies the curse of memory in approximation, as pointed out in the main text. We assume
d = 1 and consider an example
Ht(x) :=	xt-sρ(s)ds, t ≥ 0,
0
(64)
in which the density satisfies ρ(t) ∈ C(1) (R) and
ρ(t)〜t-(1+ω) as t → +∞.	(65)
Here ω > 0 indicates the decay rate of the memory effects in our target functional family {Ht }. The
smaller its value, the slower the decay and the longer the system memory. Notice that y(1) (t) = ρ(t),
and in this case there exists no β > 0 making the following condition ((13) in the main text) true:
eβty(k)(t) = o(1) as t → +∞,	and	supβ-k∣eβty(k)(t)∣ ≤ γ,	(66)
t≥0
and no rate estimate can be deduced from it.
A natural way to circumvent this obstacle is to introduce a truncation in time. With T (	1) we
can define p(t) ∈ C⑴(R) such that p(t) ≡ ρ(t) for t ≤ T, p(t) ≡ 0 for t ≥ T + 1, and p(t)
is monotonically decreasing for T ≤ t ≤ T + 1. Considering the linear functional Ht(x) :=
Rt χt-sp(s)ds, We have the truncation error estimate
〜
IHt(X)- Ht(x)∖ ≤ IIxkX
ɑ∞ ∣ρ(s)∣
〜IxkXT-ω.
(67)
NoW the conclusion of Theorem 4.2 (i.e. (63)) is applicable to the truncated {Ht} (With α = 1), and
We have for ∀β > 0, there is a linear RNN (U, W, c) such that the associated functionals {Ht} ∈ Hm
satisfy
Cγ C	∖eβty(1) (t)∖	Cω eβT
Sup kHt - Htk≤ Jm := Jm s≥p —β — = -mβ2Tω+τ∙
(68)
It is straightforWard to verify that When J= 2/T, the right-hand side of (68) achieves the minimum,
Which gives us
SuP kHt - Htk ≤ CωT 1-ω.	(69)
t∈R	m
Combining (67) and (69) gives us
sup kHt - Htk ≤ C (τ-ω + ωT1-ω) .	(70)
t∈R	m
In order to achieve an error tolerance e, according to the first term above we require T 〜E- 1, and
then according to second term We have
m = O ( ωT1 ω ) = O (ωe-1).	(71)
This estimate gives us a quantitative relationship between the degree of freedom needed and the
decay speed. When ω is small, i.e., the system has long memory, the size of the RNN required
grows exponentially.
C Fine-grained Analysis of Optimization Dynamics
C.1 Simplifications on the Loss Function
Recall the loss function
2
ExJT(x; c, W, U) = Ex
ZT
0
[c>eWtU - ρ(t)>]xT-tdt
(72)
The simplifications on Ex JT are listed as follows.
18
Published as a conference paper at ICLR 2021
1.	Take the input data x to be the white noise, so that
xT-tdt in dist=ribution dBt,	(73)
where Bt is the standard d-dimensional Wiener process. As a consequence, simplifying
(72) via Ito,s isometry gives
JT(c,W,U)
:=ExJT(x;c,W,U) = Z T c>eWtU - ρ(t)>22 dt.
0
(74)
2.	We focus on the temporal dimension and take the spatial dimension d = 1 in (74).1 2 More-
over, to investigate the effect of long-term memory, it is necessary to consider the training
on large time horizons. Hence, we take T → ∞ to get
J∞ (c,
W,b) = Γ (cT
0
eWtb - ρ(t)2 dt,
(75)
where b is the sole column of U in (74) and ρ(t) becomes a scalar-valued target. This
corresponds to the so-called single-input-single-output (SISO) system.
3.	Due to the difficulty of directly analyzing Vwewt and VWewt, We consider a further
simplified ansatz. Assume that W is a diagonal matrix with negative entries (to guarantee
the stability of the model). That is, W = -diag(w) With w ∈ R+m. Then We can combine
a = b ◦ c (Where ◦ denotes the Hadamard product) and reWrite the model as
m
ρ(t; c, W, b) := cτewtb = ^X age-wit，aτe-wt，p(t; a, w).	(76)
i=1
The optimization problem (75) becomes
min	J(a, w) :=	X aie-wit - ρ(t)	dt.	(77)
a∈Rm,w∈R+m	0	i=1
Here We omit the subscript ∞. 2
4.	We apply a continuous-time idealization of the gradient descent dynamics by considering
the gradient floW With respect to J(a, w). That is,
a0(τ) = -Va J (a(τ), w(τ)),
w0(τ) = -Vw J (a(τ), w(τ)),
With some initial value a(0) = a0 ∈ Rm, w(0) = w0 ∈ R+m.
(78)
As We Will shoW later, applying the training dynamics (78) to optimize (77) is able to serve as a
starting point in the fine-grained dynamical analysis, since it still preserves the plateauing behav-
ior observed in the optimization process (see Figure 1), provided additional structures related to
memories as discussed next.
C.2 Concrete Dynamical Analysis and the Curse of Memory in Optimization
We prove Theorem 5.1 in this section. The basic insight is, by adding long-term memories in targets,
one can increase the loss With little effect on the gradient and Hessian, Which leads to a significant
sloW doWn of the training dynamics near the short-term memory parts of the targets. Therefore,
Theorem 5.1 is proved subsequently in the folloWing procedure.
1.	We prove that Jω has a large value but small gradient when p(t; a, W) ≡ ρ(t).
1One can see that the spatial dimension plays little role in the previous approximation analysis (see the proof
of Theorem 4.2), since each spatial dimension can be handled separately.
2The time horizon is always taken as ∞ in the whole analysis. Note that here we also omit an index m
(width of the network, relating to the model capacity), since it remains unchanged in the following content if
not specified.
19
Published as a conference paper at ICLR 2021
2.	We prove that when p(t; a,w) ≡ ρ(t), the Hessian V2 Jω is positive semi-definite for
ω = 0, but for finite, small ω > 0, V2 Jω has O(1) positive eigenvalues and multiple o(1)
eigenvalues.
3.	Based on these results, we perform a local linearization analysis on the gradient flow (21)
initialized by p(t; ao, w0) ≡ p(t), from which and by continuity the timescale of plateauing
is derived.
(1)	Preliminary Results
As discussed around (20), we consider the target functional with a parametrized representation
m*
Pω ⑴=P⑴ + Pθ,ω ⑴=^X aje Wjt + Pθ(t - ∖/ 3、.
j=1
Here, aj := a*(w*) = 0, Wj > 0 and w* = Wj for any i = j, i,j ∈ [m*], 3 and m* < m. The
former requirements are just non-degenerate conditions, and the last requirement ensures that the
model can perfectly represent the well-approximated part of the target, ρ(t). The memory in the
target is controlled by ρo,ω(t) = ρo(t — l∕ω), with ρo as a fixed template function which satisfies
the following assumptions.
Assumptions on	P0.	(i)	P0(t) 6≡ 0; (ii)	P0	∈	L2(R)	∩ C2 (R);	(iii)	P0	is bounded on	R,	i.e.
kP0kL∞(R) < ∞; (iv) lim P0(t) = 0.
t→-∞
Remark C.1. The above assumptions (i)(ii)(iii) are rather natural, and (iv) only restricts the single
side tail of P0 to be zero. In the following analysis, we further focus on P0 with light tails, e.g. the
sub-Gaussian tail
|P0(t)| ≤ c0e-c1t2,	∀t : |t| ≥ t0	(79)
for some fixed positive constants c0, c1, t0. Obviously, Gaussian densities and continuous functions
with compact supports are sub-Gaussian functions.
We begin by the following preliminary estimate that is used throughout the subsequent analysis.
Lemma C.1. For any n ∈ N, ω > 0 and W > 0, let
∞
∆n,ω (W) :=
0
tn e-wt∣P0,ω (t)∣dt.
(80)
Then
•	∆n,ω (W) is monotonically decreasing on (0, ∞);
•	lim ∆n,ω(W) = 0;
ω→0+
•	In particular, if P0 is sub-Gaussian, we further have
∆n,ω(w) . ω-ne-w∕ω (cw2 + Cw) , ω ∈ (0, miη{1∕2,1/t。，2cι∕w}).	(81)
Here c2 = e411 > 1, eɜ = et0 > 1, and . hides universal constants only depending on n
and P0, t0, c0, c1.
Proof. (i) Obviously ∆n,ω(W1) ≤ ∆n,ω(W2) for any W1 > W2 > 0.
(ii)	By the assumptions on P0, we get
lim	∣Po,ω(t)|	= lim	∣P0(t — 1∕ω)∣ = lim	∣Po(s)∣	=	0,	∀t	≥	0,
ω→0+	ω→0+	s→-∞
and Mo := ∣∣po∣∣l∞(r) < ∞, which gives tne-wt∣Po,ω(t)| ≤ Motne-wt ∈ L1 ([0, ∞)) for any
n ∈ N, ω > 0 and W > 0. By Lebesgue’s dominant convergence theorem, we have
∞
0
lim ∆n,ω(W)
ω→0+
tne-wt
lim ∣Po,ω (t)∣dt = 0,
ω→0+
∀n ∈ N, ∀W > 0.
(82)
3For any n ∈ N+, [n] := {1,2, •…，n}.
20
Published as a conference paper at ICLR 2021
(iii)	Now we estimate ∆n,ω (w) under the sub-Gaussian condition (79). Suppose 0 < ω < 1/t0, we
have
∞
∣∆n,ω(w)| = / tne-wt∣ρo(t - 1∕ω)∣dt
0
1∕ω+t0	1∕ω-t0
=/	tne-wt∣ρo(t - 1∕ω)∣dt + /	tne-wt∣ρo(t - 1∕ω)∣dt
1∕ω-t0	0
∞
+ J	tne wt∣po(t — 1∕ω)∣dt，Iι + I + I3.
Then we bound I1 , I2 and I3 respectively:
∕1∕ω+t0
Ii ≤ Mo	tne-wtdt ≤ MOe-W(I∕ω-tO)
1∕ω-t0
1∕ω+t0
tndt
1∕ω-t0
Moewt0 ∙ e-w∕ω
(1 + ωt0)n+1 - (1 - ωt0)n+1
(n+ 1)ωn+1
. Moewt0ω-ne-w∕ω (to + ω),
where ω ∈ (0, 1/2), and . hides universal constants only related to n and to. Let 1/c1 := 2σ2, we
have
I2 = e-w∕ω - 'is + 1∕ω)ne-ws∣ρo(s)∣ds ≤ e-w∕ω [	(s +1∕ω)ne-ws ∙ coe-c1s2ds,
-1∕ω	-1∕ω
where
-t0	2	w2	-t0	w 2
(S + 1∕ω)ne-wse-c1s ds = e4cΓ	(s + 1∕ω)ne-c1 (s+ ⅛) ds
-1∕ω	-1∕ω
≤ eσ2w2∕2 ( (|t| + ∣1∕ω - σ2w∣)n ∙ e-2⅛dt
R
n∞
=eσ2w2∕2 Vcn(1∕ω - σ2w)n-k ∙ 2 / tke
k=o	o
__tL
2σ2 dt
≤ eσ2w2∕2 XX Cn(1∕ω)n-k(√2σ)k+1Γ (k+1)
k=o
holds for any ω ∈ (0, 2c1∕w). Here the last inequality is due to the Mellin Transform of absolute
moments of the Gaussian density (see Proposition 1 in Li et al. (2005)). The argument is similar for
I3, which gives the same bound as I2.
Combining all the estimates gives the desired conclusion. The proof is completed.	口
The main idea to analyze plateauing behaviors is to investigate the local dynamics of the gradient
flow (21) when P = ρ, then extend the results to the setting P ≈ P by continuity. Recall that both of
them are exponential sums, we can obtain the relation of parameters between P and ρ, according to
the following lemma.
Lemma C.2. For any m ∈ N+,let λ = (λι,…，λm) with λi = λj for any i = j, i,j ∈ [m]. Then
the series of functions eλit im=1 is linear independent on any interval I ⊂ R.
Proof. The aim is to show
m
X cieλit = 0, t ∈ I ⇒ ci = 0, ∀i ∈ [m].	(83)
i=1
(83) holds trivially for m = 1. Assume that (83) holds for m - 1, then
m	m-1
X cieλit = 0, t ∈ I ⇒ X cie(λi-λm)t + cm = 0, t ∈ I	(84)
i=1	i=1
m-1
⇒ X ci(λi - λm)e(λi-λm)t = 0, t ∈ I.
i=1
21
Published as a conference paper at ICLR 2021
By induction, We get ci(λi - λm) = 0 for any i = 1, ∙∙∙ ,m - 1. Since λι, ∙∙∙ ,λm are distinct,
We have Ci = 0 for any i = 1,…，m - 1. Together with (84), we get Cm = 0, which completes the
proof.	□
Definition C.1. Let m ≥ m*. For any partition P: [m] = ∪moIj with ZjI Cj = 0 for any
jι = j2, j1,j2 ∈ {0} ∪ [m*], and Io = ∪=1I0,r with Io,n ∩ I0,r2 = 0 for any ri = r,
r1,r2 ∈ [io], where Tj = 0 for any j ∈ [m*] and Io,r = 0 for any r ∈ [io] (if Io = 0), define the
affine space (with respect to P):
M*P :=	(a,	w)	∈	Rm	×	R+m	: X ai =	aj*,	wi =	wj* for any i ∈	Ij,	j ∈	[m*];
i∈Ij
ai = 0, wi = vr 6= wj* for any i ∈ Io,r ,
i∈I0,r
r ∈ [io] andj ∈ [m*]
.
Denote the collection of all such affine spaces by M* := P M*P.
The following lemma characterizes the relation of parameters, by showing that M* is exactly the
set of equivalent points to (a*, w*) for the purpose of representation via exponential sums.
Lemma C.3. For any (a, W) ∈ Rm X Rm, p(t; a, W) ≡ p(t) ⇔ (a, W) ∈ M*.
Proof. (i)(u) Since (a,w) ∈ M*, there exists P such that (a, W) ∈ MP. Then for any t ≥ 0,
m
ρ(t; a, W) = ^X aie-wit
i=1
--*	∙	- - *
m	io	m
X Xaie-wit = X X aie-wit+ X Xaie-wit
i0	m*	m*
X	X	ai	e-vrt	+	X	X ai	e-w*t	=	X	*-7=P(D
r=1	i∈I0,r	j=1	i∈Ij	j=1
(ii) (⇒) Let Ij	= i ∈	[m]	:	Wi	= Wj*	for anyj ∈	[m*], and Io	= i ∈	[m]	:	Wi	6=	Wj*	for any j ∈
**
[m*]}. Recall that p(t) = Ej=I aje Wj t is non-degenerate: a* = 0, Wj > 0 and w* = w** for any
i 6= j, i,j ∈ [m*], we get [m] = ∪jm=*oIj, Ij1 ∩Ij2 = 0 for any j1 6= j2, j1,j2 ∈ {0} ∪ [m*].
Combining Lemma C.2 and the non-degeneracy of P , Ij = 0 for any j ∈ [m*]. Assume that
there are io different components in (Wi)i∈ι, say vi,…,vi0, then Vr = Wj for any r ∈ [io] and
j ∈ [m*]. Let Io,r = i ∈ Io : Wi = vr for any r ∈ [io], we get Io,r 6= 0 for any r ∈ [io]
(if Io 6= 0), and Io = ∪ir0=1Io,r, and Io,r1 ∩ Io,r2 = 0 for any r1 6= r2, r1,r2 ∈ [io]. Hence
[m] = ∪jm=*oIj with Io = ∪ir0=1Io,r forms a P defined in Definition C.1, and
*
m	m*
0 ≡ ρ(t; a, w) — p(t) = ^X aie-wit — ^X a*e-w*t
i=1	j =1
m*	m*
= XX aie-wit - X aj*e-wj*t
j=o i∈Ij	j =1
io	/ m*	m*	.
=XX aie-wit + IXX a--X Lwt
r=1 i∈Io,r	j=1 i∈Ij	j =1
=Xio	X ai!e-vrt+Xm*	Xai -aj*!e-wj*t.
r=1	i∈Io,r	j =1	i∈Ij
Again by Lemma C.2, we have i∈I ai = aj* for any j ∈ [m*] and i∈I ai = 0 for any
r ∈ [io], which gives (a, w) ∈ MP. The proof is completed.	□
22
Published as a conference paper at ICLR 2021
Remark C.2. Let Io = 0. 4 It is straightforward to check that for any partition P, the dimension
_ r^. *
of MP is Ej=I(IIj| — 1) = m — m*. In addition, it can be verified that the cardinality of M* is
m*! 1 mm ρ where {[^ } is the Stirling number ofthe second kind. 5
(2)	Loss and Gradient
Now We show that as ω → 0+, the loss Jω remains lower bounded, while ∣∣VJω ∣∣2 converges to 0.
This implies that the loss will saturate at a high value when learning long-term memories.
Proposition C.1. For any (a, W) ∈ Rm X Rm satisfying ρ(t; a, W) ≡ p(t), we have
Jω(a,w) ≥ C(ρ0) >0, ∀ω ∈ (0, C0(ρ0)),	(85)
where C(ρ0), C0(ρ0) > 0 are constants only depending on ρ0. That is, the loss is lower bounded
uniformly in ω.
Proof. Recall the assumptions on ρ0, we have ρ0 (t) 6≡ 0 and ρ0 ∈ C(R). Let t1 ∈ R such
that ρ0(t1) = 0. By continuity, there exists δo > 0 such that ∣ρo(t)∣ ≥ ∣ρo(tι)∣∕2 for any t ∈
[tι 一 δo,tι + δo]. Hence, for any ω > 0 sufficiently small such that -1∕ω <tι 一 δo,we have
∞	∞	t1+δ0	1
∣ρ0,ω ∣ L2[0,∞) =J	P2(t- 1∕ω)dt = J ɪ ρ2(t)dt ≥ J	ρ0(t)dt ≥ -δo∣ρo(tι)∣2 > 0.
JO	J-ω	7tι-δθ
Fix any (a, W) ∈ Rm × Rm such that p(t; ^, W) ≡ p(t). Then
Jω (a,W) = kp(t；a,W) — P(t) — P0,ω (t)kL2[0,∞) = kP0,ω kL2[0,∞) ≥ -δolPo(tι)l2 > 0,
which completes the proof.	□
Proposition C.2. For any (a, W) ∈ Rm × Rm satisfying ρ(t; a, W) ≡ p(t), we have
lim ∣VJω (a, W)∣2 =0.	(86)
ω→0+
In particular, if ρ0 has the sub-Gaussian tail (79), the estimate
∣VJω(a,W)∣2 . √mω-1e-wmin/ω (Cwm + Cwmn) (1 + ∣a∣∞)	(87)
holds for any ω ∈ (0, min{1∕-, 1∕t0, -c1∕Wmin}). Here Wmin := mini∈[m] Wi > 0,c2,c3 > 1 are
constants only related to C1, t0, and . hides universal constants only depending on ρ0, t0, C0, C1.
Proof. A straightforward computation shows, for k = 1, 2, ∙∙∙ ,m,
J (a,W) = 2 X
∂ak	i=1
ai
Wk + Wi
*
m*	*
_ X _aj_
j =1 Wk + Wj*
一 ∆0,ω (Wk ),
篙(a,W) = -2ak X
(Wk + Wi )2
*
m*
- X
j=1
*
aj
(Wk+Wjy
+ ak∆1,ω (Wk).
(88)
(89)
m
m
a
Fix any (^, W) ∈ Rm × Rm satisfying p(t; a, W) ≡ p(t). By Lemma C.3, we have (^,W) ∈ M*.
Recall Definition C.1, there exists a partition P: [m] = ∪jm=*0Ij with I0 = ∪ir0=1I0,r, where Ij 6= 0
for any j ∈ [m*] and ‰r = 0 for any r ∈ [io] (if Io = 0), such that (^, W) ∈ MP, which gives
4That is, the non-degenerate case. Obviously, Io = 0 implies an uncountable MP, but they are all
degenerate.
5The result follows from basic knowledge of combinatorics. See details in the proof of Theorem D.1.
23
Published as a conference paper at ICLR 2021
that 52i∈ι7- ^i = aj, Wi = w； for any i ∈ Ij and j ∈ [m*]; ∑i∈ι0 r ai = 0, Wi = Vr= wj for any
i ∈ Io,r, r ∈ [io] and j ∈ [mj]. Therefore, for any n ∈ N+, We have
m
ai
=(Wk + Wi)
,	- - *	- - *
io	欠	m	+	m
ai	ai
(Wk + Wi)n +与 J (Wk + Wi)n
r=1 i∈I0,r	j=1 i∈Ij	j=1
；
a
(Wk + Wj)n
i0 P a m*	P	a m*	；
X 乙i∈I0,r ai	+ X	乙i∈Ij	ai	- X	aj	= 0
r=ι (Wk+Vr)n j=ι (Wk+Wjyn j=ι (Wk+Wj)n	.
(90)
This yields
J (a,W)= ʤ(Wk),	J @W) = 2&k 'B(Wk),
and hence
m
kVJω (^,W)k2 = 4 X [∆0,ω (Wk ) + ak∆2,ω (Wk)].
k=1
ByLemmaC.1, Wegetlimω→o+ ∣∣VJω (a,W)∣∣2 = 0.
Ifρ0 has the sub-Gaussian tail (79), again by Lemma C.1, the estimate
∆n,ω (Wk) ≤ ∆n,ω (Wmin) . S-ne-wm^ 卜Wmin + Cwmin)	(91)
holds for any n ∈ N, ω ∈ (0, min{1∕2,1/to, 2cι∕t^mi∏}) and k ∈ [m]. Here t^mi∏ := mini∈[m] Wi >
0, c2, c3 > 1 are constants only related to c1, t0, and . hides universal constants only depending on
n and ρ0, t0, c0, c1. Therefore
kVJω (a,W)k2 . √mωTe-wm^ω kwmin + Cwmin) (1 + kak∞), ω ∈ (0,1].
The proof is completed.	□
(3)	Eigenvalues of Hessian
NoW We shoW that minimal eigenvalues of V2Jω also converges to 0 as ω → 0+.
Proposition C.3. For any (a, w) ∈ Rm X Rm satisfying p(t; a, w) ≡ p(t), denote the eigenvalues
of V2 Jω(a, w) by λ1(ω) ≥ λ2(ω) ≥ ∙∙∙ ≥ λ2m(ω). If m > mj, we have
λk(ω)	>	0,	k =1, 2,∙∙∙,m0,	(92)
lim λk(ω)	=	0,	k = m0 + 1,m0 +	2,…，2m	(93)
ω→0+
for ω > 0 sufficiently small, where m0 ≤ 2mj + |I0 | ≤ m + mj . In particular, if ρ0 has the
sub-Gaussian tail (79), the estimate
∣λk(ω)∣ . ω-2e-wmin/ω kw2in + Cwm, (1 + ∣∣ak∞) k = m0 + 1,m0 + 2,…，2m (94)
holds for any ω ∈ (0, min{1/2, 1/t0, 2C1/Wmin}). Here Wmin := mini∈[m] Wi > 0, C2, C3 > 1 are
constants only related to C1, t0, and . hides universal constants only depending on ρ0, t0, C0, C1.
24
Published as a conference paper at ICLR 2021
Proof. A straightforward computation shows, for k,j = 1, 2, ∙∙∙ ,m,
序 J (aW = —2—.
∂ak∂aj	Wk + Wj
aθ⅛GWW=(WuWj)2，k=j，
/J- (a" -2
∂ak ∂Wk
m
X
i=1
(Wk + Wiy2
--*
m
X
j0=i
*
a3'
(Wk + W*z )2
-2Wk2 + 2A,ω (Wk),
∂2Jω (	)_	4αkθj
∂Wk ∂Wj °'w	(Wk + Wj )3，
k = j,
∂2Jω
∂Wk ∂Wk
(a, w) = 4αk E
Qi
i=1
(Wk + Wi)3
*
m
X
j0=i
*
aj'
(Wk + W*,)3
a2
+ 2姬3 - 2αkδ2,w (Wk).
(95)
(96)
(97)
(98)
(99)
电
—
m
—
Fix any (Q, w^) ∈ Rm × Rm satisfying p(t; Q, w^) ≡ p(t). By (90), We have
存 J (aw) = —2—.
∂αk ∂aj	Wk + Wj
:J (a, W) =「 jaj F (k = j),
∂ak∂Wj	(Wk + Wj )2
邪儿(…4	4ak aj	n ..、
∂Wk∂Wj(% W)=(Wk+ Wj )3 (k = j
a 2jω /ʌ ʌ,ʌ
∂ak∂wk	,
-2W + 2δ1,3 (Wk ),
∂2 J	a2
M @汕=弱-2ak" M
Let
Ja,w) ：= llρ(t;a, w) - 0(力臣[0产)	(100)
*	2
m	m
=X aiβ-Wit - X aje-Wt	,
and
Eω ("「Dian；： (w))
Diag(∆ι,ω (w))
-Diag(a 0 ∆2,ω(w))
where ∆n,ω (w) (n = 1, 2) is performed element-wisely. One can verify that
V2 Jω(a, w) = V2,7(a, w) + 2Eω(a, w).	(101)
Then we analyze V2J(a, W) and Eω (^, W) respectively.
(i)	V2J(a, W). Obviously (a, W) is a global minimizer of J(a, w) due to J(a, W) = 0. Hence
VJ(a, W) = 0 and V2 J(a, W) is positive semi-definite. We further show that V27(a, W) has multi-
ple zero eigenvalues when m > m*. In fact, since
∂2J ,ʌ 八	2	∂2J ,ʌ 八 -2^j	∂2J ,ʌ 八 4akaj
∂ak∂aj’(0, W) = Wk + Wj , ∂ak∂wj a W)= (Wk + Wj)2 , ∂wk∂Wj (a, W)	(Wk + Wj)3
it is straightforward to verify that for any i, j ∈ Ip, p ∈ [m*] and any i, j ∈ ∑0,r, r ∈ [i0],
V2J(a,w)i，: = V2J (^,w)j∖ ^j ∙ V2J (a,w)m+i，: = a-V2J(a,w)m+j，:,
where Ai,: denotes the i-th row of matrix A. Notice that Pi∈ι0 V2 J(a, W)m+i,: = 0 for any
r ∈ [i0], we conclude that the Hessian V2J(a, W) has at most 2m* + i0 + i2 ≤ 2m* + |Z0| ≤
m + m* different rows, 6 which yields rank(V2 J(a, W)) ≤ 2m* + |Z0| ≤ m + m*. Therefore,
6Here i2 := |{r ∈ [i0] : ∣Z0,r| ≥ 2}|. When I0 = 0, the upper bound is 2m*; when I0 = 0, since
Z0,r = 0 for	any r ∈	[i0], let iɪ	:= ∣{r	∈ [i0] : ∣I0,r∣ = 1}∣ and i2 defined	as before. Then i0	= iɪ + i2,
∣I0∣ = Pr=1	区y∣ ≥	iι +2i2 =	i0 + i2.	The last inequality follows from ∣I0∣	= m — Pm=IIZjI	≤ m — m*.
25
Published as a conference paper at ICLR 2021
the number of zero eigenvalues of V2e7(a, W) ≥ dim {x ∈ R2m : V2e7(^, W) ∙ x = 0} = 2m —
rank(V2e7(^, W)) ≥ 2(m — m*) — |Io| ≥ m — m*. Since V2tj(a, W) is positive semi-definite, all
the non-zero eigenvalues must be positive.
(ii)	Eω(a, W).Let
Gk1) := {y ∈ R : ∣y∣ ≤ A,ω(Wk)∣},
GF = {y ∈ R ： |y + ak∆2,ω(Wk)| ≤ ∣∆1,ω(Wk)∣}.
By GerShgorin's circle theorem, for any eigenvalue of Eω(^,W), Say λ(ω), We have λ(ω) ∈
Skm=1(G(k1) ∪ G(k2)). Combining with Lemma C.1, we get
∣λ(ω)∣ ≤ max (∣^k∣∣∆2,ω(Wk)| + ∣∆1,ω(Wk)|) → 0, ω → 0+.	(102)
k∈[m]
If ρ0 has the sub-Gaussian tail (79), by (91), We further have
∣λ(ω)∣ ≤ maχ (∣^k∣∣∆2,ω(Wk)| + ∣∆1,ω(Wk)|)
k∈[m]
.ω-2e-wmiMω 卜豹n + Cwmin)(1 + ∣∣^k∞),	ω ∈ (0, 1],	(103)
where ω ∈ (0, min{1∕2,1/t0, 2c1∕Wmin}), Wmin ：= mi□i∈m] Wi > 0, c2, c3 > 1 are constants only
related to c1, t0, and . hides universal constants only depending on ρ0, t0, c0, c1.
Combining (i), (ii) and applying WeyrS theorem gives the desired result.	□
(4) Local Linearization Analysis
The previous analysis can now be tied directly to a quantitative dynamics via linearization argu-
ments. It is shown that under mild assumptions, the gradient flow (21) can become trapped in
plateaus with an exponentially large timescale. That is, the curse of memory occurs, this time in
optimization dynamics instead of approximation rates.
Theorem C.1 (Restatement of Theorem 5.1). For any ω > 0, m ∈ N+ and θ0 = (a0, W0) ∈
Rm × R+m, 0 < δ	1, define the hitting time
τ0 =	τ0(δ; ω, m, θ0) := inf {τ	≥ 0 :	∣θω(τ) — θ0∣2 > δ} ,	(104)
τ0 =	τ0(δ; ω, m, θo):= inf {τ	≥ 0:	Jω(θω(T)) — J(θo)∣	> δ} .	(105)
Assume that m > m*, and the initialization satisfies p(t; θ0) ≈ p(t). Then we have
lim τ0(δ; ω, m, θ0) = lim τ00 (δ; ω, m, θ0) = +∞.	(106)
ω→0+	ω→0+
In particular, if ρ0 has the sub-Gaussian tail (79), and the initialization is bounded as (a0, W0) ∈
[al0 , ar0]m × [Wl0 , Wr0]m with constants al0 < ar0, 0 < Wl0 < Wr0, we further have
τ0(δ; ω,m,θo) ≥ To(δ; ω, m, θo) & ω2ew0ω min < ^=,ln(1 + δ) ∖	(107)
for any ω ∈ (0, min{1/2, 1/t0, 2c1/Wr0}) sufficiently small, where & hides universal constants only
depending on ρ0, t0, c0, c1, Wr0, al0 and ar0.
Proof. Consider the asymptotic expansion with the form
∞
θω(T)=θω0(T)+Xδiθωi(T)=θω0(T)+δθω1(T)+δ2θω2(T)+o(δ2),	(108)
i=1
for some δ ∈ (0,1) (with δ《1) and θω (τ) = O(1) (τ ≥ 0, i = 0,1,…).7 For consistency, we
have θ00(0) = θo and θ∖(0) = 0 for i = 1,2, ∙ ∙ ∙. By continuity, τo > 0 and ∣∣θω(τ) — θ0∣∣2 ≤ δ for
any T ∈ [0, T0]. The aim is to quantify the scale ofT0.
7Here θωi (τ) denotes the i-th term in the asymptotic expansion of θω (τ), not the i-th power.
26
Published as a conference paper at ICLR 2021
Let go := VJω (θo) and Ho := V2 Jω (θo). The local linearization on (21) shows
Μθω (T) = -g0 - H0(θω (T) - θO) + O(δ2),
dτ
Combining with (108), we have
-	dθω(τ) = -go -小耽(τ) - θo),	θω(0) = θo,
dT
-	dθω(τ) = -Hoθω(τ),	θω(0)=0,
dT
-	dθω(τ) = -Hoθω(τ)+o(i), θω(0)=0,
dT
Therefore
τ ∈ [0, τo].
at O(1) scale,
at O(δ) scale,
at O(δ2) scale.
e-H0sds go ,
which gives
θω(τ) = θo
e-H0sds go + O(δ2 ), T ∈ [0, To].
(109)
To achieve a parameter separation gap δo, i.e. kθω (τ) - θok2 = δo with δo = cδ, c ∈ (0, 1], we need
to take τ such that
e-H0sds go
2 ≥ δ20.
(110)
Let Ho = P>ΛP be the eigenvalue decomposition with P orthogonal and Λ = diag(λι, •…λ2m)
consisting of the eigenvalues of Ho with λι ≥ ∙∙∙ ≥ λ2m. Then
e-H0sds go
P>	τ e-Λsds Pgo
≤ ∣∣goI∣2 ∙ max V max
i∈[2m],λi6=o
≤
2
ι⅛le
Z τ e-Λs
o
ds kgo k2
2
-λiτ - 1ι, τ
2
It is straightforward to verify that h(τ; λ):=击∣e-λτ — 1∣, T ≥ 0 monotonically decreases on
λ ∈ R for any τ ≥ 0. 8 Hence
e-H0sds go
≤ ∣g01∣2 ∙ ʃ -⅛m(e λ2mτ -1), λ2m < 0,
2 ≤ kgok2 I T,	λ2m ≥0,
and the right hand side monotonically increases on T ≥ 0. Combining (110), (111) gives
δo ≤∣go∣2 ∙｛卡(e-λ2mτ -1), λ2m < 0,
(111)
(112)
We discuss for different cases:
(i)	∣go∣2 = 0. Obviously the inequality (112) fails since (110) fails for any T ≥ 0, which gives
To = +∞;
(ii)	kgo∣2 = 0 and λ2m ≥ 0. By (112), we get T ≥ 2k^;
(iii)	∣go∣2 6= 0 and λ2m < 0. By (112), we get
T ≥-λm ln(1+δo 瑞
8With the convention that h(τ ; 0) = τ for any τ > 0, and h(0; λ) ≡ 0 for any λ ∈ R.
27
Published as a conference paper at ICLR 2021
If -λ2m ≤ 2kg0k2, we have
1
T ≥
-λ2m
δ0
-λ2m
02kg0k2
2kg0k2
if -λ2m > 2kg0k2, we have τ ≥
Combining (i), (ii), (iii) gives
1 + O δ0
ln(1+δo)
-λ2m
ln0 + <⅜2⅛)
δ0⅛⅛
-rmr	UOir (1 + O(δ0));
2kg0k2	2kg0k2
τ0 = τ0(δ; ω,m,θ0) & min{京,ln(1+r
(113)
Let the initialization satisfy p(t; θ0) ≡ p(t), and assume m > m*. According to Propositions C.2
and C.3, we have
lim kg0 k2 = 0, lim λ2m = 0 ⇒ lim τ0 (δ; ω, m, θ0) = +∞.
ω→0+	ω→0+	ω→0+
Ifρ0 has the sub-Gaussian tail (79), again by Propositions C.2 and C.3, we further have
(114)
τo(δ; ω, m, θ0) & ω2ew0,mi"ω min
, ln(1 + δ)
1
,min (1+ ka0k∞)
for any ω ∈ (0, min{1/2, 1/t0, 2c1/w0,min}), where w0,min := mini∈[m] w0,i > 0, c2, c3
(115)
> 1 are
constants only related to c1, t0, and & hides universal constants only depending on ρ0, t0, c0, c1.
Since the initialization is bounded as (a0, w0) ∈ [al0, ar0]m × [wl0, wr0]m with al0 < ar0, 0 < wl0 < wr0,
let c0a = max{|al0|, |ar0|}, we get
τo(δ; ω,m, θο) & ω2ew% min
, ln(1 + δ)
1
& ω2ew% min
,ln(1+δ) ,
(116)
where & hides universal constants only related to wr0, al0 and ar0.
The last task is to show the dynamics of the loss is much slower than the parameter separation when
there is plateauing. The argument is trivial since for any τ ∈ [0, τ0],
Jω(θω(τ))- Jω(θ0) = g0>(θω(τ) - θ0) + (θω(τ) - θ0)>H0(θω(τ) - θ0) + o(δ2)
≥ -kg0 k2 kθω (τ)- θ0 k2 + λ2mkθω (τ) - θ0k22 +o(δ2)
= o(1)O(δ) + o(1)O(δ2) + o(δ2)
= o(δ2 ),	ω → 0+ .
By continuity, the proof is completed.	口
Remark C.3. The estimate in Theorem C.1 shows a lower bound on the escape time, hence it does
not appear to preclude the situation that the plateauing lasts forever. However, in the proof above,
if one supposes τ0 = +∞ in (104), i.e. the hypothetical situation where the parameters are trapped
forever, and write go := Pgo = (g0,1,…，g0,2m), we have
2	τ	2	2m
=g0 U e-Asds) go = χ(go,i)2(h(τ; λi))2 ≥ (go,j)2(h(τ; λj))2
for any j such that λj < 0. If go,j = 0, (109) gives
τ e-H0sds go	+ O(δ2)
2
≥ lg0,jl(e-λT - 1) + O(δ2) → +∞,	τ →∞,
-λj
which is a contradiction. That is to say, the parameter separation has to achieve the gap δ within a
finite time, even if it is exponentially large.
kθω(τ)-θok2≥
o
28
Published as a conference paper at ICLR 2021
Remark C.4. Recall Lemma C.3, ρ(t; a0,w0) ≡ p(t) if and only if (a0,w0) ∈ M* = UP MP,
where P is a partition over [m] as defined in Definition C.1. That is, as a union of affine spaces,
M* is in fact an equivalent Setfor qualified initializations. As discussed in Remark C.2, when there
is no degeneracy, the CaadimMty of M "S m*! {mJ® the Inumbe ofP λ with each MP an
(m - m*)-dimensional affine space; when there is degeneracy in some M*P, it then becoms an
uncountable set. Certainly, initializations sufficiently near M* are also qualified by continuity.
Remark C.5. Motivated by the idea of weights degeneracy (see Definition C.1), we can further
apply similar methods to a (global) landscape analysis on the loss function Jω. The results there
show that the plateaus are all over the landscape, even provided general targets (without memory
structures). See details in Appendix D.
C.3 Numerical Experiments
C.3. 1 Motivating Tests
In this section we give details of Figure 1.
(1)	Learning linear functionals using linear RNNs (with GD optimizer)
The target functional is HT (x) = R0T ρ(t)xT-tdt with white noise x, while the representation ρ is
selected as the exponential sum or the scaled Airy function:
1.	Exponential sum: ρ(t) = [c*]>ew*tb*, where c*,b* are standard normal random vectors
of m* dimensions, and W = —I 一 Z>Z with Z ∈ Rm* ×m* is a Gaussian random matrix
with i.i.d. entries having variance 1/m*.
2.	Airy function: ρ(t) = Ai(s0[t — t0]), where Ai(t) is the Airy function of the first kind,
given by the improper integral
Ai(t)
ɪ lim Z cos uu— + tu) du.
π ξ→∞ 0	3
(117)
Note that in the first example, the memory of target functional decays quickly. However, for the
second example, the effective rate of decay is controlled by the parameter t0. For t ≤ t0, the Airy
function is oscillatory. Hence for large t0, a large amount of memory is present in the target. In
Figure 1 we set m* = 8 for exponential sums, and t0 = 3, s0 = 2.25 for Airy functions.
In Figure 1 (a) and (b), we plot the gradient descent dynamics on training the linear RNNs (dis-
cretized using Euler method, hence equivalent to residual RNNs). We observe an efficient training
process for the exponential sum case, while “plateauing” behaviors in the Airy function case. This
causes a severe slow down of training. In addition, we also find that the plateauing effect gets
worse as t0 (or s0) is increased, which corresponds to more complex Airy functions in the sense of
more memory effects. That is, the long-term memory adversely affects the optimization process via
gradient descent.
(2)	Learning nonlinear functionals using nonlinear RNNs (with Adam optimizer)
To show that the plateauing behavior may be generic, we also consider a nonlinear forced dynamical
system target, the Lorenz 96 system (Lorenz, 1996):
K
y = —y + X + £zk/K,
k=1
Zk = 2[zk+1(zk-i — Zk+2) — Zk + y], k =1,…，K,
(118)
with cyclic indices Zk+K = Zk, and x is an external stochastic noise. When the unresolved variables
Zk are unknown, the dynamics of the resolved variable y driven by x is a nonlinear dynamical system
with memory effects (but not a linear functional). We use a standard nonlinear RNN (with the tanh
activation) to learn the sequence-to-sequence mapping x0:T 7→ y0:T with the Adam optimizer.
Figure 1 (c) shows that the training of the Lorenz 96 system with the presence of memory also
exhibits the plateauing phenomenon.
29
Published as a conference paper at ICLR 2021
l∕ω=l 1
l∕ω=l2
10°
10°
≡ 10°
IoT
IoT
≡ 10^1
10°-
10。二
10° 二
≡10^2
10°：
l∕ω=10
---Jm
10^2
10^2
10°-
10°
IOT
10^2
IoT
IWmiI2
Figure 2: The training dynamics of the target functional defined by (119) using the model (120).
Here We take the width m = 2 in ρ. The corresponding gradient flow (21) is numerically solved
by the Adams-Bashforth-Moulton method. Observe that the plateauing time increases rapidly as the
memory becomes longer (ω decreases).
l∕ω=13
----Jm
l∕ω=14
10°
IoT
10^2
10°
MmlI2
l∕ω=15
---Jm ' IMmIl2

In all cases, the trained RNN model has a hidden dimension of 16 and the total length of the path is
T = 6.4. The continuous-time RNN is discretized using the Euler method with step size 0.1.
C.3.2 Long-term Memory Significantly Contributes to Plateaus
Recall the simple example of target with memory
*	(t — 1∕ω)2
ρ(t) = a*e-w t + coe--2σ^,	w* > 0.	(119)
We aim to learn ρ with the exponential sum
m
ρm(t; a,w) = X aie-wit,	a ∈ Rm, W ∈ Rm.	(120)
i=1
That is, the simplified linear RNN model with a diagonal recurrent kernel (see (76) and Ap-
pendix C.1). The optimization is performed by the gradient flow (21), i.e. a continuous-time
idealization of the gradient descent dynamics. The ODE (21) is numerically solved by the Adams-
Bashforth-Moulton method. The results are illustrated in Figure 2. It is shown that the plateauing
time increases rapidly as the memory 1 /ω becomes longer.
C.3.3 Numerical Verifications
(1)	The timescale estimate
We first numerically verify the timescale proved in Theorem 5.1 (or Theorem C.1). That is, the
time of plateauing (and also parameter separation kθ(τ) - θ(0)k2) is exponentially large as the
memory 1∕ω → +∞. The results are shown in Figure 3, where we observe good agreement with
the predicted scaling.
(2)	General cases
To facilitate mathematical tractability, the analysis so far is done on the restrictive cases of the di-
agonal recurrent kernel W with negative entries, linear activations and the gradient flow training
dynamics. However, we show here that the plateauing behavior - which we now understood as a
generic feature of long-term memory of the target functional and its interaction with the optimiza-
tion dynamics - is present even for general cases, and hence our simplified analytical setting is
representative of the general situation.
In Figure 4, we still take the target functional as defined in (119), but apply more general models
to learn it, including using full (non-diagonal) recurrent kernels of the RNN with no restrictions on
30
Published as a conference paper at ICLR 2021
Figure 3: The timescale of plateauing and parameter separation. Here the model and target are both
selected the same as Figure 2, but with a larger width m = 10. We see the logarithm of time of
PlateaUing and parameter separation is almost linear to the memory 1∕ω.
Parameter separation time
entries, using non-linear (tanh) activations and using the Adam optimizer. Furthermore, we do not
take the Ito isometry simplification and instead use actual input sample paths of finite time horizons,
jUst as one woUld do in training RNNs in practice. We observe that the plateaUing behavior is
present in all cases. Moreover, in the last case of Adam (which can be thought of as a momentum-
based optimization method), the plateauing behavior is somewhat alleviated, although the separation
of timescales is still present. This is consistent with our supplemental analysis in Appendix E,
where we show that the momentum-based methods will speed up training based on our dynamical
(Appendix C.2) and landscape analysis (Appendix D) of plateauing.
epoch	epoch	epoch
S
∖Λ
3 10-2
(a) White noise inputs
(b) Cosine noise inputs
Figure 4: Numerical verifications of the plateauing behavior under general settings, with non-
diagonal recurrent kernels, the non-linear activation (tanh), and the Adam (momentum-based) op-
timizer. Here We use the target functional the same as Figure 2 with the memory 1∕ω = 20. The
time horizon is chosen as T = 32, and 128 input samples are generated from a standard white noise.
The learning rate is 1.0 for GD and 0.001 for Adam. 10 initializations are sampled and trained
for each experiment. We consider two possible input distributions: a) white noise inputs; b) inputs
of the form Xt = Pj=ι αj cos(λjt), where λj 〜U[0,10] and ɑj 〜N(0,1). We observe that
plateaus occur in all cases and the momentum generally improves the situation but still not resolve
the difficulty.

31
Published as a conference paper at ICLR 2021
min	Jm(a, w) :=
m ,w∈R+m	0
D	Landscape Analysis
As mentioned in Remark C.5, we can perform a global landscape analysis on the loss function based
on the idea of weights degeneracy, which arises from Definition C.1. Recall that the loss function
reads
∞ X aie-wit - ρ(t)	dt.	(121)
The main results of the appendix are summarized as follows.
•	In Theorem D.1, we prove that the loss function has infinitely many critical points, which
form a factorial number of affine spaces;
•	In Theorem D.2, we prove that such (critical) affine spaces are much more than global
minimizers provided the target being an exponential sum; 9
•	In Theorem D.3, we prove that on such (critical) affine spaces, the Hessian is singular in
the sense of processing multiple zero eigenvalues;
•	In Proposition D.1, we prove that the (critical) affine spaces contain both saddles and de-
generate stable points which are not global optimal.
Instead of a local dynamical analysis in Appendix C.2, we generalize similar methods to a global
landscape analysis here, and the results hold for the loss function associated with general targets.
More specifically, these results complement our main results (see Theorem 5.1 or Theorem C.1) in
the following aspects.
•	It is shown that the weights degeneracy is quite common in the whole landscape of the loss
function. Unfortunately, the weights degeneracy often worsens the landscape to a large
extent;
•	It is shown that the weights degeneracy leads to a large number of stable areas (i.e. critical
affine spaces), but most of them contribute to non-global minimizers;
•	It is shown that these stable areas can also be quite flat, which often connect with local
plateaus;
•	For the structure of these stable areas, there are both saddles and degenerate critical points
(not global optimal). In certain regimes, even saddles can be rather difficult to escape from
(see Theorem 5.1 or Theorem C.1).
As a consequence, the optimization problem of linear RNNs is globally and essentially difficult to
solve.
D. 1 S ymmetry Analysis on the Landscape
This subsection consists of two parts: in Appendix D.1.1, we give main results provided the ex-
istence of weights degeneracy; in Appendix D.1.2, we give sufficient conditions to guarantee the
existence. Since the key observation to utilize weights degeneracy is to notice the permutation sym-
metry of coordinates of gradients, we also called it “symmetry analysis”.
D.1.1 Generic Theory
We begin with the following definition, which describes the concept of weights degeneracy in a
natural and rigorous way.
Definition D.1. (coincided critical solutions and affine spaces) Let d ∈ N+ and 1 ≤ d ≤ m. We
say (a, W) is a d-coincided critical solution of Jm, if NJm (a, W) = 0, and W = (Wi) ∈ Rm has d
different components. The coincided critical affine spaces are defined as coincided critical solutions
that form affine spaces.
9The global minimizers are distinct when the target is an exponential sum. Here we compare the number of
(critical) affine spaces with the number of global minimizers (both of them are finite). When the target is not an
exponential sum, the same conclusion holds if there are still finite number of global minimizers. See Remark
D.3 in Appendix D.1.1 for details.
32
Published as a conference paper at ICLR 2021
To guarantee the existence of such solutions, it is necessary to have the following definition.
Definition D.2. (^,W) ∈ Rm 0 Rm is called the non-degenerate global minimizer of Jm, if and
only if
Jm(^,W)=	inf	Jm(a,w),	(122)
a∈Rm,w∈R+m
and (a, W) takes a non-degenerate form
ai = 0, Wi = Wjfor i = j, i,j = 1, 2,…，m.	(123)
For convenience, we also define an index set
N := {n ∈ N+ : Jm has non - degenerate global minimizers for any m ≤ n} ,	(124)
which is used frequently in the following analysis. For any f ∈ L2 [0, ∞), let L[f] be the Laplace
transform of f, i.e. L[f](s) = R0∞ e-stf (t)dt, s > 0. We begin with the following lemma.
Lemma D.1. Assume that P is smooth and √W ∣L[ρ](w)∣ → 0 as W → 0+ and W → ∞. Then we
have 1 ∈ N and thus N= 0.
2W ∙ a2 - 2L[p](W) ∙a + kρkL2[0,∞)
Proof. We aim to show that there exists ^ = 0 and W > 0, such that
Jι(a,W) =	inf	Jι(a,W).	(125)
a∈R,w∈R+
The basic idea is to limit the unbounded domain a ∈ R, W ∈ R+ to a compact set without effecting
the minimization of J1(a, W). We have
min J1 (a, W) = min min
a,w>0	w>0 a
=mnmin(2W(a- 2wL[p](W))2 + hkρkL2[0,∞) - 2W(L[P](W))2
= mw>in0	kρk2L2[0,∞) - 2W(L[ρ](W))2 = J1(a(W), W),
where a(W) := 2WL[ρ](W). Write h(W) := J1(a(W), W), then h(0+) = h(∞) = kρk2L2[0,∞).
Obviously h(W) < kρk2L2[0,∞) for any W > 0, hence
min h(W) = min	h(W), 0 < Wlb < Wub < ∞,
w>0	w∈[wlb,wub]
which implies
min J1 (a, W) = min J1 (a(W), W ) = min	J1 (a(W), W).
a,w>0	w>0	w∈[wlb,wub]
That is to say, the minimization of J1 (a, W) can be equivalently performed on a 2-dimensional
smooth curve
(W, a(W))w∈[wlb,wub], which is certainly a compact set. By continuity, J1(a, W) has global minimiz-
ers, say (a, W). Obviously W > 0 and ^ = a(W) = 0 (since a = 0 implies Jι(a, w) = IlPkL2田 ∞),
certainly not a minimum), which completes the proof.	□
,,	__2, *	一—* 上
Remark D.1. Ifthe target is an exponential sum, i.e. ρ(t) = Ej=I a；e- j , we know P is smooth
and √W ∣L[p](w)∣ → 0 as W → 0+ and W → +∞; hence 1 ∈ N by Lemma D.1, and thus
m* a*
N = 0. In fact, L[ρ](w) = Ej=I wj* implies that L[ρ](w) = O(1) when W → 0+, and
L[P](W) = O(1/W) when W → +∞.
Theorem D.1. Assume that N 6= 0 with N defined as (124). Let M := sup N. Then for any
m
m, d ∈ N+, 1 ≤ d ≤ min{m, M}, there exists at least d! d d-coincided critical affine spaces
of Jm, 10 where md	∈ N+ is called the Stirling number of the second kind.
10The affine spaces degenerate to distinct points when d = m. For sufficient conditions to guarantee M > 1
(to avoid vacuous results), see Theorem D.6 and Remark D.7 in Appendix D.1.2.
33
Published as a conference paper at ICLR 2021
Proof. (i) Existence. The key observation is the permutation symmetry of VJm： by (19), if ai = aj
and Wi = Wjfor Some i = j, then J = Jand J = J.
For any m, d ∈ N+, 1 ≤ d ≤ m, suppose that W = (Wi) ∈ R+m has d different components. Then
for any partition P: {1,…，m} = ∪d=1Ij- with IjI Cj = 0 for any j1 = j2, j1,j2 = 1,…，d,
define the affine space
MP,(b,v),(m,d) := {	(a,	W) ∈	Rm	0 R+	: Wi	=	Vjfor any i ∈ Zj,	X :	ai	= bj,	j = 1,…。，d )
I	i∈Ij
for some (b, v) ∈ Rd 0 Rd+, where v has exactly d different components. Therefore, for any (a, W) ∈
MP,(b,v),(m,d), we have
2
d
Jm (a, W)
aie-wit - ρ(t)
j=1 i∈Ij	L2 [0,∞)
d
X bje-vjt - ρ(t)
j=1
2
= Jd(b, v),
L2[0,∞)
and similarly
∂Jm(a,W) = 2 f ∞ e-vst
∂ak	0
X bj e-vjt — ρ(t)) dt, k ∈Zs, S = 1, 2,…，d,
∂	(a, W) = 2akZo∞(-t)e-vst (XX bje-vjt - ρ(t) dt, k ∈ Is, s = 1, 2,…，d.
Notice that
dJd (b,v) = 2/ e-vst (X bje-Vj-P(t) dt,	S = 1, 2,…，d,
dJd (b,v)=2bs L
we have
∂Jm	∂ Jd
而T (a,W) =匹(b,v),
∞
(-t)e-vst J X bje-vjt - ρ(t) ) dt, s = 1, 2,…，d,
bs Λ (a (a, W)= ak 分 d (b, V) ,	k ∈ Is, s = 1, 2, ∙∙∙ ,d. (126)
∂Wk	∂vs
11 Since d ≤ min{m, M }, d ∈ N. In fact, for any k ∈ N+, if k ∈/ N , there exists i ≤ k
such that J(i) has no non-degenerate global minimizers, we have j ∈/ N for any j ≥ i, hence
M ≤ i - 1 ≤ k - 1. Hence M = ∞ implies N = N+ and M < ∞ implies M ∈ N, and
both of them lead to d ∈ N. Therefore, Jd has non-degenerate global minimizers, i.e. there exists
(b, V) ∈ Rd 0 r+ such that
Jd(b,v) =	inf	Jd (b,v),
b∈Rd,V∈Rd+
(127)
and (b, V) takes a non-degenerate form
O , _	.	, . ʃ.	,	-	. .	.
bi = 0, Vi = Vjfor any i = j, i,j = 1, 2,…，d.	(128)
By (127), We get VJd(b, V) = 0. Combining with (126) and (128), We obtain V Jm(a, w) = 0 for
any (a, W) ∈ MP (^ V)(m &), i.e. (a, W) belongs to a d-coincided critical affine space. Note that
the affine space is with the dimension Pjd=1 (|Ij| - 1) = m - d, since there are d linear equality
constrains on the m-dimensional vector a.
11By considering the gradient flow dynamic of Jd instead of Jm , a model reduction (from m-dimensional to
d-dimensional) is almost completed on MP,(b,v),(m,d), except for some trivial degenerate cases (e.g. ak = 0
or bs = 0).
34
Published as a conference paper at ICLR 2021
(ii)	Counting. By the structure of affine spaces discussed above, we can identify different affine
spaces with respect to the partition P . For counting the number of different partitions P :
{1, ∙∙∙ ,m} = ∪d=1Ij-, it can be decomposed into the following two steps. First, partitioning a
set of m labelled objects into d nonempty unlabelled subsets. By definition, the answer is the Stir-
m
ling number of the second kind J “ 卜 Second, assign each partition to Ii,…，Zd accordingly.
There are d! ways in total. Therefore, the number of d-coincided critical affine spaces is at least
m
d! d . The proof is completed.
□
Combining Lemma D.1, Remark D.1 and Theorem D.1 gives the following theorem, which states
that there are much more saddles and degenerate stable points which are not global optimal than
global minimizers in the landscape (provided the target being an exponential sum).
Theorem D.2. Fix any m ∈ N+ relatively large. Consider the loss Jm associated with the target
being a non-degenerate exponential sum, i.e. ρ(t) = Em=I aje~wjt, where aj = 0 and w* = Wj
for any i = j, i,j = 1, ∙∙∙ ,m. Assume that m ∈ N 12 with N defined in (124). Then in the
landscape of Jm , the number of coincided critical affine spaces is at least P oly(m) times larger
than the number of global minimizers.
Proof. (i) Global minimizers. Since the target is an exponential sum, we have Jm (a, w) ≥ 0 and
Jm (a* ,W*) = 0, where a* = Pa* and W* = Pw* with P ∈ Rm×m to be some permutation matrix.
Next we show Jm has no other global minimizers.
Suppose Jm (a, w) = 0, we have
mm
X aie-wit - X aje-wjt = 0, t ≥ 0.	(129)
i=i	j=i
It is easy to see that for any j = 1,…,m, there exists i(j) such that Wj = wj. Otherwise,
if Wi = w*, i = 1, ∙∙∙ ,m, by (83) or Lemma C.2, we have aj = 0, which is a contradiction.
Notice that wi* 6= wj* for any i 6= j, different wj* ’s will correspond to different wi ’s, hence the
correspondence is one-to-one. Therefore, let wi = wj*(i), (129) can be rewritten as
mm	m
0 = X aie-%)t - X aj(i)eFi)t = £@ — 味步一切"，t ≥ 0.
i=i	i=i	i=i
Again by Lemma C.2, we have ai = aj*(i). That is to say, Jm (a, w) = 0 implies a = Pa* and
w = Pw* with P ∈ Rm×m to be some permutation matrix. This gives m! global minimizers.
(ii)	Coincided critical affine spaces. Obviously N = 0, and M = SuP N ≥ m. According to
m
Theorem D.1, for any d ∈ N+, 1 ≤ d ≤ min{m, M} = m, we have at least d! d d-coincided
critical affine spaces of Jm . By (i), for any d ≤ m - 1, there are no global minimizers in these affine
spaces. Counting the total number
m-i
X d! md .	(130)
d=i
(iii	) Comparison. To give a bound between (130) and m!, we need an elementary recurrence
md=dmd-1+md--11.
• For d = m - 1, let pm := mm- 1 , then
pm	= (m-1)	mm--11	+	mm--12	=	(m-1)+pm-i
m(m - 1)
2
12Although the assumption m ∈ N seems strong, we will provide sufficient conditions to guarantee its
validity in Appendix D.1.2. See an complement in Theorem D.7.
35
Published as a conference paper at ICLR 2021
m
For d = m - 2, let qm := m - 2 , then
qm =	(m	- 2)	mm --	21	+	mm	-- 13	= (m - 2)pm-1 +	qm-1
=^j-[2(m — 2)(m — 1)(2m — 3) + 3(m — 2)2(m — 1)2].
Combining above gives
1	m 1	(m + 1)(3m — 2)
m ∑d！ι d > > m![(m —1)!pm + (m — 2)!qm] =	24	，
d=1
which is a quadratic polynominal on m. The proof is completed.	□
Remark D.2. We only take the last two terms of (130) for a lower bound, which is obviously rather
loose. In principle, a Poly(m) bound with higher degrees can be similarly obtained. That is to say,
on one hand, there are infinitely many critical points forming affine spaces in the landscape of Jm;
on the other hand, we see that even if only counting the number of affine spaces, there are still much
less global minimizers (provided the width m relatively large).
Remark D.3. When the target ρ is not an exponential sum, it is straightforward to see Theorem D.2
still holds if there is a finite number (with the scale of no more than factorial) of global minimizers.
Now We get down to investigate V2 Jm on the above coincided critical affine spaces. It is shown that
V2Jm is singular and can have multiple zero eigenvalues.
Theorem D.3. Fix any m, d ∈ N+, 1 ≤ d ≤ m. On the d-coincided critical affine spaces (induced
by non-degenerate global minimizers of Jd13) of Jm, V2Jm is with rank at most m + d, and hence
has at least m — d zero eigenvalues.
Proof. A straightforward computation shows that, for k,l = 1,2,…,m,
d2Jm (a w) = —2—.
∂ak ∂aι	Wk + wι
∂⅛Mw) = (W-⅛, k = l,
W—m— (a,W) =	k+2 Z (—t)e-wkt (xa aie-wi—P⑴]dt.
∂ak ∂wk	2wk2	0	i=1
(131)
(132)
(133)
Let the induced d-coincided critical affine space be MP (^ ^ (m G，as is derived in the proof of
EI	1 ʌ TC ♦	/ T^ ʌ ∖ ∙ .1	1	.	IFl ♦♦♦	i' T	1
Theorem D.1. Since (b, V) is the non-degenerate global mmιmizer of Jd, we have
(g(—t)e-wkt
0
Xm∞d
aie-wit — ρ(t)) dt = J	(—t)e-Vst (2灰e-vjt — P(t) i dt
-ɪ-t-d (b, V) = 0, k ∈ Is, s = 1, 2,…，d
2bs ∂vs
forany (a,w) ∈ MP,(b,v),(m,d). ThiS gives
d2Jm (a W) = —ak
∂ak∂wk (a,w)	2W2.
(134)
Now we show that, for any i,j ∈ Is, i = j, S = 1, 2,…，d, V2 Jm(a,W)i,： = V2Jmb(a,W)j,：. In
fact, for any k = 1,…，m,let k ∈ 工，,then by (131),
上Jm (a w) = ɪ 2 ɪ =
∂ai∂ak	Wi + Wk	Vs + ^
^Jm (a. W) = ɪ 2 ɪ = -ɪ-
∂aj ∂ak	Wj + Wk Vs + ^
13That is, the affine space MP g V)(切 〃).See details in the proof of Theorem D.1
36
Published as a conference paper at ICLR 2021
For k 6= i and k 6= j , (132) gives
d2 Jm	(a	W) _	-2ak	_	-2ak	d2 Jm	( a W) _	-2ak	_	-2αk
∂α%∂wja'w	(Wi + Wk)2	(Vs + Vs，)2 ,	∂aj∂wj(Wj + Wk)2	(Vs + Vs，)2
By(134),fork=i 6=j,
∂2 Jm	一f¾	_ — αi	∂2 Jm	—2<¾	_	_ — ɑi
∂αi∂Wk(%W)=			= 2W2	=	-. 2V2 ,	∂α>;(%W)=	(Wj + Wiy2	=	-. 2V2 ,
and similarly for k = j 6= i,
∂2 Jm	二	-2αj	二	二—αj	∂2 Jm	二一αj 二	二一αj
∂αi∂Wk( , ) =	(Wi + Wj )2	一场,	∂αj∙ ∂Wk ( , ) =	2W2	一场
That is to say, there are at most m + d different rows in the symmetric matrix V2 Jm(^, W) ∈
R2m×2m, hence rank(V2Jm(^,W)) ≤ m + d. Therefore, the number of zero eigenvalues of
V2 Jm(α, W) ≥ dim{x ∈ R2m : V2 Jm(^, W) ∙ X = 0} = 2m — rank(V2 Jm(α, W)) ≥ m — d. The
proof is completed.	□
Remark D.4. The bound in Theorem D.3 is not sharp, since the estimate on rank V2 Jm here is
loose as only rows with the same elements are considered. In practice (numerical tests), it is often
observed that there are more zero eigenvalues of rank V2Jm on the coincided critical affine space
MP ,6v),(m,d)・
Remark D.5. Theorem D.3 shows that, there are local plateaus around the d-coincided critical
affine spaces MP @ 右)⑺ &)for d ≤ m — L In addition, the 0-eigenspace of Jm is higher-
dimensional for smaller d, which may suggest that one can stuck on plateaus more easily.
D.1.2 Sufficient Conditions
There is still a gap when connecting Theorem D.1 and Theorem D.2. That is, it is necessary to
guarantee SuP N relatively large, i.e. Ji ,J2,…，Jd all have non-degenerate global minimizers for
d as large as possible. Motivated by Kammler (1979a), we can give some sufficient conditions by
limiting the target ρ within a smaller function space, the so-called completely monotonic functions.
Definition D.3. F ∈ C[0, ∞] ∩ C∞(0, ∞) is called completely monotonic, if and only if
(—1)nF (n)(t) ≥ 0, 0 <t< ∞, n = 0,1,…,
and F(∞) = 0.
Remark D.6. Several examples of completely monotonic functions:
•	ρ(t) = 1/(1 + t)α for any α > 0;
•	The non-degenerate exponential sum with positive coefficients
m*
ρ(t) = X α*e-w*t, 0 ≤ w； < …<Wm. *, αj > 0, j = 1, 2,…，m*.
j=1
Since the space of exponential sums is not close, we turn to consider the problem of finding a best
approximation to a given ρ ∈ L2 [0, ∞) from the set
Vd(R+) :=	{p	∈ Cd[0, ∞) :	[(D + Wi)…(D	+ Wd)]p = 0 for some	wi,…Wd	∈	R+}	(135)
with respect to the common L2-norm, i.e. infρ∈%(R+) kp—ρ∣∣L2[o,∞), where D denotes the common
differential operator. Obviously Vd(R+) ⊂ L2[0, ∞) and Vd(R+) ( Vd+1(R+) for any d ∈ N+.
Kammler (1979a) proves the following theorem.
Theorem D.4. Assume ρ ∈ L2[0, ∞) to be completely monotonic. Then there exists a best approx-
imation p0 to P in Vd(R+), i.e.
llp0 — PkL2[0,∞) =_ inf kp - PkL2[0,∞).	(136)
ρ∈Vd(R+)
37
Published as a conference paper at ICLR 2021
When ρ ∈/ Vd(R+), any such best approximation admits a non-degenerate form
d
P0(t) = Xbje-vjt, 0 < vι < …< Vd, bj > 0, j = 1, 2,…，d,
j=1
and satisfies the generalized Aigrain-Williams equations
L[ρ0](vj) = L[p](vj), j = 1, 2,…，d,
dsL[p0](S)L=Vj = dsL[p](S)L=v/ j = 1,2,…,d.
(137)
(138)
(139)
Note that (136) and (137) are pretty similar to Definition D.2, except for a different choice of hy-
pothesis function space. Now we show a connection between these two problems.
Theorem D.5. Assume p ∈ L2 [0, ∞) to be completely monotonic, and p ∈/ Vd(R+) for some
d ∈ N+. Then Jd has non-degenerate global minimizers (b, V) ∈ Rd 0 R；.
Proof. According to Theorem D.4, there exists a non-degenerate best approximation p0 to P from
Vd(R+), i.e.
kp0 - pkL2[0,∞) =
ρ∈Vin(R+)kp-22…
(140)
d
p0(t) = X bj e-vjt, 0 <Vι < …<Vd, bj> 0, j = 1, 2,…，d.	(141)
j=1
I-V T ∙	J / T^ ʌ ∖	∙ f	I / T ∖ 1 ʌ C	, ICll	♦	1 .	i'	. ∙ 1
We aim to prove Jd(b, V) = inf	Jd(b, v). Define the following subsets of exponential sums
b∈Rd,v∈Rd+
Vd(R+) := {p : p(t) = X aie-wit,ai ∈ R,Wi > 0 j ,
Vd,k(R+) := < p ∈ Vd(R+) : W = (Wi) has k different components >,
1 ≤ k ≤ d,
then we have inf	Jd(b,v) = inf IlP — pk22r∩ It is straightforward to verify that
b∈Rd,v∈R+	ρ∈Vd(R+) "	L [0,∞)
Vd(R+) = Sk=ι Vd,k(R+), and Vd,k(R；) = Vkk(R；) ( Vk(R；) for k = 1,…，d. By (140), We
get
kp0 - pkL2[0,∞) =p∈vVn(R+)kp- pkL2 [0,∞) ≤ ρ∈vdnf(R+)kP-PkL2[0,∞).
Since p0 ∈ Vd,d(R；), We have
Jd(b, V) = kp0 - pkL2[0,∞) = ρ∈vdnf(R+)kp - pkL2[0,∞).
The last task is to show inf kp 一 pkL2[0 ∞) = inf	∣∣p 一 pkL2[0 ∞). In fact, for any
p∈Vd(R+)	,	ρ∈Vd,d(R+)	,
P ∈ Vk,k, p(t) = Pk=I aie-wit, let α := (aι,…，ak, 0),	W := (wι,…，Wk, 1+maxι≤i≤k Wi),
we get p(t) := Pk=IL θie-wit ∈ Vk+1,k+1, which implies Vk,k ⊂ Vk+1,k+1. Therefore,
ʌ inf JP — PkL2[0,∞) = d inf	kp — PkL2[0,∞) = 1min7( ʌ .ɪnf	JP — PkL2[0,∞)]
p∈Vd(R+)	ρ∈Sk=ι Vd,k(R+)	1≤k≤d [p∈Vd,k(R+)	'J
= Imjnd i -UJn% Jlp - PkL2[0,∞) ∖ ≥ -UJny Jlp - PkL2[o,∞),
1≤k≤d [p∈Vk,k(R+)	ρ	p∈Vd,d(R+)
which completes the proof.	□
Combining Theorem D.1 and Theorem D.5 immediately gives the following result.
38
Published as a conference paper at ICLR 2021
Theorem D.6. Assume ρ ∈ L2 [0, ∞) to be completely monotonic, and ρ ∈/ V1 (R+). Let D :=
{d ∈ N+ : ρ ∈/ Vd(R+)}, D0 := supD and write m0 := min{m, D0}. Then the total number of
coincided critical affine spaces of Jm is at least Pdm=1 d! md
Proof. We have 1 ∈ D and thus D = 0, Do ≥ 1. Since Vd(R+) ( Vd+ι(R+) for any d ∈ N+,
We have D = {1, 2,…，Do} if Do < ∞, and D = N+ if Do = ∞.14 Both of them gives
{1,2,…，m0} ⊂ D, i.e. P / Vk(R+) for any k ≤ m0. By Theorem D.5, J(k)has non-degenerate
global minimizers for any k ≤ m0, i.e. m0 ∈ N. According to Theorem D.1, for any d ∈ N+,
1 ≤ d ≤ m0 = min{m, m0} ≤ min{m, M}, there exists at least d! md d-coincided critical affine
spaces of Jm,. Sum over d gives the total number Pm= ι d! {7}.	□
Remark D.7. Examples:
•	Suppose the target is ρ(t) = 1∕(1+t)α, α > 0, then D = N+ and Do = ∞. Thetotalnum-
ber of coincided critical affine spaces of the corresponding Jm is at least Pdm=1 d! md .
•	Suppose the target is an non-degenerate exponential sum with positive coefficients: ρ(t) =
Pm=I aje-wjt, where aj > 0 and Wi = w* for any i = j, i,j = 1,…，m. Then
D = {1, 2, ∙∙∙ ,m — 1} and Do = m — 1. The total number of coincided critical affine
spaces of the corresponding Jm is at least Pdm=-11 d! md , which is exactly (130).
An complement for Theorem D.2 is as folloWs.
Theorem D.7. Fix any m ∈ N+ relatively large. Consider the loss Jm associated with the target
being a non-degenerate exponential sum with positive coefficients, i.e. ρ(t) = Em=I aje~wjt,
where aj > 0 and w* = Wj for any i = j, i,j = 1,…,m. Then in the landscape of Jm,
the number of coincided critical affine spaces is at least P oly(m) times larger than the number of
global minimizers.
Proof. By Theorem D.2, We only need to shoW is m ∈ N . Since ρ ∈ L2 [0, ∞) is completely
monotonic, and ρ ∈∕ Vk (R+) for any k ≤ m — 1, then by Theorem D.5, J(k) has non-degenerate
global minimizers for any k ≤ m — 1, i.e. m — 1 ∈ N. The proof is completed by noticing that Jm
obviously has non-degenerate global minimizers, e.g. (a*,w*).	□
D.1.3 A Low-dimensional Example
To further understand the structure of coincided critical affine spaces, We focus on a specific loW-
dimensional example here. That is
min	J2 (a, w)
a∈R2 ,w∈R2+
2	2
Xaie-wit — ρ(t)
i=1	L2[o,∞)
*	*
With the target to be a non-degenerate exponential sum ρ(t) = j =1 aj*e-wjt, Where aj* 6= 0 and
w* = Wj forany i = j, i,j = 1, ∙∙∙ ,m*. Aswe will show later, the coincided critical affine spaces
of J2 contain both saddles and degenerate stable points which are not global optimal.
By Lemma D.1, Remark D.1 and Theorem D.1, we know the 1-coincided critical affine space of J2
exists, and it can be constructed by taking the non-degenerate global minimizer of Ji, say (a, W)
14In fact, Vd(R+) ( Vd+ι(R+) for any d ∈ N+ implies if ρ ∈ Vd(R+), P ∈ Vk(R+) for any k ≤ d, i.e.
d ∈ D ⇒ k ∈ D for any k ≤ d; otherwise, if ρ ∈ Vd(R+), ρ ∈ Vl (R+) for any l ≥ d, i.e. d ∈/ D ⇒ l ∈/ D
for any l ≥ d.
39
Published as a conference paper at ICLR 2021
with ^ = 0 and W > 0. Then M(^,w),(2,i) ：= {(aι,^ 一 ai,W, W) : ai ∈ R} ∈ R4 is a line 15, and
VJ2(a1,^ 一 ai,W, W) = 0 for any ai ∈ R. Denote the Hessian of J2 on the line M(^,w),(2,i) by
A(^,w)(aι), i.e. A(a,w)(aι) ：= V2 J2(a1, ^ 一 aι,W,W). We investigate the landscape of J on the
line M(a,w),(2,i) by analyzing the eigenvalue distribution of A(a,w)(aι).
Proposition D.1. Suppose m = m* = 2, and 0 < w* < W2. Let Ii ：= [0, a] and I2 ：= (-∞, 0) ∪
(^, +∞) 16. Then
1.	If a*a* < 0, the minimal eigenvalue of A(a,w)(aι) is 0 for any ai ∈ Ii, and negative for
any ai ∈ I2;
2.	If a*a* > 0 and w*/w* < 2 + √3, the minimal eigenvalue of A(^,w)(ai) is negative for
any ai ∈ Ii, and 0for any ai ∈ I2.
Proof. Write c(w) := Pm=I a* [2w(w；w*)2 一(w+⅛pj, and a2 ：= ^ 一 ai. A straightforward
computation shows that
A(^,W) (a1)
一aι
2W2
-aι
2W2
a1
a1a2
2W3
-a2
2W2
-a2
2W2
aι。2
2	2W3
2W3 + 4C(W)a2
Considering the congruent transformation of A(a,w)(ai), which does not affect the index of inertia:
A(a,w) (ai)
12 22
1Tu2U,-α2w
12 22
1TU2W—α2w
-aι
2W2
-aι
2W2
-a2
2W2
-a2
2W2
a1a2
2	2W3
2W + 4C(W)a2
aι a2
2W3
a1
l-ʌwo O O
0	0	0
0	0	0
0	2 4w13 + 4C(W)ai	a1a2 2	4W3
0	a1a2 4W3	4⅛ +4C(W)a2一
→
We see that A(^,w) (aι) has one positive eigenvalue 1/W and one eigenvalue 0. What remains are the
eigenvalues of A(a,w)(aι) = 4w3 +a*)。1
_	4W3
compute
To determine their signs, We
det(A(a,w)(aι)) = aι(^ -。1) ∙ 4c(W)
aι。2
2	4W3
4W3 + 4C(W)a2.
ʌo ʌo aι(^ — aι) ∙ ac(w) ∙ (a2 + 16W3^c(W)).
^2w3
(142)
(143)
So we need to analyze the sign of ^c(W) and ^2 + 16W3^c(W) under different assumptions on
(a*, w*).
(i)	a*a* < 0. By the optimality condition of (^, W) for Ji, we have
m*	*	m*	*
^ = 2W X -^j = 4W2 X ,, a；,
j=1 W + W*	j=1 (w + W*)2
and therefore
^ m	a*
C(W) = 8W^3 - ∑ (W + W* )3.
(144)
15Here we omit the corresponding partition P since it is unique.
16Suppose a > 0 here without loss of generality. If a < 0, we let Ii := [a, 0] and I2 := (-∞, a) ∪ (0, +∞)
and the same conclusions hold.
40
Published as a conference paper at ICLR 2021
Write Vj := w*/W, j = 1, 2, we get 0 < v； < V2, and
m*	*	m*	*
α = 2 X aj	=4 X -a-
221+ Vj 42-(1 + Vj)2 ,
j=；	j=；
W3c(W)
*
m*
-X
j=1
a*
(1 + Vj )3 .
a
8
Therefore
*
m*
8W3^c(W) = a2 — 8a ^X
j=1
*
m*
*
aj*
(1+j3
2
16
Σ
j=1
*
aj*
(1+vj)2
*
m*
16X
j=1
a*
1 + Vj
*
m*
• X
j=1
*
aj*
WvF
—
— 16a* a* (v； — V2)2
=(1 + V；)3(1 + V2)3
> 0,
which gives ^c(W) > 0 and ^2 + 16W3^c(W) > 8W3ac(W) > 0.
(ii)	a；a2 > 0, w*/w* < 2 + √3. By (145), ^c(W) < 0.
*
m*
^2 + 16W3^c(W) = 3^2 — 16^ ^X ———j--ɜ
j=； ( + Vj )
2
16 3
*
m*
X	aj
j= (1 + Vj )2
*
m
2X
j=1
*
a*
1 + Vj j=1(I + Vj)
—
*
a
*
m
3
(145)
a*2
=-4~4	[〃2 +/u1	+ 6cu；u2	—	2cu；u2 —	2cuιu2]	(C =	a；/a；,	Uj	:= 1 + Vj	> 1)
u1u2
a*2
=(s4 + c2 + 6cs2 — 2cs — 2cs3)	(S = u；/u\ > 1)
u42
=害 Y — 2s(s2 — 3s +1)c + s4].
u42
Since 4s2(s2 — 3s + 1)2 — 4s4 = 4s2(s — 1)2[(s — 2)2 — 3], and 1 < S = u；/u； = (W + w2)∕(W +
w*) < w*/w* < 2 + √3, We get ∆c < 0. This implies c2 — 2s(s2 — 3s + 1)c + s4 > 0 and
^2 + 16W3^c(W) > 0.
In both (i) and (ii), a2 + 16W36c(W) > 0, which implies that there is at least one positive diag-
onal element of A(a w)(ai) in a sufficiently small neighborhood of a； = 0 and a； = ^. By the
Rayleigh-Ritz Theorem and Weyl,s Theorem, A(a w) (a；) has at least one positive eigenvalue in this
neighborhood. However, by (142), det(A(a w)(a；)) only changes the sign at a； = 0 and a； = a.
This implies another eigenvalue of A(^ W) (a；) changes the sign at a； = 0 and a； = ^ accordingly.
By different signs of ac(W) derived in (i) and (ii), and (142), the proof is completed.	口
Remark D.8. From Proposition D.1, we see that there are both saddles and degenerate stable
points of J2 on the critical affine spaces (line) M(^,w),(2j), and each of them in fact forms Offine
spaces (lines) respectively, but they are certainly not global minimizers. Therefore, the gradient-
based algorithms can get stuck around this affine space, except that it meets saddles with negative
eigenvalues of large magnitude.
E	Momentum Helps Training: Quadratic Examples
In practice, it is often the case that training is trapped in some very flat regions (plateaus), where the
loss function has rather small gradients and negative eigenvalues of Hessian. Now we illustrate the
escape dynamics (from plateaus) via a simple quadratic example.
41
Published as a conference paper at ICLR 2021
Consider the loss function f(x) = (x21 - x22)/2 with 0 <	1. We check the escaping per-
formance for continuous-in-time analogs of two optimization algorithms: gradient decent (GD) and
momentum (heavy ball) method.
(1)	Gradient decent
Consider the gradient flow of f(x) with an initial value x0 = (δ, 1)>, where 0 < δ 1 and
δ = O(e). Thus ∣∣Vf(x0)k2 = O(e), and
x01(τ) = -x1(τ), x1 (0) = δ ⇒ x1(τ) = δe-τ
x02 (τ) = x2 (τ),	x2 (0) = 1 x2 (τ) = eτ
⇒ f (x(τ)) = (δ2e-2τ - ee2eτ)/2 =: 'ι(τ).
It is easy to show that there are different timescales of 'ι(τ). In fact, when T = O(1∕e), 'ι(τ)=
O(2)e-|O(1/)| - e|O(1)| = O(). However, when τ continuous to increase, say
T ≥ * ln δ0 =：T；,	(146)
where δo > 0 denotes the gap satisfying E =	o(δo),	We get 'ι(τ)	≤ 'ι(τf)	= O(e2)	一
ee2e∙2⅛ ln δ0/2 = O(e2) 一 δ0∕2 < -δ0∕4 for any T	≥ τ[.
(2)	Momentum
The momentum algorithm has the update rule
xk+1 = xk 一 ηVf(xk) + ρ(xk 一 xk-1),
(147)
where ρ ∈ R, η > 0 is the learning rate. The continuous-in-time analog can be derived as (similar
to the arguments in Su et al. (2014))
xk+1 一 2xk + xk-1
0 = P-----------------
η
+ (I - P) xk + 1 - Xk
√η √η
+ Vf(xk)
≈ Px00 (t) +
=0 χ0(t) + Vf (χ(t)),
with Xk := x(k√η) and the step size √η of the simple finite differences17. Let xι = xo — ηVf (xo),
we also get x0(0) = -√ηVf (x(0)).
To facilitate a comparison to GD, we take η = 1 18 and P = 1 19. Plugging the expression of f, we
can solve the ODE
X00(t) + Vf(X(t)) =	_ 0 ⇔ X XIz(T) + xi(t)=0,	X1(0) = δ, X1(0) = —δ X020(T) — EX2 (T) = 0, X2 (0) = 1, X02(0) = E X1 (T) = δ(cos T — sin T) ⇒ X x2 (t) = ^√ e√ + 1-√ e-√iτ r 「	/r .	L	TL	'21 ⇒ f (x(t)) = - δ2(cost - SinT)2 — E (——---e√ɪτ H		--e-√^τ^
Write '2(τ) := f (X(T)). It is not hard to show that there are still different timescales of '2(τ). In
fact, when T = O(l∕√E), '2(τ) = O(e2)∣O(1)∣ — e∣O(1)∣(elO(1)1 + e-|O(I)|)2 = O(e). However,
when T continuous to increase, say
T ≥ 2⅛ *=F,	(148)
we get '2(t2,)	=	O(e2) — e(^√)2e2√τ/2	+ O(e) =	O(e)	—	δo	<	—δo∕2,	hence	'2(T) ≤
O(E) — δo < —δo∕2 for any T ≥ T2.
Combining (1) and (2), we have the following conclusions.
17It is easy to check that the error of discretization is of order O(√η).
18In the continuous-in-time analog of GD, i.e. the gradient flow, the step size is taken as 1.
19As is seen later, ρ = 1 not only simplifies the analysis, but also helps to obtain the best acceleration.
42
Published as a conference paper at ICLR 2021
•	For both training dynamics, there are different timescales in the loss function. That is to
say, relatively long time is needed to escape from the plateaus;
•	Comparing (146) and (148), We get different timescales of escaping: O (1/e ∙ ln(1∕e)) for
GD and O (1∕√e ∙ ln(1∕e)) for momentum. JUst like the convex case, where momentum
improves the convergence rate by Weakening the dependence on condition number, We see
momentum can also help to escape rather flat saddles.
43