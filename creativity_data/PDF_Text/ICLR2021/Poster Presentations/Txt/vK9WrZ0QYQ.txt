Published as a conference paper at ICLR 2021
Deep Neural Tangent Kernel and Laplace
Kernel Have the Same RKHS
Lin Chen	Sheng Xu
Simons Institute for the Theory of Computing Department of Statistics and Data Science
University of California, Berkeley	Yale University
lin.chen@berkeley.edu	sheng.xu@yale.edu
Ab stract
We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural
tangent kernel and the Laplace kernel include the same set of functions, when
both kernels are restricted to the sphere Sd-1. Additionally, we prove that the
exponential power kernel with a smaller power (making the kernel less smooth)
leads to a larger RKHS, when it is restricted to the sphere Sd-1 and when it is
defined on the entire Rd .
1 Introduction
In the past few years, one of the most seminal discoveries in the theory of neural networks is the
neural tangent kernel (NTK) (Jacot et al., 2018). The gradient flow on a normally initialized, fully
connected neural network with a linear output layer in the infinite-width limit turns out to be equiv-
alent to kernel regression with respect to the NTK (This statement does not necessarily hold for a
non-linear output layer, because the NTK is non-constant (Liu et al., 2020)). Through the NTK,
theoretical tools from kernel methods were introduced to the study of deep overparametrized neural
networks. Theoretical results were thereby established regarding the convergence (Allen-Zhu et al.,
2019; Du et al., 2019b;a; Zou et al., 2020), generalization (Cao & Gu, 2019; Arora et al., 2019b), and
loss landscape (Kuditipudi et al., 2019) of overparametrized neural networks in the NTK regime.
While NTK has proved to be a powerful theoretical tool, a recent work (Geifman et al., 2020) posed
an important question whether the NTK is significantly different from our repertoire of standard
kernels. Prior work provided empirical evidence that supports a negative answer. For example,
Belkin et al. (2018) showed experimentally that the Laplace kernel and neural networks had similar
performance in fitting random labels. In the task of speech enhancement, exponential power kernels
Keγx,pσ(x, y) = e-kx-ykγ /σ, which include the Laplace kernel as a special case, outperform deep
neural networks with even shorter training time (Hui et al., 2019). The experiments in (Geifman
et al., 2020) also exhibited similar performance of the Laplace kernel and the NTK.
The expressive power of a positive definite kernel can be characterized by its associated reproducing
kernel Hilbert space (RKHS) (Saitoh & Sawano, 2016). The work (Geifman et al., 2020) considered
the RKHS of the kernels restricted to the sphere Sd-1 , {x ∈ Rd | kxk2 = 1} and presented a
partial answer to the question by showing the following subset inclusion relation
HGauss(Sd-1) (HLap(Sd-1)=HN1(Sd-1) ⊆HNk(Sd-1),
where the four spaces denote the RKHS associated with the Gaussian kernel, Laplace kernel, the
NTK of two-layer and (k + 1)-layer (k ≥ 1) fully connected neural networks, respectively. All four
kernels are restricted to Sd-1. However, the relation between HLap(Sd-1) and HNk (Sd-1) remains
open in (Geifman et al., 2020).
We make a final conclusion on this problem and show that the RKHS of the Laplace kernel and the
NTK with any number of layers have the same set of functions, when they are both restricted to
Sd-1. In other words, we prove the following theorem.
Theorem 1.	Let HLap(Sd-1) and HNk (Sd-1) be the RKHS associated with the Laplace kernel
KLap (x, y) = e-ckx-yk (c > 0) and the neural tangent kernel of a (k + 1)-layer fully connected
1
Published as a conference paper at ICLR 2021
ReLU network. Both kernels are restricted to the sphere Sd-1. Then the two spaces include the same
set of functions:
HLap(Sd-1) =HNk(Sd-1),	∀k≥ 1.
Our second result is that the exponential power kernel with a smaller power (making the kernel less
smooth) leads to a larger RKHS, both when it is restricted to the sphere Sd-1 and when it is defined
on the entire Rd .
Theorem 2.	Let HKeγx,pσ (Sd-1) and HKeγx,pσ (Rd) be the RKHS associated with the exponential power
kernel KγP(x, y) = exp (— kx-yk") (γ, σ > 0) when it is restricted to the unit sphere Sd-1 and
defined on the entire Rd, respectively. Then we have the following RKHS inclusions:
(1)	If0 <γ1 <γ2 < 2,
HKeγx2p,σ2(Sd-1)⊆HKeγx1p,σ1(Sd-1).
(2)	If 0 < γ1 < γ2 < 2 are rational,
HKeγx2p,σ2(Rd) ⊆HKeγx1p,σ1(Rd).
If it is restricted to the unit sphere, the RKHS of the exponential power kernel with γ < 1 is even
larger than that of NTK. This result partially explains the observation in (Hui et al., 2019) that the
best performance is attained by a highly non-smooth exponential power kernel with γ < 1. Geifman
et al. (2020) applied the exponential power kernel and the NTK to classification and regression tasks
on the UCI dataset and other large scale datasets. Their experiment results also showed that the
exponential power kernel slightly outperforms the NTK.
1.1	Further Related Work
Minh et al. (2006) showed the complete spectrum of the polynomial and Gaussian kernels on Sd-1.
They also gave a recursive relation for the eigenvalues of the polynomial kernel on the hypercube
{—1, 1}d. Prior to the NTK (Jacot et al., 2018), Cho & Saul (2009) presented a pioneering study on
kernel methods for neural networks. Bach (2017) studied the eigenvalues of positively homogeneous
activation functions of the form σα (u) = max{u, 0}α (e.g., the ReLU activation when α = 1) in
their Mercer decomposition with Gegenbauer polynomials. Using the results in (Bach, 2017), Bietti
& Mairal (2019) analyzed the two-layer NTK and its RKHS in order to investigate the inductive bias
in the NTK regime. They studied the Mercer decomposition of two-layer NTK with ReLU activation
on Sd-1 and characterized the corresponding RKHS by showing the asymptotic decay rate of the
eigenvalues in the Mercer decomposition with Gegenbauer polynomials. In their derivation of a
more concise expression of the ReLU NTK, they used the calculation of (Cho & Saul, 2009) on
arc-cosine kernels of degree 0 and 1. Cao et al. (2019) improved the eigenvalue bound for the k-th
eigenvalue derived in (Bietti & Mairal, 2019) when d k. Geifman et al. (2020) used the results
in (Bietti & Mairal, 2019) and considered the two-layer ReLU NTK with bias β initialized with
zero, rather than initialized with a normal distribution (Jacot et al., 2018). However, neither (Bietti
& Mairal, 2019) nor (Geifman et al., 2020) went beyond two layers when they tried to characterize
the RKHS of the ReLU NTK. This line of work (Bach, 2017; Bietti & Mairal, 2019; Geifman et al.,
2020) is closely related to the Mercer decomposition with spherical harmonics. Interested readers
are referred to (Atkinson & Han, 2012) for spherical harmonics on the unit sphere. The concurrent
work (Bietti & Bach, 2021) analyzed the eigenvalues of the ReLU NTK.
Arora et al. (2019a) presented a dynamic programming algorithm that computes convolutional NTK
with ReLU activation. Yang & Salman (2019) analyzed the spectra of the conjugate kernel (CK)
and NTK on the boolean cube. Fan & Wang (2020) studied the spectrum of the gram matrix of
training samples under the CK and NTK and showed that their eigenvalue distributions converge to
a deterministic limit. The limit depends on the eigenvalue distribution of the training samples.
2	Preliminaries
Let C denote the set of all complex numbers and write i，√-1. For Z ∈ C, write <z, =z, arg Z ∈
(一∏, ∏] for its real part, imaginary part, and argument, respectively. Let H+，{z ∈ C | =z > 0}
2
Published as a conference paper at ICLR 2021
denote the upper half-plane and H- , {z ∈ C | =z < 0} denote the lower half-plane. Write Bz (r)
for the open ball {w ∈ C | |z - w| < r} and Bz(r) for the closed ball {w ∈ C | |z - w| ≤ r}.
Suppose that f(z) has a power series representation f(z) = Pn≥0 anzn around 0. Denote
[zn]f (z) , an to be the coefficient of the n-th order term.
For two sequences {an} and {bn}, write an 〜bn if limn→∞ an = 1. Similarly, for two functions
bn
f (Z) and g(z), write f (Z)〜 g(z) as Z → zo if limz→z0 f(∣) = 1. We also use big-O and little-o
notation to characterize asymptotics.
Write L {f (t)}(s) , R0∞ f (t)e-st dt for the Laplace transform of a function f (t). The inverse
Laplace transform of F(s) is denoted by L -1{F(s)}(t).
2.1	Positive Definite Kernels
For any positive definite kernel function K(x, y) defined for x, y ∈ E, denote HK(E) its associated
reproducing kernel Hilbert space (RKHS). For any two positive definite kernel functions K1 and
K2, we write K1 4 K2 if K2 - K1 is a positive definite kernel. For a complete review of results on
kernels and RKHS, please see (Saitoh & Sawano, 2016).
We will study positive definite zonal kernels on the sphere Sd-1 = {x ∈ Rd | kxk = 1}. For a
zonal kernel K(x, y), there exists a real function K : [-1, 1] → R such that K(x, y) = K(u),
where u = x>y. We abuse the notation and use K(u) to denote K(u), i.e., K(u) here is real
function on [-1, 1].
In the sequel, we introduce two instances of the positive definite kernel that this paper will investi-
gate.
Laplace Kernel The Laplace kernel KLap(x, y) = e-ckx-yk with c > 0 restricted to the sphere
SdT is given by KLaP(x,y) = e-c√2(I-XTy) = e-c√1-u，KLaP(u), where by our convention
U = χ>y and c，√2c > 0. We denote its associated RKHS by HLap.
Exponential Power Kernel The exponential power kernel (Hui et al., 2019) with γ> 0 andσ> 0
is given by KYp (x, y) = exp (-iχ-⅛Y). If X and y are restricted to the sphere Sd-1, we have
KYp(χ,y)=eχp (-(2(I-XTy))"2).
Neural Tangent Kernel Given the input x ∈ Rd (we define d0 , d) and parameter θ, this paper
considers the following network model with (k + 1) layers
fθ(x)
(W1x + βb1) + βb2
+ βbk + βbk+1 ,
(1)
where the parameter θ encodes Wl ∈ Rdl ×dl-1, bl ∈ Rdl (l = 1, . . . , k), w ∈ Rdk, and bk+1 ∈ R.
The weight matrices W1, . . . , Wk, w are initialized with N(0, I) and the biases b1, . . . , bk+1 are
initialized with zero, where N(0, I) is the multivariate standard normal distribution. The activation
function is chosen to be the ReLU function σ(x) , max{x, 0}.
Geifman et al. (2020) and Bietti & Mairal (2019) presented the following recursive relations of the
NTK Nk(x, y) of the above ReLU network (1):
3
Published as a conference paper at ICLR 2021
∑k (χ,y) = √∑k-ι(χ,χ)∑k-ι(y,y)κι
Nk(x, y) = Σk(x, y) + Nk-1(x, y)κ0
Σ	ςji(X,y)	]
∖ √∑k-ι(χ,χ)∑k-ι(y,y))
ςji(X,y)	) + /2
√∑k-i(x,x)∑k-i(y,y))	，
(2)
where κ0 and κ1 are the arc-cosine kernels of degree 0 and 1 (Cho & Saul, 2009) given by
κo(u) = ɪ(n — arccos(u)), κι(u) = ɪ (U ∙ (π — arccos(u)) + pl — u2).
The initial conditions are
N0(X, y) = u + β2,	Σ0 (X, y) = u ,
where u = X>y by our convention.
(3)
The NTKs defined in (Bietti & Mairal, 2019) and (Geifman et al., 2020) are slightly different. There
is no bias term β2 in (Bietti & Mairal, 2019), while the bias term appears in (Geifman et al., 2020).
We adopt the more general setup with the bias term.
Lemma 3 (Proof in Appendix A.1). Σk (X, X) = 1 for any X ∈ Sd-1 and k ≥ 0.
Lemma 3 simplifies (2) and gives
Σk(u) = κ(1k) (u) ,	Nk(u) = κ(1k) (u) + Nk-1(u)κ0(κ(1k-1)(u)) + β2 ,	(4)
where κ1k)(u)，κι(κι(∙ ∙ ∙ κ1(κ1(u)) ∙…))is the k-th iterate of κι(u). For example, κf)(u) = u,
'----------------------{z-------}
k
κ(11)(u) = κ1(u) and κ(12)(u) = κ1(κ1(u)). We present a detailed derivation of (4) in Appendix A.2.
3	Results on Neural Tangent Kernel
In this section, we present an overview of our proof for Theorem 1. Since (Geifman et al.,
2020) showed HLap(Sd-1) ⊆ HNk (Sd-1), it suffices to prove the reverse inclusion HNk (Sd-1) ⊆
HLap(Sd-1). We then relate positive definite kernels with their RKHS according to the following
lemma.
Lemma 4 ((Aronszajn, 1950, p. 354) and (Saitoh & Sawano, 2016, Theorem 2.17)). Let K1, K2 :
Ω X Ω → C be two positive definite kernels. Then the Hilbert space Hκγ is a subset of Hk if and
only if there exists some constant γ > 0 such that
K1 4 γ2K2 .
Lemma 4 implies that in order to show HNk (Sd-1) ⊆ HLap(Sd-1), it suffices to show γ2KLap — Nk
is a positive definite kernel for some γ > 0. Note that both KLap and Nk are positive definite
kernels on the unit sphere. Then the Maclaurin series of KLap(u) and Nk (u) have all non-negative
coefficients by the classical approximation theory; see (Schoenberg, 1942, Theorem 2), Bingham
(1973), and (Cheney & Light, 2009, Chapter 17). Conversely, if the Maclaurin series of K(u) have
all non-negative coefficients, K(X, y) = K(X>y) is a positive definite kernel on the unit sphere. To
be precise, we have the following lemma.
Lemma 5 (Schoenberg (1942); Bingham (1973)). Suppose that K(X, y) = f (X>y) where X, y ∈
Sd-1 and f is continuous on [—1, 1].1 Then K is a positive definite kernel on Sd-1 for every d if
and only iff(u) = k∞=0 akuk, in which ak ≥ 0 and k∞=0 ak < ∞.
Thus, we turn to show that there exists γ > 0 such that γ2 [zn]KLap(z) ≥ [zn]Nk (z) holds for every
n ≥ 0.
1 When x and y live on the unit sphere (i.e., x>x = y>y = 1), their inner product x>y can be any real
number in [-1, 1].
4
Published as a conference paper at ICLR 2021
Exact calculation of the asymptotic rate of the Maclaurin coefficients is intractable for Nk due to
its recursive definition. Instead, we apply singularity analysis tools in analytic combinatorics. We
refer the readers to (Flajolet & Sedgewick, 2009) for a systematic introduction. We treat all (zonal)
kernels, KLap(u), Nk(u), κ0(u), and κ1(u), as complex functions of variable u ∈ C. To emphasize,
we use z ∈ C instead of u to denote the variable. The theory of analytic combinatorics states that
the asymptotic of the coefficients of the Maclaurin series is determined by the local nature of the
complex function at its dominant singularities (i.e., the singularities closest to z = 0).
To apply the methodology from (Flajolet & Sedgewick, 2009), we introduce some additional defi-
nitions. For R > 1 and φ ∈ (0, n/2), the ∆-domain ∆(φ, R) is defined by
∆(φ, R) , {z ∈ C | |z| < R, z 6= 1, | arg(z - 1)| > φ} .
For a complex number ζ 6= 0, a ∆-domain at ζ is the image by the mapping z 7→ ζz of ∆(φ, R) for
some R > 1 and φ ∈ (0,∏∕2). A function is ∆-analytic at Z if it is analytic on a ∆-domain at Z.
Suppose the function f(z) has only one dominant singularity and without loss of generality assume
that it lies at z = 1. We then have the following lemma.
Lemma 6 ((Flajolet & Sedgewick, 2009, Corollary VI.1)). If f is ∆-analytic at its dominant singu-
larity 1 and
f (Z)〜(1 — z)-α,	as Z → 1,z ∈ ∆
with α ∈/ {0, -1, -2, . . . }, we have
nα-1
[zn]f(z)〜5.
Γ(α)
If the function has multiple dominant singularities, the influence of each singularity is added up
(See (Flajolet & Sedgewick, 2009, Theorem VI.5) for more details). Careful singularity analysis
then gives
[zn]KLap(z)〜Cin-3/2,	[zn]Nk(z) ≤。2『3/ ,
for some positive constants C1, C2 > 0. We refer to Section 3.2 and Appendix A.4 for more detailed
steps. They are indeed of the same order of decay rate n-3/2, which implies that such γ exists. This
shows HNk(Sd-1) ⊆ HLap(Sd-1).
3.1	∆-ANALYTICITY OF NEURAL TANGENT KERNELS
We present the ∆-analyticity of the NTKs here. In light of (4), the NTKs Nk are compositions of
arc-cosine kernels κo and κι. We analytically extend κo and κι to a complex function of a complex
variable Z ∈ C. Both complex functions arccos(z) and √1 — z2 have branch points at Z = ±1.
Therefore, the branch cut of κo(z) and κι(z) is [1, ∞) ∪ (一∞, -1]. They have a single-valued
analytic branch on
D = C \ [1, ∞) \ (-∞, —1].	(5)
On this branch, we have
K (Z) = ∏ + i log(z + i√1 - z2)
0π
ki(z) = ɪ [z ∙ (π + ilog(Z + ip1 — z2) + p1 — z2],
where we use the principal value of the logarithm and square root. We then show the dominant
singularities of κ(1k)(Z) are ±1 and that κ(1k) (Z) is ∆-analytic at ±1 for any k ≥ 1. We further have
the following theorem on the ∆-singularity for Nk.
Theorem 7 (Proof in Appendix A.3). For each k ≥ 1, the dominant singularities of Nk are ±1.
There exists Rk > 1 such that Nk is analytic on {Z ∈ C | |Z| ≤ Rk} ∩ D, where D = C \ [1, ∞) \
(-∞, —1].
5
Published as a conference paper at ICLR 2021
3.2	ASYMPTOTIC RATES OF MACLAURIN COEFFICIENTS FOR Nk
The following theorem demonstrates the asymptotic rates of Maclaurin coefficients for Nk .
Theorem 8 (Proof in Appendix A.4). The n-th order coefficient of the Maclaurin series of the
(k + 1)-layer NTK in (2) satisfies [zn]Nk(z) = O(n-3/2).
In the proof of Theorem 8, we show the following asymptotics
Nk(Z) = (k + 1)(z + β2) - (√2(1 + β2)k(；+ 1) + o(1)) √1 - Z as Z → 1,	(6)
Nk(Z) = Nk(-1)+ ( √2(β∏ - 1) ∏κo(κ1(-1)) + o(1)) √T+τ as Z →-1.	⑺
When β = 1, the singularity at Z = -1 will not provide a √1 + Z term. The dominating term in (7)
is a higher power of √1 + Z. As a result, the contribution of the singularity at -1 to the Maclaurin
coefficients is o(n-3/2) and dominated by the contribution of the singularity at 1. The singularity
at Z = 1 provides a √1 - Z term and thus contributes to O(n-3/2) decay rate of [Zn]Nk(z). In
addition, from (6), we deduce
[Zn]Nk (z)	2√2k(k + 1)	k(k + 1)
-----7-TT  r^-------: -		=-
n-3/2	(2π)Γ (-2)	√2π3/2
(8)
When β = 1, both singularities ±1 contribute Θ(n-3/2) to the Maclaurin cofficients. The contribu-
tion of Z = 1 is
√2(1 + β2)k(k +1) -3/2 _ (β2 + 1) k(k + 1) -3/2
2∏r(-1)	n = -2√2∏3/2- n	.
The contribution of Z = -1 is
√2(β2 - 1)
πΓ(-1∕2)
∏ κ0(κ1(-1)) n-3/2 = (√-β2 ∏ κ0(κ1(-1)) n-3∕2
Combining them gives
[Zn]Nk (Z)
n-3/2
(β2 + 1)k(k + 1)
2√2π3∕2
1 β2 k-1
+ (-1)n√-⅛ ∏ κo(κ1(-1)).
j=1
(9)
Based on Theorem 8, we are ready to prove Theorem 1.
Proof. Let KLaP(Z) = e-cVZI-Z, where c > 0 is an arbitrary constant. We have HKLap = HLap. The
complex function KLap is analytic on C \ [1, ∞). As Z → 1, we have
KLap(Z) - 1
-c
，1 — Z + o(，1 — Z)〜，1 — Z .
By Lemma 6, we obtain
[Zn]KLaP(Z)〜√jnn-3/2 .	(IO)
2π
Note that [Zn]Nk(Z) = O(n-3/2) from Theorem 8. Therefore, there exists γ > 0 such that γ2 ∙
[Zn]KLap(Z) - [Zn]Nk(Z) > 0 for all n ≥ 0. This further implies γ2KLap(x>y) - Nk (x>y) is a
positive definite kernel. According to Lemma 4, we have HNk (Sd-1 ) ⊆ HLap(Sd-1). Note that,
due to (Geifman et al., 2020, Theorem 3), we also have HLap(Sd-1) ⊆ HNk (Sd-1). Therefore, for
anyk ≥ 1,HLap(Sd-1) =HNk(Sd-1).
□
6
Published as a conference paper at ICLR 2021
4 Results on Exponential Power Kernel
This section presents the proof of Theorem 2. We first show part (1) below by singularity analysis.
Proof of part (1) of Theorem 2. Recall that the exponential power kernel restricted to the unit sphere
with γ > 0 and σ > 0 is given by Kγ,pσ(x, y) = exp (— kx—yk" ) = exp (一 (2(1-xσ Iyy))Y ). Let
Us study the decay rate of the Maclaurin coefficients of KYpT (Z)，e-c(1-z)γ/2, where C = 2y/2/。.
The dominant singularity lies at z = 1. As z → 1, we get
KYP(z) = 1 — (c + o(1))(1 — z)γ/2 .
Applying Lemma 6 gives [zn]Kγ,pσ(z) 〜 -—(H. Therefore, a smaller Y results in a larger
RKHS.	□
Part (2) of Theorem 2 requires more technical preparation. Recall that L and L —1 denote the
Laplace transform and inverse Laplace transform, respectively. We explicitly calculate the inverse
Laplace transform L —1{exp(—sa)}(t) using Bromwich contour integral and get the following
lemma.
Lemma 9 (Proof in Appendix B.1). For a ∈ (0, 1), f(t) , L —1{exp(—sa)}(t) exists. Moreover,
f (t) is continuous in -∞ < t < ∞ and satisfies f (0) = 0 .If t > 0, we have
∞
f(t) = 1 X
π
k=0
(—1)k+1Γ(ak + ι) sin(∏ak)
k!tak+1
(11)
Based on the series representation (11), we then analyze the asymptotic rate for f(t) when a is
rational. Note that if a ∈ (0,1), We have - r—a)> °∙
Lemma 10 (Proof in Appendix B.2). Let f (t) be as defined in Lemma 9. For a = P ∈ (0,1) (P and
q are Co-Prime), we have f (t)〜—”+/(一。)as t → +∞.
Thus, We have the following corollary for general exponential power kernel.
Corollary 11. For a = P ∈ (0,1) (P and q are Co-Prime) and σ > 0, L-1{exp( —sa∕σ)}(t) is
continuous in t ∈ R and satisfies L-1 {exp(—sa∕σ)}(0) = 0. Moreover, L-1{exp( —sa∕σ)}(t)〜
C t—a—1 as t → +∞, for some Constant C > 0.
Proof. Use the property L-1{F(cs)}(t) = ɪf (C), where c > 0 and F(S) = L{f (t)}(s).	□
Before completing the proof for part (2), we need two additional lemmas from the classical ap-
proximation theory. Recall that a function f(t) is comPletely monotone ifit is continuous on [0, ∞),
infinitely differentiable on (0, ∞) and satisfies ( —1)n d f(t) ≥ 0 for every n = 0,1, 2,... and t > 0
(Cheney & Light, 2009, Chapter 14).
Lemma 12 (Schoenberg interpolation theorem (Cheney & Light, 2009, Theorem 1 of Chapter 15)).
Iff is comPletely monotone but not constant on [0, ∞), then for any n distinct Points x1, x2, . . . , xn
in any inner-Product sPace, the matrix Aij = f (kxi — xj k2) is Positive definite.
Lemma 13 (Bernstein-Widder (Cheney & Light, 2009, Theorem 1 of Chapter 14)). A function
f : [0, ∞) → [0, ∞) is comPletely monotone if and only if there is a nondecreasing bounded
function g such that f(t) = 0∞ e—stdg(s).
Now we are ready to prove part (2).
Proof of Part (2) of Theorem 2. By Lemma 12 and Lemma 4, we need to show that
c2 exp(—xγ1∕2∕σι) — exp(—xγ2∕2∕σ2)	(12)
7
Published as a conference paper at ICLR 2021
is completely monotone but not constant on [0, ∞) for some c > 0. By Lemma 13, it suffices to
check that (12) is the Laplace transform of a non-negative function on [0, ∞). By Corollary 11, for
rational γ1, γ2 ∈ (0, 1], there exists c > 0 such that
c2L-1{exp(-xγM2∕σι)} - L-1{exp(-x12/2/。?)}
is continuous and positive on [0, ∞), which completes the proof.	□
5 Numerical Results
Lap Lap
N1
N2
-N3
N4
[zn]K(z)/n-3/2
Lap LaP
N1
N 2
-N 3
N4
Figure 1: We plot [zn]K(z)/n-3/2 versus n for the Laplace kernel KLap(u) = e- 2(1-u) and
NTKs N1, . . . ,N4 with β = 0, 1.
Kernel K	[z100]K(z) 100—3/2 (β =1)	Theory			[z100]K(z) 100—3/2 (β = 0)	Theory	
		(β =	1)			(β	= 0)
KLap	0.28244	1 2√∏	≈	0.282095			
Ν1	0.261069	√2 n3/2	≈	0.253975	0.261069	√2 π3/	2 ≈ 0.253975
Ν2	0.776014	3√2 π372	≈	0.761924	0.457426	⅛ ≈ 0.444455	
Ν3	1.54607	6√2 π372	≈	1.52385	0.821694	13π	∙≡⅛l ≈ 0.800218
Ν4	2.56559	ιo√2 π72	≈	2.53975	1.32472	Equation (13) ≈ 1.29531	
Table 1: We report the numerical values of 鼠。-K/z) for the Laplace kernel KLaP(U) = e-V2(1-u)
and NTKs N1 , . . . , N4 with β = 0, 1. These numerical values are the final values of the curves in
Fig. 1. We present the theoretical prediction by the asymptotic of [zn]K(z)/n-3/2 alongside each
numerical value. The choice of β does not apply to the Laplace kernel. Therefore, we only show the
results of the Laplace kernel in the columns for β = 1 and leave blank the columns for β = 0.
We verify the asymptotics of the Maclaurin coefficients of the Laplace kernel and NTKs through
numerical results.
Fig. 1 plots 'IK(Z) versus n for different kernels, including the Laplace kernel KLaP(U) =
e-λ∕2(1-u) and NTKs Ν1,...,Ν4 with β = 0,1. All curves converge to a constant as n → ∞,
which indicates that for every kernel K(Z) considered here, We have [zn]K(Z) = Θ(n-3/2). The
numerical results agree with our theory in the proofs of Theorem 8 and Theorem 1.
Now we investigate the value of [Zn]K(Z)/n-3/2. Table 1 reports [Z100]K(Z)/100-3/2 for the
Laplace kernel and NTKs with β = 0, 1. These numerical values are the final values of the curves in
Fig. 1. The theoretical predictions are obtained through the asymptotic of [Zn]K(Z)/n-3/2, which
We shall explain below. The theoretical prediction of [z100]N4(z)∕i00-3/2 with β = 0 is presented
below due to the space limit in the table
20 + π-2 π - arccos π-1
π - arccos
≈ 1.29531 .
(13)
2√2π3/2
8
Published as a conference paper at ICLR 2021
We observe that the theoretical prediction by the asymptotic is close to the corresponding numerical
value. There are two possible reasons that account for the minor discrepancy between them. First,
the theoretical prediction reflects the situation for an infinitely large n (so that the lower order terms
become negligible), while n = 100 is clearly finite. Second, the numerical results for the Maclaurin
series are obtained by numerical Taylor expansion and therefore numerical errors could be present.
In what follows, we explain how to obtain the theoretical predictions. First, (10) gives
[zn]KLap(z)∕n-3/2 〜2√∏. As a result, the theoretical prediction for [z100]KLap(z)/100-3/2 is
2√∏. Now We explain the thereotical predictions for NTKs. When β = 1, the theoretical prediction
is given by (8). We present it in the third column of Table 1 for N1, . . . , N4. When β = 0, we plug
β = 0 into ⑼ and Obtain [zn]N∕2z) 〜2√+⅛ + √⅛‰ Qk-I KO(KK-I)). The above expres-
sion (when n = 100 on the right-hand side) is the theoretical value presented in the fifth column of
Table 1 for NTKs.
6 Discussion
Our result provides further evidence that the NTK is similar to the existing Laplace kernel. However,
the following mysteries remain open. First, if we still restrict them to the unit sphere, do they have
a similar learning dynamic when we perform kernelized gradient descent? Second, what is the
behavior of the NTK and the Laplace kernel outside of Sd-1 and in the entire space Rd? Do they
still share similarities in terms of the associated RKHS? If not, how far do they deviate from each
other and is the difference significant? Third, this work along with (Bietti & Mairal, 2019; Geifman
et al., 2020) focuses on the NTK with ReLU activation. It would be interesting to explore the
influence of different activations upon the RKHS and other kernel-related quantities. We would
like to remark that the ReLU NTK has a clean expression partly because the expectation over the
Gaussian process in the general NTK can be computed exactly if the activation function is ReLU
(which may not be true for other non-linearities, for example, it may require more work for sigmoid).
Fourth, we showed that highly non-smooth exponential power kernels have an even larger RKHS
than the NTK. It would be worthwhile comparing the performance of these non-smooth kernels and
deep neural networks through more extensive experiments in a variety of machine learning tasks.
Moreover, we show that a less smooth exponential power kernel leads to a larger RKHS and therefore
greater expressive power. Its generalization capability is a related but different topic. Analyzing the
generalization error requires more efforts in general. Researchers often use the RKHS norm to
provide an upper bound for it. We will study its generalization in future work.
Acknowledgements
We gratefully acknowledge the support of the Simons Institute for the Theory of Computing. We
thank Peter Bartlett, Mikhail Belkin, Jason D. Lee, and Iosif Pinelis for helpful discussions and
thank Mikhail Belkin and Alexandre Eremenko for introducing to us the works (Hui et al., 2019;
Liu et al., 2020) and (Flajolet & Sedgewick, 2009), respectively.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical
society, 68(3):337-404, 1950.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In 36th In-
ternational Conference on Machine Learning, ICML 2019, pp. 477-502. International Machine
Learning Society (IMLS), 2019b.
Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit sphere:
an introduction, volume 2044. Springer Science & Business Media, 2012.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629-681, 2017.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541-549, 2018.
Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. In ICLR,
2021.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems, pp. 12893-12904, 2019.
Nicholas H Bingham. Positive definite functions on spheres. In Mathematical Proceedings of the
Cambridge Philosophical Society, volume 73, pp. 145-156. Cambridge University Press, 1973.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10836-10846,
2019.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
Elliott Ward Cheney and William Allan Light. A course in approximation theory, volume 101.
American Mathematical Soc., 2009.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural
information processing systems, pp. 342-350, 2009.
Gustav Doetsch. Introduction to the theory and application of the Laplace transformation. Springer,
1974.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b.
Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-
width neural networks. arXiv preprint arXiv:2005.11879, 2020.
Philippe Flajolet and Robert Sedgewick. Analytic combinatorics. cambridge University press, 2009.
10
Published as a conference paper at ICLR 2021
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580,
2020.
Like Hui, Siyuan Ma, and Mikhail Belkin. Kernel machines beat deep neural networks on mask-
based single-channel speech enhancement. Proc. Interspeech 2019, pp. 2748-2752, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev
Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. In Advances
in Neural Information Processing Systems, pp. 14601-14610, 2019.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint
arXiv:2003.00307, 2020.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. In
International Conference on Computational Learning Theory, pp. 154-168. Springer, 2006.
Iosif Pinelis. Analyzing the decay rate of taylor series coefficients when high-order derivatives are
intractable. MathOverflow, 2020. URL https://mathoverflow.net/q/366252.
Saburou Saitoh and Yoshihiro Sawano. Theory of reproducing kernels and applications. Springer,
2016.
I J Schoenberg. Positive definite functions on spheres. Duke Mathematical Journal, 9(1):96-108,
1942.
Murray R Spiegel. Laplace transforms. McGraw-Hill New York, 1965.
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
11
Published as a conference paper at ICLR 2021
Appendices
Table of Contents
A Proofs for Neural Tangent Kernel	12
A.1 Proof of Lemma 3 ................................................. 12
A.2 Proof of Equation (4) ............................................ 12
A.3 Proof of Theorem 7 ............................................... 12
A.4 Proof of Theorem 8 ............................................... 16
B Proofs for Exponential Power Kernel	18
B.1	Proof of Lemma 9 ................................................ 18
B.2	Proof of Lemma 10 ............................................... 21
A Proofs for Neural Tangent Kernel
A.1 Proof of Lemma 3
Proof. We show it by induction. It holds when k = 0 by the initial condition (3). Assume that it
holds for some k ≥ 0, i.e., Σk (x, x) = 1. Consider k + 1. We have
Σk+1 (x, x) = κ1 (Σk (x, x)) = κ1 (1) = 1 .
□
A.2 Proof of Equation (4)
Proof. We plug Σk (x, x) = 1 into (2) and obtain
Σk(x, y) = κ1 (Σk-1 (x, y))
Nk(x, y) = Σk(x,y) + Nk-1(x, y)κ0(Σk-1(x, y)) + β2 .
Recall Σ0 (x, y) = u. By induction, we get
Σk(u) = κ(1k) (u) ,
where κ1k)(u)，κ[k)(u) = κ1(κ1(∙ ∙ ∙ κ1(κ1(u)) ∙ ∙ ∙)) is the k-th iterate of κ1(u). Then it follows
'------------------------------{z------}
k
Nk (u) = κ(1k) (u) + Nk-1 (u)κ0(κ1k-1(u)) + β2 .
□
A.3 Proof of Theorem 7
Lemma 14 and Lemma 15 demonstrate that ±1 are indeed singularities and analyze the asymptotics
for κ(1k) as z tends to ±1, respectively. Our calculation is inspired by Pinelis (2020), which only
considers k = 2.
Lemma 14. For every k ≥ 1, there exists ck (z) such that
κ(1k) (z) = z + ck(z)(1 - z)3/2 ,
where
lim ck(z)
z→1
2√2k
3π
12
Published as a conference paper at ICLR 2021
Proof. We prove by induction on k. We first prove the statement for k = 1. Let Z = 1 - reiθ
Taylor’s theorem around 1 with integral form of remainder gives
Z-w
KI(Z)= Z +	—/	2 dw,
γ π 1 - w2
where γ : [0, 1] → C is the simple straight line connecting 1 and Z taking the form γ(t) = 1 - treiθ.
It follows
Since
we have
κ1(Z) =Z+
γ
=Z+Z
γ
Z—w
π ʌ/l - w
Z—w
π√2√Γ-
■ / 1 dw
ʌ/l + W
Z—w
w W + JY ∏√2√Γ二石
•(√⅛ - 1)dw.
/Z — w	2 K--
√∖	dw = -√1 — w(w — 3z + 2)
w=1
3(i - z)3/2,
/ ∖	2^∖∕2 .	、3/2	f Z — W /
KI(Z) = Z + 1Γ(I-Z)	+ / ∏√2√r-w •(
√1+ W
- 1)dw .
We then turn to show
Zim {(1- Z)-3// • / ∏√√=-w •(√⅛w
- 1)dw = 0 .
Direct calculation gives
lim n(1 — z)-3/2 • Z Z W • ( ∕√2 — 1)dw]
z→ι V	/	JY V-WL ι √Γψ^ / ʃ
lim n (reiθT-3/ / (I- =2 (	6.-1)dt}
r—0 I	Jo	√treiθ	√2 — treiθ	J
pi — treiθ/2 - 1)dt0
lim
r→0
0.
Therefore, there exists ci(z) such that limz→ι ci(z) = 23∏2 = 0 and
κ1(Z) = Z + C1(Z)(1 - Z)3/2 .
Next, assume that the desired equation holds for some k ≥ 1. We then have
κ(1k+1)(Z) = κ1(κ(1k)(Z))
= κ1(Z + Ck(Z)(1 — Z)3/2)
= Z + Ck(Z)(1 — Z)3/2 + C1 κ(1k)(Z) • 1 — Z — Ck(Z)(1 — Z)3/2
= Z + Ck+1(Z)(1 — Z)3/2 ,
where ck+ι(z)〜 Ck(Z) + ci(k((k')(z)). Recall that when Z → 1, we have κ1k)(z) → 1 as well.
Therefore we deduce
,	,、，	，、，	，，k,、、	2√2k ,
iim Ck+ι(z) = iim Ck (Z) + iim q(kk (Z)) =	— = o.
z→1	z→1	z→1	3π
□
13
Published as a conference paper at ICLR 2021
Lemma 15. For every k ≥ 1, there exist ak ∈ R and a complex function bk(z) such that
κ(1k) (z) = ak + bk(z)(z + 1)3/2 ,
where
2 √2 k-1
a，k = κ1k)(-1) and lim bk(Z) = —— TT κ1(κj)(-1)) > 0 .
z→-1	3π
π j=1
Proof. We prove by induction on k. We first prove the statement for k = 1. Let z = -1 + reiθ.
Taylor’s theorem around -1 with integral form of remainder gives
KI(Z)=	: W 2 dw.
γπ 1- w2
where γ : [0, 1] → C is the simple straight line connecting -1 and Z taking the form γ(t) =
-1 + treiθ . Similar arguments as in the proof of Lemma 14 give
κ1(Z) = b1(Z)(Z + 1)3/2 ,
where limz→-ι bι(z) = ɪ.
Next, assume that the desired equation holds for some k ≥ 1. Define hk , κ(1k) (-1). Since κ1 is
strictly increasing on [-1, 1], κ1(-1) = 0 and κ1(1) = 1, we have h1 = 0 and hk ∈ (0, 1) for all
k > 1. Expanding κ1 around hk yields
κ1(Z) = κ1(hk) + p(Z)(Z - hk) = hk+1 + p(Z)(Z - hk) ,
where limz→hk p(Z) = κ01(hk). It follows that
κ1k+1(Z) = κ1(ak + bk(Z)(Z + 1)3/2) = hk+1 +p(κ(1k)(Z))(ak + bk(Z)(Z + 1)3/2 - hk)
= ak+1 + bk+1(Z)(Z + 1)3/2 ,
where ak+1 = hk+1 + κ01(hk)(ak - hk) and limz→-1 bk+1 (Z) = κ01(hk) limz→-1 bk(Z). By
induction, we can show that ak = hk for all k ≥ 1. Since κ01 is strictly increasing on [-1, 1],
κ01(-1) = 0, and κ01(1) = 1, we have κ01(hk) ≥ κ01(0) > 0. As a result,
zl→im-1 bk+1(Z)
2√2 YY κ1(κ"-1)) > 0 .
3π
j=1
□
In the sequel, we show that ±1 are the only dominant singularities of κ(1k) and κ(1k) is ∆-analytic at
±1 (Lemma 19).
Lemma 16. For any Z ∈ C with arg Z ∈ (0,π∕4), κι(z) ∈ H+. For any Z ∈ C with arg Z ∈
(—π∕4, 0), ki(z) ∈ H-.
Proof. The second part of the statement follows from the first according to the reflection principle.
We only prove the first part here. Let Z = reiθ with θ ∈ (0, π∕4). Taylor’s theorem with integral
form of the remainder and direct calculation give
ki(z) = κι(0)	+ k1(0)z +	j (Z —	w)κ10(w)dw =	ɪ + LZ	+ j —Z W dw ,
γ	π 2 γ π 1 - W2
where γ : [0, 1] → C is the simple straight line connecting 0 and Z taking the form γ(t) = treiθ.
Then we have
Z Z - W dw = r2e2iθ 广 ɪ - t	= dt = e2iθ Zr ： - t . dt.
JY π√1 - w2	0o π√1 - r2t2e2iθ	√q π√1 - t2e2iθ
14
Published as a conference paper at ICLR 2021
Since θ ∈ (0,π∕4), We have arg(1 - t2e2iθ) ∈ (-π, 0). Further
arg (√r-⅛) ∈ (0,π/2)	and arg (∕r π√⅛⅛dt) ∈ (0,π/2).
Noting arg(e2iθ) ∈ (0, n/2), we get
(Zz-w
Y Π√τ-W2dw) ∈ (0,π),
which gives a positive imaginary part. Combining with =(1∕π + z/2) > 0 yields the desired
statement.	□
Lemma 17. For every k ≥ 1 andε > 0, there exists δ > 0 such that κ(1k) is analytic on B1(δ) ∩H+
and B1 (δ) ∩ H- with
κ(1k) (B1 (δ) ∩H+) ⊆ B1(ε) ∩H+ ,
κ(1k) (B1 (δ) ∩H-) ⊆ B1(ε) ∩H- .
Proof. We present the proof for H+ here and that for H- can be shown similarly. We adopt an
induction argument on k.
For k = 1, κ1 is analytic on H+. Since κ1 is continuous at z = 1, for any ε > 0, there exists
0 < δ < 1/2 such that
κ1(B1(δ)∩H+) ⊆ B1(ε).
Lemma 16 implies κ1(B1(δ) ∩ H+) ⊆ H+. Combining them yields
κ1(B1(δ) ∩H+) ⊆ B1(ε) ∩H+ .	(14)
Now assume that the statement holds true for some k ≥ 1. Note that for any ε > 0, there exists
0 < δ < 1/2 such that (14) holds. Then by induction hypothesis, for this chosen δ, there exists
δ1 > 0 such that κ(1k) is analytic on B1 (δ1) ∩ H+ and
κ(1k) (B1 (δ1) ∩H+) ⊆ B1(δ) ∩H+ .
It follows
κ(1k+1) (B1 (δ1) ∩H+) ⊆ κ1 (B1 (δ) ∩H+) ⊆ B1 (ε) ∩H+ .
This completes the proof.	□
Lemma 18. ∣κι(z)∣ ≤ 1 forany |z| ≤ 1, where the equality holds ifandonly if Z = L
Proof. The Taylor series of κ1 around z = 0 is
1 . z , X	(2n - 3)!!	2n
KI(Z) = ∏ +2+⅛ (2n - 1)n!2n∏z
Therefore, for |z| ≤ 1, we have
1	|z|	∞	(2n - 3)!!	2
E(Z)l≤ ∏ + T + X (2n- l)n!2n∏lzl	≤ MU = 1.
The equality holds if and only if z = 1.
□
Lemma 19. For each k ≥ 1, there exists R > 1 such that κ(1k) is analytic on {Z ∈ C | |Z| ≤ R}∩D,
where D = C \ [1, ∞) \ (-∞, -1].
15
Published as a conference paper at ICLR 2021
Proof. For any 0 < θ < π∕2, there exists δθ > 0 such that for all |z| ≤ 1 with | arg z| ≥ θ,we have
IKI(Z)I ≤ 1 - δθ .
To see this, we use an argument similar to (Pinelis, 2020). Ifwe define φ , arg z, we have
1 + Z = ʌ ∕∣z∣2 + |z| cos φ + ɪ
π 2 V 4 π π2
1 cos θ 1
≤ 4∣+ + - + ∏2
1
2
+-----δθ ,
π
for some δθ > 0. Consider the Taylor series of κ1 around z = 0
∞
KI(Z) = ∏+2+X
n=1
(2n - 3)!!	z2n
(2n — 1)n!2nπ
We obtain
∣K1(Z)∣ ≤
1 + Z	+ X (2n	- 3)!!	∣z∣2n≤ 1	+ 1	- δθ	+ X (2n - 3)!!
π 2 Nl (2n —	1)n!2nπ 2 π Nl	(2n — 1)n!2nπ
Lemma 17 shows that there exists 0 < δ0 < 1 such that K(1k) is analytic on B1 (δ0) ∩ D. From the
argument above, we know that K1 maps A , {Z ∈ C ∣ ∣Z∣ = 1, ∣ arg Z∣ ≥ θ} to inside of the open
unit ball B0(1). Since A is compact and Lemma 18 implies that g maps B0(1) to B0(1), there exists
1 < Rθ < 1 + δ0 such that K1 maps
Aθ , ({Z ∈ C ∣ ∣Z∣ ≤ Rθ, ∣ argZ∣ ≥ θ} ∩ D) ∪ B0(1)
to Bo(1). It follows that KIk) is analytic on Aθ. Let Us pick θ ∈ (0, π∕2) such that eiθ ∈ Bι(δ0).
Then we conclude that KIk) is analytic on {z ∈ C ∣∣z∣≤ Rθ }∩ D.	□
Now we are ready to prove Theorem 7.
Proof. Since κo and κι are both analytic on D = C\ [1, ∞) \ (-∞, -1], similar arguments as in the
proof of Lemma 19 shows that K0(K(1k)(Z)) is analytic on {Z ∈ C ∣ ∣Z∣ ≤ R} ∩ D for all k ≥ 1and
some R > 1. We then show, for any k ≥ 1, there exists some Rk > 1 such that Nk(Z) is analytic
on {Z ∈ C ∣ ∣Z∣ ≤ Rk} ∩ D by induction. The function N0(Z) = Z + β2 is analytic on D. Assume
Nk-1 (Z) is analytic on {Z ∈ C ∣ ∣Z∣ ≤ Rk-1} ∩ D for some Rk-1 > 1. Recall that
Nk(Z) = K(1k)(Z) + Nk-1(Z)K0(K(1k-1)(Z)) + β2.
Then we can find some Rk > 1 such that Nk (z) is analytic on {z ∈ C ∣ ∣z∣ ≤ Rk} ∩ D.	□
A.4 Proof of Theorem 8
Proof. We first analyze the behavior of Nk(Z) as Z → 1 for any k ≥ 1. We aim to show, for any
k ≥ 1, there exists a sequence of complex functions Pk (z) with limz→ι Pk (z) = - √2(1 + β2)k(k +
1)∕2π such that
Nk(z) = (k + 1)(z + β2) + Pk(z)√1 - Z .	(15)
We prove by induction on k. Recall
K (Z) = ∏ + i log(z + i√1 - Z2)
0π
The fundamental theorem of calculus then gives for any Z ∈ D
KO(Z) = 1+ / —九1	、dw,
γ π 1 - w2
16
Published as a conference paper at ICLR 2021
where Y : [0,1] → C is the simple straight line connecting 1 and z. As Z → 1, We have √-z2 〜
√2√1-z. Therefore, similar arguments as in the proof of Lemma 14 give
κo(z) = 1 + h(z)√1 - Z ,
where lim2→ι h(z) = — *. Combining with Lemma 14 further gives, for any k ≥ 1
κo(κf)(z)) = 1 + h(κ1k) (z)) qr—T—ck(z)(1—7)3/2 = 1 + hk (z)√1 - Z ,
where limz→ι hk(z)= -与.For k = 1, we then have
NI(Z) = κι(z) + (Z + β2)κo(z) + β2 = z + dι(z)(1 — z)3/2 + (Z + β2)(1 + h(z)√1 — Z) + β2
=2(z + β2) + pi(z)√1 - Z ,
where lim^→ι di(Z)= 23∏2 and lim^→ιpi(z) = -√2(1 + β2)∕π. Assume Nk-I(Z) = k(Z +
β2) + pk-ι(Z)√1 - Z with limz→ιpk-ι(Z) = -√2(1 + β2)k(k - 1)∕(2π). We further have
Nk (z) = κik)(Z) + Nk-I(Z)κo(κikT)(Z)) + β2
=Z + dk(z)(1 — z)3/2 + (k(Z + β2) + Pk-i(z)√1 — Z) (1 + hk-i(z)√1 - Z) + β2
=(k + I)(Z + β2) + (Pk-I(Z) + k ∙ hk-1(Z)(Z + β2))√1 - Z
=(k + 1)(z + β2) + Pk(z)√1 — z .
where we set Pk(z) = Pk-I(Z) + k ∙ hk-ι(z)(z + β2) and dk(z) → 23∏2k, hk-i(z) → -咚 as
Z → 1. Moreover, we have
zli→miPk(Z)
Z鸣｛pk-1(Z)+ k ∙ hk-1(Z)(Z + β
√2(1 + β2)k(k - 1)
2∏
-k ∙ —(1 + β2)
π
√2(1 + β 2)k(k + 1)
2∏
which is desired. This proves (15).
Next we study thebehaviorofNk(Z) as Z → -1 for any k ≥ 1. We aim to show, for any k ≥ 1, there
exists a sequence of complex functions qk(z) with lim^→-ι qk(z) = √2(β2 - 1) Qk-I κo(aj)∕π
and ak , κ(ik)(-1) as defined in Lemma 15 such that
Nk (z) = Nk ( -1) + qk (Z) ʌ/1 + Z .
(16)
We again adopt induction on k. Taylor’s theorem gives
κ0(Z) = κ0(ak) +rk(Z)(Z - ak) ,
where limz→ak rk(Z) = κ00(ak) > 0. Combining with Lemma 15 further gives, for any k ≥ 1
κo(κ1k)(z)) = κo(ak) + rk(κ1k)(z))bk(z)(z + 1)3/2 = κo(ak) + rk(z)(z + 1)3/2 ,
where bk (z) → 2√2 Qj=I KKak) and rk (z) → 2√2 κ0(ak) Qj=I κ1(ak) > 0 as Z → -1 by
Lemma 15. For k = 1, the fundamental theorem of calculus gives for any Z ∈ D
κ0(Z)
/ π√⅛ dw,
where γ : [0, 1] → C is the simple straight line connecting -1 and Z. As Z → -1, we have
√1-Z2 〜 √2√+z. Therefore, similar arguments as in the proof of Lemma 14 give
κo(z) = g(z)√1 + Z ,
17
Published as a conference paper at ICLR 2021
where g(z) → 3∏2 as Z → -1. We then have
N1(z) = κ1(z) + (z + β2)κ0(z) + β2
=aι + bι(z)(z + 1)3/2 + (Z + β2)g(z)√1 + Z + β2
=(aι + β2) + qι(z)√1 + Z
=Nι(-1) + qι(z)√1 + z ,
where NI(T) = aι + β2 limz→-ι qι(z) = √∏2(β2 - 1). Assume Nk-ι(z) = Nk-ι(-1) +
qk-ι(z)√1 + z with limz→-ι qk-ι(z) = √2(β2 - 1) Q：-： κo(aj)∕π. We further have
Nk(Z) = κ(1k)(Z) + Nk-1(Z)κ0(κ(1k-1)(Z)) + β2
=ak + bk (Z)(Z + I)3/2 + Nk-I(Z) (κθ(ak-I) + rk-1(Z)(Z + I)3/2) + e
=(ak + β2 + Nk-I(Z)κθ(ak-l)) + (bk (Z) + Nk-I(Z)rk-1(Z))(Z + 1)3/2
=(ak + β2 + Nk-l(-1)κθ(ak-I)) + qk-1 (Z)K0(ak-l)√Z + 1
+ (bk (z) + Nk-I(Z)rk-i(Z))(Z + 1)3/2
=Nk (-1) + qk-l(z)κθ(ak-l)√Z + 1 + (bk (z) + Nk-I (Z)rk-1 (Z))(Z + 1)3/2
=Nk(-1) + qk(Z)√1 + Z ,
where we use the induction assumption in the fourth equation, use the fact Nk (-1) = ak + β2 +
Nk-1(-1)κ0(ak-1) in the fifth equation and define
qk (z) = qk-i(z)κo(ak-i) + (bk (z) + Nk-I(Zyrk-i(Zy)(Z + 1)
in the last equation. We also have
lim qk(Z) = lim qk-1(Z)κ0(ak-1) + (bk (Z) + Nk-1(Z)rrk-1(Z)) (Z + 1)
z→-1	z→-1
= lim qk-1(Z)κ0(ak-1)
√2(β2 - I) YT	∕ 、
=-----∏-----H κ0(aj) ,
π	j=1
which is desired. This proves (16).
Finally, according to Theorem 7, combining (15) and (16), applying (Flajolet & Sedgewick, 2009,
Theorem VI.5) with ρ = 1, r = 2, τ(Z) = (1 - Z)1/2, ζ1 = 1, ζ2 = -1, σ1(Z) = (k + 1)(Z + β2),
σ2(z) = Nk(-1), D = {z ∈ C | |z| ≤ Rk}∩ D, we conclude [zn]Nk(z) = O(n-3/2).	□
B	Proofs for Exponential Power Kernel
B.1 Proof of Lemma 9
Proof. According to (Doetsch, 1974, Theorem 28.2), we have, for 0 < a < 1,
1	x0+iT
f (t) =--- lim	exp(ts — sa)ds	(x0 ≥ 0).
2πi T→+∞ x0-iT
Also (Doetsch, 1974, Theorem 28.2) implies that f(t) is continuous in -∞ < t < +∞ and f(0) =
0.
Next we explicitly calculate f(t) using Bromwich contour integral. We denote each part of the
BromWich contour by Γo,..., Γ5 as depicted in Fig. 2. Denote the radius of the outer and inner arc
by R and r. When T → ∞, we have R =，T2 + x2 → ∞. Also we let r → 0 and Γ2, Γ4 tend to
(-∞, 0] from above and below respectively in the limit. By the residue theorem, we have
+ . . . +	exp(ts - sa)ds = 0 ,
18
Published as a conference paper at ICLR 2021
Figure 2: Bromwich contour that circumvents the branch cut (-∞, 0]
which implies
x0 +iT
lim	exp(ts — sa)ds = lim exp(ts — sa)ds
T→∞ x0-iT	T→∞ Γ0
= — lim + . . . +
exp(ts - sa)ds
, - lim(I1 + . . . + I5 ) ,
where the last two limits are taken as R → ∞, r → 0, and Γ2, Γ4 tend to (-∞, 0]. We then calculate
each part separately.
Part I:	We calculate the parts for Γ1 and Γ5. We follow the similar idea as in the proof of (Spiegel,
1965, Theorem 7.1). Along Γ1, since s = Reiθ with θ0 ≤ θ ≤ π, θ0 = arccoS(x0/R),
I1
=/"2 eReiθte-RaeiaθiReiθdθ + Z" eReiθte-RaeiaθiReiθdθ
θθο	J π∕2
, I11 + I12 .
For I11,
fπ/2
|I11| ≤
θ0
Γπ/2
≤ θ0
|eRt cosθ| ∙ Ie-Ra Cos(M ∣Rdθ
gRt cos θ ∙ e-Ra cos(aπ/2) Rdθ
V R
- Ra cos(a∏/2)
Ra cos(aπ∕2)
Z / eRt cos θdθ
θ0
Zφ0 eRt sin φdφ ,
0
R
where φo = π∕2 — θo = arcsin(x0/R). Since Sin φ ≤ Sin φo ≤ x0/R, We have
R
R
|I11| ≤
Ra cos(aπ∕2)
φ0ex0t
Ra cos(aπ∕2)
ex0t arcSin(x0 /R) .
As R → ∞, we have limR→∞ I11 = 0.
For I12,
|I12| ≤ π eRt cos θ ∙ e-Ra cos(aθ)Rdθ.
19
Published as a conference paper at ICLR 2021
First, We consider the case 0 < a < 1/2. We have aθ ≤ aπ < π∕2 and cos(aθ) ≥ cos(aπ) > 0. It
follows
π eRt cos θ ∙ e-Ra cos(aθ)Rdθ
≤ Re-Ra cos(aπ) π eRt cos θdθ
=Re-Ra CoMan) Z / e-Rt sin φdφ
0
≤ Re-Ra CoMan) /	e-2Rtφ∕ndφ
0
_ Q-Ra cos(aπ) π(1 - e-Rt)
=	2t	,
where in the last inequality we use the fact Sin φ ≥ 2φ∕π for φ ∈ [0,π∕2]. Thus, limR→∞ I12 = 0.
Next, We consider 1/2 ≤ a < 1. Define
p(θ) , Rt cos θ - Ra cos(aθ) .
We then have its second derivative as follows
p00(θ) = a2Ra cos(aθ) - Rt cos(θ) .
Choose δ to be a fixed constant in (0, ∏ (ɪ -1)). Since a ≥ 1/2, then δ <π∕2. If π∕2 + δ ≤ θ ≤ π,
p00(θ) ≥ —a2Ra — Rt cos(π∕2 + δ) = -O2Ra + Rt sin(δ).
Since a < 1, there exists some large R1 > 0 such that p00 (θ) ≥ -a2Ra + Rt sin(δ) > 0 holds for
all R> Ri. If π∕2 ≤ θ<π∕2 + δ,
p00(θ) ≥ a2Ra cos(a(π∕2 + δ)).
Since a(π∕2 + δ) < π∕2 by the choice of δ, we get cos(a(π∕2 + δ)) > 0. Then we also have
p00 (θ) > 0. Therefore, if R > R1, p(θ) is convex in θ ∈ [π∕2, π]. As a result, we get
max p(θ) ≤ max{p(π∕2), p(π)} .
θ∈[π∕2,π]
Write
Then we have
h(R,θ)，ReRt Cosθ ∙ e-Ra cos(aθ) = Rep(θ) .
max h(R, θ) ≤ max{h(R, π∕2), h(R, π)}
θ∈[n∕2,n]
=Rmax{e-Ra Cos(πa),e-Ra Cos(na)-Rt}
≤ Rmax{e-Ra Cos(等),eRa-Rt},
which goes to 0 as R → ∞. Therefore, h(R, θ) converges to 0 uniformly (as a function of θ ∈
[π∕2, π] with index R), which implies
lim
R→∞
n h(R, θ)dθ
0.
Hence, we establish limR→∞ I12 = 0 for all a ∈ (0, 1).
Combining these above, we conclude limR→∞ I1 = 0. Similarly, limR→∞ I5 = 0.
20
Published as a conference paper at ICLR 2021
Part II:	We calculate the parts for Γ2 and Γ4 . By the dominated convergence theorem, we have, for
y>0
lim I2
R→∞
r→0
y→0+
-r+iy
lim	exp(ts) exp(-sa)ds
Rr→→∞0 -R+iy
y→0+
-r+iy	∞
lim	exp(ts)
R→∞
r→0 -R+iy	k=0
y→0+
(-1)ksak
k!
ds
∞ ( 1)k
lim X	[
R→∞	k!
r→0 k=0	-
-r+iy
exp(ts)sakds .
y→0+
-R+iy
We then calculate the limit of the summand.
-r+iy
lim
Rr→→∞0 -R+iy
y→0+
exp(ts)sak ds
Z0
-∞
etx ∙ [(-x)eiπ]akdx
∞
0
1
e-txxakeiπakdx
tak+
1 Γ(ak + 1)eiπak
Similarly, we obtain the corresponding part in Γ4 :
-R+iy
lim
Rr→→∞0 -r+iy
y→0-
0
exp(ts)sak ds = -
-∞
etx ∙ [(-x)e-iπ]akdx
1
—
tak+
1 Γ(ak + 1)e-iπak
Combining the parts of Γ2 and Γ4 together, we get
∞
lim(I2 + I4) =X
k=0
(-1)k 2iΓ(ak + 1) sin(πak)
k!
tak+1
Part III:	We get the limit for Γ3 is 0 as r → 0.
Combining the three parts above, we conclude
(-1)k+1 2iΓ(ak + 1) sin(πak)
k!
tak+1
∞
1 X
π
k=0
(-1)k+1Γ(ak + 1) sin(πak)
k!tak+1
□
B.2 Proof of Lemma 10
Proof. Euler’s reflection formula gives
Γ(1 + ka)Γ(-ka)
-π
sin(πka) ,
ka ∈/ Z .
21
Published as a conference paper at ICLR 2021
According to Lemma 9, We have
1	∞
f (t) = 1 X
π乙」
k=0
(—1)k+1Γ(αk + 1) sin(παk)
k!tαk+1
∞
X
k=0
(-I)k
k!tak+1Γ(-ak)
q-1 ∞
XX
j = 1 n=0
(—1)nq+j
(nq + j )!tα(nq+j)+1Γ(-α(nq + j))
(17)
First, we show that the series in (17) converges absolutely:
q-1 ∞
XX
j = 1 n=0
|t|-a(nq+j)-1
(nq + j )!∣Γ(-α(nq + j))|
q-1
X
j=1
1 X	∣t∣-np
|t|aj+1 n=0 (nq+j)!rja(nq+j))|
q-1
X
j=1
1 X |t|-np Q巴(aj + i)
|力3+1『(一aj)| n=0	(nq + j)!
(18)
The inner summation in (18) is a power series in |t|-p. We would like to show that its radius of
convergence is ∞. Define
b = nnp1(aj+i)
n	(nq + j)!.
We have
bn+1
bn
Qnp<i≤(n+1)p(aj+ i) =	QP=1 j⅞
Qnq<i≤(n+1)q (j + i)	Qi=+q+p+1(j + i)
1	<	1	0
Q(=+q+p+1(j + i) ≤ j+nq+P+1)q-P T .
As a result, the radius of convergence is ∞. Then we have
q-1
f(t) = X
j=1
taj+1Γ(-aj)
∞
X
n=0
(—1)n(p+q)+jt-pn Qnp1(aj + i)
(nq + j)!
q-1
X
j=1
tM+1Γ(-aj)
(
j+X
j n=1
∖ J
∖
(—1)n(P+q)+jt-pn QnpI (aj + i)
(nq + j)!
≤
1
1
{z
A
✓
/
Notice that the quantity A goes to 0 as t → +∞. Therefore we deduce
(-1)j
T-■—:-----〜—
taj+1j!Γ(-aj)	ta+1Γ(-a),
1
as t —→ +∞.
□
22