Published as a conference paper at ICLR 2021
Efficient Wasserstein Natural Gradients for
Reinforcement Learning
Ted Moskovitz*1, Michael Arbel*1, Ferenc Huszar1,2 & Arthur Gretton1
1GatsbyUnit, UCL 2University of Cambridge
Ab stract
A novel optimization approach is proposed for application to policy gradient meth-
ods and evolution strategies for reinforcement learning (RL). The procedure uses a
computationally efficient Wasserstein natural gradient (WNG) descent that takes
advantage of the geometry induced by a Wasserstein penalty to speed optimiza-
tion. This method follows the recent theme in RL of including a divergence
penalty in the objective to establish a trust region. Experiments on challenging
tasks demonstrate improvements in both computational cost and performance over
advanced baselines.
1	Introduction
Defining efficient optimization algorithms for reinforcement learning (RL) that are able to leverage
a meaningful measure of similarity between policies is a longstanding and challenging problem
(Lee & Popovic, 2010; Meyerson et al., 2016; Conti et al., 2018b). Many such works rely on
similarity measures such as the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) to
define procedures for updating the policy of an agent as it interacts with the environment. These are
generally motivated by the need to maintain a small variation in the KL between successive updates
in an off-policy context to control the variance of the importance weights used in fthe estimation
of the gradient. This includes work by Kakade (2002) and Schulman et al. (2015), who propose to
use the Fisher Natural Gradient (Amari, 1997) as a way to update policies, using local geometric
information to allow larger steps in directions where policies vary less; and the work of Schulman
et al. (2017), which relies on a global measure of proximity using a soft KL penalty to the objective.
While those methods achieve impressive performance, and the choice of the KL is well-motivated,
one can still ask if it is possible to include information about the behavior of policies when measuring
similarity, and whether this could lead to more efficient algorithms. Pacchiano et al. (2019) provide a
first insight into this question, representing policies using behavioral distributions which incorporate
information about the outcome of the policies in the environment. The Wasserstein Distance (WD)
(Villani, 2016) between those behavioral distributions is then used as a similarity measure between
their corresponding policies. They further propose to use such behavioral similarity as a global soft
penalty to the total objective. Hence, like the KL penalty, proximity between policies is measured
globally, and does not necessarily exploit the local geometry defined by the behavioral embeddings.
In this work, we show that substantial improvements can be achieved by taking into account the local
behavior of policies. We introduce new, efficient optimization methods for RL that incorporate the
local geometry defined by the behavioral distributions for both policy gradient (PG) and evolution
strategies (ES) approaches. Our main contributions are as follows:
1-	We leverage recent work in (Li & Montufar, 2018a;b; Li, 2018; Li & Zhao, 2019; Chen & Li,
2018) which introduces the notion of the Wasserstein Information Matrix to define a local behavioral
similarity measure between policies. This allows us to identify the Wasserstein Natural Gradient
(WNG) as a key ingredient for optimization methods that rely on the local behavior of policies. To
enable efficient estimation of WNG, we build on the recent work of Arbel et al. (2020), and further
extend it to cases where the re-parameterization trick is not applicable, but only the score function
of the model is available.
* Denotes equal contribution. Correspondence: ted@gatsby.ucl.ac.uk.
1
Published as a conference paper at ICLR 2021
2-	This allows us to introduce two novel methods: Wasserstein natural policy gradients (WNPG)
and Wasserstein natural evolution strategies (WNES) which use the local behavioral structure of
policies through WNG and can be easily incorporated into standard RL optimization routines. When
combined in addition with a global behavioral similarity such as a WD penalty, we show substantial
improvement over using the penalty alone without access to local information. We find that such
WNG-based methods are especially useful on tasks in which initial progress is difficult.
3-	Finally, we demonstrate, to our knowledge, the first in-depth comparative analysis of the FNG
and WNG, highlighting a clear interpretable advantage of using WNG over FNG on tasks where the
optimal solution is deterministic. This scenario arises frequently in ES and in policy optimization
for MDPs (Puterman, 2010). This suggests that WNG could be a powerful tool for this class of
problems, especially when reaching accurate solutions quickly is crucial.
In Section 2, we present a brief review of policy gradient approaches and the role of divergence
measures as regularization penalties. In Section 3 we introduce the WNG and detail its relationship
with the FNG and the use of Wasserstein penalties, and in Section 4 we derive practical algorithms
for applying the WNG to PG and ES. Section 5 contains our empirical results.
2	Background
Policy Gradient (PG) methods directly parametrize a policy πθ , optimizing the parameter θ using
stochastic gradient ascent on the expected total discounted reward R(θ). An estimate gk of the
gradient of R(θ) at θk can be computed by differentiating a surrogate objective Lθ which often
comes in two flavors, depending on whether training is on-policy (left) or off-policy (right):
L⑻=E [log∏θ(at∣st)Ati, or	L⑻=E [ nθ*)、At1	(1)
L	」	L∏θk (at∣st)	_
The expectation E is an empirical average over N trajectories τi = (si1, ai1, r1i , ..., siT, aiT , rTi ) of
state-action-rewards obtained by simulating from the environment using ∏θk. The scalar At is an
estimator of the advantage function and can be computed, for instance, using
At= r + YV(st+ι) - V(St)	(2)
where γ ∈ [0, 1) is a discount factor and V is the value function often learned as a parametric
function via temporal difference learning (Sutton & Barto, 2018). Reusing trajectories can reduce
the computational cost at the expense of increased variance of the gradient estimator (Schulman
et al., 2017). Indeed, performing multiple policy updates while using trajectories from an older
policy πθold means that the current policy πθ can drift away from the older policy. On the other hand,
the objective is obtained as an expectation under πθ for which fresh trajectories are not available.
Instead, the objective is estimated using importance sampling (by re-weighting the old trajectories
according to importance weights ∏θ∕∏θoid ). When ∏θ is too far from ∏θold, the importance weight
can have a large variance. This can lead to a drastic degradation of PefOrmanCe if done naively
(Schulman et al., 2017). KL-based policy optimization (PO) aims at addressing these limitations.
KL-based PO methods ensure that the policy does not change substantially between successive
updates, where change is measured by the KL divergence between the resulting action distributions.
The general idea is to add either a hard KL constraint, as in TRPO (Schulman et al., 2015), or a soft
constraint, as in PPO (Schulman et al., 2017), to encourage proximity between policies. In the first
case, TRPO recovers the FNG with a step-size further adjusted using line-search to enforce the hard
constraint. The FNG permits larger steps in directions where policy changes the least, thus reducing
the number of updates required for optimization. In the second case, the soft constraint leads to an
objective of the form:
maximize© L(θ) - βE [KL(∏θ% (∙∣sj∏θ (∙∣st))].	⑶
The KL penalty prevents the updates from deviating too far from the current policy πθk , thereby
controlling the variance of the gradient estimator. This allows making multiple steps with the same
simulated trajectories without degradation of performance. While both methods take into account
the proximity between policies as measured using the KL, they do not take into account the behavior
of such policies in the environment. Exploiting such information can greatly improve performance.
2
Published as a conference paper at ICLR 2021
Behavior-Guided Policy Optimization. Motivated by the idea that policies can differ substantially
as measured by their KL divergence but still behave similarly in the environment, Pacchiano et al.
(2019) recently proposed to use a notion of proximity in behavior between policies for PO. Exploit-
ing similarity in behavior during optimization allows to take larger steps in directions where policies
behave similarly despite having a large KL divergence. To capture a sense of global behavior, they
define a behavioral embedding map (BEM) Φ that maps every trajectory τ to a behavior variable
X = Φ(τ) belonging to some embedding space E. The behavior variable X provides a simple yet
meaningful representation of each the trajectory τ. As a random variable, X is distributed according
to a distribution qθ , called the behavior distribution. Examples of Φ include simply returning the
final state of a trajectory (Φ(τ) = sT) or its concatenated actions (Φ(τ) = [a0, . . . , aT]). Proximity
between two policies πθ and πθ0 is then measured using the Wasserstein distance between their be-
havior distributions qθ and qθ0. Although, the KL could also be used in some cases, the Wasserstein
distance has the advantage of being well-defined even for distributions with non-overlapping sup-
port, therefore allowing more freedom in choosing the embedding Φ (see Section 3.1). This leads to
a penalized objective that regulates behavioral proximity:
β
maximize© L(θ) - ,W2(qθk,q§),
(4)
where β ∈ R is a hyper-parameter controlling the strength of the regularization. To compute the
penalty, Pacchiano et al. (2019) use an iterative method from Genevay et al. (2016). This procedure
is highly accurate when the Wasserstein distance changes slowly between successive updates, as
ensured when β is large. At the same time, larger values for β also mean that the policy is updated
using smaller steps, which can impede convergence. An optimal trade-off between the rate of con-
vergence and the precision of the estimated Wasserstein distance can be achieved using an adaptive
choice ofβ as done in the case of PPO Schulman et al. (2017). Fora finite value of β, the penalty ac-
counts for global proximity in behavior and doesn’t explicitly exploit the local geometry induced by
the BEM, which can further improve convergence. We introduce an efficient method that explicitly
exploits the local geometry induced by the BEM through the Wasserstein Natural gradient (WNG),
leading to gains in performance at a reduced computational cost. When global proximity is impor-
tant to the task, we show that using the Wasserstein penalty in Equation (4) and optimizing it using
the WNG yields more efficient updates, thus converging faster than simply optimizing Equation (4)
using standard gradients.
3	The Wasserstein Natural Gradient
The Wasserstein natural gradient (WNG) (Li & Montufar, 2018a;b) corresponds to the steepest-
ascent direction ofan objective within a trust region defined by the local behavior of the Wasserstein-
2 distance (W2). The W2 between two nearby densities qθ and qθ+u can be approximated by
computing the average cost of moving every sample X from qθ to a new sample X0 approxi-
mately distributed according to q©+u using an optimal vector field of the form Vχfu(χ) so that
X0 = X + Vxfu(X) (see Figure 6). Optimality of Vxfu is defined as a trade-off between accu-
rately moving mass from qθ to qθ+u and reducing the transport cost measured by the average squared
norm of Vxfu
fpvθEqθ [fU(X)]> U - 2Eqθ [kvxfu(x)k2]
(5)
where the optimization is over a suitable set of smooth real valued functions on E . Hence, the
optimal function fu solving Equation (5) defines the optimal vector field Vxfu (x). Proposition 1
makes this intuition more precise and defines the Wasserstein Information Matrix.
Proposition 1 (Adapted from Defintion 3 Li & Zhao (2019)) The second-order Taylor expansion
of W2 between two nearby parametric probability distributions qθ and qθ+u is given by
W22(qθ, qθ+u) = u>G(θ)u + o(kuk2)
(6)
where G(θ) is the Wasserstein Information Matrix (WIM), with components in a basis (e1, ..., ep)
Gj,j0(θ) =Eqθ Vxfj(X)>Vxfj0(X)
(7)
The functions fj solve Equation (5) with u chosen as ej . Moreover, for any given u, the solution fu
to Equation (5) satisfies Eθ[kVxfu(X)k2] = u>G(θ)u.
3
Published as a conference paper at ICLR 2021
- - -
Ooo
111
J0Jij
-Fisher NGD
—KL-penalty
---SGD
I''' "iδ2 ........Io3 ''
Iterations
(e) diagonal parametrization
O3O2
1 1
Suo-Ws-
----WNGbX
Fisher NgS
----l∕l∕2-penalty
Figure 1: Different optimization methods using an objective L(θ) = Eqθ [ψ(x)] where qθ is a
gaussian of 100 dimensions with parameters θ = (μ,v). Here μ in bold is the mean vector, V
parameterizes the covariance matrix Σ, which is chosen to be diagonal. Two parameterizations for
the covariance matrix are considered: Σii = evi (log-diagonal) and Σii = vi (diagonal). ψ(x) is the
sum of sinc functions over all dimensions. Training is up to 4000 iterations, with λ = .9 and β = .1
unless they are varied. In Figure 1 (c), σ and μ refer to the Std of the first component of the gaussian
σ = √Σ11 and μ = μ1. More details about the experimental setting are provided in Appendix D.3.
—KL-penalty
SGD
When qθ and qθ+u are the behavioral embedding distributions of two policies πθ and πθ+u, the
function fu allows to transport behavior from a policy πθ to a behavior as close as possible to πθ+u
with the least cost. We thus refer to fu as the behavioral transport function. The function fu
determines how hard it is to change behavior locally from policy πθ in a direction u, thus providing
a tool to find update directions u with either maximal or minimal change in behavior.
Probing all directions in a basis (e1, ..., ep) of parameters allows us to construct the WIM G(θ)
in Equation (7) which summarizes proximity in behavior along all possible directions u using
u>G(θ)u = Eqθ[kVxfu(X)k2]. For an objective L(θ), such as the expected total reward of a
policy, the Wasserstein natural gradient (WNG) is then defined as the direction u that locally in-
creases L(θ +u) the most with the least change in behavior as measured by fu. Formally, the WNG
is related to the usual Euclidean gradient g = VθL(θ) by
gW = argmax2g>u - u>G(θ)u.	(8)
u
From Equation (8), the WNG can be expressed in closed-form in terms of G(θ) and g as gW =
G-1 (θ)g. Hence, WNG ascent is simply performed using the update equation θk+1 = θk + λgkW .
We’ll see in Section 4 how to estimate WNG efficiently without storing or explicitly inverting the
matrix G. Next, we discuss the advantages of using WNG over other methods.
3.1	Why use the Wasserstein Natural Gradient?
To illustrate the advantages of the WNG, we consider a simple setting where the objective is of the
form L(θ) = Eqθ [ψ(x)], with qθ being a gaussian distribution. The optimal solution in this example
is a deterministic point mass located at the global optimum x? of the function ψ(x). This situation
arises systematically in the context of ES when using a gaussian noise distribution with learnable
mean and variance. Moreover, the optimal policy of a Markov Decision Processes (MDP) is neces-
sarily deterministic (Puterman, 2010). Thus, despite its simplicity, this example allows us to obtain
closed-form expressions for all methods while capturing a crucial property in many RL problems
(deterministic optimal policies) which, as we will see, results in differences in performance.
4
Published as a conference paper at ICLR 2021
Wasserstein natural gradient vs Fisher natural gradient While Figure 1 (c) shows that both
methods seem to reach the same solution, a closer inspection of the loss, as shown in Figure 1 (d) and
(e) for two different parameterizations of qθ, shows that the FNG is faster at first, then slows down
to reach a final error of 10-4. On the other hand, WNG is slower at first then transitions suddenly
to an error of 10-8. The optimal solution being deterministic, the variance of the gaussian qθ needs
to shrink to 0. In this case, the KL blows up, while the W2 distance remains finite. As the natural
gradient methods are derived from those two divergences (Theorem 2 of Appendix B), they inherit
the same behavior. This explains why, unlike the WNG, the FNG doesn’t achieve the error of 10-8.
Beyond this example, when the policy πθ is defined only implicitly using a generative network, as
in Tang & Agrawal (2019), the FNG and KL penalty are ill-defined since πθk and πθk+1 might have
non-overlapping supports. However, the WNG remains well-defined (see Arbel et al. (2020)) and
allows for more flexibility in representing policies, such as with behavioral embeddings.
Wasserstein penalty vs Wasserstein natural gradient The Wasserstein penalty Equation (4) en-
courages global proximity between updates qθk . For small values of the penalty parameter β , the
method behaves like standard gradient descent (Figure 1 (a)). As β increases, the penalty encourages
more local updates and thus incorporates more information about the local geometry defined by qθ .
In fact, it recovers the WNG direction (Theorem 2 of Appendix B) albeit with an infinitely small
step-size which is detrimental to convergence of the algorithm. To avoid slowing-down, there is an
intricate balance between the step-size and penalty β that needs to be maintained (Schulman et al.,
2017). All of these issues are avoided when directly using the WNG, as shown in Figure 1 (a), which
performs the best and tolerates the widest range of step-sizes Figure 1 (f). Moreover, when using the
log-diagonal parameterization as in Figure 1 (d,a), the WNGD (in red) achieves an error of 1e-8,
while W2-penalty achieves a larger error of order 1e-0 for various values of the β. When using the
diagonal parameterization instead, as shown in Figure 1 (e), both methods achieve a similar error
of 1e-6. This discrepancy in performance highlights the robustness of WNG to parameterization of
the model.
Combining WNG and a Wasserstein penalty. The global proximity encouraged by a W2 penalty
can be useful on its own, for instance, to explicitly guarantee policy improvement as in (Pacchiano
et al., 2019, Theorem 5.1). However, this requires estimating the W2 at every iteration, which can be
costly. Using WNG instead of the usual gradient can yield more efficient updates, thus reducing the
number of time W2 needs to be estimated. The speed-up can be understood as performing second-
order optimization on the W2 penalty since the WNG arises precisely from a second-order expansion
of the W2 distance, as shown in Section 3 (See also Example 2 in Arbel et al. (2020)).
4	Policy Optimization using B ehavioral Geometry
We now present practical algorithms to exploit the behavioral geometry induced by the embeddings
Φ. We begin by describing how to efficiently estimate the WNG.
Efficient estimation of the WNG can be performed using kernel methods, as shown in Arbel
et al. (2020) in the case where the re-parametrization trick is applicable. This is the case, if for
instance, the behavioral variable is the concatenation of actions X = [a0, ..., aT] and if actions are
sampled from a gaussian with mean and variance parameterized by a neural network, as is often
done in practice for real-valued actions. Then X can be expressed as X = Bθ (Z) where Bθ is a
known function and Z is an input sample consisting in the concatenation of states [s0, ..., sT] and
the gaussian noise used to generate the actions. However, the proposed algorithm is not readily
applicable if for instance the behavioral variable X is a function of the reward.
We now introduce a procedure that extends the previous method to more general cases, including
those where only the score Vθ log qθ is available without an explicit re-parametrization trick. The
core idea is to approximate the functions fej defining G(θk ) in Equation (7) using a linear combi-
nations of user-specified basis functions (h1 (x), ..., hM (x)):
M
fej (X) = E αmhm(x),	⑼
m=1
The number M controls the computational cost of the estimation and is typically chosen on the order
of M = 10. The basis can be chosen to be data-dependent using kernel methods. More precisely,
5
Published as a conference paper at ICLR 2021
Algorithm 1: Wasserstein Natural Policy Gradient
1:	Input Initial policy πθ0
2:	for iteration k = 1, 2, ... do
3:	Obtain N rollouts {τ}nN=1 of length T using policy πθk
4:	Compute loss L(θk ) in a forward pass
5:	Compute gradient gk in the backward pass on L(θk)
6:	Compute Behavioral embeddings {Xn = Φ(τn)}nN=1
7:	Compute WNG ^W using Algorithm 3 with samples {Xn}N=ι and gradient estimate gk.
8:	Update policy using: θk+ι = θk + λgW.
9:	end for * V
we use the same approach as in Arbel et al. (2020), where we first subsample M data-points Ym
from a batch of N variables Xn and M indices im from {1, ..., d} where d is the dimension of Xn.
Then, each basis can of the form hm(x) = ∂imK(Ym, x) where K is a positive semi-definite kernel,
such as the gaussian kernel K(x, y) = exp(- kx-yk ). This choice of basis allows US to provide
guarantees for functions fj in terms of the batch size N and the number of basis points M (Arbel
et al., 2020, Theorem 7). Plugging-in each fj in the transport cost problem Equation (5) yields a
quadratic problem of dimension M in the coefficients αj :
maximizeαj 2J.,j αj - (αj)>Lαj
where L is a square matrix of size M × M independent of the index j and J is a Jacobian matrix
of shape M × P with rows given by Jmg = VθEqθfc [hm(X)]. There are two expressions for J,
depending on the applicability of the re-parametrization trick or the availability of the score
一	ʌ r_ 一	_____ 一， 一…	一	^	。_ 一	____ ______ ....
Jm,. = E qθ [Vχhm(X )Vθ Bθ (Z)] or Jm,. = E q@ [Vθ log qθ (X )hm(X )]	(10)
Computing J can be done efficiently for moderate size M by first computing a surrogate vector of
V of size M whose Jacobian recovers J using automatic differentiation software:
- -	^	。一	，一…	_ -	^	C	，一一	，一」	....
Vm = Eqθ [hm(Xn)] ,	or	Vm = E qθ [log qθ (Xn )hm(Xn)] .	(11)
The optimal coefficients αj are then simply expressed as α = Lt J. Plugging-in the optimal func-
tions in the expression of the Wasserstein Information Matrix (Equation (7)), yields a low rank
approximation of G of the form G = J>LtJ. By adding a small diagonal perturbation matrix eI,
it is possible efficiently compute (G + eI) 1^ using a generalized Woodbury matrix identity which
yields an estimator for the Wasserstein Natural gradient
产=1 (^ - J> (J J> + eL)t Jgy	(12)
The pseudo-inverse is only computed for a matrix of size M. Using the Jacobian-vector product,
Equation (12) can be computed without storing large matrices G as shown in Algorithm 3.
Wasserstein Natural Policy Gradient (WNPG). It is possible to incorporate local information
about the behavior of a policy in standard algorithms for policy gradient as summarized in Algo-
rithm 1. In its simplest form, one first needs to compute the gradient gk of the objective L(θk) using,
for instance, the REINFORCE estimator computed using N trajectories τn. The trajectories are then
used to compute the BEMS which are fed as input, along with the gradient gk to get an estimate of
the WNG gkW . Finally, the policy can be updated in the direction of gkW. Algorithm 1 can also be
used in combination with an explicit W2 penalty to control non-local changes in behavior of the
policy thus ensuring a policy improvement property as in (Pacchiano et al., 2019, Theorem 5.1). In
that case, WNG enhances convergence by acting as a second-order optimizer, as discussed in Sec-
tion 3.1. The standard gradient gk in Algorithm 1 is then simply replaced by the one computed in
(Pacchiano et al., 2019, Algorithm 3). In Section 5, we show that this combination, which we call
behavior-guided WNPG (BG-WNPG), leads to the best overall performance.
Wasserstein Natural Evolution Strategies (WNES). ES treats the total reward observed on a
trajectory under policy πθ as a black-box function L(θ) (Salimans et al., 2017; Mania et al., 2018;
6
Published as a conference paper at ICLR 2021
Algorithm 2: Wasserstein Natural Evolution Strategies
1:	Input Initial policy πθ0, α > 0, δ ≤ 1
2:	for iteration k = 1, 2, ... do
3:	Sample ” ...,	〜N (0,I).
4:	Perform rollouts {τn }nN-1 of length T using the perturbed parameters {θen = θk + σn }nN=1
and compute behavioral embeddings {Xn = Φ(τn)}nN=1
5:	Compute gradient estimate of L(θn) using Equation (13) and trajectories {τn}nN=1.
6:	Compute Jacobian matrix J appearing in Algorithm 3 using Equation (14).
7:	Compute WNG ^W using Algorithm 3, with samples {Xn}N=ι and computed gk and J.
8:	Update policy using Equation (15).
9:	end for
Choromanski et al., 2020). Evaluating it under N policies whose parameters θen are gaussian pertur-
bations centered around θk and with variance σ can give an estimate of the gradient ofL(θk):
1N
gk = Nσ X (L(θn) -L(θk)) (θn - θk).
n=1
(13)
Instead of directly updating the policy using Equation (13), it is possible to encourage either prox-
imity or diversity in behavior using the embeddings Xn = Φ(τn) of the trajectories τn generated for
each perturbed policy πθe . Those embeddings can be used as input to Algorithm 3 (see appendix),
along with Equation (13) to estimate the gW, which captures similarity in behavior. The algorithm
remains unchanged except for the estimation of the Jacobian J of Equation (10) which becomes
1N
Jm,. = Nσ ɪs hm(Xn)(θn - Ok )∙
(14)
n=1
The policy parameter can then be updated using an interpolation between gk and the WNG ^W, i.e.,
∆θk (X (1 - δ)gk + δgW
(15)
with δ ≤ 1 that can also be negative. Positive values for δ encourage proximity in behavior, the
limit case being δ = 1 where a full WNG step is taken. Negative values encourage repulsion and
therefore need to compensated by gk to ensure overall policy improvement. Algorithm 2 summarizes
the whole procedure, which can be easily adapted from existing ES implementations by calling a
variant of Algorithm 3. In particular, it can also be used along with an explicit W2 penalty, in which
case the proposed algorithm in Pacchiano et al. (2019) is used to estimate the standard gradient gk
of the penalized loss. Then the policy is updated using Equation (15) instead of gk. We refer to this
approach as behavior-guided WNES (BG-WNES).
5 Experiments
We now test the performance of our estimators for both policy gradients (PG) and evolution strate-
gies (ES) against their associated baseline methods. We show that in addition to an improved com-
putational efficiency, our approach can effectively utilize the geometry induced by a Wasserstein
penalty to improve performance, particularly when the optimization problem is ill-conditioned. Fur-
ther experimental details can be found in the appendix, and our code is available online1.
Policy Gradients. We first apply WNPG and BG-WNPG to challenging tasks from OpenAI Gym
(Brockman et al., 2016) and Roboschool (RS). We compare performance against behavior-guided
policy gradients (BGPG), (Pacchiano et al., 2019), PPO with clipped surrogate objective (Schulman
et al., 2017) (PPO (Clip)), and PG with no trust region (None). From Figure 2, we can see that BGPG
outperforms the corresponding KL-based method (PPO) and vanilla PG, as also demonstrated in
the work of Pacchiano et al. (2019). Our method (WNPG) matches or exceeds final performance
of BGPG on all tasks. Moreover, combining both (BG-WNPG) produces the largest gains on all
1https://github.com/tedmoskovitz/WNPG
7
Published as a conference paper at ICLR 2021
∣.00
Ie6
(RS) Reacher
0.0	0.5	1.0	1.5	2.0
Ie6
750
T
g 500
Φ
Q≤ 250
O
3000
2000
1000
0
0	50000 100000 150000 200000
Timesteps
0.00	0.25	0.50	0.75	1.00
Ie6
InvertedDoubIePenduIum
0.00	025	0.50	0.75	1.00
Timesteps	le6
BGPG
Timesteps le6
Hopper
None
PPO (Clip)
WNPG
BG-WNPG

Figure 2: WNG-based algorithms provide large gains on tasks where initial progress is difficult.
The performance mean ± standard deviation is plotted versus time steps for 5 random seeds on each
task.
environments. Final mean rewards are reported in Table 1. It is also important to note that WNG-
based methods appear to offer the biggest advantage on tasks where initial progress is difficult. To
investigate this further, we computed the hessian matrix at the end of training for each task and
measured the ratios of its largest eigenvalue to each successive eigenvalue (Figure 3). Larger ratios
indicate ill-conditioning, and it is significant that WNG methods produce the greatest improvement
on the environments with the poorest conditioning. This is consistent with the findings in Arbel
et al. (2020) that showed WNG to perform most favorably compared to other methods when the
optimization problem is ill-conditioned, and implies a useful heuristic for gauging when WNG-
based methods are most useful for a given problem.
Evolution Strategies To test our estimator
for WNES, as well as BG-WNES, we applied
our approach to the environment introduced by
Pacchiano et al. (2019), designed to test the
ability of behavior-guided learning to succeed
despite deceptive rewards. During the task, the
agent receives a penalty proportional to its dis-
tance from a goal, but a wall is placed directly
in the agent’s path (Figure 7). This barrier
induces a local maximum in the objective—a
naive agent will simply walk directly towards
the goal and get stuck at the barrier. The idea
is that the behavioral repulsion fostered by ap-
plying a positive coefficient to the Wasserstein
(RS) Walker2d
-----(RS) Halfcheetah
-----(RS) Ant
(RS) Reacher
50
40
30
20
IO
0
0	200	400	600	800 IOOC
i
Figure 3: Condition numbers for different tasks.
penalty (β > 0) will encourage the agent to
seek novel policies, helping it to eventually circumvent the wall. As in Pacchiano et al. (2019),
we test two agents, a simple point and a quadruped. We then compare our method with vanilla ES
as described by Salimans et al. (2017), ES with gradient norm clipping, BGES (Pacchiano et al.,
2019), and NSR-ES (Conti et al., 2018a). In Figure 4, we can see that WNES and BG-WNES im-
prove over the baselines for both agents. To test that the improvement shown by BG-WNES wasn’t
simply a case of additional “repulsion” supplied by the WNG to BGES, we also tested BGES with
an increased β = 0.75, compared to the default of 0.5. This resulted in a decrease in performance,
attesting to the unique benefit provided by the WNES estimator.
Computational Efficiency We define the computational efficiency of an algorithm as the rate with
which it accumulates reward relative to its runtime. To test the computational efficiency of our
8
Published as a conference paper at ICLR 2021
Point
-600
-700
-800
-900
-1000
0	100000	200000	300000	400000	500000
Timesteps
(a)
Figure 4: WNES methods more reliably overcome local maxima. Results obtained on the point
(a) and quadruped (b) tasks. The mean ± standard deviation is plotted across 5 random seeds.
-6000
-3000
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5
Timesteps	1e7
(b)
• None
• PPO (Clip)
• BGPG
• WNPG
• BG-WNPG
-14000
-15000
-16000
-17000
-18000
-19000
Point
QUadrUPed
ES
ES + grad-clip
WNES
NSR-ES
BGES
BGES (β 0 7.5 5)
BG-WNES
Figure 5: WNG methods improve computational efficiency. Average reward per minute is plotted
for both gradient tasks (left) and ES tasks (right) for the runs depicted above.
approach, we plotted the total reward divided by wall clock time obtained by each agent for each
task (Fig. 5). Methods using a WNG estimator were the most efficient on each task for both PG and
ES agents. On several environments used for the policy gradient tasks, the added cost of BG-WNPG
reduced its efficiency, despite having the highest absolute performance.
6	Conclusion
Explicit regularization using divergence measures between policy representations has been a com-
mon theme in recent work on policy optimization for RL. While prior works have previously fo-
cused on the KL divergence, Pacchiano et al. (2019) showed that a Wasserstein regularizer over
behavioral distributions provides a powerful alternative framework. Both approaches implicitly de-
fine a form of natural gradient, depending on which divergence measure is chosen. Through the
introduction of WNPG and WNES, we demonstrate that directly estimating the natural gradient of
the un-regularized objective can deliver greater performance at lower computational cost. These
algorithms represent novel extensions of previous work on the WNG to problems where the re-
parameterization trick is not available, as well as to black-box methods like ES. Moreover, using the
WNG in conjunction with a WD penalty allows the WNG to take advantage of the local geometry
induced by the regularization, further improving performance. We also provide a novel compar-
ison between the WNG and FNG, showing that the former has significant advantages on certain
problems. We believe this framework opens up a number of avenues for future work. Developing
a principled way to identify useful behavioral embeddings for a given RL task would allow to get
the highest benefit form WNPG and WNES. From a theoretical perspective, it would be useful to
characterize the convergence boost granted by the combination of explicit regularization and the
corresponding natural gradient approach.
Acknowledgments The authors would like to thank Jack Parker-Holder for sharing his code for
BGPG and BGES, as well as colleagues at Gatsby for useful discussions.
9
Published as a conference paper at ICLR 2021
References
Shun-ichi Amari. Neural learning in structured parameter spaces - natural riemannian gradient. In
M. C. Mozer, M. I. Jordan, and T. Petsche (eds.), Advances in Neural Information Processing
Systems 9, pp.127-133. MIT Press,1997.
M Arbel, A Gretton, W Li, and G Montufar. Kernelized wasserstein natural gradient. In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=Hklz71rYvS.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Yifan Chen and Wuchen Li. Natural gradient in Wasserstein statistical manifold. arXiv:1805.08380
[cs, math], May 2018. URL http://arxiv.org/abs/1805.08380. arXiv: 1805.08380.
Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang
Yang, Atil Iscen, Jasmine Hsu, and Vikas Sindhwani. Provably robust blackbox optimization for
reinforcement learning. In Conference on Robot Learning, pp. 683-696, 2020.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff
Clune. Improving exploration in evolution strategies for deep reinforcement learning via a popu-
lation of novelty-seeking agents. In Advances in Neural Information Processing Systems 31, pp.
5027-5038. Curran Associates, Inc., 2018a.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff
Clune. Improving exploration in evolution strategies for deep reinforcement learning via a pop-
ulation of novelty-seeking agents. In Advances in neural information processing systems, pp.
5027-5038, 2018b.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
AUde Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization
for large-scale optimal transport. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3440-
3448. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6566-stochastic-optimization-for-large-scale-optimal-transport.
pdf.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79-86, 1951.
Seong Jae Lee and Zoran Popovic. Learning behavior styles with inverse reinforcement learning.
ACM transactions on graphics (TOG), 29(4):1-7, 2010.
Wuchen Li. Geometry of probability simplex via optimal transport. arXiv:1803.06360 [math],
March 2018. URL http://arxiv.org/abs/1803.06360. arXiv: 1803.06360.
Wuchen Li and Guido Montufar. Natural gradient via optimal transport. arXiv:1803.07033 [cs,
math], March 2018a. URL http://arxiv.org/abs/1803.07033. arXiv: 1803.07033.
Wuchen Li and Guido Montufar. Ricci curvature for parametric statistics via optimal transport.
arXiv:1807.07095 [cs, math, stat], July 2018b. URL http://arxiv.org/abs/1807.
07095. arXiv: 1807.07095.
Wuchen Li and Jiaxi Zhao. Wasserstein information matrix. arXiv:1910.11248 [cs, math, stat],
November 2019. URL http://arxiv.org/abs/1910.11248. arXiv: 1910.11248.
10
Published as a conference paper at ICLR 2021
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Advances in Neural Information Processing Systems,
pp.1800-1809, 2018.
Elliot Meyerson, Joel Lehman, and Risto Miikkulainen. Learning behavior characterizations for
novelty search. In Proceedings of the Genetic and Evolutionary Computation Conference 2016,
pp. 149-156, 2016.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choromanski,
and Michael I Jordan. Learning to score behaviors for guided policy optimization. arXiv preprint
arXiv:1906.04349, 2019.
Martin L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, Inc., 2010.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as
a Scalable Alternative to Reinforcement Learning. arXiv e-prints, art. arXiv:1703.03864, March
2017.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/1502.
05477.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.
html.
Yunhao Tang and Shipra Agrawal. Implicit Policy for Reinforcement Learning. arXiv:1806.06798
[cs, stat], February 2019. URL http://arxiv.org/abs/1806.06798. arXiv:
1806.06798.
Cedric Villani. OPTIMAL TRANSPORT: old and new. SPRINGER-VERLAG BERLIN AN, 2016.
11
Published as a conference paper at ICLR 2021
Environment	BG-WNPG (ours)	WNPG (ours)	BGPG	PPO (Clip)	None
(RS) Walker2d	739.51±81.10	683.38?	652.53	516.47	529.60
(RS) Halfcheetah	1817.68 ± 79.64	1655.04	1668.75*	1412.57	1334.27
(RS) Ant	2072.56 ± 85.24	1844.14*	1735.98	1627.01	1566.63
(RS) Reacher	4.37 ± 2.23	2.13*	0.46	-0.37	-6.46
DoublePendulum	7254.98 ± 419.36	6740.96*	6526.48	4401.24	3964.51
Hopper	2439.56 ± 394.39	2235.23*	1934.43	1890.12	1816.58
Table 1: Final average return over 5 trials for the PG experiments depicted in Figure 2. ± values
denote one standard deviation across trials. The value for the best-performing method is listed
in bold, while a * denotes the second best-performing method. BG-WNPG reaches the highest
performance on all tasks. WNPG beats the best-performing baseline (BGPG) on all tasks except
HalfCheetah, where the difference is small.
A Background
A. 1 Policy Optimization
An agent interacting with an environment form a system that can be described by a state variable s
belonging to a state space S. In the Markov Decision Process (MDP) setting, the agent can interact
with the environment by taking an action a from a set of possible actions A given the current state
s of the system. As a consequence, the system moves to a new state s0 according to a probability
transition function P(s0|a, s) which describes the probability of moving to state s0 given the previous
state s and action a. The agent also receives a partial reward r which can be expressed as a possibly
randomized function of the new state s0, r = r(s0). The agent has access to a set of possible
policies ∏θ(a∣s) parametrized by θ ∈ Rp and that generates an action a given a current state s.
Thus, each policy can be seen as a probability distribution conditioned a state s. Using the same
policy induces a whole trajectory of state-action-rewards τ = (st, at, rt)t≥0 which can be viewed as
a sample from a trajectory distribution Pθ defined over the space of possible trajectories τ . Hence,
for a given random trajectory τ induced by a policy πθ , the agent receives a total discounted reward
R(τ) := Pt∞=1 γt-1r(st) with discount factor 0 < γ < 1. This allows to define the value function
as the expected total reward conditioned on a particular initial state s:
Vθ (St) = EPθ |st
∞
X γl-1r(sl+t)
l=1
(16)
When the gradient of the score function V log ∏θ(a|s) is available, the policy gradient theorem
allows us to express the gradient of R(θ):
∞
Vθ R(θ) = Epθ X γtV log ∏θ (at∣St)Aθ(st,at)	(17)
t=0
where the expectation is taken over trajectories τ under Pθ and Aθ (s, a) represents the advantage
function which can be expressed in terms of the value function Vθ(s) in terms of
Aθ(st, at) = Est+1|st,at [r(st+1) + γVθ(st+1)] - Vθ(st).
The agent seeks an optimal policy πθ? that maximizes the expected total reward under the trajectory
distribution: R(θ) = EPθ [R(τ)].
B	Wasserstein Natural gradient
Connection to the Fisher natural gradient and proximal methods. Both WNG and FNG are
obtained from a proximity measure between probability distributions:
Proposition 2 Let D(θ, θ0) be either the KL-divergence KL(πθ, πθ0 ) or the Wasserstein-2 distance
between the behavioral distributions W2(qθ, qθ0 ) and let gD be either the FNG gF or WNG gW,
12
Published as a conference paper at ICLR 2021
then
gD = lim arg max β(L® + β-1u) - LIek)- βD (θk
β→+∞	u	2
(18)
Equation (18) simply states that the both WNG and FNG arise as limit cases of penalized objectives
provided the strength of the penalty β diverges to infinity and the step-size is shrank proportion-
ally to β-1. An additional global rescaling by β of the total objective prevents it from collapsing
to 0. Intuitively, performing a Taylor expansion of Equation (18) recovers an equation similar to
Equation (8). Equation (18) shows that using a penalty that encourages global proximity between
successive policies, it is possible to recover the local geometry of policies (captured by the local ) by
increasing the strength of the penalty using appropriate re-scaling. This also informally shows why
both natural gradients are said to be invariant to re-parametrization (Arbel et al., 2020, Proposition
1), since both KL and W2 remains unchanged if qθ is parameterized in a different way.
C Algorithm for estimating WNG
Algorithm 3: Efficient Wasserstein Natural Gradient
1:	Input mini-batch of samples {Xn}N=ι distributed according to qθ, gradient direction g, basis
functions h1, ..., hM, regularization parameter .
2:	Output Wasserstein Natural gradient ^W
3:	Compute a matrix C of shape M × Nd using Cm,(n,i) J ∂ihm(Xn).
4:	Compute similarity matrix L J 焉 CCT.
5:	Compute surrogate vector V using Equation (11).
6:	for iteration= 1, 2, ...M do
7:	Use automatic differentiation on Vm to compute Jacobian matrix J in Equation (10).
8:	end for
9:	Compute a matrix D of shape M × M using D J JJ> + L.
10:	Compute a vector b of size M using b J Jg.
11:	Solve linear system of size M : b J solve (D, b)
12:	Return ^W J ɪ(g — J>b)
D Additional Experimental Details
D. 1 Policy Gradient Tasks
We conserve all baseline and shared hyperparameters used by Pacchiano et al. (2019). More
precisely, for each task we ran a hyperparameter sweep over learning rates in the set
{1e-5, 5e-5, 1e-4, 3e-4}, and used the concatenation-of-actions behavioral embedding Φ(τ) =
[a0, a1, . . . , aT] with the base network implementation the same as Dhariwal et al. (2017).
13
Published as a conference paper at ICLR 2021
Figure 7: A visualization of the quadruped task. The agent receives receives more reward the closer
it is to the goal (green). A naive agent will get stuck in the local maximum at the wall if it attempts
to move directly to the goal.
The WNG hyperparameters were also left the same as in Arbel et al. (2020). Specifically, the number
of basis points was set as M = 5, the reduction factor was bounded in the range [0.25, 0.75], and
∈ [1e-10, 1e5].
D.2 Evolution Strategies Tasks
As with the policy gradient tasks, we conserved all baseline and shared hyperparameters used by
Pacchiano et al. (2019). Specifically, for the point task, we set the learning rate to be η = 0.1,
the standard deviation of the noise to be σ = 0.01, the rollout length H was 50 time steps, and
the behavioral embedding function to be the last state Φ(τ) = sH . For the quadruped task we set
η = 0.02, σ = 0.02, H = 400, and Φ(τ) = PtH=0 rt Pit=0 ei (reward-to-go encoding; see
Pacchiano et al. (2019) for more details). Both tasks used 1000-dimensional random features and
embeddings from the n = 2 previous policies to compute the WD.
For WNG, the same hyperparameters were used as in the policy gradient tasks.
D.3 Experimental setting of Figure 1
The Objective We consider a function ψ(x) is the sum of sinc functions over all dimensions of
x ∈ R100
ψ(x) = X sin(χi) - 1	(19)
xi
i=1
Such function is highly non-convex and admits multiple bad local minima with the global minimum
of ψ(x) reached for x? = 0. However, we do not make use of this information during optimization.
To alleviate the non-convexity of this loss, we consider a gaussian relaxation objective L(θ) obtained
by taking the expectation of ψ(x) over the 100 dimensional vector x w.r.t. to a gaussian qθ with
parameter vector θ . Thus the objective function to be optimized is a function of θ :
L(θ) = Eqθ [ψ(x)]	(20)
The parameter vector θ is of the form θ = (μ, v), where μ is the mean of the gaussian qθ and V is
a vector in R100 parameterizing the covariance matrix Σ of the gaussian qθ. We will later consider
two parameterizations for the covariance matrix.
The minimal value of L(θ) is reached when the gaussian qθ is degenerate with Σ = 0 and mean
μ = x? = 0. Hence, the mean parameter of the global minimum of L(θ) recover the global
optimum of ψ .
Parameterization of the gaussian We choose the covariance matrix of the gaussian to be diagonal
and consider two parameterizations for the covariance matrix Σ: diagonal and log-diagonal. For the
diagonal parameterization the Covariance Σii = vi and for the log-diagonal we set Σii = exp(2vi).
14
Published as a conference paper at ICLR 2021
Optimization methods We consider different optimization methods using the same objective
L(θ). For the penalty methods, we use the closed form expressions for the both the Wasserstein
distance and KL which are available explicitly in the case of gaussians.
For the Natural gradient methods (WNG) and (FNG), we use the closed form expressions which are
also available in the gaussian case. We denote them as VWL(θ) for (WNG) and VFL(θ) for FNG
and express them in terms of the euclidean/Standard gradient VL(θ):
• Diagonal parameterization:
-WNG:
	VvW L(θ)	二 4 * ΣVvL(θ),	VW L(θ)	=VμL(θ)	(21)
-FNG:					
	VvF L(θ) =	2Σ2VΣL(θ),	VF L(θ)=	∑VμL(θ)	(22)
• Log-diagonal parameterization:					
-WNG:					
	VvWL(θ)	= Σ-1VvL(θ),	VW L(θ)-	=VμL(θ)	(23)
-FNG:					
	VvF L(θ) =	.5 * Vv L(θ),	VF L(θ)=	∑VμL(θ)	(24)
Training details Training is up to 4000 gradient iterations, with λ = .9 and β = .1 unless they
are varied.
15