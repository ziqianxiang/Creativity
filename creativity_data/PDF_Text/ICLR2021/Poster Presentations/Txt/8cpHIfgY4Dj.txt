Published as a conference paper at ICLR 2021
FOCAL: Efficient Fully-Offline Meta-
Reinforcement Learning via Distance Metric
Learning and Behavior Regularization
Lanqing Li* 1； Rui Yang2 j Dijun Luo1
1	Tencent AI Lab
2	Department of Automation, Tsinghua University
lanqingli1993@gmail.com, dijunluo@tencent.com
yangrui19@mails.tsinghua.edu.cn
Ab stract
We study the offline meta-reinforcement learning (OMRL) problem, a paradigm
which enables reinforcement learning (RL) algorithms to quickly adapt to unseen
tasks without any interactions with the environments, making RL truly practical in
many real-world applications. This problem is still not fully understood, for which
two major challenges need to be addressed. First, offline RL usually suffers from
bootstrapping errors of out-of-distribution state-actions which leads to divergence
of value functions. Second, meta-RL requires efficient and robust task inference
learned jointly with control policy. In this work, we enforce behavior regular-
ization on learned policy as a general approach to offline RL, combined with a
deterministic context encoder for efficient task inference. We propose a novel
negative-power distance metric on bounded context embedding space, whose gra-
dients propagation is detached from the Bellman backup. We provide analysis and
insight showing that some simple design choices can yield substantial improve-
ments over recent approaches involving meta-RL and distance metric learning.
To the best of our knowledge, our method is the first model-free and end-to-end
OMRL algorithm, which is computationally efficient and demonstrated to outper-
form prior algorithms on several meta-RL benchmarks.1
1	Introduction
Applications of reinforcement learning (RL) in real-world problems have been proven successful in
many domains such as games (Silver et al., 2017; Vinyals et al., 2019; Ye et al., 2020) and robot
control (Johannink et al., 2019). However, the implementations so far usually rely on interactions
with either real or simulated environments. In other areas like healthcare (Gottesman et al., 2019),
autonomous driving (Shalev-Shwartz et al., 2016) and controlled-environment agriculture (Binas
et al., 2019) where RL shows promise conceptually or in theory, exploration in real environments is
evidently risky, and building a high-fidelity simulator can be costly. Therefore a key step towards
more practical RL algorithms is the ability to learn from static data. Such paradigm, termed ”offline
RL” or ”batch RL”, would enable better generalization by incorporating diverse prior experience.
Moreover, by leveraging and reusing previously collected data, off-policy algorithms such as SAC
(Haarnoja et al., 2018) has been shown to achieve far better sample efficiency than on-policy meth-
ods. The same applies to offline RL algorithms since they are by nature off-policy.
The aforementioned design principles motivated a surge of recent works on offline/batch RL (Fuji-
moto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Siegel et al., 2020). These papers propose
remedies by regularizing the learner to stay close to the logged transitions of the training datasets,
namely the behavior policy, in order to mitigate the effect of bootstrapping error (Kumar et al., 2019),
where evaluation errors of out-of-distribution state-action pairs are never corrected and hence easily
diverge due to inability to collect new data samples for feedback. There exist claims that offline RL
* Correspondence to: Lanqing Li <lanqingli1993@gmail.com>, Dijun Luo <dijunluo@tencent.com>
^ Work done while an intern at TenCent AI Lab.
1Source code: https://github.com/FOCAL-ICLR/FOCAL-ICLR/
1
Published as a conference paper at ICLR 2021
can be implemented successfully without explicit correction for distribution mismatch given suffi-
ciently large and diverse training data (Agarwal et al., 2020). However, we find such assumption un-
realistic in many practices, including our experiments. In this paper, to tackle the out-of-distribution
problem in offline RL in general, we adopt the proposal of behavior regularization by Wu et al.
(2019).
For practical RL, besides the ability to learn without exploration, it’s also ideal to have an algo-
rithm that can generalize to various scenarios. To solve real-world challenges in multi-task setting,
such as treating different diseases, driving under various road conditions or growing diverse crops in
autonomous greenhouses, a robust agent is expected to quickly transfer and adapt to unseen tasks,
especially when the tasks share common structures. Meta-learning methods (Vilalta & Drissi, 2002;
Thrun & Pratt, 2012) address this problem by learning an inductive bias from experience collected
across a distribution of tasks, which can be naturally extended to the context of reinforcement learn-
ing. Under the umbrella of this so-called meta-RL, almost all current methods require on-policy
data during either both meta-training and testing phases (Wang et al., 2016; Duan et al., 2016; Finn
et al., 2017) or at least testing stage (Rakelly et al., 2019) for adaptation. An efficient and robust
method which incorporates both fully-offline learning and meta-learning in RL, despite few attempts
(Li et al., 2019b; Dorfman & Tamar, 2020), has not been fully developed and validated.
In this paper, under the first principle of maximizing practicality of RL algorithm, we propose an
efficient method that integrates task inference with RL algorithms in a fully-offline fashion. Our
fully-offline context-based actor-critic meta-RL algorithm, or FOCAL, achieves excellent sample
efficiency and fast adaptation with limited logged experience, on a range of deterministic continuous
control meta-environments. The primary contribution of this work is designing the first end-to-end
and model-free offline meta-RL algorithm which is computationally efficient and effective without
any prior knowledge of task identity or reward/dynamics. To achieve efficient task inference, we
propose an inverse-power loss for effective learning and clustering of task latent variables, in analogy
to coulomb potential in electromagnetism, which is also unseen in previous work. We also shed light
on the specific design choices customized for OMRL problem by theoretical and empirical analyses.
2	Related Work
Meta-RL Our work FOCAL builds upon the meta-learning framework in the context of reinforce-
ment learning. Among all paradigms of meta-RL, this paper is most related to the context-based and
metric-based approaches. Context-based meta-RL employs models with memory such as recurrent
(Duan et al., 2016; Wang et al., 2016; Fakoor et al., 2019), recursive (Mishra et al., 2017) or proba-
bilistic (Rakelly et al., 2019) structures to achieve fast adaptation by aggregating experience into a
latent representation on which the policy is conditioned. The design of the context usually leverages
the temporal or Markov properties of RL problems.
Metric-based meta-RL focuses on learning effective task representations to facilitate task inference
and conditioned control policies, by employing techniques such as distance metric learning (Yang &
Jin, 2006). Koch et al. (2015) proposed the first metric-based meta-algorithm for few-shot learning,
in which a Siamese network (Chopra et al., 2005) is trained with triplet loss to compare the similarity
between a query and supports in the embedding space. Many metric-based meta-RL algorithms
extend these works (Snell et al., 2017; Sung et al., 2018; Li et al., 2019a).
Among all aforementioned meta-learning approaches, this paper is most related to the context-
based PEARL algorithm (Rakelly et al., 2019) and metric-based prototypical networks (Snell et al.,
2017). PEARL achieves SOTA performance for off-policy meta-RL by introducing a probabilis-
tic permutation-invariant context encoder, along with a design which disentangles task inference
and control by different sampling strategies. However, it requires exploration during meta-testing.
The prototypical networks employ similar design of context encoder as well as an Euclidean dis-
tance metric on deterministic embedding space, but tackles meta-learning of classification tasks
with squared distance loss as opposed to the inverse-power loss in FOCAL for the more complex
OMRL problem.
Offline/Batch RL To address the bootstrapping error (Kumar et al., 2019) problem of offline
RL, this paper adopts behavior regularization directly from Wu et al. (2019), which provides a
relatively unified framework of several recent offline or off-policy RL methods (Haarnoja et al.,
2
Published as a conference paper at ICLR 2021
2018; Fujimoto et al., 2019; Kumar et al., 2019). It incorporates a divergence function between
distributions over state-actions in the actor-critic objectives. As with SAC (Haarnoja et al., 2018),
one limitation of the algorithm is its sensitivity to reward scale and regularization strength. In our
experiments, we indeed observed wide spread of optimal hyper-parameters across different meta-RL
environments, shown in Table 4.
Offline Meta-RL To the best of our knowledge, despite attracting more and more attention, the
offline meta-RL problem is still understudied. We are aware of a few papers that tackle the same
problem from different angles (Li et al., 2019b; Dorfman & Tamar, 2020). Li et al. (2019b) focuses
on a specific scenario where biased datasets make the task inference module prone to overfit the
state-action distributions, ignoring the reward/dynamics information. This so-called MDP ambigu-
ity problem occurs when datasets of different tasks do not have significant overlap in their state-
action visitation frequencies, and is exacerbated by sparse rewards. Their method MBML requires
training of offline BCQ (Fujimoto et al., 2019) and reward/dynamics models for each task, which
are computationally demanding, whereas our method is end-to-end and model-free.
Dorfman & Tamar (2020) on the other hand, formulate the OMRL as a Bayesian RL (Ghavamzadeh
et al., 2016) problem and employs a probabilistic approach for Bayes-optimal exploration. Therefore
we consider their methodology tangential to ours.
3	Preliminaries
3.1	Notations and Problem Statement
We consider fully-observed Markov Decision Process (MDP) (Puterman, 2014) in determinis-
tic environments such as MuJoCo (Todorov et al., 2012). An MDP can be modeled as M =
(S, A, P, R, ρ0, γ) with state space S, action space A, transition function P(s0|s, a), bounded re-
ward function R(s, a), initial state distribution ρ0(s) and discount factor γ ∈ (0, 1). The goal is
to find a policy ∏(a∣s) to maximize the cumulative discounted reward starting from any state. We
introduce the notion of multi-step state marginal of policy π as μ∏(s), which denotes the distribu-
tion over state space after rolling out π for t steps starting from state s. The notation Rπ (s) denotes
the expected reward at state S when following policy ∏: Rn(S) = Ea〜∏ [R(s, a)]. The state-value
function (a.k.a. value function) and action-value function (a.k.a Q-function) are therefore
∞
Vn (S)= X YtEst 〜μ∏ (s)[R(St)]	⑴
Qn(S,a) = R(S, a) + γEs0〜P(s0∣s,a) [Vn (s )]	(^)
Q-learning algorithms are implemented by iterating the Bellman optimality operator B, defined as:
(BQ)(S, a) := R(S, a) + γEP(s0 |s,a) [maxQ(S0, a0)]	(3)
a0
When the state space is large/continuous, Q is used as a hypothesis from the set of function approx-
imators (e.g. neural networks).
In the offline context of this work, given a distribution of tasks p(T) where every task is an
MDP, we study off-policy meta-learning from collections of static datasets of transitions Di =
{(Si,t, ai,t, Si,t, ri,t)∣t = 1,…,N} generated by a set of behavior policies {βi (a∣S)} associated with
each task index i. A key underlying assumption of meta-learning is that the tasks share some com-
mon structures. By definition of MDP, in this paper we restrict our attention to tasks with shared
state and action space, but differ in transition and reward functions.
We define the meta-optimization objective as
L(θ) = ETi〜P(T) [Lτi (θ)]	(4)
where LTi (θ) is the objective evaluated on transition samples drawn from task Ti . A common
choice ofp(T) is the uniform distribution on the set of given tasks {Ti|i = 1, ..., n}. In this case,
the meta-training procedure turns into minimizing the average losses across all training tasks
^
θmeta
1
arg min—
θn
n
XE[Lk(θ)]
k=1
(5)
3
Published as a conference paper at ICLR 2021
3.2	Behavior Regularized Actor Critic (BRAC)
Similar to SAC, to constrain the bootstrapping error in offline RL, for each individual task Ti , be-
havior regularization (Wu et al., 2019) introduces a divergence measure between the learner πθ and
the behavior policy πb in value and target Q-functions. For simplicity, we ignore task index in this
section:
∞
VΠD(s) = X YtEst 〜μπ (S) [Rπ (St)- αD(πθ (∙lst),πb(∙lst))]	⑹
t=0
QD(s,a) = Qψ(s,a) - YaD(πθ(IS),πb"S))	⑺
where Q denotes a target Q-function without gradients and D denotes a sample-based estimate of the
divergence function D. In actor-critic framework, the loss functions of Q-value and policy learning
are given by, respectively,
Lcritic =E(s,a,r,s0)〜D [(r + YQD (S0, aO)- Qψ (S, a))]
a0 〜∏θ (∙∣s0)
Lactor = -E(s,a,r,s0)〜D [Ea00 〜∏θ (∙∣s)[Qψ (S,a )] - aD]
(8)
(9)
3.3	Context-Based meta-RL
Context-based meta-RL algorithms aggregate context information, typically in form of task-specific
transitions, into a latent space Z. It can be viewed as a special form of RL on partially-observed
MDP (Kaelbling et al., 1998) in which a latent representation z as the unobserved part of the state
needs to be inferred. Once given complete information of z and S combined as the full state, the
learning of the universal policy πθ(S, z) and value function Vπ (S, z) (Schaul et al., 2015) becomes
RL on regular MDP, and properties of regular RL such as the existence of optimal policy and value
functions hold naturally. We therefore formulate the context-based meta-RL problem as solving a
task-augmented MDP (TA-MDP). The formal definitions are provided in Appendix B.
4	Method
Based on our formulation of context-based meta-RL problem, FOCAL first learns an effective rep-
resentation of meta-training tasks on latent space Z, then solves the offline RL problem on TA-MDP
with behavior regularized actor critic method. We illustrate our training procedure in Figure 1 and
describe the detailed algorithm in Appendix A. We assume that pre-collected datasets are available
for both training and testing phases, making our algorithm fully offline. Our method consists of
three key design choices: deterministic context encoder, distance metric learning on latent space as
well as decoupled training of task inference and control.
4.1	Deterministic Context Encoder
Similar to Rakelly et al. (2019), We introduce an inference network qφ(z∣c), parameterized by φ,
to infer task identity from context C 〜 C. In terms of the context encoder design, recent meta-RL
methods either employ recurrent neural networks (Duan et al., 2016; Wang et al., 2016) to capture the
temporal correlation, or use probabilistic models (Rakelly et al., 2019) for uncertainties estimation.
These design choices are proven effective in on-policy and partially-offline off-policy algorithms.
However, since our approach aims to address the fully-offline meta-RL problem, we argue that a
deterministic context encoder works better in this scenario, given a few assumptions:
First, we consider only deterministic MDP in this paper, where the transition function is a Dirac
delta distribution. We assume that all meta-learning tasks in this paper are deterministic MDPs,
which is satisfied by common RL benchmarks such as MuJoCo. The formal definitions are detailed
in Appendix B. Second, we assume all tasks share the same state and action space, while each
is characterized by a unique combination of transition and reward functions. Mathematically, this
means there exists an injective function f : T → P × R, where P and R are functional spaces of
transition probability P : S × A × S → {0, 1} and bounded reward R : S × A → R respectively. A
4
Published as a conference paper at ICLR 2021
Figure 1: Meta-training procedure. The inference network qφ uses context data c to compute
the latent context variable z, which conditions the actor and critic, and is optimized by the distance
metric learning (DML) objective. The learning of context encoder (Ldml) and control policy (Lactor,
Lcritic) are decoupled in terms of gradients.
stronger condition of this injective property is that for any state-action pair (s, a), the corresponding
transition and reward are point-wise unique across all tasks, which brings the following assumption:
Assumption 1 (Task-Transition Correspondence). We consider meta-RL with a task distribution
p(T) to satisfy task-transition correspondence if and only if YTI, T 〜P(T), (s, a) ∈ S × A:
Pi(∙∣s,a) = P2(∙∣s,a),R1 (s,a) = R2(s,a) ^⇒ T = T2	(10)
Under the deterministic MDP assumption, the transition probability function P(∙∣s, a) is associated
with the transition map t : S ×A → S (Definition B.3). The task-transition correspondence suggests
that, given the action-state pair (s, a) and task T, there exists a unique transition-reward pair (s0, r).
Based on these assumptions, one can define a task-specific map fT : S × A → S × R on the set of
transitions DT :
fT(st,at) = (St,rt), ∀T 〜P(T), (St,at,st,rt) ∈ DT	(II)
Recall that all tasks defined in this paper share the same state-action space, hence {fT|T 〜P(T)}
forms a function family defined on the transition space S × A × S × R, which is also by definition
the context space C . This lends a new interpretation that as a task inference module, the context
encoder qφ (z|c) enforces an embedding of the task-specific map fτ on the latent space Z, i.e.
qφ : S × A × S × R → Z. Following Assumption 1, every transition {si , ai , s0i , ri } corresponds to
a unique task Ti , which means in principle, task identity can be inferred from any single transition
tuple. This implies the context encoder should be permutation-invariant and deterministic, since the
embedding of context does not depend on the order of the transitions nor involve any uncertainty.
This observation is crucial since it provides theoretical basis for few-shot learning (Snell et al., 2017;
Sung et al., 2018) in our settings. In particular, when learning in a fully-offline fashion, any meta-
RL algorithm at test-time cannot perform adaptation by exploration. The theoretical guarantee that a
few randomly-chosen transitions can enable effective task inference ensures that FOCAL is feasible
and efficient.
4.2	Distance Metric Learning (DML) of Latent Variables
In light of our analysis on the context encoder design, the goal of task inference is to learn a robust
and effective representation of context for better discrimination of task identities. Unlike PEARL,
which requires Bellman gradients to train the inference network, our insight is to disentangle the
learning of context encoder from the learning of control policy. As explained in previous reasoning
about the deterministic encoder, the latent variable is a representation of the task properties involv-
ing only dynamics and reward, which in principle should be completely captured by the transition
datasets. Given continuous neural networks as function approximators, the learned value functions
conditioned on latent variable z cannot distinguish between tasks if the corresponding embedding
vectors are too close (Appendix C). Therefore for implementation, we formulate the latent vari-
able learning problem as obtaining the embedding qφ : S × A × S × R → Z of transition data
5
Published as a conference paper at ICLR 2021
N UOBUBE-P 山NSj
Inverse-square
N uoωu ① ιψp 山 Nsj
Inverse
Linear
Square
Oooooo
2 1 12 3
- - -
N uoωu ①E_p 山 Nsj
t-SNE dimension 1
—Inverse-square
Inverse
—Linear
---Square
Half-Cheetah-Vel
0.0	0.5
1.0	1.5
Sample Steps
(a) context embedding	(b) test-task performance
Figure 2: (a) t-SNE visualization of embedding vectors drawn from 20 randomized tasks on Half-
Cheetah-Vel. Inverse-power distance metric losses (DML) achieve better clustering. Data points
are color-coded according to task identity. (b) FOCAL trained with inverse-power DML losses
outperform the linear and square distance losses.
Di = {(si,t, ai,t, s0i,t, ri,t)|t = 1, ..., N} that clusters similar data (same task) while pushes away
dissimilar samples (different tasks) on the embedding space Z , which is essentially distance metric
learning (DML) (Sohn, 2016). A common loss function in DML is contrasitive loss (Chopra et al.,
2005; Hadsell et al., 2006). Given input data xi, xj ∈ X and label y ∈ {1, ..., L}, it is written as
Lcmont(xi,xj;q) = 1{yi	=	yj}||qi -	qj||22	+	1{yi	6= yj}max(0,m-	||qi -	qj||2)2	(12)
where m is a constant parameter, qi = qφ (xi) is the embedding vector of xi. For data point
of different tasks/labels, contrastive loss rewards the distance between their embedding vectors by
L2 norm, which is weak when the distance is small, as in the case when z is normalized and qφ is
randomly initialized. Empirically, we observe that objectives with positive powers of distance lead to
degenerate representation of tasks, forming clusters that contain embedding vectors of multiple tasks
(Figure 2a). Theoretically, this is due to the fact that an accumulative L2 loss of distance between
data points is proportional to the dataset variance, which may lead to degenerate distribution such
as Bernoulli distribution. This is proven in Appendix B. To build robust and efficient task inference
module, we conjecture that it’s crucial to ensure every task embedding cluster to be separated from
each other. We therefore introduce a negative-power variant of contrastive loss as follows:
Ldml (Xi, Xj ;	q) = 1{yi	=	yj}||qi	- qj ||2	+ 1{yi	=	yj}β ∙ Γ∣	∣	∣n	. -	(13)
||qi-qj||2 +
where > 0 is a small hyperparameter added to avoid division by zero, the power n can be any
non-negative number. Note that when n = 2, Eqn 13 takes form analogous to the Cauchy graph
embedding introduced by Luo et al. (2011), which was proven to better preserve local topology and
similarity relationships compared to Laplacian embeddings. We experimented with 1 (inverse) and
2 (inverse-square) in this paper and compare with the classical L1, L2 metrics in Figure 2 and §5.2.1.
5 Experiments
In our experiments, we assess the performance of FOCAL by comparing it with several baseline
algorithms on meta-RL benchmarks, for which return curves are averaged over 3 random seeds.
Specific design choices are examined through 3 ablations and supplementary experiments are pro-
vided in Appendix E.
5.1	Sample Efficiency and Asymptotic Performance
We evaluate FOCAL on 6 continuous control meta-environments of robotic locomotion, 4 of which
are simulated via the MuJoCo simulator (Todorov et al., 2012), plus variants of a 2D navigation
problem called Point-Robot. 4 (Sparse-Point-Robot, Half-Cheetah-Vel, Half-Cheetah-Fwd-Back,
6
Published as a conference paper at ICLR 2021
Ant-Fwd-Back) and 2 (Point-Robot-Wind, Walker-2D-Params) environments require adaptation by
reward and transition functions respectively.
Figure 3: Performance vs. number
of transition steps sampled for train-
ing. Top: Average episodic testing re-
turn of FOCAL vs. other baselines on
4 meta-environments with different re-
ward functions across tasks. Bottom:
Average episodic testing return of FO-
CAL vs. other baselines on 2 meta-
environments with different transition
dynamics across tasks.
These meta-RL benchmarks were previously introduced by Finn et al. (2017) and Rakelly et al.
(2019), with detailed description in Appendix D. For data generation, we train stochastic SAC mod-
els for every single task and roll out policies saved at each checkpoint to collect trajectories. The
offline training datasets are selections of the saved trajectories, which facilitates tuning of the per-
formance level and state-action distributions of the datasets for each task.
For OMRL, there are two natural baselines. The first is by naively modifying PEARL to train and
test from logged data without exploration, which we term Batch PEARL. The second is Contextual
BCQ. It incorporates latent variable z in the state and perform task-augemented variant of offline
BCQ algorithm (Fujimoto et al., 2019). Like PEARL, the task inference module is trained using
Bellman gradients. Lastly, we include comparison with the MBML algorithm proposed by Li et al.
(2019a). Although as discussed earlier, MBML is a model-based, two-stage method as opposed
to our model-free and end-to-end approach, we consider it by far the most competitive and related
OMRL algorithm to FOCAL, due to the lack of other OMRL methods.
As shown in Figure 3, we observe that FOCAL outperforms other offline meta-RL methods across
almost all domains. In Figure 4b, we also compared FOCAL to other algorithm variants including
a more competitive variant of Batch PEARL by applying the same behavior regularization. In both
trials, FOCAL with our proposed design achieves the best overall sample efficiency and asymptotic
performance.
We started experiments with expert-level datasets. However, for some tasks such as Ant and Walker,
we observed that a diverse training sets result in a better meta policy (Table 2). We conjecture that
mixed datasets, despite sub-optimal actions, provides a broader support for state-action distributions,
making it easier for the context encoder to learn the correct correlation between task identity and
transition tuples (i.e., transition/reward functions). While using expert trajectories, there might be
little overlap between state-action distributions across tasks (Figure 8), which may cause the agent
to overfit to spurious correlation. This is the exact problem Li et al. (2019b) aims to address, termed
MDP ambiguity. Such overfitting to state-action distributions leads to suboptimal latent representa-
tions and poor robustness to distribution shift (Table 6), which can be interpreted as a special form
of memorization problem in classical meta-learning (Yin et al., 2019). MDP ambiguity problem is
addressed in an extension of FOCAL (Li et al., 2021).
5.2	Ablations
Based on our previous analysis, we examine and validate three key design choices of FOCAL by the
following ablations. The main results are illustrated in Figure 4 and 5.
7
Published as a conference paper at ICLR 2021
FOCAL (ours)
NU。-SuSW-PWNSJ
Ooooo
O 5 510
UO-SUeE-P 山 NS

(a) context embedding	(b) test-task performance
Figure 4: Comparative study of 4 algorithm variants: FOCAL with deterministic/probabilistic
context encoder, Batch PEARL with/without behavior regularization. (a) t-SNE visualization
of the embedding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points are
color-coded according to task identity. (b) Return curves on tasks with different reward functions
(Half-Cheetah-Vel) and transition dynamics (Walker-2D-Params).
5.2.1	Power Law of Distance Metric Loss
To show the effectiveness of our proposed negative-power distance metrics for OMRL problem,
we tested context embedding loss with different powers of distance, from L-2 to L2 . A t-SNE
(Van der Maaten & Hinton, 2008) visualization of the high-dimensional embedding space in Fig-
ure 2a demonstrates that, distance metric loss with negative powers are more effective in separating
embedding vectors of different tasks, whereas positive powers exhibit degenerate behaviors, leading
to less robust and effective conditioned policies. By a physical analogy, the inverse-power losses
provide ”repulsive forces” that drive apart all data points, regardless of the initial distribution. In
electromagnetism, consider the latent space as a 3D metal cube and the embedding vectors as po-
sitions of ”charges” of the same polarity. By Gauss’s law, at equilibrium state, all charges are
distributed on the surface of the cube with densities positively related to the local curvature of the
surface. Indeed, we observe from the ”Inverse-square” and ”Inverse” trials that almost all vectors
are located near the edges of the latent space, with higher concentration around the vertices, which
have the highest local curvatures (Figure 7).
To evaluate the effectiveness of different powers of DML loss, Table 1: Embedding Statistics on
we define a metric called effective separation rate (ESR) which Half-Cheetah-Vel (latent space di-
computes the percentage of embedding vector pairs of differ- mension l = 5).
ent tasks whose distance on latent space Z is larger than the
expectation of randomly distributed vector pairs, i.e.,	2l/3	Loss	RMS	ESR
on (-1, 1)l. Table 1 demonstrates that DML losses of nega-	Inverse-square	1.282	0.861
tive power are more effective in maintaining distance between	Inverse	1.217	0.840
embeddings of different tasks, while no significant distinction	Linear	1.385	0.819
is shown in terms of RMS distance, which is aligned with our	Square	1.415	0.506
insight that RMS or effectively classical L2 objective, can be
optimized by degenerate distributions (Lemma B.1). This is the core challenge addressed by our
proposed inverse-power loss.
5.2.2	Deterministic vs. Probabilistic Context Encoder
Despite abundance successes of probabilistic/variational inference models in previous work
(Kingma & Welling, 2013; Alemi et al., 2016; Rakelly et al., 2019), by comparing FOCAL with
deterministic and probabilistic context encoder in Figure 4b, we observe experimentally that the for-
mer performs significantly better on tasks differ in either reward or transition dynamics in the fully
offline setting. Intuitively, by our design principles, this is due to
1. Offline meta-RL does not require exploration. Also when Assumption 1 is satisfied, there
is not need for reasoning about uncertainty during adaption.
8
Published as a conference paper at ICLR 2021
(a) context embedding
(b) test-task performance
Figure 5: FOCAL vs. FOCAL with coupled gradients. (a) t-SNE visualization of the embed-
ding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points are color-coded
according to task identity. (b) Return curves on Walker-2D-Params.
2.	The deterministic context encoder in FOCAL is trained with carefully designed metric-
based learning objective, detached from the Bellman update, which provides better effi-
ciency and stability for meta-learning.
Moreover, the advantage of our encoder design motivated by Assumption 1 is also reflected in Figure
4a, as our proposed method is the only variant that achieves effective clustering of task embeddings.
The connection between context embeddings and RL performance is elaborated in Appendix C.
5.2.3	Context Encoder Training Strategies
The last design choice of FOCAL is the decoupled training of context encoder and control pol-
icy illustrated in Figure 1. To show the necessity of such design, in Figure 4 we compare our
proposed FOCAL with a variant by allowing backpropagation of the Bellman gradients to context
encoder. Figure 5a shows that our proposed strategy achieves effective clustering of task context and
therefore better control policy, whereas training with Bellman gradients cannot. As a consequence,
the corresponding performance gap is evident in Figure 5b. We conjecture that on complex tasks
where behavior regularization is necessary to ensure convergence, without careful tuning of hyper-
parameters, the Bellman gradients often dominate over the contribution of the distance metric loss.
Eventually, context embedding collapses and fails to learn effective representations.
Additionally however, we observed that some design choices of the behavior regularization, partic-
ularly the value penalty and policy regularization in BRAC (Wu et al., 2019) can substantially affect
the optimal training strategy. We provide more detailed discussion in Appendix E.2.
6	Conclusion
In this paper, we propose a novel fully-offline meta-RL algorithm, FOCAL, in pursuit of more practi-
cal RL. Our method involves distance metric learning of a deterministic context encoder for efficient
task inference, combined with an actor-critic apparatus with behavior regularization to effectively
learn from static data. By re-formulating the meta-RL tasks as task-augmented MDPs under the
task-transition correspondence assumption, we shed light on the effectiveness of our design choices
in both theory and experiments. Our approach achieves superior performance compared to existing
OMRL algorithms on a diverse set of continuous control meta-RL domains. Despite the success, the
strong assumption we made regarding task inference from transitions can potentially limit FOCAL’s
robustness to common challenges in meta-RL such as distribution shift, sparse reward and stochastic
environments, which opens up avenues for future work of more advanced OMRL algorithms.
7	Acknowledgements
The authors are grateful to Yao Yao, Zhicheng An and Yuanhao Huang for running part of the base-
line experiments. A special thank to Yu Rong and Peilin Zhao for providing insightful comments
and being helpful during the working process.
9
Published as a conference paper at ICLR 2021
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning (ICML), 2020.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Jonathan Binas, Leonie Luginbuehl, and Yoshua Bengio. Reinforcement learning for sustainable
agriculture. In ICML 2019 Workshop Climate Change: How Can AI Help, 2019.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision
andPatternRecognition (CVPR'05), volume 1,pp. 539-546. IEEE, 2005.
Ron Dorfman and Aviv Tamar. Offline meta reinforcement learning. arXiv preprint
arXiv:2008.02598, 2020.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2 : Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola. Meta-q-learning. arXiv
preprint arXiv:1910.00125, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. arXiv preprint arXiv:1703.03400, 2017.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062, 2019.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nat
Med, 25(1):16-18, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-
tion (CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll,
Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for
robot control. In 2019 International Conference on Robotics and Automation (ICRA), pp. 6023-
6029. IEEE, 2019.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, pp. 11784-11794, 2019.
10
Published as a conference paper at ICLR 2021
Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang. Finding task-
relevant features for few-shot learning by category traversal. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 1-10, 2019a.
Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Iskov Christensen, and
Hao Su. Multi-task batch reinforcement learning with metric learning. arXiv, pp. arXiv-1909,
2019b.
Lanqing Li, Yuanhao Huang, and Dijun Luo. Improved context-based offline meta-rl with attention
and contrastive learning. arXiv preprint arXiv:2102.10774, 2021.
Dijun Luo, Chris HQ Ding, Feiping Nie, and Heng Huang. Cauchy graph embedding. In ICML,
2011.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. arXiv preprint arXiv:1707.03141, 2017.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331-5340, 2019.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International conference on machine learning, pp. 1312-1320. PMLR, 2015.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked: Be-
havioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396,
2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pp. 4077-4087, 2017.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances in
neural information processing systems, pp. 1857-1865, 2016.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial
intelligence review, 18(2):77-95, 2002.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
11
Published as a conference paper at ICLR 2021
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763, 2016.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Liu Yang and Rong Jin. Distance metric learning: A comprehensive survey. Michigan State Uni-
versiy, 2(2):4, 2006.
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforce-
ment learning. In AAAI,pp. 6672-6679, 2020.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. arXiv preprint arXiv:1912.03820, 2019.
12
Published as a conference paper at ICLR 2021
Appendices
A Pseudo-code
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
1
2
3
4
5
6
7
8
Algorithm 1: FOCAL Meta-training
Given:
•	Pre-collected batch Di = {(si,j , ai,j , s0i,j , ri,j )}j :1...N ofa set of training tasks {Ti}i=1...n
drawn from p(T)
•	Learning rates α1 , α2 , α3
Initialize context replay buffer Ci for each task Ti
Initialize inference network qφ(z∖c), learning policy ∏(a|s, Z) and Q-network Qψ (s, z, a) with
parameters φ, θ and ψ
while not done do
for each Ti do
for t = 0, T - 1 do
I Sample mini-batches of B transitions {(si t, a% t, s^, r t)}t:i B 〜 Di and update
ICi	——，，
end
end
Sample mini-batches of M tasks 〜p(T)
for step in training steps do
for each Ti do
Sample mini-batches Ci and b 〜Ci for context encoder and policy training
for each Tj do
Sample mini-batches cj from Cj
LdmI = Ldmt(Ci, Cj; q)
end
Liactor = Lactor (bi, q(Ci))
Licritic = Lcritic(bi, q(Ci))
end
φ J φ - αNφ Pjj LdmI
θ J θ - α2Vθ Pi Lactor
ψ J ψ - αKψ Pi Lcritic
end
end
Algorithm 2: FOCAL Meta-testing
Given:
•	Pre-collected batch Di0 = {(si0,d0, ai0,d0, s0i0,d0, ri0,d0)}d0:1...M ofa set of testing tasks
{Ti0}i0=1...m drawn from p(T)
Initialize context replay buffer Ci0 for each task Ti
for each Ti0 do
for t = 0, T - 1 do
Sample mini-batches of B transitions cio = {(sio,t, &心3 s[,力 rio,t)}t：i...B 〜 Di，and
update Ci0
Compute zi0 = qφ(Ci0 )
Roll out policy πθ(a∖s, zi0) for evaluation
end
end
13
Published as a conference paper at ICLR 2021
B Definitions and Proofs
Lemma B.1. The contrastive loss of a given dataset X = {xi|i = 1, ..., N} is proportional to the
variance ofthe random variable X 〜X
Proof. Consider the contrastive loss Pi6=j(xi - xj)2, which consists of N(N - 1) pairs of different
samples (xi , xj ) drawn from X. It can be written as
E(Xi-Xj)2 = 2 1(N - 1)∑x2 - ∑>XjJ	(14)
The variance of X 〜X is expressed as
Var(X) = (X - X)2	(15)
=X2 - (X)2	(16)
=N X x2 - N(X Xi)2	(17)
ii
(N-1)XXi2 -XXiXjj	(18)
where X denotes the expectation of X .By substituting Eqn 18 into 14, We have
X(Xi-Xj)2 =2N2(Var(X))	(19)
i6=j
□
Definition B.1 (Task-Augmented MDP). A task-augmented Markov Decision Process (TA-MDP)
can be modeled as M = (S, Z, A, P, R, ρ0, γ) where
•	S : state space
•	Z: contextual latent space
•	A: action space
•	P: transition function P (s0, z0|s, z, a) = Pz (s0 |s, a) if there is no intra-task transition
•	R: reward function R(s, z, a) = Rz (s, a)
•	ρ0 (s, z): joint initial state and task distribution
•	γ ∈ (0, 1): discount factor
Definition B.2. The Bellman optimality operator Bz on TA-MDP is defined as
(BzQ)(S,z, a) := R(s,z,a) + γEp(so,zo∣s,z,a) [maaxQ(s0, z0,a0)]	(20)
Definition B.3 (Deterministic MDP). For a deterministic MDP, a transition map t : S × A → S
exists such that:
P(s0|s, a) = δ(s0 - t(s, a))	(21)
where δ(X - y) is the Dirac delta function that is zero almost everywhere except X = y.
14
Published as a conference paper at ICLR 2021
C Importance of Distance Metric Learning for Meta-RL on
Tas k-Augmented MDPs
We provide an informal argument that enforcing distance metric learning (DML) is crucial for meta-
RL on task-augmented MDPs (TA-MDPs). Consider a classical continuous neural network Nθ
parametrized by θ with L ∈ N layers, nl ∈ N many nodes at the l-th hidden layer for l = 1, ..., L,
input dimension n0, output dimension nL+1 and nonlinear continuous activation function σ : R →
R. It can be expressed as
Nθ(x)：= Al+i ◦ gl。AL ◦…。σι ◦ Aι(x)	(22)
where Al : Rnl-1 → Rnl is an affine linear map defined by Al (x) = Wlx + bl for nl × nl-1
dimensional weight matrix Wl and nl dimensional bias vector bl and σl ： Rnl → Rnl is an element-
wise nonlinear continuous activation map defined by σl(z) ：= (σ(z1), ..., σ(znl ))|. Since every
affine and activation map is continuous, their composition Nθ is also continuous, which means by
definition of continuity:
∀ > 0, ∃η > 0 s.t.	(23)
∣xι - x2∣ < η ⇒ ∖Nθ(xι) - Nθ(x2)∣ < E	(24)
where | ∙ | in principle denotes any valid metric defined on Euclidean space Rn0. A classical example
is the Euclidean distance.
Now consider Nθ as the value function on TA-MDP with deterministic embedding, approximated
by a neural network parameterized by θ:
Qθ(s,a, z) ≈ Qθ(s,a,z) = Rz(s,a) + 7旧§，〜pz(s0∣s,a) [Vθ(s0)]	(25)
The continuity of neural network implies that for a pair of sufficiently close embedding vectors
(zi , zj ), there exists sufficiently small η > 0 and E > 0 that
一 “I	I	、lzA/	∖	ZA/	ʌ I	∕r∖∕∖
Z1,Z2 ∈ Z, ∣zι - Z2∣ < η ⇒ ∣Qθ(s,a, Zi) - Qθ(s,a,Z2)| < E	(26)
Eqn 26 implies that for a pair of different tasks (Ti, Tj) 〜p(T), if their embedding vectors are
sufficiently close in the latent space Z, the mapped values of meta-learned functions approximated
by continuous neural networks are suffciently close too. Since by Eqn 25, due to different transi-
tion functions Pzi (s0|s, a), Pzj (s0|s, a) and reward functions Rzi (s, a), Rzj (s, a) of (Ti, Tj), the
distance between the true values of two Q-functions ∣Qθ (s,a,zi) - Qθ (s,a,Zj )| is not guaranteed
to be small. This suggests that a meta-RL algorithm with suboptimal representation of context em-
bedding z = qφ(c), which fails in maintaining effective distance between two distinct tasks Ti, Tj,
is unlikely to accurately learn the value functions (or any policy-related functions) for both tasks
simultaneously. The conclusion can be naturally generalized to the multi-task meta-RL setting.
15
Published as a conference paper at ICLR 2021
D Experimental Details
D.1 Details of the Main Experimental Result (Figure 3 and 4)
The main experimental result in the paper is the comparative study of performance of FOCAL and
three baseline OMRL algorithms: Batch PEARL, Contextual BCQ and MBML, shown in Figure
3. Here in Figure 6 we plot the same data for the full number of steps sampled in our experiments.
Some of the baseline experiments only lasted for 106 steps due to limited computational budget,
but are sufficient to support the claims made in the main text. We directly adopted the Contextual
BCQ and MBML implementation from MBML’s official source code2 and perform the experiments
on our own dataset generated by SAC algorithm3 The DML loss used in experiments in Figure 3
is inverse-squared, which gives the best performance among the four power laws we experimented
with in Figure 2.
——FOCAL ——Batch PEARL ——Contextual BCQ ——MBML
Figure 6: Average episodic testing return of FOCAL vs. other baselines on five meta-environments.
In addition, we provide details on the offline datasets used to produce the result. The performance
levels of the training/testing data for the experiments are given in Table 2, which are selected for the
best test-time performance over four levels: expert, medium, random, mixed (consist of all logged
trajectories of trained SAC models from beginning (random quality) to end (expert quality)). For
mixed data, the diversity of samples is optimal but the average performance level is lower than
expert. A summary of the fixed datasets used for producing Figure 3 and 6 is given in Table 3.
Lastly, shown in in Figure 7, we also present a faithful 3D projection (not processed by t-SNE) of la-
tent embeddings in Figure 4a. Evidently, our proposed method is the only algorithm which achieves
effective clustering of different task embeddings. As validation of our intuition about the analogy
between the DML loss and electromagnetism discussed in §5.2.1, the learned embeddings do clus-
2https://github.com/Ji4chenLi/Multi-Task-Batch-RL
3For sparse reward environments like Sparse-Point-Robot, we observed no adapation at test time (return
stays zero) for both Contextual BCQ and MBML, which might be due to incorrect implementation or just that
both algorithms fail to adapt in sparse reward scenarios. To avoid drawing conlusion too hastily, we chose to
not present Contextual BCQ and MBML result for Sparse-Point-Robot at the moment.
16
Published as a conference paper at ICLR 2021
Table 2: Quality of data used for best test-time performance. We maintain the same quality of data
for training and testing due to algorithm’s sensitivity to distribution shift. From our experiments, we
observe that for some envs/tasks, datasets with the best performance generate the best testing result,
whereas for some envs/tasks, the diversity of data matters the most.
Meta Env	Training Data	Testing Data
Sparse-Point-Robot	expert	expert
Half-Cheetah-Vel	expert	expert
Ant-Fwd-Back	mixed	mixed
Half-Cheetah-Fwd-Back	mixed	mixed
Walker-2D-Params	mixed	mixed
Point-Robot-Wind	expert	expert
Table 3: Details of the fixed datasets used for producing Figure 3 and 6. The three numbers in the
”Checkpoints” column stand for starting epoch: ending epoch: checkpoint spacing.
Meta Env	# of tasks	Checkpoints	# of trajectories	Trajectory Steps	Obs Dim	Act Dim	Dataset Size
Sparse-Point-Robot	-100~~	-2200:4800:200-	51	200	2	2	31G
Half-Cheetah-Vel	100	50000:950000:50000	53	1000	20	6	36G
Ant-Fwd-Back	2	10000:790000:10000	55	200	27	8	1.1G
Half-Cheetah-Fwd-Back	2	5000:640000:5000	6	1000	20	6	554M
Walker-2D-Params	50	50000:950000:50000	53	200	17	6	4.6G
Point-Robot-Wind	50	2200:11800:200	51	200	2	2	54G
ter around the corners and edges of the bounded 3D-projected latent space, which are locations of
highest local curvatures.
Figure 7: 3D projection of the embedding vectors ∈ (-1, 1)l drawn from 20 randomized tasks on
Walker-2D-Params. Data points are color-coded according to task identity.
17
Published as a conference paper at ICLR 2021
D.2 Description of the Meta Environments
•	Sparse-Point-Robot: A 2D navigation problem introduced in PEARL (Rakelly et al.,
2019). Starting from the origin, each task is to guide the agent to a specific goal located on
the unit circle centered at the origin. Non-sparse reward is defined as the negative distance
from the current location to the goal. In sparse-reward scenario, reward is truncated to 0
when the agent is outside a neighborhood of the goal controlled by the goal radius. While
inside the neighborhood, agent is rewarded by 1 - distance at each step, which is a positive
value.
•	Point-Robot-Wind: A variant of Sparse-Point-Robot. Task differ only in transition func-
tion. Each task is associated with the same reward but a distinct ”wind” sampled uniformly
from [-l, l]2. Every time the agent takes a step, it drifts by the wind vector. We use l = 0.05
in this paper.
•	Half-Cheetah-Fwd-Back: Control a Cheetah robot to move forward or backward. Re-
ward function is dependent on the walking direction.
•	Half-Cheetah-Vel: Control a Cheetah robot to achieve a target velocity running forward.
Reward function is dependent on the target velocity.
•	Ant-Fwd-Back: Control an Ant robot to move forward or backward. Reward function is
dependent on the walking direction.
•	Walker-2D-Params: Agent is initialized with some system dynamics parameters random-
ized and must move forward, it is a unique environment compared to other MuJoCo en-
vironments since tasks differ in transition function. Transitions function is dependent on
randomized task-specific parameters such as mass, inertia and friction coefficients.
D.3 Hyperparameter Settings
The details of important hyperparameters used to produce the experimental results in the paper are
presented in Table 4 and 5.
Table 4: Hyperparameters used to produce Figure 3. Meta batch size refers to the number of tasks
used for computing the DML loss Lidjml at a time. Larger meta batch size leads to faster convergence
but requires greater computing power.
Hyperparameters	SParSe-Point-Robot	Point-Robot-Wind	HaIf-Cheetah-VeI	Ant-Fwd-Back	Half-Cheetah-Fwd-Back	WaIker-2D-Params
reward scale	100	100	5	5	5	5
DML loss weight(β)	1	1	10	1	1	10
behavior regularization strength(α)	0	0	50	1e6	500	50
value penalty (in BRAC)	N/A	N/A	False	True	True	False
buffer size (per task)	1e4	1e4	1e4	1e4	1e4	1e4
batch size	256	256	256	256	256	256
meta batch size	16	16	16	4	4	16
g」r(f-divergence discriminator)	1e-4	1e-4	1e-4	1e-4	1e-4	1e-4
dmlJr(αι)	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
actorJr(α2)	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
CiiticJr(α3)	1e-3	1e-3	1e-3	1e-3	1e-3	1e-3
discount factor	0.9	0.9	0.99	0.99	0.99	0.99
# training tasks	80	40	80	2	2	20
# testing tasks	20	10	20	2	2	5
goal radius	0.2	N/A	N/A	N/A	N/A	N/A
latent space dimension	5	5	20	20	5	20
network width (context encoder)	200	200	200	200	200	200
network depth (context encoder)	3	3	3	3	3	3
network width (others)	300	300	300	300	300	300
network depth (others)	3	3	3	3	3	3
maximum episode length		20			20		200	200		200		200	
18
Published as a conference paper at ICLR 2021
Table 5: Hyperparameters used to produce Figure 2a
(a) Compared to Half-Cheetah-Vel experiment
in Table 4, latent space dimension were reduced
to speed up computation. Also the value penalty
is used in behavior regularization.
Hyperparameters	Half-Cheetah-Vel
reward scale	5
behavior regularization strength(α)	500
value penalty (in BRAC)	True
buffer size (per task)	1e4
batch size	256
meta batch size	16
gJr(f-divergence discriminator)	1e-4
dmlJr(αι)	1e-3
actor_lr(a2)	1e-3
CritiCJr(α3)	1e-3
discount factor	0.99
# training tasks	80
# testing tasks	20
latent space dimension	5
network width (context encoder)	200
network depth (context encoder)	3
network width (others)	300
network depth (others)	3
maximum episode length	200	
(b) The DML loss weight β and coefficient (defined in Eqn
13) used in experiments of Figure 2a to match the scale of
objective functions of different power laws. The weights are
chosen such that all terms are equal when the average distance
of xi and xj per dimension is 0.5, a reasonable value given
x ∈ (-1, 1)l.
Trials	β	e
Inverse-Square		
Inverse	2	0.1
Linear	8	0.1
Square	16	0.1
19
Published as a conference paper at ICLR 2021
E Additional Experiments
E.1 Sensitivity to Distribution Shift
Since in OMRL, all datasets are static and fixed, many challenges from classical supervised learning
such as over-fitting exist. By developing FOCAL, we are also interested in its sensitivity to distribu-
tion shift for better understanding of OMRL algorithms. Since for each task Ti, our data-generating
behavior policies βi(a∣s) are trained from random to expert level, We select three performance lev-
els (expert, medium, random) of datasets to study how combinations of training/testing sets with
different qualities/distributions affect performance. An illustration of the three quality levels on
Sparse-Point-Robot is shoWn in Fig 8.
Figure 8: Distribution of rollout trajectories of trained SAC policies of three performance levels:
random, medium and expert. Since reWard is sparse, only states that lie in the red circle are given
non-zero reWards, making meta-learning more challenging and sensitive to data distributions.
Table 6: Average testing return of FOCAL on Sparse-Point-Robot tasks With different quali-
ties/distributions of training/testing sets. The numbers in parenthesis are the performance drop due
to distribution shift (compared to the scenario Where the testing distribution equals the training dis-
tribution).
Training	Testing	FOCAL(drop)
expert	expert	8.16(-)
medium	medium	8.44(-)
random	random	2.34(-)
expert	medium	7.12(1.04)
expert	random	4.43(3.73)
medium	expert	8.25(0.19)
medium	random	6.76(1.68)
Table 6 shoWs the average return at test-time for various training and testing distributions. Sensitiv-
ity to distribution shift is confirmed since training/testing on the similar distribution of data result
in relatively higher performance. In particular, this is significant in sparse reWard scenario since
Assumption 1 is no longer satisfied. With severe over-fitting and the MDP ambiguity problem elab-
orated in the last paragraph of §5.1, performance of meta-RL policy is inevitably compromised by
distribution mismatch betWeen training/testing datasets.
E.2 Value Penalty and Policy Regularization in BRAC
Discussed in §3.2, BRAC (Wu et al., 2019) introduces possible regularization in the value/Q-function
(Eqn 6/7) and therefore the critic loss (Eqn 8), as Well as in the actor loss (Eqn 9). If regularization
is applied on both or only on the policy, it is referred to as value penalty and policy regularization
respectively. In the BRAC paper, Wu et al. (2019) performed extensive tests and concluded that the
tWo designs yield similar performance, With value penalty being slightly better overall. Since BRAC
is designed for single-task offline RL, We again tested both on our OMRL setting. In general, We
20
Published as a conference paper at ICLR 2021
FOCAL (QUrS)
FOCAL, CoUPled
Walker-2 D-Pa rams
(a) context embedding
(b) test-task performance
Figure 9: FOCAL vs. FOCAL with coupled gradients and policy regularization. The task
representation alone of the coupled training scheme might not be superior, but the policy per-
formance can be improved due to end-to-end optimization. (a) t-SNE visualization of the embed-
ding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points are color-coded
according to task identity. (b) Return curves on Walker-2D-Params.
found that on complex tasks such as Ant, value penalty usually requires extremely large regulariza-
tion strength (Table 4) to converge. Since the regularization is added to the value/Q-function, this
results in very large nagative Q value (Figure 10) and exploding Bellman gradients. In this scenario,
training the context embedding with backpropogated Bellman gradients often yields sub-optimal la-
tent representation and policy performance (Fig 5), which leads to our design of decoupled training
strategy discussed in §5.2.3.
For policy regularization however, the learned value/Q-function approximates the real value (Figure
11a), leading to comparable order of magnitude for the three losses Ldml, Lactor and Lcritic. In this
case, the decoupled training of context encoder, actor and critic, may give competitive or even better
performance due to end-to-end optimization, shown in Figure 9.
E.3 Divergence of Q-functions in Offline Setting
The necessity of applying behavior regularization on environment like Ant-Fwd-Back and Walker-
2D-Params to prevent divergence of value functions is demonstrated in Figure 10 and 11.
(a) FOCAL, value penalty	(b) Batch PEARL
Figure 10: FOCAL with value penalty vs. Batch PEARL on Ant-Fwd-Back. The Q-function
learned by Batch PEARL diverges (> 1011) whereas the Q-function of FOCAL, despite its large
order of magnitude due to value penalty, converges eventually given proper regularization (α =
106)
21
Published as a conference paper at ICLR 2021
Figure 11: FOCAL with policy regularization vs. Batch PEARL on Walker-2D-Params. The Q-
function learned by Batch PEARL diverges (> 108) whereas the Q-function of FOCAL, converges
to the true discounted cumulative return (≈ 200 for γ = 0.99).
22
Published as a conference paper at ICLR 2021
F Implementation
We build our algorithm on top of PEARL and BRAC, both are derivatives of the SAC algorithm.
SAC is an off-policy actor-critic method with a maximum entropy RL objective which encourages
exploration and learning a stochastic policy. Although exploration is not needed in fully-offline
scenarios, we found empirically that a maximum entropy augmentation is still beneficial for OMRL,
which is likely due to the fact that in environments such as Ant, different actions result in same next
state and reward, which encourages stochastic policy.
All function approximators in FOCAL are implemented as neural networks with MLP structures.
For normalization, the last activation layer of context encoder and policy networks are invertible
squashing operators (tanh), making Z a bounded Euclidean space (-1, 1)l, which is reflected in
Figure 7.
As in Figure 1, the whole FOCAL pipeline involves three main objectives. The DML loss for
training the inference network qφ(z∖c) is given by Eqn 13, for mini-batches of transitions drawn
from training datasets: Xi 〜 Di, Xj 〜 Dj. The embedding vector q$, qj are computed as the
average embedding over xi and xj . The actor and critic losses are the task-augmented version of
Eqn 8 and 9:
LCritiC = E(s,a,r,s0)〜D
a0 〜∏θ(∙∣s0)
[(r + YQD(S0, z, aO)- Qψ(S, z, a))2]
(27)
(28)
Lactor = -E(s,a,r,s0)〜D
[Ea00〜∏θ(∙∣s) [Qψ (S, z,
where Q is a target network and W indicates that gradients are not being computed through it. As
discussed in (Kumar et al., 2019; Wu, Tucker, and Nachum, 2019), the divergence function DD can
take form of Kernel MMD (Gretton et al., 2012), Wasserstein Divergence (Arjovsky, Chintala, and
Bottou, 2017) or f-divergences (Nowozin et al., 2016) such as KL divergence. In this paper, we use
the dual form (Nowozin, Cseke, and Tomioka, 2016) ofKL divergence, which learns a discriminator
g with minimax optimization to circumvent the need of a cloned policy for density estimation.
In principle, as a core design choice of PEARL, the context used to infer qφ(z∖c) can be sampled
with a different strategy than the data used to compute the actor-critic losses. In OMRL however, we
found this treatment unnecessary since there is no exploration. Therefore training of DML and actor-
critic objectives are randomly sampled from the same dataset, which form an end-to-end algorithm
described in Algorithm 1 and 2.
23