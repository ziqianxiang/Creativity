Published as a conference paper at ICLR 2021
Neural Networks for Learning Counterfac-
tual G-Invariances from Single Environments
S Chandra Mouli
Department of Computer Science
Purdue University
chandr@purdue.edu
Bruno Ribeiro
Department of Computer Science
Purdue University
ribeiro@cs.purdue.edu
Ab stract
Despite —or maybe because of— their astonishing capacity to fit data, neural net-
works are believed to have difficulties extrapolating beyond training data distribu-
tion. This work shows that, for extrapolations based on finite transformation groups,
a model’s inability to extrapolate is unrelated to its capacity. Rather, the shortcom-
ing is inherited from a learning hypothesis: Examples not explicitly observed with
infinitely many training examples have underspecified outcomes in the learner’s
model. In order to endow neural networks with the ability to extrapolate over group
transformations, we introduce a learning framework counterfactually-guided by the
learning hypothesis that any group invariance to (known) transformation groups
is mandatory even without evidence, unless the learner deems it inconsistent with
the training data. Unlike existing invariance-driven methods for (counterfactual)
extrapolations, this framework allows extrapolations from a single environment.
Finally, we introduce sequence and image extrapolation tasks that validate our
framework and showcase the shortcomings of traditional approaches.
1 Introduction
Neural networks are widely praised for their ability to interpolate the training data. However, in some
applications, they have also been shown to be unable to learn patterns that can provably extrapolate
out-of-distribution (beyond the training data distribution) (Arjovsky et al., 2019; D’Amour et al.,
2020; de Haan et al., 2019; Geirhos et al., 2020; McCoy et al., 2019; Scholkopf, 2019).
Recent counterfactual-based learning frameworks for extrapolation tasks —such as ICM and IRM (Ar-
jovsky et al., 2019; Besserve et al., 2018; Johansson et al., 2016; Louizos et al., 2017; Peters et al.,
2017; Scholkopf, 2019; Krueger et al., 2020) detailed in Section 2— assume the learner is given
data from multiple environmental conditions (say environments E1 and E2) and is expected to learn
patterns that work well over an unseen environment E3. In particular, the key idea behind IRM is
to force the neural network to learn an internal representation of the input data that is invariant to
environmental changes between E1 and E2, and, hence, hopefully also invariant to E3, which may not
be true for nonlinear classifiers (Rosenfeld et al., 2020). While successful for a class of extrapolation
tasks, these frameworks require multiple environments in the training data. But, are we asking the
impossible? Can humans even perform single-environment extrapolation?
Young children, unlike monkeys and baboons, assume that a conditional stimulus F given another
stimulus D extrapolates to a symmetric relation D given F without ever seeing any such examples (Sid-
man et al., 1982). E.g., if given D, action F produces a treat, the child assumes that given F, action
D also produces a treat. Young children differ from primates in their ability to use symmetries to
build conceptual relations beyond visual patterns (Sidman and Tailby, 1982; Westphal-Fitch et al.,
2012), allowing extrapolations from intelligent reasoning. However, forcing symmetries against data
evidence is undesirable, since symmetries can provide valuable evidence when they are broken.
Unfortunately, single-environment extrapolations have not been addressed in the literature. The
challenge comes from a learning framework where examples not explicitly observed with infinitely
many independent training examples are underspecified in the learner’s statistical model, which is
shared by both objective (frequentist) and subjective (Bayesian) learner’s frameworks. For instance,
1
Published as a conference paper at ICLR 2021
consider a supervised learning task where the training data contains infinitely many sequences
x(tr) =(A,B) associated with label y(tr) = C, but no examples of a sequence x(tr) =(B,A). If given a
test example x(te) =(B,A), the hypothesis considers it to be out of distribution and the prediction
P (Y (te) = C|X (te) = (B,A)) is undefined, since P (X (tr) = (B,A)) = 0. This happens regardless of
a prior over P(X(tr)). This unseen-is-underspecified learning hypothesis is not guaranteed to push
neural networks to assume symmetric extrapolations without evidence.
Contributions. Since symmetries are intrinsically tied to human single-environment extrapolation
capabilities, this work explores a learning framework that modifies the learner’s hypothesis space
to allow symmetric extrapolation (over known groups) without evidence, while not losing valuable
antisymmetric information if observed to predict the target variable in the training data. Formally, a
symmetry is an invariance to transformations of a group, known as a G-invariance. In Theorem 1 we
show that the counterfactual invariances needed for symmetry extrapolation —denoted Counterfactual
G-invariances (CG-invariances)— are stronger than traditional G-invariances. Theorem 2, then, intro-
duces a condition in the structural causal model where G-invariances of linear automorphism groups
are safe to use as CG-invariances. With that, Theorem 3 defines a partial order over the appropriate
invariant subspaces that we use to learn the correct G-invariances from a single environment without
evidence, while retaining the ability to be sensitive to antisymmetries shown to be relevant in the
training data. Finally, we introduce sequence and image counterfactual extrapolation tasks with
experiments that validate the theoretical results and showcase the advantages of our approach.
2 Related Work
Counterfactual inference and invariances. Recent efforts have brought counterfactual inference to
machine learning models. Independent causal mechanism (ICM) and Invariant Risk Minimization
(IRM) methods (Arjovsky et al., 2019; Besserve et al., 2018; Johansson et al., 2016; Parascandolo
et al., 2018; Scholkopf, 2019), Causal Discovery from Change (CDC) methods (Tian and Pearl,
2001), and representation disentanglement methods (Bengio et al., 2020; Goudet et al., 2017) broadly
look for representations, classifiers, or mechanism descriptions, that are invariant across multiple
environments observed in the training data or inferred from the training data (Creager et al., 2020).
They rely on multiple environment samples in order to reason over new environments. To the
best of our knowledge there is no clear effort for extrapolations from a single environment. The
key similarity between the ICM framework and our framework is the assumption of independently
sampled mechanisms (the transformations) and causes.
Domain adaptation and domain generalization. Domain adaptation and domain generalization
(e.g. (Long et al., 2017; Muandet et al., 2013; Quionero-Candela et al., 2009; Rojas-Carulla et al.,
2018; Shimodaira, 2000; Zhang et al., 2015) and others) ask questions about specific —observed or
known— changes in the data distribution rather than counterfactual questions. A key difference is
that counterfactual inference accounts for hypothetical interventions, not known ones.
Forced G-invariances. Forcing a G-invariance may contradict the training data, where the target
variable is actually influenced by the transformation of the input. For instance, handwritten digits are
not invariant to 180o rotations, since digits 6 and 9 would get confused. Data augmentation is a type
of forced G-invariance (Chen et al., 2020; Lyle et al., 2020) and hence, will fail to extrapolate. Other
works forcing G-invariances that will also fail include (not an extensive list): Zaheer et al. (2017) and
Murphy et al. (2019a;b) for permutation groups over set and graph inputs; Cohen and Welling (2016),
Cohen et al. (2019) for dihedral and spherical transformation groups over images.
Learning invariances from training data. The parallel work of Benton et al. (2020) considers
learning image invariances from the training data, however does not consider extrapolation tasks.
Moreover, it does not provide a concrete theoretical proof of invariance, relying on experimental
results over interpolation tasks for validation. Another parallel work (Zhou et al., 2021) uses meta-
learning to learn symmetries that are shared across several tasks (or environments). The works of
van der Wilk et al. (2018) and Anselmi et al. (2019) focus on learning invariances from training data
for better generalization error of the training distribution. However, none of these works consider the
extrapolation task. In contrast, our framework formally considers counterfactual extrapolation, for
which we provide both theoretical and experimental results.
2
Published as a conference paper at ICLR 2021
3 Extrapolations from a Single Environment
Geometrically, extrapolation can be thought as rea-
soning beyond a convex hull ofa set of training points
(Haffner, 2002; Hastie et al., 2012; Xu et al., 2021).
However, for neural networks —with their arbitrary
representation mappings— this geometric interpre-
tation can be insufficient. Rather, we believe extrap-
olations are better described through counterfactual
reasoning (Neyman, 1923; Rubin, 1974; Pearl, 2009;
Scholkopf, 2019). Specifically in our task, We ask:
After seeing training data from environment A, the
learner wants to extrapolate and predict what would
have been the output if the training environment were
B. Extrapolations differ from traditional domain
adaptation due to its counterfactual nature —a what-if
question of an intervention that can only be imagined
if given offline data (Bareinboim et al., 2020; Pearl
and Mackenzie, 2018), rather than a knoWn distribu-
tional change.
Figure 1: Illustration of our structural causal
model (SCM), Where gray nodes indicate
observed variables (in training). X and
X e are obtained from X(hid) and are
UI-Ui
coupled by sharing UD . HoWever, UI and
UI can have different support, resulting in
different distributions over X and X e .
UI -UI
Specifically, our frameWork folloWs the independent causal mechanism principle (Scholkopf, 2019;
Peters et al., 2017): A mechanism describing a variable given its causes is independent of all other
mechanisms describing other variables. For instance, in the causal model UX → X → Y J UY,
this implies that the conditional distribution P(Y|X) is not influenced by any change in P(X).
3.1	Transformation groups
We focus on extrapolations tied to finite linear automorphism groups acting on the input data. We
start With an example. Consider an input x ∈ X = R3n2 representing a vectorized n × n RGB image.
We can define at least three linear automorphism groups: (1) Grot ≡ {T(k)}k∈{o◦,9。。,180。,270。},
Which rotates the image by k degrees, (2) Gcolor ≡ {T(α)}α∈S3, Which permutes the RGB channels of
the image, and (3) Gvflip ≡ {T(v), T(0)}, Which flips the image vertically. More generally, a linear
automorphism group G satisfies six properties: (automorphism) ∀T ∈ G, T : X → X ;(identity)
I(x) = x, I ∈ G; (is closed under composition) ∀T, T0 ∈ G, T ◦ T0 ∈ G, Where T ◦ T0(x) =
T(T0(χ)); (associative) ∀T,T0,T* ∈ G, T ◦ (T0 ◦ Tt) = (T ◦ T0) ◦ Tt; (has inverses) ∀T ∈ G,
∃T-1 ∈ G s.t. T-1 ◦ T = I; and (is linear) T ∈ G is a linear function.
Besides images, sequences x = (x1, x2, . . .) are another input of interest, Where x ∈ X for some
appropriately defined set X . Here, the symmetric group (permutation group) Sn, is the set of all
permutations Sn = {π | π : {1, . . . , n} → {1, . . . , n} is a bijection} equipped With the composition
operator. Attributed graphs (A, X) ∈ X, Where A is tensor of edge properties and X is a matrix of
node attributes, are also of interest for the permutation group Sn .
Subgroups and overgroups. Just as We can compose image transformations to make neW image
transformations, We can also compose automorphism groups into larger automorphism groups
(overgroups). For instance, We can compose rotations and image flips to form a linear automorphism
group G{rot,vflip} = hGrot ∪ GvfliPi containing all such compositions, where〈•〉is the group join operator.
FolloWing standard notation, We say Grot ≤ G{rot,vflip} to indicate that Grot is a subgroup of G{rot,vflip},
or, equivalently, G{rot,vflip} is an overgroup of Grot. Henceforth, we use G{1,...,m} ≡ h∪im=1Gii to
denote the group generated by the groups G1 , . . . , Gm .
3.2	The causal mechanism and an economical data generation process
We assume that a fundamentally economical process created the training data, where the focus was
on sampling diverse environments in a way that mattered to the task. For instance, image datasets
will contain mostly upright pictures, rather than images over all possible orientations, but we will
assume the dataset curators strive for a somewhat diverse set of subjects for each label (e.g., a good
representation of different types of subjects and environmental conditions). Hence, the absence of
variation over image orientations in the dataset can be counted as evidence against its effect on the
image labels.
3
Published as a conference paper at ICLR 2021
We describe the data generation with the help of a structural causal model (SCM) (Pearl, 2009, Defi-
nition 7.1.1) illustrated in Figure 1. Consider a supervised task over inputs X and their corresponding
outputs Y , which are random variables defined over a suitable space. The hidden random variable
X(hid) := g(Uu),	(1)
where g : U → X is a measurable map (deterministic function) that describes the input X in some
unknown canonical form, where Uu is a random variable (e.g., Uu 〜Uniform(0,1)). Next, We
define how X(hid) is modified by transformations into the observed input X .
Transformation of X(hid) into X. Consider a collection of finite linear automorphism groups
G1, . . . , Gm. LetI ⊆ {1, . . . , m} be a subset and D ⊆ {1, . . . , m}\I be a subset of its complement.
We will later define the target variable to be dependent only on the groups indexed by D. Consider
independent and identically distributed random variables UD and UI that select transformations in
the respective overgroups GD = h∪j∈D Gj i and GI = h∪i∈IGii. We note in passing that we allow
GD ∩ GI 6= {Tidentity} even though GD ∩ GI = {Tidentity} makes the counterfactual task easier. The
observed input is defined as
X :=TUD,UI ◦X(hid),	(2)
where TUD , UI is a transformation in GD∪I indexed by two independent hidden environment back-
ground random variables UD , UI . The reader can roughly interpret UD and UI as the random seeds
of a random number generator that gives ordered sequences of transformations from GD and GI re-
spectively. If these ordered sequences are, say, TD(1), . . . , TD(a) and TI(1), . . . , TI(b), then TUD,UI is the
transformation obtained after interleaving the two sequences of transformations and composing them
in order: TUD,UI = TI(1) ◦ TD(1) ◦ TI(2) ◦   Note that TI(i) or TD(i) could be identity transformations.
Appendix B.1 shows that this indexing is surjective, i.e., it can index every transformation in GD∪I.
Target variable. The output Y associated with X is given by
Y := h(X(hid),UD,UY),	(3)
where h is a deterministic function and UY is an independent random variable.
A distribution over the set of background random variables Uall = {Uu, UY , UD, UI} along with
Equations (2) and (3) induces a joint distribution P(Y, X). If the support of UI is a singleton set
{c} for some constant c, then (Y, X) are said to be sampled using an economical data generation
process. In other words, the training data can contain just one value for the variable UI since the
outputs Y do not depend on UI . For instance, if GI is the rotation group, and the image label Y
does not depend on image rotation, then the observed images can be all upright since the sampling is
economical. This is not a required condition for our method to work, however.
Extrapolation as counterfactual reasoning. We can now ask “what would have happened to Y if
we had given specific values of UI to the data generation process in Equations (2) and (3) rather than
sampling from P (UI)”. For instance, would the class of an image change if we had flipped the image
along the vertical axis? Would we re-classify outlier events if we changed the order of events in a
stationary time series? These are counterfactual queries over environment background variables UI .
We now describe the counterfactual variable in our task via variable coupling (Pitman, 1976; Propp
and Wilson, 1996), which we believe gives a standard-statistics-friendly description of counterfactual
SCMs (Shpitser and Pearl, 2007). The coupling of two independent variables D1 and D2 is a proof
technique that creates a random vector (d1, d2), such that Di and D； have the same marginal distri-
butions, i = 1,2, but makes D； and d2 structurally dependent. For instance, consider independent
6-sided and 12-sided dice, denoted D1 and D2 respectively. Let D1； = (U + 1) mod 6 + 1 and
D2； = (U + 2) mod 12 + 1, where U is a 12-sided die roll and 1, 2 ∈ {0, 1} are two independent
coin flips. Then, the tuple (D1； , D2； ) has coupled the variables D1 and D2 via the common random
variable U.
Definition 1 (Counterfactual coupling (CFC)). The counterfactual coupling of the observed data
(Y, X) is a vector (Y, X, X¾ JUJ, where Y = h(X (hid) ,Ud,Uy) , X = TUDUI ◦ X Xhhd),and
XU工-u工=TUD Uτ ◦ X(hid), for appropriately defined Uu, Ud, UY, Ui, UI. The subscript UI — UI
denotes the counterfactual variable to X when UI replaces UI in the data generation process. For a
constant u, XUI -u gives the same definition as the twin network method of Balke and Pearl (1994).
4
Published as a conference paper at ICLR 2021
The support of UI in Definition 1 can be very different from that of UI, potentially inducing a
different distribution over X e than X even if the variables X e and X are structurally
UI-Ui	UI-Ui	J
dependent via UD . Armed with Definition 1, we are now ready to describe our task.
3.3	Extrapolation model
We start by defining counterfactual G-invariant (CG-invariant) representations.
Definition 2 (CG-invariant representations). Let the vector (X, XU -Ue ) denote the counterfactual
coupling of the random variable X given in Definition 1 for any UeI. A representation function
Γ : X → Rd, d ≥ 1, is deemed CG-invariant if
Γ(X) = Γ(XUI-UeI) ,	(4)
where the equality implies that Γ(XUI -u) = Γ(XUI -u0 ), ∀u ∈ supp(UI), ∀u0 ∈ supp(UI) and
supp(A) is the support of random variable A.
Extrapolated model from training to test data. Let (Y,X(tr))〜P(Y,X) and (Y,X(te))〜
P (Y, XU工—U工),for some appropriately defined UI 〜P (UI), be the random variables describing
the training and test data, respectively. We do not have access to test data at training time. Let
Γtrue : X → Rd, d ≥ 1, be a representation of the input data. Consider a function gtrue : Rd →
Im P (Y = y|X (tr)) —where Im P (∙) is the image of P (∙)— (e.g., gtrue could be a feedforward
network with softmax output) and
Y|X(tr) = Y|X叫 with Y|X(T)〜gttue(Γtrue(X(tr))),	(5)
where =d means the random variables have the same distribution. Then, ifΓtrue(X) = Γtrue(XU -Ue ),
then we have that, by our definition of X (te) and X(tr) , gtrue ◦ Γtrue extrapolates:
Y|X(te) = Y|X(te), with Y|X(te)〜gtrue(Γtrue(X(te))).	(6)
Alas, learning Γtrue is the real challenge: (i) We do not know I (and, hence, we do not know the group
GI which is related to the CG-invariance); (ii) this would also require knowing P(UI), which we
don’t. Without an observed XU -Ue , the statistical assumption that examples not explicitly observed
with infinitely large training data have underspecified outcomes in the learner’s statistical model
does not push the model towards learning Γtrue. We must change this assumption.
4 CG-invariances for Extrapolation
In this section we introduce our learning framework, which seeks to use the training data to approx-
imate Γtrue and gtrue of Equation (6). Our framework regularizes neural network weights towards
representations that are invariant to groups that negligibly impact training data accuracy. We over-
come some key challenges: (a) Theorem 1 below shows that CG-invariances (Definition 2) are
stronger than G-invariances. After that, Theorem 2 defines conditions under which G-invariances
suffice as CG-invariances, and (b) We derive an optimization objective where all G-invariances are
mandatory, except the ones deemed inconsistent with the training data, replacing the traditional
unseen-is-underspecified learning hypothesis.
Our first question is whether CG-invariances are just G-invariances. Theorem 1 shows they are not.
Theorem 1 (CG-invariance is stronger than G-invariance). Let the vector (X, XU -Ue ) denote
the counterfactual coupling of the observed variable X given in Definition 1. For a representation
Γ : X →Rd, d ≥ 1, let
G-inv : ∀TI ∈ GI, Γ(X) = Γ(TI ◦ X),
CG-inv : Γ(X) = Γ(XUI-UeI ) ,
denote the conditions on Γ for GI -invariance and CG-invariance respectively. Then, CG-inv =⇒ G-
inv, but G-inv =6⇒ CG-inv.
The proof in Appendix B.2 constructs a task over images and a representation Γ that is GI -invariant
but is not CG-invariant (for appropriately chosen GI and GD). The following condition ensures that a
GI -invariance is also a CG-invariance.
5
Published as a conference paper at ICLR 2021
Theorem 2. If GI is a normal subgroup of Gd∪i, then CG-inv ^⇒ G-inv.
A subgroup H of a group G is called normal (denoted H E G) if for all h ∈ H and g ∈ G,
ghg-1 ∈ H. Proof in the Appendix B.2 utilizes the fact that if GI E GD∪I, then any T ∈ GD∪I can
be written as T = TI ◦ TD for some TI ∈ GI, TD ∈ GD . Throughout the rest of the paper, we will
assume that GI is a normal subgroup of GD∪I in the SCM Equation (2).
4.1	CONSTRUCTING SUBSPACES OF VEC(X) PARTIALLY ORDERED BY INVARIANCE STRENGTH
As discussed before, we do not know GI. In this subsection, we build neural network weights that
are invariant to GM for different subsets M ⊆ {1, . . . , m}. A detailed step-by-step example of this
construction for 3 × 3 images is shown in Appendix C. We start by restating the Reynolds operator,
which has been extensively used in the literature of G-invariant representations without attribution:
Lemma 1 (Reynolds operator (Mumford et al. (1994), Definition 1.5)). Let G be a (finite) linear
automorphism group over vec(X). Then,
T = |G| X t	⑺
T∈G
is a G-invariant linear automorphism, i.e., NT ∈ G and ∀x ∈ vec(X), it must be that T(Ttx) = Tx.
一一	≈τ 一	一 一	,.	≈r2	≈Γ. 一一- 一	一	一 ≈Γ	一- 一	一  一
Since T is a projection operator (i.e., T = T), all the eigenvalues of T are either 0 or 1. Using
this fact, We now describe G-invariant neurons using the left eigenspace of T corresponding to the
eigenvalue 1.
Lemma 2._ If W denotes the left eigenspace corresponding to the eigenvalue 1 of the Reynolds
operator T for the group G, then ∀b ∈ R, the linear transformation γ(x; w, b) = WTx + b is
invariant to all transformations T ∈ G, i.e., γ(Tx; w, b) = γ(x; w, b), if and only if w ∈ W.
The above property of the Reynolds operator can be leveraged to build neural networks that adhere to
particular group symmetries, as done by Yarotsky (2018) and van der Pol et al. (2020). If we knew
GI, restricting the parameters of each neuron to the left 1-eigenspace of the Reynolds operator of GI
would give us a way to build a GI-invariant neural network.
Alas, we do not know I, and consequently we do not know GI . Instead, we want to construct bases for
the complete space vec(X) such that they are partially ordered by their invariance strength: From most
invariant bases to least. In other words, we construct bases for subspaces BM for M ⊆ {1, . . . , m}
such that any weight vector w ∈ BM is (a) invariant to the groups Gi for i ∈ M , and (b) not invariant
to any group Gj forj ∈ {1, . . . , m}\M. Later, we will use this partial order to define a regularization
term for our method. Theorem 3 shows how these bases can be constructed inductively, where we
start with the most invariant subspace (when M = {1, . . . , m}) and judiciously work our way over
increasingly less invariant subspaces. A reader more interested in the algorithm can first refer to the
pseudocode in Appendix D or the example in Appendix C (Step 2).
Theorem 3 (G-invariant subspace bases can be partially ordered by invariance strength). Let Wi ⊆
vec(X) be the left eigenspace corresponding to the eigenvalue 1 of the Reynolds operator Ti for
group Gi, i = 1, . . . , m. We construct the invariant subspace partitions
BM = ∩ Wi ；	BM = orth B)M (BM) ,	∀M ∈ ρ({1, ...,m}) \ 0,	(8)
i∈M
where 夕 is the power set, B)M =㊉N)m BN, orth/】(A2) removes from the subspace A2 its
orthogonal projection onto the subspace A1, and is the direct sum operator. Then, the linear
transformation γ(x; w, b) = wTx+b, b ∈ R, ∀w ∈ BM \ {0}, is GM-invariant but not Gj -invariant
∀j ∈ {1, . . . , m} \ M.
The proof in Appendix B.3 shows that BM contains all the vectors w that are invariant to GM but
could also contain vectors that are invariant to some overgroup of GM . Thus, each step of our
inductive method performs a Gram-Schmidt orthogonalization in order to satisfy condition (b) above:
we need to remove from BM all weight vectors that are invariant to more groups in addition to those
indexed by M (i.e., supersets of M). In addition, if needed, we obtain the basis for the rest of the
space through B⑪=OrthB)0 (vec(X)), the orthogonal complement of B)0.
6
Published as a conference paper at ICLR 2021
Note that if w ∈ BN, then w is never GH -invariant for H ) N as we remove all such w from BN.
Hence, the partial order of nested subsets in 夕({1,..., m}) induces a partial order of invariance
strengths in the bases of the input domain vec(X ) (see Figure 5 for an example). We define level of
invariance (or invariance strength) of a subspace BM as the size of M (i.e., |M |).
Practical aspects. Our algorithm should output dX = dim(vec(X)) basis vectors covering the entire
space (i.e., our new neuron, described later in Equation (10), still has dX + 1 parameters as the
original one). Thus we stop the algorithm in Theorem 3 once dX basis vectors are found. Moreover,
the algorithm needs to run only once for groups G1 , . . . , Gm , and the results can be reused for other
neural architectures. While the worst-case runtime of finding the bases could be exponential in m, it
is unclear whether this exponential runtime can actually happen in practice (all of our experimental
runtimes take less than one minute in commodity machines).
4.2 LEARNING CG-INVARIANT REPRESENTATIONS WITHOUT KNOWLEDGE OF GI.
We are now ready to learn a CG-invariant representation using neural networks Γ and g. Let
G1 , . . . , Gm be known linear automorphism groups. Under the assumption of Theorem 2, we just
need Γ to be GI -invariant, with GI ≡ h∪i∈IGii, butI ⊆ {1, . . . , m} is unknown to us. We achieve
the correct GI-invariance by redefining the neuron weights of Γ using the subspaces of Theorem 3
and proposing a regularized objective that pushes Γ towards the strongest overgroup G-invariance
that does not significantly hurt the training data, where significantly is controlled by a regularization
strength λ > 0.
More formally, let Γ : vec(X) × RdX ×H × RH → Rd, H ≥ 1, d ≥ 1, be a neural network layer with
H neurons, parameterized by free parameters Ω ∈ RdX ×H and b ∈ RH. The H neurons are arranged
in an appropriate architecture as described in Section 5, but reader can imagine a feedforward layer
for now. Let g : Rd → ImP (Y |X) be a link function. The training data D(tr) = {(yi(tr), x(itr))}iN=1 is
assumed to be sampled according to the SCM data generation process in Equations (1) to (3), with
the hidden GI satisfying the conditions in Theorem 2.
Let BM ∈ RdX ×dM be a matrix whose columns are the orthogonal basis of subspace BM 6= {0}
(from Theorem 3) with dimension dM. Any vector w ∈ BM can be expressed as a linear combination
of these basis columns. The coefficients of the linear combination form our learnable parameters.
These neuron weights Ω have a correspondence to the nonzero subspace bases Bmi , Bm2 ,..., BMB:
-3M],1 …3M1,H -
Ω =	...... .	, where B ≤ dχ,	(9)
3Mb,i …3Mb,h
and ωMi,h ∈ RdMi ×1 represents the learnable parameters for the subspace BMi and the h-th neuron.
The h-th neuron in Γ, h ∈ {1, . . . , H}, has the form
Γ(h)(x) = σx> XBMiωMi,h + bh,	(10)
σ(∙) is a nonpolynomial activation function, and bh ∈ R is a bias parameter. Our optimization
objective is then
Ω, b, Wg = argmin X L (y(tr), g(Γ(x叫 Ω, b); Wg)) + λR(Ω)	(11)
n,b,wg (y(tr),χ(tr) )∈D(tr)
where L : Y × ImP (Y |X) → R≥0 is a nonnegative loss function, and λ > 0 is a regularization
strength. The regularization penalty R(Ω) is given by,
R(Ω) = |{Mi : |Mi| >l, 1 ≤ i ≤ B}| + X 1{kωMi,∙k2 > 0} ,	(12)
i:|Mi|=l
1≤i≤B
where l = min{∣M∕ ∙ 1{∣∣ωMi,. k2 > 0}, 1 ≤ i ≤ B}.
Intuition behind the penalty in Equation (12): A subspace BMi is said to be used in the computation
of neuron h (Equation (10)) if the corresponding parameter ωMi,h is nonzero. Then, let BMk be the
least invariant subspace used by any neuron (i.e., |Mk| is the lowest among all used subspaces) and
|Mk | = l. The first term in the penalty counts the number of subspaces BMi (used or unused) that
7
Published as a conference paper at ICLR 2021
are invariant to more groups than BMk (i.e., |Mi| > |Mk|). This term ensures that the optimization
tries to use subspaces that are higher in the partial order with invariance to more groups. The second
term in the penalty counts the number of subspaces BMi that have the same level of invariance
as BMk (i.e., |Mi| = |Mk|), and also have the corresponding coefficients ωMi,h nonzero (i.e., the
subspace BMi is used). The larger the second term, farther away the optimization is from increasing
the least level of invariance from l to l + 1. We present a differentiable approximation of the penalty
in Appendix F along with an example computation of Equation (12) in Figure 8.
Limitations of Equation (12): Recall that we stop the algorithm in Theorem 3 once the basis for
Vec(X) is found. In such cases, there could be parameters Ω0 and Ω0 that assign positive weights
corresponding to the same subspace bases, but with Ω0 invariant to more groups than Ω00. The penalty
in Equation (12) however cannot distinguish between these two sets of weights as they use the same
subspaces and thus, R(Ω0) = R(Ω00). We provide an example in the case of sequence inputs in
Appendix F.3 and leave the solution as future work.
Selecting regularization strength λ: We use a held-out training set to find the best validation accuracy
achieved by any value of λ. Then, among all the values of λ that achieve validation accuracy within
5% of the best validation accuracy, we choose the largest λ (i.e., we opt for maximum invariance
without significantly affecting validation performance).
5	CG-invariant Neural Architectures
For image tasks: We can apply the CG-regularization of Equations (10) and (11) in the convolu-
tional layers of a CNN architecture like VGG (Simonyan and Zisserman, 2014). Mostly, the VGG
architecture remains the same with the exception that the convolutional filters are obtained using the
subspaces from Theorem 3 for the given groups. Once the filter is obtained as a linear combination
of the bases, it is convolved with the image or the feature maps. This will ensure that the model is
CG-invariant to the transformations of smaller patches in the image. A sum-pooling layer over the
entire channel is applied after all the convolutional layers to ensure that the model can be CG-invariant
to the transformations on the whole image. See Appendix E.1 for an example architecture.
For sequence and array tasks (sets, graph & tensor tasks), the architecture is more direct: One can
simply apply a feedforward network with as many hidden layers as needed. Each neuron of the first
layer is as given by Equation (10), ensuring that the first layer can be CG-invariant to the given groups
if needed. Other layers can have regular neurons since stacking dense layers after a CG-invariant
layer does not undo the CG-invariance. See Appendix E.2 for an example architecture.
6	Empirical Results
We now provide empirical results of 12 different tasks to showcase the properties and advantages of
our framework 1. Due to space limitations, our results are only briefly summarized here, with most of
the details described in Appendix G. Appendix A also shows a task where CG-invariance is stronger
than G-invariance, showing the practical relevance of Theorem 1.
Validation of our learning framework (CGreg): In 12 different image and sequence tasks, we
confirmed that our CG-regularization of Equation (11) is able to selectively learn to be invariant to
the largest overgroup that doesn’t contradict the training data, all of this without any evidence in the
data supporting the invariance. The results are summarized in Table 1, which also shows that both
standard neural networks and forced G-invariant networks do not extrapolate to new environments
when I = 0 and I ( {1,...,m}, respectively.
X (hid) and Transformation groups: X (hid) is the canonically ordered input (e.g., upright images,
sorted sequences). Our task considers m linear automorphism groups G1, . . . , Gm. We generate GI
from a subset I ⊆ {1, . . . , m} of the groups, i.e., GI = hGi∈I i. We construct GD using a subset of
{1, . . . , m} \ I, while ensuring that GI E GD∪I in order to fulfill the conditions in Theorem 2.
For image tasks, X(hid) is an upright MNIST image and the m = 3 groups are Grot , Gcolor, Gvertical-flip.
For sequence tasks, we sample X (hid) as a sequence of n sorted integers from a fixed vocabulary and
consider m = n2 permutation groups for all the pair-wise permutations: G1,2 , G2,3, G1,3 , . . . , Gn-1,n,
where Gi,j := {Tidentity, Ti,j} and Ti,j swaps positions i and j in the sequence.
1Public code available at: https://github.com/PurdueMINDS/NN_CGInvariance
8
Published as a conference paper at ICLR 2021
Table 1: Extrapolation accuracy (± 95% confidence interval, bold means p < 0.05 significant)
Image transformation groups {Grot, Gvertical-flip, Gcolor} Task: Predict digit & which transformations of GD was applied to image							Sequences {G1,2, . . . , Gn-1,n} Tasks depend on I (see Appendix G)			
MNIST {3,4} images				MNIST all images			Sequence Tasks			
I	VGG	+G-inv	+CGreg	VGG	+G-inv	+CGreg	I	Transformer	Best FF+G-inv	FF+CGreg
0	96.06±0.63	15.96±2.17	94.49±01.49	89.35±0.52	15.64±1.55	90.89±0.93	0	100.00±0.00	23.38±1.88	95.70±03.05
color	15.06±6.70	50.05±2.17	94.16±06.43	4.51±1.36	47.61±0.45	88.69±2.11	{(i,i+2k)}i,k	0.85±0.37	0.97±0.60	71.85±26.61
rot,vflip	54.87±0.90	32.05±1.16	95.78±07.11	25.91±0.95	44.41±3.28	62.68±6.02	{(i,j)}j>i≥2	12.15±16.05	10.68±1.49	42.08±18.99
rot,col,vflip	49.52±2.37	97.19±1.02	94.89±07.49	11.27±0.34	68.46±2.83	64.99±2.76	{(Kj)}j>i≥1	20.26±32.08	100.00±0.00	100.00±00.00
Training data: The training data is sampled via the SCM equations using an economical data
generation process. We decompose the transformation TUI,UD into a transformation TUD ∈ GD
followed by another transformation TU0 |UD ∈ GI to obtain X = TU0 |UD ◦ TUD ◦ X(hid). This
decomposition is made possible from our assumption that GI is a normal subgroup of GD∪I (Theo-
rem 2). Under the assumption of economic sampling of the training data, in all our experiments we
simply set TU0 |UD = Tidentity ∈ GI, whereas TUD is randomly sampled from GD . Finally, following
Equation (3), the label Y is a combination of the original label of X(hid) and the transformation TUD .
Example (Table 1, row: rot,vflip): For image tasks, if GI = Grot, vertical-flip and GD = Gcolor, then
the training data consists of upright and unflipped images (as TU0 |UD = Tidentity) with different
permutations of the color channels (random transformations TUD ∈ Gcolor are chosen). The task is to
predict the original label of the image (i.e., the digit) and the transformation TD (i.e., the color).
Extrapolation task: The extrapolated test data consists of samples from the coupled random variable
XUI—UI = TUIUD ◦ X(hid) (Definition 1). As before, We decompose TUIUD = TU0∣UD ◦ TUD With
TUD ∈ GD and TUe0|U ∈ GI. However, there is no economic sampling for the test data: TUe0|U and
TUD are sampled randomly from GI and GD respectively. The task is the same as in the training data.
Example (Table 1, roW: rot,vflip): For image tasks, if GI = Grot, vertical-flip and GD = Gcolor, then the
extrapolation test data consists of images randomly rotated, flipped and color permuted, While the
task is the same: predict the digit and its color.
Results: Standard neural netWorks such as CNNs (e.g., VGG (Simonyan and Zisserman, 2014))
(for images) and GRUs/Transformers (Cho et al., 2014; VasWani et al., 2017) (for sequences) fail
whenever the extrapolation task requires some invariance (I = 0), but excel at the interpolation task
(I = 0). Adding forced Gd∪i-invariances via G-CNNs (Cohen and Welling, 2016) (for images)
and permutation-invariant models (Lee et al., 2019; Murphy et al., 2019a; Zaheer et al., 2017) (for
sequences) clearly fails When D 6= 0 but succeeds When D = 0. Our CG-regularized neural netWork
representations, on the other hand, achieve high extrapolation accuracy across all tasks for all choices
ofI ⊆ {1, . . . , m} and D ⊆ {1, . . . , m} \ I. These results plainly shoW that our approach is able
to selectively learn to be invariant only to the appropriate groups. Furthermore, this GI -invariance
is achieved Without any evidence in the training data, thanks to our novel learning paradigm that
considers all G-invariances mandatory unless contradicted by the training data.
7	Conclusion
This Work studied the task of learning representations that can extrapolate beyond the training
data distribution (environment), even When presented With a single training environment. We
considered the case of (counterfactual) extrapolation from linear automorphism groups and described
a frameWork Where all G-invariances (and CG-invariances via Theorem 2) are mandatory, except
the ones deemed inconsistent With the training data (i.e., rather than learning G-invariances, We
unlearn them). Our frameWork reframes the standard statistical learning hypothesis that unseen-data
means underspecified-models With a learning hypothesis that forces models to have all (knoWn)
G-invariances (symmetries) that do not contradict the data, With our empirical results supporting the
proposed approach. Finally, this learning paradigm offers a promising novel research direction for
neural netWork extrapolations.
Acknowledgments
This Work Was funded in part by the National Science Foundation (NSF) AWards CAREER IIS-
1943364 and CCF-1918483, the Purdue Integrative Data Science Initiative, and the Wabash Heartland
Innovation NetWork. Any opinions, findings and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the vieWs of the sponsors.
9
Published as a conference paper at ICLR 2021
References
Fabio Anselmi, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio. Symmetry-adapted
representation learning. Pattern Recognition, 86:201-208, FebrUary 2019. ISSN 0031-3203. doi:
10.1016/j.patcog.2018.07.025.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Alexander Balke and JUdea Pearl. CoUnterfactUal probabilities: CompUtational methods, boUnds and
applications. In Uncertainty Proceedings 1994, pages 46-54. Elsevier, 1994.
Elias Bareinboim, JUan Correa, DUligUr Ibeling, and Thomas Icard. On Pearl’s hierarchy and the
foUndations of caUsal inference. ACM special volume in honor of Judea Pearl, 2020.
YoshUa Bengio, Tristan DeleU, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa
BilaniUk, AnirUdh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle
caUsal mechanisms. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=ryxWIgBFPS.
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in
neUral networks from data. NeurIPS, 2020.
Michel Besserve, Naji Shajarisales, Bernhard Scholkopf, and Dominik Janzing. Group invariance
principles for caUsal generative models. In International Conference on Artificial Intelligence and
Statistics, pages 557-565, 2018.
Shuxiao Chen, Edgar Dobriban, and Jane H. Lee. A group-theoretic framework for data augmenta-
tion. Journal of Machine Learning Research, 21(245):1-71, 2020. URL http://jmlr.org/
papers/v21/20-163.html.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in
Statistical Translation, page 103, 2014.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference
on machine learning, pages 2990-2999, 2016.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on ho-
mogeneous spaces. In Advances in Neural Information Processing Systems, pages 9145-9156,
2019.
Elliot Creager, Jorn-Henrik Jacobsen, and Richard Zemel. Exchanging Lessons Between Algorithmic
Fairness and Domain Generalization. arXiv:2010.07249 [cs], October 2020.
Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,
Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification
presents challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395,
2020.
Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In
Advances in Neural Information Processing Systems, pages 11698-11709, 2019.
Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine
Intelligence, 2(11):665-673, 2020.
Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and
Michele Sebag. Causal generative neural networks. arXivpreprint arXiv:171L08936, 2017.
Patrick Haffner. Escaping the convex hull with extrapolated vector machines. In Advances in Neural
Information Processing Systems, pages 753-760, 2002.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning, volume 1.
Springer series in statistics, 2012.
10
Published as a conference paper at ICLR 2021
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International conference on machine learning, pages 3020-3029, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron Courville. Out-of-Distribution Generalization via Risk Extrapolation (REx).
arXiv:2003.00688 [cs, stat], March 2020.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International conference on machine learning, pages 2208-2217. PMLR,
2017.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal
effect inference with deep latent-variable models. In Advances in Neural Information Processing
Systems, pages 6446-6456, 2017.
Clare Lyle, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On the
benefits of invariance in neural networks. arXiv preprint arXiv:2005.00178, 2020.
R Thomas McCoy, Junghyun Min, and Tal Linzen. Berts of a feather do not generalize together:
Large variability in generalization across models with similar test set performance. arXiv preprint
arXiv:1911.02969, 2019.
Krikamol MUandeL David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pages 10-18, 2013.
David Mumford, John Fogarty, and Frances Kirwan. Geometric invariant theory, volume 34. Springer
Science & Business Media, 1994.
R. Murphy, B. Srinivasan, V. Rao, and B. Ribeiro. Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs. In International Conference on Learning Representa-
tions, 2019a.
Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for
graph representations. In Proceedings of the 36th International Conference on Machine Learning,
2019b.
Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy Pool-
ing: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs. In International
Conference on Learning Representations, September 2018.
J Neyman. Sur les applications de la theorie des probabilites aux experiences agricoles: essai
des principes (masters thesis); justification of applications of the calculus of probabilities to
the solutions of certain questions in agricultural experimentation. excerpts english translation
(reprinted). Stat Sci, 5:463-472, 1923.
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Scholkopf. Learning
independent causal mechanisms. In International Conference on Machine Learning, pages 4036-
4044. PMLR, 2018.
J Pearl and D Mackenzie. The ladder of causation. The book of why: the new science of cause and
effect. New York (NY): Basic Books, pages 23-52, 2018.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference. The MIT
Press, 2017.
11
Published as a conference paper at ICLR 2021
JW Pitman. On coupling of markov chains. Zeitschriftfur Wahrscheinlichkeitstheorie Und Verwandte
Gebiete, 35(4):315-322,1976.
James Gary Propp and David Bruce Wilson. Exact sampling with coupled markov chains and
applications to statistical mechanics. Random Structures & Algorithms, 9(1-2):223-252, 1996.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset
shift in machine learning. The MIT Press, 2009.
Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for
causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization.
arXiv preprint arXiv:2010.05761, 2020.
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies.
Journal of educational Psychology, 66(5):688, 1974.
Bernhard Scholkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Ilya Shpitser and Judea Pearl. What counterfactuals can be tested. In Proceedings of the Twenty-Third
Conference on Uncertainty in Artificial Intelligence, 2007.
Murray Sidman and William Tailby. Conditional discrimination vs. matching to sample: An expansion
of the testing paradigm. Journal of the Experimental Analysis of behavior, 37(1):5-22, 1982.
Murray Sidman, Ricki Rauzin, Ronald Lazar, Sharon Cunningham, William Tailby, and Philip
Carrigan. A search for symmetry in the conditional discriminations of rhesus monkeys, baboons,
and children. Journal of the experimental analysis of behavior, 37(1):23-44, 1982.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Jin Tian and Judea Pearl. Causal discovery from changes. UAI, 2001.
Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp
homomorphic networks: Group symmetries in reinforcement learning. Advances in Neural
Information Processing Systems, 33, 2020.
Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the
marginal likelihood. In Advances in Neural Information Processing Systems, pages 9938-9948,
2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998-6008, 2017.
Gesche Westphal-Fitch, Ludwig Huber, Juan Carlos Gomez, and W Tecumseh Fitch. Production
and perception rules underlying visual patterns: effects of symmetry and hierarchy. Philosophical
Transactions of the Royal Society B: Biological Sciences, 367(1598):2007-2022, 2012.
Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie
Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=UH-cmocLJC.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arXiv:1804.10306
[cs], April 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep Sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
pages 3391-3401. Curran Associates, Inc., 2017.
12
Published as a conference paper at ICLR 2021
KUn Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation: A causal
view. In AAAI, volume 1, pages 3150-3157, 2015.
Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=-QxT4mJdijq.
13
Published as a conference paper at ICLR 2021
Supplementary Material of “Neural Networks for Learning
Counterfactual G-Invariances from Single Environments”
A The practical importance of Theorem 1
TralnIng data： XM =	・ X^6, γ= Tie)
Test data： χ(6 = 7∙⑻。］+*。χ(ftf6, Y=T■网
ɪ(θ) α τ1+20)	7(90) o 7(+20)
Figure 2:	An example task where CG-invariance is stronger than G-invariance. The task is to predict
the orientation of the image while being CG-invariant to horizontal translations.
There are real tasks where CG-invariance is stronger than G-invariance. We consider a task with
60 × 60 image shown in Figure 2 and two transformation groups: the rotation group Grot and the
cyclic horizontal-translation group Gh-transiate = Zεo. Each transformation T§) ∈ Grot rotates the
image along its center by θ°, whereas every transformation T(+h) ∈ Gh-transiate translates the image
horizontally by h pixels while wrapping around the edges. Let GI = Grot and GD = Gh-translate.
The training data consists of images X = T(θ◦) ◦ X(hld) for all T(θ°) ∈ Gr∩t, whereas the test data
consists of images XU工一寸工=T(θ°) ◦ T(+20) ◦ X(hid) for all T(θ°) ∈ Grot The task is to predict
the orientation of the image, i.e., degrees of rotation. It is easy to see that the label requires CG-
invariance to Gh-translate but sensitivity to Grot. We train a strictly Gh-translate-invariant model on this
dataset; whereas the model is able to achieve a 100% accuracy on training, it does poorly with 75%
on test dataset, showing that it is not enough to be GI -invariant to achieve CG-invariance.
B Proofs
B.1	GENERATING ANY T ∈ GD∪I USING NOISES UI AND UD
The structural causal model for X in Equation (2) requires that any T ∈ GD∪I can be indexed by
the hidden background variables UI and UD . We first interpret UD (or UI) as the random seed of
a random number generator that gives an ordered sequence of transformations of GD (or GI ). We
assume that these background noise variables can generate any sequence of transformations from
within their respective groups. Let TD(1), . . . , TD(a) and TI(1) , . . . , TI(b) be those ordered sequences
respectively generated by UD and UI . Then we can obtain a transformation in GD∪I by interleaving
these two sequences (in order): TUD,UI = TI(1) ◦ TD(1) ◦ TI(2) ◦ . Note that UI and UD can always
sample the identity transformation from the respective groups in the corresponding sequences, i.e.,
TI(i) or TD(i) can be identity.
Now, it is a known result in group theory that any T ∈ hGD ∪ GIi is such that T = T1 ◦ T2 ◦ T3 ◦ . . .,
where Ti is in either GI or GD. Then, if T1 ∈ GD, we can write T1 = TI(1) ◦ TD(1) with TI(1) =
Tidentity ∈ GI and TD(1) = T1 ∈ GD . Continuing in a similar fashion, we can find two sequences
of transformations, one from GI and the other from GD , such that interleaving and composing the
resultant sequence of transformations gives us any transformation from GD∪I . This property of the
noises to appropriately index any T ∈ GD∪I will be used in the proof of Theorems 1 and 2.
14
Published as a conference paper at ICLR 2021
B.2	Proof of Theorems 1 and 2
Theorem 1 (CG-invariance is stronger than G-invariance). Let the vector (X, XU工一仃工)denote
the counterfactual coupling of the observed variable X given in Definition 1. For a representation
Γ : X → Rd, d ≥ 1, let
G-inv : ∀Ti ∈ Gi, Γ(X) = Γ(TI ◦ X),
CG-inv ： Γ(X) = Γ(XuI.uI),
denote the conditions on Γ for GI -invariance and CG-invariance respectively. Then, CG-inv =⇒ G-
inv, but G-inv =6⇒ CG-inv.
Label: Upright
Label: Flat
2n + l
2n + l
T(+5)
Label: Upright
T(M°) o τ(+5)
— Label: Flat
Figure 3:	Counterexample to show that GI -invariance does not imply CG-invariance. Given images
of a rod (shown in brown), we wish to predict the orientation of the rod, i.e., whether the rod is
upright or flat. In this example, we have GD = Grot and GI = Gh-translate as any horizontal translation
does not affect the orientation of the rod. Γ : X → R sums the pixel values across the green shaded
region, and is clearly G-invariant to horizontal translations. However, Γ is not CG-invariant.
Proof. First, we will show that CG-invariance =⇒ G-invariance, i.e., for any CG-invariant repre-
sentation Γ : X → Rd , we will show that Γ is also G-invariant to GI.
Consider any U ∈ SUPP(UI) and say the input was generated as XUI… = TUDUI… ◦ X(hld). In
other words, UI took the value u in the structural causal equation for generating the observed input
(Equation (2)). We will prove G-invariance for this input XUI—u, i.e., Γ(T∣◦ XUI^u) = Γ(XuI—u)
for any TI ∈ GI.
Recall that TUD,UI—u was generated by interleaving two separate sequences of transformations
obtained via the background variables UD and UI respectively (Appendix B.1). In other words,
We can write Tud,UI—u = TII) ◦ τD1) ◦ TII2 ◦ ... ◦ T(*), where TIi ∈ GI and TD) ∈ GD
and T(*)depends upon which of the respective sequences before interleaving is longer. Then,
TI ◦ TUDUI—u = TI ◦ TII) ◦ TDI) ◦ TI2) ◦ ... ◦ t(*), Further, if we write TI(I) = TI ◦ TI1), then
we have TI ◦ TUDUI—u = TI(I) ◦ TDI) ◦ TI2) ◦ ... ◦ T(*).
Now we can find a u such that UI J e generates the sequence of transformations TI(I),TI2),....
Interleaving this sequence with the sequence generated by UD , we get TUD,UI—ue = TI0(1) ◦ TD(1) ◦
TI2 ◦ ... ◦ T(*). Denote XUI —u = Τu0,uI—u ◦ X(hid). Since Γ is CG-invariant, we have from
15
Published as a conference paper at ICLR 2021
Definition 2 that
Γ(Xuι-u)=Γ(Xuι-e)
= Γ(Tud,Ui ◦ X(hid))
=Γ(T∣ ◦ TuD,Ui Ju ◦ X(hid))	(from construction of e)
= Γ(TI ◦ XUIJu).
Since this holds for all U ∈ SUPP(Uι), We have that Γ(X) = Γ(T∣ ◦ X).
Next, we will show G-invariance =6⇒ CG-invariance by constructing a counterexample. Let X(hid) ∈
R(2n+1)×(2n+1) be the (2n + 1) × (2n + 1) grayscale image of an uPright rod as shoWn in Figure 3.
Consider two groups that act on this image: the rotation group Grot = {T(k)}k∈{o◦,90^,180^,270^} and
the cyclic horizontal-translation grouP Gh-translate = {T(+u)}u∈Zn. Let GD = Grot and GI = Gh-translate
and the label of the image Y deterministically given by the orientation of the rod: upright (Y = 0) or
flat (Y = 1). The top row of Figure 3 depicts the data in training which is transformed by Gh-translate
only via the identity T (+0) (i.e., no translation).
Now consider a representation Γ : R(2n+1)×(2n+1) → R such that Γ(X) = Pi2=n1+1 Xn,i finds the
sum of the middle row of the image. Note that (a) Γ is able to distinguish between the labels for the
training data, and (b) Γ is Gh-translate-invariant.
We can define the random VariableS UI and UI such that X = T(90°) ◦ X(hid) and XUI JUI =
T(90O)O T(+5) ◦ X(hid). Then, as shown in Figure 3, Γ(X^ JUj = Γ(T(90O)O T(+5) ◦ X(hid))=
Γ(T(90°) ◦ X(hid)), thus showing that Γ is not CG-invariant.
□
Theorem 2. If GI is a normal subgroup of Gd∪i, then CG-inv ^⇒ G-inv.
Proof. The proof that CG-invariance =⇒ G-invariance (from Theorem 1) still holds here. We only
need to prove the converse: G-invariance =⇒ CG-invariance when GI is a normal subgroup of
GD∪I. We begin with a representation Γ that is GI -invariant and consider the simpler case when
UD generates a transformation sequence of length 1 (from GD). In other words, X is obtained by:
X = TI(1) O TD O TI(2) O X (hid) for arbitrary transformations TD ∈ GD and TI(1), TI(2) ∈ GI.
Then for any UeI, we have that XU JUe = TeI(1) O TD O TeI(2) O X (hid) with TeI(1), TeI(2) ∈ GI. Note that
UI only affects the transformations from GI . The condition for CG-invariance with respect to GI
requires that
requirement: Γ(X) = Γ(TI(1) OTD O TI(2) OX(hid)) = Γ(TeI(1) OTD O TeI(2) OX(hid)) = Γ(XU JUe ) .
(13)
Since GI is a normal subgroup of GD∪I and GD ≤ GD∪I, we have
∀TD ∈ GD, ∀TI ∈ GI, TD O TI O TD-1 ∈ GI,
or equivalently,
∀TD ∈ GD, ∀TI ∈ GI, ∃TI0 , s.t.,
TD O TI O TD-1 = TI0
=⇒	TD O TI = TI0 O TD	(14)
(A special case is when the groups GD and GI commute, as then TD O TI = TI O TD.)
Then,
Γ(X) = Γ(TI(1) OTD O TI(2) OX(hid))
=	Γ(TD O TI(2) OX(hid))	(Γ is invariant to GI)
=	Γ(TI0 O TD O X(hid))	(there exists such a TI0 ∈ GI)
=	Γ(TD O X(hid))	(Γ is invariant to GI)
16
Published as a conference paper at ICLR 2021
Similarly, We can prove for the coupled variable that Γ(X^一万工)=Γ(^Ze∣1) ◦ TD ◦ TI2 ◦ X(hid))=
Γ(TD ◦ X(hid)), thus satisfying the requirement of CG-invariance in Equation (13).
Extension to the case When UD generates transformation sequences of length greater than one is
trivial. Any transformation TUDUI = TII) ◦ TDI ◦ T∣2) ∙∙∙ ◦ T(*)can be written in the form
TI ◦ TDI) ◦ TD2 ◦…by repeatedly applying the normal subgroup property in Equation (14). Then
Γ(Tud,ui ◦ X(hid)) = Γ(tI ◦ TD ◦ TD2 ◦•••◦ X(hid)) = Γ(TD1) ◦ TD2 ◦•••◦ X(hid)) as Γ is GI-
invariant. Using a similar argument, we can show for the coupled variable that Γ(TU ,Ue ◦ X(hid)) =
Γ(TD12 ◦ TDD2) ◦…◦ X(hid)), thus proving that Γ is CG-invariant, i.e., Γ(X) = Γ(X^一万工).	□
B.3 Proofs of Lemma 1, Lemma 2 and Theorem 3
Lemma 1 (Reynolds operator (Mumford et al. (1994), Definition 1.5)). Let G be a (finite) linear
automorphism group over vec(X ). Then,
T = |G| X T	⑺
T∈G
is a G-invariant linear automorphism, i.e., ∀T∣ ∈ G and ∀x ∈ vec(X), it must be that T(Ttx) = Tx.
Proof. Consider an arbitrary transformation Tt ∈ G. Then
1
T ◦ Tt =商 X T ◦ Tt
T∈G
=X X T 0
|G| 二,
11 T 0∈Gt
where we define Gt = {T ◦ Tt : ∀T ∈ G}. Now, in order to prove T ◦ Tt = T, we only need to show
that Gt = G. Since groups are closed under compositions, we have ∀T ∈ G, T ◦ Tt ∈ G, and thus
Gt ⊆ G. Finally, since Tt is a bijection and Ta ◦ Tt = Tb ◦ Tt only if Ta = Tb for any Ta , Tb ∈ G, it
must be that |Gt| = |G|. Hence, Gt = G.
□
Lemma 2._ If W denotes the left eigenspace corresponding to the eigenvalue 1 of the Reynolds
operator T for the group G, then ∀b ∈ R, the linear transformation γ(x; w, b) = WTx + b is
invariant to all transformations T ∈ G, i.e., γ(Tx; w, b) = γ(x; w, b), if and only if w ∈ W.
Proof. Sufficiency: Let {wT}dW1 be the set of left eigenvectors of T with eigenvalue 1 and constitute
the orthogonal basis for W. Consider any non-zero w0 ∈ W, then
dW	dW
(w0)T = X αiwT = X αiwTT
i=1	i=1
(15)
for some coefficients {ai}ddWι, where we used the fact that WTT = WT , 1 ≤ i ≤ dw. For any
x ∈ vec(X) and any T ∈ G we have,
γ(Tx; W0, b) = (W0)T(Tx) + b
dW
=X aiWTT(Tx) + b	(using Equation (15))
i=1
dW
=ɑi wT T X + b	(from Lemma 1)
i=1
= γ(x; W0, b)
17
Published as a conference paper at ICLR 2021
Necessity: Given a non-zero w ∈ W and b ∈ R, let γ(Tx; w, b) = γ(x; w, b) for all x ∈ vec(X)
and all T ∈ G . Then,
wT Tx = wT x , ∀x, ∀T
=⇒ wTT = wT , ∀T
=⇒ wT X T = |G|wT	(summing over all T ∈ G)
T∈G
=⇒ WTT = WT .
Hence proved that WT is a left eigenvector of T with eigenvalue 1.
□
Theorem 3 (G-invariant subspace bases can be partially ordered by invariance strength). Let Wi ⊆
vec(X) be the left eigenspace corresponding to the eigenvalue 1 of the Reynolds operator Ti for
group Gi, i = 1, . . . , m. We construct the invariant subspace partitions
BM = ∩	Wi	；	BM	= orthB)M (BM) ,	∀M	∈	夕({1,...,m})\0,	(8)
i∈M
where Q is the power set, B)M =㊉N)m BN, orth/】(A2) removes from the subspace A2 its
orthogonal projection onto the subspace A1, and is the direct sum operator. Then, the linear
transformation γ(x; W, b) = WTx+b, b ∈ R, ∀W ∈ BM \ {0}, is GM -invariant but not Gj -invariant
∀j ∈ {1, . . . , m} \ M.
Proof. Throughout this proof, we will slightly abuse notation by calling a W ∈ vec(X) as G-invariant
for some group G, where We mean the transformation γ(∙; w, b), b ∈ R is G-invariant.
Consider the subspace B)M = LN)M BN, where L is the direct sum operator. Essentially, B)M
is the direct sum of all the subspaces corresponding to the strict supersets of M . Using induction on
the size of M, we first show that B)M = N)M BN. The statement trivially holds for B){1,...,m}.
Then the induction hypothesis is: for all sets M such that |M| > k, we have B)M = LN)M BeN.
We prove that the statement holds for any set M with |M | = k as follows,
B)M = M BN
N)M
=MM	(BN ㊉ B)N)
N)M
|N|=|M|+1
=	㊉	(OrthB)N (BN)㊉ B)N)	(Definition of BN)
N)M
|N|=|M|+1
= MM	(BN ㊉ B)N) (For vector subspaces V and W, orthw (V)㊉ W = V ㊉ W)
N)M
|N|=|M|+1
= MM	(BN ㊉ B)N)	(Inductive hypothesis holds for sets N as |N| > k)
N)M
|N|=|M|+1
= M BeN .
N)M
This proves our claim that B)M = LN)M BN = LN)M BeN.
Now we are ready to prove the theorem. We begin by showing that any nonzero W ∈ vec(X) is
Gm-invariant where GM = h∪i∈MGii iff W ∈ BM. Since W ∈ BM ^⇒ W ∈ Wi, ∀i ∈ M,
we have from Lemma 2 that any W ∈ BM is Gi -invariant for all i ∈ M. Then it is easy to see that
18
Published as a conference paper at ICLR 2021
Full basis: (
9 eigenvectors
(a) Wrot
(b) Wcol
Figure 4: (a) 1-eigenspace of the Reynolds operator for the rotation group. The eigenspace has
nine basis vectors v ∈ R27 (stacked). We are representing these eigenvectors in R3×3×3 instead
to emphasize that these are rotation-invariant. (b) 1-eigenspace of the Reynolds operator for the
color-permutation group. The eigenspace again has nine basis vectors v ∈ R27 but we represent them
in R3×3×3 to emphasize that these are invariant to permutations of color channels.
any nonzero w is GM -invariant iff it is Gi-invariant for all i ∈ M. It is possible to have BM = {0}
implying that there is no nonzero w ∈ vec(X) that is GM -invariant.
Next note that for all N ) M, we have BN ⊆ BM (using the definition of BM). Then, their direct
sum is the smallest subspace containing all such BN and thus, N)M BN ⊆ BM. From our claim
earlier, this implies that B)M = N)M BN ⊆ BM. Finally, we have BM = orthB)M (BM) ⊆ BM
for all M . Thus, we have proved that any nonzero w ∈ BM also lies in BM and hence is invariant to
GM .
In the sequel, we will prove that any w ∈ BM is not Gj -invariant for any j ∈ {1, . . . , m} \ M. Let
P ) M. Then it is clear that B)M = LN)M BeN ⊇ BeP, which implies from the first part of our
proof that any w ∈ vec(X) that is GP -invariant lies inside B)M. The orthogonalization step ensures
that BM ⊥ B)M and thus, BM ⊥ BeP and BM ∩ BeP = {0}. Hence there is no nonzero w ∈ BM
such that w is GP -invariant. This applies for all supersets P ) M.
Finally, we consider supersets of M of the form P0 = M ∪ {j} for j ∈ {1, . . . , m} \ M. If a
nonzero w ∈ BM is invariant to Gj, then it will hold that w is invariant to GP0 , P0 ) M, resulting
in a contradiction. Hence, we have that if BM 6= {0}, any w ∈ BM \ {0} is GM -invariant but not
Gj -invariant for any j ∈ {1, . . . , m} \ M .
□
C Example construction of CG-invariant neurons
In this section, we will present a detailed example of the construction of CG-invariant neurons.
Consider a 3 × 3 image with 3 channels, thus X = R3×3×3. Then, a convolutional filter w ∈ X =
R3×3×3 multiplies elementwise with the image x ∈ X .
Consider m = 2 groups Grot and Gcol, the former rotates the image patch by 90-degree multiples and
the latter permutes the color channels of the image. Our goal is to enforce invariance to rotation and
color channel unless contradicted by training data. Note that vec(X) = R27.
Step 1: Construct 1-eigenspace of Reynolds operator for each group. Since we only consider
linear automorphism groups, each transformation T in the group can be written as T (x) = Tx,
where T is a matrix of size R27×27 and x ∈ vec(X) = R27. Given a group, we can directly use
Lemma 1 to construct the Reynolds operator by averaging over all the linear transformations (or
corresponding matrices) in the group. Then, we can use standard methods in linear algebra to find the
1-eigenspace of the Reynolds operator (i.e., find the eigenvectors with corresponding eigenvalues
equal to 1).
Let Wrot and Wcol be the 1-eigenspaces of the Reynolds operator of the groups Grot and Gcol re-
spectively. Figure 4 shows these eigenspaces with the eigenvectors arranged in R3×3×3 instead of
R27 . The figure shows that the eigenvectors in Wrot are invariant to rotations of 90-degree multiples
whereas the eigenvectors in Wcol have the same values across the RGB channels, and thus are invariant
to permutation of these channels. Lemma 2 proves this invariance-property for the 1-eigenspaces of
the Reynolds operator of any finite linear automorphism group.
19
Published as a conference paper at ICLR 2021
M - {rot, col}
SJOlQaQaSDq ∞
M = {rot}
SlsJ0Q SlSDq 9
M = {col}
SlsJ0Q s∙-SDq 9
sjo:PaQ SlSDq Er
s~0β3AU3-3 6
Figure 5: The subspaces BM for all M ⊆ {rot, col}. For instance, B{rot,col} on the top has 3
basis vectors (represented in R3×3×3) and each of these vectors are both rotation-invariant and
channel-permutation invariant. On the other hand, B{rot} (of dimension 6) is rotation invariant but
strictly not channel-permutation invariant. Finally, the vectors in B° are neither rotation-invariant
nor channel-permutation invariant. All the basis vectors together cover the entire space R27 (i.e.,
dim(B{rot,col}) + dim(B{ro}) + dim(B{col}) + dim(B0) = 3 + 6 + 6 + 12 = 27).
Step 2: Construct BM for all M ⊆ {rot, col}. Now, given Wrot and Wcol, we will construct basis
for the subspaces BM for all M ⊆ {rot, col} using Theorem 3.
1.	Set M = {rot, col}.
B{rot,col} = Wrot ∩ Wcol
B{rot,col} = B{rot,col} .	(because B){rot,col} = {0})
The intersection of subspaces Wrot ∩ Wcol can be computed using standard methods in
linear algebra. The subspace B{rot,col} with 3 basis vectors is visualized in the topmost
level of Figure 5. As before the basis vectors of the subspace are represented in R3×3×3 .
It is clear that the basis vectors are invariant to both rotation and permutation of the
channels. This property will hold for any linear combination of the basis vectors, i.e., for
any w ∈ B{rot,col} .
2.	Set M = {rot}.
B{rot} = Wrot
B{rot} = orthB){rot} (B{rot} )
= orthB{rot,col} (B{rot} )	(because B){rot} = B{rot,col})
The subspace B{rot} consists of all vectors that are invariant to rotation but also includes
vectors that are invariant to both rotation and channel-permutation. Thus, we need to remove
from B{rot} the projection of B{rot} on B{rot,col}.
20
Published as a conference paper at ICLR 2021
The subspace B{rot} with 6 basis vectors is visualized in middle level of Figure 5. It is clear
that the basis vectors are invariant to rotation but not invariant to channel-permutations.
Again, this property holds for any linear combination of the basis vectors.
3.	Set M = {col}.
B{col} = Wcol
B{col} = orthB){col} (B{col})
= orthB{rot,col} (B{col} )	(because B){col} = B{rot,col})
The subspace B{col} is obtained in a similar fashion. B{col} has 6 basis vectors and is
visualized in middle level of Figure 5. It is clear that the basis vectors are invariant to
channel-permutations but not invariant to rotation. This property holds for any linear
combination of the basis vectors.
4.	Set M = 0.
B0 = OrthB)0 (Vec(X)),
where B)0 = B{rot,coi}㊉ B{rot}㊉ B{coi}. The subspace B0 represents the rest of the space
that is neither rotation-invariant nor channel-permutation-invariant. This subspace has
12 basis vectors and is visualized in the bottommost level of Figure 5.
Finally, we have B = 4 subspaces (enumerated above) with a total of 27 basis vectors covering the
entire space vec(X) = R27.
Step 3: Neuron construction. For each subspace BM , M ⊆ {rot, col}, we denote BM as the
corresponding matrix with columns as the basis vectors of the subspace BM. As described above any
linear combination of the basis vectors of BM are invariant to all groups indexed by M and nothing
more (e.g., B{rot} consists of vectors invariant to rotation but not invariant to channel-permutation).
In the following, we consider a single neuron and drop the subscript h from ωM,h (where h repre-
sented the h-th neuron in Equation (10)). Recall that ωM ∈ RdM are the learnable parameters of
the neuron corresponding to each basis vector of the subspace BM, and dM is the dimension of the
subspace BM. Then, ω{rot,col} ∈ R3 represents the coefficients in the linear combination of the basis
vectors in B{rot,col}. The linear combination is given by the matrix-vector product B{rot,col}ω{rot,col}.
Similarly, ω{rot} ∈ R6, ω{col} ∈ R6, ω0 ∈ R12 represent the coefficients of the basis vectors in the
columns of B{rot}, B{col} and B0 respectively.
Then, a CG-invariant neuron is given by,
Γ(x) = xTw + b ,
where
w = B{rot,col}ω{rot,col} + B{rot}ω{rot} + B{col}ω{col} + B0ω0 ,
and ω{rot,col}, ω{rot}, ω{col}, ω0, b ∈ R are the only learnable parameters. The total number of
parameters is 28, same as that of the standard neuron with input x ∈ R27 .
Now, if for example the optimization finds ω{rot,col} 6= 0, ω{rot} = 0, ω{col} = 0 and ω0 = 0, then
the neuron Γ(∙) is invariant to both rotation and channel-permutation.
Our regularization in Equation (11) forces the optimization to find maximum invariance as long as
training performance is unaffected. A more comprehensive example of the computation of the penalty
is given in Appendix F.
D Pseudocode for Theorem 3
We present the algorithm for Theorem 3 in Algorithm 1. The loops in the algorithm iterate over the
different subsets M ⊆ {1, . . . , m} in descending order of their sizes. The worst-case complexity of
the algorithm is exponential in m (to iterate over all subsets). However, since the algorithm stops after
finding all the basis for the space vec(X), it is unclear if the worst-case runtime occurs in practice.
21
Published as a conference paper at ICLR 2021
Figure 6: An example architecture of CG-invariant CNN architecture.
Moreover, the algorithm only needs to run once for a given collection of groups and the results can
be reused in all experiments.
Algorithm 1: Procedure to construct basis for the subspaces BM of Theorem 3.
Input: Left 1-eigenspaces of the Reynolds operator Wι, W2,..., Wm for groups
G1 , G2 , . . . , Gm respectively.
Result: Basis for nonzero subspaces BM1, BM2, . . . , BMB, with B ≤ dX and
Mi ⊆ {1, . . . ,m}.
// Initialization
l — m ;
Cf ;
k — 1 ；
while l ≥ 0 do
// A counter for the subspaces.
/* l will denote the size of subsets M ⊆ {1, . . . , m}, denoting
the level of invariance.
Pl TM ：|M| =l,M ⊆{1,...,m}};
for M in Pl do
if M〜=0 then
I BM 一 ∩i∈MWi ；	/* Intersection of 1-eigenspaces.
else
I BM J Vec(X) ;	/* Used to find the subspace B^ .
end
// Direct sum of subspaces of supersets of M.
B)M = N)M BN ;
BM J orthB)M (BM) ;
if BM 6= 0 then
Mk J M ；	/* Record current subspace to return
k J k + 1 ;
end
CJC ㊉ BM ;
if dim(C) = dim(vec(X)) then
I break while;	/ * Found basis for the entire space.
end
end
end
lJl-1
*/
*/
*/
*/
*/
BJk-1;
return BM1, BM2, . . . ,BMB ;
/* Number of subspaces. */
E Architectures
E.1 Images
An example CG-inVariant CNN architecture is depicted in Figure 6. Majority of the CNN architecture
remains the same with the exception that the filters are obtained using the bases of the subspaces
obtained in Theorem 3 for the giVen set of groups. Figure 5 shows example subspaces along with
22
Published as a conference paper at ICLR 2021
Figure 7: An example architecture of CG-invariant feedforward network.
their basis vectors when the groups are just Grot and Gcolor, and the kernel size is 3 × 3 applied over
an input with 3 channels. One can similarly obtain these subspaces for other groups, different kernel
sizes and different number of input channels. Then, the filter is obtained as a linear combination of
these basis vectors, where the coefficients form the learnable parameters. The G-invariance of the
filter then depends upon which of these coefficients are nonzero. Once the filter is obtained, it is
convolved with the image or the feature maps. This will ensure that the model can be CG-invariant to
transformations of smaller patches in the image if needed.
Max-pooling layers function in the standard way. After all the convolutional and max-pooling layers,
we use a sum-pooling layer over the entire channel to ensure that the model can be invariant to the
transformations (e.g., rotations) on the whole image if needed. Finally, any number of dense layers
can be added after the sum-pooling layer.
In our experiments, we use the three groups Grot, Gcolor and Gvertical-flip to construct the subspaces for
the filters of the first convolutional layer, but remove Gcolor in the further layers as we do not wish to
be invariant to channel permutation after the first layer.
E.2 Sequences
A CG-invariant architecture for sequences is depicted in Figure 7. Consider a sequence X =
[x1, . . . , xn] ∈ Rp×n of length n and groups G1, . . . , Gm as before. In the following discussion, we
will assume that the groups are permutation groups over the sequence elements. However, one could
also consider other groups over X .
First, each element of the sequence is passed through a shared feedforward network φ that returns
a representation Z ∈ Rp0×n. Then, Theorem 3 finds the bases for BM, M ⊆ {1, . . . , m} until
all the p0n basis vectors are found covering the space Rp0 ×n . The weight vectors for the h-th
neuron of the CG-invariant layer is obtained as a linear combination of these basis vectors via the
learnable parameters Ω (Equation (10)). Finally, any number of dense layers can be stacked after the
CG-invariant layer for the final output.
F	Regularization
F.1 Example
Figure 8 shows an example computation of the penalty in Equation (12). The example considers
an image task with m = 3 groups: Grot , Gcol , Gvflip. Each cell in the figure shows one subset
M ⊆ {rot, col, vflip}. The subsets are arranged according to their levels of invariance, i.e., by
the size of |M |. For example, the topmost cell {rot, col, vflip} denotes the subspace with all the
invariances whereas the bottommost cell 0 denotes the subspace with no invariance.
The colors indicate the state of the parameters Ω ata single point in the optimization. The cells are
colored green or red depending on whether the subspace is used or unused respectively, i.e., whether
the parameters corresponding to the subspace are nonzero or not. The least invariant subspaces used
23
Published as a conference paper at ICLR 2021
LEVEL 3
LEVEL 2
LEVEL 1
LEVEL0
Figure 8: (Best viewed in color) Describing the computation of the penalty. The cells denote different
subsets M ⊆ {rot, col, vflip}. Red colored cells denote that the parameters corresponding to these
subspaces are zero (i.e., the subspaces are unused) and the green colored cells denote otherwise (i.e.,
the subspaces are used). In this example, the least invariant subspaces used are in Level 1. The
penalty counts all the subspaces (used or unused) that are in higher levels (i.e., with |M| > 1) and
adds it to the number of subspaces of the same level that are used.
at this point are in Level 1 (i.e., invariant to a single group). The penalty counts (a) all subspaces
with higher levels of invariance irrespective of whether the subspace is used or not, and (b) counts
all the used subspaces with the same level of invariance. The former penalizes the use of subspaces
lower in the partial order and ensures that subspaces with higher levels of invariance are used. The
latter approximates the effort to reach a higher level of invariance.
F.2 Differentiable Approximation
Recall that the regularization penalty R(Ω) in Equation (12) is given by,
R(Ω) =	fι(Ω)	:=	|{Mi	:	|Mi|	>l, 1 ≤ i ≤ B}| + X	1{||3.“52 >	0}	,	(16)
i:|Mi |=l
1≤i≤B
where I = min{∣M∕ ∙ 1{口3村”.k2 > 0}, 1 ≤ i ≤ B}.
R(Ω) is clearly discrete but can be approximated by a differentiable formula. First, We replace the
indicator function 1{z > 0} in Equation (16) with the approximation 1{z > 0} = τz∕(τz + 1),
where τ ≥ 1 is a temperature hyperparameter.
Then, in order to obtain R(Ω) = fι(Ω) for the minimum l defined in Equation (16), we use the
following recursion: R(Ω) = Rm(Ω), and
Rι(Ω) = (1 - βι(Ω)) ∙ Ri-ι(Ω) + fι(Ω)βι(Ω) l = 1,...,m,
with the base case Ro(Ω) = 0, and βι(Ω) = i{PNi:INi∣=l, 1≤i≤B。3超,52 > 0}. βι(Ω) is
approximately one if at least one neuron h has nonzero ωNi,h parameters for some Ni ⊆ {1, . . . , m}
of size l (i.e., with l groups). Then the recursion finds fι(Ω) with l defined as the size of the least
invariant subspace used.
F.3 LIMITATION OF R(Ω)
As explained in Section 4.2, there could be overgroups (out of the total 2m groups considered) with
different levels of invariance, but penalized similarly by Equation (12). This scenario arises only in
cases when Theorem 3 does not construct subspace basis for all the 2m overgroups, i.e., the basis for
vec(X ) is found prior to that. In this section, we provide such an example scenario with sequence
inputs and the transposition groups considered in Section 6.
24
Published as a conference paper at ICLR 2021
M = {(*5J) ∙i,3 ∈ H ∖ {1}∕ < J}
Z
oJaαSDq I
i-o4-Jaα SgSDq I
Figure 9: (Best viewed in color) The subspaces BM for different M ⊆ {(i, j)}1≤i<j≤n indexing
the m = n2 transposition groups Gi,j over sequences of length n = 5 and dimension d = 1. Each
of the subspaces BM is of dimension 1. For each basis vector shown above, elements sharing the
same color have the same value. At the topmost level, we have the subspace with most invariance,
i.e., invariant to the full permutation group Sn . Following many levels with empty subspaces, we
have subspaces BMp for Mp = {(i, j) | i, j ∈ [n] \ {p}, i < j}, where [n] = {1, . . . , n}. In other
words, the subspace BMp is invariant to all transpositions except those that move index p. Note
that we have covered the entire space Rn with these n independent subspaces of dimension 1.
Let X ∈ X = Rn be a 1-dimensional sequence of length n. The transposition groups are
{Gi,j }1≤i<j≤n, where Gi,j = {Tidentity, Ti,j } and Ti,j swaps positions i and j in the sequence.
Given these m = n2 groups, we can use Lemmas 1 and 2, and Theorem 3 to find the invariant
subspaces BM for subsets M ⊆ {(i, j) | 1 ≤ i < j ≤ n} indexing the transposition groups. The
basis vectors for these subspaces constructed for sequence length n = 5 are visualized in Figure 9.
There are n 1-dimensional subspaces. Let the vectors b\$, b∖{i}... b∖{n-i} denote these n basis
vectors. The notation \A means that the vector has the same value for all positions k ∈ {1, . . . , n} \A
(cf. Figure 9). Let n = 5 and note that any weight vector ω ∈ R5 can be written as,
ω = α1b∖0 + α2b∖{1} + α3b∖{2} + α4b∖{3} + α5b∖{4} ∙	(17)
where α ∈ R5 .
Let α0 = (1, 0, 1, 0, 1)T. From a quick read of Figure 9, we see that the weight ω0 obtained by
substituting α0 in Equation (17) is such that ω10 = ω30 = ω50 and ω20 = w40 . For any input x ∈ R5, the
neuron σ(ω0T x + b) is invariant to any permutation of x1, x3 and x5, and, transposition of x2 and
x4. The penalty R(ω0) = 3 as there are 2 subspaces used at the lowest level and there is 1 subspace
above the lowest level (see Equation (12)).
Now let α00 = (1, 0, 1, 0, 1.5)T. The weight ω00 obtained by substituting α00 in Equation (17) is such
that ω100 = ω300 = ω500 but ω200 6= ω400. For input x ∈ R5, the neuron σ(ω00T x + b) is invariant to any
permutation ofx010, x030 and x050, but sensitive to the transposition ofx2 and x4. The penalty R(ω00) = 3
as the same subspaces are used as before.
In the first case, with all the parameters being equal (especially α03 = α05), ω0 lies in a smaller (more
invariant) subspace of SPan(b∖0, b∖{2}, b∖{4}). In the second case, since a《=αg’, the same does
not hold for ω00. The penalty R(∙), which only counts the subspaces used (in this case, b∖0, b∖{2}
and b∖{4}), is unable to distinguish between these two weight vectors ω0 and ω00, one clearly more
invariant than the other.
25
Published as a conference paper at ICLR 2021
In this specific case with transposition groups over sequences, one could add another penalty term that
regularizes the parameters αi to share the same value (e.g., entropy regularization of the parameters).
We leave further investigation into the general scenario with other groups for future work.
G	Datasets and Empirical Results
G.1 Images
Datasets. We consider the standard MNIST dataset and its subset MNIST-34 that contains only the
digits 3 & 4 alone. We chose to experiment on the MNIST-34 dataset since it does not have digits that
can be confused with a rotation transformation (e.g., 6 and 9) or are invariant to some rotations (e.g.,
0, 1 and 8), thus avoiding any confounding factors while testing our hypothesis. We also experiment
on the full MNIST dataset to depict the scenario when the data does contain these contradictions.
First, we modify all the images in the dataset to have three RGB color channels and color each digit
red initially, i.e., all active pixels in the digit are set to (255, 0, 0). We sample X(hid) from this dataset
with the target digit as its original label.
Groups. We consider m = 3 linear automorphism groups on images: the rotation group Grot =
{T(0°),T(90O),T(180O),t(270°)} that rotates the entire image by multiples of 90°, the Channel-
permutation group Gcolor = {Tα}α∈S3 that permutes the three RGB channels of the image, and the
vertical flip group Gvertical-flip = {T(0), T(v)} that vertically flips the image.
Tasks. For both MNIST and MNIST-34 datasets, we consider 4 classification tasks where each
task represents the case when the target Y is invariant to a different subset of {Grot, Gvertical-flip, Gcolor},
i.e., invariant to all three groups, to two, to one, invariant to none (and sensitive to the remaining
groups). We consider the following subsets I: i) {rot, color, vertical-flip}, ii) {rot, vertical-flip},
iii) {color}, iv) 0, and generate GI = h∪i∈ιGii as the join of the respective groups. Setting
D = {rot, color, vertical-flip} \ I, we generate GD = h∪j∈DGji from the join of groups in the
complement set (our choices ensure that GI E GD∪I , thus satisfying the conditions of Theorem 2).
Training data: X(hid) is the canonically ordered (standard) image in the MNIST datasets. Recall
that the training data is sampled via an economical data generation process. Thus the training data
consists only of images under transformations that have an effect on the label, i.e., transformations
from GD .
Recall from Equation (2) that the observed input is obtained as X = TUI,UD ◦X(hid), a transformation
of the canonical input X(hid). Since GI E GD (by construction), we have that any TUI,UD =
TU0 |UD ◦ TUD, i.e., the transformation can be decomposed into one transformation from GD followed
by another transformation from GI . UI0 | UD in the subscript indicates that the transformation
TU0 |UD ∈ GI also depends on UD. Under the assumption of economic sampling of training data, in
all our experiments we sample a single value for TU0 |UD ∈ GI: we simply use TU0 |UD = Tidentity
(one could consider any other transformation in GI as well).
In conclusion, we obtain the observed image X in the training data by applying a random transfor-
mation from GD to X (hid) and then applying a constant transformation (e.g., Tidentity) from GI to the
result. The task is to predict the original label of the image (i.e., the digit) and the transformation TUD
that was applied to obtain X (recall from Equation (3) that Y is a function of both X(hid) and UD).
For instance, if GI = Grot, vertical-flip and GD = Gcolor, then the training data consists of upright and
unflipped images (as TU0 |UD is chosen to be identity transformation) with different permutations
of the color channels (since random transformations are sampled from GD) resulting in digits with
different colors. Then, the task is to predict the digit and its color.
Extrapolation task: The extrapolated test data consists of samples from the coupled random variable
XU工.u工(Definition 1). Unlike the training data that was economically sampled (i.e., with a single
transformation from GI), the extrapolated test data is obtained via the full range of transformations
in GI. Recall from Definition 1 that XU .Ue = TUe ,U ◦ X(hid). As before, we decompose
TUe ,U = TUe0 |U ◦ TUD. However, there is no economic sampling for the test data: TUe0 |U and
TUD are sampled randomly from GI and GD respectively.
26
Published as a conference paper at ICLR 2021
Table 2: (MNIST-34.) Validation and Extrapolation test accuracies (%) with 95% confidence intervals
for different CG-regularization strength λ in Equation (11). λ is chosen only based on the validation
accuracy: maximum λ with validation accuracy within 5% of the best validation accuracy (bold
values indicate the performance of this choice of λ).
Model	λ	rot,color,vflip		I ⊆ {rot, color, vflip}				Val. acc (%)	0 Test acc (%)
				rot,vflip		color			
		Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)		
VGG + CG-reg	0.0	99.94 ( 0.17)	49.51 ( 2.36)	99.83 ( 0.09)	43.57( 3.69)	97.15 ( 0.27)	15.71 ( 5.46)	95.65 ( 0.39)	96.30 ( 0.68)
	0.1	99.92 ( 0.09)	78.72 (25.75)	99.86 ( 0.21)	73.98 (16.33)	96.48 ( 0.77)	96.27 ( 1.01)	94.95 ( 0.54)	95.56 ( 0.61)
	1.0	99.71 ( 0.22)	85.42 (29.66)	99.77 ( 0.25)	75.64 (20.52)	96.12 ( 1.26)	96.20 ( 1.11)	94.01 ( 1.51)	94.42 ( 1.38)
	2.0	99.55 ( 0.33)	94.88 ( 0.84)	99.56 ( 0.66)	82.59 (28.92)	95.23 ( 0.99)	95.61 ( 1.75)	94.05 ( 1.50)	94.49 ( 1.49)
	10.0	99.00 ( 1.18)	94.89 ( 7.49)	98.43 ( 2.00)	95.78 ( 7.11)	93.34 ( 8.42)	94.16 ( 6.43)	88.42 (19.36)	88.68 (20.13)
Table 3: (MNIST.) Validation and Extrapolation test accuracies (%) with 95% confidence intervals
for different CG-regularization strength λ in Equation (11). λ is chosen only based on the validation
accuracy: maximum λ with validation accuracy within 5% of the best validation accuracy (bold
values indicate the performance of this choice of λ).
Model	λ	rot,color,vflip		I ⊆ {rot, color, vflip}				0	
				rot,vflip		color			
		Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)
VGG + CG-reg	0.0	99.17 ( 0.17)	11.93 ( 1.87)	98.80 ( 0.14)	25.81 ( 0.92)	91.50 ( 0.35)	4.19 ( 2.12)	91.80 ( 0.60)	91.60 ( 0.32)
	0.1	98.62 ( 0.05)	29.48 ( 0.98)	98.34 ( 0.17)	30.12 ( 4.05)	90.11 ( 0.58)	87.23 ( 3.68)	88.30 ( 1.21)	88.48 ( 1.17)
	1.0	98.49 ( 0.24)	44.23 (15.45)	98.29 ( 0.21)	40.13 ( 4.83)	90.24 ( 0.46)	90.23 ( 0.99)	88.76 ( 1.28)	88.65 ( 1.30)
	2.0	98.45 ( 0.13)	55.24 ( 2.29)	98.34 ( 0.34)	47.14 (15.17)	89.98 ( 0.24)	89.71 ( 0.89)	89.50 ( 1.35)	89.45 ( 1.43)
	10.0	97.76 ( 0.74)	64.99 ( 2.76)	95.21 ( 6.55)	62.68 ( 6.02)	88.80 ( 2.11)	88.69 ( 2.11)	90.54 ( 1.04)	90.89 ( 0.43)
In conclusion, we obtain the observed image X in the test data by applying a random transformation
from GD to X (hid) and then applying a random transformation from GI to the result. The task
is the same as in the training data: to predict the original label of the image (i.e., the digit) and
the transformation TUD that was applied to obtain X . Note that the label does not depend on the
transformation TUe 0 |U ∈ GI that was applied.
Once again, if GI = Grot, vertical-flip and GD = Gcolor, then the extrapolated test data consists of images
randomly rotated, flipped and channel permuted, while the task is the same: predict the digit and
its color.
In order to evaluate the models, we use 5-fold cross-validation procedure as follows. We divide the
training and test datasets that are pre-split in MNIST and MNIST-34 datasets into 5 folds each. We
use the above procedure to transform the training data and the test data. Then in each iteration i of the
cross-validation procedure, we leave out i-th fold of the transformed training data and i-th fold of the
extrapolated test data. Further, we use 20% of the training data as validation data for hyperparameter
tuning and early stopping.
Baselines and Architecture. For all methods, we use a VGG architecture (Simonyan and Zisser-
man, 2014) with 8 convolutional layers each having 128 channels except the first layer which has
64 channels. All convolutional layers have a receptive field of size 3 × 3, stride 1 and padding 1.
A max-pooling layer is added after every two convolutional layers. Two feedforward layers at the
end give the final output. We compare our approach with the standard CNNs and Group-equivariant
CNNs (G-CNNs) (Cohen and Welling, 2016) with the p4m group. We modified G-CNN such that it
has invariances to all the 3 groups strictly enforced via a) coset-pooling (Cohen and Welling, 2016)
after each layer and b) adding together the 3 input RGB channels. For our approach, we replace the
standard convolutional layer in the VGG architecture by CG-invariant layers with bases constructed
from Grot , Gcolor and Gvertical-flip. An example architecture with only 2 convolutional layers is shown in
Figure 6.
We optimize all models using SGD with momentum with learning rate in {10-2, 10-3, 10-4} and a
batch size of 64. We use early stopping on validation loss to select the best model. Further, we use
validation loss to select the best set of hyperparameters for each model. We choose the maximum
value of λ with validation accuracy within a 5% threshold of the maximum validation accuracy
27
Published as a conference paper at ICLR 2021
Table 4: Sequence tasks. The first column defines the target Y for a given sequence (Xi)i1=01. The
second column denotes GI , the group of transformations to which Y is invariant. Recall that GI is
constructed as the join of a subset of 120 transposition groups.
Target Y	GI	GD
Ytask-1 = Pi1=0 1 Xi	h{Gi,j}1≤i<j≤ni Ytask-2 =	i1=0 2 Xi	h{Gi,j}2≤i<j≤ni Ytask-3 =	i=1 (X2i - X2i-1 )	h{Gi,i+2k }1≤i<i+2k≤ni KaSk-4 = Pl=1Qj=11(Xj ≥20)	{Id}	—	—	{Id} {Id} {Id} h{Gi,j}1≤i<j≤ni
obtained from any value of λ. Tables 2 and 3 show the effect of regularization strength on the
performance of the model. We observe that λ = 10 performs considerably well across all tasks.
G.2 Sequences
Datasets. For sequence tasks, we generate X(hid) = (Xi)i1=0 1 as a sequence of n = 10 canonically
ordered integers uniformly sampled with replacement from a fixed vocabulary set {1, . . . , 99}. The
canonical ordering is fixed for a given set of integers sampled: the corresponding sequence X(hid) is
always either in an increasing order or a decreasing order.
Groups. We consider m = n2 permutation groups for all the pair-wise permutations:
G1,2, G2,3, G1,3 , . . . , Gn-1,n, where Gi,j := {Tidentity, Ti,j } and Ti,j swaps positions i and j in
the sequence. For I ⊆ {(1, 2), (2, 3), (1, 3), . . . , (n - 1, n)}, GI is defined as before as the join
h∪(i,j)∈I Gi,j i. We choose 4 different subsets I of the given m groups indicated by the second
column of Table 4. For our choices of I= 0, We set D = 0 to ensure that GI E Gd∪i, i.e., GI is a
normal subgroup of GD∪I.
Tasks. The label for the sequence X (hid) is obtained by applying an arithmetic function to X (hid) that
is invariant to the chosen group GI. The arithmetic functions are given in the first column of Table 4.
Ytask-1 is invariant to any permutation of the input elements Xi , 1 ≤ i ≤ n. Ytask-2 is invariant to any
permutation of input elements Xi With indices i > 1 but sensitive to permutations that move X1.
Ytask-3 is invariant to permutations that move elements at even indices to even indices and elements
at odd indices to odd indices respectively. Finally, Ytask-4 is sensitive to all permutations (i.e., no
invariance).
Training data: Recall that X (hid) is in a sorted order. Since the training data is sampled economically, it
consists only of sequences under transformations that have an effect on the label, i.e., transformations
from GD. The observed input is obtained as X = TUI,UD ◦ X(hid), a transformation of the sorted
input X(hid).	Since	GI	E	GD	(by construction), We have that any	TUI,UD	=	TU0 |UD	◦	TUD,
i.e., the transformation can be decomposed into one transformation from GD folloWed by another
transformation from GI. UI0 | UD in the subscript indicates that the transformation TU0 |UD ∈ GI also
depends on UD . Under the assumption of economic sampling of training data, in all our experiments
We sample a single value for TUI0 |UD ∈ GI: We simply use TUI0 |UD = Tidentity.
In conclusion, We obtain the observed sequence X in the training data by applying a random
transformation TUD ∈ GD to X (hid) and then applying a constant transformation (e.g., Tidentity) from
GI to the result. The target Y is computed by applying the arithmetic function corresponding to the
task (see Table 4) to TUD ◦ X(hid) (recall from Equation (3) that Y is a function of both X(hid) and
UD).
Extrapolation task: The extrapolated test data consists of samples from the coupled random variable
XU工.u工(Definition 1). Unlike the training data that was economically sampled (i.e., with a single
transformation from GI), the extrapolated test data is obtained via the full range of transformations
in GI. Recall from Definition 1 that XU .Ue = TUe ,U ◦ X(hid). As before, we decompose
TUe ,U = TUe0 |U ◦ TUD. However, there is no economic sampling for the test data: TUe0 |U and
TUD are sampled randomly from GI and GD respectively.
28
Published as a conference paper at ICLR 2021
Table 5: (Sequence tasks) Extrapolation test accuracies (%) with 95% confidence intervals for all
the models (bold means p < 0.05 significant). The standard sequence models cannot extrapolate
when I= 0 whereas the forced G-invariant models cannot unlearn the invariances and fail when
I ( {1, . . . , m}.
GI
Model	h{Gi,j}1≤i<j≤ni	h{Gi,j}2≤i<j ≤n i	h{Gi,i+2k}1≤i<i+2k≤ni	{Id}
DeepSets (Zaheer et al., 2017)	100.00 ( 0.00)	2.36( 2.37)	0.97 ( 0.60)	16.12 ( 8.21)
Janossy pooling (Murphy et al., 2018)	96.64 ( 3.13)	9.55 ( 1.61)	0.78 ( 0.52)	21.22 ( 2.94)
Set Transformer (Lee et al., 2019)	99.57 ( 0.33)	10.68 ( 1.49)	0.75 ( 0.28)	23.38 ( 1.88)
Transformer (Vaswani et al., 2017)	20.26 (32.08)	12.15 (16.05)	0.85 ( 0.37)	100.00 ( 0.00)
GRU (Cho et al., 2014)	0.48 ( 0.48)	0.47 ( 0.38)	0.90 ( 0.77)	99.41 ( 1.58)
FF + CG-reg. (ours)	100.00 ( 0.00)	42.08 (18.99)	71.85 (26.61)	95.70 ( 3.05)
Table 6: (Sequence Tasks) Validation and Extrapolation test accuracies (%) with 95% confidence
intervals for different CG-regularization strength λ in Equation (11). λ is chosen only based on the
validation accuracy: maximum λ with validation accuracy within 5% of the best validation accuracy
(bold values indicate the performance of this choice of λ).
GI
h{Gi,j}1≤i<j≤ni	h{Gi,j}2≤i<j≤ni	h{Gi,i+2k}1≤i<i+2k≤ni	{Id}
Model	λ	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)	Val. acc (%)	Test acc (%)
FF + CG-reg.	0.0	80.80 (84.15)	54.34 (87.19)	100.00 ( 0.00)	80.36(74.04)	99.95 ( 0.10)	22.78 (14.52)	99.92 ( 0.27)	99.86 ( 0.15)
	0.1	80.83 (84.04)	59.18 (91.12)	100.00 ( 0.00)	65.13 (80.43)	99.99 ( 0.05)	60.66 (49.83)	99.95 ( 0.10)	99.98 ( 0.05)
	1.0	80.72 (84.48)	80.03 (87.52)	99.04 ( 4.22)	61.81 (66.24)	100.00 ( 0.00)	68.34 (36.98)	99.81 ( 0.55)	99.76 ( 0.65)
	2.0	82.56 (72.85)	63.16 (99.44)	100.00 ( 0.00)	77.97 (48.87)	99.99 ( 0.05)	69.20 (31.40)	99.46 ( 0.53)	99.37 ( 0.53)
	10.0	80.97 (74.10)	62.83 (100.17)	98.14 ( 2.71)	42.08 (18.99)	100.00 ( 0.00)	71.85 (26.61)	95.56 ( 3.34)	95.70 ( 3.05)
	100.0	100.00 ( 0.00)	100.00 ( 0.00)	15.65 ( 3.63)	2.29 ( 0.96)	93.42 (14.90)	27.64 (24.85)	65.92 (10.38)	65.42 (10.30)
In conclusion, we obtain the observed sequence X in the test data by applying a random transformation
TUD ∈ GD to X(hid) and then applying a random transformation from GI to the result. The target Y is
computed in a similar fashion as in the training data by applying the appropriate arithmetic function
to TUD ◦ X(hid). Note that Y is invariant to GI .
Example: Consider the the first row of Table 4 with I = {(i, j)}1≤i<j≤n, i.e., it contains all the
m = n2 groups. Then, the group GI is simply the full permutation group over n elements. The
target is defined as the sum of elements (which is fully permutation-invariant). The sequences in
the training data are always sorted (because of the economic sampling of training data), whereas the
sequences in test data have arbitrarily different permutations (by sampling random transformations
from GI). The task is simply to compute the sum of the elements of the sequence.
Sizes of the training data and the extrapolated test data are fixed at 8000 and 2000 respectively. We
repeat all the experiments for 5 different random seeds.
Baselines and Architecture. We compare our approach with a) standard sequence models, specifi-
cally Transformers (Vaswani et al., 2017) and GRUs (Cho et al., 2014), and b) forced permutation-
invariant set models, specifically DeepSets (Zaheer et al., 2017), SetTransformer (Lee et al., 2019)
and Janossy Pooling (Murphy et al., 2018). An example of the proposed CG-invariant feedforward
architecture is depicted in Figure 7.
We optimize all models using Adam (Kingma and Ba, 2014) with an initial learning rate in
{10-2, 10-3, 10-4} and a batch size of 128. We use validation loss for early-stopping and to
select the best hyperparameters for all models. Once again, we choose the best value for the CG-
regularization strength λ by choosing the maximum value of λ with validation accuracy within 5% of
the maximum validation accuracy obtained from any λ. Table 6 shows the effect of regularization
strength on the performance of the model. We observe that although λ = 10 performs comparably
to the rest in validation accuracy and is chosen consistently, it does not achieve the best possible
extrapolation accuracy.
29
Published as a conference paper at ICLR 2021
Table 5 shows the complete set of results for all the models. The table clearly shows the issue with
standard sequence models (cannot extrapolate when I= 0) and the issue with forced G-invariant
models (fail when I ( {1, . . . , m}). In Table 4 of the main text, we show the results for the best
model out of all the permutation-invariant models in the column Best FF+G-inv.
30