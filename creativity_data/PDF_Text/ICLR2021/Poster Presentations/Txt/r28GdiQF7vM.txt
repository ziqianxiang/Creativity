Published as a conference paper at ICLR 2021
Sharper Generalization Bounds for Learning
with Gradient-dominated Objective Functions
YunWen Lei1,2 Yiming Ying3*
1 School of Computer Science, University of Birmingham, Birmingham B15 2TT, United Kingdom
2 Department of Computer Science, TU Kaiserslautern, Kaiserslautern 67653, Germany
3Department of Mathematics and Statistics, State University of New York at Albany, USA
y.lei@bham.ac.uk yying@albany.edu
Ab stract
Stochastic optimization has become the workhorse behind many successful ma-
chine learning applications, which motivates a lot of theoretical analysis to un-
derstand its empirical behavior. As a comparison, there is far less work to study
the generalization behavior especially in a non-convex learning setting. In this pa-
per, we study the generalization behavior of stochastic optimization by leveraging
the algorithmic stability for learning with β-gradient-dominated objective func-
tions. We develop generalization bounds of the order O(1∕(nβ)) plus the Conver-
gence rate of the optimization algorithm, where n is the sample size. Our stabil-
ity analysis significantly improves the existing non-convex analysis by removing
the bounded gradient assumption and implying better generalization bounds. We
achieve this improvement by exploiting the smoothness of loss functions instead
of the Lipschitz condition in Charles & Papailiopoulos (2018). We apply our gen-
eral results to various stochastic optimization algorithms, which show clearly how
the variance-reduction techniques improve not only training but also generaliza-
tion. Furthermore, our discussion explains how interpolation helps generalization
for highly expressive models.
1 Introduction
Stochastic optimization has found tremendous applications in training highly expressive machine
learning models including deep neural networks (DNNs) (Bottou et al., 2018), which are ubiquitous
in modern learning architectures (LeCun et al., 2015). Oftentimes, the models trained in this way
have not only very small training errors or even interpolate the training examples, but also surpris-
ingly generalize well to testing examples (Zhang et al., 2017). While the low training error can be
well explained by the over-parametrization of models and the efficiency of the optimization algo-
rithm in identifying a local minimizer (Bassily et al., 2018; Vaswani et al., 2019; Ma et al., 2018), it
is still unclear how the highly expressive models also achieve a low testing error (Ma et al., 2018).
With the recent theoretical and empirical study, it is believed that a joint consideration of the in-
teraction among the optimization algorithm, learning models and training examples is necessary to
understand the generalization behavior (Neyshabur et al., 2017; Hardt et al., 2016; Lin et al., 2016).
The generalization error for stochastic optimization typically consists of an optimization error and
an estimation error (see e.g. Bousquet & Bottou (2008)). Optimization errors arise from the sub-
optimality of the output of the chosen optimization algorithms, while estimation errors refer to
the discrepancy between the testing error and training error at the output model. There is a large
amount of literature on studying the optimization error (convergence) of stochastic optimization al-
gorithms (Bottou et al., 2018; Orabona, 2014; Karimi et al., 2016; Ying & Zhou, 2017; Liu et al.,
2018). In particular, the power of interpolation is clearly justified in boosting the convergence rate
of stochastic gradient descent (SGD) (Bassily et al., 2018; Vaswani et al., 2019; Ma et al., 2018).
In contrast, there is far less work on studying estimation errors of optimization algorithms. In a
seminal paper (Hardt et al., 2016), the fundamental concept of algorithmic stability was used to
study the generalization behavior of SGD, which was further improved and extended in Charles
& Papailiopoulos (2018); Zhou et al. (2018b); Yuan et al. (2019); Kuzborskij & Lampert (2018).
* Corresponding author: Yiming Ying
1
Published as a conference paper at ICLR 2021
However, these results are still not quite satisfactory in the following three aspects. Firstly, the ex-
isting stability bounds in non-convex learning require very small step sizes (Hardt et al., 2016) and
yield suboptimal generalization bounds (Yuan et al., 2019; Charles & Papailiopoulos, 2018; Zhou
et al., 2018b). Secondly, majority of the existing work has focused on functions with a uniform
Lipschitz constant which can be very large in practical models if not infinite (Bousquet & Elisse-
eff, 2002; Hardt et al., 2016; Charles & Papailiopoulos, 2018; Kuzborskij & Lampert, 2018), e.g.,
DNNs. Thirdly, the existing stability analysis fails to explain how the highly expressive models still
generalize in an interpolation setting, which is observed for overparameterized DNNs (Arora et al.,
2019; Brutzkus et al., 2017; Bassily et al., 2018; Belkin et al., 2019).
In this paper, we make attempts to address the above three issues using novel stability analysis
approaches. Our main contributions are summarized as follows.
1.	We develop general stability and generalization bounds for any learning algorithm to optimize
(non-convex) β-gradient-dominated objectives. Specifically, we show that the excess generalization
error is bounded by O(1∕(nβ)) plus the convergence rate of the algorithm, where n is the sample
size. This general theorem implies that overfitting will never happen in this case, and generalization
would always improve as we increase the training accuracy, which is due to an implicit regularization
effect of gradient dominance condition. In particular, we show that interpolation actually improves
generalization for highly expressive models. In contrast to the existing discussions based on either
hypothesis stability or uniform stability which imply at best a bound of O(1/√nβ), the main idea is
to consider a weaker on-average stability measure which allows us to replace the uniform Lipschitz
constant in Hardt et al. (2016); Kuzborskij & Lampert (2018); Charles & Papailiopoulos (2018) with
the training error of the best model.
2.	We apply our general results to various stochastic optimization algorithms, and highlight the ad-
vantage over existing generalization analysis. For example, we derive an exponential convergence of
testing errors for SGD in an interpolation setting, which complements the exponential convergence
of optimization errors (Bassily et al., 2018; Vaswani et al., 2019; Ma et al., 2018) and extends the
existing results (Pillaud-Vivien et al., 2018; Nitanda & Suzuki, 2019) from a strongly-convex setting
to a non-convex setting. In particular, we show that stochastic variance-reduced optimization out-
performs SGD by achieving a significantly faster convergence of testing errors, while this advantage
is only shown in terms of optimization errors in the literature (Reddi et al., 2016; Lei et al., 2017;
Nguyen et al., 2017; Zhou et al., 2018a; Wang et al., 2019).
2	Related Work
Algorithmic Stability. We first review the related work on stability and generalization. Algorith-
mic stability is a fundamental concept in statistical learning theory (Bousquet & Elisseeff, 2002;
Elisseeff et al., 2005), which has a deep connection with learnability (Shalev-Shwartz et al., 2010;
Rakhlin et al., 2005). The important uniform stability was introduced in Bousquet & Elisseeff
(2002), where the authors showed that empirical risk minimization (ERM) enjoys the uniform sta-
bility if the objective function is strongly convex. This concept was extended to study randomized
algorithms such as bagging and bootstrap (Elisseeff et al., 2005). An interesting trade-off between
uniform stability and convergence was developed for iterative optimization algorithms, which was
then used to study convergence lower bounds of different algorithms (Chen et al., 2018). While gen-
eralization bounds based on stability are often stated in expectation, uniform stability was recently
shown to guarantee almost optimal high-probability bounds based on elegant concentration inequal-
ities for weakly-dependent random variables (Maurer, 2017; Feldman & Vondrak, 2019; Bousquet
et al., 2020). Other than the standard classification and regression setting, uniform stability was very
successfully to study transfer learning (Kuzborskij & Lampert, 2018), PAC-Bayesian bounds (Lon-
don, 2017), privacy learning (Bassily et al., 2019) and pairwise learning (Lei et al., 2020b). Some
other stability measures include the uniform argument stability (Liu et al., 2017), hypothesis sta-
bility (Bousquet & Elisseeff, 2002), hypothesis set stability (Foster et al., 2019) and on-average
stability (Shalev-Shwartz et al., 2010). An advantage of on-average stability is that it is weaker than
the uniform stability and can imply better generalization by exploiting either the strong convexity
of the objective function (Shalev-Shwartz & Ben-David, 2014, Corollary 13.7) or the more relaxed
exp-concavity of loss functions (Koren & Levy, 2015; Gonen & Shalev-Shwartz, 2017). Since
gradient-dominance condition is another relaxed extension of strong convexity, we use on-average
stability to study generalization bounds.
2
Published as a conference paper at ICLR 2021
Generalization analysis. We now review related work on generalization analysis for stochastic op-
timization. In a seminal paper (Hardt et al., 2016), the authors used the nonexpansiveness of gradient
mapping to develop uniform stability bounds for SGD to optimize convex, strongly convex and even
non-convex objective functions. This inspired some interesting work on stochastic optimization. An
interesting data-dependent stability bound was developed for SGD, a nice property of which is that it
shows how the initialization would affect generalization (Kuzborskij & Lampert, 2018). These sta-
bility bounds were integrated into a PAC-Bayesian analysis of SGD, yielding generalization bounds
for arbitrary posterior distributions (London, 2017). Almost optimal generalization bounds were
developed for differentially private stochastic convex optimization (Bassily et al., 2019). The on-
average variance of stochastic gradients was used to refine the generalization analysis of SGD (Hardt
et al., 2016) in non-convex optimization (Zhou et al., 2018b). The uniform stability was also studied
for SGD implemented in a stagewise manner (Yuan et al., 2019) and stochastic gradient Langevin
dynamics in a non-convex setting (Li et al., 2020; Mou et al., 2018). Very recently, the discussions in
Hardt et al. (2016) were extended to tackle non-smooth (Lei & Ying, 2020; Bassily et al., 2020) and
non-Lipscthiz functions (Lei & Ying, 2020). The most related work is Charles & Papailiopoulos
(2018), where some general hypothesis stability bounds were developed for learning algorithms that
converge to optima. A very interesting point is that their bounds depend only on the convergence of
the algorithm to a global minimum and the geometry of loss functions around the global minimum.
However, their discussion imply at best the slow generalization bounds O(1∕√nβ) for β-gradient-
dominated objective functions, and can not explain the benefit of low optimization errors in helping
generalization. The underlying reason is that they used the pointwise hypothesis stability and did
not consider the smoothness of loss functions. We aim to improve these results by leveraging the
weaker on-average stability and smoothness of loss functions.
Other than the stability approach, there is interesting generalization analysis of SGD based on either
a uniform convergence approach (Lin et al., 2016), an integral operator approach (Lin & Rosasco,
2017; Ying & Pontil, 2008; Dieuleveut & Bach, 2016; Dieuleveut et al., 2017; MUcke et al., 2019)
or an information-theoretic approach (Xu & Raginsky, 2017; Negrea et al., 2019; Bu et al., 2020).
3	Main Results
Letρbe a probability measure defined on a sample space Z = X ×Y with X ⊆ Rd andY ⊆ R, from
which a training dataset S = z1, . . . , zn is drawn independently and identically. The aim is to find
a good model w from a model parameter space W based on the training dataset S. The performance
of a prescribed model w on a single example z can be measured by a nonnegative loss function
f(w; z), where f : W × Z 7→ R+. In machine learning we often apply an (randomized) algorithm
A : ∪nZn 7→ W to S to produce an output model A(S) ∈ W. Oftentimes, the constructed model w
would have a small empirical risk FS(w) = n PZi f (w； Zi). However, We are mostly interested in
the generalization performance of a model w on testing examples measured by the population (true)
risk F(w) = Ez f(w; z) , where Ez denotes the expectation with respect to (w.r.t.) z. The gap
ES,A F (A(S)) - FS(A(S)) between the population risk and empirical risk is called the estimation
error, which is due to the approximation of ρ by sampling. Here EA denotes the expectation w.r.t.
the randomness of the algorithm A. For example, if A is SGD, then EA denotes the expectation
w.r.t. the random indices of training examples selected for the gradient computation. A powerful
tool to study the estimation error is the algorithmic stability (Bousquet & Elisseeff, 2002; Elisseeff
et al., 2005; Shalev-Shwartz et al., 2010; Hardt et al., 2016), which measures the sensitivity of the
algorithm’s output w.r.t. the perturbation of a training dataset. Below we give formal definitions of
stability measures, whose connection to generalization is established in Theorem A.1.
Definition 1 (Uniform Stability). A randomized algorithm A has uniform stability if for all datasets
S, Se ∈ Zn that differ by at most one example, we have supz EA f (A(S); z) - f (A(Se); z) ≤ .
The following on-average stability is similar to the average-RO stability in Shalev-Shwartz et al.
(2010). The difference is we do not use an absolute value. For m ∈ N, we denote [m] = {1, . . . , m}.
Definition 2 (On-average Stability). Let S = {zi,..., Zn} and S = {zi,..., zn} be drawn in-
dependently from ρ. For each i ∈ [n], denoteS(i) = {zi,...,Zi-ι,Zi,Zi+ι,...,Zn}. We say an
algorithm A has on-average stability E if n Pn=i E$ s a [f (A(S(i)); Zi) - f (A(S); Zi)] ≤ e.
3
Published as a conference paper at ICLR 2021
In this paper, We are interested in the excess generalization error F(A(S)) - F(w*), where W ∈
arg minw∈W F(w) is the best model with the least testing error (population risk). For this purpose,
we introduce some basic assumptions. A basic assumption in non-convex learning is the smoothness
of loss functions (Ghadimi & Lan, 2013; Karimi et al., 2016), meaning the gradients are Lipschitz
continuous. Let ∣∣∙ ∣∣2 denote the Euclidean norm and V denote the gradient operator.
Assumption 1 (Smoothness Assumption). We assume for all z ∈ Z, the differentiable function
w 7→ f(w; z) is L-smooth, i.e., ∣Vf (w; z) - Vf (w0; z)∣2 ≤ L∣w - w0∣2 for all w, w0 ∈ W.
Another assumption is the Polyak-Lojasiewicz (PL) condition on the objective function, which is
common in non-convex optimization (Zhou et al., 2018b; Reddi et al., 2016; Karimi et al., 2016;
Wang et al., 2019; Lei et al., 2017), and was shown to hold true for deep (linear) and shallow neural
networks (Hardt & Ma, 2016; Charles & Papailiopoulos, 2018; Li & Yuan, 2017).
Assumption 2 (Polyak-LojasieWicz Condition). Denote FS = infw,∈w FS(w0). We assume FS
satisfies PL or gradient-dominated condition (in expectation) with parameter β > 0, i.e.,
ES[Fs(w) - FS] ≤ ɪES[∣VFs(w)∣2],	∀w ∈ W.	(3.1)
2β
It is Worthy of mentioning that our results in this section continue to hold if the global PL condition
is relaxed to a local PL condition, i.e., (3.1) holds for w in a neighborhood of the minimizer of FS.
The existing stability analysis often imposes a bounded gradient assumption beloW (Bousquet &
Elisseeff, 2002; Hardt et al., 2016; Charles & Papailiopoulos, 2018; Yuan et al., 2019; Kuzborskij
& Lampert, 2018). Indeed, the resulting stability bounds depend on the uniform Lipschitz constant
G (see eq. (3.4)), Which can be prohibitively large in practical models, e.g., DNNs, or even infinite,
e.g. least squares regression in an unbounded domain.
Assumption 3 (Bounded Gradient Assumption). We assume ∣Vf (w; z)∣2 ≤ G for all w ∈ W,
z ∈ Z and a constant G > 0.
Our main result to be proved in Appendix B removes Assumption 3 and replaces the uniform Lips-
chitz constant G by the minimal empirical risk FS, Which is significantly smaller than the Lipschitz
constant. Note the assumption L ≤ nβ∕4 is mild, and the previous generalization bounds become
vacuous as O(1) (Yuan et al., 2019; Charles & Papailiopoulos, 2018) if this assumption is violated.
Theorem 1 (Main Theorem). LetAssumptions 1, 2 hold and WS = A(S). If L ≤ nβ/4, then
16LE[Fs]	LE [Fs(WS) - FS]
—nβ 一 +	2β
E[F (WS) - FS] ≤
(3.2)
An important implication is as follows. Since E[Fs] ≤ E[Fs(w*)] = F(w*) and FS ≤ FS(wS),
Eq. (3.2) implies an upper bound on the excess generalization error E[F(WS)] - F(w*) and
E[F(ws) - FS(ws)] = O(nβ + E[FS(WS)- FS] ).	(3.3)
The above two terms can be explained as follows. The term O(1∕(nβ)) reflects the intrinsic com-
plexity of the problem, while E[Fs (WS) 一 FS] is called the optimization error. An interesting
observation is that the overfitting phenomenon would never happen for learning under the PL condi-
tion (analogous to learning with strongly convex objectives where the global minimizer generalizes
well (Bousquet & Elisseeff, 2002)). Indeed, if the optimization algorithm finds more and more ac-
curate solutions, it achieves the limiting generalization bound O(1∕(nβ)). This shows an important
message that optimization can be beneficial to generalization. This seemingly counterintuitive phe-
nomenon is due to the implicit regularization enforced by the PL condition (analogous to the strong
convexity condition). Another notable property is that Theorem 1 applies to any algorithm. We can
plug any known optimization error bounds into it to immediately get generalization bounds.
Remark 1. We show that our result significantly improves the existing stability analysis. The
work (Charles & Papailiopoulos, 2018) showed the pointwise hypothesis stability is controlled
by 篝2 +
2√2GyE[Fs(ws) - Fs]∕β, which together with the connection between stability and
generalization (cf. (A.1)), implies with probability 1 - δ that
F(wS) ≤ FS(wS) +
(M2	24MG2	24MG,2E[Fs(WS) - FS] ∖ 1
(菽 + ~δβΓ +	√βδ	)
(3.4)
4
Published as a conference paper at ICLR 2021
The above bound requires the bounded gradient assumption ∣∣Vf (w; z)k2 ≤ G and the bounded
loss assumption 0 ≤ f(w; z) ≤ M for all w ∈ W and z ∈ Z, which are successfully removed
in our generalization analysis. Furthermore, our generalization bound significantly improves (3.4).
Indeed, assume E[Fs(WS) - FS] ≤ e2β for some e > 0, then (3.3) implies
E [F (ws)] = E[Fs (ws)] + O(+ + e2),	(3.5)
while (3.4) becomes F(WS) = FS(WS) + O (√⅛β + √e^. To achieve the generalization guarantee
O(1∕√nβ), the above bound requires the optimization accuracy E = O(1∕(nβ)), while our bound
(3.5) only requires the accuracy E = O(1∕√nβ) but gets the significantly better generalization
bound 1∕(nβ). We actually develop a better stability bound. Specifically, the pointwise hypothesis
stability is bounded by O(a + E) in Charles & Papailiopoulos (2018) while We show that the on-
average stability is bounded by O(赤 + e2) , which is significantly tighter if nβ ≤ E ≤ 1 (ignoring
constant factors). It should be mentioned that Charles & Papailiopoulos (2018) did not impose a
smoothness assumption. However, the smoothness assumption is widely used in non-convex opti-
mization to derive meaningful rates (Ghadimi & Lan, 2013). As compared to probabilistic bounds
in Charles & Papailiopoulos (2018), our bounds are stated in expectation. The extension to high-
probability bounds will lead to additional O(1/√n) term (Feldman & Vondrak, 2019).
Remark 2 (Bounded gradient assumption). Very recently, the bounded gradient assumption was
also removed for the stability analysis (Lei & Ying, 2020). However, their analysis considered SGD
applied to convex loss functions. As a comparison, we study stability and generalization in a non-
convex learning setting, and our analysis applies to any stochastic optimization algorithms.
Remark 3. If A is ERM, Theorem 1 immediately implies E[F(WS) 一 FS] ≤ 16LE[Fs]. If FS
is β-strongly convex and L < nβ∕2, it was shown for ERM that E[F(ws) ― FS] ≤ *E [Fs]
(Shalev-Shwartz & Ben-David, 2014, Corollary 13.7). Their result is extended here from a strongly
convex setting to a gradient-dominated setting, and from the particular ERM to any algorithm.
As a direct corollary, we can derive the following optimistic bound in the interpolation setting, which
is the most intriguing case for over-parameterized or highly expressive DNN models.
Corollary 2. Let Assumptions 1, 2 hold and wS = A(S). If E[FS] = 0 and L < nβ∕2, then
E[F(WS)] ≤ 2βE[Fs(WS)].
Remark 4. Corollary 2 shows a benefit of interpolation in boosting the generalization by achieving
a generalization bound O(E) for any E > 0 if we minimize FS sufficiently well. This benefit can
not be explained by the existing discussions (Hardt et al., 2016; Charles & Papailiopoulos, 2018) as
they imply the same generalization bound O(1∕√nβ) in the interpolation setting. Although it was
observed that interpolation helps in training (Bassily et al., 2018; Vaswani et al., 2019; Ma et al.,
2018; Oymak & Soltanolkotabi, 2020; Allen-Zhu et al., 2019; Zou et al., 2018), it is still largely
unclear, as indicated in Ma et al. (2018), that how interpolation helps in generalization. Corollary 2
shows new insights on how interpolation from highly expressive models helps generalization.
We now move on to the discussion on the critical assumption in Corollary 2, i.e. L < nβ∕2.
According to the proof, the two parameters L and β can be replaced by their local counterparts, i.e.,
the smoothness and PL condition related to a particular minimizer W0 of FS(i) (Eqs. (B.6), (B.7)).
For example, β can be replaced by 11 ∣∣VFs(w0)∣2/(Fs(w0) 一 FS), which can be larger than β.
Below are some examples on explaining L∕β < n∕2. As we will see, the quantity L∕β reflects
the complexity of the problem (related to condition number as shown in Examples 1, 2). Therefore,
the condition L∕β < n∕2 imposes implicitly a constraint on the complexity of the problems. This
explains why the optimization algorithm would never overfit when applied to gradient-dominated
objective functions if L∕β < n∕2, as shown in Theorem 1.
Example 1. Let φ : Rd → Rm be a feature map, and ' : R × R → R+ be a loss function which is l`-
smooth and σ'-strongly convex w.r.t. the first argument. Consider f (w; z) = '((w, φ(xi)i,yi) with
h∙, ∙i being an inner product. Then, FS satisfies the PL condition with the parameter σmin(∑s)σg,
where ∑s = ɪ Pn=1 φ(χi)φ(χi)> is the empirical covariance matrix, A> denotes the transpose of
a matrix A and σm0 in(A) means the minimal non-zero singular value ofA. The empirical counterpart
(we have an expectation w.r.t. S in PL condition) of L∕β is of the order of σmax(ΣS)∕σm0 in(ΣS),
where σmax(A) means the maximal singular value (we give details in Appendix E.1).
5
Published as a conference paper at ICLR 2021
Example 2. Consider neural networks with a single hidden layer with d inputs, m hidden neu-
rons and a single output neuron, for which the prediction function takes the form hv,w =
Pkm=1 vkφ hwk, xi . Here wk ∈ Rd and vk ∈ R denote the weight of the edges connecting the k-th
hidden node to the input and output node, respectively, while φ : R 7→ R is the activation function.
Analogous to Arora et al. (2019); Oymak & Soltanolkotabi (2020), we fix v = (v1 , . . . , vm)> with
|vk| = a for some a > 0 and train w = (w1, w2, . . . , wm)> ∈ Rm×d from S. The loss function
then takes the form f (w; Z) = (v> φ(wx) - y) 2. Ifwe consider the identity activation function, i.e.,
φ(t) = t, then FS satisfies the PL condition with the parameter σmin (ΣS), where σmin(A) denotes
the minimal singular value of A and ΣS = 1 Pi=ι Xix>. The empirical counterpart of L∕β is of
the order of σmax(∑s)∕σm⅛(∑s) (We give details in Appendix E.2 for a general activation function).
It is possible to get generalization bounds under some other conditions. Since one-point strong
convexity condition together with smoothness assumption implies the PL condition (Yuan et al.,
2019), all our results apply to one-point strongly convex functions. We can also get generalization
bounds for objective functions satisfying the quadratic growth condition (Necoara et al., 2018),
which is weaker than the PL condition. However, we need to impose a realizability condition which
was also imposed in Charles & Papailiopoulos (2018). The proof of Theorem 3 is given in Section C.
Let w(S) denote the Euclidean projection ofw onto the set of global minimizers of FS in W.
Definition 3 (Quadratic Growth Condition). We say FS : W 7→ R satisfies the quadratic growth
condition (in expectation) with parameter β ifEFS(w) -FS ] ≥ β2 E[kw - W(S)k2] for all W ∈ W.
Theorem 3. Let Assumption 1 hold and FS satisfy the quadratic growth condition with parameter β.
Iftheproblem is realizable, i.e., E[FS] = 0 and L ≤ nβ∕4, then E[F(ws)] ≤ 2Lβ-1E[FS(WS)].
Finally, we consider any optimization algorithms applied to gradient-dominated and Lipschitz con-
tinuous functions. We do not require loss functions to be smooth here. It shows that the excess
generalization bound can decay as fast as O(1∕(nβ)) if we solve the optimization problem to a suf-
ficient accuracy, which is much better than the generalization bound O(1∕√nβ) in Charles & Pa-
pailiopoulos (2018). Recall the analysis in Charles & Papailiopoulos (2018) requires Assumptions
2, 3 and a further assumption on boundedness of loss functions. The proof is given in Section C.
Theorem 4. Let Assumptions 2, 3 hold and wS = A(S). Then the following inequality holds
E[F(ws) - FS] ≤ 2G2 + G(EwS(W)- FSD2.
nβ	2β
4 Applications
In this section, we apply Theorem 1 to different stochastic optimization algorithms such as stochastic
gradient descent, randomized coordinate descent, and stochastic variance-reduced optimization. In
particular, we study the number of stochastic gradient evaluations required to achieve a prescribed
generalization bound, which is summarized in Table 1. We always assume L ≤ nβ∕4 in this section.
4.1	Stochastic Gradient Descent
We need some notations to state results on SGD. Specifically, denote by w1 ∈ W an initial point of
SGD. At the t-th iteration, we first randomly select an index it 〜unif[n], and then update {wt}t by
wt+1 = Wt - ηtVf (Wt; Zit),	(4.1)
where {ηt }t is a sequence of positive step sizes and unif[n] denotes the uniform distribution over
[n]. The proof of Theorem 5 is given in Appendix D.1.
Theorem 5.	Let Assumptions 1, 2 hold with L ≤ nβ∕4. Let A be SGD with the step size sequence
ηt = 2β(t+1)2. Then E[F(WT +ι)] — F(w*) = O(卷 + Te3). We can take O(金)stochastic
gradient evaluations to get excess generalization bounds O(1∕(nβ)).
Remark 5. We compare Theorem 5 with the recent generalization analysis of SGD under the PL
condition. Based on pointwise hypothesis stability analysis and the optimization error bound in
Karimi et al. (2016), it was shown with probability at least 1 - δ (Charles & Papailiopoulos, 2018)
F (WT+1) - F (W*)= o( √nβδ + Ti⅛).	(4.2)
6
Published as a conference paper at ICLR 2021
Algorithm	Complexity for 1∕(nβ)	Complexity for E if E [FS] = 0
SGD	n 	三		β2 log 淙
RCD	d log n		d *g β	
SVRG, SCSG			 (n + n 3 ∕β) log n	(n + n2 ∕β) log 古
SARAH, SPiderBoost	(n + 1∕β2) log n	(n + 1∕β2) IogV
SNVRG	(n + √n∕β) log4 n	(n + √n∕β) log4”看
Table 1: Iteration complexity for different optimization algorithms to achieve a stated generalization
bound under Assumptions 1, 2. In the second column, we present the number of stochastic gradient
evaluations to achieve excess generalization bounds O(l∕(nβ)). In the third column, We present
the number of stochastic gradient evaluations to achieve generalization bounds O() if E[FS] = 0.
We ignore constant factors. It is knoWn that variance-reduction techniques improve the iteration
complexity to achieve small training errors. Our stability analysis shoWs that such an improvement
is also achieved for testing errors. Note that the stability analysis in Charles & Papailiopoulos (2018)
can at most imply an excess generalization bound O(1/√nβ) for these algorithms.
The above bound indicates that O(n2∕β) stochastic gradient evaluations are needed to get the excess
generalization bounds O(1∕√nβ). Based on the uniform stability bound in Hardt et al. (2016) and
the optimization error bound in Karimi et al. (2016), it Was shoWn in Yuan et al. (2019) that
E[F (WT +ι)] — F (w*) = θ(n-1(βT) 1+L/^ ) + O( Te )∙	(4.3)
1 + L∕β - 2 + 3L∕β
By taking an optimal T = n1+2L/e β 1+2L∕β (ignoring a constant factor) to balance the above
two terms, we derive E[F(WT +1)] - F(w*)
1	1+L∕β	L∕β
O ( n 1+2L∕β β 1 + 2L∕β
If L∕β is moderately
large, then this bound quickly becomes E[F(WT +1)] - F(w*) = O(1∕√nβ). With high proba-
bility at least 1 - δ, it was shown that SGD with the step size η =(t+2)iOg(t+2)gets the bound
F(WT +ι) - FS(WT +ι) = O(√clog T/√nδ) (Zhou et al., 2018b). However, it is not clear how the
optimization errors decay with such step sizes. Typically, c should be of the order O(1∕β) as shown
in Karimi et al. (2016) and therefore the stability analysis in Zhou et al. (2018b) can at best achieve
the generalization bounds O (√log T/√nβ). To summarize, the existing stability analysis generally
implies the generalization bound O(1∕√nβ) for SGD in learning with gradient-dominated objec-
tives (Charles & Papailiopoulos, 2018; Zhou et al., 2018b; Yuan et al., 2019), which is significantly
improved to O(1∕(nβ)) in our paper by the refined stability analysis. It is worth mentioning that,
in this comparison, we have used the same optimization error bounds in Karimi et al. (2016), and
the analysis in Charles & Papailiopoulos (2018); Zhou et al. (2018b); Yuan et al. (2019) requires a
bounded gradient assumption and a bounded loss assumption, which are removed in our analysis.
The above iteration complexity in Theorem 5 can be further improved if we impose a restricted
secant inequality (Karimi et al., 2016) on FS, which has been considered for non-convex optimiza-
tion, e.g., optimizing neural networks (Li & Yuan, 2017). This is a slightly stronger assumption than
the PL condition as shown in Karimi et al. (2016).
Definition 4 (Restricted Secant Inequality). We say FS : W 7→ R satisfies the restricted secant
inequality with parameter β if E[(w - w(s), VFs(w)〉] ≥ βE[∣∣w - W(S)Il2] for all W ∈ W.
Theorem 6.	Assume FS satisfies the restricted secant inequality with parameter β. Let Assumption
1 hold with L ≤ nβ∕4. Let A be SGD with ηt = 1∕(β (t + 1)). Then one can take O(n∕β) stochastic
gradient evaluations to achieve the excess generalization bounds O(1∕(nβ)).
Below we apply Theorem 1 to establish fast generalization bounds in an interpolation setting. Our
analysis shows that interpolation actually boosts SGD by achieving an exponential convergence of
testing errors, which can not be derived from the bound (3.4) in Charles & Papailiopoulos (2018).
Theorem 7.	Let Assumptions 1, 2 hold with L ≤ nβ∕4, and E[FS] = 0. Let A be SGD with
ηt = β∕L2. Then E[F(WT +1)] ≤ L(I-g/L ) 叫FS(wι)]. We can take O(β-2 log(1∕(βe)))
stochastic gradient evaluations to achieve the generalization bound O() for any > 0.
The above linear convergence does not contradict existing minimax lower bounds where the benefit
of interpolation is not considered. The proofs for Theorems 6, 7 are given in Appendix D.1.
7
Published as a conference paper at ICLR 2021
Remark 6. We discuss some recent work on error bounds in low-noise conditions. Optimization
errors of SGD were studied for general non-convex objectives (Vaswani et al., 2019; Ma et al., 2018)
and gradient-dominated objectives (Bassily et al., 2018). For binary classification problems with the
specific squared loss, it was shown SGD achieves an exponential convergence of testing classifica-
tion errors under a margin condition, i.e., positive and negative classes are separated by a margin that
is strictly positive (Pillaud-Vivien et al., 2018). This was extended to general convex loss functions
under the same margin condition (Nitanda & Suzuki, 2019). These discussions consider regular-
ized objective functions (Pillaud-Vivien et al., 2018; Nitanda & Suzuki, 2019), which are strongly
convex. The exponential convergence in Pillaud-Vivien et al. (2018); Nitanda & Suzuki (2019) was
established for the testing classification errors, i.e., 0-1 loss. As a comparison, we establish an expo-
nential convergence for the testing errors measured by loss functions used in training. In addition,
the exponential convergence in Pillaud-Vivien et al. (2018); Nitanda & Suzuki (2019) comes into
effect only after a sufficiently large number of iterations, which is not required in Theorem 7.
4.2	Randomized Coordinate Descent
Randomized coordinate descent (RCD) is an efficient optimization algorithm particularly useful for
high-dimensional learning problems (Nesterov, 2012). At each iteration it firstly randomly selects
a single coordinate it ∈ {1, . . . , d}, and then performs the update along the it-th coordinate as
wt+ι = Wt - nN it FS (Wt )e〃, where NiFS denotes the derivative of FS w.r.t. the i-th coordinate
and ei is a vector in Rd with the i-th coordinate being 1 and other coordinates being 0.
Theorem 8. Let Assumptions 1 and 2 hold with L ≤ nβ∕4. Let A be RCD with n = '/L. Then
E[F(wτ +ι)] — F(w*) = O(nβ + 1(1 — dL)T). We take O((dlogn)∕β) Stochasticgradient eval-
uations to get excess generalization bounds O(1∕(nβ)). If E[FS] = 0, we take O(β-1dlog 1∕(β6))
stochastic gradient evaluations to get generalization bounds O() for any > 0.
The detailed proof for the above theorem is given in Appendix D.2. As indicated in Remark 1, the
discussion in Charles & Papailiopoulos (2018) can only imply the generalization bound O(1∕√nβ).
4.3	Stochastic Variance-Reduced Optimization
SGD needs a diminishing step size due to the inherent variance of stochastic gradients, which gen-
erally yields a sublinear convergence rate (Bottou et al., 2018). Recently, there is a large amount of
work to accelerate SGD by using some different gradient estimates with a reduced variance (Johnson
& Zhang, 2013; Xiao & Zhang, 2014; Zhang et al., 2013; Allen-Zhu & Hazan, 2016; Fang et al.,
2018; Wang et al., 2019; Nguyen et al., 2017; Zhou et al., 2018a; Schmidt et al., 2017; Defazio et al.,
2014; Reddi et al., 2016). This class of algorithms proceeds in epochs. Let Wo be an initialization
point. At the beginning of s-th epoch, we set a reference point w0 = Ws-ι, draw a batch Is ⊆ [n]
and compute vo = NfIS(W0), where we denote fɪ(w)= 十 Pi∈ιf(w;Zi) for I ⊆ [n] and |I|
is the cardinality of I. The batch Is can be equal to [n] (Johnson & Zhang, 2013; Xiao & Zhang,
2014; Wang et al., 2019; Reddi et al., 2016) or drawn with replacement according to the uniform
distribution over [n] (Lei et al., 2017; Fang et al., 2018). Then we proceed with ms inner iterations
by using some gradient estimators with reduced variances. At the t-th inner iteration, we first draw a
batch It ⊆ [n] from the uniform distribution over [n]. The original SVRG (Johnson & Zhang, 2013;
Reddi et al., 2016; Xiao & Zhang, 2014) uses the gradient estimator (we omit the dependency on s)
vt = NfIt (wt) - NfIt (w0)+	v0.	(4.4)
Recently a different update of gradient estimator is proposed (Nguyen et al., 2017; Fang et al., 2018)
vt = NfIt (wt) - NfIt (wt-1)+	vt-1.	(4.5)
An important observation is that the variance of vt diminishes to zero as we are approaching the
minimum, which allows us to update the iterate with a constant step size wt+1 = wt - ηvt (John-
son & Zhang, 2013). The framework of stochastic variance-reduced optimization is described in
Algorithm 1 in Appendix D.3. The following theorem gives generalization bounds O(1∕(nβ)) for
stochastic variance-reduced optimization, which significantly improves the bound O(1∕√nβ) based
on (3.4). The proof is given in Appendix D.3.
Theorem 9. Let Assumptions 1 and 2 hold with L ≤ nβ ∕4. Let A be either the SARAH in Nguyen
et al. (2017) or the SpiderBoost in Wang et al. (2019). We can take O n+ 1∕β2 log n stochastic
gradient evaluations to get excess generalization bounds O(1∕(nβ)). If E[FS] = 0, we take O((n +
1∕β2 log 1∕(β ) stochastic gradient evaluations to get generalization bounds O() for any > 0.
8
Published as a conference paper at ICLR 2021
Figure 1: κI versus |I |
Figure 2: Testing error versus number of passes
As compared to SGD (Section 4.1), Theorem 9 shows SARAH/SpiderBoost requires significantly
fewer iterations to achieve the same testing errors. This shows a clear advantage of stochastic
variance-reduced optimization over SGD in generalization other than training. Other than SARAH
and SpiderBoost, we also develop generalization bounds for SVRG in Reddi et al. (2016), SCSG in
Lei et al. (2017) (Theorem D.3) and SNVRG-PL in Zhou et al. (2018a) (Theorem D.4).
5 Simulations and Conclusions
Simulations. We report some preliminary experiments to support our theory. We consider the
dataset IJCNN available from the LIBSVM website (Chang & Lin, 2011) and report the average of
experimental results from 25 repetitions. In our first experiment, we aim to check how the condition
σmaχ(∑s)∕σmin(∑s) ≤ n/4 would be satisfied in practice. To this aim, We randomly pick a subset
I ⊂ {l, 2,...,n} and build an empirical covariance matrix ∑i = ∣1∣ Piiel XiX>, where |I| denotes
the cardinality of I. Then we compute the term KI := σσmx∑∑I∣I∣ ∙ Figure 1 plots the KI as a function
of |I|. It is clear that the condition κI ≤ 1/4 is violated if |I| is small. As |I| increases, κI decreases
and can be as small as 10-3. Then, the condition KI ≤ 1/4 holds trivially for sufficiently large n.
Theorem 1 implies that overfitting would never happen for learning with gradient-dominated func-
tions. Our second experiment aims to verify this phenomenon. We consider a generalized linear
model for binary classification with the loss function f (w; Z) = ('(wτx) - y) , where ' is the
logistic link function `(a) = (1 + exp(-a))-1. It was shown that the corresponding objective func-
tion is gradient-dominated (Foster et al., 2018). We use 80 percents of the dataset for training and
reserve the remaining 20 percents for testing. We apply SGD with the step size ηt = 1/(1 + 0.001t)
and compute the testing error of {wt} on the testing dataset. In Figure 2, we plot the testing errors
versus the number of passes (iteration number divided by sample size). It is clear that the testing er-
ror continue to decrease along the learning process, and there is no overfitting even after 100 passes
of the dataset. This is well consistent with Theorem 1.
Conclusions. We study stochastic optimization under the PL condition. We show that the general-
ization errors can be bounded by O (1 / (nβ)) plus the convergence rate of algorithms. An observation
is that the optimization always helps in generalization under the PL condition. Our analysis based
on a weak on-average stability measure removes the bounded gradient assumption in the literature,
and can imply significantly better bounds. In particular, we show how the interpolation accelerates
the generalization. Our study relies on an essential PL condition on the objective function. While
this assumption is widely used in the non-convex learning setting, it would be very interesting to
extend the discussions here to general non-convex objective functions.
Acknowledgments
The work of Yunwen Lei is supported by the National Natural Science Foundation of China (Grant
No. 61806091) and the Alexander von Humboldt Foundation. The work of Yiming Ying is sup-
ported by NSF grants IIS-1816227 and IIS-2008532.
9
Published as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699-707, 2016.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, pp. 6155-6166, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv preprint arXiv:1811.02564, 2018.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing Systems,
pp. 11279-11288, 2019.
RaefBassily, Vitaly Feldman, Cristobal Guzman, and KUnal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems,
2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Olivier Bousquet and Leon Bottou. The tradeoffs of large scale learning. In Advances in Neural
Information Processing Systems, pp. 161-168, 2008.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2(Mar):499-526, 2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610-626, 2020.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli. Tightening mutual information based
bounds on generalization error. IEEE Journal on Selected Areas in Information Theory, 2020.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology, 2(3):27, 2011.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 744-753,
2018.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, pp. 1646-1654, 2014.
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-
sizes. Annals of Statistics, 44(4):1363-1399, 2016.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger con-
vergence rates for least-squares regression. The Journal of Machine Learning Research, 18(1):
3520-3570, 2017.
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.
10
Published as a conference paper at ICLR 2021
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algo-
rithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279, 2019.
Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-
convex learning and optimization. In Advances in Neural Information Processing Systems, pp.
8759-8770, 2018.
Dylan J Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Srid-
haran. Hypothesis set stability and generalization. In Advances in Neural Information Processing
Systems, pp. 6726-6736. 2019.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning: Impli-
cations to exp-concave empirical risk minimization. The Journal of Machine Learning Research,
18(1):8245-8257, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In European Conference on Machine
Learning, pp. 795-811, 2016.
Tomer Koren and Kfir Levy. Fast rates for exp-concave empirical risk minimization. In Advances
in Neural Information Processing Systems, pp. 1477-1485, 2015.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, pp. 2820-2829, 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
scsg methods. In Advances in Neural Information Processing Systems 30, pp. 2348-2358. 2017.
YunWen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, pp. 5809-5819. PMLR,
2020.
YunWen Lei, Ting Hu, Guiying Li, and Ke Tang. Stochastic gradient descent for nonconvex learning
Without bounded gradient assumptions. IEEE Transactions on Neural Networks and Learning
Systems, 31(10):4394-4400, 2020a.
YunWen Lei, Antoine Ledent, and Marius Kloft. Sharper generalization bounds for pairWise learn-
ing. Advances in Neural Information Processing Systems, 33, 2020b.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020.
Yuanzhi Li and Yang Yuan. Convergence analysis of tWo-layer neural netWorks With relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. Jour-
nal of Machine Learning Research, 18(1):3375-3421, 2017.
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco. Generalization properties and implicit
regularization for multiple passes SGM. In International Conference on Machine Learning, pp.
2340-2348, 2016.
11
Published as a conference paper at ICLR 2021
Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Jing Rong, and Tianbao Yang. Fast rates of ERM and
stochastic approximation: Adaptive to error bound conditions. In Advances in Neural Information
Processing Systems, pp. 4683-4694, 2018.
Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, pp. 2159-2167, 2017.
Ben London. A PAC-bayesian analysis of randomized learning with application to stochastic gradi-
ent descent. In Advances in Neural Information Processing Systems, pp. 2931-2940, 2017.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec-
tiveness of sgd in modern over-parametrized learning. In International Conference on Machine
Learning, pp. 3325-3334, 2018.
Andreas Maurer. A second-order look at stability and generalization. In Conference on Learning
Theory, pp. 1461-1475, 2017.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-
convex learning: Two theoretical viewpoints. In Conference on Learning Theory, pp. 605-638,
2018.
Nicole Mucke, Gergely Neu, and Lorenzo Rosasco. Beating Sgd saturation with tail-averaging and
minibatching. In Advances in Neural Information Processing Systems, pp. 12568-12577, 2019.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, pp. 1-39, 2018.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances
in Neural Information Processing Systems, pp. 11015-11025, 2019.
Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341-362, 2012.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for ma-
chine learning problems using stochastic recursive gradient. In International Conference on Ma-
chine Learning, pp. 2613-2621, 2017.
Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates
of expected classification errors. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 1417-1426, 2019.
Francesco Orabona. Simultaneous model selection and optimization through parameter-free
stochastic learning. In Advances in Neural Information Processing Systems, pp. 1116-1124, 2014.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing
error for stochastic gradient methods. In Conference On Learning Theory, pp. 250-296, 2018.
Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.
Analysis and Applications, 3(04):397-417, 2005.
Sashank Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International Conference on Machine Learning, pp.
314-323, 2016.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
12
Published as a conference paper at ICLR 2021
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal ofMachine Learning Research, 11(Oct):2635-2670, 2010.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2019.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In
Advances in Neural Information Processing Systems, pp. 2199-2207, 2010.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In International Conference on Artificial
Intelligence and Statistics, pp. 1195-1204, 2019.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pp.
2403-2413. 2019.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems, pp. 2524-2533, 2017.
Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations of
Computational Mathematics, 8(5):561-596, 2008.
Yiming Ying and Ding-Xuan Zhou. Unregularized online learning algorithms with general loss
functions. Applied and Computational Harmonic Analysis, 42(2):224-244, 2017.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over sgd. In Advances in Neural Information Processing Systems, pp. 2604-2614,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number inde-
pendent access of full gradients. In Advances in Neural Information Processing Systems, pp.
980-988, 2013.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent for
nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 3921-3932.
2018a.
Yi Zhou, Yingbin Liang, and Huishuai Zhang. Generalization error bounds with probabilistic guar-
antee for SGD in nonconvex optimization. arXiv preprint arXiv:1802.06903, 2018b.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
A Stability and Generalization
We first give the definition of pointwise hypothesis stability. For any i ∈ [n], denote S\zi =
{z1, . . . ,zi-1,zi+1,. . . , zn}.
Definition 5 (Pointwise Hypothesis Stability). We say a randomized algorithm A has pointwise
hypothesis stability if for all i ∈ [n] there holds ES,A f (A(S); zi) - f(A(S\zi); zi) ≤ .
Theorem A.1 establishes the key connection between the generalization and various stability mea-
sures. Part (a) and part (b) show that the algorithm with either uniform stability or pointwise hypoth-
esis stability generalizes well to testing examples (Bousquet & Elisseeff, 2002). Initially, they were
developed for deterministic algorithms (Bousquet & Elisseeff, 2002), which were then extended to
the setting of randomized algorithms (Elisseeff et al., 2005). Part (c) shows the connection between
the generalization and the on-average stability (Shalev-Shwartz et al., 2010). Note part (b) involves
a square root of 1∕δ instead of a log(1∕δ).
13
Published as a conference paper at ICLR 2021
Theorem A.1 (Generalization by Stability). Let A be a randomized algorithm.
(a)	If A has uniform stability , then ES,A FS (A(S)) - F (A(S))	≤ .
(b)	Let M > 0. If A has pointwise hypothesis stability and 0 ≤ f(w; z) ≤ M for all w ∈ W and
z ∈ Z. Then for all δ ∈ (0, 1) with probability at least 1 - δ
F(A(S)) ≤ FS(A(S)) + (M2 [Mn)2.	(A.1)
(c)	If A has on-average stability , then ES,A F (A(S)) - FS (A(S)) ≤ .
Proof. The proof of Part (a) can be found in Hardt et al. (2016, Theorem 2.2). Part (b) was first
proved for deterministic algorithms (Bousquet & Elisseeff, 2002, Theorem 11), and then extended
to randomized algorithms (Elisseeff et al., 2005, Theorem 12). We prove Part (c) here due to its
simplicity. Since Zi and Zi are drawn from the same distribution, We know
1n
Es,a[F(A(S))- FS(A(S))] = — EESeA W(A(S(i))) - FS(A(S))]
ni=1
1n
=n E ES,e,A [f (A(S (i)); a) - f (A(S); Zi)],
i=1
where the last identity holds since Zi is independent of A(S(i)). The proof is complete by noting the
definition of on-average stability.	□
B Proof of Theorem 1
In this section, we prove Theorem 1. We begin our analysis with some useful properties of smooth
functions. Ifg is L-smooth, we have the following self-bounding property (Srebro et al., 2010)
INg(W)II2 ≤ 2L(g(w) - inf g(w0)), ∀w ∈ W	(B.1)
and the following elementary inequality for all w, wZ ∈ W (Nesterov, 2012)
LIw - wZ I2
g(w) ≤ g(W) + (Vg(W), W - Wi +-------2-----.	(B.2)
In particular, if g is further nonnegative, then
IVg(w)I22 ≤ 2Lg(w), ∀w ∈ W.	(B.3)
The following lemma follows directly from the self-bounding property of smooth loss functions.
Lemma B.1. Assume F is L-smooth. Then (w can depend on S)
E[kVF(w)k2] ≤ 2LE[F(w) - FS].
Proof. Recall w* = argminw∈w F(w). According to the self-bounding property (B.1) and the
definition of w* we know
E[IVF(w)I22] ≤ 2LE[F (w) - F(w*)] = 2LE[F (w) - FS(w*)]
≤ 2LE[F(w) - FS],
where we have used E[FS(w*)] = F(w*) since w* is independent of S, and FS ≤ FS(w*) due to
the definition of FS. The proof is complete.	□
In the following lemma, we derive the on-average stability bounds under the PL condition. Recall
for any w, we denote by w(S) the Euclidean projection of w onto the set of global minimizers of
FS in W .
Lemma B.2. If Assumptions 1, 2 hold, then A has on-average stability satisfying
E ≤ nβ (E[FS ] + E[F(W(S))]) + E[F(WS ) - F(W(S))] + E[FS - FS (wS )].
14
Published as a conference paper at ICLR 2021
Proof. Let S = {Zι,..., Zn} be drawn independently from ρ. For each i ∈ [n], let S(i) be defined
in Definition 2. For each i ∈ [n], we denote WS(i) = A(S (i)) and W(SS(i()i)) the projection of WS(i)
onto the set of global minimizer of FS(i) . We decompose f(WS(i) ； Zi) - f(WS； Zi) as follows
f IWSS ； Zi) - f (ws； Zi) = (f (WS(i) ； Zi) - f(WS：「； Zi))
+ (f(W(S(i))； Zi)- f (WSS)； Zi)) + (f(W(S)； Zi)- f (ws； Zi)). (B.4)
We now address the above three terms separately. We first address f(WS(S(i()i))； Zi) - f(WS(S)； Zi).
According to the definition of FS, S, S(i), we know
(S(i))	F	(S(i))	F	(S(i))	(S(i))
f (WS(i) ； Zi) = nFS (WS (i) ) -nFS(i) (WS(i) ) + f (WS(i) ； Zi).
Since Zi and Zi follow from the same distribution, we know E[f (WSS()))； Zi)] = E[f (WSS)； Zi)] and
further get
E[f(W(S(i()i))； Zi)] =nE[FS(W(S(i()i)))] - nE[FS(i) (W(S(i()i)))] + E[f (W(SS)；
S(i)	S(i)	S(i)	S
It then follows that
(S(i))	(S)	(S(i))	(S(i))
E	f(WS(i)	；Zi)	-	f(WS	；Zi)	=nE	FS(WS(i)	) - FS(i)(WS(i)	)
=nEhFS(W(SS(i()i)))-wi∈nWf FS(W)i,
zi
(B.5)
where We have used the following identity due to the symmetry between Zi and Zi
E[Fs(i) (wιS)))] = E[Fs] = E[ inf, FS(w)].
w∈W
By the PL condition ofFS, it then follows from (B.5) that (in our assumption ofPL condition, w may
depend on S. This was also imposed in the literature (Yuan et al., 2019; Charles & Papailiopoulos,
2018; Zhou et al., 2018b). Indeed, the PL condition was often shown for empirical functions FS)
E[f(W(S：)；Zi)-f(W(S); Zi)] ≤ 2nE[kVFs(W(SC))k2].
2β
(B.6)
According to the definition of WS(S(i()i)) we know VFS(i) (W(SS(i()i))) = 0 and therefore ((a + b)2 ≤
a2 + b2)
2
(i)	(i)	1	(i)	1	(i)
kVFS(WSSi)))k2 = ^VFS(i)(WSSi) )) - -Vf(WSSi) )；Zi) + -Vf(W(Si))；Zi)Il
nn
≤ n 皿芭；)； Zi)k2+n kVf (WSS：；))； Zi)k2
≤ n⅞f (WlS"； 5i) + n⅜f (WlS"； Zi)，
2
(B.7)
where we have used the self-bounding property of smooth loss functions (B.3). Since Zi and Zi
follow from the same distribution, we know
E[f (W(S：)； Zi)]= E[f (WSS)； Zi)],	E[f(W篇))； Zi)]= E[f (W(S)； Zi)].
It then follows that
E[kVFs(w(Si(i)))k2] ≤ 4LE[f(W(S)；Zi)] + 4LE[f(w(S);Zi)],
which, combined with (B.6), gives
E[f (w(S(J； Zi) — f (W(S)； Zi)] ≤ ∣ββ(E[f (W(S)； Zi)] + E[f (w(S); Zi)]).
Taking a summation of the above inequality for i = 1, . . . ,n, we get
n
X E[f (W(Si) )； Zi) - f (W(S)； Zi)] ≤ -L (e[Fs]+E[Fs(w&]).
i=1	β
(B.8)
(S(i) )	(S(i) )
We then address f (WS(i) ； Zi) - f (WS(i) ； Zi). Since WS(i) and WS(i) are independent of Zi, we
know
E[f(WS(i)；Zi) - f(WS(S(i()i))；Zi)] = E[F (WS (i)) - F(W(SS(i()i)))] = E[F(WS) - F (W(SS))], (B.9)
where we have used the symmetry between Zi and Zi.
15
Published as a conference paper at ICLR 2021
Finally, we address f(wS(S); zi) - f(wS; zi). By the definition of w(SS) we know
n
X (f (W(S); Zi)- f(ws； Zi)) = n(Fs - FS(WS)).
i=1
Plugging (B.8), (B.9) and the above inequality back into (B.4), we derive
(B.10)
n	2L
X Ef (WS(i); Zi)- f (WS; Zi)] ≤ ɪ (E[FS] + E[Fe(WS )]) +
i=1	β
nE[F(ws) - F(W(S))] + nE[FS - FS(WS)].
The proof is complete by recalling the definition of on-average stability and E[FSe(W(SS))] =
E[F (W(S))].	□
We further require a lemma relating the convergence in terms of function values to the convergence
in terms of models. This shows that the PL condition is stronger than a quadratic growth condi-
tion (Karimi et al., 2016).
Lemma B.3 (Karimi et al. 2016). If FS satisfies the PL condition with parameter β> 0. Then for
all W ∈ W we have
E[FS(W) - FS (W(S))] ≥ 2βE[kW - W(S)k22].	(B.11)
We are now in a position to prove Theorem 1.
Proof of Theorem 1. Plugging the on-average stability established in Lemma B.2 back into Part (c)
of Theorem A.1, we derive
E [F (ws ) - FS (ws )] ≤ nβ (E[Fs ]+ E[F (WSS))]) +
E[F(ws) - F(wSS))] + E[Fs - FS(WS)], (B.12)
from which we derive
E[F(W(S))- FS] ≤ nβ (e[Fs] + E[F(W(S))]).	(B.13)
By (B.2), we know the following inequality for all γ > 0
F (ws ) - F(WS)) ≤ hVF(WS)), WS - W((SS) + L IlWS - WS)II2
≤ INf(WSS))Il2I∣ws - W(S)k2 + 2I∣ws - W(S) I∣2
≤ 4γINF(W(S))k2 + (Y + 2) I∣ws - W(S)Il2,
where we have used the Cauchy-Schwartz inequality. This together with Lemma B.1 with W = WS(S)
implies that
E[F(ws) - F(WSS))] ≤ 福E[F(WSS))- FS] + (γ + LL)E[∣ws - WSS) 闿.
Plugging (B.13) into the above inequality, we get
E [F (ws ) - F (WSS))] ≤ 2LY nβ (E[Fs ]+ E[F (WSS))]) + (γ + L )E[∣ws - WSS)II2].
Taking γ = L/2, we then get
E[F(ws) - F(WSS))] ≤ nβ (E[Fs] + E[F(WSS))]) + LE[∣ws - WSS)Il2].
Plugging the above inequality back into (B.12), we derive the following inequality
E[F(ws) - Fs(ws)] ≤ 件(e[Fs] + E[F(W(S'))]) + LE[∣ws - W(S)∣2] + E[Fs - FS(WS)].
It then follows that
E[F(WS)-FS] ≤ nβ(E[FS]+E[F(WSS))]) + LE[∣∣ws - W(S)Il2].	(B.14)
16
Published as a conference paper at ICLR 2021
Since L ≤ nβ∕4,it follows from (B.13) that
E [F (W(S))- FS ] ≤ 1(E[Fs ]+ E[F (W(S))])
and therefore
E[F(W(S))] ≤ 3E[Fs].
We can plug the above inequality back into (B.14) and derive
E[F(WS) — FS] ≤ 16LeFS] + LE[kws — wSS)k2].	(B.15)
The stated bound then follows from (B.11). The proof is complete.	口
Our analysis in the proof of Theorem 1 actually gives
E[F(ws) — Fs] ≤ nL≡ + LEFSLSl.
Since we assume E[Fs] = 0 in Corollary 2, we only need the condition L < nβ∕2 to get Corollary
2.
C Proof of Theorem 3 and Theorem 4
In this section, we present the proof of Theorem 3 and Theorem 4.
(S(i))
ProofofTheorem 3. Let W be the projection of w；(i) onto the set of global minimizer of FS. Then
by the quadratic growth condition, we know
E[Fs(WSSi())))— FS] ≥ 2叫||喏())) — WlI2].
This together with (B.5) and non-negativity of f implies
nβ e[∣∣w(Si(J-W∣∣2] ≤ E[f (W(Si(J； zi)] = E[F (WSc) ))] = E[F (W(S))],	(CI)
where we have used the symmetry between S and S(i). By the realizability condition, we know
almost surely that
f (W(S)； Zi) = f (W； Zi) = 0
and Vf (W; Zi) = 0. It then follows from the smoothness assumption that
E[f (WSS(i)); Zi) — f (W(S); Zi)] = E[f(W(SC); Zi) — f(W; Zi)]
≤ E[hW(S：) - W, Vf (W； Zi)) + L kW篇)) - Wk2]
―E[LHW(S(i))	<LE[F(W(S))I
=E 2 kWs(i)— 叫周 ≤ nβ .
We can plug (B.9), (B.10) and the above inequality back into (B.4), and derive the following bound
on the on-average stability
e ≤ LE[:；')] + E[F(ws) — F(W(S))] + E[Fs — FS(WS)].
We then can analyze analogously to the proof of Theorem 1 but using the above stability bound and
get the stated generalization bound. The proof is complete.	口
Proof of Theorem 4. Similar to (B.7) but using the boundedness of gradients, we know
kVFs(W篇)))k2 ≤ 4nG2.
We can plug this inequality into (B.6) and derive
E[f (W(S：)； Zi)- f(w(S)； Zi)] ≤ 2β 4G-=2Gr
nn
17
Published as a conference paper at ICLR 2021
Taking a summation of the above inequality gives
n
XEf(w篇)); Zi)- f(w(SS; Zi)] ≤ 2G2∕β.	(C.2)
i=1
Plugging (C.2), (B.9) and (B.10) back into (B.4), we derive the following inequality
n	2G2
£e[/(ws(i); Zi) - f(ws; Zi)] ≤ --+ + nE[F(WS) - F(WS))] + nE[Fs - FS(WS)]
i=1	β
≤	+ nGE[kwS - W(S)k2] + nE[_Fs - FS(WS)],
β
where in the last step we have used the inequality F(WS) - F(WS(S)) ≤ GkWS - W(SS) k2 due to
the boundedness of gradients. According to the definition of on-average stability, we know that the
on-average stability of A satisfies
2G2
C ≤ ~ββ + GE[∣∣ws - W黑)|图 + E[Fs - FS(WS)]
≤ -G2+ G(E[FS (W⅛ - FS ])+ e[Fs - Fs (WS)]，
where we have used Lemma B.3. According to Part (c) of Theorem A.1, it follows that
E[F(ws) - Fs(ws)] ≤ 2G2 + G(E[FSS^- FS+ E[Fs - FS(WS)].
The stated bound then follows directly. The proof is complete.	口
D	Proofs on Applications
In this section, we prove generalization bounds for various stochastic optimization algorithms.
D.1 Stochastic Gradient Descent
We consider here SGD. In the following proposition, we establish the variance of stochastic gra-
dients for SGD under the PL condition. The variance was also studied in a general nonconvex
setting (Lei et al., 2020a).
Proposition D.1. Let Assumptions 1, 2 hold. Let {Wt}t be the sequence produced by SGD with step
size sequence {ηt}t∈N. If there exists t0 ∈ N such that ηt ≤ β∕L2 for all t ≥ t0, then
E[∣Nf(wt; Zit)Il2] ≤ -Lmax{E[Fs(wt0)], 2E[Fs]} ∀t ≥ to∙
Proof. By (B.2) and the update (4.1), we know
FS(wt+ι) ≤ FS(Wt) + hwt+ι - Wt, VFsSWt) + "际+；Wtk
=Fs(Wt) - ηthVf (wt; %), VFs(w∕ + Lη2kvf(Wt；Zit)k2
≤ Fs(Wt)- ηthVf (wt； Zit), VFs(wt)i + L2ηtf (wt； zQ,
where we have used (B.3). Taking expectations on both sides we get the following inequality for all
t ≥ t0
E[Fs(wt+ι)] ≤ E[Fs(wt)] - ηtE[∣VFs(wt)∣2] + L2η2E[f(wt； zQ]
≤ E[Fs(wt)] - 2ηtβE[Fs(wt) - FS] + ηtβE[Fs(wt)],	(D.1)
where we have used the PL condition and ηt ≤ β∕L2 in the last step. It then follows the following
inequality for all t ≥ t0
E[Fs(wt+ι)] ≤ (1 - ηtβ)E[FS(wt)] + ηtβ ∙ 2E[Fs] ≤ max {E[Fs(wt)], 2E[Fs]}.
Applying this inequality recursively, we derive
_ r _	,	、r	- _ r _	,	、r___-U r、	..
E[Fs(wt+ι)] ≤ max{E[Fs(wt0)], 2E[Fs]} ∀t ≥ to.
This together with (B.3) implies the following inequality for all t ≥ t0
E[∣Vf(wt; Zit)k2] ≤ 2LE[f(wt; Zit)] ≤ -Lmax{E[Fs(wt0)], 2E[Fs]}.
The proof is complete.	口
18
Published as a conference paper at ICLR 2021
We now prove generalization bounds in Theorem 5. We denote B B if there exist some constants
1	l-∖	1.1	7~1	, 苴/	I >
c1 and c2 > 0 such that c1 B ≤ B ≤ c2B.
Proofof Theorem 5. Let to = [L2∕β2C. It is clear that η ≤ β∕L2 for all t ≥ to. Let σ =
2L max{E[FS (w1)], . . . , E[FS (wt0)], 2E[FS]}. According to the self-bounding property (B.3) and
Proposition D.1, We know that E[Rf (wt； Zit)Il2] ≤ σ2 for all t ∈ N. The following optimization
error bound was established in Karimi et al. (2016)
Lσ2
E[FS(Wt+1) - Fs] ≤ 2tβ2 .
We can plug the above inequality into (3.2) with A(S) = wT+1, and get
Since
we further get
「/	、	「	16LE[Fs ] L2σ2
E[F(WT +1) - Fs] ≤ nβ + + 4Tβ3.
E[Fs] ≤ E[Fs(w*)] = F(w*),
E[F(wτ +ι)] - F(w*) ≤ 16LFeW*)+ 4Tβ3.
nβ	β
(D.2)
(D.3)
By taking T N n∕β2, we get E[F(WT +ι) 一 FS] = O(1∕(nβ)). This corresponds to O(n∕β2)
stochastic gradient evaluations. The proof is complete.	□
Lemma D.2. Assume FS satisfies the restricted secant inequality with parameter β. Let A be SGD
with the step size sequence ηt = 1∕(β(t + 1)). Then there exists some σ ∈ R such that
E[IwT-wT(S)I22] ≤σ2∕(β2T).
Proof of Lemma D.2. Analogous to the proof of Theorem 5, we can find σ ∈ R+ such that
E[∣Vf (wt； Zit)Il2] ≤ σ2 for all t ∈ N. Since W((SI is a projection of wt+ι onto the set of global
minimizer of FS, we know
kwt+1 一 w(+1k2 ≤ kwt+1 - W(S) k2 = kwt 一 ηttvf (wt； Zit) — W(S)k2
=kwt - W(S) 112+ η2kvf(wt；Zit)k2 + 2ηthw(S)- wt, Vf(Wt； Zityi.
Taking an expectation and using E[IVf (wt； Zit)I22] ≤ σ2, we derive
E[Iwt+1 一 wt(+S)1I22] ≤ E[Iwt 一 wt(S)I22] + ηt2σ2 +2ηtE[hwt(S) 一 wt,VFS(wt)i]
≤ E[kwt- W(S)k2] + η2σ2 - 2ηtβE[kWt- W(S)k2]
= (I- 2ηtβ)E[kwt - W(S)∣∣2] + η2σ2,
where we have used the restricted secant inequality. For the step size ηt = 1∕(β(t + 1)), we have
E[∣wt+ι — W(S)L∣∣2] ≤ ---E[∣wt — W(S)k2] +—7^σ——.
t+1 t+1 2 t + 1 t t 2	β2(t + 1)2 .
Multiplying both sides by t(t + 1), we derive
2
t(t + 1)E[kwt+1 - wt+ιk2] ≤ (t - 1)tE[kwt - WtSk2] + 京.
β
Taking a summation of the above inequality from t = 1 to T - 1 gives
(T - 1)TE[∣WT - W(S)k2] ≤ σ2(T - 1)∕β2.
The proof is complete.	□
Proof of Theorem 6. It was shown that functions satisfying restricted secant inequality with parame-
terβ also satisfies the PL condition with parameter β ∕L (Karimi et al., 2016). Therefore (B.15) holds
with β there replaced by β∕L. According to Lemma D.2, we know E[IwT -w(TS) I22] ≤ σ2∕(β2T).
We can plug this inequality back into (B.15) with A(S) = wT+1, and get
E[F (WT+ι)] -F (w*)=O( Fn?+β
where we have used (D.3). By taking T N n∕β, we get E [F(WT +ι)] - F(w*) = O(1∕(nβ)). This
corresponds to O(n∕β) stochastic gradient evaluations. The proof is complete.	□
19
Published as a conference paper at ICLR 2021
ProofofTheorem 7. Let η = β∕L2. According to the assumption E[F⅛] = 0 and (D.1), We know
E[Fs(wt+ι)] ≤ E[Fs(wt)] — 2ηβE[F⅛(wt)]+ ηβE[Fs(wt)] =(1 — ηβ)E[F⅛(wt)]. (D.4)
Applying this inequality recursively, we get E[FS (wT +1)] ≤ (1 - ηβ)TE[FS(w1)]. We can plug
the above inequality back into (3.2) with A(S) = wT+1 and get
E[F(wτ +1)] ≤ ≡sw^ ≤ ^^TE[Fs(wι)] ≤ LexP(JE[Fs2,
where we have used the elementary inequality
1 — a ≤ exP(—a).	(D.5)
To achieve E[F(wT+1)] ≤ , we can take T such that
exp (— β2T∕L2) X eβ 0 T X β-2 log(1∕(βe)).
The proof is complete.	□
D.2 Randomized Coordinate Descent
We prove here the generalization bounds for randomized coordinate descent. We further assume that
the gradient is coordinate-wise Lipschitz continuous in the sense that
FS(w + aei) ≤ FS(w) + αViF⅛(w) + Lα2∕2, ∀α ∈ R, W ∈ Rd, i ∈ [d].
Proof of Theorem 8. According to Theorem 3 in Karimi et al. (2016), we know
叫FS(WT +ι) — FS] ≤(1 — dL)叫FS(wι) — FS].
Plugging the above inequality back into (3.2) and using (D.3), we get
E[F(WT +i)] 一 F(W) ≤----i-S^ + 与(1 一 βj-) E[FS(WI)]
nβ	2β	dL
(D.6)
=o( FW) + o( 1exp
nβ	β
where we have used (D.5). To achieve the excess generalization bounds O(1∕(nβ)), we require T
satisfying
exP
X n-1	0 T X X.
β
If E[Fs] = 0, then it follows from (3.2), (D.6) and (D.5) that
E[F(WT +1)] ≤ 2zeχp ( 一 dLβ)E[FS(WI)].
2β	dL
To achieve the generalization bound , we require T satisfying
exP
The proof is complete.
X β6 ^⇒ T X β-1 dlog 1∕(β6).
□
1
2
3
4
5
6
7
8
9
10
11
D.3 Stochastic Variance-Reduced Optimization
We prove here generalization bounds for various stochastic variance-reduced optimization algo-
rithms. We formulate the framework in Algorithm 1.
Algorithm 1: Stochastic Variance Reduced Optimization
Input: step size η, initialization W0, {m§}
for s = 1, 2, . . . do
set wo = Ws-ι
draw a batch Is ⊆ [n]
compute v° = Vffs(Wo)
update w1 = w0 — ηv0
for t = 1, . . . , ms — 1 do
draw a batch It ⊆ [n]
compute vt by either (4.4) or (4.5)
update wt+ι = Wt — ηvt
set WS as Wis, where i§ is drawn according to a distribution on [m§]
choose the output from {Ws} according to some strategy
20
Published as a conference paper at ICLR 2021
We now consider the stochastic variance-reduced gradient descent (SVRG) (Reddi et al., 2016) and
stochastically controlled stochastic gradient (SCSG) (Lei et al., 2017).
Theorem D.3. Let Assumptions 1 and 2 hold with L ≤ nβ∕4. Let A be either the SVRG in Reddi
et al. (2016) or the SCSG in Lei et al. (2017). Then we can take O((n + n2∕β) logn) stochastic
gradient evaluations to get excess generalization bounds O(1∕(nβ)). Furthermore, if E[FS] = 0,
then we can take O((n + n2∕β) log 1∕(βe)) stochastic gradient evaluations to achieve the gener-
alization bound O() for any > 0.
Proof. To achieve E[Fs(A(S)) - FS] ≤ 2∕n,it was shown that SVRG and SCSGrequires O((n +
n2 ∕β) log n) stochastic gradient evaluations (Reddi et al., 2016; Lei et al., 2017). We plug this
optimization error bound into Theorem 1 and get E[F(A(S))] - F(w*) = O(1∕(nβ)).
We now consider the case E[Fs] = 0. According to (3.2), to achieve generalization bound O(e), it
suffices that E[F(A(S))-Fs ] = O(β6). This can be achieved by taking O((n+n 3 ∕β) log 1∕(βe))
stochastic gradient evaluations (Reddi et al., 2016; Lei et al., 2017). The proof is complete. □
We now present the proof of Theorem 9 on the behavior of the stochastic recursive gradient algo-
rithm (SARAH) (Nguyen et al., 2017) and SpiderBoost (Wang et al., 2019).
Proofof Theorem 9. To achieve E[FS(A(S)) - FS] ≤ 2∕n, it was shown that SARAH and SPi-
derBoost requires O n + 1∕β2 logn stochastic gradient evaluations (Nguyen et al., 2017; Wang
etal., 2019). We plug this optimization error bound into Theorem 1 and get E [F (A(S))] - F (w*)=
O(1∕(nβ)).
We now consider the case E[FS] = 0. According to (3.2), to achieve generalization bound O(), it
suffices that E[F(A(S))-FS ] = O(β6). This can be achieved by taking O((n +1∕β2) log 1∕(βe))
stochastic gradient evaluations (Nguyen et al., 2017; Wang et al., 2019). The proof is complete. □
Finally, we consider SNVRG-PL (Zhou et al., 2018a).
Theorem D.4. Let Assumptions 1 and 2 hold with L ≤ nβ ∕4. Let A be the SNVRG-PL in Zhou
et al. (2018a). Then we can take O((n + √n∕β) log4 n) StOChaStiC gradient evaluations to get
excess generalization bounds O(1∕(nβ)). Furthermore, if E[FS] = 0, then we can take O((n +
√n∕β) log4 表)StOChaStiC gradient evaluations to achieve the generalization bound O(E) for any
> 0.
Proof. To achieve E[FS(A(S)) - FS] ≤ 2∕n, it was shown that SNVRG-PL requires O((n +
√n∕β) log4 n) stochastic gradient evaluations (Zhou et al., 2018a). We plug this optimization error
bound into Theorem 1 and get E[F(A(S))] - F(w*) = O(1∕(nβ)).
We now consider the case E[Fs] = 0. According to (3.2), to achieve generalization bound
O(e), it suffices that E[F(A(S)) - FS] = O(β6). This can be achieved by taking O((n +
√n∕β) log4 1∕(β6)) stochastic gradient evaluations (Zhou et al., 2018a). The proof is com-
plete.	□
E	Discussions of Examples
In this section, we present some discussions on understanding the assumption L∕β < n∕2 in Theo-
rem 2.
E.1 Discussion of Example 1
We first give the definition of strong convexity. For any differentiable function g : W 7→ R, we say
g is σ-strongly convex if for any w, w0 ∈ W there holds
g(w0) ≥ g(w) + hw0 - w, Vg(w)i + 2 ∣∣w - w0∣∣2∙
21
Published as a conference paper at ICLR 2021
Introduce g : Rn → R+ by g(v) = 1 Pn=I '(vi, yi). Then the function FS can be written as
1n
FS(W) = - £'((w, φ(xi)i,yi) = g(Aw),
n i=1
where A = (φ(xι),..., φ(χn))> ∈ Rn×m is the matrix formed from the data. It is known that if g
is σg-strongly convex, then FS satisfies the PL condition (Karimi et al., 2016; Necoara et al., 2018)
FS(w) — Fs ≤
1
2σg (σmin (A))
2 kVFs(w)k2.
(E.1)
Since ' is σ'-strongly convex We know for any v, v0 ∈ Rn
nnn	n
g(VO) = n X '⑻,yi) ≥ n X'(Vi，yi) + n X '03, yi)(v0 -Vi) + m Xm-
vi0)2
i=1	i=1	i=1
i=1
=g(v) + "g(v), v0 - Vi + σ'kv0 - vk2.	(E.2)
2n
That is, g is σn'-strongly convex. This together with (E.1) shows that
FS(W) - FS ≤ —/ n ,八∖2 ∣∣vFS(w)k2 = 9~~0]、l∣VFS(w)k2,	(E.3)
2σe(σmin(A))2	2FinNs)
where we have used
-(σ0niπ(A))2 = Lm in(A> A) = b°nin(£s )∙	(E.4)
n mn	n mn	mn
For any v, v0 ∈ Rn, it follows from the L'-strong smoothness of ' that
1 n	L2 n	L2
kVg(v) - Vg(v0)k2 = - X ∣'0(Vi, yl) - '0(vi, yj)∣2 ≤ L X ® - v『=n∣∣v - v0∣2.
n i=1	n i=1	n
That is, g is L-smooth. It then follows
∣VFS(W) -VFS(W0)∣2 = A>Vg(AW) -A>Vg(AW0)2 ≤σmax(A)∣Vg(AW) - Vg(AW0)∣2
≤ L'σmaX(A) ∣A(w - w0)∣∣2 ≤ L'σmax(A) ∣w - w0∣2.
nn
This together with (E.4) shows (σm0 in replaced by σmax)
∣∣VFs(w) -VFs(w0)k2 ≤ L'σmax(∑s)∣w - w0∣2.	(E.5)
It is reasonable to assume that L is of the order of the smoothness of FS . In this case, it follows
from (E.3) and (E.5) that empirical counterpart of L∕β is of the order of 0皿a(∑s)∕σmn 1∏(∑s).
E.2 Discussion of Example 2
We recall some notations in Example 2. Consider single-hidden-layer neural networks with d inputs,
m hidden neurons and a single output, for which the prediction function takes the form hv,w =
Pkm=1VkφhWk, xi	. Here Wk ∈ Rd and Vk ∈ R denote the weight of the edges connecting the
k-th hidden node to the input and output node, respectively, while φ : R 7→ R is the activation
function. We fix v with |Vk| = a for some a > 0 and train W = (W1, W2, . . . , Wm)> ∈ Rm×d from
S. Note we only use the PL condition FS (W) - FS ≤ 2β ∣VFs (w)k2 for w = WSS(i)) in the proof
(S(i) )
of Theorem 1 (only in (B.6)). We fix W = WS(i) here. Analogous to Soltanolkotabi et al. (2019),
we define the Jacobian matrix J = (Ji, J2,..., Jn) ∈ Rmd×n at W = WSS(J with
V1 φ0 (W1> xj )xj
Jj =	.	I
Vmφ (Wm xj )xj
and rj = v>φ(Wxj) - yj forj ∈ [n]. It was shown that (Soltanolkotabi et al., 2019)
VFS (w) = n J r, for r =(rι,...,rn)>,	(E.6)
and therefore
k VFS(W)k2 = n2 r> j> jr = *r>(J>Jjo)故电产
22
Published as a conference paper at ICLR 2021
According to the definition Jj , we know
	v1φ0(W1>xj0 )xj0 Jj Jj0 =卜 1φ0(w> Xj )x>,...,vm φ0(wmXj )x>)	.	I vm φ0(Wm>xj0)xj0 m = a2	φ0(Wk>xj)φ0(Wk>xj0)xj>xj0 . k=1
It then follows that
	J>J = a2 XX (φ0(XWk)(φ0(XWk))>) Θ (XX>),	(E.7) k=1
where X = (x1, . . . , xn)> ∈ Rn×d is the data matrix and denotes the Hadamard (entry-wise)
product of matrices. According to the definition of r, We know FS(w) = * ∣∣rk2. Then, it follows
from (E.6) and (E.7) that
	∣VFs(W)k2 = n22r> (XX W(XWk)(φ0(XWk))>) Θ (XX>))r n	k=1 2m ≥ nσmin(X (φ0(XWk)(φ0(XWk))>) Θ (XX>))kr∣∣2 n	k=1 2m =—σmin(X Q(XWk)(φ0(XWk)) ) Θ (XX>))FS(w). n	k=1
That is, we can take the parameter of the PL condition as
	2m β =2ησmin(X (φ0(XWk)(φ0(XWk))>) Θ (XX>)). n	k=1
It is reasonable to assume that L is of the order of a2σmax(Pm=I (φ0(XWk)(φ0(XWk))>) Θ
(XX>)) (Soltanolkotabi et al., 2019). In this case, we have the empirical counterpart of L∕β is of
the order of
σmax( P=ImXWk)(φ0(XWk))>) Θ (XX>))
	max	k=1 σmin(Pm=1 (φ0(XWk)(φ0(XWk))>) Θ (XX>))
If we consider the identify activation function, i.e., φ(t) = t, then it follows from the definition of
ΣS that	σmax XX>	σmax ΣS L∕β X 	7	v÷ = 	7——Γ. “σ1min(XX >)	σmin(∑s)
23