Published as a conference paper at ICLR 2021
Efficient Generalized Spherical CNNs
Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker,
Augustin Marignier, Matthew A. Price, Mayeuld'Avezac & Jason D. McEwen*
Kagenova Limited, Guildford GU5 9LD, UK
Ab stract
Many problems across computer vision and the natural sciences require the anal-
ysis of spherical data, for which representations may be learned efficiently by en-
coding equivariance to rotational symmetries. We present a generalized spherical
CNN framework that encompasses various existing approaches and allows them
to be leveraged alongside each other. The only existing non-linear spherical CNN
layer that is strictly equivariant has complexity OpC2L5q, where C is a measure
of representational capacity and L the spherical harmonic bandlimit. Such a high
computational cost often prohibits the use of strictly equivariant spherical CNNs.
We develop two new strictly equivariant layers with reduced complexity OpCL4q
and OpCL3 log Lq, making larger, more expressive models computationally fea-
sible. Moreover, we adopt efficient sampling theory to achieve further computa-
tional savings. We show that these developments allow the construction of more
expressive hybrid models that achieve state-of-the-art accuracy and parameter ef-
ficiency on spherical benchmark problems.
1	Introduction
Many fields involve data that live inherently on spherical manifolds, e.g. 360° photo and video Con-
tent in virtual reality and computer vision, the cosmic microwave background radiation from the Big
Bang in cosmology, topographic and gravitational maps in planetary sciences, and molecular shape
orientations in molecular chemistry, to name just a few. Convolutional neural networks (CNNs) have
been tremendously effective for data defined on Euclidean domains, such as the 1D line, 2D plane,
or nD volumes, thanks in part to their translation invariance properties. However, these techniques
are not effective for data defined on spherical manifolds, which have a very different geometric
structure to Euclidean spaces (see Appendix A). To transfer the remarkable success of deep learning
to data defined on spherical domains, deep learning techniques defined inherently on the sphere are
required. Recently, a number of spherical CNN constructions have been proposed.
Existing CNN constructions on the sphere fall broadly into three categories: fully real (i.e. pixel)
space approaches (e.g. Boomsma & Frellsen, 2017; Jiang et al., 2019; Perraudin et al., 2019; Co-
hen et al., 2019); combined real and harmonic space approaches (Cohen et al., 2018; Esteves et al.,
2018; 2020); and fully harmonic space approaches (Kondor et al., 2018). Real space approaches
can often be computed efficiently but they necessarily provide an approximate representation of
spherical signals and the connection to the underlying continuous symmetries of the sphere is lost.
Consequently, such approaches cannot fully capture rotational equivariance. Other constructions
take a combined real and harmonic space approach (Cohen et al., 2018; Esteves et al., 2018; 2020),
where sampling theorems (Driscoll & Healy, 1994; Kostelec & Rockmore, 2008) are exploited to
connect with underlying continuous signal representations to capture the continuous symmetries of
the sphere. However, in these approaches non-linear activation functions are computed pointwise
in real space, which induces aliasing errors that break strict rotational equivariance. Fully harmonic
space spherical CNNs have been constructed by Kondor et al. (2018). A continual connection with
underlying continuous signal representations is captured by using harmonic signal representations
throughout. Consequently, this is the only approach exhibiting strict rotational equivariance. How-
ever, strict equivariance comes at great computational cost, which can often prohibit usage.
φCorresponding author: jason.mcewen@kagenova.com
1
Published as a conference paper at ICLR 2021
In this article we present a generalized framework for CNNs on the sphere (and rotation group),
which encompasses and builds on the influential approaches of Cohen et al. (2018), Esteves et al.
(2018) and Kondor et al. (2018) and allows them to be leveraged alongside each other. We adopt a
harmonic signal representation in order to retain the connection with underlying continuous repre-
sentations and thus capture all symmetries and geometric properties of the sphere. We construct new
fully harmonic (non-linear) spherical layers that are strictly rotationally equivariant, are parameter-
efficient, and dramatically reduce computational cost compared to similar approaches. This is
achieved by a channel-wise structure, constrained generalized convolutions, and an optimized de-
gree mixing set determined by a minimum spanning tree. Furthermore, we adopt efficient sampling
theorems on the sphere (McEwen & Wiaux, 2011) and rotation group (McEwen et al., 2015a) to im-
prove efficiency compared to the sampling theorems used in existing approaches (Driscoll & Healy,
1994; Kostelec & Rockmore, 2008). We demonstrate state-of-the-art performance on all spherical
benchmark problems considered, both in terms of accuracy and parameter efficiency.
2	Generalized S pherical CNNs
We first overview the theoretical underpinnings of the spherical CNN frameworks introduced by
Cohen et al. (2018), Esteves et al. (2018), and Kondor et al. (2018), which make a connection to
underlying continuous signals through harmonic representations. For more in-depth treatments of
the underlying harmonic analysis we recommend Esteves (2020), Kennedy & Sadeghi (2013) and
Gallier & Quaintance (2019). We then present a generalized spherical layer in which these and
other existing frameworks are encompassed, allowing existing frameworks to be easily integrated
and leveraged alongside each other in hybrid networks.
Throughout the following we consider a network composed of S rotationally equivariant layers
Ap1q,….，ApSq, where the i-th layer APiq maps an input activation f PiTq P HpiTq onto an out-
put activation fpiq P Hpiq . We focus on the case where the network input space Hp0q consists of
spherical signals (but note that input signals on the rotation group may also be considered).
2.1	Signals on the Sphere and Rotation Group
Let L2(Ω) denote the space of square-integrable functions over domain Ω. A signal f P L2(Ω)
on the sphere (Ω “ S2) or rotation group (Ω “ SO(3)) can be rotated by P P SO(3) by defining
the action of rotation on signals by RPf (ω) “ f Pρ´1ω'q for ω P Ω. An operator A : L2 (Ωι) →
L2(Ω2), where Ωι, Ω2 P {S2, SO(3)}, is then equivariant to rotations if RPpApf)) “ ApRpf q
for all f P L2pΩ1) and P P SOp3), i.e. rotating the function before application of the operator is
equivalent to application of the operator first, followed by a rotation.
A spherical signal f P L2 pS2) admits a harmonic representation pf0, f1,...,) where f' P C2''1 are
the harmonic coefficients given by the inner product Xf, Ymm〉, where Ymm are the spherical harmonic
functions of degree ' and order |m| ≤ '. Likewise a signal f P L2pSOp3)) on the rotation group ad-
mits a harmonic representation pf0, f1,...) where f' P cp2''ιqχp2''ιq are the harmonic coefficients
with pm, n)-th entry Xf, Dmmy for integers |m|, |n| ≤ ', where D' : SO(3) → cp2''ιqχp2''ιq is
the unique 2' ` 1 dimensional irreducible group representation of SOp3) on Cp2''1q. The rotation
f → Rpf of a signal f P L2 (Ω) can be described in harmonic space by f' → DepPqf'
A signal on the sphere or rotation group is said to be bandlimited at L if, respectively, Xf, Ym' y “ 0
or Xf, Dmny “ O for ' > L. Furthermore, a signal on the rotation group is said to be azimuthally
bandlimited at N if, additionally, Xf, Dm' ny “ 0 for |n|2N. Bandlimited signals therefore ad-
mit finite harmonic representations (∕0,…，/'´1). In practice real-world signals can be accurately
represented by suitably bandlimited signals; henceforth, we assume signals are bandlimited.
2.2	Convolution on the Sphere and Rotation Group
A standard definition of convolution between two signals f, ψ P L2pΩ) on either the sphere (Ω “
S2) or rotation group (Ω “ SO(3)) is given by
pf ‹ ψ)pP) “ Xf, Rpψy “
ʃ
Jω
dμ(ω)f(ω) ψ*(ρTω),
(1)
2
Published as a conference paper at ICLR 2021
where dμ(ω) denotes the Haar measure on Ω and ∙* complex conjugation (e.g. Wandelt & Gorski,
2001; McEwen et al., 2007; 2013; 2015b; 2018; Cohen et al., 2018; Esteves et al., 2018). In partic-
ular, the convolution satisfies
((RPf q< ψ)(ρ1q = χRρf, Rpi ψy “ xf, Rρ-iρi ψy “ PRpPf < ψqqpρq	⑵
and is therefore a rotationally equivariant linear operation, which we shall denote by Lpψq .
The convolution of bandlimited signals can be computed exactly and efficiently in harmonic space
as
({ < ψq' = f'Ψ'*,	' = 0,...,L ´ 1,	(3)
which for each degree ` is a vector outer product for signals on the sphere and a matrix product for
signals on the rotation group (see Appendix B for further details). Convolving in this manner results
in signals on the rotation group (for inputs on both the sphere and rotation group). However, in the
spherical case, if the filter is invariant to azimuthal rotations the resultant convolved signal may be
interpreted as a signal on the sphere (see Appendix B).
2.3	Generalized Signal Representations
The harmonic representations and convolutions described above have proven useful for describing
rotationally equivariant linear operators Lpψq. Cohen et al. (2018) and Esteves et al. (2018) define
spherical CNNs that sequentially apply this operator, with intermediary representations taking the
form of signals on SO(3q and S2 respectively. Alternatively, for intermediary representations we
now consider the more general space of signals introduced by Kondor et al. (2018), to which the
aforementioned notions of rotation and convolution naturally extend.
In describing the generalization we first note from Section 2.1 that all bandlimited signals on the
sphere and rotation group can be represented as a set of variable length vectors of the form
f = tft P C2''1 ：' = 0,..,L ´ 1; t = 1,…,τfu,	(4)
where Tf = 1 for signals on the sphere and Tf = min(2' ` 1, 2N — 1) for signals on the rotation
group. The generalization is to let FL be the space of all such sets of variable length vectors, with
Tf unrestricted. This more general space contains the spaces of bandlimited signals on the sphere
and rotation group as strict subspaces. For a generalized signal f P FL we adopt the terminology of
Kondor et al. (2018) by referring to ff as the t-th fragment of degree ' and to Tf = (τf,…,午；-1),
specifying the number of fragments for each degree, as the type of f . The action of rotations upon
FL can be naturally extended from their action upon L2 (S2) and L2 (SO(3)). For f P FL we
define the rotation operator f → RPf by ff → D'(ρ)f', allowing US to extend the usual notion of
equivariance to operators A : FL → Fl.
2.4	Generalized Convolutions
The convolution described by Equation 1 provides a learnable linear operator Lpψq that satisfies the
desired property of equivariance. Nevertheless, given the generalized interpretation of signals on S2
and SO(3) as signals in FL, the notion of convolution can also be generalized (Kondor et al., 2018).
In order to linearly and equivariantly transform a signal f P FL of type Tf into a new signal f * ψ P
``
Fl of any desired type Tpf*ψq, We may specify a filter ψ = {ψ' P Cf Pf *ψq : ' = 0,…，L — 1},
which in general is not an element of Fl, and define a transformation f → f * ψby
_____ `	Tf 八 八
(f * ψqt = X ft1 ψt,t1,	' = 0,…，L — 1; t = 1,…,Tpf *ψq.	(5)
t1“1
The degree-` fragments of the transformed signal (f * ψ) are simply linear combinations of the
degree-` fragments of f, with no mixing between degrees. Equation 3 shows that this is precisely
the form taken by convolution on the sphere and rotation group. In fact Kondor & Trivedi (2018)
show that all equivariant linear operations take this general form; the standard notion of convolution
is just a special case. One benefit to the generalized notion is that the filter ψ is not forced to occupy
the same domain as the signal f, thus allowing control over the type Tpf*ψq of the transformed
signal. We use LpGψq to denote this generalized convolutional operator.
3
Published as a conference paper at ICLR 2021
2.5	Non-linear Activation Operators
For FL to be a useful representational space, it must be possible to not only linearly but also non-
linearly transform its elements in an equivariant manner. However, equivariance and non-linearity is
not enough. Equivariant linear operators cannot mix information corresponding to different degrees.
Therefore it is of crucial importance that degree mixing is achieved by the non-linear operator.
2.5.1	Pointwise Activations
When the type τf of f P FL permits an interpretation as a signal on S2 or SOp3q we may perform
an inverse harmonic transform to map the function onto a sample-based representation (e.g. Driscoll
& Healy, 1994; McEwen & Wiaux, 2011; Kostelec & Rockmore, 2008; McEwen et al., 2015a).
A non-linear function σ : C → C may then be applied pointwise, i.e. separately to each sample,
before performing a harmonic transform to return to a representation in FL . We denote the corre-
SPonding non-linear operator as NPf) “ F(σ(F —1(f))), where F represents the harmonic (i.e.
Fourier) transform on S2 or SOp3q. The computational cost of the non-linear operator is dominated
by the harmonic transforms. While costly, fast algorithms can be leveraged (see Appendix A). While
inverse and forward harmonic transforms on S2 or SOp3) that are based on a sampling theory main-
tain perfect equivariance for bandlimited signals, the pointwise application of σ (most commonly
ReLU) is only equivariant in the continuous limit L → 8. For any finite bandlimit L, aliasing
effects are introduced such that equivariance becomes approximate only, as shown by the following
experiments.
We consider 100 random rotations ρ P SOp3), for each of 100 random signal-filter pairs pf, ψ),
and compute the mean equivariance error dpApRρf), Rρ pAf)) for operator A, where dpf, g) “
}f ´ g}{}f} is the relative distance between signals. For convolutions the equivariance error is
4.4 X 10—7 for signals on S2 and 5.3 X lθ´7 for signals on SO(3) (achieving floating point pre-
cision). By comparison the equivariance error for a pointwise ReLU is 0.34 for signals on S2 and
0.37 for signals on SOp3). Only approximate equivariance is achieved for the ReLU since the non-
linear operation spreads information to higher degrees that are not captured at the original bandlimit,
resulting in aliasing. To demonstrate this point we reduce aliasing error by oversampling the real-
space signal. When oversampling by 2^ or 8^ for signals on SO(3) the equivariance error of the
ReLU is reduced to 0.10 and 0.01, respectively. See Appendix D for further experimental details.
Despite the high cost of repeated harmonic transforms and imperfect equivariance, this is never-
theless the approach adopted by Cohen et al. (2018), Esteves et al. (2018) and others, who find
empirically that such models maintain a reasonable degree of equivariance.
2.5.2	Tensor-Product Activations
In order to define a strictly equivariant non-linear operation that can be applied to a signal f P FL of
any type τf the decomposability of tensor products between group representations may be leveraged,
as first considered by Thomas et al. (2018) in the context of neural networks.
Given two group representations D'1 and D'2 of SO(3) on C2'1'1 and C2'2'1 respectively, the
tensor-product group representation D'1 b D'2 of SO(3) on C2'1'1 b C2'2'1 is defined such that
(D'1 b D'2 )(ρ) “ D'1 (ρ) b D'2 (ρ) for all ρ P SO(3). Decomposing D'1 b D'2 into a direct sum
of irreducible group representations then constitutes finding a change of basis for C2'1'1 b C2'2'1
such that (D'1 b D'2 )(ρ) is block diagonal, where for each ` there is a block equal to D'(ρ). The
necessary change of basis for u'1 b v'2 P C2'1'1 b C2'2'1 is given by
'1	'2
(u'1 b v'2)m =	∑	∑ 勰；第双碌1 vm,	(6)
m1=-'1 m2=—'2
where Cm'11,',m2,'2,m P C denote Clebsch-Gordan coefficients whose symmetry properties are such that
(u'1 b V'2 )mn is non-zero only for ∣'1 一 '21 ≤ ' ≤ '1 ' '2. The use of Equation 6 arises naturally in
quantum mechanics when coupling angular momenta.
This property is useful since if f'1 P C2'1'1 and f'2 P C2'2'1 are two fragments that are equivariant
with respect to rotations of the network input, then a rotation ofρ applied to the network input results
4
Published as a conference paper at ICLR 2021
in f'1 b f'2 transforming as
产 b 产 s' → r(D'ι(ρ)产 q b Pd'2 (P)产 qs'	⑺
“ [(D'1 b D'2 )(ρ)(f'1 b 产 qs'	⑻
“ D'(ρ)rf'1 b f'2 s',	⑼
where the final equality follows by block diagonality with respect to the chosen basis. Therefore, if
fragments f'1 and f'2 are equivariant with repsect to rotations of the network input, then so is the
fragment (C'1,'2,'qj(f'1 b f'2) P C2''1, where We have written Equation 6 more compactly. We
now describe how Kondor et al. (2018) use this fact to define equivariant non-linear transformations
of elements in F L .
A signal f “ {f' P C2''1 : ' = 0,..,L ´ 1; t = 1,…,τf} P F L may be equivariantly and
non-linearly transformed by an operator Nb : FL → FL defined as
Nb(f q = t(c'ι,'2,'qJ(.f'ιι b f' q ： ` = 0,…,l ´ 1； ('1,'2qp pL； tι = 0,...,τf1 % = 0,...,τf2 u,
(10)
where for each degree ` P t0, ..., L ´ 1u the set
pL = {('1,'2q P {0,…,L ´ 1u2: |'i ´ '2 | ≤ ` ≤ '1 ` '2}	(II)
is defined in order to avoid the computation of trivially equivariant all-zero fragments. We make the
dependence on p'L explicit since we redefine it in Section 3.
Unlike the pointwise activations discussed in the previous section this operator is strictly equivariant,
with a mean relative equivariance error of 5.0 X lθ´7 (see Appendix D). Note that applying this
operator to signals on the sphere or rotation group results in generalized signals that are no longer
on the sphere or rotation group. This is the rationale for the generalization to FL : to unlock the
ability to introduce non-linearity in a strictly equivariant manner. Note, however, that g = Nb (fq
has type Tg = (τg,…,t^´1) where Tg = 2p'ɪ '2)pp` TfI τf2 and therefore application of this non-
linear operator results in a drastic expansion in representation size, which is problematic.
2.6	Generalized Spherical CNNs
Equipped with operators to both linearly and non-linearly transform elements of FL, with the latter
also performing degree mixing, we may consider a network with representation spaces Hp0q =
... = HpSq = FL . We consider the s-th layer of the network to take the form of a triple Apsq =
(Lι,N,L2 such that Apsq(f PSTqq = L2(N(Lι(f PSTqqqq, where Li, L2 : FL → FL are linear
operators and N : FL → FL is a non-linear activation operator.
The approaches of Cohen et al. (2018) and Esteves et al. (2018) are encompassed in this framework
as APSq = (LPψq , Nσ, Iq, where I denotes the identity operator and filters ψ may be defined to
encode real-space properties such as localization (see Appendix C). The framework of Kondor et al.
(2018) is also encompassed as APSq = (I,Nb, LPGψqq. Here the generalized convolution LPGψq comes
last to counteract the representation-expanding effect of the tensor-product activation and prevent it
from compounding as signals pass through the network. Appendix E lends intuition regarding rela-
tionships that may be captured by tensor-product activations followed by generalized convolutions.
For any intermediary representation fPiq P FL we may transition from equivariance with respect to
the network input to invariance by discarding all but the scalar-valued fragments corresponding to
' = 0 (equivalent to average pooling for signals on the sphere and rotation group). Finally, note that
within this general framework we are free to consider hybrid approaches whereby layers proposed
by Cohen et al. (2018); Esteves et al. (2018); Kondor et al. (2018) and others, and those presented
subsequently, can be leveraged alongside each other within a single model.
3	Efficient Generalized S pherical CNNs
Existing approaches to spherical convolutional layers that are encompassed within the above frame-
work are computationally demanding. They require the evaluation of costly harmonic transforms
5
Published as a conference paper at ICLR 2021
on the sphere and rotation group. Furthermore, the only strictly rotationally equivariant non-linear
layer is that of Kondor et al. (2018), which has an even greater computational cost, scaling with the
fifth power of bandlimit — thereby limiting spatial resolution — and quadratically with the number
of fragments per degree — thereby limiting representational capacity. This often prohibits the use
of strictly equivariant spherical networks.
In this section we introduce a channel-wise structure, constrained generalized convolutions, and an
optimized degree mixing set in order to construct new strictly equivariant layers that exhibit much
improved scaling properties and parameter efficiency. Furthermore, we adopt efficient sampling
theory on the sphere and rotation group to achieve additional computational savings.
3.1	Efficient Generalized Spherical Layers
For an activation f P FL the value Tf “ L ∏ Tf represents a resolution-independent proxy
for its representational capacity. Kondor et al. (2018) consider the separate fragments contained
within f to subsume the traditional role of separate channels and therefore control the capacity
of intermediary network representations through specification of Tf. This is problematic because,
whereas activation functions usually act on each channel separately and therefore have a cost that
scales linearly with representational capacity (usually controlled by the number of channels), for the
activation function Nb not only does the cost scale quadratically with representational capacity Tf,
but so too does the size of Nbpfq. This feeds forward the quadratic dependence to the cost of, and
number of parameters required by, the proceeding generalized convolution.
More specifically, note that computation of g “ Nbpf) requires the computation of XL´1 Tg frag-
ments, where Tg “ ^p`ɪ 七)pp` τf1 τf2. The size of PL is OpL') for each ' and therefore the ex-
panded representation has size XL´1 t/, of order OpTyL3). By exploiting the sparsity of Clebsch-
Gordan coefficients (Cm,'m' m “ 0 if mi ' m2 ‰ m) each fragment (C'1,'2,')Jpf'1 b f') Can
m1,m2,m
be computed in O('min('1,'2)). Hence, the total cost of computing all necessary fragments has
complexity OpC2L5), where C “ Tf captures representational capacity.
3.1.1	Channel-Wise Tensor-Product Activations
As is more standard for CNNs we maintain a separate channels axis, with network activations taking
the form pfi, ..., fK) P FLK where fi P FL all share the same type Tf. The non-linearity Nb may
then be applied to each channel separately at a cost that is reduced by K-times relative to its applica-
tion on a single channel with the same total number of fragments. This saving arises since for each
' we need only compute K ^p`ɪ '2)pp' Tf1 Tf2 fragments rather than ^p`ɪ '2)pp' (Kt；1 )(Kt'2 ).
Figure 1 visualizes this reduction for the case K “ 3. Note, however, that for practical applications
K „ 100 is more typical. The K-times reduction in cost is therefore substantial and allows for
intermediary activations with orders of magnitude more representational capacity.
By introducing this multi-channel approach and using C “ K rather than C “ Tf to control
representational capacity, we reduce the complexity ofNb with respect to representational capacity
from OpC2) toOpC).
N
'=0	'=1	'=2
(a) Prior approach to applying a tensor-product based non-linear operator
Figure 1: Comparison (to scale) of the expansion caused by the tensor-product activation applied to
inputs of equal representational capacity but different structure. With depth representing the number
of channels and width the number of fragments for each degree, itis clear that by grouping fragments
into K separate channels the expansion (and therefore cost) can be K-times reduced. Visualization
corresponds to inputs with pL, K) equal to p3, 1) and p3, 3) for panel (a) and (b), respectively.
N
'=0, 1, 2
(b) Ours
6
Published as a conference paper at ICLR 2021
3.1.2	Constrained Generalized Convolution
Although much reduced, for a signal f P F LKin the channel-wise application of Nb still results in a
drastically expanded representation g “ Nbpfq, to which a representation-contracting generalized
convolution must be applied in order to project onto a new activation g1 “ LpGψq pgq P F LKout of
the desired type τg1 and number of channels Kout. However, under our multi-channel structure
computational and parameter efficiency can be improved significantly by decomposing LpGψq into
three separate linear operators, LpGψ1q, LpGψ2q and LpGψ3q.
The first, LpGψ1q, acts uniformly across channels, performing a linear projection down onto the
desired type, and should be interpreted as a learned extension of Nb which undoes the drastic
expansion. The second, LpGψ2q , then acts channel-wise, taking linear combinations of the (con-
tracted number of) fragments within each channel. The third, LpGψ3q, acts across channels, tak-
ing linear combinations to learn new features. More concretely, the three filters are of the form
``	`	`
ψι “ tψ' P C g g1 : ' = 0,…,L ´ 1}, ψ2 “ tψ2 P C g1 g1 : ' = 0,...,L ´ 1; k = 1,…,Kin}
and ψ3 “ tψ3 P CKinXKout ： ' “ 0,...,L — 1}, rather than a single filter of the form
ψ “ {ψ' P CKin xτ'x Kout χ τ'1 ： ` “ 0,...,L — ",leading to a large reduction in the number of pa-
rameters as Tg is invariably very large.
By applying the first step uniformly across channels we minimize the parametric dependence on the
expanded representation and allow new features to be subsequently learned much more efficiently.
Together the second and third steps can be seen as analogous to depthwise separable convolutions
often used in planar convolutional networks.
3.1.3	Optimized Degree Mixing Sets
We now consider approaches to reduce the OpL5q complexity with respect to spatial resolution L. In
the definition of Nb each element of PL independently defines an equivariant fragment. Therefore a
restricted Nb in which only a subset of PL is used for each degree ' still defines a strictly equivariant
operator, while reducing computational complexity. In order to make savings whilst remaining at
resolution L it is necessary to consider subsets of PL that scale better than O(L2). The challenge
is to find such subsets that do not hamper the ability of the resulting operator to inject non-linearity
and mix information corresponding to different degrees `.
Whilst various subsetting approaches are possible, the following argument motivates an approach
that We have found to be particularly effective. If ('1,'3) P PL, then representational space is
designated to capture the relationship between '1 and '3-degree information. However, if resources
have been designated already to capture the relationship between '1 and '2-degrees, as well as
between '2 and '3-degrees, then some notion of the relationship between '1 and '3-degrees has been
captured already. Consequently, it is unnecessary to designate further resources for this purpose.
More generally, consider the graph GL “(NL, PL) with nodes NL “ {0,...,L—1} and edges PL. A
restricted tensor-product activation can be constructed by using a subset of PL that corresponds to a
subgraph of G：. The subgraph of GL captures some notion of the relationship between incoming '1
and '2-degree information ifit contains a path between nodes '1 and '2. Therefore we are interested
in subgraphs for which there exists a path between any two nodes if there exists such a path in the
original graph, guaranteeing that any degree-mixing relationship captured by the original graph is
also captured by the subgraph.
The smallest subgraph satisfying this property is a minimum spanning tree (MST) of GL. The set
of edges corresponding to any MST has at most L elements and we choose to consider its union
with the set of loop-edges in GL (of the form ('1, '1)), which proved particularly important for
injecting non-linearity. We denote the resulting set as PL and note that it satisfies ∣PL∣ ≤ 2L.
Therefore the tensor-product activation Nb corresponding to Equation 11 with PL replaced by PL
has reduced spatial complexity O(L4). Given that many minimal spanning trees of the unweighted
graph GL exist for each ', we select the ones that minimize the cost of the resulting activation Nb by
assigning to each edge ('1,'2) in GL a weight equal to the cost of computing (C`1 ,'2,')J (f'1 b f')
and selecting the MST of the weighted graph.
7
Published as a conference paper at ICLR 2021
/1
(a) Full PL set of size O(L2)
(b) MST subset of size O(L)
(c) RMST subset of size Oplog Lq
Figure 2: Visualization of the mixing set PL (for L “ 7 and ' “ 4) and the approaches to subsetting
based on the minimum spanning tree (MST) and reduced minimum spanning tree (RMST) mixing
polices, which reduces related computation costs from OpL2q to, respectively, OpLq or Oplog Lq.
An example of PL and a MST subset PL is shown in Figure 2, where the dashed line in Figure 2c
shows the general form of the MST. Using this as a principled starting point we consider the further
reduced MST (RMST) subset PL corresponding to centering the MST at the edge (',') and retaining
only the edges that fall a distance of2i away on the dotted line for some i P N. We use Nb to denote
the corresponding operator and note that it has further reduced spatial complexity of OpL3 log Lq.
We demonstrate in Section 4 that networks that make use of the MST tensor-product activation
achieve state-of-the-art performance. Replacing the MST with RMST activation results in a small
but insignificant degradation in performance, which is offset by the reduced computational cost.
3.1.4	Reduction in Computational and Memory Footprints
The three modifications proposed in Sections 3.1.1 to 3.1.3 are motivated by improved scaling prop-
erties. Importantly, they also translate to large reductions in the computational and memory cost
of strictly equivariant layers in practice, as detailed in Appendix F. Even at a modest bandlimit of
L “ 64 and relatively small number of channels K “ 4, for example, the modifications together
lead to a 51-times reduction in the number of flops required for computations and 16-times reduction
in the amount of memory required to store representations, weights and gradients for training.
3.2	Efficient Sampling Theory
By adopting sampling theorems on the sphere we provide access to underlying continuous signal
representations that fully capture the symmetries and geometric properties of the sphere, and allow
standard convolutions tobe computed exactly and efficiently through their harmonic representations,
as discussed in greater detail in Appendices A and B. We adopt the efficient sampling theorems on
sphere and rotation group of McEwen & Wiaux (2011) and McEwen et al. (2015a), respectively,
which reduce the Nyquist rate by a factor of two compared to those of Driscoll & Healy (1994)
and Kostelec & Rockmore (2008), which have been adopted in other spherical CNN constructions
(e.g. Cohen et al., 2018; Kondor et al., 2018; Esteves et al., 2018; 2020). The sampling theorems
adopted are equipped with fast algorithms to compute harmonic transforms, with complexity OpL3q
for the sphere and OpL4q for the rotation group. When imposing an azimuthal bandlimit N ! L, the
complexity of transforms on the rotation group can be reduced to OpNL3q, which we often exploit
in our standard (non-generalized) convolutional layers.
4	Experiments
Using our efficient generalized spherical CNN framework (implemented in the fourPiAI 1 code)
We construct networks that We apply to numerous spherical benchmark problems. We achieve state-
of-the-art performance, demonstrating enhanced equivariance without compromising representa-
tional capacity or parameter efficiency. In all experiments we use a similar architecture, consisting
of 2-3 standard convolutional layers (e.g. S2 or SO(3) convolutions proceeded by ReLUs), followed
by 2-3 of our efficient generalized layers. We adopt the efficient sampling theory described in Sec-
tion 3.2 and encode localization of spatial filters as discussed in Appendix C. Full experimental
details may be found in Appendix G.
1httPs://www.kagenova.com/Products/fourPiAI/
8
Published as a conference paper at ICLR 2021
Table 1: Test accuracy for spherical MNIST digits clas-
sification problem
Table 2: Test root mean squared (RMS)
error for QM7 regression problem
	NR/NR	R/R	NR/R	Params		RMS	Params
Planar CNN	99.32	90.74	11.36	58k	Montavon et al. (2012)	5.96	-
Cohen et al. (2018)	95.59	94.62	93.40	58k	Cohen et al. (2018)	8.47	1.4M
Kondor et al. (2018)	96.40	96.60	96.00	286k	Kondor etal. (2018)	7.97	>1.1M
Esteves et al. (2020)	99.37	99.37	99.08	58k	OUrS (MST)	3.16	337k
Ours (MST)	99.35	99.38	99.34	-58k-	OUrS (RMST)		3.46	335k
Ours (RMST)	99.29	99.17	99.18	57k			
Table 3: SHREC’17 object retrieval competition metrics (perturbed micro-all)
	P@N	R@N	F1@N	mAP	NDCG	Params
Kondor et al. (2018)	0.707	0.722	0.701	0.683	0.756	>1M
Cohen et al. (2018)	0.701	0.711	0.699	0.676	0.756	1.4M
Esteves et al. (2018)	0.717	0.737	-	0.685	-	500k
Ours	0.719	0.710	0.708	0.679	0.758	250k
4.1	Rotated MNIST on the Sphere
We consider the now standard benchmark problem of classifying MNIST digits projected onto the
sphere. Three experimental modes NR/NR, R/R and NR/R are considered, indicating whether the
training/test sets have been randomly rotated (R) or not (NR). Results are presented in Table 1,
which shows that we closely match the prior state-of-the-art performance obtained by Esteves et al.
(2020) on the NR/NR and R/R modes, whilst outperforming all previous spherical CNNs on the
NR/R mode, demonstrating the increased degree of equivariance achieved by our model.
Results are shown for models using both the MST-based and RMST-based mixing sets within the
tensor-product activation. The results obtained when using the full sets PL are very similar to those
obtained when using the MST-based sets (e.g. full sets achieved an accuracy of 99.39 for R/R).
4.2	Atomization Energy Prediction
We consider the problem of regressing the atomization energy of molecules given the molecule’s
Coulomb matrix and the positions of the atoms in space, using the QM7 dataset (Blum & Rey-
mond, 2009; Rupp et al., 2012). Results are presented in Table 2, which shows that we dramatically
outperform other approaches, whilst using significantly fewer parameters.
4.3	3D Shape Retrieval
We consider the 3D shape retrieval problem on the SHREC’17 (Savva et al., 2017) competition
dataset, containing 51k 3D object meshes. We follow the pre-processing step of Cohen et al. (2018),
where several spherical projections of each mesh are computed, and use the official SHREC’17 data
splits. Results are presented in Table 3 for the standard SHREC precision and recall metrics, which
shows that we achieve state-of-the-art performance compared to other spherical CNN approaches,
achieving the highest three of five performance metrics, whilst using significantly fewer parameters.
5	Conclusions
We have presented a generalized framework for CNNs on the sphere that encompasses various ex-
isting approaches. We developed new efficient layers to be used as primary building blocks in this
framework by introducing a channel-wise structure, constrained generalized convolutions, and opti-
mized degree mixing sets determined by minimum spanning trees. These new efficient layers exhibit
strict rotational equivariance, without compromising on representational capacity or parameter effi-
ciency. When combined with the flexibility of the generalized framework to leverage the strengths
of alternative layers, powerful hybrid models can be constructed. On all spherical benchmark prob-
lems considered we achieve state-of-the-art performance, both in terms of accuracy and parameter
efficiency. In future work we intend to improve the scalability of our generalized framework further
still. In particular, we plan to introduce additional highly scalable layers, for example by extending
scattering transforms (Mallat, 2012) to the sphere, to further realize the potential of deep learning
on a host of new applications where spherical data are prevalent.
9
Published as a conference paper at ICLR 2021
References
Lorenz Blum and Jean-Louis Reymond. 970 million druglike small molecules for virtual screening
in the chemical universe database GDB-13. Journal of the American Chemical Society, 131:8732,
2009.
Wouter Boomsma and Jes Frellsen. Spherical convolutions and their application in molecular mod-
elling. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3433-3443. CUr-
ran Associates, Inc., 2017.
Taco Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. In International
Conference on Learning Representations, 2018. URL https://arxiv.org/abs/1801.10130.
Taco Cohen, MaUrice Weiler, Berkay KicanaoglU, and Max Welling. GaUge eqUivariant convo-
lUtional networks and the icosahedral CNN. arXiv preprint arXiv:1902.04615, 2019. URL
https://arxiv.org/abs/1902.04615.
James Driscoll and Dennis Healy. CompUting FoUrier transforms and convolUtions on the sphere.
Advances in Applied Mathematics, 15:202-250, 1994.
Carlos Esteves. Theoretical aspects of groUp eqUivariant neUral networks. arXiv preprint
arXiv:2004.05154, 2020. URL https://arxiv.org/abs/2004.05154.
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning
SO(3) eqUivariant representations with spherical CNNs. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pp. 52-68, 2018. URL https://arxiv.org/abs/1711.
06721.
Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical CNNs. arXiv
preprint arXiv:2006.10731, 2020. URL https://arxiv.org/abs/2006.10731.
Jean Gallier and Jocelyn QUaintance. Aspects of Harmonic Analysis and Representation Theory.
2019. URL https://www.seas.upenn.edu/„jean/nc-harmonic.pdf.
Dennis Healy, Daniel Rockmore, Peter Kostelec, and S. Moore. FFTs for the 2-sphere - improve-
ments and variations. Journal of Fourier Analysis and Applications, 9(4):341-385, 2003.
ChiyU Jiang, Jingwei HUang, Karthik Kashinath, Philip MarcUs, Matthias Niessner, et al. Spherical
CNNs on UnstrUctUred grids. arXiv preprint arXiv:1901.02039, 2019. URL https://arxiv.
org/abs/1901.02039.
Rodney A Kennedy and Parastoo Sadeghi. Hilbert space methods in signal processing. Cambridge
University Press, 2013.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic gradient descent. In ICLR:
International Conference on Learning Representations, 2015. URL https://arxiv.org/abs/
1412.6980.
Risi Kondor and ShUbhendU Trivedi. On the generalization of eqUivariance and convolUtion in neUral
networks to the action of compact groUps. In International Conference on Machine Learning, pp.
2747-2755, 2018. URL https://arxiv.org/abs/1802.03690.
Risi Kondor, Zhen Lin, and ShUbhendU Trivedi. Clebsch-Gordan nets: a fUlly foUrier space spherical
convolUtional neUral network. In Advances in Neural Information Processing Systems, pp. 10117-
10126, 2018. URL https://arxiv.org/abs/1806.09231.
Peter Kostelec and Daniel Rockmore. FFTs on the rotation groUp. Journal of Fourier Analysis and
Applications, 14:145-179, 2008.
Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics,
65(10):1331-1398, 2012. URL https://arxiv.org/abs/1101.2286.
Domenico Marinucci and Giovanni Peccati. Random Fields on the Sphere: Representation, Limit
Theorem and Cosmological Applications. Cambridge University Press, 2011.
10
Published as a conference paper at ICLR 2021
Jason McEwen and Yves Wiaux. A novel sampling theorem on the sphere. IEEE Transactions on
Signal Processing, 59(12):5876-5887, 2011. URL https://arxiv.org/abs/1110.6298.
Jason McEwen, Michael P. Hobson, Daniel J. Mortlock, and Anthony N. Lasenby. Fast directional
continuous spherical wavelet transform algorithms. IEEE Trans. Sig. Proc., 55(2):520-529, 2007.
URL https://arxiv.org/abs/astro-ph/0506308.
Jason McEwen, Pierre Vandergheynst, and Yves Wiaux. On the computation of directional scale-
discretized wavelet transforms on the sphere. In Wavelets and Sparsity XV, SPIE international
symposium on optics and photonics, invited contribution, volume 8858, 2013. URL https:
//arxiv.org/abs/1308.5706.
Jason McEwen, Martin Buttner, Boris Leistedt, Hiranya V Peiris, and Yves Wiaux. A novel sam-
pling theorem on the rotation group. IEEE Signal Processing Letters, 22(12):2425-2429, 2015a.
URL https://arxiv.org/abs/1508.03101.
JaSon McEwen, Boris Leistedt, Martin Buttner, Hiranya Peiris, and Yves Wiaux. Directional spin
wavelets on the sphere. IEEE Trans. Sig. Proc., submitted, 2015b. URL https://arxiv.org/
abs/1509.06749.
Jason McEwen, Claudio Durastanti, and Yves Wiaux. Localisation of directional scale-discretised
wavelets on the sphere. Applied Comput. Harm. Anal., 44(1):59-88, 2018. URL https://arxiv.
org/abs/1509.06767.
Gregoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe,
Alexandre Tkatchenko, Anatole V. Lilienfeld, and Klaus-Robert Muller. Learning invariant repre-
sentations of molecules for atomization energy prediction. In F. Pereira, C. J. C. Burges, L. Bot-
tou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp.
440-448. Curran Associates, Inc., 2012.
Nathanael Perraudin, Michael Defferrard, Tomasz Kacprzak, and Raphael Sgier. Deepsphere: Ef-
ficient spherical convolutional neural network with HEALPix sampling for cosmological appli-
cations. Astronomy and Computing, 27:130-146, 2019. URL https://arxiv.org/abs/1810.
12186.
Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Muller, and O. Anatole von Lilienfeld. Fast
and accurate modeling of molecular atomization energies with machine learning. Physical Review
Letters, 108:058301, 2012. URL https://arxiv.org/abs/1109.2618.
Manolis Savva, Fisher Yu, Hao Su, Asako Kanezaki, Takahiko Furuya, Ryutarou Ohbuchi, Zhichao
Zhou, Rui Yu, Song Bai, Xiang Bai, et al. Large-scale 3d shape retrieval from shapenet core55:
Shrec’17 track. In Proceedings of the Workshop on 3D Object Retrieval, pp. 39-50. Eurographics
Association, 2017.
Max Tegmark. An Icosahedron-Based method for pixelizing the celestial sphere. Astrophys. J. Lett.,
470:L81, October 1996. URL https://arxiv.org/abs/astro-ph/9610094.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018. URL https://arxiv.org/abs/1802.08219.
Stefano Trapani and Jorge Navaza. Calculation of spherical harmonics and Wigner d functions by
FFT. Applications to fast rotational matching in molecular replacement and implementation into
AMoRe. Acta Crystallographica Section A, 62(4):262-269, 2006.
Benjamin Wandelt and Krzysztof Gorski. Fast convolution on the sphere. Phys. Rev. D., 63(12):
123002, 2001. URL https://arxiv.org/abs/astro-ph/0008227.
11
Published as a conference paper at ICLR 2021
A Representations of Signals on the S phere and Rotation Group
To provide further context for the discussion presented in the introduction and to elucidate the prop-
erties of different sampling theory on the sphere and rotation group, we concisely review represen-
tations of signals on the sphere and rotation group.
A. 1 Discretization
It is well-known that a completely regular point distribution on the sphere does in general not exist
(e.g. Tegmark, 1996). Consequently, while a variety of spherical discretization schemes exists (e.g.
icosahedron, HEALPix, graph, and other representations), it is not possible to discretize (i.e. to
sample or pixelize) the sphere in a manner that is invariant to rotations, i.e. a discrete sampling of
rotations of the samples on the sphere will in general not map onto the same set of sample positions.
This differs to the Euclidean setting and has important implications when constructing convolution
operators on the sphere, which clearly are a critical component of CNNs.
Since convolution operators are in general built using a translation operator - equivalently a rotation
operator when on the sphere - it is thus not possible to construct a convolution operator directly on a
discretized representation of the sphere that captures all of the symmetries of the underlying spher-
ical manifold. While approximate discrete representations can be considered, and are nevertheless
useful, such representations cannot capture all underlying spherical symmetries.
A.2 Sampling Theory
Alternative representations, however, can capture all underlying spherical symmetries. Sampling
theories on the sphere (e.g. Driscoll & Healy, 1994; McEwen & Wiaux, 2011) provide a mechanism
to capture all information content of an underlying continuous function on the sphere from a finite
set of samples (and similarly on the rotation group; Kostelec & Rockmore 2008; McEwen et al.
2015a). A sampling theory on the sphere is equivalent to a cubature (i.e. quadrature) rule for the
exact integration of a bandlimited functions on the sphere. While optimal cubature on the sphere
remains an open problem, the most efficient sampling theory on the sphere and rotation group is that
developed by McEwen & Wiaux (2011) and McEwen et al. (2015a), respectively.
On a compact manifold like the sphere (and rotation group), harmonic (i.e. Fourier) space is discrete.
Hence, a finite set of harmonic coefficients captures all information content of an underlying contin-
uous bandlimited signal. Since such a representation provides access to the underlying continuous
signal, all symmetries and geometric properties of the sphere are captured perfectly. Such repre-
sentations have been employed extensively in the construction of wavelet transforms on the sphere,
where the use of sampling theorems on the sphere and rotation group yield wavelet transforms of
discretized continuous signals that are theoretically exact (e.g. McEwen et al., 2013; 2015b; 2018).
Harmonic signal representations have also been exploited in spherical CNNs to access all underlying
spherical symmetries and develop equivariance network layers (Cohen et al., 2018; Kondor et al.,
2018; Esteves et al., 2018; 2020).
A.3 Exact and Efficient Computation
Signals on the sphere f P L2pS2q may be decomposed into their harmonic representations as
8	'
f(ω) = ∑ ∑ fm Ym (ω),	(12)
'=0 m = —'
where their spherical harmonic coefficients are given by
fm =〈f,Ym y= f dμ(ω)f (ω) Ym*(ω),	(13)
S2
for ω P S2 . Similarly, signals on the rotation group g P L2 (SO(3qq may be decomposed into their
harmonic representations as
8 2' ' 1 JL JL ，，	，，
g(ρq = ∑ -8∏2- ∑	∑ gmnDmn(ρ)	(14)
'=0	m = -'n = 一'
12
Published as a conference paper at ICLR 2021
where their harmonic (Wigner) coefficients are given by
g^mn “ RDm^ “	dμpPqgpPq DmnPPq,
SOp3q
(15)
for ρ P SOP3q. Note that we adopt the convention where the conjugate of the Wigner D-function
is used in Equation 14 since this leads to a convenient harmonic representation when considering
convolutions (cf. McEwen et al., 2015a; 2018).
As mentioned above, sampling theory pertains to strategies to capture all of the information content
of band limited signals from a finite set of samples. Since the harmonic space of the sphere and rota-
tion group is discrete, this is equivalent to an exact quadrature rule for the computation of harmonic
coefficients by Equation 13 and Equation 15 from sampled signals.
The canonical equiangular sampling theory on the sphere was that developed by Driscoll & Healy
(1994), and subsequently extended to the rotation group by Kostelec & Rockmore (2008). More
recently, novel sampling theorems on the sphere and rotation group were developed by McEwen
& Wiaux (2011) and McEwen et al. (2015a), respectively, that reduce the Nyquist rate by a factor
of two. Previous CNN constructions on the sphere (e.g. Cohen et al., 2018; Kondor et al., 2018;
Esteves et al., 2018; 2020) have adopted the more well-known sampling theories of Driscoll &
Healy (1994) and Kostelec & Rockmore (2008). In contrast, we adopt the more efficient sampling
theories of McEWen & WiaUx (2011) and McEWen et al. (2015a) to provide additional efficiency
savings, implemented in the open source Ssht2 and so3 3 software packages (we also make use
of a TensorFlow implementation of these algorithms in our private tensossht 4 code). Note also
that the sampling schemes associated with the theory of McEwen & Wiaux (2011) (and other minor
variants implemented in ssht) align more closely with the one-to-two aspect ratio of common
spherical data, such as 360° photos and videos.
All of the sampling theories discussed are equipped with fast algorithms to compute harmonic trans-
forms, with complexity OPL3q for transforms on the sphere (Driscoll & Healy, 1994; McEwen &
Wiaux, 2011) and complexity OPL4q for transforms on the rotation group (Kostelec & Rockmore,
2008; McEwen et al., 2015a). Note that algorithms that achieve slightly lower complexity have been
developed (Driscoll & Healy, 1994; Healy et al., 2003; Kostelec & Rockmore, 2008) but these are
known to suffer stability issues (Healy et al., 2003; Kostelec & Rockmore, 2008). By imposing
an azimuthally bandlimit N, where typically N ! L, the complexity of transforms on the rotation
group can be reduced to OPN L3q (McEwen et al., 2015a), which we exploit in our networks.
These fast algorithms to compute harmonic transforms on the sphere and rotation group can be lever-
aged to yield the exact and efficient computation of convolutions through their harmonic representa-
tions (see Appendix B). By computing convolutions in harmonic space, pixelization and quadrature
errors are avoided and computational complexity is reduced to the cost of the respective harmonic
transforms.
B	Convolution on the Sphere and Rotation Group
For completeness we make explicit the standard (non-generalized) convolution operations on the
sphere and rotation group that we adopt. The general form of convolution for signals f P L2(Ω)
either on the sphere (Ω “ S2) or rotation group (Ω “ SO(3)) is specified by Equation 1, with har-
monic representation given by Equation 3. Here we provide specific expressions for the convolution
fora variety of cases, describe the normalization constants that arise and may be absorbed into learn-
able filters, and derive the corresponding harmonic forms. In practice all convolutions are computed
in harmonic space since the computation is then exact, avoiding pixelisation or quadrature errors,
and efficient when fast algorithms to compute harmonic transforms are exploited (see Appendix A).
2http://www.spinsht.org/
3 http://www.sothree.org/
4Available on request from https://www.kagenova.com/.
13
Published as a conference paper at ICLR 2021
B.1 Convolution on the S phere
Given two spherical signals f, ψ P L2 pS2 q their convolution, which in general is a signal on the
rotation group, may be decomposed as
pf ‹ ψqpρq “ xf, Rρψy
“ f dΩ(ω)f(ω)ψ*(ρTω)
S2
“∑∑∑fmDmn(P)ψn*[ dΩ(ωqγm3汇,*3
'm'1m1 n	JS2
“ ∑∑ ∑ fm Dm*n(ρ)ψn*δ''i 2
'm '1m1 n
“ ∑ fm ψn*) Dmn(p),
mn
(16)
(17)
(18)
(19)
(20)
yielding harmonic coefficients
(f * ψqmmn = 28'21 fm ψn*.	(21)
The constants 8π2/(2' + 1) may be absorbed into learnable parameters.
B.2	Convolution on the S phere with Axisymmetric Filters
When convolving a spherical signal f P L2(S2) with an axisymmetric spherical filter ψ P L2(S2)
that is invariant to azimuthal rotations, the resultant (f ‹ ψ) may be interpreted as a signal on the
sphere. To see this note that an axisymmetric filter ψ has harmonic coefficients ψnn = ψ' δn0 that
are non-zero only for m “ 0. Denoting rotations by their zyz-Euler angles ρ “ (α, β, γ) and
substituting into Equation 20 we see that the convolution may be decomposed as
(f < ψ)(α,β,γ)= £ fmψ'*δnθ) Dmn(α,β,γ)
mn
=EfmΨ0*Dm*o(α,β, 0)
`m
“∑ fm ψ0*C 2'4'-1Ym (β,αq
0m	V 十
(22)
(23)
(24)
We may therefore interpret (f ‹ ψ) as a signal on the sphere with spherical harmonic coefficients
Pf < ψqm =" 2' 十 ι fmψ0*.	(25)
The constants y∕4π{(21 + 1) may be absorbed into learnable parameters.
14
Published as a conference paper at ICLR 2021
B.3	Convolution on the Rotation Group
Given two signals f, ψ P L2 pSOp3qq on the rotation group their convolution may then be decom-
posed as
pf ‹ ψqpρq “ xf, Rρψy	(26)
“ f	dμpρ1qfpρ1qψ*pρTρ1q	(27)
SOp3q
“ f	dμpρ1q „∑ 2''21 ∑ fmnDmn(ρ1)]∣∑ 2∣⅛1 ∑ ψmKDm，n，(PTP)
SOp3q	` 8π mn	`1	8π	m1 n1
(28)
Yɔ 2' + 1 v-ɔ ` 2' 2'1 + 1 v-ɔ ` ` * f
“W ~ιπ2~ ∑fmn∑ ιπ^ ∑, ψmιn1 Js0p3q
mn 1	m1 n1	SOp3q
2' ` 1	`	2'1 ` 1	`1 *
“W ~ιπ2~ ∑fmn∑ ιπ^ ∑, ψmιn1 Js0p3q
mn 1	m1 n1	SOp3q
dμpP1)Dmn (PI)DmJ (PTPI)
(29)
dμ(P1)Dm*n (p1) ∑ Dkm 1 PPqDJL (p1)
k
“ ∑ 28⅛1 ∑ fmn ∑ ¾1 ∑ ψm*m ∑ Dkm，(P) 28∏21 以^
`	mn	`1	m1 n1	k
“ ∑ 28∏21 DmmI pp)^∑ fmnnψm*in),
mm1	n
(30)
(31)
(32)
where for Equation 30 we make use of the relation (e.g. Marinucci & Peccati, 2011; McEwen et al.,
2018)
Dmn(PTP1) “ ∑ Dkm(P)Dkn(P1).	(33)
k
This decomposition yields harmonic coefficients
Pf * ψymn= ∑ fmmι ψnmι.	(34)
m1
C Filters on the S phere and Rotation Group
When defining filters we look to encode desirable real-space properties, such as locality and regular-
ity. However, in practice considerable computation may be saved by defining the filters in harmonic
space and saving the cost of harmonic transforming ahead of harmonic space convolutions. We de-
scribe here how filters motivated by their real space properties may be defined directly in harmonic
space.
C.1 Dirac Delta Filters on the Sphere
Spherical filters may be constructed as a weighted sum of Dirac delta functions on the sphere. This
construction is useful as the harmonic representation has an analytic form that may be computed
efficiently. Furthermore, various real space properties can be encoded through sensible placement
of the Dirac delta functions.
The spherical Dirac delta function δω1 centered at ω1 “ (θ1 , φ1) P S2 is defined as
δω1 (ω) = ~~-δR(cos θ ´ Cos θ1)δRpφ ´ φ1),	(35)
sin θ
where δR is the familiar Dirac delta function on the reals centered at 0. The Dirac delta on the sphere
may be represented in harmonic space by
(δωi)m = Ym*(ω1 ) = Nm Pm Pcos θ')e´im”,	(36)
15
Published as a conference paper at ICLR 2021
which follows form the sifting property of the Dirac delta, and where Ym denote the spherical
harmonic functions, Pm (x) are associated Legendre functions and
N` = d2' + 1 (l´ m)!	(37)
Nm “ ∖∣ 4π (l + m)!	(37)
is a normalizing constant.
This representation may then be used to define a filter ψ P L2 (S2q as a weighted sum of spher-
ical Dirac delta functions, with weights wij assigned to Dirac delta functions centered at points
t(θi, φjq : i “ 1, ..., Nθ; j “ 1, ..., Nφu. The associated harmonic space representation is given by
ψm “ ∑ Wij Nm Pm (Cos θMm	(38)
i,j
“ ∑ NmPm(cos θi) ∑ Wije-mφj,	(39)
ij
where fast Fourier transforms may be leveraged to compute the inner sum if the Dirac deltas are
spaced evenly azimuthally (e.g. if φj “ 2πj{Nφ). Alternative arbitrary samplings can of course be
considered if useful for a problem at hand.
When defining filters in this manner one should be careful not to over-parametrize by assigning
more weights than needed to define a filter at the harmonic bandlimit of the signal with which we
wish to convolve. For example, if the filter is to be convolved with a signal bandlimited at L then
a maximum of 2L ´ 1 Dirac deltas should be placed along each ring of constant θ. One may also
choose to interpolate the weights from a smaller number of learnable parameters acting as anchor
points, allowing higher resolution filters to be defined with fewer learnable parameters.
C.2 Dirac Delta Filters on the Rotation Group
Similarly a Dirac delta function δρ1 on the rotation group SO(3q centered at position
ρ1 “ (α1, β1, γ1q P SO(3q is defined as
δρ1 (P) “ SineδR(α ´ α1)δR(cosβ ´ cosβ1qδR(γ ´ γ1q,
(40)
with harmonic form
(δρi)mn = Dmn(ρ1) “ eTmα'dmn(β1)eTnγ1,	(41)
where d`mn are Wigner (small) d-matrices.
The filter ψ P L2 (SO(3)) corresponding to a weighted sum of Dirac deltas with weights Wijk as-
signed to Dirac delta functions centered at points t(αi, βj, γk) : i “ 1, ..., Nα; j “ 1, ..., Nβ; k “
1, ..., Nγ u has harmonic form
ψmn “ ∑ Wijk e´ig,dmn(βi)eTrnY	(42)
i,j,k
“ ∑ dmn(βi) ∑ e-imaj ∑ WijkefY,	(43)
where again fast Fourier transforms may be leveraged to compute the inner two sums assuming the
Dirac deltas are spaced evenly in α and γ. The outer sums of Equation 39 and Equation 43 can also
be computed by fast Fourier transforms by decomposing the Wigner d-matrices into their Fourier
representation (cf. Trapani & Navaza, 2006; McEwen & Wiaux, 2011). One should again be careful
not to over-parametrize.
D Equivariance Tests
To test rotational equivariance of operators we consider Nf “ 100 random signals tfiuiN“f1 in
L2(Ω1) with harmonic coefficients sampled from the standard normal distribution and NP “ 100
16
Published as a conference paper at ICLR 2021
random rotations {ρjUN“Pi sampled uniformly on SO(3). In order to measure the extent to which an
operator A : L2(Ωι) → L2(Ω2) is equivariant we evaluate the mean relative error
dpApR f q R (Af.qq “ ɪɪ Nf NP }A(Rρjfiq ´ RPj(Afiqq}
d(A(Rpj fiq, RPj pAfiqq “ Nf NPN j“1	}A(RPjfi)}(44)
resulting from pre-rotation of the signal, followed by application of A, as opposed to post-rotation
after application of A, where the operator norm } ∙ } is defined using the inner product〈∙, ∙'yuPΩ2q.
Table 4 presents the mean relative equivariance errors computed. We consider the three standard
convolutions described in Appendix B (with a random filter ψi for each signal fi, generated in
the same manner as fi), the pointwise ReLU activation described in Section 2.5.1 for signals on
the sphere (Ωι “ S2) and rotation group (Ωι “ SO(3)), and the composition of tensor-product
activation with a generalized convolution, described in Sections 2.5.2 and 2.4, respectively. We
follow the tensor-product activation with a generalized convolution in order to project down onto
the sphere to allow the same notion of error to be adopted as for the other operators. For consistency
with the context in which we leverage these operators, all experiments are performed using single-
precision arithmetic.
We see that all three standard notions of convolution and the composition of the tensor-product
activation and generalized convolution are all strictly equivariant to floating point machine precision,
with errors on the order of lθ´7. The pointwise ReLU operator is not strictly equivariant, with a
mean relative error of 0.37 for signals on the rotation group and 0.34 for signals on the sphere. These
errors reduce when the signals are oversampled before application of the ReLU, indicating that the
error is due to aliasing induced by the spreading of information to higher degrees not captured
at the original bandlimit. For example, for the pointwise ReLU operator on the rotation group
oversampling by factors of 2 ^, 4 ^ and 8 X results in a reduction in the mean relative equivariance
error from 0.37 to 0.098, 0.032 and 0.0096, respectively.
Table 4: Layer equivariance tests
Layer	Mean Relative Error
S2 to S2 conv.	4.4 X	lθ´7
S2 to SO(3q conv.	5.3 X	10—7
SO(3q to SO(3q conv.	9.3 X	lθ´7
Tensor-product activation → Generalized conv.	5.0 X	10-7
S2 ReLU	~~34^	lθ´1
S2 ReLU (2 X oversampling)	8.9 X	10-2
S2 ReLU (4 X oversampling)	2.9 X	10—2
S2 ReLU (8 X oversampling)	1.3 X	10-2
SO(3q ReLU	~~37^	lθ´1
Sθ(3q ReLU (2^ oversampling)	9.8 X	10—2
Sθ(3q ReLU (4^ oversampling)	3.2 X	10-2
Sθ(3q ReLU (8^ oversampling)	9.6 X	10-3
E Connection B etween the Tensor Product Activation and
Pointwise Squaring
To provide some intuition on the manner in which the tensor-product based activation introduces
non-linearity into representations we describe its relationship to pointwise squaring for signals on
the sphere. Here we consider the operator N : L2(S2q → L2(S2q satisfying (N fq(xq “ f2 (xq
for all x P S2, which differs subtly to Nσ with σ(xq “ x2 (using notation from Section 2.5.1),
which corresponds to obtaining a sample-based representation at a finite bandlimit L and applying
the squaring at the sample positions. For the special case σ(xq “ x2 we can directly compute the
harmonic representation corresponding to the equivariance-preserving continuous limit L → 8.
17
Published as a conference paper at ICLR 2021
4 2 0 a> 6
P P U* O O
Wwwll
SdOK
l×l×l×
Ooo
W 5
uo-np
(a) Computational cost
5 3 11
Ooo-
Ill1
3 Ox
c
£ 20x
υ
5 IOx
CC
Ox
O 20	40	60	80 IOO 120
Bandlimit L
(b) Memory requirements
Figure 3: Comparison of computation and memory footprints between the generalized spherical
CNN layers of Kondor et al. (2018) and our efficient generalized layers. The reduction in cost due
to our efficient layers is given as multiplicative factors in the lower plot of each panel.
Given a spherical signal f P L2(S2) with generalized representation f “ {f' P C2''1 : ' “
0, .., L ´ 1u, the generalized representation of the signal f2 P L2pS2q is given as
f2 “t ∑	(G'1,'2,'qj(f'1 b f02q : ' = 0,1,...,L ´ 1U,	(45)
p`l ,22)PP',L
where G'1 ,'2,' P Cp2'1'ιqχp2'2'ιqχp2''1q are Gaunt coefficients defined as
Gmjm2m3 “	dμ(ω)Y⅛ (ω)Ym22(ω)Ym33*(ω).	(46)
S2
Gaunt coefficients are related to the Clebsch-Gordan coefficients by
Gj1 j2 j3	wj1j2j3 Cj1j2j3	(47)
m1 m2 m3 “ w	m1 m2 m3 ,
where wj1j2j3 =(´l)m3 √jgjp1 C0100j3.
Therefore, the continuous squaring operation
corresponds to passing f through a tensor-product activation Nb followed by a generalized con-
volution back down onto the sphere (single fragment per degree) with weight assigned to the
('1,'2) P P'L fragment in degree-` given by wl1l2l3 .
This demonstrates that activations that are learnable within our framework can have very simple real-
space interpretations. Even when confining outputs to the sphere we found it to be beneficial to allow
the down-projection to be learnable rather than enforcing the weights given above for pointwise
squaring. Learned activations will remain quadratic, however, given that output fragments are linear
combinations of products between input fragments.
F Comparison of Computational and Memory Footprints
We perform a quantitative analysis of the computational cost and memory requirements of our pro-
posed layers, and comparisons to prior approaches, to demonstrate how the complexity savings made
through our proposals in Section 3 translate to tangible efficiency improvements.
We consider the simplest comparison, between a multi-channel generalized signal f “
(f1, ..., fKq P FLK where each channel fi has type τfi “ (1, ..., 1q (and therefore corresponds to a
signal on the sphere) and a uni-channel signal g P FL of type τg “ (K, ..., Kq. The setting corre-
sponding to signal f captures the efficiency improvements of our proposed layers, while the setting
corresponding to g represents the case without these improvements. Notice that the total number of
fragments is the same in both f and g. We compare the number of floating point operations (FLOPs)
and amount of memory required to perform a tensor-product based activation followed by a general-
ized convolution projecting back down onto a signal of the same type as the input. When applied to
f, MST mixing sets PL (Section 3.1.3) and constrained generalized convolutions (Section 3.1.2) are
18
Published as a conference paper at ICLR 2021
S2 Conv.
ReLU
S2 Layer
SO(3)
Layer
Efficient
Efficient
Gen. Layer	Gen. Layer
Figure 4: Visualization of the architecture used for the convolutional base in our hybrid models. The
input to the first convolutional layer is a signal on the sphere. The output from the final convolutional
layer are scalar values corresponding to fragments of degree ` “ 0, which are then mapped through
some fully connected layers to give the model output.
Constrained
Gen. Conv.
Tensor
Products
Constrained
Gen. Conv.
Tensor
Products
Constrained
Gen. Conv.
Efficient
Gen. Layer
used. When applied to g full mixing sets PLand unconstrained generalized convolutions are used,
as in Kondor et al. (2018). Considered are the costs for a single training instance (batch size of 1).
Figure 3 shows the computational costs and memory requirements, in terms of floating point op-
erations and megabytes respectively, for K “ 4 and various spatial bandlimits L. We adopt the
convention whereby complex addition and multiplication require 2 and 6 floating point operations
respectively. At low bandlimits we see the saving arising from the channel-wise structure. Note
that the saving illustrated here is relatively small since K “ 4 for these experiments (so that the L
scaling is apparent), whereas in practice typically K „ 100. The saving then increases linearly in
response to increases in the bandlimit of the input, as expected given the OpL5q and OpL4q spa-
tial complexities. We see that even in this simple case, with a relatively small number of channels
(K “ 4), both the computational and memory footprints are reduced by orders of magnitude. At a
bandlimit of L “ 128 the computational cost is 101-times reduced and the memory requirement is
29-times reduced.
G	Additional Information on Experiments
G. 1 Rotated MNIST on the Sphere
For our MNIST experiments we used a hybrid model with the architecture shown in Figure 4. The
first block includes a directional convolution on the sphere that lifts the spherical input (τf(。)“ 1)
onto the rotation group (TfpIq “ min(2' ' 1, 2Nl — 1)). The second block includes a convolution
on the rotation group, hence its input and output both live on the rotation group. We then apply a
restricted generalized convolution to map to type τf(3)“「丁皿明/√2' ` 11, where Tmax=5. The same
type is used for the following three channel-wise tensor-product activations and two restricted gen-
eralized convolutions until the final restricted generalized convolution maps down to a rotationally
invariant representation (Tfp5)= δ'0). As is traditional in convolution networks We gradually de-
crease the resolution, with pL0, L1, L2, L3, L4, L5) = p20, 10, 10, 6, 3, 1), and increase the number
of channels, with pK0, K1, K2, K3, K4, K5) = p1, 20, 22, 24, 26, 28). We proceed these convolu-
tional layers with a single dense layer of size 30, sandwiched between two dropout layers (keep
probability 0.5), and then fully connect to the output of size 10.
We train the network for 50 epochs on batches of size 32, using the Adam optimizer (Kingma & Ba,
2015) with a decaying learning rate starting at 0.001. For the restricted generalized convolutions
we follow the approach of Kondor et al. (2018) by using L1 regularization (regularization strength
10-5) and applying a restricted batch normalization across fragments, where the fragments are only
scaled by their average and not translated (to preserve equivariance).
19
Published as a conference paper at ICLR 2021
G.2 Atomization Energy Prediction
When regressing the atomization energy of molecules there are two inputs to the model: the number
of atoms of each element contained in the molecule; and spherical cross-sections of the potential
energy around each atom. We adopt the high-level QM7-specific architecture of Cohen et al. (2018)
which contains a spherical CNN as a sub-model, for which we substitute our own. This results in an
overall model that is invariant to both rotations of the molecule around each constituent atom and to
permutations of the ordering of the atoms.
The first (non-spherical) input is mapped onto a scalar output using a multi-layer perceptron (MLP)
with three hidden layers of sizes 100, 100 and 10 (and ReLU activations). The second input, multiple
spherical cross sections for each atom, are separately projected using a shared spherical CNN (of
architecture described below) onto lower dimensional vectors of size 64. The mean vector is then
taken across atoms (ensuring invariance w.r.t. permutations of the atoms) and mapped onto a scalar
output using an MLP with a single hidden layer of size 512 (with a ReLU activation). The predicted
energy is then taken to be the sum of the two scalar outputs.
As a starting point we train the first MLP to regress the atomization energies alone (achieving RMS
„20), before pairing it with the spherical model (and its connected MLP). We then train the joint
model for 60 epochs, again with the Adam optimizer, a decaying learning rate (starting at 2.5 X
10—4), regularizing the efficient generalized layers with L? regularization (strength 2.5 X lθ´6) and
batch sizes of 32.
For the spherical component we again adopt the convolutional architecture shown in
Figure 4 except with one fewer efficient generalized layer. We use bandlimits of
pL0, L1 , L2, L3, L4q“p10, 6, 6, 3, 1q, channels of pK0, K1, K2 , K3, K4q“p5, 16, 24, 32, 40q and
τmax “ 6. One minor difference is that this time we include a skip connection between the ` “ 0
components of the fourth and fifth layer. We proceed the convolutional layers with two dense layers
of size p256, 64q and use batch normalization between each layer.
G.3 3D Shape Retrieval
To project the 3D meshes of the SHREC’17 data onto spherical representations (bandlimited at
L “ 128) we adopt the preprocessing approach of Cohen et al. (2018) and augment the data with
random rotations and translations.
We construct a model with an architecture that is again similar to that described in Appendix G.1
but with an additional axisymmetric convolutional layer prepended to the start of the net-
work and one fewer efficient generalized layers. We use bandlimits pL0, L1, L2 , L3, L4, L5q “
p128, 32, 16, 16, 6, 1q, channels pK0,K1,K2,K3,K4,K5q “ p6, 20, 30, 40, 60, 70q and τmax “ 6
for the efficient generalized layers. The convolutional layers are followed by a dense layer of size
128 which is fully connected to the output (of size 55).
We again train with the Adam optimizer, a decaying learning rate (starting at 5 X 10 ´4) and batch
sizes of 8, this time until performance on the validation set showed no improvement for at least
4 epochs (36 epochs in total). We perform batch normalization between convolutional layers and
dropout preceding the dense layer. We regularize the efficient generalized layers with L2 regu-
larization (strength 10 ´5). When testing our model We average the output probabilities over 15
augmentations of the data.
20