Published as a conference paper at ICLR 2021
Early Stopping in Deep Networks: Double De-
scent and How to Eliminate it
Reinhard Heckel'" & Fatih Furkan Yilmaz*
±Dept. of Electrical and Computer Engineering, Technical University of Munich
* Dept. of Electrical and Computer Engineering, Rice University
Ab stract
Over-parameterized models, such as large deep networks, often exhibit a double
descent phenomenon, where as a function of model size, error first decreases, in-
creases, and decreases at last. This intriguing double descent behavior also occurs
as a function of training epochs and has been conjectured to arise because training
epochs control the model complexity. In this paper, we show that such epoch-wise
double descent occurs for a different reason: It is caused by a superposition of two
or more bias-variance tradeoffs that arise because different parts of the network are
learned at different epochs, and mitigating this by proper scaling of stepsizes can
significantly improve the early stopping performance. We show this analytically
for i) linear regression, where differently scaled features give rise to a superposi-
tion of bias-variance tradeoffs, and for ii) a wide two-layer neural network, where
the first and second layers govern bias-variance tradeoffs. Inspired by this theory,
we study two standard convolutional networks empirically and show that elimi-
nating epoch-wise double descent through adjusting stepsizes of different layers
improves the early stopping performance.
1	Introduction
Most machine learning algorithms learn a function that predicts a label from features. This function
lies in a hypothesis class, such as a neural networks parameterized by its weights. Learning amounts
to fitting the parameters of the function by minimizing an empirical risk over the training examples.
The goal is to learn a function that performs well on new examples, which are assumed to come
from the same distribution as the training examples.
Classical machine learning theory says that the test error or risk as a function of the size of the
hypothesis class is U-shaped: a small hypothesis class is not sufficiently expressive to have small
error, and a large one leads to overfitting to spurious patterns in the data. The superposition of those
two sources of errors, typically referred to as bias and variance, yields the classical U-shaped curve.
However, increasing the model size beyond the number of training examples can decrease the error
again. This phenomena, dubbed “double descent” by Belkin et al. (2019) has been observed as early
as 1995 by Opper (1995), and is relevant today because most modern machine learning models, in
particular deep neural networks, operate in the over-parameterized regime, where the error often de-
creases again as a function of model size, and where the model is sufficiently expressive to describe
any data, even noise.
Interestingly, this double descent behavior also occurs as a function of training time, as observed
by Nakkiran et al. (2020a) and as illustrated in Figure 1. The left panel of Figure 1 shows that as
a function of training epochs, the test error first decreases, increases, and then decreases again. It
is important to understand this so-called epoch-wise double descent behavior to determine the early
stopping time that gives the best performance. Early stopping, or other regularization techniques,
are critical for learning from noisy labels (Arpit et al., 2017; Yilmaz & Heckel, 2020).
Nakkiran et al. (2020a) conjectured that epoch-wise double descent occurs because the training time
controls the “effective model complexity”. This conjecture is intuitive, because the model-size, and
thus the size of the hypothesis class, can be controlled by regularizing the empirical risk via early
stopping the gradient descent iterations, as formalized in the under-parameterized regime by Yao
1
Published as a conference paper at ICLR 2021
best early stopping time
epoch
Figure 1: Left: The test and train error curves of an over-parameterized 5-layer convolutional net-
work trained on the CIFAR-10 training set with 20% random label noise. As observed by Nakkiran
et al. (2020a), the performance shows a double descent behavior. Right: As we show here, the risk
of a regression problem can be decomposed as the sum of two bias-variance tradeoffs. Both exam-
ples: Early stopping the training where the test error achieves its minima is critical for performance.
et al. (2007); RaskUtti et al. (2014); Buhlmann & Yu (2003). Specifically, limiting the number
of gradient descent iterations ensures that the functions parameters lie in a ball around the initial
parameters. While this conjecture might be true for certain problem setups, it is not consistent with
our empirical observation for the 5-layer CNN studied by Nakkiran et al. (2020a): Specifically, the
empirically measured overall bias in Figure 1 is increasing for some iterations, whereas an increasing
model size would imply that it is decreasing (see Appendix B.2 for details on this experiment).
In this paper, we show empirically and theoretically that epoch-wise double descent—at least in
the setups we observed it— arises for a different reason: It is explained by a superposition of bias-
variance tradeoffs, as illustrated for a toy-regression example in the right panel of Figure 1. If
the risk can be decomposed into two U-shaped bias-variance tradeoffs with minima at different
epochs/iterations, then the overall risk/test error has a double descent behavior.
We also note that epoch-wise double descent is not a phenomena tied to over-parameterization. Both
under- and overparameterized models can have epoch-wise double descent as we show in this paper.
1.1	Contributions
The goal of this paper is to understand the epoch-wise double descent behavior. Our main finding is
that epoch-wise double descent can be explained as a superposition of bias variance tradeoffs, and
arises naturally in some standard neural networks because parts of the network are learned faster
than others. Our contributions are as follows:
First, we consider a linear regression model and theoretically characterize the risk of early stopped
least squares. We show that if features have different scales, then the early stopped least squares
estimate as a function of the early stopping time is a superposition of bias-variance tradeoffs, which
yields a double descent like curve (see Figure 1, right panel).
Second, we characterize the early stopped risk of a two-layer neural network theoretically and show
that it is upper bounded by a curve consisting of over-lapping bias-variance tradeoffs that are gov-
erned by the initializations and stepsizes of the two layers. The initialization scales and stepsizes of
the weights in the first and second layer determine whether double descent occurs or not. We provide
numerical examples showing how epoch-wise double descent occurs when training such a two-layer
network on data, and how it can be eliminated by scaling the stepsizes of the layers accordingly.
Third, we study a standard 5-layer convolutional network as well as ResNet-18 empirically. For the
5-layer convolutional network we find—similarly as for the two-layer model—epoch-wise double
descent occurs because the convolutional layers (representation layers) are learned faster than the
final, fully connected layer.Similarly, for ResNet-18, we find that later layers are learned faster than
early layers, which again results in double descent. In both cases, epoch-wise double descent can be
eliminated through adjusting the stepsizes of different coefficients or layers.
In summary, we provide new examples on when epoch-wise double descent occurs, as well as ana-
lytical results explaining epoch-wise double descent theoretically. Our theory is constructive in that
it suggests a simple and effective mitigation strategy: scaling stepsizes appropriately. We also note
that epoch-wise double descent should be eliminated by adjusting the stepsizes and/or the initializa-
tion, because this often translates to better overall performance.
2
Published as a conference paper at ICLR 2021
1.2	Related works
There is a large number of works that have studied early stopping theoretically. Intuitively, each step
of an iterative algorithm reduces the bias but increases variance. Thus early stopping can ensure that
neither bias nor variance are too large. A variety of papers Yao et al. (2007); Raskutti et al. (2014);
Buhlmann & Yu (2003); Wei et al. (2019) formalized this intuition and developed theoretically sound
early stopping rules. Those works do not, however, predict when a double descent curve can occur.
A second, more recent line of works, studies early stopping from a different perspective, namely that
of gradient descent fitting different components ofa signal or different labels at different speeds. For
linear least squares, the data in the direction of singular vectors associated with large singular values
is fitted faster than that in the direction of singular vectors associated with small singular values.
Advani et al. (2020) have shown this for a linear least squares problem or stated differently, a linear
neural network with a single layer. Li et al. (2020); Arora et al. (2019) have shown that this view
explains why neural network often fit clean labels before noisy ones, and Heckel & Soltanolkotabi
(2020b) have used this view to prove that convolutional neural networks provably denoise images.
Our theoretical results for neural networks build on a line of works that relate the dynamics of
gradient descent to those of an associated linear model or a kernel method in the highly overparam-
eterized regime Jacot et al. (2018); Lee et al. (2018); Arora et al. (2019); Du et al. (2018); Oymak
& Soltanolkotabi (2020); Oymak et al. (2019); Heckel & Soltanolkotabi (2020b). We use the same
proof strategy as those papers to characterize the early stopping performance of a simple two-layer
neural network, but in contrast to those earlier works, we develop early stopping results and optimize
over both the weights in the first and second layer, as opposed to only optimizing over the weights
in the first layer. That is important, because we want to demonstrate that initialization and stepsize
choices of different layers lead to different bias-variance tradeoffs.
Next, we note that there is an emerging line of works that theoretically establishes double descent
behavior as a function of the model complexity (e.g., measured by the number of parameters) for
linear regression Hastie et al. (2019); Belkin et al. (2020), for random feature regression Mei &
Montanari (2019); d’Ascoli et al. (2020), and for binary linear regression Deng et al. (2020).
A number of recent theoretical double-descent works Jacot et al. (2020); Yang et al. (2020); d’Ascoli
et al. (2020) have decomposed the risk into bias and variance terms, and studied their behavior.
Those works demonstrate that the bias typically decreases as a function of the model size, and the
variance first increases, and then decreases, which can yield a double-descent behavior. As we
demonstrate in Appendix B.2, the epoch-wise double descent phenomena for the standard CNN
studied by Nakkiran et al. (2020a) cannot be explained with this observation: The variance is in-
creasing as a function of training epochs, as opposed to being unimodal.
Finally, our suggestion to mitigate epoch-wise double descent with step-size adaption and early stop-
ping is a form of regularization. Related work for model-wise double descent shows that model-wise
double descent can be mitigated with ('2) regularization Nakkiran et al. (2020b), and '2 regulariza-
tion and early stopping are strongly related (Ali et al., 2019).
2	Early- s topped gradient descent for linear least squares
We start by studying the risk of early stopped gradient descent for fitting a linear model to data
generated by a Gaussian linear model. Our main finding is that the risk as a function of the early
stopping time is characterized by a superposition of U-shaped bias-variance tradeoffs, and if the
features of the Gaussian linear model have different scales, those bias-variance tradeoff curves add
up to a double descent shaped risk curve. We also show that the performance of the estimator can be
improved through double descent elimination by scaling the stepsizes associated with the features.
2.1	Data model and risk
Consider a regression problem, and suppose data is generated from a Gaussian linear model as
y = hx, θ*i + z, where X ∈ Rd is a zero-mean Gaussian feature vector with diagonal co-variance
matrix Σ = diag(σ12 , . . . , σd2), and z is independent, zero-mean Gaussian noise with variance σ2 .
We are given a training set D = {(X1, y1), . . . , (Xn, yn)} consisting ofn data points drawn iid from
this Gaussian linear model. We consider the class of linear estimators parameterized by a vector
3
Published as a conference paper at ICLR 2021
θ ∈ Rd, which We estimate based on the training data D. The linear estimator predicts the label
associated with a feature vector X as y = XTθ. The (mean-squared) risk of this estimator is
R(θ) = E (y - XTθ)2 ,
where expectation is over an example (X, y ) drawn independently (of the training set) from the
underlying linear model.
2.2	Early-stopped least squares estimate
We consider the estimate based on early stopping gradient descent applied to the empirical risk
1n
R⑹=—X3 - xTθ).
n i=1
We initialize gradient descent with θ0 = 0 and iterate, for t = 1, 2, . . ., with updates θt+1 =
θt 一 1 diag(η)VJR(θt), where diag(η) is a diagonal matrix containing the stepsizes η > 0 associated
with each of the features as entries. Note that we allow for different stepsizes for all of the features.
In the following, we study the properties of the iterates t, i.e., θt .
2.3	Risk of early stopped least squares
The main result of this section is that in the underparameterized regime, where d n, the risk of
gradient descent after t iterations, R(θt), is very close to a risk expression defined as
d2
R(Ot)= σ2 + X σ2 (θi )2(1 - ηiσ2)2t + (I-(I- ηiσ2 )t)2,	(I)
n
i=1 ×---------------{z-----------------}
Ui(t)
as formalized by the theorem below. We focus on the underparameterized regime here, because in
the over-parameterized regime our estimator cannot achieve small risk in general. In Section 3 we
study a more general setting in the overparameterized regime.
Theorem 1. Suppose that the stepsizes obey η ≤ σ2 ,for all i = 1,...,d. With probability at least
1 - 2d-5 - 2de-n/8 - e-d - 2e-32 over the random training set generated by a linear Gaussian
model with parameters θ* and Σ, the difference of the early stopped risk and the risk expression
in (1) at iteration t is at most
∖R(θt)- R ⑻ )1 ≤C (m≡⅛4 n Qwk2+dσ2 log(d))+σ2 √d).	⑵
Here, c is a numerical constant.
Theorem 1 guarantees that with high probability the risk R(θt) is well approximated by the risk
___________	∙-v
expression R(θt), provided the model is sufficiently underparameterized (i.e., d/n is small).
As a consequence, the risk of early stopped least-squares is a superposition of U-shaped bias-
variance tradeoffs, and if the features are differently scaled, this can give rise to epoch-wise double
descent. To see this, first note that the terms Ui(t) in the risk expression (1) are U-shaped as a func-
tion of the early stopping time t, because σ2 (θ7)2 (1 -5σ2)21 decreases in t and σ2 (1 - (1 - ηiσ2)t)2
increases in t; see Figure 2a for an example. The minima of the individual U-shaped curves Ui (t)
depend on the product of the stepsize and the i-th features’ variance, ηiσi2 ; the larger this product,
the earlier (as a function of the number of iterations, t) the respective U-shaped curve reaches its
minimum. Therefore, if we add up two (or more) such U-shaped curves with minima at different
iterations, the resulting risk curve can have a double descent shape (again, see Figure 2a). This
establishes our claim that differently scaled features can give rise to epoch-wise double descent.
Finally we note that the reason why we refer to the U-shaped curves as bias-variance tradeoffs, is
that the terms Pd=ι σ2(θ*)2(1 - ηiσ2)2t and Pd=I σ2(1 - (1 - ηiσ2)t)2 in the risk expression (1)
are approximately equal to the bias and the variance of the model θt in the standard textbook bias-
variance decomposition of the risk, see Appendix A.2 for a detailed discussion.
4
Published as a conference paper at ICLR 2021
a) constant stepsize	b) elimination with diff. stepsizes c) before & after elimination
Figure 2: Early stopped least squares risk for a two-feature Gaussian linear model. a: Two U-
shaped bias-variance tradeoffs Ui(t) for the parameters θɪ = 1.5,σι = 1,ηι = 0.05 (bias-variance
1) and θ2 = 10,σ2 = 0.15,η2 = 0.05 (bias-variance 2), along with their sum (1+2) which deter-
mines the risk. b: Same plot, but this time the bias-variance tradeoff U2 (t) is shifted to the left by
increasing the stepsize η2 according to Proposition 1 (yielding bias-variance tradeoff 3), so that its
minimum overlaps with that of bias-variance tradeoff 1. This eliminates double descent and gives
better performance. c: The resulting risk curves before and after elimination, demonstrating that the
minimum of the risk after double descent elimination is smaller than before elimination.
Improving performance by eliminating double descent: Epoch-wise double descent can be
eliminated by properly scaling the stepsizes associated with each of the features, so that the minima
of the individual bias-variance tradeoffs overlap at the same iteration t:
Proposition 1. Pick an optimal early stopping time t ≥ 1. The minimum of the risk expression
mi□ηι,…,ηd mint R(θt) is achieved at iteration i by choosing the stepsizes pertaining to thefeatures
as ηi = σ2 (1 — ( σ2(θσ)2+σ2∕n) ).
Elimination of double descent is illustrated in Figure 2b. By eliminating double descent optimally so
that all the individual bias-variance tradeoffs Ui(t) achieve their minima at the same early stopping
point t, we achieve the lowest overall risk at the optimal early stopping point. Thus eliminating dou-
ble descent is important for optimal performance. In practice we typically do not know the variances
of the features and therefore may not be able to optimally choose the stepsizes. However, we may
be able to mitigate double descent sub-optimally by treating the stepsizes as hyperparameters.
3 Early stopping in two layer neural networks
In this section, we establish a bound on the risk of a two-layer neural network and show that this
bound can be interpreted as a super-position of U-shaped bias-variance tradeoffs, similar to the
expression governing the risk of the linear model from the previous section. The risk of the two-
layer network is governed by two associated kernels pertaining to the first and second layer, and
the initialization scale and stepsizes of the weights in the first and second layer determine whether
double descent occurs or not. We also show in an experiment that if double descent occurs, it can be
eliminated by adapting the stepsizes of the two layers.
Network model: We consider a two-layer neural network with ReLU activation functions and
k neurons in the hidden layer: fw,v(x) = √relu(xTW)v.. Here, X ∈ Rd is the input of the
network, W ∈ Rd×k and v ∈ Rk are the weights of the first and second layer. Moreover, relu(z) =
max(z, 0) is the rectified linear unit, applied elementwise.
Data model: We assume that we are given a training setD = {(x1, y1), . . . , (xn, yn)} with exam-
ples (xi, yi) drawn iid from some joint distribution. For convenience, we assume that the datapoints
are normalized, i.e., kxik2 = 1, and the labels are bounded, i.e., |yi| ≤ 1.
5
Published as a conference paper at ICLR 2021
Training with early stopped gradient descent: We train the network with early stopped and
randomly initialized gradient descent on a quadratic loss. We choose the weights at initialization as
[W0]i,j 〜N(0, ω2),	[v0]i 〜Uniform({-ν, ν}).	(3)
Here, ω and ν are parameters that trade off the magnitude of the weights of the first and second
layer. Note that with this initialization, for a fixed unit norm feature vector x, we have fW0,v0 (x) =
O(νω). We apply gradient descent to the mean-squared loss
1n
L(W, V) = 2^3 - fW,v(Xi))2.
i=1
The gradient descent updates are vt+ι = Vt-ηVvL(Wt, Vt) and Wt+ι = WLNWLNt, Vt),
where η is a constant learning rate. We study the risk of the network as a function of the iterations t.
Evaluation and performance metric: Our goal is to bound the test error as a function of the
iterations of gradient descent. Let ` : R × R → [0, 1] be a loss function that is 1-Lipschitz in its
first argument and obeys '(y, y) = 0;a concrete example is the loss '(z, y) = |z - y| for arguments
z, y ∈ [0, 1]. The test error or risk is defined, as before, as R(f) = E [`(f (X), y)] , where expectation
is over examples (X, y) drawn from the unknown joint distribution from which the training set is
drawn as well.
3.1	Risk of early stopped neural network training
Our main result is a bound on the test error of the two layer neural network trained for t iterations,
in the regime where the network is very wide. The result depends on the Gram matrix Σ ∈ Rn×n
determined by two kernels associated with the first and second layer of the network. The (i, j)-th
entry of the Gram matrix as a function of the training examples is defined as
Σij = ν2K1(Xi,Xj) + ω2K2(Xi, Xj),
(4)
with kernels
KI(Xi xj) = 1 (1 -	Cos	π(Pij))	Pij,	and	K2(χi,	Xj) =	KI(Xi, xj) +	J1	-	P2j/(2n),
where Pij = hXi, Xji (recall that we assume kXik2 = 1, for all i). Our result depends on the singular
values and vectors of this Gram matrix: Σ = Pin=1 σi2uiuiT . We are now ready to state our result.
Theorem 2. Let α > 0 be the smallest eigenvalue of the Gram matrix Σ, suppose that the network
is sufficiently wide, i.e., k ≥ Ω (仪^ /：(> ω))，and suppose the initialization scale parameters obey
νω ≤ α/,32log(2n∕δ) and V + ω ≤ 1 for some δ ∈ (0,1). Then, with probability at least 1 - δ,
the risk of the network trained with gradient descent for t iterations is at most
R(fWt,vt ) ≤ ∖ X X hui, yi2 (1 - ησ2)2t + ∖ - X hui, yi2 —~~Q Jσi ) + O(√= ).
Nn 匕	Nn W	σ2	√n
(5)
Regarding the assumptions of the theorem, we remark that while the exponent of n and α in the
width-condition (k ≥ Ω (仪^ mi10(νω))) Can be improved, the width condition ensures that the
network is sufficiently wide so that the network operates in the kernel regime where the network
behaves similar to an associated linear model. Regarding the assumption that the smallest eigenvalue
of the Gram matrix obeys α > 0, Theorem 3.1 by Du et al. (2019) shows that if no two Xi, Xj are
parallel, then α > 0, for a very related Gram matrix (specifically, the Gram matrix only consisting
of the kernel K1 defined above). As argued in that work, for most real-world datasets no two inputs
are parallel, therefore this assumption is rather mild.
The risk bound established by Theorem 2 can be interpreted as a superposition of n-many U-shaped
bias variance tradeoffs, similar to the expression (1) governing the risk of early stopped linear least
6
Published as a conference paper at ICLR 2021
squares. Specifically, the i-th “bias” term hui, yi2 (1 - ησi2)2t decreases in the number of gradient
descent iterations t, while the i-th “variance” termhUi, y)2 (1-(=σ2 ) increases in the number
of gradient descent iterations. The speed at which the two terms increase and decrease, respectively,
is determined by the singular value σi2. Those singular values, in turn, are determined by the kernels
K1 and K2, the random initialization (in particular the scale parameters ν, ω), and the distribution of
the examples. Whether epoch-wise double descent occurs or not depends on those singular values
and therefore on the kernels, the initialization, and the distribution of the examples, as illustrated
with the following numerical example.
Numerical example to illustrate the theorem: We draw data from the linear model specified in
Section 2.1 with geometrically decaying diagonal co-variance entries and zero additive noise. We
then train the network for different initialization scale parameters ω, ν once with the same stepsize
for both layers (η = 8e-5), and once with a smaller stepsize for the second layer, i.e., ηW = 8e-5
and ηv = 1e-6. In the top row of Figure 3, it can be seen that the empirical risk has a double-descent
behavior if both layers are initialized at the same scale (i.e., ω = ν = 1).
To understand the relation to the theorem better, we also plot in Figure 3 the extent to which the
singular values are associated with the parameters in the first and second layer. To capture this, we
first comment on the relation of the parameters of the first and second layer to the singular values
σi2 and vectors ui2 of the Gram matrix: In the wide-network regime in which the theorem applies,
the networks output is well approximated by its linearization around around the initialization. With
this, the networks predictions for the training examples are approximately
"	,.v. .	# ≈ J Vectv(W) = X σiui(viT,WVect(W) +viT,vv),
fW,v(xn)	i=1
(6)
where J ∈ Rn×dk+k is (approximately) the Jacobian of the network at initialization and J =
Pin=1 σiuiviT is its singular value decomposition. Here, we denote by vi,W ∈ Rdk and vi,v ∈ Rk
the parts of the right-singular vectors of the Jacobian associated with the weights in the first and
second layer, respectively. The norm of those vectors measures to what extent the singular value σi
is associated with the weights in the first and second layer.
Returning to the numerical example, as the bottom row of Figure 3 shows, ifwe initialize both layers
at the same scale (ω = ν = 1), then most of the large singular values are associated, for the most
part, with the weights in the second layer. This leads to double descent, that can be mitigated by
choosing a smaller stepsize associated with the weights in the second layer.
Improving performance by eliminating double descent: Similarly as for the linear least squares
problem studied in the previous section, it is possible to shape the bias variance tradeoffs by adapting
the stepsizes (or through initialization of the layers). In Figure 3, we illustrate this behavior: Double
descent is eliminated by choosing a smaller stepsize for the second layer, or by choosing a smaller
initialization for the first layer, as suggested by our theoretical results, and similar to the linear least
squares setup as discussed in the previous section. Also note that, not only does choosing a smaller
stepsize for the second layer eliminate double descent, it also gives a better overall risk.
To understand the relation to the kernels, suppose we choose the initialization equally, i.e., ω = 1
and ν = 1. If we update the variables of the second layer (i.e., v) with a much larger stepsize than
that of the first layer (i.e., W), then the kernel associated with the second layer dominates and the
network behaves like a random feature model Rahimi & Recht (2008). Similarly, if we update the
variables of the first layer with a much larger stepsize than that of the second layer, then the network
behaves like a network with the final layer weights v fixed. Thus, the stepsizes trade off the impact
of the two kernels, and this tradeoff yields a double descent curve.
4 Early stopping in convolutional neural networks
We finally study the training of a standard 5-layer convolutional neural network (CNN) and a stan-
dard ResNet-18 model on the (10 class classification) CIFAR-10 dataset. Both networks were stud-
ied in Nakkiran et al. (2020a). As shown in that paper, the risk has a double descent behavior if the
7
Published as a conference paper at ICLR 2021
t iterations
100 101 102 103 104 105
t iterations
101σi
102
Figure 3: Top row: Risk of the two-layer neural network trained on data drawn from a linear
model with diagonal covariance matrix with geometrically decaying variances. The risk has a double
descent curve unless we either i) initialize the first layer with a smaller initialization strength ω than
the second one ν, or we ii) choose a smaller stepsize for the weights in the second layer. Both
improves the risk as suggested by the theory. Bottom row: The norms kvi,Wk22 and kvi,vk22
measure to what extend the singular values σi are associated with the weights in the first (W) and
second (v) layer respectively. Double descent occurs when singular values are mostly associated
with the second layer, because then those weights are learned faster relative to the first layer weights.
Figure 4: Mitigating double descent for a 5-layer CNN. Left: Norm of the parts of the right singular
vectors of the Jacobian associated with the weights in the convolutional vi,C and fully connected
layers vi,F as a function of the singular values, σi, showing that large singular values pertain mostly
to the fully connected layer. This causes the fully connected layer to be learned faster than the
convolution layer. Middle and Right: Performance when trained with the i) same stepsize for all
layers, and ii) a smaller stepsize for the fully connected layer. Decreasing the learning rate of the
fully connected layer causes itto be learned at a similar speed as the convolutional layers and thereby
eliminates double descent and increases performance (i.e., the minima ofii is smaller than that of i).
network is trained on a dataset with label noise, and we consider the same setup with 20% random
label noise. While we have no theoretical results for those two complicated neural network models,
we demonstrate—inspired by our theory—that epoch-wise double descent can be eliminated and the
early stopping performance can be improved by adjusting the stepsizes/learning rates.
5-layer CNN: The 5-layer CNN consists for 4 convolutional layers followed by a fully connected
layer. Figure 4 shows that, just like for the two-layer network from the previous section, double
descent can be eliminated by changing stepsizes, this time by decreasing the stepsize of the final
fully connected layer. The intuition behind this is that large singular values of the Jacobian of the
network at initialization are mostly associated with the last fully connected layer, measured in the
same way as in the previous section. This causes the convolutional layers to be learned slower than
the fully connected layer which results in double descent. Analogously as before, decreasing the
stepsize pertaining to the fully connected layer eliminates double descent.
ResNet-18: We next consider the popular ResNet-18 model. ResNet-18 has a double descent
behavior when trained on the noisy CIFAR-10 problem Nakkiran et al. (2020a). Inspired by our
8
Published as a conference paper at ICLR 2021
theory, we again hypothesize that the double descent behavior occurs because some layer(s) of the
ResNet-18 model are fitted at a faster rate than others. If that hypothesis is true, then scaling the
learning rates of some layers should eliminate double descent. Indeed, Figure 6 in the appendix
shows that when scaling the stepsizes of the later half of the layers of the network mitigates double
descent.
Code
Code to reproduce the experiments is available at https://github.com/MLI-lab/early_
stopping_double_descent.
Acknowledgements
F. F. Yilmaz and R. Heckel are (partially) supported by NSF award IIS-1816986. R. Heckel also
acknowledges support by the TUM Institute of Advanced Study, and the authors would like to thank
Fanny Yang and Alexandru Tifrea for discussions and helpful comments on this manuscript.
References
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis of
Optimization and Generalization for Overparameterized Two-Layer Neural Networks. In Inter-
national Conference on Machine Learning, pp. 322-332, 2019.
Devansh Arpit, Stanislaw Jastrzkebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep networks. In International Conference
on Machine Learning, pp. 233-242, 2017.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167-1180, 2020.
Peter Buhlmann and Bin Yu. Boosting With the L2 Loss. Journal of the American Statistical
Association, 98(462):324-339, 2003.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:
The Power of Initialization and a Dual View on Expressivity. In Advances in Neural Information
Processing Systems 29, pp. 2253-2261. 2016.
Stephane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double Trouble in Double
Descent : Bias and Variance(s) in the Lazy Regime. In International Conference on Machine
Learning, 2020.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A Model of Double Descent for High-
dimensional Binary Linear Classification. arXiv:1911.05822 [cs, eess, stat], 2020.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient Descent Finds Global
Minima of Deep Neural Networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
9
Published as a conference paper at ICLR 2021
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient Descent Provably Optimizes
Over-parameterized Neural Networks. In International Conference on Learning Representations,
2018.
Simon Foucart and Rauhut, Holger. A Mathematical Introduction to Compressive Sensing. Springer
Berlin Heidelberg, 2013.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in High-
Dimensional Ridgeless Least Squares Interpolation. arXiv:1903.08560 [cs, math, stat], 2019.
Reinhard Heckel and Mahdi Soltanolkotabi. Compressive sensing with un-trained neural networks:
Gradient descent finds the smoothest approximation. In International Conference on Machine
Learning, 2020a.
Reinhard Heckel and Mahdi Soltanolkotabi. Denoising and Regularization via Exploiting the Struc-
tural Bias of Convolutional Generators. In International Conference on Learning Representations,
2020b.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems, pp.
8571-8580. 2018.
Arthur Jacot, Befin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning,
2020.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on
Learning Representations, 2018.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient Descent with Early Stopping
is Provably Robust to Label Noise for Overparameterized Neural Networks. In International
Conference on Artificial Intelligence and Statistics, 2020.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv:1908.05355 [math, stat], 2019.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2012. ISBN 978-0-262-30473-3.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
Double Descent: Where Bigger Models and More Data Hurt. In International Conference on
Learning Representations, 2020a.
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal Regularization Can
Mitigate Double Descent. arXiv:2003.01897 [cs, math, stat], 2020b.
Manfred Opper. Statistical Mechanics of Learning : Generalization. In The Handbook of Brain
Theory and Neural Networks, pp. 922-925. 1995.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: Global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 2020.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization Guarantees
for Neural Networks via Harnessing the Low-rank Structure of the Jacobian. arXiv:1906.05392
[cs, math, stat], 2019.
Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In Advances
in Neural Information Processing Systems 20, pp. 1177-1184. 2008.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early Stopping and Non-parametric Regres-
sion: An Optimal Data-dependent Stopping Rule. Journal of Machine Learning Research, 15:
335-366, 2014.
10
Published as a conference paper at ICLR 2021
Martin Wainwright. High Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Univer-
sity Press, 2019.
Yuting Wei, Fanny Yang, and Martin J. Wainwright. Early Stopping for Kernel Boosting Algorithms:
A General Analysis With Localized Complexities. IEEE Transactions on Information Theory, 65
(10):6685-6703, 2019.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking Bias-Variance
Trade-off for Generalization of Neural Networks. In International Conference on Machine Learn-
ing, 2020.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On Early Stopping in Gradient Descent
Learning. Constructive Approximation, 26(2):289-315, 2007.
Fatih Furkan Yilmaz and Reinhard Heckel. Image recognition from raw labels collected without
annotators. arXiv:1910.09055 [cs, stat], 2020.
A Supporting Material for: Early- s topped gradient descent for
LINEAR LEAST SQUARES
A.1 Intuition for the risk expression (1)
Below, we provide a proof of Theorem 1. Here we provide intuition why the risk is governed by the
risk expression (1).
First, note that the risk of the estimator can be written as a function of the variances of the features,
σ2, and of the coefficients of the underlying true linear model, θ* = [θɪ,..., θd], as
d
R(θ)= σ2 + Xσ2曜-θi)2.	⑺
i=1
where we used that z and x are drawn independently.
Next, recall that we consider the estimate based on early stopping gradient descent applied to the
empirical risk
R(θ) = ∣∣Xθ - y∣∣2.
Here, the matrix X ∈ Rn×d contains the scaled training feature vectors √nx1,..., √nXn as rows,
and y = √1n [yι,... ,yn are the corresponding scaled responses.
The gradient descent iterates obey
θt+1 — θ* = (I — diag(η)XTX)(θt — θ*) + diag(η)XTz,
where z = [z1 , . . . , zn] is the noise. As we formalize below, in the under-parameterized regime
where n d, we have that XT X ≈ Σ2 . Therefore the original iterates are close to the proximal
iterates θt defined by
θt+ι — θ* = (I — diag(η)ΣT∑) (θt — θ*) + diag(η)XTz.	(8)
The proximal iterates are, up to the extra term diag(η)XT z, equal to the iterates of gradient descent
applied to the population risk R(θ). Note that in contrast to the literature where it is common to
bound the deviation of the original iterates from the iterates on the population risk Raskutti et al.
(2014), here We control the deviation of the original iterates to the proximal iterates θt.
The iterates θt can easily be written out in closed form. To do so, first note that for the recursion
θt+1 = αθt + Y we have θt = αtθ0 + Y Pi- αi = αtθ0 + Y∙⅛-0T, where we used the formula for
a geometric series. Using this relation, and that we are starting our iterations at θi0 = 0, we obtain
∙-v
for the i-th entry of θt that
θ -θi = (1 - mσ^tθ* + σiXTzI-(I -i*熄2),
11
Published as a conference paper at ICLR 2021
where Xi isthe i-th column of X (not the i-th example/feature vector!). Nextnote that, E [(xTz)2] ≈
σ2σ2 because the entries of Z are N(0,σ2) distributed, and the entries of Xi are 1∕√nN(0,σ2)
distributed. Using this expectation in the iterates θt, and evaluating the risk of those iterates via
the formula for the risk given by (7) yields the risk expression (1). The proof of Theorem 1 in the
appendix makes this intuition precise by formally bounding the difference of the proximal iterates
to the original iterates.
A.2 Motivation for calling the U-shaped curves bias-variance tradeoffs
ʌ ʌ
Let θ = θ (D) be the parameter obtained based on the training data (for example by early stopping).
ʌ
The textbook bias-variance decomposition of the risk of θ is
ED [r(Θ)]
Ex (hx, θ*i- ED h(x, O)])2
'------------------------:
{z'*'^^^~
Bias(θ)
+ ED,x
}、
(〈X, θ - EDhDX, θ)i)
^^^—^{l^^^^^
Variance(θ)
+σ2.
}
The first term above is the bias of the hypothesis h(x) =(x, θ} It measures how well the average
function can estimate the true underlying function h(x) = (x, θ*). A low bias means that the
hypothesis accurately estimates the true underlying function (x, θ*). The second term is the variance
of the method. The variance of the method measures the variance of the hypothesis over the training
sets.
Recall from the previous paragraph that the estimate θt approximates the original iterations θt well
provided that the model is sufficiently underparameterized, i.e., d/n is small. It is straightforward to
verify that Bias(θt) = Pd=ι(θ*)2(1 - ηiσ2)2t and Variance(θt) = PId=I σ2(1 -(1 - ηiσ2)t)2,
exactly equal to the bias and variance terms in the risk expression (1). It follows that the bias and
variance of the original gradient descent iterates θt are also approximately equal to the terms in
the risk expression (1). The U-shaped curves are then the bias-variance terms pertaining to the i-th
feature; this formally establishes the U-shaped curves as bias-variance tradeoffs.
A.3 Numerical results for linear least squares
In this section we provide further numerical results for linear least squares. We consider a linear
model with d = 700 features, and with n = 6d examples. We let a fraction 6/7 of the features have
singular value σi = 1 and associated model coefficient θi = 1, and the rest, 1/7 of the features, have
singular value σi = 0.1 and θi = 10. In Figure 5(a) we show the risk obtained by simulating the risk
empirically along with the risk expression R(θt) given by equation (1). It can be seen that the risk
____________	∙-v
expression R(θt) slightly under-estimates the true risk. The quality of the estimate becomes better
as we increase n; in Figure 5(b) we show simulations for the same configuration but with n = 10d.
B	S upporting Material for: Early stopping in convolutional
NEURAL NETWORKS
B.1	ResNet- 1 8 training double descent eliminination
In Figure 6 we provide test and train error curves for ResNet-18 trained with different stepsizes on
noisy CIFAR-10. The results show that, as mentioned in the main body, double descent is eliminated
by choosing the stepsizes appropriately.
In more detail: ResNet-18 consists of 18 layers in total, where there are 4 residual blocks, each fea-
turing 4 convolutional layers with residual connections, between the first standalone convolutional
layer and the last fully-connected layer. We consider standard SGD training of ResNet-18 on noisy
CIFAR-10 with an initial learning rate of η = 0.1 and inverse square-root decay with decay rate
T = 512. This is the standard training setup for ResNet-18, and is exactly the setup for which for
which Nakkiran et al. reported double-descent behavior. We found that similar to the 5-layer con-
volutional network, double descent occurs in ResNet-18 because some of the networks’ layers are
learned at a different rates than others. We found that the weights of the last fully-connected layer
12
Published as a conference paper at ICLR 2021
(a) n = 5d
Figure 5: The risk of early-stopped gradient least-squares R(θt) based on numerical simulation of
the Gaussian model along with the risk expression R(θt) given in (1). We averaged over 100 runs of
gradient descent, and the shaded region corresponds to one standard deviation over the runs. It can
be seen that the risk expression slightly underestimates the true risk, but other than that describes
the behavior of the risk well.
Figure 6: Left: Test error of the ResNet-18 trained with the i) same stepsize for all layer, and
with ii) a smaller stepsize for the latter half of the layers. Decreasing the learning rate of the last
layers causes the last layers to be learned at a similar speed as the first and thereby eliminates double
descent. Right: The training error curves for i) and ii).
as well as the last two residual blocks were learned faster relative to the other layers. Following
the method inspired by our theory for the linear case and two-layer network and empirical obser-
vations from the 5-layer convolutional network, we eliminate the double descent by decreasing the
stepsizes of these layers to 10-4 from 10-1 after a few epochs. Note that ResNet-18 has a different
architecture than the simple 5-layer convolutional network and the values chosen differ for the two
networks. This is expected as double descent depends on many factors such as the underlying data
distribution as well as the network architecture and training.
B.2	Numerical bias-variance decomposition for the 5 -layer CNN
As discussed before, classical machine learning theory for the underparameterized regime estab-
lishes the bias-variance tradeoff as a result of the bias decreasing and the variance increasing as a
function of the model size (complexity). In the over-parameterized regime, the bias often contin-
ues to decrease, while the variance also decreases. This has been established in a number of recent
works Jacot et al. (2020); Yang et al. (2020); d’Ascoli et al. (2020), and provides a bias-variance
decomposition of the model-wise double-descent shaped risk curve.
In this paper, we demonstrated that epoch-wise double descent occurs for a different reason than
the model-wise double descent. Namely, epoch-wise double descent can be explained as a temporal
superposition of multiple bias-variance tradeoff curves rather than with a unimodal variance curve.
That also means that the overall bias (i.e., the sum of the individual bias terms) might not be decreas-
ing and the overall variance might not be uni-modal like in the model-wise case. To demonstrate that
13
Published as a conference paper at ICLR 2021
----- bias
-----variance
Figure 7: Bias and variance as a function on training epochs for training a 5-layer CNN until
convergence. The overall variance is increasing, and the overall bias has a double-descent like
shape. The training and test error curves show the interpolation of the training set and double
descent behavior of the error in this interval.
-----test error
-----train error
t iterations
Figure 8: Left: As we show here, multiple descent curves can arise for a regression problem. Here,
the risk can be decomposed as the sum of three bias-variance tradeoffs. Right: Risk of the two-layer
neural network trained on the data drawn from a linear model with diagonal covariance matrix with
geometrically decaying variances and additive noise yields multi-descent behavior. Both examples:
Scaling the stepsizes of the different layers/components eliminates the multi-descent similar to the
case of the double descent and improves the optimal early stopping performance for both the linear
model and the two-layer neural network as predicted by our theory.
it is in fact not, in Figure 7, we plot the numerically computed bias and variance terms (computed as
proposed in Yang et al. (2020)) for the CNN experiment from Section 4 along with the risk, which
shows that in fact the overall bias is increasing, while the variance has a double-descent like shape.
C Multi-descent
We note that in principle we can also observe multiple descents as a function of training time. Specif-
ically, recall the risk expression for the linear case, equation 1. It consists of d many bias-variance
tradeoffs, so in principle those curves might give rise not only to epoch-wise double descent, but
to multiple descent. See Figure 8, left panel, in which we show an example of three bias-variance
tradeoffs that add up to a multi-descent curve.
Likewise multi-descent can occur for neural networks. In Figure 8, right panel, we demonstrate this
for the two-layer network introduced in Section 3. For multi-descent to occur in a neural network,
we require a very particular setup. Specifically, for the two-layer neural network we consider, we
found that the existence of the multiple descents depends heavily on the noise in the data generation
process. We found for the two-layer neural network, multi-descent to occur only for a particular
range of noise levels. In more detail, we draw data from a linear model specified in Section 2.1,
in exactly the same way as for the simulations in the main body; but this time we added noise (the
noise variance σ2 of the additive noise z is non-equal to zero). Specifically, the 50-dimensional
feature vectors were chosen by drawing from a Gaussian with diagonal covariance matrix with
14
Published as a conference paper at ICLR 2021
geometrically decaying sigma values starting from σ1 = 4 and with noise variance σ = 11. We
generated n = 100 examples, and the network has a width of k = 250.
Intuitively, multiple descents could be observed in other empirical scenarios and for other archi-
tectures based on our theoretical and experimental findings for the linear case and the two-layer
neural network. However, we did not observe multi-descent in a practical setup (such as for training
CIFAR-10 with a convolutional network), as it requires a very particular setup (i.e., combination of
underlying data distribution, network architecture, and training). Image classification datasets are
considered to be minimally noisy and highly structured and this particular setup does not seem to
occur in practice even with the artificially injected label noise, at least we didn’t observe it when
training standard networks on CIFAR-10, and it hasn’t been reported elsewhere.
D Proof of Theorem 1
The difference of the risk and risk expression can be bounded by
∣R(θt) - R(θt)∣ ≤ ∣R(θt) - R(θt)∣ + ∣R(θt) - R(θt)∣ .	(9)
We bound the two terms on the righ-hand-side separately. We start with bounding the first term by
applying the lemma below.
Lemma 1. Define X so that X = XΣ. Suppose that ^I 一 XTX∣∣ ≤ G with E ≤ mini ηiσi?. Then
2 maxi ηi σi
∣R(θt) - R(θt)∣ ≤ (1 - (1 - minmσ2/2)t)28maxi η2σ4 e2	max ∣∣∑θ' - Σθ*∣∣2.	(10)
I	I	i	mini η2σ4	'∈{i,...,k} Il	l∣2
In order to apply the lemma, we start by verifying its condition. Towards this goal, consider the ma-
∙-v	∙-v
trix X = XΣ and note that the entries of X are iid N(0, 1/n). A standard concentration inequality
from the compressive sensing literature (specifically (Foucart & Rauhut, Holger, 2013, Chapter 9))
states that, for any β ∈ (0, 1),
p h∣∣ι - XTJX∣∣ ≥ β] ≤ e-nβ2+4d.
With β = 75dW We obtain that, with probability at least 1 一 e-d,
Next, we bound the term on the RHS of in (10), with the following lemma.
Lemma 2. Provided that ηiσi2 ≤ 1 for all i, with probability at least 1 - 2d(e-β2 /2 + e-n/8),
max∣∣∑θ' - Σθ*∣∣2 ≤ 2k∑θ*k2 +4dσ2β2.
Applying the lemma With β2 = 10 log(d), We obtain that With probability at least 1 - 2d-5 -
2de-n/8 - e-d
We have
IRe)- R ⑻ )∣ ≤ 8≡⅛4 W (2”的2+4 dσ2i0iog(2d)). (II)
We are noW ready to bound the second term in (9):
Lemma 3. With probability at least 1 — 4e-譬,we have that
2
∣R(θt) - R(θt)∣ ≤ ~nβ3√d,	(12)
__	∙-v
with R(θt) as defined in ⑴.
Applying the tWo bounds (11) and (12) to the RHS of the bound (9) concludes the proof. The
remainder of the proof is devoted to proving the three lemmas above.
15
Published as a conference paper at ICLR 2021
D.1 Proof of Lemma 1
Recall that the iterates of the original and closely related problem are given by
θt+1 - θ* = (I - diag(η)XTX)(θt - θ*) + diag(η)XTz,
θt+1 - θ* = (I - diag(η)∑T∑) (θt — θ*) + diag(η)XTz.
∙-v	∙-v
Note that X = XΣ, where we defined X which has iid Gaussian entries N (0, 1/n). With this
notation, and using that Σ is diagonal and therefore commutes with diagonal matrices, we obtain
the following expressions for the residuals of the two iterates:
Σθt+1 - Σθ* = (I - diag(η)Σ2XTX)(∑θt - Σθ*) + diag(η)Σ2XTZ
Σθt+1 - Σθ* = (I - diag(η)Σ2) (Σθt - Σθ*) + diag(η)∑2XTz.
The difference between the residuals is
Σθt+1 - Σθt+1 = (I - diag(η)Σ2XTX)(Σθt - Σθ*) - (I - diag(η)Σ2) (∑θt - Σθ*)
=Σθt - Σθt - diag(η)Σ2XTX(∑θt - Σθ*) + diag(η)Σ2(∑θt - Σθ*)
=(I - diag(η)Σ2XTX)(∑θt - Σθt) + diag(η)Σ2(I - XTX)(∑θt - ∑θ*),
where the last equality follows by adding and subtracting diag(η)Σ2XTX(∑θt - Σθ*) and rear-
ranging the terms. It follows that
∣∣Σθt+1 - Σθt+1( ≤ (1 - min ηiσ2∕2)∣∣Σθt - ∑θ1 +max，向€ max∣∣Σ<?' - Σθ*∣∣ .
(13)
Here, we used the bound
∣∣I - diag(η)∑2XTX∣∣ ≤ ∣∣I - diag(η)∑2∣∣ + ∣∣diag(η)∑2(I - XTX)∣∣
≤ (1 - min ηiσi2) + max ηiσi2
ii
≤ (1 - min ηiσ2/2).
i
Here, we used that ηiσi2 ≤ 1, by assumption, and the last inequality follows by the assumption
e ≤ Imini ηiσ22. Iterating the bound (13) yields
2 max ηi σi
Σθt - Σθt∣∣2 ≤
1 - (1 - mini ηiσ2∕2)t
mini ηiσ2∕2
max ηiσ2e max∣∑θ' - ∑θ*∣∣2
which concludes the proof.
D.2 Proof of Lemma 2
Recall that
σ侬-θi) = σi(1 - ηiσ2)tθi + XTZ(I-(I-ηiσ2力.
With ηiσi2 ≤ 1, by assumption, it follows that
σ2(θt -θ"2 ≤ 2σ2(θ"2 + 2(xTz)2.	(14)
Conditioned on z, the random variable XTZ is zero-mean Gaussian with variance ∣∣zk2∕n. Thus,
P IjXTz|2 ≥ kZk2β2] ≤ 2e-β2/2. Moreover, as used previously in (16), with probability at least
1 - 2e-n/8, ∣z∣22 ≤ 2σ2. Combining the two with the union bound, we obtain
P [|XTz|2 ≥ Qβ2] ≤ 2e-β2/2 + 2e-n/8.
in
Using this bound in inequality (14), We have that, with probability at least 1 - 2(e-β2/2 + e-n/8)
that
σ2(θt -θ"2 ≤ 2σ2⑹2 +41"
n
16
Published as a conference paper at ICLR 2021
By the union bound over all i we therefore get that
mtax∣∣∑θt - Σθ*∣∣2 ≤ 2k∑θ*k2 +4dσ2β2,
with probability at least 1 - 2d(e-β2 /2 + e-n/8).
D.3 Proof of Lemma 3
We have
d
R(Ot) = σ2 + £蟾((I-ηiσ2)tθ* + σixTz
i=1
1 - (1 - ηiσi2)t 2
σ2
d
σ2 + X(σi(1 -
i=1 1
mσi)tθt + XTZ(I-(I- ηiσ2)t)2.
—一一	J
{z
Zi
The random variable Zi , conditioned on z, is a squared Gaussian with variance upper bounded by
k√2 and has expectation
E [Zi] = σ2(1 -小。2产⑹)2 +	(1 - (1 -小02为2.
n
By a standard concentration inequality of sub-exponential random variables (See e.g. (Wainwright,
2019, Chapter 2, Equation 2.21)), we get, for β ∈ (0, √d) and conditioned on z, that the event
Eι = [X(Zi- E[Zi]) ≤ 呼√dβf	(15)
occurs with probability at least 1 - 2e-先. With the same standard concentration inequality for
sub-exponential random variables, we have that the event
(16)
_e2
also occurs with probability at least 1-2e-^. By the union bound, both events hold simultaneously
with probability at least 1 - 4e-譬.On both events, we have that
∣R(θt) - RR(θt)∣
d1
X(Zi- E [Zi])+ n (IlZlI2- σ2)(I-(I- ησ2W2
i=1	n
∣d	∣
≤ ∣∣X(Zi - E [Zi])∣∣ + d ∣∣IzI22 - σ2∣∣
∣ i=1	∣
≤ 回2√dβ + d ɪσ2β
n nn
2σ2 r- C d 1	2 c
≤ --dββ +——√nσ2β
2
≤ _β3√d.
n
concluding the proof of our lemma.
E Proof of Proposition 1
d
By equation (1), the risk expression is a sum of U-shaped curves: R(θt) = σ2 + id=1 Ui(t). We
start by considering one such U-shaped curve, and find its minimum as a function of the number of
17
Published as a conference paper at ICLR 2021
iterations, t. Towards this end, we set the derivative of one such U-shaped curve, given by
∂	σ2
∂kUi(t) = σ2(θi) 21og(1 - ηiσ2)(1 - ηiσ2)2 + —2((1 - η σ2) - I)IOg(I - ηiσ2)(1 - ηiσ2)
∂k	n
22
=2log(1 - ηiσ2)(1 - ησ2)t ((I - ηiσ2 )t(σ2 (θΓ)2 +-)-)
nn
to zero, which gives that the minimum occurs when
ηi
1
σi
(1-( σw‰ [
(17)
For the iteration t which satisfies this equation, we get
mtin Ui (t)
σ2 /nσ2(θ 勃2
b2/n + σ2(θi)2 ,
thus this minimum is independent of the iteration t and independent of the stepsize, provided their
relation is as described in (17) above.
F Proof and statements for neural networks
In this section, we prove the following result, which is a slightly more formal version of our main
result for neural networks, Theorem 2.
Theorem 3. Draw a dataset D = {(x1, y1), . . . , (xn, yn)} consisting of n examples i.i.d. from a
distribution with kxik2 = 1 and |yi| ≤ 1. Let Σ ∈ Rn×n be the corresponding Gram matrix defined
in (4), and suppose its smallest singular value obeys α > 0.
Pick an error parameter ξ ∈ (0, 1) and a failure probability δ ∈ (0, 1), and consider the two-layer
neural network fw V(X) = -⅛ relu(xT W)V, with parameters Wd×k, V ∈ Rk initialized according
,v	k
to (3) with initialization scale parameters ν, ω obeying νω ≤ ξ/,32log(2n∕δ) and V + ω ≤ L
Suppose that the network is sufficiently overparameterized, i.e.,
n10
k ≥ Ω 11	--4 .	(18)
α11 min(ν, ω)ξ4
Then, the risk of the network trained with gradient descent with constant stepsize η for t iterations
obeys, with probability at least 1 - δ,
R(fWt,vt) ≤
1n
n Ehui, yi2 (I - ησi2)2t +
n i=1
t n X hui, yi2 ^⅛σr + √1n + “a).
(19)
Theorem 2 directly follows by choosing the error parameter as ξ = O(α).
F.1 Proof of Theorem 3
In this section, we provide a proof of Theorem 2. Our proof relies on the observation that highly
overparameterized neural networks behave as associated linear models, as established in a large
number of prior works (Arora et al., 2019; Du et al., 2018; Oymak & Soltanolkotabi, 2020; Oymak
et al., 2019; Heckel & Soltanolkotabi, 2020b).
The proof consists of two parts. First, we control the empirical risk as a function of the number
of gradient descent steps, t. Second, we control the generalization error, i.e., the gap between the
population risk and the empirical risk by bounding the Rademacher complexity of the function class
consisting of two-layer networks trained with t iterations of gradient descent. Recall that our result
depends on the singular values and vectors of the gram matrix of kernels associated with the two-
layer network. The Gram matrix is given as the expectation of the outer product of the Jacobian of
the network at initialization:
n
Σ=E J (W0, V0)JT(W0, V0) =Xσi2uiuiT.
i=1
Here, expectation is with respect to the random initialization W0 , V0.
18
Published as a conference paper at ICLR 2021
Bound on the training error: We start with a results that controls the training error and ensures
that the coefficients of the neural network move little from its initialization.
Theorem 4. Pick an error parameter ξ ∈ (0, 1) and any failure probability δ ∈ (0, 1), and choose
ν, ω so that they satisfy νω ≤ ξ/ ,32log(2n∕δ). Suppose that the network is sufficiently overpa-
rameterized, i.e.,
k ≥ ω (α⅛⅛).
(20)
i)	Then, with probability at least 1 - δ, the mean squared loss after t iterations of gradient
descent obeys
n
X(yi - fWt,vt (xi))2 ≤
i=1
n
X(1 - ησi2)2t hui, yi2 + ξkyk2.
i=1
(21)
∖
ii)	Moreover, the coefficients overall deviate little from its initialization, i.e.,
Wt - W0k2F + kvt - v0k22
n
X
i=1
i, yi 1-(1-ησ)t
σi
^{^^™
Q:=
十 — √∕n . (22)
α
≤ t
J
Here, ∣∣∙∣∣f denotes the FrobeniuS norm. In addition each of the coefficients changes only
little, i.e., for all iterations t
4	“	/	4 l、n 2
kwt,r - w0,rk2 ≤ (V + αVn) √α,
∣vt,r 一 V0,r | ≤ (θ(ω/log(nk∕δ)) + 4√n^ /^22.
Here, wt,r is the r-th row ofWt, and vt,r is the r-th entry of vt.
(23)
(24)
Bound on the empirical risk: Because we train with respect to the `2 -loss but define the risk
with respect to the (generic Lipschitz) loss `, the empirical risk and training loss are not the same.
Nevertheless, we can upper bound the empirical risk computed over the training set at iteration t
with the training loss at iteration t:
1n
R(fWt,Vt)= n£'(ZWt,vt (xi),yi)
(i) 1 n
≤ n Z1 lfWt,Vt (Xi) 一 yi|
≤ n n (fWt,Vt (xi) - yi)2
(ii)
≤
∖
1n
-fhu yi (I- ησ2)2t + ξ,
n i=1
where (i) follows from '(z, y) = '(z, y) 一 '(y, y) ≤ |z 一 y| because the loss is 1-Lipschitz. EqUa-
tion (ii) is the most interesting one, and follows from Theorem 4, equation (21), and holds with
probability at least 1 一 δ. This bound is proven by showing that, provided the network is sufficiently
wide, the training loss behaves as gradient descent applied to a linear least-squares problem with
dynamics governed by the gram matrix Σ.
ʌ
Bound on the generalization error: Next, we bound the generalization error R(f) 一 R(f) by
bounding the Rademacher complexity of the functions that gradient descent can reach with t gradient
descent iterations.
19
Published as a conference paper at ICLR 2021
Let F be a class of functions f : Rd → R. Let 1, . . . , n be iid Rademacher random variables, i.e.,
random variables that are chosen uniformly from {-1, 1}. Given the dataset D, define the empirical
Rademacher complexity of the function class F as
RD(F) = LEe
n
n
sup	if(xi)
f∈F i=1
Here, D = {(x1 , y1 ), . . . , (xn, yn)} is the training set, consisting of n points drawn iid from the
example generating distribution. By a standard result from statistical learning theory, a bound on
the Radermacher complexity directly gives a bound on the generalization error for each predictor in
a class of predictors.
Theorem 5 ( (Mohri et al., 2012, Thm. 3.1)). Suppose '(∙, ∙) is bounded in [0,1] and 1 -Lipschitz
in its first argument. With probability at least 1 - δ over the random dataset D consisting of n iid
examples, we have that
八og(2∕δ)
V 2n
ʌ
sup R(f) - R(f) ≤ 2Rd (F) + 3
f∈F
We consider the class of neural networks with weights close to the random initialization W0, v0,
defined as:
FQ,M = {fW,v: W ∈W,v∈ V},	(25)
with
W = {w ： kW - WokF ≤ Q,kwr - w0,rk2 ≤ ωM, forall r},
V = {v: kv - v0k2 ≤ Q, |vr - v0,r| ≤ νM, for all r} .
The Rademacher complexity of this class of functions is controlled with the following result.
Lemma 4. Let W0 be drawn from a Gaussian distribution with N (0, ω2 ) entries, and suppose
the entries of v0 are draw uniformly from {-ν, ν}. Assume the (xi, yi) are drawn iid from some
distribution with kxik2 = 1 and |yi| ≤ 1. With probability at least 1 - δ over the random train-
ing set, provided that ,log(2n∕δ)∕2k ≤ 1/2, the empirical Rademacher complexity of FQM is,
simultaneously for all Q, bounded by
RD(Fqm) ≤ √n(ν + ω) + νω(5M2√k + 4M/log(2∕δ)∕2).	(26)
We set M = O(ξk-1/4). With this choice, the term on the right hand side above is bounded by
νω(5M2√k + 4M/log(2∕δ)∕2) ≤ O(ξ∕α),
where We used νω ≤ 1 and VZIokI/f”2 ≤ 1, by assumption (18). Note that by (23) and by (24)
combined with the assumption (18) we have that kwr - w0,rk2 ≤ ωM and |vr - v0,r| ≤ νM, as
desired.
Let Qi = i for i = 1, 2, . Simultaneously for all i, by the lemma above, for this choice of M, the
function class FQi ,M has Rademacher complexity bounded by
RD(FQiM) ≤ √i(V + ω) + O(ξ∕αX	(27)
We next choose the radius Q as defined in (22). Let i* be the smallest integer such that Q ≤ Q%*, so
that Qi* ≤ Q + 1. We have that i* ≤ O(pn∕a) and
RD(FQi* ,M) ≤ (^√n" (V + ω) + O(6/a)
≤ U n X (hui, yi 1-(： ησ2)t)2 + √n + O(ξ∕α),	(28)
20
Published as a conference paper at ICLR 2021
by the assumption of the theorem on k being sufficiently large, and by ν + ω ≤ 1. Next, from a
union bound over the finite set of integers i = 1,...,i*,we obtain
max. sup R(f)	一 R(f)	≤ ʌ - X(hui, yi 1(1ησi)~ )	+吃	+ O(ξ∕α),	(29)
i=1,...,i* f∈FQi,M	n n = ∖	σi	)	√n
as desired.
Final bound on the risk: Combining the bound on the training with the generalization bound
yields the upper bound (19) on the risk of the network trained for t iterations of gradient descent.
The remainder of the proof is devoted to proving Theorem 4 and Lemma 4.
F.2 Preliminaries
f (W, V)=√
√k relu(XW)v,
(30)
We start with introducing some useful notation. First note that the prediction of the neural network
for then training data points as a function of the parameters are
relu(x1T W)v
.
.
.
relu(xTn W)v
where Xn×d is the feature matrix and W ∈ Rd×k and v ∈ Rk are the trainable weights of the
network. The transposed Jacobian of the function f is given by
JT(W,v) = JJ1T2T(W(W, v)) ∈ Rdk+k×n,	(31)
where we defined the Jacobians corresponding to the weights of the first layer, W, and the second
layer, v, respectively as
V ι XT diag (relu0 (XW ι))
.	∈ Rdk×n, J2T(W)=
vtXTdiag(relu0(XWt))
Here, relu0(x) = l{χ≥Q} is the derivative of the relu activation function, which is the step function.
Our results depend on the singular values and vectors of the expected Jacobian at initialization:
√krelu(XW)T ∈ Rk×n.
JJ (W, V)=√
k
E JJ(Wo, vo)JT(Wo, vo)] = ν2 X E 卜elu0(Xw0,')relu0(Xw0,')[ Θ XXT
'=1
+ -E [relu(XW0)relu(XW0)T],
k
where Θ is the Hadamard product, and where we used that the entries ofvo are choosen iid uniformly
from {-ν, ν}. Expectation is over the weights Wo at initialization, which are iid N(0, ω2 ). This
yields
[E [J(Wo,vo)JT(Wo,vo)]]ij = ν2K1(xi,xj) +ω2K2(xi,xj),	(32)
where K1 and K2 are two kernels associated with the first and second layers of the network and are
given by
Kι(xi, Xj) = [e 卜elu0(Xw')relu0(Xw')Tii
ij
with ρij
=2 (1 — COST(Pij) In hχi, Xji
⅛⅛ andby
K2(xi, Xj) = ω12k [e [relu(XW)relu(XW)T]]ij
[E [E [relu(Xw)relu(Xw)t]]..
ω2	ij
2(q1 -ρ2j/π+(1—cos-1(Pij )/n)pj)∣∣χik2kχj ∣∣2
21
Published as a conference paper at ICLR 2021
For both of those expressions, we used the calculations from (Daniely et al., 2016, Sec. 4.2) for the
final expressions of the kernels. Also note that, by assumption kxi k2 = 1.
F.3 Proof of Theorem 4 (bound on the training error)
In this subsection, we prove Theorem 4.
F.3.1 The dynamics of linear and nonlinear least-squares
Theorem 4 relies on approximating the trajectory of gradient descent applied to the training
loss with an associated linear model that approximates the non-linear neural network in the
highly-overparameterized regime. This strategy has been used in a number of recent publica-
tions (Arora et al., 2019; Du et al., 2018; Oymak & Soltanolkotabi, 2020; Oymak et al., 2019;
Heckel & Soltanolkotabi, 2020b); in order to avoid repetition, we rely on a statement (Heckel &
Soltanolkotabi, 2020a, Theorem 4), which bounds the error between the true trajectory of gradient
descent and the trajectory of an associated linear problem.
Let f : RN → Rn be a non-linear function with parameters θ ∈ RN , and consider the non-linear
least squares problem
L(θ) = 2 kf (θ)-yk2.
The gradient descent iterations starting from an initial point θ0 are given by
Θt+1 = θt- ηVL(θt) where VL(θ) = JT(θ)(f(θ) - y),	(33)
where J (θ) ∈ Rn×N is the Jacobian of f at θ (i.e., [J (θ)]i,j = fj). The associated linearized
least-squares problem is defined as
1
Llin(θ) = 2 kf (θo) + J(θ - θo) - yk2∙	(34)
Here, J ∈ Rn×N, refered to as the reference Jacobian, is a fixed matrix independent of the parameter
θ that approximates the Jacobian mapping at initialization, J(θ0). Starting from the same initial
point θ0, the gradient descent updates of the linearized problem are
θt+ι = θt- ηJT f (Θo) + J(θt- Θo) - y) ∙	(35)
To show that the non-linear updates (33) are close to the linearized iterates (35), we make the fol-
lowing assumptions:
i)	We assume that the singular values of the reference Jacobian obey for some α, β
√2α ≤ σn ≤ σι ≤ β∙	(36a)
Furthermore, we assume that the norm of the Jacobian associated with the nonlinear model
f is bounded in a radius R around the random initialization
kJ (θ)k ≤β for all θ∈ BR(θ0).	(36b)
Here, BR(θ0) := {θ : kθ - θ0k ≤ R} is the ball with radius R around θ0.
ii)	We assume the reference Jacobian and the Jacobian of the nonlinearity at initialization
J(θ0) are 0-close:
kJ(θ0) - Jk ≤ 0 .	(36c)
iii)	We assume that within a radius R around the initialization, the Jacobian varies by no more
than :
kJ (θ)-J (θ0 )k ≤ I， for all θ ∈ Br(Θo)∙	(36d)
Under these assumptions the difference between the non-linear residual
rt := f(θt) - y
and the linear residual
∙-v
r := f(θo) + J(θt- θo) - y
are close throughout the entire run of gradient descent.
22
Published as a conference paper at ICLR 2021
Theorem 6 ((Heckel & Soltanolkotabi, 2020a, Theorem 4), Closeness of linear and nonlinear least-
-squares problems). Assume the Jacobian J (θ) ∈ Rn×N associated with the function f(θ) obeys
Assumptions (36a), (36b), (36c), and (36d) around an initial point θ0 ∈ RN with respect to a ref-
erence Jacobian J ∈ Rn×N and with parameters α, β, 0, , obeying 2β(0 + ) ≤ α2, and R.
Furthermore, assume the radius R is given by
R = 2II J^r0∣∣2 + 5 α (EO + e)kr0k2∙	(37)
Here, Jt is the pseudo-inverse of J. We run gradient descent with stepsize η ≤ $ on the linear and
non-linear least squares problem, starting from the same initialization θ0. Then, for all iterations t,
i)	the non-linear residual converges geometrically
krtk2 ≤ (1- ηα2)t kr0k2,	(38)
ii)	the residuals of the original and the linearized problems are close
krt- rt k2 ≤ !n¾⅛ krok2,	(39)
iii)	the parameters of the original and the linearized problems are close
∣∣θt - θt∣L ≤ 2∙5 α (EO + E)kr0k2,	(40)
iv)	and the parameters are not far from the initialization
kθt - θ0k2 ≤ 2 ∙	(41)
Theorem 6 above formalizes that in a (small) radius around the initialization, the non-linear problem
behaves very similar to its associated linear problem. As a consequence, to characterize the dynam-
ics of the nonlinear problem, it suffices to characterize the dynamics of the linearized problem. This
is the subject of our next theorem, which is a standard result on the gradient iterations of a least
squares problem, see for example (Heckel & Soltanolkotabi, 2020b, Thm. 5) for the proof.
Theorem 7 (E.g. Theorem 5 in Heckel & Soltanolkotabi (2020b)). Consider a linear least squares
problem (34) and let J = in=1 σiuiviT be the singular value decomposition of the matrix J. Then
the linear residual rt after t iterations ofgradient descent with updates (35) is
n
rt = X(1 — ησ2)t Ui hui, roi .	(42)
i=1
Moreover, using a step size satisfying η ≤ 方,the linearized iterates (35) obey
∣∣θt- θ0∣∣2 = X (hUi,roi l-⅛⅛)2.	(43)
i=1
F.3.2 Proving Theorem 4 by applying Theorem 6
We are now ready to prove Theorem 4. We apply Theorem 6 to the predictions of the network
given by f(W, v) defined in (30) with parameter θ = (W, v). As reference Jacobian we choose
a matrix J ∈ Rn×dk+k that satisfies JJT = E J (WO, vO)JT (WO, vO) (where expectation is
over the random initialization (WO, vO)), and at the same time is very close to the Jacobian of f at
initialization, i.e., to J (WO, vO). Towards this goal, we apply Theorem 6 with the following choices
of parameters:
α = σmin(Σ)∕√2, β = 10√n(ω + V), E = ɪ6ξθ-, Eo = 2 j(ω2 + V2)今 log(kn∕δ).
(44)
Note that assumption (20) guarantees that EO ≤ E, a fact we used later.
We now verify that the conditions of Theorem 4 are satisfied for this choice of parameters with
probability at least 1 - δ. Specifically we show that each of the conditions holds with probability at
least 1 - δ. By a union bound, the success probability is then at least 1 - Ω(δ), and by rescaling δ
by a constant, the conditions are satisfied with probability at least 1 - δ.
23
Published as a conference paper at ICLR 2021
Bound on residual: We need a bound on the network outputs at initialization as well as on the
initial residual to verify the conditions of the theorem. We start with the former:
kf (W0, v0)k2 = √1k IlrelU(XWO)V0k2
≤ νωP8log(2n∕δ)kX∣F
=νω ʌ/ 8 log(2n∕δ) √n,
(45)
where the inequality holds with probability at least 1 - δ, by Gaussian concentration (see Lemma 6
in Heckel & Soltanolkotabi (2020b) and recall that W0 has iidN(0, ω2) entries). Moreover, the last
equality follows from kxi k2 = 1.
It follows that, with probability at least 1 - δ, the initial residual is bounded by
I∣r0k2 = √1krelu(XW0)v0 - y
≤ νωʌ/8 log(2n∕δ)√n + √n
≤ 2√n
(46)
where the first inequality holds by the triangle inequality and using the assumption |yi | ≤ 1, and the
second inequality by νω，8 log(2n∕δ) ≤ 1, again by assumption.
Radius in the theorem: In order to verify the condition of the theorem, we need to control the
radius in the theorem, which we do next. With our assumptions and the choices of parameters above,
the radius in the theorem, defined in equation (37), obeys
R = 2 Il Jtr0 Il 2 + 5 α (EO + e)kr0k2
α
kr0k2
(iv)
≤ min(ω, ν)
⅛ξ α !
kXk(ν + 3ω))
______ - /
{^^^^^∙-/^^^^^^^^^^^
R
(47)
Here, (i) follows from the fact that ∣∣Jtr0∣∣2 ≤ √2αkr0k2, (ii) from S + C ≤ 2e = 1 ξα3 (by
definition ofE0 and E), and (iii) from the bound on the residual (46). For (iv) we used assumption (20)
in the theorem.
Verifying Assumptions (36a) and (36b): By definition JJT = Σ, thus the lower bound in as-
sumption (36a) holds by the definition of a as σn(Σ) ≥ √2α. Regarding the upper bound of (36a),
note that
kJk2≤ν2kK1kF+ω2kK2kF
≤ ν2n + ω2n,
where K1 ∈ Rn×n and K2 ∈ Rn×n are the kernel matrix with entries K1 (xi, xj) and K2(xi, xj).
It follows that ∣Jk ≤ 2(ω + V)√n ≤ β, as desired. This concludes the verification of (36a).
24
Published as a conference paper at ICLR 2021
To verify assumption (36b) note that
kJ (W, v)k ≤ kJ1(W)k + kJ2(W)k
≤√1k (kXkkvk2 + kXWkF )
≤ √k (kXk (kv0k2 + kv - v0k2 + kW - WokF) + kXWokF)
≤ √n10(ω + V) = β.
For the last inequality, We used that kv0k2 = ν√k, and that kv - vo∣∣2 + ∣∣W - Wo∣∣f ≤ R ≤
√kmin(ω, V), by the bound on the radius in (47), and finally that ∣∣XWo∣f ≤ ω6 ʌ/k with prob-
ability at least 1 - δ provided that k ≥ log(n∕δ), which holds by assumption. For this inequality
we used that xiT W0 is a Gaussian vector with iid N (0, ω2) entries. It follows that assumption (36b)
holds with probability at least 1 - δ, as desired.
Verifying Assumption (36c): We start with stating a concentration lemma from Heckel &
Soltanolkotabi (2020b).
Lemma 5 (Concentration lemma (Heckel & Soltanolkotabi, 2020b, Lemma 3)). Consider the par-
tial Jacobian J1 (W), and let W ∈ Rn×k be generated at random with i.i.d. N(0, ω2) entries, and
suppose the v` are drawn from a distribution with ∣V'∣ ≤ V. Then, with probability at least 1 - δ,
v 2
Jι(W, V)JT (W, v) - E Ji(W, V)JT (W, v)]∣∣ ≤ √k ∣Xk2 √log(2n∕δ).
Lemma 6. Let J2(W) = √⅛relu(XW), with W generated at random with i.i.d. N(0, ω2) entries.
With probability at least 1 - δ,
ω2
∣∣J2(W)J2r (W) - E J2(W)JT (W)] Il ≤ 3 √k kXk2 log(kn∕δ).	(48)
Combining the statements of the two lemmas, it follows that, with probability at least 1 - 2δ,
Il J(W, V)JT(W, v) - EJ(W, V)JT(W, v)] ∣∣ ≤ (ω2 + v2)3-√k∣∣X∣∣2 log(kn∕δ)
≤ (ω2 + v2)3√knlog(kn∕δ).	(49)
To show that (49) implies the condition in (36c), we use the following lemma.
Lemma 7 ((Oymak et al., 2019, Lem. 6.4)). Let J0 ∈ Rn×N, N ≥ nand let Σ be n × npsd matrix
obeying ∣∣ JoJT — Σ∣∣ ≤ e2, for a scalar e ≥ 0. Then there exists a matrix J ∈ Rn×N obeying
Σ = JJT such that
IlJ - JOIl ≤ 2∈∙
From Lemma 7 combined with equation (49), there exists a matrix J ∈ Rn×N that obeys
kJ-J(Wo,vo)k ≤o,
eo = 2 j(ω2 + V2)今 log(kn∕δ).
This part of the proof also specifies our choice of the matrix J as a matrix that is o close to the
Jacobian at initialization, J(Wo), and that exists by Lemma 7 above.
Verifying Assumption (36d): We control the perturbation around the random initialization.
Lemma 8. Let Wo have iid N(0, ω2) entries and let vo have (arbitrary) entries in {-V, +V}
entries. Thenfor all W and V obeying,for some R ≤ 2 √k,
kw - WokF ≤ ωR,	kv - V0k2 ≤ vR,
25
Published as a conference paper at ICLR 2021
the Jacobian in (31) obeys
kJ (W, V)-J (Wo, vo)k ≤ kXk√k
(ωR + νR + V√2(2tR)1∕3),
with probability at least 1 -
ne-1 R4/3k7/3
Recall the definition R = √k (1因鼠；+39)) from (47). From R ≤ (kR)1/3 for R ≤ √k, the
bound provided by lemma 8 guarantees that
kJ (W, V)-J (Wo, vo)k ≤ kXk ω√3ν (kR)1∕3
1	α3
=E = 16 ξβ2,
where the second inequality follows by choosing R = √k (^^V+3ω)) ∙ This holds With Proba-
bility at least
1	-1R”/3	1	-2-17ξ4α8k3 (i) 1 S
1 — ne 2	= 1 — ne	β8	≥ 1 — δ,
where in (i) we used (20). Therefore, AssumPtion (36d) holds with high Probability by our choice
of e = ⅛ ξ α.
Concluding the proof of Theorem 4: By the Previous ParagraPhs, the assumPtions of Theorem 4
are satisfied with Probability at least 1 — O(δ). Therefore we can bound bound the training error and
the deviation of the coefficients from the initialization as follows.
Training error: We bound the training error in (20). The training error at iteration t is bounded
by
Ilf(Wt, Vt)- yk2 ≤ krtk2 + krt - rtk2
(i)	弋r 门	2∖2t t ∖2	2β(EO + E)Il ∣∣
≤ t i=1 QF hui,roi +	kr0k2
≤) ʌ XX (1 — ησ2)2t hUi, yi2 + kf(Wo, w0)k2 + 2β⅛2+⅛kro∣2
i=1	e(ln 2)α
(iii) n
≤ uX (1 — ησi2) hui, yi + ξkyk2,
i=1
where inequality (i) follows from bounding the linear residual Ilrtk2 with theorem 7, as well as
bounding the distance between the linear residual and the non-linear one with (39). Inequal-
ity (ii) follows from ro = f(Wo,vo) — y, and finally (iii) follows from ∣∣f(Wo, v0)k2 ≤
νωP8log(2n∕δ)∣∣yk2 ≤ ξ∣∣yk2,by (45),and a(EO + E) ≤ αβ22e = 8ξα ≤ ξ.
Distance from initialization: We next bound the distance from the initialization, i.e., we estab-
lish (22). Combining equation (43) in theorem 7 with equation (39) in theorem 6, we obtain
JkWt — WOkF + ∣∣vt — vo∣∣2 ≤
(i)
≤
tXX (hUi, roi I-(I-ησ2)t )2+2.5 β4 (EO + E)krok2
i=1	σi	α
ʌ χ(hui,yi ɪ——^ησi-L) + Lkf(Wo,vo)k2+ 2.5%(EO+ E)∣∣rok
i=1	σi	α	α
2
(ii)
≤t
n
hui, yi
i=1
1-(1-ησ2)t)2 + ξ
σi	Ja
26
Published as a conference paper at ICLR 2021
where (i) follows from r° = f(Wo, v°)-y and (ii) follows from 2.5与(∈o+e)kr0k2 ≤ * I∣r0k2 ≤
ι5α (IIf(Wo, v0)k2 + IlyIl2), where We used s + E ≤ 2e = 1 ξɑ3, by definition of e, e°, combined
with ∣∣f(Wo, v0)k2 ≤ ξ∕4√n∙
Bound on change of coefficients: Finally, we establish the bounds on the change of the individual
coefficients (23) and (24). We start with the weights in the first layer, wr . The gradient with respect
to Wr is given by RwrL(W, V) = [JT(W, v)]rr, where [JT(W, v)]r is the submatrix of the
Jacobian multiplying with the weight wr , and r is the residual. Therefore, we obtain
t-1
IWt,r - Wo,r I2 =	(Wτ +1,r - Wτ,r)
τ=o	2
t-1
≤ Xη[J1T(Wτ,vτ)]rrτ2
τ=o
(i) z 4 l、
≤(V+En
t-1
ηXIrτI2
τ=o
≤ (V + 4√n)√nηX(1 - ηα2)τkroII2
α	k τ=o
≤) (V + 4 √n) √n 3 IroI2.
α	k α2	2
Here, (i) follows from
Il [JT (Wτ, VT )]r∣∣ = √k diag(relu0(XWr ))X
≤(V+α √n) √n
where the last inequality follows from |v「,r 一 vo,r∣ ≤ ∣∣Vτ,r - v0,r∣∣2 ≤ R ≤ 4√n, by (47).
Moreover, for (ii) we used that, by (38), the non-linear residuals converge geometrically, and (iii)
follows from the formula for a geometric series. This conclude the proof of the bound (23).
Analogously, we obtain
t-1
∣Vr - V0,r | ≤ X η∣[J2T (Wτ )]r r |
τ=o
≤ ^O(ωPι°g(nk7δ)) + α √n) √kα2 肛。12
where we used that
√k dXw0,r i2 + IlX(W0,r - wr 州?)
√k (θ(ωPnlog(nk∕δ)) + √n∣∣wo,r - w"∣2
√k f O(ω ρlog(nk7δ))+√nɑ2).
Here, the last inequality follows by using that the entries of Xwo,r are not independent but N (0, ω2)
distributed, and by taking an union bound over all entries of that vector and over all Xwo,r . This
concludes the proof of the bound (24).
27
Published as a conference paper at ICLR 2021
F.3.3 Proof of lemma 8
First note that
kJ(W,v)-J(W0,v0)k ≤ kJ1(W, v) - J1(W0, v0)k + kJ2(W, v) - J2(W0, v0)k
≤ kJ1(W, v) - J1(W, v0)k + kJ1(W, v0) - J1(W0, v0)k
+ kJ2 (W, v) - J2 (W0, v0)k .	(50)
In the reminder of the proof we bound the three terms above. We start by bounding the third term
in (50) as:
kJ2(W, v) - J2(W0, v0)k ≤ √1kkrelu(XW) -relu(XW0)∣∣F
≤√1k kxw - XW0Hf
≤√1k kXkkW — W0Hf .
We proceed with bounding the first term in (50) as:
HJ1(W, v) - J1(W, v0)H2 = (J1(W, v) - J1(W, v0))(J1(W, v) - J1(W, v0))T
=1∣∣relu0(XW)diag((vι - v1 )2,...,(Vt - v0)2)relu0(XW)T Θ XXt ∣∣
≤ 1 ∣∣Xk2 max ∣∣relu0(xT1 W)diag(v - v0)∣∣2
kj
≤ kkv - v0k2∣Xk2.
k
Next we establish below that with probability at least 1 - ne-kq2/2, the second term in in (50) is
bounded as
∣Ji(W, v0) - J1(W0, v0 )k ≤ 十 ∣v0k∞ kXkP2q	(51)
provided that
∣w - w0k ≤√q⅛ω = ωR,
2k
where the last inequality follows from setting q = (2kR)2/3 (note that the assumption RR ≤ ɪ√k
ensures q ≤ k). Putting those three bounds together in (50) yields
kJ (W, v) - J (W0, v0)k ≤ ∣Xk √k (kW - W0∣f + kv - v0∣∣2 + ∣∣v0k∞√2(2kW3),
(52)
which established the claim.
It remains to prove (51). Towards this goal, first note that
kJ1(W, v) - J1(W0, v0)k ≤ kJ1(W, v) - J1(W, v0)k + kJ1(W, v0) - J1(W0, v0)k (53)
We proceed with bounding the second term in the RHS of (53) as:
kJ1 (W, v0) - J1(W0, v0)k2 ≤ k kv0k∞kXk2 max∣∣σ0(xT W) - σ0(xj W0)∣∣2.	(54)
kj
Because relu0 is the step function, we have to bound the number of sign flips between the matrices
XW and XW0. For this we use the lemma below:
Lemma 9. Let ∣v∣∏(q) be the q-th smallest entry of V in absolute value. Suppose that, for all i, and
q ≤ k,
∣w - w0k ≤ √q W W]π(q).
Then
miax ∣∣σ0(xiTW) - σ0(xTW0)∣∣ ≤ √2q.
28
Published as a conference paper at ICLR 2021
For the entries W0 being iid N(0, ω2), we note that, with probability at least 1 - ne-kq2/2, the q-th
smallest entry of xiT W0 ∈ Rk obeys
IXTw0l∏(q)〉q
Fl 一≥ 2kω
for all i = 1, . . . , n.
(55)
We are now ready to conclude the proof of the lemma. By equation (54),
kJι(W, v0)-Jι(W0, v0)k ≤√k MUXk mjxg0 (XT W) - σ0(χT W0)∣∣
≤√1k WUXkp2q
provided that
IW - W0k ≤√⅛ω
2k
with probability at least 1 - ne-kq2/2 .
F.4 Proof of Lemma 4 (bound on the Rademacher complexity)
Our proof follows that of a related result, specifically (Arora et al., 2019, Lem. 6.4) which pertains
to a two-layer ReLU network where only the first layer is trained and the second layers’ coefficients
are fixed. Our goal is to bound the empirical Rademacher complexity
RD (FQM)=n E
n 1k
sup F ⅛Σvrrelu(wrTXi)
W∈W,v∈V i=1	k r=1
where expectation is over the iid Rademacher random variables i, and where {X1, . . . , Xn} are the
training examples.
The derivation of the bound on the Rademacher complexity is based on the intuition that if the
parameter M of the constraint kwr - w0,r k2 ≤ ωM is sufficiently small, then relu0 (wrTXi) is
constant for most r, because |w0T,rXi| is bounded away from ωM with high probability, by anti-
concentration of a Gaussian. For those coefficient vectors r for which relu0(wrTXi) is constant, we
have relu(wrTXi) = relu0(w0T,rXi)wrTXi. For the other coefficients, we can bound the difference of
those two values as
relu(wrTXi) - relu0 (w0T,r Xi)wrT Xi =relu0(wrTXi)wrTXi - relu0(w0T,rXi)wrTXi
=relu0(wrTXi)wrTXi - relu0(w0T,rXi)w0T,rXi
+ relu0(w0T,rXi)w0T,rXi- relu0 (w0T,r Xi)wrT Xi
=relu(wrTXi) - relu(w0T,rXi) + relu0(w0T,rXi) hw0,r - wr,Xii
≤2kwr - w0,r k2 kXik2
≤2ωM,
where the last inequality holds for W ∈ W . It follows that
RD (FQM) ≤ n E
n 1k
sup -,Σ> ⅛Σvrrelu0(wrT0Xi)wrTXi
W∈W,v∈V i=1	k r=1
2M n k
十 n√k 之 r=1rr
{relu0(wrT,0xi)6=relu0(wrTxi)}
LE sup €T Ji (Wo, V)W
n W∈W,v∈V
2ωM n k
+ -√r∑ ∑vri
n k i=1 r=1
{relu0(wrT,0xi)6=relu0(wrTxi)},
(56)
29
Published as a conference paper at ICLR 2021
where J1 is the Jacobian defined in (31), and where we use w = vect(W) ∈ Rdk for the vectorized
version of the matrix W with a slight abuse of notation. With this notation, we can bound the first
term in (56) by
1	(i) 1
-E sup €T Ji(Wo, V)W = -E sup eτ (J1(W0, V)W - Ji(Wo, vo)wo)
n	W∈W,v∈V	n W∈W,v∈V
—E	sup	eτ (JI(W0, v)w - JI(W0, v)wo + JI(W0, v)wo - Ji(Wo, vo)wo)
n	W∈W,v∈V
—E	sup	WT (JI(W0, v)(w - wo) + J2(W0)(v - vo))
n	W∈W,v∈V
一E	sup eτ (J1(W0, vo)(w - wo) + Ji(Wo, V - vo)(w - wo) + J2(W0)(v - vo))
n	W∈W,v∈V
≤) — E [忖Ji(Wo, V0)∣∣2] Q + — E [忖J2(W0)∣∣2] Q + √kνωM2
(出)1 ~	、 L L …	—
≤ -Q(V + ω)√n + VkνωM2.	(57)
n
Here, equality (i) follows because WJi(Wo, Vo)wo has zero mean, inequality (ii) follows from the
Cauchy-Schwarz inequality as well as from
kJi(Wo,V - Vo)(w - wo)k2
—k
√k kXk 工 |vr - v0,r |kwr
—wo,"∣2 ≤ √kνωM2√n,
≤
and inequality (iii) follows from E [kAWk2] ≤
JE [kAek2]
kAkF , by Jensen’s inequality, and
from the bounds kJi(Wo, Vo)kF ≤ ν and kJ2(Wo)kF ≤ ω, which holds with probability at least
1 - δ provided that ,log(2n∕δ)∕2k ≤ 1/2, which in turn holds by assumption.
We next upper bound the second term in (56). Following the argument in (Arora et al., 2019, Proof
of Lemma 5.4), we get
XX Vr l{relu0(wT0Xi) = relu0(wTXi)} ≤ 2νkn (M + \^~^2^~^} ,	(58)
i=i r=i
with probability at least — - δ. Here, we used that vr ≤ |vo,r| + |vr - vo,r | ≤ 2ν. Putting the bounds
on the first and second term in (56) (given by inequality (57) and inequality (58)) together, we get
that, with probability at least — - δ, the Rademacher complexity is upper bounded by
RD (F) ≤ √Qn (ν + ω) + νω(5M 2√k + 4M /log(2∕δ))
which concludes our proof.
30