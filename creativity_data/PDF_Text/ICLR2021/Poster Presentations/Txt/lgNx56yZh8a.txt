Published as a conference paper at ICLR 2021
Bayesian Few-Shot Classification
WITH ONE-VS-EACH POLYA-GAMMA AUGMENTED
Gaussian Processes
Jake C. Snell
University of Toronto
Vector Institute
jsnell@cs.toronto.edu
Richard Zemel
University of Toronto
Vector Institute
Canadian Institute for Advanced Research
zemel@cs.toronto.edu
Ab stract
Few-shot classification (FSC), the task of adapting a classifier to unseen classes
given a small labeled dataset, is an important step on the path toward human-like
machine learning. Bayesian methods are well-suited to tackling the fundamental
issue of overfitting in the few-shot scenario because they allow practitioners to
specify prior beliefs and update those beliefs in light of observed data. Contem-
porary approaches to Bayesian few-shot classification maintain a posterior dis-
tribution over model parameters, which is slow and requires storage that scales
with model size. Instead, we propose a Gaussian process classifier based on a
novel combination of Polya-Gamma augmentation and the one-vs-each Softmax
approximation (Titsias, 2016) that allows us to efficiently marginalize over func-
tions rather than model parameters. We demonstrate improved accuracy and un-
certainty quantification on both standard few-shot classification benchmarks and
few-shot domain transfer tasks.
1	Introduction
Few-shot classification (FSC) is a rapidly growing area of machine learning that seeks to build
classifiers able to adapt to novel classes given only a few labeled examples. It is an important
step towards machine learning systems that can successfully handle challenging situations such as
personalization, rare classes, and time-varying distribution shift. The shortage of labeled data in
FSC leads to uncertainty over the parameters of the model, known as model uncertainty or epistemic
uncertainty. If model uncertainty is not handled properly in the few-shot setting, there is a significant
risk of overfitting. In addition, FSC is increasingly being used for risk-averse applications such as
medical diagnosis (Prabhu, 2019) and human-computer interfaces (Wang et al., 2019) where it is
important for a few-shot classifier to know when it is uncertain.
Bayesian methods maintain a distribution over model parameters and thus provide a natural frame-
work for capturing this inherent model uncertainty. In a Bayesian approach, a prior distribution is
first placed over the parameters of a model. After data is observed, the posterior distribution over
parameters is computed using Bayesian inference. This elegant treatment of model uncertainty has
led to a surge of interest in Bayesian approaches to FSC that infer a posterior distribution over the
weights ofa neural network (Finn et al., 2018; Yoon et al., 2018; Ravi & Beatson, 2019).
Although conceptually appealing, there are several practical obstacles to applying Bayesian infer-
ence directly to the weights of a neural network. Bayesian neural networks (BNNs) are expensive
from both a computational and memory perspective. Moreover, specifying meaningful priors in pa-
rameter space is known to be difficult due to the complex relationship between weights and network
outputs (Sun et al., 2019).
Gaussian processes (GPs) instead maintain a distribution over functions rather than model parame-
ters. The prior is directly specified by a mean and covariance function, which may be parameterized
by deep neural networks. When used with Gaussian likelihoods, GPs admit closed form expressions
for the posterior and predictive distributions. They exchange the computational drawbacks of BNNs
1
Published as a conference paper at ICLR 2021
for cubic scaling with the number of examples. In FSC, where the number of examples is small, this
is often an acceptable trade-off.
When applying GPs to classification with a softmax likelihood, the non-conjugacy of the GP prior
renders posterior inference intractable. Many approximate inference methods have been proposed to
circumvent this, including variational inference and expectation propagation. In this paper we inves-
tigate a particularly promising class of approaches that augment the GP model with a set of auxiliary
random variables, such that when they are marginalized out the original model is recovered (Albert
& Chib, 1993; Girolami & Rogers, 2006; Linderman et al., 2015). Such augmentation-based ap-
proaches typically admit efficient Gibbs sampling procedures for generating posterior samples which
when combined with Fisher’s identity (Douc et al., 2014) can be used to optimize the parameters of
the mean and covariance functions.
In particular, augmentation with Polya-Gamma random variables (Polson et al., 2013) makes in-
ference tractable in logistic models. Naively, this is useful for handling binary classification, but
in this paper We show how to extend Polya-Gamma augmentation to multiple classes by using the
one-vs-each softmax approximation (Titsias, 2016), which can be expressed as a product of logistic
sigmoids. We further show that the one-vs-each approximation can be interpreted as a composite
likelihood (Lindsay, 1988; Varin et al., 2011), a connection which to our knowledge has not been
made in the literature.
In this work, we make several contributions:
•	We show how the one-vs-each softmax approximation (Titsias, 2016) can be interpreted as
a composite likelihood consisting of pairwise conditional terms.
•	We propose a novel GP classification method that combines the one-vs-each softmax ap-
PrOximatiOn with Polya-Gamma augmentation for tractable inference.
•	We demonstrate competitive classification accuracy of our method on standard FSC bench-
marks and challenging domain transfer settings.
•	We propose several new benchmarks for uncertainty quantification in FSC, including cali-
bration, robustness to input noise, and out-of-episode detection.
•	We demonstrate improved uncertainty quantification of our method on the proposed bench-
marks relative to standard few-shot baselines.
2	Related Work
Our work is related to both GP methods for handling non-conjugate classification likelihoods and
Bayesian approaches to few-shot classification. We summarize relevant work here.
2.1	GP Classification
Non-augmentation approaches. There are several classes of approaches for applying Gaussian
processes to classification. The most straightforward method, known as least squares classifica-
tion (Rifkin & Klautau, 2004), treats class labels as real-valued observations and performs infer-
ence with a Gaussian likelihood. The Laplace approximation (Williams & Barber, 1998) constructs
a Gaussian approximate posterior centered at the posterior mode. Variational approaches (Titsias,
2009; Matthews et al., 2016) maximize a lower bound on the log marginal likelihood. In expectation
propagation (Minka, 2001; Kim & Ghahramani, 2006; Hernandez-Lobato & Hernandez-Lobato,
2016), local Gaussian approximations to the likelihood are fitted iteratively to minimize KL diver-
gence from the true posterior.
Augmentation approaches. Augmentation-based approaches introduce auxiliary random vari-
ables such that the original model is recovered when marginalized out. Girolami & Rogers (2006)
propose a Gaussian augmentation for multinomial probit regression. Linderman et al. (2015) uti-
lize Polya-Gamma augmentation (Polson et al., 2013) and a stick-breaking construction to decom-
pose a multinomial distribution into a product of binomials. Galy-Fajou et al. (2020) propose a
logistic-softmax likelihood for classification and uses Gamma and Poisson augmentation in addition
to Polya-Gamma augmentation in order to perform inference.
2
Published as a conference paper at ICLR 2021
2.2	Few-shot Classification
Meta-learning. A common approach to FSC is meta-learning, which seeks to learn a strategy
to update neural network parameters when faced with a novel learning task. The Meta-learner
LSTM (Ravi & Larochelle, 2017) learns a meta-level LSTM to recurrently output a new set of pa-
rameters for a base learner. MAML (Finn et al., 2017) learns initializations of deep neural networks
that perform well on task-specific losses after one or a few steps of gradient descent by backpropa-
gating through the gradient descent procedure itself. LEO (Rusu et al., 2019) performs meta-learning
in a learned low-dimensional latent space from which the parameters of a classifier are generated.
Metric learning. Metric learning approaches learn distances such that input examples can be
meaningfully compared. Siamese Networks (Koch, 2015) learn a shared embedding network along
with a distance layer for computing the probability that two examples belong to the same class.
Matching Networks (Vinyals et al., 2016) uses a nonparametric classification in the form of atten-
tion over nearby examples, which can be interpreted as a form of soft k-nearest neighbors in the
embedding space. Prototypical Networks (Snell et al., 2017) make predictions based on distances to
nearest class centroids. Relation Networks (Sung et al., 2018) instead learn a more complex neural
network distance function on top of the embedding layer.
Bayesian Few-shot Classification. More recently, Bayesian FSC approaches that attempt to infer
a posterior over task-specific parameters have appeared. Grant et al. (2018) reinterpret MAML
as an approximate empirical Bayes algorithm and propose LLAMA, which optimizes the Laplace
approximation to the marginal likelihood. Bayesian MAML (Yoon et al., 2018) instead uses Stein
Variational Gradient Descent (SVGD) (Liu & Wang, 2016) to approximate the posterior distribution
over model parameters. VERSA (Gordon et al., 2019) uses amortized inference networks to obtain
an approximate posterior distribution over task-specific parameters. ABML (Ravi & Beatson, 2019)
uses a few steps of Bayes by Backprop (Blundell et al., 2015) on the support set to produce an
approximate posterior over network parameters. CNAPs (Requeima et al., 2019) modulate task-
specific Feature-wise Linear Modulation (FiLM) (Perez et al., 2018) layer parameters as the output
of an adaptation network that takes the support set as input.
GPs for Few-shot Learning. There have been relatively few works applying GPs to few-shot
learning. Tossou et al. (2020) consider Gaussian processes in the context of few-shot regression
with Gaussian likelihoods. Deep Kernel Transfer (DKT) (Patacchiola et al., 2020) uses Gaussian
processes with least squares classification to perform few-shot classification and learns covariance
functions parameterized by deep neural networks. More recently, Titsias et al. (2020) applies GPs to
meta-learning by maximizing the mutual information between the query set and a latent representa-
tion of the support set.
3	Background
In this section We first review Polya-Gamma augmentation for binary classification and the one-vs-
each approximation before we introduce our method in Section 4.
3.1	pOlya-Gamma Augmentation
The Polya-Gamma augmentation scheme was originally introduced to address Bayesian inference
in logistic models (Polson et al., 2013). Suppose we have a vector of logits ψ ∈ RN with corre-
sponding binary labels y ∈ {0, 1}N. The logistic likelihood is
N	N (eψi)yi
p(ylψ) = ∏σ(ψi)yi(I- σ(ψi))1-yi = ∏ 1 + eψi,	(I)
i=1	i=1	e
where σ(∙) is the logistic sigmoid function. Let the prior over ψ be Gaussian: p(ψ) = N(ψ∣μ, Σ).
In Bayesian inference, we are interested in the posterior p(ψ∣y) a p(y∣ψ)p(ψ) but the form of
(1) does not admit analytic computation of the posterior due to non-conjugacy. The main idea of
POlya-Gamma augmentation is to introduce auxiliary random variables ω to the likelihood such
that the original model is recovered when ω is marginalized out: p(y∣ψ) = /p(ω)p(y∣ψ, ω) dω.
3
Published as a conference paper at ICLR 2021
Conditioned on ω 〜 PG(1,0), the batch likelihood is proportional to a diagonal Gaussian (See
Section A for a full derivation):
N
p(y∣ψ, ω) H Y e-ωiψi /2eκiψi H N(Ω-1κ | ψ, Ω-1),	(2)
i=1
where Ki = yi 一 1/2 and Ω = diag(ω). The conditional distribution over ψ given y and ω is now
tractable:
p(ψ∣y, ω) H p(y∣ψ, ω)p(ψ) hN(Ψ∣Σ(Σ 1μ + κ),Σ),	(3)
where Σ = (Σ-1 + Ω)-1. The conditional distribution of ω given ψ and y can also be easily
computed:
p(3i∖yi,ψi) H PG(ωi∣1,0)e-ωiψ2/2 H PG(ω∕1,ψi),	(4)
where the last expression follows from the exponential tilting property of Polya-Gamma random
variables. This suggest a Gibbs sampling procedure in which iterates ω(t) 〜 p(ω∣y, ψ(t-1)) and
ψ(t) 〜p(ψ∣X, y, ω(t)) are drawn sequentially until the Markov chain reaches its stationary distri-
bution, which is thejoint posterior p(ψ, ω∣y). Fortunately, efficient samplers for the Polya-Gamma
distribution have been developed (Windle et al., 2014) to facilitate this.
3.2 One-vs-Each Approximation to Softmax
The one-vs-each (OVE) approximation (Titsias, 2016) was formulated as a lower bound to the soft-
max likelihood in order to handle classification over a large number of output classes, where com-
putation of the normalizing constant is prohibitive. We employ the OVE approximation not to deal
with extreme classification, but rather due to its compatibility with Polya-Gamma augmentation, as
we shall soon see. The one-vs-each approximation can be derived by first rewriting the softmax
likelihood as follows:
efi	1
p(y = i | f) , j = 1 + Pj=i e-(fi-fj),	⑸
where f , (f1, . . . , fC)> are the logits. Since in general Qk(1 + αk) ≥ (1 + Pk αk) for αk ≥ 0,
the softmax likelihood (5) can be bounded as follows:
p(y = i |f) ≥ Y1 + e-(")= Y σ(fi- fj )，	4 * (6)
j6=i	j6=i
which is the OVE lower bound. This expression avoids the normalizing constant and factorizes
into a product of pairwise sigmoids, which is amenable to Polya-Gamma augmentation for tractable
inference.
4 ONE-VS-EACH POLYA-GAMMA GPS
In this section, we first show how the one-vs-each (OVE) approximation can be interpreted as a
pairwise composite likelihood. We then we introduce our method for GP-based Bayesian few-shot
classification, which brings together OVE and Polya-Gamma augmentation in a novel combination.
4.1 OVE as a Composite Likelihood
Titsias (2016) showed that the OVE approximation shares the same global optimum as the softmax
maximum likelihood, suggesting a close relationship between the two. We show here that in fact
OVE can be interpreted as a pairwise composite likelihood version of the softmax. Composite
likelihoods (Lindsay, 1988; Varin et al., 2011) are a type of approximate likelihood often employed
when the exact likelihood is intractable or otherwise difficult to compute. Given a collection of
marginal or conditional events {E1, . . . , EK} and parameters f, a composite likelihood is defined
as:
K
LCL(f|y),YLk(f|y)wk,	(7)
k=1
4
Published as a conference paper at ICLR 2021
where Lk (f | y) α p(y ∈ Ek | f) and Wk ≥ 0 are arbitrary weights.
In order to make the connection to OVE, it will be useful to let the one-hot encoding of the label y
be denoted as y ∈ {0, 1}C. Define a set of C(C - 1)/2 pairwise conditional events Eij, one each
for all pairs of classes i 6= j, indicating the event that the model’s output matches the target label for
classes i andj conditioned on all the other classes:
p(y ∈ Eij | f)，p(yi,yj∖ y-j, f),	(8)
where ij denotes the set of classes not equal to either i or j . This expression resembles the pseu-
dolikelihood (Besag, 1975), but instead of a single conditional event per output site, the expression
in (8) considers all pairs of sites. Stoehr & Friel (2015) explored similar composite likelihood gen-
eralizations of the pseudolikelihood in the context of random fields.
Now suppose that yc = 1 for some class c ∈/ {i, j}. Then p(yi, yj | yij, f) = 1 due to the one-hot
constraint. Otherwise either yi = 1 or yj = 1. In this case, assume without loss of generality that
yi = 1 and yj = 0 and thus
efi
p(yi,yj | y-ij,f) = efi + efj = σ(f - fj).	⑼
The composite likelihood defined in this way with unit component weights is therefore
LovE(f |y) = YYp(yi,yj卜-j,f) = YYσ(fi - f 产.	(10)
i j 6=i	i j 6=i
Alternatively, we may simply write LOVE(f | y = i) = Qj6=i σ(fi - fj), which is identical to the
OVE bound (6).
4.2	GP Classification with the OVE Likelihood
We now turn our attention to GP classification. Suppose we have access to examples X ∈ RN ×D
with corresponding one-hot labels Y ∈ {0, 1}N×C, where C is the number of classes. We consider
the logits jointly as a single vector
f, (f11,...,fN1 ,f12,...,fN2,...,f1C,...,fNC)>	(11)
and place an independent GP prior on the logits for each class: fc(x) 〜 GP(m(x),k(x,x0)).
Therefore We have p(f |X) = N(f ∣μ, K), where μ = m(xi) and K is block diagonal with Kj =
k(xi, xj) for each block Kc.
The Polya-Gamma integral identity used to derive (2) does not have a multi-class analogue and thus
a direct application of the augmentation scheme to the softmax likelihood is nontrivial. Instead, we
propose to directly replace the softmax with the OVE-based composite likelihood function from (10)
with unit weights. The posterior over f when using OVE as the likelihood function can be expressed
as:
N
p(f∣X, y) (X p(f∣X)YY
σ(fiyi -fic0),	(12)
i=1 c0 6=yi
to which POlya-Gamma augmentation can be applied as we show in the next section. Our motiva-
tion for using a composite likelihood therefore differs from the traditional motivation, which is to
avoid the use of a likelihood function which is intractable to evaluate. Instead, we employ a com-
posite likelihood because it makes posterior inference tractable when coupled with POlya-Gamma
augmentation.
Prior work on Bayesian inference with composite likelihoods has shown that the composite posterior
is consistent under fairly general conditions for correctly specified models (Miller, 2019) but can
produce overly concentrated posteriors (Pauli et al., 2011; Ribatet et al., 2012) since each component
likelihood event is treated as independent when in reality there may be significant dependencies.
Nevertheless, we show in Section 5 that in practice our method exhibits competitive accuracy and
strong calibration relative to baseline few-shot learning algorithms. We leave further theoretical
analysis of the OVE composite posterior and its properties for future work.
5
Published as a conference paper at ICLR 2021
Compared to choices of likelihoods used by previous approaches, there are several reasons to prefer
OVE. Relative to the Gaussian augmentation approach of Girolami & Rogers (2006), Polya-Gamma
augmentation has the benefit of fast mixing and the ability ofa single value ofω to capture much of
the marginal distribution over function values1. The stick-breaking construction of Linderman et al.
(2015) induces a dependence on the ordering of classes, which leads to undesirable asymmetry.
Finally, the logistic-softmax likelihood of Galy-Fajou et al. (2020) requires three augmentations and
careful learning of the mean function to avoid a priori underconfidence (see Section F.1 for more
details).
4.3	Posterior Inference via Gibbs Sampling
We now describe how we perform tractable posterior inference in our model with Gibbs sampling.
Define the matrix A , OVE-MATRIX(Y) to be a CN × CN sparse block matrix with C row
partitions and C column partitions. Each block Acc0 is a diagonal N × N matrix defined as follows:
Acc0，diag(Y∙c0) - l[c = c0]In,	(13)
where Y© denotes the c0th column of Y. Now the binary logit vector ψ，Af ∈ RCN will have
entries equal to fiyi - fic for each unique combination of c and i, of which there are CN in total.
The OVE composite likelihood can now be written as L(ψ∣Y) = 2n QNC σ(ψj), where the 2n
term arises from the N cases in which ψj = 0 due to comparing the ground truth logit with itself.
Analogous to (2), the likelihood of ψ conditioned on ω andY is proportional to a diagonal Gaussian:
NC
L(ψ∣Y, ω) H Y e-ωjψ/2eκjψ H N(Ω-1κ∣ψ, Ω-1),	(14)
j =1
where Kj = 1/2 and Ω = diag(ω). By exploiting the fact that ψ = Af, we can express the
likelihood in terms of f and write down the conditional composite posterior as follows:
p(f|X, Y, ω) H N(Ω-1κ∣Af, Ω-1)N(f ∣μ, K) H N(f∣Σ(K-1μ + A>κ), Σ),	(15)
where Σ = (K-1 + A>ΩA)-1, which is an expression remarkably similar to (3). Analogous to
(4), the conditional distribution over ω given f and the data becomes p(ω∣y, f) = PG(ω∣1, Af).
The primary computational bottleneck of posterior inference lies in sampling f from (15). Since
Σ is a CN X CN matrix, a naive implementation has complexity O(C3N3). By utilizing the
matrix inversion lemma and Gaussian sampling techniques summarized in (Doucet, 2010), this can
be brought down to O(CN3). Details may be found in Section B.
4.4	Learning Covariance Hyperparameters for Few-shot Classification
We now describe how we apply OVE POlya-Gamma augmented GPs to few-shot classification.
We assume the standard episodic few-shot setup in which one observes a labeled support set S =
(X, Y). Predictions must then be made for a query example (x*, y*). We consider a zero-mean
GP prior over the class logits fc(x)〜GP(0, kθ(x, x0)), where θ are learnable parameters of our
covariance function. These could include traditional hyperparameters such as lengthscales or the
weights ofa deep neural network as in deep kernel learning (Wilson et al., 2016).
We consider two objectives for learning hyperparameters of the covariance function: the marginal
likelihood (ML) and the predictive likelihood (PL). Marginal likelihood measures the likelihood of
the hyperparameters given the observed data and is intuitively appealing from a Bayesian perspec-
tive. On the other hand, many standard FSC methods optimize for predictive likelihood on the query
set (Vinyals et al., 2016; Finn et al., 2017; Snell et al., 2017). Both objectives marginalize over latent
functions, thereby making full use of our Bayesian formulation.
The details of these objectives and how we compute gradients can be found in Section C. Our
learning algorithm for both marginal and predictive likelihood may be found in Section D. Details
of computing the posterior predictive distribution p(y* |x*, X, Y, ω) may be found in Section E.
Finally, details of our chosen “cosine” kernel may be found in Section H.
1See in particular Appendix C of (Linderman et al., 2015) for a detailed explanation of this phenomenon.
6
Published as a conference paper at ICLR 2021
5 Experiments
In this section, we present our results on few-shot classification both in terms of accuracy and un-
certainty quantification. Additional results comparing the one-vs-each composite likelihood to the
softmax, logistic softmax, and Gaussian likelihoods may be found in Section F.
One of our aims is to compare methods based on uncertainty quantification. We therefore developed
new benchmark evaluations and tasks: few-shot calibration, robustness, and out-of-episode detec-
tion. In order to empirically compare methods, we could not simply borrow the accuracy results
from other papers, but instead needed to train each of these baselines ourselves. For all baselines
except Bayesian MAML, ABML, and Logistic Softmax GP, we ran the code from (Patacchiola
et al., 2020) and verified that the accuracies matched closely to their reported results. We have made
PyTorch code for our experiments publicly available2.
5.1	Few-shot Classification
For our few-shot classification experiments, we follow the training and evaluation protocol of Patac-
chiola et al. (2020). We train both 1-shot and 5-shot versions of our model in four different settings:
Caltech-UCSD Birds (CUB) (Wah et al., 2011), mini-Imagenet with the split proposed by Ravi &
Larochelle (2017), as well as two cross-domain transfer tasks. The first transfer task entails training
on mini-ImageNet and testing on CUB, and the second measures transfer from Omniglot (Lake et al.,
2011) to EMNIST (Cohen et al., 2017). Experimental details and an overview of the baselines we
used can be found in Section G. Classification results are shown in Table 1 and 2. We find that our
proposed Polya-Gamma OVE GPs yield strong classification results, outperforming the baselines in
five of the eight scenarios.
Table 1: Average accuracy and standard deviation (percentage) on 5-way FSC. Baseline results
(through DKT) are from Patacchiola et al. (2020). Evaluation is performed on 3,000 randomly
generated test episodes. Standard deviation for the remaining methods are computed by averaging
over 5 batches of 600 episodes with different random seeds. The best results are highlighted in bold.
Method	CUB		mini-ImageNet	
	1-shot	5-shot	1-shot	5-shot
Feature Transfer	46.19 ± 0.64	68.40 ± 0.79	39.51 ± 0.23	60.51 ± 0.55
Baseline++	61.75 ± 0.95	78.51 ± 0.59	47.15 ± 0.49	66.18 ± 0.18
MatchingNet	60.19 ± 1.02	75.11 ± 0.35	48.25 ± 0.65	62.71 ± 0.44
ProtoNet	52.52 ± 1.90	75.93 ± 0.46	44.19 ± 1.30	64.07 ± 0.65
RelationNet	62.52 ± 0.34	78.22 ± 0.07	48.76 ± 0.17	64.20 ± 0.28
MAML	56.11 ± 0.69	74.84 ± 0.62	45.39 ± 0.49	61.58 ± 0.53
DKT + Cosine	63.37 ± 0.19	77.73 ± 0.26	48.64 ± 0.45	62.85 ± 0.37
Bayesian MAML	55.93 ± 0.71	72.87 ± 0.26	44.46 ± 0.30	62.60 ± 0.25
Bayesian MAML (Chaser)	53.93 ± 0.72	71.16 ± 0.32	43.74 ± 0.46	59.23 ± 0.34
ABML	48.80 ± 0.40	70.91 ± 0.32	40.88 ± 0.25	58.19 ± 0.17
Logistic Softmax GP + Cosine (ML)	60.23 ± 0.54	74.58 ± 0.25	46.75 ± 0.20	59.93 ± 0.31
Logistic Softmax GP + Cosine (PL)	60.07 ± 0.29	78.14 ± 0.07	47.05 ± 0.20	66.01 ± 0.25
OVE PG GP + Cosine (ML) [ours]	63.98 ± 0.43	77.44 ± 0.18	50.02 ± 0.35	64.58 ± 0.31
OVE PG GP + Cosine (PL) [ours]	60.11 ± 0.26	79.07 ± 0.05	48.00 ± 0.24	67.14 ± 0.23
5.2	Uncertainty Quantification through Calibration
We next turn to uncertainty quantification, an important concern for few-shot classifiers. When used
in safety-critical applications such as medical diagnosis, it is important for a machine learning sys-
tem to defer when there is not enough evidence to make a decision. Even in non-critical applications,
precise uncertainty quantification helps practitioners in the few-shot setting determine when a class
has an adequate amount of labeled data or when more labels are required, and can facilitate active
learning.
2https://github.com/jakesnell/ove-polya-gamma-gp
7
Published as a conference paper at ICLR 2021
Table 2: Average accuracy and standard deviation (percentage) on 5-way cross-domain FSC, with
the same experimental setup as in Table 1. Baseline results (through DKT) are from (Patacchiola
et al., 2020).
Omniglot→EMNIST	mini-ImageNet→ CUB
Method	1-shot	5-shot	1-shot	5-shot
Feature Transfer	64.22 ± 1.24	86.10 ± 0.84	32.77 ± 0.35	50.34 ± 0.27
Baseline++	56.84 ± 0.91	80.01 ± 0.92	39.19 ± 0.12	57.31 ± 0.11
MatchingNet	75.01 ± 2.09	87.41 ± 1.79	36.98 ± 0.06	50.72 ± 0.36
ProtoNet	72.04 ± 0.82	87.22 ± 1.01	33.27 ± 1.09	52.16 ± 0.17
RelationNet	75.62 ± 1.00	87.84 ± 0.27	37.13 ± 0.20	51.76 ± 1.48
MAML	72.68 ± 1.85	83.54 ± 1.79	34.01 ± 1.25	48.83 ± 0.62
DKT + Cosine	73.06 ± 2.36	88.10 ± 0.78	40.22 ± 0.54	55.65 ± 0.05
Bayesian MAML	63.94 ± 0.47	65.26 ± 0.30	33.52 ± 0.36	51.35 ± 0.16
Bayesian MAML (Chaser)	55.04 ± 0.34	54.19 ± 0.32	36.22 ± 0.50	51.53 ± 0.43
ABML	73.89 ± 0.24	87.28 ± 0.40	31.51 ± 0.32	47.80 ± 0.51
Logistic Softmax GP + Cosine (ML)	62.91 ± 0.49	83.80 ± 0.13	36.41 ± 0.18	50.33 ± 0.13
Logistic Softmax GP + Cosine (PL)	70.70 ± 0.36	86.59 ± 0.15	36.73 ± 0.26	56.70 ± 0.31
OVE PG GP + Cosine (ML) [ours]	68.43 ± 0.67	86.22 ± 0.20	39.66 ± 0.18	55.71 ± 0.31
OVE PG GP + Cosine (PL) [ours]	77.00 ± 0.50	87.52 ± 0.19	37.49 ± 0.11	57.23 ± 0.31
We chose several commonly used metrics for calibration. Expected calibration error (ECE) (Guo
et al., 2017) measures the expected binned difference between confidence and accuracy. Maximum
calibration error (MCE) is similar to ECE but measures maximum difference instead of expected
difference. Brier score (BRI) (Brier, 1950) is a proper scoring rule computed as the squared error
between the output probabilities and the one-hot label. For a recent perspective on metrics for
uncertainty evaluation, please refer to Ovadia et al. (2019). The results for representative approaches
on 5-shot, 5-way CUB can be found in Figure 1. Our OVE PG GPs are the best calibrated overall
across the metrics.
Confidence
Confidence	Confidence
Confidence	Confidence	Confidence
Figure 1: Reliability diagrams, expected calibration error (ECE), maximum calibration error (MCE),
and Brier Score (BRI) for 5-shot 5-way tasks on CUB (additional calibration results can be found in
Appendix I). Metrics are computed on 3,000 random tasks from the test set. The last two plots are
our proposed method.
5.3	Robustness to Input Noise
Input examples for novel classes in FSC may have been collected under conditions that do not
match those observed at training time. For example, labeled support images in a medical diagnosis
application may come from a different hospital than the training set. To mimic a simplified version
of this scenario, we investigate robustness to input noise. We used the Imagecorruptions package
(Michaelis et al., 2019) to apply Gaussian noise, impulse noise, and defocus blur to both the support
set and query sets of episodes at test time and evaluated both accuracy and calibration. We used
corruption severity of 5 (severe) and evaluated across 1,000 randomly generated tasks on the three
8
Published as a conference paper at ICLR 2021
datasets involving natural images. The robustness results for Gaussian noise are shown in Figure 2.
Full quantitative results tables for each noise type may be found in Section J. We find that in general
Bayesian approaches tend to be robust due to their ability to marginalize over hypotheses consistent
with the support labels. Our approach is one of the top performing methods across all settings.
CUB	mini-lmagenet	mini-lmagenet → CUB
⅛≡∣i ⅛ii≡
FeatureTransfer
Baseline++
Matching Networks
Prototypical Networks
ReIationNet
DKT + Cosine
Bayesian MAML
Bayesian MAML (Chaser)
Logistic Softmax GP ML + Cosine
Logistic Softmax GP PL + Cosine
OVE PG GP ML + Cosine
OVE PG GP PL + Cosine
Figure 2: Accuracy (↑) and Brier Score Q) When corrupting both support and query with Gaussian
noise on 5-way 5-shot tasks. Quantitative results may be found in Appendix J.
5.4 Out-of-Episode Detection
Finally, we measure performance on out-of-episode detection, another application in which uncer-
tainty quantification is important. In this experiment, we used 5-way, 5-shot support sets at test time
but incorporated out-of-episode examples into the query set. Each episode had 150 query examples:
15 from each of 5 randomly chosen in-episode classes and 15 from each of 5 randomly chosen out-
of-episode classes. We then computed the AUROC of binary outlier detection using the negative of
the maximum logit as the score. Intuitively, if none of the support classes assign a high logit to the
example, it can be classified as an outlier. The results are shown in Figure 3. Our approach generally
performs the best across the datasets.
CUB	mini-lmagenet
Feature Transfer
. ._______. c. ∣n Baseline++
mιnι-lmagenet→CUB - .a t κ. „ G
Matching Networks
Prototypical Networks
ReIationNet
DKT + Cosine
Bayesian MAML
-------------- ∣.	■ Bayesian MAML (Chaser)
∣L∣∣∣⅛∣ I I - Logistic Softmax GP ML + Cosine
Logistic Softmax GP PL + Cosine
OVE GP ML + Cosine
OVE GP PL + Cosine
Figure 3: Average AUROC (↑) for out-of-episode detection. The AUC is computed separately for
each episode and averaged across 1,000 episodes. Bars indicate a 95% bootstrapped confidence
interval.
6 Conclusion
In this work, we have proposed a Bayesian few-shot classification approach based on Gaussian pro-
cesses. Our method replaces the ordinary softmax likelihood with a one-vs-each pairwise composite
likelihood and applies Polya-Gamma augmentation to perform inference. This allows us to model
class logits directly as function values and efficiently marginalize over uncertainty in each few-shot
episode. Modeling functions directly enables our approach to avoid the dependence on model size
that posterior inference in weight-space based models inherently have. Our approach compares
favorably to baseline FSC methods under a variety of dataset and shot configurations, including
dataset transfer. We also demonstrate strong uncertainty quantification, robustness to input noise,
and out-of-episode detection. We believe that Bayesian modeling is a powerful tool for handling
uncertainty and hope that our work will lead to broader adoption of efficient Bayesian inference in
the few-shot scenario.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We would like to thank Ryan Adams, Ethan Fetaya, Mike Mozer, Eleni Triantafillou, Kuan-Chieh
Wang, and Max Welling for helpful discussions. JS also thanks SK T-Brain for supporting him on
an internship that led to precursors of some ideas in this paper. Resources used in preparing this
research were provided, in part, by the Province of Ontario, the Government of Canada through
CIFAR, and companies sponsoring the Vector Institute (https://www.vectorinstitute.
ai/partners). This project is supported by NSERC and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract
number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors and should not be interpreted as necessarily
representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC,
or the U.S. Government.
References
James H. Albert and Siddhartha Chib. Bayesian analysis of binary and polychotomous response
data. Journal ofthe American Statistical Association, 88(422):669-679, June 1993.
Julian Besag. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society: Series
D (The Statistician), 24(3):179-195, 1975.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In International Conference on Machine Learning, 2015.
Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,
78(1):1-3, 1950.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classification. In International Conference on Learning Representations, 2019.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. EMNIST: Extending
MNIST to handwritten letters. In 2017 International Joint Conference on Neural Networks
(IJCNN), pp. 2921-2926, 2017.
Randal Douc, Eric Moulines, and David Stoffer. Nonlinear Time Series: Theory, Methods and
Applications with R Examples. CRC Press, 2014.
Arnaud Doucet. A Note on Efficient Conditional Simulation of Gaussian Dis-
tributions.	2010. URL https://www.cs.ubc.ca/~arnaud/doucet_
simulationconditionalgaussian.pdf.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1126-1135, International Convention Centre, Sydney, Australia, August 2017. PMLR.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 31, pp. 9516-9527. Curran Associates, Inc.,
2018.
R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2):
179-188, 1936.
Theo Galy-Fajou, Florian Wenzel, Christian Donner, and Manfred Opper. Multi-class Gaussian
process classification made conjugate: Efficient inference via data augmentation. In Ryan P.
Adams and Vibhav Gogate (eds.), Proceedings of the 35th Uncertainty in Artificial Intelligence
Conference, volume 115 of Proceedings of Machine Learning Research, pp. 755-765, Tel Aviv,
Israel, July 2020. PMLR.
10
Published as a conference paper at ICLR 2021
Mark Girolami and Simon Rogers. Variational Bayesian multinomial probit regression with Gaus-
Sian process priors. Neural COmPutatiOn,18(8):1790-1817, August 2006.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard Turner. Meta-
learning probabilistic inference for prediction. In InternatiOnal COnference On Learning RePre-
sentatiOns, 2019.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical Bayes. In InternatiOnal COnference On Learning RePresenta-
tiOns, 2018.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), PrOceedings Of the 34th InternatiOnal
COnference On Machine Learning, volume 70 of PrOceedings Of Machine Learning Research, pp.
1321-1330, International Convention Centre, Sydney, Australia, August 2017. PMLR.
Daniel Hernandez-Lobato and Jose Miguel Hernandez-Lobato. Scalable Gaussian process classifi-
cation via expectation propagation. In Arthur Gretton and Christian C. Robert (eds.), PrOceedings
Of the 19th InternatiOnal COnference On Artificial Intelligence and Statistics, volume 51 of PrO-
ceedings Of Machine Learning Research, pp. 168-176, Cadiz, Spain, May 2016. PMLR.
Nathan Hilliard, LaWrence Phillips, Scott Howland, Artem Yankov, Courtney D. Corley, and
Nathan O. Hodas. Few-Shot Learning with Metric-Agnostic Conditional Embeddings.
arXiv:1802.04376 [cs, stat], February 2018.
Yehuda Hoffman and Erez Ribak. Constrained realizations of Gaussian fields-A simple algorithm.
The AstrOPhysical JOurnal, 380:L5-L8, 1991.
Hyun-Chul Kim and Zoubin Ghahramani. Bayesian Gaussian process classification with the EM-EP
algorithm. IEEE TransactiOns On Pattern Analysis and Machine Intelligence, 28(12):1948-1959,
2006.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In InternatiOnal
COnference On Learning RePresentatiOns, 2015.
Diederik P. Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural InfOrmatiOn PrOcessing Systems, volume 28, pp. 2575-2583. Curran Associates, Inc.,
2015.
Gregory Koch. Siamese Neural NetwOrks fOr One-ShOt Image RecOgnitiOn. Master’s Thesis, Uni-
versity of Toronto, 2015.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of
simple visual concepts. In PrOceedings Of the Annual Meeting Of the COgnitive Science SOciety,
volume 33, 2011.
Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made
easy: Stick-breaking with the Polya-Gamma augmentation. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett (eds.), Advances in Neural InfOrmatiOn PrOcessing Systems, vol-
ume 28, pp. 3456-3464. Curran Associates, Inc., 2015.
Bruce G. Lindsay. Composite Likelihood Methods. COntemPOrary Mathematics, 80:221-239, 1988.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural InfOrmatiOn PrOcessing Systems, volume 29, pp. 2378-2386. Curran Associates, Inc.,
2016.
Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On
sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In
Arthur Gretton and Christian C. Robert (eds.), PrOceedings Of the 19th InternatiOnal COnference
On Artificial Intelligence and Statistics, volume 51 of PrOceedings Of Machine Learning Research,
pp. 231-239, Cadiz, Spain, May 2016. PMLR.
11
Published as a conference paper at ICLR 2021
Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexan-
der S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking Robustness in Object De-
tection: Autonomous Driving when Winter is Coming. In NeurIPS 2019 Machine Learning for
Autonomous Driving Workshop, 2019.
Jeffrey W. Miller. Asymptotic normality, concentration, and coverage of generalized posteriors.
arXiv:1907.09611 [math, stat], July 2019.
Thomas Peter Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis,
Massachusetts Institute of Technology, 2001.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty? Evalu-
ating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. dAlche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems ,volume 32, pp.13991-14002. Curran Associates, Inc., 2019.
Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O’Boyle, and Amos Storkey.
Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels. In Advances in Neural
Information Processing Systems, 2020.
Francesco Pauli, Walter Racugno, and Laura Ven. Bayesian composite marginal likelihoods. Statis-
tica Sinica, pp. 17, 2011.
Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual
reasoning with a general conditioning layer. Proceedings of the AAAI Conference on Artificial
Intelligence, 32(1), April 2018.
Nicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian inference for logistic models
using Polya-Gamma latent variables. Journal of the American Statistical Association, 108(504):
1339-1349, December 2013.
Viraj Uday Prabhu. Few-Shot Learning For Dermatological Disease Diagnosis. Master’s Thesis,
Georgia Institute of Technology, 2019.
Sachin Ravi and Alex Beatson. Amortized Bayesian meta-learning. In International Conference on
Learning Representations, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations, 2017.
James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast
and flexible multi-task classification using conditional neural adaptive processes. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 32, pp. 7959-7970. Curran Associates, Inc., 2019.
Mathieu Ribatet, Daniel Cooley, and Anthony C. Davison. Bayesian inference from composite
likelihoods, with an application to spatial extremes. Statistica Sinica, 2012.
Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. Journal of Machine
Learning Research, 5(Jan):101-141, 2004.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference
on Learning Representations, 2019.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30, pp. 4077-4087. Curran
Associates, Inc., 2017.
Julien Stoehr and Nial Friel. Calibration of conditional composite likelihood for Bayesian inference
on Gibbs random fields. In International Conference on Artificial Intelligence and Statistics,
2015.
12
Published as a conference paper at ICLR 2021
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional Variational Bayesian
Neural Networks. In International Conference on Learning Representations, 2019.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In David
van Dyk and Max Welling (eds.), Proceedings of the Twelth International Conference on Artificial
Intelligence and Statistics, volume 5 of Proceedings ofMachine Learning Research, pp. 567-574,
Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, April 2009. PMLR.
Michalis K. Titsias. One-vs-each approximation to softmax for scalable estimation of probabili-
ties. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 29, pp. 4161-4169. Curran Associates, Inc., 2016.
Michalis K. Titsias, Sotirios Nikoloutsopoulos, and Alexandre Galashov. Information Theoretic
Meta Learning with Gaussian Processes. arXiv:2009.03228 [cs, stat], October 2020.
Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, and Alexandre Lacoste.
Adaptive Deep Kernel Learning. arXiv:1905.12131 [cs, stat], December 2020.
Cristiano Varin, Nancy Reid, and David Firth. An overview of composite likelihood methods. Insti-
tute of Statistical Science, Academia Sinica, 2011.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 29, pp. 3630-3638. Curran
Associates, Inc., 2016.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-
UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of
Technology, 2011.
Kuan-Chieh Wang, Jixuan Wang, and Khai Truong. Customizable Facial Gesture Recognition For
Improved Assistive Technology. In ICLR AI for Social Good Workshop, 2019.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-
Independent Weight Perturbations on Mini-Batches. In International Conference on Learning
Representations, 2018.
Christopher K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342-1351, 1998.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning.
In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Con-
ference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning
Research, pp. 370-378, Cadiz, Spain, May 2016. PMLR.
Jesse Windle, Nicholas G. Polson, and James G. Scott. Sampling Polya-Gamma random variates:
Alternate and approximate techniques. arXiv:1405.0506 [stat], May 2014.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31, pp. 7332-7342. Curran Associates, Inc., 2018.
13
Published as a conference paper at ICLR 2021
A Derivation of POLYA-GAMMA Augmented Logistic Likelihood
in this section, we show the derivation for the augmented logistic likelihood presented in Section 3.1.
First, recall the logistic likelihood:
N	N (eψi)yi
p(ylψ) = ∏σ(ψi)yi(I- σ(ψi))1-yi = ∏ ψ~~Φ~,
1 + eψi
i=1	i=1
(16)
where σ(∙) is the logistic sigmoid function. We have a Gaussian prior p(ψ) = N(ψ∣μ, Σ) and
introduce Polya-Gamma auxiliary random variables ω to the likelihood such that the original model
is recovered when ω is marginalized out: p(y∣ψ) = /p(ω)p(y∣ψ, ω) dω.
The Polya-Gamma distribution ω 〜 PG(b, C) can be written as an infinite convolution of Gamma
distributions:
D ɪ X∞	Ga(b,1)
2	2π2	(k — 1/2)2 + c2∕(4π2).
The following integral identity holds for b > 0:
(1+ψb = 2-beκψ /∞ e-ωψ^p(ω) dω
(17)
(18)
where K = a — b/2 and ω 〜PG(b, 0). Specifically, when a = y and b = 1, we recover an individual
term of the logistic likelihood (16):
(eψ)y	1	∞	2
P(y∣Ψ) = 1k+⅛ = 2eκψ / e-ωψ /2P(ω) dω,	(19)
where K = y — 1/2 and ω 〜 PG(1,0). Conditioned on ω, the batch likelihood is proportional to a
diagonal Gaussian:
N
p(y∣ψ, ω) H Y e-ωiψi /2eκiψi H N(Ω-1κ | ψ, Ω-1),	(20)
i=1
where Ki = yi — 1/2 and Ω = diag(ω). The conditional distribution over ψ given y and ω is now
tractable:
p(ψ∣y, ω) H p(y∣ψ, ω)p(ψ) hN(ψ∣Σ(Σ 1μ + κ), Σ),	(21)
where Σ = (Σ-1 + Ω)-1.
B Efficient Gibbs Sampling
The Gibbs conditional distribution over f is given by:
p(f|X, y, ω) = N(f∣Σ(K-1μ + A>κ), Σ),	(22)
where Σ = (K-1+A>ΩA)-1. Naively sampling from this distribution requires O(C3N3) compu-
tation since Σ is a CN X CN matrix. Here we describe a method for sampling from this distribution
that requires O(CN 3) computation instead.
First, we note that (22) can be interpreted as the conditional distribution p(f |z = Ω-1κ) resulting
from the following marginal distributionp(f) and conditionalp(z|f):
p(f )= N(f∣μ, K)	(23)
p(z∣f)= N(z|Af, Ω-1),	(24)
where we have made implicit the dependence on X, Y, and ω for brevity of notation. Equivalently,
the distribution over f and z can be represented by the partitioned Gaussian
f 〜N μ	K	KA1	1
Z ”	Aμ	, AK	AKA> + Ω-1
14
Published as a conference paper at ICLR 2021
The conditional distribution p(f |z) is given as:
p(f |z) = N(f ∣Σ(K-1μ + A>Ωz), Σ),	(26)
where Σ = (KT + A>ΩA)-1. Note thatp(f |z = Ω-1κ) recovers our desired Gibbs conditional
distribution from (22).
An efficient approach to conditional Gaussian sampling is due to Hoffman & Ribak (1991) and
described in greater clarity by Doucet (2010). The procedure is as follows:
1.	Sample fo 〜p(f) and zo 〜p(z∣f).
2.	Return f = fo + KA>(AKA> + Ω-1)-1(Ω-1κ - z0) as the sample fromp(f∣z).
K is block diagonal and thus sampling from p(f) requires O(CN3) time. Af can be computed in
O(CN) time, since each entry is the difference between fiyi and fic for some i and c. Overall, step
1 requires O(CN3) time.
We now show how to compute f from step 2in O(CN3) time. We first expand (AKA> + Ω-1)-1:
(AKA> + Ω-1)-1 = Ω - ΩA(K-1 + A>ΩA)-1 A>Ω	(27)
We substitute into the expression for ff:
f = fo + KA>(Ω - ΩA(K-1 + A>ΩA)-1A>Ω)(Ω-1κ - zo)	(28)
=fo + KA>Ω(Ω-1κ - zo) - KA>ΩA(K-1 + A>ΩA)-1 A>Ω(Ω-1κ - zo)	(29)
=fo + Kv - KA>ΩA(K-1 + A>ΩA)-1v,	(30)
where we have defined V，A>Ω(Ω-1κ - zo).
(31)
(32)
(33)
(34)
(35)
(36)
Now let d，(d1,..., dN, d2,..., d$,..., df,..., d%)>, where d = Yic Pco ωc. Define Yt to
be the CN X N matrix produced by vertically stacking diag(YTc), and let Wt be the CN X N matrix
produced by vertically stacking diag((ωc,..., ωN)>). A>ΩA may then be written as follows:
A>ΩA = D - SPS>, where
D = Ω + diag(d),
S =	Yt	Wt	,
P	0N	IN
IN	0N	.
Substituting (31) into (30):
f = fo + Kv - KA>ΩA(K-1 + D - SPS>)-1v.
Now we expand (K-1 + D - SPS>)-1 :
(K-1 +D- SPS>)-1 = E - ES(S>ES - P-1)-1S>E,
where E = (K-1 + D)-1 = K(K+D-1)-1D-1 is a block-diagonal matrix that can be computed
in O(CN3) time, since D is diagonal and K is block diagonal. Now, substituting (36) back into
(35),
f = fo + Kv - KA>ΩAEv + KA>ΩAES(S>ES - P-I)-1S>Ev.	(37)
Note that (S>ES - P-1)-1 is a 2N X 2N matrix and thus can be inverted in O(N3) time. The
overall complexity is therefore O(CN3).
C Marginal Likelihood and Predictive Likelihood Objectives
Marginal Likelihood (ML). The log marginal likelihood can be written as follows:
Lml(Θ; X, Y) , logPθ(YX) = log /p(ω)pθ(Y∣ω, X) dω
= log /p(ω) / L(f∣Y, ω)pθ(f|X) df dω
(38)
15
Published as a conference paper at ICLR 2021
The gradient of the log marginal likelihood can be estimated by posterior samples ω 〜pθ(ω∣X, Y).
In practice, we use a stochastic training objective based on samples ofω from Gibbs chains. We use
Fisher’s identity (Douc et al., 2014) to derive the following gradient estimator:
Vθ LML
/Pθ(ω∣X, Y)Vθ logPθ(Y∣ω, X) dω ≈
1M
M ∑ Vθlogpθ(Y|X,ω(m)),
m=1
(39)
where ω(1), . . . , ω(M) are samples from the posterior Gibbs chain. As suggested by Patacchiola
et al. (2020), who applied GPs to FSC via least-squares classification, we merge the support and
query sets during learning to take full advantage of the available data within each episode.
Predictive Likelihood (PL). The log predictive likelihood for a query example x* is:
Lpl(Θ; X, Y, x*
y*) , logpθ(y* |x*, X, Y) = log
p(ω)pθ(y* |x*, X,Y, ω) dω.
(40)
We use an approximate gradient estimator again based on posterior samples ofω:
VθLPL ≈
/Pθ(ω∣X, Y)Vθ logpθ(y* |x*, X, Y) dω ≈
1M
M E Vθ logPθ(y*∣x*, X, Y,ω(m)).
m=1
(41)
We note that this is not an unbiased estimator of the gradient, but find it works well in practice.
D Learning Algorithm
Our learning algorithm for both marginal and predictive likelihood is summarized in Algorithm 1.
Algorithm 1 One-VS-Each Polya-Gamma GP Learning
Input: Objective L ∈ {LML , LPL}, Task distribution T, number of parallel Gibbs chains M,
number of steps T , learning rate η.
Initialize hyperparameters θ randomly.
repeat
Sample S = (X, Y), Q = (X*, Y*)〜T
if L = LML then
X — X ∪ X*, Y - Y ∪ Y*
end if
A —OVE-MATRIX(Y)
for m = 1 to M do
ω(m) 〜PG(1,0), f0m) 〜pθ(f|X)
for t = 1 to T do
ψ” ― Aft-)
ωt(m) 〜 PG(1,ψt(m))
ftm 〜Pθ(f|X, Y, ω严)
end for
end for
if L = LML then
θ 一 θ + M PM=I Vθ logPθ(Y|X, ω铲)
else
θ 一 θ + M Pm=I Pj Vθ logPθ(y*j |x*j, S, ω铲)
end if
until convergence
E Posterior Predictive Distribution
The posterior predictive distribution for a query example x* conditioned on ω is:
p(y*|x*, X, Y, ω)
p(y*|f*)p(f*|x*, X, Y, ω) df*,
(42)
16
Published as a conference paper at ICLR 2021
where f* are the query example,s logits. The predictive distribution over f* can be obtained by
noting that ψ and the query logits are jointly Gaussian:
ψ ] Ak A AKAT + Ω-1	AK* D
f*]~N (0，[	(AK*)t	K** J b
(43)
where K* is the NC × C block diagonal matrix with blocks Kθ(X, x*) and K** is the C × C
diagonal matrix with diagonal entries kθ(x*, x*). The predictive distribution becomes:
p(f*∣x*, X, Y, ω) = N(f*∣μ*, Σ*), where
μ* = (AK*)t(AKAt + Ω-1)-1Ω-1κ and	(44)
Σ* = K** - (AK*)t(AKAt + Ω-1)-1AK*.
With p(f* |x* , X, Y, ω ) in hand, the integral in (42) can easily be computed numerically for each
class c by forming the corresponding OVE linear transformation matrix Ac and then performing 1D
Gaussian-Hermite quadrature on each dimension of N(ψ*|Acμ*, AcΣ*Ac>).
F Detailed C omparis on of Likelihoods
In this section we seek to better understand the behaviors of the softmax, OVE, logistic softmax,
and Gaussian likelihoods for classification. For convenience, we summarize the forms of these
likelihoods in Table 3.
Table 3: Likelihoods used in Section F.
Likelihood	L(f|y=c)
Softmax	exp(fc) Pc0 exP(fc0 )
Gaussian	YN(2 • l[c0 = c] - 1 | μ = fc0 ,σ2 = 1)
Logistic Softmax (LSM)	c0 σfc) Pc0 σ(fc0)
One-vs-Each (OVE)	Yσ(fc-fc0) c0 6=c
F.1 Histogram of Confidences
We sampled logits from f 〜N(0,1) and plotted a histogram and kernel density estimate of the
maximum output probability maxc p(y = c | f) for each of the likelihoods shown in Table 3, where
C = 5. The results are shown in Figure 4. Logistic softmax is a priori underconfident: it puts
little probability mass on confidence above 0.4. This may be due to the use of the sigmoid function
which squashes large values of f. Gaussian likelihood and OVE are a priori overconfident in that
they put a large amount of probability mass on confident outputs. Note that this is not a complete
explanation, because GP hyperparameters such as the prior mean or Gaussian likelihood variance
may be able to compensate for these imperfections to some degree. Indeed, we found it helpful to
learn a constant mean for the logistic softmax likelihood, as mentioned in Section G.2.
F.2 Likelihood Visualization
In order to visualize the various likelihoods under consideration, we consider a trivial classification
task with a single observed example. We assume that there are three classes (C = 3) and the single
example belongs to the first class (y = 1). We place the following prior on f = (f1, f2, f3)T:
0	1
p(f) =	N f μ =	0	, Σ =	0
0	0
000#!.
(45)
0
1
0
17
Published as a conference paper at ICLR 2021
Figure 4: Histogram and kernel density estimate of confidence for randomly generated function
samples fc 〜N(0,1). Normalized output probabilities were computed for C = 5 and a histogram
of maxc p(y = c|f) was computed for 50,000 randomly generated simulations.
In other words, the prior for f1 and f2 is a standard normal and f3 is clamped at zero (for ease of
visualization). The likelihoods are plotted in Figure 5 and the corresponding posteriors are plotted
in Figure 6.
(a) Softmax
(b) Gaussian
(c) Logistic Softmax
(d) One-vs-Each
Figure 5: Plot of L(f | y = 1), where f3 is clamped to 0. The Gaussian likelihood penalizes configu-
rations far away from (f1, f2) = (1, -1). Logistic softmax is much flatter compared to softmax and
has visibly different contours. One-vs-Each is visually similar to the softmax but penalizes (f1 , f2)
near the origin slightly more.
(a) Softmax
(b) Gaussian	(c) Logistic Softmax (d) One-vs-Each
Figure 6: Plot of posterior p(f | y = 1), where f3 is clamped to 0. The mode of each posterior
distribution is similar, but each differs slightly in shape. Gaussian is more peaked about its mode,
while logistic softmax is more spread out. One-vs-Each is similar to softmax, but is slightly more
elliptical.
18
Published as a conference paper at ICLR 2021
F.3 2D Iris Experiments
We also conducted experiments on a 2D version of the Iris dataset (Fisher, 1936), which contains
150 examples across 3 classes. The first two features of the dataset were retained (sepal length and
width). We used a zero-mean GP prior and an RBF kernel k(x, x0) = exp (-；d(x, x0)2) , where
d(∙, ∙) is Euclidean distance. We considered training set sizes with 1, 2, 3, 4, 5, 10, 15, 20, 25, and
30 examples per class. For each training set size, we performed GP inference on 200 randomly
generated train/test splits and compared the predictions across Gaussian, logistic softmax, and one-
vs-each likelihoods.
Predictions at a test point x* were made by applying the (normalized) likelihood to the posterior
predictive mean f*. The predictive probabilities for each likelihood is shown in Figure 7 for a
randomly generated train/test split with 30 examples per class. Test predictive accuracy, Brier score,
expected calibration error, and evidence lower bound (ELBO) results across various training set
sizes are shown in Figure 8.
The ELBO is computed by treating each likelihood’s posterior q(f |X, Y) as an approximation to
the softmax posterior p(f |X, Y).
ELBO(q) =Eq[logp(f|X)] +Eq[logp(Y|f)] -Eq[logq(f|X,Y)]
=logp(x) -KL(q(f|X,Y)||p(f|X,Y)).
Even though direct computation of the softmax posterior p(f |X, y) is intractable, computing the
ELBO is tractable. A larger ELBO indicates a lower KL divergence to the softmax posterior.
One-vs-Each performs well for accuracy, Brier score, and ELBO across the training set sizes. Gaus-
sian performs best on expected calibration error through 15 examples per class, beyond which one-
vs-each is better.
(a) Gaussian	(b) Logistic Softmax	(c) One-vs-Each
Figure 7: Training points (colored points) and maximum predictive probability for various like-
lihoods on the Iris dataset. The Gaussian likelihood produces more warped decision boundaries
than the others. Logistic softmax tends to produce lower confidence predictions, while one-vs-each
produces larger regions of greater confidence than the others.
(a) Accuracy
(b) Brier
(c) ECE
(d) ELBO
Figure 8: Comparison across likelihoods in terms of test predictive accuracy, Brier score, expected
calibration error (computed with 10 bins), and ELBO. Results are averaged over 200 randomly
generated splits for each training set size (1, 2, 3, 4, 5, 10, 15, 20, 25, and 30 examples per class).
Error bars indicate 95% confidence intervals.
19
Published as a conference paper at ICLR 2021
G	Few-shot Experimental Details
Here we provide more details about our experimental setup for our few-shot classification experi-
ments, which are based on the protocol of (Patacchiola et al., 2020).
G. 1 Datasets
We used the four dataset scenarios described below. The first three are the same used by Chen et al.
(2019) and the final was proposed by Patacchiola et al. (2020).
•	CUB. Caltech-UCSD Birds (CUB) (Wah et al., 2011) consists of 200 classes and 11,788
images. A split of 100 training, 50 validation, and 50 test classes was used (Hilliard et al.,
2018; Chen et al., 2019).
•	mini-Imagenet. The mini-Imagenet dataset (Vinyals et al., 2016) consists of 100 classes
with 600 images per class. We used the split proposed by Ravi & Larochelle (2017), which
has 64 classes for training, 16 for validation, and 20 for test.
•	mini-Imagenet→CUB. This cross-domain transfer scenario takes the training split of
mini-Imagenet and the validation & test splits of CUB.
•	Omniglot → EMNIST. We use the same setup as proposed by Patacchiola et al. (2020).
Omniglot (Lake et al., 2011) consists of 1,623 classes, each with 20 examples, and is aug-
mented by rotations of 90 degrees to create 6,492 classes, of which 4,114 are used for
training. The EMNIST dataset (Cohen et al., 2017), consisting of 62 classes, is split into
31 training and 31 test classes.
G.2 Few- shot Classification Baselines
Here we explain the few-shot baselines in greater detail.
•	Feature Transfer (Chen et al., 2019) involves first training an off-line classifier on the
training classes and then training a new classification layer on the episode.
•	Baseline++ (Chen et al., 2019) is similar to Feature Transfer except it uses a cosine dis-
tance module prior to the softmax during fine-tuning.
•	Matching Networks (Vinyals et al., 2016) can be viewed as a soft form of k-nearest neigh-
bors that computes attention and sums over the support examples to form a predictive dis-
tribution over classes.
•	Prototypical Networks (Snell et al., 2017) computes class means (prototypes) and forms
a predictive distribution based on Euclidean distance to the prototypes. It can be viewed as
a Gaussian classifier operating in an embedding space.
•	MAML (Finn et al., 2017) performs one or a few steps of gradient descent on the support
set and then makes predictions on the query set, backpropagating through the gradient
descent procedure. For this baseline, we simply quote the classification accuracy reported
by (Patacchiola et al., 2020).
•	RelationNet (Sung et al., 2018) rather than using a predefined distance metric as in Match-
ing Networks or Prototypical Networks instead learns a deep distance metric as the output
of a neural network that accepts as input the latent representation of both examples. It is
trained to minimize squared error of output predictions.
•	Deep Kernel Transfer (DKT) (Patacchiola et al., 2020) relies on least squares classifica-
tion (Rifkin & Klautau, 2004) to maintain tractability of Gaussian process posterior infer-
ence. In DKT, a separate binary classification task is formed for each class in one-vs-rest
fashion by treating labels in {-1, +1} as continuous targets. We include the results of DKT
with the cosine kernel as implemented by Patacchiola et al. (2020), which is parameterized
slightly differently from the version we used in (47):
kCkS(x, x0; θ, α, V) = Softplus(α) ∙ softplUS(V) ∙ ∣∣ gθ(X)Ilgθ(X L .	(46)
dktv ,	；	J PLkgθ(x)kkgθ(x0)k
20
Published as a conference paper at ICLR 2021
•	Bayesian MAML (Yoon et al., 2018) relies on Stein Variational Gradient Descent (SVGD)
(Liu & Wang, 2016) to get an approximate posterior distribution in weight-space. We com-
pare to both the non-chaser version, which optimizes cross-entropy of query predictions,
and the chaser version, which optimizes mean squared error between the approximate pos-
terior on the support set and the approximate posterior on the merged support & query set.
The non-chaser version is therefore related to predictive likelihood methods and the chaser
version is more analogous to the marginal likelihood methods. For the non-chaser version,
we used 20 particles and 1 step of adaptation at both train and test time. For the chaser
version, we also used 20 particles. At train time, the chaser took 1 step and the leader 1
additional step. At test time, we used 5 steps of adaptation. Due to the slow performance of
this method, we followed the advice of Yoon et al. (2018) and only performed adaptation
on the final layer of weights, which may help explain the drop in performance relative to
MAML. The authors released Tensorflow code for regression only, so we reimplemented
this baseline for classification in PyTorch.
•	Amortized Bayesian Meta-Learning (ABML) (Ravi & Beatson, 2019) performs a few
steps of Bayes-by-backprop (Blundell et al., 2015) in order to infer a fully-factorized ap-
proximate posterior over the weights. The authors did not release code and so we imple-
mented our own version of ABML in PyTorch. We found the weighting on the inner and
outer KL divergences to be important for achieving good performance. We took the neg-
ative log likelihood to be mean cross entropy and used an inner KL weight of 0.01 and
an outer KL weight of 0.001. These values were arrived upon by doing a small amount
of hyperparameter tuning on the Omniglot→ EMNIST dataset. We used α = 1.0 and
β = 0.01 for the Gamma prior over the weights. We only applied ABML to the weights
of the network; the biases were learned as point estimates. We used 4 steps of adaptation
and took 5 samples when computing expectations (using any more than this did not fit
into GPU memory). We used the local reparameterization trick (Kingma et al., 2015) and
flipout (Wen et al., 2018) when computing expectations in order to reduce variance. In or-
der to match the architecture used by Ravi & Beatson (2019), we trained this baseline with
32 filters throughout the classification network. We trained each 1-shot ABML model for
800 epochs and each 5-shot ABML model for 600 epochs as the learning had not converged
within the epoch limits specified in Section G.3.
•	Logistic Softmax GP (Galy-Fajou et al., 2020) is the multi-class Gaussian process classi-
fication method that relies on the logistic softmax likelihood. Galy-Fajou et al. (2020) did
not consider few-shot, but we use the same objectives described in Section 4.4 to adapt this
method to FSC. In addition, we used the cosine kernel (see Section H for a description) that
we found to work best with our OVE PG GPs. For this method, we found it important to
learn a constant mean function (rather than a zero mean) in order to improve calibration.
G.3 Training Details
All methods employed the commonly-used Conv4 architecture (Vinyals et al., 2016) (see Table 4
for a detailed specification), except ABML which used 32 filters throughout. All of our experiments
used the Adam (Kingma & Ba, 2015) optimizer with learning rate 10-3. During training, all models
used epochs consisting of 100 randomly sampled episodes. A single gradient descent step on the
encoder network and relevant hyperparameters is made per episode. All 1-shot models are trained
for 600 epochs and 5-shot models are trained for 400 epochs, except for ABML which was trained
for an extra 200 epochs. Each episode contained 5 classes (5-way) and 16 query examples. At test
time, 15 query examples are used for each episode. Early stopping was performed by monitoring
accuracy on the validation set. The validation set was not used for retraining.
We train both marginal likelihood and predictive likelihood versions of our models. For POlya-
Gamma sampling we use the PyPolyaGamma package3. During training, we use a single step of
Gibbs (T =1). For evaluation, we run until T = 50. In both training and evaluation, we use M = 20
parallel Gibbs chains to reduce variance.
3https://github.com/slinderman/pypolyagamma
21
Published as a conference paper at ICLR 2021
Table 4: Specification of Conv4 architecture. Conv2d layers are 3 × 3 with stride 1 and same
padding. MaxPool2d layers are 2 × 2 with stride 2 and valid padding.
Output Size	Layers	Output Size	Layers
1 × 28 × 28	Input image	3 × 84 × 84	Input image
	Conv2d		Conv2d
	BatchNorm2d		BatchNorm2d
64 × 14 × 14		64 × 42 × 42	
	ReLU		ReLU
	MaxPool2d		MaxPool2d
	Conv2d		Conv2d
	BatchNorm2d		BatchNorm2d
64 × 7 × 7		64 × 21 × 21	
	ReLU		ReLU
	MaxPool2d		MaxPool2d
	Conv2d		Conv2d
	BatchNorm2d		BatchNorm2d
64 × 3 × 3		64 × 10 × 10	
	ReLU		ReLU
	MaxPool2d		MaxPool2d
	Conv2d		Conv2d
	BatchNorm2d		BatchNorm2d
64 × 1 × 1		64 × 5 × 5	
	ReLU		ReLU
	MaxPool2d		MaxPool2d
64	Flatten	1600	Flatten
(a) Omniglot→EMNIST dataset.	(b) All other datasets.
H Effect of Kernel Choice on Classification Accuracy
In this section, we examine the effect of kernel choice on classification accuracy for our proposed
One-Vs-Each Polya-Gamma OVE GPs.
Cosine Kernel. In the main paper, we showed results for the following kernel, which we refer to
as the “cosine” kernel due to its resemblance to cosine similarity:
kcos(x, x0； θ, α) = exp(α) gθ(：)>｝厂? ,	(47)
kgθ(x)kkgθ(x0)k
where gθ (∙) is a deep neural network that outputs a fixed-dimensional encoded representation of the
input and α is the scalar log output scale. Both θ and α are considered hyperparameters and learned
simultaneously as shown in Algorithm 1. We found that this kernel works well for a range of datasets
and shot settings. We note that the use of cosine similarity is reminiscent of the approach taken by
Baseline++ method of (Chen et al., 2019), which computes the softmax over cosine similarity to
class weights.
Here we consider three additional kernels: linear, RBF, and normalized RBF.
Linear Kernel. The linear kernel is defined as follows:
1
kl (x, x ； θ,α) = D exp(α)gθ(x)>gθ(x ),	(48)
where D is the output dimensionality of gθ(x). We apply this dimensionality scaling because the
dot product between gθ (x) and gθ (x0 ) may be large depending on D.
RBF Kernel. The RBF (also known as squared exponential) kernel can be defined as follows:
krbf(x, x0; θ,α, ') = exp(α) exp (-Drl 1 /八2 ∣∣gθ(x) - gθ(x0)k] ,	(49)
2 exp(')
where ` is the log lengthscale parameter (as with α, we learn the ` alongside θ).
22
Published as a conference paper at ICLR 2021
Normalized RBF Kernel. Finally, we consider a normalized RBF kernel similar in spirit to the
cosine kernel:
krbf-norm (x, x0; θ, α, `) = exp(α) exp
1
2exp(')2
Il	gθ(X)
Ukgθ(x)k
gθ(X0)
kgθ(XMII J
(50)
The results of our Polya-Gamma OVE GPs with different kernels can be found in Tables 5
and 6. In general, we find that the cosine kernel works best overall, with the exception of
Omniglot→EMNIST, where RBF does best.
Table 5: Classification accuracy for Polya-Gamma OVE GPs (our method) using different kernels.
Cosine is overall the best, followed closely by linear. RBF-based kernels perform worse, except for
the Omniglot→EMNIST dataset. Evaluation is performed on 5 randomly generated sets of 600 test
episodes. Standard deviation of the mean accuracy is also shown. ML = Marginal Likelihood, PL =
Predictive Likelihood.
Kernel	Objective	CUB		mini-ImageNet	
		1-shot	5-shot	1-shot	5-shot
Cosine	ML	63.98 ± 0.43	77.44 ± 0.18	50.02 ± 0.35	64.58 ± 0.31
Linear	ML	62.48 ± 0.26	77.94 ± 0.21	50.81 ± 0.30	66.66 ± 0.45
RBF	ML	58.49 ± 0.40	75.50 ± 0.18	50.33 ± 0.26	64.62 ± 0.37
RBF (normalized)	ML	62.75 ± 0.32	78.71 ± 0.08	50.26 ± 0.31	64.84 ± 0.39
Cosine	PL	60.11 ± 0.26	79.07 ± 0.05	48.00 ± 0.24	67.14 ± 0.23
Linear	PL	60.44 ± 0.39	78.54 ± 0.19	47.29 ± 0.31	66.66 ± 0.36
RBF	PL	56.18 ± 0.69	77.96 ± 0.19	48.06 ± 0.28	66.66 ± 0.39
RBF (normalized)	PL	59.78 ± 0.34	78.42 ± 0.13	47.51 ± 0.20	66.42 ± 0.36
Table 6: Cross-domain classification accuracy for Polya-Gamma OVE GPs (our method) using dif-
ferent kernels. The experimental setup is the same as Table 5.
Kernel	Objective	Omniglot→EMNIST		mini-ImageNet→CUB	
		1-shot	5-shot	1-shot	5-shot
Cosine	ML	68.43 ± 0.67	86.22 ± 0.20	39.66 ± 0.18	55.71 ± 0.31
Linear	ML	72.42 ± 0.49	88.27 ± 0.20	39.61 ± 0.19	55.07 ± 0.29
RBF	ML	78.05 ± 0.38	88.98 ± 0.16	36.99 ± 0.07	51.75 ± 0.27
RBF (normalized)	ML	75.51 ± 0.47	88.86 ± 0.16	38.42 ± 0.16	54.20 ± 0.13
Cosine	PL	77.00 ± 0.50	87.52 ± 0.19	37.49 ± 0.11	57.23 ± 0.31
Linear	PL	75.87 ± 0.43	88.77 ± 0.10	36.83 ± 0.27	56.46 ± 0.22
RBF	PL	74.62 ± 0.35	89.87 ± 0.13	35.06 ± 0.25	55.12 ± 0.21
RBF (normalized)	PL	76.01 ± 0.31	89.42 ± 0.16	37.50 ± 0.28	56.80 ± 0.39
I Additional Calibration Results
In Figure 9, we include calibration results for mini-Imagenet and Omniglot→EMNIST. They follow
similar trends to the results presented in Section 5.2.
23
Published as a conference paper at ICLR 2021
m∣n ∣-l magenet
δ∙BJn;MV
FeetureTnanefer
0.β
0.4
0.2
ECE: B. 368
NCE: θ.641
BRI: θ.748
0.6
MAML
Baseline++
Matching Networks
Prototypical Networlra
ReIatIonNet
3.β
3.β
0.8
0.6
0.4
ECE: 0.027
MCE: Θ.Θ49
BRI: θ.494
0.2
， ME: t
∣∣BRI::
~l~r^r^r^r^l_rl ∣ o.o l——I
0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence	Confidence
θ.θlθ
9.971
θ.538
0.8
0.6
0.4
0.2
θ∙°o⅛
3.2
OVE GP PL+ Cosine
0.2 0.4 0.6 0.8 1.0
Confidence
XoeJnIMV
FeatureTransgr
1.0
0.6 0.8 1.0 0 0O O
ECE: Θ.Θ94
MCE: Θ.Θ92
BRI: Θ.2Θ6
Baseline++
0.8
0.6
0.4
0.2
0.2 0.4 0.6 0.8 1.0 0°0∙0
1.0
1.0
Matching NelWorto
0.8
0.6
0.4
0.2
0.2 0.4 0.6 0.8 1.0 ° °0.0
PrOtOtypICal Networks
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.2 0.4 0.6 0.8 1.0
1.0
RelatIonNet
ECE: θ.552
MCE: 0.594
BRI: Θ.61B
1.0
00OO 0.2 0.4 0.6 0.8 1.0
θθo⅛
DKT + Cosine
0.8
0.6
0.4
0.2
0.2 0.4 0.6 0.8 1.0
δ∙BJn;WV
Bayeslan MAML
Bayeelan MAML(Chaeer)
OVEGP ML+ Cosine
Logistic Sortnax GPML + Cosine	LOCIStIC Sortnax GP PL+ Cosine
OVEGP PL+Cosine
Confidence
mini-l magenet → CUB
FeatureTranefer
Baseline++
Matching Networks
Prototypical Networks
ReIatIonNet
□KT + Cosine
Bayeelan MAML(Chaeer)
0.2
0.2
0.2
ECE: Θ.Θ48
HCE: θ.077
BRI: θ.619
∣ECE: Θ.Θ66∣
NCE: θ.26θ
BRI: θ.639
00OO
~l~r^r^r^r^l_rl I o.o l——I	l
0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence	Confidence
00OO 0.2
OVEGP ML+ Cosine
Logistic Sortnax GPML + Cosine	LOCIStIC Sortnax GP PL+ Cosine
OVEGP PL+Cosine
Figure 9: Reliability diagrams, expected calibration error, maximum calibration error, and Brier
scores for 5-shot 5-way tasks on mini-Imagenet, Omniglot→EMNIST, and mini-Imagenet→CUB.
Metrics are computed on 3,000 random tasks from the test set.
24
Published as a conference paper at ICLR 2021
J Quantitative Robustness to Input Noise Results
In this section we include quantitative results for the robustness to input noise results presented in
Figure 2. Results for Gaussian noise are shown in Table 7, impulse noise in Table 8, and defocus
blur in Table 9.
Table 7: Accuracy (%) and Brier Score when applying Gaussian noise corruption of severity 5 to
both the support and query set of test-time episodes. Results were evaluated across 1,000 randomly
generated 5-shot 5-way tasks.
Method	CUB		mini-ImageNet		mini-ImageNet→CUB	
	Acc. (↑)	Brier Q)	Acc. (↑)	Brier (；)	Acc.(↑)	Brier Q)
Feature Transfer	30.45	0.775-	22.58	0.799	22.75^^	0.799
Baseline++	22.60	0.798	23.82	0.797	24.13	0.797
MatchingNet	26.72	0.803	24.80	0.797	23.59	0.804
ProtoNet	32.28	0.778	29.97	0.781	32.30	0.779
RelationNet	25.23	0.799	23.69	0.800	20.00	0.800
DKT + Cosine	29.54	0.779	27.78	0.792	31.94	0.782
Bayesian MAML	22.79	0.905	20.52	0.963	20.46	0.949
Bayesian MAML (Chaser)	20.20	1.133	20.41	1.118	21.39	1.039
LSM GP + Cosine (ML)	27.92	0.787	22.43	0.798	22.36	0.799
LSM GP + Cosine (PL)	31.21	0.772	31.77	0.768	34.74	0.754
OVE PG GP + Cosine (ML) [ours]	32.27	^^0774-	29.99	0.776	29.97^^	0.784
OVE PG GP + Cosine (PL) [ours]	33.01	0.771	33.29	0.760	31.41	0.764
Table 8: Accuracy (%) and Brier Score when applying impulse noise corruption of severity 5 to
both the support and query set of test-time episodes. Results were evaluated across 1,000 randomly
generated 5-shot 5-way tasks.
Method	CUB		mini-ImageNet		mini-ImageNet→CUB	
	Acc. (↑)	Brier (；)	Acc. (↑)	Brier (；)	Acc. (↑)	Brier Q)
Feature Transfer	30.20	0776-	23.54	0.798	22.87^^	0.799
Baseline++	28.05	0.790	23.72	0.798	25.58	0.795
MatchingNet	28.25	0.790	23.80	0.803	23.21	0.811
ProtoNet	32.12	0.774	28.81	0.783	32.70	0.775
RelationNet	25.23	0.799	23.13	0.800	20.00	0.800
DKT + Cosine	29.74	0.778	29.11	0.789	32.26	0.781
Bayesian MAML	22.76	0.903	20.50	0.970	20.56	0.950
Bayesian MAML (Chaser)	20.25	1.172	20.51	1.116	21.45	1.022
LSM GP + Cosine (ML)	28.18	0.787	21.82	0.799	23.64	0.797
LSM GP + Cosine (PL)	32.10	0.769	30.22	0.776	35.09	0.751
OVE PG GP + Cosine (ML) [ours]	31.41	0778-	29.66	0.778	30.28^^	0.783
OVE PG GP + Cosine (PL) [ours]	33.36	0.772	33.23	0.761	32.06	0.762
25
Published as a conference paper at ICLR 2021
Table 9: Accuracy (%) and Brier Score when applying defocus blur corruption of severity 5 to
both the support and query set of test-time episodes. Results were evaluated across 1,000 randomly
generated 5-shot 5-way tasks.
Method	CUB		mini-ImageNet		mini-ImageNet→CUB	
	Acc. (↑)	Brier Q)	Acc. (↑)	Brier (；)	Acc.(↑)	Brier Q)
Feature Transfer	38.03	0734-	33.06	0.791	33.47^^	0.792
Baseline++	42.55	0.710	35.89	0.761	39.88	0.740
MatchingNet	44.43	0.682	34.43	0.754	35.95	0.741
ProtoNet	46.78	0.676	36.92	0.737	41.45	0.714
RelationNet	40.81	0.759	30.11	0.790	25.69	0.794
DKT + Cosine	45.34	0.695	38.29	0.737	45.17	0.703
Bayesian MAML	42.65	0.697	30.63	0.808	37.32	0.736
Bayesian MAML (Chaser)	40.66	0.881	29.93	1.121	31.33	1.125
LSM GP + Cosine (ML)	45.37	0.706	34.10	0.769	39.66	0.753
LSM GP + Cosine (PL)	48.55	0.690	39.46	0.737	43.15	0.714
OVE PG GP + Cosine (ML) [ours]	46.46	^^0701-	37.65	0.775	43.48^^	0.723
OVE PG GP + Cosine (PL) [ours]	49.44	0.695	38.95	0.780	43.66	0.720
26