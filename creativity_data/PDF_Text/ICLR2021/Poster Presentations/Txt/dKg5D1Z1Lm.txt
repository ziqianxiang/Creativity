Published as a conference paper at ICLR 2021
Non-asymptotic Confidence Intervals of Off-
policy Evaluation: Primal and Dual Bounds
Yihao Feng *, Ziyang Tang *
University of Texas at Austin
{yihao, ztang}@cs.utexas.edu
Na Zhang
Tsinghua University
zhangna@pbcsf.tsinghua.edu.cn
Qiang Liu
University of Texas at Austin
lqiang@cs.utexas.edu
Ab stract
Off-policy evaluation (OPE) is the task of estimating the expected reward of a given
policy based on offline data previously collected under different policies. Therefore,
OPE is a key step in applying reinforcement learning to real-world domains such as
medical treatment, where interactive data collection is expensive or even unsafe. As
the observed data tends to be noisy and limited, it is essential to provide rigorous
uncertainty quantification, not just a point estimation, when applying OPE to
make high stakes decisions. This work considers the problem of constructing non-
asymptotic confidence intervals in infinite-horizon off-policy evaluation, which
remains a challenging open question. We develop a practical algorithm through
a primal-dual optimization-based approach, which leverages the kernel Bellman
loss (KBL) of Feng et al. (2019) and a new martingale concentration inequality
of KBL applicable to time-dependent data with unknown mixing conditions. Our
algorithm makes minimum assumptions on the data and the function class of the
Q-function, and works for the behavior-agnostic settings where the data is collected
under a mix of arbitrary unknown behavior policies. We present empirical results
that clearly demonstrate the advantages of our approach over existing methods.
1	Introduction
Off-policy evaluation (OPE) seeks to estimate the expected reward of a target policy in reinforcement
learnings (RL) from observational data collected under different policies (e.g., Murphy et al., 2001;
Fonteneau et al., 2013; Jiang & Li, 2016; Liu et al., 2018a). OPE plays a central role in applying
reinforcement learning (RL) with only observational data and has found important applications in
areas such as medicine, self-driving, where interactive “on-policy” data is expensive or even infeasible
to collect. A critical challenge in OPE is the uncertainty estimation, as having reliable confidence
bounds is essential for making high-stakes decisions. In this work, we aim to tackle this problem by
providing non-asymptotic confidence intervals of the expected value of the target policy. Our method
allows us to rigorously quantify the uncertainty of the prediction and hence avoid the dangerous case
of being overconfident in making costly and/or irreversible decisions.
However, off-policy evaluation per se has remained a key technical challenge in the literature (e.g.,
Precup, 2000; Thomas & Brunskill, 2016; Jiang & Li, 2016; Liu et al., 2018a), let alone gaining
rigorous confidence estimation of it. This is especially true when 1) the underlying RL problem is
long or infinite horizon, and 2) the data is collected under arbitrary and unknown algorithms (a.k.a.
behavior-agnostic). As a consequence, the collected data can exhibit arbitrary dependency structure,
which makes constructing rigorous non-asymptotic confidence bounds particularly challenging.
Traditionally, the only approach to provide non-asymptotic confidence bounds in OPE is to combine
importance sampling (IS) with concentration inequalities (e.g., Thomas et al., 2015a;b), which,
however, tends to degenerate for long/infinite horizon problems (Liu et al., 2018a). Furthermore,
* Equal contribution.
1
Published as a conference paper at ICLR 2021
neither can this approach be applied to the behavior-agnostic settings, nor can it effectively handle
the complicated time dependency structure inside individual trajectories. Instead, it requires to use a
large number of independently collected trajectories drawn under known policies.
In this work, we provide a practical approach for Behavior-agnostic, Off-policy, Infinite-horizon,
Non-asymptotic, Confidence intervals based on arbitrarily Dependent data (BONDIC). Our method
is motivated by a recently proposed optimization-based (or variational) approach to estimating OPE
confidence bounds (Feng et al., 2020), which leverages a tail bound of kernel Bellman statistics (Feng
et al., 2019). Our approach achieves a new bound that is both an order-of-magnitude tighter and
computationally efficient than that of Feng et al. (2020). Our improvements are based on two pillars
1) developing a new primal-dual perspective on the non-asymptotic OPE confidence bounds, which is
connected to a body of recent works on infinite-horizon value estimation (Liu et al., 2018a; Nachum
et al., 2019a; Tang et al., 2020a; Mousavi et al., 2020); and 2) offering a new tight concentration
inequality on the kernel Bellman statistics that applies to behavior-agnostic off-policy data with
arbitrary dependency between transition pairs. Empirically, we demonstrate that our method can
provide reliable and tight bounds on a variety of well-established benchmarks.
Related Work Besides the aforementioned approach based on the combination of IS and concen-
tration inequalities (e.g., Thomas et al., 2015a), bootstrapping methods have also been widely used in
off-policy estimation (e.g., White & White, 2010; Hanna et al., 2017; Kostrikov & Nachum, 2020).
But the latter is limited to asymptotic bounds. Alternatively, Bayesian methods (e.g. Engel et al.,
2005; Ghavamzadeh et al., 2016a) offers a different way to estimate the uncertainty in RL, but fails to
guarantee frequentist coverage. In addition, Distributed RL (Bellemare et al., 2017) seeks to quantify
the intrinsic uncertainties inside the Markov decision process, which is orthogonal to the estimation
of uncertainty that we consider.
Our work is built upon the recent advances in behavior-agnostic infinite-horizon OPE, including
Liu et al. (2018a); Feng et al. (2019); Tang et al. (2020a); Mousavi et al. (2020), as well as the
DICE-family (e.g., Nachum et al., 2019a; Zhang et al., 2020a; Yang et al., 2020b). In particular, our
method can be viewed as extending the minimax framework of the infinite-horizon OPE in the infinite
data region by Tang et al. (2020a); Uehara et al. (2020); Jiang & Huang (2020) to the non-asymptotic
finite sample region.
Outline For the rest of the paper, we start with the problem statement in Section 2 , and an overview
on the two dual approaches to infinite-horizon OPE that are tightly connected to our method in
Section 3. We then present our main approach in Section 4 and perform empirical studies in Section 5.
The proof and an abundance of additional discussions can be found in Appendix.
2	Background, Data Assumption, Problem Setting
Consider an agent acting in an unknown environment. At each time step t, the agent observes the
current state St in a state space S, takes an action at 〜∏(∙ | St) in an action space A according to
a given policy π; then, the agent receives a reward rt and the state transits to s0t = st+1, following
an unknown transition/reward distribution (rt, st+ι)〜 P(∙ | st, at). Assume the initial state so is
drawn from an known initial distribution D0. Let γ ∈ (0, 1) be a discount factor. In this setting, the
expected reward of π is defined as Jπ
EnIPT=0Ytrt | s0 ~ Do]
, which is the expected total
discounted rewards when we execute π starting from Do for T steps. In this work, we consider the
infinite-horizon case with T → +∞.
Our goal is to provide an interval estimation of Jπ for a general and challenging setting with
significantly released constraints on the data. In particular, we assume the data is behavior-agnostic
and off-policy, which means that the data can be collected from multiple experiments, each of which
can execute a mix of arbitrary, unknown policies, or even follow a non-fixed policy. More concretely,
suppose that the model P is unknown, and we have a set of transition pairs Dn = (si, ai, ri, s0i)in=1
collected from previous experiments in a sequential order, such that for each data point i, the (ri, s0i)
is drawn from the model P(∙ | Si,ai), while (si,ai) is generated with an arbitrary black box given
the previous data points. We formalize both the data assumption and goal as below.
2
Published as a conference paper at ICLR 2021
Assumption 2.1 (Data Assumption). Assume the data Dn = (si, a%, r, si)n=ι is drawnfrom an ar-
bitrary joint distribution, such that for each i = 1, . . . , n, conditional on D<i := (sj, aj, rj, s0j)j<i ∪
(Si, ai), the subsequent local reward and next state (ri, Si) are drawnfrom P(∙ | si, ai).
Goal Given a confidence level δ ∈ (0,1), we want to construct an interval [J-, J+] ⊂ R based on
the data Dn, such that Pr(Jn ∈ [J , J +]) ≥ 1 一 δ, where Pr(∙) IS w.r.t. the randomness ofthe data.
The partial ordering on the data points is introduced to accommodate the case that si+1 equals s0j
for some j ≤ i. The data assumption only requires that (ri, Si) is generated from P(∙ | s%, ai), and
imposes no constraints on how (si, ai) is generated. This provides great flexibility in terms of the data
collection process. In particular, we do not require (si, ai)in=1 to be independent as always assumed
in recent works (Liu et al., 2018a; Mousavi et al., 2020).
A crucial fact is that our data assumption actually implies a martingale structure on the empirical
Bellman residual operator of the Q-function, As we will show in Section 4.1, this enables us to derive
a key concentration inequality underpinning our non-asymptotic confidence bounds.
Here, we summarize a few notations that will simplify the presentation in the rest of work. First of
all, we append each (si, ai, ri, s0i) with an action a0i ~ π(∙ | Si) following s0i . This can be done for
free as long as π is given (See the Remark in Section 3). Also, we write xi = (Si, ai), x0i = (S0i, a0i),
and yi = (x0i, ri) = (S0i, a0i, ri). Correspondingly, define X = S × A to be the state-action space and
Y = X × R. Denote Pπ(y | x) = P(S0, r | x)π(a0 | S0). In this way, the observed data can be written
as pairs of {xi, yi}n=ι, and Assumption 2.1 is equivalent to saying that y% ~ Pn (∙ | Xi) given D<i,
which is similar to a supervised learning setting. We equalize the data Dn with its empirical measure
Dn = Pn=ι δχi,yjn, where δ is the Delta measure.
3	Two dual approaches to Infinite-horizon off-policy estimation
The deficiency of the traditional IS methods on long-/infinite-horizon RL problems (a.k.a. the curse
of horizon (Liu et al., 2018a)) has motivated a line of work on developing efficient infinite-horizon
value estimation (e.g., Liu et al., 2018a; Feng et al., 2019; Nachum et al., 2019a; Zhang et al., 2020a;
Mousavi et al., 2020; Tang et al., 2020a). The main idea is to transform the value estimation problem
into estimating either the Q-function or the visitation distribution (or its related density ratio) of the
policy ∏. This section introduces and reinterprets these two tightly connected methods, which serves
to lay out a foundation for our main confidence bounds from a primal and dual perspective.
Given a policy π, its Q-function is defined as qn (x) = En [Pt∞=0 γtrt | x0 = x], where the expecta-
tion is taken when we execute π initialized from a fixed state-action pair (S0, a0) = x0 = x. Let Dn,t
be the distribution of (xt, yt) = (st, at, s[, at, rt) when executing policy π starting from so 〜Do for
t steps. The visitation distribution of π is defined as Dn = Pt∞=0 γt Dn,t. Note that Dn integrates to
1/(1 一 γ), while we treat it as a probability measure in the notation.
The expected reward Jncan be expressed using either qn or Dn as follows:
∞
Jn ：= En EYtrt = Er~D∏[r] = Eχ~D∏,o [q∏(x)],	(1)
t=o
where r 〜D∏ (resp. X 〜D∏,o) denotes sampling from the r-(resp. x-) marginal distribution of D∏
(resp. Dn,o). Eq. (1) plays a key role in the infinite-horizon value estimation by transforming the
estimation of Jn into estimating either qn or Dn.
Value Estimation via Q Function Because D∏,o(x) = Do(s)∏(a∣s) is known, we can estimate
J∏ by Eχ~Dπ,o [q(χ)] with any estimation q of the true Q-function q∏; the expectation under X 〜D∏,o
can be estimated to any accuracy with Monte Carlo. To estimate qn, we consider the empirical and
expected Bellman residual operator:
Rq(X, y) = q(X)- Yq(X0) — r,	Rnq(X) = Ey~p∏(∙∣x) [Rq(X,y)].	⑵
It is well-known that qn is the unique solution of the Bellman equation Rnq = 0. Since yi ~ Pn (∙∣Xi)
for each data point in Dn, ifq = qn, then Rq(Xi, yi), i = 1, . . . , n are all zero-mean random variables.
3
Published as a conference paper at ICLR 2021
Let ω be any function from X to R, then	i Rq(xi, yi)ω(xi) also has zero mean. This motivates the
following functional Bellman loss (Feng et al., 2019; 2020; Xie & Jiang, 2020),
T / . r∖ ∖
LW (q; Dn)
1n
SUP	Rq(xi,yi)ω(xi),
ω∈W n i=1
(3)
where W is a set of functions ω : X → R. To ensure that the sup is finite, W is typically set to be an
unit ball of some normed function space Wo, such that W = {ω ∈ Wo : kωkW ≤ 1}. Feng et al.
(2019) considers the simple case when W is taken to be the unit ball K of the reproducing kernel
Hilbert space (RKHS) with a positive definite kernel k : X × X → R, in which case the loss has a
simple closed form solution:
LK(q; Dn)
∖
1n
2 E Rq(xi,yi)k(xi,xj)Rq(xj,y).
n ij=1
(4)
Note that the RHS of Eq. (4) is the square root of the kernel Bellman V-statistics in Feng et al.
(2019). Feng et al. (2019) showed that, when the support of data distribution Dn covers the whole
space (which may require an infinite data size) and k is an integrally strictly positive definite kernel,
LK(q; Dn) = 0 iff q = qπ. Therefore, one can estimate qπ by minimizing LK(q, Dn).
Remark The empirical Bellman residual operator R can be extended to Rq(x, y) = q(x) - r -
Ymm Pm=I q(s0, a'), where {a'}m=1 are i.i.d. drawn from π(∙∣s0). As m increases, this gives a lower
variance estimation of Rnq. If m = +∞, we have Rq(x, y) = q(x) - r - yE。,〜∏(.∣ s，)[q(s0, a0)],
which coincides with the operator used in the expected SARSA (Sutton & Barto, 1998). In fact,
without any modification, all results in this work can be applied to Rq for any m.
Value Estimation via Visitation Distribution Another way to estimate Jn in Eq. (1) is to approx-
imate Dn with a weighted empirical measure of the data (Liu et al., 2018a; Nachum et al., 2019a;
Mousavi et al., 2020; Zhang et al., 2020a). The key idea is to assign an importance weight ω(xi) to
each data point xi in Dn. We can choose the function ω : X → R properly such that Dn and hence
J∏ can be approximated by the ω-weighted empirical measure of Dn as follows:
1n
Jn ≈ jω := EDω [r] = 一 ω ω ω (Xi)ri,
n n i=1
1n
Dn ≈ Dn := n〉： ω(Xi )δxi,yi ∙	(5)
n i=1
Intuitively, ω can be viewed as the density ratio between Dn and Dn, although the empirical measure
Dn may not have well-defined density. Liu et al. (2018a); Mousavi et al. (2020) proposed to estimate
ω by minimizing a discrepancy measure between Dnω and Dn. To see this, note that D = Dn if and
only if ∆(D, q) = 0 for any function q, where
∆(D, q) = ED [γq(x0) - q(x)] - EDπ [γq(x0) - q(x)]
ED [γq(x0) - q(x)] + EDπ,0 [q(x)],
(6)
using the fact that EDπ [γq(x0) - q(x)] = -EDπ,0 [q(x)] (Theorem 1, Liu et al., 2018a). Also note
that the RHS of Eq. (6) can be practically calculated given any D and q without knowing Dπ. Let Q
be a set of functions q: X → R. One can define the following loss for ω:
IQHD n ) = SUp{'D n ,q%
.. ... .. ^
(7)
^
Similar to LW (q; Dn), when Q is a ball in RKHS, IQ (ω; Dn) also has a bilinear closed form
analogous to Eq. (4); see Mousavi et al. (2020) and Appendix F. As we show in Section 4, IQ(ω; Dn)
and LW (q; Dn) are connected to the primal and dual views of our confidence bounds, respectively.
4 Main Approach
Let Q be a large enough function set including the true Q-function qn , that is, qn ∈ Q. Following
+
Feng et al. (2020), a confidence interval [JQ-,W, JQ+,W] of Jn can be constructed as follows:
. W = sup {Ed∏,0 [q] s.t. LW(q; Dn) ≤ εn^ ,
(8)
4
Published as a conference paper at ICLR 2021
and JQ-,W is defined in a similar way by replacing sup on q ∈ Q with inf.
The idea here is to seek the extreme q function with the largest (resp. smallest) expected values in set
F := Q ∩ {q : LK (q; Dn ) ≤ εn }. Therefore, Eq. (8) would be a 1 - δ confidence interval if qπ is
included in F with at least probability 1 - δ, which is ensured when qπ ∈ Q and
一 ，一 ， q 、	.	一
Pr(LW(q∏； Dn) ≤ εn) ≥ 1 - δ.	(9)
Feng et al. (2020) showed that in the RKHS case when W = K, Eq. (9) can be achieved with
εn = t2cq∏,k(n-1 rl0g≡ + ^ ,	(10)
πn nn
When n is an even number, where cqπ,k = supχ,y Rq∏(x, y)2k(x, x). This was proved using Hoeffd-
ing’s inequality for U-statistics (Hoeffding, 1963). To solve Eq. (8) efficiently, Feng et al. (2020) took
Q to be a ball in RKHS with random feature approximation. Unfortunately, this method as described
by Eq. (8)-(10) has two major disadvantages:
1)	Bound Needs to Be Tightened (Section 4.1) The bound of εn = O(n-1/4) in Eq. (10) is
sub-optimal in rate. In Section 4.1, we improve it by an εn = O(n-1/2) bound under the mild
Assumption 2.1, which gets rid of the independence requirement between the transition pairs. Our
tightened bound is achieved by firstly noting a Martingale structure on the empirical Bellman operator
under Assumption 2.1, and then applying an inequality in Pinelis (1992).
2)	Dependence on Global Optimization (Section 4.2) The bound in Eq. (8) is guaranteed to be a
1 - δ confidence bound only when the maximization in Eq. (8) is solved to global optimality. With
a large n, this leads to a high computational cost, even when choosing Q as the RKHS. Feng et al.
(2020) solved Eq. (8) approximately using a random feature technique, but this method suffers from
a gap between the theory and practice. In Section 4.2, we address this problem by presenting a dual
form of Eq. (8), which sidesteps solving the challenging global optimization in Eq. (8). Moreover,
the dual form enables us to better analyze the tightness of the confidence interval and issues regarding
the choices of Q and W .
4.1	A Tighter Concentration Inequality
In this section, we explain our method to improve the bound in Eq. (10) by giving a tighter concen-
tration inequality for the kernel Bellman loss in Eq. (4). We introduce the following semi-expected
kernel Bellman loss:
_ , , ^
LK(q; Dn)
1n
. E R∏q(xi)k(xi,xj)R∏q(xj),
n ij=1
(11)
∖
in which we replace the empirical Bellman residual operator Rq in Eq. (3) with its expected counter-
part R∏q, but still take the empirical average over {χi}n=ι in Dn. For a more general function set
W, we can similarly define LW(q; Dn) by replacing Rq with R∏q in Eq. (3). Obviously, we have
LW(q; Dn) = 0 when q = q∏.
Theorem 4.1 below shows that Lκ(q; Dn) concentrates around LK(q; Dn) with an O(n 1/2) error
under Assumption 2.1. At a first glance, it may seem surprising that the concentration bound is able
to hold even without any independence assumption between {xi}. An easy way to make sense of this
is by recognizing that the randomness in yi conditional on xi is aggregated through averaging, even
when {xi } are deterministic. As Assumption 2.1 does not impose any (weak) independence between
{χi}, we cannot establish that Lκ(q; Dn) concentrates around its mean ED九[Lκ(q; Dn)] (which is
a full expected kernel bellman loss), without introducing further assumptions.
Theorem 4.1.	Assume K is the Unit ball OfRKHS With a positive definite kernel k(∙, ∙). Let cq,k :=
suPx∈x,y∈γ (Rq(x, y) — R∏q(x))2k(x, x) < ∞. Under Assumption 2.1,forany δ ∈ (0,1), with at
5
Published as a conference paper at ICLR 2021
_ ʌ _ ,,
LK(q; Dn) - LK(q;
least probability 1 - δ, we have
2cq,k log(2∕δ)
.
n
In particular, when q = q∏, we have Cqπ,k = supχ,y (RR q∏ (x, y))2k(x, x), and
Lk (q∏ ； Dn) ≤ TE .
n
(12)
(13)
Intuitively, to see Why We can expect an O(n-1/2) bound, note that Lκ(q, Dn) consists of the square
root of the product of two Rq terms, each of which contributes an O(n-1/2) error w.r.t. R∏q.
Technically, the proof is based on a key observation: Assumption 2.1 ensures that Zi := Rq(xi, yi)-
Rπq(xi), i = 1, . . . , n forms a martingale difference sequence w.r.t. {D<i : ∀i = 1, . . . , n}, in
the sense that E[Zi | D<i] = 0, ∀i. See Appendix B for details. The proof also leverages a special
property of RKHS and applies a Hoeffding-like inequality on the Hilbert spaces as in Pinelis (1992)
(see Appendix B). For other more general function sets W, we establish in Appendix E a similar
bound by using Rademacher complexity, although it yields a less tight bound than Eq. (12) when
W=K.
4.2 Dual Confidence B ounds
We derive a dual form of Eq. (8) that sidesteps the need for solving the challenging global optimization
in Eq. (8). To do so, let us plug the definition of LW (q; Dn) into Eq. (3) and introduce a Lagrange
multiplier:
：W =	SUp	inf inf EDn 0 [q]	- λ ( 1	X h(Xi)R虱xi, yi) - εn	)
,W	q∈Q h∈W λ≥0 π,0	n	i=1
= SUp inf E ED∏ ,0 [q] - 1 X ω(xi)Rq(Xi)+ εη kωkwj
q∈Q ω∈Wo	π,0	n i=1	Wo
(14)
(15)
where we use ω(X) = λh(X). Exchanging the order of min/max and some further derivation yields
the following main result.
Theorem 4.2.	I) Let W be the unit ball of a normed function space Wo. We have
JQ,W ≤	FQ (ω) :=	EDω [r]	+	IQ(ω; Dn) + εn ∣∣ωkWo ,	∀ω ∈ Wo ,
JQW ≥	FQ (ω) :=	ED ωω [r]	-	I-Q(ω; Dn)- εn 11ω k Wo	,	∀ω ∈ Wo	,
(16)
1	zɔ	C	_ CI	1 1	T /	A ∖	τ /	A ∖	∙ r zɔ	zɔ	I ` .ι	i
where -Q = {-q : q ∈ Q} and hence I-Q(ω;	Dn)	= IQ(ω;	Dn)	if Q	= -Q.	Further,	we have
++
JQ+,W = inf ω∈Wo FQ+(ω) and JQ-,W = SUpω∈Wo FQ-(ω) if Q is convex and there exists a q ∈ Q
that satisfies the strict feasibility condition that LW (q; Dn) < εn.
II) For Dn and δ ∈ (0,1), assume Wo and εn ∈ R satisfy Eq. (9) (e.g., via Theorem 4.1). Then
for any function set Q with qπ ∈ Q, and any function ω+, ω- ∈ Wo (the choice of Q, ω+, ω- can
depend on Dn arbitrarily), we have
Pr (Jn ∈ [Fq (ω-), F+(ω+)]) ≥ 1 - δ.	(17)
rɪ-il	/C,	i'	.Λ	∙	∙	1 Λ	1 ∙	1 -	Zr1∖ i'	1 ∙	i'	IT	/	r∖ ∖	∙	.
Theorem 4.2 transforms the original bound in Eq. (8), framed in terms of q and LW (q; Dn), into
a form that involves the density-ratio ω and the related loss IQ(ω; Dn). The bounds in Eq. (16)
can be interpreted as assigning an error bar around the ω-based estimator Jω = ED“ [r] in Eq. (5),
with the error bar of I±Q(ω; Dn) + εn ∣ω∣W . Specifically, the first term I±Q(ω; Dn) measures
the discrepancy between Dnω and Dπ as discussed in Eq. (7), whereas the second term captures the
randomness in the empirical Bellman residual operator Rqπ .
6
Published as a conference paper at ICLR 2021
Compared with Eq. (8), the global maximization on q ∈ Q is now transformed inside the IQ(ω; Dn)
term, which yields a simple closed form solution in the RKHS case (see Appendix F). In practice, we
can optimize ω+ and ω- to obtain the tightest possible bound (and hence recover the primal bound)
+
by minimizing/maximizing FQ+(ω) and FQ- (ω), but it is not necessary to solve the optimization to
global optimality. When Wo is an RKHS, by the standard finite representer theorem (Scholkopf &
Smola, 2018), the optimization on ω reduces to a finite dimensional optimization, which can be sped
up with any favourable approximation techniques. We elaborate on this in Appendix D.
Length of the Confidence Interval The form in Eq. (16) also makes it much easier to analyze the
tightness of the confidence interval. Suppose ω = ω+ = ω- and Q = -Q, the length of the optimal
confidence interval is
length([J-,W, J+,WD= ω∈f {2IQ(ω; Dn) +2εn kωkWo}∙
Given εn is O(n-1/2), We can make the overall length of the optimal confidence interval also
O(n-1/2), as long as Wo is rich enough to include a good density ratio estimator ω* that satisfies
Iq (ω*; D n) = O(n-1/2) and has a bounded norm ∣∣ω*kwθ.
We can expect to achieve Io(ω*; Dn) = O(n-1/2), when (1) Q has an O(n-1/2) sequential
Rademacher complexity (Rakhlin et al., 2015) (e.g., a finite ball in RKHS); and (2) Dn is collected
folloWing a Markov chain With strong mixing condition and Weakly converges to some limit distribu-
tion D∞ whose support is X, and therefore we can define ω* as the density ratio between Dn and
D∞ . Refer to Appendix C for more discussions. Indeed, our experiments shoW that the lengths of
practically constructed confidence intervals do tend to decay with an O(n-1/2) rate.
Choice of W and Q To ensure the concentration inequality in Theorem 4.1 is valid, the choice of
Wo cannot depend on the data Dn. Therefore, We should use a separate holdout data to construct a
data-dependent Wo. In contrast, the choice of Q can depend on the data Dn arbitrarily, since it is a
part of the optimization bound Eq. (8) but not in the tail bound Eq. (9). In this light, one can construct
the best possible Q by exploiting the data information in the most favourable Way. For example,
We can construct an estimator of q ≈ q∏ based on any state-of-the-art method (e.g., Q-learning or
model-based methods), and set Q to be a ball centering around q such that q∏ - q ∈ Q. This enables
post-hoc analysis based on prior information on qπ , as suggested in Feng et al. (2020).
Mis-specification of Q and Oracle Upper/Lower Estimates Our result relies on the assumption
that qπ ∈ Q. However, as with other statistical estimation problems, there exists no provably way to
empirically verify the correctness of model assumptions such as qπ ∈ Q. Because empirical data only
reveals the information of the unknown function (in our case qπ) on a finite number data points, but no
conclusion can be made on the unseeing data points without imposing certain smoothness assumption.
Typically, what we can do is the opposite: reject qπ ∈ Q when the Bellman loss LW (q; Dn) of all q
in Q is larger than the threshold εn .
We highlight that, even without verifying qπ ∈ Q, our method can still be viewed as a confidence
interval of a best possible (oracle) upper and lower estimation given the data Dn plus the assumption
that qπ ∈ Q, defined as
：,* = SUp {ED∏,0[q]
q∈Q
s.t.	Rq(xi, yi ) = Rqπ (xi , yi ),	∀i = 1, . .
. ,n .	(18)
^
^
In fact, it is impossible to derive empirical upper bounds lower than JQ *, as there is no way to
+
distinguish q and qπ if Rq(xi, yi) = Rqπ(xi, yi) for all i. But our interval [JQ,K, JQ+,K] provides a
+
1 - δ confidence outer bound of [JQ-,*, JQ+,*] once Eq. (9) holds, regardless if qπ ∈ Q holds or not.
Hence, it is of independent interest to further explore the dual form of Eq. (18), which is another
starting point for deriving our bound. We have more discussion in Appendix G.
Lastly, we argue that it is important to include the Q in the bound. Proposition G.1 in Appendix
shows that removing the q ∈ Q constraint in Eq. (18) would lead to an infinite upper bound,
7
Published as a conference paper at ICLR 2021
Figure 1: Results on Inverted-Pendulum. (a) The confidence interval (significance level δ = 0.1) of our method
(green) and that of Feng et al. (2020) (blue) when varying the data size n. (b) The length of the confidence
intervals (δ = 0.1) of our method scaling with the data size n. (c) The confidence intervals when we vary the
significance level δ (data size n = 5000). (d) The significance level δ vs. the empirical failure rate δ of capturing
the true expected reward by our confidence intervals (data size n = 5000). We average over 50 random trials for
each experiment.
(d) δ
unless the {si, si}n=ι from Dn almost surely covers the whole state space S in the sense that
Prs~Do (S ∈ {si, si}i=1) = 1.
5	Experiments
We compare our method with a variety of existing algorithms for obtaining asymptotic and non-
asymptotic bounds on a number of benchmarks. We find our method can provide confidence interval
that correctly covers the true expected reward with probability larger than the specified success
probability 1 - δ (and is hence safe) across the multiple examples we tested. In comparison, the
non-asymptotic bounds based on IS provide much wider confidence intervals. On the other hand, the
asymptotic methods, such as bootstrap, despite giving tighter intervals, often fail to capture the true
values with the given probability in practice.
Environments and Dataset Construction We test our method on three environments: Inverted-
Pendulum and CartPole from OpenAI Gym (Brockman et al., 2016), and a Type-1 Diabetes medical
treatment simulator.1 We follow a similar procedure as Feng et al. (2020) to construct the behavior
and target policies. more details on environments and data collection procedure are included in
Appendix H.1.
Algorithm Settings We test the dual bound described in our paper. Throughout the experiment,
we always set W = K, the unit ball of the RKHS with positive definite kernel k, and set Q = rQK,
the ball of radius r。in the RKHS with another kernel k. We take both kernels to be Gaussian RBF
kernel and choose rQ and the bandwidths of k and k using the procedure in Appendix H.2. We use a
fast approximation method to optimize ω in FQ+(ω) and FQ-(ω) as shown in Appendix D. Once ω is
found, we evaluate the bound in Eq. (16) exactly to ensure that the theoretical guarantee holds.
Baseline Algorithms We compare our method with four existing baselines, including the IS-based
non-asymptotic bound using empirical Bernstein inequality by Thomas et al. (2015b), the IS-based
bootstrap bound of Thomas (2015), the bootstrap bound based on fitted Q evaluation (FQE) by
Kostrikov & Nachum (2020), and the bound in Feng et al. (2020) which is equivalent to the primal
bound in (8) but with looser concentration inequality (they use a εn = O(n-1/4) threshold).
Results Figure 1 shows our method obtains much tighter bounds than Feng et al. (2020), which
is because we use a much tighter concentration inequality, even the dual bound that we use can
be slightly looser than the primal bound used in Feng et al. (2020). Our method is also more
computationally efficient than that of Feng et al. (2020) because the dual bound can be tightened
1 https://github.com/jxx123/simglucose.
8
Published as a conference paper at ICLR 2021
draweR ycilo
Number of Transitions, n	Number of Transitions, n	Number of Transitions, n
(a) Pendulum	(b) CartPole	(C) Type 1 diabetes
Figure 2: Results on different environments when we use a significance level of δ = 0.1. The colored bars
represent the ConfidenCe intervals of different methods (averaged over 50 random trials); the blaCk error bar
represents the stand derivation of the end points of the intervals over the 50 random trials.
approximately while the primal bound requires to solve a global optimization problem. Figure 1 (b)
shows that we provide inCreasingly tight bounds as the data size n inCreases, and the length of the
interval deCays with an O(n-1/2 ) rate approximately. Figure 1 (C) shows that when we inCrease the
signifiCanCe level δ, our bounds beCome tighter while still Capturing the ground truth. Figure 1 (d)
shows the perCentage of times that the interval fails to Capture the true value in a total of 100 random
trials (denoted as δ) as we vary δ. We Can see that δ remains Close to zero even when δ is large,
suggesting that our bound is very Conservative. Part of the reason is that the bound is ConstruCted by
Considering the worse Case and we used a Conservative ChoiCe of the radius rQ and CoeffiCient cqπ ,k
in Eq. (13) (See Appendix H.2).
In Figure 2 we Compare different algorithms on more examples with δ = 0.1. We Can again see that
our method provides tight and Conservative interval that always Captures the true value. Although
FQE (Bootstrap) yields tighter intervals than our method, it fail to Capture the ground truth muCh
more often than the promised δ = 0.1 (e.g., it fails in all the random trials in Figure 2 (a)).
We ConduCt more ablation studies on different hyper-parameter and data ColleCting proCedure. See
Appendix H.2 and H.3 for more details.
6	Conclusion
We develop a dual approaCh to ConstruCt high ConfidenCe bounds for off-poliCy evaluation with an
improved rate over Feng et al. (2020). Our method Can handle dependent data, and does not require a
global optimization to get a valid bound. EmpiriCal results demonstrate that our bounds is tight and
valid Compared with a range of existing baseline. Future direCtions inClude leveraging our bounds for
poliCy optimization and safe exploration.
References
Sylvain Arlot, Gilles BlanChard, Etienne Roquain, et al. Some nonasymptotiC results on resampling
in high dimension, I: confidence regions. The Annals ofStatistics, 38(1):51-82, 2010.
Kavosh Asadi, Evan Cater, Dipendra Misra, and MiChael L Littman. EquivalenCe between wasserstein
and value-aware loss for model-based reinforcement learning. arXiv preprint arXiv:1806.01265,
2018.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
9
Published as a conference paper at ICLR 2021
Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice:
Off-policy confidence interval estimation. In Advances in Neural Information Processing Systems,
2020.
Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning, 2020.
Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with Gaussian processes. In
Proceedings ofthe 22nd international conference on Machine learning, pp. 201-208, 2005.
Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the Bellman equation. In Advances
in Neural Information Processing Systems, pp. 15456-15467, 2019.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel Bellman statistics. In International Conference on Machine Learning, 2020.
Raphael Fonteneau, Susan A. Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforcement
learning based on the synthesis of artificial trajectories. Annals of Operations Research, 208(1):
383-416, 2013.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016a.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016b.
Josiah P Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals for
off-policy evaluation. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Botao Hao, Yaqi Duan, Hao Lu, Csaba Szepesvari, Mengdi Wang, et al. Bootstrapping statistical
inference for off-policy evaluation. arXiv preprint arXiv:2102.03607, 2021.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Nan Jiang and Jiawei Huang. Minimax confidence interval for off-policy evaluation and policy
optimization. In Advances in Neural Information Processing Systems, 2020.
Nan Jiang and Lihong Li. Doubly robust off-policy evaluation for reinforcement learning. In
Proceedings of the 23rd International Conference on Machine Learning, pp. 652-661, 2016.
Ilya Kostrikov and Ofir Nachum. Statistical bootstrapping for uncertainty estimation in off-policy
evaluation. arXiv preprint arXiv:2007.13609, 2020.
Soumendra Nath Lahiri. Resampling methods for dependent data. Springer Science & Business
Media, 2013.
Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris Harris, and Dale
Schuurmans. A maximum-entropy approach to off-policy evaluation in average-reward MDPs. In
Advances in Neural Information Processing Systems, 2020.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018a.
Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A. Faisal, Finale Doshi-
Velez, and Emma Brunskill. Representation balancing MDPs for off-policy policy evaluation. In
Advances in Neural Information Processing Systems 31 (NeurIPS), pp. 2649-2658, 2018b.
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy
evaluation via conditional importance sampling. In International Conference on Machine Learning,
2020.
Ali Mousavi, Lihong Li, Qiang Liu, and Denny Zhou. Black-box off-policy estimation for infinite-
horizon reinforcement learning. In International Conference on Learning Representations, 2020.
10
Published as a conference paper at ICLR 2021
Susan A. Murphy, Mark van der Laan, and James M. Robins. Marginal mean models for dynamic
regimes. Journal of the American Statistical Association, 96(456):1410-1423, 2001.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems, pp. 2318-2328, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral
methods. Electron. J. Probab, 20(79):1-32, 2015.
Iosif Pinelis. An approach to inequalities for the distributions of infinite-dimensional martingales. In
Probability in Banach Spaces, 8: Proceedings of the Eighth International Conference, pp. 128-134.
Springer, 1992.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
Doina Precup. Temporal abstraction in reinforcement learning. ProQuest Dissertations and Theses,
2001.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the 17th International Conference on Machine Learning, pp. 759-
766, 2000.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform
martingale laws of large numbers. Probability Theory and Related Fields, 161(1-2):111-153,
2015.
Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal
of Machine Learning Research, 11(2), 2010.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. Adaptive Computation and Machine Learning series,
2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Alex Smola, Arthur Gretton, Le Song, and Bernhard SchOlkopf. A hilbert space embedding for
distributions. In Algorithmic learning theory, pp. 13-31. Springer, 2007.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, March 1998. ISBN 0-262-19398-1.
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias reduction
in infinite horizon off-policy estimation. In International Conference on Learning Representations
(ICLR), 2020a.
Ziyang Tang, Yihao Feng, Na Zhang, Jian Peng, and Qiang Liu. Off-policy interval estimation with
lipschitz value iteration. In Advances in Neural Information Processing Systems, 2020b.
Philip S Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts, 2015.
Philip S. Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning, pp. 2139-
2148, 2016.
11
Published as a conference paper at ICLR 2021
Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy
improvement. In Proceedings of the 32nd International Conference on Machine Learning, pp.
2380-2388, 2015a.
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy
evaluation. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015b.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for
off-policy evaluation. Proceedings of the 37th International Conference on Machine Learning,
2020.
Junfeng Wen, Bo Dai, Lihong Li, and Dale Schuurmans. Batch stationary distribution estimation. In
International Conference on Machine Learning, 2020.
Martha White and Adam White. Interval estimation for reinforcement-learning algorithms in
continuous-state domains. In Advances in Neural Information Processing Systems, pp. 2433-
2441, 2010.
Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Conference on Uncertainty in Artificial Intelligence (UAI), 2020.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for rein-
forcement learning with marginalized importance sampling. In Advances in Neural Information
Processing Systems, pp. 9668-9678, 2019.
Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, and Dale Schuurmans. Offline policy selection
under uncertainty. arXiv preprint arXiv:2012.06919, 2020a.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized Lagrangian. In Advances in Neural Information Processing Systems, 2020b.
Ming Yin and Yu-Xiang Wang. Asymptotically efficient off-policy evaluation for tabular reinforce-
ment learning. In Proceedings of the International Conference on Artificial Intelligence and
Statistics (AISTATS), 2020.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near optimal provable uniform convergence in off-policy
evaluation for reinforcement learning. arXiv preprint arXiv:2007.03760, 2020.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of
stationary values. In International Conference on Learning Representations, 2020a.
Shangtong Zhang, Bo Liu, and Shimon Whiteson. Gradientdice: Rethinking generalized offline
estimation of stationary values. In International Conference on Machine Learning, 2020b.
12
Published as a conference paper at ICLR 2021
A Proof of the Dual B ound in Theorem 4.2
Proof. Introducing a Lagrange multiplier, the bound in (8) is equivalent to
. W = m∈aχm≥in {ED∏,0 [q] - λ (m∈W1X h(Xi)Rq(xi,yi)
- εn
max min min ED	[q]
q∈Q λ≥0 h∈W	Dπ,0
max min ED [q] -
q∈Q ω∈Wo	Dπ,0
1n
-λ —〉h(xi)Rq(xi,yi)
n i=1
))
!)
1n
dω(xi)Rq(xi, yi) +εn kωkW	,
n i=1	o
	
where we use ω = λh(x), such that λ is replaced by kωkW . Define
1n
M (q, ω; Dn)= ED∏,0 [q]-工 ω(xi )Rq(Xi,yi) + εη kωkwo
i=1
=ED n [r] + ∆(D n , q) + εn kωkWo.
Then we have
ʌ
ʌ
ω
maxM(q, ω; Dn) = EDω [r] +max∆(Dn, q) + εn ∣∣ωkw
q∈Q	n	q∈Q
_	「 r	_	，	ʌ	..
o
=EDω [r] + IQ(ω; Dn) + εn kωkWc
=F+3.
o
Therefore,
「八7 = max min M(q, ω; DnJ
,W q∈Q ω∈Wo	n
-	♦	71 ʃ /	A ∖
≤ min max M(q, ω; Dn)
ω∈Wo q∈Q
=CmWoF+ (ω).
The lower bound follows analogously. The strong duality holds when the Slater’s condition is satisfied
(Nesterov, 2013), which amounts to saying that the primal problem in (8) is convex and strictly
feasible; this requires that Q is convex and there exists at least one solution q ∈ Q that satisfy that
constraint strictly, that is, LW (q; Dn) < εn; note that the objective function Q is linear on q and the
constraint function LW (q; Dn) is always convex on q (since it is the sup a set of linear functions on
q following (3)).
□
B Proof of Concentration B ound in Theorem 4.1
Our proof require the following Hoeffding inequality on Hilbert spaces by Pinelis (Theorem 3, 1992);
see also Section 2.4 of Rosasco et al. (2010).
Lemma B.1. (Theorem 3, Pinelis, 1992) Let H be a Hilbert space and {fi }in=1 is a Martingale
sequence in H that satisfies supi ∣fi ∣H ≤ σ almost surely. We have for any > 0,
≤ 2 exp
H
Therefore, with probability at least 1 一 δ, we have ∣∣ 1 Pn= 1 fi ∣∣h ≤ J2σ2 long(2/^.
13
Published as a conference paper at ICLR 2021
Lemma B.2. Let k(x, x0) be a positive definite kernel whose RKHS is Hk. Define
fi(∙) = Rq(xi, yi)k(xi, ∙) - Rnq(xi)k(xi, ∙).
Assume Assumption 2.1 holds, then {fi }in=1 is a Martingale difference sequence in Hk w.r.t. T<i :
(xj, yj)j< ∪ (Xi). That is, E [fi+ι(∙) | T<i] = 0. In addition,
1n
-X fi
n
i=1
2	1n
=n X (Rq(χi, yi) - Rnq(χi)) k(χi, Xj) (Rq(χj, yj) - Rnq(χj)),
Hk	ij=1
and kfik2Hk ≤ cq,k for ∀i = 1, . . . ,n.
Proof of Theorem 4.1. Following Lemma B.1 and Lemma B.2, since {fi }in=1 is a Martingale differ-
ence sequence in Hk with kfikH ≤ cq,k almost surely, we have with probability at least 1 - δ,
1n
—X (Rq(xi,yi) - Rnq(xi)) k(xi,Xj) (Rq(xj,y) - Rnq(xj))
n ij=1
1n
-X fi
n
i=1
2
≤
Hk
2cq,k log(2∕δ)
n
Using Lemma B.3 below, we have
∣Lκ(q; Dn)-LK(q; Dn)∣≤ 1Xfi	≤ r2cq'kl?2/').
i=1	Hk
This completes the proof.
□
Lemma B.3. Assume k(X, X0 ) is a positive definite kernel. We have
∣2	1 n
LK(q; Dn) - Lκ(q; Dn)∣ ≤ / X (Rq(χi,yi) - Rnq(χi)) k(χi,χj) (Rq(χj,yj∙) - Rnq(χj)).
n ij=1
Proof. Define
1n
^(∙) = —〉Rq(χi, yi)k(χi, ∙)
n
i=1
1n
g(∙) = — ERnq(Xi)k(Xi, •).
i=1
Then we have
1n
2
IIgkHk =—工 Rq(Xi,yi)k(Xi,xj)Rq(Xj,yj∙) = Lκ(q; Dn),
—	ij=1
1n
21
IIgkHk =—工 Rnq(Xi)k(Xi, Xj)Rnq(Xj) = Lκ(q; Dn),
—	ij=1
1n
kg - gkHk = — X (Rq(Xi,yi) - Rnq(Xi)) k(Xi,Xj) (Rq(Xj,y) - Rnq(∙
— ij=1
The result then follows the triangle inequality IkgkHk — kgkHJ ≤kg - gkH.
□
B.1 CALCULATION OF cqπ ,k
The practical calculation of the coefficient cqπ ,kin the concentration inequality was discussed in Feng
et al. (2020), which we include here for completeness.
Lemma B.4. (Feng et al. (2020) Lemma 3.1) Assume the reward function and kernel function is
bounded with supx |r(X)| ≤ rmax and supx,x0 |k(X, X0)| ≤ Kmax, we have:
cqπ ,k
sup (Rqn(x, y))2k(X, x) ≤
x∈X,y∈Y
4KmaXrmaX
(i-γ)2
In practice, we get access to Kmax from the kernel function that we choose (e.g., Kmax = 1for RBF
kernels), and rmaX from the knowledge on the environment.
14
Published as a conference paper at ICLR 2021
C More on the Tightness of the Confidence Interval
The benefit of having both upper and lower bounds is that we can empirically access the tightness of
+
the bound by checking the length of the interval [FQ-(ω-), FQ+(ω+)]. However, from the theoretical
perspective, it is desirable to know a priori that the length of the interval will decrease with a fast rate
as the data size n increases. We now show that this is the case if Wo is chosen to be sufficiently rich
so that it includes a ω ∈ Wo such that Dnω ≈ Dπ .
__	_ _	一	_	q*一	_
Theorem C.1. Assume Wo is sufficiently rich to include a good ω* in Wo with Dn ≈ Dn in that
sup
q∈Q
[Rq(x; x0,r)i - EDnhRq(x; x0，r)i | ≤ nCα，
(19)
where c and α are two positive coefficients. Then we have
max
- Jπ ,
Jπ - JQ-,W	≤
nα+ɛn ∣∣ω*kwo.
Assumption (19) holds if Dn is collected following a Markov chain with certain strong mixing
condition and weakly converges to some limit discussion D∞ whose support is X, for which we can
define ω*(x) = D∏(x)∕D∞(x). In this case, if Q is a finite ball in RKHS, then we can achieve (19)
with α = 1/2, and yields the overall bound of rate O(n-1/2). For more general function classes,
a depends on the martingale Rademacher complexity of function set RRQ = {Rq(χ, y): q ∈ Q}
Rakhlin et al. (2015). In our empirical reults, we observe that the gap of the practically constructed
bounds tend to follow the O(n-1/2) rate.
Proof. Note that
Jπ = EDπ [r] = EDπ [r],
and
Q(ω; Dn) = SUp IEdω [γq(x0) - q(x)] - Ed∏ [γq(x0) - q(x)]}.
Because ω* ∈ W, we have
JWQ - Jn ≤ F+(ω*)- Jn
=ED n [r] - EDn [r] + IQ(ωn; Dn) + εn ∣∣ω IlWo
=SUp{edω [Rq(χ,y)] -Ed∏ [Rq(χ,y)]} + εn ∣∣ω*∣wo
≤ F + εn llω*kWo .
nα	o
The case of lower bound follows similarly.	□
D OPTIMIZATION ON Wo
Consider the optimization of ω in Wo
F+(ω) := 1 XX riω(xi)+Q(ω; Dn) + ∣∣ω∣W J2cqn,k log,20	(20)
n	on
i=1
Assume Wo is the RKHS of kernel k(χ, x), that is, Wo = Hk. By the finite representer theorem of
RKHS (Smola et al., 2007). the optimization of ω in RKHS Hk can be reduced to a finite dimensional
optimization problem. Specifically, the optimal solution of (20) can be written into a form of
ω(x) = Pin=1 k(x, xi)αi with ∣ω∣2H = Pin,j=1 k(xi, xj)αiαj for some vector α := [αi]in=1 ∈ Rn.
Write K = [k(xi, xj)]in,j=1 and r = [ri]in=1. The optimization of ω reduces to a finite dimensional
optimization on α:
min r>κ Kα + IQ (Kα; D n) + √αKαJ 20qn ,k 匕虱?/"),
α∈Rn n	n	n
15
Published as a conference paper at ICLR 2021
where
Iβ(Kα; Dn) = max [ed∏ 0 [q] +-(Tq)>Kα],
q∈Q	π,0	n
and Tq = [γq(x0i) - q(xi)]in=1. When Q is RKHS, we can calculate IQ (Kα; Dn) using (22) in
section F.
This computation can be still expensive when n is large. Fortunately, our confidence bound holds
for any ω; better ω only gives tighter bounds, but it is not necessary to find the global optimal ω .
Therefore, one can use any approximation algorithm to find ω , which provides a trade-off of tightness
and computational cost. We discuss two methods:
1)	Approximating α The length of α can be too large when n is large. To address this, we assume
αi = g(xi, θ), where g is any parametric function (such as a neural network) with parameter θ which
can be much lower dimensional than α. We can then optimize θ with stochastic gradient descent, by
approximating all the data averaging n Pn=1(∙) with averages over small mini-batches; this would
introduce biases in gradient estimation, but it is not an issue when the goal is only to get a reasonable
approximation.
2)	Replacing kernel k Assume the kernel k yields a random feature expansion: k(x, x) =
Ee〜∏ [φ(χ,β)φ(x,β)], where φ(χ,β) is a feature map with parameter β and ∏ is a distribution
of β. We draw {βi}im=1 i.i.d. from π, where m is taken to be much smaller than n. We replace k with
⅛(χ, X)= + Pm=I φ(χ, βi)φ(x, βi) and Hk with H^, That is, we consider to solve
J	f +	1 G	八	∣2cq ^ log(2∕δ) 1
J+,W = ωmin ∖ F+(ω) := n X riω(xi) + IQ (ω; Dn) + kωkH^ y π, n—.
It is known that any function ω in H^ can be represented as ω(x) = m Pm=I wiφ(x, βi), for
some W = [wi]m=1 ∈ Rm and satisfies ∣∣ω∣∣H = * Pm=I w2∙ In this way, the problem reduces
to optimizing an m-dimensional vector w , which can be solved by standard convex optimization
techniques.
E Concentration Inequalities of General Functional Bellman
Losses
When K is a general function set, one can still obtain a general concentration bound using Rader-
macher complexity. Define Rq ◦ W := {h(x, y) = Rq(x, y)ω(x) : ω ∈ W}. Using the standard
derivation in Radermacher complexity theory in conjunction with Martingale theory (Rakhlin et al.,
2015), we have
n X(Rq(xi, yi) - R∏q(xi))ω(xi) I ≤ 2Rad(Rq ◦ W) + J
i=1
2cqlog(2∕δ)
sup
ω∈W
—一 ,,^
n
where Rad(Rq ◦ K) is the sequential Radermacher complexity as defined in (Rakhlin et al., 2015).
A triangle inequality yields
| Lk(q; Dn) - Lk(q; Dn) | ≤ sup 1 - X(Rq(xi,yi) - R∏q(xi))ω(xi)”
ω∈W n i=1
Therefore,
,一 , ʌ _ , ʌ . . 一一 __________^ 一一 /
| Lw(q; Dn)- LW(q; Dn) | ≤ 2Rad(Rq ◦W) + y -
2cqlog(2∕δ)
(21)
n
where Cq,w = supω∈w supχ,y (Rq(x, y) - R∏q(χ))2ω(x)2. When W equals the unit ball K of the
RKHS related to kernel k, we have cq,k = cq,W, and hence this bound is strictly worse than the
bound in Theorem 4.1.
16
Published as a conference paper at ICLR 2021
「	「	ɪ /	A. ∖	χ-⅛	ɪʌ ɪ 7- ɪ ɪ , (
F	CLOSED	FORM	OF IQ(ω;	Dn)	WHEN Q	IS RKHS
Similar to LK(q; Dn), when Q is taken to be the unit ball K of the RKHS of a positive definite kernel
k(χ, x), (7) can be expressed into a bilinear closed form shown m MoUsavi et al. (2020):
Iq(ω; Dn) = A — 2B + C,	(22)
where
A = E(x,X)〜D∏,0×D∏,0 [k(X, X)]
B = E(X㈤〜Dn×d∏,o [Tnk(X，x)i
C = E(X㈤〜Dω×DωhτnTnk(x,x)i,
were T∏f (x) = Yf (x ) - f (x); in T∏T∏k(x, x), we apply T∏ and T∏ in a sequential order by
treating k as a function of X and then of x.
G More on the Oracle Bound and its dual form
The oracle bound (18) provides another starting point for deriving optimization-based confidence
bounds. We derive its due form here. Using Lagrangian multiplier, the optimization in (18) can be
rewritten into
JQ = = maxminM(q,ω; Dn),	(23)
" q∈Q ω
1n
where M*(q,ω; Dn = Ed∏,o [q]——Xω(xi) (Rq(xi,yi) - Rq∏(xi,yi,,
- i=1
where ω now serves as the Lagrangian multiplier. By the weak duality, we have
Jq,+ ≤ F+*(ω) := Edω[r] + I-(ω; Dn) + R(ω, q∏),	∀ω.
n 、 _ _ /
X---------------} '---{---}
{z'∖^^^^
known
unknown
and
1n
.,	、 ɪ x---Λ , k ^	,
R(ω,q∏) = - 2^ω(xi)Rq∏ (Xi).
i=1
EI 1 ∙ . ∙	/'ll	∙ ∙ 1 1 ∕' .1 1	F	1 n /'	_ 、A，	1	Γ ʃ——	ʃ-l- 1 ,-
The derivation follows similarly for the lower bound. So for any ω ∈ W0, we have [J- *, JQ J ⊆
,,
产Q,*(ω), FQ,*(ω)]∙
Here the first two terms of 户Q * (ω) can be empirically estimated (it is the same as the first two terms
of (16)), but the third term R(ω, qn) depends on the unknown qn and hence need to be further upper
bounded.
Our method can be viewed as constraining ω in W, which is assumed to be the unit ball of Wo , and
applying a worst case bound:
FQ,*(ω) := EDω [r] + IQ(ω; D n) + R(ω, qn),	∀ω ∈ Wo
≤ Edω [r] + lQ(ω; Dn) + IlwkW SuP R(h, q∏),	∀ω ∈ Wo
n	o h∈W
≤ ED ω [r] + IQ(ω; Dn) + IlwkWoLW (q∏, Dn), ∀ω ∈ Wo
w.p.1-δ
≤	EDω[r]+ IQ(ω; Dn) + C kwkWo , ∀ω ∈ Wo
=FQ (ω).
where the last step applies the high probability bound that Pr(LW (qn, Dn) ≤ ε) ≥ 1 - δ. Similar
derivation on the lower bound counterpart gives
Pr ([F‰(ω), FQ,*(ω)i ⊆ [Fq (ω), FQ(ω)]) ≥ 1 - δ.
Q
Therefore, our confidence bound [FQ- (ω), FQQ(ω)] is a 1 - δ confidence outer bound of the oracle
bound [J-,*, JQ,*] ⊆ [FQ,*(ω), FQ,*(ω)].
17
Published as a conference paper at ICLR 2021
Introducing Q is necessarily Our method does not require any independence assumption between
the transition pairs, the trade-off is that that we have to assume that qπ falls into a function set
Q that imposes certain smoothness assumption. This is necessary because the data only provide
information regarding qπ on a finite number of points, and qπ can be arbitrarily non-smooth outside of
the data points, and hence no reasonable upper/lower bound can be obtained without any smoothness
condition that extend the information on the data points to other points in the domain.
Proposition G.1. Unless Prs~Dπ,0 (S ∈ {si, si}n=ι) = 0, for any U ∈ R, there exists a function
q: S × A → R, such that
EDπ,0 [q] = u,	Rq(xi,yi) = Rqπ(xi, yi), ∀i = 1, . . . , n.
Proof. Let Qnull be the set of functions that are zero on {si, s0i}in=1, that is,
Qnull = {g : S × A → R : g(s, a) = 0, ∀s ∈ {si, s0i}in=1 , a ∈ A}.
Then we have
Rπ (qπ + g)(xi, yi ) = Rπqπ (xi , yi ),	∀i = 1, . . . , n.
and
EDπ,0 [qπ + g] = EDπ,0[qπ] +EDπ,0[g] = Jπ +EDπ,0[g].
Taking g(s, a) = zI(s ∈/ {si, s0i}in=1), where z is any real number. Then we have
ED∏,0 [q∏ + g] = Jn + ZPrs~D∏,0 (S ∈ {si,si}n=I).
Because Prs~Dπ,o (s ∈ {si, si}n=ι). = 0, We can take Z to be arbitrary value to make Edπ,0 [q∏ + g]
to take arbitrary value.	口
18
Published as a conference paper at ICLR 2021
)htgnel lavretni(01gol
IO2 ......IO3 .......IO4
IO2 IO3 IO4
(a) Number of transitions, n
(b) Number of transitions n
Figure 3: Ablation study on the radius rQ of the function class Q. The default collecting procedure
uses a horizon length of H = 50. The discounted factor is γ = 0.95 by default.
—H- scale=l
—θ- scale=10
scale=100
H	Ablation Study and Experimental Details
H.1 Experimental Details
Environments and Dataset Construction We test our method on three environments: Inverted-
Pendulum and CartPole from OpenAI Gym (Brockman et al., 2016), and a Type-1 Diabetes
medical treatment simulator. For Inverted-Pendulum we discretize the action space to be
{-1, -0.3, -0.2, 0, 0.2, 0.3, 1}. The action space of CartPole and the medical treatment simulator
are both discrete.
Policy Construction We follow a similar setup as Feng et al. (2020) to construct behavior and
target policies. For all of the environments, we constraint our policy class to be a softmax policy and
use PPO (Schulman et al., 2017) to train a good policy π, and we use different temperatures of the
softmax policy to construct the target and behavior policies (we set the temperature τ = 0.1 for target
policy and τ = 1 to get the behavior policy, and in this way the target policy is more deterministic
than the behavior policy). We consider other choices of behavior policies in Section H.3.
For horizon lengths, We fix γ = 0.95 and set horizon length H = 50 for Inverted-Pendulum,
H = 100 for CartPole, and H = 50 for Diabetes simulator.
Algorithm Settings We test the bound in Eq.(16)-(17). Throughout the experiment, we always set
W = K, a unit ball of RKHS with kernel k(∙, ∙). We set Q = rgK, the zero-centered ball of radius
rQ In an RKHS with kernel k(∙, ∙). We take both k and k to be Gaussian RBF kernel. The bandwidth
of k and k are selected to make sure the function Bellman loss is not large on a validation set. The
radius is selected to be sufficiently large to ensure that q* is included in Q. To ensure a sufficiently
large radius, we use the data to approximate a q so that its functional Bellman loss is small than
∈n. Then we set γq = 10 * ∣∣qkzκ. We optimize ω using the random feature approximation method
described in Appendix D. Once ω+ and ω- are found, we evaluate the bound in Eq. (16) exactly, to
ensure the theoretical guarantee holds.
H.2 Sensitivity to Hyper-Parameters
We investigate the sensitivity of our algorithm to the choice of hyper-parameters. The hyper-parameter
mainly depends on how we choose our function class Q and W .
Radius of Q Recall that we choose Q to be a ball in RKHS with radius rQ, that is,
Q = TQK = {rQf : f ∈ K},
where K is the unit ball of the RKHS with kernel k. Ideally, we want to ensure that tq ≥ k q* k k so
that q* ∈ Q.
19
Published as a conference paper at ICLR 2021
PJEMər
0.0	0.5	1.0	1.5	2.0
Temperature T
(a) Varying temperature τ.
q-u-IEAJəei
(b) Varying the bandwidth of kernels in Wo and Q.
hk = 0.1
hk = 0.5
hk = 0.7
hk = 1.0
hk = 5.0
Figure 4: Ablation studies on Inverted-Pendulum. We change the temperature τ of the behavior policies in (a),
and change the bandwidth of the kernel k of Wo and the kernel k of Q (denoted by h吞 in (b)).
Since it is hard to analyze the behavior of the algorithm when q* is unknown, we consider a synthetic
environment where the true q* is known. This is done by explicitly specifying a q* inside K and then
infer the corresponding deterministic reward function r(x) by inverting the Bellman equation:
r(x)：= q*(χ) - YEχo~p∏(∙∣x)[q*(χ0)].
Here r is a deterministic function, instead of a random variable, with an abuse of notation. In this
way, we can get access to the true RKHS norm of q*:
ρ* = kq*kκ.
For simplicity, we set both the state space S and action space A to be R and set a Gaussian policy
∏(a∣s) 8 exp(f (s, a)∕τ), where T is a positive temperature parameter. We set T = 0.1 as target
policy and τ = 1 as behavior policy.
Figure 3 shows the results as we set rQ to be ρ*, 10ρ* and 100ρ*, respectively. We can see that the
tightness of the bound is affected significantly by the radius when the number n of samples is very
small. However, as the number n of samples grow (e.g., n ≥ 2 × 103 in our experiment), the length
of the bounds become less sensitive to the changing of the predefined norm of Q.
Similarity Between Behavior Policy and Target Policy We study the performance of changing
temperature of the behavior policy. We test on Inverted-Pendulum environment as previous described.
Not surprisingly, we can see that the closer the behavior policy to the target policy (with temperature
T = 0.1), the tighter our confidence interval will be, which is observed in Figure 4(a).
TΓ* F ♦ 1,T i> TΓ⅛ TΓ* Trr I	I ɪɪ τ , 1 ,1	I ,	F	,F KF ♦ F,F ∙ 1	IT F 7
Bandwidth of RBF kernels We study the results as we change the bandwidth in kernel k and k
for W and Q, respectively. Figure 4(b) shows the length of the confidence interval when we use
different bandwidth pairs in the Inverted-Pendulum environment. We can see that we get relatively
tight confidence bounds as long as we set the bandwidth in a reasonable region (e.g., we set the
_ - . - - . . . r . . r - _ - . - - . r . r .
bandwidth of k in [0.1,0.5], the bandwidth of k in [0.5, 3]).
H.3 Sensitivity to the Data Collection Procedure
We investigate the sensitivity of our method as we use different behavior policies to collect the dataset
D n.
Varying Behavior Policies We study the effect of using different behavior policies. We consider
the following cases:
1. Data is collected from a single behavior policy of form πα = απ + (1 - α)π0, where π is
the target policy and π0 is another policy. We construct π and π0 to be Gaussian policies of
form π(a∣s) 8 exp(f (s, a)∕τ) with different temperature T, where temperature for target
policy is T = 0.1 and temperature for π0 is T = 1.
20
Published as a conference paper at ICLR 2021
)htgnel lavretni( go
number of transitions, n
-0.35
-0.40
-0.45
-0.50
-0.55
-0.60
Trajectory length T
Trajectory length T
(a) Varying n, α, fixed γ = 0.95	(b) Varying n, T, fixed γ = 0.95 (c)Varying n, α, fixed γ = 0.99
Figure 5: Ablation studies on the data collection procedure, as we (a) change the behavior policies, and (b)-(c)
change the trajectory lengths. The other settings are the same as that in Figure 3.
2. The dataset Dn is the combination of the data collected from multiple behavior policies of
form πα defined as above, with α ∈ {0.0, 0.2, 0.4, 0.6, 0.8}.
We show in Figure 5(a) that the length of the confidence intervals by our method as we vary the
number n of transition pairs and the mixture rate α. We can see that the length of the interval decays
with the sample size n for all mixture rate α. Larger α yields better performance because the behavior
policies are closer to the target policy.
Varying Trajectory Length T in Dn As we collect Dn, we can either have a small number of
long trajectories, or a larger number of short trajectories. In Figure 5(b)-(c), we vary the length T of
the trajectories as We collect Dn, while fixing the total number n of transition pairs. In this way, the
number of trajectories in each Dn would be m = n/T.We can see that the trajectory length does not
impact the results significantly, especially when the length is reasonably large (e.g., T ≥ 20).
21
Published as a conference paper at ICLR 2021
I More Related Works
We give a more detailed overview of different approaches for uncertainty estimation in OPE.
Finite-Horizon Importance Sampling (IS) Assume the data is collected by rolling out a known
behavior policy π0 up to a trajectory length T , then we can estimate the finite horizon reward
by changing E∏,p[∙] to E∏o,p[∙] with importance SamPling(e.g., PrecUP et al., 2000; Precup, 2001;
Thomas et al., 2015a;b). Taking the trajectory-wise importance sampling as an example, assume we
collect a set of independent trajectories τi := {sit, ait, rti}tT=-01, i = 1, . . . , m up to a trajectory length
T by unrolling a known behavior policy ∏o. When T is large, We can estimate J* by a weighted
averaging:
m	T-1	i i	T-1
JIS =	-1 X 3(Ti)J(Ti),	where ω(Ti)	= Y ⅛	, J(Ti)	= X Ytri.	(24)
m i=1	皆 π0⑷ISt)	=
One can construct non-aSymPtotiC confidence bounds based on JIS using variants of concentration
inequalities (Thomas, 2015; Thomas et al., 2015b). Unfortunately, a key problem with this IS
estimator is that the importance weight 3(Ti) is a product of the density ratios over time, and hence
tends to cause an explosion in variance when the trajectory length T is large. Although improvement
can be made by using per-step and self-normalized weights (Precup, 2001), or control variates (Jiang
& Li, 2016; Thomas & Brunskill, 2016), the curse of horizon remains to be a key issue to the classical
IS-based estimators (Liu et al., 2018a).
Moreover, due to the time dependency between the transition pairs inside each trajectory, the non-
asymptotic concentration bounds can only be applied on the trajectory level and hence decay with
the number m of independent trajectories in an O(1/√m) rate, though m can be small in practice.
We could in principle apply the concentration inequalities of Markov chains (e.g., Paulin, 2015) to
the time-dependent transition pairs, but such inequalities require to have an upper bound of certain
mixing coefficient of the Markov chain, which is unknown and hard to construct empirically. Our
work addresses these limitations by constructing a non-asymptotic bound that decay with the number
n = mT of transitions pairs, while without requiring known behavior policies and independent
trajectories.
Infinite-Horizon, Behavior-Agnostic OPE Our work is closely related to the recent advances in
infinite-horizon and behavior-agnostic OPE, including, for example, Liu et al. (2018a); Feng et al.
(2019); Tang et al. (2020a); Mousavi et al. (2020); Liu et al. (2020); Yang et al. (2020b); Xie et al.
(2019); Yin & Wang (2020), as well as the DICE-family (e.g., Nachum et al., 2019a;b; Zhang et al.,
2020a; Wen et al., 2020; Zhang et al., 2020b). These methods are based on either estimating the value
function, or the stationary visitation distribution, which is shown to form a primal-dual relation (Tang
et al., 2020a; Uehara et al., 2020; Jiang & Huang, 2020) that we elaborate in depth in Section 3.
Besides Feng et al. (2020) which directly motivated this work, there has been a recent surge of interest
in interval estimation under infinite-horizon OPE (e.g., Liu et al., 2018b; Jiang & Huang, 2020; Duan
et al., 2020; Dai et al., 2020; Feng et al., 2020; Tang et al., 2020b; Yin et al., 2020; Lazic et al.,
2020). For example, Dai et al. (2020) develop an asymptotic confidence bound (CoinDice) for DICE
estimators with an i.i.d assumption on the off-policy data; Duan et al. (2020) provide a data dependent
confidence bounds based on Fitted Q iteration (FQI) using linear function approximation when the
off-policy data consists of a set of independent trajectories; Jiang & Huang (2020) provide a minimax
method closely related to our method but do not provide analysis for data error; Tang et al. (2020b)
propose a fixed point algorithm for constructing deterministic intervals of the true value function
when the reward and transition models are deterministic and the true value function has a bounded
Lipschitz norm.
Model-Based Methods Since the model P is the only unknown variable, we can construct an
estimator P of P using maximum likelihood estimation or other methods, and plug it into Eq. (1)
to obtain a plug-in estimator J = J ^. This yields the model-based approach to OPE (e.g., Jiang
π,P
& Li, 2016; Liu et al., 2018b). One can also estimate the uncertainty in J ^ by propagating the
π,P
. .∙ . ∙ A Z	A	1∙ . 1 CCTC 1 ʌ	. 1 CCCC' 1 . ∙ . ∙ < 1 .	1 .	.
uncertatinty in P (e.g., Asadi et al., 2018; Duan et al., 2020), but it is hard to obtain non-asymptotic
22
Published as a conference paper at ICLR 2021
1	∙	11	i'{'	♦,[	1	1	A ∙	1.1	∙	1	1 ∙	11 T	1
and computationally efficient bounds unless P is assumed to be simple linear models. In general,
estimating the whole model P can be an unnecessarily complicated problem as an intermediate step
of the possibly simpler problem of estimating Jπ,P.
Bootstrapping, Bayes, Distributional RL As a general approach of uncertainty estimation, boot-
strapping has been used in interval estimation in RL in various ways (e.g., White & White, 2010;
Hanna et al., 2017; Kostrikov & Nachum, 2020; Hao et al., 2021). Bootstrapping is simple and
highly flexible, and can be applied to time-dependent data (as appeared in RL) using variants of block
bootstrapping methods (e.g., Lahiri, 2013; White & White, 2010). However, bootstrapping typically
only provides asymptotic guarantees; although non-asymptotic bounds of bootstrap exist (e.g., Arlot
et al., 2010), they are sophistic and difficult to use in practice and would require to know the mixing
condition for the dependent data. Moreover, bootstrapping is time consuming since it requires to
repeat the whole off-policy evaluation pipeline on a large number of resampled data.
Bayesian methods (e.g., Engel et al., 2005; Ghavamzadeh et al., 2016b; Yang et al., 2020a) offer
another general approach to uncertainty estimation in RL, but require to use approximate inference
algorithms and do not come with non-asymptotic frequentist guarantees. In addition, distributional
RL (e.g., Bellemare et al., 2017) seeks to quantify the intrinsic uncertainties inside the Markov
decision process, which is orthogonal to the epistemic uncertainty that we consider in off-policy
evaluation.
23