Published as a conference paper at ICLR 2021
Neural Mechanics: symmetry and broken con-
SERVATION LAWS IN DEEP LEARNING DYNAMICS
Daniel Kunin*, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, Hidenori Tanaka*,
Stanford University
f Physics & Informatics Laboratories, NTT Research, Inc.
Ab stract
Understanding the dynamics of neural network parameters during training is one of
the key challenges in building a theoretical foundation for deep learning. A central
obstacle is that the motion of a network in high-dimensional parameter space
undergoes discrete finite steps along complex stochastic gradients derived from
real-world datasets. We circumvent this obstacle through a unifying theoretical
framework based on intrinsic symmetries embedded in a network’s architecture
that are present for any dataset. We show that any such symmetry imposes strin-
gent geometric constraints on gradients and Hessians, leading to an associated
conservation law in the continuous-time limit of stochastic gradient descent (SGD),
akin to Noether’s theorem in physics. We further show that finite learning rates
used in practice can actually break these symmetry induced conservation laws.
We apply tools from finite difference methods to derive modified gradient flow,
a differential equation that better approximates the numerical trajectory taken by
SGD at finite learning rates. We combine modified gradient flow with our frame-
work of symmetries to derive exact integral expressions for the dynamics of certain
parameter combinations. We empirically validate our analytic expressions for
learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting
symmetry, our work demonstrates that we can analytically describe the learning
dynamics of various parameter combinations at finite learning rates and batch sizes
for state of the art architectures trained on any dataset.
1	Introduction
Just like the fundamental laws of classical and quantum mechanics taught us how to control and
optimize the physical world for engineering purposes, a better understanding of the laws governing
neural network learning dynamics can have a profound impact on the optimization of artificial neural
networks. This raises a foundational question: what, if anything, can we quantitatively understand
about the learning dynamics of large-scale, non-linear neural network models driven by real-world
datasets and optimized via stochastic gradient descent with a finite batch size, learning rate, and with
or without momentum? In order to make headway on this extremely difficult question, existing works
have made major simplifying assumptions on the network, such as restricting to identity activation
functions Saxe et al. (2013), infinite width layers Jacot et al. (2018), or single hidden layers Saad &
Solla (1995). Many of these works have also ignored the complexity introduced by stochasticity and
discretization by only focusing on the learning dynamics under gradient flow. In the present work, we
make the first step in an orthogonal direction. Rather than introducing unrealistic assumptions on the
model or learning dynamics, we uncover restricted, but meaningful, combinations of parameters with
simplified dynamics that can be solved exactly without introducing a major assumption (see Fig. 1).
To find the parameter combinations, we use the lens of symmetry to show that if the training loss
doesn’t change under some transformation of the parameters, then the gradient and Hessian for those
parameters have associated geometric constraints. We systematically apply this approach to modern
neural networks to derive exact integral expressions and verify our predictions empirically on large
scale models and datasets. We believe our work is the first step towards a foundational understanding
* Equal contribution. Correspondence to kunin@stanford.edu & hidenori.tanaka@ntt-research.com
1
Published as a conference paper at ICLR 2021
of neural network learning dynamics that is not based in simplifying assumptions, but rather the
simplifying symmetries embedded in a network’s architecture. Our main contributions are:
1.	We leverage continuous differentiable symmetries in the loss to unify and generalize geo-
metric constraints on neural network gradients and Hessians (section 3).
2.	We prove that each of these differentiable symmetries has an associated conservation law
under the learning dynamics of gradient flow (section 4).
3.	We construct a more realistic continuous model for stochastic gradient descent by modeling
weight decay, momentum, stochastic batches, and finite learning rates (section 5).
4.	We show that under this more realistic model the conservation laws of gradient flow are bro-
ken, yielding simple ODEs governing the dynamics for the previously conserved parameter
combinations (section 6).
5.	We solve these ODEs to derive exact learning dynamics for the parameter combinations,
which we validate empirically on VGG-16 trained on Tiny ImageNet with and without batch
normalization (section 6).
2	Related work
The goal of this work is to construct a theoret-
ical framework to better understand the learn-
ing dynamics of state-of-the-art neural networks
trained on real-world datasets. Existing works
have made progress towards this goal through
major simplifying assumptions on the architec-
ture and learning rule. Saxe et al. (2013; 2019)
and Lampinen & Ganguli (2018) considered
linear neural networks with specific orthogo-
nal initializations, deriving exact solutions for
the learning dynamics under gradient flow. The
theoretical tractability of linear networks has
further enabled analyses on the properties of
loss landscapes Kawaguchi (2016), convergence
Arora et al. (2018a); Du & Hu (2019), and
implicit acceleration by overparameterization
Arora et al. (2018b). Saad & Solla (1995) and
Goldt et al. (2019) studied single hidden layer
architectures with non-linearities in a student-
teacher setup, deriving a set of complex ODEs
describing the learning dynamics. Such shal-
low neural networks have also catalyzed recent
major advances in understanding convergence
(a) Parameter Dynamics (b) Neuron Dynamics
Figure 1: Neuron level dynamics are simpler
than parameter dynamics. We plot the per-
parameter dynamics (left) and per-channel squared
Euclidean norm dynamics (right) for the convo-
lutional layers of a VGG-16 model (with batch
normalization) trained on Tiny ImageNet with
SGD with learning rate η = 0.1, weight decay
λ = 10-4, and batch size S = 256. While the pa-
rameter dynamics are noisy and chaotic, the neuron
dynamics are smooth and patterned.
properties of neural networks Du et al. (2018b); Mei et al. (2018). Jacot et al. (2018) considered
infinitely wide neural networks with non-linearities, demonstrating that the network’s prediction
becomes linear in its parameters. This setting allows for an insightful mathematical formulation
of the network’s learning dynamics as a form of kernel regression where the kernel is defined by
the initialization (though see also Fort et al. (2020)). Arora et al. (2019) extended these results to
convolutional networks and Lee et al. (2019) demonstrated how this understanding also allows for
predictions of parameter dynamics.
In the present work, we make the first step in an orthogonal direction. Instead of introducing unrealistic
assumptions, we discover restricted combinations of parameters for which we can find exact solutions,
as shown in Fig. 1. We make this fundamental contribution by constructing a framework harnessing
the geometry of the loss shaped by symmetry and realistic continuous equations of learning.
Geometry of the loss. A wide range of literature has discussed constraints on gradients originating
from specific architectural building blocks of networks. For the first part of our work, we simplify,
unify, and generalize the literature through the lens of symmetry.
The earliest works understanding the importance of invariances in neural networks come from the loss
landscape literature Baldi & Hornik (1989) and the characterization of critical points in the presence
2
Published as a conference paper at ICLR 2021
of explicit regularization Kunin et al. (2019). More recent works have studied implicit regularization
originating from linear Arora et al. (2018b) and homogeneous Du et al. (2018a) activations, finding
that gradient geometry plays an important role in constraining the learning dynamics. A different line
of research studying the generalization capacity of networks has noticed similar gradient structures
Liang et al. (2019). Beyond theoretical studies, geometric properties of the gradient and Hessian have
been applied to optimize Neyshabur et al. (2015), interpret Bach et al. (2015), and prune Tanaka et al.
(2020) neural networks.
Gradient properties introduced by batch Ioffe & Szegedy (2015), weight Salimans & Kingma (2016)
and layer Ba et al. (2016) normalization have been intensely studied. Van Laarhoven (2017); Zhang
et al. (2018) showed that normalization layers are scale invariant, but have an implicit role in
controlling the effective learning rate. Cho & Lee (2017); Hoffer et al. (2018); Chiley et al. (2019);
Li & Arora (2019); Wan et al. (2020) have leveraged the scale invariance of batch normalization to
understand geometric properties of the learning dynamics. Most recently, Li et al. (2020) studied
the role of gradient noise in reconciling the empirical dynamics of batch normalization with the
theoretical predictions given by continuous models of gradient descent.
Equations of learning. To make experimentally testable predictions on learning dynamics, we
introduce a continuous model for stochastic gradient descent (SGD). There exists a large body of
works studying this subject using stochastic differential equations (SDEs) in the continuous-time
limit Mandt et al. (2015; 2017); Li et al. (2017); Smith &Le (2017); Chaudhari & Soatto (2018);
Jastrzebski et al. (2017); ZhU et al. (2018); An et al. (2018). Each of these works involves making
specific assumptions on the loss or the noise in order to derive stationary distributions. More careful
treatment of stochasticity led to flUctUation dissipation relationships at steady state withoUt sUch
assUmptions Yaida (2018). In oUr analysis, we apply a more recent approach Li et al. (2017); Barrett
& Dherin (2020), inspired by finite difference methods, that aUgments SDE model with higher-order
terms to accoUnt for the effect of a finite step size and cUrvatUre in the learning trajectory.
3 S ymmetries in the Loss shape Gradient and Hes s ian Geometries
While we initialize neUral networks randomly,
their gradients and Hessians at all points in train-
ing, no matter the loss or dataset, obey certain
geometric constraints. Some of these constraints
have been noticed previoUsly as a form of im-
plicit regUlarization, while others have been
leveraged algorithmically in applications from
network prUning to interpretability. Remark-
ably, all these geometric constraints can be Un-
derstood as conseqUences of nUmeroUs differ-
entiable symmetries in the loss introdUced by
neUral network architectUres. A set of parame-
ters observes a differentiable symmetry in the
loss if the loss doesn’t change Under a certain dif-
ferentiable transformation of these parameters.
This invariance introdUces associated geometric
constraints on the gradient and Hessian.
Consider a fUnction f(θ) where θ ∈ Rm. This
fUnction possesses a differentiable symmetry if
it is invariant Under the differentiable action ψ
(a) Translation (b) Scale (c) Rescale
FigUre 2: Visualizing symmetry. We visUal-
ize the vector fields associated with simple net-
work components that have translation, scale, and
rescale symmetry. In (a) we consider the vector
field associated with a neUron σ [w1 w2]| x
where σ is the softmax fUnction. In (b) we con-
sider the vector field associated with a neUron
BN [w1 w2] [x1 x2]| where BN is the batch
normalization fUnction. In (c) we consider the vec-
tor field associated with a linear path w2w1x.
of a groUp G on the parameter vector θ, i.e., if θ 7→ ψ(θ, α) where α ∈ G, then F (θ, α) =
f (ψ(θ, α)) = f (θ) for all (θ, α). The existence of a symmetry enforces a geometric strUctUre on the
gradient, VF. Evaluating the gradient at the identity element I of G (So that for all θ, ψ(θ, I) = θ)
yields the resUlt,
hVf, ∂αψ∣α=Ii = 0,
(1)
where h , i denotes the inner product. This equality implies that the gradient Vf is perpendicular
to the vector field ∂ɑψ∣α=ι that generates the symmetry, for all θ. The symmetry also enforces a
geometric structure on the Hessian, HF. Evaluating the Hessian at the identity element yields the
3
Published as a conference paper at ICLR 2021
result,
Hf [∂θ ψ∣α=I ]∂αψ∣α=I + [∂θ ∂αψ∣α=I ]▽/ = 0,	(2)
which constrains the Hessian Hf. See appendix A for the derivation of these properties and other
geometric consequences of symmetry.
We will now consider the specific setting of a neural network parameterized by θ ∈ Rm, the training
loss L(θ), and three families of symmetries (translation, scale, and rescale) that commonly appear in
modern network architectures.
Translation symmetry. Translation symmetry is defined by the group R and action θ 7→ θ + α1A
where 1A is the indicator vector for some subset A of the parameters {θ1, . . . , θm}. The loss function
L thus possesses translation symmetry if L(θ) = L(θ + α1A) for all α ∈ R. Such a symmetry in
turn implies the loss gradient ∂θL = g is orthogonal to the indicator vector ∂αψ∣α=ι = 1a,
hg, 1Ai = 0,	(3)
and that the Hessian matrix H = ∂θ2L has the indicator vector in its kernel,
H1A = 0.	(4)
Softmax function. Any network using the softmax function gives rise to translation symmetry for
the parameters immediately preceding the function. Let z = Wx + b be the input to the softmax
function such that σ(z)i = Pezezj. Notice that shifting any column of the weight matrix Wi or the
bias vector b by a real constant has no effect on the output of the softmax as the shift factors from
both the numerator and denominator canceling its effect. Thus, the loss function is invariant w.r.t.
this translation, yielding the gradient constraints h篝,1)= h∂, 1〉= 0, visualized in Fig. 2 for the
toy model where wi ∈ R2 .
Scale symmetry. Scale symmetry is defined by the group GL1+ (R) and action θ 7→ αA θ where
αA	=	α1A	+	1Ac.	The loss function possesses scale symmetry if	L(θ)	=	L(αA	θ) for all
α ∈ GL1+(R). This symmetry immediately implies the loss gradient is everywhere perpendicular to
the parameter vector itself ∂ɑψ∣α=ι = θ Θ 1/ = θ/,
hg, θAi = 0,	(5)
and relates to the Hessian matrix, where [∂a∂θψ∣ɑ=ι]∂θL = diag(1/)g = g/, as
HθA + gA = 0.	(6)
Batch normalization. Batch normalization leads to scale invariance during training. Let z = w|x + b
be the input to a neuron with batch normalization such that BN(z) = √E[z])where E[z] is the
sample mean and Var(z) is the sample variance given a batch of data. Notice that scaling w and b
by a non-zero real constant has no effect on the output of the batch normalization as it factors from
z, E[z], and Var(z) canceling its effect. Thus, these parameters observe scale symmetry in the loss
and their gradients satisfy〈券, W + (¾b ,b) = 0, as has been previously noted by Ioffe & Szegedy
(2015); Van Laarhoven (2017), and visualized in Fig. 2 for the toy model where w, b ∈ R.
Rescale symmetry. Rescale symmetry is defined by the group GL+ (R) and action θ → ɑ∕ι Θ
a/1 Θ θ where Ai and A2 are two disjoint sets of parameters. The loss function possesses rescale
symmetry if L(θ) = L(α∕ι Θ a/： Θ θ) for all α ∈ GL+ (R). This symmetry immediately implies
the loss gradient is everywhere perpendicular to the sign inverted parameter vector ∂aψ∣α=ι =
θ/1 - θ/2 = θ Θ (1/1 - 1/2),
hg,θ/i- θ∕2 i =0	⑺
and relates to the Hessian matrix, where [∂a∂θψ∣ɑ=ι]∂θL = diag(1∕ι 一 1/2 )g = g/i 一 g/2, as
H(θAι - θ4) + g/i - g/2 = 0.	⑻
Homogeneous activation. For networks with continuous, homogeneous activation functions
φ(z) = φ0(z)z (e.g. ReLU, Leaky ReLU, linear), this symmetry emerges at every hidden neu-
ron by considering all incoming and outgoing parameters to the neuron. For example, consider a
hidden neuron with ReLU activation φ(z) = max{0, z}, such that w2φ(w∣x + b) is the compu-
tational path through this neuron. Scaling wi and b by a real constant and w2 by its inverse has
4
Published as a conference paper at ICLR 2021
no effect on the computational path as the constants can be passed through the ReLU activation
canceling their effects. Thus, these parameters observe rescale symmetry and their gradients satisfy
(∂∂L, wi)+〈 ∂∂b，b〉— (∂∂L，W2)= O, as has been previously noted by Du et al.(2018a); Liang
et al. (2019); Tanaka et al. (2020), and visualized in Fig. 2 for the toy model where w1, w2 ∈ R and
b = 0.
4 S ymmetry leads to Conservation Laws Under Gradient Flow
We now explore how geometric constraints on gradients and Hessians, arising as a consequence
of symmetry, impact the learning dynamics given by stochastic gradient descent (SGD). We will
consider a model parameterized by θ, a training dataset {x1, . . . , xN} of size N, and a training loss
L(θ) = N PN=I '(θ, Xi) With corresponding gradient g(θ) = ∂∂θ.
The gradient descent update with learning rate
η is θ(n+1) = θ(n) - ηg(θ(n)), Which is a for-
Ward Euler discretization With step size η of the
ordinary differential equation (ODE)
dθ
dt
-g(θ).
(9)
(a) Translation
(b) Scale
(c) Rescale
In the limit as η → 0, gradient descent exactly
matches the dynamics of this ODE, Which is
commonly referred to as gradient floW Kush-
ner & Yin (2003). Equipped With a continuous
model for the learning dynamics, We noW ask
hoW do the dynamics interact With the geometric
properties introduced by symmetries?
Symmetry leads to conservation. Strikingly
similar to Noether’s theorem, Which describes
a fundamental relationship betWeen symmetry
and conservation for physical systems governed
by Lagrangian dynamics, here We shoW that
symmetries of a netWork architecture have corre-
sponding conserved quantities through training
under gradient floW.
Figure 3: Visualizing conservation. Associated
With each symmetry is a conserved quantity con-
straining the gradient floW dynamics to a sur-
face. For translation symmetry (a) the floW is
constrained to a hyperplane Where the intercept is
conserved. For scale symmetry (b) the floW is con-
strained to a sphere Where the radius is conserved.
For rescale symmetry (c) the floW is constrained
to a hyperbola Where the axes are conserved. The
color represents the value of the conserved quan-
tity, Where blue is positive and red is negative, and
the black lines are level sets.
Theorem 1. Symmetry and conservation laws in neural networks. Every differentiable symmetry
ψ(α,θ) of the loss that satisfies hθ, [∂ɑ∂θ ψ∣a=ι ]g(θ)i = 0 has the corresponding conservation law,
dt hθ,∂αψ∣α=Ii =0,
through learning under gradient flow.
(10)
To prove Theorem 1, consider projecting the gradient floW learning dynamics in equation (9) onto
the generator vector field ∂ɑψ∣α=ι associated with a symmetry. As shown in section 3, the gradient
of the loss g(θ) is always perpendicular to the vector field ∂aψ∣α=ι. Thus, the projection yields
a differential equation, which can be simplified to equation (10). See appendix B for a complete
derivation.
Application of this general theorem to the translation, scale, and rescale symmetries, identified in
section 3, yields the following conservation law of learning,
Translation: hθA(t), 1i = hθA(0), 1i	(11)
Scale： ∣θA(t)∣2 = ∣Θa(0)∣2	(12)
Rescale：	∣Θai(t)|2	-∣θA2(t)|2 =	∣Θai(0)∣2	—	Wa2(O)I2	(13)
Each of these equations define a conserved constant of learning through training. For parameters with
translation symmetry, their sum (hθA(t), 1i) is conserved, effectively constraining their dynamics
to a hyperplane. For parameters with scale symmetry, their euclidean norm (∣θ∕(t)∣2) is conserved,
5
Published as a conference paper at ICLR 2021
effectively constraining their dynamics to a sphere Ioffe & Szegedy (2015); Van Laarhoven (2017). For
parameters with rescale symmetry, their difference in squared euclidean norm (∣θ∕ι (t) |2 — ∣Θa2 (t) |2)
is conserved, effectively constraining their dynamics to a hyperbola Du et al. (2018a). In Fig. 3 we
visualize the level sets of these conserved quantities for the toy models discussed in Fig. 2.
5 A Realistic Continuous Model for Stochastic Gradient Descent
In section 4 we combined the geometric constraints introduced by symmetries with gradient flow to
derive conservation laws for simple combinations of parameters during training. However, empirically
we know these laws are broken, as demonstrated in Fig. 1. What causes this discrepancy? Gradient
flow is too simple of a continuous model for realistic SGD training. It fails to incorporate the effect
of weight decay introduced by explicit regularization, momentum introduced by commonly used
hyperparameters, stochasticity introduced by random batches, and discretization introduced by a
finite learning rate. Here, we construct a more realistic continuous model for stochastic gradient
descent.
Modeling weight decay. Explicit regularization through the addition of an L2 penalty on the
parameters, with regularization constant λ, is very common practice when training modern deep
learning models. This is generally implemented not by modifying the training loss, but rather directly
modifying the optimizer’s update equation. For stochastic gradient descent, the result leads to the
updated continuous model
dθ
瓦=-g(θ)- λθ∙
(14)
Modeling momentum. Momentum is a common extension to SGD that uses an exponentially
moving average of gradients to update parameters rather than a single gradient evaluation Rumelhart
et al. (1986). The method introduces two additional hyperparameters, a damping coefficient α and a
momentum coefficient β, and applies the two step update equation, θ(n+1) = θ(n) - ηv(n+1) where
v(n+1) = βv(n) + (1 - α)g(θ(n)). When α = β = 0, we regain classic gradient descent. In general,
α effectively reduces the learning rate and β controls how past gradients are used in future updates
resulting in a form of “inertia” accelerating and smoothing the descent trajectory. Rearranging the
two-step update equation, we find that gradient descent with momentum is a first-order discretization
with step size η(1 - α) of the ODE1,
dθ
(1-β) d = -g(θ)∙
(15)
Modeling Stochasticity. Stochastic gradients gB(θ) arise when We consider a batch B of size S
drawn uniformly from the indices {1,∙∙∙, N} forming the unbiased gradient estimate ga(θ)=
SS Pi∈B V'(θ, Xi). When the batch size is much smaller than the size of the dataset, S《N, then
we can model the batch gradient as an average of S i.i.d. samples from a noisy version of the true
gradient g(θ). Using the central limit theorem, we assume ^b(θ) 一 g(θ) is a Gaussian random
variable with mean μ = 0 and covariance matrix Σ(θ). However, because both the batch gradient
and true gradient observe the same geometric properties introduced by symmetry, the noise has a
special low-rank structure. As we showed in section 3, the gradient of the loss, regardless of the
batch, is orthogonal to the generator vector field ∂ɑψ∣α=ι associated with a symmetry. This implies
the stochastic noise must also observe the same property. In order for this relationship to hold for
arbitrary noise, then ∑(θ)∂αψ∣α=ι = 0. In other words, the differential symmetry inherent in neural
network architectures projects the noise introduced by stochastic gradients onto low rank subspaces,
leaving the gradient flow dynamics in these directions unchanged2.
Modeling discretization. The effect of discretization when modeling continuous dynamics is a well
studied problem in the numerical analysis of partial differential equations. One tool commonly used
in this setting, is modified equation analysis Warming & Hyett (1974), which determines how to
better model discrete steps with a continuous differential equation by introducing higher order spatial
or temporal derivatives. We present two methods based on modified equation analysis, which modify
gradient flow to account for the effect of discretization.
1A more detailed derivation for this ODE can be found in appendix C.
2Stochasticity and discretization can lead to non-trivial interactions requiring analysis using stochastic
differential equation as explained in appendix F.
6
Published as a conference paper at ICLR 2021
Modified loss. Gradient descent always moves
in the direction of steepest descent on a loss
function L at each step, however, due to the
finite nature of the learning rate, it fails to re-
main on the continuous steepest descent path
given by gradient flow. Li et al. (2017); Feng
et al. (2019) and most recently Barrett & Dherin
(2020), demonstrate that the gradient descent
trajectory closely follows the steepest descent
path of a modified loss function L. The diver-
gence between these trajectories fundamentally
depends on the learning rate η and the curvature
H. As derived in Barrett & Dherin (2020), and
summarized in appendix D, this divergence is
given by the gradient correction - 2 Hg, which
is the gradient of the squared norm - 4 |VL|2.
Thus, the modified loss IS L = L + 4∣VL∣2 and
the modified gradient flow ODE is
(a) Modified Loss
Figure 4: Modeling discretization. We visual-
ize the trajectories of gradient descent and mo-
mentum (black dots), gradient flow with and with-
out momentum (blue lines), and the modified dy-
namics (red lines) on the quadratic loss L(w) =
w| -21.5.5 -12.5 w. On the left we visualize gradi-
ent dynamics using modified loss. On the right
we visualize momentum dynamics using modified
flow. In both settings the modified continuous dy-
namics visually track the discrete dynamics better
than the original continuous dynamics. See ap-
pendix D for further details.
(b) Modified Flow
dθ	η
dt = -g(θ) - 2H(θ)g(θ).	(16)
See Fig. 4 for an illustrative example of this
method applied to a quadratic loss in R2 .
Modified flow. Rather than modifying gradient
flow with higher order “spatial” derivatives of
the loss function, here we introduce higher order temporal derivatives. We start by assuming the
existence of a continuous trajectory θ(t) that weaves through the discrete steps taken by gradient
descent and then identify the differential equation that generates the trajectory. Rearranging the update
equation for gradient descent, θt+1 = θt -ηg(θt), and assuming θ(t) = θt and θ(t+η) = θt+1, gives
the equality -g(θt) = ""+?-"(", which Taylor expanding the right side results in the differential
equation -g(θt)=d + 2兼 + O(η2). Notice that in the limit as η → 0 we regain gradient flow.
For small η, we obtain a modified version of gradient flow with an additional second-order term,
dθ
dt
-g(θ) -
η d2θ
2 dt2.
(17)
This approach to modifying first-order differential equation with higher order temporal derivatives was
applied by Kovachki & Stuart (2019) to construct a more realistic continuous model for momentum,
as illustrated in Fig. 4.
6 Combining S ymmetry and Modified Gradient Flow to Derive
Exact Learning Dynamics
As shown in section 4, each symmetry results in a conserved quantity under gradient flow. We now
study how weight decay, momentum, stochastic gradients, and finite learning rates all interact to
break these conservation laws. Remarkably, even when using a more realistic continuous model for
stochastic gradient descent, as discussed in section 5, we can derive exact learning dynamics for the
previously conserved quantities. To do this we (i) consider a realistic continuous model for SGD,
(ii) project these learning dynamics onto the generator vector fields ∂aψ∣α=ι associated with each
symmetry, (iii) harness the geometric constraints from section 3 to derive simplified ODEs, and (iv)
solve these ODEs to obtain exact dynamics for the previously conserved quantities. For simplicity,
we first consider the continuous model of SGD incorporating weight decay and modified loss, but not
momentum and stochasticity3. In this setting, the exact dynamics, as fully derived in appendix E, for
the parameter combinations tied to the symmetries are,
3See appendix F for a discussion on how to handle stochasticity in this setting using It6 calculus.
7
Published as a conference paper at ICLR 2021
Translation:
hθ(t), 1Ai = e-λthθ(0), 1Ai
(18)
Scale:
Rescale:
∣Θa (t)l2
e-2λt∣θA(0)∣2 + η Z
0
e-2λ(t-τ)
|gA|2 dτ
∣θAι (t)∣2-∣θA2(t)∣2
(19)
(20)
e-2λt(∣θA1 (0)∣2 -∣θA2(0)|2) + ηZ0 e-2Mt-τ) (∣gθA1∣2 - ∣gθA2∣2) dτ
Notice how these equations are equivalent to the conservation laws derived in section 4 when
η = λ = 0. Remarkably, even in typical hyperparameter settings (weight decay, stochastic batches,
finite learning rates), these solutions match nearly perfectly with empirical results4 from modern
neural networks (VGG-16) trained on real-world datasets (Tiny ImageNet), as shown in Fig. 5. We
will now discuss each equation individually.
Translation dynamics. For parameters with
translation symmetry, equation 18 implies that
the sum of these parameters ((θ∕(t), 1))decays
exponentially to zero at a rate proportional to
the weight decay. Equation 18 does not directly
depend on the learning rate η nor any informa-
tion of the dataset or task. This is due to the
lack of curvature in the gradient field for these
parameters (as shown in Fig. 2). This implies
that at initialization we can deterministically pre-
dict the trajectory for the parameter sum as sim-
ple exponential functions with a rate defined
by the weight decay. The first row in Fig. 5
demonstrates this qualitatively, as all trajectories
are smooth exponential functions that converge
faster for increasing levels of weight decay.
Scale dynamics. For parameters with scale
symmetry, equation 19 implies that the norm for
these parameters (∣Θa∣2) is the sum of an expo-
nentially decaying memory of the norm at initial-
ization and an exponentially weighted integral
of gradient norms accumulated through training.
Compared to the translation dynamics, the scale
dynamics do depend on the data through the gra-
dient norms accumulated throughout training.
Without weight decay λ = 0, the first term stays
constant and the second term grows monotoni-
cally. With weight decay λ > 0, the first term
decays monotonically to zero, while the second
term can decay or grow, but always stays pos-
itive. The second row in Fig. 5 demonstrates
these qualitative relationships. Without weight
decay the norms increase monotonically as pre-
dicted and with weight decay the dynamics are
non-monotonic and present more complex be-
havior. To better understand the forces driving
λ = 0 λ = 10-4	λ = 10-3
2.04
012340123401234
×10j	×103	×103
Time (η × steps)
Figure 5: Exact dynamics of VGG-16 on Tiny
ImageNet. We plot the column sum of the final
linear layer (top row) and the difference between
squared channel norms of the fifth and fourth con-
volutional layer (bottom row) of a VGG-16 model
without batch normalization. We plot the squared
channel norm of the second convolution layer (mid-
dle row) of a VGG-16 model with batch normal-
ization. Both models are trained on Tiny ImageNet
with SGD with learning rate η = 0.1, weight decay
λ, batch size S = 256, for 100 epochs . Colored
lines are empirical and black dashed lines are the
theoretical predictions from equations (18), (19),
and (20). See appendix J for more details on the
experiments.
these complex dynamics, we can examine the time derivative of equation 19,
-d ∣θA(t)l2 = -2λ∣θA(t)∣2 + η ∣gA∣2.	(21)
dt
From this equation we see that there is a competition between a centripetal effect due to weight
decay (-2λ∣θ∕(t)∣2) and a centrifugal effect due to discretization (η |g/|2). The centripetal effect
4See appendix J for more details on the experiments and how we compute the integrals terms in the exact
solutions using stochastic gradients.
8
Published as a conference paper at ICLR 2021
due to weight decay is a direct consequence of its regularizing influence, pulling the parameters
towards the origin. The centrifugal effect due to discretization originates from the spherical geometry
of the gradient field in parameter space - because scale symmetry implies the gradient is always
orthogonal to the parameter itself, each discrete update with a finite learning rate effectively pushes
the parameters away from the origin. At the stationary state of the dynamics, these forces will balance
leading the dynamics of these parameters to be constrained to the surface of a high-dimensional
sphere. In particular, at stationarity, then +∣θ(t)∣2 = 0, which rearranging equation (21) gives the
condition ω(t) ≡ ∣ 耨 /∖θ∖ = ^n. Consistent with the results of Wan et al. (2020), this implies that
at stationarity the angular speed ω(t) of the weights is constant and governed only by the learning rate
η and weight decay constant λ. When considering stochasticity explicitly, as explained in appendix F,
then these dynamics also depend on the covariance of the gradient noise Σ(θ) and batch size S.
Rescale dynamics. For parameters with rescale symmetry, equation (20) is the sum of an exponen-
tially decaying memory of the difference in norms at initialization and an exponentially weighted
integral of difference in gradient norms accumulated through training. Similar to the scale dynamics,
the rescale dynamics do depend on the data through the gradient norms, however unlike the scale
dynamics we have no guarantee that the integral term is always positive. This leads to quite sophisti-
cated, complex dynamics, consistent with the third row in Fig. 5. Despite the complexity, our theory,
nevertheless, quantitatively matches the empirics. The only apparent pattern from the empirics is
that for large enough weight decay, the regularization dominates any complexity introduced by the
gradient norms and the difference in parameter norms decays exponentially to zero.
Harmonic oscillation with momentum. We
will now consider a continuous model of SGD
with momentum. As discussed in appendix G,
We consider the continuous model incorporat-
ing weight decay, momentum, stochasticity, and
modified flow. Under this model, the solutions
We obtain take the form of driven harmonic os-
cillators where the driving force is given by the
gradient norms, the friction is defined by the
momentum constant, the spring coefficient is
defined by the regularization rate, and the mass
is defined by the the learning rate and momen-
tum constant. For most standard hyperparameter
choices, these solutions are in the overdamped
setting and align well with the first-order solu-
tions for SGD without momentum up to a time
rescaling, as shown in the left and middle panel
of Fig. 6. However, for large values of beta
we can push the solution into the underdamped
regime where we would expect harmonic oscil-
0.30
E 0∙15
0.00
-0.15
-0.30
β = 0
β = 0.9
β = 0.99
0.0 0.4 0.8 1.2 1.6 0.0 0.4 0.8 1.2 1.6 0.0 0.4 0.8 1.2 1.6
Time (η × steps)
Figure 6: Momentum leads to harmonic oscil-
lation. We plot the column sum of the final
linear layer of a VGG-16 model (without batch
normalization) trained on Tiny ImageNet with
SGD with learning rate η = 0.1, weight decay
λ = 5 X 10-3, batch size S = 256 and momentum
β ∈ {0, 0.9, 0.99}. Colored lines are empirical
and black dashed lines are the theoretical predic-
tions from equations (34).
lation and indeed, we can empirically verify our predictions, even at scale for VGG-16 trained on
Tiny ImageNet, as in right panel of Fig. 6.
7 Conclusion
Despite being the central guiding principle in the exploration of the physical world Anderson (1972);
Gross (1996), symmetry has been underutilized in understanding the mechanics of neural networks.
In this paper, we constructed a unifying theoretical framework harnessing the geometric properties of
symmetry and more realistic continuous equations for learning dynamics that model weight decay,
momentum, stochasticity, and discretization. We use this framework to derive exact dynamics for
meaningful combinations of parameters, which we experimentally verified on large scale neural
networks and datasets. For example, in the case of a VGG-16 model with batch normalization trained
on Tiny-ImageNet (one of the model/dataset combinations we considered in section 6) there are
12, 751 distinct parameter combinations whose dynamics we can analytically describe. Overall, this
work provides a first step towards understanding the mechanics of learning in neural networks without
unrealistic simplifying assumptions.
9
Published as a conference paper at ICLR 2021
Acknowledgements
We thank Daniel Bear, Lauren Gillespie, Kyogo Kawaguchi, Brett Larsen, Eshed Margalit, Alain
Studer, Sho Sugiura, Umberto Maria Tomasini, and Atsushi Yamamura for helpful discussions. This
work was funded in part by the IBM-Watson AI Lab. D.K. thanks the Stanford Data Science Scholars
program for support. J.S. thanks the Mexican National Council of Science and Technology (CONA-
CYT) for support. S.G. thanks the James S. McDonnell and Simons Foundations, NTT Research, and
an NSF CAREER Award for support. D.L.K.Y thanks the McDonnell Foundation (Understanding
Human Cognition Award Grant No. 220020469), the Simons Foundation (Collaboration on the
Global Brain Grant No. 543061), the Sloan Foundation (Fellowship FG-2018-10963), the National
Science Foundation (RI 1703161 and CAREER Award 1844724), and the DARPA Machine Common
Sense program for support and the NVIDIA Corporation for hardware donations.
References
Jing An, Jianfeng Lu, and Lexing Ying. Stochastic modified equations for the asynchronous stochastic
gradient descent. Information and Inference: A Journal of the IMA, 2018.
Philip W Anderson. More is different. Science,177(4047):393-396,1972.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. arXiv preprint arXiv:1812.03981, 2018c.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, pp. 8141-8150, 2019.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick KlaUschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7), 2015.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, 1989.
David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint
arXiv:2009.11162, 2020.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de la Fuente,
Vishal Subbiah, and Michael James. Online normalization for training neural networks. In
Advances in Neural Information Processing Systems, pp. 8433-8443, 2019.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in
Neural Information Processing Systems, pp. 5225-5235, 2017.
Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
arXiv preprint arXiv:1901.08572, 2019.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Systems,
pp. 384-395, 2018a.
10
Published as a conference paper at ICLR 2021
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Yuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu. Uniform-in-time weak error
analysis for stochastic gradient descent algorithms via diffusion approximation. arXiv preprint
arXiv:1902.00635, 2019.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. Adv. Neural Inf. Process. Syst., 33,
2020.
Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova. Dynamics
of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In
Advances in Neural Information Processing Systems,pp. 6981-6991, 2019.
David J Gross. The role of symmetry in fundamental physics. Proceedings of the National Academy
of Sciences, 93(25):14256-14259, 1996.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In Advances in Neural Information Processing Systems,
pp. 2160-2170, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
StanislaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pp. 586-594, 2016.
Nikola B Kovachki and Andrew M Stuart. Analysis of momentum methods. arXiv preprint
arXiv:1906.04285, 2019.
Daniel Kunin, Jonathan M Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of
regularized linear autoencoders. arXiv preprint arXiv:1901.08168, 2019.
Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applica-
tions, volume 35. Springer Science & Business Media, 2003.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In International Conference on Learning Representations (ICLR),
2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8572-8583,
2019.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110, 2017.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv
preprint arXiv:1910.07454, 2019.
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020.
11
Published as a conference paper at ICLR 2021
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic
differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry,
and complexity of neural networks. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 888-896, 2019.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Continuous-time limit of stochastic gradient
descent revisited. NIPS-2015, 2015.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization
in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422-2430,
2015.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. nature, 323(6088):533-536, 1986.
David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural
networks. Advances in neural information processing systems, 8:302-308, 1995.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing systems, pp.
901-909, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., May 2019.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in neural information processing
systems, pp. 2510-2518, 2014.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467, 2020.
Twan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint
arXiv:1706.05350, 2017.
Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics of deep neural
networks with batch normalization and weight decay. arXiv preprint arXiv:2006.08419, 2020.
RF Warming and BJ Hyett. The modified equation approach to the stability and accuracy analysis of
finite-difference methods. Journal of computational physics, 14(2):159-179, 1974.
Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. arXiv preprint arXiv:1810.12281, 2018.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. arXiv
preprint arXiv:1803.00195, 2018.
12
Published as a conference paper at ICLR 2021
A	S ymmetry and Geometry
Here we derive in detail the geometric properties of the loss landscape introduced by symmetry, as
discussed in section 3. Consider a function f(θ) where θ ∈ Rm. This function possesses a symmetry
if it is invariant under the action ψ of a group G on the parameter vector θ, i.e., if θ 7→ ψ(θ, α)
where α ∈ G, then F (θ, α) = f (ψ(θ, α)) = f(θ) for all (θ, α). Symmetry enforces a geometric
relationship between the gradient VF and Hessian HF of the composition F(θ, α) with the gradient
Vf and Hessian Hf of the original function f (θ). This relationship can be described by five
constraints on Vf and Hf. Considering these general formulae when f(θ) = L(θ), the training loss
of a neural network, yields fifteen distinct equations describing the geometrical relationships between
architectural symmetries and the loss landscapes, some of which have been identified individually in
existing literature (Table 1).
	Translation	Scale	Rescale
V p VF	^^∂θF	-	1,2,5	6
	daF	—	3, 4	7,8,9
	-∂2F	-—	3, 5	—
HF	∂θ∂αF	—	—	—
	选F	一	—	9
Table 1: Unifying existing literature through symmetry. Here we provide references to existing
literature describing geometric properties of either the gradient or Hessian introduced by a network’s
architecture. All of these properties can be unified as consequences of either a translation, scale, or
rescale symmetry in the training loss.
1.	Ioffe & Szegedy (2015) has motivated the effectiveness of Batch Normalization by its scale
invariant property. In particular, they have noted that Batch Normalization will stabilize
back-propagation because “The scale does not affect the layer Jacobian nor, consequently,
the gradient propagation. Moreover, larger weights lead to smaller gradients, and Batch
Normalization will stabilize the parameter growth.”
2.	Van Laarhoven (2017) has then shown that the role of L2 regularization when combined
with batch Ioffe & Szegedy (2015), weight Salimans & Kingma (2016) or layer Ba et al.
(2016) normalization is not to regularize the function, but to effectively control the learning
rate.
3.	Zhang et al. (2018) has thoroughly studied mechanisms of weight decay regularization and
derived various geometric properties of loss landscapes along the way. Arora et al. (2018c)
has theoretically analyzed the automatic tuning property of learning rate in networks with
Batch Normalization.
4.	Li et al. (2020) studied the interaction of weight decay and batch normalization in the setting
of stochastic gradients.
5.	Neyshabur et al. (2015) has identified that SGD is not rescale equivariant even when network
outputs are rescale invariant. This is a problem because gradient descent performs very
poorly on unbalanced networks due to the lack of equivariance. Motivated by the issue, they
have introduced a new optimizer, Path-SGD, that is rescale equivariant.
6.	Arora et al. (2018b) proved that the weights of linear artificial neural networks satisfy strong
balancedness property.
7.	Du et al. (2018a) studied implicit regularization in networks with homogeneous activation
functions. To do that, they showed conservation law of parameters with rescale invariance.
8.	Liang et al. (2019) have proposed a new capacity measure to study generalization that
respects rescale invariance of networks. Along the way, they showed geometric properties
of gradients and Hessians for networks with rescale invariance. However, their results were
restricted to a layer without biases.
9.	Tanaka et al. (2020) have proved gradient properties of parameters with rescale invariance at
neuron level including biases.
13
Published as a conference paper at ICLR 2021
A.1 Gradient Geometry
If a function f posses a symmetry, then there exists a geometric constraint on the relationship between
the gradients VF and Vf at all (θ, α),
VF = (dθθF) = (dΨFdψ) = (Vf).
∂αF	∂ψ F∂αψ	0
The top element of the gradient relationship, ∂θF, evaluated at any (θ, α), yields the property
∂θψ Vf ∣ψ(θ,α) = Vf∣θ,	(22)
which describes how the symmetry transformation affects the function’s gradients despite leaving the
output unchanged. The bottom element of the gradient relationship, ∂αF, evaluated at the identity
element of G yields the property
hVf,∂αψ∣α=I i =0,	(23)
which implies the gradient Vf is perpendicular to the vector field ∂aΨ∣α=ι that generates the
symmetry, for all θ. In the specific setting when f(θ) = L(θ), the training loss of a neural network,
these gradient properties are summarized in Table 2 for the translation, scale, and rescale symmetries
described in section 3.
	Translation	Scale	Rescale
g(θ) =	g (ψ(θ, α))	diag(αA)g (ψ(θ, α))	diag(αA1	α-A21)g (ψ(θ, α))
g(θ) ⊥	1A	θA	θA1 - θA2
Table 2: Geometric properties of the gradient. The gradients of a neural network with either
translation, scale or rescale symmetry observe certain geometric properties no matter the dataset or
step in training.
Notice that the first row of Table 2 implies that symmetry transformations affect learning dynamics
governed by gradient descent for scale and rescale symmetries, while it does not for translation
symmetry. These observations are in agreement with Van Laarhoven (2017) who has shown that
effective learning rate is inversely proportional to the norm of parameters immediately preceding the
batch normalization layers and Neyshabur et al. (2015) who have noticed that SGD is not invariant to
the rescale symmetry that the network output respects and proposed Path-SGD to fix the discrepancy.
A.2 Hessian Geometry
If a function f posses a symmetry, then there also exists a geometric constraint on the relationship
between the Hessian matrices HF and Hf at all (θ, α),
HF
∂θ2F ∂θ∂αF
∂α∂θF ∂α2F
( ∂ψ F∂θ2 ψ + ∂ψ2F(∂θψ)2
∂ψ2F∂θψ∂αψ+∂ψF∂θ∂αψ
∂ψ2 F∂θψ∂αψ + ∂ψ F∂θ∂αψ	Hf 0
(∂αψ)l∂ψ F∂ɑψ + (∂ψ F)l∂αψ) = V 0	0) .
The first diagonal element, ∂θ2F, evaluated at any (θ, α), yields the property
∂2Ψ Vf ∣ψ(θ,a) + (∂θψ)2 Hf ∣ψ(θ,α) = Hf ∣θ ,	(24)
which describes how the symmetry transformation affects the function’s Hessian despite leaving the
output unchanged. The off-diagonal elements, ∂θ∂αF = ∂α∂θF, evaluated at the identity element of
G yields the property
Hf [∂θ ψ∣α=I ]∂αψ∣α=I + [∂θ ∂α∣α=I WVf = 0,	(25)
which implies the geometry of gradient and Hessian are connected through the action of the symmetry.
Lastly, the second diagonal element, ∂α2F, represents an equality, evaluated at the identity element of
G yields the property
(∂αΨ∣α=I)|Hf (∂ɑψ∣α=I) +〈Vf, ∂?ψ∣α=Ii = 0,	(26)
14
Published as a conference paper at ICLR 2021
which combines the geometric relationships in equation 23 and equation 25. In the specific setting
when f (θ) = L(θ), the training loss of a neural network, these Hessian properties are summarized in
Table 2 for the translation, scale, and rescale symmetries described in section 3.
	Translation	Scale	Rescale
H(θ) =	H(ψ(θ, α))	diag(α2A)H(ψ(θ, α))	diag(αA1 Θ。工：)H(ψ(θ,α))
0=	H1A	HθA + gA	H (θA1 - θA2 ) + gA1 - gA2
0=	1|AH1A	θA HΘa	(θAι - θA2 )lH(θAι - θA2) + gAi θA1 + gA2 θA2
Table 3: Geometric properties of the Hessian. The Hessian matrix of a neural network with either
translation, scale or rescale symmetry observe certain geometric properties no matter the dataset or
step in training.
The translation, scale, and rescale symmetries identified in section 3 is not an exhaustive list of the
symmetries present in neural network architectures. For example, more general rescale symmetries
can be defined by the group GL1+(R) and action θ 7→ αAk1 α-Ak2 θ, which occur in networks with
quadratic activation functions. A stronger form of rescale symmetry also occurs for linear networks
under the action of the group GLk+(R) of k × k invertible matrices, as noticed previously by Arora
et al. (2018a); Du et al. (2018a). Interestingly, some of the gradient and Hessian properties for scale
symmetry can also be easily proven as consequences of Euler’s Homogeneous Function Theorem
when k = 0.
B Conservation Laws
Here we repeat Theorem 1 and provide a detailed derivation.
Theorem. Symmetry and conservation laws in neural networks. Every differentiable symmetry
ψ(α, θ) of the loss that satisfies (θ, [∂α∂θψ∣a=ι ]g(θ)i = 0 has the corresponding conservation law
ddt hθ, ∂αψ∣α=ι i = 0 through learning under gradient flow.
Proof. Project the gradient flow learning dynamics,d=-g(θ), onto the vector field that generates
the symmetry ∂ɑψ∣α=ι, evaluated at the identity element,
dθ
hdpdɑψ∣α=Ii = h-g(θ), dαψ[α=Ii = 0.
We can factor the left side of this equation as
h-Γ ,dαψ∣α=I i =* hθ, dαψ∣α=I i - hθ, ~T,dαψ∖α=I i
dt	dt	dt
=^77hθ, dαψ∖α=I i - hθ,dadθ ψ∖α=I 万^i
dt	dt
=d hθ, dαψ∖α=Ii + hθ,dɑdθψ∖α=Ig(θ)i
By assumption, hθ, [∂ɑ∂θψ∖α=ι]g(θ)i = 0, implying d@ ∂αψ∖α=ιi = 0.
□
The condition hθ, [∂α∂θψ ∖α=I]g(θ)i = 0 holds for the translation, scale, and rescale symmetries we
consider in section 3. For translation symmetry, ∂α∂θψ∖α=I = 0. For scale symmetry, ∂α∂θψ∖α=I =
I and	hθA, g(θA)i	= 0. For rescale symmetry, ∂α∂θψ∖α=I	=	I0	-0I	and	hθA1,g(θA1)i	-
hθA2, g(θA2)i = 0.
15
Published as a conference paper at ICLR 2021
C Limiting Differential Equations for Learning Rules
Here we identify ordinary differential equations whose first-order discretization give rise to the
gradient descent and classical momentum algorithms. These differential equations can be understood
as the limiting dynamics for their respective discrete algorithms as the learning rate η → 0.
C.1 Gradient Descent
Gradient descent with learning rate η is given by the update equation
θk+1 = θk - ηg(θk),
and initial condition θ0 . Rearranging the difference between consecutive updates gives
θk+1 - θk
η
-g (θk).
This is a discretization with step size η of the first order ODE
ddtθ
-g(θ),
where We used the forward Euler discretization dθk = θk+1t-θk. This ODE is commonly referred to
as gradient flow.
C.2 Classical Momentum
Classical momentum with learning rate η, damping coefficient α, and momentum parameter β, is
given by the update equation5
vk+1 = βvk + (1 - α)g(θk),
θk+1 = θk - ηvk+1 ,
and initial conditions v0 = 0 and some θ0 . The difference between consecutive updates is
θk+1 - θk = -ηvk+1
= -ηβvk - η(1 - α)g(θk)
= β (θk - θk-1) - η(1 - α)g(θk).
Rearranging this equation we get
θk + 1 - θk βθk - θk-1
η(1 - α)	η(1 - α)
-g (θk ).
This is a discretization with step size η(1 - α) of the first order ODE
(i-β) dθ = -g(θ),
where we used the forward Euler discretization 捋 θk = θk+-a)k and backward Euler discretization
dt θk = θη--- . We will refer to this equation as momentum flow. A more detailed derivation
for this ODE under Nesterov variants of classical momentum can be found in Su et al. (2014) and
Kovachki & Stuart (2019).
5The default PyTorch implementation does not perform damping on the gradient in the first momentum
buffer v1 .
16
Published as a conference paper at ICLR 2021
D Modified Equation Analysis
Gradient descent always moves in the direction of steepest
descent on a loss function, however, due to the finite nature
of the learning rate, it fails to remain on the continuous
steepest descent path. The divergence between the discrete
and continuous trajectories fundamentally depends on the
learning rate and the curvature of the loss. It is thus natural
to assume there exists a more realistic continuous model
for SGD that incorporates both these terms in a non-trivial
way. How can we better model the discrete dynamics of
gradient descent with a continuous differential equation?
Intuition from finite difference methods. To answer this
question We will take inspiration from tools developed for
finite difference methods. Finite difference methods are
a class of numerical techniques for approximating deriva-
tives in the analysis of partial differential equations (PDE).
These approximations are applied iteratively to construct
numerical solutions to a PDE given some initial conditions.
However, this discretization process introduces numerical
artifacts, which can lead to a significant difference between
the numerical solution and the true solution for the PDE.
Modified equation analysis is a method for understand-
ing this difference by modeling the numerical artifacts as
higher-order spatial or temporal derivatives modifying the
original PDE. This approach can be used to construct mod-
ified continuous dynamics that better approximate discrete
dynamics, as illustrated in Fig. 7.
Figure 7: Circular motion. Consider
the vector field f (x) = [1 -11] X and the
discrete dynamics xt+1 = xt + ηf(xt)
(black dots), the continuous dynamics
X = f (x) (blue line), and the modified
continuous dynamics X = f (x) + 2x.
We visualize the trajectories given by
these dynamics using the initial condi-
tion X0 = [ 1 0 ]| (white circle) and a
step size η = 0.1. As we can see,
the modified continuous trajectory better
matches the discrete trajectory.
D. 1 Modified Loss
Taking inspiration from modified equation analysis, Li et al. (2017); Feng et al. (2019) and most
recently Barrett & Dherin (2020), demonstrate that the trajectory given by gradient descent closely
follows the steepest descent path of a modified loss function L, rather than the original loss L. As
explained in Barrett & Dherin (2020), assume there exists a modified vector field with corrections gi
in powers of the learning rate to the original vector field g that the discrete dynamics follow. In other
words, rather than considering the dynamics given by gradient flow, dθ = -g(θ), We consider the
modified differential equation,
ddtθ = -g(θ)+ηgι(θ)+η2g2(θ) + ∙∙∙∙
Truncating the modified vector field up to the order η and using backward error analysis we can
derive that the first-order correction gι = 一 2 Hg, which is the gradient of the squared norm
一 4 |VL|2. Thus, the truncated modified differential equation is simply gradient flow on a modified
loss L = L + 4|VL|2.
Convex quadratic loss. To illustrate modified loss, we will consider the trajectories of gradient
descent wt, gradient flow W(t), and modified gradient flow W(t) on the convex quadratic loss
L(W) = 1 w|Aw, where A * 0 is some positive definite matrix, as shown in Fig. 4. For a
finite learning rate η and initial condition w0, gradient descent is given by the update formula
wt+ι = Wt 一 ηAwt. Gradient flow is defined as + W= -AW, a linear first-order differential
equation. For the initial condition w0, the resulting initial value problem can be solved exactly
giving the gradient flow trajectory W(t) = S-1e-ΛtSw0, where A = S-1ΛS is the diagonalization
of the curvacture matrix. For learning rate η, the modified loss is L(W) = 2WTAW + ηwlA2w
and modified gradient flow is defined as 舟W = -AW — 2 A2W. For the initial condition wo, the
resulting initial value problem can be solved exactly giving the modified gradient flow trajectory
W(t)= STe-C+ 2λ )tSwo.
17
Published as a conference paper at ICLR 2021
D.2 Modified Flow
Rather than modify gradient flow with higher order “spatial” derivatives of the loss function, here
we introduce higher order temporal derivatives. We start by assuming the existence of a continuous
trajectory θ(t) that weaves through the discrete steps taken by SGD and then identify the differential
equation that generates the trajectory. Rearranging the update equation for SGD, θt+1 = θt -ηgB(θt),
and assuming θ(t) = θt and θ(t + η) = θt+ι, gives the equality -gs(θt) = "(")-""), which
Taylor expanding the right side results in the differential equation -gB(θt)=富 + 2需 + O(η2).
Notice that in the limit as η → 0 we regain gradient flow. For small η, η 1, we obtain a modified
version of gradient flow with an additional second-order term. This approach of modifying first-order
differential equations with higher order temporal derivatives was applied by Kovachki & Stuart (2019)
to modify momentum flow, capturing the harmonic motion of momentum.
Convex quadratic loss. To illustrate modified flow, we will consider the trajectories of momentum
wt, momentum flow W(t), and modified momentum flow W(t) on the convex quadratic loss L(W)=
1 w|Aw, where A * 0 is some positive definite matrix, as shown in Fig. 4. For a finite learning rate
η, dampening α = 0, momentum constant β, and initial conditions v0 = 0, w0 , then momentum
is given by the pair of recursive update equations vt+1 = βvt + Awt and wt+1 = wt - ηvt+1.
Momentum flow is defined as (1 一 β) £W = -AW, a linear first-order differential equation. For
a given initialization w0, the resulting initial value problem can be solved exactly as in the case of
gradient flow. Modified momentum flow is defined as η (1 + β) -d2W + (1 - β)条W = -AW), a linear
second-order differential equation. For a given initialization W0 and the assumed initial condition
-W(0) = 0, then the resulting initial value problem can be solved exactly as a system of damped
harmonic oscillators.
E Deriving the Exact Learning Dynamics of SGD
We consider a continuous model of SGD without momentum incorporating weight decay (equation 14)
and modified loss (equation 16), such that
L = Lλ + 4 ∣VLλ∣2
where Lλ = L + 2 ∣θ∣2 is the regularized loss. The gradient of the modified loss is,
Ve = VLλ + 2 HλVLλ =(1 +
g+ λ+
θ + 2 (Hg + λHθ),
where we used VLλ = g + λθ and Hλ = H + λI . Thus, the equation of learning we consider is
dθ = -Ve(θ)
dt
g- λ+
θ - 2 (Hg + λHθ).
(27)
To incorporate the effect of stochasticity (equation 31) in this equation of learning we could replace the
full-batch gradient g and Hessian H with their stochastic batch counterparts gB and HB respectively.
However, careful treatment of these terms using stochastic calculus is needed when integrating the
resulting stochastic differential equation, which we discuss at the end of this section.
Translation dynamics. In the case of parameters with translation symmetry, the effect of discretiza-
tion essentially leaves the dynamics for the constant of learning unchanged. Combining the geometric
properties of gradient (hg, 1Ai = 0) and Hessian (H1A = 0) introduced by translation symmetry
with the equation of learning (equation 27) gives the differential equation,
dθ
瓦+vl,
hθ, 1Ai = 0,
(28)
where we used the simplification,
λ+
H1A=0	H1A =0
η z ʌ { z }1	{
hθ, iAi + 2(wcσ+λ JHMAn
18
Published as a conference paper at ICLR 2021
and ignored the O(ηλ2) term as we set the weight decay constant λ to be as small as the learning rate
η in practice. The solution to this differential equation is
hθ(t), 1Ai =e-λthθ(0),1Ai.
Scale dynamics. In the case of parameters with scale symmetry, the effect of discretization does
distort the original geometry. A finite learning rate leads to a centrifugal force that monotonically
increases the previously conserved quantity (∣θ∣2), while weight decay acts as a centripetal force
decreasing the quantity. Combining the geometric constraints on the gradient (hg, θAi = 0) and
Hessian (HθA = -gA) introduced by scale symmetry with the equation of learning (equation 27)
gives the following differential equation,
(d + VL,θA) = (λ + 1 d) lθAl2 - 2 |gA|2 = 0,	(29)
dt	2 dt	2
where we used the simplifications hdθ,θ∕) = 2 第 ∣Θa∣2,
hg,θAi=0	2	hg,HθA}∣=-g2|	-hg}Ai=0
hvL,θ∕i=(ι + η2λ)^A{{ + (λ + ηλ-卜θ,θ∕i + 2( ZHgX{ +λJH^)
=(λ+警 )∣θA∣2- 2∣gA∣2,
and ignored the O(ηλ2) term. The solution to this differential equation is
∣θ∕(t)l2
e-2λt∣θA(0)l2 + η J。e-2"(LT)
|gA|2 dτ.
Rescale dynamics. In the case of parameters with rescale symmetry, the effect of discretization also
distorts the original geometry. However, unlike in the sphere, the force originating from discretization
can both increase or decrease the previously conserved quantity (∣θ∕ι (t)∣2 - 怛/2 (t)∣2). Combining
the geometric properties of gradient (hg, θA1 i - hg, θA2i = 0) and Hessian (HθA1 - HθA2+ gA1 -
gA2 = 0) introduced by rescale symmetry with the equation of learning (equation 27) gives the
following differential equation,
(dt + VL,θAι>-(dt + DL,θA2) = (λ +2d) (lθAι (t)|2 - lθA2 (t)F)-2 (|gAi|2 - |gA2 |2)
dt dt	2 dt	2
(30)
where we used the simplification,
= 0,
hg,θAl i-hg,θA2 i=。
hVθ Le,θAι i - hVLe,θA2 i = (1 + η2 卜JhgjθAτ^-hg7θAy) + (λ + η2- ) (lθAι |2 - lθA2 |2)
/hg,-gAl +gA2 i = 一|gAi |2 + |gA2 |2	-hg,θAι i+hg,"A2 i=。∖
+ 2 I	hg,HθA1 ʌ- HθA2{	+λ④HAJHjJ{ I
=(λ+?)(ιθA1ι jA212)- Ms-…
and ignored the O(ηλ2) term. This is the same differential equation as in equation (29), just with a
different forcing term. Thus, the solution to this differential equation is
∣θAι (t)∣2 -∣θA2 (t)∣2 = e-2λt(∣θAι (0)∣2 -∣θA2 (0)|2) + η f( e-2λ((T)
|gA1 |2 - |gA2 |2 dτ.
19
Published as a conference paper at ICLR 2021
F Modeling Stochasticity
Stochastic gradients gB(θ) arise when We consider a batch B of size S drawn uniformly from the
indices {1,..., N} forming the unbiased gradient estimate ^b(θ) = 1 Pii∈B V'(θ, Xi). When the
batch size is much smaller than the size of the dataset, S N, then we can model the batch gradient
as an average of S i.i.d. samples from a noisy version of the true gradient g(θ). Using the central limit
theorem, we assume gB(θ) 一 g(θ) is a Gaussian random variable with mean μ = 0 and covariance
matrix Σ(θ) = 1 G(θ)G(θ)1. Under this assumption, the stochastic gradient update can be written as
θ(n+1) = θ(n) 一 ηg(θ(n)) + √G(θ)ξ, where ξ is a standard normal random variable. This update
is an Euler-Maruyama discretization with step size η of the stochastic differential equation
dθ
—g(θ)dt +
(31)
where Wt is a standard Wiener process. Equation (31) has been derived as a model for SGD in many
previous works Mandt et al. (2015). In order to simplify the analysis, many of these works have then
made additional assumptions on the covariance matrix Σ(θ) = G(θ)G(θ)1, such as Σ(θ) = H(θ)
where H(θ) is the Hessian matrix JaStrzebSki et al. (2017), Σ(θ) = C where C is some constant
matrix Mandt et al. (2015), and Σ(θ) = I where I is the identity matrix Chaudhari & Soatto (2018).
However, without any additional assumptions, the differential symmetries intrinsic to neural network
architectures add fundamental constraints on Σ.
As we showed in section 3, the gradient of the loss, regardless of the batch, is orthogonal to the
generator vector field ∂aψ∣α=ι associated with a symmetry. This implies the stochastic noise must
also observe the same property, h-√∣G(θ)ξ, ∂ɑψ∣α=ιi = 0. In order for this relationship to hold
for arbitrary noise6 ξ, then G(θ)l∂ɑψ∣α=ι = 0. In other words, the differential symmetry inherent
in neural network architectures projects the noise introduced by stochastic gradients onto low rank
subspaces.
Scale Symmetry with Stochasticity. In the previous section, we performed our analysis using
continuous-time model with deterministic gradients to facilitate calculations, and replaced them with
stochastic batch gradients upon discretization when we evaluated the results empirically. Instead, we
can also directly model the stochastic noise arising from batch gradient with an Itδ process to perform
analogous analysis in continuous-time as pointed out by Li et al. (2021) in a recent follow-up work.
First, we assume that the time evolution of network parameters θ(t) during SGD training is described
by the It6 process dθ = μdt + PnG(θ)dWt, where
μ = —g — λθ — 2(Hg + λHθ) + O(η2) + O(ηλ) + O(λ2).
Assuming a set of parameters A respect scale symmetry, then applying It6's lemma to the function
∣θ∕(t)∣2 yields the It6 process,
d∣θA(t)∣2 = {2θAμ + S Tr[G(θA)TG(Θa)]} dt + 2 JS^θlG(θ∕)dWt
={—2λ∣θA∣2 — η∣gA∣2 + S Tr[G(θA)TG(θ∕)]} dt.
Notice this is a deterministic ODE equivalent to the previously derived ODE with an additional
forcing term accounting for the variance of the noise. We can perform an analogous analysis for the
case of translation and rescale symmetry.
We can also consider the effect of stochasticity without the complexity of stochastic calculus by
considering the dynamics in the discrete setting. As explained in appendix I, this is possible for the
case without momentum, but becomes much more complicated once we consider momentum as well.
6We do not need to assume the noise is Gaussian in order for this property to be true. However, we adopt this
commonly accepted assumption to contextualize our work within the literature modeling SGD with an SDE.
20
Published as a conference paper at ICLR 2021
G Deriving the Exact Learning Dynamics of SGD with Momentum
We consider a continuous model of SGD with momentum incorporating weight decay (equation 14),
momentum (equation 15), stochasticity (equation 31), and modified flow (equation 17). As discussed
in appendix C, we can model the effect of momentum by considering the forward Euler discretization
θk+--θk and the backward EUler discretization -β θk-θk-)1. These terms introduce the numerical
artifacts n(1-a)皋θ and n(1-a)β泵θ respectively, as explained by the modified flow analysis in
appendix D. Incorporating these elements with weight decay gives the equation of learning,
η(1--α)(i+β) £ θ+(i - β)少+λθ=-g(θ).
2 dt dt
Incorporating the effect of stochasticity into this equation of learning gives the Langevin equation,
η(I- O) (1 + β)dv
-(1 - β)vdt - λθdt - g(θ)dt +
(32)
dθ = vdt,
where Wt is a standard Weiner process. Compared to the modified loss route (described in the
previous section), it is much more natural and simple to account for the effect of stochasticity with
modified flow.
Translation dynamics. Combining the geometric properties of gradient (hg, 1Ai = 0), Hessian
(H 1a = 0), and stochasticity (G(θ)l1∕ = 0) introduced by translation symmetry with the Langevin
equation of learning (equation 32) gives the differential equation,
(η(I - O) (1 + β)-d22 + (1- β) + + λ) hθ 1ai = 0.	(33)
2	dt2	dt
This is the differential equation for a harmonic oscillator with Y = n.-0-8+.) and ω =
γ η(i-02λ(i+β). Assuming the initial condition 舟hθ(t), IAilt=° = 0, then the general solution
(as derived in appendix H) is
hθ(t),1Ai=
e-γt kosh (√Y2 - ω21) + 〃/2Sinh (√γ2 - ω21) ) (θ(0), 1人)
< e-γt(1+ γt)hθ(0), IAi
e-"γt ( cos (√ω2 — γ21) +
√ωY-γ2 Sin (√ω2 - Y21) hθ hθ(0), 1Ai
Y>ω
Y=ω
Y<ω
(34)
Scale dynamics. Combining the geometric constraints on the gradient (hg, θAi = 0), Hessian
(HΘa = -gA), and stochasticity (G(θ∕)lθ∕ = 0) introduced by scale symmetry with the Langevin
equation of learning (equation 32) gives the following differential equation7,
η(1 - α)(1 + β) d2	(1 - β) d ∖	，2	η(1 - α)(1 + β) dθA
----4----旃 + ^^dt + λ) lθAl =---2---- -dΓ
(35)
This is the differential equation for a driven harmonic oscillator with Y = 以「Q-：1+万),ω =
Jη(1-t4λ(1+β), and f(t) = 2 ∣dθA∣2. Assuming the initial condition 品∣θA∣2∣t=0 = 0, then the
general solution (derived in appendix H) is
jθA(t)lh + Rot e-Mt-τ)( Sinh( √gt-τ))) 2∣dθA (τ )∣2 dτ γ>ω
∣θA(t)∣2 =	∣θA(t)lh + R0 e-γ(t-τ)(t-τ)2 ∣簪(τ)∣2 dτ	γ = ω (36)
∣θA(t)∣h + Rot e-")( W「-TD ) 2 ∣ 智(τ) ∣2 dτ	γ<ω
7The derivation of this ODE uses〈 d2θA ,θ A = 1 条 ∣Θa∣2 — | dθA |2.
21
Published as a conference paper at ICLR 2021
where ∣θ∕(t)∣h is the solution to the homogeneous harmonic oscillator
e-γt {osh (pγ2 - ωt) + √γY-ω2 Sinh (,γ2 - ω21)) ∣Θa(0)∣2 γ > ω
lθA⑴|h =<e-γtα + Yt)lθA(O)|2	Y = 3
e-γt 卜os (p32 - Y21) + √ωY-γ2 sin (,ω2 - Y21)) ∣Θa(0)∣2	γ<ω
(37)
Rescale dynamics. Combining the geometric properties of gradient hg, θA1 - θA2 i = 0, Hessian
(H(θAι - θ∕2)+ gAι - gA2 = 0), and StOChaStiCity (G(ΘaJiΘ∕i - G(θA2 )lθ∕2 = O) introduced
by rescale symmetry with the Langevin equation of learning (equation 32) gives the differential
equation,
η(1 — a)(1 + β) d2
4 dt2
+ (1-0d + λ) (∣θA1∣2-∣θA212)
η(1 - α)(1 + β)
2
(38)
This is the same harmonic oscillator given by equation (35) with the different forcing term f (t)
八 dθAl ∣2 _ I dθA2 12、
[dt 1 - 1 dt 1 )
2
.The general solution is given by equation (36) and (37) replacing ∣θ∣2 and
解I2 by ∣θAι I2 - ∣θA212 and |竽∣2 - |d⅛212 respectively.
H General solutions for ODEs
H.1 Exponential Growth
Here we will solve for the general solution of the homogenous first-order linear differential equation,
d
dt + λ)x⑴
O.
Assume a solution of the form x(t) = eαt. Plugging this in gives the auxiliary equation α + λ = O.
Thus, the general solution to the differential equation with initial condition x(O) is
x(t) = e-λtx(O).
Now we will solve the inhomogenous differential equation,
(d+ + λ) x(t) = f ⑴.
dt
Multiply both sides by eλt and factor the left hand side using the product rule such that the differential
equation simplies to K (eλtx(t)) = eλtf (t). Integrate this equation and using the fundamental
theorem of calculus rearrange to get the solution
Z
0
x(t)
e-λtx(O) +
e-λ(t-τ)f(τ)dτ.
H.2 Harmonic Oscillator
Here we will solve the general solution for a harmonic oscillator,
(d2
(dt2
d
+2γdt+32
x(t)
O.
Assume a solution of the form x(t) = eαt. Plugging this in gives the auxiliary equation α2 + 2Yα +
32 = 0 with solutions α± = -γ ± ,γ2 - ω2. Thus, the general solution to the oscillator equation
with initial conditions x(0) and * (0) = 0 is
x(t) = e-γt(Cie√γ2-ω2t + C2e-C2-ω2t)
22
Published as a conference paper at ICLR 2021
where C1 , C2 are constants
Y+ pγ2-ω2 X	= -γ+ pγ 2-ω2 χ(0).
1	2pγ2 - ω2	(),	2pγ2 - ω2	()
Using hyperbolic functions the solution simplifies as
x(t) = e-γt (cosh (P Y2 — ωt) + P : =2 Sinh (pY2
	
x(0).
The form of this general solution implicitly assumes γ > ω, the overdamped setting. When γ = ω ,
the critically damped setting, then the solution reduces to
x(t) = e-γt(C1 + C2t),
where C1 = x(0) and C2 = γx(0). When γ < ω, the underdamped setting, then the solution reduces
to
x(t) =
where C1 = x(0) and C2
H.3 Driven Harmonic Oscillator
Here we will solve the general solution for a driven harmonic oscillator,
d2 d
(薜 + 2γdt+ω x x(t) =f(t)∙
First notice that if xh(t) is a solution to the homogenous harmonic oscillator and xd(t) a specific
solution to the driven harmonic oscillator, then
x(t) = xh (t) + xd (t),
is the general solution to the driven harmonic oscillator. We will use the Fourier transform to solve
for xd(t).
ɪ . ʌ / ∖	/X—7 % ∖ / \	1 P/ ∖	/X—7 ；八/\1 .1 1 -	♦ .	C	C / , \	1 P / , \
Let Xd(τ) = WLxd)(T) and f (T) = (VLf )(τ) be the Fourier transforms of Xd(t) and f (t) respec-
tively. Applying the Fourier transform to the driven harmonic oscillator equation and rearranging
gives
xd(τ) = (-T2 + 2γiτ + ω2)-1 f(τ),
which implies by the inverse Fourier transform that xd(t) is the convolution,
xd(t) = (G * f)(t),
where G(t) is Green’s function (the driven solution xd(t) for the dirac delta forcing function δ0),
C-Yt
G(t) = Θ(t)	_2 (e√γ2-ω2t — e-√γ2-ω2t),
2 γ2 - ω2
which again using hyperbolic functions simplifies as
sinh
G(t) = Θ(t)e-γt
,γ2 — ω2
	
This form of Green’s function is again implicitly assuming γ > ω. When γ = ω, the function
simplifies to
G(t) = Θ(t)e-γtt,
and when γ < ω, the function simplifies to
G(t) = Θ(t)e-γt
√ω2 - γ2
Noticing that both G and f are only supported on [0, ∞), their convolution can be simplified and the
general solution for the driven harmonic oscillator is
x(t)
xh(t)+Zt
0
G(t - T)f(T)dT.
23
Published as a conference paper at ICLR 2021
I Deriving Dynamics in the Discrete Setting
In section 4 we identified certain parameter combinations associated with network symmetries that
are conserved under gradient flow. However, as we explained in section 5, these conservation laws
are not observed empirically. To remedy this discrepancy we constructed more realistic continuous
models for SGD, incorporating weight decay, momentum, stochasticity, and finite learning rates. In
section 6 we derived the exact dynamics for the parameter combinations under this more realistic
setting, demonstrating near perfect alignment with the empirical dynamics. What would happen
if we instead derived the dynamics for the parameter combinations directly in the discrete setting
of SGD? Here, we will identify these discrete dynamics and discuss the relationship between the
discrete equations and the continuous solutions.
Gradient descent with learning rate η and weight decay constant λ is given by the update equation
θ(n+1) = (1 - ηλ)θ(n) - ηg(θ(n)), and initial condition θ(0). Using this update equation the sum of
the parameters after n + 1 steps can be “unrolled” as,
n
hθ(n+1), 1Ai = (1 - ηλ)n+1hθ(0), 1Ai + η X(1 - ηλ)n-ihg(θ(i)), 1Ai.
i=0
Similarly, the squared Euclidean norm of the parameters after n + 1 steps can be “unrolled” as,
∣θ(n+1)l2 = (i-ηλ)2(n+1)∣θ(0)∣2+η2 XX (i-ηλ)2(I)以(。⑴)∣2-2η XX (i-ηλ)2(I)+1hg(θ⑴),θA)).
i=0	i=0
Combining these unrolled equations, with the gradient properties of symmetry discussed in section 3,
gives the discrete dynamics for the parameter combinations,
Translation:	Dθ(n),1A E = (1 - ηλ)n Dθ(0), 1A E	(39)
Scale:	θA(n)2	=	(1 - ηλ)2n	θA(0)2+η2nX-1(1	- ηλ)2(n-1-i)	gA(i)2	(40)
i=0
Rescale:	θA(n1)2-	θA(n2)2 =	(41)
(1 - ηλ)2n	θA(01)2	-	θA(02)2	+ η2 nXi=-01(1	- ηλ)2(n-1-i)	gA(i)12-gA(i)22
Notice the striking similarity between the continuous equations (18), (19), (20) presented in section 6
and the discrete equations (39), (40), (41). The exponential function with decay rate λ from the
continuous solutions are replaced by a power of the base (1 - ηλ) in the discrete setting. The
integral of exponentially weighted gradient norms from the continuous solutions are replaced by a
Riemann sum of power weighted gradient norms in the discrete setting. This is further confirmation
that the continuous solutions we derived, and the modified gradient flow equation of learning used,
well approximate the actual empirics. While the equations derived in the discrete setting remove
any uncertainty about the exactness of the theoretical predictions, they provide limited qualitative
understanding for the empirical learning dynamics. This is especially true if we consider the learning
dynamics with momentum. In this setting, the process of “unrolling” is much more complicated and
the harmonic nature of the empirics, easily derived in the continuous setting, is hidden in the discrete
algebra.
24
Published as a conference paper at ICLR 2021
J Experimental Details
An open source version of our code, used to generate all the figures in this paper, is available at
github.com/danielkunin/neural-mechanics.
Dataset. While we ran some initial experiments on Cifar-100, the dataset used in all the empirical
figures in this documents was Tiny Imagenet. It is used for image categorization an consists of
100,000 training images at a resolution of 64 × 64 spanning 200 classes.
Model. We use standard VGG-16 models for all out experiments with the following modifications:
•	The last three fully connected layers at the end have been adjusted for an input at the Tiny
ImageNet resolution (64 × 64) and thus consist of 2048, 2048, and 200 layers respectively.
•	In addition to the standard arrangement of conv layers for the VGG-16, we consider a
variant where we add a batch normalization layer between every convolutional layer and its
activation function.
Training hyperparameters. Certain hyperparameters were varied during training. Below we outline
the combinations explored. All models were initialized using Kaiming Normal, and no learning rate
drops or warmup were used.
Model	Dataset	Epochs	Batch size	Opt.	LR	Mom.	WD	Damp.
VGG-16	Tiny ImageNet	100	256	SGD	[0.1,0.01]	-	[0,0.001,0.0005, 0.0001]	0
VGG-16 w/BN	Tiny ImageNet	100	256	SGD	[θ.1,0.0l]	-	[0,0.001,0.0005, 0.0001]	0
VGG-16	Tiny ImageNet	100	128	SGDM	0.1	[0, 0.9, 0.99]	[0,0.001,0.0005, 0.0001]	0
VGG-16 w/BN	Tiny ImageNet	100	128	SGDM	0.1	[θ, 0.9, 0.99]	[0,0.001,0.0005, 0.0001]	0
Table 4: Training hyperparameters.
Counting number of symmetries. Here we explain how to count the number of symmetries a
VGG-16 model contains.
•	Scale symmetries appear once per channel at every layer preceding a batch normalization
layer. ForourVGG-16model Withbatchnorm: 2∙64+2∙128+3∙256+3∙512+3∙512+3 =
4, 227.
•	Rescale symmetries appear once per channel Where there are afine transforms betWeen
layers as Well as once per input neuron to a fully connected layer. Note that the sizes of
the fully connected layers depend on input image size and the number of classes. For our
VGG-16 model with and without batchnorm on Tiny ImageNet, this is: (2 ∙ 64 + 2 ∙ 128 +
3 ∙ 256 + 3 ∙ 512 + 3 ∙ 512 + 3) + (2048 + 1024 + 1024) = 8, 323.
•	Translation symmetries appear once per input value to the softmax function, which for the
case of classification is always equal to the number of classes, plus the bias term. For our
case this is 200 + 1 = 201.
	#Params	# Scale # Rescale # Translation
	 VGG-16 on Tiny ImageNet VGG-16 w/BN on Tiny ImageNet	18,067,464 18,075,912	0	8,323	201 4,227	8,323	201
Table 5: Counting the number of symmetries.
Computing the theoretical predictions. Some of the expressions shown in equations (18), (19), and
(20) in section 6 are not trivial to compute as they involve an integral of an exponentially weighted
gradient term. To tackle this problem, we wrote custom optimizers in PyTorch with additional buffers
to approximate the integral via a Riemann sum. At every update step, the argument in the integral
term was computed from the batch gradients, scaled appropriately, and was accumulated in the
buffer. Note that the above sum needs to be scaled by the learning rate, which is the coarseness
of the grid of this Riemann sum. Checkpoints of the model and optimizer states were stored at
25
Published as a conference paper at ICLR 2021
pre-defined frequencies during training. Our visualizations involve computing the right hand side
of equations (18), (19), and (20) from the model states and the left hand side of the same equations
from the integral buffers stored in the optimizer states as explained above. These two quantities are
referred to as “empirical” and “theoretical” in the figures and are depicted with solid color lines and
dotted lines, respectively.
J.1 Additional Empirics
In this work we made exact predictions for the dynamics of combinations of parameters during
training with SGD. Importantly, these predictions were made at the neuron level, but could be
aggregated for each layer. Here we plot our predictions at both layer and neuron levels for VGG-16
models trained on Tiny ImageNet.
λ = 0 λ = 10-4	λ = 5 × 10-4	λ = 10-3
Figure 8: The planar dynamics of VGG-16 on Tiny ImageNet. We plot the column sum of the
final linear layer of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with
SGD with learning rate η = 0.1, weight decay λ ∈ {0,10-4, 5 X 10-4,10-3}, and batch size
S = 256. Colored lines are empirical column sums of the last layer through training and black dashed
lines are the theoretical predictions of equation (18).
900
11111 1 theory
λ
5 X 10
600
300
conv. 10
0
conv. 11
conv. 12
Time (η × steps)
Rnv. 13
Figure 9: The spherical dynamics of VGG-16 BN on Tiny ImageNet. We plot the squared
Euclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained on
Tiny ImageNet with SGD with learning rate η = 0.1, weight decay λ ∈ {0,10-4,5 × 10-4, 10-3},
and batch size S = 256. Colored lines represent empirical layer-wise squared norms through training
and the white dashed lines the theoretical predictions given by equation (19).
1200
26
Published as a conference paper at ICLR 2021
=，隹=」=隹=
01234012340123401234
XlO3	XlO3	×103	×10j
Time (η × steps)
IIJjjJ theory
conv 10-9
conv 11-10
conv 12-11
conv 13-12
Figure 10: The hyperbolic dynamics of VGG-16 on Tiny ImageNet. We plot the difference
between the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model
(without batch normalization) trained on Tiny ImageNet with SGD with learning rate η = 0.1, weight
decay λ ∈ {0,10-4,5 X 10-4,10-3}, and batch size S = 256. Colored lines represent empirical
differences in consecutive layer-wise squared norms through training and the white dashed lines the
theoretical predictions given by equation (20).
Oooo
6.。= 0
-0.30
0 4 8 12 16
Time (η X steps)
Figure 11: The planar dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the
column sum of the final linear layer of a VGG-16 model (without batch normalization) trained on Tiny
ImageNet with Momentum with learning rate η = 0.1, weight decay λ ∈ {0, 10-4, 5 × 10-4, 10-3},
momentum coefficient β ∈ {0, 0.9, 0.99}, and batch size S = 128. Colored lines are empirical
column sums of the last layer through training and black dashed lines are the theoretical predictions
of equation (34).
27
Published as a conference paper at ICLR 2021
07
O 4 8 12 16
O 4 8 12 16
O 4 8 12 16
O 4 8 12 16
Time (η X steps)
Figure 12: The spherical dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot
the squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normal-
ization) trained on Tiny ImageNet with Momentum with learning rate η = 0.1, weight decay
λ ∈ {0, 10-4, 5 × 10-4, 10-3}, momentum coefficient β ∈ {0, 0.9, 0.99}, and batch size S = 128.
Colored lines represent empirical layer-wise squared norms through training and the white dashed
lines the theoretical predictions given by equation (36) and (37)
Z = I——"= —-一 7Al =
Figure 13: The hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the
difference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16
model (without batch normalization) trained on Tiny ImageNet with Momentum with learning rate
η = 0.1, weight decay λ ∈ {0, 10-4, 5×10-4, 10-3}, momentum coefficient β ∈ {0, 0.9, 0.99}, and
batch size S = 128. Colored lines represent empirical differences in consecutive layer-wise squared
norms through training and the black dashed lines the theoretical predictions given by equation (36)
and (37) replacing ∣θ∣2 and 摩∣2 by I。/」2 -∣θ∕212 and |dθA112 -|dθ-A2-12 respectively.
28
Published as a conference paper at ICLR 2021
.....theory
CθnVIΛ conv. 1
conv. 2
Time (η X steps)
Figure 14: The per-neuron spherical dynamics of SGD on VGG-16 BN on Tiny ImageNet. We
plot the per-neuron squared Euclidean norms for convolutional layers of a VGG-16 model (with batch
normalization) trained on Tiny ImageNet with SGD with learning rate η = 0.1, weight decay λ = 0,
and batch size S = 256. Colored lines represent empirical layer-wise squared norms through training
and the black dashed lines the theoretical predictions given by equation (19).
,■■■ theory
conv. 2-1
conv. 3-2
Time (η X steps)
Figure 15: The per-neuron hyperbolic dynamics of SGD on VGG-16 on Tiny ImageNet. We
plot the per-neuron difference between the squared Euclidean norms for consecutive convolutional
layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with SGD with
learning rate η = 0.1, weight decay λ = 0, and batch size S = 256. Colored lines represent empirical
differences in consecutive layer-wise squared norms through training and the black dashed lines the
theoretical predictions given by equation (20).
conv. 4-3
conv. 5-4
conv. 6-5
conv. 7-6
conv. 8-7
conv. 9-8
conv. 10-9
conv. 11-10
conv. 12-11
conv. 13-12
29
Published as a conference paper at ICLR 2021

β = 0.9
β = 0.99
theory
Conv. 1
conv. 2
conv. 4
conv. 5
conv. 6
conv. 7
conv. 8
conv. 9
Time (η X steps)
Figure 16:	The per-neuron spherical dynamics of Momentum on VGG-16 on Tiny ImageNet.
We plot the per-neuron squared Euclidean norms for convolutional layers of a VGG-16 model
(with batch normalization) trained on Tiny ImageNet with Momentum with learning rate η = 0.1,
weight decay λ = 0, momentum coefficient β ∈ {0.9,0.99}, and batch size S = 128. Colored
lines represent empirical layer-wise squared norms through training and the black dashed lines the
theoretical predictions given by equation (36) and (37).
β = 0.9
β = 0.99
theory
-0.08
-0.08
-0.08
-0.16
0.16
0.08
0.00
-o.oβ
-0.08
-0.08
-0.08
Conv 2-1
conv. 3-2
Conv 4-3
Conv 5-4
Conv 6-5
Conv 7-6
Conv 8-7
Conv 9-8
ConV. 10-9
ConV. 11-10
-0.08
Time (η X steps)
Figure 17:	The per-neuron hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet.
We plot the per-neuron difference between the squared Euclidean norms for consecutive convolutional
layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with Momentum
with learning rate η = 0.1, weight decay λ = 0, momentum coefficient β ∈ {0.9,0.99}, and batch
size S = 128. Colored lines represent empirical differences in consecutive layer-wise squared norms
through training and the black dashed lines the theoretical predictions given by equation (36) and
(37) replacing ∣θ∣2 and |dθ∣2 by ∣θ∕ι ∣2 -∣θ∕212 and |dθA^-12 -|dθA212 respectively.
30