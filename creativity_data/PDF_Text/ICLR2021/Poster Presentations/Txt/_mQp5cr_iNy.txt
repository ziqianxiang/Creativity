Published as a conference paper at ICLR 2021
Adversarially Guided Actor-Critic
Yannis Flet-Berliac*
Inria, Scool team
Univ. Lille, CRIStAL, CNRS
yannis.flet-berliac@inria.fr
Johan Ferret*
Google Research, Brain team
Inria, Scool team
Univ. Lille, CRIStAL, CNRS
Olivier Pietquin	Philippe Preux
Google Research, Brain team	Inria, Scool team
Univ. Lille, CRIStAL, CNRS
Matthieu Geist
Google Research, Brain team
Ab stract
Despite definite success in deep reinforcement learning problems, actor-critic
algorithms are still confronted with sample inefficiency in complex environments,
particularly in tasks where efficient exploration is a bottleneck. These methods
consider a policy (the actor) and a value function (the critic) whose respective losses
are built using different motivations and approaches. This paper introduces a third
protagonist: the adversary. While the adversary mimics the actor by minimizing the
KL-divergence between their respective action distributions, the actor, in addition to
learning to solve the task, tries to differentiate itself from the adversary predictions.
This novel objective stimulates the actor to follow strategies that could not have
been correctly predicted from previous trajectories, making its behavior innovative
in tasks where the reward is extremely rare. Our experimental analysis shows that
the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more
exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods
on a set of various hard-exploration and procedurally-generated tasks.
1	Introduction
Research in deep reinforcement learning (RL) has proven to be successful across a wide range
of problems (Silver et al., 2014; Schulman et al., 2016; Lillicrap et al., 2016; Mnih et al., 2016).
Nevertheless, generalization and exploration in RL still represent key challenges that leave most
current methods ineffective. First, a battery of recent studies (Farebrother et al., 2018; Zhang et al.,
2018a; Song et al., 2020; Cobbe et al., 2020) indicates that current RL methods fail to generalize
correctly even when agents have been trained in a diverse set of environments. Second, exploration has
been extensively studied in RL; however, most hard-exploration problems use the same environment
for training and evaluation. Hence, since a well-designed exploration strategy should maximize the
information received from a trajectory about an environment, the exploration capabilities may not be
appropriately assessed if that information is memorized. In this line of research, we choose to study
the exploration capabilities of our method and its ability to generalize to new scenarios. Our evaluation
domains will, therefore, be tasks with sparse reward in procedurally-generated environments.
In this work, we propose Adversarially Guided Actor-Critic (AGAC), which reconsiders the actor-critic
framework by introducing a third protagonist: the adversary. Its role is to predict the actor’s actions
correctly. Meanwhile, the actor must not only find the optimal actions to maximize the sum of
expected returns, but also counteract the predictions of the adversary. This formulation is lightly
inspired by adversarial methods, specifically generative adversarial networks (GANs) (Goodfellow
et al., 2014). Such a link between GANs and actor-critic methods has been formalized by Pfau &
Vinyals (2016); however, in the context of a third protagonist, we draw a different analogy. The
adversary can be interpreted as playing the role of a discriminator that must predict the actions of the
actor, and the actor can be considered as playing the role of a generator that behaves to deceive the
predictions of the adversary. This approach has the advantage, as with GANs, that the optimization
procedure generates a diversity of meaningful data, corresponding to sequences of actions in AGAC.
* Equal contribution.
1
Published as a conference paper at ICLR 2021
This paper analyses and explores how AGAC explicitly drives diversity in the behaviors of the agent
while remaining reward-focused, and to which extent this approach allows to adapt to the evolving
state space of procedurally-generated environments where the map is constructed differently with
each new episode. Moreover, because stability is a legitimate concern since specific instances of
adversarial networks were shown to be prone to hyperparameter sensitivity issues (Arjovsky & Bottou,
2017), we also examine this aspect in our experiments.
The contributions of this work are as follow: (i) we propose a novel actor-critic formulation inspired
from adversarial learning (AGAC), (ii) we analyse empirically AGAC on key reinforcement learning as-
pects such as diversity, exploration and stability, (iii) we demonstrate significant gains in performance
on several sparse-reward hard-exploration tasks including procedurally-generated tasks.
2	Related Work
Actor-critic methods (Barto et al., 1983; Sutton, 1984) have been extended to the deep learning setting
by Mnih et al. (2016), who combined deep neural networks and multiple distributed actors with an
actor-critic setting, with strong results on Atari. Since then, many additions have been proposed, be it
architectural improvements (Vinyals et al., 2019), better advantage or value estimation (Schulman
et al., 2016; Flet-Berliac et al., 2021), or the incorporation of off-policy elements (Wang et al., 2017;
Oh et al., 2018; Flet-Berliac & Preux, 2020). Regularization was shown to improve actor-critic
methods, either by enforcing trust regions (Schulman et al., 2015; 2017; Wu et al., 2017), or by
correcting for off-policiness (Munos et al., 2016; Gruslys et al., 2018); and recent works analyzed its
impact from a theoretical standpoint (Geist et al., 2019; Ahmed et al., 2019; Vieillard et al., 2020a;b).
Related to our work, Han & Sung (2020) use the entropy of the mixture between the policy induced
from a replay buffer and the current policy as a regularizer. To the best of our knowledge, none of
these methods explored the use of an adversarial objective to drive exploration.
While introduced in supervised learning, adversarial learning (Goodfellow et al., 2015; Miyato et al.,
2016; Kurakin et al., 2017) was leveraged in several RL works. Ho & Ermon (2016) propose an
imitation learning method that uses a discriminator whose task is to distinguish between expert trajec-
tories and those of the agent while the agent tries to match expert behavior to fool the discriminator.
Bahdanau et al. (2019) use a discriminator to distinguish goal states from non-goal states based on
a textual instruction, and use the resulting model as a reward function. Florensa et al. (2018) use a
GAN to produce sub-goals at the right level of difficulty for the current agent, inducing a form of
curriculum. Additionally, Pfau & Vinyals (2016) provide a parallel between GANs and the actor-critic
framework.
While exploration is driven in part by the core RL algorithms (Fortunato et al., 2018; Han & Sung,
2020; Ferret et al., 2021), it is often necessary to resort to exploration-specific techniques. For
instance, intrinsic motivation encourages exploratory behavior from the agent. Some works use
state-visitation counts or pseudo-counts to promote exhaustive exploration (Bellemare et al., 2016a),
while others use curiosity rewards, expressed in the magnitude of prediction error from the agent, to
push it towards unfamiliar areas of the state space (Burda et al., 2018). Ecoffet et al. (2019) propose a
technique akin to tree traversal to explore while learning to come back to promising areas. Eysenbach
et al. (2018) show that encouraging diversity helps with exploration, even in the absence of reward.
Last but not least, generalization is a key challenge in RL. Zhang et al. (2018b) showed that, even
when the environment is not deterministic, agents can overfit to their training distribution and that it
is difficult to distinguish agents likely to generalize to new environments from those that will not.
In the same vein, recent work has advocated using procedurally-generated environments, in which
a new instance of the environment is sampled when a new episode starts, to assess generalization
capabilities better (Justesen et al., 2018; Cobbe et al., 2020). Finally, methods based on network
randomization (Igl et al., 2019), noise injection (Lee et al., 2020), and credit assignment (Ferret et al.,
2020) have been proposed to reduce the generalization gap for RL agents.
3	Background and Notations
We place ourselves in the Markov Decision Processes (Puterman, 1994) framework. A Markov
Decision Process (MDP) is a tuple M = {S, A, P, R, γ}, where S is the state space, A is the action
2
Published as a conference paper at ICLR 2021
space, P is the transition kernel, R is the bounded reward function and γ ∈ [0, 1) is the discount factor.
Let π denote a stochastic policy mapping states to distributions over actions. We place ourselves
in the infinite-horizon setting, i.e., we seek a policy that optimizes J(π) = Eπ[Pt∞=0 γtr (st, at)].
The value of a state is the quantity V π(s) = Eπ[Pt∞=0 γtr (st, at) |s0 = s] and the value of a
state-action pair Qπ(s, a) of performing action a in state s and then following policy π is defined as:
Qπ(s, a) = Eπ [Pt∞=0 γtr (st, at) |s0 = s, a0 = a]. The advantage function, which quantifies how
an action a is better than the average action in state s, is Aπ (s, a) = Qπ(s, a) - Vπ(s). Finally, the
entropy Hn of a policy is calculated as: Hn (S) = E∏(∙∣s) [- log ∏(∙∣s)].
Actor-Critic and Deep Policy Gradients. An actor-critic algorithm is composed of two main
components: a policy and a value predictor. In deep RL, both the policy and the value function are
obtained via parametric estimators; we denote θ and φ their respective parameters. The policy is
updated via policy gradient, while the value is usually updated via temporal difference or Monte
Carlo rollouts. In practice, for a sequence of transitions {st, at, rt, st+1}t∈[0,N], we use the following
policy gradient loss (including the commonly used entropic penalty):
t+N
1 t+
LPG = - N ^X (AtOlog π (at0 |st0, θ) + αHπ (StO, θ)),
t0=t
where α is the entropy coefficient and At is the generalized advantage estimator (Schulman et al.,
2016) defined as: At = PttO+=Nt (γλ)tO-t(rtO + γVφold(StO+1) - Vφold (StO )), with λ a fixed hyperpa-
rameter and Vφold the value function estimator at the previous optimization iteration. To estimate the
value function, We solve the non-linear regression problem minimizeφ Pt+=N(Vφ(s∕) - Vto)2 where
VV = At + Vφold (st0 ).
4	Adversarially Guided Actor-Critic
To foster diversified behavior in its trajectories, AGAC introduces a third protagonist to the actor-critic
framework: the adversary. The role of the adversary is to accurately predict the actor’s actions,
by minimizing the discrepancy between its action distribution πadv and the distribution induced
by the policy π . Meanwhile, in addition to finding the optimal actions to maximize the sum of
expected returns, the actor must also counteract the adversary’s predictions by maximizing the
discrepancy between π and πadv (see Appendix B for an illustration). This discrepancy, used as a
form of exploration bonus, is defined as the difference of action log-probabilities (see Eq. (1)), whose
expectation is the Kullback-Leibler divergence:
DκL(∏(∙∣s)k∏adv(∙∣s))= E∏(∙∣s) [log ∏(∙∣s) - log ∏adv(∙∣s)].
Formally, for each state-action pair (st, a。in a trajectory, an action-dependent bonus log ∏(a∕st)-
log ∏adv(αt∣st) is added to the advantage. In addition, the value target of the critic is modified to
include the action-independent equivalent, which is the KL-divergence DκL(∏(∙∣st) IInadV(∙∣st))∙ We
discuss the role of these mirrored terms below, and the implications of AGAC’s modified objective
from a more theoretical standpoint in the next section. In addition to the parameters θ (resp. θold the
parameter of the policy at the previous iteration) and φ defined above (resp. φold that of the critic),
we denote ψ (resp. ψold) that of the adversary.
AGAC minimizes the following loss:
LAGAC = LPG + βV LV + βadvLadv .
In the new objective LPG = -N PN=0(AAGAClog ∏ (at∣st,θ) + αHπ(st,θ)), AGAC modifies At as:
AAGAC = At + c ( log∏(at∣st, θold) - log∏adv(αt∣st, ψold),
(1)
with c a varying hyperparameter that controls the dependence on the action log-probability difference.
To encourage exploration without preventing asymptotic stability, c is linearly annealed during the
course of training. LV is the objective function of the critic defined as:
LV = N X (Vφ(St)- (Vt + cDKL(π(∙∣st,θoId)IlnadV(∙∣st, ψoId))))
(2)
3
Published as a conference paper at ICLR 2021
Finally, Ladv is the objective function of the adversary:
1N
Ladv = N): DKL (n('|st, θold) IInadv(Ist, ψ)).	(3)
Eqs. (1), (2) and (3) are the three equations that our method modifies (we color in blue the specific
parts) in the traditional actor-critic framework. The terms βV and βadv are fixed hyperparameters.
Under the proposed actor-critic formulation, the probability of sampling an action is increased if
the modified advantage is positive, i.e. (i) the corresponding return is larger than the predicted
value and/or (ii) the action log-probability difference is large. More precisely, our method favors
transitions whose actions were less accurately predicted than the average action, i.e. log n(a∣s) 一
log∏adv(a∣s) ≥ Dkl(∏(∙∣s)k∏adv(∙∣s)). ThiS is particularly visible for λ → 1, in which case the
generalized advantage is At = Gt 一 Vφold(st), resulting in the appearance of both aforementioned
mirrored terms in the modified advantage:
AAGAC = Gt- Wφold + C (log∏(at∣st) - log∏adv(at∣st) - DKLd(∏(∙∣st)k∏adv(∙∣st))),
with Gt the observed return, Vtφold the estimated return and DKL(∏(∙∣st)k∏adv(∙∣st)) the estimated
KL-divergence (estimated components of Vφold (st) from Eq. 2).
To avoid instability, in practice the adversary is a separate estimator, updated with a smaller learning
rate than the actor. This way, it represents a delayed and more steady version of the actor’s policy,
which prevents the agent from having to constantly adapt or focus solely on fooling the adversary.
4.1	Building Motivation
In the following, we provide an interpretation of AGAC by studying the dynamics of attraction and
repulsion between the actor and the adversary. To simplify, we study the equivalent of AGAC in a
policy iteration (PI) scheme. PI being the dynamic programming scheme underlying the standard
actor-critic, we have reasons to think that some of our findings translate to the original AGAC algorithm.
In PI, the quantity of interest is the action-value, which AGAC would modify as:
QAπGkAC = Qπk + c (log πk - logπadv),
with πk the policy at iteration k . Incorporating the entropic penalty, the new policy πk+1 verifies:
∏k+ι = arg max Jpi (∏) = arg max EsEa 〜∏(∙∣s) [Q∏kAc(s,a) — α log π(a∣s)].
ππ
We can rewrite this objective:
Jpi (n) = EsEa 〜∏(∙∣s)[Q∏kAc(s,a) — α log n(a∣s)]
=EsEa〜∏(∙∣s)[Q∏k (s, a) + c(log∏k(a|s) - log∏adv(a∣s)) - αlogπ(a∣s)]
=EsEa〜∏(∙∣s)[Q∏k (s, a) + C (log ∏k(a|s) - log n(a∣s) + log n(a∣s) - log ∏adv(a∣s)) - α log π(a∣s)]
=EshEa〜∏(∙∣s)[Q∏k (s,a)] - cDκL(∏(∙∣s)∣∣∏k(∙∣s)) + cDKL(∏(∙∣s)∣∣∏adv(∙∣s)) + αH(n(∙∣s)) ].
} X}
πk is attractive	πadv is repulsive	enforces stochastic policies
Thus, in the pi scheme, AGAC finds a policy that maximizes Q-values, while at the same time
remaining close to the current policy and far from a mixture of the previous policies (i.e., πk-1, πk-2,
πk-3, . . . ). Note that we experimentally observe (see Section 5.3) that our method performs better
with a smaller learning rate for the adversarial network than that of the other networks, which could
imply that a stable repulsive term is beneficial.
This optimization problem is strongly concave in π (thanks to the entropy term), and is state-wise a
Legendre-Fenchel transform. its solution is given by (see Appendix E for the full derivation):
πk
πk + 1 Z	\一
πadv
C
ɑ exp Qk.
α
This result gives us some insight into the behavior of the objective function. Notably, in our example,
if πadv is fixed and C = α, we recover a KL-regularized pi scheme (Geist et al., 2019) with the
modified reward r - C log πadv.
4
Published as a conference paper at ICLR 2021
4.2	Implementation
In all of the experiments, we use PPO (Schulman et al., 2017) as the base algorithm and build on it to
incorporate our method. Hence,
_	1 t+N	∏(ato〔st，,θ)	AGAC	∏(at |st，,θ)
PG - -N N min ［而EzdyAt0 , p(∏(ato |st，,θold), 1 - e, + e
with AtA0GAC given in Eq. (1), N the temporal length considered for one update of parameters and
the clipping parameter. Similar to RIDE (RaileanU & Rocktaschel, 2019), We also discount PPO by
episodic state visitation counts, except for VizDoom (cf. Section 5.1). The actor, critic and adversary
use the convolutional architecture of the Nature paper of DQN (Mnih et al., 2015) With different
hidden sizes (see Appendix D for architecture details). The three neural netWorks are optimized
using Adam (Kingma & Ba, 2015). Our method does not use RNNs in its architecture; instead,
in all our experiments, We use frame stacking. Indeed, Hausknecht & Stone (2015) interestingly
demonstrate that although recurrence is a reliable method for processing state observation, it does not
confer any systematic advantage over stacking observations in the input layer of a CNN. Note that
the parameters are not shared betWeen the policy, the critic and the adversary and that We did not
observe any noticeable difference in computational complexity When using AGAC compared to PPO.
We direct the reader to Appendix C for a list of hyperparameters. In particular, the c coefficient of the
adversarial bonus is linearly annealed.
At each training step, We perform a stochastic optimization step to minimize LAGAC using stop-gradient:
θ — Adam(θ, VθLPG,η),	φ — Adam (φ, VφLV,小), ψ — Adam(ψ, VψLadv,η2).
5	Experiments
In this section, We describe our experimental study in Which We investigate: (i) Whether the adversarial
bonus alone (e.g. Without episodic state visitation count) is sufficient to outperform other methods
in VizDoom, a sparse-reWard task With high-dimensional observations, (ii) Whether AGAC succeeds
in partially-observable and procedurally-generated environments With high sparsity in the reWards,
compared to other methods, (iii) hoW Well AGAC is capable of exploring in environments Without
extrinsic reWard, (iv) the training stability of our method. In all of the experiments, lines are average
performances and shaded areas represent one standard deviation. The code for our method is released
at github.com/yfletberliac/adversarially-guided-actor-critic.
(a)	(b)	(c)	(d)
Figure 1: (a,b) Frames from the 3-D navigation task VizdoomMyWayHome. (c) MiniGrid-
KeyCorridorS6R3. (d) MiniGrid-ObstructedMazeFull.
Environments. To carefully evaluate the performance of our method, its ability to develop robust
exploration strategies and its generalization to unseen states, We choose tasks that have been used
in prior Work, Which are tasks With high-dimensional observations, sparse reWard and procedurally-
generated environments. In VizDoom (Kempka et al., 2016), the agent must learn to move along
corridors and through rooms Without any reWard feedback from the 3-D environment. The MiniGrid
environments (Chevalier-Boisvert et al., 2018) are a set of challenging partially-observable and
sparse-reWard gridWorlds. In this type of procedurally-generated environments, memorization is
impossible due to the huge size of the state space, so the agent must learn to generalize across the
different layouts of the environment. Each gridWorld has different characteristics: in the MultiRoom
5
Published as a conference paper at ICLR 2021
tasks, the agent is placed in the first room and should reach a goal placed in the most distant room. In
the KeyCorridor tasks, the agent must navigate to pick up an object placed in a room locked by a
door whose key is in another room. Finally, in the ObstructedMaze tasks, the agent must pick up a
box that is placed in a corner of a 3x3 maze in which the doors are also locked, the keys are hidden in
boxes and balls obstruct the doors. All considered environments (see Fig. 1 for some examples) are
available as part of OpenAI Gym (Brockman et al., 2016).
Baselines. For a fair assessment of our method, we compare to some of the most prominent methods
specialized in hard-exploration tasks: RIDE (RaiIeanU & Rocktaschel, 2019), based on an intrinsic
reward associated with the magnitude of change between two consecutive state representations and
state visitation, Count as CoUnt-Based Exploration (Bellemare et al., 2016b), which we coUple with
IMPALA (Espeholt et al., 2018), RND (BUrda et al., 2018) in which an exploration bonUs is positively
correlated to the error of predicting featUres from the observations and ICM (Pathak et al., 2017),
where a modUle only predicts the changes in the environment that are prodUced by the actions of the
agent. Finally, we compare to most the recent and best performing method at the time of writing in
procedUrally-generated environments: AMIGo (Campero et al., 2021) in which a goal-generating
teacher provides coUnt-based intrinsic goals.
5.1	Adversarially-based Exploration (No Episodic Count)
Table 1: Average retUrn in VizDoom at different timesteps.
Nb. of Timesteps	2M	4M	6M	8M	10M
AGAC	0.74 ± 0.05	0.96 ± 0.001	0.96 ± 0.001	0.97 ± 0.001	0.97 ± 0.001
RIDE	0.	0.	0.95 ± 0.001	0.97 ± 0.001	0.97 ± 0.001
ICM	0.	0.	0.95 ± 0.001	0.97 ± 0.001	0.97 ± 0.001
AMIGo	0.	0.	0.	0.	0.
RND	0.	0.	0.	0.	0.
Count	0.	0.	0.	0.	0.
In this section, we assess the benefits of Using an adversarially-based exploration bonUs and examine
how AGAC performs withoUt the help of coUnt-based exploration. In order to provide a comparison to
state-of-the-art methods, we choose VizDoom, a hard-exploration problem Used in prior work. In this
game, the map consists of 9 rooms connected by corridors where 270 steps separate the initial position
of the agent and the goal Under an optimal policy. Episodes are terminated either when the agent finds
the goal or if the episode exceeds 2100 timesteps. Importantly, while other algorithms (RaileanU
& Rocktaschel, 2019; Campero et al., 2021) benefit from coUnt-based exploration, this stUdy has
been condUcted with oUr method not benefiting from episodic coUnt whatsoever. ResUlts in Table 1
indicate that AGAC clearly oUtperforms other methods in sample-efficiency. Only the methods ICM
and RIDE succeed in matching the score of AGAC, and with about twice as much transitions (〜3M
vs. 6M). Interestingly, AMIGo performs similarly to CoUnt and RND. We find this resUlt sUrprising
because AMIGo has proven to perform well in the MiniGrid environments. Nevertheless, it appears
that concurrent works to ours experienced similar issues with the accompanying implementation1.
The results of AGAC support the capabilities of the adversarial bonus and show that it can, on its own,
achieve significant gains in performance. However, the VizDoom task is not procedurally-generated;
hence we have not evaluated the generalization to new states yet. In the following section, we use
MiniGrid to investigate this.
5.2	Hard-Exploration Tasks with Partially-Observable Environments
We now evaluate our method on multiple hard-exploration procedurally-generated tasks from Mini-
Grid. Details about MiniGrid can be found in Appendix C.1. Fig. 2 indicates that AGAC significantly
outperforms other methods on these tasks in sample-efficiency and performance. AGAC also outper-
forms the current state-of-the-art method, AMIGo, despite the fact that it uses the fully-observable
version of MiniGrid. Note that we find the same poor performance results when training AMIGo in
MiniGrid, similar to Vizdoom results. For completeness, we also report in Table 2 of Appendix A.1
the performance results with the scores reported in the original papers Raileanu & Rocktaschel
1AMIGo implementation GitHub Issue.
6
Published as a conference paper at ICLR 2021
I* Jlf . .
Enα>B qctbq><
MiniGrid-KeyCorτidorS4R3
0.0
0.5	1-0	1.5	2.0
Number of Timesteps 1θ8
Enα>B a6ala><
MiniGrid-ObstructedMaze-IQ
MiniGrid-Obstructed Maze-2Q
MiniGrid-KeyCorτidorS5R3
0.5	1-0	1-5	2.0
Numberof Timesteps 1θ8
Figure 2: Performance evaluation of AGAC.
e0∙6
So.4
⅛
e
^0.2
<
0.0
0.0	0.5	1-0	1.5	2.0
Number of Timesteps 1θ8
(2019) and Campero et al. (2021). We draw similar conclusions: AGAC clearly outperforms the
state-of-the-art RIDE, AMIGo, Count, RND and ICM.
In all the considered tasks, the agent must learn to generalize across a very large state space because
the layouts are generated procedurally. We consider three main arguments to explain why our method
is successful: (i) our method makes use of partial observations: in this context, the adversary has a
harder time predicting the actor’s actions; nevertheless, the mistakes of the former benefit the latter in
the form of an exploration bonus, which pushes the agent to explore further in order to deceive the
adversary, (ii) the exploration bonus (i.e. intrinsic reward) does not dissipate compared to most other
methods, as observed in Fig. 9 in Appendix A.4, (iii) our method does not make assumptions about
the environment dynamics (e.g., changes in the environment produced by an action as in Raileanu &
Rocktaschel (2019)) since this can hinder learning When the space of state changes induced by an
action is too large (such as the action of moving a block in ObstructedMaze).
In Appendix A.3, We also include experiments in tWo environments With extremely sparse reWard
signals: KeyCorridorS8R3 and ObstructedMazeFull. Despite the challenge, AGAC still manages to
find reWards and can perform Well by taking advantage of the diversified behaviour induced by our
method. To the best of our knoWledge, no other method ever succeeded to perform Well (> 0 average
return) in those tasks. We think that given more computing time, AGAC’s score could go higher.
5.3	Training Stability
Figure 3: Sensitivity analysis of AGAC in KeyCorridorS4R3.
Here We Want to analyse the stability of the method When changing hyperparameters. The most
important parameters in AGAC are c, the coefficient for the adversarial bonus, and the learning rates
ratio V = η2. We choose KeyCorridorS4R3 as the evaluation task because among all the tasks
considered, its difficulty is at a medium level. Fig. 3 shoWs the learning curves. For readability, We
plot the average return only; the standard deviation is sensibly the same for all curves. We observe
that deviating from the hyperparameter values found using grid search results in a sloWer training.
Moreover, although reasonable, c appears to have more sensitivity than ν .
7
Published as a conference paper at ICLR 2021
RND	Count	Random	RIDE	AGAC
OEEEE
EBEEE
Figure 5: State visitation heatmaps for RND, Count, a random uniform policy, RIDE, and AGAC
trained in a singleton environment (top row) and procedurally-generated environments (bottom row)
without extrinsic reward for 10M timesteps in the MUltiRoomN10S6 task.
5.4	Exploration in Reward-free Environment
To better understand the effectiveness of our method
and inspect how the agent collects rewards that would
not otherwise be achievable by simple exploration
heuristics or other methods, we analyze the perfor-
mance of AGAC in another (procedurally-generated)
challenging environment, MultiRoomN10S6, when
there is no reward signal, i.e. no extrinsic reward.
Beyond the good performance of our method when
extrinsic rewards are given to the agent, Fig. 4 in-
dicates that the exploration induced by our method
makes the agent succeed in a significant proportion
of the episodes: in the configuration “NoExtrinsi-
cReward” the reward signal is not given (the goal is
MiniGrid-MuItiRoom-NIO-Se
Figure 4: Average return on N10S6 with and
without extrinsic reward.
invisible to the agent) and the performance of AGAC stabilizes around an average return of 〜0.15.
Since the return of an episode is either 0 or 1 (depending on whether the agent reached the goal
state or not), and because this value is aggregated across several episodes, the results indicate that
reward-free AGAC succeeds in 〜15% of the tasks. Comparatively, random agents have a zero average
return. This poor performance is in accordance with the results in Raileanu & Rocktaschel (2019) and
reflects the complexity of the task: in order to go from one room to another, an agent must perform a
specific action to open a door and cross it within the time limit of 200 timesteps. In the following, we
visually investigate how different methods explore the environments.
5.5	Visualizing Coverage and Diversity
In this section, we first investigate how different methods explore environments without being guided
by extrinsic rewards (the green goal is invisible to the agent) on both procedurally-generated and
singleton environments. In singleton environments, an agent has to solve the same task in the
same environment/maze in every episode. Fig. 5 shows the state visitation heatmaps (darker areas
correspond to more visits) after a training of 10M timesteps. We observe that most of the methods
explore inefficiently in a singleton environment and that only RIDE succeeds in reaching the fifth
room while AGAC reaches the last (tenth) room. After training the agents in procedurally-generated
environments, the methods explore even less efficiently while AGAC succeeds in exploring all rooms.
8
Published as a conference paper at ICLR 2021
Figure 6: State visitation heatmaps of the last ten episodes of an agent trained in procedurally-
generated environments without extrinsic reward for 10M timesteps in the MultiRoomN10S6 task.
The agent is continuously engaging in new strategies.
We now qualitatively study the diversity of an agent’s behavior when trained with AGAC. Fig. 6
presents the state visitation heatmaps of the last ten episodes for an agent trained in procedurally-
generated environments in the MultiRoomN10S6 task without extrinsic reward. The heatmaps
correspond to the behavior of the resulting policy, which is still learning from the AGAC objective.
Looking at the figure, we can see that the strategies vary at each update with, for example, back-
and-forth and back-to-start behaviors. Although there are no extrinsic reward, the strategies seem
to diversify from one update to the next. Finally, Fig. 7 in Appendix A.2 shows the state visitation
heatmaps in a different configuration: when the agent has been trained on a singleton environment
in the MultiRoomN10S6 task without extrinsic reward. Same as previously, the agent is updated
between each episode. Looking at the figure, we can make essentially the same observations as
previously, with a noteworthy behavior in the fourth heatmap of the bottom row where it appears the
agent went to the fourth room to remain inside it. Those episodes indicate that, although the agent
sees the same environment repeatedly, the successive adversarial updates force it to continuously
adapt its behavior and try new strategies.
6	Discussion
This paper introduced AGAC, a modification to the traditional actor-critic framework: an adversary
network is added as a third protagonist. The mechanics of AGAC have been discussed from a policy
iteration point of view, and we provided theoretical insight into the inner workings of the proposed
algorithm: the adversary forces the agent to remain close to the current policy while moving away
from the previous ones. In a nutshell, the influence of the adversary makes the actor conservatively
diversified.
In the experimental study, we have evaluated the adversarially-based bonus in VizDoom and em-
pirically demonstrated its effectiveness and superiority compared to other relevant methods (some
benefiting from count-based exploration). Then, we have conducted several performance experiments
using AGAC and have shown a significant performance improvement over some of the most popular
exploration methods (RIDE, AMIGo, Count, RND and ICM) on a set of various challenging tasks
from MiniGrid. These procedurally-generated environments have served another purpose which is
to validate the capacity of our method to generalize to unseen scenarios. In addition, the training
stability of our method has been studied, showing a greater but acceptable sensitivity for c, the
adversarial bonus coefficient. Finally, we have investigated the exploration capabilities of AGAC in a
reward-free setting where the agent demonstrated exhaustive exploration through various strategic
choices, confirming that the adversary successfully drives diversity in the behavior of the actor.
9
Published as a conference paper at ICLR 2021
References
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the
impact of entropy on policy optimization. In International Conference on Machine Learning, pp.
151-160, 2019.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Representation Learning, 2017.
Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette.
Learning to understand goal specifications by modelling reward. In International Conference on
Learning Representations, 2019.
Andrew Barto, Richard Sutton, and Charles Anderson. Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):
834-846, 1983.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016a.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016b.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2018.
Andres Campero, Roberta Raileanu, Heinrich Kuttler, Joshua B. Tenenbaum, Tim Rocktaschel, and
Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. International
Conference on Learning Representations, 2021.
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym- minigrid, 2018.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In International Conference on Representation Learning,
2016.
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation
to benchmark reinforcement learning. In International Conference on Machine Learning, pp.
2048-2056. PMLR, 2020.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new
approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron,
Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance
weighted actor-learner architectures. In International Conference on Machine Learning, pp.
1407-1416, 2018.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you
need: Learning skills without a reward function. In International Conference on Learning
Representations, 2018.
Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in
dqn. arXiv preprint arXiv:1810.00123, 2018.
Johan Ferret, Raphael Marinier, Matthieu Geist, and Olivier Pietquin. Self-attentional credit as-
signment for transfer in reinforcement learning. In International Joint Conference on Artificial
Intelligence, pp. 2655-2661, 2020.
10
Published as a conference paper at ICLR 2021
Johan Ferret, Olivier Pietquin, and Matthieu Geist. Self-imitation advantage learning. In International
Conference on Autonomous Agents and Multiagent Systems, 2021.
Yannis Flet-Berliac and Philippe Preux. Only relevant information matters: Filtering out noisy
samples to boost rl. In International Joint Conference on Artificial Intelligence, pp. 2711-2717,
2020.
Yannis Flet-Berliac, Reda Ouhamma, Odalric-Ambrym Maillard, and Philippe Preux. Learning
value functions in deep policy gradients using residual variance. In International Conference on
Learning Representations, 2021.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. In International Conference on Machine Learning, pp. 1515-1528,
2018.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alexander
Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane
Legg. Noisy networks for exploration. In International Conference on Representation Learning,
2018.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In International Conference on Machine Learning, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi
Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning. In
International Conference on Learning Representations, 2018.
Seungyul Han and Youngchul Sung. Diversity actor-critic: Sample-aware entropy regularization for
sample-efficient exploration. arXiv preprint arXiv:2006.01419, 2020.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents, 2015.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in Neural Information Processing Systems, 2019.
Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and
Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural
level generation. In NeurIPS Workshop on Deep Reinforcement Learning, 2018.
MichaI Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech ja´kowski. Vizdoom:
A doom-based ai research platform for visual reinforcement learning. In IEEE Conference on
Computational Intelligence and Games, pp. 1-8. IEEE, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Representation Learning, 2015.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
International Conference on Learning Representations, 2017.
Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique
for generalization in deep reinforcement learning. In International Conference on Learning
Representations, 2020.
11
Published as a conference paper at ICLR 2021
Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International
Conference on Learning Representations, 2016.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. In International Conference on Learning Representa-
tions, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
R6mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062,
2016.
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International
Conference on Machine Learning, 2018.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787,
2017.
David Pfau and Oriol Vinyals. Connecting generative adversarial networks and actor-critic methods.
arXiv preprint arXiv:1610.01945, 2016.
Martin Puterman. Markov Decision Processes. Wiley, 1994. ISBN 978-0471727828.
Roberta Raileanu and Tim Rocktaschel. Ride: Rewarding impact-driven exploration for procedurally-
generated environments. In International Conference on Learning Representations, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In International Conference on
Learning Representations, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning, 2014.
Xingyou Song, Yiding Jiang, Yilun Du, and Behnam Neyshabur. Observational overfitting in
reinforcement learning. In International Conference on Learning Representations, 2020.
Richard Stuart Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University
of Massachusetts Amherst, 1984.
Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R6mi Munos, and Matthieu Geist.
Leverage the average: an analysis of regularization in rl. In Advances in Neural Information
Processing Systems, 2020a.
Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Munchausen reinforcement learning. In
Advances in Neural Information Processing Systems, 2020b.
Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg,
and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning.
Nature, 575, 11 2019.
12
Published as a conference paper at ICLR 2021
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. In International Conference
on Learning Representations, 2017.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
Neural Information Processing Systems,pp. 5279-5288, 2017.
Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in
continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018a.
Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep
reinforcement learning. arXiv preprint arXiv:1804.06893, 2018b.
13
Published as a conference paper at ICLR 2021
A Additional Experiments
A.1 MiniGrid Performance
In this section, we report the final performance of all methods considered in the MiniGrid experiments
of Fig. 2 With the scores reported in Raileanu & Rocktaschel (2019) and CamPero et al. (2021). All
methods have a budget of 200M frames.
Table 2: Final average performance of all methods on several MiniGrid environments.
Task	KC-S4R3	KC-S5R3	MR-N10S10	OM-2Dlhb	OM-1Q	OM-2Q
AGAC	0.95	0.93	052	0.64	0.78	0.63
RIDE	0.19	0.	0.40	0.	0.	0.
AMIGo	0.54	0.	0.	0.20	0.	0.
RND	0.	0.	0.	0.03	0.	0.
Count	0.	0.	0.	0.	0.	0.
ICM	0.	0.	0.	0.	0.	0.
A.2 State Visitation Heatmaps in Singleton Environment with No Extrinsic
Reward
In this section, We provide additional state visitation heatmaps. The agent has been trained on
a singleton environment from the MultiRoomN10S6 task Without extrinsic reWard. The last ten
episodes of the training suggest that although the agent experiences the same maze over and over
again, the updates force it to change behavior and try neW strategies.
OEEEE
Figure 7: State visitation heatmaps of the last ten episodes of an agent trained in a singleton
environment With no extrinsic reWard 10M timesteps in the MultiRoomN10S6 task. The agent is
continuously engaging into neW strategies.
A.3 (Extremely) Hard-Exploration Tasks with Partially-Observable
Environments
In this section, We include additional experiments on one of the hardest tasks available in MiniGrid.
The first is KeyCorridorS8R3, Where the size of the rooms has been increased. In it, the agent
has to pick up an object Which is behind a locked door: the key is hidden in another room and
the agent has to explore the environment to find it. The second, ObstructedMazeFull, is similar to
ObstructedMaze4Q, Where the agent has to pick up a box Which is placed in one of the four corners
of a 3x3 maze: the doors are locked, the keys are hidden in boxes and the doors are obstructed by
balls. In those difficult tasks, only our method succeeds in exploring Well enough to find reWards.
14
Published as a conference paper at ICLR 2021
Figure 8: Performance evaluation of AGAC compared to RIDE, AMIGo, Count, RND and ICM on
extremely hard-exploration problems.
A.4 Mean Intrinsic Reward
In this section, we report the mean intrinsic reward computed for an agent trained in Multi-
RoomN12S10 to conveniently compare our results with that of Raileanu & Rocktaschel (2019).
We observe in Fig. 9 that the intrinsic reward is consistently larger for our method and that, contrary
to other methods, does not converge to low values. Please note that, in all considered experiments,
the adversarial bonus coefficient c in Eq. 2 and 3 is linearly annealed throughout the training since it
is mainly useful at the beginning of learning when the rewards have not yet been met. In the long run,
this coefficient may prevent the agent from solving the task by forcing it to always favour exploration
over exploitation.
MiniGrid-MultiRoom-N12-S10
0.0	0.2	0.4	0.6	0.8	1-0
Number of Timesteps	1eβ
Figure 9: Average intrinsic reward for different methods trained in MultiRoomN12S10.
B Illustration of AGAC
Figure 10: A simple schematic illustration of AGAC. Left: the adversary minimizes the KL-divergence
with respect to the action probability distribution of the actor. Right: the actor receives a bonus when
counteracting the predictions of the adversary.
15
Published as a conference paper at ICLR 2021
C Experimental Details and Hyperparameters
C.1 MiniGrid setup
Here, we describe in more details the experimental setup we used in our MiniGrid experiments.
There are several different MiniGrid scenarios that we consider in this paper. MultiRoom corresponds
to a set of navigation tasks, where the goal is to go from a starting state to a goal state. The notation
MultiRoom-N2S4 means that there are 2 rooms in total, and that each room has a maximal side
of 4. In order to go from one room to another, the agent must perform a specific action to open
a door. Episodes are terminated with zero reward after a maximum of 20 × N steps with N the
number of rooms. In KeyCorridor, the agent also has to pick up a key, since the goal state is behind
a door that only lets it in with the key. The notation KeyCorridor-S3R4 means that there are 4 side
corridors, leading to rooms that have a maximal side of 3. The maximum number of steps is 270. In
ObstructedMaze, keys are hidden in boxes, and doors are obstructed by balls the agent has to get out
of its way. The notation ObstructedMaze-1Dl means that there are two connected rooms of maximal
side 6 and 1 door (versus a 3x3 matrix and 2 doors if the leading characters are 2D), adding h as a
suffix places keys in boxes, and adding b as a suffix adds balls in front of doors. Using Q as a suffix
is equivalent to using lhb (that is, both hiding keys and placing balls to be moved). The maximum
number of steps is 576. ObstructedMazeFull is the hardest configuration for this scenario, since it has
the maximal number of keys, balls to move, and doors possible.
In each scenario, the agent has access to a partial view of the environment, a 7x7 square that includes
itself and points in the direction of its previous movement.
C.2 Hyperparameters
In all experiments, we train six different instances of our algorithm with different random seeds. In
Table 3, we report the list of hyperparameters.
Table 3: Hyperparameters used in AGAC.
Parameter
Value
Horizon T
Nb. epochs
Nb. minibatches
Nb. frames stacked
Nonlinearity
Discount γ
GAE parameter λ
PPO clipping parameter
βV
c anneal schedule
βadv
Adam stepsize η1
Adam stepsize η2
2048
ELU (Clevert et al., 2016)
0.99
0.95
0.2
0.5
4∙10-4
linear
4∙10-5
3∙10-4
9∙10-5
(4 ∙ 10-5 in VizDoom)
0.3 ∙ηι
4
8
4
c
16
Published as a conference paper at ICLR 2021
D Implementation Details
In Fig. 11 is depicted the architecture of our method.
	 ACtOr E	Adversary
		
		
Fully-Connected
Hidden size 512
Conv2
32-f liters
3x3
Stride 2
Conv2
32-filters
3x3
Stride 2
Conv3
32-filters
3x3
Stride 2
Conv1
32-filters
3x3
Conv1
32-filters
3x3
Conv3
32-filters
3x3
Stride 2
Fully-Connected
Hidden size 512
Figure 11: Artificial neural architecture of the critic, the actor and the adversary.
E Proof of Section 4.1 results
In this section, we provide a short proof for the result of the optimization problem in Section 4.1. We
recall the result here:
∏k+ι = arg max JPI (π) Y
π
with the objective function:
JPI(∏) = EsEa〜∏(∙∣s)[Q∏k(s,a) + C (log∏k(a|s) — log∏adv(α∣s)) — αlogπ(a∣s)].
Proof. We first consider a simpler optimization problem: arg maxπ hπ, Qπki + αH(π), whose
solution is known (Vieillard et al., 2020a, Appendix A). The expression for the maximizer is the
α-scaled softmax:
π* .	exp( Qnk)
hi, exp(Qnk)i.
We now turn towards the optimization problem of interest, which we can rewrite as:
arg maxhπ, Qπk + c (log πk — log πadv)i + αH(π).
π
17
Published as a conference paper at ICLR 2021
By the simple change of variable Qπk = Qπk + c (log πk - logπadv), we can reuse the previous
solution (replacing Qπk by Qπk). With the simplification:
C
ex Qnk + C (log ∏k - log ∏adv) = ∏ nk_∖ a ex Qk
α	∖∏adv)	α ,
We obtain the result and conclude the proof.	□
18