Published as a conference paper at ICLR 2021
Multi-Level Local SGD: Distributed SGD for
Heterogeneous Hierarchical Networks
Timothy Castiglia, Anirban Das, and Stacy Patterson*
Ab stract
We propose Multi-Level Local SGD, a distributed stochastic gradient method for
learning a smooth, non-convex objective in a multi-level communication network
with heterogeneous workers. Our network model consists of a set of disjoint sub-
networks, with a single hub and multiple workers; further, workers may have dif-
ferent operating rates. The hubs exchange information with one another via a
connected, but not necessarily complete, communication network. In our algo-
rithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke
paradigm, and the hubs periodically average their models with neighboring hubs.
We first provide a unified mathematical framework that describes the Multi-Level
Local SGD algorithm. We then present a theoretical analysis of the algorithm;
our analysis shows the dependence of the convergence error on the worker node
heterogeneity, hub network topology, and the number of local, sub-network, and
global iterations. We illustrate the effectiveness of our algorithm in a multi-level
network with slow workers via simulation-based experiments.
1	Introduction
Stochastic Gradient Descent (SGD) is a key algorithm in modern Machine Learning and optimiza-
tion (Amari, 1993). To support distributed data as well as reduce training time, Zinkevich et al.
(2010) introduced a distributed form of SGD. Traditionally, distributed SGD is run within a hub-
and-spoke network model: a central parameter server (hub) coordinates with worker nodes. At each
iteration, the hub sends a model to the workers. The workers each train on their local data, taking
a gradient step, then return their locally trained model to the hub to be averaged. Distributed SGD
can be an efficient training mechanism when message latency is low between the hub and workers,
allowing gradient updates to be transmitted quickly at each iteration. However, as noted in Moritz
et al. (2016), message transmission latency is often high in distributed settings, which causes a large
increase in overall training time. A practical way to reduce this communication overhead is to allow
the workers to take multiple local gradient steps before communicating their local models to the hub.
This form of distributed SGD is referred to as Local SGD (Lin et al., 2018; Stich, 2019). There is
a large body of work that analyzes the convergence of Local SGD and the benefits of multiple local
training rounds (McMahan et al., 2017; Wang & Joshi, 2018; Li et al., 2019).
Local SGD is not applicable to all scenarios. Workers may be heterogeneous in terms of their com-
puting capabilities, and thus the time required for local training is not uniform. For this reason, it can
be either costly or impossible for workers to train in a fully synchronous manner, as stragglers may
hold up global computation. However, the vast majority of previous work uses a synchronous model,
where all clients train for the same number of rounds before sending updates to the hub (Dean et al.,
2012; Ho et al., 2013; Cipar et al., 2013). Further, most works assume a hub-and-spoke model, but
this does not capture many real world settings. For example, devices in an ad-hoc network may not
all be able to communicate to a central hub in a single hop due to network or communication range
limitations. In such settings, a multi-level communication network model may be beneficial. In fly-
ing ad-hoc networks (FANETs), a network architecture has been proposed to improve scalability by
partitioning the UAVs into mission areas (Bekmezci et al., 2013). Here, clusters of UAVs have their
own clusterheads, or hubs, and these hubs communicate through an upper level network, e.g., via
satellite. Multi-level networks have also been utilized in Fog and Edge computing, a paradigm de-
*T. Castiglia, A. Das, and S. Patterson are With the Department of Computer Science, Rensselaer PolyteCh-
nic Institute, 110 8th St, Troy, NY 12180, castit@rpi.edu, dasa2@rpi.edu, sep@cs.rpi.edu.
1
Published as a conference paper at ICLR 2021
signed to improve data aggregation and analysis in wireless sensor networks, autonomous vehicles,
power systems, and more (Bonomi et al., 2012; Laboratory, 2017; Satyanarayanan, 2017).
Motivated by these observations, we propose Multi-Level Local SGD (MLL-SGD), a distributed
learning algorithm for heterogeneous multi-level networks. Specifically, we consider a two-level
network structure. The lower level consists of a disjoint set of hub-and-spoke sub-networks, each
with a single hub server and a set of workers. The upper level network consists of a connected,
but not necessarily complete, hub network by which the hubs communicate. For example, in a Fog
Computing application, the sub-network workers may be edge devices connected to their local data
center, and the data centers act as hubs communicating over a decentralized network. Each sub-
network runs one or more Local SGD rounds, in which its workers train for a local training period,
followed by model averaging at the sub-network’s hub. Periodically, the hubs average their models
with neighbors in the hub network. We model heterogeneous workers using a stochastic approach;
each worker executes a local training iteration in each time step with a probability proportional to
its computational resources. Thus, different workers may take different numbers of gradient steps
within each local training period. Note since MLL-SGD averages every local training period, regard-
less of how many gradient steps each worker takes, slow workers do not slow algorithm execution.
We prove the convergence of MLL-SGD for smooth and potentially non-convex loss functions. We
assume data is distributed in an IID manner to all workers. Further, we analyze the relationship
between the convergence error and algorithm parameters and find that, for a fixed step size, the
error is quadratic in the number of local training iterations and the number of sub-network training
iterations, and linear in the average worker operating rate. Our algorithm and analysis are general
enough to encompass several variations of SGD as special cases, including classical SGD (Amari,
1993), SGD with weighted workers (McMahan et al., 2017), and Decentralized Local SGD with an
arbitrary hub communication network (Wang & Joshi, 2018). Our work provides novel analysis of
a distributed learning algorithm in a multi-level network model with heterogeneous workers.
The specific contributions of this paper are as follows. 1) We formalize the multi-level network
model with heterogeneous workers, and we define the MLL-SGD algorithm for training models
in such a network. 2) We provide theoretical analysis of the convergence guarantees of MLL-
SGD with heterogeneous workers. 3) We present an experimental evaluation that highlights our
theoretical convergence guarantees. The experiments show that in multi-level networks, MLL-SGD
achieves a marked improvement in convergence rate over algorithms that do not exploit the network
hierarchy. Further, when workers have heterogeneous operating rates, MLL-SGD converges more
quickly than algorithms that require all workers to execute the same number of training steps in each
local training period.
The rest of the paper is structured as follows. In Section 2, we discuss related work. Section 3
introduces the system model and problem formulation. We describe MLL-SGD in Section 4, and
we present our main theoretical results in Section 5. Proofs of these results are deferred to the
appendix. We provide experimental results in Section 6. Finally, we conclude in Section 7.
2	Related Work
Distributed SGD is a well studied subject in Machine Learning. Zinkevich et al. (2010) introduced
parallel SGD in a hub-and-spoke model. Variations on Local SGD in the hub-and-spoke model
have been studied in several works (Moritz et al., 2016; Zhang et al., 2016; McMahan et al., 2017).
Many works have provided convergence bounds of SGD within this model (Wang et al., 2019b; Li
et al., 2019). There is also a large body of work on decentralized approaches for optimization using
gradient based methods, dual averaging, and deep learning (Tsitsiklis et al., 1986; Jin et al., 2016;
Wang et al., 2019a). These previous works, however, do not address a multi-level network structure.
In practice, workers may be heterogeneous in nature, which means that they may execute training
iterations at different rates. Lian et al. (2017) addressed this heterogeneity by defining a gossip-
based asynchronous SGD algorithm. In Stich (2019), workers are modeled to take gradient steps at
an arbitrary subset of all iterations. However, neither of these works address a multi-level network
model. Grouping-SGD (Jiang et al., 2019) considers a scenario where workers can be clustered into
groups, for example, based on their operating rates. Workers within a group train in a synchronous
manner, while the training across different groups may be asynchronous. The system model differs
2
Published as a conference paper at ICLR 2021
significantly from that in MLL-SGD in that as the model parameters are partitioned vertically across
multiple hubs, and workers communicate with every hub.
Several recent works analyze Hierarchical Local SGD (HL-SGD), an algorithm for training a model
in a hierarchical network. Different from MLL-SGD, HL-SGD assumes the hub network topology is
a hub-and-spoke and also that workers are homogeneous. Zhou & Cong (2019) and Liu et al. (2020)
analyze the convergence error of HL-SGD, while Abad et al. (2020) analyzes convergence time.
Unlike HL-SGD, MLL-SGD accounts for an arbitrary hub communication graph, and MLL-SGD
algorithm execution does not slow down in the presence of heterogeneous worker operating rates.
Several other works seek to encapsulate many variations of SGD under a single framework.
Koloskova et al. (2020) created a generalized model that considers a gossip-based decentralized
SGD algorithm where the communication network is time-varying. However, this work does not
account for a multi-level network model nor worker heterogeneity. Wang et al. introduced the Co-
operative SGD framework (Wang & Joshi, 2018), a model that includes communication reduction
through local SGD steps and decentralized mixing between homogeneous workers. Cooperative
SGD also allows for auxiliary variables. These auxiliary variables can be used to model SGD in a
multi-level network, but only when sub-network averaging is immediately followed by hubs averag-
ing with their neighbors in the hub network. Our model is more general; it considers heterogeneous
workers and it allows for an arbitrary number of averaging rounds within each sub-network be-
tween averaging rounds across sub-networks, which is more practical in multi-level networks where
inter-hub communication is slow or costly.
3	System Model and Problem Formulation
In this section, we introduce our system model, the objective function that we seek to minimize, and
the assumptions we make about the function.
We consider a set of D sub-networks D = {1, . . . , D}. Each sub-network d ∈ D has a single hub
and a set of workers M(d), with | M(d) | = N(d) . Workers in M(d) only communicate with their
own hub and not with any other workers or hubs. We define the set of all workers in the system
as M = SdD=1 M(d). Let | M | = N. Each worker i holds a set S(i) of local training data. Let
S = SiN=1 S(i). The set of all D hubs is denoted C. The hubs communicate with one another via an
undirected, connected communication graph G = (C, E). Let Nd = {j | ed,j ∈ E} denote the set
of neighbors of the hub in sub-network d in the hub graph G.
Let the model parameters be denoted by x ∈ Rn . Our goal is to find an x that minimizes the
following objective function over the training set:
F(X) = ∣S∣ X f(x; S)	⑴
|S| s∈S
where f (∙) is the loss function. The workers collaboratively minimize this loss function, in part by
executing local iterations of SGD over their training sets. For each executed local iteration, a worker
samples a mini-batch of data uniformly at random from its local data. Let ξ be a randomly sampled
mini-batch of data and let g(x; ξ)= 6 Ps∈ξ Nf (x; S) be the mini-batch gradient. For simplicity,
we use g(x) instead of g(x; ξ) from here on.
Assumption 1. The objective function and the mini-batch gradients satisfy the following:
1a The objective function F : Rn → R is continuously differentiable, and the gradient is
Lipschitz with constant L > 0, i.e., kNF (x) - NF (y)k2 ≤ Lkx - yk2 for all x, y ∈ Rn.
1b The function F is lower bounded, i.e., F(x) ≥ Finf > -∞ for all x ∈ Rn.
1c The mini-batch gradients are unbiased, i.e., Eξ∣χ[g(x)] = VF(x) forall X ∈ Rn.
1d There exist scalars β ≥ 0 and σ ≥ 0 such that Eξ∣χkg(x)-VF (x)k2 ≤ β∣∣VF (x)∣∣2 +σ2
for all X ∈ Rn.
Assumption 1a requires that the gradients do not change too rapidly, and Assumption 1b requires
that our objective function is lower bounded by some Finf. Assumptions 1c and 1d assume that
3
Published as a conference paper at ICLR 2021
Algorithm 1 Multi-Level Local SGD
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Initialize: y1(d) for hubs d = 1, . . . , D
for k = 1, . . . , K do
parallel for d ∈ D do
parallel for i ∈ M(d) do
(i)	(d)
Xk J yk
for j = k, . . . , k + τ - 1 do
(i)	(i)	(i)
xk+1 J xk - ηgk
end for
end parallel for
z(d) J P	v(i)x(i)
z J	i∈M(d) v xk+1
if k mod q ∙ T = 0 then
yk(d+)1 J Pj∈N(d) Hj,dz(j)
else
yk(d+)1 J z(d)
end if
end parallel for
end for
. Workers receive updated model from hub
. Local iteration (probabilistic)
. Hub d computes average of its workers’ models
. Hub d averages its model with neighboring hubs
the local data at each worker can be used as an unbiased estimate for the full dataset with the same
bounded variance. These assumptions are common in convergence analysis of SGD algorithms (e.g.,
Bottou et al. (2018)).
4 Algorithm
We now present our Multi-Level Local SGD (MLL-SGD) algorithm. The pseudocode is shown in
Algorithm 1. Each sub-network trains in parallel and, periodically, the hubs average their models
with neighboring hubs. The steps corresponding to Local SGD are shown in lines 5-10. Each hub
and worker stores a copy of the model. For worker i ∈ M(d), we denote its copy of the local model
by x(i). We denote the model at hub d by y(d). The hub first sends its model to its workers, and
the workers update their local models to match their hub’s model. Workers then execute multiple
local training iterations, shown in line 7, to refine their local models independently. To represent the
different rates of computation at each worker, we use a probabilistic approach. We assume that, in
expectation, a worker i execute τ(i) local iterations for every τ time steps (τ (i) ≤ τ). We thus define
(i)
the N-Vector P where each entry Pi = T^ is the probability with which worker i executes a local
gradient step in each iteration k. Worker i updates its local model at iteration k as follows:
x(ki+) 1 = x(ki)
(2)
—
where η is the step size and gk(i) is a random variable such that
gk(i) =
g(x(ki)) w/ probability pi
w/ probability 1 - pi .
(3)
After τ time steps, the hub updates its model based on the models of its workers (line 10). For
each worker i, we assign a positive weight w(i) . Let v(i) be the weight for worker i normalized
within its sub-network: v(i) = P——Wi--------(^, where d(i) denotes the sub-network of worker i.
j∈M(d(i)) w
Each hub’s updates its model to be a weighted average over the workers’ models in its sub-network:
y(d) = Pi∈M(d) v(i)x(i). Weights may be assigned for different reasons. If all worker gradients
are treated equally, then w(i) = 1 and v(i) = N儿). We may also weight a worker,s gradient
proportional to its local dataset size, in which case w(i) = | S(i) | and v(i)
latter approach is used in Federated Averaging (McMahan et al., 2017).
|S⑸|
Pr∈M(d(i)) IS(r) I
. The
0
After q iterations ofLocal SGD in each sub-network (q ∙ T time steps), the hubs average their models
with their neighbors in the hub communication network (line 12). The weight assigned to each hub’s
4
Published as a conference paper at ICLR 2021
model is defined by a D × D matrix H so that:
y(d) = X Hj,dy(j).	(4)
j∈N(d)
Define the total weight in the network to be wtot = Pi∈M w(i) . Let b be a D-vector with each
component d given by bd = (Pi∈M(d) w(i))/wtot. We assume H meets the following requirements.
Assumption 2. The matrix H satisfies the following:
2a If (i, j) ∈ E, then Hi,j > 0. Otherwise, Hi,j = 0.
2b H is column stochastic, i.e., PiD=1 Hi,j = 1.
2c For all i, j ∈ D, we have bi Hi,j = bj Hj,i.
Assumption 2 implies that H has one as a simple eigenvalue, with corresponding right eigenvector
b and left eigenvector 1D. Further, all of its other eigenvalues have magnitude strictly less than 1
(since G is connected) (Rotaru & Nageli, 2004). By defining H in this way, We ensure that the
contributions from the workers’ gradients in each hub are incorporated in proportion to the workers’
weights. This weighted averaging approach allows us to naturally extend Federated Averaging to
the multi-level network model.
5 Analysis
We note that hubs are essentially stateless in MLL-SGD, as the hub models are copied to all workers
after each sub-network or hub averaging. Thus, our analysis focuses on how worker models evolve.
We first present an equivalent formulation of the MLL-SGD algorithm in terms of the evolution of
the worker models. We then present our main result on the convergence of MLL-SGD.
The system behavior can be summarized by the following update rule for worker models:
Xk+1 = (Xk-ηGk)Tk	(5)
where n × N matrix Xk = [x(k1) , . . . , x(kN)], n × N matrix Gk = [gk(1), . . . , gk(N)], and N × N matrix
Tk is a time-varying operator that captures the three stages in MLL-SGD: local iterations, hub-and-
spoke averaging within each sub-network, and averaging across the hub network. We define Tk as
follows:
(Z if k mod qτ = 0
Tk = V ifk mod τ = 0andk mod qτ 6= 0	(6)
11 otherwise.
For local iterations, Tk = I, as there are no interactions between workers or hubs. For sub-network
averaging, V is an N × N block diagonal matrix, with each block V(d) corresponding to a single
sub-network d. The matrix V(d) is an N(d) × N(d) matrix where each entry is Vi(,dj) = v(i) . Finally,
we define an N × N matrix Z that captures the sub-network averaging and hub network averaging
in one operation that involves all workers. The components of Z are given by
Zi,j = Hd(i),d(j)v(i).	(7)
Let a be an N-vector with each component a% = W^ representing the weight of worker i, nor-
malized over all worker weights. We observe that Z and V satisfy the following: each have a right
eigenvector of a and left eigenvector of 1TN with eigenvalue 1 and all other eigenvalues have magni-
tude strictly less than 1. The proof of these properties can be found in the appendix. These properties
are necessary (but not sufficient) to ensure that the worker models converge to a consensus model,
where each worker’s updates have been incorporated according to the worker’s weight.
As is common, we study an averaged model over all workers in the system (Yuan et al., 2016; Wang
& Joshi, 2018). Specifically, we define a weighted average model:
uk = Xk a.	(8)
5
Published as a conference paper at ICLR 2021
We identify the recurrence relation of uk. If we multiply a on both sides of (5):
Xk+1a= (Xk-ηGk)Tka	(9)
uk+1 = uk -ηGka	(10)
N
uk+1 = uk - η	aigk(i)	(11)
i=1
where (10) follows from a being a right eigenvector of V and Z with eigenvalue 1. We note that
uk is updated via a stochastic gradient descent step using a weighted average of several mini-batch
gradients. Since F(∙) may be non-convex, SGD may converge to a local minimum or saddle point.
Thus, we study the gradients of uk as k increases.
We next provide the main theoretical result of the paper.
Theorem 1. Under Assumptions 1 and 2, if η satisfies the following for all i ∈ M:
(4Pi- P2 - 2) ≥ ηL (aiPi(β + 1) - aiP2 + P2) + 8L2η2q2τ2Γ	(12)
where Γ = ɪ-^ + i-—^ + .二产 and Z = max{∣λ2(H)∣, |1n(H) ∣}, then the expected square norm
of the average model gradient, averaged over K iterations, is bounded as follows:
K
1K
E IK EkVF(Uk)k2	≤
2 (F(XI) - Finf D
k=1
+ 4L2η2σ2q3τ3 (-
qτ
+ 4L2η2σ2 (I-I)
ηK
1
K
τ2
N
+ σ2ηL X ai2Pi
i=1
(q-1)(2q + 1) + (τ - 1)(2τ +1)
(13)
—
F+A+o⅛)P
6
6
P
K--→ σ2ηL X a2Pi +4L2η2σ2q2τ2 (三 + 占 + ɪ) P
+ 4L2η2σ2 (I-I)卜2
(q-1)(2q + 1) + (τ - 1)(2τ +1)
(14)
6
6
P
where P = PiN=1 aiPi.
The proof of Theorem 1 is provided in the appendix. The first term in (13) is the same as in central-
ized SGD (Bottou et al., 2018). As K → ∞, this term goes to zero. The second term is similar to
centralized SGD as well. If the stochastic gradients have high variance, then the convergence error
will be larger. This term is also related to the convergence error in distributed SGD (Bottou et al.,
2018), which is equivalent to MLL-SGD when there is one sub-network, q = τ = 1, ai = 1/N,
and Pi = 1 for all i. MLL-SGD has a dependence on the probabilities of gradient steps and worker
weights, replacing the N in the equivalent term in distributed SGD.
The third and fourth terms in (13) are additive errors that depend on the topology of the hub network.
The value of ζ is given by the second largest eigenvalue of H, by magnitude, which is an indication
of the sparsity of the hub network. When worker weights are uniform, a fully connected hub graph
G will have ζ = 0, while a sparse G will typically have ζ close to 1. It is interesting to note that ζ
only depends on H, and not Z or V, meaning the convergence error does not depend on how worker
weights are distributed within sub-networks.
We also note the third and fourth terms depend on P, the weighted average probability of the work-
ers. The convergence error increases as the average worker operating rate increases. This relation
is expected as more local iterations will increase convergence error (Wang & Joshi, 2018). It is
interesting to note that the convergence error does not depend on the distribution of P, meaning that
a skewed and uniform distribution with the same average probability would have the same conver-
gence error. We observe that the condition on η in (12) cannot always be satisfied given certain
probabilities. Specifically, when there exists a Pi ≤ 2 - √2 ≈ 0.59, then the left-hand side will
be non-positive, and the inequality can no longer be satisfied. Although this may be a conserva-
tive bound, intuitively, when Pi ’s are below this threshold, the algorithm may not make sufficient
progress in each time step to guarantee convergence.
6
Published as a conference paper at ICLR 2021
(a) Training loss of CNN trained on EMNIST.
(b) Test accuracy of CNN trained on EMNIST.
(c) Training loss of ResNet-18 trained on CIFAR-10. (d) Test accuracy of ResNet-18 trained on CIFAR-10.
Figure 1: Effect of a hierarchy with different values of τ and q .
The third and fourth terms also grow with q and τ , the number of local iterations per hub network
averaging and sub-network averaging steps, respectively. The longer workers train locally without
reconciling their models, the more their models will diverge, leading to larger convergence error. We
can see that T plays a slightly larger role in convergence error than q. For a given q ∙ T, meaning a
given number of time steps between hub averaging steps, a larger τ leads to higher convergence error
than a larger q would. Thus, there is a slight penalty to performing more local iterations between
sub-network averaging steps. We explore this more in Section 6.
We note that when setting ai = 1/N and pi = 1 for all workers i, and setting q = 1, MLL-SGD
reduces to Cooperative SGD. However, the bound in Theorem 1 differs from that of Cooperative
SGD. Specifically, Theorem 1 has error terms dependent on T2 as opposed to T in Cooperative
SGD. This discrepancy is due to accommodating all possible values of pi . More details can be
found in Appendix C.4.
In the following corollary, We analyze the convergence rate of Algorithm 1 when η = √~^.
Corollary 1. Let η = L√1κ and let q2T2 ≤ ∖∕K. If qτ < K, then
E K X INF (Uk )k2 ≤ O ( √K ) (F (XI) - Finf ]) + O ( √K )
(15)
Under the conditions given in Corollary 1, MLL-SGD achieves the same asymptotic convergence
rate as Local SGD and HL-SGD.
6 Experiments
In this section, we show the performance of MLL-SGD compared to algorithms that do not account
for hierarchy and heterogeneous worker rates. We also explore the impact of the different algorithm
parameters that show up in Theorem 1.
We use the EMNIST (Cohen et al., 2017) and CIFAR-10 (Krizhevsky et al., 2009) datasets. For
all experiments, we provide results for training a simple Convolutional Neural Network (CNN) on
EMNIST and training ResNet-18 on CIFAR-10. The CNN has two convolutional layers and two
fully connected layers. We train the CNN with a step size of 0.01. For ResNet, we use a standard
approach of changing the step size from 0.1 to 0.01 to 0.001 over the course of training (He et al.,
2016). We conduct experiments using Pytorch 1.4.0 and Python 3.
We compare MLL-SGD with Distributed SGD, Local SGD, and HL-SGD. Distributed SGD is equiv-
alent to MLL-SGD when there is one hub, q = T = 1, and ai = 1/N and pi = 1 for all i, which
means Distributed SGD averages all worker models at every iteration. Thus, we use Distributed
7
Published as a conference paper at ICLR 2021
Figure 2: Effect of worker distribution on CNN
trained on EMNIST.
Figure 3: Effect of worker distribution on ResNet-
18 trained on CIFAR-10.
Figure 4: Effect of heterogeneous operating rates
on CNN trained on EMNIST.
Figure 5: Effect of heterogeneous operating rates
on ResNet-18 trained on CIFAR-10.
SGD as a baseline for convergence error and accuracy in some experiments. Local SGD is equiva-
lent to MLL-SGD when ai = 1/N and pi = 1 for all i, when the hub network is fully connected,
and q = 1. HL-SGD extends Local SGD to allow q > 1. For all experiments, we let τ = 32 for
Local SGD. We let qτ = 32 for all HL-SGD and MLL-SGD variations to be comparable with Local
SGD. In all experiments, we measure training loss and test accuracy of the averaged model uk every
32 iterations.
We first explore the effect of different values of τ and q in MLL-SGD. We configure a multi-level
network with a fully connected hub network and with 10 hubs, each with 10 workers. We use two
configurations for MLL-SGD, one with τ = 8 and q = 4, and one with τ = 4 and q = 8. Distributed
SGD and Local SGD treat the hubs as pass-throughs, and average all workers every iteration and
every τ iterations respectively. Workers are split into five groups of 20 workers each. Each group is
assigned a percentage of the full dataset: 5%, 10%, 20%, 25%, and 40%. Workers within a group
partition the data evenly. The workers weights are assigned based on dataset sizes. In Figures 1a
and 1c we plot the training loss, and in Figures 1b and 1d we plot the test accuracy for the CNN and
ResNet, respectively. We observe that as q increases, while keeping qτ = 32, MLL-SGD improves
and approaches the Distributed SGD baseline. Thus, increasing the number of sub-network training
rounds improves the convergence behavior of MLL-SGD. The benefit is more pronounced in training
ResNet on CIFAR.
We next investigate how the number and sizes of the sub-networks impacts the convergence of MLL-
SGD. From a pool of 100 workers, we distribute them across 5, 10, and 20 sub-networks. The hub
network is a path graph, which yields the largest ζ while keeping the network connected. This
hub network topology the worst-case scenario in terms of the convergence bound. Note that as the
number of hubs increases, the larger ζ becomes. We let ai = 1/N and pi = 1 for all workers i. We
set q = 4 and τ = 8. We also include results using Local SGD with 1 hub and 100 workers. The
results of this experiment are shown in Figures 2 and 3. In the case of the CNN, the difference in
training loss is minimal among the MLL-SGD variations. In the case of ResNet we can see that as
the number of hubs increase, the convergence rate decreases. This is in line with Theorem 1 since an
increased number hubs corresponds with an increased ζ . Interestingly, despite the low hub network
connectivity, MLL-SGD outperforms Local SGD. This shows that MLL-SGD still benefits from a
hierarchy even when hub connectivity is sparse.
Next, we explore the impact of different distributions of worker operating rates. According to The-
orem 1, the average probability across workers plays a role in the error bound. To see if this holds
in practice, we compare four different MLL-SGD setups, all of which includes a complete hub net-
work, 10 hubs, each with 10 workers, ai = 1/N, and an average probability amongst workers of
0.55: (i) all workers with a pi = 0.55 (Fixed); (ii) workers in each sub-network with probability
ranging from 0.1 to 1 at steps of 0.1 (Uniform Distribution); (iii) 90 workers with pi = 0.5 and 10
workers with pi = 1 (Skewed 1); (iv) 90 workers with pi = 0.6 and 10 workers with pi = 0.1
(Skewed 2). We include a case where all workers have pi = 1 as a baseline (Prob=1). In Figures 4
8
Published as a conference paper at ICLR 2021
(a) Training loss of CNN with respect to time slots. (b) Test accuracy of CNN with respect to time slots.
(c) Training loss of ResNet with respect to time slots. (d) Test accuracy of ResNet with respect to time slots.
Figure 6: Comparing convergence time of Local SGD, HL-SGD, and MLL-SGD.
and 5 we can see that in all cases except the baseline, the convergence rate is similar in both models.
This is in line with our theoretical results, since all cases have the same average worker probability.
Finally, we compare the convergence time of MLL-SGD against algorithms that wait for slower
workers: Local SGD and HL-SGD. We simulate real-time with time slots. In every time slot, each
worker will take a gradient step with a probability pi . Note when pi = 1 for a worker i, the number
of gradient steps taken will match the number of time slots T . Otherwise, the number of gradient
steps taken Win be T ∙ Pi in expectation. MLL-SGD Win wait T time slots before averaging worker
models in a sub-network, regardless of the number of gradient steps taken, while Local SGD and
HL-SGD will wait for all workers to take τ gradient steps. This approach allows us to compare the
progress of each algorithm over time. In this experiment, we set pi = 0.9 for 90% of workers and
pi = 0.6 for 10% of the workers. As in the previous experiments, we use a multi-level network with
a fully connected hub network and with 10 hubs, each with 10 workers. We study MLL-SGD with
two parameter settings, τ = 32, q = 1 and τ = 8, q = 4. We also include results for Local-SGD
and HL-SGD. By comparing MLL-SGD with τ = 32, q = 1 with Local-SGD, we can evaluate the
impact of using a local training period based on time rather than a number of worker iterations. By
comparing MLL-SGD with τ = 8, q = 4 with HL-SGD, we can evaluate this impact in a multi-level
network.
In Figures 6a and 6c, we plot the training loss, and in Figures 6b and 6d, we plot the test accuracy for
the CNN and ResNet, respectively. We can see that MLL-SGD with q = 1 converges more quickly,
in both loss and accuracy, than Local SGD, and that MLL-SGD with q = 4 converges more quickly
than HL-SGD. These trends hold in both the CNN and ResNet models. The results show that in this
experimental setup, waiting for slow workers is detrimental to the overall convergence time.
7	Conclusion
We have introduced MLL-SGD, a variation of Distributed SGD in a multi-level network model.
Our algorithm incorporates the heterogeneity of worker devices using a stochastic approach. We
provide theoretical analysis of the algorithm’s convergence, and we show how the convergence
error depends on the average worker rate, the hub network topology, and the number of local, sub-
network averaging, and hub averaging steps. Finally, we provide experimental results that illustrate
the effectiveness of MLL-SGD over Local SGD and HL-SGD. In future work, we plan to analyze
the effects of non-IID data on convergence error.
Acknowledgments
This work is supported by the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part
of the IBM AI Horizons Network (http://ibm.biz/AIHorizons), and by the National Science Foun-
dation under grants CNS 1553340 and CNS 1816307.
9
Published as a conference paper at ICLR 2021
References
M. Abad, E. Ozfatura, D. Gunduz, and O. Ercetin. Hierarchical federated learning across heteroge-
neous cellular networks. In ICASSP 2020,pp. 8866-8870. IEEE, 2020.
S.	Amari. Backpropagation and stochastic gradient descent method. NeurocomPuting, 5(4-5):185-
196, 1993.
I.	Bekmezci, O. Sahingoz, and S. TemeL Flying ad-hoc networks (fanets): A survey. Ad Hoc
Networks, 11(3):1254-1270, 2013.
F. Bonomi, R. Milito, J. Zhu, and S. Addepalli. Fog computing and its role in the internet of things.
In Proceedings of the First Edition of the MCC WorkshoP on Mobile Cloud ComPuting, 2012.
L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y. LeCun, U. Muller, E. Sackinger,
P. Simard, et al. Comparison of classifier methods: a case study in handwritten digit recognition.
In Proceedings of the 12th IAPR International Conference on Pattern Recognition, volume 2, pp.
77-82. IEEE, 1994.
L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam
Review, 60(2):223-311, 2018.
J.	Cipar, Q. Ho, J. Kim, S. Lee, G. Ganger, G. Gibson, K. Keeton, and E. Xing. Solving the straggler
problem with bounded staleness. In 14th WorkshoP on Hot ToPics in OPerating Systems, 2013.
G. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: Extending mnist to handwritten letters.
In 2017 International Joint Conference on Neural Networks, pp. 2921-2926, 2017.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, et al. Large scale distributed deep networks. In Advances in neural information Process-
ing systems, pp. 1223-1231, 2012.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR 2016,
pp. 770-778, 2016.
Q. Ho, J. Cipar, H. Cui, S. Lee, J. Kim, P. Gibbons, G. Gibson, G. Ganger, and E. Xing. More
effective distributed ml via a stale synchronous parallel parameter server. In Advances in neural
information Processing systems, pp. 1223-1231, 2013.
W. Jiang, G. Ye, L. Yang, J. Zhu, Y. Ma, X. Xie, and H. Jin. A novel stochastic gradient descent
algorithm based on grouping over heterogeneous cluster systems for distributed deep learning. In
CCGRID 2019, pp. 391-398. IEEE, 2019.
P. Jin, Q. Yuan, F. Iandola, and K. Keutzer. How to scale distributed deep learning? arXiv PrePrint
arXiv:1611.04581, 2016.
A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Stich. A unified theory of decentralized sgd
with changing topology and local updates. arXiv PrePrint arXiv:2003.10422, 2020.
A.	Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
National Renewable Energy Laboratory. Demonstrating distributed grid-edge control hierarchy.
https://www.nrel.gov/docs/fy17osti/67784.pdf, 2017. Accessed: 2020-07-24.
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang. On the convergence of fedavg on non-iid data.
arXiv PrePrint arXiv:1907.02189, 2019.
X. Lian, W. Zhang, C. Zhang, and J. Liu. Asynchronous decentralized parallel stochastic gradient
descent. arXiv PrePrint arXiv:1710.06952, 2017.
T. Lin, S. Stich, K. Patel, and M. Jaggi. Don’t use large mini-batches, use local sgd. arXiv PrePrint
arXiv:1808.07217, 2018.
L. Liu, J. Zhang, S. Song, and K. Letaief. Client-edge-cloud hierarchical federated learning. In ICC
2020, pp. 1-6. IEEE, 2020.
10
Published as a conference paper at ICLR 2021
B.	McMahan, E. Moore, D. Ramage, S. Hampson, and B. Aguera y Arcas. Communication-efficient
learning of deep networks from decentralized data. In Proc. of the 20th Intl. Conf. on Artificial
Intelligence and Statistics, 2017.
P. Moritz, R. Nishihara, I. Stoica, and M. Jordan. Sparknet: Training deep networks in spark. 4th
International Conference on Learning Representations, 2016.
T. Rotaru and H. Nageli. Dynamic load balancing by diffusion in heterogeneous systems. Journal
ofParallel and Distributed Computing, 64(4):481—497, 2004.
M. Satyanarayanan. The emergence of edge computing. Computer, 50(1):30-39, 2017.
S. Stich. Local SGD converges fast and communicates little. In International Conference on Learn-
ing Representations, 2019.
J. Tsitsiklis, D. Bertsekas, and M. Athans. Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE transactions on automatic control, 31(9):803-812, 1986.
J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.
J. Wang, A. K. Sahu, Z. Yang, G. Joshi, and S. Kar. Matcha: Speeding up decentralized sgd via
matching decomposition sampling. arXiv preprint arXiv:1905.09435, 2019a.
S. Wang, T. Tuor, T. Salonidis, K. Leung, C. Makaya, T. He, and K. Chan. Adaptive federated
learning in resource constrained edge computing systems. IEEE Journal on Selected Areas in
Communications, 2019b.
K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal
on Optimization, 26(3):1835-1854, 2016.
J. Zhang, C. De Sa, I. Mitliagkas, and C. Re. Parallel sgd: When does averaging help? arXiv
preprint arXiv:1606.07365, 2016.
F. Zhou and G. Cong. A distributed hierarchical sgd algorithm with sparse global reduction. arXiv
preprint arXiv:1903.05133, 2019.
M. Zinkevich, M. Weimer, L. Li, and A. Smola. Parallelized stochastic gradient descent. Advances
in Neural Information Processing Systems, 2010.
A	Code Repository
The code used in our experiments can be found at: https://github.com/rpi-nsl/MLL-SGD. This code
simulates a multi-level network with heterogeneous workers, and trains a model using MLL-SGD.
B Additional Experiments
Our experiments in Section 6 explore how changing MLL-SGD parameters affect training on a non-
convex function. In this section, show the results of the same experiments on a convex loss function.
We train a logistic regression model on the MNIST dataset (Bottou et al., 1994). We train a binary
classification model with half the classes being 0 and the other half being 1 and use a step size of
0.2. We run all experiments for 32,000 iterations.
We rerun our first experiment from Figure 1 with logistic regression trained on MNIST. Figures 7a
and 7b show the training loss and test accuracy, respectively. As with the non-convex functions, we
can see that MLL-SGD with larger q approaches the Distributed SGD baseline.
We rerun our second experiment comparing different hub and worker distributions with logistic
regression trained on MNIST. Figure 8 shows the training loss. The three variations of MLL-SGD
do not show much difference in terms of convergence rate, indicating that ζ has little effect in this
case. However, they still outperform Local SGD due to q being larger.
11
Published as a conference paper at ICLR 2021
(a) Training loss of logistic regression trained on
MNIST.
Figure 7: Effect of a hierarchy with different values of τ and q .
(b) Test accuracy of logistic regression trained on
MNIST.
Figure 8: Effect of worker distribution on logistic
regression trained on MNIST.
Figure 9: Effect of heterogeneous operating rates
on logistic regression trained on MNIST.
We rerun our third experiment comparing different worker operating rates distributions with logistic
regression trained on MNIST. Figure 9 shows the training loss. As with the non-convex functions,
all MLL-SGD variations with the same average probability have similar convergence rate.
We rerun our first experiment from Figure 6 with logistic regression trained on MNIST. Figures
10a and 10b show the training loss and test accuracy, respectively. We can see an improvement in
convergence rate of MLL-SGD over both Local SGD and HL-SGD.
C Proof of Theorem 1
For our proof we adopt a similar approach to that in Wang & Joshi (2018). This section is structured
as follows. We first define some notation and make some observations in Section C.1. Our support-
ing lemmas are stated in Section C.2. We close with the full proof of Theorem 1 in Section C.3.
C.1	Preliminaries
For simplicity of notation, Weletk ∙ k denote the l2 vector norm. Let the weighted Frobenius norm
of an N × M matrix X with an N -vector a be defined as follows:
NM
kXk2Fa = Tr((diag(a))1/2XXT (diag(a))1/2) =XXai|xi,j|2.	(16)
i=1 j=1
The matrix operator norm for a square matrix Q is defined as:
kQkop = Jλmax(QT Q).	(17)
12
Published as a conference paper at ICLR 2021
6 4 2 0
8 8 8 8
Sc5 6 6
(b) Test accuracy of logistic regression with re-
spect to time slots.
0.37-
0.36-
产5-
0.34-
0.33-
0.32-
0	10000	20000	30000
Time slots
(a) Training loss of logistic regression with re-
spect to time slots.
Figure 10: Comparing convergence time of Local SGD and MLL-SGD.
We define the set of Bernoulli random variables Θ = {θk1, . . . , θkN}, where
1 with probability pi
0 with probability (1 - pi).
Let Ξk = {ξk(1), ..., ξk(N)} be the set of mini-batches used by the N workers at time step k. Without
loss of generality, we assign a mini-batch to each worker, even if it does not execute a gradient step
in that iteration. An equivalent definition of gk(i) is then
gk(i) = θkig(ξk(i)).	(18)
For simplicity of notation, let Ek be equivalent to Eθk ,ξk IXk.
We note that Assumption 1c implies:
Ek[gk(i)] = piEk[g(x(ki))]	(19)
=Pi VF (Xki)).	(20)
Further, when i 6= j :
Ek [(gk(i))Tgk(j)] = pipjEk [(g(x(ki)))]TEk[g(x(kj))]	(21)
= pipjVF(x(ki) )T VF(x(kj) ).	(22)
We also note that Assumption 1d implies:
Ek gk(i) -VF(x(ki))2 =Ek	gk(i)2+VF(x(ki))2-2(gk(i))TVF(x(ki))	(23)
=Ek gk(i)2 + VF(x(ki))2 -2Ek(gk(i))TVF(x(ki))	(24)
= piEk g(x(ki))2+ VF(x(ki))2 -2piEkg(x(ki))TVF(x(ki))	(25)
= piEk g(x(ki))2 +pi VF(x(ki))2
-2piEkg(x(ki))TVF(x(ki))+(1-pi) VF(x(ki))2 (26)
= piEk g(x(ki) ) -	VF(x(ki))2+(1-pi)	VF(x(ki))2	(27)
≤ piβ VF(x(ki))	2 +piσ2 + (1 -pi) VF(x(ki))2		(28)
= (pi(β-1)+1)	VF(x(ki))2 + piσ2.		(29)
13
Published as a conference paper at ICLR 2021
Finally, we define the weighted average stochastic gradient and the weighted average batch gradient
as:
NN
Gk = Xaigki), Hk = XaiVF(Xki)).
i=1	i=1
C.2 Lemmas and Propositions
Next, we state our supporting lemmas and propositions.
Proposition 1. The matrices Z and V satisfy the following properties:
1.	Z and V each have a right eigenvector of a with eigenvalue 1.
2.	Z and V each have a left eigenvector of 1TN with eigenvalue 1.
3.	All other eigenvalues ofZ and V have magnitude strictly less than 1.
Proof. Assumption 2 indicates that H is a Generalized Diffusion Matrix as defined in Rotaru &
Nageli (2004).
Recall Assumption 2:
Assumption 2. The matrix H satisfies the following:
2a If (i, j) ∈ E, then Hi,j > 0. Otherwise, Hi,j = 0.
2b H is column stochastic, i.e., PiD=1 Hi,j = 1.
2c For all i, j ∈ D, we have bi Hi,j = bj Hj,i.
If we show this implies that Z and V are Generalized Diffusion Matrices with the same properties
to those in Assumption 2 with vector a, then the properties in the proposition are satisfied.
Since H and b are non-negative, then Z is also non-negative. It is also clear that Z is column
stochastic by construction. It is left to prove that:
Zi,j aj = Zj,i ai.	(30)
Applying the definition of Z to the left side, we have:
Zi,j aj = Hd(i),d(j)v(i)aj	(31)
Since we know that H is a Generalized Diffusion Matrix with vector b, we know that:
Hi,jbj = Hj,ibi	(32)
bi
Hij = Hj,i b.	(33)
Plugging this in for Hd(i),d(j), we have:
Zi,j aj	bd(i) (i) 二 Hd(j),d(i) τ-v aj		(34)
	bd(j)		
	r∈M(d(i)) w(r)	wtot	W(i)	Wj)	
	H Hd(j),d(i)	vɔ wtot	r∈M(d(j))	w(r) Pr∈M(d(i)) w(r) wtot	(35) (35)
	w(i)	w(j)		
	- Hd(j),d(i)	vɔ	(r) wtot	r∈M(d(j)) w(r)		(36)
	Hd(j),d(i)v(j)ai		(37)
	Zj,i ai .		(38)
Therefore, Z is a Generalized Diffusion Matrix.
14
Published as a conference paper at ICLR 2021
(40)
(41)
We can show that V is also a Generalized Diffusion Matrix with the vector a. V is constructed to be
non-negative and column stochastic. It is left to prove that
Vi,j aj = Vj,i ai.	(39)
When i, j are outside a block V(d), then Vi,j = Vj,i = 0, so the equation is trivially satisfied. When
within a block, in terms of w, we have:
Vi,j aj = Vj,i ai
w(i *)	w(j)	w(j)	w(i)
Pr∈M(d(i)) w(r) Wtot	Pr∈M(d(j)) w(r) Wtot
Noting that we are within a block, therefore d(i) = d(j), we can see that both sides are equal:
W(i)W(j) = W(j)W(i).	(42)
Therefore, V is a Generalized Diffusion Matrix.
□
Proposition 2. Given a diffusion matrix H with the properties in Assumption 2, ifZ constructed as
follows,
Zi,j = Hd(i),d(j)v(i)	(43)
then the largest eigenvalues of Z are the eigenvalues of H, and zero otherwise.
Proof. In order to prove the relationship of the eigenvalues of Z and H, we prove the following two
points separately:
1.	The rank of Z is the same as H.
2.	All non-zero eigenvalues of H are eigenvalues of Z with the same multiplicity.
For the rank of Z, we take a look at how each column is constructed. Consider column j of Z:
Zj = [H1,d(j)v(1),..., H1,d(j)v(N(1)), H2,d(j)v(N(1)+1),..., HD,d(j)v(N)]T.	(44)
For two columns i and j where d(i) = d(j), these columns are identical. Therefore, the rank of Z
will be, at most, the number of hubs, D. Further, we can see that the elements of a column j in Z
are simply scaled elements of column d(j) in H. So any linearly dependent columns in H will also
be linearly dependent in Z. Therefore, the rank of the two matrices are the same.
For the second point, we show there is a bijective mapping from eigenpairs of H to eigenpairs of Z.
Let (λ, y) be an eigenpair of H (with λ 6= 0), i.e.
Hy = λy.	(45)
Define the N -vector x with components xi = v(i)yd(i). We will show that Z x = λx. Looking at
the i-th entry of the vector Z x, we have
N
(Z x)i =XZi,jxj.	(46)
j=1
Applying the definition of Z and x, we obtain
N1
亿 X)i = E ViyHd(i),d(j)v(j)yd(j)	(47)
j=1 v
1D
=河 EHd(i),ιyl E V⑻	(48)
1D
=河 EHd(i),ιyl.	(49)
15
Published as a conference paper at ICLR 2021
Note that the m-th entry of the vector Hy equals PlD=1 Hm,lyl = λym. Applying this equality, we
obtain
(Z X)i = v(i)λyd(i)	(5O)
= λxi.	(51)
Therefore, for any eigenpair (λ, y) of H, we can find an eigenpair (λ, x) ofZ. It is left to prove that
this mapping is a bijection.
Suppose eigenvalue λ of H has multiplicity k > 1. We consider any two of the k eigenpairs (λ, c)
and (λ, d). Let the corresponding eigenpairs ofZ be (λ, e) and (λ, f). We know that e 6= f because
c and d are unique, and there must exist an index i such that v(i) cd(i) 6= v(i)dd(i). Therefore, the
mapping of eigenpairs of H to eigenpairs of Z is a bijection.	□
Proposition 3. Given definition of Z and V in Proposition 1, it is the case that
ZV=VZ=Z.
(52)
Proof. First, we prove that V Z = Z. Note that the i-th row of V contains either vi or zero. Looking
at an arbitrary entry i, j of V Z we have:
(V Z)i,j = v(i) X Zr,j	(53)
r∈Md(i)
(V Z)i,j = v(i)Hd(i),d(i)	(54)
(VZ)i,j=Zi,j.	(55)
Next we prove that Z V = Z. Note that for any row i in Z, Zi,j = Zi,k when d(j) = d(k).
N
(Z V)i,j =Zi,jXVr,j.	(56)
r=1
Since V is column stochastic:
(Z V)i,j =Zi,j.	(57)
□
Proposition 4. LetA = a1T. Given our definition of Tk in (6),
TkA=ATk =A	(58)
for all k.
Proof. We prove each of the three cases of Tk: I, V, and Z. Clearly, IA = AI = A. It is left to
prove V A = AV = AandZA = AZ = A.
We can see that ZA = A since a is a right eigenvector of Z with eigenvalue 1: ZA = Z a 1T =
a 1T = A. Similarly, we can see that A Z = a 1T Z = a 1T = A as 1T is a left eigenvector of Z.
The same holds for V.	□
Lemma 1. Under Assumptions 1c and 1d, the variance of the weighted average stochastic gradient
is bounded as follows:
N2
EkUlGk-Hkk2] ≤ Xa2 (Pi(β — 1) + 1) IlVF(Xki)R + "σ2
i=1
NN
+ X X alaj(1 - pj)(1 - pl)VF (X(kj))T VF (X(kl)). (59)
l=1 j6=l
16
Published as a conference paper at ICLR 2021
Proof.
Ek[kGk-Hkk2]=园® [Xai(g(i IF(Xk))| j	(60)
N	NN
=Ek Xa2kgki) -VF(Xk)k2 + XXaι%固-NF(Xkj)),gkl)-VF(Xkl)))(61)
i=1	l=1 j6=l
N	NN
=Xai2Ekkgk(i)-VF(Xk)k2+XXalajEk	gk(j) -VF(X(kj)),gk(l) -VF(X(kl))	.
i=1	l=1 j 6=l
(62)
Looking at the cross-terms in (62):
Ek Dgk(j) - VF (X(kj)), gk(l) - VF (X(kl))E
=Ek h(gk(j))T gk(l)i -Ek h(gk(j))T VF (X(kl))i
-Ek hVF(X(kj))Tgk(l)i +VF(X(kj))TVF(X(kj))	(63)
= pjplVF(X(kj))TVF(X(kl)) - pjVF(X(kj))TVF(X(kl))
- plVF (X(kj))T VF (X(kl)) +VF(X(kj))TVF(X(kj))	(64)
= (1 - pj)(1 - pl)VF(X(kj))TVF(X(kl)).	(65)
Plugging (65) into (62) we have:
N
Ek[kGk -Hkk2] =Xai2Ekkgk(i) - VF(Xk)k2
i=1
NN
+ X X alaj (1 - pj)(1 - pl)VF (X(kj))T VF (X(kl))	(66)
N2
≤ Xai2 (pi(β - 1) + 1) ||VF(X(ki))|| + piσ2
i=1
NN
+ X X alaj (1 - pj)(1 - pl)VF (X(kj))T VF (X(kl))	(67)
l=1 j 6=l
where (67) follows from Assumption 1d and (29).	□
Lemma 2. Under Assumptions 1c and 1d, the squared norm of the stochastic gradients is bounded
by:
N	|	|2 N
Ek	[kGkk2] ≤ X [a2(Pi(e	+I)-P2)	+aiP2]	||VF(Xki))||	+ Xa2Piσ2	(68)
i=1	i=1
Proof.
Ek [kGkk2 =Ek [kGk-Ek[Gk]k2 +kEk[Gk]k2
2
(69)
N
Ek kGk -	aipiVF(X(ki))k2
i=1
|N
N
X aipiVF(X(ki))
i=1
(70)
Ek
Gk -XaiVF(X(ki))+Xai(1 -pi)VF(X(ki))
i=1
i=1
N
X aipiVF(X(ki))
i=1
2
. (71)
+
N
2
+
17
Published as a conference paper at ICLR 2021
Applying the definition of Gk to (71) we get:
Ek kGkk2
N
=Ek	X Iaigki)- aNF(Xki)) + ai(1 -PiWF(Xki))]
i=1
N2
=Ek X Mgki)- aiVF(Xki)) + ai(1 -PiNF(Xki))Il
i=1
+ IIX aiPiVF(X(ki))
I i=1
+ IIX aiPiVF(X(ki))
I i=1
(72)
2
NN
+Ek X X alaj (gk(j) - VF (X(kj))) + (1 - Pj)VF (X(kj)),
j=1 l=1,l6=j
(gk(l) - VF (X(kl))) + (1 - Pl)VF (X(kl))	.	(73)
Let the cross-terms in (73) be
NN
CR=Ek X X alaj (gk(j) - VF (X(kj))) + (1 - Pj)VF (X(kj)),
j=1 l=1,l6=j
(gk(l) -VF(X(kl)))+(1-Pl)VF(X(kl))
(74)
We can simplify CR as follows:
NN
CR=X X alajEk (gk(j) - VF (X(kj)))T (gk(l) -VF(X(kl)))
j=1 l=1,l6=j
+(gk(j) - VF (X(kj)))T (1 - Pl)VF (X(kl)) + (1 - Pj)VF (X(kj))T (gk(l) -VF(X(kl)))
+(1-Pj)(1-Pl)VF(X(kj))TVF(X(kl))Ti	(75)
NN
=X X alajhEk h(gk(j) - VF(X(kj)))T(gk(l) - VF(X(kl)))i
j=1 l=1,l6=j
+ (Ek[gk(j)] -VF(X(kj)))T(1-Pl)VF(X(kl))
+(1-Pj)VF(X(kj))T(Ek[gk(l)] -VF(X(kl)))
+ (1 - Pj)(1 - Pl)VF (X(kj))T VF (X(kl))T i.	(76)
Applying Assumption 1c to (76), we get:
NN
CR=X X alaj Ek (gk(j) - VF (X(kj)))T (gk(l) - VF (X(kl)))
j=1 l=1,l6=j
+(Pj - 1)(1 - Pl)VF (X(kj))T VF (X(kl)) + (Pl -1)(1-Pj)VF(X(kj))TVF(X(kl))
+(1-Pj)(1-Pl)VF(X(kj))TVF(X(kl))i	(77)
NN
=X X alajhEk h(gk(j) - VF (X(kj)))T (gk(l) - VF (X(kl)))i
j=1 l=1,l6=j
-(1-Pl)(1-Pj)VF(X(kj))TVF(X(kl))i.	(78)
18
Published as a conference paper at ICLR 2021
Applying (78) to (73) we have:
N2
Ek [kGkk2] = Ek XMgki)- a"F(Xki))+ ai(1- Pi)VF(Xki)U
i=1
NN
+X X alajhEk h(gk(j)-VF(X(kj)))T(gk(l) - VF(X(kl)))i
j=1 l=1,l6=j
-(1-pl)(1-pj)VF(X(kj))TVF(X(kl))i
N 2
+ X aipiVF(X(ki))	.	(79)
i=1
Expanding the first term in (79) we have:
N	2N	2
Ek [kGkk2]	=Ek	Xaigk(i)	-aiVF(X(ki))	+Xai2(1-pi)2	VF(X(ki))
i=1	i=1
N
+ XEk aigk(i) - aiVF(X(ki)), ai(1 - pi)VF(X(ki))
i=1 X-----------------V--------------------}
CR1
N
+ X Ek	ai(1 - pi)VF(X(ki)), aigk(i) - aiVF(X(ki))
i=1 X-------------------V-------------------}
CR2
NN
+X X alajhEk h(gk(j) -VF(X(kj)))T(gk(l)-VF(X(kl)))i
j=1 l=1,l6=j
- (1 - pl)(1 - pj)VF(X(kj))TVF(X(kl))i
N
+ X aipiVF(X(ki))
i=1
(80)
We simplify CR1:
CR1 = hDaipiEk(g(X(ki)))T -aiVF(X(ki)),ai(1 -pi)VF(X(ki))Ei	(81)
= hDai(pi - 1)VF(X(ki)),ai(1 -pi)VF(X(ki))Ei	(82)
= -ai2(1 -pi)2 VF(X(ki))2 .	(83)
Similarly, for CR2:
CR2 = -ai2(1 -pi)2 VF(X(ki))2 .	(84)
19
Published as a conference paper at ICLR 2021
Plugging (83) and (84) back into (80):
Ek kGkk2 =Ek
N2
XMga)-aiVF (Xk)U
i=1
N2
- Xai2(1 -pi)2 VF(x(k))
i=1
NN
+X X alajhEk h(gk(j) -VF(x(kj)))T(gk(l) - VF(x(kl)))i
j=1 l=1,l6=j
-(1-pl)(1-pj)VF(x(kj))TVF(x(kl))
N
+ X aipiVF(x(ki))
i=1
(85)
We can simplify by observing that:
Ek kGk - Hkk2 =Ek
N2
X aigk( ) - aiVF(x(k))
i=1
NN
+X X alajEk (gk(j) -VF(x(kj)))T(gk(l) -VF(x(kl)))
j=1 l=1,l6=j
(86)
N
X aipiVF(x(ki))
i=1
which gives us:
N2
Ek kGkk2 =Ek kGk-Hkk2 - X ai2(1 - pi)2 VF (x(ki))	+
i=1
NN
- X X alaj(1 - pl)(1 - pj)VF(x(kj))TVF(x(kl)) (87)
j=1 l=1,l6=j
Applying Lemma 1 to (87):
N
Ek kGkk2 ≤Xai2
i=1
(pi(β - 1) + 1) VF(x(ki))2 + piσ2
N
≤ X ai2
i=1
N2
- Xai2(1 -pi)2 VF(x(k))	+
i=1
(pi(β-1)+1) VF(x(ki))2+piσ2
N
X aipiVF(x(ki))
i=1
N	2N	2
- Xai2(1 -pi)2 VF(x(ki))	+ X aipi2 VF(x(ki))
i=1	i=1
N	2N
X [a2 (PKe +I)-p2) + aip2] ∣∣vF(Xki))I + Xa2Piσ2
i=1	i=1
where equation (89) follows from Jensen’s inequality.
(88)
(89)
(90)
□
2
Lemma 3. Under Assumption 1c, the expected inner product of the batch gradient and the weighted
average stochastic gradient is equal to:
1N	2
Ek[hVF(Uk), Gki] = 2kVF(Uk)k2 + X ai ∣∣PiVF(Xki))∣∣
2	i=1 2
N2
- X y∣VF(Uk) - PiVF(Xki))∣∣	(91)
i=1
20
Published as a conference paper at ICLR 2021
Proof.
Ek[hVF(Uk), Gki] =Ek	VF(Uk),XN aigk(i)
= VF(Uk),XpiaiVF(X(ki))
N
= Xai VF(Uk),piVF(X(ki))
i=1
N
(92)
(93)
(94)
X a2i [kVF(Uk)k2 + kPiVF(Xki))k2 -kVF(Uk)-PiVF(Xki))∣∣[
i=1
(95)
N2
=2kVF(Uk)k2 + X a2i∣∣PiVF(Xki))∣∣
i=1
N ∣	∣2
-X 1i∣∣VF(Uk) - PiVF(Xki))∣∣	(96)
i=1
where (93) follows from (20), and (95) follows from the fact that, for arbitrary vectors y and z,
2yTz = ||y||2 + ||z||2 - ||y - z||2.
□
Lemma 4. Under Assumption 1, following the update rule given in (5), if all model parameters are
initialized at the same x1 , the expected weighted average gradient is bounded as follows:
E
1
IK EkVF(Uk)k2 ≤
2(F(xι)- Fin])
k=1
ηK
N	2L2 K
+ σ2ηL X α2pi + -ɪ X Ek X k( I - A )kFa
i=1
k=1
K
KN
-K XX ai ((4pi - p2 - 2) - ηL @pi(e + 1) - ap2 + p2)) E ∣∣VF(Xki)
k=1 i=1
where A = a 1T.
Proof. According to Assumption 1a,
Ek [F (uk+l)] - F (Uk) ≤ Ek hVF (uk)), uk + 1 - Uki + 2 kuk + 1 - ukk2
η2 L
=-ηEk KVF(Uk ), Gk i] +-2 Ek [IIGk ∣∣2].
(97)
(98)
(99)
Plugging in Lemmas 2 and 3, we get:
Ek[F (Uk+1)] - F(Uk)
N	2N
≤-η 2 kVF(Uk)k2 + X a2ip2∣∣VF(Xki))∣∣ - X a2ikVF(Uk) - PiVF(Xki))∣2
2	i=1 2	i=1 2
2 N	2	22 N
+ ɪXWPi(B +1) -α2p2 + aiP2) ∣∣VF(Xki))∣∣ +一2-XαiPi	(IOO)
i=1	i=1
N	2 2L N
=-2kVF(Uk)k2 + 2 X aikVF(Uk)- PiVF(Xki))k2 + -ɪ X ^Pi
2	2 i=1	2 i=1
N2
-2 Xɑi(P2 - ηL (aiPi(β +1) - aiP2 + P2)) ∣∣vf(Xki))∣∣ . (IOI)
i=1
21
Published as a conference paper at ICLR 2021
After some rearranging, we obtain:
kVF(uk)k2 ≤ 2 (F(Uk)- Ek[F(Uk+1)]) + σ2ηLXT a2pi + XX ai∣∣VF(Uk) - PiVF(Xki))『
η	i=1	i=1
N2
—Xai(P2 — ηL (aiPi(β + 1) - aiP2 + P2)) ∣∣VF(Xki)) ] .	(102)
i=1
Taking the total expectation over all iterations:
E E XX kVF(Uk)k2l ≤ 2 (F(XM FnD + σ2ηLXX a2pi
k=1	η	i=1
KN
+K XX aiEkVF(Uk) - piVF(X(ki))k2
k=1 i=1
KN	2
-K XXai(P2 — ηL (aiPi(β + 1) - aiP2 + P2)) ElVF(Xki))].
k=1 i=1
(103)
The third term in (103) can be bounded as:
N
X aiEkVF(Uk) -piVF(X(ki))k2
i=1
N
=XaiEkVF(Uk) - VF (X(ki)) + (1 -pi)VF(X(ki))k2	(104)
i=1
N
≤ X 2aiEkVF(Uk) - VF (X(ki))k2 + 2ai(1 -pi)2EkVF(X(ki))k2 (105)
i=1
NN
≤ X 2aiL2EkUk -X(ki)k2+X2ai(1-pi)2EkVF(X(ki))k2	(106)
i=1	i=1
where (105) follows from the fact that ky + zk2 ≤ 2kyk2 + 2kzk2, and (106) follows from (105)
by Assumption 1a.
Recalling the definition of the weighted Frobenius norm and the definition of u, we can simplify the
first term in (106):
N
X 2aiL2EkUk - X(ki)k2 = 2L2EkUk1T - Xk k2Fa	(107)
i=1
= 2L2E llXk a1T -Xkll2F	(108)
= 2L2EkXk(I-A)k2Fa.	(109)
22
Published as a conference paper at ICLR 2021
Plugging (106) and (109) back into (103), we obtain:
E K XX kVF(Uk)k2 ≤ 2 (F(XI)K- FnD + σ2ηLXX a2pi
K k=1	ηK	i=1
2L2 K	1 K N
+ -K-EEk Xk(I - A)kFa + 女 EE2ai(1-Pi) EkVF(Xki))k2
k=1	k=1 i=1
1	K N	2
-KXXai (p2 - ηL (aiPi(β + 1) - aip2 + P2)) EIVF(Xki))I	(IIO)
k=1 i=1
=-(F(UK- FinfD + σ2ηL XX a2Pi + 彳 X Ek Xk(I - A) kF.
η	i=1	k=1
KN	2
-K XXai ((4Pi- P2 - -) - ηL (a,ipi (β + 1)- aip2 + PIy) E∣∣VF (Xki))] . (111)
k=1 i=1
□
Lemma 5. Given the properties of Z and V given in Propositions 1 and 2, it is the case that:
∣Zj-A∣op=ζj,	kV-Akop= 1,	kI-Akop= 1	(112)
where A = a 1T and Z = max{∣λ2(H)∣, ∣λ(H)∣}.
Proof. According to the definition of the matrix operator norm,
∣∣Zj - A∣∣op = /λmaχ((Zj- A)T(Zj- A))	(113)
=/λmaχ(Z2j- AZj- ZjA + A)	(114)
=Jλmaχ(Z2j- A)	(115)
where (114) follows from Aj = A, and (115) follows from A Z = ZA = A.
We can simplify (115) further:
=Jλmaχ(Z2j- A2j )	(116)
=√λmθx(Z - A)2j	(117)
where (117) follows from the commutability of Z and A.
Based on Proposition 2, the non-zero eigenvalues of Z are the same as H. As shown in Lemma 6
of Rotaru & Nageli (2004), for a matrix Z with the properties in Proposition 1, the spectral norm of
Z - A is equal to ζ.
Therefore:
∣∣Zj- AUop = √ζ2j	(118)
= ζj.	(119)
Similarly for V:
kv - Akop = /λmaχ((V - A)T (V - A))	(120)
=√λmaχ(V - AV - VA + A)	(121)
=√λmaχ(V - A)	(122)
(123)
23
Published as a conference paper at ICLR 2021
where (121) follows from Aj = A and Vj = V, and (122) follows from AV = VA = A.
Note that the eigenvalues of each block V(d) are N (d) - 1 zeros and a one. The set of eigenvalues
of V will include D ones. If D > 1, then based on Lemma 6 of Rotaru & Nageli (2004) and
Proposition 1, the spectral norm ofV - A is 1, so
kv - Akop = √	(124)
= 1.	(125)
Since the eigenvalues ofI are all 1, and I is commutable with A, we can similarly say:
kI-Akop= 1.	(126)
□
Lemma 6. Given two matrices C ∈ RN×M andD ∈ RM×N, and an N -vector a,
Tr((diag(a))1/2 C D (diag(a))1/2) ≤ kCkFakDkFa.	(127)
Proof. We define the i-th row of C as ciT and the i-th column of D as di . We can rewrite the trace
as:
NM
Tr((diag(a))1/2 C D (diag(a))1/2) = X X ai Ci,j Dj,i	(128)
i=1 j=1
N
= X aiciTdi.	(129)
i=1
Placing a squared norm around (129), we can apply the Cauchy-Schwartz inequality:
N	2	N	N XaiciTdi ≤ XaikciTk2	X NM =(旺 ai C2,j)( = kCk2FakDk2Fa. Lemma 7. Given two matrices C ∈ RM ×N and D ∈ RN ×N kCDkFa ≤ kCkFakDk	ai kdi k2	(130) NM ∑∑a∙ D2,j I	(131) i=1 j=1 (132) □ , and an N -vector a, then op.	(133)
Proof. We define the i-th row of C as ciT and the set I the squared Frobenius norm as: M k C D k2Fa = X kciT D (diag(a))1/ i=1 M = X kciT D (diag(a))1/ i∈I M = X kciT (diag(a))1/2 i∈I M ≤ X kciT (diag(a))1/2 i∈I =kCk2FakDk2op.	= {i ∈ [1, M] : kciT k 6= 0}. We can rewrite 2 k2	(134) 2 k2	(135) k2 kcT D (diag⑷尸/2 k2	(136) kcTmiagm))1/2 k2 k2kDko2p	(137) (138) □
24
Published as a conference paper at ICLR 2021
C.3 Proof of Theorem 1
We recall Theorem 1.
Theorem 1. Under Assumptions 1 and 2, if η satisfies the following for all i ∈ M:
(4pi - P2 - 2) ≥ ηL (aiPi(β + 1) - aiP2 + P2) + 8L2η2q2τ2Γ	(12)
where Γ = ɪ-^ + i-—^ + .二产 and Z = max{∣λ2(H)∣, |1n(H) |}, then the expected square norm
of the average model gradient, averaged over K iterations, is bounded as follows:
E
1
K EkVF(Uk)k2	≤
2(F(x1)-Finf])
k=1
+ 4L2η2σ2q3τ3 (-
qτ
+ 4L2η2σ2 (I-I)
ηK
1
K
τ2
F + ɪ +(1⅛) P
N
+ σ2ηL X ai2Pi
i=1
(q-1)(2q + 1) + (τ - 1)(2τ +1)
(13)
K
—
6
6
P
K→-→	σ2ηLX a2Pi +4L2η2σ2q2τ2 (= + ɪ + (ɪ) P
+ 4L2η2σ2 (2)卜2
(q-1)(2q + 1) + (τ - 1)(2τ +1)
(14)
6
6
P
where P = PiN=1 aiPi.
We now give the proof of Theorem 1 using Lemmas 2-7.
Proof. Using our intermediate result from Lemma 4, we decompose Xk(I - A) using our recursive
definition of Xk :
Xk(I-A)= (Xk-1 -η Gk-1) Tk-1(I - A)	(139)
= Xk-1(I - A) Tk-1 -η Gk-1(Tk-1 - A)	(140)
= [(Xk-2 -η Gk-2) Tk-2(I - A)] Tk-1 -η Gk-1(Tk-1 - A)	(141)
= [Xk-2(I - A) Tk-2 -η Gk-2(Tk-2 - A)] Tk-1 -η Gk-1(Tk-1 - A)	(142)
= Xk-2(I - A) Tk-2 Tk-1 -η Gk-2(Tk-2 Tk-1 - A) - η Gk-1(Tk-1 - A). (143)
where (140) follows from the commutability of Tk and A by Proposition 4.
Continuing this, we end up with:
k-1	k-1	k-1
Xk(I-A) =X1(I-A) YTl-ηX Gs	YTl-A .	(144)
l=1	s=1	l=s
Since all workers initialize their models to the same vector, X1(I - A) Qlk=-11 Tk = 0, and thus we
have:
E kXk (I - A)k2Fa =η2E
kX-1Gs	kY-1Tl -A 2
s=1	l=s	Fa
(145)
Let k = jqτ + lτ + f, where j is the number of hub network averaging rounds, l is the number of
sub-network averaging rounds since the last hub network averaging round, and f is the number of
local iterations since the last sub-network averaging round. Define:
k-1
Φs,k-1 = YTl.
l=s
25
Published as a conference paper at ICLR 2021
Noting that Vj = V, and VZ = ZV = Z by Proposition 3, Φs,fc-1 can be expressed as:
I jqτ + lτ < s < jqτ + lτ + f
V jqτ < s ≤ jqτ + lτ
Z	(j	— 1)qτ < s ≤	jqτ
φs,k-i =	< Z2	(j	— 2)qτ < s ≤	(j — 1)qτ	(146)
.
.
.
Zj	1 ≤ S ≤ qτ.
For r < j, let
(r+1)qτ	(r+1)qτ
Yr = X GS ,	Qr = X	VF(XS)
s=rqτ +1	s=rqτ +1
Wealsolet Yj1 = Pj=+T+ι Gs, Yj2 = PS=+Tτ+f+ι Gs, QjI= Pw+T+ι VF(Xs), andQj2 =
Pj=+T++f+ι VF(Xs). With this in mind, We can split the sum in (145) into batches for each hub
network averaging period:
qτ
X Gs (Φs,k-1 — A)= Y0(Zj — A)	(147)
s = 1
2qτ
X Gs (Φs,k-1 — A) = YI(ZjT — A)	(148)
s=qτ +1
♦ ♦♦
jqτ
E Gs (Φs,k-1 — A) = Yj-1(Z — A)	(149)
s=(j-1)qτ+1
jqτ+iτ +f
X Gs (Φs,k-1 — A)= Yjι (V — A)+ Yj2 (I — A).	(150)
s=jqτ+1
Summing this all together, We get:
k-1	j-1
X Gs (Φs,k-1 — A) = X Yr (Zj-r — A)+ Yji (V — A)+ Yj2 (I — A).	(151)
s=1	r=0
Plugging (151) into (145):
EilXk (I — A)IIFa = η2E
j-1
X Yr (Zj-r — A)+ Yji (V — A)+ Yj2 (I — A)
r=0
2
Fa
j-1
=η2E X(Yr - Qr )(Zj-r — A) + (Yji — Qji )(V — A)
r=0
j-1
+ (Yj2 — Qj2 )(I — A) + X Qr (Zj-r — A) + Qji (V — A) + Qj2 (I — A)
r=0
2
Fa
j -1
≤ 2η2E X(Yr - Qr)(Zj-r — A) + (Yji - Qji)(V — A) + (Yj2 — Qj?” — A))
r=0
'--------------------------------------V----------------------------------
Ti
(152)
(153)
2
Fa
.}
+ 2η2E
j-1
E Qr (Zj-r — A)+ Qji (V — A) + Qj2 (I — A)
r=0
^^^^^^^^^^^^^^^^^^^^^^^^^^^^≡
T2
2
Fa
.}
(154)
26
Published as a conference paper at ICLR 2021
where (154) follows from the fact that ky + zk2 ≤ 2kyk2 + 2kzk2.
We first put a bound on T1:
T1 =2η2E	(Yr-Qr)(Zj-r-A)+(Yj1 -Qj1)(V-A)+(Yj2-Qj2)(I-A)
r=0	Fa
j-1
=2η2 XE(Yr-Qr)(Zj-r-A)2Fa+E(Yj1-Qj1)(V-A)2Fa
+ E (Yj2 -Qj2)(I-A)2Fa
(155)
j-1 j-1
+2η2X X E
n=0 l=0,l6=n
Tr (diag(a))1/2 (Zj-n - A)(Yn - Qn)T (Yl - Ql)(Zj-l -A) (diag(a))1/2
'------------------------------------------V-----------------------------------------}
TR
_________________________________ - /
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TR0
j-1
+4η2XETr (diag(a))1/2 (V - A)(Yj1 - Qj1)T (Yl - Ql)(Zj-l - A) (diag(a))1/2
l=0
、
}
{z^^
TR1
j-1
+4η2XETr (diag(a))1/2 (I - A)(Yj2 - Qj2)T (Yl - Ql)(Zj-l - A) (diag(a))1/2
l=0
'----------------------------------------------------------------------------------
}
{z^^
TR2
+ 4η2E Tr (diag(a))1/2 (V - A)(Yj1 - Qj1)T (Yj2 - Qj2)(I - A) (diag(a))1/2. (156)
'----------------------------------V---------------------------------}
TR3
TR can be bounded as:
TR≤ (Zj-n - A)(Yn - Qn)T Fa (Yl - Ql)(Zj-l - A)Fa	(157)
≤ (Zj-n - A)op kYn - QnkFa kYl-QlkFa (Zj-l-A)op	(158)
≤ ζ2j-n-l kYn-QnkFa kYl-QlkFa	(159)
≤ 2Z2j-n-l [kYn- QnkFa + kYl- QlkFai	(160)
where (157) follows from Lemma 6, (158) follows from Lemma 7, and (159) follows from Lemma 5.
We can similarly bound T R1 and TR3 :
j -1
TR1 ≤2η2Xζj-l	E	Yj1	-Qj12Fa+EkYl-Qlk2Fa
l=0
j -1
TR2 ≤2η2Xζj-l	E	Yj2	-Qj22Fa+EkYl-Qlk2Fa
l=0
TR3 ≤2η2	hE	Yj1	-Qj12Fa+EYj2-Qj2Fai	.
(161)
(162)
(163)
27
Published as a conference paper at ICLR 2021
Summing T R0 through T R3, we get:
3	j-1 j-1
XTRt≤η2X X ζ2j-n-l EkYn-Qnk2Fa+EkYl-Qlk2Fa
t=0	n=0 l=0,l6=n
j-1
+2η2Xζj-l EYj1-Qj12Fa+EkYl-Qlk2Fa
l=0
j-1
+2η2Xζj-l EYj2-Qj22Fa+EkYl-Qlk2Fa
l=0
+2η2EYj1-Qj12Fa	+2η2EYj2-Qj22Fa	(164)
j-1 j-1
≤ 2η2 X X ζ2j-n-lE kYn -Qnk2Fa
n=0 l=0,l6=n
j-1	j-1
+ 2η2 X ζj-lE kYl - Qlk2Fa + 2η2 X ζj-lE kYl - Qlk2Fa
l=0	l=0
jj
+ 2η2 X	ζj-lE	Yj1	-	Qj12Fa	+2η2Xζj-lEYj2-Qj22Fa	(165)
l=0	l=0
j-1	j-1	j-1
= 2η2 X ζj-nE kYn - Qnk2Fa X ζj-l +4η2Xζj-lEkYl-Qlk2Fa
n=0	l=0,l6=n	l=0
jj
+ 2η2E Yj1 -Qj12FaXζj-l +2η2EYj2 -Qj22FaXζj-l	(166)
l=0	l=0
where (165) follows from the symmetry of the n and l indices.
Plugging (166) back into (156):
j-1
T1 ≤2η2XEk(Yr-Qr)k2Fa (Zj-r-A)o2p+2η2E(Yj1-Qj1)2Fa kV - Ako2p
r=0
j-1	j-1
+ 2η2E	(Yj2	- Qj2)2Fa	kI - Ak2op	+ 2η2	X ζj-nE kYn	-	Qnk2Fa	X	ζj-l
n=0	l=0,l6=n
jj
+ 2η2E Yj1 -Qj12FaXζj-l +2η2EYj2 -Qj22FaXζj-l
l=0	l=0
j-1
+4η2Xζj-lEkYl-Qlk2Fa	(167)
l=0
j-1
≤ 2η2 X E k(Yr - Qr)k2Fa ζ2(j-r) + 2η2E (Yj1 - Qj1)2Fa
r=0
j-1	j-1
+2η2E(Yj2-Qj2)2Fa	+2η2Xζj-nEkYn-Qnk2Fa	X	ζj-l
n=0	l=0,l6=n
jj
+ 2η2E Yj1 -Qj12FaXζj-l +2η2EYj2 -Qj22FaXζj-l
l=0	l=0
j-1
+4η2Xζj-lEkYl-Qlk2Fa	(168)
l=0
28
Published as a conference paper at ICLR 2021
where (167) follows from Lemma 7, and (168) follows from Lemma 5.
We further bound T1 :
j-1
T1 ≤2η2	E k(Yr	- Qr)k2Fa ζ2(j-r)	+	2η2E	(Yj1	- Qj1)2Fa
r=0
j-1	ζ
+ 2η2E ∣∣(Yj2 -Qj2)∣∣Fa + 2η2 XZj-nEkYn- QnkFa 占
n=0	1 - ζ
+ 2η2E^γji - QjJIk 117 + 2η2E 恒2 - Qj21民 117
a 1- ζ	a 1- ζ
j-1
+4η2	ζj-lEkYl -Qlk2Fa	(169)
l=0
=2η2 X (ZMi + 2Zj-r + ζj-r+Z1) Ek(Yr- Qr)kFa
+ 2η2 (2)EuYjI- QjJIFa + 2η2 (2)E IIγj2 - Qj2 IIFa	(170)
where (169) follows from the summation formulae of a power series:
j	j	j-1	j-1
X ζZT ≤ X ZjT ≤ 1-7,	X ζj-l ≤ X ZjT ≤ 二	(171)
l=0	l=-∞	l=0	l=-∞
Taking a closer look at E k(Yr - Qr)k2F for 0 ≤ r < j:
I (r+1)qτ
Ek(Yr- Qr)kFa = E X (Gs-VF(Xs))
Is=rqτ +1
N
XaiE
i=1
(r+1)qτ
X	(gsi -VF(x(si)))
s=rqτ +1
N	(r+1)qτ	2
≤Xaiqτ	X E II(gsi -VF(x(si)))II
i=1	s=rqτ +1
(172)
(173)
(174)
2
(N	(r + 1)qτ	2
Xai X	(pi(β-1)+1)E IIVF(x(si))II
i=1	s=rqτ+1
N
+ q2τ2σ2 X aipi
i=1
(175)
(N	(r + 1)qτ	2∖
X ai X	(Pi(β - 1) + 1) E Ivf (Xsi))ii I + q2 τ2σ2 P . (176)
i=1	s=rqτ+1
where (175) follows from Assumption 1d and (29).
Similarly, for r = j1 and r = j2 :
/ N	jqτ+lτ	U	.l2\
E∣∣(Yjι - QjI)IIFa ≤ IT I ∑>i E	(Pi(β - 1) + 1) E∣∣VF (Xsi))II I + l2τ2σ2 P (177)
i=1	s=jqτ+1
/ N	jqτ+lτ+f-1	U	∣∣2∖
E II(Yj2-Qj2)IIFa	≤ (f-1) I	ai	(pi(β-1)+1)E	IIVF(X(si))II	I
i=1	s=jqτ+lτ+1
+(f-1)2σ2P.
(178)
29
Published as a conference paper at ICLR 2021
Plugging (176), (177), and (178) into (170), We can bound Ti as follows:
Ti ≤ 2η2σ2 (卜τ2 E (Z2(—)+ 2ζj— + Z^) ) + (W) (l2τ2 + (f - 1)2)) P
j-1 /	Z j-r + 1 ∖ (r+1)qτ N	l,	ll2
+ 2η2qτ E (Z 2(j-r) +2Z j-r + ζτ-- ) E E ai(g(β - 1) + 1) E∣∣VF (Xsi)) ∣∣
r=0 '	", s=rqτ +1 i=1
/2_ ζ∖	(N	jqτ+lτ	ll	ll2∖
+ 2η2 τ-∣ lτ E a E (pi(β - 1) + 1) E∣∣VF (Xsi)) ∣∣
' i=	∖i=1	s=jqτ +1	)
/2 — Z、	/ N	jqτ+lτ +f-1	11	H 2 ∖
+ 2η2 1(f - 1) I Ea E	(Pi(β - 1) + 1) E∣∣VF (Xsi))∣∣).	(179)
' i	∖ i=1	s=jqτ+lτ +1	/
Referring back to Lemma 4, our goal is to sum T1 over k = 1,...,K iterations. First, we sum over
the j-th sub-network update period up to the j-th hub network averaging, for l = 0,...,q - 1 and
f = 1,...,τ:
q-1 T	j-1 /	广 j—r + 1 ∖
EET1 ≤ 2η2σ2q3τ3 E (Z2(j-r) + 2Zj-r + ^^^^) P
l=0 f =1	r=0 ∖	- Z ×
2 - C' ( 3 q(q -1)(2q +1) l	T(T - 1)(2τ +1)
口 乂 T------6----+ q----6---
j-1 /	Z j-r+1 ∖ (r+1)qτ N	∣ ∣	∣ ∣ 2
+ 2η2q2τ2 E (Z2(j-r) + 2Zj-r + z1--) E	Eαi(Pi(β - 1) + 1)E∣∣VF(Xsi))∣∣
r=0、	/ s=rqτ +1 i=1
2 Z	N	j(qτ +1)	2
+ η2 占 q(q- 1)τ2Eαi E	(Pi(β - 1) + 1)e∣∣vf(Xsi))∣∣
i=1	s=jqτ +1
/2 _ z∖	N	j(qτ+1)+τ-1	∣ ∣	∣ ∣ 2
+ η2 占 q2τ (τ - 1) E a E	(Pi(β - 1) + 1) E∣∣VF (Xsi)) ∣∣ .	(180)
i=1	s=j(qτ +1)+1
Let:
Γr = (Z2(j-r) + 2Zj-r + jr+1) .	(181)
Note that Γ7- = 31-^ > ɪ-1. Using this inequality, we can bound the sum of the last three terms of
(180) to get 2q2τ2 Pr=O Γr:
q-1 τ	j-1	j-r+1
EE
T1 ≤ 2η2σ2q3τ3 E (Z2(j-r) + 2Zj-r + Zr^- ) P
l=0 f =1	r=0、	Z ×
+ 2η2σ2 (|-1) (τ3 q(q- 1产 +1) + J(T - I)(2τ + 1) ) P
j	(r+1)qτ N	2
+ 2η2q2 τ2 Err	E	Eai(Pi(β- 1) + 1) E∣∣VF (Xsi)) ∣∣ .	(182)
r=0	s=rqτ +1 i=1
30
Published as a conference paper at ICLR 2021
Summing (182) over the hub network averaging periods j = 0,..., K∕(qτ) - 1, We obtain:
κ∕Sτ )-1 q-1 τ	κ∕Sτ )-i j-1 /
X	XXT1	≤ 2η2σ2q3τ3	X	X	Z2(L)+ 2ζj-r	+
j=0	l = 0 f =1	j=0	r=0	∖
Zj-r+1ʌ P
1 - C p
+ 2η2σ2K (金)}2 (q- %+1) + (T- 1)”1)) P
K∕Sτ )-1 j	(r+1)qτ N	2
+ 2η2q2τ2 XX Γr X X ai(Pi(β- 1) + 1) EllVF (*)∣∣
j=0	r=0 s=rqτ +1 i=1
(183)
KIlqT)-2 KIlqT)-1 /	Aj-叶八
2η2σ2q3τ3	X X	R2(j-r) + 2<j-r + Zr-T)P
r=0	j=r+1
+ 2η2σ2K (金)卜2 Iq- %+1) + (T- 1)(2τ+1)) P
KKqT)-1	(KlkqT)-1	∖	/ (r+1)qτ	N	2、
+ 2η2q2τ2	X I X	Γj 11 X	X ai (pi(β - 1) + 1) E∣∣VF (Xsi))|| ).
r=0	∖ j=r	I	∖s=rqτ +1	i=1	/
(184)
Applying the following summation formula to sum over Γ7-, we obtain
K∕(qτ )-1
X
j=r
.	ζj-r+1 ∖	∞
+ 2Cj-r + J 1≤ X
j=r
1
≤ T- C2
C j-r+1
+ 2Cj-r + ζ---
1 - Z
+2+
+ 1 - C + (1 - Z)2.
(185)
(186)
We let Γ = ɪ-1^ + 1-ζ +(1二产.We can also apply this following summation formula to the first
term in (184):
K∕(qτ)-1
X	(C2(j-r) + 2Cj-r +
j=r+1
ζ1-+1) ≤ X (C2(j-r) +2Cj-r +
1 - C	j=r+1
≤ 1⅛ + ɪ +(1⅛.
(187)
(188)
Applying the summation formula in (188), plugging Γ in, and indexing the iterations in terms of k,
we bound (184) as:
XT1 ≤ 2η2σ2q3τ3 (K - 1)(二 + 六 +	) P
M	∖qτ	J "-c2	1 -c (1 -c)27
+ 2η2σ2κ (W) f ①-%+1) + (T- 丁1)) P
KN	2
+ 2η2q2τ2Γ XX
ai (Pi(β - 1) + 1) E∣∣VF(Xki)) || .	(189)
k=1i=1
31
Published as a conference paper at ICLR 2021
Now we bound T2 :
T2 = 2η2E
j-1
Qr(Zj-r-A)+Qj1(V-A)+Qj2(I-A)
r=0
2
Fa
(190)
j-1
2η2 X E	Qr(Zj-r	- A)2Fa	+	2η2E	Qj1(V - A)2Fa	+	2η2E	Qj2(I	- A)2Fa
j-1 j-1
+ 2η2 X X E Tr ((diag(a))1/2 (Zj-n - A) QT Ql(ZjT- A)(diag(a))1/2)
n=0 l=0,l=n	、	{z	'
TR0
X---------------------------------------{-------------------------------------
TR00
j-1
+4η2XE Tr (diag(a))1/2 (V - A) QjT1 Ql(Zj-l - A) (diag(a))1/2
l=0
X-----------------------------------------------------------------------
SZ
TR01
j-1
+4η2XE Tr (diag(a))1/2 (I - A) QjT2 Ql(Zj-l - A) (diag(a))1/2
l=0
X-------------------------------------..------------------------------------}
∙•^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^≡"^
TR02
+ 4η2E hTr (diag(a))1/2 (V - A) QjT1 Qj2 (I - A) (diag(a))1/2i . (191)
^{^^
TR03
T R0 can be bounded as:
TR0 ≤	(Zj-n - A) QTn	Ql(Zj-l - A)Fa
≤	(Zj-n-A)op kQnkFa kQlkFa (Zj-l-A)op
≤ 2Z2j-n-l [kQnkFa + HQlkFj
(192)
(193)
(194)
where (192) follows from Lemma 6. We can similarly bound TR01 through TR03:
j-1
TR01 ≤2η2Xζj-l EQj12Fa+EkQlk2Fa
l=0
j-1
TR02 ≤2η2Xζj-l EQj22Fa+EkQlk2Fa
l=0
TR03≤2η2EQj12Fa+2η2EQj22Fa.
(195)
(196)
(197)
Y
}
}
}
32
Published as a conference paper at ICLR 2021
Summing T R00 through T R30 , we get:
3	j-1 j-1
TR0t ≤η2	ζ2j-n-lE EkQnk2Fa +EkQlk2Fa
t=0	n=0 l=0,l6=n
j-1	j-1
+ 2η2 X ζj-lE kQlk2Fa +2η2Xζj-lEkQlk2Fa
l=0	l=0
jj
+ 2η2 X ζj-lE Qj12Fa +2η2Xζj-lEQj22Fa	(198)
l=0	l=0
j-1	j-1	j-1
≤ 2η2 X ζj-nE kQnk2Fa X ζj-l +4η2Xζj-lEkQlk2Fa
n=0	l=0,l6=n	l=0
jj
+ 2η2E Qj12Fa X ζj-l +2η2EQj22FaXζj-l	(199)
l=0	l=0
where (199) follows from the symmetry of the indices n and l.
Plugging (199) back into (191):
T2 ≤2η2XEQr(Zj-r-A)2Fa+2η2EQj1(V-A)2Fa+2η2EQj2(I-A)2Fa
j-1	j-1	j-1
+ 2η2 X ζj-nE kQnk2Fa X ζj-l +4η2Xζj-lEkQlk2Fa
n=0	l=0,l6=n	l=0
jj
+ 2η2E Qj12Fa X ζj-l + 2η2E Qj22Fa X ζj-l (200)
l=0	l=0
j-1
≤ 2η2XEkQrk2Fa	(Zj-r	- A)2op +	2η2E	Qj12Fa	kV - Ak2op	+2η2EQj22Fa	kI - Ako2p
r=0
j-1	j-1	j-1
+ 2η2 X ζj-nE kQnk2Fa X ζj-l +4η2Xζj-lEkQlk2Fa
n=0	l=0,l6=n	l=0
jj
+ 2η2E Qj12Fa X ζj-l + 2η2E Qj22Fa X ζj-l (201)
l=0	l=0
j-1
≤2η2Xζj-rEkQrk2Fa+2η2EQj12Fa+2η2EQj22Fa
r=0
j-1	j-1	j-1
+ 2η2 X ζj-nE kQnk2Fa X ζj-l +4η2Xζj-lEkQlk2Fa
n=0	l=0,l6=n	l=0
jj
+ 2η2E Qj12Fa X ζj-l + 2η2E Qj22Fa X ζj-l (202)
l=0	l=0
where (201) follows from Lemma 7, and (202) follows from Lemma 5.
33
Published as a conference paper at ICLR 2021
We further bound T2 :
j-1
T2 ≤2η2Xζj-rEkQrk2Fa+2η2EQj12Fa+2η2EQj22Fa
r=0
j-1	j-1
+ 2η2 X Zj-nEkQnkFa T-Z +4η2 XZjTEkQlkFa
n=0	l=0
+ 2η2E ^QjJlFa占+ 2〃网心区六
(203)
j-1
≤ 2η2X
r=0
-r) + 2ζj-r +
1-ζ
E kQrk2Fa
+ 2η2 (I-I) EUQj11lFa + 2η2 (I-I) EHQj2 IlFa
where (203) follows from the summation formulae of a power series in (171).
After applying the definition of Q to (203), we obtain:
2
(204)
j-1
T2 =2η2X
r=0
+ 2ζj-r +
+ 2η (2
j-1
≤ 2η2 qτ
r=0
lτ
Zj-r+1
1 - Z
2
qτ
qτ
E EVF(Xrqτ +s)
s 1
s=1
E	VF(Xjqτ+s)	+2η2
+ 2ζj-r +
s=1
Fa
ζj-r+1
ɪ-r，
2-Z	lf-1
(1 - Z ) E £ VF (xjqτ+lτ+s
ls 1
(205)
s=1
+ 2η2iτ (餐
1 - ζ
qτ
E kVF (Xr
qτ +s)kFa
s=1
lτ
E kVF (Xj
qτ+s)kFa
s=1
)
2
2 Z f-1
+ 2η2(f - 1)(f-7 EEkVF(XjqT+lτ+s)kFa
s=1
where (206) follows from (205) by Jensen’s inequality.
Summing over all iterates in the j-th sub-network update period, we obtain:
(206)
q-1 τ
l=0 f=1
j-1
T2 ≤ 2η2q2τ2
r=0
-r) + 2ζj-r +
+ η2qτ(q -I) (I-1
1-ζ
qτ
qτ
E kVF (Xr
qτ+s)kFa
s=1
E kVF(Xjqτ+s)k2Fa
s=1
+ η2qτ(T-1) (1-1) X EkVF(XjqT +qT +s )kFa
(207)
j qτ
≤ 2η2q2τ2XΓrXEkVF(Xrqτ+s)k
r=0	s=1
2
Fa
(208)
Summing over all iterations and applying the summation bound in (186) to (208):
Kl(qτ )-1 q-1 T
j=0 l=0 f=1
T2 ≤ 2η2q2τ2Γ	E kVF(Xk)k2Fa
(209)
k=1
K
34
Published as a conference paper at ICLR 2021
Summing T1 and T2 , we obtain
K
K
K
2L2
2L2
2L2
-K- ΣEk Xk(I-A)kFa ≤ ^K^ΣT1 + ^K^ΣT2
(210)
k=1
k=1
≤ 4L2η2σ2q3τ3 (-
qτ
+ 4L2η2σ2 (占
k=1
1
----
K
Z2 + ⅛ + α⅛)P
τ2
(q-1)(2q +1) + (τ - 1)(2τ + 1)
6
6
P
1KN	2
+ 8L2η2q2τ2ΓK XXai (Pi(β - 1) + 1) E IlVF(Xki))|| .
k=1 i=1
(211)
Plugging T1 and T2 back into Lemma 4, we arrive at
N
K
K
1K
K ∑kVF(uk)k2
≤ 2 (F(XI) - Finf])
ηK
+ σ2
2L2
2L2
ηLΣ a2pi + ^^ΣT1 + 丁 ET2
i=1
k=1
KN
-K XXai ((4Pi- P2 - 2) - ηL (a,iPi(β + 1) - aipi + Py) ElVF(Xki))I
k=1
2
(212)
k=1 i=1
2(F(xι)- Finf])
ηK
+ 4L2η2σ2q3τ3 (-
qτ
+ 4L2η2σ2 (I-I)
N
+ σ2ηL X ai2pi
i=1
1
---
K
Z2 + A + 0⅛)P
τ2
(q-1)(2q +1) + (τ - 1)(2τ +1)
E
6
6
P
KN	2
-K XXai ((4pi - P2 - 2) - ηL (aipi (β + 1) - aiP2 + p2) - 8L2η2q2τ 2Γ) E∣∣VF (Xki)) ||
k=1 i=1
(213)
If η satisfies the following for i = 1, . . . , N,
(4pi — P2 — 2) ≥ ηL (aiPi(β + 1) — aiP2 + p2) + 8L2η2q2τ2Γ
then we can simplify (213):
(214)
E
1
m £ RF(Uk)Il2 ≤
K
2(F(xι)- Fin])
k=1
ηK
N
+ σ2ηL X ai2pi
i=1
K
殍 + ⅛ + o⅛)P
+ 4L2η2σ2q3τ3(ɪ - ɪ
qτ K
+ 4L2η2σ2 (占)(τ2
(q-1)(2q +1) + (τ - 1)(2τ +1)
P.
(215)
6
6
□
C.4 Comparison to Cooperative SGD
We note that when setting ai = 1/N and pi = 1 for all workers i, and setting q = 1, MLL-SGD
reduces to Cooperative SGD (Wang & Joshi, 2018). However, the bound in Theorem 1 differs when
compared to the bound of Cooperative SGD. Specifically, Theorem 1 has error terms dependent on
τ2 as opposed to τ .
This is due to the formulation of gk(i). Namely:
Ek [gk(i)] = PiEk [g(X(ki))]	(216)
= PiVF(X(ki)).	(217)
35
Published as a conference paper at ICLR 2021
Because we cannot assume pi = 1, there are cross terms in the expressions in equations (156)
and (173) that do not cancel out. Thus, we needed to use a more conservative analysis at these
steps on the proof. This is the reason that plugging in a value of pi = 1 is not enough to recover
the same bound as in Cooperative SGD. A similar discrepancy can be observed when comparing
with Koloskova et al. (2020).
36