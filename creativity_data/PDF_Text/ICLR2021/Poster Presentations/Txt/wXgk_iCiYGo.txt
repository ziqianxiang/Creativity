Published as a conference paper at ICLR 2021
A Diffusion Theory For Deep Learning Dynam-
ics: Stochastic Gradient Descent Exponen-
tially Favors Flat Minima
Zeke Xie1,2, Issei Sato 1,2, and Masashi Sugiyama2,1
1The University of Tokyo
2RIKEN Center for AIP
xie@ms.k.u-tokyo.ac.jp
{sato,sugi}@k.u-tokyo.ac.jp
Ab stract
Stochastic Gradient Descent (SGD) and its variants are mainstream methods for
training deep networks in practice. SGD is known to find a flat minimum that
often generalizes well. However, it is mathematically unclear how deep learning
can select a flat minimum among so many minima. To answer the question
quantitatively, we develop a density diffusion theory to reveal how minima selection
quantitatively depends on the minima sharpness and the hyperparameters. To the
best of our knowledge, we are the first to theoretically and empirically prove that,
benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD
favors flat minima exponentially more than sharp minima, while Gradient Descent
(GD) with injected white noise favors flat minima only polynomially more than
sharp minima. We also reveal that either a small learning rate or large-batch training
requires exponentially many iterations to escape from minima in terms of the ratio
of the batch size and learning rate. Thus, large-batch training cannot search flat
minima efficiently in a realistic computational time.
1	Introduction
In recent years, deep learning (LeCun et al., 2015) has achieved great empirical success in various
application areas. Due to the over-parametrization and the highly complex loss landscape of deep
networks, optimizing deep networks is a difficult task. Stochastic Gradient Descent (SGD) and its
variants are mainstream methods for training deep networks. Empirically, SGD can usually find flat
minima among a large number of sharp minima and local minima (Hochreiter & Schmidhuber, 1995;
1997). More papers reported that learning flat minima closely relate to generalization (Hardt et al.,
2016; Zhang et al., 2017a; Arpit et al., 2017; Hoffer et al., 2017; Dinh et al., 2017; Neyshabur et al.,
2017; Wu et al., 2017; Dziugaite & Roy, 2017; Kleinberg et al., 2018). Some researchers specifically
study flatness itself. They try to measure flatness (Hochreiter & Schmidhuber, 1997; Keskar et al.,
2017; Sagun et al., 2017; Yao et al., 2018), rescale flatness (Tsuzuku et al., 2019; Xie et al., 2020b),
and find flatter minima (Hoffer et al., 2017; Chaudhari et al., 2017; He et al., 2019b; Xie et al., 2020a).
However, we still lack a quantitative theory that answers why deep learning dynamics selects a flat
minimum.
The diffusion theory is an important theoretical tool to understand how deep learning dynamics
works. It helps us model the diffusion process of probability densities of parameters instead of model
parameters themselves. The density diffusion process of Stochastic Gradient Langevin Dynamics
(SGLD) under injected isotropic noise has been discussed by (Sato & Nakagawa, 2014; Raginsky
et al., 2017; Zhang et al., 2017b; Xu et al., 2018). Zhu et al. (2019) revealed that anisotropic diffusion
of SGD often leads to flatter minima than isotropic diffusion. A few papers has quantitatively
studied the diffusion process of SGD under the isotropic gradient noise assumption. Jastrzebski
et al. (2017) first studied the minima selection probability of SGD. Smith & Le (2018) presented
a Beyesian perspective on generalization of SGD. Wu et al. (2018) studied the escape problems of
1
Published as a conference paper at ICLR 2021
SGD from a dynamical perspective, and obtained the qualitative conclusion on the effects of batch
size, learning rate, and sharpness. Hu et al. (2019) quantitatively showed that the mean escape time
of SGD exponentially depends on the inverse learning rate. Achille & Soatto (2019) also obtained
a related proposition that describes the mean escape time in terms of a free energy that depends on
the Fisher Information. Li et al. (2017) analyzed Stochastic Differential Equation (SDE) of adaptive
gradient methods. Nguyen et al. (2019) mainly contributed to closing the theoretical gap between
continuous-time dynamics and discrete-time dynamics under isotropic heavy-tailed noise.
However, the related papers mainly analyzed the diffusion process under parameter-independent
and isotropic gradient noise, while stochastic gradient noise (SGN) is highly parameter-dependent
and anisotropic in deep learning dynamics. Thus, they failed to quantitatively formulate how SGD
selects flat minima, which closely depends on the Hessian-dependent structure of SGN. We try
to bridge the gap between the qualitative knowledge and the quantitative theory for SGD in the
presence of parameter-dependent and anisotropic SGN. Mainly based on Theorem 3.2 , we have four
contributions:
•	The proposed theory formulates the fundamental roles of gradient noise, batch size, the
learning rate, and the Hessian in minima selection.
•	The SGN covariance is approximately proportional to the Hessian and inverse to batch size.
•	Either a small learning rate or large-batch training requires exponentially many iterations to
escape minima in terms of ratio of batch size and learning rate.
•	To the best of our knowledge, we are the first to theoretically and empirically reveal that
SGD favors flat minima exponentially more than sharp minima.
2	Stochastic Gradient Noise and SGD Dynamics
We mainly introduce the necessary foundation for the proposed diffusion theory in this section. We
denote the data samples as {xj}jm=1, the model parameters as θ and the loss function over data
samples x as L(θ, x). For simplicity, we denote the training loss as L(θ). Following Mandt et al.
(2017), we may write SGD dynamics as
eiff,∕∖∖	Gτ∕n∖
θt+ι = θt - η W t = θt - η W t + ηC(θt)2Q,	(I)
∂θt	∂θt
where L(θ) is the loss of one minibatch, Zt 〜N(0, I), and C(θ) represents the gradient noise
covariance matrix. The classic approach is to model SGN by Gaussian noise, N(0, C(θ)) (Mandt
et al., 2017; Smith & Le, 2018; Chaudhari & Soatto, 2018).
Stochastic Gradient Noise Analysis. We first note that the SGN we study is introduced by minibatch
training, C(θt) 1 Zt = "Lθθt —吗；?, which is the difference between gradient descent and stochastic
gradient descent. According to Generalized Central Limit Theorem (Gnedenko et al., 1954), the
mean of many infinite-variance random variables converges to a stable distribution, while the mean
of many finite-variance random variables converges to a Gaussian distribution. As SGN is finite in
practice, we believe the Gaussian approximation of SGN is reasonable.
Simsekli et al. (2019) argued that SGN is Levy noise (stable variables), rather than Gaussian noise.
They presented empirical evidence showing that SGN seems heavy-tailed, and the heavy-tailed
distribution looks closer to a stable distribution than a Gaussian distribution. However, this research
line (Simsekli et al., 2019; Nguyen et al., 2019) relies on a hidden strict assumption that SGN must
be isotropic and obey the same distribution across dimensions. Simsekli et al. (2019) computed
“SGN” across n model parameters and regarded “SGN" as n samples drawn from a single-variant
distribution. This is why one tail-index for all parameters was studied in Simsekli et al. (2019). The
arguments in Simsekli et al. (2019) did not necessarily hold for parameter-dependent and anisotropic
Gaussian noise. In our paper, SGN computed over different minibatches obeys a n-variant Gaussian
distribution, which can be parameter-dependent and anisotropic.
In Figure 1, we empirically verify that SGN is highly similar to Gaussian noise instead of heavy-tailed
Levy noise. We recover the experiment of Simsekli et al. (2019) to show that gradient noise is
approximately Levy noise only if it is computed across parameters. Figure 1 actually suggests that
2
Published as a conference paper at ICLR 2021
0.20
(a) “SGN" across Parame- (b) L6vy noise
1""s-
D.10	0.15
NoteeNoim
ters
Noise Norm
0.000 QMS 0.010 OAlS 0S2Q 0.025 QA30 0.035 QMO
NolseNorm
0∞
Nals« Nann
(c) SGN across mini- (d) Gaussian noise
batches
Figure 1: The Stochastic Gradient Noise Analysis. The histogram of the norm of the gradient noises
computed with the three-layer fully-connected network on MNIST (LeCun, 1998). (a) and (c): the
histograms of the norms of two kinds of gradient noise: (a) “SGN” is computed over parameters,
which is actually stochastic gradient rather than SGN; (c) SGN is computed over minibatches. (b)
and (d): the histograms of the norms of (scaled) Gaussian noise and Levy noise. Based on (a) and (b),
Simsekli et al. (2019) argued that gradient noise across parameters is heavy-tailed LeVy noise. Based
on (c) and (d), we show that SGN without the isotropic restriction is approximately Gaussian.
the contradicted observations are from the different formulations of gradient noise. Simsekli et al.
(2019) studied the distribution of SGN as a single-variant distribution, while we relax it as a n-variant
distribution. Our empirical analysis in Figure 1 holds well at least when the batch size B is larger than
16, which is common in practice. Similar empirical evidence can be observed for training ResNet18
(He et al., 2016) on CIFAR-10 (Krizhevsky et al., 2009), seen in Appendix C.
Panigrahi et al. (2019) also observed that for batch sizes 256 and above, the distribution of SGN
is best described as Gaussian at-least in the early phases of training. Comparing our results with
Panigrahi et al. (2019), we noticed that the Gaussianity of SGN may depend on more unknown factors.
First, SGN on random models is more Gaussian than well-trained models. Second, the layer/network
matters. Because SGN on some layers/networks is more Gaussian than other layers/networks.
The isotropic gradient noise assumption is too rough to capture the Hessian-dependent covariance
structure of SGN, which we will study in Figure 2 later. Our theory that focuses on parameter-
dependent and anisotropic SGN brings a large improvement over existing parameter-independent
and isotropic noise, although Simsekli et al. (2019) brought an improvement over more conventional
parameter-independent and isotropic Gaussian noise. A more sophisticated theory is interesting under
parameter-independent anisotropic heavy-tailed noise, when the batch size is too small (B 〜1) to
apply Central Limit Theorem. We will leave it as future work.
SGD Dynamics. Let us replace η by dt as unit time. Then the continuous-time dynamics of SGD
(Coffey & Kalmykov, 2012) is written as
dθ = - dL(θ) dt + [2D(θ)]2 dWt,
∂θ
(2)
where dWt ~ N(0, Idt) and D(θ) = 2C(θ). We note that the dynamical time t in the continuous-
time dynamics is equal to the product of the number of iterations T and the learning rate η: t = ηT.
The associated Fokker-Planck Equation is written as
dP∂θ,t) =V∙ [P(θ,t)VL(θ)] + V ∙ VD(θ)P(θ, t)	(3)
=X 焉[p (θ,t) * ] + XX ∂⅛ Dj (θ)P (θ,t),	(4)
i " L	"」 i j "J
where V is a nabla operator, and Dij is the element in the ith row and jth column of D. In standard
SGLD, the injected gradient noise is fixed and isotropic Gaussian, D = I.
3
Published as a conference paper at ICLR 2021
(a) Pretrained Model
(b) Pretrained Model
(c) Random Model
(d) Random Model
Figure 2: We empirically verified C(θ) = HBθ) by using three-layer fully-connected network on
MNIST (LeCun, 1998). The pretrained Models are usually near critical points, while randomly
Initialized Models are far from critical points. We display all elements H(i,j) ∈ [1e - 4, 0.5] of the
Hessian matrix and the corresponding elements C(i,j) of gradient noise covariance matrix in the
space spanned by the eigenvectors of Hessian. Another supplementary experiment on Avila Dataset
(De Stefano et al., 2018) in Appendix C reports Cavila ≈ 1.004H. The small difference factor
between the empirical result and the ideal Equation is mainly because the pretrained network is not
perfectly located at a critical point.
The next question is how to formulate the SGN covariance C(θ) for SGD? Based on Smith & Le
(2018), we can express the SGN covariance as
C⑹=B
m
—X VL(θ, Xj)VL(θ, Xj)> - VL(θ)VL(θ)>
m j=1
≈ Bm X vL(θ,χj )vL(θ, xj )>.
j=1
(5)
The approximation is true near critical points, due to the fact that the gradient noise variance dominates
the gradient mean near critical points. We know the observed fisher information matrix satisfies
FIM(θ) ≈ H(θ) near minima, referring to Chapter 8 of (PaWitan, 2001). Following JaStrzebSki et al.
(2017); Zhu et al. (2019), we obtain
C(θ) ≈ ； X VL(θ, Xj)VL(θ, Xj)> = ⅛FIM(θ) ≈ ɪH(θ),	(6)
Bm	B	B
j=1
which approximately gives
D(θ)
2。⑹=2BH⑻
(7)
near minima. It indicates that the SGN covariance C(θ) is approximately proportional to the
Hessian H(θ) and inverse to the batch size B. Obviously, we can generalize Equation 7 by
D(θ) = ηC(θ) = 2B[H(θ)]+ near critical points, when there exist negative eigenvalues in H
along some directions. We use [∙]+ to denote the positive semidefinite transformation of a sym-
metric matrix: if we have the eigendecomposation H = Udiag(Hι, ∙∙∙ , Hn-ι, Hn)U>, then
[H]+ = Udiag(∣Hι∣,…，∣Hn-ι∣, |Hn|)U>.
We empirically verify this relation in Figure 2 for pretrained fully-connected networks, and a follow-
up paper Xie et al. (2020c) first verified this relation for randomly initialized fully-connected networks
on real-world datasets. The Pearson Correlation is up to 0.999 for pretrained networks. We note that,
the relation still approximately holds for even the randomly network, which is far from critical points.
The correlation is especially high along the flat directions with small-magnitude eigenvalues of the
Hessian (Xie et al., 2020c). We emphasize that previous papers with the isotropic LeVy or Gaussian
noise approximation all failed to capture this core relation in deep learning dynamics.
3	SGD Diffusion Theory
We start the theoretical analysis from the classical Kramers Escape Problem (Kramers, 1940). We
assume there are two valleys, Sharp Valley a1 and Flat Valley a2, seen in Figure 3. Also Col b is the
4
Published as a conference paper at ICLR 2021
(a) 1-Dimensional Escape
(b) High-Dimensional Escape
Figure 3: Kramers Escape Problem. a1 and aa are minima of two neighboring valleys. b is the saddle
point separating the two valleys. c locates outside of Valley a1.
boundary between two valleys. What is the mean escape time for a particle governed by Equation 2
to escape from Sharp Valley a1 to Flat Valley a2 ? The mean escape time is widely used in related
statistical physics and stochastic process (Van Kampen, 1992; Nguyen et al., 2019).
Gauss’s Divergence Theorem (Arfken & Weber, 1999; Lipschutz et al., 2009) states that the surface
integral of a vector field over a closed surface, which is called the flux through the surface, is equal
to the volume integral of the divergence over the region inside the surface. We respectively denote
the mean escape time as τ, the escape rate as γ, and the probability current as J. We apply Gauss’s
Divergence Theorem to the Fokker-Planck Equation resulting in
V ∙ [P(θ, t)VL(θ)] + V∙VD(θ)P(θ,t)= dP∂θ,t) = -v∙ J(θ,t).
The mean escape time is expressed (Van Kampen, 1992) as
1	P(θ ∈ Va)
=—Σ--------.
Y	RSaJ ∙ dS
(8)
(9)
where P(θ ∈ Va) = V P (θ)dV is the current probability inside Valley a, J is the probability
current produced by the probability source P(θ ∈ Va), j = JS J ∙ dS is the probability flux (surface
integrals of probability current), Sa is the surface (boundary) surrounding Valley a, and Va is the
volume surrounded by Sa . We have j = J in the case of one-dimensional escape.
Classical Assumptions. We state three classical assumptions first for the density diffusion theory.
Assumption 1 is the common second order Taylor approximation, which was also used by (Mandt
et al., 2017; Zhang et al., 2019). Assumptions 2 and 3 are widely used in many fields’ Kramers
Escape Problems, including statistical physics (Kramers, 1940; Hanggi, 1986), chemistry (Eyring,
1935; Hanggi et al., 1990), biology (Zhou, 2010), electrical engineering (Coffey & Kalmykov, 2012),
and stochastic process (Van Kampen, 1992; Berglund, 2013). Related machine learning papers
OaStrzebSki et al., 2017) usually used Assumptions 2 and 3 as the background of Kramers Escape
Problems.
Assumption 1 (The Second Order Taylor Approximation). The loss function around critical points
θ? can be approximately written as
L(θ) = L(θ?) + g(θ*)(θ — θ?) + 1(O — θ*)>H (θ*)(θ — θ?).
Assumption 2 (Quasi-Equilibrium Approximation). The system is in quasi-equilibrium near minima.
Assumption 3 (Low Temperature Approximation). The gradient noise is small (low temperature).
We will dive into these two assumptions deeper than previous papers for SGD dynamics. Assumptions
2 and 3 both mean that our diffusion theory can better describe the escape processes that cost more
iterations. As this class of “slow” escape processes takes main computational time compared with
“fast” escape processes, this class of “slow” escape process is more interesting for training of deep
neural networks. Our empirical analysis in Section 4 supports that the escape processes in the
5
Published as a conference paper at ICLR 2021
wide range of iterations (50 to 100,000 iterations) can be modeled by our theory very well. Thus,
Assumption 2 and 3 are reasonable in practice. More discussion can be found in Appendix B.
Escape paths. We generalize the concept of critical points into critical paths as the path where 1)
the gradient perpendicular to the path direction must be zero, and 2) the second order directional
derivatives perpendicular to the path direction must be nonnegative. The Most Possible Paths (MPPs)
for escaping must be critical paths. The most possible escape direction at one point must be the
direction of one eigenvector of the Hessian at the point. Under Assumption 3, the probability density
far from critical points and MPPs is very small. Thus, the density diffusion will concentrate around
MPPs. Draxler et al. (2018) reported that minima in the loss landscape of deep networks are connected
by Minimum Energy Paths (MEPs) that are essentially flat and Local MEPs that have high-loss saddle
points. Obviously, MPPs in our paper correspond to Local MEPs. The density diffusion along MEPs,
which are strictly flat, is ignorable according to our following analysis.
The boundary between Sharp Valley a1 and Flat Valley a2 is the saddle point b. The Hessian at b, Hb,
must have only one negative eigenvalue and the corresponding eigenvector is the escape direction.
Without losing generality, we first assume that there is only one most possible path through Col b
existing between Sharp Valley a1 and Flat Valley a2.
SGLD diffusion. We first analyze a simple case: how does SGLD escape sharp minima? Researchers
are interested in SGLD, when the injected noise dominates SGN as η → 0 in final epochs. Because
SGLD may work as a Bayesian inference method in this limit (Welling & Teh, 2011). SGLD is
usually simplified as Gradient Descent with injected white noise, whose behavior is identical to
Kramers Escape Problem with thermo noise in statistical physics. We present Theorem 3.1. We leave
the proof in Appendix A.1. We also note that more precise SGLD diffusion analysis should study a
mixture of injected white noise and SGN.
Theorem 3.1 (SGLD Escapes Minima). The loss function L(θ) is of class C2 and n-dimensional.
Only one most possible path exists between Valley a and the outside of Valley a. If Assumption 1, 2,
and 3 hold, and the dynamics is governed by SGLD, then the mean escape time from Valley a to the
outside of Valley a is
-det(Hb) 1	(∆L∖
det(Ha) ∣H□ ex八ɪj .
We denote that Ha and Hb are the Hessians of the loss function at the minimum a and the saddle
point b, ∆L = L(b) - L(a) is the loss barrier height, e indicates the escape direction, and Hbe is
the eigenvalue of the Hessian Hb corresponding to the escape direction. The diffusion coefficient D
is usually set to 1 in SGLD.
T = 1 = 2πV
SGD diffusion. However, SGD diffusion is essentially different from SGLD diffusion in several
aspects: 1) anisotropic noise, 2) parameter-dependent noise, and 3) the stationary distribution of SGD
is far from the Gibs-Boltzmann distribution, P(θ)=十 exp (-L(θ)). These different characteristics
make SGD diffusion behave differently from known physical dynamical systems and much less
studied than SGLD diffusion. We formulate Theorem 3.2 for SGD. We leave the proof in Appendix
A.2.The theoretical analysis of SGD can be easily generalized to the dynamics with a mixture of SGN
and injected white noise, as long as the eigenvectors of D(θ) are closely aligned with the eigenvectors
of H(θ).
Theorem 3.2 (SGD Escapes Minima). The loss function L(θ) is of class C2 and n-dimensional.
Only one most possible path exists between Valley a and the outside of Valley a. If Assumption 1, 2,
and 3 hold, and the dynamics is governed by SGD, then the mean escape time from Valley a to the
outside of Valley a is
T=2π ∣He∣exp
2B∆L
η
+ ⅛≡制
where s ∈ (0, 1) is a path-dependent parameter, and Hae and Hbe are, respectively, the eigenvalues
of the Hessians at the minimum a and the saddle point b corresponding to the escape direction e.
Multiple-path escape. Each escape path contributes to the total escape rate. Multiple paths combined
together have a total escape rate. If there are multiple parallel from the start valley to the end valley,
we can compute the total escape rate easily based on the following computation rule. The computation
6
Published as a conference paper at ICLR 2021
⑶-log(γ) = O( 1)	(b) - log(γ)=O(B)	(c) - log(γ) = O( 1)
Figure 4:	The mean escape time analysis of SGD by using Styblinski-Tang Function. The Pearson
Correlation is higher than 0.99. Left Column: Sharpness. Middle Column: Batch Size. Right Column:
Learning Rate.
rule is based on the fact that the probability flux integrals are additive. We can easily generalize the
mean escape time analysis into the cases that there are multiple parallel escape paths indexed by p.
As for multiple-valley escape problems, we can always reduce a multiple-valley escape problem into
multiple two-valley escape problems. We also note that, while Theorem A.2 does not depend the
dimensionality directly, higher dimensionality may increase the number of escape paths and loss
valleys, and change the spectrum of the Hessians.
Rule 1. If there are multiple MPPs between the start valley and the end valley, then γtotal =	p γp.
Thus, we only need to find the saddle points that connect two valleys as we analyzed in the paper and
analyze the escape rates.
Minima selection. Now, we may formulate the probability of minima selection as Proposition 1.
We leave the proof in Appendix A.3. In deep learning, one loss valley represents one mode and
the landscape contain many good modes and bad modes. SGD transits from one mode to another
mode during training. The mean escape time of one mode corresponds to the number of iterations
which SGD spends on this mode during training, which is naturally proportional to the probability of
selecting this mode after training.
Proposition 1. Suppose there are two valleys connected by an escape path. If all assumptions of
Theorem 3.2 hold, then the stationary distribution of locating these valleys is given by
P(θ ∈ Va) =	,
v τv
where v is the index of valleys, and τv is the mean escape time from Valley v to the outside of Valley v.
4 Empirical Analysis
In this section, we try to directly validate the escape formulas on real-world datasets. Each escape
process, from the inside of loss valleys to the outside of loss valleys, are repeatedly simulated for 100
times under various gradient noise scales, batch sizes, learning rates, and sharpness.
HoW to compare the escape rates under the same settings with various minima sharpness? Our
method is to multiply a rescaling factor √k to each parameter, and the Hessian will be proportionally
rescaled by a factor k. If we let L(θ) = f(θ) → L(θ) = f (√kθ), then H(θ) = V2f (θ) → H(θ)=
kV2f (θ). Thus, we can use k to indicate the minima sharpness. The theoretical relations of SGD
we try to validate can be formulated as: (1) — lοg(γ) = O( 1), (2) — lοg(γ) = O(B), and (3)
-lOg(Y) = O( η).
The mean escape time analysis of SGD. Styblinski-Tang Function, which has multiple minima and
saddle points, is a common test function for nonconvex optimization. We conduct an intuitional
10-dimensional experiment, where the simulations start from a given minimum and terminate when
reaching the boundary of the loss valley. The number of iterations is recorded for calculating
7
Published as a conference paper at ICLR 2021
⑶-IOg(Y) = O(1)
(b) - log(γ) = O(B)
(C)-Iog(Y) = O( 1)
Figure 5:	The mean escape time analysis of SGD by training neural networks on Avila Dataset. Left
Column: Sharpness. Middle Column:BatCh Size. Right Column: Learning Rate. We leave the results
on Banknote AuthentiCation, CardiotoCography, and Sensorless Drive Diagnosis in Appendix D.
(a) Y = O(k)
(C) Y = O(k)
(d) -lOg(Y) = O(D)
Figure 6:	The mean esCape time analysis of SGLD. Subfigure (a) and (b): Styblinski-Tang FunCtion.
Subfigure (C) and (d): Neural Network.
the esCape rate. We also train fully ConneCted networks on four real-world datasets, inCluding a)
Avila, b) Banknote AuthentiCation, C) CardiotoCography, d) Dataset for Sensorless Drive Diagnosis
(De Stefano et al., 2018; Dua & Graff, 2017). Figure 4 and Figure 5 Clearly verifies that the esCape
rate exponentially depends on the minima sharpness (refleCted by k), the batCh size, and the learning
rate on both test funCtions and real-world training, whiCh fully supports our theoretiCal results.
Model arChiteCture and details: We used fully-ConneCted networks with the depth 2 and the width 10
in Figure 5. The experiments using LogistiC Regression and Fully-ConneCted networks with the depth
3 are presented in Appendix E. We leave more experimental details and results in Appendix D.1 and
Appendix E.
The mean escape time analysis of SGLD. We try to validate Y = O(k) and - log(γ) = O(D) for
SGLD (dominated by injeCted Gaussian noise). Figure 6 shows that SGLD only favors flat minima
polynomially more than sharp minima as Theorem 3.1 indiCates. Figure 6 also verifies that the
injeCted gradient noise sCale exponentially affeCts flat minima seleCtion.
8
Published as a conference paper at ICLR 2021
5	Discussion
SGD favors flat minima exponentially more than sharp minima. We can discover a few inter-
esting insights about SGD by Theorem 3.2. Most importantly, the mean escape time exponentially
depends on the eigenvalue of the Hessian at minima along the escape direction, Hae . Thus, SGD
favors flat minima exponentially more than sharp minima. We claim one main advantage of SGD
comes from the exponential relation of the mean escape time and the minima sharpness. The measure
of “sharpness” has reformed in contexts of SGLD and SGD. In the context of SGLD, the “sharpness”
is quantified by the determinant of the Hessian. In the context of SGD, the “sharpness” is quantified
by the top eigenvalues of the Hessian along the escape direction. Based on the proposed diffusion
theory, recent work (Xie et al., 2020c) successfully proved that SGD favors flat minima significantly
more than Adam.
The ratio of the batch size and the learning rate exponentially matters. Theorem 3.2 explains
why large-batch training can easily get trapped near sharp minima, and increasing the learning rate
proportionally is helpful for large-batch training (Krizhevsky, 2014; Keskar et al., 2017; Sagun
et al., 2017; Smith et al., 2018; Yao et al., 2018; He et al., 2019a). We argue that the main cause
is large-batch training expects exponentially longer time to escape minima. Note that, as the mean
escape time in the theorems is equivalent to the product of the learning rate and the number of
iterations, both the number of iterations and dynamical time exponentially depend on the ratio of the
batch size and the learning rate. The practical computational time in large-batch training is usually
too short to search many enough flat minima. We conjecture that exponentially increasing training
iterations may be helpful for large batch training, while this is often too expensive in practice.
Low dimensional diffusion. Most eigenvalues of the Hessian at the loss landscape of over-
parametrized deep networks are close to zero, while only a small number of eigenvalues are large
(Sagun et al., 2017; Li et al., 2018). Zero eigenvalues indicate zero diffusion along the corresponding
directions. Thus, we may theoretically ignore these zero-eigenvalue directions. This also indicates
that the density diffusion is ignorable along an essentially flat MEP in Draxler et al. (2018).
As the escape rate exponentially depends the corresponding eigenvalues, a small number of large
eigenvalues means that the process of minima selection mainly happens in the relatively low di-
mensional subspace corresponding to top eigenvalues of the Hessian. Gur-Ari et al. (2018) also
reported a similar finding. Although the parameter space is very high-dimensional, SGD dynamics
hardly depends on those “meaningless” dimensions with small second order directional derivatives.
This novel characteristic of SGD significantly reduces the explorable parameter space around one
minimum into a much lower dimensional space.
High-order effects. As we have applied the second-order Taylor approximation near critical points,
our SGD diffusion theory actually excludes the third-order and higher-order effect. The asymmetric
valley in He et al. (2019b), which only appears in high-order analysis, is beyond the scope of this
paper. However, we also argue that the third-order effect is much smaller than the second-order effect
under the low temperature assumption in Kramers Escape Problems. We will leave the more refined
high-order theory as future work.
6	Conclusion
In this paper, we demonstrate that one essential advantage of SGD is selecting flat minima with an
exponentially higher probability than sharp minima. To the best of our knowledge, we are the first to
formulate the exponential relation of minima selection to the minima sharpness, the batch size, and
the learning rate. Our work bridges the gap between the qualitative knowledge and the quantitative
theoretical knowledge on the minima selection mechanism of SGD. We believe the proposed theory
not only helps us understand how SGD selects flat minima, but also will provide researchers a
powerful theoretical tool to analyze more learning behaviors and design better optimizers in future.
Acknowledgement
We thanks Dr. Yuanqian Tang for helpful discussion. MS was supported by the International Research
Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study.
9
Published as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Where is the information in a deep neural network? arXiv
preprint arXiv:1905.12213, 2019.
George B Arfken and Hans J Weber. Mathematical methods for physicists, 1999.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, MaXinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep networks. In International Conference on Machine Learning, pp. 233-242,
2017.
Nils Berglund. Kramers’ law: Validity, derivations and generalisations. Markov Processes and
Related Fields, 19(3):459-490, 2013.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. In International Conference on Learning Representations, 2017.
William Coffey and Yu P Kalmykov. The Langevin equation: with applications to stochastic problems
in physics, chemistry and electrical engineering, volume 27. World Scientific, 2012.
Claudio De Stefano, Marilena Maniaci, Francesco Fontanella, and A Scotto di Freca. Reliable
writer identification in medieval manuscripts through page layout features: The “avila” bible case.
Engineering Applications of Artificial Intelligence, 72:99-110, 2018.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In International Conference on Machine Learning, pp. 1019-1028, 2017.
FeliX DraXler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In International Conference on Machine Learning, pp.
1309-1318, 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Henry Eyring. The activated compleX in chemical reactions. The Journal of Chemical Physics, 3(2):
107-115, 1935.
BV Gnedenko, AN Kolmogorov, BV Gnedenko, and AN Kolmogorov. Limit distributions for sums
of independent. Am. J. Math, 105, 1954.
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
Peter Hanggi. Escape from a metastable state. Journal of Statistical Physics, 42(1-2):105-148, 1986.
Peter Hanggi, Peter Talkner, and Michal Borkovec. Reaction-rate theory: fifty years after kramers.
Reviews of modern physics, 62(2):251, 1990.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
FengXiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems,
pp. 1141-1150, 2019a.
10
Published as a conference paper at ICLR 2021
Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima.
In Advances in Neural Information Processing Systems 32,pp. 2549-2560. 2019b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
Advances in neural information processing systems, pp. 529-536, 1995.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1729-1739, 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex
stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1):3-32, 2019.
StaniSlaW JaStrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, ASja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative vieW: When does sgd escape local
minima? In International Conference on Machine Learning, pp. 2703-2712, 2018.
Hendrik Anthony Kramers. BroWnian motion in a field of force and the diffusion model of chemical
reactions. Physica, 7(4):284-304, 1940.
Alex Krizhevsky. One Weird trick for parallelizing convolutional neural netWorks. arXiv preprint
arXiv:1404.5997, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun. The mnist database of handWritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Advances in Neural Information Processing Systems, pp. 6389-6399, 2018.
Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient
algorithms. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2101-2110. JMLR. org, 2017.
Seymour Lipschutz, Murray R Spiegel, and Dennis Spellman. Vector analysis and an introduction to
tensor analysis. McGraW-Hill, 2009.
Stephan Mandt, MattheW D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949-5958,
2017.
Thanh Huy Nguyen, UmUt Simsekli, Mert Gurbuzbalaban, and Gael Richard. First exit time analysis
of stochastic gradient descent under heavy-tailed gradient noise. In Advances in Neural Information
Processing Systems, pp. 273-283, 2019.
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli. Non-gaussianity of
stochastic gradient noise. arXiv preprint arXiv:1910.09626, 2019.
11
Published as a conference paper at ICLR 2021
Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood. Oxford
University Press, 2001.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pp.
1674-1703, 2017.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient langevin dynamics
by using fokker-planck equation and ito process. In International Conference on Machine Learning,
pp. 982-990, 2014.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient
noise in deep neural networks. In International Conference on Machine Learning, pp. 5827-5837,
2019.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018.
Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le. Don’t decay the learning rate, increase the
batch size. In International Conference on Learning Representations, 2018.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale
invariant definition of flat minima for neural networks using pac-bayesian analysis. arXiv preprint
arXiv:1901.04653, 2019.
Nicolaas Godfried Van Kampen. Stochastic processes in physics and chemistry, volume 1. Elsevier,
1992.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of
loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Lei Wu, Chao Ma, and E Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, and Masashi Sugiyama. Artificial
neural variability for deep learning: On overfitting, noise memorization, and catastrophic forgetting.
arXiv preprint arXiv:2011.06220, 2020a.
Zeke Xie, Issei Sato, and Masashi Sugiyama. Stable weight decay regularization. arXiv preprint
arXiv:2011.11152, 2020b.
Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adai: Separating the
effects of adaptive learning rate and momentum inertia. arXiv preprint arXiv:2006.15815, 2020c.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics
based algorithms for nonconvex optimization. In Advances in Neural Information Processing
Systems, pp. 3122-3133, 2018.
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. In Advances in Neural Information Processing
Systems, pp. 4949-4959, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Machine Learning,
2017a.
12
Published as a conference paper at ICLR 2021
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights from
a noisy quadratic model. In Advances in Neural Information Processing Systems, pp. 8196-8207,
2019.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. In Conference on Learning Theory, pp. 1980-2022, 2017b.
Huan-Xiang Zhou. Rate theories for biologists. Quarterly reviews of biophysics, 43(2):219-293,
2010.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In ICML,
pp. 7654-7663, 2019.
A	Proofs
A.1 Proof of Theorem 3.1
Proof. This proposition is a well known conclusion in statistical physics under Assumption 1, 2 and
3. We still provide an intuitional proof here, and the following proof of SGD Diffusion will closely
relate to this proof. We decompose the proof into two steps: 1) compute the probability of locating in
valley a, P(θ ∈ Va), and 2) compute the probability flux j = JS J ∙ dS.
Without losing generality, we first prove the one-dimensional case.
Step 1: Under Assumption 1, the stationary distribution around minimum a is P(θ) =
P (a) exp[- L⑻TLs) ], where T = D. Under Assumption 3, We may only consider the second
order Taylor approximation of the density function around critical points. We use the T notation
as the temperature parameter in the stationary distribution, and use the D notation as the diffusion
coefficient in the dynamics, for their different roles.
P(θ∈Va)			(10)
=	P (θ)dV θ∈Va			(11)
=	P (a) exp θ∈Va	L(θ) - L(a) — T	dθ	(12)
=P (a)	exp	1 (θ — a)> Hα(θ — a) + O(∆θ3) — T		dθ	(13)
_Pz √2∏T)2 =P (a)	1	.			(14)
Ha
Step 2:
J =P (θ)VL(θ) + P (θ)VD + DVP (θ)
J =P (θ) VL(θ) + VD - VD = (T - 1)VL Apply this result to the Fokker-Planck Equation 4, we have V∙V[D(θ)P (θ,t)] =V∙ DVP(θ,t) + V∙ (T - 1)	D VL ⑹)	(16) (17) (18) VL(θ) P(θ, t)	(19)
13
Published as a conference paper at ICLR 2021
And thus we obtain the Smoluchowski equation and a new form of J
dP∂θ^) = S DGVL(θ)+ V) P(θ,t = -V∙ J(θ,t),	(20)
J(θ) = D exp (-LTθ)) V 卜Xp (Lθ) P(θ)卜	(21)
We note that the probability density outside Valley a must be zero, P(c) = 0. As we want to compute
the probability flux escaping from Valley a in the proof, the probability flux escaping from other
valleys into Valley a should be ignored. Under Assumption 2, we integrate the equation from Valley a
to the outside of Valley a along the most possible escape path
J
(22)
(23)
(24)
(25)
We move J to the outside of integral based on Gauss’s Divergence Theorem, because J is fixed
on the escape path from one minimum to another. As there is no field source on the escape path,
JV V ∙ J(θ)dV = 0. Then VJ(θ) = 0. Obviously, only minima are probability sources in deep
learning. Under Assumption 3 and the second-order Taylor approximation, we have
(26)
(27)
(28)
(29)
Based on the results of Step 1 and Step 2, we obtain
γ
J
P(θ ∈ Va)	P(θ∈ Va)
DP(a) exp (LTa))
D√Ha∣HbI	( ∆Lab
--------exp L—
∆Lab
2π	exp(- ɪ)
2πT
√Ha∣Hbi
(30)
(31)
(32)
(33)
1
We generalize the proof of one-dimensional diffusion to high-dimensional diffusion
14
Published as a conference paper at ICLR 2021
Step 1:
P (θ ∈ Va)
=	P (θ)dV
/	「L(θ)- L(a)]m∕
=Je	P (a) exp [-----T-----] dV
m、f	1 1 (θ - a)>Ha(θ - a)+ O(∆θ3)] W
=P(a)	exp   --------------不-------------dV
θ∈Va	T
W、(2∏T)2
=P (a)-------F
det(Ha)2
Step 2: Based on the formula of the one-dimensional probability current and flux, we obtain
Sb
J ∙ dS
=Jb / exp [- 11" TI" bb
Sb	T
(2πT)中
b (Qn=ι1 Hbi) 1
So we have
τ =2π
Qn-II Hbi
det(Ha)∣Hbe∣
exp
=2π, F det(Hb),exp (∆L
=V det(Ha) |Hbe| PI D
(34)
(35)
(36)
(37)
(38)
(39)
(40)
(41)
(42)
(43)
□
A.2 Proof of Theorem 3.2
Proof. We decompose the proof into two steps and analyze the one-dimensional case like before.
The following proof is similar to the proof of SGLD except that we make Ta the temperature near the
minimum a and Tb the temperature near the saddle point b.
One-dimensional SGD Diffusion:
Step 1: Under Assumption 3, we may only consider the second order Taylor approximation of the
density function around critical points.
P(θ ∈ Va )			(44)
=	P (θ)dV			(45)
θ∈Va =	P (a) exp	L(θ) - L(a) — [	Ta	]	dV	(46)
θ∈Va =P (a)	exp	1 (θ — a)> Ha (θ — a) + O(∆θ3) — Ta		dθ	(47)
_PZ √2∏Ta)1 =P(a)	ι Ha			(48)
Step 2:
J =P (θ)VL(θ) + P (θ)VD + DVP (θ)	(49)
J =P(θ) VL(θ) + VD - DVL(θ) - DL(θ)V (T)	(50)
15
Published as a conference paper at ICLR 2021
According to Equation 7, V (1) is ignorable near the minimum a and the col b, thus
Apply this result to the Fokker-Planck Equation 4, we have
V∙V[D(θ)P (θ,t)]	(52)
=V∙ DVP(θ,t) + V∙ (T - 1)VL(θ) P(θ,t)	(53)
And thus we obtain the Smoluchowski equation and a new form of J
dP(θΓ) = VjD (VVL(θ) + V) P(θ, t)l = -V∙ J,	(54)
∂t	T
We note that the Smoluchowski equation is true only near critical points. We assume the point s is
the midpoint on the most possible path between a and b, where L(s) = (1 - s)L(a) + sL(b). The
temperature Ta dominates the path a → s, while temperature Tb dominates the path s → b. So we
have
V exp :⑹ T L(S)) P(θ) = JDTeXp (L⑹ T L(S)) .	(56)
Under Assumption 2, we integrate the equation from Valley a to the outside of Valley a along the
most possible escape path
Left = ZC ∂∂θ[exp (L(θ) T L(S)) P(θ)]dθ
a ∂ θ	T
=aS ∂ K (一)P 小
+ Zs ∂θ K ( T )P 小
= [P(s) - exp ( Laa- L(S) ) P(α)] + [0 - P(s)]
L L(a) - L(S) A p>∕、
=- eχp I ——T-- ) P(α)
Right = - J/c DTeXp :(θ) - L(S)) dθ
(57)
(58)
(59)
(60)
(61)
(62)
We move J to the outside of integral based on Gauss’s Divergence Theorem, because J is fixed
on the escape path from one minimum to another. As there is no field source on the escape path,
JV V ∙ J(θ)dV = 0 and VJ(θ) = 0. Obviously, only minima are probability sources in deep learning.
So we obtain
exp (Ls)T-L(S)) P(α)
Ra DT exp (Lθ-L(s)dθ
(63)
16
Published as a conference paper at ICLR 2021
Under Assumption 3, we have
a
≈Za
D-1 exp
D-1 exp
W 5
dθ
,T d
L(b) — L(S) + 2 (θ — b)> Hb(θ — b)
Tb
dθ
(64)
(65)
Z+∞
exp
∞
=Db-1 exp
- L(s)
Tb
L(b) — L(S) + 2 (θ — b)>Hb(θ — b)
Tb
2∏Tb
K.
Based on the results of Step 1 and Step 2, we have
γ
J
(θ∈Va) P (θ∈Va)
P (a) exp ( L⑺TFs'
1
dθ
(66)
(67)
(68)
So we have
D-IeXp (L(⅞Ls)
VZTbHa |Hb|
—----ɪ=- exp
2∏√τa
PTbHa∣Hb∣
-----— exp
2∏√τ;
—
)q2Hb p(a)q2Ha
L(s) - L(a)	L(b) - L(s)
—
Ta	Tb
s∆L (1 - s)∆L\
Ta
Tb
(69)
(70)
(71)
T=γ=2π∖O⅛ exp
s∆L + (1 -
s)∆L
Tb
(72)
In the Case of pure SGN, Ta = 2B Ha and Tb = - 2B Hb gives
11
T=γ = 2π 两exp
2B∆L s (1 - s)
—(H+哥)卜
(73)
We generalize the proof above into the high-dimensional SGD diffusion.
Step 1:
P (θ ∈ Va)
= P (θ)dV
=P (a) exp
θ∈Va
(74)
(75)
=P (a)
(2π)2
1	丁 _i	_i
-ɔ(θ - a)>(D-2HaD-2)(θ - a) dV
2
(76)
det(D-1Ha)1
(77)
Step 2: Based on the formula of the one-dimensional probability Current and flux, we obtain the
high-dimensional flux esCaping through Col b:
J J ∙ dS
Sb
=J1d
Sb
=J1d
exp -∣(θ - b)>[D-2HbD-2]⊥e(θ - b) dS
(2π)n-1
(Qi=e(D-1Hbi)) 1 ,
(78)
(79)
(80)
17
Published as a conference paper at ICLR 2021
where [∙]⊥e indicates the directions perpendicular to the escape direction e. So We have
1 / det(HaD-1)	S s∆L	(1 - s)∆L∖
IF - det(HbD-i)Hbelexp 卜丁 — -L)	(81)
Ta and Tb are the eigenvalues of Ha-1 Da and Hb-1 Db corresponding to the escape direction.
We know Da = 备 Ha and Db = 备[Hb] +. AS D must be positive semidefinite, we re-
place Hb = U>diag(Hbi,…，Hb(n-i),Hbe)U by its positive semidefinite analog [Hb]+ =
U>diag(Hbi,…，Hb(n-i), ∣Hbe∣)Ub. Thus, we have
T =1 = 2∏6exp [ 2bδl (HL + ⅛41	(82)
γ	|Hbe |	η	Hae	|Hbe |
□
A.3 Proof of Proposition 1
Proof. A stationary distribution must have a balanced probability flux between valleys. So the
probability flux of each valley must be equivalent,
P(θ ∈ V1)γ12 = P(θ ∈ V2)γ21	(83)
As τ = Y-1, it leads to P(θ ∈ Vv) H Tv. We normalize the total probability to 1, then we obtain the
result.	□
B Assumptions
Assumption 2 indicates that the dynamical system is in equilibrium near minima but not necessarily
near saddle points. It means that dP∂θ,t) = 一▽ ∙ J(θ, t) ≈ 0 holds near minima aι and a2, but not
necessarily holds near saddle point b. Quasi-Equilibrium Assumption is actually weaker but more
useful than the conventional stationary assumption for deep learning (Welling & Teh, 2011; Mandt
et al., 2017). Under Assumption 2, the probability density P can behave like a stationary distribution
only inside valleys, but density transportation through saddle points can be busy. Quasi-Equilibrium
is more like: stable lakes (loss valleys) is connected by rapid Rivers (escape paths). In contrast, the
stationary assumption requires strictly zero flux between lakes (loss valleys). Little knowledge about
density motion can be obtained under the stationary assumption.
Low Temperature Assumption is common (Van Kampen, 1992; Zhou, 2010; Berglund, 2013; Jas-
trzebski et al., 2017), and is always justified when B is small. Under Assumption 3, the probability
densities will concentrate around minima and MPPs. Numerically, the 6-sigma rule may often provide
good approximation for a Gaussian distribution. Assumption 3 will make the second order Taylor
approximation, Assumption 1, even more reasonable in SGD diffusion.
Here, we try to provide a more intuitive explanation about Low Temperature Assumption in the
domain of deep learning. Without loss of generality, we discuss it in one-dimensional dynamics. The
temperature can be interpreted as a real number D. In SGD, we have the temperature as D = 2BH.
In statistical physics, if ∆L is large, then we call it Low Temperature Approximation. Note that
∆L appears insides an exponential function in the theoretical analysis. People usually believe that,
numerically, ∆L > 6 can make a good approximation, for a similar reason of the 6-sigma rule in
statistics. In the final training phase of deep networks, a common setting is η = 0.01 and B = 128.
Thus, we may safely apply Assumption 3 to the loss valleys which satisfy the very mild condition
H > 2.3 X 10-4. Empirically, the condition ∆L > 2.3 X 10-4 holds well in sGd dynamics. It
also suggests that, we can adjust the learning rate to let SGD search among loss valleys with certain
barrier heights.
C The Stochastic Gradient Noise Analysis
Figure 7 demonstrates that the SGN is also approximately Gaussian on a randomly initialized ResNet
with B = 50 on CIFAR-10. We also note that the SGN on ResNet seems less Gaussian than SGN on
18
Published as a conference paper at ICLR 2021
76543210
Oooooooo
11111111
Aua
At",uφa
00
a
050,030j010°
Illlll
0.05	0.10	0.15	0.20	0.25
Noise Norm
(a)	Gradient Noise of one minibatch
parameters
across
(b)	L6vy noise
nl.
1
ua
1
1
ua
IO0
0.00 0.02	0.04 0.06 0.08 0.10 0.12
Noise Norm
(c) Gradient Noise of one parameter across
minibatches
IOOd
0.000
0.002	0.004	0.006	0.008	0.010
Noise Norm
(d) Gaussian noise
Figure 7:	The Gradient Noise Analysis. The histogram of the norm of the gradient noises computed
with ResNet18 (He et al., 2016) on CIFAR-10 (Krizhevsky et al., 2009).
10 12
Oooo
--
∙le>03 80N
-o.oι
0.00
0.01
Hesslan
0.02
——0=ι, coβħo.782⅛ Pβaraon: 0.9529
—B=IaCOeT:0.0776, ⅛aιsoι>: 0.9497
—B=20, Ca«f:0.039. Pearsan: 0.9411
B=30, Caer:0.0262, PearSen: 0.9413
NabeCovarB=I
• NabeCavarB=IO
• NabeCavarB=SO
NabeCaVarB=30
10 12
Oooo
--
8S8 X .IeAOU 8ON
-0.03 -0.02 -0.01	0.00	0.01	0.02	0.03
Hesslan
-0.03
-0.02
0.03
Figure 8:	The plot of the SGN covariance and the Hessian by training fully-connected network on
MNIST. We display all elements Haj) ∈ [-0.03,0.03] of the Hessian matrix and the corresponding
elements in gradient noise covariance matrix in the original coordinates.
19
Published as a conference paper at ICLR 2021
a 0.4
0.5
Hesslan
Figure 9: The plot of the SGN covariance and the Hessian by training fully-connected network on
Avila. We display all elements H(i,j) ∈ [1e - 4, 0.5] of the Hessian matrix and the corresponding
elements in gradient noise covariance matrix in the space spanned by the eigenvectors of Hessians.
——B=l, COeM∙0MS, l⅛a∣son: 0.997β
—B=10, CθβtΛM0Λ, Feaison: 0.994«
—B =20, COeT9.9987, Fβaιsαn: 0.995β
B-3β, CMr:14327, Faison: 0.9977
NobeCovarB=I
MobeCovarB=IO
MobeCovarB=SO
Mobe Covar
fully-connected networks with the same batch size. Panigrahi et al. (2019) presented more results on
the Gaussianity of SGN under various conditions.
By Figure 8, We validate C = H in the original coordinates on MNIST. By Figure 9, We also validate
C = B on another dataset, Avila, in the space spanned by the eigenvectors of Hessian. The relation
C = B can still be observed in these two cases.
Data Precessing: We perform the usual per-pixel zero-mean and unit-variance normalization on
MNIST. We leave the preprocessing of Avila in D. Model: Fully-connected netWorks.
D	Main Experiments
Figure 10, 11, and 12 respectively validate that the exponential relation of the escape rate With the
Hessian, the batch size and the learning rate.
D.1 Experimental Settings
Datasets: a) Avila, b) Banknote Authentication, c) Cardiotocography, d) Dataset for Sensorless Drive
Diagnosis.
Data Precessing: We perform per-pixel zero-mean and unit-variance normalization on input data.
For simplicity, We also transform multi-class problems into binary-class problems by grouping labels,
although this is unnecessary.
Model: TWo-layer fully-connected netWorks With one hidden layer and 10 neurons per hidden layer.
Initializations: To ensure the initialized models are near minima, We first pretrain models With
200-1000 epochs to fit each data set as Well as possible. We set the pretrained models’ parameters as
the initialized θt=0 .
Valleys’ Boundary: In principle, any small neighborhood around θt=0 can be regarded as the inside
of the start valleys. In our experiments, We set each dimension’s distance from θt=0 should be less
than 0.05, namely ∣∆θi | ≤ 0.05 for each dimension i. If we rescale the landscape by a factor k, the
neighborhood Will also be rescaled by k. Although We don’t knoW Which loss valleys exist inside the
neighborhood, we know the landscape of the neighborhood is invariant in each simulation.
Hyperparameters: In Figure 10: (a) η = 0.001, B = 1, (b) η = 0.015, B = 1, (c) η = 0.005, B =
1, (d) η = 0.0005, B = 1. In Figure 11: (a) η = 0.02, (b) η = 0.6, (c) η = 0.18, (d) η = 0.01. In
Figure 12: (a) B = 1, (b) B = 1, (c) B = 1, (d) B = 1. In Figure 13: (a) η = 0.0002, B = 100,
(b) η = 0.001, B = 100, (c) η = 0.0002, B = 100, (d) η = 0.0001, B = 100. In Figure 14: (a)
η = 0.0002, B = 100, D = 0.0002, (b) η = 0.001, B = 100, D = 0.0001, (c) η = 0.0002, B =
100, D = 0.0005, (d) η = 0.0001, B = 100, D = 0.0003. We note that the hyperparameters need
be tuned for each initialized pretrained models, due to the stochastic property of deep learning.
20
Published as a conference paper at ICLR 2021
(a) Avila
(b) Banknote
(c) Cardiotocography
(d) Diagnosis
Figure 10: The escape rate exponentially depends on the “path Hessians” in the dynamics of SGD.
-log(γ) is linear with 1. The “path Hessians” indicates the eigenvalues of Hessians corresponding
to the escape directions.
21
Published as a conference paper at ICLR 2021
(a) Avila
(c) Cardiotocography
Figure 11: The escape rate exponentially depends on the batch size in the dynamics of SGD. - log(γ)
is linear with B.
(d) Diagnosis
22
Published as a conference paper at ICLR 2021
(a) Avila
(b) Banknote
(c) Cardiotocography
Figure 12: The escape rate exponentially depends on the learning rate in the dynamics of SGD.
-log(γ) is linear with ɪ. The estimated escape rate has incorporated η as the time unit.
(d) Diagnosis
23
Published as a conference paper at ICLR 2021
(a) Avila
(b) Banknote
(c) Cardiotocography
(d) Diagnosis
Figure 13: The relation of the escape rate and the isotropic diffusion coefficient D. The escape
formula that - log(γ) is linear with D is validated.
According to our experience, we can always find the hyperparameters to discover the quantitative
relations as long as the pretrained model fits the data set well enough. The fined-tuned requirement
can be avoided in Section E, because the models in Section E are artificially initialized.
Observation: we observe the number of iterations from the initialized position to the terminated
position. We repeat experiments 100 times to estimate the escape rate γ and the mean escape time τ.
As the escape time is a random variable obeying an exponential distribution, t 〜Exponential(γ),
the estimated escape rate can be written as
100-2
Y =「100 J
i=1 ti
(84)
The 95% confidence interval of this estimator is
Y(1-
1.96	1.96
-/	) ≤ γ ≤ γ(i +—/	).
√100	√100
(85)
D.2 Experiments on SGLD
Experimental Results: Figure 13 shows a highly precise exponential relation of the escape rate and
the diffusion coefficient in the figure. Figure 14 shows a proportional relation of the escape rate and
the Hessian determinant in the figure. Overall, the empirical results support the density diffusion
theory in the dynamics of white noise. In experiments on SGLD, we carefully adjust the injected
gradient noise scale in experiment to ensure that D is significantly smaller than the loss barrier’
height and large enough to dominate SGN scale. If D is too large, learning dynamics will be reduced
to Free Brownian Motion.
24
Published as a conference paper at ICLR 2021
(a) Avila
(b) Banknote
(c) Cardiotocography
Figure 14: The relation of the escape rate and the Hessian determinant in the dynamics of white
noise.The escape formula that Y is linear with k is validated.
(d) Diagnosis
25
Published as a conference paper at ICLR 2021
E Experiments on More Models
We supply experiments of training three models on artificial Gaussian datasets. In these experiments,
we can analytically know the locations of the minima, Hessians and loss barriers, as each input feature
is Gaussian noise.
E.1 Experiments Settings
Data Set: We generate 50000 Gaussian samples and random two-class labels as the training data set,
{(x⑺,y⑺)|x⑴〜N(0,I),y(i) ∈ {0, 1},i ∈ {1, 2,…，50000}}.
Hyperparameters: In Figure 15: (a) η = 0.0001, B = 100, (b) η = 0.001, B = 100, (c) η =
0.0003, B = 100. In Figure 16: (a) η = 0.0001, B = 50,D = 0.2, (b) η = 0.001, B = 50,D =
0.0005, (c) η = 0.0003, B = 1, D = 0.0003. In Figure 17: (a) η = 0.006, B = 50, (b) η =
0.05, B = 50, (c) η = 0.005, B = 1. In Figure 18: (a) η = 0.006, (b) η = 0.06, (c) η = 0.1. In
Figure 19: (a) B = 1, (b) B = 1, (c) B = 1. We note that the hyperparameters are recommended
and needn’t be fine tuned again. The artificially initialized parameters avoids the stochastic property
of the initial states.
Experiment Setting 1: Styblinski-Tang Function is a commonly used function in nonconvex opti-
mization, written as
1n
f (θ) = 2 ∑(θ4 - ≡2 +5θi).
i=1
We use high-dimensional Styblinski-Tang Function as the test function, and Gaussian samples as
training data.
L(θ) = f(θ - x),
where data samples X 〜N(0, I). The one-dimensional Styblinski-Tang Function has one global
minimum located at a = -2.903534, one local minimum located at d, and one saddle point b =
0.156731 as the boundary separating Valley a1 and Valley a2. For a n-dimensional Styblinski-Tang
Function, we initialize parameters θt=o = √(-2.903534,…，-2.903534), and set the valley's
boundary as θi < √0.156731, where i is the dimension index. We record the number of iterations
required to escape from the valley to the outside of valley. The setting 1 does not need labels.
Experiment Setting 2: We study the learning dynamics of Logistic Regression. Parameters Initial-
ization: θt=o = (0,…，0). Valley Boundary: -0.1 < θi < 0.1. Due to the randomness of training
data and the symmetry of dimension, the origin must be a minimum and there are a lot unknown
valleys neighboring the origin valley. And we can set an arbitrary boundary surrounding the origin
valley group, and study the mean escape time from the group of valleys.
Experiment Setting 3: We study the learning dynamics of MLP with ReLu activations, cross entropy
losses, depth as 3, and hidden layers, width as 10. Parameters Initialization: θt=o = (0.1,…，0.1)
with a small Gaussian noise = (0, 0.01I). Valley Boundary: 0.05 < θi < 0.15. To prevent the
gradient disappearance problem of deep learning, we move the starting point from the origin. For
symmetry breaking of deep learning, we add a small Gaussian noise to each parameter’s initial value.
Due to the complex loss landscape of deep networks, we can hardly know the exact information
about valleys and cols. However, the escape formula can still approximately hold even if an arbitrary
boundary surrounding an arbitrary group of valleys. We set the batch size as 1 in this setting. When
the batch size is small, the gradient noise is more like a heavy-tailed noise. We can validate whether
or not the propositions can hold with very-small-batch gradient noise in practice.
E.2 Experiments Results
Figure 15 shows the relation of the escape rate and the isotropic diffusion coefficient D. Figure 16
shows the relation of the escape rate and the Hessian determinant in the dynamics of white noise.
Figure 17 shows the relation of the escape rate and the second order directional derivative in the
dynamics of SGD. Figure 18 shows the relation of the escape rate and the batch size in the dynamics
of SGD. Figure 19 shows the relation of the escape rate and the learning rate in the dynamics of SGD.
26
Published as a conference paper at ICLR 2021
(a) Styblinski-Tang Function	(b) Logistic Regression	(c) MLP
Figure 15:	The relation of the escape rate and the diffusion coefficient D in the dynamics of SGLD.
The escape formula that - log(γ) is linear with D is validated in the setting of Styblinski-Tang
Function, Logistic Regression and MLP.
(a) Styblinski-Tang Function
(b) Logistic Regression
(c) MLP
Figure 16:	The relation of the escape rate and the Hessian determinants in the dynamics of SGLD.
The escape formula that γ is linear with k is validated in the setting of Styblinski-Tang Function,
Logistic Regression and MLP.
(c) MLP
(a) Styblinski-Tang Function	(b) Logistic Regression
Figure 17:	The escape rate exponentially depends on the sharpness in the dynamics of SGD. The
escape formula that - log(γ) is linear with 1 is validated in the setting of Styblinski-Tang Function,
Logistic Regression and MLP.
27
Published as a conference paper at ICLR 2021
(a) Styblinski-Tang Function
(b) Logistic Regression
(c) MLP
Figure 18:	The escape rate exponentially depends on the batch size in the dynamics of SGD. The
escape formula that - log(γ) is linear with B is validated in the setting of Styblinski-Tang Function,
Logistic Regression and MLP.
(a) Styblinski-Tang Function
(b) Logistic Regression
(c) MLP
Figure 19:	The escape rate exponentially depends on the learning rate in the dynamics of SGD. The
escape formula that - log(γ) is linear with 1 is validated in the setting of Styblinski-Tang Function,
Logistic Regression and MLP.
28