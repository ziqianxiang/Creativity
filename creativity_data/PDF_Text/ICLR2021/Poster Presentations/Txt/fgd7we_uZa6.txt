Published as a conference paper at ICLR 2021
How Much Over-parameterization Is Suffi-
cient to Learn Deep ReLU Networks ?
Zixiang Chen:f Yuan Cao:f Difan Zou:f Quanquan Gu:
:Department of Computer Science, University of California, Los Angles
{chenzx19,yuancao,knowzou,qgu}@cs.ucla.edu
Ab stract
A recent line of research on deep learning focuses on the extremely over-
parameterized setting, and shows that when the network width is larger than
a high degree polynomial of the training sample size n and the inverse of the target
error ´1, deep neural networks learned by (stochastic) gradient descent enjoy
nice optimization and generalization guarantees. Very recently, it is shown that
under certain margin assumptions on the training data, a polylogarithmic width
condition suffices for two-layer ReLU networks to converge and generalize (Ji
and Telgarsky, 2020). However, whether deep neural networks can be learned
with such a mild over-parameterization is still an open question. In this work, we
answer this question affirmatively and establish sharper learning guarantees for
deep ReLU networks trained by (stochastic) gradient descent. In specific, under
certain assumptions made in previous work, our optimization and generalization
guarantees hold with network width Polylogarithmic in n and e´1. Our results
push the study of over-parameterized deep neural networks towards more practical
settings.
1 Introduction
DeeP neural networks have become one of the most imPortant and Prevalent machine learning models
due to their remarkable Power in many real-world aPPlications. However, the success of deeP learning
has not been well-exPlained in theory. It remains mysterious why standard oPtimization algorithms
tend to find a globally oPtimal solution, desPite the highly non-convex landscaPe of the training loss
function. Moreover, desPite the extremely large amount of Parameters, deeP neural networks rarely
over-fit, and can often generalize well to unseen data and achieve good test accuracy. Understanding
these mysterious Phenomena on the oPtimization and generalization of deeP neural networks is one
of the most fundamental Problems in deeP learning theory.
Recent breakthroughs have shed light on the oPtimization and generalization of deeP neural networks
(DNNs) under the over-Parameterized setting, where the hidden layer width is extremely large (much
larger than the number of training examPles). It has been shown that with the standard random
initialization, the training of over-Parameterized deeP neural networks can be characterized by a
kernel function called neural tangent kernel (NTK) (Jacot et al., 2018; Arora et al., 2019b). In
the neural tangent kernel regime (or lazy training regime (Chizat et al., 2019)), the neural network
function behaves similarly as its first-order Taylor exPansion at initialization (Jacot et al., 2018;
Lee et al., 2019; Arora et al., 2019b; Cao and Gu, 2019), which enables feasible oPtimization and
generalization analysis. In terms of oPtimization, a line of work (Du et al., 2019b; Allen-Zhu et al.,
2019b; Zou et al., 2019; Zou and Gu, 2019) Proved that for sufficiently wide neural networks,
(stochastic) gradient descent (GD/SGD) can successfully find a global oPtimum of the training loss
function. For generalization, Allen-Zhu et al. (2019a); Arora et al. (2019a); Cao and Gu (2019)
established generalization bounds of neural networks trained with (stochastic) gradient descent, and
showed that the neural networks can learn target functions in certain reProducing kernel Hilbert sPace
(RKHS) or the corresPonding random feature function class.
Although existing results in the neural tangent kernel regime have Provided imPortant insights
into the learning of deeP neural networks, they require the neural network to be extremely wide.
* Equal contribution.
1
Published as a conference paper at ICLR 2021
The typical requirement on the network width is a high degree polynomial of the training sample
size n and the inverse of the target error e´1. As there still remains a huge gap between such
network width requirement and the practice, many attempts have been made to improve the over-
parameterization condition under various conditions on the training data and model initialization
(Oymak and Soltanolkotabi, 2019; Zou and Gu, 2019; Kawaguchi and Huang, 2019; Bai and Lee,
2019). For two-layer ReLU networks, a recent work (Ji and Telgarsky, 2020) showed that when the
training data are well separated, polylogarithmic width is sufficient to guarantee good optimization
and generalization performances. However, their results cannot be extended to deep ReLU networks
since their proof technique largely relies on the fact that the network model is 1-homogeneous, which
cannot be satisfied by DNNs. Therefore, whether deep neural networks can be learned with such a
mild over-parameterization is still an open problem.
In this paper, we resolve this open problem by showing that polylogarithmic network width is
sufficient to learn DNNs. In particular, unlike the existing works that require the DNNs to behave
very close to a linear model (up to some small approximation error), we show that a constant linear
approximation error is sufficient to establish nice optimization and generalization guarantees for
DNNs. Thanks to the relaxed requirement on the linear approximation error, a milder condition on
the network width and tighter bounds on the convergence rate and generalization error can be proved.
We summarize our contributions as follows:
•	We establish the global convergence guarantee of GD for training deep ReLU networks based
on the so-called NTRF function class (Cao and Gu, 2019), a set of linear functions over random
features. Specifically, we prove that GD can learn deep ReLU networks with width m “ polypRq
to compete with the best function in NTRF function class, where R is the radius of the NTRF
function class.
•	We also establish the generalization guarantees for both GD and SGD in the same setting. Specifi-
cally, We prove a diminishing statistical error for a Wide range of network width m P (Ω(1), 8),
while most of the previous generalization bounds in the NTK regime only works in the setting
2
where the network width m IS much greater than the sample size n. Moreover, we establish O(e 2)
1
O(e 1) sample complexities for GD and SGD respectively, which are tighter than existing bounds
for learning deep ReLU networks (Cao and Gu, 2019), and match the best results when reduced to
the two-layer cases (Arora et al., 2019b; Ji and Telgarsky, 2020).
•	We further generalize our theoretical analysis to the scenarios with different data separability
assumptions in the literature. We show if a large fraction of the training data are well separated,
the best function in the NTRF function class with radius R “ Op1) can learn the training data
with error up to e. This together with our optimization and generalization guarantees immediately
suggests that deep ReLU networks can be learned with network width m “ Ω(1), which has a
logarithmic dependence on the target error e and sample size n. Compared with existing results
(Cao and Gu, 2020; Ji and Telgarsky, 2020) which require all training data points to be separated
in the NTK regime, our result is stronger since it allows the NTRF function class to misclassify a
small proportion of the training data.
For the ease of comparison, we summarize our results along with the most related previous results in
Table 1, in terms of data assumption, the over-parameterization condition and sample complexity.
It can be seen that under data separation assumption (See Sections 4.1, 4.2), our result improves
existing results for learning deep neural networks by only requiring a polylog(n, e´1) network width.
Notation. For two scalars a and b, we denote a ^ b “ minta, bu. For a vector x P Rd we use }x}2
to denote its Euclidean norm. For a matrix X, we use }X}2 and }X}F to denote its spectral norm
and Frobenius norm respectively, and denote by Xij the entry of X at the i-th row and j-th column.
Given two matrices X and Y with the same dimension, we denote〈X, Y〉“ Xi j XijYj.
Given a collection of matrices W = {Wι,…，WL} P bN]Rml 'm1 and a function f (W) over
bL“iRmi Xm, we define by NWlfpW) the partial gradient of f (W) with respect to Wl and denote
Vwf (W) “ tVwi f (W)UL“i. Wealso denote B(W,τ) “ { W1 : maxlp[L] }W： ´ Wl}f ≤ T(
for τ20. For two collection of matrices A “ {Aι,…,An}, B “ {Bι,…,Bn}, we denote
〈A,B〉“Xin“1〈Ai,Bi〉and}A}2F“Xin“1}Ai}2F.
2
Published as a conference paper at ICLR 2021
Table 1: Comparison of neural network learning results in terms of over-parameterization condition
and sample complexity. Here is the target error rate, n is the sample size, L is the network depth.
	Assumptions	Algorithm	Over-para. Condition	Sample Complexity	Network
Zou et al. (2019)	Data nondegeneration	GD	Ω'n12L16Pn2 ' e´* 1))	-	Deep
This paper	Data nondegeneration	GD	Ω'l22 n12)	-	Deep
Cao and Gu (2020)	Data separation	GD	Ω(e´14q - e”)	OP「4q. eopLq^^	Deep
Ji and Telgarsky (2020)	Data separation	GD	polylogPn, e´1)	o（「2q	Shallow
This paper	Data separation	GD	polylogPn, e´1 )∙ poly(Lq	OP12q∙ eOpLq	Deep
Cao and Gu (2019)	Data separation	SGD	Ω PeT q∙poly(Lq	~ .	C . OPe )∙poIy(L)	Deep
Ji and Telgarsky (2020)	Data separation	SGD	polylog(eT q	OPef	Shallow
This paper	Data separation	SGD	polylog(eT q∙ poly(Lq	OPef ∙ polyPLq	Deep
Algorithm 1 Gradient descent with random initialization
Input: Number of iterations T, step size η, training set S “ tpxi, yiqin“1u, initialization Wp0q
fort “ 1,2,. . . ,T do
Update WPtq = WPtTq ´ η ∙ RWLSPWPtTq).
end for
Output: WP0q,...,WPTq.
Given two sequences {x" and {yn}, We denote Xn “ OPynq if |xn| ≤ Cι∣yn∣ for some absolute
positive constant Ci, Xn “ Ω(yn) if |xn|2C2∣yn∣ for some absolute positive constant C2, and
Xn = Θpynq if C3∣yn∣ < |xn| ≤ C4∣yn∣ for some absolute constants C3, C4 > 0. We also use O(∙),
Ω(∙) to hide logarithmic factors In O(∙)and Ω(∙) respectively. Additionally, we denote Xn = polyPyn)
ifXn = OPynDq for some positive constant D, and Xn = polylogPynq ifXn = polyPlogPynqq.
2	Preliminaries on learning neural networks
In this section, we introduce the problem setting in this paper, including definitions of the neural
network and loss functions, and the training algorithms, i.e., GD and SGD with random initialization.
Neural network function. Given an input x P Rd, the output of deep fully-connected ReLU network
is defined as follows,
fwPx) = m1{2WLb(WL—i …σ(Wix)∙∙∙),
where Wi P Rm'd, W2,…，Wlt P Rm'm, WL P R1'm, and σ(x) = max{0, x} is the ReLU
activation function. Here, without loss of generality, we assume the width of each layer is equal to m.
Yet our theoretical results can be easily generalized to the setting with unequal width layers, as long
as the smallest width satisfies our overparameterization condition. We denote the collection of all
weight matrices as W = {Wi, . . . , WLu.
Loss function. Given training dataset {xi, yiUi“i,...,n with input Xi P Rd and output yi P {—1, +1},
we define the training loss function as
1n
LS(W) = - XLi(W),
i“i
where Li PW) = ` yifWPxi) = log 1 ` expP—yifWPxi)) is defined as the cross-entropy loss.
Algorithms. We consider both GD and SGD with Gaussian random initialization. These two algo-
rithms are displayed in Algorithms 1 and 2 respectively. Specifically, the entries in Wpoq, ∙∙∙ , W? 1
are generated independently from univariate Gaussian distribution NP0, 2{m) and the entries in WPL0q
are generated independently from NP0, 1{m). For GD, we consider using the full gradient to update
the model parameters. For SGD, we use a new training data point in each iteration.
Note that our initialization method in Algorithms 1, 2 is the same as the widely used He initialization
(He et al., 2015). Our neural network parameterization is also consistent with the parameterization
used in prior work on NTK (Jacot et al., 2018; Allen-Zhu et al., 2019b; Du et al., 2019a; Arora et al.,
2019b; Cao and Gu, 2019).
3
Published as a conference paper at ICLR 2021
Algorithm 2 Stochastic gradient desecent (SGD) with random initialization
Input: Number of iterations n, step size η, initialization Wp0q
for i “ 1, 2, . . . , n do
Draw (xi, yi) from D and compute the corresponding gradient RWLi(W(i´1)).
Update Wpiq = W(i—1q ´ η ∙ NWLi(Wpiτq).
end for
Output: Randomly choose W uniformly from {W(0q,..., W(nTq}.
3 Main theory
In this section, we present the optimization and generalization guarantees ofGD and SGD for learning
deep ReLU networks. We first make the following assumption on the training data points.
Assumption 3.1. All training data points satisfy }xi }2 “ 1, i “ 1, . . . , n.
This assumption has been widely made in many previous works (Allen-Zhu et al., 2019b;c; Du et al.,
2019b;a; Zou et al., 2019) in order to simplify the theoretical analysis. This assumption can be relaxed
to be upper bounded and lower bounded by some constant.
In the following, we give the definition of Neural Tangent Random Feature (NTRF) (Cao and Gu,
2019), which characterizes the functions learnable by over-parameterized ReLU networks.
Definition 3.2 (Neural Tangent Random Feature, (Cao and Gu, 2019)). Let Wp0q be the initialization
weights, and FWp0q,W(x) “ fWp0q (x) ` xRfWp0q (x), W ´ Wp0qy be a function with respect to
the input x. Then the NTRF function class is defined as follows
F(Wp0q,R) “ {Fwpo),w(∙) : W P B(Wp0q,R ∙ m´1/2)(.
The function class FWp0q,W (x) consists of linear models over random features defined based on
the network gradients at the initialization. Therefore it captures the key “almost linear” property of
wide neural networks in the NTK regime (Lee et al., 2019; Cao and Gu, 2019). In this paper, we
use the NTRF function class as a reference class to measure the difficulty of a learning problem. In
what follows, we deliver our main theoretical results regarding the optimization and generalization
guarantees of learning deep ReLU networks. We study both GD and SGD with random initialization
(presented in Algorithms 1 and 2).
3.1 Gradient descent
The following theorem establishes the optimization guarantee of GD for training deep ReLU networks
for binary classification.
Theorem 3.3. For δ,R > 0, let entrf “ infFPF(w(o),r)n´1 Xn“1 '[yiF(Xi)S be the minimum
training loss achievable by functions in F (Wp0q, R). Then there exists
m* (δ, R, L) “ O(POly (R, L) ∙ log4{3(n{δ)),
such that if m2m*(δ, R, L), with probability at least 1 一 δ over the initialization, GD with step
size η “ Θ(L-1mT) can train a neural network to achieve at most 36ntrf training loss within
T “。(L2R2EITrf) iterations.
Theorem 3.3 shows that the deep ReLU network trained by GD can compete with the best function in
the NTRF function class F (Wp0q, R) if the network width has a polynomial dependency in R and L
and a logarithmic dependency in n and 1{δ. Moreover, if the NTRF function class with R “ O(1)
can learn the training data well (i.e., ENTRF is less than a small target error E), a polylogarithmic (in
terms of n and e´1) network width suffices to guarantee the global convergence of GD, which directly
improves over-paramterization condition in the most related work (Cao and Gu, 2019). Besides, we
remark here that this assumption on the NTRF function class can be easily satisfied when the training
data admits certain separability conditions, which we discuss in detail in Section 4.
Compared with the results in (Ji and Telgarsky, 2020) which give similar network width requirements
for two-layer networks, our result works for deep networks. Moreover, while Ji and Telgarsky (2020)
4
Published as a conference paper at ICLR 2021
essentially required all training data to be separable by a function in the NTRF function class with a
constant margin, our result does not require such data separation assumptions, and allows the NTRF
function class to misclassify a small proportion of the training data points*.
We now characterize the generalization performance of neural networks trained by GD. We denote
L0D^1PWq “ E(χ,y)~D[1{fw(x)' y V 0}] as the expected 0-1 loss (i.e., expected error) of fw(x).
Theorem 3.4. Under the same assumptions as Theorem 3.3, with probability at least 1 ´ δ, the iterate
Wpt) of Algorithm 1 satisfies that
LDT(Wpt))W 2Ls (Wpt)) + θ(4LL2RCn ^(弋 + T )^(^)
for all t “ 0, . . . , T .
Theorem 3.4 shows that the test error of the trained neural network can be bounded by its training error
plus statistical error terms. Note that the statistical error terms is in the form of a minimum between
two terms 4LL2Rʌ∕m∕n and L3/2R∕√n ' L11/3R4{3∕m1/6. Depending on the network width m,
one of these two terms will be the dominating term and diminishes for large n: (1) if m “ o(n),
the statistical error will be 4LL2Ry∕m∕n, and diminishes as n increases; and (2) if m “ Ω(n), the
statistical error is L3/2R∕√n ' L11/3R4/3∕m1/6, and again goes to zero as n increases. Moreover,
in this paper we have a specific focus on the setting m “ O(1), under which Theorem 3.4 gives a
statistical error of order Or(n~1{2). This distinguishes our result from previous generalization bounds
for deep networks (Cao and Gu, 2020; 2019), which cannot be applied to the setting m “ O(1).
We note that for two-layer ReLU networks (i.e., L “ 2) Ji and Telgarsky (2020) proves a tighter
Or(1∕n1{2) generalization error bound regardless of the neural networks width m, while our result
(Theorem 3.4), in the two-layer case, can only give Or(1∕n1{2) generalization error bound when
3
m “ O(1) or m “ Ω(n3). However, different from our proof technique that basically uses the
(approximated) linearity of the neural network function, their proof technique largely relies on the
1-homogeneous property of the neural network, which restricted their theory in two-layer cases. An
interesting research direction is to explore whether a Or(1∕n1{2) generalization error bound can be
also established for deep networks (regardless of the network width), which we will leave it as a
future work.
3.2 Stochastic gradient descent
Here we study the performance of SGD for training deep ReLU networks. The following theorem
establishes a generalization error bound for the output of SGD.
Theorem 3.5. For δ,R > 0, let entrf “ infFPFpwPθq,Rq n´1 Xn“1 '[yiF(Xi)S be the minimum
training loss achievable by functions in F (Wp0), R). Then there exists
m*(δ, R,L) “ O(poly(R,L)∙log4∕3(n∕δ)),
such that if m》m* (δ, R, L), with probability at least 1 一 δ, SGD with step size η “ Θ (m´1 ∙
(LR2n~1 e´rf ^ LT)) achieves
ErLD´1 (W)S ≤ 8L2R2 + 8⅛Zδq + 24eNTRF,
nn
where the expectation is taken over the uniform draw of W from {Wp0q,..., WpnT)}.
1
For any e > 0, Theorem 3.5 gives a O(e 1) sample complexity for deep ReLU networks trained with
SGD to achieve O(eNTRF + e) test error. Our result extends the result for two-layer networks proved in
(Ji and Telgarsky, 2020) to multi-layer networks. Theorem 3.5 also provides sharper results compared
with Allen-Zhu et al. (2019a); Cao and Gu (2019) in two aspects: (1) the sample complexity is
improved from n “ θ(e´2) to n “ θ(e´1); and (2) the overparamterization condition is improved
from m》poly(e 1) to m “ Ω(1).
*A detailed discussion is given in Section 4.2.
5
Published as a conference paper at ICLR 2021
4	Discussion on the NTRF Class
Our theoretical results in Section 3 rely on the radius (i.e., R) of the NTRF function class F pWp0q, Rq
and the minimum training loss achievable by functions in FpWp0q, Rq, i.e., NTRF. Note that a larger
R naturally implies a smaller NTRF, but also leads to worse conditions on m. In this section, for
any (arbitrarily small) target error rate e > 0, we discuss various data assumptions studied in the
literature under which our results can lead to Opq training/test errors, and specify the network width
requirement.
4.1	Data Separability by Neural Tangent Random Feature
In this subsection, we consider the setting where a large fraction of the training data can be linearly
separated by the neural tangent random features. The assumption is stated as follows.
Assumption 4.1. There exists a collection of matrices U*
tUι,…，ULU satisfying
∑3∣u*}F = 1,such that for at least p1 ´ P) fraction of training data we have
yiXVfwρoq pXi), U*y 2 m1{2γ,
where Y is an absolute positive COnStantt and P P [0,1).
The following corollary provides an upper bound of eNTRF under Assumption 4.1 for some R.
Proposition 4.2. Under Assumption 4.1, for any e, δ > 0, if R》C[log1{2pn{δ) ' log(1∕e)‰∕γ for
some absolute constant C, then with probability at least 1 ´ δ,
eNTRF =FPF(Wp0),RqnT EwF(Xi)) W e ' P ∙ Op"
Proposition 4.2 covers the setting where the NTRF function class is allowed to misclassify training
data, while most of existing work typically assumes that all training data can be perfectly separated
with constant margin (i.e., P “ 0) (Ji and Telgarsky, 2020; Shamir, 2020). Our results show that
for sufficiently small misclassification ratio P “ Ope), we have eNTRF “ Ope) by choosing the
radius parameter R logarithimic in n, δ11, and e´1. Substituting this result into Theorems 3.3,
3.4 and 3.5, it can be shown that a neural network with width m “ polypL, logpn∕δ), logp1∕e))
suffices to guarantee good optimization and generalization performances for both GD and SGD.
Consequently, we can obtain that the bounds on the test error for GD and SGD are OpnT/2) and
1
Opn 1) respectively.
4.2	Data Separability by Shallow Neural Tangent Model
In this subsection, we study the data separation assumption made in Ji and Telgarsky (2020) and show
that our results cover this particular setting. We first restate the assumption as follows.
Assumption 4.3. There exists up∙) : Rd → Rd and Y 2 0 such that ∣∣upz)}2 ≤ 1 for all Z P Rd, and
y J d σ1pχz, Xiy) ∙ χupz), Xi>dμNpz) 2 Y
for all i P [n], where μNp∙) denotes the standard normal distribution.
Assumption 4.3 is related to the linear separability of the gradients of the first layer parameters at
random initialization, where the randomness is replaced with an integral by taking the infinite width
limit. Note that similar assumptions have also been studied in (Cao and Gu, 2020; Nitanda and
Suzuki, 2019; Frei et al., 2019). The assumption made in (Cao and Gu, 2020; Frei et al., 2019) uses
gradients with respect to the second layer weights instead of the first layer ones. In the following, we
mainly focus on Assumption 4.3, while our result can also be generalized to cover the setting in (Cao
and Gu, 2020; Frei et al., 2019).
tThe factor m1{2 is introduced here since } Vwpoq f (Xi)}f is typically of order O(m1/2).
6
Published as a conference paper at ICLR 2021
In order to make a fair comparison, we reduce our results for multilayer networks to the two-layer
setting. In this case, the neural network function takes form
fWpxq “ m1{2W2σpW1xq.
Then we provide the following proposition, which states that Assumption 4.3 implies a certain choice
of R “ Op1q such the the minimum training loss achieved by the function in the NTRF function
class FpWp0q, Rq satisfies NTRF “ Opq, where is the target error.
Proposition 4.4. Suppose the training data satisfies Assumption 4.3. For any e, δ > 0, let R “
C logpn{δq ` logp1{q {γ for some large enough absolute constant C. If the neural network width
satisfies m “ Ω(log(n∕δ)∕γ2), then with probability at least 1 一 δ, there exist FW(o)W(Xi) P
F(Wp0q,R) such that ''yi ∙ FWpoq,w(xi)) ≤ e, @i p[n].
Proposition 4.4 shows that under Assumption 4.3, there exists FWp0q w(∙) P F(Wp0q,R) with
R “ O(1∕γ) such that the cross-entropy loss of FW(o)W(∙) at each training data point IS bounded by
e. This implies that entrf ≤ e. Moreover, by applying Theorem 3.3 with L = 2, the condition on the
neural network width becomes m “ Ω(1∕γ8y, which matches the results proved in Ji and Telgarsky
(2020). Moreover, plugging these results on m and eNTRF into Theorems 3.4 and 3.5, we can conclude
that the bounds on the test error for GD and SGD are θ(n´1分) and θ(n´1) respectively.
4.3	Class-dependent Data Nondegeneration
In previous subsections, we have shown that under certain data separation conditions eNTRF can be
sufficiently small while the corresponding NTRF function class has R of order O(1). Thus neural
networks with polylogarithmic width enjoy nice optimization and generalization guarantees. In this
part, we consider the following much milder data separability assumption made in Zou et al. (2019).
Assumption 4.5. For all i ‰ i1 if yi ‰ yi, then }xi — Xj ∣∣22φ for some absolute constant φ.
In contrast to the conventional data nondegeneration assumption (i.e., no duplicate data points) made
in Allen-Zhu et al. (2019b); Du et al. (2019b;a); Zou and Gu (2019)§, Assumption 4.5 only requires
that the data points from different classes are nondegenerate, thus we call it class-dependent data
nondegeneration.
We have the following proposition which shows that Assumption 4.5 also implies the existence of a
good function that achieves e training error, in the NTRF function class with a certain choice of R.
Proposition 4.6. Under Assumption 4.5, if
R = Ω'n3/2φT/2 log(nb´j´1)),	m = Ω(L22n12φ-4),
We have entrf ≤ e with probability at least 1 — δ.
Proposition 4.6 suggests that under Assumption 4.5, in order to guarantee entrf ≤ e, the size of
NTRF function class needs to be Ω(n3{2). Plugging this into Theorems 3.4 and 3.5 leads to vacuous
bounds on the test error. This makes sense since Assumption 4.5 basically covers the “random label”
setting, which is impossible to be learned with small generalization error. Moreover, we would like
to point out our theoretical analysis leads to a sharper over-parameterization condition than that
proved in Zou et al. (2019), i.e., m “ ΩΩ'n14 isL16φ´4 ' n12L16。一隆一1), if the network depth satisfies
L ≤ O(n1{3 _ e―1{6).
5	Proof sketch of the main theory
In this section, we introduce a key technical lemma in Section 5.1, based on which we provide a
proof sketch of Theorems 3.3. The full proof of all our results can be found in the appendix.
^We have shown in the proof of Theorem 3.3 that m “ Ω(R8) (see (A.1) for more detail).
§Specifically, Allen-Zhu et al. (2019b); Zou and Gu (2019) require that any two data points (rather than data
points from different classes) are separated by a positive distance. Zou and Gu (2019) shows that this assumption
is equivalent to those made in Du et al. (2019b;a), which require that the composite kernel matrix is strictly
positive definite.
7
Published as a conference paper at ICLR 2021
5.1	A key technical lemma
Here we introduce a key technical lemma used in the proof of Theorem 3.3.
Our proof is based on the key observation that near initialization, the neural network function can be
approximated by its first-order Taylor expansion. In the following, we first give the definition of the
linear approximation error in a τ -neighborhood around initialization.
eapp (T q :“ sup	sup	∣fwι (Xi) — fw(xi)-χιVfw (Xi), W1 ´ wy∣.
i“1,...,n W1,WPBpWp0q,τq
If all the iterates of GD stay inside a neighborhood around initialization with small linear approxima-
tion error, then we may expect that the training of neural networks should be similar to the training of
the corresponding linear model, where standard optimization techniques can be applied. Motivated
by this, we also give the following definition on the gradient upper bound of neural networks around
initialization, which is related to the Lipschitz constant of the optimization objective function.
M(τ) :“ sup sup sup	}Vwιfw(xi)∣∣F∙
i“1,...,n l“1,...,L WPBpWp0q,τq
By definition, We can choose W* P B(Wp0q, RmT{2) such that n´1 Xn“1 '(yiFw(o),w* (xi)) “
eNTRF. Then we have the following lemma.
Lemma 5.1. Set η “ O(LTM(τ)´2). Suppose that W* P B(Wp0q,τ) and Wptq P B(Wp0q,τ)
for all 0 ≤ t ≤ t1 — 1. Then it holds that
1 I1 r∕w(t八 V }Wp0q — W*}F ´ }Wpt1q ´ W*}F + 2t1ηeNTRF
t1 t“0 LS(W " ------------------tiη(3 - 4eaρρ(τ))-------------.
Lemma 5.1 plays a central role in our proof. In specific, if Wptq P B(Wp0q, τ) for all t ≤ t1, then
Lemma 5.1 implies that the average training loss is in the same order of eNTRF as long as the linear
approximation error eapp (τ) is bounded by a positive constant. This is in contrast to the proof in Cao
and Gu (2019), Where eapp (τ) appears as an additive term in the upper bound of the training loss,
thus requiring eapp (τ) “ O(eNTRF) to achieve the same error bound as in Lemma 5.1. Since We can
show that eaρρ “ θ(m´1/6) (See Section A.1), this suggests that m “ Ω(1) is sufficient to make the
average training loss in the same order of eNTRF.
Compared with the recent results for two-layer networks by Ji and Telgarsky (2020), Lemma 5.1
is proved with different techniques. In specific, the proof by Ji and Telgarsky (2020) relies on the
1-homogeneous property of the ReLU activation function, which limits their analysis to two-layer
networks with fixed second layer weights. In comparison, our proof does not rely on homogeneity, and
is purely based on the linear approximation property of neural networks and some specific properties
of the loss function. Therefore, our proof technique can handle deep networks, and is potentially
applicable to non-ReLU activation functions and other network architectures (e.g, Convolutional
neural networks and Residual networks).
5.2 Proof sketch of Theorem 3.3
Here we provide a proof sketch of Theorem 3.3. The proof consists of two steps: (i) showing that
all T iterates stay close to initialization, and (ii) bounding the empirical loss achieved by gradient
descent. Both of these steps are proved based on Lemma 5.1.
Proofsketch of Theorem 3.3. Recall that we choose W* P B(Wp0q, Rm—1/2) such that
n´1 Xn“i '(yiFw(o),w* (xi)) “ entrf. We set τ “ Or(L1/2m—1/2R), which is chosen slightly
larger than m—1/2R since Lemma 5.1 requires the region B(Wp0q, τ) to include both W* and
tWptqut“0,...,t1. Then by Lemmas 4.1 and B.3 in Cao and Gu (2019) we know that eapp (τ) “
O(T4/3m1/2L3) “ O(R4/3L11∕3mT/6). Therefore, we can set m “ Ω(R8L22) to ensure that
eapp(τ) ≤ 1/8.
Then we proceed to show that all iterates stay inside the region B(Wp0q , τ). Since the L.H.S. of
Lemma 5.1 is strictly positive and eapp (T) ≤ 1/8, we have for all t ≤ T,
}Wp0q - W*}F - }Wptq - W*}F2-2tηeNTRF,
8
Published as a conference paper at ICLR 2021
which gives an upper bound of }Wptq — W*}f. Then by the choice of η, T, triangle inequality,
and a simple induction argument, we see that }Wptq — Wp0q}f ≤ m—1{2R ` ?2T%ntrf “
O(L1{2m-1{2R), which verifies that WPtq P B(Wp°q, τ) for t = 0,..., T — 1.
The second step is to show that GD can find a neural network with at most 3NTRF training loss within
T iterations. To show this, by the bound given in Lemma 5.1 with e&pp ≤ 1/8, we drop the terms
}Wptq — w*}F and rearrange the inequality to obtain
1 T ´1	1
T ∑ LSPWptq) & ηTT}Wp0q — W*}F + 2eNTRF.
We see that T is large enough to ensure that the first term in the bound above is smaller than eNTRF.
This implies that the best iterate among Wp0q,..., WpTTq achieves an empirical loss at most
3NTRF.
□
6	Conclusion
In this paper, we established the global convergence and generalization error bounds of GD and SGD
for training deep ReLU networks for the binary classification problem. We show that a network width
condition that is polylogarithmic in the sample size n and the inverse of target error e´1 is sufficient
to guarantee the learning of deep ReLU networks. Our results resolve an open question raised in Ji
and Telgarsky (2020).
Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. ZC, YC and QG are
partially supported by the National Science Foundation CAREER Award 1906169, IIS-2008981 and
Salesforce Deep Learning Research Award. DZ is supported by the Bloomberg Data Science Ph.D.
Fellowship. The views and conclusions contained in this paper are those of the authors and should
not be interpreted as representing any funding agencies.
References
Allen-Zhu, Z., Li, Y. and Liang, Y. (2019a). Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in Neural Information Processing Systems.
Allen-Zhu, Z., Li, Y. and Song, Z. (2019b). A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning.
Allen-Zhu, Z., Li, Y. and Song, Z. (2019c). On the convergence rate of training recurrent neural
networks. In Advances in Neural Information Processing Systems.
Arora, S., Du, S., Hu, W., Li, Z. and Wang, R. (2019a). Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. In International Conference
on Machine Learning.
ARORA, S., DU, S. S., HU, W., LI, Z., SALAKHUTDINOV, R. andWANG, R. (2019b). On exact
computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems.
Bai, Y. and Lee, J. D . (2019). Beyond linearization: On quadratic and higher-order approximation
of wide neural networks. In International Conference on Learning Representations.
Bartlett, P. L., Foster, D. J. and Telgars ky, M. J. (2017). Spectrally-normalized margin
bounds for neural networks. In Advances in Neural Information Processing Systems.
Bartlett, P. L. and Mendelson, S. (2002). Rademacher and Gaussian complexities: Risk
bounds and structural results. Journal ofMachine Learning Research 3 463-482.
Cao, Y. and Gu, Q. (2019). Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems.
9
Published as a conference paper at ICLR 2021
Cao, Y. and Gu, Q . (2020). Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. In the Thirty-Fourth AAAI Conference on Artificial Intelligence.
Cesa-Bianchi, N., Conconi, A. and Gentile, C. (2004). On the generalization ability of on-line
learning algorithms. IEEE Transactions on Information Theory 50 2050-2057.
Chizat, L., Oyallon, E. and Bach, F. (2019). On lazy training in differentiable programming. In
Advances in Neural Information Processing Systems.
Du, S., Lee, J., Li, H., Wang, L. and Zhai, X. (2019a). Gradient descent finds global minima of
deep neural networks. In International Conference on Machine Learning.
Du, S. S., Zhai, X., Poczos, B. and Singh, A. (2019b). Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations.
Frei, S., Cao, Y. and Gu, Q. (2019). Algorithm-dependent generalization bounds for overparame-
terized deep residual networks. In Advances in Neural Information Processing Systems.
He, K., Zhang, X., Ren, S. and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international conference
on computer vision.
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems.
JI, Z. and TELGARS KY, M. (2018). Risk and parameter convergence of logistic regression. arXiv
preprint arXiv:1803.07300 .
Ji, Z. and Telgarsky, M. (2020). Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations.
Kawaguchi, K. and Huang, J. (2019). Gradient descent finds global minima for generalizable deep
neural networks of practical sizes. In 2019 57th Annual Allerton Conference on Communication,
Control, and Computing (Allerton). IEEE.
Krizhevsky, A. et al. (2009). Learning multiple layers of features from tiny images .
Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Sohl-Dickstein, J. and Pennington, J.
(2019). Wide neural networks of any depth evolve as linear models under gradient descent. In
Advances in Neural Information Processing Systems.
MOHRI, M., ROSTAMIZADEH, A. and TALWALKAR, A. (2018). Foundations of machine learning.
MIT press.
Nitanda, A. and Suzuki, T. (2019). Refined generalization analysis of gradient descent for
over-parameterized two-layer neural networks with smooth activations on classification problems.
arXiv preprint arXiv:1905.09870 .
Oymak, S. and Soltanolkotabi, M. (2019). Towards moderate overparameterization: global
convergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674 .
S HALEV- S HWARTZ, S. and BEN-DAVID, S. (2014). Understanding machine learning: From theory
to algorithms. Cambridge university press.
SHAMIR, O. (2020). Gradient methods never overfit on separable data. arXiv preprint
arXiv:2007.00028.
VERSHYNIN, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027 .
Zou, D., Cao, Y., Zhou, D. and Gu, Q. (2019). Gradient descent optimizes over-parameterized
deep ReLU networks. Machine Learning .
Zou, D. and Gu, Q. (2019). An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems.
10
Published as a conference paper at ICLR 2021
A Proof of Main Theorems
In this section we provide the full proof of Theorems 3.3, 3.4 and 3.5.
A.1 Proof of Theorem 3.3
We first provide the following lemma which is useful in the subsequent proof.
Lemma A.1 (Lemmas 4.1 and B.3 in Cao and Gu (2019)). There exists an absolute constant κ such
that, with probability at least 1 —OpnL2 exp[—Ω(mτ>3L)S, for any T ≤ κL-6[log(m)S-3% it
holds that
eapp(τq ≤ O(T4{3L3m1{2}, M(T"Op?mq.
Proofof Theorem 3.3. Recall that W* is chosen such that
1n
一X '(yiFW(0),W* (xi)) “ eNTRF
n i“1
and W* P B(Wp0q, RmT/2). Note that to apply Lemma 5.1, we need the region B(Wp0q, τ) to in-
clude both W* and {Wptq}t=o,...,tι. This motivates us to set T “ (Ο(L1{2m~1{2R), which is slightly
larger than m~1{2R. With this choice of T, by Lemma A.1 we have eapp(τ) “ O(T4{3m1{2L3) “
O(R4∕3L11∕3m-1∕6). Therefore, we can set
m “ Ω(R8L22)	(A.1)
to ensure that eapp(T) ≤ 1/8, where Ω(∙) hides polylogarithmic dependencies on network depth L,
NTRF function class size R, and failure probability parameter δ. Then by Lemma 5.1, we have with
probability at least 1 — δ, we have
t1—1
}Wp0q — W* }F — }Wpt1q — W*}F > η ∑ LS(Wpt)) — 2t1ηeNTRF	(A.2)
t“0
as long as Wpoq,..., Wpt1—1q P B(Wp0q, t). In the following proof we choose η = 6(L-1m-1)
and T = JLR2 m_1n—1e―TrfS.
We prove the theorem by two steps: 1) We show that all iterates {W(0), ∙∙∙ , W(T)} will stay inside
the region B(Wp0) , T); and 2) we show that GD can find a neural network with at most 3eNTRF
training loss within T iterations.
All iterates stay inside B(Wp0q,τ). We prove this part by induction. Specifically, given t1 ≤ T, we
assume the hypothesis WHt P B(Wp0q, τ) holds for all t < t1 and prove that Wpt1q P B(Wp0q, τ).
First, it is clear that Wpoq P B(Wp0q,τ). Then by (A.2) and the fact that LS(W)20, we have
}Wpt1) — W*}F ≤ }Wpoq — W*}F + 2ηt1eNTRF
Note that T “ rLR2mTηTe[TRFs and W* P B(Wpoq, R ∙ m´ 1{2), we have
L
£ }WPt't — W*}F = }Wpt1q — W*}F ≤ CLR2m—1,
l“1
where C24 is an absolute constant. Therefore, by triangle inequality, we further have the following
for all l P rLs,
}Wpt1q — Wpp0q}F ≤ }W户—w*}f + }Wpoq — W;}f
≤ ?CLRm―1{2 + RmT/2
≤ 2√CLRmT∕2.	(A.3)
Therefore, it is clear that }Wpt q — Wpoq }f ≤ 2?CLRmT{2 ≤ T based on our choice of T
previously. This completes the proof of the first part.
11
Published as a conference paper at ICLR 2021
Convergence of gradient descent. (A.2) implies
}Wp0q ´ W*}F ´ }WpTq ´ W*}F > η^ ∑ LSPWPt)) ´ 2Tcntrf).
t“0
Dividing by ηT on the both sides, we get
1 T T
T ∑ LS(WPtq)W
T t“0
}wp0q ´ w*}f
ηT
LR2 m´
' 2cNTRF ≤ ---'-----' 2cNTRF ≤ 3eNTRF,
ηT
where the second inequality is by the fact that W* P B(Wp0q,R ∙ m´1/2) and the last in-
equality is by our choices of T and η which ensure that TneLR2mTeKTRf∙ Notice that
T “「刀尺之馆一1一4卷TRFs = O(L2R2eKTRF)∙ This completes the proof of the second part, and
we are able to complete the proof.	□
A.2 Proof of Theorem 3.4
Following Cao and Gu (2020), we first introduce the definition of surrogate loss of the network,
which is defined by the derivative of the loss function.
Definition A.2. We define the empirical surrogate error ES (W) and population surrogate error
ED (W) as follows:
1n
ES(W) :=----2' [yi ∙ fWpxiq‰, ED(W) :“ E(x,y)~D { 一 ' “y ∙ fW(x)‰(∙
n
i“1
The following lemma gives uniform-convergence type of results for ES (W) utilizing the fact that
一'()is bounded and Lipschitz continuous.
Lemma A.3. For any R,δ > 0, suppose that m = Ω(L12Rr2') ∙ riog(1{δ)S3{2, Then with probability
at least 1 ´ δ, it holds that
|ED(W) ´ ES(W)I ≤ O (min {dLL3/2RcmLR +
*1)+o D
for all W P B(WS), R ∙ m´1/2)
We are now ready to prove Theorem 3.4, which combines the trajectory distance analysis in the proof
of Theorem 3.3 with Lemma A.3.
Proof of Theorem 3.4. With exactly the same proof as Theorem 3.3, by (A.3) and induction we have
Wp0q, Wp1q,..., WpTq P B(Wp0q, RmT/2) with R = O(?LR). Therefore by Lemma A.3, we
have
IED(Wptq) ´ ES(Wptq)∣ ≤ O( min #4LL2Rcm, L?R + "[弋4 ]) + o (c⅛M)
n n	m1/6	n
for all t = 0,1,... ,T. Note that we have 1{z < 0} ≤ —2'1(z). Therefore,
ELD´1 (Wptq) ≤ 2Ed(Wptq)
≤ 2Ls (Wptq) + θ( min ,LL2Rcm, L?R + L^3 +) + θ( c l°g≡)
n n	m1/6	n
for t = 0,1,..., T, where the last inequality is by ES(W) ≤ LS(W) because —'1Pz) ≤ '(z) for all
Z P R. This finishes the proof.	□
12
Published as a conference paper at ICLR 2021
A.3 Proof of Theorem 3.5
In this section we provide the full proof of Theorem 3.5. We first give the following result, which is
the counterpart of Lemma 5.1 for SGD. Again we pick W* P B (Wp0q, RmT2) SUCh that the loss
of the corresponding NTRF model Fwp0),w* (x) achieves entrf∙
LemmaA.4. Set η “ O(LTM(T)´2). Suppose that W* P B(Wp0q,τ) and WirnI P B(Wp0q,τ)
for all 0 ≤ n ≤ n 一 1. Then it holds that
n
}Wpoq — W*}F — }Wpn1q — W*}F》(2 — 4eapp(τ))η工Li(WpiTq) — 2naNTRF∙
We introduce a surrogate loss Ei(W) “ 一'1[yi ∙ fw(xi)] and its population version ED(W) “
E(χ,y)~D [—'1[y ∙ fw(x)SS，which have been used in (Ji and Telgarsky, 2018; Cao and Gu, 2019;
Ji and Telgarsky, 2020). Our proof is based on the application of Lemma A.4 and an online-to-
batch conversion argument (Cesa-Bianchi et al., 2004; Cao and Gu, 2019; Ji and Telgarsky, 2020).
We introduce a surrogate loss Ei(W) “ 一'1[yi ∙ fw(xi)S and its population version ED(W) “
E(χ,y)~D[—'1(y ∙ fw(x))S, which have been used in (Ji and Telgarsky, 2018; Cao and Gu, 2019;
Nitanda and Suzuki, 2019; Ji and Telgarsky, 2020).
Proof of Theorem 3.5. Recall that W* is chosen such that
1n
一X '(yiFwP0),W* (xi)) “ eNTRF
n i“1
and W* P B(Wpoq, RmT/2). To apply Lemma A.4, we need the region B(Wpoq, τ) to include
both W* and {Wptq}t=o,…,〃.This motivates us to set T “ O(L1/2mT/2R), which is slightly
larger than mT/2R. With this choice of T, by Lemma A.1 we have eapp(τ) “ O(T4{3m1{2L3) “
O(R4∕3L11∕3mT∕6). Therefore, we can set
m “ Ω(R8L22)
to ensure that eapp(τ) ≤ 1/8, where Ω(∙) hides polylogarithmic dependencies on network depth L,
NTRF function class size R, and failure probability parameter δ.
Then by Lemma A.4, we have with probability at least 1 — δ,
n1
}Wpoq — W*}F — }Wpn1q — W*}F > n£ Li(WpiTq) — 2n%NTRF
i“1
(A.4)
as long as Wpoq,..., W(n1τq P B(Wpoq,τ).
We then prove Theorem 3.5 in two steps: 1) all iterates stay inside B(Wp0), τ); and 2) convergence
of online SGD.
All iterates stay inside B(Wp0) , τ). Similar to the proof of Theorem 3.3, we prove this part by
induction. Assuming Wpi) satisfies Wpi) P B(Wpoq,τ) for all i ≤ n1 — 1, by (A.4), we have
}Wpn1q — W* }F ≤ }Wpoq — W* }F + 2nηeNTRF
≤ LR2 • m´1 + 2nηeNTRF,
where the last inequality is by W* P B(Wpoq, Rm—1/2). Then by triangle inequality, we further get
}W(n1q — WMF ≤ }W(n1q — W*}f + }W* — W(oq}F
≤ }Wpn1q — W* }f + }W* — Wpoq}f
≤ O(?LRmT{ + ?nneNTRF).
Then by our choices of η = θ'm´1 ∙ (LR2nTe[TRF ^ l´1)), we have ∣∣Wpn1q — Wpoq}f ≤
2VzLRmT2 ≤ τ. This completes the proof of the first part.
13
Published as a conference paper at ICLR 2021
Convergence of online SGD. By (A.4), we have
}Wp0q ´ W*}F ´ }Wpnq ´ W*}F > η( E Li(WpiT)) ´ 2neNTRF).
i“1
Dividing by ηn on the both sides and rearranging terms, we get
i E T rw(iT八 < }Wp0q ´ w*}F ´ }Wpnq ´ w*}F	L2R2
—> Li(W v )) ≤------------------------------------+ 2eNTRF ≤------+ 3eNTRF,
n	ηn	n
i“1
where the second inequality follows from facts that W* P B(Wp°q, R ∙ m´1分) and η “ Θ (m´1
,
PLR2n´1 e´rf ^ LT)). ByLemma 4.3 in (Jiand TeIgarsky, 2020) and the fact that Ei(Wpiτq) ≤
Li (Wpiτ)),we have
1n
-∑ LDT(Wpiτq)≤
n
i“1
≤
≤
n
—E ED(WpiT))
n i“1
8 E Ei(WpiT)) + 8logpl∕δq
nn
i“1
8L2R2 + 8⅛≡+ 24cntrf.
n
n
This completes the proof of the second part.
□
B Proof of Results in Section 4
B.1	Proof of Proposition 4.2
We first provide the following lemma which gives an upper bound of the neural network output at the
initialization.
Lemma B.1 (Lemma 4.4 in Cao and Gu (2019)). Under Assumption 3.1, if meCL log(nL{δ)
with some absolute constant C, with probability at least 1 一 δ, we have
Ifwpoq (xi)| ≤ Caiog(n{δ)
for some absolute constant C.
Proof of Proposition 4.2. Under Assumption 4.1, we can find a collection of matrices U* “
{U*,…，ULU with XL“i ∣U*}F = 1 such that yi<Vfwpoq (xi), U*〉》m1/2γ for at least 1 - P
fraction of the training data. By Lemma B.1, for all i P [n] we have ∣fwpoq (xi)| ≤ C∕lcg(n∕δ) for
some absolute constant C. Then for any positive constant λ, we have for at least 1 ´ ρ portion of the
data,
yi'fwpoq (Xi) + <Vfwpoq, λU*y) > m1{2λγ ´ caiog(n∕δ).
For this fraction of data, we can set
λ “ C 1[log1/2(n∕δ) + log(1∕e)‰
m1{2γ	，
where C1 is an absolute constant, and get
m1{2λγ ´ Caiog(n∕δ)》log(1∕e).
Now we let W* “ Wp0) + λU*. By the choice of R in Proposition 4.2, we have W* P
B(Wp0q, R ∙ m´1/2). The above inequality implies that for at least 1 — P fraction of data, we
have '(yiFW(0),w* (Xi)) ≤ e. For the rest data, we have
yi(fw(0) (xi)+<VfW(O), λu*y)》—Calog(n∕δ) — λ} vfwP0q }2 > —CIR
14
Published as a conference paper at ICLR 2021
for some absolute positive constant Ci, where the last inequality follows from fact that} Vfwpoq ∣∣2 “
Orpm1{2q (see Lemma A.1 for detail). Then note that we use cross-entropy loss, it follows that for
this fraction of training data, we have '(yiFw(o),w* (Xi)) ≤ C2R for some constant C2. Combining
the results of these two fractions of training data, we can conclude
n
eNTRF ≤ n´1 X '(yiFw(0),w*(xi)) ≤ (1 ´ ρ)e ' P ∙O(R)
i“1
This completes the proof.
□
B.2 Proof of Proposition 4.4
Proof of Proposition 4.4. We are going to prove that Assumption 4.3 implies the existence of a good
function in the NTRF function class.
By Definition 3.2 and the definition of cross-entropy loss, our goal is to prove that there exists
a collection of matrices W “ {Wi, W2} satisfying max{}W1 — w10q}F, }W2 — W20q}2} ≤
R ∙ m´1/ such that
yi ∙ [fwpoq (xi)+xVWIfWp0), W1 — WpOqy ' χvW2 fwpoq, w2 — W20qy‰ > log(2∕e).
We first consider VW1 fWp0q (Xi), which has the form
(VWIfWp0)(xi))j = m1{2 ∙ w20j ∙ σ1(Xwp0j, Xiy)∙ xi
Note that w2p0,jq and wp10,jq are independently generated from N(0, 1∕m) and N (0, 2I∕m) respectively,
thus We have P(∣w20q∣20.47m—1{2)21∕2. By Hoeffeding’s inequality, We know that with
probability at least 1 — exp(—m∕8), there are at least m∕4 nodes, whose union is denoted by S,
satisfying ∣w20q |20.47mT2. Then we only focus on the nodes in the set S. Note that Wp0q and
W2p0q
are independently generated. Then by Assumption 4.3 and Hoeffeding’s inequality, there exists
a function u(∙) : Rd → Rd such that with probability at least 1 — δ1,
S W yi ∙ χu(wp0jq, Xiy ∙ σ1(χwi0j, Xiy) > Y—d 2logS1/δ q'.
Define Vj “ U(wp0j)∕w2,j if ∣w2,j| > 0.47m―1{2 and Vj “ 0 otherwise. Then we have
m
W y「w20q ∙xvj, χiy ∙ σ1(xwp0j, χiyq “ W yi ∙ xu(wp0jq, χiy ∙ σ1(χw10j, χiyq
j“1	jPS
> |S∣γ — a2∣s| iog(i∕δ1).
Set δ “ 2nδ1 and apply union bound, we have with probability at least 1 — δ∕2,
m
W yi. w20j ∙<Vj, Xi y ∙ σ1(Xwp0j, Xi〉)》|S ∣γ — a2|S | log(2n∕δ).
j“1
Therefore, note that with probability at least 1 — exp(—m∕8), we have |S|2m∕4. Moreover, in
Assumption 4.3, by yi P {±1} and ∣σ1(∙)∣, }u(∙)}2, }Xi}2 ≤ 1 for i = 1,..., n, we see that Y ≤ 1.
Then if m 》 32 log(n∕δ)∕γ2, with probability at least 1 — δ∕2 — exp — 4 log(n∕δ)∕γ2 》 1 — δ,
m
W yi ∙ w20j ∙xvj, Xi〉∙ σ1(xwp0j, Xi〉qeIS |Y ∕2∙
j“1
Let U “ (vi, V2,…，Vm)J∕√m∣s∣, we have
yiχvWιfWpoq (Xi), Uy = aS∣ W yi . w20j ∙xvj，Xiy ∙ σ1(xw10j, Xiy)》Z72一4~~^^
15
Published as a conference paper at ICLR 2021
where the last inequality is by the fact that |S|em{4. Besides, note that by concentration and
Gaussian tail bound, We have |fw(o)(xi)| ≤ Clog(n/δ) for some absolute constant C. Therefore,
let Wι “ wP0q ' 4(log(2∕e) + Clog(n∕δ))mT2U∕γ and W2 “ w20q, we have
yi , [fwP0q pxiq ' χVwιfwp0q, wι ´ WpOqy ' χVw2fwp0q, w2 ´ wg。，〉] > log(2/e). (B.D
Note that }u(∙)}2 ≤ 1, we have }U}f ≤ 1/0.47 ≤ 2.2. Therefore, we further have }wι 一
wp0q}F W 8.8γT (log(2∕e) ' Clog(n∕δ)) ∙ m´"2. This implies that W P B(wp0q, Rq with
R “ O( log (n∕(δe))∕γ). Applying the inequality '(log(2∕e)) W e on (B.1) gives
'(yi ' FW(0),W(Xi)) W e
for all i “ 1,..., n. This completes the proof.	□
B.3 Proof of Proposition 4.6
Based on our theoretical analysis, the major goal is to show that there exist certain choices of R
and m such that the best NTRF model in the function class F (wp0q , R) can achieve e training
error. In this proof, we will prove a stronger results by showing that given the quantities of R
and m specificed in Proposition 4.6, there exists a NTRF model with parameter w* that satisfies
n 1 Xi = I '(yiFWp0q,W* (xi)) W e.
In order to do so, we consider training the NTRF model via a different surrogate loss function.
2
Specifically, we consider squared hinge loss '(χ) “( max{λ — x, 0}) , where λ denotes the target
margin. In the later proof, we choose λ “ log(1∕e) ' 1 such that the condition '(x) W 1 can
guarantee that Xelog(e). Moreover, we consider using gradient flow, i.e., gradient descent with
infinitesimal step size, to train the NTRF model. Therefore, in the remaining part of the proof, we
consider optimizing the NTRF parameter w with the loss function
1n
LS(W) “ n X''yiFWp0q,w(xi)).
i“1
Moreover, for simplicity, we only consider optimizing parameter in the last hidden layer (i.e., Wl´i).
Then the gradient flow can be formulated as
dwd[(t) “一Vwlt LS (W(t)),	dW≡ “ 0 for any l ‰ L — 1.
Note that the NTRF model is a linear model, thus by Definition 3.2, we have
VWL_1LS(w(t)q = yiL(yiFwP0),w(t)(χiq) ∙ VWL´Fw(0),w(t)(χiq
“ yi'1'yiFWp0q,W(t)(xi)) ` VWpOqIfWp0q (xi).	(BZ
Then it is clear that NWL´` LS(W(t)) has fixed direction throughout the optimization.
In order to prove the convergence of gradient flow and characterize the quantity of R, We first provide
the following lemma which gives an upper bound of the NTRF model output at the initialization.
Then we provide the following lemma which characterizes a lower bound of the Frobenius norm of
the partial gradient NWL´` LS(W).
Lemma B.2 (Lemma B.5 in Zou et al. (2019)). Under Assumptions 3.1 and 4.5, if m “ Ω(n2φ-1),
then for all t20, with probability at least 1 — exp ( — O(mφ∕n)), there exist a positive constant C
such that
}VWl — iLS(w(t))}F》
Cmφ
n5
n
X L1 'yiFWpoq ,wpt) (xiq)
i“1
2
We slightly modified the original version of this lemma since we use different models (we consider
NTRF model while Zou et al. (2019) considers neural network model). However, by (B.2), it is clear
,1 ,,1	1 ∙	X—7	/ɪɪ T∖	1	1 1	,	C,1	1 ∙ . C-	1	1	Il ,,1
that the gradient VLS(w) can be regarded as a type of the gradient for neural network model at the
initialization (i.e., RWL´` LS(W(O))) is valid. Now we are ready to present the proof.
16
Published as a conference paper at ICLR 2021
Proofof Proposition 4.6. Recall that We only consider training the last hidden weights, i.e., Wl´i,
via gradient flow with squared hinge loss, and our goal is to prove that gradient flow is able to
find a NTRF model within the function class F pWp0q , Rq around the initialization, i.e., achieving
n´1 XNι '(yiFwPoq,w* (Xi)) ≤ e. Let WPtq be the weights at time t, gradient flow implies that
dLSWtqq = TVWL´iLS(W(t))}F W ´Cmφ(亭(yiFwPoq,wq(Xi)))2 “	,
where the first equality is due to the fact that we only train the last hidden layer, the first inequality
is by Lemma B.2 and the second equality follows from the fact that'(∙) = —2V'(∙). Solving the
above inequality gives
LS(W(t)) ≤ LS(W(0)) ∙ exp (— 4Cmφt).
(B.3)
Then, set T = θ(n3mTφT ∙ log(Ls(W(0)){e1)) and e1 = 1{n, we have LS(Wptqq ≤ e1. Then
it follows that'ViFWPo),Wptq(Xiq) ≤ 1, which implies that yiFW(oq,wptq(xi)》log(eq and thus
n´1 Xn“i '(yiFw(o),w* (Xiq) ≤ e. Therefore, W(Tq is exactly the NTRF model we are looking
for.
The next step is to characterize the distance between W(Tq and W(0q in order to characterize the
quantity of R. NOtethat }VwltLs(Wptqq}F24CmφLs(Wptqq∕n3, we have
dJLs(W(tqq	IVwltLs(Wptqq}F V	ll▽	r	C 1{2m1{2φ1{2
dt=W —}VWLTLS (W(tqq}F3-3i12.
dt	2 Lrs(W(tqq	n
Taking integral on both sides and rearranging terms, we have
(]}vwltLs (Wptqq}f dt W C1/2，" ∙ (JLS (Wpoqq- JLS (Wptqq).
Note that the L.H.S. of the above inequality is an upper bound of }W(tq — W(0q}F, we have for any
t > 0,
}W(tq ´ W(0q}F W C 1/^2φ1/2 ∙ JLS(W(0qq “ O^"”D
where the second inequality is by Lemma B.1 and our choice of λ = log(1∕eq ` 1. This implies that
there exists a point W* within the class F(Wp0q, r) with
R = O (n3/2 log 'n/pδeq))
such that
n
eNTRF := n´1 X '(yiFw(o),w* (Xiq) W e.
i“1
Then by Theorem 3.3, and, more specifically, (A.1), we can compute the minimal required neural
network width as follows,
m = Ω (R8L22q = ω( L2n12).
This completes the proof.	□
C Proof of Technical Lemmas
Here we provide the proof of Lemmas 5.1, A.3 and A.4.
17
Published as a conference paper at ICLR 2021
C.1 Proof of Lemma 5.1
The detailed proof of Lemma 5.1 is given as follows.
ProofofLemma 5.1. Based on the update rule of gradient descent, i.e., Wpt'1q “ WPtq 一
ηVw LS pwptqq, we have the following calculation.
}wptq ´ w*}F ´ }wpt'1q ´ w*}F
nL
“ T XxWPtq ´ w*, VWLipWmE }VwιLs(WPtq)}F,	(C.1)
loooooooooooooooooooooooooooooooooooooooooooon loooooooooooooooooooooooooooon
V	V
I1	I2
where the equation follows from the fact that LSPWPtq) “ n´1 XNi LiPWPtq). In What follows,
we first bound the term I1 on the R.H.S. of (C.1) by approximating the neural network functions with
linear models. By assumption, for t “ 0, . . . , t1 ´ 1, Wptq, W* P BPWp0q, τ). Therefore by the
definition of app Pτ),
yi '〈VfWpt) PXiq, WPtq	- w*y ≤	yi	∙	'fwPt	pxiq	´	fW*	(xi))	' Eapp(τq	(Cz
Moreover, we also have
0 ≤ yi ' (fW* (xi) ´ fWp0) (xi) ´〈VfWpO) (xiq, W* ´ WPOqy) ' e&pp(Tq
“ yi ' (fW* (xiq ´ FWp0),W* (xiq) ' eapp(τq,	(C.3)
where the equation follows by the definition of FWpoq ,w* (xq. Adding (C.3) to (C.2) and canceling
the terms yi ∙ fw* (xiq, we obtain that
yi '〈VfWpt) (xiq, WPtq	- W*y ≤	yi	∙	(fWpt)	(Xiq	—	FWp0),W* (xiq)	'	2eapp(Tq.	(C.4)
We can now give a lower bound on first term on the R.H.S. of (C.1). For i “ 1, . . . , n, applying the
chain rule on the loss function gradients and utilizing (C.4), we have
xWptq ´ W*, VwLi(Wptqqy “ '1(yifWpt) (Xiq) ∙ yi ∙ xWptq ´ W*, VWfWpt) (x。〉
》' (yifWpt) (Xiq) ' (yifWpt) (xiq — yifW* (xiq ' 2eapp(tq)
》(1 ´ 2eapp(τqq'(yifwpt) (xiq) ´ '(yiFWp0),W* (xiq), (C.5)
where the first inequality is by the fact that '1 (yifwpt) (Xiq) < 0, the second inequality is by convexity
of '(∙q and the fact that ´'1 (yifwpt) (Xiq) ≤ '(yifwpt) (xiq).
We now proceed to bound the term I2 on the R.H.S. of (C.1). Note that we have '1(∙q < 0, and
therefore the Frobenius norm of the gradient VWl LS (WPtqq can be upper bounded as follows,
}VWlLS(WPtqq}F“
1n
—X ' (yifWpt (Xiq)VWlfWpt) (xiq
n i“1
F
1n
≤ n X ´' (yifWpt) (xiq) ' Il VWl fWpt) (xiq}F,
i“1
where the inequality follows by triangle inequality. We now utilize the fact that cross-entropy loss
satisfies the inequalities —'1 (∙q ≤ '(∙q and —'1(∙q ≤ 1. Therefore by definition of M(τq, we have
L
∑ }VwlLs(WPtqq}F ≤
l“1
θ(LM(τq2) ∙ ^ 1E ´'1(yifWpt) (Xiq))2
≤ O(LM(τq2) ∙ LS(Wptqq.
(C.6)
Then we can plug (C.5) and (C.6) into (C.1) and obtain
}wptq — w*}F — }wpt'1q — w*}F
18
Published as a conference paper at ICLR 2021
2η n
》^n Σ Ip1 ´ 2eapppτ qq''yifWPtq PXiq) ´ ''yi FW(0q,W* pxiq)[ ´ O'η LMPT q ) , LSPW q
i“1
>
2 ´ 4eapp(τq ηLs(WPtq)´^n X '(yiFWp0q,W*
i“1
PXiq ,
where the last inequality is by η “ OPLTM (τ)´2) and merging the third term on the second line
into the first term. Taking telescope sum from t “ 0 to t “ t1 ´ 1 and plugging in the definition
n Xn“1 ''yiFw(0),w* (Xi)) “ eNTRF completes the proof.	□
C.2 Proof of Lemma A.3
Proof of Lemma A.3. We first denote W “ B(WP0q , Rr • mT2), and define the corresponding
neural network function class and surrogate loss function class as F “ tfW (Xq : W P Wu and
G “ {一'[y • fw (x)S : W P W U respectively.
By standard uniform convergence results in terms of empirical Rademacher complexity (Bartlett and
Mendelson, 2002; Mohri et al., 2018; Shalev-Shwartz and Ben-David, 2014), with probability at least
1 ´ δ we have
sup |Es(Wq´ ED(Wq∣ = sup
WPW	WPW
1n
---X '1[yn ' fW (xi)‰ ' E(x,y)~D' [y • fw (x)‰
ni“1
≤ 2Rpn(Gq + Cl Jlogp1/δq,
n
where C1 is an absolute constant, and
Rn(G) “ Eξi~Unif(t±1})	SUP ɪ X ξi'1[yn • fw(xi)‰ )
WPW ni“1
is the empirical Rademacher complexity of the function class G . We now provide two bounds
on Rn (Gq, whose combination gives the final result of Lemma A.3. First, by Corollary 5.35 in
(Vershynin, 2010), with probability at least 1 — L • exp(-Ω(m)), }Wp0q}2 ≤ 3 for all l P [L].
Therefore for all W P W, we have } Wi ∣∣2 ≤ 4. Moreover, standard concentration inequalities on the
norm of the first row of WPO) also implies that} Wi ∣∣220.5 for all W P W and l P [L]. Therefore,
an adaptation of the bound in (Bartlett et al., 2017)，gives
^ 一、 <v
Rpn(Fq ≤ O
L
Π}W112 •
i“1
L
X
i“1
}WJ - W"2{3 ff3{2
-}Wi}2{3-
& O( WuW {手• [pmn ∙∣wj ´w”F q2{3
& W2R • Cm).
F
(C.7)
We now derive the second bound on Rn(Gq, which is inspired by the proof provided in (Cao and
Gu, 2020). Since y P { + 1,1}, ∣'1(z)∣ ≤ 1 and '1(z) is 1-Lipschitz continuous, by standard empirical
Rademacher complexity bounds (Bartlett and Mendelson, 2002; Mohri et al., 2018; Shalev-Shwartz
and Ben-David, 2014), we have
^ ^
Rn(G q≤ Rn(F)= Eξi~Unif({±1})
1n
SuP — E ξifw(xn)
WPW ni“1
TBartIett et al. (2017) only proved the Rademacher complexity bound for the composition of the ramp loss
and the neural network function. In our setting essentially the ramp loss is replaced with the ´'1(∙) function,
which is bounded and 1-Lipschitz continuous. The proof in our setting is therefore exactly the same as the proof
given in (Bartlett et al., 2017), and we can apply Theorem 3.3 and Lemma A.5 in (Bartlett et al., 2017) to obtain
the desired bound we present here.
19
Published as a conference paper at ICLR 2021
where RnpFq is the empirical Rademacher complexity of the function class F. We have
RpnrFs ≤ Eξ∖ sup — £ ξi “fWpxiq ´ FWpo)W pxiq‰ f ' Eξ ∖ sup - £ ξiFWP0q Wpxiq f ,
WPW n i“1	,	WPW n i“1	,
looooooooooooooooooooooooooooomooooooooooooooooooooooooooooon loooooooooooooooooooomoooooooooooooooooooon
*V
I2
"V"
I1
(C.8)
where FWp0),W pxq “ fWp0) pxq ` VWfWp0) pxq, W ´ Wp0q . For I1, by Lemma 4.1 in (Cao and
Gu, 2019), with probability at least 1 ´ δ{2 we have
I1 ≤ max IfW(Xi) — FWp0q,Wpxiq I ≤ θ(L3R4∕3mT∕6aiog(m)),
For I2, note that Eξ [ SupWPW Xn“1 ξifWpo) pxiq] “ 0. By CaUchy-SchWarz inequality We have
1L
I “ -	Eξ
n l“1
RmT2
<	----
n
sup Tr
}W i}f WRmT2
n
W J £ ξiVWι fWp0) pxiq
i“1
)1
L
£ Eξ
l“1
n
ξiVWlfWp0) pxiq
i“1
F
Therefore
L
n
RmT2
I ≤ ----
n
RmT2
n
eEξ ››	ξiVWlfWp0) pxiq
l“1
Lf
i“1
, 〜
L ∙ R
≤ O ,
n
£ f£ ››VWlfWp0)pxiq››2F
l“1
),
i“1
n
2
F
where we apply Jensen’s inequality to obtain the first inequality, and the last inequality follows by
Lemma B.3 in (Cao and Gu, 2019). Combining the bounds of I1 and I2 gives
L3Rr4{3
RnrF] ≤ O + ~m76~)
Further combining this bound With (C.7) and recaling δ completes the proof.
□
C.3 Proof of Lemma A.4
Proof of Lemma A.4. Different from the proof of Lemma 5.1, online SGD only queries one data to
update the model parameters in each iteration, i.e., Wi'1 “ Wi — ηVLi'ι (Wpiqq. By this update
rule, We have
}Wpiq — w*}F ´ }Wpi'1q — w*}F
L
“ 2ηχwpi) ´ w*, VWLi'1 (Wpi)q〉´ η2 £ }Vw,Li'1pwpiqq}F.	(c.9)
With exactly the same proof as (C.5) in the proof of Lemma 5.1, We have
XWptq ´ w*, VwLipWpt)q〉》pl ´ 2eaρρpτqq'(yifwpt) Xq) ´ '(yiFWp0),w* Xq), (C.10)
for all i = 0,..., n1 — l. By the fact that —'1p∙q ≤ 'p∙q and ´'p∙q ≤ l, we have
LL
£ }VW1 Li'1pWpi) q}F ≤ £ '(yi'1fWt pxi'1q) ∙ }VW1 fwpi) pxi'1q}F
l“1	l“1
20
Published as a conference paper at ICLR 2021
≤ O(LM(T)2) ∙ Li'iPWpiqq.	(C.11)
Then plugging (C.10) and (C.11) into (C.9) gives
}wpiq — w*}F — }wpi'1q — w*}F
》(2 ´ 4eapp(τ))ηLi'i(Wpiq) ´ 2η'(yiFwpoq,w. (xi)) ´ O(η2LM(T)2)Li'i(Wpiq)
》(2 — 4eapp(τ))ηLi'l(WPiq) — 2η'(yiFWp0q ,W* (xi)),
where the last inequality is by η “ O(LTM(τ)´2) and merging the third term on the second line
into the first term. Taking telescope sum over i “ 0, . . . , n1 ´ 1, we obtain
}wp0q — w*}F ´ }wpn1q — w*}F
n1	n1
>(2 ´ 4eapp(τ))η二 Li(WPiTq)´ 2η= '(yiFwpoq,w* (xi)).
1
nn
>(2 ´ 4eapp(τ))η二 Li(WPiTq)´ 2η= '(yiFwpoq,w* (xi)).
1
3n
》(2 — 4eapp(τ))η X Li(WPiTq)´ 2nηeNTRF∙
This finishes the proof.	□
D Experiments
In this section, we conduct some simple experiments to validate our theory. Since our pa-
per mainly focuses on binary classification, we use a subset of the original CIFAR10 dataset
(Krizhevsky et al., 2009), which only has two classes of images. We train a 5-layer fully-
connected ReLU network on this binary classification dataset with different sample sizes (n P
t100, 200, 500, 1000, 2000, 5000, 10000u), and plot the minimal neural network width that is required
to achieve zero training error in Figure 1 (solid line). We also plot O(n), O(log3(n)), O(log2 (n))
and O(log(n)) in dashed line for reference. It is evident that the required network width to achieve
zero training error is polylogarithmic on the sample size n, which is consistent with our theory.
50
£ 45
Ξ 40
M 35
l3°
E25
3 20
⊂ 15
M io
5
0
；Ο(n)
∕og3(n))
(∕og2(n))
______,
Ο(fog(n))
2000	4000	6000	8000	10000
Sample Size (π)
(a) “cat" vs, "dog"
50-
£ 45-
Ξ 40-
I 35-
Z 30-
Q
一
3 20-
⊂ 15-
至10-
5-
Figure 1: Minimum network width that is required to achieve zero training error with respect to
the training sample size (blue solid line). The hidden constants in all Ο(∙) notations are adjusted to
ensure their plots (dashed lines) start from the same point.
21