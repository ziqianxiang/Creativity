Published as a conference paper at ICLR 2021
RNNLogic: Learning Logic Rules for Reason-
ing on Knowledge Graphs
Meng Qu*1,2,Junkun Chen*3, Louis-Pascal Xhonneux1,2, Yoshua Bengio1,2,5, Jian Tang1,4,5
1	Mila- Quebec AI Institute
2	Universite de Montreal
3	Tsinghua University
4	HEC Montreal
5	Canadian Institute for Advanced Research (CIFAR)
Ab stract
This paper studies learning logic rules for reasoning on knowledge graphs. Logic
rules provide interpretable explanations when used for prediction as well as being
able to generalize to other tasks, and hence are critical to learn. Existing methods
either suffer from the problem of searching in a large search space (e.g., neural logic
programming) or ineffective optimization due to sparse rewards (e.g., techniques
based on reinforcement learning). To address these limitations, this paper proposes
a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent
variable, and simultaneously trains a rule generator as well as a reasoning predictor
with logic rules. We develop an EM-based algorithm for optimization. In each
iteration, the reasoning predictor is first updated to explore some generated logic
rules for reasoning. Then in the E-step, we select a set of high-quality rules from all
generated rules with both the rule generator and reasoning predictor via posterior
inference; and in the M-step, the rule generator is updated with the rules selected
in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.
1	Introduction
Knowledge graphs are collections of real-world facts, which are useful in various applications. Each
fact is typically specified as a triplet (h, r, t) or equivalently r(h, t), meaning entity h has relation
r with entity t. For example, Bill Gates is the Co-founder of Microsoft. As it is impossible to
collect all facts, knowledge graphs are incomplete. Therefore, a fundamental problem on knowledge
graphs is to predict missing facts by reasoning with existing ones, a.k.a. knowledge graph reasoning.
This paper studies learning logic rules for reasoning on knowledge graphs. For example, one may
extract a rule ∀X,Y,Z hobby(X, Y) J friend(X, Z) ∧ hobby(Z,Y), meaning that if Z is a
friend of X and Z has hobby Y, then Y is also likely the hobby of X . Then the rule can be applied
to infer new hobbies of people. Such logic rules are able to improve interpretability and precision
of reasoning (Qu & Tang, 2019; Zhang et al., 2020). Moreover, logic rules can also be reused and
generalized to other domains and data (Teru & Hamilton, 2020). However, due to the large search
space, inferring high-quality logic rules for reasoning on knowledge graphs is a challenging task.
Indeed, a variety of methods have been proposed for learning logic rules from knowledge graphs. Most
traditional methods such as path ranking (Lao & Cohen, 2010) and Markov logic networks (Richard-
son & Domingos, 2006) enumerate relational paths on graphs as candidate logic rules, and then learn
a weight for each rule as an assessment of rule qualities. There are also some recent methods based
on neural logic programming (Yang et al., 2017) and neural theorem provers (Rocktaschel & Riedel,
2017), which are able to learn logic rules and their weights simultaneously in a differentiable way.
Though empirically effective for prediction, the search space of these methods is exponentially large,
making it hard to identify high-quality logic rules. Besides, some recent efforts (Xiong et al., 2017)
formulate the problem as a sequential decision making process, and use reinforcement learning to
search for logic rules, which significantly reduces search complexity. However, due to the large
action space and sparse reward in training, the performance of these methods is not yet satisfying.
*Equal contribution.
1
Published as a conference paper at ICLR 2021
In this paper, we propose a principled probabilistic approach called RNNLogic which overcomes the
above limitations. Our approach consists of a rule generator as well as a reasoning predictor with
logic rules, which are simultaneously trained to enhance each other. The rule generator provides
logic rules which are used by the reasoning predictor for reasoning, while the reasoning predictor
provides effective reward to train the rule generator, which helps significantly reduce the search space.
Specifically, for each query-answer pair, e.g., q = (h, r, ?) and a = t, we model the probability of the
answer conditioned on the query and existing knowledge graph G, i.e., p(a|G, q), where a set of logic
rules z 1 is treated as a latent variable. The rule generator defines a prior distribution over logic rules
for each query, i.e., p(z|q), which is parameterized by a recurrent neural network. The reasoning
predictor computes the likelihood of the answer conditioned on the logic rules and the existing
knowledge graph G, i.e., p(a|G, q, z). At each training iteration, we first sample a few logic rules
from the rule generator, and further update the reasoning predictor to try out these rules for prediction.
Then an EM algorithm (Neal & Hinton, 1998) is used to optimize the rule generator. In the E-step, a
set of high-quality logic rules are selected from all the generated rules according to their posterior
probabilities. In the M-step, the rule generator is updated to imitate the high-quality rules selected in
the E-step. Extensive experimental results show that RNNLogic outperforms state-of-the-art methods
for knowledge graph reasoning 2 . Besides, RNNLogic is able to generate high-quality logic rules.
2	Related Work
Our work is related to existing efforts on learning logic rules for knowledge graph reasoning. Most
traditional methods enumerate relational paths between query entities and answer entities as candidate
logic rules, and further learn a scalar weight for each rule to assess the quality. Representative methods
include Markov logic networks (Kok & Domingos, 2005; Richardson & Domingos, 2006; Khot et al.,
2011), relational dependency networks (Neville & Jensen, 2007; Natarajan et al., 2010), rule mining
algorithms (Galarraga et al., 2013; Meilicke et al., 2019), path ranking (Lao & Cohen, 2010; Lao et al.,
2011) and probabilistic personalized page rank (ProPPR) algorithms (Wang et al., 2013; 2014a;b).
Some recent methods extend the idea by simultaneously learning logic rules and the weights in
a differentiable way, and most of them are based on neural logic programming (Rocktaschel &
Riedel, 2017; Yang et al., 2017; Cohen et al., 2018; Sadeghian et al., 2019; Yang & Song, 2020) or
neural theorem provers (ROcktaScheI & Riedel, 2017; Minervini et al., 2020). These methods and
our approach are similar in spirit, as they are all able to learn the weights of logic rules efficiently.
However, these existing methods try to simultaneously learn logic rules and their weights, which is
nontrivial in terms of optimization. The main innovation of our approach is to separate rule generation
and rule weight learning by introducing a rule generator and a reasoning predictor respectively, which
can mutually enhance each other. The rule generator generates a few high-quality logic rules, and
the reasoning predictor only focuses on learning the weights of such high-quality rules, which
significantly reduces the search space and leads to better reasoning results. Meanwhile, the reasoning
predictor can in turn help identify some useful logic rules to improve the rule generator.
The other kind of rule learning method is based on reinforcement learning. The general idea is to
train pathfinding agents, which search for reasoning paths in knowledge graphs to answer questions,
and then extract logic rules from reasoning paths (Xiong et al., 2017; Chen et al., 2018; Das et al.,
2018; Lin et al., 2018; Shen et al., 2018). However, training effective pathfinding agents is highly
challenging, as the reward signal (i.e., whether a path ends at the correct answer) can be extremely
sparse. Although some studies (Lin et al., 2018) try to get better reward by using embedding-based
methods for reward shaping, the performance is still worse than most embedding-based methods. In
our approach, the rule generator has a similar role to those pathfinding agents. The major difference
is that we simultaneously train the rule generator and a reasoning predictor with logic rules, which
mutually enhance each other. The reasoning predictor provides effective reward for training the rule
generator, and the rule generator offers high-quality rules to improve the reasoning predictor.
Our work is also related to knowledge graph embedding, which solves knowledge graph reasoning
by learning entity and relation embeddings in latent spaces (Bordes et al., 2013; Wang et al., 2014c;
Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Cai & Wang, 2018; Dettmers et al., 2018;
Balazevic et al., 2019; Sun et al., 2019). With proper architectures, these methods are able to learn
1More precisely, z is a multiset. In this paper, we use “set” to refer to “multiset” for conciseness.
2The codes of RNNLogic are available: https://github.com/DeepGraphLearning/RNNLogic
2
Published as a conference paper at ICLR 2021
Query
q = (h,r,?)
Logic Rules
High-quality Logic Rules
2,〜p&w(z,∣g,q,a)
RUle Generator
Pe (z|q)
Answer
a = t
Reasoning Predictor
Pw(α∣9q,z)
•一""Likelihood
Knowledge Graph
Figure 1: RNNLogic consists of a rule generator pθ and a reasoning predictor pw . Given a query,
the rule generator generates logic rules for the reasoning predictor. The reasoning predictor takes
the generated rules as input, and reasons on a knowledge graph to predict the answer. RNNLogic is
optimized with an EM-based algorithm. In each iteration, the rule generator produces some logic
rules, and we update the reasoning predictor to explore these rules for reasoning. Then in the E-step,
a set of high-quality rules are identified from all generated rules via posterior inference. Finally in the
M-step, the rule generator is updated to be consistent with the high-quality rules identified in E-step.
some simple logic rules. For example, TransE (Bordes et al., 2013) can learn some composition
rules. RotatE (Sun et al., 2019) can mine some composition rules, symmetric rules and inverse rules.
However, these methods can only find some simple rules in an implicit way. In contrast, our approach
explicitly trains a rule generator, which is able to generate more complicated logic rules.
There are some works studying boosting rule-based models (Goldberg & Eckstein, 2010; Eckstein
et al., 2017), where they dynamically add new rules according to the rule weights learned so far. These
methods have been proven effective in binary classification and regression. Compared with them, our
approach shares similar ideas, as we dynamically update the rule generator with the feedback from
the reasoning predictor, but we focus on a different task, i.e., reasoning on knowledge graphs.
3	Model
In this section, we introduce the proposed approach RNNLogic which learns logic rules for knowledge
graph reasoning. We first formally define knowledge graph reasoning and logic rules.
Knowledge Graph Reasoning. Let pdata(G, q, a) be a training data distribution, where G is a
background knowledge graph characterized by a set of (h, r, t)-triplets which we may also write as
r(h, t), q = (h, r, ?) is a query, and a = t is the answer. Given G and the query q, the goal is to
predict the correct answer a. More formally, we aim to model the probabilistic distribution p(a|G, q).
Logic Rule. We perform knowledge graph reasoning by learning logic rules, where logic rules in this
paper have the conjunctive form ∀{Xi}i=° r(Xo, Xi) - r1(X0, Xi) ∧ …∧ rι(Xι-ι,Xι) with l
being the rule length. This syntactic structure naturally captures composition, and can easily express
other common logic rules such as symmetric or inverse rules. For example, let r-1 denote the inverse
relation of relation r, then each symmetric rule can be expressed as ∀{X,Y} r(X, Y) - r-1(X, Y).
In RNNLogic, we treat a set of logic rules which could explain a query as a latent variable we have to
infer. To do this, we introduce a rule generator and a reasoning predictor using logic rules. Given a
query, the rule generator employs a recurrent neural network to generate a set of logic rules, which are
given to the reasoning predictor for prediction. We optimize RNNLogic with an EM-based algorithm.
In each iteration, we start with updating the reasoning predictor to try out some logic rules generated
by the rule generator. Then in the E-step, we identify a set of high-quality rules from all generated
rules via posterior inference, with the prior from the rule generator and likelihood from the reasoning
predictor. Finally in the M-step, the rule generator is updated with the identified high-quality rules.
3.1	Probabilistic Formalization
We start by formalizing knowledge graph reasoning in a probabilistic way, where a set of logic rules z
is treated as a latent variable. The target distribution p(a|G, q) is jointly modeled by a rule generator
and a reasoning predictor. The rule generator pθ defines a prior over a set of latent rules z conditioned
on a query q, while the reasoning predictor pw gives the likelihood of the answer a conditioned on
latent rules z, the query q, and the knowledge graph G. Thus p(a|G, q) can be computed as below:
pw,θ (a|G, q) = Epw (a|G, q, z)pθ (z|q) = EpO(z∣q)[pw (a|G, q, z)].	(1)
z
3
Published as a conference paper at ICLR 2021
The goal is to jointly train the rule generator and reasoning predictor to maximize the likelihood of
training data. Formally, the objective function is presented as below:
max OGw) = E(G,q,a)〜Pdata [log Pw,θ (HG, q)] = E(G,q,a)〜Pdata [log Epθ (z∣q)[Pw (a|G, q, Z)]].⑵
θ,w
3.2	Parameterization
Rule Generator. The rule generator defines the distributionpθ(z|q). For a query q, the rule generator
aims at generating a set of latent logic rules z for answering the query.
Formally, given a query q = (h, r, ?), we generate compositional logic rules by only considering the
query relation r without the query entity h, which allows the generated rules to generalize across
entities. For each compositional rule in the abbreviation form r — r ι ∧ …∧ ri, it can be viewed as a
sequence of relations [r, ri, r2 …ri, eend], where r is the query relation or the rule head, {ri}i=ι
are the body of the rule, and rEND is a special relation indicating the end of the relation sequence.
Such relation sequences can be effectively modeled by recurrent neural networks (Hochreiter &
Schmidhuber, 1997), and thus we introduce RNNθ to parameterize the rule generator. Given a query
relation r, RNNθ sequentially generates each relation in the body of a rule, until it reaches the ending
relation rEND. In this process, the probabilities of generated rules are simultaneously computed. With
such rule probabilities, we define the distribution over a set of rules z as a multinomial distribution:
Pθ(z|q) = Mu(z∣N, RNNθ(∙∣r)),	(3)
where Mu stands for multinomial distributions, N is a hyperparameter for the size of the set z, and
RNNθ(∙∣r) defines a distribution over compositional rules with rule head being r. The generative
process of a rule set z is quite intuitive, where we simply generate N rules with RNNθ to form z .
Reasoning Predictor with Logic Rules. The reasoning predictor definespw(a|G, q, z). For a query
q, the predictor uses a set of rules z to reason on a knowledge graph G and predict the answer a.
Following stochastic logic programming (Cussens, 2000), a principled reasoning framework, we
use a log-linear model for reasoning. For each query q = (h, r, ?), a compositional rule is able
to find different grounding paths on graph G , leading to different candidate answers. For exam-
ple, given query (Alice, hobby, ?), a rule hobby J friend ∧ hobby can have two ground-
ings, hobby (Alice, Sing) J friend(Alice, Bob) ∧hobby (Bob, Sing) and hobby(Alice, Ski) J
friend(Alice, Charlie) ∧ hobby(Charlie, Ski), yielding two candidate answers Sing and Ski.
Let A be the set of candidate answers which can be discovered by any logic rule in the set z . For
each candidate answer e ∈ A, we compute the following scalar scorew(e) for that candidate:
scorew (e) =	scorew (e|rule) = Σ Σ	ψw(rule) ∙ φw(path),
rule∈z	rule∈z path∈P (h,rule,e)
(4)
where P(h, rule, e) is the set of grounding paths which start at h and end at e following a rule (e.g.,
Alice -f-r-ie-n→d Bob -h-o-b-b→y Sing). ψw(rule) and φw(path) are scalar weights of each rule and path.
Intuitively, the score of each candidate answer e is the sum of scores contributed by each rule, i.e.,
scorew(e|rule). To get scorew(e|rule), we sum over every grounding path found in the graph G.
For the scalar weight ψw(rule) of each rule, it is a learnable parameter to optimize. For the score
φw(path) of each specific path, we explore two methods for parameterization. One method always
sets φw(path) = 1. However, this method cannot distinguish between different relational paths. To
address the problem, for the other method, we follow an embedding algorithm RotatE (Sun et al.,
2019) to introduce an embedding for each entity and model each relation as a rotation operator
on entity embeddings. Then for each grounding path of rule starting from h to e, if we rotate the
embedding of h according to the rotation operators defined by the body relations of rule, we should
expect to obtain an embedding close to the embedding of e. Thus we compute the similarity between
the derived embedding and the embedding of e as φw(path), which can be viewed as a measure of the
soundness and consistency of each path. For example, given a path Alice -f-ri-e-n→d Bob -h-o-b-b→y Sing,
we rotate Alice’s embedding with the operators defined by friend and hobby. Afterwards we
compute the similarity of the derived embedding and embedding of Sing as φw(path). Such a method
allows us to compute a specific φw(path) for each path, which further leads to more precise scores
for different candidate answers. See App. C for more details of the parameterization method.
4
Published as a conference paper at ICLR 2021
Once we have the score for every candidate answer, we can further define the probability that the
answer a of the query q is entity e by using a softmax function as follows:
pw(a = e|G, q,z)
exp(scorew (e))
Peo∈A exp(scorew (e0))
(5)
3.3	Optimization
Next, we introduce how we optimize the reasoning predictor and rule generator to maximize the
objective in Eq. (2). At each training iteration, we first update the reasoning predictor pw according
to some rules generated by the generator, and then update the rule generator pθ with an EM algorithm.
In the E-step, a set of high-quality rules are identified from all generated rules via posterior inference,
with the rule generator as the prior and the reasoning predictor as the likelihood. In the M-step, the
rule generator is then updated to be consistent with the high-quality rules selected in the E-step.
Formally, at each training iteration, we start with maximizing the objective O(θ, w) in Eq. (2) with
respect to the reasoning predictor pw . To do that, we notice that there is an expectation operation
with respect to pθ(z|q) for each training instance (G, q, a). By drawing a sample Z 〜pθ(z|q) for
query q, we can approximate the objective function of w at each training instance (G, q, a) as below:
O(G,q,a)(w) = logEpθ(z∣q)[pw(a|G, q, z)] ≈ logPw(a|G, q, Z)	(6)
Basically, We sample a set of rules Z from the generator and feed Z into the reasoning predictor. Then
the parameter w of the reasoning predictor is updated to maximize the log-likelihood of the answer a.
With the updated reasoning predictor, we then update the rule generator pθ to maximize the objective
O(θ, w). In general, this can be achieved by REINFORCE (Williams, 1992) or the reparameterization
trick (Jang et al., 2017; Maddison et al., 2017), but they are less effective in our problem due to the
large number of logic rules. Therefore, an EM framework is developed to optimize the rule generator.
E-step. Recall that when optimizing the reasoning predictor, we draw a set of rules Z for each data
instance (G, q, a), and let the reasoning predictor use Z to predict a. For each data instance, the E-step
aims to identify a set of K high-quality rules ZI from all generated rules Z, i.e., ZI ⊂ Z, |zi| = K.
Formally, this is achieved by considering the posterior probability of each subset of logic rules ZI,
i.e., pθ,w(ZI|G, q, a) 8 pw(a∣G, q, ZI)pθ(ZI|q), with prior of ZI from the rule generator pθ and
likelihood from the reasoning predictor pw . The posterior combines knowledge from both the rule
generator and reasoning predictor, so the likely set of high-quality rules can be obtained by sampling
from the posterior. However, sampling from the posterior is nontrivial due to its intractable partition
function, so we approximate the posterior using a more tractable form with the proposition below:
Proposition 1 Consider a data instance (G, q, a) with q = (h, r, ?) and a = t. For a set ofrules Z
generated by the rule generator pθ, we can compute the following score H for each rule ∈ Z:
)
H (rule)
scorew (t|rule) -
Σ
scorew (e|rule)
+ log RNNθ (rule |r),
(7)
where A is the set ofall candidate answers discovered by rules in Z^, Scorew (e|rule) is the score
that each rule contributes to entity e as defined by Eq. (4), RNNθ (rule|r) is the prior probability of
rule computed by the generator. Suppose s = maxe∈A |scorew (e)| < 1. Then for a subset of rules
ZI ⊂ Z with ∣ZI | = K, the log-probability logPθ,w(ZI |G, q, a) can be approximated asfollows:
logPθ,w (ZI|G, q, a)-
Σ
rule∈zI
H (rule) + γ(ZI) + const
2
+ O(s4)
(8)
where const is a constant term that is independentfrom ZI, Y(ZI) = log(K!/ 口也屹∈^ nrule!), with K
being the given size of set ZI and nrule being the number of times each rule appears in ZI.
We prove the proposition in App. A.1. In practice, we can apply weight decay to the weight of logic
rules in Eq. (4), and thereby reduce s = maxe∈A |scorew(e)| to get a more precise approximation.
The above proposition allows us to utilize (Prule∈z H(rule) +γ(ZI) +const) to approximate the log-
posterior logpθ,w (ZI|G, q, a), yielding a distribution q(ZI) H exp(Prule“ H(rule) + Y(ZI)) as a
5
Published as a conference paper at ICLR 2021
good approximation of the posterior. It turns out that the derived q(zI) is a multinomial distribution,
and thus sampling from q(zι) is more tractable. Specifically, a sample ZI from q(zι) can be formed
with K logic rules which are independently sampled from Z, where the probability of sampling each
rule is computed as exp(H(rule))∕(P必屹∈ exp(H(rule0))). We provide the proof in the App. A.2.
Intuitively, we could view H(rule) of each rule as an assessment of the rule quality, which considers
two factors. The first factor is based on the reasoning predictor pw , and it is computed as the score
that a rule contributes to the correct answer t minus the mean score that this rule contributes to other
candidate answers. If a rule gives higher score to the true answer and lower score to other candidate
answers, then the rule is more likely to be important. The second factor is based on the rule generator
pθ, where we compute the prior probability for each rule and use the probability for regularization.
Empirically, We find that picking K rules with highest H(rule) to form ZI works better than sampling
from the posterior. Similar observations have been made on the node classification task (Qu et al.,
2019). In fact, ZI formed by the top-K rules is an MAP estimation of the posterior, and thus the
variant of picking top-K rules yields a hard-assignment EM algorithm (Koller & Friedman, 2009).
Despite the reduced theoretical guarantees, we use this variant in practice for its good performance.
M-step. Once We obtain a set of high-quality logic rules ZI for each data instance (G, q, a) in the
E-step, we further leverage those rules to update the parameters θ of the rule generator in the M-step.
Specifically, for each data instance (G, q, a), We treat the corresponding rule set ZI as part of the
(now complete) training data, and update the rule generator by maximizing the log-likelihood of ZI:
O(G,q,a)(θ) = logPθ(ZI|q) = E log RNNθ(rule|r) + const.
rule ∈zι
(9)
With the above objective, the feedback from the reasoning predictor can be effectively distilled into
the rule generator. In this way, the rule generator will learn to only generate high-quality rules for the
reasoning predictor to explore, which reduces the search space and yields better empirical results.
For more detailed analysis of the EM algorithm for optimizing O(θ, w), please refer to App. B.
Algorithm 1 Workflow of RNNLogic
while not converge do
For each instance, use the rule generator pθ to generate a set of rules Z (|Z| = N).
For each instance, update the reasoning predictor Pw based on generated rules Z and Eq. (6).
□	E-step:
For each instance, identify K high-quality rules ZI from Z according to H(rule) in Eq. (7).
□	M-Step:
For each instance, update the rule generator pθ according to the identified rules and Eq. (9).
end while
During testing, for each query, use pθ to generate N rules and feed them into pw for prediction.
4	Experiment
4.1	Experiment Settings
Datasets. We choose four datasets for evaluation, including FB15k-237 (Toutanova & Chen, 2015),
WN18RR (Dettmers et al., 2018), Kinship and UMLS (Kok & Domingos, 2007). For Kinship and
UMLS, there are no standard data splits, so we randomly sample 30% of all the triplets for training,
20% for validation, and the rest 50% for testing. The detailed statistics are summarized in the App. D.
Compared Algorithms. We compare the following algorithms in experiment:
□ Rule learning methods. For traditional statistical relational learning methods, we choose Markov
logic networks (Richardson & Domingos, 2006), boosted relational dependency networks (Natarajan
et al., 2010) and path ranking (Lao & Cohen, 2010). We also consider neural logic programming
methods, including NeuralLP (Yang et al., 2017), DRUM (Sadeghian et al., 2019) and NLIL (Yang &
Song, 2020). In addition, we compare against CTP (Minervini et al., 2020), a differentiable method
based on neural theorem provers. Besides, we consider three reinforcement learning methods, which
6
Published as a conference paper at ICLR 2021
Table 1: Results of reasoning on FB15k-237 and WN18RR. H@k is in %. [*] means the numbers are
taken from the original papers. [“ means We rerun the methods with the same evaluation process.
Category	Algorithm	MR	MRR	FB15k-237 H@1	H@3	H@10	MR	MRR	WN18RR H@1	H@3	H@10
	TransE*	357	0.294	-	-	46.5	3384	0.226	-	-	50.1
	DistMult*	254	0.241	15.5	26.3	41.9	5110	0.43	39	44	49
No Rule Learning	ComplEx*	339	0.247	15.8	27.5	42.8	5261	0.44	41	46	51
	ComplEx-N3*	-	0.37	-	-	56	-	0.48	-	-	57
	ConvE*	244	0.325	23.7	35.6	50.1	4187	0.43	40	44	52
	TuckER*	-	0.358	26.6	39.4	54.4	-	0.470	44.3	48.2	52.6
	RotatE*	177	0.338	24.1	37.5	53.3	3340	0.476	42.8	49.2	57.1
	PathRank	-	0.087	7.4	^^92-	11.2	-	0.189	17.1	20.0	22.5
Rule Learning	NeUraILPt	-	0.237	17.3	25.9	36.1	-	0.381	36.8	38.6	40.8
	DRUMt	-	0.238	17.4	26.1	36.4	-	0.382	36.9	38.8	41.0
	NLIL*	-	0.25	-	-	32.4	-	-	-	-	-
	M-Walk*	-	0.232	16.5	24.3	-	-	0.437	41.4	44.5	-
RNNLogic	w/o emb.	538	0.288	20.8	31.5	44.5	7527	0.455	41.4	47.5	53.1
	with emb.	232	0.344	25.2	38.0	53.0	4615	0.483	44.6	49.7	55.8
Table 2: Results of knowledge graph reasoning on the FB15k-237 and WN18RR datasets with only
(h, r, ?)-queries. H@k is in %. [*] means that the numbers are taken from the original papers.
Category	Algorithm	MR	MRR	FB15k-237 H@1	H@3	H@10	MR	MRR	WN18RR H@1	H@3	H@10
Rule	MINERVA*	-	0.293	21.7	32.9	45.6	-	0.448	41.3	45.6	51.3
Learning	MultiHopKG*	-	0.407	32.7	-	56.4	-	0.472	43.7	-	54.2
RNNLogic	w/o emb.	459.0	0.377	28.9	41.2	54.9	7662.8	0.478	43.8	50.3	55.3
	with emb.	146.1	0.443	34.4	48.9	64.0	3767.0	0.506	46.3	52.3	59.2
are MINERVA (Das et al., 2018), MultiHopKG (Lin et al., 2018) and M-Walk (Shen et al., 2018).
□ Other methods. We also compare with some embedding methods, including TransE (Bordes et al.,
2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), ComplEx-N3 (Lacroix et al.,
2018), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and RotatE (Sun et al., 2019).
□ RNNLogic. For RNNLogic, we consider two model variants. The first variant assigns a constant
score to different grounding paths in the reasoning predictor, i.e., φw (path) = 1 in Eq. (4), and
we denote this variant as w/o emb.. The second variant leverages entity embeddings and relation
embeddings to compute the path score φw(path), and we denote the variant as with emb..
Evaluation Metrics. During evaluation, for each test triplet (h, r, t), we build two queries (h, r, ?)
and (t, r-1, ?) with answers t and h. For each query, we compute a probability for each entity, and
compute the rank of the correct answer. Given the ranks from all queries, we report the Mean Rank
(MR), Mean Reciprocal Rank (MRR) and Hit@k (H@k) under the filtered setting (Bordes et al.,
2013), which is used by most existing studies. Note that there can be a case where an algorithm
assigns the same probability to the correct answer and a few other entities. For such a case, many
methods compute the rank of the correct answer as (m + 1) where m is the number of entities
receiving higher probabilities than the correct answer. This setup can be problematic according to Sun
et al. (2020). For fair comparison, in that case we compute the expectation of each evaluation metric
over all the random shuffles of entities which receive the same probability as the correct answer. For
example, if there are n entities which have the same probability as the correct answer in the above
case, then we treat the rank of the correct answer as (m + (n + 1)/2) when computing Mean Rank.
Besides, we notice that in MINERVA (Das et al., 2018) and MultiHopKG (Lin et al., 2018), they only
consider queries in the form of (h, r, ?), which is different from our default setting. To make fair
comparison with these methods, we also apply RNNLogic to this setting and report the performance.
Experimental Setup of RNNLogic. For each training triplet (h, r, t), we add an inverse triplet
(t, r-1, h) into the training set, yielding an augmented set of training triplets T. We use a closed-
world assumption for model training, which assumes that any triplet outside T is incorrect. To build
a training instance from pdata, we first randomly sample a triplet (h, r, t) from T, and then form an
instance as (G = T \ {(h, r, t)}, q = (h, r, ?), a = t). Basically, we use the sampled triplet (h, r, t)
to construct the query and answer, and use the rest of triplets in T to form the background knowledge
graph G . During testing, the background knowledge graph G is formed with all the triplets in T. For
the rule generator, the maximum length of generated rules is set to 4 for FB15k-237, 5 for WN18RR,
and 3 for the rest, which are selected on validation data. See App. D for the detailed setting.
7
Published as a conference paper at ICLR 2021
Table 3: Results of reasoning on the Kinship and UMLS datasets. H@k is in %.
Category	Algorithm	MR	MRR	Kinship H@1	H@3	H@10	MR	MRR	UMLS H@1	H@3	H@10
	DistMult	~8.5-	0.354	18.9	40.0	75.5	14.6	0.391	25.6	44.5	66.9
No Rule Learning	ComplEx	7.8	0.418	24.2	49.9	81.2	13.6	0.411	27.3	46.8	70.0
	ComplEx-N3	-	0.605	43.7	71.0	92.1	-	0.791	68.9	87.3	95.7
	TuckER	6.2	0.603	46.2	69.8	86.3	5.7	0.732	62.5	81.2	90.9
	RotatE	3.7	0.651	50.4	75.5	93.2	4.0	0.744	63.6	82.2	93.9
	MLN	10.0	0.351	18.9	40.8	70.7	~7.6-	0.688	58.7	75.5	86.9
	Boosted RDN	25.2	0.469	39.5	52.0	56.7	54.8	0.227	14.7	25.6	37.6
Rule Learning	PathRank	-	0.369	27.2	41.6	67.3	-	0.197	14.8	21.4	25.2
	NeuralLP	16.9	0.302	16.7	33.9	59.6	10.3	0.483	33.2	56.3	77.5
	DRUM	11.6	0.334	18.3	37.8	67.5	8.4	0.548	35.8	69.9	85.4
	MINERVA	-	0.401	23.5	46.7	76.6	-	0.564	42.6	65.8	81.4
	CTP	-	0.335	17.7	37.6	70.3	-	0.404	28.8	43.0	67.4
RNNLogic	w/o emb.	-39-	0.639	49.5	73.1	92.4	~5.3-	0.745	63.0	83.3	92.4
	with emb.	3.1	0.722	59.8	81.4	94.9	3.1	0.842	77.2	89.1	96.5
4.2	Results
Comparison against Existing Methods. We present the results on the FB15k-237 and WN18RR
datasets in Tab. 1 and Tab. 2. The results on the Kinship and UMLS datasets are shown in Tab. 3.
We first compare RNNLogic with rule learning methods. RNNLogic achieves much better results
than statistical relational learning methods (MLN, Boosted RDN, PathRank) and neural differentiable
methods (NeuralLP, DRUM, NLIL, CTP). This is because the rule generator and reasoning predictor
of RNNLogic can collaborate with each other to reduce search space and learn better rules. RNNLogic
also outperforms reinforcement learning methods (MINERVA, MultiHopKG, M-Walk). The reason is
that RNNLogic is optimized with an EM-based framework, in which the reasoning predictor provides
more useful feedback to the rule generator, and thus addresses the challenge of sparse reward.
We then compare RNNLogic against state-of-the-art embedding-based methods. For RNNLogic with
embeddings in the reasoning predictor (with emb.), it outperforms most compared methods in most
cases, and the reason is that RNNLogic is able to use logic rules to enhance reasoning performance.
For RNNLogic without embedding (w/o emb.), it achieves comparable results to embedding-based
methods, especially on WN18RR, Kinship and UMLS where the training triplets are quite limited.
Quality of Learned Logic Rules. Next, we study the quality of rules learned by different methods
for reasoning. For each trained model, we let it generate I rules with highest qualities per query
relation, and use them to train a predictor w/o emb. as in Eq. (5) for reasoning. For RNNLogic, the
quality of each rule is measured by its prior probability from the rule generator, and we use beam
search to infer top-I rules. The results at different I are in Fig. 2, where RNNLogic achieves much
better results. Even with only 10 rules per relation, RNNLogic still achieves competitive results.
Performance w.r.t. the Number of Training Triplets. To better evaluate different methods under
cases where training triplets are very limited, in this section we reduce the amount of training data on
Kinship and UMLS to see how the performance varies. The results are presented in Fig. 4. We see
that RNNLogic w/o emb. achieves the best results. Besides, the improvement over RotatE is more
significant as we reduce training triplets, showing that RNNLogic is more robust to data sparsity.
Kinship
0.4
0.3
0.2
0.1
0.0
WNl8RR
Algorithm
--Empirical Precision
-χ- NeuraILP
--- Path Ranking
_____________________ RNNLogIc
20	40	60	80	1∞
# LθS∣C Rules
Kinship
Figure 2: Performance w.r.t. # logic rules. RNNLogic achieves
competitive results even with 10 rules per query relation.
Figure 3: Performance w.r.t. em-
bedding dimension.
8
Published as a conference paper at ICLR 2021
Performance w.r.t. Embedding Dimension. RNNLogic with emb. uses entity and relation embed-
dings to improve the reasoning predictor. Next, we study its performance with different embedding
dimensions. The results are presented in Fig. 3, where we compare against RotatE (Sun et al.,
2019). We see that RNNLogic significantly outperforms RotatE at every embedding dimension. The
improvement is mainly from the use of logic rules, showing that our learned rules are indeed helpful.
Comparison of Optimization Algorithms. RNNLogic uses an EM algorithm to optimize the rule
generator. In practice, the generator can also be optimized with REINFORCE (Williams, 1992) (see
App. F for details). We empirically compare the two algorithms in the w/o emb. case. The results on
Kinship and UMLS are presented in Tab. 4. We see EM consistently outperforms REINFORCE.
Figure 4: Performance w.r.t. # training triplets. RNNLogic is
more robust to data sparsity even without using embeddings.
	Kinship MRR	UMLS MRR
REINFORCE	0.312	0.504
EM	0.639	0.745
Table 4:	Comparison between
REINFORCE and EM.
Case Study of Generated Logic Rules. Finally, we show some logic rules generated by RNNLogic
on the FB15k-237 dataset in Tab. 5. We can see that these logic rules are meaningful and diverse.
The first rule is a subrelation rule. The third and fifth rules are two-hop compositional rules. The rest
of logic rules have even more complicated forms. This case study shows that RNNLogic can indeed
learn useful and diverse rules for reasoning. For more generated logic rules, please refer to App. E.
Table 5:	Case study of the rules generated by the rule generator.
Appears_in_TV_Show(X, Y) — Actor-of (X, Y)
Appears _in_TV_Show(X, Y) — Creator_of(x, U) ∧ Has .Producer (U, V) ∧ APPears_in_TV_ShoW(V,Y)
ORG. _in_State(X,Y) — ORG. _in_City(X, U) ∧ City」ocates_in_State(U,Y)
ORG. _in_State(x, Y) — ORG. _in_City(x, U) ∧ Address_of_PERS. (U, V) ∧ Born_in(V, W) ∧ Town_in_State(W,Y)
Person-Nationality(X, Y) — Born_in(X, U) ∧ Place_in_Country(U, Y)
Person-Nationality(X, Y) — Student_of_Educational_Institution(X, U) ∧ ORG. .Endowment.Currency(U, V)∧
Currency-Used in Region(V, W) ∧ Region in Country(W, Y)
5 Conclusion
This paper studies learning logic rules for knowledge graph reasoning, and an approach called
RNNLogic is proposed. RNNLogic treats a set of logic rules as a latent variable, and a rule generator
as well as a reasoning predictor with logic rules are jointly learned. We develop an EM-based
algorithm for optimization. Extensive expemriments prove the effectiveness of RNNLogic. In the
future, we plan to study generating more complicated logic rules rather than only compositional rules.
Besides, we plan to extend RNNLogic to other reasoning problems, such as question answering.
Acknowledgments
This project is supported by the Natural Sciences and Engineering Research Council (NSERC)
Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft
Research and Mila, Samsung Electronics Co., Ldt., Amazon Faculty Research Award, Tencent AI
Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06). This project
was also partially funded by IVADO Fundamental Research Project grant PRF-2019-3583139727.
9
Published as a conference paper at ICLR 2021
References
Ivana Balazevic, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge
graph completion. In EMNLP, 2019.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NeurIPS, 2013.
Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings.
In NAACL, 2018.
Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William Wang. Variational knowledge graph reasoning.
In NAACL, 2018.
William W Cohen, Fan Yang, and Kathryn Rivard Mazaitis. Tensorlog: Deep learning meets
probabilistic databases. Journal of Artificial Intelligence Research, 2018.
James Cussens. Stochastic logic programs: sampling, inference and applications. In UAI, 2000.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement learning. In ICLR, 2018.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In AAAI, 2018.
Jonathan Eckstein, Noam Goldberg, and Ai Kagawa. Rule-enhanced penalized regression by column
generation using rectangular maximum agreement. In ICML, 2017.
Luis Antonio Galarraga, Christina TeflioUdi, Katja Hose, and Fabian Suchanek. Amie: association
rule mining under incomplete evidence in ontological knowledge bases. In WWW, 2013.
Noam Goldberg and Jonathan Eckstein. Boosting classifiers with tightened l0-relaxation penalties.
In ICML, 2010.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 1997.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
ICLR, 2017.
Tushar Khot, Sriraam Natarajan, Kristian Kersting, and Jude Shavlik. Learning markov logic networks
via functional gradient boosting. In ICDM, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In ICLR, 2014.
Stanley Kok and Pedro Domingos. Learning the structure of markov logic networks. In ICML, 2005.
Stanley Kok and Pedro Domingos. Statistical Predicate invention. In ICML, 2007.
DaPhne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
Press, 2009.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomPosition for
knowledge base comPletion. In ICML, 2018.
Ni Lao and William W Cohen. Relational retrieval using a combination of Path-constrained random
walks. Machine learning, 2010.
Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a large scale
knowledge base. In EMNLP, 2011.
Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hoP knowledge graPh reasoning with
reward shaPing. In EMNLP, 2018.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In ICLR, 2017.
10
Published as a conference paper at ICLR 2021
Christian Meilicke, Melisachew Wudage Chekol, Daniel Ruffinelli, and Heiner Stuckenschmidt.
Anytime bottom-up rule learning for knowledge graph completion. In IJCAI, 2019.
Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim RocktascheL
Learning reasoning strategies in end-to-end differentiable proving. In ICML, 2020.
Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, and Jude Shavlik. Boosting
relational dependency networks. In ICILP, 2010.
Radford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremental, sparse,
and other variants. In Learning in graphical models. Springer, 1998.
Jennifer Neville and David Jensen. Relational dependency networks. Journal of Machine Learning
Research, 2007.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge
graphs. In AAAI, 2016.
Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In NeurIPS, 2019.
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In ICML, 2019.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 2006.
Tim Rocktaschel and Sebastian Riedel. End-to-end differentiable proving. In NeurIPS, 2017.
Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end
differentiable rule mining on knowledge graphs. In NeurIPS, 2019.
Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, and Jianfeng Gao. M-walk: Learning to
walk over graphs using monte carlo tree search. In NeurIPS, 2018.
Slavko Simic. On a global upper bound for jensen’s inequality. Journal of mathematical analysis and
applications, 2008.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by
relational rotation in complex space. In ICLR, 2019.
Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha Talukdar, and Yiming Yang. A re-evaluation
of knowledge graph completion methods. In ACL, 2020.
Komal K Teru and William L Hamilton. Inductive relation prediction on knowledge graphs. In ICML,
2020.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In Workshop on Continuous Vector Space Models and their Compositionality, 2015.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
embeddings for simple link prediction. In ICML, 2016.
William Yang Wang, Kathryn Mazaitis, and William W Cohen. Programming with personalized
pagerank: a locally groundable first-order probabilistic logic. In CIKM, 2013.
William Yang Wang, Kathryn Mazaitis, and William W Cohen. Proppr: Efficient first-order proba-
bilistic logic programming for structure discovery, parameter learning, and scalable inference. In
Workshops at AAAI, 2014a.
William Yang Wang, Kathryn Mazaitis, and William W Cohen. Structure learning via parameter
learning. In CIKM, 2014b.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In AAAI, 2014c.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 1992.
11
Published as a conference paper at ICLR 2021
Wenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method
for knowledge graph reasoning. In EMNLP, 2017.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knowledge bases. In ICLR, 2015.
Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge
base reasoning. In NeurIPS, 2017.
Yuan Yang and Le Song. Learn to explain efficiently via neural logic inductive learning. In ICLR,
2020.
Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient
probabilistic logic reasoning with graph neural networks. In ICLR, 2020.
12
Published as a conference paper at ICLR 2021
A Proofs
This section presents the proofs of some propositions used in the optimization algorithm of RNNLogic.
Recall that in the E-step of the optimization algorithm, we aim to sample from the posterior distribu-
tion over rule sets. However, directly sampling from the posterior distribution is intractable due to the
intractable partition function. Therefore, we introduce Proposition 1, which gives an approximation
distribution with more tractable form for the posterior distribution. With this approximation, sampling
becomes much easier. In Section A.1, we present the proof of Proposition 1. In Section A.2, we show
how to perform sampling based on the approximation of the posterior distribution.
A.1 Proof of Proposition 1
Next, we prove proposition 1, which is used to approximate the true posterior probability in the
E-step of optimization. We first restate the proposition as follows:
Proposition Consider a data instance (G, q, a) with q = (h, r, ?) and a = t. For a Set of rules Z
generated by the rule generator pθ, we can compute the following score H for each rule ∈ Z:
H(rule) = {scorew(t|rule) 一 ɪ- ^X Scorew(e|rule)} + logRNNθ(rule|r),
where A is the set ofall candidate answers discovered by rules in Z^, Scorew (e|rule) is the score
that each rule contributes to entity e as defined by Eq. (4), RNNθ (rule|r) is the prior probability of
rule computed by the generator. Suppose s = maxe∈A |Scorew (e)| < 1. Then for a subset of rules
ZI ⊂ Z with ∣zι | = K, the log-probability log pθ,w(zɪ |G, q, a) could be approximated as follows:
log pθ,w (ZI |G, q, a) 一	H (rule) + γ(ZI) + const ≤ s2 + O(s4)
rule∈zI
where const is a constant term that is independentfrom zI, γ(zI) = log(K!/ Y[rule∈^ nrule!), with K
being the given size of set ZI and nrule being the number of times each rule appears in ZI.
Proof: We first rewrite the posterior probability as follows:
logPθ,w(zI|G, q, a) = logPw(a|G, q, ZI) + logpθ(zI|q) + const
exp(Scorew (t))
=log ----------7--------+ logMU(ZI|K, RNNθ(∙∣r)) + const,
e∈A exp(Scorew (e))
where const is a constant term which does not depend on the choice of zI, and RNNθ(∙∣r) defines a
probability distribUtion over all the composition-based logic rUles. The probability mass fUnction of
the above multinomial distribution Mu(zI|K, RNNθ(∙∣r)) can be written as below:
K!
Mu(zI |K, q) = —------------- TT RNNθ(rule|r) rule,
ΓU ∈ nrule!	∈^
where K is the size of set zI and nrule is the number of times a rule appears in zI .
Letting γ(zI) = log(K!/ Q皿比∈z nrule!), then the posterior probability can then be rewritten as:
logPθ,w(zI|G, q, a)
= log
= log
exp(Scorew (t))
Ee∈A exp(scorew(e))
exp(Scorew (t))
Pe∈A exp(scorew(e))
K!
+ log Y=F--------------- + log TT RNNθ (rule∣r)nrule + const
rule∈z^ ∈ n rule!	之 ∈^
+ γ(zI) +	log RNNθ (rule|r) + const
scorew(t) - log	exp(score
e∈A
rule∈zI
w(e)) + γ(ZI) +	log RNNθ (rule|r) + const.
rule∈zI
The above term log e∈A exp(scorew(e)) makes the posterior distribution hard to deal with, and
thus we approximate it using Lemma 1, which we prove at the end of this section.
13
Published as a conference paper at ICLR 2021
Lemma 1 Let e ∈ A be a finite set of entities, let |scorew (e)| ≤ s < 1, and let scorew be a
function from entities to real numbers. Then the following inequalities hold:
0 ≤ log
exp(scorew
scorew(e) + log(|A|)
≤ s2 + O(s4).
Hence, using the lemma we can get the following upper bound of the posterior probability:
log pθ,w(zI |G, q, a)
= Scorew(t) — log ɪ2 exp(scorew(e)) + γ(zι) + ɪ2 logRNNθ(rule|r) + const
e∈A	rule∈zI
scorew (e) + γ(zI) +	log RNNθ (rule|r) + const
rule∈zI
H (rule) + γ(zI) + const,
rule∈zI
≤scor
and also the following lower bound of the posterior probability:
log pθ,w (ZI |G, q, a)
=Scorew(t) — log ɪ2 exp(scorew(e)) + γ(zI) + ɪ2 logRNNθ(rule|r) + const
e∈A	rule∈zI
Scorew (e) + γ(zI) + ɪ2 log RNNθ(rule|r) + const — s2 —
rule∈zI
=	H (rule) + γ(zI) + const — s2 — O(s4),
rule∈zI
where const is a constant term which does not depend on zI .
By combining the lower and the upper bound, we get:
log pθ,w (zI |G, q, a) —	X H (rule) + γ(zI) + const ≤ s2 + O(s4)
rule∈zI
≥Sco
Thus, it only remains to prove Lemma 1 to complete the proof. We use Theorem 1 from (Simic,
2008) as a starting point:
Theorem 1 Suppose that X = {xi}n=ι represents a finite sequence ofreal numbers belonging to a
fixed closed interval I = [a, b], a < b. If f is a convex function on I, then we have that:
1XX f(Xi)-f(1 XX Xi
nn
i=1	i=1
≤ f(a)+f(b) - 2f (2).
As (— log) is convex and exp(Scorew(e)) ∈ [exp(—s), exp(s)], Theorem 1 gives us that:
- ]AA[ X log (exp(scorew(e))) + log (击 X exp(scorew(e))
e∈A	e∈A
≤ — log(exp(—s)) — log(exp(s)) + 2 log
exp(—s) + exp(s)
2
14
Published as a conference paper at ICLR 2021
After some simplification, we get:
log
eX∈A
exp(scorew(e))
≤ X |A|Scorew(e) + Iog(IAl) + 2log (exp( S)2+exp(S))
=X TATScorew(e) + lοg(∣A∣) + 2s - 2lοg2 + 2lοg(1 + exp(-2s))
e∈A |A|
≤ X tatscorew(e) + lοg(∣A∣) + s2 + O(s4),
e∈A ∣A∣
(10)
where the last inequality is based on Taylor's series log(1 + ex) = log2+ 2x + 8x2 + O(x4) with
∣x∣ < 1. On the other hand, according to the well-known Jensen’s inequality, we have:
log (∣A∣ X exp(scorew(e)) ) ≥ ∣-A∣ X log(eXP(Scorew(e))),
e∈A	e∈A
which implies:
log (X exp(scorew(e))) ≥ X ∣Aiscorew(e) + log(∣A∣).	(11)
By combining Eq. (10) and Eq. (11), we obtain:
Scorew(e) + log(∣A∣)	≤ S2 + O(S4).
This completes the proof.
.
A.2 Sampling Based on the Approximation of the True Posterior
Based on Proposition 1, the log-posterior probability log pθ,w(ZI ∣G, q, a) could be approximated by
(Prule∈z H(rule) +γ(ZI) +const), with const being a term that does not depend on ZI. This implies
that we could construct a distribution q(zI) a exp(P皿卜∈zl H(rule) + Y(ZI)) to approximate the
true posterior, and draw samples from q as approximation to the real samples from the posterior.
It turns out that the distribution q(ZI) is a multinomial distribution. To see that, we rewrite q(ZI) as:
q(zι) = ZIeXP ( X H(rule) + Y(zi))
rule∈zI
=ZIeXP(Y(ZI)) Y exp (H(rule))
rule∈zI
1 K!
=Z Q--------行 Y exP(H(rule))
Znrule∈z nrule * rule∈z
1 K! n
=ZoQ—nτ∣ ∏ qr(rule)
Z Ilrule ∈z nrule ∣ rule ∈z
=V0 MU(ZI ∣K,qr),
Z
where nrule is the number of times a rule appears in the set ZI, qr is a distribution over all the generated
logic rules Z with q『(rule) = exp(H(rule))/ P必屹o∈^ exp(H(rulez)), Z and Z0 are normalization
terms. By summing over ZI on both sides of the above equation, we obtain Z 0 = 1, and hence:
q(ZI) = Mu(ZI ∣K, qr).
15
Published as a conference paper at ICLR 2021
To sample from such a multinomial distribution, we could simply sample K rules independently from
the distribution qr, and form a sample ZI with these K rules.
In practice, we observe that the hard-assignment EM algorithm (Koller & Friedman, 2009) works
better than the standard EM algorithm despite the reduced theoretical guarantees. In the hard-
assignment EM algorithm, We need to draw a sample ZI with the maximum posterior probability.
Based on the above approximation q(zI) of the true posterior distribution pθ,w(zI |G, q, a), we could
simply construct such a sample ZI with K rules which have the maximum probability under the
distribution q『.By definition, we have q『(rule) α exp(H(rule)), and hence drawing K rules with
maximum probability under qr is equivalent to choosing K rules with the maximum H values.
B More Analysis of the EM Algorithm
In RNNLogic, we use an EM algorithm to optimize the rule generator. In this section, we show why
this EM algorithm is able to maximize the objective function of the rule generator.
Recall that for a fixed reasoning predictor pw , we aim to update pθ to maximize the log-likelihood
function logpw,θ(a|G, q) for each data instance (G, q, a). Directly optimizing logpw,θ(a|G, q) is
difficult due to the latent logic rules, and therefore we consider the following evidence lower bound
of the log-likelihood function:
logPw,θ(a|G, q) ≥ Eq(ZI)[logPw(a|G, q, ZI) + logPθ(ZI|q) - logq(zI)] = LELBO(q,Pθ), (12)
where q(ZI) is a variational distribution, and the equation holds when q(ZI) = pθ,w (ZI |G, q, a).
With this lower bound, we can optimize the log-likelihood function logpw,θ(a|G, q) with an E-step
and an M-step. In the E-step, we optimize q(ZI) to maximize LELBO(q,Pθ), which is equivalent to
minimizing KL(q(ZI)∣∣Pθ,w(ZI|G, q, a)), By doing so, we are able to tighten the lower bound. Then
in the M-step, we further optimize θ to maximize LELBO (q, Pθ). Next, we introduce the details.
E-step. In the E-step, our goal is to update q to minimize KL(q(ZI)∣∣Pθ,w(ZI|G, q, a)). However,
there are a huge number of possible logic rules, and hence optimizing q(ZI) on every possible rule set
ZI is intractable. To solve the problem, recall that we generate a set of logic rules Z when optimizing
the reasoning predictor, and here we add a constraint to q based on Z. Specifically, we constrain the
sample space of q(ZI) to be all subsets of Z with size being K, i.e., ZI ⊂ Z and ∣ZI | = K. In other
words, we require Pzl⊂z ∣zj∣=κ q(ZI) = 1. With such a constraint, we can further use proposition 1
to construct the variational distribution q to approximate Pθ,w (ZI |G, q, a), as what is described in the
model section.
M-step. In the M-step, our goal is to update Pθ to maximize the lower bound LELBO(q, Pθ). To
do that, we notice that there is an expectation operation with respect to q(ZI) in LELBO(q,Pθ). By
drawing a sample from q(ZI), LELBO(q,Pθ) can be estimated as follows:
LELBO(q,Pθ) = Eq(zI)[logPw(a∣G, q, ZI)+logpθ(ZI|q) - log q(ZI)]
'logPw(a|G, q, ZI) + logPθ(ZI|q) - log q(ZI),
(13)
where ZI 〜q(ZI) is a sample drawn from the variational distribution. By ignoring the terms which
are irrelevant toPθ, we obtain the following objective function for θ:
log Pθ (ZI |q),
(14)
which is the same as the objective function described in the model section.
As a result, by performing the E-step and the M-step described in the model section, we are able to
update Pθ to increase the lower bound LELBO(q,Pθ), and thereby push up the log-likelihood function
logpw,θ(a|G, q). Therefore, we see that the EM algorithm can indeed maximize logpw,θ(a∖G, q)
with respect to Pθ .
C Details ab out Parameterization and Implementation
Section 3.2 of the paper introduces the high-level idea of the reasoning predictor with logic rules
and the rule generator. Due to the limited space, some details of the models are not covered. In this
section, we explain the details of the reasoning predictor and the rule generator.
16
Published as a conference paper at ICLR 2021
C.1 Reasoning Predictor with Logic Rules
We start with the reasoning predictor with logic rules. Recall that for each query, our reasoning
predictor leverages a set of logic rules z to give each candidate answer a score, which is further used
to predict the correct answer from all candidates.
Specifically, let A denote the set of all the candidate answers discovered by logic rules in set z. For
each candidate answer e ∈ A, we define the following function scorew to compute a score:
scorew (e) =	scorew (e|rule) = Σ Σ	ψw (rule) ∙ φw (path),
rule∈z	rule∈z path∈P (h,rule,e)
(15)
where P(h, rule, e) is the set of grounding paths which start at h and end at e following a rule (e.g.,
Alice -f-r-ie-n→d Bob -h-o-b-b→y Sing). ψw(rule) and φw(path) are scalar weights of each rule and path.
For the scalar weight ψw(rule) of a rule, we initialize ψw(rule) as follows:
ψw(rule) = E(G,q,a)〜Pdata |P(h, rule, t)| - ∣A? X |P(h, rule, e)| ,
e∈A
(16)
where ∣P(h, rule, t)∣ is the number of relational paths starting from head entity h, following the
relations in rule and ending at tail entity t. The form is very similar to the definition of H values for
logic rules, and the value can effectively measure the contribution of a rule to the correct answers.
We also try randomly initializing rule weights or initializing them as 0, which yield similar results.
For the scalar score φw(path) of a path, we either fix it to 1, or compute it by introducing entity and
relation embeddings. In the second case, we introduce an embedding for each entity and relation in
the complex space. Formally, the embedding of an entity e is denoted as xe , and the embedding of a
relation r is denoted as x「For a grounding path path = eo -→ eι -→ e2 •…eι-ι —→ eι,we follow
the idea in RotatE (Sun et al., 2019) and compute φw(path) in the following way:
Φw (path) = σ(δ — d(Xeo ◦ x∏ ◦ x^ ◦•••◦ Xrl , Xeι)),	(17)
where σ(x)=耳I-X is the sigmoid function, d is a distance function between two complex vectors,
δ is a hyperparameter, and ◦ is the Hadmard product in complex spaces, which could be viewed as a
rotation operator. Intuitively, for the embedding xe0 of entity e0, we rotate xe0 by using the rotation
operators defined by {rk}k=ι, yielding (x∣0 ◦ x∏ ◦ Xr2 ◦…。x∏). Then we compute the distance
between the new embedding and the embedding xel of entity el, and further convert the distance to a
value between 0 and 1 by using the sigmoid function and a hyperparameter δ.
C.2 Rule Generator
This paper focuses on compositional rules, which have the abbreviation form r J ri ∧ …∧ r?
and thus could be viewed a sequence of relations [r, ri, r2 …r1, “nd], where r is the query
relation or the head of the rule, {ri }li=1 are the body of the rule, and rEND is a special relation
indicating the end of the relation sequence. We introduce a rule generator RNNθ parameterized with
an LSTM (Hochreiter & Schmidhuber, 1997) to model such sequences. Given the current relation
sequence [r, ri, r2 •…r/, RNNθ aims to generate the next relation r%+ι and meanwhile output the
probability of ri+i. The detailed computational process towards the goal is summarized as follows:
•	Initialize the hidden state of the RNNθ as follows:
h0 = f(vr),
where vr is a parameter vector associated with the query relation or the head relation r, f is a
linear transformation.
•	Sequentially compute the hidden state at different positions by using the LSTM gate:
ht = LSTM(ht-i,g([vr,vrt]),
where vrt is a parameter vector associated with the relation rt, [vr, vrt] is the concatenation of vr
and vrt , g is a linear transformation.
17
Published as a conference paper at ICLR 2021
•	Generate rt+1 and its probability based on ht+1 and the following vector:
softmax(o(ht+1)).
Suppose the set of relations is denoted as R. We first transform ht+1 to a |R|-dimensional vector
by using a linear transformation o, and then apply softmax function to the |R|-dimensional vector
to get the probability of each relation. Finally, we generate rt+1 according to the probability vector.
D Experimental Setup
D. 1 Dataset Statistics
The statistics of datasets are summarised in Table 6.
Table 6: Statistics of datasets.
Dataset	#Entities	#Relations	#Train	#Validation	#Test
FB15K-237	14,541	237	272,115	17,535	20,466
WN18RR	40,943	11	86,835	3,034	3,134
Kinship	104	25	3,206	2,137	5,343
UMLS	135	46	1,959	1,306	3,264
D.2 Experimental Setup of RNNLogic
Next, we explain the detailed experimental setup of RNNLogic. We try different configurations of
hyperparameters on the validation set, and the optimal configuration is then used for testing. We
report the optimal hyperparameter configuration as below.
Data Preprocessing. For each training triplet (h, r, t), we add an inverse triplet (t, r-1 , h) into
the training set, yielding an augmented set of training triplets T. To build a training instance
from pdata, we first randomly sample a triplet (h, r, t) from T, and then form an instance as (G =
T \ {(h, r, t)}, q = (h, r, ?), a = t). Basically, we use the sampled triplet (h, r, t) to construct
the query and answer, and use the rest of triplets in T to form the background knowledge graph G.
During testing, the background knowledge graph G is constructed by using all the triplets in T.
Reasoning Predictor. For the reasoning predictor in with embedding cases, the embedding dimension
is set to 500 for FB15k-237, 200 for WN18RR, 2000 for Kinship and 1000 for UMLS. We pre-train
these embeddings with RotatE (Sun et al., 2019). The hyperparameter δ for computing φw(path)
in Equation (17) is set to 9 for FB15k-237, 6 for WN18RR, 0.25 for Kinship and 3 for UMLS. We
use the Adam (Kingma & Ba, 2014) optimizer with an initial learning rate being 5 × 10-5, and we
decrease the learning rate in a cosine shape.
Rule Generator. For the rule generator, the maximum length of generated rules is set to 4 for
FB15k-237, 5 for WN18RR, and 3 for Kinship and UMLS. These numbers are chosen according to
model performace on the validation data. The size of input and hidden states in RNNθ are set to 512
and 256. The learning rate is set to 1 × 10-3 and monotonically decreased in a cosine shape. Beam
search is used to generate rules with high probabilities, so that we focus on exploiting these logic
rules which the rule generator is confident about. Besides, we pre-train the rule generator by using
sampled relational paths on the background knowledge graph formed with training triplets, which
prevents the rule generator from exploring meaningless logic rules in the beginning of training.
EM Optimization. During optimization, we sample 1000 rules from the rule generator for each data
instance. In the E-step, for each data instance, we identify 300 rules as high-quality logic rules.
Evaluation. In testing, for each query q = (h, r, ?), we use the rule generator pθ to generate 1000
logic rules, and let the reasoning predictor pw use the generated logic rules to predict the answer a.
18
Published as a conference paper at ICLR 2021
E Case S tudy
We present more rules learned by RNNLogic on FB15k-237 dataset and UMLS dataset in Table 7
at the last page. In this table, h → t means triplet (h, r, t) and h 4- t means triplet (h, r-1, t), or
equivalently (t, r, h).
F Connection with REINFORCE
In terms of optimizing the rule generator, our EM algorithm has some connections with the RE-
INFORCE algorithm (Williams, 1992). Formally, for a training instance (G, q, a), REINFORCE
computes the derivative with respect to the parameters θ of the rule generator as follows:
Epθ(z∣q)[R ∙ Vθ logPθ(z|q)] ' R ∙ Velogpθ(Z|q),	(18)
where Z is a sample from the rule generator, i.e., Z 〜pe(z∣q). R is a reward from the reasoning
predictor based on the prediction result on the instance. For example, we could treat the probability
of the correct answer computed by the reasoning predictor as reward, i.e., R = Pw (a∖G, q, z).
In contrast, our EM optimization algorithm optimizes the rule generator with the objective function
defined in Equation (2), yielding the derivative V logpe(ZI ∖q). Comparing the derivative to that in
REINFORCE (Eq. (18)), we see that EM only maximizes the log-probability for rules selected in the
E-step, while REINFORCE maximizes the log-probability for all generated rules weighted by the
scalar reward R. Hence the two approaches coincide if R is set to 1 for the selected rules from the
approximate posterior and 0 otherwise. In general, finding an effective reward function to provide
feedback is nontrivial, and we empirically compare these two optimization algorithms in experiments.
19
Published as a conference paper at ICLR 2021
Table 7: Logic rules learned by RNNLogic.
Relation		J	Rule (Explanation)	
V Appears_in_TV_Show X 	→	Y	J	“ HaS-ActOr ɪ X J	Y (Definition. An actor of a show appears in the show, obviously.)	
		J	TT Creator_of TT PrOducer-Of TT Appears_in_TV_Show λλ X	> U J	V --	→ Y	
			(The creator X and the producer V of another show U are likely to	
			appear in the same show Y.)	
		J	“ Actor_of τ τ AWard-NOminated τ r Winner-Of p 广 X J	U J	V J	Y	
		J	“ Writer-Of τ τ Creater_of τ r Actor_of P 广 X	> U J	V	› Y	
		J	TT Student-Of TT Student-Of TT Appears_in_TV_Show 工7 X	> U J	V -—	→ Y	
			(Two students X and V in the same school U are likely to appear in the	
			same show Y.)	
V ORG.,in_State X	>	Y	J	TT ORG._in_City TT City_in_State λλ X	→ U 			→ Y (Use the city to indicate the state directly.)	
		J	TT ORG._in_City TT Lives_in TT Born_in ttλ Town_in-State X	→ U J	V	› W	→ (Use the person living in the city to induct the state.)	Y
		J	V Sub-ORG. _of TT ORG._in_State λλ X J	U	› Y	
		J	TT Sub-ORG. ,of TT Sub-ORG. ,of TT ORG.,in_State λλ X	> U J	V	› Y	
		J	ORG._in_City TT ORG. _in_City F r ORG._in_State λλ X	→ U J	- V	› Y (Two organizations in the same city are in the same state.)	
“ Person-Nationality X	→	Y	J	TT Born_in TT Place_in_Country 工7 Y	∖	/	~l ∖ V X 	→ U 	→ Y (Definition.)	
		J	“ Spouse rr Person-Nationality P广 Y	ɪ	∖	/	-i ∖ V X 	→ U 	→ Y	
		J	(By a fact that people are likely to marry a person of same nationality.) V Student_of TT ORG. _Endowment_Currency “ RegiOn-Currency Λ 	∖ I / 	1 1/ 4		
			Wr RegiOn_in_Country〉Y (Use the currency to induct the nationality.)	
				
		J	“ Born_in ττ Born_in τ r Person-Nationality 'r X	> U J	V	› Y	
		J	“ Politician_of TT Politician_of TT Person-Nationality X	> U J	V	→	Y
“ Manifestation_of X	>	Y	J	Treats	Prevents	Precedes X J	U	> V J	Y	
		J	X JCOmPIiCateS U JPrecedeS Y	
		J	“ Locationqf τ∙7∙ Is_a 、丁 Precedes P广 X	› U	→ V J	Y	
		J	TT Complicates TT Precedes TT OccurS_in 工7 X J_-C	U	> V J	Y	
		J	TT Location_of TT OccurS_in TT OccurS_in 工7 X	> U J	V J	Y	
		J	“ Precedes τ τ Occurs_in τ r Degree-Of P 广 X	> U J	V J	Y	
Affects X V		Y	J	“ Result-Of rr OccurS_in 、丁 Precedes P广 X	> U	→ V	→ Y	
		J	TT Precedes TT Produces TT Occurs_in 工7 X J	U	> V J	Y	
		J	“ Prevents τ D Disrupts τ r Co-occurs _with P 广 X J	U	› V	→ Y	
		J	TT Result-Of TT Complicates TT Precedes 工7 X J	U 			> V	→ Y	
		J	“ ASSeSSeS-Effect-Of ττ MethOd-Of 、丁 Complicates 'r X J	U	› V	→ Y	
		J	TT PrOceSS-Of TT InteractS-With TT Causes 工7 X	> U	→ V	→ Y	
		J	TT ASSeSSeS-Effect-Of TT ReSuIt-Of TT Precedes 工7 X J	U J	V	› Y	
20