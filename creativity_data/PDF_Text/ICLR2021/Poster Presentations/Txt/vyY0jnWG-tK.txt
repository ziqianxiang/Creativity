Published as a conference paper at ICLR 2021
Physics-aware, probabilistic model order re-
DUCTION WITH GUARANTEED STABILITY
Sebastian Kaltenbach, Phaedon-Stelios Koutsourelakis
Professorship of Continuum Mechanics
Technical University of Munich
{sebastian.kaltenbach,p.s.koutsourelakis}@tum.de
Ab stract
Given (small amounts of) time-series’ data from a high-dimensional, fine-grained,
multiscale dynamical system, we propose a generative framework for learning an
effective, lower-dimensional, coarse-grained dynamical model that is predictive
of the fine-grained system’s long-term evolution but also of its behavior under
different initial conditions. We target fine-grained models as they arise in physi-
cal applications (e.g. molecular dynamics, agent-based models), the dynamics of
which are strongly non-stationary but their transition to equilibrium is governed
by unknown slow processes which are largely inaccessible by brute-force simu-
lations. Approaches based on domain knowledge heavily rely on physical insight
in identifying temporally slow features and fail to enforce the long-term stability
of the learned dynamics. On the other hand, purely statistical frameworks lack
interpretability and rely on large amounts of expensive simulation data (long and
multiple trajectories) as they cannot infuse domain knowledge. The generative
framework proposed achieves the aforementioned desiderata by employing a flex-
ible prior on the complex plane for the latent, slow processes, and an intermediate
layer of physics-motivated latent variables that reduces reliance on data and im-
bues inductive bias. In contrast to existing schemes, it does not require the a priori
definition of projection operators or encoders and addresses simultaneously the
tasks of dimensionality reduction and model estimation. We demonstrate its ef-
ficacy and accuracy in multiscale physical systems of particle dynamics where
probabilistic, long-term predictions of phenomena not contained in the training
data are produced.
1 Introduction
High-dimensional, nonlinear systems are ubiquitous in engineering and computational physics.
Their nature is in general multi-scale1. E.g. in materials, defects and cracks occur on scales of
millimeters to centimeters whereas the atomic processes responsible for such defects take place
at much finer scales (Belytschko & Song, 2010). Local oscillations due to bonded interactions of
atoms (Smit, 1996) take place at time scales of femtoseconds (10-15s), whereas protein folding pro-
cesses which can be relevant for e.g. drug discovery happen at time scales larger than milliseconds
(10-3s). In Fluid Mechanics, turbulence phenomena are characterized by fine-scale spatiotemporal
fluctuations which affect the coarse-scale response (Laizet & Vassilicos, 2009). In all of these cases,
macroscopic observables are the result of microscopic phenomena and a better understanding of the
interactions between the different scales would be highly beneficial for predicting the system’s evo-
lution (Givon et al., 2004). The identification of the different scales, their dynamics and connections
however is a non-trivial task and is challenging from the perspective of statistical as well as physical
modeling.
1With the term multiscale we refer to systems whose behavior arises from the synergy of two or more
processes occurring at different (spatio)temporal scales. Very often these processes involve different physical
descriptions and models (i.e. they are also multi-physics). We refer to the description/model at the finer scale
as fine-grained and to the description/model at the coarser scale as coarse-grained.
1
Published as a conference paper at ICLR 2021
Figure 1: Visual summary of proposed framework. The low-dimensional variables z act via a
probabilistic map G as generators of an intermediate layer of latent, physically-motivated variables
X that are able to reconstruct the high-dimensional system x with another probabilistic map F .
In this paper we propose a novel physics-aware, probabilistic model order reduction framework with
guaranteed stability that combines recent advances in statistical learning with a hierarchical archi-
tecture that promotes the discovery of interpretable, low-dimensional representations. We employ
a generative state-space model with two layers of latent variables. The first describes the latent
dynamics using a novel prior on the complex plane that guarantees stability and yields a clear dis-
tinction between fast and slow processes, the latter being responsible for the system’s long-term
evolution. The second layer involves physically-motivated latent variables which infuse inductive
bias, enable connections with the very high-dimensional observables and reduce the data require-
ments for training. The probabilistic formulation adopted enables the quantification of a crucial,
and often neglected, component in any model compression process, i.e. the predictive uncertainty
due to information loss. We finally want to emphasize that the problems of interest are Small Data
ones due to the computational expense of the physical simulators. Hence the number of time-steps
as well as the number of time-series used for training is small as compared to the dimension of the
system and to the time-horizon over which predictions are sought.
2	Physics-Aware, Probabilistic Model Order Reduction
Our data consists of N times-series {x(0i:)T }iN=1 over T time-steps generated by a computational
physics simulator. This can represent positions and velocities of each particle in a fluid or those of
atoms in molecular dynamics. Their dimension is generally very high i.e. xt ∈ M ⊂ Rf (f >> 1).
In the context of state-space models, the goal is to find a lower-dimensional set of collective variables
or latent generators zt and their associated dynamics. Given the difficulties associated with these
tasks and the solutions that have been proposed in statistics and computational physics literature,
we advocate the use of an intermediate layer of physically-motivated, lower-dimensional variables
Xt (e.g. density or velocity fields), the meaning of which will become precise in the next sections.
These variables provide a coarse-grained description of the high-dimensional observables and imbue
interpretability in the learned dynamics. Using Xt alone (without zt ) would make it extremely
difficult to enforce long-term stability (see Appendix H.2) while ensuring sufficient complexity in
the learned dynamics (Felsberger & Koutsourelakis, 2019; Champion et al., 2019). Furthermore and
even if the dynamics of xt are first-order Markovian, this is not necessarily the case for Xt (Chorin
& Stinis, 2007). The latent variables zt therefore effectively correspond to a nonlinear coordinate
transformation that yields not only Markovian but also stable dynamics (Gin et al., 2019). The
general framework is summarized in Figure 1 and we provide details in the next section.
2
Published as a conference paper at ICLR 2021
2.1	Model Structure
Our model consists of three levels. At the first level, we have the latent variables zt which are
connected with Xt in the second layer through a probabilistic map G. The physical variables Xt
are finally connected to the high-dimensional observables through another probabilistic map F . We
parametrize F, G with deep neural networks and denote by θ1 and θ2 the corresponding parameters
(see Appendix D). In particular, we postulate the following relations:
zt,j =	zt-1,j	exp(λj)	+ σj J,j	λj	∈	C,	J,j 〜CN(0, I), j = 1,	2, . . . , h	(I)
Xt =G(zt,θ1)	(2)
xt = F (Xt, θ2)	(3)
We assume that the latent variables zt are complex-valued and a priori independent. Complex vari-
ables were chosen as their evolution includes a harmonic components which are observed in many
physical systems. In Appendix H.1 we present results with a real-valued latent variables zt,j and
illustrate their limitations. We model their dynamics with a discretized Ornstein-Uhlenbeck process
on the complex plane with initial conditions zoj 〜CN(0, σ0j)2. The parameters associated with
this level are denoted summarily by θ0 = {σ02,j, σj2, λj}jh=1. These, along with θ1, θ2 mentioned
earlier, and the state variables Xt and zt have to be inferred from the data xt . We explain each of
the aforementioned components in the sequel.
2.1.1 Stable low-dimensional Dynamics
While the physical systems (e.g. particle dynamics) of interest are highly non-stationary, they gen-
erally converge to equilibrium in the long-term. We enforce long-term stability here by ensuring that
the real-part of the λj∙ 's in Equation (1) is negative, i.e.:
λj = <(λj) +i =(λj) with<(λj) < 0	(4)
which guarantees first and second-order stability i.e. the mean as well as the variance are bounded
at all time steps.
The transition density each process zt,j is given by:
<(zt,j )	<(zt-1,j )	σj2
P (ZtjI ZtT,j) = NU=(Ztj)_| | Sj Rj[=(zt-1j)] , I22 J	⑸
where the orthogonal matrix Rj depends on the imaginary part of λj :
R	cos(=(λj)) - sin(=(λj))	6
Rj = sin(=(λj))	cos(=(λj))	(6)
and the decay rate sj depends on the real part of λj:
sj = exp(<(λj))	(7)
i.e. the closer to zero the latter is, the ”slower” the evolution of the corresponding process is. As in
probabilistic Slow Feature Analysis (SFA) (Turner & Sahani, 2007; Zafeiriou et al., 2015), we set
σj2 = 1 - exp(2 <(λj)) = 1 - sj2 and σ02,j = 1. As a consequence, a priori, the latent dynamics are
stationary3 and an ordering of the processes Zt,j is possible on the basis of <(λj ). Hence the only
independent parameters are the λj , the imaginary part of which can account for periodic effects in
the latent dynamics (see Appendix B).
The joint density of zt can finally be expressed as:
p(z0:T ) = Y Yp(Zt,j | Zt-Ij, θo)p(ZOj lθ0) I	⑻
j=1 t=1
The transition density between states at non-neighbouring time-instants is also available analytically
and is useful for training on longer trajectories or in cases of missing data. Details can be found in
Appendix B.
2A short review of complex normal distributions, denoted by CN, can be found in Appendix A.
3More details can be found in Appendix B.
3
Published as a conference paper at ICLR 2021
2.1.2 Probabilistic Generative Mapping
We employ fully probabilistic maps between the different layers which involve two conditional
densities based on Equations (2) and (3), i.e.:
p(xt | Xt, θ2)	and	p(Xt | zt, θ1)	(9)
In contrast to the majority of physics-motivated papers (Chorin & Stinis, 2007; Champion et al.,
2019) as well as those based on transfer-operators Klus et al. (2018), we note that the generative
structure adopted does not require the prescription of a restriction operator (or encoder) and the
reduced variables need not be selected a priori but rather are adapted to best reconstruct the observ-
ables.
The splitting of the generative mapping into two parts through the introduction of the intermediate
variables Xt has several advantages. Firstly, known physical dependencies between the data x and
the physical variables X can be taken into account, which reduces the complexity of the associated
maps and the total number of parameters. For instance, in the case of particle simulations where
X represents a density or velocity field, i.e. it provides a coarsened or averaged description of the
fine-scale observables, it can be used to (probabilistically) reconstruct the positions or velocities
of the particles. This physical information can be used to compensate for the lack of data when
only few training sequences are available (Small data) and can seen as a strong prior to the model
order reduction framework. Due to the lower dimension of associated variables, the generative map
between zt and Xt can be more easily learned even with few training samples. Lastly, the inferred
physical variables X can provide insight and interpretability to the analysis of the physical system.
2.2	Inference and Learning
Given the probabilistic relations above, our goal is to infer the state variables X0(1:T:n) , z0(1:T:n) as well
as all model parameters θ. We follow a hybrid Bayesian approach in which the posterior of the state
variables is approximated using structured Stochastic Variational Inference (Hoffman et al., 2013)
and MAP point estimates for θ = {θ0, θ1, θ2} are computed.
The application of Bayes’ rule leads to the following posterior:
p(X01铲,Z(W, θ∣x01τn))
P(χ01*χ017n), z(1?, θ) P(X0T, zl⅞n), θ)
p(x01τn) X(1Tn), θ) p(X017n) | Z01?, θ) p(zl⅞n) | θ) p(θ)
P(X01Tn))
(10)
(11)
where p(θ) denotes the prior on the model parameters. In the context of variational inference, we
use the following factorization of the approximate posterior4:
n / h	\
qΦ(X01τn), zo1τn)) = Y I Y qΦ(z()T,j | XOiT) I qΦ(XOiT)	(12)
i=1 j=0
We approximate the conditional posterior of z given X with a complex multivariate normal which
is parameterized using a tridiagonal precision matrix as proposed in Archer et al. (2015); Bamler
& Mandt (2017). This retains dependencies between temporally neighbouring z, but the number of
parameters grows linearly with the dimension of z which leads to a highly scalable algorithm. For
the variational posterior of X we employ a Gaussian with a diagonal covariance, i.e.:
qφ(z°-T,j I x(iT) = CN(μφ(xθiτ), [BφXiT)Bφ(x0iT)τi-1)	qφ(xOiT) = N(μφi,∑φ)
(13)
We denote summarily with φ the parameters involved and note that deep neural networks are used
for the mean μφ(x0iT) as well as the upper bidiagonal matrix Bφ(x0：T). Details on the neural net
architectures employed are provided in Section 4 and in Appendix D.
4We note that this factorization does not introduce any error due to the conditional independence of x, z
given X .
4
Published as a conference paper at ICLR 2021
It can be readily shown that the optimal parameter values are found by maximizing the Evidence
Lower Bound (ELBO)F(qφ(X01Tn), z(1Tn)), θ) which is derived in Appendix C. We compute
Monte Carlo estimates of the gradient of the ELBO with respect to φ and θ with the help of the
reparametrization trick (Kingma & Welling, 2013) and carry out stochastic optimization with the
ADAM algorithm (Kingma & Ba, 2014).
2.3	Predictions
Once state variables have been inferred and MAP estimates θMAP for the model parameters have
been obtained, the reduced model can be used for probabilistic future predictions. In order to do so
for a time sequence used in training, we employ the following Monte Carlo scheme to generate a
sample xT +P, i.e. P time-steps into the future:
1.	Sample XT and ZT from the inferred posterior qφ(zo:T | Xo：T)qφ(Xθ:T).
2.	Propagate zT for P time steps forward by using the conditional density in Equation (5).
3.	Sample XT+P and xT+P from p(XT +P | zT+P, θ1MAP) and p(xT +P | XT+P, θ2MAP)
respectively.
More importantly perhaps, the trained model can be used for predictions under new initial con-
ditions, e.g. x°. To achieve this, first the posterior p(zo∣xo) H ʃp(xo∣Xo, θMAP) p(Xo |
z0, θ1MAP) dX0 must be found before the Monte Carlo steps above can be employed starting at
T = 0.
3 Related Work
The main theme of our work is the learning of low-dimensional dynamical representations that are
stable, interpretable and make use of physical knowledge.
Linear latent dynamics: In this context, the line of work that most closely resembles ours pertains
to the use of Koopman-operator theory (Koopman, 1931) which attempts to identify appropriate
transformations of the original coordinates that yield linear dynamics (Klus et al., 2018). We note
that these approaches (Lusch et al., 2018; Champion et al., 2019; Gin et al., 2019; Lee & Carlberg,
2020) require additionally the specification of an encoder i.e. a map from the original description
to the reduced coordinates which we avoid in the generative formulation adopted. Furthermore only
a small fraction are probabilistic and can quantify predictive uncertainties but very often employ
restrictive parametrizations for the Koopman matrix in order to ensure long-term stability (Pan &
Duraisamy, 2020). To the best of our knowledge, none of the works along these lines employ
additional, physically-motivated variables and as a result have demonstrated their applicability only
in lower-dimensional problems and require very large amounts of training data or some ad hoc pre-
processing. We provide comparative results with Koopman-based deterministic and probabilistic
models in Appendix H.3.
Data-driven discovery of nonlinear dynamics: The data-driven discovery of governing dynamics
has received tremendous attention in recent years. Efforts based on the Mori-Zwanzig formalism
can accurately identify dynamics of pre-defined variables, which also account for memory effects,
but cannot reconstruct the full fine-grained picture or make predictions about other quantities of
interest (Chorin & Stinis, 2007; Kondrashov et al., 2015; Ma et al., 2019). Similar restrictions apply
when neural-network-based models are employed as e.g. (Chen et al., 2018; Li et al., 2020). Efforts
based on the popular SINDy algorithm (Brunton et al., 2016) require additionally data of the time-
derivatives of the variables of interest which when estimated with finite-differences introduce errors
and reduce robustness. Sparse Bayesian learning tools in combination with physically-motivated
variables and generative models have been employed by (Felsberger & Koutsourelakis, 2019) but
cannot guarantee the long-term stability of the learned dynamics as we also show in Appendix H.2
and in the context of the systems investigated in section 4.
Infusing domain knowledge from physics: Several efforts have been directed in endowing neural
networks with invariances or equivariances arising from physical principles. Usually those pertain
to translation or rotation invariance and are domain-specific as in Schutt et al. (2017). More gen-
eral formulations such as Hamiltonian (Greydanus et al., 2019; Toth et al., 2019) and Lagrangian
5
Published as a conference paper at ICLR 2021
Dynamics (Lutter et al., 2019) are currently restricted in terms of the dimension of the dynamical
system. Physical knowledge has been exploited in conjunction with Gaussian Processes in (Camps-
Valls et al., 2018) as well as in the context of PDEs for constructing reduced-order models as in
(Grigo & Koutsourelakis, 2019) or for learning modulated derivatives using Graph Neural Networks
as in (Seo et al., 2020). Another approach involves using physical laws as regularization terms or
for augmenting the loss function as in (Raissi et al., 2019; Lusch et al., 2018; Zhu et al., 2019;
Kaltenbach & Koutsourelakis, 2020). In the context of molecular dynamics multiple schemes for
Coarse-graining which also guarantee long-term stability have been proposed by Noe (2018) and WU
et al. (2017; 2018). In our formulation, physically-motivated latent variables are used to facilitate
generative maps to very high-dimensional data and serve as the requisite information bottleneck in
order to reduce the amount of training data needed.
Slowness and interpretability: Finally, in contrast to general state-space models for analyzing
time-series data such as Karl et al. (2016); Rangapuram et al. (2018); Li et al. (2019), the prior
proposed on the complex-valued zt enable the discovery of slow features which are crucial in pre-
dicting the evolution of multiscale systems and in combination with the variables Xt can provide
interpretability and insight into the underlying physical processes.
4	Experiments
The high-dimensional, fine-grained model considered consists of f identical particles which can
move in the bounded one-dimensional domain s ∈ [-1, 1] (under periodic boundary conditions).
The variables xt consist therefore of the coordinates of the particles at each time instant t and the di-
mension of the system f is equal to the number of particles. We consider two types of stochastic par-
ticle dynamics that correspond to an advection-diffusion-type (section 4.1) and a viscous-Burgers’-
type (section 4.2) behavior. In all experiments, the physically-motivated variables Xt relate to a
discretization of the particle density into d = 25 equally-sized bins for advection-diffusion-type
dynamics and into d = 64 equally-sized bins for viscous-Burgers’-type dynamics. In order to auto-
matically enforce the conservation of mass at each time instant, we make use of the softmax function
i.e. the particle density at a bin k is expressed as	；xp(Xtt,'	. Given this, the probabilistic map
l=0 exp(Xt,l )
(F in Equation (3)) corresponds to a multinomial density, i.e.:
exp(Xt,k)
Pd=0 exp(Xt,ι)
where mk (xt ) is the number of particles in bin k. The underlying assumption is that, given Xt, the
coordinates of the particles xt are conditionally independent. This does not imply that they move
independently nor that they cannot exhibit coherent behavior (Felsberger & Koutsourelakis, 2019).
Furthermore, the aforementioned model automatically satisfies permutation-invariance which a cen-
tral feature of the fine-grained dynamics. This is another advantage of the physically motivated
intermediate variables X as enforcing such symmetries/invariances is not a trivial task even when
highly expressive models (e.g. neural networks) are used (Rezende et al., 2019). The practical
consequence of Equation (14) is that no parameters θ2 need to be inferred.
The second map pertains to p(Xt | zt, θ1) which we represent with a multivariate normal distribu-
tion with a mean and a diagonal covariance matrix modeled by a neural network with parameters
θ1 . Details of the parameterization can be found in Appendix D.
We assess the performance of the method by computing first- and second-order statistics as illus-
trated in the sequel as well as in Appendix F. We provide comparative results on the same examples
in Appendix H where we report on the performance of various alternatives, such as a formulation
without the latent variables zt as well as deterministic and probabilistic Koopman-based models.
Moreover a short study on the effect of the amount of training data is included in Appendix G.
4.1 Particle Dynamics: Advection-Diffusion
We train the model on N = 64 time-series of the positions of f = 250 × 103 particles over
T = 40 time-steps which were simulated as described in Appendix D.1. Furthermore, we made
p(xt|Xt)
f!
m1(xt)! m2(xt)! . . . mk(xt)!
d
Y
k=1
mk(xt)
(14)
6
Published as a conference paper at ICLR 2021
use of h = 5 complex, latent processes zt,j . Details regarding the variational posteriors and neural
network architectures involved can be found in the Appendix D.1.
In Figure 2, the estimates of the complex-valued parameters λj are plotted as well as the inferred
and predicted time-evolution of 2 associated processes zt,j on the complex plane. We note the clear
separation of time-scales in the first plot with two slow processes, one intermediate and two fast
ones. This is also evident in the indicative trajectories on the complex plane. A detailed discussion
of the map G learned (Equation (2)) between zt and Xt can be found in the Appendix E.
Figure 2: Estimated λj (left) and the time evolution of two zt,j processes where one is slow (middle)
and the other fast (right).
In Figure 3 We compare the true particle density with the one predicted by the trained reduced model.
We note that the latter is computed by reconstructing the Xt futures. We observe that the model is
able to accurately track first-order statistics well into the future.
0	25	50	75	100	125	150	175	200
1.0
0.5
0.0
-0.5
-1.0
Figure 3: Particle density: Inferred and predicted posterior mean (bottom) in comparison with the
ground truth (top). The red line divides inferred quantities from predicted ones. The horizontal
axis corresponds to time-steps and the vertical to the one-dimensional spatial domain of the problem
s ∈ [-1, 1].
A more detailed view of the predictive estimates with snapshots of the particle density at selected
time instances is presented in Figure 4. Here, not only the posterior mean but also the associated
uncertainty is displayed. We want to emphasize the last Figure at t = 1000 when the steady state
has been reached which clearly shows that our model is capable of converging to stable equilibrium.
Figure 4:	Predicted particle density profiles at t = 80, 120, 160, 1000 (from left to right).
Since the proposed model is capable of probabilistically reconstructing the whole fine-grained pic-
ture, i.e. xt , predictions with regards to any observable can be obtained. In Appendix F.1 we assess
the accuracy of predictions in terms of second-order statistics, and in particular for the probability
of finding simultaneously a pair of particles at two specified positions.
7
Published as a conference paper at ICLR 2021
Finally, we demonstrate the accuracy of the trained in model in producing predictions under unseen
initial conditions as described in section 2.3. Figure 5 depicts a new initial condition in terms of
the particle density according to which particle positions x0 were drawn. The posterior on the
corresponding z0 (see section 2.3) can be used to reconstruct the density as shown also on the same
Figure.
Figure 5:	New initial condition (not used in training) in terms of the particle density. Reference
shows the actual initial condition, whereas the posterior mean and uncertainty bounds correspond to
the reconstruction of the initial condition based on the inferred latent variables z0 .
Figure 6 shows predictions of the particle density (i.e. first-order statistics) at various timesteps. We
want to emphasise the frame on the right, for t = 500 which shows the steady state of the system.
We note that even though the initial condition was not contained in the training data, our framework
is able to correctly track the system’s evolution and to predict the correct steady state.
Figure 6:	Predictions of the particle density at t = 25, 75, 125, 500 (left to right) for on the new
initial condition in Figure 5.
4.2 Particle Dynamics: Viscous Burgers’ Equation
In this example, we made use of N = 64 sequences of f = 500 × 103 particles over T = 40 time-
steps. Details regarding the physical simulator, the stochastic interactions between particles as well
as the associated network architectures are contained in Appendix D.2. As in the previous example,
we employed the particle density with the softmax transformation for Xt and h = 5 complex-valued
processes zt at the lowest model level.
In Figure 7 the estimated values for λj are shown where the clear separation of time-scales with
three slow and two fast processes can be observed.
Figure 7: Estimated λj for the viscous Burgers’ system.
In Figure 8 we compare the evolution of the true particle density with the (posterior mean of) the
model-predicted one. We point out the sharp front at the lower left corner which is characteristic of
the Burgers’ equation and which eventually dissipates due to the viscosity. This is captured in the
inferred as well as in the predicted solution.
8
Published as a conference paper at ICLR 2021
1.0
0.5
S 0.0
-0.5
-1.0
0	25	50	75	100	125	150	175	200
Timesteps in At
Figure 8: Particle density: Inferred and predicted posterior mean (bottom) in comparison with the
ground truth (top). The red line divides inferred quantities from predicted ones. The horizontal
axis corresponds to time-steps and the vertical to the one-dimensional spatial domain of the problem
s ∈ [-1, 1]
A more detailed view on the predictive results with snapshots of the particle density at selected
time instances is presented in Figure 9. We emphasize again the stable convergence of the learned
dynamics to the steady state as well as the accuracy in capturing, propagating and dissipating the
shock front.
Figure 9: Predicted particle density profiles at t = 40, 80, 120, 160, 1000 (from left to right).

We compare the accuracy of the predictions for second-order statistics of the fine-grained system
in terms of the two-particle probability in Appendix F.2 where excellent agreement with the ground
truth, i.e. the one computed by simulating the fine-grained system, is observed.
5 Conclusions
We presented a framework for efficiently learning a lower-dimensional, dynamical representation
of a high-dimensional, fine-grained system that is predictive of its long-term evolution and whose
stability is guaranteed. We infuse domain knowledge with the help of an additional layer of latent
variables. The latent variables at the lowest level provide an interpretabable separation of the time-
scales and ensure the long-term stability of the learned dynamics. We employed scalable variational
inference techniques and applied the proposed model in data generated from very large systems of
interacting particles. In all cases accurate probabilistic predictions were obtained both in terms of
first- and second-order statistics over a very long time range into the future. More importantly, the
ability of the trained model to produce predictions under new, unseen initial conditions was demon-
strated. An obvious limitation for the applicability of the proposed method to general dynamical
systems pertains to the physically motivated variables X . While such variables are available for
several classes of physical systems, they need to be re-defined when moving to new problems and
expert elicitation might be necessary. Furthermore, if an incomplete list of such variables is avail-
able from physical insight, this would need to be complemented by additional variables discovered
from data. To that end, one could envision that some of the abstract latent variables zt or functions
thereof, e.g. f(zt), could be employed in a generative map of the form xt = F(Xt, f(zt)) instead
of Equation (3). A final deficiency of the proposed model is the lack of an automated procedure for
determining the appropriate number of z. We believe that the ELBO, which provides a lower bound
on the model evidence, could be used for this purpose.
9
Published as a conference paper at ICLR 2021
References
H. H. Andersen, M. H0jbjerre, D. S0rensen, and P. S. Eriksen. The Multivariate Complex
Normal Distribution, pp. 15-37. Springer New York, New York, NY, 1995. ISBN 978-1-
4612-4240-6. doi: 10.1007/978-1-4612-4240-62. URL https://doi.org/10.1007/
978-1-4612-4240-6_2.
Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box
variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.
Robert Bamler and Stephan Mandt. Structured black box variational inference for latent time series
models. arXiv preprint arXiv:1707.01069, 2017.
Ted Belytschko and Jeong-Hoon Song. Coarse-graining of multiscale crack propagation. Interna-
tional journal for numerical methods in engineering, 81(5):537-563, 2010.
Steven L Brunton, Joshua L Proctor, andJ Nathan Kutz. Discovering governing equations from data
by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of
sciences, 113(15):3932-3937, 2016.
Gustau Camps-Valls, Luca Martino, Daniel H Svendsen, Manuel Campos-Taberner, Jordi Mufioz-
Mari, Valero Laparra, David Luengo, and Francisco Javier Garcia-Haro. Physics-aware gaussian
processes in remote sensing. Applied Soft Computing, 68:69-82, 2018.
Kathleen P Champion, Steven L Brunton, and J Nathan Kutz. Discovery of nonlinear multiscale
systems: Sampling strategies and embeddings. SIAM Journal on Applied Dynamical Systems, 18
(1):312-333, 2019.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. In Advances in neural information processing systems, pp. 6571-6583, 2018.
Alina Chertock and Doron Levy. Particle Methods for Dispersive Equations. Journal of
Computational Physics, 171(2):708-730, August 2001. ISSN 0021-9991. doi: 10.1006/
jcph.2001.6803. URL http://www.sciencedirect.com/science/article/pii/
S0021999101968032.
Alexandre Chorin and Panagiotis Stinis. Problem reduction, renormalization, and memory. Com-
munications in Applied Mathematics and Computational Science, 1(1):1-27, 2007.
Georges-Henri Cottet and Petros D. Koumoutsakos. Vortex Methods: Theory and Practice. Cam-
bridge University Press, Cambridge; New York, 2 edition edition, March 2000. ISBN 978-0-521-
62186-1.
L Felsberger and PS Koutsourelakis. Physics-constrained, data-driven discovery of coarse-grained
dynamics. Communications in Computational Physics, 25(5):1259-1301, 2019. doi: 10.4208/
cicp.OA-2018-0174.
Craig Gin, Bethany Lusch, Steven L Brunton, and J Nathan Kutz. Deep learning models for global
coordinate transformations that linearize pdes. arXiv preprint arXiv:1911.02710, 2019.
D. Givon, R. Kupferman, and A. Stuart. Extracting Macroscopic Dynamics: Model Problems and
Algorithms. Nonlinearity, 2004.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems, pp. 15379-15389, 2019.
Constantin Grigo and Phaedon-Stelios Koutsourelakis. A physics-aware, probabilistic machine
learning framework for coarse-graining high-dimensional systems in the small data regime. Jour-
nal of Computational Physics, 397:108842, 2019.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
10
Published as a conference paper at ICLR 2021
Sebastian Kaltenbach and Phaedon-Stelios Koutsourelakis. Incorporating physical constraints in a
deep probabilistic machine learning framework for coarse-graining dynamical systems. Journal
of Computational Physics, 419:109673, 2020.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep varia-
tional bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint
arXiv:1605.06432, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Stefan Klus, Feliks NUske, Peter Koltai, Hao Wu, Ioannis Kevrekidis, Christof Schutte, and Frank
Noe. Data-Driven Model Reduction and Transfer Operator Approximation. Journal ofNonlinear
Science, 28(3):985-1010,June2018. ISSN 1432-1467. doi: 10.1007∕s00332-017-9437-7. URL
https://doi.org/10.1007/s00332-017-9437-7.
Dmitri Kondrashov, Mickael D Chekroun, and Michael GhiL Data-driven non-markovian closure
models. Physica D: Nonlinear Phenomena, 297:33-55, 2015.
B. O. Koopman. Hamiltonian Systems and Transformations in Hilbert Space. Proceedings of the
National Academy of Sciences ofthe United States of America, 17(5):315-318, 1931. ISSN 0027-
8424. URL https://www.jstor.org/stable/86114.
S Laizet and JC Vassilicos. Multiscale generation of turbulence. Journal of Multiscale Modelling, 1
(01):177-196, 2009.
Kookjin Lee and Kevin T Carlberg. Model reduction of dynamical systems on nonlinear manifolds
using deep convolutional autoencoders. Journal of Computational Physics, 404:108973, 2020.
Ju Li, Panayotis G. Kevrekidis, C. William Gear, and Ioannis G. Kevrekidis. Deciding the Nature
of the Coarse Equation Through Microscopic Simulations: The Baby-Bathwater Scheme. SIAM
Rev., 49(3):469-487, July 2007. ISSN 0036-1445. doi: 10.1137/070692303. URL http:
//dx.doi.org/10.1137/070692303.
Longyuan Li, Junchi Yan, Xiaokang Yang, and Yaohui Jin. Learning interpretable deep state space
model for probabilistic time series forecasting. In IJCAI, pp. 2901-2908, 2019.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. arXiv preprint arXiv:2001.01328, 2020.
Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings
of nonlinear dynamics. Nature communications, 9(1):1-10, 2018.
Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning. arXiv preprint arXiv:1907.04490, 2019.
Chao Ma, Jianchun Wang, and Weinan E. Model Reduction with Memory and the Machine Learning
of Dynamical Systems. Communications in Computational Physics, 25(4):947-962, April 2019.
ISSN 1815-2406. doi: 10.4208/cicp.OA-2018-0269. Place: Wanchai Publisher: Global Science
Press WOS:000455963100001.
Frank Noe. Machine learning for molecular dynamics on long timescales. arXiv preprint
arXiv:1812.07669, 2018.
Shaowu Pan and Karthik Duraisamy. Physics-informed probabilistic learning of linear embeddings
of nonlinear dynamics with guaranteed stability. SIAM Journal on Applied Dynamical Systems,
19(1):480-509, 2020.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
11
Published as a conference paper at ICLR 2021
Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and
Tim Januschowski. Deep state space models for time series forecasting. In Advances in neural
information processing Systems, pp. 7785-7794, 2018.
Danilo Jimenez Rezende, Sebastien Racaniere, Irina Higgins, and Peter Toth. Equivariant Hamilto-
nian Flows. arXiv:1909.13739 [cs, stat], September 2019. URL http://arxiv.org/abs/
1909.13739. arXiv: 1909.13739.
Stephen Roberts. Convergence of a Random Walk Method for the Burgers Equation. Mathematics
of Computation, 52(186):647-673, 1989. ISSN 0025-5718. doi: 10.2307/2008486. URL http:
//www.jstor.org/stable/2008486.
Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Muller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature communications, 8(1):1-8,
2017.
Sungyong Seo, Chuizheng Meng, and Yan Liu. Physics-aware difference graph networks for
sparsely-observed dynamics. In International Conference on Learning Representations, 2020.
Berend Smit. Understanding molecular simulation: from algorithms to applications. Academic
Press, 1996.
Peter Toth, Danilo Jimenez Rezende, Andrew Jaegle, SebaStien Racaniere, Aleksandar Botev, and
Irina Higgins. Hamiltonian generative networks. arXiv preprint arXiv:1909.13789, 2019.
Richard Turner and Maneesh Sahani. A Maximum-Likelihood Interpretation for Slow Feature Anal-
ysis. Neural Computation, 19(4):1022-1038, April 2007. ISSN 0899-7667. doi: 10.1162/neco.
2007.19.4.1022. URL https://doi.org/10.1162/neco.2007.19.4.1022.
Hao Wu, Feliks Nuske, Fabian Paul, Stefan Klus, Peter Koltai, and Frank Noe. Variational koopman
models: slow collective variables and molecular kinetics from short off-equilibrium simulations.
The Journal of Chemical Physics, 146(15):154104, 2017.
Hao Wu, Andreas Mardt, Luca Pasquali, and Frank Noe. Deep generative markov state models. In
Advances in Neural Information Processing Systems, pp. 3975-3984, 2018.
Lazaros Zafeiriou, Mihalis A Nicolaou, Stefanos Zafeiriou, Symeon Nikitidis, and Maja Pantic.
Probabilistic slow features for behavior analysis. IEEE transactions on neural networks and
learning systems, 27(5):1034-1048, 2015.
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-
constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification
without labeled data. Journal of Computational Physics, 394:56-81, 2019.
12
Published as a conference paper at ICLR 2021
A Complex Normal distribution
In this Appendix, the complex random normal distribution is reviewed. The mathematical definitions
introduced follow Andersen et al. (1995):
A P-Variate complex normal random variable Y ∈ Cp with Y 〜CN(μc, ∑c) is defined by a
complex mean vector μc ∈ Cp and a complex Covariance Matrix ∑c ∈ C+×p. The density with
respect to Lesbegue measures on Cp can be stated as:
fγ(y) = ∏-p det(∑c)-1exp (-(y - μc)*Σ-1(y - μc))	(15)
where * indicates the conjugate transpose of a matrix.
This complex normal random variable has similar properties to the well-known, real-valued coun-
terpart. For instance, linear transformations of complex random normal variables are again complex
random normal variables.
These properties directly follow from the fact, that for a complex random normal variable there
exists an isomorphic transformation to a real valued 2p-variate normal random variable W ∈ R2p .
This random normal variable is defined with mean
μR
<(μc)
=(μc)
(16)
and covariance
∑ =1 ]<(∑C) -=(∑c)]	(17)
ςr = 2 [=(∑c) <(∑c) J	(17)
Therefore: W ~ N(μR, Σr)
As an example the real valued isomorphic counterpart of the standard complex random normal
distribution CN(0,1) is the bivariate normal distribution N(0, 21).
13
Published as a conference paper at ICLR 2021
B Choice of variance for a-priori steady state
We derive transient and stationary properties of the complex-valued latent processes zt,j and justify
our choices for the model parameters. Based on Equation (1) and for each process j, the dynamics
can be written in terms of the real and imaginary parts in two dimensions as follows:
<(zt,j)
=(zt,j)
Aj
<(zt,j-1)
=(zt,j-1)
+ σj
<(t,j)
=(t,j)
(18)
or:
t-1
<(zt,j) At	<(zt,0)	XAk	<(t-k,j)	19
=(zt,j)	=Aj	=(zt,0)	+σj Aj	=(t-k,j)	(19)
k=0
where <(j,j), =(Etj)〜N (0,1/2). The matrix Aj is given by (See also Equations (6), (7)):
Aj = sjRj = e<(λj)
cos(=(λj))
sin(=(λj))
and can be diagonalized as Aj = VPj- V * where:
1 J 1
√2 [-i
Pj
- sin(=(λj))
cos(=(λj))
eλj	0
0	eʌj ,
(20)
(21)
V
1
i
V * is the conjugate transpose of V and Xj denotes the complex conjugate of λj-. Due to the linearity
of the model and the Gaussian initial conditions, i.e. <(zo,j), =(zo,j)〜N(0, σ2,j-/2), the marginal
will remain Gaussian at all times t. A direct consequence of the above is that the mean of real and
imaginary parts is always zero, i.e.:
(j)	E[<(zt,j)]	0
μt =回=(zt,j )]J = H.
Furthermore, the covariance Ct(j) is given by:
2	t-1
Cj= E <(zt,j)	[<(zt,j) =(Ztj)]=等Aj(AT)t + σk X Aj(Aj)k
L L "」	j	L-C
which upon use of the diagonalized form of Aj yields:
2
CCj) —	0,j V 0
Ct =	2 V	e2t<(λj)
e2t<(λj)	σ2	0
e	VT	L∖∕^
0 V + 2 V	1-e2t<(λj)
J	1-e2<(λj)
1-e2<(λj)	V T
0
(22)
(23)
(24)
As mentioned earlier, a necessary condition for the long-term stability of the processes
<(λj ) < 0, in which case and as t → ∞ it leads to:
0
1
1-e2<(λj)
2
Cjj → Cj = VV
—11	评
1-e2<(λj) V T =	σj	I
0	= 2(1 - e2<(%))
is that
(25)
This implies that real and imaginary parts are asymptotically uncorrelated with a variance
2
——2⅛(λ-yy. By setting σj = 1 - e2<(λj) We enable direct comparisons between zt,j solely on
the basis of Re(λj) i.e. the degree of slowness (Turner & Sahani, 2007; Zafeiriou et al., 2015)).
In this case the asymptotic variance of real and imaginary parts becomes 1/2. Finally by setting
σ0,j = 1 we ensure that, a priori, the processes zt,j are stationary, with C(j) = C∞ = 21, i.e. no
a-priori bias is introduced with regards to their transient characteristics.
We finally note that the autocovariance Dτ(j) of each of these stationary processes j is given by:
Dτ(j)=E=<((zztt++ττ,jj))	[<(zt,j)=(zt,j)]	=AjτE<=((zztt,jj))	[<(zt,j)=(zt,j)]	= Ajτ	Ct(j)
,	,	(26)
where Aj and the covariance Ct(j ) are given above. By exploiting the diagonalization of Aj and
that C(j) = C∞ = 11 (for the parameter values discussed earlier), we obtain that:
D(j)	eτ<(λj) cos(τ=(λj)) - sin(τ =(λj))	(27)
τ	sin(τ =(λj))	cos(τ =(λj))
One can clearly observe harmonic (cross-)correlation terms which depend on the imaginary part of
the λj and can capture persistent periodic effects of the dynamical system in the long-time range.
14
Published as a conference paper at ICLR 2021
C Derivation of the ELBO
This section contains details of the derivation of the Evidence-Lower-Bound (ELBO) which serves
as the objective function for the determination of the parameters φ and θ during training. In partic-
ular:
log p(x01τn)∣θ)
= log R p(x(01:T:n),
= log R p(x0:T |
(x(1:n)
≥ R log p(x0:T
(1:n)	(1:n)	(1:n) (1:n)
, X0:T , z0:T , θ ) dX0:T	z0:T
(1:n)	(1:n)	(1:n)	(1:n)
|X0:T，z0:T，θ)p(X0:T，z0:T，θ)
“ qφ(XOITn), z(12n))
(1:n)	(1:n)	(1:n)	(1:n)
|X0:T ，z0:T ，θ)p(X0:T ，z0:T ，θ)
qφ(x01Tn)
F (qφ(X01τn), z(1τn)), θ)
qφ (X01τn), z(1τn))
qφ(x01Tn)
(1:n)) dX(1:n) d (1:n)
, z0:T ) dX0:T dz0:T
(1:n)) dX(1:n) d (1:n)
, z0:T ) dX0:T dz0:T
(28)
15
Published as a conference paper at ICLR 2021
D Details for experiments
This appendix contains details for our experiments involving moving particles that have stochas-
tic interactions corresponding to either an Advection-Diffusion behaviour or viscous Burgers’ type
behaviour.
D. 1 Particle Dynamics: Advection-Diffusion
For the simulations presented f = 250 × 103 particles were used, which, at each microscopic time
step δt = 2.5 X 10-3 performed random, non-interacting, jumps of size δs =焉,either to the left
with probability pleft = 0.1875 or to the right with probability pright = 0.2125. The positions were
restricted in [-1, 1] with periodic boundary conditions. It is well-known (Cottet & Koumoutsakos,
2000) that in the limit (i.e. f → ∞) the particle density ρ(s, t) can be modeled with an advection-
diffusion PDE with diffusion constant D = (Pleft + Pright)第 and velocity V = (Pright — Pleft) ⅛:
%+ vdP = D姓
∂t+ ∂s	D∂s2,
s∈ (—1, 1)..
(29)
From this simulation every 800th microscopic time step the particle positions were extracted and
used as training data for our system. Sample initial conditions are shown in Figure 10:
Figure 10:	Sample initial conditions for the advection-diffusion type dynamics.
The architecture of the neural networks for the generative mappings described above as well as for
the variational posteriors introduced in Section 2.2 can be seen in Figure 11. The neural network
used for the generative mapping between the low-dimensional states zt and the mean and covariance
for Xt consists of only one dense layer, whereas the variational posterior on z0:T is parameterized
using a dense Layer with ReLu activation followed by another dense layer.
Zt
Xo：T
Dense Layer with 50 Neurons
Mx,t ∑x,t
Figure 11:	Neural Net architecture used for the particle dynamics corresponding to an advection-
diffusion equation.
D.2 Particle Dynamics: Viscous Burgers’ equation
The second test-case involved a fine-grained system of f = 500 × 103 particles which perform
interactive random walks i.e. the jump performed at each fine-scale time-step δt = 2.5 × 10-3
depends on the positions of the other walkers. In particular we adopted interactions as described in
Roberts (1989); Chertock & Levy (2001); Li et al. (2007) so as, in the limit (i.e. when f → ∞, δt →
0, δs → 0), the particle density ρ(s, t) follows a viscous Burgers’ equation with ν = 0.0005:
∂ρ 1 ∂ρ2 ∂2ρ
∂t + 2 而=ν∂t2,
s ∈ (—1, 1).
(30)
16
Published as a conference paper at ICLR 2021
From this simulation every 800th microscopic time step the particle positions were extracted and
used as training data for our system. Sample initial conditions are shown in Figure 12.
Figure 12:	Sample initial conditions for the Burger’s type dynamics.
The architecture of the neural networks for the generative mappings described above as well as for
the variational posteriors introduced in Section 2.2 can be seen in Figure 13. The neural network
used for the generative mapping between the low-dimensional states zt and the mean and covariance
for Xt consists of several dense layers with ReLu activation and Dropout layers to avoid overfitting,
whereas the variational posterior on z0:T is parameterized using two dense layers with ReLu activa-
tion followed by another dense layer.
Xo.τ
%(Xθ:T)	BO(XOb)
Mx,t ∑x,i
Figure 13:	Neural Net architecture used for the particle dynamics corresponding to a viscous Burg-
ers’ equation.
17
Published as a conference paper at ICLR 2021
E Detailed analysis of the generative mapping and the slow
LATENT VARIABLES
In this Appendix we take a closer look at the generative mapping and the (slow) latent variables z
learned . For the Advection-Diffusion example, we discovered two slow processes (Section 4.1), z1
and the marginally faster process z2 . The rest of the processes were very fast in comparison and
took values close to the zero point of the complex plane during the inference as well as during the
prediction phase.
In order to visualize the influence through the generative mapping of these two slow processes, we
set the value of all other processes to zero and then reconstructed the fine-grained state based on dif-
ferent absolute values of z1 and z2. The result are shown in Figure 14 in terms of the reconstructed
particle density. It is clearly visible that those two processes are responsible for a variety of density
profiles. In accordance with their slowness, z1 (the slightly slower process) is responsible for the
most striking changes, whereas the other slow process generates some smaller scale fluctuations.
Figure 14: Reconstruction of the particle density profiles for various values of the two slowest
processes z1 and z2 identified. All other latent variables zt,j were set to zero.
18
Published as a conference paper at ICLR 2021
F Two-point Probability
This appendix contains the predictive estimates for the two-point probability, i.e. the probability of
finding two particles simultaneously in two bins (b1, b2). This two-point probability can be com-
puted based on the reconstructed fine-grained system and corresponds to a second order statistic.
F.1 Particle Dynamics: Advection-Diffusion
The estimated two-point probability as well as the comparison to test data is shown for two indicative
time-instants in Figure 15 and 16. We note a very good agreement with the ground truth.
O 5 IO 15	20
*1
・0.0024
・0.0022
・0.0020
・0.0018
・0.0016
・0.0014
・0.0012
・0.0010
Figure 15:	Two-point probability at time step 90: On the left the two-point probability of the data
is shown as reference, the figure in the middle contains the predictive posterior mean whereas the
figure on the right contains the standard deviation. The figure on the left and the figure in the middle
share the same colorbar.
O 5	10	15	20
b1
・0.0024
・0.0022
・0.0020
・0.0018
・0.0016
・0.0014
・0.0012
・0.0010
O 5	10	15	20
*1
-0.∞010
-0.00008
-0.∞006
-0.∞004
-0.∞002
-0.∞000
Figure 16:	Two-point probability at time step 140: On the left the two-point probability of the data
is shown as reference, the figure in the middle contains the predictive posterior mean whereas the
figure on the right contains the standard deviation. The figure on the left and the figure in the middle
share the same colorbar.
F.2 Particle Dynamics: Viscous Burgers’ equation
The estimated two-point probability as well as the comparison to test data is shown for two indicative
time-instants in Figure 17 and 18. We note a very good agreement with the ground truth.
0.0∞350
0.0∞325
0.000300
0.0∞275
0.0∞250
0.0∞225
0.0∞200
0.0∞175
0.0∞150
O
10
20
30
40
50
60
*1
-0.000030
-0.000025
-0.000020
-0.000015
-0.000010
-0.000005
-o.ooo∞o
Figure 17:	Two-point probability at time step 90: On the left the two-point probability of the data
is shown as reference, the figure in the middle contains the predictive posterior mean whereas the
figure on the right contains the standard deviation. The figure on the left and the figure in the middle
share the same colorbar.
19
Published as a conference paper at ICLR 2021
20	40	60
*1
0.0∞350
0.0∞325
0.000300
0.0∞275
0.0∞250
0.0∞225
0.0∞200
0.0∞175
0.0∞150
O
10
20
30
40
50
60
20	40	60
*1
-0.000030
-0.000025
-0.000020
-0.000015
-0.000010
-0.000005
-0.000∞0
Figure 18:	Two-point probability at time step 140: On the left the two-point probability of the data
is shown as reference, the figure in the middle contains the predictive posterior mean whereas the
figure on the right contains the standard deviation. The figure on the left and the figure in the middle
share the same colorbar.
20
Published as a conference paper at ICLR 2021
G Effect of the amount of training data
This section contains a study on the influence of the amount of training data. We illustrate this in the
context of the Advection-Diffusion example (section 4.1) by using 16 time sequences instead of the
64 employed earlier. In the figures below we note that the proposed model is capable capturing the
main features of the system’s dynamics as well as the correct steady state even with fewer training
data. We believe that this is due to the intermediate layer of physically-motivated variables Xt
which introduces an information bottleneck. Finally, and as one would expect, fewer training data
leads to increased predictive uncertainty.
S	S
Figure 19:	Predicted particle density profiles at t = 80, 160, 1000 (from left to right) with 64 sam-
ples.
S	S
Figure 20:	Predicted particle density profiles at t = 80, 160, 1000 (from left to right) with 16 sam-
ples.
21
Published as a conference paper at ICLR 2021
H Comparison with other approaches
This appendix contains results obtained by other methods for the test cases discussed in the main
text. The simulation data is identical to the one used for the proposed method and details of the
specific algorithms are described in the following.
H. 1 Real-valued latent space
To demonstrate the utility of a complex-valued latent space, the two examples were also solved with
a real-valued zt,j. The only difference here is the restriction of the latent variables zt,j and the λj to
real values.
A model with real-valued latent space is also capable of ensuring the stability but it is not capable
of capturing periodic components of the dynamics. As most physical systems (including the two
examples) contain such components, the algorithm is not able to accurately model the dynamics and
fails in generating reliable predictions. This can be readily observed in the extrapolative predictions
of Figures 21 and 22 where, apart for the biased results, one can also note increased predictive
uncertainty.
Figure 21:	Advection-Diffusion system: Predictions at t = 0, 80, 160 and 1000 obtained with real-
valued latent variables zt,j .
Figure 22:	Burgers’ system: Predictions at t = 0, 80, 160 and 1000 obtained with real-valued latent
variables zt,j .
22
Published as a conference paper at ICLR 2021
H.2 No stable latent space
Another possibility is to employ only the physically motivated latent variables Xt and remove com-
Pletely the latent variables Zt in the first layer. This approach is similar to the one investigated in
Felsberger & Koutsourelakis (2019) and Kaltenbach & Koutsourelakis (2020) as well as to the idea
of neural ODES (Chen et al., 2018). In this case, one must learn directly the dynamics of Xt and for
this purpose We employed a three-layer, fully-connected neural network NN as follows:
Xt+1 = NN(Xt) + σa E 〜N(0, I).	(31)
The learned dynamics are in general non-linear and stability is not guaranteed. As it can be observed
in Figures 23 and 24, the trained model is capable of producing accurate predictions for some time-
steps but eventually in both cases predictions become unstable. This could also be problematic when
the trained model is used to make predictions with new initial conditions as the chaotic nature of the
nonlinear dynamics can lead to significant errors even for shorter time horizons.
Figure 23:	Advection-Diffusion system: Predictions at t = 0, 80,160 and 1000 obtained without Zt
and with the model of Equation (31).
Figure 24: Burgers’ system: Predictions at t = 0, 80, 160 and 1000 obtained without Zt and with
the model of Equation (31).
23
Published as a conference paper at ICLR 2021
H.3 Koopman-based models
The final alternative explored involved probabilistic and deterministic Koopman-based models for
the latent dynamics. We kept the generative framework of our model and did not use an encoder
as for instance in Gin et al. (2019) in order to remove the effect of the associated model choice.
For the same reason, we retained the intermediate variables Xt even though these do not appear in
any known Koopman-operator implementations. We replaced our complex-valued dynamics of the
latent processes zt,j with the models described in the sequel.
H.3.1 Probabilistic Koopman-based model
We used real-valued latent variables zt which are not a-priori independent and whose dynamics are
parameterized with a Koopman matrix K and a diagonal noise matrix W :
Zt+1 = Kzt + We,	€ 〜N(0,I).	(32)
The learned matrix K is not guaranteed to be stable in the absence of additional constraints but
in both cases examined the eigenvalues of the learned K were real smaller than one. Long-term
predictions were stable and for the Burgers’ case in Figure 26 we were also able to reach the true
steady state. For the (simpler) Advection-Diffusion example in Figure 25 an incorrect steady state
was reached and the predictive quality started to deteriorate after some time-steps. In comparison
to our framework, the probabilistic Koopman-based model does not provide a direct separation
between slow and fast processes and therefore its interpretability is reduced.
Figure 25: Advection-Diffusion system: Predictions at t = 0, 80, 160 and 1000 obtained with the
probabilistic Koopman-based model of Equation (32)).
Figure 26: Burgers’ system: Predictions at t = 0, 80, 160 and 1000 obtained with the probabilistic
Koopman-based model of Equation (32)).
H.3.2 Non-probabilistic Koopman Learning
We also used real-valued latent variables zt with deterministic dynamics which were parameterized
as follows:
zt+1 = Kzt	(33)
The absence of noise in comparison to Equation (32), led in both cases to an estimate for the Koop-
man matrix K that did not yield stable predictions (each of the learned K matrices had at least one
eigenvalue which was larger than 1). We speculate that the lack of stochasticity made the model less
capable of dealing with the information loss. We also note in Figures 27 and 28 that the predictions
obtained are, with an exception of a few time-steps, highly inaccurate.
24
Published as a conference paper at ICLR 2021
Figure 27: Advection-Diffusion system: Predictions at t
deterministic Koopman-based model of Equation (33)).
0, 20, 80 and 160 obtained with the
Figure 28: Burgers’ system: Predictions at t = 0, 20 and 80 obtained with the deterministic
Koopman-based model of Equation (33)).
25