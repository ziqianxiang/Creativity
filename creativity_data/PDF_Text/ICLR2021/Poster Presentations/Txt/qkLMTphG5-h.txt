Published as a conference paper at ICLR 2021
Repurposing Pretrained Models for Robust
Out-of-domain Few-Shot Learning
Namyeong Kwon, HWidong Na*
Samsung Advanced Institute of Technology (SAIT), South Korea
{ny.kwon,hwidong.na}@samsung.com
Gabriel Huang
Mila, Universite de Montreal
Simon Lacoste-Julient
Mila, Universite de Montreal
SAIT AI Lab, Montreal
Ab stract
Model-agnostic meta-learning (MAML) is a popular method for few-shot learning
but assumes that we have access to the meta-training set. In practice, training on
the meta-training set may not always be an option due to data privacy concerns,
intellectual property issues, or merely lack of computing resources. In this paper,
we consider the novel problem of repurposing pretrained MAML checkpoints to
solve new few-shot classification tasks. Because of the potential distribution mis-
match, the original MAML steps may no longer be optimal. Therefore we propose
an alternative meta-testing procedure and combine MAML gradient steps with ad-
versarial training and uncertainty-based stepsize adaptation. Our method outper-
forms “vanilla” MAML on same-domain and cross-domains benchmarks using
both SGD and Adam optimizers and shows improved robustness to the choice of
base stepsize.
1 Introduction
Deep learning approaches have shown improvements based on massive datasets and enormous com-
puting resources. Despite their success, it is still challenging to apply state-of-the-art methods in the
real world. For example, in semiconductor manufacturing (Nishi & Doering, 2000), collecting each
new data point is costly and time consuming because it requires setting up a new manufacturing pro-
cess accordingly. Moreover, in the case of a “destructive inspection”, the cost is very high because
the wafer must be destroyed for measurement. Therefore, learning from small amounts of data is
important for practical purposes.
Meta-learning (learning-to-learn) approaches have been proposed for learning under limited data
constraints. A meta-learning model optimizes its parameters for the best performance on the distri-
bution of tasks. In particular, few-shot learning (FSL) formulates “learning from limited data” as an
n-way k-shot problem, where n is the number of classes and k is the number of labeled samples per
class. For each task in FSL, a support set is provided for training, while a query set is provided for
evaluation. Ideally, a meta-learning model trained over a set of tasks (meta-training) will generalize
well to new tasks (meta-testing).
Model-agnostic meta-learning (MAML) (Finn et al., 2017) is a general end-to-end approach for
solving few-shot learning tasks. MAML is trained on the meta-training tasks to learn a model
initialization (also known as checkpoint) such that a few gradient steps on the support set will yield
the best predictions on the query set.
However, in practice it may not always be possible to retrain or finetune on the meta-training set.
This situation may arise when the meta-training data is confidential, subject to restrictive licences,
contains private user information, or protected intellectual property such as semiconductor manu-
* Work done as visiting researchers at SAIT AI Lab, Montreal.
t Canada CIFAR AI Chair
1
Published as a conference paper at ICLR 2021
facturing know-how. Another reason is that one may not have the computing resources necessary
for running large-scale meta-training.
In this paper, we consider the novel problem of repurposing MAML checkpoints to solve new few-
shot classification tasks, without the option of (re)training on the meta-training set. Since the meta-
testing set (new tasks) may differ in distribution from the meta-training set, the implicit assumption
-in end-to-end learning- of identically distributed tasks may not hold, so there is no reason Why
the meta-testing gradient steps should match the meta-training. Therefore, we investigate various
improvements over the default MAML gradient steps for test time adaptation.
Conceptually, our approach consists of collecting information While training the model on a neW
support set and then proposing Ways to use this information to improve the adaptation. In this paper,
We consider the variance of model parameters during ensemble training as a source of information
to use. We propose algorithms that uses this information both to adapt the stepsizes for MAML as
Well as to generate “task-specific” adversarial examples to help robust adaptation to the neW task.
Our main contributions are the folloWing:
•	We motivate the novel problem of repurposing MAML checkpoints to solve cross-domain feW-
shot classification tasks, in the case Where the meta-training set is inaccessible, and propose a
method based on uncertainty-based stepsize adaptation and adversarial data augmentation, Which
has the particularity that meta-testing differs from meta-training steps.
•	Compared to “vanilla” MAML, our method shoWs improved accuracy and robustness to the
choice of base stepsizes on popular cross-domain and same-domain benchmarks, using both the
SGD and Adam (Kingma & Ba, 2014) optimizers. Our results also indicate that adversarial train-
ing (AT) is helpful in improving the model performance during meta-testing.
To the best of our knoWledge, our Work is the first feW-shot learning method to combine the use
of ensemble methods for stepsize computation and generating adversarial examples from the meta-
testing support set for improved robustness. Moreover, our empirical observation of improving
over the default meta-testing procedure of MAML motivates further research on alternative Ways to
leverage published model checkpoints.
2	Related Work
2.1	Meta-learning and Model-agnostic meta-learning
FSL approaches deal With an extremely small amount of training data and can be classified into three
categories. First, metric-based approaches solve feW-shot tasks by training a feature extractor that
maximizes inter-class similarity and intra-class dissimilarity (Vinyals et al., 2016; Snell et al., 2017;
Sung et al., 2018). Second, memory-based approaches utilize previous tasks for neW tasks With
external memories (Santoro et al., 2016; Mishra et al., 2018). Third, optimization-based approaches
search for good initial parameters during training and adapt the pretrained model for neW tasks (Finn
et al., 2017; Lee & Choi, 2018; Grant et al., 2018). We focus on the optimization-based approaches
and suggest better training especially for the general family of MAML methods.
2.2	Uncertainty for model training
Uncertainty is an important criterion for measuring the robustness of a neural netWork (NN).
Bayesian neural netWorks (BNN) (Blundell et al., 2015) obtain model uncertainty by placing prior
distributions over the Weights p(ω). This uncertainty has been used to adapt the stepsizes during
continual learning in Uncertainty-guided Continual BNNs (UCB) (Ebrahimi et al., 2020). For each
parameter, UCB scales its stepsize inversely proportional to the uncertainty of the parameter in the
BNN to reduce changes in important parameters While alloWing less important parameters to be
modified faster in favor of learning neW tasks. Our approach also decreases the stepsizes for “uncer-
tain” parameters, but using a different notion of uncertainty, and instead in the context of FSL With
a pretrained MAML checkpoint.
Unlike BNNs, deep ensembles (Lakshminarayanan et al., 2017) estimate uncertainty of NNs us-
ing ensembles over randomly initialized parameters combined With adversarial training (AT). Deep
ensembles have been successfully used to boost predictive performance and AT has been used to im-
2
Published as a conference paper at ICLR 2021
prove robustness to adversarial examples. Lakshminarayanan et al. (2017) showed that they produce
predictive uncertainty estimates comparable in quality to BNNs. Deep ensembles, however, are not
directly applicable for FSL tasks, as training randomly initialized parameters from scratch with a
limited amount of training data yields poor performance. Our approach is partly inspired from deep
ensembles, adapting it to the FSL setting by using instead a parameter perturbations of the MAML
checkpoint model rather than a random initialization. We use in particular a multiplicative Gaussian
perturbation that rescales the parameters, as the information content of the weights is said to be
invariant to their scale (Wen et al., 2018).
2.3	Adversarial training for attack and defense
Deep NNs are sensitive to adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Moosavi-
Dezfooli et al., 2016). These methods generate an adversarial sample to fool a trained model, where
the generated image looks identical to the original one for humans. Goodfellow et al. (2015) pro-
posed the fast gradient sign method (FGSM) which generates adversarial example using sign of
input gradient. While stronger attacks have been proposed (such as using projected gradient de-
scent (Madry et al., 2018)), we will focus on the FGSM approach in this paper for simplicity. In
FSL, adversarial attack and defense have also been studied. ADversarialMeta-Learner (Yin et al.,
2018) utilized one-step adversarial attack for generating adversarial samples during meta-training.
There is little consideration, however, for the degradation of accuracy in the original sample in ad-
versarial defense approaches. Xie et al. (2020) reported improvement using generated examples
for adversarial attack on the large scale training. Motiian et al. (2017) propose adversarial domain
adaptation for FSL. Despite similar keywords, their work is distinct from ours and relies on using
generative adversarial networks discriminators to confuse domains, whereas we rely on adversarial
examples for improved robustness. To our best knowledge, there is no prior FSL work which uses
adversarial examples to enhance the model at test-time.
2.4	Domain adaptation
Domain adaptation methods (DA) (Ben-David et al., 2010) attempt to alleviate the distribution
mismatch between source and target domains. Most of the recently proposed DA approaches are
based on generative adversarial networks (GANs) (Goodfellow et al., 2014). GAN-based DA ap-
proaches require having access to the large amount of unlabeled data from both the source and target
datasets (Tzeng et al., 2017; Zhang et al., 2019; Wilson & Cook, 2020). DA methods cannot be di-
rectly applied in the FSL scenario due to limited number of target domain samples (shots). Some
domain-adaptive FSL (DA-FSL) methods have been proposed in the case where only a very few
samples from the target domain are available (Motiian et al., 2017; Zhao et al., 2020).
DA and DA-FSL methods cannot be directly applied to our setting because we assume that the
meta-training dataset is inaccessible — mirroring real world situations in which access to the meta-
training set is restricted by privacy and confidentiality concerns. Our approach differs from DA and
DA-FSL by not requiring access to the meta-training dataset (source domain).
3	Proposed Method
At meta-testing time, MAML normally uses the support set to compute fixed gradient steps, which
were “calibrated” using end-to-end learning during meta-training. That is, the learned initialization
is such that a fixed combination of stepsize and loss result in the desired result. However, those
stepsizes and losses may be suboptimal on the new task, especially if the new task is out-of-domain
with respect to the meta-training tasks.
Our method is based on the assumption that the support set can be used to improve the meta-testing
procedure itself, beyond merely serving as training examples. We start by leveraging the support
set to estimate task-specific uncertainties over the model parameters. Then, we propose two im-
provements over the “vanilla” MAML gradient steps : we scale the gradient steps using layer-wise
stepsizes computed from the support set and we train using task-specific adversarial examples.
3
Published as a conference paper at ICLR 2021
3.1	Motivating hypotheses
At meta-testing time, we start by assuming that we can estimate task-specific uncertainties over the
model parameters. One possibility, which we adopt in Section 3.2, is to train deep ensembles (Lak-
shminarayanan et al., 2017) on the support set and use the ensemble to estimate variances over model
parameters. Each model learns slightly different parameters, and yields slightly different gradients.
We regard the variance over the parameters and input gradients as task-specific uncertainties. Given
the uncertainty estimates, we propose two modifications to the original MAML gradient steps.
Proposal 1 (Task-specific stepsizes) Use lower Stepsizes for model parameters with higher
variance.
In our case, the variance could be further amplified if high-variance components were to be
moved with large stepsizes. Therefore, we carefully move high-variance components with a
lower stepsize with the hope is that we can limit the variance over the model parameters. This
approach can be related with the fact that lower stepsizes should be taken for SGD when the
gradients are very noisy (Dieuleveut et al., 2020).
Proposal 2 (Task-specific adversarial examples) Use adversarial examples with higher ad-
versarial perturbation on input components with higher variance over the input gradient.
The intuition is that if only slightly perturbed models (from the ensemble) disagree on parts of
the input gradient, then it means that they disagree on what to learn and therefore those parts of
the input are more vulnerable to attack. Therefore, we propose to use AT with stronger adver-
sarial perturbation in the weak parts of the input, with the hope to incur improved robustness
on those parts ofthe input.
We regard adversarial training as a form of data augmentation or regularization at meta-testing time,
which we hope allows the model to overcome the limited size of the support set.
3.2	Uncertainty-based gradient steps at test-time
We improve over the default MAML gradient steps by implementing the ideas presented in the previ-
ous section. The resulting approach is detailed in Algorithm 1 and is a combination of uncertainty-
based stepsize adaption (USA, based on Proposal 1) and uncertainty-based fast gradient sign method
(UFGSM), adversarial training (AT), and generating additional adversarial examples from deep en-
sembles (EnAug) which are based on Proposal 2.
Denote L(D, θ) = 1/|D| P(x,y)∈D lθ(x, y) the cross-entropy loss for model θ over the labeled
dataset D = {(x,y)... }, Aθ(x, y) = X + esign(Vχlθ(x, y)) the FGSM adversarial example
computed from (x, y) and LAT(D, θ) = 1/|D| P(x,y)∈D lθ(Aθ(x, y), y) the resulting adversarial
cross-entropy, which we refer to as AT. For easy reference, all notations are summarized in Ap-
pendix A.1.
Starting from the pretrained MAML checkpoint θ0, we perturb the model parameters with multi-
plicative Gaussian noise to create a deep ensemble (θ0m)1≤m≤M (lines 2-4). Then, we repeat the
following for T steps. At time t, run gradient descent on each model θtm of the ensemble (line 7),
where the loss is a combination of the usual cross-entropy L and AT loss LAT as in (Lakshmi-
narayanan et al., 2017) but on the support set, and the stepsizes αadap are updated using USA (details
below). Also, we generate adversarial examples using FGSM and UFGSM (details below) and
store them into DAug (lines 8 and 11). Finally, we run T gradient steps on the original checkpoint,
where the loss is a combination of cross-entropy on the support set DSpt, AT loss on the support
set, and cross-entropy on the ensemble-augmented support set DAug (lines 13-16). Note that we
recover the original MAML steps for (λAT = λAug = 0, λα = 1), while we refer to the case
(λAT = λAug = 1, λα = 0) as our full method.
4
Published as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
2
3
4
5
6
7
Algorithm 1: Uncertainty-based Gradient Steps at Test-time
Data: New task support set DSpt = {(x1, y1), . . . , (xn×k, yn×k)} with n ways and k shots.
Require: Base stepsize α, pretrained weights θ0, Gaussian variance σ, AT coefficient , size of
ensemble M, number of gradient steps T, selection coefficients in {0, 1} for: base
stepsize λα, adversarial loss λAT, and augmented cross-entropy λAug .
DAug J 0, aadap J a
for m = 1 to M do
θ0m J θ0(1 + N (0, σ))	.Initialize deep ensemble
end
for t = 1 to T do
form= 1 to M do
θm J θm-1 - aadap Θ Vθmι [L(DSpt,θm-1) + LAT(DSpt, θm-1)]
DAug J DAug ∪ {(Aθm (x, y), y) | (x, y) ∈ DSpt}	.EnAug
end
aadap j usA(α,θ1:M)	.Algorithm 2
DAug j DAug ∪ { UFGSM(θo,θJM,x,y) | (x,y) ∈ DSpt}	.Algorithm 3
end
α J λαα+ (1 -λα)αadap
for t = 1 to T do
θt J θt-1 - α Θ Vθt-1 [L(DSpt, θt-1) + λATLAT(DSpt, θt-1) + λAugL(DAug, θt-1)]
end
USA. We propose uncertainty-based stepsize adaptation (USA), which assigns lower stepsizes
to layers1 with higher uncertainty (Algorithm 2) - We call this loosely an “inverse-relationship”
below. Let a denote the default (scalar) stepsize and θ1:M the parameters of the ensemble models,
where M is the size of the ensemble and the number of layers in model is L. To compute the
adapted layer-wise stepsizes αadap, we compute u, the parameter-wise standard deviation of the
model parameters over the ensemble (line 2), apply an inverse-relationship transformation which
flips the max with the min (line 3), average over each layer (line 4) and L1 -normalize the result (line
5). The design choices for USA is explained in Appendix A.2. The resulting stepsizes αadap have
an inverse-relationship with the variance of each layer. We give an example of applying USA to the
4-ConvNet architecture on miniImageNet (Figure 1). On the left, we plot the layer-wise standard
deviations of the parameters and on the right the corresponding USA stepsizes, which follow an
inverse relationship with the standard deviations.
Algorithm 2: USA	_ Algorithm 3: UFGSM	
Function USA(α, θ1M) U = Std(θ/M)		l')pfin P* M ^i τ*ι M avNcvm X- } —	X—Min(X)	
	1	Define: MinMaxNorm(X) = Maχ(X)-Min(X) Function UFGSM(θ, θ1M, x, y)
c = Max(u) - u + Min(u)	2	U = Std({Vχlθ0(x, y) | θ0 ∈ θ1M})
μ j average of C over each layer l	3	u = MinMaxNorm(u)
αadap = aμl/(1/L PL=I μl)	4	x0 = x + UΘ sign(Vx lθ (x, y))
return αadap	5	return x0
end	6	end
UFGSM. We also propose uncertainty-based FGSM (UFGSM) to generate adversarial examples,
with higher adversarial perturbation on input components with higher variance over the input gradi-
ent (Algorithm 3). Starting from an image x and label y, we compute the input gradient for each
model from the ensemble and calculate the standard deviation u over the ensemble (line 2). Then,
we linearly map u between 0 and 1 (line 3), compute the FGSM adversarial example for the pre-
1In this study, we choose to use layer-wise stepsizes, but it is also possible to use kernel-wise or parameter-
wise stepsizes. The motivation for using layer-wise stepsizes is because features from the same layer tend to
have the same level of abstraction (Zeiler & Fergus, 2014).
5
Published as a conference paper at ICLR 2021
2 1 8 6 4 2 0
J0----
O Oooo
(UoQETʌəp p,wpuυ)uτυHOU∩
0.012	■ Convolution layer weight	Batch normalization layer gamma
■ Convolution layer bias	■ Batch normalization layer beta
■ Linear layer weight and bias
8 6 4
Ooo
Ooo
■ ■ ■
Ooo
əzɪsds
(a) Uncertainty of layers
Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Here α = 0.01. (a) is standard
deviation of trained ensemble models’ weights. Each layer has a difference uncertainty. (b) is
adapted stepsize by USA. Each layer has a different stepsize based on the uncertainty.
0.002
0
Layer
(b) Uncertainty-based stepsize adaption
x
+×
vs.
U	sign(Vχlθ (x,y))	X0(UFGSM)	x0(FGSM)
π
n
Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with = 0.05. Starting from the clean
image x, We add the signed gradient sig∏(Vχlθ (x, y)) after rescaling it by the uncertainty over the
input gradient u, to generate the adversarial example x0(UFGSM). Note how UFGSM generated a
more natural image than FGSM (rightmost, u = 1).
trained model θ0 and rescale it using u, so that areas of higher variance get more perturbation. We
give an example of applying UFGSM to miniImageNet in Figure 2.
4	Experimental Evaluation
We train MAML on miniImageNet (Vinyals et al., 2016) training split; We then apply our method on
the resulting checkpoint. We evaluate our model on the test split of miniImageNet - for the same-
domain setting 一 as well as CUB-200-2011 (Welinder et al., 2010), Traffic Sign (HoUben et al., 2013)
and VGG Flower (Nilsback & Zisserman, 2008) 一 for the cross-domain setting. These datasets are
denoted as Mini, Birds, Signs and Flowers respectively. A desirable feature for an optimizer is
to maintain good performance in a broad range of stepsizes (Asi & Duchi, 2019). Therefore, we
evaluate our approach not only at the optimal stepsize, but also over a broad range of base stepsizes2
from 10-4 to 1. We evaluate the performance with three metrics: All, Top-1 and Top-40%.3 If two
methods have comparable Top-1 performance, but one has better Top-40% performance, then that
method is more robust to the choice of base stepsize. Detailed experimental setup and pretrained
model selection are included in Appendix A.3. Our code is available at https://github.com/
NamyeongK/USA_UFGSM/.
4.1	Main Results
Our main results are in Table 1. We compare the default MAML steps, which we take as our base-
line (denoted as SGD), with our full method (denoted as SGD+All), which consists of combining
2We determine the ranges of stepsize based on training performance on miniImageNet and keep it the same
for other datasets. We selected the minimum and maximum stepsize where performance decreased drastically.
3All is the average accuracy over all stepsizes. Top-1 is the accuracy of the best performing stepsize. Top-
40% is the average of the top 40% accuracies among all the stepsizes
6
Published as a conference paper at ICLR 2021
Table 1: Main results. We compare the default MAML steps (SGD) with our method (SGD+All) on
same-domain and cross-domain benchmarks.
Metric	Dataset	5-way 1-shot		5-way 5-shot		10-way 1-shot	
		SGD	SGD+All	SGD	SGD+All	SGD	SGD+All
	Mini	37.21±0.22	36.24±0.22	38.14±o.39	43.48±0.38	21.31±0.37	22.07±0.4
	Birds	30.07±0.52	29.83±0.46	33.29±0.29	38.07±0.33	17.54±0.06	17.81±0.12
All	Flowers	40.54±0.48	40.86±0.45	44.55±0.43	53.10±0.55	26.79±0.41	28.07±0.41
	Signs	34.19±0.76	34.76±0.61	45.39±0.45	51.02±0.5	23.77±0.13	24.88±0.13
	Avg.	35.50±0.49	35.42±0.44	40.34±0.39	46.42±0.44	22.35±0.24	23.21±0.26
	Mini	46.71±0.81	46.62±0.58	62.46±0.68	62.16±0.66	31.10±0.66	31.42±0.82
	Birds	36.03±0.48	36.43±0.58	50.71±0.54	51.82±0.64	23.16±0.21	23.61±0.39
Top-1	Flowers	51.97±0.79	54.36±1.04	70.63±0.76	74.52±0.69	38.61±0.49	40.68±0.58
	Signs	43.44±0.99	44.10±1.03	72.52±0.84	74.13±0.90	35.10±0.21	35.11±0.26
	Avg.	44.54±0.77	45.37±0.81	64.08±0.71	65.66±0.72	31.99±0.39	32.70±0.51
	Mini	45.07±0.52	45.79±0.44	55.38±0.46	60.70±0.91	28.65±0.64	30.56±0.77
	Birds	34.88±0.55	35.77±0.55	45.50±0.49	51.03±0.63	21.78±0.07	22.87±0.30
Top-40%	Flowers	49.98±0.69	53.16±0.73	65.16±0.73	73.55±0.86	36.14±0.53	39.75±0.61
	Traffic	41.81±1.02	43.39±1.01	67.34±0.43	71.45±0.90	33.56±0.11	34.27±0.17
	Avg.	42.94±0.70	44.52±0.68	58.34±0.53	64.18±0.82	30.03±0.34	31.86±0.46
USA, UFSGM, EnAug, and AT (λAT = λAug = 1, λα = 0 in Algorithm 1). In terms of abso-
lute performance (Top-1 row), our method outperforms the baseline on cross-domain tasks (Birds,
Flowers, Signs), while the performance is comparable on same-domain tasks (Mini). In terms of
robustness to the base stepsize (All and Top-40% rows), our method outperforms the baseline over a
large range of stepsizes for 5-way 5-shot and 10-way 1-shot tasks, while for 5-way 1-shot the results
are either comparable (All) or better (Top-40%) depending on the metric considered. More results
can be found in Appendix A.6.
4.2	Discussion
Ablation Study. We perform an ablation study for the 5-way 1-shot case in Table 2 and plot
the accuracy at different base stepsizes for the Flowers dataset in Figure 3. Comparing SGD to
SGD+AT in Table 2, we observe that adversarial training is beneficial over the baseline (Top1 and
Top-40%), except for large stepsizes with SGD, which is reflected in the All metric and in the dip in
performance in Figure 3a. Notice also how the use of AT flattens the accuracy curve near the optimal
stepsize. Comparing SGD to SGD+USA, we observe a very small but consistent improvement over
the baseline in the vicinity of the optimal stepsize (Top-1 and Top-40%). Comparing SGD+USA to
SGD+USA+UFGSM, we observe improvement over a wide range of stepsizes, which is reflected
in the curves and in All and Top-40% metrics. Comparing SGD+USA+UFGSM+EnAug+AT to
SGD+USA+UFGSM+EnAug shows that AT+EnAug is beneficial in terms of absolute performance
(Top-1) as well as creating robustness to the choice of stepsize (Top-40%). The drop for the All
metric is explained by the dip in the curve for the largest stepsizes. Overall, the best absolute
performance (Top-1) is always obtained through some use of adversarial training, while using the
full method consistently results in increased robustness to the choice of stepsize (best Top-40%
performance). Appendix A.5 shows various AT results.
UFGSM vs. FGSM. We compare UFGSM to FGSM (Goodfellow et al., 2015) in Table 3. The
results show that our uncertainty-based approach consistently outperforms FGSM, which suggests
that the uncertainty information extracted from the support set was useful.
SGD vs. Adam. Our method can be used with SGD and Adam, however, we have mainly fo-
cused on SGD throughout the paper. SGD tends to yield the best results (Top-1 and Top-40%),
as shown in Table 4 for the baseline and full method. More results for Adam can be found in
Sections A.6 and A.8 of the appendix.
Best checkpoint vs. Overfitted checkpoint. We also consider the problem of repurposing an
overfitted checkpoint. We find that our method improves both absolute performance and robustness
7
Published as a conference paper at ICLR 2021
Table 2: Ablation Study for 5-way 1-shot classification with SGD optimizer. SGD+AT corresponds
to using (λAT = λα = 1, λAug = 0) in Algorithm 1, SGD+USA to (λAT = λAug = λα = 0),
SGD+USA+UFGSM to (λAT = λα = 0, λAug = 1) and SGD+USA+UFGSM+EnAug+AT to
(λAT = λAug = 1, λα = 0).
Metric	Dataset	5-way 1-shot				
		SGD	SGD+AT	SGD+USA	SGD+USA +UFGSM	SGD+USA +UFGSM +EnAug+AT
	Mini	37.21±0.22	34.60±0.18	37.53±0.27	38.71±0.28	36.24±0.22
	Birds	30.07±0.52	28.81±0.45	30.39±0.53	31.07±0.50	29.83±0.46
All	Flowers	40.54±0.48	38.80±0.44	41.09±0.49	42.33±0.60	40.86±0.45
	Signs	34.19±0.76	33.42±0.59	35.02±0.82	36.10±0.48	34.76±0.61
	Avg.	35.50±0.49	33.91±0.41	36.01±0.36	37.05±0.27	35.42±0.44
	Mini	46.71±0.81	46.58±0.65	46.72±0.71	46.66±0.77	46.62±0.58
	Birds	36.03±0.48	36.40±0.48	36.08±0.49	36.02±0.48	36.43±0.58
Top-1	Flowers	51.97±0.79	54.55±0.84	52.01±0.77	52.30±0.77	54.36±1.04
	Signs	43.43±0.99	43.98±0.98	43.42±1.06	43.96±0.64	44.10±1.03
	Avg.	44.54±0.77	45.38±0.74	44.56±0.44	44.73±0.41	45.37±0.81
	Mini	45.07±0.52	45.53±0.50	45.23±0.59	45.76±0.62	45.79±0.44
	Birds	34.88±0.55	35.57±0.60	35.17±0.55	35.30±0.48	35.77±0.55
Top-40%	Flowers	49.98±0.69	52.92±0.68	50.39±0.71	51.07±0.67	53.16±0.73
	Signs	41.83±1.02	43.19±0.96	42.08±1.06	43.18±0.73	43.39±1.01
	Avg.	42.94±0.70	44.30±0.68	43.22±0.43	43.83±0.39	44.52±0.68
8
∞00 0
9。。°
寸。。
7。。
i
Ni.
Si.
i
II.
1852963374
5444333422
Ooooooo OO
Kɔeɪnoɔv
Stepsize
(b) With Adam optimizer
∞00 0
9。。°
寸。。
7。。
i
Ni.
Si.
i
II.
(a) With SGD optimizer
Figure 3: Ablation study for 5-way 1-shot classification on Flowers dataset.
Kɔeɪnoɔv

8
Published as a conference paper at ICLR 2021
Table 3:	Comparing UFGSM against
FGSM (Goodfellow et al., 2015) on Table 4: Comparing SGD vs. Adam for default MAML	
5-way 1-shot tasks. For all metrics steps (baseline) and our method (baseline+all) on 5-way and datasets, the proposed uncertainty- 1-shot classification.	
based method is better.	.	
		 、，，∙	ɪʌ .	.	Baseline	BaSeline+All
	SGD+USA	Metric DataSet	
Metric	Dataset __ SGD+US：	 SGD	Adam	SGD	Adam
		+UFGSM	+fgsΜ	Mini	37.21±o.22~31.87±0.20~36.24±o.22~33.57±o.29
	Mini	38.71 ±0.28	36.97±0.54	Birds	30.07±o,52	28.O2±o,39	29.83±o,46	29.51 ±o.46
	Birds	31.07±0.50	30.05±0.55	All	Flowers	40.54±0.48	40.33±0.57 40.86±0.45	44.58±0.70
All	Flowers	42.33±0.60	41.17±0.55	Signs	34.19±0.76	33.98±0.43	34.76±0.61	35.10±0.43
	Signs	36.10±o.48	34.78±o,5o		Avg.	35∙50±o∙49	33.55±o.22	35.42±o.44	35.69±o∙29
	Avg.	37.05±0.27	35.74±0.27	Mini	46.71 ±0.8i	43.O8±o,io 46.62±0.58	44.72±0.35
	Mini	46 66±077	43 93±o85	Birds	36.03±o.48	34.OO±o.63	36.43±0.58	35.45±o.67
	Birds	36.02±0.48	34.52±o..74	Top-1	Flowers	51.97±0.79	51.28±o.47	54.36±1.o4	56.04±1.14
Top-1	Flowers	52.30±0.77	5o.63±o.84	Signs	43.43±0.99	42.53±o.79	44.10±1.03	43.o1±o.85
	Signs	4396±0 64	42 62±ioo		Avg.	44S4+0.77 42.72±032	45.37±0.81	44.8θ±0.54
	Ag	44 73±.	42 93±.	Mini	45.07±o.52	4O.18±o.i4	45.79±o∙4t 42.92±o.35 Avg.	44.73±0.41~~42.93±θ.47	Birds	34.88±o.55	33.38±o.6i	35.77±o.55	34.54±o.66
	Mini	45.76±0.62	43.18±o.87	Top-4o% Flowers	49.98±0.69 48.18±o.44	53.16±o.73 54.08±0.76
	Birds	35.30±0.48	33.88±o.75	Signs	41.83±1.02	4o.9o±o.7o	43.39±1.01	42.26±o.77
Top-40%	Flowers	51.07±0.67	49.47±o.69	Avg.	42.94±0.70	4o.41±o.29	44.52±0.68	43.45±o.4o
	SiOnQ	43 18	4193 S				
	gns	.	±o.∕3	.	±o.9o		
	Avg.	43.83±0.39	42.11±o.46
to stepsize. In fact, the gains are more substantial on the overfitted (worse) checkpoint than the best
checkpoint (Appendix A.8).
Validating Proposal 1 and 2. Our method is built on the hypotheses that we should take small
stepsizes on high uncertainty parameters and add more adversarial perturbation on high uncertainty
input gradients. To validate those choices, we have tried taking large stepsizes on high uncertainty
parameters, and using less adversarial perturbation on high uncertainty input gradients, which re-
sulted in lower accuracy and robustness to the stepsize (Appendix A.7).
Freezing most uncertain layers. We observed that the batch normalization (BN) layers have the
most uncertainty (Figure 1). In Proposal 1, we argued that the higher uncertainty component ampli-
fies the error when we use bigger stepsize. As suggested by an anonymous reviewer, this motivates
an additional experiment that we perform where we “freeze” the BN layer during meta-testing (i.e.
we manually set the stepsize for the BN scale and shift parameters to zero). We performed the BN
layer freezing experiment on SGD+All (see detailed results in Table 10 and Figure 12 in Appendix
A.9). It turns out that SGD+All (w/ freezing BN) outperforms SGD+All (w/o freezing BN) on the
All metric, with most improvement in the higher stepsize range (>0.1). This is consistent with our
intuition that updating high-uncertainty layers (such as BN) with large stepsizes can be harmful.
5	Conclusion
In this paper we considered the novel problem of repurposing pretrained MAML checkpoints for
out-of-domain few-shot learning. Our method uses deep ensembles to estimate model parameter
and input gradient uncertainties over the support set, and builds upon the default MAML gradient
steps through the addition of uncertainty-based adversarial training and adaptive stepsizes. Our
experiments over popular few-shot benchmarks show that our method yields increased accuracy and
robustness to the choice of base stepsize. More generally, our results motivate the use of adversarial
learning as a data augmentation scheme for improving few-shot generalization. In the future, it
would be interesting to apply our method to related settings such as domain adaption and transfer
learning.
Acknowledgments
We thank Hugo Larochelle and Reza Babanezhad for insightful discussions, Damien Scieur and
Emmanuel Bengio for helpful feedback on the manuscript. We also thank the anonymous reviewers
for their comments and suggestions.
9
Published as a conference paper at ICLR 2021
This research was partially supported by the Canada CIFAR AI Chair Program, the NSERC Dis-
covery Grant RGPIN-2017-06936 and a Google Focused Research award. Simon Lacoste-Julien is
a CIFAR Associate Fellow in the Learning in Machines & Brains program.
References
Hilal Asi and John C. Duchi. The importance of better models in stochastic optimization. In National
Academy of Sciences, 2019.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. In Machine Learning, 2010.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, 2015.
Aymeric Dieuleveut, Alain Durmus, Francis Bach, et al. Bridging the gap between constant step size
stochastic gradient descent and markov chains. In Annals of Statistics. Institute of Mathematical
Statistics, 2020.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In International Conference on Learning Rep-
resentations, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, 2017.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, 2014.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. In International Conference on Learning Representa-
tions, 2018.
Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection
of traffic signs in real-world images: The german traffic sign detection benchmark. In Interna-
tional Joint Conference on Neural Networks, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint
arXiv:1412.6980, 2014.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017.
Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and
subspace. In International Conference on Machine Learning, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In International Conference on Learning Representations, 2018.
Seyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In Computer Vision and Pattern Recognition, 2016.
Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial
domain adaptation. In Advances in Neural Information Processing Systems, 2017.
10
Published as a conference paper at ICLR 2021
Maria Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.
Yoshio Nishi and Robert Doering. Handbook of semiconductor manufacturing technology. CRC
press, 2000.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations, 2017.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International Conference on Machine
Learning, 2016.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern
Recognition, 2018.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-
dataset: A dataset of datasets for learning to learn from few examples. In International Conference
on Learning Representations, 2020.
Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot
classification via learned feature-wise transformation. In International Conference on Learning
Representations, 2020.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Conference on Computer Vision and Pattern Recognition, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and
Pietro Perona. Caltech-ucsd birds 200. 2010.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-
independent weight perturbations on mini-batches. In International Conference on Learning
Representations, 2018.
Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. In ACM
Transactions on Intelligent Systems and Technology, 2020.
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial
examples improve image recognition. In Computer Vision and Pattern Recognition, 2020.
Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. In arXiv
preprint arXiv:1806.03316, 2018.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, 2014.
Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial
domain adaptation. In Computer Vision and Pattern Recognition, 2019.
An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji Rong Wen, and Ping
Luo. Domain-adaptive few-shot learning. In arXiv, 2020.
11
Published as a conference paper at ICLR 2021
A Appendix
A. 1 Notations and Acronyms
Notations and acronyms used repeatedly throughout the paper are summarized below.
Notation	Meaning
USA I/USA FGSM UFGSM I/UFGSM EnAug T M θ0 θtm θT α αadap DSpt DAug	Uncertainty-based stepsize adaption Inverse USA Fast gradient sign method optimization (Goodfellow et al., 2015) Uncertainty-based FGSM Inverse UFGSM Ensemble augmentation Number of test-time gradient steps Size of deep ensemble Pretrained MAML checkpoint to repurpose Ensemble element m at time t Model parameters after meta-testing Base stepsize Uncertainty-based stepsize (parameter-wise) Support set (test-time) Adversarial examples based on DSpt
A.2 Design choices for USA
Here, we introduce the design choices of the Algorithm 2.
Line 3: We calculate the maximum Max(u) and minimum Min(u) values among the estimated stan-
dard deviations u for each parameter. As proposal 1, in order to assign to low stepsize to high uncer-
tainty, the standard deviation of each parameter is subtracted from the maximum value Max(u) - u
for inverse-relationship. At this time, the largest value among parameter values becomes 0. To pre-
vent this, we add the smallest Max(u) - u + Min(u). Note that all values of u are non-negative. We
also tried alternatives ways like non-inverse-relationship and relative standard deviation, but it did
not seem to work better. To keep the same scale of the uncertainty before and after the transforma-
tion, we chose the inverse-relationship using Max(u) - u + Min(u).
Line 4: In this study, we use layer-wise adapted stepsize. In order to assign the same stepsize for
each layer, we calculate the average of the parameters of each layer and the values of the parameters
of the corresponding layer are replaced with the average value. It repeats all layers and performs the
corresponding operation.
Line 5: To more easily compare the effectiveness of USA with the baseline, we set the average
stepsize equals to the base stepsize, i.e. 1∕∣αadap∣ P ɑadap = α. If USA is applied without this
rescaling, than the average stepsize can change, which makes it difficult to distinguish the effect of
using an overall different stepsize (for constant SGD e.g.) vs. our adaptation of stepsizes.
A.3 Experimental setup and pretrained model selection
Our baseline model was trained using the same hyperparameters with miniImageNet training of
MAML except the inner loop stepsize. The inner loop stepsize was set to 0.1 for reproducing the
5-way 1-shot accuracy reported in the original MAML paper. We trained the model for 150,000 iter-
ations. A pretrained model was selected with the validation accuracy among miniImageNet training
checkpoints. We used the checkpoint with the highest validation accuracy as the pretrained model
θ0 . The highest test accuracy we achieved was 49.24% during meta-training. For the checkpoint
that we chose with the highest validation performance, the test accuracy was 47.58%. Note that the
cross-domain performance highly depended on which checkpoint we used. See Fig.4 in Appendix
A.4.
12
Published as a conference paper at ICLR 2021
The size of ensemble is M = 5 which is the same as the deep ensembles (Lakshminarayanan
et al., 2017). The parameter for the FGSM is = 0.05. The scale value a of Gaussian random
perturbation for ensemble model training is σ = 0.05. The gradient step is T = 10 which is same
with miniImageNet test of MAML.
We do not split those datasets except miniImageNet, because we do not use the datasets in the meta-
training. We used the same splits as Ravi & Larochelle (2017) for miniImageNet. Tseng et al. (2020)
used a randomly split manner for the cross-domain test and Triantafillou et al. (2020) used all traffic
signs dataset for the test.
A.4 Cross-domain accuracy while meta-training MAML on miniImageNet
The performance of the cross-domain significantly differed depending on the selected checkpoint.
Figure 4 shows the performance for every 200 iterations. We selected the checkpoint based on
miniImageNet validation performance, which is the highest at the 57,200 iteration.
0.55
0.54
0.53
0.52
0.51
0.5
0.49
0.48
0.47
0.46
0.45
0.44
0.43
214987654
.440.333333
0.0.0.0.0.0.0.0.
0导号
。导 8ET
OomT
OoVC
00Z9ZT
OoZZZl
OOO8iτ
Ooos^
OOofnT
008goT
008IOT
008Z6
009E6
00968
009S8
009T8
00寸工
00 寸 mz
§s
Oozgg
息19
OoZz.6
OoOES
Ooos
OooS 寸
00§
0089m
008Zm
0088Z
。屋Z
009。Z
00991
00 寸 Zl
0导8
0?
OoZ
Figure 4:	Cross-domain performance while meta-training on miniImageNet. X-axis is training iter-
ation on meta-training on miniImageNet and y-axis is classification accuracy.
A.5 Additional results for AT-only setting.
To evaluate the effect of AT, we only applied AT in MAML (λα = λAT = 1, λAug = 0). Table 5,
Figure 5 and 6 show AT results on 5-way 1-shot with SGD and Adam. Adam is worse than SGD in
the results. However, we found that AT and Adam are good combination for a meta-test training.
A.6 Additional results for 5-way 5-shot and 10-way 1-shot classification.
We present some additional results for 5-way 5-shot and 10-way 1-shot classification in this section.
Table 6 shows the 5-way 5-shot and 10-way 1-shot classification results. Especially in the case of
the Top-40% Avg our method significantly increased the performance about 5.84% and 1.83% in 5-
way 5-shot and 10-way 1-shot respectively. It means that it can increase the probability to select of
13
Published as a conference paper at ICLR 2021
Table 5: 5-way 1-shot classification results. In order to verify the effectiveness of AT, we applied
only AT among the proposed method and the all proposed method.
Metric	Dataset	Baseline SGD	Ours (SGD)		Baseline Adam	Ours (Adam)	
			AT Only	All		AT Only	All
	Mini	37.21±o.22	34.60±0.18	36.24±0.22	31.87±0∙2o	33.40±0.33	33.57±0.29
	Birds	30.07±O∙52	28.81±0.45	29.83±0.46	28.02±o∙39	29.37±0.51	29.51±0.46
All	Flowers	40.54±0.48	38.80±0.44	40.86±0.45	40.33±o∙57	44.53±0.68	44.58±0.70
	Signs	34.18±0.76	33.42±0.59	34.76±0.61	33.98±o.43	34.96±0.47	35.10±0.43
	Avg.	35.50±o.49	33.91±0.41	35.42±o.44	33.55±0.22	35.57±0.29	35.69±0.29
	Mini	46.71±o.8i	46.58±0.65	46.62±o.58	43.08±0.10	44.80±0.40	44.72±0.35
	Birds	36.03±o.48	36.40±0.48	36.43±0.58	34.00±o.63	35.28±0.66	35.45±0.67
Top-1	Flowers	51.97±o.79	54.55±0.84	54.36±1.04	51.28±o.47	56.10±1.06	56.04±1.14
	Signs	43.44±o.99	43.98±0.98	44.10±1.03	42.53±o.79	43.13±0.83	43.01±0.85
	Avg.	44.54±o.77	45.38±0.74	45.37±0.8i	42.72±o.32	44.83±0.51	44.80±0.54
	Mini	45.07±o.52	45.53±0.50	45.79±m	40.18±o.i4	42.59±0.43	42.92±0.35
	Birds	34.88±0.55	35.57±0.60	35.77±0.55	32.38±o.6i	34.26±0.73	34.54±0.66
Top-40%	Flowers	49.98±o.69	52.92±0.68	53.16±0.73	48.18±o.44	53.88±0.72	54.08±0.76
	Signs	41.81±i.02	43.19±0.96	43.39±1.01	40.90±o.7o	42.06±0.74	42.26±0.77
	Avg.	42.94±o.7o	44.30±0.68	44.52±0.68	40.41±o∙29	43.20±0.42	43.45±0.40
64248642386422
444λ3333λ2222λ
Ooo Oooo Oooo
6 2 8 4
3 3 2 2
. . . ______ .
Oooo
4 2 3 8 6 4 2
3 302 2 2 2
a a QQQa
AaBJnOOV
I
l~.0
0
1.0
0
MTO
10.0
00
0
000
X。。.。
I000.0
l~.0
1.0
l~∙0
10.0
c。。.。
l~00°0
X。。.。
I000.0
I
l~.0
0
1.0
0
WTO
•
10.0
coo M
"00.0
100.0
000
000
IOOO.0
864248642386422
4444a3333a2222a
6666 6666 6666
I
■0
MTO

≡°0
2
a
(a) Mini	(b) Birds	(c) Flowers	(d) Signs
Figure 5:	5-way 1-shot classification results with AT on SGD.
optimal stepsize for a new task. As shown in Figure 7, there are more flatten curve near the highest
accuracy in both tasks.
A.7 Validating proposals 1 and 2 with “Inverse” counterparts.
To validate proposal 1, we evaluate the inverse strategy of USA, denoted I/USA, which assigns
higher stepsizes to layers with higher uncertainty. Specifically, we flip the stepsizes by replacing line
3 of Algorithm 2 with C J u. To measure the effect ofUSA, We only applied USA in MAML (λα =
λAT = λAug = 0). The results for 5-way 1-shot classification are in Table 7. USA outperforms the
baselines over broad ranges of stepsizes compared to Adam and SGD. I/USA significantly decreased
the performance of all metrics. Therefore, USA reflected useful knowledge well into the stepsize for
a new task. See accuracy by stepsizes in Fig 8. Figure 8 shows the results for verifying of Proposal
1. USA shows more flatten curve in Top-1 accuracy ranges than SGD and I/USA. I/USA degraded
the performance through all ranges of stepsizes.
For verifying the proposal 2, we evaluated three methods UFGSM , I/UFGSM and FGSM on USA.
I/UFGSM is inversely implemented method of UFGSM. FGSM uses examples generated by FGSM
instead of UFGSM. To measure the effect of UFGSM, we only applied UFGSM in MAML with
USA (λα = λAT = 0, λAug = 1). Table 7 shows 5-way 1-shot classification results. UFGSM
outperforms all the other methods for every metric. In spite of I/UFGSM had stronger adversar-
ial perturbation than UFGSM, the performance was worse than UFGSM. Note that I/UFGSM and
FGSM showed almost similar performances. The reason is that most of the input pixels have an un-
certainty close to 0; therefore, when scaling after inverse, most pixels have a value of 1. Therefore,
the generated examples are almost similar to the adversarial example in which FGSM is applied.
See accuracy by stepsizes in Fig 9. Figure 9 shows the result for verifying of proposal 2. UFGSM
14
Published as a conference paper at ICLR 2021
0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
(a) Mini
0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
Stepsize
(b) Birds
0.57
0.54
0.51
0.48
0.45
0.42
0.39
0.36
0.33
0.3
0.27
0.24
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
(c) Flowers	(d) Signs
Figure 6:	5-way 1-shot classification results with AT on Adam.
Table 6: 5-way 5-shot and 10-way 1-shot classification results using SGD. Our method outperformed
both tasks. Especially 5-way 5-shot classification performance significantly improved than baseline.
Metric	Dataset	5-way 5-shot			10-way 1-shot		
		Baseline SGD	Ours (SGD)		Baseline SGD	Ours (SGD)	
			SGD+AT	SGD+All		SGD+AT	SGD+All
	Mini	38.14±0.39	41.06±0.35	43.48±o.38	21.31±0.37	20.97±0.37	22.07±0.40
	Birds	33.29±o.29	36.04±0.28	38.07±0.33	17.54±o.o6	17.08±0.07	17.81±0.12
All	Flowers	44.55±o.43	48.94±0.48	53.10±0.55	26.79±o.4i	26.50±0.42	28.07±0.41
	Signs	45.39±o.45	47.98±0.55	51.02±0.50	23.77±o.i3	23.99±0.11	24.88±0.13
	Avg.	40.34±o.39	43.51±0.41	46.42±o.44	22.35±o.24	22.14±0.24	23.21±0.26
	Mini	62.46±o.68	62.07±0.61	62.16±o.66	31.10±o.66	31.18±0.81	31.42±0.82
	Birds	50.71±o∙54	51.93±0.62	51.82±0.64	23.16±0.2i	23.42±0.37	23.61±0.39
Top-1	Flowers	70.63±o.76	74.14±0.72	74.52±0.69	38.61±o.49	40.84±0.58	40.68±0.58
	Signs	72.52±o.84	73.66±0.98	74.13±0.90	35.10±o.2i	34.76±0.17	35.11±0.26
	Avg.	64.08±o.7i	65.45±0.73	65.66±0.72	31.99±o∙39	32.55±0.48	32.70±0.51
	Mini	55.38±o.46	59.97±0.69	60.70±o.9i	28.65±o.64	30.22±0.74	30.56±0.77
	Birds	45.50±o.49	50.48±0.53	51.03±0.63	21.78±o.07	22.61±0.22	22.87±0.30
Top-40%	Flowers	65.16±o.73	72.61±0.78	73.55±0.86	36.14±o.53	39.40±0.66	39.75±0.61
	Traffic	67.34±o.43	70.56±0.87	71.45±0.90	33.56±o.ii	34.21±0.13	34.27±0.17
	Avg.	58.34±o∙53	63.40±0.72	64.18±0.82	30.03±o.34	31.61±0.44	31.86±0.46
shows flatter curve near the best performing accuracy than SGD, I/UFGSM and FGSM. FGSM and
I/UFGSM show very similar trends. We explained the reason why the two methods gave similar
results. As can be seen in Fig 10, almost all of the input gradient uncertainty is near zero. When
we inverse the value, almost all the value is 1 (See Fig 11). UFGSM improves performance despite
less adversarial perturbation than I/UFGSM and FGSM. Through this, even a small change can help
model learning if correct (useful) information is reflected.
A.8	5-way 5-shot classification results on overfitted checkpoint
As can seen in Appendix A.4, overfitted models on miniImageNet degraded performance over all
datasets. We investigated the 5-way 5-shot classification results on the checkpoint after meta-
training with MAML for 150K iterations. Table 8 and 9 show the results with SGD and Adam.
The number in the parentheses is the difference from the baseline for each checkpoint (e.g. SGD or
Adam). The results using the (Last) checkpoint are worse than (Validation) checkpoint due to the
overfitting. However the performance boost from using our method is more substantial on the over-
fitted (Last) checkpoint than the best (Validation) checkpoint, both in terms of absolute performance
(Top-1) and robustness to choice of base stepsize (other metrics).
A.9 Freezing most uncertain layers.
Since some of the BatchNorm parameters have the highest uncertainty (see Figure 1a), we have
experimented freezing the BatchNorm parameters during meta-testing (i.e. setting their stepsize to
zero). It turns out that SGD+All (w/ freezing BN) outperforms SGD+All (w/o freezing BN) on the
All metric (Table 10), with most improvement in the higher stepsize range (>0.1) (Figure 12). We
15
(a) Mini
(b) BirdS
(C) FIOWerS
(d) SignS
Figures'way'ShOt(ToP) and 1'Way 1—shot (BottOm) ClaSSifiCation With MAML(SGD) and OUr
PrOPOSed method，！！ terms Of robustnes-OUrmethOd OUtPerformS MAML(SGD) more effectively.
IIl additiony We ShoW that OUr method OUtPerfOrmS τoi accuracy for BirdFilers and SigiIs.
AISO OUrmethod ShoWS Hatter CUrVe Ilear highest accuracy SeCtiOns，ThiSinCreaSeS the PrObabiHty
Of OPtimal StePSiZe SeleCtn∙
OOOO	O O
g	2	2	"	"	g	4	H
2	4	8	2	6	4	4	s
0.0001
0.0004
0.0007
0.001
0.004
0.007
0.01
0.04
0.07
Accuracy
(b) BirdS
(C) FIOWerS
0.0001
0.0004
0.0007
0.001
0.004
0.007
0.01
0.04
0.07
(d) SignS
Figure00∙way lihsCIaSSifiCat5'IIfOr Verify5'g PrOPOSalL We COmPared to USA and I∕USA∙
IIOtiCe a SimiIar trend for'Way'shot SGDy WhereSGD (W/ freezing BN) also OUtPerformS SGD
(w∕0 freezing BN) on the A= metricy With most improvementthe higher StePSiZe rang
16
OOOO OOOO O
P i∙j i∙j ω ω P ⅛ ⅛ lz∣ lz∣ P ⅛∖
2 48 2 6448 2 664
PUbliShed as a ConferenCe PaPersICLR 2021
Published as a conference paper at ICLR 2021
Table 7: 5-way 1-shot classification results for verifying proposal 1 and 2. I/USA and I/UFGSM
degraded performance than USA and UFGSM respectively. Our approach which is applied USA
and UFGSM outperformed SGD on almost of the metrics.
All
Metric
Top-1
Top-40%
Dataset	Baseline	
	SGD	Adam
Mini	37.21	31.85
Birds	30.07	28.17
Flowers	40.54	40.49
Signs	34.21	34.28
Avg.	35.50	33.68
Mini	46.71	43.07
Birds	36.03	34.30
Flowers	51.97	51.38
Signs	43.44	43.00
Avg.	44.54	42.93
Mini	45.07	40.16
Birds	34.88	32.64
Flowers	49.98	48.28
Signs	41.81	41.32
Avg.	42.94	40.60
(a) Mini
SGD		+UFGSM
+USA	+I/USA	
37.53	28.73	-38.71
30.39	24.98	31.07
41.09	30.52	42.33
35.02	27.90	36.10
36.01	28.03	37.05
46.72	44.13	-46.66
36.08	34.20	36.02
52.01	49.34	52.30
43.42	41.03	43.96
44.56	42.17	44.73
45.23	38.33	-45.76
35.17	30.28	35.30
50.39	42.19	51.07
42.08	35.78	43.18
43.22	36.64	43.83
SGD+USA
+I/UFGSM +FGSM
8642386422
3333λ2222λ
■ ■ ■ ■ - - ■ ■ ■ ■ --
Oooo Oooo
AaBJnOOV
SgDGD
-o-+UFGSM
-×→I/UFGSM
+FGSM
I
9.0
0
0
g.0
8息
x0
6000.0
sooo
I。。。.。
(c) Flowers
(b) Birds
37.25	36.97
30.27	30.05
41.44	41.17
34.98	34.78
35.98	35.74
44.31	43.93
34.77	34.52
50.90	50.63
42.82	42.62
43.20	42.93
43.60	43.18
34.20	33.88
49.79	49.47
42.17	41.92
42.44	42.11
I
l~.0
0
1.0
0
WTO
Io∙o
go
0
000
X。。.。
I000.0
(d) Signs
Figure 9:	5-way 1-shot classification results to verify proposal 2. We compared to UFGSM,
I/UFGSM and FGSM.
20	40 æ 80
(a) Uncertainty map (u)
(b) Flatten uncertainty
(c) Histogram of uncertainty
Figure 10:	(a) shows the re-scaled input gradient uncertainty to generate the UFGSM example. (b)
is a flatten plot of the re-scaled input gradient uncertainty. (c) shows a histogram of the uncertainty.
17
Published as a conference paper at ICLR 2021
(a) Uncertainty map (u)
(b) Flatten uncertainty
(c) Histogram of uncertainty
Figure 11:	(a) shows the re-scaled inverse input gradient uncertainty to generate the UFGSM exam-
ple. (b) is a flatten plot of the re-scaled inverse input gradient uncertainty. (c) shows a histogram of
the uncertainty.
Table 8: 5-way 5-shot classification results between selected checkpoint with SGD
Metric	Dataset	Checkpoint (Validation)			Checkpoint (Last)		
		Baseline SGD	Ours (SGD)		Baseline SGD	Ours (SGD)	
			AT Only	All		AT Only	All
	Mini	38.14±0.39	41.06±0.35	43.48±o.38	34.98±o.54	38.66±0.52	41.89±0.61
		G)	(2.92)	(5.34)	(-)	(3.68)	(6.91)
	Birds	33.29±o.29	36.04±0.28	38.07±0.33	31.33±o.5i	34.34±0.49	36.90±0.5
		(-)	(2.75)	(4.78)	(-)	(3.01)	(5.57)
All	Flowers	44.55±o.43	48.94±0.48	53.10±0.55	40.96±o.67	45.6±0.6	50.58±0.67
		(-)	(4.39)	(8.55)	(-)	(4.64)	(9.62)
	Signs	45.39±o.45	47.98±0.55	51.02±0.50	42.05±o.35	46.01±0.36	49.88±0.35
		(-)	(2.59)	(5.63)	(-)	(3.96)	(7.83)
	Avg.	40.34±o.39	43.51±0.41	46.42±0.44	37.33±o.52	41.15±0.49	44.81±0.53
		G)	(3.17)	(6.08)	(-)	(3.82)	(7.48)
	Mini	62.46±o.68	62.07±0.61	62.16±o.66	58.49±o.72	58.44±0.93	58.72±1.08
		(-)	(-0.39)	(-0.3)	(-)	(-0.05)	(0.23)
	Birds	50.71±o∙54	51.93±0.62	51.82±0.64	46.77±o.53	48.33±0.72	48.58±0.7
		(-)	(1.22)	(1.11)	(-)	(1.56)	(1.81)
Top-1	Flowers	70.63±o.76	74.14±0.72	74.52±0.69	66.31±i.02	68.99±0.81	69.73±0.71
		(-)	(3.51)	(3.89)	(-)	(2.68)	(3.42)
	Signs	72.52±o.84	73.66±0.98	74.13±0.90	67.75±o.57	70.71±0.48	71.08±0.53
		(-)	(1.14)	(1.61)	(-)	(2.96)	(3.33)
	Avg.	64.08±o.7i	65.45±0.73	65.66±0.72	59.83±o.7i	61.62±0.73	62.03±0.75
		(-)	(1.37)	(1.58)	G)	(1.79)	(2.2)
	Mini	55.38±o.46	59.97±0.69	60.70±o.9i	50.56±o.72	56.44±0.87	57.22±1
		(-)	(4.59)	(5.32)	(-)	(5.88)	(6.66)
	Birds	45.50±o.49	50.48±0.53	51.03±0.63	42.13±o.55	47.22±0.73	47.98±0.73
		(-)	(4.98)	(5.53)	(-)	(5.09)	(5.85)
Top-40%	Flowers	65.16±o.73	72.61±0.78	73.55±0.86	59.8±i.02	67.37±0.83	68.93±0.79
		(-)	(7.45)	(8.39)	(-)	(7.57)	(9.13)
	Signs	67.34±o.43	70.56±0.87	71.45±0.90	61.95±o.66	68.03±0.64	69.41±0.5
		(-)	(3.22)	(4.11)	(-)	(6.08)	(7.46)
	Avg.	58.34±o∙53	63.40±0.72	64.18±0.82	53.61±o.74	59.77±0.77	60.89±0.76
		(-)	(5.06)	(5.84)	(-)	(6.16)	(7.28)
18
Published as a conference paper at ICLR 2021
All
Table 9: 5-way 5-shot classification results of selected checkpoint with Adam
Metric
Top-1
Dataset	Checkpoint (Validation)			Checkpoint (Last)		
	Baseline Adam	Ours (Adam)		Baseline Adam	Ours (Adam)	
		AT Only	All		AT Only	All
Mini	40.08±i.33	41.38±1.07	41.64±o.97	37.25±o.64	39.47±0.56	40.36±0.62
Birds	G) 36.68±i.05	(1.3) 38.79±1	(1.56) 38.78±0.91	(-) 35.39±o.67	(2.21) 37.86±0.72	(3.1) 38.15±0.69
Flowers	G) 58.00±i.55	(2.11) 60.92±1.24	(2.10) 60.36±1.14	(-) 54.36±o.79	(2.47) 59.09±0.63	(2.76) 59.32±0.69
Signs	(-) 52.65±o.57	(2.92) 52.37±0.6	(2.36) 52.53±0.64	(-) 50.48±o.6	(4.73) 51.13±0.45	(4.96) 51.95±0.49
Avg.	(-) 46.85±i.i2	(-0.28) 48.37±0.98	(-0.12) 48.33±0.91	(-) 44.37±o.67	(0.65) 46.89±0.59	(1.47) 47.45±0.62
	(-)	(1.52)	(1.48)	(-)	(2.52)	(3.08)
Mini	58.89±o.73	60.35±0.83	60.08±o.83	55.25±i.02	57.79±0.81	57.86±0.92
Birds	G) 47.92±o.75	(1.46) 50.88±0.66	(1.19) 50.84±0.75	(-) 46.02±o.85	(2.54) 49.89±1	(2.61) 49.78±0.97
Flowers	(-) 73.21±i.i9	(2.96) 77.51±0.94	(2.92) 77.45±0.89	(-) 69.09±o.77	(3.87) 76.61±0.51	(3.76) 76.51±0.71
Signs	G) 70.98±i.07	(4.3) 71.30±1.52	(4.24) 71.72±1.52	(-) 67.51±o.72	(7.52) 69.75±0.83	(7.42) 70.30±0.79
Avg.	(-) 62.75±o.93	(0.32) 65.01±0.99	(0.74) 65.02±1	(-) 59.47±o.84	(2.24) 63.51±0.79	(2.79) 63.61±0.85
	G)	(2.26)	(2.27)	(-)	(4.04)	(4.14)
Top-40%
Mini
Birds
Flowers
Signs
Avg.
5864248642386422
Q4444633336n6
Oooo Oooo Oooo
(a) Mini
28446284
54413322
0
Ooo Oooo
AaBJnOOV
52.17±1.89
(-)
43.88±1.06
(-)
68.43±1.76
(-)
66.85±0.78
(-)
57.83±1.37
(-)
54.96±1.18
(2.79)
47.93±0.91
(4.05)
73.86±1.02
(5.43)
67.36±1.3
(0.51)
61.03±1.1
(3.2)
64238642
33312222
0 . . . .
Ooo Oooo
AaBJnOOV
I
to
0
I∙o
0
MTO
10.0
c。。.。
900
100.0
000
9000
I。。。
Stepsize
w/o BN freezing
-X-w/ BN freezing
/
(
Γ~i
"00
I00.0
000
"ooo
1000.0
Stepsize
(b) Birds
55.75±1.07	46.95±o.82
(3.58)	(-)
48.26±0.87	41.41±o∙8
(4.38)	(-)
74.03±1.02	63.44±o.77
(5.60)	(-)
67.89±1.39	62.95±o.8i
(1.04)	(-)
61.48±1.09	53.69±o.8
(3.65)	(-)
51.81±0.71	53.29±0.76
(4.86)	(6.34)
46.71±0.96	47.11±0.88
(5.3)	(5.7)
72.41±0.79	72.56±0.64
(8.97)	(9.12)
65.43±0.68	66.37±0.71
(2.48)	(3.42)
59.09±0.78	59.83±0.75
(5.4)	(6.14)
8628466284462842
67∙∙∙α∙J44633256
Oooo Oooo Oooo
(c) Flowers
(d) Signs
Figure 12: 5-way 1-shot (Top) and 5-way 5-shot (Bottom) classification results of freezing BN
layers. We tested freezing BN on SGD+All. SGD+All (w/ BN freezing) outperformed SGD+All
(w/o BN freezing) in the higher stepsize range (>0.1).
19
Published as a conference paper at ICLR 2021
Table 10: 5-way 1-shot and 5-way 5-shot classification results of freezing BN layers. We tested
freezing BN on our SGD+All. SGD+All (w/ freezing BN) outperformed in All metric on 5-way
1-shot and 5-way 5-shot.
Metric	Dataset	5-way 1-shot (SGD+All)		5-way 5-shot (SGD+All)	
		w/ freezing BN	w/o freezing BN	w/ freezing BN	w/o freezing BN
	Mini	37.13±0.24	36.24±0.22	45.42±0.42	43.48±0.38
	Birds	30.74±0.50	29.83±0.46	40.29±0.47	38.07±0.33
All	Flowers	43.88±0.56	40.86±0.45	59.41±0.68	53.10±0.55
	Signs	36.37±0.67	34.76±0.61	54.65±0.56	51.02±0.50
	Avg.	37.03±0.49	35.42±0.44	49.94±0.54	46.42±0.44
	Mini	46.48±0.54	46.62±0.58	62.18±0.66	62.16±0.66
	Birds	36.41±0.59	36.43±0.58	51.90±0.73	51.82±0.64
Top-1	Flowers	54.16±0.80	54.36±1.04	74.59±0.79	74.52±0.69
	Signs	44.18±1.09	44.10±1.03	73.98±0.88	74.13±0.90
	Avg.	45.31±0.76	45.37±0.81	65.66±0.76	65.66±0.72
	Mini	45.44±0.48	45.79±m	60.62±0.66	60.70±0.91
	Birds	35.67±0.54	35.77±0.55	51.04±0.73	51.03±0.63
Top-40%	Flowers	52.97±0.71	53.16±0.73	73.56±0.73	73.55±0.86
	Traffic	43.38±0.99	43.39±1.01	71.21±0.79	71.45±0.90
	Avg.	44.37±0.68	44.52±0.68	64.11±0.73	64.18±0.82
20