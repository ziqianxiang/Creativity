Published as a conference paper at ICLR 2021
Targeted Attack against Deep Neural Net-
works via Flipping Limited Weight Bits
JiaWang Bai 1，2 L Baoyuan Wu 3，4, Yong Zhang 5, Yiming Li 1, Zhifeng Li 5, Shu-Tao Xia 1，2
1	Tsinghua Shenzhen International Graduate School, Tsinghua University
2	PCL Research Center of Networks and Communications, Peng Cheng Laboratory
3	School of Data Science, The Chinese University of Hong Kong, Shenzhen
4	Secure Computing Lab of Big Data, Shenzhen Research Institute of Big Data
5	Tencent AI Lab
Ab stract
To explore the vulnerability of deep neural networks (DNNs), many attack
paradigms have been well studied, such as the poisoning-based backdoor attack
in the training stage and the adversarial attack in the inference stage. In this pa-
per, we study a novel attack paradigm, which modifies model parameters in the
deployment stage for malicious purposes. Specifically, our goal is to misclassify
a specific sample into a target class without any sample modification, while not
significantly reduce the prediction accuracy of other samples to ensure the stealth-
iness. To this end, we formulate this problem as a binary integer programming
(BIP), since the parameters are stored as binary bits (i.e., 0 and 1) in the mem-
ory. By utilizing the latest technique in integer programming, we equivalently
reformulate this BIP problem as a continuous optimization problem, which can be
effectively and efficiently solved using the alternating direction method of mul-
tipliers (ADMM) method. Consequently, the flipped critical bits can be easily
determined through optimization, rather than using a heuristic strategy. Extensive
experiments demonstrate the superiority of our method in attacking DNNs. The
code is available at: https://github.com/jiawangbai/TA-LBF.
1 Introduction
Due to the great success of deep neural networks (DNNs), its vulnerability (Szegedy et al., 2014;
Gu et al., 2019) has attracted great attention, especially for security-critical applications (e.g., face
recognition (Dong et al., 2019) and autonomous driving (Eykholt et al., 2018)). For example, back-
door attack (Saha et al., 2020; Xie et al., 2019) manipulates the behavior of the DNN model by
mainly poisoning some training data in the training stage; adversarial attack (Goodfellow et al.,
2015; Moosavi-Dezfooli et al., 2017) aims to fool the DNN model by adding malicious perturba-
tions onto the input in the inference stage.
Compared to the backdoor attack and adversarial attack, a novel attack paradigm, dubbed weight
attack (Breier et al., 2018), has been rarely studied. It assumes that the attacker has full access to
the memory of a device, such that he/she can directly change the parameters of a deployed model
to achieve some malicious purposes (e.g., crushing a fully functional DNN and converting it to a
random output generator (Rakin et al., 2019)). Since weight attack neither modifies the input nor
control the training process, both the service provider and the user are difficult to realize the existence
of the attack. In practice, since the deployed DNN model is stored as binary bits in the memory,
the attacker can modify the model parameters using some physical fault injection techniques, such
as Row Hammer Attack (Agoyan et al., 2010; Selmke et al., 2015) and Laser Beam Attack (Kim
et al., 2014). These techniques can precisely flip any bit of the data in the memory. Some previous
works (Rakin et al., 2019; 2020a;b) have demonstrated that it is feasible to change the model weights
via bit flipping to achieve some malicious purposes. However, the critical bits are identified mostly
tThis work was done when JiaWang Bai was an intern at TenCent AI Lab.
Correspondence to: Baoyuan Wu (wubaoyuan@cuhk.edu.cn) and Shu-Tao Xia (xiast@sz.tsinghua.edu.cn).
1
Published as a conference paper at ICLR 2021
0
1
0
1
0
1
1
0
1
0
1
1
0
1
1
0
0
1
1
0
0
0
0
0
0
1
1
0
0
1
0
0
1
1
1
0
0
1
0
0
1
0
0
0
0
1
1
0
1
1
0
1
1
0
1
0
1
1
0
0
1
0
1
1
1
1
1
0
0
1
0
1
0
1
0
0
0
0
1
0
0
1
1
0
1
1
1
1
1
0
1
0
0
1
0
0
1
1
0
1
1
1
1
1
0
0
0
0
1
0
0
0
1
0
1
0
1
1
1
0
Attacker: identify and
flip critical bits
DNN in the
memory
Other Samples
A Specific Sample Other Samples
Behave Normally
A Specific Sample
Do not modify SamPIeS Classified into
the target class	Behave Normally
Figure 1: Demonstration of our proposed attack against a deployed DNN in the memory. By flipping
critical bits (marked in red), our method can mislead a specific sample into the target class without
any sample modification while not significantly reduce the prediction accuracy of other samples.
using some heuristic strategies in their methods. For example, Rakin et al. (2019) combined gradient
ranking and progressive search to identify the critical bits for flipping.
This work also focuses on the bit-level weight attack against DNNs in the deployment stage, whereas
with two different goals, including effectiveness and stealthiness. The effectiveness requires that
the attacked model can misclassify a specific sample to a attacker-specified target class without
any sample modification, while the stealthiness encourages that the prediction accuracy of other
samples will not be significantly reduced. As shown in Fig. 1, to achieve these goals, we propose
to identify and flip bits that are critical to the prediction of the specific sample but not significantly
impact the prediction of other samples. Specifically, we treat each bit in the memory as a binary
variable, and our task is to determine its state (i.e., 0 or 1). Accordingly, it can be formulated as a
binary integer programming (BIP) problem. To further improve the stealthiness, we also limit the
number of flipped bits, which can be formulated as a cardinality constraint. However, how to solve
the BIP problem with a cardinality constraint is a challenging problem. Fortunately, inspired by
an advanced optimization method, the 'p-box ADMM (WU & Ghanem, 2018), this problem can be
reformulated as a continuous optimization problem, which can further be efficiently and effectively
solved by the alternating direction method of mUltipliers (ADMM) (Glowinski & Marroco, 1975;
Gabay & Mercier, 1976). ConseqUently, the flipped bits can be determined throUgh optimization
rather than the original heUristic strategy, which makes oUr attack more effective. Note that we
also condUct attack against the qUantized DNN models, following the setting in some related works
(Rakin et al., 2019; 2020a). Extensive experiments demonstrate the sUperiority of the proposed
method over several existing weight attacks. For example, oUr method achieves a 100% attack
sUccess rate with 7.37 bit-flips and 0.09% accUracy degradation of the rest Unspecific inpUts in
attacking a 8-bit qUantized ResNet-18 model on ImageNet. Moreover, we also demonstrate that the
proposed method is also more resistant to existing defense methods.
The main contribUtions of this work are three-fold. 1) We explore a novel attack scenario where the
attacker enforces a specific sample to be predicted as a target class by modifying the weights of a
deployed model via bit flipping withoUt any sample modification. 2) We formUlate the attack as a
BIP problem with the cardinality constraint and propose an effective and efficient method to solve
this problem. 3) Extensive experiments verify the sUperiority of the proposed method against DNNs
with or withoUt defenses.
2	Related Works
Neural Network Weight Attack. How to pertUrb the weights of a trained DNN for malicioUs
pUrposes received extensive attention (LiU et al., 2017a; 2018b; Hong et al., 2019). LiU et al.
(2017a) firstly proposed two schemes to modify model parameters for misclassification withoUt
and with considering stealthiness, which is dUbbed single bias attack (SBA) and gradient descent
2
Published as a conference paper at ICLR 2021
attack (GDA) respectively. After that, Trojan attack (Liu et al., 2018b) was proposed, which injects
malicious behavior to the DNN by generating a general trojan trigger and then retraining the model.
This method requires to change lots of parameters. Recently, fault sneaking attack (FSA) (Zhao
et al., 2019) was proposed, which aims to misclassify certain samples into a target class by modify-
ing the DNN parameters with two constraints, including maintaining the classification accuracy of
other samples and minimizing parameter modifications. Note that all those methods are designed
to misclassify multiple samples instead of a specific sample, which may probably modify lots of
parameters or degrade the accuracy of other samples sharply.
Bit-Flip based Attack. Recently, some physical fault injection techniques (Agoyan et al., 2010;
Kim et al., 2014; Selmke et al., 2015) were proposed, which can be adopted to precisely flip any bit
in the memory. Those techniques promote researchers to study how to modify model parameters at
the bit-level. As a branch of weight attack, the bit-flip based attack was firstly explored in (Rakin
et al., 2019). It proposed an untargeted attack that can convert the attacked DNN to a random
output generator with several bit-flips. Besides, Rakin et al. (2020a) proposed the targeted bit Trojan
(TBT) to inject the fault into DNNs by flipping some critical bits. Specifically, the attacker flips the
identified bits to force the network to classify all samples embedded with a trigger to a certain
target class, while the network operates with normal inference accuracy with benign samples. Most
recently, Rakin et al. (2020b) proposed the targeted bit-flip attack (T-BFA), which achieves malicious
purposes without modifying samples. Specifically, T-BFA can mislead samples from single source
class or all classes to a target class by flipping the identified weight bits. It is worth noting that the
above bit-flip based attacks leverage heuristic strategies to identify critical weight bits. How to find
critical bits for the bit-flip based attack method is still an important open question.
3	Targeted Attack with Limited B it-Flips (TA-LBF)
3.1	Preliminaries
Storage and Calculation of Quantized DNNs. Currently, it is a widely-used technique to quantize
DNNs before deploying on devices for efficiency and reducing storage size. For each weight in
l-th layer of a Q-bit quantized DNN, it will be represented and then stored as the signed integer in
two’s complement representation (v = [vQ; vQ-1; ...; v1] ∈ {0, 1}Q) in the memory. Attacker can
modify the weights of DNNs through flipping the stored binary bits. In this work, we adopt the
layer-wise uniform weight quantization scheme similar to Tensor-RT (Migacz, 2017). Accordingly,
each binary vector V can be converted to a real number by a function h(∙), as follow:
Q-1
h(v) = (-2Q-1 ∙ VQ + X 2i-1∙ Vi) ∙ ∆l,	⑴
i=1
where l indicates which layer the weight is from, ∆l > 0 is a known and stored constant which
represents the step size of the l-th layer weight quantizer.
Notations. We denote a Q-bit quantized DNN-based classification model as f : X → Y, where
X ∈ Rd being the input space and Y ∈ {1, 2, ..., K} being the K-class output space. Assuming
that the last layer of this DNN model is a fully-connected layer with B ∈ {0, 1}K×C×Q being
the quantized weights, where C is the dimension of last layer’s input. Let Bi,j ∈ {0, 1}Q be the
two’s complement representation of a single weight and Bi ∈ {0, 1}C×Q denotes all the binary
weights connected to the i-th output neuron. Given a test sample x with the ground-truth label s,
f(x; Θ, B) ∈ [0, 1]K is the output probability vector and g(x; Θ) ∈ RC is the input of the last
layer, where Θ denotes the model parameters without the last layer.
Attack Scenario. In this paper, we focus on the white-box bit-flip based attack, which was first
introduced in (Rakin et al., 2019). Specifically, we assume that the attacker has full knowledge of
the model (including it’s architecture, parameters, and parameters’ location in the memory), and can
precisely flip any bit in the memory. Besides, we also assume that attackers can have access to a
small portion of benign samples, but they can not tamper the training process and the training data.
Attacker’s Goals. Attackers have two main goals, including the effectiveness and the stealthiness.
Specifically, effectiveness requires that the attacked model can misclassify a specific sample to a pre-
defined target class without any sample modification, and the stealthiness requires that the prediction
accuracy of other samples will not be significantly reduced.
3
Published as a conference paper at ICLR 2021
3.2 The Proposed Method
Loss for Ensuring Effectiveness. Recall that our first target is to force a specific image to be
classified as the target class by modifying the model parameters at the bit-level. To this end, the
most straightforward way is maximizing the logit of the target class while minimizing that of the
source class. For a sample x, the logit of a class can be directly determined by the input of the last
layer g(x; Θ) and weights connected to the node of that class. Accordingly, we can modify weights
only connected to the source and target class to fulfill our purpose, as follows:
Lι(x; Θ, B, Bs,Bt) =max (m — p(x;θ,Bt) + δ, 0) + max(p(x; Θ, BS)- m + δ, 0),	⑵
where p(x; Θ, Bi) = [h(Bi,1); h(Bi,2); ...; h(Bi,C)]>g(x; Θ) denotes the logit of class i (i = s
or i = t), h(∙) is the function defined in Eq. (1), m = max	p(x; Θ, Bi), and δ ∈ R
i∈{0,...,K}\{s}
indicates a slack variable, which will be specified in later experiments. The first term of L1 aims at
increasing the logit of the target class, while the second term is to decrease the logit of the source
class. The loss L1 is 0 only when the output on target class is more than m + δ and the output on
source class is less than m - δ. That is, the prediction on x of the target model is the predefined
target class. Note that Bs, Bt ∈ {0,1}C×Q are two variables We want to optimize, corresponding to
the weights of the fully-connected layer w.r.t. class s and t, respectively, in the target DNN model.
B ∈ {0, 1}K×C×Q denotes the weights of the fully-connected layer of the original DNN model, and
it is a constant tensor in L1. For clarity, hereafter we simplify L1(x; Θ, B, Bs, Bt) as L1(Bs, Bt),
since x and Θ are also provided input and weights.
Loss for Ensuring Stealthiness. As we mentioned in Section 3.1, we assume that the attacker can
get access to an auxiliary sample set {(xi, yi)}iN=1. Accordingly, the stealthiness of the attack can
be formulated as follows:
N
一 ʌ ʌ . X~*> - , ʌ ,	_ _	ʌ ʌ .	一
L2(B s,B t) = £'(f(Xi； θ, b{1,...,K}∖{s,t}, Bs, Bt),yi),	⑶
i=1
where B{i,...,κ}∖{s,t} denotes {Bι, B2,…,BK}∖{Bs, Bt}, and f (xi； Θ, B{i,...,κ}∖{s,t}, Bs, Bt)
indicates the posterior probability of xi w.r.t. class j, caclulated by Softmax(p(xi； Θ, Bj)) or
Softmax(p(xi； Θ,Bj)). '(∙, ∙) is specified by the cross entropy loss. To keep clarity, Xi, Θ and
B{1,...,K}\{s,t} areomittedinL2(Bs, Bt) .
Besides, to better meet our goal, a straightforward additional approach is reducing the magnitude of
the modification. In this paper, we constrain the number of bit-flips less than k. Physical bit flipping
techniques can be time-consuming as discussed in (Van Der Veen et al., 2016; Zhao et al., 2019).
Moreover, such techniques lead to abnormal behaviors in the attacked system (e.g., suspicious cache
activity of processes), which may be detected by some physical detection-based defenses (Gruss
et al., 2018). As such, minimizing the number of bit-flips is critical to make the attack more efficient
and practical.
Overall Objective. In conclusion, the final objective function is as follows:
一 ,d d 、 一 一 ，d ʌ .
min L1(Bs, Bt) + λL2(BS, Bt),
^s,^t	(4)
s.t. B S ∈{0,1}C×Q, B t ∈{0, 1}c ×q, dH (Bs, B s) + dH (Bt, B t) ≤ k,
where dH (∙, ∙) denotes the Hamming distance and λ > 0 is a trade-off parameter.
For the sake of brevity, Bs and Bt are concatenated and further reshaped to the vector b ∈ {0, 1}2CQ.
Similarly, BS and Bt are concatenated and further reshaped to the vector b ∈ {0, 1}2cq. Besides,
for binary vector b and b, there exists a nice relationship between Hamming distance and Euclidean
distance: dH (b, b) = ||b - b||22. The new formulation of the objective is as follows:
min	Lι(b) + λL2(b),	s.t.	b ∈	{0,	1}2cq, ||b —	b||2	— k ≤	0.	⑸
b	v 7
Problem (5) is denoted as TA-LBF (targeted attack with limited bit-flips). Note that TA-LBF is a
binary integer programming (BIP) problem, whose optimization is challenging. We will introduce
an effective and efficient method to solve it in the following section.
4
Published as a conference paper at ICLR 2021
3.3 An Effective Optimization Method for TA-LBF
To solve the challenging BIP problem (5), we adopt the generic solver for integer programming,
dubbed 'p-Box ADMM (WU & Ghanem, 2018). The solver presents its superior performance in
many tasks, e.g., model pruning (Li et al., 2019), clustering (Bibi et al., 2019), MAP inference (Wu
et al., 2020a), adversarial attack (Fan et al., 2020), etc.. It proposed to replace the binary constraint
equivalently by the intersection of two continuous constraints, as follows
b ∈{0,1}2CQ ⇔ b ∈ (Sb ∩Sp),	(6)
where Sb = [0,1]2CQ indicates the box constraint, and Sp = {b : ||B - ɪ||2 = 2CQ} denotes the
`2 -sphere constraint. Utilizing (6), Problem (5) is equivalently reformulated as
min	L1 (b)	+	λL2 (b),	s.t. b =	u1, b = u2,	||b - b||22	- k + u3 =	0,	(7)
b,Ul ∈Sb ,U2 ∈Sp ,U3 ∈R+
where two extra variables u1 and u2 are introduced to split the constraints w.r.t. b. Besides, the non-
negative slack variable u3 ∈ R+ is used to transform ∣∣b-b∣∣2 -k ≤ 0 in (5) into ∣∣b-b∣∣2-k+u3 =
0. The above constrained optimization problem can be efficiently solved by the alternating direction
method of multipliers (ADMM) (Boyd et al., 2011).
Following the standard procedure of ADMM, we firstly present the augmented Lagrangian function
of the above problem, as follows:
>>
L(b, u1, u2,u3, z1, z2, z3) =L1(b) + λL2(b) + z1>(b - u1) + z2>(b - u2)
+z3(IIb - b||2 - k + u3) + CI(UI) + c2(u2) + c3(u3)	(8)
+ ρ21 ||B - u1||2 + ρ22 ||b - u2||2 + P (IIb - M2 - k + u3)2,
where z1, z2 ∈ R2CQ and z3 ∈ R are dual variables, and ρ1, ρ2, ρ3 > 0 are penalty factors, which
will be specified later. c1(u1) = I{u1∈Sb}, c2(u2) = I{u2∈Sp}, and c3(u3) = I{u3∈R+} capture
the constraints Sb, Sp and R+, respectively. The indicator function I{a} = 0 if a is true; otherwise,
I{a} = +∞. Based on the augmented Lagrangian function, the primary and dual variables are
updated iteratively, with r indicating the iteration index.
Given (br, Zr, Zr, zɜ), update (u；+1, Ur+1,U3+1). Given (br, Zr, Zr, zɜ), (uι, u2, u3) are inde-
pendent, and they can be optimized in parallel, as follows
'u1+1 = arg min (z3)>(br - uι) + ρ21 ||br - uι∣∣2 = PSb(br + ⅛),
uι∈Sb
u2+1 = arg min (Zr)>(br - u2) + ρ22||br - u2||2 = PSp(Br + Zr),
u2∈Sp	2	(9)
U3+1 = arg min zɜ(∣∣b —针||2 — k + u3) + ρ3(∣∣b — br ∣∣2 — k + u3)2
u3 ∈R+
、	=PR+(-||b -针||2 + k - ρ3),
where PSb(a) = min((1, max(0, a)) with a ∈ Rn is the projection onto the box constraint Sb;
Psp(a)= 空 ∣∣aa∣∣ + 22 with a = a - ∣ indicates the projection onto the '2-sphere constraint Sp
(Wu & Ghanem, 2018); PR+ (a) = max(0, a) with a∈R indicates the projection onto R+.
Given (u3+1 , ur+1,u3+1, Zr, Zr, zɜ), update br+1. Although there is no closed-form solution
to br+1, it can be easily updated by the gradient descent method, as both Lι(b) and L2(b) are
differentiable w.r.t. b, as follows
Br+1 — br - η
∂L(b, ur+1, ur+1,ur+1, Zr, Zr ,z3) ∣
∂b	∣3=6r
(10)
where η > 0 denotes the step size. Note that we can run multiple steps of gradient descent in the
above update. Both the number of steps andη will be specified in later experiments. Besides, due to
the space limit, the detailed derivation of ∂L∕∂b will be presented in Appendix A.
5
Published as a conference paper at ICLR 2021
Given (br+1, u「1, ur+1,ur+1), update (z：+1, Z2+1, z3r+1). The dual variables are updated by
the gradient ascent method, as follows
f zr+1 = Zr + ρι(br+1 - urι+1),
Z zr+1 = Zr+P2(Br+1- ur+1),	(11)
I z3+1 = Z+ρ3(∣∣b-Br+1∣∣2- k+ur+1).
Remarks. 1) Note that since (ur1+1, ur2+1, ur3+1) are updated in parallel, their updates belong to
the same block. Thus, the above algorithm is a two-block ADMM algorithm. We provide the
algorithm outline in Appendix B. 2) Except for the update of br+1, all other updates are very Sim-
ple and efficient. The computational cost of the whole algorithm will be analyzed in Appendix
C. 3) Due to the inexact solution to Br+1 using gradient descent, the theoretical convergence of
the whole ADMM algorithm cannot be guaranteed. However, as demonstrated in many previous
works (Gol’shtein & Tret’yakov, 1979; Eckstein & Bertsekas, 1992; Boyd et al., 2011), the inexact
two-block ADMM often shows good practical convergence, which is also the case in our later ex-
periments. Besides, the numerical convergence analysis is presented in Appendix D. 4) The proper
adjustment of (ρ1, ρ2, ρ3) could accelerate the practical convergence, which will be specified later .
4	Experiments
4.1	Evaluation Setup
Settings. We compare our method (TA-LBF) with GDA (Liu et al., 2017a), FSA (Zhao et al., 2019),
T-BFA (Rakin et al., 2020b), and TBT (Rakin et al., 2020a). All those methods can be adopted
to misclassify a specific image into a target class. We also take the fine-tuning (FT) of the last
fully-connected layer as a baseline method. We conduct experiments on CIFAR-10 (Krizhevsky
et al., 2009) and ImageNet (Russakovsky et al., 2015). We randomly select 1,000 images from each
dataset as the evaluation set for all methods. Specifically, for each of the 10 classes in CIFAR-10,
we perform attacks on the 100 randomly selected validation images from the other 9 classes. For
ImageNet, we randomly choose 50 target classes. For each target class, we perform attacks on
20 images randomly selected from the rest classes in the validation set. Besides, for all methods
except GDA which does not employ auxiliary samples, we provide 128 and 512 auxiliary samples
on CIFAR-10 and ImageNet, respectively. Following the setting in (Rakin et al., 2020a;b), we
adopt the quantized ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) as the target
models. For our TA-LBF, the trade-off parameter λ and the constraint parameter k affect the attack
stealthiness and the attack success rate. We adopt a strategy for jointly searching λ and k, which is
specified in Appendix E.3. More descriptions of our settings are provided in Appendix E.
Evaluation Metrics. We adopt three metrics to evaluate the attack performance, i.e., the post attack
accuracy (PA-ACC), the attack success rate (ASR), and the number of bit-flips (Nflip). PA-ACC
denotes the post attack accuracy on the validation set except for the specific attacked sample and the
auxiliary samples. ASR is defined as the ratio of attacked samples that are successfully attacked into
the target class among all 1,000 attacked samples. Nflip is the number of bit-flips required for an
attack. A better attack performance corresponds to a higher PA-ACC and ASR, while a lower Nflip .
Besides, we also show the accuracy of the original model, denoted as ACC.
4.2	Main Results
Results on CIFAR-10. The results of all methods on CIFAR-10 are shown in Table 1. Our method
achieves a 100% ASR with the fewest Nflip for all the bit-widths and architectures. FT modifies the
maximum number of bits among all methods since there is no limitation of parameter modifications.
Due to the absence of the training data, the PA-ACC of FT is also poor. These results indicate that
fine-tuning the trained DNN as an attack method is infeasible. Although T-BFA flips the second-
fewest bits under three cases, it fails to achieve a higher ASR than GDA and FSA. In terms of
PA-ACC, TA-LBF is comparable to other methods. Note that the PA-ACC of TA-LBF significantly
outperforms that of GDA, which is the most competitive w.r.t. ASR and Nflip among all the baseline
methods. The PA-ACC of GDA is relatively poor, because it does not employ auxiliary samples.
Achieving the highest ASR, the lowest Nflip, and the comparable PA-ACC demonstrates that our
optimization-based method is more superior than other heuristic methods (TBT, T-BFA and GDA).
6
Published as a conference paper at ICLR 2021
and Nflip are calculated by attacking the 1,000 images. Our method is denoted as TA-LBF.
DataSet	Method	Target Model	PA-ACC ASR (%)	(%)	Nflip	Target Model	PA-ACC ASR (%)	(%)	Nflip
Table 1: Results of all attack methods across different bit-widths and architectures on CIFAR-10 and
ImageNet (bold: the best; underline: the second best). The mean and standard deviation of PA-ACC
	FT	ReSNet 8-bit	85.01 ±2.90	100.0	1507.51±86.54	VGG 8-bit	84.31±3.10	98.7	11298.74±830.36
	TBT		88.07±0.84	97.3	246.70±8.19		77.79±23.35	51.6	599.40±19.53
	T-BFA		87.56±2.22	98.7	9.91 ±2.33		89.83±3.92	96.7	14.53±3.74
CIFAR-10	FSA GDA	ACC: 92.16%	88.38±2.28 86.73±3.50	98.9 99.8	185.51±54.93 26.83±12∙50	ACC: 93.20%	88.80±2.86 85.51±2.88	96.8 100.0	253.92±122.06 21.54±6.79
	TA-LBF		88.20±2.64	100.0	5.57±i.58		86.06±3.17	100.0	7.40±2.72
	-FT-	ReSNet 4-bit	84.37±2.94	100.0	-392.48±47.26	VGG 4-bit	83.31±3.76	94.5	2270.52±324.69
	TBT		87.79±1.86	96.0	118.20±15.03		83.90±2.63	62.4	266.40±18.70
	T-BFA		86.46±2.80	97.9	8.80±2.oi		88.74±4.52	96.2	11.23±2.36
	FSA	ACC: 91.90%	87.73±2.36	98.4	76.83±25.27	ACC: 92.61%	87.58±3.06	97.5	75.03±29.75
	GDA		86.25±3.59	99.8	14.08±7.94		85.08±2.82	100.0	10.31±3.77
	TA-LBF		87.82±2.60	100.0	5.25±i.09		85.91±3.29	100.0	6.26±2.37
	FT	ReSNet 8-bit	59.33±0.93	100.0	277424.29±12136.34	VGG 8-bit	62.08±2.33	100.0	1729685.22±137539.54
	TBT		69.18±0.03	99.9	577.40±19.42		72.99±0.02	99.2	4115.26±191.25
	T-BFA		68.71 ±0.36	79.3	24.57±20.03		73.09±0.12	84.5	363.78±153.28
ImageNet	FSA GDA	ACC: 69.50%	69.27±0.15 69.26±0.22	99.7 100.0	441.21±119.45 18.54±6.14	ACC: 73.31%	73.28±0.03 73.29±o.02	100.0 100.0	1030.03±260.30 197.05±49.85
	TA-LBF		69.41 ±o.08	100.0	7.37±2.i8		73.28±0.03	100.0	69.89±i8.42
	-FT-	ReSNet 4-bit	15.65±4.52	100.0	135854.50±21399.94	VGG 4-bit	l7.76±1.7i	100.0	1900751.70±37329.44
	TBT		66.36±0.07	99.8	271.24±15.98		71.18±0.03	100.0	3231.00±345.68
	T-BFA		65.86±0.42	80.4	24.79±19.02		71.49±0.15	84.3	350.33±158.57
	FSA GDA	ACC: 66.77%	66.44±0.21 66.54±0.22	99.9 100.0	157.53 ±33.66 11.45±3.82	ACC: 71.76%	71.69±0.09 71.73±0.03	100.0 100.0	441.32±111.26 107.18±28.70
	TA-LBF		66.69±o.07	100.0	7.96 ±2.50		71.73±0.03	100.0	69.72±i8.84
Results on ImageNet. The results on ImageNet are shown in Table 1. It can be observed that GDA
shows very competitive performance compared to other methods. However, our method obtains the
highest PA-ACC, the fewest bit-flips (less than 8), and a 100% ASR in attacking ResNet. For VGG,
our method also achieves a 100% ASR with the fewest Nflip for both bit-widths. The Nflip results of
our method are mainly attributed to the cardinality constraint on the number of bit-flips. Moreover,
for our method, the average PA-ACC degradation over four cases on ImageNet is only 0.06%, which
demonstrates the stealthiness of our attack. When comparing the results of ResNet and VGG, an
interesting observation is that all methods require significantly more bit-flips for VGG. One reason
is that VGG is much wider than ResNet. Similar to the claim in (He et al., 2020), increasing the
network width contributes to the robustness against the bit-flip based attack.
4.3	Resistance to Defense Methods
Resistance to Piece-wise Clustering. He et al. (2020) proposed a novel training technique, called
piece-wise clustering, to enhance the network robustness against the bit-flip based attack. Such
a training technique introduces an additional weight penalty to the inference loss, which has the
effect of eliminating close-to-zero weights (He et al., 2020). We test the resistance of all attack
methods to the piece-wise clustering. We conduct experiments with the 8-bit quantized ResNet
on CIFAR-10 and ImageNet. Following the ideal configuration in (He et al., 2020), the clustering
coefficient, which is a hyper-parameter of piece-wise clustering, is set to 0.001 in our evaluation.
For our method, the initial k is set to 50 on ImageNet and the rest settings are the same as those in
Section 4.1. Besides the three metrics in Section 4.1, we also present the number of increased Nflip
compared to the model without defense (i.e., results in Table 1), denoted as ∆Nflip.
The results of the resistance to the piece-wise clustering of all attack methods are shown in Table
2. It shows that the model trained with piece-wise clustering can improve the number of required
bit-flips for all attack methods. However, our method still achieves a 100% ASR with the least
number of bit-flips on both two datasets. Although TBT achieves a smaller ∆Nflip than ours on
CIFAR-10, its ASR is only 52.3%, which also verifies the defense effectiveness of the piece-wise
clustering. Compared with other methods, TA-LBF achieves the fewest ∆Nflip on ImageNet and the
best PA-ACC on both datasets. These results demonstrate the superiority of our method over other
methods when attacking models trained with piece-wise clustering.
7
Published as a conference paper at ICLR 2021
Table 2: Results of all attack methods against the models with defense on CIFAR-10 and ImageNet
(bold: the best; underline: the second best). The mean and standard deviation of PA-ACC and Nflip
are calculated by attacking the 1,000 images. Our method is denoted as TA-LBF. ∆Nflip denotes
the increased Nflip compared to the corresponding result in Table 1.
Defense
Dataset
Method
ACC
(%)
PA-ACC
(%)
ASR
(%)
Nflip
∆Nflip
gniretsulC esiw-eceiP
FT
TBT
T-BFA
FSA
GDA
TA-LBF
	84.06±3.56 87.05±i.69	99.5 52.3	1893.55±68.98 254.20±10.22	386.04 7.50
Ql ∩1	85.82±i.89	98.6	45.51 ±9.47	35.60
91.01	86.61±2.51	98.6	246.11±75.36	60.60
	84.12±4.77	100.0	52.76±16.29	25.93
	87.30±2.74	100.0	18.93±7.11	13.36
teNegamI
FT		43.44±2.07	92.2	762267.56±52179.46	484843.27
TBT		63.07±0.04	81.8	1184.14±30.30	606.74
T-BFA	63.62	62.82±0.27	90.1	273.56±191.29	248.99
FSA		63.26±0.21	99.5	729.94±491.83	288.73
GDA		63.14±0.48	100.0	107.59±31.15	89.05
TA-LBF		63.52±0.14	100.0	51.11±4.33	43.74
yticapaC ledoM regraL
teNegam
FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF
94.29
71.35
86.46±2.84
89.72±2.99
91.16±1.42
90.70±2.37
89.83±3.02
90.96±263
63.51±1.29
71.12±0.04
70.84±0.30
71.30±0.04
71.30±0.05
71.30±0.04
100.0	2753.43±188.27	1245.92
89.5	366.90±12.09	120.20
98.7	17.91±4.64	8.00
98.5	271.27±65.18	85.76
100.0	48.96±21.03	22.13
100.0	8.79±2.44	3.22
100.0	507456.61±34517.04	230032.32
99.9	1138.34±44.23	560.94
88.9	40.23±27.29	15.66
100.0	449.70±106.42	8.49
100.0	20.01±6.04	1.47
100.0	8.48±2.52	1.11
ASR	- PA-ACC	Nflip
90β0706°
(求)US4 / □□4wd
O5/O
亲)US4 / □□4wd
3 N
/
1	5	10	20	50	100 200	5	10	15	20	25	30	25	50	100	200	400	800
λ	k	N
Figure 2: Results of TA-LBF with different parameters λ, k, and the number of auxiliary samples
N on CIFAR-10. Regions in shadow indicate the standard deviation of attacking the 1,000 images.
Resistance to Larger Model Capacity. Previous studies (He et al., 2020; Rakin et al., 2020b)
observed that increasing the network capacity can improve the robustness against the bit-flip based
attack. Accordingly, we evaluate all attack methods against the models with a larger capacity using
the 8-bit quantized ResNet on both datasets. Similar to the strategy in (He et al., 2020), we increase
the model capacity by varying the network width (i.e., 2× width in our experiments). All settings
of our method are the same as those used in Section 4.1.
The results are presented in Table 2. We observe that all methods require more bit-flips to attack
the model with the 2× width. To some extent, it demonstrates that the wider network with the same
architecture is more robust against the bit-flip based attack. However, our method still achieves a
100% ASR with the fewest Nflip and ∆Nflip . Moreover, when comparing the two defense methods,
we find that piece-wise clustering performs better than the model with a larger capacity in terms of
∆Nflip . However, piece-wise clustering training also causes the accuracy decrease of the original
model (e.g., from 92.16% to 91.01% on CIFAR-10). We provide more results in attacking models
with defense under different settings in Appendix F.
8
Published as a conference paper at ICLR 2021
Figure 3: Visualization of decision boundaries of the original model and the post attack models. The
attacked sample from Class 3 is misclassified into the Class 1 by FSA, GDA, and our method.
4.4	Ablation Study
We perform ablation studies on parameters λ and k, and the number of auxiliary samples N. We use
the 8-bit quantized ResNet on CIFAR-10 as the representative for analysis. We discuss the attack
performance of TA-LBF under different values of λ while k is fixed at 20, and under different values
of k while λ is fixed at 10. To analyze the effect of N, we configure N from 25 to 800 and keep other
settings the same as those in Section 4.1. The results are presented in Fig. 2. We observe that our
method achieves a 100% ASR when λ is less than 20. As expected, the PA-ACC increases while the
ASR decreases along with the increase of λ. The plot of parameter k presents that k can exactly limit
the number of bit-flips, while other attack methods do not involve such constraint. This advantage
is critical since it allows the attacker to identify limited bits to perform an attack when the budget is
fixed. As shown in the figure, the number of auxiliary samples less than 200 has a marked positive
impact on the PA-ACC. It’s intuitive that more auxiliary samples can lead to a better PA-ACC. The
observation also indicates that TA-LBF still works well without too many auxiliary samples.
4.5	Visualization of Decision B oundary
To further compare FSA and GDA with our method, we visualize the decision boundaries of the
original and the post attack models in Fig. 3. We adopt a four-layer Multi-Layer Perceptron trained
with the simulated 2-D Blob dataset from 4 classes. The original decision boundary indicates that
the original model classifies all data points almost perfectly. The attacked sample is classified into
Class 3 by all methods. Visually, GDA modifies the decision boundary drastically, especially for
Class 0. However, our method modifies the decision boundary mainly around the attacked sample.
Althoug FSA is comparable to ours visually in Fig. 3, it flips 10× bits than GDA and TA-LBF. In
terms of the numerical results, TA-LBF achieves the best PA-ACC and the fewest Nflip. This finding
verifies that our method can achieve a successful attack even only tweaking the original classifier.
5	Conclusion
In this work, we have presented a novel attack paradigm that the weights of a deployed DNN can be
slightly changed via bit flipping in the memory, to give a target prediction for a specific sample, while
the predictions on other samples are not significantly influenced. Since the weights are stored as
binary bits in the memory, we formulate this attack as a binary integer programming (BIP) problem,
which can be effectively and efficiently solved by a continuous algorithm. Since the critical bits are
determined through optimization, the proposed method can achieve the attack goals by flipping a
few bits, and it shows very good performance under different experimental settings.
Acknowledgments
This work is supported in part by the National Key Research and Development Program of China
under Grant 2018YFB1800204, the National Natural Science Foundation of China under Grant
61771273, the R&D Program of Shenzhen under Grant JCYJ20180508152204044. Baoyuan Wu is
supported by the Natural Science Foundation of China under grant No. 62076213, and the university
development fund of the Chinese University of Hong Kong, Shenzhen under grant No. 01001810.
9
Published as a conference paper at ICLR 2021
References
Michel Agoyan, Jean-Max Dutertre, Amir-Pasha Mirbaha, David Naccache, Anne-Lise Ribotta, and
Assia Tria. HoW to flip a bit? In IOLTS,pp. 235-239, 2010.
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:
A survey. IEEE Access, 6:14410-14430, 2018.
JiaWang Bai, Bin Chen, Yiming Li, Dongxian Wu, WeiWei Guo, Shu-tao Xia, and En-hui Yang.
Targeted attack for deep hashing based retrieval. In ECCV, 2020.
Adel Bibi, Baoyuan Wu, and Bernard Ghanem. Constrained k-means With general pairWise and
cardinality constraints. arXiv preprint arXiv:1907.10410, 2019.
Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the
alternating direction method of multipliers. NoW Publishers Inc, 2011.
Jakub Breier, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu. Practical fault
attack on deep neural netWorks. In CCS, pp. 2204-2206, 2018.
Yair Carmon, Aditi Raghunathan, LudWig Schmidt, John C. Duchi, and Percy Liang. Unlabeled
data improves adversarial robustness. In NeurIPS, 2019.
Weilun Chen, Zhaoxiang Zhang, Xiaolin Hu, and Baoyuan Wu. Boosting decision-based black-
box adversarial attacks With random sign flip. In Proceedings of the European Conference on
Computer Vision, 2020.
Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and Jun Zhu. Efficient
decision-based black-box adversarial attacks on face recognition. In CVPR, pp. 7714-7722, 2019.
Min Du, Ruoxi Jia, and DaWn Song. Robust anomaly detection and backdoor attack detection via
differential privacy. ICLR, 2020.
Jonathan Eckstein and Dimitri P Bertsekas. On the douglas—rachford splitting method and the
proximal point algorithm for maximal monotone operators. Mathematical Programming, 55(1-
3):293-318, 1992.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, ChaoWei Xiao, Atul
Prakash, Tadayoshi Kohno, and DaWn Song. Robust physical-World attacks on deep learning
visual classification. In CVPR, pp. 1625-1634, 2018.
Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang Li, Zhifeng Li, and Yujiu Yang.
Sparse adversarial attack via perturbation factorization. In European Conference on Computer
Vision, 2020.
Yan Feng, Bin Chen, Tao Dai, and Shutao Xia. Adversarial attack on deep product quantization
netWork for image retrieval. In AAAI, 2020.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via finite element approximation. Computers & mathematics with applications, 2(1):
17-40, 1976.
Roland GloWinski and A Marroco. Sur l’approximation, par e´le´ments finis d’ordre un, et
la re´solution, par pe´nalisation-dualite´ d’une classe de proble`mes de dirichlet non line´aires.
ESAIM: Mathematical Modelling and Numerical Analysis-Modelisation Mathematique et Anal-
yse Numerique, 9(R2):41-76,1975.
E Gi Gol’shtein and NV Tret’yakov. Modified lagrangians in convex programming and their gener-
alizations. In Point-to-Set Maps and Mathematical Programming, pp. 86-97. Springer, 1979.
Ian J GoodfelloW, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
10
Published as a conference paper at ICLR 2021
Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas Juffinger, Sioli O’Connell,
Wolfgang Schoechl, and Yuval Yarom. Another flip in the wall of rowhammer defenses. In IEEE
S&P, pp. 245-261,2018.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, 7:47230-47244, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016.
Zhezhi He, Adnan Siraj Rakin, Jingtao Li, Chaitali Chakrabarti, and Deliang Fan. Defending and
harnessing the bit-flip based adversarial weight attack. In CVPR, pp. 14095-14103, 2020.
SanghyUn Hong, Pietro Frigo, Yigitcan Kaya, Cristiano Giuffrida, and TUdor DUmitras. Terminal
brain damage: Exposing the graceless degradation in deep neural networks under hardware fault
attacks. In USENIX Security Symposium, pp. 497-514, 2019.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong ZhU, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. QUantization and training of neUral networks for
efficient integer-arithmetic-only inference. In CVPR, pp. 2704-2713, 2018.
YoongU Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, DonghyUk Lee, Chris Wilkerson,
Konrad Lai, and OnUr MUtlU. Flipping bits in memory withoUt accessing them: An experimental
stUdy of dram distUrbance errors. ACM SIGARCH Computer Architecture News, 42(3):361-372,
2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning mUltiple layers of featUres from tiny images.
Technical report, 2009.
TUanhUi Li, BaoyUan WU, YUjiU Yang, Yanbo Fan, Yong Zhang, and Wei LiU. Compressing convo-
lUtional neUral networks via factorized convolUtional filters. In CVPR, 2019.
Yiming Li, BaoyUan WU, Yong Jiang, Zhifeng Li, and ShU-Tao Xia. Backdoor learning: A sUrvey.
arXiv preprint arXiv:2007.08745, 2020.
Kang LiU, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-prUning: Defending against backdoor-
ing attacks on deep neUral networks. In RAID, pp. 273-294, 2018a.
Yannan LiU, Lingxiao Wei, Bo LUo, and Qiang XU. FaUlt injection attack on deep neUral network.
In ICCAD, pp. 131-138, 2017a.
Yingqi LiU, Shiqing Ma, YoUsra Aafer, Wen-ChUan Lee, JUan Zhai, Weihang Wang, and XiangyU
Zhang. Trojaning attack on neUral networks. In NDSS, 2018b.
YUnfei LiU, XingjUn Ma, James Bailey, and Feng LU. Reflection backdoor: A natUral backdoor
attack on deep neUral networks. ECCV, 2020a.
YUntao LiU, Yang Xie, and AnkUr Srivastava. NeUral trojans. In ICCD, pp. 45-48, 2017b.
YUntao LiU, Ankit Mondal, Abhishek Chakraborty, Michael ZUzak, Nina Jacobsen, Daniel Xing,
and AnkUr Srivastava. A sUrvey on neUral trojans. In ISQED, 2020b.
Szymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, 2017.
Seyed-Mohsen Moosavi-Dezfooli, AlhUssein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial pertUrbations. In CVPR, pp. 1765-1773, 2017.
Jonathan Pan. Blackbox trojanising of deep learning models: Using non-intrUsive network strUctUre
and binary alterations. arXiv preprint arXiv:2008.00408, 2020.
Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Bit-flip attack: CrUshing neUral network with
progressive bit search. In ICCV, pp. 1211-1220, 2019.
Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neUral network attack with bit trojan.
In CVPR, pp. 13198-13207, 2020a.
11
Published as a conference paper at ICLR 2021
Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali Chakrabarti, and Deliang Fan. T-bfa:
Targeted bit-flip adversarial weight attack. arXiv preprint arXiv:2007.12336, 2020b.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal ofcomputer vision, 115(3):211-252, 2015.
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor at-
tacks. In AAAI, pp. 11957-11965, 2020.
Bodo Selmke, Stefan Brummer, Johann Heyszl, and Georg Sigl. Precise laser fault injections into
90 nm and 45 nm sram-cells. In CARDIS, pp. 193-205. Springer, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS,
pp. 8000-8010, 2018.
Victor Van Der Veen, Yanick Fratantonio, Martina Lindorfer, Daniel Gruss, Clementine Maurice,
Giovanni Vigna, Herbert Bos, Kaveh Razavi, and Cristiano Giuffrida. Drammer: Deterministic
rowhammer attacks on mobile platforms. In CCS, pp. 1675-1689, 2016.
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In IEEE
S&P, pp. 707-723, 2019.
Baoyuan WU and Bernard Ghanem. 'p-box admm: A versatile framework for integer programming.
IEEE transactions on pattern analysis and machine intelligence, 41(7):1695-1708, 2018.
Baoyuan Wu, Li Shen, Tong Zhang, and Bernard Ghanem. Map inference via l2-sphere linear
program reformulation. International Journal of Computer Vision, pp. 1-24, 2020a.
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter:
On the transferability of adversarial examples generated with resnets. In ICLR, 2020b.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-
alization. In NeurIPS, 2020c.
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against feder-
ated learning. In ICLR, 2019.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. ICLR, 2018.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. NDSS, 2017.
Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, and Wei Liu. Exact
adversarial attack to image captioning via structured output learning with latent variables. In
CVPR, 2019.
Fan Yao, Adnan Siraj Rakin, and Deliang Fan. Deephammer: Depleting the intelligence of deep
neural networks through targeted chain of bit flips. In USENIX Security Symposium, pp. 1463-
1480, 2020.
Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In-So Kweon. Cd-uap: Class discriminative
universal adversarial perturbation. In AAAI, 2020a.
Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. Understanding adversarial exam-
ples from the mutual influence of images and perturbations. In CVPR, 2020b.
Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, and Xue Lin. Fault sneaking attack:
A stealthy framework for misleading deep neural networks. In ACM DAC, pp. 1-6. IEEE, 2019.
12
Published as a conference paper at ICLR 2021
A UPDATE b BY GRADIENT DESCENT
In this section, We derive the gradient of L w.r.t. b, which is adopted to update Br+1 by gradient
descent (see Eq. (10)). The derivation consists of the following parts.
■ -¼	♦	,♦	<∙<Λz->	∕j`∖∕<Λj` ι -	1	1	r∙ . 1	.	1 f` ∙ . ∙
Derivation of ∂L1 (b)∕∂b. For clarity, here we firstly repeat some definitions,
L1 (Bs, Bt) = max (m - p(x; Θ, Bt) + δ, 0) + max(p(x; Θ, Bs)- m + δ,0),	(12)
p(x; Θ, Bi) = [h(Bi,1); h(Bi,2);…;h(Bi,c)]>g(x; Θ),	(13)
Q-1
h(v) = (-2Q-1 ∙ VQ + £ 2i-1 ∙ vi) ∙ ∆.	(14)
i=1
Then, we obtain that
C	，	_	ʌ	.
∂p(x; Θ, Bi)
ʌ
∂B i
Γ ( c、Nh(B i,1)、>	( C、Nh(B i,C)、>]
EQ； θ) ∙ (	) ；…;gC(x； θ) ∙ (Dg )],
∂ Bi,1	VBi,c
(15)
where ^hv = [20; 21,..., 2Q-2; -2Q-1] ∙ ∆ is a constant, and here l indicates the last layer;
gj(x； Θ) denotes the j-th entry of the vector g(x； Θ). Utilizing (15), we have
_ , ʌ ʌ ,
∂L1(Bs, Bt)
ʌ
∂B s
_ , ʌ ʌ .
∂L1(Bs, Bt)
ʌ
∂B t
d dpxθB1, if p(x; θ, Bs) >m - δ
0, otherwise
∂	dp(x;®,Rt)
-	∂Bt-,
0, otherwise
if p(x； Θ, Bt) <m+δ
(16)
Thus, we obtain that
_	, ʌ.	ʌ ʌ . ʌ ʌ .
∂L1(b)	「 I	∕∂L1(Bs,B t).、	I	∕∂L1(B s,B t)λ1
-ʃ = [Reshape( —7^-)； Reshape( -7^~~-)],
∂b	∂Bs	∂Bt
(17)
where ReShaPe( ∙) elongates a matrix to a vector along the column.
Derivation of ∂L2(b)∕∂b. For clarity, here we firstly repeat the following definition
N
L2(Bs, Bt) = X'(f (xi； Θ, B{1,...,κ}∖{s,t}, Bs, Bt),y，，	(18)
i=1
1	f /	χ-χ n	SS ∖cc,	//	χ-χ n ∖∖	CC	/	/	χ-χ n ∖ ∖ ∙
where fj(xi； Θ, B{1,...,K}\{s,t}, Bs, Bt) = Softmax(p(xi； Θ, Bj)) or Softmax(p(xi； Θ, Bj)) in-
dicates the posterior probability of xi w.r.t. class j, and we simply denote f(xi) ∈ [0, 1]K as the
posterior probability vector of xi. {(xi, yi)}N=1 denotes the auxiliary sample set. '(∙, ∙) is specified
as the cross entropy loss. Then, we have
_ ʌ ʌ .	1∖T	ʌ ..
∂ L2(B s, B t)	/	、,/ CC	∂ ∂ (x dp(xi； Θ, B s)
---T^---- = £ (I(yi = S) - fs(xi θ, B{1,…,K}∖{s,t}, Bs, Bt))--^--
∂Bs	i=1	∂Bs
N
dL2(Bs, Bt) _X (	_	B	B B ^ʌ dP(xi； θ, Bt)
T^	= T (I(yi = t) - ft(xi; θ, B{1,...,K}∖{s,t}, Bs, Bt)) ∙	T^
∂Bt	i=1	∂Bt
where I(a) = 1 of a is true, otherwise I(a) = 0. Thus, we obtain
_ , ʌ. _ _ ʌ ʌ . _ ʌ ʌ .
3= ReShaPe(*B))；ReShaPe().
∂ b	L '	∂Bs	，	'	∂B t	1
(19)
(20)
(21)
■ -¼	♦	，♦	i> <Λ7-∕J,∖ ∕<ΛJ^	A	1∙	.	1 - ∕rι∖	1	. ∙1 ∙	1 ■	∕<r∖	1 /C Λ ∖	Λ .
Derivation of ∂L(b)∕∂b. According to Eq. (8), and utilizing Eqs. (17) and (21), we obtain
ʌ ,
∂L(b)
ʌ
∂ b
_ , ʌ.
∂L1(b)
∂b
^r^κ∖
+----X——+Z1+Z2+P1(b-u1)+P2 (b-U2)+2(b-b)∙[z3+ρ3M-b∣∣2-k+u3].
∂b
(22)
13
Published as a conference paper at ICLR 2021
B Algorithm Outline
Algorithm 1 Continuous optimization for the BIP problem (5).
Input: The original quantized DNN model f with weights Θ, B, attacked sample X with ground-
truth label s, target class t, auxiliary sample set {(xi, yi)}iN=1, hyper-parameters λ, k, and δ.
Output: b.
1:	Initial u1, u0, u3, z0, z0, z0, b>0 and let r — 0;
2:	while not converged do
3:	Update ur1+1, ur2+1 and ur3+1 as Eq. (9);
4:	Update 针+1 as Eq. (10);
5:	Update z1r+1, z2r+1 and z3r+1 as Eq. (11);
6:	r — r + 1.
7:	end while
C Complexity Analysis
Table 3: Running time (seconds) of attacking one image for different methods. The mean and
standard deviation are calculated by 10 attacks.
	FT	TBT	T-BFA	FSA	GDA	IA-LBF
CIFAR-10	15.54±1.64	-389.12±27.79-	35.05±15.79	~Hn ±0.48	0.67±0.54	113.38±6.54
ImageNet	124.32±3.61	31425.81±540.60	19.16±3.52	65.28±2.49	61.97±1.59	222.95±9.39
The computational complexity of the proposed algorithm (i.e., Algorithm 1) consists of two parts,
the forward and backward pass. In terms of the forward pass, since Θ, B{1,...,K}\{s,t} are fixed
during the optimization, their involved terms, including g(x; Θ) and p(x; Θ, Bi)|i6=s,t, are cal-
culated only one time. The main cost from BS and Bt is O(2(N + 1)C2Q) per iteration, as
there are N + 1 samples. In terms of the backward pass, the main cost is from the update
of b>r+1, which is O(2(N + 1)CQ) per iteration in the gradient descent. Since all other UP-
dates are very simple, their costs are omitted here. Thus, the overall computational cost is
O(Touter[2(N + 1)CQ ∙ (C + Tinner)]), With Touter being the iteration of the overall algorithm
and Tinner indicating the number of gradient steps in updating br+1. As shown in Section D, the
proposed method TA-LBF always converges very fast in our experiments, thus Touter is not very
large. As demonstrated in Section E.3, Tinner is set to 5 in our experiments. In short, the proposed
method can be optimized very efficiently.
Besides, we also compare the computational complexity of different attacks empirically. Specifi-
cally, we compare the running time of attacking one image of different methods against the 8-bit
quantized ResNet on CIFAR-10 and ImageNet dataset. As shown in Table 3, TBT is the most time-
consuming method among all attacks. Although the proposed TA-LBF is not superior to T-BFA,
FSA, and GDA in running time, this gap can be tolerated when attacking a single image in the
deployment stage. Besides, our method performs better in terms of PA-ACC, ASR, and Nflip as
demonstrated in our experiments.
D Numerical Convergence Analysis
We present the numerical convergence of TA-LBF in Fig. 4. Note that ||b - u1||22 and ||b - u2||22
characterize the degree of satisfaction of the box and '2-sphere constraint, respectively. For the two
examples of CIFAR-10 and ImageNet, the values of both indicators first increase, then drop, and
finally close to 0. Another interesting observation is that L1 + λL2 first decreases evidently and
then increases slightly. Such findings illustrate the optimization process of TA-LBF. In the early
iterations, modifying the model parameters tends to achieve the two goals mentioned in Section 3.1;
in the late iterations, b is encouraged to satisfy the box and l2-sphere constraint. We also observe
that both examples stop when meeting ∣∣b - u1∣∣2 ≤ 10-4 and ∣∣b -如||2 ≤ 10-4 and do not
14
Published as a conference paper at ICLR 2021
CIFAR-IO
# iterations
Figure 4: Numerical convergence analysis of TA-LBF w.r.t. the attacked sample on CIFAR-10 and
ɪ	ʌ T ,	, ∙ 1 -c-r T	, ,1	1	I' I I ɪ'	IIr)IIf	I l9 FC , ʌ z> , 1 ∙ cc∙ .
ImageNet, respectively. We present the values of ||b - u1 ||22, ||b - u2||22 and L1 + λL2 at different
iterations in attacking 8-bit quantized ResNet. Note that λ in the left figure is 100 and λ in the right
figure is 104 .
o.oιo
o.ooo
<q
=0.006
0.004
W 0.008
<邕 0.002
0	500	1000	1500
# iterations
12472.5
12470.0
12467.5
Z
12465.0 ⅛
十
12462.5 ςj
12460.0
12457.5
12455.0
exceed the maximum number of iterations (i.e., 2000). The numerical results demonstrate the fast
convergence of our method in practice.
E	Evaluation Setup
E.1 Baseline Methods
Since GDA (Liu et al., 2017a) and FSA (Zhao et al., 2019) are originally designed for attacking
the full-precision network, we adapt these two methods to attack the quantized network by applying
quantization-aware training (Jacob et al., 2018). We adopt the 'o-norm for FSA (LiU et al., 2017a)
and modification compression for GDA (Zhao et al., 2019) to reduce the number of the modified
parameters. Among three types of T-BFA (Rakin et al., 2020b), we compare to the most comparable
method: the 1-to-1 stealthy attack scheme. The purpose of this attack scheme is to misclassify
samples of a single source class into the target class while maintaining the prediction accuracy of
other samples. Besides, we take the fine-tuning (FT) of the last fully-connected layer as a basic
attack and present its results. We perform attack once for each selected image except TBT (Rakin
et al., 2020a) and totally 1,000 attacks on each dataset. The attack objective of TBT is that the
attacked DNN model misclassifies all inputs with a trigger to a certain target class. Due to such
objective, the number of attacks for TBT is equal to the number of target classes (i.e., 10 attacks on
CIFAR-10 and 50 attacks on ImageNet).
E.2 Target Models
According to the setting in (Rakin et al., 2020a;b), we adopt two popular network architectures:
ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) for evaluation. On CIFAR-10,
we perform experiments on ResNet-20 and VGG-16. On ImageNet, we use the pre-trained ResNet-
18* and VGG-16t network. We quantize all networks to the 4-bit and 8-bit quantization level using
the layer-wise uniform weight quantization scheme, which is similar to the one involved in the
Tensor-RT solution (Migacz, 2017).
E.3 Parameter Settings of TA-LBF
For each attack, we adopt a strategy for jointly searching λ and k. Specifically, for an initially given
k, we search λ from a relatively large initial value and divide it by 2 if the attack does not succeed.
The maximum search times of λ for a fixed k is set to 8. If it exceeds the maximum search times,
*Downloaded from https://download.pytorch.org/models/resnet18-5c106cde.pth
,Downloaded from https://download.pytorch.org/models/vgg16_bn-6c64b313.pth
15
Published as a conference paper at ICLR 2021
Table 4: Results of all attack methods against models trained with piece-wise clustering on CIFAR-
10 (bold: the best; underline: the second best). We adopt different clustering coefficients, including
0.0005, 0.005, and 0.01. The mean and standard deviation of PA-ACC and Nflip are calculated by
attacking the 1,000 images. Our method is denoted as TA-LBF. ∆Nflip denotes the increased Nflip
compared to the corresponding result in Table 1.
Clustering Coefficient	Method	ACC (%)	PA-ACC (%)	ASR (%)	Nflip	∆Nflip
	FT		84.28±3.49	100.0	1868.26±72.48	360.75
	TBT		87.97±i.75	66.1	250.30±10.97	3.60
0.0005	T-BFA	91.42	86.20±i.96	98.5	30.95±6.5o	21.04
	FSA		87.17±2.44	98.5	222.70±56.52	37.19
	GDA		85.28±4.16	100.0	41.33±12.84	14.50
	TA-LBF		87.92±2.54	100.0	13.47±5.34	7.90
	FT		81.08±3.61	97.9	1774.69±51.47	267.18
	TBT		82.96±2.18	12.7	246.80±16.06	0.10
0.005	T-BFA	88.03	80.80±2.64	98.1	61.72±12.17	51.81
	FSA		83.10±2.75	98.4	231.66±89.21	46.15
	GDA		79.23±6.25	99.9	64.87±22.78	38.04
	TA-LBF		83.63±3.47	100.0	25.52±11.59	19.95
	FT		78.73±3.54	98.3	1748.54±46.19	241.03
	TBT		79.86±2.04	10.1	236.50±10.93	-10.20
0.01	T-BFA	85.65	76.67±3.41	98.1	55.49±ii.77	45.58
	FSA		80.45±3.14	98.0	220.28±ioi.oi	34.77
	GDA		75.33±7.83	99.7	59.17±23.63	32.34
	TA-LBF		80.51±4.39	100.0	24.60±13.03	19.03
we double k and search λ from the relatively large initial value. The maximum search times of k
is set to 4. On CIFAR-10, the initial k and λ are set to 5 and 100. On ImageNet, λ is initialized
as 104 ; k is initialized as 5 and 50 for ResNet and VGG, respectively. On CIFAR-10, the δ in L1
is set to 10. On ImageNet, the δ is set to 3 and increased to 10 if the attack fails. u1 and u2 are
initialized as b and u3 is initialized as 0. z1 and z2 are initialized as 0 and z3 is initialized as 0. b is
initialized as b. During each iteration, the number of gradient steps for updating b is 5 and the step
size is set to 0.01 on both datasets. Hyper-parameters (ρ1, ρ2, ρ3) (see Eq. (11)) are initialized as
(10-4,10-4,10-5) on both datasets, and increase by Pi J Pi X 1.01, i = 1,2,3 after each iteration.
The maximum values of (ρ1 , ρ2, ρ3) are set to (50, 50, 5) on both datasets. Besides the maximum
number of iterations (i.e., 2000), We also set another stopping criterion, i.e., ||B — uι∣∣2 ≤ 10-4 and
∣∣b-U2∣∣2 ≤ ιo-4.
F More Results on Resistance to Defense Methods
F.1 Resistance to Piece-wise Clustering
We conduct experiments using the 8-bit quantized ResNet on CIFAR-10 With different clustering
coefficients. We set the maximum search times of k to 5 for clustering coefficient 0.005 and 0.01
and keep the rest settings the same as those in Section 4.1. The results are presented in Table 4.
As shoWn in the table, all values of Nflip are larger than attacking models Without defense for all
methods, Which is similar to Table 2. Our method achieves a 100% ASR With the feWest Nflip under
the three clustering coefficients. Although TBT obtains a smaller ∆Nflip than our method, it fails
to achieve a satisfactory ASR. For example, TBT achieves only a 10.1% ASR When the clustering
coefficient is set to 0.01. We observe that for all clustering coefficients, piece-Wise clustering reduces
the original accuracy. Such a phenomenon is more significant as the clustering coefficient increases.
The results also shoW that there is no guarantee that if the clustering coefficient is larger (e.g., 0.01),
the model is more robust, Which is consistent With the finding in (He et al., 2020).
16
Published as a conference paper at ICLR 2021
Table 5: Results of all attack methods against models with a larger capacity on CIFAR-10. We adopt
3× and 4× width networks. The mean and standard deviation of PA-ACC and Nflip are calculated
by attacking the 1,000 images. ∆Nflip denotes the increased Nflip compared to the corresponding
result in Table 1._________________________________________________________________
Model Width	Method	ACC (%)	PA-ACC (%)	ASR (%)	Nflip	∆Nflip
	FT		86.96±2.79	100.0	4002.52±281.24	2495.01
	TBT		90.67±5.23	74.1	504.70±20.44	258.00
3×	T-BFA	94.90	92.18±1.14	98.9	30.50±7.52	20.59
	FSA		91.40±2.38	99.0	342.20±79.44	156.69
	GDA		90.79±2.91	100.0	67.53±27.45	40.70
	TA-LBF		91.42±2.81	100.0	12.29±4.18	6.72
	FT		86.94±2.78	100.0	4527.68±369.35	3020.17
	TBT		85.39±5.08	90.1	625.50±32.38	378.80
4×	T-BFA	95.02	92.49±i.22	99.4	19.14±5.04	9.23
	FSA		91.60±2.42	98.7	338.93±100.12	153.42
	GDA		90.76±3.00	100.0	66.92±40.32	40.09
	TA-LBF		90.94±3.11	100.0	8.37±2.80	2.80
F.2 Resistance to Larger Model Capacity
Besides the results of networks with a 2× width shown in Section 4.3, we also evaluate all methods
against models with a 3× and 4× width. All settings are the same as those used in Section 4.1. The
results are provided in Table 5. Among all attack methods, our method is least affected by increasing
the network width. Especially for the network with a 4× width, our ∆Nflip is only 2.80. The results
demonstrate the superiority of the formulated BIP problem and optimization. Moreover, compared
with piece-wise clustering, having a larger model capacity can improve the original accuracy, but
increases the model size and the computation complexity.
G	Discussions
G. 1 Comparing Backdoor, Adversarial, and Weight Attack
An attacker can achieve malicious purposes utilizing backdoor, adversarial, and weight attacks. In
this section, we emphasize the differences among them.
Backdoor attack happens in the training stage and requires that the attacker can tamper the training
data even the training process (Liu et al., 2020b; Li et al., 2020). Through poisoning some training
samples with a trigger, the attacker can control the behavior of the attacked DNN in the inference
stage. For example, images with reflections are misclassified into a target class, while benign images
are classified normally (Liu et al., 2020a). However, such an attack paradigm causes the accuracy
degradation on benign samples, which makes it detectable for users. Besides, these methods also
require to modify samples in the inference stage, which is sometimes impossible for the attacker.
Many defense methods against backdoor attack have been proposed, such as the preprocessing-
based defense (Liu et al., 2017b), the model reconstruction-based defense (Liu et al., 2018a), and
the trigger synthesis-based defense (Wang et al., 2019).
Adversarial attack modifies samples in the inference stage by adding small perturbations that re-
main imperceptible to the human vision system (Akhtar & Mian, 2018). Since adversarial attack
only modifies inputs while keeping the model unchanged, it has no effect on the benign samples.
Besides the basic white-box attack, the black-box attack (Wu et al., 2020b; Chen et al., 2020) and
universal attack (Zhang et al., 2020b;a) have attracted wide attention. Inspired by its success in the
classification, it also has been extended to other tasks, including image captioning (Xu et al., 2019),
retrieval (Bai et al., 2020; Feng et al., 2020), etc.. Similarly, recent studies have demonstrated many
defense methods against adversarial attack, including the preprocessing-based defense (Xie et al.,
2018), the detection-based defense (Xu et al., 2017), and the adversarial learning-based defense
(Carmon et al., 2019; Wu et al., 2020c).
17
Published as a conference paper at ICLR 2021
Weight attack modifies model parameters in the deployment stage, which is the studied paradigm in
this work. Weight attack generally aims at misleading the DNN model on the selected sample(s),
while having a minor effect on other samples (Zhao et al., 2019; Rakin et al., 2020b). Many studies
(Yao et al., 2020; Breier et al., 2018; Pan, 2020) have demonstrated that the DNN parameters can
be modified in the bit-level in memory using fault injection techniques (Agoyan et al., 2010; Kim
et al., 2014; Selmke et al., 2015) in practice. Note that the defense methods against weight attack
have been not well studied. Although some defense methods (He et al., 2020) were proposed, they
cannot achieve satisfactory performance. For example, our method can still achieve a 100% attack
success rate against two proposed defense methods. Our work would encourage further investigation
on the security of the model parameters from both attack and defense sides.
G.2 Comparing TA-LBF with Other Weight Attacks
We compare our TA-LBF with other weight attack methods, including TBT (Rakin et al., 2020a), T-
BFA (Rakin et al., 2020b), GDA (Liu et al., 2017a), and FSA (Zhao et al., 2019) in this section. TBT
tampers both the test sample and the model parameters. Specifically, it first locates critical bits and
generates a trigger, and then flips these bits to classify all inputs embedded with the trigger to a target
class. However, the malicious samples are easily detected by human inspection or many detection
methods (Tran et al., 2018; Du et al., 2020). We do not modify the samples to perform TA-LBF,
which makes the attack more stealthy. Rakin et al. (2020b) proposed T-BFA which misclassifies
all samples (N-to-1 version) or samples from a source class (1-to-1 version) into a target class.
Our method aims at misclassifying a specific sample, which meets the attacker’s requirement in
some scenarios. For example, the attacker wants to manipulate the behavior of a face recognition
engine on a specific input. Since it affects multiple samples, T-BFA maybe not stealthy enough
in attacking real-world applications. GDA (Liu et al., 2017a) and FSA (Zhao et al., 2019) modify
model parameters at the weight-level rather than bit-level. They are designed for misclassifying
multiple samples from arbitrary classes, which makes it infeasible for them to only modify the
parameters connected to the source and target class. They modify more parameters than our method
as shown in the experiments, it might be due to the reason discussed above. Besides, TBT, T-BFA,
and GDA determine the critical weights to modify using heuristic strategies, while our TA-LBF
adopts optimization-based methods. Although FSA applies ADMM for solving the optimization
problem, it has no explicit constraint to control the number of modified parameters, which makes it
intends to modify more parameters than GDA and our TA-LBF.
H Trade-off between Three Evaluation Metrics
CIFAR-IO
等7
Figure 5: Curves of the trade-off between PA-ACC and Nflip and the trade-off between PA-ACC
and ASR for the proposed TA-LBF on two datasets.
In this section, we investigate the trade-off between three adopted evaluation metrics (i.e., PA-ACC,
ASR, and Nflip) for our attack. All experiments are conducted on CIFAR-10 and ImageNet dataset
in attacking the 8-bit quantized ResNet.
We firstly discuss the trade-off between PA-ACC and Nflip by fixing the ASR as 100% using the
search strategy in Appendix E.3 and adjusting the initial λ and k to obtain different attack results.
The two curves on the left show that increasing the Nflip can improve the PA-ACC when Nflip
is relatively small; the PA-ACC decreases with the increase of Nflip when Nflip is greater than a
threshold. This phenomenon demonstrates that constraining the number of bit-flips is essential to
ensure the attack stealthiness, as mentioned in Section 3.2. To study the trade-off between PA-ACC
18
Published as a conference paper at ICLR 2021
and ASR, we fix the parameter k as 10 for approximately 10 bit-flips and adjust the parameter λ to
obtain different PA-ACC and ASR results. The trade-off curves between PA-ACC and ASR show
that increasing ASR can decrease the PA-ACC significantly. Therefore, how to achieve high ASR
and PA-ACC simultaneously is still an important open problem.
19