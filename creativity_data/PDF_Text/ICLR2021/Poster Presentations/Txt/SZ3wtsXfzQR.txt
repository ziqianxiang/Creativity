Published as a conference paper at ICLR 2021
Theoretical bounds on estimation error
for meta-learning
James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi & Richard Zemel
Ab stract
Machine learning models have traditionally been developed under the assumption
that the training and test distributions match exactly. However, recent success in
few-shot learning and related problems are encouraging signs that these models
can be adapted to more realistic settings where train and test distributions differ.
Unfortunately, there is severely limited theoretical support for these algorithms
and little is known about the difficulty of these problems. In this work, we provide
novel information-theoretic lower-bounds on minimax rates of convergence for
algorithms that are trained on data from multiple sources and tested on novel
data. Our bounds depend intuitively on the information shared between sources
of data, and characterize the difficulty of learning in this setting for arbitrary
algorithms. We demonstrate these bounds on a hierarchical Bayesian model of
meta-learning, computing both upper and lower bounds on parameter estimation
via maximum-a-posteriori inference.
1	Introduction
Many practical machine learning applications deal with distributional shift from training to testing.
One example is few-shot classification (Ravi & Larochelle, 2016; Vinyals et al., 2016), where new
classes need to be learned at test time based on only a few examples for each novel class. Recently,
few-shot classification has seen increased success; however, theoretical properties of this problem
remain poorly understood.
In this paper we analyze the meta-learning setting, where the learner is given access to samples
from a set of meta-training distributions, or tasks. At test-time, the learner is exposed to only a
small number of samples from some novel task. The meta-learner aims to uncover a useful inductive
bias from the original samples, which allows them to learn a new task more efficiently.1 While
some progress has been made towards understanding the generalization performance of specific
meta-learning algorithms (Amit & Meir, 2017; Khodak et al., 2019; Bullins et al., 2019; Denevi et al.,
2019; Cao et al., 2019), little is known about the difficulty of the meta-learning problem in general.
Existing work has studied generalization upper-bounds for novel data distributions (Ben-David et al.,
2010; Amit & Meir, 2017), yet to our knowledge, the inherent difficulty of these tasks relative to the
i.i.d case has not been characterized.
In this work, we derive novel bounds for meta learners. We first present a general information
theoretic lower bound, Theorem 1, that we use to derive bounds in particular settings. Using this
result, we derive lower bounds in terms of the number of training tasks, data per training task, and
data available in a novel target task. Additionally, we provide a specialized analysis for the case
where the space of learning tasks is only partially observed, proving that infinite training tasks or data
per training task are insufficient to achieve zero minimax risk (Corollary 2).
We then derive upper and lower bounds for a particular meta-learning setting. In recent work, Grant
et al. (2018) recast the popular meta-learning algorithm MAML (Finn et al., 2017) in terms of
inference in a Bayesian hierarchical model. Following this, we provide a theoretical analysis of a
hierarchical Bayesian model for meta-linear-regression. We compute sample complexity bounds for
posterior inference under Empirical Bayes (Robbins, 1956) in this model and compare them to our
predicted lower-bounds in the minimax framework. Furthermore, through asymptotic analysis of the
error rate of the MAP estimator, we identify crucial features of the meta-learning environment which
are necessary for novel task generalization.
1 Note that this definition encompasses few-shot learning.
1
Published as a conference paper at ICLR 2021
Our primary contributions can be summarized as follows:
•	We introduce novel lower bounds on minimax risk of parameter estimation in meta-learning.
•	Through these bounds, we compare the relative utility of samples from meta-training tasks
and the novel task and emphasize the importance of the relationship between the tasks.
•	We provide novel upper bounds on the error rate for estimation in a hierarchical meta-linear-
regression problem, which we verify through an empirical evaluation.
2	Related work
An early version of this work (Lucas et al., 2019) presented a restricted version of Theorem 1.
The current version includes significantly more content, including more general lower bounds and
corresponding upper bounds in a hierarchical Bayesian model of meta-learning (Section 5).
Baxter (2000) introduced a formulation for inductive bias learning where the learner is embedded
in an environment of multiple tasks. The learner must find a hypothesis space which enables good
generalization on average tasks within the environment, using finite samples. In our setting, the
learner is not explicitly tasked with finding a reduced hypothesis space but instead learns using a
general two-stage approach, which matches the standard meta-learning paradigm (Vilalta & Drissi,
2002). In the first stage an inductive bias is extracted from the data, and in the second stage the
learner estimates using data from a novel task distribution. Further, we focus on bounding minimax
risk of meta learners. Under minimax risk, an optimal learner achieves minimum error on the hardest
learning problem in the environment. While average case risk of meta learners is more commonly
studied, recent work has turned attention towards the minimax setting (Kpotufe & Martinet, 2018;
Hanneke & Kpotufe, 2019; 2020; Mousavi Kalan et al., 2020; Mehta et al., 2012). The worst-case
error in meta-learning is particularly important in safety-critical systems, for example in medical
diagnosis.
Mousavi Kalan et al. (2020) study the minimax risk of transfer learning. In their setting, the learner
is provided with a large amount of data from a single source task and is tasked with generalizing
to a target task with a limited amount of data. They assume relatedness between tasks by imposing
closeness in parameter-space (whereas in our setting, we assume closeness in distribution via KL
divergence). They prove only lower bounds, but notably generalize beyond the linear setting towards
single layer neural networks.
There is a large volume of prior work studying upper-bounds on generalization error in multi-task
environments (Ben-David & Borbely, 2008; Ben-David et al., 2010; Pentina & Lampert, 2014;
Amit & Meir, 2017; Mehta et al., 2012). While the approaches in these works vary, one common
factor is the need to characterize task-relatedness. Broadly, these approaches either assume a shared
distribution for sampling tasks (Baxter, 2000; Pentina & Lampert, 2014; Amit & Meir, 2017), or
a measure of distance between distributions (Ben-David & Borbely, 2008; Ben-David et al., 2010;
Mohri & Medina, 2012). Our lower-bounds utilize a weak form of task relatedness, assuming that
the environment contains a finite set that is suitably separated in parameter space but close in KL
divergence—this set of assumptions also arises often when computing i.i.d minimax lower bounds
(Loh, 2017).
One practical approach to meta-learning is learning a linear mapping on top of a learned feature space.
Prototypical Networks (Snell et al., 2017) effectively learn a discriminative embedding function
and performs linear classification on top using the novel task data. Analyzing these approaches
is challenging due to metric-learning inspired objectives (that require non-i.i.d sampling) and the
simultaneous learning of feature mappings and top-level linear functions. Though some progress has
been made (Jin et al., 2009; Saunshi et al., 2019; Wang et al., 2019; Du et al., 2020). Maurer (2009),
for example, explores linear models fitted over a shared linear feature map in a Hilbert space. Our
results can be applied in these settings if a suitable packing of the representation space is defined.
Other approaches to meta-learning aim to parameterize learning algorithms themselves. Traditionally,
this has been achieved by hyper-parameter tuning (Rasmussen & Nickisch, 2010; MacKay et al.,
2019) but recent fully parameterized optimizers also show promising performance in deep neural
network optimization (Andrychowicz et al., 2016), few-shot learning (Ravi & Larochelle, 2016),
unsupervised learning (Metz et al., 2019), and reinforcement learning (Duan et al., 2016). Yet another
2
Published as a conference paper at ICLR 2021
approach learns the initialization of task-specific parameters, that are further adapted through regular
gradient descent. Model-Agnostic Meta-Learning (Finn et al., 2017), or MAML, augments the global
parameters with a meta-initialization of the weight parameters. Grant et al. (2018) recast MAML
in terms of inference in a Bayesian hierarchical model. In Section 5, we consider learning in a
hierarchical environment of linear models and provide both lower and upper bounds on the error of
estimating the parameters of a novel linear regression problem.
Lower bounding estimation error is a critical component of understanding learning problems (and
algorithms). Accordingly, there is a large body of literature producing such lower bounds (Khas’ min-
skii, 1979; Yang & Barron, 1999; Loh, 2017). We focus on producing lower-bounds for parameter
estimation using local packing sets, but expect that extending these results to density estimation or
non-parametric estimation is feasible.
3	Novel task environment risk
Most existing theoretical work studying out-of-distribution generalization focuses on providing
upper-bounds on generalization performance (Ben-David et al., 2010; Pentina & Lampert, 2014;
Amit & Meir, 2017). We begin by instead exploring the converse: what is the best performance we
can hope to achieve on any given task in the environment? After introducing notation and minimax
risks, we then show how these ideas can be applied, using meta linear regression as an example.
A full reference table for notation can be found in Appendix A and a short summary is given here.
We consider algorithms that learn in an environment (Z, P), with data domain Z = X × Y and P a
space of distributions with support Z. In the typical i.i.d setting, the algorithm is provided training
data S ∈ Zk, consisting of k i.i.d samples from P ∈ P.
In the standard multi-task setting, we sample training data from a set of training tasks
{P1 , . . . , PM+1} ⊂ P. We extend this to a meta-learning, or novel-task setting by first draw-
ing S1:M : n training data points from the first M distributions, for a total of nM samples. We
call this the meta-training set. We then draw a small sample of novel data, called a support set,
SM+1 ∈ Zk, fromPM+1.
Consider a symmetric loss function `(a, b) = ψ(ρ(a, b)) for non-decreasing ψ and arbitrary metric ρ.
We seek to estimate the output of θ : P → Ω, a functional that maps distributions to a metric space Ω.
For example, θ(P) may describe the coefficient vector ofa high-dimensional hyperplane when P is a
space of linear models, and ρ may be the Euclidean distance.
The i.i.d minimax risk Before studying the meta-learning setting, we first begin with a definition
of the i.i.d minimax risk that measures the worst-case error of the best possible estimator,
R* = inf sup Es-pk ['(θ(S),θp)] .	(1)
θ P∈p	L	」
For notational convenience, we denote the output of θ(P) by θP. The estimator for θ is denoted,
θ: Zk → Ω, and maps k samples from P to an estimate of θp.
Novel-task minimax risk In the novel-task setting, we wish to estimate θPM+1 , the parameters of
the novel task distribution PM+1. We consider two-stage estimators for θPM+1 . In the first stage,
the meta-learner uses a learning algorithm f : S±m→ Θs^m , that maps the meta-traιnιng set to an
estimation algorithm, Θsim : Zk → Ω. In the second stage, the learner computes Θsim (SM +ι),
the estimate of θPM+1 .
The novel-task minimax risk is given by,
RP(β) = f f	sup E S1：M~PnM [' ((f(Sl:M))(SM +i),Θpm+i)],	⑵
P1,…,pM + 1∈QP SM + 1~pM +1
where QβP = {(P1, . . . , PM+1) ∈ P : DKL (PM+1kPi) ≤ β, for i = 1, . . . , M}. This ensures a
degree of relatedness between the novel and meta-training tasks.
The estimator for θM+1 now depends additionally on the Mn samples in S1:M, where only k Mn
samples from PM +ι are available to the learner. Thus, RP addresses the domain shift expected at
3
Published as a conference paper at ICLR 2021
Figure 1: Meta-learning 1D-regression: The parameters of a 1D regression model are fitted from a
small support set. The training distributions (P1, P2, P3) give a useful inductive bias for fitting P4
using only 5 points. The MLE solution on the novel task for those 5 points is also displayed.
test-time in the meta-learning setting and allows the learner to use data from multiple tasks. The goal
of f is to learn an inductive bias from S1:M such that a good estimate is possible with only k data
points from PM+1. In this setting, k is equivalent to the number of shots in few-shot learning.
An example with meta-linear regression We present here a short summary based on meta linear
regression, which we will analyze in more detail in Section 5.
In Figure 1, we show observed data samples from a family of polynomial regression models. Our aim
is to output an algorithm which recovers the parameters of a new polynomial function from limited
observations-we choose a MAP estimator which is described fully in Section 5. In the bottom right,
we are given only 5 data points from a novel task distribution and estimate the parameters of the
model with both the MLE and MAP estimators — the MLE overfits the support set while the MAP
estimator is close to the true function.
In terms of the terminology used above, the set,
P = {pθ (y) = N (x>θ, σ2) : θ ∈ Rd, x = [1, x, . . . , xd-1]},
is the space of polynomial regression models, parameterized by θ. For this problem, we take
`(θ, θ) = kθ - θk22. In Figure 1, tasks are generated with p(θ) = N(τ, σθ2), for unknown, sparse,
τ ∈ Rd. Thus, each model is a polynomial function with few large coefficients. The algorithm f,
first takes samples from P1,P2 ,P3 and computes an estimate, τ. This estimate of τ is then used
to compute Θ(Sm+1； τ) = argmaxθ4 p(θ4∣τ, SM +1). Note that this approach is able to learn the
correct inductive bias from the data, without requiring a carefully designed regularizer. The lower
bounds we derive in Section 4 can be applied to problems of this general type, and the upper and
lower bounds in Section 5 apply specifically to meta-learning linear regression. 4
4 Information theoretic lower bounds on novel task
GENERALIZATION
In this section, we first present our most general result: Theorem 1. Using this, we derive Corollary 1
that gives a lower bound in terms of the sample size in the training and novel tasks. Corollary 1
recovers a well-known i.i.d lower bound (Theorem 2) when Mn = 0, and, importantly, highlights
that the novel task data is significantly more valuable than the training task data. Additionally, we
provide a specialized bound that applies when the environment is partially observed — proving that
in this setting training task data is insufficient to drive the minimax risk to zero.
4
Published as a conference paper at ICLR 2021
In Theorem 1, we assume that P contains J distinct 2δ-separated distributions but only M + 1 ≤ J
tasks are visible to the learner. Intuitively, the error rate lower-bound shrinks as the amount of
information shared between the training tasks and the novel task grows. All proofs are given in
Appendix B.1. Recall `(a, b) = ψ(ρ(a, b)) for non-decreasing ψ and arbitrary metric ρ.
Theorem 1 (Minimax novel task risk lower bound). Let J ⊂ P contain J distinct distributions such
that ρ(θP, θP0) ≥ 2δ and DKL (P kP0) ≤ β for all P, P0 ∈ J. Let π be a random ordering of the
J elements, and Z∣π be a vector of k i.i.d Samplesfrom P∏m+i . Further, define W∣π to be an n X M
matrix whose jth column consist of n i.i.d samples from Pπj. Then,
RP(β) ≥ ψ(δ) (1 - I(nM+1； W)+I(nM+1； Z) + 1).
P	log2 J
Note that δ is a property of the so-called packing set, J , and may depend on the sample size, β, and
other properties of P. For example, practical instances of this bound typically require ψ(δ) = O(1/k)
or similar, as in Theorem 3 below. To derive this result, we bound the statistical estimation error by
the error on a corresponding decoding problem where we must predict the novel task index, given the
meta-training set S1:M and SM+1. Fano’s inequality provides best-case error rates for this problem.
Using Theorem 1, we derive our first bound on the novel-task minimax risk that depends on the
number of meta-training tasks (M) and datapoints per training task (n, k), via a local-packing
argument. The following corollary implies that if we have J meta-training tasks in our 2δ-packing
that are close (in terms of their pairwise KL distance), then learning a novel task from training
samples drawn from the meta-training tasks requires significantly more examples; in particular,
learning the novel task from samples drawn from the meta training set requires Ω( J) times the sample
complexity of the novel task. This matches our intuition that learning the novel task implies the
ability to distinguish it from all J well-separated meta-training tasks.
Corollary 1. Assume the same setting as in Theorem 1. Then,
RP(β) ≥ ψ(δ)
1-
1 + ((Mn) + k) J2 Pl≤i,j≤J DKL (PikPj)
lθg2 J
RP ≥ ψ(δ)
A tighter bound on partially observed environments We now consider the special case of Theo-
rem 1 when M < J - 1, meaning that the meta-training tasks cannot cover the full packing set. In
this setting, we prove that no algorithm can generalize perfectly to tasks in unseen regions of the
space with small k, regardless of the number of data points n observed in each meta-training task.
Corollary 2. Assume the same setting as in Theorem 1, with M + 1 < J. Then,
log2(J- M)- Jk Pl≤i,j≤J DKL (PikPj) - 1 !
lθg2 J	..
In this work, we have focused on the setting where W contains an equal number of samples from
each of the meta-training tasks — this is the sampling scheme shown in Figure 2. However, it is
possible to extend these results to different sampling schemes for W . For example, in the appendix
We derive bounds with W|n as a mixture distribution. Surprisingly, despite task identity being hidden
from the learner, the asymptotic rate for these two sampling schemes match.
4.1	Measuring Task-Relatedness
The use of local packing requires the design ofan appropriate set of distributions whose corresponding
parameters are 2δ-separated but maintain small KL divergences. In the multi-task setting such an
assumption is intuitively reasonable: challenging tasks should require separated parameters for ideal
explanations (2δ-separated) but should satisfy some relatedness measure (small KL). Importantly,
these parameters can depend on sample size and other problem-specific variables. As we will see
shortly, lower bounds on minimax risk in the i.i.d setting may also assume the same notion of
relatedness for the local-packing in P .
Task relatedness is a necessary feature for upper-bounds on novel task generalization, but are
typically difficult to define (see e.g. Ben-David & Borbely (2008)). Our lower bounds utilize a
5
Published as a conference paper at ICLR 2021
R ≥ ψ(δ)	1 -
relatively weak notion of task-relatedness, and thus may be overly pessimistic compared to the upper
bounds computed in existing work. However, task relatedness of this form can be formulated in a
representation space shared across tasks and thus can be applied in settings like those explored by e.g.
Du et al. (2020). Deriving lower bounds under the different task relatedness assumptions present in
the literature would make for exciting future work.
4.2	COMPARISON TO RISK OF i.i.d LEARNERS
From the statement of Theorem 1 it is not clear how this lower-bound compares to that of the i.i.d
learner which has access only to the k samples from SM+1. To investigate the benefit of additional
meta-training tasks, we compare our derived minimax risk lower bounds to those achieved by i.i.d
learners. To do so, we revisit standard minimax lower bounds that can be found in e.g. Loh (2017).
Theorem 2 (IID minimax lower-bound). Suppose {P1, . . . , PJ} ⊆ P satisfy ρ(θPi , θPj ) ≥ 2δ for
all i 6= j . Then,
J Pι≤ij≤J DKL (PikP∙) + 1!
lθg2 J	. ,
We include a proof of this result in Appendix B.1, using local-packing as in our meta-learning bounds.
As hoped, Corollary 1 recovers Theorem 2 when there are no training tasks available. Moreover, this
i.i.d bound is strictly larger than the one computed in Corollary 1 in general. Note that while this i.i.d
minimax risk is asymptotically tight for several learning problems (Loh, 2017; Raskutti et al., 2011),
there is no immediate guarantee that the same is true for our meta-learning minimax bounds. We
investigate the quality of these bounds by providing comparable upper bounds in the next section.
5 Analysis of a hierarchical Bayesian model of meta-learning
Our goal is to analyze the sample complexity of meta-learning for linear regression, where samples
are drawn from multiple meta-training tasks and we want to generalize to a new task with only a few
data points. After introducing the setting, we will compute lower-bounds on the minimax risk using
our results from Section 4, revealing a 2d scaling on the meta-training sample complexity. Following
the lower bound, we derive an accompanying upper-bound on the risk of a MAP estimator, derived
from an empirical Bayes estimate over a hierarchical Bayesian model. Asymptotic analysis of this
bound reveals that if the observed samples from the novel task vary considerably more than the task
parameters, then observing more meta-training samples may significantly improve convergence in
the small k regime. This is validated empirically in Section 6.
For i = 1...M + 1, where M + 1 is the total number of tasks, we define,
yi = Xiθi + i,	Xi ∈ Rni ×d, yi ∈ Rni , i ∈ Rni
G 〜N(0,σ2l),	σi ∈ R+
Each task has some design matrix Xi and unknown parameters θi . For simplicity, we assume known
isotropic noise models and that ni = n for all i ≤ M , with nM+1 = k.
Our meta learner will fit the data using an empirical Bayes estimate in a hierarchical Bayesian model:
θi = T + ξ, T ∈ Rd, ξ ∈ Rd, ξ 〜N(0,σ2l), σθ ∈ R+
We will consider the Maximum a Posterior estimator,
Θm+1 = argmax p(Θm+ι∣yι,∙∙∙, YM+1),
θM+1
and will characterize its risk, E[∣∣Θm+1 - Θm+ι∣∣2], where the expectation is with respect to
sampled data only. The posterior distribution under the Empirical Bayes estimate for T is given in
Appendix C.2. The derivation is standard but dense and we recommend dedicated readers to consult
Gelman et al. (2013), or an equivalent text, for more details.
6
Published as a conference paper at ICLR 2021
5.1	Minimax lower bounds
We now compute lower bounds for parameter estimation with meta-learning over multiple linear
regression tasks. Beginning with a definition of the space of data generating distributions,
PLR = {pθ(y) = N (Xθ, σ2I) : θ ∈ B2(1), X ∈ Rn×d.}
where θ are the parameters to be learned, and X is the design matrix of each linear regression task in
the environment. We write Y = maxi σmaχ(Xi/√n), which we assume is bounded for all X and n
(an assumption that is validated for random Gaussian matrices by Raskutti et al. (2011)).
Theorem 3 (Meta linear regression lower bound). Consider PLR defined as above and let `(a, b) =
(Ila — b∣∣2)2 ∙ If d ≥ 2 and 2-dM + kn-1 ≥ max{卷，dσ2∕(256γ2n) ,then,
/	dσ2
≥	∖γ2(2-dnM + k)
The proof is given in Appendix B.5. We see that the size of the meta-training set has an inverse expo-
nential scaling in the dimension, d. This reflects the complexity of the space growing exponentially
in dimensions and the need for a matching growth in data size to cover the environment sufficiently.
5.2 Minimax upper bounds
To compute upper bounds on the estimation error, we require an additional assumption. Namely,
we will assume that the design matrices also have bounded minimum singular values, 0 < s ≤
σmin(X∕√n) (see RaSkUttietal.(2011)for somejustifiCation). For the upper-bounds, We allow the
bounds on the singular values of the design matrices and the observation noise in the novel task to be
different than those in the meta-training tasks. We note that we can still recover the setting assumed
in the lower bounds, where all tasks match on these parameters, as a special case.
The learner observes n data points from each linear regression model in {Pθ1 , . . . , PθM} ⊂ P. We
then bound the error of estimating θM+1, for which k samples are available.
The expected error rate of the MAP estimator can be decomposed as the posterior variance and bias
squared. In the appendix we provide a detailed derivation of these results. The bound depends on
dimensionality d, the observation noise in each task σi2, the number of tasks M, the number of data
points in each meta-training task n, and the number of data points in the novel task k.
Theorem 4 (Meta Linear Regression Upper Bound). Let Θm+1 be the maximum-a-posteriori estima-
tor, μθM + 1 | Y1:M + 1 , Then,
RPLR ≤	SUp	E[kθM+1 - θM+1k2] ≤ O (dσM+ιC(M,n,k12D(M,n,k^
θ1,...,θM+1∈B2(1)
where,
C(M, n,k) = k + n(M+κ2)s2 IA , and, D(M, n, k) = [k + (n + Aι)(Mn + AG
Expectations are taken over the data conditioned on θ1, . . . , θM+1. Additional terms not depending
on d, M, n, k are defined in Appendix C.2.
While the bounds presented in Theorem 4 are relatively complicated, we can probe the asymptotic
convergence of the MAP estimator to the true task parameters, θM+1. In the following section, we
will discuss some of the consequences of this result and its implications for our lower bounds.
5.3 Asymptotic behavior of the MAP estimator
We first notice that when k is small, the risk cannot be reduced to zero by adding more meta-training
data. Recent work has suggested such a relationship may be inevitable (Hanneke & Kpotufe, 2020).
Our lower bound presented in Corollary 2 agrees that more samples from a small number of meta-
training tasks will not reduce the error to zero. However, unlike our lower bounds based on local
7
Published as a conference paper at ICLR 2021
dσM2 +1
2α2M
+ M + κ2
-1
packing, the lower bounds presented in this section predict that if the meta-training tasks cover the
space sufficiently then an optimal algorithm might hope to reduce the error entirely with enough
samples. We hypothesize that this gap is due to limitations in the standard proof techniques we utilize
for the lower-bounds when the number of tasks grows, and expect a sharper bound may be possible.
To emulate the few-shot learning setting where k is relatively small, we consider n → ∞, with k and
M fixed. In this case, the risk is bounded as,
SUp	E[∣∣Θm+1 - Θm+1k2] ≤ O
θ1 ,...,θM+1∈B2 (1)
where α2 = σM+ι∕σθ, is the ratio of the observation noise to the variance in sampling θ, and K is
the condition number of the design matrices. This leads to a key takeaway: if the observed samples
from PM+1 vary considerably more than the parameters θ, then observing more samples in S1:M
will significantly improve convergence towards the true parameters in the small k regime. Further,
adding more tasks (increasing M) also improves these constant factors by removing the dependence
on the condition number, κ.
6	Empirical Investigations
In this section, we provide additional quantitative exploration of the upper bound studied in Section 5.
The aim is to take steps towards relating the bounds to experimental results; we know of little
theoretical work in meta-learning that attempt to relate their results to practical empirical datasets.
Full details of the experiments in this section can be found in Appendix D.
6.1	Hierarchical Bayes polynomial regression
We first focus on the setting of polynomial regression over inputs in the range [-1, 1]. Some examples
of these functions and samples are presented in Figure 1, alongside the MAP and MLE estimates for
the novel task.
Figure 2 shows the analytical expected error rate (risk) under various environment settings. We
observe that even in this simple hierarchical model, the estimator exhibits complex behavior that is
correctly predicted by Theorem 4. In Figure 2A, we varied the novel task difficulty by increasing
the novel task observation noise (σM2 +1). We plot three curves for three different dataset size
configurations. When the novel task is much noisier than the source tasks, it is greatly beneficial to
add more meta-training data (blue vs. red). And while larger k made little difference when the novel
task was relatively difficult (blue vs. green), the expected loss was orders of magnitude lower when
the novel task became easier. In Figure 2B, we fixed the relative task difficulty and instead varied k
and M . The x-axis now indicates the total data Mn + k available to the learner. We observed that
adding more tasks has a large effect in the low-data regime but, as predicted, the error has a non-zero
asymptotic lower-bound — eventually it is more beneficial to add more novel-task data samples.
These empirical simulations verify that our theoretical analysis is predictive of the behavior of this
meta learning algorithm, as each of these observations can be inferred from Theorem 4. While this
model is simple, it captures key features of and provides insight into the more general meta-learning
problem. We also explored a non-linear sinusoid meta-regression problem (Finn et al., 2017), finding
that our theory is largely predictive of the general trends in this setting too.
6.2	Sinusoid regression with MAML
Following the connections between MAML and hierarchical Bayes explored by Grant et al. (2018),
we also explored regression on sinusoids using MAML. Our aim was to investigate how predictive
our linear theory is for this highly non-linear problem setting. As in Finn et al. (2017), we sample
sinusoid functions by placing a prior over the amplitude and phase. In other works (Finn et al.,
2017; Grant et al., 2018) the same prior is used for the training and testing stages. However, to
better measure generalization to novel tasks we use different prior distributions when training versus
evaluating the model.
8
Published as a conference paper at ICLR 2021
Expected error over varying novel task difficulty
(A)
Figure 2: The expected error rate of the hierarchical MAP estimator, GM +1, over different environ-
ment hyperparameter settings. A) The novel task observation noise is increased, making the novel
task harder to learn. B) We increase the size of the dataset, in one case adding new tasks (M) and in
the other adding new novel task data samples (k).
Expeded error over varying dataset sizes
2000	4000	6000	80∞	10000	12000	14000
Total data samples (Mn + k)
(B)
We display the risk averaged over 30 trials in Fig-
ure 3. We varied the novel task difficulty by in-
creasing the observation noise in the novel task.
We plot separate curves for different dataset size
configurations, and observe that the empirical re-
sults align fairly well with the results derived
by sampling the hierarchical model (Figure 2A).
Adding more meta-training data (increasing n) is
beneficial (green vs. yellow) and adding more test
data-points (higher k) is also beneficial (red vs.
green). Here however, these relationships did not
interact with the task difficulty, as the wins for in-
creased meta-training and meta-testing data were
consistent, until task noise prevents any setting of
the model from performing the task.
Figure 3: Average risk for regressing sinusoid
functions with MAML.
7	Conclusion
Meta-learning algorithms identify the inductive bias from source tasks and make models more
adaptive towards unseen novel distribution. In this paper, we take initial steps towards characterizing
the difficulty of meta-learning and understanding how these limitations present in practice. We have
derived both lower bounds and upper bounds on the error of meta-learners, which are particularly
relevant in the few-shot learning setting where k is small. Our bounds capture key features of the
meta-learning problem, such as the effect of increasing the number of shots or training tasks. We
have also identified a gap between our lower and upper bounds when there are a large number of
training tasks, which we hypothesize is a limitation of the proof technique that we applied to derive
the lower bounds — suggesting an exciting direction for future research.
8	Acknowledgements
This work benefited greatly from the input of many other researchers. In particular, we extend our
thanks to Shai Ben-David, Karolina Dziugaite, Samory Kpotufe, and Daniel Roy for discussions and
feedback on the results presented in this work. We thank Ahmad Beirami and anonymous reviewers
for their valuable feedback that led to significant improvements to this paper. We also thank Elliot
Creager, Will Grathwohl, Mufan Li, and many of our other colleagues at the Vector Institute for
feedback that greatly improved the presentation of this work. Resources used in preparing this research
were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and
companies sponsoring the Vector Institute (www.vectorinstitute.ai/partners).
9
Published as a conference paper at ICLR 2021
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-bayes theory.
arXiv preprint arXiv:1711.01244, 2017.
Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Matthew W. Hoffman, David Pfau,
Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In
Advances in Neural Information Processing Systems 29,pp. 3981-3989, 2016.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable multiple-
task learning guarantees. Machine learning, 73(3):273-287, 2008.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Brian Bullins, Elad Hazan, Adam Kalai, and Roi Livni. Generalize across tasks: Efficient algorithms
for linear representation learning. In AUrelien Garivier and Satyen Kale (eds.), Proceedings of
the 30th International Conference on Algorithmic Learning Theory, volume 98 of Proceedings
of Machine Learning Research, pp. 235-246, Chicago, Illinois, 22-24 Mar 2019. PMLR. URL
http://proceedings.mlr.press/v98/bullins19a.html.
Tianshi Cao, Marc Law, and Sanja Fidler. A theoretical analysis of the nUmber of shots in few-shot
learning. arXiv preprint arXiv:1909.11722, 2019.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
GiUlia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regUlarization. In Kamalika ChaUdhUri and RUslan SalakhUtdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volUme 97 of
Proceedings of Machine Learning Research, pp. 1566-1575, Long Beach, California, USA, 09-15
JUn 2019. PMLR.
Simon S DU, Wei HU, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter AbbeeL R1$八2$:
Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.
Robert Fano. Transmission of information. A Statistical Theory of Communication, 1961.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.
Bayesian data analysis. Chapman and Hall/CRC, 2013.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical Bayes. arXiv preprint arXiv:1801.08930, 2018.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. In Advances in
Neural Information Processing Systems 32, pp. 9871-9881. Curran Associates, Inc., 2019.
Steve Hanneke and Samory Kpotufe. A no-free-lunch theorem for multitask learning. arXiv preprint
arXiv:2006.15785, 2020.
Rong Jin, Shijun Wang, and Yang Zhou. Regularized distance metric learning:theory and algorithm.
In Advances in Neural Information Processing Systems 22, pp. 862-870, 2009.
Rafail Z Khas’ minskii. A lower bound on the risks of non-parametric estimates of densities in the
uniform metric. Theory of Probability & Its Applications, 23(4):794-798, 1979.
10
Published as a conference paper at ICLR 2021
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. arXiv preprint arXiv:1902.10644, 2019.
Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the benefits of labels in covariate-
shift. In Sebastien Bubeck, Vianney PercheL and PhiliPPe Rigollet (eds.), Proceedings of the 31st
Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp.
1882-1886. PMLR, 06-09 Jul 2018.
Po-Ling Loh. On lower bounds for statistical learning theory. Entropy, 19(11):617, 2017.
James Lucas, Mengye Ren, and Richard Zemel. Information-theoretic limitations on novel task
generalization. Neurips 2019 Workshop on Machine Learning with Guarantees, 2019.
Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger B. Grosse. Self-tuning
networks: Bilevel oPtimization of hyPerParameters using structured best-resPonse functions. In
7th International Conference on Learning Representations, 2019.
Andreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327-350,
2009.
Nishant Mehta, Dongryeol Lee, and Alexander Gray. Minimax multi-task learning and a generalized
loss-comPositional Paradigm for mtl. Advances in Neural Information Processing Systems, 25:
2150-2158, 2012.
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning uPdate
rules for unsuPervised rePresentation learning. In 7th International Conference on Learning
Representations, 2019.
Mehryar Mohri and Andres Munoz Medina. New analysis and algorithm for learning with drifting
distributions. In International Conference on Algorithmic Learning Theory, PP. 124-138. SPringer,
2012.
Mohammadreza Mousavi Kalan, Zalan Fabian, Salman Avestimehr, and Mahdi Soltanolkotabi.
Minimax lower bounds for transfer learning with linear and one-hidden layer neural net-
works. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, PP. 1959-1969. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
151d21647527d1079781ba6ae6571ffd-Paper.pdf.
Anastasia Pentina and ChristoPh LamPert. A PAC-bayesian bound for lifelong learning. In Interna-
tional Conference on Machine Learning, PP. 991-999, 2014.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high-dimensional
linear regression over '_q-balls. IEEE transactions on information theory, 57(10):6976-6994,
2011.
Carl Edward Rasmussen and Hannes Nickisch. Gaussian Processes for machine learning (GPML)
toolbox. J. Mach. Learn. Res., 11:3011-3015, 2010.
Sachin Ravi and Hugo Larochelle. OPtimization as a model for few-shot learning. International
Conference on Learning Representations, 2016.
Herbert Robbins. An emPirical bayes aPProach to statistics. In Proceedings of the Third Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, PP. 157-163, Berkeley, Calif., 1956. University of California Press.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh KhandeParkar. A
theoretical analysis of contrastive unsuPervised rePresentation learning. In International Confer-
ence on Machine Learning, PP. 5628-5637, 2019.
Jake Snell, Kevin Swersky, and Richard Zemel. PrototyPical networks for few-shot learning. In
Advances in Neural Information Processing Systems, PP. 4077-4087, 2017.
11
Published as a conference paper at ICLR 2021
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial
intelligence review,18(2):77-95, 2002.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
John Von Neumann. Some matrix-inequalities and metrization of matric space. 1937.
Boyu Wang, Hejia Zhang, Peng Liu, Zebang Shen, and Joelle Pineau. Multitask metric learning:
Theory and algorithm. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of
Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pp. 3362-
3371. PMLR, 16-18 Apr 2019.
Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-
gence. Annals of Statistics, pp. 1564-1599, 1999.
12
Published as a conference paper at ICLR 2021
A Notation
X
Y
Z
P
J
P
Pk
P1:M
P1:M
Ω
θ
ʌ
θ
I(X;Y)
S, SM+1
SP
[N], for N ∈ N
Bp(r)
Description
The domain of the data, e.g. Rd
The range of the data, e.g. R
The product space X × Y
A collection of distributions over Z
A (finite) subset of distributions in P
An element of P
The product distribution, whose sam-
ples correspond to k independent draws
from P
The product distribution, ΠiM=1Pi, for
Pi ∈ P
The mixture distribution,吉 PM=I Pi,
for Pi ∈ P
A metric space, containing parameters
for each distribution
A functional, mapping distributions in
P to parameters in Ω
An estimator θ : Zn → F
The mutual information between ran-
dom variables X and Y .
Denotes training datasets drawn i.i.d
from some P ∈ P. Typically S =
{z1 , . . . , zn}, SM +1 = {z10 , . . . , zk0 }
Denotes a meta-training set drawn i.i.d
from P1:M
Indicates the set {1, . . . , N}
The p-norm ball of radius r, centered at
0.
Table 1: Summary of notation used in this manuscript
B Lower Bound Proofs
We will make use of several standard results below, which we present here.
Lemma 1. Fano’s Inequality (Fano, 1961; Cover & Thomas, 2012) For any estimator Y of a
random variable Y such that Y → Z → Y forms a Markov chain, it holds that,
,ʌ
P(Y = Y) ≥
H (Y |Z) - 1 _ H (Y) - I (Y; Z) - 1
i0g2 ∣y∣	=	i0g2 |YI
Lemma 2. Mutual information equality (Khas’ minskii, 1979) Consider random variables
Z1 , Z2 , Y, then,
I(Y;(Z1,Z2))+I(Z1;Z2) =I(Z1;(Z2,Y))+I(Z2;Y)
Lemma 3. Local packing lemma (Loh, 2017) Consider distributions P1 , . . . , PJ ∈ P. Let Y be a
random variable distributed uniformly on [J] and let Z|{Y = j} be a vector of k i.i.d samples from
13
Published as a conference paper at ICLR 2021
Pj. Then,
I(Y; Z) ≤ J X DKL (PikPj).
1≤i,j≤J
We will require a novel local packing bound for the novel-task risk, which we present in Lemma 5.
B.1 IID Lower Bound
We first prove the i.i.d result, which will serve as a guide for our novel lower bounds.
Theorem 2 (IID minimax lower-bound). Suppose {P1, . . . , PJ} ⊆ P satisfy ρ(θPi , θPj ) ≥ 2δ for
all i 6= j . Then,
J Pl≤j,j≤J DKL (PikPj) + 1!
lθg2 J	).
R ≥ ψ(δ)	1 -
Proof. First, notice that,
sup ES〜Pk ['(θ(S),θp)]
P∈P
1J
≥ J X ES〜Pik ['(θ(S),θPi)].
i=1
Now define the decision rule,
ʌ , .-
f (S) = argmin ρ(θ(S),θp7-)],
1≤j≤J
with ties broken arbitrarily. We proceed by bounding the expected loss. First, using Markov’s
inequality,
ES〜Pik h'(θ(S),θpi)i ≥ Ψ(δ)Ps〜Pik [ψ(ρ(θ(S),θPi)) ≥ ψ(δR ,
=ψ(δ)Ps^P,k IPe(S ),θPi) ≥ δi.
Next, consider the case ρ(θ(S), θi)) < δ. Through the triangle inequality,
ʌ , ʌ ,
ρ(θ(S),θP7-) ≥ ρ(θPi,θPj)-ρ(θ(S),0PJ
ʌ ,
≥ 2δ - δ>ρ(θ(S),θPi)
Thus, the probability that the distance is less than δ is at least as large as the probability that the
estimator is correct.
ψ(δ)Ps〜Pik h'(θ(S),θPi) ≥ ψ(δ)i ≥ ψ(δ)P(f(S) = i).
Now, using Fano,s inequality with Y = ∏m+i, and Y = f (S) (and the corresponding Markov chain
πM+1 → S → f(S)), we have,
1
J
J
X P(f(S) 6=i) ≥
i=1
log2 J - I(πM+1; Z) - 1
log2 J
Combining the above inequalities with the Local Packing Lemma gives the final result.	□
B.2	Proof of Theorem 1
Theorem 1 (Minimax novel task risk lower bound). Let J ⊂ P contain J distinct distributions such
that ρ(θP, θP0) ≥ 2δ and DKL (PkP0) ≤ β for all P, P0 ∈ J. Let π be a random ordering of the
J elements, and Z∣π be a vector of k i.i.d Samplesfrom P∏m+i . Further, define W∣π to be an n X M
matrix whose jth column consist of n i.i.d samples from Pπj. Then,
RP (β) ≥ ψ(δ)(1 - I EM+1； W)+I(；M +1； Z) + 1 ).
14
Published as a conference paper at ICLR 2021
Proof. As in the i.i.d case, we first bound the supremum from below with an average,
1J 1
0 sup	OESIM 〜(PIM )n WθSIM (SM +1),θ(P0))] ≥ J XTJ-Iy	X	Ew 〜W ∣π 卜(θw (z),θi)],
P1,…，PM+1∈P Sm+iMP；)	i=1 M M ∏ ∏∣{∏M+ι=i} z~Zlπ
where the inner sum is over all length M orderings, π with πM+1 = i.
As before, we consider the following estimator,
f(W,Z) = argmin ρ(θw (Z),θ(Pj))
1≤j≤J
Using Markov’s inequality, and then following the proof of Theorem 2, we have,
J X 7J-K	X	Ew~w∣∏h'(θw(z),θi)i ≥ J X 7J-κ	X	Ψ(δ)P[f (W,Z) = i∣∏]
i=1 (MJ ∏∣{∏M+ι=i} z~Z|n	i=1 (MJ π∣{∏m+ι=i}
= ψ(δ)P[f (W, Z) 6= πM+1]
ψ(δ)	1 -
with the use of Fano’s inequality, we arrive at,
I(πM+1； (W, ZA + 1 ʌ
lθg2 J )
Conditioned on Y , each element of W and Z are independent but they are not identically distributed.
Thus, with the application of Lemma 2,
I(πM+1; (W, Z)) ≤ I(πM+1; Z) + I(πM+1; W)
The result follows by combining these inequalities.	□
Remark In the above proof of Theorem 1, we did not need to make use of the form of the
distribution of W |Y = i, only that the correct graph structure was observed. This grants us some
flexibility, which we utilize later in Section B.4 to prove lower bounds for mixture distributions.
We now proceed with proofs of the corollaries of Section 4.
B.3	Local packing results
Lemma 4 (Meta-learning local packing). Consider the same setting as in Theorem 1, then
I(πM+1； W) ≤ J2(J- 1) X DKL (PikPj)
J (J - 1) 1≤i,j≤J
Proof. There are (J - 1)!/(J - M - 1)! orderings on the first M indices, given the (M + 1)th. We
introduce the following notation,
P- = (J(JM-!1)!	X P(W∣∏)
∏∣∏M+ι=i
1J
P = JX P-
i=1
As in previous proofs, we notice that we can write,
J - 1	1
J J - 1
P
EP-
j6=i
15
Published as a conference paper at ICLR 2021
First, note that we can upper bound I(πM+1; W) ≤ nI(πM+1; w), where w denotes a single row in
W . Further,
"	、	1 Jk i	P-i
I(∏m+1； W) = J 2^Elog -P-
i=1
=J XX DKL(P-ik J P-i + J J-IX P-j
i=1	j6=i
J J1 1
≤ J X JDKL (P-ikP-i) + j DKL ( P-ik 口 X P-j
i=1	j 6=i
≤ JJ⅛ X DKL (P-ikP-j)
J(J-) 1≤i6=j≤J
We will use convexity of the KL divergence to upper bound this quantity. Each distribution P- is an
average over a random selection of index orderings.
When applying convexity, all pairs of selections that exactly match will lead to a KL divergence
of zero. There are the same number of these in each component of P-i. Thus we care only about
selections that contain either j or i such that matching pairs of distributions exactly is not possible.
Further, we need only consider pairs of product distributions who differ only in a single, identical
position.
Each of the above described pairs of distributions has KL divergence equal to DKL (Pj kPi). We
conclude by counting the total number of orderings producing such pairs. First, there are M choices
for the index of Pj and Pi. Then, there are (J - 2)!/(J - M -1)! total orderings of the remaining
M elements. Thus, we have,
I(∏m +ι； W) ≤
1
J(j -I)
E DKL (P-ikP-j)
1≤i6=j≤J
≤	1	(J - M - 1)! M(J - 2)! X D (P k p )
≤ J(J - 1) (J - 1)! (J - M - 1)! 1≤i=j≤JKL ( jk i)
J^ 1≤X≤JDKL (PjkPi)
□
These results together provide an immediate proof of Corollary 1.
Corollary 1. Assume the same setting as in Theorem 1. Then,
RP(β) ≥ ψ(δ)
-
1 + ((J-T) + k) J12 Ρl≤i,j≤J DKL (PikPj)
lθg2 J
Proof. Putting together the results of Theorem 1, Lemma 3, and Lemma 4, and using the fact that
DKL (PikPj-) ≤ α, the result follows immediately.	□
Corollary 2. Assume the same setting as in Theorem 1, with M +1	< J. Then,
log2(J- M)- J Pl≤i,j≤JDKL (PikPj) - 1 !
RP ≥ ψ(δ)
log2 J
16
Published as a conference paper at ICLR 2021
Proof. This result follows as an application of the data processing inequality. Notice that πM+1 →
∏i:M → W forms a Markov chain. Thus,
I(∏m +1； W) ≤ I(∏M +1； ∏1:M),
by the data processing inequality. We can compute I(∏m +1； ∏LM) in closed form:
I(πM+1； π1:M ) = log j -m-
The proof is completed by plugging in the i.i.d local packing bound alongside the above. □
B.4	Bounds using mixture distributions
In this section we introduce tools to lower bound the minimax risk when the meta-training set
is sampled from a mixture over the meta-training tasks, P1M = -M PMd Pi. We note first that
Theorem 1 can be reproduced exactly when W 〜Plm. Thus, We need only provide a local packing
bound for the mixture distribution. In Lemma 5 we provide such a lower bound for the special case
where M = J - 1, so that data is sampled from a mixture over the entire environment.
Lemma 5 (Leave-one-task-out mixture local packing). Let J ⊂ P contain J distinct distributions
such that ρ(θp,θpo) ≥ 2δ for all P, P0 ∈ J and let P- = j—ɪ Pj=i Pj Let ∏ be a random
ordering of the J elements, and define W ∣π to be a vector of n i.i.d samplesfrom P^-∏m+i . Then,
I(∏M +1 ； W) ≤
1
(J-I)J2
DKL (PikPj) .
1≤i,j≤J
Proof. From Lemma 3 (and some simple arithmetic) we have,
I(∏M+1； W)
1J
J EDKL (P-ikP1J).
i=1
Note that by the definition of the mixture distribution,
J1	1
PIJ = JP-i + JPi.
Using the convexity of the KL divergence,
I(∏M +1； W)
1J
J X Dkl
(P-il J P-i + J Pi)
≤
1JJ 1	1
J £ 丁DKL (P-ikP-i) + JDKL (P-ikPi)
≤
i=1
1J
J2 ΣZDKL (P-ikPi)
i=1
JXDKL (J-I	X	PjIlPi
i=1	1≤j≤J,j 6=i
1J
J-J Xι‹Χy DKL (PjkPi)
i=1 1≤j≤J,j 6=i
(J -11)J 2	X DKL (PikPj ) .
( - )	1≤i,j≤J
Noting for the last step that the KL is zero if and only if the distributions are the same almost
everywhere.	□
17
Published as a conference paper at ICLR 2021
B.5	Proof of Hierarchical linear model lower bound
Recall that the space of distributions we consider is given by,
PLR = {pθ(y) = N (Xθ, σ2I) : θ ∈ B2(1), X ∈ Rn×d.}
Theorem 3 (Meta linear regression lower bound). Consider PLR defined as above and let `(a, b)
(Ila — b∣∣2)2 ∙ If d ≥ 2 and 2-dM + kn-1 ≥ max{ *, dσ2∕(256γ2n) ,then,
、O ( dσ2
≥	(γ2(2-dnM + k)
Proof. The proof consists of two steps, we first construct a 2δ-packing of PLR . Then, we upper
bound the KL divergence between two distributions in this packing and use Corollary 1 to give the
desired bound.
The maximal packing number J for the unit 2-norm ball can be bounded by the following,
(i)d≤j ≤ (1+21
We use a common scaling trick. First, through this bound, we can build a packing set, V , with packing
radius 1/2, giving 2d ≤ J ≤ 5d . We define a new packing set of the same cardinality by taking
θi = 4δvi for all vi ∈ V (requiring δ ≤ 1/2). Giving for all i 6= j,
kθi - θjk = 4δkvi - vjk ≥ 2δ
similarly, Iθi - θj I ≤ 4δ.
We now proceed with bounding the KL divergences.
DKL (PikPj )=壶 kXiθi- Xj θjk2
=2⅛ (θ>X>Xiθi + θ>X>Xjθj- 2θ>X>Xjθj)
≤ 2σ2 (niγ2kθik2 + nj γ2kθj' k2 - 2θ> Xi> Xj θj )
where γi = supθ √nXiθkk ∙ We write n =
DKL (PikPj) ≤ nγ2
2σ2
maxk nk and γ = maxk γk , then,
2
kθik2 + kθjk2 -而θ>X>Xjθj
2
≤ 2⅛ (kθik2 + kθjk2 +2kθikkθjk)
=桨(kθik+kθj k)2 ≤ 3¾Q ≤ β
The second line is derived using the Cauchy-Schwarz inequality, and the final inequality uses
kθik = k4δvik ≤ 4δ. We will not proceed by invoking Corollary 1 on this packing set. This will
require choosing δ to achieve our desired rate and will in turn impose constraints on the problem
dimensions to ensure the packing is valid.
Now, using Corollary 1,
Rj	≥ δ2 (I - (nM2-d + k)32γ2δ2∕σ2 + 1
Choosing δ2 = dσ2/ 128γ2 (2-dMn + k) gives,
I	(nM2-d + k)32δ2∕σ2 + 1 _ ɪ	d/4 + 1〉
d	=	d≥ / ,
for d ≥ 2. To enforce δ ≤ 1/2, we further require that,
2
d	dσ2
-dMn + k ≥ --------
一 256γ2
18
Published as a conference paper at ICLR 2021
Additionally, we may only consider packing sets with KL divergence no more than β, hence we also
require that,
2-dM + kn-1 ≥ -d
4β
Thus,
Rp ≥ O
dσ2
Y2(2-dnM + k)
□
C	Hierarchical Bayesian Linear Regression Upper Bounds
C.1 Some useful linear algebra results
Let smax (A) denotes the maximum singular value of A; smin (A) denotes the minimum singular
value of A.
Lemma 6. Singular value of sum of two matrices Let A, B ∈ Rm×n, then smax (A) + smax (B) ≥
smax(A + B). Furthermore, if A, B are positive definite, smin(A) + smin(B) ≤ smin(A + B).
Proof. The first result follows immediately from the triangle inequality of the matrix norm ∣∣∙k2.
For the second result, suppose that A and B are positive definite.
smin(A+B) = inf ∣(A + B)u∣
kuk=1
Jun=Ik(A+B)uk2
Jllinf, kAuk2+kBuk2+2hAu, Bui
kuk=1
Jun= IkAuk2 + ∣Buk2 + 2u>A> Bu
(3)
(4)
(5)
(6)
(7)
Now, notice that A>B is similar to the matrix A1/2BA1/2, which exists as A is positive definite.
This matrix is itself positive definite, and thus has non-negative eigenvalues, meaning A>B also has
all positive eigenvalues. Thus, u>A>Bu ≥ 0, for all u, and,
Smin(A + B) ≥ , Linf JAuk2 + kBuk2 kuk=1	(8)	
≥ rιlinf ι∣Auk2 + llinf ι∣Bvk2 kuk=1	kvk=1		(9)
≥ Psmin2 (A) + smin2 (B)		(10)
≥ Smin(A)+ Smin(B)	(Concavity of √∙)	(11)
□
Lemma 7. Singular value of product of two matrices Let A, B ∈ Cn×n, then smax(A)smax(B) ≥
smax(AB ), and, smin (A)smin (B ) ≤ s
min(AB).
First we prove the maximum singular value.
19
Published as a conference paper at ICLR 2021
Proof.
SmaX(AB) = sup √v*B*A*ABv kvk=1	Bv ,西,	(12)
= sup PkBvk2u*A*Au forU = kvk=1		(13)
≤	sup	PkBvk2u*A*Au kvk=1,kuk=1		(14)
= I SuP ∣∣Bvk2 SuP IlAull2 kvk=1	kuk=1		(15)
=PsmaX2 (B)Smax2(A)		(16)
= smaX(A)smaX(B).		(17)
The minimum singular value follows a similar structure. Suppose AB is full rank,		
smin(AB) = inf √v*B*A*ABv kvk=1	D q 1	(18)
= inf p∣∣Bvk2u*A*Au foru = kvk=1	v ;西,	(19)
≥	inf	PkBvk2u*A*Au kvk=1,kuk=1		(20)
=J11 inf 1kBvk2l linf JIAuk2 kvk=1	kuk=1		(21)
=Psmin2(B)smin2(A)		(22)
= smin(A)smin(B).		(23)
If AB is not full rank, then smin(AB) = smin(A)smin(B) = 0.		□
Lemma 8. (Von Neumann’s Trace Inequality (Von Neumann, 1937)) Given two n × n complex
matrices A, B, with singular vales a1 ≥ . . . ≥ an and b1 ≥ . . . ≥ bn respectively. We have,
n
|Tr(AB)| ≤ Xaibi
i=1
This is a classic result whose proof we exclude.
As a direct consequence of Lemma 8, |Tr(AB)| ≤ na1b1.
C.2 Posterior estimate
For reference, we reproduce the posterior estimate for the true parameters θM+1 . As a shorthand, we
write Y1:M+1 = (y1, . . . , yM+1).
p(τ |Yl：M) = Ng\Yi：M, ∑t∣Ylm ),	(24)
M
∑-Y1M = X X> (σ2XiX> + σ1l )-1Xi,	(25)
i=1
M
μτ∣YιM = ∑τ∣YιM X X> (σ2XiX> + σ1l)-1yi	(26)
i=1
p(θM+1 |Y1:M, yM +1) = N(μθM+ι | Yi：m+i , ςΘm+i | Yi：m+i ),	(27)
Σθ+τ |Y1：M = σθl + Στ |Y1：M	(28)
ς-M+1∣Y1M+1 = σM2+ixM+1XM +1 + ςθ+t |Yi：m	(29)
μθM+1∣Y1M+1 = ςΘm+i∣Yi:M+ι (σM+1X M +1yM +1 + 'θ+τ |Yi：m μτ |Yi：M )	(30)
20
Published as a conference paper at ICLR 2021
C.3 Upper bound for meta linear regression
In this section we prove the main upper bound result of our paper, Theorem 4.
Theorem 4 (Meta Linear Regression Upper Bound). Let θM+1 be the maximum-a-posteriori estima-
tor, μθM + 1∣Y1：M + 1 . Then,
RPLR ≤	sup	E[∣∣Θm+1 - Θm+ιk2] ≤ O (dσM+∖C (M,η,k厂 2 D(M,η,k∖
θ1 ,...,θM +1 ∈B2 (1)
where,
Mn	Mn
C(M, n,k)= k + n(M +κ2)s2 IA , and, D(M, n, k)= [k +(十 + Al)(等 + A?).
Expectations are taken over the data conditioned on θ1, . . . , θM+1. Additional terms not depending
on d, M, n, k are defined in Appendix C.2.
Before proceeding with the proof, we introduce some additional notation and technical results.
Additional notation To alleviate (only a little of) the notational clutter, we will define the following
quantities,
•	Σ = ΣθM +1 |Y1:M +1
•	Σ00 = Σθ+τ |Y1:M.
•	Smin(X/√n) = s1
•	Smin(XM +JVk) = s2
•	SmaX(X/√n) = γ1 = κs1
•	SmaX(XM +1/Vk) = γ2 = KM +1s2
•	αι = σ2∕σθ
•	α2 = σM+l∕σ2
L τ , — α2____
•	L	(M+κ2)s2
• τ1
αι
C2 q2 K 2
s1s2κM+1
τ2
Kκτ α2
2s2κM+ι
•A
s2αι
s2ɑ2
•	A1 = S22κ2M+1
22
•	A2 = M"+1
2	κτ2 s21 α2
As We have uniform bounds on the singular values of all design matrices, We introduced an auxiliary
matrix X whose largest and smallest singular values are given by √nγι and √nsι respectively.
We Will also Write S(A) = Cov[A, A], and κ(A) = SmaX(A)/Smin(a) throughout.
Bias-Variance Decomposition As is standard, We can decompose the risk into the bias and variance
of the estimator:
E[∣∣Θm+1 -	Θm+1k2] =	E[T⅛((Θm +1	- Θm +i)(Θm+1	- Θm+i)>)]	(31)
=Tr(E[(θM +1	- θM +1)(θθM+1	- θM+1)>])	(32)
=Tr(CovBM+1, 0m +1]) + Tt(E[(Θm +1 - Θm +1)]E[(Θm +1 - Θm +1)]τ)
(33)
In the next tWo sections, We Will derive upper bounds on the bias and variance terms above.
21
Published as a conference paper at ICLR 2021
C.3. 1 Variance technical lemmas
We first decompose the variance into contributions from two sources: the variance from data in the
novel task and the variance from data in the source tasks.
Lemma 9. (Variance decomposition) Let Θm +1 = μθM+i∣y>m+i as defined above. Then the
variance of the estimator can be written as
Tr(S(θM +ι)) = Tr(∑0 σ^ιXM +iXm+ι∑0) +Tr(S(Σ0Σ0Tμτ∣YiM))
Proof.
Tr(Cov[Θm +ι, Θm+i]) = Tr(S (Σ(σM+ιXM +jm +1 + ∑0-1μτ∣YiM)))	(34)
=Tr(S (∑0σM+ιXM +ιyM +1) + S(∑0∑0-1 μτγlM1))	(35)
=Tr(∑0σM2+1χM +1S(yM +1')σM2+1XM +1Σ0 + S (Σ0Σ0-1 〃^丫皿))
(36)
=Tr(∑0σM2+1χM +1XM+1Σ0) + Tr(S(£50-1〃卡皿))	(37)
□
We will now work towards a bound for each of the two variance terms in Lemma 9 separately. To do
so, we will need to produce bounds on the singular values of terms appearing in Lemma 9.
We begin with the covariance term Σ0 .
Lemma 10. (Novel task covariance singular value bound) Let L, A and s2 be as defined above.
Then,
2
SE (Σθ) ≤ M S
SmaXk 乙 ≤ ≤	O
s22
k+M⅛ 1
Proof. Using Lemma 6, we can bound SmaX(Σ0) as follows,
SmaX(Σ0) = SmaX(Σθ
M +1 |Y1:M +1 )
= SmaX ((σM-2+1XM +1 XM+1 + ς-+t |Yi：m )-1)
=VSmin(。-2+1XM +1XM +1 +，-+t∣Yim )
≤ 1/(smin(σM+1 XM +1XM +1) + smin®-，∣Yim ))
=1/(smin(σM+1 XM +1XM +1) + 1/Smax(%+T |Yi：m ))
(38)
(39)
(40)
(41)
(42)
Now, using the auxillary matrix X,
1
-1
Smax(Σ0) ≤
Smin(XM> +1XM+1)
σ2 + MM SmaX((X >C-1x )-1)
-1
σM+1
+2
σθ2 +
Msmin(X>C-1X).
-1
σ
σM+1
kS22
^+ 72Γ
σM+1	σθ +
ɪ	1
+ 疗2 I 1	1
% + Msmin(X>C-1X)」
-1
(43)
(44)
(45)
(46)
M Smin(X>(Xσ2lX> +σ2I)-1 X) _
1
1	1
≤
≤
1
1
1
22
Published as a conference paper at ICLR 2021
Above we have used Lemma 6 repeatedly, alongside the standard identity, smax(A-1) = smin(A)-1.
We continue now, additionally using Lemma 7,
smax (Σ0) ≤
ks22	1
σ + 3 I ι	ι
LσM+1	θ + Msmin(X >X )Smin((Xσ2lX>+σ2l )-1).
-1
ks2
T2
σM+1
ks2
T2
σM+1
ks2
σM+ι
ks2
σ2-
σM+1
1
2 _1_ smaχ(Xσ2IX>+σ2I)
σθ +	smin(X>X)
2 _i_ 1 smax(Xσ2IX> ) +。2
σθ + M	ns2
+
σθ2 +
1
σθ2sm
ax(XX>)+σ2
ns1
-1
-1
-1
+
σθ2 +
1
1 σ2ns1κ2+σ2
M ns；
2 2 l	Mn
s2 + n(M +κ2) + α1
a；	丁 sla；
k + Mn+ A #	.
-1
(47)
(48)
(49)
(50)
(51)
(52)
(53)
□
≤
≤
1
Next, we deal with terms appearing corresponding to the data from the source tasks.
Lemma 11. (Source tasks covariance singular value bound) Let C1 = σ2XX> + σ12I, and write
K = κ(Cι) and κ = κ(∑τ|Y1:M)∙ Then,
smaχ2Nθ+τ |Y1：M ςt |Y1：M ) ≤	2Mσ2ns2	~ f =: DI
smax(C1)Kτ + 碎
and,
smax
1
ns1 + 1
a1
: D2
Proof. Using Lemma 6 and Lemma 7 we have,
smax 2 (Σθ-+1τ |Y1:M Στ |Y1:M )
=smax2N-Ir |Y1：M ςt |为：M )	(54)
≤ smax (Σθ+τ |Y1:M )smax (Στ |Y1:M )	(55)
= smin	(Σθ+τ |Y1:M )smax (Στ |Y1:M )	(56)
= smin	(σθI + Στ |Y1:M )smax (Στ |Y1:M )	(57)
≤	smax(ςt IH:M) ≤ (σθ + Smin(∑τ∣Y1M ))2	(58)
23
Published as a conference paper at ICLR 2021
Now, using σθ2 > 0,
smax(，T∣Y1:M )
(σθ + Smin(∑τ |Yi：m ))2
≤ 2σθ2
smax(Στ |Yi:M)
sminNr | Yi：m ) + smin(匕 |Yi：M )2
1
2σθ	1	1
smax (Στ |Yi:M )κτ	κτ2
Introducing the auxillary matrix X and using Lemma 6 and Lemma 7 on Στ |Yi:M, we have
11
----------------- ≤ ——z——,十二,、-----------
2若_______,工一 2Mσθsmin(X>C-1X) + 1
Smax(£丁 |Yi：M )κτ	κT	Kτ	K：
where,
Smin(X>CC-1X) ≥
Smin(X >X)
C /方一
Smax(C1)
2
nS21
.—" .
Smax (C1)
This gives the first stated inequality,
^^2(线+丁 |Yi：M ςt |Y1：M ) ≤	2Mσ2ns2	「
____θ 1 J
Smax(Cl)Kτ	κT
The second follows as,
Smax(σ2C-1)=
≤
_______σ2________
Smin(σ2XX > + σ2I)
__________σ2__________
Smin(σ2XX> ) + Smin(σ2I)
σ2
ns2σ2 + σ2
--2---- =: D2
nsι + 1
αi
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
(67)
(68)
□
≤
In Lemma 11, we introduced additional condition numbers, which we can bound as follows,
K = K(CI)	=	κ(σθXX>	+ σ2l)	≤ κ(σjXX>)	≤	κ(σjXX>)κ(σjl)	=	κ2,	(69)
Kτ = κ(∑τ|Yi:M) ≤ κ(X>X)κ(Cι) = K2K ≤ K4.	(70)
C.3.2 Variance upper bound
We are now ready to put the above technical results together to achieve a bound on the variance of the
estimator.
Lemma 12. (Variance bound)
2	2	-2
Tr(S(Θm +1)) ≤ KM+7 +1 d k + f k + TV-
s2 L L+ AJ	(L +
Proof. First, by Lemma 9 we can decompose the overall variance into two terms:
Tr(S(θM +1)) = Tr(∑0 Om^XM> +1XM+1Σ0)+Tr(S(Σ0Σ0Tμτ∣YιM))
We deal with the left term first.
24
Published as a conference paper at ICLR 2021
Using trace permutation invariance and the von Neumann trace inequality (Lemma 8). We can upper
bound the left variance term as follows,
Tr(Σ0σM-2+1XM>+1XM+1Σ0)=σM-2+1Tr(Σ0Σ0XM>+1XM+1)
≤ dkσ-+ιSmaχ(Σ0)2 * *Smaχ2(XM +l∕√k)
= dkσM +1 smax(Σ ) s2κM+1
(71)
(72)
(73)
For the second variance term, we observe that,
Tr(Σ0Σ0-1S(μτ∣YiM )∑0-1∑0)	(74)
M
=Tr(Σ0Σ0-1S(∑τ∣YiM XX>(σ2XiX> + σ↑I)-1yi)Σ0-1Σ0)	(75)
i=1
≤ MTr(∑0∑0-1∑τ∣YiMX>C-1S(y1)C-1X∑t∣Yim∑0-1∑0)	(76)
=M Tr(∑0∑0-1∑τ∣YiM X >C-1σ2lC-1X∑τ∣YiM ∑0-1∑0)	(77)
≤ MSmax(∑0)2Smaχ2(∑-+τ∣Y1M ∑t∣Yim )σ2Tr(X > CTCTX)	(78)
Using Lemma 11, we have,
Tr(Σ0Σ0-1S(μτ∣YLM)∑0-1∑0) ≤
smax
smax
(∑0)2MDlD2Tr(X >X)Smaχ(C-1)
(Σ0)2MD1D2 min(n, d)nSmaχ(C-1)
(79)
(80)
smax(Σ0)2D2
M min(n, d)n
smax(Σ0)2D2
C--C	C	，二、
2Mσ2ns2Smin(Cι)	+
Smax(CI)KT
M min(n, d)n
C I r` ∖
Smin(CI )
(81)
κτ
2Mσ2ns2Smin(Cι)
smax(Σ0)2D2
smax (C1 )κτ
min(n, d)n
+σ2
κτ
M
(82)
σM+ι
2Mn 1 αι
KKT02 十 K2T σ2202
(83)
smax (Σ ) nd
σ2	ns2	l 1	2Mn	+ αι
M +1	-01	+ 1	KKT ɑ2	十	κTS2a2
(84)
Finally, rearranging and using Lemma 10, we can bound the sum of the two terms in the variance as
follows,
Tr(S(Θm+1)) ≤
包卢(kds2κM +1 + $ 2Mn M αι
M +1	αι + 1 KKT02 十 KTs2θ2
(85)
smax
σ2
σM+1
κ2
κM+1
(kd+
nd
ns1s2KM+1	,	9^2
01	+ s2κM+1
2Mns2KM+i + o1s2kM+1
KKT02 丁 KTS；O2
)
(86)
≤
≤
≤
≤
≤
≤
M
M
≤
2	2	-2
M +I2 M+1 k + 元HA	•	(87)
s2 L L+ A」
≤
kd +
Mnd
ns2s2KM+i
01
2	2 w2Mns2KM+i ,	0is2kM+1
十∙s2*M +1 八 KKT 02 十 KT s2 02
22
κM +1σM +1
s2
k+
Mn
k +----------TT------
(L + A1)( Mn + A2)
(88)
(89)
(
)
n
E
□
25
Published as a conference paper at ICLR 2021
C.3.3 Bounding THE BIAS
Lemma 13. (Bias upper bound) Given θι,..., Θm+i ∈ B2(l), we have,
E[(Θm +1 — Θm +1
)]≤ O d
∖
Mn
n(M+κ2)s2
«2
k +
+ A
Proof. The bias can be computed as follows,
E[(θM+1 — θM+1)] = EμθM+1Y1M+1 — θM+1
=4m+1|Y1：M+1 E(σM+1 XM+1y2 + “+τ |Y1：M μτ 1Y1：M)— θM+1
(90)
(91)
∑θM+1|Y1：M+1 S-+1xM +1xM +1θM +1 + ∑-1τ |Y1：M
(σ-2+1XM+1XM +1 + ∑-+τY1:M )-1 ∙
Eμτ |Y1：M ) — θM +1
(92)
(93)
(σM+1XM+1XM +1θM +1 + ςθ+τ |Y1：M Eμτ |Y1：M ) - θM+
(F + G)I(F θM +1 + Gμτ |Yi：m ) — θM +1
1
(94)
(95)
=(F + G)IF θm +1 — (F + G)I(F + G)θM +1 + (F + G)IGEμτ |Yi：m
=(F + G)IG(E4τ |Y1：m — θM+1),
where we wrote F = σM2+1XM+1Xm+1, and G = Σ-LT邙：M. Thus,
(96)
(97)
E[(θM +1 — θM +1)]TE[(θM +1 — θM +1)] ≤ Il(F + G)II∣2kGk2kEμτ|Y1：M — θM+1∣∣2
We can bound each term in turn. First, note that ∣∣G∣2 ≤ 1∕σ^, and we have bounded ∣∣(F + G)Tk2
above. We can write,
IIEμτ |Y1：M — θM +1∣∣2
IIEμτ |Y1：M — θM +1∣∣2
ςτ |Y1： M
ςτ |Y1： M
(X
(X
XJC-IX 血—Θm+1
XJC-IXi θi∖ — ∑τKM 与IY1：M θM +1
2
2
2
2
M
∑τKM XXirC-1Xi(θL Θm+1)
i=1
2
2
≤
≤
∖i=1	/
The last line follows from the fact that the parameters lie in a ball of unit radius. We now proceed by
bounding the sum by M times the supremum ——with some light abuse of notation,
Mτ|Y1：M — Θm +1∣2 ≤ (Smax(XTCTX)Smax(XC-1X))2
=Smax(XTCTX)4 ≤ O(1)
Thus, overall the convergence of the bias is bounded by,
E[(Θm +1 —Θm+1 )]tE[(Θm+1 — θM +1)] ≤ O(SmaX(∑∕)2) ≤ θ I d
k+
Mn
n(M+κ2)s2
«2
+ A
∖
26
Published as a conference paper at ICLR 2021
□
The proof of Theorem 4 is given by the combination of Lemma 12 and Lemma 13, and the bias-
variance decomposition of the risk .
D Additional Experiment Details
D. 1 Hierarchical Bayes Evaluation
We sample M linear models according to the hierarchical model in Section 5, with design matrices
constructed by uniformly sampling points, X 〜 U[-1,1], and storing the vector Xj = xj, for
i = 0, . . . , d in each row of Xi.
To produce the plots in Figure 2 we computed the average loss over 100 random draws of the training
data and labels from the same set of fixed θ±M+1 values. The θ values were sampled once from the
hierarchical model with τ = [0, 1, 2, 0, 0, 3, 1], and σθ2 = 0.1
Code to reproduce these plots is provided in the supplementary materials with our submission.
D.2 Sinusoid Regression with MAML
Hyper parameters
σ
M
Mq
eps_per_batch
train_ampl_range
train_phase_range
VaLamPl_range
VaLPhase_range
inner_steps
innerJr
meta_lr
n
k
nq
kq
Description
noise at test time.
number of tasks at the training tasks
number of tasks at the testing tasks
episode per batch
range of amplitude at training
range of phase at training
range of amplitude at testing
range of phase at testing
number of steps of Maml
learning rate used to optimize parameter of the model
used to optimize parameter of the meta-learner
number of datapoints at training tasks(support set)
number of datapoints at testing tasks (support set)
number of datapoints at training tasks (query set) .
number of datapoints at testing tasks (query set).
For all of these experiments we used a fully connected network with 6 layers and 40 hidden units per
layer. The network is trained using the MAML algorithm (Finn et al., 2017) with 5 inner steps using
SGD with an inner learning rate of 10-3. We used Adam for the outer loop learning with a learning
rate of 10-3 .
Expected error was computed after 500 epochs of optimization and was averaged over 30 runs. We
produced our results through a comprehensive grid search over 72 combinations of the settings below
and it required around 30 minutes to produce the output of each setting, using a system with 1 gpu
and 3 cpus. This experiment therefore lasted 20 hours in total.
M = 50,n ∈ {20,200},k ∈ {100,1000},σ ∈ [10-8,1.5],Mq = 100, eps_per_batch =
25, train_amPLrange	=	[1,4], train_phase_range	=	[0,∏∕2], VaLamPLrange	=
[3, 5], val_phase_range = [0,∏∕2], inner_steps = 5, inner_lr = 10-3, meta_lr = 10-3
27