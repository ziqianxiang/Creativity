Published as a conference paper at ICLR 2021
Relating by Contrasting: A Data-efficient
Framework for Multimodal DGMs
Yuge Shi1, Brooks Paige2,4, Philip H.S. Torr1 & N. Siddharth*1,3,4
1 University of Oxford
2University College London
3 University of Edinburgh
4The Alan Turing Institute
yshi@robots.ox.ac.uk
Abstract
Multimodal learning for generative models often refers to the learning of abstract
concepts from the commonality of information in multiple modalities, such as
vision and language. While it has proven effective for learning generalisable
representations, the training of such models often requires a large amount of
“related” multimodal data that shares commonality, which can be expensive to
come by. To mitigate this, we develop a novel contrastive framework for generative
model learning, allowing us to train the model not just by the commonality between
modalities, but by the distinction between “related” and “unrelated” multimodal
data. We show in experiments that our method enables data-efficient multimodal
learning on challenging datasets for various multimodal variational autoencoder
(VAE) models. We also show that under our proposed framework, the generative
model can accurately identify related samples from unrelated ones, making it
possible to make use of the plentiful unlabeled, unpaired multimodal data.
1 Introduction
To comprehensively describe concepts in the real world, humans collect
multiple perceptual signals of the same object such as image, sound, text
and video. We refer to each of these media as a modality, and a collection
of different media featuring the same underlying concept is characterised
as multimodal data. Learning from multiple modalities has been shown
to yield more generalisable representations (Zhang et al., 2020; Guo et al.,
2019; Yildirim, 2014), as different modalities are often complimentary in
content while overlapping for common abstract concept.
Despite the motivation, it is worth noting that the multimodal framework
Category: Laysan Albatross
"A larger sized
bird with a glowing
white body and a
large orange beak.,,
"A cigar-shaped
white bird with long
brown wings and
hooked long bill."
Figure 1: Multimodal data
from the CUB dataset
is not exactly data-efficient—constructing a suitable dataset requires a lot of “annotated” unimodal
data, as we need to ensure that each multimodal pair is related in a meaningful way. The situation
is worse when We consider more complicated multimodal settings SuCh as language-vision, where
one-to-one or one-to-many correspondence between instances of the two datasets are required, due
to the difficulty in categorising data such that commonality amongst samples is preserved within
categories. See Figure 1 for an example from the CUB dataset (Welinder et al., a); although the same
species of bird is featured in both image-caption pairs, their content differs considerably. It would be
unreasonable to apply the caption from one to describe the bird depicted in the other, necessitating
one-to-one correspondence between images and captions.
However, the scope of multimodal learning has been limited to leveraging the commonality between
“related” pairs, while largely ignoring “unrelated” samples potentially available in any multimodal
dataset—constructed through random pairing between modalities (Figure 3). We posit that if a
distinction can be established between the “related” and “unrelated” observations within a multimodal
dataset, we could greatly reduce the amount of related data required for effective learning. Figure 2
formalises this proposal. Multimodal generative models in previous work (Figure 2a) typically
assumes one latent variable z that always generates related multimodal pair (x, y). In this work
(Figure 2b), we introduce an additional Bernoulli random variable r that dictates the “relatedness”
between x and y through z, where x and y are related when r = 1, and unrelated when r = 0.
* work done while at Oxford
1
Published as a conference paper at ICLR 2021
(a) Previous (b) Ours
Figure 2: Graphical models for
multimodal generative process.
While r can encode different dependencies, here we make the sim-
plifying assumption that the Pointwise Mutual Information (PMI)
between x and y should be high when r = 1, and low when r = 0.
Intuitively, this can be achieved by adopting a max-margin metric.
We therefore propose to train the generative moels with a novel
contrastive-style loss (Hadsell et al., 2006; Weinberger et al., 2005),
and demonstrate the effectiveness of our proposed method from a few
different perspectives: Improved multimodal learning: showing
improved multimodal learning for various state-of-the-art multimodal
generative models on two challenging multimodal datasets. This is evaluated on four different
metrics (Shi et al., 2019) summarised in § 4.2; Data efficiency: learning generative models under the
contrastive framework requires only 20% of the data needed in baseline methods to achieve similar
performance—holding true across different models, datasets and metrics; Label propagation: the
contrastive loss encourages a larger discrepency between related and unrelated data, making it possi-
ble to directly identify related samples using the PMI between observations. We show that these data
pairs can be used to further improve the learning of the generative model.
2 Related Work
Contrastive loss Our work aims to encourage data-efficient multimodal generative-model learning
using a popular representation learning metric—contrastive loss (Hadsell et al., 2006; Weinberger
et al., 2005). There has been many successful applications of contrastive loss to a range of different
tasks, such as contrastive predictive coding for time series data (van den Oord et al., 2018), image
classification (Henaff, 2020), noise contrastive estimation for vector embeddings of words (Mnih and
Kavukcuoglu, 2013), as well as a range of frameworks such as DIM (Hjelm et al., 2019), MoCo (He
et al., 2020), SimCLR (Chen et al., 2020) for more general visual-representation learning. The
features learned by contrastive loss also perform well when applied to different downstream tasks —
their ability to generalise is further analysed in (Wang and Isola, 2020) using quantifiable metrics for
alignment and uniformity.
Contrastive methods have also been employed under a generative-model setting, but typically on
generative adversarial networks (GANs) to either preserve or identify factors-of-variations in their
inputs. For instance, SiGAN (Hsu et al., 2019) uses a contrastive loss to preserve identity for face-
image hallucination from low-resolution photos, while (Yildirim et al., 2018) uses a contrastive loss
to disentangle the factors of variations in the latent code of GANs. We here employ a contrastive
loss in a distinct setting of multimodal generative model learning, that, as we will show with our
experiments and analyses promotes better, more robust representation learning.
Multimodal VAEs We also demonstrate that our approach is applicable across different approaches
to learning multimodal generative models. To do so, we first summarise past work on multimodal
VAE into two categories based on the modelling choice of approximated posterior qφ(z∣x, y):
a)	Explicit joint models: qφ as single joint encoder qφ(z∣x, y).
Example work in this area include JMVAE (Suzuki et al.), triple ELBO (Vedantam et al.,
2018) and MFM (Tsai et al., 2019). Since the joint encoder require multimodal pair (x, y)
as input, these approaches typically require additional modelling components and/or infer-
ence steps to deal with missing modality at test time; in fact, all three approaches propose to
train unimodal VAEs on top of the joint model that handles data from each modality independently.
b)	Factorised joint models: qφ as factored encoders qφ(z|x, y) = f (q@x(z|x), q®® (>∣y)).
This was first seen in Wu and Goodman (2018), as the MVAE model with f defined as a product of
experts (PoE), i.e. qφ(z∣x, y) = qφx(z∣x)qφy (z∣y)p(z), allowing for cross-modality generation
without extra modelling components. Particularly, the MVAE caters to settings where data was
not guaranteed to be always related, and where additional modalities were, in terms of information
content, subsets of a primary data source—such as images and their class labels.
Alternately, Shi et al. (2019) explored an approach that explicitly leveraged the availability of
related/paired data, motivated by arguments from embodied cognition of the world. They propose
the MMVAE model, which additionally differs from the MVAE model in its choice of posterior
approximation—where f is modelled as the mixture of experts (MOE) of unimodal posteriors—to
ameliorate shortcomings to do with precision miscalibration of the PoE. Furthermore, Shi et al.
2
Published as a conference paper at ICLR 2021
(2019) also posit four criteria that a multimodal VAE should satisfy, which we adopt in this work
to evaluate the performance of our models.
Weakly-supervised learning Using generative models for label propagation (see § 4.5) is a form of
weak supervision. Commonly seen approaches for weakly-supervised training with incomplete data
include (1) graph-based methods (such as minimum cut) (Zhou et al., 2003; Zhu et al., 2003; Blum and
Chawla, 2001), (2) low-density separation methods (Joachims, 1999; Burkhart and Shan, 2020) and
(3) disagreement-based models (Blum and Mitchell, 1998; Zhou and Li, 2005; 2010). However, (1)
and (2) suffers from scalability issues due to computational inefficiency and optimisation complexity,
while (3) works well for many different tasks but requires training an ensemble of learners.
The use of generative models for weakly supervised learning has also been explored in Nigam et al.
(2000); Miller and Uyar (1996), where labels are estimated using expectation-maximisation (EM)
(Dempster et al., 1977) for instances that are unlabelled. Notably, models trained with our contrastive
objective does not need EM to leverage unlabelled data (see § 4.5) to determine the relatedness of two
examples we only need to compute a threshold (estimation of PMI) using the trained model.
3 Methodology
Given data over observations from two modalties (x, y), one can learn a multimodal VAE tar-
getting pθ(x, y, Z) = p(z)pθx (x|z)pθy (y|z), where pθ(∙∣z) are deep neural networks (decoders)
parametrised by Θ = {θx, θy}. To maximise the joint marginal likelihood logpΘ(x, y), one approxi-
mates the intractable model posterior pθ(z|x, y) with a variational posterior qφ(Z|x, y), allowing
us to optimise a variational evidence lower bound (ELB O), defined as
logPΘ(x, y) ≥ Ez 〜qφ(Z∣χ,y) log	X y)) = ELBO(x, y).	(1)
This leaves open the question of how to model the approximatd posterior qφ (z|x, y). As mentioned
in § 2, there are two schools of thinking, namely explicit joint model such as JMVAE (Suzuki et al.)
and factorised joint model including MVAE (Wu and Goodman, 2018) and MMVAE (Shi et al.,
2019). In this work we demonstrate the effectiveness of our approach across all these models.
3.1 Contrastive loss for “relatedness” learning
Where prior work always assumes (x, y) to be related, we introduce a relatedness variable r ex-
plicitly capturing this aspect. Our approach is motivated by the characteristics of pointwise mutual
information (PMI) between related and unrelated observations across modalities:
Hypothesis 3.1. Let (x, y)〜pθ (x, y) be a related data pair from two modalities, and let y 0 denote
a data point unrelated to x. Then, the pointwise mutual information I (x, y) > I(x,y0).
(a) Related (b) Unrelated
Figure 3: Constructing re-
lated & unrelated samples
Note that the PMI measures the statistical dependence between values (x, y),
which for a joint distribution p(x, y) is defined as I(x, y) = log Ppxxpyy).
Hypothesis 3.1 should be a fairly uncontroversial assumption for true gen-
erative models: we say simply that under the joint distribution for related
data pΘ(x, y), the PMI between related points (x, y) is stronger than that
between unrelated points (x, y0). In fact, we demonstrate in § 4.5 that for
a well-trained generative model, PMI is a good indicator of relatedness,
enabling pairing of random multimodal data by thresholding the PMI esti-
mated by the trained model. It is therefore possible to leverage relatedness
while training parametrised generative model by maximising the difference
in PMI between related and unrelated pairs, i.e. I(x, y) - I(x, y0). We show in Appendix A that
this is equivalent to maximising the difference between the joint marginal likelihoods of related and
unrelated pairs (Figure 3a and 3b). This casts multimodal learning as max-margin optimisation, with
the contrastive (triplet) loss as a natural choice of objective: LC (x, y, y0) = d(x, y) - d(x, y0) +m.
Intuitively, LC attempts to make distance d between a positive pair (x, y) smaller than the distance
between a negative pair (x, y0 ) by margin m. We adopt this loss to our objective by omitting margin
m and replacing d with the (negative) joint marginal likelihood - log pΘ . Following Song et al.
(2016), with N negative samples {yi0}iN=1, we have
N
LC(x,Y) = -logpΘ(x,y) + log	pΘ(x,yi0).	(2)
i=1
3
Published as a conference paper at ICLR 2021
We choose to put the sum over N within the log following conventions in previous work on contrastive
loss (van den Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020). As the loss is asymmetric,
one can average over LC(y, X) and LC (x, Y ) to ensure negative samples in both modalities are
accounted for—giving our contrastive objective:
1	1N	N
LC(χ,y) = 2{Lc(χ, Y)+LC(y,x)} = -logpθ(χ, y) + 2 ∣iog∑ ρ㊀(χ0,y) + log工 pθ(χ, y0)1
x0i=1	yi0=1
S---V---} S-----------V----------}
① ②
(3)
Note that since only the joint marginal likelihood terms are needed in (3), the contrastive learning
framework can be directly applied to any multimodal generative model without needing extra
components. We also show our contrastive framework applies when the number of modalities is
more than two (cf. Appendix C).
Dissecting LC Although similar to the VAE (1), our objective (3)
directly maximises pθ (x, y) in ①.LC by itself is not a effective
objective as ② in LC minimises pθ (x, y), which can overpower the
effect of ① during training.
Figure 4: log p(x, y) of imitation,
unrelated digits and random noise,
where p = N(mx , my).
We intuit this phenomenon using a simple example in Figure 4 with
natural images, showing the log likelihood of log p(x, y) in column
2, 3, 4 (green) on a Gaussian distribution N((mx, my); c), with the
images in the first column of Figure 4 as means and with constant variance c. While achieving high
log p(x, y) requires matching (mx, my) (col 2), we see both unrelated digits (col 3) and noise (col
4) can lead to (comparatively) poor joint log likelihoods. This indicates that the generative model
need not generate valid, unrelated images to minimise @—generating noise would have roughly the
same effect on log likelihood. As a result, the model can trivially minimise LC by generating noise
that minimises ② instead of accurate reconstruction that maximises ①.
This learning dynamic is verified empirically, as we show in Figure 9 in Appendix D: optimising
LC by itself results in the loss approaching 0 within the first 100 iterations, and both ① and ② takes
on extremely low values, resulting in a model that generates random noise.
Final objective To mitigate this issue, we need to ensure that minimising ② does not overpower
maximising ①.We hence introduce a hyperparameter Y on ① to upweight the maximisation of the
marginal likelihood, with the final objective to minimise
1 ( N	N	∖
LC(χ, y) = -γ logpθ(χ, y) + 万 I log 工 pθ(χ0, y) + log 工 pθ(χ, y0) 1, Y > 1. (4)
x0i=1	yi0 =1
S {z } S	{z	}
(1	W
We conduct ablation studies on the effect of Y in Appendix E, noting that larger Y encourages better
quality of generation and more stable training in some cases, while models trained with smaller Y
are better at predicting “relatedness” between multimodal samples. We also note that optimising (4)
maximises the pointwise mutual information I(x, y); see Appendix B for a proof.
3.2 Optimising the objective
Since in VAEs we do not have access to the exact marginal likelihood, we have to optimise an
approximated version of the true contrastive objective in (4). In (5) we list a few possible candidates
of estimators and their relationships to the (log) joint marginal likelihood:
ELBO ≤ E{zk 厅『Mg Ki XX Pt⅞! # ≤ log PΘ (x, y) ≤ E{zk }K』t K XX (P^
K k=1 qΦ (zk | x, y)	K k=1 qΦ (zk | x, y)
'-----------------------------'	'-------------------------------
IWAE	CUBO
with the ELBO from Eq. (1), the importance weighted autoencoder (IWAE), a K -sample lower
bound estimator that can compute an arbitrarily tight bound with increasing K (Burda et al., 2016),
and the χ upper bound (CUB O), an upper-bound estimator (Dieng et al., 2017).
We now discuss our choice of approximation of log pΘ(x, y) for each term in equation (4):
4
Published as a conference paper at ICLR 2021
①	Minimising this term maximises the joint marginal likelihood, which can hence be approximated
with a valid lower-bound estimator (5); the IWAE estimator being the preferred choice.
②	For this term, we consider two choices of estimators:
1)	CUBO: Since ② of (4) minimises the joint marginal likelihoods, to ensure an upper-bound
estimator to LC in (4), we need to employ an upper-bound estimator. Here we propose to
use CUBO in (5). While such a bounded approximation is indeed desirable, existing upper-
bound estimators tend to have rather large bias/variance and can therefore yield poor quality
approximations.
2)	IWAE: We therefore also propose to estimate ② with the IWAE estimator, as it provides an
arbitrarily tight, low variance lower-bound (Burda et al., 2016) to logpΘ(x, y). Although
this no longer ensure a valid bound on the objective, we hope that having a more accurate
approximation to the marginal likelihood (and by extension, the contrastive loss) can affect the
performance of the model positively.
We report results using both IWAE and CUBO estimators in § 4, denoted cI and cC respectively.
4	Experiments
As stated in § 1, we analyse the suitability of contrastive learning for multimodal generative models
from three persepctives—improved multimodal learning (§ 4.3), data efficiency (§ 4.4) and label
propagation (§ 4.5). Throughout the experiments, we take N = 5 negative samples for the contrastive
objective, set γ = 2 based on analyses of ablations in appendix E, and take K = 30 samples for our
IWAE estimators. We now introduce the datasets and metrics used for our experiments.
4.1	Datasets
MNIST-SVHN The dataset is designed to separate conceptual complexity, i.e. digit, from percep-
tual complexity, i.e. color, style, size. Each data pair contains 2 samples of the same digit, one from
each dataset (see examples in Figure 3a). We construct the dataset such that each instance from one
dataset is paired with 30 instances of the same digit from the other dataset. Although both datasets
are simple and well-studied, the many-to-many pairing between samples creates matching of different
writing styles vs. backgrounds and colors, making it a challenging multimodal dataset.
CUB Image-Captions We also consider a more challenging language-vision multimodal dataset,
Caltech-UCSD Birds (CUB) (Welinder et al., b; Reed et al., 2016). The dataset contains 11,788
photos of birds, paired with 10 captions describing the bird’s physical characteristics, collected
through Amazon Mechanical Turk (AMT). See CUB image-caption pair in Figure 1.
4.2	Metrics
Shi et al. (2019) proposed four criteria for multimodal generative models (Figure 5, left), that we
summarise and unify as metrics to evaluate these criteria for different generative models (Figure 5,
right). We now introduce each criterion and its corresponding metric in detail.
(a)	Latent accuracy (Figure 5a) Criterion: latent space factors into “shared” and “private” sub-
spaces across modalities. We fit a linear classifier on the samples from Z 〜qφ (Z|x, y) to classify
the information shared between the two modalities. For MNIST-SVHN, this can be the digit label as
shown in Figure 5a (right). We check if lz is the same as the digit label of the original inputs x and y,
with the intuition that extracting the commonality between x and y from latent representation using a
linear transform, supports the claim that the latent space has factored as desired.
Criterion Metric
P(N)
Criterion Metric
我(NMM
(a) Latent factorisation
(b) Coherent joint generation
(c) Coherent cross generation
Criterion Metric
(ZIg) qφx (z∖x)
Figure 5: Left of each pair: Four criteria for multi-modal generative models; image adapted from Shi et al.
(2019). Right of each pair: Four metrics to evaluate the model’s performance on criterion in corresponding row.
5
Published as a conference paper at ICLR 2021
(b)	Joint coherence (Figure 5b) Criterion: model generates paired samples that preserves the com-
monality observed in data. Again taking MNIST-SVHN as an example, this can be verified by taking
pre-trained MNIST and SVHN digit classifiers and applying them on the multimodal observations
generated from the same prior sample z. Coherence is computed by how often generations X and y^
classify to the same digit, i.e. whether l^ = ly.
(c)	Cross coherence (Figure 5c) Criterion: model generates data in one modality conditioned on the
other while preserving shared commonality. To compute cross coherence, we generate observations
X using latent from unimodal marginal posterior Z 〜qφ(z∖y) and y from Z 〜qφ(z∣x). Similar to
joint coherence, the criterion here is evaluated by predicting the label of the cross-generated samples
X, y using off-the-shelf MNIST and SVHN classifiers. In Figure 5c (right), the cross coherence is
the frequency of which l^ = ly and l^ = lχ.
(d)	Synergy coherence (Figure 5d) Criterion: models learnt across multiple modalities should be
no worse than those learnt from just one. For consistency, we evaluate this criterion from a coherence
perspective. Given generations X and y^ from Z ~ qφ(z∖x, y), we again examine if generated and
original labels match; i.e. if Ix = ly = ly = lχ.
See Appendix F for details on architecture. All quantitative results are reported over 5 runs. In
addition to these quantitative metrics, we also showcase the qualitative results on both datasets in
Appendix G and Appendix H.
4.3	Improved multimodal learning
Finding: Contrastive learning improves multimodal learning across all models and datasets.
MNIST-SVHN See Table 1 (top, 100% data used) for results on the full MNIST-SVHN dataset.
Note that for MMVAE, since the joint posterior qΦ is factorised as the mixture of unimodal posteriors
qφx and qφy , the model never directly takes sample from the explicit form of the joint posterior.
Instead, it takes equal number of samples from each unimodal posteriors, reflective of the equal
weighting of the mixture. As a result, it is not meaningful to compute synergy coherence for MMVAE
as it is exactly the same as the coherence of any single-way generation.
From Table 1 (top, 100% data used), we see that models trained on our contrastive objective (cI-
<MODEL> and cC-<MODEL>) improves multimodal learning performance significantly for all three
generative models evaluated on the metrics. The results showcase the robustness of our approach
from the perspectives of modelling choice and metric of interests. In particular, note that for the best
performing model MMVAE, using IWAE estimator for ② of Eq (4) (CI-MMVAE) yields slightly
better results than CUBO (cC-MMVAE), while for the two other models the performance for different
estimators are similar.
We also include qualitative results for MNIST-SVHN, including generative results, marginal likeli-
hood table and diversity analysis in Appendix G.
Table 1: Evaluation of baselines MMVAE, MVAE, JMVAE and their contrastive variations (cI-<MODEL>,
cC-<MODEL> for the IWAE and CUBO estimators used in Eq (4), respectively), on MNIST(M)-SVHN(S)
dataset, using 100% (top) and 20% (bottom) of data.
Data	Methods	Latent accuracy (%)		Joint Coherence (%)	Cross coherence (%)		Synergy coherence (%)	
		M	S		S→M	M→S	joint→M	joint→S
	MMVAE	92.48 (±0.37)	79.03 (±1.17)	42.32 (±297)	70.77 (±0.35)	85.50 (±1.05)	—	—
	cI-MMVAE	93.97 (±0.36)	81.87 (±0.52)	43.94 (±0.96)	79.66 (±0.59)	92.67 (±0.29)	—	—
	cC-MMVAE	93.10 (±0.17)	80.88 (±0.80)	45.46 (±0.78)	79.34 (±0.54)	92.35 (±0.46)	—	—
100% of	MVAE	91.65 (±0.17)	64.12 (±4.58)	9.42 (±7.82)	10.98 (±o.56)	21.88 (±2.21)	64.60 (±9.25)	52.91 (±8.11)
data used	cI-MVAE	96.97 (±0.84)	75.94 (±6.20)	15.23 (±10.46)	10.85 (±1.17)	27.70 (±2.09)	85.07 (±7.73)	75.67 (±4.13)
	cC-MVAE	97.42 (±0.40)	81.07 (±2.03)	8.85 (±3.86)	12.83 (±2.25)	30.03 (±2.46)	75.25 (±5.31)	69.42 (±3.94)
	JMVAE	84.45 (±0.87)	57.98 (±1.27)	42.18 (±1.50)	49.63 (± 1.78)	54.98 (±3.02)	85.77 (±0.66)	68.15 (±1.38)
	cI-JMVAE	84.58 (±1.49)	64.42 (±1.42)	48.95 (±2.31)	58.16 (±1.83)	70.61 (±3.13)	93.45 (±0.52)	84.00 (±0.97)
	cC-JMVAE	83.67 (±3.48)	66.64 (±2.92)	47.27 (±4.52)	59.73 (±3.85)	69.49 (±2.19)	91.21 (±6.59)	84.07 (±3.19)
	MMVAE	88.54 (±0.37)	68.90 (±1.79)	37.71 (± 0.60)	59.52 (±0.28)	76.33 (±2.23)	—	—
	cI-MMVAE	91.64 (±0.06)	73.02 (±0.80)	42.74 (±0.36)	69.51 (±1.18)	86.75 (±0.28)	—	—
	cC-MMVAE	92.10 (±0.19)	71.29 (±1.05)	40.77 (±0.93)	68.43 (±0.90)	86.24 (±0.89)	—	—
20% of	MVAE	90.29 (±0.57)	33.44 (±0.26)	10.88 (±9.15)	8.72 (±o.92)	12.12 (±3.38)	42.10 (±5.22)	44.95 (±5.92)
data used	cI-MVAE	93.72 (±1.09)	56.74 (±7.97)	12.79 (±6.82)	14.18 (±2.19)	20.23 (±4.55)	75.36 (±5.05)	64.81 (±4.81)
	cC-MVAE	92.74 (±2.97)	52.99 (±8.35)	17.95 (±12.52)	14.70 (±1.65)	24.90 (±5.77)	56.86 (±18.84)	54.28 (±9.86)
	JMVAE	77.53 (±0.13)	52.55 (±2.18)	26.37 (±0.54)	42.58 (±5.32)	41.44 (±2.26)	85.07 (±9.74)	51.95 (±2.28)
	cI-JMVAE	77.57 (±4.02)	57.91 (±1.28)	32.58 (±5.89)	51.85 (±1.27)	47.92 (±10.32)	92.54 (±1.13)	67.01 (±8.72)
	cC-JMVAE	81.11 (±2.76)	57.85 (±2.23)	34.00 (±7.18)	50.73 (±0.45)	56.89 (±6.18)	88.36 (±4.38)	68.49 (±8.82)
6
Published as a conference paper at ICLR 2021
CUB Following Shi et al. (2019), for the images in CUB, we observe and generate in feature
space instead of pixel space by preprocessing the images using a pre-trained ResNet-101 (He et al.,
2016). A nearest-neighbour lookup among all the features in the test set is used to project the feature
generations of the model back to image space. This helps circumvent CUB image complexities to
some extent—as the primary goal here is to learn good models and representations of multimodal
data, rather than a focus on pixel-level image quality of generations.
The metrics listed in § 4.2 can also be applied to CUB with some modifications. Since bird-species
classes are disjoint for the train and test sets, and as we show in Figure 1 contains substantial in-class
variance, it is not constructive to evaluate these metrics using bird categories as labels. In Shi et al.
(2019), the authors propose to use Canonical Correlation Analysis (CCA)—used by Massiceti et al.
as a reliable vision-language correlation baseline—to compute coherence scores between generated
image-caption pairs; which we employ (i.e. (b), (c), (d) in Figure 5) for CUB.
We show the results in Table 2 (top, 100% data used). We see that our contrastive approach (both
cI-<MODEL> and cC-<MODEL>) is even more effective on this challenging vision-language dataset,
with significant improvements to the correlation of generated image-caption pairs. It is also on this
more complicated dataset where the advantages of using the stable, low-variance IWAE estimator are
highlighted — for both MVAE and JMVAE, the contrastive objective with CUBO estimator suffers
from numerical stability issues, yielding close-to-zero correlations for all metrics in Table 2. Results
for these models are therefore omitted.
We also show the qualitative results for CUB in Appendix H.
Table 2: Evaluation of baseline MMVAE, MVAE, JMVAE and their contrastive variations (cI-<MODEL> and
cC-<MODEL>) on CUB image(img)-caption(cap) dataset, using 100% (top) and 20% (bottom) of data.
Data	Methods	Joint Coherence (CCA)	Cross coherence (CCA)		Synergy coherence (CCA)	
			img→cap	cap→ img	joint→img	joint→cap
	MMVAE	0.212 (±2.94e-2)	0.154 (±7.05e-3)	0.244 (±5.83e-3)	-	-
	cI-MMVAE	0.314 (±3.12e-2)	0.188 (±4.02e-3)	0.334 (±1.20e-2)	-	-
100% of data used	cC-MMVAE	0.263 (±1.47e-2)	0.244 (±2.24e-2)	0.369 (±3.18e-3)	-	-
	MVAE	0.075 (±8.71e-2)	-0.008 (±1.41e-4)	0.000 (± 1.84e-3)	-0.002 (±4.95e-3)	0.001 (±1.13e-3)
	cI-MVAE	0.209 (±3.12e-2)	0.247 (±1.12e-4)	-0.008 (±3.99e-4)	0.218 (±5.27e-3)	0.148 (±9.23e-3)
	JMVAE	0.220 (±1.19e-2)	0.157 (±4.98e-2)	0.191 (± 3.11e-2)	0.212 (±1.23e-2)	0.143 (±1.14e-1)
	cI-JMVAE	0.255 (±3.24e-3)	0.149 (±1.25e-3)	0.226 (±7.48e-2)	0.202 (±1.12e-4)	0.176 (±6.23e-2)
	MMVAE	0.117 (±1.51e-2)	0.094 (±7.21e-3)	0.153 (±1.47e-2)	-	-
	cI-MMVAE	0.206 (±1.65e-2)	0.136 (±1.51e-2)	0.251 (±2.39e-2)	-	-
20% of data used	cC-MMVAE	0.226 (±4.69e-2)	0.188 (±2.80e-2)	0.273 (±8.67e-3)	-	-
	MVAE	0.091 (±2.63e-2)	0.008 (±4.81e-3)	0.005 (±7.50e-3)	0.020 (±8.77e-3)	0.009 (±1.17e-2)
	cI-MVAE	0.132 (±3.33e-2)	0.192 (±3.91e-2)	-0.002 (±3.61e-3)	0.162 (±7.89e-2)	0.081 (±3.82e-2)
	JMVAE	0.127 (±3.76e-2)	0.118 (±3.82e-3)	0.154 (±8.34e-3)	0.181 (±1.26e-2)	0.139 (±1.33e-2)
	cI-JMVAE	0.269 (±1.20e-2)	0.134 (±4.24e-4)	0.210 (±2.35e-2)	0.192 (±1.41e-4)	0.168 (±3.82e-3)
4.4 Data Efficiency
Finding: Contrastive learning on 20% of data matches baseline models on full data.
We plot the quantitative performance of MMVAE with and without contrastive learning against the
percentage of the original dataset used, as seen in Figure 6. We observe that the performance of con-
trastive MMVAE with the IWAE (cI-MMVAE, red) and CUBO estimators (cC-MMVAE, yellow) are
consistently better than the baselines (MMVAE, blue), and that baseline performance using all related
data is matched by the contrastive MMVAE using just 10—20% of data. The partial datasets used
here are constructed by first taking n% of each unimodal dataset, then pairing to create multimodal
datasets (§ 4.1)—ensuring it contains the requisite amount of “related” samples. In addition, we
reproduce results generated from using 100% of the data in MNIST-SVHN and CUB (Tables 1 and 2)
using only 20% of the original multimodal datasets (Tables 1 and 2 (bottom)). Comparing results
between top vs. bottom in Tables 1 and 2 shows that this finding holds across the models, on both
MNIST-SVHN and CUB datasets. This shows that the efficiency gains from a contrastive approach is
invariant to VAE type, data, and metrics used, underscoring its effectiveness.
4.5	Label propagation
Finding: Generative models learned contrastively are good predictors of “relatedness”, enabling
label propagation and matching baseline performance on full datasets, using only 10% of data.
7
Published as a conference paper at ICLR 2021
Percentage or data used (%)
(a) Cross coherence
(b) Joint coherence
Figure 6: Performance of MMVAE, cI-MMVAE and cC-MMVAE using n% of MNIST-SVHN.
(c) Latent accuracy
Here, we show that our contrastive framework encourages a larger discrepancy between the PMI
of related vs. unrelated data, as set out in hypothesis 3.1, allowing one to first train the model on a
small subset of related data, and subsequently construct a classifier using PMI that identifies related
samples in the remaining data. We now introduce our pipeline for label propagation in details.
Figure 7: Pipeline of label propagation
Pipeline As showing in Figure 7, we first construct a full dataset by randomly matching instances
in MNIST and SVHN, and denote the related pairs by Fr (full, related). We further assume access
to only n% of Fr, denoted as Sr (small, related), and denote the rest as Fm, containing a mix
of related and unrelated pairs. Next, we train a generative model g on Sr . To find a relatedness
threshold, we construct a small, mixed dataset Sm by randomly matching samples across modalities
in Sr. Given relatedness ground-truth for Sm, we can compute the PMI I(x, y) = logpΘ(x, y) -
log pθx (x)pθy (y) for all pairs (x, y) in Sm and estimate an optimal threshold. This threshold can
now be applied to the full, mixed dataset Fm to identify related pairs giving us a new related dataset
Fr, which can be used to further improve the performance of the generative model g.
Results In Figure 8 (a-e), we plot the performance of baseline MMVAE (blue) and contrastive
MMVAE with IWAE (CI-MMVAE, red) and CUBO(CC-MMVAE, yellow) estimators for term ② of
Eq (4), trained with (solid curves) and without (dotted curves) label propagation. Here, the x-axis is
the proportion in size of Sr to Fr , i.e. the percentage of related data used to pretrain the generative
model before label propagation. We compare these results to MMVAE trained on all related data Fr
(cyan horizontal line) as a “best case scenario” of these training regimes.
Clearly, label propagation using a contrastive model with IWAE estimator is helpful, and in general
the improvement is greater when less data is available; Figure 8 also shows that when Sr is 10% of
Fr, cI-MMVAE is competitive with the performance of baseline MMVAE trained on Fr .
For baselines MMVAE and cC-MMVAE however, label propagation hurts performance no matter
the size of Sr , as shown by the blue and yellow curves in Figure 8 (a-e). The advantages of cI-
MMVAE is further demonstrated in Figure 8f, where we compute the precision, recall, and F1
score of relatedness prediction on Fm , for models trained on 10% of all related data. We also
compare to a simple label-propagation baseline, where the relatedness of Fm is predicted using a
siamese network (Hadsell et al., 2006) trained on the same 10% dataset. Notably, while the Siamese
baseline in Figure 8f is a competitive predictor of relatedness, cI-MMVAE has the highest F1 score
amongst all four, making it the most reliable indicator of relatedness. Beyond that, note that with
the contrastive MMVAE, relatedness can be predicted without additional training and only requires
a simple threshold computation directly computed using the generative model. The fact that the
cI-MMVAE’s relatedness-prediction performance is the only one that matches the Siamese baseline
strongly supports the view that the contrastive loss encourages generative models to utilise and better
learn the relatedness between multimodal pairs; in addition, the poor performance of cC-MMVAE
shows that the advantages of having a bounded estimation to the contrastive objective by using an
8
Published as a conference paper at ICLR 2021
(b) Cross coherence, M⇒S
Percentage of data used (∣Sr∣∕∣Fr∣%)
(a) Cross coherence, S⇒M
Percentage of data used (∣Sr∣∕∣Fr∣%)
(c) Latent accuracy, MNIST
Methods Precision (%) Recall (%) Fl
MMVAE	36J3	1320	0.193
cI-MMVAE	65.70	17.02	0.270
cC-MMVAE	42.71	15.12	0.223
Siamese	68.20	14.74	0.242
Percentage of data used (∖Sr∖∕∖Fr∖%)	Percentage of data used (∣Sr∣∕∣Fr∣%)
(d) Latent accuracy, SVHN	(e) Joint coherence	(f) Precision, recall, Fi score
Figure 8: Models with and without label propagation using MMVAE, cI-MMVAE and cC-MMVAE.
upper-bound for ② is overshadowed by the high bias of CUBO, and that one may benefit more from
choosing a low variance lower-bound like IWAE.
5	Conclusion
We introduced a contrastive-style objective for multimodal VAE, aiming at reducing the amount of
multimodal data needed by exploiting the distinction between "related" and "unrelated" multimodal
pairs. We showed that this objective improves multimodal training, drastically reduce the amount
of multimodal data needed, and establishes a strong sense of "relatedness" for the generative model.
These findings hold true across a multitude of datasets, models and metrics. The positive results
of our method indicates that it is beneficial to utilise the relatedness information when training on
multimodal data, which has been largely ignored in previous work. While we propose to utilise it
implicitly through contrastive loss, one may consider relatedness as a random variable in the graphical
model and see if explicit dependency on relatedness can be useful. It is also possible to extend
this idea to Generative adversarial networks (GANs), by employing an additional discriminator
that evaluates relatedness between generations across modalities. We will leave these interesting
directions to be explored by future work.
Acknowledgements
YS and PHST were supported by the Royal Academy of Engineering under the Research Chair
and Senior Research Fellowships scheme, EPSRC/MURI grant EP/N019474/1 and FiveAI. YS was
additionally supported by Remarkdip through their PhD Scholarship Programme. BP is supported by
the Alan Turing Institute under the EPSRC grant EP/N510129/1. Special thanks to Elise van der Pol
for helpful discussions on contrastive learning.
9
Published as a conference paper at ICLR 2021
References
A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In C. E. Brodley and
A. P. Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning (ICML
2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001, pages 19-26. Morgan Kaufmann,
2001.
A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh
annual conference on Computational learning theory, pages 92-100, 1998.
Y. Burda, R. B. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. In Y. Bengio and Y. LeCun,
editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.00519.
M. C. Burkhart and K. Shan. Deep low-density separation for semi-supervised classification. In International
Conference on Computational Science, pages 297-311, 2020.
T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton. A simple framework for contrastive learning of visual
representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607.
PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm.
Journal of the royal statistical society series b-methodological, 39(1):1-22, 1977.
A. B. Dieng, D. Tran, R. Ranganath, J. W. Paisley, and D. M. Blei. Variational inference via χ upper bound
minimization. In 31st Annual Conference on Neural Information Processing Systems, NIPS 2017, pages
2732-2741, 2017.
W. Guo, J. Wang, and S. Wang. Deep multimodal representation learning: A survey. IEEE Access, 7:63373-
63394, 2019.
R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages
1735-1742, 2006.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages
770-778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.
1109/CVPR.2016.90.
K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick. Momentum contrast for unsupervised visual representation
learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,
WA, USA, June 13-19, 2020, pages 9726-9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL
https://doi.org/10.1109/CVPR42600.2020.00975.
O. J. H6naff. Data-efficient image recognition With contrastive predictive coding. In Proceedings Ofthe
37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research, pages 4182-4192. PMLR, 2020. URL http://
proceedings.mlr.press/v119/henaff20a.html.
R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. GreWal, P. Bachman, A. Trischler, and Y. Bengio. Learning
deep representations by mutual information estimation and maximization. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenRevieW.net, 2019. URL
https://openreview.net/forum?id=Bklr3j0cKX.
C.-C. Hsu, C.-W. Lin, W.-T. Su, and G. Cheung. Sigan: Siamese generative adversarial netWork for identity-
preserving face hallucination. IEEE Transactions on Image Processing, 28(12):6225-6236, 2019.
T. Joachims. Transductive inference for text classification using support vector machines. In ICML ’99
Proceedings of the Sixteenth International Conference on Machine Learning, pages 200-209, 1999.
D. Massiceti, P. K. Dokania, N. Siddharth, and P. H. Torr. Visual dialogue Without vision or dialogue. In NeurIPS
Workshop on Critiquing and Correcting Trends in Machine Learning.
D. J. Miller and H. S. Uyar. A mixture of experts classifier With learning based on both labelled and unlabelled
data. In Advances in Neural Information Processing Systems 9, pages 571-577, 1996.
10
Published as a conference paper at ICLR 2021
A. Mnih and K. Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estima-
tion. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Process-
ing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United
States, pages 2265-2273, 2013. URL https://prOceedings .neurips.ee/paper/2013/hash/
db2b4182156b2f1f817860ac9f409ad7-Abstract.html.
K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents
using em. Machine Learning, 39(2):103-134, 2000.
S. E. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep representations of fine-grained visual descriptions.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,
June 27-30, 2016, pages 49-58. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.13. URL https:
//dOi.Org/10.1109/CVPR.2016.13.
Y. Shi, S. Narayanaswamy, B. Paige, and P. H. S. Torr. Variational mixture-of-experts autoencoders for multi-
modal deep generative models. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’AlchC-Buc, E. B.
Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 15692-15703, 2019. URL https://prOeeedings.neurips.ee/paper/2019/
hash/0ae775a8eb3b499ad1fea944e6f5e836-Abstraet.html.
H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep metric learning via lifted structured feature embedding.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,
June 27-30, 2016, pages 4004-4012. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.434. URL
https://dOi.Org/10.1109/CVPR.2016.434.
M. Suzuki, K. Nakayama, and Y. Matsuo. Joint multimodal learning with deep generative models. In Interna-
tional Conference on Learning Representations Workshop.
Y. H. Tsai, P. P. Liang, A. Zadeh, L. Morency, and R. Salakhutdinov. Learning factorized multimodal representa-
tions. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net, 2019. URL https://Openreview.net/fOrum?id=rygqqsA9KX.
A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748, 2018.
R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative models of visually grounded imagination. In 6th
International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://Openreview.net/
fOrum?id=HkCsm6lRb.
T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformity on
the hypersphere. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9929-9939.
PMLR, 2020. URL http://prOeeedings.mlr.press/v119/wang20k.html.
K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest
neighbor classification. In Advances in Neural Information Processing Systems 18 [Neural Informa-
tion Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada],
pages 1473-1480, 2005. URL https://prOeeedings.neurips.ee/paper/2005/hash/
a7f592eef8b130a6967a90617db5681b-Abstraet.html.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.
Technical Report CNS-TR-2010-001, California Institute of Technology, a.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.
Technical Report CNS-TR-2010-001, California Institute of Technology, b.
M. Wu and N. D. Goodman. Multimodal generative models for scalable weakly-supervised learn-
ing. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neu-
ral Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada,
pages 5580-5590, 2018. URL https://prOeeedings.neurips.ee/paper/2018/hash/
1102a326d5f7e9e04fe3e89d0ede88e9-Abstraet.html.
G. Yildirim, N. Jetchev, and U. Bergmann. Unlabeled disentangling of gans with guided siamese networks.
2018.
11
Published as a conference paper at ICLR 2021
I. Yildirim. From perception to conception: Learning multisensory representations. 2014.
C.	Zhang, Z. Yang, X. He, and L. Deng. Multimodal intelligence: Representation learning, information fusion,
and applications. IEEE Journal of Selected Topics in Signal Processing, pages 1-1, 2020.
D.	Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency.
In S. Thrun, L. K. Saul, and B. Scholkopf, editors, Advances in Neural Information Processing Systems 16
[Neural Information Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British
Columbia, Canada], pages 321-328. MIT Press, 2003. URL https://proceedings.neurips.cc/
paper/2003/hash/87682805257e619d49b8e0dfdc14affa-Abstract.html.
Z.-H. Zhou and M. Li. Tri-training: exploiting unlabeled data using three classifiers. IEEE Transactions on
Knowledge and Data Engineering, 17(11):1529-1541, 2005.
Z.-H. Zhou and M. Li. Semi-supervised learning by disagreement. Knowledge and Information Systems, 24(3):
415-439, 2010.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian fields and harmonic
functions. In T. Fawcett and N. Mishra, editors, Machine Learning, Proceedings of the Twentieth International
Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA, pages 912-919. AAAI Press, 2003.
URL http://www.aaai.org/Library/ICML/2003/icml03-118.php.
12
Published as a conference paper at ICLR 2021
Appendix:
A	From pointwise mutual information to joint marginal
LIKELIHOOD
In this section, we show that for the purpose of utilising the relatedness between mutlimodal pairs,
maximising the difference between pointwise mutual information between related points x, y and
unrelated points x, y0 is equivalent to maximising the difference between their log joint marginal
likelihoods.
To see this, we can expand I(x, y) - I(x, y0) as
I(x,y) - I(x, y0)
= [log pΘ(x, y) - log pΘ (x) - logpΘ(y)] - [logpΘ(x,y0) - log pΘ (x) - logpΘ (y0)]
= logpΘ(x,y) - logpΘ(x,y0) +logpΘ(y0) - log pΘ(y) .	(6)
X----------{z----------} X---------V--------}
① ②
In (6) the PMI difference is decomposed as two terms:① the difference between joint marginal
likelihoods and ② the difference between marginal likelihoods of different instances of y-s. It is
clear that since ② involves only one modality and only accounts for the difference in values between
y and y0 , it is not relevant to the relatedness of x, y .
Therefore, for the purpose of utilising relatedness information, we only need to maximise I(x, y) -
I(x, y0) through maximising term ①,i.e. the difference between joint marginal likelihoods.
B Connection of final objective to Pointwise Mutual
Information
Here, we show that minimising the objective in (4) maximises the PMI between x and y :
L(χ, y) = -Ylogpθ(x,y) + 1 I log X Pθ(χ0,y)+iog X pθ(x,y0) I
x0∈X	y0∈Y
=-(γ - 1)logpθ(x, y) - 1 log P------- pθ(XPy)------O——ʌ
2	2	y0∈Y pΘ(x, y )	x0∈X pΘ (y, x )
≈ 一(Y - I)IogPθ(χ, y) - 1 log pΘ(x, y∖	⑺
2	2	pΘ (x)pΘ (y)
X-----V-----}
I(x,y)
We see in (7) that minimising L can be decomposed to maximising both the joint marginal likelihood
pΘ(x, y) and an approximation of PMI I(x, y). Note that since Y > 1, we can be sure that the joint
marginal likelihood weighting Y - 2 is non-negative.
13
Published as a conference paper at ICLR 2021
C GENERALISATION TO M > 2 MODALITIES
In this section we show how the contrastive loss generalise to cases where number of modalities
considered M is greater than 2.
Given observations from M modalities D = {X1,X2,…，Xm,…，XM}, where Xm denotes
unimodal dataset of modalit m of size Nm, i.e. Xm = {x(mi)}iN=m1. Similar to (2), we can write the
assymetrical contrastive loss for any observation x(mi) from modality m, where negative samples are
taken for all (M - 1) other modalities:
MN
LC(Xm), Dm) = - logPΘ(XIiM) + log X XPΘ(XIi(d-i),(d+i)M, Xdj)).	⑻
d=1 j=1
d6=m j 6=i
We can therefore rewrite (3) as:
(
MN
log XXpΘ (
1:(d-1),(d+1):M
d=1 j=1
d6=m j6=i
∖
Xdj))
)
(9)
(10)
where N is the number of negative samples, all log pΘ (X1:M) are approximated by the following
joint ELBO for M modalities:
logpΘ (X1:M) ≥ Ez 〜qφ(z∣xi:M)
log
Pθ(z, X1:M)
qφ(Z | X1：M)
ELBO(X1:M).
(11)
While the above gives us the true generalisation of (3), we note that the number of times where
ELBO needs to be evaluated in (10) is O(M 2N), making it difficult to implement this objective
in practice, especially on more complicated datasets. We therefore propose a simplified version of
the objective, where we estimate the second term of (10) with N sets of random samples from all
modalities. Specifically, we can precompute the following M × N random index matrix J:
jll	jl2	•…	jlN
j21	j22	…	j2N
J =	.	.
..
..
jM 1 j'M 2	…j'MN-
(12)
where each entry of J is a random integer taken from range [1, Nm]. We can then replace the second
term of (10) random samples selected by the indices in J, giving us
LC(XIiM) ≈ - logPθ(X*jiM) + ^log Xpθ(XJ1n), X2J2n),…XMMn))) .	(13)
The number of times ELBO needs to be computed is now O(N), and is no longer relevant to the
number of modalities M .
We can now also generalise the final objective in (4) to M modalities:
L(XIiM) = -YELBO(XIiM) + llogsUmexp ELBO(XJ1n), X2J2n), .一 XMMn)) ) .	(14)
n∈N
14
Published as a conference paper at ICLR 2021
D THE INEFFECTIVENESS OF TRAINING WITH LC ONLY
We demonstrate why training with the contrastive loss proposed in (3) is ineffective, and why
additional ELBO term is needed for the final objective. As we show in Figure 9, when training with
LC only, while the contrastive loss (green) quickly drops to zero, both term ① and ② in (3) also
reduces drastically. This means the joint marginal likelihood of any generation log pΘ (x, y) is small
regardless the relatedness of (x, y).
Figure 9: First 300 iterations of training using contrastive loss LC only.
In comparison, we also plot the training curve for model trained on the final objective in (4), which
upweights term ① in (3) by γ. We see in Figure 10 that by setting Y = 2,the joint marginal likelihood
(yellow and blue curve) improves during training, while LC (green curve) gets minimised.
Figure 10: First 300 iterations of training with final loss L, where γ = 2.
15
Published as a conference paper at ICLR 2021
E	ABLATION STUDY OF γ
In § 3, we specified that γ needs to be greater than 1 to offset the negative effect of minimising ELBO
through term ② in (4). Here, We study the effect of Y in details.
Figure 11 compares latent accuracy, cross coherence and joint coherence of MMVAE on MNIST-
SVHN dataset trained on different values of γ. Note that here We only consider cases Where γ ≥ 1,
since the minimum value of γ is 1. In this case, the loss reduces to the original contrastive objective
in (3).
A feW interesting observations from the plot are as folloWs: First, When γ = 1, the model is trained
using the contrastive loss only, and as We shoWed is an ineffective objective for generative model
learning. This is verified again in Figure 11 — When γ = 1, both coherences and latent accuracies
take on extremely loW values; interestingly, there is a significant boost of performance across all
metrics by simply increasing the value of γ from 1 to 1.1; after that, as the value of γ increases,
performance on most metrics decreases monotonically (joint coherence being the only exception),
and eventually converges to baseline MMVAE (dotted lines in Figure 11). This is unsurprising, since
the final objective in (4) reduces to the original joint ELBO as γ approaches infinity.
(％) ΦUCΦ^Φ^OO^>U2□UU<
Figure 11: Performance on different metrics for different values of γ . Dotted lines represents the performance of
baseline MMVAE.
Figure 11 seems to suggest that 1.1 is the optimal value for hyperparameter γ, hoWever close
inspection of the qualitative generative results shoWs that this might not be the case. See Figure 12
for a comparison of the model’s generation betWeen MMVAE models trained on (from left to right)
γ = 1.1, γ = 2 and γ = +∞ (i.e. original MMVAE). Although γ = 1.1 yields model With high
coherence scores, We can clearly see from the left-most column of Figure 12 that the generation of
the model seems deprecated, especially for the SVHN modality, Where the backgrounds of model’s
generation appear to be unnaturally spotty and deformed. This problem is mitigated by increasing
γ — as shoWn in Figure 11, the image generation quality of γ = 2 (middle column) is not visibly
different from that of γ = +∞ (right column).
To verify this observation, We also compute the marginal log likelihood log pΘ (x, y) to quantify the
quality of generations. We compute this for all γs considered in Figure 11, and take the average
over the entire test set. From the results in Figure 13, We can see a significant increase of the log
likelihood betWeen γ = 1.1 to γ = 1. This gain in image generation quality then sloWs doWn as γ
further increases, and as all other metrics converges to the original MMVAE model.
16
Published as a conference paper at ICLR 2021
γ = 1.1
+∞
2
γ
γ
(a) Joint generation, MNIST
(b) Joint generation, SVHN
52BH3/53
¾J653∕ς3
(c) Reconstruction, MNIST
52BH3/53
^BOHIIΠSf3i
(d) Reconstruction, SVHN
物的3仙3
判313
(f) Cross generation, M→S
(e) Cross generation, S→M


Figure 12: Generations of MMVAE model trained using the final contrastive objective, with (from left to
right) γ = 1.1, 2 and +∞. Note in (c), (d), (e), (f), the top rows are the inputs and the bottom rows are their
corresponding reconstruction/cross generation.
-3000
-4000
-2000
-2500
Figure 13: Performance on different metrics for different values of γ. Dotted lines represents the performance of
baseline MMVAE.
17
Published as a conference paper at ICLR 2021
F Architecture
We use architectures listed in Table 3 for the unimodal encoder and decoder for MMVAE, MVAE
and JMVAE. For JMVAE we use an extra joint encoder, the architecture of which is described in
Table 4.
Encoder	Decoder
Input ∈ R1x28x28	Input ∈ RL
FC. 400 ReLU	FC. 400 ReLU
FC. L, FC. L	FC. 1 x 28 x 28 Sigmoid
(a)	MNIST dataset
Encoder
Input ∈ R3x32x32
4x4 conv. 32 stride 2 pad 1 & ReLU
4x4 conv. 64 stride 2 pad 1 & ReLU
4x4 conv. 128 stride 2 pad 1 & ReLU
4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1 pad 0
Decoder
Input ∈ RL
4x4 upconv. 128 stride 1 pad 0 & ReLU
4x4 upconv. 64 stride 2 pad 1 & ReLU
4x4 upconv. 32 stride 2 pad 1 & ReLU
4x4 upconv. 3 stride 2 pad 1 & Sigmoid
(b)	SVHN dataset.
Encoder	Decoder
Input ∈ R2048	Input ∈ RL
FC. 1024 ELU	FC. 256 ELU
FC. 512 ELU	FC. 512 ELU
FC. 256 ELU	FC. 1024 ELU
FC. L, FC. L	FC. 2048
(c)	CUB image dataset.
Encoder
Input ∈ R1590
Word Emb. 128
4x4 conv. 32 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 64 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 128 stride 2 pad 1 & BatchNorm2d & ReLU
1x4 conv. 256 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
1x4 conv. 512 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1 pad 0
Decoder
Input ∈ RL
4x4 upconv. 512 stride 1 pad 0 & ReLU
1x4 upconv. 256 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
1x4 upconv. 128 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
4x4 upconv. 64 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 upconv. 32 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 upconv. 1 stride 2 pad 1 & ReLU
Word Emb.T 1590
(d)	CUB-Language dataset.
Table 3:	Unimodal encoder and decoder architectures.
18
Published as a conference paper at ICLR 2021
Encoder
Input ∈ R3x32x64
4x4 conv. 32 stride 2 pad 1 & ReLU
4x4 conv. 64 stride 2 pad 1 & ReLU
4x4 conv. 128 stride 2 pad 1 & ReLU
1x4 conv. 128 stride 1x2 pad 0x1 & ReLU
4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1 pad 0
(a)	MNIST-SVHN dataset.
Encoder
Input ∈ R1x32x192
4x4 conv. 32 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 64 stride 2 pad 1 & BatchNorm2d & ReLU
4x4 conv. 128 stride 2 pad 1 & BatchNorm2d & ReLU
1x4 conv. 256 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
1x4 conv. 512 stride 1x2 pad 0x1 & BatchNorm2d & ReLU
4x4 conv. L stride 1 pad 0, 4x6 conv. L stride 1 pad 0
(b)	CUB Image-Caption dataset.
Table 4:	Joint encoder architectures.
19
Published as a conference paper at ICLR 2021
G Qualitative Results on MNIST-SVHN
G. 1 Generative Results & Marginal Likelihoods on MNIST-SVHN
G.1.1 MMVAE
MMVAE
cI-MMVAE
cC-MMVAE
(a) Joint generation, MNIST
(b) Joint generation, SVHN
(c) Reconstruction, MNIST
夕4	2 O	7夕\	\
7	'I	N C	2	3	I	I
(d) Reconstruction, SVHN
(e) Cross generation, S→M
(f) Cross generation, M→S
3 q矽43 7 P 3
3 q 8 V 3 7 7 3
,藏 EE IflB M
逐速IflD »
4 - ? 7 %
Figure 14: Generations of MMVAE model, from left to right are original model (MMVAE), contrastive loss with
IWAE estimator (cI-MMVAE) and contrastive loss with CUBO estimator (cC-MMVAE).
	log p(xm , xn)	logP(Xm | Xm, Xn)	log p(Xm | Xm)	log p(Xm | Xn)
MMVAE	-1879.00	-388.59	-388.59	-1618.53
m = MNIST, n=SVHN cI-MMVAE	-1904.15	-385.18	-385.18	-1620.77
cC-MMVAE	-1924.20	-391.88	-391.84	-1619.34
MMVAE	-1879.00	-1472.44	-1472.45	-431.66
m = SVHN, n = MNIST cI-MMVAE	-1904.15	-1491.55	-1491.56	-444.28
cC-MMVAE	-1924.20	-1490.56	-1494.75	-428.29
Table 5: Evaluating log likelihoods using original model (MMVAE), contrastive loss with IWAE estimator
(cI-MMVAE) and contrastive loss with CUBO estimator (cC-MMVAE). Likelihoods are estimated with IWAE
estimator using 1000 samples.
20
Published as a conference paper at ICLR 2021
cC-MVAE
cI-MVAE
G.1.2 MVAE
MVAE
8d 3,954 二
43 9，；54
L 々孑9WST 4
3 "£X LJ 5r9
d 21JE夕d
OaaaJOdd
7 4 "，5 白 - 4 N
Sf7£5:35?
733UG85,
4 Qn夕5 1 / 4 工
2 7 3，C S，％
Or ，今 4，• 4
—二,匕〜Jr q 7夕
/Gt> S。2 / ♦
Jo 3 q3∕∙f87
Z 3 7 〜，A 2 :
VmQ *4dx2 小
4/ PIJ55 q
，-vs7*l Tb 2 ⅜s
£672 kτ 1
7。TA :/Z 6
，外；tr G√,∙∙τ )
L∙ ? / /r ? r< 夕,土
(c) Reconstruction, MNIST
(d) Reconstruction, SVHN
(e) Cross generation, S→M
(f) Cross generation, M→S
行rG35
Il I 11”
Figure 15: Generations of MVAE model, from left to right are original model (MVAE), contrastive loss with
IWAE estimator (cI-MVAE), contrastive loss with CUBO estimator (cC-MVAE).
	log p(xm, xn)	logP(Xm | Xm, Xn)	log p(Xm | Xm)	log p(Xm | Xn)
MVAE	-404.43	-404.43	-388.27	-1847.85
m = MNIST, n=SVHN cI-MVAE	-406.85	-406.22	-388.26	-1876.95
cC-MVAE	-405.24	-432.96	-388.26	-1889.35
MVAE	-404.43	-1518.00	-1488.21	-440.32
m = SVHN, n = MNIST cI-MVAE	-406.85	-1529.32	-1498.73	-443.04
cC-MVAE	-405.24	-1520.23	-1499.47	-441.01
Table 6: Evaluating log likelihoods using original model (MVAE), contrastive loss with IWAE estimator (cI-
MVAE) and contrastive loss with CUBO estimator (cC-MVAE). Likelihoods are estimated with IWAE estimator
using 1000 samples.
21
Published as a conference paper at ICLR 2021
G.1.3 JMVAE
JMVAE
cI-JMVAE
cC-JMVAE
(b) Joint generation, SVHN
(c) Reconstruction, MNIST
(d) Reconstruction, SVHN
・■5常口嚣

Figure 16:	Generations of JMVAE model, from left to right are original model (JMVAE), contrastive loss with
IWAE estimator (cI-JMVAE), contrastive loss with CUBO estimator (cC-JMVAE).
	logP(Xm, Xn)	log p(xm | xm, xn)
JMVAE	-515.44	-497.95
m = MNIST, n = SVHN cI-JMVAE	-518.56	-511.28
cC-JMVAE	-534.26	-510.90
JMVAE	-515.44	-1515.73
m = SVHN, n = MNIST cI-JMVAE	-518.56	-1529.44
cC-JMVAE	-534.26	-1614.78
Table 7: Evaluating log likelihoods using original model (JMVAE), contrastive loss with IWAE estimator
(cI-JMVAE) and contrastive loss with CUBO estimator (cC-JMVAE). Likelihoods are estimated with IWAE
estimator using 1000 samples.
22
Published as a conference paper at ICLR 2021
G.2 Generation diversity
To further demonstrate that the improvements from our contrastive objective did not come at a price
of sacrifising generation diversity, in Figure 17 we show histograms of the number of examples
generated for each class with samples from prior. Similar to how we compute joint coherence, the
class label of generated images are determined using classifiers trained on the original MNIST and
SVHN dataset.
(g) JMVAE	(h) cI-JMVAE	(i) cC-JMVAE
Figure 17:	Number of examples generated for each of class during joint generation.
We Can see that the contrastive models (cI-* and cI-*) are capable of generating examples from
different classes, and for MMVAE and MVAE the histogram of contrastive models are more uniform
than the original models (less variance between class counts).
23
Published as a conference paper at ICLR 2021
H Qualitative results on CUB
The generative results of MMVAE, cI-MMVAE and cC-MMVAE on CUB Image-Caption dataset are
as shown in Figure 18, Figure 19 and Figure 20. Note that for the generation in the vision modality,
we reconstruct and generate features from ResNet101 and perform nearest neighbour search in all the
features in train set to showcase our generation results.
Image f Image
Sentence f Sentence
[DATA] ==> a small bird with a white belly and a grey wing and a
short beak, <eos>
[RECON] ==> a small bird with a white belly and a brown, and a
short beak, <eos>
[DAIA] — — > this bird is brown with white and has a long, pointy
beak, <eos>
[RECON] ==> this bird is black with white and has a long, pointy
beak, <eos>
[DAIA] ==> this bird has wings that are grey and has a yellow tail.
<eos>
[RECON] ==> this bird has wings that are grey and has a yellow
belly. VeOSA
[DAIA] ==> this bird is white with brown and has a veιy short beak.
VeOSA
[RECON] ==> this bird is white with belly and and a a short beak.
<eos>
[DA1A] ==> a small sized bird that has
a creamy belly with a short pointed bill,
<eos>
[DATA] ==> small, mostly yellow bird,
with brown, white, and black stripes on
his wings and tail. VeOSA
[DATA] ==> this bird is grey with
black and has a long, pointy beak.
<eos>
[GEN] ==> this SmalI
bird with a brown bill
and white brown and a
a of its. <eos>
> tms Dira is
yellow and yellov⅛ with
a grey bill, <eos>
[RECON] ==> this bird is grey with are
and has a veιy short beak . <eos>
[RECON] ==> tbls is is black bird with a
black and and a and a and beak . <eos>
[RECON] ==> this bird bird with long
white neck and orange bill. <eos>
Figure 18: Qualitative results of MMVAE on CUB Image-Caption dataset, including reconstruction (vision →
vision, language → language), cross generation (vision → language, language → vision) and joint generation
from prior samples.
IGENJ — — > this is a gray bird that has
<eos>
[GEN] ==> this particular bird has a
white belly and wings that are brown,
<eos>
> tms large
sized bird with a black
and and and and and
black bill <eos>
Figure 19: Qualitative results of MMVAE trained with contrastive loss with IWAE estimator on CUB Image-
Caption dataset, including reconstruction (vision → vision, language → language), cross generation (vision →
language, language → vision) and joint generation from prior samples.
> a small bird, with a white Belly and a grey wmg and a
short beak, <eos>
[RECON] ==> a small bird with a white belly and a brown, and a
short beak, <eos>
[DATA] ==> this bird is brown with white and has a long, pointy
beak, <eos>
[RECON] ==> this bird is black with white and has a long, pointy
beak, <eos>
[DAIA] ==> this bird, has wings that are grey and has a yellow tail.
<eos>
[RECON] ==> this bird has winqs that are σrev and has a veɪɪow
bird bird with with a
and and and black beak
IGENJ ==> this small bird is a molttled
brown all over with a short beak <eos>
[GEN] ==> this bird has wings tħat
are yellow and has a black throat.
IGENJ ==> this bird has a black
crown, a Long and black <eos>
[GEN] ==> this bird has a black crown
and a white and and white belly, <eos>
24
Published as a conference paper at ICLR 2021
gray bill <eos>
[RECON] ==> this bird bird with long
white neck and orange b∏l. <eos>
LRECONJ — — > this is is black bird with a
black and and a and a and beak . <eos>
[DATA] ==> this bird has wings that are blue and has a white beak .
[RECON] ==> this bird has wings that are blue and has an orange
bill. <eos>
[DATA] ==> this bird bas a belly that is
whit with brown sides <eos>
[DATA] ==> the bird has a grey body and a white and grey speckled
chest along With an orange beak. <eos>
[RECON] ==> the bird, has a small bill, and black, and and grey and
and and white feet. <eos>
[DATA] ==> a large and round bird with the colors of black and
wMte feathers . <eos>
[RECON] ==> a white and white bird has a black brown black and
white feet. <eos>
[DATA] ==> this bird has a belly that is white with brown sides .
[RECON] ==> this bird has a belly that is yellow and orange wings .
has very light gray legs, a black and
[DATA] ==> a large and round bird
[RECON] ==> this bird is grey with are
Joint generation from prior samples:
[GEN] ==> SmalI
yellow white grey and
white bird with a short
long beak beak,<eos>
[GEN] ==> this bird is
large yellow black with
a yellow belly and
<eos>
[GEN] ==> a bird with
Long bill bill, and and
white, and brown
crown, <eos>
Figure 20: Qualitative results of MMVAE trained with contrastive loss with CUBO estimator on CUB Image-
Caption dataset, including reconstruction (vision → vision, language → language), cross generation (vision →
language, language → vision) and joint generation from prior samples.
25