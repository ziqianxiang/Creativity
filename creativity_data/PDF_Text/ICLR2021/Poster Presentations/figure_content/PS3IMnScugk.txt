Figure 1: We first train a generative model toreconstruct training pairs (x.y) by construct-ing them from other training pairs (a). Wethen perform data augmentation by samplingfrom this model, preferentially generatingsamples in which y contains rare tokens orsubstructures (b). Dashed boxes show pre-diction targets. Conditional models trainedon the augmented dataset accurately predictoutputs y from new inputs x requiring com-positional generalization (c).
Figure 2: (a) RNN encoders produce contextual embeddings for prototype tokens. (b) In the decoder, a gatedcopy mechanism reuses prototypes and generated output tokens via an attention mechanism (dashed lines).
Figure 3: Generation of a sample. We plot normalized output scores on the left, and attention weights to thedifferent prototypes on the right. The prototypes are on the y axes. The model is recomb-2 model trained onSCAN jump split.
