Figure 1: f(x) = fpwl(x; [0 1 0.5])Figure 2: 2×2 multilinear interpolated latticewith four parameters defining a monotonicallyincreasing function in both dimensions.
Figure 2: 2×2 multilinear interpolated latticewith four parameters defining a monotonicallyincreasing function in both dimensions.
Figure 3: KFL Model Architecturelinear function to transform the feature’s input domain to [0, V[d] - 1]. For categorical inputs, welearn a one dimensional embedding for each category value to map to a real value in [0, V[d] - 1].
Figure 4: UCI Adult IncomeFigure 5: Query Result MatchingFigure 6: User Query IntentFigure 7: Comparison of test results of KFL with different M and V valuesFurthermore, each experiment highlights different qualities of KFL. Experiment 4.1 shows thatincreasing V can sometimes be more important than increasing M, where such an increase canpotentially lead to further boosts in performance with only a linear impact to the model’s time andspace efficiency. This result follows from Proposition 6. In Figure 4, we see that KFL acts like aform of regularization: KFL’s accuracy drops as M gets too high, indicating that we start to overfit and lose this regularization effect as the size of the model class increases. Experiment 4.2 showsthat KFL can also perform well for regression tasks but will likely need a larger M and V for morecomplex problems. It also shows a trade-off between the values of M and V where computationtime, number of parameters, and mse remain pretty much fixed. Experiment 4.3 demonstratesKFL’s ability to create higher-order feature interactions by keeping all features in a single lattice.
Figure 5: Query Result MatchingFigure 6: User Query IntentFigure 7: Comparison of test results of KFL with different M and V valuesFurthermore, each experiment highlights different qualities of KFL. Experiment 4.1 shows thatincreasing V can sometimes be more important than increasing M, where such an increase canpotentially lead to further boosts in performance with only a linear impact to the model’s time andspace efficiency. This result follows from Proposition 6. In Figure 4, we see that KFL acts like aform of regularization: KFL’s accuracy drops as M gets too high, indicating that we start to overfit and lose this regularization effect as the size of the model class increases. Experiment 4.2 showsthat KFL can also perform well for regression tasks but will likely need a larger M and V for morecomplex problems. It also shows a trade-off between the values of M and V where computationtime, number of parameters, and mse remain pretty much fixed. Experiment 4.3 demonstratesKFL’s ability to create higher-order feature interactions by keeping all features in a single lattice.
Figure 6: User Query IntentFigure 7: Comparison of test results of KFL with different M and V valuesFurthermore, each experiment highlights different qualities of KFL. Experiment 4.1 shows thatincreasing V can sometimes be more important than increasing M, where such an increase canpotentially lead to further boosts in performance with only a linear impact to the model’s time andspace efficiency. This result follows from Proposition 6. In Figure 4, we see that KFL acts like aform of regularization: KFL’s accuracy drops as M gets too high, indicating that we start to overfit and lose this regularization effect as the size of the model class increases. Experiment 4.2 showsthat KFL can also perform well for regression tasks but will likely need a larger M and V for morecomplex problems. It also shows a trade-off between the values of M and V where computationtime, number of parameters, and mse remain pretty much fixed. Experiment 4.3 demonstratesKFL’s ability to create higher-order feature interactions by keeping all features in a single lattice.
Figure 7: Comparison of test results of KFL with different M and V valuesFurthermore, each experiment highlights different qualities of KFL. Experiment 4.1 shows thatincreasing V can sometimes be more important than increasing M, where such an increase canpotentially lead to further boosts in performance with only a linear impact to the model’s time andspace efficiency. This result follows from Proposition 6. In Figure 4, we see that KFL acts like aform of regularization: KFL’s accuracy drops as M gets too high, indicating that we start to overfit and lose this regularization effect as the size of the model class increases. Experiment 4.2 showsthat KFL can also perform well for regression tasks but will likely need a larger M and V for morecomplex problems. It also shows a trade-off between the values of M and V where computationtime, number of parameters, and mse remain pretty much fixed. Experiment 4.3 demonstratesKFL’s ability to create higher-order feature interactions by keeping all features in a single lattice.
