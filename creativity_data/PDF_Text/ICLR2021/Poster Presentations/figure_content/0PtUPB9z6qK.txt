Figure 1: Data generating distribution supported on a line and with higher density at the extremities.
Figure 2: Samples at differentiterations of the MCMC chain ofAlgorithm 3 (left to right).
Figure 3: Samples from the GEBM at different stages of sampling using Algorithm 3 and inversetemperature β = 1, on CelebA (Left), Imagenet (Right). Each row represents a sampling trajectoryfrom early stages (leftmost images) to later stages (rightmost images).
Figure 4: Samples from the GEBM at different stages of sampling using Algorithm 3 and inversetemperature β = 1, on Cifar10 and LSUN (Right). Each row represents a sampling trajectory fromearly stages (leftmost images) to later stages (rightmost images).
Figure 5: Samples from the tempered GEBM at different stages of sampling using langevin and inversetemperature β= 100, on Cifar10 (Left), Imagenet (Middle-left), CelebA (Middle-Right) and LSUN(Right). Each row represents a sampling trajectory from early stages (leftmost images) to later stages(rightmost images).
Figure 6: Relative FID score: ratio between FID score of the GEBM QG,E and its base G. (Left)Evolution of the ratio for increasing temperature on the4 datasets after 1000 iterations of (14). (Right)Evolution of the same ratio during MCMC iteration using (14).
Figure 7: (Left): Relative error ∣Cc-cJ∣ on the estimation of the ground truth log-partition functionC by c using either KALE-DV or KALE-F Vs training Epochs on RedWine (Top) and WhiteWine(Bottom) datasets. (Right): Negative log likelihood vs training epochs on both training and test setfor 4 different learning methods (KALE-DV,KALE-F, CD and ML) on RedWine dataset.
Figure 8: Sinkhorn divergence between data generating distribution and the trained model (eitherGAN or GEBM) vs number of hidden units in G(θ1). The left figure represents the one hidden layernetwork and right one is for the MLP. Each data point represents the average over 20 independentruns for each choice of number of hidden units.
