Figure 2: (Right) Two continuous trajectories gen-erated by the DDEs are intersected, mapping -1(resp., 1) to 1 (resp., -1), while (Left) the ODEscannot represent such mapping.
Figure 1: Sketchy diagrams of the NODES and the NDDES, respectively, with the initial value h(0)and the initial function φ(t). The NODEs and the NDDEs act as the feature extractors, and thefollowing layer processes the features with a predefined loss function.
Figure 3: (Left) The data at the time t = 0 and(Right) the transformed data at a sufficient largefinal time T of the DDEs (6). Here, the trans-formed data are linearly separable.
Figure 4: Evolutions of the NDDEs (top) and the NODEs (bottom) in the feature space during thetraining procedure. Here, the evolution of the NODEs is directly produced by the code provided in(Dupont et al., 2019).
Figure 5: Presented are the training losses (a) of the NODEs and the NDDEs on fitting the functiong(x) for d = 2. Also presented are the flows, from the data at the initial time point (b), of the NODEs(d) and the NDDEs (c) after training. The flow of the NODEs is generated by the code provided in(Dupont et al., 2019).
Figure 6:	Comparison of the NDDEs (top) versus the NODEs (bottom) in the fitting a 2-D timeseries with the delay effect in the original system. From the left to the right, the true and the fittedtime series, the true and the fitted trajectories in the phase spaces, the dynamics of the parameters inthe neural networks, and the dynamics of the losses during the training processes.
Figure 7:	The training losses and the test losses of the population dynamics (top) and the Mackey-Glass system (bottom) by using the NDDEs, the NODEs and the ANODEs (where the augmenteddimension equals to 1). The first column in the figures is the training losses. The figures from thesecond column to the fourth column present the test losses over the time intervals of the length τ ,2τ, 5τ. The delays for the two DDEs are both designed as 1, respectively. The other parameters areset as r = 1.8, β = 4.0, n = 9.65, and γ = 2.
Figure 8:	The training loss (left column), the test loss (middle column), and the accuracy (rightcolumn) over 5 realizations for the three image sets, i.e., CIFAR10 (top row), MNIST (middle row),and SVHN (bottom row).
Figure 9:	The phase portraits of the population dynamics in the training and the test stages. Weonly show the 10 phase portraits from the total 100 phase portraits for the training and the testingtime-series. Here, the solid lines correspond to the true dynamics, while the dash lines correspondto the trajectories generated by the trained models in the training duration and in the test duration,respectively.
Figure 10:	The phase portraits of the Makey-Glass systems exhibiting chaotic dynamics in thetraining and the test stages. We only present the 10 phase portraits from the total 100 phase portraitsfor the training and the testing time-series. Here, the solid lines correspond to the true dynamics,while the dash lines correspond to the trajectories generated by the trained models in the trainingduration and in the test duration, respectively.
Figure 11: Evolution of the forward number of the function evaluations (NFEs) during the trainingprocess for each model on the image datesets, CIFAR10, MNIST, and SVHN.
Figure 12: Evolution of the backward number of the function evaluations (NFEs) during the trainingprocess for each model on the image datesets, CIFAR10, MNIST, and SVHN.
