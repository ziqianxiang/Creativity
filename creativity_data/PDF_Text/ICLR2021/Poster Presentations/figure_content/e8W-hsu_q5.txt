Figure 1: Overview of EquivCNP.
Figure 2: Predictive mean and variance of ConvCNP and EquivCNP. The first two columns show theprediction of the models trained on the RBF kernel and the last two columns show the prediction ofthe model trained on the Matern-2 kernel. The target function and sampled data points are the samebetween the top row and bottom row except for the context. At the top row, the context is within thevertical dash line that is sampled from the same range during the training (black circle). In the bottomrow, the new context located out of the training range (white circle) is appended.
Figure 3: The example of training data(top) and test data (bottom).
Figure 4: Image-completion task results. The top row shows the given observation and the otherrows show the mean of the conditional distribution predicted by EquivCNP with the specific groupequivariance: T (2), SO(2), R>0 × SO(2), and SE(2). Two of each column shows the same image,and the difference between two columns is the percentage of context random sampling: 25% and75%. When the size of digits is the same as that of the training set (i.e. not scaling but rotation equalsSO(2) symmetry), T(2) and SE(2) have a good quality, but when the size of digits is smaller thanthat of training set, R>0 × SO(2) has a good performance.
Figure 5: Separable LieConv. Difference between (a) normal LieConv and (b) separable LieConv isthe matrix product 0 and elemente-wise product Θ.
Figure 6: The architecture of EqUivCNP for a 1D regression task. 0 represents dot product and ㊉represents concatenation. ψ is a RBF kernel and φ = [y0, y1, . . . , yK].
Figure 7: Residual Blockand residual connections as shown in Figure 7. The channel of each residual block is 128, the averagefraction is *,and the number of MC sampling is 81.
Figure 8: Predictive mean and variance of ConvCNP and EquivCNP at periodic kernels. First twocolumns show the result without outlier observation and last two columns show the result with outlierobservation.
Figure 9: Predictive mean and variance of EquivCNP that using algorithm proposed in (Gordon et al.,2019). Blue line and region represents EquivCNP and green line and region represents GaussianProcess. Each plot shows diffent sampled data. Although the algorithm is redundant compared withour proposed Algorithm 1 due to using RBF kernel to map the output of LieConv back to a continuousfunction, the result is much smoother than Figure 2 and 8.
Figure 10:	The original data that is used for 2D image-completion task.
Figure 11:	Actual images from original MNIST.
Figure 12:	Image-completion task results using rotated-MNIST. In each image, the 1st and 4thcolumns show context pixels, the 2nd and 5th columns show ground truth images, and the 3rd and6th columns show completion results. As a result, the model misses the completion when the numberof context points is quite a few. On the other hand, when the number of context points is sufficient,the completion results seem well except the SO(2)-equivariant model.
