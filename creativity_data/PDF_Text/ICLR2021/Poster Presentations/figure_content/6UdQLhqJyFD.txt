Figure 1: (Left) Our model consists of CNNs encoding short-term dynamics of each modality andTransformers encoding long-term dynamics of audio-visual information from videos. (Right) Toalleviate excessive memory requirements, we propose an efficient parameter sharing scheme based onmatrix decomposition with low-rank approximation, which allows us to train our model end-to-end.
Figure 2: Comparison of parameter sharing schemes. Ours combines (b) and (c) but decomposesweights in each layer into private and shared parts so only the latter is shared across Transformers.
Figure 3: Loss curves during pretraining under different ablative settings. (a) compares Content-Aware Negative Sampling (CANS) that favors negatives that are dissimilar vs. similar to the positiveinstance. (b) compares different cross-Transformer weight sharing schemes; see the text for details.
