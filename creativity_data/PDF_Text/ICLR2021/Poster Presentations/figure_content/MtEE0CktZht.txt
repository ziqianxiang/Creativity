Figure 1: A motivating example of count-based exploration versus episode-level exploration score.
Figure 2: An overview of RAPID. The past episodes are assigned episodic exploration scores basedon the local view, the global view, and the extrinsic reward. Those highly scored episodes are storedin a small ranking buffer. The agent is then encouraged to reproduce the past good explorationbehaviors, i.e., the episodes in the buffer, with imitation learning.
Figure 3:	Rendering of the procedually-generated environments in our experiments.
Figure 4:	Performance of RAPID against baselines on hard-exploration environments in MiniGrid.
Figure 5:	Analysis of RAPID on MultiRoom-N12-S10. Results for other environments are in Ap-pendix D. (a)(b): the impact of the hyperparameters. (c)(d): the returns without extrinsic rewards(pure exploration) and the corresponding local exploration scores.
Figure 6:	Left: the maximum returns w.r.t. the number of rooms under room size 4 (full curvesin Appendix E). Middle: the maximum returns w.r.t. the room sizes with 4 rooms (full curves inAppendix F). Right: learning curves on MiniWolrd Maze (more results in Appendix G).
Figure 7:	Performance on MuJoCo tasks with episodic reward. i.e., a non-zero reward is onlyprovided at the end of an episode. All the experiments are run 5 times with different random seeds.
Figure 8: Sensitivity analysis ofw1 and w2 (w0 is fixed to 1) in MiniGrid-MultiRoom-N7S8-v0. Allthe experiments are run 3 × 106 timesteps. Note that 3 × 106 is more than enough for RAPID toconverge in this environment (see Figure 4). The average results over 5 independent runs are plotted.
Figure 9: Rendering of MultiRoom-N12-S10 (top row) and KeyCorridor-S4-R3(bottom row) in 4different episodes. The environments are procedually-generated, i.e., a different room is generatedin a new episode.
Figure 10: Rendering of Minigird environments used in the work.
Figure 11: Example generated mazes in 4 diffident episodes.
Figure 14: Impact of training steps S on MiniGrid environments.
Figure 15: ImpaCt of buffer size D on MiniGrid environments.
Figure 16:	ExtrinsiC rewards aChieved by RAPID and baselines with pure explorationD.4 Local Exploration Score of Pure Exploration on MiniGridAMIGO	CURIOSITY	RANDOM	COUNT	SIL	PPOR∏)E1.00.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps le7	timesteps	le70.20.0(a) MultiRoom-N7-S4	(b) MwtiRoom-N10-S40.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps	le7	timesteps	le7(c) MultiRoom-N7-S8	(d) MultiRoom-NIO-SIO0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps le?	timesteps	le7(e) KeyCorridor-S3-R2	(f) KeyCorridor-S3-R3	(g) KeyCorridor-S4-R3	(h) MultiRoom-N12-S10Figure 17:	LoCal exploration sCores aChieved by RAPID and baselines with pure exploration2OPublished as a conference paper at ICLR 2021E
Figure 17:	LoCal exploration sCores aChieved by RAPID and baselines with pure exploration2OPublished as a conference paper at ICLR 2021ELearning Curves with More RoomsEn-SJ0.00.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps le7	timesteps	le7	timesteps	le7	timesteps	le7(a) MultiRoom-N4-S4	(b) MwtiRoom-N8-S4	(C) MwtiRoom-N12-S4	(d) MwtiRoom-N16-S4-0.2	-0.2	-020.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps 1≡7	timesteps	1≡7	timesteps lβ7(e) MultiRoom-N20-S4 (f) MultiRoom-N24-S4 (g) MultiRoom-N28-S4Figure 18:	The learning curves with more roomsF Learning Curves with Large RoomsRAPID	RIDEEn⅞3.10.0	0.5	1.0	1.5	2.0	0.0Umesteps le?
Figure 18:	The learning curves with more roomsF Learning Curves with Large RoomsRAPID	RIDEEn⅞3.10.0	0.5	1.0	1.5	2.0	0.0Umesteps le?0.0 j-------------'0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps	1≡7	timesteps	lβ7(a) MultiRoom-N4-S4 (b) MultiRoom-N4-S8 (c) MultiRoom-N4-S120.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps 1≡7	timesteps	1≡7	timesteps	lβ7(d) MultiRoom-N4-S16	(e) MultiRoom-N4-S20	(f) MultiRoom-N4-S240.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0timesteps le7	timesteps	ie7	timesteps	le7(g) MultiRoom-N4-S28	(h) MultiRoom-N4-S32	(i) MultiRoom-N4-S36Figure 19:	The learning Curves with larger rooms sizes.
Figure 19:	The learning Curves with larger rooms sizes.
Figure 20: Learning curves of RAPID and the ablations on MiniWorld Maze. We observe minorperformance drop when removing the local score, substantial performance drop when removingextrinsic rewards or the buffer.
Figure 21: Impact of training steps on MiniWorld Maze.
Figure 22: Impact of buffer size on MiniWorld Maze.
Figure 23: Extrinsic rewards achieved by RAPID and baselines on MiniWorld Maze with pureexploration.
Figure 24: Local exploration scores achieved by RAPID and baselines on MiniWorld Maze withpure exploration.
Figure 25: Comparison of keeping all the state-action pairs of an episode and not keeping all thestate-action pairs (i.e., a fixed buffer size) on MultiRoom-N7-S8. The experiments are run 5 timeswith different random seeds. We observe no clear difference in the learning curves of these twoimplementations.
Figure 26: (a) is the singleton swimmer environment. In (b) the density is procedurally-generated. In(c) the velocity is procedurally-generated. All the methods tend to learn slower in the procedurally-generated settings. Nevertheless, RAPID is still able to discover good policies in these challengingvariants.
Figure 27: Annealing versus not annealing. Annealing is helpful in KeyCorridor-S3-R2. However,the policy fails to converge with annealing in MultiRoom-N12-S10.
