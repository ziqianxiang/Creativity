Figure 1: Our proposed objective includes a cross-entropy term (CE) and a supervised contrastivelearning (SCL) term, and it is formulated to push examples from the same class close and examplesfrom different classes further apart. We show examples from the SST-2 sentiment analysis datasetfrom the GLUE benchmark, where class A (shown in red) is negative movie reviews and classB (shown in blue) is positive movie reviews. Although we show a binary classification case forsimplicity, the loss is generally applicable to any multi-class classification setting.
Figure 2: tSNE plots of the learned CLS embeddings on the SST-2 test set in the few-shot learningsetting of having 20 labeled examples to fine-tune on - comparing RoBERTa-Large fine-tuned withCE only (left) and with our proposed objective CE+SCL (right) for the SST-2 sentiment analysis task.
Figure 3: tSNE plots of learned CLS embedding on SST-2 test set where we have 20, 100 labeledexamples, and full dataset respectively, comparing CE with and without SCL term. Blue: positiveexamples; red: negative examples.
