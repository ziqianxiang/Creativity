Figure 1: Overview of attention mechanism of SUPerGATs: GO, DP, MX, and SD. Blue circles (e4)represent the unnormalized attention before Softmax and red diamonds (φj) indicate the probabilityof edge between node i and j. The attention mechanism of the original GAT (Velickovic et al., 2018)is in the dashed rectangle.
Figure 2: Distribution of KL divergence between normalized attention and label-agreement on allnodes and layers for Cora dataset (Left: two-layer GAT, Right: four-layer GAT).
Figure 3: Test performance on node classification and link prediction for GO and DP attentionsagainst the mixing coefficient λE. We report accuracy (Cora, CiteSeer, PubMed) and micro f1-score(PPI) for node classification, and AUC for link prediction.
Figure 4: Mean test accuracy gains (of 5 runs) against GATGO on synthetic datasets, varying ho-mophily and average degree of the input graph.
Figure 5: The best-performed graph attention design for synthetic and real-world graphs with variousaverage degree and homophily.
Figure 6: t-SNE plots of node features and edges for synthetic graph examples. Hyperparametersare δ ∈ {0.025 (Top), 0.2 (Bottom)} and pin ∈ {0.1δ (Left), 0.5δ (Center), 0.9δ (Right)}.
Figure 7: Kernel density estimate plot of distribution of degree and per-node homophily in real-world graphs.
Figure 8: Distribution of KL divergence between normalized attention and label-agreement on allnodes and layers for Cora, CiteSeer, PubMed, and PPI (Left: two-layer GAT, Right: four-layerGAT).
Figure 9: Test performance on node classification against the mixing coefficient λE forSuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
Figure 11: Test performance on node classification against the edge sampling ratio pe forSuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
Figure 10: Test performance on node classification against the negative sampling ratio pn forSuperGATMX (Cora, CiteSeer, PubMed) and SuperGATSD (PPI).
