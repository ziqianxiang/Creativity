Figure 1: Complete architecture of EEC. (a) For each new task, a convolutional autoencoder is trained on realimages and a combination of reconstruction loss Lr and content loss Lcont . The encoded episodes are storedin memory and converted into centroids and covariance matrices when the system runs out of memory. (b) TheL = (1 - λ)Lr + λLcont	(3)where, λ is a hyperparamter that controls the contribution of each loss term towards the completeloss. During autoencoder training, classifier D acts as a fixed feature extractor and its parametersare not updated. This portion of the complete procedure is depicted in Figure 1 (a).
Figure 3: Visualization of the encoded episodes (a) and pseudo-encoded episodes generated using a differentnumber of centroids and covariance matrices (b-e). Note that the pseudo-encoded episodes generated from adifferent number of centroids are all similar to the original encoded episodes. However, storing more centroidsgenerates a feature space closer to the original feature space.
Figure 4: Reconstructed Images for CIFAR-10 (top) and ImageNet-50 (bottom) after all tasksmemory SOTA method) with margins of 16.01% and 6.26% on A30 and A50 , respectively, eventhough iCaRL has an unfair advantage of using stored real images.
Figure 5: The accuracy of EEC on A30 and A50 on Table 2: Ablation study results in terms ofImageNet-50 dataset with different memory budgets.	A30 and A50 for ImageNet-50 dataset.
Figure 6: Visualization of the encoded episodes (a), pseudo-encoded episodes generated using 200 centroidsand covariance matrices (b) and pseudo-encoded episodes generated using 2 centroids and covariance matrices(c).
