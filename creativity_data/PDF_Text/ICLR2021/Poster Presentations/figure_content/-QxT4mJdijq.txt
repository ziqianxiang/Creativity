Figure 1: Convolution as translating filters. Left:Standard 1-D convolution slides a filter w along thelength of input x. This operation is translation equivari-ant: translating x will translate y. Right: Standard con-volution is equivalent to a fully connected layer with aparameter sharing pattern: each row contains translatedcopies of the filter. Other equivariant layers will havetheir own sharing patterns.
Figure 2: We reparameterize the weights of each layer in terms of a symmetry matrix U that can enforce equiv-ariant sharing patterns of the filter parameters v. Here we show a U that enforces permutation equivariance.
Figure 3: For each task, the inner loop updates thefilter parameters v to the task using the inner loop loss.
Figure 4: After observing transla-tion equivariant data, MSR enforcesconvolutional parameter sharing on theweight matrix. An example weightmatrix is shown above.
Figure 5: The theoretical con-volutional weight symmetrymatrix for the group〈g)=C4, where g is a N2∏ -radianrotation of a 3x3 image (N ∈{0, 1, 2, 3}. Notice that theimage is flattened into alength 9 vector. The matrixπ (g) describes the action of aN2π radian rotation on this im-age.
Figure 7: MSR produced convolution filters, after meta-learning 90° rotation, 45° rotation, and45° rotation+flip equivariance in the Sec. 5.2 experiments. Notice that MSR learns to achieve thecorresponding equivariance by producing rotated/flipped versions of the same filter.
