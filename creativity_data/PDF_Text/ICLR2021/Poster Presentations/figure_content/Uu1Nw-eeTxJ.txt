Figure 1: Illustration of Hierarchical Contrastive Learning (Hictl). n is the batch size, m denotesthe number of negative samples for word-level contrastive learning. B and V indicates the bag-of-words of the instance hχi, y/ and the overall vocabulary of all languages, respectively.
Figure 2: Illustration of constructing hard negative samples (HNS). A circle (the radius is d+ =kk+ - q ∣∣2) in the embedding space represents a manifold near in which sentences are semanticallyequivalent. We can generate a coherent sample (i.e., k ) that interpolate between known PaIr q andk . The synthetic negative k can be controlled adaptively with proper difficulty during training.
Figure 3: Fine-tuning on NMT task.
Figure 4: Visualizations (t-SNE projection) of sentence embeddings output by HICTL (left) andXLM-R (right). We collect 10 sets of samples from WMT’14-19, each of them contains 100 parallelsentences distributed in 5 languages (i.e., English, French, German, Russian, and Spanish). Eachset is identified by a color and different languages marked by different shapes. We can see thata set of sentences under the same meaning are clustered more densely for HICTL than XLM-R,which reveals the strong capability of HICTL on learning universal representations across differentlanguages.
