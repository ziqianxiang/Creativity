Figure 1: Overview of our architecture. The generator is trained to infer the behavior intents z andforecast the future trajectories. In addition to a GAN loss and a prediction loss like MLE, we proposeclassified latent intent behavior that classifies the latent code Z behind trajectories, and hallucinativelearning that generates novel and plausible trajectories by mix two latent codes. White and colorpoints denote the ground truth and generated trajectories, respectively.
Figure 2: Balancing classified la-tent intent behavior and hallucina-tive learning by selecting a properÎ» = 0.5 in Algo.2 helps to get abest FDE (averaged over 2000 ran-dom samples)Table 5: Ablation study on nuScenes dataset.
Figure 3: Qualitative results of our method and Trajectron++. Compared to Trajectron++, ourmethod significantly reduces the uncertainty of the prediction in all scenes with improved accuracy.
Figure 4: The trajectories from all behavior intents generated by our method and Trajectron++. Weforce the model to predict trajectory for all behaviors no matter the behaviors are possible judgedby the model or not. White points denote the ground truth trajectories. The other points denote pre-dicted trajectories with different behavior intents. With the help of classified latent intent behavior,We obtain more diverse behaviors compared to Trajectron++. Note that red points comes from theintents which are likely under the judgement of models given the input data. The gray points comesfrom intents which are very unlikely to happen and we forcibly set it for demonstration. Note thatTrajectory++ predicts unsafe trajectory with a high likelihood. While Our method have a capabilityto predict diverse trajectories but unsafe modes have a vert low likelihood17Published as a conference paper at ICLR 2021E Pedestrian DatasetsHere, we train our model on the pedestrians dataset ETH (Pellegrini et al., 2009) and UCY (Leal-Taixe et al., 2014) without map information. A leave-one-out technique is used for evaluation,similar to previous work (Alahi et al., 2016; Gupta et al., 2018; Ivanovic & Pavone, 2019; Kosarajuet al., 2019; Sadeghian et al., 2019; Salzmann et al., 2020), where the model is trained in four datasetsand tested in the fifth dataset. The assessment uses an observation period of 8 timesteps (3.2s) and aprojected horizon of 12 timesteps (4.8s). Note that different from experiments in nuScenes dataset,our model is trained from scratch here.
