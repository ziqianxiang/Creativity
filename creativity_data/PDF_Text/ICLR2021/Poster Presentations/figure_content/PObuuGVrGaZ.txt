Figure 1: Illustrations of the effects of label smoothing on penultimate layer output. The figure isplotted on ImageNet With ReSNet-50 following (MUner et al., 2019), We also choose two semanticallysimilar classes (toy poodle and miniature poodle, in green and yellow) and one semantically differentclass (tench, in purple).① is the discovery observed by Miiller et al. that label smoothing will enforceeach example to be equidistant to its template, i.e., erasing the relative information between logits.
Figure 2: Knowledge distillation (KD) and label smoothing (LS) overview. Both the KD and LS adoptsoftened distributions for learning the target networks. The KD differs from LS in the generation ofthese distributions and the objectives for optimization. KD chooses to utilize a pre-trained teacher toproduce the supervision dynamically, while LS uses a constant uniform distribution for training. Inthe figure, the black lines are the forward pass and the red lines are the gradient propagation direction.
Figure 3: Correction effects of label smoothingon logistic loss with different a. Black dottedline presents the standard logistic loss and othercolored lines are imposed label smoothing.
Figure 4: Probability distributions with/without label smoothing on ReSNet-50. We show the first100 categories in ImageNet. The red/green bars are distributions with/without label smoothing,respectively. “Minor probability” denotes the small probabilities predicted by networks when theoutputs are used as supervisions in knowledge distillation.
Figure 5: The training and testing curves of knowledge distillation on CUB200-2011 when teachersare trained w/ and w/o label smoothing. The specific teacher and student architectures are given beloweach subfigure, therein, T indicates the teacher architecture and S indicates the student.
Figure 6: Left is the averaged train/test loss curves in distillation, right is the testing error w/best Top-1/5 accuracy. We use linear learning rate decay following other binary network trainingprotocol (Martinez et al., 2020; Liu et al., 2018). Our teacher networks are ResNet-50 with and withoutlabel smoothing which have similar performance. The student is the state-of-the-art ReActNet (Liuet al., 2020) with ResNet-18 backbone. We can observe that when the teacher is trained with labelsmoothing, the distillation loss is much higher, but the accuracy of student is still better.
Figure 7: Illustrations of BLEU score curves forteacher pre-training and student distillation. Theleft figure is teachers, pre-training with and with-out label smoothing. The right one is the distilla-tion process of students.
