Figure 1: (a, b) Block-wise comparison between the standard transformer block of Vaswani et al. (2017) and theDeLighT block. In the DeLighT transformation, the number of operations in computing attention are reducedby half while the number of parameters (and operations) in the FFN are reduced by 16 x. Transformations withlearnable parameters ( Linear and DeLighT ) are shown in color. The shape of linear transformations indicatetheir operation (expansion, reduction, etc.). (c, d) compares the DeFINE transformation (Mehta et al., 2020)with the DeLighT transformation. Compared to the DeFINE transformation, the DeLighT transformation usesgroup linear transformations (GLTs) with more groups to learn wider representations with fewer parameters.
Figure 2: Example illustrating the expansion phase in the DeLighT transformation that uses GLTs, featureshuffling, and an input mixer connection, to learn deeper and wider representations efficiently. For illustrativepurposes, we have used the same input and output dimensions.
Figure 3: Block-wise scaling efficiently allocates parameters and operations across blocks, leading to shallowerand narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output. In (b),DeLighT networks with both uniform (N=Nmin=Nmax=8) and block-wise (Nmin=4, Nmax=8) scaling haveabout 16.7 M parameters and perform 3.5 B operations (computed for a sequence length of n = 30), however,the DeLighT network with block-wise scaling delivered 2 points better perplexity.
Figure 4: Comparison of DeLighT with Transformersand Evolved Transformers at two different settings, onthe WMT’14 En-De corpus: (1) the number of parame-ters is the same and (2) the performance is the same.
Figure 5: Scaling up DeLighT models. The performance of DeLighT improves with an increase in thenumber of network parameters, across different corpora, including low-resource (WMT’16 En-Ro).
Figure 6: Sequence modeling with DeLighT. Here, green color hexagon represents the DeLighT transfor-mation.
Figure 7: This figure visualizes different variants of group linear transformations that are used in the DeLighTtransformation.
Figure 8: Impact of different transformations.
Figure 9: Impact of feature shuffling. Featureshuffling allows us to learn representations fromglobal information and improves performance.
Figure 10: Impact of reduction factor r in light-weight FFN. The ability of DeLighT transformation to learnrepresentations in high-dimensional spaces efficiently allows us to reduce the computational burden on the FFN.
Figure 11: Uniform vs. block-wise scaling. (a) contrasts the uniform and block-wise scaling methods. (b)compares the results of DeLighT with uniform and block-wise scaling methods on the WikiText-103 dataset.
Figure 12: Scaling up DeLighT. Scaling one configuration parameter (e.g., dm) while keeping other configu-ration parameters constant (e.g., Nmin, Nmax, and Wm) consistently improves performance. The numbers ontop of each bar represents network parameters (in million). Lower value of perplexity means better performance.
