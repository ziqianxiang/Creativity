Figure 1: Our VA-RED2 framework dynamically reduces the redundancy in two dimensions. Example1 (left) shows a case where the input video has little movement. The features in the temporal dimensionare highly redundant, so our framework fully computes a subset of features, and reconstructs therest with cheap linear operations. In the second example, We show that our framework can reducecomputational complexity by performing a similar operation over channels: only part of the featuresalong the channel dimension are computed, and cheap operations are used to generate the rest.
Figure 2: An illustration of dynamic convolution along temporal dimension (a) and channel dimension(b) respectively. Φt and Φs represent the temporal cheap operation and spatial cheap operationrespectively. In (a), we multiply the temporal stride S with the factor R = 2pt to reduce computation,where pt is the temporal policy output by soft modulation gate. In (b), we compute part of outputfeatures with the ratio of r = ( 1 )pc, where Pc is the channel policy. Best viewed in color.
Figure 3: Ratio of computed feature per layer and class on Mini-Kinetics-200 dataset. We pick the first25 classes of Mini-Kinetics-200 and visualize the per-block policy of X3D-M on each class. Lighter color meansfewer feature maps are computed while darker color represents more feature maps are computed.
Figure 4: Validation video clips from Mini-Kinetics-200. For each category, We plot two input video clipswhich consume the most and the least computational cost respectively. We infer these video clips with 8-framedynamic R(2+1)D-18 model trained on Mini-Kinetics-200 and the percentage indicates the ratio of actualcomputational cost of 2D convolution to that of the original fixed model. Best viewed in color.
Figure 6: Visualization of temporal-wise feature maps. We plot the temporal feature maps whichare fully computed by the original convolution and those mixed with cheaply generated feature maps.
Figure 7: Visualization of channel-wise feature maps. We plot the feature maps across the channeldimension. We contrast two kinds of feature maps: fully computed by the original convolution andthose mixed with cheaply generated feature maps. The feature maps inside the red bounding boxesare cheaply generated. The analysis is performed on 8-frame dynamic R(2+1)D-18 model which ispretrained on Mini-Kinetics-200 dataset and we extract these feature maps which are output by thefirst spatial convolution layer inside the ResBlock_1. Best viewed in color.
Figure 8: Ratio of computed feature per layer and class on Kinetics-400 dataset. We visualize the per-blockpolicy of X3D-M and R(2+1)D-18 on all 400 classes. Lighter color means fewer feature maps are computedwhile darker color represents more feature maps are computed. While X3D-M tends to consume more temporal-wise features at the early stage and compute more channel-wise features at the late stage, R(2+1)D choose toselect fewer features at early stage by both temporal-wise and channel-wise policy. For both architectures, thechannel-wise policy has more variation than the temporal-wise policy among different categories.
Figure 9: Ratio of computed feature per layer and class on Moments-In-Time dataset. We visualize theper-block policy of X3D-M and R(2+1)D-18 on all 339 classes. Lighter color means fewer feature maps arecomputed while darker color represents more feature maps are computed.
Figure 10: Computational cost distribution across different models on different datasets. We count thecomputation of each instance cost by different models on different datasets. For instance, for the upper-left one,We USe the model backbone of R(2+1)D-18 on Mini-Kinetics-200. This sub-figure indicates that there are 87.7%of videos in Mini-Kinetics-200 (Dataset) consuming 38.6 - 41.4 GFLOPS by using R(2+1)D-18 (Backbone),8.8% of videos consuming 35.9 — 38.6 GFLOPs, and 3.5% of videos consuming 41.4 — 44.2 GFLOPs.
Figure 11: Validation video clips from Kinetics-400. For each category, we plot two input video clips whichconsume the most and the least computational cost respectively. We infer these video clips with 16-framedynamic R(2+1)D-18 which is pre-trained on Kinetics-400. The percentage in the figure indicates the ratio ofthe actual computational cost of 2D convolution to that of the original fixed model. Best viewed in color.
Figure 12: Validation video clips from Moments-In-Time. For each category, we plot two input video clipswhich consume the most and the least computational cost respectively. We infer these video clips with 16-framedynamic R(2+1)D-18 which is pre-trained on Moments-In-Time. The percentage in the figure indicates the ratioof the actual computational cost of 2D convolution to that of the original fixed model. Best viewed in color.
