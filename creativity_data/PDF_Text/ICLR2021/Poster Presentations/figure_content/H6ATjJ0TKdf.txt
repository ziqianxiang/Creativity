Figure 1: The LAMP score is a squared weight magnitude, normalized by the sum of all “survivingweights” in the layer. Global pruning by LAMP is equivalent to the layerwise magnitude-basedpruning with an automatically chosen layerwise sparsity.
Figure 2: Sparsity-accuracy tradeoff curves of VGG-16, ResNet-18, DenseNet-121, and EfficientNet-B0. All models are iteratively pruned and retrained with CIFAR-10 dataset.
Figure 3: Sparsity-accuracy tradeoff curves of pruned models trained for SVHN and CIFAR-100 (onVGG-16) and Restricted ImageNet (on ResNet-34).
Figure 4: Sparsity-accuracy tradeoff curves under one-shot pruning, weight rewinding, and the SNIPsetup. One-shot pruning and the weight-rewinding experiments are done on VGG-16 trained onCIFAR-10 dataset. SNIP experiment is performed on Conv-6 trained on CIFAR-10.
Figure 5: Layerwise statistics of VGG-16 iteratively pruned on CIFAR-10. Top: Layerwise survivalrate for models with {51.2%, 26.2%, 13.4%, 6.87%, 3.52%} weights surviving. Bottom: Numberof nonzero weights for models with {3.52%, 1.80%, 0.92%, 0.47%, 0.24%} weights surviving.
Figure 6: Layerwise survival rates of one-shot pruned ResNet-50 trained on ImageNet dataset. Theglobal survival rates are {80.0%, 41.0%, 21.0%} (from lighter to darker).
Figure 7: Performance of LAMP and baselines on VGG-16 trained on CIFAR-10 dataset, using thetraining schedules from Liu et al. (2019).
