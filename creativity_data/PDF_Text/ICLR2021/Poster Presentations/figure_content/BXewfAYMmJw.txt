Figure 1: Out-of-Domain (OOD) Classification. A classifier focuses on all factors of variation(FoV) in an image. For OOD data, this can be problematic: a FoV might be a spurious correla-tion, hence, impairing the classifier,s performance. An ensemble, e.g., a classifier with a commonbackbone and multiple heads, each head invariant to all but one FoV, increases OOD robustness.
Figure 2: Counterfactual Generative Network (CGN). Here, We illustrate the architecture used forthe ImageNet experiments. The CGN is split into four mechanisms, the shape mechanism f shape, thetexture mechanism ftext, the background mechanism fbg, and the composer C. Components withtrainable parameters are blue, components with fixed parameters are green. The primary supervisionis provided by an unconstrained conditional GAN (cGAN) via the reconstruction loss Lrec. ThecGAN is only used for training, as indicated by the dotted lines. Each mechanism takes as input thenoise vector U (sampled from a spherical Gaussian) and the label y (drawn uniformly from the setof possible labels Y) and minimizes its respective loss (Lshape, Ltext, and Lbg). To generate a setof counterfactual images, we sample u and then independently sample y for each mechanism.
Figure 3: MNISTs. Left: Samples of the different MNIST variations (for brevity, We show only thefirst four classes). Right: Counterfactual samples generated by our CGN. Note that the CGN learnedclass-conditional distributions, i.e., it generates varying shapes, colors, and textures.
Figure 4: ImageNet Counterfactuals. The CGN successfully learns the disentangled shape, texture,and background mechanisms, and enables the generation of numerous permutations thereof.
Figure 5: Individual IM Outputs over Training. We show pre-masks fn, masks m, foregrounds f,and backgrounds b. The arrows indicate the beginning and end of the training. The initial output ofthe pre-trained models is gradually transformed while the composite image only marginally changes.
Figure 6: Colored MNIST. (left) Examples of data points. (right) Training a disentangled VAE withtwo latent dimensions on colored MNIST.
Figure 7: MNIST Ablation Study. To improve visibility, we start with 104 counterfactual datapoints, below the performance is marginally better than the fully biased baseline. The CF ratio indi-cates how many counterfactuals we generate per sampled noise. For colored MNIST, the maximumCF ratio is ten as there are only ten possible colors per shape.
Figure 8: IM Outputs for ’jay’. From top toFigure 9: IM Outputs for ’wallaby’. From topbottom: m, m, f, b, Xgen.
Figure 9: IM Outputs for ’wallaby’. From topbottom: m, m, f, b, Xgen.
Figure 10: IM Outputs for ’king penguin’.
Figure 11: IM Outputs for ’vizsla’. From topto bottom: m, m, f, b, xgen.
Figure 12: IM Outputs for ’barn’. From top tobottom: m, m, f, b, Xgen.
Figure 13: IM Outputs for ’speedboat’. Fromtop to bottom: m, m, f, b, Xgen.
Figure 14: IM Outputs for ’viaduct’. From topto bottom: m, m, f, b, xgen.
Figure 15: IM Outputs for ’cauliflower’.
Figure 16: IM Outputs for ’bell pepper’.
Figure 17: IM Outputs for ’strawberry’.
Figure 18: IM Outputs for ’geyser’. From topto bottom: m, m, f, b, Xgen.
Figure 19: IM Outputs for ’agaric’. From topto bottom: m, m, f, b, Xgen.
Figure 20: Interpolating Shapes. We interpolate between u and y pairs.
Figure 23: MNIST Counterfactuals. From Left to Right: colored, double-colored-, and WildlifeMNIST.
Figure 24: ImageNet Counterfactuals. Top: Counterfactual Images. Bottom: ImageNet labels.
Figure 25: IM Outputs over Training for 'dalmatian' The arrows indicate the beginning and endof the training.
Figure 26: IM Outputs over Training for 'cauliflower' The arrows indicate the beginning and endof the training.
Figure 27: IM Outputs over Training for 'castle' The arrows indicate the beginning and end ofthe training.
Figure 28: IM Outputs over Training for ,ringtailed lemur' The arrows indicate the beginningand end of the training.
Figure 29: IM Outputs over Training for ,mushroom, The arrows indicate the beginning and endof the training.
Figure 30: Texture-Background Entanglement. For relatively small objects, the texture maps canstill show traces of the background. A possible remedy would be to choose the patch size for Ltextdependent on the relative object size.
Figure 31: Background Residues. Especially for large objects, i.e., where large regions need to bein-painted, there can be faint artifacts visible. For the composite images, this is not a problem as anobject will cover the residue.
Figure 32: Reduced Realism. As evidence by the lower IS, the generated images Xgen are generallylower in realism. This reduced realism is due to the constraints that We enforce and the simplifiedcomposition mechanism. A solution might be to add a shallow refinement network after the com-poser.
