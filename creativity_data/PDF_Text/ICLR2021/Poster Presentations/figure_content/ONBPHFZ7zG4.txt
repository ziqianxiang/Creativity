Figure 1: Average (estimated) first-visit times, comparing -greedy policies (a) without and with (b)temporal persistence, in an open gridworld (blue represents fewer steps to and red states rarely ornever seen). Greedy policy moves directly down from the top center. See Appendix for details.
Figure 2: (a) Modified chain MDP, action a° moves right, aι terminates with specified reward.
Figure 3:	Comparing -greedy with z-greedy on four small-scale domains requiring exploration.
Figure 4:	Results on the Atari-57 benchmark for (a) Rainbow-based and (b) R2D2-based agents.
Figure 5: Results on the Atari-57 selected games showing R2D2-based agents.
Figure 6: Environments used in this work: (a) DeepSea, (b) GridWorld, (c) MountainCar, (d)CartPole, (e) Atari-57.
Figure 7: Stochastic Gridworld experiments. (a) We show averaged training performance (over100 episodes) with respect to the noise scale. (b) Example learning curves from these experimentsshowing the effect of stochasticity on both agents.
Figure 8: Stochastic MountainCar experiments. (a) We show averaged training performance (over1000 episodes) with respect to the noise scale. (b) Example learning curves from these experimentsshowing the effect of stochasticity on both agents.
Figure 9: Sticky-action Atari-57 summary curves Rainbow-based agents.
Figure 10: Sticky-action Atari-57 results for selected games showing Rainbow-based agents. Curvesare averages over 3 seeds.
Figure 11: Example Gridworlds with varying density of (top) obstacles and (bottom) traps. Theshades of grey represent the type of each cell: white cells are open states, light grey cells are thestart state, grey cells are traps, dark grey cells are goal states, and black cells are obstacles. Theseenvironments are randomly generated to a target density of obstacle / trap, while ensuring there existsa path between the start and goal states.
Figure 12: Gridworld with obstacles at varying density. (a) We show averaged training performance(over 1000 episodes) with respect to the obstacle density. (b) Example learning curves from theseexperiments showing the effect on both agents.
Figure 13: Gridworld with traps at varying density. (a) We show averaged training performance (over1000 episodes) with respect to the trap density. (b) Example learning curves from these experimentsshowing the effect on both agents.
Figure 14: Adversarial modification to DeePSea environment causes ez-greedy to perform no betterthan e-greedy.
Figure 15: Experiment in the Gridworld domain comparing Rmax, with visitation thresholds 1 and10 with e-Greedy and ez-Greedy.
Figure 16: Percent relative improvement of exploration methods (z-Greedy and CTS) over -Greedyfor the Rainbow-based agents on Atari-57 per-game. We report this for both final performance (top)and average over training (bottom).
Figure 17: Percent relative improvement of exploration methods (z-Greedy and RND) over -Greedyfor the R2D2-based agents on Atari-57 per-game. We report this for both final performance (top) andaverage over training (bottom).
Figure 18: Atari-57 summary curves for R2D2-based methods (top) and Rainbow-based methods(bottom).
Figure 19: Per-game Atari-57 results for Rainbow-based methods.
Figure 20: Per-game Atari-57 results for R2D2-based methods.
Figure 21: Per-game Sticky-action Atari-57 results for Rainbow-based methods.
