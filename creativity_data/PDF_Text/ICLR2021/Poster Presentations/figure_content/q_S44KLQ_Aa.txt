Figure 1: Correlation between ∣∣x*∣∣ι and r = ∣∣Φx* kι or U = ∣∣WTΦx* kι. In(a) and (b) for sparse vectorswith ∣∣x* ∣o = 15. In (c) and (d) for non-sparse vectors ∣∣x* ∣o = N. The non-zero components of x* aredrawn i.i.d. from N(0,1) with N = 1000. One can see that for sparse x*, r and U are correlated with ∣x* ∣ι(Spearman coefficients 0.91,0.92), whereas there is a much weaker correlation for non-sparse vectors (0.38,0.37).
Figure 2: Correlation between u(i) and ∣∣x(j) — x* k ι ina trained instance of NA-ALISTA from different iterations(i, j). (a): (0, 1), (b): (5, 6), (c): (14, 15), (d): (5, 8). The Spearman coefficients are (0.85,0.96,0.97,0.93)showing that the strong correlation, is even preserved across multiple iterations (d), suggesting the use of arecurrent neural network to predict θ(k,x ). Training was performed with N = 1000, H = 128 and K = 16.
Figure 3: The reconstruction error for ALISTA, AGLISTA, ALISTA-AT and NA-ALISTA over the number ofiterations K for SNR=40dB (3a) and SNR=20dB (3b). NA-ALISTA outperforms all competitors. Results forsettings with smaller N can be found in Appendix A.
Figure 4: Reconstruction error over different compression ratios. For a constant expected sparsity of S = 50and M = 250 measurements and K = 26 iterations, the input size N varies. Both under a SNR of 40dB and20dB NA-ALISTA increases its reconstruction margin to competitors as N increases and the compression ratiobecomes more challenging.
Figure 5: Reconstruction error for varying settingsof LSTM size in NA-ALISTA. Larger N profit morefrom larger H, but in all settings an exponential in-crease of the LSTM size only yields a marginal im-provement once H = 64 is surpassed.
Figure 6: Predicted step sizes and thresholds from atrained instance of NA-ALISTA (N = 1000, M =250, S = 50, K = 16), highlighting the adpativityof NA-ALISTA. Inference to obtain these values isperformed on the test set.
Figure 7: Training curves for N = 1000, K = 16,SNR= 40 from the learned algorithms we compare inthis paper, showing that NA-ALISTA outperforms thecompetitors after only a few epochs of training. Eachepoch consists of 50,000 random sparse vectors.
Figure 8: Comparison of the ratio θ(k) /γ(k) with thetrue 'ι-error ||x* — x(k) ||i at each iteration for NA-ALISTA for the mean of a batch of randomly drawntest data {x*} and its standard deviation. Togetherthese terms behave as desired, see Eq. (7).
Figure 9: Pilot-based multipath channel estimation with compressed sensing. Here, Φ ∈ CM ×N is a subsampledDFT with NFFT = 1024. A random realization of the CIR x* and the pilot values y are shown.
Figure 10: Channel estimation error vs. iterations Kfor an 10MHz LTE system at SNR=10dB 3. Amount ofpilots M = 100, as used in the standard but sampledrandomly.
Figure 11: The reconstruction error for ALISTA, ALISTA-AT and NA-ALISTA over the number of iterationsrun for different noise and N settings. In 11a, for the standard setting in the literature with N = 500 and anoise level of 40dB NA-ALISTA performs on par with competitors after 16 iterations. For an increased N =1000under the same noise level in 11c, our algorithm outperforms the other methods clearly. For a noise level of20dB all algorithms perform similarly for N = 500 and N = 1000 and NA-ALISTA outperforms the others atN = 2000.
Figure 12: Wall clock time for a single iteration of NA-ALISTA, AGLISTA and ALISTA with M =250 and 500and N ranging from 750 to 4000 for a single batch of size 5000, averaged over 100 batches. Computations wererun on a system with a NVIDIA Tesla P100 GPU and Intel(R) Xeon(R), with the GPU enabled (a) and CPUonly (b).
Figure 13: Channel estimation error vs. iterations K for an 10MHz LTE system at SNR=10dB. (a) amountof sampled pilots M = 100, while (b) reduces this to M = 75, the reconstruction error is still close to usingM = 75.
