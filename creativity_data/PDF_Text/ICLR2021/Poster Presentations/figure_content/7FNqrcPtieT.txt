Figure 1: Left: Jacobian (i.e. first order) Penalization method are short-sighted and do not exploitfully the data-manifold Right: Data-Augmentation respecting the geometry of the data-manifold.
Figure 2: Labelled data samples With class y = 0 (green triangle) and y = +1 (red dot) are placed onthe Left/Right boundary of the unit square. Unlabelled data samples (blue stars) are uniformly placedwithin the unit square. We consider a simple regression setting with loss function '(f,y) = 2 (f—y)2.
Figure 3: Left: For a fixed data-augmentation scheme, generalization properties for λ spanning twoorders of magnitude. Right: Influence of the quantity of the data-augmentation of the generalizationproperties.
Figure 4: Learning curve test (NLL) of the Π-model with λ = 10 for different “quality" of data-augmentation. The data manifold is of dimension d = 10 in an ambient space of dimensionD = 100. For xi = Φ(zi) and 1 ≤ k ≤ d, the data-augmentation scheme is implemented asSεω[k] (xi) = Φ(zi + ε ω[k]) where ω[k] is a sample from a Gaussian distribution whose last (d - k)coordinates are zero. In other words, the data-augmentation scheme only explores k dimensions outof the d dimensions of the data-manifold. We use ε = 0.3 in all the experiments. Left: Learningcurves (Test NLL) for data-augmentation dimension k ∈ [5, 10] Right: Test NLL at epoch N = 200(see left plot) for data-augmentation dimension k ∈ [5, 10].
Figure 5: Mean-Teacher (MT) learning curves (Test NLL) for different values of the exponentialsmoothing parameter βMT ∈ (0, 1). For βMT ∈ {0.9, 0.95, 0.99, 0.995}, the final test NLL obtainedthrough the MT approach is identical to the test NLL obtained through the Π-model. In all theexperiments, we used λ = 10 and used SGD with momentum β = 0.9.
