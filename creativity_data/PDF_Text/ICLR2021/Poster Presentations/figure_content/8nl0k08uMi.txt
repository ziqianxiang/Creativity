Figure 1: Effects of reducing class selectivity on test accuracy in ResNet18 trained on Tiny Imagenet. (a)Test accuracy (y-axis) as a function of regularization scale (α, x-axis and intensity of blue). (b) Identical to (a),but for a subset of α values. The center of each violin plot contains a boxplot, in which the darker central linesdenote the central two quartiles. (c) Test accuracy (y-axis) as a function of mean class selectivity (x-axis) fordifferent values of α. Error bars denote 95% confidence intervals. *p < 0.01, **p < 5 × 10-10 difference fromα = 0, t-test, Bonferroni-corrected. See Appendix A.4 and A.12 for ResNet20 and VGG results, respectively.
Figure 2: Checking for off-axis selectivity. (a) Mean CCA distance (ρ, y-axis) as a function of layer (x-axis)between pairs of replicate ResNet18 networks (see Section 4.2 or Appendix A.3.2) trained with α = -2 (i.e.
Figure 3: Effects of increasing class selectivity on test accuracy in ResNet18 trained on Tiny ImageNet.
Figure 4: Increasing class selectivity has deleterious effects on test accuracy compared to reducing classselectivity. (a) Test accuracy (y-axis) as a function of regularization scale magnitude (∣α∣)for negative (blue) Vspositive (red) values of α. Solid line in distributions denotes mean, dashed line denotes central two quartiles.
Figure A1: Manipulating class selectivity by regularizing against it in the loss function. (a) Mean classselectivity index (y-axis) as a function of layer (x-axis) for different regularization scales (α; denoted by intensityof blue) for ResNet18. (b) Similar to (a), but mean is computed across all units in a network instead of perlayer. (b) Similar to (a), but mean is computed across all units in a network instead of per layer. (c) and (d) areidentical to (a) and (b), respectively, but for ResNet20. Error bars denote bootstrapped 95% confidence intervals.
Figure A2: Effects of reducing class selectivity on test accuracy in ResNet20 trained on CIFAR10. (a)Test accuracy (y-axis) as a function of regularization scale (α, x-axis and intensity of blue). (b) Identical to (a),but for a subset of α values. The center of each violin plot contains a boxplot, in which the darker central linesdenote the central two quartiles. (c) Test accuracy (y-axis) as a function of mean class selectivity (x-axis) fordifferent values of α. Error bars denote 95% confidence intervals. *p < 0.05, **p < 5 × 10-6 difference fromα = 0, t-test, Bonferroni-corrected.
Figure A3: Using CCA to check whether class selectivity is rotated off-axis in ResNet20 trained onCIFAR10. Similar to Figure 2, we plot the average CCA distance ratio (y-axis) as a function of α (x-axis,intensity of blue). The distance ratio is significantly greater than the baseline for all values of α (p < 5 × 10-6,paired t-test). Error bars = 95% confidence intervals.
Figure A4: An upper bound for off-axis class selectivity. (a) Upper bound on class selectivity (y-axis) asa function of layer (x-axis) for different regularization scales (α; denoted by intensity of blue) for ResNet18trained on Tiny ImageNet. (b) Mean class selectivity (y-axis) as a function of regularization scale (α; x-axis)for ResNet20 trained on CIFAR10. Diamond-shaped data points denote the upper bound on class selectivityfor a linear projection of activations as described in Appendix A.7, while circular points denote the amount ofaxis-aligned class selectivity for the corresponding values of α. (c) (a), but for ResNet20 trained on CIFAR10.
Figure A5: Regularizing to increase class selectivity (a) Mean class selectivity index (y-axis) as a function oflayer (x-axis) for different regularization scales (α; denoted by intensity of red) for ResNet18. (b) Similar to (a),but mean is computed across all units in a network instead of per layer. (c) and (d) are identical to (a) and (b),respectively, but for ResNet20. Note that the inconsistent effect of larger α values in (c) and (d) is addressed inAppendix A.10. Error bars denote bootstrapped 95% confidence intervals.
Figure A6: Effects of increasing class selectivity on test accuracy on ResNet20 trained on CIFAR10. (a)Test accuracy (y-axis) as a function of regularization scale (α; x-axis, intensity of red). (b) Identical to (a), butfor a subset of α values. The center of each violin plot contains a boxplot, in which the darker central linesdenote the central two quartiles. (c) Test accuracy (y-axis) as a function of mean class selectivity (x-axis) fordifferent values of α. Error bars denote 95% confidence intervals. *p < 2 × 10-4, **p < 5 × 10-7 differencefrom α = 0, t-test, Bonferroni-corrected.
Figure A7: Regularizing to increase class selectivity does not cause units to die in ResNet18 trained onTiny ImageNet. (a) Proportion of dead units (y-axis) for different for different regularization scales (α; x-axis,intensity of red). Error bars denote 95% confidence intervals.
Figure A8: Removing dead units partially stabilizes the effects of large positive regularization scales inResNet20. (a) Proportion of dead units (y-axis) as a function of layer (x-axis) for different regularization scales(α, intensity of red). (b) Mean class selectivity index (y-axis) as a function of regularization scale (α; x-axis andintensity of red) after removing dead units. Removing dead units from the class selectivity calculation establishesa more consistent relationship between α and the mean class selectivity index (compare to Figure A5d). (c)Test accuracy (y-axis) as a function of mean class selectivity (x-axis) for different values of α after removingdead units from the class selectivity calculation. Error bars denote 95% confidence intervals. *p < 2 × 10-4,**p < 5 × 10-7 difference from α = 0 difference from α = 0, t-test, Bonferroni-corrected. All results shownare for ResNet20.
Figure A9: Reviving dead units does not rescue the performance deficits caused by increasing selectivityin ResNet20. (a) Test accuracy (y-axis) as a function of regularization scale (α; x-axis) for different leaky-ReLUnegative slopes (intensity of red). Leaky-ReLUs completely solve the dead unit problem but do not fully rescuetest accuracy for networks with α > 0. (b) Mean class selectivity index (y-axis) as a function of regularizationscale (α; x-axis and intensity of red) for leaky-ReLU negative slope = 0.5. *p < 0.001, **p < 2 × 10-4,***p < 5 × 10-10 difference from α = 0, t-test, Bonferroni-corrected. Error bars denote bootstrapped 95%confidence intervals.
