Figure 1: Training error on local datasets fortwo clients respectively with and w/o BN, whereBN harmonizes the loss surface.
Figure 2: Error surface of a client for modelparameter w ∈ [0.001, 12] and BN parameterγ ∈ [0.001, 4]. Averaging model and BN pa-rameters leads to worse solutions.
Figure 3: Convergence of the training loss of FedBN and FedAVg on the digits classification datasets.
Figure 4: Analytical experimental results on: (a) Analysis on different local updating epochs.
Figure 5: Performance on benchmark experimentsand standard deviation of theaccuracy on each dataset over trials are shown in Fig. 56. From the results, we can make thefollowing observation: (1) FedBN achieves the highest accuracy, consistently outperforming thestate-of-the-art and baseline methods; (2) FedBN achieves the most significant improvements onSVHN whose image appearance is very different from others (i.e., presenting more obvious featureshift); (3) FedBN shows a smaller variance in error over multiple runs, indicating its stability.
Figure 6: Data visualization. (a) Examples from each dataset (client). (b) Non-iid feature distribu-tions across the datasets (over random 100 samples for each dataset).
Figure 7: Training loss over epochs with different local update frequency.
Figure 8: Test set accuracy curve (average of 5 datasets) of using different local updating epochs Eand batch size B for FedBN.
Figure 9: Training loss on synthetic data. Data in client 1 is generated from Diagonal Gaussian,client 2 is generated from combination of Diagonal Gaussian and Full Gaussian.
