Figure 1: Distribution of per-class test errors of a ResNet-50 on ImageNet (left). While the average error rateis 〜24%, some classes achieve an error as high as 〜80%. An adversary can thus significantly degrade testperformance (right) by choosing pte (y) with more weight on these classes.
Figure 2: Comparison of performance on ImageNet under adversarial label distributions. For eachmethod, we vary the KL divergence threshold τ , and for each τ report the maximal validation errorinduced by the adversarial shift within the threshold. Subplots (a) (b) compare the performanceof ADVSHIFT trained with different DRO radius r against the default ERM training. We subtractthe baseline error of ERM from all values for easy visualization. Absolute values can be found inFigure 8 in the Appendix. Combined with (c), (d), we see that AdvShift can reduce the adversarialvalidation error by over 〜2.5% compared to the baseline method and is consistently superior tothe agnostic, balanced and fixed methods. Figure 3(c) illustrates adversarial distributions forvarying thresholds τ .
Figure 3: Subplots (a) (b) show violin plots of the distribution of errors for both the baseline andour AdvShift methods over the course of training. On the training set, AdvShift significantlyreduces the worst-case error, evidenced by lower upper endpoints of the distribution. On the validationset, the reduction is consistent, albeit less pronounced owing to a generalisation gap. Subplot (c)illustrates adversarial distributions at KL distances of 1, 2 and 3 for model trained with baseline.
Figure 4: Evolution of learned adversarial distribution (π) across training epochs. Starting off from auniform distribution over labels, the adversary quickly infers the relative difficulty of a small fractionof labels, assigning nearly 2× the weight on them compared to the average. This distribution remainslargely stable in subsequent iterations, getting gradually more concentrated as training converges.
Figure 5:	Ablation of loss clipping threshold. We see that when the clipping threshold is either toolarge or too small, validation performance of the model tends to suffer.
Figure 6:	Ablation of gradient stabilisation parameter , which is a constant added to the gradientupdates to prevent iterates from reaching the vertices of the simplex. We see that without anygradient stabilisation, the model’s performance rapidly degrades as the adversarial radius increases.
Figure 7: Illustration that training label frequency does not strongly correlate with test error. Observethat several classes with a high error appear frequently in the training set.
Figure 8:	Comparison of performance of various methods on ImageNet under adversarial labeldistributions. For each plot, we vary a KL divergence threshold τ , and for a given τ construct thelabel distribution which results in maximal test error for the baseline model. We then compute thetest error under this distribution. Note that the case τ = 0 corresponds to using the train distribution,while τ = +∞ corresponds to using the worst-case label distribution, which is concentrated onthe worst-performing label. Our proposed AdvShift can reduce the adversarial test error by over〜2.5% over the baseline method.
Figure 9:	Comparison of performance of various methods on CIFAR-100.
Figure 10:	Comparison of performance of various methods on CIFAR-100 (unnormalised).
