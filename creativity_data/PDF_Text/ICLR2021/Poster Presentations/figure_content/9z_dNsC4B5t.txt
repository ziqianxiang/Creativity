Figure 1: Illustration of the novel few-shot domain generalization scenario using the 5-way,1-shot setting. The training set in the upper box contains the meta-source domains Ds and themeta-target domain Dt , which are from different domains. Each training task contains meta-sourcedomains with five different classes and one example of each meta-source domain, and more than fourexamples for evaluation in the meta-target domain. The test set is defined in the same way but withall source domains S covering classes not present in any of the datasets in the training set, and morethan four examples are used for evaluation in the target domain T.
Figure 2: Impact of Target Set Size. The performance increases for larger target sets and plateausat around 125 for few-shot classification on miniImageNet and around 256 for domain generalizationon PACS. TBN here is based on VERSA. MetaNorm generates proper normalization statistics with areasonable batch size.
Figure 3: The dataflow of the implementation for few-shot learning. “N” indicates support size,“M" indicates query size, “C" indicates the channel of activations, “W" indicates the width ofactivations, “H” indicates the height of activations.
Figure 4: Training loss. Results of using the ProtoNets algorithm on miniImageNet with respect totraining loss versus iterations. Our MetaNorm achieves fastest training convergence.
