Figure 1: (a) Rapid Task Solving in Novel Environments (RTS) setup. A new environment is sampledin every episode. Each episode consists of a sequence of tasks which are defined by sampling a newgoal state and a new initial state. The agent has a fixed number of steps per episode to complete asmany tasks as possible. (b) Episodic Planning Network (EPN) architecture. The EPN uses multipleiterations ofa single shared self-attention function over memories retrieved from an episodic storage.
Figure 2: Memory&Planning Game. (a) Example 4 Ã— 4 environment (not observable by the agent)and state-goal observation. (b) Training curves. Performance measured by the average reward perepisode, which corresponds to the average number of tasks completed within a 100-step episode(showing the best runs from a large hyper-parameter sweep for each model). (c) Performance mea-sured in the last third of the episodes (post-training), relative to an oracle with perfect informationthat takes the shortest path to the goal. (d) Example trajectory of a trained EPN agent in the firstthree tasks of an episode. In the first task, the agent explores optimally without repeating states. Inthe subsequent tasks, the agent takes the shortest path to the goal. (e) Number of steps taken by anagent to reach the nth goal in an episode.
Figure 3: One-Shot StreetLearn. (a) Four example states from two randomly sampled neighbor-hoods. (b) Example connectivity graphs. (c) Evaluation performance measured on neighborhoods ofa held-out city throughout the course of training (showing the best run from a large hyper-parametersweep for each model). (d) Performance in last third of episode relative to an oracle with perfectinformation. (e) Number of steps taken by an agent to reach the nth goal in an episode.
Figure 4: Iteration analysis and generalization. (a) Performance of an EPN agent on held-out neigh-borhoods with 5 intersections using planners with 1, 2 and 4 self-attention iterations (showing 3 runsfor each condition). (b) Performance on new neighborhoods that were larger (7 and 9 intersections)than the ones used during training (5 intersections). (c) Distance-to-goal decoding accuracy from theoutput of the planner after 1 to 6 iterations. (d) The ability of EPN activations to predict distance togoal expands out from the goal state (blue arrow) as the number of self-attention iterations increases.
Figure 5:	Comparison between architecture variants. The Nxk architecture variant, which scaleslinearly with the total number of memories, recovers 92% of the performance of the A2A variant,which scales quadratically.
Figure 6:	(a) Distance-to-goal decoding loss from the output of the planner (with frozen weights)after 1 to 6 iterations. Each additional iteration improves distance decoding. (b) The ability of EPNsto infer distance to goal expands with the number of self-attention iterations, even when that numbergoes beyond the number of iterations used during training of the EPN.
