Figure 1: (a) Data in image classification with standard augmentation techniques, as well as otherdomains in which neural networks are commonly used, lies on low dimensional class manifolds—in this case those generated by the action of continuous transformations on images in the trainingset. Tangent vectors at a point on the manifold corresponding to an application of a rotation or atranslation are illustrated in green. The dimension of the manifold is determined by the dimension ofthe symmetry group, and is typically small. (b) The multiple manifold problem. Our model problem,capturing this low dimensional structure, is the classification of low-dimensional submanifolds ofa sphere Sn0-1. The difficulty of the problem is set by the inter-manifold separation ∆ and thecurvature κ. The depth and width of the network required to provably reduce the generalizationerror efficiently are set by these parameters.
Figure 2: (a) Depth acts as a fitting resource. As L increases, the rotationally-invariant kernel ΘΘ (aslight modification of the deterministic kernel in Theorem 2) decays more rapidly as a function ofangle between the inputs ∠(x, x0) (n is held constant). Below the curves we show an isometric chartaround a point x ∈ M+. Once the decay scale ofΘ is small compared to the inter-manifold distance∆ and the curvature of M-, the network output can be changed at x while only weakly affectingits value on M-. This is one mechanism that relates the depth required to solve the classificationproblem to the data geometry. (b) Width acts as a statistical resource. The dynamics at initializationare governed by Θ, a random process over the network parameters. As n is increased, the normalizedfluctuations of Θ around ΘΘ decrease (here L = 10). These two phenomena are related, since thefluctuations also grow with depth, as evinced by the scaling in Theorem 2.
Figure 3: The coaxial circlesgeometry.
