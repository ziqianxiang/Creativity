Figure 1: Neuron level dynamics are simplerthan parameter dynamics. We plot the per-parameter dynamics (left) and per-channel squaredEuclidean norm dynamics (right) for the convo-lutional layers of a VGG-16 model (with batchnormalization) trained on Tiny ImageNet withSGD with learning rate η = 0.1, weight decayλ = 10-4, and batch size S = 256. While the pa-rameter dynamics are noisy and chaotic, the neurondynamics are smooth and patterned.
Figure 3: Visualizing conservation. AssociatedWith each symmetry is a conserved quantity con-straining the gradient floW dynamics to a sur-face. For translation symmetry (a) the floW isconstrained to a hyperplane Where the intercept isconserved. For scale symmetry (b) the floW is con-strained to a sphere Where the radius is conserved.
Figure 4: Modeling discretization. We visual-ize the trajectories of gradient descent and mo-mentum (black dots), gradient flow with and with-out momentum (blue lines), and the modified dy-namics (red lines) on the quadratic loss L(w) =w| -21.5.5 -12.5 w. On the left we visualize gradi-ent dynamics using modified loss. On the rightwe visualize momentum dynamics using modifiedflow. In both settings the modified continuous dy-namics visually track the discrete dynamics betterthan the original continuous dynamics. See ap-pendix D for further details.
Figure 5: Exact dynamics of VGG-16 on TinyImageNet. We plot the column sum of the finallinear layer (top row) and the difference betweensquared channel norms of the fifth and fourth con-volutional layer (bottom row) of a VGG-16 modelwithout batch normalization. We plot the squaredchannel norm of the second convolution layer (mid-dle row) of a VGG-16 model with batch normal-ization. Both models are trained on Tiny ImageNetwith SGD with learning rate η = 0.1, weight decayλ, batch size S = 256, for 100 epochs . Coloredlines are empirical and black dashed lines are thetheoretical predictions from equations (18), (19),and (20). See appendix J for more details on theexperiments.
Figure 6: Momentum leads to harmonic oscil-lation. We plot the column sum of the finallinear layer of a VGG-16 model (without batchnormalization) trained on Tiny ImageNet withSGD with learning rate η = 0.1, weight decayλ = 5 X 10-3, batch size S = 256 and momentumβ ∈ {0, 0.9, 0.99}. Colored lines are empiricaland black dashed lines are the theoretical predic-tions from equations (34).
Figure 7: Circular motion. Considerthe vector field f (x) = [1 -11] X and thediscrete dynamics xt+1 = xt + ηf(xt)(black dots), the continuous dynamicsX = f (x) (blue line), and the modifiedcontinuous dynamics X = f (x) + 2x.
Figure 8: The planar dynamics of VGG-16 on Tiny ImageNet. We plot the column sum of thefinal linear layer of a VGG-16 model (without batch normalization) trained on Tiny ImageNet withSGD with learning rate η = 0.1, weight decay λ ∈ {0,10-4, 5 X 10-4,10-3}, and batch sizeS = 256. Colored lines are empirical column sums of the last layer through training and black dashedlines are the theoretical predictions of equation (18).
Figure 9: The spherical dynamics of VGG-16 BN on Tiny ImageNet. We plot the squaredEuclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained onTiny ImageNet with SGD with learning rate η = 0.1, weight decay λ ∈ {0,10-4,5 × 10-4, 10-3},and batch size S = 256. Colored lines represent empirical layer-wise squared norms through trainingand the white dashed lines the theoretical predictions given by equation (19).
Figure 10: The hyperbolic dynamics of VGG-16 on Tiny ImageNet. We plot the differencebetween the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model(without batch normalization) trained on Tiny ImageNet with SGD with learning rate η = 0.1, weightdecay λ ∈ {0,10-4,5 X 10-4,10-3}, and batch size S = 256. Colored lines represent empiricaldifferences in consecutive layer-wise squared norms through training and the white dashed lines thetheoretical predictions given by equation (20).
Figure 11: The planar dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot thecolumn sum of the final linear layer of a VGG-16 model (without batch normalization) trained on TinyImageNet with Momentum with learning rate η = 0.1, weight decay λ ∈ {0, 10-4, 5 × 10-4, 10-3},momentum coefficient β ∈ {0, 0.9, 0.99}, and batch size S = 128. Colored lines are empiricalcolumn sums of the last layer through training and black dashed lines are the theoretical predictionsof equation (34).
Figure 12: The spherical dynamics of Momentum on VGG-16 on Tiny ImageNet. We plotthe squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normal-ization) trained on Tiny ImageNet with Momentum with learning rate η = 0.1, weight decayλ ∈ {0, 10-4, 5 × 10-4, 10-3}, momentum coefficient β ∈ {0, 0.9, 0.99}, and batch size S = 128.
Figure 13: The hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot thedifference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16model (without batch normalization) trained on Tiny ImageNet with Momentum with learning rateη = 0.1, weight decay λ ∈ {0, 10-4, 5×10-4, 10-3}, momentum coefficient β ∈ {0, 0.9, 0.99}, andbatch size S = 128. Colored lines represent empirical differences in consecutive layer-wise squarednorms through training and the black dashed lines the theoretical predictions given by equation (36)and (37) replacing ∣θ∣2 and 摩∣2 by I。/」2 -∣θ∕212 and |dθA112 -|dθ-A2-12 respectively.
Figure 14: The per-neuron spherical dynamics of SGD on VGG-16 BN on Tiny ImageNet. Weplot the per-neuron squared Euclidean norms for convolutional layers of a VGG-16 model (with batchnormalization) trained on Tiny ImageNet with SGD with learning rate η = 0.1, weight decay λ = 0,and batch size S = 256. Colored lines represent empirical layer-wise squared norms through trainingand the black dashed lines the theoretical predictions given by equation (19).
Figure 15: The per-neuron hyperbolic dynamics of SGD on VGG-16 on Tiny ImageNet. Weplot the per-neuron difference between the squared Euclidean norms for consecutive convolutionallayers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with SGD withlearning rate η = 0.1, weight decay λ = 0, and batch size S = 256. Colored lines represent empiricaldifferences in consecutive layer-wise squared norms through training and the black dashed lines thetheoretical predictions given by equation (20).
Figure 16:	The per-neuron spherical dynamics of Momentum on VGG-16 on Tiny ImageNet.
Figure 17:	The per-neuron hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet.
