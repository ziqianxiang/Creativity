Figure 1: Visualization of encoding (Algorithm 1) and decoding (Algorithm 2) of our full-modelinstance-adaptive method. Each step is denoted with a code, e.g. E9, which refers to line 9 of theencoding algorithm. EE and ED denote entropy encoding and decoding, respectively. Both the latentrepresentation Z and the parameter updates δ are encoded in their respective bitstreams bz and b§.
Figure 2: (a) Averaged RDM performance over all videos of the XiPh-5N 2fps datase for four dif-ferent rate-distortion tradeoffs with β = 3e-3, 1e-3, 2.5e-4, and 1e-4 (from left to right). Ourfull-model finetuning outperforms encoder-only and direct latent optimization with approximately1 dB gain for the same rate. (b) Finetuning progression of the sunflower video over time. Be-tween each dot, 500 training steps are taken, showing that already at the start of finetuning largeRDM gains are achieved and the RDM performance continues to improve during finetuning. (c)Ablation where we show the effect of both quantization- (Qt(δ)) and model rate aware (M Loss)finetuning. Case VI shows the upper bound on achievable finetuning performance when (naively)not taking into account quantization and model update rate.
Figure 3: Empirical distribution of parameter updates for the model finetuned on the sunflowervideo with β = 2.5e-4. Columns denote parameter groups. Top: Histograms of the model updatesδ. Bottom: Histogram of bit allocation for δ. Subtitles indicate the total number of bits for eachparameter group, both expressed in bits per pixel (b/px) and bits per parameter (b/param).
Figure 4: (Left) Illustrative example of the quantization effect and the corresponding gradient (usingthe Straight-Through estimator) for parameter update δ. (Middle) The true bitrate overhead (blue)and its continuous proxy (orange) of a (slab-only) Gaussian prior (α = 0) and their gradients withrespect to unquantized δ . (Right) The true bitrate overhead (blue) and its continuous proxy (orange)of a spike-and-slab prior (α = 4) and their gradients with respect to unquantized δ. One can see howthe effect of the spike (almost) fully disappears in the gradient of the quantized bitrate overhead, asthe largest amount of mass of the spike distribution is part of the center quantization bin.
Figure 5: RD performance of global baseline for all datapoints in Xiph and Xiph-5N.
Figure 6: The mean-scale hyperprior model architecture visualized using the VAE framework.
Figure 7: Compression performance as a function of number of finetuning frames for thesunflower video. The dashed red line indicates sampling at 2 fps (used in our main experiments).
Figure 8: RDM performance for instance-adaptive encoder-only and full-model finetuning, com-pared with the performance of the global model, split per video. The instance-adaptive models usedto create each graph are finetuned on the corresponding single video.
Figure 9: Progression of finetuning over time for all videos in Xiph-5N 2fps.
Figure 10: Histograms showing the distribution of quantized model updates S when finetuning with(top row) and without (bottom row) the model rate regularizer M.
