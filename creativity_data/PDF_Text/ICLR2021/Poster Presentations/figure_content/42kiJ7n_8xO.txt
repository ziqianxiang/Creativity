Figure 1: Results from training GRUs on 3-class categorical data. (a, d) Final hidden states, hT , ofmany test samples, colored by their label, with a few example hidden states trajectories shown explicitly. Theinitial state h0 is shown as a black square, and the three thick solid lines are the three readouts, colored bytheir respective class. (b, e) The hidden deflections, ht - ht-1 for t = 1, . . . , T, from various words inthe vocabulary, with the average deflection of each shown as a solid line. (c, f) Approximate fixed points,h* ≈ F(h*, X = 0), colored by their predicted label (See Appendix A.1 for details). The inset shows thevariance explained as a function of number of PC dimensions. As in (a, d), the solid lines are readout vectorsfor each class. (g) The LSA score vectors projected into the top two variance dimensions. (h) Percentage ofvariance explained versus number of dimensions for LSA.
Figure 2: Evidence integration is accomplished via eigenmodes with long timescales, aligned with thefixed-point manifold. Each step along the x-axis represents a particular eigenmode of the dynamics, withcorresponding eigenvalue λa; the black (left y-axis) points show the time constant (in units of tokens) associatedwith each mode, given by Ta := |log ∣λa||-1; the blue (right y-axis) points show the fraction of the mode's(right) eigenvector which lies in the fixed-point plane. From these plots, it is apparent that only a few modes(highlighted in gray) both: (i) do not decay appreciably on the time scale of the average document length,and (ii) aligned with, and create motion within, the fixed-point manifold. These are the integration modesresponsible for accumulating evidence. We see (a) two integration modes in three-class categorical tasks, (b)three integration modes in four-class categorical tasks, and two integration modes in both (c) five-class orderedand (d) three-class ordered tasks. These plots characterize LSTMs, with other architectures shown in theAppendices.
Figure 3: Higher-dimensional simplexes in N -class categorical classification. (a) Various simplex fixed-point manifolds seen in synthetic data for N = 2, 3, 4, colored by predicted label. (b) In red, the dimensionalityof fixed-point manifolds from synthetic data as a function of number of classes, N . Here, dimensionality is thenumber of dimensions needed to capture more than 95% of the variance in the fixed-point space. Each datapointis an average over 10 initializations. The black dotted line is the dimensionality of an (N - 1)-simplex. (c) Thefixed point manifold and readouts for 4-class AG news, colored by predicted label.
Figure 4: Results from training GRUs on 5-class ordered data. (a, d) Final hidden states, hT, of many testsamples, colored by their label, with a few example hidden states trajectories shown explicitly. The initial stateh0 is shown as a black square, and the three thick solid lines are the five readouts, colored by their respectiveclass. (b, e) The hidden deflections, ht - ht-1 for t = 1, . . . , T, from the various words in the vocabulary,with the average deflection of each shown as a solid line. (c, f) Approximate fixed points, h* ≈ F(h*, X = 0),colored by their predicted label (see Appendix A.1 for details). The inset shows the variance explained as afunction of number of PC dimensions. As in (a, d), the solid lines are readout vectors for each class. (g) TheLSA score vectors projected into the top two variance dimensions. (h) Percentage of variance explained versusnumber of dimensions for LSA.
Figure 5: GRUs trained on multi-labeled synthetic (a-c) and natural (d-f) GoEmotions dataset with onlytwo labels. (a, d) Final hidden states, hT, of a synthetic network, colored by their label, with a few exampletrajectories. The two thick solid lines are readout vectors. The network classifies examples using a 2D planeattractor. (b, e) The hidden deflections, ht - ht-1 for t = 1, . . . , T, from the various words in the vocabulary,with the average deflection of each shown as a solid line. (c, f) Approximate fixed points, h* ≈ F(h*, X = 0),colored by their predicted label; these form a square (cf. the triangle in Fig. 1). The inset shows the varianceexplained (two dimensions are sufficient to capture nearly all of the variance).
Figure 6: Various dimensionality measures as a function of number of classes, N, for the categorical syntheticdata. Specifically, Left: the hidden state space and Right: the fixed point space dimensionality. This is foran `2 of 5 × 10-4 and each datapoint is an average over 10 initializations. The dotted black line shows thepredicted dimensionality of a regular (N - 1)-simplex.
Figure 7: How the dimensionality and accuracy of categorical synthetic data changes as a function of `2regularization. Each datapoint is an average over 10 initializations. Left: Hidden state space dimensionality(global participation ratio) and Right: test accuracy as a function of number of classes, N , for the unorderedsynthetic data for several different values of '2. The dotted black line shows the predicted regular (N - 1)-simplex dimensionality.
Figure 8: Readout (RO) measures as a function of the number of classes, N for synthetic categorical data.
Figure 9: Left: Full space of possible scores as a subspace of R3 . Right: Two-dimensional space resultingfrom projecting out the (1, 1, 1) direction of the R3 subspace, forming a regular 2-simplex.
Figure 10: Synthetic ordered data for N = 2. Left: Final hidden states for 600 test samples, colored by theirlabel, with a few example hidden states trajectories shown explicitly. The initial state h0 is shown as a blacksquare, and the three thick solid lines are the three readouts, colored by their respective class. Center: Thehidden deflections from the three words in the vocabulary, with the average deflection of each shown as a solidline. Right: Fixed points, colored by their predicted label. The inset shows the variance explained.
Figure 11: Synthetic ordered data for N = 3 (top row) and N = 5 (bottom row). Left: Final hidden statesfor 600 test samples, colored by their label, with a few example hidden states trajectories shown explicitly. Theinitial state h0 is shown as a black square, and the three thick solid lines are the three readouts, colored bytheir respective class. Center: The hidden deflections from the three words in the vocabulary, with the averagedeflection of each shown as a solid line. Right: Fixed points, colored by their predicted label. The inset showsthe variance explained.
Figure 12: Synthetic multi-labeled example with N = 3. Left: A few example hidden states trajectories.
Figure 13: Trajectories, individual-word deflections, fixed-point manifolds, and eigenmodes learned by aLSTM on the 3- and 4-class AG News task. Solid lines in the trajectories and fixed-points plots show readoutvectors. Note the similarity between this manifold and that learned by the GRU, described in the main textFigure 1. The words we use to generate the deflections are sox, bankruptcy, military, and computer.
Figure 14: Trajectories, deflections, fixed-point manifolds, and eigenmodes learned by a UGRNN on 3- and4-class AG News. Solid lines in the trajectories and fixed-points plots show readout vectors. Notice that in the4-class case, unlike for GRUs or LSTMs, the UGRNN seems to always learn a square manifold. This is likelydue to correlations in the input data (see Figure 29 for details).
Figure 16: GRU on four-class subset of the DBPedia ontology datasetcreate a 3-class subset by removing examples labeled with 2 and 4 stars. These figures complementFigure 1 in the main text.
Figure 18: LSTM on five-class and three-class YelpE.4 Amazon 5-class and 3-class star predictionAs another example of an ordered dataset, Figures 21, 22, and 23 show results for networks trainedon a 3-class and 5-class subsets of Amazon reviews. These reviews are naturally five star; we createa 3-class subset by removing examples labeled with 2 and 4 stars.
Figure 19: GRU on five-class and three-class Yelp0-BUSeELLFixedpointsFigure 20: UGRNN on five-class and three-class YelpModesEigenmode“approval”, and “annoyance” (these were selected as they were the classes with the largest numberof examples). These results are presented in Figure 24. For this network, despite having threeclasses, we find that the fixed points are largely two dimensional (Fig. 24a). The timescales of theeigenvalues of the Jacobian computed at these fixed points have two slow modes (Fig. 24b), whichoverlap with the two modes (Fig. 24c); thus we have a roughly 2D plane attractor. However, theparticipation ratio (Fig. 24d) indicates that the dimensionality of this attractor is slightly higher thanthe 2D case shown in Fig. 5. We suspect that these differences are due to the strong degree of classimbalance present in the GoEmotions dataset. There are very few examples with multiple labels, forany particular combination of labels. In synthetic multi-labeled data (which is class balanced), wesee much clearer 3D structure when training a 3 class network (Fig. 12).
Figure 20: UGRNN on five-class and three-class YelpModesEigenmode“approval”, and “annoyance” (these were selected as they were the classes with the largest numberof examples). These results are presented in Figure 24. For this network, despite having threeclasses, we find that the fixed points are largely two dimensional (Fig. 24a). The timescales of theeigenvalues of the Jacobian computed at these fixed points have two slow modes (Fig. 24b), whichoverlap with the two modes (Fig. 24c); thus we have a roughly 2D plane attractor. However, theparticipation ratio (Fig. 24d) indicates that the dimensionality of this attractor is slightly higher thanthe 2D case shown in Fig. 5. We suspect that these differences are due to the strong degree of classimbalance present in the GoEmotions dataset. There are very few examples with multiple labels, forany particular combination of labels. In synthetic multi-labeled data (which is class balanced), wesee much clearer 3D structure when training a 3 class network (Fig. 12).
Figure 21: LSTM on five-class and three-class Amazon2 -0--2-Fixedpoints-2	0	2PC #1三1 O.dX山 ue>1 over-ap0	7PC #1Eigenmode1 OFigure 22: GRU on five-class and three-class AmazonF	THE EFFECT OF `2 REGULARIZATION: COLLAPSE, CONTEXT, ANDCORRELATIONSRegularizing the parameters of the network during training can have a strong effect on the dimension
Figure 22: GRU on five-class and three-class AmazonF	THE EFFECT OF `2 REGULARIZATION: COLLAPSE, CONTEXT, ANDCORRELATIONSRegularizing the parameters of the network during training can have a strong effect on the dimensionof the resulting dynamics. We describe this effect first for the datasets with ordered labels, Yelp andAmazon reviews. We penalize the squared '2 -norm of the parameters, adding the term λ∣∣θ∣∣2 to thecross-entropy prediction loss; λ is the `2 penalty and θ are the network parameters.
Figure 24: Analyzing networks trained on a 3-class version of the GoEmotions dataset. (a) Approximatefixed points (gray circles) and readout vectors. Inset shows the variance explained by the different principalcomponents. The dynamics are (largely) 2D. (b) Across these fixed points, we see two slow time constants.
Figure 23: UGRNN on five-class and three-class Amazon至-EUoωuφluQWhen the regularization is sufficient to collapse the manifold to a 1D line, the dynamics are quitesimilar to the 1D line attractors studied in Maheswaranathan et al. (2019). A single accumulatedvalence score is tracked by the network as it moves along the line; this tracking occurs via a singleeigenmode with a time constant comparable to the average document length, aligned with the fixed-point manifold. The difference between the binary- and 5-class line-attractor networks are largelyin the way the final states are classified; in the 5-class case, the line attractor is divided into sectionsbased largely on the angle the line makes with the readout vector of each class.
Figure 25: Performances of the LSTM, GRU, and UGRNN on ordered five-class datasets (both Yelp andAmazon reviews) as a function of `2 regularization. Networks shown in the main text and appendix section Eare highlighted with a line.
Figure 26: Geometry of a GRU trained on the five-class Amazon dataset, which due to high `2 penalty λ(here λ = 0.72) has collapsed to a 1D manifold, rather than the 2D manifolds seen in higher-performingmodels with lower `2 penalty. This 1D collapse is seen in all models, in both the Yelp and Amazon datasets.
Figure 27: Jacobian spectra at an arbitrary fixed point of networks, trained on Yelp, with sufficiently high `2-regularization penalty λ to force the classification manifold to collapse to zero-dimensional (above, λLSTM =0.268, λGRU = 0.1, λUGRNN = 0.268). These spectra display no modes which can integrate information on thetimescale of document length, i.e. no modes with close to unit magnitude.
Figure 28: Fixed-point manifolds and predictions for RNNs trained on five-class Yelp with lower `2 penaltiesthan used in the main text. These networks outperform bag-of-words models and suffer a performance hit uponshuffling the test sentences, indicating an ability to process context (through a mechanism which we have notstudied in this paper). The key point is: though these manifolds clearly extend into more than two dimensions(as indicated by the PCA explained-variance insets, the classes are nearly separable just by using the top twoprincipal components. Thus, the mechanisms we identify in the main text still seem to underlie the operationsof these networks, with contextual processing occurring on top of this integration.
Figure 29: Fixed-point manifolds of LSTM, GRU, and UGRNN trained on 4-class AG News, with sufficientlyhigh `2 regularization penalty λ to collapse the manifold from a tetrahedron to a square (above, λLSTM = 0.3,λGRU = 0.1, λUGRNN = 0.3). Across 10 random seeds per architecture, we observe the vertex of the fixed-pointsquare corresponding to the Sports category always opposite to either the Business or Sci/Tech categories. Thisfact reflects correlations among the classes, shown in Figure 30. Insets: Variance explained by dimension afterPCA projection, showing the 2D nature of the manifold.
Figure 30: Correlations between evidence, provided by individual tokens, for the four classes present in theAG News dataset. The most significant correlations, seen in the middle column, are the negative correlationsbetween the category pairs {Sports, Business} and {Sports, Sci/Tech}. These correlations influence the geom-etry of networks trained with sufficiently high `2 penalty to collapse the manifold to a square (see Figure 29).
Figure 31: A view of the speed variation across the triangular manifold corresponding to a GRU trained on3-class AG News (see Figure 1 in the main text). Colors are given by the log of the speed, see colorbar at theright. Points within the outer contour line correspond to speeds less than the inverse of the average documentlength; points within the inner contour are ten times slower still. Vertical slices in the middle and rightmost plotshow the planar character of the manifold. The three horizontal dotted lines in the leftmost panels correspondto slices plotted in Figure 32.
