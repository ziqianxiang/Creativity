Figure 1:	Attention distribution that each decoder layer (x-axis) attending to encoder layers (y-axis).
Figure 2:	Relative changes of (a) model performance and (b) length of output when maskingindividual encoder layer in the trained Seq2Seq models. As seen, masking the embedding layer leadsto a significant drop of model performance and increase of output length.
Figure 3:	Log scale singular values of the three sub-embedding matrices in the fine-grained layerattention models. Higher log eigenvalues denote more expressivity of the dimensions.
Figure 4: Log scale singular values ofthe embeddings.
