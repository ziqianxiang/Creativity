Figure 1: Sinter (left) and Sintra (right). The Sinter increases as layer goes deeper, especially for GPT2’slast layer. The Sintra are generally high. This means arbitrary vectors have high cosine similarities.
Figure 3: The MMS for all the models. GPT2has significantly higher MMS scores than othermodels from layer 1 to layer 11. This means thecluster effects are more severe in GPT2.
Figure 4: Si0nter (left) and Si0ntra (right). The adjusted Si0nter are close to zero, meaning that the space isisotropic under the adjusted measure.
Figure 5: The 2-D and 3-D view of low-dimensional manifold in GPT/GPT2’s embedding spaces3-D plots are shown in Figure 5c and 5d to demonstrate two manifolds, a band shaped manifold and aSwiss Roll shaped manifold. These plots were computed over PTB dataset. Similar results have beenobtained from WikiText-2 in supplementary. Figure 6 tracks the progression of a narrow band into aSwiss Roll. The Swiss Roll becomes taller and taller with deeper and deeper layers.
Figure 6: The evolution from a narrow band into a taller and taller Swiss Roll with deeper layers.
Figure 7: Embeddings for symbol tokens and word tokens, in layer 3 of BERT and GPT. This showsthat GPT has manifold structure, such that vectors are along the spiral band. BERT’s space is closerto a Euclidean space as similar vectors are in concentrated clusters.
Figure 8: Word frequency heatmap in GPT layer 8 and 9. Red is high frequency, blue is low. Highfrequency words are at the front end of the Swiss Roll, while low frequency words at the other end.
Figure 9: The average LID using Euclideandistance. ELMo’s original embdding dimensionis 1024, larger than other models’ 768.
Figure 12: Local Intrinsic Dimensions. The LID increases as layer goes deeper, reflecting embeddingsspreading out in all models’ deeper layers (becoming more locally isotropic).
Figure 21: The adjusted inter-type cosines, com-puted using K from the criteria of minimizingDB index. The values are still close to 0.
Figure 22: BERT Layer 3 Punctuation(a) Frequent words and infrequent words are on the main cluster, but at two sides. An evidence that words aredistributed based on the frequency.
Figure 23: BERT Layer 3 Words(a) Punctuation are random. Some occupy both islands, some do not.
Figure 24:	GPT2 Layer 3 PunctuationBased on these observations, we have concluded that frequency plays an important role in the tokendistributions. High frequent words and low frequent words are often taking opposite sides of thespace. This is also revealed in Section 4.3. We are yet not clear what causes this, but we suspectit is related to the training process. During training, high frequent words are updated more times.
Figure 25:	GPT2 Layer 3 WordsD.3 Embedding of Translation Language Model XLMWe also perform analysis and visualization on the XLM model (Conneau & Lample, 2019). BERTis mask language model (MLM), GPT is causal language model (CLM), and XLM is translationlanguage model (TLM). We provide visualization of XLM’s 6 layers embeddings here. This is onWikiText-2 dataset.
Figure 27: LID estimate using different number of samples for nearest neighbor search.
Figure 29:	GPT2 Layer 3 Punctuation. The position ID is monotonically increasing along themanifold.
Figure 30:	GPT2 Layer 3 Words. The position ID is monotonically increasing along the manifold.
Figure 31: The context and positions in the embedding space.
