Figure 1: Learning rate with different schedules on CIFAR when retraining for 72 epochs. In (a),the learning rate is fixed to the last learning rate of original training (i.e. 0.001). In (b), the learningrate is ”rewound” to previous 72 epochs (which is 0.01), and is dropped to 0.001 after 32 epochs.
Figure 2:	One-shot structured pruning on CIFAR-10 dataset using 'ι-norm filters pruning (Li et al.,2016) while varying retraining budgets. As can be seen, learning rate schedule matters. Schedulesthat employ large learning rates (LRW, SLR, CLR) are significantly better than fine-tuning. Amongthem, CLR performs best in most cases.
Figure 3:	One-shot unstructured pruning on CIFAR-10 dataset using MWP (Han et al., 2015) ((a)and (b)) and structured pruning on ImageNet with 'ι-norm filters pruning (Li et al., 2016) (c).
Figure 4: Iterative pruning on CIFAR-10 dataset using '「norm filters pruning Li et al. (2016) ((a)and (b)) and on ImageNet using magnitude-based weights pruning (Han et al., 2015) ((c)).
Figure 5:	Results from pruning with high compression ratios for one-shot structured pruning onCIFAR-10 dataset using '「norm filters pruning (Li et al., 2016). With a proper learning rate schedule,it is possible to achieve almost no accuracy drop while having much more compact models and usingslightly more training budgets.
Figure 6:	One-shot structured pruning on CIFAR-10 dataset using PFEC (Li et al., 2016) andrandomly filters pruning with different retraining schemes.
Figure 7: One-shot and Iterative unstructured pruning on CIFAR-10 dataset using MWP (Han et al.,2015) and randomly weights pruning with different retraining schemes.
Figure 8: 'ι-norm Filters Pruning (Li et al., 2016) on CIFAR-10 with different number of warmupepochs.
