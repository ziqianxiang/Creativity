Figure 1: We show an agent in gridworld environment trained with no function approximators, andits optimal policy is intrinsically not robust to perturbations of state observations. The red square andblue circle are the starting point and target (reward +1) of the agent, respectively. The green trianglesare traps, with reward -1 once encountered. The adversary is allowed to perturb the observation toadjacent states along four directions: up, down, left, and right. Adversary earns +1 at traps and -1 atthe target. We set γ = 0.9 for both agent and adversary. This example shows that the vulnerabilityof a RL agent does not only come from the errors in function approximators such as DNNs.
Figure 2: SA-MDP introduces an adver-sary on state observations in a MDP.
Figure 3: Our “Optimal” Attack and Robust Sarsa attack (a previous strong attack proposed in Zhanget al. (2020b)) on Ant and HalfCheetah environments. Previous strong attacks make the agent failand receive a small positive reward (less than 1/10 of the reward without attack). Our attack is strongenough to trick the agent into moving to the opposite direction, receiving a large negative reward.
Figure 4: The performance underthe strongest attack for SA-PPOHopper with different regulariza-tion κ. Even we increase regular-ization, it cannot outperform ourATLA agents.
