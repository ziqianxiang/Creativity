Figure 1: Variations of the synthetic dataset ImageNet-9, as described in Table 1. We label eachimage with its pre-trained ResNet-50 classification—green, if corresponding with the original label;red, if not. The model correctly classifies the image as “insect” when given: the original image, onlythe background, and two cases where the original foreground is present but the background changes.
Figure 2: We train models on each of the “background-only” datasets, then evaluate each on itscorresponding test set as well as the Original test set. Even though the model only learns frombackground signal, it achieves (much) better than random performance on both the correspondingtest set and Original. Here, random guessing would give 11.11% (the dotted line).
Figure 3: We compare test accuracies on Mixed-Same and Mixed-Rand and observe that trainingwith more data reduces the BG-Gap (B G- Gap measures the effect of backgrounds on modelpredictions). While this trend is true for models trained on both IN-9 and ImageNet, the trend is mostnoticeable for models trained on the largest training set, the full ImageNet dataset—this is shown onthe far right side of the graph.
Figure 4: The adversarial backgrounds that most frequently fool IN-9L-trained models into classify-ing a given foreground as insect, ordered by the percentage of foregrounds fooled. The total portionof images that can be fooled (by any background from this class) is 66.55%.
Figure 5: We compare the test performance of a model trained on the synthetic Mixed-Rand datasetwith a model trained on Original. We evaluate these models on variants of IN-9 that containidentical foregrounds. For the Original-trained model, test performance decreases significantlywhen the background signal is modified during testing. However, the Mixed-Rand-trained model isrobust to background changes, albeit at the cost of lower accuracy on images from Original.
Figure 6: Saliency maps for the the Original and Mixed-Rand models on two images. Asexpected, the Mixed-Rand model appears to place more importance on foreground pixels.
Figure 7: We categorize each test set image based on how a model classifies the full image, thebackground alone, and the foreground alone (cf. Table 3). The model trained on Original needs thebackground for correct classification on 35% of images (measured by adding “BG Required” and“BG+FG Required), while a model trained on Mixed-Rand is much less reliant on background. Themodel trained on Only-BG-T requires the background most, as expected; however, the model oftenmisclassifies both the full image and the background, so the “BG Irrelevant” subset is still sizable.
Figure 8: Measuring progress on each of the synthetic ImageNet-9 datasets with respect to progresson the standard ImageNet test set. Higher accuracy on ImageNet generally corresponds to higheraccuracy on each of the constructed datasets, but the rate at which accuracy grows varies based onthe types of features present in each dataset. Each pre-trained model corresponds to a vertical line onthe plot—we mark ResNet-50 and MobileNet-v3s models for reference.
Figure 9: Visualization of how Only-BG-T is created.
Figure 10:	We train models on IN-9LB at different levels of fine-grainedness (more training classesis more fine-grained). The B G-Gap, or the difference between the test accuracies on Mixed-Sameand Mixed-Rand, decreases as we make the classification task more fine-grained, but the decreaseis small compared to the size of the BG-Gap.
Figure 11:	We train models on different-sized subsets of IN-9LB. The largest training set we useis the full IN-9LB dataset, which is 4 times larger than ImageNet-9. While performance on all testdatasets improves as the amount of training data increases, the BG-Gap has almost the same sizeregardless of the amount of training data used.
Figure 12: We train models on different-sized subsets of ImageNet. We use a pre-trained ResNet-50for the rightmost datapoints corresponding to training on the full ImageNet dataset, which is about 30times larger than ImageNet-9. The BG-Gap begins to decrease when the training dataset set size issufficiently large.
Figure 13: We compare various different methods of training models and measure their BG-Gap,or the difference between Mixed-Same and Mixed-Rand test accuracy. We find that (1) Pre-trained IN models have surprisingly small BG-Gap. (2) Increasing fine-grainedness (IN-9LB Coarsevs. IN-9LB Fine) and dataset size (IN-9 vs. IN-9L) decreases the BG-Gap only slightly. (3)`p-robust training does not help. (4) Training on MIXED-RAND (cf. Section 3 appears to be the mosteffective strategy for reducing the B G- Gap. For such a model, the Mixed-Same and Mixed-Randaccuracies are nearly identical.
Figure 14: Comparing model accuracy on Only-BG-T across different foreground object boundingbox sizes. We observe that the model is more likely to succeed when shown only image backgroundsif the removed foreground objects have smaller bounding boxes. The dotted line represents the overallaccuracy of the model on Only-BG-T (averaged over all bounding box sizes).
Figure 15: Backgrounds can also be modified in other ways; for example, it can be blurred. Ourevaluations on this dataset show similar results.
Figure 16: ImageNet-9 variations—Dog.
Figure 17: ImageNet-9 variations—Bird.
Figure 18: ImageNet-9 variations—Vehicle.
Figure 19: ImageNet-9 variations—Reptile.
Figure 20: ImageNet-9 variations—Carnivore.
Figure 21: ImageNet-9 variations—Instrument.
Figure 22: ImageNet-9 variations—Primate.
Figure 23: ImageNet-9 variations—Fish.
Figure 24: Histogram of insect backgrounds grouped by how often they cause (non-insect) fore-grounds to be classified as insect by a IN-9L-trained model. We visualize the five backgrounds thatfool the classifier on the largest percentage of images in Figure 4.
Figure 25: Most adversarial backgrounds—Dog.
Figure 27: Most adversarial backgrounds—Vehicle.
Figure 28: Most adversarial backgrounds—Reptile.
Figure 29: Most adversarial backgrounds—Carnivore.
Figure 30: Most adversarial backgrounds—Instrument.
Figure 31: Most adversarial backgrounds—Primate.
Figure 32: Most adversarial backgrounds—Fish.
Figure 33: Images that are incorrectly classified (as the class on the top row, which is the same classthat their background alone from Only-BG-T is classified as), but are correctly classified (as theclass on the bottom row) when the background is randomized. Note that these images have confusingbackgrounds that could be associated with another class.
