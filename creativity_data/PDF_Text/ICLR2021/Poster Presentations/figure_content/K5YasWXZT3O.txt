Figure 1: Toy examples illustrating TERM as a function of t: (a) finding a point estimate from a set of 2Dsamples, (b) linear regression with outliers, and (c) logistic regression with imbalanced classes. While positivevalues oft magnify outliers, negative values suppress them. Setting t“0 recovers the original ERM objective (1).
Figure 2: TERM objectives for a squared lossproblem with N “ 3. As t moves from ´8to `8, t-tilted losses recover min-loss, avg-loss, and max-loss. TERM is smooth for allfinite t and convex for positive t.
Figure 3: TERM (t“一2) Com-pletely removes the impact of noisyannotators, reaching the perfor-mance limit set by Genie ERM.
Figure 4: TERM-PCA flexiblytrades the performance on the high(H) edu group for the performanceon the low (L) edu group.
Figure 5: TERM (t“100) is com-petitive with state-of-the-art meth-ods for classification with imbal-anced classes.
Figure 6: Q1 paq and Q2 paq are close to Q0paq, which indicates that the solution obtained fromsolving Q3 paq (which is Q2paq) is a tight approximation of the globally optimal solution of Q0paq.
Figure 7: Correctness of Algorithm 4. The y-axis is thenormalized distance between θ obtained by running Alg. 4and the optimal solution θ* via the full batch method. Fordifferent values of t, θ can be arbitrarily close to θ*.
Figure 8: As t → '8, the objective becomes lesssmooth in the vicinity of the final solution, hence suf-fering from slower convergence. For negative values oft, TERM converges fast due to the smoothness in thevicinity of solutions despite its non-convexity.
Figure 9:	For positive values of t, TERM focuses on the samples with relatively large losses (rare instances).
Figure 10:	The tradeoffs between the average-loss and the max/min-loss offered by TERM on the pointestimation (top) and logistic regression (bottom) toy examples presented in Figure 1, empirically validatingTheorems 1- 4. Positive values of t trade the average-loss for the max-loss, while negative values of t trade theaverage-loss for the min-loss. Increasing t from —8 to `8 results in the reduction of loss variance, allowingthe solution to tradeoff between bias/variance and potentially improve generalization.
Figure 11:	Robust regression on synthetic data. In the presence of random noise, TERM with negative t’s (blue,t 二 一2) can fit structured clean data at all noise levels, while ERM (purple) and TERM with positive t’s (red)overfit to corrupted data. We color inliers in green and outliers in brown for visualization purposes.
Figure 12:	In the presence of random noise with the same mean as that of clean data, TERM with negativet’s (blue) can still surpass outliers in all cases, while ERM (purple) and TERM with positive t’s (red) overfitto corrupted data. While the performance drops for 80% noise, TERM can still learn useful information, andachieves much lower error than ERM.
Figure 13: Robust classification using synthetic data. On this toy problem, we show that TERM with negativet’s (blue) can be robust to random noisy samples. The green line corresponds to the solution of the generalizedcross entropy (GCE) baseline (Zhang & Sabuncu, 2018). Note that on this toy problem, GCE is as good asTERM with negative t’s, despite its inferior performance on the real-world CIFAR10 dataset.
Figure 14: TERM with negative t’s (blue) cannot fit clean data if the noisy samples (brown) are adversarial orstructured in a manner that differs substantially from the underlying true distribution.
Figure 15: TERM achieves higher test accuracythan the baselines, and can match the performanceof Genie ERM (i.e., training on all the clean datacombined).
Figure 16: TERM FL (t “ 0.1) significantlyincreases the accuracy on the worst-performingdevice (similar to q-FFL (Li et al., 2020b)) whileobtaining a similar average accuracy (Table 7).
