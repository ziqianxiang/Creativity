Figure 1: Deterministic environments. Fig. 1a shows the deterministic 1D world. The agent cango left right. Fig. 1b shows the deterministic 2D. The agent can go left, up, right and down. Fig 1cshows the deterministic tree world. The agent can go left and right.
Figure 2: Estimated empowerment during the training in deterministic environments. Fig. 2ashows the deterministic 1D world and its training results. The agent can go left right. Fig. 2b showsthe deterministic 2D world and its training results. The agent can go left, up, right and down. Fig2c shows the deterministic tree world and its training results. The agent can go left and right. Greenshows the distribution of sf.
Figure 3: Deterministic grid world with 4 rooms. The environment is a 25 × 25 gridworld with 4 rooms. The agent can go left, up, right and down. The agent starts from (4,4) and (10, 4) with Tmax = 25. Green shows the distribution of sf .
Figure 4: Estimated empowerment during the training in stochastic environments. The envi-ronments are equal to Fig. 1 except for their stochasticity. Green shows the distribution of sf .
Figure 5: Stochastic gird world with 35 rooms. The environment is a 15 × 15 grid world with 35rooms (black cells surrounded by gray walls). We set Tmax = 25. The agent can go up, down andright. It starts from (0, 6). Once the agent enters a room, the environment returns done and then thefinal action is available only. Green shows the distribution of sf . The external reward is composedof -0.1 for every time step, +1 for entering the normal room and +100 for entering the special room.
Figure 6: Learned behaviors by Algorithm 2 in HalfCheetah-v3. The question mark iswritten when the result of the flip is unknown. The various behaviors such as forward flip,backflip, etc. are learned by Algorithm 2.
