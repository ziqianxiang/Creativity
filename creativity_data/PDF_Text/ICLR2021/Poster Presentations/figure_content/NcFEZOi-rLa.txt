Figure 1: A shape biased model (trained on Stylized Ima-geNet) makes predictions based on the object’s shape, ordoes it? Extracting binary (3rd column) and semantic (4thcol.) segmentation maps with a one convolutional layer read-out module shows that, while the model classifies the imagelevel shape label correctly as a ‘bird’, it fails to encode thefull object shape (3rd col.) as well as fails to categoricallyassign every object pixel to the ‘bird’ class (4th col.).
Figure 2: Illustration of the techniques used to quantify shape in this paper. (A) Estimating thedimensionality of semantic concepts in latent representations: We stylize each image with fivetextures to generate image pairs which share the semantic concepts shape (right pair) and texture(left pair). We feed these image pairs (shown is shape) to an encoder, E(∙), and calculate the mutualinformation between the two latent representations, za and zb , to estimate the dimensionality, |zshape |.
Figure 3: Analyzing the number of dimensions in a ResNet50 which encode shape (|zshape|) andtexture (|ztexture |) over the course of ImageNet (IN, left two) and Stylized ImageNet (SIN, right)training. Dimensions are estimated using stage four, |z(4) | = 1024, and stage five, |z(5) | = 2048,latent representations. When training begins, z is very sensitive to texture but over the courseof training learns to focus on the shape instead (faster in SIN case). The vertical lines representmultiplying the learning rate by a factor of 0.1. Note that the estimated dimensions differ slightlyfrom Table 1 as we trained the IN and SIN models used in this figure from scratch.
Figure 4: Stage-wise predictions of read-out module onbinary (Bin) and semantic (Sem) segmentation.
Figure 6: Binary and semantic segmentation masks extracted fromCNNs trained on ImageNet (IN) and Stylized ImageNet (SIN).
Figure 5: Quantifying the shape andsemantic information encoded by aCNN over the course of ImageNettraining. Vertical lines represent thelearning rate decay.
Figure 7: Shape encoding results by means of training a read-out module on the latent representa-tions of an ImageNet (i.e., texture-biased model) (left two) and stylized ImageNet (shape-biasedmodel)(right two) trained ResNet-50 for binary and semantic segmentation when removing all buttop X% shape or texture-specific neurons.
Figure 8: Shape (left) and texture (right) encoding dimensions estimated on each layer ofAlexNet (Krizhevsky et al., 2012). Shape biased AlexNet trained on Stylized ImageNet (Geirhoset al., 2018) encode more shape at the later layers of the network which is consistent with the findingsfor ResNets (He et al., 2016).
