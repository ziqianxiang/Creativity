Figure 1:	The computational graph of the control-certificate jointly learning framework in multi-agent systems.
Figure 2:	Neural network architecture of the control policy. The blue part indicates the quantity-permutationinvariant observation encoder, which maps Oi(t) ∈ Rn×lNi(t)1 With time-varying dimension to a fixed lengthvector. The network takes the state si and local observation oi as input to compute a control action ui . Theneural network of the decentralized CBF hi has a similar architecture except that the output is a scalar.
Figure 3: Illustrations of the 2D environments used in the experiments. The Navigation and Predator-Preyenvironments are adopted from the multi-agent particle environment (Lowe et al., 2017). The Nested-Ringsenvironment is adopted from Rodrlguez-Seda et al. (2014).
Figure 4: Safety rate and reward in the 2D tasks. Results are taken after each method converged and areaveraged over 10 independent trials.
Figure 5: Environments and results of 3D tasks. In Maze and Tunnel, the initial and target locations of eachdrone are randomly chosen. The drones start from the initial locations and aim to reach the targets withoutcollision. The results are taken after each method converged and are averaged over 10 independent trials.
Figure 6: Generalization capability of MDBC in the 3D tasks. MDBC can be trained with 8 agent in oneenvironment and generalize to 1024 agents in another environment in testing.
Figure 8: Generalization capability of our method in the 2D tasks. Our method is trained with 8agents and tested with up to 1024 agents.
Figure 9: Visualization of the learned CBF in the Maze environment with 2 agents. The red area is where thedistance between agents is less that the safe threshold.
