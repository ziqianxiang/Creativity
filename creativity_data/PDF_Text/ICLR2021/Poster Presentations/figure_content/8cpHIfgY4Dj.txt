Figure 1: Meta-training procedure. The inference network qφ uses context data c to computethe latent context variable z, which conditions the actor and critic, and is optimized by the distancemetric learning (DML) objective. The learning of context encoder (Ldml) and control policy (Lactor,Lcritic) are decoupled in terms of gradients.
Figure 2: (a) t-SNE visualization of embedding vectors drawn from 20 randomized tasks on Half-Cheetah-Vel. Inverse-power distance metric losses (DML) achieve better clustering. Data pointsare color-coded according to task identity. (b) FOCAL trained with inverse-power DML lossesoutperform the linear and square distance losses.
Figure 3: Performance vs. numberof transition steps sampled for train-ing. Top: Average episodic testing re-turn of FOCAL vs. other baselines on4 meta-environments with different re-ward functions across tasks. Bottom:Average episodic testing return of FO-CAL vs. other baselines on 2 meta-environments with different transitiondynamics across tasks.
Figure 4: Comparative study of 4 algorithm variants: FOCAL with deterministic/probabilisticcontext encoder, Batch PEARL with/without behavior regularization. (a) t-SNE visualizationof the embedding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points arecolor-coded according to task identity. (b) Return curves on tasks with different reward functions(Half-Cheetah-Vel) and transition dynamics (Walker-2D-Params).
Figure 5: FOCAL vs. FOCAL with coupled gradients. (a) t-SNE visualization of the embed-ding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points are color-codedaccording to task identity. (b) Return curves on Walker-2D-Params.
Figure 6: Average episodic testing return of FOCAL vs. other baselines on five meta-environments.
Figure 7: 3D projection of the embedding vectors ∈ (-1, 1)l drawn from 20 randomized tasks onWalker-2D-Params. Data points are color-coded according to task identity.
Figure 8: Distribution of rollout trajectories of trained SAC policies of three performance levels:random, medium and expert. Since reWard is sparse, only states that lie in the red circle are givennon-zero reWards, making meta-learning more challenging and sensitive to data distributions.
Figure 9: FOCAL vs. FOCAL with coupled gradients and policy regularization. The taskrepresentation alone of the coupled training scheme might not be superior, but the policy per-formance can be improved due to end-to-end optimization. (a) t-SNE visualization of the embed-ding vectors drawn from 20 randomized tasks on Walker-2D-Params. Data points are color-codedaccording to task identity. (b) Return curves on Walker-2D-Params.
Figure 10: FOCAL with value penalty vs. Batch PEARL on Ant-Fwd-Back. The Q-functionlearned by Batch PEARL diverges (> 1011) whereas the Q-function of FOCAL, despite its largeorder of magnitude due to value penalty, converges eventually given proper regularization (α =106)21Published as a conference paper at ICLR 2021Figure 11: FOCAL with policy regularization vs. Batch PEARL on Walker-2D-Params. The Q-function learned by Batch PEARL diverges (> 108) whereas the Q-function of FOCAL, convergesto the true discounted cumulative return (≈ 200 for γ = 0.99).
Figure 11: FOCAL with policy regularization vs. Batch PEARL on Walker-2D-Params. The Q-function learned by Batch PEARL diverges (> 108) whereas the Q-function of FOCAL, convergesto the true discounted cumulative return (≈ 200 for γ = 0.99).
