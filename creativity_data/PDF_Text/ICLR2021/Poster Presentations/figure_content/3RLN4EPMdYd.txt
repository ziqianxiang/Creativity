Figure 1: The overall framework of the proposed hierarchical approach. Given the context framesand label maps extracted by the segmentation network, our model predicts the future frames by es-timating the semantic label maps using a stochastic sequence estimator (Section 2.1) and convertingthe predicted labels to RGB frames by using a conditional image sequence generator (Section 2.2).
Figure 2: Quantitative comparisons of the long-term prediction on human dancing sequences.
Figure 3: Qualitative comparisons of long-term video generation results across models on humandancing sequences. All models are conditioned on the same context frames. Click the image to playthe video in a browser. More results are available at https://1konny.github.io/HVP/.
Figure 4: Quantitative comparisons of the long-term generation quality on KITTI Benchmark.
Figure 5: Qualitative comparisons of long-term video generation results across models. Althoughall models succeed in generating plausible frames in a short-term (t = 1 〜40), only our approachcan generate persistent and convincing futures even in the end (t = 251 〜500). Click the image toplay the video in a browser. More results are available at https://1konny.github.io/HVP/.
Figure 7: Long-term prediction results on 256 × 512 Cityscapes dataset. Click the image to play thevideo in a browser. For more qualitative results and details, please refer to Figure H and Figure J inthe appendix and the project website: https://1konny.github.io/HVP/.
Figure 8: Quantitative comparisons of long-term prediction performances among ablated models.
Figure A: Qualitative comparisons of frame-wise video prediction results across models: (top pane)given the same context frames, we sample 100 predictions for each model and show their bestpredictions in terms of the cosine similarity metric. (bottom pane) for each best prediction, we showthe semantic label maps predicted by DensePose (AlP Guler et al., 2018).
Figure B: Diverse video prediction results on a 128 × 128 dance sequence. All samples are condi-tioned on the same context frames.
Figure D: Qualitative comparisons of frame-wise video prediction results across models: (top pane)given the same context frames, we sample 100 predictions for each model and show their bestpredictions in terms of the cosine similarity metric. (bottom pane) for each best prediction, we showthe semantic label maps predicted by pre-trained semantic segmentation network (Zhu et al., 2019).
Figure E: Diverse video prediction results on a 256 × 256 KITTI sequence. All samples are condi-tioned on the same context frames.
Figure F: Long-term prediction results on a high-resolution KITTI sequence. Both the frames andthe label maps are predicted by our method. Our method can generate high-resolution frames (256 ×256 pixels) into the long-term future without particular quality degradation.
Figure G: Qualitative comparisons of long-term generation results across the ablated models. Weablate SVG-extend to produce four baselines: LSTM where stochastic estimation modules are re-moved, CNN where stochastic and recurrent modules are removed, SVGsoft where the discretizationprocess is removed and the model is modified to observe and estimate continuous logits of semanticsegmentation model, and SVGrgbprop where an output of the structure generator at time t is trans-lated to RGB frame by the pixel generator and then further processed by the semantic segmentationmodel before used as an input at time t + 1. Please refer to the Section 5 for the discussion.
Figure H: Quantitative comparisons of future segmentation models. All models predict structures ofmoving objects (8 classes) up to 29th frame given 4 context frames (2, 5, 8, and 11th). The oracle isobtained by running the pre-trained segmentation model on the ground-truth frames. The results ofcompared methods (S2S and F2F) are based on the pre-trained models provided by the authors.
Figure I: Qualitative results of ablating Gedge on the 256 × 512 Cityscapes dataset.
Figure J: Long-term prediction results on 256 × 512 Cityscapes dataset. We predict 96 future frames(l4, 17,…，299th) given 4 context frames. OUr structure generator is extended to produce additionalinstance boundary map for improved instance boundary demarcation in the image generator. Reddashed boxes indicate some biases in the prediction, such as static and cyclic motions in the predictedstructures, which are attributed to the short training sequences in the Cityscapes dataset. Please referto Section 4.4 for the description.
