Figure 1: GTS architecture.
Figure 2: One-day forecast (METR-LA)OooooooonOooooooo64208642ILII(SPUouαjs) qIJodaJ」ajdaJEJJ6U'Ξ-Q-IΠFigure 3: Training time per epoch. Not learn-ing a graph (DCRNN) is the fastest to trainand learning a graph by using LDS needsorders of magnitude more time. Our modellearns a graph with a favorable overhead.
Figure 3: Training time per epoch. Not learn-ing a graph (DCRNN) is the fastest to trainand learning a graph by using LDS needsorders of magnitude more time. Our modellearns a graph with a favorable overhead.
Figure 4: Effect of regularization (PMU). “Degree” means expected degree of the graph.
Figure 5: Learned structures (PMU). Left: difference between Aa and θ (in terms of average crossentropy) as λ varies. Right: Aa overlaid with θ (orange edges exist in θ but not in Aa) when λ = 20.
Figure 6: Effect of regularization (left: METR-LA; right: PEMS-BAY; 60 min forecast). Averagecross entropy measures departure from Aa.
Figure 7: Example time series from the PMU data set. Data have been standardized and the verticalaxes do not show the raw value.
