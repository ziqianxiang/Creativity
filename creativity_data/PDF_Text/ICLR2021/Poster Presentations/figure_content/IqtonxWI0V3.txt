Figure 1: A ReLU network function before (left) and after (right) extraction. Left : Hyperplanesseparate the input space into linear regions. Most of them do not contain any data points. Each datapoint occupies its own linear region. Right: After extraction, the function remains unchanged on thelinear regions of training samples. Test samples now fall into regions of training samples.
Figure 2: Angle and distance between coefficients of the linear regions of test data and training dataused for correct (blue) and incorrect (red) classification by the extracted function. All angles forCNNs (left) are close to orthogonal, while FCNs (right) shows clear correlations between angles,distances and correctness of prediction.
Figure 3: Left: Comparison of average test accuracy (dotted red) and agreement with labels assignedby the original network (blue) for networks Narrow and Wide while being transformed into tropicalfunctions. The letters D,M,C denote dense, maxpooling and convolutional layers, respectively. Right:Average test accuracy for all CIFAR networks while being transformed into a tropical function.
Figure 4: Mean and standard deviation of the performance of extracted tropical function duringtraining over 5 networks. The performance of the CNN network function suffers strongly fromextraction early in training, whereas the FCN shows a slow, gradual decline.
Figure 5: Pearson correlation between the linear coef-ficients of two separately trained networks averagedover all input dimensions. Black dots indicate a reduc-tion of the learning rate by a factor of 10. Coefficientsare more correlated after re-training for FCNs thanfor CNNs, suggesting that FCNs encode more infor-mation in the coefficients of linear regions than CNNsing the correlation factors over the dimensions. We experiment with two pairs of FCNs and CNNstrained on CIFAR10 on MNIST. Figure 5 shows the evolution of the correlation during training,confirming that the similarity of coefficient values is also larger for FCNs than for CNNs if measuredby correlation. The correlation of linear coefficients after re-training and convergence for the FCNs issignificant. Interestingly, for the networks trained on CIFAR10, we notice jumps in the correlationvalues precisely when the learning rates get decreased.
