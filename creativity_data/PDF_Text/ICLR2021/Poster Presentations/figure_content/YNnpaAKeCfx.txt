Figure 1: The black path in the left figure shows how FairBatch adjusts the batch-size ratios ofsensitive groups using two reweighting parameters λ1 and λ2 (hyperparameters employed in ourframework to be described in Sec. 2), thus minimizing their ED disparity, i.e., achieving equalizedodds. The code in the right figure shows how easily FairBatch can be incorporated in a PyTorchmachine learning pipeline. It requires a single-line change to replace the existing sampler withFairBatch, marked in blue.
Figure 2: F (λ) is not convex, but quasi-convex.
Figure 3:	EO disparity Curves of algorithms on the synthetiC dataset.
Figure 4:	DP disparity curves of algorithms on the synthetic dataset.
Figure 5: Epochs-fairness disparity curves of all algorithms together.
Figure 6: Accuracy-fairness disparity trade-off curves of FairBatch on the synthetic dataset.
Figure 7: Comparison of the weight changes on AdaFair and FairBatch w.r.t. equalized odds on thesynthetic dataset.
Figure 8: Fairness curves of FairBatch on the synthetic dataset, with/without loss-based weight-ing (Loshchilov and Hutter, 2016).
