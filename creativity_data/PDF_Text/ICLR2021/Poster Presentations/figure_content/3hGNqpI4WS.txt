Figure 1: Deployment efficiency is defined as the number of changes in the data-collection policy (I), whichis vital for managing costs and risks of new policy deployment. Online RL algorithms typically requiremany iterations of policy deployment and data collection, which leads to extremely low deployment efficiency.
Figure 2: Evaluation of BREMEN with the existing methods (ME-TRPO, SAC, BCQ, BRAC) under deploymentconstraints (to 5-10 deployments with batch sizes of 200k and 100k). The average cumulative rewards and theirstandard deviations with 5 random seeds are shown. Vertical dotted lines represent where each policy deploymentand data collection happen. BREMEN is able to learn successful policies with only 5-10 deployments, while thestate-of-the-art off-policy (SAC), model-based (ME-TRPO), and recursively-applied offline RL algorithms (BCQ,BRAC) often struggle to make any progress. For completeness, we show ME-TRPO(online) and SAC(online)which are their original optimal learning curves without deployment constraints, plotted with respect to samplesnormalized by the batch size. While SAC(online) substantially outperforms BREMEN in sample efficiency, ituses 1 deployment per sample, leading to 100k-500k deployments required for learning. Interestingly, BREMENachieves even better performance than the original ME-TRPO(online), suggesting the effectiveness of implicitbehavior regularization. For SAC and ME-TRPO under deployment-constrained evaluation, their batch sizebetween policy deployments differs substantially from their standard settings, and therefore we performedextensive hyper-parameter search on the relevant parameters such as the number of policy updates betweendeployments, as discussed in Appendix F.2.1.
Figure 3: Robotics environments and the results under deployment constraints (10 deployments with batch sizesof 25k). The performances are averaged over 5 seeds. BREMEN seems the only method that shows both stableimprovement and solving tasks without large degradation or sub-optimal convergence.
Figure 4: We examine average cumulative rewards (top) and corresponding KL divergence between the lastdeployed policy and the target policy (bottom) with batch size 200K in limited deployment settings. The behaviorinitialized policy remains close to the last deployed policy during improvement without explicit value penalty—αDκL(∏θ ∣∣∏β). The explicit penalty is controlled by a coefficient α.
Figure 5: From the view of both sample and deployment efficiency at certain cumulative reward threshold, weevaluate BREMEN in Ant (left) and HalfCheetah (right). x and y axes respectively represent the number ofsamples and the number of deployments.Each data point comes from running BREMEN with different rewardthresholds and batch sizes. The numbers above the points (e.g. 1000, 2000, ...) represent the reward threshold.
Figure 6: Comparison to the pessimistic reward shaping incorporated into BREMEN. Soft rewardpenalty (MOPO-like, orange) performs well in the environments where the incomplete models appear tobe fatal.
Figure 7: Four standard MuJoCo benchmark environments used in our experiments.
Figure 8:	Search on the number of iterations for SAC policy optimization between deployments. The number oftransitions per one data-collection is 200K.
Figure 9:	Search on the number of iterations for BREMEN policy optimization between deployments. Thenumber of transitions per one data-collection is 200K.
Figure 10:	Search on the Gaussian noise parameter σ in HalfCheetah. The number of transitions per onedata-collection is 200K.
Figure 11: Comparison of the number of dynamics models in deployment-efficient settings.
Figure 12: Comparison of the number of dynamics models in offline settings.
Figure 13: Average cumulative rewards (top row) and corresponding KL divergence of learned policies from thelast deployed policy (second row) and model errors (bottom row) in offline settings with 1M dataset (no noise).
Figure 14: Performance in Offline RL experiments (Table 1). (top row) dataset size is 1M, (second row) 100K,and (bottom row) 50K, respectively. Note that x-axis is the number of iterations with policy optimization in alog-scale.
Figure 15:	Performance in Offline RL experiments with -greedy dataset noise = 0.1. Dataset size is 1M.
Figure 16:	Performance in Offline RL experiments with -greedy dataset noise = 0.3. Dataset size is 1M.
Figure 17: Performance in Offline RL experiments with gaussian dataset noise N(0, 0.12). Dataset size is 1M.
Figure 18: Performance in Offline RL experiments with gaussian dataset noise N(0, 0.32). Dataset size is 1M.
Figure 19: Performance in Offline RL experiments with completely random behaviors. Dataset size is 1M.
Figure 20: Performance in Deployment-Efficient RL experiments with different reward function of HalfCheetah.
