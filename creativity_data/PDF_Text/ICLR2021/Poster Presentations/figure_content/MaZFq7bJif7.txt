Figure 1: Snitch Localization in CATER (Girdhar &Ramanan, 2020) is an object permanence task where thegoal is to classify the final location of the snitch objectwithin a 2D grid space.
Figure 2: An overview of the Hopper framework. Hopper first obtains frame representations from the inputvideo. Object representations and object tracks are then computed to enable tracking-integrated object-centricreasoning for the Multi-hop Transformer (details in Section 4).
Figure 3: Qualitative result & interpretability of our model. We highlight the object attended per hop andper head from Hopper-multihop. The frame border of attended object is colored based on the hop index(accordingly: Hop1, Hop2, Hop3, Hop4, and Hop5). The bounding box of the most attended object in eachhop shares the same color as the color of the hop index. Please zoom in to see the details. Best viewed in color.
Figure 4: Hop Index vs. Frame Index: We plot theindex of frame of the most attended object identifiedby each hop. Each hop has its unique color, andthe transparency of the dot denotes the normalizedfrequency of that frame index for that particular hop.
Figure 5: Diagnostic analysis of the performance interms of when snitch becomes last visible in the video.
Figure 6: Visualization of attention weights & interpretability of our model. In (a), we highlight object(s)attended in every hop from Hopper-multihop (frame border is colored accordingly: Hop1, Hop2, Hop3,Hop4, and Hop5). In (b), we visualize the attention weights per hop (the smaller attention weight that anobject has, the larger opacity is plotted for that object entity). As shown, Hopper-multihop performs 5 hopsof reasoning for the video ‘CATERh_054110’. Our model performs reasoning by hopping over frames andmeanwhile selectively attending to objects in the frame. Please zoom in to see the details. Best viewed in color.
Figure 7: We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop.
Figure 8:	We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop.
Figure 9:	We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop.
Figure 10:	We visualize the	attention weights per hop	and	per head	from Transformers in our Hopper-multihop. Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1,Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.
Figure 11:	We visualize the attention weights per hop and per head from Transformers in our Hopper-multihop. Objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1,Hop2, Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.
Figure 12: Tracking-integrated object representation visualization. Hopper utilizes tracking-integrated objectrepresentations since tracking can link object representations through time and the resulting representations aremore informative and consistent. We visualize the object track representations of video ‘CATERh_054110’(attention weights from Hopper-multihop for this video are shown in Figure 6). Here, every column is fora track and every row is for a frame. The bounding box and object class label computed from the objectrepresentation are plotted (404 is the object class label for 0, i.e., none object, and 140 is the object class labelfor the snitch). As shown in the figure, the tracks that are obtained from our designed Hungarian algorithm arenot perfect but acceptable since having perfect tracking here is not the goal of this paper. Our model Hopper-multihop takes in the (imperfect) object track representations (along with the coarse-grained frame track) asthe source input to the Multi-hop Transformer, and then further learns the most useful and correct task-orientedtrack information implicitly (as shown in Figure 6). Hopper-multihop preforms 5 hops of reasoning for thisvideo; objects attended in every hop are highlighted (whose frame border is colored accordingly: Hop1, Hop2,Hop3, Hop4, and Hop5). Please zoom in to see the details. Best viewed in color.
Figure 13: Histogram of the frame index of the last visible snitch of every video. We find that CATER is highlyimbalanced for the Snitch Localization task in terms of the temporal cues: e.g., snitch is entirely visible at theend of the video for 58% samples. This temporal bias results in high-accuracy even if it ignores all but the lastframe of the video. Our dataset CATER-h addresses this issue with a balanced dataset.
Figure 14: Data distribution over classes in CATER-h.
Figure 15: Failure cases. Hopper produces wrong Top-1, Top-5 prediction and terrible L1 results for thesefailure cases. Similarly, we highlight the attended object per hop and per head (Hop1, Hop2, Hop3, Hop4, andHop5). Case (a). ‘CATERh_048295’: the occlusion has made the Snitch Localization task extremely difficultsince when the snitch got occluded it was contained by the brown cone. Meanwhile, Hopper fails to attend tothe immediate container of the last visible snitch (should be the brown cone at frame 5) in Hop 2. Case (b).
Figure 16: Architecture of the Multi-hop Transformer (MHT) that learns a comprehensive videoquery representation and meanwhile encourages multi-step compositional long-term reasoning of aspatiotemporal sequence. As inputs to this module, the ‘Source Sequence’ is [Tf, To], where [ , ] denoteconcatenation; and the ‘Target Video Query’ is E ∈ R1×d. ‘Final Video Query Representation’ is e ∈ R1×d.
