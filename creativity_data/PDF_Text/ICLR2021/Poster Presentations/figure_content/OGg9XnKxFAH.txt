Figure 1: In the multi-input multi-output (MIMO) configuration, the network takes M = 3 inputs and gives Moutputs. The hidden layers remain unchanged. The black connections are shared by all subnetworks, while thecolored connections are for individual subnetworks. (a) During training, the inputs are independently sampledfrom the training set and the outputs are trained to classify their corresponding inputs. (b) During testing, thesame input is repeated M times and the outputs are averaged in an ensemble to obtain the final prediction.
Figure 2: Illustration of MIMO applied to a synthetic regression problem. (left) Example of MIMO learningM = 3 diverse predictors. As M increases, predicting with MIMO comes with a higher bias but a smallervariance (two middle panels respectively). Despite the slight increase in bias, the decrease in variance translatesinto an improved generalization performance (right).
Figure 3: Accuracy landscape and function space landscape comparison of individual subnetworks for MIMO(top row) and the naive multiheaded architecture (bottom row). (left): The test accuracy in the weight spacesection containing M = 3 trained subnetworks and the origin. For the MIMO architecture, the individualsubnetworks converge to three distinct low-loss basins, while naive multihead leads to the same mode. (middle-left to right): The blue, red and green panels show the disagreement between the three trained subnetworks forthe same section of the weight space. For the MIMO architecture, the subnetworks often disagree, while for thenaive multihead architecture they are all essentially equivalent.
Figure 4: Analyzing the subnetworks on the CIFAR10 dataset. (left): Histogram of the conditional variancesof the pre-activations w.r.t. each input (M = 2, ResNet28-10). (middle-left): Scatter plot of the conditionalvariances of the pre-activations w.r.t. each input. Almost all the pre-activations only have variance with respectto one of the inputs: the subnetwork they that are part of (M = 3, ResNet28-10). (middle-right): Trainingtrajectories of the subnetworks. The subnetworks converge to different local optima (M = 3, SmallCNN).
Figure 5: The performance of the subnetworks and the ensemble of the subnetworks as the number of subnetworks(M) varies. M = 1 is equivalent to a standard neural network (ResNet-28-10).
Figure 6: (a) Performance of MIMO (M = 2) as a function of ρ on ImageNet. At ρ = 0, the subnetworksare independent and they are limited by the network capacity. With ρ > 0, the subnetworks are able to sharefeatures and better utilize the network capacity. Wide ResNet has 2× more filters. (b) Repeating examples in thesame batch improves convergence and yields a slight boost in performance.
Figure 7: Accuracy and log-likelihood versus varying L1 regularization for ResNet28-10 on CIFAR10 (top row)and CIFAR100 (bottom row). Since MIMO better exploits the capacity of the network, its performance is moresensitive to the constraining of the capacity as the regularization increases. The larger the ensemble size, thestronger the effect.
Figure 8: Accuracy and log-likelihood versus varying L2 regularization for ResNet28-10 on CIFAR10 (top row)and CIFAR100 (bottom row). Since MIMO better exploits the capacity of the network, its performance is moresensitive to the constraining of the capacity as the regularization increases. The larger the ensemble size, thestronger the effect.
Figure 9: Accuracy and log-likelihood versus varying sparsity of a ResNet28-10 on CIFAR10 (top row) andCIFAR100 (bottom row). Since MIMO better exploits the capacity of the network, its performance is moresensitive to the sparsification of the network (as induced by an increasing L1 regularization). The larger theensemble size, the stronger the effect.
