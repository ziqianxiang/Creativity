Figure 1: Signal Propagation Plot for a ResNetV2-600 at initialization with BatchNorm, ReLUactivations and He init, in response to anN(0, 1) input at 512px resolution. Black dots indicate theend of a stage. Blue plots use the BN-ReLU-Conv ordering while red plots use ReLU-BN-Conv.
Figure 2: SPPs for three different variants of the ResNetV2-600 network (with ReLU activations).
Figure 4: Residual Blocks for pre-activation ResNets (He et al., 2016a). Note that some variantsswap the order of the nonlinearity and the BatchNorm, resulting in signal propagation which ismore similar to that of our normalizer-free networks.
Figure 5: Residual Blocks for post-activation (original) ResNets (He et al., 2016b).
Figure 6: Signal Propagation Plot for a ResNetV2-600 with ReLU and He initialization, withoutany normalization, on a semilog scale. The scales of all three properties grow logarithmically dueto signal explosion.
Figure 7: Signal Propagation Plot for 600-layer ResNetV2s with linear activations, comparingBatchNorm against with normalizer-free scaling. Note that the max-pooling operation in the ResNetstem has here been removed so that the inputs to the first blocks are centered.
Figure 8: Signal Propagation Plot for a Normalizer-Free ResNetV2-600 with ReLU and Scaled WS,using Y = √∕2, the gain for ReLU from (He et al., 2015). As this gain (derived from JEgxJ2]) islower than the correct gain (JVar(Ig(X))), signals attenuate progressively in the first stage, then arefurther downscaled at each transition which uses a β value that assumes a higher incoming scale.
Figure 9: Signal Propagation Plot for a Normalizer-Free ResNetV2-600 with ReLU, Scaled WSwith correctly chosen gains, and unmodified Squeeze-and-Excite Blocks. Similar to understimatingγ values, unmodified S+E blocks will attenuate the signal.
Figure 10: Signal Propagation Plot for a ResNet-600 with FixUp. Due to the zero-initializedweights, FixUp networks have constant variance in a stage, and still demonstrate variance resetsacross stages.
Figure 11: Signal Propagation Plot for a ResNet-600 V1 with post-activation ordering. As thisvariant applies BatchNorm at the end of the residual block, the Residual Average Channel Varianceis kept constant at 1 throughout the model. This ordering also applies BatchNorm on shortcut 1x1convolutions at stage transition, and thus also displays variance resets.
