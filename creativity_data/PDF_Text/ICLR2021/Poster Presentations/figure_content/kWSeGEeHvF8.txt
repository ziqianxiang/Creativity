Figure 1: In Off-Policy Evaluation (top)the goal is to estimate the value of a sin-gle policy given only data. Offline PolicySelection (bottom) is a closely relatedproblem: given a set of N policies, at-tempt to pick the best given only data.
Figure 2: Error is a natural measure foroff-policy evaluation. However for pol-icy selection, it is sufficient to (i) rankthe policies as measured by rank correla-tion, or (ii) select a policy with the lowestregret.
Figure 3: Online evaluation of policy checkpoints for 4 Offline RL algorithms with 3 random seeds. We observea large degree of variability between the behavior of algorithms on different tasks. Without online evaluation,tuning the hyperparameters (e.g., choice of Offline RL algorithm and policy checkpoint) is challenging. Thishighlights the practical importance of Offline policy selection when online evaluation is not feasible. SeeFigure A.7 for additional tasks.
Figure 4: DOPE RL Unplugged Mean overall performance of baselines.
Figure 5: DOPE D4RL Mean overall performance of baselines.
Figure 6: Rank correlation for each baseline algorithm for each RL Unplugged task considered.
Figure 7: Scatter plots of estimate vs ground truth return for MB-AR and FQE-D on selected tasks.
