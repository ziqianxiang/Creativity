Figure 1: (a) Results when using sin activation function in a 2-layer MLP. We initialize the first layer usingrandom normal distribution with mean zero and vary the standard deviation σ as shown in the plots. Initializationscheme for the top layer is kept unchanged and uses a glorot uniform initializer (Glorot & Bengio, 2010). Theplot shows the drastic changes in generalization ability solely due the changes in scaling on CIFAR-10 dataset.
Figure 2: Results when using sin activation function in a 2-layer MLP applied on CIFAR-10 dataset (Krizhevsky,2009). We initialize W1 using random normal distribution with mean zero and vary the standard deviation σas shown in the plots. The initialization scheme for W2 is kept unchanged, using a Glorot uniform initializer(Glorot & Bengio, 2010). (a) shows the the evolution and rate of attaining perfect training accuracy. (b) plotsthe norm of the gradients of W1 over norm of W1 . As elucidated in (Chizat & Bach, 2018), increasing thescale initialization leads to gradients being increasingly smaller than the weights and thus weights not beingable to move very far from initialization. (c) shows how example gradient alignment can capture differencesin generalization ability in case of sin activation as the scale of initialization is increased. Plot (d) shows thatrepresentation alignment is also able to discriminate generalization ability induced at high scale of initialization.
Figure 3: Comparing different gradient-based measures for the simple case of having two samples from thesame class where a = V'ι and b = V'2.
Figure 4: Results when using ReLU activation in a 2-layer MLP with Softmax cross-entropy loss functionwhen trained on CIFAR-10 dataset. Similar to Figure 2, W1 is initialized with random normal distribution withmean zero and varying standard deviation scale σ as shown in the plots. (a) shows how the test accuracy dropsand saturates as σ is increased. (c) shows how gradients start to show misalignment as the scale is increased.
Figure 5: Plots (a) and (b) shows how representation alignment increases with generalization performance asthe architecture is improved from 2-layer MLP with ReLU activation to a ConvNet architecture (exact details inthe appendix) on CIFAR-10 dataset. We see further increase when even bigger and widely used architectureslike ResNet-50 and DenseNet-121 are employed on the same task. Note that we keep all the hyperparameterssame across architectures in this experiment. Plots (c) and (d) elucidates the drop in representation alignmentwhen the labels are shuffled in the case of 2-layer MLP.
Figure 6: Results when using sin activation function on CIFAR-100 dataset.
Figure 7: Results when using sin activation function on SVHN dataset.
Figure 8: Results when using sin activation function on CIFAR-10 dataset with scale of initialization set to 1.0and varying learning rates for the first layer as shown in the plot.
Figure 9: Results when using sin activation function on CIFAR-10 dataset with scale of initialization set to1e-2 and varying learning rates for the first layer as shown in the plot.
Figure 10: Results when using sin activation function on CIFAR-10 dataset when the first layer weights arefrozen and varying scale of initialization as shown in the plot.
Figure 11: Results when using sin activation function on CIFAR-10 dataset with 4-layer MLP.
Figure 12: Additional plots when using ReLU activation function with softmax cross-entropy on CIFAR-10dataset.
Figure 13: Results when using ReLU activation function with softmax cross-entropy on CIFAR-100 dataset.
Figure 14: Results when using ReLU activation function with softmax cross-entropy on SVHN dataset.
Figure 15: Results when using ReLU activation function with hinge loss on CIFAR-10 dataset.
Figure 16: Results when using ReLU activation function with hinge loss on SVHN dataset.
Figure 17: Results when using ReLU activation function with hinge loss on CIFAR-100 dataset.
Figure 18: Results when using ReLU activation function on CIFAR-10 dataset with 4-layer MLP.
Figure 19: Results when using ReLU activation function with squared loss on CIFAR-10 dataset.
Figure 20: Results when using ReLU activation function with squared loss on CIFAR-100 dataset.
Figure 21: Results when using ReLU activation function with squared loss on SVHN dataset.
Figure 22: Results when using linear activation function with softmax cross entropy loss on CIFAR-10 dataset.
Figure 23:	Results when using linear activation function with softmax cross entropy loss on CIFAR-100 dataset.
Figure 24:	Results when using linear activation function with softmax cross entropy loss on SVHN dataset.
Figure 25:	Results when using Sigmoid activation function on CIFAR10 dataset.
Figure 26: Results when using Sigmoid activation function on CIFAR100 dataset.
Figure 27: Results when using Sigmoid activation function on SVHN dataset.
