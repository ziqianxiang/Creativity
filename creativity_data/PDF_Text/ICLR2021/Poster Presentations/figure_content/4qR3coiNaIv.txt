Figure 1: Overview. AVRIL is a framework for BIRL that works through an approximation in thevariational Bayesian framework, considering the reward to be a latent representation of behaviour.
Figure 2: Graphical modelfor Bayesian IRLbehaviour. This approach has its advantages, in both meaningful interpretation of the latent reward(which is non-existent in adversarial methods), and that we forgo the practical difficulties of alter-nating min-max optimisation (Kodali et al., 2017) while maintaining a generative view of the policy.
Figure 3: Control environments performance. We plot the average returns received by the poli-cies when deployed live in the environment against the number of trajectories seen during training.
Figure 5: Gridworld example. Scaled heat-maps of: the ground truth reward; the relative stateoccupancy of the expert demonstrations; the reward posterior mean; and reward standard deviation.
Figure 4: (Top) A state-action re-ward is learnt and plotted for anotherwise average patient as theirblood oxygen level changes. (Bot-tom) The associated weights givena state-only reward as a linearfunction of the state-space.
