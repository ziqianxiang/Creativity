Figure 1: Neither SMP nor NerveNet leverage the agent’s morphological information, or the posi-tive effects are outweighted by their negative effect on message passing.
Figure 2: Amorpheus architecture. Lines with squares at the end denote concatenation. Arrowsgoing separately through encoder and decoder denote that rows of the input matrix are processedindependently as batch elements. Dashed arrows denote message-passing in a transformer block.
Figure 3: Amorpheus consistently outperforms SMP on MTRL benchmarks from Huang et al.
Figure 4: MTRL performance onWalkers (Wang et al., 2018).
Figure 5: State-dependent masks of AMORPHEUS (3rd attentionlayer) within a Walker-7 rollout.
Figure 6: In the first attention layer of a Walker-7 rollout, nodes attend to an upper leg (column-wise mask sum 〜3) When the leg is closer to the ground (normalized angle 〜0).
Figure 8: Removing the return limit slightly de-teriorates the performance of NerveNet on Walk-ers.
Figure 7: Smaller learning rate make SMP toyield better results on Walker++.
Figure 9: Examples of graph topologies used in the structure ablation experiments.
Figure 10: Walker++ masks for the 3 attention layers on Walker-7 at the beginning of training.
Figure 11: Walker++ masks for the 3 attention layers on Walker-7 after 2.5 mil frames.
Figure 12: Walker++ masks for the 3 attention layers on Walker-7 at the end of training.
Figure 13:	Absolutive cumulative change in the attention masks for three different models onWalker-7.
Figure 14:	Residual connection ablation experiment.
