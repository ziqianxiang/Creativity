Figure 1: Noise norm vs gradient normof ResNet20 at initialization. The noisevariance is chosen such that SGD sat-isfies (5, 10-5)-DP after 90 epochs inAbadi et al. (2016).
Figure 2: Stable rank ∣∣ ∙ kF/k∙∣∣2 (TroPPet al., 2015) of batch gradient matrix ofgiven grouPs (with p Parameters). Thesetting is ResNet20 on CIFAR-10. Thestable rank is small throughout training.
Figure 3: Overview of the proposed GEP approach. 1) We estimate an anchor subspace on somenon-sensitive data; 2) We project the private gradients into the anchor subspace, producing low-dimensional embeddings and residual gradients; 3) We perturb the gradient embedding and residualgradient separately to guarantee differential privacy. The auxiliary data are only required to sharesimilar features as the private data. In our experiments, we use 2000 images from ImageNet asauxiliary data for MNIST, SVHN, and CIFAR-10 datasets.
Figure 4: Relative projection error (∣∣ nr ∣∣ / ∣∣gk) of the secondstage in ResNet20. The number of anchor gradients is 2000. Thedimension of anchor subspace is k. The learning rate is decayedby 10 at epoch 30. The left plot uses random samples fromImageNet. The right plot uses random samples from test data.
Figure 5: Stable rank of theresidual gradient matrix versusoriginal gradient matrix. Thegradients are computed on fullbatch data for the first stage inResNet20. The dimension of an-chor subspace is k = 1000.
Figure 6:	Test accuracy when varying the dimension of anchor subspace. GEP significantly outper-forms B-GEP for all k. Moreover, the performance of GEP is not that sensitive to k .
Figure 7:	Projection error rate of random basis vectors. The dimension of subspace is denoted by k .
