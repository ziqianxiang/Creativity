Figure 1: Left: Given a single image of a novel visual concept (e.g., a gadwall), a person can generalizein various ways, including imagining what this gadwall would look like from different viewpoints (top) andrecognizing new gadwall instances (bottom). Right: Inspired by this, we introduce a general feedback-basedbowtie network that facilitates the interaction and cooperation between a generative module and a discriminativemodule, thus simultaneously addressing few-shot recognition and novel-view synthesis in the low-data regime.
Figure 2: Architecture of our feedback-based bowtie network. The whole network consists of a view synthesismodule and a recognition module, which are linked through feedback connections in a bowtie fashion.
Figure 4: Ablation on Î»cat. Categorical loss trades off the performance between view synthesis and recognition.
Figure A:	Architecture of ResBlock used in the view synthesis module. The default kernel size is 3 and the strideis 1.
Figure B:	Architecture of the discriminator in the view synthesis module. The kernel size of all the convolutionlayers is 3 and the stride is 2.
Figure C:	Architecture of the generator in the view synthesis module. Res-Up module is a combination of aResBlock and an upsampling layer.
Figure D: Qualitative comparison of synthesized images from multiple viewpoints between our FBNet andstate-of-the-art HoloGAN on the NAB and DOG datasets. Images in the same row/column are from the sameviewpoint/object. The overall quality of the synthesized images for both methods indicates the general difficultyof the task, due to weak supervision and lack of data. However, our FBNet still captures the shape and attributeswell, even though the data is scarcely limited. This shows the strong adaptability of FBNet for few-shot learning.
Figure E: Synthesized 128-resolution images by our FBNet on the CUB dataset.
