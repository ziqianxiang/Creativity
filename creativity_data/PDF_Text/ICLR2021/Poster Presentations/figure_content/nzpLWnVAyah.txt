Figure 1: Our proposed fine-tuning strategy leads to very stable results with very concentrated development setperformance over 25 different random seeds across all three datasets on BERT. In particular, we significantlyoutperform the recently proposed approach of Lee et al. (2020) in terms of fine-tuning stability.
Figure 2: Language modeling perplexity for three failed (a) and successful (b) fine-tuning runs of BERT on RTEwhere we replace the weights of the top-k layers with their pre-trained values. We can observe that it is oftensufficient to reset around 10 layers out of 24 to recover back the language modeling abilities of the pre-trainedmodel. (c) shows the average training loss and development accuracy (±1std) for 10 failed fine-tuning runs onRTE. Failed fine-tuning runs lead to a trivial training loss suggesting an optimization problem.
Figure 3: Development set results on down-sampled MRPC, CoLA, and QNLI using the default fine-tuningscheme of BERT (Devlin et al., 2019). The leftmost boxplot in each sub-figure shows the development accuracywhen training on the full training set.
Figure 4:	Gradient norms (plotted on a logarithmic scale) of different layers on RTE for a failed and successfulrun of BERT fine-tuning. We observe that the failed run is characterized by vanishing gradients in the bottomlayers of the network. Additional plots for other weight matrices can be found in the Appendix.
Figure 5:	Box plots showing the fine-tuning performance of (a) BERT, (b) RoBERTa, (c) ALBERT for differentlearning rates α with and without bias correction (BC) on RTE. For BERT and ALBERT, having bias correctionleads to more stable results and allows to train using larger learning rates. For RoBERTa, the effect is lesspronounced but still visible.
Figure 7: 2D loss surfaces in the subspace spanned by δ1 = θf - θp and δ2 = θs - θp on RTE, MRPC, andCoLA. θp, θf, θs denote the parameters of the pre-trained, failed, and successfully trained model, respectively.
Figure 6: The bias correction term ofADAM (β1 = 0.9 and β2 = 0.999).
Figure 8: Development set accuracy for multiple fine-tuning runs on RTE. The models for (a) are trained with10 different seeds, and models for (b) are taken at the end of the training, and trained with different seeds andhyperparameters.
Figure 9:	Full ablation of fine-tuning BERT on RTE. For each setting, we vary only the number of training steps,learning rate, and usage of bias correction (BC). All other hyperparameters are unchanged. We fine-tune 25models for each setting. ? shows the setting which we recommend as a new baseline fine-tuning strategy.
Figure 10:	Gradient norms (plotted on a logarithmic scale) of additional weight matrices of BERT fine-tuned onRTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a single failedand single successful, respectively.
Figure 11:	Gradient norms (plotted on a logarithmic scale) of additional weight matrices of RoBERTa fine-tunedon RTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a singlefailed and single successful, respectively.
Figure 12:	Gradient norms (plotted on a logarithmic scale) of additional weight matrices of ALBERT fine-tunedon RTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a singlefailed and single successful, respectively.
Figure 13:	2D gradient norm surfaces in the subspace spanned by δ1 = θf - θp and δ2 = θs - θp for BERTfine-tuned on RTE, MRPC and CoLA. θp , θf , θs denote the parameters of the pre-trained, failed, and successfullytrained model, respectively.
Figure 14:	The test accuracy and training loss of (a) 10 successful runs with our fine-tuning scheme and (b) 10failed runs with fine-tuning scheme Devlin on RTE. Solid line shows the mean, error bars show ±1std.
