Figure 1: Evaluation of different econas proxies on NAS-Bench-201 CIFAR-10. FLOPS andruntime are normalized to the FLOPS/runtime of a single baseline (full training) epoch.
Figure 2: Correlation of validation accuracy to final test accuracy during the first 12 epochs oftraining for three datasets on the NAS-Bench-201 search space. Zero-cost and EcoNAS proxies arealso labeled for comparison.
Figure 3: Performance of zero-cost metrics on PyTorchCV models (averaged over 5 seeds).
Figure 4: Search speedup with the synflow zero-cost proxy on NAS-Bench-201 CIFAR-100.
Figure 5: Search speedup with the synflow zero-cost proxy on NAS-Bench-ASR TIMIT.
Figure 6: Higher batch sizes when training econas proxies have diminishing returns in terms ofmeasured speedup. This measurement is done for 10 randomly-sampled NAS-Bench-201 modelson the CIFAR-10 dataset.
Figure 7: Search speedup with the synflow zero-cost proxy on NAS-Bench-101 CIFAR-10.
Figure 8: Evaluation of all zero-cost proxies on different datasets and search algorithms: randomsearch (RAND) and aging evolution (AE). RAND benefits greatly from a strong metric (such assynflow) but may deteriorate with a weaker metric as shown in the plot. However, AE benefitswhen a strong metric is used and is resilient to weaker metrics as well - it is able to recover andachieve the top accuracy in most cases.
