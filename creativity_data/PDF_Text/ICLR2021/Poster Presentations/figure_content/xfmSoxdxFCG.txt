Figure 1: Network architecture. Several groups of PNs corresponding to different modalities send their activ-ities to the layer of KCs, which are inhibited through the reciprocal connections to the APL neuron.
Figure 2: The encoding method. The input vector consists of two blocks separated by the (thick) blue line.
Figure 3: Panel A: average cosine similarity between the points within the cluster vs. maximum cosine similar-ity (minimal distance) to a point from the closest cluster. Solid lines correspond to mean±std for the individualclusters. Numbers next to GloVe in the legend correspond to the number of largest elements in the word vectorthat are mapped to 0 under the naive discretization procedure. Panel B: an example of a cluster generated bythe agglomerative clustering for our method, the integer number associated with each node corresponds to thenumber of daughter leaves in that cluster. The root node corresponds to “interchange (42)”.
Figure 4: For every word (highlighted in green) in context (left), 10 nearest neighbor words in the binaryhashing space are shown (right). Context allows the algorithm to disambiguate the target word,s meaning.
Figure 5: Spearman’s correlationon word similarity datasets (see Sec-tion 3.1) vs. training time. Each pointis one epoch.
Figure 6: Examples of three queries and corresponding word cloud visualization for top four activated KCs(by each query).
