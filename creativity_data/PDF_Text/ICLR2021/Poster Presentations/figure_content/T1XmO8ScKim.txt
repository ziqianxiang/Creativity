Figure 1: The PNCNN operating on SuperPixel-MNIST images shown on the left. The mean andelementwise uncertainty of the Gaussian process feature maps are shown as they are transformedthrough the network by the convolution layers. Observation points shown as green dots in Ïƒ(x).
Figure 2: Left: Qualitative convergence of the mean and uncertainty of the first 3 channels of thefeature maps is shown in RGB color as the input test resolution is increased. Middle: Medianpredicted uncertainties over spatial locations as a function of the test resolution. Right: Using thepredictions of the highest resolution model as ground truth, the distribution of prediction residualsis shown in a Q-Q plot for each layer (shifted horizontally for clarity) with the black lines showingthe theoretical relationship, and the overall distribution histogram is shown on the right.
Figure 3: Zero shot generalization to other resolutions: Having trained on MNIST at a given trainingresolution shown by the color, we evaluate the performance of an PNCNN, PNCNN without uncer-tainty, and an ordinary CNN on varying test resolutions. Notably, PNCNN with uncertainty is themost robust.
