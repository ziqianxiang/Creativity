Figure 1: An example of DNN training under the bit representation with precision n = 3.
Figure 2: Quantization schemes achieved with or without layer-wise regularization reweigh-ing. The compression rate and the accuracy after finetuning are listed in the legend.
Figure 3: Layer-wise precision comparison of the quantization schemes achieved underdifferent regularization strengths.
Figure 4: Range of testing accuracy and bit reduction rate achieved from 5 repeated runswith different random seeds. Solid line links the average performance, error bar marks themaximal and minimal performance achieved with each set of hyperparameters.
Figure 5: Quantization schemes achieved with or without layer-wise regularization reweigh-ing. The compression rate and the accuracy after finetuning are listed in the legend.
Figure 6: Quantization schemes achieved with or without layer-wise regularization reweigh-ing. The compression rate and the accuracy after finetuning are listed in the legend.
Figure 7: Layer-Wise precision comparison betWeen the quantization schemes achievedWith BSQ and the scheme achieved With HAWQ (Dong et al., 2019) on the ResNet-20model.
Figure 8: Layer-wise precision comparison of the quantization schemes achieved underdifferent regularization strengths with 2-bit activation.
Figure 9: Layer-wise precision comparison of the quantization schemes achieved underdifferent regularization strengths with 3-bit activation.
