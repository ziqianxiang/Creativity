Figure 1: Comparison of decision boundaries. The black and green circles represent training samplesfrom classes 1 and 2, respectively. The dashed black line indicates the decision boundary betweenclasses 1 and 2 and thus satisfies P (c = 1 | x) = (c = 2 | x) = 0.5. The dashed blue and red linesrepresent the boundaries between the posterior probabilities of the mixture components.
Figure 3: Snapshots of the training process of SDGM. The meanings of lines and circles are thesame as Figure 1. There are three components for each class in the initial stage of learning. As thetraining progresses, one of the components for each class becomes small gradually and is finallyremoved.
Figure 4: Prior for each Weight wcmh . Bymaximizing the evidence term of the poste-rior of w, the precision of the prior Î±cmhachieves infinity if the corresponding Weightwcmh is redundant.
Figure 5: Visualization of CNN features on MNIST (D = 2) after end-to-end learning. In thisvisualization, five convolutional layers with four max pooling layers between them and a fully con-nected layer with a two-dimensional output are used. (a) results when a fully connected layer withthe softmax function is used as the last layer. (b) when SDGM is used as the last layer instead. Thecolors red, blue, yellow, pink, green, tomato, saddlebrown, lightgreen, cyan, and black representclasses from 0 to 9, respectively. Note that the ranges of the axis are different between (a) and (b).
Figure 6:	Changes in learned class boundaries according to the number of initial components. Theblue and green markers represent the samples from class 1 and class 2, respectively. Samples in redcircles represent relevant vectors. The black lines are class boundaries where P (c | x) = 0.5.
Figure 7:	Evaluation results using synthetic data. (a) recognition error rate, (b) the number ofcomponents after training, (c) the number of nonzero weights after training, and (d) weight reductionratio. The error bars indicate the standard deviation for five trials.
