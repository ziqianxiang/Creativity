Figure 1: The process to handle visual reasoning in dynamic scenes. The trajectories of the target blue andyellow spheres are marked by the sequences of bounding boxes. Object attributes of the blue sphere and yellowsphere and the collision event are marked by blue, yellow and purple colors. Stroboscopic imaging is applied formotion visualization.
Figure 2: DCL's architecture for Counterfactual questions during inference. Given an input video and itscorresponding question and choice, we first use a program parser to parse the question and the choice intoexecutable programs. We adopt an object trajectory detector to detect trajectories of all objects. Then, theextracted objects are sent to a dynamic predictor to predict their dynamics. Next, the extracted objects are sent tothe feature extractor to extract latent representations for objects and events. Finally, we feed the parsed programsand latent representation to the symbolic executor to answer the question and optimize concept learning.
Figure 3: Examples of CLEVRER-Grounding and CLEVRER-Retrieval Datasets. The target region are markedby purple boxes and stroboscopic imaging is applied for visualization purposes. In CLEVRER-Retrieval, wemark randomly-selected positive and negative gallery videos with green and red borders, respectively.
Figure 4: Typical videos and question-answer pairs of the block tower dataset. Stroboscopic imaging is appliedfor motion visualization.
Figure 5: Typical examples of CLEVRER-Grounding datasets. The target regions are bounded with purpleboxes.
Figure 6: A exemplar query expression and 4 of its associated positive videos from CLEVRER-Retrieval dataset.
Figure 7: An typical query video and its associated positive expressions from CLEVRER-Retrieval dataset.
