Figure 1: Bias and variance.
Figure 2:	The number of regularization samples with respect to training epochs. The distillationsettings are the same as the settings in Tab. 1.
Figure 3:	Computational graph of knowledge distillation with our proposed weighted soft labels.
Figure 4: Visualization of the resemblances introduced by soft label regularizers: (a) VGG-19(Teacher) → VGG-16 (Student), (b) ResNet-50 (Teacher) → ResNet-18 (Student). And seman-tic similarity between label names: (c) LCH similarity (Pedersen et al., 2004), (d) WUP similarity(Pedersen et al., 2004). Darker areas denote larger values.
