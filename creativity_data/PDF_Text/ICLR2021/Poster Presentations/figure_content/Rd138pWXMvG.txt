Figure 1:	A simple example of our generative process describing dataset curation, with S = 3.
Figure 2:	A The standard generative model for supervised tasks, assuming no data curation. BGenerative model of data curation, where the consensus and noconsensus images are known. CGenerative model of data curation, where the noconsensus images are unknown. The underlyingimages, X are no longer observed. Instead, we observe Z, where Z = X if consensus is reached(H = Y2 = •…=YS) and Z = None otherwise.
Figure 3: Illustrative example of artificial clustering induced in the dataset by rejection of ambiguousno-consensus points. The input points for “cat” and “dog” classes are generated from separate2D Gaussian distributions, and the classifier (and decision boundary) comes from the ratio of theGaussian probability density functions. For the consensus processes, we used S = 7 (we used arelatively large value to make the effects unambiguous).
Figure 4:	Tempering with data generated from a toy Gaussian process model. A Performance oftempered posteriors on GP regression. The green dashed line highlights λ = 1. The black linerepresents the mean of 20 runs (translucent blue lines). B GP classification. C GP classificationwith our model of consensus formation. We train and test using our exact log-likelihood, and weinclude knowledge of the no-consensus input datapoints in the datasets. D Data generated from ourmodel of consensus formation, with S = 4. Training uses P (Ys|X, θ) log-likelihood (excludingnoconsensus points), but testing uses the exact test-log-likelihood (Eq. 6 and 7). The red dashedline lies at λ = 1/S = 1/4. E Data generated from our model of consensus formation, with S = 4.
Figure 5:	A Test log-likelihood with tempering. B Change in test log-likelihoods from the baselineat λ = 1. We see standard cold posterior effects at low noise levels, which reverse at higher noieslevels. C, D As A, B but for test accuracy.
Figure 6:	A The number of errors (defined as labellers who disagree with the most popular category)out of around 50 labels. B The test log-likelihood for different values of λ for training with thestandard single-labels provided by CIFAR-10 and the 50 labels provided by CIFAR-10H. Note thathere we have swapped the identities of the train and test set. C As in B, but for accuracy.
Figure 7:	A Test log-likelihood with tempering for different probabilities of using a noisy label, p.
Figure 8: SGD in a ResNet18 with batchnorm removed with a tempered maximum a-posteriori loss.
Figure 9: Toy BNN model. A Test log-likelihood vs λ for different settings of S. B The optimal λ,plotted against S.
Figure 10: Modified decision tree from Willett et al. (2013) showing how we truncated the decisiontree. Images in the same box are viewed as the same class.
