Figure 1: Empirical timescale distributions and best distribution fits for an LSTM language modeltrained on natural language (PTB; blue), and the same model trained on a Markovian corpus (Markov-PTB; red). Left: An Inverse Gamma distribution is the best fit for the PTB corpus (dotted blue), whilea narrow Gaussian is the best fit for the Markovian corpus (dotted red). Right: We determined thebest fits by varying distribution parameters (a for the Inverse Gamma, μ for the narrow Gaussian)and computing the Kolmogorov-Smirnov statistic to measure the difference between the empiricaland theoretical distributions (lower is better). The LSTM trained on natural language (blue) is best fitby the Inverse Gamma distribution with α = 1.4, while the LSTM trained on a Markovian corpus(red) is best fit by the narrow Gaussian distribution with μ = 0.5.
Figure 2: The left plot shows the performance of the baseline (blue) and multi-timescale (red) modelson the Dyck-2 grammar as a function of the maximum timescale in a sequence (20 repetitions,standard deviation). A correctly predicted sequence is considered to have all the outputs correct.
Figure 3: Information routing across different units of the multi-timescale LSTM for PTB dataset.
Figure 4: Estimated timescale is highly correlated with assigned timescale, shown for all 1150 unitsin LSTM layer 2 of the multi-timescale language model.
Figure 5:	Forget gate values of LSTM units for a test sentence from the PTB dataset. Units are sortedtop to bottom by increasing mean forget gate value, and averaged in groups of 10 units to enablevisualization. Figure 5b also shows average assigned timescale (rounded off) of the units.
Figure 6:	Change in cell state of all the three layers for both the baseline and Multi-timescale languagemodels in word ablation experiment. A curve with a steep slope indicates that cell state differencedecays quickly over time, suggesting that the LSTM layer retains information of shorter timescales.
Figure 7:	Change in cell states of 100-unit groups having different average timescale of layer 2 inMulti-timescale model in word ablation experiment for PTB dataset. As the assigned timescale to thegroup decreases the slope of the curve decreases indicating retained information of smaller timescale.
Figure 8:	Change in cell states of 100-unit groups having different average timescale of layer 2 inMulti-timescale model in word ablation experiment for WikiText-2 dataset.
Figure 9: Assigned timescale to LSTM units of layer2 of multi-timescale language model correspond-ing to different shape parameter α.
Figure 10: Performance of multi-timescale models for different shape parameters α on both PTB andWikiText-2 dataset.
Figure 11: Information routing across different units of the multi-timescale LSTM for WikiText-2dataset.
Figure 12: Information routing across different units of the multi-timescale LSTM trained with alegacy version of pytorch on the Penn Treebank dataset.
