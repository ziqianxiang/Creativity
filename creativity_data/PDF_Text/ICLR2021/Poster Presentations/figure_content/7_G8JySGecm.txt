Figure 1: Example of observation and actions from Zork 1To overcome the bottleneck, the agent should be able to infer that ‘take lantern’ is worthyenough to be chosen in preference to other actions even though it gives no immediate reward.
Figure 3: Performance of MC-LAVEplanning on ZORK 1 for varying δ. Theresults are averaged over 100 trials,and error bars indicate the standard er-rors.
Figure 5: Q-NetworkFigure 4: Policy networkB Experiments detailsHyperparameters	Zork1	Deephome	Ludicorp	PENTARI	Detective	Library	Balances	Temple	Ztuuλ (discount factor)	0.95	0.95	0.95	0.95	0.95	0.95	0.95	0.95	0.95# of max iterations	4	4	4	4	4	4	4	4	4δ (neighborhood threshold)	-03-	0.3	0.3	03	0.3	0.3	0.3	0.3	0.3Max search depth	10	10	20	10	10	15	15	10	15Max episode length	35	35	50	35	50	35	35	35	35# of simulations per action	50	50	50	50	50	50	50	50	50# of planning workers	25	25	25	25	25	25	25	25	25cLAVE	1.0	1.0	2.0	1.0	0.1	1.0	1.0	1.0	1.0cPUCT	50	20	50	50	200	20	50	50	50Table 4: Configurations of MC-LAVE-RL used in our experimental results. Hyperparameters inthe upside of the table were globally adapted in the planning-learning framework and the otherhyperparameters are used only in the MCTS planning phase.
Figure 4: Policy networkB Experiments detailsHyperparameters	Zork1	Deephome	Ludicorp	PENTARI	Detective	Library	Balances	Temple	Ztuuλ (discount factor)	0.95	0.95	0.95	0.95	0.95	0.95	0.95	0.95	0.95# of max iterations	4	4	4	4	4	4	4	4	4δ (neighborhood threshold)	-03-	0.3	0.3	03	0.3	0.3	0.3	0.3	0.3Max search depth	10	10	20	10	10	15	15	10	15Max episode length	35	35	50	35	50	35	35	35	35# of simulations per action	50	50	50	50	50	50	50	50	50# of planning workers	25	25	25	25	25	25	25	25	25cLAVE	1.0	1.0	2.0	1.0	0.1	1.0	1.0	1.0	1.0cPUCT	50	20	50	50	200	20	50	50	50Table 4: Configurations of MC-LAVE-RL used in our experimental results. Hyperparameters inthe upside of the table were globally adapted in the planning-learning framework and the otherhyperparameters are used only in the MCTS planning phase.
Figure 6:	Illustrative examples on Zork 1 that represents the behavior of learned policy by baselinesand our algorithms.
Figure 7:	(Left) The t-SNE visualization for the semantic neighborhood of language actions definedby the pre-trained word embedding. The language-actions sharing similar meanings are located atsimilar points in the embedding space. (Right) The t-SNE visualization for the neighborhood oflanguage actions defined by the word embedding fine-tuned by Qφ training. The training objectiveof Qφ does not account for the semantics of language actions but only considers rewards, thus theresulting embedding loses the linguistic semantics of words.
