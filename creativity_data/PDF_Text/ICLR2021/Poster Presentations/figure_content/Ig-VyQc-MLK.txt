Figure 1:	Weights remaining at each training step for methods that reach accuracy within one Per-centage point of ResNet-50 on ImageNet. Dashed line is a result that is achieved retroactively.
Figure 2:	Comparisons in the SNIP, GraSP, and SynFlow papers. Does not include MNIST. SNIPlacks baselines beyond MNIST. GraSP includes random, LTR, and other methods; it lacks magnitudeat init and ablations. SynFlow has other methods at init but lacks baselines or ablations.
Figure 3: Accuracy of early pruning methods when pruning at initialization to various sparsities.
Figure 4: Ablations on subnetworks found by applying magnitude pruning, SNIP, GraSP, and Syn-Flow at initialization. (We ran limited ablations on ResNet-50 due to resource limitations.)Magnitude pruning. Since the magnitude pruning masks can be shuffled within each layer, its prun-ing decisions are sensitive only to the per-layer initialization distributions. These distributions arechosen using He initialization: normal with a per-layer variance determined by the fan-in or fan-out(He et al., 2015). These variances alone, then, are sufficient information to attain the performanceof magnitude pruning—performance often competitive with SNIP, GraSP, and SynFlow. Withoutthis information, magnitude pruning performs worse (purple line): if each layer is initialized withvariance 1, it will prune all layers by the same fraction no differently than random pruning. Thisdoes not affect SNIP,5 GraSP, or SynFlow, showing a previously unknown benefit of these methods:they maintain accuracy in a case where the initialization is not informative for pruning in this way.
Figure 5: Percent of neurons (Conv. channels) with sparsity ≥ s% at the highest matching sparsity.
Figure 6: Accuracy of early pruning methods when pruning at the iteration on the x-axis. SParSitieSare the highest matching sparsities. LTR prunes after training and initializes to the weights from theSpecified iteration. Vertical lineS are iterationS where the learning rate dropS by 10x.
Figure 7: SynfloW replication experiments.
Figure 8: Accuracy of the baseline methods.
Figure 10: Accuracy of early pruning methods and ablations when pruning at the iteration on thex-axis. Sparsities are the highest matching sparsities. Vertical lines are iterations where the learningrate drops by 10x.
Figure 12: A comparison of the ablations for SNIP (from the main body, Figure 4) and for iterativeSNIP. (ResNet-50 is not included due to computational limitations.)29Published as a conference paper at ICLR 2021H Variants of GraSPIn Figure 13, we show three variants of GraSP: pruning weights with the highest scores (the ver-sion of GraSP from Wang et al.), pruning weights with the lowest scores (the inversion experimentfrom Section 5), and pruning weights with the lowest magnitude GraSP scores (our proposal foran improvement to GraSP as shown in Figure 14). This figure is intended to make the comparisonbetween these variants clearer; figure 4 is too crowded for these distinctions to be easily visible.
Figure 13: Accuracy of three different variants of GraSP30Published as a conference paper at ICLR 2021I Comparisons to Improved GraSP and SynFlowIn Figure 14 below, we compare the early pruning methods with our improvements to GraSP andSynFlow. This figure is identical to Figure 3, except that we modify GraSP to prune the weights withthe lowest-magnitude GraSP scores (Appendix H) and we modify SynFlow to randomly shuffle theper-layer pruning masks after pruning (Section 5).
Figure 14: The best variants of pruning methods at initialization from our experiments. GraSP andSynFlow have been modified as described in Section 5.
Figure 15: Per-layer sparsities produced by each pruning method at the highest matching sparsity.
Figure 16: For each actual sparsity (x-axis), the ratio of the effective parameter-count of the ran-domly shuffled ablation to the effective parameter-count of the network pruned with the unmodifiedpruning method. Note that we have zoomed into the range of ratios between 0.99 and 1.05.
