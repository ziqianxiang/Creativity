Figure 1: Zero-shot synthesis performance of our method. (a), (b), and (c) are from datasets,respectively, iLab-20M, RaFD, and Fonts. Bottom: training images (attributes are known). Top:Test image (attributes are a query). Training images go through an encoder, their latent featuresget combined, passed into a decoder, to synthesize the requested image. Sec. 4.2 shows how wedisentangle the latent space, with explicit latent feature swap during training.
Figure 2: (a) Samples from our proposed Fonts dataset, shown in groups. In each group, we vary oneattribute but keep others the same. (b) (Sub-)multigraph of our Fonts dataset. Each edge connect twoexamples sharing an attribute. Sets S1 and S2 cover sample i.
Figure 3: Architecture of GZS-Net, consisting of an encoder E: maps sample onto latent vector, anda decoder D: maps latent vector onto sample. The latent space is pre-partitioned among the attributeclasses (3 shown: identity, pose, background). (a, left) considered examples: a center image (x, redborder) and 3 images sharing one attribute with it, as well as a no overlap image sharing no attributes(x, black border). (a, right) standard reconstruction loss, applied for all images. (b) One-overlapattribute swap: Two images with identical values for one attribute should be reconstructed intonearly the original images when the latent representations for that attribute are swapped (”no-op”swap; left: identity; middle: pose; right: background). (c) Cycle swap: given any example pair,we randomly pick an attribute class j . We encode both images, swap representations of j , decode,re-encode, swap on j again (to reverse the first swap), and decode to recover the inputs. Thisunsupervised cycle enforces that double-swap on j does not destroy information for other attributes.
Figure 4: Zero-shot synthesis performance compare on Fonts. 7-11 and 18-22 columns are inputgroup images and we want to combine the specific attribute of them to synthesize an new images. 1-5and 12-16 columns are synthesized images use auto-encoder + Exhaustive Swap (AE+ES), β-VAE+ Exhaustive Swap (β-VAE+ES), β-TCVAE + Exhaustive Swap (β-TCVAE+ES), auto-encoder +Directly Supervision (AE+DS) and GZS-Net respectively. 6 and 17 columns are ground truth (GT)AE+ES 喘 p-1Sae AE÷DS GZS-Net GT ； Size IX 黑 StyIeLetter•	The fourth baseline, AE+DS, is an auto-encoder where its latent space is partitioned and eachpartition receives direct supervision from one attribute. Further details are in the Appendix.
Figure 5: Zero-shot synthesis qualitative performance on ilab-20M. Columns left of the dashed lineare output by methods: the first five are baselines, followed by three GZS networks. The baselines are:(1) is an auto-encoder with direct supervision (AE+DS); (2, 3, 4) are three GAN baselines changingonly one attribute; (5) is starGAN changing two attributes. Then, first two columns by GZS-Net areablation experiments: trained with part of the objective function, and the third column is output by aGZS-Net trained with all terms of the objective. starGAN of (Choi et al., 2018) receives one inputimage and edit information (explanation in Appendix Section B.4). ELEGANT uses identity andbackground images (in Appendix Section B.3, uses all input images). Others use all three inputs.
Figure 6: GZS-Net zero-shot synthesis performance on RaFD. 1-2 and 6-7 columns are the synthe-sized novel images using auto-encoder + Directly Supervision (AE+DS) and GZS-Net respectively.
Figure 7: (a) Dataset details for training object recognition task, where the x-axis represents differentidentities (1004) and the y-axis represents the backgrounds (111) and poses (6) each purple andbrown pixel means our dataset covers the specific combination of attributes. (b) object recognitionaccuracy (%) on 37469 test examples, after training on (augmented) datasets.
Figure 8: Samples from the Fonts dataset, a new parametric dataset we created by rendering charactersunder 5 distinct attributes. In each row, we keep all attributes the same but vary one.
Figure 10: Zero-shot synthesis for ELEGANT, investigating the modification of pose and backgroundattributes on an identity image. Details are in Section B.3.
Figure 11: Training performance of ELEGANT. The left 2 columns (A and B) are input image. Thefollowing 4 columns are the generated synthesized images: A’ and B’ are reconstructions of theinput (acceptable quality, suggesting convergence of generator), whereas C and D are the result ofswapping features before the generator: C (/ D) uses the latent features of A (/ B) except by swappingbackground with B (/ A). All (C, D, A’, B’) share the same generator.
