Figure 1: High-level overview of the different methods developed in this work. Adopting thelognormal observation of the neural gradients, we suggest three different measures we can analyticallyoptimize: two for pruning and one for quantization. These analytical measures are used in fourdifferent schemes.
Figure 2: Identifying the distribution of neural gradients (normal vs. lognormal): (a) probabilitydensity function of gradient magnitudes; (b) the same histogram on a logarithmic scale — noticethat the shape looks very much like the symmetrical shape of the regular normal distribution; (c)quantiles of gradient magnitudes against the quantiles of a normal distribution; and (d) quantiles ofgradient magnitudes against the quantiles of a lognormal distribution; Additional layers’ distributionin Fig. A.2. In order to emphasize the difference between neural gradients and weights gradients weshow the quantiles of weights gradient in Fig. A.23Lognormal4o0o© s。=UenbseαTheoretical quantiles3Published as a conference paper at ICLR 2021Table 1: Mean ± std (p-value) over all layers over time of KS test on different models and datasetsfor normal and lognormal distribution. Notice the lognormal distribution gets the higher fit across allmodels. We compare to additional distributions such as Laplace, uniform, Cauchy and loglaplace inAppendix A.1 Also, in Table A.2 we show there is a good match to the lognormal through training(not only on average), and in Fig. A.3 we show this holds also for specific layers over time .
Figure 3: (a) Expected relative error as a function of n2 for FP8 with a lognormal distribution. Thesimulation is in good agreement with the analytical results stated by Eq. (4). (b) Ideal bit allocation forthe exponent (E*) as a function of σ, using Eq.(4). Notice that σ varies between datasets, imposingdifferent optimal allocations. (c) Ideal bit allocation assuming a normal distribution, which leadsto wrong FP formats, e.g., the number of bits assigned to the exponent decreases when σ (dynamicrange) increases.
Figure 4: Effect of stochastic pruning on a lognormally distributed tensor. The threshold can be foundfrom Eq. (6) according to the gradients’ distribution. S is the ratio of values mapped to 0, notice thata large fraction of values, all values α ∙ ε ≤ X ≤ α are mapped to ±α — two values that can have aspecial encoding, thus reducing the tensors’ memory footprint — details in Appendix A.10.
Figure 5: (Left) Comparison of the homogeneous and heterogeneous stochastic pruning methodsagainst pruning with SAW (Aamir Raihan & Aamodt, 2020), which uses the ”top-k” method andATP (Ye et al., 2019), which also uses stochastic pruning, but assumes the neural gradients arenormally distributed. These are compared on 3 different architectures (ResNet18, VGG16 andDenseNet121). Our proposed methods achieve higher sparsity while maintaining baseline validationaccuracy. (Right) Comparison of the required and achieved sparsity of our method, ATP (Ye et al.,2019) and applying ”top-k” followed by stochastic pruning for ResNet-18 (additional models inFig. A.24). The validation accuracy for our method is never less than the accuracy of the othermethods for the same achieved sparsity. Notice the large deviation in the other methods between theachieved and required sparsity. This emphasizes the importance of using the correct distribution inorder to both enjoy the improved performance of stochastic pruning over regular ”top-k” and maintainthe ability to fully control the achieved sparsity. Additional details and results are in Appendix A.12----ReSNet 18——VGG16——DenseNetlZl•	Ours Homogenous (stochastic)■	Ours Hetrogenous (stochastic)× SAW (deterministic)▼	ATP (stochastic)Pruning. We can use stochastic pruning to prune the neural gradients during training precisely to
