Figure 3: Mean (solid lines)and range (shaded region)of training loss during fine-tuning BERT, across 50 ran-dom trials. Bias correctionspeeds up convergence andshrinks the range of trainingloss.
Figure 1:	Bias in the Adam up-date as a function of training iter-ations. Vertical lines indicate thetypical number of iterations usedto fine-tune BERT on four smalldatasets and one large dataset(MNLI). Small datasets use feweriterations and are most affected.
Figure 2:	Performance dis-tribution box plot across 50random trials and the fourdatasets with and withoutAdam bias correction. Biascorrection reduces the vari-ance of fine-tuning results bya large margin.
Figure 4: Expected test performance (solid lines) with standard deviation (shaded region) over thenumber of random trials allocated for fine-tuning BERT. With bias correction, we reliably achievegood results with few (i.e., 5 or 10) random trials.
Figure 5: Validation performance distribution ofre-initializing different number of layers of theBERT model.
Figure 6: Mean (solid lines) and Range (shadedregion) of training loss during fine-tuning BERT,across 20 random trials. Re-init leads to fasterconvergence and shrinks the range.
Figure 7: L2 distance to the initial parameters during fine-tuning BERT on RTE. Re-init reduces theamount of change in the weights of top Transformer blocks. However, re-initializing too many layerscauses a larger change in the bottom Transformer blocks.
Figure 8: Mean (solid lines) and range (shaded region) of validation performance trained withdifferent number of iterations, across eight random trials.
Figure 9:	Validation accuracy on RTE with controlled random seeds. The min, mean, and max valuesof controlling one of the random seeds are also included. Re-init 5 usually improves the validationaccuracy.
Figure 10:	The standard deviation of the validation accuracy on RTE with controlled random seeds.
Figure 11:	Mean (solid lines) and range (shaded region) of training loss during fine-tuning BERT,across 50 random trials. Bias correction speeds up convergence and reduces the range of the trainingloss.
Figure 12:	Expected validation performance (solid lines) with standard deviation (shaded region) overthe number of random trials allocated for fine-tuning BERT. With bias correction, we can reliablyachieve good results with few (i.e., 5 or 10) random trials.
Figure 13:	Validation performance distribution of re-initializing different number of layers of BERTon the downsampled datasets.
Figure 14:	Mean (solid lines) and range (shaded region) of validation performance trained withdifferent number of iterations, across eight random trials.
Figure 17: L2 distance to the initialization during fine-tuning BERT on STS-B.
Figure 18: L2 distance to the initialization during fine-tuning BERT on CoLA.
