Figure 1: Existing semantic image synthesis models heavily rely on the VGG-based perceptualloss to improve the quality of generated images. In contrast, our model can synthesize diverse andhigh-quality images while only using an adversarial loss, without any external supervision.
Figure 2: OASIS multi-modal synthesis results. The 3D noise can be sampled globally (first 2 rows),changing the whole scene, or locally (last 2 rows), partially changing the image. For the latter, wesample different noise per region, like the bed segment (in red) or arbitrary areas defined by shapes.
Figure 3: SPADE (left) vs. OASIS (right). OASIS outperforms SPADE, while being simplerand lighter: it uses only adversarial loss supervision and a single segmentation-based discriminator,without relying on heavy external networks. Furthermore, OASIS learns to synthesize multi-modaloutputs by directly re-sampling the 3D noise tensor, instead of using an image encoder as in SPADE.
Figure 4: LabelMix regularization. Real X and fake X images are mixed using a binary maskM, sampled based on the label map, resulting in LabelMix(x,x). The consistency regularizationthen minimizes the L2 distance between the logits of DLabelMix(x x) and LabelMix(Dx,Dx). In thisvisualization, black corresponds to the fake class in the N+1 segm, entation output.
Figure 5: Qualitative comparison of OASIS with other methods on ADE20K. Trained with onlyadversarial supervision, our model generates images with better perceptual quality and structure.
Figure A: VGG and adversarial G losses for SPADE and OASIS, trained with the perceptual lossA.3 Per-class IoU scoresAs seen in Table 1 in the main paper, OASIS significantly outperforms previous approaches inmIoU. We found that the improvement comes mainly from the better IoU scores achieved for less-represented semantic classes. To illustrate the gain, we report per-class IoU scores on ADE20k,COCO-Stuff and Cityscapes in Tables B, C and D. For visualization purposes, we sorted the seman-tic classes of all datasets, ordering by their pixel-wise frequency in the training images.
Figure D: Qualitative comparison of OASIS with other methods on Cityscapes.
Figure E: Qualitative comparison of OASIS with SPADE and SPADE+ using ADE20K (row 1-3),COCO-stuff (row 4-6) and Cityscapes (row 7-9).
Figure F: Images generated by OASIS on ADE20K with 256 × 256 resolution using different 3Dnoise inputs. For each label map the noise is re-sampled globally (first row) or locally in the areasmarked in red (second row). Note that the images are not stitched together but generated in singleforward passes.
Figure G:	Failure mode of OASIS. Our model generates diverse images, sometimes producingobject with outlier colors and textures. We compare to Park et al. (2019) and Liu et al. (2019).
Figure H:	Images generated by OASIS in one forward pass (no collage), with different noise vectorsfor different image regions.
Figure I:	Global latent space interpolations between images generated by OASIS for various outdoorand indoor scenes in the ADE20K dataset at resolution 256 × 256.
Figure J:	Latent space interpolations in local regions of the 3D noise, corresponding to a singlesemantic class. The noise is only changed within the restricted area. Trained on the ADE20Kdataset at resolution 256 × 256.
Figure K:	After training, the OASIS discriminator can be used to segment images. Columns 1-3 show the ground truth label map, real image, and segmentation of the discriminator. Using thepredicted label map the generator can produce multiple versions of the original image by resamplingnoise (Recreations 1-3). Note that this alleviates the need of ground truth maps during inference.
Figure L: Visual examples of LabelMix regularization. Real X and fake X images are mixed using abinary mask M, sampled based on the label map, resulting in LabelMix(x,x). The consistency reg-ularization then minimizes the distance between the logits of DLabelMix(x x) and LabelMix(Dx,Dx).
