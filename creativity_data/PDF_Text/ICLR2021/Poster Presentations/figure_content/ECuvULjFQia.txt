Figure 1: Comparison of architectures. The data generator is a Markov reward process (no actions) with anepisode length of n. x denotes the initial observation. y = Pi yi is the n-step return (no bootstrapping).
Figure 2: Visualization of the LDT framework for the special case of n = 1. Circled nodes are part of thedataset. X denotes the input, x* is the privileged data, y is the label. S is the student network with parameters θ,T is the teacher network with parameters φ.
Figure 3: (a) Data generation diagram of task A. (b) Test accuracies on task A. For every combination, we reportthe maximum test accuracy achieved, averaged over 5 random seeds. (c) Data generation diagram of task B.
Figure 4: A datapoint in the Game-of-Life task (left) and the results of the experiment (right).
Figure 5: Test losses for the MuJoCo reward prediction task. Evolution of mean squared error between predictedand true normalized n-step reward on a held-out test set. We ran each configuration with 8 different randomseeds and show the aggregated curves.
Figure 7: Ablation study resultsFigure 6: Generalization gaps (Test-set MSE minus Training-set MSE) for the different approaches and domains.)MF—Aux---LDT8Published as a conference paper at ICLR 2021(i)	Fixed untrained teacher (FT): The teacher still provides target activations for the student, but wekeep the teacher’s weights fixed to the initial random weights. Using no meta-gradient updates,the student essentially fails to learn, validating the need to train the teacher.
Figure 6: Generalization gaps (Test-set MSE minus Training-set MSE) for the different approaches and domains.)MF—Aux---LDT8Published as a conference paper at ICLR 2021(i)	Fixed untrained teacher (FT): The teacher still provides target activations for the student, but wekeep the teacher’s weights fixed to the initial random weights. Using no meta-gradient updates,the student essentially fails to learn, validating the need to train the teacher.
Figure 8: All results of ablation studies.
