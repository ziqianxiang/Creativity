Figure 1: Model-based approximatepolicy iteration. The agent updates itspolicy using targets computed via plan-ning and optionally acts via planningduring training, at test time, or both.
Figure 2: For nodes at depth d <DUCT, we select actions according topUCT (Section 2), while for nodes atdepth DUCT ≤ d < Dtree, we selectactions by sampling from πθ,t. Nodesat depth d = Dtree (and deeper) are notexpanded; instead, we stop the searchand backup using vθ,t. The search bud-get B is equal to the number of nodes inthe tree aside from the root st0 .
Figure 3: Contributions of planning to perfor-mance, where 0.0 is the performance attained bya randomly initialized policy (Table 8), and 1.0that obtained by full MuZero (Table 7). Grey barsshow medians across environments, and error barsshow 95% confidence intervals of the median.
Figure 4: Effect of design choices on the strength of the policy prior. All colored lines show mediannormalized reward across ten seeds (except Go, which uses five seeds), with error bars indicatingmin and max seeds. Rewards are normalized by the median scores in Table 9. All agents use searchfor learning and acting during training only. (a) Reward as a function of Dtree . Here, DUCT = Dtreeand the number of simulations is the same as the baseline. (b) Reward as a function of DUCT . Here,Dtree = ∞ and the number of simulations is the same as the baseline. (c) Reward as a function ofsearch budget during learning. Here, DUCT = 1 and Dtree = ∞ (except in Go, where DUCT = ∞).
Figure 5: Effect of search at evaluation as a function of the number of simulations, normalized by themedian scores in Table 10. All colored lines show medians across seeds, with error bars indicatingmin and max seeds. (a) MCTS with the learned model. (b) MCTS with the environment simulator.
Figure 6: Generalization to out-of-distribution mazes in Minipacman. All points are medians acrossseeds (normalized by the median scores in Table 10), with error bars showing min and max seeds.
Figure 7: (Left) Hero, (Right) Ms.PacmanThe Atari Learning Environment [8] is a challenging benchmark of 57 classic Atari 2600 gamesplayed from pixel observations. We evaluate on Ms. Pacman and Hero. Each observation consists ofthe 4 previous frames and action repeats is 4. Note that, as MuZero does not incorporate recurrence,this limited number of frames makes some aspects of the the environment partially observable (suchas when the ghosts turn from being edible to inedible).
Figure 8: Control Suite environment: Left Acrobot Middle Cheetah Right Humanoid.
Figure 9: Sokoban environment.
Figure 10: Contributions of the use of planning to performance. A breakdown containing the sameinformation as Figure 3 with error bars showing the maximum and minimum seeds.
Figure 11: Effect of search at evaluation as a function of the number of simulations for breadth-firstsearch (BFS) with the learned model. All colored lines show medians across seeds, with error barsindicating min and max seeds.
Figure 12: Effect of search on generalization to new in-distribution mazes in Minipacman. Allpoints are medians across seeds, with error bars showing min and max seeds. Colors indicate agentstrained on different numbers of unique mazes. The dotted lines indicate equivalent performance tothe baseline. The maps on the right give examples of the types of mazes seen during train and test.
Figure 13: Contributions of planning to the performance of the “Learn (L)”, “Learn+Data(L+D)”, and “Learn+Data+Eval (L+D+E)“ variants when training with and without an observation-reconstruction loss. Variants trained with the reconstruction loss are denoted “+obs” in the legend.
Figure 14: Learning curves for the “Learn (L)”, “Learn+Data (L+D)”, and “Learn+Data+Eval(L+D+E)“ variants when training with and without an observation-reconstruction loss. Variantstrained with the reconstruction loss are denoted “+obs” in the legend. The variants without thereconstruction loss are the same as those shown in Section D.3.
Figure 15: Learning curves for Acrobot.
Figure 16: Learning curves for Cheetah.
Figure 17: Learning curves for Humanoid.
Figure 18: Learning curves for Hero.
Figure 19: Learning curves for Ms. Pacman.
Figure 20: Learning curves for Minipacman.
Figure 21: Learning curves for Sokoban.
Figure 22: Learning curves for Go, including extra experiments on search budget and DUCT .
