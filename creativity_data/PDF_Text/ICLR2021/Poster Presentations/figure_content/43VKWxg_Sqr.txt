Figure 1: We train audiovisual (AV) exemplar autoencoders that capture personalized in-the-wildWeb speech as shown in the top-row. We then show three representative applications of ExemplarAutoencoders: (a) Our approach enables zero-shot natural voice synthesis from an Electrolarynx ora TTS used by a speech-impaired person; (b) Without any knowledge of Chinese and Hindi, ourapproach can generate Chinese and Hindi speech for John Oliver, an English-speaking late-nightshow host; and (c) We can generate audio-visual content for historical documents that could not beotherwise captured.
Figure 2: Prior approaches can be classified into two groups: (a) Zero-shot conversion learns ageneric low-dimensional embedding from a training set that is designed to be agnostic to speakeridentity. We empirically observe that such generic embeddings may struggle to capture stylisticdetails of in-the-wild speech that differs from the training set. (b) Person-specific one-on-one con-version learns a translation engine specialized to specific input and output speakers, restricting themto known input speakers at test time. (c) In this work, we combine the zero-shot nature of genericembeddings with the stylistic detail of person-specific translation systems. Simply put, given a tar-get speech with a particular style and ambient environment, we learn an autoencoder specific tothat target speech. At test time, one can translate any input speech into the target simply by passingit through the target exemplar autoencoder.
Figure 3: Autoencoders with sufficiently small bottlenecks project out-of-sample data onto the man-ifold spanned by the training set (Bengio et al., 2017), which can be easily seen in the case of linearautoencoders learned via PCA (Bishop, 2006). We observe that acoustic features (MEL spec-tograms) of words spoken by different speakers (from the VCTK dataset (Veaux et al., 2016)) easilycluster together irrespective of who said them and with what style. We combine these observationsto design a simple style transfer engine that works by training an autoencoder on a single target stylewith a MEL-spectogram reconstruction loss. Such exemplar autoencoders project out-of-sampleinput speech content onto the style-specific manifold of the target.
Figure 4: Network Architecture: (a) The voice-conversion network consists of a content encoder,and an audio decoder (denoted as green). This network serves as a person & attributes-specific auto-encoder at training time, but is able to convert speech from anyone to personalized audio for thetarget speaker at inference time. (b) The audio-video synthesis network incorporates a video decoder(denoted as yellow) into the voice-conversion system. The video decoder also regards the contentencoder as its front-end, but takes the unsampled content code as input due to time alignment. Thevideo architecture is mainly borrowed from StackGAN (Radford et al., 2016; Zhang et al., 2017),which synthesizes the video through 2 resolution-based stages.
Figure 5: Data Distribution of CelebAudio: CelebAudio contains 100 different speeches. Theaverage speech length is 23 mins. Each speech ranges from 3 mins (Hillary Clinton) to 113 mins(Neil Degrasse Tyson).
Figure 6: Visual Comparisons of Audio-to-Video Synthesis: We contrast our approach withSpeech2vid (Chung et al., 2017) (first row) and LipGAN (KR et al., 2019) (second row) using theirpublicly available codes. While Speech2Vid synthesize new sequences by morphing mouth shapes,LipGAN pastes modified lip region on original videos. This leads to artifacts (see zoom-in views atbottom row) when morphed mouth shapes are very different from the original or in case of dynamicfacial movements. Our approach (third row), however, generates full face and does not have theseartifacts.
