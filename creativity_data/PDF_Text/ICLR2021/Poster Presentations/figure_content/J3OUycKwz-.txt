Figure 1: Method for mapping processing timescales of individual units. A. Example sentences forthe model to process in the Intact Context and Random Context condition. In the Intact Contextcondition, the shared segment is preceded by an intact context from the corpus; while in the Ran-dom Context condition, this preceding context segment is replaced by randomly sampled contextsegments. B. Schematic hidden state activation of the neural network. When the model starts toprocess the shared segment preceded by different context between the two context conditions, thehidden unit activation difference (i.e. the mean absolute difference of unit activation between thetwo conditions) decreases over time with different rates. The expected decreasing pattern of acti-vation difference of a long-timescale unit and a short-timescale unit are shown schematically in thegreen and red curves, respectively.
Figure 2: Context effect measured by cell-state vector correlation at different layers in word-levelLSTM (WLSTM) and character-level LSTM (CLSTM). A. Correlation curves of the WLSTM cell-state vectors across the Intact Context condition and Random Context condition as a function ofinput token. In both models, the correlation increased as the models began to process the sharedsegment. Higher-level cell states exhibited a slower increase in correlation, compared to lower-levelcell states, indicating that the higher-levels retain more of the prior context information for longer.
Figure 3: Timescale organization in word-level LSTM (WLSTM) and character-level LSTM(CLSTM) language model. A. Absolute activation difference for each WLSTM hidden unit overtime, with units (rows) sorted by timescales. A small set of long-timescale units (top) sustain anactivation difference during shared segment processing, but most (bottom) are context-insensitiveshort-timescale units. B. Absolute activation difference for each CLSTM unit over time, with unitssorted by timescales. Similar to the WLSTM, a small set of long-timescale CLSTM hidden unitsmaintain long-range contextual information.
Figure 4: Timescale and connectivity organization in a word-level LSTM. A. Long-timescale unitsexhibited stronger projections from the hidden state at time t to the forget gate and input gate attime t + 1. B. Strength of hidden-forget gate and hidden-input gate projections from a high-degree“syntax” unit to all other units. The units receiving strong projections (|z-score|> 5) are labeled. C.
