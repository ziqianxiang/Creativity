Figure 1: The procedure of extracting data flow given a source code. The graph in the rightmost isdata flow that represents the relation of"Where-the-value-comes-from" between variables.
Figure 2: An illustration about GraphCodeBERT pre-training. The model takes source code pairedwith comment and the corresponding data flow as the input, and is pre-trained using standard maskedlanguage modeling (Devlin et al., 2018) and two structure-aware tasks. One structure-aware task is topredict where a variable is identified from (marked with orange lines) and the other is data flow edgesprediction between variables (marked with blue lines).
Figure 3: An example of the Node Alignment task.
Figure 4: MRR score on the validation dataset of Ruby for codesearch with varying length of input sequence.
Figure 5: We take a comment and a source code as the input (first row), and use GraphCodeBERTwith and without data flow to predict the probability of the source code matching the comment (thirdrow). The label is 1 if the comment correctly describes the source code otherwise 0 (second row).
Figure 6:	A case of GraPhCodeBERT output for the code clone detection task.
Figure 7:	A case of GraphCodeBERT output for the code translation task.
Figure 8:	Two cases of GraphCodeBERT output for the code refinement task.
Figure 9:	TWo examples on code search task and retrieved results from different models.
Figure 10:	An examples on code clone detection task and model prediction from different models.
Figure 11:	Error cases of GraphCodeBERT on the natural language code search.
