Figure 1: Illustration of our structural causalmodel (SCM), Where gray nodes indicateobserved variables (in training). X andX e are obtained from X(hid) and areUI-Uicoupled by sharing UD . HoWever, UI andUI can have different support, resulting indifferent distributions over X and X e .
Figure 2:	An example task where CG-invariance is stronger than G-invariance. The task is to predictthe orientation of the image while being CG-invariant to horizontal translations.
Figure 3:	Counterexample to show that GI -invariance does not imply CG-invariance. Given imagesof a rod (shown in brown), we wish to predict the orientation of the rod, i.e., whether the rod isupright or flat. In this example, we have GD = Grot and GI = Gh-translate as any horizontal translationdoes not affect the orientation of the rod. Γ : X → R sums the pixel values across the green shadedregion, and is clearly G-invariant to horizontal translations. However, Γ is not CG-invariant.
Figure 4: (a) 1-eigenspace of the Reynolds operator for the rotation group. The eigenspace hasnine basis vectors v ∈ R27 (stacked). We are representing these eigenvectors in R3×3×3 insteadto emphasize that these are rotation-invariant. (b) 1-eigenspace of the Reynolds operator for thecolor-permutation group. The eigenspace again has nine basis vectors v ∈ R27 but we represent themin R3×3×3 to emphasize that these are invariant to permutations of color channels.
Figure 5: The subspaces BM for all M ⊆ {rot, col}. For instance, B{rot,col} on the top has 3basis vectors (represented in R3×3×3) and each of these vectors are both rotation-invariant andchannel-permutation invariant. On the other hand, B{rot} (of dimension 6) is rotation invariant butstrictly not channel-permutation invariant. Finally, the vectors in B° are neither rotation-invariantnor channel-permutation invariant. All the basis vectors together cover the entire space R27 (i.e.,dim(B{rot,col}) + dim(B{ro}) + dim(B{col}) + dim(B0) = 3 + 6 + 6 + 12 = 27).
Figure 6: An example architecture of CG-invariant CNN architecture.
Figure 7: An example architecture of CG-invariant feedforward network.
Figure 8: (Best viewed in color) Describing the computation of the penalty. The cells denote differentsubsets M ⊆ {rot, col, vflip}. Red colored cells denote that the parameters corresponding to thesesubspaces are zero (i.e., the subspaces are unused) and the green colored cells denote otherwise (i.e.,the subspaces are used). In this example, the least invariant subspaces used are in Level 1. Thepenalty counts all the subspaces (used or unused) that are in higher levels (i.e., with |M| > 1) andadds it to the number of subspaces of the same level that are used.
Figure 9: (Best viewed in color) The subspaces BM for different M ⊆ {(i, j)}1≤i<j≤n indexingthe m = n2 transposition groups Gi,j over sequences of length n = 5 and dimension d = 1. Eachof the subspaces BM is of dimension 1. For each basis vector shown above, elements sharing thesame color have the same value. At the topmost level, we have the subspace with most invariance,i.e., invariant to the full permutation group Sn . Following many levels with empty subspaces, wehave subspaces BMp for Mp = {(i, j) | i, j ∈ [n] \ {p}, i < j}, where [n] = {1, . . . , n}. In otherwords, the subspace BMp is invariant to all transpositions except those that move index p. Notethat we have covered the entire space Rn with these n independent subspaces of dimension 1.
