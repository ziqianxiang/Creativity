Figure 1: Samples of our model showing diverse, high-fidelity colorizations.
Figure 2: Depiction of ColTran. It consists of 3 individual models: an autoregressive colorizer (left), a colorupsampler (middle) and a spatial upsampler (right). Each model is optimized independently. The autoregressivecolorizer (ColTran core) is an instantiation of Axial Transformer (Sec. 3.2, Ho et al. (2019b)) with conditionaltransformer layers and an auxiliary parallel head proposed in this work (Sec. 4.1). During training, the ground-truth coarse low resolution image is both the input to the decoder and the target. Masked layers ensure thatthe conditional distributions for each pixel depends solely on previous ground-truth pixels. (See Appendix Gfor a recap on autoregressive models). ColTran upsamplers are stacked row/column attention layers thatdeterministically upsample color and space in parallel. Each attention block (in green) is residual and consists ofthe following operations: layer-norm → multihead self-attention → MLP.
Figure 3: Per pixel log-likelihood of coarse colored 64 × 64 images over the validation set as a functionof training steps. We ablate the various components of the ColTran core in each plot. Left: ColTran withConditional Transformer Layers vs a baseline Axial Transformer which conditions via addition (ColTran-B).
Figure 4: Left: FID of generated 64 × 64 coarse samples as a function of training steps for λ = 0.01 andλ = 0.0. Center: Final FID scores as a function of λ. Right: FID as a function of log-likelihood.
Figure 5: We display the per-pixel, maximum predicted probability over 512 colors as a proxy for uncertainty.
Figure 6: Left: FID vs training steps, with and without polyak averaging. Right: The effect of K in top-Ksampling on FID. See Appendix B and EACKNOWLEDGEMENTSWe would like to thank Mohammad Norouzi, Rianne van den Berg, Mostafa Dehghani for their usefulcomments on the draft and Avital Oliver for assistance in the Mechanical Turk setup.
Figure 7: Ablated models. Gated: Gated conditioning layers as done in (Oord et al., 2016) and cAtt + cMLP,global: Global conditioning instead of pointwise conditioning in cAtt and cLN.
Figure 8: We train our colorization model on ImageNet and display high resolution colorizations from LSUNH Row/Column Self-attentionIn the following we describe row self-attention, that is, we omit the height dimension as all operationsare performed in parallel for each column. Given the representation of a single row within of animage x3. ∈ RW×D, row-wise self-attention block is applied as follows:[q, k, v] = LN(Xi,.)UqkvA = SoftmaX (qk>∕pDh)SA(xi,∙) = AvMSA(Xi,∙) = [SAι(xi,∙), SA2(xi,∙),…，SAk(xi,∙)] UoutUqkv ∈ RD×3Dh	(14)A ∈ RW×W	(15)(16)Uout ∈ Rk∙Dh×D (17)LN refers to the application of layer normalization (Ba et al., 2016). Finally, we apply residualconnections and a feed-forward neural network with a single hidden layer and ReLU activation(MLP) after each self-attention block as it is common practice in transformers.
Figure 9: We train our colorization model on ImageNet and display low resolution colorizations from Celeb-A17Published as a conference paper at ICLR 2021Figure 10: Top: Colorizations Bottom: Ground truth. From left to right, our colorizations have a progressivelyhigher fooling rate.
Figure 10: Top: Colorizations Bottom: Ground truth. From left to right, our colorizations have a progressivelyhigher fooling rate.
Figure 11: In each column, we display the ground truth followed by 3 samples. Left: Diverse and real. Center:Realism improves from left to right. Right: Failure casesFigure 12: We display the per-pixel, maximum predicted probability over 512 colors as a proxy for uncertainty.
Figure 12: We display the per-pixel, maximum predicted probability over 512 colors as a proxy for uncertainty.
