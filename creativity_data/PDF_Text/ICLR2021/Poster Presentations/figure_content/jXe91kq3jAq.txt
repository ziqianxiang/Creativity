Figure 1: Visual illustration of the2D root position of the quadrupedtrained with LSP on an environ-ment with random obstacles andtransferred to this environmentwith obstacles aligned in a line.
Figure 2: (a) Online planning where actions are sampled from a distribution with parameters θi (b) Fullyamortized policy with parameters φ (c) [LSP ] Skills are sampled from a distribution with parameters θi, andactions are sampled from a skill-conditioned policy with parameters φ. Here a are actions, s states and z latentplan variables. θi represents parameters of the planning distribution and φ are the parameters of the policy.
Figure 3: LSP samples kills Z from a distributionoptimized with CEM online, and samples actions afor every latent state S from skill-conditioned policy.
Figure 4: Single task performance. x-axis represents number of environment interactions in terms of fractionof 1 Million. Comparison between LSP, and the baselines Dreamer, HIRO, RandSkillsInp, and Random Skillson a suite of challenging individual locomotion tasks. Cheetah Pixels, Quadruped Run Pixels, and WalkerRunPixels are environments with image-based observations while the rest have proprioceptive features (states) asobservations to the agent. Higher is better.
Figure 5: Transfer. x-axis represents no. of environment steps in terms of fraction of 1 Million. Comparisonbetween our method (LSP), and the baselines Dreamer, HIRO, and a variant of our method that keeps thepolicy fixed in the transfer environment. The agents need to transfer to a different locomotion task, after beingpretrained on another task. (Higher is better).
Figure 6: (a) Quadruped obstacle environments. Visual illustration of the environment variants we consider.
Figure 7: Qualitative results visualizing some learned behaviors of LSP for the quadruped agent whiletransferring from the walking to the goal reaching task. Each image depicts the agent performing a different skillfor 40 time steps in the environment starting from the same initial state at the origin depicted by the red spot. Wesee that the learned skills are diverse, correspond to reasonable gaits, and help in performing the task. Videovisualizations are in the website https://sites.google.com/view/partial-amortization-hierarchy/homeTransfer in RL. Multiple previous works have investigated the problem of transferring policiesto different environments. Progressive Networks (Rusu et al., 2016) bootstrap knowledge frompreviously learned tasks by avoiding catastrophic forgetting of the learned models, (Byravan et al.,2020) perform model-based value estimation for learning an amortized policy and transfer to taskswith different specified reward functions keeping the dynamics the same, while Plan2Explore (Sekaret al., 2020) first learns a global world model without task rewards through a self-supervised objective,and given a user-specified reward function at test time, quickly adapts to it. In contrast to these,several meta RL approaches learn policy parameters that generalize well with little fine-tuning (oftenin the form of gradient updates) to target environments (Finn et al., 2017; Xu et al., 2018; Wang et al.,2016; Rakelly et al., 2019; Yu et al., 2019).
Figure 8:	Quadruped obstacle environ-ment trajectories. Visual illustration ofthe 2D root position of the quadrupedfor the 1st 40k steps after transfer to thecove environment. On the left is Dreamerand on the right is LSP with fixed policy.
Figure 9:	Quadruped obstacle environ-ment transfer learning curves. We plottotal rewards against number of transfertraining time steps on the sparse coveenvironment. Note that after 70k steps(70 episodes), the LSP agent is able reli-ably rech the target every time, whereasdreamer does not even come close to thetarget, stunted by the sparse reward.
Figure 10:	Example of collapsed skillpolicy. Here we show skill sequencessampled from a policy where input noiseis not added to the skill predictor and sub-sequently the skill policy seems to per-form the same motion for different skills(turning clockwise), despite inclusion ofthe mutual information objective. Eachrow is a different latent variable.
