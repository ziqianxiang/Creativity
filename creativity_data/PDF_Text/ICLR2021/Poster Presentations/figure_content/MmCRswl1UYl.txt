Figure 1: The problem setting: A OTT-QA model needs to retrieve from two candidate pools andthen perform multi-hop reasoning to find answers.
Figure 2: The ‘de-contextualization’ annotation phase of OTT-QA. In the first step, the annotator isrestricted to add phrases from the context. In the second step, the annotator is specifically requestedto make the sentence more concise and natural.
Figure 3: Left: Iterative 3-step retrieval over individual blocks (baseline). Right: Fusion 1-stepretrieval over fused groups, which greatly lowers the cost of iterative encoding and retrieving.
Figure 4: Left: Single-block reader with input shorter than 512 tokens (baseline). Right: Cross-block reader with length over 4K tokens, and A denotes the global state assigned to local block A.
Figure 7: More examples from OTT-QAA1: COVID-19 -> 19-20 Season -> 25.3A2: 27.5 -> Cleveland -> J. B. BickerStaffA3: 25.3 -> 19-20 Season -> COVID-19A4: 25.3 < 27.5 -> 17-18A.3 Question TypesWe randomly sampled 100 questions from the dataset to manually analyze the kinds of inferencechains seen in OTT-QA and divide the major types into the following categories:1.	Single hop questions (13%) require reading one table or one passage to answer.
Figure 8: The decomposition of the original table into segments.
Figure 9: Fusion: 1) GPT-2 query augmentation, 2) nearest neighbor search over passages.
Figure 10: Breakdown for iterative retriever Figure 11: Breakdown for fusion retriever ++ sing-block reader.	cross-block reader.
Figure 12: The main error types in the retriever.
Figure 13: Analyzing retriever performance.
