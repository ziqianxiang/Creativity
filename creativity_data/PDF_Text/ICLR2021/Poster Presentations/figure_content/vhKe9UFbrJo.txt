Figure 1: Multimodal datafrom the CUB datasetis not exactly data-efficient—constructing a suitable dataset requires a lot of “annotated” unimodaldata, as we need to ensure that each multimodal pair is related in a meaningful way. The situationis worse when We consider more complicated multimodal settings SuCh as language-vision, whereone-to-one or one-to-many correspondence between instances of the two datasets are required, dueto the difficulty in categorising data such that commonality amongst samples is preserved withincategories. See Figure 1 for an example from the CUB dataset (Welinder et al., a); although the samespecies of bird is featured in both image-caption pairs, their content differs considerably. It would beunreasonable to apply the caption from one to describe the bird depicted in the other, necessitatingone-to-one correspondence between images and captions.
Figure 2: Graphical models formultimodal generative process.
Figure 3: Constructing re-lated & unrelated samplesNote that the PMI measures the statistical dependence between values (x, y),which for a joint distribution p(x, y) is defined as I(x, y) = log Ppxxpyy).
Figure 4: log p(x, y) of imitation,unrelated digits and random noise,where p = N(mx , my).
Figure 5: Left of each pair: Four criteria for multi-modal generative models; image adapted from Shi et al.
Figure 6: Performance of MMVAE, cI-MMVAE and cC-MMVAE using n% of MNIST-SVHN.
Figure 7: Pipeline of label propagationPipeline As showing in Figure 7, we first construct a full dataset by randomly matching instancesin MNIST and SVHN, and denote the related pairs by Fr (full, related). We further assume accessto only n% of Fr, denoted as Sr (small, related), and denote the rest as Fm, containing a mixof related and unrelated pairs. Next, we train a generative model g on Sr . To find a relatednessthreshold, we construct a small, mixed dataset Sm by randomly matching samples across modalitiesin Sr. Given relatedness ground-truth for Sm, we can compute the PMI I(x, y) = logpΘ(x, y) -log pθx (x)pθy (y) for all pairs (x, y) in Sm and estimate an optimal threshold. This threshold cannow be applied to the full, mixed dataset Fm to identify related pairs giving us a new related datasetFr, which can be used to further improve the performance of the generative model g.
Figure 8: Models with and without label propagation using MMVAE, cI-MMVAE and cC-MMVAE.
Figure 9: First 300 iterations of training using contrastive loss LC only.
Figure 10: First 300 iterations of training with final loss L, where γ = 2.
Figure 11: Performance on different metrics for different values of γ . Dotted lines represents the performance ofbaseline MMVAE.
Figure 12: Generations of MMVAE model trained using the final contrastive objective, with (from left toright) γ = 1.1, 2 and +∞. Note in (c), (d), (e), (f), the top rows are the inputs and the bottom rows are theircorresponding reconstruction/cross generation.
Figure 13: Performance on different metrics for different values of γ. Dotted lines represents the performance ofbaseline MMVAE.
Figure 14: Generations of MMVAE model, from left to right are original model (MMVAE), contrastive loss withIWAE estimator (cI-MMVAE) and contrastive loss with CUBO estimator (cC-MMVAE).
Figure 15: Generations of MVAE model, from left to right are original model (MVAE), contrastive loss withIWAE estimator (cI-MVAE), contrastive loss with CUBO estimator (cC-MVAE).
Figure 16:	Generations of JMVAE model, from left to right are original model (JMVAE), contrastive loss withIWAE estimator (cI-JMVAE), contrastive loss with CUBO estimator (cC-JMVAE).
Figure 17:	Number of examples generated for each of class during joint generation.
Figure 18: Qualitative results of MMVAE on CUB Image-Caption dataset, including reconstruction (vision →vision, language → language), cross generation (vision → language, language → vision) and joint generationfrom prior samples.
Figure 19: Qualitative results of MMVAE trained with contrastive loss with IWAE estimator on CUB Image-Caption dataset, including reconstruction (vision → vision, language → language), cross generation (vision →language, language → vision) and joint generation from prior samples.
Figure 20: Qualitative results of MMVAE trained with contrastive loss with CUBO estimator on CUB Image-Caption dataset, including reconstruction (vision → vision, language → language), cross generation (vision →language, language → vision) and joint generation from prior samples.
