Figure 1: Simple latent regression on a fixed, pretrained generator can perform a number of image manipulationtasks based on single examples without requiring labelled concepts during training. We use this to probe theability of GANs to compose scenes from image parts, suggesting that a compositional representation of objectsand their properties exists already at the latent level.1Attribute EditingMUltimodal Editing1	IntroductionNatural scenes are comprised of disparate parts and objects that humans can easily segment andinterchange (Biederman, 1987). Recently, unconditional generative adversarial networks (Karraset al., 2017; 2019b;a; Radford et al., 2015) have become capable of mimicking the complexity ofnatural images by learning a mapping from a latent space noise distribution to the image manifold.
Figure 2: Image completions using the latent space regressor. Given a partial image, a masked regressorrealistically reconstructs the scene in a way that is consistent with the given context. The completions (“Inverted”)can vary depending on the exposed context region of the same input.
Figure 3:	We train a latent space regressor E topredict the latent code Z that, When passed through afixed generator, reconstructs input x. At training andtest time, We can also modify the encoder input Withadditional binary mask m (Eqn. 3). Inference requiresonly a forWard pass and the input x can be unrealistic,as the encoder and generator serve as a prior to mapthe image back to the image manifold.
Figure 4:	Composition of unrealistic input collagesis a balance of tWo factors: We Want to reconstructthe input (loW L1 ), but still retain realistic images(loW FID). Using automatic collages of synthesizedimage parts, We plot this tradeoff of masked L1 errorbetWeen the input collages and output composites,and FID change betWeen the output composites andre-encoded images on different image domains.
Figure 5: Trained only on a masked reconstruction objective, a regressor into the latent space of a pretrainedGAN allows the generator to recombine components of its generated images, despite strong misalignments andmissing regions in the input. Here, we show automatically generated collaged inputs from extracted image partsand the corresponding outputs of the generators.
Figure 6: Comparing reconstruction of image collages (masked L1 ) to realism of the generated outputs onrandom church collages (left) and face collages (right) across different image reconstruction methods, broadlycharacterized as autoencoders, GAN-based optimization, GANs with an encoder to perform latent regression,and a combination of GAN, regression, and optimization. An ideal composition has low L1 and high density(close to real image manifold), and each method exhibits different tradeoffs in reconstruction and realism.
Figure 7: Comparing composition and interpolation. We aim to apply the same target modification (whitewindows; Target), to two context sources (Context), where the collage of the two images is shown in the thirdcolumn (Collage). Compared to Latent and Pixel α-blending, inverting the collages into the latent space via theencoder (Composition) better matches the context and target regions, while at the same time ensuring globalcoherence between the target and context images.
Figure 8: Image variation when replacing single parts. In ProGAN (a), replacing single parts leads to changebeyond the part that is being changed. Changes in StyleGAN (b) are visually more localized. This method canbe used to find correlated regions even without semantic labels, shown for cars (c).
Figure 9: Given a masked real image, a regressor without knowledge of masked images (RGB) is unable torealistically reconstruct the scene, while the regressor trained on masks inputs inpaints in the unknown region ina way that is consistent with the given context (RGBM).
Figure 10: Using latent regression, we can perform multimodal editing to a context image. Each editingoperation uses a single pair of images. Here, our editing mask is a rough rectangular box. Note how in case ofTree 2, the pasted region includes part of the church; the regressor attempts to include this part, and the resultingre-generated building therefore looks different from the building in the other examples.
Figure 11: We demonstrate the transfer capability of the latent code manipulations. From a single pair ofimages, we can compute a manipulation vector and apply it to a secondary image, which edits the secondaryimage accordingly.
Figure 12: We perform dataset rebalancing using latent space regression. From pretrained attribute detectors,a smaller fraction of males smile than females in CelebA-HQ, and the GAN samples mimic this trend. Byswapping mouths in the GAN samples, we can equalize male and female smiling rates. Next, we do this swapand invert operation on the dataset and retrain a GAN on the modified dataset, which also improves balance insmiling rates although the effect is stronger for the male category.
Figure 13: Comparison of image editing outcomes when adding trees (Target) to different scenes (Context).
Figure 14:	Comparison of image editing outcomes when changing the sky of a scene, using similar encoder-based Composition, Latent α-blend, and Pixel α-blend methods as above.
Figure 15:	Comparison of image editing outcomes using the same encoder-based Composition, Latent α-blend,and Pixel α-blend methods as above when adding a smile to a face. We also compare these approaches to themodifications produced by edit vector learned from attribute labels.
Figure 16: Qualitative examples of reconstructions from masked inputs on encoders trained with differentcombinations of loss terms.
Figure 17: Qualitative examples of automatic image composition. We extract parts of sampled images andoverlay them to form a rough collage. We compare Poisson blending the image parts (we use the bottom-mostimage layer to fill in any remaining holes in the collaged image) to encoder-based methods that invert the collagethrough the generator, either without knowledge of missing pixels (RGB and RGB Fill methods), or with theobjective of inpainting the missing regions (RGBM). While the RGB and RGB Fill methods also demonstrate analignment-correcting effect, we find quantitatively that the RGBM encoder tends to have lower FID over 50Ksamples (Tab. 3).
Figure 18:	Qualitative examples of image composition comparing different encoder-decoder or generator-onlysetups. For each example, we show the input collage created from randomly selected parts of images (Input), andfour autoencoder-based methods (CycleGAN, SPADE, Inpaint, DIP) in which the encoder and decode is trainedjointly; as these are fully convolutional they lack global semantic priors. Next, we show four optimization-basedmethods (ProGAN & StyleGAN LBFGS, Multi-Code Prior, StyleGAN Projection); these can better match thetarget collage after optimizing for reconstruction, but they are orders of magnitude slower, making real-timeinteraction infeasible, and also lose semantic priors (as in the “floating tower” in the third example). We thenshow the combination of our encoder with LBFGS optimization, and our encoder using only a forward pass.
Figure 19:	Qualitative examples of face composites across GAN-based generators. We show the input image,a collage from parts of random images (Input). We compare to Inpainting, Im2StyleGAN which iterativelyoptimizes for a latent code, the ALAE autoencoder where the generator and encoder is trained jointly, the In-domain encoder and diffusion which encodes and optimizes the latent, a masked version of the Pixel2Style2Pixel(PSP) network, and our masked encoder approaches (ProGAN Encoder and StyleGAN encoder).
Figure 20: Qualitative examples of automatic image composition using a pretrained saliency network togenerate input collages rather than a segmentation network. Note that in the second car example, the collageoverlays the blue car with the yellow car still visible, but inversion via the encoder corrects this inconsistency.
Figure 21: Qualitative examples of collages based on user-selected regions and the corresponding generatoroutput. Note that the selected regions do not have to coincide neatly with object boundaries, and the generatorwill blend inconsistencies in the inputs together.
Figure 22: We investigate the capabilities of our instance-based regression and editing approach to modifylighting effects, similar to Bau et al. (2020). Editing lighting induces non-local changes, as the network mustalso change regions outside of the modified region to ensure global consistency - for example the reflectionof the window on the floor (ProGAN Living Room), or changes in the illumination of the building (StyleGANChurch) - indicating a tradeoff between reconstructing the input versus creating a realistic image overall.
Figure 23: To improve the reconstruction towards real faces, yet preserve the properties of real-time composition,we can slightly finetune the regressor towards a context image (30 gradient steps, < 5 seconds). Subsequentmodifications to the context image can be performed using just a feed-forward pass, enabling fast editing tocreate image composites.
Figure 24: Random samples of automatically generated colleges using a pretrained ProGAN generator.
Figure 25: Random samples of automatically generated colleges using a pretrained StyleGAN generator.
Figure 26: When changing semantic parts of a face (such as the eyes or the hair, shown in red), the resultingchange in the image is often limited to those parts, indicating that the model parses faces into meaningful andintuitive components.
Figure 27: Our encoder-generator pair can be used to detect correlated parts of objects. Here, the value indicateshow much a pixel changes when the superpixel shown in red is changed; oftentimes, semantic segments emerge,such as the car body, windows, the street, or the background.
