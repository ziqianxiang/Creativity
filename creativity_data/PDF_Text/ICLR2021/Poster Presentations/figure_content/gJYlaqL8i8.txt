Figure 1: Sampled transitions on Pendulum-v0 from various sampling strategies: (a) Sampling by TD-error, (b)Sampling by Q-value, (c) Sampling uniformly at random. Samples highlighted in black, orange, and cyan boxesdenote that their state has the rod in downward, upright, and horizontal positions with appropriate amount ofactions, respectively. Samples in red boxes have excessively large actions.
Figure 2: An overview of our neural experience replay sampler (NERS) framework. We first sample transitionsproportionally to scores previously calculated. Then, our neural sampling policy evaluates them. Specifically,NERS consists of three networks fl, fg and fs. The first two networks obtain local and global contexts byconsidering various features, respectively. Then the last network evaluates the relative importance (score) byfs . The importance set is used when to sample transitions later and train the agent. This design satisfies thepermutation-equivariant property.
Figure 3: Learning curves of off-policy RL algorithms with various sampling methods on classical andBox2D continuous control tasks. The solid line and shaded regions represent the mean and standard deviation,respectively, across five runs with random seeds.
Figure 4: (a): ComParison of NERS and variants of NERS only with few features (reward, TD-error, andtimesteP) and without global context across five instances with random seeds, resPectively. (b)-(c): Learningcurves under SAC over five instances with random seeds across five instances with random seeds, resPectively.
Figure 5: Curves of the average of Q-values (a/b/c) and Log(TD-errors) (d/e/f) of samPled transitionsacross five instances with random seeds, resPectively. One can show that NERS basically selectstransitions with high TD-errors in the beginning and both high TD-errors and Q-values finally.
