Figure 1: Seed vs. moco-v2 (chen et al.,2020c)) on ImageNet-1K linear probe accuracy.
Figure 2: Illustration of our self-supervised distillation pipeline. The teacher encoder is pre-trained by SSL andkept frozen during the distillation. The student encoder is trained by minimizing the cross entropy of probabilitiesfrom teacher & student for an augmented view of an image, computed over a dynamically maintained queue.
Figure 3: ImageNet-1k Top-1 accuracy for semi-supervised evaluations using 1% (red line), 10% (blue line)of the annotations for linear fine-tuning, in comparison with the fully supervised (green line) linear evaluationbaseline for SEED. For the points whose Teacher’s number of parameters is at 0, we show the semi-supervisedlinear evaluation results of MoCo-V2 without any distillation. The Student models tend to perform better on thesemi-supervised tasks after distillation from larger Teachers.
Figure 4: ImageNet-1k Accuracy (%) of student network (EfficientNet-B0 and ResNet-18) transferred to otherdomains (CIFAR-10, CIFAR-100, SUN-397 datasets) with and without distillation from lager architectures(ResNet-50/101/152).
Figure 5: Accuracy (%) of student networks(EfficientNet-b0 and ResNet-18) on ImageNet dis-tilled from wider MoCo-v2 pre-trained ResNet(ReSNet-50∕101∕152×2).
Figure 6: We experiment with different strategies of using views during distillation, which include: (a). Identicalview for distillation. (b). Cross view distillation. (c). Large-small cross view distillation. (d). Large-smallidentical view distillation.
Figure 7: Linear evaluation accuracy (%) of distillation between ResNet-18 (as the Student) and ResNet-50 (asthe Teacher) using different size of queue When LR=0.03 and weight decay=1e-6. Note the axis is the log(∙)value of queue lengths.
