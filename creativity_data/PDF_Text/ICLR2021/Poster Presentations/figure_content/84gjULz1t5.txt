Figure 1: Linear regression problem.
Figure 2: LogistiC regression problem in the heterogeneous Case (full-batCh gradient).
Figure 3: Logistic regression in the heterogeneous case (mini-batch gradient).
Figure 4: Stochastic optimization on deep neural network (* means divergence).
Figure 5: Relative compression error kx-X(X)k2 for p-norm b-bit quantizationTo verify Theorem 3, we compare the compression error of the quantization method defined in (20)with different norms (p = 1, 2, 3, . . . , 6, ∞). Specifically, we uniformly generate 100 random vec-tors in R10000 and compute the average compression error. The result shown in Figure 5 verifiesour proof in Theorem 3 that the compression error decreases when p increases. This suggests that∞-norm provides the best compression precision under the same bit constraint.
Figure 6: Comparison of compression errorkx-Q(X)k2kxk2between different compression methodsD ExperimentsD.1 Parameter sensitivityIn the linear regression problem, the convergence of LEAD under different parameter settings of αand γ are tested. The result showed in Figure 7 indicates that LEAD performs well in most settingsand is robust to the parameter setting. Therefore, in this paper, we simply set α = 0.5 and γ = 1.0for LEAD in all experiment, which indicates the minor effort needed for parameter tuning.
Figure 7: Parameter analysis on linear regression problem.
Figure 8: Logistic regression in the homogeneous case (full-batch gradient)10010-110-110-10.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00Bits transmitted	1e10(b)Loss f (Xk)(a) Loss f(Xk)Figure 9: Logistic regression in the homogeneous case (mini-batch gradient)—— DGD (32 bits)—— NIDS (32 bits)——QDGD (2 bits)--- DeePSqUeeZe (2 bits)——CHOCO-SGD (2 bits)—— LEAD (2 bits)0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4Bits transmitted	1e9(b)Loss f (Xk)D.3 Parameter settings
Figure 9: Logistic regression in the homogeneous case (mini-batch gradient)—— DGD (32 bits)—— NIDS (32 bits)——QDGD (2 bits)--- DeePSqUeeZe (2 bits)——CHOCO-SGD (2 bits)—— LEAD (2 bits)0.0	0.2	0.4	0.6	0.8	1.0	1.2	1.4Bits transmitted	1e9(b)Loss f (Xk)D.3 Parameter settingsThe best parameter settings we search for all algorithms and experiments are summarized in Ta-bles 1- 4. QDGD and DeePSqUeeze are more sensitive to Y and CHOCO-SGD is slight more robust.
