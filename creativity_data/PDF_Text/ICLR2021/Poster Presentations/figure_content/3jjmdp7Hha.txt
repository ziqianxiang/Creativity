Figure 1: An example training step of meta back-translation to train a forward model translating English (En)into German (De). The step consists of two phases, illustrated from left to right in the figure. Phase 1: abackward model translates a De sentence taken from a monolingual corpus into a pseudo En sentence, and theforward model updates its parameters by back-propagating from canonical training losses on the pair (pseudoEn, mono De). Phase 2: the updated forward model computes a cross-entropy loss on a pair of ground truthsentences (real En, real De). As annotated with the red path in the figure, this cross-entropy loss depends on thebackward model, and hence can be back-propagated to update the backward model. Best viewed in colors.
Figure 2: Probability of pseudo-parallel fromthe forward model for WMT’14 En-Fr. MetaBTproduces less diverse data to fit the model better.
Figure 3: Training PPL and Validation BLEU for WMTEn-De throughout the forward model’s training. MetaBTleads to consistently higher validation BLEU by generatingpseudo-parallel data that avoids overfitting for the forwarwdmodel, evident by a higher training PPL.
Figure 4: Percentage of words in the pseudo source sentences that are in the low-resource vocabulary throughouttraining. MetaBT learns to favor the sentences that are more similar to the data from the low-resource language.
Figure 5: Histogram of differencesin length between the reference andsystem outputs. MLE-trained BTtends to generate slightly more out-puts with lengths that greatly differfrom the reference.
