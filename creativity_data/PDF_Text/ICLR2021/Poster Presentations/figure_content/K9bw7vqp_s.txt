Figure 1: Illustration of achieving N:M structure sparsity. (Left) In a weight matrix of 2:4 sparseneural network, whose shape is R X C (e.g., R = output_channels and C = input_channels in alinear layer), at least two entries would be zero in each group of 4 consecutive weights. (Middle &Right) The process that the original matrix is compressed, which enables processing of the matrixto be further accelerated by designated processing units (e.g., Nvidia A100).
Figure 2: In this figure, represents element-wise multiplication and indicates matrix multipli-cation. (a) This figure shows the forward and backward pass during training an N:M sparse network.
Figure 3: We compare two networks respectively trained with regular SGD method and STE-modified gradient descent. (a) This figure shows sparse networks trained with STE has a significantperformance drop in top-1 accuracy compared with dense networks. (b) This figure illustrates thelayer-wise SAD between the weights after certain number of iterations and the initial weights, fortwo networks trained with STE (sparse forward) and regular SGD(dense forward). Compared withnetworks trained with sparse forward gradient, the one with dense forward gradient displays smallerSAD, indicating fewer updates in its sparse network architectures.
Figure 4: (a) This figure illustrates SAD as a function of training epoch number with 4 differentsettings of λW in the SR-STE term. When λW < 0, the perturbations brought by coarse gradientsof sparse wights are widened, SAD gets higher and the top-1 accuracy becomes lower. When λWis set to a reasonable positive value, sparse nets received high performance and low SAD. (b) Thisfigure compares the top-1 accuracy curves of sparse net trained with STE, sparse net trained withSR-STE, and dense net. Sparse networks naively trained with STE have significant performancedrop compared with dense ones. After introducing the SR-STE term into optimization process, thesparse network’s performance jumps to a comparable level with dense networks.
Figure 5: Illustration of kernel shape in ResNet50 with 2:8 structured sparsity trained model,layer1.1.conv2: (0,32) denotes layer name: (index of input channel, index of output channel).
Figure 6: (a) This figure compares the top-1 accuracy curves of different N:M patterns with SR-STE. (b) This figure illustrates the SAD curves as each training epoch number with 3 differentsparse-refined formulationA.5 other refined formulationwe present another sparse-refined regularization term, i.e., sign constant, as follow:. .〜-. .- ...
