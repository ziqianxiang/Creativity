Figure 1: Example of do-interventions on exposed variablesin CausalWorld.
Figure 2: Example tasks from the task generators provided in the benchmark. The goal shape isvisualized in opaque red and the blocks in blue.
Figure 3: Key components for generic training and evaluation of RL agents. Left: A learningcurriculum which is composed of various intervention actors that decide which variables to interveneon (for a valid intervention, values need to be in the allowed training space (ATS)). Right: Evaluationprotocols are shown which may intervene on variables at episode resets or within episodes (for avalid intervention, values need to be in the evaluation space (ES)). Middle: we represent the ATSand ES, where each intervention results in one point in the spaces. As shown ATS and ES mayintersect, eg. if the protocols are meant to evaluate in-distribution generalization.
Figure 4: A subset of an SCM represented as a DAG for an environment in CausalWorld with oneblock on the floor. Here, we only show a subset of the causal variables affecting the block positionattimet+1.
Figure 5:	Fractional success curves averaged over five random seeds for the tasks and learningalgorithms specified above, under three different training curricula: (0) no curriculum, (1) goalposition and orientation randomization in space A every episode and (2) a curriculum where weintervene on all variables in space A simultaneously every episode.
Figure 6:	Evaluation scores for pushing baselines. Each protocol was evaluated for 200 episodes andeach bar is averaged over five models with different random seeds. The variables listed under eachprotocol are sampled from the specified space at the start of every episode while all other variablesremain fixed [bp block pose, bm block mass, bs block size, gp goal pose, ff floor friction].
Figure 7: Example ”pixel” mode observations returned at each step of the environment.
Figure 8: Structured observation description. For the scene features, all the blocks feature vectorare concatenated first. Following that the partial goals feature vector are concatenated in the sameorder. Lastly, if there is any obstacles/ fixed blocks, their feature vectors are concatenated at the endfollowing the same description as the partial goal features.
Figure 9: The TriFinger platform.
Figure 10: An example of model selection in CausalWorld by evaluating generalization acrossthe various axes using the previously mentioned protocols. Here we compare two agents trained ondifferent curricula using PPO.
Figure 11: Evaluation scores, for pushing, picking, pick and place and stacking2 baselines, from topto bottom respectively. Each protocol was evaluated for 200 episodes and each bar is averaged overfive models with different random seeds [bp block pose, bm block mass, bs block size, gp goal pose,ff floor friction].
