Figure 1: An overview of Contrastive Discriminator (ContraD). Overall, the representation of Con-traD is not learned from the discriminator loss (Ldis), but from two contrastive losses Lc+on and Lc-on,each is for the real and fake samples, respectively. Here, sg(∙) denotes the stop-gradient operation.
Figure 2: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods. All the models are trained on CIFAR-10 with an SNDCGAN architecture.
Figure 3: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods. All the models are trained on CIFAR-100 with an SNDCGAN architecture.
Figure 4: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods. All the models are trained on CelebA-HQ-128 with an SNDCGAN architecture.
Figure 5: Visualization of conditionally generated samples via conditional DDLS (Section 3.3) withContraD. We use an SNDCGAN generator trained with an SNResNet-18 ContraD for the generation.
Figure 6: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods, along with real images from the training set. All the models are trained on AFHQ-Dog (4,739 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) withψ = 0.7 to produce the images at the bottom row. We use the pre-trained models officially releasedby the authors to obtain the results of the baseline StyleGAN2 and ADA.
Figure 7: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods, along with real images from the training set. All the models are trained on AFHQ-Cat (5,153 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) with ψ =0.7 to produce the images at the bottom row. We use the pre-trained models officially released bythe authors to obtain the results of the baseline StyleGAN2 and ADA.
Figure 8: Qualitative comparison of unconditionally generated samples from GANs with differenttraining methods, along with real images from the training set. All the models are trained on AFHQ-Wild (4,738 images) with StyleGAN2. We apply the truncation trick (Karras et al., 2019) withψ = 0.7 to produce the images at the bottom row. We use the pre-trained models officially releasedby the authors to obtain the results of the baseline StyleGAN2 and ADA.
Figure 9: Comparion of the FID distribution of the top 25% of trained models (Kurach et al., 2019)on CIFAR-10 (SNDCGAN) for different batch sizes.
