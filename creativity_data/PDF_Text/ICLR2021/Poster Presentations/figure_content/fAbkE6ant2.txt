Figure 1: (a) and (b) illustrate the paradigms of end-to-end (E2E) learning and locally supervised learning (K = 2).
Figure 2: The linear separability (left, measured by test errors), mutual information with the input x (middle), andmutual information with the label y (right) of the intermediate features h from different layers when the greedysupervised learning (greedy SL) algorithm is adopted with K local modules. The ends of local modules aremarked using larger markers with black edges. The experiments are conducted on CIFAR-10 with a ResNet-32.
Figure 3: Comparisons of InfoPro and state-of-the-art locallearning methods in terms of the test errors at the final layer(left) and the task-relevant information capture by intermediatefeatures, I(h, y) (right). Results of ResNet-32 on CIFAR-10are reported. We use the contrastive loss in LInfoPro.
Figure 4: Sensitivity tests. The CIFAR-10 test errors ofResNet-32 trained using InfoPro (K=4) are reported. Wevary λ1 and λ2 for 1st and 3rd local modules respectively,with all other modules unchanged. We do not consider λ1 =λ2 = 0, where We obviously have CInfOPrO = LjnfoPro ≡ 0.
Figure 5: Illustration of theMNIST-STL10 dataset.
Figure 6: The estimates of mutual information between the intermediate features h and the three labels ofMNIST-STL10 (see: Figure 5), i.e. yι (left, background), y2 (middle, digit) and y3 (right, position of digit).
Figure 7:	Visualization of the reconstruction results obtained from the decoder w.
Figure 8:	Test errors ofResNet-110 trained by InfoPro (Contrast) on CIFAR-10,with varying width of the decoder W (Ieft) and the projection head φ (right).
Figure 9: Performance of InfoPro (Con-trast) with varying temperature τ . Testerrors of ResNet-110 on CIFAR-10.
