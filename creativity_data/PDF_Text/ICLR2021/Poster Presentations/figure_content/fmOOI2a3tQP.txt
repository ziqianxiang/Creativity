Figure 1: Visualizations of the typicalMTRL setting and the HiP-MDP setting.
Figure 2: Graphical model of HiP-BMDP setting (left). Flow diagram of learning a HiP-BMDP (right). Twoenvironment ids are selected by permuting a randomly sampled batch of data from the replay buffer, and the lossobjective requires computing the Wasserstein distance of the predicted next-step distribution for those states.
Figure 3:	Variation inCheetah-Run-V0 tasks.
Figure 4: Multi-Task Setting. Zero-shot generalization performance on the extrapolation tasks. We see thatour method, HiP-BMDP, performs best against all baselines across all environments. Note that the environmentsteps (on the x-axis) denote the environment steps for each task. Since we are training over four environments,the actual number of steps is approx. 3.2 million.
Figure 5: FeW-shot generalization performance on the interpolation (2 left) and extrapolation (2 right) tasks.
Figure 7: Zero-shot generalization performance on the evaluation tasks in the MTRL setting with partialobservability. HiP-BMDP (ours) consistently outperforms other baselines (left). We also show decreasingperformance by HiP-BMDP as p increases (right). 10 seeds, 1 stderr shaded.
Figure 8: Variation in Walker (V1) across different tasks.
Figure 9: Multi-Task Setting. Performance on the training tasks.Note that the environment steps (onthe x-axis) denote the environment steps for each task. Since we are training over four environments,the actual number of steps is approximately 3.2 million.
Figure 10: Zero-shot generalization performance on the interpolation tasks in the MTRL setting. HiP-BMDP(ours) consistently outperforms other baselines. 10 seeds, 1 standard error shaded.
Figure 11: Average per-step model error (in latent space) after unrolling the transition model for 5 steps.
Figure 12: Average per-step model error (in latent space)after unrolling the transition model for 100 steps.
Figure 13: Few-shot generalization performance on the interpolation tasks on Walker-Run-V0, Walker-Walk-V0Walker-Stand-V0, Cheetah-Run-V0 (top row), Walker-Run-V1, Walker-Walk-V1 and Walker-Stand-V1 (bottomrow) respectively. We note that for the Walker-Walk-V1, the proposed approach (blue) converges faster to athreshold reward (green) than the baseline. In other environments, the gains are quite small.
Figure 14: Few-shot generalization performance on the extrapolation tasks on Walker-Run-V0, Walker-Walk-V0, Walker-Stand-V0, Cheetah-Run-V0 (top row), Walker-Run-V1, Walker-Walk-V1 and Walker-Stand-V1(bottom row) respectively. We note that for the Walker-Walk-V1, the proposed approach (blue) converges fasterto a threshold reward (green) than the baseline. In other environments, the gains are quite small.
