Figure 1: Sample images from the ObjectNetdataset along with our manually annotatedobject bounding boxes from Chairs,Teapots and T-shirts categories. Theleftmost column shows three chair examplesfrom the ImageNet dataset. ImageNet scenesoften have a single isolated object in themwhereas images in the ObjectNet datasetcontain multiple objects. Further, ObjectNetobjects cover a wider range of variationin contrast, rotation, scale, and occlusioncompared to ImageNet objects (See argumentsin Barbu et al. (2019)). In total, we annotated18,574 images across 113 categories incommon between the two datasets. This figureis modified from Figure 2 in Barbu et al.
Figure 2: Performance of deepobject recognition models onObjectNet dataset. Resultsof our analysis by applyingAlexNet, VGG-19, andResNet-152 models to objectbounding boxes (i.e., isolatedobjects) are shown in blue(overlaid in Fig. 1 from theObjectNet paper). Feedingobject boxes to models insteadof the entire scene improvesthe accuracy about 10-15%.
Figure 3: Left two panels) Average top-1 and top-5 accuracy of models over 1130 images from the ObjectNetdataset corrupted by 14 natural image distortions. Right two panels) Average classification accuracy per each ofthe 113 categories (left) and each of the 14 distortion types (right). In all cases, applying models to the isolatedobject (using bounding boxes) leads to higher robustness than applying them to the full image.
Figure 4: The effect of background subtraction (a.k.a foreground detection)on adversarial robustness (here against the FGSM attack). Two modelsare trained and tested on clean and noisy data from MNIST (left) andFashionMNIST (right) datasets. In the noise case, the object is overlaidin a white noise field (no noise on the object itself).
Figure 5: Model accuracy against adversarial perturbations (left) andnoise corruptions (middle). The right panel shows a sample chair imagealong with its bounding box and segmentation mask.
Figure 6: Performance (top-1) of the ResNet-18 model against geometric transformations.
Figure 7: Left) Four easiest (highest classification confidence; 1st rows) and four hardest (lowest confidence;2nd rows) samples per category from the ObjectNet dataset for the ResNet-152 model (predictions shown inblue). The easy samples seem to be easier for humans (qualitatively), while the harder ones also seem to beharder. Right) A collection of challenging objects from the ObjectNet dataset for humans. Can you guess thecategory of the annotated objects in these images? Keys are: row 1: skirt, fan, desk lamp, safety pin, stillcamera, and spatula, row 2: vase, shovel, vacuum cleaner, printer, remote control, and pet food container,and row 3: sandal, vase, match, spatula, stuffed animal, and shovel. Appxs. C & D contain more examples.
Figure 8: Frequency of the images (one object label per image) over the 113 categories of ObjectNet overlappedwith the ImageNet. The right bar chart is the continuation of of the left one.
Figure 9: Performance of models onGoogLeNet).
Figure 10: Performance of modelsMNASNet).
Figure 11: Performance of models on the entire image (from left to right: AlexNet, VGG, and GoogLeNet).
Figure 12: Performance of modelsthe entire image (from left to right: ResNet, Inception3, and MNASNet).
Figure 13: Correctly classified and misclassified examples from the Alarm clock class by the ResNet model.
Figure 14: Correctly classified and misclassified examples from the Banana class by the ResNet model.
Figure 15: Correctly classified and misclassified examples from the Band-Aid class by the ResNet model.
Figure 16: Correctly classified and misclassified examples from the Bench class by the ResNet model.
Figure 17: Correctly classified and misclassified examples from the Plate class by the ResNet model.
Figure 18: Correctly classified and misclassified examples from the Broom class by the ResNet model.
Figure 19: Correctly classified and misclassified examples from the Candle class by the ResNet model.
Figure 20: Correctly classified and misclassified examples from the Fan class by the ResNet model.
Figure 21: Correctly classified and misclassified examples from the Ruler class by the ResNet model.
Figure 22: Correctly classified and misclassified examples from the Safety-pin class by the ResNet model.
Figure 23: Correctly classified and misclassified examples from the Teapot class by the ResNet model.
Figure 24: Correctly classified and misclassified examples from the TV class by the ResNet model.
Figure 25: Correctly classified and misclassified examples from the Sock class by the ResNet model.
Figure 26: Correctly classified and misclassified examples from the Sunglasses class by the ResNet model.
Figure 27: Correctly classified and misclassified examples from the Tie class by the ResNet model.
Figure 28: Correctly classified and misclassified examples from the Mug class by the ResNet model.
Figure 29: Correctly classified and misclassified examples from the Hammer class by the ResNet model.
Figure 30: Correctly classified and misclassified examples from the Bicycle class by the ResNet model.
Figure 31: Correctly classified and misclassified examples from the Cellphone class by the ResNet model.
Figure 32: Correctly classified and misclassified examples from the Chair class by the ResNet model.
Figure 33: A selection of challenging objects that are hard to be recognized by humans. Can you guess thecategory of the annotated objects in these images? Keys are as follows:row 1: (skirt, skirt, desk lamp, safety pin, still camera, spatula, tray),row 2: (vase, pillow, sleeping bag, printer, remote control, pet food container, detergent),row 3: (vacuum cleaner, vase, vase, shovel, stuffed animal, sandal, sandal),row 4: (sock, shovel, shovel, skirt, skirt, match, spatula),row 5: (padlock, padlock, microwave, orange, printer, trash bin, tray)39Published as a conference paper at ICLR 2021Figure 34: A selection of challenging objects that are hard to be recognized by humans (continued from above).
Figure 34: A selection of challenging objects that are hard to be recognized by humans (continued from above).
Figure 35: Top: Some examples from the ImageNet dataset highlighting why multi-label annotations arenecessary: (a) Multiple objects. Here, the image label is desk but screen, monitor, coffee mug and manymore objects in the scene could count as correct labels, (b) Non-salient object. A scene where the target labelpicket fence is counter intuitive because it appears in the image background, while classes groom, bow-tie,suit, gown, and possibly hoopskirt are more prominently displayed in the foreground. c) Synonym or subsetrelationships: This image has ImageNet label African elephant, but can be labeled tusker as well, because everyAfrican elephant with tusks is a tusker. d) Unclear images: This image is labeled lake shore, but could alsobe labeled seashore as there is not enough information in the scene to distinguish the water body between alake or sea. Image compiled from (Shankar et al., 2020). Bottom: Some annotation problems in the ImageNetdataset. Example images along with their ground truth (GT) labels (red) and predicted classes (PC) by a model(blue) are shown. Top-left) Similar labels, Top-right) non-salient GT, Bottom-left) challenging images, andBottom-right) incorrect GT. Please see Lee et al. (2017) for details.
Figure 37: Top-1 (dashed lines) and Top-5 (solid lines) accuracy of models over 14 natural image distortionsat 3 severity levels (using object bounding box).
Figure 38: Top-1 (dashed lines) and Top-5 (solid lines) accuracy of models over 14 natural image distortionsat 3 severity levels (using full image).
Figure 39: Top-left) Model, trained on clean MNIST data, is attacked by FGSM, Top-right) Foregrounddetection over a model that has been trained on clean data. Bottom-left) Model, trained on noisy MNISTdata, is attacked by FGSM, Bottom-right) Foreground detection of a model that has been trained on noisy data.
Figure 40: Top-left) Model, trained on clean FashionMNIST data, is attacked by FGSM, Top-right) Foregrounddetection over a model that has been trained on clean data. Bottom-left) Model, trained on noisy FashionMNISTdata, is attacked by FGSM, Bottom-right) Foreground detection of a model that has been trained on noisy data.
Figure 41: Top: Distribution of object scale in ObjectNet dataset (113 annotated classes), Bottom: Distributionobject scale in ILSVRC2012-2014 single-object localization (dark green) and PASCAL VOC 2012 (light blue)validation sets. Object scale is fraction of image area occupied by an average object instance. Please see Fig.
Figure 42: Distribution of object location in ObjectNet (113 annotated classes).
