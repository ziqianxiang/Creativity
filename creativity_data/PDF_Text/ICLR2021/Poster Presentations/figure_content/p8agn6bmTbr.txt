Figure 1: (a) Checkerboard task. Given two binary target locations (left or right) with randomlyselected binary colors (red or green), one has to discern the dominant color in the checkerboard andreach to the target of the dominant color. On every trial, there is a correct color and direction choice.
Figure 2: SGD with random initialization leads to minimal representations. (a) Small FC net-work trained on the n = 2 checkerboard task. Max usable direction and color information: 1 bit.
Figure 3: Usable fine and coarse class information in a ResNet-18 on CIFAR-10. The fine classes(show in blue) correspond to the 10 CIFAR-10 classes. The coarse classes correspond a superclassconsisting of all the even and odd classes. We trained the network to output the correct coarse class,which corresponds to 1 bit of information. Through training epochs, while the validation accuracy(green dashed line) is increasing, the information about the coarse class also increases towards 1 bit.
Figure 4: Sensitivity to hyper-parameters. (a-c) The usable coarse and fine label informationthrough training with a batch size of 64, 256, and 512 (a batch size of 128 was used in Fig 3. Thelearning dynamics only undergo a compression at small batch sizes of 128 or less. The validationaccuracy is higher for smaller batch sizes as well. The plot of a batch size of 1024 is in Fig 8.
Figure 5: Different Architecture, Task, and learning schedule (a) Using an All-CNN architectureSpringenberg et al. (2015), we observe a similar trend in the learning dynamics of usable informa-tion, with a increase and decrease in the fine label information during the CIFAR-10 task. Thisdecrease does not lead to a completely minimal representation, though it does become close to min-imal. (b) We trained a ResNet-18 on the coarse labels in the CIFAR-100 task, and tracked theinformation the network had about the fine and coarse label through training. We find that the net-work converges to an approximately minimal representation, though it did not undergo a noticeableincrease and decrease in the fine label information, suggesting that this learning motif depends onthe structure of the task. (c) Pretraining the network to output the fine labels before epoch 20 led toimproved final performance (85.6% vs 83.5%) in (b). Note that the validation accuracy for the first20 epochs was the validation accuracy on the ‘fine’ labels task, and was the validation accuracy onthe ‘coarse’ task after epoch 20.
Figure 6: Usable color and direction information in a network through training following pretrainingthe network to output color, not direction. Pretraining occurred for the first 20 epochs, indicated bythe dashed red line. Subsequently, the network was trained to output direction, as in Fig 2. (a)Usable information for Small FC trained on the N = 2 CB task. Usable color information increasedin training, and decreased when the loss function changed. However, the asymptotic representationis not minimal. (b) Medium FC trained on N = 10 CB task. Similarly, the network formed arepresentation of color during pretraining, but the asymptotic representation is not minimal. (c)Medium FC trained on N = 20 checkerboard task. (d) Visualization of the Small FC network in (a)showing that an optimal representation is not formed. The asymptotic representation in the last areahas separate representations for red and green crosses. These should be overlapping in a minimalrepresentation.
Figure 7: (a) Final usable information and validation accuracy (green dashed line) as a function ofpretraining epoch for the CB task (n = 2) averaged over 8 random initializations. (b) Final usableinformation and accuracy as a function of pretraining epoch for the CB task (n = 10) averagedover 8 random initializations. (c) Final usable information and accuracy as a function of pretrainingepoch for the CB task (n = 20) averaged over 8 random initializations. (d) Final usable informationand accuracy as a function of pretraining epoch for the CB task (n = 25) averaged over 8 randominitializations. Error bars show the S.E.M.
Figure 8: (Left) Usable information for initial learning rate of 0.001 in CIFAR-10. The informationabout the fine labels does not decrease, and the validation accuracy only reaches 92%, in co ntrastto Fig 3 where the validation accuracy reached 96%. (Right) Usable information for batch size of1024 in CIFAR-10.
Figure 9: Evolution of usable information for eight random initializations for the2 CB task.
Figure 10: Evolution of usable information for eight random initializations for the n10 CB task.
Figure 11: Evolution of usable information for eight random initializations for the9.S3.C20 CB task.
Figure 12: Evolution of usable information for eight random initializations for the n = 2 CB taskwith 20 epochs of pretraining. If the the usable information was negative, indicating that the decoderoverfit, We set the usable information to 0. Note that this occurred for a very small number of points.
Figure 13: Evolution of usable information for eight random initializations for the n = 10 CB taskwith 20 epochs of pretraining.
Figure 14: Evolution of usable information for eight random initializations for the n = 20 CB taskwith 20 epochs of pretraining.
