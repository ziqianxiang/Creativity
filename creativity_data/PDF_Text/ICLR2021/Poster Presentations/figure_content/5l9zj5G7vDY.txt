Figure 1: A schematic representation of the spectrum of modeling strategies. Solid arrows with speech bubblesdenote (dynamic) messages being passed between sub-systems (dotted arrows denote the lack thereof). Gist: onone end of the spectrum, (Figure 1a), we have the strategy of abstracting each intersection as a sub-system thatinteract with neighboring sub-systems. On the other end of the spectrum (Figure 1c) we have the strategy ofmodeling the entire grid with one monolithic system. The middle ground (Figure 1b) we explore involves lettingthe model develop a notion of locality by (say) abstracting entire avenues with a single sub-system.
Figure 2: Schematic illustration of the proposed architecture. An observation is addressed to moduleswith embeddings situated in vicinity of its embedded location. Likewise, modules with embeddingsin the vicinity of an embedded query location are polled to produce a prediction.
Figure 3: Rollouts (OOD) with 5 bouncing balls, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate is reconstructed by stitching together 16 patches of size 11 × 11 produced by the models (queriedon a 4 × 4 grid). Gist: S2GRU succeeds at keeping track of all bouncing balls over long rollouthorizons (25 frames).
Figure 4: Performance metrics on OOD one-step forward prediction task. Gist: S2GRU outperformsall RNN baselines OOD.
Figure 5: Visualization of the spatial locations each module is responsible for modeling (i.e. theenclaves Xm, defined in Section 3). The central ball does not bounce, i.e. it is stationary in allsequences. Gist: the modules focus attention on challenging regions, e.g. the corners of the arenaand the surface of the fixed ball.
Figure 6: Ablation over the local and the non-local terms in the input and inter-cell attentionmechanisms (KMDPAs). For a set number of bouncing balls, each sub-plot shows how the balancedaccuracy changes with the fraction of views (crops) available to the model. Gist: Both local andnon-local terms in KMDPA contribute to the overall performance. The non-local term is moreimportant for the input attention, whereas the local term is more important for the inter-cell attention.
Figure 8: Performance metrics (larger the better) as a function of the probability that an agent willnot supply information to the world model but still query it. Gist: while all models lose performanceas fewer agents share observations, we find S2RMs to be most robust.
Figure 7: The effect of removing ran-dom modules at test time. Gist: Perfor-mance degrades gracefully as modulesare removed, suggesting that modulescan function even when their counter-parts are removed, and that there is lim-ited co-adaptation between them.
Figure 9: Joint visualization of spatial enclaves and the interaction graph between modules in thegrid-world environment of Eichenberger et al. (2019), as detailed in Appendix A. The images showwhich spatial locations a module attends to via the local attention (spatial enclaves), whereas thepresence of an edge indicates that the corresponding modules may interact via inter-cell attention.
Figure 10: Human readable illustrations of the Starcraft2 (SMAC) scenarios we consider in this work.
Figure 11: Baseline encoder and decoder architectures for the Bouncing Ball task.
Figure 12: S2RM encoder and decoder architectures for the Bouncing Ball task.
Figure 13: Baseline encoder and decoder architectures for the Starcraft2 task.
Figure 14: S2RM encoder and decoder architectures for the Starcraft2 task.
Figure 15: Rollouts (OOD) with 1 bouncing ball, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate was reconstructed by stitching together 11 × 11 patches from the models (queried on a 4 × 4grid).
Figure 16: Rollouts (OOD) with 2 bouncing balls, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate was reconstructed by stitching together 11 × 11 patches from the models (queried on a 4 × 4grid).
Figure 17:	Rollouts (ID) with 3 bouncing balls, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate was reconstructed by stitching together 11 × 11 patches from the models (queried on a 4 × 4grid).
Figure 18:	Rollouts (OOD) with 4 bouncing balls, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate was reconstructed by stitching together 11 × 11 patches from the models (queried on a 4 × 4grid).
Figure 19: Rollouts (OOD) with 6 bouncing balls, from top to bottom: ground-truth, S2GRU, RIMs,RMC, LSTM. Note that all models were trained on sequences with 3 bouncing balls, and the globalstate was reconstructed by stitching together 11 × 11 patches from the models (queried on a 4 × 4grid).
Figure 20: Balanced accuracy (arithmetic mean of recall and specificity) achieved by all evaluatedmodels for one-step forward prediction task with various number of balls and fractions of availableviews. All models were trained on video sequences with 3 balls and a constant number of crops /views (10 views, corresponding to the right-most columns labelled 1.0). The color map is consistentacross all plots.
Figure 21: F1-Score (harmonic mean of precision and recall) achieved by all evaluated models forone-step forward prediction task with various number of balls and fractions of available views. Allmodels were trained on video sequences with 3 balls and a constant number of crops / views (10views, corresponding to the right-most columns labelled 1.0). The color map is consistent across allplots.
