Figure 1: Left: The test and train error curves of an over-parameterized 5-layer convolutional net-work trained on the CIFAR-10 training set with 20% random label noise. As observed by Nakkiranet al. (2020a), the performance shows a double descent behavior. Right: As we show here, the riskof a regression problem can be decomposed as the sum of two bias-variance tradeoffs. Both exam-ples: Early stopping the training where the test error achieves its minima is critical for performance.
Figure 2: Early stopped least squares risk for a two-feature Gaussian linear model. a: Two U-shaped bias-variance tradeoffs Ui(t) for the parameters θɪ = 1.5,σι = 1,ηι = 0.05 (bias-variance1) and θ2 = 10,σ2 = 0.15,η2 = 0.05 (bias-variance 2), along with their sum (1+2) which deter-mines the risk. b: Same plot, but this time the bias-variance tradeoff U2 (t) is shifted to the left byincreasing the stepsize η2 according to Proposition 1 (yielding bias-variance tradeoff 3), so that itsminimum overlaps with that of bias-variance tradeoff 1. This eliminates double descent and givesbetter performance. c: The resulting risk curves before and after elimination, demonstrating that theminimum of the risk after double descent elimination is smaller than before elimination.
Figure 3: Top row: Risk of the two-layer neural network trained on data drawn from a linearmodel with diagonal covariance matrix with geometrically decaying variances. The risk has a doubledescent curve unless we either i) initialize the first layer with a smaller initialization strength ω thanthe second one ν, or we ii) choose a smaller stepsize for the weights in the second layer. Bothimproves the risk as suggested by the theory. Bottom row: The norms kvi,Wk22 and kvi,vk22measure to what extend the singular values σi are associated with the weights in the first (W) andsecond (v) layer respectively. Double descent occurs when singular values are mostly associatedwith the second layer, because then those weights are learned faster relative to the first layer weights.
Figure 4: Mitigating double descent for a 5-layer CNN. Left: Norm of the parts of the right singularvectors of the Jacobian associated with the weights in the convolutional vi,C and fully connectedlayers vi,F as a function of the singular values, σi, showing that large singular values pertain mostlyto the fully connected layer. This causes the fully connected layer to be learned faster than theconvolution layer. Middle and Right: Performance when trained with the i) same stepsize for alllayers, and ii) a smaller stepsize for the fully connected layer. Decreasing the learning rate of thefully connected layer causes itto be learned at a similar speed as the convolutional layers and therebyeliminates double descent and increases performance (i.e., the minima ofii is smaller than that of i).
Figure 5: The risk of early-stopped gradient least-squares R(θt) based on numerical simulation ofthe Gaussian model along with the risk expression R(θt) given in (1). We averaged over 100 runs ofgradient descent, and the shaded region corresponds to one standard deviation over the runs. It canbe seen that the risk expression slightly underestimates the true risk, but other than that describesthe behavior of the risk well.
Figure 6: Left: Test error of the ResNet-18 trained with the i) same stepsize for all layer, andwith ii) a smaller stepsize for the latter half of the layers. Decreasing the learning rate of the lastlayers causes the last layers to be learned at a similar speed as the first and thereby eliminates doubledescent. Right: The training error curves for i) and ii).
Figure 7: Bias and variance as a function on training epochs for training a 5-layer CNN untilconvergence. The overall variance is increasing, and the overall bias has a double-descent likeshape. The training and test error curves show the interpolation of the training set and doubledescent behavior of the error in this interval.
Figure 8: Left: As we show here, multiple descent curves can arise for a regression problem. Here,the risk can be decomposed as the sum of three bias-variance tradeoffs. Right: Risk of the two-layerneural network trained on the data drawn from a linear model with diagonal covariance matrix withgeometrically decaying variances and additive noise yields multi-descent behavior. Both examples:Scaling the stepsizes of the different layers/components eliminates the multi-descent similar to thecase of the double descent and improves the optimal early stopping performance for both the linearmodel and the two-layer neural network as predicted by our theory.
