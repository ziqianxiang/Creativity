Figure 1: Overview of our BiPointNet on PointNet base model, applying Entropy-Maximizing Ag-gregation (EMA) and Layer-wise Scale Recovery (LSR). EMA consists of the transformation unitand the aggregation unit for maximizing the information entropy of feature after binarization. LSRwith the learnable layer-wise scaling factor α is applied to address the scale distortion of bi-linearlayers (which form the BiMLPs), flexibly restore the distorted output to reasonable valuesZhang et al., 2021) has been studied extensively by the model binarization community, the methodsdeveloped are not readily transferable for 3D point cloud networks due to the fundamental differ-ences between 2D images and 3D point clouds. First, to gain efficiency in processing unordered 3Dpoints, many point cloud learning methods rely heavily on pooling layers with large receptive fieldto aggregate point-wise features. As shown in PointNet (Qi et al., 2017b), global pooling providesa strong recognition capability. However, this practice poses challenges for binarization. Our analy-ses show that the degradation of feature diversity, a persistent problem with binarization (Liu et al.,2019a; Qin et al., 2020b; Xie et al., 2017), is significantly amplified by the global aggregation func-tion (Figure 2), leading to homogenization of global features with limited discriminability. Second,the binarization causes immense scale distortion at the point-wise feature extraction stage, whichis detrimental to model performance in two ways: the saturation of forward-propagated featuresand backward-propagated gradients hinders optimization, and the disruption of the scale-sensitivestructures (Figure 3) results in the invalidation of their designated functionality.
Figure 2: Aggregation-induced feature homogenization. (a) shows the activation of each test samplein a batch of ModelNet40. In (b)-(d), the single feature vectors pooled from all points are mapped tocolors. The diversity of colors represents the diversity of pooled features. The original aggregationdesign is incompatible with binarization, leading to the homogenization of output features in (c),whereas our proposed EMA retains high information entropy, shown in (d)the binary feature B ∈ {-1, 1}i×k, where i takes n when the feature is modeled independently andtakes 1 when the feature is aggregated globally. The single unit is thus represented asB = Sign(Y) = sign(Ω(X)).	(3)Similarly, when B, Y and X denote the random variables sampled from B, Y and X, we representtheir probability mass function as pB (b), pY (y) and pX (x).
Figure 4: (a) Information entropy of the aggregated features. With EMA, our BiPointNet achieves ahigher information entropy. (b) Regularizer loss comparison. Our PointNet has a low loss, indicatingthat the scale distortion is reduced and T-Net is not disrupted. (c) Ratio of zero-gradient activationsin back-propagation. LSR alleviates the scale distortion, enhancing the optimization processis not related to n. Hence, average pooling can be regarded as a flexible alternative because itsperformance is independent of the input number n.
Figure 3: Scale Distortion. Fig-ures (b)-(d) show the trans-formed input. Compared withthe input (a), the scales of (b) infull-precision PointNet and (c)in our BiPointNet are normal,while the scale of (d) in BNN issignificantly distortedWe discuss two major impacts of the scale distortion on the performance of binarized point cloudlearning models. First, the scale distortion invalidates structures designed for 3D deep learning that6Published as a conference paper at ICLR 2021Table 1: Ablation study for our BiPointNet of various tasks on ModelNet40 (classification),ShapeNet Parts (part segmentation), and S3DIS (semantic segmentation). EMA and LSR and com-plementary to each other, and they are useful across all three applicationsMethod	Bit-width	Aggr.	ModelNet40	ShapeNet Parts	S3DIS				OA	mIoU	mIoU	OAFull Prec.	32/32	MAX	88.2	84.3	54.4	83.5	32/32	AVG	86.5	84.0	51.5	81.5BNN	1/1	MAX	7.1	54.0	9.5	45.0
Figure 5: (a) Time cost comparison. Our BiPointNet achieves 14.7× speedup on ARM A72 CPUdevice. (b) Storage usage comparison. Our BiPointNet enjoys 18.9× storage saving on all devices.
Figure 6: Structures of different PointNet implementations. Three fully connected layers are used inall six variants: Full Precision FC, Binarization FC with BN, Binarization FC w/o BN. Full PrecisionFC contains a full precision fully connected layer and a ReLU layer. Original BN is merged intothe later layer. Binarization FC with BN also contains two layers: a quantized fully connected layerand a batch normalization layer. Binarization FC w/o BN is formed by a single quantized fullyconnected layerB.3	Ablation analysis of time cost and quantization sensitivitySetup	Bit-width	FL	LL	BN	OA	Storage & Saving Ratio	Time & Speedup Ratio								A72	A53(a)	32/32	32/32	32/32	Merged	86.8	3.16MB / 1.0×	131ms / 1.0×	67ms / 1.0×(b)	1/1	32/32	32/32	Not Merged	85.62	0.17MB / 18.9×	9.0ms / 14.7×	5.5ms / 12.1×(c)	1/1	32/32	1/1	Not Merged	84.60	0.12MB / 26.3×	9.0ms / 14.7×	5.3ms / 12.6×(d)	1/1	1/1	32/32	Not Merged	5.31	0.16MB / 19.7×	11.5ms / 11.4×	6.5ms / 10.3×(e)	1/1	1/1	1/1	Not Merged	4.86	0.12MB / 26.3×	11.4ms / 11.5×	6.4ms / 10.4×(f)	1/1	32/32	32/32	Not Used	85.13	0.15MB / 21.0×	8.1ms / 16.1×	4.8ms / 13.9×Table 4: Comparison of different configurations in deployment on ARM devices. The storage-savingratio and speedup ratio are calculated according to the full precision model as the first row illustrates.
Figure 7: The information entropy of BNN, XNOR and our BiPointNetTable 3 shows that XNOR can alleviate the aggregation-induced feature homogenization. The point-wise scaling factor helps the model to achieve comparable adjustment capacity as full-precisionlinear layers. Therefore, although XNOR suffers from feature homogenization at the beginning ofthe training process, it can alleviate this problem with the progress of training and achieve acceptableperformance, as shown in Figure 7.
