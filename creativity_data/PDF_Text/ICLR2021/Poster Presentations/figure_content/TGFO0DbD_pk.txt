Figure 1: Supe-RL overview.
Figure 2: Left: indoor navigation (GA, Rainbow, ERL-R, GRainbow, SGRainbow). Right: aquaticnavigation (GA, PPO, ERL, GPPO). (A, C) Average success rate. (B, D) Average total reward.
Figure 3: Left: cumulative number of Supe-RL genetic soft updates in continuous aquatic naviga-tion. Right: cumulative number of injection of the DRL agent using ERL in the same environment.
Figure 4: Performance of Supe-RL, PPO and ERL in MuJoCo benchmarks: (A) Reacher-v2; (B)HalfCheetah-v2; (C) Hopper-v2; (D) Ant-v2.
Figure 5: Comparison with CEM-RL in aquatic navigation: (A) average success rate; (B) averagetraining time. Comparison with PDERL in the navigation tasks: (C) Value-Based implementation ofPDERL in the discrete task; (D) Policy-Gradient implementation of PDERL in the continuous task.
Figure 6: Bound analysis informal verification tools.
Figure 7: Additional value-based experiments: Average success rate for (A) SGD and Adam GRain-bow; (B) Tuning of Ï„0 for SGRainbow; (C) Ablation on population experiences in agent buffer.
Figure 9: (A) Average success rate and (B) total reward when the bad initial seed is removed fromthe indoor simulations. (C) Average success rate and (D) total reward when the bad initial seed isremoved from the aquatic simulations.
Figure 8: (A) Average success rate and (B) total reward considering only the simulation with thebad initial seed in the indoor navigation. (C) Average success rate and (D) total reward consideringonly the aquatic simulation with the bad initial seed.
