Figure 1: The network architecture for the inverse dynamics model used in GLAMOR. ResNets areused to encode state features and an LSTM predicts the action sequence.
Figure 2: Both in Atari and on tasks from the Deepmind Control Suite, GLAMOR outperforms priormethods. The goal achievement rate is averaged over all games / control tasks and over three seeds.
Figure 3: (a) The agent starts in the center and must travel to the goal tile. Top shows the rate atwhich the agent eventually achieved the goal and bottom shows the rate at which the agent achievedthe goal with the shortest available path. The amount of compute used for planning is shown onthe x-axis. As the planning budget increases, both the number of successfully reached goals and thenumber of goals achieved optimally improves substantially. Brighter means a higher achievementrate. (b) In “naive-end”, the agent greedily tries to take a shortest path to the goal for T timestepsand is evaluated at the end. In “plan-end”, the agent explicitely constructs a plan to achieve the goalstate at the end of its trajectory. GLAMOR (Ours) can choose to terminate its episode early.
Figure 4: Using intermediate information to guide the planning process helps GLAMOR achievemore goals than when it only looks at the estimated probability of reaching a goal at the end of theepisode.
Figure 5:	Hyperparameters used to train GLAMOR.
Figure 6:	In this experiment, we test how changing the training policy affects performance in Pong.
Figure 7:	When training agents with off-policy data collected with a random policy, GLAMORoutperfoms GCSL and can achieve most goals.
Figure 8:	Goal achievement rates for DM Control and Atari. Searching for a high scoring actionsequence results in more goals achieved. However, even when using compute equivalent to a model-free agent, GLAMOR performs remarkably well.
Figure 9:	Training Curves for Atari Tasks. GLAMOR achieves more goals (often with many fewersteps) than both GCSL and DISCERN.
Figure 10:	Training Curves for Control Tasks. GLAMOR achieves more goals (often with manyfewer steps) than both GCSL and DISCERN.
Figure 11: Goal states (above) and states achieved by the fully trained GLAMOR agent (below)averaged over 5 trials for each Atari game tested. Variance comes from environment and planningstochasticity. Note that on most games, GLAMOR learns to control the positions of both directlyand indirectly controllable objects in the frame.
Figure 12: Goal states (above) and states achieved by the fully trained agent (below) averaged over 5trials for each control task tested. Variance comes from planning stochasticity since the environmentdynamics are deterministic. In most environments, GLAMOR learns to control the agent’s state tomatch the visually specified goal. Note that in the finger environment, GLAMOR learns to controlthe position of the finger despite not often being able to control the angle of the spinner.
