Figure 1: Theoretical analysis of cross-entropy in top-k and top-p sampling: (a) approximationto H (PMk , PM) obtained in Thm. 2, (b) approximation to H(PMp, PM) obtained in Thm. 4 fors = 1.1, N = 50, 000. Note that H(PMk , PM) grows sharply for small values of k and the growthis negligible for large values of k, whereas H(PMp, PM) grows nearly linearly with p.
Figure 2:	Cross-entropy rate in different sampling methods: (a) top-k, (b) top-p, and (c) mirostat.
Figure 3:	Percentage repetition vs. observed cross-entropy rate for (a) different sampling methods,(b) different temperature values, T , (c) for n-gram tokens and different sampling methods, (d) dif-ferent LMs. Note that repetitions vary heavily with observed cross-entropy rate in the generated textand mostly independent of sampling method used. Further, large LMs like GPT-2-XL with 1558Mparameters seem to have slightly less repetitions for the same cross-entropy rate.
Figure 4:	Percentage repetition vs. (a) observed cross-entropy rate for top-k sampling with varyingk, top-k sampling with k = 3, 6 and varying repetition penalties, (b) repetition penalties for top-ksampling with k = 3, 6.
Figure 5:	Cross-entropy rate vs. number of tokens generated for different sampling methods. Weobserve (a) boredom trap for small values of k and p, (b) confusion trap for large values of k andp, (c) human-like cross-entropy rate for moderate k and p, (d) mirostat sampling that shows controlover cross-entropy over varying lengths of texts.
Figure 6: Human evaluation of top-p and mirostat sampling for (a) fluency, (b) coherence, and (c)quality. Detecting human generated text from texts generated by GPT-2 under (d) top-p sampling,(e) mirostat sampling. For a given observed cross-entropy rate, notice the similarity between humanevaluations of top-p and mirostat sampling across all values of observed cross-entropy rate.
Figure 7: Relation between surprise values and attributes of generated text such as repetitions andincoherence: (a) top-p sampling with p = 0.4 and average observed surprise = 1.471, (b) top-psampling with p = 0.4 and average surprise = 1.887, (c) top-p sampling with p = 1.0 and averageobserved surprise = 6.342, (d) mirostat sampling with target surprise, Ï„ = 1.8 and average observedsurprise = 1.828, (e) human-generated text, average observed surprise = 5.301. We observe thatrepetitions in generated texts are correlated with dips in surprise values, whereas incoherence iscorrelated with large and increasing surprise values with indices.
Figure 8: Theoretical analysis of surprise values in top-k sampling under Zipfian statistics. We notethat surprise values increase sharply for small values of k, whereas surprise values hardly changefor large values of k .
Figure 9: Theoretical analysis of surprise values in top-p sampling. We observe that unlike top-ksampling, surprise values grow linearly with p in top-p sampling.
Figure 10:	Comparing mirostat Alg. 1 with mirostat 2 Alg. 2: (a) Observed vs. target cross-entropyrate, (b) percentage n-gram repetitions vs. observed cross-entropy rate. We observe that both algo-rithms give equal performance in terms of controlling cross-entropy and repetitions.
Figure 11:	Comparing mirostat Alg. 1 with mirostat average Alg. 3: (a) Observed vs. target cross-entropy rate, (b) percentage n-gram repetitions vs. observed cross-entropy rate. We observe thatmirostat average performs poorly in controlling cross-entropy and has worse 6-gram percentagerepetition vs. observed cross-entropy rate curve.
Figure 12:	Percentage repetition vs. observed cross-entropy rate for (a) n-gram tokens and differentsampling methods, (b) different temperature values, T, for CTRL language model.
