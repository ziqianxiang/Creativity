Figure 1: The poisoning pipeline. Poisoned images (labrador retriever class) are inserted into adataset and cause a newly trained victim model to mis-classify a target (otter) image. We showsuccessful poisons for a threat model where 0.1% of training data is changed within an '∞ bound ofε = 8. Further visualizations of poisoned data can be found in the appendix.
Figure 2: Average batch cosine similarity, perepoch, between the adversarial gradient VLadv(θ)and the gradient of each mini-batch VL(θ) for apoisoned and a clean ResNet-18. Crucially, thegradient alignment is strictly positive.
Figure 3: Poisoning ImageNet. Left: Clean images (above), with their poisoned counterparts (below)from a successful poisoning of a randomly initialized ResNet-18 trained on ImageNet for a poisonbudget of 0.1% and an '∞ bound of ε = 8. Right Top: ResNet-18 results for different budgets andvarying ε-bounds. Right Bot.: More architectures (Simonyan & Zisserman, 2014; He et al., 2015;Sandler et al., 2018) with a budget of 0.1% and ε = 16.
Figure 4: Defense strategies against poisoning.
Figure 5: The bound considered in Prop. 1, evaluated during training of a poisoned and a cleanmodel, using a practical estimation of the lower bound via αkL ≈ 1. This is an upper bound of αkLas ak < L is necessary for the convergence of (clean) gradient descent.
Figure 6: Clean images (above), with their poisoned counterparts (below) from a successful poisoningof a ResNet-18 model trained on ImageNet. The poisoned images (taken from the Labrador Retrieverclass) successfully caused mis-classification of a target (otter) image under a threat model given by abudget 0.1% and an '∞ bound of ε = 8.
Figure 7: Clean images (above), with their poisoned counterparts (below) from a successful poisoningof a randomly initialized ResNet-18 trained on ImageNet. The poisoned images (taken from theLabrador Retriever class) successfully caused mis-classification of a target (otter) image under athreat model given by a budget of 0.1% and an '∞ bound of e = 16.
Figure 8: Clean images (above), with their poisoned counterparts (below) from a successful poisoningof a Google Cloud AutoML model trained on ImageNet. The poisoned images (taken from theLabrador Retriever class) successfully caused mis-classification of a target (otter) image. This isaccomplished with a poison budget of 0.1% and an '∞ bound of ε = 32 - the black-box attack againstautoML requires an increased perturbation magnitude, in contrast to the other gray-box experimentsin this work.
Figure 9:	Cross entropy loss (Top) and accuracy (Bottom) for a given target with its adversarial label(left), and with its original label (right) shown for a poisoned and a clean ResNet-18. The cleanmodel is used as victim for the poisoned model. The loss is averaged 8 times for the poisoned model.
Figure 10:	CIFAR-10 comparison without time and memory constraints for a ResNet18 with realistictraining. Budget 1%, ε = 16. Note that the x-axis is logarithmic.
Figure 11: Average batch cosine similarity, per epoch, between the adversarial gradient and thegradient of each mini-batch (left), and with its clean counterpart VLt(θ) := VθL(χt, yt) (right) for apoisoned and a clean ResNet-18. Each measurement is averaged over an epoch. Learning rate dropsare marked with gray vertical bars.
Figure 12: Ablation Studies. Left: avg. poison success for Euclidean Loss, cosine similarity and thePoison Frogs objective (Shafahi et al., 2018) for thin ResNet-18 variants. Right: Avg. poison successvs number of pretraining epochs.
Figure 13: Direct transfer results on common architectures. Averaged over 10 runs with budget of0.1% and ε-bound of 16. Note that for these transfer experiments, the model was only trained onthe "brewing" network, without knowledge of the victim. This shows a transferability to unknownarchitectures.
