Figure 1: Quant-Noise trains models to be resilient to inference-time quantization by mimicking theeffect of the quantization method during training time. This allows for extreme compression rateswithout much loss in accuracy on a variety of tasks and benchmarks.
Figure 2: Performance as a function of model size. We compare models quantized with PQ and trained withthe related Quant-Noise to the state of the art. (a) Test perplexity on Wikitext-103 (b) Dev Accuracy on MNLI(c) ImageNet Top-1 accuracy. Model size is shown in megabytes on a log scale. Red and gray coloring indicatesexisting work, with different colors for visual distinction.
Figure 3: Effect of Quantization Parameters. We report the influence of the proportion of blocksto which we apply the noise. We focus on Transformer for Wikitext-103 language modeling. Weexplore two settings: iPQ and int8. For iPQ, We use 夕Proxy.
Figure 4:	Quantizing with a larger number of centroids. Results are shown on Wikitext-103 valid.
Figure 5:	(a) Effect of Initial Model Size for more shallow models (b) Effect of Initial Model Sizemore skinny models25.022.520.017.515.0Order of Quantizationattn att embemb ffiι ffiιemb ffiι att ffiι att embffiι emb fl&i att emb att24.0I 23.5t 23.0I 22.5>22.0Quantization Block Size324 8	16
Figure 6:	Effect of Quantization on Model Structures. Results are shown on the validation set ofWikitext-103. (a) Quantizing Attention, FFN, and Embeddings in different order. (b) More Extremecompression of different structures.
