Figure 1: (a) DAG of a spatial dependency layer. The input feature vector (red node) is gradu-ally refined (green nodes) as the computation progresses through the four sub-layers until the outputfeature vector is produced (blue node). In each sub-layer, the feature vector is corrected based oncontextual information. Conditional maps within sub-layers are implemented as learnable determin-istic functions with shared parameters; (b) VAE with SDN decoder reconstructing a ‘celebrity’.
Figure 2: Spatial dependency layer. (a) a 3-stage pipeline; (b) dependencies between the inputXLk and the output XLk+1 modeled by a spatial dependency layer Lk . Solid arrows representdirect, and dashed indirect (non-neighboring) dependencies. Note that the sub-layers from Figure1a are regarded as ‘latent’ here; (c) dependencies modeled by a 2 × 2 convolutional layer.
Figure 4: SDN-VAE ResNet block as modified IAF-VAE ResNet block. Two notable changes(marked in red) include applying: (a) ResSDN layers instead of convolutional layers on the top-down path; (b) gated rather than a sum residual; (figure adapted from Kingma et al. (2016))Other notable modifications include: (a) gated residual instead of a sum residual in the ResNet block(Figure 4), for improved training stability; (b) more flexible observation model based on a discretizedmixture of logistics instead of a discretized logistic distribution, for improved performance; and (c)mixed-precision (Micikevicius et al., 2018), which reduced memory requirements thus allowingtraining with larger batches. Without ResSDN layers, we refer to this architecture as IAF-VAE+.
Figure 5: Linear interpolation between test images. Given a pair of images from the test dataset(on the far right and far left), linear interpolation was performed on the layer 5 at a temperature of0.9 (the sequence above) and on the layer 4 at a temperature of 1.0 (the sequence below).
Figure 6: (left) Unconditional sampling. Samples were drawn from the SDN-VAE prior at varyingtemperatures; (right) Sampling around a test image. Conditioned on the top 2 layers of the latentcode of the image in the center, the surrounding images were sampled at a temperature of 1.0.
Figure 7: β-VAE disentanglement results. The plots compare the CNN and SDN-based VAEarchitectures, with respect to the β-VAE (left) and FactorVAE (right) disentanglement metrics.
Figure 8: ResSDN layer and the baseline blocks.
Figure 9: MSE-based nearest neighbors. On the left, shown are images from Figure 6. On theright, shown are nearest neighbors in terms of mean squarred error (MsE), found in the training dataset. There are no signs that generated samples are replicas of the training ones.
Figure 10: Additional samples synthesized at different temperatures. As we decrease the temper-ature, getting closer to the mean of the prior, the photographs become smoother i.e. more ’generic’.
Figure 11: Sampling around a test image, for varying temperatures and number of fixed layers.
Figure 12: β-VAE learning curves for CNN and SDN-based VAE architectures. Presented arethe following quantities measured over the course of training: (a) evidence lower bound (ELBO);(b) conditional log-likelihood (reconstruction) term; (c) KL divergence term; (d) β-scaled KL diver-gence term; Top-to-bottom, the rows are related to β = {1, 32, 100, 300, 500}. Each pair (β value,VAE architecture) is trained for 10 different random seeds. Linear β-annealing procedure was per-formed, where β was increased from 0 to its final value across the span of 100K training iterations(the end of the annealing procedure is denoted by the vertical line in the plots of the column (c)).
