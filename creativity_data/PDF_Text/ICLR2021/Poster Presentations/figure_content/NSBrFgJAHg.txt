Figure 1: Despite GNN model sizes rarely exceeding1MB, the OPs needed for inference grows at leastlinearly with the size of the dataset and node features.
Figure 2: While CNNs operate on regular grids,GNNs operate on graphs with varying topology. Anodeâ€™s neighborhood size and ordering varies forGNNs. Both architectures use weight sharing.
Figure 3: Analysis of values collected immediately after aggregation at the final layer of FP32 GNNS trained onCora. Generated using channel data collected from 100 runs for each architecture. As in-degree grows, so doesthe mean and variance of channel values after aggregation.
Figure 4: High-level view of the stochastic element of Degree-Quant. Protected (high in-degree) nodes, in blue,operate at full precision, while unprotected nodes (red) operate at reduced precision. High in-degree nodescontribute most to poor gradient estimates, hence they are stochastically protected from quantization more often.
Figure 5: qmax with absolute min/max and percentileranges, applied to INT8 GCN training on Cora. We ob-serve that the percentile max is half that of the absolute,doubling resolution for the majority of values.
Figure 6: Analysis of how INT8 GAT performancedegrades on Cora as individual elements are reducedto 4-bit precision without DQ. For GAT the messageelements are crucial to classification performance.
Figure 8: Degradation of INT8 GIN on Cora as indi-vidual elements are converted to INT4 without Degree-Quant.
Figure 7: Degradation of INT8 GCN on Cora as indi-vidual elements are converted to INT4 without Degree-Quant.
Figure 9: In-degree distribution for each of the six datasets assessed. Note that a log y-axis is used for alldatasets except for MNIST and CIFAR-10.
Figure 10: Validation loss curves for GIN models evaluatedon REDDIT-BINARY. Results averaged across 10-fold cross-validation. We show four DQ-INT8 experiments each with a differ-ent values for (pmin,pmax) and our FP32 baseline.
Figure 11: Diagram representing how DQ makes use of a topology-aware quantization strategy that is bettersuited for GNNs. The diagram illustrates this for a GCN layer. At every training stage, a degree-based mask isgenerated. This mask is used in all quantization layers located after each of the stages in the message-passingpipeline. By retaining at FP32 nodes with higher-degree more often, the noisy updates during training have alesser impact and therefore models perform better, even at INT4.
Figure 12: Diagram representing how nQAT is implemented for GNNs. The diagram illustrates this for a GCNlayer. The stochastic stage only takes place when quantizing the weights, the remaining of the quantizationmodules happen following a standard QAT strategy. A QAT diagram would be similar to this one but fullyquantizing the weights.
Figure 13: Diagrams representing how the output graph-summarization stages for graph-level tasks (e.g. graphclassification, graph regression) are implemented when making use of DQ (left) and nQAT (right). GNNsmaking use of DQ during the node-aggregation stages (see fig. 11), do not use the stochastic element of DQ inthe output MLP layers but still make use of percentiles. For models making use of nQAT, the final MLP stillmakes use of stochastic quantization of weights.
