Figure 1:	Overview of our two-stage framework for building deep one-class classifier. (a) In the firststage, we learn representations from one-class training distribution using self-supervised learningmethods, and (b) in the second stage, we train one-class classifiers using learned representations.
Figure 2:	One-class classification on different levels of uniformity of inlier distributions. (a) Whenrepresentations are uniform, isolating outliers is hard. (b) Reducing uniformity makes boundary be-tween inlier and outlier clear. (c) Distribution augmentation allows inlier distribution more compact.
Figure 3:	Distribution-augmented contrastivelearning. Not only learning to discriminate differ-ent instances from an original distribution (e.g.,two images of different dogs on the left), it alsolearns to discriminate instances from differentdistributions created via augmentations, such asrotations (e.g., two images of the same dog withdifferent rotation degrees on the top).
Figure 4: Ablation study of contrastive representations trained with different batch sizes. We use(a) the MMD distance between the representations and the data sampled from uniform distributionsto measure uniformity. Small MMD distance means being similar to uniform distribution. We alsoreport (b) one-class classification performance in AUCs evaluated by kernel OC-SVMs and (c) theperformance with MLP heads of different depths.
Figure 5: Realistic evaluation of anomaly detection under (5a) unsupervised and (5b, 5c) one-class orsemi-supervised settings. For unsupervised settings, we either learn representation or build detectorfrom a training set containing both inlier and outlier data without their labels. For one-class or semi-supervised settings, we learn from a training set containing only a small amount of one-class (inlier)data. We report AUCs on CIFAR-10 and OC-SVM with RBF kernel is used for evaluation.
Figure 6: Visual explanations of deep one-class classifiers on cat-vs-dog and CelebA eyeglassesdatasets. (a) input images, (b-e) images with heatmaps using integrated gradients [62], and (f-i)those using GradCAM [61]. RotNet*: RotNet + KDE. More examples are in Appendix A.6.
Figure 7: AUCs of Contrastive representationstrained from various augmented distributionson CIFAR-10. From left to right, we train byaCCumulating distributions. For example, anentry with “+rot90, hflip” is trained with 4distributions, i.e., original, hflip, rot90 androt90, hflip.
Figure 8: Scatter plots between uniformity metric (log(MMD)) and one-class classification AUCson CIFAR-10. MMD scores are obtained using g ◦ f. For AUCs, We show both evaluated using g ◦ f(left) and f (right). Data points are obtained from vanilla and DistAug contrastive representationstrained with batch sizes of 32, 64, 128, 256 and 512 using 5 different random seeds.
Figure 9: Self-supervised representations are trained from different data sizes, from 50 to 5000, onCIFAR-10. Standard deviations are obtained by sampling subsets 5 times. Classifiers are trainedwith the full (5000) train set for fair evaluation of representations. We provide baseline representa-tions of ResNet-18 using random weights and ImageNet-pretrained ResNet-50. Standard deviationsare computed by running 5 times with different seeds.
Figure 10:	Classification performance under unsupervised learning setting of representation andclassifier. For unsupervised setting, training set contains both inlier and outlier examples withouttheir labels, whereas for one-class setting, training set contains only inlier examples. For represen-tations trained with different outlier ratios, classification performances are evaluated with differentclassifiers, such as OC-SVMs with RBF and linear kernels, and Gaussian density estimation.
Figure 11:	Classification performance under unsupervised representation learning and one-classclassifier learning settings. For unsupervised representation learning, training set contains both inlierand outlier examples without their labels. For one-class classifier learning, a small portion of inlierdata is used for training an OC-SVM with RBF kernel.
Figure 12:	Visual explanations on CelebA eyeglasses dataset. (a) input images, (b-e) images withheatmaps using integrated gradients [62], and (f-i) those using GradCAM [61]. RotNet*: RotNet +KDE.
Figure 13: Visual explanations on CelebA eyeglasses dataset. (a) input images, (b-e) images withheatmaps using integrated gradients [62], and (f-i) those using GradCAM [61]. RotNet*: RotNet +KDE.
Figure 14: Visual explanations on Cat-Vs-dog dataset. (a) input images, (b-e) images with heatmapsusing integrated gradients [62], and (f-i) those using GradCAM [61]. RotNet*: RotNet + KDE.
Figure 15: Visual explanations on Cat-Vs-dog dataset. (a) input images, (b-e) images with heatmapsusing integrated gradients [62], and (f-i) those using GradCAM [61]. RotNet*: RotNet + KDE.
Figure 16: Visualization of defect localization on bottle category of MVTec dataset [31] using rep-resentations trained with rotation prediction on patches. From left to right, defective input data intest set, ground-truth mask, and localization visualization via heatmap.
Figure 17: Visualization of defect localization on cable category of MVTec dataset [31] using rep-resentations trained with rotation prediction on patches. From left to right, defective input data intest set, ground-truth mask, and localization visualization via heatmap.
Figure 18: Visualization of defect localization on capsule category of MVTec dataset [31] usingrepresentations trained with rotation prediction on patches. From left to right, defective input datain test set, ground-truth mask, and localization visualization via heatmap.
Figure 19: Visualization of defect localization on screw category of MVTec dataset [31] using rep-resentations trained with rotation prediction on patches. From left to right, defective input data intest set, ground-truth mask, and localization visualization via heatmap.
Figure 20: Visualization of defect localization on transistor category of MVTec dataset [31] usingrepresentations trained with rotation prediction on patches. From left to right, defective input datain test set, ground-truth mask, and localization visualization via heatmap.
Figure 21: Visualization of defect localization on grid category of MVTec dataset [31] using repre-sentations trained with rotation prediction on patches. From left to right, defective input data in testset, ground-truth mask, and localization visualization via heatmap.
Figure 22: Visualization of defect localization on leather category of MVTec dataset [31] usingrepresentations trained with rotation prediction on patches. From left to right, defective input datain test set, ground-truth mask, and localization visualization via heatmap.
