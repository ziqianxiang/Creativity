Figure 1: Unexplained OoD failure: Existing theory can explain why classifiers rely on the spuriousfeature when the invariant feature is in itself not informative enough (Fig 1a). But when invariantfeatures are fully predictive of the label, these explanations fall apart. E.g., in the four-point-datasetof Fig 1b, one would expect the max-margin classifier to easily ignore spurious correlations (also seeSec 3). Yet, why do classifiers (including the max-margin) rely on the spurious feature, in so manyreal-world settings where the shapes are perfectly informative of the object label (e.g., Fig 1c)? Weidentify two fundamental factors behind this behavior. In doing so, we also identify and explainother kinds of vulnerabilities such as the one in Fig 1d (see Sec 4).
Figure 2: Geometric skews: Fig 2a demonstrates failure of the max-margin classifier in the easy-to-learn Binary-MNIST task of Sec 3. Fig 2b forms the basis for our theory in Sec 4 by showingthat the random-features-based max-margin has norms increasing with the size of the dataset. InFig 2c, we visualize the failure mechanism arising from the “geometric skew”: with respect to thexinv-based classifier, the closest minority point is farther away than the closest majority point, whichcrucially tilts the max-margin classifier towards wsp > 0. Fig 2d, as further discussed in Sec 4,stands as an example for how a wide range of OoD failures can be explained geometrically.
Figure 3: Statistical skews: In Fig 3a, We plot the slow convergence of Wsp/√W2v+w2p ∈ [0,1] underlogistic loss with learning rate 0.001 and a training set of 2048 from D2-dim with B = 1. In Fig 3band Fig 3c, we demonstrate the effect of statistical skews in neural networks. Here, during test-timewe shift both the scale of the spurious feature (from its original scale of 0.1 as marked by the redtriangle) and its correlation. Observe that the network trained on the statistically-skewed Sexp ismore vulnerable to these shifts compared to no-skew Scon.6Isolating the statistical skew effect is however challenging in practice: any gradient-descent-trainedmodel is likely to be hurt by both statistical skews and geometric skews, and we’d have to some-how disentangle the two effects. We handle this by designing the following experiment. We firstcreate a control dataset Scon where there are no geometric or statistical skews. For this, we takea set of images Sinv (with no spurious features), and create two copies of it, Smaj and Smin wherewe add spurious features, positively and negatively aligned with the label, respectively, and defineScon = Smaj ∪ Smin . Next, we create an experimental dataset Sexp with a statistical skew in it. Wedo this by taking Scon and duplicating Smaj in it so that the ratio |Smaj | : |Smin| becomes 10 : 1.
Figure 4: Other failure modes: We visualize the different (straightforward) ways in which a max-margin classifier can be shown to fail in tasks where one of the Constraints in Sec 3.1 is disobeyed.
Figure 5: Validating geometric skews in MNIST and CIFAR10: In Fig 5a, we show the OODaccuracy drop of a random features based max-margin model trained to classify two classes inCIFAR10. In the next few images, we demonstrate that MNIST and CIFAR10 datasets have theproperty that the more the datapoints in the dataset, the larger the norm required to fit them. Specif-ically, in Fig 5b and Fig 5c, we plot the max-margin norms of a random features representation (seeApp B.1 for the definitions of two plotted lines). In Fig 5d, Fig 5e, Fig 5f, we plot the distance frominitialization of neural network models (presented for the sake of completeness).
Figure 6: Our CIFAR10 examples: In Fig 6a we visualize the dataset discussed in App C.3.1.
Figure 7: The channels in the CIFAR-10 dataset from Section C.3.2: In the top image, is ourmain dataset, where one can see that the third channel has a faint copy of the original “invariantfeature” and an extra vertical line added onto it. In the bottom image, is the control dataset wherethe last channel only contains the vertical line (thus making the spurious feature orthogonal to theinvariant features).
Figure 8: More experiments on CIFAR10 example from App C.3.2: Here the red triangle corre-sponds to the value of the scale of spurious feature during training. ‘Non-orthogonal’ correspondsto our main setting, and ‘Orthogonal’ corresponds to the control setting where the original image inthe third channel is zeroed out.
Figure 9: Our cats vs. dogs examples: We present the dataset from App C.3.3 for various valuesof B which determines how much of the image resides in the second channel vs. the third channel.
Figure 10: Experiments on Cat vs Dogs dataset from App C.3.3: Each of the first three plots abovecorresponds to a different value of 1 -p i.e., the proportion of the minority, greenish datapoints. Weplot the OoD accuracy on various distributions for varying values of B ∈ [-1, 1]. The final plotprovides a visualization of these failure mode as explained in App C.3.3.
Figure 11:	Our Binary-MNIST examples: In Fig 11a we present the dataset from App C.3.4 forvarious values of B which determines how much of the image resides in the first channel. Fig 11bpresents the dataset from App C.3.5 where the second channel pixels are individually picked to alignwith the label with probability p (the difference is imperceptible because the pixels are either 0 or0.1). Fig 11c presents the dataset for App C.4.1, where all pixels in the second channel are either 0or B.
Figure 12:	Failure on the Binary-MNIST dataset in App C.3.4. In Fig 12a, we visualize thefailure observed on this dataset. Here we can think of the spurious feature as the difference betweenthe two channels (projected along a direction wdiff where they are informative of the label) and theinvariant feature as the sum of the two channels. Fig 12b and Fig 12c show the OoD performanceunder different shifts in the scale of the spurious feature B. During training time this is set to thevalue given by ‘Domain1’ and/or ‘Domain2’.
Figure 13:	Experiments for the high-dimensional spurious features dataset in App C.3.5. Thered triangles here denote the values of p used during training.
Figure 14:	Experiments validating the effect of statistical skews on the MNIST dataset inApp C.4.1. The red triangle denotes the value of B during training.
Figure 15: Experiments validating the effect of statistical skews on the MNIST dataset inApp C.4.1. The red triangle denotes the value of B during training.
Figure 16: Experiments validating geometric and statistical skews on the obesity dataset: InFig 16a, we show that the `2 norm required to fit the data in the invariant feature space grows withdataset size. As discussed earlier, this would result in a geometric skew, which explains why amax-margin classifier would rely on the spuriously correlated feature. In Fig 16b, we show that thespurious component wsp/kwk of the gradient descent trained classifier converges to zero very slowlydepending on the level of statistical skew.9Note that this sort of an algorithm is applicable only in settings like those in fairness literature wherewe know which subset of the data corresponds to the minority group and which subset correspondsto the majority group.
