Figure 1: Loss landscapes of a two-parametermodel. Averaging gradients forgoes informa-tion that can identify patterns shared across dif-ferent environments.
Figure 2: Inconsistency ingradient directions.
Figure 3: Plotted are con-for HA “ diagp0.05, 1q andHB “ diagp1, 0.05q. HA^Bretains the original volumes,while for ha`b it is 5 x big-ger. This magnification showsinconsistency of A and B.
Figure 4: Magnitude of gradient (aver-age or masked) on random data (∣θ∣ =3000, t “ 0.8d).
Figure 5: A 4-dimensional instantiation of the synthetic memorization dataset for visualization. Every exampleis a dot in both circles, and it can be classified by finding either of the “oracle” decision boundaries shown.
Figure 6: Results on the synthetic dataset.
Figure 7: Gradient correlations.
Figure 8: As the AND-mask thresholdincreases, memorization on CIFAR-10with random labels is quickly hindered.
Figure 9: The AND-mask preventsoverfitting to the incorrectly labeledportion of the training set (left) withouthurting the test accuracy (right).
Figure 10: Performance of the neural network in Equation 3 for two different parameters. Any reasonablemodification on。6 (say ±1) leaves the performance on environment A unchanged, while the performance onenvironment B quickly degrades.
Figure 11:	While the arithmetic mean of the two loss surfaces on the left is identical in all three cases (thirdcolumn), the geometric mean has weaker and weaker gradients (black arrow) the more inconsistent the two losssurfaces become.
Figure 12:	Plotted are contour lines θJH´1θ = 1 for HA = diag(0.01,1) and HB = diag(1, 0.01). It isconvenient to provide this visualization because it is linked to the matrix determinant: Vol({θJH-1θ = 1}) “πʌ/det (H). The geometric average retains the volume of the original ellipses, while the volume of Ha`B is 25times bigger. This magnification indicates that landscape A is not consistent with landscape B .
Figure 13: The spiralsused as the mechanismin the synthetic memo-rization dataset.
Figure 14: Learning curves for the evaluated methods. The top row shows the accuracy on the training set, thebottom row shows the accuracy on the test set.
Figure 15: Relationship between number of training environments and test accuracy for the AND-mask methodcompared to the baseline. We show the best performance out of five runs using the settings that were used forthe experiment in the main text.
Figure 16: Dashed lines show test acc,solid lines show training acc.
Figure 17: Screenshots of 6 levels of CoinRun (from OpenAI).
Figure 18: Learning curves for the behavioral cloning experiment on CoinRun. Training loss is shown on theleft, test loss is shown on the right. We show the mean over the top-10 runs for each method. The shaded regionscorrespond to the 95% confidence interval of the mean based on bootstrapping.
