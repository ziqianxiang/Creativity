Figure 1: An illustration of self-supervised online adversarial purification (SOAP). Left: joint train-ing of the classification and the auxiliary task; Right: input adversarial example is purified iterativelyto counter the representational shift, then classified. Note that the encoder is shared by both classi-fication and purification.
Figure 2: Input digits of the encoder (left) and output digits of the decoder (right). From top tobottom are the clean digits, adversarially perturbed digits and purified digits, respectively. Redrectangles: the adversary fools the model to incorrectly classify the perturbed digit 8 as a 3 and thepurification corrects the perception back to an 8.
Figure 3: Auxiliary loss vs. pfy. SOAP (green plot) reduces the high adversarial auxiliary loss(orange plot) to the low clean level (blue plot). The vertical dashed line is the value of adv. Thetrained models are FCN and ResNet-18 for MNIST and CIFAR10, respectively, with a PGD attack.
Figure 4: Adversarial and purified CIFAR10 examples by SOAP with Wide-ResNet-28 under PGDattacks. True classes are shown on the top of each column and the model predictions are shownunder each image.
Figure 5: Purification against auxiliary-aware PGD attacks. Plots are classification accuracy before(blue) and after (orange) purification.
Figure 6: An illustration of auxiliary self-supervised tasks.
Figure 7: Comparison of training efficiency between SOAP, vanilla training (‘No Def’) and adver-sarial training (FGSM and PGD). The y-axis is the average time consumption of 30 training epochs.
Figure 8: MNIST examples with data reconstruction.
Figure 9: CIFAR10 examples with rotation prediction.
Figure 10: CIFAR10 examples with label consistency.
