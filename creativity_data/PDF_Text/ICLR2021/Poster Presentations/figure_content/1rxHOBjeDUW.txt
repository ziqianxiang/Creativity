Figure 1:	Reward trajectories as a function of training step (in millions) for VizDoom (columns 1-3)and DMLab (columns 4-6) with (a) Very Sparse, (b) Sparse and (c) Dense settings. For VizDoomtasks, we show all the 10 runs per method. For DMLab tasks, we show the averaged episode rewardsover 30 runs of our exploration with the 95% confidence intervals.
Figure 2:	Evolution examples of the drop probability distribution p on Very Sparse DMLab envi-ronments with (left) Image Action, (middle) Noise and (right) Noise Action settings. Each figureshows a histogram per p value according to training iterations (the more front is the more recent).
Figure 3: Classification accuracy ofInception-ResNet-v2 equipped with VIB(Alemi et al., 2017) and DB on ImageNetvalidation set (Russakovsky et al., 2015).
Figure 4: Classification accuracy ofInception-ResNet-v2 equipped with themutual information-based feature selec-tion and DB on ImageNet validation set(Russakovsky et al., 2015), using the samenumber of features.
Figure 5: Classification accuracy ofInception-ResNet-v2 equipped WithVCEB (Fischer, 2020; Fischer & Alemi,2020) on ImageNet (Russakovsky et al.,2015). ρ is annealed from 100 to the finalρ over the first 100000 training steps.
Figure 6: (a) A few samples from Occluded CIFAR dataset (Achille & Soatto, 2018). (b)-(d)Test error plots on the primary task (i.e. the classification of occluded CIFAR images) and on thenuisance tasks (i.e. classification of the MNIST digits). For all the three types of tasks, we use thesame feature extractor trained for the primary task, where its deterministic representation is usedonly for training and test of the nuisance (deterministic) task.
Figure 7: Grad-CAM (Selvaraju et al., 2017) visualization for the last convolutional layer of thefeature extractor on the Occluded CIFAR classification task. For the visualization, d = 128, andβ = 5.623/d for (b) are used. Primary denotes the maps of the logits for the primary labels.
Figure 8: Example observations from VizDoom (Kempka et al., 2016) and DMLab (Beattie et al.,2016) environments with “Image Action” (first) and “Noise” (second) settings.
