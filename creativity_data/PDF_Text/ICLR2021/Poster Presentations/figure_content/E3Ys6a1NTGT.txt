Figure 1: Tabular gridworld experiments.
Figure 2: Performance of deep FDPO algorithms on a dataset of 500000 transitions, as the datacollection policy is interpolated from near-optimal to random. Note that here, the only pessimisticalgorithm evaluated is proximal.
Figure 3: Bandit-like MDP, with accompanying dataset. μ gives the true mean of each action. n»gives the counts of the pulls used to construct dataset D, and μD gives our empirical estimate ofthe mean reward. On this problem, any algorithm that selects the action with the highest empiricalmean reward will almost always pick a suboptimal action. In contrast, a pessimistic algorithm,which selects the action with the highest lower bound, will almost always pick the correct action.
