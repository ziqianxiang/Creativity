Figure 1: Our task learns a segmenter given partially labeled training images and applies it to testimages. A common baseline is to propagate labels within an image based on feature similarity. Wemodel it as semi-supervised metric learning and learn the pixel-wise feature by contrasting it withinand across images. Our results are fuller and more accurate, approaching the ground-truth.
Figure 2: We propose a unified framework for weakly supervised semantic segmentation with dif-ferent types of annotations. We demonstrate consistent performance gains compared to the state-of-the-art (SOTA) methods: Chang et al. (2020) for image tags, Song et al. (2019) for bounding boxes,and Tang et al. (2018b) for points and scribbles. For tags and boxes, Class Activation Maps (CAM)(Zhou et al., 2016) are often used to localize semantics as an initial mask and iteratively refine thesegmentation model, whereas for labeled points and scribbles, Conditional Random Fields (CRF)are used to propagate semantic labels to unlabeled regions based on low-level image similarity.
Figure 4: Four types of pixel-to-segment attraction and repulsion relationships. A pixel is attractedto (repelled by) segments: a) of similar (different) visual appearances such as color or texture, b)of the same (different) class labels, c) in images with common (distinctive) labels, d) of nearby(far-away) feature embeddings. They form different positive and negative sets.
Figure 5: Our method uses labeled and unlabeled portions of the training data more extensively.
Figure 6: Our results on Pascal and DensePose under various weak supervision settings are consis-tently better aligned with region boundaries and visually closer to fully supervised counterparts.
Figure 7: Our segmentation results get better with more types of regularizations. We compare visualresults by adding more regularizations. As we introduce more relationships for regularization, weobserve significant improvement and our results are visually closer to fully supervised counterparts.
Figure 8: Visual comparison of baseline method (c), our SPML (d) and fully-supervised SegSort(e) on VOC and DensePose. On VOC (top 6 rows), our baseline method is based on Lee et al.
Figure 9: Our segmentation results get better with more types of regularizations. We compare visualresults by adding more regularizations. As we introduce more relationships for regularization, weobserve significant improvement and our results are visually closer to fully supervised counterparts.
Figure 10: Visual examples of nearest neighbor segment retrievals. We observe that retrieved seg-ments (right) appear in the similar semantic context as the query segments (left). For examples,given a bottle next to a desktop, our model retrieves bottles also next to a desktop.
Figure 11: Visual examples of semantic annotations used on VOC. For image tag and bounding boxannotation, we use the classifier trained by Wang et al. (2020) to infer CAM as semantic annotation.
Figure 12: Preparing training labels on DensePose dataset. From left to right are input image,our training labels and ground-truth mask. For each keypoint, a Gaussian heat map is applied todetermine labelled, unknown and background region. The white region denotes unknown pixels, towhich we propagate labels from annotated or background region.
