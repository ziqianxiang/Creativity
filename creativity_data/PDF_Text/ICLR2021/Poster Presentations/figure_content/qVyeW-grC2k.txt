Figure 1: Samples of thePathfinder task.
Figure 2: Required attention span ondifferent tasks.
Figure 3: Trade-off between performance (y axis) and resources (x axis). On the left, circle sizecorresponds to memory footprint. On the right, it corresponds to examples per second.
Figure 4: Left: The cosine similarity between the embedding learned for each pixel intensity. Right:Each tile shows the cosine similarity between the position embedding of the pixel with the indicatedrow and column and the position embeddings of all other pixels.
Figure 5: Attention map for different examples from the Pathfinder task. Each map presents theattention distribution, given the CLS token at the final layer as the query, averaged across all headsin a vanilla Transformer model. Note that for visualization, we use attention-rollout (Abnar &Zuidema, 2020) for more precise input attribution.
