Figure 1:	Several image modalities have no preferred orientation for tasks such as classification. Weimprove their generative modeling by utilizing image symmetries within a GAN framework.
Figure 2:	An abbreviated illustration of group-convolutions used in our generator networks.
Figure 3: Qualitative GAN interpolation (White, 2016) results. (a) Selected spherical interpolationsbetween generated RotMNIST samples in either latent space (top) or between labels (bottom).
Figure 4: Selected generated samples using the best performing equivariant models with no aug-mentation. Random samples are available in App. A. Layout inspired by Karras et al. (2020a).
Figure 5: Top: Improved Precision and Recall (Kynkaanniemi et al., 2019) analysis of ablationsfor all snapshots of trained models in each setting (closer to top-right is better). Bottom: GANconvergence (FID vs. generator updates) of standard GANs vs. our proposed models for all datasets.
Figure 6: Convergence plots of all GAN ablation settings on Rotated MNIST across data availabil-ities (rows) and loss functions (columns). Frechet distance to the validation set is evaluated every1,000 generator iterations, for 20,000 iterations total. Experiments are repeated with 3 different ran-dom seeds and average trajectories are reported with standard deviation error bars. This figure isbest interpreted alongside Table 2 which lists best performances for each configuration.
Figure 7: GAN convergence (FID vs. generator updates) for baseline comparisons of the best per-forming methods (left) and ablations (right) for all datasets. Readers are encouraged to zoom-in forbetter inspection.
Figure 8: Random 64 × 64 Food-101 samples from arbitrarily chosen classes with no truncationtaken from the best performing model snapshot with p4-equivariance (without augmentation) in thediscriminator.
Figure 9: Random 128 × 128 ANHIR samples with no truncation taken from the best performingmodel snapshot with p4m-equivariance in both generator and discriminator (without augmentation).
Figure 10: Random 256 × 256 LYSTO samples with no truncation taken from the best performingmodel snapshot with p4m-equivariance in both generator and discriminator (without augmentation).
Figure 11: Random samples for RotMNIST (28 × 28) and CIFAR-10 (32 × 32) sampled withσ = 0.75 truncation trained without augmentation.
Figure 12: Arbitrarily selected sample translations from input map images (Col. 1) using either base-line Pix2Pix with publicly available pre-trained weights (Col. 2) or Pix2Pix with a p4-equivariantdiscriminator (Col. 3). Real aerial images are shown in Col. 4.
Figure 13: Residual blocks in the group-equivariant settings used in RGB image generation archi-tectures. The choice of p4 or p4m is dataset-specific. The generator uses ResBlockG (left) andthe discriminator uses ResBlockD (right). The first residual block in the convolutional sequence ineither network uses z2-p4m group-convolutions for the initial layer. The non-equivariant settings re-place all group-convolutions and normalizations within the residual blocks with standard techniques.
