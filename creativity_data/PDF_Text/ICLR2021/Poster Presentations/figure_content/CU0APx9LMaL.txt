Figure 1: (a) Example of micro-architectures(cells) trained on TIMIT. The architecturespace has 8, 242 unique cells. (b) Anoverview of the macro-architecture.
Figure 2: Correlation of best validation and final test PER for top, middle and bottom 1, 000 models.
Figure 3: Distribution of selected edges for each node (first row) and distribution of skip-connectionsfor possible edges (second row). Top, middle and bottom are grouped by final test PER.
Figure 4: Ranking of models according to their test PER, whenthey are grouped according to the number of Linear layers(left) and skip-connections (right) used in their designs.
Figure 5: Cell architectures in our search space (from left toright): TDS-like, with best validation and test PERs, and oneof the efficient pareto points. Note that the TDS-like cell couldalso have different convolution selected as the first operation.
Figure 6: Test PER versus latency, color-coded by the number of parameters. Bestmodels, Pareto-optimal models, and selectedTDS-like models are highlighted.
Figure 7: Transfer between TIMIT and Lib-rispeech on validation and test datasets with highvalidation and test correlation of 0.85 and 0.86,respectively.
Figure 8: Performance of NAS algorithms on our ASR search space, where shaded regions markinterquartile ranges. The dashed line represents the best model in the dataset across all seeds.
Figure 9: Correlation of best validation and final test PER. Figures in the columns are for top, middleand bottom 1, 000 models. Each row represents the results for runs with unique initialization.
Figure 10: Cell architectures in our search space with best validation and test PERs for the threeruns.
Figure 11: Positions of models in the ranking according to their test PER, when they are groupedaccording to the number of Convolution layer (first column), Linear layer (second column), Zeroop (third column) and Skip-connections (last column). Each row correspond to the results for a runwith unique inititialization.
