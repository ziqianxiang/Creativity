Figure 1: The generative language-grounded policy for vision-and-language navigation.
Figure 2: The precision of actions by the generative (red) and discriminative (blue) models on thereference trajectory (dashed lines) and on navigation trajectories (solid lines). The horizontal axisis the time step either on reference trajectory or on navigation trajectory. The vertical axis is theproportion that the agent chooses the shortest path action in each time step.
Figure 3: Token-wise prediction entropy (TENT) for two navigation instances from validation-seen(top) and validation-unseen (bottom) sets in R2R. The vertical axis corresponds to the 1-TENTdrawn at each time step t ∈ N ∪{0},as t + ∆ (1 - S (Wk)), where ∆ = 0.05 so that one vertical-tickcorresponds to 0.05. We draw multiple lines that correspond to different time steps colored fromblue to red and green. We attach the panoramic views for some of the trial time steps.
Figure 4: The relation for notations of the instruction X,VLN agent.
Figure 5: The agreement of actions between the generative and discriminative models on shortestpaths (dashed lines) and on navigation trials (solid lines). The horizontal axis corresponds to thetime step of trials.
Figure 6: We present SR (top), CLS (middle) and SDTW (bottom) with the confidence interval of2σ. Crimson for generative policy and cyan for discriminative policy. Both policies are trained withthe fidelity-oriented manner.
