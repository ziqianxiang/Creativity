Figure 1: We train agents to perform PointGoal navigation in visually complex Gibson (Xia et al.,2018) and Matterport3D (Chang et al., 2017) environments such as the ones shown here. Theseenvironments feature detailed scans of real-world scenes composed of up to 600K triangles andhigh-resolution textures. Our system is able to train agents using 64×64 depth sensors (a high-resolution example is shown on the left) in these environments at 19,900 frames per second, andagents with 64×64 RGB cameras at 13,300 frames per second on a single GPU.
Figure 2: The batch simulation and rendering architecture. Each component communicates at thegranularity of batches of N elements (e.g., N =1024), minimizing communication overheads andallowing components to independently parallelize their execution over each batch. To fit the workingset for large batches on the GPU, the renderer maintains KN unique scene assets in GPU memoryand shares these assets across subsets of the N environments in a batch. To enable experiencecollection across a diverse set of environments, the renderer continuously updates the set of K in-memory scene assets using asynchronous transfers that overlap rollout generation and learning.
Figure 3: SPL vs. wall-clock time (RGBagents) on a RTX 3090 over 48 hours(time required to reach 2.5 billion sampleswith BPS). BPS exceeds 80% SPL in 10hours and achieves a significantly higherSPL than the baselines.
Figure 4: SPL vs. wall-clock time (BPStraining Depth agents over 2.5 billionsamples on 8 Tesla V100s) for variousbatch sizes (N). N =256 finishes after 2×the wall-clock time as N =1024, but bothachieve statistically similar SPL.
Figure 5: bps runtime breakdown. Inference rep-resents policy evaluation cost during rollout gen-eration. Learning represents the total cost of pol-icy optimization.
Figure A1: BPS’s validation set SPL forDepth vs. number of training samples acrossa range of batch sizes. This graph showsthat sample efficiency slightly decreases withlarger batch sizes (with the exception ofN =512 vs. N =1024, where N =1024 ex-hibits better validation score). Ultimately,the difference in converged performance isless than 1% SPL between different batchsizes. Although N =256 converges the fastestin terms of training samples needed, Fig. 4shows that N =256 performs poorly in termsof SPL achieved per unit of training time.
Figure A2: Frames per second achievedby the standalone renderer on a RTX 3090across a range of resolutions and batch sizesfor a RGB sensor on the Gibson dataset. Per-formance saturates at a batch size of 512. Forlower batch sizes, increasing resolution hasa minimal performance impact, because theGPU still isn’t fully utilized. As resolutionincreases with larger batches, the relative de-crease in performance from higher resolutionincreases.
Figure A3: The effect of the Lamb optimizer versus the baseline Adam optimizer on sample effi-ciency while training a Depth sensor driven agent. Lamb maintains a consistent lead in terms ofSPL throughout training, especially in the first half of training.
