Figure 2: Exponent bias shifts the minifloatdistribution to align with the maximum expo-nent of the value distributionrτττττττ>rτττmπ∏xɪrττfrrn∏χ""(a) Minifloat (b) BM (Shared exp. bias)Figure 1: Minifloat and Block Minifloat (BM) ten-sor representations2	Block MINIFLOAT Representation2.1	MINIFLOAT NUMBER FORMATEquation (1) computes the real value of a minifloat number, where (e, m) denote the number ofexponent and mantissa bits in the representation.
Figure 1: Minifloat and Block Minifloat (BM) ten-sor representations2	Block MINIFLOAT Representation2.1	MINIFLOAT NUMBER FORMATEquation (1) computes the real value of a minifloat number, where (e, m) denote the number ofexponent and mantissa bits in the representation.
Figure 3: End-to-end Training with Block Minifloat (BM). All off-chip memory transfers are lowprecision BM tensors. BM alignments, weight updates, quantization, batchnormalization and ReLUare executed in on-chip scalar FP32 units. The register file (RF) stores a block of VW.
Figure 4: Validation perplexity of LSTMmodel on Penn Treebank4.3	Language Modelling with LSTMWe compared 8-bit formats for language modeling on the Penn Treebank dataset Marcus et al. (1993).
Figure 5: Experiments for minimising data loss with 6-bit Block Minifloat (BM6)1.0	1.5	2.0	2.5	3.0	3.5base(c) Minifloat scaling by varying theexponent base(％) MOUJ8punOoooo1 8 6 4 24.5	Empirical AnalysisEffect of Denormal Numbers: To study the effect that denormal numbers have on training conver-gence in sub 8-bit networks, we trained ResNet-18 on ImageNet for BM6 with denormals (ours)and without denormals, using QPyTorch library (Zhang et al., 2019). Results are plotted againstfloating-point accuracy in Figure 5a. Without denormals, small numbers are flushed-to-zero andtraining stagnates immediately. Although not shown here, 8-bit representations with more than e = 3bits do not suffer similar accuracy degradation without denormals. This investigation confirms theimportance of denormal numbers for training BM formats with fewer exponent bits, and differentiatesour software and hardware experiments substantially from previous 8-bit regimes.
Figure 6: Computational density v ResNet-18 ac-curacy on ImageNet5	Hardware EvaluationIn this section, we evaluate the proposed block minifloat representation in hardware and compareagainst competitive integer and floating-point arithmetic. Figure 6 summarizes our results with aplot of computational density (measured as operations per unit silicon area) and ResNet-18 trainingaccuracy on ImageNet. Computational density was obtained from an RTL design of single-cyclefused multiply-add (FMA) units and 4x4 systolic array multipliers. We performed synthesis at750MHz for 28nm silicon technology and recorded area and power measurements for each numberrepresentation. Table 6 provides a subset of these results, with coverage of all BM formats suppliedin Appendix A.4.
Figure 7: Train loss and top-1 validation accuracy for the full spectrum of Block Minifloat formatstrained on ImageNet using a ResNet-18 modelA.3.2 Comparison with Block Floating Point (BFP) on ImageNetAs described in Section 1, block minifloats bridge the gap between narrow floating-point and blockfloating point (BFP) representations. The main idea is that better outcomes in terms of accuracyand hardware efficiency can be achieved by exploring the spectrum between the two representations.
Figure 8: Training convergence curves for Transformer on IWSLT’14 DE-En datasetA.3.4 SSD-LITE (MOBILENET-V2) (VOC)We adapted a PyTorch implementation of SSD-lite from an online repository 7. The base networkis MobileNet-V2 (Sandler et al., 2018) which was pretrained on ImageNet. The enitre network istrained on VOC2012 and VOC2007 trainval datasets and evaluated on VOC2007 validation dataset.
Figure 9: Training convergence curves for EfficientNet-b0 on ImageNet0A.4 Hardware SynthesisFused multply-add (FMA) units were designed in RTL for floating-point and block minifloat rep-resentations. We modified code from the Deepfloat9 repository for FP32, FP16 and FP8 units. TheBM units with Kulisch accumulation were hand written in Verilog following the block design given7Implementation available at https://github.com/qfgaohao/pytorch-ssd8Implementation available at https://github.com/narumiruna/efficientnet-pytorch9Implementation available at https://github.com/facebookresearch/deepfloat14Published as a conference paper at ICLR 2021in Figure 10. All designs were synthesised at 750Mhz using Cadence RTL compiler 14.11 and acommercial 28nm standard cell library. Since GEMM hardware is typically designed from tiles ofsmaller computational units, we also provide synthesis results for small 4x4 systolic array multipliers.
Figure 10: Block diagram of block minifloat multiply-add; A*B + C, where A and B are minifloatsand C is an integerTable 9: Synthesized logic area and powerof single-cycle fused multiply-Add (FMA) at750 MHz on 28nm chip.
