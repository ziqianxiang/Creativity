Figure 1: Dot products between absolute position vectors 6(top row) and relative position vectors(bottom row). Darker means the two position vectors are closer.
Figure 2: Identical word probing. Darker in the i-th row and j-th column means that the i-th wordsgenerally attend more on the j -th words.
Figure 3: φ(m) in (b) is a sum of many cosine functions of individual frequencies with increasingm, which determines the closeness between arbitrary two m-distance position vectors. As shown in(a), each frequency could play different roles: 1) the extremely small frequencies have few effectson the overall word representation (WEx + Px) in Eq. 5 since it makes such position embeddingbeing almost identical with increasing positions; 2) some smaller frequencies can be beneficial toguarantee Property 1 if ωi < L; 3) some bigger frequencies would promote the locally attendingmechanism since such cos functions in Eq. 6 drop dramatically in the beginning ifωi is great enough;4) Some big frequencies which ωi > Π would be smooth factors for the overall pattern since it wouldbe randomly impose a bias to all positions.
Figure 4: The learned frequencies in learnable sinusoidal APE in the pre-trained language modeland downstream tasks(1/10000)2i/D in the Transformer (Vaswani et al., 2017) can be considered as a special case whenit enumerates various frequencies ranging from 1/10000 to 1 under a specific distribution.
Figure 5: In which offset ranges the properties correlate with the performance in downstream tasks.
Figure 6:	Cosine similarities between any two relative position vectors. Cosine similarities biggerthan 95% are in blue.
Figure 7:	Experimental results on SQuADs with BERT-medium. X-axis: epoch number (first trainedon 128-length seq. with 10 epochs and then 512-length with 2 epochs). Y-axis: F1 score.
Figure 8: Dot products between absolute position vectors evolving with training steps.
Figure 9: Position-wise correlation matrix (PWQ,1 (WK,1)TPT) for first 128 positions in BERTpre-trained modelsthe suitable parameterization type is not the main concern in this paper, we adopted the typicalones, namely, the fully-learnable, (learnable or fixed) sinusoidal APEs/RPEs. The fundamentaldifference between these PE parameterizations needs further investigation. More recently, (Wang &Chen, 2020) empirically study the behaviour of many position embeddings and their performancein Transformers for various NLP tasks.
Figure 10:	Identical word probing with different types of trained models.
Figure 11:	Identical word probing (models with more PEs are shown here comparing to Fig. 2).
