Figure 1: Visualization of the algorithm components. Solid lines indicate the forward flow of datawhereas dashed lines indicate the backward flow of gradients. Data is stored in the fixed buffer.
Figure 2: Empirical distribution of the risk-event of O-RAAC, O-D4PG, and the behavior policies.
Figure 3: Evolution of car states and input control when following learned policies for RAAC,WCPG and D4PG. We use policies from 5 independent seeds for each algorithm. RAAC learns tosaturate the velocity below the speed limit.
Figure 4: Experimental results across several Mujoco tasks for the Medium variant of each dataset.
Figure 5: Experimental results across several Mujoco tasks for the Expert variant of each dataset.
Figure 6: Effect of the hyperparameter λ on the CVaR of the returns for each of the MuJoCo envi-ronments. As λ → 0, the policy imitates the behavior policy has poor risk-averse performance. Asλ → 1, the policy suffers from the bootstrapping error and the performance is also low. The best λis environment dependent.
