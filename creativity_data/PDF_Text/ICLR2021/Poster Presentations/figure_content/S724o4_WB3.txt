Figure 1: 1D illustration of differentimplicit biases: two-layer sigmoid net-work trained with preconditioned GD.
Figure 2: Geometric illustration(2D) of how the interpolating θpdepends on the preconditioner.
Figure 3: Population risk ofpreconditioned linear regressionvs. time with the following P : I(red), Σx1 (blue) and (X>X)t(cyan). Time is rescaled differ-ently for each curve (convergencespeed is not comparable). Ob-serve that GD and sample NGDgive the same stationary risk.
Figure 5: We set eigenvalues of ΣX as two point masses with κX = 20 and kΣX k2F = d; empirical values(dots) are computed with n = 300. (a) NGD (blue) achieves minimum variance. (b) GD (red) achieves lowerbias under isotropic signal: Σθ = Id. (c) NGD achieves lower bias under “misalignment”: ΣX = Σθ-1.
Figure 4: Misspecified biaswith Σθ =Id (favors GD) andfc (x) = α(x>X -tr(Σχ)),where α controls the extentof nonlinearity. Predictionsare generated by matching σ2with second moment of f *.
Figure 6: Illustration ofisotropic and misaligned θ*.
Figure 7: Bias-variance tradeoffwith κX = 25, Σθ = Id andSNR=32/5. As we additively (ii)or geometrically (iii) interpolatefrom GD to NGD (left to right),the stationary bias (blue) increasesand the stationary variance (or-ange) decreases.
Figure 8: Comparison between NGD and GD. Error bar is one standard deviation away from mean over fiveindependent runs. Numbers in parentheses denote amount of unlabeled examples for estimating the Fisher.
Figure 9: (a) numbers in parentheses indicate the amount of unlabeled data used in estimating the Fisher F ;we expect the estimated Fisher to be closer to the sample Fisher when the number of unlabeled data is small. (a)additive interpolation P = (F + αId)-1; larger damping parameter yields update closer to GD. (b) geometricinterpolation P = F-α; larger α parameter yields update closer to that of NGD (blue).
Figure 10: Illustration of implicit bias of GD and NGD. We set n = 100, d = 50, and regress a two-layerReLU network with 50 hidden units towards a teacher model of the same architecture on Gaussian input. Thex-axis is rescaled for each optimizer such that the final training error is below 10-3. GD finds solution withsmall changes in the parameters, whereas NGD finds solution with small changes in the function. Note that thesample Fisher (cyan) has implicit bias similar to GD and does not resemble NGD (population Fisher).
Figure 11: Illustration of the implicit bias of preconditioned gradient descent that interpolates between GDand NGD on MNIST. Note that as the update becomes more similar to NGD (smaller damping or larger α), thedistance traveled in the parameter space increases, where as the distance traveled on the output space decreases.
Figure 12: We set Σθ = ΣrX, γ =2, κ = 20, and plot the stationary bias(well-specified) under varying r.
Figure 13: yzyτK-τy∕n on two-layer neural network (CIFAR-10).
Figure 14: Epoch-wise double descent.
Figure 15: Illustration of the “multiple-descent” curve of the risk for γ > 1. We take n = 300, eigenvalues ofΣX as three equally-spaced point masses with κX = 5000 and kΣX k2F = d, and Σθ = Σ-X1 (misaligned).
Figure 16: We set eigenvalues of ΣX as a uniform distribution with κX = 20 and kΣX k2F = d.
Figure 17:	We construct eigenvalues of ΣX with a polynomial decay: λi(ΣX) = i-1 and then rescale theeigenvalues such that κX = 500 and kΣX k2F = d.
Figure 18:	Well-specified bias against different extent of “alignment”. We set n = 300, eigenvalues of ΣXas two point masses with κX = 20, and take Σθ = ΣrX and vary r from -1 to 0. (a) GD achieves lower biaswhen ∑θ is isotropic, NGD dominates when Σχ = ∑-1, whereas P = ∑χ1/2 (interpolates between GDand NGD) is advantageous in between. (b) optimal early stopping bias follows similar trend as stationary bias.
Figure 19:	Optimal early stopping risk vs. increasing model misspecification. We follow the same setup asFigure 5(c). (a) Σθ = Id (favors GD); unlike Figure 5(c), GD has lower early stopping risk even under largeextent of misspecification. (b) Σθ = Σ-X1 (favors NGD); NGD is also advantageous under early stopping.
Figure 20:	Population risk of the preconditioned update in RKHS that interpolates between GD and NGD. Weuse the IMQ kernel and set n = 1000, d = 5, N = 2500, σ2 = 5 × 10-4. The x-axis has been rescaled foreach curve and thus convergence speed is not directly comparable. Note that (a) large λ (i.e., GD-like update)is beneficial when r is large, and (b) small λ (i.e., NGD-like update) is beneficial when r is small.
Figure 21:	(a)(b) Additional label noise experiment on CIFAR-10. (c)(d) Population risk of two-layer neuralnetworks in the misalignment setup with synthetic Gaussian data. We set n = 200, d = 50, the dampingcoefficient λ = 10-6, and both the student and the teacher are two-layer ReLU networks with 50 hidden units.
Figure 22: Illustration of the monotonicity of the bias term under Σθ = Id . We consider two distributions ofeigenvalues for ΣX: two equally weighted point masses (circle) and a uniform distribution (star), and vary thecondition number κX and overparameterization level Y. In all cases the bias in monotone in α ∈ [0, 1].
