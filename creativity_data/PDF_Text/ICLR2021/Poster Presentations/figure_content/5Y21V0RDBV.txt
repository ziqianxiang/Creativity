Figure 1: Joint Coherence vs. Log-Likelihoods for MNIST-SVHN-Text.
Figure 2: Ten samples from the PolyMNISTdataset. Each column depicts one tuple thatconsists of five different “modalities”.
Figure 3: Performance on PolyMNIST as a function of the number of input modalities, averagedover all subsets of the respective size. Performance is measured in terms of three different metrics(larger is better) and markers denote the means (error bands denote standard deviations) over fiveruns. Left: Linear classification accuracy of digits given the latent representation computed fromthe respective subset. Center: Coherence of conditionally generated samples (excluding the inputmodality). Right: Log-likelihood of all generated modalities. Not shown: The joint coherence is3.6 (±1.5), 20.0 (±1.9), and 12.1 (±1.6) percent for MVAE, MMVAE, and MoPoE respectively.
Figure 4: Qualitative results for bimodal CelebA. The images are conditionally generated byMoPoE-VAE using the text on top of each column.
Figure 5: Coherence and Log-Likelihoods for MNIST-SVHN-Text. The three figures show theevaluation for the conditional generation of a single modality given the other two in relation to thejoint log-likelihood given these two modalities, e.g. in the first row we generate SVHN samplesconditioned on MNIST and Text. The points in the figures are the mean values of 5 different runswith the lines being the standard deviations in bopth directions, coherence and log-likelihoods.
Figure 6: Coherence and Log-Likelihoods for MNIST-SVHN-Text. The three rows of figures showthe evaluation for the conditional generation of two modalities given the remaining one in relationto the joint log-likelihood given this single modality, e.g. in the first row we generate SVHN andText samples conditioned on MNIST. The points in the figures are the mean values of 5 differentruns with the lines being the standard deviations in both directions, coherence and log-likelihoods.
Figure 7: Qualitative comparison of randomly generate MNIST-SVHN-Text samples.
Figure 8: Reconstructions across all modalities for all models. In every pair of rows, we show onerow of test images followed by one row of respective reconstructions.
Figure 9: Ten unconditionally generated images from the respective five modalities for each model.
Figure 10:	Conditionally generated images of the first modality given the respective test examplefrom the second modality shown in the first row. Column-wise, we take different samples from theapproximate posterior, which should result in stylistic variations for generated outputs, but whichshould ideally not change the digit labels.
Figure 11:	Conditionally generated images of the first modality given the four test examples fromthe remaining modalities shown in the first four rows. Column-wise, we take different samples fromthe approximate posterior, which should result in stylistic variations for generated outputs, but whichshould ideally not change the digit labels. Compared to the results from Figure 10, the MoPoE-VAEgenerates more coherent samples when conditioned on four instead of one input modality.
Figure 12: Coherence of generated bimodal CelebA samples. For every subplot, image and textare generated conditionally by the the modality or subset of modalities in the caption. We see thatdifferent attributes are not learned equally well.
Figure 13:	Learned Latent Representations for the bimodal CelebA dataset.
Figure 14:	Qualitative Results of randomly generated CelebA samples.
