Figure 1: The behavior of (EG) in the bilinear min-max problem L(θ, φ) = θφ with θ, φ ∈ [-1, 1]. Given the clipping at[-1, 1], this problem is smooth with L = 1; instead, in the unconstrained case, both (BD) and (LC) fail. Still, even in theconstrained case, running (EG) with a step-size only slightly above the 1/L bound (L = 1, γ = 1.04) results in a dramaticconvergence failure (left plot). Tuning the step-size of (EG) resolves this problem (center), but a constant step-size makesthe algorithm unnecessarily conservative towards the end. The proposed AdaProx algorithm automatically exploits previousgradient data to perform more informative extra-gradient steps in later ones, thus achieving faster convergence without tuning.
Figure 2: Numerical comparison between the extra-gradient (EG), Bach-Levy (BL) and AdaProx algorithms (red circles,green squares and blue triangles respectively). The figure on the left shows the methods, convergence in a 100 × 100 bilineargame; the one on the right shows the methods, convergence in a non-convex/non-concave covariance learning problem. In bothcases, the parameters of the EG and BL algorithms have been tuned with a grid search (AdaProx has no parameters to tune).
