Figure 1: Upper plot: distribution of runtime in seconds. Lower plot: difference with the bounds obtainedby Gurobi with a cut from Ak per neuron; higher is better. Results for the SGD-trained network from Bunelet al. (2020a). The width at a given value represents the proportion of problems for which this is the result.
Figure 2: Pointwise comparison for a subset of the methods on the data presented in Figure 1. Darker colourshades mean higher point density (on a logarithmic scale). The oblique dotted line corresponds to the equality.
Figure 3: Cactus plots on properties from Lu & Kumar (2020), displaying the percentage of solved propertiesas a function of runtime. Baselines are represented by dotted lines.
Figure 4: Mk plotted on the (zk, Xk) plane, under the assumption that lk ≤ 0 and Uk ≥ 0.
Figure 5: Example network architecture in which Mk ⊂ A0k, with pre-activation bounds computed withCk = Mk. For the bold nodes (the two hidden layers) a ReLU activation follows the linear function. Thenumbers between parentheses indicate multiplicative weights, the others additive biases (if any).
Figure 6: Upper plot: distribution of runtime in seconds. Lower plot: difference with the bounds obtained byGurobi with a cut from Ak per neuron; higher is better. Results for the network adversarially trained with themethod by Madry et al. (2018), from Bunel et al. (2020a). The width at a given value represents the proportionof problems for which this is the result.
Figure 7: Pointwise comparison for a subset of the methods on the data presented in Figure 6. Darker colourshades mean higher point density (on a logarithmic scale). The oblique dotted line corresponds to the equality.
Figure 8: Upper plot: distribution of runtime in seconds. Lower plot: difference with the bounds obtainedby Gurobi with a cut from Ak per neuron; higher is better. Results for the SGD-trained network from Bunelet al. (2020a). The width at a given value represents the proportion of problems for which this is the result.
Figure 9: Upper plot: distribution of runtime in seconds. Lower plot: difference with the bounds obtainedby Gurobi with a cut from Ak per neuron; higher is better. Results for the SGD-trained network from Bunelet al. (2020a). The width at a given value represents the proportion of problems for which this is the result.
Figure 10: Upper plot: distribution of runtime in seconds. Lower plot: difference with the bounds obtainedby Gurobi with a cut from Ak per neuron; higher is better. MNIST results for a network adversarially trainedwith the method by Wong & Kolter (2018), from Lu & Kumar (2020). The width at a given value representsthe proportion of problems for which this is the result.
Figure 11: Pointwise comparison for a subset of the methods on the data presented in Figure 10. Comparisonof runtime (left) and improvement from the Gurobi Planet bounds. For the latter, higher is better. Darker colourshades mean higher point density (on a logarithmic scale). The oblique dotted line corresponds to the equality.
