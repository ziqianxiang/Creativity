Figure 1: Negative Data Aug-mentation for GANs.
Figure 2: Negative augmentations produce out-of-distribution samples lacking the typical structureof natural images; these negative samples can be used to inform a model on what it should not learn.
Figure 3: Schematic overview of our NDA framework. Left: In the absence of NDA, the supportof a generative model pθ (blue oval) learned from samples (green dots) may “over-generalize” andinclude samples from P1 or P2. Right: With NDA, the learned distribution P⅛ becomes disjointfrom NDA distributions P1 and P2, thus pushing Pθ closer to the true data distribution Pdata (greenoval). As long as the prior is consistent, i.e. the supports of P1 and P2 are truly disjoint from Pdata,the best fit distribution in the infinite data regime does not change.
Figure 4: Histogram of differ-ence in the discriminator out-put for a real image and it’sJigsaw version.
Figure 5: Toy Datasets used in Numerosity experiments.
Figure 6: Left: Distribution over number of dots. The arrows are the number of dots the learningalgorithm is trained on, and the solid line is the distribution over the number of dots the modelgenerates. Right: Distribution over number of CLEVR objects the model generates. GeneratingCLEVR is harder so we explore only one, but the behaviour with NDA is similar to dots.
Figure 7: Qualitative results on Cityscapes.
Figure 8:	Histogram of D(clean) - D(corrupt) for 3 different corruptions.
