Figure 1: Frequency profiles (Φs (λ))Since ChebNet and CayleyNet are spectral-designed, their frequency responses do not change fordifferent graphs. They are presented in Figure 1 for first 5 and 7 supports respectively. The results inFigure 1 confirm the theoretical analyses in Theorem 2 and Theorem 3. The full frequency profilesare not illustrated because they consist of zeros outside the diagonal. Analyzing the frequency profileof ChebNet, one can argue that the convolutions mostly cover the spectrum. However, none of thekernels focuses on some certain parts of the spectrum. As an example, the second kernel is mostly alow-pass and high-pass filter and stops the middle band, while the third one passes very high, verylow and middle bands, but stops almost first and third quarter of the spectrum. Therefore, if therelation between input-output pairs can be figured out by just a low-pass, high-pass or some specific1https://github.com/balcilar/gnn-spectral-expressive-power6Published as a conference paper at ICLR 2021band-pass filter, a high number of convolution kernels is needed. However, in the literature, only 2or 3 kernels are generally used in experiments (Defferrard et al., 2016; Kipf & Welling, 2017).
Figure 2: Frequency profiles of GCN on 1D, Cora, CiteSeer graph and GIN on 1D and CiteSeergraph with = 1, 0, -1, -2When the given graph is a regular graph where each node degree is the same (2 for 1D graph case),theoretical frequency responses become certain as seen in Figure 2a in blue for GCN and Figure 2bfor GIN. When e = 2, 1D graph's (p = 2) frequency responses of GCN and GIN are the sameexcept scaling factor as seen in blue Figure 2a yellow in Figure 2b. However in realistic graphs,both GIN and GCN are not spectral-designed, their frequency responses differ for different graphs.
Figure 3: Frequency profiles of GATdifferences between frequency responses in different layer or different attention head, we stacked alltogether in the same heat map.
Figure 4: a) Schematic of the graph convolution layer defined in equation 3. The graph has 12 nodesand 12 edges. In the l-th layer, each node has a 2-length feature vector H1(l) and H2(l) representedby colors. The l + 1-th layer, it has a 3-length feature vector, denoted H1(l+1) , H2(l+1) and H3(l+1) .
Figure 5: Full frequency response of GCN on 1D, Cora and CiteSeer graphsThe three standard frequency responses in Figure 2a have almost the same low-pass filter shape. Itcorresponds to a function composed of a decreasing part on the three first quarters of the eigenvaluesrange, followed by an increasing part on the remaining range. This observation is coherent with thetheoretical analysis. Hence, kernels used in GCN are transferable across the three graphs at hand.
Figure 6: Heat map of GCN’s frequency profiles on ENZYMES and PROTEIN dataset graphs.
Figure 7: Full frequency profiles of GIN-0 and frequency responses of different e values for Coragraph.
Figure 8: Heat map of different valued GIN frequency profiles on ENZYMESFigure 9: Heat map of different valued GIN frequency profiles on PROTEINH.5 GATOne can see that, each support is a function of trainable weights (W (l,s) , a(l,s)) in GAT, frequencyprofiles cannot be directly computed similarly to previous ones. We did bunch of simulations forCora graph. As the proposed method used for Cora problem, we have generated 8 different con-volution supports corresponding to 8 pairs of W (l,s) ∈ R1433×8 (1433 features for each node) anda(I,S) ∈ R16×1 trainable weights for the first layer (Velickovic et al., 2018). We produce 240 (30for each support) random pairs of W (l,s) and a(l,s) where activation function is LeakyReLU has0.2 negative slope as in (Velickovic et al., 2018). Later, We calculated frequency response of gener-ated supports by Corollary 1.1. The mean and standard deviation of the frequency profiles for thesesimulated GAT supports are shown in Figure 3 a and its expected and standard deviation of the fullfrequency response shown in Figure 10.
Figure 9: Heat map of different valued GIN frequency profiles on PROTEINH.5 GATOne can see that, each support is a function of trainable weights (W (l,s) , a(l,s)) in GAT, frequencyprofiles cannot be directly computed similarly to previous ones. We did bunch of simulations forCora graph. As the proposed method used for Cora problem, we have generated 8 different con-volution supports corresponding to 8 pairs of W (l,s) ∈ R1433×8 (1433 features for each node) anda(I,S) ∈ R16×1 trainable weights for the first layer (Velickovic et al., 2018). We produce 240 (30for each support) random pairs of W (l,s) and a(l,s) where activation function is LeakyReLU has0.2 negative slope as in (Velickovic et al., 2018). Later, We calculated frequency response of gener-ated supports by Corollary 1.1. The mean and standard deviation of the frequency profiles for thesesimulated GAT supports are shown in Figure 3 a and its expected and standard deviation of the fullfrequency response shown in Figure 10.
Figure 10: Full frequency profile of GAT and its standard deviation.
Figure 11: Input image, and its filtering results by Φ1, Φ2 and Φ3 respectivelyIn order to assess ChebNet, GCN, GIN and GAT, we use a 3-layer GNN architecture whose input is aone-length feature (intensity of the pixel) and the number of neurons in hidden layers is respectively32, 64 and 64; the output layer is an MLP that projects the final node representation onto the singleoutput for each node. We used roughly 30k trainable parameter in ChebNet with 5 supports. Forthe other methods, we tuned the hidden neuron numbers in order to be sure that they have a similarnumber of trainable parameters. Since the aim is not assessing the generalization performance, wedo not use any regularization or dropout to address overfitting, but simply force the GNN to learnthe input-output relation. We keep the iterations till there is no improvement for consecutive 100iterations or maximum 3000 iterations.
Figure 12: The output of GNNs trained with band-pass task. Images are taken from ChebNet, GIN,GAT and GCN respectively.
Figure 13: Sample graph in Band-Pass graph dataset. Random rotated and translated image patternwith frequency of 1, random sampled points and their watershed regions, and graph represent theconnected region and average region intensity value respectively.
Figure 14: Two sample graphs in MNIST-75 dataset (from 0 and from 1 class), the location of thenodes is just for illustration. Models do not use the node positions.
