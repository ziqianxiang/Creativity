Figure 1: Convergence comparison (CIFAR-10 test accuracy) under different attacks. (In Appendix C.2, onecan find additional CIFAR-100 experiments, more discussions, and bigger plots.)We instantiate m = 10 workers and one master node executing data-parallel SGD for 140 passes (i.e.
Figure 3: Performance comparison under the variance attack.
Figure 4: Performance comparison under the sign-flipping attack.
Figure 5: Performance comparison under the delayed-gradient attack.
Figure 6: Performance comparison under the label-flipping attack.
Figure 7: Performance comparison under the safeguard attack with re-scale factor 0.6. (Recall this attack isdesigned to maximally impact the performance of our algorithm.)22Published as a conference paper at ICLR 2021Re-scale factor 0.6. In Figure 7, the performance of our (single and double) safeguard algorithms indeedget hurt a bit. Recall in Figure 7 the re-scale factor 0.6 is chosen to maximally impact our algorithm. The testaccuracy drops from 91.7% to 89.3% under CIFAR-10; and drops from 68.0% to 60.0% under CIFAR-100 (forboth single and double safeguards). In these cases, we confirm that both versions of the safeguard algorithmsdid not catch any bad worker. However, this still significantly outperforms all prior works.
Figure 8: Performance comparison under the safeguard attack with re-scale factor 0.7. (In this case, our ouralgorithm can catch some bad workers, and thus perform nearly optimally.)Re-scale factor 0.7. In Figure 8, we present the scenario when the re-scale factor is 0.7, so that the safeguardalgorithms can occasionally catch some bad workers (depending on the randomness and learning rate). Weconfirm that in the three runs of single safeguard, it catches 1, 2, 3 bad workers for CIFAR-10, and 1, 0, 0 badworkers for CIFAR-100 respectively; in the three runs of double safeguard, it catches 1, 2, 4 bad workers forCIFAR-10, and 2, 2, 2 bad workers for CIFAR-100 respectively.
