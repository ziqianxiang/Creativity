Figure 1: Model diagrams for the generative and inference procedures of the current frame xt , forvarious neural video compression methods. Random variables are shown in circles, all other quan-tities are deterministically computed; solid and dashed arrows describe computational dependencyduring generation (decoding) and inference (encoding), respectively. Purple nodes correspond toneural encoders (CNNs) and decoders (DNNs), and green nodes implement temporal autoregressivetransform. (a) TAT; (b) SSF; (c) STAT or STAT-SSF; the magenta box highlights the additionalproposed scale transform absent in SSF, and the red arrow from wt to vt highlights the proposed(optional) structured prior. See Appendix Fig. 7 for computational diagrams of the structured prior.
Figure 2: Visualizing the proposed STAT-SSF-SP model on one frame of UVG video “Shake-NDry”. Two methods in comparison, STAT-SSF (proposed) and SSF (Agustsson et al., 2020), havecomparable reconstruction quality to STAT-SSF-SP but higher bit-rate; the (BPP, PSNR) for STAT-SSF-SP, STAT-SSF, and SSF are (0.046, 36.97), (0.053, 36.94), and (0.075, 36.97), respectively. Inthis example, the warping prediction μt = hμ(^t-ι, [Wte) incurs large error around the dog's mov-ing contour, but models the mostly static background well, with the residual 山tents [Vte taking upan order of magnitude higher bit-rate than [W te in the three methods. The proposed scale parameter^t gives the model extra flexibility when combining the noise ^ (decoded from ([Vt], [Wt])) withthe warping prediction μt (decoded from [W te only) to form the reconstruction Xt = μt + ^t Θ ^:the scale σt downweights contribution from the noise yt in the foreground where it is very costly,and reduces the residual bit-rate R( [Vt]) (and thus the overall bit-rate) compared to STAT-SSF andSSF (with similar reconstruction quality), as illustrated in the third and fourth figures in the top row.
Figure 3: Rate-Distortion Performance of various models and ablations. Results are evaluated on(a) UVG and (b) MCLJCV datasets. All the learning-based models (except VCII (Wu et al., 2018))are trained on Vimeo-90k. STAT-SSF-SP (proposed) achieves the best performance.
Figure 4:	Qualitative comparisons of various methods on a frame from MCL-JCV video 30. Fig-ures in the bottom row focus on the same image patch on top. Here, models with the proposedscale transform (STAT-SSF and STAT-SSF-SP) outperform the ones without, yielding visually moredetailed reconstructions at lower rates; structured prior (STAT-SSF-SP) reduces the bit-rate further.
Figure 5:	Ablations & Comparisons. (a) An ablation study on our proposed components. (b)Performance of SSF (Agustsson et al., 2020) trained on different datasets. Both sets of results areevaluated on UVG.
Figure 6: Backbone module architectures, where “5x5/2, 128” means 5x5 convolution kernel withstride 2; the number of filters is 128.
Figure 7: Computational flowchart for the proposed STAT-SSF-SP model. The left two subfig-ures show the decoder and encoder flowcharts for wt and vt , respectively, with “AT” denotingautoregressive transform. The right two subfigures show the prior distributions that are used for en-tropy coding wt and vt, respectively, with p(wt, wth) = p(wth)p(wt|wth), and p(vt, vth|wt, wth) =p(vth)p(vt |vth, wt , wth), with wth and vth denoting hyper latents (see (Agustsson et al., 2020) fora description of hyper-priors); note that the priors in the SSF and STAT-SSF models (withoutthe proposed structured prior) correspond to the special case where the HyperDecoder for vtdoes not receive wth and wt as inputs, so that the entropy model for vt is independent of wt :p(vt,vth) =p(vth)p(vt|vth).
