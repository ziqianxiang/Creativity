Figure 1: Comparison between backdoor examples generated by our method and by the pre-vious backdoor attacks. Given the original image (leftmost), we generate the corresponding back-door images using patch-based attacks (GU et al., 2017; LiU et al., 2018b), blending-based attack(Chen et al., 2017), SIG (Barni et al., 2019), ReFool (Liu et al., 2020), and our method. For eachmethod, We show the image (top), the magnified (×2) residual map (bottom). The images generatedfrom the previous attacks are unnatural and can be detected by humans. In constrast, ours is almostidentical to the original image, and the difference is unnoticeable.
Figure 2: Process of creating the warping field M and using it to generate poisoned images.
Figure 3: Effect of different hyper-parameters on the warping result. For each warped image,we show the image (top), the magnified (×2) residual map (bottom). The PSNR and LPIPS (Zhanget al., 2018) scores are computed at resolution 224×224.
Figure 4: Training pipeline with three running modes.
Figure 5: Attack experiments. In (b), we provide the clean (top) and backdoor (bottom) images.
Figure 6:	Human inspection tests: (a) Success fooling rates of each backdoor method, (b) Themost distinguishable cases from WaNet.
Figure 7:	Experiments on verifying WaNet by the state-of-the-art defense and visualization methods.
Figure 8: Ablation studies on CIFAR-10 dataset: (a) Role of the noise mode training, (b,c) Networkperformance when changing warping hyper-parameters.
Figure 9: Networks, performance against Neural Cleanse with and without noise mode.
Figure 11: Fine-pruning against all-to-all scenario.
Figure 12: STRIP against all-to-all scenario.
Figure 13: Additional images for mentioned backdoor attack methods.
Figure 15: Additional trigger patterns optimized by Neural Cleanse for the target label (small isbad).
