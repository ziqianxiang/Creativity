Figure 1: An illustrative example of the construction of the CNML distribution. Left We wish toestimate the p(y|x) at some query point x, marked by the vertical red line. Middle We compute theMLE assuming we knew the true label y, for every possible value of y. Right Finally, we normalizethe predictions across all MLE models to produce pNML. The final prediction will likely exhibit largeamounts of uncertainty on queries x far from the dataset, because it is easier for the individual MLEestimates to overfit to these outliers.
Figure 2: A diagram of thediscretized logistic architecture.
Figure 3: A comparison of uncertainty estimates between quantized NML and bootstrapped ensem-bles. Left: A small training dataset collected using the ground truth function sin(x) and quantizedinto 32 bins. Each dot represents a data point. Middle: Predictions from quantized CNML, wheredarker regions indicate outputs with high estimated probability. The ground truth is marked by adotted blue line. Note that in regions where there is little data, the NML distribution tends to cor-rectly output a very diffuse distribution with uncertain outputs. Right: Predictions from a bootstrapensemble of 32 models. In out-of-support regions far from the data, the bootstrap ensemble tends tounderestimate uncertainty and produce overconfident predictions.
