Figure 1: Overview of HamburgerThough self-attention and its variants achieved great success, researchers are confronted with (1)developing new global context modules based on self-attention, typically via hand-crafted engineering,and (2) explaining why current attention models work. This paper bypasses both issues and findsa method to easily design global context modules via a well-defined white-box toolkit. We try toformulate the human inductive bias, like the global context, as an objective function and use theoptimization algorithm to solve such a problem to design the module’s architecture. The optimizationalgorithm creates a computational graph, takes some input, and finally outputs the solution. We applythe computational graph of optimization algorithms for the central part of our context module.
Figure 2: One-step GradientA thought experiment is to consider t → ∞, leading to a fullyconverged result h* and infinite terms in Eq. (12). We supposethat both F and G are Lipschitz with constants Lh w.r.t. h, Lx w.r.t. x, and LG, and Lh < 1. Notethat these assumptions apply to a large number of optimization or numerical methods. Then we have:Proposition 1 {hi }t has linear convergence.
Figure 3: Ablation on d and rBreads and Hams We ablate each part of theHamburger. Removing MD (ham) causes the mostsevere decay in performance, attesting to the impor-tance of MD. Even if only the parameter-free MD isadded (only ham), the performance can visibly im-prove. Parameterization also helps the Hamburgerprocess the extracted features. Bread, especiallyupper bread, contributes considerable performance.
Figure 4: Ablation on K3 4 5 6 7 8 9 10 12 15 20 30eval177.7577.50L 77.25-77.00-76.75-76.506Published as a conference paper at ICLR 2021Iterations K We test more optimization steps in the evaluation stage. In general, the same Kfor training and test is recommended. K = 6 is enough for CD and NMF, while even K = 1 isacceptable for VQ. Typically 3~6 steps are enough since simple MD's prior is still biased, and fullconvergence can overfit it. The few iterations are cheap and act as early stopping.
Figure 5: Accumulative RatioFigure 6: Visualization of feature maps3.3	A Comparison with AttentionThis section shows the advantages of MD-based Hamburger over attention-related context modules incomputational cost, memory consumption, and inference time. We compare Hamburger (Ham) withself-attention (SA) (Vaswani et al., 2017), Dual Attention (DA) module from DANet (Fu et al., 2019),Double Attention module from A2 Net (Chen et al., 2018b), APC module from APCNet (He et al.,2019b), DM module from DMNet (He et al., 2019a), ACF module from CFNet (Zhang et al., 2019b),reporting parameters and costs of processing a tensor Z ∈ R1×512×128×128 in Tab. 3. Excessivememory usage is the key bottleneck of cooperating with attention in real applications. Hence we alsoprovide the GPU load and inference time on NVIDIA TITAN Xp. In general, Hamburger is light incomputation and memory compared with attention-related global context modules.
Figure 6: Visualization of feature maps3.3	A Comparison with AttentionThis section shows the advantages of MD-based Hamburger over attention-related context modules incomputational cost, memory consumption, and inference time. We compare Hamburger (Ham) withself-attention (SA) (Vaswani et al., 2017), Dual Attention (DA) module from DANet (Fu et al., 2019),Double Attention module from A2 Net (Chen et al., 2018b), APC module from APCNet (He et al.,2019b), DM module from DMNet (He et al., 2019a), ACF module from CFNet (Zhang et al., 2019b),reporting parameters and costs of processing a tensor Z ∈ R1×512×128×128 in Tab. 3. Excessivememory usage is the key bottleneck of cooperating with attention in real applications. Hence we alsoprovide the GPU load and inference time on NVIDIA TITAN Xp. In general, Hamburger is light incomputation and memory compared with attention-related global context modules.
Figure 7: Impacts of K on NMF-76.50INm 寸tn9N8 6 OI NISI ONE匕Figure 8: Impacts of K on CDVQ_MEAN77.2577.0076.7576.5076.2576.001 2 3 4 5 6 7 8 9 10 12 15 20 30evalFigure 9: Impacts of K on VQ19Published as a conference paper at ICLR 2021G	An Intuitive IllustrationIn this section, we hope to give an example to help our readers develop insight into why the low-rankassumption is useful for modeling the representations’ global context.
Figure 8: Impacts of K on CDVQ_MEAN77.2577.0076.7576.5076.2576.001 2 3 4 5 6 7 8 9 10 12 15 20 30evalFigure 9: Impacts of K on VQ19Published as a conference paper at ICLR 2021G	An Intuitive IllustrationIn this section, we hope to give an example to help our readers develop insight into why the low-rankassumption is useful for modeling the representations’ global context.
Figure 9: Impacts of K on VQ19Published as a conference paper at ICLR 2021G	An Intuitive IllustrationIn this section, we hope to give an example to help our readers develop insight into why the low-rankassumption is useful for modeling the representations’ global context.
