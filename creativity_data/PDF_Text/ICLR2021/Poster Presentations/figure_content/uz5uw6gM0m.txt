Figure 1: Our framework shows that it is possible to learn analytic functions such as the gravitationalforce law, decision trees with different functions at the leaf nodes, and programming constructs suchas those on the right, all using a non-modular monolithic architecture.
Figure 2: Some of the task codings which fit in our framework. On the left, we show a task codingvia clusters. Here, c(i) is the code for the ith cluster. On the right, we show a task coding based onlow-depth decision trees. Here, ci is the ith coordinate of the code c of the input datapoint.
Figure 3: Binary classification on multiple clusters, results are an average over 3 trials. A single neuralnetwork does well even when there are multiple clusters. The error does not increase substantially onincreasing the number of clusters k3	ExperimentsWe next empirically explore the learnability of multiple functions by a two layer neural network whenthe tasks are coded by well-separated clusters or decision trees, and more generally the learnabilityof SQL-style aggregation for a fixed database. We find good agreement between the empiricalperformance and the bounds of Section 2. See Appendix D for more details of the experimental setup.
Figure 4: Learning random homogeneous polynomials of 4 variables and degree 4 on the leaves of adecision tree, the results are averaged over 7 trials. (a) Sample complexity scales as eO(h IOg(I/'"γ’with error , where error is measured by (1-Test R-squared). (b) For fixed tree depth, accuracyincreases with increasing margin.
Figure 5: RMSE vs number of bodies k for learning gravitational force law for different kernels.
Figure 6: Experiment where data is clustered into tasks with a separate linear function for each task.
