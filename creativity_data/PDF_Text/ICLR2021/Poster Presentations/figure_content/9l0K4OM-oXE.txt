Figure 1: The pipeline of backdoor erasing techniques. (a) The standard finetuning process, (b) ourproposed NAD approach, and (c) our NAD framework using ResNet (He et al., 2016) as an exam-ple. NAD erases backdoor trigger following a two-step procedure: 1) obtain a teacher network byfinetuning the backdoored network with a subset of clean training data, then 2) combine the teacherand the student through the neural attention distillation process. The attention representations arecomputed after each residual group, and the NAD distillation loss is defined in terms of the attentionrepresentations of the teacher and the student networks.
Figure 2: Performance of4 backdoor erasing methods under different % of available clean data. Theplots show the average ASR (left) and ACC (right) over all 6 attacks. NAD significantly reduces theASR to nearly 0% with 20% clean data.
Figure 4: Comparison of 4 distillation combinations on CIFAR-10. The B, B-F, and C representlo0oooolooo098765432(％) AOaInOOe Uee-O100	„	-	~BadNets Trojan Blend CL SIGbackdoored model, finetuned backdoored model, and model trained on the clean subset, respectively.
Figure 5: Performance of NAD with teachers trained on various % of clean CIFAR-10 data.
Figure 6: Examples of backdoored CIFAR-10 images by the 6 attacks.
Figure 7: Convergence rate comparison between MCR and NAD against BadNets attack with 5%clean training data. We show the best result ofMC at the connection point t = 0.3. Note that it takeslonger for MCR to converge yet its ASR is still higher than that of the NAD’s.
Figure 8: Parameter analysis: performance of our NAD approach under different β.
Figure 9: Iterative NAD and Finetuning against Figure 10: Erasing all-target BadNets attack.
Figure 11: The activation map of one backdoored image at Group 3 of WRN-16-1 for (a) BadNets,(b) Finetuned BadNets with 5% of clean training data, and (c) BadNets erased by our NAD with 5%of clean training data. Each small patch is a channel (64 channels in total). The small red circleshighlight the regions that are fired by the trigger pattern at different channels of the activation map.
Figure 12: The attention maps derived by 5 different attention functions are shown for (a) BadNet,(b) Finetuned BadNet by 5% clean training data, and (c) BadNets erased by our NAD.
Figure 13:	The learning curves (test ASR and ACC) of the NAD student network and a Finetuningnetwork on CIFAR-10 against BadNets (left) and CL (right). In NAD, the student network tends tooverfit to the teacher network, unless an early stopping is applied based on the validation ACC.
Figure 14:	The triggers used by an adaptive BadNets attack against our NAD under different α (ascaling factor of the original black-white square). The trigger patterns are all placed at the center ofthe image.
