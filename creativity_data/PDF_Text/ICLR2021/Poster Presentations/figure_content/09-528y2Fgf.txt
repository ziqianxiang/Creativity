Figure 1: The architecture of TUPE. The positional correlation and word correlation are computedseparately, and added together in the self-attention module. The positional attention related to the[CLS] token is treated more positionless, to encourage it captures the global information.
Figure 2: Visualizations of the four correlations (Eq. (6)) on a pre-trained BERT model for a sampledbatch of sentences. From left to right: word-to-word, word-to-position, position-to-word, andposition-to-position correlation matrices. In each matrix, the (i-th, j-th) element is the correlationbetween i-th word/position and j-th word/position. We can find that the correlations between a wordand a position are not strong since the values in the second and third matrices look uniform.
Figure 3: Instead of adding the absolute positional embedding to the word embedding in the input(left), We compute the positional correlation and word correlation separately with different projectionmatrices, and add them together in the self-attention module (right).
Figure 4: Illustration of untying [CLS]. Vij denotes the positional correlation of pair (i,j). The firstrow and first column are set to the same values respectively.
Figure 5: Both TUPE-A and TUPE-R converge much faster than the baselines, and achieve betterperformance in downstream tasks while using much fewer pre-training steps.
Figure 6: The visualization of learned positional correlations by TUPE-A.
