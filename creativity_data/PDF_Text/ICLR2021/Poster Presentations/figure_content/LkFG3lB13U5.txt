Figure 1: Validation accuracy of adaptive and non-adaptive methods, as well as SCAFFOLD, usingconstant learning rates η and ηl tuned to achieve the best training performance over the last 100communication rounds; see Appendix D.2 for grids.
Figure 2: Validation accuracy (averaged over the last 100 rounds) of FedAdam, FedYogi, andFedAvgM for various client/server learning rates combination on the SO NWP task. For FedAdamand FEDYOGI, we set τ = 10-3.
Figure 3: Validation performance of FEDADAGRAD, FEDADAM, and FEDYOGI for varying τ onvarious tasks. The learning rates η and ηl are tuned for each τ to achieve the best training performanceon the last 100 communication rounds.
Figure 4: Validation accuracy on EMNIST CR using constant learning rates η, ηl, and τ tuned toachieve the best training performance on the last 100 communication rounds; see Appendix D forhyperparameter grids.
Figure 5:	Validation accuracy (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on theCIFAR-10 task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 6:	Validation accuracy (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on theCIFAR-100 task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 7:	Validation MSE (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on theEMNIST AE task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 8:	Validation accuracy (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on theEMNIST CR task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 9:	Validation accuracy (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on theShakespeare task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 10:	Validation recall@5 (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on the StackOverflow LR task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 11:	Validation accuracy (averaged over the last 100 rounds) of FedAdagrad, FedAdam,FedYogi, FedAvgM, and FedAvg for various client/server learning rates combination on the StackOverflow NWP task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set τ = 10-3.
Figure 12:	The best server learning rate in our hyperparameter tuning grids for each client learning rate,optimizers, and task. We select the server learning rates based on the average validation performanceover the last 100 communication rounds. For FedAdagrad, FedAdam, and FedYogi, we fixτ = 10-3 . We omit all client learning rates for which all server learning rates did not change theinitial validation loss by more than 10%.
Figure 13:	Validation performance of FEDADAGRAD, FEDADAM, and FEDYOGI for varying τ onvarious tasks. The learning rates η and ηl are tuned for each τ to achieve the best training performanceon the last 100 communication rounds.
Figure 14: The distribution of the number of unique labels amongfederated CIFAR-100 dataset.
