Figure 1: Waypoints for audio-visual navigation: Given egocentric audio-visual sensor inputs (depth and binauralsound), the proposed agent builds up both geometric and acoustic maps (top right) as it moves in the unmappedenvironment. The agent learns encodings for the multi-modal inputs together with a modular navigation policyto find the sounding goal (e.g., phone ringing in top left corner room) via a series of dynamically generatedaudio-visual waypoints. For example, the agent in the bedroom may hear the phone ringing, identify that it is inanother room, and decide to first exit the bedroom. It may then narrow down the phone location to the diningroom, decide to enter it, and subsequently find it. Whereas existing hierarchical navigation methods rely onheuristics to determine subgoals, our model learns a policy to set waypoints jointly with the navigation task.
Figure 2: Model architecture. OUr audio-visual navigation model uses the egocentric stream of depth images andbinaural audio (Bt) to learn geometric (Gt) and acoustic (At) maps for the 3D environment. The multi-modalcues and partial maps (left) inform the RL policyâ€™s prediction of intermediate waypoints (center). For eachwaypoint, the agent plans the shortest navigable path (right). From this sequence of waypoints, the agent reachesthe final AudioGoal efficiently.
Figure 3: Navigation trajectories on top-down maps vs. all existing AudioGoal methods. Agent path fades fromdark blue to light blue as time goes by. Green is the shortest geodesic path in continuous space. All agents havereached the goal. Our waypoint model navigates to the goal more efficiently. The agent,s inputs are egocentricviews (Fig. 1); figures show the top-down view for ease of viewing the full trajectories.
Figure 4: Analysis of selected waypoints (a,c) and accuracy vs. microphone noise (b). See text.
