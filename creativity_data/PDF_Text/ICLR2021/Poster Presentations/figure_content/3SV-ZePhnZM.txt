Figure 1: Comparison of performance for incremental few-shot regression. Red dots denote testsamples for base task, purple dots denotes test sample for novel tasks, grey lines denote modelpredictions, and black crosses denotes few-shot training samples in novel tasks. (a) IDLVQ-R; (b)neural networks incrementally fine-tuned with novel data only in each session; (c) neural networksincrementally fine-tuned with exemplars and novel training samples; (d) offline neural networkstrained with training samples from all tasks.
Figure 2: Comparison results of different few-shot settings, evaluated with ResNet18 on CUBdatasetA.6 Visualization of IDLVQ-CWe show the visualization of standard neural networks and IDLVQ-C with/without intra-class vari-ation loss in Fig. 3. MNIST dataset is used as a toy example for visualization. Classes 0-7 are oldclasses with sufficient training samples, and classes 8 and 9 are novel classes with few-shot train-ing samples. It can be observed in Fig. 3(a) and 3(b) that standard neural networks and IDLVQ-C(without intra-class variation loss) trained by cross-entropy loss do not have compact intra-classvariation. Consequently, features of novel classes are more likely to overlap with old classes. Inthis case, the performance of class-incremental learning degrades very quickly because the classifiercannot distinguish between features from different classes. In comparison, the proposed IDLVQ-Cmakes intra-class variation compact and leaves large margin between classes in Fig 3(c). As a result,the features of novel classes are less likely to overlap existing classes. The compact intra-class varia-tion and large margin between classes make features of novel classes distinguishable so that learningnovel classes is easier. In addition, the margin based loss only updates the model parameters whennecessary and avoids catastrophic forgetting of old classes.
Figure 3: Visualization of feature spaces in different methods. Dots represent features of samplesand crosses denote references vectors of classes. (a) standard neural networks; (b) IDLVQ-C withoutLintra; (c) IDLVQ-C.
