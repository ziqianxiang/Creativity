Figure 1: For random forest students and teachers, cross-fitting improves student performance when the teacheroverfits, while loss correction improves student performance when the teacher underfits.
Figure 2: On CIFAR-10 with ResNet students and teachers, cross-fitting reduces the effect of teacher overfitting,and loss correction yields an additional small performance boost. Here, the test loss is cross-entropy.
Figure 3: On CIFAR-10 with ResNet students and teachers, large values of the loss correction hyperparameter α(corresponding to the orthogonal loss correction) lead to large variance and training instability, while intermediatevalues improve upon cross-fit KD without loss correction (α = 0). Here, the test loss is cross-entropy.
Figure 4:0⊃< ttωH0.8850.β801 2 3	5	10	15	20Teacher's max tree depth1 2 3	5	10	15	20Teacher's max tree depthA3e」n8±s"l1 2 3	5	10	15	20Teacher's max tree depth(a)	Adult dataset0.94(b)	FICO dataset(c)	Higgs dataset0.90o< 0.β8⅛® 0.86
Figure 5: Tabular random forest distillation with varying teacher complexity.
