Figure 1:	Left: 3D probability distribution tensor, only nonzero values are indicated with coloredcubes, all empty space is assumed to be filled with zero-valued cubes. Middle left: an example ofan additive transformation conditioned on x3: y1 = x1 + bt1 (x3)e, y2 = x2 + bt2(x3)e, y3 = x3.
Figure 2:	Visualization of an IDF that has learned to factorize the probability distribution of the toyexample in (5). Left: empirical densities of the data, the data transformed by one additive bijector,and the data transformed by two additive bijectors. Right: similar to the left plot, but with datasampled from the model.
Figure 3: Left: Bits per dimension for models trained for 2000 epochs on CIFAR-10. Continuous(cont) and discrete (disc) models are trained with different combinations of translations activationfunctions (→) and backward (—) substitute functions for the straight-through estimators: a softrounding function (σT ), a hard rounding function (rnd) and the identity function (id). σT interpolatesbetween the identity function at T = 0 and the hard rounding function at T = 1. Right: Bits perdimension of IDF models trained for 300K iterations (±960 epochs) with different architecturesfor the translation networks in the coupling layers: convolutional neural networks, ResNets andDenseNets. The models have 3 levels. The discrete ResNet model for 10 flows per level was unstableso the result for this model is omitted. The models with the DenseNet architecture correspond to thebest models used by Hoogeboom et al. (2019a). The ConvNets and ResNets have approximatelyequal number of parameters and depth as the DenseNets. Note that the ConvNets used to produce theresults in Figure 5 of (Hoogeboom et al., 2019b) are much shallower. The DenseNet++ architecturescorrespond to the proposed IDF++ model of Section 6.
Figure 4: Top two panels: Agreement between the approximate (true) gradient and finite differencegradient of the discrete (continuous) model at model initialization (left), and after 3000 trainingiterations (right). The curves were obtained by averaging over 10 batches. The higher variance forthe 8-bit case is due to the larger number of different datapoints, leading to more variance in the databatches. Bottom two panels: Visualization of the loss surface (left) and trajectory of the optimizer onthe loss surface (right) for the discrete model on the 1-bit toy example, following (Li et al., 2018).
Figure 5: probability distribution of the extension of the toy example of Eq. (5) to more bits.
Figure 6: Visualization of the trajectory of the optimization on the loss surface (Li et al., 2018) forthe continuous model for the toy example with 1-bit data.
Figure 7: Visualization of the trajectory of the optimization on the loss surface (Li et al., 2018) forthe discrete model for the toy example with 8-bit data.
Figure 8: Visualization of the trajectory of the optimization on the loss surface (Li et al., 2018) forthe continuous model for the toy example with 8-bit data.
Figure 9: Soft rounding function σT for different temperatures T . In the limit T = 0 the softrounding function reduces to the hard rounding function b∙]. At T = 1 the soft rounding function isindistinguishable from the identity function.
Figure 10: Schematic visualization of the effect of inverse permutations on the IDF++ architecture.
Figure 11: The performance of IDF++ models and their continuous counterparts on the validation setfor 8 and 10 flows per level (fpl). The models are trained on 80% of the training data.
