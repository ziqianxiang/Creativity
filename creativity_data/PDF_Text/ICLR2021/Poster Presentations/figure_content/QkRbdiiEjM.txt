Figure 1: The RNN-like architecture of AdaGCN with each base classifier fθI) sharing the sameneural network architecture fθ. Wl and θι denote node weights and parameters computed after thel-th base classifier, respectively.
Figure 2: Comparison of the graph model architec-tures. fa in JK network denotes one aggregationlayer with aggregation function such as concatena-tion or max pooling.
Figure 3:	Comparison of test accuracy of different models as the layer increases. We regard the l-thbase classifier as the l-th layer in AdaGCN as both of them are leveraged to exploit the informationfrom l-th order of neighbors for current nodes.
Figure 4:	Left: Per-epoch training time of AdaGCN vs other methods under 5 runs on four datasets.
Figure 5: AdaSGC vs AdaGCN.
