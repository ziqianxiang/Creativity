Figure 1: Comparison (to scale) of the expansion caused by the tensor-product activation applied toinputs of equal representational capacity but different structure. With depth representing the numberof channels and width the number of fragments for each degree, itis clear that by grouping fragmentsinto K separate channels the expansion (and therefore cost) can be K-times reduced. Visualizationcorresponds to inputs with pL, K) equal to p3, 1) and p3, 3) for panel (a) and (b), respectively.
Figure 2: Visualization of the mixing set PL (for L “ 7 and ' “ 4) and the approaches to subsettingbased on the minimum spanning tree (MST) and reduced minimum spanning tree (RMST) mixingpolices, which reduces related computation costs from OpL2q to, respectively, OpLq or Oplog Lq.
Figure 3: Comparison of computation and memory footprints between the generalized sphericalCNN layers of Kondor et al. (2018) and our efficient generalized layers. The reduction in cost dueto our efficient layers is given as multiplicative factors in the lower plot of each panel.
Figure 4: Visualization of the architecture used for the convolutional base in our hybrid models. Theinput to the first convolutional layer is a signal on the sphere. The output from the final convolutionallayer are scalar values corresponding to fragments of degree ` “ 0, which are then mapped throughsome fully connected layers to give the model output.
