Figure 1: From state observation at time step t, the agent generates a natural language instruction“go to key and press grab,” which guides the agent to grab the key. After the instruction is fulfilledand the agent grabs the key, the agent generates a new instruction at t + 1.
Figure 2: (Left) Example view of game interface that the worker would see on AMT. On the leftthe worker is given the goal and recipes; the board is in the middle; the worker provides annotationson the right. (Right) Example sequence of instructions provided by the Turker for the given task ofStone Pickaxe.
Figure 3: (Left) High-level language generator. (Right) Low-level policy conditioned on language.
Figure 4: Comparing baselines with our method on accuracy. Human demonstrations are necessaryto complete tasks with 3 or more steps. Averaged over 3 runs.
Figure 5: Ablation of our method with varying amounts of human annotations (25%, 50%, 75% and100%). For each fraction, we sample that number of demonstrations from the dataset for each typeof task. Averaged over 3 runs.
Figure 6: Generated language at test time for a 2-step craft. We only display key frames of thetrajectory which led to changes in the language. These key frames match changes in the inventoryto the object mentioned in the generated instruction. Qualitatively, the generated instructions areconsistent during what we would describe as a sub-task. Quantitatively, the network will spend onaverage 4.8 steps in the environment for the same generated language output.
Figure 7: Workflow to collect human demonstrations for our dataset.
Figure 8: Demo instructions for AMT workers.
Figure 9: Example board and goal configuration where the goal is to make an iron ore. The workeruses the recipes provided to give appropriate instructions and execute accordingly.
Figure 10: A more in-depth example of 3 out of the 14 training tasks to show how the subtasks arerelated (red boxes = final craft, blue boxes = raw material).
Figure 11: (Left) Instruction frequency (Middle) Word frequency (Right) Histogram of instructionlengths.
Figure 12: At each time step we encode state relevant observations including the goal, inventory,and grid. This encoding is utilized by both the language generator and the language-conditionedpolicy. The boxes in green, denote the observations that were encoded using the GloVe embedding.
