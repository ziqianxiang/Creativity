Figure 1: An illustration of the processing pipeline. An input graph G is processed by a messagepassing network (Equation 1). The output layer is a GEN, and produces node scores ~ν and edgescores E. Algorithm 1 translates the node and edge scores into an edit script S which, when appliedto the input graph G, constructs the predicted graph S(G).
Figure 2: Two graph time series from the degree rules dataset. Blue arrows indicate graph dynamics,labelled with the teaching signal.
Figure 3: An illustration of the conversion of a script S = del2, insχι, ins~2, del3 to a graph mappingψδ. We start with an initial isomorphism ψ between the graph H and the graph H = δ(G) (left)and then adjust the mapping from left to right until we arrive at a final graph map between G and H(right). The mapping is obtained by following the arrows from G to H. For simplicity we assumethat the graphs have no edges. Also note that we include the ’virtual nodes’ M + 1, . . . , M + N inthe illustration.
Figure 4: A graphical illustration of the construction of a teaching signal viaAlgorithm 2 for the inputs G =	({1, 2, 3}, {(1, 2), (1, 3), (3, 1)}, X), H =({1,2,3,4,5,6},{(1,2),(2,3),(3,1),(4,5),(5,4)},X0), and ψ with ψ(1) = 3, ψ(2) = 2,and ψ(3) = 1. The mapping ψ is shown in blue, the auxiliary construction of H (lines 2-5 in thealgorithm) in red and dashed. After constructing H, We first compute shortest paths from 3 toeach node in C1 and C2 (top right). This defines the shortest path tree with children ch(3) = (4, 6)and ch(4) = 5. Accordingly, node ψ-1(3) = 1 needs to insert tWo nodes (4 and 6), and nodeψ-1(4) = 4 needs to insert one node (5). Since this is not possible in one step, the first step insertsonly node 4 (ν1,1 = +1) and the second step nodes 5 and 6 (ν2,1 = +1, ν2,4 = +1). The remainingedit scores ensure that the correct edges are inserted and deleted, yielding the final script at thebottom. This is exactly the script generated by ψ, except for four superfluous auxiliary edits, namelyeins1,4, eins1,6, edel1,4, and edel1,6.
Figure 5: An illustration of the loss `node (left) and the loss `edge (right) for different teacher andadjacency matrix inputs.
Figure 6: Left: The node edit probabilities p(ins∣ν) (blue), p(del∣ν) (orange), and p(rep∣ν) (red) forβ = 10 for varying ν (left). Right: The crossentropy loss (solid) and the GEN loss times β (dashed)for varying V and for ^ = +1 (blue), V= -1 (orange), and ^ = 0 (red).
Figure 7: Left top: The edge insertion probability p(eins|, a) if the edge is not present. Right top:The crossentropy loss (solid) and the GEN edge loss times β (dashed) for ^ = +1 (blue) and ^ = 0(red) if the edge is not present. Left bottom: The edge deletion probability p(edel|, a) if the edge ispresent. Right bottom: The crossentropy loss (solid) and the GEN edge loss times β (dashed) forV = -1 (orange) and V = 0 (red) if the edge is present.
Figure 8: An illustration of the edit cycles dataset, where each graph is shown in black, whereascolored arrows illustrate the system dynamics. Each colored arrow is labelled with the edit scoresnecessary to move from one graph to the next.
Figure 9: An example 6 × 6 grid evolving over time according to the rules of Conway’s game of lifewith alive cells marked blue and dead cells white. Note that even a single alive cell close to a ’glider’shape breaks the shape and leads to the entire board becoming empty over time.
Figure 10:	An example time series from the Boolean dataset. The initial formula on the left issimplified until no simplification rule applies anymore. Each arrow indicates one simplification step.
Figure 11:	An example time series from the Peano dataset. The initial formula on the left is evaluateduntil only a single number - namely the result of the addition - is left. Exponents indicate the index ofeach node in the graph and blue arrow labels indicate the teaching protocol.
