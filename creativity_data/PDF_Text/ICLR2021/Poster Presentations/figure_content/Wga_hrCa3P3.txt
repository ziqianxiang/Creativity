Figure 1: Concept. (a) Training seq2seq With teacher forcing. (b) Naive contrastive learning With randomlysampled negative examples. (c) Our method, CLAPS, which generates hard negative and positive examples.
Figure 2: Accuracy of classify-ing a positive pair from negativepairs varying batch size without anyto have meaningfully difficult examples in the batch. Moreover, discriminating positive and naivenegative pairs becomes even more easier for models pretrained on large text corpora.
Figure 3: Generation of imposters and distant-targets with perturbation. (a) We add small perturbationδt to ht for Zy so that its conditional likelihood is minimized to generate an invalid sentence. (b) We add largeperturbation Zt to ht for Zy by maximizing the distance from Zχ, the representation of source sentence butenforcing its likelihood high to preserve the original semantics.
Figure 4: Visualization. (a) Embedding space with-out contrastive learning. (b) Embedding space with ourproposed contrastive learning, CLAPS.
