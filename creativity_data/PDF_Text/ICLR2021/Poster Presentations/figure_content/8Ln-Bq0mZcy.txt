Figure 1: Partners form conventions in a collaborative task through repeated interactions. An AI agent canadapt quickly to conventions with new partners by reusing a shared rule representation gt and learning a newpartner-specific convention representation gp. Certain collaborative tasks, such as friendly Rock-Paper-Scissors,are more convention-dependent than others, such as 4-player chess.
Figure 2: Collaborative contextual bandit task. On the left we see a task with 2 contexts represented by rowsand 4 arms represented by cells. At each context, the prize if both partners pull the same green arm is 1, and 0otherwise. In the middle, we see a possible convention that two partners may converge to: pulling a1 in contextA, and pulling a2 in context B. On the right, when the task changes in context A but stays the same in context B,we expect the same two partners to continue to pull a2 in context B.
Figure 3: Generating process of part-ner’s actions for a two-player MDP. Foreach task we first sample a reward func-tion Rt for the MDP. This determinesthe Q-function at each state. For eachpartner i at each state s of task t, wesample a function ρi that maps the Q-function at state s to an action aist .
Figure 4: Policy network With separate task/partner modules to separate rule/convention representations. Thetask module gt maps the state observations to an action distribution gt (a|s) and to latent variables z. Eachpartner module maps Z to another action distribution gp(a|z). The policy of the ego agent when playing withpartner i is set to be proportional to the product of gt(a∣s) and gp(a|z).
Figure 5: User study on the collaborative contextual bandit task. Participants were asked to coordinate on theTrain task and, immediately afterwards, with the same partner on the Test task (shown on the right). Somecontexts have two equally optimal actions (green boxes), requiring symmetry breaking. On the left we plot thescore, averaged across participants, of the first try at each context. Our participants were able to coordinate wellon Test-C on the first try, even though the context requires symmetry breaking.
Figure 6: Contextual bandit game: adapting to a single new partner. In Figures 6a-c, we train and test onhand-designed partners. “Arms m” refers to a task having m contexts with symmetries (exact task details in theAppendix). In Figure 6d, we train and test on partner policies derived from the data collected in our user study,and we use the Train task shown in Figure 5.
Figure 7: Wasserstein distance between task module output and true marginals assuming uniform preferenceover optimal actions. Lower is better. We do not compare with BaselineModular since it does not have outputthat can be interpreted as marginals. The left three sets of bars are for the contextual bandit task, and the right setof bars are for the blocks task. We omit Hanabi since it is not easy to determine which actions are optimal. It isinteresting that even without regularization λ = 0.0, the distance to ground truth is still lower than the baselines,suggesting the model is learning some level of task-specific representation just due to the architecture.
Figure 8: Block placing task: each row displays a new round of the task. On the left, we see the goal-grid andhow it appears to each player. Since P2 cannot see the goal-grid, we show a fully grey grid. On the right-side wesee the working grid evolve over the course of 6 turns. P1 edits the red block on turns 1, 3, 5 and P2 edits theblue block on turns 2, 4, 6.
Figure 9: Adapting to a single new partner for block placing task and Hanabi.
Figure 10: Zero shot performance on new task/partner combination. Higher is better. Orange refers to our methodwith λ = 0.5 and grey refers to BaselineModular. Arms m means the task has m contexts with symmetries. Wecreate the new task by altering contexts where there is only one good action (as a result, we omit Arms 4 since ithas no contexts with only one good action). We do not compare with BaselineAgg since it is non-modular andcannot be composed for new task/partner combinations.
Figure 11: Contextual bandit game: adapting to a single new partner. Similar to Figure 6, but instead we trainand test on self-play partners.
