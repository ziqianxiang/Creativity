Figure 1: Suppose we observe a Cheetah balancing on its front leg (left). Given a simulator for theenvironment, Deep RLSP is able to infer how the cheetah must have acted to end up in this position. Itcan then imitate these actions in order to recreate this skill. Note that the state contains joint velocitiesin addition to positions, which makes the task more tractable than this picture might suggest.
Figure 2: We sample a few states from a policy performing a specific skill to provide as input. Here,Deep RLSP learns to balance the cheetah on the front leg from a single state. We provide videos of theoriginal skills and learned policies at: https://sites.google.com/view/deep-rlsp.
Figure 3: Reproduction of part of Figure 2 in Shah et al. (2019) illustrating the gridworld environmentsthat we test on.
Figure 4: Learning curves for training a discriminator to distinguish the learned skill from the originalskill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is moresimilar to the original skill, that is, higher is better.
Figure 7: Deep RLSP learning the balancing skill from 50 states. The first row shows the original policy from DADS, the next five rows show the sampled states fromthis policy, the second to last row is the GAIL algorithm, and the last row shows the policy learned by Deep RLSP.
Figure 10: Deep RLSP learning the jumping skill from 50 states. The first row shows the original policy from DADS, the next five rows show the sampled states fromthis policy, the second to last row is the GAIL algorithm, and the last row shows the policy learned by Deep RLSP.
