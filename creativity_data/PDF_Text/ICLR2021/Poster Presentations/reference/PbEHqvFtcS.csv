title,year,conference
 Byzantine stochastic gradient descent,2018, In Advances in NeuralInformation Processing Systems
 Natasha 2: Faster Non-Convex Optimization Than SGD,2018, In NeurIPS
 How To Make the Gradients Small Stochastically,2018, In NeurIPS
 Feature purification: How adversarial training performs robust deeplearning,2020, arXiv preprint arXiv:2005
 A little is enough: Circumventing defenses for distributedlearning,2019, In Advances in Neural Information Processing Systems
 Machine learning withadversaries: Byzantine tolerant gradient descent,2017, In NIPS
 Practical byzantine fault tolerance,1999, In OSDI
 Distributed statistical machine learning in adversarial settings:Byzantine gradient descent,2017, Proceedings of the ACM on Measurement and Analysis of Computing Systems
 Spider: Near-optimal non-convex optimizationvia stochastic path-integrated differential estimator,2018, In Advances in Neural Information Processing Systems
 Distributed robust learning,2014, arXiv preprint arXiv:1409
 Escaping from saddle pointsâ€”online stochastic gradientfor tensor decomposition,2015, In Proceedings of the 28th Annual Conference on Learning Theory
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 The byzantine generals problem,1982, ACM Transactions onProgramming Languages and Systems (TOPLAS)
 Nonconvex Finite-Sum Optimization Via SCSGMethods,2017, In NIPS
 Distributed algorithms,1996, Elsevier
 Towards deeplearning models resistant to adversarial attacks,2018, In ICLR
 Optimum bounds for the distributions of martingales in banach spaces,1994, The Annals of Probability
 Defending non-bayesian learning against adversarial attacks,2016, ISDC
 Securing distributed machine learning in high dimensions,2018, arXiv preprintarXiv:1804
 Generalized Byzantine-tolerant SGD,2018, arXiv preprintarXiv:1802
 Fall of empires: Breaking byzantine-tolerant SGD byinner product manipulation,2020, In Uncertainty in Artificial Intelligence
 Byzantine-resilient stochastic gradient descentfor distributed learning: A lipschitz-inspired coordinate-wise median approach,2019, arXiv preprintarXiv:1909
 Byzantine-robust distributed learning:Towards optimal statistical rates,2018, arXiv preprint arXiv:1803
 Defending against saddle point attack inbyzantine-robust distributed learning,2019, In International Conference on Machine Learning
