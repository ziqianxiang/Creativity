title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Proceedings of ICML
 A conditional gradient method with linear rate of convergence forsolving convex linear systems,2004, Mathematical Methods of Operations Research
 A fast iterative shrinkage-thresholding algorithm for linear inverseproblems,2009, SIAM J
 Convex optimization: Algorithms and complexity,2015, Foundation and Trends inMachine Learning
 Generalized gradients of lipschitz functionals,1981, Advances in Mathematics
 SAGA: A fast incremental gradientmethod with support for non-strongly convex composite objectives,2014, In Proceedings of NeurIPS
 Efficiency of minimizing compositions of convexfunctions and smooth maps,2019, Mathematical Programming
 Deep sparse rectifier neural networks,2011, InProceedings of AISTATS
 Deep Learning,2016, The MIT fPress
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Proceedings of NeurIPS
 Acceleratingstochastic gradient descent for least squares regression,2018, In Proceedings of COLT
 Gradient-based learning applied todocument recognition,1998, In Proceedings of the IEEE
 Fastand furious convergence: Stochastic second order methods under interpolation,2020, In Proceedings ofAISTATS
 Primal-dual subgradient methods for convex problems,2009, Mathematical programming
 Smooth minimization of non-smooth functions,2005, Mathematical Programming
 Proximal algorithms,2014, Foundation and Trends in Optimization
 Pegasos: Primal estimated sub-gradientsolver for svm,2007, In Proceedings of ICML
 Stochastic gradient descent for non-smooth optimization: convergenceresults and optimal averaging schemes,2013, In Proceedings of ICML
 Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron,2019, In Proceedings ofAISTATS
 On complexity of finding stationarypoints of nonsmooth nonconvex functions,2020, In Proceedings of ICML
