title,year,conference
 Deep rewiring: Trainingvery sparse deep networks,2017, arXiv preprint arXiv:1711
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Flownet: Learning optical flow withconvolutional networks,2015, In Proceedings of the IEEE international conference on computer vision
 Rigging the lottery:Making all tickets winners,2019, arXiv preprint arXiv:1911
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Dynamic network surgery for efficient dnns,2016, InAdvances in neural information processing systems
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Adam: A method for stochastic optimization,2015, In Proceedingsof the International Conference on Learning Representations (ICLR)
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Scalable training of artificial neural netWorks With adaptive sparse connec-tivity inspired by netWork science,2018, Nature communications
 Parameter efficient training of deep convolutional neural netWorksby dynamic sparse reparameterization,2019, arXiv preprint arXiv:1902
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 Xnor-net: Imagenetclassification using binary convolutional neural netWorks,2016, In European conference on computervision
 Faster r-cnn: ToWards real-time objectdetection With region proposal netWorks,2015, In Advances in neural information processing systems
 Comparing reWinding and fine-tuning in neuralnetWork pruning,2020, arXiv preprint arXiv:2003
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (ACL) 
 Pcnn: Pattern-based fine-grained regular pruningtowards optimizing cnn accelerators,2020, arXiv preprint arXiv:2002
 Raft: Recurrent all-pairs field transforms for optical flow,2020, arXiv preprintarXiv:2003
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Learning structured sparsity indeep neural networks,2016, In Advances in neural information processing systems
 Under-standing straight-through estimator in training activation quantized neural nets,2019, arXiv preprintarXiv:1903
