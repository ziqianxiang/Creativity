title,year,conference
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, In InternationalConference on Machine Learning
 Layer normalization,2016, arXiv preprintarXiv:1607
 Rezero is all you need: Fast convergence at large depth,2020, arXiv preprintarXiv:2003
 Understanding batch normal-ization,2018, In Advances in Neural Information Processing Systems
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE conference on computervision and pattern recognition
 Batch normalization biases residual blocks towards the identity functionin deep networks,2020, Advances in Neural Information Processing Systems
 Im-proved training of Wasserstein GANs,2017, In Advances in neural information processing systems
 Array program-ming with numpy,1476, Nature
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Deep residual learning for image recog-nition,2016, In CVPR
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Self-normalizingneural networks,2017, In Advances in neural information processing systems
 Large scale learning of general visual representations for transfer,2019, arXivpreprint arXiv:1912
 Fully convolutional networks for semanticsegmentation,2015, In Proceedings of the IEEE conference on computer vision and pattern recognition
 SGDR: stochastic gradient descent with warm restarts,2017, In 5thInternational Conference on Learning Representations
 Towards understanding regularization inbatch normalization,2019, In 7th International Conference on Learning Representations
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 All you need is a good init,2016, In 4th International Conference onLearning Representations
 Spectral normalizationfor generative adversarial networks,2018, In ICLR
 A robust hybrid of lasso and ridge regression,2007, 2007
 Deep isometric learning forvisual recognition,2020, In International Conference on Machine Learning
 Weight standardization,2019, arXivpreprint arXiv:1903
 Mean shift rejection: Training deep neural networkswithout minibatch statistics or normalization,2019, arXiv preprint arXiv:1911
 ImageNet large scale visualrecognition challenge,2015, iJcv
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing systems
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the iEEE conference oncomputer vision and pattern recognition
 On the generalization benefit of noise in stochasticgradient descent,2020, In international Conference on Machine Learning
 Highway networks,2015, arXivpreprintarXiv:1505
 Rethinking the incePtion architecture forcomPuter vision,2016, In 2016 iEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In international Conference on Machine Learning
 Fixing the train-test resolutiondiscrePancy,2019, In Advances in Neural information Processing Systems
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 Circumventingoutliers of autoaugment with knowledge distillation,2020, In ECCV
 GrouP normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionandpattern recognition
 Mobiledets: Searching for object detection architec-tures for mobile accelerators,2020, arXiv preprint arXiv:2004
 Mean field residual netWorks: On the edge of chaos,2017, In Advancesin neural information processing systems
 Wide residual netWorks,2016, In Proceedings of the BritishMachine Vision Conference 2016
 mixup: Beyond em-pirical risk minimization,2018, In 6th International Conference on Learning Representations
 Fixup initialization: Residual learning Withoutnormalization,2019, In 7th International Conference on Learning Representations
