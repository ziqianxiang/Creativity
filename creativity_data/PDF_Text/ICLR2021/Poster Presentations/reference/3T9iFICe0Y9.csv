title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in Neural Information Processing Systems
 Unitary evolution recurrent neural networks,2016, InInternational Conference on Machine Learning
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 An iterative construction of solutions of the tap equations for the sherrington-kirkpatrick model,2014, Communications in Mathematical Physics
 LIBSVM: A library for support vector machines,2011, ACMTransactions on Intelligent Systems and Technology
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Kernel methods for deep learning,2009, In Advances in NeuralInformation Processing Systems
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, In Advancesin Neural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Avoiding pathologies in verydeep networks,2014, In Artificial Intelligence and Statistics
 Finding structure in time,1990, Cognitive Science
 Anextensive experimental survey of regression methods,2019, Neural Networks
 Deep convolutionalnetworks as shallow gaussian processes,2019, In International Conference on Learning Representations
 Recurrent orthogonal networks and long-memorytasks,2016, In International Conference on Machine Learning
 Long short-term memory,1997, Neural Computation
 Simple and effective regularization methods for training onnoisily labeled data with generalization guarantee,2020, In International Conference on LearningRepresentations
 Why do deep residual networks generalizebetter than deep feedforward networks?-a neural tangent kernel perspective,2020, arXiv preprintarXiv:2002
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 Finite versus infinite neural networks: an empirical study,2020, arXivpreprint arXiv:2007
 Bayesian Learning for Neural Networks,1995, PhD thesis
 The role ofover-parametrization in generalization of neural networks,2019, In International Conference on LearningRepresentations
 Bayesian deep convolutional networks with many channelsare gaussian processes,2019, In International Conference on Learning Representations
 A max-affine spline perspective ofrecurrent neural networks,2018, In International Conference on Learning Representations
 Tensor programs I: Wide feedforward or recurrent neural networks of any architectureare gaussian processes,2019, arXiv preprint arXiv:1910
 Tensor programs II: Neural tangent kernel for any architecture,2020, arXiv preprintarXiv:2006
 Tensor programs III: Neural matrix laws,2020, arXiv preprint arXiv:2009
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
