title,year,conference
 Better fine-tuning by reducing representational collapse,2020, ArXiv
 Learning representations by maximizingmutual information across views,2019, In NeurIPS
 A large annotatedcorpus for learning natural language inference,2015, In EMNLP
 Learning imbalanced datasetswith label-distribution-aware margin loss,2019, In NeurIPS
 A simple framework forcontrastive learning of visUal representations,2020, In ICML
 Bigself-sUpervised models are strong semi-sUpervised learners,2020, In NeurIPS
 Understanding back-translation atscale,2018, In EMNLP
 Largemargin deep networks for classification,2018, In NeurIPS
 Cert: Contrastive self-supervised learning for language understand-ing,2020, ArXiv
 Data-efficientimage recognition with contrastive predictive coding,2019, ArXiv
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, In ICLR
 Teaching machines to read and comprehend,2015, In NeurIPS
 Distilling the knowledge in a neural network,2015, InNeurIPS Deep Learning and Representation Learning Workshop
 Learning deeP rePresentations by mutual information estimation and maximization,2019, InICLR
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2020, In ACL
 Supervised contrastive learning,2020, In NeurIPS
 Learning multiple layers of features from tiny images,2009, 2009
 Hybrid discriminative-generative training via contrastive learning,2020, ArXiv
 Large-margin softmax loss for convolu-tional neural networks,2016, In ICML
 Learning word embeddings efficiently with noise-contrastive estima-tion,2013, In NeurIPS
 Cross-entropy loss and low-rank features haveresponsibility for adversarial examples,2019, ArXiv
 On discriminative vs,2001, generative classifiers: A comparison oflogistic regression and naive bayes
 Adversarialnli: A new benchmark for natural language understanding,2020, 2020
 Representation learning with contrastive predictive coding,2018, ArXiv
 Language modelsare unsupervised multitask learners,2019, 2019
 Atheoretical analysis of contrastive unsupervised representation learning,2019, volume 97 of Proceedingsof Machine Learning Research
 Facenet: A unified embedding for face recognitionand clustering,2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Improved deep metric learning with multi-class n-pair loss objective,2016, In NeurIPS
 Trainingconvolutional networks with noisy labels,2015, In ICLR
 Rethinking the inception archi-tecture for computer vision,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Contrastive multiview coding,2020, In ECCV
 Unsupervised dataaugmentation for consistency training,2019, arXiv: Learning
 Billion-scale Semi-supervised learning for image classification,2019, arXiv preprint arXiv:1905
 Wide residual networks,2016, ArXiv
 mixup: Beyond empirical riskminimization,2018, In ICLR
 Revisiting few-samplebert fine-tuning,2020, ArXiv
 Character-level convolutional networks for text classification,2015, InNeurIPS
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
