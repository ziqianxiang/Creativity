title,year,conference
 Learning implicit fields for generative shape modeling,2019, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Learning shape templates with structured implicit functions,2019, In Proceedings ofthe IEEE International Conference on Computer Vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Fastfood-approximating kernel expansions in Ioglineartime,2013, In Proceedings of the international conference on machine learning
 Deep hybrid neUral-kernel networks Using randomfoUrier featUres,2018, Neurocomputing
 Nerf: Representing scenes as neUral radiance fields for view synthesis,2020, arXiv preprintarXiv:2003
 Deep neUral networks are easily fooled: High confi-dence predictions for Unrecognizable images,2015, In Proceedings of the IEEE conference on computervision and pattern recognition
 Random featUres for large-scale kernel machines,2008, In Advances inneural information processing systems
 Weighted sUms of random kitchen sinks: Replacing minimizationwith randomization in learning,2009, In Advances in neural information processing systems
 Random featUres forsparse signal classification,2016, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Implicit neUral representations with periodic activation fUnctions,2020, arXiv preprintarXiv:2006
 But how does it work in theory? linear svm withrandom features,2018, In Advances in Neural Information Processing Systems
 Foundations ofsignalprocessing,2014, CambridgeUniversity Press
 Deep speCtral kernel learning,2019, In Proceedings of the28th International Joint Conference on Artificial Intelligence
 Orthogonal random features,2016, In Advances in Neural Information ProcessingSystems
 ReConstruCting Continuousdistributions of 3d protein struCture from Cryo-em images,2020, In ICLR
