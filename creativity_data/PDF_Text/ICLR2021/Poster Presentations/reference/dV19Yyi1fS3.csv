title,year,conference
 Classy vision,2019, 2019
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Quasi-recurrent neuralnetworks,2016, arXiv preprint arXiv:1611
 Adabert: Task-adaptive bert compression with differentiableneural architecture search,2020, arXiv preprint arXiv:2001
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Language modeling with gatedconvolutional networks,2017, In Proc
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Compressing deep convolutionalnetworks using vector quantization,2014, arXiv preprint arXiv:1412
 Efficientsoftmax approximation for gpus,2016, arXiv
 Deep learning withlimited numerical precision,2015, In ICML
 Deep residual learning for imagerecognition,2015, CoRR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Deep networks withstochastic depth,2016, In ECCV
 Condensenet: Anefficient densenet using learned group convolutions,2018, In CVPR
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
zip: Compressing text classification models,2016, arXiv preprint arXiv:1612
 Quantizing deep convolutional netWorks for efficient inference: AWhitepaper,2018, arXiv preprint arXiv:1806
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Optimal brain damage,1990, In NIPS
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In ICCV
 Shufflenet V2: practical guidelines forefficient CNN architecture design,2018, CoRR
 Atensorized transformer for language modeling,2019, arXiv preprint arXiv:1906
 Pointer Sentinel MixtureModels,2016, arXiv
 Recovering fromrandom pruning: On the plasticity of deep convolutional neural networks,2018, In WACV
 Variational dropout sparsifies deep neuralnetworks,2017, In ICML
 How to construct deeprecurrent neural networks,2014, In Proceedings of the Second International Conference on LearningRepresentations (ICLR 2014)
 Automatic differentiation inpytorch,2017, 2017
 Languagemodels are unsupervised multitask learners,2019, 2019
 Compressivetransformers for long-range sequence modelling,2019, arXiv preprint arXiv:1911
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Conference on Computer Vision and PatternRecognition
 Highway networks,2015, arXiv preprintarXiv:1505
 And the bit goesdown: Revisiting the quantization of neural networks,2019, CoRR
 AdaPtive attentionsPan in transformers,2019, arXiv preprint arXiv:1905
 Aug-menting self-attention with Persistent memory,2019, arXiv preprint arXiv:1907
 Patient knowledge distillation for bert modelcomPression,2019, EMNLP
 On the imPortance of initializationand momentum in deep learning,2013, In International conference on machine learning
 Well-read students learn better:The impact of student initialization on knowledge distillation,2019, arXiv preprint arXiv:1908
 Improving the speed of neural networks oncpus,2011, 2011
 Attention is all you need,2017, In NIPS
 Regularization of neuralnetworks using DropConnect,2013, In ICML
 HAQ: hardware-aware automatedquantization,2018, CoRR
 Accelerating neural transformer via an average attentionnetwork,2018, arXiv preprint arXiv:1805
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2017, CoRR
 Extreme language model compressionwith optimal subwords and shared projections,2019, arXiv preprint arXiv:1909
