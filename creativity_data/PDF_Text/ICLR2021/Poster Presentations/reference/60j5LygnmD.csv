title,year,conference
 High-dimensional dynamics of generalization errorin neural networks,1710, arXiv:1710
 Meta-Iearning withdifferentiable closed-form solvers,1805, arXiv:1805
 ACloser Look at Few-shot Classification,1904, arXiv:1904
 A New Meta-Baselinefor Few-Shot Learning,2003, arXiv:2003
 On Lazy Training in Differentiable Program-ming,2020, arXiv:1812
 Why Does MAML Outperform ERM? AnOptimization Perspective,2010, arXiv:2010
 A Baselinefor Few-Shot Image Classification,1909, arXiv:1909
 DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,2014, ICML
 Meta-Learning and Universality: Deep Representations and Gra-dient Descent can Approximate any Learning Algorithm,2018, arXiv:1710
 Model-Agnostic Meta-Learning for Fast Adap-tation of Deep Networks,1703, arXiv:1703
 RECASTINGGRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES,2018, ICLR
 Surprises in High-Dimensional Ridgeless Least Squares Interpolation,1903, arXiv:1903
 Meta-Learning in NeuralNetworks: A Survey,2004, arXiv:2004
 Neural Tangent Kernel: Convergence andGeneralization in Neural Networks,1806, arXiv:1806
 Multi-Step Model-Agnostic Meta-Learning: Convergenceand Improved Algorithms,2002, arXiv:2002
 Scaling Laws for Neural LanguageModels,2020, arXiv:2001
 Adaptive Gradient-Based Meta-Learning Methods,1906, arXiv:1906
 Meta-learning formixed linear regression,2020, arXiv:2002
 Deep learning,1476, Nature
 Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradi-ent Descent,1902, arXiv:1902
 Meta-SGD: Learning to Learn Quickly forFew-Shot Learning,1707, arXiv:1707
 A Survey on Transfer Learning,1041, IEEE Transactions on KnowledgeandDataEngineering
 Rapid Learning or FeatureReuse? Towards Understanding the Effectiveness of MAML,1909, arXiv:1909
 A CONSTRUCTIVEPREDICTION OF THE GENERALIZATION ERROR ACROSS SCALES,2020, ICLR
 Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,1903, arXiv:1903
 Provable Meta-Learning of Linear Representa-tions,2020, arXiv:2002
 Match-ing Networks for One Shot Learning,1606, arXiv:1606
 Global Convergence and Generalization Bound ofGradient-Based Meta-Learning with Deep Neural Nets,2006, arXiv:2006
 On the Global Optimality of Model-Agnostic Meta-Learning,2020, arXiv:2006
