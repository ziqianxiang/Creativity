title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Post training 4-bit quantization of convolutionalnetworks for rapid-deployment,2019, In Advances in Neural Information Processing Systems
 Practical gauss-newton optimisation for deeplearning,2017, arXiv preprint arXiv:1706
 Pact: Parameterized clipping activation for quantized neuralnetworks,2018, arXiv preprint arXiv:1805
 Learning to prune deep neural networks via layer-wiseoptimal brain surgeon,2017, In Advances in Neural Information Processing Systems
 Hawq: Hes-sian aware quantization of neural networks with mixed-precision,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Learned step size quantization,2020, In International Conference on LearningRepresentations
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in neural information processing systems
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Improving posttraining neural quantization: Layer-wise calibration and integer programming,2020, arXiv preprintarXiv:2006
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Deep learning via hessian-free optimization,2010, 2010
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Up ordown? adaptive rounding for post-training quantization,2020, arXiv preprint arXiv:2004
 Loss aware post-training quantization,2019, arXiv preprintarXiv:1911
 Model compression via distillation and quanti-zation,2018, arXiv preprint arXiv:1802
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In Advances in neural information processing systems
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Faster gaze prediction withdense networks and fisher pruning,2018, arXiv preprint arXiv:1801
 Haq: Hardware-aware automated quan-tization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Towards accurate post-training networkquantization via bit-split and stitching,2020, In Proc
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
4	Implementation DetailsThe ImageNet dataset consists of 1,2020,2M training images and 50K test images
