title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In CVPR
 XNOR-Net++: Improved binary neural networks,2019, InBMVC
 A simple framework forcontrastive learning of visual representations,2020, ICML
 On the efficacy of knowledge distillation,2019, In ICCV
 An analysis of single-layer networks in unsupervisedfeature learning,2011, In International conference on artificial intelligence and statistics
 Differentiable feature aggregation search for knowledge distillation,2020, In ECCV
 Deep residual learning for image recog-nition,2016, In CVPR
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Acomprehensive overhaul of feature distillation,2019, In ICCV
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In AAAI
 MobileNets: Efficient convolutional neural networks formobile vision applications,2017, arXiv:1704
 Arbitrary style transfer in real-time with adaptive instance normal-ization,2017, In ICCV
 Like what you like: Knowledge distill via neuron selectivitytransfer,2017, arXiv:1707
 QUEST:Quantized embedding space for transferring knowledge,2020, In ECCV
 Decoupling representation and classifier for long-tailed recognition,2020, In ICLR
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In NeurIPS
 Fast convnets using group-wise brain damage,2016, In CVPR
 Self-supervised knowledge distillationusing singular value decomposition,2018, In ECCV
 Block-wisely supervised neural architecture search with knowledge distillation,2020, In CVPR
 Local correlationconsistency for knowledge distillation,2020, In ECCV
 Relational knowledge distillation,2019, In CVPR
 Heterogeneous knowledge distillation usinginformation flow modeling,2020, In CVPR
 Automatic differentiation inpytorch,2017, 2017
 Correlation congruence for knowledge distillation,2019, In ICCV
 XNOR-Net: ImageNetclassification using binary convolutional neural networks,2016, In ECCV
 Fitnets: Hints for thin deep nets,2015, ICLR
 Mo-bileNetV2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, TIT
 Contrastive representation distillation,2020, In ICLR
 Similarity-preserving knowledge distillation,2019, In ICCV
 Quantized convolutionalneural networks for mobile devices,2016, In CVPR
 Look at boundary: Aboundary-aware face alignment algorithm,2018, In CVPR
 Wide residual networks,2016, In BMVC
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2017, In ICLR
 Neural architecture search with reinforcement learning,2017, ICLR
