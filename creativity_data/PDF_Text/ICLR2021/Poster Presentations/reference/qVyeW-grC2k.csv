title,year,conference
 Quantifying attention flow in transformers,2020, In Proceedings ofthe 58th Annual Meeting of the Association for Computational Linguistics
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Masked language modeling for pro-teins via linearly scalable long-context transformers,2020, arXiv preprint arXiv:2006
 Transformer-XL: Attentive Language Models Beyond a Fixed-LengthContext,2019, In Proceedings of ACL 2019
 Frustratingly Short AttentionSpans in Neural Language Modeling,2017, In Proceedings of ICLR 2017
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A deep relevance matching model forad-hoc retrieval,2016, In Proceedings of the 25th ACM International on Conference on Informationand Knowledge Management
 REALM: Retrieval-Augmented Lan-guage Model Pre-Training,2020, In Proceedings of ICML 2020
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 The hardware lottery,2020, arXiv preprint arXiv:2009
 Parallel and serial grouping of image elements in visual percep-tion,2010, J Exp Psychol Hum Percept Perform
 Universal Language Model Fine-tuning for Text Classifica-tion,2018, In Proceedings of ACL 2018
 TriviaQA: ALarge Scale Distantly Supervised Challenge Dataset for Reading Comprehension,2017, In Proceedingsof ACL 2017
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, arXiv preprint arXiv:2006
 Disentangling neural mecha-nisms for perceptual grouping,2020, In International Conference on Learning Representations
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 The NarrativeQA Reading Comprehension Challenge,2018, Transactions ofthe Association for Computational Linguistics
 Learning multiple layers of features from tiny images,2009, Technical report
 Learninglong-range spatial dependencies with horizontal gated recurrent units,2018, In Advances in neuralinformation processing systems
 Generating wikipedia by summarizing long sequences,2018, arXiv preprintarXiv:1801
 Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks,2019, In Advances in Neural Information ProcessingSystems
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Listops: A diagnostic dataset for latent tree learning,2018, arXivpreprint arXiv:1804
 Image transformer,2018, arXiv preprint arXiv:1802
 LanguageModels are Unsupervised Multitask Learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Biological structure and function emerge from scaling unsupervised learningto 250 million protein sequences,2019, bioRxiv
 Efficient content-based sparseattention with routing transformers,2020, arXiv preprint arXiv:2003
 Transfer learningin natural language processing,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Tutorials
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of EMNLP 2013
 LXMERT: Learning Cross-Modality Encoder Representations fromTransformers,2019, In Proceedings of EMNLP 2019
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Constructing Datasets for Multi-hop Read-ing Comprehension Across Documents,2018, In Transactions of the Association for ComputationalLinguistics
 Beyond 512 tokens:Siamese multi-depth transformer-based hierarchical encoder for document matching,2020, CoRR
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 The diverse suite ofTransformers come with a plethora of hardware constraints and implementation details,2020, To succeed
