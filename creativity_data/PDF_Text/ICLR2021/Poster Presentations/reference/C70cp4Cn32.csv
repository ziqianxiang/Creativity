title,year,conference
 Hierarchical federated learning across heteroge-neous cellular networks,2020, In ICASSP 2020
 Backpropagation and stochastic gradient descent method,1993, NeurocomPuting
 Optimization methods for large-scale machine learning,2018, SiamReview
 Solving the stragglerproblem with bounded staleness,2013, In 14th WorkshoP on Hot ToPics in OPerating Systems
 Deep residual learning for image recognition,2016, In CVPR 2016
 Moreeffective distributed ml via a stale synchronous parallel parameter server,2013, In Advances in neuralinformation Processing systems
 A novel stochastic gradient descentalgorithm based on grouping over heterogeneous cluster systems for distributed deep learning,2019, InCCGRID 2019
 A unified theory of decentralized sgdwith changing topology and local updates,2020, arXiv PrePrint arXiv:2003
 Learning multiple layers of features from tiny images,2009, 2009
 Asynchronous decentralized parallel stochastic gradientdescent,2017, arXiv PrePrint arXiv:1710
 Client-edge-cloud hierarchical federated learning,2020, In ICC2020
 Communication-efficientlearning of deep networks from decentralized data,2017, In Proc
 Sparknet: Training deep networks in spark,2016, 4thInternational Conference on Learning Representations
 The emergence of edge computing,2017, Computer
 Distributed asynchronous deterministic and stochasticgradient optimization algorithms,1986, IEEE transactions on automatic control
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Matcha: Speeding up decentralized sgd viamatching decomposition sampling,2019, arXiv preprint arXiv:1905
 A distributed hierarchical sgd algorithm with sparse global reduction,2019, arXivpreprint arXiv:1903
 Parallelized stochastic gradient descent,2010, Advancesin Neural Information Processing Systems
