title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In Proc
 In Proc,2017, of WMT
 An empirical study ofgeneration order for machine translation,2020, In Proc
 Recurrent stacking of layers for compact neural machine translationmodels,2019, In Proc
 Universal trans-formers,2019, In Proc
 Convolutionalsequence to sequence learning,2017, In Proc
 Mask-predict: Paralleldecoding of conditional masked language models,2019, In Proc
 Aligned crossentropy for non-autoregressive machine translation,2020, In Proc
 Non-autoregressive neural machine translation,2018, In Proc
 Insertion-based decoding with automatically inferred gen-eration order,2019, TACL
 Levenshtein transformer,2019, In Proc
 Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation,2020, In Proc
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In Proc
 Fast decoding in sequence models using discrete latent variables,2018, In Proc
 Recurrent continuous translation models,2013, In Proc
 Non-autoregressive machinetranslation with disentangled context transformer,2020, In Proc
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In Proc
 Sequence-level knowledge distillation,2016, In Proc
 From research to production and back: Ludicrouslyfast neural machine translation,2019, In Proc
 Deterministic non-autoregressive neuralsequence modeling by iterative refinement,2018, In Proc
 Hint-based training fornon-autoregressive machine translation,2019, In Proc
 In Proc,2018, of EMNLP
 Multilingual denoising pre-training for neural machine translation,2020, TACL
 FlowSeq: Non-autoregressive conditional sequence generation with generative flow,2019, In Proc
 Mixedprecision training,2018, In Proc
 BLEU: a method for automaticevaluation of machine translation,2002, In Proc
 In Proc,2021, of ICLR
 A call for clarity in reporting BLEU scores,2018, In Proc
 Using the output embedding to improve language models,2017, In Proc
 Guiding non-autoregressive neural machine translationdecoding with reordering information,2021, In Proc
 Non-autoregressivemachine translation with latent alignments,2020, In Proc
 Neural machine translation of rare words withsubword units,2016, In Proc
 Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation,2020, In Proc
 Speeding up neural machine translation decoding by shrinkingrun-time vocabulary,2017, In Proc
 Blockwise parallel decoding for deep au-toregressive models,2018, In Proc
 Insertion transformer:flexible sequence generation via insertion operations,2019, In Proc
 Fast structureddecoding for sequence models,2019, In Proc
 ENGINE: Energy-based in-ference networks for non-autoregressive machine translation,2020, In Proc
 Attention is all you need,2017, In Proc
 In Proc,2019, of ACL
 Non-autoregressivemachine translation with auxiliary regularization,2019, In Proc
 Accelerating neural transformer via an average attentionnetwork,2018, In Proc
 Understanding knowledge distillation in non-autoregressive machine translation,2020, In Proc
 Improving non-autoregressive neural machine translation withmonolingual data,2020, In Proc
