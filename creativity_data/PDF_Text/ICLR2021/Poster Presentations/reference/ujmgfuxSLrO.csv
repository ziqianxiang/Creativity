title,year,conference
 Attention is all you need,2017, In Advances in neural information processing systems
 Exploring the limits of transfer learning with a unified text-to-text transformer,2019, arXivpreprint arXiv:1910
 Language modelsare few-shot learners,2020, arXiv preprint arXiv:2005
 BERT: Pre-training of deep bidirectionaltransformers for language understanding,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Improvingneural networks by preventing co-adaptation of feature detectors,2012, arXiv preprint arXiv:1207
 Regularization of neural networks usingdropconnect,2013, In International conference on machine learning
 Regularizing and optimizing LSTM languagemodels,2018, In International Conference on Learning Representations
 Pyramidal recurrent unitfor language modeling,2018, In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing
 Shufflenet: An extremely efficient convolutionalneural network for mobile devices,2018, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Transformer-xl:Attentive language models beyond a fixed-length context,2019, In Association for Computational Linguistics
 Reformer: The efficient transformer,2020, In InternationalConference on Learning Representations
 An analysis of encoder representations in transformer-based machinetranslation,2018, In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP
 Fixed encoder self-attention patterns in transformer-based machine translation,2020, arXiv preprint arXiv:2002
 Synthesizer: Rethinkingself-attention in transformer models,2020, arXiv preprint arXiv:2005
 The evolved transformer,2019, In Proceedings of the 36th InternationalCOnference on Machine Learning 
 Albert:A lite bert for self-supervised learning of language representations,2020, In InternatiOnal COnference On LearningRepresentatiOns
 Efficientnet: Rethinking model scaling for convolutional neural networks,2019, InKamalika Chaudhuri and Ruslan Salakhutdinov
 Learning deeptransformer models for machine translation,2019, In PrOceedings Of the 57th Annual Meeting Of the AssOciatiOn fOrCOmputatiOnal Linguistics
 Deep residual learning for image recognition,2016, InPrOceedings Of the IEEE cOnference On cOmputer visiOn and pattern recOgnitiOn
 Neural machine translation of rare words with subwordunits,2016, In PrOceedings Of the 54th Annual Meeting Of the AssOciatiOn fOr COmputatiOnal Linguistics (VOlume 1:LOng Papers)
 Adaptive input representations for neural language modeling,2019, In InternatiOnalCOnference On Learning RepresentatiOns
 Efficient softmaxapproximation for GPUs,2017, In InternatiOnal COnference On Machine Learning
 DeFINE: DeepFactorized Input Token Embeddings for Neural Sequence Modeling,2020, In InternatiOnal COnference On LearningRepresentatiOns
 Groupreduce: Block-wise low-rank ap-proximation for neural language model shrinking,2018, In Advances in Neural InfOrmatiOn PrOcessing Systems
 Distilling the knowledge in a neural network,2015, In NIPS DeepLearning and Representation Learning Workshop
 Benefits of depth in neural networks,2016, COLT
 Classical structuredprediction losses for sequence to sequence learning,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Bleu: a method for automatic evaluation ofmachine translation,2002, In Proceedings of the 40th annual meeting on association for computational linguistics
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations
 Latent alignment and variationalattention,2018, In Advances in Neural Information Processing Systems
 Pointer sentinel mixture models,2017, InInternational Conference on Learning Representations
 Improving neural language models with a continuouscache,2017, In International Conference on Learning Representations
 An analysis of neural language modeling at multiplescales,2018, arXiv preprint arXiv:1803
