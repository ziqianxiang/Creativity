title,year,conference
 Towards feder-ated learning at scale: System design,2019, In A
 LEAF: A benchmark for federated settings,2018, arXiv preprint arXiv:1812
 EMNIST: Extending MNISTto handwritten letters,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 On the ineffectiveness of variance reduced optimization for deeplearning,2018, arXiv preprint arXiv:1812
 SAGA: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In NIPS
 The non-IID data quagmire ofdecentralized machine learning,2019, arXiv preprint arXiv:1910
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 SCAFFOLD: Stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 First analysis of local GD on heteroge-neous data,2019, arXiv preprint arXiv:1909
 Adam: A method for stochastic optimization,2015, In 3rd InternationalConference on Learning Representations
 Pachinko allocation: DAG-structured mixture models of topiccorrelations,2006, In Proceedings of the 23rd international conference on Machine learning
 On the convergence ofFedAvg on non-IID data,2019, arXiv preprint arXiv:1907
 On the convergence of stochastic gradient descent with adaptivestepsizes,2018, arXiv preprint arXiv:1805
 Adaptive gradient methods with dynamicbound of learning rate,2019, In 7th International Conference on Learning Representations
 Adaptive bound optimization for online convexoptimization,2010, In COLT
 Adaptive bound optimization for online convexoptimization,2010, In COLT The 23rd Conference on Learning Theory
 Stochastic variancereduction for nonconvex optimization,2016, arXiv:1603
 On the convergence of ADAM and beyond,2019, arXivpreprint arXiv:1904
 The error-feedback framework: Better rates forSGD with delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Cooperative SGD: A unified framework for the design and analysis ofcommunication-efficient SGD algorithms,2018, arXiv preprint arXiv:1808
 Global convergence of adaptive gradient methods for anover-parameterized neural network,2019, arXiv preprint arXiv:1902
 Group normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Local AdaAlter: Communication-efficient stochastic gradient descent with adaptive learning rates,2019, arXiv preprint arXiv:1911
 Parallel restarted SGD with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Adaptive methodsfor nonconvex optimization,2018, In Advances in Neural Information Processing Systems
 Why ADAM beats SGD for attention models,2019, arXiv preprintarxiv:1912
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing systems
