title,year,conference
 On exactcomputation with an infinitely wide neural net,2019, In Advances in Neural Information Processing Systems
 Metainit: Initializing learning by learning to initialize,2019, In Advancesin Neural Information Processing Systems
 Graphneural tangent kernel: Fusing graph neural networks with graph kernels,2019, In Advances in Neural InformationProcessing Systems
 Dropout as a bayesian approximation: Representing model uncertainty indeep learning,2016, In international conference on machine learning
 Deep convolutional networks asshallow gaussian processes,2019, In International Conference on Learning Representations
 Variational bayesian multinomial probit regression with gaussian processpriors,2006, Neural Computation
 Googlevizier: A service for black-box optimization,2017, In Proceedings of the 23rd ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining
 Google vizier:A service for black-box optimization,2017, In Proceedings of the 23rd ACM SIGKDD international conference onknowledge discovery and data mining
 On calibration of modern neural networks,2017, InInternational Conference on Machine Learning
 Classification by pairwise coupling,1998, In Advances in neural informationprocessing systems
 Why relu networks yield high-confidencepredictions far away from the training data and how to mitigate the problem,2019, In The IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Benchmarking neural network robustness to common corruptions andperturbations,2019, In International Conference on Learning Representations
 Probabilistic backpropagation for scalable learning of bayesianneural networks,2015, In International Conference on Machine Learning
 Using deep belief nets to learn covariance kernels for gaussianprocesses,2008, In Advances in neural information processing systems
 Infinite width attention networks,2020, InInternational Conference on Machine Learning (ICML)
 Evaluation of neural architectures trained with square loss vs cross-entropy inclassification tasks,2020, arXiv preprint arXiv:2006
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, arXiv preprint arXiv:1502
 Neural tangent kernel: Convergence and generalization inneural networks,2018, In Advances in Neural Information Processing Systems
 Simple and scalable predictive uncertaintyestimation using deep ensembles,2017, In Advances in Neural Information Processing Systems
 Wide neural networks of any depth evolve as linear models under gradient descent,2019, InAdvances in neural information processing systems
 Finite versus infinite neural networks: an empirical study,2020, Advances in Neural InformationProcessing Systems
 The large learningrate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 The evidence framework applied to classification networks,1992, NEURAL COMPUTATION
 Mnist-c: A robustness benchmark for computer vision,2019, arXiv preprintarXiv:1906
 On the importance of strong baselines in bayesian deeplearning,2018, arXiv preprint arXiv:1811
 Elliptical slice sampling,2010, 2010
 Bayesian Learning for Neural Networks,1994, PhD thesis
 Neural tangents: Fast and easy infinite neural networks in python,2019, arXiv preprintarXiv:1912
 Bayesian deep convolutional networks with many channels aregaussian processes,2019, In International Conference on Learning Representations
 Probabilistic outputs for support vector machines and comparisons to regularized likelihoodmethods,1999, Advances in large margin classifiers
 Dataset Shift inMachine Learning,0262, The MIT Press
 Regularized least-squares classification,2003, Nato Science Series SubSeries III Computer and Systems Sciences
 Deep bayesian bandits showdown: An empirical comparisonof bayesian deep networks for thompson sampling,2018, In International Conference on Learning Representations
 Neural kernels without tangents,2020, ArXiv
 Efficientnet: Rethinking model scaling for convolutional neural networks,2019, arXivpreprint arXiv:1905
 Calibratingdeep convolutional gaussian processes,2018, arXiv preprint arXiv:1805
 Stochastic variational deep kernellearning,2016, In Advances in Neural Information Processing Systems
 The case for bayesian deep learning,2020, arXiv preprint arXiv:2001
 Deep kernel learning,2016, In ArtificialIntelligence and Statistics
 Mean field residual networks: On the edge of chaos,2017, In Advances in NeuralInformation Processing Systems
 Tensor programs i: Wide feedforward or recurrent neural networks of any architecture are gaussianprocesses,2019, arXiv preprint arXiv:1910
 A mean fieldtheory of batch normalization,2019, arXiv preprint arXiv:1902
 Wide residual networks,2016, In British Machine Vision Conference
 mixup: Beyond empirical riskminimization,2017, arXiv preprint arXiv:1710
