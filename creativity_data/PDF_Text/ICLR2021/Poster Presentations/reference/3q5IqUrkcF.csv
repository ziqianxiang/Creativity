title,year,conference
 A continuous-time view of early stopping for leastsquares regression,2019, In International Conference on Artificial Intelligence and Statistics
 The implicit regularization of stochastic gradientflow for least squares,2020, In International Conference on Machine Learning
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, In Proceedings of the National Academy ofSciences
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, In Advances in Neural Information Processing Systems
 On dissipative symplectic integration withapplications to gradient-based optimization,2020, arXiv:2004
 Implicit regularization of discrete gradientdynamics in linear neural networks,2019, In Advances in Neural Information Processing Systems
 Characterizing implicit bias interms of optimization geometry,2018, In International Conference on Machine Learning
 Flat minima,1997, Neural Computation
 The break-even Point on oPtimization trajectories of deeP neuralnetworks,2021, In International Conference on Learning Representations
 Universal statistics of fisher information in deePneural networks: Mean field aPProach,2019, In Proceedings of Machine Learning Research
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InInternational Conference on Learning Representations
 Wide neural networks of any dePth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 The largelearning rate Phase of deeP learning: the cataPult mechanism,2020, arXiv:2003
 Visualizing the loss landscaPeof neural nets,2018, In Advances in Neural Information Processing Systems
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Toward a theory of optimization for over-parameterizedsystems of non-linear equations: the lessons of deep learning,2020, arXiv:2003
 The numerics of gans,2017, In Advances inNeural Information Processing Systems
 On the importanceof single directions for generalization,2018, In International Conference on Learning Representations
 Stochastic gradient descent on separabledata: Exact convergence with a fixed learning rate,2019, In International Conference on ArtificialIntelligence and Statistics
 Gradient descent gan optimization is locally stable,2017, InAdvances in neural information processing systems
 Generalization in deep networks: The role of distance frominitialization,2018, In Advances in Neural Information Processing Systems
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In Conference on Learning Representations
 The effect of network width onstochastic gradient descent and generalization: an empirical study,2019, In International Conference onMachine Learning
 Training generative adversarial networks by solving ordinary differentialequations,2020, In Advances in Neural Information Processing Systems
 Implicit regularization in deep learning may not be explainable bynorms,2020, In Advances in Neural Information Processing Systems
 Sgd implicitly regularizes generalization error,2018, In NIPS 2018 Workshop
 Empirical analysis of thehessian of over-parametrized neural networks,2017, arXiv:1706
 Integration methods andoptimization algorithms,2017, In Advances in Neural Information Processing Systems
 Cyclical learning rates for training neural networks,2017, In Winter Conference onApplications of Computer Vision
 Connecting optimization and regularizationpaths,2018, In Advances in Neural Information Processing Systems
 Learning sparse neuralnetworks via sensitivity-driven regularization,2018, In International Conference on Neural InformationProcessing Systems
 Error analysis of floating-point computation,1960, Numerische Mathematik
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Kernel and rich regimes in overparametrized models,2020, InProceedings of Thirty Third Conference on Learning Theory
 Direct runge-kutta discretizationachieves acceleration,2018, In Advances in Neural Information Processing Systems
 A type of generalization error induced byinitialization in deep neural networks,2019, Proceedings of Machine Learning Research
 Gradient descent optimizes over-parameterized deep relu networks,2019, Machine Learning
