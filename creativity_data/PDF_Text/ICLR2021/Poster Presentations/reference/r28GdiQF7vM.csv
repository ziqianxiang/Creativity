title,year,conference
 Variance reduction for faster non-convex optimization,2016, InInternational Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 On exponential convergence of sgd in non-convexover-parametrized learning,2018, arXiv preprint arXiv:1811
 Private stochasticconvex optimization with optimal rates,2019, In Advances in Neural Information Processing Systems
 Stability of stochastic gradientdescent on nonsmooth convex losses,2020, In Advances in Neural Information Processing Systems
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Optimization methods for large-scale machinelearning,2018, SIAM Review
 The tradeoffs of large scale learning,2008, In Advances in NeuralInformation Processing Systems
 Sharper bounds for uniformly stablealgorithms,2020, In Conference on Learning Theory
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Stability and generalization of learning algorithmsthat converge to global optima,2018, In International Conference on Machine Learning
 Stability and convergence trade-off of iterative optimizationalgorithms,2018, arXiv preprint arXiv:1804
 Nonparametric stochastic approximation with large step-sizes,2016, Annals of Statistics
 Spider: Near-optimal non-convex op-timization via stochastic path-integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 High probability generalization bounds for uniformly stable algo-rithms with nearly optimal rate,2019, In Conference on Learning Theory
 Hypothesis set stability and generalization,2019, In Advances in Neural Information ProcessingSystems
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojasieWicz condition,2016, In European Conference on MachineLearning
 Fast rates for exp-concave empirical risk minimization,2015, In Advancesin Neural Information Processing Systems
 Data-dependent stability of stochastic gradient descent,2018, InInternational Conference on Machine Learning
 Deep learning,2015, Nature
 Non-convex finite-sum optimization viascsg methods,2017, In Advances in Neural Information Processing Systems 30
 Fine-grained analysis of stability and generalization for stochasticgradient descent,2020, In International Conference on Machine Learning
 Stochastic gradient descent for nonconvex learningWithout bounded gradient assumptions,2020, IEEE Transactions on Neural Networks and LearningSystems
 Sharper generalization bounds for pairWise learn-ing,2020, Advances in Neural Information Processing Systems
 On generalization error bounds of noisy gradient methodsfor non-convex learning,2020, In International Conference on Learning Representations
 Fast rates of ERM andstochastic approximation: Adaptive to error bound conditions,2018, In Advances in Neural InformationProcessing Systems
 Algorithmic stability and hypothesiscomplexity,2017, In International Conference on Machine Learning
 A PAC-bayesian analysis of randomized learning with application to stochastic gradi-ent descent,2017, In Advances in Neural Information Processing Systems
 A second-order look at stability and generalization,2017, In Conference on LearningTheory
 Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints,2018, In Conference on Learning Theory
 Beating Sgd saturation with tail-averaging andminibatching,2019, In Advances in Neural Information Processing Systems
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Stochastic gradient descent with exponential convergence ratesof expected classification errors,2019, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Simultaneous model selection and optimization through parameter-freestochastic learning,2014, In Advances in Neural Information Processing Systems
 Exponential convergence of testingerror for stochastic gradient methods,2018, In Conference On Learning Theory
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2019, IEEE Transactions on InformationTheory
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, In International Conference on ArtificialIntelligence and Statistics
 Information-theoretic analysis of generalization capability of learn-ing algorithms,2017, In Advances in Neural Information Processing Systems
 Online gradient descent learning algorithms,2008, Foundations ofComputational Mathematics
 Unregularized online learning algorithms with general lossfunctions,2017, Applied and Computational Harmonic Analysis
 Stagewise training accelerates convergenceof testing error over sgd,2019, In Advances in Neural Information Processing Systems
 Generalization error bounds with probabilistic guar-antee for SGD in nonconvex optimization,2018, arXiv preprint arXiv:1802
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
