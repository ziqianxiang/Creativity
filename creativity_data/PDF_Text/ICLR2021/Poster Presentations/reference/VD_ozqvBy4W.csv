title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Style transformer: Unpaired text styletransfer without disentangled latent representation,2019, arXiv preprint arXiv:1905
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Plug and play language models: a simple approach to controlled textgeneration,2019, arXiv preprint arXiv:1912
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Automatic evaluation of machine translation quality using n-gram co-occurrence statistics,2002, In Proceedings ofthe second international conference on Human LanguageTechnology Research
 Zero-shot question generation fromknowledge graphs for unseen predicates and entity types,2018, arXiv preprint arXiv:1802
 Hafez: an interactive poetrygeneration system,2017, In Proceedings ofACL 2017
 Learningto write with cooperative discriminators,2018, arXiv preprint arXiv:1805
 The curious case of neural textdegeneration,2019, arXiv preprint arXiv:1904
 Control-ling output length in neural encoder-decoders,2016, arXiv preprint arXiv:1609
 Meteor: An automatic metric for mt evaluation with high levels ofcorrelation with human judgments,2007, In Proceedings of the second workshop on statistical machinetranslation
 Foundations of statisticalnatural language processing,1999, MIT press
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 What makes a good conversation?how controllable attributes affect human judgments,2019, arXiv preprint arXiv:1902
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Style transfer from non-parallel textby cross-alignment,2017, In Advances in neural information processing systems
 Synthesizer:Rethinking self-attention in transformer models,2020, arXiv preprint arXiv:2005
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Huggingface’s trans-formers: State-of-the-art natural language processing,2019, ArXiv
 Unsupervisedtext style transfer using language models as discriminators,2018, In Advances in Neural InformationProcessing Systems
 Fine-tuning language models from human preferences,2019, arXivpreprint arXiv:1909
” Thafs nota new thing to see in these ads,2008, Just three years ago
