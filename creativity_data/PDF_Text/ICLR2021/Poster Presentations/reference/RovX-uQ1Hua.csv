title,year,conference
 Split and rephrase: Better evaluation and stronger baselines,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume2: Short Papers)
 Globally normalized transition-based neural networks,2016, InProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Structured prediction energy networks,2016, In Maria FlorinaBalcan and Kilian Q
 Scheduled sampling for sequenceprediction with recurrent neural networks,2015, In Advances in Neural Information Processing Systems
 Report onthe 11th IWSLT evaluation campaign,2014, In Proceedings of the International Workshop on SpokenLanguage Translation
 Learning phrase representations using RNN encoder-decoder forstatistical machine translation,2014, In Proceedings of the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Empirical analysis of beam search performance degradation inneural sequence models,2019, volume 97 of Proceedings of Machine Learning Research
 Search-based structured prediction,2009, Machine learning
 Adaptive importancesampling for value function approximation in off-policy reinforcement learning,2009, Neural Networks
 Unifying human and statistical evaluationfor natural language generation,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Teaching machines to read and comprehend,2015, In Advances in NeuralInformation Processing Systems
 Long short-term memory,0899, Neural Comput
 The curious case of neural textdegeneration,2020, In International Conference on Learning Representations
 Way off-policy batch deep reinforcement learning ofimplicit human preferences in dialog,2019, arXiv preprint arXiv:1907
 A good sample is hard to find: Noise injection samplingand self-training for neural language generation models,2019, In Proceedings of the 12th InternationalConference on Natural Language Generation
 Deep reinforcementlearning for sequence-to-sequence models,2019, IEEE Transactions on Neural Networks and LearningSystems
 SEARNN: TrainingRNNs With global-local losses,2018, In International Conference on Learning Representations
 Paraphrase generation With deep reinforcementlearning,2018, In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 CoT: Cooperative training forgenerative modeling of discrete data,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 ReWard augmented maximum likelihood for neural structuredprediction,2016, In D
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting of the Association forComputational Linguistics
 Distributional reinforcement learningfor energy-based sequential models,2019, arXiv preprint arXiv:1912
 ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training,2020, InFindings of the Association for Computational Linguistics: EMNLP 2020
 Sequence level trainingwith recurrent neural networks,2016, In International Conference on Learning Representations
 Get to the point: Summarization withpointer-generator networks,1073, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 A deepreinforcement learning chatbot,2017, arXiv preprint arXiv:1709
 Minimumrisk training for neural machine translation,2016, In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers)
 Revisiting precision recall definition for generativemodeling,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Policy gradient meth-ods for reinforcement learning with function approximation,2000, In Advances in neural informationprocessing systems
 Evaluating text GANs as languagemodels,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Consistencyof a recurrent language model with respect to incomplete decoding,2020, In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 A study of reinforcement learningfor neural machine translation,2018, In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2020, In Proceedings of the 37th International Conferenceon Machine Learning
 Neural questiongeneration from text: A preliminary study,2017, In National CCF Conference on Natural LanguageProcessing and Chinese Computing
 Fine-tuning language models from human preferences,2019, arXivpreprint arXiv:1909
