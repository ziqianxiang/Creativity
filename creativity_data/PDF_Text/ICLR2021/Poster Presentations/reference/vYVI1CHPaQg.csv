title,year,conference
 QSGD: Randomized quantization forcommunication-optimal stochastic gradient descent,2016, arXiv preprint arXiv:1610
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 On biased compressionfor distributed learning,2020, arXiv preprint arXiv:2002
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Convex optimization using sparsified stochastic gradient descent withmemory,2018, Technical report
 Semi-cyclicstochastic gradient descent,2019, arXiv preprint arXiv:1904
 Variance reduction with sparse gradients,2020, arXivpreprint arXiv:2001
 SGD: General analysis and improved rates,2019, Proceedings of the 36th InternationalConference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Nonconvex variance reduced optimization with arbitrarysampling,2019, Proceedings of the 36th International Conference on Machine Learning
 Natural compression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Gaia: Geo-distributed machine learning approaching LAN speeds,2017, In14th Symposium on Networked Systems Design and Implementation
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojaSieWiCZ condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Scaffold: Stochastic controlled averaging for on-device federatedlearning,2019, arXiv preprint arXiv:1910
 Error feedbackfixes signSGD and other gradient compression schemes,2019, arXiv preprint arXiv:1901
 Tighter theory for local SGD onidentical and heterogeneous data,2020, In The 23rd International Conference on Artificial Intelligenceand Statistics (AISTATS 2020)
 Distributed learning Withcompressed gradients,2018, arXiv preprint arXiv:1806
 Adam: A method for stochastic optimization,2015, Published as aconference paper at the 3rd International Conference for Learning Representations
 Decentralized stochastic optimizationand gossip algorithms With compressed communication,2019, arXiv preprint arXiv:1902
 Acceleration for compressed gradientdescent in distributed and federated optimization,2020, arXiv preprint arXiv:2002
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 Quartz: Randomized dual coordinate ascent witharbitrary sampling,2015, In Advances in Neural Information Processing Systems
 NUQSGD: Improved communicationefficiency for data-parallel SGD via nonuniform quantization,2019, arXiv preprint arXiv:1908
 Picture coding using pseudo-random noise,1962, IRE Transactions on InformationTheory
 Robust andcommunication-efficient federated learning from non-iid data,2019, IEEE transactions on neuralnetworks and learning systems
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Managed communication and consistency for fast data-paralleliterative analytics,2015, In Proceedings of the Sixth ACM Symposium on Cloud Computing
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2016, arXiv preprint arXiv:1604
