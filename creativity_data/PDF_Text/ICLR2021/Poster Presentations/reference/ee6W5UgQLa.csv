title,year,conference
 Tabfact: A large-scale dataset for table-based fact verification,2019, arXivpreprint arXiv:1909
 Uniter: Learning universal image-text representations,2020, In Proceedings of the 2020European Conference on Computer Vision
 Bert: Pre-training of deep bidirectional transform-ers for language understanding,2019, In North American Association for Computational Linguistics(NAACL)
 Drop: A reading comprehensionbenchmark requiring discrete reasoning over paragraphs,2019, In North American Association forComputational Linguistics (NAACL)
 Clevr: Adiagnostic dataset for compositional language and elementary visual reasoning,2017, In ComputerVision and Pattern Recognition (CVPR)
 HoW much reading does reading comprehension require?a critical investigation of popular benchmarks,2018, In EMNLP
 Natural questions: abenchmark for question ansWering research,2019, Transactions of the Association for ComputationalLinguistics
 Oscar: Object-semantics aligned pre-training for vision-languagetasks,2020, In European Conference on Computer Vision
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Vilbert: Pretraining task-agnostic visiolinguisticrepresentations for vision-and-language tasks,2019, In Advances in Neural Information ProcessingSystems
 Faster r-cnn: ToWards real-time objectdetection With region proposal netWorks,2015, In Advances in neural information processing systems
 Vl-bert: Pre-trainingof generic visual-linguistic representations,2020, In Proceedings of the 2020 International Conferenceon Learning Representations
 Constructing datasets for multi-hop reading comprehensionacross documents,2017, arXiv preprint arXiv:1710
