title,year,conference
 Bayesian dark knowl-edge,2015, In Advances in Neural Information Processing Systems
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Pattern recognition and machine learning,2006, springer
 Imagenet: A large-scale hi-erarchical image database,2009, In The IEEE Conference on Computer Vision and Pattern Recognition
 A unified bias-variance decomposition,2000, In International Conference on MachineLearning
 Deep learning,2016, MIT press
 Deep residual learning for image recog-nition,2016, In The IEEE Conference on Computer Vision and Pattern Recognition
 A com-prehensive overhaul of feature distillation,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Like what you like: Knowledge distill via neuron selectivitytransfer,2017, arXiv preprint arXiv:1707
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In Advances in neural information processing systems
 Bias plus variance decomposition for zero-one loss functions,1996, InInternational Conference on Machine Learning
 Smooth sUbdivision sUrfaces based on triangles,1987, Masterâ€™s thesis
 Uniform convergence may be unable to explain generaliza-tion in deep learning,1161, In Advances in Neural Information Processing Systems
 Distillation asa defense to adversarial perturbations against deep neural networks,2016, In IEEE Symposium onSecurity and Privacy
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision
 Wordnet:: Similarity-measuring therelatedness of concepts,2004, In AAAI Conference on Artificial Intelligence
 Correlation congruence for knowledge distillation,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Fitnets: Hints for thin deep nets,2015, In International Conference on LearningRepresentations
 Distilling knoWledge from a deep pose regressor netWork,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinkingthe inception architecture for computer vision,2016, In The IEEE Conference on Computer Vision andPattern Recognition
 Learningefficient detector With semi-supervised adaptive distillation,2019, arXiv preprint arXiv:1901
 Similarity-preserving knoWledge distillation,2019, In Proceedings of theIEEE International Conference on Computer Vision
 Preparing lessons: Improve knoWledge distillationWith better supervision,2019, arXiv preprint arXiv:1911
 Rethinking bias-variancetrade-off for generalization of neural netWorks,2020, arXiv preprint arXiv:2002
 Revisiting knoWledge distillationvia label smoothing regularization,2020, In The IEEE Conference on Computer Vision and PatternRecognition
 Paying more attention to attention: Improving the per-formance of convolutional neural networks via attention transfer,2017, In International Conference onLearning Representations
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
