title,year,conference
 Certifiably robust variationalautoencoders,2021, arXiv preprint arXiv:2102
 Return of the devil in thedetails: Delving deep into convolutional nets,2014, arXiv preprint arXiv:1405
 Invariance reduces variance: Understanding dataaugmentation in deep learning and beyond,2019, arXiv preprint arXiv:1907
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Parsevalnetworks: Improving robustness to adversarial examples,2017, arXiv preprint arXiv:1704
 Spherical CNNs,2018, arXiv preprintarXiv:1801
 Improving generalization performance using double backpropa-gation,1992, IEEE Transactions on Neural Networks
 Fractional max-pooling,2014, arXiv preprint arXiv:1412
 Towardsconceptual compression,2016, In Advances In Neural Information Processing Systems
 Momentum contrast forunsupervised visual representation learning,2019, arXiv preprint arXiv:1911
 Data-efficientimage recognition with contrastive predictive coding,2019, arXiv preprint arXiv:1905
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Onthe benefits of invariance in neural networks,2020, arXiv preprint arXiv:2005
 Wasserstein dependency measure for representation learning,2019, In Advances in NeuralInformation Processing Systems
 On varia-tional bounds of mutual information,2019, arXiv preprint arXiv:1905
 The manifoldtangent classifier,2011, Advances in neural information processing systems
 Transformation invariancein pattern recognitiontangent distance and tangent propagation,1998, In Neural networks: tricks of thetrade
 Robust large margin deepneural networks,2017, IEEE Transactions on Signal Processing
 Tensor field networks: Rotation-and translation-equivariant neural networks for 3d pointclouds,2018, arXiv preprint arXiv:1802
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 Whatmakes for good views for contrastive learning,2020, arXiv preprint arXiv:2005
 Lipschitz-margin training: Scalable certifi-cation of perturbation invariance for deep neural networks,2018, In Advances in neural informationprocessing Systems
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Understanding contrastive representation learning through align-ment and uniformity on the hypersphere,2020, arXiv preprint arXiv:2005
 Contrastivetraining for improved out-of-distribution detection,2020, arXiv preprint arXiv:2007
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Deeper interpretability of deep networks,2018, arXiv preprint arXiv:1811
 Multi-scale pyramid poolingfor deep convolutional representation,2015, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition Workshops
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Deep sets,2017, In Advances in neural information processing systems
2 Contrastive learningSimilar to Chen et al,2016, (2020a)
1 Comparison with ensemblingFeature averaging is an approach that bears much similarity with ensembling,2021, To experimentallycompare these two approaches
