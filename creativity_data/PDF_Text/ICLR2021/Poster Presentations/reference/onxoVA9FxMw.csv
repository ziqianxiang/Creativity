title,year,conference
 Ordinal embedding: Approximation algorithms and dimensionalityreduction,2008, In Ashish Goel
 What does bert lookat? an analysis of bert’s attention,2019, arXiv preprint arXiv:1906
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Convolutionalsequence to sequence learning,2017, arXiv preprint arXiv:1705
 Finite sample prediction and recovery boundsfor ordinal embedding,2016, In Daniel D
 Rethinking positional encoding in language pre-training,2020, arXivpreprint arXiv:2006
 Learning to encode position fortransformer With continuous dynamical model,2020, arXiv preprint arXiv:2003
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Euclidean embeddings of finite metric spaces,2013, Discrete Mathematics
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Self-attention with relative position representa-tions,2018, arXiv preprint arXiv:1803
 The analysis of proximities: Multidimensional scaling with an unknown distancefunction,1962, Psychometrika
 Local ordinal embedding,2014, volume 32 of Proceedings ofMachine Learning Research
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A multiscale visualization of attention in the transformer model,2019, arXiv preprintarXiv:1906
 Self-attention with structural positionrepresentations,2019, arXiv preprint arXiv:1909
 What do position embeddings learn? an empirical study of pre-trained language model positional encoding,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Nezha: Neural contextualized representation for chineselanguage understanding,2019, arXiv preprint arXiv:1909
 Huggingface’s transformers: State-of-the-art natural language processing,2019, ArXiv
 Self-attention withfunctional time representation learning,2019, In Advances in Neural Information Processing Systems
 Tener: Adapting transformer encoder forname entity recognition,2019, arXiv preprint arXiv:1911
 7 and Fig,2019, 10
