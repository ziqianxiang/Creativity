title,year,conference
 Fine-grained analysisof sentence embeddings using auxiliary prediction tasks,2016, arXiv:1608
 Continuous distributed representation of biologicalsequences for deep proteomics and genomics,2015, PLOS One
 Learning protein sequence embeddings using information fromstructure,2019, In International Conference on Learning Representations
 The protein data bank,2000, Nucleic Acids Research
 On identifiability in Transformers,2020, In International Conference on Learning Representations
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 ProtTrans: Towards cracking the language of life’s code through self-supervised deeplearning and high performance computing,2020, arXiv preprint arXiv:2007
 What BERT is not: Lessons from a new suite of psycholinguistic diagnostics forlanguage models,2020, Transactions of the Association for Computational Linguistics
 Assessing BERT’s syntactic abilities,2019, arXiv preprint arXiv:1901
 Amino acid substitution matrices from protein blocks,0027, Proceedings ofthe National Academy of Sciences
 Attention is not Explanation,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Comprehensive structural classification of ligand-binding motifsin proteins,2009, Structure
 Open sesame: Getting inside BERT’s linguisticknowledge,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
 Linguisticknowledge and transferability of contextual representations,2019, In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Progen: Language modeling for protein generation,2020, arXivpreprint arXiv:2004
 Distributed representationsof words and phrases and their compositionality,2013, In C
 NGLview-interactive molecular graphics forJupyter notebooks,1367, Bioinformatics
 Probing neural network comprehension of natural languagearguments,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Dissecting contextualword embeddings: Architecture and representation,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing
 An analysis of encoder representations in Transformer-based machine translation,2018, In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP
 Evaluating protein transfer learning with TAPE,2019, In Advances in NeuralInformation Processing Systems
 Visualizing and measuring the geometry of BERT,2019, In H
 Accelerating protein designusing autoregressive generative models,2019, bioRxiv
 Biological structure and function emerge from scaling unsupervised learningto 250 million protein sequences,2019, bioRxiv
 Inferring protein 3D structure from deep mutation scans,2019, NatureGenetics
 Protein phosphorylation,1975, Annual Review of Biochemistry
 Unsupervised attention-guided atom-mapping,2020, ChemRxiv
 Clustering huge protein sequence sets in linear time,2018, NatureCommunications
 UniRef clusters: a comprehensive and scalable alternative for improving sequencesimilarity searches,1367, Bioinformatics
 Assessing social and intersectional biases in contextualized wordrepresentations,1323, In Advances in Neural Information Processing Systems 32
 Attention inter-pretability across NLP tasks,2019, arXiv preprint arXiv:1909
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 A multiscale visualization of attention in the Transformer model,2019, In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations
 Analyzing the structure of attention in a Transformer languagemodel,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
 Investigating gender bias in language models using causal mediation analysis,2020, InAdvances in Neural Information Processing Systems
 Does BERT make anysense? Interpretable word sense disambiguation with contextualized embeddings,2019, arXiv preprintarXiv:1909
 XLNet: Generalized autoregressive pretraining for language understanding,2019, In H
 Fine-grained sentiment analysis with faithfulattention,2019, arXiv preprint arXiv:1908
