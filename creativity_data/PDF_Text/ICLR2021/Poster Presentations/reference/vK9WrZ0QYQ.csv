title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Theory of reproducing kernels,1950, Transactions of the American mathematicalsociety
 Deep equals shallow for relu networks in kernel regimes,2021, In ICLR
 On the inductive bias of neural tangent kernels,2019, In Advances inNeural Information Processing Systems
 Positive definite functions on spheres,1973, In Mathematical Proceedings of theCambridge Philosophical Society
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems
 Towards understanding thespectral bias of deep learning,2019, arXiv preprint arXiv:1912
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Introduction to the theory and application of the Laplace transformation,1974, Springer
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks,2020, arXiv preprint arXiv:2005
 Analytic combinatorics,2009, cambridge University press
 Kernel machines beat deep neural networks on mask-based single-channel speech enhancement,2019, Proc
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Explaining landscape connectivity of low-cost solutions for multilayer nets,2019, In Advancesin Neural Information Processing Systems
 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning,2020, arXiv preprintarXiv:2003
 Analyzing the decay rate of taylor series coefficients when high-order derivatives areintractable,2020, MathOverflow
 Theory of reproducing kernels and applications,2016, Springer
 A fine-grained spectral perspective on neural networks,2019, arXiv preprintarXiv:1907
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
