title,year,conference
 Xnor-net++: Improved binary neural networks,2019, arXivpreprint arXiv:1909
 Pact: Parameterized clipping activation for quantized neuralnetworks,2018, arXiv preprint arXiv:1805
 Quantization of deep neural networks foraccumulator-constrained processors,2020, Microprocessors and Microsystems
 Learning accuratelow-bit deep neural networks with stochastic quantization,2017, arXiv preprint arXiv:1708
 Binarizedneural networks,2016, In Advances in neural information processing Systems
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Fbgemm: Enabling high-performance low-precision deep learning inference,2021, arXivpreprint arXiv:2101
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Trainingquantized nets: A deeper understanding,2017, In Advances in Neural Information Processing Systems
 Towards accurate binary convolutional neural network,2017, InAdvances in Neural Information Processing Systems
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Apprentice: Using knowledge distillation techniques to improvelow-precision network accuracy,2017, arXiv preprint arXiv:1711
 Wrpn: wide reduced-precisionnetworks,2017, arXiv preprint arXiv:1709
 Espresso: Efficient forward prop-agation for binary deep neural networks,2018, 2018
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Accumulation bit-width scaling for ultra-low precision training ofdeep networks,2019, arXiv preprint arXiv:1901
 Imagenet pre-trained models with batch normal-ization,2016, arXiv preprint arXiv:1612
 Haq: Hardware-aware automated quan-tization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Train-ing deep neural networks with 8-bit floating point numbers,2018, In Advances in neural informationprocessing Systems
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
 Adaptive layerwise quantization for deep neuralnetwork compression,2018, In 2018 IEEE International Conference on Multimedia and Expo (ICME)
