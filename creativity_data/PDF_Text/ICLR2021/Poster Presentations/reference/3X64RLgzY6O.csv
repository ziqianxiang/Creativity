title,year,conference
 A continuous-time view of early stopping for leastsquares regression,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 The implicit regularization of stochastic gradientflow for least squares,2020, arXiv preprint arXiv:2003
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Stability of stochastic gradientdescent on nonsmooth convex losses,2020, arXiv preprint arXiv:2006
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, arXiv preprint arXiv:2002
 Sgd: General analysis and improved rates,2019, arXiv preprint arXiv:1901
 Characterizing implicit bias interms of optimization geometry,2018, In ICML
 Shape matters: Understanding theimplicit bias of the noise covariance,2020, arXiv preprint arXiv:2006
 Matrix analysis,2012, Cambridge university press
 On the diffusion approximation of noncon-vex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 The break-even point on optimization trajectories of deep neuralnetWorks,2019, In International Conference on Learning Representations
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 Gradient descent aligns the layers of deep linear netWorks,2019, In 7thInternational Conference on Learning Representations
 Gradient descent follows theregularization path for general losses,2020, In Conference on Learning Theory
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Data-dependent stability of stochastic gradient descent,2018, InInternational Conference on Machine Learning
 The two regimes of deep network training,2020, arXiv preprintarXiv:2002
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Implicit bias in deep linear classification: Initialization scale vs training accuracy,2020, arXivpreprint arXiv:2007
 Convergence of gradient descent on separable data,2019, In The 22ndInternational Conference on Artificial Intelligence and Statistics
 Stochastic gradient descent on separabledata: Exact convergence with a fixed learning rate,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Onthe heavy-tailed theory of stochastic gradient descent for deep neural networks,2019, arXiv preprintarXiv:1912
 Connecting optimization and regulariza-tion paths,2018, In Advances in Neural Information Processing Systems
 Introduction to the non-asymptotic analysis of random matrices,2010, arXiv preprintarXiv:1011
 Onthe noisy gradient descent that generalizes as sgd,2020, The 37th International Conference on MachineLearning
 Global convergence of langevin dynamicsbased algorithms for nonconvex optimization,2018, In Advances in Neural Information ProcessingSystems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, The 36thInternational Conference on Machine Learning
