title,year,conference
 Online embedding com-pression for text classification using low rank matrix factorization,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Input sparsity and hardness for robust subspace ap-proximation,2015, In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science
 Reducing transformer depth on demand withstructured dropout,2019, In International Conference on Learning Representations
 Learning dictionary via subspacesegmentation for sparse representation,2011, In 2011 18th IEEE International Conference on ImageProcessing
 Albert: A lite bert for self-supervised learning of language representations,2019, In InternationalConference on Learning Representations
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Attentive studentmeets multi-task teacher: Improved knowledge distillation for pretrained models,2019, arXiv preprintarXiv:1911
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Least squares quantization in pcm,1982, IEEE transactions on information theory
 Superviseddictionary learning,2009, In Advances in neural information processing systems
 Distilling transformers into simple neuralnetworks with unlabeled transfer data,2019, arXiv preprint arXiv:1910
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Q-bert: Hessian based ultra low precision quantization of bert,2020, In AAAI
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Dictionary learning,2011, IEEE Signal Processing Magazine
 One-class active learning for outlier detection with mul-tiple subspaces,2019, In Proceedings of the 28th ACM International Conference on Information andKnowledge Management
 Coresets for near-convex functions,2020, arXiv preprintarXiv:2006
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Structured pruning of large language models,2019, arXivpreprint arXiv:1910
 HUggingface's transformers:State-of-the-art natural language processing,2019, ArXiv
 On compressing deep models by lowrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
