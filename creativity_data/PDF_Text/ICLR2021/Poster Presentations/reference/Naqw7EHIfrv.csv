title,year,conference
 Effectiveness of self-supervised pre-training for speech recognition,2019, arXiv preprint arXiv:1911
 An information-maximization approach to blind separa-tion and blind deconvolution,1995, Neural computation
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Attention-based models for speechrecognition,2015, In NIPS
 Unsupervised speech representationlearning using wavenet autoencoders,2019, IEEE Trans
 Empirical evaluationof gated recurrent neural networks on sequence modeling,2014, In NIPS 2014 Workshop on DeepLearning
 A recurrent latent variable model for sequential data,2015, In Advances in neural informationprocessing Systems
 An unsupervised autoregressive model for speechrepresentation learning,2019, In Interspeech
 Generative pre-training for speech with autoregressive predictivecoding,2020, In ICASSP
 An unsupervised autoregressive modelfor speech representation learning,2019, Interspeech
 Unsupervised discovery of temporal structurein noisy data with dynamical components analysis,2019, In NIPS
 Theoretical neuroscience: computational and mathematicalmodeling of neural systems,2001, Computational Neuroscience Series
 Machine learning for neural decoding,2020, eNeuro
 Connectionist temporal classification:Labelling unsegmented sequence data with recurrent neural networks,2006, In ICML
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In International Conference on Artificial Intelligence andStatistics
 Learning deep representations by mutual information estimationand maximization,2018, In ICLR
 End-to-end speech recognition with word-basedRNN language models,2018, In SLT
 Im-proving transformer-based speech recognition using unsupervised pre-training,2019, arXiv:1910
 A comparative study on transformer vs rnn inspeech applications,2019, In ASRU
 Learningrobust and multilingual speech representations,2020, arXiv preprint arXiv:2001
 Auto-encoding variational bayes,2014, In ICLR
 An introduction to Kolmogorov complexity and its applications,2008, Springer
 Disentangled sequential autoencoder,2018, arXiv preprintarXiv:1803
 Deep contextualized acousticrepresentations for semi-supervised speech recognition,2020, In ICASSP
 Tera: Self-supervised learning of transformer encoderrepresentation for speech,2020, arXiv preprint arXiv:2007
 Masked pre-trained encoder base on joint ctc-transformer,2020, arXiv preprintarXiv:2005
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Wasserstein dependency measure for representation learning,2019, In NIPS
 Librispeech: an asr corpusbased on public domain audio books,2015, In ICASSP
 Learning problem-agnostic speechrepresentations from multiple self-supervised tasks,2019, In Interspeech
 Learn-ing problem-agnostic speech representations from multiple self-supervised tasks,2019, arXiv preprintarXiv:1904
 Numerical and physical modeling of the dynamics of the lorenz system,2014, Numericalanalysis and Applications
 Multi-task self-supervised learning for robust speech recognition,2020, In ICASSP
 wav2vec: Unsupervisedpre-training for speech recognition,2019, Interspeech
 Amortizedinference regularization,2018, In NIPS
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 Vae with a vampprior,2018, In International Conference on ArtificialIntelligence and Statistics
 On mutualinformation maximization for representation learning,2019, In ICLR
 Attention is all you need,2017, In NIPS
 On deep multi-view representationlearning,2015, In ICML
 Unsupervised pre-training of bidirectionalspeech encoders via masked reconstruction,2020, In ICASSP
 ESPnet: End-to-end speech processing toolkit,2018, In Interspeech
 Slow feature analysis: Unsupervised learning of invari-ances,2002, Neural computation
