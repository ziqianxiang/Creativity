title,year,conference
 On the optimization of deep networks: Implicit accelerationby overparameterization,2018, In 35th International Conference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, In H
 On implicit regularization: Morse functions and applications to matrixfactorization,2020, arXiv preprint arXiv:2001
 Julia: A fast dynamic languagefor technical computing,2012, arXiv preprint arXiv:1209
 Nonconvex optimization meets low-rank matrix factorization:An overview,2019, IEEE Transactions on Signal Processing
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss,2020, volume 125 of Proceedings of Machine Learning Research
 Generalized gradients and applications,1975, Transactions of the American MathematicalSociety
 Implicit regularization of discrete gradientdynamics in linear neural networks,2019, In H
 The implicit bias of depth: How incrementallearning drives generalization,2020, In International Conference on Learning Representations
 In I,2017, Guyon
 Implicit bias of gradient descent onlinear convolutional networks,2018, In S
 The clarke and michel-penot subdifferentials of theeigenvalues of a symmetric matrix,1999, Comput
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In S
 Gradient descent aligns the layers of deep linear networks,2019, InInternational Conference on Learning Representations
 A refined primal-dual analysis of the implicit bias,2019, arXiv preprintarXiv:1906
 On approximationguarantees for greedy low rank optimization,2017, arXiv preprint arXiv:1703
 Gradient descent onlyconverges to minimizers,2016, In Conference on learning theory
 First-order methods almost always avoid saddle points,2017, arXiv preprint arXiv:1710
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Sebastien Bubeck
 Lexicographicand depth-sensitive margins in homogeneous and non-homogeneous deep models,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov
 Convergence of gradient descent on separable data,2019, In KamalikaChaudhuri and Masashi Sugiyama
 Stochastic gradient descent on separabledata: Exact convergence with a fixed learning rate,2019, In Kamalika Chaudhuri and Masashi Sugiyama
 First-order methods almost always avoidsaddle points: The case of vanishing step-sizes,2019, In H
 Implicit regularization in deep learning may not be explainable bynorms,2020, arXiv preprint arXiv:2005
 The implicit bias of gradient descent on separabledata,2018, In International Conference on Learning Representations
 Rank-onematrix pursuit for matrix completion,2014, In International Conference on Machine Learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In I
 Greedy learning of generalized low-rank models,2016, In IJCAIInternational Joint Conference on Artificial Intelligence
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
 Ensembles Semi-analytiques,1965, IHES notes
