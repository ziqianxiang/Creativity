title,year,conference
 Monotonic infinite lookback attention for simultaneousmachine translation,2019, In ACL
 Layer normalization,2016, arXiv preprintarXiv:1607
 Weight uncertainty inneural networks,2015, arXiv preprint arXiv:1505
 Towards evaluating the robustness of neural networks,2017, In 2017ieee symposium on security and privacy (sp)
 In Interspeech,2020, ISCA
 Net2net: Accelerating learning via knowledgetransfer,2015, arXiv preprint arXiv:1511
 Monotonic chunkwise attention,2017, arXiv preprintarXiv:1712
 Monotonic chunkwise attention,2018, In International Conferenceon Learning Representations
 XcePtion: Deep learning with depthwise separable convolutions,2017, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Towards better decoding and language model integration insequence to sequence models,2016, arXiv preprint arXiv:1612
 End-to-end con-tinuous speech recognition using attention-based recurrent nn: First results,2014, arXiv preprintarXiv:1412
 An online attention-based model forspeech recognition,2018, arXiv preprint arXiv:1811
 Convolutionalsequence to sequence learning,2017, arXiv preprint arXiv:1705
 Sequence transduction with recurrent neural networks,2012, arXiv preprintarXiv:1211
 Conformer: Convolution-augmented transformerfor sPeech recognition,2020, arXiv preprint arXiv:2005
 Contextnet: ImProving convolutional neural networks forautomatic sPeech recognition with global context,2020, arXiv preprint arXiv:2005
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Anonline sequence-to-sequence model using partial conditioning,2016, In Advances in Neural InformationProcessing Systems 29
 Large-scale multilingual speechrecognition with a streaming end-to-end model,2019, arXiv preprint arXiv:1909
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Parallel rescoringwith transformer for streaming on-device speech recognition,2020, arXiv preprint arXiv:2008
 Toward domain-invariantspeech recognition via large scale training,2018, In 2018 IEEE Spoken Language Technology Workshop(SLT)
 Cascaded encoders for unifying streaming andnon-streaming asr,2020, arXiv preprint arXiv:2010
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Librispeech: an asr corpusbased on public domain audio books,2015, In 2015 IEEE International Conference on Acoustics
 Efficient knowledge distillation for rnn-transducer models,2020, arXiv preprintarXiv:2011
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Minimum word error rate training for attention-based sequence-to-sequence models,2018, In 2018 IEEE International Conference on Acoustics
 Online and Linear-Time Attention byEnforcing Monotonic Alignments,2017, In Proc
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 Two-pass end-to-end speech recogni-tion,2019, arXiv preprint arXiv:1908
 Lingvo: a modular and scalable frameworkfor sequence-to-sequence modeling,2019, arXiv preprint arXiv:1902
 Towards onlineend-to-end transformer automatic speech recognition,2019, arXiv preprint arXiv:1910
 Streaming transformer asr with blockwisesynchronous inference,2020, arXiv preprint arXiv:2006
 Simultaneous deep transfer acrossdomains and tasks,2015, In Proceedings of the IEEE International Conference on Computer Vision
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Low latencyend-to-end streaming speech recognition with a scout network,2020, arXiv preprint arXiv:2003
 A learning algorithm for continually running fully recurrentneural networks,1989, Neural computation
 Dynamicsparsity neural networks for automatic speech recognition,2020, arXiv preprint arXiv:2005
 Transformer-transducer: End-to-end speech recognition with self-attention,2019, arXiv preprint arXiv:1910
 Bignas: Scaling up neural archi-tecture search with big single-stage models,2020, arXiv preprint arXiv:2003
