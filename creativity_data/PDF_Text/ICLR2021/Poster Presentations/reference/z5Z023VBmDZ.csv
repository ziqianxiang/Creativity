title,year,conference
 Less is more: A comprehensive framework for the number of componentsof ensemble classifiers,2017, arXiv preprint arXiv:1709
 A downsampled variant of imagenet as an alternative tothe cifar datasets,2017, arXiv preprint arXiv:1707
 Big neural networks waste capacity,2013, arXiv preprint arXiv:1301
 Understanding deep architectures using a recursiveconvolutional network,2013, arXiv preprint arXiv:1312
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Densely connected convolutionalnetworks,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 The relative performance of ensemble methods with deepconvolutional neural networks for image classification,2017, arXiv preprint arXiv:1704
 Why m heads are better than one:Training a diverse ensemble of deep networks,2015, arXiv preprint arXiv:1511
 Outrageously largeneural networks: The sparsely-gated mixture-of-experts layer,2017, arXiv preprint arXiv:1701
 Hardware for machine learning: Challengesand opportunities,2017, In 2017 IEEE Custom Integrated Circuits Conference
 Efficient processing of deep neural networks: Atutorial and survey,2017, Proceedings of the IEEE
 Machine learning at facebook: Understanding inference at the edge,2019, In 2019 IEEEInternational Symposium on High Performance Computer Architecture (HPCA)
 Wide residual networks,2016, arXiv preprint arXiv:1605
