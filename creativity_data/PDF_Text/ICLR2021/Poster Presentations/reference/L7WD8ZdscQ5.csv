title,year,conference
 Optimal regularized dual averaging methods for stochasticoptimization,2012, In Advances in Neural Information Processing Systems
 On the convergence of adamand adagrad,2020, ArXiv
 Introductory lectures on stochastic optimization,2018, The mathematics of data
 Global convergence of theheavy-ball method for convex optimization,2015, In 2015 European Control Conference (ECC)
 Understanding the role of momentumin stochastic gradient methods,2019, In Advances in Neural Information Processing Systems
 Tight analyses for non-smooth stochastic gradient descent,2019, In Annual Conference on Learning Theory
 Accelerated gradient methods for stochastic optimiza-tion and online learning,2009, In Advances in Neural Information Processing Systems
 Making the last iterate of sgd informationtheoretically optimal,2019, In Annual Conference on Learning Theory
 The unusual effectiveness of averaging in gan training,2014, InInternational Conference on Learning Representations
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Accelerating sgd with momentum for over-parameterized learn-ing,2020, In International Conference on Learning Representations
 Variants of rmsprop and adagrad with logarithmicregret bounds,2017, In International Conference on Machine Learning
 Making gradient descent optimal forstrongly convex stochastic optimization,2011, arXiv preprint arXiv:1109
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 On the convergence of the stochasticheavy ball method,2020, ArXiv
 Heavy-ball algorithmsalways escape saddle points,2019, arXiv preprint arXiv:1907
 Non-ergodic con-vergence analysis of heavy-ball algorithms,2019, In AAAI
 The strength of nesterovâ€™s extrapolation in theindividual convergence of nonsmooth optimization,2020, IEEE Transactions on Neural Networks andLearning Systems
 Primal averaging: A new gradient evaluation stepto attain the optimal individual convergence,2020, IEEE Transactions on Cybernetics
 Sadam: A variant of adam for stronglyconvex functions,2020, In International Conference on Learning Representations
 Unified convergence analysis of stochastic momentummethods for convex and non-convex optimization,2016, arXiv preprint arXiv:1604
 Online convex programming and generalized infinitesimal gradient ascent,2003, InProceedings of the International Conference on Machine Learning
