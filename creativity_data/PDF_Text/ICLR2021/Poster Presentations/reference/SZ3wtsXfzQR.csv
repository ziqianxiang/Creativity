title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, InAdvances in Neural Information Processing Systems 29
 A notion of task relatedness yielding provable multiple-task learning guarantees,2008, Machine learning
 A theory of learning from different domains,2010, Machine learning
 Generalize across tasks: Efficient algorithmsfor linear representation learning,2019, In AUrelien Garivier and Satyen Kale (eds
 A theoretical analysis of the nUmber of shots in few-shotlearning,2019, arXiv preprint arXiv:1909
 Learning-to-learn stochas-tic gradient descent with biased regUlarization,2019, In Kamalika ChaUdhUri and RUslan SalakhUtdinov(eds
 Transmission of information,1961, A Statistical Theory of Communication
 Recasting gradient-based meta-learning as hierarchical Bayes,2018, arXiv preprint arXiv:1801
 On the value of target data in transfer learning,2019, In Advances inNeural Information Processing Systems 32
 A no-free-lunch theorem for multitask learning,2020, arXiv preprintarXiv:2006
 Provable guarantees for gradient-basedmeta-learning,2019, arXiv preprint arXiv:1902
 Information-theoretic limitations on novel taskgeneralization,2019, Neurips 2019 Workshop on Machine Learning with Guarantees
 Self-tuningnetworks: Bilevel oPtimization of hyPerParameters using structured best-resPonse functions,2019, In7th International Conference on Learning Representations
 Transfer bounds for linear feature learning,2009, Machine learning
 Minimax multi-task learning and a generalizedloss-comPositional Paradigm for mtl,2012, Advances in Neural Information Processing Systems
 Meta-learning uPdaterules for unsuPervised rePresentation learning,2019, In 7th International Conference on LearningRepresentations
 New analysis and algorithm for learning with driftingdistributions,2012, In International Conference on Algorithmic Learning Theory
 In H,2020, Larochelle
 Minimax rates of estimation for high-dimensionallinear regression over '_q-balls,2011, IEEE transactions on information theory
 OPtimization as a model for few-shot learning,2016, InternationalConference on Learning Representations
 An emPirical bayes aPProach to statistics,1956, In Proceedings of the Third BerkeleySymposium on Mathematical Statistics and Probability
 PrototyPical networks for few-shot learning,2017, InAdvances in Neural Information Processing Systems
 A perspective view and survey of meta-learning,2002, Artificialintelligence review
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Some matrix-inequalities and metrization of matric space,1937, 1937
 Multitask metric learning:Theory and algorithm,2019, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Information-theoretic determination of minimax rates of conver-gence,1999, Annals of Statistics
