title,year,conference
 Tensorflow: a system for large-scalemachine learning,2016, In OSDI
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Theano: new features and speedimprovements,2012, arXiv preprint arXiv:1211
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 A Cellular Computer to Implement the Kalman Filter Algorithm,1969, PhD thesis
 State-of-the-artspeech recognition with sequence-to-sequence models,2018, In 2018 IEEE International Conference onAcoustics
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Depth-adaptive transformer,2020, ArXiv
 Pipedream: Fast and efficient pipeline parallel dnn training,2018, arXiv preprintarXiv:1806
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Beyond human-level accuracy,2019, Proceedingsof the 24th Symposium on Principles and Practice of Parallel Programming
 Gpipe: Efficient training of giantneural networks using pipeline parallelism,2019, Advances in Neural Information Processing Systems32
 Beyond Data and Model Parallelism for Deep NeuralNetworks,2019, In Proceedings of the Conference on Systems and Machine Learning (SysML)
 In-datacenter performance analysis ofa tensor processing unit,2017, In Proceedings of the 44th Annual International Symposium on ComputerArchitecture
 ImageNet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In EMNLP
 Scalable parallel programming withcuda,2008, Queue
 Wavenet: A generative model for rawaudio,2016, arXiv preprint arXiv:1609
 Optimizing data-intensive computations in existing libraries withsplit annotations,2019, In Proceedings of the 27th ACM Symposium on Operating Systems Principles
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Automatic differentiation inpytorch,2017, 2017
 Large-scale deep unsupervised learning usinggraphics processors,2009, In Proceedings of the 26th annual international conference on machinelearning
 Zero: Memory optimizationtowards training a trillion parameter models,2019, arXiv preprint arXiv:1910
 Fast transformer decoding: One write-head is all you need,2019, arXiv preprintarXiv:1911
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Natural tts synthesis by conditioningwavenet on mel spectrogram predictions,2018, In 2018 IEEE International Conference on Acoustics
 Mixture models for diversemachine translation: Tricks of the trade,2019, arXiv preprint arXiv:1902
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 Summarizing cpu and gpu designtrends with product data,2019, arXiv preprint arXiv:1911
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Condconv: Conditionally parameter-ized convolutions for efficient inference,2019, In Advances in Neural Information Processing Systems
 Understandingdeep learning requires rethinking generalization,2017, 2017
