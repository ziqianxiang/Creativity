title,year,conference
 On the optimization of deep networks: implicitacceleration by overparameterization,2018, In International Conference on Machine Learning
 Shaping the learning landscape in neuralnetworks around wide flat minima,2020, Proceedings of the National Academy of Sciences
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 Statistical mechanical approaches to models with many poorlyknown parameters,2003, Physical Review E
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Meta-learning with warped gradient descent,2020, In International Conference on LearningRepresentations
 Deep ensembles: a loss landscape perspec-tive,2020, arXiv preprint arXiv:1912
 Dropout as a Bayesian approximation: representing modeluncertainty in deep learning,2016, In International Conference on Machine Learning
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Deep pyramidal residual networks,1610, In Conferenceon Computer Vision and Pattern Recognition
 Deep residual learning for image recog-nition,2016, pp
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In International Conference on Learning Representations
 Long short-term memory,1997, Neural Computation
 Densely connectedconvolutional networks,2018, arXiv preprint arXiv:1608
 Batch normalization: accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Number 4,1951, American Mathematical Soc
 Multiplicative interactions andwhere to find them,2020, In International Conference on Learning Representations
 Fantas-tic generalization measures and where to find them,2020, In International Conference on LearningRepresentations
 Continual reinforcement learning withcomplex synapses,2018, In International Conference on Machine Learning
 On large-batch training for deep learning: generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Adam: a method for stochastic optimization,2015, In InternationalConference on Learning Representations
 On analytical methods in probability theory,1931, Math
 Learning multiple layers of features from tiny images,2009, Technical report
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems 30
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 WhyM heads are better than one: training a diverse ensemble of deep networks,2015, arXiv preprint:arXiv:1511
 Deep learning theory review: An optimal controland dynamical systems perspective,2019, arXiv preprint arXiv:1908
 In Advances in Neural InformationProcessing Systems 32,2019, 2019
 Second-order optimization for neural networks,2016, PhD thesis
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 K for theprice of 1: parameter-efficient multi-task and transfer learning,2019, In International Conference onLearning Representations
 Meta networks,2017, In International Conference on MachineLearning
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Entropic gradientdescent algorithms and wide flat minima,2020, arXiv preprint arXiv:2006
 Learning multiple visual domains withresidual adapters,2017, In Advances in Neural Information Processing Systems 30
 Empirical analysis ofthe Hessian of over-parametrized neural networks,2018, arXiv preprint arXiv:1706
 Continuallearning with hypernetworks,2020, In International Conference on Learning Representations
 Sloppy-model universality class and theVandermonde matrix,2006, Physical Review Letters
 Bayesian learning via stochastic gradient Langevin dynamics,2011, InInternational Conference on Machine Learning
 BatchEnsemble: an alternative approach to efficientensemble and lifelong learning,2020, In International Conference on Learning Representations
 Fluctuation-dissipation relations for stochastic gradient descent,2018, arXiv preprintarXiv:1810
 Shakedrop regularizationfor deep residual learning,2019, IEEEAccess
 LSUN: construction ofa large-scale image dataset using deep learning with humans in the loop,2015, arXiv preprintarXiv:1506
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference
 The anisotropic noise in stochasticgradient descent: its behavior of escaping from sharp minima and regularization effects,2018, arXivpreprint arXiv:1803
 Fast contextadaptation via meta-learning,2019, In International Conference on Machine Learning
