title,year,conference
 Saliency-driven classimpressions for feature visualization of deep neural networks,2020, arXiv preprint arXiv:2007
 Eval-uating saliency map explanations for convolutional neural networks: a user study,2020, In Proceedingsofthe 25th International Conference on Intelligent User Interfaces
 On pixel-wise explanations for non-linear classifier decisions by layer-wiserelevance propagation,2015, PloS one
 Network dissection:Quantifying interpretability of deep visual representations,2017, In Proceedings of the IEEE conferenceon computer vision and pattern recognition
 A psychophysics approach for quantitative comparisonof interpretable computer vision models,2019, arXiv preprint arXiv:1912
 Human-centered tools forcoping with imperfect algorithms during medical decision-making,2019, In Proceedings of the 2019CHI Conference on Human Factors in Computing Systems
 Curve detectors,2020,	Distill
 Machine learning interpretability: Asurvey on methods and metrics,2019, Electronics
 Ittakes two to tango: Towards theory of ai’s mind,2017, arXiv preprint arXiv:1704
 Are visual explanations useful? a case study in model-in-the-loop prediction,2020, arXiv preprint arXiv:2007
 Explainexplore: Visual exploration of machine learningexplanations,2020, In 2020 IEEE Pacific Visualization Symposium (PacficVs)
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Why model why? assessing the strengths and limitations oflime,2020, arXiv preprint arXiv:2012
 Challenging common interpretability assump-tions in feature attribution explanations,2020, arXiv preprint arXiv:2012
 Visualizing higher-layerfeatures ofa deep network,2009, University of Montreal
 Representativity and consistency measures for deep neural net-work explanations,2020, arXiv preprint arXiv:2009
 Net2vec: Quantifying and explaining how concepts are encoded byfilters in deep neural networks,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Towards automatic concept-basedexplanations,2019, In Advances in Neural Information Processing Systems
 Deep Learning,2016, MIT Press
 European Union regulations on algorithmic decision-makingand a “right to explanation”,2017, AI magazine
 Deep neural networks: a new framework for modeling biological vision andbrain information processing,2015, Annual review of vision science
 Evolving complexyet interpretable representations: application to alzheimer’s diagnosis and prognosis,2020, In 2020IEEE Congress on Evolutionary Computation (CEC)
 Unmasking clever hans predictors and assessing what machines reallylearn,2019, Nature communications
 Towards falsifiable interpretability research,2020, arXiv preprintarXiv:2010
 Deep learning,2015, nature
 The mythos of model interpretability,2018, Queue
 Methods for interpreting and Un-derstanding deep neural networks,2018, Digital Signal Processing
 On the importance ofsingle directions for generalization,2018, arXiv preprint arXiv:1803
 Deep neural networks are easily fooled: High confi-dence predictions for unrecognizable images,2015, In Proceedings of the IEEE conference on computervision and pattern recognition
 Synthesizing thepreferred inputs for neurons in neural networks via deep generator networks,2016, In Advances inneural information processing systems
 Multifaceted feature visualization: Uncoveringthe different types of features learned by each neuron in deep neural networks,2016, arXiv preprintarXiv:1602
 ” i know it when i see it”,2017, visualization and intuitive interpretability
 Feature visualization,2017, Distill
 The building blocks of interpretability,2018, Distill
 Psychopy2: Experiments in behavior madeeasy,2019, Behavior research methods
 Learning to generate reviews and discoveringsentiment,2017, arXiv preprint arXiv:1704
 ”why should i trust you?” explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 Toward interpretable machine learning: Transparent deep neural networks andbeyond,2020, arXiv preprint arXiv:2003
 Quantifying interpretability and trust in machine learningsystems,2019, arXiv preprint arXiv:1901
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE international conference on computer vision
 How useful are the machine-generated interpretations togeneral users? a human evaluation on guessing the incorrectly predicted labels,2020, arXiv preprintarXiv:2008
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Quantifying explainability of saliency methods in deep neural net-works,2020, arXiv preprint arXiv:2009
 Evaluation of post-hoc xai approaches through synthetic tabular data,2020, In International Symposium on Methodologiesfor Intelligent Systems
 Cnn explainer: Learning convolutional neural networks withinteractive visualization,2020, arXiv preprint arXiv:2004
 Understanding neuralnetworks through deep visualization,2015, arXiv preprint arXiv:1506
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 Object detectorsemerge in deep scene cnns,2014, arXiv preprint arXiv:1412
 Visualizing deep neural networkdecisions: Prediction difference analysis,2017, arXiv preprint arXiv:1702
