title,year,conference
 Learning imbalanceddatasets With label-distribution-aWare margin loss,2019, In Advances in Neural Information ProcessingSystems
 Understanding and utilizingdeep neural netWorks trained With noisy labels,2019, In International Conference on Machine Learning
 Learning With boundedinstance-and label-dependent label noise,2017, arXiv preprint arXiv:1709
 Class-balanced loss based oneffective number of samples,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Classification in the presence of label noise: a survey,2013, IEEEtransactions on neural networks and learning systems
 Curriculumnet: Weakly supervised learning from large-scale Web images,2018, InProceedings of the European Conference on Computer Vision (ECCV)
 Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels,2018, InAdvances in neural information processing systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Simple and effective regularization methods for training on noisily labeleddata With generalization guarantee,2020, In International Conference on Learning Representations
 Bidirectional lstm-crf models for sequence tagging,2015, arXivpreprint arXiv:1508
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In International Conferenceon Machine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient descent With early stopping is prov-ably robust to label noise for overparameterized neural netWorks,2019, arXiv preprint arXiv:1903
 Large-scalelong-tailed recognition in an open world,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
" Decoupling"" when to update"" from"" how to update""",2017, InAdvances in Neural Information Processing Systems
 Learning from binary labelswith instance-dependent corruption,2016, arXiv preprint arXiv:1605
 Coresets for robust training of neuralnetworks against noisy labels,2020, Advances in Neural Information Processing Systems
" Minimum"" norm"" neural networks are splines",2019, arXiv preprintarXiv:1910
 Automatic differentiation inpytorch,2017, 2017
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Local and global asymptotic inference in smoothing splinemodels,2013, The Annals of Statistics
 Robust and on-the-flydataset denoising for image classification,2020, arXiv preprint arXiv:2003
 Adaptive piecewise polynomial estimation via trend filtering,2014, The Annals ofStatistics
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Dynamic curriculum learning forimbalanced data classification,2019, In Proceedings of the IEEE International Conference on ComputerVision
 Learning to model the tail,2017, In Advances inNeural Information Processing Systems
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, In Advances in Neural Information Processing Systems
 Improved sample complexities for deep networks and robust classificationvia an all-layer margin,2019, arXiv preprint arXiv:1910
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In Advances in Neural Information ProcessingSystems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in neural information processing systems
 of neg,2020, reviews	Acc
