title,year,conference
 Gated feedback recurrentneural networks,2015, In ICML
 Whatyou can cram into a single vector: Probing sentence embeddings for linguistic properties,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Integrating topics andsyntax,2005, In Advances in neural information processing systems
 Colorlessgreen recurrent networks dream hierarchically,2018, In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Kenlm: Faster and smaller language model queries,2011, In Proceedings of the sixthworkshop on statistical machine translation
 Long short-term memory,1997, Neural Computation
 Modeling long distance dependence in language: Topic mixturesvs,1996, dynamic cache models
 In H,2020, Larochelle
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Dynamic evaluation of trans-former language models,2019, arXiv preprint arXiv:1904
 Critical behavior from deep dynamics: a hidden dimension innatural language,2016, arXiv preprint arXiv:1606
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 Multi-timescale longshort-term memory neural network for modelling sentences and documents,2015, In Proceedings of the2015 Conference on Empirical Methods in Natural Language Processing
 Mogrifier lstm,2019, In International Conferenceon Learning Representations
 On the state of the art of evaluation in neural languagemodels,2018, In International Conference on Learning Representations
 Pointer sentinel mixturemodels,2017, In International Conference on Learning Representations
 Regularizing and optimizing LSTMlanguage models,2018, In International Conference on Learning Representations
 A hybrid approach to adaptive statistical language modeling,1994, Technical report
 Parallels in the sequentialorganization of birdsong and human speech,2019, Nature communications
 Lstm networks canperform dynamic counting,2019, ACL 2019
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Cached long short-term memory neuralnetworks for document-level sentiment classification,2016, In Proceedings of the 2016 Conferenceon Empirical Methods in Natural Language Processing
 Exploring semantic properties of sentence em-beddings,2018, In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers)
