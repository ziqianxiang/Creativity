title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Benign ovefitting in linearregression,2019, arXiv preprint arXiv:1906
 Reconciling modern machine learningand the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 A new look at an old problem: A universal learningapproach to linear regression,2019, arXiv preprint arXiv:1905
 A model of double descent for high-dimensional binary linear classification,2019, arXiv preprint arXiv:1911
 Wonder: Weighted one-shot distributed ridge regression in highdimensions,2019, arXiv preprint arXiv:1903
 High-dimensional asymptotics of prediction: Ridge regressionand classification,2018, The Annals ofStatistics
 Classifiers in almost empty spaces,2000, In Proceedings 15th International Conferenceon Pattern Recognition
 Scaling description of generalization withnumber of parameters in deep learning,2019, arXiv preprint arXiv:1901
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Matrix Analysis,1990, Cambridge UniversityPress
 Optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization,2018, arXiv preprintarXiv:1805
 An analytic theory of generalization dynamics and transferlearning in deep linear networks,2018, arXiv preprint arXiv:1809
 Eigenvalues of covariance matrices: Application toneural-network learning,1991, Physical Review Letters
 Second order properties of error surfaces: Learning timeand generalization,1991, In Advances in neural information processing Systems
 Just interpolate: Kernel” ridgeless” regression can gener-alize,2018, arXiv preprint arXiv:1808
 Minimizers of the empirical risk and risk mono-tonicity,2019, In Advances in Neural Information Processing Systems
 Asymptotic risk of least squares minimum norm esti-mator under the spike covariance model,2019, arXiv preprint arXiv:1912
 Ramanujan graphs and the solution ofthe kadison-singer problem,2014, arXiv preprint arXiv:1408
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Understanding overfitting peaks in generalization error: Analytical risk curves forl2 and l1 penalized interpolation,2019, ArXiv
 Harmless interpolation of noisy data inregression,2019, arXiv preprint arXiv:1903
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Statistical mechanics of learning: Generalization,1995, The Handbook of Brain Theoryand Neural Networks
 Learning to generalize,2001, Frontiers of Life
 Random features for large-scale kernel machines,2008, In Advances inneural information processing Systems
 A problem of dimensionality: A simple example,1979, IEEE Transactions on patternanalysis and machine intelligence
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 On the number of variables to use in principal component regression,2019, InAdvances in Neural Information Processing Systems
