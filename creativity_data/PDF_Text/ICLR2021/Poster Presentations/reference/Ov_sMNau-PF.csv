title,year,conference
 A simple but tough-to-beat baseline for sentenceembeddings,2016, 2016
 Representation learning: A revieW and neWperspectives,2013, IEEE Trans
 ELECTRA: Pre-training text encoders as discriminators rather than generators,2020, In ICLR
 Un-supervised cross-lingual representation learning at scale,2020, In Dan Jurafsky
 Declutr: Deep contrastive learning forunsupervised textual representations,2020, arXiv preprint arXiv:2006
 Learning distributed representations of sentencesfrom unlabelled data,2016, In Proceedings of the 2016 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Learning distributed representations of sentencesfrom unlabelled data,2016, arXiv preprint arXiv:1602
 Skip-thought vectors,2015, In C
 An efficient framework for learning sentence repre-sentations,2018, In International Conference on Learning Representations
 Attention is all you need,2017, In I
 Sbert-wk: A sentence embedding method by dissecting bert-basedword models,2020, arXiv preprint arXiv:2002
 Towards universal paraphrasticsentence embeddings,2015, arXiv preprint arXiv:1511
 Huggingfaceâ€™s trans-formers: State-of-the-art natural language processing,2019, ArXiv
 Xlnet: Generalized autoregressive pretraining for language understand-ing,2019, In H
 How transferable are features in deepneural networks? In Z,2014, Ghahramani
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In The IEEE International Conference on Computer Vision (ICCV)
