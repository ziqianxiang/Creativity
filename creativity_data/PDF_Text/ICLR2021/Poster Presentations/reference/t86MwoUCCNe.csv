title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In Advances inNeUral Information Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in NeuralInformation Processing Systems
 Demystifying parallel and distributed deep learning: Anin-depth concurrency analysis,2019, ACM Computing Surveys (CSUR)
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedingsof COMPSTATâ€™2010
 Communication quan-tization for data-parallel training of deep neural networks,2016, In 2016 2nd Workshop onMachine Learning in HPC Environments (MLHPC)
 vqsgd: Vector quantizedstochastic gradient descent,2019, arXiv preprint arXiv:1911
 Lattice quantization,1988, In Advances in Electronicsand Electron Physics
 Advances and open problems in federated learning,2019, arXiv preprintarXiv:1912
 Moniqua: Modulo quantized communication in decen-tralized SGD,2020, In Proc
 Ratq: A universal fixed-length quantizer forstochastic optimization,2020, In Proceedings of the 23rd International Conference on ArtificialIntel ligence and Statistics (AISTATS)
 Gesammelte Abhand lungen,1911, Teubner
 Distributedlearning with compressed gradient differences,2019, arXiv preprint arXiv:1901
 Nuqsgd: Improved commu-nication efficiency for data-parallel sgd via nonuniform quantization,2019, arXiv preprintarXiv:1908
 1-bit stochastic gradient descentand application to data-parallel distributed training of speech dnns,2014, In Interspeech 2014
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 Atomo: Communication-efficient learning via atomic sparsification,2018, InAdvances in Neural Information Processing Systems
 Gradient sparsification forcommunication-efficient distributed optimization,2018, In Advances in Neural InformationProcessing Systems
