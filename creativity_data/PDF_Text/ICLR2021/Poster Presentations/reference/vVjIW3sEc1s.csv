title,year,conference
 A latent variable modelapproach to pmi-based word embeddings,2016, Transactions of the Association for ComputationalLinguistics
 A simple but tough-to-beat baseline for sentenceembeddings,2017, In Proceedings of the International Conference on Learning Representations
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Notes on noise contrastive estimation and negative sampling,2014, arXiv preprintarXiv:1410
 Distributional structure,1954, Word
 Mining and summarizing customer reviews,2004, In Proceedings of the TenthACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Skip-thought vectors,2015, In Advances in neural information processing systems
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Predicting what you already know helps:provable self-supervised learning,2020, arXiv preprint arXiv:2008
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 An efficient framework for learning sentence representa-tions,2018, In Proceedings of the International Conference on Learning Representations
 Noise contrastive estimation and negative sampling for conditionalmodels: Consistency and statistical efficiency,2018, In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the ACL: Human Language Technologies
 Inferring networks of substitutable and com-plementary products,2015, CoRR
 Learned in translation:Contextualized word vectors,2017, In Advances in Neural Information Processing Systems
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Efficient estimation of word represen-tations in vector space,2013, arXiv preprint arXiv:1301
 All-but-the-top: Simple and effective postprocessing for wordrepresentations,2018, In Proceedings of the International Conference on Learning Representations
 Unsupervised learning of sentence embed-dings using compositional n-gram features,2018, Proceedings of the North American Chapter of theACL: Human Language Technologies
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Zero-shot text classification with generative language models,2019, arXivprepring arXiv:1912
 Learning to generate reviews and discoveringsentiment,2017, arXiv preprint arXiv:1704
 Improving languageunderstanding by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Unsupervised pretraining for sequence to sequencelearning,2017, In Proceedings of the 2017 Conference on Empirical Methods in Natural LanguageProcessing
 It's not just size that matters: Small language models are alsofew-shot learners,2020, arXiv preprint arXiv:2009
 olmpics-on what language modelpre-training captures,2019, arXiv preprint arXiv:1912
 Contrastive estimation reveals topicposterior information to linear models,2020, arXiv preprint arXiv:2003
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Understanding contrastive representation learning through align-ment and uniformity on the hypersphere,2020, arXiv preprint arXiv:2005
 HUggingface's trans-formers: State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Character-level convolutional networks for text clas-sification,2015, In Advances in Neural Information Processing Systems 28
 Words W and w0 are substitutableif they have identical conditional probabilities for every context s ∈ S and thus can replace occur-rences of each other while still providing meaningful completions,1954, By definition
 This completes the proof,2021,	□28Published as a conference paper at ICLR 2021E
