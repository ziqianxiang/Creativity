title,year,conference
 Synthetic and natural noise both break neural machine trans-lation,2018, ArXiv
 A large anno-tated corpus for learning natural language inference,2015, ArXiv
 Don’t take the easy way out: Ensemblebased methods for avoiding known dataset biases,2019, ArXiv
 Bornagain neural networks,2018, In ICML
 Pretrainedtransformers improve out-of-distribution robustness,2020, In ACL
 Training products of experts by minimizing contrastive divergence,2002, NeUraIComputation
 Distilling the knowledge in a neural network,2015, ArXiv
 How much reading does reading comprehensionrequire? a critical investigation of popular benchmarks,2018, In EMNLP
 End-to-end bias mitigation bymodelling biases in corpora,2020, In ACL
 Berts of a feather do not generalize together:Large variability in generalization across models with similar test set performance,2019, ArXiv
 Right for the wrong reasons: Diagnosing syntacticheuristics in natural language inference,2019, ArXiv
 Syntactic data augmenta-tion increases robustness to inference heuristics,2020, In ACL
 Most ”babies” are ”little” and most ”problems” are ”huge”:Compositional entailment in adjective-nouns,2016, In ACL
 Framenet+: Fast paraphrastic tripling of framenet,2015, In ACL
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, ArXiv
 Semantic proto-roles,2015, Transactions of the Association for ComputationalLingUiStics
 Winogrande: An adver-sarial winograd schema challenge at scale,2020, In AAAI
 Bidirectional attentionflow for machine comprehension,2017, ArXiv
 Training region-based object detectors withonline hard example mining,2016, 2016IEEE COnferenCe on COmPUter ViSiOn and Pattern ReCOgnitiOn(CVPR)
 Dataset cartography: Mapping and diagnosing datasets withtraining dynamics,2020, 2020
 Fever: a large-scaledataset for fact extraction and verification,2018, In NAACL
 RobUstness may be at oddswith accuracy,2019, arXiv: Machine Learning
 Performance impact caUsed by hidden bias of training data for recognizing textUalentailment,2018, ArXiv
 Well-read stUdents learn better:On the importance of pre-training compact models,2019, arXiv: CompUtation and Language
 Are all training examples created eqUal? an empiricalstudy,2018, ArXiv
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, ArXiv
 Making neural qa as simple as possible but notsimpler,2017, In CoNLL
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proceedings of the 2018 Conference of theNorth American ChaPter of the Association for ComPUtational LingUistics: HUman LangUageTechnologies
 Huggingface’s trans-formers: State-of-the-art natural language processing,2019, ArXiv
 Robust naturallanguage inference models with example forgetting,2019, ArXiv
 Learning and evaluatinggeneral linguistic intelligence,2019, ArXiv
 Gender bias in corefer-ence resolution: Evaluation and debiasing methods,2018, ArXiv
