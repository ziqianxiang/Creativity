title,year,conference
 The sample complexity of pattern classification With neural netWorks: the size of theWeights is more important than the size of the netWork,1998, IEEE Transactions on Information Theory
 Spectrally-normalized margin bounds forneural netWorks,2017, In Advances in Neural Information Processing Systems
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 The lottery tickethypothesis at scale,2019, arXiv preprint arXiv:1903
 Disentangling feature and lazylearning in deep neural networks: an empirical study,2019, ArXiv
 Understanding the difficulty of training deep feedforwardneural networks,2010, In Yee Whye Teh and Mike Titterington (eds
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, 2015 IEEE International Conference onComputer Vision (ICCV)
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, 2015 IEEE International Conference onComputer Vision (ICCV)
 Deep residual learning for imagerecognition,2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Densely connectedconvolutional networks,2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, In Proceedings of the IEEE
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In H
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 The role ofover-parametrization in generalization of neural networks,2019, In International Conference on LearningRepresentations
 Random features for large-scale kernel machines,2008, In Advances inneural information processing systems
 Theimpact of neural network overparameterization on gradient confusion and stochastic gradientdescent,2019, ArXiv
 Implicit neural representations with periodic activation functions,2020, In arXiv
 Chervonenkis: On the uniform convergence of relative frequencies of events totheir probabilities,1971, 1971
 The marginalvalue of adaptive gradient methods in machine learning,2017, In I
 Disentangling trainability and general-ization in deep learning,2019, ArXiv
 Understandingdeep learning requires rethinking generalization,2016, CoRR
 Gradient descent optimizes over-parameterized deep relu networks,2019, Machine Learning
 The details of the ConvNet architecture are included inappendix H,2010, For any layer that doesnâ€™t involve changing the initialization scale
