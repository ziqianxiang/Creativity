title,year,conference
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Enhanced LSTMfor natural language inference,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics
 Exploring the role of loss functions in multiclassclassification,2020, In 2020 54th Annual Conference on Information Sciences and Systems (CISS)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In international conference on machine learning
 Deep Learning,2016, MIT Press
 Pairwise word interaction modeling with deep neural networks for semanticsimilarity measurement,2016, In Proceedings of the 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
 On loss functions for deep neural networks inclassification,2017, arXiv preprint arXiv:1702
 The implicit bias of gradient descent on nonseparable data,2019, InConference on Learning Theory
 Joint ctc-attention based end-to-end speech recog-nition using multi-task learning,2017, In 2017 IEEE international conference on acoustics
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Librispeech: an asr corpusbased on public domain audio books,2015, In 2015 IEEE International Conference on Acoustics
 Back to the future: Radial basis function networks revisited,2016, InArthur Gretton and Christian C
 Everything old is new again: a fresh look at historical approaches in machinelearning,2002, PhD thesis
 Convergence analysis of two loss functions in soft-max re-gression,2015, IEEE Transactions on Signal Processing
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Huggingfaceâ€™s trans-formers: State-of-the-art natural language processing,2019, ArXiv
 Distance-based learning from errors forconfidence calibration,2019, arXiv preprint arXiv:1912
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Dive into Deep Learning,2020, 2020
