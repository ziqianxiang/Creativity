title,year,conference
 The fifth pascal recognizingtextual entailment challenge,2009, In TAC
 Multitask learning,1997, Machine learning
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 The pascal recognising textual entailmentchallenge,2005, In Machine Learning Challenges Workshop
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Using noise to compute error surfaces in connectionist networks:A novel means of reducing catastrophic forgetting,2002, Neural computation
 A joint many-taskmodel: Growing a neural network for multiple nlp tasks,2016, arXiv preprint arXiv:1611
 One model to learn them all,2017, arXiv preprint arXiv:1706
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Fully character-level neural machine translationwithout explicit segmentation,2017, Transactions of the Association for Computational Linguistics
 Multi-task deep neural networksfor natural language understanding,2019, arXiv preprint arXiv:1901
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Snr: Sub-network routingfor flexible parameter sharing in multi-task learning,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Why there are complementarylearning systems in the hippocampus and neocortex: insights from the successes and failures ofconnectionist models of learning and memory,1995, Psychological review
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, In Psychology of learning and motivation
 Mad-x: An adapter-based frame-work for multi-task cross-lingual transfer,2020, arXiv preprint arXiv:2005
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Choice of plausible alternatives:An evaluation of commonsense causal reasoning,2011, In 2011 AAAI Spring Symposium Series
 An overview of multi-task learning in deep neural networks,2017, arXiv preprintarXiv:1706
 Get to the point: Summarization with pointer-generator networks,2017, arXiv preprint arXiv:1704
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In Advances in Neural Information Processing Systems
 Bert and pals: Projected attention layers for efficient adap-tation in multi-task learning,2019, arXiv preprint arXiv:1902
 Continuallearning with hypernetworks,2019, arXiv preprint arXiv:1906
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In Advances in Neural Information Processing Systems
 Understanding and improving information trans-fer in multi-task learning,2020, arXiv preprint arXiv:2005
 Oracle: Order robust adaptivecontinual learning,2019, arXiv preprint arXiv:1902
