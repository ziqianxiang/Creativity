title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Proceedings of the 35th International Conference onMachine Learning
 Layer normalization,2016, arXiv
 Limit of the smallest eigenvalue of a large dimensional sample covariancematrix,1993, TheAnnalsofProbabilily
 Deep rewiring: Trainingvery sparse deep networks,2018, In Proceedings of the 6th International Conference on LearningRepresentations
 Exact natural gradient in deep linearnetworks and application to the nonlinear case,2018, In Advances in Neural Information ProcessingSystems
 Nonconvex low-rank tensor completionfrom noisy data,2019, In Advances in Neural Information Processing Systems
 DO-Conv: Depthwise over-parameterized convolutional layer,2020, arXiv
 Report onthe 11th IWSLT evaluation campaign,2014, In Proceedings of the International Workshop on SpokenLanguage Translation
 Nonconvex optimization meets low-rank matrix factoriza-tion,2019, IEEE Transactions on Signal Processing
 ImageNet: A large-scalehierarchical image database,2009, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Predictingparameters in deep learning,2013, In Advances in Neural Information Processing Systems
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Convolutionalsequence to sequence learning,2017, In Proceedings of the 35th International Conference on MachineLearning
 Separable layers enable structured efficient linearsubstitutions,2019, arXiv
 Implicit bias of gradient descenton linear convolutional networks,2019, In Advances in Neural Information Processing Systems
 ExpandNets: Linear over-parameterizationto train compact convolutional networks,2020, arXiv
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Norm matters: Efficient and accuratenormalization schemes in deep networks,2018, In Advances in Neural Information Processing Systems
 Low-rank compression of neural nets: Learningthe rank of each layer,2020, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the 32nd International Conference on MachineLearning
 Paraphrasing complex network: Network compres-sion via factor transfer,2018, In Advances in Neural Information Processing Systems
 Factorized higher-order CNNs with an application to spatio-temporal emotion estimation,2020, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems
 SNIP: Single-shot network pruningbased on connection sensitivity,2018, In Proceedings of the 6th International Conference on LearningRepresentations
 All you need is a good init,2016, In Proceedings of the 4th InternationalConference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature Communications
 ACDC: A structuredefficient linear layer,2015, arXiv
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In Proceedings of the 36th International Conference onMachine Learning
 Machine Learning: A Probabilistic Perspective,2012, MIT Press
 Compressing deepneural networks using a rank-constrained topology,2015, In Proceedings of the 16th Annual Conferenceof the International Speech Communication Association
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2014, In Proceedings of the 2nd International Conferenceon Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2015, In Proceedings of the 3rd International Conference on Learning Representations
 Regularization of neuralnetworks using DropConnect,2013, In Proceedings of the 30th International Conference on MachineLearning
 EigenDamage: Structured pruningin the Kronecker-factored eigenbasis,2019, In Proceedings of the 36th International Conference onMachine Learning
 Feature normalized knowledge distillation for imageclassification,2020, In Proceedings of the European Conference on Computer Vision
 Snapshot distillation: Teacher-studentoptimization in one generation,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Mean field residual networks: On the edge of chaos,2017, InAdvances in Neural Information Processing Systems
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2020, In Proceedings of the 8th International Conferenceon Learning Representations
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference
 MLPrune: Multi-layer pruning for automated neural networkcompression,2019, OpenReview
