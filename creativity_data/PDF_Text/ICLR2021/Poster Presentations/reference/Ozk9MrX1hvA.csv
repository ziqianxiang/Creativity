title,year,conference
 Better fine-tuning by reducing representational collapse,2020, arXiv preprint arXiv:2008
 Learning with pseudo-ensembles,2014, In Advancesin neural information processing systems
 Mixmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 MixText: Linguistically-informed interpolation of hid-den space for semi-supervised text classification,2020, In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Advaug: Robust adversarialaugmentation for neural machine translation,2020, In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics
 Autoaugment: Learningaugmentation policies from data,2018, ArXiv
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Affinity and diversity:Quantifying mechanisms of data augmentation,2020, arXiv preprint arXiv:2002
 Explaining and harnessing adversarialexamples,2015, 2015
 Augmenting data with mixup for sentence classifi-cation: An empirical study,2019, arXiv preprint arXiv:1905
 Data-efficient image recognition with contrastive predictive coding,2019, arXivpreprint arXiv:1905
 Augmix: A simple data processing method to improve robustness and uncertainty,2020, In8th International Conference on Learning Representations
 Learning deep representations by mutual information estimationand maximization,2019, In 7th International Conference on Learning Representations
 Adversarial logit pairing,2018, arXiv preprintarXiv:1803
 Supervised contrastive learning,2020, arXiv preprintarXiv:2004
 Adversarial training for large neural language models,2020, arXiv preprint arXiv:2004
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Semi-supervised learning with ladder networks,2015, In Advances in neural information processing systems
 Improving neural machine translation mod-els with monolingual data,2016, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, arXiv preprint arXiv:2001
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, EMNLP 2018
 Conditional bert contextualaugmentation,2019, In International Conference on Computational Science
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Unsupervised dataaugmentation for consistency training,2019, arXiv preprint arXiv:1904
 Qanet: Combining local convolution with global self-attention for reading compre-hension,2018, In 6th International Conference on Learning Representations
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Improving the robustness of deepneural networks via stability training,2016, In Proceedings of the ieee conference on computer visionand pattern recognition
 Freelb: Enhancedadversarial training for language understanding,2020, In 8th International Conference on LearningRepresentations
