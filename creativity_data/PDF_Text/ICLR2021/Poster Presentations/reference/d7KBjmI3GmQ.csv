title,year,conference
 From ’f’to ’a’ on the n,2019,y
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2019, ArXiv
 On calibration of modern neural networks,2017, ICML
 Deep anomaly detection with outlier exposure,2019, ICLR
 Natural adversarial examples,2019, ArXiv
 Albert: A lite bert forself-supervised learning of language representations,2020, ArXiv
 Roberta: A robustly optimized bert pretraining approach,2019, ArXiv
 Language models are unsupervisedmultitask learners,2019, 2019
 MCTest: A challenge dataset for the open-domainmachine comprehension of text,2013, In Proceedings of the 2013 Conference on Empirical Methods inNatural Language Processing
 A survey of evaluation metrics used for nlgsystems,2020, 2020
 Computing machinery and intelligence,1950, 1950
