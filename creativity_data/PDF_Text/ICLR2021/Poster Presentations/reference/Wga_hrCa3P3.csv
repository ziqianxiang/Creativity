title,year,conference
 Better fine-tuning by reducing representational collapse,2020, arXiv preprint arXiv:2008
 Neural machine translation by jointlylearning to align and translate,2015, International Conference on Learning Representations
 Language gans falling short,2019, In International Conference on Learning Representations
 A simple framework forcontrastive learning of visual representations,2020, International Conference on Machine Learning
 Cross-lingual language model pretraining,2019, In Advances inNeural Information Processing Systems
 Unified language model pre-training for natural language understandingand generation,2019, Advances in Neural Information Processing Systems
 Explaining and harnessing adversarialexamples,2015, International Conference on Learning Representations
 Large margin neural language model,2018, EmpiricalMethods in Natural Language Processing
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2020, ACL
 Generatingdiverse and consistent QA pairs from contexts with information-maximizing hierarchical condi-tional vaes,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Manual and automatic evaluation of summaries,2002, ACL Workshopon Automatic Summarization
 An efficient framework for learning sentence representa-tions,2018, International Conference on Learning Representations
 Generation and comprehension of unambiguous object descriptions,2016, 2016 IEEE Conferenceon Computer Vision and Pattern Recognition
 Distributed representa-tions of words and phrases and their compositionality,2013, Advances in neural information processingsystems
 Adversarial training methods for semi-supervised text classification,2017, International Conference on Learning Representations
 Ssmba: Self-supervised manifold based dataaugmentation for improving out-of-domain robustness,2020, Empirical Methods in Natural LanguageProcessing
 Bleu: a method for automaticevaluation of machine translation,2002, Proceedings of the 40th Annual Meeting of the Association forComputational Linguistics
 A deep reinforced model for abstractivesummarization,2018, International Conference on Learning Representations
 A call for clarity in reporting bleu scores,2018, Proceedings of the Third Conference onMachine Translation: Research Papers
 Sequence level train-ing with recurrent neural networks,2016, nternational Conference on Learning Representations
 Facenet: A unified embedding for facerecognition and clustering,2015, Proceedings of the IEEE conference on computer vision and patternrecognition
 Get to the point: Summarization with pointer-generator networks,2017, Annual Meeting of the Association for Computational Linguistics
 Relevance of unsupervisedmetrics in task-oriented dialogue for evaluating natural language generation,2017, CoRR
 Context-aware captions from context-agnostic supervision,2017, IEEE Conference on Computer Vision andPattern Recognition
 Huggingfaceâ€™s transformers: State-of-the-art natural language processing,2019, ArXiv
 Reducing word omission errors inneural machine translation: A contrastive learning approach,2019, Annual Meeting of the Associationfor Computational Linguistics
 Pegasus: Pre-training with extractedgap-sentences for abstractive summarization,2020, ICML
 Freelb: Enhancedadversarial training for natural language understanding,2019, In International Conference on LearningRepresentations
