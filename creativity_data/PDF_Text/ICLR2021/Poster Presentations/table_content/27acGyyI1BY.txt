Table 1: Final MSE Loss on 1D regression tasks with standard error (lower is better). Bold indicatesthat the model performance is within error at the 95% confidence level, with Underline indicating thebest estimate for top-performer. NPs perform similarly to NDPs and their variants on the exponentialand oscillator tasks, where y-valUes are close to zero. For the sine and linear tasks, where y valUesvary significantly over the time range, NPs perform worse than NDPs and their variants.
Table 2: Table of ratios, of Neural ODE Process and Neural Process training times on different 1Dsynthetic datasets. We see that NDP/NP is the lowest (i.e. fastest) in each case.
Table 3: Final MSE values for NDPs training on the sine dataset with different sized ODEs, with NPperformance included for reference. Peak performance is found when dim(l) = 2, which is to beexpected as the true dynamics are 2-dimensional. For dim(l) = 1, the MSE is highest, and withinerror of the NP. Performance degrades with increasing dim(l), with overfitting becoming a problemfor dim(l) = 20.
Table 4: Task details for 1D regression. a and b are sampled uniformly at random from the givenranges. t is sampled at 100 regularly spaced intervals over the given range. 490 training examplesand 10 test examples were used in every case.
