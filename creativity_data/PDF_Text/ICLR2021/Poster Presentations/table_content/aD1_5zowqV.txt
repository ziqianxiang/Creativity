Table 1: FID on CIFAR-10x+e X X	x+e X XairplanebirdcarcatdeerdogfroghorseshiptruckModels	FIDVAE (Kingma & Welling, 2014)	79.09DCGAN (Radford et al., 2016)	32.71NCSNv2 (Song & Ermon, 2020)t	28.90Short-run EBM (Nijkamp et al., 2019)	23.02WGAN-GP (Gulrajani et al., 2017)*	21.40mr-Langevin (Block et al., 2020)t	19.54CF-EBM	14.35
Table 2: FID on CelebA	Figure 6: Image inpaintingQuantitative Results: The quantitative results for unconditional image generation on CIFAR-10and CelebA are given in Table 1 and Table 2. In Table 2, the result marked with f is experimented7Published as a conference paper at ICLR 2021on the center-cropped (140 × 140) and resized images of CelebA whereas others are trained on theresized CelebA. In all cases, the CF-EBM outperforms other methods by a wide margin. In Table 3,we compare our method with some representative open-sourced EBMs and the score-based modelin terms of the number of parameters and the relative computational time. All of the methods relyon Langevin dynamics for sampling. We use 50 Langevin steps for sampling while all other threemethods require 60, 100 and even more steps, respectively. We observe that our model consistentlystands out in terms of memory and computation efficiency while keeping the lowest FID on CIFAR-10.
Table 3: Comparison of efficiencythe denoised results x, which are obtained by LangeVin dynamics initialized with X + e, on all classesof CIFAR-10, implying a good generalization ability. (ii) Image inpainting: We mask 25% areas ofeach image, and use the Langevin dynamics of the learned EBM to recover the missing areas, whichcan be viewed as associative memory (Hopfield, 1982). As shown in Figure 6, each recovered imageexhibits a meaningful but different recovered pattern than the ground truth. We also provide diverseinpainting results in Figure 11 of Appendix A.4, indicating a good mode coverage. We also providenearest neighbors retrieved from training data for the generated images in Figure 12 of Appendix A.4to show that our model doesn’t memorize the training images but synthesizes novel ones.
Table 4: Comparison of AUROC scores in OOD detection. The higher the score, the better theperformance. The other three models are all conditional generative models except for our CF-EBM.
Table 5: Quantitative results of different methods on different image translation datasetsQualitative Results: As can be seen in Figure 7, our model can generate sharper images withsuperior visual quality than the baselines. It better preserves the source content while evolving thestyle from the source domain to the target domain. We put the comparison between our model andone-sided translation model CUT (Park et al., 2020) in Appendix A.9.3. More translation results onphoto — Vangogh and Yosemite summer — winter can also be found in Appendix A.9.4.
Table 6: The neural network architecture for energy function at resolution 256 X 256. c is the channelmultiplier and we set c = 32 for all experiments.
Table 7: FID on ImageNet-1k (32 × 32). The channel multiplier is ch = 48.
Table 8: Effects of different activation functionsFigure 14: Visualization of different activation functions and their derivativesA.6.2 NormalizationSince the objective in Eq. (2) is similar to that in Wasserstein GAN (Arjovsky et al., 2017), andthe bottom-up energy function in the EBM acts as a discriminator, we thus naturally considerspectral normalization (Miyato et al., 2018) to improve the performance and stability. The spectralnormalization constrains the Lipschitz constant of the learnable neural network parameters, which iswidely used to stabilize the training of the discriminator network of GAN.
Table 9: Ablation study on CIFAR10.
Table 10: Log-likelihood in Nats on Continuous MNIST.
Table 11: FID for image generation on CelebA-HQ (128 × 128). Results of GAN-based approachesare taken from Zhang et al. (2020).
