Table 1: Overview of the number of parameters in (coupled) embedding matrices of state-of-the-artmultilingual (top) and monolingual (bottom) models with regard to overall parameter budget. |V |:vocabulary size. N, Nemb: number of parameters in total and in the embedding matrix respectively.
Table 2: Effect of decoupling the input and output embedding matrices on performance on multipletasks in xtreme. PT: Pre-training. FT: Fine-tuning. The decoupled model has input and outputembeddings with the same size (E = 768) as the embedding of the coupled model. The Transformerparts of the models are the same (i.e., 12 layers with H = 768).
Table 3: Performance of models with a large input and small output embedding size and vice versa.
Table 4: Effect of an increased output embedding size Eout on tasks in XTREME. All three modelshave Ein = 128 and 12 Transformer layers with H = 768.
Table 5: Effect of additional capacity via more Transformer layers during pre-training. Both modelshave Ein = 128. The Eout = 768 model has a larger output embedding size Eout and 12 Transformerlayers. In contrast, the model with 11 additional Transformer layers has Eout = 128. Those addi-tional layers are dropped after pre-training, leaving 12 layers for fair comparison during fine-tuning.
Table 6: Effect of reinvesting the input embedding parameters to increase the hidden dimension Hand number of Transformer layers L on XTREME tasks. Ein = 128, Eout = 768, H = 768 for allmodels except for the baseline, which has coupled embeddings and Ein = Eout = 768.
Table 7: Comparison of our model to other models on the xtreme leaderboard. Details aboutVECO are due to communication with the authors.
Table 8: Results on word embedding association tests for the input (I) and output (O) embeddingsof models (left) and the modelsâ€™ masked language modeling performance (right). The first tworows show the performance of coupled and decoupled embeddings with the same embedding sizeEin = Eout = 768. The last three rows show the performance as we increase the output embeddingsize with Ein = 128.
Table 9: Probing analysis of Tenney et al. (2019) with mix strategy.
Table 10: Fine-tuning hyperparameters for all models except RemBERT.
Table 11: Statistics for the datasets in xtreme, including the number of training, development, andtest examples as well as the number of languages for each task.
Table 12: Effect of reducing the embedding size E for monolingual vs. multilingual models onMNLI and XNLI performance respectively. Monolingual numbers are from Lan et al. (2020) andhave vocabulary size of 30k.
Table 13: Effect of an increased output embedding size Eout and additional layers during pre-trainingL = 15 on English BERTBase (Ein = 128).
Table 14: Hyperparameters for RemBERT architecture and pre-training.
Table 15: Hyperparameters for RemBERT fine-tuning.
Table 16: Comparison of our model to other models on the xtreme leaderboard. Details aboutVECO are due to communication with the authors. Avgtask is averaged over tasks whereas Avg isaveraged over task categories just like Table 7.
