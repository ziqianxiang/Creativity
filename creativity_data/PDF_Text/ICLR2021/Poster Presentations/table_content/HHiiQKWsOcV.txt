Table 1: Accuracy of BERT trained on SNLI (DeYoung et al., 2020) as noise is injected on humanidentified rationales/non-rationales. RP and RH are Revised Premise and Revised Hypothesis testsets in Kaushik et al. (2020). MNLI-M and MNLI-MM are MNLI (Williams et al., 2018) dev sets.
Table 2: Out-of-domain accuracy of models trained on original only, CAD, and original andsentiment-flipped reviewsTraining data	SVM	NB	BiLSTM (SA)	BERTAccuracy on	Amazon Reviews			CAD (3.4k)	79.3	78.6	71.4	83.3Orig. & Hu et al. (2017)	66.4	71.8	62.6	78.4Orig. & Li et al. (2018)	62.9	65.4	57.6	61.8Orig. & Sudhakar et al. (2019)	64.0	69.3	54.7	77.2Orig. & Madaan et al. (2020)	74.3	73.0	63.8	71.3Orig. (3.4k)	74.5	74.3	68.9	80.0Accuracy on Semeval 2017 (Twitter)CAD (3.4k)	66.8	72.4	58.2	82.8Orig. & Hu et al. (2017)	60.9	63.4	56.6	79.2Orig. & Li et al. (2018)	57.6	60.8	54.7	62.7Orig. & Sudhakar et al. (2019)	59.4	62.6	54.9	72.5Orig. & Madaan et al. (2020)	62.8	63.6	54.6	79.3Orig. (3.4k)	63.1	63.7	50.7	72.6Accuracy on Yelp ReviewsCAD (3.4k)	85.6	86.3	73.7	86.6Orig. & Hu et al. (2017)	77.4	80.4	68.8	84.7
Table 3: Accuracy of various sentiment analysis classifiers trained on 1.7k original reviews fromKaushik et al. (2020) as noise is injected on rationales/non-rationales identified via human feedback.
Table 4: Accuracy of various sentiment analysis classifiers trained on 1.7k original reviews fromKaushik et al. (2020) as noise is injected on rationales/non-rationales identified via Attention masks.
Table 5: Accuracy of various sentiment analysis classifiers trained on 1.7k original reviews fromKaushik et al. (2020) as noise is injected on rationales/non-rationales identified via Allen NLPSaliency Interpreter.
Table 6: Accuracy of various sentiment analysis classifiers trained on reviews from Zaidan et al.
Table 7: Accuracy of various sentiment analysis classifiers trained on reviews from Zaidan et al.
Table 8: Accuracy of various sentiment analysis classifiers trained on reviews from Zaidan et al.
Table 9: Accuracy of various models for sentiment analysis trained with various datasets. O refersto the in-sample test set from Kaushik et al. (2020) whereas R refers to the counterfactually revisedcounterparts of the same.
Table 10: Accuracy of BERT trained on subsample of SNLI (DeYoung et al., 2020) (where numberof rationale tokens and non rationale tokens are within 30% of one another) as noise is injected onhuman identified rationales/non-rationales. RP and RH are Revised Premise and Revised Hypoth-esis test sets in Kaushik et al. (2020). MNLI-M and MNLI-MM are MNLI (Williams et al., 2018)dev sets.
