Table 1: Hyperparameters of our experimentsHyperparameter	Value	CommentAmorpheus		-Learning rate	0.0001	-Gradient clipping	0.1	- Normalisation	LayerNorm	As an argument to TransformerEncoder in torch.nn- Attention layers	3	- Attention heads	2	- Attention hidden size	256	- Encoder output size	128	Training		- runs	3	per benchmarkAmorpheus makes use of gradient clipping and a smaller learning rate. We found, that SMP alsoperforms better with the decreased learning rate (0.0001) as well and we use it throughout the work.
Table 2: Full list of environments used in this work.
Table 3: Initial results on generalisation. The numbers show the average performance of three seedsevaluated on 100 rollouts and standard error of the mean. While the average values are higherfor Amorpheus on 5 out of 7 benchmarks, high variance of both methods might be indicative ofinstabilities in generalisation behaviour due to large differences between the training and testingtasks.
