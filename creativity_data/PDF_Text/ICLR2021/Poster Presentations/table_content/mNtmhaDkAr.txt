Table 1: Instantiations of the target feature It in our synthetic experiments. The spurious feature Isis always the presence of the symbol 2. Features are intended to differ in how hard they are for anLSTM to detect given sequential input (measured by MDL per §2.2, reported in k-bits).
Table 2: Examples of features used to generate fine-tuning sets with target/spurious features ofvarying extractability scores. Top examples show a case in which t and s both occur and the sentenceis acceptable, and bottom examples show a case in which s occurs without t and the sentence isunacceptable. Only s is highlighted since t is often defined over the structure of the sentence (seetext) and thus difficult to localize to a few tokens. Table 9 in the Appendix has neither examples.
Table 3: Summary of extractability (MDL in bits) for t and s for each template and each model.
Table 4: Spearman’s ρ: MDL vs. Average Test F-score: Correlations when Probing Accuracy is NotControlled.
Table 5: AUC. Each column presents the Spearman correlation between the given measure and thes-only rate at which the model first acquired the target feature. * indicates a significant correlation.
Table 6: Hyper and System Parameters. We use Hugging Face for the underlying model imple-mentations.
Table 7: We measure the target MDL directly on the toy data, where we can access the targetfeature. Recall that in natural conditions we can not generate examples with the target feature (freeof spurious features), so we used a dataset comprising both and s-only examples. In the simulatedsetting, our approach for measuring the target extractability indirectly by using both and s-onlyexamples reports results similar to those when directly using target and neither. These values are inkbits.
Table 8: List of templates that are used in Section 4. These are discussed in greater detail below.
Table 9: neither examples for Table 2.
Table 10: List of templates that are used in Section 4. These do not include the spurious featureswhich are discussed in Section 4.1. For NPL each N represents a noun phrase. Np-neg is validafter a negation (might contain a polarity item). Np is valid after a determiner (cannot contain anunlicensed polarity item). Np-ever is not valid after a determiner (contains an unlicensed polarityitem). These phrases have complex nesting behavior and can become arbitrary long. In addition,a sentence might consist of multiple independent clauses, each of which is given by one of thesetemplates. For SVA, the base templates do not have the additional nouns in the starred parentheticals,while the nested templates have zero or more. For GAP, the harder set of templates include ISL(island) examples as an additional s-only example that force the model to not violate (one specifictype) of island constraint. For complete details and lexicons see the source.
