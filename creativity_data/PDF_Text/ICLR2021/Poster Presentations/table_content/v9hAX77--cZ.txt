Table 1: Test accuracy (%) of different editors andedit encoders. See more ComParisons in § D.1.
Table 2: Edited programs C+ from each editor (w/ Seq Edit Encoder) given hf∆(C-0 , C+0 ), C-i onFixers. We show where our editor succeeds (Example 1) and fails (Example 2). More examples canbe found in § D.2.
Table 3: Test (dev) accuracy by % of Graph2Edit w/ SeqEdit Encoder on GHE-gold after one iteration of imita-tion learning. Examples (simplified) illustrate how thebase editor works when trained via supervised learningor with different imitation learning strategies. Colors in-dicate correct or incorrect edits.__________________________	Supervised	DAgger	PostRefinew/ 20% data	42.30 (44.22)	42.70 (45.03)	43.94 (46.10)w/ full data	54.49 (55.43)	53.85 (55.57)	54.91 (56.48)Example	... Add[Id->Token] Add["VAR1"] ...	... Add[Id->Token] Add["VAR1"] Delete["VAR1"] Add["StringX"]	... Add[Id->Token] Add["StringX"] ...
Table 4: A more comprehensive comparison with existing approaches. *: numbers copied fromoriginal paper; -: missing evaluation in the original paper; other numbers without indication are re-ported by us. For baselines re-tested by us, we use implementations kindly provided by their authorsand make sure that our reproduced version has comparable performance as what was reported intheir paper.
Table 5: Edited programs C+ from each editor (w/ Seq Edit Encoder) given hf∆(C-0 , C+0 ), C-i onFixers. We show where our editor succeeds in Example 1-2 and fails in Example 3-4.
Table 6: The nearest neighbors of given edit pairs based on their edit representations.
Table 7: Analysis of DAGGERSAMPLING algorithms with β being 0 and 0.5 on dev set. We analyzeeach algorithm’s behavior about “adding then deleting” the same token.
