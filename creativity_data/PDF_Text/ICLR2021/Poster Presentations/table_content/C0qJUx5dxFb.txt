Table 1: CIFAR-10, WRN 28-10. Mean ±std. over 5 seeds. Late-phase BatchNorm (LPBN).
Table 2: Mean CIFAR-100 test set accuracy (%) ± std. over 5 seeds, WRN 28-10. Different late-phase weight augmentations are compared to the base architecture and to an upper bound consistingof an ensemble of models. Deep ens. stands for deep ensemble, LPBN for late-phase BatchNorm.
Table 3: Additional architectures, CIFAR-10 (C10) andCIFAR-100 (C100). Mean test set acc. ± std. over 3seeds (%). Late-phase BatchNorm weights.
Table 4: CIFAR-100, WRN-28-10, uncertainty representation results. Mean ± std. over 5 seeds(except deep ensembles). This first group of methods yield a single model; the second group requirestest-time averaging over models while training as efficiently as K=1; the last group are full deepensembles which require training K=10 models from scratch (Deep ens.). We report in-distributiontest set acc. (%) and negative log-likelihood (NLL), and in-distribution vs. out-of-distribution (OOD)discrimination performance (average AUROC over four OOD datasets, see main text).
Table 5: Validation set acc. (%) on ImageNet. Mean ±std. over 5 seeds. BatchNorm late-phase and baselinetrained for 20 epochs with SGD.
Table 7: Specification of the hypernetwork used for each convolutional layer of the WRN, indexedby its depth in the network. A depth marked by * refers to the residual connection spanning acrossthe specified layers. The characteristics of each layer is described in the format input-channels ×[kernel-size] × output-channels under Conv-layer. Layers within the same group are generated bythe same hypernetwork. Each hypernetwork has a unique parameter tensor of shape Hnet-PS, which,when multiplied by a layer and weight embedding of shape Emb-PS and reshaped appropriately,generates the primary network parameter of shape Base-PS.
Table 8: CIFAR-100 test set accuracy (%)depending on different values of K for WRN28-10, SGD. Mean ± std. over 5 seeds.
Table 9: Applying late-phase weights to a pretrained WRN 28-10, CIFAR-100, SGD. Mean ±std. over 5 seeds.
Table 10: Gradient accumulation control, CIFAR-100, WRN 28-10, SGD. Mean ± std. over 5 seeds.
Table 11: CIFAR-100 test set accuracy (%) depending on different values of σ0 for WRN 28-10SGD with late-phase BatchNorm weights (LPBN). Mean ± std. over 5 seeds.
Table 13: Performance of a WRN 28-10 on CIFAR-100 with different dropout probability p. ForMC-dropout we average over 10 different samples. Mean ± std. over 5 seeds.
Table 12: CIFAR-10 and CIFAR-100 testset accuracy (%) depending on different latephase timing T0 for WRN 28-10, SGD. Mean± std. over 5 seeds.
Table 14: Final training set loss on CIFAR datasets, WRN 28-10, SGD. Mean ± std. over 5 seeds.
Table 15: Performance of models trained on a reduced CIFAR-10 training set and evaluated on thefull CIFAR-10 test set. Mean ± std. over 5 seeds.
Table 16: OOD performance measured by the AUROC, and robustness measured by the MeanCorruption Error (mCE). We train the models on CIFAR-100 and attempt to discriminate test setimages from novel ones drawn from the SVHN, LSUN, Tiny ImageNet (TIN) and CIFAR-10 dataset.
Table 17: Training time in seconds and hours on CIFAR-10 for 200 epochs on a single NVIDIAGeForce 2080 Ti GPU.
