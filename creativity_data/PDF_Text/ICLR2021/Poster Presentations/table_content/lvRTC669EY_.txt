Table 1: The stag-huntgame, a > b ≥ d > c.
Table 2: Results in the standard setting ofAgar.io. PBT: population training of parallelPG policies; RR: best policy in the RR phase(w=[1, 1]); RPG: fine-tuned policy; RND: PGwith RND bonus in the original game.
Table 3: Results in the aggressive setting of Agar.io: PBT: populationtraining of parallel PG policies; RR: w=[0, 0] is the best candidatevia RR; RPG: fine-tuned policy; RND: PG with RND bonus.
Table 4: Stats. of the adaptive agent in Monster-Huntwith hold-out test-time opponents. #C(oop.)-H(unt):both agents catch the monster; #S(ingle)-H(unt): theadaptive agent meets the monster alone; #Apple: ap-ple eating. The adaptive policy successfully exploitsdifferent opponents and rarely meets the monster alone.
Table 5: Adaptation test in Agar.io. Op-ponent type is switched half-way perepisode. #Attack, #Coop.: episodestatistics; Rew.: agent reward. Adaptiveagents’ rewards are close to oracles.
Table 6: PPO hyper-parameters used in Gridworld games, learning rate is linearly annealed duringtraining.
Table 7: PPO hyper-parameters used in Agar.ioPPO-GRU was trained with 128 parallel environment threads. Agar.io’s episode length was uniform-randomly sampled between 300 and 400 both when training and evaluating. Buffer data were splitto small chunks with length = 32 in order to diversify training data and stabilize training process.
Table 8: Frequencies of 4 types of events and rewards of different policies of Agar.io after completelytraining.
Table 9: Evaluation of different policy profiles obtained via RR in original Iterative Stag-Hunt. Notethat w = [4, 0, 0, 0] has the best performance among the policy profiles, and is the optimal NE withno further fine-tuning.
Table 10: Statistics of the adaptive policy in Iterative Stag-Hunt with 4 hand-designed opponents withdifferent behavior preferences. #Stag: the adaptive agent hunts the stag; #Hare: the adaptive agenteats the hare; The adaptive policy successfully exploits different opponents, including cooperatingwith TFT opponent, which is totally different from trained opponents.
