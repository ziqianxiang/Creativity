Table 1: Evaluation on word similarity datasets via Spearman’s rank correlation coefficient. BothGloVe and word2vec use 300d pretrained embeddings. Hyperparameter settings for our model:K = 400, w = 11. Results for our algorithm are reported only for a fixed hash length, k = 51. SeeTable 7 for results as a function of hash length.
Table 2: Comparison to common binarization methods. This table is a simplified version (for hashlength k = 4) of the complete evaluation for a range of hash lengths reported in Tables 7, 8, 9. Eachbinarization technique was evaluated on three continuous embeddings: pretrained GloVe, pretrainedword2vec, GloVe trained on OpenWebText (the same dataset that was used for training our fruitfly embeddings), format: pretrained GloVe/ pretrained word2vec/ GloVe on OWT. Hyperparametersettings for our model: K = 400, w = 11. Best result in bold; second best underlined.
Table 3: SCWS dataset: mean and std forSpearman rank correlation. The best windowvalue is also shown.
Table 4: WiC dataset: mean and std for accu-racy. The best window value is also shown.
Table 5: Accuracy for document classification task. We use 300d pretrained models for GloVe andword2vec, and pretrained bert-large-uncased model for BERT. For NLB, 300d GloVe embeddingswere binarized into 256 and 512 bits. For our model, hash length 30 is used. For fair comparison,all models use the same vocabulary of 20k words.
Table 6: Training time (per epoch) and memory footprint ofour method on GPUs and CPUs. For the GPU implemen-tation, three V100 GPUs interconnected with 100GB/s (bi-directional) NVLink were used. For the CPU implemen-tation, the computation was done on two 22-core CPUs.
Table 7: Evaluation on word similarity datasets. For each dataset and hash length, the best (sec-ond best) score is in bold (underlined). The performance for GloVe embeddings is reported nextto the name of each dataset in the format 300d/100d. Spearman’s rank correlation coefficient isreported for common baselines that binarize GloVe (300d) embeddings together with our results.
Table 8: Evaluation on word similarity datasets, analogous to Table 7, for 300d word2vec embed-dings.
Table 9: Evaluation on word similarity datasets, analogous to Table 7. The 300d GloVe embeddingstrained from scratch on the same OpenWebText dataset as our algorithm.
