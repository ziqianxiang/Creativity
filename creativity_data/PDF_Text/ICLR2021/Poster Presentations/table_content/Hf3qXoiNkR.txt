Table 1: Breakdown of the 1,000 top certain/ incorrect training examples.
Table 2: MNLI matched dev accuracies, HANS accuracies and MNLI matched hard test set. We repothe All numbers are averaged on 6 runs (with standard deviations). Detailed results for HANS aregiven Appendix A.4. Reported results are indicated with *.* Utama et al. (2020) is a concurrentwork where they use a BERT-base fine-tuned on 2000 random examples from MNLI as a weaklearner and “PoE + An.” refers to the annealing mechanism proposed by the authors. I and O arerespectively referring to in-distribution and out-of-distribution sets.
Table 3: F1 Scores on SQuAD and Adversarial QA. The AddOneSent set is model agnostic whilewe use the AddSent set obtained using an ensemble of BiDAF models (Seo et al., 2017). * arereported results. I and O are respectively referring to in-distribution and out-of-distribution sets.
Table 4: Accuracies on MNLI (matched) and HANS when trained on a subset MNLI. Trainingexamples are selected from the whole MNLI training set adversarially as detailed in the text. We useBERT-base as the main model and TinyBERT as the weak leaner. I and O are respectively referringto in-distribution and out-of-distribution sets.
Table 5: Accuracies on the FEVER dev set (Thorne et al., 2018) and symmetric hard test (Schusteret al., 2019).
Table 6: Weak learners are able to detect previously reported dataset biases without explicitly mod-eling them.
Table 7: HANS results per heuristic. All numbers are an average on 6 runs (with standard devia-tions). I and O are respectively referring to in-distribution and out-of-distribution sets.
Table 8: Accuracies on the MNLI dev mismatched and HARD test mismatched set. I and O arerespectively referring to in-distribution and out-of-distribution sets.
Table 9: Transfer accuracies on NLI benchmarks. We train BERT-base on SNLI and tested on thetarget test set. * are reported results.
