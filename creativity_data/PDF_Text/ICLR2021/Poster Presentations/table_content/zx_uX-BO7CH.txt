Table 1: Evaluation metrics on continual learning benchmarks considered. All methods use the samebackbone network and 50 memory slots per task, * denotes a dynamic architecture method that has aseparate network per taskMethod	PMNIST			CORe50			ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMa)	LA(↑)GEM	74.84±0.95	8.57±0.33	81.74±0.77	42.56±0.86	7.36±0.90	46.84±2.22AGEM	68.67±0.71	13.98±0.68	81.54±0.25	40.28±3.15	11.08±4.01	48.68±1.51MER	76.59±0.74	6.88±0.59	82.09±0.33	39.28±1.25	9.08±1.25	45.52±0.96ER-Ring	76.02±0.59	8.57±0.33	83.69±0.44	41.72±1.30	9.10±0.80	48.18±0.81MIR	76.58±0.10	8.34±0.11	83.57±0.07	43.50±1.92	6.14±0.91	45.98±1.14CTN (ours)	79.01±0.65	6.69±0.51	85.11±0.45	54.17±0.85	5.50±1.01	55.32±0.34Independent*	81.05±0.29	0.00	81.05±0.29	53.54±1.10	0.00	53.54±1.10Offline	84.95±0.95	-	-	58.69±0.41	-	-Method		Split CIFAR			Split miniIMN		ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMa)	LA(↑)GEM	57.77±0.86	10.93±1.03	66.45±0.06	55.04±1.88	7.81±1.70	60.13±1.36AGEM	58.27±0.86	8.76±0.67	66.12±1.17	51.14±2.16	6.99±1.96	55.11±0.76MER	61.32±0.86	11.90±0.86	72.51±0.41	57.94±1.08	8.98±0.79	66.11±0.76ER-Ring	61.36±1.01	7.20±0.72	67.05±1.08	53.43±1.18	11.21±1.35	63.46±1.05MIR	63.37±1.99	10.53±1.63	73.27±0.77	51.97±1.58	10.37±2.72	60.63±3.43
Table 2: Evaluation metrics on the Small Split CIFAR benchmarks, M denotes the memory per taskMethod	Reduced Split CIFAR 25%, M = 50			Reduced Split CIFAR 25%, M = 25			ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMa)	LA(↑)GEM	51.01±0.95	5.65±1.09	53.39±0.98	47.33±0.89	8.77±1.58	53.79±1.35ER-Ring	52.02±0.90	4.31±0.94	53.97±0.65	48.15±0.87	8.22±1.17	53.88±0.93MIR	50.82±0.83	5.22±0.68	53.27±1.05	47.19±0.54	8.41±0.94	53.51±0.74CTN	61.27±0.93	4.19±0.78	61.92±1.15	56.17±1.63	7.71±1.22	61.40±0.64Method	Reduced Split CIFAR 10%, M = 50			Reduced Split CIFAR 10%, M = 25			ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMa)	LA(↑)GEM	44.06±1.31	6.96±0.87	48.67±0.84	42.67±1.62	8.39±1.35	49.46±0.40ER-Ring	44.60±1.65	6.07±1.77	48.36±0.46	43.09±1.22	7.68±1.94	49.40±1.40MIR	46.63±0.56	4.38±0.45	48.35±0.52	44.12±0.94	6.84±1.05	48.48±0.76CTN	56.61±0.74	4.33±0.48	58.77±0.99	52.64±0.63	6.74±0.73	57.27±1.02Table 3: ACC(↑) of each component in CTN on Split CIFAR and Split mini Imagenet with 50 memoryslots per task. BC: behavioral cloning (Eq. 6), C: controller, BO: Bilevel optimization (Eq. 3)	BC C BO	Split CIFAR			Split miniIMN				ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMQ)	LA(↑)CTN	XXX	67.65±0.43	6.33±0.70	73.43±0.45	65.82±0.59	3.02±1.13	67.73±1.73	XX	66.37±0.53	9.64±0.98	75.40±0.60	60.04±1.37	10.48±0.99	69.87±0.60	XX	64.46±1.16	8.51±1.53	72.23±0.54	61.01±1.09	5.31±0.94	64.35±0.83
Table 3: ACC(↑) of each component in CTN on Split CIFAR and Split mini Imagenet with 50 memoryslots per task. BC: behavioral cloning (Eq. 6), C: controller, BO: Bilevel optimization (Eq. 3)	BC C BO	Split CIFAR			Split miniIMN				ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMQ)	LA(↑)CTN	XXX	67.65±0.43	6.33±0.70	73.43±0.45	65.82±0.59	3.02±1.13	67.73±1.73	XX	66.37±0.53	9.64±0.98	75.40±0.60	60.04±1.37	10.48±0.99	69.87±0.60	XX	64.46±1.16	8.51±1.53	72.23±0.54	61.01±1.09	5.31±0.94	64.35±0.83	X	62.76±0.49	10.10±0.78	72.12±0.41	58.95±1.76	9.08±1.61	66.94±0.83ER		61.36±1.01	7.20±0.72	67.05±1.08	53.43±1.18	11.21±1.35	63.46±1.054.3	Results on Learning with Limited Training DataOne important goal of continual learning is to be able to learn with a limited amount of training dataper task. This setting is much more challenging because it tests the learner’s ability to quickly acquireknowledge only with limited training samples by utilizing its past experiences. In this experiment,we explore how different memory-based methods perform with only limited training samples pertask and memory size. We consider the Split CIFAR benchmark; however, we reduce the amount oftraining data per task significantly. Particularly, we only consider 25% and 10% of the original dataper task while the test data remains the same. We name the new benchmarks Reduced Split CIFAR25% and Reduced Split CIFAR 10%, respectively. Notably, the Reduced Split CIFAR 10% only hasfive samples per class, which is extremely challenging. We compare CTN with GEM, ER, and MIRon these benchmarks with the memory size of 50 and 25 samples per task.
Table 4: Model complexity of CTN with various backbone architecturesBackbone		Controller		Total	IncreaseStructure	# Params	Structure	# Params		MLP [784-256-256-10]	269,322	Linear model	17,728	287,050	6.58%ResNet18 (Lopez-Paz & Ranzato, 2017)	1,095,555	Linear model	20,992	1,116,547	1.92%ResNet18 (He et al., 2016)	11,202,162	Linear model	59,200	11,261,362	0.53%Table 5: Averaged running time (in seconds) of compared methods on the task-aware continuallearning benchmarks. All methods use M=50 memory slots per task, Ring buffer, and up to fourgradient updates per samplesBenchmark \ Method	ER-Ring	MIR	AGEM	CTN	GEMpMNIST	61	92	90	110	103Split CIFAR100	632	1030	680	910	1700Split miniIMN	1320	2130	1700	1890	2850of these components and report the results in Table 3. Notably, CTN with only the controller (C)is equivalent to training the base network and the controller using the vanilla experience replayapproach. Despite this, the controller can offer significant improvements over ER: over 5% ACC(↑)in Split miniIMN. When the controller is optimized by our proposed bilevel optimization (C + BO),the performances are further improved, showing that our proposed bilevel objective achieves a bettertrade-off between alleviating forgetting and facilitating knowledge transfer. Lastly, the behavioralcloning strategy can help alleviate forgetting and further strengthen the results. Overall, each of the
Table 5: Averaged running time (in seconds) of compared methods on the task-aware continuallearning benchmarks. All methods use M=50 memory slots per task, Ring buffer, and up to fourgradient updates per samplesBenchmark \ Method	ER-Ring	MIR	AGEM	CTN	GEMpMNIST	61	92	90	110	103Split CIFAR100	632	1030	680	910	1700Split miniIMN	1320	2130	1700	1890	2850of these components and report the results in Table 3. Notably, CTN with only the controller (C)is equivalent to training the base network and the controller using the vanilla experience replayapproach. Despite this, the controller can offer significant improvements over ER: over 5% ACC(↑)in Split miniIMN. When the controller is optimized by our proposed bilevel optimization (C + BO),the performances are further improved, showing that our proposed bilevel objective achieves a bettertrade-off between alleviating forgetting and facilitating knowledge transfer. Lastly, the behavioralcloning strategy can help alleviate forgetting and further strengthen the results. Overall, each of theproposed components adds positive contributions to the base model, and they work collectively as aholistic method and achieved state-of-the-art results in continual learning.
Table 6: Summary of datasets used in our experimentsDataset	Classes	Train	Test	DimensionMNIST (LeCun et al., 1998)	10	1,000	10,00	28×28CIFAR100 (Krizhevsky & Hinton, 2009)	100	50,000	10,000	3×32×32miniIMN (Vinyals et al., 2016)	100	50,000	10,000	3×84×84CORe50 (Lomonaco & Maltoni, 2017)	50	119,894	44,971	3×84×84For each benchmark, we normalize the pixel values to [0, 1] by dividing their values by 255.0 as usedin Lopez-Paz & Ranzato (2017), no other data preprocessing steps are performed.
Table 7: Evaluation metrics on continual learning benchmarks considered. All methods use the samebackbone network for all benchmarks, episodic memory size is M=50 samples per taskMethod	PMNIST			CORe50			ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMQ)	LA(↑)Finetune	61.66±1.50	20.67±1.64	80.89±0.45	4.38±0.10	49.66±1.14	49.08±1.20LwF	63.31±3.56	14.29±3.05	75.76±1.43	31.20±0.66	20.44±1.37	49.20±1.10EWC	67.34±3.00	11.00±2.36	76.59±1.49	31.86±3.90	14.34±3.08	42.98±2.50GEM	74.84±0.95	8.57±0.33	81.74±0.77	42.56±0.86	7.36±0.90	46.84±2.22KDR	72.97±0.58	9.20±0.44	81.40±0.41	OOM	OOM	OOMAGEM	68.67±0.71	13.98±0.68	81.54±0.25	40.28±3.15	11.08±4.01	46.68±1.51MER	76.59±0.74	6.88±0.59	82.09±0.33	39.28±1.25	9.08±1.25	45.52±0.96ER-Ring	76.02±0.59	8.57±0.33	83.69±0.44	41.72±1.30	9.10±0.80	48.18±0.81MIR	76.58±0.10	8.34±0.11	83.57±0.07	43.50±1.92	6.14±0.91	45.98±1.14BCL	7.91±0.34	6.23±0.14	83.75±0.28	44.72±1.31	5.97±0.88	47.68±0.87CTN (ours)	79.01±0.65	6.69±0.51	85.11±0.45	54.17±0.85	5.50±1.01	55.32±0.34Independent*	81.05±0.29	0.00	81.05±0.29	53.54±1.10	0.00	53.54±1.10Offline	84.95±0.95	-	-	89.73±0.91	-	-Method		Split CIFAR		Split miniIMN			ACC(↑)	FMa)	LA(↑)	ACC(↑)	FMQ)	LA(↑)Finetune	33.52±3.13	33.88±2.78	65.15±1.18	31.51±2.00	26.00±2.12	55.83±1.42
Table 8: Alternative strategies to reduce forgetting in CTN’s inner optimization. BC: behaviouralcloning strategy in Eq. 6Method	Split CIFAR			Split miniIMN			ACC(↑)	FMQ)	LA(↑)	ACC(↑)	FMa)	LA(↑)CTN-BC	67.65±0.43	6.33±0.70	73.43±0.45	65.82±0.59	3.02±1.13	67.73±1.73CTN-EWC	60.33±1.44	9.33±1.55	68.78±0.24	57.69±0.96	5.59±0.45	61.53±1.38CTN-GEM	64.40±2.52	8.06±1.92	71.49±0.46	60.65±0.80	5.83±0.84	64.42±0.46-	λ (CTN): [1, 10, 25, 50, 100]-	γ (GEM): [0, 0.5, 1]• Semantic memory size in percentage of total memory (CTN): [10%, 20%, 30%, 40%]D	Variants of CTNIn this section, we explored alternative strategies for alleviating catastrophic forgetting in CTN’sinner optimization problem, which is experience replay (ER) to train the base model φ. Particularly,instead of the behavioural cloning strategy in Eq. 6, we consider two strategy to alleviate forgetting inER by combining ER with EWC (Kirkpatrick et al., 2017) and GEM (Lopez-Paz & Ranzato, 2017).
