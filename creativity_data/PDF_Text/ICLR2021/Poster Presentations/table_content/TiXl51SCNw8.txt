Table 1: Accuracy-#Bits tradeoff under different regularization strengths. “FT” stands for finetuning.
Table 2: Quantization results of ResNet-20 models on the CIFAR-10 dataset. BSQ is compared withDoReFa-Net (Zhou et al., 2016), PACT (Choi et al., 2018), LQ-Net (Zhang et al., 2018), DNAS (Wuet al., 2019) and HAWQ (Dong et al., 2019). “MP” denotes mixed-precision quantization.
Table 3: Quantization results of ResNet-50 and Inception-V3 models on the ImageNet dataset. BSQis compared with DoReFa-Net (Zhou et al., 2016), PACT (Choi et al., 2018), LSQ (Esser et al., 2019),LQ-Net (Zhang et al., 2018), Deep Compression (DC) (Han et al., 2015a), Integer (Jacob et al., 2018),RVQ (Park et al., 2018), HAQ (Wang et al., 2019) and HAWQ (Dong et al., 2019).
Table 4: Accuracy-#Bits tradeoff with 2-bit activation. “FT” stands for finetuning.
Table 5: Accuracy-#Bits tradeoff with 3-bit activation. “FT” stands for finetuning.
Table 6: Quantization schemes of ResNet-50 models on ImageNet dataset achieved by BSQ in Table 3.
Table 7: Quantization schemes of Inception-V3 model on ImageNet dataset achieved by BSQ in Ta-ble 3. The scheme on the left is achieved with α = 1e-2 and the one on the right is achieved with α =2e-2. Except for the first 5 convolutional layers and the final FC layer, each row in the table reportsthe precision assigned to the layers within the inception block. The order from left to right follows theparameter definition order provided in the torchvision package implementation (https://github.
