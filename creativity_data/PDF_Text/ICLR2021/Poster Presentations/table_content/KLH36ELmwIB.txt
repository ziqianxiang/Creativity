Table 1: Comparison of searched CNN in theDARTS search space on two different datasets.
Table 2: Comparison of the state-of-the-art models on CIFAR-10 (left) and ImageNet (right). OnCIFAR-10 dataset, our average result is obtained on 5 independently searched models to assure therobustness. For ImageNet, networks in the top block are directly searched on ImageNet; the middleindicates that architectures are searched on CIFAR-10 and then transferred to ImageNet; the bottomindicates models have SE and Swish. We search in S0 for CIFAR-10 and S5 for ImageNet.
Table 3: Searching performance on NAS-Bench-201 (Dong & Yang, 2020). Our method robustlyobtains new SOTA. Averaged on 4 runs of searching. 1st: first-order,2nd: second-orderMethod	Cost CIFAR-10	CIFAR-100	ImageNet16-120	(hours)	valid	test	valid	test	valid	testDARTS1st (2019b)	3.2	39.77±0.00	54.30±0.00	15.03±0.00	15.61±0.00	16.43±0.00	16.32±0.00DARTS2nd (2019b)	10.2	39.77±0.00	54.30±0.00	15.03±0.00	15.61±0.00	16.43±0.00	16.32±0.00GDAS (2019b)	8.7	89.89±0.08	93.61±0.09	71.34±0.04	70.70±0.30	41.59±1.33	41.71±0.98SETN (2019a)	9.5	84.04±0.28	87.64±0.00	58.86±0.06	59.05±0.24	33.06±0.02	32.52±0.21DARTS-	3.2	91.03±0.44	93.80±0.40	71.36±1.51	71.53±1.51	44.87±1.46	45.12±0.82DARTS- (best)	3.2	91.55	94.36	73.49	73.51	46.37	46.34optimal	n/a	91.61	94.37	73.49	73.51	46.77	47.31Transfer results on objection detection We further evaluate the transfer-ability of our models ondown-stream object detection task by replacing the backbone of RetinaNet (Lin et al., 2017) onMMDetection toolbox platform (Chen et al., 2019a). Specifically, with the same training setting asChu et al. (2020b), our model achieves 32.5% mAP on the COCO dataset, surpassing other similar-sized models such as MobileNetV3, MixNet, and FairDARTS. The detailed results are shown inAppendix (Table 11).
Table 4: We remove the strong con-straints on the number of skip connec-tions as 2 and dropout (priors) for P-DARTS and compare its performancew/ and w/o DARTS-.
Table 7: Comparison in various search spaces. We report the lowest error rate of 3 found archi-tectures. *: under Chen & Hsieh (2020)'s training settings where all models have 20 layers and36 initial channels (the best is shown in boldface). > under Zela et al. (2020)‘S settings whereCIFAR-100 models have 8 layers and 16 initial channels (The best is in boldface and underlined).
Table 5: Comparison of PC-DARTS remov-ing the strong prior (i.e. channel shuffle) andcombining DARTS-. The results are from 3independent runs on CIFAR-10. The GPUmemory cost is on a batch size of 256.
Table 6: Searching performance onCIFAR-10 in S3 w.r.t the initial lin-ear decay rate β0 . Each setting isrun for three times.
Table 8: Searching performance on CIFAR-10 in S0, S2 and S3 using longer epochs. Following Biet al. (2019), #P means the number of parametric operators in the normal cell. Averaged on 3 runsof search.
Table 9: Comparison of searched CNN architectures in four reduced search spaces S1-S4 (Zelaet al., 2020) on CIFAR-10 and CIFAR-100. We report the mean±std of test error over 3 foundarchitectures retrained from scratch, alongside with eigenvalue (EV) that corresponds to the bestvalidation accuracy. We follow the same settings as Zela et al. (2020).
Table 10: Comparison of searched models on CIFAR-100. : Reported by Dong & Yang (2019b),?: Reported by Zela et al. (2020), tRerun their code.
Table 11: Transfer results on COCO datasets of various drop-in backbones.
Table 12: List of experiments conducted in this paperMethod	Search Space	Dataset	Figures		Tables				Main text	Supp.	Main text	Supp.
