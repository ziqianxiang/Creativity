Table 1: Visual overview of con-clusions from the 1D experimentin Figure 3. This shows that NN,BNN, fBNN and auNN increas-ingly expand their capabilities.
Table 2: Brier score and expected calibration error (ECE) for auNN and DGP in the large scaleclassfication datasets HIGGS and SUSY (the lower the better in both metrics). The standard error (onthree splits) is close to zero in all cases, see Table 13 in Appendix C.
Table 3: Average training time per batch over 50 independent runs (in seconds). The standard error islow in all cases, see Table 14 in Appendix C.
Table 4: Test NLL for the gap splits of the six UCI datasets (mean and one standard error, the lowerthe better). Last column is the per-group (weight-space stochasticity vs activation-level stochasticity)average rank.
Table 5: Test RMSE for the gap splits of the six UCI datasets (mean and one standard error, the lowerthe better). Last column is the per-group (weight-space stochasticity vs activation-level stochasticity)average rank.
Table 6: Test NLL for the standard splits of the six UCI datasets (mean and one standard error,the lower the better). Last column is the per-group (weight-space stochasticity vs activation-levelstochasticity) average rank.
Table 7: Test RMSE for the standard splits of the six UCI datasets (mean and one standard error,the lower the better). Last column is the per-group (weight-space stochasticity vs activation-levelstochasticity) average rank.
Table 8: Standard error obtained by auNN and DGP in three splits of the large scale classificationdatasets HIGGS and SUSY.
Table 9: Test NLL of auNN and DGP for different values of M (number of inducing points) and D (number of hidden units). Mean and one standard error over 5independent runs on the UCI Kin8 dataset are shown. The lower the better.
Table 10: Test RMSE of auNN and DGP for different values of M (number of inducing points) and D (number of hidden units). Mean and one standard error over 5independent runs on the UCI Kin8 dataset are shown. The lower the better.
Table 11: Test NLL of auNN and DGP for different values of M (number of inducing points) and D (number of hidden units) as the depth increases from L = 2 toL = 4. Mean and one standard error over 5 independent runs on the UCI Power dataset are shown. The lower the better.
Table 12: Test RMSE of auNN and DGP for different values of M (number of inducing points) and D (number of hidden units) as the depth increases from L = 2 toL = 4. Mean and one standard error over 5 independent runs on the UCI Power dataset are shown. The lower the better.
Table 13: Standard error for the results in Table 2. Three random train-test splits are considered.
Table 14: Standard error for the results in Table 3. Fifty independent runs are considered.
Table 15: Training computational cost for the models compared in this paper. The running time (inseconds) corresponds to the mean and one standard error over 10 independent runs of the experimentin Section 3.1. More details in Appendix D.
