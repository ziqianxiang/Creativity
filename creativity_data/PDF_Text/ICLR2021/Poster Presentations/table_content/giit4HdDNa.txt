Table 1: Classification accuracy on MNIST andCIFAR10 test sets for NODE, NODE with data-dependant flow (NODE DC) and open-loop N-CODE (ours)The 10-class prediction vector is obtained as a lineartransformation of the final state x(T ) of the dynam-ics with a softmax activation. We aimed at simplicityby defining the mapping γ as a two-layer perceptronwith 10 hidden units. We varied how γ affects θ bydefining different linear heads for each convolutionin the block. (see C.2 of Appendix). We did notperform any fine-tuning, although we did augmentthe state dimension, akin to ANODE, by increasingthe channel dimension by 10, which helped stabilizetraining. We show that the open-loop control outperforms competing models. Results are shown inTable. 1 with further details in Section C.2 of Appendix.
Table 2: Frechet Inception Distance (FID) for several recent architec-tures (lower is better). ”Components” are the numbers of componentsused in the gaussian mixture estimation of the latent distribution.
Table A.1: Model architectures for the different data sets tested. FCn and Convn represent, respec-tively, fully connected and convolutional layers with n output/filters. We apply a component-wisenormalisation of the control components which proved crucial for good performance of the model.
