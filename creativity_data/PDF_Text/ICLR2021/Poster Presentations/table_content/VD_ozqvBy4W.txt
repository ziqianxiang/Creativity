Table 1: CoCon samples with multiple content inputs, given same prompt text (underlined), exhibit-ing control over generations. More samples are in the Appendix (Table 18 and 19).
Table 2: Content similarity and quality of generated content-conditioned samples. BLEU, NIST andMETEOR values are reported in scale of (×10-2).
Table 3: Evaluation of topic-controlled generations. Topic accuracy report ratio of samples that wereclassified as their target topic.
Table 4: Evaluation of sentiment-controlled generations. Sentiment accuracy report ratio of samplesthat were classified as their target sentiment.
Table 5: Human evaluation of topic/sentiment-controlled generations on relevance with target topicor sentiment and their fluency scores (↑ better for all metrics).
Table 6: Human evaluation of CoCon generations with GPT-2 text as content input (CoCon+) versusother text generators for content similarity with GPT-2 text, relevance with target topic/sentiment andtheir fluency scores (↑ better for all metrics).
Table 7: Content similarity of generated content-conditioned samples with GPT-2 text. BLEU, NISTand METEOR values are reported in scale of (×10-2), ↑ better for all metrics.
Table 8: Human perceived fluency scores of CoCon variants’ topic- and sentiment-controlled gen-erations.
Table 9:	Generated content-conditioned text samples from CoCon and its ablated variants, startingfrom the same prompt text (underlined). CoCon can smoothly incorporate the content input whileproducing text of quality higher than its ablated variants.
Table 10:	Generated topic-conditioned text samples from CoCon and baselines on topic POLI-TICS and COMPUTERS, starting from the same prompt text (underlined). Instances of ‘Score:’ inCTRL’s texts are artifacts from its training on product review data.
Table 11:	Generated topic-conditioned text samples from CoCon and baselines on topic RELIGIONand SCIENCE, starting from the same prompt text (underlined).
Table 12:	Generated sentiment-conditioned text samples from CoCon and baselines, starting fromthe same prompt text (underlined).
Table 13:	CoCon sentiment-conditioned text samples generated with other sentiment attribute mark-ers as content input, prompt texts are underlined.
Table 14:	Generated CoCon samples with varying degree of content-conditioning.
Table 15:	Generated CoCon samples with varying degree of topic content-conditioning.
Table 16:	Generated CoCon samples with varying degree of sentiment content-conditioning.
Table 17:	PPLM samples generated with CoCon-conditioning with different content inputs.
Table 18:	Generated CoCon samples, with multiple content inputs and a single prompt text (under-lined).
Table 19:	More generated CoCon samples, with multiple content inputs and a single prompt text(underlined).
