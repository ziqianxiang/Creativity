Table 1: Text classification results on AG-News. Our approach with different initializations achieves within 0.5%accuracy with 40× fewer parameters, outperforming the published compression baselines. Init: initializationmethod, Acc: accuracy, # Emb: number of (non-zero) embedding parameters.
Table 2: Language modeling on PTB (top) and WikiText-103 (bottom). We outperform existing vocabularyselection, low-rank, tensor-train, and post-compression (hashing) baselines on performance and compressionmetrics. Ppl: perplexity, # Emb: number of (non-zero) embedding parameters.
Table 3: On Movielens 25M, ANT outperforms MF and mixed dimensional embeddings. nbANT automaticallytunes ∣A∣ ("denotes ∣A∣ discovered by NBANT) to achieve a balance between performance and compression.
Table 4: ANT scales to Amazon Product reviews, the largest existing dataset for recommender systems with233M reviews spanning 43.5M users and 15.2M products, and is able to perform well under compression.
Table 5: Word association results after training language models with ANTon the word-level PTB dataset. Left: the non-anchor words most induced bya given anchor word. Right: the largest (non-anchor, anchor) entries learnt inT after sparse '1-regularization. Bottom: movie clusters obtained by sortingmovies with the highest coefficients with each anchor embedding.
Table 6: Table of hyperparameters for text classification experiments on AG-News, DBPedia, Sogou-News, andYelp-review datasets. All text classification experiments use the same base CNN model with the exception ofdifferent output dimensions (classes in the dataset): 4 for AG-News, 14 for DBPedia, 5 for Sogou-News, and 5for Yelp-review.
Table 7: Table of hyperparameters for language modeling experiments using LSTM on PTB dataset.
Table 8: Table of hyperparameters for language modeling experiments using AWD-LSTM on PTB dataset.
Table 9: Table of hyperparameters for language modeling experiments using AWD-LSTM on WikiText-103dataset.
Table 10: Table of hyperparameters for movie recommendation experiments on Movielens 1M (top) andMovielens 25M (bottom). Initial K and ∆K are used for NBANT experiments.
Table 11: More text classification results on (from top to bottom) AG-News, DBPedia, Sogou-News, andYelp-review. Domain knowledge is derived from WordNet and co-occurrence statistics. Our approach withdifferent initializations and domain knowledge achieves within 1% accuracy with 21× fewer parameters onDBPedia, within 1% accuracy with 10× fewer parameters on Sogou-News, and within 2% accuracy with 22×fewer parameters on Yelp-review. Acc: accuracy, # Emb: # (non-zero) embedding parameters.
Table 12: Language modeling using LSTM (top) and AWD-LSTM (bottom) on PTB. We outperform the existingvocabulary selection, low-rank, tensor-train, and post-compression (hashing) baselines. 200/256 represents theembedding dimension. Incorporating domain knowledge further reduces parameters. Ppl: perplexity, # Emb:number of (non-zero) embedding parameters.
Table 13: Language modeling results on WikiText-103. We reach within 3 points perplexity with Z 16× reductionand within l3 points perplexity with Z 80× reduction, outperforming the frequency (Hash Embed) andpost-processing hashing (Sparse Hash) baselines.
Table 14: On Movielens 1M, ANT outperforms MF and mixed dimensional embeddings. nbANT automaticallytunes ∣A∣ (*denotes ∣A∣ discovered by NBANT) to achieve a balance between performance and compression.
Table 15: An example of model selection on the trained language models using LSTM trained on PTB. Tuningthe hyperparameter λ1 and evaluating eq (36) allows us to perform model selection by controlling the trade-offbetween sparsity as determined by the number of anchors used and prediction performance.
