Table 1: Results of RAAC, WCPG, and D4PG in the car example. RAAC learns a policy that sat- urates the velocity before the risky region. WCPG and D4PG learn to accelerate as fast as possible, reaching the goal first with highest average returns but suffer from events with large penalty. We report mean (standard deviation) of each quantity.				Algorithm	CVaR0.1	Mean	Risky Steps	Total StepsRAAC	48.0 (8.3)	48.0 (8.3)	0 (0)	33 (1)WCPG	15.8 (3.3)	79.8 (1.3)	13 (0)	24 (0)D4PG	15.6 (4.4)	79.8 (2.0)	13 (0)	24 (0)4 Experimental EvaluationIn this section, we test the performance of O-RAAC using D = CVaRα=0.1 as risk distortion. Weuse Acerbi’s formula (7) to compute the risk distortion. In particular, we ask:1.	How does RAAC perform as a risk-averse agent in the off-policy setting? (Section 4.1)2.	How does O-RAAC perform as a risk-averse agent in the offline setting? (Section 4.2)3.	How does O-RAAC perform as a risk-neutral agent in the offline setting? (Section 4.3)For further details such as hyperparameter selection and extended results please refer to Appendix A.
Table 2: Performance metrics on offline MuJoCo data sets with medium (first column block) andexpert (second column block) datasets. We compare the 0.1-CVaR and mean of the episode returns,and the average episode duration. We report the mean (standard deviation) of metric. In all environ-ments, O-RAAC has a higher CVaR than benchmarks. In environments that terminate, O-RAAC hasa longer duration too. Finally, O-RAAC has comparable risk-neutral performance to benchmarks.
