Table 1: Comparison of our proposed method (batch, counterfactual IRL) with related works in IRL.
Table 2: Mean cumulative reward and standard deviation for running learnt policy in the environment.
Table 3: Average accuracy and standard deviation for matching the actions in the expert policy.
Table 4: Accuracy on match-ing expert actions.
Table 5: Hyperparameters for training Î¼-network for estimating feature expectations.
Table 6: Hyperparameters used for training Q-network to find optimal policy for a setting of thereward weights.
Table 7: Hyperparameters used for training Q-network to solve the simulated environment.
Table 8: Hyperparameter search range for Counterfactual Recurrent Network. C is the size of theinput and R is the size of the balancing representation built by the Counterfactual Recurrent Network.
Table 9: Hyperparameter search range for the RNN. C is the size of the input.
Table 10: Hyperparameter search range for MLP. C is the size of the input.
