Table 1: The effects of pretraining image augmentations on the transfer performance for supervisedand unsupervised models. The strongest result for each downstream task is marked bold.
Table 2: Transfer performance with pretraining on various datasets. “ImageNet-10%” denotessubsampling 1/10 of the images per class on the original ImageNet. “ImageNet-100” denotessubsampling 100 classes in the original ImageNet. Supervised pretraining uses the labels in thecorresponding dataset, and unsupervised pretraining follows MoCo-v2. Supervised models forCelebA and Places are trained with identity and scene categorization supervision, while supervisedmodels for COCO and Synthia are trained with semantic bounding box and segmentation supervisionfor detection and segmentation networks, respectively.
Table 3: Exemplar-based supervised pretraining which does not enforce explicit constraints on thepositives. It shows consistent improvements over the MoCo baselines by using labels.
Table 4: 5-way few-shot recognition on Mini-ImageNet.
Table 5: Facial landmark pre-diction on MAFL.
Table 6: Longer supervised pretraining for object detection transfer on PASCAL VOC.
Table 7: The effects of pretraining image augmentations on the transfer performance for supervisedand unsupervised models.
Table 8: Transfer performance with pretraining on various datasets. “ImageNet-10%” denotessubsampling 1/10 of the images per class on the original ImageNet. “ImageNet-100” denotessubsampling 100 classes in the original ImageNet. Supervised pretraining uses the labels in thecorresponding dataset, and unsupervised pretraining follows MoCo-v2. Supervised models forCelebA and Places are trained with identity and scene categorization supervision, while supervisedmodels for COCO and Synthia are trained with semantic bounding box and segmentation supervisionfor detection and segmentation networks, respectively.
Table 9: Exemplar-based supervised pretraining which does not enforce explicit constraints on thepositives. It shows consistent improvements over the MoCo baselines by using labels.
Table 10: An ablation study of parameter k and τ for MoCo and Exemplar pretraining.
