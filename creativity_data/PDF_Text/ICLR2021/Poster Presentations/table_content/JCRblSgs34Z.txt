Table 1: Comparison between the exact spectral norm of the jacobian of convolution layers (computedusing Sedghi et al. (2019); Ryu et al. (2019)) and our proposed bound for the a Resnet-18 networkpre-trained on ImageNet. Our bound is within 1.5 times the exact spectral norm (except the first layer)while being significantly faster to compute compared to the exact methods. All running times werecomputed on GPU using tensorflow. For Sedghi et al. (2019)’s method, to have a fair comparison,we only compute the largest singular value (and not all singular values) for all individual matrices inparallel using power iteration (on 1 GPU) and take the maximum. We observe that for different filters,different components of our bound give the minimum value. Also, the values of the four bounds canbe very different for different convolution filters (example: filter in the first layer).
Table 2: Comparison of various methods used for computing the norm of convolution layers. n is theheight and width for a square input, cin is the number of input channels, cout is the number of outputchannels, h and w are the height and width of the convolution filter. For Sedghi et al. (2019), we onlycompute the largest singular value using power iteration (i.e not all singular values).
Table 3: Tightness analysis between our proposed bound and exact method by Sedghi et al. (2019).
Table 4: Comparison between our proposed regularizer (a) and singular value clipping by Sedghiet al. (2019) (b) on test accuracy. Results are on CIFAR-10 dataset using a Resnet-32 network.
Table 5: Comparison betWeen robustness certificates computed using CNN-Cert (Boopathy et al.,2018) and the method proposed in Singla & Feizi (2020) coupled With our spectral bound, fordifferent values of γ for a single hidden layer convolutional neural netWork With tanh activationfunction. Certified Robust Accuracy is computed as the fraction of correctly classified samples Withrobustness-certificate (computed using Singla & Feizi (2020)) greater than 0.5.
Table 6: Effect of increasing β on the bounds (∖∕hW∣∣R∣∣2) of each layer18Published as a conference paper at ICLR 2021Figure 1: We plot the effect of increasing β on the sum of the bounds (computed using our methodi.e ∖∕hW∣∣R∣∣2) and sum of true spectral norms (J(I) ∣∣2 computed using Sedghi et al. (2019)) for aResnet-32 neural network trained on CIFAR-10 dataset (without any weight decay). We observe thatthe gap between the two decreases as we increase β . In Table 6, we show the effect of increasing βon the bounds of individual layers of the same network.)Y	MNIST			CIFAR-10			Standard Accuracy	Empirical Robust Accuracy	Certified Robust Accuracy	Standard Accuracy	Empirical Robust Accuracy	Certified Robust Accuracy^0	98.68%	87.81%	0.00%	56.22%	14.88%	0.00%0.01	97.08%	92.92%	91.25%	53.52%	31.82%	17.39%0.02	96.36%	90.98%	89.58%	49.55%	31.80%	25.93%0.03	95.54%	89.99%	88.75%	46.56%	31.98%	29.26%Table 7: Comparison between certified robust accuracy for different values of the regularizationparameter γ for a single hidden layer convolutional neural network with softplus activation function.
Table 7: Comparison between certified robust accuracy for different values of the regularizationparameter γ for a single hidden layer convolutional neural network with softplus activation function.
