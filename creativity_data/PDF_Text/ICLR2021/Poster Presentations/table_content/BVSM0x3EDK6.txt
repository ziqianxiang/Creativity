Table 1: Average accuracy and 5-run standard deviation (in parenthesis) of MNIST10K model onMNIST-M, SVHN, SYNTH, USPS and their average (DG-avg); and average accuracy of 15 types ofcorruptions in MNIST-C. Both RandConv variants significantly outperform all other methods.
Table 2: Mean and 5-run standard deviation (in parenthesis) results for domain generalization onPACS. Best results with our Deep-All baseline are in bold. The domain name in each columnrepresents the target domain. Base column indicates different baselines and results under differentbaselines are not directly comparable. MLDG and CIDDF used domain labels for training.
Table 3: Accuracy of ImageNet-trained AlexNet on ImageNet-Sketch (IN-S) data. Our methodsoutperform PAR by 5% and are on par with a Stylized-ImageNet (SIN) trained model. Note that PARwas built on top of a stronger baseline than our model, and both PAR and SIN fine-tuned the baselinemodel which helped the performance, while we train RandConv model from scratch.
Table 4: Generalization results on PACS with RandConv and SIN pretrained AlexNet. ImageNetcolumn shows how the pretrained model is trained on ImageNet (baseline represents training theImageNet model using only the classification loss); PACS column indicates the methods used forfinetuning on PACS. Best and second best accuracy for each target domain are highlighted in boldand underlined.
Table 5: Accuracy of ImageNet-trained ResNet-18 on ImageNet-Sketch data.
Table 6: Top 1 Accuracy of ImageNet-trained ResNet-18 on ImageNet-R data. ImageNet-200 are theoriginal ImageNet data with the same 200 classes as ImageNet-R.
Table 7: Generalization results on PACS with RandConv pretrained model using ResNet-18. Ima-geNet column shows how the pretrained model is trained on ImageNet (baseline represents trainingusing only the classification loss); PACS column indicates the methods used for finetuning on PACS.
Table 8: Ablation study of hyperparameter p for RCimg1 on digits recognition benchmarks. DG-Avgis the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results are bold.
Table 9: Ablation study of multi-scale RandConv on digits recognition benchmarks for RCmix andRCimg,p=0.5. Best entries for each variant are bold.
Table 10: Ablation study of consistency loss weight Î» on digits recognition benchmarks for RCmix1-7and RCimg1-7,p=0.5 . DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS.
