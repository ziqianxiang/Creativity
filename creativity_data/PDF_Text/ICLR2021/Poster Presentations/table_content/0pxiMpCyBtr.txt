Table 1: Table of major notationLattice models, proposed in Garcia et al. (2012), are interpolated look-up tables such that the functionparameters are the function values sampled on a regular grid. They have been shown to exhibitefficient training procedures such that the resulting model is guaranteed to satisfy various types ofshape constraints, including monotonicity and convexity (Gupta et al., 2016; 2018; Cotter et al.,2019), without severely restricting the model class. See Figure 2 for an example.
Table 2: Experiment Dataset SummaryDataset	Type	# Features	# Monotonic	# Train	# TestAdult Income	Classification	14	4	32,561	16,282Query Result Matching	Regression	14	11	805,660	201,415User Query Intent	Classification	24	16	522,302	130,576We compare KFL against two previously proposed baseline lattice models: (1) calibrated multilinearinterpolation lattice, and (2) calibrated simplex interpolation lattice, which use the input calibrationlayer described in Section 3.5 and Gupta et al. (2016). We note that we do not compare KFL to deeplattice networks (DLN) (You et al., 2017) in this paper. In a DLN, KFL would act as a replacementlayer for the lattice layers, which only comprise a subset of the model layers. We believe that such acomparison would be dominated by the other layers and not properly show how KFL differs frompreviously proposed lattice models. All our models were implemented using TensorFlow (Abadiet al., 2015) and TensorFlow Lattice (Google AI Blog, 2017). Open-source code for KFL has beenpushed to the TensorFlow Lattice 2.0 library and can be downloaded at github.com/tensorflow/lattice.
Table 3: UCI Adult Income Results	M	V	Train Acc.	Test Acc.	Train Time (s)	Eval Time (μs)	# ParametersMultilinear	-	2	85.60 ± 0.02	85.51 ± 0.05	333	3700	16, 552Simplex	-	2	86.71 ± 0.21	85.72 ± 0.15	151	2200	16, 552KFL	1	4	86.31 ± 0.06	86.05 ± 0.07	70	1800	226KFL	2	8	86.51 ± 0.02	86.24 ± 0.05	85	2100	3954.2	Query Result MatchingThis experiment is a regression problem of predicting the matching quality score of a result to a query.
Table 4: Query Result Matching Results	M	V	Train MSE	Test MSE	Train Time (s)	Eval Time (μs)	# ParametersMultilinear	-	2	0.5694 ± 0.0010	0.5702 ± 0.0009	4606	2835	16, 512Simplex	-	2	0.5718 ± 0.0066	0.5732 ± 0.0066	2150	1952	16, 512KFL	25	8	0.5728 ± 0.0054	0.5735 ± 0.0055	870	1847	2954KFL	50	4	0.5700 ± 0.0037	0.5707 ± 0.0038	868	1839	29794.3	User Query IntentFor this real-world problem, the goal is to classify the user intent into one of two classes. For thebaseline multilinear and simplex models, a single lattice with all 24 features is infeasible. When thenumber of features is too large for a single lattice, we follow the technique described in Canini et al.
Table 5: User Query Intent Results	M	V	Train Acc.	Test Acc.	Train Time (s)	Eval Time (μs)	# ParametersMultilinear	-	2	69.48 ± 0.02	69.20 ± 0.01	13, 559	15, 932	102, 619Simplex	-	2	69.48 ± 0.03	69.34 ± 0.02	4478	6559	102, 619KFL	100	4	69.53 ± 0.04	69.35 ± 0.04	1842	2534	9920KFL	100	8	69.74 ± 0.08	69.53 ± 0.07	3507	2560	19, 5208Published as a conference paper at ICLR 20215 DiscussionFor each one of our experiments, we can see very similar results: (1) the accuracy or mse of KFL iscomparable to or slightly better than the baselines, (2) the training time is significantly reduced, (3)the number of parameters is significantly reduced, (4) KFL can use larger lattice sizes V becausethe number of parameters scales linearly and not as a power of D, (5) increasing M increases thecapacity of the KFL model class where the value for M that makes KFL expressive enough toperform well is a relatively small (and going above such an M no longer provides any more value),and (6) the evaluation speed of KFL compared to multilinear and simplex interpolation aligns withour theoretical runtime analysis, where KFL is significantly faster than multilinear interpolation(particularly when the number of features is large, e.g. Experiment 4.3) and comparable to simplexinterpolation.
