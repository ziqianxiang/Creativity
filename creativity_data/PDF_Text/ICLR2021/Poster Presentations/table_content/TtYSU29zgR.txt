Table 1: DDPG hyperparameters search.
Table 2: Hyperparameter search on Humanoid for 11 demonstrations. (tested/best)A.3 BC ImplementationWe used a 3-layer neural network, the first layer has size 128 with relu activation, the second layer hassize 64 and the last layer has size of the action space with a tanh activation scaled to the action rangeof the environment. We found out that normalizing the observations with the average and standarddeviation of the expert demonstrationsâ€™ observations helped performance. We trained the networkusing the mean squared error as a loss, with an Adam optimizer. We ran an hyperpameter search onthe learning rate in {10-5, 10-4, 10-3} and on the batch size {128, 256}.
