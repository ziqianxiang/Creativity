Table 1: BLEU scores on WMT’16 RO-EN and SQuAD for machine translation and question generation.
Table 2: Rouge and Meteor on Xsum test set for text summarization.					Method	Aug.	Rouge-1	Rouge-2	Rouge-L	METEORText Summarization - XSum					PTGEN-COVG	-	28.10	8.02	21.72	12.46CONVS2S	-	31.89	11.54	25.75	13.20Scratch-T5-MLE	-	31.44	11.07	25.18	13.01Stcratch-CLAPS	Pos.+Neg.	33.52	12.59	26.91	14.18T5-MLE	-	36.10	14.72	29.16	15.78α-T5-MLE (α = 0.7)	-	36.68	15.10	29.72	15.78α-T5-MLE (α = 2.0)	-	34.18	13.53	27.35	14.51T5-SSMBA	Pos.	36.58	14.81	29.68	15.38T5-WordDropout Contrastive	Neg.	36.88	15.11	29.79	15.77R3F	-	36.96	15.12	29.76	15.68T5-MLE-contrastive	-	36.34	14.81	29.41	15.85T5-CLAPS w/o negative	Pos.	37.49	15.31	30.42	16.36T5-CLAPS w/o positive	Neg.	37.72	15.49	30.74	16.06T5-CLAPS	Pos.+Neg.	37.89	15.78	30.59	16.38PEGASUS (Zhang et al., 2020)	-	47.21	24.56	39.25	-9.	T5-CLAPS: Our full model which jointly maximizes the log likelihood, contrastive learningobjective, and KL-divergence as described in the Eq. 9.
Table 3: Greedy decoding from hidden representationof imposters and distant-targets. The answer span ishighlighted for QG.
Table 4: The statistics and the data source of WMT’16 RO-EN, Xsum, and SQuAD.
Table 5: Generated summaries by CLAPS from XsUm dataset.
Table 6:	Generated Questions by CLAPS from SQuAD. Answer spans are highlighted.
Table 7:	Translation of Romanian by CLAPS from WMT’16 RO-EN.
