Table 1: Experimental results on GLUE dev set.
Table 2: Experimental results on SuperGLUE dev set.
Table 3: Effect of HyperGrid Transformers across all model sizes. HyperGrid improves single-model co-training consis- tently overly different model sizes. Improvement over Su- perGLUE is greater than GLUE.				3.1.3	Effect of Modeling ChoicesTo ascertain the effectiveness of our approach, we test different architectural variants of HyperGridTransformers, along with other architectural variants considered during model development.
Table 4: Ablation Study. OG stands for Output Gating.
Table 5: Test set performance on GLUE (Wang et al., 2018). Models with * are large ensembles. Allmodels are single-tasked fine-tuned except ours. Parameter costs are reported considering ensemblesand cost required to fit all of GLUE and SuperGLUE.
Table 6: Test set performance on SuperGLUE (Wang et al., 2019). Our HyperGrid Transformersachieves competitive performance to the state-of-the-art with a single model. Parameter costs refersto total number of parameters used to fit all GLUE and SuperGLUE tasks7Published as a conference paper at ICLR 2021Setup We run experiments with a 3B and 11B HyperGrid Transformer model in multi-task2 setup(GLUE + SuperGLUE). We initialize with the T5 pre-trained checkpoints. Since this is a relativelyexpensive run, we only train the single model HyperGrid once using a 32 Ã— 128 grid with the LG(local-global) setting. For GLUE, we compare against baselines reported in (Clark et al., 2020)which includes models such as BERT (Devlin et al., 2018), ALBERT Lan et al. (2019), RoBERTa(Liu et al., 2019b), and XLNet (Yang et al., 2019). Note that all these models are ensembles andheavily rely on task-specific fine-tunining strategies. More details can be found in the supplementarymaterial.
Table 7: More results (SuperGLUE + GLUE) studies on applying HyperGrid to different parts ofthe Transformer.
