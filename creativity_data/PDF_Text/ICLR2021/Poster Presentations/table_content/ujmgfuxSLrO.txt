Table 1: Comparison with baseline transformers on machine translation corpora. DeLighT modelsrequire significantly fewer parameters to achieve similar performance. Here, * and ^ indicate the best reportedtransformer baselines from Wu et al. (2019) and Ghazvininejad et al. (2019), respectively.
Table 2: DeLighT networks are deep, light-weight and efficient as compared to transformers.
Table 3: Comparison with state-of-the-art methods on machine translation corpora. DeLighT deliverssimilar or better performance than state-of-the-art models With fewer parameters. Here, f indicates that thenetwork uses neural architecture search (NAS) and ∣ indicates that full network parameters are not reported.
Table 4: Results on the WikiText-103 dataset. Compared to Transformer-XL, DeLighT delivers similar orbetter performance (lower perplexity) with fewer parameters. *For Transformer-XL, We reproduce results usingthe official source code. For evaluating Transformer-XL with a context length of 480, we set the mem_lenhyper-parameter to 480 in the official evaluation scripts.
Table 5: Comparison with baseline transformers in terms of training speed and memory consumption. In R4,we implemented CUDA kernels for grouping and ungrouping functions only (see Appendix E). We expectDeLighT to be more efficient with a single and dedicated CUDA kernel for grouping, transformation, featureshuffling, and ungrouping. Memory consumption is measured on a single NVIDIA GP100 GPU (16 GB memory)with a maximum of 4096 tokens per batch and without any gradient accumulation.
Table 6: DeLighT requires less regularization as compared to baseline transformers (Dataset: WMT’14 En-De).
Table 7: Ablations on different aspects of the DeLighT block, including uniform vs. block-wise scaling,depth scaling, and width scaling. Rows partially highlighted in color have the same configuration (repeated forillustrating results). Our experimental setup is similar to Section 4, except that we train our models for 50Kiterations. Multiplication and addition operations (MACs) are computed for 20 time steps.
Table 8: Effect of the position of DeLighT transformation. Lower value of perplexity means better perfor-mance.
