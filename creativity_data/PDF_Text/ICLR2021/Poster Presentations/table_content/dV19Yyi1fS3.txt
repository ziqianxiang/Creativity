Table 1: Comparison of different quantization schemes with and without Quant-Noise on language mod-eling and image classification. For language modeling, we train a Transformer on the Wikitext-103 benchmarkand report perplexity (PPL) on test. For image classification, we train a EfficientNet-B3 on the ImageNet-1kbenchmark and report top-1 accuracy on validation and use our re-implementation of EfficientNet-B3. Theoriginal implementation of Tan & Le (2019) achieves an uncompressed Top-1 accuracy of 81.9%. For bothsettings, we report model size in megabyte (MB) and the compression ratio compared to the original model.
Table 2: Decomposing the impact of the different compression schemes. (a) we train Transformers withAdaptive Input and LayerDrop on Wikitext-103 (b) we pre-train RoBERTA base models with LayerDrop andthen finetune on MNLI (c) we train an EfficientNet-B3 on ImageNet. We report the compression ratio w.r.t. tothe original model (“comp.”) and the resulting size in MB.
Table 3: Quant-Noise: Finetuning vs training. We report performance after iPQ quantization. We train withthe φproxy noise and finetune with Quant-Noise, and use it during the transfer to MNLI for each RoBERTa model.
Table 4: Compression of ResNet-50 with Quant-Noise. We compare to Stock et al. (2019) in boththe small and large blocks regime. For fair comparison, we hold the compression rate constant.
Table 5: Exact versus proxy noise function fordifferent block selections with iPQ. We com-pare exact φPQ and the approximation φproxy withblocks selected from all subvectors or subvectorsfrom the same cluster.
Table 6: Performance on Wikitext-103. We report test set perplexity and model size in megabytes.
Table 7: Performance on MNLI. We report accuracy and size in megabytes. * indicates distillationusing BERT Large. f indicates training with data augmentation. Work from SUn et al. (2019)and Zhao et al. (2019) do not report results on the dev set. Cao et al. do not report model size. Higheraccuracy is better.
Table 8: Performance on ImageNet. We report accuracy and size in megabytes. Higher accuracy isbetter.
Table 9: Effect of Quantization Parameters. We report the influence of the Quant-Noise rate pwith Scalar Quantization (int8). We focus on EfficientNet for ImageNet classification.
Table 10: Comparison of different approaches to int4 and int8 with and without Quant-Noise on language modeling and image classification. For language modeling, we train a Transformeron the Wikitext-103 benchmark. We report perplexity (PPL) on the test set. For image classification,we train a EfficientNet-B3 on the ImageNet-1K benchmark. We report top-1 accuracy on thevalidation set. For both setting, we also report model size in megabyte (MB) and the compressionratio compared to the original model.
Table 11: Performance on Wikitext-103 when using STE in the backward pass of the Layer-Drop pruning noise.
