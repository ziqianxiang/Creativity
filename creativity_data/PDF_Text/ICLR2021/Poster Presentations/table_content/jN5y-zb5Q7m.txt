Table 1: Predictive performance in terms of BLEU, %WER and NLL on newstest14 and LibriSpeech.
Table 2: Sequence-level Error Detection % Prediction Rejection Ratio in Beam-Search decoding.
Table 3: Token-level Error Detection %AUPR for LibriSPeech in Beam-Search Decoding regime.
Table 4:OOD Detection % ROC-AUC in Beam-Search decoding regime for ASR and NMT.
Table 5: Comparison of info-theoretic and heuristic measures on OOD detection (% ROC-AUC).
Table 6: DescriPtion of ASR Datasets					Dataset	Subset	Hours	Utterances	Words / Utterance	Domain	Train	960	281.2K	33.4		Dev-Clean	5.4	2703	17.8	LibrisPeech	Dev-Other Test-Clean	5.3 5.4	2864 2620	18.9 20.1	Story Books	Test-Other	5.1	2939	17.8	AMI	Eval	-	12643	7.1	MeetingsCommon-Voice FR	Test	-	14760	9.5	Generalepochs, where an epoch is a full pass through the entire training set. Checkpoints over the last 30epochs were averaged together, which proved to be crucial to ensuring good performance. Trainingtook 8 days using 8 V100 GPUs. Models were trained on the full 960 hours of the LibriSpeechdataset (Panayotov et al., 2015) in exactly the same configuration as described in (Mohamed et al.,2019). LibriSPeech is a dataset with 〜1000 hours of read books encoded in 16-bit, 16kHz FLACformat. The reference transcriptions were tokenized using a vocabulary of 5000 tokens, as per thestandard reciPe in Fairseq for the VGG-transformer (Ott et al., 2019; Mohamed et al., 2019). ForOOD detection we considered the evaluation subset of the AMI dataset (Kraaij et al., 2005), which isa dataset of meeting transcriPtions, as well as the Russian and French datasets of the Common VoiceProject (Ardila et al., 2019), which consist of PeoPle reading diverse text from the internet. AMI isencoded in 16-bit, 16Khz WAV format. Common Voice data was stored as 24kHz 32-bit MP3 fileswhich were converted into 16-bit 16kHz WAV format via the SOX tool. WER was evaluated using
Table 7: DescriPtion of NMT DatasetsDataset	Subset LNG	Sentences Words / Sent.	DomainEn WMT’14 EN-FR	Train	En Fr	一	29 2 40.8M	29.2	nt	Bh 33.5	Policy, News, WebEn WMT’17 EN-DE	Train	En De	4 5M	26.2	「	1 24.8	Policy, News, Web-	En Newstest14	-	Fr -	De	27.0 3003	32.1	News 28.2En Khresmoi-Summary Dev+Test Fr De	19.0 1500	21.8	Medical 17.9This work considered ensembles of Transformer-Big (Vaswani et al., 2017) neural machine translation(NMT) models. An ensemble 10 models was constructed using a different seed for both initializationand mini-batch shuffling in each model. NMT models were trained on the WMT’14 English-Frenchand WMT’17 English-German datasets. All models were trained using the standard Fairseq (Ott et al.,2019) imPlementation and reciPe, which is consistent with the baseline setuP in described in (Ottet al., 2018b). The data was tokenized using a BPE vocabulary of 40,000 tokens as Per the standardreciPe (Sennrich et al., 2015). For each dataset and translation direction an ensemble of 10 modelswas trained using different random seeds. All 10 models were used during inference. Models trainedon WMT’17 English-German were trained for 193000 stePs of gradient descent, which corresPondsto roughly 49 ePochs, while WMT’14 English-French models were trained for 800000 stePs ofgradient descent, which corresPonds to roughly 19 ePochs. Models were checkPoint-averaged acrossthe last 10 ePochs. All models were trained using mixed-Precision training. Models were evaluatedon newstest14, which was treated as in-domain data. OOD data was constructed by considering17
Table 8: Predictive performance in terms of BLEU, %WER and NLL on newstest14 and LibriSpeech.
Table 9: Predictive performance in terms of BLEU, %WER and NLL on newstest14 and LibriSpeech.
Table 10: Sequence-level Error Detection % PRR in Beam-Search decoding using PEP(y|x, D).
Table 11: %AUPR for LibriSpeech in Beam-Search Decoding regime using PPE(y|x, D).
Table 12: %AUPR for LibriSpeech in Beam-Search Decoding regime using PEP(y|x, D).
Table 13:OOD Detection % ROC-AUC in Beam-Search decoding regime for ASR and NMT. T =10Task	OOD Data	B	ENS-PrEx TU		ENS-ExPr TU		ENS-PrEx KU				I(B) C-IW	ENS-ExPr KU		M SBW			H^ (B) C-IW	H(B) S-IW	H^ (B) C-IW	H(B) S-IW	I(B) 气-IW	K(B) C-IW	M CBW	M (-BW		K(B) C-IW	M CBW		LTC	1	64.1	77.3	63.3	77.2	78.5	78.4	78.3	81.7	65.2	75.3	78.0	78.9		5	65.4	79.2	64.7	79.2	79.1	78.9	78.8	83.6	66.7	76.8	78.6	82.0	PRM	1	92.7	91.6	90.9	91.6	98.7	98.6	98.6	98.5	61.3	94.0	98.6	97.7ENFR		5	93.3	92.1	91.7	92.7	98.7	98.7	98.6	98.7	62.6	95.1	98.7	98.2	L-FR	1	55.2	33.4	51.5	35.7	86.6	88.0	88.9	84.8	58.2	82.6	88.3	85.6		5	57.2	42.1	53.4	46.0	88.1	89.6	90.4	94.8	60.1	85.9	90.3	94.4	L-DE	1	12.4	6.9	11.2	7.6	35.1	38.2	40.4	39.2	19.7	27.6	40.1	44.4		5	13.1	14.5	11.8	15.6	38.9	42.7	45.4	67.8	19.4	32.0	46.6	67.6Table 14: OOD Detection % ROC-AUC in Beam-Search decoding regime for ASR and NMT.
Table 14: OOD Detection % ROC-AUC in Beam-Search decoding regime for ASR and NMT.
Table 15: OOD Detection % ROC-AUC in Teacher-Forcing regimeTask	OOD Data	ENS-PrEx TU		ENS-ExPr TU		I(1) /C-IW	ENS-PrEx KU			I(1) C-IW	ENS-ExPr KU		M S-IW		H (i) HC-IW	H^(1) HS-IW	H (i) HC-IW	H^(1) HS-IW		K⑴ ~C-IW	M CIIW	M (-1W		K⑴ C-IW	M CIIW		L-DEEN	98.3	98.9	98.0	99.0	99.4	99.5	99.5	99.4	55.2	97.6	99.5	96.7ENDE	L-DEDE	22.4	11.3	18.6	11.2	58.7	64.0	67.1	41.0	41.3	54.7	61.4	48.7	L-ENEN	62.2	58.3	56.0	56.7	79.6	81.6	82.7	76.6	31.2	59.6	79.9	82.9	L-FREN	100.0	99.8	100.0	99.8	98.1	97.2	96.2	94.1	71.6	94.6	96.1	85.6	L-FREN	99.6	98.4	99.5	98.6	99.9	99.9	99.8	99.6	67.6	98.7	99.9	97.2ENFR	L-FRFR	28.8	12.3	25.7	12.7	75.1	78.2	80.0	58.9	55.5	72.2	76.9	61.5	L-ENEN	39.4	40.5	37.7	40.7	84.3	86.2	87.4	54.2	68.1	82.2	84.8	56.2	L-DEEN	100.0	99.9	100.0	99.9	97.6	97.0	96.4	95.3	76.2	95.2	96.7	88.024Published as a conference paper at ICLR 2021OOD detection of foreign languages and the copy-through effect. Here, in-domain data is En-Deand En-Fr newstest14 for En-De and En-Fr models, respectively. As OOD data we also considernewstest14 data, but where the either source, target or both languages are changed. The resultsshow that when the source and target languages are both changed (L-DEEN, L-FREN), then this isan easy to detect scenario, as copy-though is forcibly avoided. We also consider situations wherewe forcibly initiate copy-through. Here, we have matched pairs of source-source or target-targetlanguage. Measures of total uncertainty fail, while measures of knowledge uncertainty do not.
Table 16: Effect of length-normalization on sequence-level Error Detection % PRR.
Table 17: Effect of length-normalization on OOD Detection % ROC-AUC for ASR and NMT.
Table 18: Comparison of information-theoretic and heuristic measures on OOD detection (% ROC-AUC).______________________________________________________________________________________Task	OOD Data	T	B	Info.Theor.		V(B) [P]	V(B)[P ]	Heuristic		X-BLEU	X-WER				M CBW	M (BW			V(B)[ln P]	V(B) [ln P]			LTO		1	76.6	73.9	52.6	72.0	71.0	72.7	74.3	71.8		1	20	77.0	76.1	51.2	72.7	73.8	74.6		ASR	AMI	1	1	96.2	96.4	41.5	87.3	86.1	95.8	95.9	95.8			20	94.8	97.4	42.3	85.6	84.0	96.7			C-FR		1	99.9	99.8	7.9	82.0	97.6	98.0	99.5	99.7		1	20	99.9	99.8	12.5	77.6	99.0	98.4			LTC	10	1	72.2	73.3	46.0	68.7	65.6	72.3	65.1	58.2			5	72.6	75.0	46.1	68.6	66.5	72.5			PRM	10	1	96.7	96.2	32.7	88.8	90.4	93.4	80.8	73.5NMT			5	96.9	96.5	33.7	88.7	91.5	93.0			L-FR	10	1	72.1	70.9	59.2	76.4	85.1	71.1	46.8	52.7			5	74.8	79.6	58.0	77.7	89.4	72.2			L-DE	10	1	80.1	76.0	69.1	82.7	76.9	78.3	36.1	38.7			5	82.1	89.2	69.8	90.0	86.0	86.2		J Checkpoint EnsemblesThis work focused mainly on ensemble of models constructed by training from different random
Table 19: Predictive performance in terms of BLEU, %WER and NLL on newstest14 and LibriSpeech.
Table 20: Predictive performance in terms of BLEU, %WER and NLL on newstest14 and LibriSpeech.
Table 21: OOD Detection % ROC-AUC in Beam-Search decoding regime for ASR and NMT.
