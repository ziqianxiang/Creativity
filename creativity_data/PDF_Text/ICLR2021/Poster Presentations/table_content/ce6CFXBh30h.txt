Table 1: Performance Comparison on Batch-IID & NonIID Tasks We use 100 clients (F =0.05) for 200rounds. We measure global model accuracy and averaged communication costs. Note that the SL (SupervisedLearning) models learn on both S and U with full labels, and are utilized as the upper bounds for each experiment.
Table 2: Averaged Local Performance on Streaming-NonIID Task We use 10 clients 100 rounds. Wemeasure averaged local model accuracy and communication costs. Note that the SL models learn on both S andU with full labels, and are utilized as the upper bounds for each experiment.
Table 3: Network Architecture of ResNet-9Layer	Filter Shape	Stride	OutputInput	N/A	N/A	32 × 32 × 3-一 Conv 1 一	一^3 × ^3 × 3 × 64 ―	一^Γ 一	-32×32 × 64 -Conv 2	3×3×64× 128	1	32 × 32 × 128Pool 1	2×2	2	16 × 16 × 128Conv 3	3 × 3 × 128 × 128	1	16 × 16 × 128Conv 4	3 × 3 × 128 × 128	1	16 × 16 × 128Conv 5	3 × 3 × 128 × 256	1	16 × 16 × 256Pool 2	2×2	2	8 × 8 × 256Conv 6	3 × 3 × 256 × 512	1	8 × 8 × 512Pool 3	2×2	2	4× 4× 512Conv 7	3× 3× 512× 512	1	4× 4× 512Conv 8	3× 3× 512× 512	1	4× 4× 512Pool4	4 × 4	4	1 X 1 X 512Softmax	512 × 10	_ N/A—	--1 ×「× 1个一 一We build ResNet-9 networks as ourbase architecture for all base modeland our method. In the architecture,the first two convolutional neural lay-
Table 4: Hyper-Parameters & Training Setups We provide all hyper-parameters and training setups for allbaseline models and our method. Detailed hyper-parameters are also available in the code.
Table 6: Performance Comaprison utilizing AlexNet-Like architecture We use 100 clients for 100 roundsfor streaming task and 200 rounds for batch tasks. We measure global model accuracy, while varying experimen-tal settings (i.e. fraction of available clients and the accessibility of labeled data).
