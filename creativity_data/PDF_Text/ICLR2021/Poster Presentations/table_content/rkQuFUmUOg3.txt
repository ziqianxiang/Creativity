Table 1: Performance on Unseen Datasets (Meta-Test) MetaD2A conducts amortized inference on unseentarget datasets after meta-training on source database consisting of subsets of ImageNet-1K and architecturesof NAS-Bench-201 search space. Meta-training time is 12.7/8.4 GPU hours for the generator/the predictor. Forfair comparison, the parameters of searched architectures are trained on each dataset from scratch instead oftransferring parameters from ImageNet. T is the time to construct precomputed architecture database for eachtarget. We report accuracies with 95% confidence intervals.
Table 2: Performance on Few-shot Classification Taskneural architectures searched by our model on a meta-training set of mini-imagenet. As shown inthe Table 2, the few-shot classification results on MiniImageNet further clearly show the MetaD2A’seffectiveness over existing Meta-NAS methods, as well as the conventional meta-learning methodswithout NAS (Finn et al., 2017; Antoniou et al., 2018).
Table 5: Effectiveness of Set Encoding to Accurately Pre-dict the Accuracy of Multiple DatasetsThe Proposed Set Encoder + GE (Ours) X X ∣	0.8085efficient is the linear correlation between the actual performance and the predicted performance(higher the better). Using both the dataset and the computational graph of the target architectureas inputs, instead of using graphs only (Graph Encoder Only), clearly leads to better performanceto support multiple datasets. Moreover, the predictor with the proposed set encoder clearly shows ahigher correlation than other set encoders (DeepSet (Zaheer et al., 2017), SetTransformer (Lee et al.,2019a), and Statistical Pooling (Lee et al., 2020)).
Table 6: Hyperparameter setting of MetaD2A on NAS-Bench-201 Search SpaceWe use embedding features as inputs of the proposed set encoder instead of raw images, where theembedding features are generated by ResNet18 (He et al., 2016) pretrained with ImageNet-1K (Denget al., 2009). We adopt the teacher forcing training strategy (Jin et al., 2018), which performs thecurrent decoding process after correcting the decoded graph as the true graph until the previousstep. This strategy is only used during meta-training and we progress subsequent generation basedon the currently decoded graph part without the true graph information in the meta-test. We usemini-batch gradient descent to train the model with Eq. (1). The values of hyperparameters whichwe used for both MetaD2A generator and predictor in this paper are described in Table 6. To trainsearched neural architectures for all datasets, we follow the hyperparameter setting of NAS-Bench-201 (Dong & Yang, 2020), which is used for training searched neural architectures on CIFAR10 andCIFAR100. While we report accuracy after training 50 epoch for MNIST, the accuracy of 200 epochare reported for all datasets except MNIST.
