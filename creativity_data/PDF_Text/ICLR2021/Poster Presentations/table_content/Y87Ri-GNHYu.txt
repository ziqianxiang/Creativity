Table 1: Accuracy evaluated on 100 games for 35 unseen crafts. Our method outperforms baselines.
Table 2: Evaluation of few-Shot taSkS for our method againSt baSeline comPariSonS. We conSiderthree SettingS for how many demonStrationS are given to the model: 5% (20 demoS), 10% (40 de-moS), 100%. Variance reSultS are included in SuPPlementary material. ReSultS are averaged acroSS 3SeedS.
Table 3: List of recipes for which we have collected annotations, labeled by the number of stepsneeded to complete it and other recipes which may share sub-tasks of underlying structure.
Table 4: SUmmary StatistiCs for tasks of Varying difficulty.
Table 5: Examples of randomly sampled instructions.
Table 6: Instruction sorted by usage frequency.
Table 7: We compare to some related datasets/environments (Chevalier-Boisvert et al., 2019b; Jianget al., 2019; Hu et al., 2019; Anderson et al., 2018). We don’t report dataset size for an environmentthat generates synthetic language. Note that 〜means limited evaluation, they demonstrate unseenevaluation on one setting only). Most notably, our work focuses on developing a method that per-forms well on unseen tasks. We want to clarify that unseen means tasks/environments which theagent has never received supervised reward for. This is not the same as generating a new configura-tion of a task that the agent received reward for in training.
Table 8: Accuracy of IL (with and without language) evaluated over 100 games with 3 differentseeds. _______________________________________________________________________Steps	IL no language	IL Gen. Language	IL Disc. Language1-step	18.00%± 3.55%	19.33% ± 1.89%	20.00% ± 1.35%2-step	0.00%± 0.00%	9.33% ± 2.05%	8.33%± 0.98%3-step	0.00%± 0.00%	4.33% ± 0.47%	0.00%± 0.00%5-step	0.00%± 0.00%	0.00%± 0.00%	0.00%± 0.00%C.2 Demonstration Only and Few-ShotAs shown in Table 9, we find low deviation in our multiple variance runs. However, we do observein some cases such as IL+RL 10% and 100% higher variance because in some trials the model wasnot able to solve any of the 3-step tasks and in others it was. In the cases of low variance, either themodel was able to consistently or not solve the tasks.
Table 9: Variance results from Table 3 in the main paper, which presents accuracy.
Table 10: Comparison of 2-5 step tasks where only reward is provided to the agent. We believeIL+RL is not able to adapt to these new tasks, given reward only, since it has overfit to the originaltraining tasks. We find that our method outperforms baselines in this setting.
Table 11: Step-by-step discriminated high-level instructions for seen crafts.
Table 12: Step-by-step generated high-level instructions for seen crafts.
Table 13: Step-by-step generated high-level instructions for unseen crafts.
Table 14: Example of instruction and inventory side-by-side for 3 unseen tasks. As in Figure 6 fromthe main paper, the inventory changes when a subtask, given by the instruction, is completed.
