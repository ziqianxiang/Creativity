Table 1: Spearman ρ of zero-cost proxies on NAS-Bench-201.
Table 2: Spearman ρ of zero-cost proxies on other NAS search spaces.
Table 3: Comparison to prior work on NAS-Bench-101 dataset.
Table 4: Number of top-5% most-accurate models within the top 64 models returned by synflow.
Table 5: For 1000 clusters of models with edit distance 1, we empirically measure the probabilitythat the synflow proxy will select the most accurate model from each cluster.
Table 6: EcoNAS training hyper-parameters for NAS-Bench-201.
Table 7: Zero-cost NAS comparison with baseline algorithms on NAS-Bench-201 CIFAR-100. Weshow accuracy after 50 trained models and the number of models to reach 73.5% accuracy.
Table 8: Zero-cost NAS comparison with baseline algorithms on NAS-Bench-ASR. We show PERafter 50 trained models and the number of models to reach PER=21.3%.
Table 9: Spearman ρ of zero-cost proxies for the top 10% of points on all NAS search spaces.
Table 10: Percentage of top-10% most-accurate models within the top-10% of models ranked byeach zero-cost metric.
Table 11: Number of top-5% most-accurate models within the top-64 models returned by eachmetric.
Table 12: Rank correlation coefficient for the local neighbourhoods (edit distance = 1) of 1000clusters in each search space.
Table 13: For 1000 clusters of points with edit distance = 1. We count the number of times whereinthe top model returned by a zero-cost metric matches the top model according to validation accuracy.
Table 14: All metrics remain fairly constant when varying the initialization seed - the variations areonly observed at the third significant digit. Dataload is random with 128 samples and initializationis done with default PyTorch initialization scheme.
Table 15: fisher becomes noticeably better when biases are initialized to zero; otherwise, metricsseem to perform independently of initialization method. Results averaged over 3 seeds.
Table 16: Surprisingly, grasp becomes worse with more (random) data, while grad_norm andsnip degrade very slightly. Other metrics seem to perform independently of the number of samplesin the minibatch. Initialization is done with default PyTorch initialization scheme.
