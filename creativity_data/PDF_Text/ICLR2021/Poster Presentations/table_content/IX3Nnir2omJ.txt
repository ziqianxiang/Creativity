Table 1: ImageNet Top-1 Accuracy (%) for ResNets with FixUp (Zhang et al., 2019) or SkipInit(De & Smith, 2020), Normalizer-Free ResNets (ours), and Batch-Normalized ResNets. We train allvariants both with and without additional regularization (stochastic depth and dropout). Results aregiven as the median accuracy ± the standard deviation across 5 random seeds. * indicates a settingwhere two runs collapsed and results are reported only using the 3 seeds which train successfully.
Table 2: ImageNet Top-1 Accuracy (%) for Normalizer-Free ResNets and Batch-NormalizedResNet-50s on ImageNet, using very small batch sizes trained for 15 epochs. Results are givenas the median accuracy ± the standard deviation across 5 random seeds. Performance degradesseverely for Batch-Normalized networks, while Normalizer-Free ResNets retain good performance.
Table 3: ImageNet Top-1 Accuracy (%) comparison for NF-RegNets and recent state-of-the-artmodels. “w/ Augs” refers to accuracy with advanced augmentations: for EfficientNets, this is withAutoAugment or RandAugment. For NF-RegNets, this is with CutMix + MixUp. NF-RegNetresults are reported as the median and standard deviation across 5 random seeds.
Table 4: Training speed (in training iterations per second) comparisons of NF-ResNets and BN-ResNets on a single 16GB V100 for various batch sizes.
Table 5: Training speed (in training iterations per second) comparisons of NF-RegNets and Batch-Normalized EfficientNets on a single 16GB V100 for various batch sizes.
Table 6: Results on Pascal VOC Semantic Segmentation.
Table 7: Results on NYUv2 Depth Estimation.
