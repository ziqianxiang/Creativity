Table 1: Comparison of SSD with different outlier detectors using only unlabeled training data.
Table 2: Comparing performance of self-supervised (SSD) and supervised representations. We alsoprovide results for few-shot OOD detection (SSDk) for comparison with our baseline SSD detector.
Table 3: Comparison of SSD with other detectors for anomaly detection task on CIFAR-10 dataset.
Table 4: Comparison of SSD+, i.e., incorporating labels in the SSD detector, with state-of-the-art detectors based on supervised training.					In-distribution (Out-of-distribution)	CIFAR-10 (CIFAR-100)	CIFAR-10 (SVHN)	CIFAR-100 (CIFAR-10)	CIFAR-100 (SVHN)	AverageSoftmax-probs (Hendrycks & Gimpel, 2017)	89.8	95.9	78.0	78.9	85.6ODIN(Liang et al., 2018)f	89.6	96.4	77.9	60.9	81.2Mahalnobis (Lee et al., 2018b)∣	90.5	99.4	55.3	94.5	84.8Residual Flows (Zisselman & Tamar, 2020)1	89.4	99.1	77.1	97.5	90.7Gram Matrix (Sastry & Oore, 2019)	79.0	99.5	67.9	96.0	85.6Outlier exposure (Hendrycks et al., 2019a)	93.3	98.4	75.7	86.9	88.6Rotation-loss + Supervised (Hendrycks et al., 2019b)	90.9	98.9	-	一	-Contrastive + Supervised (Winkens et al., 2020)*	92.9	99.5	78.3	95.6	91.6CSI (Tack et al., 2020)	92.2	97.9	-	一	-SSD+	93.4	99.9	78.3	98.2	92.4SSDk + (k = 5)	94.1	99.6	84.1	97.4	93.8* Uses 4 × wider ResNet-50 network, * Requires additional out-of-distribution data for tuning.
Table 5: Test Accuracy and AUROCwith different temperature values in NT-Xent (Equation 1) loss. Using CIFAR-10 as in-distribution and CIFAR-100 asOOD dataset with ResNet18 network.
Table 6: Experimental results of SSD detector with multiple metrics for ImageNet dataset.
Table 7: Experimental results of SSD detector with multiple metrics over CIFAR-10, CIFAR-100, and STL-IO dataset.
