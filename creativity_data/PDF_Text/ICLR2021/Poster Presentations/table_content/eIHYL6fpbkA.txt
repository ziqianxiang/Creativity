Table 1: Accuracy (%) comparison of the OAT model with Standard, PGD, and TRADES on CI-FAR10, CIFAR100, and ImgNet10 (64×64) under different threat models. We show the improvedresults compared to the counterpart of each model in bold.
Table 2: Comparison of OAT using various OOD datasets for improving robust generalization onCIFAR10 and ImgNet10 (64 × 64). The None represents the baseline model (PGD).
Table 3: Accuracy (%, mean over 5 runs) comparison of the OAT model using various datasets withthe baseline models. We show the improved results compared to the counterpart of each model inbold. Pseudo-label is the model trained using pseudo-labeled data (from 80M-TI) in conjunctionwith the target dataset. Fusion is the model that applies OAT80M-TI to the Pseudo-label model. Adetailed description can be found in Appendix D.
Table 4: Implementation details for the experiments of OAT-ATarget	architecture	α	training steps	batch sizeCIFAR	WRN-34-10 (Zagoruyko & Komodakis, 2016)	1.0	80K	128ImgNet10	ResNet18 (He et al., 2016)	1.0	15.4K	128Table 5: Implementation details for the experiments of OAT-STarget	N	architecture	α	training steps	batch sizeCIFAR	2500 50K	ResNet18	1.0	4000 78K	128 128ImgNet10	100	WRN-22-10	1.0	200	100(64 x 64)	9894		0.2	15.4K	128ImgNet10	100	ResNet18	1.0	400	100(160 x 160)	9894		0.1	38.5K	128D Sourcing OOD DatasetsWe created OOD datasets from 80M-TI, using the work of Carmon et al. (2019) for CIFAR10 andCIFAR100, respectively. In other words, we trained a 11-way (1 more class for OOD) classifier ona training set consisting of the CIFAR10 dataset and 1M randomly sampled images from 80M-TIwith keywords that did not appear in CIFAR10. We then applied the classifier on 80M-TI and sortedthe images based on confidence in the OOD class. The 1M and 5M images selected in the orderof the highest confidence were used for OAT-A and OAT-S, respectively. In Table 3, Fusion has abatch size of target n + pseudo-labelednn + OOD nn, which would be compared with the Pseudo-labelmodel that is trained with a batch size of targetɪ + pseudo-labeled2. In addition, We resized (using a
Table 5: Implementation details for the experiments of OAT-STarget	N	architecture	α	training steps	batch sizeCIFAR	2500 50K	ResNet18	1.0	4000 78K	128 128ImgNet10	100	WRN-22-10	1.0	200	100(64 x 64)	9894		0.2	15.4K	128ImgNet10	100	ResNet18	1.0	400	100(160 x 160)	9894		0.1	38.5K	128D Sourcing OOD DatasetsWe created OOD datasets from 80M-TI, using the work of Carmon et al. (2019) for CIFAR10 andCIFAR100, respectively. In other words, we trained a 11-way (1 more class for OOD) classifier ona training set consisting of the CIFAR10 dataset and 1M randomly sampled images from 80M-TIwith keywords that did not appear in CIFAR10. We then applied the classifier on 80M-TI and sortedthe images based on confidence in the OOD class. The 1M and 5M images selected in the orderof the highest confidence were used for OAT-A and OAT-S, respectively. In Table 3, Fusion has abatch size of target n + pseudo-labelednn + OOD nn, which would be compared with the Pseudo-labelmodel that is trained with a batch size of targetɪ + pseudo-labeled2. In addition, We resized (using abilinear interpolation) ImageNet to dimensions of64 × 64 and 160 × 160 and divided it into datasetscontaining 10 and 990 classes, respectively; these are called ImgNet10 (train set size = 9894 andtest set size = 3500) and ImgNet990, respectively. The classes were divided based on the Imagenettedataset (Howard), and the experimental results for the differently divided dataset (Imagewoof) can
Table 6: Accuracy (%) comparison of the OAT model with Standard and PGD on ImgNet10 (64×64)under different threat models. We show the improved results compared to the counterpart of eachmodel in bold.
Table 7: Accuracy (%, mean over 5 runs) comparison of the OAT models with the baseline models.
Table 8: Adversarial robustness (%) as hyperparameter α changesα	PGD20	CW201.0	57.45	52.652.0	58.25	52.163.0	58.31	52.024.0	59.04	51.715.0	59.48	51.2416Published as a conference paper at ICLR 2021Table 9: Error rate (%) comparison of OAT with Mixup on CIFAR10, CIFAR100, and ImgNet10.
Table 9: Error rate (%) comparison of OAT with Mixup on CIFAR10, CIFAR100, and ImgNet10.
