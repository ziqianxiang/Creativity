Table 1: Comparison of SOLAR against DSSM, DSSM+GLaS, and SNRM baselines. SOLARâ€™smetrics are better than the industry-standard DSSM model while training 10x faster and evaluating2x faster (SOLAR-CPU vs DSSM-GPU evaluation). GLaS regularizer improves the metrics but stilllags behind SOLAR.
Table 2: Comparison of different scoring schemes. SOLAR inference admits any aggregation schemethat is monotonic as a function of true label probabilities. We observe that summing probabilities,log-probabilities and logits all give nearly the same precision with logits being slightly better.
Table 3: Filtering Noisy Candidates: We only retain candidates which appear in at-least d of theK models in the top-m buckets. For K = 16, m = 100 and B = 30000, precision and averagecandidate set size is listed above. Recall improves a lot when we filter out noisy candidates.
Table 4: SOLAR vs popular Extreme Classification benchmarks. Embedding models AnnexML andSLEEC clearly underperform compared to SOLAR. SOLAR even outperforms the state-of-the-artnon-embedding baselines like Parabel and Slice. The gains in P@5 are particularly huge (45.32% vs31.57%). SLEEC and SLICE do not scale up to 3M labels (corroborated on XML-Repo).
Table 5: Training and Evaluation speeds against the fastest baselines.
Table 6: Effect of B, K on P @1/3/5 for Amz-670K dataset (with m = 25). We increment Blinearly and K exponentially and choose an optimal trade-off between precision and inference time(shown in ms/point).
Table 7: Sample queries and the respective top predictions from SOLAR and DSSM+GLaS. The firstquery is a relatively frequent one. The second and third are relatively infrequent. We can see that oninfrequent queries, SOLAR is more robust than dense embedding models.
Table 8: SOLAR vs Parabel on Word Prediction for French to English translation.
