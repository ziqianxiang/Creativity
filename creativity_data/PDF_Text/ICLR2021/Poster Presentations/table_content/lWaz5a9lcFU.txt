Table 1: Comparison of EEC and EECS to approaches that store and use real samples (denoted withan S) and those that generate samples, for class-incremental learning on MNIST, SVHN, CIFAR-10and ImageNet-50 datasets. The difference between EEC and the SOTA approach is shown in bold.
Table 3: Convolutional Autoencoder Architecture for MNIST, SVHN, CIFAR-10 and ImageNet-50datasetsC HyperparametersHyperparameters	ValuesNo. of epochs	-100-Starting Learning Rate (LR)	0.001LR DeCay Milestones	{50}LR DeCay FaCtor	1/10miniBatCh Size	128OPtimizer	AdamWeight DeCay	0.0005	Î»		0.7Table 4: Hyper-parameters for EEC autoencoder trainingHyperparameters	ValuesNo. of epochs (1st increment)	200No. of ePoChs (next inCrements)	45Starting Learning Rate (LR)	0.1LR DeCay Milestones	{60,120,160}LR DeCay FaCtor	1/5miniBatCh Size	128
Table 4: Hyper-parameters for EEC autoencoder trainingHyperparameters	ValuesNo. of epochs (1st increment)	200No. of ePoChs (next inCrements)	45Starting Learning Rate (LR)	0.1LR DeCay Milestones	{60,120,160}LR DeCay FaCtor	1/5miniBatCh Size	128OPtimizer	SGDMomentum	0.9Weight Decay	0.0005Table 5: Hyper-parameters for EEC classifier training14Published as a conference paper at ICLR 2021Methods	Accuracy (%)iCaRL (Rebufi et al., 2017)	595EEIL (Castro et al., 2018)	640BiC (Wuet al., 2019)	643LUCIR (Hou et al., 2019)	634EEC (Ours)	656
Table 5: Hyper-parameters for EEC classifier training14Published as a conference paper at ICLR 2021Methods	Accuracy (%)iCaRL (Rebufi et al., 2017)	595EEIL (Castro et al., 2018)	640BiC (Wuet al., 2019)	643LUCIR (Hou et al., 2019)	634EEC (Ours)	656FearNet (Kemker & Kanan, 2018)	662CAN (Xiang etal., 2019)	671EEC-P (Ours)	76.7Table 6: Comparison of EEC against episodic memory approaches and approaches that use a pre-trained CNN in terms of average incremental accuracy (%) with 10 classes per increment. EEC-Puses a pre-trained CNN for feature extraction.
Table 6: Comparison of EEC against episodic memory approaches and approaches that use a pre-trained CNN in terms of average incremental accuracy (%) with 10 classes per increment. EEC-Puses a pre-trained CNN for feature extraction.
Table 7: Performance of EEC for different values of rcovariance matrix) for each of the two classes and then used pseudorehearsal to generate pseudo-encoded episodes. Figure 6 shows the comparison between the original feature space (Figure 6 (a)),pseudo-encoded episodes generated with 200 centroids using memory integration (Figure 6 (b)) andpseudo-encoded episodes generated using a single centroid per class (2 centroids) as in FearNet(Figure 6 (c)). The feature space generated by FearNet is almost circular and does not resemble theoriginal feature space. Also, there is an overlap between the feature spaces of the two classes whichwill lead to similar images for the two classes which will eventually hurt classifier performance. Incontrast, the pseudo-encoded episodes generated by memory integration capture the overall conceptof the original feature space without any overlap between the feature spaces of the two classes.
