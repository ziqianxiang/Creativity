Table 1: Classification accuracies on CIFAR10 for Resnet with varying depths and sparsities using SNIP (Lee et al. (2018)) and our algorithm SBP-SR							ALGORITHM	90%	95%	98%	99.5%	99.9%ResNet32	SNIP	92.26 ± 0.32	91.18 ± 0.17	87.78 ± 0.16	77.56±0.36	9.98±0.08	SBP-SR	92.56 ± 0.06	91.21 ± 0.30	88.25 ± 0.35	79.54±1.12	51.56±1.12ResNet50	SNIP	91.95 ± 0.13	92.12 ± 0.34	89.26 ± 0.23	80.49±2.41	19.98±14.12	SBP-SR	92.05 ± 0.06	92.74± 0.32	89.57 ± 0.21	82.68±0.52	58.76±1.82ResNet 104	SNIP	93.25 ± 0.53	92.98 ± 0.12	91.58 ± 0.19	33.63±33.27	10.11±0.09	SBP-SR	94.69 ± 0.13	93.88 ± 0.17	92.08 ± 0.14	87.47±0.23	72.70±0.48In this paper, we provide novel algorithms for Sensitivity-Based Pruning (SBP), i.e. pruning schemesthat prune a weight W based on the magnitude of |W募 | at initialization where L is the loss.
Table 2: Classification accuracies for CIFAR10 and CIFAR100 after pruningSparsity	CIFAR10				CIFAR100		90%	95%	98%	90%	95%	98%ResNet32 (NO PRUNING)	94.80	-	-	74.64	-	-OBD LeCun et al. (1990)	93.74	93.58	93.49	73.83	71.98	67.79Random Pruning	89.95±0.23	89.68±0.15	86.13±0.25	63. 13±2.94	64.55±0.32	19.83±3.21MBP	90.21±0.55	88.35±0.75	86.83±0.27	67.07±0.31	64.92±0.77	59.53±2.19SNIP LEE ET AL. (2018)	92.26 ± 0.32	91.18 ± 0.17	87.78 ± 0.16	69.31 ± 0.52	65.63 ± 0.15	55.70 ± 1.13GRASP WANG ET AL. (2020)	92.20±0.31	91.39±0.25	88.70±0.42	69.24 ± 0.24	66.50 ± 0.11	58.43 ± 0.43GRASP-SR	92.30±0.19	91.16±0.13	87.8 ± 0.32	69.12 ± 0.15	65.49 ± 0.21	58.63 ± 0.23SynFlow Tanaka et al. (2020)	92.01±0.22	91.67±0.17	88.10 ± 0.25	69.03 ± 0.20	65.23 ± 0.31	58.73 ± 0.30SBP-SR (STABLE RESNET)	92.56 ± 0.06	91.21 ± 0.30	88.25 ± 0.35	69.51 ± 0.21	66.72 ± 0.12	59.51 ± 0.15ResNet50 (NO PRUNING)	94.90	-	-	74.9	-	-Random Pruning	85.11±4.51	88.76±0.21	85.32±0.47	65.67±0.57	60.23±2.21	28.32±10.35MBP	90.11 ± 0.32	89.06 ± 0.09	87.32 ± 0.16	68.51 ± 0.21	63.32 ± 1.32	55.21 ± 0.35SNIP	91.95 ± 0.13	92.12 ± 0.34	89.26 ± 0.23	70.43 ± 0.43	67.85 ± 1.02	60.38 ± 0.78GRASP	92.10 ± 0.21	91.74 ± 0.35	89.97± 0.25	70.53±0.32	67.84±0.25	63.88±0.45SynFlow	92.05 ± 0.20	91.83 ± 0.23	89.61± 0.17	70.43±0.30	67.95±0.22	63.95±0. 1 1SBP-SR	92.05 ± 0.06	92.74± 0.32	89.57 ± 0.21	71.79 ± 0.13	68.98 ± 0.15	64.45 ± 0.34ResNet104 (NO PRUNING)	94.92	-	-	75.24	-	-
Table 3: Classification accuracies on Tiny ImageNet for Resnet with varying depths	ALGORITHM	85%	90%	95%ResNet32	SBP-SR	57.25 ± 0.09	55.67 ± 0.21	50.63±0.21	SNIP	56.92± 0.33	54.99±0.37	49.48±0.48	GRASP	57.25±0.11	55.53±0.11	51.34±0.29	S ynFlow	56.75±0.09	55.60±0.07	51.50±0.21ResNet50	SBP-SR	59.8±0.18	57.74±0.06	53.97±0.27	SNIP	58.91±0.23	56.15±0.31	51.19±0.47	GRASP	58.46±0.29	57.48±0.35	52.5±0.41	S ynFlow	59.31±0.17	57.67±0.15	53.14±0.31ResNet104	SBP-SR	62.84±0.13	61.96±0.11	57.9±0.31	SNIP	59.94±0.34	58.14±0.28	54.9±0.42	GRASP	61.1±0.41	60.14±0.38	56.36±0.51	S ynFlow	61.71±0.08	60.81±0.14	55.91±0.43To confirm these results, we also test SBP-SR against other pruning algorithms on Tiny ImageNet.
Table 4: Test accuracy on MNIST with V-CNN for different depths with sparsity 50% using SBP(SNIP)	L = 10	L = 50	L=100Ordered phase Init	98.12±0.13	10.00±0.0	10.00±0.0EOC INIT	98.20±0.17	98.75±0.1 1	10.00±0.0EOC + ReScaling	98.18±0.21	98.90±0.07	99.15±0.08However, V-CNN is a toy example that is generally not used in practice. Standard CNN architecturessuch as VGG are popular among practitioners since they achieve SOTA accuracy on many tasks.
Table 5: Classification accuracy on CIFAR10 for VGG16 and 3xVGG16 with varying sparsities	ALGORITHM	85%	90%	95%VGG16	SNIP	93.09±0.11	92.97±0.08	92.61±0.10	SYNFLOW	93.21±0.13	93.05±0.11	92. 19±0. 12	EOC + RESCALING	93.15±0.12	92.90±0.15	92.70±0.063XVGG16	SNIP	93.30±0.10	93.12±0.20	92.85±0. 15	SYNFLOW	92.95±0.13	92.91±0.21	92.70±0.20	EOC + RESCALING	93.97±0.17	93.75±0.15	93.40±0.16Summary of numerical results. We summarize in Table 6 our numerical results. The letter ‘C’refers to ‘Competition’ between algorithms in that setting, and indicates no clear winner is found,while the dash means no experiment has been run with this setting. We observe that our algorithmSBP-SR consistently outperforms other algorithms in a variety of settings.
Table 6: Which algorithm performs better? (according to our results)DATASET	ARCHITECTURE	85%	90%	95%	98%CIFAR1 0	RESNET32	-	C	C	GRASP	RESNET50	-	C	SBP-SR	GRASP	RESNET104	-	SBP-SR	SBP-SR	SBP-SR	VGG16	C	C	C	-	3XVGG16	EOC+RESC	EOC+RESC	EOC+RESC	-CIFAR1 00	RESNET32	-	SBP-SR	SBP-SR	SBP-SR	RESNET50	-	SBP-SR	SBP-SR	SBP-SR	RESNET104	-	SBP-SR	SBP-SR	SBP-SRTINY IMAGENET	RESNET32	C	C	SYNFLOW	-	RESNET50	SBP-SR	C	SBP-SR	-	RESNET104	SBP-SR	SBP-SR	SBP-SR	-5	ConclusionIn this paper, we have formulated principled guidelines for SBP at initialization. For FNNN and CNN,we have shown that an initialization on the EOC is necessary followed by the application of a simplerescaling trick to train the pruned network. For Resnets, the situation is markedly different. Thereis no need for a specific initialization but Resnets in their original form suffer from an explodinggradient problem. We propose an alternative Resnet parameterization called Stable Resnet, whichallows for more stable pruning. Our theoretical results have been validated by extensive experiments
Table 7: Theoretical upper bound of Proposition 4 and empirical observations for a FFNN withN= 100andL= 100Gamma	γ=2	γ=5	γ= 10Upper b ound	5.77	0.81	0.72Empirical ob s ervation	≈1	0.79	0.69Proof. Let x ∈ (0, 1) and kx = (1 - x)ΓLN2, where ΓL = Pi6=i ζi. We haveP(scr ≤ x) ≥ P(max |Wii0 | < |W|(kx)),where |W |(kx) is the kxth order statistic of the sequence {|Wii |, l 6= l0, i ∈ [1 : Mi]}; i.e|W |(1) > |W |(2) > ... > |W|(kx).
Table 8: Classification accuracy on ImageNet (Top-1) for ResNet50 with varying sparsities (TODO: Theseresults will be updated to include confidence intervals)Algorithm	85%	90%	95%SNIP	69.05	64.25	44.90GRASP	69.45	66.41	62.10SynFlow	69.50	66.20	62.05SBP-SR	69.75	67.02	62.6635Published as a conference paper at ICLR 2021Table 9: Test accuracy of pruned neural network on CIFAR10 with different activation functionsResnet32	Algo	90	98	99.5	99.9Relu	SBP-SR	92.56(0.06)	88.25(0.35)	79.54(1.12)	51.56(1.12)	SNIP	92.24(0.25)	87.63(0.16)	77.56(0.36)	10(0)Tanh	SBP-SR	90.97(0.2)	86.62(0.38)	75.04(0.49)	51.88(0.56)	SNIP	90.69(0.28)	85.47(008)	10(0)	10(0)Resnet50 RelU	SBP-SR	92.05(0.06)	89.57(0.21)	82.68(0.52)	58.76(1.82)	SNIP	91.64(0.14)	89.20(0.54)	80.49(2.41)	19.98(14.12)Tanh	SBP-SR	90.43(0.32)	88.18(0.10)	80.09(0.0.55)	58.21(1.61)	SNIP	89.55(0.10)	10(0)	10(0)	10(0)G	Additional Experiments
Table 9: Test accuracy of pruned neural network on CIFAR10 with different activation functionsResnet32	Algo	90	98	99.5	99.9Relu	SBP-SR	92.56(0.06)	88.25(0.35)	79.54(1.12)	51.56(1.12)	SNIP	92.24(0.25)	87.63(0.16)	77.56(0.36)	10(0)Tanh	SBP-SR	90.97(0.2)	86.62(0.38)	75.04(0.49)	51.88(0.56)	SNIP	90.69(0.28)	85.47(008)	10(0)	10(0)Resnet50 RelU	SBP-SR	92.05(0.06)	89.57(0.21)	82.68(0.52)	58.76(1.82)	SNIP	91.64(0.14)	89.20(0.54)	80.49(2.41)	19.98(14.12)Tanh	SBP-SR	90.43(0.32)	88.18(0.10)	80.09(0.0.55)	58.21(1.61)	SNIP	89.55(0.10)	10(0)	10(0)	10(0)G	Additional ExperimentsIn Table 10, we present additional experiments with varying Resnet Architectures (Resnet32/50), andsparsities (up to 99.9%) with Relu and Tanh activation functions on Cifar10. We see that overall,using our proposed Stable Resnet performs overall better that standard Resnets.
Table 10: Test accuracy of pruned vanilla-CNN on CIFAR10 with different depth/sparsity levelsResnet32	Algo	90	98	99.5	99.9Relu	SBP-SR	92.56(0.06)	88.25(0.35)	79.54(1.12)	51.56(1.12)	SNIP	92.24(0.25)	87.63(0.16)	77.56(0.36)	10(0)Tanh	SBP-SR	90.97(0.2)	86.62(0.38)	75.04(0.49)	51.88(0.56)	SNIP	90.69(0.28)	85.47(0.18)	10(0)	10(0)Resnet50 RelU	SBP-SR	92.05(0.06)	89.57(0.21)	82.68(0.52)	58.76(1.82)	SNIP	91.64(0.14)	89.20(0.54)	80.49(2.41)	19.98(14.12)Tanh	SBP-SR	90.43(0.32)	88.18(0.10)	80.09(0.0.55)	58.21(1.61)	SNIP	89.55(0.10)	10(0)	10(0)	10(0)37Published as a conference paper at ICLR 2021H	On The Lottery Ticket HypothesisThe Lottery Ticket Hypothesis (LTH) (Frankle and Carbin, 2019) states that “randomly initializednetworks contain subnetworks that when trained in isolation reach test accuracy comparable to theoriginal network”. We have shown so far that pruning a NN initialized on the EOC will outputsparse NNs that can be trained after rescaling. Conversely, if we initialize a random NN with anyhyperparameters (σw, σb), then intuitively, we can prune this network in a way that ensures thatthe pruned NN is on the EOC. This would theoretically make the sparse architecture trainable. Weformalize this intuition as follows.
