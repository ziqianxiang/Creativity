Table 1: Performance on molecule generation trained on Zinc250k (Irwin et al., 2012), calculatedon 10k samples and averaged over 4 runs. The standard deviation of those runs can be found inAppendix C. Scores of the baselines are taken from their respective papers.
Table 2: Results on the graph coloring problem. Runtimes are measured on a NVIDIA TitanRTXGPU with a batch size of 128. The standard deviations of the results can be found in Appendix D.2.
Table 3: Results on set modeling. Metric used is bits per categorical variable (dimension).
Table 4: Performance on molecule generation trained on Zinc250k (Irwin et al., 2012) with standarddeviation is calculated over 4 independent runs. See Table 1 for baselines.
Table 5: Performance on molecule generation Moses (Polykovskiy et al., 2018), calculated on 10ksamples and averaged over 4 runs. Score for GraphAF taken from Shi et al. (2020), and JT-VAE fromPolykovskiy et al. (2018).
Table 6: Hyperparameter overview for the set modeling experiments presented in Table 3Hyperparameters	Categorical NF	Var. dequant.	Discrete NFLatent dimension	{2, 4,6}	1	16#Encoding couplings	-/4/4	4	-#Coupling layers	8	8	{4,8}Coupling network	Transformer	Transformer	Transformer- Number of layers	2	2	2- Hidden size	256	256	256Mask	Channel mask	Chess mask	Chess mask#mixtures	8	8	-Batch size	1024	1024	1024Training iterations	100k	100k	100kOptimizer	{Adam, RAdam}	RAdam	{SGD, Adam, RAdam}Learning rate	7.5e-4	7.5e-4	{1e-3, 1e-4, 1e-5}Learning rate decay	0.999975	0.999975	0.999975Temperature (GS)	-	-	{01, 0.2, 0.5}D.2 Graph coloringDataset details In our experiments, we focus on the 3-color problem meaning that a graph has to becolored using K = 3 colors. We generate the datasets by randomly sampling a graph and using anSAT solver2 for finding one valid coloring assignment. In case no solution can be found, we discard
Table 7: Hyperparameter overview for graph coloring experiments presented in Table 2Hyperparameters	GraphCNF	Variational AE	AutoregressiveLatent dimension	{2,4}∕{2, 4,6,8}	4	-#Coupling layers	{6,8}	-	-(Coupling) network	GAT	GAT	GAT- Number of layers	{3,4,5}	5	{3,4, 5, 6,7}- Hidden size	384	384	384- Number of heads	4	4	4Mask	Channel mask	-	-#mixtures	{4, 8, 16}∕{4,8,16}	-	-Batch size	384/ 128	384/ 128	384 / 128Training iterations	200k	200k	100kOptimizer	RAdam	RAdam	RAdamLearning rate	7.5e-4	7.5e-4	7.5e-4KL scheduler	-	{1.0, 0.1→0.5, 0.1→1.0}	-Table 8: Results on the graph coloring problem, including standard deviation over 3 seeds. Thecolumn time is excluded since the execution time per batch is constant over seeds.
Table 8: Results on the graph coloring problem, including standard deviation over 3 seeds. Thecolumn time is excluded since the execution time per batch is constant over seeds.
Table 9: Hyperparameter overview for molecule generation experiments presented in Table 1 and 5Hyperparameters	GraphCNFLatent dimension (V/E)	{4,6,8}∕{2, 3,4}#Coupling layers (f1/f2/f3)	4∕{4,6}∕{4,6}Coupling network (f1/f2,3)	Relational GCN / Edge-GNN- Number of layers (f1/f2/f3)	{3/3/3, 3/4/4, 4/4/4}- Hidden size (V/E)	{256, 384} / {128, 192}Mask	Channel mask#mixtures (V/E)	{8, 16}/{4, 8, 16}Batch size (Zinc250k/Moses)	64/96Training iterations	150kOptimizer	RAdam (Liu et al., 2020)Learning rate	2e-4, 5e-4, 7.5e-4, 1e-3D.4 Language modelingDataset details The three datasets we use for language modeling are the Penn Treebank (Marcuset al., 1994), text8 and Wikitext103 (Merity et al., 2017). The Penn Treebank with a preprocessing ofMikolov et al. (2012) consists of approximately 5M characters and has a vocabulary size of K = 51.
Table 10: Hyperparameter overview for the language modeling experiments presented in Table 11Hyperparameters	Penn Treebank	text8	Wikitext103(Max) Sequence length	288	256	256Latent dimension	{2,3,4}	{2,3,4}	{8,10, 12}#Coupling layers	1	1	1Coupling network	LSTM	LSTM	LSTM- Number of layers	1	2	2- Hidden size	1024	1024	1024- Dropout	{0.0, 03}	0.0	0.0- Input dropout	{0.0, 0.05, 01, 0.2}	{0.0, 0.05, 0.1}	{0.0, 0.05, 0.1}#mixtures	51	27	64Batch size	128	128	128Training iterations	100k	150k	150kOptimizer	RAdam	RAdam	RAdamLearning rate	7.5e-4	7.5e-4	7.5e-4Table 11: Results on language modeling. The reconstruction error is shown in brackets.			Model	Penn Treebank	text8	Wikitext103LSTM baseline	1.28 ±0.01	1.44 ±0.01	4.81 ±0.05Latent NF - 1 layer	1.30±0.01 (0.01)	1.61±0.02 (0.03)	6.39±0.19 (1.78)Categorical NF - 1 layer	1.27 ±0.01 (0.00)	1.45 ±0.01 (0.00)	5.43 ±0.09 (0.32)
Table 11: Results on language modeling. The reconstruction error is shown in brackets.			Model	Penn Treebank	text8	Wikitext103LSTM baseline	1.28 ±0.01	1.44 ±0.01	4.81 ±0.05Latent NF - 1 layer	1.30±0.01 (0.01)	1.61±0.02 (0.03)	6.39±0.19 (1.78)Categorical NF - 1 layer	1.27 ±0.01 (0.00)	1.45 ±0.01 (0.00)	5.43 ±0.09 (0.32)D.5 Real-World experimentsTo verify that Categorical Normalizing Flows can also be applied to real-world data, we have experi-mented on a credit-card risk record (Dua and Graff, 2019). The data contains different categoricalattributes regarding potential credit risk, and we have used the following 9 attributes: “check-ing_status”, “credit_history”, “savings_status”, “employment”, “housing”, “job”, “own_telephone”,“foreign_worker”, and “class”. The task is to model the joint density function of those 9 attributesover a dataset of 1000 entries. We have used the first 750 entries for training, 100 for validation, and150 for testing. The results are shown in Table 12. Both the LatentNF and CNF perform equally well,given the small size of the dataset and number of categories. The results have been averaged overthree seeds. However, we experienced a relatively high standard deviation due to the small size of thetraining and test set.
Table 12: Results on credit card risk dataset (Dua and Graff, 2019). The reconstruction error is shownin brackets where applicable.
