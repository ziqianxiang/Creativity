Table 1: Relationship between Φz and transformer self-attention. k : a function describing how the transformerintegrates positional information; n: sequence length;q : number of references or attention heads; d: di-mension of the embeddings; p: number of supports inz. Typically, p d. In recent transformer architec-tures, positional encoding requires learning additionalparameters (〜qd2).
Table 2: Classification accuracy (top 1/5/10) on test set for SCOP 1.75 for different unsupervisedand supervised baselines, averaged from 10 different runs (q references × p supports).
Table 3: Results for prediction of chromatin profiles on the DeepSEA dataset. The metrics are areaunder ROC (auROC) and area under PR curve (auPRC), averaged over 919 chromatin profiles. Dueto the huge size of the dataset, We only provide results based on a single run.
Table 4: Classification accuracies for SST-2 reported on standard validation set, averaged from 10different runs (q references X P supports).
Table 5: Classification accuracy (top 1/5/10) results of our unsupervised embedding for SCOP 1.75with pre-trained ESM models (Rives et al., 2019).
Table 6: Classification accuracies for 5000 samples of CIFAR-10 using CKN features (Mairal, 2016)and forming Gram matrix. A random baseline would yield 10%.
Table 7: Hyperparameter search range for CIFAR-10Hyperparameter	Search rangeEntropic regularization ε	[1.0; 0.1; 0.01; 0.001]Position encoding bandwidth σpos	[0.5; 0.6; 0.7; 0.8; 0.9; 1.0]Table 8: Classification results using unsupervised representations for CIFAR-10 for two featureconfigurations (extracted from a 2-layer unsipervised CKN with different number of filters). Weconsider here our embedding with one reference and different number of supports, learned withK-means, with or without position encoding (PE).
Table 8: Classification results using unsupervised representations for CIFAR-10 for two featureconfigurations (extracted from a 2-layer unsipervised CKN with different number of filters). Weconsider here our embedding with one reference and different number of supports, learned withK-means, with or without position encoding (PE).
Table 9: Hyperparameter search grid for SCOP 1.75Hyperparameter	Search rangeε for Sinkhorn	[1.0; 0.5; 0.1; 0.05; 0.01]λ for classifier (unsupervised setting)	1/2range(5,20)λ for classifier (supervised setting) [1e-6;1e-5;1e-4;1e-3]Table 10: Hyperparameter search grid for SCOP 1.75 baselines.
Table 10: Hyperparameter search grid for SCOP 1.75 baselines.
Table 11: Classification accuracy (top 1/5/10) results of our unsupervised embedding for SCOP 1.75.
Table 12: Classification accuracy (top 1/5/10) results of our unsupervised embedding for SCOP1.75. We show the results for different number of Nystrom anchors. The number of references andsupports are fixed to 1 and 100.
Table 13: Classification accuracy (top 1/5/10) of supervised models for SCOP 1.75. The accuraciesobtained by averaging 10 different runs. We show the results of using either one reference with 50supports or 5 references with 10 supports. Here DeepSF is a 10-layer CNN model.
Table 14: Classification accuracy (top 1/5/10) results of our supervised embedding for SCOP 1.75.
Table 15: Model architecture for DeePSEA dataset.
Table 16: Results for Prediction of chromatin Profiles on the DeePSEA dataset. The metrics arearea under ROC (auROC) and area under PR curve (auPRC), averaged over 919 chromatin Profiles.
Table 17: Accuracies on standard validation set for SST-2 with our unsupervised features dependingon the number of references and supports. The references were computed using K-means on samplesfor multiple references and K-means on patches for multiple supports. The size of the input BERTfeatures is (length × dimension). The accuracies are averaged from 10 different runs. (q referencesX P supports)BERT Input Feature Size	(30 × 768)		(66 × 768)	Features	Pre-trained	Fine-tuned	Pre-trained	Fine-tuned[CLS]	84.6±0.3	90.3±0.1	86.0±0.2	92.8±0.1Flatten	84.9±0.4	91.0±0.1	85.2±0.3	92.5±0.1Mean pooling	85.3±0.3	90.8±0.1	85.4±0.3	92.6±0.2Φz (1 × 3)	85.5±0.1	90.9±0.1	86.5±0.1	92.6±0.1Φz (1 × 10)	85.1±0.4	90.9±0.1	85.9±0.3	92.6±0.1Φz (1 × 30)	86.3±0.3	90.8±0.1	86.6±0.5	92.6±0.1Φz (1 × 100)	85.7±0.7	90.9±0.1	86.6±0.1	92.7±0.1Φz (1 × 300)	86.8±0.3	90.9±0.1	87.2±0.1	92.7±0.1Table 18: Hyperparameter search grid for SST-2.
Table 18: Hyperparameter search grid for SST-2.
Table 19: Hyperparameter search grid for SST-2 baselines.
Table 20: Classification accuracy on standard validation set of supervised models for SST-2, withpre-trained BERT (30 × 768) features. The accuracies of our embedding were averaged from 3 dif-ferent runs before being run 10 times for the best results for comparison with baselines, cf. Section 5.
Table 21: Classification accuracy on standard validation set of all baselines for SST-2, with pre-trained BERT (30 × 768) features, averaged from 10 different runs.
