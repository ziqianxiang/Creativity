Table 1: The R2 scores for full reconstruction (FR), full reconstruction with PI (FR+PI), maskedreconstruction (MR) and DAPC (MR+PI). We also demonstrate the improvements in percentagebrought by adding PI. On average, PI improves full reconstruction by 4.09% and masked recon-struction by 17.22%. Additionally, in the last column, we show PI objective alone is not powerfulenough to learn predictive components (R2 scores are low).
Table 2: Ablation study on different variantsof DAPC. We give WERs (%) of ASR mod-els pretrained with different variants on WSJ.
Table 3: WERs (%) obtained by ASR models pre-trained with different representation learning meth-ods on the test_clean partition of Librispeech. Mod-els are pretrained on 960h unlabeled data and fine-tuned on 100h labeled data. Our results are averagedover 3 random seeds.
Table 4:	The R2 scores of recovered 3D trajectory of noisy Lorenz attractor by different methods.
Table 5:	The R2 scores for the ablation study of (deterministic) DAPC for Lorenz attractor.
Table 6: The R2 scores for full reconstruction only and full reconstruction with PI.
Table 7: The R2 scores for CPC with different temporal lags (k).
Table 8: The R2 scores for CPC with different temporal lags (k) on the temperature dataset.
Table 9: WER results of different methods on LibriSpeech. All representation methods are pre-trained on the full corpus (960h) and finetuned on train_clean_100 (100h). For MR and DAPC, thedefault ASR recipe uses characters as token set and decodes with word RNNLM. For results denotedwith “(sub-word)”, the ASR recipe uses 5000 unigrams as token set and decodes with token-levelRNNLM. All the results are averaged over 3 seeds.
Table 10: WERs of models pretrained on 81h split si284 and finetuned on 81h split si284.
Table 11: We compare different methods based on their key features: generative/discriminative,contrastive/non-contrastive, whether using past-future mutual information estimation (p-f MI), andmasking.
