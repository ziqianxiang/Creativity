Table 1: Comparison OfLA-GAN with its respective baselines AltGAN and ExtraGrad (see § 5.1 for naming),using FID (lower is better) and IS (higher is better), and best obtained scores. EMA denotes exponential movingaverage, see § F. All methods use Adam, see § G for detailed description. Results are averaged over 5 runs. Werun each experiment for 500K iterations. See § H and § 5.2 for details on architectures and hyperparameters andfor discussion on the results, resp. Our overall best obtained FID scores are 12.19 on CIFAR-10 and 2.823 onSVHN, see § I for samples of these generators. Best scores obtained for each metric and dataset are highlightedin yellow. For each column the best score is in bold along with any score within its standard deviation reach.
Table 2: Summary of the recently reported best scores on CIFAR-10 and benchmark with LA-GAN, usingpublished results. Note that the architectures are not identical for all the methods-see § 5.2.
Table 3: FID (loWer is better) results on MNIST, averaged over 5runs. Each experiment is trained for 100K iterations. Note thatUnrolled-GAN is computationally more expensive: in the orderof the ratio 4 : 22-as We used 20 steps of unrolling What gave bestresults. See 5.2 & H for implementation and discussion, resp.
Table 4: List of hyperparameters used in Figure 4. η denotes the learning rate, β1 is defined in equation 5, and αand k in Alg. 1.
Table 5: DCGAN architectures (Radford et al., 2016) used for experiments on MNIST. We use ker and pad todenote kernel and padding for the (transposed) convolution layers, respectively. With h×w we denote the kernelsize. With cin → yout we denote the number of channels of the input and output, for (transposed) convolutionlayers.
Table 6: ResNet blocks used for the ResNet architectures (see Table 7), for the Generator (left) and theDiscriminator (right). Each ResNet block contains skiP connection (byPass), and a sequence of convolutionallayers, normalization, and the ReLU non-linearity. The skip connection of the ResNet blocks for the Generator(left) upsamples the input using a factor of 2 (We use the default PyTorch upsampling algorithm-nearestneighbor), whose output is then added to the one obtained from the ResNet block listed above. For clarity welist the layers sequentially, hoWever, note that the bypass layers operate in parallel With the layers denoted as“feedforWard” (He et al., 2016). The ResNet block for the Discriminator (right) differs ifit is the first block in thenetWork (folloWing the input to the Discriminator), ` = 1, or a subsequent one, ` > 1, so as to avoid performingthe ReLU non-linearity immediate on the input.
Table 7: Deep ResNet architectures used for experiments on ImageNet, SVHN and CIFAR-10, Where G-ResBlock and D-ResBlock for the Generator (left) and the Discriminator (right), respectively, are described inTable 6. The models’ parameters are initialized using the Xavier initialization (Glorot & Bengio, 2010). ForImageNet experiments, the generator’s input is of dimension 512 instead of 128.
Table 8: Hyperparameters used on MNIST.
Table 9: Hyperparameters used on SVHN.
Table 10: Hyperparameters that we used for our experiments on CIFAR-10.
Table 11: Hyperparameters used for our AltGAN experiments on ImageNet.
Table 12: Hyperparameters used for our ExtraGrad experiments on ImageNet.
Table 13: Comparison of the LA-GAN optimizer with its respective baselines AltGAN and ExtraGrad (see § 5.1for naming), using FID (lower is better) and IS (higher is better). EMA denotes exponential moving average(with fixed β = 0.9999, see § F). With suffix -R and -A We denote that We use RAdam (Liu et al., 2020) andAdam (Kingma & Ba, 2015) optimizer, respectively. Results are averaged over 5 runs. We run each experimenton MNIST for 100K iterations, and for 500K iterations for the rest of the datasets. See § H and § 5.2 for detailson architectures and hyperparameters and for discussion on the results, resp.
