Table 1: Evaluation results for unanimously-annotated on-screen and off-screen mixtures. Traininguses unsupervised or semi-supervised examples, with either 0% or 25% synthetic off-screen (SOff)examples (Section 4.2). Cross-entropy (CE) losses include active combinations (AC), multipleinstance (MI), and exact. Note that “MixIT*” indicates SI-SNR of an oracle estimate derived usingMixIT with reference mixtures, and Xon is the on-screen estimate produced by mixing separatedsources with classifier probabilities. On-screen MoMs have a median input SI-SNR of 4.4 dB.
Table 2: Audio-visual enhancement results on the Mandarin test set (Hou et al., 2018).
Table 3: Audio-visual separation results on AudioSet-SingleSource musical instruments test set (Gao& Grauman, 2019).
Table 4: Audio-visual separation results on MUSIC test set (Zhao et al., 2018)Method	In-Domain	Supervised	SDR	SIR	SARSound-of-Pixels (Zhao et al., 2018)	✓	✓	5.4	11.0	9.8Sound-of-Motions (Zhao et al., 2019)	✓	✓	4.8	11.0	8.7MP-Net Xu et al. (2019)	✓	✓	5.7	11.4	10.4Co-Separation (Gao & Grauman, 2019)	✓	✓	7.4	13.7	10.8Cascaded Opponent Filter (Zhu & Rahtu, 2020b)	✓	✓	10.1	16.7	13.0A(Res-50, att) + S(DV3P) (Zhu & Rahtu, 2020a)	✓	✓	9.4	15.6	12.7A(Res-50, class.) + S(DV3P) (Zhu & Rahtu, 2020a)	✓	✓	10.6	17.2	12.8AudioScope Xon	X	X	-0.5	2.8	11.2AudioScope source with max ym	X	X	-2.0	3.3	7.6AudioScope best source (oracle)	X	X	7.1	14.9	12.5AudioScope MixIT* (oracle)	X	X	8.8	13.0	13.1We see a similar pattern compared to the results for AudioSet-SingleSource in Table 3: non-oraclemethods that use the predicted on-screen probability ym do not perform very well. However, oracleselection of the best source, or oracle remixing of the sources, both achieve better performance thana number of recent specialized supervised in-domain systems from the literature, though they donot achieve state-of-the-art performance. These results seem to suggest that the predictions ym areless accurate for this restricted-domain task, but the excellent oracle results suggest potential. Inparticular, non-oracle performance could improve if the classifier were more accurate, perhaps by
Table 5: Ablations related to audio and video embeddings.
Table 6: Ablations related to attentional pooling run for the unsupervised setting.
Table 7: Ablations for different data configurations.
Table 8: Ablations for number of max output sources.
Table 9: Results for baseline two-output separation model without on-screen classifier.
Table 10: TDCN++ separation network architecture for an input mixture waveform correspondingto a time-length of 5 seconds, sampled at 16 kHz. The output of the separation network are M = 4separated sources. The dilation factor for each block is defined as Di = 2mod(i,8) , i = 0, . . . , 31.
Table 11: Audio and image embedding network architectures for an input segment log-mel spec-trogram corresponding to a 0.96 seconds, sampled at 16 kHz and an input image represented as anRGB tensor with shape 128 × 128 × 3, respectively. The log-mel spectrogram input to the audioembedding network has a number of input channels Cin = 1 and the input video frame to the imageembedding network has a number of input channels Cin = 3. Each depth-wise (DW) convolution orregular convolution is followed by a batch normalization layer and a ReLU activation. * denotes thelayer from which the local 8 × 8 spatial feature map is extracted, as described in Section 3.3.
Table 12: Results of human annotation task on on-screen MoM test set.
