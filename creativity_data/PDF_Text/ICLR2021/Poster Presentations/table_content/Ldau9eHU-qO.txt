Table 1: All labels and modalities used in thedemonstrations.
Table 2: Accuracy (mean and std deviation; higher mean is better) and MAE (lower is better) forsampled trajectories under all models, conditioned on a fixed spatial label. Bold numbers designatewhen the weak version of the model (utilising the weak user labels) outperforms the non-weak one.
Table 3: Accuracy (mean and std deviation) for sampled trajectories under all models, conditionedon a fixed effort label. Bold numbers designate when the weak version of the model (utilising theweak user labels) outperforms the non-weak one.
Table 4: All labels and modalities used in the ad-ditional pouring demonstrations.
Table 5: Network architectures used for the reported models. (a) is a 2D convolutional network, (b)is a 1D convolutional network, (c) is a fully-connected MLP network, (d) is a Temporal ConvolutionNetwork, made of stacked temporal blocks and dilated convolutions, described in Bai et al. (2018)MLPFC (2x8) μ,lοg(σ)FC (32)FC (32)Concatenated [i; T][1 X 36](c) MLPFor all experiments, the values (unless when set to 0) for the three coefficients from Equation 9 are:• α = 1, β = 0.1, γ = 10The values are chosen empirically in a manner such that all the loss terms have similar magnitudeand thus none of them overwhelms the gradient updates while training the full model.
