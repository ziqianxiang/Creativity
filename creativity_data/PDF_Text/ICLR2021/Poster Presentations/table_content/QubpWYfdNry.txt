Table 1: Results summary for the Inverted Pendulum and Reacher environment realmsAlgorithms evaluated:No differencesDiferenCes between the agent and the expert domainsEmbodimentAppearanceEmbodiment and appearanceDisentanGAILReacher Inverted Pendulum Reacher Inverted Pendulum Reacher Inverted Pendulum Reacher Inverted Pendulum0.973 ± 0.0741.021 ± 0.023DisentanGAIL (No prior) 1.004 ± 0.012 1.015 ± 0.023TPIL	0.251 ± 0.111 0.812 ± 0.1620.941 ± 0.045 0.954 ± 0.081TPIL (×5 experience)DisentanGAIL (DCL)No regularization0.683 ± 0.1580.894 ± 0.1340.988 ± 0.042
Table 2: Results summary for the ‘high dimensional’ environment realmsEnvironment realmsAlgorithms evaluated:	Hopper	Half-Cheetah	7DOF-Pusher	7DOF-StrikerDisentanGAIL	0.749 ± 0.026	0.712 ± 0.036	0.901 ± 0.044	0.921 ± 0.061DisentanGAIL (No prior)	0.622 ± 0.051	0.660 ± 0.231	0.677 ± 0.072	0.707 ± 0.150TPIL	0.362 ± 0.057	0.066 ± 0.020	-0.020 ± 0.068	0.081 ± 0.065DisentanGAIL (DCL)	0.619 ± 0.036	0.486 ± 0.058	0.747 ± 0.054	0.554 ± 0.134No regularization	0.543 ± 0.039	0.247 ± 0.052	0.657 ± 0.080	0.504 ± 0.069No regularization (source)	0.866 ± 0.100	0.914 ± 0.041	0.901 ± 0.044	0.837 ± 0.0387DOF-Striker. We use DisentanGAIL to perform observational imitation with the ‘source’ and ‘tar-get’ environments differing greatly both in terms of appearance and agent embodiment, as detailedin Appendix C. We compare DisentanGAIL with the previously-introduced baselines. To providean upper bound on the expected performance of DisentanGAIL, we additionally evaluate the No la-tent representation regularization baseline with the ‘observer’ agent learning in the original ‘source’environment, i.e., imitating with no domain differences.
Table 3: Example visual trajectories collected in the sample taskvisual trajectories	{x0y0, x1y1, x2y2}	BE:	{10,11,11} {10, 11, 11} {10, 11, 11} {10,11,11}Bπ:	{00,00,00} {00, 01, 00} {00, 00, 01} {00, 00, 00}Previous work (Stadie et al., 2017) proposed to optimize the GAIL objective described in section 6,subject to constraining the mutual information to be 0:arg max JG(θ, BE ∪ Bπ) s.t. I(zi, di|BE ∪ Bπ) = 0.
Table 4: Environment-realm specific hyper-parametersRealm	Pθ1	Sθ2	Tφ	|BE|	|BP.x|	∣B∏ I	|T |Inverted Pendulum	2 × {16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {1 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {32-FC, ReLU} {1-FC, Sigmoid}	2 × {32-FC, Tanh} {1-FC}	10000	10000	10000	50Reacher	2 × {16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {1 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {32-FC, ReLU} {1-FC, Sigmoid}	2 × {32-FC, Tanh} {1-FC}	10000	10000	10000	50Hopper	{16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {48 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	20000	20000	100000	200Half-Cheetah	{16 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {48 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	20000	20000	100000	2007DOF-Pusher	{24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {40 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {64 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	10000	10000	100000	2007DOF-Striker	{24 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {32 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {40 × (3, 3)-conv, Tanh, (2, 2)-MaxPool} {64 × (3, 3)-conv, (2, 2)-MaxPool}	2 × {100-FC, ReLU} {1-FC, Sigmoid}	2 × {128-FC, Tanh} {1-FC}	10000	10000	100000	200Mutual information learning for as many iterations as the number of time-steps collected. Then,by averaging the mutual information estimates accumulated from performing (ii), we update thecoefficients β and λ. Finally, we also perform (iii) Agent learning for as many iterations as thenumber of time-steps collected. We utilize the same agent set of visual trajectories Bπ in all differentlearning steps. A formal summary of DisentanGAIL is reported below in Algorithm 1.
Table 5: Description of the implemented environment realmsRealm	Environment	Characteristics dim(A)	dim(O)	Semantic goalInverted Pendulum 1-Linked Inverted Pendulum 2-Linked Inverted Pendulum 1-Linked Colored Inverted Pendulum 2-Linked Colored Inverted Pendulum		1 link, standard coloring	1 2 links, standard coloring	1 1 link, alternative coloring	1 2 link, alternative coloring	1	32 × 32 × 3	Keeping the links vertically balanced above the moving cart.
Table 6:	Results summary for DisentanGAIL with tighter expert demonstration constraintsDiferenceS between the agent and the expert domainsNo differences	Embodiment	Appearance	Embodiment and appearanceAlgorithms evaluated:	ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted PendulumDisentanGAIL-Imax = 0.99 0.973 ± 0.074 1.021 ± 0.023 0.941 ± 0.045 0.954 ± 0.081 0.885 ± 0.064^^0.894 ± 0.231 ^^0.860 ± 0.081 0.918 ± 0.115DisentanGAIL-Imax = 0.75 0.983 ±	0.067	1.019 ± 0.021	0.841 ± 0.113	0.892 ± 0.131	0.903	±	0.064	0.887 ±	0.180	0.897 ± 0.056	0.772 ± 0.200DisentanGAIL-Imax = 0.500.987 ±	0.045	1.013 ± 0.016	0.885 ± 0.104	0.853 ± 0.208	0.861	±	0.077	0.942 ±	0.131	0.837 ± 0.071	0.790 ± 0.234DisentanGAIL-Imax = 0.25 0.975 ±	0.028	1.020 ± 0.025	0.927 ± 0.052	0.882 ± 0.173	0.861	±	0.088	0.930 ±	0.156	0.848 ± 0.055	0.720 ± 0.251DisentanGAIL-Imax = 0.010.921 ±	0.094	0.992 ± 0.041	0.905 ± 0.057	0.790 ± 0.201	0.652 ±	0.188	0.589 ±	0.224	0.576 ± 0.141	0.630 ± 0.345Table 7:	Results summary for DisentanGAIL with missing domain information disguising preven-tion techniquesDifferences between the agent and the expert domainsNo differences	Embodiment	Appearance	Embodiment and appearanceAlgorithms evaluated:	ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted PendulumDisentanGAIL	0.973 ± 0.074	1.021	± 0.023	0.941	± 0.045	0.954 ± 0.081	0.885 ± 0.064	0.894	± 0.231	0.860	±	0.081	0.918 ± 0.115DisentanGAIL (No SN)	0.957 ± 0.084	1.018	± 0.024	0.844	±	0.105	0.937 ± 0.094	0.853 ± 0.092	0.871	± 0.153	0.861	±	0.049	0.864 ± 0.184DisentanGAIL (No 2St) 0.982 ± 0.075	1.020	± 0.023	0.907	± 0.109	0.900 ± 0.156	0.857 ± 0.127	0.799	± 0.199	0.858	±	0.044	0.789 ± 0.182DisentanGAIL (No Prev) 0.949 ± 0.105	1.002	± 0.022	0.866	± 0.056	0.969 ± 0.085	0.768 ± 0.115	0.778	± 0.235	0.763 ±	0.100	0.750 ± 0.225Second, we analyze directly the effects that tighter constraints have on the performance of Disen-tanGAIL. The performance curves for different values of Imax are shown in Fig. 7 and a summary
Table 7:	Results summary for DisentanGAIL with missing domain information disguising preven-tion techniquesDifferences between the agent and the expert domainsNo differences	Embodiment	Appearance	Embodiment and appearanceAlgorithms evaluated:	ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted Pendulum ReaCher Inverted PendulumDisentanGAIL	0.973 ± 0.074	1.021	± 0.023	0.941	± 0.045	0.954 ± 0.081	0.885 ± 0.064	0.894	± 0.231	0.860	±	0.081	0.918 ± 0.115DisentanGAIL (No SN)	0.957 ± 0.084	1.018	± 0.024	0.844	±	0.105	0.937 ± 0.094	0.853 ± 0.092	0.871	± 0.153	0.861	±	0.049	0.864 ± 0.184DisentanGAIL (No 2St) 0.982 ± 0.075	1.020	± 0.023	0.907	± 0.109	0.900 ± 0.156	0.857 ± 0.127	0.799	± 0.199	0.858	±	0.044	0.789 ± 0.182DisentanGAIL (No Prev) 0.949 ± 0.105	1.002	± 0.022	0.866	± 0.056	0.969 ± 0.085	0.768 ± 0.115	0.778	± 0.235	0.763 ±	0.100	0.750 ± 0.225Second, we analyze directly the effects that tighter constraints have on the performance of Disen-tanGAIL. The performance curves for different values of Imax are shown in Fig. 7 and a summaryof the results is given in Table 6. Overall, DisentanGAIL appears to be quite robust to all settingstested, excluding the extreme Imax = 0.01. In general, however, a lower mutual information upper-limit appears to have a negative effect on the performance in most experiments, especially when the‘expert’ agent’s embodiment and the ‘observer’ agent’s embodiment differ. This is likely because atighter constraint does not permit the discriminator to utilize enough information about single ob-servations, thus, providing a less informative learning signal to finetune the agent’s behavior. Theeffects of varying Imax appear to be less accentuated in the experiments performed in the ReaCherrealm. This is likely because the exploratory policy in this environment covers a greater range ofstates than in the Inverted Pendulum realm, with a more diverse range of goal-Completion levels.
Table 8:	Results summary for the experiments considering further background domain differencesin the ‘target’ environmentsEnvironment realmsAlgorithms evaluated:Hopper7DOF-PusherDisentanGAIL0.709 ± 0.0780.835 ± 0.039DisentanGAIL (original target) 0.749 ± 0.026	0.901 ± 0.044We present the performance curves in Fig. 9 and a summary of the results in Table 8. The ob-tained results show that background domain differences have a limited effect on DisentanGAIL’sfinal performance. However, they have a more prominent effect on DisentanGAIL’s efficiency, mak-ing it converge in an increased number of epochs. This is particularly noticeable in the Hopperrealm’s results. We hypothesize this is because in the new target environments there is a greateramount of domain information that requires to be ‘disentangled’ from the useful goal-completioninformation. For example, in the locomotion realms, the pre-processor encodes goal-completioninformation about the relative position of the agent with respect to the floor tiles in the two envi-ronments (as empirically suggested in Section D.1). In the new target environment of the Hopperrealm, this information needs to be also disentangled from domain information regarding the tiles’
