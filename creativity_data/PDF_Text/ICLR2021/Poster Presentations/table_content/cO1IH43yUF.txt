Table 1: Mean test performance and standard deviation. We compare fine-tuning with the completeBERT model (Standard) and fine-tuning with the partially re-initialized BERT (Re-init). We showresults of fine-tuning for 3 epochs and for longer training (Sec 6). We underline and highlight in bluethe best and number statistically equivalent to it among each group of 4 numbers. We use a one-tailedStudentâ€™s t-test and reject the null hypothesis when p < 0.05.
Table 2: Mean test performance and standard deviation on four datasets. Numbers that are statisticallysignificantly better than the standard setting (left column) are in blue and underlined. The results ofRe-init and Longer are copied from Table 1. All experiments use Adam with debiasing (Section 4).
Table 3: The datasets used in this work. We apply non-standard data splits to create test sets. SCCstands for Spearman Correlation Coefficient and MCC stands for Matthews Correlation Coefficient.
Table 4: Comparing BERT fine-tuning with mixed precision and full precision. The differencebetween the two numbers on any dataset is not statistically significant.
Table 5: Comparing BERT fine-tuning with and without bias correction on the MNLI dataset. Whenwe have a large dataset, there is no significant difference in using bias correction or not.
Table 6: Fine-tuning hyper-parameters of BERT and its variants as reported in the official repositoryof each model.
Table 7: Average Test performance with standard deviation on four small datasets with four differentpre-trained models. For each setting, the better numbers are bolded and are in blue if the improvementis statistically significant.
