Table 1: Results in TREC 2019 Deep Learning Track. Results not available are marked as “n.a.”,not applicable are marked as “-”. Best results in each category are marked bold. Dense Retrievalbaselines use the same BERT-Siamese but different training strategies.
Table 2: Retrieval results (Answer Coverage at Top-20/100)on Natural Questions (NQ) and Trivial QA (TQA) in thesetting from Karpukhin et al. (2020).
Table 3: Relative gains in thefirst stage retrieval of a commercialsearch engine. The gains are fromchanging the training of a produc-tion DR model to ANCE.
Table 4: OpenQA Test Scores in Single TaskSetting. ANCE+Reader switches the retrieveof the OpenQA systems from DPR to ANCEand keeps their QA components, which isRAG-Token on Natural Questions (NQ) andDPR Reader on Trivia QA (TQA). T5 resultsare "closed-book". The others are open-book.
Table 5: Efficiency of ANCE Serving and Training.
Table 6: Coverage of TREC 2019 DL Track labels on Dense Retrieval methods. Overlap with BM25is calculated on top 100 retrieved documents.
Table 7: Results of different hyperparameter configurations. “Top K Neg” lists the top k denseretrieved candidates from which we sampled the ANCE negatives from.
Table 8: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs betterthan BM25. Snippets are manually extracted. The documents at the first disagreed ranking positionsare shown. ANCE won on all of them. The NDCG@10 of ANCE and BM25 in the correspondingquery is listed.
Table 9: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performsworse than BM25. Snippets are manually extracted. The documents at the first positions where BM25wins are shown. The NDCG@10 of ANCE and BM25 on the corresponding query is listed. Typos inthe query are from the realist web search queries in TREC.
