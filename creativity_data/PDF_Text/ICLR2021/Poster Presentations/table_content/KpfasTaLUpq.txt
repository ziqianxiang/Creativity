Table 1: Analysis of transformers. Time complex. indicates time complexity when full paralleliza-tion is assumed. N: source/target length; E: encoder depth; D: decoder depth; T: # NAR iterations.
Table 2: Test BLEU and speed comparisons with varying numbers of encoder (E) and decoder (D)layers on large bitext. Best performance is bolded.
Table 3: Test BLEU comparisons with iterative NAR methods. T indicates the average # iterations.
Table 4: Left: WMT14 EN→DE test results in BLEU using reordered English input. Right:WMT14 EN→DE test results in BLEU that analyze the effects of distillation in fast translationmethods. All distillation data are obtained from a transformer large. E : encoder depth; D: de-coder depth; T: # iterations. Imputer (Saharia et al., 2020) uses 12 self-attention layers over theconcatenated source and target, instead of the encoder-decoder architecture.
Table 5: Test BLEU and speed comparisons with varying numbers of encoder (E) and decoder (D)layers.
Table 6: Autoregressive (left) and non-autoregressive (right) fairseq hyperparameters and set-ting.
Table 7: Sample translation outputs from the ZH→EN validation data.
