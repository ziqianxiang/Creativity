Table 1: Shared hyperparametersHyperparameters	ValueDirichlet alpha	0.3Exploration fraction	0.25Exploration temperature	1.0Temperature decay schedule	5 × 103Temperature decay rate	0.95Replay capacity	5 × 105Min replay	105Sequence length	5C.1.1 MAZESFor Minipacman, we altered the environment to support the use of both procedurally generatedmazes (“in-distribution” mazes) and the standard maze (“out-of-distribution” mazes), both of size15x19. Figure 6 shows example mazes of both types. In all experiments except generalization, wetrained agents on an unlimited number of the “in-distribution” mazes, and tested on other mazes alsodrawn from this set. For the generalization experiments, we trained agents on a fixed set of either 5,10, or 100 “in-distribution” mazes. Then, at evaluation time, we either tested on mazes drawn fromthe full in-distribution set or from the out-of-distribution set.
Table 2: Hyperparameters for MinipacmanHyperparameters	ValueLearning rate	10-3Discount factor	0.97Batch size	512n-step return length	10Replay samples per insert ratio	0.25Learner steps	2 × 105Policy loss weight	1.
Table 3: Hyperparameters for AtariHyperparameters	ValueLearning rate	10-3Discount factor	0.995Batch size	2048n-step return length	10Replay samples per insert ratio	0.25Learner steps	1.5 × 105Policy loss weight	1.
Table 4: Hyperparameters for control suite.
Table 5: Hyperparameters for SokobanHyperparameters	ValueLearning rate	10-3Discount factor	0.99Batch size	2048n-step return length	10Replay samples per insert ratio	0.4Learner steps	3 × 105Policy loss weight	1.
Table 6: Hyperparameters for GoHyperparameters	ValueLearning rate	4 × 10-4Discount factor	-1Batch size	16λ	0.99Learner steps	105Policy loss weight	1.0Value loss weight	0.25Num simulations	150Dirichlet alpha	0.25Exploration fraction	0.4D Additional Results and AnalysisD.1 Extensions to figures in main paperFigure 10 shows the same information as Figure 3 (contributions of search to performance) but splitinto separate groups and with error bars shown. Figure 11 shows the same information as Figure 5(effect of search at evaluation as a function of the number of simulations) but using breadth-firstsearch with a learned model. Figure 12 shows the same information as Figure 6 (effect of searchon generalization to new mazes) but for the in-distribution mazes instead of the out-of-distribution23
Table 7: Values obtained by the baseline vanilla MuZero agent (corresponding to the“Learn+Data+Eval” agent in Figure 3), computed from the average of the last 10% of scores seenduring training. Shown are the median across ten seeds, as well as the worst and best seeds. Medianvalues are used to normalize the results in Figure 3.
Table 8: Values obtained by MuZero at the very start of training (i.e., with a randomly initializedpolicy). Values are computed from the average of the first 1% of scores seen during training. Shownare the median across ten seeds, as well as the worst and best seeds. Median values are used tonormalize the results in Figure 3.
Table 9: Values obtained by a version of MuZero that uses no search at evaluation time (correspond-ing to the “Learn+Data” agent in Figure 3). Shown are the median across ten seeds, as well as theworst and best seeds. Median values are used to normalize the results in Figure 4.
Table 10: Values obtained by a baseline vanilla MuZero agent, evaluated offline from a checkpointsaved at the very end of training. For each seed, values are the average over 50 (control tasks andAtari) or 1000 episodes (Minipacman and Sokoban). These values are used to normalize the resultsin Figure 5 and Figure 6. Note that for Minipacman, the scores reported here are for agents that wereboth trained and tested on either the in-distribution mazes or the out-of-distribution mazes. Shownare the median across ten seeds, as well as the worst and best seeds.
Table 11: Values in Figure 3. Each column shows scores where 0 corresponds to the reward obtainedby a randomly initialized agent (Table 8) and 100 corresponds to full MuZero (“Learn+Data+Eval”,Table 7).
Table 12: Effect of the different contributions of search, modeled as Reward 〜 Environment+ TrainUpdate * TrainAct + TestAct over N = 400 data points, using the levels foreach variable as defined in the table in Figure 3. This ANOVA indicates that both the environment,model-based learning, model-based acting during training, and model-based acting during testingare all significant predictors of reward. We did not detect an interaction between model-based learn-ing and model-based acting during learning.
Table 13: Effect of tree depth, Dtree, modeled as Reward 〜 Environment * Iog(Dtree) overN = 375 data points. Where Dtree = ∞, we used the value for the maximum possible depth(i.e. the search budget). Top: this ANOVA indicates that both the environment and tree depthare significant predictors of reward, and that there is an interaction between environment and treedepth. Bottom: individual Spearman rank correlations between reward and log(Dtree) for eachenvironment. p-values are adjusted for multiple comparisons using the Bonferroni correction.
Table 14: Effect of exploration vs. exploitation depth, Duct, modeled as Reward 〜Environment * log(DUCT) over N = 375 data points. Where DUCT = ∞, we used thevalue for the maximum possible depth (i.e. the search budget). Top: this ANOVA indicates thatneither the environment nor exploration vs. exploitation depth are significant predictors of reward.
Table 15: Effect of the training search budget, B, on the strength of the policy prior, modeledas Reward 〜 Environment * log(B) + log(B)2 over N = 450 data points. Top: thisANOVA indicates that the environment and budget are significant predictors of reward, and thatthere is a second-order effect of the search budget, indicating that performance goes down with toomany simulations. Additionally, there is an interaction between environment and budget. Bottom:individual Spearman rank correlations between reward and log(B) for each environment. p-valuesare adjusted for multiple comparisons using the Bonferroni correction. Note that the correlation forGo does not include values for B > 50 (and thus is largely flat, since Go does not learn for smallvalues of B).
Table 16: Effect the evaluation search budget, B, on generalization reward when using the learnedmodel with MCTS, modeled as Reward 〜 Environment * log(B) over N = 350 datapoints. Top: this ANOVA indicates that the environment and budget are significant predictors ofreward, and that there is an interaction between environment and budget. Bottom: individual Spear-man rank correlations between reward and log(B) for each environment. p-values are adjusted formultiple comparisons using the Bonferroni correction.
Table 17: Effect the evaluation search budget, B, on generalization reward when using the simulatorwith MCTS, modeled as Reward 〜 Environment * log(B) over N = 350 data points. Top:this ANOVA indicates that the environment and budget are significant predictors of reward, andthat there is an interaction between environment and budget. Bottom: individual Spearman rankcorrelations between reward and log(B) for each environment. p-values are adjusted for multiplecomparisons using the Bonferroni correction.
Table 18: Rank correlations between the search budget, B, and generalization reward in Minipacmanfor different types of mazes and models. p-values are adjusted for multiple comparisons using theBonferroni correction.
Table 19: Effect the evaluation search budget (B), the number of unique training mazes (M), andtest level on generalization reward in Minipacman when using the simulator with MCTS, modeledas Reward 〜log(M) * log(B) + Test Level over N = 360 data points. ThisANOVAindicates that the both the number of training mazes and the search budget are significant predictorsof reward, and that there is an interaction between them.
