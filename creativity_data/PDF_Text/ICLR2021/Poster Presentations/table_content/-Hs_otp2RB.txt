Table 1: Robustness of downstream classification tasks under adversarial attack. We considerclassifiers trained either on the reconstructed image (denoted p(y∣X)) or on the latent representations(p(y∣z)). We show accuracy when the model is attacked, resulting in perturbed embeddings Z andreconstructions (x*). Parentheses show the drop in accuracy resulting from the attack - the smallerthe drop in magnitude the betterDataset	Task	Accuracy by Model				VAE	β -TCVAE	Seatbelt-VAESVHN	PMLP(yIx)	0.17 (-0.35)	0.22 (-0.29)	0.35 (-0.15)	PConv (y|X)	0.13 (-0.54)	0.36 (-0.28)	0.41 (-0.26)	pMLP(y|z)	0.15 (-0.57)	0.46 (-0.23)	0.57 (-0.21)CIFAR10	PMLP(yIx)	0.17 (-0.32)	0.25 (-0.21)	0.38 (-0.09)	PConv (y Ix)	0.07 (-0.37)	0.32 (-0.10)	0.34 (-0.07)	PMLP(yIz)	0.16 (-0.41)	0.26 (-0.23)	0.39 (-0.09)5.3	Protection to Downstream TasksFinally, we consider the protection that Seatbelt-VAEs might provide to downstream tasks, notingthat VAEs are often used as subcomponents in larger ML systems (Higgins et al., 2017b), or as amechanism to protect another model from attack (Schott et al., 2019; Ghosh et al., 2019). Table 1shows results for classification tasks using 2-layer MLPs and fully-convolutional nets trained on thereconstructions or on the embeddings. It shows the drop in accuracy caused by an adversary thatpicks a target with a different label and attacks the VAEs’ embedding using the attack objective with
