Table 1: Performance metrics of GVCL-F and GVCL compared to baselines (more in Appendix J).
Table 2: Relative performance improvement from adding FiLM layers on several benchmarks, forVI and non-VI based algorithms. VI-based approaches see a much more significantly gain overEWC, suggesting that FiLM layers synergize very well with VI and address the pruning issue.
Table 3: Best (selected) hyperparameters for continual learning experiments for variousalgorithms. We fix Online EWC’s γ = 1.
Table 4: Performance metrics of GVCL-F, GVCL and various baseline algorithms on Easy-CHASY.
Table 5: Performance metrics of GVCL-F, GVCL and various baseline algorithms on Hard-CHASY.
Table 6: Performance metrics of GVCL-F, GVCL and various baseline algorithms on Split-MNIST.
Table 7: Performance metrics of GVCL-F, GVCL and various baseline algorithms on Split-CIFAR.
Table 8: Performance metrics of GVCL-F, GVCL and various baseline algorithms on Mixed Visiontasks. Separate and joint training results for both MAP and β-VI models are also presentedApproaches(Ranked)→- HAT→- GVCL-F—Progressive—PathNet→- Online EWC-F→- Online EWC→- VCL-F—IMM-Mean—SGD-Frozen→- GVCL—IMM-Mode→- SGD→- VCL→- LWFJoint-(S-VI, FiLM)
Table 9:	ECE of all 8 mixed vision tasks for a model trained continually using GVCL-F or HAT.
