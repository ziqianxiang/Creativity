Table 1: Reconstruction NMSE in dB for NA-ALISTA with varying inputs (r(k) , u(k)) for different neuralnetwork architectures with K = 16, SNR= 40. For the LSTM it does not seem to matter which quantitieswe use to estimate the `1 -error, since all perform equally well. The MLP is outperformed by all recurrentarchitectures. Even though the Vanilla-RNN can perform on par with the LSTM, it suffers from serious traininginstability with training resulting in NaNs for N = 500 and N = 2000 (*) even when trained with a significantlysmaller learning rate, which lead us to use the stable LSTM in our experiments.
