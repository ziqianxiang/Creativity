Table 1: Computational cost of recentlyproposed fine-tuning algorithms. Weshow Forward Passes (FP), BackwardPasses (BP) as well as computation costas a factor of forward passes (xFP). Sis the number of gradient ascent steps,with a minimum of S ≥ 1Our method is significantly more computationally effi-cient than adversarial based fine-tuning methods, as seen in Table 1. We show that this efficiencydoes not hurt performance; we can match or exceed FreeLB and SMART on a large number of tasks.
Table 2: We present our best results on the GLUE development set for various fine-tuning methodsapplied to the RoBERTa Large model. On the left side table, we present our best numbers andnumbers published in other papers. On the right side, we present median numbers from 10 runs forthe mentioned methods.
Table 3: To remain consistent with prior experiments, we report an average of 5 runs of zero-shotsresults on the XNLI test set for our method applied to XLM-R Large. Various versions of ourmethod win over the majority of languages. The bottom row shows the current SOTA on XNLI,which requires the pre-training of a novel model.
Table 4: Our results on various summarization data-sets. We report Rouge-1, Rouge-2 and Rouge-Lper element in table. Following PEGASUS, we bold the best number and numbers within 0.15 ofthe best.
Table 5: Task specific hyper parameters for GLUE experimentsHyper parameter ValueOptimizer Adam-betas	Adam (0.9, 0.98)	Hyper parameter	ValueAdam-eps	1e-6	λ	[0.1,0.5, 1.0, 5.0]LR Scheduler	polynomial decay	Noise Types	[U, N]Dropout	0.1	σ	1e-5Weight Decay	0.01		Warmup Updates	0.06 * max updates		Table 6: Hyper parameters for R3F and R4F experiments on GLUEHyper Parameter	CNN/Dailymail	Gigaword	Reddit TIFUMax Tokens	1024	2048	2048Total updates	80000	200000	200000Warmup Updates	1000	5000	5000Table 7: Task specific hyper parameters for Summarization experiments.
Table 6: Hyper parameters for R3F and R4F experiments on GLUEHyper Parameter	CNN/Dailymail	Gigaword	Reddit TIFUMax Tokens	1024	2048	2048Total updates	80000	200000	200000Warmup Updates	1000	5000	5000Table 7: Task specific hyper parameters for Summarization experiments.
Table 7: Task specific hyper parameters for Summarization experiments.
Table 9: Hyper parameters for R3F and R4F experiments on XNLI.
Table 8: Hyper parameters for R3F and R4F experiments on Summarization experiments.
