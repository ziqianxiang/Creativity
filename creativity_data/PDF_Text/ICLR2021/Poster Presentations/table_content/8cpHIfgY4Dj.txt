Table 2: Quality of data used for best test-time performance. We maintain the same quality of datafor training and testing due to algorithm’s sensitivity to distribution shift. From our experiments, weobserve that for some envs/tasks, datasets with the best performance generate the best testing result,whereas for some envs/tasks, the diversity of data matters the most.
Table 3: Details of the fixed datasets used for producing Figure 3 and 6. The three numbers in the”Checkpoints” column stand for starting epoch: ending epoch: checkpoint spacing.
Table 4: Hyperparameters used to produce Figure 3. Meta batch size refers to the number of tasksused for computing the DML loss Lidjml at a time. Larger meta batch size leads to faster convergencebut requires greater computing power.
Table 5: Hyperparameters used to produce Figure 2a(a) Compared to Half-Cheetah-Vel experimentin Table 4, latent space dimension were reducedto speed up computation. Also the value penaltyis used in behavior regularization.
Table 6: Average testing return of FOCAL on Sparse-Point-Robot tasks With different quali-ties/distributions of training/testing sets. The numbers in parenthesis are the performance drop dueto distribution shift (compared to the scenario Where the testing distribution equals the training dis-tribution).
