Table 1: Detection of errors and OOD examples for regression and classification tasksDataset	∣		Single		SGB	Ensemble SGLB	vSGLB	Single		SGB	Ensemble SGLB	vSGLB		SGB	SGLB				SGB	SGLB					ZZ	Classification		% PRR (T)		Classification % AUC-ROC (T)					 Adult		 TU	72	72	72	72		 72	53	50	52	51	51	KU	—	—	49	49	38	—	—	89	89	85Amazon	TU	71	69	70	68	68	86	87	86	86	86	KU	—	—	64	61	40	—	—	88	74	67Click	TU	43	44	43	44	44	61	67	64	64	68	KU	—	—	22	22	11	—	—	91	92	90Internet	TU	76	79	77	79	79	67	68	70	69	68	KU	—	—	69	72	61	—	—	87	89	81KDD-Appetency	TU	68	69	69	69	69	29	48	47	50	52	KU			64	54	14			90	91	93KDD-Churn	TU	47	45	48	46	46	81	57	82	75	60	KU	—	—	33	35	28	—	—	99	98	92KDD-Upselling	TU	56	56	57	57	56	53	51	62	60	47	KU			45	49	33			97	97	78Kick	TU	44	45	44	44	45	45	37	52	58	38	KU	—	—	34	34	20	—	—	98	98	89
Table 2: Comparison of our implementation With existing methodsDataset	Deep. Ens.	RMSE NGBoost	CatBoost	Deep. Ens.	NLL NGBoost	CatBoostBoston	3.28 ± 1.00	2.94 ± 0.53	3.06 ± 0.68	2.41 ± 0.25	2.43 ± 0.15	2.47 ± 0.20Concrete	6.03 ± 0.58	5.06 ± 0.61	5.21 ± 0.53	3.06 ± 0.18	3.04 ± 0.17	3.06 ± 0.13Energy	2.09 ± 0.29	0.46 ± 0.06	0.57 ± 0.06	1.38 ± 0.22	0.60 ± 0.45	1.24 ± 1.28Kin8nm	0.09 ± 0.00	0.16 ± 0.00	0.14 ± 0.00	-1.20 ± 0.02	-0.49 ± 0.02	- 0.63 ± 0.02Naval	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	-5.63 ± 0.05	-5.34 ± 0.04	-5.39 ± 0.04PoWer	4.11 ± 0.17	3.79 ± 0.18	3.55 ± 0.27	2.79 ± 0.04	2.79 ± 0.11	2.72 ± 0.12Protein	4.71 ± 0.06	4.33 ± 0.03	3.92 ± 0.08	2.83 ± 0.02	2.81 ± 0.03	2.73 ± 0.07Wine	0.64 ± 0.04	0.63 ± 0.04	0.63 ± 0.04	0.94 ± 0.12	0.91 ± 0.06	0.93 ± 0.08Yacht	1.58 ± 0.48	0.50 ± 0.20	0.82 ± 0.40	1.18 ± 0.21	0.20 ± 0.26	0.41 ± 0.39Year MSD	8.89 ± NA	8.94 ± NA	8.99 ± NA	3.35 ± NA	3.43 ± NA	3.43 ± NAA.2 Parameter tuningFor all approaches, We use grid search to tune learning-rate in {0.001, 0.01, 0.1}, tree depth in{3, 4, 5, 6}. We fix subsample to 0.5 for SGB and to 1 for SGLB. This is done to avoid joint random-ization effects of SGB sampling and SGLB noise in gradients. We also set diffusion-temperature=N and model-shrink-rate =* for SGLB.
Table 3: Datasets descriptionDataset	# Examples	# FeaturesClassification		Adult(Kohavi,1996)	48842	14Amazon (Kaggle, 2017)	32769	9Click (KDD, 2012)	399482	11Internet (UCI, 1997)	10108	68KDD-Appetency (KDD, 2009)	50000	419KDD-Churn (KDD, 2009)	50000	419KDD-Upselling (KDD, 2009)	50000	419Kick (Kaggle, 2011)	72983	43Regression		Boston (UCI)	506	13Concrete (UCI)	1030	8Energy (UCI)	768	8Kin8nm (UCI)	8192	8Naval (UCI)	11934	16Power (UCI)	9568	4Protein (UCI)	45730	9Wine (UCI)	1599	11
Table 4: NLL and RMSE/Error rate for regression and classification	Single		Ensemble			Single		Ensemble		Dataset	SGB	SGLB	SGB	SGLB	vSGLB	SGB	SGLB	SGB	SGLB	vSGLB	Classification NLL Q)						Classification		% Error (J)		 Adult	0.276	0.273	0.276	0.271	0.274	12.8	12.7	12.8	12.6	12.7Amazon	0.141	0.142	0.140	0.142	0.143	4.7	4.6	4.6	4.5	4.5Click	0.393	0.392	0.392	0.391	0.392	15.6	15.7	15.6	15.6	15.6Internet	0.224	0.218	0.221	0.217	0.218	9.9	10.0	9.7	10.0	10.0Appetency	0.073	0.073	0.073	0.073	0.073	1.8	1.8	1.8	1.8	1.8Churn	0.235	0.236	0.233	0.234	0.235	7.3	7.2	7.3	7.2	7.2Upselling	0.168	0.168	0.168	0.168	0.168	5.0	4.9	5.0	5.0	4.9Kick	0.287	0.286	0.286	0.285	0.286	9.5	9.6	9.5	9.4	9.6Dataset	Regression NLL (J)					Regression RMSE (J)				BostonH	2.47	2.52	2.46	2.50	2.50	3.06	3.12	3.04	3.10	3.27Concrete	3.06	3.06	3.05	3.05	3.06	5.21	5.11	5.21	5.10	5.37Energy	1.24	1.70	1.13	1.52	0.70	0.57	0.54	0.57	0.54	0.64Kin8nm	-0.63	-0.65	-0.63	-0.65	-0.60	0.14	0.14	0.14	0.14	0.15Naval-p	-5.39	-5.42	-5.61	-5.65	-5.39	0.00	0.00	0.00	0.00	0.00Power-p	2.72	2.71	2.66	2.66	2.69	3.55	3.56	3.52	3.54	3.64Protein	2.73	2.73	2.61	2.64	2.70	3.92	3.96	3.90	3.93	4.02
Table 5: Comparison with random forest: NLL and error rate for classification	Single		Ensemble		Single		Ensemble	Dataset	SGLB	RF	SGLB	RF	SGLB	RF	SGLB	RF	Classification NLL			0)	Classification % Error (J)				 Adult	0.273	0.300	0.271	0.300	12.7	13.9	12.6	13.9Amazon	0.142	0.183	0.142	0.183	4.6	5.6	4.5	5.6Click	0.392	0.411	0.391	0.411	15.7	16.0	15.6	16.0Internet	0.218	0.275	0.217	0.274	10.0	11.2	10.0	11.0KDD-Appetency	0.073	0.083	0.073	0.083	1.8	1.8	1.8	1.8KDD-Churn	0.236	0.249	0.234	0.249	7.2	7.3	7.2	7.3KDD-Upselling	0.168	0.202	0.168	0.202	4.9	7.4	5.0	7.4Kick	0.286	0.311	0.285	0.311	9.6	10.4	9.4	10.4Table 6: Comparison with random forest: RMSE for regressionDataset	Single		Ensemble		SGLB	RF	SGLB	RFBostonH	3.12	2.98	3.10	2.98Concrete	5.11	4.96	5.10	4.95Energy	0.54	0.50	0.54	0.50Kin8nm	0.14	0.15	0.14	0.15Naval-p	0.00	0.00	0.00	0.00
Table 6: Comparison with random forest: RMSE for regressionDataset	Single		Ensemble		SGLB	RF	SGLB	RFBostonH	3.12	2.98	3.10	2.98Concrete	5.11	4.96	5.10	4.95Energy	0.54	0.50	0.54	0.50Kin8nm	0.14	0.15	0.14	0.15Naval-p	0.00	0.00	0.00	0.00Power-p	3.56	3.53	3.54	3.53Protein	3.96	4.19	3.93	4.19Wine-qu	0.65	0.58	0.65	0.58Yacht	0.84	0.84	0.84	0.84Year	8.96	9.43	8.94	9.4316Published as a conference paper at ICLR 2021Table 7: Comparison with random forest: detection of errors and OOD examples for regression andclassification (virtual and true ensembles)Dataset	∣		VSGLB	vRF	SGLB	RF	VSGLB	VRF	SGLB	RF		Classification		% PRR (↑)		Classification % AUC-ROC (↑)				 Adult	— TU	72	70	72	70	51	58	51	58
Table 7: Comparison with random forest: detection of errors and OOD examples for regression andclassification (virtual and true ensembles)Dataset	∣		VSGLB	vRF	SGLB	RF	VSGLB	VRF	SGLB	RF		Classification		% PRR (↑)		Classification % AUC-ROC (↑)				 Adult	— TU	72	70	72	70	51	58	51	58	KU	38	20	49	22	85	86	89	87Amazon	TU	68	63	68	64	86	48	86	49	KU	40	45	61	52	67	55	74	53Click	TU	44	36	44	37	68	71	64	71	KU	11	20	22	21	90	81	92	80Internet	TU	79	69	79	68	68	72	69	72	KU	61	38	72	36	81	79	89	79KDD-APPetenCy	TU	69	56	69	56	52	82	50	81	KU	14	34	54	34	93	97	91	98KDD-Chum	TU	46	39	46	39	60	75	75	78	KU	28	13	35	9	92	93	98	94KDD-Upselling	TU	56	66	57	66	47	73	60	72	KU	33	45	49	44	78	90	97	92Kick	TU	45	38	44	38	38	64	58	63	KU	20	27	34	29	89	89	98	89
