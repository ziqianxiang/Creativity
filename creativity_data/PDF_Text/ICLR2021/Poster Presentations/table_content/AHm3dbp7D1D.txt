Table 1: ImageNet-1k test accuracy (%) using KNN and linear classification for multiple students and MoCo-v2 pre-trained deeper teacher architectures. X denotes MoCo-V2 self-supervised learning baselines beforedistillation. * indicates using a deeper teacher encoder pre-trained by SWAV, where additional small-patches arealso utilized during distillation and trained for 800 epochs. K denotes Top-1 accuracy using KNN. T-1 and T-5denote Top-1 and Top-5 accuracy using linear evaluation. First column shows Top-1 Acc. of Teacher network.
Table 2: Object detection and instance segmentation results using contrastive self-supervised learning andSEED distillation using ResNet-18 as backbone: bounding-box AP (APbb) and mask AP (APmk) evaluatedon VOC07-val and COCO testing split. More results on different backbones can be found in the Appendix.
Table 3: ImageNet-1k Accuracy (%) of student net-work (ResNet-18) distilled from variants of self-supervised ResNet-50. P-E/D-E represent the pre-training and distillation epochs. T./S.-Top representtesting accuracy of Teacher and Student. * representsdistillation using additional small patches. First rowis the ResNet-18 SSL baseline using MoCo-v2 trainedfor 200 epochs.
Table 4: Top-1/5 accuracy of linear classifica-tion results on ImageNet using different distillationstrategies on ResNet-18 (student) and ResNet-50(teacher) architectures.
Table 5: Effect of Ï„T for the distillation of ResNet-18(student), ResNet-50 (teacher) on multiple datasets.
Table 6: Linear evaluations on ImageNet of EfficientNet and MobileNet pre-trained using MoCo-v2. A deeperprojection head largely boosts the linear evaluation performances on smaller architectures.
Table 7: Before and after distillation Top-1/5 test accuracy (%) on ImageNet of EfficientNet-b0 and MobileNet-large without deeper MLPs.
Table 8: ImageNet-1k test accuracy (%) under KNN and linear classification on ResNet-50 encoder withdeeper, MoCo-V2/SWAV pre-trained teacher architectures. X denotes MoCo-V2 self-supervised learningbaselines before distillation. * indicates using a stronger teacher encoder pre-trained by SWAV with additionalsmall-patches during distillation.
Table 9: Object detection and instance segmentation fine-tuned on VOC07: bounding-box AP (APbb) and maskAP (APmk) evaluated on VOC07-val. The first row shows the baseline from MoCo-v2 backbones withoutdistillation.
Table 10: Object detection and instance segmentation fine-tuned on COCO: bounding-box AP (APbb) andmask AP (APmk) evaluated on COCO-val2017. The first several rows show the baselines from unsupervisedbackbones without distillation.
Table 11: linear evaluations on imagenet of resnet-18 after distillation from the Swav pre-trained resnet-50using either single view, cross-views, or small patch views.
Table 12: CIFAR-100 Top-1 Accuracy(%) of ResNet-18 with (or without) distillation at different phase: self-supervised pre-training stage, and supervised classification fine-tuning. All backbone parameters of ResNet-18are trainable in experiments.
Table 13: Linear evaluation accuracy (%) of dis-tillation between ResNet-18 (as the Student) andResNet-50 (as the Teacher) using different learn-ing rates when the queue size is 65,536 and weightdecay=1e-6.
Table 14: Linear evaluation accuracy (%) of distillationbetween ResNet-18 (as the Student) and ResNet-50 (asthe Teacher) using different weight decays when thequeue size is 65,536 and LR=0.03.
