Table 2: Dev set results of standard models usingdifferent decoding algorithms. b: beam size. Wereport the average of 3 runs for top-k sampling.
Table 1: BLEU/ROUGE ⑴ and perplexity (J)using standard models on test sets. GOLDachieves better metric scores despite high held-out perplexity. Experiments are run using a fixedrandom seed (12); attempted three random seeds(1, 12, 123) and all BLEU/R-2 scores are within0.1 points of the reported. Refer to Table 3 fortransformer results.
Table 3: Results using transformer models on test sets. The advantage of GOLD is maintained onadvanced models based on transformers and pretraining.
Table 4: BLEU/ROUGE (↑) on test sets, using standard models finetuned with on-Policy objectives.
Table 5: Human comparisons on 200 randomlyselected test examples for each task. Win: %generations from GOLD-trained BART that arebetter than from MLE-trained BART, given thesame source.
Table 6: NQG generations using standard models. Words to query on are bolded. Long generationsfrom MLE-trained model often result in repetition or hallucination. More examples in appendix.
Table 7: NQG and CNN/DM examples based on transformer models. For NQG, words to query onare bolded.
Table 8: XSum and IWSLT14 De-En examples based on transformer models.
