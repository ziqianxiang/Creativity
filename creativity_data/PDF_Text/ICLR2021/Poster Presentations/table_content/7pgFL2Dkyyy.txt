Table 1: Effectiveness of Normalize+Scale, Attributes Normalization and Class Normalization.
Table 2: Generalized Zero-Shot Learning results. S, U denote generalized seen/unseen accuracyand H is their harmonic mean. Bold/normal blue font denotes the best/second-best result.
Table 3: Continual Zero-Shot Learning results with and without CN. Best scores are in bold blue.
Table 4: Hyperparameters for ZSL experiments	SUN	CUB	AwA1	AwA2Batch size	128	512	128	128Learning rate	0.0005	0.005	0.005	0.002Number of epochs	50	50	50	50Lent weight	0.001	0.001	0.001	0.001Number of hidden layers	2	2	2	2Hidden dimension	2048	2048	1024	512γ	5	5	5	5Table 5: Additional GZSL ablation studies. From this table one can observe the sensitivity ofnormalize+scale to γ value. We also highlight γ importance in Section 3.
Table 5: Additional GZSL ablation studies. From this table one can observe the sensitivity ofnormalize+scale to γ value. We also highlight γ importance in Section 3.
Table 6: Training time for the recent ZSL methods that made their official implementations publiclyavailable. We reran them on the corresponding datasets with the official hyperparameters and trainingsetups. All the comparisons are done on the same machine and hardware: NVidia GeForce RTX2080 Ti GPU, Intel Xeon Gold 6142 CPU and 64 GB RAM. N/C stands for “no code” meaning thatauthors didn’t release the code for a particular dataset.
Table 7: Incorporating Class Normalization into RelationNet (Sung et al., 2018) and CVC-ZSL(Li et al., 2019) based on the official source code and running hyperparameters. For some reason,our results differ considerably from the reported ones on AwA2 for RelationNet and on SUN forCVC-ZSL. Adding CN provides the improvement in all the setups.
Table 8: Ablating other methods for AN and NS importance. For CVC-ZSL, we used the officiallyprovided code with the official hyperparameters. When we do not employ AN, we standardize themto zero-mean and unit-variance: otherwise training diverges due to too high attributes magnitudes.
Table 9: Hyperparameters range for CZSL experimentsSampling distributionGradient clipping valueAttribute embedder learning rateAttribute embedder momentumImage encoder learning rateuniform, normal10, 1000.001, 0.0050.9, 0.950.001, 0.005—EWC + oursMAS + OUrSA-GEM + ours4035-3025---EWC55 — MAS
Table 10: Checking how a model performs when we replace AN with the standardization procedureand with the standardization procedure, accounted for 1/d factor from (88). In the latter case, theperformance is noticeably improved.
