Table 1: CompariSon with State-of-the-art methodS on WebViSion-V1.0. NumberS denote accuracy (%) onthe clean WebViSion-V1.0 validation Set and the ILSVRC 2012 validation Set. CleanNet (Lee et al., 2018) andDiStill (Zhang et al., 2020) require data with clean annotationS.
Table 2: Low-shot image classification on VOC07 and PlaceS205 uSing linear SVMS trained on fixed repre-SentationS. We vary the number of labeled exampleS per-claSS (k), and report the average mAP (for VOC) andaccuracy (for PlaceS) acroSS 5 independent runS. WebViSion-V0.5 haS the Same number of training SampleS aSImageNet. The self-supervised learning methods* are trained for 200 epochs, while other methods are trainedfor 90 epochS. MoPro outperformS vanilla CE pretrained on Web dataSetS, aS well aS Self-SuperviSed learningand supervised learning methods pretrained on ImageNet.
Table 3: Low-resource finetuning on ImageNet. A pretrained model is finetuned with 1% or 10% of Ima-geNet training data. Weakly-supervised learning with MoPro substantially outperforms self-supervised learningmethods: PCL (Li et al., 2020b), SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), and SwAV (Caronet al., 2020). Result for random init. is from Zhai et al. (2019).
Table 4: Object detection and instance segmentation using Mask-RCNN with R50-FPN fine-tuned onCOCO train2017. We evaluate bounding-box AP (APbb) and mask AP (APmk) on val2017. Weakly-supervised learning with MoPro outperforms both supervised learning on ImageNet and self-supervised learn-ing (MoCo (He et al., 2019)) on one billion Instagram images.
Table 5: Evaluation of model robustness on images with artistic and natural distribution shifts. Weaklysupervised learning with MoPro leads to a more robust and well-calibrated model.
Table 6: Ablation study Where different components are removed from MoPro. Models are pre-trained onWebVision-V0.5 and finetuned on 1% of ImageNet data.
Table 7: Low-shot image classification experiments. Mean and standard deviation are calculated across 5 runs.
