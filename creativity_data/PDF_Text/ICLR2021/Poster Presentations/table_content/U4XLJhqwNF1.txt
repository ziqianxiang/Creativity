Table 1: Linear classification protocol on ImageNet-1KPretext Task	Arch.	Head	#epochs Top-1 Acc. (%)	ImageNet Classification	R50	-	90	76.5Exemplar (Dosovitskiy et al., 2014)	R50w3×	-	35	46.0Relative Position (Doersch et al., 2015)	R50w2×	-	35	51.4Rotation (Gidaris et al., 2018)	Rv50w4×	-	35	55.4Jigsaw (Noroozi & Favaro, 2016)	R50	-	90	45.7Methods based on contrastive learning:				InsDisc (Wu et al., 2018)	R50	Linear	200	54.0Local Agg. (Zhuang et al., 2019)	R50	Linear	200	58.2CPC v2 (Henaff et al., 2019)	R170w	-	~200	65.9CMC (Tian et al., 2019)	R50	Liner	240	60.0AMDIM (Bachman et al., 2019)	AMDIMlarge	-	150	68.1PIRL (Misra & van der Maaten, 2020)	R50	Linear	800	63.6SimCLR (Chen et al., 2020a)	R50	MLP	1000	69.3MoCo (He et al., 2020)	R50	Linear	200	60.6MoCo (He et al., 2020) + CO2	R50	Linear	200	63.5MoCo v2 (Chen et al., 2020b)	R50	MLP	200	67.5MoCo v2 (Chen et al., 2020b) + CO2	R50	MLP	200	68.0Table 2: Top-5 accuracy for semi-supervised learning on ImageNet				
Table 2: Top-5 accuracy for semi-supervised learning on ImageNet				Pretext Task		1% labels	10% labels	Supervised Baseline		48.4	80.4	InsDisc (Wu et al., 2018)		39.2	77.4	PIRL (Misra & van der Maaten, 2020)		57.2	83.8	MoCo (He et al., 2020)		62.4	84.1	MoCo (He et al., 2020) + CO2		66.2	85.2	MoCo v2 (Chen et al., 2020b)		69.5	85.1	MoCo v2 (Chen et al., 2020b) + CO2		70.6	85.4	Our method is simple and low-cost. It captures the similarity to each ni while introducing unno-ticeable computational overhead with only one extra loss term computed. This is unlike clusteringbased unsupervised learning methods, which are costly, since they explicitly compute the similaritysets in the training set after every training epoch (Caron et al., 2018; Zhuang et al., 2019; Li et al.,2020; Caron et al., 2020).
Table 3: Transfer learning performance on PASCAL VOC datasets	Image		Object		Semantic	Classification		Detection		SegmentationPretext Task	mAP	AP50	APaU	AP75	mIoUImageNet Classification	88.0	81.3	53.5	58.8	74.4Rotation (Gidaris et al., 2018)	63.9	72.5	46.3	49.3	-Jigsaw (Noroozi & Favaro, 2016)	64.5	75.1	48.9	52.9	-InsDisc (Wu et al., 2018)	76.6	79.1	52.3	56.9	-PIRL (Misra & van der Maaten, 2020)	81.1	80.7	54.0	59.7	-SimCLR (Chen et al., 2020a)*	-	81.8	55.5	61.4	-BYOL (Grill et al., 2020)*	-	81.4	55.3	61.1	-SwAV (Caron et al., 2020)*	-	81.5	55.4	61.4	-SimSiam (Chen & He, 2020)*	-	82.4	57.0	63.7	-MoCo (He et al., 2020)	-	81.5	55.9	62.6	72.5MoCo (He et al., 2020) (our impl.)	79.7	81.6	56.2	62.4	72.6MoCo (He et al., 2020) +CO2	82.6	81.9	56.0	62.6	73.3MoCo v2 (Chen et al., 2020b)	85.0	82.4	57.0	63.6	74.2MoCo v2 (Chen et al., 2020b) +CO2	85.2	82.7	57.2	64.1	74.7* Results reported in Chen & He (2020).					method. For the hyper-parameters of CO2, we set τcon as 0.04, α as 10 for MoCo-based CO2,
Table 4: Linear classification accuracyusing an end-to-end encoder and withdifferent choices of Lcon . The resultsare summarized as mean and standarddeviation over three different runs.
