Table 1: Results on code search. GraphCodeBERT outperforms other models significantly (p < 0.01).
Table 2: Results on code clone detection. Graph-CodeBERT outperforms other pre-trained methodssignificantly (p < 0.01).
Table 3: Results on code translation. GraphCodeBERToutperforms other models significantly (p < 0.05).
Table 4: Results on code refinement.
Table 5: Ablation study on natural language code searchNode-vs. Token-level Attention Table 6 shows how frequently a special token [CLS] that is usedto calculate probability of correct candidate attends to code tokens (Codes) and variables (Nodes).
Table 6: Attention distribution (%) between code tokens (codes) and variables (nodes) across differentprogramming language on natural language code search test sets. The first row is the ratio of thenumber of code tokens to nodes, and the second row is attention distribution of [CLS] token.
Table 7: Data statistics about the filtered dataset. For each query in the development and testing sets,the answer is retrieved from the whole candidate codes (i.e. the last row).
Table 8: Results on natural language code search using the setting of Husain et al. (2019).
Table 9: Three examples that translate from Java to C# programming language on code translationtask. [src] represents the source input, [ref] represents the reference, [sys] represents Transformerwithout data flow and [ours] represents GraphCodeBERT.
Table 10: Three examples on code refinement task. [src] represents the source input, [ref] representsthe reference, [sys] represents Transformer without data flow and [ours] represents GraphCodeBERT.
Table 11: Error cases of GraphCodeBERT on the code translation task. [src] represents the sourceinput, [ref] represents the reference and [ours] represents GraphCodeBERT.
