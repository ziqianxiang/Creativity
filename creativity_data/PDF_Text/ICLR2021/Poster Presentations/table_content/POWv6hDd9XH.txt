Table 1: Ablation study.
Table 2: Accuracy comparison on weight-only quantized post-training models. Activations here are un-quantized and kept full precision. We also conduct variance study for our experiments. Bold values indicatesbest results. * indicates our implementation based on open-source codes.
Table 3: Accuracy comparison on fully quantized post-training models. Activations here are quantized to 4-bit.
Table 4: Performance as well as training cost comparison with quantization-aware training (QAT).
Table 5: Objection detection task (MS COCO) comparison on fully quantized post-training models. Activationshere are quantized to 8-bit. We report the bounding box mean Average Precision (mAP) metric.
Table 6: Impact of the first and the last layerModels	No Quantization		Precision 4/8			Precision 2/8			First	Last	Accuracy	Model Size	Latency	Accuracy	Model Size	Latency	✓	✓	70.76	5.81 MB	70.72 ms	66.30	3.15 MB	59.84 msResNet-18	X	✓	70.66	5.81 MB	53.76 ms	65.95	3.15 MB	31.20 msFP: 71.08	✓	X	70.64	5.57 MB	70.08 ms	64.87	2.79 MB	58.72 ms	X	X	70.58	5.56 MB	53.28 ms	64.53	2.78 MB	30.88 ms	✓	✓	71.80	2.26 MB	32.80 ms	59.59	1.74 MB	30.40 msMobileNetV2	X	✓	71.69	2.26 MB	32.64 ms	59.13	1.74 MB	30.24 msFP: 72.49	✓	X	71.42	1.65 MB	31.52 ms	56.29	0.83 MB	28.48 ms	X	X	71.42	1.65 MB	31.36 ms	55.58	0.82 MB	28.32 ms	✓	✓	72.98	3.19MB	31.84 ms	65.66	1.84 MB	23.20 msRegNet-600MF	X	✓	72.89	3.19 MB	31.68 ms	65.83	1.85 MB	22.88 msFP: 73.71	✓	X	72.69	2.94 MB	31.20 ms	62.93	1.47 MB	22.40 ms	X	X	72.73	2.94 MB	31.04 ms	63.08	1.47 MB	22.08 ms13Published as a conference paper at ICLR 2021AJOIngVFigure 3: Effect of #data points and data source.
