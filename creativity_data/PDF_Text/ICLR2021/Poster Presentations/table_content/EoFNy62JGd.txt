Table 1: Mean Â± std (p-value) over all layers over time of KS test on different models and datasetsfor normal and lognormal distribution. Notice the lognormal distribution gets the higher fit across allmodels. We compare to additional distributions such as Laplace, uniform, Cauchy and loglaplace inAppendix A.1 Also, in Table A.2 we show there is a good match to the lognormal through training(not only on average), and in Fig. A.3 we show this holds also for specific layers over time .
Table 2: Ideal sign-exponent-mantissa bit allocations based on the optimization of Equation 4 and thegradient statistics of ResNet18, ResNet101 and SqueezeNet in Cifar100 and ImageNet datasets. Theanalysis matches the previously suggested allocation by Sun et al. (2019) for FP8, showing a clearbenefit of 1-5-2 for gradient quantization.
Table 3: Floating-point formats for neural gradient representations. The table reports the validationaccuracy of ResNet18 on ImageNet & Cifar100, ResNet101 on Cifar100 and SqueezeNet on Ima-geNet. E* denotes the optimal exponent bit-width, which minimizes the expected relative error ofEq. (4). N/A refers to cases of invalid format (e.g., 1-5-0 is the optimal FP6 format for ImageNet, butFP6 cannot have E* + 1 = 6 exponent bits). Note that in all cases, the formats that are predicted byEq. (4) to minimize the relative error also provide the highest accuracies (bolded). We marked (by t)the results for the optimal format assuming normal distribution for neural gradients in Eq. (3).
Table 4: Comparison of the suggested gradient scaling against different global fixed loss scaling andglobal dynamic loss scaling (Micikevicius et al., 2018) in ResNet18, on the ImageNet dataset for theoptimal format in FP4 (1-3-0).
