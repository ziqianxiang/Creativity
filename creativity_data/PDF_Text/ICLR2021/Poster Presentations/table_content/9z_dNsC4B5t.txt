Table 1: Effect of KL Term in MetaNorm for few-shot classification with MAML (Finn & Levine,2018) on miniImageNet and domain generalization on PACS with ResNet-18. More few-shotclassification results with ProtoNets (Snell et al., 2017) and VERSA (Gordon et al., 2019), as wellas domain generalization results on Office-Home are provided in the appendix. Best performingmethods and any other runs within the 95% confidence margin in bold. The KL term is crucial.
Table 2: Sensitivity to Algorithm. Few-shot results on miniImageNet using different algorithms.
Table 3: Sensitivity to Dataset. Few-shotclassification on Meta-Dataset using Pro-toNets. MetaNorm performs best overall.
Table 4: Sensitivity to Domains. Performance comparison on domain generalization. MetaNormconsistently achieves the best performance among all normalization methods.
Table 5: Few-Shot Domain Generalization. Comparison with different normalizations usingMAML and ProtoNets on the Few-shot DomainNet dataset. Best performing methods and anyother runs within the 95% confidence margin denoted in bold. Reported results use “Painting” as thetarget domain, all based on our implementations. MetaNorm consistently achieves top performance.
Table 6:	Training classes of Few-shot DomainNetFurniture: bathtub, ceiling fan, couch, fence, hot tub, mailboxMammal: tiger, rhinoceros, bat, cat, lion, pandaTool: anvil, basket, broom, sword, pliersCloth: belt, camouflage, eyeglasses, crown, bowtieElectricity: calculator, computer, camera, cooler, dishwasherBuilding: bridge, jail, pool, tent, castleOffice: alarm clock, binoculars, backpack, book, bandageHuman Body: arm, ear, face, beard, elbow, finger, brain, eye, foot, kneeRoad Transportation: ambulance, bus motorbike, bicycle, trainFood: birthday cake, cookie, hot dog, peanut, sandwich, bread, donut, pizza, steak, lollipopNature: beach, lightning, ocean, river, sun, cloud, moon, rain, tornadoCold Blooded: crab, frog, crocodile, lobster, fish, octopus, sharkMusic: cello, guitar, saxophone, violin, clarinet, harp, tromboneFruit: apple, banana, blackberry, blueberry, grapes, pearSport: baseball, baseball bat, basketball, snorkel, yoga, tennis racquetTree: bush, grass, cactus, tree, flowerBird: bird, owlVegetable: asparagus, broccoli, carrot, mushroom, onionShape: circle, hexagon
Table 7:	Validation classes of Few-shot DomainNetFurniture: stairs, ladderMammal: monkeyTool: paint canCloth: purse, t-shirtElectricity: radioBuilding: pondOffice: nailHuman Body: skull, toothRoad Transportation: firetruckFood: -Nature: star, hurricaneCold Blooded: sea turtleMusic: -Fruit: strawberrySport: hockey stickTree:-Bird: penguinVegetable: -Shape: -
Table 8:	Test classes of Few-shot DomainNetFurniture: teapot, toothpaste, stove, umbrellaMammal: mouse,Tool: bucket, paint canCloth: sweater, shoe, flip flopsElectricity: television, stereo, toaster, flashlightBuilding: waterslide, gardenOffice: map, clock, calendar, scissorsHuman Body: finger, nose, toeRoad Transportation: bulldozerFood: peanutNature: mountain, sunCold Blooded: lobster, scorpionMusic: harpFruit: pineappleSport: soccer ball, hockey stickTree: house plant, leafBird: swanVegetable: string beanShape: squiggle
Table 9: Inference function f∖(∙)Output size	Layersw×h	Input flattened vector of the activation map128	fully connected, ELU128	fully connected, ELUw×h	fully connected to μTable 10: Inference function fg(∙)Output size	Layersw×h	Input flattened vector of the activation map and μ128	fully connected, ELU128	fully connected, ELUw×h	fully connected to σE Extra Results for Effect of KL TermIn this Appendix we consider extra results for the ablation on measuring the effect of the KL term.
Table 10: Inference function fg(∙)Output size	Layersw×h	Input flattened vector of the activation map and μ128	fully connected, ELU128	fully connected, ELUw×h	fully connected to σE Extra Results for Effect of KL TermIn this Appendix we consider extra results for the ablation on measuring the effect of the KL term.
Table 11: Effect of KL Term in MetaNorm for few-shot classification on miniImageNet withProtoNets and VERSA. Best performing methods and any other runs within 95% confidence margindenoted in bold.
Table 12: Effect of KL Term in MetaNorm for domain generalization on Office-Home.
Table 13: Sensitivity to Algorithm. Few-shot results on Omniglot using MAML. Best performingmethods and any other runs within the 95% confidence margin in bold. Transductive results indicatedabove dashed line.
Table 14: Sensitivity to Algorithm. Few-shot results on Omniglot using VERSA. Best performingmethods and any other runs within the 95% confidence margin in bold. Transductive results indicatedabove dashed line.
Table 15: Sensitivity to Algorithm. Few-shot results on Omniglot using ProtoNets. Best performingmethods and any other runs within 95% confidence margin denoted in bold. Transductive resultsindicated above dashed line.
Table 16: Sensitivity to Dataset. Few-shot classification results on Meta-Dataset using ProtoNets. The ± sign indicates the 95% confidence interval over tasks. Bestperforming methods and any other runs within the 95% confidence margin in bold. Results of other methods provided by Bronskill et al. (2020).
Table 17: Effect of number of units of hidden layers in MetaNorm for few-shot classification withMAML (Finn & Levine, 2018) on miniImageNet. The ± sign indicates the 95% confidence intervalover tasks. We achieve best results with 128 units of hidden layers.
