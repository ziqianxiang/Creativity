Table 1: Action recognition results using different number of input frames and different searchspace. We choose R(2+1)D-18 on Mini-Kinetics-200 and study the performance with differentnumber of input frames and different search space (denoted as Sea. Sp.). Search space of2 means thatboth temporal-wise and channel-wise policy network have 2 alternatives: computing all feature maps,or computing only 1) ofthe feature maps. Similarly, search space 3 have 3 alternatives: computing 1)all feature maps, 2) 1 of feature maps, 3) 4 of feature maps. X denote the base model and âœ“ denotethe dynamic model trained using our proposed approach VA-RED2 . We also report the average speedof different models in terms of number of clips processed in one second (clip/second).
Table 2: Action recognition results on Mini-Kinetics-200. We set the search space as 2 andtrain all the models with 16 frames. The metricspeed uses clip/second as the unit.
Table 3: Action recognition results withTemporal Pyramid Network (TPN) on Mini-Kinetics-200. TPN-8f and TPN-16f indicatethat we use 8 frames and 16 frames as input tothe model respectively.
Table 4: Comparison with CorrNet (Wang et al., 2020) and AR-Net (Meng et al., 2020) onMini-Kinetics-200. We set the search space as 2 and train all the models with 16 frames.
Table 5: Action recognition results on Kinetics-400. We set the search space as 2, meaning modelscan choose to compute all feature maps or 1 of them both on temporal and channel-wise convolutions.
Table 7: Comparison with network pruning methods.
Table 6: Action recognition results on Moments-In-Time. We set the search space as 2, i.e., mod-els can choose to compute all feature maps or 2 ofthem both on temporal and channel-wise convolu-tions. The speed uses clip/second as the unit.
Table 8: Action localization results on J-HMDB. Weset the search space as 2 for dynamic models. The speeduses clip/second as the unit.
Table 9: Effect of efficiency loss on Kinetics-400. Eff. denotes the efficiency loss.
Table 10: Ablation experiments on dynamic modeling along temporal and channel dimensions.
Table 11: VA-RED2 on semantic segmentation. We choose dilated ResNet-18 as our backbonearchitecture and set the search space as 2. Models are trained for 100K iterations with batch size of 8.
Table 12: Quantitative results of redundancy experiments. We compute the correlation coefficient,RMSE and redundancy proportions (RP) for feature maps in well-known pretrained video modelson Moments-in-Time and Kinetics-400 datasets. RP is calculated as the number of tensors withboth CC and RMSE above redundancy thresholds of 0.85 and 0.001, respectively. We show resultscorresponding to averaging the per layer values for all videos in the validation sets. We observe thatnetworks trained on Moments-In-Time (and evaluated on the Moments in Time validation set) tend topresent slightly less redundancy than their Kinetics counterparts, and the time dimension tends to bemore redundant than the channel dimension in all cases. We observe severe redundancy across theboard (with some dataset-model pairs achieving upwards of 0.8 correlation coefficient between theirfeature maps), which further motivates our redundancy reduction approach.
Table 13: Comparison between the performance of VA-RED2 on 120-epoch X3D model and256-epoch X3D model. We choose X3D-M as our backbone architecture and set the search space as2. We train one group of models for 120 epochs and the other for 256 epochs.
