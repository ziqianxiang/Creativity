Table 1: The 8 modified subdatasets created from ImageNet-9, which are visualized in Figure 1.
Table 2: Performance of state-of-the-art computer vision models on selected test sets of ImageNet-9. We include both pre-trained ImageNet models and models of different architectures that wetrain on IN-9L. The BG-Gap is defined as the difference in test accuracy between Mixed-Sameand Mixed-Rand and helps assess the tendency of such models to rely on background signal.
Table 3: Prediction categories we study for a given image-model pair. For a given image, a model canmake differing predictions based on the presence or absence of its foreground/background. We labeleach possible case based on how the background classification relates to the full image classificationand the foreground classification. To proxy classifying full images, foregrounds, and backgroundsseparately, we classify Original, Mixed-Rand, and Only-BG-T (respectively). “BG Irrelevant”demarcates images where the foreground classification result is the same as that of the full image (interms of correctness). We show illustrative examples of BG Required and BG+FG Required below.
Table 4: The 9 classes of ImageNet-9.
Table 5: The test accuracies, in percentages, of ResNet-50 models trained on all variants of ImageNet-9, and a pre-trained ImageNet ResNet-50. The bottom row and the second-to-last-row test the samepre-trained ImageNet model; however, the bottom row tests the model on the Full-IN version of eachdataset variation. Testing on Full-IN shows similar trends as testing on ImageNet-9. Note that theMixed-Next test accuracy is actually higher than the Mixed-Rand test accuracy in the bottomrow because the next class is often very similar to the previous class in Full-IN.
Table 6: Adversarial backgrounds attack success rates for 4 models analyzed in this paper. TheOriginal and the Mixed-Rand are trained on equally small datasets, IN-9L is trained on 4x moredata, and the ImageNet model is trained on the most data.
