Table 1: Summary of pre-trained models analyzed, including the source of the model, the type ofTransformer used, the number of layers and heads, the total number of model parameters, the sourceof the pre-training dataset, and the number of protein sequences in the pre-training dataset.
Table 2: Datasets used in analysisDataset	Train size	Validation sizeProteinNet	-25299-	224Secondary Structure	8678	2170Binding Sites / PTM	5734	1418C Additional Results of Attention AnalysisC.1 Secondary Structure(b) ProtAlbert(a) TaPeBert(c) ProtBert80%-60%-40%-20%-302826242220
Table 3: Amino acids and the corresponding maximally attentive heads in the standard and randomizedversions of TapeBert. The differences between the attention percentages for TapeBert and thebackground frequencies of each amino acid are all statistically significant (p < 0.00001) taking intoaccount the Bonferroni correction. See Appendix B.2 for details. The bolded numbers represent thehigher of the two values between the standard and random models. In all cases except for Glutamine,which was the amino acid with the lowest top attention proportion in the standard model (7.1), thestandard TapeBert model has higher values than the randomized version.
