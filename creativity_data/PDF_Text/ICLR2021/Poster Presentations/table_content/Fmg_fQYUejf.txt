Table 1: Comparison between the proposed method (MC-SGD) and other baselines.
Table 2: Performance of the stable SGD method with access to episodic memory	Permuted MNIST	Rotated MNISTMethod	ACAZyge%)	Forgetting	ACACuvrearCayge(%)	ForgettingStable SGD	80.1 (± 0.51)	0.09(± 0.01)	70.8 (± 0.78)	0.10 (± 0.02)Stable SGD + episodic memory	82.2 (± 0.83 )	0.09 (± 0.01)	73.3 (± 0.97)	0.10 (± 0.03)Moreover, in Fig. 18, we compare the loss on the interpolation paths between SGD, Stable-SGD withmemory, and MC-SGD methods. To show Stable SGD minima, we use we notation. We can see thatthe additional episodic memory helped the stable-SGD method to gain a significant boost. Still, theregularization term in the MC-SGD method helps it find minima with better mode connectivity.
