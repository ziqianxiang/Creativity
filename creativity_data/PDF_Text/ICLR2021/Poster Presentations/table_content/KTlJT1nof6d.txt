Table 1: Comparison of low-rank and sparse training in a common evaluation setting for “ticket-guessing” (Wang et al., 2020; Su et al., 2020). Best results overall are italicized; best results fromfull low-memory training are bolded. For complete results and deviations see Tables 5 and 6.
Table 2: Comparison of low-rank and tensor-decomposition training of WideResNet28-10 onCIFAR-10 (mean of 3 trials) with different regularization and initialization. Best errors bolded.
Table 3: Overcomplete ResNet performance (mean of3 trials). Best at each depth is bolded; cases where we match the next deeper network with around the same training memory are Underlined.										data	ResNet depth	metric	unfactorized WD	DO-Conv* WD	full		deep		wide						WD	FD	WD	FD	WD	FD		test accuracy (%)	92.66	N/A	92.11	93.32	91.37	93.11	92.26	93.36	32	training param. (M)	0.46	N/A	0.95	0.95	1.43	1.43	2.84	2.84		testing param. (M)	0.46	0.46	0.46	0.46	0.46	0.46	0.46	0.46CIFAR		test accuracy (%)	93.15	93:38	92.95	94.13	92.52	93.89	93.08	93.8310	56	training param. (M)	0.85	N/A	1.72	1.72	2.59	2.59	5.16	5.16		testing param. (M)	0.85	0.85	0.85	0.85	0.85	0.85	0.85	0.85		test accuracy (%)	93.61	93:93	93.73	94.28	93.25	94.42	93.53	94.13	110	training param. (M)	1.73	N/A	3.47	3.47	5.21	5.21	10.39	10.39		testing param. (M)	1.73	1.73	1.73	1.73	1.73	1.73	1.73	1.73		test accuracy (%)	68.17	N/A	68.84	70.32	67.84	70.25	69.23	70.53	32	training param. (M)	0.47	N/A	0.95	0.95	1.44	1.44	2.84	2.84		testing param. (M)	0.46	0.46	0.46	0.46	0.46	0.46	0.46	0.46CIFAR		test accuracy (%)	70.25	7078	70.6	72.79	69.62	72.28	70.69	73.01100	56	training param. (M)	0.86	N/A	1.73	1.73	2.60	2.60	5.17	5.17		testing param. (M)	0.86	0.86	0.86	0.86	0.86	0.86	0.86	0.86		test accuracy (%)	72.11	7222	72.33	73.98	71.28	74.17	71.7	73.69	110	training param. (M)	1.73	N/A	3.48	3.48	5.22	5.22	10.40	10.40
Table 4: Comparison of BERT unsupervised pretrainingusing LAMB (You et al., 2020) and our Frobenius de-Cay scheme FLAMBe. Evaluation is conducted by fine-tuning the final model on the SQuAD question-answeringtask (Rajpurkar et al., 2016). Guided by IWSLT results,FLAMBe is applied only to the Output-Value form inMHA; regular weight-decay is used on all other parame-ters. Spectral initialization of MHA is used for FLAMBeas well, but we find the effect minimal. Regularization co-efficients for both methods were obtained by tuning on theuncompressed model, targeting the unsupervised loss.
Table 5: Pruning, sparse training, and low-rank training for VGG-19NFC, i.e. the model ofSimonyan & Zisserman (2015) with no fully-connected layers, as in Wang et al. (2020).
Table 6: Pruning, sparse training, and low-rank training for ResNet-32x2, i.e. the model ofHe et al. (2016) with twice the number of filters, as in Wang et al. (2020).
Table 7: Comparison of low-rank and tensor-decomposition training (mean of 3 trials) ofWideResNet28-10 on CIFAR-10. The best error for each compression-decomposition settingis bolded; in addition, the best error at each compression level is underlined.
Table 8: Overcomplete ResNet performance (mean of 3 trials). The best accuracy at each depth isbolded; cases where we match the next deeper network with around the same training memory areunderlined. Note that training times are reported for roughly similar machine types; all ResNet56and ResNet110 times are reported on identical machine types.
Table 9: Comparison of our knowledge distillation approach with past work in which the samestudent network attains roughly similar performance.
