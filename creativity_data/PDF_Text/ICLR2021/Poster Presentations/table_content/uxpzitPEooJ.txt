Table 1: Depending on the choice of F (quantity that we want to preserve) and OG, we have differentprojection/lift operators and resulting OGb on the coarse graph.
Table 2: The error reduction after applying GOREN.
Table 3: Loss: quadratic loss. Laplacian: combinatorial Laplacian for both original and coarsegraphs. Each entry x(y) is: x = loss w/o learning, and y = improvement percentage.
Table 4: Loss: quadratic loss. Laplacian: normalized Laplacian for original and coarse graphs. Eachentry x(y) is: x = loss w/o learning, and y = improvement percentage.
Table 5: Loss: Eigenerror. Laplacian: combinatorial Laplacian for original graphs and doubly-weighted Laplacian for coarse ones. Each entry x(y) is: x = loss w/o learning, and y = improve-ment percentage. f stands for out of memory.
Table 6: Model comparison between MLP and GOREN . Loss: quadratic loss. Laplacian: combina-torial Laplacian for both original and coarse graphs. Each entry x(y) is: x = loss w/o learning, andy = improvement percentage.
Table 8: Relative eigenvalue error (Eigenerror) by different coarsening algorithm and the improve-ment (in percentage) after applying GOREN.
Table 9: Loss: quadratic loss. Laplacian: combinatorial Laplacian for both original and coarsegraphs. Each entry x(y) is: x = loss w/o learning, and y = improvement percentage. BL stands forthe baseline.
Table 10: Loss: quadratic loss. Laplacian: normalized Laplacian for both original and coarse graphs.
Table 11: Loss: conductance difference. Each entry x(y) is: x = loss w/o learning, and yimprovement percentage. f stands for out of memory error.
Table 12: Loss: Eigenerror. Laplacian: combinatorial Laplacian for original graphs and doubly-weighted Laplacian for coarse graphs. Each entry x(y) is: x = loss w/o learning, and y = improve-ment percentage. f stands for out of memory error.
