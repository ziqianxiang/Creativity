Table 1: Unsupervised clustering performance of different methods on three datasets. The first sectorpresents the results from the literature, the later ones display the results of the baseline and theproposed MiCE. In the last two sectors, the bold results indicating the one with the highest values.
Table 2: Statistics of the datasets.
Table 3: Ablations of MiCE on the probabilistic model (left) and different ways of learning the gatingand expert prototypes (right). Each row shows the ACC (%) on CIFAR-100 when applying the singlechange to MiCE. The assumptions are detailed in the Appendix C.
Table 4: Comparing the cluster accuracy ACC (%) of SCAN (Van Gansbeke et al., 2020) and MiCEon CIFAR-10. Following SCAN, we show the data augmentation strategy in the parenthesis if it isdifferent from the one MiCE and MoCo use. “SimCLR” indicates the augmentation strategy usedin (Chen et al., 2020), and “RA” is the RandAugment (Cubuk et al., 2020). For a fair comparison, thefirst sector compares MiCE to SCAN without the self-labeling step, and the second sector comparesSCAN with self-labeling to MiCE with pre-training.
