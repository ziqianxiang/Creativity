Table 1: Performance of the next strategy and dialogue-act prediction of various models. We reportthe F1 and ROC AUC scores. Significance tests were performed as described in §4 and the bestresults (along with all statistically insignificant values) are bolded.
Table 2: Downstream evaluation of negotiation dia- logue generation and negotiation outcome prediction. The best results (along with all statistically insignifi- cant values to those) are bolded.						Table 3: DialoGraph ablation analy- sis. This shows that all the different com- ponents provide complementary benefits. We also evaluate without BERT for com- parison with previously published works.	Model	Generation				Outcome Prediction			BLEU	BERTSCore							Precision	Recall	F1	RC-Acc	Model	BERT Score F1HED	20.9	21.8	22.3	22.1	35.2	DIALOGRAPH	27.4FeHED	23.7	27.1	26.8	27.0	42.3	w/o Strategy (ST)	26.8HED+RNN	22.5	22.9	22.7	22.8	47.9	w/o ST, Dialogue Acts (DA)	26.3HED+Transformer	24.4	27.4	28.1	27.7	53.7	w/o ST, DA, BERT	22.7DialoGraph	24.7	27.8	28.3	28.1	53.1		Seller (Bot)Hey, you need a router? UiInformalU4 Great, will it support laptop?幺 Nah.. few months only. We're moving... U7D Definitely both .... $X and we're square U52 Oh, it SUPPortS multiple devices … u?u6 Great! Is there any scratch/damage to it?Trade InInformalu3
Table 4: Human evaluation ratings on a scale of 1-5 for various models. We also provide the averagesale price ratio (§2.4). Negative ratio means that average sale price was lower than the buyer’s target.
Table 5: Examples of strategies and their least / highly associated strategies based on associationscores extracted using the cluster attention scores given by the ASAP layer.
Table 6: The list of dialogue acts that we use to annotate the data.
Table 7: The details of 15 Negotiation Strategies proposed by Zhou et al. (2019).
Table 8: The details of 21 Negotiation Strategies (<start> added by us) used by Zhou et al.
Table 9: We report the number of nodes and edges in our strategy-graphs. Each node correspondsto a particular utterance-strategy pair.
Table 10: Here we describe the search-space of all the hyper-parameters used in our experiments.
Table 11: Dataset statistics.
Table 12: Examples of the generated dialogues of various models when we keep the buyer utterancessame. We see that DialoGraph gets the best deal for the same dialogue context and is morepersistent. The FeHED and HED models accept offers more readily. We provide more examples ofDIALOGRAPH in Table 13.
Table 13: More examples of the generated dialogues of DialoGraph.
