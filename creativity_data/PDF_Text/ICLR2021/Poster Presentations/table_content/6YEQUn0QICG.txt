Table 1: We report results on three different real-world datasets with format mean(std) from 5-trialrun. For Office-Caltech 10, A, C, D ,W are abbreviations for Amazon, Caltech, DSLR and WebCam,for DomainNet, C, I, P, Q, R, S are abbreviations for Clipart, Infograph, Painting, Quickdraw, Realand Sketch. For ABIDE, we list the abbreviations for the clients (i.e., medical institutions).
Table 2: Notations occurred in the paper.
Table 3: Model architecture of the benchmark experiment. For convolutional layer (Conv2D), welist parameters with sequence of input and output dimension, kernal size, stride and padding. Formax pooling layer (MaxPool2D), we list kernal and stride. For fully connected layer (FC), we listinput and output dimension. For BatchNormalization layer (BN), we list the channel dimension.
Table 4: Settings for convergence rate. Each dataset has 1 client With 743 samples, local updateepoch is set to 1.
Table 5: Settings for local update epochs. Each dataset has 1 client With 743 samples, local updateepoch for all datasets is set to 1, 4, 8, 16 successively.
Table 6: Settings for local dataset size, we set local update epochs to 1 and each dataset has 1 client.
Table 7: Settings for statistical heterogeneity, [1, 10] for the range from 1 to 10. We increase numberof clients step by step and number of samples will increase accordingly.
Table 8: Settings for comparison with SOTA, we use 1 client with 743 samples and 1 local updateepoch for comparison experiment.
Table 9: Model architecture for Office-Caltech10 and DomainNet experiment. For convolutionallayer (Conv2D), we list parameters with sequence of input and output dimension, kernal size, strideand padding. For max pooling layer (MaxPool2D), we list kernal and stride. For fully connectedlayer (FC), we list input and output dimension. For BatchNormalization layer (BN), we list thechannel dimension.
Table 10: Data summary of the dataset used in our study.
Table 11: The detailed statistics reported with format mean (std) of accuracy presented on Fig. 5 .
Table 12: Testing accuracy on each testing sets with format mean(std) from 5-trial run.
Table 13: Test sets accuracy using different combinations of batch size B and local update epoch Eon benchmark experiment with the default non-iid setting.
Table 14: Model performance over varying dataset sizes on local clientsE.6 Training on Unequal Dataset SizeIn our benchmark experiment (Section 5.1), we truncate the sample size of the five datasets to theirsmallest number. This data preprocessing intends to strictly control non-related factors (e.g., imbal-anced sample numbers across clients), so that the experimental findings can more clearly reflect theeffect of local BN. In this regard, truncating datasets is a reasonable way to make each client havean equal number of data points and local update steps. It is also possible to keep the data sets in their24Published as a conference paper at ICLR 2021original size (which is unequal), by allowing clients with less data to repeat sampling. In this way,all clients use the same batch size and same local iterations of each epoch. We add results of such asetting with 10% and full original datasize in Table 15 and Table 16 respectively. It is observed thatFedBN still consistently outperforms other methods.
Table 15: Testing accuracy of each clients when clients’ training samples are unequal using 10% oforiginal data. The number of training samples for each client are denoted under their names.
Table 16: Testing accuracy of each clients when clients’ training samples are unequal using full sizedata. The number of training samples for each client are denoted under their names.
Table 17: Generalizing the global model to unseen-domain clients.
