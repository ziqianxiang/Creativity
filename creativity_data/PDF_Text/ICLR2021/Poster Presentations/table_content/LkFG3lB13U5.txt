Table 1: Average validation performance over the last100 rounds: % accuracy for rows 1-5; Recall@5 (X100)for Stack Overflow LR; and MSE (×1000) for EMNISTAE. Performance within 0.5% of the best result for eachtask are shown in bold.
Table 2: Dataset statistics.
Table 3: EMNIST autoencoder model architecture. We use a sigmoid activation at all dense layers.
Table 4: EMNIST character recognition model architecture.
Table 5: Shakespeare model architecture.
Table 6: Stack Overflow next word prediction model architecture.
Table 8: The base-10 logarithm of the client (ηl) and server (η) learning rate combinations that achievethe accuracies from Table 1. See Appendix D.2 for a full description of the grids.
Table 9: The base-10 logarithm of the parameter τ (as defined in Algorithm 2) that achieve thevalidation metrics in Table 1.
Table 10: Test set performance for Stack Overflow tasks after training: Accuracy for NWP andRecall@5 (×100) for LR. Performance within within 0.5% of the best result are shown in bold.
Table 11: (Top) Test accuracy (%) of a model trained centrally with various optimizers. (Bottom)Average test accuracy (%) over the last 100 rounds of various federated optimizers on the EMNISTCR task, using constant learning rates or the EXPDECAY schedule forηl. Accuracies (for the federatedtasks) within 0.5% of the best result are shown in bold.
