Table 1: Comparison of contrastive representation learning methods and i-Mix in different domains.
Table 2: Comparison of MoCo v2 and i-Mixon large-scale datasets.
Table 3: Comparison of MoCo v2 and i-Mix with and without data augmentations.
Table 4: Comparison of MoCo v2 and i-Mix in transfer learning.
Table C.1: Comparison of N-pair contrastive learning and SimCLR with i-MixUp and i-CutMix onthem with ResNet-50 on CIFAR-10 and 100. We run all experiments for 1000 epochs. i-MixUpimproves the accuracy on the downstream task regardless of the data distribution shift between thepretext and downstream tasks. i-CutMix shows a comparable performance with i-MixUp when thepretext and downstream datasets are the same, but it does not when the data distribution shift occurs.
Table C.2: Comparison of the N-pair self-supervised and supervised contrastive learning methodsand i-Mix on them with ResNet-50 on CIFAR-10 and 100. We also provide the performance offormulations proposed in prior works: SimCLR (Chen et al., 2020a) and its supervised version (Khoslaet al., 2020). We run all experiments for 1000 epochs. i-Mix improves the accuracy on the downstreamtask regardless of the data distribution shift between the pretext and downstream tasks, except thecase that the pretest task has smaller number of classes than that of the downstream task. Thequality of representation depends on the pretext task in terms of the performance of transfer learning:self-supervised learning is better on CIFAR-10, while supervised learning is better on CIFAR-100.
Table C.3: Comparison of N-pair contrastive learning and i-Mix with ResNet-50 on CIFAR-10 and100 in terms of the Frechet embedding distance (FED) between training and test data distribution onthe embedding space, and training and test accuracy. â†‘ (1) indicates that the higher (lower) number isthe better. i-Mix improves contrastive learning in all metrics, which shows that i-Mix is an effectiveregularization method for the pretext task, such that the learned representation is more generalized.
