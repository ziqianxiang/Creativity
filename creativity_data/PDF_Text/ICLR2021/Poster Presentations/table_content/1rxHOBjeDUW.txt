Table 1: Comparison of the average episodic sum of rewards in VizDoom (over 10 runs) and DMLab(over 30 runs) under three noise settings: Image Action (IA), Noise (N) and Noise Action (NA).
Table 2: Results of the adversarial robustness for Drop-Bottleneck (DB) and Variational Information Bottle-neck (VIB) (Alemi et al., 2017) with the targeted `2and '∞ attacks (Carlini & Wagner, 2017). Succ. de-notes the attack success rate in % (lower is better), andDist. is the average perturbation distance over success-ful adversarial examples (higher is better).
Table 3: Results of the adversarial robustness for Drop-Bottleneck (DB) and the mutual information-based fea-ture selection with the targeted '2 and '∞ attacks (Car-lini & Wagner, 2017), using the same number of fea-tures. Succ. denotes the attack success rate in % (loweris better), and Dist. is the average perturbation distanceover successful adversarial examples (higher is better).
Table 4: Results of the adversarial robustness for Drop-Bottleneck (DB) and Variational ConditionalEntropy Bottleneck (VCEB) (Fischer, 2020; Fischer & Alemi, 2020) with the targeted '2 and '∞attacks (Carlini & Wagner, 2017). Succ. denotes the attack success rate in % (lower is better), andDist is the average perturbation distance over successful adversarial examples (higher is better). *Pfor VCEB is annealed from 100 to the final ρ over the first 100000 training steps.
Table 5: Comparison of the average episodic sum of rewards in DMLab tasks (over 30 runs), wherePPO + Ours (No-Drop-Bottleneck) denotes our exploration method without DB. The original (i.e.
Table 6: Hyperparameters of PPO (Schulman et al., 2017), PPO + ICM (Pathak et al., 2017), PPO +ECO (Savinov et al., 2019), and PPO + Ours for the VizDoom experiments.
Table 7: Hyperparameters of PPO (Schulman et al., 2017), PPO + ICM (Pathak et al., 2017), PPO +EC/ECO (Savinov et al., 2019), and PPO + Ours for the DMLab experiments.
Table 8: The network architecture for the occluded image classification experiments.
