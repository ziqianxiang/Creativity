Published as a conference paper at ICLR 2021
How Neural Networks Extrapolate:
From Feedforward to Graph Neural Networks
Keyulu Xu*, Mozhi Zhang^, Jingling Lir Simon S. Du§, Ken-ichi Kawarabayashi’,
Stefanie Jegelkat
* Massachusetts Institute of Technology (MIT)
^University of Maryland
§University of Washington
’National Institute of Informatics (NII)
keyulu@mit.edu, keyulux@csail.mit.edu
Ab stract
We study how neural networks trained by gradient descent extrapolate, i.e., what
they learn outside the support of the training distribution. Previous works report
mixed empirical results when extrapolating with neural networks: while feedfor-
ward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate
well in certain simple tasks, Graph Neural Networks (GNNs) - structured networks
with MLP modules - have shown some success in more complex tasks. Working
towards a theoretical explanation, we identify conditions under which MLPs and
GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly
converge to linear functions along any direction from the origin, which implies that
ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably
learn a linear target function when the training distribution is sufficiently “diverse”.
Second, in connection to analyzing the successes and limitations of GNNs, these
results suggest a hypothesis for which we provide theoretical and empirical evi-
dence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g.,
larger graphs or edge weights) relies on encoding task-specific non-linearities in
the architecture or features. Our theoretical analysis builds on a connection of
over-parameterized networks to the neural tangent kernel. Empirically, our theory
holds across different training settings.
1	Introduction
Humans extrapolate well in many tasks. For example, we can apply arithmetics to arbitrarily large
numbers. One may wonder whether a neural network can do the same and generalize to examples
arbitrarily far from the training data (Lake et al., 2017). Curiously, previous works report mixed
extrapolation results with neural networks. Early works demonstrate feedforward neural networks,
a.k.a. multilayer perceptrons (MLPs), fail to extrapolate well when learning simple polynomial
functions (Barnard & Wessels, 1992; Haley & Soloway, 1992). However, recent works show Graph
Neural Networks (GNNs) (Scarselli et al., 2009), a class of structured networks with MLP building
blocks, can generalize to graphs much larger than training graphs in challenging algorithmic tasks,
such as predicting the time evolution of physical systems (Battaglia et al., 2016), learning graph
algorithms (Velickovic et al., 2020), and solving mathematical equations (Lample & Charton, 2020).
To explain this puzzle, we formally study how neural networks trained by gradient descent (GD)
extrapolate, i.e., what they learn outside the support of training distribution. We say a neural network
extrapolates well if it learns a task outside the training distribution. At first glance, it may seem
that neural networks can behave arbitrarily outside the training distribution since they have high
capacity (Zhang et al., 2017) and are universal approximators (Cybenko, 1989; Funahashi, 1989;
Hornik et al., 1989; Kurkova, 1992). However, neural networks are constrained by gradient descent
training (Hardt et al., 2016; Soudry et al., 2018). In our analysis, we explicitly consider such implicit
bias through the analogy of the training dynamics of over-parameterized neural networks and kernel
regression via the neural tangent kernel (NTK) (Jacot et al., 2018).
1
Published as a conference paper at ICLR 2021
Figure 1: How ReLU MLPs extrapolate. We train MLPs to learn nonlinear functions (grey) and
plot their predictions both within (blue) and outside (black) the training distribution. MLPs converge
quickly to linear functions outside the training data range along directions from the origin (Theorem 1).
Hence, MLPs do not extrapolate well in most nonlinear tasks. But, with appropriate training data,
MLPs can provably extrapolate linear target functions (Theorem 2).
GNN Architectures
hUk )=自 MLP( k)(hVk -1), h尸,w (v, u))
ML MLP has to learn non-linear steps
hUk) = ^inv MLP(k)(hVk-1), hi(k-1), w(v, U))
M MLP learns linear steps
DP Algorithm
(Target Function)
f: hard for extrapolation
X	f(G)
G: input graph	h: input transform
g: easier for extrapolation
h (G) -g→ f (G)
(a)	Network architecture
(b)	Input representation
d [ k ][ u ]=mnv
d[k - 1][V] + w(v, u)
Figure 2: How GNNs extrapolate. Since MLPs can extrapolate well when learning linear functions,
we hypothesize that GNNs can extrapolate well in dynamic programming (DP) tasks if we encode
appropriate non-linearities in the architecture (left) and input representation (right; through domain
knowledge or representation learning). The encoded non-linearities may not be necessary for
interpolation, as they can be approximated by MLP modules, but they help extrapolation. We support
the hypothesis theoretically (Theorem 3) and empirically (Figure 6).
Starting with feedforward networks, the simplest neural networks and building blocks of more
complex architectures such as GNNs, we establish that the predictions of over-parameterized MLPs
with ReLU activation trained by GD converge to linear functions along any direction from the origin.
We prove a convergence rate for two-layer networks and empirically observe that convergence often
occurs close to the training data (Figure 1), which suggests ReLU MLPs cannot extrapolate well for
most nonlinear tasks. We emphasize that our results do not follow from the fact that ReLU networks
have finitely many linear regions (Arora et al., 2018; Hanin & Rolnick, 2019; Hein et al., 2019).
While having finitely many linear regions implies ReLU MLPs eventually become linear, it does not
say whether MLPs will learn the correct target function close to the training distribution. In contrast,
our results are non-asymptotic and quantify what kind of functions MLPs will learn close to the
training distribution. Second, we identify a condition when MLPs extrapolate well: the task is linear
and the geometry of the training distribution is sufficiently “diverse”. To our knowledge, our results
are the first extrapolation results of this kind for feedforward neural networks.
We then relate our insights into feedforward neural networks to GNNs, to explain why GNNs
extrapolate well in some algorithmic tasks. Prior works report successful extrapolation for tasks that
can be solved by dynamic programming (DP) (Bellman, 1966), which has a computation structure
aligned with GNNs (Xu et al., 2020). DP updates can often be decomposed into nonlinear and linear
steps. Hence, we hypothesize that GNNs trained by GD can extrapolate well in a DP task, if we
encode appropriate non-linearities in the architecture and input representation (Figure 2). Importantly,
encoding non-linearities may be unnecessary for GNNs to interpolate, because the MLP modules
can easily learn many nonlinear functions inside the training distribution (Cybenko, 1989; Hornik
et al., 1989; Xu et al., 2020), but it is crucial for GNNs to extrapolate correctly. We prove this
hypothesis for a simplified case using Graph NTK (Du et al., 2019b). Empirically, we validate the
hypothesis on three DP tasks: max degree, shortest paths, and n-body problem. We show GNNs with
appropriate architecture, input representation, and training distribution can predict well on graphs
with unseen sizes, structures, edge weights, and node features. Our theory explains the empirical
success in previous works and suggests their limitations: successful extrapolation relies on encoding
task-specific non-linearities, which requires domain knowledge or extensive model search. From a
broader standpoint, our insights go beyond GNNs and apply broadly to other neural networks.
2
Published as a conference paper at ICLR 2021
To summarize, we study how neural networks extrapolate. First, ReLU MLPs trained by GD converge
to linear functions along directions from the origin with a rate of O(1/t). Second, to explain why
GNNs extrapolate well in some algorithmic tasks, we prove that ReLU MLPs can extrapolate well in
linear tasks, leading to a hypothesis: a neural network can extrapolate well when appropriate non-
linearities are encoded into the architecture and features. We prove this hypothesis for a simplified
case and provide empirical support for more general settings.
1.1	Related work
Early works show example tasks where MLPs do not extrapolate well, e.g. learning simple polynomi-
als (Barnard & Wessels, 1992; Haley & Soloway, 1992). We instead show a general pattern of how
ReLU MLPs extrapolate and identify conditions for MLPs to extrapolate well. More recent works
study the implicit biases induced on MLPs by gradient descent, for both the NTK and mean field
regimes (Bietti & Mairal, 2019; Chizat & Bach, 2018; Song et al., 2018). Related to our results, some
works show MLP predictions converge to “simple” piecewise linear functions, e.g., with few linear
regions (Hanin & Rolnick, 2019; Maennel et al., 2018; Savarese et al., 2019; Williams et al., 2019).
Our work differs in that none of these works explicitly studies extrapolation, and some focus only
on one-dimensional inputs. Recent works also show that in high-dimensional settings of the NTK
regime, MLP is asymptotically at most a linear predictor in certain scaling limits (Ba et al., 2020;
Ghorbani et al., 2019). We study a different setting (extrapolation), and our analysis is non-asymptotic
in nature and does not rely on random matrix theory.
Prior works explore GNN extrapolation by testing on larger graphs (Battaglia et al., 2018; Santoro
et al., 2018; Saxton et al., 2019; Velickovic et al., 2020). We are the first to theoretically study GNN
extrapolation, and we complete the notion of extrapolation to include unseen features and structures.
2	Preliminaries
We begin by introducing our setting. Let X be the domain of interest, e.g., vectors or graphs. The
task is to learn an underlying function g : X → R with a training set {(xi , yi)}in=1 ⊂ D, where
yi = g(xi ) and D is the support of training distribution. Previous works have extensively studied
in-distribution generalization where the training and the test distributions are identical (Valiant, 1984;
Vapnik, 2013); i.e., D = X. In contrast, extrapolation addresses predictions on a domain X that is
larger than the support of the training distribution D. We will say a model extrapolates well if it has a
small extrapolation error.
Definition 1. (Extrapolation error). Let f : X → R be a model trained on {(xi, yi)}in=1 ⊂ D with
underlying function g : X → R. Let P be a distribution over X \ D and let ` : R × R → R be a loss
function. We define the extrapolation error of f as Ex〜P['(f (x),g(x))].
We focus on neural networks trained by gradient descent (GD) or its variants with squared loss. We
study two network architectures: feedforward and graph neural networks.
Graph Neural Networks. GNNs are structured networks operating on graphs with MLP mod-
ules (Battaglia et al., 2018; Xu et al., 2019). Let G = (V, E) be a graph. Each node u ∈ V has a
feature vector xu, and each edge (u, v) ∈ E has a feature vector w(u,v). GNNs recursively compute
node representations h(uk) at iteration k (Gilmer et al., 2017; Xu et al., 2018). Initially, h(u0) = xu.
(k)
For k = 1..K, GNNs update hu by aggregating the neighbor representations. We can optionally
compute a graph representation hG by aggregating the final node representations. That is,
h(uk) = X MLP(k)h(uk-1), h(vk-1), w(v,u),	hG = MLP(K+1) X h(uK).	(1)
v∈N (u)	u∈G
The final output is the graph representation hG or final node representations h(uK) depending on the
task. We refer to the neighbor aggregation step for h(uk) as aggregation and the pooling step in hG as
readout. Previous works typically use sum-aggregation and sum-readout (Battaglia et al., 2018). Our
results indicate why replacing them may help extrapolation (Section 4).
3
Published as a conference paper at ICLR 2021
Figure 3: Conditions for ReLU MLPs to extrapolate well. We train MLPs to learn linear functions
(grey) with different training distributions (blue) and plot out-of-distribution predictions (black).
Following Theorem 2, MLPs extrapolate well when the training distribution (blue) has support in all
directions (first panel), but not otherwise: in the two middle panels, some dimensions of the training
data are constrained to be positive (red arrows); in the last panel, one dimension is a fixed constant.
(a) Different target functions
(b) Different training distributions for linear target
Figure 4: Extrapolation performance of ReLU MLPs. We plot the distributions of MAPE (mean
absolute percentage error) of MLPs trained with various hyperparameters (depth, width, learning rate,
batch size). (a) Learning different target functions; (b) Different training distributions for learning
linear target functions: “all” covers all directions, “fix1” has one dimension fixed to a constant, and
“negd” has d dimensions constrained to negative values. ReLU MLPs generally do not extrapolate
well unless the target function is linear along each direction (Figure 4a), and extrapolate linear target
functions if the training distribution covers sufficiently many directions (Figure 4b).
3	How Feedforward Neural Networks Extrapolate
Feedforward networks are the simplest neural networks and building blocks of more complex
architectures such as GNNs, so we first study how they extrapolate when trained by GD. Throughout
the paper, we assume ReLU activation. Section 3.3 contains preliminary results for other activations.
3.1	Linear Extrapolation Behavior of ReLU MLPs
By architecture, ReLU networks learn piecewise linear functions, but what do these regions precisely
look like outside the support of the training data? Figure 1 illustrates examples of how ReLU
MLPs extrapolate when trained by GD on various nonlinear functions. These examples suggest that
outside the training support, the predictions quickly become linear along directions from the origin.
We systematically verify this pattern by linear regression on MLPs’ predictions: the coefficient of
determination (R2) is always greater than 0.99 (Appendix C.2). That is, ReLU MLPs “linearize”
almost immediately outside the training data range.
We formalize this observation using the implicit biases of neural networks trained by GD via the
neural tangent kernel (NTK): optimization trajectories of over-parameterized networks trained by
GD are equivalent to those of kernel regression with a specific neural tangent kernel, under a set of
assumptions called the “NTK regime” (Jacot et al., 2018). We provide an informal definition here;
for further details, we refer the readers to Jacot et al. (2018) and Appendix A.
Definition 2. (Informal) A neural network trained in the NTK regime is infinitely wide, randomly
initialized with certain scaling, and trained by GD with infinitesimal steps.
Prior works analyze optimization and in-distribution generalization of over-parameterized neural
networks via NTK (Allen-Zhu et al., 2019a;b; Arora et al., 2019a;b; Cao & Gu, 2019; Du et al.,
2019c;a; Li & Liang, 2018; Nitanda & Suzuki, 2021). We instead analyze extrapolation.
Theorem 1 formalizes our observation from Figure 1: outside the training data range, along any
direction tv from the origin, the prediction of a two-layer ReLU MLP quickly converges to a linear
4
Published as a conference paper at ICLR 2021
function with rate O( t). The linear coefficients βv and the constant terms in the convergence rate
depend on the training data and direction v. The proof is in Appendix B.1.
Theorem 1. (Linear extrapolation). Suppose we train a two-layer ReLU MLP f : Rd → R
with squared loss in the NTK regime. For any direction v ∈ Rd, let x0 = tv. As t → ∞,
f (xo + hv) 一 f (xo) → βv ∙ h for any h > 0, where βv is a Constant linear coefficient. Moreover,
given C > 0, for t = O(ɪ), we have | f(x0+hh)-f(x0)一 βv| < c.
ReLU networks have finitely many linear regions (Arora et al., 2018; Hanin & Rolnick, 2019), hence
their predictions eventually become linear. In contrast, Theorem 1 is a more fine-grained analysis
of how MLPs extrapolate and provides a convergence rate. While Theorem 1 assumes two-layer
networks in the NTK regime, experiments confirm that the linear extrapolation behavior happens
across networks with different depths, widths, learning rates, and batch sizes (Appendix C.1 and C.2).
Our proof technique potentially also extends to deeper networks.
Theorem 1 implies which target functions a ReLU MLP may be able to match outside the training data:
only functions that are almost-linear along the directions away from the origin. Indeed, Figure 4a
shows ReLU MLPS do not extrapolate target functions such as x>Ax (quadratic), Pd=1 cos(2∏ ∙ x(i))
(cos), and Pd=1 λ∕x⑴(sqrt), where x(i) is the i-th dimension of x. With suitable hyperparameters,
MLPs extrapolate the L1 norm correctly, which satisfies the directional linearity condition.
Figure 4a provides one more positive result: MLPs extrapolate linear target functions well, across
many different hyperparameters. While learning linear functions may seem very limited at first, in
Section 4 this insight will help explain extrapolation properties of GNNs in non-linear practical tasks.
Before that, we first theoretically analyze when MLPs extrapolate well.
3.2	When ReLU MLPs Provably Extrapolate Well
Figure 4a shows that MLPs can extrapolate well when the target function is linear. However, this is
not always true. In this section, we show that successful extrapolation depends on the geometry of
training data. Intuitively, the training distribution must be “diverse” enough for correct extrapolation.
We provide two conditions that relate the geometry of the training data to extrapolation. Lemma 1
states that over-parameterized MLPs can learn a linear target function with only 2d examples.
Lemma 1. Let g(x) = β>x be the target function for β ∈ Rd. Suppose {xi}in=1 contains an
orthogonal basis {x^i}d=1 and {-x^i}d=1. Ifwe train a two-layer ReLU MLP f on {(xi, yi)}n=1
with squared loss in the NTK regime, then f(x) = β>x for all x ∈ Rd.
Lemma 1 is mainly of theoretical interest, as the 2d examples need to be carefully chosen. Theorem 2
builds on Lemma 1 and identifies a more practical condition for successful extrapolation: if the
support of the training distribution covers all directions (e.g., a hypercube that covers the origin), the
MLP converges to a linear target function with sufficient training data.
Theorem 2. (Conditions for extrapolation). Let g(x) = β>x be the target function for β ∈ Rd.
Suppose {xi}in=1 is sampled from a distribution whose support D contains a connected subset S,
where for any non-zero w ∈ Rd, there exists k > 0 so that kw ∈ S. Ifwe train a two-layer ReLU
MLP f : Rd → R on {(xi, yi)}n=1 with squared loss in the NTK regime, f (x) -→ β>x as n → ∞.
Experiments: geometry of training data affects extrapolation. The condition in Theorem 2
formalizes the intuition that the training distribution must be “diverse” for successful extrapolation,
e.g., D includes all directions. Empirically, the extrapolation error is indeed small when the condition
of Theorem 2 is satisfied (“all” in Figure 4b). In contrast, the extrapolation error is much larger when
the training examples are restricted to only some directions (Figure 4b and Figure 3).
Relating to previous works, Theorem 2 suggests why spurious correlations may hurt extrapolation,
complementing the causality arguments (Arjovsky et al., 2019; Peters et al., 2016; Rojas-Carulla
et al., 2018). When the training data has spurious correlations, some combinations of features are
missing; e.g., camels might only appear in deserts in an image collection. Therefore, the condition for
Theorem 2 no longer holds, and the model may extrapolate incorrectly. Theorem 2 is also analogous
to an identifiability condition for linear models, but stricter. We can uniquely identify a linear function
if the training data has full (feature) rank. MLPs are more expressive, so identifying the linear target
function requires additional constraints.
5
Published as a conference paper at ICLR 2021
qUdM UWO ldllll IIIICdl
(a) tanh activation
(b) cosine activation
(c) quadratic activation
Figure 5: Extrapolation performance of MLPs with other activation. MLPs can extrapolate well
when the activation is “similar” to the target function. When learning quadratic with quadratic
activation, 2-layer networks (quad-2) extrapolate well, but 4-layer networks (quad-4) do not.
To summarize, we analyze how ReLU MLPs extrapolate and provide two insights: (1) MLPs
cannot extrapolate most nonlinear tasks due to their linear extrapolation (Theorem 1); and (2) MLPs
extrapolate well when the target function is linear, if the training distribution is “diverse” (Theorem 2).
In the next section, these results help us understand how more complex networks extrapolate.
3.3	MLPs with Other Activation Functions
Before moving on to GNNs, we complete the picture of MLPs with experiments on other activation
functions: tanh σ(x) = tanh(x), cosine σ(x) = cos(x) (Lapedes & Farber, 1987; McCaughan,
1997; Sopena & Alquezar, 1994), and quadratic σ(x) = x2 (Du & Lee, 2018; Livni et al., 2014).
Details are in Appendix C.4. MLPs extrapolate well when the activation and target function are
similar; e.g., tanh activation extrapolates well when learning tanh, but not other functions (Figure 5).
Moreover, each activation function has different limitations. To extrapolate the tanh function with
tanh activation, the training data range has to be sufficiently wide. When learning a quadratic function
with quadratic activation, only two-layer networks extrapolate well as more layers lead to higher-order
polynomials. Cosine activations are hard to optimize for high-dimensional data, so we only consider
one/two dimensional cosine target functions.
4	How Graph Neural Networks Extrapolate
Above, we saw that extrapolation in nonlinear tasks is hard for MLPs. Despite this limitation,
GNNs have been shown to extrapolate well in some nonlinear algorithmic tasks, such as intuitive
physics (Battaglia et al., 2016; Janner et al., 2019), graph algorithms (Battaglia et al., 2018; Velickovic
et al., 2020), and symbolic mathematics (Lample & Charton, 2020). To address this discrepancy, we
build on our MLP results and study how GNNs trained by GD extrapolate.
4.1	Hypothesis: Linear Algorithmic Alignment Helps Extrapolation
We start with an example: training GNNs to solve the shortest path problem. For this task, prior
works observe that a modified GNN architecture with min-aggregation can generalize to graphs larger
than those in the training set (Battaglia et al., 2018; Velickovic et al., 2020):
hUk)= min MLP(k)(hUkT),hVkT),w(v,u)).	⑵
v∈N (u)
We first provide an intuitive explanation (Figure 2a). Shortest path can be solved by the Bellman-Ford
(BF) algorithm (Bellman, 1958) with the following update:
d[k][u] = min d[k - 1][v] + w(v, u),	(3)
v∈N (u)
where w(v, u) is the weight of edge (v, u), and d[k][u] is the shortest distance to node u within k
steps. The two equations can be easily aligned: GNNs simulate the BF algorithm if its MLP modules
learn a linear function d[k- 1][v] +w(v, u). Since MLPs can extrapolate linear tasks, this “alignment”
may explain why min-aggregation GNNs can extrapolate well in this task.
6
Published as a conference paper at ICLR 2021
For comparison, we can reason why we would not expect GNNs with the more commonly used
sum-aggregation (Eqn. 1) to extrapolate well in this task. With sum-aggregation, the MLP modules
need to learn a nonlinear function to simulate the BF algorithm, but Theorem 1 suggests that they
will not extrapolate most nonlinear functions outside the training support.
We can generalize the above intuition to other algorithmic tasks. Many tasks where GNNs extrapolate
well can be solved by dynamic programming (DP) (Bellman, 1966), an algorithmic paradigm with a
recursive structure similar to GNNs’ (Eqn. 1) (Xu et al., 2020).
Definition 3. Dynamic programming (DP) is a recursive procedure with updates
Answer[k][s] = DP-Update({Answer[k - 1][s0]} , s0 = 1...n),	(4)
where Answer[k][s] is the solution to a sub-problem indexed by iteration k and state s, and DP-Update
is a task-specific update function that solves the sub-problem based on the previous iteration.
From a broader standpoint, we hypothesize that: if we encode appropriate non-linearities into the
model architecture and input representations so that the MLP modules only need to learn nearly linear
steps, then the resulting neural network can extrapolate well.
Hypothesis 1. (Linear algorithmic alignment). Let f : X → R be the underlying function and N
a neural network with m MLP modules. Suppose there exist m linear functions {gi}im=1 so that by
replacing N’s MLP modules with gi’s, N simulates f. Given > 0, there exists {(xi, f (xi))}in=1 ⊂
D ( X so that N trained on {(xi, f(xi))}in=1 by GD with squared loss learns f with kf - fk < .
Our hypothesis builds on the algorithmic alignment framework of (Xu et al., 2020), which states that
a neural network interpolates well if the modules are “aligned” to easy-to-learn (possibly nonlinear)
functions. Successful extrapolation is harder: the modules need to align with linear functions.
Applications of linear algorithmic alignment. In general, linear algorithmic alignment is not
restricted to GNNs and applies broadly to neural networks. To satisfy the condition, we can encode
appropriate nonlinear operations in the architecture or input representation (Figure 2). Learning DP
algorithms with GNNs is one example of encoding non-linearity in the architecture (Battaglia et al.,
2018; Corso et al., 2020). Another example is to encode log-and-exp transforms in the architecture to
help extrapolate multiplication in arithmetic tasks (Trask et al., 2018; Madsen & Johansen, 2020).
Neural symbolic programs take a step further and encode a library of symbolic operations to help
extrapolation (Johnson et al., 2017; Mao et al., 2019; Yi et al., 2018).
For some tasks, it may be easier to change the input representation (Figure 2b). Sometimes, we
can decompose the target function f as f = g ◦ h into a feature embedding h and a “simpler”
target function g that our model can extrapolate well. We can obtain h via specialized features or
feature transforms using domain knowledge (Lample & Charton, 2020; Webb et al., 2020), or via
representation learning (e.g., BERT) with unlabeled out-of-distribution data in X \ D (Chen et al.,
2020; Devlin et al., 2019; Hu et al., 2020; Mikolov et al., 2013b; Peters et al., 2018). This brings a
new perspective of how representations help extrapolation in various application areas. For example,
in natural language processing, pretrained representations (Mikolov et al., 2013a; Wu & Dredze,
2019) and feature transformation using domain knowledge (Yuan et al., 2020; Zhang et al., 2019)
help models generalize across languages, a special type of extrapolation. In quantitative finance,
identifying the right “factors” or features is crucial for deep learning models as the financial markets
may frequently be in extrapolation regimes (Banz, 1981; Fama & French, 1993; Ross, 1976).
Linear algorithmic alignment explains successful extrapolation in the literature and suggests that
extrapolation is harder in general: encoding appropriate non-linearity often requires domain expertise
or model search. Next, we provide theoretical and empirical support for our hypothesis.
4.2 Theoretical and Empirical Support
We validate our hypothesis on three DP tasks: max degree, shortest path, and n-body problem, and
prove the hypothesis for max degree. We highlight the role of graph structures in extrapolation.
Theoretical analysis. We start with a simple yet fundamental task: learning the max degree of a
graph, a special case of DP with one iteration. As a corollary of Theorem 1, the commonly used
sum-based GNN (Eqn. 1) cannot extrapolate well (proof in Appendix B.4).
7
Published as a conference paper at ICLR 2021
sum pooling	max/min p∞ling
70.6
J	43.8
0.0	65 0.0	0.0	6∙1 0.0
extrapolate—interpolate	extrapolate—interpolate
max degree	shortest path
(a) Importance of architecture
original features
improved features
Figure 6:	Extrapolation for algorithmic tasks. Each column indicates the task and mean average
percentage error (MAPE). Encoding appropriate non-linearity in the architecture or representation
is less helpful for interpolation, but significantly improves extrapolation. Left: In max degree and
shortest path, GNNs that appropriately encode max/min extrapolate well, but GNNs with sum-pooling
do not. Right: With improved input representation, GNNs extrapolate better for the n-body problem.
general
tree
∞mplete
expander
cycle
ladder
4regular
path
0.0
0.0
0.1
0.1
64 11.0
12.5
94.5
path
tree
cycle
ladder
4regular
general
expander
∞mplete
8.9
33.6
(b) Shortest path with GNNs that encode min
(a) Max degree with GNNs that encode max
Figure 7:	Importance of the training graph structure. Rows indicate the graph structure covered
by the training set and the extrapolation error (MAPE). In max degree, GNNs with max readout
extrapolate well if the max/min degrees of the training graphs are not restricted (Theorem 3). In
shortest path, the extrapolation errors of min GNNs follow a U-shape in the sparsity of the training
graphs. More results may be found in Appendix D.2.
Corollary 1. GNNs with sum-aggregation and sum-readout do not extrapolate well in Max Degree.
To achieve linear algorithmic alignment, we can encode the only non-linearity, the max function, in
the readout. Theorem 3 confirms that a GNN with max-readout can extrapolate well in this task.
Theorem 3. (Extrapolation with GNNs). Assume all nodes have the same feature. Let g and
g0 be the max/min degree function, respectively. Let {(Gi , g(Gi)}in=1 be the training set. If
{(g(Gi),g0(Gi),g(Gi) ∙ Nimax,g0(Gi) ∙ Nmm)}n=ι SPans R4, where Nmax and Nmln are the
number of nodes that have max/min degree on Gi, then one-layer max-readout GNNs trained
on {(Gi, g(Gi))}in=1 with squared loss in the NTK regime learn g.
Theorem 3 does not follow immediately from Theorem 2, because MLP modules in GNNs only
receive indirect supervision. We analyze the GraPh NTK (Du et al., 2019b) to prove Theorem 3 in
Appendix B.5. While Theorem 3 assumes identical node features, we empirically observe similar
results for both identical and non-identical features (Figure 16 in Appendix).
Interpretation of conditions. The condition in Theorem 3 is analogous to that in Theorem 2. Both
theorems require diverse training data, measured by graPh structure in Theorem 3 or directions in
Theorem 2. In Theorem 3, the condition is violated if all training graphs have the same max or min
node degrees, e.g., when training data are from one of the following families: path, C-regular graphs
(regular graphs with degree C), cycle, and ladder.
Experiments: architectures that help extrapolation. We validate our theoretical analysis with
two DP tasks: max degree and shortest path (details in Appendix C.5 and C.6). While previous works
only test on graphs with different sizes (Battaglia et al., 2018; Velickovic et al., 2020), we also test on
graphs with unseen structure, edge weights and node features. The results support our theory. For
max degree, GNNs with max-readout are better than GNNs with sum-readout (Figure 6a), confirming
Corollary 1 and Theorem 3. For shortest path, GNNs with min-readout and min-aggregation are
better than GNNs with sum-readout (Figure 6a).
Experiments confirm the importance of training graphs structure (Figure 7). Interestingly, the two
tasks favor different graph structure. For max degree, as Theorem 3 predicts, GNNs extrapolate well
when trained on trees, complete graphs, expanders, and general graphs, and extrapolation errors are
8
Published as a conference paper at ICLR 2021
higher when trained on 4-regular, cycles, or ladder graphs. For shortest path, extrapolation errors
follow a U-shaped curve as we change the sparsity of training graphs (Figure 7b and Figure 18 in
Appendix). Intuitively, models trained on sparse or dense graphs likely learn degenerative solutions.
Experiments: representations that help extrapolation. Finally, we show a good input representa-
tion helps extrapolation. We study the n-body problem (Battaglia et al., 2016; Watters et al., 2017)
(Appendix C.7), that is, predicting the time evolution ofn objects in a gravitational system. Following
previous work, the input is a complete graph where the nodes are the objects (Battaglia et al., 2016).
The node feature for u is the concatenation of the object’s mass mu, position x(ut) , and velocity vu(t)
at time t. The edge features are set to zero. We train GNNs to predict the velocity of each object u at
time t + 1. The true velocity f(G; u) for object u is approximately
f (G; U) ≈ Vu + aU ∙ dt, aU = C ∙ X II t m t∣∣3 ∙ (Xv - Xu) ,	⑸
v6=u kxtu -	xtv k23
where C is a constant. To learn f, the MLP modules need to learn a nonlinear function. Therefore,
GNNs do not extrapolate well to unseen masses or distances (“original features” in Figure 6b). We
instead use an improved representation h(G) to encode non-linearity. At time t, we transform the
edge features of (u, V) from zero to w(uu)v) = mv ∙ (XWt) - Xut))/kXut) - Xvt)∣∣2∙ The new edge
features do not add information, but the MLP modules now only need to learn linear functions, which
helps extrapolation (“improved features” in Figure 6b).
5	Connections to Other Out-of-Distribution Settings
We discuss several related settings. Intuitively, from the viewpoint of our results above, methods in
related settings may improve extrapolation by 1) learning useful non-linearities beyond the training
data range and 2) mapping relevant test data to the training data range.
Domain adaptation studies generalization to a specific target domain (Ben-David et al., 2010; Blitzer
et al., 2008; Mansour et al., 2009). Typical strategies adjust the training process: for instance, use
unlabeled samples from the target domain to align the target and source distributions (Ganin et al.,
2016; Zhao et al., 2018). Using target domain data during training may induce useful non-linearities
and may mitigate extrapolation by matching the target and source distributions, though the correctness
of the learned mapping depends on the label distribution (Zhao et al., 2019).
Self-supervised learning on a large amount of unlabeled data can learn useful non-linearities beyond
the labeled training data range (Chen et al., 2020; Devlin et al., 2019; He et al., 2020; Peters et al.,
2018). Hence, our results suggest an explanation why pre-trained representations such as BERT
improve out-of-distribution robustness (Hendrycks et al., 2020). In addition, self-supervised learning
could map semantically similar data to similar representations, so some out-of-domain examples
might fall inside the training distribution after the mapping.
Invariant models aim to learn features that respect specific invariances across multiple training
distributions (Arjovsky et al., 2019; Rojas-Carulla et al., 2018; Zhou et al., 2021). If the model indeed
learns these invariances, which can happen in the linear case and when there are confounders or
anti-causal variables (Ahuja et al., 2021; Rosenfeld et al., 2021), this may essentially increase the
training data range, since variations in the invariant features may be ignored by the model.
Distributional robustness considers small adversarial perturbations of the data distribution, and
ensures that the model performs well under these (Goh & Sim, 2010; Sagawa et al., 2020; Sinha et al.,
2018; Staib & Jegelka, 2019). We instead look at more global perturbations. Still, one would expect
that modifications that help extrapolation in general also improve robustness to local perturbations.
6	Conclusion
This paper is an initial step towards formally understanding how neural networks trained by gradient
descent extrapolate. We identify conditions under which MLPs and GNNs extrapolate as desired. We
also suggest an explanation how GNNs have been able to extrapolate well in complex algorithmic
tasks: encoding appropriate non-linearity in architecture and features can help extrapolation. Our
results and hypothesis agree with empirical results, in this paper and in the literature.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We thank Ruosong Wang, Tianle Cai, Han Zhao, Yuichi Yoshida, Takuya Konishi, Toru Lin, Weihua
Hu, Matt J. Staib, Yichao Zhou, Denny Wu, Tianyi Yang, and Dingli (Leo) Yu for insightful
discussions. This research was supported by NSF CAREER award 1553284, NSF III 1900933, and a
Chevron-MIT Energy Fellowship. This research was also supported by JST ERATO JPMJER1201
and JSPS Kakenhi JP18H05291. MZ was supported by ODNI, IARPA, via the BETTER Program
contract 2019-19051600005. The views, opinions, and/or findings contained in this article are those of
the author and should not be interpreted as representing the official views or policies, either expressed
or implied, of the Defense Advanced Research Projects Agency, the Department of Defense, ODNI,
IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright annotation therein.
References
Kartik Ahuja, Jun Wang, Amit Dhurandhar, Karthikeyan Shanmugam, and Kush R. Varshney.
Empirical or invariant risk minimization? a sample complexity perspective. In International
Conference on Learning Representations, 2021.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in Neural Information Processing Systems,
pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019b.
Martin Arjovsky, Leon Bottou,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In International Conference on Learning Representations,
2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, pp. 8139-8148, 2019b.
Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu.
Harnessing the power of infinitely wide deep nets on small-data tasks. In International Conference
on Learning Representations, 2020.
Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of
two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning
Representations, 2020.
Rolf W Banz. The relationship between return and market value of common stocks. Journal of
financial economics, 9(1):3-18, 1981.
Etienne Barnard and LFA Wessels. Extrapolation and interpolation in neural network classifiers.
IEEE Control Systems Magazine, 12(5):50-53, 1992.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in Neural Information Processing
Systems, pp. 4502-4510, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
10
Published as a conference paper at ICLR 2021
Richard Bellman. On a routing problem. Quarterly ofapplied mathematics, 16(1):87-90, 1958.
Richard Bellman. Dynamic programming. Science, 153(3731):34-37, 1966.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems, pp. 12873-12884, 2019.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning
bounds for domain adaptation. In Advances in neural information processing systems, pp. 129-136,
2008.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems, pp. 10835-10845, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in Neural Information Processing Systems, 31:3036-
3046, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2933-2943, 2019.
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro LiO, and Petar Velickovic. Principal
neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems,
2020.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International Conference on Machine Learning, 2018.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems, pp. 5724-5734, 2019b.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019c.
Eugene F Fama and Kenneth R French. Common risk factors in the returns on stocks and bonds.
Journal of financial economics, 33(1):3-56, 1993.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks, 2(3):183-192, 1989.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
11
Published as a conference paper at ICLR 2021
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1273-1272, 2017.
Joel Goh and Melvyn Sim. Distributionally robust optimization and its tractable approximations.
Operations research, 58:902-917, 2010.
Pamela J Haley and DONALD Soloway. Extrapolation limitations of multilayer feedforward neural
networks. In International Joint Conference on Neural Networks, volume 4, pp. 25-30. IEEE,
1992.
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In International
Conference on Machine Learning, pp. 2596-2604, 2019.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 41-50,
2019.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.
Pretrained transformers improve out-of-distribution robustness. In Association for Computational
Linguistics, 2020.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. In International Conference on Learning
Representations, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, and Jiajun
Wu. Reasoning about physical interactions with object-centric models. In International Conference
on Learning Representations, 2019.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reason-
ing. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2989-2998,
2017.
Vera Kurkova. Kolmogorov’s theorem and multilayer neural networks. Neural networks, 5(3):
501-506, 1992.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
Guillaume Lample and Franois Charton. Deep learning for symbolic mathematics. In International
Conference on Learning Representations, 2020.
Alan Lapedes and Robert Farber. Nonlinear signal processing using neural networks: Prediction and
system modelling. Technical report, 1987.
12
Published as a conference paper at ICLR 2021
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Andreas Madsen and Alexander Rosenberg Johansen. Neural arithmetic units. In International
Conference on Learning Representations, 2020.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient Descent Quantizes ReLU Network
Features. arXiv e-prints, art. arXiv:1803.08367, March 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In Conference on Learning Theory, 2009.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro-
symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In
International Conference on Learning Representations, 2019.
David B McCaughan. On the properties of periodic perceptrons. In International Conference on
Neural Networks, 1997.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems, 2013b.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. In International Conference on Learning Representations, 2021.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
International Conference on Learning Representations, 2020.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant
prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 78(5):947-1012, 2016.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, 2018.
Mateo Rojas-Carulla, Bernhard SchOlkopf, Richard Turner, and Jonas Peters. Invariant models for
causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk mini-
mization. In International Conference on Learning Representations, 2021.
Stephen A Ross. The arbitrage theory of capital asset pricing. Journal of Economic Theory, 13(3):
341-360, 1976.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020.
Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. Measuring abstract
reasoning in neural networks. In International Conference on Machine Learning, pp. 4477-4486,
2018.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? In Conference on Learning Theory, 2019.
13
Published as a conference paper at ICLR 2021
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical
reasoning abilities of neural models. In International Conference on Learning Representations,
2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
Mei Song, Andrea Montanari, and P Nguyen. A mean field view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, 115:E7665-E7671, 2018.
JM Sopena and R Alquezar. Improvement of learning in recurrent networks by substituting the
sigmoid activation function. In International Conference on Artificial Neural Networks, pp.
417-420. Springer, 1994.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel
methods. In Advances in Neural Information Processing Systems, pp. 9134-9144, 2019.
Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic
logic units. In Advances in Neural Information Processing Systems, pp. 8035-8044, 2018.
Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth annual ACM symposium
on Theory of computing, pp. 436-445. ACM, 1984.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution
of graph algorithms. In International Conference on Learning Representations, 2020.
Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea
Tacchetti. Visual interaction networks: Learning a physics simulator from video. In Advances in
neural information processing systems, pp. 4539-4547, 2017.
Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall OReilly, and Jonathan
Cohen. Learning representations that support extrapolation. In International Conference on
Machine Learning, pp. 10136-10146. PMLR, 2020.
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, and Joan Bruna.
Gradient dynamics of shallow univariate relu networks. In Advances in Neural Information
Processing Systems, pp. 8376-8385, 2019.
Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pp. 833-844, 2019.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453-5462, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken ichi Kawarabayashi, and Stefanie Jegelka.
What can neural networks reason about? In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=rJxbJeHFPS.
14
Published as a conference paper at ICLR 2021
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-
symbolic vqa: Disentangling reasoning from vision and language understanding. In Advances in
Neural Information Processing Systems ,pp.1031-1042, 2018.
Michelle Yuan, Mozhi Zhang, Benjamin Van Durme, Leah Findlater, and Jordan Boyd-Graber.
Interactive refinement of cross-lingual word embeddings. In Proceedings of Empirical Methods in
Natural Language Processing, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.
Mozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi, Stefanie Jegelka, and Jordan Boyd-Graber.
Are girls neko or shojo? cross-lingual alignment of non-isomorPhic embeddings with iterative
normalization. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 3180-3189, 2019.
Han Zhao, Shanghang Zhang, GUanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in neural information
processing systems, pp. 8559-8570, 2018.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532, 2019.
Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In
International Conference on Learning Representations, 2021.
15
Published as a conference paper at ICLR 2021
A Theoretical Background
In this section, we introduce theoretical background on neural tangent kernel (NTK), which draws an
equivalence between the training dynamics of infinitely-wide (or ultra-wide) neural networks and
that of kernel regression with respect to the neural tangent kernel.
Consider a general neural network f(θ, x) : X → R where θ ∈ Rm is the parameters in the network
and x ∈ X is the input. Suppose we train the neural network by minimizing the squared loss over
training data, '(θ) = 1 pn=1(f (θ, Xi) - yi)2, by gradient descent with infinitesimally small learning
rate, i.e., dθtt) = -V'(θ(t)). Let u(t) = (f(θ(t), Xi))n=ι be the network outputs. u(t) follows the
dynamics
du(t)
dt
-H(t)(u(t) - y),
(6)
where H(t) is an n × n matrix whose (i, j)-th entry is
HY 八	/ df (θ(t), Xi) df (θ(t), Xj 八
H (t)ij = ∖ —∂θ—，—∂θ— A
(7)
A line of works show that for sufficiently wide networks, H(t) stays almost constant during training,
i.e., H(t) = H(0) in the limit (Arora et al., 2019a;b; Allen-Zhu et al., 2019a; Du et al., 2019c;a; Li
& Liang, 2018; Jacot et al., 2018). Suppose network parameters are randomly initialized with certain
scaling, as network width goes to infinity, H(0) converges to a fixed matrix, the neural tangent kernel
(NTK) (Jacot et al., 2018):
NTK(X, X0)
E df( (θ(t), X)df(θ9, x0)
θ 〜w∖	∂ θ	,	∂θ
(8)
where W is Gaussian.
Therefore, the learning dynamics of sufficiently wide neural networks in this regime is equivalent
to that of kernel gradient descent with respect to the NTK. This implies the function learned by a
neural network at convergence on any specific training set, denoted by fNTK(X), can be precisely
characterized, and is equivalent to the following kernel regression solution
fNTK(x) = (NTK(x, X1),…,NTK(x, Xn)) ∙ NTK-inY,	(9)
where NTKtrain is the n × n kernel for training data, NTK(X, Xi ) is the kernel value between test
data X and training data Xi , and Y is the training labels.
We can in fact exactly calculate the neural tangent kernel matrix for certain architectures and activation
functions. The exact formula of NTK with ReLU activation has been derived for feedforward neural
networks (Jacot et al., 2018), convolutional neural networks (Arora et al., 2019b), and Graph Neural
Networks (Du et al., 2019b).
Our theory builds upon this equivalence of network learning and kernel regression to more precisely
characterize the function learned by a sufficiently-wide neural network given any specific training set.
In particular, the difference between the learned function and true function over the domain of X
determines the extrapolation error.
However, in general it is non-trivial to compute or analyze the functional form of what a neural
network learns using Eqn. 9, because the kernel regression solution using neural tangent kernel only
gives point-wise evaluation. Thus, we instead analyze the function learned by a network in the NTK’s
induced feature space, because representations in the feature space would give a functional form.
Lemma 2 makes this connection more precise: the solution to the kernel regression using neural
tangent kernel, which also equals over-parameterized network learning, is equivalent to a min-norm
solution among functions in the NTK’s induced feature space that fits all training data. Here the
min-norm refers to the RKHS norm.
Lemma 2. Let φ(X) be a feature map induced by a neural tangent kernel, for any X ∈ Rd. The
solution to kernel regression Eqn. 9 is equivalent to fNTK (X) = φ(X)>βNTK, where βNTK is
mβin kβ k2
s.t. φ(Xi)>β = yi, for i = 1, ..., n.
16
Published as a conference paper at ICLR 2021
We prove Lemma 2 in Appendix B.6. To analyze the learned functions as the min-norm solution
in feature space, we also need the explicit formula of an induced feature map of the corresponding
neural tangent kernel. The following lemma gives a NTK feature space for two-layer MLPs with
ReLU activation. It follows easily from the kernel formula described in Jacot et al. (2018); Arora
et al. (2019b); Bietti & Mairal (2019).
Lemma 3. An infinite-dimensional feature map φ(x) induced by the neural tangent kernel of a
two-layer multi-layer perceptron with ReLU activation function is
φ (x) = C (X ∙ I (w(k)>X ≥ 0), w(k)> X ∙ I (w(k)>X ≥ 0),…),	(10)
where WIkk 〜N (0, I), with k going to infinity. C is a constant, and I is the indicator function.
We prove Lemma 3 in Appendix B.7. The feature maps for other architectures, e.g., Graph Neural
Networks (GNNs) can be derived similarly. We analyze the Graph Neural Tangent Kernel (GNTK)
for a simple GNN architecture in Theorem 3.
We then use Lemma 2 and 3 to characterize the properties of functions learned by an over-
parameterized neural network. We precisely characterize the neural networks’ learned functions in
the NTK regime via solving the constrained optimization problem corresponding to the min-norm
function in NTK feature space with the constraint of fitting the training data.
However, there still remains many technical challenges. For example, provable extrapolation (exact
or asymptotic) is often not achieved with most training data distribution. Understanding the desirable
condition requires significant insights into the geometry properties of training data distribution, and
how they interact with the solution learned by neural networks. Our insights and refined analysis
shows in Rd space, we need to consider the directions of training data. In graphs, we need to consider,
in addition, the graph structure of training data. We refer readers to detailed proofs for the intuition
of data conditions. Moreover, since NTK corresponds to infinitely wide neural networks, the feature
space is of infinite dimension. The analysis of infinite dimensional spaces poses non-trivial technical
challenges too.
Since different theorems have their respective challenges and insights/techniques, we refer the
interested readers to the respective proofs for details. In Lemma 1 (proof in Appendix B.2), Theorem 2
(proof in Appendix B.3), and Theorem 1 (proof in Appendix B.1) we analyze over-parameterized
MLPs. The proof of Corollary 1 is in Appendix B.4. In Theorem 3 we analyze Graph Neural
Networks (proof in Appendix B.5).
B	Proofs
B.1 Proof of Theorem 1
To show neural network outputs f(X) converge to a linear function along all directions v, we will
analyze the function learned by a neural network on the training set {(Xi, yi)}in=1, by studying the
functional representation in the network’s neural tangent kernel RKHS space.
Recall from Section A that in the NTK regime, i.e., networks are infinitely wide, randomly initialized,
and trained by gradient descent with infinitesimally small learning rate, the learning dynamics of the
neural network is equivalent to that of a kernel regression with respect to its neural tangent kernel.
For any X ∈ Rd, the network output is given by
f (x) = (<φ(x),φ(xi)>,…,<φ(x),φ(xn)〉)∙ NTK-1nY,
where NTKtrain is the n × n kernel for training data, φ(X), φ(Xi) is the kernel value between test
data X and training data Xi , and Y is training labels. By Lemma 2, the kernel regression solution is
also equivalent to the min-norm solution in the NTK RKHS space that fits all training data
f(X) = φ(X)>βNTK,	(11)
where the representation coefficient βNTK is
mβin kβ k2
s.t. φ(Xi)>β = yi, for i = 1, ..., n.
17
Published as a conference paper at ICLR 2021
The feature map φ(x) for a two-layer MLP with ReLU activation is given by Lemma 3
φ (x) = Cz(X ∙ I (w(k)>x ≥ 0), w(k)Tx ∙ I (w(k)>x ≥ 0),…),	(12)
where Wlk)〜N(0,1), with k going to infinity. CZ is a constant, and I is the indicator function.
Without loss of generality, we assume the bias term to be 1. For simplicity of notations, we denote
each data X plus bias term by, i.e., X = [x|1] (Bietti & Mairal, 2019), and assume constant term is 1.
Given any direction v on the unit sphere, the network outputs for out-of-distribution data xo = tv
and X = xo + hv = (1 + λ)xo, where we introduce the notation of X and λ for convenience, are
given by Eqn. 11 and Eqn. 12
f(Xo) =β>TK(Xo ∙ I (w(k)TXo ≥ 0), w(k)TXo ∙I (w(k)TXo ≥ 0),..),
f (X) =β>TK (X ∙ I (w(k)TX ≥ 0), w(k)TX ∙ I (w(k)TX ≥ 0),...),
where we have x^o = [xo|1] and X = [(1 + λ)xo∣1]. It follows that
f (X) - f(Xo) = β>TK (x ∙ I (w(k)TX ≥ 0)- Xo ∙ I (w(k)TXo ≥ 0),	(13)
w(k)TX ∙ I (w(k)TX ≥ 0)- w(k)TXo ∙ I (w(k)TXo ≥ 0), ∙∙∙)	(14)
By re-arranging the terms, we get the following equivalent form of the entries:
X	∙	I (WTX ≥ 0)— Xo ∙ I (WT Xo ≥ 0)	(15)
=X	∙	(I (wτX ≥ 0)— I (wTXo ≥ 0)+1 (WTXO ≥ 0)) — Xo	∙ I (WTXO	≥ 0)	(16)
=X	∙	(I (wTX ≥ 0) — I (wTXo ≥ 0)) + (X — Xo) ∙ I (wTXo	≥ 0)	(17)
=[x∣1] ∙ (I (wTX ≥ 0) — I (WTxO ≥ 0)) + [hv∣0] ∙ I (WTxO ≥ 0)	(18)
Similarly, we have
wTX	∙	I (WTX ≥ 0) — WTxO ∙ I (WTxO ≥ 0)	(19)
=wt^	∙	(I (wt^ ≥ 0) — I (WTx^o ≥ 0) + I (WTX^o ≥ 0))	— WTx^o	∙ I (WTx^o ≥ 0)	(20)
=wτ^	∙	(I (wτ^ ≥ 0) — I (WTx^o ≥ 0)) + wτ (^ — x^o)	∙	I	(WTaO	≥ 0)	(21)
=wτ [x∣1] ∙ (I (wτ^ ≥ 0) — I (WTx^o ≥ 0)) + wτ[hv∣0]	∙	I	(WTx^o	≥ 0)	(22)
Again, let us denote the part of entk corresponding to each W by βw. Moreover, let us denote the
part corresponding to Eqn. 18 by βW and the part corresponding to Eqn. 22 by βW. Then we have
f(X)— f(Xo)
h
J β1T [x∕h∣1∕h] ∙ (I (WTX ≥ 0) — I (WTXO ≥ 0)) dP(w)
+
+
+
(23)
(24)
Jew
/ ew
/ β
[v∣0] ∙ I (WTXO ≥ 0) dP(w)
- wτ [x∕h∣1∕h] ∙ (I (wτX ≥ 0) — I (WTXO ≥ 0)) dP(w)
- wτ[v∣0] ∙ I (WTXO ≥ 0) dP(w)
(25)
(26)
(27)
Note that all βw are finite constants that depend on the training data. Next, we show that as t → ∞,
each of the terms above converges in O(1∕e) to some constant coefficient βv that depend on the
training data and the direction v. Let us first consider Eqn. 25. We have
JI (WTXO ≥ 0) dP(w) = /i (wτ[xo∣1] ≥ 0) dP(w)
=/l (wτ[xo∕t∣1∕t] ≥ 0) dP(w)
→ /l (wτ[v∣0] ≥ 0) dP(w)	as t → ∞
(28)
(29)
(30)
18
Published as a conference paper at ICLR 2021
Because βw1 are finite constants, it follows that
/ βw>[v∣0] ∙
I (w>x^o ≥ 0)dP(w) → / βW
[v|0] ∙ I (w> [v|0] ≥ 0)dP(w),
(31)
where the right hand side is a constant that depends on training data and direction v . Next, we show
the convergence rate for Eqn. 31. Given error > 0, because βw1> [v|0] are finite constants, we need
to bound the following by C ∙ E for some constant C,
|/l (w>x^o ≥ 0)— I (w>[v∣0] ≥ 0)dP(w)∣	(32)
=| / l (w>[xo∣1] ≥ 0)— I (w>[x0∣0] ≥ 0)dP(w)∣	(33)
Observe that the two terms in Eqn. 33 represent the volume of half-(balls) that are orthogonal to
vectors [x0|1] and [x0|0]. Hence, Eqn. 33 is the volume of the non-overlapping part of the two
(half)balls, which is created by rotating an angle θ along the last coordinate. By symmetry, Eqn. 33 is
linear in θ. Moreover, the angle θ = arctan(C/t) for some constant C. Hence, it follows that
| /i (w>[xo∣1] ≥ 0)— I (w>[xo∣0] ≥ 0)dP(w)∣ = Ci ∙ arctan(C2∕t)	(34)
≤ C1∙C2∕t	(35)
= O(1/t)	(36)
In the last inequality, we used the fact that arctan x < x for x > 0. Hence, O(1∕t) < E implies
t = O(1∕E) as desired. Next, we consider Eqn. 24.
J βW> [x∕h∣1∕h] ∙ (I (w>X ≥ 0)— I (w>x^o ≥ 0)) dP(w)
Let us first analyze the convergence of the following:
| JI (w>^ ≥ 0)— I (w>x^o ≥ 0)dP(w)∣
=|/i (w>[(1 + λ)xo∣1] ≥ 0)— I (w>[xo∣1] ≥ 0)dP(w)dP(w)∣
=| /i (w>[xo∣ɪɪʌ-] ≥ 0)— I (w>[xo∣1] ≥ 0)dP(w)dP(w)∣ → 0
(37)
(38)
(39)
(40)
The convergence to 0 follows from Eqn. 34. Now we consider the convergence rate. The angle θ is at
most 1 — ι+λ times of that in Eqn. 34. Hence, the rate is as follows
1 - τ⅛ )∙ O
τ+λ ∙ O
$ ∙ O
O(^-
(h + t)t
(41)
Now we get back to Eqn. 24, which simplifies as the following.
βw1>
tv 1
V + T | h
• (I (w>X ≥ 0)— I (w>x^o ≥ 0)) dP(w)
(42)
We compare the rate of growth of left hand side and the rate of decrease of right hand side (indicators).
t- ∙ /7 h、= --------------> 0 as t → ∞	(43)
h (h + t)t h + t
丁 • ~7Z-----7-	= ~7Z---T—	-→	0 as t -→	∞	(44)
h (h	+ t)t (h + t)t
Hence, the indicators decrease faster, and it follows that Eqn. 24 converges to 0 with rate O(ɪ).
Moreover, we can bound w with standard concentration techniques. Then the proofs for Eqn. 26 and
Eqn. 27 follow similarly. This completes the proof.
19
Published as a conference paper at ICLR 2021
B.2 Proof of Lemma 1
Overview of proof. To prove exact extrapolation given the conditions on training data, we analyze
the function learned by the neural network in a functional form. The network’s learned function can
be precisely characterized by a solution in the network’s neural tangent kernel feature space which
has a minimum RKHS norm among functions that can fit all training data, i.e., it corresponds to the
optimum of a constrained optimization problem. We show that the global optimum of this constrained
optimization problem, given the conditions on training data, is precisely the same function as the
underlying true function.
Setup and preparation. Let X = {x1, ..., xn} and Y = {y1, ..., yn} denote the training set input
features and their labels. Let βg ∈ Rd denote the true parameters/weights for the underlying linear
function g, i.e.,
g(x) = βg>x	for all x ∈ Rd
Recall from Section A that in the NTK regime, where networks are infinitely wide, randomly
initialized, and trained by gradient descent with infinitesimally small learning rate, the learning
dynamics of a neural network is equivalent to that of a kernel regression with respect to its neural
tangent kernel. Moreover, Lemma 2 tells us that this kernel regression solution can be expressed in
the functional form in the neural tangent kernel’s feature space. That is, the function learned by the
neural network (in the ntk regime) can be precisely characterized as
f(x) = φ(x)>βN
TK,
where the representation coefficient βNTK is
min kβ k2	(45)
s.t. φ(xi)>β = yi, for i = 1, ..., n.	(46)
An infinite-dimensional feature map φ(x) for a two-layer ReLU network is described in Lemma 3
φ (x) = c0 (x ∙ I (w(k)>X ≥ 0), w(k)>X ∙ I (w(k)>X ≥ 0),…),
where w(k) 〜N(0, I), with k going to infinity. c0 is a constant, and I is the indicator function. That
is, there are infinitely many directions w with Gaussian density, and each direction comes with two
features. Without loss of generality, we can assume the scaling constant to be 1.
Constrained optimization in NTK feature space. The representation or weight of the neural
network’s learned function in the neural tangent kernel feature space, βNTK, consists of weight
vectors for each X ∙ I (w(k)> X ≥ 0)∈ Rd and w(k)> X ∙ I (w(k)>X ≥ 0)∈ R. For simplicity
of notation, we will use w to refer to a particular w, without considering the index (k), which
does not matter for our purposes. For any W ∈ Rd, We denote by βw = (β((1),…,β(d)) ∈ Rd
the weight vectors corresponding to X ∙ I (w>X ≥ 0), and denote by βW ∈ Rd the weight for
w>X ∙ I (w>X ≥ 0).
Observe that for any W 〜N(0, I) ∈ Rd, any other vectors in the same direction will activate the
same set of Xi ∈ Rd. That is, if w>Xi ≥ 0 for any W ∈ Rd, then (k ∙ w)>Xi ≥ 0 for any k > 0.
Hence, we can reload our notation to combine the effect of weights for W’s in the same direction.
This enables simpler notations and allows us to change the distribution of W in NTK features from
Gaussian distribution to uniform distribution on the unit sphere.
More precisely, we reload our notation by using βw and βw0 to denote the combined effect of all
weights (βkW,…，βkW) ∈ Rd and βkW ∈ R for all kw with k > 0 in the same direction of w. That
is, for each W 〜Uni(unit sphere) ∈ Rd, we define βWj as the total effect of weights in the same
direction
βW) = / βj)I (kWw>Uuk = 1)dP(u), for j = [d]	(47)
20
Published as a conference paper at ICLR 2021
where U 〜N(0, I). Note that to ensure the βw is a well-defined number, here We can work with
the polar representation and integrate with respect to an angle. Then βw is well-defined. But for
simplicity of exposition, we use the plain notation of integral. Similarly, we define βw0 as reloading
the notation of
βw= Z βuI (τw>u-1 = 1)∙ kuk dP(u)	(48)
J "wk∙ Iluk	k Ilwk
Here, in Eqn. 48 we have an extra term of ∣W∣ compared to Eqn. 47 because the NTK features that
Eqn. 48 corresponds to, w>x ∙ I (wτx ≥ 0), has an extra w> term. So we need to take into account
the scaling. This abstraction enables us to make claims on the high-level parameters βw and βw0 only,
which we will show to be sufficient to determine the learned function.
Then we can formulate the constrained optimization problem whose solution gives a functional form
of the neural network’s learned function. We rewrite the min-norm solution in Eqn. 45 as
s.t.
mβin	βw(1)2 + βw(2)2 + ... + βw(d)2 + (βw0 )2 dP(w)
/
w>xi ≥0
βWXi + βw ∙ wτXi dP(w) = β>Xi
∀i ∈ [n],
(49)
(50)
where the density of w is now uniform on the unit sphere of Rd . Observe that since w is from a
uniform distribution, the probability density function P(w) is a constant. This means every Xi is
activated by half of the w on the unit sphere, which implies we can now write the right hand side of
Eqn. 50 in the form of left hand side, i.e., integral form. This allows us to further simplify Eqn. 50 as
/	(βW + βw ∙ wτ - 2 ∙ β>) Xi dP(w) = 0
w>xi≥0
∀i ∈ [n],
(51)
where Eqn. 51 follows from the following steps of simplification
Z	β(1)x(1)	+	..第)x(d)	+ β'w	∙	WTXidP(W) = βg1)x(1) + …βgdxS	∀i	∈	[n],
w>xi≥0
v⇒ /	βWM1) + ... + βWd)x(d) + βw ∙ wτXi dP(w)
w>xi≥0
= R	IdP(W)	•	/	dP(W)	•(同I)χ(1)	+ …+ 噌*	∀i	∈	[n],
w>xi ≥0	w>xi ≥0
^⇒	/	ewɔX(I) + ... + βWd)x(d) + βw ∙ WTXidP(W)
w>xi≥0
=2 •	/	βg1)Xi1) + ... + βgd)Xid)dP(w) ∀i ∈ [n],
w>xi≥0
^⇒	/	(βW + βw ∙ wτ — 2 ∙ β>) Xi dP(w) = 0 ∀i ∈ [n].
w>xi≥0
Claim 1. Without loss of generality, assume the scaling factor c in NTK feature map φ(X) is 1. Then
the global optimum to the constraint optimization problem Eqn. 49 subject to Eqn. 51, i.e.,
s.t.
mβin	βw(1)2 + βw(2)2 + ... + βw(d)2 + (βw' )2 dP(w)
/	(βW + βw ∙ wτ — 2 ∙ β) Xi dP(w) = 0
w>xi≥0
∀i ∈ [n].
(52)
(53)
satisfies βw + βW ∙ W = 2βg for all W.
21
Published as a conference paper at ICLR 2021
This claim implies the exact extrapolation we want to prove, i.e., fNTK(x) = g(x). This is because,
if our claim holds, then for any x ∈ Rd
fNTκ(x) = f	β>x + βW ∙ w>x dP(w)
w>x≥0
=I 2 ∙ β>X dP(w)
w>x≥0
=/	dP(w) ∙ 2β>x
w>x≥0
=2 ∙ 2β>x = g(x)
Thus, it remains to prove Claim 1. To compute the optimum to the constrained optimization problem
Eqn. 52, we consider the Lagrange multipliers. It is clear that the objective Eqn. 52 is convex.
Moreover, the constraint Eqn. 53 is affine. Hence, by KKT, solution that satisfies the Lagrange
condition will be the global optimum. We compute the Lagrange multiplier as
L(β, λ) =	βw(1)2 + βw(2)2 +... + βw(d)2 + (βw0 )2 dP(w)
n
-X λi ∙
i=1
Z	(βW + βw ∙ w> - 2 ∙ β>) Xi dP(w)
w>xi≥0
(54)
(55)
Setting the partial derivative of L(β, λ) with respect to each variable to zero gives
∂L
∂βwk)
n
2β(k)P(w) + X λi ∙ Xy) ∙ I (w>Xi ≥ 0) = 0
i=1
∂ L
βw
∂L
∂λi
n
2βWP(W) + X λi ∙ w>Xi ∙ I (w>Xi ≥ 0) = 0
i=1
/	(βW + βw ∙ w> - 2 ∙ β>) Xi dP(w) = 0
w>xi≥0
(56)
(57)
(58)
It is clear that the solution in Claim 1 immediately satisfies Eqn. 58. Hence, it remains to show there
exist a set of λi for i ∈ [n] that satisfies Eqn. 56 and Eqn. 57. We can simplify Eqn. 56 as
n
βWk = C ∙ X λi ∙ x(k) ∙ I (w>Xi ≥ 0),	(59)
i=1
where c is a constant. Similarly, we can simplify Eqn. 57 as
n
βw = C ∙ X λi ∙ w>Xi ∙I (wτXi ≥ 0)	(60)
i=1
Observe that combining Eqn. 59 and Eqn. 60 implies that the constraint Eqn. 60 can be further
simplified as
βw0 = wτβw	(61)
It remains to show that given the condition on training data, there exists a set of λi so that Eqn. 59
and Eqn. 61 are satisfied.
Global optimum via the geometry of training data. Recall that we assume our training data
{(Xi, yi)}in=1 satisfies for any w ∈ Rd, there exist d linearly independent {Xiw}id=1 ⊂ X, where
X = {Xi}in=1, so that wτXiw ≥ 0 and -Xiw ∈ X for i = 1..d, e.g., an orthogonal basis of Rd and
their opposite vectors. We will show that under this data regime, we have
22
Published as a conference paper at ICLR 2021
(a)	for any particular w, there indeed exist a set of λi that can satisfy the constraints Eqn. 59 and
Eqn. 61 for this particular w.
(b)	For any w1 and w2 that activate the exact same set of {xi }, the same set of λi can satisfy the
constraints Eqn. 59 and Eqn. 61 of both w1 and w2 .
(c)	Whenever we rotate a w1 to a w2 so that the set of xi being activated changed, we can still find
λi that satisfy constraint of both w1 and w2 .
Combining (a), (b) and (c) implies there exists a set of λ that satisfy the constraints for all w. Hence,
it remains to show these three claims.
We first prove Claim (a). For each w, we must find a set of λi so that the following hold.
n
βWk) = C ∙ X λi∙ x(k) ∙ I (WTxi ≥ 0),
i=1
βw0 = w>βw
βw + βw ∙ W = 2βg
Here, βg and w are fixed, and w is a vector on the unit sphere. It is easy to see that βw is then
determined by βg and W, and there indeed exists a solution (solving a consistent linear system).
Hence we are left with a linear system with d linear equations
n
β((k) = C ∙ X λi∙ x(k) ∙ I (w>xi ≥ 0) ∀k ∈ [d]
i=1
to solve with free variables being λi so that W activates xi, i.e., W>xi ≥ 0. Because the training
data {(xi, yi)}in=1 satisfies for any W, there exist at least d linearly independent xi that activate W.
This guarantees for any W we must have at least d free variables. It follows that there must exist
solutions λi to the linear system. This proves Claim (a).
Next, we show that (b) for any W1 and W2 that activate the exact same set of {xi}, the same set of λi
can satisfy the constraints Eqn. 59 and Eqn. 61 of both W1 and W2 . Because W1 and W2 are activated
by the same set of xi , this implies
nn
βwι = C ∙〉: λi ∙ xi ∙ I (w1 xi ≥ 0) = C ∙): λi ∙ xi ∙ I (w2 xi ≥ 0) = ∣βw2
i=1	i=1
Since λi already satisfy constraint Eqn. 59 for W1, they also satisfy that for W2. Thus, it remains
to show that 0皿 + βW1∙ wι = β3 + βW2 ∙ wι assuming 0皿=β3, βW1=w>βwι, and
βw0 2 = W2>βw2. This indeed holds because
	βw1 + βw0 1	• Wl = βw2 + βW2 ∙ W2
^⇒	βw 1 ∙ w> =	β'w2 • w>
^⇒	W1>βw1W1>	= W2 βw2 W2
^⇒	W1>W1βw>1	= W2>W2βw>2
^⇒	1 ∙ βWι =1	• βw>2
^⇒	βw1 = βw1	
Here, we used the fact that W1 and W2 are vectors on the unit sphere. This proves Claim (b).
Finally, we show (c) that Whenever we rotate a W1 to a W2 so that the set of xi being activated
changed, we can still find λi that satisfy constraint of both W1 and W2 . Suppose we rotate W1 to
W2 so that W2 lost activation with x1, x2, ..., xp which in the set of linearly independent xi’s being
activated by W1 and their opposite vectors -xi are also in the training set (without loss of generality).
Then W2 must now also get activated by -x1, -x2, ..., -xp. This is because if W2>xi < 0, we must
have W2> (-xi) > 0.
Recall that in the proof of Claim (a), we only needed the λi from linearly independent xi that we
used to solve the linear systems, and their opposite as the free variables to solve the linear system of
23
Published as a conference paper at ICLR 2021
d equations. Hence, we can set λ to 0 for the other xi while still satisfying the linear system. Then,
suppose there exists λi that satisfy
d
βWι) = C ∙ X λi∙ x(k)
i=1
where the xi are the linearly independent vectors that activate w1 with opposite vectors in the training
set, which we have proved in (a). Then we can satisfy the constraint for βw2 below
pd
βW2 = c ∙ X ^i∙(-g产+ X M x(k)
i=1	i=p+1
by setting λi = -λi for i = 1 …p. Indeed, this gives
pd
βW2 = C ∙X(-λi) ∙(-Xi产+ X λi∙ x(k
i=1	i=p+1
d
=C ∙ X λi∙ x(k)
i=1
Thus, we can also find λi that satisfy the constraint for βw2. Here, we do not consider the case where
w2 is parallel with an xi because such w2 has measure zero. Note that we can apply this argument
iteratively because the flipping the sign always works and will not create any inconsistency.
Moreover, we can show that the constraint for βw0 2 is satisfied by a similar argument as in proof of
Claim (b). This follows from the fact that our construction makes βw1 = βw2. Then we can follow
the same argument as in (b) to show that Bw、+ βWI ∙ wι = βw2 + βW2 ∙ wι. This completes the
proof of Claim (c).
In summary, combining Claim (a), (b) and (c) gives that Claim 1 holds. That is, given our training
data, the global optimum to the constrained optimization problem of finding the min-norm solution
among functions that fit the training data satisfies βw + βW ∙ W = 2βg. We also showed that this claim
implies exact extrapolation, i.e., the network’s learned function f (x) is equal to the true underlying
function g(x) for all x∈ Rd. This completes the proof.
B.3 Proof of Theorem 2
Proof of the asymptotic convergence to extrapolation builds upon our proof of exact extrapolation,
i.e., Lemma 1. The proof idea is that if the training data distribution has support at all directions,
when the number of samples n → ∞, asymptotically the training set will converge to some imaginary
training set that satisfies the condition for exact extrapolation. Since if training data are close the
neural tangent kernels are also close, the predictions or learned function will converge to a function
that achieves perfect extrapolation, that is, the true underlying function.
Asymptotic convergence of data sets. We first show the training data converge to a data set that
satisfies the exact extrapolation condition in Lemma 1. Suppose training data {xi }in=1 are sampled
from a distribution whose support contains a connected set S that intersects all directions, i.e., for
any non-zero w ∈ Rd, there exists k > 0 so that kw ∈ S .
Let us denote by S the set of datasets that satisfy the condition in Lemma 1. In fact, we will use a
relaxed condition in the proof of Lemma 1 (Lemma 1 in the main text uses a stricter condition for
simplicity of exposition). Given a general dataset X and a dataset S ∈ S of the same size n, let
σ(X, S) denote a matching of their data points, i.e., σ outputs a sequence of pairs
σ(X, S)i = (xi, si) for i ∈ [n]
s.t. X = {xi}in=1
S= {si}in=1
Let ` : Rd × Rd → R be the l2 distance that takes in a pair of points. We then define the distance
between the datasets d(X, S) as the minimum sum of l2 distances of their data points over all
24
Published as a conference paper at ICLR 2021
possible matching.
min P ` (σ (X , S) )
d(X, S) =	mσin i=1 (σ (X, )i)
I ∞
|X| = |S| = n
|X| 6= |S|
We can then define a “closest distance to perfect dataset” function D* : X → R which maps a dataset
X to the minimum distance of X to any dataset in S
D* (X) = min d (X, S)
It is easy to see that for any dataset X = {xi}in=1, D* (X) can be bounded by the minimum of the
closest distance to perfect dataset D* of sub-datasets of X of size 2d.
bn/2dc
D* ({xi}in=1) ≤ mk=in1
D* {xj }jk=*2(dk-1)*2d+1	(62)
This is because for any S ∈ S , and any S ⊆ S0 , we must have S0 ∈ S because a dataset satisfies
exact extrapolation condition as long as it contains some key points. Thus, adding more data will not
hurt, i.e., for any X1 ⊆ X2, we always have
D* (X1) ≤ D* (X2)
Now let us denote by Xn a random dataset of size n where each xi ∈ Xn is sampled from the
training distribution. Recall that our training data {xi}in=1 are sampled from a distribution whose
support contains a connected set S* that intersects all directions, i.e., for any non-zero w ∈ Rd, there
exists k > 0 so that kw ∈ S*. It follows that for a random dataset X2d of size 2d, the probability
that D* (X2d) > happens is less than 1 for any > 0.
First there must exist S0 = {si}i2=d1 ∈ S of size 2d, e.g., orthogonal basis and their opposite vectors.
Observe that if we scale any si by k > 0, the resulting dataset is still in S by the definition of S. We
denote the set of datasets where we are allowed to scale elements of S0 by S0 . It follows that
P (D*(X2d) >) =P min d (X2d, S) >
≤ P min d (X2d, S) >
1 - P min min X ` (σ (X2d, S)i) ≤
≤1-P
min min mnax ` (σ (X2d, S)i) ≤
σ i=1	i
S∈S0
≤δ<1
where we denote the bound of P (D* (X2d) > ) by δ < 1, and the last step follows from
P min min mnax ` (σ (X2d, S)i) ≤	> 0
S∈S0 σ i=1
which further follows from the fact that for any si ∈ S0 , by the assumption on training distribution,
we can always find k > 0 so that ksi ∈ S*, a connected set in the support of training distribution. By
the connectivity of support S*, ksi cannot be an isolated point in S*, so for any > 0, we must have
/
∣∣x-ksik≤e,x∈S*
fX (x)dx > 0
25
Published as a conference paper at ICLR 2021
Hence, We can now apply Eqn. 62 to bound D* (Xn). Given any e > 0, we have
P(D*(Xn) >e) = 1 - P(D*(Xn) ≤ E)
bn/2dc
≤ ι - p (m=in d* (>j }k=2k-i)*2d+j ≤
/	bn∕2dC	∖
≤ 1 - I1 - Y P (d* ({Xj j2(k-i)*2d+ι) > e) J
bn∕2dc
= Y P D* {xj}jk=*2(dk-1)*2d+1 >E
k=1
≤ δbn∕2dc
Here δ < 1. This implies D* (Xn ) -p→ 0, i.e.,
lim P (D*(Xn) > E) = 0 ∀E > 0	(63)
n→∞
Eqn. 63 says as the number of training samples n → ∞, our training set will converge in probability
to a dataset that satisfies the requirement for exact extrapolation.
Asymptotic convergence of predictions. Let NTK(x, x0 ) : Rd × Rd → R denote the neural
tangent kernel for a two-layer ReLU MLP. It is easy to see that if X → x*, then NTK(x, ∙) →
NTK(x*, ∙) (Arora et al.(2019b)). Let NTKtrain denote the n X n kernel matrix for training data.
We have shown that our training set converges to a perfect data set that satisfies conditions of exact
extrapolation. Moreover, note that our training set will only have a finite number of (not increase with
n) xi that are not precisely the same as those in a perfect dataset. This is because a perfect data only
contains a finite number of key points and the other points can be replaced by any other points while
still being a perfect data set. Thus, we have NTKtrain → N*, where N* is the n × n NTK matrix for
some perfect data set.
Because neural tangent kernel is positive definite, we have NTKt-ra1in → N*-1 . Recall that for any
x ∈ Rd, the prediction of NTK is
fNTK(x) = (NTK(X, Xi),…,NTK(x, Xn)) ∙NTK-1nΥ,
where NTKtrain is the n × n kernel for training data, NTK(x, xi ) is the kernel value between test
data X and training data Xi , and Y is training labels.
Similarly, we have (NTK(X, X1), ..., NTK(X, Xn)) → (NTK(X, X*1), ..., NTK(X, X*n)), where xi* is
a perfect data set that our training set converges to. Combining this with NTKt-ra1in → N *-1 gives
fNTK -p→ fN*TK = g,
where fNTK is the function learned using our training set, and fN*TK is that learned using a perfect data
set, which is equal to the true underlying function g. This completes the proof.
B.4	Proof of Corollary 1
In order for GNN with linear aggregations
h(uk) = X MLP(k)h(uk),h(vk),X(u,v),
v∈N (u)
hG =MLP(K+1)Xh(uK),
u∈G
to extrapolate in the maximum degree task, it must be able to simulate the underlying function
hG = max 1
u∈G
v∈N (u)
26
Published as a conference paper at ICLR 2021
Because the max function cannot be decomposed as the composition of piece-wise linear functions,
the MLP(K+1) module in GNN must learn a function that is not piece-wise linear over domains
outside the training data range. Since Theorem 1 proves for two-layer overparameterized MLPs,
here we also assume MLP(K+1) is a two-layer overparameterized MLP, although the result can be
extended to more layers. It then follows from Theorem 1 that for any input and label (and thus
gradient), MLP(K+1) will converge to linear functions along directions from the origin. Hence, there
are always domains where the GNN cannot learn a correct target function.
B.5	Proof of Theorem 3
Our proof applies the similar proof techniques for Lemma 1 and 2 to Graph Neural Networks (GNNs).
This is essentially an analysis of Graph Neural Tangent Kernel (GNTK), i.e., neural tangent kernel of
GNNs.
We first define the simple GNN architecture we will be analyzing, and then present the GNTK for
this architecture. Suppose G = (V, E) is an input graph without edge feature, and xu ∈ Rd is the
node feature of any node u ∈ V . Let us consider the simple one-layer GNN whose input is G and
output is hG
hG
W(2)
max
u∈G
W(1)xv
v∈N (u)
(64)
Note that our analysis can be extended to other variants of GNNs, e.g., with non-empty edge features,
ReLU activation, different neighbor aggregation and graph-level pooling architectures. We analyze
this GNN for simplicity of exposition.
Next, let us calculate the feature map of the neural tangent kernel for this GNN. Recall from Section A
that consider a graph neural network f (θ, G) : G → R where θ ∈ Rm is the parameters in the
network and G ∈ G is the input graph. Then the neural tangent kernel is
H _/ ∂f(θ,Gi) ∂f (θ,Gj) ∖
Hj = ∖	∂θ	,	∂θ ∕,
where θ are the infinite-dimensional parameters. Hence, the gradients with respect to all parameters
give a natural feature map. Let us denote, for any node u, the degree of u by
hu =	xv
(65)
v∈N (u)
It then follows from simple computation of derivative that the following is a feature map of the GNTK
for Eqn. 64
(66)
where w(k) 〜N(0, I), with k going to infinity. C is a constant, and I is the indicator function.
Next, given training data {(Gi, yi}in=1, let us analyze the function learned by GNN through the
min-norm solution in the GNTK feature space. The same proof technique is also used in Lemma 1
and 2.
Recall the assumption that all graphs have uniform node feature, i.e., the learning task only considers
graph structure, but not node feature. We assume xv = 1 without loss of generality. Observe that in
this case, there are two directions, positive or negative, for one-dimensional Gaussian distribution.
Hence, we can simplify our analysis by combining the effect of linear coefficients for w in the same
direction as in Lemma 1 and 2.
Similarly, for any w, let us define βw ∈ R as the linear coefficient corresponding to
P I (U = arg max w>hv ) ∙ h〃 in RKHS space, and denote by βW ∈ R the weight for
u∈G	v∈G
max (w>hu). Similarly, we can combine the effect of all β in the same direction as in Lemma 1
u∈G
27
Published as a conference paper at ICLR 2021
and 2. We define the combined effect with βw and βw0 . This allows us to reason about w with two
directions, + and -.
Recall that the underlying reasoning function, maximum degree, is
g(G) = max hu.
u∈G u
We formulate the constrained optimization problem, i.e., min-norm solution in GNTK feature space
that fits all training data, as
min β fβW + 3WdP(W)
β,β J
s.t. / ɪ2 I (U = arg max W ∙ hv ) ∙ βw ∙ hu + max (w ∙ hu) ∙ βWdP(w) = max hu
u∈Gi	v∈G	u∈Gi	u∈Gi
∀i ∈ [n],
where Gi is the i-th training graph and W 〜N(0,1). By combining the effect of β, and taking the
derivative of the Lagrange for the constrained optimization problem and setting to zero, we get the
global optimum solution satisfy the following constraints.
n
β+ = C ∙ ɪ2 λi ∙ ɪ2 hu ∙ I (U = arg max hv
i=1	u∈Gi	v∈Gi
n
β- = c ∙ y λi ∙ y	hu ∙ I (U = arg min hv
v∈Gi
i=1	u∈Gi	i
n
β+ = c ∙ > λi ∙ max hu
+	i u∈Gi u
i=1
n
3!_ = c ∙	λi ∙ min hu
-	i u∈Gi u
i=1
max hu = β+ ∙ I ( u = arg max hv I ∙ hu + β+ ∙ max hu
u∈Gi u +	v∈Gi v u + u∈Gi u
u∈Gi
+ β- ∙ y I ( u = arg min hv I ∙ hu + βL ∙ min hu
v∈Gi	u∈Gi
u∈Gi
∀i ∈ [n]
(67)
(68)
(69)
(70)
(71)
(72)
where c is some constant, λi are the Lagrange parameters. Note that here we used the fact that there
are two directions +1 and -1. This enables the simplification of Lagrange derivative. For a similar
step-by-step derivation of Lagrange, refer to the proof of Lemma 1.
Let us consider the solution β+0 = 1 and β+ = β- = β-0 = 0. It is clear that this solution can fit
the training data, and thus satisfies Eqn. 71. Moreover, this solution is equivalent to the underlying
reasoning function, maximum degree, g(G) = maxu∈G hu.
Hence, it remains to show that, given our training data, there exist λi so that the remaining four
constraints are satisfies for this solution. Let us rewrite these constraints as a linear systems where
the variables are λi
n
c £% ∙
i=1
/ P
u∈Gi
Σu∈Gi
∖
hu ∙ I u u = arg max hv ∣
∖	v∈Gi	)
hu ∙ I I u = arg min hv ∣
∖	v∈Gi J
max hu
u∈Gi
min hu
u∈Gi
(73)
28
Published as a conference paper at ICLR 2021
By standard theory of linear systems, there exist λi to solve Eqn. 73 if there are at least four training
data Gi whose following vectors linear independent
P
u∈Gi
Σu∈Gi
∖
hu ∙ I U u = arg max hv ∣
v∈	v∈Gi	'
hu ∙ I I u = arg min h ∣
V	v∈Gi	)
max hu
u∈Gi
min hu
u∈Gi u
/max hu ∙ Nmax∖
u∈Gi
min hu∙ Nimin
u∈Gi
max hu
u∈Gi
min hu
u∈Gi
(74)
Here, Nimax denotes the number of nodes that achieve the maximum degree in the graph Gi, and
Nimin denotes the number of nodes that achieve the min degree in the graph Gi. By the assumption
of our training data that there are at least four Gi 〜G with linearly independent Eqn. 74. Hence, our
simple GNN learns the underlying function as desired.
This completes the proof.
B.6 Proof of Lemma 2
Let W denote the span of the feature maps of training data xi, i.e.
W = span(φ(x1) ,φ(x2) , ..., φ (xn)) .
Then we can decompose the coordinates of fNTK in the RKHS space, βNTK, into a vector β0 for the
component of fNTK in the span of training data features W, and a vector β1 for the component in the
orthogonal complement W>, i.e.,
βNTK = β0 + β1 .
First, note that since fNTK must be able to fit the training data (NTK is a universal kernel as we will
discuss next), i.e.,
φ(xi)>βNTK = yi.
Thus, we have φ(xi)>β0 = yi. Then, β0 is uniquely determined by the kernel regression solution
with respect to the neural tangent kernel
fNTK(X)= (<φ(x), φ(xi)>,…，<φ(x), φ(xn)>) ∙ NTK-a1nY,
where NTKtrain is the n × n kernel for training data, φ(x), φ(xi) is the kernel between test data x
and training data xi, and Y is training labels.
The kernel regression solution fNTK is uniquely determined because the neural tangent kernel NTKtrain
is positive definite assuming no two training data are parallel, which can be enforced with a bias
term (Du et al., 2019c). In any case, the solution is a min-norm by pseudo-inverse.
Moreover, a unique kernel regression solution fNTK that spans the training data features corresponds
to a unique representation in the RKHS space β0 .
Since β0 and β1 are orthogonal, we also have the following
kβNTKk22 = kβ0 + β1k22 = kβ0k22 + kβ1k22.
This implies the norm of βNTK is at least as large as the norm of any β such that φ(xi)>βNTK = yi.
Moreover, observe that the solution to kernel regression Eqn. 9 is in the feature span of training data,
given the kernel matrix for training data is full rank.
fNTK(X)= (<Φ(x), Φ(xi)>,…，<Φ(x), Φ(Xn)》) ∙ NTK-a1nY.
Since β1 is for the component of fNTK in the orthogonal complement of training data feature span,
we must have β1 = 0. It follows that βNTK is equivalent to
mβin kβk2
s.t. φ(xi)>β = yi, for i = 1, ..., n.
as desired.
29
Published as a conference paper at ICLR 2021
B.7 Proof of Lemma 3
We first compute the neural tangent kernel NTK(x, x0) for a two-layer multi-layer perceptron (MLP)
with ReLU activation function, and then show that it can be induced by the feature space φ(x)
specified in the lemma so that NTK(x, x0) = φ(x), φ(x0) .
Recall that Jacot et al. (2018) have derived the general framework for computing the neural tangent
kernel of a neural network with general architecture and activation function. This framework is
also described in Arora et al. (2019b); Du et al. (2019b), which, in addition, compute the exact
kernel formula for convolutional networks and Graph Neural Networks, respectively. Following the
framework in Jacot et al. (2018) and substituting the general activation function σ with ReLU gives
the kernel formula for a two-layer MLP with ReLU activation. This has also been described in several
previous works (Du et al., 2019c; Chizat et al., 2019; Bietti & Mairal, 2019).
Below we describe the general framework in Jacot et al. (2018) and Arora et al. (2019b). Let σ
denote the activation function. The neural tangent kernel for an h-layer multi-layer perceptron can be
recursively defined via a dynamic programming process. Here, Σ(i) : Rd × Rd → R for i = 0...h is
the covariance for the i-th layer.
Σ(0) (x, x0) = x>x0,
∧(i) (x x0) =	Σ(i-1) (x, x)	Σ(i-1) (x, x0)
∧	(x,x)=	Σ(i-1) (x0, x) Σ(i-1)(x0,x0)	,
Σ(i)(x, x0) = C ∙ E	[σ(u)σ(v)].
u,v〜N(0,∧(i))
The derivative covariance is defined similarly:
Σ (i)(x, χ0) = c ∙ E	[σ(u)σ(v)].
u,v〜N(0,∧(i))
Then the neural tangent kernel for an h-layer network is defined as
hh
NTK(h-1)(x, x0) = X Σ(i-1)(x, x0) ∙ Y Σ(k)(x, x0),
i=1	k=i
where we let Σ(h) (x, x0) = 1 for the convenience of notations.
We compute the explict NTK formula for a two-layer MLP with ReLU activation function by
following this framework and substituting the general activation function with ReLU, i.e. σ(a) =
max(0, a) = a ∙ I(a ≥ 0) and σ(a) = I(a ≥ 0).
NTK(I) (x, x0)= X 卜(I)(x, x0) ∙ Y Σ (k)(x, x0)J
i=1	k=i
=Σ(0)(x, x0) ∙ Σ(I)(x, x0) + Σ(1)(x, x0)
So We can get the NTK via Σ(I) (x, x0) and Σ(I) (x, x0), Σ(O) (x, x0). Precisely,
Σ(0) (x, x0) = x>x0,
∧(1) (x, x0)
x>x
x0> x
Xo	∙( x	χ0),
Σ(I) (x, x0) = c ∙ E
u,v〜N(0,∧(1))
[u ∙ I(u ≥ 0) ∙ V ∙ I(v ≥ 0)].
To sample from N (0, ∧(1)), we let L be a decomposition of ∧(1), such that ∧(1) = LL>. Here,
we can see that L = (x, x0)>. Thus, sampling from N(0, ∧(1)) is equivalent to first sampling
W 〜N(0, I), and output
Lw = w> (x, x0).
Then we have the equivalent sampling (u, v) = (w>x, w>x0). It follows that
Σ(1)(x, x0) = c ∙ E	[w>x ∙ I (w>x ≥ 0)∙ w>x0 ∙ I (w>x0 ≥ 0)]
W 〜N (0,i)
30
Published as a conference paper at ICLR 2021
It follows from the same reasoning that
Σ(1)(x, x0) = C ∙ E	[I (w>x ≥ 0)∙ I (w>x0 ≥ 0)].
W 〜N (0,I)
The neural tangent kernel for a two-layer MLP with ReLU activation is then
NTK(I) 2 3 4(x, x0) = Σ(0)(x, x0) ∙ Σ⑴(x, x0) + Σ(1)(x, x0)
=c ∙	E	[x>x0	∙	I (w>x	≥ 0)∙ I (w>x0	≥ 0)]
W 〜N (0,i )
+ c ∙	E	[w>X	∙	I (w>x	≥ 0)	∙ w>x0 ∙ I	(w>x0	≥ 0)].
W 〜N (0,I)
Next, we use the kernel formula to compute a feature map for a two-layer MLP with ReLU activation
function. Recall that by definition a valid feature map must satisfy the following condition
NTK(1)(x, x0) = φ(x), φ(x0)
It is easy to see that the way we represent our NTK formula makes it easy to find such a decomposition.
The following infinite-dimensional feature map would satisfy the requirement because the inner
product of φ(x) and φ(x0) for any x, x0 would be equivalent to the expected value in NTK, after we
integrate with respect to the density function of w.
φ (x) = c0 (x ∙ I (w(k)>x ≥ 0), w(k)>x ∙ I (w(k)>x ≥ 0),…),
where WIk)〜N(0, I), with k going to infinity. c0 is a constant, and I is the indicator function. Note
that here the density of features of φ(x) is determined by the density of w, i.e. Gaussian.
C	Experimental Details
In this section, we describe the model, data and training details for reproducing our experiments. Our
experiments support all of our theoretical claims and insights.
Overview. We classify our experiments into the following major categories, each of which includes
several ablation studies:
1) Learning tasks where the target functions are simple nonlinear functions in various dimen-
sions and training/test distributions: quadratic, cosine, square root, and l1 norm functions,
with MLPs with a wide range of hyper-parameters.
This validates our implications on MLPs generally cannot extrapolate in tasks with nonlinear
target functions, unless the nonlinear function is directionally linear out-of-distribution. In
the latter case, the extrapolation error is more sensitive to the hyper-parameters.
2) Computation of the R-Squared of MLP’s learned functions along (thousands of) randomly
sampled directions in out-of-distribution domain.
This validates Theorem 1 and shows the convergence rate is very high in practice, and often
happens immediately out of training range.
3) Learning tasks where the target functions are linear functions with MLPs. These validate
Theorem 2 and Lemma 1, i.e., MLPs can extrapolate if the underlying function is linear
under conditions on training distribution. This section includes four ablation studies:
a) Training distribution satisfy the conditions in Theorem 2 and cover all directions, and
hence, MLPs extrapolate.
b) Training data distribution is restricted in some directions, e.g., restricted to be posi-
tive/negative/constant in some feature dimensions. This shows when training distribu-
tion is restrictive in directions, MLPs may fail to extrapolate.
c) Exact extrapolation with infinitely-wide neural networks, i.e., exact computation
with neural tangent kernel (NTK) on the data regime in Lemma 1. This is mainly
for theoretical understanding.
4) MLPs with cosine, quadratic, and tanh activation functions.
31
Published as a conference paper at ICLR 2021
5)	Learning maximum degree of graphs with Graph Neural Networks. Extrapolation on
graph structure, number of nodes, and node features. To show the role of architecture for
extrapolation, we study the following GNN architecture regimes.
a)	GNN with graph-level max-pooling and neighbor-level sum-pooling. By Theorem 3,
this GNN architecture extrapolates in max degree with appropriate training data.
b)	GNN with graph-level and neighbor-level sum-pooling. By Corollary 1, this default
GNN architecture cannot extrapolate in max degree.
To show the importance of training distribution, i.e., graph structure in training set, we study
the following training data regimes.
a)	Node features are identical, e.g., 1. In such regimes, our learning tasks only consider
graph structure. We consider training sets sampled from various graph structure,
and find only those satisfy conditions in Theorem 3 enables GNNs with graph-level
max-pooling to extrapolate.
b)	Node features are spurious and continuous. This also requires extrapolation on OOD
node features. GNNs with graph-level max-pooling with appropriate training sets also
extrapolate to OOD spurious node features.
6)	Learning the length of the shortest path between given source and target nodes, with Graph
Neural Networks. Extrapolation on graph structure, number of nodes, and edge weights. We
study the following regimes.
a)	Continuous features. Edge and node features are real values. This regime requires
extrapolating to graphs with edge weights out of training range.
Test graphs are all sampled from the “general graphs” family with a diverse range of structure.
Regarding the type of training graph structure, we consider two schemes. Both schemes
show a U-shape curve of extrapolation error with respect to the sparsity of training graphs.
a)	Specific graph structure: path, cycle, tree, expander, ladder, complete graphs, general
graphs, 4-regular graphs.
b)	Random graphs with a range of probability p of an edge between any two nodes.
Smaller p samples sparse graphs and large p samples dense graphs.
7)	Physical reasoning of the n-Body problem in the orbit setting with Graph Neural Networks.
We show that GNNs on the original features from previous works fail to extrapolate to unseen
masses and distances. On the other hand, we show extrapolation can be achieved via an
improved representation of the input edge features. We consider the following extrapolation
regimes.
a)	Extrapolation on the masses of the objects.
b)	Extrapolation on the distances between objects.
We consider the following two input representation schemes to compare the effects of how
representation helps extrapolation.
a)	Original features. Following previous works on solving n-body problem with GNNs,
the edge features are simply set to 0.
b)	Improved features. We show although our edge features do not bring in new information,
it helps extrapolation.
C.1 Learning Simple Non-Linear Functions
Dataset details. We consider four tasks where the underlying functions are simple non-linear
functions g : Rd → R. Given an input x ∈ Rd, the label is computed by y = g(x) for all x. We
consider the following four families of simple functions g.
a) Quadratic functions g(x) = x>Ax. In each dataset, we randomly sample A. In the simplest
case where A = I, g(x) = Pid=1 xi2 .
a) Cosine functions g(x) = Pd=ι Cos (2π ∙ xi).
C) Square root functions g(x) = Pd=I √xi. Here, the domain X of X is restricted to the space
in Rd with non-negative value in each dimension.
32
Published as a conference paper at ICLR 2021
d) L1 norm functions g(x) = |x|1 = Pid=1 |xi|.
We sample each dataset of a task by considering the following parameters
a)	The shape and support of training, validation, and test data distributions.
i)	Training, validation, and test data are uniformly sampled from a hyper-cube. Training
and validation data are sampled from [-a, a]d with a ∈ {0.5, 1.0}, i.e., each dimension
of x ∈ Rd is uniformly sampled from [-a, a]. Test data are sampled from [-a, a]d
with a ∈ {2.0, 5.0, 10.0}.
ii)	Training and validation data are uniformly sampled from a sphere, where every point
has L2 distance r from the origin. We sample r from r ∈ {0.5, 1.0}. Then, we
sample a random Gaussian vector q in Rd. We obtain the training or validation data
X = q∕kqk2 ∙ r. This corresponds to uniform sampling from the sphere.
Test data are sampled (non-uniformly) from a hyper-ball. We first sample r uniformly
from [0.0, 2.0], [0.0, 5.0], and [0.0, 10.0]. Then, we sample a random Gaussian vector
q in Rd. We obtain the test data X = q∕∣∣q∣∣2 ∙ r. This corresponds to (non-uniform)
sampling from a hyper-ball in Rd.
b)	We sample 20, 000 training data, 1, 000 validation data, and 20, 000 test data.
c)	We sample input dimension d from {1, 2, 8}.
d)	For quadratic functions, we sample the entries of A uniformly from [-1, 1].
Model and hyperparameter settings. We consider the multi-layer perceptron (MLP) architecture.
MLP(X) = W⑷∙ σ (W(d-1)σ (…σ (W⑴x)))
We search the following hyper-parameters for MLPs
a)
b)
c)
Number of layers d from {2, 4}.
Width of each W(k) from {64, 128, 512}.
Initialization schemes.
i)
ii)
The default initialization in PyTorch.
The initialization scheme in neural tangent kernel theory, i.e., we sample entries of Wk
from N(0, 1) and scale the output after each
dimension of W(k).
W (k) by 代
, where dk is the output
d) Activation function σ is set to ReLU.
We train the MLP with the mean squared error (MSE) loss, and Adam and SGD optimizer. We
consider the following hyper-parameters for training
a)	Initial learning rate from {5e - 2, 1e - 2, 5e - 3, 1e - 3}. Learning rate decays 0.5 for
every 50 epochs
b)	Batch size from {32, 64, 128}.
c)	Weight decay is set to 1e - 5.
d)	Number of epochs is set to 250.
Test error and model selection. For each dataset and architecture, training hyper-parameter setting,
we perform model selection via validation set, i.e., we report the test error by selecting the epoch
where the model achieves the best validation error. Note that our validation sets always have the same
distribution as the training sets.
We train our models with the MSE loss. Because we sample test data from different ranges, the mean
absolute percentage error (MAPE) loss, which scales the error by the actual value, better measures
the extrapolation performance
1
n
MAPE
Ai - Fi
Ai
33
Published as a conference paper at ICLR 2021
where Ai is the actual value and Fi is the predicted value. Hence, in our experiments, we also report
the MAPE.
C.2 R-squared for Out-of-distribution Directions
We perform linear regression to fit the predictions of MLPs along randomly sampled directions in
out-of-distribution regions, and compute the R-squared (or R2) for these directions. This experiment
is to validate Theorem 1 and show that the convergence rate (to a linear function) is very high in
practice.
Definition. R-squared, also known as coefficient of determination, assesses how strong the linear
relationship is between input and output variables. The closer R-squared is to 1, the stronger the
linear relationship is, with 1 being perfectly linear.
Datasets and models. We perform the R-squared computation on over 2, 000 combinations of
datasets, test/train distributions, and hyper-parameters, e.g., learning rate, batch size, MLP layer,
width, initialization. These are described in Appendix C.1.
Computation. For each combination of dataset and model hyper-parameters as described in Sec-
tion C.1, we save the trained MLP model f : Rd → R. For each dataset and model combination, we
then randomly sample 5, 000 directions via Gaussian vectors N(0, I). For each of these directions
w, we compute the intersection point xw of direction w and the training data distribution support
(specified by a hyper-sphere or hyper-cube; see Section C.1 for details).
We then collect 100 predictions of the trained MLP f along direction w (assume w is normalized)
with
{(xw+k ∙ io ∙ W) ,f(xw+k ∙ io ∙ w)}k=00,	g
where r is the range of training data distribution support (see Section C.1). We perform linear
regression on these predictions in Eqn. 75, and obtain the R-squared.
Results. We obtain the R-squared for each combination of dataset, model and training setting, and
randomly sampled direction. For the tasks of learning the simple non-linear functions, we confirm
that more than 96% of the R-squared results are above o.99. This empirically confirms Theorem 1
and shows that the convergence rate is in fact fast in practice. Along most directions, MLP’s learned
function becomes linear immediately out of the training data support.
C.3 Learning Linear Functions
Dataset details. We consider the tasks where the underlying functions are linear g : Rd → R. Given
an input x ∈ Rd, the label is computed by y = g(x) = Ax for all x. For each dataset, we sample
the following parameters
a)	We sample io, ooo training data, i, ooo validation data, and 2, ooo test data.
b)	We sample input dimension d from {i, 2, 32}.
c)	We sample entries of A uniformly from [-a, a], where we sample a ∈ {5.o, io.o}.
d)	The shape and support of training, validation, and test data distributions.
i)	Training, validation, and test data are uniformly sampled from a hyper-cube. Train-
ing and validation data are sampled from [-a, a]d with a ∈ {5.o, io.o}, i.e., each
dimension of x ∈ Rd is uniformly sampled from [-a, a]. Test data are sampled from
[-a, a]d with a ∈ {2o.o, 5o.o}.
ii)	Training and validation data are uniformly sampled from a sphere, where every point
has L2 distance r from the origin. We sample r from r ∈ {5.o, io.o}. Then, we
sample a random Gaussian vector q in Rd. We obtain the training or validation data
X = q∕kqk2 ∙ r. This corresponds to uniform sampling from the sphere.
34
Published as a conference paper at ICLR 2021
Test data are sampled (non-uniformly) from a hyper-ball. We first sample r uniformly
from [0.0, 20.0] and [0.0, 50.0],. Then, we sample a random Gaussian vector q in Rd.
We obtain the test data X = q∕∣∣q∣∣2 ∙ r. This corresponds to (non-uniform) sampling
from a hyper-ball in Rd .
e)	We perform ablation study on how the training distribution support misses directions. The
test distributions remain the same as in d).
i)	We restrict the first dimension of any training data xi to a fixed number 0.1, and
randomly sample the remaining dimensions according to d).
ii)	We restrict the first k dimensions of any training data xi to be positive. For input
dimension 32, we only consider the hyper-cube training distribution, where we sample
the first k dimensions from [0, a] and sample the remaining dimensions from [-a, a].
For input dimensions 1 and 2, we consider both hyper-cube and hyper-sphere training
distribution by performing rejection sampling. For input dimension 2, we consider k
from {1, 2}. For input dimension 32, we consider k from {1, 16, 32}.
iii)	We restrict the first k dimensions of any training data xi to be negative. For input
dimension 32, we only consider the hyper-cube training distribution, where we sample
the first k dimensions from [-a, 0] and sample the remaining dimensions from [-a, a].
For input dimensions 1 and 2, we consider both hyper-cube and hyper-sphere training
distribution by performing rejection sampling. For input dimension 2, we consider k
from {1, 2}. For input dimension 32, we consider k from {1, 16, 32}.
Model and hyperparameter settings. For the regression task, we search the same set of hyper-
parameters as those in simple non-linear functions (Section C.1).We report the test error with the
same validation procedure as in Section C.1.
Exact computation with neural tangent kernel Our experiments with MLPs validate Theorem 2
asymptotic extrapolation for neural networks trained in regular regimes. Here, we also validate
Lemma 1, exact extrapolation with finite data regime, by training an infinitely-wide neural network.
That is, we directly perform the kernel regression with the neural tangent kernel (NTK). This
experiment is mainly of theoretical interest.
We sample the same test set as in our experiments with MLPs. For training set, we sample 2d training
examples according to the conditions in Lemma 1. Specifically, we first sample an orthogonal basis
and their opposite vectors X = {ei, -ei}id=1. We then randomly sample 100 orthogonal transform
matrices Q via the QR decomposition. Our training samples are QX, i.e., multiply each point in X
by Q. This gives 100 training sets with 2d data points satisfying the condition in Lemma 1.
We perform kernel regression on these training sets using a two-layer neural tangent kernel (NTK).
Our code for exact computation of NTK is adapted from Arora et al. (2020); Novak et al. (2020).
We verify that the test losses are all precisely 0, up to machine precision. This empirically confirms
Lemma 1.
Note that due to the difference of hyper-parameter settings in different implementations of NTK, to
reproduce our experiments and achieve zero test error, the implementation by Arora et al. (2020) is
assumed.
C.4 MLPs with cosine, quadratic, and tanh Activation
This section describes the experimental settings for extrapolation experiments for MLPs with cosine,
quadratic, and tanh activation functions. We train MLPs to learn the following functions:
a)	Quadratic function g(x) = x>Ax, where A is a randomly sampled matrix.
b)	Cosine function g(x) = Pid=1 cos(2π ∙ Xi).
c)	Hyperbolic tangent function g(x) = Pid=1 tanh(xi ).
d)	Linear function g(X) = WX + b.
35
Published as a conference paper at ICLR 2021
Dataset details. We use 20,000 training, 1,000 validation, and 20,000 test data. For quadratic,
we sample input dimension d from {1, 8}, training and validation data from [-1, 1]d, and test data
from [-5, 5]d. For cosine, we sample input dimension d from {1, 2}, training and validation data
from [-100, 100]d, and test data from [-200, 200]d. For tanh, we sample input dimension d from
{1, 8}, training and validation data from [-100, 100]d, and test data from [-200, 200]d. For linear,
we use a subset of datasets from Appendix C.3: 1 and 8 input dimensions with hyper-cube training
distributions.
Model and hyperparameter settings. We use the same hyperparameters from Appendix C.1,
except we fix the batch size to 128, as the batch size has minimal impact on models. MLPs with cos
activation is hard to optimize, so we only report models with training MAPE less than 1.
C.5 Max Degree
Dataset details. We consider the task of finding the maximum degree on a graph. Given any input
graph G = (V, E), the label is computed by the underlying function y = g(G) = max Pv∈N (u) 1.
For each dataset, we sample the graphs and node features with the following parameters
a)	Graph structure for training and validation sets. For each dataset, we consider one of the
following graph structure: path graphs, cycles, ladder graphs, 4-regular random graphs,
complete graphs, random trees, expanders (here we use random graphs with p = 0.8 as they
are expanders with high probability), and general graphs (random graphs with p = 0.1 to
0.9 with equal probability for a broad range of graph structure). We use the networkx library
for sampling graphs.
b)	Graph structure for test set. We consider the general graphs (random graphs with p = 0.1 to
0.9 with equal probability).
c)	The number of vertices of graphs |V | for training and validation sets are sampled uniformly
from [20...30]. The number of vertices of graphs |V | for test set is sampled uniformly from
[50..100].
d)	We consider two schemes for node features.
i)	Identical features. All nodes in training, validation and set sets have uniform feature 1.
ii) Spurious (continuous) features. Node features in training and validation sets are
sampled uniformly from [-5.0, 5.0]3, i.e., a three-dimensional vector where each
dimension is sampled from [-5.0, 5.0]. There are two schemes for test sets, in the first
case we do not extrapolate node features, so we sample node features uniformly from
[-5.0, 5.0]3. In the second case we extrapolate node features, we sample node features
uniformly from [-10.0, 10.0]3.
e)	We sample 5, 000 graphs for training, 1, 000 graphs for validation, and 2, 500 graphs for
testing.
Model and hyperparameter settings. We consider the following Graph Neural Network (GNN)
architecture. Given an input graph G, GNN learns the output hG by first iteratively aggregating and
transforming the neighbors of all node vectors h(uk) (vector for node u in layer k), and perform a max
or sum-pooling over all node features hu to obtain hG . Formally, we have
h(uk) =	X	MLP(k)	h(vk-1),	h(uk-1)	,	hG	= MLP(K+1)	graph-pooling{h(uK)	: u ∈	G}	.
v∈N (u)
(76)
Here, N(u) denotes the neighbors of u, K is the number of GNN iterations, and graph-pooling is a
hyper-parameter with choices as max or sum. h(u0) is the input node feature of node u. We search the
following hyper-parameters for GNNs
a)	Number of GNN iterations K is 1.
b)	Graph pooling is from max or sum.
c)	Width of all MLPs are set to 256.
36
Published as a conference paper at ICLR 2021
d)	The number of layers for MLP(k) with k = 1..K are set to 2. The number of layers for
MLP(K+1) is set to 1.
We train the GNNs with the mean squared error (MSE) loss, and Adam and SGD optimizer. We
search the following hyper-parameters for training
a)	Initial learning rate is set to 0.01.
b)	Batch size is set to 64.
c)	Weight decay is set to 1e - 5.
d)	Number of epochs is set to 300 for graphs with continuous node features, and 100 for graphs
with uniform node features.
Test error and model selection. For each dataset and architecture, training hyper-parameter setting,
we perform model selection via validation set, i.e., we report the test error by selecting the epoch
where the model achieves the best validation error. Note that our validation sets always have the same
distribution as the training sets. Again, we report the MAPE for test error as in MLPs.
C.6 Shortest Path
Dataset details. We consider the task of finding the length of the shortest path on a graph, from a
given source to target nodes. Given any graph G = (V, E), the node features, besides regular node
features, encode whether a node is source s, and whether a node is target t. The edge features are a
scalar representing the edge weight. For unweighted graphs, all edge weights are 1. Then the label
y = g(G) is the length of the shortest path from s to t on G.
For each dataset, we sample the graphs and node, edge features with the following parameters
a)	Graph structure for training and validation sets. For each dataset, we consider one of the
following graph structure: path graphs, cycles, ladder graphs, 4-regular random graphs,
complete graphs, random trees, expanders (here we use random graphs with p = 0.6 which
are expanders with high probability), and general graphs (random graphs with p = 0.1 to
0.9 with equal probability for a broad range of graph structure). We use the networkx library
for sampling graphs.
b)	Graph structure for test set. We consider the general graphs (random graphs with p = 0.1 to
0.9 with equal probability).
c)	The number of vertices of graphs |V | for training and validation sets are sampled uniformly
from [20...40]. The number of vertices of graphs |V | for test set is sampled uniformly from
[50..70].
d)	We consider the following scheme for node and edge features. All edges have continuous
weights. Edge weights for training and validation graphs are sampled from [1.0, 5.0]. There
are two schemes for test sets, in the first case we do not extrapolate edge weights, so
we sample edge weights uniformly from [1.0, 5.0]. In the second case we extrapolate
edge weights, we sample edge weights uniformly from [1.0, 10.0]. All node features are
[h, I(v = s), I(v = t)] with h sampled from [-5.0, 5.0].
e)	After sampling a graph and edge weights, we sample source s and t by randomly sampling
s, t and selecting the first pair s, s whose shortest path involves at most 3 hops. This enables
us to solve the task using GNNs with 3 iterations.
f)	We sample 10, 000 graphs for training, 1, 000 graphs for validation, and 2, 500 graphs for
testing.
We also consider the ablation study of training on random graphs with different p. We consider
p = 0.05..1.0 and report the test error curve. The other parameters are the same as described above.
Model and hyperparameter settings. We consider the following Graph Neural Network (GNN)
architecture. Given an input graph G, GNN learns the output hG by first iteratively aggregating and
37
Published as a conference paper at ICLR 2021
transforming the neighbors of all node vectors h(uk) (vector for node u in layer k), and perform a max
or sum-pooling over all node features hu to obtain hG . Formally, we have
h(uk) = min MLP(k) h(vk-1), h(uk-1), w(uv) ,	hG = MLP(K+1) min hu .	(77)
v∈N (u)	,	u∈G
Here, N (u) denotes the neighbors of u, K is the number of GNN iterations, and for neighbor
aggregation we run both min and sum. h(u0) is the input node feature of node u. w(u,v) is the input
edge feature of edge (u, v). We search the following hyper-parameters for GNNs
a)	Number of GNN iterations K is set to 3.
b)	Graph pooling is set to min.
c)	Neighobr aggregation is selected from min and sum.
d)	Width of all MLPs are set to 256.
e)	The number of layers for MLP(k) with k = 1..K are set to 2. The number of layers for
MLP(K+1) is set to 1.
We train the GNNs with the mean squared error (MSE) loss, and Adam and SGD optimizer. We
consider the following hyper-parameters for training
a)	Initial learning rate is set to 0.01.
b)	Batch size is set to 64.
c)	Weight decay is set to 1e - 5.
d)	Number of epochs is set to 250.
We perform the same model selection and validation as in Section C.5.
C.7 N-Body Problem
Task description. The n-body problem asks a neural network to predict how n stars in a physical
system evolves according to physics laws. That is, we train neural networks to predict properties of
future states of each star in terms of next frames, e.g., 0.001 seconds.
Mathematically, in an n-body system S = {Xi}in=1, such as solar systems, all n stars {Xi}in=1 exert
distance and mass-dependent gravitational forces on each other, so there were n(n - 1) relations or
forces in the system. Suppose Xi at time t is at position xit and has velocity vit . The overall forces a
star Xi receives from other stars is determined by physics laws as the following
Fi = GE
j6=i
mi × mj
kxt -Xjk3
(78)
where G is the gravitational constant, and mi is the mass of star Xi. Then acceralation ait is
determined by the net force Fit and the mass of star mi
ait =Fit/mi	(79)
Suppose the velocity of star Xiat time t is vit. Then assuming the time steps dt, i.e., difference be-
tween time frames, are sufficiently small, the velocity at the next time frame t+1 can be approximated
by
vt+1 = Vi + at ∙ dt.	(80)
Given mi, xit , and vit , our task asks the neural network to predict vit+1 for all stars Xi. In our task,
we consider two extrapolation schemes
a)	The distances between stars kxit - xtj k2 are out-of-distribution for test set, i.e., different
sampling ranges from the training set.
b)	The masses of stars mi are out-of-distribution for test set, i.e., different sampling ranges
from the training set.
Here, we use a physics engine that we code in Python to simulate and sample the inputs and labels.
We describe the dataset details next.
38
Published as a conference paper at ICLR 2021
Dataset details. We first describe the simulation and sampling of our training set. We sample 100
videos of n-body system evolution, each with 500 rollout, i.e., time steps. We consider the orbit
situation: there exists a huge center star and several other stars. We sample the initial states, i.e.,
position, velocity, masses, acceleration etc according to the following parameters.
a)	The mass of the center star is 100kg.
b)	The masses of other stars are sampled from [0.02, 9.0]kg.
c)	The number of stars is 3.
d)	The initial position of the center star is (0.0, 0.0).
d)	The initial positions xit of other objects are randomly sampled from all angles, with a
distance in [10.0, 100.0]m.
e)	The velocity of the center star is 0.
f)	The velocities of other stars are perpendicular to the gravitational force between the center
star and itself. The scale is precisely determined by physics laws to ensure the initial state is
an orbit system.
For each video, after we get the initial states, we continue to rollout the next frames according the
physics engine described above. We perform rejection sampling of the frames to ensure that all
pairwise distances of stars in a frame are at least 30m. We guarantee that there are 10, 000 data points
in the training set.
The validation set has the same sampling and simultation parameters as the training set. We have
2, 500 data points in the validation set.
For test set, we consider two datasets, where we respectively have OOD distances and masses. We
have 5, 000 data points for each dataset.
a)	We sample the distance OOD test set to ensure all pairwise distances of stars in a frame are
from [1..20]m, but have in-distribution masses.
b)	We sample the mass OOD test set as follows
i)	The mass of the center star is 200kg, i.e., twice of that in the training set.
ii)	The masses of other stars are sampled from [0.04, 18.0]kg, compared to [0.02, 9.0]kg
in the training set.
iii)	The distances are in-distribution, i.e., same sampling process as training set.
Model and hyperparameter settings. We consider the following one-iteration Graph Neural
Network (GNN) architecture, a.k.a. Interaction Networks. Given a collection of stars S = {Xi}in=1,
our GNN runs on a complete graph with nodes being the stars Xi . GNN learns the star (node)
representations by aggregating and transforming the interactions (forces) of all other node vectors
ou = MLP⑵( X MLP(I) (hv, hu, w(u,v)) 1 .	(81)
∖v∈S∖{u}	)
Here, hv is the input feature of node v, including mass, position and velocity
hv = (mv, xv, vv )
w(u,v) is the input edge feature of edge (u, v). The loss is computed and backpropagated via the
MSE loss of
k[o1, ..., on] - [ans1, .., ansn]k2,
where oi denotes the output of GNN for node i, and ansi denotes the true label for node i in the next
frame.
We search the following hyper-parameters for GNNs
a)	Number of GNN iterations is set to 1.
39
Published as a conference paper at ICLR 2021
b)	Width of all MLPs are set to 128.
c)	The number of layers for MLP(1) is set to 4. The number of layers for MLP(2) is set to 2.
d)	We consider two representations of edge/relations w(i,j) .
i)	The first one is simply 0.
ii)	The better representation, which makes the underlying target function more linear, is
mj	t t
w(i,j) = kχt - χt k3 ∙ (xj - Xi)
We train the GNN with the mean squared error (MSE) loss, and Adam optimizer. We search the
following hyper-parameters for training
a)	Initial learning rate is set to 0.005. learning rate decays 0.5 for every 50 epochs
b)	Batch size is set to 32.
c)	Weight decay is set to 1e - 5.
d)	Number of epochs is set to 2, 000.
D	Visualization and Additional Experimental Results
D. 1 Visualization Results
In this section, we show additional visualization results of the MLP’s learned function out of training
distribution (in black color) v.s. the underlying true function (in grey color). We color the predictions
in training distribution in blue color.
In general, MLP’s learned functions agree with the underlying true functions in training range (blue).
This is explained by in-distribution generalization arguments. When out of distribution, the MLP’s
learned functions become linear along directions from the origin. We explain this OOD directional
linearity behavior in Theorem 1.
Finally, we show additional experimental results for graph-based reasoning tasks.
40
Published as a conference paper at ICLR 2021
Figure 8: (Quadratic function). Both panels show the learned v.s. true y = x12 + x22 . In each
figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distribution
predictions in blue. The support of training distribution is a square (cube) for the top panel, and is a
circle (sphere) for the bottom panel.
41
Published as a conference paper at ICLR 2021
Figure 9: (Cos function). Both panels ShoW the learned v.s. true y = cos(2π ∙ xι)+ cos(2π ∙ x2).
In each figure, we color OOD predictions by MLPs in black, underlying function in grey, and in-
distribution predictions in blue. The support of training distribution is a square (cube) for both top
and bottom panels, but With different ranges.
42
Published as a conference paper at ICLR 2021
Figure 10: (Cos function). Top panel shows the learned v.s. true y = cos(2π ∙ xι)+ cos(2π ∙ x2)
where the support of training distribution is a circle (sphere). Bottom panel shows results for cosine
in 1D, i.e. y = cos(2π ∙ x). In each figure, We color OOD predictions by MLPS in black, underlying
function in grey, and in-distribution predictions in blue.
43
Published as a conference paper at ICLR 2021
Figure 11: (Sqrt function). Top panel shows the learned v.s. true y = √∕x1 + √∕x2 where the support
of training distribution is a square (cube). Bottom panel shows the results for the square root function
in 1D, i.e. y = √∕x. In each figure, We color OOD predictions by MLPS in black, underlying function
in grey, and in-distribution predictions in blue.
44
Published as a conference paper at ICLR 2021
Figure 12: (L1 function). Both panels show the learned v.s. true y = |x|. In the top panel, the MLP
successfully learns to extrapolate the absolute function. In the bottom panel, an MLP with different
hyper-parameters fails to extrapolate. In each figure, we color OOD predictions by MLPs in black,
underlying function in grey, and in-distribution predictions in blue.
45
Published as a conference paper at ICLR 2021
Figure 13: (L1 function). Both panels show the learned v.s. true y = |x1 | + |x2|. In the top panel,
the MLP successfully learns to extrapolate the l1 norm function. In the bottom panel, an MLP with
different hyper-parameters fails to extrapolate. In each figure, we color OOD predictions by MLPs in
black, underlying function in grey, and in-distribution predictions in blue.
46
Published as a conference paper at ICLR 2021
Figure 14: (Linear function). Both panels show the learned v.s. true y = x1 + x2, with the support
of training distributions being square (cube) for top panel, and circle (sphere) for bottom panel. MLPs
successfully extrapolate the linear function with both training distributions. This is explained by
Theorem 2: both sphere and cube intersect all directions. In each figure, we color OOD predictions
by MLPs in black, underlying function in grey, and in-distribution predictions in blue.
47
Published as a conference paper at ICLR 2021
D.2 Extra Experimental Results
In this section, we show additional experimental results.
48
Published as a conference paper at ICLR 2021
10^3
all fix1 pos1 pos16 pos32
Figure 15: Density plot of the test errors in MAPE. The underlying functions are linear, but we
train MLPs on different distributions, whose support potentially miss some directions. The training
support for “all” are hyper-cubes that intersect all directions. In “fix1”, we set the first dimension of
training data to a fixed number. In “posX”, we restrict the first X dimensions of training data to be
positive. We can see that MLPs trained on “all” extrapolate the underlying linear functions, but MLPs
trained on datasets with missing directions, i.e., “fix1” and “posX”, often cannot extrapolate well.
49
Published as a conference paper at ICLR 2021
86.6
path 4-regular ladder cycle expander ∞mplete tree general
Figure 16: Maximum degree: continuous and “spurious” node features. Here, each node has a
node feature in R3 that shall not contribute to the answer of maximum degree. GNNs with graph-level
max-pooling extrapolate to graphs with OOD node features and graph structure, graph sizes, if trained
on graphs that satisfy the condition in Theorem 3.
50
Published as a conference paper at ICLR 2021
Figure 17: Maximum degree: max-pooling v.s. sum-pooling. In each sub-figure, left column
shows test errors for GNNs with graph-level max-pooling; right column shows test errors for GNNs
with graph-level sum-pooling. x-axis shows the graph structure covered in training set. GNNs with
sum-pooling fail to extrapolate, validating Corollary 1. GNNs with max-pooling encodes appropriate
non-linear operations, and thus extrapolates under appropriate training sets (Theorem 3).
51
Published as a conference paper at ICLR 2021
O
山 d4IΛI
p=1.0
Figure 18: Shortest path: random graphs. We train GNNs with neighbor and graph-level min on
random graphs with probability p of an edge between any two vertices. x-axis denotes the p for the
training set, and y-axis denotes the test/extrapolation error on unseen graphs. The test errors follow a
U-shape: errors are high if the training graphs are very sparse (small p) or dense (large p). The same
pattern is obtained if we train on specific graph structure.
52