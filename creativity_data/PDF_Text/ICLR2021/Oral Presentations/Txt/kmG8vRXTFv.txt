Published as a conference paper at ICLR 2021
Augmenting Physical Models with Deep Net-
works for Complex Dynamics Forecasting
*	Yuan Yin1	* Vincent Le Guen2,3	* Jeremie Dona1	* Emmanuel de Bezenac1
*	Ibrahim Ayed1,4	Nicolas Thome2	Patrick Gallinari1,5
1	Sorbonne UniVersite, CNRS, LIP6, Paris, France
2	Conservatoire National des Arts et Metiers, CEDRIC, Paris, France
3	EDF R&D, Chatou, France
4	Theresis Lab, Thales
5	Criteo AI Lab, Paris, France
Ab stract
Forecasting complex dynamical phenomena in settings where only partial knowl-
edge of their dynamics is available is a prevalent problem across various scientific
fields. While purely data-driven approaches are arguably insufficient in this context,
standard physical modeling based approaches tend to be over-simplistic, inducing
non-negligible errors. In this work, we introduce the APHYNITY framework, a
principled approach for augmenting incomplete physical dynamics described by
differential equations with deep data-driven models. It consists in decomposing the
dynamics into two components: a physical component accounting for the dynamics
for which we have some prior knowledge, and a data-driven component accounting
for errors of the physical model. The learning problem is carefully formulated
such that the physical model explains as much of the data as possible, while the
data-driven component only describes information that cannot be captured by
the physical model, no more, no less. This not only provides the existence and
uniqueness for this decomposition, but also ensures interpretability and benefits
generalization. Experiments made on three important use cases, each representative
of a different family of phenomena, i.e. reaction-diffusion equations, wave equa-
tions and the non-linear damped pendulum, show that APHYNITY can efficiently
leverage approximate physical models to accurately forecast the evolution of the
system and correctly identify relevant physical parameters.
1	Introduction
Modeling and forecasting complex dynamical systems is a major challenge in domains such as
environment and climate (Rolnick et al., 2019), health science (Choi et al., 2016), and in many
industrial applications (Toubeau et al., 2018). Model Based (MB) approaches typically rely on
partial or ordinary differential equations (PDE/ODE) and stem from a deep understanding of the
underlying physical phenomena. Machine learning (ML) and deep learning methods are more prior
agnostic yet have become state-of-the-art for several spatio-temporal prediction tasks (Shi et al.,
2015; Wang et al., 2018; Oreshkin et al., 2020; Dona et al., 2020), and connections have been drawn
between deep architectures and numerical ODE solvers, e.g. neural ODEs (Chen et al., 2018; Ayed
et al., 2019b). However, modeling complex physical dynamics is still beyond the scope of pure ML
methods, which often cannot properly extrapolate to new conditions as MB approaches do.
Combining the MB and ML paradigms is an emerging trend to develop the interplay between the two
paradigms. For example, Brunton et al. (2016); Long et al. (2018b) learn the explicit form of PDEs
directly from data, Raissi et al. (2019); Sirignano & Spiliopoulos (2018) use NNs as implicit methods
for solving PDEs, Seo et al. (2020) learn spatial differences with a graph network, Ummenhofer et al.
(2020) introduce continuous convolutions for fluid simulations, de Bezenac et al. (2018) learn the
* Equal contribution, authors sorted by reverse alphabetical order.
1
Published as a conference paper at ICLR 2021
Figure 1: Predicted dynamics for the damped pendulum vs. ground truth (GT) trajectories d2θ/dt2 +
ω0 sin θ + αdθ∕dt = 0. We show that in (a) the data-driven approach (Chen et al., 2018) fails to
properly learn the dynamics due to the lack of training data, while in (b) an ideal pendulum cannot
take friction into account. The proposed APHYNITY shown in (c) augments the over-simplified
physical model in (b) with a data-driven component. APHYNITY improves both forecasting (MSE)
and parameter identification (Error T0) compared to (b).
velocity field of an advection-diffusion system, Greydanus et al. (2019); Chen et al. (2020) enforce
conservation laws in the network architecture or in the loss function.
The large majority of aforementioned MB/ML hybrid approaches assume that the physical model
adequately describes the observed dynamics. This assumption is, however, commonly violated in
practice. This may be due to various factors, e.g. idealized assumptions and difficulty to explain
processes from first principles (Gentine et al., 2018), computational constraints prescribing a fine
grain modeling of the system (Ayed et al., 2019a), unknown external factors, forces and sources
which are present (Large & Yeager, 2004). In this paper, we aim at leveraging prior dynamical
ODE/PDE knowledge in situations where this physical model is incomplete, i.e. unable to represent
the whole complexity of observed data. To handle this case, we introduce a principled learning
framework to Augment incomplete PHYsical models for ideNtIfying and forecasTing complex
dYnamics (APHYNITY). The rationale of APHYNITY, illustrated in Figure 1 on the pendulum
problem, is to augment the physical model when—and only when—it falls short.
Designing a general method for combining MB and ML approaches is still a widely open problem,
and a clear problem formulation for the latter is lacking (Reichstein et al., 2019). Our contributions
towards these goals are the following:
•	We introduce a simple yet principled framework for combining both approaches. We decom-
pose the data into a physical and a data-driven term such that the data-driven component only
models information that cannot be captured by the physical model. We provide existence
and uniqueness guarantees (Section 3.1) for the decomposition given mild conditions, and
show that this formulation ensures interpretability and benefits generalization.
•	We propose a trajectory-based training formulation (Section 3.2) along with an adaptive
optimization scheme (Section 3.3) enabling end-to-end learning for both physical and deep
learning components. This allows APHYNITY to automatically adjust the complexity of
the neural network to different approximation levels of the physical model, paving the way
to flexible learned hybrid models.
•	We demonstrate the generality of the approach on three use cases (reaction-diffusion, wave
equations and the pendulum) representative of different PDE families (parabolic, hyper-
bolic), having a wide spectrum of application domains, e.g. acoustics, electromagnetism,
chemistry, biology, physics (Section 4). We show that APHYNITY is able to achieve perfor-
mances close to complete physical models by augmenting incomplete ones, both in terms of
forecasting accuracy and physical parameter identification. Moreover, APHYNITY can also
be successfully extended to the partially observable setting (see discussion in Section 5).
2
Published as a conference paper at ICLR 2021
2	Related work
Correction in data assimilation Prediction under approximate physical models has been tackled
by traditional statistical calibration techniques, which often rely on Bayesian methods (Pernot &
Cailliez, 2017). Data assimilation techniques, e.g. the Kalman filter (Kalman, 1960; Becker et al.,
2019), 4D-var (Courtier et al., 1994), prediction errors are modeled probabilistically and a correction
using observed data is applied after each prediction step. Similar residual correction procedures
are commonly used in robotics and optimal control (Chen, 2004; Li et al., 2014). However, these
sequential (two-stage) procedures prevent the cooperation between prediction and correction. Besides,
in model-based reinforcement learning, model deficiencies are typically handled by considering only
short-term rollouts (Janner et al., 2019) or by model predictive control (Nagabandi et al., 2018). The
originality of APHYNITY is to leverage model-based prior knowledge by augmenting it with neurally
parametrized dynamics. It does so while ensuring optimal cooperation between the prior model and
the augmentation.
Augmented physical models Combining physical models with machine learning (gray-box or
hybrid modeling) was first explored from the 1990’s: Psichogios & Ungar (1992); Thompson &
Kramer (1994); Rico-Martinez et al. (1994) use neural networks to predict the unknown parameters
of physical models. The challenge of proper MB/ML cooperation was already raised as a limitation
of gray-box approaches but not addressed. Moreover these methods were evaluated on specific
applications with a residual targeted to the form of the equation. In the last few years, there has
been a renewed interest in deep hybrid models bridging data assimilation techniques and machine
learning to identify complex PDE parameters using cautiously constrained forward model (Long
et al., 2018b; de Bezenac et al., 2018), as discussed in introduction. Recently, some approaches have
specifically targetted the MB/ML cooperation. HybridNet (Long et al., 2018a) and PhICNet (Saha
et al., 2020) both use data-driven networks to learn additive perturbations or source terms to a given
PDE. The former considers the favorable context where the perturbations can be accessed, and
the latter the special case of additive noise on the input. Wang et al. (2019); Mehta et al. (2020)
propose several empirical fusion strategies with deep neural networks but lack theoretical groundings.
PhyDNet (Le Guen & Thome, 2020) tackles augmentation in partially-observed settings, but with
specific recurrent architectures dedicated to video prediction. Crucially, all the aforementioned
approaches do not address the issues of uniqueness of the decomposition or of proper cooperation
for correct parameter identification. Besides, we found experimentally that this vanilla cooperation
is inferior to the APHYNITY learning scheme in terms of forecasting and parameter identification
performances (see experiments in Section 4.2).
3	THE APHYNITY MODEL
In the following, we study dynamics driven by an equation of the form:
dXt
dt
F(Xt)
(1)
defined over a finite time interval [0, T], where the state X is either vector-valued, i.e. we have
Xt ∈ Rd for every t, (pendulum equations in Section 4), or Xt is a d-dimensional vector field
over a spatial domain Ω ⊂ Rk, with k ∈ {2,3}, i.e. Xt(X) ∈ Rd for every (t,x) ∈ [0,T] X Ω
(reaction-diffusion and wave equations in Section 4). We suppose that we have access to a set of
observed trajectories D = {X∙ : [0,T] → A | ∀t ∈ [0, T], dXt/dt = F(Xt)}, where A is the set
of X values (either Rd or vector field). In our case, the unknown F has A as domain and we only
assume that F ∈ F, with (F, ∣∣∙∣∣) a normed vector space.
3.1	Decomposing dynamics into physical and augmented terms
As introduced in Section 1, we consider the common situation where incomplete information is
available on the dynamics, under the form of a family of ODEs or PDEs characterized by their
temporal evolution Fp ∈ Fp ⊂ F. The APHYNITY framework leverages the knowledge of Fp while
mitigating the approximations induced by this simplified model through the combination of physical
and data-driven components. F being a vector space, we can write:
F = Fp + Fa
3
Published as a conference paper at ICLR 2021
where Fp ∈ Fp encodes the incomplete physical knowledge and Fa ∈ F is the data-driven augmenta-
tion term complementing Fp . The incomplete physical prior is supposed to belong to a known family,
but the physical parameters (e.g. propagation speed for the wave equation) are unknown and need to
be estimated from data. Both Fp and Fa parameters are estimated by fitting the trajectories from D.
The decomposition F = Fp + Fa is in general not unique. For example, all the dynamics could
be captured by the Fa component. This decomposition is thus ill-defined, which hampers the
interpretability and the extrapolation abilities of the model. In other words, one wants the estimated
parameters of Fp to be as close as possible to the true parameter values of the physical model and
Fa to play only a complementary role w.r.t Fp, so as to model only the information that cannot be
captured by the physical prior. For example, when F ∈ Fp , the data can be fully described by the
physical model, and in this case it is sensible to desire Fa to be nullified; this is of central importance
in a setting where one wishes to identify physical quantities, and for the model to generalize and
extrapolate to new conditions. In a more general setting where the physical model is incomplete, the
action of Fa on the dynamics, as measured through its norm, should be as small as possible.
This general idea is embedded in the following optimization problem:
min	∣∣Fak subject to ∀X ∈D,∀t, dXt = (FP + Fa)(Xt)
Fp∈Fp,Fa∈F	dt
(2)
The originality of APHYNITY is to leverage model-based prior knowledge by augmenting it with
neurally parametrized dynamics. It does so while ensuring optimal cooperation between the prior
model and the augmentation.
A first key question is whether the minimum in Eq. (2) is indeed well-defined, in other words whether
there exists indeed a decomposition with a minimal norm Fa . The answer actually depends on the
geometry of Fp , and is formulated in the following proposition proven in Appendix B:
Proposition 1 (Existence of a minimizing pair). If Fp is a proximinal set1, there exists a decomposi-
tion minimizing Eq. (2).
Proximinality is a mild condition which, as shown through the proof of the proposition, cannot be
weakened. It is a property verified by any boundedly compact set. In particular, it is true for closed
subsets of finite dimensional spaces. However, if only existence is guaranteed, while forecasts would
be expected to be accurate, non-uniqueness of the decomposition would hamper the interpretability
of Fp and this would mean that the identified physical parameters are not uniquely determined.
It is then natural to ask under which conditions solving problem Eq. (2) leads to a unique decom-
position into a physical and a data-driven component. The following result provides guarantees on
the existence and uniqueness of the decomposition under mild conditions. The proof is given in
Appendix B:
Proposition 2 (Uniqueness of the minimizing pair). If Fp is a Chebyshev set1 , Eq. (2) admits a
unique minimizer. The Fp in this minimizer pair is the metric projection of the unknown F onto Fp .
The Chebyshev assumption condition is strictly stronger than proximinality but is still quite mild
and necessary. Indeed, in practice, many sets of interest are Chebyshev, including all closed convex
spaces in strict normed spaces and, if F = L2 , Fp can be any closed convex set, including all finite
dimensional subspaces. In particular, all examples considered in the experiments are Chebyshev sets.
Propositions 1 and 2 provide, under mild conditions, the theoretical guarantees for the APHYNITY
formulation to infer the correct MB/ML decomposition, thus enabling both recovering the proper
physical parameters and accurate forecasting.
3.2	S olving APHYNITY with deep neural networks
In the following, both terms of the decomposition are parametrized and are denoted as Fpθp and
Fpθa . Solving APHYNITY then consists in estimating the parameters θp and θa . θp are the physical
parameters and are typically low-dimensional, e.g. 2 or 3 in our experiments for the considered
physical models. For Fa , we need sufficiently expressive models able to optimize over all F : we
1A proximinal set is one from which every point of the space has at least one nearest point. A Chebyshev set
is one from which every point of the space has a unique nearest point. More details in Appendix A.
4
Published as a conference paper at ICLR 2021
thus use deep neural networks, which have shown promising performances for the approximation of
differential equations (Raissi et al., 2019; Ayed et al., 2019b).
When learning the parameters of Fpθp and Faθa , we have access to a finite dataset of trajectories
discretized with a given temporal resolution ∆t: Dtrain = {(X(∆t)o≤k≤bτ∕∆tC}ι≤i≤N. Solving
Eq. (2) requires estimating the state derivative dXt/dt appearing in the constraint term. One solution
is to approximate this derivative using e.g. finite differences as in (Brunton et al., 2016; Greydanus
et al., 2019; Cranmer et al., 2020). This numerical scheme requires high space and time resolutions
in the observation space in order to get reliable gradient estimates. Furthermore it is often unstable,
leading to explosive numerical errors as discussed in Appendix D. We propose instead to solve Eq. (2)
using an integral trajectory-based approach: we compute Xeki∆t,X from an initial state X0(i) using
the current Fpθp + Faθa dynamics, then enforce the constraint Xeki ∆t,X = Xki∆t. This leads to our
final objective function on (θp, θa):
min	Faθa	subject to ∀i, ∀k, Xek(i∆) t = Xk(i∆)t	(3)
θp,θa
where Xek(i∆) is the approximate solution of the integral R X(0i) +k∆t(Fpθp + Fθa )(Xs) dXs obtained
k t	X0( )	a
by a differentiable ODE solver.
In our setting, where we consider situations for which Fpθp only partially describes the physical
phenomenon, this coupled MB + ML formulation leads to different parameter estimates than using the
MB formulation alone, as analyzed more thoroughly in Appendix C. Interestingly, our experiments
show that using this formulation also leads to a better identification of the physical parameters θp than
when fitting the simplified physical model Fpθp alone (Section 4). With only an incomplete knowledge
on the physics, θp estimator will be biased by the additional dynamics which needs to be fitted in the
data. Appendix F also confirms that the integral formulation gives better forecasting results and a
more stable behavior than supervising over finite difference approximations of the derivatives.
3.3	Adaptively constrained optimization
The formulation in Eq. (3) involves constraints which are difficult to enforce exactly in practice.
We considered a variant of the method of multipliers (Bertsekas, 1996) which uses a sequence of
Lagrangian relaxations Lλj (θp, θa):
Lλj (θp,θa) = kFtθa k + λj ∙ Lt. (θp,θa)	(4)
Where Ltraj (θp,θa) = PN=1 PT=δ t kXh∆ t - Xh∆ tk.
This method needs an increasing sequence (λj)j such
that the successive minima of Lλj converge to a so-
lution (at least a local one) of the constrained prob-
lem Eq. (3). We select (λj )j by using an iterative
strategy: starting from a value λ0, we iterate, mini-
mizing Lλj by gradient descent2, then update λj with:
λj+1 = λj + τ2Ltraj(θj+1), where τ2 is a chosen
hyper-parameter and θ = (θp, θa). This procedure is
summarized in Algorithm 1. This adaptive iterative
procedure allows us to obtain stable and robust results,
in a reproducible fashion, as shown in the experiments.
4	Experimental validation
We validate our approach on 3 classes of challenging physical dynamics: reaction-diffusion, wave
propagation, and the damped pendulum, representative of various application domains such as
chemistry, biology or ecology (for reaction-diffusion) and earth physic, acoustic, electromagnetism or
2Convergence to a local minimum isn’t necessary, afew steps are often sufficient for a successful optimization.
Algorithm 1: APHYNrTY
Initialization: λ0 ≥ 0, τ1 > 0, τ2 > 0;
for epoch = 1 : Nepochs do
for iter in 1 : Niter do
for batch in 1 : B do
θj + 1 = θj -
T1V [λjLtraj (θj ) + kFak]
λj+1 = λj+ τ2Ltraj(θj+1)
5
Published as a conference paper at ICLR 2021
even neuro-biology (for waves equations). The two first dynamics are described by PDEs and thus in
practice should be learned from very high-dimensional vectors, discretized from the original compact
domain. This makes the learning much more difficult than from the one-dimensional pendulum case.
For each problem, we investigate the cooperation between physical models of increasing complexity
encoding incomplete knowledge of the dynamics (denoted Incomplete physics in the following) and
data-driven models. We show the relevance of APHYNITY (denoted APHYNITY models) both in
terms of forecasting accuracy and physical parameter identification.
4.1	Experimental setting
We describe the three families of equations studied in the experiments. In all experiments, F = L2 (A)
where A is the set of all admissible states for each problem, and the L2 norm is computed on Dtrain
by: kF k2 ≈ Pi,k kF(Xk(i∆)t)k2. All considered sets of physical functionals Fp are closed and convex
in F and thus are Chebyshev. In order to enable the evaluation on both prediction and parameter
identification, all our experiments are conducted on simulated datasets with known model parameters.
Each dataset has been simulated using an appropriate high-precision integration scheme for the
corresponding equation. All solver-based models take the first state X0 as input and predict the
remaining time-steps by integrating F through the same differentiable generic and common ODE
solver (4th order Runge-Kutta)3. Implementation details and architectures are given in Appendix E.
Reaction-diffusion equations We consider a 2D FitzHugh-Nagumo type model (Klaasen & Troy,
1984). The system is driven by the PDE 察 =a∆u + RU(U,v; k),* =b∆v + Rv(u, V) where
a and b are respectively the diffusion coefficients of u and v, ∆ is the Laplace operator. The local
reaction terms are RU (u, v; k) = u - u3 - k - v, Rv (u, v) = u - v. The state is X = (u, v)
and is defined over a compact rectangular domain Ω with periodic boundary conditions. The
considered physical models are: • Param PDE (a, b), with unknown (a, b) diffusion terms and
without reaction terms: Fp = {Fpa,b : (u, v) 7→ (a∆u, b∆v) | a ≥ amin > 0, b ≥ bmin > 0};
• Param PDE (a, b, k), the full PDE with unknown parameters: Fp = {Fpa,b,k : (u, v) 7→
(a∆u + RU (u, v; k), b∆v + Rv (u, v) | a ≥ amin > 0, b ≥ bmin > 0, k ≥ kmin > 0}.
Damped wave equations We investigate the damped-wave PDE: d2w - c2∆w + k端=0 where
k is the damping coefficient. The state is X = (w,答)and We consider a compact spatial do-
main Ω with Neumann homogeneous boundary conditions. Note that this damping differs from
the pendulum, as its effect is global. Our physical models are: • Param PDE (c), without damp-
ing term: Fp = {Fpc : (u, v) 7→ (v, c2∆u) | c ∈ [, +∞) with > 0}; • Param PDE (c, k):
Fp = {Fpc,k : (u, v) 7→ (v, c2∆u - kv) | c, k ∈ [, +∞) with > 0}.
Damped pendulum The evolution follows the ODE d2θ∕dt2 + ω0 Sin θ + αdθ∕dt = 0, where
θ(t) is the angle, ω0 the proper pulsation (T0 the period) and α the damping coefficient. With
state X = (θ, dθ/dt), the Ode is Fpj0,α : X → (dθ/dt, -ω2 sin θ - αdθ∕dt). Our physi-
cal models are: • Hamiltonian (Greydanus et al., 2019), a conservative approximation, with
Fp = {FpH : (u, v) 7→ (∂y H(u, v), -∂xH(u, v)) | H ∈ H1(R2)}, H1(R2) is the first order Sobolev
space. • Param ODE (ω0), the frictionless pendulum: FP = {Fpω0,α=0∣ ω0 ∈ [e, +∞) with e > 0}
• Param ODE (ω0, α), the full pendulum equation: FP = {Fpω0,α∣ ω0, α ∈ [e, +∞) with e > 0}.
Baselines As purely data-driven baselines, we use Neural ODE (Chen et al., 2018) for the three
problems and PredRNN++ (Wang et al., 2018, for reaction-diffusion only) which are competitive
models for datasets generated by differential equations and for spatio-temporal data. As MB/ML
methods, in the ablations studies (see Appendix F), we compare for all problems, to the vanilla
MB/ML cooperation scheme found in (Wang et al., 2019; Mehta et al., 2020). We also show results
for True PDE/ODE, which corresponds to the equation for data simulation (which do not lead to zero
error due to the difference between simulation and training integration schemes). For the pendulum,
we compare to Hamiltonian neural networks (Greydanus et al., 2019; Toth et al., 2020) and to the the
deep Galerkin method (DGM, Sirignano & Spiliopoulos, 2018). See additional details in Appendix E.
6
Published as a conference paper at ICLR 2021
Table 1: Forecasting and identification results on the (a) reaction-diffusion, (b) wave equation, and
(c) damped pendulum datasets. We set for (a) a = 1 × 10-3, b = 5 × 10-3, k = 5 × 10-3, for
(b) c = 330, k = 50 and for (c) T0 = 6, α = 0.2 as true parameters. log MSEs are computed
respectively over 25, 25, and 40 predicted time-steps. %Err param. averages the results when several
physical parameters are present. For each level of incorporated physical knowledge, equivalent best
results according to a Student t-test are shown in bold. n/a corresponds to non-applicable cases.
Dataset	Method		log MSE	%Err param.	kFak2
	Data-	Neural ODE	-3.76±0.02	n/a	n/a
	driven	PredRNN++	-4.60±0.01	n/a	n/a
(a)	Incomplete	Param PDE (a, b)	-1.26±0.02	67.6	n/a
Reaction-	physics	APHYNITY Param PDE (a, b)	-5.10±0.21	2.3	67
					
diffusion					
		Param PDE (a, b, k)	-9.34±0.20	0.17	n/a
	Complete	APHYNrTY Param PDE (a,b,k)	935±0.02	0.096	1.5e-6
	physics	True PDE	…二8.8i±0.05 …	n/a	n/a
		APHYNrTY True PDE	-9.17±0.02	n/a	1.4e-7
	Data-driven	Neural ODE	-2.51±0.29	n/a	n/a
	Incomplete	Param PDE (c)	0.51±0.07	10.4	n/a
(b)	physics	APHYNrTY Param PDE (c)	-4.64±0.25	0.31	71.
Wave equation		Param PDE (c, k)	-4.68±0.55	1.38	n/a
	Complete	Aphynity Param pde (c, k)	-6.09±0.28	0.70	4.54
	physics	"TruePDE						…二4.66±0.30 …	n/a	n/a
		APHYNrTY True PDE	-5.24±0.45	n/a	0.14
	Data-driven	Neural ODE	-2.84±0.70	n/a	n/a
		Hamiltonian	-0.35±0.10	n/a	n/a
	Incomplete physics	APHYNITY Hamiltonian	-3.97±1.20	n/a	623
(c)		Param ODE (ωo)	…;0.14±0.10 …	13.2	n/a
Damped pendulum		Deep Galerkin Method (ω0)	-3.10±0.40	22.1	n/a
		APHYNrTY Param ODE (ω0)	-7.86±0.60	4.0	132
		Param ODE (ω0 , α)	-8.28±0.40	0.45	n/a
		Deep Galerkin Method (ω0 , α)	-3.14±0.40	7.1	n/a
	Complete physics	APHYNITY Param ODE (ω0, α)	-8.31±0.30	0.39	8.5
		True OdE	-8.58±0.20	n/a	n/a
		APHYNITY True ODE	-8.44±0.20	n/a	2.3
4.2	Results
We analyze and discuss below the results obtained for the three kind of dynamics. We successively
examine different evaluation or quality criteria. The conclusions are consistent for the three problems,
which allows us to highlight clear trends for all of them.
Forecasting accuracy The data-driven models do not perform well compared to True PDE/ODE
(all values are test errors expressed as log MSE): -4.6 for PredRNN++ vs. -9.17 for reaction-diffusion,
-2.51 vs. -5.24 for wave equation, and -2.84 vs. -8.44 for the pendulum in Table 1. The Deep Galerkin
method for the pendulum in complete physics DGM (ω0 , α), being constrained by the equation,
outperforms Neural ODE but is far inferior to APHYNITY models. In the incomplete physics case,
DGM (ω0) fails to compensate for the missing information. The incomplete physical models, Param
PDE (a, b) for the reaction-diffusion, Param PDE (c) for the wave equation, and Param ODE (ω0)
and Hamiltonian models for the damped pendulum, have even poorer performances than purely
data-driven ones, as can be expected since they ignore important dynamical components, e.g. friction
in the pendulum case. Using APHYNITY with these imperfect physical models greatly improves
forecasting accuracy in all cases, significantly outperforming purely data-driven models, and reaching
results often close to the accuracy of the true ODE, when APHYNITY and the true ODE models are
integrated with the same numerical scheme (which is different from the one used for data generation,
hence the non-null errors even for the true equations), e.g. -5.92 vs. -5.24 for wave equation in
3This integration scheme is then different from the one used for data generation, the rationale for this choice
being that when training a model one does not know how exactly the data has been generated.
7
Published as a conference paper at ICLR 2021
(a) Param PDE (a, b), diffusion-only (b) APHYNITY Param PDE (a, b)
Figure 2: Comparison of predictions of two components u (top) and v (bottom) of the reaction-
diffusion system. Note that t = 4 is largely beyond the dataset horizon (t = 2.5).
(c) Ground truth simulation
(a) Neural ODE	(b) APHYNITY Param PDE (c)	(c) Ground truth simulation
Figure 3: Comparison between the prediction of APHYNITY when c is estimated and Neural ODE
for the damped wave equation. Note that t + 32, last column for (a, b, c) is already beyond the
training time horizon (t + 25), showing the consistency of APHYNITY method.
Table 1. This clearly highlights the capacity of our approach to augment incomplete physical models
with a learned data-driven component.
Physical parameter estimation Confirming the phenomenon mentioned in the introduction and
detailed in Appendix C, incomplete physical models can lead to bad estimates for the relevant physical
parameters: an error respectively up to 67.6% and 10.4% for parameters in the reaction-diffusion
and wave equations, and an error of more than 13% for parameters for the pendulum in Table 1.
APHYNITY is able to significantly improve physical parameters identification: 2.3% error for the
reaction-diffusion, 0.3% for the wave equation, and 4% for the pendulum. This validates the fact
that augmenting a simple physical model to compensate its approximations is not only beneficial for
prediction, but also helps to limit errors for parameter identification when dynamical models do not
fit data well. This is crucial for interpretability and explainability of the estimates.
Ablation study We conduct ablation studies to validate the importance of the APHYNITY aug-
mentation compared to a naive strategy consisting in learning F = Fp + Fa without taking care on
the quality of the decomposition, as done in (Wang et al., 2019; Mehta et al., 2020). Results shown in
Table 1 of Appendix F show a consistent gain of APHYNITY for the three use cases and for all phys-
ical models: for instance for Param ODE (a, b) in reaction-diffusion, both forecasting performances
(log MSE =-5.10 vs. -4.56) and identification parameter (Error= 2.33% vs. 6.39%) improve. Other
ablation results are provided in Appendix F showing the relevance of the the trajectory-based approach
described in Section 3.2 (vs supervising over finite difference approximations of the derivative F).
Flexibility When applied to complete physical models, APHYNITY does not degrade accuracy,
contrary to a vanilla cooperation scheme (see ablations in Appendix F). This is due to the least action
principle of our approach: when the physical knowledge is sufficient for properly predicting the
observed dynamics, the model learns to ignore the data-driven augmentation. This is shown by the
norm of the trained neural net component Fa , which is reported in Table 1 last column: as expected,
kFa k2 diminishes as the complexity of the corresponding physical model increases, and, relative to
incomplete models, the norm becomes very small for complete physical models (for example in the
pendulum experiments, we have kFak = 8.5 for the APHYNITY model to be compared with 132
and 623 for the incomplete models). Thus, we see that the norm of Fa is a good indication of how
imperfect the physical models Fp are. It highlights the flexibility of APHYNITY to successfully
adapt to very different levels of prior knowledge. Note also that APHYNITY sometimes slightly
improves over the true ODE, as it compensates the error introduced by different numerical integration
methods for data simulation and training (see Appendix E).
Qualitative visualizations Results in Figure 2 for reaction-diffusion show that the incomplete
diffusion parametric PDE in Figure 2(a) is unable to properly match ground truth simulations: the
8
Published as a conference paper at ICLR 2021
behavior of the two components in Figure 2(a) is reduced to simple independent diffusions due to
the lack of interaction terms between u and v . By using APHYNITY in Figure 2(b), the correlation
between the two components appears together with the formation of Turing patterns, which is very
similar to the ground truth. This confirms that Fa can learn the reaction terms and improve prediction
quality. In Figure 3, we see for the wave equation that the data-driven Neural ODE model fails
at approximating dw/dt as the forecast horizon increases: it misses crucial details for the second
component dw/dt which makes the forecast diverge from the ground truth. APHYNITY incorporates a
Laplacian term as well as the data-driven Fa thus capturing the damping phenomenon and succeeding
in maintaining physically sound results for long term forecasts, unlike Neural ODE.
Extension to non-stationary dynamics We provide additional results in Appendix G to tackle
datasets where physical parameters of the equations vary in each sequence. To this end, we design
an encoder able to perform parameter estimation for each sequence. Results show that APHYNITY
accommodates well to this setting, with similar trends as those reported in this section.
Additional illustrations We give further visual illustrations to demonstrate how the estimation of
parameters in incomplete physical models is improved with APHYNITY. For the reaction-diffusion
equation, we show that the incomplete parametric PDE underestimates both diffusion coefficients.
The difference is visually recognizable between the poorly estimated diffusion (Figure 4(a)) and the
true one (Figure 4(c)) while APHYNITY gives a fairly good estimation of those diffusion parameters
as shown in Figure 4(b).
(a) a = 0.33 × 10-3,b =
0.94 × 10-3, diffusion estimated
with Param PDE (a, b)
(b) a =	0.97 × 10-3, b =
4.75 × 10-3, diffusion estimated
with APHYNITY Param PDE (a, b)
(c) a = 1.0× 10-3, b = 5.0× 10-3,
true diffusion
Figure 4: Diffusion predictions using coefficient learned with (a) incomplete physical model Param
PDE (a, b) and (b) APHYNITY-augmented Param PDE(a, b), compared with the (c) true diffusion
5 Conclusion
In this work, we introduce the APHYNITY framework that can efficiently augment approximate
physical models with deep data-driven networks, performing similarly to models for which the
underlying dynamics are entirely known. We exhibit the superiority of APHYNITY over data-driven,
incomplete physics, and state-of-the-art approaches combining ML and MB methods, both in terms
of forecasting and parameter identification on three various classes of physical systems. Besides,
APHYNITY is flexible enough to adapt to different approximation levels of prior physical knowledge.
An appealing perspective is the applicability of APHYNITY on partially-observable settings, such as
video prediction. Besides, we hope that the APHYNITY framework will open up the way to the design
of a wide range of more flexible MB/ML models, e.g. in climate science, robotics or reinforcement
learning. In particular, analyzing the theoretical decomposition properties in a partially-observed
setting is an important direction for future work.
Acknowledgements:
Funding (P. Gallinari), Chaires de recherche et d’enseignement en intelligence artificielle (Chaires
IA), DL4Clim project.
9
Published as a conference paper at ICLR 2021
References
Ibrahim Ayed, Nicolas Cedilnik, Patrick Gallinari, and Maxime Sermesant. Ep-net: Learning cardiac
electrophysiology models for physiology-based constraints in data-driven predictions. In Yves
CoUdiere, Valery Ozenne, Edward J. Vigmond, and Nejib Zemzemi (eds.), Functional Imaging
and Modeling of the Heart - 10th International Conference, FIMH 2019, Bordeaux, France, June
6-8, 2019, Proceedings, volume 11504 of Lecture Notes in Computer Science, pp. 55-63. Springer,
2019a.
Ibrahim Ayed, Emmanuel de Bezenac, Arthur Pajot, Julien Brajard, and Patrick Gallinari. Learning
dynamical systems from partial observations. arXiv preprint arXiv:1902.11136, 2019b.
Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, James Taylor, and Gerhard Neu-
mann. Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces.
International Conference on Machine Learning (ICML), 2019.
Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods (Optimization and
Neural Computation Series). Athena Scientific, 1 edition, 1996.
Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data
by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of
Sciences, 113(15):3932-3937, 2016.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems (NeurIPS), pp.
6571-6583, 2018.
Wen-Hua Chen. Disturbance observer based control for nonlinear systems. IEEE/ASME transactions
on mechatronics, 9(4):706-710, 2004.
Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Leon Bottou. Symplectic recurrent neural
networks. International Conference on Learning Representations (ICLR), 2020.
Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter
Stewart. RETAIN: An interpretable predictive model for healthcare using reverse time attention
mechanism. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3504-3512,
2016.
Philippe Courtier, J-N Thepaut, and Anthony Hollingsworth. A strategy for operational implemen-
tation of 4d-var, using an incremental approach. Quarterly Journal of the Royal Meteorological
Society, 120(519):1367-1387, 1994.
Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho.
Lagrangian neural networks. ICLR 2020 Deep Differential Equations Workshop, 2020.
Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientific knowledge. International Conference on Learning Representations
(ICLR), 2018.
Jeremie DonW, Jean-Yves Franceschi, Sylvain Lamprier, and Patrick Gallinari. Pde-driven spa-
tiotemporal disentanglement. International Conference on Learning Representations (ICLR),
2020.
John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of
computational and applied mathematics, 6(1):19-26, 1980.
James Fletcher and Warren Moors. Chebyshev sets. Journal of the Australian Mathematical Society,
98:161-231, 04 2014. doi: 10.1017/S1446788714000561.
P. Gentine, M. Pritchard, S. Rasp, G. Reinaudi, and G. Yacalis. Could machine learning break the
convection parameterization deadlock? Geophysical Research Letters, 45(11):5742-5751, 2018.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems (NeurIPS), pp. 15353-15363, 2019.
10
Published as a conference paper at ICLR 2021
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
12519-12530, 2019.
Gordon G Johnson. A nonconvex set which has the unique nearest point property. Journal of
Approximation Theory, 51(4):289 - 332, 1987.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.
Gene A. Klaasen and William C. Troy. Stationary wave solutions of a system of reaction-diffusion
equations derived from the fitzhugh-nagumo equations. SIAM Journal on Applied Mathematics,
44(1):96-110, 1984. doi: 10.1137/0144008.
William Large and Stephen Yeager. Diurnal to decadal global forcing for ocean and sea-ice models:
The data sets and flux climatologies, 05 2004.
Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for
unsupervised video prediction. In Computer Vision and Pattern Recognition (CVPR). 2020.
Shihua Li, Jun Yang, Wen-Hua Chen, and Xisong Chen. Disturbance observer-based control:
methods and applications. CRC press, 2014.
Yun Long, Xueyuan She, and Saibal Mukhopadhyay. Hybridnet: integrating model-based and
data-driven learning to predict evolution of dynamical systems. Conference on Robot Learning
(CoRL), 2018a.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-Net: Learning PDEs from data. In
International Conference on Machine Learning (ICML), 2018b.
Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, and Jeff Schneider. Neural dynamical
systems. ICLR 2020 Deep Differential Equations Workshop, 2020.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynam-
ics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis
expansion analysis for interpretable time series forecasting. International Conference on Learning
Representations (ICLR), 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. 2019.
Pascal Pernot and Fabien Cailliez. A critical review of statistical calibration/prediction models
handling data inconsistency and model inadequacy. AIChE Journal, 63(10):4642-4665, 2017.
Dimitris C Psichogios and Lyle H Ungar. A hybrid neural network-first principles approach to process
modeling. AIChE Journal, 38(10):1499-1511, 1992.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 473:686-707, 2019.
Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno
Carvalhais, and & Prabhat. Deep learning and process understanding for data-driven Earth system
science. Nature, 566:195-204, 2019.
R Rico-Martinez, JS Anderson, and IG Kevrekidis. Continuous-time nonlinear signal processing: a
neural network based approach for gray box identification. In Proceedings of IEEE Workshop on
Neural Networks for Signal Processing, pp. 596-605. IEEE, 1994.
11
Published as a conference paper at ICLR 2021
David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran,
Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al.
Tackling climate change with machine learning. In NeurIPS 2019 workshop on Climate Change
with Machine Learning, 2019.
Priyabrata Saha, Saurabh Dash, and Saibal Mukhopadhyay. PhICNet: Physics-incorporated convolu-
tional recurrent neural networks for modeling dynamical systems. arXiv preprint arXiv:2004.06243,
2020.
Sungyong Seo, Chuizheng Meng, and Yan Liu. Physics-aware difference graph networks for sparsely-
observed dynamics. International Conference on Learning Representations (ICLR), 2020.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In
Advances in neural information processing systems (NeurIPS), pp. 802-810, 2015.
Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial
differential equations. Journal of computational physics, 375:1339-1364, 2018.
Michael L Thompson and Mark A Kramer. Modeling chemical processes using prior knowledge and
neural networks. AIChE Journal, 40(8):1328-1340, 1994.
Peter Toth, Danilo Jimenez Rezende, Andrew Jaegle, Sebastien Racani3re, Aleksandar Botev, and
Irina Higgins. Hamiltonian generative networks. International Conference on Learning Represen-
tations (ICLR), 2020.
Jean-Frangois Toubeau, Jeremie Bottieau, Frangois Vanee, and Zacharie De Greve. Deep Iearning-
based multivariate probabilistic forecasting for short-term scheduling in power markets. IEEE
Transactions on Power Systems, 34(2):1203-1215, 2018.
Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation
with continuous convolutions. International Conference on Learning Representations (ICLR),
2020.
Qi Wang, Feng Li, Yi Tang, and Yan Xu. Integrating model-driven and data-driven methods for
power system frequency stability assessment and control. IEEE Transactions on Power Systems,
34(6):4557-4568, 2019.
Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. PredRNN++: Towards
a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In International
Conference on Machine Learning (ICML), 2018.
12
Published as a conference paper at ICLR 2021
A Reminder on proximinal and Chebyshev sets
We begin by giving a definition of proximinal and Chebyshev sets, taken from (Fletcher & Moors,
2014):
Definition 1. A Proximinal set ofa normed space (E, k ∙ k) is a subset C ⊂ E such that every X ∈ E
admits at least a nearest point in C.
Definition 2. A Chebyshev set ofa normed space (E, k ∙ ∣∣) is a subset C ⊂ E such that every X ∈ E
admits a unique nearest point in C.
Proximinality reduces to a compacity condition in finite dimensional spaces. In general, it is a weaker
one: Boundedly compact sets verify this property for example.
In Euclidean spaces, Chebyshev sets are simply the closed convex subsets. The question of knowing
whether it is the case that all Chebyshev sets are closed convex sets in infinite dimensional Hilbert
spaces is still an open question. In general, there exists examples of non-convex Chebyshev sets, a
famous one being presented in (Johnson, 1987) for a non-complete inner-product space.
Given the importance of this topic in approximation theory, finding necessary conditions for a set to
be Chebyshev and studying the properties of those sets have been the subject of many efforts. Some
of those properties are summarized below:
•	The metric projection on a boundedly compact Chebyshev set is continuous.
•	If the norm is strict, every closed convex space, in particular any finite dimensional subspace
is Chebyshev.
•	In a Hilbert space, every closed convex set is Chebyshev.
B Proof of Propositions 1 and 2
We prove the following result which implies both propositions in the article:
Proposition 3. The optimization problem:
F ∈min H ∣∣Fa∣ subject to ∀X ∈ D,∀t, dXt = (FP + Fa)(Xt)	(5)
is equivalent a metric projection onto Fp.
If FP is proximinal, Eq. (5) admits a minimizing pair.
If FP is Chebyshev, Eq. (5) admits a unique minimizing pair which FP is the metric projection.
Proof. The idea is to reconstruct the full functional from the trajectories of D. By definition, A is the
set of points reached by trajectories in D so that:
A = {x ∈ Rd | ∃X∙ ∈ D, ∃t, Xt = χ}
Then let Us define a function FD in the following way: For a ∈ A,we can find X∙ ∈ D and to such
that Xt0 = a. Differentiating X at t0 , which is possible by definition of D, we take:
FD(a)
dXt
dt
t=t0
For any (FP, Fa) satisfying the constraint in Eq. (5), we then have that (FP + Fa)(a) = dXt/dt|t0 =
FD(a) for all a ∈ A. Conversely, any pair such that (FP, Fa) ∈ FP × F and FP + Fa = FD, verifies
the constraint.
Thus we have the equivalence between Eq. (5) and the metric projection formulated as:
mFiPn∈imFizPe	FD-FP
(6)
13
Published as a conference paper at ICLR 2021
If Fp is proximinal, the projection problem admits a solution which we denote Fp? . Taking Fa? =
FD - Fp?, we have that Fp? + Fa? = FD so that (Fp?, Fa?) verifies the constraint of Eq. (2). Moreover,
if there is (Fp, Fa) satisfying the constraint of Eq. (2), we have that Fp + Fa = FD by what was
shown above and kFak = kFD - Fpk ≥ kFD - Fp?k by definition of Fp?. This shows that (Fp?, Fa?)
is minimal.
Moreover, if Fp is a Chebyshev set, by uniqueness of the projection, if Fp 6= Fp? then kFa k > kFa? k.
Thus the minimal pair is unique.
□
C Parameter estimation in incomplete physical models
Classically, when a set Fp ⊂ F summarising the most important properties of a system is available,
this gives a simplified model of the true dynamics and the adopted problem is then to fit the trajectories
using this model as well as possible, solving:
minimize
Fp ∈ Fp
subject to
EX 〜D L(X X0 ,X)
dXg
Vg ∈ I, χo = g and∀t, d^ = FP(Xt)
(7)
X
where L is a discrepancy measure between trajectories. Recall that XX0 is the result trajectory of
an ODE solver taking X0 as initial condition. In other words, we try to find a function FP which
gives trajectories as close as possible to the ones from the dataset. While estimation of the function
becomes easier, there is then a residual part which is left unexplained and this can be a non negligible
issue in at least two ways:
•	When F 6∈ FP , the loss is strictly positive at the minimum. This means that reducing the
space of functions FP makes us lose in terms of accuracy.4
•	The obtained function FP might not even be the most meaningful function from FP as it
would try to capture phenomena which are not explainable with functions in FP , thus giving
the wrong bias to the calculated function. For example, if one is considering a dampened
periodic trajectory where only the period can be learned in FP but not the dampening, the
estimated period will account for the dampening and will thus be biased.
This is confirmed in the paper in Section 4: the incomplete physical models augmented with
APHYNITY get different and experimentally better physical identification results than the physical
models alone.
Let us compare our approach with this one on the linearized damped pendulum to show how estimates
of physical parameters can differ. The equation is the following:
d2θ 2	dθ
涯+ω0θ+α d = 0
We take the same notations as in the article and parametrize the simplified physical models as:
dθ
Fp:χ → (dt, -aθ)
where a > 0 corresponds to ω02. The corresponding solution for an initial state X0, which we denote
Xa, can then written explicitly as:
θa = θo cos √at
Let us consider damped pendulum solutions X written as:
θt = θ0e-t cost
which corresponds to:
F : χ→ M2(。+ d))
4This is true in theory, although not necessarily in practice when F overfits a small dataset.
14
Published as a conference paper at ICLR 2021
It is then easy to see that the estimate of a with the physical model alone can be obtained by
minimizing:
ZT
0
|e-t cos t — cos √at∣2
This expression depends on T and thus, depending on the chosen time interval and the way the
integral is discretized will almost always give biased estimates. In other words, the estimated value
ofa will not give us the desired solution t 7→ cos t.
On the other hand, for a given a, in the APHYNITY framework, the residual must be equal to:
dθ
Fra : X → (0,(a — 2)θ — 2击)
in order to satisfy the fitting constraint. Here a corresponds to 1 + ω02 not to ω02 as in the simplified
case. Minimizing its norm, we obtain a = 2 which gives us the desired solution:
θt = θ0e-t cost
with the right period.
D Discussion on supervision over derivatives
In order to find the appropriate decomposition (Fp, Fa), we use a trajectory-based error by solving:
minimize
Fp ∈ Fp , Fa ∈ F
kFa k
subject to
dXeg
∀g ∈ I,Xg = g and∀t, -dtt- = (FP + Fa)(Xg),
∀X ∈ D, L(X,XeX0) = 0
(8)
In the continuous setting where the data is available at all times t, this problem is in fact equivalent to
the following one:
minimize
Fp ∈ Fp
EX〜D	Il --t	Fp(Xt)
(9)
where the supervision is done directly over derivatives, obtained through finite-difference schemes.
This echoes the proof in Section B of the Appendix where F can be reconstructed from the continuous
data.
However, in practice, data is only available at discrete times with a certain time resolution. While
Eq. (9) is indeed equivalent to Eq. (8) in the continuous setting, in the practical discrete one, the way
error propagates is not anymore: For Eq. (8) it is controlled over integrated trajectories while for
Eq. (9) the supervision is over the approximate derivatives of the trajectories from the dataset. We
argue that the trajectory-based approach is more flexible and more robust for the following reasons:
•	In Eq. (8), if Fa is appropriately parameterized, it is possible to perfectly fit the data
trajectories at the sampled points.
•	The use of finite differences schemes to estimate F as is done in Eq. (9) necessarily induces
a non-zero discretization error.
•	This discretization error is explosive in terms of divergence from the true trajectories.
This last point is quite important, especially when time sampling is sparse (even though we do observe
this adverse effect empirically in our experiments with relatively finely time-sampled trajectories).
The following gives a heuristical reasoning as to why this is the case. Let F = F + be the
function estimated from the sampled points with an error such that kk∞ ≤ α. Denoting X the
corresponding trajectory generated by F , we then have, for all X ∈ D:
∀t, -(Xd-X)t = F(Xt) — F(Xt) — e(X.
15
Published as a conference paper at ICLR 2021
Integrating over [0, T] and using the triangular inequality as well as the mean value inequality,
supposing that F has uniformly bounded spatial derivatives:
Vt ∈ [0,T], k(X - X)tk ≤ kVFk∞
Z kXs - Xes k
0
+ αt
which, using a variant of the Gronwall lemma, gives Us the inequality:
α
∀t ∈ [0,T],kXt-Xtk≤wc(exp(kVF k∞t)- 1)
When α tends to 0, we recover the true trajectories X . However, as α is bounded away from 0 by
the available temporal resolution, this inequality gives a rough estimate of the way X diverges from
them, and it can be an equality in many cases. This exponential behaviour explains our choice of a
trajectory-based optimization.
E Implementation details
We describe here the three use cases studied in the paper for validating APHYNITY. All experiments
are implemented with PyTorch (Paszke et al., 2019) and the differentiable ODE solvers with the
adjoint method implemented in torchdiffeq.5
E.1 Reaction-diffusion equations
The system is driven by a FitzHugh-Nagumo type PDE (Klaasen & Troy, 1984)
∂u	∂v
=a = ɑ∆u + Ru(u, v; k), — = b∆v + Rv (u, V)
where a and b are respectively the diffusion coefficients of u and v, ∆ is the Laplace operator. The
local reaction terms are Ru(u, v; k) = u - u3 - k - v, Rv (u, v) = u - v.
The state X = (u, V) is defined over a compact rectangular domain Ω = [-1, 1]2 with periodic
boundary conditions. Ω is spatially discretized with a 32 X 32 2D uniform square mesh grid.
The periodic boundary condition is implemented with circular padding around the borders. ∆ is
systematically estimated with a 3 × 3 discrete Laplace operator.
Dataset Starting from a randomly sampled initial state Xinit ∈ [0, 1]2×32×32, we generate states by
integrating the true PDE with fixed a, b, and k in a dataset (a = 1×10-3, b = 5×10-3, k = 5×10-3).
We firstly simulate high time-resolution (δtsim = 0.001) sequences with explicit finite difference
method. We then extract states every δtdata = 0.1 to construct our low time-resolution datasets.
We set the time of random initial state to t = -0.5 and the time horizon to t = 2.5. 1920 sequences
are generated, with 1600 for training/validation and 320 for test. We take the state at t = 0 as X0
and predict the sequence until the horizon (equivalent to 25 time steps) in all reaction-diffusion
experiments. Note that the sub-sequence with t < 0 are reserved for the extensive experiments in
Appendix G.1.
Neural network architectures Our Fa here is a 3-layer convolution network (ConvNet). The two
input channels are (u, V) and two output ones are (察,∂v)∙ The PUreIy data-driven Neural ODE uses
such ConvNet as its F. The detailed architecture is provided in Table 2. The estimated physical
parameters θp in Fp are simply a trainable vector (a, b) ∈ R2+ or (a, b, k) ∈ R3+.
5https://github.com/rtqichen/torchdiffeq
16
Published as a conference paper at ICLR 2021
Table 2: ConvNet architecture in reaction-diffusion and wave equation experiments, used as data-
driven derivative operator in APHYNITY and Neural ODE (Chen et al., 2018).
Module	Specification
2D Conv. 2D Batch Norm. ReLU activation 2D Conv. 2D Batch Norm. ReLU activation 2D Conv.	3 × 3 kernel, 2 input channels, 16 output channels, 1 pixel zero padding No average tracking — 3 × 3 kernel, 16 input channels, 16 output channels, 1 pixel zero padding No average tracking — 3 × 3 kernel, 16 input channels, 2 output channels, 1 pixel zero padding
Optimization hyperparameters We choose to apply the same hyperparameters for all the reaction-
diffusion experiments: Niter = 1, λ0 = 1, τ1 = 1 × 10-3, τ2 = 1 × 103.
E.2 Wave equations
The damped wave equation is defined by
∂2w	2	∂w
/ TZ + kit
0
where C is the wave speed and k is the damping coefficient. The state is X = (w,舞).
We consider a compact spatial domain Ω represented as a 64 X 64 grid and discretize the Laplacian
operator similarly. ∆ is implemented using a 5 × 5 discrete Laplace operator in simulation whereas
in the experiment is a 3 × 3 Laplace operator. Null Neumann boundary condition are imposed for
generation.
Dataset δt was set to 0.001 to respect Courant number and provide stable integration. The simu-
lation was integrated using a 4th order finite difference Runge-Kutta scheme for 300 steps from an
initial Gaussian state, i.e for all sequence at t = 0, we have:
w(x, y, t = 0) = C × exp
(X-XO)2 + (y-yO)2
(10)
σ^
The amplitude C is fixed to 1, and (x0, y0) = (32, 32) to make the Gaussian curve centered for all
sequences. However, σ is different for each sequence and uniformly sampled in [10, 100]. The same
δt was used for train and test. All initial conditions are Gaussian with varying amplitudes. 250
sequences are generated, 200 are used for training while 50 are reserved as a test set. In the main
paper setting, c = 330 and k = 50. As with the reaction diffusion case, the algorithm takes as input a
state Xto = (w, ddt )(to) and predicts all states from to + δt UP to to + 25δt.
Neural network architectures The neural network for Fa is a 3-layer convolution neural network
with the same architectUre as in Table 2. For Fp , the parameter(s) to be estimated is either a scalar
c ∈ R+ or a vector (c, k) ∈ R2+ . Similarly, NeUral ODE networks are bUild as presented in Table 2.
Optimization hyperparameters We Use the same hyperparameters for the experiments:
N iter = 3, λo = 1, τ1 = 1 × 10-4, τ2= 1 × 102.
E.3 Damped pendulum
We consider the non-linear damped pendUlUm problem, governed by the ODE
d2θ	2	dθ
涯 + ω0 Sm θ + α dt
0
where θ(t) is the angle, ω0 = Tπ is the proper pulsation (To being the period) and α is the damping
coefficient. With the state X = (θ,等),the ODE can be written as dXt = F(Xt) with F : X →
(dθt, -ω0Sin θ - α dθ).
17
Published as a conference paper at ICLR 2021
Dataset For each train / validation / test split, we simulate a dataset with 25 trajectories of 40
timesteps (time interval [0, 20], timestep δt = 0.5) with fixed ODE coefficients (T0 = 12, α = 0.2)
and varying initial conditions. The simulation integrator is Dormand-Prince Runge-Kutta method
of order (4)5 (DOPRI5, Dormand & Prince, 1980). We also add a small amount of white gaussian
noise (σ = 0.01) to the state. Note that our pendulum dataset is much more challenging than the
ideal frictionless pendulum considered in Greydanus et al. (2019).
Neural network architectures We detail in Table 3 the neural architectures used for the damped
pendulum experiments. All data-driven augmentations for approximating the mapping Xt 7→ F (Xt)
are implemented by multi-layer perceptrons (MLP) with 3 layers of 200 neurons and ReLU activation
functions (except at the last layer: linear activation). The Hamiltonian (Greydanus et al., 2019; Toth
et al., 2020) is implemented by a MLP that takes the state Xt and outputs a scalar estimation of the
Hamiltonian H of the system: the derivative is then computed by an in-graph gradient of H with
respect to the input: F (Xt) =
∂H
∂(dθ/ dt),
Table 3: Neural network architectures for the damped pendulum experiments. n/a corresponds to
non-applicable cases.
Method	Physical model	Data-driven model
Neural ODE	n/a	MLP(in=2, units=200, layers=3, out=2)
Hamiltonian APHYNITY Hamiltonian	MLP(in=2, Units=200,layers=3, out=1) MLP(in=2, units=200, layers=3, out=1)	n/a MLP(in=2, units=200, layers=3, out=2)
Param ODE (ω0) APHYNITY Param ODE (ω0)	1 trainable parameter ω0 1 trainable parameter ω0	n/a MLP(in=2, units=200, layers=3, out=2)
Param ODE (ω0 , α) APHYNITY Param ODE (ω0 , α)	2 trainable parameters ω0 , λ 2 trainable parameters ω0 , λ	n/a MLP(in=2, units=200, layers=3, out=2)
Optimization hyperparameters The hyperparameters of the APHYNITY optimization algorithm
(N iter, λ0, τ1, τ2) were cross-validated on the validation set and are shown in Table 4. All models
were trained with a maximum number of 5000 steps with early stopping.
Table 4: Hyperparameters of the damped pendulum experiments.
Method	Niter	λ0	τ1	τ2
APHYNITY Hamiltonian	5	1	1	0.1
APHYNITY ParamODE (ω0)	5	1	1	10
APHYNITY ParamODE (ω0 , λ)	5	1000	1	100
F Ablation study
We conduct ablation studies to show the effectiveness of APHYNITY’s adaptive optimization and
trajectory-based learning scheme.
F.1 Ablation to vanilla MB/ML cooperation
In Table 5, we consider the ablation case with the vanilla augmentation scheme found in Le Guen
& Thome (2020); Wang et al. (2019); Mehta et al. (2020), which does not present any proper
decomposition guarantee. We observe that the APHYNITY cooperation scheme outperforms this
vanilla scheme in all case, both in terms of forecasting performances (e.g. log MSE= -0.35 vs. -3.97
for the Hamiltonian in the pendulum case) and parameter identification (e.g. Err Param=8.4% vs.
2.3 for Param PDE (a, b for reaction-diffusion). It confirms the crucial benefits of APHYNITY’s
principled decomposition scheme.
18
Published as a conference paper at ICLR 2021
Table 5: Ablation study comparing APHYNITY to the vanilla augmentation scheme (Wang et al.,
2019; Mehta et al., 2020) for the reaction-diffusion equation, wave equation and damped pendulum.
Dataset	Method	log MSE	%Err Param.	IFaI2
	Param. PDE (a, b) with vanilla aug.	-4.56±0.52	8.4	(7.5±1.4)e1
	APHYNITY Param. PDE (a, b)	-5.10±0.21	2.3	(6.7±0.4)e1
Reaction- diffusion	Param. PDE (a, b, k) with vanilla aug.	-8.04±0.03	25.4	(1.5±0.2)e-2
	APHYNITY Param. PDE (a, b, k)	-9.35±0.02	0.096	(1.5±0.4)e-6
	True PDE with vanilla aug.	-8.12±0.05	n/a	(6.1±2.3)e-4
	APHYNITY True PDE	-9.17±0.02	n/a	(1.4±0.8)e-7
	Param PDE (c) with vanilla aug.	-3.90 ± 0.27	0.51	88.66
Wave	APHYNITY Param PDE (c)	-4.64±0.25	0.31	71.0
equation	Param PDE (c, k) with vanilla aug.	-5.96 ± 0.10	0.71	25.1
	APHYNITY Param PDE (c, k)	-6.09±0.28	0.70	4.54
	Hamiltonian with vanilla aug.	-0.35±0.1	n/a	837±117
	APHYNITY Hamiltonian	-3.97±1.2	n/a	623±68
	Param ODE (ω0) with vanilla aug.	-7.02±1.7	4.5	148±49
Damped pendulum	APHYNITY Param ODE (ω0)	-7.86±0.6	4.0	132±11
	Param ODE (ω0, α) with vanilla aug.	-7.60±0.6	4.65	35.5±6.2
	APHYNITY Param ODE (ω0 , α)	-8.31±0.3	0.39	8.5±2.0
	Augmented True ODE with vanilla aug.	-8.40±0.2	n/a	3.4±0.8
	APHYNITY True ODE	-8.44±0.2	n/a	2.3±0.4
F.2 Detailed ablation study
We conduct also two other ablations in Table 6:
•	derivative supervision: in which Fp + Fa is trained with supervision over approximated
derivatives on ground truth trajectory, as performed in Greydanus et al. (2019); Cranmer et al.
(2020). More precisely, APHYNITY's Ltraj is here replaced with Lderiv = k 等一F(Xt)Il
as in Eq.(9), where ddtt is approximated by finite differences on Xt.
•	non-adaptive optim.: in which we train APHYNITY by minimizing IFa I without the
adaptive optimization of λ shown in Algorithm 1. This case is equivalent to λ = 1, τ2 = 0.
We highlight the importance to use a principled adaptive optimization algorithm (APHYNITY
algorithm described in paper) compared to a non-adpative optimization: for example in the reaction-
diffusion case, log MSE= -4.55 vs. -5.10 for Param PDE (a, b). Finally, when the supervision occurs
on the derivative, both forecasting and parameter identification results are systematically lower than
with APHYNITY’s trajectory based approach: for example, log MSE=-1.16 vs. -4.64 for Param PDE
(c) in the wave equation. It confirms the good properties of the APHYNITY training scheme.
19
Published as a conference paper at ICLR 2021
Table 6: Detailed ablation study on supervision and optimization for the reaction-diffusion equation,
wave equation and damped pendulum.
Dataset	Method	log MSE	%Err Param.	kFak2
	Augmented Param. PDE (a, b) derivative supervision	-4.42±0.25	12.6	(6.8±0.6)e1
	Augmented Param. PDE (a, b) non-adaptive optim.	-4.55±0.11	7.5	(7.6±1.0)e1
	APHYNITY Param. PDE (a, b)	-5.10±0.21	2.3	(6.7±0.4)e1
Reaction-	Augmented Param. PDE (a, b, k) derivative supervision	-4.90±0.06	11.7	(1.9±0.3)e-1
diffusion	Augmented Param. PDE (a, b, k) non-adaptive optim.	-9.10±0.02	0.21	(5.5±2.9)e-7
	APHYNITY Param. PDE (a, b, k)	-9.35±0.02	0.096	(1.5±0.4)e-6
	Augmented True PDE derivative supervision	-6.03±0.01	n/a	(3.1±0.8)e-3
	Augmented True PDE non-adaptive optim.	-9.01±0.01	n/a	(1.5±0.8)e-6
	APHYNITY True PDE	-9.17±0.02	n/a	(1.4±0.8)e-7
	Augmented Param PDE (c) derivative supervision	-1.16±0.48	12.1	0.00024
	Augmented Param PDE (c) non-adaptive optim.	-2.57±0.21	3.1	43.6
	APHYNITY Param PDE (c)	-4.64±0.25	0.31	71.0
Wave equation	Augmented Param PDE (c, k) derivative supervision	-4.19±0.36	7.2	0.00012
	Augmented Param PDE (c, k) non-adaptive optim.	-4.93±0.51	1.32	0.054
	APHYNITY Param PDE (c, k)	-6.09±0.28	0.70	4.54
	Augmented True PDE derivative supervision	-4.42 ± 0.33	n/a	6.02e-5
	Augmented True PDE non-adaptive optim.	-4.97±0.49	n/a	0.23
	APHYNITY True PDE	-5.24±0.45	n/a	0.14
	Augmented Hamiltonian derivative supervision	-0.83±0.3	n/a	642±121
	Augmented Hamiltonian non-adaptive optim.	-0.49±0.58	n/a	165±30
	APHYNITY Hamiltonian	-3.97±1.2	n/a	623±68
	Augmented Param ODE (ω0) derivative supervision	-1.02±0.04	5.8	136±13
	Augmented Param ODE (ω0) non-adaptive optim.	-4.30±1.3	4.4	90.4±27
Damped pendulum	APHYNITY Param ODE (ω0)	-7.86±0.6	4.0	132±11
	Augmented Param ODE (ω0 , α) derivative supervision	-2.61±0.2	5.0	3.2±1.7
	Augmented Param ODE (ω0 , α) non-adaptive optim.	-7.69±1.3	1.65	4.8±7.7
	APHYNITY Param ODE (ω0, α)	-8.31±0.3	0.39	8.5±2.0
	Augmented True ODE derivative supervision	-2.14±0.3	n/a	4.1±0.6
	Augmented True ODE non-adaptive optim.	-8.34±0.4	n/a	1.4±0.3
	APHYNITY True ODE	-8.44±0.2	n/a	2.3±0.4
G	Additional experiments
G.1 Reaction-diffusion systems with varying diffusion parameters
We conduct an extensive evaluation on a setting with varying diffusion parameters for reaction-
diffusion equations. The only varying parameters are diffusion coefficients, i.e. individual a and b for
each sequence. We randomly sample a ∈ [1 × 10-3, 2 × 10-3] and b ∈ [3 × 10-3, 7 × 10-3]. k is
still fixed to 5 × 10-3 across the dataset.
In order to estimate a and b for each sequence, we use here a ConvNet encoder E to estimate
parameters from 5 reserved frames (t < 0). The architecture of the encoder E is similar to the one
in Table 2 except that E takes 5 frames (10 channels) as input and E outputs a vector of estimated
(a, b) after applying a Sigmoid activation scaled by 1 X 10 2 (to avoid possible divergence). For the
baseline Neural ODE, we concatenate a and b to each sequence as two channels.
In Table 7, we observe that combining data-driven and physical components outperforms the pure
data-driven one. When applying APHYNITY to Param PDE (a, b), the prediction precision is
significantly improved (log MSE: -1.32 vs. -4.32) with a and b respectively reduced from 55.6% and
54.1% to 11.8% and 18.7%. For complete physics cases, the parameter estimations are also improved
for Param PDE (a, b, k) by reducing over 60% of the error of b (3.10 vs. 1.23) and 10% to 20% of the
errors of a and k (resp. 1.55/0.59 vs. 1.29/0.39).
The extensive results reflect the same conclusion as shown in the main article: APHYNITY improves
the prediction precision and parameter estimation. The same decreasing tendency of kFa k is also
confirmed.
20
Published as a conference paper at ICLR 2021
Table 7: Results of the dataset of reaction-diffusion with varying (a, b). k = 5 × 10-3 is shared
across the dataset.
	Method	log MSE	%Err a	%Err b	%Err k	kFak2
Data- driven	Neural ODE (Chen et al., 2018)	-3.61±0.07	n/a	n/a	n/a	n/a
Incomplete	Param PDE (a, b)	-1.32±0.02	55.6	54.1	n/a	n/a
physics	APHYNITY Param PDE (a, b)	-4.32±0.32	11.8	18.7	n/a	(4.3±0.6)e1
	Param PDE (a, b, k)	-5.54±0.38	1.55	3.10	0.59	n/a
Complete	APHYNITY Param PDE (a, b, k)	-5.72±0.25	1.29	1.23	0.39	(5.9±4.3)e-1
physics	TruePDE		-8.86±0.02	n/a	n/a	n/a	n/a
	APHYNITY True PDE	-8.82±0.15	n/a	n/a	n/a	(1.8±0.6)e-5
G.2 Additional results for the wave equation
We conduct an experiment where each sequence is generated with a different wave celerity. This
dataset is challenging because both c and the initial conditions vary across the sequences. For each
simulated sequence, an initial condition is sampled as described previously, along with a wave
celerity c also sampled uniformly in [300, 400]. Finally our initial state is integrated with the same
Runge-Kutta scheme. 200 of such sequences are generated for training while 50 are kept for testing.
For this experiment, we also use a ConvNet encoder to estimate the wave speed c from 5 consecutive
reserved states (w,端).The architecture of the encoder E is the same as in Table 2 but with 10 input
channels. Here also, k is fixed for all sequences and k = 50. The hyper-parameters used in these
experiments are the same than described in the Section E.2.
The results when multiple wave speeds c are in the dataset are consistent with the one present when
only one is considered. Indeed, while prediction performances are slightly hindered, the parameter
estimation remains consistent for both c and k. This extension provides elements attesting for the
robustness and adaptability of our method to more complex settings. Finally the purely data-driven
Neural-ODE fails to cope with the increasing difficulty.
Table 8: Results for the damped wave equation when considering multiple c sampled uniformly in
[300, 400] in the dataset, k is shared across all sequences and k = 50.
	Method	log MSE	%Error c	%Error k	kFak2
Data- driven	Neural ODE	0.056±0.34	n/a	n/a	n/a
Incomplete	Param PDE (c)	-1.32±0.27	23.9	n/a	n/a
physics	APHYNITY Param PDE (c)	-4.51±0.38	3.2	n/a	171
	Param PDE (c, k)	-4.25±0.28	3.54	1.43	n/a
Complete	APHYNITY Param PDE (c, k)	-4.84±0.57	2.41	0.064	3.64
physics	…TruePDE"(C「k)………一……	-4.51±O；29	n/a	n/a	n/a
	APHYNITY True PDE (c, k)	-4.49±0.22	n/a	n/a	0.0005
G.3 Damped pendulum with varying parameters
To extend the experiments conducted in the paper (section 4) with fixed parameters (T0 = 6, α = 0.2)
and varying initial conditions, we evaluate APHYNITY on a much more challenging dataset where
we vary both the parameters (T0 , α) and the initial conditions between trajectories.
We simulate 500/50/50 trajectories for the train/valid/test sets integrated with DOPRI5. For each
trajectory, the period T0 (resp. the damping coefficient α) are sampled uniformly in the range [3, 10]
(resp. [0, 0.5]).
We train models that take the first 20 steps as input and predict the next 20 steps. To account for the
varying ODE parameters between sequences, we use an encoder that estimates the parameters based
21
Published as a conference paper at ICLR 2021
on the first 20 timesteps. In practice, we use a recurrent encoder composed of 1 layer of 128 GRU
units. The output of the encoder is fed as additional input to the data-driven augmentation models
and to an MLP with final softplus activations to estimate the physical parameters when necessary
(ω0 ∈ R+ for Param ODE (ω0), (ω0, α) ∈ R2+ for Param ODE (ω0, α)).
In this varying ODE context, we also compare to the state-of-the-art univariate time series forecasting
method N-Beats (Oreshkin et al., 2020).
Results shown in Table 9 are consistent with those presented in the paper. Pure data-driven models
Neural ODE (Chen et al., 2018) and N-Beats (Oreshkin et al., 2020) fail to properly extrapolate
the pendulum dynamics. Incomplete physical models (Hamiltonian and ParamODE (ω0)) are even
worse since they do not account for friction. Augmenting them with APHYNITY significantly and
consistently improves forecasting results and parameter identification.
Table 9: Forecasting and identification results on the damped pendulum dataset with different
parameters for each sequence. log MSEs are computed over 20 predicted time-steps. For each level
of incorporated physical knowledge, equivalent best results according to a Student t-test are shown in
bold. n/a corresponds to non-applicable cases.
Method		log MSE	%Error T0	%Error α	kFak2
data-	Neural ODE (Chen et al., 2018)	-4.35±0.9	n/a	n/a	n/a
driven	N-Beats (Oreshkin et al., 2020)	-4.57±0.5	n/a	n/a	n/a
	Hamiltonian (Greydanus et al., 2019)	-1.31±0.4	n/a	n/a	n/a
Incomplete	APHYNITY Hamiltonian	-4.72±0.4	n/a	n/a	5.6±0.6
physics	Param ODE (ω0)	…-2.66±0.9…	"'2T.5±19'"	n/a	n/a
	APHYNITY Param ODE (ω0)	-5.94±0.7	5.0±1.8	n/a	0.49±0.1
	Param ODE (ω0 , α)	-5.71±0.4	4.08±0.8	152±129	n/a
Complete	APHYNITY Param ODE (ωo, α)	622±0.7	3.26±0.6	62±27	(5.39±0.1)e-10
physics	True ODE	-8.58±0.1	n/a	n/a	n/a
	APHYNITY True ODE	-8.58±0.1	n/a	n/a	(2.15±1.6)e-4
22