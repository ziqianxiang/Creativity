Published as a conference paper at ICLR 2021
Score-Based Generative Modeling through
Stochastic Differential Equations
Yang Song*	Jascha Sohl-Dickstein	Diederik P. Kingma
Stanford University	Google Brain	Google Brain
yangsong@cs.stanford.edu	jaschasd@google.com	durk@google.com
Abhishek Kumar	Stefano Ermon	Ben Poole
Google Brain	Stanford University	Google Brain
abhishk@google.com	ermon@cs.stanford.edu	pooleb@google.com
Ab stract
Creating noise from data is easy; creating data from noise is generative modeling.
We present a stochastic differential equation (SDE) that smoothly transforms a com-
plex data distribution to a known prior distribution by slowly injecting noise, and a
corresponding reverse-time SDE that transforms the prior distribution back into the
data distribution by slowly removing the noise. Crucially, the reverse-time SDE
depends only on the time-dependent gradient field (a.k.a., score) of the perturbed
data distribution. By leveraging advances in score-based generative modeling, we
can accurately estimate these scores with neural networks, and use numerical SDE
solvers to generate samples. We show that this framework encapsulates previous
approaches in score-based generative modeling and diffusion probabilistic mod-
eling, allowing for new sampling procedures and new modeling capabilities. In
particular, we introduce a predictor-corrector framework to correct errors in the
evolution of the discretized reverse-time SDE. We also derive an equivalent neural
ODE that samples from the same distribution as the SDE, but additionally enables
exact likelihood computation, and improved sampling efficiency. In addition, we
provide a new way to solve inverse problems with score-based models, as demon-
strated with experiments on class-conditional generation, image inpainting, and
colorization. Combined with multiple architectural improvements, we achieve
record-breaking performance for unconditional image generation on CIFAR-10
with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99
bits/dim, and demonstrate high fidelity generation of 1024 X 1024 images for the
first time from a score-based generative model.
1 Introduction
Two successful classes of probabilistic generative models involve sequentially corrupting training
data with slowly increasing noise, and then learning to reverse this corruption in order to form a
generative model of the data. Score matching with Langevin dynamics (SMLD) (Song & Ermon,
2019) estimates the score (i.e., the gradient of the log probability density with respect to data) at each
noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales
during generation. Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015;
Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption,
using knowledge of the functional form of the reverse distributions to make training tractable. For
continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale.
We therefore refer to these two model classes together as score-based generative models.
Score-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du &
Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho
et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai
* Work partially done during an internship at Google Brain.
1
Published as a conference paper at ICLR 2021
Forward SDE (data T noise)
dx = f(x, t)dt + g(2)dw
><χ(T)
score function
x(θ)y^÷ dx= [f(x^) - 52(^χ IogPt(x)] d⅛ + 5Wdw
Reverse SDE (noise T data)
Figure 1: Solving a reverse-
time SDE yields a score-based
generative model. Transform-
ing data to a simple noise dis-
tribution can be accomplished
with a continuous-time SDE.
This SDE can be reversed if We
know the score of the distribu-
tion at each intermediate time
step, Vx log Ptpx).
et al., 2020). To enable new sampling methods and further extend the capabilities of score-based
generative models, we propose a unified framework that generalizes previous approaches through the
lens of stochastic differential equations (SDEs).
Specifically, instead of perturbing data with a finite number of noise distributions, we consider a
continuum of distributions that evolve over time according to a diffusion process. This process
progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not
depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold
random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time
SDE (Anderson, 1982), which can be derived from the forward SDE given the score of the marginal
probability densities as a function of time. We can therefore approximate the reverse-time SDE by
training a time-dependent neural network to estimate the scores, and then produce samples using
numerical SDE solvers. Our key idea is summarized in Fig. 1.
Our proposed framework has several theoretical and practical contributions:
Flexible sampling and likelihood computation: We can employ any general-purpose SDE solver
to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not
viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers
with score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981) and HMC (Neal et al.,
2011); and (ii) deterministic samplers based on the probability flow ordinary differential equation
(ODE). The former unifies and improves over existing sampling methods for score-based models.
The latter allows for fast adaptive sampling via black-box ODE solvers, flexible data manipulation
via latent codes, a uniquely identifiable encoding, and notably, exact likelihood computation.
Controllable generation: We can modulate the generation process by conditioning on information
not available during training, because the conditional reverse-time SDE can be efficiently estimated
from unconditional scores. This enables applications such as class-conditional generation, image
inpainting, colorization and other inverse problems, all achievable using a single unconditional
score-based model without re-training.
Unified framework: Our framework provides a unified way to explore and tune various SDEs for
improving score-based generative models. The methods of SMLD and DDPM can be amalgamated
into our framework as discretizations of two separate SDEs. Although DDPM (Ho et al., 2020) was
recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show
that with better architectures and new sampling algorithms allowed by our framework, the latter can
catch up—it achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10,
as well as high-fidelity generation of 1024 X 1024 images for the first time from a score-based model.
In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99
bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.
2 Background
2.1 Denoising score matching with Langevin dynamics (SMLD)
Let Pσ pX | x) ：“ N pX; x,σ2 Iq be a perturbation kernel, and Pσ px) ：“ ,Pdata px)Pσ px | x)dx, where
Pdatapxq denotes the data distribution. Consider a sequence of positive noise scales σm⅜n = σι V
σ V …V (JN = σmax. Typically, σmin is small enough such that Pσmin px) « Pdatapx), and σmax is
2
Published as a conference paper at ICLR 2021
large enough such that pσmax pxq « Npx; 0, σm2 ax Iq. Song & Ermon (2019) propose to train a Noise
Conditional Score Network (NCSN), denoted by sθpx, σq, with a weighted sum of denoising score
matching (Vincent, 2011) objectives:
N
θ* “ argmin £ £/4.(*)/式 刃 χ)[ksθ px,σiq ´ Vx log 1%5 । xq k 2 ‰.	⑴
θ	i“1
Given sufficient data and model capacity, the optimal score-based model Se* (x, σq matches
Vx logpσ Pxq almost everywhere for σ P {。力仪广 For sampling, Song & Ermon (2019) run M steps
of Langevin MCMC to get a sample for each pσi pxq sequentially:
Xm = XmT + QSθ* (χ∕T,σi、'声湾,m = 1, 2,…，M,	⑵
where ∈i > 0 is the step size, and Zm is standard normal. The above is repeated for i = N, N —
1,…，1 in turn with XN 〜N (x | 0, σm1 明1) and x0 = xm'i when i < N .As M → 8 and ei → 0
for all i, x1M becomes an exact sample from pσmin (xq « pdata(xq under some regularity conditions.
2.2 Denoising diffusion probabilistic models (DDPM)
Sohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales
0 < βι, β2, ∙∙∙ ,βN < 1. For each training data point X0 〜 Pdata(xq, a discrete Markov chain
{x0, xι,…,XNU is constructed such thatP(Xi | x-1) = N(Xi; ?1 — βix—i, βiIq, and therefore
Pai(Xi | X0q = N (Xi； ?aiX0, (1 — 0i)I), where α :“ "；=式1 — βj). Similar to SMLD, We can
denote the perturbed data distribution as Pai (X) := ’ Pdata (X)Pai (X | x)dx. The noise scales are pre-
scribed such that XN is approximately distributed according to N(0, Iq. A variational Markov chain
in the reverse direction is parameterized withpθ(x-ι∣Xi) = N(x"1； ?T17r(Xi + βise(xi, i)), βiI),
1 ´βi
and trained with a re-weighted variant of the evidence lower bound (ELBO):
N
θ* = argmin £ (1 ´ αi )Epdata(x)EPai (x|x) [% (X,i) ´ Vx log Pai(X 1 X)k2s.	⑶
θ	i“1
After solving Eq. (3) to get the optimal model sθ* (X, i), samples can be generated by starting from
XN 〜 N(0, I) and following the estimated reverse Markov chain as below
Xi—1 = /1 1 a (Xi + βiSθ*(xi, i)) + aβiZi, i = N, N — 1,…，1.	(4)
1 — βi
We call this method ancestral sampling, since it amounts to performing ancestral sampling from
the graphical model ∏‰ pθ(xi—i | Xi). The objective Eq. (3) described here is LSimPIe in Ho et al.
(2020), written in a form to expose more similarity to Eq. (1). Like Eq. (1), Eq. (3) is also a weighted
sum of denoising score matching objectives, which implies that the optimal model, Se* (X, i), matches
the score of the perturbed data distribution, Vx log Pai (X). Notably, the weights of the i-th summand
in Eq. (1) and Eq. (3), namely σ2 and (1 — αj are related to corresponding perturbation kernels in the
same functional form: σ291/ErkVx logPσi(X | x)∣∣2] and (1 — 0i)91/ErkVx logPai(X | x)k2S.
3	Score-based generative modeling with SDEs
Perturbing data with multiple noise scales is key to the success of previous methods. We propose to
generalize this idea further to an infinite number of noise scales, such that perturbed data distributions
evolve according to an SDE as the noise intensifies. An overview of our framework is given in Fig. 2.
3.1	Perturbing data with SDEs
Our goal is to construct a diffusion process tX(t)utT“0 indexed by a continuous time variable t P r0, Ts,
such that X(0) 〜 P0, for which we have a dataset of i.i.d. samples, and X(T) 〜 PT, for which we
have a tractable form to generate samples efficiently. In other words, P0 is the data distribution and
Pt is the prior distribution. This diffusion process can be modeled as the solution to an Ito SDE:
dX = f(X, t)dt + g(t)dw,	(5)
3
Published as a conference paper at ICLR 2021
Data	Forward SDE	Prior	Reverse SDE	Data
--------------- do: = f(x,t)dt + g(t)dw ------------及@)--------- da: = [f(x, t) - 52(i)V1 logpf (a:)] dt + g(t)dw
Po3)-------------------------Pt{x) -----------------------A Pτ[x)-------------------------Pt{x) -----------------------A Po(N)
Figure 2: Overview of score-based generative modeling through SDEs. We Can map data to a
noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling
(SeCtion 3.2). We Can also reverse the assoCiated probability flow ODE (SeCtion 4.3), whiCh yields a
deterministiC proCess that samples from the same distribution as the SDE. Both the reverse-time SDE
and probability flow ODE can be obtained by estimating the score Vx logPt(X) (Section 3.3).
where W is the standard Wiener process (a.k.a., Brownian motion), f(∙,t) : Rd → Rd is a vector-
valued function called the drift coefficient of x(t), and g(∙) : R → R is a scalar function known as
the diffusion coefficient of X(t). For ease of presentation we assume the diffusion coefficient is a
scalar (instead of a d X d matrix) and does not depend on x, but our theory can be generalized to hold
in those cases (see Appendix A). The SDE has a unique strong solution as long as the coefficients
are globally Lipschitz in both state and time (0ksendal, 2003). We hereafter denote by Pt(X) the
probability density of x(t), and use pst(x(t) | x(s)) to denote the transition kernel from x(s) to x(t),
where 0 ≤ S < t ≤ T.
Typically, PT is an unstructured prior distribution that contains no information of P0 , such as a
Gaussian distribution with fixed mean and variance. There are various ways of designing the SDE in
Eq. (5) such that it diffuses the data distribution into a fixed prior distribution. We provide several
examples later in Section 3.4 that are derived from continuous generalizations of SMLD and DDPM.
3.2	Generating samples by reversing the SDE
By starting from samples of x(T) „ PT and reversing the process, we can obtain samples x(0) „ P0.
A remarkable result from Anderson (1982) states that the reverse of a diffusion process is also a
diffusion process, running backwards in time and given by the reverse-time SDE:
dx = f (x,t) — g(t)2Vx logPt(X)Sdt ' g(t)dW,	(6)
where W is a standard Wiener process when time flows backwards from T to 0, and dt is an
infinitesimal negative timestep. Once the score of each marginal distribution, Vx log Pt (x), is known
for all t, we can derive the reverse diffusion process from Eq. (6) and simulate it to sample from P0 .
3.3	Estimating scores for the SDE
The score of a distribution can be estimated by training a score-based model on samples with
score matching (Hyvarinen, 2005; Song et al., 2019a). To estimate VX log Pt(X), we can train a
time-dependent score-based model sθ(x, t) via a continuous generalization to Eqs. (1) and (3):
θ* “ argminEt{λ(t)Eχ(0)Eχ(t)∣χ(0)[ ∣∣sθ(x(t),t) ― VXpt) logPot(x(t) ∣ x(0))∣∣2 ‰).⑺
Here λ : r0, Ts → R>0 is a positive weighting function, t is uniformly sampled over [0,T],
x(0) „ P0(x) and x(t) „ P0t(x(t) | x(0)). With sufficient data and model capacity, score matching
ensures that the optimal solution to Eq. (7), denoted by sθ* (x, t), equals VX logPt(X) for almost all
x and t. As in SMLD and DDPM, we can typically choose λ91{E[ ∣∣Vχ(t) logP0t(x(t) | x(0))∣∣22 ‰.
Note that Eq. (7) uses denoising score matching, but other score matching objectives, such as sliced
4
Published as a conference paper at ICLR 2021
score matching (Song et al., 2019a) and finite-difference score matching (Pang et al., 2020) are also
applicable here.
We typically need to know the transition kernel p0t pxptq | xp0qq to efficiently solve Eq. (7). When
f (∙,t) is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are
often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in Sarkka
& Solin (2019)). For more general SDEs, we may solve Kolmogorov,s forward equation (0ksendal,
2003) to obtain p0tpxptq | xp0qq. Alternatively, we can simulate the SDE to sample from p0t pxptq |
xp0qq and replace denoising score matching in Eq. (7) with sliced score matching for model training,
which bypasses the computation of Vχptq logpot(x(t) | x(0)) (see Appendix A).
3.4	Examples: VE, VP SDEs and beyond
The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different
SDEs. Below we provide a brief discussion and relegate more details to Appendix B.
When using a total of N noise scales, each perturbation kernel pσi (x | x0q of SMLD corresponds to
the distribution ofxi in the following Markov chain:
Xi “ xi-1 '
Jσ2 ´ σ2τzi´l,
i = 1,…，N,
(8)
where zi´1 „ N (0, Iq, and we have introduced σ0 “ 0 to simplify the notation. In the limit of
N → 8, {σi}gι becomes a function σ(t), Zi becomes z(t), and the Markov chain {xi}i“\ becomes
a continuous stochastic process tx(tqut1“0, where we have used a continuous time variable t P r0, 1s
for indexing, rather than an integer i. The process tx(tq}t1“0 is given by the following SDE
dx “\ ∕dtσ≡dw.
dt
(9)
Likewise for the perturbation kernels tpαi (x | x0q}iN“1 of DDPM, the discrete Markov chain is
Xi = √1 ´ βiXi´1 ' √βiZ"i, i = 1,…，N.	(10)
As N → 8, Eq. (10) converges to the following SDE,
dx = ´}β(t)x dt ' aβ(t) dw.	(11)
Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs
Eqs. (9) and (11). Interestingly, the SDE of Eq. (9) always gives a process with exploding variance
when t → 8, whilst the SDE of Eq. (11) yields a process with a fixed variance of one when the initial
distribution has unit variance (proof in Appendix B). Due to this difference, we hereafter refer to
Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.
Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on
likelihoods (see Section 4.3), given by
dx = — ；β(t)x dt ' ʌ/β(t)(1 ´ e´2，t βpsqdsqdw.	(12)
When using the same β(tq and starting from the same initial distribution, the variance of the stochastic
process induced by Eq. (12) is always bounded by the VP SDE at every intermediate time step (proof
in Appendix B). For this reason, we name Eq. (12) the sub-VP SDE.
Since VE, VP and sub-VP SDEs all have affine drift coefficients, their perturbation kernels p0t(x(tq |
x(0qq are all Gaussian and can be computed in closed-forms, as discussed in Section 3.3. This makes
training with Eq. (7) particularly efficient.
4	Solving the reverse SDE
After training a time-dependent score-based model sθ, we can use it to construct the reverse-time
SDE and then simulate it with numerical approaches to generate samples from p0 .
5
Published as a conference paper at ICLR 2021
Table 1: Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained
with the same computation (number of score function evaluations). Mean and standard deviation
are reported over five sampling runs. “P1000” or “P2000”: predictor-only samplers using 1000 or
2000 steps. “C2000”: corrector-only samplers using 2000 steps. “PC1000”: Predictor-Corrector (PC)
samplers using 1000 predictor and 1000 corrector steps.
Variance Exploding SDE (SMLD)
Variance Preserving SDE (DDPM)
FID J ^∖Sampler PrediS6^^≥^^	P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
ancestral sampling	4.98 土 .06	4.88 土 .06		3.62 ± .03	3.24 ±.02	3.24 ± .02		3.21 ± .02
reverse diffusion	4.79 土 .07	4.74 土 .08	20.43 土 .07	3.60 ± .02	3.21 ± .02	3.19 ± .02	19.06 ± .06	3.18 ± .01
probability floW	15.41 土 .15	10.54 ± .08		3.51 土 .04	3.59 ± .04	3.23 ± .03		3.06 ± .03
4.1 General-purpose numerical SDE solvers
Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical
methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloe-
den & Platen, 2013), which correspond to different discretizations of the stochastic dynamics. We
can apply any of them to the reverse-time SDE for sample generation.
Ancestral sampling, the sampling method of DDPM (Eq. (4)), actually corresponds to one special
discretization of the reverse-time VP SDE (Eq. (11)) (see Appendix E). Deriving the ancestral
sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose reverse
diffusion samplers (details in Appendix E), which discretize the reverse-time SDE in the same way
as the forward one, and thus can be readily derived given the forward discretization. As shown in
Table 1, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and
DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models,
see Appendix F.)
4.2	Predictor-corrector samplers
Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we
have a score-based model Se* (x, t) « NX log pt(x), We can employ score-based MCMC approaches,
such as Langevin MCMC (Parisi, 1981; Grenander & Miller, 1994) or HMC (Neal et al., 2011) to
sample from pt directly, and correct the solution of a numerical SDE solver.
Specifically, at each time step, the numerical SDE solver first gives an estimate of the sample
at the next time step, playing the role of a “predictor”. Then, the score-based MCMC approach
corrects the marginal distribution of the estimated sample, playing the role of a “corrector”. The
idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for
solving systems of equations (AllgoWer & Georg, 2012), and We similarly name our hybrid sampling
algorithms Predictor-Corrector (PC) samplers. Please find pseudo-code and a complete description
in Appendix G. PC samplers generalize the original sampling methods of SMLD and DDPM: the
former uses an identity function as the predictor and annealed Langevin dynamics as the corrector,
While the latter uses ancestral sampling as the predictor and identity as the corrector.
We test PC samplers on SMLD and DDPM models (see Algorithms 2 and 3 in Appendix G) trained
With original discrete objectives given by Eqs. (1) and (3). This exhibits the compatibility of PC
samplers to score-based models trained With a fixed number of noise scales. We summarize the
performance of different samplers in Table 1, Where probability floW is a predictor to be discussed
in Section 4.3. Detailed experimental settings and additional results are given in Appendix G. We
observe that our reverse diffusion sampler alWays outperform ancestral sampling, and corrector-only
methods (C2000) perform Worse than other competitors (P2000, PC1000) With the same computation
(In fact, We need Way more corrector steps per noise scale, and thus more computation, to match the
performance of other samplers.) For all predictors, adding one corrector step for each predictor step
(PC1000) doubles computation but alWays improves sample quality (against P1000). Moreover, it
is typically better than doubling the number of predictor steps Without adding a corrector (P2000),
Where We have to interpolate betWeen noise scales in an ad hoc manner (detailed in Appendix G) for
SMLD/DDPM models. In Fig. 9 (Appendix G), We additionally provide qualitative comparison for
6
Published as a conference paper at ICLR 2021
Table 2: NLLs and FIDs (ODE) on CIFAR-10.			Table 3: CIFAR-10 sample quality.		
Model	NLL Test J	FID J	Model	FIDJ	IST
RealNVP (Dinh et al., 2016)	3.49	-	Conditional		
iResNet (Behrmann et al., 2019)	3.45	-	BigGAN (Brock et al., 2018)	14.73	9.22
Glow (Kingma & Dhariwal, 2018)	3.35	-	StyleGAN2-ADA (Karras et al., 2020a)	2.42	10.14
MintNet (Song et al., 2019b)	3.32	-	门—―			
Residual Flow (Chen et al., 2019)	3.28	46.37	Unconditional		
FFJORD (Grathwohl et al., 2018)	3.40	-	StyIeGAN2-ADA (Karras et al., 2020a)	2.92	9.83
Flow++ (Ho et al., 2019)	3.29	-	NCSN (Song & Ermon, 2019)	25.32	8.87 ± .12
DDPM (L) (Ho et al., 2020)	W 3.70*	13.51	NCSNv2 (Song & Ermon, 2020)	10.87	8.40 ± .07
DDPM (Lsimple) (Ho et al., 2020)	W 3.75*	3.17	DDPM (Ho et al., 2020)	3.17	9.46 ± .11
—			DDPM++	2.78	9.64
DDPM	3.28	3.37	DDPM++ cont. (VP)	2.55	9.58
DDPM cont. (VP)	3.21	3.69	DDPM++ cont. (sub-VP)	2.61	9.56
DDPM cont. (sub-VP)	3.05	3.56	DDPM++ cont. (deep, VP)	2.41	9.68
DDPM++ cont. (VP)	3.16	3.93	DDPM++ cont. (deep, sub-VP)	2.41	9.57
DDPM++ cont. (sub-VP)	3.02	3.16	NCSN++	2.45	9.73
DDPM++ cont. (deep, VP)	3.13	3.08	NCSN++ cont. (VE)	2.38	9.83
DDPM++ cont. (deep, sub-VP)	2.99	2.92	NCSN++ cont. (deep, VE)	2.20	9.89
models trained with the continuous objective Eq. (7) on 256 X 256 LSUN images and the VE SDE,
where PC samplers clearly surpass predictor-only samplers under comparable computation, when
using a proper number of corrector steps.
4.3	Probability flow and connection to neural ODEs
Score-based models enable another numerical method for solving the reverse-time SDE. For all
diffusion processes, there exists a corresponding deterministic process whose trajectories share the
same marginal probability densities tptpxqutT“0 as the SDE. This deterministic process satisfies an
ODE (more details in Appendix D.1):
dx “ If(x,t) — 2g(t)2Vxlogpt(x)]dt,	(13)
which can be determined from the SDE once scores are known. We name the ODE in Eq. (13) the
probability flow ODE. When the score function is approximated by the time-dependent score-based
model, which is typically a neural network, this is an example of a neural ODE (Chen et al., 2018).
Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the
density defined by Eq. (13) via the instantaneous change of variables formula (Chen et al., 2018).
This allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an
example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset
in Table 2. We compute log-likelihoods on uniformly dequantized data, and only compare to models
evaluated in the same way (omitting models evaluated with variational dequantization (Ho et al.,
2019) or discrete data), except for DDPM (L/Lsimple) whose ELBO values (annotated with *) are
reported on discrete data. Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain
better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we
trained another DDPM model with the continuous objective in Eq. (7) (i.e., DDPM cont.), which
further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared
to VP SDEs; (iv) With improved architecture (i.e., DDPM++ cont., details in Section 4.4) and the
sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even
without maximum likelihood training.
Manipulating latent representations By integrating Eq. (13), we can encode any datapoint x(0q
into a latent space x(T q. Decoding can be achieved by integrating a corresponding ODE for the
reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing
flows (Dinh et al., 2016; Kingma & Dhariwal, 2018), we can manipulate this latent representation for
image editing, such as interpolation, and temperature scaling (see Fig. 3 and Appendix D.4).
Uniquely identifiable encoding Unlike most current invertible models, our encoding is uniquely
identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy,
the encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020). This
is because our forward SDE, Eq. (5), has no trainable parameters, and its associated probability flow
7
Published as a conference paper at ICLR 2021
ODE Evaluation Points
Evaluation number
NFE=14 NFE=86 NFE=548
Interpolation
Figure 3: Probability flow ODE enables fast sampling with adaptive stepsizes as the numerical
precision is varied (left), and reduces the number of score function evaluations (NFE) without harming
quality (middle). The invertible mapping from latents to images allows for interpolations (right).
ODE, Eq. (13), provides the same trajectories given perfectly estimated scores. We provide additional
empirical verification on this property in Appendix D.5.
Efficient sampling As with neural ODEs, we can sample xp0q „ p0 by solving Eq. (13) from
different final conditions xpTq „ pT . Using a fixed discretization strategy we can generate com-
petitive samples, especially when used in conjuction with correctors (Table 1, “probability flow
sampler”, details in Appendix D.3). Using a black-box ODE solver (Dormand & Prince, 1980) not
only produces high quality samples (Table 2, details in Appendix D.4), but also allows us to explicitly
trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations
can be reduced by over 90% without affecting the visual quality of samples (Fig. 3).
4.4	Architecture improvements
We explore several new architecture designs for score-based models using both VE and VP SDEs
(details in Appendix H), where we train models with the same discrete objectives as in SMLD/DDPM.
We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our
optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with
PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.
By switching to the continuous training objective in Eq. (7), and increasing the network depth, we can
further improve sample quality for all models. The resulting architectures are denoted as NCSN++
cont. and DDPM++ cont. in Table 3 for VE and VP/sub-VP SDEs respectively. Results reported in
Table 3 are for the checkpoint with the smallest FID over the course of training, where samples are
generated with PC samplers. In contrast, FID scores and NLL values in Table 2 are reported for the
last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in Table 3,
VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically
observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that
practitioners likely need to experiment with different SDEs for varying domains and architectures.
Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets
new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly,
we can achieve better FID than the previous best conditional generative model without requiring
labeled data. With all improvements together, we also obtain the first set of high-fidelity samples
on CelebA-HQ 1024 ^ 1024 from score-based models (See Appendix H.3). Our best model for
likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a
log-likelihood of 2.99 bits/dim with the continuous objective in Eq. (7). To our best knowledge, this
is the highest likelihood on uniformly dequantized CIFAR-10.
5	Controllable generation
The continuous structure of our framework allows us to not only produce data samples from p0 , but
also from p0pxp0q | yq if ptpy | xptqq is known. Given a forward SDE as in Eq. (5), we can sample
8
Published as a conference paper at ICLR 2021
Figure 4: Left: Class-conditional samples on 32 X 32 CIFAR-10. Top four rows are automobiles and
bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows)
results on 256 X 256 LSUN. First column is the original image, second column is the masked/gray-
scale image, remaining columns are sampled image completions or colorizations.
from ptpxptq | yq by starting from pT pxpT q | yq and solving a conditional reverse-time SDE:
dx = {f(x,t) ´ g(t)2rVχ logPt(x) ' Vχ logPtPy | x)]}dt ' g(t)dw.	(14)
In general, we can use Eq. (14) to solve a large family of inverse problems with score-based generative
models, once given an estimate of the gradient of the forward process, Vx log Pt py | xpt)). In some
cases, it is possible to train a separate model to learn the forward process log Pt py | xpt)) and
compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge.
In Appendix I.4, we provide a broadly applicable method for obtaining such an estimate without the
need of training auxiliary models.
We consider three applications of controllable generation with this approach: class-conditional
generation, image imputation and colorization. When y represents class labels, we can train a
time-dependent classifier Pt py | xpt)) for class-conditional sampling. Since the forward SDE
is tractable, we can easily create training data pxpt), y) for the time-dependent classifier by first
sampling pxp0), y) from a dataset, and then sampling xpt) „ P0tpxpt) | xp0)). Afterwards, we
may employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the
time-dependent classifier Ptpy | xpt)). We provide class-conditional CIFAR-10 samples in Fig. 4
(left), and relegate more details and results to Appendix I.
Imputation is a special case of conditional sampling. Suppose we have an incomplete data point
y where only some subset, Q(y) is known. Imputation amounts to sampling from p(x(0) | Q(y)),
which we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special
case of imputation, except that the known data dimensions are coupled. We can decouple these data
dimensions with an orthogonal linear transformation, and perform imputation in the transformed
space (details in Appendix I.3). Fig. 4 (right) shows results for inpainting and colorization achieved
with unconditional time-dependent score-based models.
6	Conclusion
We presented a framework for score-based generative modeling based on SDEs. Our work enables a
better understanding of existing approaches, new sampling algorithms, exact likelihood computation,
uniquely identifiable encoding, latent code manipulation, and brings new conditional generation
abilities to the family of score-based generative models.
While our proposed sampling approaches improve results and enable more efficient sampling, they
remain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying
ways of combining the stable learning of score-based generative models with the fast sampling of
implicit models like GANs remains an important research direction. Additionally, the breadth of
samplers one can use when given access to score functions introduces a number of hyper-parameters.
Future work would benefit from improved methods to automatically select and tune these hyper-
parameters, as well as more extensive investigation on the merits and limitations of various samplers.
9
Published as a conference paper at ICLR 2021
Acknowledgements
We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and
Han Zhang for their insightful discussions during the course of this project. This research was
partially supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR
(FA95501910024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple
PhD Fellowship in AI/ML.
References
Eugene L Allgower and Kurt Georg. Numerical continuation methods: an introduction, volume 13.
Springer Science & Business Media, 2012.
Brian D O Anderson. Reverse-time diffusion equation models. Stochastic Process. Appl., 12(3):
313-326, May 1982.
Jens Behrmann, Will GrathWohL Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582,
2019.
Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through
infusion training. arXiv preprint arXiv:1703.06975, 2017.
AndreW Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations, 2018.
Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and
Bharath Hariharan. Learning gradient fields for shape generation. In Proceedings of the European
Conference on Computer Vision (ECCV), 2020.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad:
Estimating gradients for Waveform generation. arXiv preprint arXiv:2009.00713, 2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows
for invertible generative modeling. In Advances in Neural Information Processing Systems, pp.
9916-9926, 2019.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of
computational and applied mathematics, 6(1):19-26, 1980.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 32, pp. 3608-3618. Curran Associates, Inc.,
2019.
Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical Association,
106(496):1602-1614, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational
walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural
Information Processing Systems, pp. 4392-4402, 2017.
10
Published as a conference paper at ICLR 2021
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Confer-
ence on Learning Representations, 2018.
Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of
the Royal Statistical Society: Series B (Methodological), 56(4):549-581, 1994.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pp. 2722-2730, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33, 2020.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian
smoothing splines. Communications in Statistics-Simulation and Computation, 19(2):433-450,
1990.
AaPo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(Apr):695-709, 2005.
Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, Remi Tachet des Combes, and Ioannis Mitliagkas.
Adversarial score matching and imProved samPling for image generation. arXiv preprint
arXiv:2009.05475, 2020.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
imProved quality, stability, and variation. In International Conference on Learning Representations,
2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, PP. 4401-4410, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. Advances in Neural Information Processing
Systems, 33, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and imProving the image quality of StyleGAN. In Proc. CVPR, 2020b.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, PP. 10215-10224, 2018.
Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, vol-
ume 23. SPringer Science & Business Media, 2013.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiPle layers of features from tiny images. 2009.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. DeeP learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Dimitra Maoutsa, Sebastian Reich, and Manfred OPPer. Interacting Particle solutions of fokker-Planck
equations through gradient-log-density estimation. arXiv preprint arXiv:2006.00702, 2020.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permu-
tation invariant graPh generation via score-based generative modeling. volume 108 of Proceedings
of Machine Learning Research, PP. 4474-4484, Online, 26-28 Aug 2020. PMLR.
11
Published as a conference paper at ICLR 2021
Bemt 0ksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65-84.
Springer, 2003.
Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient learning of
generative models via finite-difference score matching. arXiv preprint arXiv:2007.03317, 2020.
Giorgio Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378-384,
1981.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. In Advances in Neural Information Processing Systems, pp. 14837-14847, 2019.
Geoffrey Roeder, Luke Metz, and Diederik P Kingma. On linear identifiability of learned representa-
tions. arXiv preprint arXiv:2007.00810, 2020.
Simo Sarkka and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge
University Press, 2019.
John Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian
Methods, pp. 455-466. Springer, 1989.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning,
pp. 2256-2265, 2015.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11895-11907, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in Neural Information Processing Systems, 33, 2020.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in
Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp. 204, 2019a.
Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with
masked convolutions. In Advances in Neural Information Processing Systems, pp. 11002-11012,
2019b.
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn
high frequency functions in low dimensional domains. NeurIPS, 2020.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661-1674, 2011.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Richard Zhang. Making convolutional networks shift-invariant again. In ICML, 2019.
12
Published as a conference paper at ICLR 2021
Appendix
We include several appendices with additional details, derivations, and results. Our framework
allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which
we provide a detailed discussion in Appendix A. We give a full derivation of VE, VP and sub-VP
SDEs in Appendix B, and discuss how to use them from a practitioner’s perspective in Appendix C.
We elaborate on the probability flow formulation of our framework in Appendix D, including a
derivation of the probability flow ODE (Appendix D.1), exact likelihood computation (Appendix D.2),
probability flow sampling with a fixed discretization strategy (Appendix D.3), sampling with black-
box ODE solvers (Appendix D.4), and experimental verification on uniquely identifiable encoding
(Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, the
DDPM-type ancestral sampler for SMLD models in Appendix F, and Predictor-Corrector samplers in
Appendix G. We explain our model architectures and detailed experimental settings in Appendix H,
with 1024 X 1024 CelebA-HQ samples therein. Finally, We detail on the algorithms for controllable
generation in Appendix I, and include extended results for class-conditional generation (Appendix I.1),
image inpainting (Appendix I.2), colorization (Appendix I.3), and a strategy for solving general
inverse problems (Appendix I.4).
A The framework for more general SDEs
In the main text, we introduced our framework based on a simplified SDE Eq. (5) where the diffusion
coefficient is independent of xptq. It turns out that our framework can be extended to hold for more
general diffusion coefficients. We can consider SDEs in the following form:
dx “ fpx, tqdt ` Gpx, tqdw,	(15)
where f(∙,t) : Rd → Rd and G(∙,t) : Rd → Rd'd. We follow the Ito interpretation of SDES
throughout this paper.
According to (Anderson, 1982), the reverse-time SDE is given by (cf ., Eq. (6))
dx = {f(x, tq ´ V ∙ [G(x, t)G(x,t)TS — G(x, t)G(x,t)TVχ logpt(x)}dt ' G(x,t)dw, (16)
where we define V ∙ F(X) :“ (V ∙ f1(x), V ∙ f2(x), ∙∙∙ , V ∙ fd(x))T for a matrix-valued function
F(X) :“ (f1(x), f2(x),…，fd(x))T throughout the paper.
The probability flow ODE corresponding to Eq. (15) has the following form (cf., Eq. (13), see a
detailed derivation in Appendix D.1):
dx = {f(x,t) — 2V ∙ [G(x,t)G(x,t)TS — 1 G(x,t)G(x,t)TVχ logPt(X“dt.	(17)
Finally for conditional generation with the general SDE Eq. (15), we can solve the conditional
reverse-time SDE below (cf., Eq. (14), details in Appendix I):
dx = tf (x, t) — V ∙ rG(x,t)G(x,t)Ts — G(x,t)G(x,t)TVxlogpt(x)
—G(x,t)G(x,t)T Vχ log pt (y | x)}dt ' G(x,t)dw. (18)
When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the
transition kernel p0t (x(t) | x(0)) in closed form. This hinders the training of score-based models,
because Eq. (7) requires knowing Vxptq logp0t(x(t) | x(0)). To overcome this difficulty, we can
replace denoising score matching in Eq. (7) with other efficient variants of score matching that do not
require computing Vxptq logp0t(x(t) | x(0)). For example, when using sliced score matching (Song
et al., 2019a), our training objective Eq. (7) becomes
θ* = argmin Et jλ(t)Eχ(o)Eχ(t)Ev〜Pv
2 ksθ (x(tq,tqk2 + vτsθ (x(t),tqv
(19)
,
where λ : [0, TS → R' is a positive weighting function, t 〜U(0, T), Ervs = 0, and Covrvs = I.
We can always simulate the SDE to sample from p0t(x(t) | x(0)), and solve Eq. (19) to train the
time-dependent score-based model sθ (x, t).
13
Published as a conference paper at ICLR 2021
B VE, VP AND SUB-VP SDES
Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM
are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively.
We additionally introduce sub-VP SDEs, a modification to VP SDEs that often achieves better
performance in both sample quality and likelihoods.
First, when using a total of N noise scales, each perturbation kernel pσi px | x0q of SMLD can be
derived from the following Markov chain:
Xi “ Xi-1 ' yσ2 - σ2τZJι, i = 1,…，N,
(20)
where Zi-ι „ Np0, I), x° „ pdata, and we have introduced σ0 “ 0 to simplify the notation. In the
limit of N → 8, the Markov chain tXiuiN“1 becomes a continuous stochastic process tXptqut1“0,
{σi}g] becomes a function σpt), and Zi becomes zpt), where we have used a continuous time variable
t P [0,1S for indexing, rather than an integer i p {1,2,…，N}. Let X (N) “ Xi, σ (六)“
and z (N) “ Zi for i “ 1, 2,…，N. We can rewrite Eq. (20) as follows with ∆t “ 焉;
t P {0, Nn,…，Nl(:
σi ,
and
Xpt ' ∆t) “ Xpt) ' aσιp + ∆t) ´ σ2pt) zpt) « Xpt) ' (d rjI(I)S ∆t zpt),
where the approximate equality holds when ∆t ! 1. In the limit of ∆t → 0, this converges to
dX =\W≡dw,
dt	,
(21)
which is the VE SDE.
For the perturbation kernels tpαi px | x0quiN“1 used in DDPM, the discrete Markov chain is
Xi “ √1 ´ βiXi-1 ' VβiZi-i, i = 1,…，N,
where Zi-1 „ Np0, Iq. To obtain the limit of this Markov chain when N →
auxiliary set of noise scales {βi = Nβi}3, and re-write Eq. (22) as below
(22)
8, we define an
Xi “ ≠ - N Xi-1 + G Zi-1, i = 1,…,N.
(23)
In the limit of N → 8, {βi}=ι becomes a function β(t) indexed by t P [0,1]. Let β (N) “ βi,
x(Nq “ Xi, ZpNq “ z%. We Can rewrite the Markov chain Eq.(23) as the following with ∆t “ N
and t P{0,1,…，NN1 u:
xpt ` ∆tq = a` ´ βpt + ∆t)∆t xptq ` aβpt ` ∆tq∆t zptq
«Xpt)—gβpt ` ∆t)∆t Xpt) ` a β pt ` ∆t)∆t Zpt)
«Xpt)—2 βpt)∆t Xpt) ` aβpt)∆t z pt),
where the approximate equality holds when ∆t ! 1. Therefore, in the limit of ∆t →
converges to the following VP SDE:
dX “ — ^βpt)X dt ' aβpt) dw.
(24)
0, Eq. (24)
(25)
So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to
discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding
variance when t → 8. In contrast, the VP SDE yields a process with bounded variance. In addition,
the process has a constant unit variance for all t P r0, 8) when ppXp0)) has a unit variance. Since the
VP SDE has affine drift and diffusion coefficients, we can use Eq. (5.5l) in Sarkka & Solin (2019) to
obtain an ODE that governs the evolution of variance
d∑!≡= βpt)pI — ∑vρpt)),
14
Published as a conference paper at ICLR 2021
where ΣVPptq :“ Covrxptqs for txptqut1“0 obeying a VP SDE. Solving this ODE, we obtain
∑vp(t) = I + e，0 -βpsqds(∑vp(0) ´ iq,	(26)
from which itis clear that the variance ΣVPptq is always bounded given ΣVPp0q. Moreover, ΣVPptq ”
I if ΣVp(0q “ I. Due to this difference, we name Eq. (9) as the Variance Exploding (VE) SDE, and
Eq. (11) the Variance Preserving (VP) SDE.
Inspired by the Vp SDE, we propose a new SDE called the sub-VP SDE, namely
dx “ 一gβ(t)x dt + ʌ/β(t)(1 ´ e´2 't βpsqdsqdw.	(27)
Following standard derivations, it is straightforward to show that Erx(tqs is the same for both Vp and
sub-Vp SDEs; the variance function of sub-Vp SDEs is different, given by
Σsub-vp(t) = I + e∙'t βpsqdsI + e´'t βpsqds(∑sub-vp(oq ´ 2I),	(28)
where Σsub-Vp(tq :“ Covrx(tqs for a process tx(tqut1“0 obtained by solving Eq. (27). In addition,
We observe that (i) ∑sub-vp(tq W ∑vp(t) for all t20 with ∑sub-vp(θq = ∑vp(0) and shared β(s);
and (ii) limt—g ∑sub-VP(tq “ limt—g Σγp(t) “ I if limt—g ,0 β(s)ds = 8. The former is why We
name Eq. (27) the sub-Vp SDE—its variance is always upper bounded by the corresponding Vp
SDE. The latter justifies the use of sub-Vp SDEs for score-based generative modeling, since they can
perturb any data distribution to standard Gaussian under suitable conditions, just like Vp SDEs.
VE, Vp and sub-Vp SDEs all have affine drift coefficients. Therefore, their perturbation kernels
pot(x(tq | x(0qq are all Gaussian and can be computed with Eqs. (5.50) and (5.51) in Sarkka & Solin
(2019):
‘ N(x(t);x(0), [σ2(t) — σ2(0)]l),	(VE SDE)
pot(x(tq | x(0)) = v N(x(t); x(0)e-2，t βps)ds,I ´ IeTt βps)ds)	(Vp SDE) . (29)
N(x(t); x(0)e—2 ,t βps)ds, [1 ´ e´ ,t βpsqdsS2I)	(SUb-VP SDE)
As a result, all SDEs introduced here can be efficiently trained with the objective in Eq. (7).
C SDEs in the wild
Below we discuss concrete instantiations of VE and Vp SDEs whose discretizations yield SMLD
and DDpM models, and the specific sub-Vp SDE used in our experiments. In SMLD, the noise
scales {σi}N^ι is typically a geometric sequence where σmi∩ is fixed to 0.01 and σmaχ is chosen
according to Technique 1 in Song & Ermon (2020). Usually, SMLD models normalize image inputs
to the range [0,1]. Since {σi}g] is a geometric sequence, we have σ(N)
=σi = σmin (σmx) “T
for i = 1, 2, ∙∙∙ , N. In the limit of N → 8, we have σ(t)
corresponding VE SDE is
“σmin ´σmax) for t P (0,1]. The
dx = σmin^ σmax) tc∕2i0g σmax dw,	t P (0, IS,
σmin	σmin
and the perturbation kernel can be derived via Eq. (29):
p0t(Xpt)I X(0)) = N (Xpt) x(0), σmin (*)F , t P(0,1s.
(30)
(31)
There is one subtlety when t = 0: by definition, σ(0) = σ0 = 0 (following the convention in Eq. (20)),
but σ(0') := limt—o` σ(t) = σmi∏ ‰ 0. In other words, σ(t) for SMLD is not differentiable since
σ(0) ‰ σ(0'), causing the VE SDE in Eq. (21) undefined for t = 0. In practice, we bypass this issue
by always solving the SDE and its associated probability flow ODE in the range t P [, 1s for some
small constant e > 0, and we use e = 10-5 in our VE SDE experiments.
15
Published as a conference paper at ICLR 2021
Variance of Perturbation Kernels
0.0	0.2	0.4	0.6	0.8	1.0
(a) SMLD
Mean of Perturbation Kernels
0.0	0.2	0.4	0.6	0.8	1.0
(b) DDPM (mean)
Variance of Perturbation Kernels
Q.8.6,42 Q
Iooooo
(υuueue>
0.0	0.2	0.4	0.6	0.8	1.0
t
(c) DDPM (variance)
Figure 5: Discrete-time perturbation kernels and our continuous generalizations match each other
almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b)
compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c)
compares the variance of perturbation kernels for DDPM and VP SDE.
For DDPM models, {/力此1 is typically an arithmetic sequence where βi “ βNn ` NIN匕)(βmax —
βmin) for i = 1,2,…，N. Therefore, β(t) = βmin ' t(βmax — βmin) for t P [0,1S in the limit of
N → 8. This corresponds to the following instantiation of the VP SDE:
dx = ´ 2 (βmin + t(β
max
´ βmin))xdt +
βjβmin + t(βmax ´ βmin)dw,
t P r0, 1s,
(32)
where x(0)〜Pdata(x)∙ In our experiments, we let βm⅛ = 0.1 and βmax = 20 to match the settings in
Ho et al. (2020). The perturbation kernel is given by
p0t (x(t) | x(0))
=N (x(t); e´1 t2pβmx-%nq—1 tβminx(0), I — le´2t2pβmx-%nq—tβmin) , t p[0, 1]. (33)
For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical
instability issues for training and sampling at t = 0, due to the vanishing variance of x(t) as t → 0.
Therefore, same as the VE SDE, we restrict computation to t P [e, 1] for a small e > 0. For sampling,
we choose e = 10-3 so that the variance of x(e) in VP SDE matches the variance of xι in DDPM;
for training and likelihood computation, we adopt e = 10-5 which empirically gives better results.
As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation
kernels of SDEs and original discrete Markov chains in Fig. 5. The SMLD and DDPM models both
use N = 1000 noise scales. For SMLD, we only need to compare the variances of perturbation
kernels since means are the same by definition. For DDPM, we compare the scaling factors of means
and the variances. As demonstrated in Fig. 5, the discrete perturbation kernels of original SMLD and
DDPM models align well with perturbation kernels derived from VE and VP SDEs.
For sub-VP SDEs, we use exactly the same β(t) as VP SDEs. This leads to the following perturbation
kernel
p0t (x(t) | x(0))
=N ´χ(t)
;e
1
4
—e´ 1 t2(βmax-Bmin)-tBmin ]2 ])
t P r0, 1].
(34)
We also restrict numerical computation to the same interval of re, 1] as VP SDEs.
Empirically, we observe that smaller e generally yields better likelihood values for all SDEs. For
sampling, it is important to use an appropriate e for better Inception scores and FIDs, although
samples across different e look visually the same to human eyes.
D	Probability flow ODE
D. 1 Derivation
The idea of probability flow ODE is inspired by Maoutsa et al. (2020), and one can find the derivation
of a simplified case therein. Below we provide a derivation for the fully general ODE in Eq. (17). We
16
Published as a conference paper at ICLR 2021
consider the SDE in Eq. (15), which possesses the following form:
dx “ fpx, tqdt ` Gpx, tqdw,
where f (∙,t) : Rd → Rd and G(∙,t) : Rd → Rd'd. The marginal probability density pt(x(t))
evolves according to Kolmogorov,s forward equation (Fokker-Planck equation) (0ksendal, 2003)
Bp (xq	d B	1 d	d	B2	d
“	“ 一	∑	钉 rfi(x,t)pt(x)s	+	5 ∑ ∑	A A	I ∑	Gik(X,t)Gjk(χ,t)pt(χ)].	(35)
Bt	i“1	Bxi	2 i“1 j“1	BxiBxj	k“1
We can easily rewrite Eq. (35) to obtain
Bpt(X)
Bt
d B	1 d d B2 d
“ 一Σ Bxrfi(χ,tqpt(χ)s + 2£ £ BxBX[∑ Gik(x,t)Gjk(x,t)pt(x)]
i“1	i“1 j“1	j k“1
“一∑ Bx rfi(x,t)pt(x)s +	2Σ	BXi [∑	BBj [∑	Gik(x,t)Gjk(x,t)pt(x)l].	(36)
i“1 xi	i“1 xi j“1	xj k“1
Note that
dd
Σ B- ∖ Σ Gik(X,t)Gjk(x,t)pt(x)]
j“1 Bxj k“1
d	d	dd
“ ∑ B- ∑ ∑ Gik (x,t)Gjk (x,t∕∣ pt(x) + ∑ ∑ Gik(x,t)Gjk(x,t)pt(x)钎 log pt(x)
j“1 Bxj k“1	j“1 k“1	Bxj
“pt(x)V ∙ [G(x,t)G(x,t)TS + pt(x)G(x,t)G(x,t)TVχ logpt(x),
based on which we can continue the rewriting of Eq. (36) to obtain
d	ddd
~ιτ “一∑1 Bxi rfi(x,t)pt(x)s + 2 ∑1 Bxi [ ∑1 Bxj [∑ Gik(X, tqGjk (x,t)pt(x)ll
d
“一∑ Bχ rfi(x,t)pt(x)s
1d B
+ 2 ∑ BxIpt(x)V ∙ [G(x, t)G(x, t)TS + pt(x)G(x, t)G(x, t)TVχ logpt(x)]
i“1	i
d
“一∑ 二｛fi(x,tqpt(x)
i“1 Bxi
—1 [V ∙ [G(x,t)G(x,t)TS + G(x,t)G(x,t)TVχlogPt(XqIpt(Xq)
d
“一∑ 钉 fi(x,tqpt(xqs,	1
i“1 Bxi
(37)
where we define
11
f(x,tq “ f(x,tq — 2V ∙ [G(x,tqG(x,tqτS — 2G(x,tqG(x,tqτVχ logPt(x).
Inspecting Eq. (37), we observe that it equals Kolmogorov’s forward equation of the following
SDE with G(x, tq :“ 0 (Kolmogorov’s forward equation in this case is also known as the Liouville
equation.)
dx“	f(x, tqdt + G(x, tqdw,
which is essentially an ODE:
dx“	f(x, tqdt,
same as the probability flow ODE given by Eq. (17). Therefore, we have shown that the probability
flow ODE Eq. (17) induces the same marginal probability density pt (xq as the SDE in Eq. (15).
17
Published as a conference paper at ICLR 2021
D.2 Likelihood computation
The probability flow ODE in Eq. (17) has the following form when We replace the score Nx log Pt (x)
with the time-dependent score-based model sθ px, tq:
dx = If(x,t) — 2V ∙ [G(x,t)G(x,t)TS — 2G(x,t)G(x,t)Tse(x,t)卜t. (38)
loooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooon
V
“ ∙fθ (x,t)
With the instantaneous change of variables formula (Chen et al., 2018), we can compute the log-
likelihood of p0 (xq using
logp0 (x(0qq “
logPT(X(T))+ f V ∙ fθ(x(t),t)dt,
0
(39)
where the random variable x(t) as a function of t can be obtained by solving the probability flow
ODE in Eq. (38). In many cases computing V ∙ fθ (x, t) is expensive, so we follow Grathwohl et al.
(2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990).
In particular, we have
V ∙ fθ(x,t) = Ep(e)[eTVfθ(x,t)e],	(40)
where Vf⅛ denotes the Jacobian of f⅛(∙, t), and the random variable E satisfies Ep(e)[w] = 0 and
Covp() rs = I. The vector-Jacobian product TVffθ(x, t) can be efficiently computed using reverse-
mode automatic differentiation, at approximately the same cost as evaluating fθ (x, t). As a result,
we can sample E „ P(E) and then compute an efficient unbiased estimate to V ∙ fθ (x, t) using
ETVfθ(x, t)E. Since this estimator is unbiased, we can attain an arbitrarily small error by averaging
over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. (40)
to Eq. (39), we can compute the log-likelihood to any accuracy.
In our experiments, we use the RK45 ODE solver (Dormand & Prince, 1980) provided by
scipy.integrate.solve_ivp in all cases. The bits/dim values in Table 2 are computed
with atol=1e-5 and rtol=1e-5, same as Grathwohl et al. (2018). To give the likelihood results
of our models in Table 2, we average the bits/dim obtained on the test dataset over five different runs
with e = lθ´5 (see definition of e in Appendix C).
D.3 Probability flow sampling
Suppose we have a forward SDE
dx = f(x, t)dt + G(t)dw,
and one of its discretization
xi'1 = Xi + fi(xi) + GiZi, i = 0,1, ∙∙∙ ,N — 1,	(41)
where zi „ N(0, I). We assume the discretization schedule of time is fixed beforehand, and thus
we absorb the dependency on ∆t into the notations of fi and Gi. Using Eq. (17), we can obtain the
following probability flow ODE:
dx = If (x,t) — 1 G(t)G(t)TVx log pt(x)}dt.
(42)
We may employ any numerical method to integrate the probability flow ODE backwards in time for
sample generation. In particular, we propose a discretization in a similar functional form to Eq. (41):
xi = xi'1 — fi'l(xi'l) + 2Gi + IGIlSe* (xi'1, i + 1q,	i = 0, 1,…，N — 1,
where the score-based model Se* (xi, i) is conditioned on the iteration number i. This is a determin-
istic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional
randomness once the initial sample xN is obtained from the prior distribution. When applied to
SMLD models, we can get the following iteration rule for probability flow sampling:
Xi = xi+1 + 1 (σ2+ι — σ2)se* g`i,bi`i),	i = 0,1, ∙ ∙ ∙ ,N — 1.	(43)
Similarly, for DDPM models, we have
xi = (2 — a1 - βi+1 qxi+1 + 2βi'isθ* (xi'i, i + 1q, i = 0, 1, ∙∙∙ , n — 1.	(44)
18
Published as a conference paper at ICLR 2021
D.4 Sampling with black-box ODE s olvers
For producing figures in Fig. 3, We use a DDPM model trained on 256 X 256 CelebA-HQ with the
same settings in Ho et al. (2020). All FID scores of our models in Table 2 are computed on samples
from the RK45 ODE solver implemented in scipy.integrate.solve_ivp with atol=1e-5
and rtol=1e-5. We use e “ lθ´5 for VE SDEs and e “ lθ´3 for VP SDEs (see also Appendix C).
Aside from the interpolation results in Fig. 3, we demonstrate more examples of latent space
manipulation in Fig. 6, including interpolation and temperature scaling. The model tested here is a
DDPM model trained with the same settings in Ho et al. (2020).
Although solvers for the probability flow ODE allow fast sampling, their samples typically have
higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this
empirical observation for both the discretization strategy in Appendix D.3, and black-box ODE
solvers introduced above. Moreover, the performance of probability flow ODE samplers depends on
the choice of the SDE—their sample quality for VE SDEs is much worse than VP SDEs especially
for high-dimensional data.
D.5 Uniquely identifiable encoding
As a sanity check, we train two models (denoted as “Model A” and “Model B”) with different
architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per
resolution trained using the continuous objective in Eq. (7), and Model B is all the same except that it
uses 8 layers per resolution. Model definitions are in Appendix H.
We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in
Fig. 7. In Fig. 8, we show the dimension-wise differences and correlation coefficients between latent
encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model
A and Model B provide encodings that are close in every dimension, despite having different model
architectures and training runs.
E Reverse diffusion sampling
Given a forward SDE
dx “ fpx, tqdt ` Gptqdw,
and suppose the following iteration rule is a discretization of it:
Xi'1 “ Xi ' fi(xi) ' GiZi, i = 0,1,…，N — 1	(45)
where zi „ N p0, Iq. Here we assume the discretization schedule of time is fixed beforehand, and
thus we can absorb it into the notations of fi and Gi .
Based on Eq. (45), we propose to discretize the reverse-time SDE
dx = f (x, t) — GptqGptqTVxlogPt(X)Sdt ' G(t)dw,
with a similar functional form, which gives the following iteration rule for i p {0,1,…，N — 1}:
xi “ xi'1 — fi'1 (xi'l) ' Gi'1Gτ'isθ⅛ (xi'1, i ' 1q ' Gi'1zi'1,	(46)
where our trained score-based model Se* (xi, i) is conditioned on iteration number i.
When applying Eq. (46) to Eqs. (10) and (20), we obtain a new set of numerical solvers for the
reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the “predictor” part of
Algorithms 2 and 3. We name these sampling methods (that are based on the discretization strategy
in Eq. (46)) reverse diffusion samplers.
As expected, the ancestral sampling of DDPM (Ho et al., 2020) (Eq. (4)) matches its reverse diffusion
counterpart when βi → 0 for all i (which happens when ∆t → 0 since βi “ βi∆t, see Appendix B),
19
Published as a conference paper at ICLR 2021
Figure 6: Samples from the probability flow ODE for VP SDE on 256 ^ 256 CelebA-HQ. Top:
spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of
embedding).
20
Published as a conference paper at ICLR 2021
-IOO
o
U O
① nra> 4UΦ4E-J
20	40	60
Dimension
80
100
Figure 7:	Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10
image. “Model A” and “Model B” are separately trained with different architectures.
Model A
vs.
Model B
Model A (shuffled)
vs.
Model B (shuffled)
0
100	200	300
Difference in encodings
600
200
⊂ 400
□
o
mωpow
Model A
0
0.00
0.25	0.50	0.75	1.00
Correlation Coefficient
S
Figure 8:	Left: The dimension-wise difference between encodings obtained by Model A and B. As a
baseline, we also report the difference between shuffled representations of these two models. Right:
The dimension-wise correlation coefficients of encodings obtained by Model A and Model B.
because
xi “一J	C 二(xi'1 + βi'1sθ* (xi'1, i + 1)) + 7∕βi'1zi'1
√1 — βi'1
“ (1+
« (1 '
“(1+
« (1 '
“ 2 —
« 2 —
2βi'1 ' 0(βi'l)) pxi'1 ' βi'1sθ* (xi'1, i ' 1)) ' 7βi'1zi'1
2βi'l) (xi'1 ' βi'1sθ* (xi'1, i ' 1)) + 7βi'1zi'1
2βi'l) xi'1 ' βi'1 sθ* (xi'1, i ' 1) ' 2万2+ 1sθ* (xi'1, i + 1) ' 7βi'1zi'1
2βi'l) xi'1 ' βi'1 sθ* (xi'1, i + 1) ' 7βi'1zi'1
(1 ´ 2βi'l) xi'1 ' βi'1sθ* (xi'1, i + 1) ' 7βi'1zi'1
(1 ´ 2βi'l) ' 0(βi'l) xi'1 ' βi'1sθ* (xi'1, i + 1) + 7βi'1zi'1
“p2 ´ ʌ/1 ´ βi'1 )xi'1 ' βi'1sθ* pxi'1,i ' 1) ' ʌ/βi'1zi'1.
Therefore, the original ancestral sampler of Eq. (4) is essentially a different discretization to the same
reverse-time SDE. This unifies the sampling method in Ho et al. (2020) as a numerical solver to the
reverse-time VP SDE in our continuous framework.
F Ancestral sampling for SMLD models
The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a
sequence of noise scales σι < σ2 < ∙∙∙ < on as in SMLD. By perturbing a data point x° with these
noise scales sequentially, we obtain a Markov chain xo → xi → …→ XN, where
PpXi | Xi—i) “ N (Xi； Xi—1, (σ2 — σ?i)I), i = 1, 2,…，N.
21
Published as a conference paper at ICLR 2021
Algorithm 1 Predictor-Corrector (PC) sampling
Require:
N : Number of discretization steps for the reverse-time SDE
M : Number of corrector steps
1:	Initialize xN „ pT pxq
2:	for i “ N ´ 1 to 0 do
3:	Xi D Predictor(xi'ι)
4:	for j “ 1 to M do
5:	Xi D Corrector(Xiq
6:	return X0
Here we assume σ0 “ 0 to simplify notations. Following Ho et al. (2020), we can compute
——Kσ2 ´ σL 1q τ∖
x0,—σ—IJ.
22
q(Xi —1 | Xi, X0q “ N 卜i —1; ~2∑~Xi ' (1-σ´ɪ)
If We parameterize the reverse transition kernel as pθ(x-1 | Xi) “ N(xi—1； μe(xi, i), τ2iq, then
Lt—1 “ EqrDKL(q(Xi—1 | Xi, X0))} Pθ(Xi—1 | Xi)S
“ Eq
1
2T2
&Xi + ´l — σσ21)X0 — μθ(Xi,i)∣∣ [ + C
1
2T2
22
Xi(X0,z)——i-i´1 Z — μθ(Xi(X0, z),i)
σi
l：j ' C,
Where Lt—1 is one representative term in the ELBO objective (see Eq. (8) in Ho et al. (2020)), C is
a constant that does not depend on θ, z „ N(0, I), and Xi (X0 , z) “ X0 + σiz. We can therefore
parameterize μe(Xi, i) via
μθ(Xi, i)“ Xi + (σ2 —。2—1)sθ(Xi, i),
where sθ(Xi, i) is to estimate z{σi. As in Ho et al. (2020), we let Ti “ ʌʌi´1]2 σiτq. Through
ancestral sampling on "二1pθ(Xi—1 | Xi), we obtain the following iteration rule
C C	I σ2 1 (σ2 — σ2 1)
Xi—1 “ Xi + (σ2 — G2_1)sθ* (Xi, i) + J---σ-----— Zi, i = 1, 2,…，N,	(47)
where XN „ N(0,σNI), θ* denotes the optimal parameter of sθ, and Zi „ N(0, I). We call
Eq. (47) the ancestral sampling method for SMLD models.
G	Predictor-Corrector samplers
Predictor-Corrector (PC) sampling The predictor can be any numerical solver for the reverse-
time SDE with a fixed discretization strategy. The corrector can be any score-based MCMC approach.
In PC sampling, we alternate between the predictor and corrector, as described in Algorithm 1. For
example, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed
Langevin dynamics (Song & Ermon, 2019) as the corrector, we have Algorithms 2 and 3 for VE and
VP SDEs respectively, where tiuiN“—01 are step sizes for Langevin dynamics as specified below.
The corrector algorithms We take the schedule of annealed Langevin dynamics in Song & Ermon
(2019), but re-frame it with slight modifications in order to get better interpretability and empirical
performance. We provide the corrector algorithms in Algorithms 4 and 5 respectively, where we call
r the “signal-to-noise” ratio. We determine the step size using the norm of the Gaussian noise kZk2 ,
norm of the score-based model ∣∣se* ∣∣2 and the signal-to-noise ratio r. When sampling a large batch
of samples together, we replace the norm ∣∣∙∣2 with the average norm across the mini-batch. When
the batch size is small, we suggest replacing ∣∣z∣2 with ʌ/d, where d is the dimensionality of z.
22
Published as a conference paper at ICLR 2021
Algorithm 2 PC sampling (VE SDE)
Algorithm 3 PC sampling (VP SDE)
1 2	: xN „ N p0, σm2axIq for i = N — 1 to 0 do	1 2	: XN „ N(0, Iq for i “ N — 1 to 0 do
3	Xi D xi'1 + (σ2+1 — σi )sθ* (xi + 1, σi'1 q	3	χi D (2 — ?1 —〈+1 )χi+ι + βi+ιsθ* (χi+ι, i ÷ 1q
4	Z „ N(0, Iq	4	Z „ N(0,I)
5	χi D xi + Jσ2+1 ´ σ2 Z	5	/ , rτ>	~	Predictor xi D- xi + Vβi + 1Z
6	for j = 1 to M do	6	for j “ 1 to M do	Corrector
7	z „ N(0,Iq	7	Z „ N(0,I)
8	Xi D Xi + QSθ* (xi ,σi q + √2ei z	8	Xi D Xi + ∈iSθ* (xi, i) + √2eiZ
9	return xo	9	return Xo
Algorithm 4 Corrector algorithm (VE SDE). Algorithm 5 Corrector algorithm (VP SDE).
Require: {σi }N“1,r, N, M.		Require： {βi}N“i, {ai}N“「r,N,M.	
1	: x0N „ Np0,σm2axIq	1	: x0N „ Np0,I)
2	for i D N to 1 do	2	for i D N to 1 do
3	for j D 1 to M do	3	for j D 1 to M do
4	:	z„Np0,Iq	4	:	z„Np0,I)
5	g D sθ*(xjT,σi) C D 2Pr Ilzll2/IIgll2)2	5	g D sθ*(xjT,i) C D 2αiprkzk2 / l∣gk2)2
6		6	
7	xj D XjT + C g + √2e Z	7	Xj D XjT + C g + √26 Z
8	x0τ D xM	8	x0τ D xM
	return x00		return x00
Denoising For both SMLD and DDPM models, the generated samples typically contain small
noise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. (2020), FIDs can be
significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of
the reason why NCSN models trained with SMLD has been performing worse than DDPM models
in terms of FID, because the former does not use a denoising step at the end of sampling, while the
latter does. In all experiments of this paper we ensure there is a single denoising step at the end of
sampling, using Tweedie’s formula (Efron, 2011).
ω⊂o⅞⊃-ro>ω Uo-t;Ury ①」oɔs -son
Figure 9: PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total
computation, and the horizontal axis represents the amount of computation allocated to the corrector.
Samples are the best when computation is split between the predictor and corrector.
Training We use the same architecture in Ho et al. (2020) for our score-based models. For the
VE SDE, we train a model with the original SMLD objective in Eq. (1); similarly for the VP SDE,
we use the original DDPM objective in Eq. (3). We apply a total number of 1000 noise scales for
training both models. For results in Fig. 9, we train an NCSN++ model (definition in Appendix H) on
23
Published as a conference paper at ICLR 2021
Table 4: Comparing different samplers on CIFAR-10, where “P2000” uses the rounding interpolation
between noise scales. Shaded regions are obtained with the same computation (number of score
function evaluations). Mean and standard deviation are reported over five sampling runs.
	Variance Exploding SDE (SMLD)				Variance Preserving SDE (DDPM)		
FID [ ^∖Sampler ^redIctor	P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
ancestral sampling reverse diffusion probability flow	4.98 ± .06 4.79 ± .07 15.41 ± .15	4.92 ± .02 4.72 ± .07 12.87 ±.09	20.43 ± .07	3.62 ± .03 3.60 ± .02 3.51 ± .04	3.24 ± .02 3.21 ± .02 3.59 ±.04	3.11 ± .03 3.10 ± .03	19.06 ± .06 3.25 ± .04	3.21 ± .02 3.18 ± .01 3.06 ± .03
Table 5: Optimal signal-to-noise ratios of different samplers. “P1000” or “P2000”: predictor-only
samplers using 1000 or 2000 steps. “C2000”: corrector-only samplers using 2000 steps. “PC1000”:
PC samplers using 1000 predictor and 1000 corrector steps.
	VE SDE (SMLD)				VP SDE (DDPM)			
r	Sampler Predicto铲士	P1000	P2000	C2000	PC1000	P1000	P2000	C2000	PC1000
ancestral sampling	-	-		0.17	-	-		-0.01-
reverse diffusion	-	-	0.22	0.16	-	-	0.27	0.01
probability flow	-	-		0.17	-	-		0.04
256 X 256 LSUN bedroom and ChUrch_outdoor(Yu et al., 2015) datasets with the VE SDE and our
continuous objective Eq. (7). The batch size is fixed to 128 on CIFAR-10 and 64 on LSUN.
Ad-hoc interpolation methods for noise scales Models in this experiment are all trained with
1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires
2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific
architecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional
embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales
at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song &
Ermon (2019)). Specifically, for SMLD models, we keep σmin and σmax fixed and double the number
of time steps. For DDPM models, we halve βmin and βmax before doubling the number of time steps.
Suppose {se(x, i)}N01 is a score-based model trained on N time steps, and let {sθ(x, i)}2N0^1
denote the corresponding interpolated score-based model at 2N time steps. We test two different
interpolation strategies for time steps: linear interpolation where s1θ px, iq “ sθ px, i{2q and rounding
interpolation where s1θpx, iq “ sθpx, ti{2uq. We provide results with linear interpolation in Table 1,
and give results of rounding interpolation in Table 4. We observe that different interpolation methods
result in performance differences but maintain the general trend of predictor-corrector methods
performing on par or better than predictor-only or corrector-only samplers.
Hyper-parameters of the samplers For Predictor-Corrector and corrector-only samplers on
CIFAR-10, we search for the best signal-to-noise ratio (r) over a grid that increments at 0.01.
We report the best r in Table 5. For LSUN bedroom/church_outdoor, we fix r to 0.075. Unless
otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector
steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size
is 1024 on CIFAR-10 and 8 on LSUN bedroom/church_outdoor.
H	Architecture improvements
We explored several architecture designs to improve score-based models for both VE and VP SDEs.
Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art
likelihood on uniformly dequantized CIFAR-10, and enables the first high-fidelity image samples of
resolution 1024 X 1024 from score-based generative models. Code and checkpoints are open-sourced
at https://github.com/yang-song/score_sde.
24
Published as a conference paper at ICLR 2021
H.	1 Settings for architecture exploration
Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per
50k iterations. For VE SDEs, We consider two datasets: 32 X 32 CIFAR-10(KrizheVsky et al., 2009)
and 64 X 64 CelebA (Liu et al., 2015), pre-processed following Song & Ermon (2020). We compare
different configurations based on their FID scores aVeraged oVer checkpoints after 0.5M iterations.
For VP SDEs, we only consider the CIFAR-10 dataset to saVe computation, and compare models
based on the aVerage FID scores oVer checkpoints obtained between 0.25M and 0.5M iterations,
because FIDs turn to increase after 0.5M iterations for VP SDEs.
All FIDs are computed on 50k samples with tensorflow,gan. For sampling, we use the PC
sampler discretized at 1000 time steps. We choose reVerse diffusion (see Appendix E) as the predictor.
We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of
0.16, but saVe the corrector step for VP SDEs since correctors there only giVe slightly better results
but require double computation. We follow Ho et al. (2020) for optimization, including the learning
rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are
trained with the original discrete SMLD and DDPM objectiVes in Eqs. (1) and (3) and use a batch
size of 128. The optimal architectures found under these settings are subsequently transferred to
continuous objectiVes and deeper models. We also directly transfer the best architecture for VP SDEs
to sub-VP SDEs, giVen the similarity of these two SDEs.
Q
lL
4.5
4.0
3.5
3.0
2.5
ClFAR-IO	CeIebA
dataset
4.5
4.0
3.0
Q
lj^ 3.5
CiFAR-IO	CeIebA
dataset
Progressive Arch, (input, output)
none, none
input-skip, none
residual, none
none, output skip
input-skip, output skip
residual, output skip
I I none, residual
input-skip, residual
I I residual, residual
2.5
dataset
Figure 10: The effects of different architecture components for score-based models trained with VE
perturbations.
ClFAR-IO	CeIebA
dataset
Our architecture is mostly based on Ho et al. (2020). We additionally introduce the following
components to maximize the potential improvement of score-based models.
1.	Upsampling and downsampling images with anti-aliasing based on Finite Impulse Re-
sponse (FIR) (Zhang, 2019). We follow the same implementation and hyper-parameters in
StyleGAN-2 (Karras et al., 2020b).
2.	Rescaling all skip connections by 1{?2. This has been demonstrated effective in several best-
in-class GAN models, including ProgressiveGAN (Karras et al., 2018), StyleGAN (Karras
et al., 2019) and StyleGAN-2 (Karras et al., 2020b).
3.	Replacing the original residual blocks in DDPM with residual blocks from BigGAN (Brock
et al., 2018).
4.	Increasing the number of residual blocks per resolution from 2 to 4.
25
Published as a conference paper at ICLR 2021
5.	Incorporating progressive growing architectures. We consider two progressive architectures
for input: “input skip” and “residual”, and two progressive architectures for output: “output
skip” and “residual”. These progressive architectures are defined and implemented according
to StyleGAN-2.
We also tested equalized learning rates, a trick used in very successful models like Progressive-
GAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). However, we found it harmful at an
early stage of our experiments, and therefore decided not to explore more on it.
The exponential moving average (EMA) rate has a significant impact on performance. For models
trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models
trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999
for VE and VP models respectively.
H.2 Results on CIFAR- 1 0
All architecture components introduced above can improve the performance of score-based models
trained with VE SDEs, as shown in Fig. 10. The box plots demonstrate the importance of each
component when other components can vary freely. On both CIFAR-10 and CelebA, the additional
components that we explored always improve the performance on average for VE SDEs. For
progressive growing, it is not clear which combination of configurations consistently performs the
best, but the results are typically better than when no progressive growing architecture is used.
Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip
connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution
instead of 2, and 5) uses “residual” for input and no progressive growing architecture for output. We
name this model “NCSN++”, following the naming convention of previous SMLD models (Song &
Ermon, 2019; 2020).
We followed a similar procedure to examine these architecture components for VP SDEs, except that
we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture
worked decently well for VP SDEs, ranked 4th place over all 144 possible configurations. The top con-
figuration, however, has a slightly different structure, which uses no FIR upsampling/downsampling
and no progressive growing architecture compared to NCSN++. We name this model “DDPM++”,
following the naming convention of Ho et al. (2020).
The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10,
whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention
used in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID
value over the course of training, rather than the average FID value over checkpoints after 0.5M
iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations
(used for comparing VP SDE models) in our architecture exploration.
Switching from discrete training objectives to continuous ones in Eq. (7) further improves the FID
values for all SDEs. To condition the NCSN++ model on continuous time variables, we change
positional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to
random Fourier feature embeddings (Tancik et al., 2020). The scale parameter of these random
Fourier feature embeddings is fixed to 16. We also reduce the number of training iterations to 0.95M
to suppress overfitting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++
trained with the VE SDE, resulting in a model called “NCSN++ cont.”. In addition, we can further
improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for
NCSN++ cont., resulting in the model denoted as “NCSN++ cont. (deep)”. All quantitative results
are summarized in Table 3, and we provide random samples from our best model in Fig. 11.
Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model
“DDPM++ cont.”. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to
2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance,
we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the
ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization
strategy of the original DDPM method does not match the variance of the continuous process well
When t → 0, which significantly hurts FID scores. As shown in Table 2, the likelihood values are
3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with
26
Published as a conference paper at ICLR 2021
⅛H 宓∙κ≡∙v∙≡Ee
■■■■■ ■备ESQ∙E>g
Figure 11: Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).
3ea?
, r⅞*□一 nxal∙u≡Γ∙
际hsB*⅛巴缪丁福□&凶raw%
F9X %茎富1|3，!|河糜Itt..八⅜≡4
asfift⅛f 像JuLlHa£超率 Cv□!3
■WS—BAES ∕I≡A≡∖
M≡ - ð tsH
* “∙∙sw≡w ■<■
fHH王城sslls5ff
学 Sl 鹿务ΛτIHlr/
knvBnDHS- 3Q∙∙JI■■黑
SM - S
・长曲■■ ∙∙aE 1。过W W・F i
阖匕κr⅛∙“HIXHn 一早・ 力颗
~a9■学∙□I9ISIJ 鼬∙∙F、91»■
44l<¼e S ∙P**B匪时女呼数E *的盗
27
Published as a conference paper at ICLR 2021
Figure 12: Samples on 1024 X 1024 CelebA-HQ from a modified NCSN++ model trained with the
VE SDE.
28
Published as a conference paper at ICLR 2021
0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a
model “DDPM++ cont. (deep)”. Its FID score is 2.41, same for both VP and sub-VP SDEs. When
trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values
are reported for the last checkpoint during training.
H.3 High resolution images
Encouraged by the success of NCSN++ on CIFAR-10, We proceed to test it on 1024 X 1024 CelebA-
HQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and
VQ-VAE-2 (Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and
trained a model similar to NCSN++ With the continuous objective (Eq. (7)) for around 2.4M iterations
(please find the detailed architecture in our code release.) We use the PC sampler discretized at 2000
steps With the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise
ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 16. We use
the “input skip” progressive architecture for the input, and “output skip” progressive architecture for
the output. We provide samples in Fig. 12. Although these samples are not perfect (e.g., there are
visible flaWs on facial symmetry), We believe these results are encouraging and can demonstrate the
scalability of our approach. Future Work on more effective architectures are likely to significantly
advance the performance of score-based generative models on this task.
I Controllable generation
Consider a forWard SDE With the folloWing general form
dx “ fpx, tqdt ` Gpx, tqdw,
and suppose the initial state distribution is p0pxp0q | yq. The density at time t is ptpxptq | yq When
conditioned on y. Therefore, using Anderson (1982), the reverse-time SDE is given by
dx = {f(x,t) — V ∙[G(x,t)GPx,t)τS — G(x,t)G(x,t)TVχ logPtpx | y)}dt + G(x,t)dw.(48)
Since Ptpxpt) | y)9pt(x(t))p(y | x(t)), the score Vx logpt(x(t) | y) can be computed easily by
Vx log ptpxptq | yq “ Vx log ptpxptqq ` Vx log ppy | xptqq.	(49)
This subsumes the conditional reverse-time SDE in Eq. (14) as a special case. All sampling methods
We have discussed so far can be applied to the conditional reverse-time SDE for sample generation.
I.1	Class-conditional sampling
When y represents class labels, We can train a time-dependent classifier Pt (y | x(t)) for class-
conditional sampling. Since the forWard SDE is tractable, We can easily create a pair of training
data (x(t), y) by first sampling (x(0), y) from a dataset and then obtaining x(t) „ P0t (x(t) | x(0)).
AfterWards, We may employ a mixture of cross-entropy losses over different time steps, like Eq. (7),
to train the time-dependent classifier Pt (y | x(t)).
To test this idea, We trained a Wide ResNet (Zagoruyko & Komodakis, 2016)
(Wide-ResNet-28-10) on CIFAR-10 With VE perturbations. The classifier is condi-
tioned on log σi using random Fourier features (Tancik et al., 2020), and the training objective is
a simple sum of cross-entropy losses sampled at different scales. We provide a plot to shoW the
accuracy of this classifier over noise scales in Fig. 13. The score-based model is an unconditional
NCSN++ (4 blocks/resolution) in Table 3, and We generate samples using the PC algorithm With
2000 discretization steps. The class-conditional samples are provided in Fig. 4, and an extended set
of conditional samples is given in Fig. 13.
I.2	Imputation
Imputation is a special case of conditional sampling. Denote by Ω(x) and Ω (x) the known and un-
known dimensions of X respectively, and let fc(∙,t) and Gω(∙, t) denote f (∙,t) and G(∙, t) restricted
to the unknown dimensions. For Ve/VP SDEs, the drift coefficient f (∙,t) is element-wise, and the
diffusion coefficient G(∙,t) is diagonal. When f(∙,t) is element-wise, fc(∙,t) denotes the same
29
Published as a conference paper at ICLR 2021
Figure 13: Class-conditional image generation by solving the conditional reverse-time SDE with PC.
The curve shows the accuracy of our noise-conditional classifier over different noise scales.



30
Published as a conference paper at ICLR 2021
element-wise function applied only to the unknown dimensions. When G(∙, t) is diagonal, Gω(∙, t)
denotes the sub-matrix restricted to unknown dimensions.
For imputation, our goal is to sample from p(Ω(x(0)) | Ω(x(0)) “ y). Define a new diffusion
process z(t) “ Ω(x(t)), and note that the SDE for z(t) can be written as
dz = f。(z,t)dt + Gω(z,t)dw.
The reverse-time SDE, conditioned on Ω(x(0)) = y, is given by
dz = {f°(z,tq ´ V ∙ G(Z,t)G0(z,t)T]
—Gω(Z,t)Gc(z,t)TVZ logPt(Z | Ω(z(0qq = y)}dt + G©(z,t)dw.
Although pt(z(tq | Ω(x(0)) = y) is in general intractable, it can be approximated. Let A denote the
event Ω(x(0)) = y. We have
Pt(z(t) | Ω(x(0)) = y) = Pt(z(t) | A) = Jpt(z(t) | Ω(x(t)), A)pt(Ω(x(t)) | A)dΩ(x(t))
=Ept(Ω(x(t))∣A)[pt(z(t) | Ω(x(t)),A)S
« Ept(Ω(x(t))∣A)rpt(z(t) | a(x(t)))s
,	,< .O.......
« pt(z(t) | Ω(x(t))),
1	ʌ /	/ 1 ∖ ∖ ∙	F	1 i'	/ rʌ /	I Λ ∖	1 ∙ 1	∙	♦	11	,	. 1 1	1∙	. ∙1
where Ω(x(t)) is a random sample from pt(Ω(x(t)) | A), which is typically a tractable distribution.
Therefore,
___ - ，，、=-，，、、 、 ________________ - , , . ^...........
VZ logpt(z(t) | Ω(x(0)) = y) « Vz logPt(z(t) | Ω(x(t)))
=Vz log Pt ([z(t);。(x(t))S),
where rz(t); Ω(x(t))S denotes a vector u(t) such that Ω(u(t)) = Ω(x(t)) and Ω(u(t))=
z(t), and the identity holds because VZ logpt(rz(t); Ω(x(t))S) = VZ logpt(z(t) | Ω(x(t))) +
__ - , ^......... ..... - ， , , . ^........
Vzlogp(Ω(x(t))) = Vzlogpt(z(t) | Ω(x(t))).
We provided an extended set of inpainting results in Figs. 14 and 15.
I.3	Colorization
Colorization is a special case of imputation, except that the known data dimensions are coupled.
We can decouple these data dimensions by using an orthogonal linear transformation to map the
gray-scale image to a separate channel in a different space, and then perform imputation to complete
the other channels before transforming everything back to the original image space. The orthogonal
matrix we used to decouple color channels is
/0.577 —0.816
0.577	0.408
0.577	0.408
0.007 ].
—0.707
Because the transformations are all orthogonal matrices, the standard Wiener process w(t) will still
be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same
imputation method in Appendix I.2. We provide an extended set of colorization results in Figs. 16
and 17.
I.4	S olving general inverse problems
Suppose we have two random variables x and y, and we know the forward process of generating y
from x, given by p(y | x). The inverse problem is to obtain x from y, that is, generating samples
from p(x | y). In principle, we can estimate the prior distribution p(x) and obtain p(x | y) using
Bayes’ rule: p(x | y) = p(x)p(y | x){p(y). In practice, however, both estimating the prior and
performing Bayesian inference are non-trivial.
Leveraging Eq. (48), score-based generative models provide one way to solve the inverse problem.
Suppose we have a diffusion process tx(t)utT“0 generated by perturbing x with an SDE, and a
31
Published as a conference paper at ICLR 2021
time-dependent score-based model se* (x(t), t) trained to approximate Vχ logPtpxpt)). Once We
have an estimate of Vx log Ptpxpt) | y), we can simulate the reverse-time SDE in Eq.(48) to sample
from p0 pxp0) | y) “ ppx | y). To obtain this estimate, We first observe that
Vx log Ptpxpt) | y) “ Vx log Ptpxpt) | ypt), y)Ppypt) | y)dypt),
where ypt) is defined via xpt) and the forward process Ppypt) | xpt)). Now assume two conditions:
•	Ppypt) | y) is tractable. We can often derive this distribution from the interaction between
the forward process and the SDE, like in the case of image imputation and colorization.
•	Pt pxpt) | ypt), y) « Ptpxpt) | ypt)). For small t, ypt) is almost the same as y so the
approximation holds. For large t, y becomes further away from xpt) in the Markov chain,
and thus have smaller impact on xpt). Moreover, the approximation error for large t matter
less for the final sample, since it is used early in the sampling process.
Given these two assumptions, we have
Vx log Ptpxpt) | y) « Vx log Ptpxpt) | ypt))Ppypt) | y)dy
« vx logPtpxptq | yptqq
“ vx logPtpXpt)) + vx logPtpypt) | xpt))
«se*pχpt),t) + vxlogPtpypt) | χpt)),
(50)
where ypt) is a sample from Ppypt) | y). Now we can plug Eq. (50) into Eq. (48) and solve the
resulting reverse-time SDE to generate samples from Ppx | y).
32
Published as a conference paper at ICLR 2021
Figure 14: Extended inpainting results for 256 X 256 bedroom images.
33
Published as a conference paper at ICLR 2021
Figure 15: Extended inpainting results for 256 X 256 church images.
34
Published as a conference paper at ICLR 2021
Figure 16: Extended colorization results for 256 X 256 bedroom images.
35
Published as a conference paper at ICLR 2021
Figure 17: Extended colorization results for 256 X 256 church images.
36