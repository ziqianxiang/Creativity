Published as a conference paper at ICLR 2021
Gradient Projection Memory for Continual
Learning
Gobinda Saha, Isha Garg & Kaushik Roy
School of Electrical and Computer Engineering, Purdue University
gsaha@purdue.edu, gargi@purdue.edu, kaushik@purdue.edu
Ab stract
The ability to learn continually without forgetting the past tasks is a desired at-
tribute for artificial learning systems. Existing approaches to enable such learning
in artificial neural networks usually rely on network growth, importance based
weight update or replay of old data from the memory. In contrast, we propose a
novel approach where a neural network learns new tasks by taking gradient steps
in the orthogonal direction to the gradient subspaces deemed important for the
past tasks. We find the bases of these subspaces by analyzing network represen-
tations (activations) after learning each task with Singular Value Decomposition
(SVD) in a single shot manner and store them in the memory as Gradient Pro-
jection Memory (GPM). With qualitative and quantitative analyses, we show that
such orthogonal gradient descent induces minimum to no interference with the
past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse im-
age classification datasets with short and long sequences of tasks and report better
or on-par performance compared to the state-of-the-art approaches1.
1	Introduction
Humans exhibit remarkable ability in continual adaptation and learning new tasks throughout their
lifetime while maintaining the knowledge gained from past experiences. In stark contrast, Artifi-
cial Neural Networks (ANNs) under such Continual Learning (CL) paradigm (Ring, 1998; Thrun &
Mitchell, 1995; Lange et al., 2021) forget the information learned in the past tasks upon learning new
ones. This phenomenon is known as ‘Catastrophic Forgetting’ or ‘Catastrophic Interference’ (Mc-
closkey & Cohen, 1989; Ratcliff, 1990). The problem is rooted in the general optimization meth-
ods (Goodfellow et al., 2016) that are being used to encode input data distribution into the parametric
representation of the network during training. Upon exposure to a new task, gradient-based opti-
mization methods, without any constraint, change the learned encoding to minimize the objective
function with respect to the current data distribution. Such parametric updates lead to forgetting.
Given a fixed capacity network, one way to address this problem is to put constraints on the gra-
dient updates so that task specific knowledge can be preserved. To this end, Kirkpatrick et al.
(2017), Zenke et al. (2017), AljUndi et al. (2018), Serra et al. (2018) add a penalty term to the objec-
tive function while optimizing for new task. Such term acts as a structural regularizer and dictates
the degree of stability-plasticity of individUal weights. ThoUgh these methods provide resoUrce effi-
cient solUtion to the catastrophic forgetting problem, their performance sUffer while learning longer
task seqUence and when task identity is Unavailable dUring inference.
Approaches (Lopez-Paz & Ranzato, 2017; ChaUdhry et al., 2019a) that store episodic memories
of old data essentially solve an optimization problem with ‘explicit’ constraints on the new gradient
directions so that losses for the old task do not increase. In ChaUdhry et al. (2019b) the performance
of old task is retained by taking gradient steps in the average gradient direction obtained from the
new data and memory samples. To minimize interference, Farajtabar et al. (2020) store gradient
directions (instead of data) of the old tasks and optimize the network in the orthogonal directions
to these gradients for the new task, whereas Zeng et al. (2018) Update gradients orthogonal to the
old inpUt directions Using projector matrices calcUlated iteratively dUring training. However, these
methods either compromise data privacy by storing raw data or Utilize resoUrces poorly, which limits
their scalability.
1OUr code is available at https://github.com/sahagobinda/GPM
1
Published as a conference paper at ICLR 2021
In this paper, we address the problem of catastrophic forgetting in a fixed capacity network when data
from the old tasks are not available. To mitigate forgetting, our approach puts explicit constraints
on the gradient directions that the optimizer can take. However, unlike contemporary methods, we
neither store old gradient directions nor store old examples for generating reference directions. In-
stead we propose an approach that, after learning each task, partitions the entire gradient space of the
weights into two orthogonal subspaces: Core Gradient Space (CGS) and Residual Gradient Space
(RGS) (Saha et al., 2020). Leveraging the relationship between the input and the gradient spaces, we
show how learned representations (activations) form the bases of these gradient subspaces in both
fully-connected and convolutional networks. Using Singular Value Decomposition (SVD) on these
activations, we show how to obtain the minimum set of bases of the CGS by which past knowledge
is preserved and learnability for the new tasks is ensured. We store these bases in the memory which
we define as Gradient Projection Memory (GPM). In our method, we propose to learn any new
task by taking gradient steps in the orthogonal direction to the space (CGS) spanned by the GPM.
Our analysis shows that such orthogonal gradient descent induces minimum to no interference with
the old learning, and thus effective in alleviating catastrophic forgetting. We evaluate our approach
in the context of image classification with miniImageNet, CIFAR-100, PMNIST and sequence of
5-Datasets on a variety of network architectures including ResNet. We compare our method with
related state-of-the-art approaches and report comparable or better classification performance. Over-
all, we show that our method is memory efficient and scalable to complex dataset with longer task
sequence while preserving data privacy.
2	Related Works
Approaches to continual learning for ANNs can be broadly divided into three categories. In this
section we present a detailed discussion on the representative works from each category, highlighting
their contributions and differences with our approach.
Expansion-based methods: Methods in this category overcome catastrophic forgetting by dedicat-
ing different subsets of network parameters to each task. With no constraint on network architec-
ture, Progressive Neural Network (PGN) (Rusu et al., 2016) preserves old knowledge by freezing
the base model and adding new sub-networks with lateral connections for each new task. Dynam-
ically Expandable Networks (DEN) (Yoon et al., 2018) either retrains or expands the network by
splitting/duplicating important units on new tasks, whereas Sarwar et al. (2020) grow the network
to learn new tasks while sharing part of the base network. Li et al. (2019) with neural architecture
search (NAS) find optimal network structures for each sequential task. RCL (Xu & Zhu, 2018) adap-
tively expands the network at each layer using reinforcement learning, whereas APD (Yoon et al.,
2020) additively decomposes the parameters into shared and task specific parameters to minimize
the increase in the network complexity. In contrast, our method avoids network growth or expensive
NAS operations and performs sequential learning within a fixed network architecture.
Regularization-based methods: These methods attempt to overcome forgetting in fixed capacity
model through structural regularization which penalizes major changes in the parameters that were
important for the previous tasks. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017)
computes such importance from diagonal of Fisher information matrix after training, whereas Zenke
et al. (2017) compute them during training based on loss sensitivity with respect to the parameters.
Additionally, Aljundi et al. (2018) compute importance from sensitivity of model outputs to the
inputs. Other methods, such as PackNet (Mallya & Lazebnik, 2018) uses iterative pruning to fully
restrict gradient updates on important weights via binary mask, whereas HAT (Serra et al., 2018)
identifies important neurons by learning attention masks that control gradient propagation in the
individual parameters. Saha et al. (2020) using a PCA based pruning on activations (Garg et al.,
2020) partition the parametric space of the weights (filters) into core and residual (filter) spaces
after learning each task. The past knowledge is preserved in the frozen core space, whereas the
residual space is updated when learning the next task. In contrast to these methods, we do not
ascribe importance to or restrict the gradients of any individual parameters or filters. Rather we put
constraints on the ‘direction’ of gradient descent.
Memory-based methods: Methods under this class mitigate forgetting by either storing a subset
of (raw) examples from the past tasks in the memory for rehearsal (Robins, 1995; Rebuffi et al.,
2017; Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019a;b; Riemer et al., 2019) or synthesiz-
ing old data from generative models to perform pseudo-rehearsal (Shin et al., 2017). For instance,
2
Published as a conference paper at ICLR 2021
Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) avoids interference with previous
task by projecting the new gradients in the feasible region outlined by previous task gradients cal-
culated from the samples of episodic memory. Averaged-GEM (A-GEM) (Chaudhry et al., 2019a)
simplified this optimization problem to projection in one direction estimated by randomly selected
samples from the memory. Guo et al. (2020) propose a unified view of episodic memory-based CL
methods, that include GEM and A-GEM and improves performance over these methods utilizing
loss-balancing update rule. Additionally, Experience Replay (ER) (Chaudhry et al., 2019b) and
Meta-Experience Replay (MER) (Riemer et al., 2019) mitigate forgetting in online CL setup by
jointly training on the samples from new tasks and episodic memory. All these methods, however,
rely on the access to old data which might not be possible when users have concern over data pri-
vacy. Like all the memory-based methods we also use a storage unit which we call GPM. However,
we do not save any raw data in GPM, thus satisfy data privacy criterion.
Our method is closely related to recently proposed Orthogonal Gradient Descent (OGD) (Farajtabar
et al., 2020) and Orthogonal Weight Modulation (OWM) (Zeng et al., 2018). OGD stores a set of
gradient directions in the memory for each task and minimizes catastrophic forgetting by taking
gradient steps in the orthogonal directions for new tasks. In contrast to OGD, we compute and
store the bases of core gradient space from network representations (activations) which reduces
the memory requirement by orders of magnitude. Moreover, OGD is shown to work under locality
assumption for small learning rates which limits its scalability in learning longer task sequences with
complex dataset. Since our method does not use gradient directions (like OGD) to describe the core
gradient spaces, we do not need to obey such assumptions, thus can use higher learning rates. On the
other hand, OWM reduces forgetting by modifying the weights of the network in the orthogonal to
the input directions of the past tasks. This is achieved by multiplying new gradients with projector
matrices. These matrices are computed from the stored past projectors and the inputs with recursive
least square (RLS) method at each training step. However, such an iterative method not only slows
down the training process but also shows limited scalability in end-to-end task learning with modern
network architectures. Like OWM, we aim to encode new learning in the orthogonal to the old input
directions. In contrast to iterative projector computation in OWM, we identify a low-dimensional
subspace in the gradient space analyzing the learned representations with SVD in one-shot manner
at the end of each task. We store the bases of these subspaces in GPM and learn new tasks in the
orthogonal to these spaces to protect old knowledge. We quantitatively show that our method is
memory efficient, fast and scalable to deeper networks for complex long sequence of tasks.
3	Notations and Background
In this section, we introduce the notations used throughout the paper and give a brief overview of
SVD for matrix approximation. In section 4, we establish the relationship between input and gradient
spaces. In section 5 we show the steps of our algorithm that leverage such relationship.
Continual Learning: We consider supervised learning setup where T tasks are learned sequen-
tially. Each task has a task descriptor, τ ∈ {1, 2., T} with a corresponding dataset, Dτ =
{(xi,τ, yi,τ)in=τ 1} having nτ example pairs. Let’s consider an L layer neural network where at each
layer network computes the following function for task τ :
xli+,τ1 = σ(f(Wτl,xli,τ)).	(1)
Here, l = 1, ...L, σ(.) is a non-linear function and f(., .) is a linear function. We will use vector
notation for input (xi,τ) in fully connected layers and matrix notation for input (Xi,τ) in convolu-
tional layers. At the first layer, xi1,τ = xi,τ represents the raw input data from task τ, whereas in the
subsequent layers we define xli,τ as the representation of input xi,τ at layer l. Set of parameters of
the network is defined by, Wτ = {(Wτl)lL=1}, where W0 denotes set of parameters at initialization.
Matrix approximation with SVD: SVD can be used to factorize a rectangular matrix, A =
U ΣV T ∈ Rm×n into the product of three matrices, where U ∈ Rm×m and V ∈ Rn×n are
orthogonal, and Σ contains the sorted singular values along its main diagonal (Deisenroth et al.,
2020). If the rank of the matrix is r (r ≤ min(m, n)), A can be expressed as A = Pir=1 σiuiviT,
where ui ∈ U and vi ∈ V are left and right singular vectors and σi ∈ diag(Σ) are singular values.
Also, k-rank approximation to this matrix can be expressed as, Ak = Pik=1 σiuiviT, where k ≤ r
and its value can be chosen by the smallest k that satisfies ||Ak||2F ≥ th||A||2F. Here, ||.||F is the
Frobenius norm of the matrix and th (0 < th ≤ 1) is the threshold hyperparameter.
3
Published as a conference paper at ICLR 2021
Ci × k × k
-C
1T∑
X
PnZ
Input
(a) Forward PaSS
C0
	W	
Weight
C
Output
Weight Gradient
Co
	%L	
Figure 1: Illustration of convolution operation in matrix multiplication format during (a) Forward
Pass and (b) Backward Pass.
4	Input and Gradient S paces
Our algorithm leverages the fact that stochastic gradient descent (SGD) updates lie in the span of
input data points (Zhang et al., 2017). In the following subsections we will establish this relationship
for both fully connected and convolutional layers. The analysis presented in this section is generally
applicable to any layer of the network for any task, and hence we drop the task and layer identifiers.
4.1	Fully Connected Layer
Let’s consider a single layer linear neural network in supervised learning setup where each (input,
label) training data pair comes from a training dataset, D. Let, x ∈ Rn is the input vector, y ∈ Rm
is the label vector in the dataset and W ∈ Rm×n are the parameters (weights) of the network. The
network is trained by minimizing the following mean-squared error loss function
L =1 IIWx - y∣l2.	(2)
We can express gradient of this loss with respect to weights as
Vw L = (Wx — y)xT = δxT,	(3)
where δ ∈ Rm is the error vector. Thus, the gradient update will lie in the span of input (x),
where elements in δ scale the magnitude of x by different factors. Here, we have considered per-
example loss (batch size of 1) for simplicity. However, this relation also holds for mini-batch setting
(see appendix B.1). The input-gradient relation in equation 3 is generically applicable to any fully
connected layer of a neural network where x is the input to that layer and δ is the error coming from
the next layer. Moreover, this equation also holds for network with non-linear units (e.g. ReLU) and
cross-entropy losses except the calculation of δ will be different.
4.2	Convolutional Layer
Filters in a convolutional (Conv) layer operate in a different way on the inputs than the weights in
a fully connected (FC) layer. Let’s consider a Conv layer with the input tensor X ∈ RCi×hi×wi
and filters W ∈ RCo×Ci×k×k. Their convolution(X, W, *〉produces output feature map, O ∈
RCo×ho×wo (Liu et al., 2018). Here, Ci (Co) denotes the number of input (output) channels of the
Conv layer, hi, wi (ho, wo) denote the height and width of the input (output) feature maps and k is
the kernel size of the filters. As shown in Figure 1(a), ifX is reshaped into a (ho × wo) × (Ci × k × k)
matrix, X and W is reshaped into a (Ci × k × k) × Co matrix, W, then the convolution can be
expressed as matrix multiplication between X and W as O=X W, where O ∈ R(h0 ×w0)×Co. Each
row of X contains an input patch vector, Pj ∈ R(Ci×k×k)×1, where j = 1,2...,n (n = h * wc).
Formulation of convolution in terms of matrix multiplication provides an intuitive picture of the
gradient computation during backpropagation. Similar to the FC layer case, in Conv layer, during
backward pass an error matrix ∆ of size (h0 × w0) × Co (same size as O) is obtained from the next
layer. As shown in Figure 1(b), the gradient of loss with respect to filter weights is calculated by
VWL = XT∆,	(4)
where, VWL is of shape (Ci × k × k) × Co (same size as W). Since, columns ofXT are the input
patch vectors (p), the gradient updates of the convolutional filters will lie in the space spanned
by these patch vectors.
4
Published as a conference paper at ICLR 2021
5	Continual Learning with Gradient Projection Memory (GPM)
In this section, we describe our continual learning algorithm which leverages the relationship be-
tween gradient and input spaces to identify the core gradient spaces of the past tasks. We show how
gradient descent orthogonal to these spaces enable us to learn continually without forgetting.
Learning Task 1: We learn the first task (τ = 1) using dataset, D1 without imposing any constraint
on parameter updates. At the end of Task 1, we obtain a learned set of parameters W1. To preserve
the knowledge of the learned task, we impose constraints on the direction of gradient updates for the
next tasks. To do so, we partition the entire gradient space into two (orthogonal) subspaces: Core
Gradient Space (CGS) and Residual Gradient Space (RGS), such that gradient steps along CGS
induce high interference on the learned tasks whereas gradient steps along RGS have minimum to
no interference. We aim to find and store the bases of the CGS and take gradient steps orthogonal to
the CGS for the next task. In our formulation, each layer has its own CGS.
To find the bases, after learning Task 1 , for each layer we construct a representation matrix, Rl1 =
[xl1,1,xl2,1, ..., xlns,1] (for Conv layers Rl1 = [(X1l,1)T, (X2l,1)T, ..., (Xnls,1)T] ) concatenating ns
representations along the column obtained from forward pass ofns random samples from the current
training dataset through the network. Next, we perform SVD on Rl1 = U1lΣl1(V1l)T followed by its
k-rank approximation (Rl1)k according to the following criteria for the given threshold, lth :
ll(Rl)kIlF ≥ethl∣RlllF.	(5)
We define the space, Sl = span{ul1,1, ul2,1 , ..., ulk,1}, spanned by the first k vectors in U1l as the
space of significant representation for task 1 at layer l since it contains all the directions with
highest singular values in the representation. For the next task, we aim to take gradient steps in a
way that the correlation between this task specific significant representation and the weights in each
layer is preserved. Since, inputs span the space of gradient descent (section 4), the bases of Sl will
span a subspace in the gradient space which we define as the Core Gradient space (CGS). Thus
gradient descent along CGS will cause maximum change in the input-weight correlation whereas
gradient steps in the orthogonal directions to CGS (space of low representational significance) will
induce very small to no interference to the old tasks. We define this subspace orthogonal to CGS as
Residual Gradient space (RGS). We save the bases of the CGS in the memory, M = {(Ml)lL=1 },
where Ml = [ul1,1, ul2,1, ..., ulk,1]. We define this memory as Gradient Projection Memory (GPM).
Learning Task 2 to T: We learn task 2 with the examples from dataset D2 only. Before taking
gradient step, bases of the CGS are retrieved from GPM. New gradients (Vwι L2) are first projected
onto the CGS and then projected components are subtracted out from the new gradient so that
remaining gradient components lie in the space orthogonal to CGS. Gradients are updated as
FC Layer: VWl L2 = VWl L2 - (VWl L2 )Ml (Ml )T ,	(6)
Conv Layer: VWl L2 = VWl L2 - Ml(Ml)T (VWl L2).	(7)
At the end of the task 2 training, we update the GPM with new task-specific bases (of CGS). To
obtain such bases, we construct Rl2 = [xl1,2, xl2,2, ..., xlns,2] using data from task 2 only. However,
before performing SVD and subsequent k-rank approximation, from R2l we eliminate the common
directions (bases) that are already present in the GPM so that newly added bases are unique and
orthogonal to the existing bases in the memory. To do so, we perform the following step :
R2 = R2 - Ml (Ml )T (R2 )= R2 - Rl2,Proj.	(8)
Afterwards, SVD is performed on R2 (= U Σ2 (Vl) T) and k new orthogonal bases are chosen for
minimum value of k satisfying the following criteria for the given threshold, lth :
IR2,projIlF + II(R2)k∣lF ≥%I∣R2llF.	(9)
GPM is updated by adding new bases as Ml = [Ml, U^12,..., Uk 2]. Thus after learning each new
task, CGS grows and RGS becomes smaller, where maximum size of Ml (hence the dimension of
the gradient bases) is fixed by the choice of initial network architecture. Once the GPM update is
complete we move on to the next task and repeat the same procedure that we followed for task 2.
The pseudo-code of the algorithm is given in Algorithm 1 in the appendix.
5
Published as a conference paper at ICLR 2021
6	Experimental Setup
Datasets: We evaluate our continual learning algorithm on Permuted MNIST (PMNIST) (Le-
cun et al., 1998), 10-Split CIFAR-100 (Krizhevsky, 2009), 20-Spilt miniImageNet (Vinyals et al.,
2016) and sequence of 5-Datasets (Ebrahimi et al., 2020b). The PMNIST dataset is a variant of
MNIST dataset where each task is considered as a random permutation of the original MNIST
pixels. For PMNIST, we create 10 sequential tasks using different permutations where each task
has 10 classes (Ebrahimi et al., 2020a). The 10-Split CIFAR-100 is constructed by splitting 100
classes of CIFAR-100 into 10 tasks with 10 classes per task. Whereas, 20-Spilt miniImageNet, used
in (Chaudhry et al., 2019a), is constructed by splitting 100 classes of miniImageNet into 20 sequen-
tial tasks where each task has 5 classes. Finally, we use a sequence of 5-Datasets including CIFAR-
10, MNIST, SVHN (Netzer et al., 2011), notMNIST (Bulatov, 2011) and Fashion MNIST (Xiao
et al., 2017), where classification on each dataset is considered as a task. In our experiments we do
not use any data augmentation. The dataset statistics are given in Table 4 & 5 in the appendix.
Network Architecture: We use fully-connected network with two hidden layer of 100 units each
for PMNIST following Lopez-Paz & Ranzato (2017). For experiments with split CIFAR-100 we
use a 5-layer AlexNet similar to Serra et al. (2018). For split miniImageNet and 5-Datasets, similar
to Chaudhry et al. (2019b), we use a reduced ResNet18 architecture. No bias units are used and batch
normalization parameters are learned for the first task and shared with all the other tasks (follow-
ing Mallya & Lazebnik (2018)). Details on architectures are given in the appendix section C.2. For
permuted MNIST, we evaluate and compare our algorithm in ‘single-head’ setting (Hsu et al., 2018;
Farquhar & Gal, 2018) where all tasks share the final classifier layer and inference is performed
without task hint. For all other experiments, we evaluate our algorithm in ‘muti-head’ setting, where
each task has a separate classifier on which no gradient constraint is imposed during learning.
Baselines: We compare our method with state-of-the art approaches from both memory based
and regularization based methods that consider sequential task learning in fixed network archi-
tecture. From memory based approach, we compare with Experience Replay with reservoir sam-
pling (ER-Res) (Chaudhry et al., 2019b), Gradient Episodic Memory (GEM)(LoPez-PaZ & Ran-
zato, 2017), Averaged GEM (A-GEM) (Chaudhry et al., 2019a), Orthogonal Gradient Descent
(OGD) (Farajtabar et al., 2020) and Orthogonal Weight Modulation (OWM) (Zeng et al., 2018).
Moreover, We compare with Sate-of-the-art HAT (Serra et al., 2018) baseline and Elastic Weight
Consolidation (EWC) (Kirkpatrick et al., 2017) from regularization based methods. Additionally,
we add ‘multitask’ baseline where all the tasks are learned jointly using the entire dataset at once
in a single network. Multitask is not a continual learning strategy but will serve as upper bound
on average accuracy on all tasks. Details on the implementation along with the hyperparameters
considered for each of these baselines are provided in section C.4 and Table 6 in the appendix.
Training Details: We train all the models with plain stochastic gradient descent (SGD). For each
task in PMNIST and split miniImageNet we train the network for 5 and 10 epochs respectively with
batch size of 10. In Split CIFAR-100 and 5-Datasets experiments, we train each task for maximum
of 200 and 100 epochs respectively with the early termination strategy based on the validation loss
as proposed in Serra et al. (2018). For both datasets, batch size is set to 64. For GEM, A-GEM
and ER-Res the episodic memory size is chosen to be approximately the same size as the maximum
GPM size (GPM_Max). Calculation of GPM size is given in Table 7 in the appendix. Moreover,
selection of the threshold values (th) in our method is discussed in section C.5 in the appendix.
Performance Metrics: To evaluate the classification performance, we use the ACC metric, which
is the average test classification accuracy of all tasks. To measure the forgetting we report backward
transfer, BWT which indicates the influence of new learning on the past knowledge. For instance,
negative BWT indicates (catastrophic) forgetting. Formally, ACC and BWT are defined as:
1 T	1	T-1
ACC = TE Rτ,i, BWT = T-I E Rτ,i- Ri,i.	(10)
i=1	- i=1
Here, T is the total number of sequential tasks and RT,i is the accuracy of the model on ith task
after learning the Tth task sequentially (Lopez-Paz & Ranzato, 2017).
7	Results and Discussions
Single-head inference with PMNIST: First, we evaluate our algorithm in single-head setup for 10
sequential PMNIST tasks. In this setup task hint is not necessary. As HAT cannot perform infer-
6
Published as a conference paper at ICLR 2021
Figure 2: (a) Memory utilization and (b) per epoch training time for PMNIST tasks for different
methods. Memory utilization for different approaches for (c) CIFAR-100, (d) miniImageNet and (e)
5-Datasets tasks. For memory, size of GPM-Max and for time, method with highest complexity is
used as references (value of 1). All the other methods are reported relative to these references.
Table 1: Continual learning on different datasets. Methods that do not adhere to CL setup is indicated
by (*). All the results are (re) produced by us and averaged over 5 runs. Standard deviations are
reported in Table 8 and 9 in the appendix.
(a)	(b)
PMNIST	CIFAR-100	miniImageNet	5-Datasets
Methods	ACC (%)	BWT	Methods	ACC (%)	BWT	ACC (%)	BWT	ACC (%)	BWT
OGD	82.56	- 0.14	OWM	50.94	- 0.30	-	-	-	-
OWM	90.71	- 0.01	EWC	68.80	- 0.02	52.01	- 0.12	88.64	- 0.04
GEM	83.38	- 0.15	HAT	72.06	- 0.00	59.78	- 0.03	91.32	- 0.01
A-GEM	83.56	- 0.14	A-GEM	63.98	- 0.15	57.24	- 0.12	84.04	-0.12
ER_Res	87.24	- 0.11	ER-ReS	71.73	- 0.06	58.94	- 0.07	88.31	- 0.04
EWC	89.97	- 0.04	GPM (ours)	72.48	- 0.00	60.41	- 0.00	91.22	- 0.01
GPM (ours)	93.91	-0.03	Multitask*	79.58	-	69.46	-	91.54	-
Multitask* 96.70
ence without task hint, it is not included in the comparison. Since network size is very small (0.1M
parameters) with 87% parameters in the first layer, We choose threshold value (Cth) of 0.95 for that
layer and 0.99 for the other layers to ensure better learnability. From the results, shown in Table 1(a),
We observe that our method (GPM) achieves best average accuracy (93.91 ± 0.16%). In addition,
We achieve least amount of forgetting, except OWM, which essentially trades off accuracy to mini-
mize forgetting. Figure 2(a) compares the memory utilization of all the memory-based approaches.
While OWM, GEM, A-GEM and ER-Res use memory of size of GPM-Max, we obtain better per-
formance by using only 69% of the GPM_Max. Moreover, compared to OGD, we use about 400
times lower memory and achieve 〜10% better accuracy. In Figure 2(b), we compare the per epoch
training time of different memory based methods and found our method to be the fastest primarily
due to the precomputation of the reference gradient bases (of CGS). Additionally, in single-epoch
setting (Lopez-Paz & Ranzato, 2017), as shown in Table 8 in the appendix, we obtain best average
accuracy (91.74 ± 0.15%), which demonstrates the potential for our algorithm in online CL setup.
Split CIFAR-100: Next, we switch to multi-head setup which enables us to compare with strong
baselines such as HAT. For ten split CIFAR-100 tasks, as shown in Table 1(b), we outperform all the
memory based approaches while using 45% less memory (Figure 2(c)). We also outperform EWC
and our accuracy is marginally better than HAT while achieving zero forgetting. Also, we obtain
〜20% better accuracy than OWM, which have high forgetting (BWT=-0.30) thus demonstrating
its limited scalability to convolutional architectures.
Split miniImageNet: With this experiment, we test the scalability of our algorithm to deeper net-
work (ResNet18) for long task sequence from miniImageNet dataset. The average accuracies for
different methods after learning 20 sequential tasks are given in Table 1(b). Again, in this case
we outperform A-GEM, ER_ReS and EWC using 76% of the GPM-Max (Figure 2(d)). Also, we
achieve marginally better accuracy than HAT, however unlike HAT (and other methods) we com-
pletely avoid forgetting (BWT=0.00). Moreover, compared other methods sequential learning in our
method is more stable, which means accuracy of the past tasks have minimum to no degradation
over the course of learning (shown for task 1 accuracy in Figure 4 in the appendix).
5-Datasets: Next, we validate our approach on learning across diverse datasets, where classification
on each dataset is treated as one task. Even in this challenging setting, as shown in in Table 1(b),
7
Published as a conference paper at ICLR 2021
Table 2: Total wall-clock training time measured on a single GPU after learning all the tasks.
(a)		(b)			
Methods	Training Time [s]	Methods	Training Time [s]		
	PMNIST		CIFAR-100	miniImageNet	5-Datasets
OGD	1658	OWM	1856	-	-
OWM	396	EWC	1352	4138	7613
GEM	1639	HAT	1248	3077	7246
A-GEM ER-ReS EWC GPM (ours)	445 259 645 245	A-GEM	2678	6069	12077
		ER_Res	1147	2775	7015
		GPM (ours)	770	3387	5008
Table 3: Continual learning of 20-task from CIFAR-100 Superclass dataset. (f) denotes the result
reported from APD. (*) indicates the methods that do not adhere to CL setup. Single-task learning
(STL), where a separate network in trained for each task, serves as an upper bound on accuracy.
Metric	Methods					
	STLf*	PGNf	DENf	RCLf	APDf	GPM (ours)
ACC (%)	61.00	50.76	51.10	51.99	56.81	57.72
Capacity (%)	2000	271	191	184	130	100
We achieve better accuracy (91.22 ± 0.20%) then A-GEM, ER_Res and EWC utilizing 78% of the
GPM_Max (Figure 2(e)). Though, HAT performs marginally better than our method, both HAr and
We achieve the loWest BWT (-0.01). In this experiment, We have used tasks that are less related to
each other. After learning 5 such tasks 78% of the gradient space is already constrained. Which
implies, if the tasks are less or non-similar, GPM Will get populated faster and reach to its maximum
capacity after Which no neW learning Will be possible. Since, We use a fixed capacity netWork and
the size of GPM is determined by the netWork architecture, the ability of learning sequences of
hundreds of such tasks With our method Will be limited by the chosen netWork capacity.
Training Time. Table 2 shoWs the total training time for all the sequential tasks for different al-
gorithms. This includes time spent for memory management for the memory-based methods, fisher
importance calculation for EWC and learning activation masks for HAT. Details of time measure-
ment are given in appendix section C.6. For PMNIST, CIFAR-100, and 5-dataset tasks our algorithm
trains faster than all the other baselines While spending only 0.2%, 3% and 6% of its total training
time in GPM update (using SVD) respectively. Since each miniImageNet tasks are trained for only
10 epochs, our method have relatively higher overhead (30% of the total time) due to GPM update,
thus runs a bit slower than the fastest ER_Res. Overall, our formulation uses GPM bases and projec-
tion matrices of reasonable dimensions (see appendix section C.8); precomputation of Which at the
start of each task leads to fast per-epoch training. This gain in time essentially compensates for the
extra time required for the GPM update, which is done only once per task, enabling fast training.
Comparison with Expansion-based methods. To compare our method with the state-of-the-art ex-
pansion based methods we perform experiment with 20-task CIFAR-100 Superclass dataset (Yoon
et al., 2020). In this experiment, each task contains 5 different but semantically related classes from
CIFAR-100 dataset. Similar to APD, here we use the LeNet-5 architecture. Details of architec-
ture and training setup are given in appendix section C.3. Results are shown in Table 3 where ACC
represents average accuracy over 5 different task sequences (used in APD) and Capacity denotes
percentage of network capacity used with respect to the original network. We outperform all the
CL methods achieving best average accuracy (57.72 ± 0.37%) with BWT of -0.01 using the small-
est network. For instance, we outperform RCL and APD utilizing 84% and 30% fewer network
parameters respectively, which shows that our method induces more sharing between tasks.
Overall, we outperform the memory-based methods with less memory utilization, achieve better
accuracy than expansion-based methods using smaller network, and obtain better or on-par per-
formance compared to HAT in the given experimental setups. However, in the class-incremental
learning setup (Rebuffi et al., 2017), method (Kamra et al., 2017) that uses data replay achieves
better performance than GPM (see experiment in appendix section D.3). In this setup, we believe a
subset of old data replay either from storage or via generation is inevitable for attaining better per-
8
Published as a conference paper at ICLR 2021

6 4 2
4unou
—0.5	0.0	0.5
activation
-1.0
1.0
.0
1
O -
4 3 2
4unou
(彭 Aoe-Inuuq
(c)
ACC
BWT
O
Figure 3: Histograms of interference activations as a function of threshold, (th) at (a) Conv layer 2
(b) FC layer 2 for split CIFAR-100 tasks. (c) Impact of th on ACC (%) and BWT(%). With increas-
ing value of th, spread of interference reduces, which improves accuracy and reduces forgetting.
formance with minimal forgetting (Rajasegaran et al., 2019). In that quest, a hybrid approach such
as combining GPM with small data replay would be an interesting direction for future exploration.
Controlling Forgetting: Finally, we discuss the factors that implicitly or explicitly control the
amount of forgetting in our algorithm. As discussed in section 5, we propose to minimize interfer-
ence by taking gradient steps orthogonal to the CGS, where CGS bases are computed such that space
of significant representations of the past tasks can be well approximated by these bases. The degree
of this approximation is controlled by the threshold hyperparameter, th (through equation 5, 9). For
instance, a low value of th (closer to 0) would allow the optimizer to change the weights along the
directions where past data has higher representational significance, thereby significantly altering the
past input-weight correlation inducing (catastrophic) interference. On the other hand, a high value
of th (closer to 1) would preserve such correlation, however learnability of the new task might
suffer due to high volume of constraints in the gradient space. Therefore, in our continual learning
algorithm, th mediates the stability-plasticity dilemma. To show this analytically, let’s consider a
network after learning T sequential tasks with weights of the network at any layer, l expressed as :
T-1
WTl = W1l +	∆Wil→i+1 = W1l + ∆W1l→T .
(11)
i=1
Here, W1l is the weights after task 1 and ∆W1l→T is the change of weights from task 1 to T.
Weight update with our method ensures that ∆W1l→T lie in the orthogonal space of the data (rep-
resentations) of task 1. Linear operation at layer l with data from task 1 (x1) would produce:
WTl xl1 = W1lxl1 + ∆W1l→T xl1 . If ∆W1l→T xl1 = 0, then the output of the network for task 1 data
after learning task T will be the same as the output after learning task 1 (i.e. WTl xl1 = W1lxl1), that
means no interference for task 1. We define ∆W1l→T xl1 as the interference activation for task 1
at layer l (for any task, τ < T : ∆Wτl→T xlτ ). As discussed above, degree of such interference is
dictated by th. Figure 3(a)-(b) (and Figure 5 in appendix) show histograms (distributions) of inter-
ference activations at each layer of the network for split CIFAR-100 experiment. For lower value
of th, these distributions have higher variance (spread) implying high interference, whereas with
increasing value of th, the variance reduces around the (zero) mean value. As a direct consequence,
as shown in Figure 3(c), backward transfer reduces for increasing th with improvement in accuracy.
8	Conclusion
In this paper we propose a novel continual learning algorithm that finds important gradient sub-
spaces for the past tasks and minimizes catastrophic forgetting by taking gradient steps orthogonal
to these subspaces when learning a new task. We show how to analyse the network representations
to obtain minimum number of bases of these subspaces by which past information is preserved and
learnability for the new tasks is ensured. Evaluation on diverse image classification tasks with differ-
ent network architectures and comparisons with state-of-the-art algorithms show the effectiveness
of our approach in achieving high classification performance while mitigating forgetting. We also
show our algorithm is fast, makes efficient use of memory and is capable of learning long sequence
of tasks in deeper networks preserving data privacy.
Acknowledgments
This work was supported in part by the National Science Foundation, Vannevar Bush Faculty Fel-
lowship, Army Research Office, MURI, and by Center for Brain Inspired Computing (C-BRIC), one
of six centers in JUMP, a Semiconductor Research Corporation program sponsored by DARPA.
9
Published as a conference paper at ICLR 2021
References
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In The European Conference on Com-
Puter Vision (ECCV),pp.139-154, 2018.
Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for con-
tinual learning with orthogonal gradient descent. ArXiv, abs/2006.11942, 2020.
Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online], 2011. URL http:
//yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with A-GEM. In International Conference on Learning RePresentations, 2019a.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K.
Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic
memories. ArXiv, abs/1902.10486, 2019b.
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for Machine Learning.
Cambridge University Press, 2020.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In International Conference on Learning ReP-
resentations, 2020a.
Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adver-
sarial continual learning. In The EuroPean Conference on ComPuter Vision (ECCV), 2020b.
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for contin-
ual learning. ArXiv, abs/1910.07104, 2020.
Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. ArXiv,
abs/1805.09733, 2018.
Isha Garg, Priyadarshini Panda, and Kaushik Roy. A low effort approach to structured cnn design
using pca. IEEEAccess, 8:1347-1360, 2020.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. DeeP learning, volume 1.
MIT Press, 2016.
Yunhui Guo, Mingrui Liu, Tianbao Yang, and Tajana Rosing. Improved schemes for episodic
memory-based lifelong learning. In Advances in Neural Information Processing Systems 33,
2020.
Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learn-
ing scenarios: A categorization and case for strong baselines. In NeurIPS Continual learning
WorkshoP, 2018. URL https://arxiv.org/abs/1810.12488.
Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual
learning. ArXiv, abs/1710.10368, 2017.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the National Academy of Sciences, 114:3521 - 3526,
2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021.
10
Published as a conference paper at ICLR 2021
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual
structure learning framework for overcoming catastrophic forgetting. In Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research,pp. 3925-3934. PMLR, 09-15 Jun 2019.
Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong. Frequency-domain dynamic pruning
for convolutional neural networks. In Advances in Neural Information Processing Systems 31, pp.
1043-1053. 2018.
David Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, volume 30, 2017.
Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7765-
7773, 2018.
Michael Mccloskey and Neil J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. The Psychology ofLearning and Motivation, 24:104-169, 1989.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Jathushan Rajasegaran, Munawar Hayat, Salman H Khan, Fahad Shahbaz Khan, and Ling Shao.
Random path selection for continual learning. In Advances in Neural Information Processing
Systems, volume 32, pp. 12669-12679, 2019.
Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychological review, 97 2:285-308, 1990.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. iCaRL:
Incremental classifier and representation learning. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5533-5542, 2017.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. In International Conference on Learning Representations, 2019.
Mark B. Ring. Child: A first step towards continual learning. In Learning to Learn, 1998.
Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123-
146, 1995.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. ArXiv,
abs/1606.04671, 2016.
Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. Space: Structured compression and
sharing of representational space for continual learning. ArXiv, abs/2001.08650, 2020.
Syed Shakib Sarwar, Aayush Ankit, and Kaushik Roy. Incremental learning in deep convolutional
neural networks using partial network sharing. IEEEAccess, 8:4615^628, 2020.
Joan Serra, Dldac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings ofMachine Learning Research, pp. 4548T557.
PMLR, 10-15 Jul 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, volume 30, 2017.
Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics Auton. Syst., 15:25T6,
1995.
11
Published as a conference paper at ICLR 2021
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems, vol-
ume 29, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. ArXiv, abs/1708.07747, 2017.
Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Pro-
cessing Systems, VolUme 31, pp. 899-908, 2018.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. In 6th International Conference on Learning Representations, 2018.
Jaehong Yoon, Saehoon Kim, EUnho Yang, and SUng JU Hwang. Scalable and order-robUst con-
tinUal learning with additive parameter decomposition. In International Conference on Learning
Representations, 2020.
GUanxiong Zeng, Yang Chen, Bo CUi, and Shan YU. ContinUoUs learning of context-dependent
processing in neUral networks. ArXiv, abs/1810.01256, 2018.
Friedemann Zenke, Ben Poole, and SUrya GangUli. ContinUal learning throUgh synaptic intelli-
gence. In Proceedings of the 34th International Conference on Machine Learning, volUme 70 of
Proceedings of Machine Learning Research, pp. 3987-3995. PMLR, 06-11 AUg 2017.
ChiyUan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning reqUires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
12
Published as a conference paper at ICLR 2021
A Appendix
B	Algorithm
B.1	Input and Gradient Spaces (Cont.)
Since, the batch loss is the summation of the losses due to individual examples, the total batch loss
for n samples can be expressed as
nn
nn
Lbatch = X Li = X 2 IIWXi - yi I∣2.
(12)
i=1	i=1
The gradient of this loss with respect to weights can be expressed as
VW Lbatch = δlXT + δ2XT + ... + δnXT .
(13)
The gradient update will remain in the subspace spanned by the n input examples.
B.2	Algorithm Pseudo Code
Algorithm 1 Algorithm for Continual Learning with GPM
1:	function TRAIN (fw, Dtrain α, Qh )
2:	Initialize, Ml J [ ], for all l = 1,2,.…L // till L-1 if multi-head setting
3： Mj{(Ml)L=ι}
4:	W J W0
5:	for τ ∈ 1, 2,  , T do
6:	repeat
7:	Bn 〜DTrain // sample a mini-batch of size n from task T
8:	gradient, VWLτ J SGD(Bn, fW)
9:	VWLτ J PROJECT(VW Lτ, M) // see equation (6, 7)
10:	W J W - αVW Lτ
11:	until convergence
12:
13:	// Update Memory (GPM)
14:	Bns 〜DTrain // sample a mini-batch of size n from task T
15:	// construct representation matrices for each layer by forward pass (section 5)
16:	Rτ J forward(Bns, fW), where Rτ = {(Rτl )lL=1}
17:	for layer, l = 1, 2, ...L do
18:	Rlτ J PROJECT(RT, Ml) // see equation (8)
19:	UT j SVD(RT)
20:	k J Criteria(RIT, RT, ^Ith) // see equation (9)
21:	Ml J [Ml, UT[0： k]]
22:	end for
23:	end for
24:	return fW , M
25:	end function
C Experimental Details
C.1 Dataset Statistics
Table 4 and Table 5 show the summary of the datasets used in the experiments.
13
Published as a conference paper at ICLR 2021
Table 4: Dataset Statistics.
	PMNIST	Split CIFAR-100	Split-miniImageNet
num. of tasks	10	10	20
input size	1 × 28 × 28	3 × 32 × 32	3 × 84 × 84
# Classes/task	10	10	5
# Training samples/tasks	54,000	4,750	2,375
# Validation Samples/tasks	6,000	250	125
# Test samples/tasks	10,000	1,000	500
Table 5: 5-Datasets Statistics. For the datasets with monochromatic images, we replicate the image
across all RGB channels so that size of each image becomes 3 × 32 × 32.
	CIFAR-10	MNIST	SVHN	Fashion MNIST	notMNIST
# Classes	10	10	10	10	10
# Training samples	47,500	57,000	69,595	57,000	16,011
# Validation Samples	2,500	3,000	3,662	3,000	842
# Test samples	10,000	10,000	26,032	10,000	1,873
C.2 Architecture details
AlexNet-Hke architecture: This is the same architecture used by Serra et al. (2018) with batch
normalization added in each layer except the classifier layer. The network consists of3 convolutional
layers of 64, 128, and 256 filters with 4 × 4, 3 × 3, and 2 × 2 kernel sizes, respectively, plus two
fully connected layers of 2048 units each. Rectified linear units is used as activations, and 2 × 2
max-pooling after the convolutional layers. Dropout of 0.2 is used for the first two layers and 0.5
for the rest.
Reduced ResNet18 architecture: This is the similar architecture used by Lopez-Paz & Ranzato
(2017). For miniImageNet experiment, we use convolution with stride 2 in the first layer. For both
miniImageNet and 5-Datasets experiments we replace the 4 × 4 average-pooling before classifier
layer with 2 × 2 average-pooling.
All the networks use ReLU in the hidden units and softmax with cross entropy loss in the final layer.
C.3 CIFAR- 1 00 Superclass Experiment
For this experiment, similar to APD (Yoon et al., 2020), we use a modified LeNet-5 architecture
with 20-50-800-500 neurons. All the baseline results are reported from APD. Like APD, we do
not use any data augmentation or preprocessing. We keep 5% of training data from each task for
validation. We train the network with our algorithm with batch size of 64 and initial learning rate of
0.01. We Train each task for a maximum of 50 epochs with decay schedule and early termination
strategy similar to Serra et al. (2018). We use Eth = 0.98 for all the layers and increasing the value
of th by 0.001 for each new tasks.
C.4 Baseline Implementations
GEM (LoPez-Paz & Ranzato, 2017), A-GEM (Chaudhry et al., 2019a), ER-Res (Chaudhry et al.,
2019b) and OWM (Zeng et al., 2018) are implemented from their respective official implementa-
tions. EWC and HAT are implemented from the official implementation provided by Serra et al.
(2018). While, OGD is imPlemented from adaPting the code Provided by Bennani et al. (2020).
14
Published as a conference paper at ICLR 2021
C.5 Threshold hyperparameter
As discussed in section 5 and section 7, the threshold hyperparameter, th controls the degree of
interference through the approximation of space of significant representations of the past tasks.
Since in neural network, characteristics of learned representations vary for different architectures
and different dataset, using the same value of th may not be useful in capturing the similar space
of significance. In our experiments we use th in the range of 0.95 to 1. For PMNIST experiment,
as discussed in section 7, we use th = 0.95 in the first layer and 0.99 in the other layers. For split
CIFAR-100 experiment, we use th = 0.97 for all the layers and increasing the value of th by 0.003
for each new tasks. For split miniImageNet experiment, we use th = 0.985 for all the layers and
increasing the value of th by 0.0003 for each new tasks. For experiment with 5-Datasets, we use
th = 0.965 for all the layers across all the tasks.
C.6 Training Time Measurement
We measured per epoch training times (in Figure 2(b)) for computation in NVIDIA GeForce GTX
1060 GPU. For ten sequential tasks in PMNIST experiment, we computed per epoch training time
for each task and reported the average value over all the tasks.
Training time for different algorithms reported in Table 2(a) for PMNIST tasks were measured on
a Single NVIDIA GeForce GTX 1060 GPU. For all the other datasets, training time for different
algorithms reported in Table 2(b) were measured on a Single NVIDIA GeForce GTX 1080 Ti GPU.
C.7 list of hyperparameters
Table 6: List of hyperparameters for the baselines and our approach. Here, ‘lr’ represents (initial)
learning rate. In the table we represent PMNIST as ‘perm’, 10-Split CIFAR-100 as ‘cifar’, Split
miniImageNet as ‘minImg’ and 5-Datasets as ‘5data’.
Methods	Hyperparameters
OGD	lr: 0.001 (perm) # stored gradients : 200/task (perm)
OWM	lr : 0.01 (Cifar), 0.3(Perm)
GEM	lr : 0.1 (perm) memory size (samples) : 1000 (perm) memory strength, γ : 0.5 (perm)
A-GEM	lr : 0.05 (cifar), 0.1 (perm, minImg, 5data) memory size (samples) : 1000 (perm), 2000 (cifar), 500 (minImg), 3000 (5data)
ER_Res	lr : 0.05 (cifar), 0.1 (perm, minImg, 5data) memory size (samples) : 1000 (perm), 2000 (cifar), 500 (minImg), 3000 (5data)
EWC	lr : 0.03 (perm, minImg, 5data), 0.05 (cifar) regularization coefficient : 1000 (perm), 5000 (cifar, minImg, 5data)
HAT	lr : 0.03 (minImg), 0.05 (Cifar), 0.1 (5datα) smax : 400 (cifar, minImg, 5data) c : 0.75 (Cifar, minImg, 5data)
Multitask	lr : 0.05 (Cifar), 0.1 (perm, minImg, 5data)
GPM (ours)	lr : 0.01 (perm, Cifar), 0.1 (minImg, 5data) ns : 100 (minImg, 5data), 125 (Cifar), 300 (perm)
15
Published as a conference paper at ICLR 2021
C.8 GPM SIZE
As discussed in section 4, the gradient update will lie in the span of input vectors (x) in fully con-
nected layers, whereas the gradient updates of the convolutional filters will lie in the space spanned
by the input patch vectors (p). Therefore, each basis stored in the GPM for a particular layer, l will
have the same dimension as xl or pl . Thus, for any particular layer, GPM matrix, Ml can have a
maximum size of : size(xl) × size(xl) or size(pl) × size(pl). Maximum size of the GPM, which
We refer as GPM_Max, is computed by including GPM matrices (Ml) from all the layers. Thus the
size of GPM-Max is fixed by the choice of network architecture. In Table 7, we show the maximum
size of GPM matrix (Ml) for each layers for the architectures that We have used in our experiments
along with the size of GPM-Max.
Table 7: Size of GPM matrices for each layer for the architectures used in our experiments. Maxi-
mum sizes of the GPM in terms of number of parameters are also given.
Network	Size of maximum Ml	GPMJMax
		(parameters)
MLP	784 X 784, 100 X 100, 100 X 100	0.63M
(3 layers)		
AlexNet	48 x 48, 576 X 576, 512 X 512,	5.84M
(5 layers)	1024 X 1024, 2048 X 2048	
ResNet18	27 X 27, 180 X 180, 180 X 180, 180 X 180, 180 X 180,	
(17 layers+	180 X 180, 360 X 360, 20 X 20, 360 X 360, 360 X 360,	8.98M
3 short-cut	360 X 360, 720 X 720, 40 X 40, 720 X 720, 720 X 720,	
connections)	720 X 720, 1440 X 1440, 80 X 80, 1440 X 1440,	
	1440 X 1440	
D Additional Results
D. 1 Result Tables
Table 8 contains the additional results for PMNIST experiment in single-epoch setting along with
the standard deviation values for the results shown in Table 1(a) for multi-epoch (5 epoch) setting.
Method that does not adhere to CL setup is indicated by (*) in the table. Results are reported from
5 different runs.
Table 8: Continual learning on PMNIST in single-epoch and multi-epoch setting.
Methods	1 Epoch		5 Epochs	
	ACC (%)	BWT	ACC (%)	BWT
OGD	85.18 ± 0.29	-0.06 ± 0.00	82.56 ± 0.66	- 0.14 ± 0.01
OWM	90.55 ± 0.15	- 0.01 ± 0.00	90.71 ± 0.11	- 0.01 ± 0.00
GEM	88.65 ± 0.27	- 0.07 ± 0.00	83.38 ± 0.56	- 0.15 ± 0.01
A-GEM	87.80 ± 0.16	- 0.08 ± 0.00	83.56 ± 0.16	- 0.14 ± 0.00
ER-ReS	90.63 ± 0.27	- 0.05 ± 0.00	87.24 ± 0.53	- 0.11 ± 0.01
EWC	88.27 ± 0.39	- 0.04 ± 0.01	89.97 ± 0.57	- 0.04 ± 0.01
GPM (ours)	91.74 ± 0.15	- 0.03 ± 0.00	93.91 ± 0.16	-0.03 ± 0.00
Multitask*	95.21 ± 0.01	-	96.70 ± 0.02	-
16
Published as a conference paper at ICLR 2021
Table 9: Continual learning on different datasets along with the standard deviation values for the
results shown in Table 1(b).
Methods	CIFAR-100		miniImageNet		5-Datasets	
	ACC (%)	BWT	ACC (%)	BWT	ACC (%)	BWT
OWM	50.94 ± 0.60	- 0.30 ± 0.01	-	-	-	-
EWC	68.80 ± 0.88	- 0.02 ± 0.01	52.01 ± 2.53	- 0.12 ± 0.03	88.64 ± 0.26	- 0.04 ± 0.01
HAT	72.06 ± 0.50	- 0.00 ± 0.00	59.78 ± 0.57	- 0.03 ± 0.00	91.32 ± 0.18	- 0.01 ± 0.00
A-GEM	63.98 ± 1.22	- 0.15 ± 0.02	57.24 ± 0.72	- 0.12 ± 0.01	84.04 ± 0.33	-0.12 ± 0.01
ER-ReS	71.73 ± 0.63	- 0.06 ± 0.01	58.94 ± 0.85	- 0.07 ± 0.01	88.31 ± 0.22	- 0.04 ± 0.00
GPM (ours)	72.48 ± 0.40	- 0.00 ± 0.00	60.41 ± 0.61	- 0.00 ± 0.00	91.22 ± 0.20	- 0.01 ± 0.00
Multitask*	79.58± 0.54	-	69.46 ± 0.62	-	91.54 ± 0.28	-
D.2 k VALUES
Table 10 (a) and (b) show the number of new bases added at each layer per PMNIST and 10-split
CIFAR-100 task respectively. Total number of bases in the GPM after learning all the tasks is also
given.
Table 10: Number of new bases (k) added to the GPM at different layers after each (a) PMNIST
task and (b) 10-split CIFAR-100 task (for a random seed configuration).
(a)	____________________________(b)_________________________
k	k
Task ID	FC1	FC2	FC3	Task ID	th	Conv1	Conv2	Conv3	FC1	FC2
1	81	60	41	1	0.970	7	125	197	80	98
2	71	22	19	2	0.973	3	44	74	81	101
3	70	11	11	3	0.976	0	20	29	71	93
4	61	4	7	4	0.979	1	21	26	76	99
5	57	2	4	5	0.982	2	33	28	73	98
6	50	1	2	6	0.985	0	16	19	71	98
7	46	0	2	7	0.988	0	16	14	71	99
8	41	0	1	8	0.991	2	41	26	76	104
9	35	0	1	9	0.994	6	56	34	84	109
10	31	0	0	10	0.997	1	45	26	86	113
Total	543	100	88		Total	22	417	473	769	1012
Table 11: Continual learning of Digit dataset tasks in class-incremental learning setup (Kamra et al.,
2017).(f) denotes the result reported from DGDMN.
Metric	Methods EWCt	DGRt	DGDMNt GPM (ours)
ACC (%) BWT	10.00	59.60	81.80	70.67 - 1.00	- 0.43	- 0.15	- 0.26
D.3 Class-Incremental Learning
In this section, we evaluate our algorithm in a class-incremental learning setup (Rebuffi et al., 2017),
where disjoint classes are learned one by one and classification is performed within all the learned
classes without task hint (Kamra et al., 2017). This setup is different (Hsu et al., 2018) from the
(single-head/multi-head) evaluation setups used throughout this paper. Also, this scenario is very
challenging and often infeasible for the regularization (HAT, EWC etc.) and expansion-based (DEN,
APD etc.) methods which do not use old data replay. Using the experimental setting similar to
DGDMN (Kamra et al., 2017), we implemented the ‘Digit dataset’ experiment where a single class
17
Published as a conference paper at ICLR 2021
of MNIST digit is learned per task. Results are listed in Table 11, where the baselines are reported
from DGDMN. While EWC forgets catastrophically, we perform better than DGR (Shin et al.,
2017), which employs data replay through old data generation. DGDMN, an improved data replay
method, outperforms all. In this setup, we believe a subset of old data replay either from storage or
via generation is inevitable for attaining better performance with minimal forgetting (Rebuffi et al.,
2017; Rajasegaran et al., 2019).
D.4 Additional Plots
Oooo
7 6 5 4
(<⅛) ADSnDDV
——EWC
——HAT
——A-GEM
--ER_Res
——GPM
20
Tasks
Figure 4:	Evolution of task 1 accuracy over the course of incremental learning of 20 sequential tasks
from miniImageNet dataset. Learned accuracy in our method remains stable throughout learning.
4unou
-0.5	0.0	0.5	1.0
(a)
8 6 4 2
εth=0.5
εth=0.7
εth=0.8
二 εth=0.9
εth=0.95
activation
Figure 5:	Illustration of how threshold hyperparameter controls the degree of interference at (a)
Conv layer 1 (b) Conv layer 3 (c) FC layer 1 with the histogram plots of interference activations
from Split CIFAR-100 experiment. With increasing th, spread of the inference activation decreases
resulting in minimization of forgetting.
18