Published as a conference paper at ICLR 2021
Co-Mixup: Saliency Guided Joint Mixup with
Supermodular Diversity
Jang-Hyun Kim, Wonho Choo, Hosan Jeong, Hyun Oh Song
Department of Computer Science and Engineering, Seoul National University
Neural Processing Research Center
{janghyun,wonho.choo,grazinglion,hyunoh}@mllab.snu.ac.kr
Abstract
While deep neural networks show great performance on fitting to the training
distribution, improving the networks’ generalization performance to the test
distribution and robustness to the sensitivity to input perturbations still
remain as a challenge. Although a number of mixup based augmentation
strategies have been proposed to partially address them, it remains unclear
as to how to best utilize the supervisory signal within each input data for
mixup from the optimization perspective. We propose a new perspective on
batch mixup and formulate the optimal construction of a batch of mixup
data maximizing the data saliency measure of each individual mixup data
and encouraging the supermodular diversity among the constructed mixup
data. This leads to a novel discrete optimization problem minimizing the
difference between submodular functions. We also propose an efficient
modular approximation based iterative submodular minimization algorithm
for efficient mixup computation per each minibatch suitable for minibatch
based neural network training. Our experiments show the proposed method
achieves the state of the art generalization, calibration, and weakly super-
vised localization results compared to other mixup methods. The source
code is available at https://github.com/snu- mllab/Co- Mixup.
1	Introduction
Deep neural networks have been applied to a wide range of artificial intelligence tasks such
as computer vision, natural language processing, and signal processing with remarkable
performance (Ren et al., 2015; Devlin et al., 2018; Oord et al., 2016). However, it has been
shown that neural networks have excessive representation capability and can even fit random
data (Zhang et al., 2016). Due to these characteristics, the neural networks can easily overfit
to training data and show a large generalization gap when tested on previously unseen data.
To improve the generalization performance of the neural networks, a body of research has
been proposed to develop regularizers based on priors or to augment the training data
with task-dependent transforms (Bishop, 2006; Cubuk et al., 2019). Recently, a new task-
independent data augmentation technique, called mixup, has been proposed (Zhang et al.,
2018). The original mixup, called Input Mixup, linearly interpolates a given pair of input data
and can be easily applied to various data and tasks, improving the generalization performance
and robustness of neural networks. Other mixup methods, such as manifold mixup (Verma
et al., 2019) or CutMix (Yun et al., 2019), have also been proposed addressing different ways
to mix a given pair of input data. Puzzle Mix (Kim et al., 2020) utilizes saliency information
and local statistics to ensure mixup data to have rich supervisory signals.
However, these approaches only consider mixing a given random pair of input data and do
not fully utilize the rich informative supervisory signal in training data including collection
of ob ject saliency, relative arrangement, etc. In this work, we simultaneously consider mix-
matching different salient regions among all input data so that each generated mixup example
accumulates as many salient regions from multiple input data as possible while ensuring
Correspondence to: Hyun Oh Song.
1
Published as a conference paper at ICLR 2021
Input Mixup
WeaSel (0.6)
Deer (0.4)
Input batch
Random sampled pair
CutMix	Puzzle Mix
Co-Mixup
Weasel (0.8)
Deer (0.2)
Weasel (0.5)	Bird (0.4)
Deer (0.5)	Fish (0.6)
Bird (0.4)
Dog (0.6)
Shark (1.0)
Bird (0.2)
Dog (0.3)
Fish (0.5)
Figure 1: Example comparison of existing mixup methods and the proposed Co-Mixup. We
provide more samples in Appendix H.
diversity among the generated mixup examples. To this end, we propose a novel optimization
problem that maximizes the saliency measure of each individual mixup example while
encouraging diversity among them collectively. This formulation results in a novel discrete
submodular-supermodular ob jective. We also propose a practical modular approximation
method for the supermodular term and present an efficient iterative submodular minimization
algorithm suitable for minibatch-based mixup for neural network training. As illustrated in
the Figure 1, while the proposed method, Co-Mixup, mix-matches the collection of salient
regions utilizing inter-arrangements among input data, the existing methods do not consider
the saliency information (Input Mixup & CutMix) or disassemble salient parts (Puzzle Mix).
We verify the performance of the proposed method by training classifiers on CIFAR-100,
Tiny-ImageNet, ImageNet, and the Google commands dataset (Krizhevsky et al., 2009;
Chrabaszcz et al., 2017; Deng et al., 2009; Warden, 2017). Our experiments show the models
trained with Co-Mixup achieve the state of the performance compared to other mixup
baselines. In addition to the generalization experiment, we conduct weakly-supervised ob ject
localization and robustness tasks and confirm Co-Mixup outperforms other mixup baselines.
2	Related works
Mixup Data augmentation has been widely used to prevent deep neural networks from
over-fitting to the training data (Bishop, 1995). The ma jority of conventional augmentation
methods generate new data by applying transformations depending on the data type or
the target task (Cubuk et al., 2019). Zhang et al. (2018) proposed mixup, which can be
independently applied to various data types and tasks, and improves generalization and
robustness of deep neural networks. Input mixup (Zhang et al., 2018) linearly interpolates
between two input data and utilizes the mixed data with the corresponding soft label for
training. Following this work, manifold mixup (Verma et al., 2019) applies the mixup in the
hidden feature space, and CutMix (Yun et al., 2019) suggests a spatial copy and paste based
mixup strategy on images. Guo et al. (2019) trains an additional neural network to optimize
a mixing ratio. Puzzle Mix (Kim et al., 2020) proposes a mixup method based on saliency
and local statistics of the given data. In this paper, we propose a discrete optimization-based
mixup method simultaneously finding the best combination of collections of salient regions
among all input data while encouraging diversity among the generated mixup examples.
Saliency The seminal work from Simonyan et al. (2013) generates a saliency map using a
pre-trained neural network classifier without any additional training of the network. Following
the work, measuring the saliency of data using neural networks has been studied to obtain a
more precise saliency map (Zhao et al., 2015; Wang et al., 2015) or to reduce the saliency
computation cost (Zhou et al., 2016; Selvaraju et al., 2017). The saliency information
is widely applied to the tasks in various domains, such as ob ject segmentation or speech
recognition (Jung and Kim, 2011; Kalinli and Narayanan, 2007).
2
Published as a conference paper at ICLR 2021
Submodular-Supermodular optimization A submodular (supermodular) function is a
set function with diminishing (increasing) returns property (Narasimhan and Bilmes, 2005). It
is known that any set function can be expressed as the sum of a submodular and supermodular
function (Lovasz, 1983), called BP function. Various problems in machine learning can be
naturally formulated as BP functions (Fujishige, 2005), but it is known to be NP-hard (Lovasz,
1983). Therefore, approximate algorithms based on modular approximations of submodular
or supermodular terms have been developed (Iyer and Bilmes, 2012). Our formulation falls
into a category of BP function consisting of smoothness function within a mixed output
(submodular) and a diversity function among the mixup outputs (supermodular).
3	Preliminary
Existing mixup methods return {h(x1, xi(1)), . . . , h(xm, xi(m))} for given input data
{x1 , . . . , xm}, where h : X × X → X is a mixup function and (i(1), . . . , i(m)) is a ran-
dom permutation of the data indices. In the case of input mixup, h(x, x0) is λx + (1 - λ)x0,
where λ ∈ [0, 1] is a random mixing ratio. Manifold mixup applies input mixup in the hidden
feature space, and CutMix uses h(x, x0) = 1B x + (1 - 1B) x0, where 1B is a binary
rectangular-shape mask for an image x and represents the element-wise product. Puzzle
Mix defines h(x, x0) as Z Θ Πlx + (1 一 Z) Θ Π0lx0, where Π is a transport plan and Z is a
discrete mask. In detail, for X ∈ Rn, Π ∈ {0,1}n and Z ∈ Ln for L = {L | l = 0,1,..., L}.
In this work, we extend the existing mixup functions as h : Xm → Xm0 which performs
mixup on a collection of input data and returns another collection. Let xB ∈ Rm×n denote
the batch of input data in matrix form. Then, our proposed mixup function is
h(xB) = g(Z1 ΘxB), . . . ,g(Zm0 ΘxB) ,
where Zj ∈ Lm×n for j = 1,..., m0 with L = {L | l = 0,1,...,L} and g : Rm×n → Rn
returns a column-wise sum of a given matrix. Note that, the kth column of Zj , denoted as
Zj,k ∈ Lm , can be interpreted as the mixing ratio among m inputs at the kth location. Also,
we enforce kZj,k k1 = 1 to maintain the overall statistics of the given input batch. Given
the one-hot target labels yB ∈ {0, 1}m×C of the input data with C classes, we generate soft
target labels for mixup data as yB Oj for j = 1,...,m0, where O =1 Pk=IZj,k ∈ [0,1]m
represents the input source ratio of the j th mixup data. We train models to estimate the
soft target labels by minimizing the cross-entropy loss.
4	Method
4.1	Objective
Saliency Our main ob jective is to maximize the saliency measure of mixup data while
maintaining the local smoothness of data, i.e., spatially nearby patches in a natural image
look similar, temporally adjacent signals have similar spectrum in speech, etc. (Kim et al.,
2020). As we can see from CutMix in Figure 1, disregarding saliency can give a misleading
supervisory signal by generating mixup data that does not match with the target soft label.
While the existing mixup methods only consider the mixup between two inputs, we generalize
the number of inputs m to any positive integer. Note, each kth location of outputs has m
candidate sources from the inputs. We model the unary labeling cost as the negative value
of the saliency, and denote the cost vector at the kth location as ck ∈ Rm . For the saliency
measure, we calculate the gradient values of training loss with respect to the input and
measure `2 norm of the gradient values across input channels (Simonyan et al., 2013; Kim
et al., 2020). Note that this method does not require any additional architecture dependent
modules for saliency calculation. In addition to the unary cost, we encourage adjacent
locations to have similar labels for the smoothness of each mixup data. In summary, the
objective can be formulated as follows:
m0 n	m0	m0 n
ΣΣck| Zj,k+β	(1 一 Zj|,kZj,k0 ) 一 ηΣΣlog p(Zj,k),
j=1k=1	j=1 (k,k0)∈N	j=1 k=1
3
Published as a conference paper at ICLR 2021
9
n
1.5
>
1.0
0.5
Diverse z* Salient
but not salient but not diverse
0
4
stnuoC
0
12
Number
small
large
3456
of mixed inputs
4
3
2
1
0
ycneilas hctaB
no-mix CutMix Co-Mixup
Input PuzzleMix
1.0
X
0.8
S
Φ
0.6
0.4
0 0.2 0.4 0.6 0.8 1.0
τ
O □
D D
T
T
(a)	(b)	(c)	(d)
Figure 2: (a) Analysis of our BP optimization problem. The x-axis is a one-dimensional
arrangement of solutions: The mixed output is more salient but not diverse towards the
right and less salient but diverse on the left. The unary term (red) decreases towards the
right side of the axis, while the supermodular term (green) increases. By optimizing the sum
of the two terms (brown), We obtain the balanced output z*. (b) A histogram of the number
of inputs mixed for each output given a batch of 100 examples from the ImageNet dataset.
As τ increases, more inputs are used to create each output on average. (c) Mean batch
saliency measurement of a batch of mixup data using the ImageNet dataset. We normalize
the saliency measure of each input to sum up to 1. (d) Diversity measurement of a batch of
mixup data. We calculate the diversity as 1 一 Ej Ej,=j Joj0/m, where Oj = Oj/∣∣θj∣∣ι∙ We
can control the diversity among Co-Mixup data (red) and find the optimum by controlling τ.
where the prior P is given by zj,k 〜 L Multi(L, λ) with λ = (λι,...,λm) 〜
Dirichlet(α, . . . , α), which is a generalization of the mixing ratio distribution of Zhang
et al. (2018), and N denotes a set of adjacent locations (i.e., neighboring image patches in
vision, subsequent spectrums in speech, etc.).
Diversity Note that the naive generalization above leads to the identical outputs because
the ob jective is separable and identical for each output. In order to obtain diverse mixup
outputs, we model a similarity penalty between outputs. First, we represent the input source
information of the jth output by aggregating assigned labels as kn=1 zj,k . For simplicity,
let us denote kn=1 zj,k as Oj . Then, we measure the similarity between Oj ’s by using the
inner-product on Rm . In addition to the input source similarity between outputs, we model
the compatibility between input sources, represented as a symmetric matrix Ac ∈ R+m×m .
Specifically, Ac[i1, i2] quantifies the degree to which input i1 and i2 are suitable to be
mixed together. In summary, we use inner-product on A = (1 一 ω)I + ωAc for ω ∈ [0, 1],
resulting in a supermodular penalty term. Note that, by minimizing hOj, Oj0iA = Oj|AOj0 ,
∀j 6= j0 , we penalize output mixup examples with similar input sources and encourage each
individual mixup examples to have high compatibility within. In this work, we measure
the distance between locations of salient ob jects in each input and use the distance matrix
Ac[i,j] = kargmaxk si [k] 一 argmaxk sj [k]k1, where si is the saliency map of the ith input and
k is a location index (e.g., k is a 2-D index for image data). From now on, we denote this
inner-product term as the compatibility term.
Over-penalization The conventional mixup methods perform mixup as many as the
number of examples in a given mini-batch. In our setting, this is the case when m = m0 .
However, the compatibility penalty between outputs is influenced by the pigeonhole principle.
For example, suppose the first output consists of two inputs. Then, the inputs must be used
again for the remaining m0 一 1 outputs, or only m 一 2 inputs can be used. In the latter case,
the number of available inputs (m 一 2) is less than the outputs (m0 一 1), and thus, the same
input must be used more than twice. Empirically, we found that the remaining compatibility
term above over-penalizes the optimization so that a substantial portion of outputs are
returned as singletons without any mixup. To mitigate the over-penalization issue, we apply
clipping to the compatibility penalty term. Specifically, we model the objective so that no
extra penalty would occur when the compatibility among outputs is below a certain level.
4
Published as a conference paper at ICLR 2021
Now we present our main objective as following:
z* =	argmin	f (z),
zj,k ∈Lm , kzj,k k1 =1
where
m0 n	m0
f(z):=XXck|zj,k +βX X (1 - zj|,k zj,k0 )	(1)
j=1 k=1	j=1 (k,k0)∈N
{m0 m0 n n ∣ 1	/ n ∖ I	m0 n
τ, XX Xzj,k	A X zj0,k	-η XX
logp(zj,k).
j=1 j0 6=j k=1	k=1	 j=1 k=1
、--------------------{--------------------}
=fc (z)
In Figure 2, we describe the properties of the BP optimization problem of Equation (1)
and statistics of the resulting mixup data. Next, we verify the supermodularity of the
compatibility term. We first extend the definition of the submodularity of a multi-label
function as follows (Windheuser et al., 2012).
Definition 1. For a given label set L, a function s : Lm × Lm → R is pairwise submodular,
if ∀x, x0 ∈ Lm, s(x, x) + s(x0, x0) ≤ s(x, x0) + s(x0, x). A function s is pairwise supermodular,
if -s is pairwise submodular.
Proposition 1. The compatibility term fc in Equation (1) is pairwise supermodular for
every pair of (zj1 ,k , zj2 ,k ) if A is positive semi-definite.
Proof. See Appendix B.1.	□
Finally note that, A = (1 - ω)I + ωAc, where Ac is a symmetric matrix. By using
spectral decomposition, Ac can be represented as UDU| , where D is a diagonal matrix
and U|	U =	UU|	=	I.	Then, A = U((1 -	ω)I	+ ωD)U |	, and thus for small ω >	0,	we can
guarantee A to be positive semi-definite.
4.2	Algorithm
Our main objective consists of modular (unary, prior), submodular (smoothness), and super-
modular (compatibility) terms. To optimize the main objective, we employ the submodular-
supermodular procedure by iteratively approximating the supermodular term as a modular
function (Narasimhan and Bilmes, 2005). Note that zj represents the labeling of the jth out-
put and oj represents the aggregated input source information of the jth output, kn=1 zj,k.
Before introducing our algorithm, we first inspect the simpler case without clipping.
Proposition 2. The compatibility term fc without clipping is modular with respect to zj .
Proof. Note, A is a positive symmetric matrix by the definition. Then, for an index
j0 , we can represent fc without clipping in terms of oj0 as Pjm=1 Pjm0=1 j0 6=j oj| Aoj0 =
0	00	0
2 Pj=1,j=j0 oj Aoj0 +Pj = 1,j=j0 Pj0 = 1,j0∈{jo,j} 0j Aθj0 = (2 Pj = 1,j=j0 Aoj)loj0 +c = vjooj0 +
c, where v-j0 ∈ Rm and c ∈ R are values independent with oj0 . Finally, v-|j oj0 + c =
En=I VjOzjo,k + C is a modular function of zj0.	□
By Proposition 2, we can apply a submodular minimization algorithm to optimize the
objective with respect to zj when there is no clipping. Thus, we can optimize the main
objective without clipping in coordinate descent fashion (Wright, 2015). For the case with
clipping, we modularize the supermodular compatibility term under the following criteria:
1. The modularized function value should increase as the compatibility across outputs
increases.
2. The modularized function should not apply an extra penalty for the compatibility
below a certain level.
5
Published as a conference paper at ICLR 2021
2. Perform modularization of Equation (1) with respect to zj
Input batch
Mixed output batch
1. Compute
modular
approximated
diversity
for zj
Figure 3: Visualization of the proposed mixup procedure. For a given batch of input data
(left), a batch of mixup data (right) is generated, which mix-matches different salient regions
among the input data while preserving the diversity among the mixup examples. The
histograms on the right represent the input source information of each mixup data (oj).
Borrowing the notation from the proof in Proposition 2, for an index j, fc(z) = max{τ, v-|j oj +
c} = max{τ - c, v-|j oj } + c. Note, oj = kn=1 zj,k represents the input source information of
the jth output and v-j = 2 Pjm0=1,j0 6=j Aoj0 encodes the status of the other outputs. Thus,
we can interpret the supermodular term as a penalization of each label of oj in proportion to
the corresponding v-j value (criterion 1), but not for the compatibility below τ - c (criterion
2). As a modular function which satisfies the criteria above, we use the following function:
fc(z) ≈ max{τ 0, v-j }|oj for ∃τ0 ∈ R.	(2)
Note that, by satisfying the criteria above, the modular function reflects the diversity and
over-penalization desiderata described in Section 4.1. We illustrate the proposed mixup
procedure with the modularized diversity penalty in Figure 3.
Proposition 3. The modularization given by Equation (2) satisfies the criteria above.
Proof. See Appendix B.2.
□
By applying the modular approximation de-
scribed in Equation (2) to fc in Equation (1),
we can iteratively apply a submodular min-
imization algorithm to obtain the final so-
lution as described in Algorithm 1. In de-
tail, each step can be performed as follows:
1) Conditioning the main objective f on
the current values except zj , denoted as
fj(zj) = f(zj;z1:j-1,zj+1:m0).	2) Modu-
larization of the compatibility term of fj
as Equation (2), resulting in a submodular
function fj. We denote the modularization
operator as Φ, i.e., fj = Φ(fj). 3) Applying
a submodular minimization algorithm to fj .
Please refer to Appendix C for implementa-
tion details.
Algorithm 1 Iterative submodular minimiza-
tion_______________________________________
Initialize z as z(0).
Let z(t) denote a solution of the tth step.
Φ: modularization operator based on Equa-
tion (2).
for t = 1, . . . , T do
for j = 1, . . . , m0 do
fj( ) (zj) := f (zj ; z1(:j)-1, zj(+-1:m) 0).
严= φf,).
Solve Zjt) = argmin Wt)(Zj)
end for
end for
return Z(T)
Analysis Narasimhan and Bilmes (2005) proposed a modularization strategy for general
supermodular set functions, and apply a submodular minimization algorithm that can
monotonically decrease the original BP objective. However, the proposed Algorithm 1
based on Equation (2) is much more suitable for minibatch based mixup for neural network
training than the set modularization proposed by Narasimhan and Bilmes (2005) in terms of
complexity and modularization variance due to randomness. For simplicity, let us assume
6
Published as a conference paper at ICLR 2021
each zj,k is an m-dimensional one-hot vector. Then, our problem is to optimize m0n one-hot
m-dimensional vectors.
To apply the set modularization method, we need to assign each possible value of zj,k as an
element of {1, 2, . . . , m}. Then the supermodular term in Equation (1) can be interpreted as
a set function with m0nm elements, and to apply the set modularization, O(m0nm) sequential
evaluations of the supermodular term are required. In contrast, Algorithm 1 calculates v-j
in Equation (2) in only O(m0) time per each iteration. In addition, each modularization step
of the set modularization method requires a random permutation of the m0nm elements. In
this case, the optimization can be strongly affected by the randomness from the permutation
step. As a result, the optimal labeling of each zj,k from the compatibility term is strongly
influenced by the random ordering undermining the interpretability of the algorithm. Please
refer to Appendix D for empirical comparison between Algorithm 1 and the method by
Narasimhan and Bilmes (2005).
5 Experiments
We evaluate our proposed mixup method on generalization, weakly supervised ob ject local-
ization, calibration, and robustness tasks. First, we compare the generalization performance
of the proposed method against baselines by training classifiers on CIFAR-100 (Krizhevsky
et al., 2009), Tiny-ImageNet (Chrabaszcz et al., 2017), ImageNet (Deng et al., 2009), and the
Google commands speech dataset (Warden, 2017). Next, we test the localization performance
of classifiers following the evaluation protocol of Qin and Kim (2019). We also measure
calibration error (Guo et al., 2017) of classifiers to verify Co-Mixup successfully alleviates
the over-confidence issue by Zhang et al. (2018). In Section 5.4, we evaluate the robustness
of the classifiers on the test dataset with background corruption in response to the recent
problem raised by Lee et al. (2020) that deep neural network agents often fail to generalize
to unseen environments. Finally, we perform a sensitivity analysis of Co-Mixup and provide
the results in Appendix F.3.
5.1	Classification
We first train PreActResNet18 (He et al., 2016), WRN16-8 (Zagoruyko and Komodakis,
2016), and ResNeXt29-4-24 (Xie et al., 2017) on CIFAR-100 for 300 epochs. We use stochastic
gradient descent with an initial learning rate of 0.2 decayed by factor 0.1 at epochs 100
and 200. We set the momentum as 0.9 and add a weight decay of 0.0001. With this setup,
we train a vanilla classifier and reproduce the mixup baselines (Zhang et al., 2018; Verma
et al., 2019; Yun et al., 2019; Kim et al., 2020), which we denote as Vanilla, Input, Manifold,
CutMix, Puzzle Mix in the experiment tables. Note that we use identical hyperparameters
regarding Co-Mixup over all of the experiments with different models and datasets, which
are provided in Appendix E.
Table 1 shows Co-Mixup significantly outperforms all other baselines in Top-1 error rate.
Co-Mixup achieves 19.87% in Top-1 error rate with PreActResNet18, outperforming the best
baseline by 0.75%. We further test Co-Mixup on different models (WRN16-8 & ResNeXt29-
4-24) and verify Co-Mixup improves Top-1 error rate over the best performing baseline.
Dataset (Model)	Vanilla	Input	Manifold	CutMix	Puzzle Mix	Co-Mixup
CIFAR-100 (PreActResNet18)	23.59	22.43	21.64	21.29	20.62	19.87
CIFAR-100 (WRN16-8)	21.70	20.08	20.55	20.14	19.24	19.15
CIFAR-100 (ResNeXt29-4-24)	21.79	21.70	22.28	21.86	21.12	19.78
Tiny-ImageNet (PreActResNet18)	43.40	43.48	40.76	43.11	36.52	35.85
ImageNet (ResNet-50, 100 epochs)	24.03	22.97	23.30	22.92	22.49	22.39
Google commands (VGG-11)	4.84	3.91	3.67	3.76	3.70	3.54
Table 1: Top-1 error rate on various datasets and models. For CIFAR-100, we train each
model with three different random seeds and report the mean error.
We further test Co-Mixup on other datasets; Tiny-ImageNet, ImageNet, and the Google
commands dataset (Table 1). For Tiny-ImageNet, we train PreActResNet18 for 1200 epochs
7
Published as a conference paper at ICLR 2021
Figure 4: Confidence-Accuracy plots for classifiers on CIFAR-100. From the figure, the
Vanilla network shows over-confident predictions, whereas other mixup baselines tend to have
under-confident predictions. We can find that Co-Mixup has best-calibrated predictions.
following the training protocol of Kim et al. (2020). As a result, Co-Mixup consistently
improves Top-1 error rate over baselines by 0.67%. In the ImageNet experiment, we follow
the experimental protocol provided in Puzzle Mix (Kim et al., 2020), which trains ResNet-50
(He et al., 2015) for 100 epochs. As a result, Co-Mixup outperforms all of the baselines in
Top-1 error rate. We further test Co-Mixup on the speech domain with the Google commands
dataset and VGG-11 (Simonyan and Zisserman, 2014). We provide a detailed experimental
setting and dataset description in Appendix F.1. From Table 1, we confirm that Co-Mixup
is the most effective in the speech domain as well.
5.2	Localization
We compare weakly supervised ob ject localization (WSOL) performance of classifiers trained
on ImageNet (in Table 1) to demonstrate that our mixup method better guides a classifier
to focus on salient regions. We test the localization performance using CAM (Zhou et al.,
2016), a WSOL method using a pre-trained classifier. We evaluate localization performance
following the evaluation protocol in Qin and Kim (2019), with binarization threshold 0.25 in
CAM. Table 2 summarizes the WSOL performance of various mixup methods, which shows
that our proposed mixup method outperforms other baselines.
5.3	Calibration
We evaluate the expected calibration error (ECE) (Guo et al., 2017) of classifiers trained
on CIFAR-100. Note, ECE is calculated by the weighted average of the absolute difference
between the confidence and accuracy of a classifier. As shown in Table 2, the Co-Mixup
classifier has the lowest calibration error among baselines. From Figure 4, we find that other
mixup baselines tend to have under-confident predictions resulting in higher ECE values
even than Vanil la network (also pointed out by Wen et al. (2020)), whereas Co-Mixup has
best-calibrated predictions resulting in relatively 48% less ECE value. We provide more
figures and results with other datasets in Appendix F.2.
Task	Vanilla	Input	Manifold	CutMix	Puzzle Mix	Co-Mixup
Localization (Acc. %) (↑)	54.36	55.07	54.86	54.91	55.22	55.32
Calibration (ECE %)1)	3.9	17.7	13.1	5.6	7.5	1.9
Table 2: WSOL results on ImageNet and ECE (%) measurements of CIFAR-100 classifiers.
5.4	Robustness
In response to the recent problem raised by Lee et al. (2020) that deep neural network agents
often fail to generalize to unseen environments, we consider the situation where the statistics
of the foreground ob ject, such as color or shape, is unchanged, but with the corrupted
(or replaced) background. In detail, we consider the following operations: 1) replacement
with another image and 2) adding Gaussian noise. We use ground-truth bounding boxes
to separate the foreground from the background, and then apply the previous operations
independently to obtain test datasets. We provide a detailed description of datasets in
Appendix G.
8
Published as a conference paper at ICLR 2021
With the test datasets described above, we evaluate the robustness of the pre-trained
classifiers. As shown in Table 3, Co-Mixup shows significant performance gains at various
background corruption tests compared to the other mixup baselines. For each corruption
case, the classifier trained with Co-Mixup outperforms the others in Top-1 error rate with
the performance margins of 2.86% and 3.33% over the Vanilla model.
Corruption type	Vanilla	Input	Manifold	CutMix	Puzzle Mix	Co-Mixup
Random replacement	41.63	39.41	39.72	46.20	39.23	38.77
	(+17.62)	(+16.47)	(+16.47)	(+23.16)	(+16.69)	(+16.38)
Gaussian noise	29.22	26.29	26.79	27.13	26.11	25.89
	(+5.21)	(+3.35)	(+3.54)	(+4.09)	(+3.57)	(+3.49)
Table 3: Top-1 error rates of various mixup methods for background corrupted ImageNet
validation set. The values in the parentheses indicate the error rate increment by corrupted
inputs compared to clean inputs.
5.5	Baselines with multiple inputs
To further investigate the effect of the number of inputs for the mixup in isolation, we
conduct an ablation study on baselines using multiple mixing inputs. For fair comparison, we
use Dirichlet(α, . . . , α) prior for the mixing ratio distribution and select the best performing
α in {0.2, 1.0, 2.0}. Note that we overlay multiple boxes in the case of CutMix. Table 4
reports the classification test errors on CIFAR-100 with PreActResNet18. From the table,
we find that mixing multiple inputs decreases the performance gains of each mixup baseline.
These results demonstrate that mixing multiple inputs could lead to possible degradation of
the performance and support the necessity of considering saliency information and diversity
as in Co-Mixup.
# inputs for mixup Input Manifold CutMiX ∣ Co-MiXup
# inputs # inputs	2 3	22.43 23.03	21.64 22.13	21.29 22.01	19.87
# inputs	4	23.12	22.07	22.20	
Table 4: Top-1 error rates of mixup baselines with multiple mixing inputs on CIFAR-100
and PreActResNet18. We report the mean values of three different random seeds. Note
that Co-Mixup optimally determines the number of inputs for each output by solving the
optimization problem.
6 Conclusion
We presented Co-Mixup for optimal construction of a batch of mixup examples by finding
the best combination of salient regions among a collection of input data while encouraging
diversity among the generated mixup examples. This leads to a discrete optimization
problem minimizing a novel submodular-supermodular ob jective. In this respect, we present
a practical modular approximation and iterative submodular optimization algorithm suitable
for minibatch based neural network training. Our experiments on generalization, weakly
supervised ob ject localization, and robustness against background corruption show Co-Mixup
achieves the state of the art performance compared to other mixup baseline methods. The
proposed generalized mixup framework tackles the important question of ‘what to mix?’
while the existing methods only consider ‘how to mix?’. We believe this work can be applied
to new applications where the existing mixup methods have not been applied, such as
multi-label classification, multi-ob ject detection, or source separation.
9
Published as a conference paper at ICLR 2021
Acknowledgements
This research was supported in part by Samsung Advanced Institute of Technology, Samsung
Electronics Co., Ltd, Institute of Information & Communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00882,
(SW STAR LAB) Development of deployable learning intelligence via self-sustainable and
trustworthy machine learning), and Research Resettlement Fund for the new faculty of Seoul
National University. Hyun Oh Song is the corresponding author.
References
C. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1):108-116, 1995.
C. M. Bishop. Pattern recognition and machine learning. springer, 2006.
P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an
alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.
E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning
augmentation strategies from data. In CVPR, 2019.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F. F. Li. Imagenet: a large-scale hierarchical
image database. In CVPR, 2009.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
S.	Fujishige. Submodular functions and optimization. Elsevier, 2005.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks.
In ICML, 2017.
H. Guo, Y. Mao, and R. Zhang. Mixup as locally linear out-of-manifold regularization. In
AAAI, 2019.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
CVPR, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In
ECCV, 2016.
T.	Horel and Y. Singer. Maximization of approximately submodular functions. In NeurIPS,
2016.
R. Iyer and J. Bilmes. Algorithms for approximate minimization of the difference between
submodular functions, with applications. arXiv preprint arXiv:1207.0560, 2012.
C. Jung and C. Kim. A unified spectral-domain approach for saliency detection and its
application to automatic object segmentation. IEEE Transactions on Image Processing,
21(3):1272-1283, 2011.
O. Kalinli and S. S. Narayanan. A saliency-based auditory attention model with applications
to unsupervised prominent syllable detection in speech. In Eighth Annual Conference of
the International Speech Communication Association, 2007.
J.-H. Kim, W. Choo, and H. O. Song. Puzzle mix: Exploiting saliency and local statistics
for optimal mixup. In ICML, 2020.
A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images.
Citeseer, 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
10
Published as a conference paper at ICLR 2021
K.	Lee, K. Lee, J. Shin, and H. Lee. Network randomization: A simple technique for
generalization in deep reinforcement learning. In ICLR, 2020.
L.	Lovasz. SUbmodUlar functions and convexity. In Mathematical Programming The State of
the Art, pages 235-257. Springer, 1983.
T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. VirtUal adversarial training: a regUlarization
method for sUpervised and semi-sUpervised learning. IEEE transactions on pattern analysis
and machine intelligence, 41(8):1979-1993, 2018.
M.	Narasimhan and J. A. Bilmes. A sUbmodUlar-sUpermodUlar procedUre with applications
to discriminative strUctUre learning. UAI, 2005.
A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. Senior, and K. KavUkcUoglU. Wavenet: A generative model for raw aUdio. arXiv
preprint arXiv:1609.03499, 2016.
Z. Qin and D. Kim. Rethinking softmax with cross-entropy: NeUral network classifier as
mUtUal information estimator. arXiv preprint arXiv:1911.10688, 2019.
S. Ren, K. He, R. Girshick, and J. SUn. Faster r-cnn: Towards real-time ob ject detection
with region proposal networks. In NeurIPS, 2015.
R. R. SelvarajU, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:
VisUal explanations from deep networks via gradient-based localization. In ICCV, 2017.
K. Simonyan and A. Zisserman. Very deep convolUtional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolUtional networks: VisUalising
image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, A. CoUrville, D. Lopez-Paz, and
Y. Bengio. Manifold mixUp: Better representations by interpolating hidden states. In
ICML, 2019.
L. Wang, H. LU, X. RUan, and M.-H. Yang. Deep networks for saliency detection via local
estimation and global search. In CVPR, 2015.
P. Warden. URL https://research.googleblog.com/2017/08/launching-speech-commands-
dataset.html., 2017.
Y. Wen, G. Jerfel, R. MUller, M. W. DUsenberry, J. Snoek, B. Lakshminarayanan, and
D. Tran. Improving calibration of batchensemble with data aUgmentation. In ICML
Workshop on Uncertainty and Robustness in Deep Learning, 2020.
T. WindheUser, H. Ishikawa, and D. Cremers. Generalized roof dUality for mUlti-label
optimization: Optimal lower boUnds and persistency. In ECCV, 2012.
S. J. Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3-34, 2015.
S. Xie, R. Girshick, P. Dollar, Z. TU, and K. He. Aggregated residUal transformations for
deep neUral networks. pages 1492-1500, 2017.
S. YUn, D. Han, S. J. Oh, S. ChUn, J. Choe, and Y. Yoo. CUtmix: RegUlarization strategy
to train strong classifiers with localizable featUres. In ICCV, 2019.
S. ZagorUyko and N. Komodakis. Wide residUal networks. arXiv preprint arXiv:1605.07146,
2016.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning
reqUires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
H. Zhang, M. Cisse, Y. N. DaUphin, and D. Lopez-Paz. mixUp: Beyond empirical risk
minimization. In ICLR, 2018.
11
Published as a conference paper at ICLR 2021
R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection by multi-context deep learning.
In CVPR, 2015.
B.	Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for
discriminative localization. In CVPR, 2016.
A Supplementary notes for objective
A.1 Notations
In Table 5, we provide a summary of notations in the main text.
Notation	Meaning
m, m0, n ck ∈ Rm (1 ≤ k ≤ n) zj,k ∈ Lm (1 ≤ j ≤ m0, 1 ≤ k ≤ n) zj ∈ Lm×n oj ∈ Rm A ∈ Rm×m	# inputs, # outputs, dimension of data labeling cost for m input sources at the kth location input source ratio at the kth location of the jth output labeling of the jth output aggregation of the labeling of the jth output compatibility between inputs
Table 5: A summary of notations.
A.2 Interpretation of compatibility
In our main ob jective Equation (1), we introduce a compatibility matrix A = (1 - ω)I + ωAc
between inputs. By minimizing (θj,θjo)a for j = j0, We encourage each individual mixup
examples to have high compatibility within. Figure 5 explains how the compatibility term
works by comparing simple cases. Note that our framework can reflect any compatibility
measures for the optimal mixup.
Output 1
Output 2
Figure 5: Let us consider Co-Mixup with three inputs and two outputs. The figure represents
two Co-Mixup results. Each input is denoted as a number and color-coded. Let us assume
that input 1 and input 2 are more compatible, i.e., A12》A23 and A12》A13. Then, the
left Co-Mixup result has a larger inner-product value hpι,o±a than the right. Thus the
mixup result on the right has higher compatibility than the result on the left within each
output example.
B Proofs
B.1	Proof of proposition 1
Lemma 1. For a positive semi-definite matrix A ∈ R+m×m and x, x0 ∈ Rm, s(x, x0) = x|Ax0
is pairwise supermodular.
Proof. s(x, x) + s(x0, x0) - s(x, x0) - s(x0, x) = x|Ax + x|Ax - 2x|Ax0 = (x - x0)|A(x - x0),
and because A is positive semi-definite, (X — x0)TA(X — x0) ≥ 0.	□
12
Published as a conference paper at ICLR 2021
Proposition 1. The compatibility term fc in Equation (1) is pairwise supermodular for
every pair of (zj1 ,k , zj2 ,k ) if A is positive semi-definite.
Proof. For j1 and j2, s.t., j1 6= j2, max nτ, Pjm=1 Pjm0=1,j06=j(Pkn=1 zj,k)|A(Pkn=1 zj0,k)o =
max{τ, c + 2zj| ,k Azj2 ,k} = - min{-τ, -c - 2zj| ,k Azj2,k}, for ∃c ∈ R. By Lemma 1,
-zj| ,k Azj2 ,k is pairwise submodular, and because a budget additive function preserves
submodularity (Horel and Singer, 2016), min{-τ, -c - 2zj| ,kAzj2,k} is pairwise submodular
with respect to (zjι,k,Zj2,k).	口
B.2	Proof of proposition 3
Proposition 3. The modularization given by Equation (2) satisfies the criteria.
Proof. Note, by the definition in Equation (1), the compatibility between the jth and j0th
outputs is oj|0 Aoj, and thus, v-|j oj represents the compatibility between the jth output
and the others. In addition, koj k1 = k kn=1 zj,kk1 = kn=1 kzj,kk1 = n. In a local view,
for the given oj, let us define a vector o0j as o0j [i1] = oj[i1] + α and o0j [i2] = oj[i2] - α
for α > 0. Without loss of generality, let us assume v-j is sorted in ascending order.
Then, v-|j oj ≤ v-|j o0j implies i1 > i2 , and because the max function preserves the ordering,
max{τ 0, v-j }|oj ≤ max{τ 0, v-j}| o0j . Thus, the criterion 1 is locally satisfied. Next, for
τ0 > 0, k max{τ 0, v-j }|oj k1 ≥ τ0 koj k1 = τ0n. Let ∃i0 s.t. for i < i0, v-j [i] < τ0, and for
i ≥ i0, v-j [i] ≥ τ0. Then, for oj containing positive elements only in indices smaller than i0,
max{τ 0, v-j }|oj = τ0n which means there is no extra penalty from the compatibility. In this
respect, the proposed modularization satisfies the criterion 2 as well.	口
C Implementation details
We perform the optimization after down-sampling the given inputs and saliency maps to the
specified size (4 × 4). After the optimization, we up-sample the optimal labeling to match the
size of the inputs and then mix inputs according to the up-sampled labeling. For the saliency
measure, we calculate the gradient values of training loss with respect to the input data and
measure `2 norm of the gradient values across input channels (Simonyan et al., 2013). In
classification experiments, we retain the gradient information of network weights obtained
from the saliency calculation for regularization. For the distance in the compatibility term,
we measure `1 -distance between the most salient regions.
For the initialization in Algorithm 1, we use i.i.d. samples from a categorical distribution
with equal probabilities. We use alpha-beta swap algorithm from pyGCO1 to solve the
minimization step in Algorithm 1, which can find local-minima of a multi-label submodular
function. However, the worst-case complexity of alpha-beta swap algorithm with |L| = 2 is
O(m2n), and in the case of mini-batch with 100 examples, iteratively applying the algorithm
can become a bottleneck during the network training. To mitigate the computational overhead,
we partition the mini-batch (each of size 20) and then apply Algorithm 1 independently per
each partition.
The worst-case complexity theoretic of the naive implementation of Algorithm 1 increases
exponentially as |L| increases. Specifically, the worst-case theoretic complexity of the alpha-
beta swap algorithm is proportional to the square of the number of possible states of zj,k,
which is proportional to m|L|-1. To reduce the number of possible states in a multi-label case,
we solve the problem for binary labels (|L| = 2) at the first inner-cycle and then extend to
multi labels (|L| = 3) only for the currently assigned indices of each output in the subsequent
cycles. This reduces the number of possible states to O(m + m|L|-1) where m《m. Here,
m means the number of currently assigned indices for each output.
1https://github.com/Borda/pyGCO
13
Published as a conference paper at ICLR 2021
Based on the above implementation, we train models with Co-Mixup in a feasible time.
For example, in the case of ImageNet training with 16 Intel I9-9980XE CPU cores and 4
NVIDIA RTX 2080Ti GPUs, Co-Mixup training requires 0.964s per batch, whereas the
vanilla training without mixup requires 0.374s per batch. Note that Co-Mixup requires
saliency computation, and when we compare the algorithm with Puzzle Mix, which performs
the same saliency computation, Co-Mixup is only slower about 1.04 times. Besides, as we
down-sample the data to the fixed size regardless of the data dimension, the additional
computation cost of Co-Mixup relatively decreases as the data dimension increases. Finally,
we present the empirical time complexity of Algorithm 1 in Figure 6. As shown in the figure,
Algorithm 1 has linear time complexity over |L| empirically. Note that we use |L| = 3 in all
of our main experiments, including a classification task.
)sm( emiT noitucexE egarevA
10
8
6
4
)sm( emiT noitucexE egarevA
2
0
2	3	4	2 10 20	50	100
|L|	m
Figure 6: Mean execution time (ms) of Algorithm 1 per each batch of data over 100 trials.
The left figure shows the time complexity of the algorithm over |L| and the right figure shows
the time complexity over the number of inputs m. Note that the other parameters are fixed
equal to the classification experiments setting, m = m0 = 20, n = 16, and |L| = 3.
D Algorithm Analysis
In this section, we perform comparison experiments to analyze the proposed Algorithm 1.
First, we compare our algorithm with the exact brute force search algorithm to inspect
the optimality of the algorithm. Next, we compare our algorithm with the BP algorithm
proposed by Narasimhan and Bilmes (2005).
D.1 Comparison with Brute Force
To inspect the optimality of the proposed algorithm, we compare the function values of
the solutions of Algorithm 1, brute force search algorithm, and random guess. Due to the
exponential time complexity of the brute force search, we compare the algorithms on small
scale experiment settings. Specifically, we test algorithms on settings of (m = m0 = 2, n = 4),
(m = m0 = 2, n = 9), and (m = m0 = 3, n = 4) varying the number of inputs and outputs
(m, m0) and the dimension of data n. We generate unary cost matrix in the ob jective f by
sampling data from uniform distribution.
We perform experiments with 100 different random seeds and summarize the results on
Table 6. From the table, we find that the proposed algorithm achieves near optimal solutions
over various settings. We also measure relative errors between ours and random guess,
(f(zours) - f(zbrute))/(f(zrandom) - f(zbrute)). As a result, our algorithm achieves relative
error less than 0.01.
D.2 Comparison with another BP algorithm
We compare the proposed algorithm and the BP algorithm proposed by Narasimhan and
Bilmes (2005). We evaluate function values of solutions by each method using a random
14
Published as a conference paper at ICLR 2021
Configuration	Ours	Brute force (optimal)	Random guess	ReL error
(m = m0 = 2, n = 4)	1.91	1.90	3.54	0.004
(m = m0 = 2, n = 9)	1.93	1.91	3.66	0.01
(m = m0 = 3, n = 4)	2.89	2.85	22.02	0.002
Table 6: Mean function values of the solutions over 100 different random seeds. Rel. error
means the relative error between ours and random guess.
unary cost matrix from a uniform distribution. We compare methods over various scales by
controlling the number of mixing inputs m.
Table 7 shows the averaged function values with standard deviations in the parenthesis. As
we can see from the table, the proposed algorithm achieves much lower function values and
deviations than the method by Narasimhan and Bilmes (2005) over various settings. Note
that the method by Narasimhan and Bilmes (2005) has high variance due to randomization
in the algorithm. We further compare the algorithm convergence time in Table 8. The
experiments verify that the proposed algorithm is much faster and effective than the method
by Narasimhan and Bilmes (2005).
Algorithm	m=5	m= 10	m = 20	m = 50	m = 100
Ours	3.1 (1.7)	15 (6.6)	54 (15)	205 (26)	469 (31)
Narasimhan	269 (58)	1071 (174)	4344 (701)	24955 (4439)	85782 (14337)
Random	809 (22)	7269 (92)	60964 (413)	980973 (2462)	7925650 (10381)
Table 7: Mean function values of the solutions over 100 different random seeds. We report
the standard deviations in the parenthesis. Random represents the random guess algorithm.
Algorithm	m=5	m= 10	m = 20	m = 50	m = 100
Ours	0.02	0.04	0.11	0.54	2.71
Narasimhan	0.06	0.09	0.27	1.27	7.08
Table 8: Convergence time (s) of the algorithms.
E Hyperparameter settings
We perform Co-Mixup after down-sampling the given inputs and saliency maps to the
pre-defined resolutions regardless of the size of the input data. In addition, we normalize the
saliency of each input to sum up to 1 and define unary cost using the normalized saliency.
As a result, we use an identical hyperparameter setting for various datasets; CIFAR-100,
Tiny-ImageNet, and ImageNet. In details, we use (β, γ, η, τ) = (0.32, 1.0, 0.05, 0.83) for all of
experiments. Note that τ is normalized according to the size of inputs (n) and the ratio of
the number of inputs and outputs (m/m0), and we use an isotropic Dirichlet distribution
with α = 2 for prior p. For a compatibility matrix, we use ω = 0.001.
For baselines, we tune the mixing ratio hyperparameter, i.e., the beta distribution parameter
(Zhang et al., 2018), among {0.2, 1.0, 2.0} for all of the experiments if the specific setting is
not provided in the original papers.
F Additional Experimental Results
F.1 Another Domain: Speech
In addition to the image domain, we conduct experiments on the speech domain, verifying
Co-Mixup works on various domains. Following (Zhang et al., 2018), we train LeNet (LeCun
15
Published as a conference paper at ICLR 2021
Figure 7: Confidence-Accuracy plots for classifiers on CIFAR-100. Note, ECE is calculated
by the mean absolute difference between the two values.
et al., 1998) and VGG-11 (Simonyan and Zisserman, 2014) on the Google commands dataset
(Warden, 2017). The dataset consists of 65,000 utterances, and each utterance is about
one-second-long belonging to one out of 30 classes. We train each classifier for 30 epochs
with the same training setting and data pre-processing of Zhang et al. (2018). In more detail,
we use 160 × 100 normalized spectrograms of utterances for training. As shown in Table 9,
we verify that Co-Mixup is still effective in the speech domain.
Model	Vanilla	Input	Manifold	CutMix	Puzzle Mix	Co-Mixup
LeNet	11.24	10.83	12.33	12.80	10.89	10.67
VGG-11	4.84	3.91	3.67	3.76	3.70	3.57
Table 9: Top-1 classification test error on the Google commands dataset. We stop training if
validation accuracy does not increase for 5 consecutive epochs.
F.2 Calibration
In this section, we summarize the expected calibration error (ECE) (Guo et al., 2017) of
classifiers trained with various mixup methods. For evaluation, we use the official code
provided by the TensorFlow-Probability library2 and set the number of bins as 10. As
shown in Table 10, Co-Mixup classifiers have the lowest calibration error on CIFAR-100
and Tiny-ImageNet. As pointed by Guo et al. (2017), the Vanilla networks have over-
confident predictions, but however, we find that mixup classifiers tend to have under-confident
predictions (Figure 7; Figure 8). As shown in the figures, Co-Mixup successfully alleviates
the over-confidence issue and does not suffer from under-confidence predictions.
Dataset	Vanilla	Input	Manifold	CutMix	Puzzle Mix	Co-Mixup
CIFAR-100	3.9	17.7	13.1	5.6	7.5	1.9
Tiny-ImageNet	4.5	6.2	6.8	12.0	5.6	2.5
ImageNet	5.9	1.2	1.7	4.3	2.1	2.1
Table 10: Expected calibration error (%) of classifiers trained with various mixup methods
on CIFAR-100, Tiny-ImageNet and ImageNet. Note that, at all of three datasets, Co-Mixup
outperforms all of the baselines in Top-1 accuracy.
F.3 Sensitivity analysis
We measure the Top-1 error rate of the model by sweeping the hyperparameter to show the
sensitivity using PreActResNet18 on CIFAR-100 dataset. We sweep the label smoothness
coefficient β ∈ {0, 0.16, 0.32, 0.48, 0.64}, compatibility coefficient γ ∈ {0.6, 0.8, 1.0, 1.2, 1.4},
clipping level T ∈ {0.79, 0.81,0.83, 0.85,0.87}, compatibility matrix parameter ω ∈ {0, 5 ∙
10-4,10-3, 5 ∙ 10-3,10-2}, and the size of partition m ∈ {2, 4,10, 20, 50}. Table 11 shows
that Co-Mixup outperforms the best baseline (PuzzleMix, 20.62%) with a large pool of
2https://www.tensorflow.org/probability/api_docs/python/tfp/stats/expected_calibration_error
16
Published as a conference paper at ICLR 2021
Puzzle	Co-Mixup
Vanilla
1
Input
Manifold	CutMix
ycaruccA
0 Confidence 1 0	1 0	1 0	1 0	1 0
Figure 8: Confidence-Accuracy plots for classifiers on Tiny-ImageNet.
Manifold	CutMix
Puzzle	Co-Mixup
Vanilla
1
ycaruccA
Input
0 Confidence 1 0	1 0	1 0
10
10
1
Figure 9:	Confidence-Accuracy plots for classifiers on ImageNet.
hyperparameters. We also find that Top-1 error rate increases as the partition batch size
increases until m = 20.
m
Smoothness coefficient, β	β=0 20.29	β = 0.16 20.18	β = 0.32 19.87	β= 0.48 20.35	β= 0.64 21.24
Compatibility coefficient, γ	γ = 0.6 20.3	Y = 0.8 19.99	γ=1.0 19.87	γ=1.2 20.09	γ= 1.4 20.13
Clipping parameter, τ	τ = 0.79 20.45	T = 0.81 20.14	τ= 0.83 19.87	τ=0.85 20.15	τ= 0.87 20.23
Compatibility matrix parameter, ω	ω=0 20.51	ω 二 5 ∙ 10-4 20.42	ω = 10-3 19.87	ω = 5∙10-3 20.18	ω = 10-2 20.14
Partition size, m	m=2 20.3	m=4 20.22	m = 10 20.15	m = 20 19.87	m = 50 19.96
Table 11: Hyperparameter sensitivity results (Top-1 error rates) on CIFAR-100 with PreAc-
tResNet18. We report the mean values of three different random seeds.
F.4 Comparison with non-mixup baselines
We compare the generalization performance of Co-Mixup with non-mixup baselines, verifying
the proposed method achieves the state of the art generalization performance not only for
the mixup-based methods but for other general regularization based methods. One of the
regularization methods called VAT (Miyato et al., 2018) uses virtual adversarial loss, which
is defined as the KL-divergence of predictions between input data against local perturbation.
We perform the experiment with VAT regularization on CIFAR-100 with PreActResNet18 for
300 epochs in the supervised setting. We tune α (coefficient of VAT regularization term) in
{0.001, 0.01, 0.1}, (radius of `-inf ball) in {1, 2}, and the number of noise update steps in {0,
1}. Table 12 shows that Co-Mixup, which achieves Top-1 error rate of 19.87%, outperforms
the VAT regularization method.
G Detailed description for background corruption
We build the background corrupted test datasets based on ImageNet validation dataset
to compare the robustness of the pre-trained classifiers against the background corruption.
17
Published as a conference paper at ICLR 2021
VAT loss coefficient	# update=0		# update=1	
	C=1	C=2	C=1	C=2
α = 0.001	23.38	23.62	24.76	26.22
α = 0.01	23.14	23.67	28.33	31.95
α = 0.1	23.65	23.88	34.75	39.82
Table 12: Top-1 error rates of VAT on CIFAR-100 dataset with PreActResNet18.
ImageNet consists of images {x1, ..., xM}, labels {y1, ..., yM}, and the corresponding ground-
truth bounding boxes {b1 , ..., bM}. We use the ground-truth bounding boxes to separate the
foreground from the background. Let zj be a binary mask of image xj , which has value 1
inside of the ground-truth bounding box bj . Then, we generate two types of background
corrupted sample Xj by considering the following operations:
1.	Replacement with another image as Xj = Xj Θ Zj + xij)Θ (1 — Zj) for a random
permutation {i(1), ..., i(M)}.
2.	Adding Gaussian noise as Xj = Xj Θ Zj + C Θ (1 — Zj), where e 〜N(0, 0.12). We clip
pixel values of Xj to [0, 1].
Figure 10 visualizes subsets of the background corruption test datasets.
(b)
Figure 10:	Each subfigure shows background corrupted samples used in the robustness
experiment. (a) Replacement with another image in ImageNet. (b) Adding Gaussian noise.
The red boxes on the images represent ground-truth bounding boxes.
H Co-Mixup generated samples
In Figure 12, we present Co-Mixup generated image samples by using images from ImageNet.
We use an input batch consisting of 24 images, which is visualized in Figure 11. As can be
seen from Figure 12, Co-Mixup efficiently mix-matches salient regions of the given inputs
maximizing saliency and creates diverse outputs. In Figure 12, inputs with the target objects
on the left side are mixed with the objects on the right side, and objects on the top side are
mixed with the objects on the bottom side. In Figure 13, we present Co-Mixup generated
image samples with larger τ using the same input batch. By increasing τ , we can encourage
Co-Mixup to use more inputs to mix per each output.
18
Published as a conference paper at ICLR 2021
Figure 11: Input batch.
19
Published as a conference paper at ICLR 2021
Figure 12: Mixed output batch.
20
Published as a conference paper at ICLR 2021
Figure 13: Another mixed output batch with larger τ .
21