Table 1: Recovery rate comparison of DSR and five baselines on the Nguyen symbolic regressionbenchmark suite. A bold value represents statistical significance (p < 10-3) across all benchmarks.
Table 2: Benchmark symbolic regression problem specifications. Input variables are denotedby x and/or y. U(a, b, c) denotes c random points uniformly sampled between a and b foreach input variable; training and test datasets use different random seeds. E(a, b, c) denotes cpoints evenly spaced between a and b for each input variable; training and test datasets use thesame points (except Neat-6, which uses E(1, 120, 120) as test data, and the Jin tests, which useU(-3, 3, 30) as test data). To simplify notation, libraries are defined relative to a “base” libraryL0 = {+, -, ×, ÷, sin, cos, exp, log, x}. Placeholder operands are denoted by •, e.g. •2 correspondsto the square operator.
Table 3: Tuned hyperparameters for RNN-based algorithms.
Table 4: Tuned hyperparameters for GP.
Table 5: Recovery rate comparison of DSR and literature-reported values from SSR (Huynh et al.,2016). A bold value represents statistical significance (p < 10-3) across all benchmarks.
Table 6: Comparison of DSR and literature-reported values from GrammarVAE (Kusner et al., 2017).
Table 7: RMSE comparison of DSR and literature-reported values from BSR (Jin et al., 2019).
Table 8: Comparison of median values of RMSE for DSR and literature-reported values from Neat-GP(Trujillo et al., 2016).
Table 9: Recovery rate comparison of DSR and five baselines on variations of the Nguyen symbolicregression benchmark suite. A bold value represents statistical significance (p < 10-3) across eachset of benchmarks.
Table 10: Comparison of NRMSE on the test data for DSR and five baselines on original andvariations of the Nguyen symbolic regression benchmark suite.
Table 11: Comparison of runtimes across the Nguyen benchmark set.
