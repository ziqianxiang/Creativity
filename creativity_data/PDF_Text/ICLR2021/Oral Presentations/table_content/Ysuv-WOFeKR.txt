Table 1: Hyperparameters for soft-actor critic (SAC)Hyperparameter	value usedTarget network update period	1000 stepsdiscount factor Î³	0.99policy learning rate	3e-4Q-function learning rate	3e-4reward scale	1.0automatic entropy tuning	enablednumber of update steps per env step	12304 + 7Figure 9: Policy and Q-function network architectures. We use a convolutional neural network to representthe Q-function for SAC, shown in this figure. The policy network is identical, except it does not take in anaction as an input and outputs a 7D action instead of a scalar Q-value.
