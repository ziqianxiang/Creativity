Table A: Comparison of Different Camera Initializations: First table shows annotation time required for theStyleGAN dataset, and training times of the view-model and keypoint-model on the dataset with respectiveannotations (binned viewpoints or cameras computed with SFM from annotated keypoints). The view-modelrequires significantly less annotation time, and its final performance is comparable to the keypoint-model. Sec-ond table shows the difference of the camera parameters after training both methods (which optimize camerasduring training). They converge to very similar camera positions. This shows that coarse view annotation alongwith camera optimization during training is sufficient in training high accuracy inverse graphics networks.
Table B: User study results: (a): Quality of 3D estimation (shape, texture and overall). (b): Annotatorsagreement analysis. “No agreement” stands for the case where all three annotators choose different options.
