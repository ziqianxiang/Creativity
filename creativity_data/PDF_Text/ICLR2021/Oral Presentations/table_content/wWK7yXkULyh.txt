Table 1: This table summarizes the performance of mongoose, slide and Full-Computation implementedwith PyTorch (Paszke et al., 2019). P @1/5 is top-1/5 precision. Time represents convergence time and Memrepresents memory consumption.
Table 2: Time and memory advantage com-pared to Full-Computation on one layer.
Table 3: Speed with respect to accuracy	P @1	P @5	Speed (batch/s)Infrequent Scheduler	0.39	0.193	31.2W/O Scheduler	0.521	0.254	0.4mongoose Scheduler	0.519	0.255	77In this section, we demonstrate the superiority ofour smart scheduling algorithm over the two base-lines we mentioned in Section 1 on Wiki-325K.
Table 4: This table compares MONGOOSE and HNSW. P @1/5 is the top-1/5 precision.
Table 5: Statistics for our benchmark datasetDataset	Wiki10-31k	DelicioUS-200K	Wiki-325K	Amz-670KOutput Dimension	30938	205443	325056	670091Input Dimension	101938	782585	1617899	135909Training Samples	14146	^6616	1778351	490449Testing Samples	196606	—	100095	587084	—	153025	—We present statistics on the 3 datasets we test on from the Extreme Classification Repository (Bhatiaet al., 2016). While the number of datapoints in each dataset is not large (on the order of 200Kat most), the key feature is the sheer size of the input and output dimensions. In particular, eachdataset has over 10,000 output classes, which, using a conventional nueral network, requires a matrixmultiplication involving over 10,000 neurons at the final layer.
Table 6: mongoose performance as afunction of number of hashes.
Table 7: This table summarizes the performance of mongoose, slide and Full-Computation implementedwith PyTorch (Paszke et al., 2019). P@1 is the top-1 accuracy and P@5 is the top-5 accuracy. Time representsconvergence time.
