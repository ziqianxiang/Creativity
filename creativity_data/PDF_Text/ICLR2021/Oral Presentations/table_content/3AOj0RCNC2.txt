Table 1: Continual learning on different datasets. Methods that do not adhere to CL setup is indicatedby (*). All the results are (re) produced by us and averaged over 5 runs. Standard deviations arereported in Table 8 and 9 in the appendix.
Table 2: Total wall-clock training time measured on a single GPU after learning all the tasks.
Table 3: Continual learning of 20-task from CIFAR-100 Superclass dataset. (f) denotes the resultreported from APD. (*) indicates the methods that do not adhere to CL setup. Single-task learning(STL), where a separate network in trained for each task, serves as an upper bound on accuracy.
Table 4: Dataset Statistics.
Table 5: 5-Datasets Statistics. For the datasets with monochromatic images, we replicate the imageacross all RGB channels so that size of each image becomes 3 × 32 × 32.
Table 6: List of hyperparameters for the baselines and our approach. Here, ‘lr’ represents (initial)learning rate. In the table we represent PMNIST as ‘perm’, 10-Split CIFAR-100 as ‘cifar’, SplitminiImageNet as ‘minImg’ and 5-Datasets as ‘5data’.
Table 7: Size of GPM matrices for each layer for the architectures used in our experiments. Maxi-mum sizes of the GPM in terms of number of parameters are also given.
Table 8: Continual learning on PMNIST in single-epoch and multi-epoch setting.
Table 9: Continual learning on different datasets along with the standard deviation values for theresults shown in Table 1(b).
Table 10: Number of new bases (k) added to the GPM at different layers after each (a) PMNISTtask and (b) 10-split CIFAR-100 task (for a random seed configuration).
Table 11: Continual learning of Digit dataset tasks in class-incremental learning setup (Kamra et al.,2017).(f) denotes the result reported from DGDMN.
