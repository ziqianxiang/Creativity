Table 1: Effect of model and training data changes on supervised model quality. We measure policy accuracyas well as average SoS score achieved by each agent against 6 of the original DipNet model. We measure theSoS scores in two settings: with all 7 agents sampling orders at a temperature of either 0.5 or 0.1.
Table 2: Average SoS score of our agent in anonymous games against humans on webdiplomacy.net.
Table 3: Average SoS scores achieved by each of the agents against each other. DipNet agents from (Paquetteet al., 2019) and the Blueprint agent use a temperature of 0.1. BRBot scores higher than SearchBot against theBlueprint, but SearchBot outperforms BRBot in a head-to-head comparison.
Table 4: Average SoS score of one expert human playing against six bots under repeated play. A score lessthan 14.3% means the human is unable to exploit the bot. Five games were played for each power for eachagent, for a total of 35 games per agent. For each power, the human first played all games against DipNet, thenthe blueprint model described in Section 3.1, and then finally SearchBot.
Table 5: Exploitability of 256 iterations ofRM using either the final iteration’s policy or the average iteration’spolicy, and either using the policy from a single run or the policy average over multiple runs. Subgame 1and Subgame 2 are subgames from Diplomacy S1901 and F1901, respectively. 10x10 Random and 100x100Random are random matrix two-player zero-sum games with entries having value in [0, 1) sampled uniformlyrandomly. For Subgame 1 and Subgame 2, “Ave. of Final Policies” and “Ave. of Ave. Policies” are the averageof 1,024 runs with different random seeds. For 10x10 Random and 100x100 Random, 10,000 seeds were used.
Table 6: Average SoS score of our agent in anonymous games against humans on webdiplomacy.net.
Table 7: Average SoS score of one expert human playing against six bots under repeated play. A score lessthan 14.3% suggests the human is unable to exploit the bot. Five games were played for each power for eachagent, for a total of 35 games per agent. For each power, the human first played all games against DipNet, thenthe blueprint model described in Section 3.1, and then finally SearchBot.
