Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,add position embeddings, and feed the resulting sequence of vectors to a standard Transformerencoder. In order to perform classification, we use the standard approach of adding an extra learnable“classification token” to the sequence. The illustration of the Transformer encoder was inspired byVaswani et al. (2017).
Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.
Figure 3: Transfer to ImageNet. Whilelarge ViT models perform worse than BiTResNets (shaded area) when pre-trained onsmall datasets, they shine when pre-trained onlarger datasets. Similarly, larger ViT variantsovertake smaller ones as the dataset grows.
Figure 4: Linear few-shot evaluation on Ima-geNet versus pre-training size. ResNets per-form better with smaller pre-training datasetsbut plateau sooner than ViT, which performsbetter with larger pre-training. ViT-b is ViT-Bwith all hidden dimensions halved.
Figure 5: Performance versus cost for different architectures: Vision Transformers, ResNets, andhybrids. Vision Transformers generally outperform ResNets with the same computational budget.
Figure 6: Representative ex-amples of attention from theoutput token to the inputspace. See Appendix D.6 fordetails.
Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-ilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the positionembedding of the patch with the indicated row and column and the position embeddings of all otherpatches. Right: Size of attended area by head and network depth. Each dot shows the mean attentiondistance across images for one of 16 heads at one layer. See Appendix D.6 for details.
Figure 8: Scaling different model dimensions of the Vision Transformer.
Figure 9: Position embeddings of models trained with different hyperparameters.
Figure 10: Size of attended area by head and network depth. Attention distance was computed for128 example images by averaging the distance between the query pixel and all other pixels, weightedby the attention weight. Each dot shows the mean attention distance across images for one of 16heads at one layer. Image width is 224 pixels.
Figure 11: Left: Real wall-clock timings of various architectures across input sizes. ViT modelshave speed comparable to similar ResNets. Right: Largest per-core batch-size fitting on device withvarious architectures across input sizes. ViT models are clearly more memory-efficient.
Figure 12: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).
Figure 13: Further example attention maps as in Figure 6 (random selection).
