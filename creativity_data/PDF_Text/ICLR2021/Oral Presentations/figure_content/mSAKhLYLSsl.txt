Figure 1: Dataset Condensation (left) aims to generate a small set of synthetic images that can match theperformance of a network trained on a large image dataset. Our method (right) realizes this goal by learninga synthetic set such that a deep network trained on it and the large set produces similar gradients w.r.t. itsweights. The synthetic data can later be used to train a network from scratch in a small fraction of the originalcomputational load. CE denotes Cross-Entropy.
Figure 2: Visualization of condensed 1 im-age/class with ConvNet for MNIST, Fashion-MNIST, SVHN and CIFAR10.
Figure 3: Absolute and relative testing accuraciesfor varying the number of condensed images/class forMNIST, FashionMNIST, SVHN and CIFAR10. Therelative accuracy means the ratio compared to its upper-bound, i.e. training with the whole dataset.
Figure 4: Continual learning performance inaccuracy (%). Herding denotes the originalE2E (Castro et al., 2018). T1, T2, T3 are threelearning stages. The performance at each stage isthe mean testing accuracy on all learned tasks.
Figure F5: The performance correlation between the training on proxy dataset and whole-dataset. For eachproxy dataset, the best 10 models are selected based on validation set performance. In the figure, each pointrepresents an architecture.
Figure F6: Ablation study on the hyper-parameters K and T when learning 10 images/class condensed sets.
Figure F7: The synthetic images for MNIST, FaShionMNIST, SVHN and CIFAR10 produced by our methodwith ConvNet under 10 imageS/ClaSS Setting.
Figure F8: Qualitative comparison between the condensed images produced by DD and ours under 10 im-ages/class setting. LeNet and AlexCifarNet are utilized for MNIST and CIFAR10 respectively.
