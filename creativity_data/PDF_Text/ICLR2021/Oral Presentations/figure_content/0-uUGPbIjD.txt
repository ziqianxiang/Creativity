Figure 1: Left: Score of SearchBot using different numbers of sampled subgame actions, against 6 DipNetagents ((Paquette et al., 2019) at temperature 0.1). A score of 14.3% would be a tie. Even when sampling onlytwo actions, SearchBot dramatically outperforms our blueprint, which achieves a score of 20.2%. Middle: Theeffect of the number iterations of sampled regret matching on SearchBot performance. Right: The effect ofdifferent rollout lengths on SearchBot performance.
Figure 2: Score of the exploiting agent against the blueprint and SearchBot-clone as a function of training time.
Figure 3: Left: Distance of the RM average strategy from equilibrium as a function of the RM iteration,computed as the sum of all agentsâ€™ exploitability in the matrix game in which RM is employed. RM reducesexploitability, while the blueprint policy has only slightly lower exploitability than the uniform distributionover the 50 sampled actions used in RM (i.e. RM iteration 1). For comparison, our human evaluations used256-2048 RM iterations, depending on the time per turn. Right: Comparison of convergence of individualstrategies to the average of two independently computed strategies. The similarity of these curves suggests thatindependent RM computations lead to compatible equilibria. Note: In both figures, exploitability is averagedover all phases in 28 simulated games; per-phase results are provided in Appendix E.
Figure 4: Architecture of the model used for imitation learning in no-press. Diplomacy.
Figure 5: Features used for the board state encoding.
Figure 6: Architecture of an encoder GNN layer.
Figure 7: Illustration of featurized order decoding.
Figure 8: Exploitability as a function of RM iteration at each phase of 7 simulated games with our searchagent (until the game ends or the search agent is eliminated). Aggregate results in Figure 3.
