Figure 1: Workflow for automated decision making with transparency. Our probabilistic classifierproduces a distribution over outputs. In cases of high uncertainty, CLUE allows us to identify featureswhich are responsible for class ambiguity in the input (denoted by ∆ and highlighted in dark blue).
Figure 2: Left: Training points and predictive distribution for variational Bayesian Logistic Regressionon the Moons dataset. Center: Aleatoric entropy Ha matches regions of class non-separability. Right:Epistemic entropy He grows away from the data. Both uncertainties are detailed in Appendix B.2.
Figure 3: Left: Taking a step in the direction of maximum sensitivity leads to a seemingly noisy inputconfiguration for which H is small. Right: Minimizing CLUE’s uncertainty-based objective in termsof a DGM’s latent variable z produces a plausible digit with a corrected lower portion.
Figure 4: Latent codes are decoded into inputs forwhich a BNN generates uncertainty estimates; theirgradients are backpropagated to latent space.
Figure 5: Example image and tabular CLUEs.
Figure 6: Pipeline for computational evaluation of CoUnterfactUaI explanations of uncertainty. TheVAEAC which We treat as a data generating process is colored in green. Colored in orange is theauxiliary DGM used by the approach being evaluated. For approaches that do not use an auxiliaryDGM, like Uncertainty Sensitivity Analysis, the orange element will not be present.
Figure 7: MNIST knee-points.
Figure 8: Experimental workflow for our tabular data user study.
Figure 9: Example question shown to main survey participants for the COMPAS dataset: Given theuncertain example on the left and the certain example in the middle, will the model be certain on thetest example on the right? The red text highlights the features that differ between context points.
Figure 10: Left: CLUEs are similarly informative under encoder-based and encoder-free initializa-tions. The colorbar indicates the original samples’ uncertainty. Its horizontal blue line denotes ourrejection threshold. Right: Auxiliary DGMs with more capacity result in more relevant CLUEs.
Figure 11: Left: Training points and BNN predictive distribution obtained on the moons dataset withSG-HMC. Center: Aleatoric entropy Ha expressed by the model matches regions of class overlap.
Figure 12: We generate 5 possible CLUEs for 11 MNIST digits score above the uncertainty rejectionthreshold. Below each digit or counterfactual is the predictive entropy it is assigned H and the classof maximum probability c.
Figure 13: The leftmost entry is an uncertain COMPAS test sample. To its right are four candidateCLUEs. The first three successfully reduce uncertainty past our rejection threshold, while therightmost does not.
Figure 14: Left: A digit from the MNiST test set with large predictive entropy. Center: The samedigit after a step is taken in the direction of -VxH. Non-zero weight is assigned to pixels that arealways zero valued. Right: Uncertainty sensitivity analysis for the entire MNiST test set.
Figure 15: Left: CLUE latent trajectory for a test point from the Credit dataset in a two-dimensionallatent space. The blue dot marks the start of the trajectory and the orange one marks the end. Uncer-tainty levels are displayed in greyscale. Right: Changes in aleatoric entropy for inputs regeneratedfrom latent codes along the trajectory.
Figure 16:	High confidence MNIST test examples together with LIME and SHAP explanations forthe top 3 predicted classes. The model being investigated is a BNN with architecture described inAppendix B. The highest probability class is denoted by y.
Figure 17:	Ten MNIST test digits for which our BNN’s predictive entropy is above the rejectionthreshold. A single CLUE example is provided for each one. For each digit, the top scoring class isdenoted by y. LIME and SHAP explanations are provided for the three most likely classes.
Figure 18:	CLUEs generated for MNIST digits for which our BNN’s predictive entropy is above therejection threshold. The BNNs predictive entropy for both original inputs and CLUEs is shown underthe corresponding images.
Figure 19:	U-FIDO counterfactuals generated for MNIST digits for which our BNN’s predictiveentropy is above the rejection threshold. The BNNs predictive entropy for both original inputs andcounterfactuals is shown under the corresponding images.
Figure 21: Amount of uncertainty explained away and `1 distance between original inputs and CLUEsfor every dataset under consideration and different capacity VAEs.
Figure 22: CLUE ∆H vs prediction change for all datasets under consideration. Prediction changerefers to the proportion of CLUEs classified differently than their corresponding original inputs. Allvalues shown are averages across all testset points above the uncertainty rejection threshold.
Figure 23: Left: Prediction change refers to the proportion of CLUEs classified differently thantheir corresponding original inputs. Setting a value of λy of around 0.7 results in class predictionsfor CLUEs being closer to the true labels than the original class predictions. Right: Reduction inpredictive entropy achieved by CLUE. All values shown are averages across all testset points abovethe uncertainty rejection threshold.
Figure 24: Amount of noise uncertainty explained away vs `1 shift in input space for all datasets underconsideration when applying CLUE to regular NNs. The colorbar indicates the original samples’predictive uncertainty.
Figure 25: Predictive entropy estimates for artificial MNIST digits generated from a 2-dimensionalVAE latent space. The MNIST test set digits have been projected onto the latent space and aredisplayed with a different color per class.
Figure 26: In its first stage, the two-level VAE maps input samples to approximate posteriors in theouter latent space. The aggregate posterior over this latent space need not resemble the isotropicGaussian prior. The second VAE maps samples from the outer latent space to approximate posteriorsin the inner latent space. The aggregate posterior over the inner latent space more closely matchesthe prior.
Figure 27: Left: Digits generated from the inner latent space of a VAEAC trained on MNIST with atwo-level mechanism. Right: Digits generated from the latent space of a VAEAC trained on MNIST.
Figure 28: Setup of tabular user studies.
Figure 29: Example Tabular Main Survey questionsCurrent Charge MisdemeanourReoffended BeforeCurrent ChargeFelonyCurrent ChargeFelonyNo, the Al will be uncertain on the example on the right.
Figure 30:	Examples of high uncertainty digits containing characteristics that are uncommon in ourmodified MNIST dataset. Their corresponding CLUEs and ∆CLUEs are displayed beside them.
Figure 31:	MNIST User Study SetupThe first variant was shown to 5 graduate students with machine learning expertise who only receivedcontext points and rejection labels (uncertain or not). This group was able to correctly classify 67%of the new test points as high or low uncertainty. The second variant was shown to 5 other graduatestudents with machine learning expertise who received context points together with CLUEs in casesof high uncertainty. This group was able to reach an accuracy of 88% on new test points. This userstudy suggests CLUEs are useful for practitioners in image-based settings as well.
