Figure 1: How ReLU MLPs extrapolate. We train MLPs to learn nonlinear functions (grey) andplot their predictions both within (blue) and outside (black) the training distribution. MLPs convergequickly to linear functions outside the training data range along directions from the origin (Theorem 1).
Figure 2: How GNNs extrapolate. Since MLPs can extrapolate well when learning linear functions,we hypothesize that GNNs can extrapolate well in dynamic programming (DP) tasks if we encodeappropriate non-linearities in the architecture (left) and input representation (right; through domainknowledge or representation learning). The encoded non-linearities may not be necessary forinterpolation, as they can be approximated by MLP modules, but they help extrapolation. We supportthe hypothesis theoretically (Theorem 3) and empirically (Figure 6).
Figure 3: Conditions for ReLU MLPs to extrapolate well. We train MLPs to learn linear functions(grey) with different training distributions (blue) and plot out-of-distribution predictions (black).
Figure 4: Extrapolation performance of ReLU MLPs. We plot the distributions of MAPE (meanabsolute percentage error) of MLPs trained with various hyperparameters (depth, width, learning rate,batch size). (a) Learning different target functions; (b) Different training distributions for learninglinear target functions: “all” covers all directions, “fix1” has one dimension fixed to a constant, and“negd” has d dimensions constrained to negative values. ReLU MLPs generally do not extrapolatewell unless the target function is linear along each direction (Figure 4a), and extrapolate linear targetfunctions if the training distribution covers sufficiently many directions (Figure 4b).
Figure 5: Extrapolation performance of MLPs with other activation. MLPs can extrapolate wellwhen the activation is “similar” to the target function. When learning quadratic with quadraticactivation, 2-layer networks (quad-2) extrapolate well, but 4-layer networks (quad-4) do not.
Figure 6:	Extrapolation for algorithmic tasks. Each column indicates the task and mean averagepercentage error (MAPE). Encoding appropriate non-linearity in the architecture or representationis less helpful for interpolation, but significantly improves extrapolation. Left: In max degree andshortest path, GNNs that appropriately encode max/min extrapolate well, but GNNs with sum-poolingdo not. Right: With improved input representation, GNNs extrapolate better for the n-body problem.
Figure 7:	Importance of the training graph structure. Rows indicate the graph structure coveredby the training set and the extrapolation error (MAPE). In max degree, GNNs with max readoutextrapolate well if the max/min degrees of the training graphs are not restricted (Theorem 3). Inshortest path, the extrapolation errors of min GNNs follow a U-shape in the sparsity of the traininggraphs. More results may be found in Appendix D.2.
Figure 8: (Quadratic function). Both panels show the learned v.s. true y = x12 + x22 . In eachfigure, we color OOD predictions by MLPs in black, underlying function in grey, and in-distributionpredictions in blue. The support of training distribution is a square (cube) for the top panel, and is acircle (sphere) for the bottom panel.
Figure 9: (Cos function). Both panels ShoW the learned v.s. true y = cos(2π ∙ xι)+ cos(2π ∙ x2).
Figure 10: (Cos function). Top panel shows the learned v.s. true y = cos(2π ∙ xι)+ cos(2π ∙ x2)where the support of training distribution is a circle (sphere). Bottom panel shows results for cosinein 1D, i.e. y = cos(2π ∙ x). In each figure, We color OOD predictions by MLPS in black, underlyingfunction in grey, and in-distribution predictions in blue.
Figure 11: (Sqrt function). Top panel shows the learned v.s. true y = √∕x1 + √∕x2 where the supportof training distribution is a square (cube). Bottom panel shows the results for the square root functionin 1D, i.e. y = √∕x. In each figure, We color OOD predictions by MLPS in black, underlying functionin grey, and in-distribution predictions in blue.
Figure 12: (L1 function). Both panels show the learned v.s. true y = |x|. In the top panel, the MLPsuccessfully learns to extrapolate the absolute function. In the bottom panel, an MLP with differenthyper-parameters fails to extrapolate. In each figure, we color OOD predictions by MLPs in black,underlying function in grey, and in-distribution predictions in blue.
Figure 13: (L1 function). Both panels show the learned v.s. true y = |x1 | + |x2|. In the top panel,the MLP successfully learns to extrapolate the l1 norm function. In the bottom panel, an MLP withdifferent hyper-parameters fails to extrapolate. In each figure, we color OOD predictions by MLPs inblack, underlying function in grey, and in-distribution predictions in blue.
Figure 14: (Linear function). Both panels show the learned v.s. true y = x1 + x2, with the supportof training distributions being square (cube) for top panel, and circle (sphere) for bottom panel. MLPssuccessfully extrapolate the linear function with both training distributions. This is explained byTheorem 2: both sphere and cube intersect all directions. In each figure, we color OOD predictionsby MLPs in black, underlying function in grey, and in-distribution predictions in blue.
Figure 15: Density plot of the test errors in MAPE. The underlying functions are linear, but wetrain MLPs on different distributions, whose support potentially miss some directions. The trainingsupport for “all” are hyper-cubes that intersect all directions. In “fix1”, we set the first dimension oftraining data to a fixed number. In “posX”, we restrict the first X dimensions of training data to bepositive. We can see that MLPs trained on “all” extrapolate the underlying linear functions, but MLPstrained on datasets with missing directions, i.e., “fix1” and “posX”, often cannot extrapolate well.
Figure 16: Maximum degree: continuous and “spurious” node features. Here, each node has anode feature in R3 that shall not contribute to the answer of maximum degree. GNNs with graph-levelmax-pooling extrapolate to graphs with OOD node features and graph structure, graph sizes, if trainedon graphs that satisfy the condition in Theorem 3.
Figure 17: Maximum degree: max-pooling v.s. sum-pooling. In each sub-figure, left columnshows test errors for GNNs with graph-level max-pooling; right column shows test errors for GNNswith graph-level sum-pooling. x-axis shows the graph structure covered in training set. GNNs withsum-pooling fail to extrapolate, validating Corollary 1. GNNs with max-pooling encodes appropriatenon-linear operations, and thus extrapolates under appropriate training sets (Theorem 3).
Figure 18: Shortest path: random graphs. We train GNNs with neighbor and graph-level min onrandom graphs with probability p of an edge between any two vertices. x-axis denotes the p for thetraining set, and y-axis denotes the test/extrapolation error on unseen graphs. The test errors follow aU-shape: errors are high if the training graphs are very sparse (small p) or dense (large p). The samepattern is obtained if we train on specific graph structure.
