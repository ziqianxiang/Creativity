Figure 1: From MaxEnt to EBM through Information Geometry. The Generalized MaxEnt specification (leftpanel) is looking for a distribution p that lies on the moment constraints manifold C and that minimizes theforward KL DKL(p, a). The solution is provided by Information Geometry: (1) build the exponential family Edetermined by a and φ, (2) p lies at the intersection between C and E, (3) for any distribution c satisfying theconstraints, the “Pythagorean identity” holds: DKL (c||a) = DKL (c||p) + DKL (p||a); in particular p is unique.
Figure 2: Eval. metrics Eφ(s), Dkl(∏θ Ila) (] better), Self-BLEU-5 (] better), and Distinct-1 (↑ better), aggre-gated across 17 point-wise experiments (single words, wordlists, discriminators), performed at each 10 gradientupdates, for policies obtained from GDC against three training baselines REINFORCE , REINFORCEP(x) andZIEGLER . See Appendix H for a detailed view for each experiment and more evaluation metrics.
Figure 3: GDC steadily decreases the KLdeviation between the trained policy πθ andthe target distribution p. The Figure is aggre-gated across 17 point-wise constraints ex-periments, see Appendix H for a separateview of each experiment.
Figure 4: “Zipf-like” token frequency anal-ysis on sets of 68000 generated samplesfrom each method (only samples strictly sat-isfying the constraints are kept, for fair com-parison). Longer tails mean a lower concen-tration of mass on the high frequency tokens,and therefore indicate more vocabulary rich-ness. See Appendix H.4 for details.
Figure 5: Transitivity of Information Projection (aka Generalized MaxEnt).
Figure 6: Ablation experiment elaborating the effectiveness of the adaptive step in the DPG algorithm ex-plained in section 2. We compare three adaptivity variants, based on the KL divergence (DPG-KLD), on theTVD distance (DPG-TVD) and with no adaptation. We find similar convergence rates for both KLD and TVDadaptive DPG compared to a much slower convergence without adaptation.
Figure 7: Supervised experiment when fine-tuning GPT-2 on a corpus of sentences containing the word”amazing”. Left: validation loss development during fine-tuning. Right: percentage of samples generatedusing the fine-tuned model and containing the word “amazing”. Here, the best model according to the validationloss is only able to achieve Eφ(x) = 0.5625. Higher values of Eφ(x) tend to occur with higher validation loss,i.e when overfitting.
Figure 8: Case of a pointwise binary requirement r(x) = 1: comparison with Reinforce and Ziegler. Thecurves correspond to different DKL(∙,a) levels. The manifold C is the set of distributions C s.t. c(x) >0 → r(x) = 1, or, equivalently s.t. Ex〜cr(x) = 1. The curved lines represent increasing levels of theKL divergence Dkl(q, a). According to Reinforce, any distribution PR s.t. Ex〜PRr(x) = 1, that is, anydistribution on C, is optimal. According to Ziegler, to each temperature β > 0 is associated an optimaldistribution PZ = argmi% βDκL(q,a) — Ex〜qr(x), which does not directly lie on C — this is because,as indicated in (Ziegler et al., 2019), this distribution is of the form PZ(x) (X a(x)er(x)/e, giving positiveprobability to all x’s in the support of a, including to points not lying on C. Our own optimal P does lie on Cby definition, while minimizing the KL divergence from a.
Figure 9: Exp1: Single Distributional Constraint. Balancing demographics can be represented easily throughdistributional constraints. By using a constraint SuCh as Ex〜pφfemaie(x) = 0.5, We can target balancing thefemale biographies in the distribution of all generations. Note that a point-wise objective Ex〜pφfemaie(x)=1.0 Would maximize the presence of female biographies at the expense of other demographics, inducing bias inthe opposite direction. The plot shows how Ex〜pφfemaie(x) evolves towards the defined expectation: GDC isable to reduce the bias of GPT-2bio to obtain 36.7% female biographies rather than just 7%.
Figure 10: Exp2: Multiple Distributional Constraints This experiment demonstrates the flexibility of GDCin dealing with several distributional constraints at once, even when these constraints have different objectives(increase, decrease, or keep fixed). We challenge the flexibility of GDC by setting four distributional constraintswith four arbitrary expectation values targeting Eφscience and Eφart at 40% and Eφsports and Eφbusiness at10%. In the figure, from left to right, we can note the increase of Eφscience and Eφart from 1.5% to 20.3% andfrom 10% to 31.6% respectively. Interestingly, the initial Eφbusiness of GPT-2bio (10.9%) is already very closeto the desired expectation (10%), and we can see that during the course of the training, GDC keeps this valuefixed as it is already satisfying the corresponding target distributional constraint. Eφsports initially starts higherthan the target distributional constraint 10%, and we can note that GDC succeeds to reduce it from 19.6% to11.9%.
Figure 11: Exp3: Hybrid constraints In this experiment, we specify two types of constraints: pointwise withEφart (x) = 1.0 and distributional with Eφf emale (x) = 0.5 (henceforth Hybrid). GDC in a single train-ing procedure is able to increase the expectation of biographies about females from 7.4% to 36.6% and Artprofessions from 11.4% to 88.6%.
Figure 12: Exp4: Hybrid constraints. In this experiment, we specify two types of constraints: pointwise withEφsports (x) = 1.0 and distributional with Eφfemale (x) = 0.5. GDC in a single training procedure is able toincrease the expectation of biographies about females from 7.4% to 31.9% and Sports professions from 17.5%to 92.9%.
Figure 13: Exp5: Hybrid constraints. In this experiment, we specify two types of constraints: pointwise withEφbusiness(x) = 1.0 and distributional with Eφf emale (x) = 0.5. GDC in a single training procedure is ableto increase the expectation of biographies about females from 7.4% to 37.7% and Business professions from10.1% to 82.4%.
Figure 14:	Exp6: Hybrid constraints. In this experiment, we specify two types of constraints: pointwise withEφscience(x) = 1.0 and distributional with Eφf emale (x) = 0.5. GDC in a single training procedure is able toincrease the expectation of biographies about females from 7.4% to 28.8% and Science professions from 1.2%to 74.7%.
Figure 15:	DKL(p, πθ) against the training steps for GDC and the three baselines introduced in section §3.2 forthe single-word control task. Curves are displayed for nine different single-word constraints of varying raritylevels (1/100, 1/1000, 1/10000). GDC exhibits much better convergence behaviour than the other baselines,showing its superiority in approximating the desired distribution p.
Figure 16:	DKL (p, πθ) against the training steps for GDC and the three baselines introduced in section 3.2for word-list constraints. Curves are displayed for 4 word-lists: kitchen , fantasy, politics, computers. GDCexhibits much better convergence behaviour than the other baselines, showing its superiority in approximatingthe desired distribution p.
Figure 17:	DKL (p, πθ) against the training steps for GDC and the three baselines introduced in section 3.2for classifier-based control. Curves are displayed using 4 different classifiers: very positive, positive, and verynegative sentiment, and click-bait. GDC exhibits much better convergence behaviour than the other baselines,showing its superiority in approximating the desired distribution p.
Figure 18:	Line plot of different evaluation metrics against the training steps when controlling for the word“wikileaks” (with initial occurrence probability of 1/10000) as a single-word constraint.
Figure 19:	Line plot of different evaluation metrics against the training steps when controlling for the word“vampire” (with initial occurrence probability of 1/10000) as a single-word constraint.
Figure 20:	Line plot of different evaluation metrics against the training steps when controlling for the word“amusing” (with initial occurrence probability of 1/10000) as a single-word constraint.
Figure 21:	Line plot of different evaluation metrics against the training steps when controlling for the word“Paris” (with initial occurrence probability of 1/1000) as a single-word constraint.
Figure 22: Line plot of different evaluation metrics against the training steps when controlling for the word“restaurant” (with initial occurrence probability of 1/1000) as a single-word constraint.
Figure 23: Line plot of different evaluation metrics against the training steps when controlling for the word“amazing” (with initial occurrence probability of 1/1000) as a single-word constraint.
Figure 24:	Line plot of different evaluation metrics against the training steps when controlling for the word“Canada” (with initial occurrence probability of 1/1000) as a single-word constraint.
Figure 25:	Line plot of different evaluation metrics against the training steps when controlling for the word“China” (with initial occurrence probability of 1/100) as a single-word constraint.
Figure 26:	Line plot of different evaluation metrics against the training steps when controlling for the word“US” (with initial occurrence probability of 1/100) as a single-word constraint.
Figure 27:	Line plot of different evaluation metrics against the training steps when controlling for the kitchenword-list.
Figure 28:	Line plot of different evaluation metrics against the training steps when controlling for the fantasyword-list.
Figure 29: Line plot of different evaluation metrics against the training steps when controlling for the politicsword-list.
Figure 30: Line plot of different evaluation metrics against the training steps when controlling for the com-puters word-list.
Figure 31: Line plot of different evaluation metrics against the training steps when controlling for the politicsword-list.
Figure 32:	Line plot of different evaluation metrics against the training steps for positive sentiment classifier-based control with.
Figure 33:	Line plot of different evaluation metrics against the training steps for very negative sentimentclassifier-based control with.
Figure 34: Line plot of different evaluation metrics against the training steps for click-bait classifier-basedcontrol with.
Figure 35: Token frequency against token rank for single-word constraints. Longer tail means more diversegenerations.
Figure 36: Token frequency against token rank for word-list constraints. Longer tail means more diversegenerations.
Figure 37: Token frequency against token rank for classifier-based constraints. Longer tail means more diversegenerations.
