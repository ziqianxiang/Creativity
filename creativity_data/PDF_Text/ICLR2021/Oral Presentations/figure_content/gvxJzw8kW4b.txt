Figure 1: Example comparison of existing mixup methods and the proposed Co-Mixup. Weprovide more samples in Appendix H.
Figure 2: (a) Analysis of our BP optimization problem. The x-axis is a one-dimensionalarrangement of solutions: The mixed output is more salient but not diverse towards theright and less salient but diverse on the left. The unary term (red) decreases towards theright side of the axis, while the supermodular term (green) increases. By optimizing the sumof the two terms (brown), We obtain the balanced output z*. (b) A histogram of the numberof inputs mixed for each output given a batch of 100 examples from the ImageNet dataset.
Figure 3: Visualization of the proposed mixup procedure. For a given batch of input data(left), a batch of mixup data (right) is generated, which mix-matches different salient regionsamong the input data while preserving the diversity among the mixup examples. Thehistograms on the right represent the input source information of each mixup data (oj).
Figure 4: Confidence-Accuracy plots for classifiers on CIFAR-100. From the figure, theVanilla network shows over-confident predictions, whereas other mixup baselines tend to haveunder-confident predictions. We can find that Co-Mixup has best-calibrated predictions.
Figure 5: Let us consider Co-Mixup with three inputs and two outputs. The figure representstwo Co-Mixup results. Each input is denoted as a number and color-coded. Let us assumethat input 1 and input 2 are more compatible, i.e., A12》A23 and A12》A13. Then, theleft Co-Mixup result has a larger inner-product value hpι,o±a than the right. Thus themixup result on the right has higher compatibility than the result on the left within eachoutput example.
Figure 6: Mean execution time (ms) of Algorithm 1 per each batch of data over 100 trials.
Figure 7: Confidence-Accuracy plots for classifiers on CIFAR-100. Note, ECE is calculatedby the mean absolute difference between the two values.
Figure 8: Confidence-Accuracy plots for classifiers on Tiny-ImageNet.
Figure 9:	Confidence-Accuracy plots for classifiers on ImageNet.
Figure 10:	Each subfigure shows background corrupted samples used in the robustnessexperiment. (a) Replacement with another image in ImageNet. (b) Adding Gaussian noise.
Figure 11: Input batch.
Figure 12: Mixed output batch.
Figure 13: Another mixed output batch with larger τ .
