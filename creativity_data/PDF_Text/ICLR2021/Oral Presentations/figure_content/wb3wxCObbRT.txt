Figure 1: Growing Networks during Training. We define an architecture configuration space and simultane-ously adapt network structure and weights. (a) Applying our approach to CNNs, we maintain auxiliary variablesthat determine how to grow and prune both filters (i.e. channel-wise) and layers, subject to practical resourceconstraints. (b) By starting with a small network and growing its size, we utilize fewer resources in early trainingepochs, compared to pruning or NAS methods. (c) Consequently, our method significantly reduces the totalcomputational cost of training, while delivering trained networks of comparable or better size and accuracy.
Figure 2: Technical Framework. (a) We periodically restructure a CNN by querying binary indicators thatdefine a two-level configuration space for filters and layers. (b) To make optimization feasible while growingnetworks, we derive these binary indicators from trainable continuous mask variables. We employ a structuredextension of continuous sparsification (Savarese et al., 2020), combined with sampling. Binary stochasticauxiliary variables q, sampled according to σ(βs), generate the discrete components active at a particular time.
Figure 3: Performance/FLOPs trade-offs forpruned MobileNetV1 on ImageNet.
Figure 5: Tracking validation accuracy, Com-plexity and layers for Basic3ResNet growing.
Figure 4: Epoch-wise training FLOPs for channel growing aResNet-20.
Figure 6: Pruned architectures obtained by ablated methodswith different parameter sparsity.
Figure 7: Epoch-wise retained channel ratio dy- Figure 8: Epoch-wise retained channel ratio dy-namics for each layer in ResNet-20.	namics for each layer in VGG-16.
Figure 9: Visualization of retained channel ratio dynamics for each stage in ResNet-20.
Figure 10: Visualization of retained channel ratio dynamics for each stage in VGG-16.
Figure 12: Structure-wise separate temperature dynamicsin channel growing.
Figure 11: Pruned architectures obtainedby ablated methods with different FLOPssparsity.
Figure 13: Structure-wise separate decayed tem- Figure 14: Track of epoch-wise train-time FLOPsperature dynamics in channel growing.	for channel growing in ResNet-20.
