Figure 1: A. Example of sampling an expression from the RNN. For each token, the RNN emits acategorical distribution over tokens, a token is sampled, and the parent and sibling of the next tokenare used as the next input to the RNN. Subsequent tokens are sampled autoregressively until the treeis complete (i.e. all tree branches reach terminal nodes). The resulting sequence of tokens is thetree’s pre-order traversal, which can be used to reconstruct the tree and instantiate its correspondingexpression. Colors correspond to the number of children for each token. White circles representempty tokens. Numbers indicate the order in which tokens were sampled. B. The library of tokens. C.
Figure 2: A - D. Empirical reward distributions for Nguyen-8. Each curve is a Gaussian kerneldensity estimate (bandwidth 0.25) of the rewards for a particular training iteration, using either thefull batch of expressions (A and C) or the top ε fraction of the batch (B and D), averaged over alltraining runs. Black plots (A and B) were trained using the risk-seeking policy gradient objective.
Figure 3: Recovery for variousablations of Algorithm 1 acrossall Nguyen benchmarks. Errorbars represent standard error.
Figure 4: Recovery vs dataset noiseand dataset size across all Nguyenbenchmarks. Error bars represent stan-dard error.
Figure 5: Recovery vsadded reward noise onNguyen-4. Error bars rep-resent standard error.
Figure 6: Reward training curves for DSR, PQT, VPG, and GP for the Nguyen benchmarks. Eachcurve shows the best reward (1/(1+ NRMSE)) found so far as a function of expressions evaluated,averaged across all training runs. A value of 1.0 denotes that all training runs recovered the correctexpression. Error bands represent standard deviation.
Figure 7: Recovery rate training curves for DSR, PQT, VPG, and GP for the Nguyen benchmarks.
Figure 8: Empirical reward distributions for Nguyen benchmarks, with and without risk-seekingpolicy gradients. Each group of four plots corresponds to a particular benchmark expression. Eachcurve is a Gaussian kernel density estimate (bandwidth 0.25) of the rewards for a particular trainingiteration, using either the full batch of expressions (plots labeled “Full batch”) or the top ε fractionof the batch (plots labeled “Top ε batch”), averaged over all training runs. Black plots were trainedusing the risk-seeking policy gradient objective. Blue plots were trained using the standard policygradient objective. Colorbars indicate training step. Triangle markings denote the empirical mean ofthe distribution at the final training step.
Figure 9: Training curves for all Nguyen benchmarks, with and without risk-seeking policy gradients.
