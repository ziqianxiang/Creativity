Figure 1: The first column shows images generated by off-the-shelf 2D GANs trained on RGBimages only, while the rest show that our method can unsupervisedly reconstruct 3D shape (viewedin 3D mesh, surface normal, and texture) given a single 2D image by exploiting the geometric cuescontained in GANs. The last two columns depicts 3D-aware image manipulation effects (rotationand relighting) enabled by our framework. More results are provided in the Appendix.
Figure 2: Framework outline. Starting with an initial ellipsoid 3D shape (viewed in surface nor-mal), our approach renders various ‘pseudo samples’ with different viewpoints and lighting condi-tions. GAN inversion is applied to these samples to obtain the ‘projected samples’, which are usedas the ground truth of the rendering process to refine the initial 3D shape. This process is repeateduntil more precise results are obtained.
Figure 3: Method overview. (a) Given a single image, Step 1 initializes the depth with ellipsoid(viewed in surface normal), and optimizes the albedo network A. (b) Step 2 uses the depth andalbedo to render ‘pseudo samples’ with various random viewpoint and lighting conditions, and con-ducts GAN inversion to them to obtain the ‘projected samples’. (c) Step 3 refines the depth mapby optimizing (V, L, D, A) networks to reconstruct the projected samples. The refined depth andmodels are used as the new initialization to repeat the above steps.
Figure 4: Qualitative comparisons. (a) shows the reconstructed 3D mesh, surface normal, andtextured mesh of our method. (b) shows the results of Unsup3d (Wu et al., 2020). We see thatresults in (a) are more accurate and realistic.
Figure 5: Qualitative comparison on buildings. The first row shows the input image and ourrelighting effects described in Sec.4.2. The second row shows the recovered shape (viewed in surfacenormal and mesh) of our method, while the last row shows the results of Unsup3d.
Figure 6: Results withoutsymmetry assumption.
Figure 7: 3D-aware image manipulation, including rotation and relighting. We show results ob-tained via both 3D mesh and GANs. The input of the first row is a real natural image. Our methodachieves photo-realistic manipulation effects obeying the objects’ underlying 3D structures.
Figure 8: Qualitative comparison on face ro-tation. ”Ours (GAN)” and ”Ours (3D)” indicateresults generated by GAN and rendered from3D mesh respectively. The face identities in thebaseline methods tend to drift during rotation.
Figure 10: This is an extension of Fig.1. The last row shows a car example without removing thebackground.
Figure 11: Effects of iterative training. The reconstructed object shapes get more precise at lattertraining stages.
Figure 12: (a) Our results with the symmetry assumption. (b) Our results without the symmetryassumption. (c) The results of Unsup3d Wu et al. (2020) without the symmetry assumption. Ourmethod achieves competitive results even without using the symmetry assumption.
Figure 13: Results on challenging cases. Our method achieves reasonable results for most cases.
Figure 14: Effects of different shape prior. We show results of the original ellipsoid shape, asym-metric shape with ellipsoid for the left half and sphere for the right half, shape with its positionshifted by 1/6 and 1/4 image width, weaker shape prior whose height is half of the original one, andflat shape.
