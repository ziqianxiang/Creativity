Figure 1: Left: demonstrating expansion assumption. Verifying the expansion assumption re-quires access to the population distribution and therefore we use the distribution generated by Big-GAN (Brock et al., 2018). We display typical examples of mistakenly classified images and theircorrectly classified neighbors, found by searching the entire GAN manifold (not just the trainingset). For contrast, we also display their nearest neighbors in the training set of 100K GAN images,which are much further away. This supports the intuition and assumption that expansion holds forthe population set but not the empirical set. (More details are in Section E.1.) Right: assump-tions and setting for pseudolabeling. For self-training with pseudolabels, the region of correctlypseudolabeled examples (in green) will be used to denoise examples with incorrect pseudolabels (inred), because by expansion, the green area will have a large mass which is at least c - 1 times themass of the red area. As explained in the introduction, this ensures that a classifier which fits thepseudolabels and is consistent w.r.t. input transformations will achieve high accuracy on true labels.
Figure 2: To prove Claim A.2, We first note that in the simplified setting, if B(X) ∩B(z) = 0 thenG(X) = G(Z) by the assumption that RB(G) = 0 (see left). By the definition of N?(•), this impliesthat all points x ∈ N?(V ) \ M(Gpl) must satisfy G(x) 6= G?(x), as x matches the label of itsneighbor in V ⊆ M(G). HoWever, all points in X \ M(Gpl) must satisfy Gpl(X) = G?(X), andtherefore G(X) 6= Gpl(X). These sets are depicted on the right.
Figure 3: Self-training corrects mistakenly labeled examples that are close to correctly labeledneighbors. We partition examples in M0 (defined in Section E.1) into 5 bins based on their `2distance from the neighbor used to initialize the projection, and plot the percentage of examples ineach bin whose labels were corrected by self-training. The bins are chosen to be equally sized. Theplot suggests that as a mistakenly labeled example is closer to a correctly labeled example in inputspace, it is more likely to be corrected by self-training. This supports our theoretical intuition thatinput-consistency-regularized self-training denoises pseudolabels by bootstrapping an incorrectlypseudolabeled example with its correctly pseudolabeled neighbors.
