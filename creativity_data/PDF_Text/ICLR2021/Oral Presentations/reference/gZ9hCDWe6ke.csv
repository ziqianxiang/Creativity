title,year,conference
 End-to-end object detection with transformers,2020, In ECCV
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Masked language modeling for pro-teins via linearly scalable long-context transformers,2020, arXiv preprint arXiv:2006
 Deformableconvolutional networks,2017, In ICCV
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Nas-fpn: Learning scalable feature pyramid archi-tecture for object detection,2019, In CVPR
 Deep residual learning for image recog-nition,2016, In CVPR
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Ccnet:Criss-cross attention for semantic segmentation,2019, In ICCV
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, arXiv preprint arXiv:2006
 Adam: A method for stochastic optimization,2015, In ICLR
 Reformer: The efficient transformer,2020, In ICLR
 Deep feature pyramidreconfiguration for object detection,2018, In ECCV
 Microsoft coco: Common objects in context,2014, In ECCV
 Focal loss for dense objectdetection,2017, In ICCV
 Generating wikipedia by summarizing long sequences,2018, In ICLR
 Path aggregation network for instancesegmentation,2018, In CVPR
 Image transformer,2018, In ICML
 Blockwiseself-attention for long document understanding,2019, arXiv preprint arXiv:1911
 Stand-alone self-attention in vision models,2019, In NeurIPS
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In NeurIPS
 Efficient content-based sparseattention with routing transformers,2020, arXiv preprint arXiv:2003
 Revisiting the sibling head in object detector,2020, In CVPR
 Sparse sinkhorn attention,2020, InICML
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Raft: Recurrent all-pairs field transforms for optical flow,2020, In ECCV
 Fcos: Fully convolutional one-stage objectdetection,2019, In ICCV
 Attention is all you need,2017, In NeurIPS
 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,2020, arXiv preprintarXiv:2003
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Aggregated residual trans-formations for deep neural networks,2017, In CVPR
 Auto-fpn: Automatic netWorkarchitecture adaptation for object detection beyond classification,2019, In ICCV
 Big bird: Transformers for longersequences,2020, arXiv preprint arXiv:2007
 Bridging the gap betWeenanchor-based and anchor-free detection via adaptive training sample selection,2020, In CVPR
 M2det: Asingle-shot object detector based on multi-level feature pyramid netWork,2019, In AAAI
 An empirical study of spatialattention mechanisms in deep netWorks,2019, In ICCV
