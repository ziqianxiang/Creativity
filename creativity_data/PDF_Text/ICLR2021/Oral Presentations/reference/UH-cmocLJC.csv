title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Understanding deep neuralnetworks with rectified linear units,2018, In International Conference on Learning Representations
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Generalization oftwo-layer neural networks: An asymptotic viewpoint,2020, In International Conference on LearningRepresentations
 On a routing problem,1958, Quarterly ofapplied mathematics
 Dynamic programming,1966, Science
 A theory of learning from different domains,2010, Machine learning
 On the inductive bias of neural tangent kernels,2019, In Advances inNeural Information Processing Systems
 Learningbounds for domain adaptation,2008, In Advances in neural information processing systems
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, In Advances in Neural Information Processing Systems
 A simple framework forcontrastive learning of visual representations,2020, In International Conference on Machine Learning
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, Advances in Neural Information Processing Systems
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of control
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, In Advancesin Neural Information Processing Systems
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Linearized two-layersneural networks in high dimension,2019, arXiv preprint arXiv:1904
 Extrapolation limitations of multilayer feedforward neuralnetworks,1992, In International Joint Conference on Neural Networks
 Complexity of linear regions in deep networks,2019, In InternationalConference on Machine Learning
 Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem,2019, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Multilayer feedforward networks areuniversal approximators,1989, Neural networks
 Reasoning about physical interactions with object-centric models,2019, In International Conferenceon Learning Representations
 Inferring and executing programs for visual reason-ing,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Kolmogorovâ€™s theorem and multilayer neural networks,1992, Neural networks
 Buildingmachines that learn and think like people,2017, Behavioral and brain sciences
 Deep learning for symbolic mathematics,2020, In InternationalConference on Learning Representations
 Nonlinear signal processing using neural networks: Prediction andsystem modelling,1987, Technical report
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 On the computational efficiency of training neuralnetworks,2014, In Advances in neural information processing systems
 Neural arithmetic units,2020, In InternationalConference on Learning Representations
 Domain adaptation: Learning boundsand algorithms,2009, In Conference on Learning Theory
 On the properties of periodic perceptrons,1997, In International Conference onNeural Networks
 Exploiting similarities among languages for machinetranslation,2013, arXiv preprint arXiv:1309
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in Neural Information ProcessingSystems
 Optimal rates for averaged stochastic gradient descent under neuraltangent kernel regime,2021, In International Conference on Learning Representations
 Neural tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 The risks of invariant risk mini-mization,2021, In International Conference on Learning Representations
 Distributionally robustneural networks,2020, In International Conference on Learning Representations
 Measuring abstractreasoning in neural networks,2018, In International Conference on Machine Learning
 Analysing mathematicalreasoning abilities of neural models,2019, In International Conference on Learning Representations
 Thegraph neural network model,2009, IEEE Transactions on Neural Networks
 Certifying some distributional robustness withprincipled adversarial training,2018, In International Conference on Learning Representations
 A mean field view of the landscape of two-layersneural networks,2018, Proceedings of the National Academy of Sciences
 Distributionally robust optimization and generalization in kernelmethods,2019, In Advances in Neural Information Processing Systems
 Neural arithmeticlogic units,2018, In Advances in Neural Information Processing Systems
 A theory of the learnable,1984, In Proceedings of the sixteenth annual ACM symposiumon Theory of computing
 Neural executionof graph algorithms,2020, In International Conference on Learning Representations
 Visual interaction networks: Learning a physics simulator from video,2017, In Advances inneural information processing systems
 Learning representations that support extrapolation,2020, In International Conference onMachine Learning
 Representation learning on graphs with jumping knowledge networks,2018, In InternationalConference on Machine Learning
 Neural-symbolic vqa: Disentangling reasoning from vision and language understanding,2018, In Advances inNeural Information Processing Systems 
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
 Domain generalization with mixstyle,2021, InInternational Conference on Learning Representations
