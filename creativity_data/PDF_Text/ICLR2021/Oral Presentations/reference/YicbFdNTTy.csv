title,year,conference
 Quantifying attention flow in transformers,2020, In ACL
 Learning representations by maximizingmutual information across views,2019, In NeurIPS
 Adaptive input representations for neural language modeling,2019, InICLR
 Language models arefew-shot learners,2020, arXiv
 End-to-end object detection with transformers,2020, In ECCV
 Generative pretraining frompixels,2020, In ICML
 A simple frameworkfor contrastive learning of visual representations,2020, In ICML
 UNITER: UNiversal Image-TExt Representation Learning,2020, In ECCV
 Generating long sequences with sparsetransformers,2019, arXiv
 Imagenet: A large-scale hierarchicalimage database,2009, In CVPR
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 On robustness and transferability of convo-lutional neural networks,2020, arXiv
 Deep residual learning for image recog-nition,2016, In CVPR
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Axial attention in multidi-mensional transformers,2019, arXiv
 Relation networks for objectdetection,2018, In CVPR
 Ccnet: Criss-cross attention for semantic segmentation,2020, In ICCV
 Data-efficient image recognition with contrastive predictive coding,2020, InICML
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, 2015
 Adam: A method for stochastic optimization,2015, In ICLR
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convo-lutional neural networks,2012, In NIPS
 Backpropa-gation applied to handwritten zip code recognition,1989, Neural Computation
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, arXiv
 VisualBERT: ASimple and Performant Baseline for Vision and Language,2019, In Arxiv
 Object-centric learning with slot atten-tion,2020, arXiv
 ViLBERT: Pretraining Task-Agnostic Visi-olinguistic Representations for Vision-and-Language Tasks,2019, In NeurIPS
 Cats and dogs,2012, In CVPR
 Image transformer,2018, In ICML
 Weight standardization,2019, arXivpreprint arXiv:1903
 Improving language under-standing with unsupervised learning,2018, Technical Report
 Languagemodels are unsupervised multitask learners,2019, Technical Report
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In ICCV
 Videobert: A jointmodel for video and language representation learning,2019, In ICCV
 Fixing the train-test resolutiondiscrepancy,2019, In NeurIPS
 Fixing the train-test resolutiondiscrepancy: Fixefficientnet,2020, arXiv preprint arXiv:2003
 Attention is all you need,2017, In NIPS
 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,2020, arXiv preprintarXiv:2003
 Non-local neural networks,2018, InCVPR
 Scaling autoregressive video models,2019, InICLR
 Visual transformers: Token-based image representation and processingfor computer vision,2020, arxiv
 Group normalization,2018, In ECCV
 Self-training with noisy studentimproves imagenet classification,2020, In CVPR
 S4L: Self-Supervised Semi-Supervised Learning,2019, In ICCV
 Alarge-scale study of representation learning with the visual task adaptation benchmark,2019, arXivpreprint arXiv:1910
 Exploring self-attention for image recognition,2020, InCVPR
