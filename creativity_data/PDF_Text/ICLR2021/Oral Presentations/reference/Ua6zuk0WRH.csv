title,year,conference
 Attention augmentedconvolutional networks,2019, CoRR
 Faster transformer decoding:N-gram masked self-attention,2020, CoRR
 The best of both worlds: Combiningrecent advances in neural machine translation,1008, In Proceedings of the 56th Annual Meeting ofthe Association for Computational Linguistics
 Generating long sequences with sparsetransformers,2019, CoRR
 Initialization matters: Orthogonal predic-tive state recurrent neural networks,2018, In 6th International Conference on Learning Representations
 The geometry of random features,2018, In International Conference on ArtificialIntelligence and Statistics
 KAMA-NNs:Low-dimensional rotation based neural networks,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 The unreasonable effectivenessof structured random orthogonal embeddings,2017, In Advances in Neural Information ProcessingSystems 30: Annual Conference on Neural Information Processing Systems 2017
 Protein interaction networksrevealed by proteome coevolution,2019, Science
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Energy-based models foratomic-resolution protein conformations,2020, arXiv preprint arXiv:2004
 Compiling machine learning programs via high-level tracing,2018, In Conference on Machine Learning and Systems 2018
 Dual attentionnetwork for scene segmentation,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 Musictransformer: Generating music with long-term structure,2019, In 7th International Conference onLearning Representations
 Generative models for graph-based protein design,2019, In Advances in Neural Information Processing Systems
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, CoRR
 Reformer: The efficient transformer,2020, In8th International Conference on Learning Representations
 Revealing the dark secrets ofbert,2019, arXiv preprint arXiv:1908
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, CoRR
 Parallel prefix computation,0004, J
 Demystifyingorthogonal Monte Carlo and beyond,2020, CoRR
 Simplified self-attention for transformer-basedend-to-end speech recognition,2020, CoRR
 Progen: Language modeling for protein generation,2020, CoRR
 Induction ofpotent neutralizing antibody responses by a designed protein nanoparticle vaccine for respiratorysyncytial virus,2019, Cell
 Image transformer,2018, In Proceedings of the 35th International Conferenceon Machine Learning
 Com-pressive transformers for long-range sequence modelling,2020, In International Conference on LearningRepresentations
 Random features for large-scale kernel machines,2007, In Advances inNeural Information Processing Systems 20
 Biological structure and function emerge from scaling unsupervised learning to 250million protein sequences,2019, bioArxiv
 Efficient content-based sparseattention with routing transformers,2020, CoRR
 Factorized attention:Self-attention with linear complexities,2018, CoRR
 Energy and policy considerations fordeep learning in NLP,2019, CoRR
 Neuroevolution of self-interpretable agents,2020, CoRR
 Long range arena: A benchmark for efficienttransformers,2021, 2021
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems 30
 Graph attention networks,2018, In 6th International Conference on Learning Representations
 A multiscale visualization of attention in the transformer model,2019, arXiv preprintarXiv:1906
 Analyzing the structure of attention in a transformer languagemodel,2019, CoRR
 Bertology meets biology: Interpreting attention in protein language models,2020, CoRR
 Pointer networks,2015, In Advances in NeuralInformation Processing Systems 28: Annual Conference on Neural Information Processing Systems2015
 Linformer: Self-attention withlinear complexity,2020, CoRR
 Drawing early-bird tickets: Toward more efficient training ofdeep networks,2020, In International Conference on Learning Representations
 Orthogonal random features,2016, In Advances in Neural Information ProcessingSystems 29: Annual Conference on Neural Information Processing Systems 2016
 Deepreinforcement learning with relational indUctive biases,2019, In 7th International Conference onLearning Representations
 Aligning books and movies: Towards story-like visUal explanations by watchingmovies and reading books,2015, In 2015 IEEE International Conference on Computer Vision
 The illustration is based on Vig et al,2020, (Vig
