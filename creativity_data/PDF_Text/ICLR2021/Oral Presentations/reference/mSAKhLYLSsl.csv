title,year,conference
 Gradient based sample selectionfor online continual learning,2019, In Advances in Neural Information Processing Systems
 Layer normalization,2016, arXiv preprintarXiv:1607
 Flexible dataset distillation: Learn labelsinstead of images,2020, Neural Information Processing Systems Workshop
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Generic methods for optimization-based modeling,2012, In Artificial Intelligence andStatistics
 Dynamic few-shot visual learning without forgetting,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Federated learning via synthetic data,2020, arXiv preprintarXiv:2008
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 A database for handwritten text recognition research,1994, IEEE Transactions onpattern analysis and machine intelligence
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, ArXiv
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Soft-label anonymous gastric x-rayimage distillation,2020, In 2020 IEEE International Conference on Image Processing (ICIP)
 Random search and reproducibility for neural architecture search,2020, InUncertainty in Artificial Intelligence
 Data-free knowledge distillation for deepneural networks,2017, In LLD Workshop at Neural Information Processing Systems (NIPS )
 Gradient episodic memory for continual learning,2017, In Advances in NeuralInformation Processing Systems
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In International conference on machine learning (ICML)
 Conditional generative adversarial nets,2014, arXiv preprintarXiv:1411
 Zero-shot knowledge distillation in deep networks,2019, In Proceedings of the36th International Conference on Machine Learning
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 icarl:Incremental classifier and representation learning,2017, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 A generative process for samplingcontractive auto-encoders,2012, arXiv preprint arXiv:1206
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Learning optimized map estimates in continuously-valued mrf models,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Gradient matching generative networks forzero-shot learning,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Active learning for convolutional neural networks: A core-setapproach,2018, ICLR
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Prototypical networks for few-shot learning,2017, InAdvances in neural information processing systems
 Genera-tive teaching networks: Accelerating neural architecture search by learning to generate synthetictraining data,2020, International Conference on Machine Learning
 An empirical study of example forgetting during deep neural networklearning,2019, ICLR
 Instance normalization: The missing in-gredient for fast stylization,2016, arXiv preprint arXiv:1607
 Matching networks for oneshot learning,2016, In Advances in neural information processing systems
 Dataset distillation,2018, arXivpreprint arXiv:1811
 Group normalization,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 On rectified linear units for speech processing,2013, 2013 IEEE International Conference onAcoustics
 Distilled one-shot federatedlearning,2020, arXiv preprint arXiv:2009
 Deep leakage from gradients,2019, In Advances in NeuralInformation Processing Systems
 Our performance is not sensitive to hyper-parameter se-lection,1000, The testing accuracy for various K and T
 These optimal coresets are selected by ranking their performance,1000, Obviously
