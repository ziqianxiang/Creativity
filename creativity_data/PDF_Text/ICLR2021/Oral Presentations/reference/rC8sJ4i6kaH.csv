title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, arXiv preprint arXiv:1802
 A theoretical analysis of contrastive unsupervised representation learning,2019, arXiv preprintarXiv:1902
 Co-training and expansion: Towards bridgingtheory and practice,2005, In Advances in neural information processing systems
 Does unlabeled data Provably help? worst-case analysisof the sample complexity of semi-supervised learning,2008, 2008
 A theory of learning from different domains,2010, Machine learning
 Mixmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Semi-Supervised Learning,0262, The MITPress
 A simple framework forcontrastive learning of visual representations,2020, arXiv preprint arXiv:2002
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Self-training avoids using spuriousfeatures under domain shift,2020, arXiv preprint arXiv:2006
 Spectral graph theory,1997, Number 92
 Pac generalization bounds for co-training,2002, In Advances in neural information processing systems
 Error bounds for transductive learning via compres-sion and clustering,2004, In Advances in Neural Information Processing Systems
 Self-ensembling for visual domain adapta-tion,2017, arXiv preprint arXiv:1706
 Unsupervised domain adaptation by backpropagation,2015, InInternational conference on machine learning
 Unsupervised representation learning bypredicting image rotations,2018, arXiv preprint arXiv:1803
 Effective semisupervised learning on mani-folds,2017, In Conference on Learning Theory
 Unsupervised learning:foundations of neural computation,1999, MIT press
 Cycada: Cycle-consistent adversarial domain adaptation,2018, In Internationalconference on machine learning
 Expander graphs and their applications,2006, Bulletinof the American Mathematical Society
 Learningdiscrete representations via information maximizing self-augmented training,2017, arXiv preprintarXiv:1702
 Label propagation for deep semi-supervised learning,2019, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Semi-supervised classification with graph convolutional net-works,2016, arXiv preprint arXiv:1609
 Understanding self-training for gradual domainadaptation,2020, arXiv preprint arXiv:2002
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, 2013
 Predicting what you already know helps:Provable self-supervised learning,2020, arXiv preprint arXiv:2008
 Transfer featurelearning with joint distribution adaptation,2013, In Proceedings of the IEEE international conferenceon computer vision
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Deterministic pac-bayesian generalization bounds for deepnetworks via generalizing noise-resilience,2019, arXiv preprint arXiv:1905
 Unsupervised learning of visual representations by solving jigsawpuzzles,2016, In European Conference on Computer Vision
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Statistical and algorithmic insights for semi-supervisedlearning with self-training,2020, ArXiv
 Contextencoders: Feature learning by inpainting,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Deep co-training for semi-supervised image recognition,2018, In Proceedings of the european conference on computer vision(eccv)
 Asymmetric tri-training for unsuperviseddomain adaptation,2017, arXiv preprint arXiv:1702
 Learning with labeled and unlabeled data,2000, Technical report
 A dirt-t approach to unsuperviseddomain adaptation,2018, arXiv preprint arXiv:1802
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, arXiv preprint arXiv:2001
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Whatmakes for good views for contrastive learning,2020, arXiv preprint arXiv:2005
 Contrastive estimation reveals topicposterior information to linear models,2020, arXiv preprint arXiv:2003
 De-mystifying self-supervised learning: An information-theoretical framework,2020, arXiv preprintarXiv:2006
 Deep domain confusion:Maximizing for domain invariance,2014, arXiv preprint arXiv:1412
 Adversarial discriminative domainadaptation,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Probabilistic lipschitzness: A niceness assumption for determinis-tic labels,2013, 2013
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, In Advances in Neural Information Processing Systems
 Improved sample complexities for deep networks and robust classifica-tion via an all-layer margin,2019, arXiv preprint arXiv:1910
 Unsupervised dataaugmentation for consistency training,2019, arXiv preprint arXiv:1904
 Billion-scale Semi-supervised learning for image classification,2019, arXiv preprint arXiv:1905
 Unsupervised word sense disambiguation rivaling supervised methods,1995, In 33rdannual meeting of the associationfor computational linguistics
 A hitting time analysis of stochastic gradientlangevin dynamics,2017, In Conference on Learning Theory
 Bridging theory and algorithmfor domain adaptation,2019, arXiv preprint arXiv:1904
 In the setting of Theorem 4,2021,3
 In the setting of Theorem 4,2019,3 and Section 3
