Table 1: Percentage (%) of environments where the final performance “improves” with regularization, by ourdefinition in Section 3.2.
Table 2: Average z-scores. Note that a negative z-score does not necessarily mean the method hurts, because itcould be higher than the baseline. The scores within 0.01 range from the highest are in bold.
Table 3: The average z-score for each regularization method, under five sampled hyperparameter settings.
Table 4: Percentage (%) of environments where performance “improves” when regularized on policy / value /policy and value networks.
Table 5: Comparison of final performance, policy entropy, and policy weight norm on PPO Humanoid.				Table 6: Effect of data augmentation on final performance on PPO Humanoid.		Reg	Return	Entropy	Policy Norm			Baseline	3485±302	-10.32	30.73		Baseline	L2Entropy	3805±349	4.46	30.97	w/o DA	3485±302	8148±335L2	8148±335	8.11	8.71	w/ DA	3483±293	9006±145Robustness to Training Noise. Recent works (Kostrikov et al.,2020; Laskin et al., 2020) have applied data augmentation (DA) toRL, mainly on image-based inputs, to improve data efficiency andgeneralization. Laskin et al. (2020) adds noise to state-based inputobservations by random scaling them as a form of DA. We apply thistechnique to both baseline and L2 regularization on PPO Humanoid.
Table 7: Baseline hyperparameter setting for A2C MuJoCo and RoboSchool tasks.
Table 8: Baseline hyperparameter setting for TRPO Mujoco and RoboSchool tasks. The original OpenAIimplementation does not support multiple actors sampling trajectories at the same time, so we modified the codeto support this feature and accelerate training.
Table 9: Baseline hyperparameter setting for PPO MuJoCo and RoboSchool tasks.
Table 10: Baseline hyperparameter setting for SAC.
Table 11: Sampled hyperparameter settings for A2C.
Table 12: Sampled hyperparameter settings for TRPO.
Table 13: Sampled hyperparameter settings for PPO	Learning rate	Nenvs	CliprangeBaseline	3e — 4 linear	1	0.2Hyperparam. 1	3e — 4 linear	8	0.2Hyperparam. 2	1e — 4 constant	8	0.2Hyperparam. 3	3e — 4 linear	4	0.1Hyperparam. 4	1e — 4 constant	2	0.2Hyperparam. 5	3e — 4 linear	1	0.120Published as a conference paper at ICLR 2021Table 14: Sampled hyperparameter settings for SAC	Learning rate τ ModeBaseline Hyperparam. 1 Hyperparam. 2 Hyperparam. 3 Hyperparam. 4 Hyperparam. 5	3e - 4	0.005 default 3e - 4 0.005 small 1e - 4 0.001 large 1e - 3 0.005 small 3e - 4	0.01 small 1e - 3 0.005 default21Published as a conference paper at ICLR 2021F STATISTICAL SIGNIFICANCE TEST OF z-SCORESFor each regularization method, we collect the z-scores produced by all seeds and all environmentsof a certain difficulty (e.g. for L2 on PPO and hard environments, we have 6 envs × 5 seeds = 30 z-scores), and perform Welch’s t-test (two-sample t-test with unequal variance) with the correspondingz-scores produced by the baseline. The resulting p-values for Table 2 in Section 3 and Table 3 in
Table 14: Sampled hyperparameter settings for SAC	Learning rate τ ModeBaseline Hyperparam. 1 Hyperparam. 2 Hyperparam. 3 Hyperparam. 4 Hyperparam. 5	3e - 4	0.005 default 3e - 4 0.005 small 1e - 4 0.001 large 1e - 3 0.005 small 3e - 4	0.01 small 1e - 3 0.005 default21Published as a conference paper at ICLR 2021F STATISTICAL SIGNIFICANCE TEST OF z-SCORESFor each regularization method, we collect the z-scores produced by all seeds and all environmentsof a certain difficulty (e.g. for L2 on PPO and hard environments, we have 6 envs × 5 seeds = 30 z-scores), and perform Welch’s t-test (two-sample t-test with unequal variance) with the correspondingz-scores produced by the baseline. The resulting p-values for Table 2 in Section 3 and Table 3 inSection 4 are presented in Table 15 and Table 16, respectively. Note that whether the significanceindicates improvement or harm depends on the relative mean z-score in Table 2 and Table 3. Forexample, for BN and dropout in on-policy algorithms, the statistical significance denotes harm, andin most other cases it denotes improvement. From the results, we observe that the improvement isstatistically significant (p < 0.05) for hard tasks in general, with only a few exceptions. In total, L2,L1 , entropy and weight clipping are all statistically significantly better than baseline. For Welch’st-test between entropy regularization and other regularizers, see Appendix I.
Table 15: P-values from Welch’s t-test comparing the z-scores of regularization methods and baseline.
Table 16: P-values from Welch’s t-test comparing the z-scores of regularization and baseline, under five sampledhyperparameter settings.
Table 17: Average z-scores comparing regularizers vs. baseline on MuJoCo under the default hyperparametersetting, where experiments in each environment are conducted over 10 random seeds.
Table 18: p-values from Welch’s t-test comparing the z-scores of regularization methods and baseline under thedefault hyperparameter setting and 10 random seeds.
Table 19:	Percentage (%) of environments where the final performance ”improves” when using regularization,under five randomly sampled training hyperparameters for each algorithm.
Table 20:	P-values from Welch’s t-test comparing the z-scores of entropy regularization and other regularizers,under the default hyperparameter setting.
Table 21:	P-values from Welch’s t-test comparing the z-scores of entropy regularization and other regularizers,under five sampled hyperparameter settings for each policy optimization algorithm.
Table 22: The average rank in the mean return for different regularization methods under default hyperparametersettings. L2 regularization tops the ranking for most algorithms and environment difficulties.
Table 23: The average rank in the mean return for different regularization methods, under five randomly sampledtraining hyperparameters for each algorithm.
Table 24: P-values from Welch’s t-test comparing the average rank of regularization and baseline, under thedefault hyperparmeter setting.
Table 25: P-values from Welch’s t-test comparing the average rank of regularization and baseline, under the 5randomly sampled hyperparmeter settings.
Table 26: Scaled returns for each regularization method under the default hyperparameter setting.
Table 27: P-values from Welch’s t-test comparing the scaled returns of regularization and baseline, under thedefault hyperparmeter setting.
Table 28: P-values from Welch’s t-test comparing the scaled returns of entropy and other regularizers, under thedefault hyperparmeter setting.
Table 29: Scaled returns for each regularization method under the five sampled hyperparameter settings.
Table 30: P-values from Welch’s t-test comparing the scaled returns of regularization and baseline, under fivesampled hyperparameters.
Table 31: P-values from Welch’s t-test comparing the scaled returns of entropy and other regularizers, under fivesampled hyperparameters.
Table 32: Percentage (%) of environments that, when using a regularization, ”improves”. For each algorithm,one single strength for each regularization is applied to all environments.
Table 33: The average z-score for different regularization methods. For each algorithm, one single strength foreach regularization is applied to all environments.
Table 34: The fixed single regularization strengths that are used in each algorithm to obtain results in Table 32and Table 33.
Table 35: The average z-score for each regularization method on DDPG, and p-value from Welch’s t-testcomparing the regularization methods to the baseline.
Table 36: Average z-scores on Atari envs with p-values testing regularizers against baseline.
