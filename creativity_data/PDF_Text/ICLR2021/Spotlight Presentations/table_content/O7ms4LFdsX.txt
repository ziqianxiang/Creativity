Table 1: EER on TIMIT speech datasetunder the same dimension setting ofsegment-level zc and sequence-leve zmfor FHVAE (Hsu et al., 2017), DS-VAE(full q) (Yingzhen & Mandt, 2018) andR-WAE(MMD), respectively. Small EERis better for zc and larger EER is betterfor zm .
Table 2: Comparison of averaged classification errors.
Table 3: Quantitative results on generated samples from the MUG facial dataset. "DS-VAE(NA)"means that number of actions is not incorporated (Yingzhen & Mandt, 2018). In "DS-VAE(NA)",samples are generated by fixing the encoded motions and randomly sampling content variable from theprior. Samples on DS-VAE(W), R-WAE(MMD) and R-WAE(GAN) are generated by incorporatingthe prior information(number of actions) into the model.
Table 4: Results of R-WAE(GAN) and R-WAE(MMD) on Sprites dataset.
Table 5: Encoder Network Architecture.
Table 6: Decoder Network Architecture.
Table 7:	Encoder Network ArchitectUre.
Table 8:	Decoder Network ArchitectUre.
Table 9: Prediction accuracy on generated video data, the experiment setting here is similar to Table 2in the main text. For predicting the static factor, We fix the static latent representation Zc and randomlysample zm, and examine whether the static information is preserved in the generated video (if so,the static attributes should be correctly predicted by a pretrained video classifier). For predicting thedynamic factor, we perform corresponding experiments analogously.
