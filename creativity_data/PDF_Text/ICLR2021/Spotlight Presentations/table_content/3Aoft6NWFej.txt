Table 1: SQuAD2.0 development set F1 scores of BERTBASE models trained with Random-Tokenand Whole-Word masking schemes and with different vocabulary sizes (30K; 10K; 2K).
Table 2: Dev/Test performance on the SQuAD, RACE, and GLUE benchmarks of BERT Base sizedmodels pretrained and evaluated according to section 4. We report EM (exact match) and F1 scoresfor SQuAD2 and accuracy for RACE. For GLUE we report the average scores on the developmentset and the official leaderboard scores on the test set (see the per-task scores in the appendix).
Table 3: Comparing the RACE scores of our PMI-Masked models with comparable published Base-sized models. The scores of prior MLMs were attained by finetuning released models in the samesetup of the PMI-Masked models (Section 4), except for those marked in ’!’,reported in Zhang & Li(2020). The number of examples reflects the amounts of text examined during training, as all priormodels train over the same sequence length as our PMI-Masked models, namely 512. AMBERT wastrained over Wikipedia+OpenWebText (47G), SpanBERT over Wikipedia+BookCorpus(16G), and RoBERTa over Wikipedia+BookCorpus+OpenWebText+Stories+CCNews(l6θG - see details in LiU et al. (2019)).
Table 4: The Single-Token per-plexity of MLMs trained for 1Msteps over Wiki+BookCorpus.
Table 5: Hyper-parameters of the architecture and pretraining, complementing the description inSection 4.
Table 6: The F1 score on the SQuAD2.0 development set of models taken at various checkpointsalong the pretraining of BERT Base sized models trained with different masking schemes. Thesescores are depicted in Figures 1 and 2. We finetuned on SQuAD2.0 with batch size of32 and learningrate of 3 ∙ 10-5 over 4 epochs without early stopping. We did this for 5 random initializations of thetask’s head and the reported score is an average of the three middle scores.
Table 7: The accuracy score on the RACE development set of models taken at various checkpointsalong the pretraining of BERT Base sized models trained with different masking schemes. Wefinetuned on RACE with batch size of 32 and learning rate of 3 ∙ 10-5 over 4 epochs without earlystopping. We did this for 5 random initializations of the task’s head and the reported score is anaverage of the three middle scores.
Table 8: Results on the different tasks of the GLUE benchmark. For all tasks the scores reflectaccuracy, except for STS-B (spearman score) and CoLA (Mathews Correlation). For results reportedon the development set (1M training steps), the average score is simply the average of reportedscores. For results reported on the test sets (2.4M training steps), the average score is the officialGLUE leaderboard score. The official score includes averaging of F1 scores for QQP and MRPC,as well as the default majority submission score of 65.1 for WNLI.
