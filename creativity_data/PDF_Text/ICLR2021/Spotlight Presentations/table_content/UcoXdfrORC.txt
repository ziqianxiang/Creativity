Table 1: Comparison of success rates ± standard de-viation across 5 random training seeds for our method,which combines Q-functions and planning with amodel, to a baseline that uses the Q-function to chooseactions directly without planning.
Table 2: Hyperparameters for distance learningnot tune this further due to computation constraints. We sample actions using the filtering schemedescribed in Nagabandi et al. (2020) to make sequences smoother in time. We initialize samplingdistributions using each environment’s data collection parameters, as shown in Table 4.
Table 3: Hyperparameters for model-based planningA.3 EnvironmentsThe Sawyer environments are adapted from the Meta-World benchmark (Yu et al., 2019a), and thedoor sliding environment is based off of the environment presented by Lynch et al. (2020). For eachtask, we define the 4-dimensional action space A such that actions control the Cartesian position ofthe robot’s end-effector, as well as the robot’s gripper.
Table 4: Environment and task detailsB	Comparative Evaluation Implementation DetailsB.1	Reinforcement Learning with Imagined Goals (Nair et al., 2018)In this section, we will discuss implementation details of our adaptation of Reinforcement Learningwith Imagined Goals (RIG). We begin by training a β-VAE with latent dimension 8. The VAE istrained on randomly sampled states from the entire offline dataset. For the loss, we use a combinationof a maximum likelihood term and a KL divergence term which constrains the latent space to a unitGaussian. In particular, We compute the mean pixel error, that is, HW∣∣s - ^k2, where S is theoriginal image, and S is the reconstruction, both normalized to be in [θ, l]. We add this to the KLdivergence between the latent distribution and the unit Gaussian, with a weighting factor of 1e-3 onthe KL penalty.
Table 5: Comparison of success rates for our method when trained using a dataset where object positions atthe start of each episode were greatly restricted, compared to uniform over the entire space. Standard deviationsare over 5 random seeds.
Table 6: Effect of training using negative mining on final arm position matching performance. A final `2distance to goal arm position of 0.05m or less is considered a success. Standard deviations of success rates arecomputed over 5 random seeds.
