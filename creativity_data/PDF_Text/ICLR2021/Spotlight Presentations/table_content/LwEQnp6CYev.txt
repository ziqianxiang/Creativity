Table 1: Summary of the desiderata satisfied by each reward function distance. Key - the distance is:a pseudometric (section 3); invariant to potential shaping [14] and positive rescaling (section 3); acomputationally efficient approximation achieving low error (section 6.1); robust to the choice ofcoverage distribution (section 6.2); and predictive of the similarity of the trained policies (section 6.3).
Table 2: Low reward distance from the ground-truth (GT) in PointMaze-Train predicts high pol-icy return even in unseen task PointMaze-Test. EPIC distance is robust to the choice of coveragedistribution D, with similar values across columns, while ERC and especially NPEC are sensitiveto D. Center: approximate distances (1000× scale) of reward functions from GT. The coveragedistribution D is computed from rollouts in PointMaze-Train of: a uniform random policy πuni,an expert π* and a Mixture of these policies. DS and DA are computed by marginalizing D. Right:mean GT return over 9 seeds of RL training on the reward in PointMaze-{Train,Test}, andreturns for AIRL’s generator policy. Confidence Intervals: see Table A.7.
Table A.1: Summary of hyperparameters and distributions used in experiments. The uniform random coverage distribution Dunif samples states and actions uniformly at random, and samples the next state from the transition dynamics. Random policy πuni takes uniform random actions. The synthetic expert policy ∏* was trained with PPO on the ground-truth reward. Mixture samples actions from either ∏uni or ∏*, switching between them at each time step with probability 0.05. Warmstart Size is the size of the dataset used to compute initialization parameters described in section A.1.2.		Parameter	Value	In experiment	Random transitions Dunif	GridWorldCoverage Distribution D	Rollouts from πuni	PointMass, HalfCheetah, Hopper	∏uni, π* and Mixture	PointMazeBootstrap Samples	10 000	AllDiscount γ	0.99	AllEPIC		State Distribution DS	Marginalized from D	AllAction Distribution DA	Marginalized from D	AllSeeds	30	AllSamples NV	32 768	AllMean Samples NM	32 768	AllNPEC		Seeds	3	AllTotal Time Steps	1 × 106	AllOptimizer	Adam	AllLearning Rate	1 × 10-2	AllBatch Size	4096	AllWarmstart Size	16 386	All
Table A.2: Hyperparameters for proximal policy optimisation (PPO) [19]. We used the implemen-tation and default hyperparameters from Hill et al. [9]. PPO was used to train expert policies onground-truth reward and to optimize learned reward functions for evaluation.
Table A.3: Hyperparameters for adversarial inverse reinforcement learning (AIRL) used in Wanget al. [24].
Table A.4: Hyperparameters for preference comparison used in our implementation of Christianoet al. [6].
Table A.5: Hyperparameters for regression used in our implementation of Christiano et al. [6, targetmethod from section 3.3].
Table A.6: Time and resources taken by different metrics to perform 25 distance comparisonson PointMass, and the confidence interval widths obtained (smaller is better). Methods EPIC,NPEC and ERC correspond to Figures 2(a), (b) and (c) respectively. EPIC Quick is an abbreviatedversion with fewer samples. RL (PPO) is estimated from the time taken using PPO to train a singlepolicy (16m:23s) until convergence (106 time steps). EPIC samples NM + NV time steps from theenvironment and performs NMNV reward queries. In EPIC Quick, NM = NV = 4096; in EPIC,NM = NV = 302768. Other methods query the reward once per environment time step.
Table A.7: Approximate distances of reward functions from the ground-truth (GT). We report the 95%bootstrapped lower and upper bounds, the mean, and a 95% bound on the relative error from the mean.
Table A.8: Approximate distances of reward functions from the ground-truth (GT) under pathologicalcoverage distributions. We report the 95% bootstrapped lower and upper bounds, the mean, and a95% bound on the relative error from the mean. Distances (1000× scale) use four different coveragedistributions D. σ independently samples states, actions and next states from the marginal distri-butions of rollouts from the uniform random policy πuni in the PointMaze-Train environment.
