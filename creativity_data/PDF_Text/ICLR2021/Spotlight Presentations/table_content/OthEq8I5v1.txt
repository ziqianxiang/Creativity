Table 1: Comparison of DISCERN with and without MUSICMethod	Push (%)	Pick & Place (%)DISCERN	7.94% ± 0.71%	4.23% ± 0.47%R (Task Reward)	11.65% ± 1.36%	4.21% ± 0.46%R+DISCERN	21.15% ± 5.49%	4.28% ± 0.52%R+DISCERN+MUSIC 95.15% ± 8.13%		48.91% ± 12.67%This is because that MUSIC emphases more on state-control and teaches the agent to interact withan object. Afterwards, DISCERN teaches the agent to move the object to the goal position in eachepisode.
Table 2: Comparison of variational MI (v)-based and MINE (m)-based MUSICMethodPush (%)Pick & Place (%)Task-r+MUSIC(v)Task-r+MUSIC(m)94.9% ± 5.83%94.83% ± 4.95%49.17% ± 4.9%50.38% ± 8.8%the performance of these two MI estimation methods are similar. However, the variational methodintroduces additional complicated sampling mechanisms, and two additional hyper-parameters, i.e.,the number of the candidates and the type of the similarity measurement (Barber & Agakov, 2003;Eysenbach et al., 2019; Warde-Farley et al., 2019). In contrast, MINE-style MUSIC is easier toimplement and has less hyper-parameters to tune. Furthermore, the derived objective improves thescalability of the MINE-style MUSIC.
Table 3: Mutual Information estimation prior and post to the trainingMutual Information Objective	Prior-train Value	Post-train ValueMI(grip_pos; object_pos)	0.003 ± 0.017	0.164 ± 0.055MI(grip_pos; object_rot)	0.017 ± 0.084	0.461 ± 0.088MI(grip_pos; object_velp)	0.005 ± 0.010	0.157 ± 0.050MI(grip_pos; object_velr)	0.016 ± 0.083	0.438 ± 0.084We can see that with the intrinsic reward MI(grip_pos; object _pos), the agent achieves a highMI after training, which means that the agent learns to better control the object positions using itsgripper. Similarly, in the second row of the table, with MI(grip_pos; object_rot), the agentlearns to control object rotation with its gripper.
