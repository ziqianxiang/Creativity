Table 1: Median execution times per batch (out of 100 runs) for various models, giving both thelargest input size that unmodified PyTorch (“PT”) could support on our GPU and larger input sizesDTR could support. Input sizes are as in Figure 4, except for TreeLSTM (complete binary trees withnodes of size 1024 × 1024) and Transformer (sequence length 256). Asterisks indicate inputs onwhich the random sampling optimization was disabled due to occasional failed trials. Even withoutsampling, DTR still occasionally failed on UNet (see Appendix E.3 for details). This behavior maybe due to PyTorch memory allocator implementation details or poor rematerialization decisionsinfluenced by variance in individual operator times.
