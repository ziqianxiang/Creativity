Table 1: Outcome of shuffled RMSprop under different hyperparameter settings. On unconstrained problems, ifthe gradient norm can decrease to zero after infinite iterations, we classify the result as “convergence to criticalpoints”. If the gradient norm keeps increasing to infinity during training, we classify the result as “divergence”.
Table 2: Performance of Adam with different β2 with ResNet-18 on CIFAR-10 (100 epochs)batch size	measure	β2=0.8	β2=0.9	β2=0.95	β2=0.99	SGD8	train acc. test acc.	10.00±0.00 10.00±0.00	10.00±0.00 10.00±0.00	44.53±32.09 42.02±29.59	99.74±0.06 70.23±0.26	100.00±0.00 70.37±0.4516	train acc. test acc.	28.70±32.39 27.64±30.55	67.27± 8.98 62.71±7.71	96.38±1.35 70.11±0.90	99.75±0.05 70.43±0.15	99.98±0.02 69.45±0.3832	train acc. test acc.	66.93±3.07 62.99±2.13	96.72±1.36 70.05±1.40	99.17±0.42 71.92±0.50	99.80±0.14 71.34±0.60	81.50±1.57 68.92±1.12Next, we demonstrate that the convergence speed of SGD is much slower than Adam under the sameexperiment setting as Table 2. We compare the average training and test accuracy at the 10-th epoch.
Table 3: Training and test accuracy at the 10-th epochbatch size	measure	β2=0.99	SGD16	train acc.	95.41±0.81	65.89±1.28	test acc.	70.02±0.17	62.62±1.2632	train acc.	97.92±0.23	57.87±0.70	test acc.	70.44±0.19	56.18±0.866	ConclusionIn this work, we study the convergence behavior of RMSprop by taking a closer look at the hyper-parameters. Specifically, for realizable problems, we provide a data-dependent threshold of β2above which we prove the convergence of randomly shuffled RMSprop and small β1 Adam withoutbounded gradient assumption. We also show that RMSprop converge into a bounded region undernon-realizable settings. These findings reveal that there is a critical threshold of β2 regarding theconvergence behavior of RMSprop, and the phase transition is supported by the numerical experiments.
