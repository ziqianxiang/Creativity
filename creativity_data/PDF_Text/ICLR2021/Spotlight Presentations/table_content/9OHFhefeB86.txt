Table 1: Results on CK+ and FER13, with comparison to CNNt(Ding et al., 2017), CNN* (GUo et al., 2016),landmark method using handcrafted features (Morales-Vargas et al., 2019), and various GNN methods. Specifically,We compare to GAT (VelickoVic et al., 2017) with different #heads (h) and #features (f). The mean testing timeon CK+: ChebNet (L=4) 12.56ms, L3Net (order 1,1,2,3) 13.02ms. GAT (h=f=8) 39.67ms, (h=f=16) 41.02ms.
Table 2: Results on NTU-RGB+D and Kinetics-Motion			NTU-RGB+D				Kinetics-Motion	Model	Bases order	#ParamS (WIo FC)	X-VieW ACC	X-Sub Acc	#ParamS (w/o FC)	AccST-GCN (Yan et al., 2018)	1	二	-	88.30 =	81.50 =	-	72.4ST-GCN	1	276M	8239	7433	14M	72.85ST-ChebNet	L=3 L=4 L=5	3ΓM 3.3M 3.5M	86:40 86.45 76.70	78724 80.20 71.42	18M 2.1M 2.3M	77.91 78.24 77.57ST-L3Net	-W- + reg0.01	3.1M	90778 88.38	83.64- 81.54	1.8M	75.20 78.49	1；1⑵3 + reg0.01	3.3M	91.52 89.87	8246 80.97	2.1M	75.07 76.684.3	Action recognitionWe test on two skeleton-based action recognition datasets, NTU-RGB+D (Shahroudy et al.,2016) and Kinetics-Motion (Kay et al., 2017). The irregular mesh is the 18/25-point bodylandmarks, with graph edges defined by body joints, shown in Fig. 1 and Fig. A.2. Weadopt ST-GCN (Yan et al., 2018) as the base architecture, and substitute the GCN layerwith new L3Net layer, called ST-L3Net. On Kinetics-Motion, we adopt the regularizationmechanism to overcome the severe data missing caused by camera out-of-view. See moreexperimental details in Appendix C.3. We benchmark performance with ST-GCN (Yanet al., 2018), ST-GCN (our implementation without using geometric information) and ST-ChebNet (replacing GCN with ChebNet layer), shown in Table 2. L3Net shows significantadvantages on two NTU tasks, cross-view and cross-subject settings. On Kinetics-Motion,L3Net regains superiority over other models after applying regularization. The results in
Table 3: Results on MNIST with grid size 7 × 7 with different levels of Gaussian noise and Permutation noise.
Table A.1: results of 1-gcn layer modelsGnn model	order	#ParamS	ring graph ACC	chain graph Acc	L=3	0.2k	-50.80 ± 0.24-	50.66 ± 0.21ChebNet	L=5	0.3k	51.14 ± 0.21	51.07 ± 0.35	L = 9	0.4k	51.68 ± 0.38	50.96 ± 0.29	L=30	1.1k	51.37 ± 0.14	50.70 ± 0.16L3Net	1	0.3k	-99.96 ± 0.08-	99.67 ± 0.12	0；1；2	0.8k	99.96 ± 0.01	99.92 ± 0.01of unit sphere, and follow Jiang et al. (2019) by moving projected digit to equator, avoidingcoordinate singularity at poles.
Table A.2: Results on SphereMNIST and SphereModelNet-40 following setup in Jiang et al. (2019)Model	SPhereMNIST ACC	SPhereModelNet-40 ACCS2CNN (Cohen et al., 2018)	9670	85.0UGSCNN (Jiang et al., 2019)	99.2	90.50GCN	95.8	87.07ChebNet(L=4)	993	88.05ChebNet(L=5)	-	88.90ChebNet(L=6)	-	88.70ChebNet(L=7)	-	88.78L3Net (1123)	99710	90.24L3Net (112)	98.90	89.67C.2 Facial Expression RecognitionLandmarks setting 15 landmarks are selected from the standard 68 facial landmarksdefined in AAM (Cootes et al., 2001), and edges are connected according to prior informationof human face, e.g., nearby landmarks on the eye are connected, see Fig. 1 (left).
Table A.3: Results on MNIST with grid size 28 × 28,L3Net-pooling uses graph pooling between convolu-tional layers.
Table A.4: Results on MNIST with grid size 14 × 14Model	bases order	#ParamS (w/o FC)	AccGCN	1	2.4k -	~93.70 ± 0.09-	L=3	6.5k	~96.06 ± 0.16-	L =4	8.6k	96.85 ± 0.11ChebNet	L=5	10.7k	97.24 ± 0.28	L=6	12.8k	97.58 ± 0.10	L=7	14.9k	97.74 ± 0.07	0112	13.3k	~97.17 ± 0.09-	1；1；2	14.8k	97.24 ± 0.12L3Net	1;1;2 reg0.001	14.8k	97.43 ± 0.07	1；1；2；3	25.1k	97.51 ± 0.07Table A.5: Results on MNIST with grid size 7 × 7 with different levels of missing valueModel	bases order	reg	#ParamS (w/o FC)	Acc(original)	Acc(psnr 18.70)	Acc(Psnr 15.33)	Acc(psnr 13.15)GCN	1	-	24k	90.02 ± 0.24	-83.44 ± 0.15-	77.23 ± 0.13	-71.67 ± 0.06	-L=3-	-	65k	92.85 ± 0.09	-87.09 ± 0.18-	82.11 ± 0.18	-76.15 ± 0.26	L=4	-	8.6k	93.12 ± 0.1	87.09 ± 0.16	82.22 ± 0.28	75.95 ± 0.22ChebNet	L=5	-	10.7k	93.2 ± 0.07	87.01 ± 0.14	82.04 ± 0.14	76.21 ± 0.38	L=6	-	12.7k	93.42 ± 0.09	87.20 ± 0.3	81.19 ± 0.29	75.24 ± 0.32	L=7	-	14.8k	93.45 ± 0.06	87.08 ± 0.11	81.00 ± 0.17	75.31 ± 0.34
Table A.5: Results on MNIST with grid size 7 × 7 with different levels of missing valueModel	bases order	reg	#ParamS (w/o FC)	Acc(original)	Acc(psnr 18.70)	Acc(Psnr 15.33)	Acc(psnr 13.15)GCN	1	-	24k	90.02 ± 0.24	-83.44 ± 0.15-	77.23 ± 0.13	-71.67 ± 0.06	-L=3-	-	65k	92.85 ± 0.09	-87.09 ± 0.18-	82.11 ± 0.18	-76.15 ± 0.26	L=4	-	8.6k	93.12 ± 0.1	87.09 ± 0.16	82.22 ± 0.28	75.95 ± 0.22ChebNet	L=5	-	10.7k	93.2 ± 0.07	87.01 ± 0.14	82.04 ± 0.14	76.21 ± 0.38	L=6	-	12.7k	93.42 ± 0.09	87.20 ± 0.3	81.19 ± 0.29	75.24 ± 0.32	L=7	-	14.8k	93.45 ± 0.06	87.08 ± 0.11	81.00 ± 0.17	75.31 ± 0.34	1；1；2	-	Ok	93.56 ± 0.08	-86.64 ± 0.16-	81.14 ± 0.30	-75.07 ± 0.08	1;1;2	0.5	8.4k	93.85 ± 0.13	87.22 ± 0.23	82.84 ± 0.11	76.48 ± 0.23L3Net	1;1;2;3	-	12.2k	93.67 ± 0.15	86.51 ± 0.38	80.68 ± 0.11	74.24 ± 0.36	1；1；2；3	0.5	12.2k	93.85 ± 0.15	87.22 ± 0.08	82.64 ± 0.31	76.08 ± 0.38C.4.2 Network architecture and training detailsWe use the same architecture for different experiment settings:GraphConv(1,32)-BN-ReLU-GraphConv(32,64)-BN-ReLU-FC(10),where GraphConv can be different types of graph convolution layers. We set batch size to100, use Adam optimizer, and set initial learning rate to 1e-3. Learning rate will drop by10 if the least validation loss remains the same for the last 15 epochs. We set total trainingepochs as 200. We use 10,000 images for training.
