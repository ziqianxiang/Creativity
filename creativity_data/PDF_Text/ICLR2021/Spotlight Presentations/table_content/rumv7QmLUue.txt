Table 1: Accuracy of pruned models on CIFAR-10/CIFAR-100 (over 3 seeds; base model accuraciesare reported in parentheses; std. dev. < 0.3 for all experiments; best results are in bold, second bestare underlined). Magnitude-based pruning (Mag.) and the proposed extension to loss preservation(Proposed: P§&∈θ ∣θi(t)∣∣θi(t)g (θi(t)) |) consistently outperform plain loss-preservation basedpruning (Loss). With more rounds, the proposed measure outperforms Magnitude-based pruning too.
Table 2: Accuracy of models pruned using different variants of gradient-norm based pruning onCIFAR-10/CIFAR-100 (over 3 seeds; base model accuracies are reported in parentheses; std. dev.
Table 3: Reduction in test accuracy for CIFAR-100 models pruned randomly, uniformly, and usingthe importance measures analyzed in the main paper. Test accuracies of base models are reported inthe table. ^ When pruning is distributed over several rounds, it often leads to small amounts of pruningin a given round. Since only integer number of filters can be pruned, a small ratio will result in nopruning at a layer with small number of filters and other layers will be pruned more to compensatefor the same. While this issue does not arise in VGG-13 and MobileNet-V1 models, for ResNet-56,which has several layers with only 16 filters per layer, often very minimal amounts of pruning takesplace. Due to this, later layers are pruned more aggressively than earlier layers, resulting in poorperformance in both 5 and 25 rounds of random/uniform pruning for ResNet-56.
