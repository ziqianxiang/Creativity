Table 1: Loss by network with different configurations of stochastic layers on ImageNet-32(similar trends appear on CIFAR-10). Left: Networks with equal number of layers, but with lowerstochastic depth as described in Section 5.1. Increasing depth up to 48 layers still shows gains,which is farther than previous work has explored. Right: Networks with 48 layers, but distributedat different resolutions. We find higher resolutions benefit more from layers.
Table 2: Our main results on standard benchmark datasets. Very deep VAEs outperformPixelCNN-based autoregressive models with fewer parameters while maintaining fast sampling.
Table 3: Effects of scaling residual initialization on very deep VAEs. We trained networks withvarying depths for 80k iterations. Scaling the last layer in the residual block by âˆšN results inhigher losses for shallower networks, but lower losses and greater stability for deeper networks. Thenumber of updates which are skipped because the gradient norm would destabilize the network issignificantly reduced with scaling.
Table 4: Key hyperparameters for experiments. We detail here the main hyperparameters used intraining. FFHQ-1024 has reduced hidden size for higher resolutions; see code for details.
