Table 1: Comparison of approaches to long-tail learning. Weight normalisation re-scales the classifi-cation weights; by contrast, we add per-label offsets to the logits. Margin approaches uniformlyincrease the margin between a rare positive and all negatives (Cao et al., 2019), or decrease themargin between all positives and a rare negative (Tan et al., 2020) to prevent rare labels’ gradientsuppression. By contrast, we increase the margin between a rare positive and a dominant negative.
Table 2: Test set balanced error (averaged over 5 trials) on real-world datasets. Here, t, ?, ^ arenumbers for “LDAM + SGD” and “LDAM + DRW” from Cao et al. (2019,Table 2, 3); “T-normalised”from Kang et al. (2020, Table 3, 7); and “Class-Balanced” from Cui et al. (2019, Table 2, 3). Here,τ = τ* refers to using the best possible tuning parameter τ; see Figure 3 for results on various T.
Table 3: Test set balanced error (averaged over 5 trials) on real-world datasets with more complexbase architectures. Employing a ResNet-152 systematically improves all methods’ performance,with logit adjustment remaining superior to existing approaches. The final row reports the results ofcombining logit adjustment with the adaptive margin loss of Cao et al. (2019), which yields furthergains on iNaturalist.
Table 4: Test set balanced error (averaged over 5 trials) on CIFAR-10-LT and CIFAR-100-LT underthe Step-100 profile; lower is better. On CIFAR-100-LT, weight normalisation edges out logitadjustment. See Figure 9 for a demonstrated that tuned versions of the same outperfom weightnormalisation.
