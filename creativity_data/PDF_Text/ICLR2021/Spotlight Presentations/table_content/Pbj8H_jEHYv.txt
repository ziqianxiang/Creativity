Table 1: Trained without normalizing inputs, mean/s.d. from 5 experiments reported. Our Cayleylayer outperforms other methods in both test and `2 certifiable robust accuracy.
Table 2: Empirical adversarial robustness for residual networks, mean and standard deviation forResNet9 from 3 experiments. Cayley layers perform competitively on clean and robust accuracy.
Table 3: Trained with normalizing inputs, mean and standard deviation from 5 experiments reported.
Table 4: Here we multiplied the input channels and output channels of each layer of KWLarge bywidth; we report on changes in accuracy and average runtime per epoch (100 epochs). Width 1 wason a Nvidia RTX 2080 Ti, while 2, 3, 6, and 8 were on a Nvidia Quadro RTX 8000. In this case,we were unable to scale BCOP to width 8 due to time and memory constraints. Generally, the widernetworks may need more epochs to converge.
Table 5: Our Cayley layer was not as fast for residual networks, possibly because they have convo-lutions with more channels and also larger spatial dimension, which is a multiplicative factor in ourruntime analysis. This is especially true for the WideResNet. For plain conv, we replaced the Cayleyconvolutional layer with a plain circular convolution, leaving the Cayley fully-connected layers. Forboth plain, we also used plain fully-connected layers.
Table 6: Additional baseline for KWLargetrained for provable robustness. Mean ands.d. over 5 trials.
Table 7: Additional baselines for ResNet9 trainedfor empirical adversarial robustness. Mean ands.d. over 3 trials.
Table 8: For BCOP, RKO and OSSN, we report the best bound over all trials from the experimentsin the repository containing BCOPâ€™s implementation (Li et al., 2019). We ran one trial of theWasserstein GAN experiment, replacing the BCOP and Bjorck layers with our Cayley convolutionaland linear layers, and achieved a significantly tighter bound. We only report on experiments usingthe GroupSort (MaxMin) activation (Anil et al., 2019) and on the STL-10 dataset.
