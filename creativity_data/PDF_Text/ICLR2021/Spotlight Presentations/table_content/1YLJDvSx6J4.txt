Table 1: GVP-GNN outperforms Structured Transformer and sets a new state-of-the art on theCATH 4.2 protein design test set (and its short and single-chain subsets) in terms of per-residueperplexity (lower is better) and recovery (higher is better). Recovery is reported as the median (overall structures) of the average % of residues correctly recovered in 100 sampled sequences.
Table 2: GVP-GNN improves over other single-structure, structure-only methods on CASP 11 and12 in terms of global (Glob) and mean per-target (Per) Pearson correlation coefficients (higher isbetter). Each method is classified as one of the three types discussed in Section 2. ProQ3D is setaside as the only method shown which additionally uses sequence-based profiles. For each metric,the top performing structure-only method is in bold, as is the top method overall (if different).
Table 3: GVP-GNN improves over single-structure methods participating in CASP 13 on the 20evaluated targets. The seven top methods highlighted by the CASP organizers are shown. GVP-GNN is the top structure-only method and the top method overall in terms of global correlation.
Table 4: Ablations of the GVP architecture decrease performance on CPD and MQA. We includeStructured GNN and GraphQA as state-of-the-art GNN references for CPD and MQA, respectively.
Table 5: Performance of the three compared model architectures on the off-center (geometric),perimeter (relational), and combined objectives. The MSE losses are standardized such that predict-ing a constant value (i.e. the mean) would result in unit loss. Results are reported as the mean ±S.D. over k = 5 random splits, where the best of three random seeds is taken for each split.
Table 6: MQA datasetsDataset	# Targets	# Structures	Includes natives?Training	480	72000	YesValidation	48	7200	YesCASP 11 stage 1	84	1680	NoCASP 11 stage 2	83	12450	NoCASP 12 stage 1	40	800	NoCASP 12 stage 2	40	5950	NoCASP 13 stage 2	20	1472	No15Published as a conference paper at ICLR 2021D	Training and HyperparametersTo train the MQA model to perform regression against the model quality score, we use a sum of anabsolute loss and a pairwise loss. That is, for each training step we intake pairs i, j where i, j arecandidate structures for the same target and computeL = H (W- y⑴)+ H(y(j) - y(j))+ H «y(i) - y(j))-(y(i) - y(j)))	⑹where H is the Huber loss. When reshuffling at the beginning of each epoch, we also randomly pairup the candidate structures for each target. Interestingly, adding the pairwise term also improvesglobal correlation, likely because the much larger number of possible pairs makes it more difficultto overfit.
Table 7: Comparison of our GVP architecture, DimeNet, and the GraphQA, another GNN-basedMQA method, on CASP 11-12. As in the main text, the global and mean per-target Pearson corre-lations are shown. DimeNet does not perform comparably to either GVP-GNN or GraphQA.
Table 8: Comparison of GVP-GNN against all 23 available single-structure MQA methods in CASP13 sorted by global correlation. The total number of predictions is shown, which may be less than1472 even though they include all 20 targets. GVP-GNN is the best-performing method in terms ofglobal correlation. In terms of per-target correlation, GVP-GNN outperforms all other structure-onlymethods and also all methods using sequence profiles except for ProQ4 and two ProQ3D variants.
Table 9: Sequence recovery on TS50. Recovery for GVP-GNN and ProteinSolver is as definedin Table 1; recovery for other methods, which model residues independently, is just classificationaccuracy. GVP-GNN is the second best-performing method, behind the CNN method DenseCPD.
