Table 1: ComPutational costs (uP to a ProPortion-ality constant) of gradient calculation methods fordense and sParse RNNs. Below T refers to the se-quence length, k the number of hidden units, p thenumber of dense recurrent Parameters, s the levelof sParsity, and d = 1 - s. The first term of thecomPute cost is for going forward and the secondterm is for either going backward or uPdating theinfluence matrix.
Table 2: Empirical computational costs of SnAp, determined by the sparsity level in the Jacobians.
Table 3: Final performance ofsparse WikiText103 language mod-eling GRU networks trained withprogressive pruning. Each row rep-resents a single training run. The‘bpc’ column gives the validationset negative log-likelihood in unitsof bits per character. The ∣θ∣ Col-umn gives the number of parametersin the network as a multiple of the‘base’ 128-unit model.
Table 4: APProximation Quality of SnAP-1 andSnAP-2. Average magnitudes in the influence ma-trix versus whether or not they are kePt by an aP-Proximate method. The “SnAP-1” and “SnAP-2”columns show the average magnitude of entrieskePt by the SnAP-1 and SnAP-2 aPProximationsresPectively. In Parentheses is the sum of the mag-nitudes of entries in this category divided by thesum of all entry magnitudes in the influence ma-trix.
