Table 1: Performance on the copying task (left) and Sequential MNIST resolution generalization (right). Whileall of the methods are able to learn to copy for the length seen during training, the RIMs model generalizes tosequences longer than those seen during training whereas the LSTM, RMC, and NTM degrade much more. Onsequential MNIST, both the proposed and the Baseline models were trained on 14x14 resolution but evaluated atdifferent resolutions (averaged over 3 trials).
Table 2: A concise comparison of recurrent models with modular memory.
Table 3: HyperparametersParameter	ValueOptimizer	Adam(Kingma & Ba, 2014)learning rate	7∙10-4batch size	64Input keys	64Input Values	Size of individual RIM * 4Input Heads	4Input Dropout	0.1Communication keys	32Communication Values	32Communication heads	4Communication Dropout	0.1C.3 Other Architectural Changes that we ExploredWe have not conducted systematic optimizations of the proposed architecture. We believe that even principledhyperparameter tuning may significantly improve performance for many of the tasks we have considered in thepaper. We briefly mention a few architectural changes which we have studied:•	On the output side, we concatenate the representations of the different RIMs, and use the concatenatedrepresentation for learning a policy (in RL experiments) or for predicting the input at the next timestep (for bouncing balls as well as all other experiments). We empirically found that adding another
Table 4: Error (CE for last 10 time steps) on the copying task. Note that while all of the methods are able tolearn to copy on the length seen during training, the RIMs model generalizes to sequences longer than thoseseen during training whereas the LSTM fails catastrophically.
Table 5: Error CE on the adding task.
Table 6: Imitation Learning: Results on the half-cheetah imitation learning task. RIMs outperforms a baselineLSTM when we evaluate with perturbations not observed during training (left). An example of an input imagefed to the model (right).
Table 7: Scores obtained using PPO with the LSTM architecture and PPO with the RIMs architecture withkA = 5.
Table 8: Transfer from WMT to IWSLT (en → de) for different models. Results in BLEU score (higher isbetter).
