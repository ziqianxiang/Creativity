Table 1: The PlaNet benchmark at 100k and 500k environment stePs. Our method (DrQ [K=2,M=2])outPerforms other aPProaches in both the data-efficient (100k) and asymPtotic Performance (500k)regimes. Random shifts only version (e.g. DrQ [K=1,M=1]) has a comPetitive Performance butis consistently inferior to DrQ [K=2,M=2], Particularly for 100k stePs. We emPhasize, that bothversions of DrQ use exactly the same number of interactions with both the environment and rePlaybuffer. Note that DrQ [K=1,M=1] is almost identical to RAD (Laskin et al., 2020), modulo somehyPer-Parameter differences.
Table 2: The action repeat hyper-parameter used for each task in the PlaNet benchmark.
Table 3: An overview of used hyper-parameters in the DeepMind control suite experiments.
Table 4: A complete overview of hyper parameters used in the Atari 100k experiments.
Table 5: Mean episode returns on each of 26 Atari games from the setup in Kaiser et al. (2019). Theresults are recorded at the end of training and averaged across 5 random seeds (the CURLâ€™s resultsare averaged over 3 seeds as reported in Srinivas et al. (2020)). On each game we mark as bold thehighest score. Our method demonstrates better overall performance (as reported in Figure 5).
