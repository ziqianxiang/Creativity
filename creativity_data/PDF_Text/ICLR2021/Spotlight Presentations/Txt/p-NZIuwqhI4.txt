Published as a conference paper at ICLR 2021
On the Theory of Implicit Deep Learning:
Global Convergence with Implicit Layers
Kenji Kawaguchi
Harvard University
Cambridge, MA 02138, USA
kkawaguchi@fas.harvard.edu
Ab stract
A deep equilibrium model uses implicit layers, which are implicitly defined
through an equilibrium point of an infinite sequence of computation. It avoids
any explicit computation of the infinite sequence by finding an equilibrium point
directly via root-finding and by computing gradients via implicit differentiation.
In this paper, we analyze the gradient dynamics of deep equilibrium models
with nonlinearity only on weight matrices and non-convex objective functions of
weights for regression and classification. Despite non-convexity, convergence to
global optimum at a linear rate is guaranteed without any assumption on the width
of the models, allowing the width to be smaller than the output dimension and the
number of data points. Moreover, we prove a relation between the gradient dy-
namics of the deep implicit layer and the dynamics of trust region Newton method
of a shallow explicit layer. This mathematically proven relation along with our
numerical observation suggests the importance of understanding implicit bias of
implicit layers and an open problem on the topic. Our proofs deal with implicit
layers, weight tying and nonlinearity on weights, and differ from those in the re-
lated literature.
1	Introduction
A feedforward deep neural network consists of a stack of H layers, where H is the depth of the
network. The value for the depth H is typically a hyperparameter and is chosen by network designers
(e.g., ResNet-101 in He et al. 2016). Each layer computes some transformation of the output of the
previous layer. Surprisingly, several recent studies achieved results competitive with the state-of-
the-art performances by using the same transformation for each layer with weight tying (Dabre &
Fujita, 2019; Bai et al., 2019b; Dehghani et al., 2019). In general terms, the output of the l-th layer
with weight tying can be written by
z(l) = h(z(l-1) ; x, θ)	for l = 1, 2, . . . , H - 1,
(1)
where x is the input to the neural network, z(l) is the output of the l-th layer (with z(0) = x), θ
represents the trainable parameters that are shared among different layers (i.e., weight tying), and
z(l-1) 7→ h(z(l-1) ; x, θ) is some continuous function that transforms z(l-1) given x and θ . With
weight tying, the memory requirement does not increase as the depth H increases in the forward
pass. However, the efficient backward pass to compute gradients for training the network usually
requires to store the values of the intermediate layers. Accordingly, the overall computational re-
quirement typically increases as the finite depth H increases even with weight tying.
Instead of using a finite depth H , Bai et al. (2019a) recently introduced the deep equilibrium model
that is equivalent to running an infinitely deep feedforward network with weight tying. Instead
of running the layer-by-layer computation in equation (1), the deep equilibrium model uses root-
finding to directly compute a fixed point z* = limι→∞ z(l), where the limit can be ensured to exist
by a choice of h. We can train the deep equilibrium model with gradient-based optimization by
analytically backpropagating through the fixed point using implicit differentiation (e.g., Griewank
& Walther, 2008; Bell & Burke, 2008; Christianson, 1994). With numerical experiments, Bai et al.
(2019a) showed that the deep equilibrium model can improve performance over previous state-of-
the-art models while significantly reducing memory consumption.
1
Published as a conference paper at ICLR 2021
Despite the remarkable performances of deep equilibrium models, our theoretical understanding of
its properties is yet limited. Indeed, immense efforts are still underway to mathematically understand
deep linear networks, which have finite values for the depth H without weight tying (Saxe et al.,
2014; Kawaguchi, 2016; Hardt & Ma, 2017; Laurent & Brecht, 2018; Arora et al., 2018; Bartlett
et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020b). In deep linear networks,
the function h at each layer is linear in θ and linear in x; i.e., the map (x, θ) 7→ h(z(l-1) ; x, θ)
is bilinear. Despite this linearity, several key properties of deep learning are still present in deep
linear networks. For example, the gradient dynamics is nonlinear and the objective function is non-
convex. Accordingly, understanding gradient dynamics of deep linear networks is considered to be
a valuable step towards the mathematical understanding of deep neural networks (Saxe et al., 2014;
Arora et al., 2018; 2019a).
In this paper, inspired by the previous studies of deep linear networks, we initiate a theoretical study
of gradient dynamics of deep equilibrium linear models as a step towards theoretically understand-
ing general deep equilibrium models. As we shall see in Section 2, the function h at each layer is
nonlinear in θ for deep equilibrium linear models, whereas it is linear for deep linear networks. This
additional nonlinearity is essential to enforce the existence of the fixed point z*. The additional non-
linearity, the infinite depth, and weight tying are the three key proprieties of deep equilibrium linear
models that are absent in deep linear networks. Because of these three differences, we cannot rely
on the previous proofs and results in the literature of deep linear networks. Furthermore, we analyze
gradient dynamics, whereas Kawaguchi (2016); Hardt & Ma (2017); Laurent & Brecht (2018) stud-
ied the loss landscape of deep linear networks. We also consider a general class of loss functions
for both regression and classification, whereas Saxe et al. (2014); Arora et al. (2018); Bartlett et al.
(2019); Arora et al. (2019a); Zou et al. (2020b) analyzed gradient dynamics of deep linear networks
in the setting of the square loss.
Accordingly, we employ different approaches in our analysis and derive qualitatively and quantita-
tively different results when compared with previous studies. In Section 2, we provide theoretical
and numerical observations that further motivate us to study deep equilibrium linear models. In Sec-
tion 3, we mathematically prove convergence of gradient dynamics to global minima and the exact
relationship between the gradient dynamics of deep equilibrium linear models and that of the adap-
tive trust region method. Section 5 gives a review of related literature, which strengthens the main
motivation of this paper along with the above discussion (in Section 1). Finally, Section 6 presents
concluding remarks on our results, the limitation of this study, and future research directions.
2	Preliminaries
We begin by defining the notation. We are given a training dataset ((xi, yi))in=1 ofn samples where
xi ∈ X ⊆ Rmx and yi ∈ Y ⊆ Rmy are the i-th input and the i-th target output, respectively.
We would like to learn a hypothesis (or predictor) from a parametric family H = {fθ : Rmx →
Rmy | θ ∈ Θ} by minimizing the objective function L (called the empirical loss) over θ ∈ Θ:
L(θ) = Pn=ι '(fθ(Xi),yi), where θ is the parameter vector and ' : Rmy × Y → R≥o is the
loss function that measures the difference between the prediction fθ (xi) and the target yi for each
sample. For example, when the parametric family of interest is the class of linear models as H =
{x 7→ W φ(x) | W ∈ Rmy ×m}, the objective function L can be rewritten as:
n
Lo(W ) = X '(Wφ(χi),yi),	⑵
i=1
where the feature map φ is an arbitrary fixed function that is allowed to be nonlinear and is chosen
by model designers to transforms an input x ∈ Rmx into the desired features φ(x) ∈ Rm. We use
vec(W) ∈ Rmym to represent the standard vectorization ofa matrix W ∈ Rmy ×m.
Instead of linear models, our interest in this paper lies on deep equilibrium models. The output z *
of the last hidden layer of a deep equilibrium model is defined by
z* = lim z(l) = lim h(z(l-1); x, θ) = h(z*; x, θ),	(3)
l→∞	l→∞
where the last equality follows from the continuity of z 7→ h(z; x, θ) (i.e., the limit commutes with
the continuous function). Thus, z* can be computed by solving the equation z* = h(z* ; x, θ) with-
out running the infinitely deep layer-by-layer computation. The gradients with respect to parameters
are computed analytically via backpropagation through z * using implicit differentiation.
2
Published as a conference paper at ICLR 2021
2.1	Deep Equilibrium Linear Models
A deep equilibrium linear model is an instance of the family of deep equilibrium models and is
defined by setting the function h at each layer as follows:
h(z(l-1); x, θ) = γ σ (A)z (l-1) + φ(x),	(4)
where θ = (A, B) with two trainable parameter matrices A ∈ Rm×m and B ∈ Rmy×m. Along
with a positive real number γ ∈ (0, 1), the nonlinear function σ is used to ensure the existence
of the fixed point and is defined by σ(A)j = Pmexp(AjAk .). The class of deep equilibrium linear
models is given by H = {x → B (limι→∞ z(l)(x, A)) | A ∈ Rm×m, B ∈ Rmy ×m}, where
z(l) (x, A) = γσ(A)z(l-1) + φ(x). Therefore, the objective function for deep equilibrium linear
models can be written as
n
L(A, B) =	` B
i=1
lim z(l)(xi, A) , yi .
(5)
The outputs of deep equilibrium linear models fθ(x) = B liml→∞ z(l) (x, A) are nonlinear and
non-multilinear in the optimization variable A. This is in contrast to linear models and deep linear
networks. From the optimization viewpoint, linear models W φ(x) are called linear because they are
linear in the optimization variables W. Deep linear networks W(H)W(H-I) ∙∙∙ W(I)X are multi-
linear in the optimization variables (W(1), W(2), . . . , W(H)) (this holds also when we replace x by
φ(x)). This difference creates a challenge in the analysis of deep equilibrium linear models.
Following previous works on gradient dynamics of different machine learning models (Saxe et al.,
2014; Ji & Telgarsky, 2020), we consider the process of learning deep equilibrium linear models via
gradient flow:
d	∂L
dtAt = - ∂A (AJBt)
dtBt = -∂B(At,Bt), ∀t ≥ 0,
dt	∂ B
(6)
where (At, Bt) represents the model parameters at time t with an arbitrary initialization (A0, B0).
Throughout this paper, a feature map φ and a real number γ ∈ (0, 1) are given and arbitrary (except
in experimental observations) and we omit their universal quantifiers for the purpose of brevity.
2.2	Preliminary Observation for Additional Motivation
Our analysis is chiefly motivated as a step towards mathematically understanding general deep equi-
librium models (as discussed in Sections 1 and 5). In addition to the main motivation, this section
provides supplementary motivations through theoretical and numerical preliminary observations.
In general deep equilibrium models, the limit, liml→∞ z(l), is not ensured to exist (see Appendix C).
In this view, the class of deep equilibrium linear models is one instance where the limit is guaranteed
to exist for any values of model parameters as stated in Proposition 1:
Proposition 1. Given any (x, A), the sequence (z(l) (x, A))l in Euclidean space Rm converges.
Proof. We use the nonlinearity σ to ensure the convergence in our proof in Appendix A.5.	□
Proposition 1 shows that we can indeed define the deep equilibrium linear model with liml→∞ z(l) =
z* (x, A). Therefore, understanding this model is a sensible starting point for theory of general deep
equilibrium models.
As our analysis has been mainly motivated for theory, it would be of additional value to discuss
whether the model would also make sense in practice, at least potentially in the future. Consider an
(unknown) underling data distribution P(x, y) = P(y|x)P(x). Intuitively, if the mean of the P(y|x)
is approximately given by a (true unknown) deep equilibrium linear model, then it would make sense
to use the parametric family of deep equilibrium linear models to have the inductive bias in practice.
To confirm this intuition, we conducted numerical simulations. To generate datasets, we first drew
uniformly at random 200 input images for input data points xi from a standard image dataset —
CIFAR-10, CIFAR-100 or Kuzushiji-MNIST (Krizhevsky & Hinton, 2009; Clanuwat et al., 2019).
We then generated targets as yi = B*(limι→∞ Z(I)(xi, A*)) + δ% where δi 1^-N(0,1). Each entry
of the true (unknown) matrices A* and B * was independently drawn from the standard normal
distribution. For each dataset generated in this way, we used stochastic gradient descent (SGD)
to train linear models, fully-connected feedforward deep neural networks with ReLU nonlinearity
3
Published as a conference paper at ICLR 2021
SSo-κffl
----Linear (best)
----Linear (worst)
----DELM (best)
---- DELM (worst)
DNN (H=2)
DNN (H=3)
DNN (H=4)
DELM (best)
DELM (worst)
5∙'τ->τ 二皿
ɪ 1 W W W W W
SSO-UW
O IOOO 2000	3000 WO 5000
epoch
O IoOO 2000	3000	4000	5000	O IOOO 2000	3000	4000	5000
epoch	epoch
(a)	Modified Kuzushiji-MNIST: Linear v.s. DELM
O IoOO 2000	3000	4000	5000
epoch
(b)	Modified Kuzushiji-MNIST: DNN v.s. DELM
Linear (best)
Linear (worst)
□ ELM (best)
DELM (worst)
----DNN (H=2)
----DNN (H=3)
----DNN (H=4)
----DELM (best)
----DELM (worst)
O IOOO 2000	3000	4000	5000	O IOOO 2000	3000	4000	5000
epoch	epoch
(c)	Modified CIFAR-10: Linear v.s. DELM
O IoOO 2000	3000	4000	5000	O IOOO 2000	3000 WO 5000
epoch	epoch
(d)	Modified CIFAR-10: DNN v.s. DELM
Linear (best)
Linear (worst)
□ ELM (best)
DELM (worst)
DNN (H=2)
----DNN (H=3)
----DNN (H=4)
----DELM (best)
----DELM (worst)
O IOOO 2000	3000	4000	5000
epoch
O IOOO 2000	3000 WO 5000
epoch
O IOOO 2000	3000	4000	5000	O IOOO 2000	3000	4000	5000
epoch	epoch
(e)	Modified CIFAR-100: Linear v.s. DELM
(f)	Modified CIFAR-100: DNN v.s. DELM
Figure 1: Preliminary observations for additional motivation to theoretically understand deep equi-
librium linear models. The figure shows test and train losses versus the number of epochs for linear
models, deep equilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs).
(DNNs), and deep equilibrium linear models. For all models, we fixed φ(x) = x. See Appendix D
for more details of the experimental settings.
The results of this numerical test are presented in Figure 1. In the figure, the plotted lines indicate
the mean values over five random trials whereas the shaded regions represent error bars with one
standard deviation. The plots for linear models and deep equilibrium linear models are shown with
the best and worst learning rates (separately for each model in terms of the final test errors at epoch
= 5000) from the set of learning rates SLR = {0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005}. The
plots for DNNs are shown with the best learning rates (separately for each depth H) from the set SLR.
As can be seen, all models preformed approximately the same at initial points, but deep equilibrium
linear models outperformed both linear models and DNNs in test errors after training, confirming
our intuition above. Moreover, we confirmed qualitatively same behaviors with four more datasets
as well as for DNNs with and without bias terms in Appendix D. These observations additionally
motivated us to study deep equilibrium linear models to obtain our main results in the next section.
The purpose of these experiments is to provide a secondary motivation for our theoretical analyses.
3 Main Results
In this section, we establish mathematical properties of gradient dynamics for deep equilibrium
linear models by directly analyzing its trajectories. We prove linear convergence to global minimum
in Section 3.1 and further analyze the dynamics from the viewpoint of trust region in Section 3.2.
3.1	Convergence Analysis
We begin in Section 3.1.1 with a presentation of the concept of the POlyak土CjasieWicz (PL) inequal-
ity and additional notation. The PL inequality is used to regularize the choice of the loss functions
` in our main convergence theorem for a general class of losses in Section 3.1.2. We conclude in
Section 3.1.3 by providing concrete examples of the convergence theorem with the square loss and
the logistic loss, where the PL inequality is no longer required as the PL inequality is proven to be
satisfied by these loss functions.
3.1.1	The POLYAK-LOJASIEWICZ Inequality and Additional Notation
In our context, the notion of the PL inequality is formally defined as follows:
4
Published as a conference paper at ICLR 2021
Definition 1. The function L0 is said to satisfy the Polyak-Lojasiewicz (PL) inequality with radius
R ∈ (0, ∞] and parameter K > 0 if 2∣∣VLVec(vec(W))k2 ≥ K(Lvec(Vec(W)) — L0 r) for all
∣∣Wkι < R, where Lvec(Vec(∙)):= L0(∙) and Lq,r := infW”|w卜<r L√W).
With any radius R > 0 sufficiently large (such that it covers the domain of L0), Definition 1 becomes
equivalent to the definition of the PL inequality in the optimization literature (e.g., Polyak, 1963;
Karimi et al., 2016). See Appendix C for additional explanations on the equivalence. In general,
the non-convex objective function L of deep equilibrium linear models does not satisfy the PL
inequality. Therefore, we cannot assume the inequality on L. However, in order to obtain linear
convergence for a general class of the loss functions `, we need some assumption on `: otherwise,
we can choose a loss ` to violate the convergence. Accordingly, we will regularize the choice of the
loss ' through the PL inequality on the function Lo : W → Pn=ι '(Wφ(xi), yi).
The PL inequality with a radius R ∈ (0, ∞] (Definition 1) leads to the notion of the global minimum
value in the domain corresponding to the radius in our analysis: LR = infA∈Rm×m b∈br L(A, B),
where BR = {B ∈ Rmy ×m | ∣B∣1 < (1 — γ)R}. With R = ∞, this recovers the global
minimum value L* in the unconstrained domain as LR = L* := infA∈Rm×m B∈Rmy×m L(A, B).
Furthermore, if a global minimum (A*, B*) ∈ Rm×m X Rmy×m exists, there exists R < ∞ such
that for any R ∈ [R, ∞) ,we have B* ∈ Br and thus LR = L*. In other words, if a global minimum
exists, using a (sufficiently large) finite radiusR < ∞ suffices to obtain LR = L*.
We close this subsection by introducing additional notation. For a real symmetric matrix M , we
use λmin (M) to represent its smallest eigenvalue. For an arbitrary matrix M ∈ Rd×d0, we let
rank(M) be its rank, ∣M ∣p be its matrix norm induced by the vector p-norm, σmin(M) be its
smallest singular value (i.e., the min(d, d0)-th largest singular value), M*j be its j-th column vector
in Rd, and Mi* be its i-th row vector in Rd0. For d ∈ N>0, we denote by Id the identify matrix in
Rd×d. We define the Jacobian matrix Jk,t ∈ Rm×m of the vector-valued function A*k 7→ σ(A)*k
by (Jk,t)ij = d¾⅛kk ∣A=At for all t ≥ 0 and k = 1, . . . , m. Finally, we define the feature matrix
Φ ∈ Rm×n by Φki = φ(xi)k fork = 1, . . .,mandi = 1, . . .,n .
3.1.2	Main Convergence Theorem
Using the PL inequality only on the loss function ` through L0 (Definition 1), we present our main
theorem — a guarantee on linear convergence to global minimum for the gradient dynamics of the
non-convex objective L for deep equilibrium linear models:
Theorem 1. Let ' : Rmy × Y → R≥o be arbitrary such that the function q → '(q, yi) is differ-
entiable for any i ∈ {1, . . . , n} (with an arbitrary my ∈ N>0 and an arbitrary Y). Then, for any
T > 0, R ∈ (0, ∞] and K > 0 such that ∣Bt∣1 < (1 — γ)R for all t ∈ [0, T] and L0 satisfies the
PL inequality with the radius R and the parameter K, the following holds:
L(AT, BT) ≤ LR + (L(A0, Bo) — L*,r) e-2κλTT,	⑺
where λT := inf t∈[0,T] λmin (Dt) > 0 and Dt is a positive definite matrix defined by
m
Dt= X [(U->)*k(U-1)k* 乳(Imy + Y2BtU-1Jkt JlN-B>)] ,	(8)
k=1
with Ut := Im — γσ(At). Furthermore, λτ ≥ 阳卜产 for any T ≥ 0 (limτ→∞ λτ ≥ 阳工产).
Proof. The additional nonlinearity σ creates a complex interaction among m hidden neurons. This
interaction is difficult to be factorized out for the gradients of L with respect to A. This is different
from but analogous to the challenge to deal with nonlinear activations in the loss landscape of (non-
overparameterized) deep nonlinear networks, for which previous works have made assumptions of
sparse connections to factorize the interaction (Kawaguchi et al., 2019). In contrast, we do not rely
on sparse connections. Instead, we observe that although it is difficult to factorize this complex
interaction (due to the nonlinearity σ) in the space of loss landscape, we can factorize it in the space
of gradient dynamics. See Appendix A.1 for the proof overview and the complete proof.	□
Theorem 1 shows that in the worst case for λT, the optimality gap decreases exponentially towards
___________________________________2 K T
zero as L(AT,	BT)	—	LR	≤	Coe	m(1+γ)2	, where	Co	= L(Ao,	Bo)	—	L*	R. Therefore, for any
5
Published as a conference paper at ICLR 2021
____2 K τ
desired accuracy e > 0, setting C°e m(1+γ)2	≤ E and solving for T yield that
L(AT,Bt) — LR ≤ E foranyT ≥ m(^ log " B0) - L0，R.	(9)
2κ	E
Theorem 1 also states that the rate of convergence improves further depending on the quality of the
matrix Dt (defined in equation (8)) in terms of its smallest eigenvalue over the particular trajectory
(At, Bt) up to the specific time t ≤ T; i.e., λT = inf t∈[0,T] λmin (Dt). This opens up the direction
of future work for further improvement of the convergence rate through the design of initialization
(A0 , B0 ) to maximize λT for trajectories generated from a specific initialization scheme.
3.1.3	Examples: Square Loss and Logistic Loss
The main convergence theorem in the previous subsection is stated for any radius R ∈ (0, ∞] and
parameter κ > 0 that satisfy the conditions on kBt k1 and the PL inequality (see Theorem 1). The
values of these variables are not completely specified there as they depend on the choice of the loss
functions `. In this subsection, we show that these values can be specified further and the condition
on PL inequality can be discarded by considering a specific choice of loss functions `.
In particular, by using the square loss for `, we prove that we can set R = ∞ and κ = 2σmin (Φ)2:
Corollary 1. Let '(q, yi) = ∣∣q 一 yik2 where yi ∈ Rmy for i = 1, 2,..., n (with an arbitrary
my ∈ N>0). Assume that rank(Φ) = min(n, m). Then for any T > 0,
L(AT, BT) ≤ L* + (L(Ao, B。)一 Lx) e-4σmin(φ)2λTT,
where σmin(Φ) > 0, L。:= infw∈Rmy×m Lo(W), and λτ := inf闫。,。λmin(Dt) ≥ 死二丁尸.
Proof. This statement follows from Theorem 1. The conditions on ∣Bt ∣1 and the PL inequality (in
Theorem 1) are now discarded by using the property of the square loss `. See Appendix A.3 for the
complete proof.	□
In Corollary 1, the global linear convergence is established for the square loss without the notion of
the radius R as we set R = ∞. Even with the square loss, the objective function L is non-convex.
Despite the non-convexity, Corollary 1 shows that for any desired accuracy E > 0,
L(AT,Bt) - L* ≤ E for any T ≥ m1+⅛ log L(A0,B0)- L。.	(10)
4σmin(Φ)2	E
Corollary 1 allows both cases of m ≤ n and m > n. In the case of over-parameterization m > n,
the covariance matrix ΦΦ> ∈ Rm×m (or XX> with φ(x) = x) is always rank deficient because
rank(ΦΦ>) = rank(Φ) ≤ n < m. This implies that the Hessian of L0 is always rank deficient,
because the Hessian of L。is V2Lvec(Vec(W)) = 2[ΦΦ> 0 Imy] ∈ Rmym×mym (see Appendix
A.3 for its derivation) and because rank([ΦΦ> 0 Imy]) = rank(ΦΦ>)rank(Imy) ≤ myn <
mym. Since the strong convexity on a twice differentiable function requires its Hessian to be of full
rank, this means that the objective L。 for linear models is not strongly convex in the case of over-
parameterization m > n. Nevertheless, we establish the linear convergence to global minimum
for deep equilibrium linear models in Corollary 1 for both cases of m > n and m ≤ n by using
Theorem 1.
For the logistic loss for `, the following corollary proves the global convergence at a linear rate:
Corollary 2. Let '(q,yi) = -y log(τ+e-q) — (1 — yi)log(1 — ɪ+^) + T∣q∣2 with an arbitrary
τ ≥ 0 where yi ∈ {0, 1} for i = 1, 2, . . . , n. Assume that rank(Φ) = m. Then for any T > 0 and
R ∈ (0, ∞] such that ∣Bt∣1 < (1 -γ)Rfor all t ∈ [0, T], the following holds:
L(At, BT) ≤ LR + (L(Ao, B。)- L*,r) e-2(2τ+ρ(R))σmin(φ)2λTT,
where σmin(Φ) > 0, λτ := inft∈[o,τ] λmm(Dt) ≥ m(1+Y)2, and
ρ(R) :
inf
W:kWk1<R,
i∈{1,...,n}
1
1 + e-Wφ(χi
1
1 + e-Wφ(χi
≥ 0.
Proof. This statement follows from Theorem 1 by proving that the condition on PL inequality is
satisfied with the parameter κ = (2τ + ρ(R))σmin(Φ)2. See Appendix A.4 for the complete proof.
□
6
Published as a conference paper at ICLR 2021
In Corollary 2, we can also set R = ∞ to remove the notion of the radius R from the statement
of the global convergence for the logistic loss. By setting R = ∞, Corollary 2 states that for any
T > 0,
L(AT, BT) ≤ L* + (L(Ao, Bo) - LM e-4τσmin(φ)2λτT,
for the logistic loss. For any τ > 0, this implies that for any desired accuracy > 0,
L(At,Bt) — L*≤ e for any T ≥「1 崂∖ log L(AOBO)- M .	(11)
4τσmin(Φ)2
In practice, we may want to set τ > 0 to regularize the parameters (for generalization) and to ensure
the existence of global minima (for optimization and identifiability). That is, ifwe set τ = 0 instead,
the global minima may not exist in any bounded space, due to the property of the logistic loss. This
is consistent with Corollary 2 in that ifτ = 0, equation (11) does not hold and we must consider the
convergence to the global minimum value LR defined in a bounded domain with a radius R < ∞.
In the case of τ = 0 and R < ∞, Corollary 2 implies that for desired accuracy > 0,
TT R∖	τ*r F m m m(1 + γ)2	1	L(AO, BO)-	l5,r
L(AT,BT) 一	LR	≤	E forany T ≥	-7-z——-Tog log-------------------,	(12)
2ρ(R)σmin(Φ)2
where we have ρ(R) > 0 because R < ∞. Therefore, Corollary 2 establish the linear convergence
to global minimum with both cases of τ > 0 and τ = 0 for the logistic loss.
3.2	Understanding Dynamics Through Trust Region Newton Method
In this subsection, we analyze the dynamics of deep equilibrium linear models in the space of the
hypothesis, fθt : x → Bt (limι→∞ z(l)(x, At)). For any functions g and g with a domain X ⊆
Rmx, We write g = g if g(x) = g(x) for all X ∈ X.
The following theorem shows that the dynamics of deep equilibrium linear models fθt can be written
as dd fθt = δ; Vtφ where is scalar and Vt follows the dynamics of a trust region Newton method of
shallow models with the (non-standard) adaptive trust region Vt . This suggests potential benefits of
deep equilibrium linear models in two aspects: when compared to shallow models, it can sometimes
accelerate optimization via the effect of the implicit trust region method (but not necessarily as the
trust region method does not necessarily accelerate optimization) and induces novel implicit bias for
generalization via the non-standard implicit trust region Vt .
Theorem 2. Let' : Rmy × Y → R≥o be arbitrary such that the function q → '(q, yi) is differen-
tiable for any i ∈ {1, . . . , n} with my = 1 and (an arbitrary Y). Then for any time t ≥ 0, there
exist a real number St > 0 such thatfor any δt ∈ (0, &],
-dfθt = 1 Vtφ,	Vec(Vt) ∈ argminL0(v),	(13)
dt δt	v∈Vt
where Vt = {v ∈ Rm : ∣∣v∣∣Gt ≤ 瓦||d Vec(BtUtT)|值}, Gt = Ut (S-I-瓦尤)U> A 0, and
L0(v) ：= Lvec(Vec(BtUtT)) + VLvec(Vec(BtUtT))>v + 2vτV2LVec(vec(BtUt-1))v.
Here, Ft ：= Pn=I V2'i(fθt(xi))(limι→∞ Z(I)(Xi, At))(limι→∞ z(l)(xi, At))T with 'i(q):=
'(q, yi) and St := Im + Y2 diag(vS) with VS ∈ Rm and (VS)k := ∣∣ J>t(BtUt^1)τ kg *k.
Proof. This is proven with the KarUSh-Kuhn-Tucker (KKT) conditions for the constrained opti-
mization problem: minimizev∈Vt Lo(v). See Appendix A.2.	□
When many global minima exist, a difference in the gradient dynamics can lead to a significant
discrepancy in the learned models: i.e., two different gradient dynamics can find significantly differ-
ent global minima with different behaviors for generalization and test accuracies (Kawaguchi et al.,
2017). In machine learning, this is an important phenomenon called implicit bias — inductive bias
induced implicitly through gradient dynamics — and is the subject of an emerging active research
area (Gunasekar et al., 2017; Soudry et al., 2018; Gunasekar et al., 2018; Woodworth et al., 2020;
Moroshko et al., 2020).
7
Published as a conference paper at ICLR 2021
As can be seen in Theorem 2, the gradient dynamics of deep equilibrium linear models fθt differs
from that of linear models Wtφ with any adaptive learning rates, fixed preconditioners, and existing
variants of Newton methods. This is consistent with our experiments in Section 2.2 and Appendix D
where the dynamics of deep equilibrium linear models resulted in the learned predictors with higher
test accuracies, when compared to linear models with any learning rates. In this regard, Theorem
2 provides a partial explanation (and a starting point of the theory) for the observed generaliza-
tion behaviors, whereas Theorem 1 (with Corollaries 1 and 2) provides the theory for the global
convergence observed in the experiments.
Theorem 2, along with our experimental results, suggests the importance of theoretically under-
standing implicit bias of the dynamics with the time-dependent trust region. In Appendix B, we
show that Theorem 2 suggests a new type of implicit bias towards a simple function as a result of
infinite depth, whereas understanding this implicit bias in more details is left as an open problem for
future work.
4 Experiments
In this section, we conduct experiments to further verify and demonstrate our theory. To compare
with the previous findings, we use the same synthetic data as that in the previous work (Zou et al.,
2020b): i.e., we randomly generate xi ∈ R10 from the standard normal distribution and set yi =
-xi + 0.1ςi for all i ∈ {1, 2, . . . , n} with n = 1000, where ςi is independently generated by the
standard normal distribution. We set φ(χ) = X and use the square loss '(q,yi) = ∣∣q - yik2.
As in the previous work, we consider random initialization and identity initialization (Zou et al.,
2020b) and report the results in Figure 2 (a). As can be seen in the figure, deep equilibrium linear
models converges to the global minimum value with all initialization and random trials, whereas
linear ResNet converges to a suboptimal value with identity initialization. This is consistent with
our theory for deep equilibrium linear models and the previous work for ResNet (Zou et al., 2020b).
We repeated the same experiment by generating (χi)k independently from the uniform distribution
of the interval [-1, 1] instead for all i ∈ {1, . . . , n} and k ∈ {1, . . . , m} with n = 1000 and
m = 10. Figure 2 (b) shows the results of this experiment with the uniform distribution and confirm
the global convergence of deep equilibrium linear models again with all initialization and random
trials. In this case, linear ResNet with identity initialization also converged to the global minimum
value. These observations are consistent with Corollary 1 where deep equilibrium linear models are
guaranteed to converge to the global minimum value without any condition on the initialization.
We now consider the rate of the global convergence. In Corollary 1, we can set λT
1
m(1+γ)2
to get a guarantee for the global linear convergence rate for all initializations in theory. However,
in practice, this is a pessimistic convergence rate and we may want to choose λT depending on a
initialization. To demonstrate this, using the same data as that in Figure 2 (a), Figure 2 (c) reports
the numerical training trajectory along with theoretical upper bounds with initialization-independent
λτ = mq+γ)2 and initialization-dependent λτ = inft∈[o,τ] λmin(Dt). As can be seen in Figure 2
(c), the theoretical bound with initialization-dependent λT demonstrates a faster and more accurate
convergence rate. A qualitatively same observation is reported for the logistic loss in Appendix D.2.
IOOO-
βoo-
∞o-
400-
200-
-β-∙∙ DELM: identity
DELM: random 1
…沁…DELM: random 2
---DELM: random 3
---Linear ReSNet
....global optima
IOOO-
β∞-
β∞-
mo-
200-
O 250	5∞	750 IOOO 1250	15(K>	37»	2000
# of iterations
(b) Uniform data
O 250	5∞	750	1000	12»	1500	1750 NOoO
# of iterations
(c) Theoretical bounds
(a) Gaussian data
O
o
Figure 2: (a)-(b): Convergence performances for deep equilibrium linear models (DELMs) with
identity initialization and random initialization of three random trials, and linear ResNet with iden-
tity initialization. (c) the numerical training trajectory of DELMs with random initialization along
with theoretical upper bounds with initialization-independent λT and initialization-dependent λT .
8
Published as a conference paper at ICLR 2021
5	Related Work
The theoretical study of gradient dynamics of deep networks with some linearized component is
a highly active area of research. Recently, Bartlett et al. (2019); Du & Hu (2019); Arora et al.
(2019a); Zou et al. (2020b) analyzed gradient dynamics of deep linear networks and proved global
convergence rates for the square loss under certain assumptions on the dataset, initialization, and
network structures. For example, the dataset is assumed to be whitened (i.e., ΦΦ> = Im or XX> =
Imx) and the initial loss is assumed to be smaller than the loss of any rank-deficient solution by Arora
et al. (2019a): the input and output layers are assumed to represent special transformations and are
fixed during training by Zou et al. (2020b).
Deep networks are also linearized implicitly in the neural tangent kernel (NTK) regime with signif-
icant over-parameterization m n (Yehudai & Shamir, 2019; Lee et al., 2019). By significantly
increasing model parameters (or more concretely the width m), we can ensure deep features or cor-
responding NTK to stay nearly the same during training. In other words, deep networks in this
regime are approximately linear models with random features corresponding to the NTK at random
initialization. Because of this implicit linearization, deep networks in the NTK regime are shown to
achieve globally minimum training errors by interpolating all training data points (Zou et al., 2020a;
Li & Liang, 2018; Jacot et al., 2018; Du et al., 2019; 2018; Chizat et al., 2019; Arora et al., 2019b;
Allen-Zhu et al., 2019; Lee et al., 2019; Fang et al., 2020; Montanari & Zhong, 2020).
These previous studies have significantly advanced our theoretical understanding of deep learn-
ing through the study of deep linear networks and implicitly linearized deep networks in the NTK
regime. In this context, this paper is expected to contribute to the theoretical advancement through
the study ofanew and significantly different type of deep models — deep equilibrium linear models.
In deep equilibrium linear models, the function at each layer A 7→ h(z(l-1); x, θ) is nonlinear due
to the additional nonlinearity σ: A 7→ h(z(l-1); x, θ) := γσ(A)z(l-1) + φ(x). In contrast, for deep
linear networks, the function at each layer W(l) 7→ h(l)(z(l-1); x, W(l)) := W (l)z(l-1) is linear (it
is linear also with skip connection). Furthermore, the nonlinearity σ is not an element-wise func-
tion, which poses an additional challenge in the mathematical analysis of deep equilibrium linear
models. The nonlinearity σ, the infinite depth, and weight tying in deep equilibrium linear models
necessitated us to develop new approaches in our proofs. The differences in the models and proofs
naturally led to qualitatively and quantitatively different results. For example, we do not require
any of over-parameterization m n, interpolation of all training data points, and any assumptions
mentioned above for deep linear networks.
Unlike previous papers, we also related the dynamics of deep equilibrium linear models to that of
a trust region Newton method of shallow models with Gt-quadratic norm. This suggested potential
benefits of deep equilibrium linear models. Our theory is consistent with our numerical observations.
6	Conclusion
For deep equilibrium linear models, despite the non-convexity, we have rigorously proven conver-
gence of gradient dynamics to global minima, at a linear rate, for a general class of loss functions,
including the square loss and logistic loss. Moreover, we have proven the relationship between the
gradient dynamics of deep equilibrium linear models and that of the adaptive trust region method.
These results apply to models with any configuration on the width of hidden layers, the number
of data points, and input/output dimensions, allowing rank-deficient covariance matrices as well as
both under-parameterization and over-parameterization.
The crucial assumption for our analysis is the differentiability of the function q → '(q, yi), which is
satisfied by standard loss functions, such as the square loss, the logistic loss, and the smoothed hinge
loss '(q, yi) = (max{0,1 一 yiq})k with k ≥ 2. However, it is not satisfied by the (non-smoothed)
hinge loss '(q, yi) = max{0,1 一 yiq}, the treatment of which is left to future work. Future work
also includes corresponding theoretical analyses with stochastic gradient descent.
Our theoretical results (in Section 3) and numerical observations (in Section 2.2 and Appendix D)
uncover the special properties of deep equilibrium linear models, providing a basis of future work
for theoretical studies of implicit bias and for further empirical investigations of deep equilibrium
models. In our proofs, the treatments of the additional nonlinearity σ, the infinite depth, and weight
tying are especially unique, and we expect our new proof techniques to be proven useful in further
studies of gradient dynamics for deep models.
9
Published as a conference paper at ICLR 2021
References
J Harold Ahlberg and Edwin N Nilson. Convergence properties of the spline fit. Journal of the
Societyfor Industrial and Applied Mathematics,11(1):95-104, 1963.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, pp. 6158-6169, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de-
scent for deep linear neural networks. In International Conference on Learning Representations,
2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems, pp. 690-701, 2019a.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In Inter-
national Conference on Learning Representations, 2019b.
Randal J Barnes. Matrix differentiation. Springs Journal, pp. 1-9, 2006.
Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initializa-
tion efficiently learns positive-definite linear transformations by deep residual networks. Neural
computation, 31(3):477-502, 2019.
Bradley M Bell and James V Burke. Algorithmic differentiation of implicit functions and optimal
values. In Advances in Automatic Differentiation, pp. 67-77. Springer, 2008.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Bruce Christianson. Reverse accumulation and attractive fixed points. Optimization Methods and
Software, 3(4):311-326, 1994.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. In NeurIPS Creativity Workshop 2019, 2019.
Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation
models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6292-
6299, 2019.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In International Conference on Learning Representations, 2019.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
International Conference on Machine Learning, pp. 1655-1664, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Cong Fang, Jason D Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field
framework for over-parameterized deep neural networks. arXiv preprint arXiv:2007.01452, 2020.
10
Published as a conference paper at ICLR 2021
Georg Frobenius. Uber matnzen aus nicht negativen elementen. Sitzungsberichte der KOmgIich
Preussischen Akademie der Wissenschaften, pp. 456—-477, 1912.
Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques Of algO-
rithmic differentiatiOn. SIAM, 2008.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural InfOrmatiOn PrOcessing
Systems,pp. 6151-6159, 2017.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural InfOrmatiOn PrOcessing Systems, pp.
9461-9471, 2018.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In InternatiOnal COnference On
Learning RepresentatiOns, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In PrOceedings Of the IEEE cOnference On cOmputer visiOn and pattern recOgnitiOn, pp.
770-778, 2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural infOrmatiOn prOcessing systems, pp. 8571-
8580, 2018.
Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Locally adaptive activation func-
tions with slope recovery for deep and physics-informed neural networks. PrOceedings Of the
ROyal SOciety A, 476(2239):20200334, 2020a.
Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Adaptive activation functions
accelerate convergence in deep and physics-informed neural networks. JOurnal Of COmputatiOnal
Physics, 404:109136, 2020b.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. arXiv
preprint arXiv:2006.06657, 2020.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and KnOwledge DiscOvery in Databases, pp. 795-811. Springer, 2016.
Kenji KaWaguchi. Deep learning Without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Kenji KaWaguchi and Yoshua Bengio. Depth With nonlinearity creates no bad local minima in
resnets. Neural Networks, 118:167-174, 2019.
Kenji KaWaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep
neural netWorks of practical sizes. In 2019 57th Annual Allerton Conference on Communication,
Control, and Computing (Allerton), pp. 92-99. IEEE, 2019.
Kenji KaWaguchi and Leslie Kaelbling. Elimination of all bad local minima in deep learning. In
International Conference on Artificial Intelligence and Statistics, pp. 853-863. PMLR, 2020.
Kenji KaWaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Kenji KaWaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling. Effect of depth and Width on local
minima in deep learning. Neural computation, 31(7):1462-1498, 2019.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Thomas Laurent and James Brecht. Deep linear netWorks With arbitrary loss: All local minima are
global. In International conference on machine learning, pp. 2902-2907. PMLR, 2018.
11
Published as a conference paper at ICLR 2021
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8572-8583,
2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant. Adding one neuron can eliminate all bad
local minima. In Advances in Neural Information Processing Systems, 2018.
Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: mem-
orization and generalization under lazy training. preprint arXiv:2007.12826, 2020.
Nenad Moraca. Bounds for norms of the matrix inverse and the smallest singular value. Linear
algebra and its applications, 429(10):2589-2601, 2008.
Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D Lee, Nathan Srebro, and Daniel
Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. arXiv
preprint arXiv:2007.06738, 2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, 2011.
Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on
Machine Learning, pp. 4790-4799. PMLR, 2019.
Quynh Nguyen. A note on connectivity of sublevel sets in deep learning. arXiv preprint
arXiv:2101.08576, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Oskar Perron. Zur theorie der matrices. Mathematische Annalen, 64(2):248-263, 1907.
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting
puzzle. arXiv preprint arXiv:1801.00173, 2017.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In International Conference on Learning
Representations, 2014.
Ohad Shamir. Are ResNets provably better than linear predictors? In Advances in Neural Informa-
tion Processing Systems, to appear, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
B Tactile Srl and Italy Brescia. Semeion handwritten digit data set. Semeion Research Center of
Sciences of Communication, Rome, Italy, 1994.
James M Varah. A lower bound for the smallest singular value of a matrix. Linear Algebra and its
Applications, 11(1):3-5, 1975.
12
Published as a conference paper at ICLR 2021
Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian
Tang. Graphmix: Regularized training of graph neural networks for semi-supervised learning.
arXiv preprint arXiv:1909.11715, 2019.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. arXiv
preprint arXiv:2002.09277, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems, pp. 6598-6608, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, 109(3):467-492, 2020a.
Difan Zou, Philip M Long, and Quanquan Gu. On the global convergence of training deep linear
resnets. In International Conference on Learning Representations, 2020b.
13
Published as a conference paper at ICLR 2021
A Proofs
In this appendix, we complete the proofs of our theoretical results. We present the proofs of Theorem
1 in Appendix A.1, Theorem 2 in Appendix A.2, Corollary 1 in Appendix A.3, Corollary 2 in
Appendix A.4, and Proposition 1 in Appendix A.5. We also provide a proof overview of Theorem 1
in the beginning of Appendix A.1.
Before starting our proofs, we first introduce additional notation used in the proofs and then dis-
cuss alternative proofs using the implicit function theorem to avoid relying on the convergence of
Neumann series.
Additional notation. Given a scalar-valued function a ∈ R and a matrix M ∈ Rd×d0, we write
	∂a	∂a	
∂a 		=	-	 . ∂Mιι .	. .	• -	 ∂Mid0 . ..	∈ Rd×d0
∂M	.	..	
	∂a	∂a	
	-	 . LdMdI	• —	 ∂MddθJ	
where Mij represents the (i, j)-th entry of the matrix M. Given a vector-valued function a ∈ Rd
and a column vector b ∈ Rd0 , we write
	∂a1	da1	
∂a ——=	——	. ∂bι .. .	• —	 dbd0 . ..	∈ Rd×d0
∂b	. dad - . dbι	.. dad • - dbd0.	
where bi represents the i-th entry of the column vector b. Similarly, given a vector-valued function
a ∈ Rd and a row vector b ∈ R1×d0, we write
∂a
∂b
∂aι
∂b11
∂aι
db1d0
∈	×d0
∂ad
∂b11
∂ad
db1d0
where bn represents the i-th entry of the row vector b. We use VaL to represent the map (A, B) →
∂∂L(A, B) (without the usual transpose used in vector calculus). Given a matrix M and a function
夕，we define VM夕 similarly as the map M → 照(M). Our proofs also use the indicator function:
l{i = k} = {0
ifi = k
if i 6= k
Finally, we recall the definition of the Kronecker product of two matrices: for matrices M ∈
RdM ×dM and M ∈ RdM ×dM,
MgM
.MiiM	…M%M ∙
..	..	..
...
MdM iM	…MdM dM M.
∈ RdMdM ×dMdM
On alternative proofs using the implicit function theorem. In our default proofs, we utilize
the Neumann series Pk∞=0 γkσ(A)k when deriving the formula of the gradients with respect to A.
Instead of using the Neumann series, we can alternatively use the implicit function theorem to derive
the formula of the gradients with respect to A. Specifically, in this alternative proof, we apply the
implicit function theorem to the function ψ defined by
ψ(vec[A], z) = z - γσ(A)z - φ(x),
where vec[A] and z ∈ Rm are independent variables of the function ψ: i.e., ψ(vec[A], z) is allowed
to be nonzero. On the other hand, the vector z satisfying ψ(vec[A], z) = 0 is the fixed point
14
Published as a conference paper at ICLR 2021
z* = liml→∞ z(l) based on equation (3). Therefore, by applying the implicit function theorem to
the function ψ, it holds that if the the Jacobian matrix
∂ψ(vec[A],z)
|z=z* is invertible, then
Since
∂z*
∂ vec[A]
∂ψ(vec[A] ,z)
∂z
∂ψ(vec[A], z)
∂z
∂z
∂ψ(vec[A],z)
∂ vec[A]
I = I - Yσ(A) is invertible, it holds that
z=z*
dz*	= - (I - Yσ(A))-1 ( dψ(Vec[A]，Z)
∂vec[A]	( Y ( )) 卜 ∂vec[A]
(14)
(15)
—
Moreover, since σ(A)z ∈ Rm is a column vector,
∂ψ(vec[A],z)
∂ vec[A]
*
∂σ(A)z I
Y d vec[A]lz=z
*
∂ vec[σ(A)z]
Y	∂ vec[A]
∂[z> 0 Im] vec[σ(A)]
∂ vec[A]
z=z*
-Y [(Z*)> 0 Im]⅞ec⅞Al.
∂ vec[A]
(16)
Combining equations (15) and (16), we have
∂z*
∂ vec[A]
=Y (I- Yσ(A))T [(z* )> 0 Im]⅞⅛⅛F .
(17)
In our proofs, whenever we require the gradients with respect to A, we can directly use equation
(17), instead of relying on the convergence of the Neumann series. For example, equation (21) in
the proof of Theorem 1 is identical to equation (17) with additional multiplication of Bq*: i.e., for
the left hand side,
∂z*	∂Bq*z*	∂Bq*U-1φ(x)
∂ vec[A]	∂ vec[A]
∂A
and for the right hand side,
γBq*(I - γσ(A))-1 [(z*)> 0 Im ] dvecσ*
∂ vec[ ]
ddσ(A)*1
Y ( ∂A*1
>
(Bq*U-1)>φ(x)>(U->)*1
¾mm )> (Bq*U T )>Φ(x)>(U->)*m
A.1 Proof of Theorem 1
We begin with a proof overview of Theorem 1. We first compute the derivatives of the output of
deep equilibrium linear models with respect to the parameters A in Appendix A.1.1. Then using
the derivatives, We rearrange the formula of NAL such that it is related to the formula of VLo in
Appendices A.1.1-A.1.3. Intuitively, we then want to understand VAL through the property of
VL0, similarly to the landscape analyses of deep linear networks by Kawaguchi (2016). However,
we note there that the additional nonlinearity σ creates a complex interaction over the dimension m
to prevent us from using such a proof approach. Instead, using the proven relation ofVAL and VL0
from Appendices A.1.1-A.1.3, we directly analyze the trajectories of the dynamics over time t in
Appendices A.1.4-A.1.5, which results in a partial factorization of the iteration over the dimension
m. Using such a partial factorization, we derive the linear convergence rate in Appendices A.1.6-
A.1.7 by using the PL inequality and the properties of induced norms.
Before getting into the details of the proof, we now briefly discuss the property of our proof in terms
of the tightness of a bound. In the condition ofkBtk1 < (1 - Y)R in the statement of Theorem 1,
the quantity (1 - Y) comes from the proof in Appendix A.1.7: i.e., it is the reciprocal of the quantity
1-γ in the upper bound Ofk(Im - Yσ(A))-1kι ≤ 11γ. Therefore, a natural question is whether or
not we can improve this bound further. This bound turns out to be tight based on the following lower
bound. The matrix Im - Yσ(A) is a Z-matrix since off-diagonal entries are less than or equal to
15
Published as a conference paper at ICLR 2021
zero. Furthermore, Im - γσ(A) is M -matrix since eigenvalues of Im - γσ(A) are the eigenvalues
of Im -	> and the eigenvalues of I - γσ(A)> are lower bounded by 1 - γ > 0. This is
because	is a stochastic matrix with the largest eigenvalue being one. Moreover, in the proof
in Appendix A.1.7, we showed that II -γσ(A)Ijj -Pi6=j II-γσ(A)Iij = 1 -γ for allj. Therefore,
using the lower bound by Moraca (2008), We have
1
1
k(I - γσ(A))-1k1
≥ maxj(II - Yσ(A)Ijj- Pi=j |I - Yσ(A儿j)	1 - Y,
which matches with the upper bound of k(Im - γσ(A))-1 k1 ≤ 1-γ . Therefore, we cannot further
improve the our bound on kBt k1 in general without making some additional assumption.
A.1.1 Rearranging THE formula OF VAL
We will use the following facts for matrix calculus (that can be derived by using definition of deriva-
tives: e.g., see Barnes, 2006):
∂M-1 _
∂a =
∂a>M-1b
-M-I ∂MM-ι
∂a
∂M
∂g(M)
∂a
-M->ab>M->
ΣΣ
∂g(M) ∂Mij
∂Mij	∂a
∂g(a)	∂g(a)	∂a
—:--------:-7---
∂M	∂a	∂M
Recall that U = I - γσ(A). From the above facts, given a function g, we have
mm
dg(U) = XX
∂Aki =W
i=1 j=1
mm
=XX
i=1 j=1
m
∂g(U) ∂Uij
∂Uij ∂Akl
∂g(U) ∂Uij ∂σ(A)ij
∂Uij ∂σ(A)ij ∂Akl
-γ
i=1 j=1
∂g(U) ∂σ(A)ij
∂Uij	∂Akl .
(18)
m
Using the quotient rule,
dσ(A)ij
∂Aki
∂	exp( Aj)
∂Aki Pt exp(Atj)
∂ exp(Aj)
)( texp(Atj)) -exp(Aij)(
d Pt eχp(Aj)
^Ak
( t exp(Atj))2
Iji = k}ljj = l} exp(Aj)(PteXp(Atj)) - ɪjj = l} exp(Aj)exp(Akj)
( t exp(Atj))2
Iji = k}ljj = l} exp(Aij) Ijj = l} exp(Aj) exp(Akj)
-------------------------------------------------------
t exp(Atj )
l{i = k}l{j = l}σ(A)ij - l{j = l}
( t exp(Atj ))2
exp(Aj)	exp( Akj)
PteXp(Atj) PteXp(Atj)
{j= = l}l{i = k}σ(A)ij - Ijj = l}σ(A)ijσ(A)j
(19)
Thus,
∂g(U) _	XX
^Ar = -YT
i=1
∂g(U) ∂σ(A)il
∂g(U)∖ > ∂σ(A)*ι
∂Akl
∈ R,
(
^Ak
)
∂UT ∂Aki	= -Y1^U；
16
Published as a conference paper at ICLR 2021
where ∂g(u) ∈ Rm×1 and "(A)*l ∈ Rm×1. This yields
∂U *l	∂Akl	J
∂g(U) =	(也”Y ∂σ(A)*ι ∈ Rι×m
∂A*ι	Y V ∂U*ι )	∂A*ι	,
where	∈ Rm×m.
Now We want to set g(U) to be the output of deep equilibrium linear models as g(U) =
Bq* (limι→∞ Z(I) (x, A)) for each q ∈ {1,..., my}. To do this, We first simplify the formula of
the output Bq* (limι→∞ Z(I) (x, A)) using the following:
(Im -"(A)) (£Ykσ(A)k)
=Im — Yσ(A)十 Yσ(A) — (γσ(A))2 十(Yσ(A))2 - (γσ(A))3 +---("(A))'+1
=I-(γσ(A))l+1.
Therefore,
(Im 一 γσ(A)) (lim Z(I)(X,A)) =，Iim(Im - γσ(A)) (E γkσ(A)kφ(x))
=(Im — ^lim(γσ(A))l+1^ φ(x)
=φ (X)
where the first line, the second line and the last line used the fact that γσ(A)j ≥ 0,
kσ(A) kι = maχ Elσ(A)j I =ι,
j V
and hence ∣∣γσ(A)∣∣ι < 1 for Y ∈ (0,1). This shows that B (lim/→∞ Z(I)(X,A)) = BU-1φ(x),
where the inverse U-1 exists as the corresponding Neumann series converges P∞=0 Ykσ(A)k since
∣∣γσ(A)∣∣ι < 1. Therefore, we can now set g(U) = Bq* (lim/→∞ Z(I)(X,A)) = Bq*U-1 φ(x).
Then, using da>M 'b = —M-TabTM->,
蚌=≡⅞⅛ = -U-T(Bq*)Tφ(x)>U-τ,
which implies that
dBq;U -1φ(X) = —(U-t (Bq*)τφ(x)TU-T)*l
∂U*ι
=—U-τ(Bq*)τφ(x)T(U-T)*ι ∈ Rm×1.	(20)
Combining (18) and (20),
∂g(U) = ∂Bq*U-1φ(x) = _ (∂g(U) ʌ T ∂σ(A)*ι
∂A*ι	∂A*ι	Y V ∂U*ι J	∂A*ι
=Y (U-T(Bq*)τφ(x)T(U-T)*ι)T (dσ^⅛)
∖ ∂A*ι )
=Y((U-T)*ι)τφ(x)Bq*U-1 ( dσ^⅛ )
∖ ∂A*ι )
17
Published as a conference paper at ICLR 2021
=Y(UT)ι*φ(x)Bq*UT (σA^^] ∈ R1×m,
∖ ∂A^ι )
where we used (U->)*ι = ((U-1)>)*ι = ((U-1)ι*)> and ((U->)*ι)> = (((U-1)ι^)τ)τ
(U-1)ι*. By taking transpose,
(dBq*UA1φ(X))	= Y (d∣AA⅛)τ (Bq*U-1)τφ(x)T(U-T)*ι ∈ Rm×1.
∖	dA	，*ι	∖ dA*ι )
By rearranging this to the matrix form,
∂Bq*U-1φ(x)
∂A
=Y [(d¾A>)τ(Bq*UT)Tφ(x)T(U-T)*1
(21)
(d¾⅛ )T(Bq*U-1 )τφ(X)T(U-τ)*m
where ABq*lUA °⑺ ∈ Rm×m. Each entry of this matrix represents the derivatives of the model
output with respect to the parameters A. We now use this to rearrange VaL(A, B) := dL^B). We
set Uiq = Bq*U-1 φ(x) and y = BU-1φ(x) and define
Jk :
∂σ(A)*k
∂A*k
∈ Rm ×m
and
n
Q := X
i=1
也)φ(xi)τ ∈ Rmy ×m
Then, using the chain rule and the above formula of dBq*UA φ⑺
∂L(A,B)
-∂A-
=XX Xy d'(yi, Ui) dyiq
—々々	d'0iq ~A
i=1q=1	q
Y X X 训Mii [jT(Bq*U-1)τφ(xi)τ(U-τ)*1 ∙∙∙	Jm(Bq*U-1)τφ(xi)T(U-T)*m]
i=1 q=1	yiq
n
YX M(PmI(Bq*U-1)τ¾d)Φ3)τ(U-τ)*1	∙∙∙ Jm(Pm1 (Bq*UT)T¾ii)φ(xs)τ(U-τ)*m]
i=1 L
n
YX [JT(P建1((BUT)T)*q¾i))φ(xi)T(U-T)*1	…Jm(P建1((BUT)T)*q¾i))φ(xi)T(U-T)*m]
i=1
n
YX [JT(BU-1)t(¾i))τφ(xi)T(U-T)*1	…Jm(BU-1)τ(⅛i))τφ(xi)T(U-T)*m]
i=1
Y (¾>)τ(BU-1)τQ(U-τ)*1
d⅛A⅛m )τ(BU-1)τQ(U-τ)*m
Summarizing the above, we have that
NAL(A, B) := dL(A,B) = γ JJT (BU-1 )τQ(U-τ)*1 ∙∙∙ Jm(BUT)TQ(U-T)",
∂ A
(22)
where VaL(A, B) ∈ Rm×m
18
Published as a conference paper at ICLR 2021
A.1.2 Rearranging THE formula of VLo
in order to relate L0 to the gradient dynamics of L, we now rearrange the formula of VL0 . We set
yiq = Wq*φ(χi) ∈ R and y% = Wφ(xi) ∈ Rmy for linear models. Then, by the chain rule,
dL0(W) = χ∖ Xy d'(yi, yi) dyiq
= h ⅛	∂W.
Since
∂Wq = {kk = q}φ(Xi)>，
we have
∂Lo (W)
∂Wk*
X d'(yi,y) dyik _ X Myi,y∕ φ()>
= ∂yik ∂WkZ = = ∂yik φ(xi) .
By rearranging this into the matrix form,
∂Lo(W )
∂W
Pn=I d¾yd φ(s)r
Pn=I ¾d φ(χi)r
n
X
i=1
一驾铲 Φ(χi)r
n
X
i=1
d>^ φ(Xi)>
L ays ψ i i .
-∂(^j∕i,yi')~
∂yii
φ(xi)>
n
X
i=1
∂'(yi,yi)
dyimy _
yi)
r
φ(xi)r ∈ Rmy×m
where d'(∂^y,yi) ∈ R1×my. Thus,
VLO(W ):= d≡=X (—)> φ(χi)> ∈Rmy ×m
(23)
A.1.3 CoMBINING THE FoRMULA oF VAL AND VL0
Combining (22) and (23) by resolving the different definitions of yi yields that
VAL(A, B)
=Y [J>(BU-1 )>VLo(BU-1)(U->)*1 …	Jm(BUT)rVLo(BUT)(U-r)*m].
(24)
Here, if there is no additional nonlinearity σ, the matrices Jk = "AAIk become identity for all
k. in that case, VA L(A, B) can be further simplified and factorize over m, which is desired for
the analysis of gradient dynamics. However, due to the additional nonlinearity, we cannot factor-
ize VA L(A, B) over m. one of the key techniques in our analysis is to keep this un-factorized
VA L(A, B) and find a way to factorize it during the update of parameters (At, Bt) in the gradient
dynamics, as shown later in this proof. To do so, we now start considering the dynamics over time t.
A.1.4 Analysing (limι→∞ z(l)(x, At))
Now let us temporarily consider a gradient dynamics discretized by the Euler method as
At+1 = At - αVAL(At, Bt),
19
Published as a conference paper at ICLR 2021
with some step size α > 0. Then,
l lim z°)(x,At+ι)) = (Im - γσ(At+ι))-1φ(x)
∖l→∞	)
=(Im - Yσ(At - a^AL(At, Bt)))-1φ(x),
where we used (limz→∞ Za)(x, At)) = U-`φ(x) from Section A.1.1. By setting 夕j(α) = σ(A —
qVaL(A, B))ij ∈ R,
σ(A - av A L(A, B))ij = ψij (a) = ψij (0) +-^da ) Q + O(a2).
By using the chain rule and setting M = A - QVAL(A, B) ∈ Rn×n,
∂ψij (Q) =	∂σ(M)ij ∂Mki
∂a	∂m ∂Mki	∂q
k = 1 l = 1	kl
mm
=-XX[ɪ{j = l}1{i = k}σ(M)ij - l{j = l}σ(M)ijσ(M)kj] VAL(A, B)kl
k=1l=1
m
=-X[l{i = k}σ(M)ij - σ(M)ijσ(M)kj ]VaL(A, B)kj
k = 1
Therefore,
dj) = - X [1 {i = k}σ(A)j - σ(A)jσ(A)kj ]VaL(A, B)kj
k=1
=-X TAjVAL(A,B)kj
k=1	kj
—
刍普VaL(A,B)*j ∈ R.
dA*j
Recalling the definition of Jk := "&" ∈ Rm×m
∂ψ^j(0) _ ∂σ(A)*j
∂q
∂A*j
VaL(A,B)*∕∙ = -JjVaL(A,B)*∕∙ ∈ Rm×1
—
Rearranging it into the matrix form,
⅛0) = -[J1VaL(A,B)*i ∙∙∙ Jm VAL (A,B)*m ] ∈ Rm×m
∂q
Putting the above equations together,
σ(A - qVaL(A, B))=以0) + α⅞≡ + O(q2)
∂q
=σ(A) - Q [J1VAL(A, B)*1 ∙∙∙ JmVAL(A, B)*m] + O(q2).
Thus,
[Im - Yσ (A - QVAL(A, B)) ] 1
=[Im	— Y [σ(A) - Q	[J1 VAL(A, B)*1	∙ ∙ ∙	JmVAL(A, B)*m] +。(。2)]]
=[Im	— γσ(A) + γQ	[J1VAL(A, B)*1	…	JmVAL(A, B)*m] + O(q2)]
=[U + QY [J1VaL(A, B)*1 ∙∙∙ JmVAL(A, B)*m]+ O(q2)]-1 .
By setting M = [J1VaL(A, B)*1 ∙∙∙	JmVAL(A, B)*m] and 夕(Q) = [U + QYM + o(q2)]-1
and by using dMa 1 = -M-1 架M-1,
[I - γσ(A - qVaL(A, B))]-1 = [U + QYM + O(q2)]-1
20
Published as a conference paper at ICLR 2021
=ψ(α)
=。(0) + 宇 α + O(o2)
∂ɑ
=U-1 - αγUTMU-1 + 2αO(α) + O(α2)
=U-1 - αγUTMU-1 + O(α2)
Summarizing above,
[ʃm - Yσ(A - θVAL(A, B))]-1	(25)
=U 1 — αγU 1 [jiVal(a, B)*1 •…JmNAL(A, B)*m] Ul + O(α2)
A.1.5 PUTTING RESULTS TOGETHER FOR INDUCED DYNAMICS
Wenow consider the dynamics of Zt := BtU-I in Rmy Xm that is induced by the gradient dynamics
of (At,Bt):
d	∂L
成At = - ∂A(At,Bj
dtBt = -∂B(At,Bt) ∀t ≥ 0∙
dt	∂ JB
Continuing the previous subsection, We first consider the dynamics discretized by the Euler method:
zt+1 := Bt+1Ut +1 = [Bt - avB L(At, Bt)][Im - γσ(At- aVAL(At, Bt))]-1,
where α > 0. Then, substituting (25) into the right-hand side of this equation,
Zt+1
=Bt+1[U-1 - αγU-1 [J1,tVAL(At,Bt)*1 ∙∙∙	Jm,tV AL(At, Bt)*m] UtT + O(o2)]
=Bt+1U-1 - αγBt+1U-1 [J1,tVAL(At, Bt)*1	∙ ∙ ∙	Jm,tVAL(At, Bt)*m] UJ1 + O(a2).
Using Bt+1 = [Bt- OVbL(At, Bt)], we have
αγBt+1Uj1 [J1,tVA L(At,Bt)*1 ∙∙∙	Jm,tV AL(At, Bt)*m] UtT
=αγBtUj1 [J1,tVAL(At, Bt )*1	∙∙∙	Jm,t VAL(At, Bt)*m] Uj1 + O(α2)
=αγZt [J1,t VAL(At, Bt)*1	∙∙∙	Jm,tVAL(At, Bt)*m]U-' + O(a2).
Since (U-τ)*fc
∂L(A,B)λ
(U 1)1*
=((UT)T)*k = ((UT)k*)τ, U-1 =	. and VaL(A,B)1
(U 1)m*
YJT(BUT)TVLo(BUT)(U- τ)*k from (24), we have that
Zt [J1,tVAL(At,Bt)*1	∙∙∙	Jm,tVAL(At,Bt)*m] Uj1
=YZt [J1,tJ>tZT VLo(Zt)(U-τ)*1	∙∙∙	Jm,tJmtZt VLO(Zt)(UJT)*m] UtT
Γ (UtT )1*,
=YZt [J1,tJ>tZ>VLo(Zt)(UJT)*1	∙∙∙	Jm,tJm,tZ>VLo(Zt)(Ujτ)*m]	.
(Ut-1)m*
m
=X Zt Jk,t JTtZTVLo(Zt)((Uj1 )k*)T(UtT)k*
k=1
m
=X Zt Jk,t JTtZTVL(Zt)(Ujτ)*k (Uj1)k*
k=1
On the other hand, using Bt+1 = [Bt - αVbL(At,Bt)] and VBL(A, B):=
dL∂ABB = (Pn=I (d'(∂yyl)τ φ(χi)τ) U-τ	= VLo(BU-1)U-τ, we have
21
Published as a conference paper at ICLR 2021
Bt+1U-1	=	Zt — αVL0 (Zt)U- >U-1. Summarizing these equations by noticing
U-TU-1 = Pm=I(U->)*k(U-1)k* yields that
m	m
Zt+1 = Zt — αVL0(Zt) X(U-T)**(UtT)k* — αγ2 X ZtJk,tJ>tZjVL0(Zt)(U-τ)*k(UtT)k* + O(a2)
+ O(02)
)*k(U-1)k*	+ O(α2)
By vectorizing both sides,
m
Dt ：= E[(U-τ)*k(U-I)k* 乳(Imy + γ2ZtJk,tJ>tZT)],
k=1
We have
vec(Zt+ι) = vec(Zt) — αDt VeC(VL0(Zt)) + O(α2).
This implies that
vec(Zt+ι) — vec(Zt)	C	〜、
------------------=-Dt VeC(VL0(Zt)) + O(α),
α
where α > 0. By recalling the definition of the Euler method and defining Z(t) = Zt, we can
reWrite this as
vec(Z(t + α)) — vec(Z(t))	C //”、、 〜 、
_((T))-------------= -Dt vec(VL0(Zt)) + O(α).
α
By taking the limit for α → 0 and going back to continuous-time dynamics, this implies that
济 VeC(Zt) = -DtVeC(VL0(Zt)).	(26)
dt
Here, we note that the complex interaction over m due to the nonlinearity is factorized out into the
matrix Dt . Furthermore, the interaction within the matrix Dt has more structures when compared
with that in the gradients themselves from (24). For example, unlike the gradients, the interaction
over m even within Dt can be factorized out in the case of my = 1 as:
m
D = X [(U-τ)*k(U-1)k* 氧(Imy + Y 2ZJk JT Z t)]
k = 1
m
X (1 + Y2ZJkJTZt) (U-T)*k(U-1)k*
k=1
U-T diag
U-T
(
Im + diag
∖
∕Γ1 + Y2 Z J1JT Z T
∕Γ7 2ZJ1J>Z
∖ Lγ2ZJmJrTZ
∖11+ Y2ZJmJmnZT
UT
Although we do not assume my = 1, this illustrates the additional structure well.
22
Published as a conference paper at ICLR 2021
A.1.6 ANALYSIS OF THE MATRIX Dt
From the definition of Dt, we have that
m
Dt = X [(U->)*k (U-1)k* 乳(Imy + YZtJk,t Jt Z>)]
k=1
mm
=X [(U->)*k(U-1)k* 乳 Imy] + X [(U->)*k(U-1)k* 乳 Y2ZtJk,tJ>tZ>]
k=1	k=1
mm
= X(u->)*k (u-1)k* 乳 Imy + X[(u->)*k (u-1)k* 乳 Y 2ZtJk,tJ>tZ>]
k=1	k=1
m
=[U->U-1 乳 Imy] + X [(U->)*k(U-1)k* 乳 Y2ZtJk,tJ>,tZ>]	(27)
k=1
Since U->U-1 is positive definite, Imy is positive definite, and a Kronecker product of two positive
definite matrices is positive definite (since the eigenvalues of Kronecker product are the products of
eigenvalues of the two matrices), we have
[U->UT 0 Imy] > 0.	(28)
Since (U->)*k(U-1)k* is positive Semidefinite, γ2Zt Jk,tJ>tZ> is positive Semidefinite, and a
Kronecker product of two positive semidefinite matrices is positive semidefinite (since the eigenval-
ues of Kronecker product are the products of eigenvalues of the two matrices), we have
[(U->)*k(U-1)k* 乳 γ2ZtJk,tJ>tZ>]之 0.
Since a sum of positive semidefinite matrices is positive semidefinite (from the definition of positive
semi-definiteness: x>Mkx ≥ 0 ⇒ x> (Pk Mk)x = Pk x>Mkx ≥ 0),
m
X[(U->)*k(UT)k* 乳 Y2ZtJk,tJ>tZ>]占 0.	(29)
k=1
Since a sum of a positive definite matrix and positive semidefinite matrix is positive definite (from the
definition of positive definiteness and positive definiteness: (x>M1x > 0 ∧ x>M2x) ⇒ x>(M1 +
M2 )x = x> M1 x + x> M2 x > 0),
m
Dt = X [(U > )*k (U 1)k* % (Imy + γ2ZtJk,tJ>tZt )]
k=1
m
=[U->U-1 % Imy] + X[(U->)*k(U-1)k* % γ2ZtJk,tJ>tZ>] X 0.
k=1
Therefore, Dt is a positive definite matrix for any t and hence
λT := inf λmin(Dt) > 0.	(30)
t∈[0,T]
A.1.7 Convergence RATE VIA POLYAK-匕OJASIEWICZ inequality and norm bounds
Let R ∈ (0, ∞] and T > 0 be arbitrary. By taking derivative of Lo(Zt) 一 LXR with respect to time
t with Zt := BtUt-1,
dt (LMZtt 一 L0R) = (XX (常(Zt)) j (ddt(Zt)) j
=X X(靠(Zt))j(§(Zt))j
dX
—L*
dt
—
23
Published as a conference paper at ICLR 2021
where We used the chain rule and the fact that + LSR = 0. By using the vectorization notation with
▽Lo(Zt) = dw0 (Zt),
dt (LO(Zt)- lo,r) = Vec [VLo(Zt)]> Vec — (Zt) ,
dt	dt
By using (26) for the equation of Vec [舟(Zt)],
dt (LO(Zt)- lo,r) = - Vec [VLo(Zt)]τ Dt VeclVLo(Zt)]
≤ -λmin(Dt)k Vec [VLo(Zt)] k2
= -λmin(Dt)kVLO(Zt)k2F
Using the condition that VLo satisfies the the Polyak-Eojasiewicz inequality with radius R, if
kZt k1 < R, then we have that for allt ∈ [0, T],
dt (LO(Zt)- lo,r) ≤ -2κλmin(Dt)(LO(Zt)- L0,R)
≤ -2κλT (Lo (Zt) - Loo,R).
By solving the differential equation, this implies that if kZt k1 < R,
LO(ZT) - lO,R ≤ (LO(ZO)- lo,r) e-2κλTT,
Since L(At, Bt) = LO(Zt), if kZtk1 < R,
L(AT,BT) ≤ LOo,R + (L(AO, BO) - LoO,R)e-2κλTT.	(31)
We now complete the proof of the first part of the desired statement by showing that kBk1 < (1 -
γ)R implies kZtk1 < R. With Z = BU -1, since any induced operator norm is a submultiplicative
matrix norm,
kZk1 = kB(Im - γσ(A))-1k1 ≤ kBk1k(Im - γσ(A))-1k1.
We can then rewrite
k(Im - γσ(A))-1k1 = k((Im - γσ(A))-1)>k∞ = k(Im - γσ(A)>)-1k∞.
Here, the matrix Im — γσ(A)> is StriCtIydiagOnanydOminant: i.e., |Im — γσ(A)> |近 > Pj6=i |Im -
γσ(A) |ij for any i. This can be shown as follows: for any j,
1 >γ u⇒ 1 >γ £ σ(A)ij
i
^⇒ 1 > Yσ(A)jj + X Yσ(A)ij
i6=j
^⇒ 1 - Yσ(A)j-j- > X Yσ(A)ij
i6=j
^--⇒ |Im - γσ(A)Ijj > X | - γσ(A)Iij
i6=j
^--⇒ |Im - γσ(A)Ijj > X |Im - γσ(A)Iij
i6=j
^--⇒ |Im -γσ(A)TIjj > X |Im - γσ(A)TIji
i6=j
This calculation also shows that IIm-γσ(A)Ijj - i6=j IIm-γσ(A)Iij = 1-γ forallj. Thus, using
the Ahlberg-Nilson-Varah bound for the strictly diagonally dominant matrix (Ahlberg & Nilson,
1963; Varah,1975; Moraca, 2008), we have
k(Im-γσ(A)>)-1k∞ ≤
1
minj(IIm - γσ(A)jj - £?当’ IIm - γσ(A)Ij)
1
1 - Y
24
Published as a conference paper at ICLR 2021
By taking transpose,
k (Im -Yσ(A))T k1 ≤ —
Summarizing above,
kZkι = kB(Im — γσ(A))-1kι ≤kBkιɪ.
1- γ
Therefore,if kBkι < R(1 - Y), then ∣∣Z∣∣ι = ∣∣B(Im - γσ(A))-1kι ≤ IlBllIT-Y < R, as desired.
Combining this with (31) implies that if kB k1 < R(1 - γ),
L(At, Bt) ≤ L0,r + (L(Ao, Bo) - LS,R)e-2κλTT
Recall that L0,r = infW"∣w∣∣1<r Lo(W) and LR = inf A∈Rm×m,B∈BR L(A,B) where BR = {B ∈
Rmy×m | IBI1 < (1 - γ)R}. Here, B ∈ BR implies that IZI1 = IBU-1I1 ≤ IBI1IU-1I1 <
(1 - γ )R∣∣U-1∣ι ≤ R, using the above upper bond ∣∣U-1∣ι = ∣∣(Im - γσ(A))-1∣ι ≤ T-Y. Since
L(A, B) = Lo(Z) with Z = BU-1, this implies that L0,r ≤ LR and thus
L(At, Bt) ≤ L0,R + (L(Aο, Bo) - L0,RLKλT ≤ LR + (L(Aο, Bo) - L0,R)e-2κλT
This completes the first part of the desired statement of Theorem 1.
The remaining task is to lower bound λT, which is completed as follows: for any (A, B),
λmin(D) = min v>Dv
v:kvk=T
=min*> (XX[(U->)*k(UtT)k* 乳(Imy + Y2ZtJk,tJ>tZ>)][v
v:kvk=T	k=T
≥ min v> IU->UT 0 Im ] V
v:kvk=T	y
λmin(IU->U-T 0Imy)
λmin(U->U-T)
σm2 in(U-T)
1	、	1
PI ≥ mρt
(32)
where the third line follows from (27)-(29), the fifth line follows from the property of Kronecker
product (the eigenvalues of Kronecker product of two matrices are the products of eigenvalues of
the two matrices), and the last inequality follows from the relation between the spectral norm and
the norm ∣∣ ∙ ∣∣ι. We now compute ∣∣U∣∣ι as: for any (A, B),
∣U∣T = ∣Im - Yσ(A)∣T
= max	|(Im - Yσ(A))ij |
ji
= max	|(Im)ij - Yσ(A)ij |
ji
= max |(Im)jj - Yσ(A)jj | + X |(Im)ij - Yσ(A)ij |
j
i6=j
=max 11 - Yσ(A)jj | + X | - Yσ(A)ij |
j	i6=j
= max1 - Yσ(A)jj +	Yσ(A)ij
j
i6=j
=max 1 + Y ( (X σ(A)i) - σ(A)jj
≤ 1 + Y.
25
Published as a conference paper at ICLR 2021
By substituting this into (32), we have that for any (A, B) (and hence for any t),
λmin(D) ≥	J ∖2 .	(33)
m(1+ γ)2
This completes the proof for both the first and second parts of the statement of Theorem 1.
□
A.2 Proof of Theorem 2
We first show that with δt > 0 sufficiently small, we have Gt 0. Recall that
Gt = Ut(S-I- "Ft) U> = UtS-1U> - δtUtFtU>.
Thus, with δt > 0 sufficiently small, for any v 6= 0,
v>Gv = v>UtSt-1Ut>v - δtv>UtFtUt>v,
which is dominated by the first term v>UtSt-1Ut>v if the matrix UtSt-1Ut> is positive definite.
Since St := Im +γ2 diag(vtS) with vtS ∈ Rm and (vtS)k := kJk>,t(BtUt-1)> k22 for k = 1, 2, . . . , m,
the matrix UtSt-1Ut> is positive definite. Thus, with δt > 0 sufficiently small, v> Gv is dominated
by the first term, which is positive (since UtSt-1Ut> is positive definite), and thus we have Gt 0.
Then we observe that the output of argminVkvkG ≤δt∣∣ 呆 Vec(BtU-1)∣∣g L0(v) is the set of solutions
of the following constrained optimization problem:
d 2
minimize L0(v)	s.t. ∣∣v∣∣G — δ2 ʒ-veC(BtU-1)	≤ 0.
v	0	t t dt	t G
Since this optimization problem is convex, one of the sufficient conditions for global optimality is
the KKT condition with a multiplier μ ∈ R:
VL0(v) + 2μGtV = 0
μ ≥ 0
Therefore, the desired statement is obtained if the above KKT condition is satisfied by v =
δt (dt Vec(BtU-I)) with some multiplier μ. The rest of this proof shows that the KKT condi-
tion is satisfied by setting V = δt (6 Vec(BtU-1)) and μ = 羡. With this choice, the last two
conditions of the KKT condition hold, since
μ =市 ≥ 0,
2δt
and
222
kvkGt — δ 万 Vec(BtUtT)=鼻 * Vec(BtUtT)—鼻 * Vec(BtUtT)	=0.
dt	G	dt	G dt	G
The remaining task is to show that VL0(v) + 2μGtv = 0 with V = δt (捋 Vec(BtU-1)) and
μ = 2∣;. From the definition of L0,
VL0(v) + 2μGtv = VLvec(Vec(BtU-1)) + V2LVec(Vec(BtU-1))v + 2μGtv.	(34)
Wenow compute and VLvec and V2Lvec. Since VL°(W):= dLW) = Pi=ι (飞,))> x>,
Vec(VL0 (W)) = XVec Imy
i=1
φ(xi)>
n
E[φ(xi ) 乳 Imy ]
i=1
26
Published as a conference paper at ICLR 2021
where
^ ：= Wφ(xi) = [φ(xi)> 於 Imy] vec[W].
Therefore,
n
VLveC(vec(W)) = VeC(VL0(W)) = £[。3)乳 Imy]
i=1
For the Hessian,
∂
V2Lvec(vec(W)) = d VeC(W)VLveC(VeC(W))
n
£[。(Xi)乳 Imy ]
i=1
n
£[。(Xi)乳 Imy ]
i=1
(*z))>
By defining 'i(z) = '(z,yi) and V2'i(z)=%
n
V2Lvec(vec(W)) = £[。(“)㊈ Imy]V2'i(Wφ(xi))[φ(g)> 乳 ^].	(35)
i=1
Since we have that
(Im -γσ(A)) (X Y kσ(A)j
=Im - Yσ(A) + Yσ(A)-(Yσ(A))2 + (Yσ(A))2 - (Yσ(A))3 +-("(A))'+1
=I-(γσ(A))l+1,
we can write:
(Im - γσ(A)) l lim Za)(X,A)) = Iim(Im — γσ(A)) (Xγkσ(A)kφ(x)
∖l→∞	) l→∞	∖ z—
=(Im - ^lim(γσ(A))l+1^ φ(x)
φ (X),
where we used the fact that γσ(A)j ≥ 0, ∣∣σ(A)k1 = maxj £分 ∣σ(A)j| = 1, and thus
∣∣γσ(A)∣∣ι < 1 for any Y ∈ (0,1). This shows that (limι→∞ Z(I)(X,A)) = z*(x,A) = U-1φ(x),
from which we have φ(xi) = Uz*(xi,A).
Substituting φ(xi) = Uz*(xi,A) into (35),
V2Lvec(vec(W))
n
=X[Uz*(Xi,A)乳 Imy]V2'i(Wφ(Xi))[z*(Xi,A)>UT 乳 Imy].
i=1
n
=X[U 乳 ImynZ*(Xi, A)乳 Imy]V24i(Wφ(Xi))[z*(Xi, A)T 乳 Imy ][UT 乳 Imy].
i=1
=[U 0 Imy ] (X [z*(Xi,A) 0 Imy ]V2'i(Wφ(Xi))[z*(Xi, A)T 0 Imy ]^ [UT 0 Imy ].
27
Published as a conference paper at ICLR 2021
In the case of my = 1, since Imy = 1, V2LveC(VeC(W)) is further simplified to:
V2LveC(VeC(W)) = U (XX V24i(Wφ(xi))z*(g,A)z*(xi,A) J Uτ.
Therefore,
V2 LveC(VeC(BtUtT)) = UtFtUi
where
n
Ft = E V2'i(BtU-1φ(xi))z*(xi,At)z* (xi,At)τ.
i=1
By plugging μ =表 and V2Lvec(vec(BtU-1)) = UtFtUτ into (34),
VL0(v) + 2μGtv = VLveC(VeC(BtUJ1)) + UtFtUTv + *Ut(S-I-瓦尤)UTv
=VLvec (VeC(BtUtT)) + ɪ UtS-1U>v.
δt
ByUSing V = δt (d VeC(BtUtT)),
VLlO(V) + 2μGtν = VLveC(VeC(BtU-1)) + UtS
By plugging (26) into 第 VeC(BtUtT) with Zt = BtU-1,
VLO(v) + 2μGtv = VLveC(VeC(BtUtT))- UtS-IUTDt VeC(VLo(Zt)).
Recall that
m
Dt = E[(U-τ)*k(U-1)k* 乳(Imy + Y2ZtJk,tJ[tZT)].
k=1
In the case of my = 1, the matrix Dt can be simplified as:
m
D = X [(U-τ)*k(U-1)k* 氧(Imy + Y2ZJkJTZτ)]
k = 1
m
=E (1 + Y2ZJk JTZt) (U-T)*k(U-1)k*
k = 1
∕Γ 1 + γ2ZJ1JT Zt ]∖
=U-τ diag	.	I I U-1
l[1+ Y2ZJmJm Z Tv
=U-τ SU-1.
Plugging this into the above equation for Dt,
VLO(v) + 2μGtv = VLveC(VeC(BtU-I))- UtS-IUTU-TStUtTVeC(VLO(Zt))
=VLveC(VeC(BtU-I))- VLveC(VeC(BtUtT)) = 0.
Therefore, the constrained optimization problem at time t is solved by
V = δt
ɑt VeC(BtUtT)),
which implies that
% VeC(BtUtT)=*VeC(Vt)，
VeC(Vt) ∈	argmin	L0(v),
v"∣vk Gt ≤δtk 品 veC(BtU-1)kGt
28
Published as a conference paper at ICLR 2021
By multiplying φ(x)> 0 Imy to each side of the equation, We have
-d[φ(x)> 0 Imy]vec(BtU-1) = Bt f lim z(l)(x,At)),
dt	y	l→∞
-1[φ(x)> 0 Imy]vec(Vt) = ɪVtφ(x),
δt	y	δt
yielding that
d-Btt f lim z(l)(x,At)) = 1 Vtφ(x).
dt l→∞	δt
This proves the desired statement of Theorem 2.
□
A.3 Proof of Corollary 1
The assumption rank(Φ) = min(n, m) implies that σmin(Φ) > 0. Moreover, the square loss
` satisfies the assumption of the differentiability. Thus, Theorem 1 implies the statement of this
corollary if Lo with the square loss satisfies the Polyak-Eojasiewicz inequality for any W ∈ Rmy ×m
with parameter K = 2σmin(Φ)2. This is to be shown in the rest of this proof. By setting 夕=Lo
in Definition 1, we have k▽夕Vec(Vec(q))k2 = ∣∣VLo(W)kF. With the square loss, we can write
Lo(W) = Pin=1 kWφ(xi) - yik22 = kWΦ - Yk2F where Φ ∈ Rm×n and Y ∈ Rmy×n with
Φji = φ(xi)j andYji = (yi)j. Thus,
VLo(W) = 2(WΦ -Y)Φ> ∈ Rmy×m.
We first consider the case of m ≤ n. In this case, we consider the vectorization Lovec(vec(W)) =
Lo (W) and derive the gradient with respect to vec(W):
VLovec (vec(W)) = 2 vec((W Φ - Y)Φ>) = 2[Φ 0Imy]vec(WΦ - Y).
Then, the Hessian can be easily computed as
V2Lovec(vec(W)) =2[Φ0Imy][Φ> 0Imy] =2[ΦΦ>0Imy],
where Imy is the identity matrix of size my by my. Since the singular values of Kronecker product
of the two matrices is the product of singular values of each matrix, we have
V2Lovec (vec(W))	2σmin(Φ)2Imym,
where we used the fact that m ≤ n in this case. Since W is arbitrary, this implies that Lovec is
strongly convex with parameter 2σmin(Φ)2 > 0 in Rmy ×m. Since a strongly convex function
with some parameter satisfies the Polyak-Eojasiewicz inequality with the same parameter (Karimi
et al., 2016), this implies that Lvoec (and hence Lo) satisfies the Polyak-Eojasiewicz inequality with
parameter 2σmin(Φ)2 > 0 in Rmy ×m in the case ofm ≤ n.
We now consider the remaining case ofm > n. In this case, using the singular value decomposition
ofΦ= UΣV>,
2∣VLo(W)kF = 2∣Φ(WΦ - Y)>kF
= 2∣UΣV>(WΦ - Y)>∣2F
= 2∣ΣV>(WΦ - Y)>∣2F
≥ 2σmin(Φ)2∣V>(WΦ-Y)>∣2F
= 2σmin(Φ)2Lo(W)
≥ 2σmin(Φ)2 (Lo(W) - L百)
for any Lq* ≥ 0, where the first line uses ∣∣q∣F = ∣∣q>∣F, the second line uses the singular value
decomposition, and the third and fourth line uses the fact that U and V are orthonormal matrices.
The forth line uses the fact that m > n in this case. Therefore, since W is arbitrary, we have
shown that Lo satisfies the Polyak-Eojasiewicz inequality for any W ∈ Rmy ×m with parameter
κ = 2σmin(Φ)2 in both cases ofm > n and m ≤ n.
□
29
Published as a conference paper at ICLR 2021
A.4 Proof of Corollary 2
The assumption rank(Φ) = m implies that σmin(Φ) > 0. Moreover, the logistic loss ` satisfies the
assumption of the differentiability. Thus, Theorem 1 implies the statement of this corollary if L0
with the logistic loss satisfies the Polyak-Ecjasiewicz inequality with the given radius R ∈ (0, ∞]
and the parameter κ = (2τ + ρ(R))σmin(Φ)2 where ρ(R) depends on R. Let R ∈ (0, ∞] be given.
Note that we have ρ(R) > 0 if R < ∞, and ρ(R) = 0 if R = ∞. If (R, τ) = (∞, 0), then
2τ + ρ(R) = 0 for which the statement of this corollary trivially holds (since the bound does not
decrease). Therefore, we focus on the remaining case of (R, τ) 6= (∞, 0). Since (R, τ) 6= (∞, 0),
we have 2τ + ρ(R) > 0. We first compute the Hessian with respect to W as:
nn
V2L0(W) = X p^i(1 - pi)φ(xi)φ(xi)> + 2τ X φ(xi)φ(xi)>,
i=1	i=1
where Pi = "e-Wφ(xi). Therefore,
nn
v>V2L0(W)v = XPi(I- pi)v>φ(xi)φ(xi)>v + 2τv> ( X φ(xi)φ(xi)> ) v
i=1	i=1
nn
≥ ρ(R) X v>φ(xi)φ(xi)>v + 2τv> X φ(xi)φ(xi)> v
i=1	i=1
= (2τ + ρ(R))v>ΦΦ>v
≥ (2τ + ρ(R))σmin(Φ)2 >0.
Therefore, L0 is strongly convex with parameter (2τ + ρ(R))σmin(Φ)2 > 0. Since a strongly con-
vex function with a parameter satisfies the Polyak-Eojasiewicz inequality with the same parameter
(Karimi et al., 2016), this implies that L0 satisfies the Polyak-Eojasiewicz inequality with the given
radius R ∈ (0, ∞] and parameter (2τ + ρ(R))σmin (Φ)2 > 0. Since R ∈ (0, ∞] is arbitrary, this
implies the statement of this corollary by Theorem 1.
□
A.5 Proof of Proposition 1
Let (x, A) be given. By repeatedly applying the definition of z(l)(x, A), we obtain
z(l)(x, A) = γσ(A)z(l-1) + φ(x)
= γσ(A)(γσ(A)z(l-2) + φ(x)) + φ(x)
l
= Xγkσ(A)kφ(x),	(36)
k=0
where σ(A)k represents the matrix multiplications ofk copies of the matrix σ(A) with σ(A)0 = Im.
In general, if σ is identity, this sequence does not converge. However, with our definition of σ, we
have
kσ(A)kι = maxE lσ(A)ij| = L
j
i
Therefore, kγσ(A)k1 = γkσ(A)k1 = γ < 1 for any γ ∈ (0, 1). Since an induced matrix norm is
sub-multiplicative, this implies that
k
kγkσ(A)kk1≤Ykγσ(A)k1=γk.
i=1
In other words, each term kγkσ(A)kk1 in the series Pk∞=0 kγkσ(A)kk1 is bounded by γk. Since
the series Pk∞=0 γk converges in R with γ ∈ (0, 1), this implies, by the comparison test, that the
series Pk∞=0 kγkσ(A)kk1 converges in R. Thus, the sequence (Plk=0 kγkσ(A)kk1)l is a Cauchy
30
Published as a conference paper at ICLR 2021
sequence. By defining sl = Plk=0 γkσ(A)k, we have ksl - sl0 k1 = k Plk=l0+1 γkσ(A)kk1 ≤
Pk=l0+1 kγkσ(A)kk1 for any l0 > l by the triangle inequality of the (matrix) norm. Since
(Plk=0 kγkσ(A)k k1)l is a Cauchy sequence, this inequality implies that (Plk=0 γkσ(A)k)l is a
Cauchy sequence (in a Banach space (Rm×m, k ∙ kι), which is isometric to Rmm under k』i). Thus,
the sequence (Plk=0 γkσ(A)k)l converges. From (36), this implies that the sequence (z(l) (x, A))l
converges.	□
B	On the Implicit Bias
In this section, we show that Theorem 2 suggests an implicit bias towards a simple function as a
result of infinite depth, whereas understanding this bias in more details is left as an open problem.
This section focuses on the case of the square loss '(q, yi) = ∣∣q - yik2 with my = 1. By solving
vec(Vt) ∈ argminv∈V Lt0(v) in Theorem 2 for the direction of the Newton method, Theorem 2
implies that
Vt= -rt>(Im-γσ(At))-1 ∈ R1×m,	(37)
where rt ∈ Rm is an error vector with each entry being a function of the residuals fθt (xi) - yi.
Since σ(At) is a positive matrix and is a left stochastic matrix due to the nonlinearity σ, the Perron-
Frobenius theorem (Perron, 1907; Frobenius, 1912) ensures that the largest eigenvalue of σ(At) is
one, any other eigenvalue in absolute value is strictly smaller than one, and any left eigenvector
corresponding the largest eigenvalue is the vector η1 = η[1, 1, . . . , 1]> ∈ Rm where ζ ∈ R is some
scalar. Thus, the largest eigenvalue of the matrix (Im — γσ(At))-1 is y-γ, any other eigenvalue is
in the form of ι-λfe7 with ∣λk∣ < 1, and any left eigenvector corresponding the largest eigenvalue
is η1 ∈ Rm. By decomposing the error vector as rt = P1rt + (1 - P1)rt, this implies that
vec(Vt) = V> =±Pirt + gγ((1 - Pι)rt) ∈ Rm, where Pi = m 11> is a projection onto the
column space of 1 = [1, 1, . . . , 1]> ∈ Rm, and gγ is a function such that for any q in its domain,
∣gγ (q)∣ < c for all γ ∈ (0, 1) with some constant c in γ.
In other words, Vec(Vt) in Theorem 2 can be decomposed into the two terms: ɪ-^Pirt (the pro-
jection of the error vector onto the column space of 1) and gγ((1 - Pi)rt) (a function of the pro-
jection of the error vector onto the null space space of 1). Here, as Y → 1, ∣ ɪ-YPIrtk → ∞ and
∣gγ ((1 - Pi )rt)∣ < c. This implies that withγ < 1 sufficiently large, the first term y-γPirt
dominates the second term gγ ((1 - Pi)rt).
Since (Pirt)>φ(x) = m 1>φ(x) with μt = Pm=i(rt)k ∈ R, this implies that with γ < 1 suf-
ficiently large, the dynamics of deep equilibrium linear models d fθt = * Vt Φ learns a simple
shallow function μm 1>φ(χ) first before learning more complicated components of the functions
through gγ((1 - Pι)rt), where μτ = R0T μtdt ∈ R. Here, μmT 1>φ(x) is a simple average model
that averages over the features mm 1>φ(χ) and multiplies it by a scaler μt. Moreover, large γ < 1
means that we have large effective depth or large weighting for deeper layers since we have a shallow
model with γ = 0 and γ is a discount factor of the infinite depth.
C	Additional Discussion
On existence of the limit. When Bai et al. (2019a) introduced general deep equilibrium models,
they hypothesized that the limit liml→∞ z(l) exists for several choices of h, and provided numerical
results to support this hypothesis. In general deep equilibrium models, depending on the values of
model parameters, the limit is not ensured to exists. For example, when h increases the norm of
the output at every layer, then it is easy to see that the sequence diverge or explode. This is also
true when we set h to be that of deep equilibrium models without the nonlinearity σ (or equivalently
redefining σ to be the identity function): if the operator norms on A are not bounded by one, then the
sequence can diverge in general. In other words, in general, some trajectory of gradient dynamics
may potentially violate the assumption of the existence of the limit when learning models. In this
view, the class of deep equilibrium linear models is one instance of general deep equilibrium models
where the limit is guaranteed to exist for any values of model parameters as stated in Proposition 1.
On the PL inequality. With R sufficiently large, the definition of the PL inequality in this paper
is simply a rearrangement in the form where L0 is allowed to take matrices as its inputs through
31
Published as a conference paper at ICLR 2021
Lvec(Vec(∙)) = Lo(∙), where vec(M) represents the standard vectorization of a matrix M. See
(Polyak, 1963; Karimi et al., 2016) for more detailed explanations of the PL inequality.
On the reditus R for the logistic loss. As shown in Section 3.1.3, we can use R = ∞ for the
square loss and the logistic loss, in order to get a prior guarantee for the global linear convergence
in theory. In practice, for the logistic loss, we may want to choose R depending on the different
scenarios, because of the following observation.For the logistic loss, we would like to set the radius
R to be large so that the trajectory on B is bounded as kBtk1 < (1 - γ)R for all t ∈ [0, T] and the
global minimum value on the constrained domain to decrease: i.e., LR → L* as R → ∞. However,
unlike in the case of the squared loss, the convergence rate decreases as we increase R in the case
of the logistic loss, because ρ(R) decreases as R increases. This does not pose an issue because we
can always pick R < ∞ so that for any t > 0 and T > 0, we have ρ(R) > cρ for some constant
cρ > 0. Moreover, this tradeoff does not appear for the square loss: i.e., we can set R = ∞ for the
square loss without decreasing the convergence rate. We can also avoid this tradeoff for the logistic
loss by simply setting R = ∞ and τ > 0.
On previous work without implicit linearization. In Section 5, we discussed the previous
work on deep neural networks with implicit linearization via significant over-parameterization.
Kawaguchi & Huang (2019) observed that we can also use the implicit linearization with mild
over-parameterization by controlling learning rates to guarantee global convergence and general-
ization performances at the same time. On the other hand, there is another line of previous work
where deep nonlinear neural networks are studied without any (implicit or explicit) linearization and
without any strong assumptions; e.g., see the previous work by Shamir (2018); Liang et al. (2018);
Nguyen (2019); Kawaguchi & Bengio (2019); Kawaguchi & Kaelbling (2020); Nguyen (2021).
Whereas the conclusions of these previous studies without strong assumptions can be directly appli-
cable to practical settings, their conclusions are not as strong as those of previous studies with strong
assumptions (e.g., implicit linearization via significant over-parameterization) as expected. The di-
rect practical applicability, however, comes with the benefit of being able to assist the progress of
practical methods (Verma et al., 2019; Jagtap et al., 2020b;a).
D Experiments
The purpose of our experiments is to provide a secondary motivation for our theoretical analyses,
instead of claiming the immediate benefits of using deep equilibrium linear models.
D.1 Experimental Setup
For data generation and all models, we set φ(x) = x. Therefore, we have m = mx.
Data. To generate datasets, we first drew uniformly at random 200 input images from a stan-
dard image dataset — CIFAR-10 (Krizhevsky & Hinton, 2009), CIFAR-100 (Krizhevsky & Hinton,
2009) or Kuzushiji-MNIST (Clanuwat et al., 2019) — as pre input data points xipre ∈ Rm0x . Out
of 200 images, 100 images to be used for training were drawn from a train dataset and the 100
other images to be used for testing were drawn from the corresponding test dataset. Then, the input
data pints xi ∈ Rmx with mx = 150 were generated as xi = Rxipre where each entry of a matrix
R ∈ Rmx×mx was set to δ/√mX with δi削.N(0,1) and was fixed over the indices i. We then
generated the targets as yi = B*(limι→∞ z(l)(χi,A*)) + δi ∈ R with Y = 0.8 where δj蚓N(0,1).
Each entry of the true (unknown) matrices A* ∈ R1×m and B* ∈ Rm×m was independently drawn
from the standard normal distribution.
Model. For DNNs, we used ReLU activation and W(I) ∈ Rm×m for l = 1, 2..., H - 1
(W(H) ∈ R1×m). Each entry of the weight matrices W(I) for DNNs was initialized to δ/√m where
δ 3. N(0,1) for all l = 1,2,..., H. Similarly, for deep equilibrium linear models, each entry of
A and B was initialized to δ/√m where δ i削.N(0,1). Linear models were initialized to represent
the exact same functions as those of initial deep equilibrium linear models: i.e., W0 = B0U0-1. We
used γ = 0.8 for deep equilibrium linear models.
Training. For each dataset, we used stochastic gradient descent (SGD) to train linear models, deep
equilibrium linear models, and fully-connected feedforward deep neural networks (DNNs). We used
the square loss '(q, yi) = ∣∣q - yik2. We fixed the mini-batch size to be 64 and the momentum
32
Published as a conference paper at ICLR 2021
coefficient to be 0.8. Under this setting, linear models are known to find a minimum norm solution
(with extra elements from initialization) (Gunasekar et al., 2017; Poggio et al., 2017). Similarly,
DNNs have been empirically observed to have implicit regularization effects (although the most
well studied setting is with the loss functions with exponential tails) (e.g., see discussions in Poggio
et al., 2017; Moroshko et al., 2020; Woodworth et al., 2020). In order to minimize the effect of
learning rates on our conclusion, we conducted experiments with all the values of learning rates
from the choices of 0.01, 0.005, 0.001, 0.0005, 0.0001 and 0.00005, and reported the results with
both the worst cases and the best cases separately for each model (and each depth H for DNNs).
All experiments were implemented in PyTorch (Paszke et al., 2019).
D.2 Additional Experiments
In this subsection, we report additional experimental results.
Additional datasets. We repeated the same experiments as those for Figure 1 with four additional
datasets - modified MNIST (LeCUn et al., 1998), SVHN (Netzer et al., 2011), SEMEION (Srl &
Brescia, 1994), and Fashion-MNIST (Xiao et al., 2017). We report the result of this experiment in
FigUre 4. As can be seen from FigUres 4, we confirmed qUalitatively the same observations as in
FigUre 1: i.e., all models preformed approximately the same at initial points, bUt deep eqUilibriUm
linear models oUtperformed both linear models and nonlinear DNNs in test errors after training.
DNNs without bias terms. In FigUres 1-4, the resUlts of DNNs are reported with bias terms.
To consider the effect of discarding bias term, we also repeated the same experiments with DNNs
withoUt bias term and reported the resUlts in FigUre 5. As can be seen from FigUres 5, we confirmed
qUalitatively the same observations: i.e., deep eqUilibriUm linear models oUtperformed nonlinear
DNNs in test errors.
DNNs with deeper networks. To consider the effect of deeper networks, we also repeated the
same experiments with deeper DNNs with depth H = 10, 100 and 200, and we reported the re-
sUlts in FigUres 6-7. As can be seen from FigUres 6-7, we again confirmed qUalitatively the same
observations: i.e., deep eqUilibriUm linear models oUtperformed nonlinear DNNs in test errors, al-
thoUgh DNNs can redUce training errors faster than deep eqUilibriUm linear models. We experienced
gradient explosion and gradient vanishing for DNNs with depth H = 100 and H = 200.
Larger datasets. In FigUres 1, we Used only 200 data points so that we can observe the effect of
indUctive bias and overfitting phenomena Under a small nUmber of data points. If we Use a large
nUmber of data points, it is expected that the benefit of the indUctive bias with deep eqUilibriUm
linear models tends to become less noticeable becaUse Using a large nUmber of data points can
redUce the degree of overfitting for all models, inclUding linear models and DNNs. However, we
repeated the same experiments with all data points of each datasets: for example, we Use 60000
training data points and 10000 test data points for MNIST. FigUre 8 reports the resUlts where the
valUes are shown with the best learning rates for each model from the set of learning rates SLR =
{0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005} (in terms of the final test errors at epoch = 100). As
can be seen in the figUre, deep eqUilibriUm linear models oUtperformed both linear models and
nonlinear DNNs in test errors.
Logistic loss and theoretical bounds. In Corollary 2, We can set λτ = 一(七产 to get a guarantee
for the global linear convergence rate for any ini-
tialization in theory. HoWever, in practice, this is
a pessimistic convergence rate and We may Want to
choose λT depending on initializations. To demon-
strate this, Figure 3 reports the numerical train-
ing trajectory along With theoretical upper bounds
with initialization-independent λτ = 一(七产 and
initialization-dependent λT = inf t∈[0,T] λmin(Dt). As
can be seen in Figure 3, the theoretical upper bound
with initialization-dependent λT demonstrates a faster
convergence rate.
Figure 3: Logistic loss and theoretical
bounds with initialization-independent
λT and initialization-dependent λT .
33
Published as a conference paper at ICLR 2021
(a) Modified MNIST: Linear V.s. DELM
一Linear (best)
一Linear (worst)
DELM (best)
DELM (worst)
彩
βo -
70`
«»■
50∙
«■
30-
20-
10∖
«
S
O IOOD 2000 3∞0 4000 5000
epoch
epoch
(b) Modified MNIST: DNN V.s. DELM
^BR8*∙∙a ^B7(5
1 1
SSO-a
一Linear (best)
一Linear (worst)
DELM (best)
DELM (worst)
DNN (H=2)
DNN (H=3)
DNN (H=4)
——DELM (best)
——DELM (worst)
ω IOT
O
C 10-*
2 10^9∙
10-12∙
O IOOD 2000 3∞0 4000 5000 O IOOD 2000 3∞0 4000 5000
epoch	epoch
(c)	Modified Fashion-MNIST: Linear V.s. DELM
O IOOD 2000 3∞0 4000 5000	O IOOD 2000 3000 4000 5000
epoch	epoch
(d)	Modified Fashion-MNIST: DNN v.s. DELM
SSo-a,υa SSO-as
O IOOD 2000 3∞0 4000 5000 O IOOD 2000 3∞0 4000 5000
epoch	epoch
(e) Modified SEMEION: Linear Vs DELM
IO2
IO1
10β
10T
(g) Modified SVHN: Linear V.s. DELM
Figure 4: Test and train losses (in log scales) Versus the number of epochs for linear models, deep
equilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs). The plotted
lines indicate the mean Values oVer fiVe random trials whereas the shaded regions represent error
bars with one standard deViations for both test and train losses. The plots for linear models and
DELMs are shown with the best and worst learning rates (in terms of the final test errors at epoch =
5000). The plots for DNNs are shown with the best learning rates for each depth H = 2, 3, and 4
(in terms of the final test errors at epoch = 5000).
IO1
10-l
10^9
10-s
10^τ
10-∙
(f) Modified SEMEION: DNN v.s. DELM
IO2
IO1
10β
IoT
10-2
10^9
10^4
IoT
(h) Modified SVHN: DNN V.s. DELM
34
Published as a conference paper at ICLR 2021
sso-tta
DNN-NB (H=2)
DNN-NB (H=3)
DNN-NB (H =4)
——DELM (best)
--DELM (worst)
IO1
IO-1
IO-9
10~5
IO-7
10-9
10-n
O IOOD 2000 3∞0 4000 50∞	O IOOD 2000 3∞0 4000 5000
epoch	epoch
(a) Modified Kuzushiji-MNIST: NN-NB v.s. DELM
O IOOD 2000 3∞0 4000 5000	O IOOD 2000 3∞0 4000 5000
epoch	epoch
(b) Modified CIFAR-10: DNN-NB v.s. DELM
SSO-tt2
DNN-NB (H=2)
DNN-NB (H=3)
DNN-NB (H =4)
-DELM (best)
DELM (worst)
10a
10β
10-3
10^6
io-9
ιo-u
IOt
IO0
10-i
10^4
10-*
10^,
10-w
(d) Modified Fashion-MNIST: DNN-NB v.s. DELM
DNN-NB (H=2)
DNN-NB (H=3)
DNN-NB (H =4)
——DELM (best)
——DELM (worst)
10a
10β
10-i
io-，
10-*
10-*
10-w
(c) Modified CIFAR-100: DNN-NB v.s. DELM
DNN-NB (H=2)
DNN-NB (H=3)
DNN-NB (H =4)
——DELM (best)
IO2
10β
S 10^2'
O 10-4∙
.E W*
to
2 10-8∙
IoM
10-12∙
(e) Modified MNIST: DNN-NB v.s. DELM
°,∙∙∙∙,∙∙∙',,,,,” 明:
1 1
sso-2
DNN-NB (H=2)
DNN-NB (H=3)
DNN-NB (H =4)
——DELM (best)
——DELM (worst)
IΛ
ω
IO1
IoT
IoT
10^s
(f) Modified SEMEION: DNN-NB v.s. DELM
Figure 5: Test and train losses (in log scales) versus the number of epochs for deep equilibrium linear
model (DELM) and deep neural network with no bias term (DNN-NB). The plotted lines indicate
the mean values over five random trials whereas the shaded regions represent error bars with one
standard deviations for both test and train losses. The plots for DELM are shown with the best and
worst learning rates (in terms of the final test errors at epoch = 5000). The plots for DNN-NB are
shown with the best learning rates for each depth H = 2, 3, and 4 (in terms of the final test errors at
epoch = 5000).
(g) Modified SVHN: DNN-NB v.s. DELM
35
Published as a conference paper at ICLR 2021
DNN-NB (H=IO)
DNN-NB (H=IOO)
DNN-NB (H=200)
DELM (best)
DELM (worst)
(a) Modified Kuzushiji-MNIST: DNN-NB v.s. DELM
0 IOOD 2000 3∞0 4000 5000 O IOOD 2000 3000 4000 5000
epoch	epoch
(b) Modified CIFAR-10: DNN-NB v.s. DELM
0 IOOD 2000 3∞0 4000 5000
epoch
0	10∞ 2000 3∞0 4000 5000
epoch
(c) Modified CIFAR-100: DNN-NB v.s. DELM
SSO-tt£
1%
βo-
70-
«0-
SO-
«-
30-
20-
10∖
«
S
«•
epoch
O IOOD 2000 3000 4000 5000
epoch
(d) Modified Fashion-MNIST: DNN-NB v.s. DELM
(e) Modified MNIST: DNN-NB v.s. DELM
0	10∞ 2000 3∞0 4000 5000
epoch
0 IOOD 2000 3∞0 4000 5000
epoch
(g) Modified SVHN: DNN-NB v.s. DELM
(f) Modified SEMEION: DNN-NB v.s. DELM
Figure 6: Test and train losses (in log scales) versus the number of epochs for deep equilibrium
linear model (DELM) and deeper neural network with no bias term (DNN-NB). The legend is the
same for all subplots and shown in subplot (a). The plotted lines indicate the mean values over five
random trials whereas the shaded regions represent error bars with one standard deviations for both
test and train losses. The plots for DELM are shown with the best and worst learning rates (in terms
of the final test errors at epoch = 5000). The plots for DNN-NB are shown with the best learning
rates for each depth H = 10, 100, and 200 (in terms of the final test errors at epoch = 5000). The
values for DNNs with depth H = 100 and 200 coincide in the plots.
36
Published as a conference paper at ICLR 2021
sso-4
O IOOO 2000 3000 4000 5000
epoch
4000 5000
DNN (H=IO)
DNN (H=IOO)
DNN (H=200)
DELM (best)
DELM (worst)
(a) Modified Kuzushiji-MNIST: DNN v.s. DELM
0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000
epoch	epoch
(b) Modified CIFAR-10: DNN v.s. DELM
0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000
epoch	epoch
(c) Modified CIFAR-100: DNN v.s. DELM
(d) Modified Fashion-MNIST: DNN v.s. DELM
(e) Modified MNIST: DNN v.s. DELM
SSO-tta
O IOOO 2000 3000 4000 5000
epoch
O l∞0 2000 3000 4000 5000
epoch
0	1000 2000 3000 4000 5000
epoch
(g) Modified SVHN: DNN v.s. DELM
0	1000 2000 3000 4000 5000
epoch
(f) Modified SEMEION: DNN v.s. DELM
Figure 7: Test and train losses (in log scales) versus the number of epochs for deep equilibrium
linear model (DELM) and deeper neural networks with ReLU (DNNs). The legend is the same for
all subplots and shown in subplot (a). The plotted lines indicate the mean values over five random
trials whereas the shaded regions represent error bars with one standard deviations for both test and
train losses. The plots for DELM are shown with the best and worst learning rates (in terms of the
final test errors at epoch = 5000). The plots for DNNs are shown with the best learning rates for
each depth H = 10, 100, and 200 (in terms of the final test errors at epoch = 5000). The values for
DNNs with depth H = 100 and 200 coincide in the plots.
37
Published as a conference paper at ICLR 2021
SSo-tt£
IO2
Vfi
IO-2-
1L
10-β
10^8 ■
10-10
20	40	60	80 IOO	O
epoch
40	60	80 IOO
epoch
DNN (H=2)
DNN (H =4)
DNN (H = IO)
Linear
DELM
(a) Modified Kuzushiji-MNIST: NN-NB V.s. DELM
O 20	40	60	80 IOO	O 20	40	60	80 IOO
epoch	epoch
(b) Modified CIFAR-10: DNN-NB v.s. DELM
(c) Modified CIFAR-100: DNN-NB V.s. DELM
(d) Modified Fashion-MNIST: DNN-NB v.s. DELM
0	20	40	®	80	100	0	20	40	60	80	100
epoch	epoch
(e) Modified MNIST: DNN-NB Vs DELM
O 20	40	60	80 IOO
epoch
O 20	40	60	80 IOO
epoch
(f) Modified SEMEION: DNN-NB v.s. DELM
Figure 8: Test and train losses (in log scales) versus the number of epochs for linear models, deep
equilibrium linear models (DELMs), and deep neural networks with ReLU (DNNs). The legend is
the same for all subplots and shown in subplot (a). The plotted lines indicate the mean values over
three random trials whereas the shaded regions represent error bars with one standard deviations.
(g) Modified SVHN: DNN-NB v.s. DELM
40	60
epoch
100
38