Published as a conference paper at ICLR 2021
Generalization bounds via distillation
Daniel Hsu	Ziwei Ji, Matus Telgarsky, Lan Wang
Columbia University, New York City University of Illinois, Urbana-Champaign
djhsu@cs.columbia.edu	{ziweiji2,mjt,lanwang2}@illinois.edu
Ab stract
This paper theoretically investigates the following empirical phenomenon: given
a high-complexity network with poor generalization bounds, one can distill it into
a network with nearly identical predictions but low complexity and vastly smaller
generalization bounds. The main contribution is an analysis showing that the orig-
inal network inherits this good generalization bound from its distillation, assum-
ing the use of well-behaved data augmentation. This bound is presented both in
an abstract and in a concrete form, the latter complemented by a reduction tech-
nique to handle modern computation graphs featuring convolutional layers, fully-
connected layers, and skip connections, to name a few. To round out the story, a
(looser) classical uniform convergence analysis of compression is also presented,
as well as a variety of experiments on cifar10 and mnist demonstrating sim-
ilar generalization performance between the original network and its distillation.
1	Overview and main results
Generalization bounds are statistical tools which take as input various measurements of a predic-
tor on training data, and output a performance estimate for unseen data — that is, they estimate
how well the predictor generalizes to unseen data. Despite extensive development spanning many
decades (Anthony & Bartlett, 1999), there is growing concern that these bounds are not only dis-
astrously loose (Dziugaite & Roy, 2017), but worse that they do not correlate with the underlying
phenomena (Jiang et al., 2019b), and even that the basic method of proof is doomed (Zhang et al.,
2016; Nagarajan & Kolter, 2019). As an explicit demonstration of the looseness of these bounds,
Figure 1 calculates bounds for a standard ResNet architecture achieving test errors of respectively
0.008 and 0.067 on mnist and cifar10; the observed generalization gap is 10-1, while standard
generalization techniques upper bound it with 1015.
Contrary to this dilemma, there is evidence that these networks can often be compressed or distilled
into simpler networks, while still preserving their output values and low test error. Meanwhile, these
simpler networks exhibit vastly better generalization bounds: again referring to Figure 1, those same
networks from before can be distilled with hardly any change to their outputs, while their bounds
reduce by a factor of roughly 1010. Distillation is widely studied (BUcilUu et al., 2006; Hinton et al.,
2015), but usually the original network is discarded and only the final distilled network is preserved.
The purpose of this work is to carry the good generalization bounds of the distilled network back to
the original network; in a sense, the explicit simplicity of the distilled network is used as a witness
to implicit simplicity of the original network. The main contributions are as follows.
•	The main theoretical contribution is a generalization bound for the original, undistilled network
which scales primarily with the generalization properties of its distillation, assuming that well-
behaved data augmentation is used to measure the distillation distance. An abstract version
of this bound is stated in Lemma 1.1, along with a sufficient data augmentation technique in
Lemma 1.2. A concrete version of the bound, suitable to handle the ResNet architecture in Fig-
ure 1, is described in Theorem 1.3. Handling sophisticated architectures with only minor proof
alterations is another contribution of this work, and is described alongside Theorem 1.3. This
abstract and concrete analysis is sketched in Section 3, with full proofs deferred to appendices.
•	Rather than using an assumption on the distillation process (e.g., the aforementioned “well-
behaved data augmentation”), this work also gives a direct uniform convergence analysis, cul-
minating in Theorem 1.4. This is presented partially as an open problem or cautionary tale, as
1
Published as a conference paper at ICLR 2021
Figure 1: Generalization bounds throughout distillation. These two subfigures track a sequence
of increasingly distilled/compressed ResNet8 networks along their horizontal axes, respectively for
cifar10 and mnist data. This horizontal axis measures distillation distance Φγ,m, as defined
below in eq. (1.1). The bottom curves measure various training and testing errors, whereas the
top two curves measure respectively a generalization bound presented here (cf. Theorem 1.3 and
Lemma 3.1), and a generalization measure. Notably, the top two curves drop throughout a long
interval during which test error remains small. For further experimental details, see Section 2.
its proof is vastly more sophisticated than that of Theorem 1.3, but ultimately results in a much
looser analysis. This analysis is sketched in Section 3, with full proofs deferred to appendices.
•	While this work is primarily theoretical, it is motivated by Figure 1 and related experiments:
Figures 2 to 4 demonstrate that not only does distillation improve generalization upper bounds,
but moreover it makes them sufficiently tight to capture intrinsic properties of the predictors,
for example removing the usual bad dependence on width in these bounds (cf. Figure 3). These
experiments are detailed in Section 2.
1.1 An abstract bound via data augmentation
This subsection describes the basic distillation setup and the core abstract bound based on data
augmentation, culminating in Lemmas 1.1 and 1.2; a concrete bound follows in Section 1.2.
Given a multi-class predictor f : Rd → Rk, distillation finds another predictor g : Rd → Rk which
is simpler, but close in distillation distance Φγ,m, meaning the softmax outputs φγ are close on
average over a set of points (zi)im=1 :
1m
Φγ,m(f,g) := — X∣∣φγ(f(zi)) - Φγ(g(zi))∣∣ι, where Φγ(f(z)) H exp (f(z)∕γ) .	(1.1)
m
i=1
The quantity γ > 0 is sometimes called a temperature (Hinton et al., 2015). Decreasing γ increases
sensitivity near the decision boundary; in this way, it is naturally related to the concept of margins
in generalization theory, as detailed in Appendix B. due to these connections, the use of softmax is
beneficial in this work, though not completely standard in the literature (BUcilUu et al., 2006).
We can now outline Figure 1 and the associated empirical phenomenon which motivates this work.
(Please see Section 2 for further details on these experiments.) Consider a predictor f which has
good test error but bad generalization bounds; by treating the distillation distance Φγ,m(f, g) as an
objective function and increasingly regularizing g, we obtain a sequence of predictors (g0, . . . , gt),
where g0 = f, which trade off between distillation distance and predictor complexity. The curves in
Figure 1 are produced in exactly this way, and demonstrate that there are predictors nearly identical
to the original f which have vastly smaller generalization bounds.
Our goal here is to show that this is enough to imply that f in turn must also have good generalization
bounds, despite its apparent complexity. To sketch the idea, by a bit of algebra (cf. Lemma A.2), we
can upper bound error probabilities with expected distillation distances and errors:
Prχ,y[argmaxf(x)y, = y] ≤ 2Eχ∣∣φγ(f (x)) - φγ(g(x))∣∣1 + 2Eχ,y(1 - φγ(g(x))y).
y0
2
Published as a conference paper at ICLR 2021
The next step is to convert these expected errors into quantities over the training set. The last term
is already in a form we want: it depends only on g, so we can apply uniform convergence with the
low complexity of g. (Measured over the training set, this term is the distillation error in Figure 1.)
The expected distillation distance term is problematic, however. Here are two approaches.
1.	We can directly apply uniform convergence; for instance, this approach was followed by Suzuki
et al. (2019), and a more direct approach is followed here to prove Theorem 1.4. Unfortunately,
it is unclear how this technique can avoid paying significantly for the high complexity of f .
2.	The idea in this subsection is to somehow trade off computation for the high statistical cost
of the complexity of f. Specifically, notice that Φγ,m (f, g) only relies upon the marginal
distribution of the inputs x, and not their labels. This subsection will pay computation to
estimate Φγ,m with extra samples via data augmentation, offsetting the high complexity of f.
We can now set up and state our main distillation bound. Suppose we have a training set ((xi,yi))in=1
drawn from some measure μ, with marginal distribution μχ on the inputs x. Suppose We also have
(zi)im=1 drawn from a data augmentation distribution νn, the subscript referring to the fact that it
depends on (χi)n=ι. Our analysis works when ∣∣dμχ∕dνnk∞, the ratio between the two densities, is
finite. If it is large, then one can tighten the bound by sampling more from νn , which is a computa-
tional burden; explicit bounds on this term will be given shortly in Lemma 1.2.
Lemma 1.1. Let temperature parameter γ > 0 be given, along with sets of multiclass predictors
F and G. Then with probability at least 1 一 2δ over an iid draw of data ((xi, yi))n=ι from μ and
(zi)im=1 from νn, every f ∈ F and g ∈ G satisfy
Radn(G)
where Rademacher complexities Radn and Radm are defined in Section 1.4.
A key point is that the Rademacher complexity Radm (F) of the complicated functions F has a
subscript “m”, which explicitly introduces a factor 1/m in the complexity definition (cf. Section 1.4).
As such, sampling more from the data augmentation measure can mitigate this term, and leave the
complexity of the distillation class G as the dominant term.
Of course, this also requires ∣∣dμχ∕dνn∣∞ to be reasonable. As follows is one data augmentation
scheme (and assumption on marginal distribution μχ) which ensures this.
Lemma 1.2. Let (xi)n=ι be a data SamPle drawn iidfrom μχ, and suppose the Corresponding
density P is supPortedon [0,1]d and is Holder continuous, meaning |p(x) — p(x0)∣ ≤ CaIlx — x0∣∣α
for some Cα ≥ 0, α ∈ [0, 1]. Define a data augmentation measure νn via the following sampling
procedure.
•	With probability 1/2, sample z uniformly within [0, 1]d.
•	Otherwise, select a data index i ∈ [n] uniformly, and sample z from a Gaussian centered at xi,
and having covariance σ2I where σ := n-1/(2a+d).
Then with probability at least 1 — 1/n over the draw of(xi)in=1,
I dμχ^L = 4+o (na√nn⅛
Though the idea is not pursued here, there are other ways to control ∣∣dμχ∕dνn∣∞, for instance via
an independent sample of unlabeled data; Lemma 1.1 is agnostic to these choices.
3
Published as a conference paper at ICLR 2021
1.2	A concrete bound for computation graphs
This subsection gives an explicit complexity bound which starts from Lemma 1.1, but bounds
kdμχ∕dνnk∞ via Lemma 1.2, and also includes an upper bound on Rademacher complexity which
can handle the ResNet, as in Figure 1. A side contribution of this work is the formalism to easily
handle these architectures, detailed as follows.
Canonical computation graphs are a way to write down feedforward networks which include dense
linear layers, convolutional layers, skip connections, and multivariate gates, to name a few, all while
allowing the analysis to look roughly like a regular dense network. The construction applies directly
to batches: given an input batch X ∈ Rn×d, the output Xi of layer i is defined inductively as
X0T :=XT,
XT := σi ([Wi∏iDiiFi]χT-i) = σi
WiΠiDiXiT-1
FiXiT-1
where: σi is a multivariate-to-multivariate ρi-Lipschitz function (measured over minibatches on ei-
ther side with Frobenius norm); Fi is a fixed matrix, for instance an identity mapping as in a residual
network’s skip connection; Di is a fixed diagonal matrix selecting certain coordinates, for instance
the non-skip part in a residual network; Πi is a Frobenius norm projection of a full minibatch; Wi is a
weight matrix, the trainable parameters; [Wi∏iDiiFi] denotes row-wise concatenation of WiniDi
and Fi .
As a simple example of this architecture, a multi-layer skip connection can be modeled by including
identity mappings in all relevant fixed matrices Fi , and also including identity mappings in the
corresponding coordinates of the multivariate gates σi . As a second example, note how to model
convolution layers: each layer outputs a matrix whose rows correspond to examples, but nothing
prevents the batch size from changes between layers; in particular, the multivariate activation before
a convolution layer can reshape its output to have each row correspond to a patch of an input image,
whereby the convolution filter is now a regular dense weight matrix.
A fixed computation graph architecture G(ρ~, b, ~r, ~s) has associated hyperparameters (ρ~, b, ~r, ~s), de-
scribed as follows. P is the set of Lipschitz constants for each (multivariate) gate, as described
before. r is a norm bound kWTk2,1 ≤ r (sum of the ∣∣ ∙ k2-norms of the rows), bi√n (where n
is the input batch size) is the radius of the Frobenius norm ball which Πi is projecting onto, and si
is the operator norm of X → [Wi∏iDiXTiFiXt]. While the definition is intricate, it cannot only
model basic residual networks, but it is sensitive enough to be able to have si = 1 and ri = 0 when
residual blocks are fully zeroed out, an effect which indeed occurs during distillation.
Theorem 1.3. Let temperature parameter γ > 0 be given, along with multiclass predictors F, and
a computation graph architecture G. Then with probability at least 1 - 2δ over an iid draw of data
((xi, yi))n=ι from μ and (Zi)n=ι from Vn, every f ∈ F satisfies
Under the ConditionSofLemma 1.2, ignoring an additionalfailureprobability 1/n, then ∣∣ dμx∣∣∞
4 + O (nɑ√(2⅛+d)).
A proof sketch of this bound appears in Section 3, with full details deferred to appendices. The
proof is a simplification of the covering number argument from (Bartlett et al., 2017a); for another
computation graph formalism designed to work with the covering number arguments from (Bartlett
et al., 2017a), see the generalization bounds due to Wei & Ma (2019).
4
Published as a conference paper at ICLR 2021
(a) Comparison of bounds on cifar10.
Figure 2: Performance of stable rank bound (cf. Theorem 1.4). Figure 2a compares Theorem 1.4
to Lemma 3.1 and the VC bound (Bartlett et al., 2017b), and Figure 2b normalizes the margin
histogram by Theorem 1.4, showing an unfortunate failure of width independence (cf. Figure 3).
For details and a discussion of margin histograms, see Section 2.
(b) Width dependence with Theorem 1.4.
1.3	A uniform-convergence approach to distillation
In this section, we derive a Rademacher complexity bound on F whose proof internally uses com-
pression; specifically, it first replaces f with a narrower network g, and then uses a covering number
bound sensitive to network size to control g. The proof analytically chooses g’s width based on the
structure of f and also the provided data, and this data dependence incurs a factor which causes
the familiar 1∕√n rate to worsen to 1/n1/4 (which appears as IIXl∣F∕n3/4). This proof is much more
intricate than the proofs coming before, and cannot handle general computation graphs, and also
ignores the beneficial structure of the softmax.
Theorem 1.4. Let data matrix X ∈ Rn×d be given, and let F denote networks of the form
X → gl(Wl …σ1(W1x)) with spectral norm IlWik2 ≤ Si, and I-LipSChitz and 1 -homogeneous
activations σi, and kWi kF ≤ Ri and width at most m. Then
Rad(F) = o ( ∣X∕4f hY SjnX(Ri∕sV5∏x in Rii1/4).
ji	i
The term Ri∕si is the square root of the stable rank of weight matrix Wi, and is a desirable quantity in
a generalization bound: it scales more mildly with width than terms like ∣∣ WiT ∣∣2,1 and k WiT k F √ width
which often appear (the former appears in Theorem 1.3 and Lemma 3.1). Another stable rank bound
was developed by Suzuki et al. (2019), but has an extra mild dependence on width.
As depicted in Figure 2, however, this bound is not fully width-independent. Moreover, we can
compare itto Lemma 3.1 throughout distillation, and not only does this bound not capture the power
of distillation, but also, eventually its bad dependence on n causes it to lose out to Lemma 3.1.
1.4 Additional notation
Given data (zi)in=1, the Rademacher complexity of univariate functions H is
1	iid
Rad(H)= E~sup — T eih(zi),	where ∈i ~ UnifOrm({-1,+1}).
h∈Hn i
Rademacher complexity is the most common tool in generalization theory (Shalev-Shwartz & Ben-
David, 2014), and is incorporated in Lemma 1.1 due to its convenience and wide use. To handle
multivariate (multiclass) outputs, the definition is overloaded via the worst case labels as Radn(F) =
supy~∈[k]n Rad({(x, y) 7→ f (x)y : f ∈ F}). This definition is for mathematical convenience, but
overall not ideal; Rademacher complexity seems to have difficulty dealing with such geometries.
Regarding norms, ∣ ∙ ∣ = ∣∣ ∙ ∣∣f will denote the Frobenius norm, and ∣∣ ∙ ∣2 will denote spectral norm.
5
Published as a conference paper at ICLR 2021
(a) Margins before distillation.
Figure 3: Width independence. Fully-connected 6-layer networks of widths {64, 256, 1024} were
trained on mnist until training error zero; the margin histograms, normalized by the generalization
bound in Lemma 3.1, all differ, and are close to zero. After distillation, the margin distributions
are far from zero and nearly the same. In the distillation legend, the second term Φγ,m denotes the
distillation distance, as defined in Equation (1.1). Experiment details and an explanation of margin
histograms appear in Section 2.
(b) Margins after distillation.
2 Illustrative empirical results
This section describes the experimental setup, and the main experiments: Figure 1 showing progres-
sive distillation, Figure 2 comparing Theorem 1.4, Lemma 3.1 and VC dimension, Figure 3 showing
width independence after distillation, and Figure 4 showing the effect of random labels.
Experimental setup. As sketched before, networks were trained in a standard way on either
cifar10 or mnist, and then distilled by trading off between complexity and distillation distance
Φγ,m . Details are as follows.
1.	Training initial network f. In Figures 1 and 2a, the architecture was a ResNet8 based on
one used in (Coleman et al., 2017), and achieved test errors 0.067 and 0.008 on cifar10
and mnist, respectively, with no changes to the setup and a modest amount of training; the
training algorithm was Adam; this and most other choices followed the scheme in (Coleman
et al., 2017) to achieve a competitively low test error on cifar10. In Figures 2b, 3 and 4, a
6-layer fully connected network was used (width 8192 in Figure 2b, widths {64, 256, 1024} in
Figure 3, width 256 in Figure 4), and vanilla SGD was used to optimize.
2.	Training distillation network g. Given f and a regularization strength λj , each distillation gj
was found via approximate minimization of the objective
g 7→ Φγ,m(f, g) + λj Complexity(g).	(2.1)
In more detail, first g0 was initialized to f (g and f always used the same architecture) and
optimized via eq. (2.1) with λ0 set to roughly risk(f)/Complexity(f), and thereafter gj+1 was ini-
tialized to gj and found by optimizing eq. (2.1) with λj+1 := 2λj. The optimization method
was the same as the one used to find f . The term Complexity(g) was some computationally
reasonable approximation of Lemma 3.1: for Figures 2b, 3 and 4, it was just Pi kWiTk2,1, but
for Figures 1 and 2a, it also included a tractable surrogate for the product of the spectral norms,
which greatly helped distillation performance with these deeper architectures.
In Figures 2b, 3 and 4, a full regularization sequence was not shown, only a single gj . This was
chosen with a simple heuristic: amongst all (gj)j≥1, pick the one whose 10% margin quantile
is largest (see the definition and discussion of margins below).
Margin histograms. Figures 2b, 3 and 4 all depict margin histograms, a flexible tool to study the
individual predictions of a network on all examples in a training set (see for instance (Schapire &
Freund, 2012) for their use studying boosting, and (Bartlett et al., 2017a; Jiang et al., 2019a) for
6
Published as a conference paper at ICLR 2021
(a) Permuting different fractions of labels.
(b) Zoomed in.
Figure 4: Label randomization. Here {0%, 25%, 50%, 75%, 100%} of the labels were permuted
across the respective experiments. In all cases, the margin distribution is collapsed to zero. For
details, including an explanation of margin histograms, see Section 2.
their use in studying deep networks). Concretely, given a predictor g ∈ G, the prediction on every
example is replaced with a real scalar called the normalized margin via
(xi , yi) 7→
g(Xi)yi - maxj=yi g(Xi)j
Radn(G)
where Radn(G) is the Rademacher complexity (cf. Section 1.4), and then the histogram of these
n scalars is plotted, with the horizontal axis values thus corresponding to normalized margins. By
using Rademacher complexity as normalization, these margin distributions can be compared across
predictors and even data sets, and give a more fine-grained analysis of the quality of the generaliza-
tion bound. This normalization choice was first studied in (Bartlett et al., 2017a), where it was also
mentioned that this normalization allows one to read off generalization bounds from the plot. Here,
it also suggests reasonable values for the softmax temperature γ.
Figure 1: effect of distillation on generalization bounds. This figure was described before;
briefly, a highlight is that in the initial phase, training and testing errors hardly change while bounds
drop by a factor of nearly 1010 . Regarding “generalization measure”, this term appears in studies
of quantities which correlate with generalization, but are not necessarily rigorous generalization
bounds (Jiang et al., 2019b; Dziugaite et al., 2020); in this specific case, the product of Frobenius
norms requires a dense ReLU network (Golowich et al., 2018), and is invalid for the ResNet (e.g., a
complicated ResNet with a single identity residual block yields a value 0 by this measure).
Figure 2a: comparison of Theorem 1.4, Lemma 3.1 and VC bounds. Theorem 1.4 was intended
to internalize distillation, but as in Figure 2a, clearly a subsequent distillation still greatly reduces the
bound. While initially the bound is better than Lemma 3.1 (which does not internalize distillation),
eventually the n1/4 factor causes it to lose out. Also note that eventually the bounds beat the VC
bound, which has been identified as a surprisingly challenging baseline (Arora et al., 2018).
Figure 3:	width independence. Prior work has identified that generalization bounds are quite
bad at handling changes in width, even if predictions and test error don’t change much (Nagarajan
& Kolter, 2019; Jiang et al., 2019b; Dziugaite et al., 2020). This is captured in Figure 3a, where
the margin distributions (see above) with different widths are all very different, despite similar test
errors. However, following distillation, the margin histograms in Figure 3b are nearly identical!
That is to say: distillation not only decreases loose upper bounds as before, it tightens them to the
point where they capture intrinsic properties of the predictors.
Figure 2b: failure of width independence with Theorem 1.4. The bound in Theorem 1.4 was
designed to internalize compression, and there was some hope of this due to the stable rank term.
7
Published as a conference paper at ICLR 2021
Unfortunately, Figure 2b shows that it doesn’t quite succeed: while the margin histograms are less
separated than for the undistilled networks in Figure 3a, they are still visibly separated unlike the
post-distillation histograms in Figure 3b.
Figure 4:	random labels. A standard sanity check for generalization bounds is whether they can
reflect the difficulty of fitting random labels (Zhang et al., 2016). While it has been empirically
shown that Rademacher bounds do sharply reflect the presence of random labels (Bartlett et al.,
2017a, Figures 2 & 3), the effect is amplified with distillation: even randomizing just 25% shrinks
the margin distribution significantly.
3 Analysis overview and sketch of proofs
This section sketches all proofs, and provides further context and connections to the literature. Full
proof details appear in the appendices.
3.1	Abstract data augmentation bounds in Section 1.1
As mentioned in Section 1.1, the first step of the proof is to apply Lemma A.2 to obtain
Prχ,y[argmaxf (x)y0 = y] ≤ 2Eχ∣%(f (x)) - φγ(g(x))∣l + 2Eχ,y(1 - Φγ(g(x))y)；
y0
this step is similar to how the ramp loss is used with margin-based generalization bounds, a connec-
tion which is discussed in Appendix B.
Section 1.1 also mentioned that the last term is easy: φγ is (1∕γ)-Lipschitz, and We can peel it off
and only pay the Rademacher complexity associated with g ∈ G .
With data augmentation, the first term is also easy:
EΦγ,m(f,g) = / kφγ(f (z)) - φγ(g(z))kι dμχ(Z) = / kφγ(f (z)) - φγ(g(z))kιdμχ dνn(z)
kφγ(f(z)) - φγ(g(z))k1dνn(z),
and now we may apply uniform convergence to Vn rather than μχ. In the appendix, this proof is
handled With a bit more generality, alloWing arbitrary norms, Which may help in certain settings. All
together, this leads to a proof of Lemma 1.1.
For the explicit data augmentation estimate in Lemma 1.2, the proof breaks into roughly two cases:
low density regions where the uniform sampling gives the bound, and high density regions where
the Gaussian sampling gives the bound. In the latter case, the Gaussian sampling in expectation
behaves as a kernel density estimate, and the proof invokes a standard bound (Jiang, 2017).
3.2	Concrete data augmentation bounds in Section 1.2
The main work in this proof is the following generalization bound for computation graphs, which
follows the proof scheme from (Bartlett et al., 2017a), though simplified in various ways, owing
mainly to the omission of general matrix norm penalties on weight matrices, and the omission of the
reference matrices. The reference matrices were a technique to center the weight norm balls away
from the origin; a logical place to center them was at initialization. However, in this distillation
setting, it is in fact most natural to center everything at the origin, and apply regularization and
shrink to a well-behaved function (rather than shrinking back to the random initialization, which
after all defines a complicated function). The proof also features a simplified (2, 1)-norm matrix
covering proof (cf. Lemma C.3).
Lemma 3.1. Let data X ∈ Rn ×d be given. Let computation graph G be given, where ∏i projects
to Frobenius-norm balls of radius bi√n, and 口卬/g」≤ r, and k [Wi∏iDiiFi]k2 ≤ Si, and
Lip(σi ) ≤ ρi, and all layers have width at most m. Then for every > 0 there exists a covering set
M satisfying
sup min ∣∣g(XT) — XIl ≤ C
g∈G X∈Ml1	11
24/3n ln(2m2)
and ln |M| ≤----------J------
C2
L	2/3
ribiρi	slρl
i	l=i+1
3
8
Published as a conference paper at ICLR 2021
Consequently,
Rad(G) ≤ 4 + 12产m)
nn
L	2/3
ribiρi	slρl
i	l=i+1
3/2
From there, the proof of Theorem 1.3 follows via Lemmas 1.1 and 1.2, and many union bounds.
3.3	Direct uniform convergence approach in Theorem 1.4
As mentioned before, the first step of the proof is to sparsify the network, specifically each matrix
product. Concretely, given weights Wi of layer i, letting XiT-1 denote the input to this layer, then
m
WiXiT-1 = X(Wiej)(Xi-1ej)T.
j=1
Written this way, it seems natural that the matrix product should “concentrate”, and that considering
all m outer products should not be necessary. Indeed, exactly such an approach has been followed
before to analyze randomized matrix multiplication schemes (Sarlos, 2006). As there is no goal of
high probability here, the analysis is simpler, and follows from the Maurey lemma (cf. Lemma C.1),
as is used in the (2, 1)-norm matrix covering bound in Lemma C.3.
Lemma 3.2. Let a network be given with 1-Lipschitz homogeneous activations σi and weight ma-
trices (W1, . . . , WL) of maximum width m, along with data matrix X ∈ Rn×d and desired widths
(k1, . . . , kL) be given. Then there exists a sparsified network output, recursively defined via
XT := Xt, and XT =∏Ni(WiMiXLι), where Mi := X ZejeT,
0	i	i-1	j∈Si kAejk
where Si is a multiset of ki = |Si| indices, Πi denotes projection onto the Frobenius-norm ball of
radius ∣∣X∣∣f IIj≤i k Wjk2, and the scaling term Zj satisfies Zj ≤ ∣∣ Wk ∣∣f/m/kj, and
L
∣σL(WL …σ1(W1XT)…)-XLkF ≤ ∣X∣f Y ∣Wi∣2
i=1
X HWE
沁 kikWik2 ,
The statement of this lemma is lengthy and detailed because the exact guts of the construction are
needed in the subsequent generalization proof. Specifically, now that there are few nodes, a gener-
alization bound sensitive to narrow networks can be applied. On the surface, it seems reasonable to
apply a VC bound, but this approach did not yield a rate better than n-1/6, and also had an explicit
dependence on the depth of the network, times other terms visible in Theorem 1.4.
Instead, the approach here, aiming for a better dependence on n and also no explicit dependence on
network depth, was to produce an ∞-norm covering number bound (see (Long & Sedghi, 2019) for a
related approach), with some minor adjustments (indeed, the ∞-norm parameter covering approach
was applied to obtain a Frobenius-norm bound, as in Lemma 3.1). Unfortunately, the magnitudes
of weight matrix entries must be controlled for this to work (unlike the VC approach), and this
necessitated the detailed form of Lemma 3.2 above.
To close with a few pointers to the literature, as Lemma 3.2 is essentially a pruning bound, it is
potentially of independent interest; see for instance the literature on lottery tickets and pruning
(Frankle & Carbin, 2019; Frankle et al., 2020; Su et al., 2020). Secondly, there is already one
generalization bound in the literature which exhibits spectral norms, due to (Suzuki et al., 2019);
unfortunately, it also has an explicit dependence on network width.
Acknowledgments
MT thanks Vaishnavh Nagarajan for helpful discussions and suggestions. ZJ and MT are grateful
for support from the NSF under grant IIS-1750051, and from NVIDIA under a GPU grant.
9
Published as a conference paper at ICLR 2021
References
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-
bridge University Press, 1999.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. 2018. arXiv:1802.05296 [cs.LG].
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In NIPS,pp. 6240-6249, 2017a.
Peter L. Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. 2017b. arXiv:1703.02930
[cs.LG].
Cristian BUcilUU, Rich Caruana, and Alexandru NicUlescU-MiziL Model compression. In KDD, pp.
535-541, 2006.
Cody A. Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, LUigi Nardi, Peter
Bailis, Kunle Olukotun, Chris Re, and Matei Zaharia. Dawnbench: An end-to-end deep learning
benchmark and competition. In NIPS ML Systems Workshop, 2017.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds
for deep (stochastic) neural networks with many more parameters than training data. 2017.
arXiv:1703.11008 [cs.LG].
Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero,
Linbo Wang, Ioannis Mitliagkas, and Daniel M. Roy. In search of robust measures of generaliza-
tion. In NeurIPS, 2020.
Dylan J. Foster and Alexander Rakhlin. '∞ vector contraction for rademacher complexity. 2019.
arXiv:1911.06468 [cs.LG].
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. 2019. arXiv:1803.03635 [cs.LG].
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Pruning neu-
ral networks at initialization: Why are we missing the mark? 2020. arXiv:2009.08576
[cs.LG].
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In COLT, 2018.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. 2015.
arXiv:1503.02531 [stat.ML].
Heinrich Jiang. Uniform convergence rates for kernel density estimation. In ICML, 2017.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generaliza-
tion gap in deep networks with margin distributions. In ICLR, 2019a. arXiv:1810.00113
[stat.ML].
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. 2019b. arXiv:1912.02178 [cs.LG].
Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
2019. arXiv:1905.12600 [cs.LG].
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. 2019. arXiv:1902.04742 [cs.LG].
Gilles Pisier. Remarques sur un resultat non publie de b. maurey. Seminaire Analysefonctionnelle
(dit), pp. 1-12, 1980.
10
Published as a conference paper at ICLR 2021
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
FOCS, pp. 143-152, 11 2006.
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D.
Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. 2020.
arXiv:2009.11094 [cs.LG].
Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura. Compression based bound for non-compressed
network: unified generalization error analysis of large compressible deep neural network. 2019.
arXiv:1909.11274 [cs.LG].
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. 2019. arXiv:1905.03684 [cs.LG].
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2016. arXiv:1611.03530 [cs.LG].
11
Published as a conference paper at ICLR 2021
A Proofs for Section 1.1
The first step is an abstract version of Lemma 1.1 which does not explicitly involve the softmax, just
bounded functions.
Lemma A.1. Let classes of bounded functions F and G be given with F 3 f : X → [0, 1]k and
G 3 g : X → [0, 1]k. Let conjugate exponents 1/p + 1/q = 1 be given. Then with probability at
least 1 一 2δ over the draw of ((xi, yi))n=ι from μ and (zi)m=I from Vn ,for every f ∈ F and g ∈ G,
1n
Ef(x)y ≤ n Xg(xi)yi + 2Radn ({(χ,y) → g(χ)y : g ∈ G
ln(1∕δ)
2n
i=1
+	( -X kf (Zi)- g(zi)kp+：，rIn^
Lq(νn) m i=1	p	2m
+ 2Radm nz 7→ min{1, kf(z) - g(z)kpp} : f ∈ F,g ∈ Go
where
Radm	z 7→ min{1, kf(z) - g(z)kpp} : f ∈ F,g ∈ G
k
≤ P X [Radm({z → f (z)y0 : f ∈ F}) + Radm({z → g(z)y0 : g ∈ G})].
y0=1
Proof of Lemma A.1. To start, for any f ∈ F and g ∈ G, write
Ef (x)y = E(f (x) -g(x))y +Eg(x)y.
The last term is easiest, and let’s handle it first: by standard Rademacher complexity arguments
(Shalev-Shwartz & Ben-David, 2014), with probability at least 1 - δ, every g ∈ G satisfies
1n
Eg(x)y ≤ - fg(Xi)yi + 2Radn({(x,y) → g(x)y : g ∈G}) + 3
n i=1
Jln(1∕δ)
V	2n
For the first term, since f : X → [0,1]k and g : X → [0,1]k, by Holder’s inequality
E(f(x) - g(x))y
/ min{1, (f(x) ― g(x))y} dμ(x,y)
≤ / min{1,kf(x)- g(x)kp} dμ(x,y)
=Z min{1, kf(x)-g(x)kp}d-μX(x)dνn(x)
n
≤Min {1,kf -gkp}∣∣Lp0j dμn∣Lq (νn).
Once again invoking standard Rademacher complexity arguments (Shalev-Shwartz & Ben-David,
2014), with probability at least 1 - δ, every mapping z 7→ min{1, kf(z) - g(z)kpp} where f ∈ F
and g ∈ G satisfies
Z min{1, kf (z)—g(z)kp} dνn(Z) ≤ mm X min{1, kf (Zi)- g(Zi)kp}+ɪln(mɪ
i=1
+ 2Radm	Z 7→ min{1, kf(Z) - g(Z)kpp} : f ∈ F,g ∈ G
Combining these bounds and unioning the two failure events gives the first bound.
12
Published as a conference paper at ICLR 2021
For the final Rademacher complexity estimate, first note r 7→ min{1, r} is 1-Lipschitz and can be
peeled off, thus
mRadm	z 7→ min{1, kf(z) - g(z)kpp} : f ∈ F,g ∈ G
≤ mRadm	z 7→ kf(z) - g(z)kpp : f ∈ F,g ∈ G
m
E sup Xikf(zi) - g(zi)kpp
fg∈∈FG i=1
km
≤ E sup	i|f(zi) - g(zi)|yp0
y0=1	fg∈∈FG i=1
k
X mRadm	z 7→ |f(z) - g(z)|yp0 : f ∈ F,g ∈ G
y0=1
Since f and g have range [0, 1]k, then (f - g)y0 has range [-1, 1] for every y0, and since r 7→ |r|p
is p-Lipschitz over [-1, 1] (for any p ∈ [1, ∞), combining this with the Lipschitz composition
rule for Rademacher complexity and also the fact that a Rademacher random vector ∈ {±1}m is
distributionally equivalent to its coordinate-wise negation -, then, for every y0 ∈ [k],
Radm({z 7→ |f (z) - g(z)|yp0 : f ∈ F,g ∈ G})
≤ pRadm({z 7→ (f (z) - g(z))y0 : f ∈ F, g ∈ G})
m
=Ee SuPsuPT Ei (f (Zi)-目^))#，
m	f∈F g∈G i=1
mm
=—Ee suP y^eif (Zi)y0 + ~ Ee suP	-Eig(Zi)y0
m f∈F i=1	m g∈G i=1
= pRadm({z 7→ f (z)y0 : f ∈ F}) +pRadm({z 7→ g(z)y0 : g ∈ G}).
□
To prove Lemma 1.1, it still remains to collect a few convenient properties of the softmax.
Lemma A.2. For any v ∈ Rk and y ∈ {1, . . . , k},
2(1 - φγ (v))y ≥ 1[y 6= argmaxvi].
i
Moreover, for any functions F with F 3 f : X → Rk,
Radn ({(x,y) → φγ(f(χ))y : f ∈ F}) = O 呼Radn(F)).
Proof. For the first property, let v ∈ Rk be given, and consider two cases. If y = arg maxi vi , then
φγ(v) ∈ [0, 1]k implies
2 1 - φγ (v)	≥ 0 = 1[y 6= arg max vi].
yi
On the other hand, if y 6= arg maxi vi, then φγ(v)y ≤ 1/2, and
2 1 - φγ (v) y ≥ 1 = 1[y 6= arg max vi].
The second part follows from a multivariate Lipschitz composition lemma for Rademacher com-
plexity due to (Foster & Rakhlin, 2019, Theorem 1); all that remains to prove is that v 7→ φγ(v)y is
(1 /γ) -LiPschitz with respect to the '∞ norm for any V ∈ Rk and y ∈ [k]. To this end, note that
d / /_ eXP(V∕γ% Pj=y eχp(v∕γ)j	d λ f ∖ -	exp(v∕γ)y exp(v∕γ)i
dVyφγ(V)y =	Y(Pj exp(v∕γ)j )2,	dVi=；")(V)y = - Y(Pj exp(v∕γ))2 ,
13
Published as a conference paper at ICLR 2021
and therefore
∣Wφγ (V)y ∣∣ι
2 eχp(V∕γ)y Ej=y exP(V∕γ )
γ( j exp(V∕γ)j )2
≤ 1,
γ
and thus, by the mean value theorem, for any u ∈ Rk and V ∈ Rk, there exists z ∈ [u, V] such that
∖φγ(v)y — φγ(u)y∣ =∣(Vφγ(Z)y,v — u)∣ ≤ kv — uk∞ ∙∣∣Vφγ(V)y I∣1 ≤ 1kv — uk∞,
and in particular V → φγ(v)∕y is (1∕γ)-Lipschitz with respect to the '∞ norm. Applying the
aforementioned Lipschitz composition rule (Foster & Rakhlin, 2019, Theorem 1),
Radn ({(x, y) → φγ(f (x))y : f ∈ F}) = Oe ( -^-Radn(F)).
□
Lemma 1.1 now follows by combining Lemmas A.1 and A.2.
Proof of Lemma 1.1. Define ψ := 1 — φγ. The bound follows by instantiating Lemma A.1 with
p = 1 and the two function classes
QF := {(X,y) 7→ ψ(f(X)y) : f ∈ F} and QG := {(X, y) 7→ ψ(g(X)y) : g ∈ G},
combining its simplified Rademacher upper bounds with the estimates for Radm(QF) and
Radm(QG) and Radn(QG) from Lemma A.2, and by using Lemma A.2 to lower bound the left
hand side with
Eψ(f(x))y = E(1 - φγ(f(x))y) ≥ 2IhargmaX f (x)y，= y],
and lastly noting that
mm
mm X kΨ(f (Zi)) — Ψ(g(Zi))kι = m X ∣1 — Φγ(f (Zi)) — 1 + Φγ(g(Zi))kι = Φγ,m(f,g).
□
To complete the proofs for Section 1.1, it remains to handle the data augmentation error, namely the
term ∣∣dμχ∕dνn∣∞. This proof uses the following result about GaUSSian kernel density estimation.
Lemma A.3 (See (Jiang, 2017, Theorem 2 and Remark 8)). Suppose density P is α-Holder Contin-
uous, meaning |p(x) — p(x0)| ≤ Cα∣x — x0∣α for some Cα ≥ 0 and α ∈ [0, 1]. There there exists
a constant C ≥ 0, depending on α, Cα, maxx∈Rd p(x), and the dimension, but independent of the
sample size, so that with probability at least 1 — 1∕n, the Gaussian kernel density estimate with
bandwidth σ21 where σ = n-1/(2a+d) satisfies
XuPd Ip(X)- Pn(X)ι≤ CLanTd).
The proof of Lemma 1.2 follows.
Proof of Lemma 1.2. The proposed data augmentation measure νn has a density pn,β over [0, 1]d,
and it has the form
pn,β (X) = β + (1— β)pn(X),
where β = 1∕2, and pn is the kernel density estimator as described in Lemma A.3, whereby
Ipn(X)- P(X)I ≤ En := O (n√θ+d)).
The proof proceeds to bound ∣∣dμχ∕dνnk∞ = ∣∣p∕pn,β ∣∣∞ by considering three cases.
14
Published as a conference paper at ICLR 2021
•	If x ∈ [0,1]d, then p(x) = 0 by the assumption on the support of μx, whereas pn,β(x) ≥
Pn (x)∕2 > 0, thus p(x)∕pn,β (x) = 0.
•	Ifx ∈ [0, 1]d and p(x) ≥ 2n, thenpn,β(x) ≥ (1 - β)p(x) - n) ≥ n/2, and
P(X) = 1 + P(X) - Pn,β(x)
Pn,β (X)	Pn,β (X)
≤ 1 + βP(χ) + (1 - β)∣P(χ) - Pn(x)l
一	Pn,β(X)	Pn,β(X)
≤ 1 +	βP(X)	+ (I -户底
≤ + (1- β)(P(X)-en) +	en/2
β
W + (1 - e)(1 - en∕P(X))十
≤ 4.
•	IfX ∈ [0, 1]d and P(X) < 2n, since Pn,β (X) ≥ β = 1∕2, then
P(X) < 强=4
Pne(X) < 丁=n.
Combining thesecases, kdμX∕dνnk∞ = ∣∣p∕pn,β∣∣∞ ≤ max{4,4g} ≤ 4 + 4^.
□
B Replacing softmax with s tandard margin (ramp) loss
The proof of Lemma 1.1 was mostly a reduction to Lemma A.1, which mainly needs bounded
functions; for the Rademacher complexity estimates, the Lipschitz property of φγ was used. As such,
the softmax can be replaced with the (1∕γ)-Lipschitz ramp loss as is standard from margin-based
generalization theory (e.g., in a multiclass version as appears in (Bartlett et al., 2017a)). Specifically,
define Mγ : Rk → [0, 1]k for any coordinate j as
[1 Z ≤ 0,
MY(v)j := 'γ(Vj - arg max Vyo),	where 'γ(z) :=〈 1 - Y Z ∈ (0,γ),
y0=j	∣0 z ≥ γ.
We now have 1[arg maxy0 f (X)y0] ≤ MY (f (X))y without a factor of 2 as in Lemma A.2, and can
plug it into the general lemma in Lemma A.1 to obtain the following corollary.
Corollary B.1. Let temperature (margin!) parameter γ > 0 be given, along with sets of multiclass
predictors F and G. Then with probability at least 1 - 2δ over an iid draw of data ((Xi, yi))in=1 from
μ and (zi)n=ι from Vn, every f ∈ F and g ∈ G satisfy
Pr[argmaχf(X)y0 = y] ≤∣lddμx∣∣	— X IlMY(f) -Mγ(g)∣∣ι + 1 XMY(g(Xiy)yi
y0	dνn ∞ m i=1	n i=1
+ Oe
∞(Radm(F)+Radm (G)) TRadn(G)
+ 3*1+	∞
Proof. Overload function composition notation to sets of functions, meaning
MY ◦F = {(X,y) → MY(f(X))y : f ∈ F}.
First note that MY is (2∕γ)-Lipschitz with respect to the '∞ norm, and thus, applying the multivari-
ate Lipschitz composition lemma for Rademacher complexity (Foster & Rakhlin, 2019, Theorem 1)
just as in the proof for the softmax in Lemma A.2,
一 ,.. 一、 ≈
Radm(MY ◦ F) = O
Radm(F)
15
Published as a conference paper at ICLR 2021
with similar bounds for Radm(Mγ ◦ G) and Radn(Mγ ◦ G). The desired statement now follows by
combining these Rademacher complexity bounds with Lemma 1.1 applied to Mγ ◦ F and Mγ ◦ G,
and additionally using 1 [arg maxyo f (x)yo = y] ≤ MY(f (x))y.	□
C S ampling tools
The proofs of Lemma 3.1 and Lemma 3.2 both make heavy use of sampling.
Lemma C.1 (Maurey (Pisier, 1980)). Suppose random variable V is almost surely supported on
a subset S of some Hilbert space, and let (V1, . . . , Vk) be k iid copies of V. Then there exist
(Vι,...,V^k) ∈ Sk with
2
EV - 1 XVi
i
≤ V1,.E..,Vk
2
EV -1X ¾	= 1 [Ek Vk2-kEV∣∣2]
i
F
≤1 EkV k2 ≤1 VuSkV k2.
F
Proof of Lemma C.1. The first inequality is via the probabilistic method. For the remaining inequal-
ities, by expanding the square multiple times,
E
V1,...,Vk
ev -1X Vi
i
2
1
≤ E	T2
V1 ,...,Vk k2
F
kEV-VikF2+ EV-Vi,EV-Vj
1 EVIkVI- EVk2	= 1	[Ek V∣∣2	-∣∣EVk2i	≤ 1 EkVk2	≤ 1 SUp k Vk2.
k	k	L	」k	k V∈s
□
A first key application of Lemma C.1 is to sparsify products, as used in Lemma 3.2.
Lemma C.2. Let matrices A ∈ Rd×m and B ∈ Rn×m be given, along with sampling budget k.
Then there exists a selection (i1, . . . , ik) of indices and a corresponding diagonal sampling matrix
M with at most k nonzero entries satisfying
M :=噌 X jy	and	MBT- AMB	≤ IkAk2kBk2∙
Proof of Lemma C.2. For convenience, define columns ai := Aei and bi := Bei for i ∈
{1, . . . , m}. Define importance weighting βi := (kaik/kAkF)2, whereby Pi βi = 1, and let V
be a random variable with
Pr V=βi-1aibiT =βi,
whereby
mm	m
EV=Xβi-1aibiTβi = X(Aei)(Bei)T = A XeieiT BT=A[I]BT=AB,
i=1	i=1	i=1
mm	m
EkVk2 = Xβ-2kaibTk2βi = Xβ-1kaik2kbik2 = X kA∣∣2kbi∣∣2 = ∣∣A∣∣2 ∙ kBk2.
i=1	i=1	i=1
By Lemma C.1, there exist indices (iι,...,ik) and matrices Vj := β-1aj bτ With
2 2
ABt - 1 X Vj	≤ EV - 1 X V = IhkAk2kBk2-kABk2i ≤ 1 |闾2|回2.
k k k k
j j
To finish, by the definition of M,
1 X Vj = 1 Xβ- 1(Aej)(Bej)T = A 1 Xβ- 1ejeɪ BT = A [M] BT.
jj	j
□
16
Published as a conference paper at ICLR 2021
A second is to cover the set of matrices W satisfying a norm bound kWTk2,1 ≤ r. The proof here
is more succinct and explicit than the one in (Bartlett et al., 2017a, Lemma 3.2).
Lemma C.3 (See also (Bartlett et al., 2017a, Lemma 3.2)). Let norm bound r ≥ 0,X ∈ Rn×d, and
integer k be given. Define a family of matrices
M := ∖ rkXkF XX SleileTl : Sl ∈{±1},iι ∈{1,...,n},jι ∈{1,...,d}1.
[k	l=l IIXejι k
Then
|M| ≤ (2nd),	SUp min ∣∣WXT — WXTk2 ≤ r kXkF .
∣∣W Tk2,ι≤rW ∈M	k
Proof. Let W ∈ Rm×d be given with kWTk2,1 ≤ r. Define sij := Wij/|Wij |, and note
WXT = XeieTW jXT = XeiWj(Xej)T = X JWjkXejk ”㈤废：i(Xej厂.
rk kF	k ej k
ij	ij	ij ×-----{-----，------Z-------}
=:qij	=:Uij
Note by Cauchy-Schwarz that
XX qj≤ r∣⅛ XX 尸 kXkF
i,j	i j
|WTk2,lkXkF ≤ I
r∣X∣F ≤ ,
potentially with strict inequality, thus q is not a probability vector. To remedy this, construct proba-
bility vector p from q by adding in, with equal weight, some Uij and its negation, so that the above
summation form of WXT goes through equally with p and with q.
Now define iid random variables (V1, . . . , Vk), where
Pr[Vl = Uij] = pij ,
EVl=X pijUij =X qij Uij =WXT,
i,j	i,j
∣Ujk = Sjjei(Xej)	∙ r∣X∣F = MHIeik2 ∙ iiXejr-	∙ r∣X∣F = r∣X∣f
kX ej k2	kX ej k2
F2
EkVlk2 = X pijkUijk2 ≤X pijr2kXkF2 = r2kXkF2.
i,j	ij
By Lemma C.1, there exist (V1, . . . , Vk) ∈ S k with
WXT- kXV
l
2
≤E
EVi- k X V
k
l
2
≤ kEkVik2 ≤ r2kXk2
kk
Furthermore, the matrices V have the form
k X V = k X SIeIiI(X ej∣)T
k 彳 l = k 彳	kXej∣k
k X sl ei∣ eT∣
k 彳 IIXejι k
XT =： W X t,
where W ∈ M. Lastly, note |M| has cardinality at most (2nd)k.
□
D Proofs for Section 1.2
The bulk of this proof is devoted to establishing the Rademacher bound for computation graphs
in Lemma 3.1; thereafter, as mentioned in Section 3, it suffices to plug this bound and the data
augmentation bound in Lemma 1.2 into Lemma 1.1, and apply a pile of union bounds.
As mentioned in Section 3, this proof follows the scheme laid out in (Bartlett et al., 2017a), with
simplifications due to the removal of “reference matrices” and some norm generality.
17
Published as a conference paper at ICLR 2021
Proof of Lemma 3.1. Let cover scale and per-layer scales (1 , . . . , L) be given; the proof will
develop a covering number parameterized by these per-layer scales, and then optimize them to derive
the final covering number in terms of . From there, a Dudley integral will give the Rademacher
bound.
Define b := b√n for convenience. As in the statement, recursively define
XT := Xt,	XT := σi ([Wi∏iDiiFi]χT-i).
The proof will recursively construct an analogous cover via
XT := Xt,	XT := σ (Wi∏iDiiFi]XT-J ,
where the choice of Wi depends on Xi-1, and thus the total cover cardinality will product (and not
simply sum) across layers. Specifically, the cover Ni for Wi is given by Lemma C.3 by plugging in
k∏iDiXi-1 ∣∣f ≤ bi, and thus it suffices to choose
^2b2
r
cover cardinality k := -LiL,	whereby mm kWi∏iDiXi-ι - WiΠiDiXT-IIl ≤ G∙
Ei	Wi∈Ni
By this choice (and the cardinality estimate in Lemma C.3, the full cover N satisfies
r2b
ln |N| = Eln |Ni| ≤E 整 ln(2m2).
Ei
i	ii
To optimize the parameters (E1, . . . , EL), the first step is to show via induction that
i
kXT - XTkF ≤ XEjPj Y SlPl.
j≤i l=j+1
The base case is simply ∣∣XT - XTk = ∣∣XT - XTk = 0, thus consider layer i > 0. Using the
♦	1	Γ∙	1 Γ∙	1 .1	.	TTT-
inductive formula for Xi and the cover guarantee on Wi ,
XT - XT∣∣ =g([Wi∏iDiiFi]XT-ι) - σi(Wi∏iDiiFi]XT-ι)∣∣
≤ PiIkWiniDiiFi]hXT-ι - [Wi∏iDiiFi]XT-J∣
≤	Pi∣∣[Wi∏iDiiFi]XT-ι -	[Wi∏iDiiFi]XT-ι∣∣ +	Pi∣[Wi∏iDiiFi]XT-ι -	[Wi∏iDiiFi]XT-ι∣∣
≤	Pi∣∣[Wi∏iDiiFi]∣∣2∣∣XT-ι - XT-ι∣∣ +	Pi∣∣[(Wi	- Wi)∏iDiXT-ιi(Fi	-	Fi)XT-1]∣∣
i-1
≤ SiPi	EjPj
j≤i-1	l=j+1
SlPl+Pi∣∣(Wi - Wi)∏iDiXτ-ι∣∣
ii
≤	EjPj	SlPl + PiEi ≤	EjPj	SlPl .
j≤i-1	l=j+1	j≤i	l=j+1
To balance (E1, . . . , EL), it suffices to minimize a Lagrangian corresponding to the cover size subject
to an error constraint, meaning
L
where ai := r2b2 ln(2m2), βi := Pi ɪɪ SlPl,
l=i+1
whose unique critical point for ~E > 0 implies the choice
1
Z
where Z := 1 X(2ɑiβ2)1/3,
18
Published as a conference paper at ICLR 2021
1	1	I I-ʌʃ U ，	.	. ∙ 11	1
whereby kXLT - XLT k ≤ automatically, and
ln ∣N∣≤ Z Zf
i
r2b2 ln(2m2)
(2αi∕βi)2/3
小 2 X	飞/步/ ln(2m2)1/3
i
2
X r2/3b2/3 ln(2m2)1∕3βy3
i
24/3 ln(2m2)
^2
L	2∕3 3
E ribiPi ∏ SiPi
i	l=i+1
τ2
ɪ2,
as desired, with τ introduced for convenience in what is to come.
For the Rademacher complexity estimate, by a standard Dudley entropy integral (Shalev-Shwartz &
Ben-David, 2014), setting T := max{τ, 1/3} for convenience,
nRad(G ) ≤
inf 4Z√n+12T ln(c)∣t√n = inf 4《√n+12T(ln √n-lnZ),
which is minimized at Z = 3T/√n, whereby
nRad(G) ≤ 12T + 6Tlnn - 12Tln(3T∕√n) = 12T(1 - ln(3T)) ≤ 12T ≤ 12τ + 4.
□
This now gives the proof of Theorem 1.3.
Proof of Theorem 1.3. With Lemma 1.1, Lemma 1.2, and Lemma 3.1 out of the way, the main work
of this proof is to have an infimum over distillation network hyperparameters (~b, ~r, ~s) on the right
hand side, which is accomplished by dividing these hyperparameters into countably many shells,
and unioning over them.
In more detail, divide (b, ~r, ~s) into shells as follows. Divide each bi and ri into shells of radius
increasing by one, meaning meaning for example the first shell for bi has bi ≤ 1, and the jth
shell has bi ∈ (j - 1, j], and similarly for ri; moreover, associate the jth shell with prior weight
qj(bi) := (j(j + 1))-1, whereby Pj≥1 qj(bi) = 1. Meanwhile, for si use a finer grid where the
first shell has si ≤ 1/L, and the jth shell has si ∈ ((j - 1)/L, j/L), and again the prior weight is
qj(si) = (j(j + 1))-1. Lastly, given a full set of grid parameters (~b, ~r, ~s), associate prior weight
q(b, ~r, ~s) equal to the product of the individual prior weight, whereby the sum of the prior weights
over the entire product grid is 1. Enumerate this grid in any way, and define failure probability
δ(~, r, s) := δ ∙ q(~, r, S).
Next consider some fixed grid shell with parameters (~b0, ~r0, ~s0) and let H denote the set of networks
for which these parameters form the tightest shell, meaning that for any g ∈ H with parameters
(~b, ~r, ~s), then (~b0, ~r0, ~s0) ≤ (~b + 1, ~r + 1, ~s + 1) component-wise. As such, by Lemma 1.1, with
probability at least 1 - δ(~b0, ~r0, ~s0), each g ∈ H satisfies
Pr[arg max f (x)y0 6= y] ≤ 2
y0
+ Oe ( γ U dV U (Radm(F) + Radm(H)) + -^-Radn(H)
+ 6 jln3~0,~0,~0)) + ln(I∕δ) (1 +
ʌ/-
∞m
19
Published as a conference paper at ICLR 2021
To simplify this expression, first note by Lemma 3.1 and the construction of the shells (relying in
particular on the finer grid for si to avoid a multiplicative factor L) that
Radm (H) = O
= Oe
= Oe
Xi
L
ri0b0iρi	s0lρl
l=i+1
2/3	3/2
L	2/3
1n (X kri + 1)(bi + 1)Pi Y (Sl + 1∕L)P1
3/2
L	2/3
ribiρi	slρl
l=i+1
3/2
and similarly for Radm(H) (the only difference being √m replaces √n). Secondly, to absorb the
term ln(q(~0, ~0, s0)), noting that ln(a) ≤ ln(γ2) + (a - γ2)∕(γ2),and also using Pi ≥ 1, then
ln(q(~,~0,~)) = O (lnY(~i + 1)2(bi + 1)2((si + 1)L)2J =O(LlnL + lnY~2/W/S2/
O(L + X ln(~2/3b2/3)+ln Y s2/3
ii
I	-
O L + ln(Υ2) + g X ~2/3b2/3 + Y s2/3
γ i	l>i
Together,
L + ⅛Σ ~ibi ∏
l=i+1
2/3
sl
2/3
O L +j2 X TibiPi Y SlPl
γ i	l=i+1
Pr[arg max f (x)
y0
O
L
Since h ∈ H was arbitrary, the bound may be wrapped in infg∈H. Similarly, unioning bound-
ing away the failure probability for all shells, since this particular shell was arbitrary, an infimum
over shells can be added, which gives the final infimum over (b, ~r, ~S). The last touch is to apply
Lemma 1.2 to bound ∣∣dμχ∕dνn∣∣∞.	□
E	Proof of stable rank bound, Theorem 1.4
The first step is to establish the sparsification lemma in Lemma 3.2, which in turn sparsifies each ma-
trix product, cannot simply invoke Lemma C.2: pre-processing is necessary to control the element-
wise magnitudes of the resulting matrix. Throughout this section, define the stable rank of a matrix
W assr(W) := ∣W ∣F2 ∕∣W ∣22 (or0whenW = 0).
20
Published as a conference paper at ICLR 2021
Lemma E.1. Let matrices A ∈ Rd×m and B ∈ Rn×m be given, along with sampling budget k.
Then there exists a selection (i1, . . . , ik) of indices and a corresponding diagonal sampling matrix
M with at most k nonzero entries satisfying
ʌ ZijeijeT	m
M := X 卡亍	ZiUkAkF √f,
and	IIABT - AMBTIl2 ≤ 4|团|2|田『.
k
Proof. Let τ > 0 be a parameter to be optimized later, and define a subset of indices S := {i ∈
{1, . . . ,m} : kAeik ≥ τ}, with Sc := {1, . . . ,m} \ S. Let Aτ denote the matrix obtained by
zeroing out columns not in S, meaning
Aτ :=	(Aei)eiT,
i∈S
whereby
kABT - ATBTkF ≤kA — Aτk∙kBk ≤ |网
y∑ kAeik2
≤ τ√m∣∣Bk.
Applying Lemma C.2 to Aτ BT gives
M .= kATk2 X eijeT = XX ZjeijeT
:= k	j=1 kAτ eTk2 = j=1 kAτ eTk
such that	∣∣AτBT-ATMBTk2 ≤ 1 ∣∣Aτk2kBk2,
k
where Zij is specified by these equalities. To simplify, note kAT k ≤ kAk, and ATM = AM.
Combining the two inequalities,
∣ABt — AMBTk ≤ kABT — ATBTk + ∣AτBT- ATMBTk ≤ T√m∣Bk + 专∣A∣kB∣.
To finish, setting τ :
for any ij ∈ S,
∣∣Ak∕√mk gives the bound, and ensures that the scaling term Zij satisfies,
Z	kATk2 < kAk2 TA m
Zj = kpTeij ≤ 亏= kAkFV ⅛.
□
With this tool in hand, the proof of Lemma 3.2 is as follows.
Proof of Lemma 3.2. Let Xj denote the network output after layer j, meaning
X0T := XT,	XjT := σj (Wj XjT-1),
whereby
kXjTkF=kσj(WjXjT-1)-σj(0)kF≤ kWjXjT-1kF≤kWjk2kXjT-1kF≤ kXkFYkWik2.
i≤j
The proof will inductively choose sampling matrices (M1, . . . , ML) as in the statement and con-
struct
X(T := xt,	XJ := Π∙σ∙(WjMjXj-ι),
where Πj denotes projection onto the Frobenius-norm ball of radius kXkF Qi≤j kWik2 (whereby
Πj Xj = Xj ), satisfying
j	j I ∕ττ-τ X
IlXj-XjIL ≤kχ∣f YkWpk2 XsW,
p=1	i=1	i
which gives the desired bound after plugging in j = L.
21
Published as a conference paper at ICLR 2021
Proceeding with the inductive construction, the base case is direct since X。= X = Xo and
∣∣Xo - Xo J] = 0, thus consider some j > 0. Applying Lemma E.1 to the matrix multiplication
WjXj-1 with kj samples, there exists a multiset of Sj coordinates and a corresponding sampling
matrix Mj , as specified in the statement, satisfying
∣∣ Wj XT-I- Wj Mj Xj-J] ≤ Pk= HWjkFkXjTkF ≤ Pk= HWj HFkXkF Y HWik2 ∙
Using the choice XT ：= ∏jσj (WjMjXT-1),
∣∣Xj- XjILT∣σj(WjXj-I)- Πjσj(WjMjXT-I)IL
≤∣∣WjXj-ι- WjMjXT"]
=∣∣WjXT-I- Wj XT-1 + Wj XT-1 - Wj Mj Xj-1∣
≤ ∣∣WjXT-I- Wj XT"1+∣∣Wj xt-i - Wj Mj XT"]
≤∣∣Wj∣∣2∣∣Xj-1- xj-ι∣ + Pr kWj kFkχ kF YkWik2
j	i<j
sr(Wj^ kX kF YkWik2
j	i≤j
≤ kχkF YkWik2 X∣WW)
i≤j	i≤j	ki
as desired.
□
To prove Theorem 1.4 via Lemma 3.2, the first step is a quick tool to cover matrices element-wise.
Lemma E.2. Let A denote matrices with at most k2 nonzero rows and k1 nonzero columns, entries
bounded in absolute value by b, and total number of rows and columns each at most m. Then there
exists a cover set M ⊆ A satisfying
/ k-------∖ k1k2
|M| ≤ mk1+k2 j ——11—L j , and SuP min ∣∣A — AkF ≤ e.
∖ e )	a∈a A∈m
Proof. Consider some fixed set of k2 nonzero rows and k1 nonzero columns, and let Mo denote the
covering set obtained by gridding the kι ∙ k2 entries at scale √^, whereby
C,-------∖ k1k2
2b√k^!	.
For any A ∈ A with these specific nonzero rows and columns, the A ∈ Mo obtained by rounding
each nonzero entry of A to the nearest grid element gives
kA - Ak2 = X (Aij-Aij )2 ≤ X (√fe I e2 X k⅛ = H
The final cover M is now obtained by unioning copies of Mo for all km km ≤ mk1+k2 possible
submatrices of size k2 X kι.	□
The proof of Theorem 1.4 now carefully combines the preceding pieces.
22
Published as a conference paper at ICLR 2021
Proof of Theorem 1.4. The proof proceeds in three steps, as follows.
1.	A covering number is estimate for sparsified networks, as output by Lemma 3.2.
2.	A covering number for general networks is computed by balancing the error terms from
Lemma 3.2 and its cover computed here.
3.	This covering number is plugged into a Dudley integral to obtain the desired Rademacher
bound.
Proceeding with this plan, let (X0T, . . . , XLT ) be the layer outputs (and network input) exactly as
provided by Lemma 3.2. Additionally, define diagonal matrices Dj := P !	elelT (with DL =
l∈Sj+1
I, where the “!” denotes unique inclusion; these matrices capture the effect of the subsequent
sparsification, and can be safely inserted after each Wj without affecting XjT , meaning
XT = Π∙σ∙ (Wj MjXT-I ) = 口汹(DjWj MjXT-1).
Let per-layer cover precisions (1, . . . , L) be given, which will be optimized away later. This proof
will inductively construct
XT ：= χτ,	χτ = ∏jσj (WjXj-1),
where Wj is a cover element for Dj Wj Mj , and inductively satisfying
kXT - XTk≤kX∣∣Fmj72 E q UkWjkF
i≤j l≤j
l6=i
To construct the per-layer cover elements Wj, first note by the form of Mj (and the scaling Zi
provided by Lemma 3.2) that
b:
max(Dj Wj Mj)l i ≤ max kWj Mj ei k ≤ Zi
i,l	, i
kWjeik
kWj eik
≤ kWjkF ʌ/kmr.
Consequently, by Lemma E.2, there exists a cover Cj of matrices of the form DjWjMj satisfying
|Cj | ≤ mkj+kj-1
2bpkj ! kjki ≤ mk+kj-ι
kj kj-1
j
and the closest cover element WjCj to Dj Wj Mj satisfies kDj Wj Mj - Wj kF ≤ j .
C	1 ∙	∙ .<	. )	∙	1	. 1	1	1	I I CT	TT-T I I	I ITT-T I I	C ,1	∙ 1
Proceeding with the induction, the base case has kX0T - X0T k = kXT - XT k = 0, thus consider
j > 0. The first step is to estimate the spectral norm of DjWjMj, which can be coarsely upper
bounded via
kDjWjMjk2 ≤ kDjWjMjk2 ≤ X kWjMjeik2 ≤ X kWjk2ʌ ≤ kWjk2m.
kj -1
ii
1 ʌ .< ɪ'	Γ∙ √V	1 TT-
By the form of Xj and Xj ,
kX; - XTk = k∏jσj(DjWjMjXT-I)- Πjσj(WjX[ι)k
≤ kDjWjMjXL - WjXT-Ik
.. ʌ τ ~~∙..
≤ kDjWjMjXj-I- DjWjMjXj-Ik + kDjWjMjXT-I- W/T-ιk
..	.... 入―	〜.	..	..	~....~∙	..
≤	kDjWjMjk2kXT-1 - XT-IkF + kDjWjMj - Wjk2kXT-1kF
≤	√mkWjkF [kX∣∣Fmjτ"2 X q Y kWjk』+ j∣XkF Y kWjk2
i<j l<j	i<j
l6=i
≤ kXkFmj/2	i	kWjkF
i≤j	l≤j
l6=i
23
Published as a conference paper at ICLR 2021
which establishes the desired bound on the error.
The next step is to optimize kj. Let E > 0 be arbitrary, and set e-1 := e-12L√mkX ∣∣f Qi=j ∣∣Wi∣∣F,
whereby
kXL-XLkF ≤ ∣,	∣Cj∣≤ mkj+kj-1 (4mLpj kXkF QikWi kF) j j-1.
The overall network cover N is the product of the covers for all layers, and thus has cardinality
satisfying
1 4 m^	4	4	I	4 4	4	(4mLPkjkXkF Qi	k WikF)
ln |N| ≤	2_^ ln |Cj | ≤ 2 kj	ln m	+	kjkj-ι ln (---------------j
≤ 2 X kj ln m + X 2kj ln CrnE kX kF )十 X 2席.X ta ∣wj |卜.
To choose (k1, . . . , kL), letting XLT denote the output of the original unsparsified network, note
firstly that the full error bound satisfies
kXL- XLk≤kxL- XLk + kXL- XLk
≤ X √α= + 2,	where α ：= ∣X∣f Y kWik2 Pr(Wiy
To choose ki , the approach here is to minimize a Lagrangian corresponding to the cover cardinality,
subject to the total cover error being E. Simplifying the previous expressions and noting 2kjkj-1 ≤
kj2 + kj2-1, whereby the dominant term in ln |N | is Pj kj2, consider Lagrangian
L(kι,..., kι,λ) := X k2+λ (X √α= - E j,
which has critical points when each ki satisfies
ki5/2	λ
-----=—.
ɑi---4
thus ki := 022//Z with Z := e2∕(2 Pj a；/5)2. As a sanity check (since it was baked into the
Lagrangian), plugging this into the cover error indeed gives
kXL - XLk ≤ X √⅛ + E = √ZX α425 + 1 = e.
ii	i
To upper bound the cover cardinality, first note that
X k2 = Z X a；25 = e4 (X a；25)5,
ii	i
whereby
ln ∣N∣ = θ([X k2i ∙ hXln k WikFij
=β	where β = O (∣∣Xk4 [YkWj 哂[X Sr(Wi)可[X ln |*|』).
ji	i
The final step is to apply a Dudley entropy integral (Shalev-Shwartz & Ben-David, 2014), which
gives
√n √2β d) = in f(4ζ √n+12 [ I- √n ]pβ!.
nRad(F) = inf(4《√ +12^
24
Published as a conference paper at ICLR 2021
Dropping the negative term gives an expression of the form aZ + b/Z, which is convex in Z > 0 and
has critical point at Z2 = b/a, which after plugging back in gives an upper bound 2√ob, meaning
1	l∖ 1/2	Lll
nRad(F) ≤ 2 (4√n ∙ 12Pβ)	= 8√3n1/4e1/4.
Dividing by n and expanding the definition of β gives the final Rademacher complexity bound. □
25