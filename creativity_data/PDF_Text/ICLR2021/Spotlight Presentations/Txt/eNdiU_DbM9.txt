Published as a conference paper at ICLR 2021
Uncertainty Sets for Image Classifiers using
Conformal Prediction
Anastasios N. Angelopoulos； Stephen Bates*, Jitendra Malik, & Michael I. Jordan
Departments of Electrical Engineering and Computer Sciences and Statistics
University of California, Berkeley
{angelopoulos,stephenbates,malik,jordan}@cs.berkeley.edu
Ab stract
Convolutional image classifiers can achieve high predictive accuracy, but quanti-
fying their uncertainty remains an unresolved challenge, hindering their deploy-
ment in consequential settings. Existing uncertainty quantification techniques,
such as Platt scaling, attempt to calibrate the network’s probability estimates, but
they do not have formal guarantees. We present an algorithm that modifies any
classifier to output a predictive set containing the true label with a user-specified
probability, such as 90%. The algorithm is simple and fast like Platt scaling, but
provides a formal finite-sample coverage guarantee for every model and dataset.
Our method modifies an existing conformal prediction algorithm to give more sta-
ble predictive sets by regularizing the small scores of unlikely classes after Platt
scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and
other classifiers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling
baseline.
1	Introduction
Imagine you are a doctor making a high-stakes medical decision based on diagnostic information
from a computer vision classifier. What would you want the classifier to output in order to make
the best decision? This is not a casual hypothetical; such classifiers are already used in medical
settings (e.g., Razzak et al., 2018; Lundervold & Lundervold, 2019; Li et al., 2014). A maximum-
likelihood diagnosis with an accompanying probability may not be the most essential piece of in-
formation. To ensure the health of the patient, you must also rule in or rule out harmful diagnoses.
In other words, even if the most likely diagnosis is a stomach ache, it is equally or more important
to rule out stomach cancer. Therefore, you would want the classifier to give you—in addition to
an estimate of the most likely outcome—actionable uncertainty quantification, such as a set of pre-
dictions that provably covers the true diagnosis with a high probability (e.g., 90%). This is called
a prediction set (see Figure 1). Our paper describes a method for constructing prediction sets from
any pre-trained image classifier that are formally guaranteed to contain the true class with the de-
sired probability, relatively small, and practical to implement. Our method modifies a conformal
predictor (Vovk et al., 2005) given in Romano et al. (2020) for the purpose of modern image classi-
fication in order to make it more stable in the presence of noisy small probability estimates. Just as
importantly, we provide extensive evaluations and code for conformal prediction in computer vision.
Formally, for a discrete response Y ∈ Y = {1, . . . , K} and a feature vector X ∈ Rd, we desire an
uncertainty set function, C(X), mapping a feature vector to a subset of {1, . . . , K} such that
P(Y ∈C(X))≥1-α,	(1)
for a pre-specified confidence level α such as 10%. Conformal predictors like our method can mod-
ify any black-box classifier to output predictive sets that are rigorously guaranteed to satisfy the
desired coverage property shown in Eq. (1). For evaluations, we focus on Imagenet classification
* Equal contribution. Blog: https://people.eecs.berkeley.edu/~angelopoulos/blog/
posts/conformal-classification
1
Published as a conference paper at ICLR 2021
Figure 1: Prediction set examples on Imagenet. We show three examples of the class fox squirrel and
the 95% prediction sets generated by RAPS to illustrate how the size of the set changes as a function of the
difficulty of a test-time image.
using convolutional neural networks (CNNs) as the base classifiers, since this is a particularly chal-
lenging testbed. In this setting, X would be the image and Y would be the class label. Note that the
guarantee in Eq. (1) is marginal over X and Y—it holds on average, not for a particular image X.
A first approach toward this goal might be to assemble the set by including classes from highest to
lowest probability (e.g., after Platt scaling and a softmax function; see Platt et al., 1999; Guo et al.,
2017) until their sum just exceeds the threshold 1 - α. We call this strategy naive and formulate
it precisely in Algorithm 1. There are two problems with naive: first, the probabilities output by
CNNs are known to be incorrect (Nixon et al., 2019), so the sets from naive do not achieve cov-
erage. Second, image classification models’ tail probabilities are often badly miscalibrated, leading
to large sets that do not faithfully articulate the uncertainty of the model; see Section 2.3. Moreover,
smaller sets that achieve the same coverage level can be generated with other methods.
The coverage problem can be solved by picking a new threshold using holdout samples. For exam-
ple, with α =10%, if choosing sets that contain 93% estimated probability achieves 90% coverage
on the holdout set, we use the 93% cutoff instead. We refer to this algorithm, introduced in Ro-
mano et al. (2020), as Adaptive Prediction Sets (APS). The APS procedure provides coverage but
still produces large sets. To fix this, we introduce a regularization technique that tempers the in-
fluence of these noisy estimates, leading to smaller, more stable sets. We describe our proposed
algorithm, Regularized Adaptive Prediction Sets (RAPS), in Algorithms 2 and 3 (with APS as a
special case). As we will see in Section 2, both APS and RAPS are always guaranteed to satisfy
Eq. (1)—regardless of model and dataset. Furthermore, we show that RAPS is guaranteed to have
better performance than choosing a fixed-size set. Both methods impose negligible computational
requirements in both training and evaluation, and output useful estimates of the model’s uncertainty
on a new image given, say, 1000 held-out examples.
In Section 3 we conduct the most extensive evaluation of conformal prediction in deep learning to
date on Imagenet and Imagenet-V2. We find that RAPS sets always have smaller average size than
naive and APSsets. For example, using a ResNeXt-101, naive does not achieve coverage, while
APS and RAPS achieve it almost exactly. However, APS sets have an average size of 19, while RAPS
sets have an average size of2 at α = 10% (Figure 2 and Table 1). We will provide an accompanying
codebase that implements our method as a wrapper for any PyTorch classifier, along with code to
exactly reproduce all of our experiments.
1.1	Related work
Reliably estimating predictive uncertainty for neural networks is an unsolved problem. Historically,
the standard approach has been to train a Bayesian neural network to learn a distribution over net-
work weights (Quinonero-Candela et al., 2005; MacKay, 1992; Neal, 2012; Kuleshov et al., 2018;
Gal, 2016). This approach requires computational and algorithmic modifications; other approaches
avoid these via ensembles (Lakshminarayanan et al., 2017; Jiang et al., 2018) or approximations of
Bayesian inference (Riquelme et al., 2018; Sensoy et al., 2018). These methods also have major
practical limitations; for example, ensembling requires training many copies of a neural network ad-
versarially. Therefore, the most widely used strategy is ad-hoc traditional calibration of the softmax
scores with Platt scaling (Platt et al., 1999; Guo et al., 2017; Nixon et al., 2019).
This work develops a method for uncertainty quantification based on conformal prediction. Originat-
ing in the online learning literature, conformal prediction is an approach for generating predictive
sets that satisfy the coverage property in Eq. (1) (Vovk et al., 1999; 2005). We use a convenient
data-splitting version known as split conformal prediction that enables conformal prediction meth-
2
Published as a conference paper at ICLR 2021
Figure 2: Coverage and average set size on Imagenet for prediction sets from three methods. All methods
use a ResNet-152 as the base classifier, and results are reported for 100 random splits of Imagenet-Val, each of
size 20K. See Section 3.1 for full details.
ods to be deployed for essentially any predictor (Papadopoulos et al., 2002; Lei et al., 2018). While
mechanically very different from traditional calibration as discussed above, we will refer to our
approach as conformal calibration to highlight that the two methodologies have overlapping but
different goals.
Conformal prediction is a general framework, not a specific algorithm—important design decisions
must be made to achieve the best performance for each context. To this end, Romano et al. (2020)
and Cauchois et al. (2020) introduce techniques aimed at achieving coverage that is similar across
regions of feature space, whereas Vovk et al. (2003); Hechtlinger et al. (2018) and Guan & Tib-
shirani (2019) introduce techniques aimed at achieving equal coverage for each class. While these
methods have conceptual appeal, thus far there has been limited empirical evaluation of this general
approach for state-of-the-art CNNs. Concretely, the only works that we are aware of that include
some evaluation of conformal methods on ImageNet—the gold standard for benchmarking com-
puter vision methods—are Hechtlinger et al. (2018), Park et al. (2019), Cauchois et al. (2020), and
Messoudi et al. (2020), although in all four cases further experiments are needed to more fully eval-
uate their operating characteristics for practical deployment. At the heart of conformal prediction
is the conformal score - a measure of similarity between labeled examples which is used to com-
pare a new point to among those in a hold out set. Our theoretical contribution can be summarized
as a modification of the conformal score from Romano et al. (2020) to have smaller, more stable
sets. Lastly, there are alternative approaches to returning prediction sets not based on conformal
prediction (Pearce et al., 2018; Zhang et al., 2018). These methods can be used as input to a con-
formal procedure to potentially improve performance, but they do not have finite-sample coverage
guarantees when used alone.
2	Methods
In developing uncertainty set methods to improve upon naive, we are guided by three desiderata.
First and most importantly, the coverage desideratum says the sets must provide 1 - α coverage, as
discussed above. Secondly, the size desideratum says we want sets of small size, since these convey
more detailed information and may be more useful in practice. Lastly, the adaptiveness desidera-
tum says we want the sets to communicate instance-wise uncertainty: they should be smaller for
easy test-time examples than for hard ones; see Figure 1 for an illustration. Coverage and size
are obviously competing objectives, but size and adaptiveness are also often in tension. The size
desideratum seeks small sets, while the adaptiveness desideratum seeks larger sets when the classi-
Algorithm 1 Naive Prediction Sets
Input: α, sorted scores s, associated permutation of classes I , boolean rand
1: procedure NAIVE(α, s, I , rand)
2:	L — 1
3:	while Pi* L=1 si < 1 - α do	. Stop if 1 - α probability exceeded
4:	L — L +1
5:	if rand then	. Break ties randomly (explained in Appendix B)
6:	U — Unif(0,1)
7:	V J (PL Si-(1-α))∕sL
8:	ifU ≤ V then
9:	L J L - 1
10:	return I1, ..., IL
Output: The 1 - α prediction set, I1, ..., IL
3
Published as a conference paper at ICLR 2021
①6更①>0。-eοU-dE①
τ: set size parameter
(a) Conformal calibration
class
(b) A RAPS prediction set
penalty
probability
Figure 3: Visualizations of conformal calibration and RAPS sets. In the left panel, the y-axis shows the
empirical coverage on the conformal calibration set, and 1 — α0 =「(n + 1)(1 — α)~∖ /n. In the right panel, the
printed numbers indicate the cumulative probability plus penalty mass. For the indicated value TcCaɪ, the RAPS
prediction set is {c, d, f, b}.
fier is uncertain. For example, always predicting a set of size five could achieve coverage, but it is
not adaptive. As noted above, both APSand RAPS achieve correct coverage, and we will show that
RAPS improves upon APS according to the other two desiderata.
We now turn to the specifics of our proposed method. We begin in Subsection 2.1 by describing an
abstract data-splitting procedure called conformal calibration that enables the near-automatic con-
struction of valid predictive sets (that is, sets satisfying Eq. (1)). Subsequently, in Subsection 2.2, we
provide a detailed presentation of our procedure, with commentary in Section 2.3. In Subsection 2.4
we discuss the optimality of our procedure, proving that it is at least as good as the procedure that
returns sets of a fixed size, unlike alternative approaches.
2.1	Conformal calibration
We first review a general technique for producing valid prediction sets, following the articulation
in Gupta et al. (2019). Consider a procedure that outputs a predictive set for each observation,
and further suppose that this procedure has a tuning parameter τ that controls the size of the sets.
(In RAPS, τ is the cumulative sum of the sorted, penalized classifier scores.) We take a small
independent conformal calibration set of data, and then choose the tuning parameter τ such that
the predictive sets are large enough to achieve 1 - α coverage on this set. See Figure 3 for an
illustration. This calibration step yields a choice of τ , and the resulting set is formally guaranteed to
have coverage 1 - α on a future test point from the same distribution; see Theorem 1 below.
Formally, let (Xi, Yi)i=1,...,n be an independent and identically distributed (i.i.d.) set of variables
that was not used for model training. Further, let C(x, u, τ) : Rd × [0, 1] × R → 2Y be a set-
valued function that takes a feature vector x to a subset of the possible labels. The second argument
u is included to allow for randomized procedures; let U1, . . . , Un be i.i.d. uniform [0, 1] random
variables that will serve as the second argument for each data point. Suppose that the sets are
indexed by τ such that they are nested, meaning larger values of τ lead to larger sets:
C(x, u, τ1) ⊆ C(x, u, τ2)	if	τ1 ≤ τ2.	(2)
To find a function that will achieve 1 - α coverage on test data, we select the smallest τ that gives
at least 1 - α coverage on the conformal calibration set, with a slight correction to account for the
finite sample size:
|{i : Y ∈C(Xi,Ui,τ)}| ≥ d(n +1)(1 - α)e
nn
(3)
Tccal = inf T τ
The set function C(x, u, τ) with this data-driven choice of τ is guaranteed to have correct finite-
sample coverage on a fresh test observation, as stated formally next.
Theorem 1 (Conformal calibration coverage guarantee). Suppose (Xi, Yi, Ui)i=1,...,n and
(Xn+1, Yn+1, Un+1) are i.i.d. and letC(x, u, τ) be a set-valued function satisfying the nesting prop-
erty in Eq. (2). Suppose further that the sets C(x, u, τ) grow to include all labels for large enough
τ: for all X ∈ Rd, C (x, u,τ) = Y for some T. Then for %闻 defined as in Eq. (3), we have the
following coverage guarantee:
P fYn+1 ∈ C(Xn+1, Un+1, TCCaI)) ≥ 1 - α∙
4
Published as a conference paper at ICLR 2021
This is the same coverage property as Eq. (1) in the introduction, written in a more explicit manner.
The result is not new—a special case of this result leveraging sample-splitting first appears in the
regression setting in Papadopoulos et al. (2002), and the core idea of conformal prediction was
introduced even earlier; see (Vovk et al., 2005).
As a technical remark, the theorem also holds if the observations to satisfy the weaker condition
of exchangeability; see Vovk et al. (2005). In addition, for most families of set-valued functions
C(x, u, τ) there is a matching upper bound:
P(Yn+1 ∈ C(Xn+1, Un+1, Tccal
1
n + 1
≤1-α+
Roughly speaking, this will hold whenever the sets grow smoothly in τ. See Lei et al. (2018) for a
formal statement of the required conditions.
2.2	Our method
Conformal calibration is a powerful general idea, allowing one to achieve the coverage desideratum
for any choice of sets C(x, u, τ). Nonetheless, this is not yet a full solution, since the quality of the
resulting prediction sets can vary dramatically depending on the design of C(x, u, τ). In particular,
we recall the size and adaptiveness desiderata from Section 1—we want our uncertainty sets to be
as small as possible while faithfully articulating the instance-wise uncertainty of each test point. In
this section, we explicitly give our algorithm, which can be viewed as a special case of conformal
calibration with the uncertainty sets C designed to extract information from CNNs.
Our algorithm has three main ingredients. First, for a feature vector x, the base model computes
class probabilities ∏χ ∈ Rk, and We order the classes from most probable to least probable. Then,
we add a regularization term to promote small predictive sets. Finally, we conformally calibrate the
penalized prediction sets to guarantee coverage on future test points.
Formally, let Px(y) = PK=I ∏χ(y0)I{∏x(yθ)>πx(y)} be the total probability mass of the set of labels
that are more likely than y. These are all the labels that Will be included before y is included. In
addition, let θχ(y) = ∣{y0 ∈ Y : ∏χ(y0) ≥ ∏x(y)}∣ be the ranking of y among the label based on the
probabilities ∏. For example, if y is the third most likely label, then θχ(y) = 3.1 We take
C*(x,u,τ):= {y : ρx(y) + ∏x(y) ∙ U + λ ∙ (θχ(y) - kreg)+ ≤ τ},
(4)
regularization
Where (z)+ denotes the positive part of z and λ, kreg ≥ 0 are regularization hyperparameters that
are introduced to encourage small set sizes. See Figure 3 for a visualization of a RAPS predictive
set and Appendix E for a discussion of hoW to select kreg and λ.
Since this is the heart of our proposal, We carefully parse each term. First, the ρx(y) term increases
as y ranges from the most probable to least probable label, so our sets Will prefer to include the y that
are predicted to be the most probable. The second term, ∏χ(y) ∙ u, is a randomized term to handle
the fact that the value Will jump discretely With the inclusion of each neW y. The randomization term
can never impact more than one value of y: there is at most one value of y such that y ∈ C(x, 0, τ)
but y ∈/ C(x, 1, τ). These first tWo terms can be vieWed as the CDF transform after arranging the
classes from most likely to least likely, randomized in the usual Way to result in a continuous uniform
random variable (cf. Romano et al., 2020). We discuss randomization further in Appendix B.
Lastly, the regularization promotes small set sizes: for values ofy that occur farther doWn the ordered
list of classes, the term λ ∙ (θχ(y) - kreg)+ makes that value of y require a higher value of T before
it is included in the predictive set. For example, if kreg = 5, then the sixth most likely value of y
has an extra penalty of size λ, so it will never be included until T exceeds Px(y) + ∏χ(y) ∙ U + λ,
whereas it enters when T exceeds Px(y) + ∏χ(y) ∙ U in the nonregularized version. Our method has
the following coverage property:
Proposition 1 (RAPS coverage guarantee). Suppose (Xi, Yi, Ui)i=1,...,n and (Xn+1, Yn+1, Un+1)
are i.i.d. and let C *(x, u, T) be defined as in Eq. (4). Suppose further that ∏χ(y) > 0 for all X and
y. Thenfor Tccal defined as in Eq. (3), we have the following Coverage guarantee:
1 - α ≤ P ( γn+1 ∈ C*(Xn+1, Un+1, Tccal
≤1-α+n⅛
1For ease of notation, we assume distinct probabilities. Else, label-ordering ties should be broken randomly.
5
Published as a conference paper at ICLR 2021
Algorithm 2 RAPS Conformal Calibration
Input: α; s ∈ [0, 1]n×K, I ∈ {1, ..., K}n×K, and one-hot y ∈ {0, 1}K corresponding respectively
to the sorted scores, the associated permutation of indexes, and labels for each ofn examples in
the calibration set; kreg ; λ; boolean rand
1:	procedure RAPSC(α,s,I,y,λ)
2:	for i ∈ {1,…,n} do
3：	Li — { j : Iij= y }
4:	Ei J ςl==00si,j + λ(Li - kreg + I) +
5:	if rand then
6:	U 〜Unif(0,1)
7：	Ei J Ei - si,Li + U * si,Li
8:	Tccal J the d(1 - α)(1 + n)e largest value in {Ei}n=ι
9：	return Tccal
Output: The generalized quantile, Tccal	. The value in Eq. (3)
Algorithm 3 RAPS Prediction Sets
Input: α, sorted scores S and the associated permutation of classes I for a test-time example, Tccal
from Algorithm 2, kreg , λ, boolean rand
1:	procedure RAPS(α, s, I, Tccal, kreg, λ, rand)
2:	L Jl j ∈ Y : ς=0si + λ(L - kreg)+ ≤ Tccal I + 1
3:	V J (Tccal -夕匕01方-λ(L - kreg)+ + sL-1)/sL-1
4:	if rand & V ≤ U 〜Unif(0,1) then
5:	L J L - 1
6:	return C = I1, ...IL	. The L most likely classes
Output: The 1 - α confidence set, C	. The set in Eq. (4)
Note that the first inequality is a corollary of Theorem 1, and the second inequality is a special case
of the remark in Section 2.1. The restriction that ∏χ(y) > 0 is not necessary for the first inequality.
2.3	Why regularize?
In our experiments, the sets from APS are larger than necessary, because APS is sensitive to the
noisy probability estimates far down the list of classes. This noise leads to a permutation problem of
unlikely classes, where ordering of the classes with small probability estimates is determined mostly
by random chance. If 5% of the true classes from the calibration set are deep in the tail due to the
permutation problem, APS will choose large 95% predictive sets; see Figure 2. The inclusion of the
RAPS regularization causes the algorithm to avoid using the unreliable probabilities in the tail; see
Figure 4. We discuss how RAPS improves the adaptiveness ofAPS in Section 4 and Appendix E.
2.4	Optimality considerations
To complement these experimental results, we now formally prove that RAPS with the correct reg-
ularization parameters will always dominate the simple procedure that returns a fixed set size. (Sec-
tion 3.5 shows the parameters are easy to select and RAPS is not sensitive to their values). For a
feature vector x, let y(j)(x) be the label with the jth highest predicted probability. We define the
top-kpredictive sets to be {y(i)(x),..., y(k)(x)}.
Proposition 2 (RAPS dominates top-k sets). Suppose (Xi, Yi, Ui)i=1,...,n and (Xn+1, Yn+1, Un+1)
are i.i.d. draws. Let k* be the smallest k such that the top-k predictive sets have Coverage at least
d(n +1)(1 一 α)e∕n on the conformal calibration points (Xi,匕)i=ι,…,n. Take C*(x, u, T) as in Eq.
(4) with any kreg ≤ k* and λ = 1. Then with T%al ChoSen as in Eq. (3), we have
C*(Xn+1 ,Un+l,Tccal) ⊆ {y(l)(x),...,y(k*)(x)}.
In words, the RAPS procedure with heavy regularization will be at least as good as the top-k proce-
dure in the sense that it has smaller or same average set size while maintaining the desired coverage
level. This is not true of either the naive baseline or the APS procedure; Table 2 shows that these
two procedures usually return predictive sets with size much larger than k*.
3 Experiments
6
Published as a conference paper at ICLR 2021
Accuracy	Coverage	Size
Model	Top-1	Top-5	Top K	Naive	APS	RAPS	Top K	Naive	APS	RAPS
ResNeXt101	0.793	0.945	0.900	0.889	0.900	0.900	2.42	17.1	19.7	2.00
ResNet152	0.783	0.94	0.900	0.895	0.900	0.900	2.63	9.78	10.4	2.11
ResNet101	0.774	0.936	0.900	0.896	0.900	0.900	2.83	10.3	10.7	2.25
ResNet50	0.761	0.929	0.900	0.896	0.900	0.900	3.14	11.8	12.3	2.57
ResNet18	0.698	0.891	0.900	0.895	0.900	0.900	5.72	15.5	16.2	4.43
DenseNet161	0.771	0.936	0.900	0.894	0.900	0.900	2.84	11.2	12.1	2.29
VGG16	0.716	0.904	0.900	0.895	0.901	0.900	4.75	13.4	14.1	3.54
Inception	0.695	0.887	0.900	0.885	0.900	0.901	6.30	75.4	89.1	5.32
ShuffleNet	0.694	0.883	0.900	0.891	0.900	0.900	6.46	28.9	31.9	5.05
Table 1: Results on Imagenet-Val. We report coverage and size of the optimal, randomized fixed sets, naive,
APS, and RAPS sets for nine different Imagenet classifiers. The median-of-means for each column is reported
over 100 different trials. See Section 3.1 for full details.
In this section we report on experiments that study the performance of the predictive sets from
naive, APS, and RAPS, evaluating each based on the three desiderata above. We begin with a brief
preview of the experiments. In Experiment 1, we evaluate naive, APS, and RAPS on Imagenet-
Val. Both APS and RAPS provided almost exact coverage, while naive sets had coverage slightly
below the specified level. APS has larger sets on average than naive and RAPS. RAPS has a
much smaller average set size than APS and naive. In Experiment 2, we repeat Experiment 1 on
Imagenet-V2, and the conclusions still hold. In Experiment 3, we produce histograms of set sizes
for naive, APS, and RAPS for several different values of λ, illustrating a simple tradeoff between
set size and adaptiveness. In Experiment 4, we compute histograms of RAPS sets stratified by
image difficulty, showing that RAPS sets are smaller for easier images than for difficult ones. In
Experiment 5, we report the performance of RAPS with many values of the tuning parameters.
In our experiments, we use nine standard, pretrained Imagenet classifiers from the torchvision
repository (Paszke et al., 2019) with standard normalization, resize, and crop parameters. Before
applying naive, APS, or RAPS, we calibrated the classifiers using the standard temperature scal-
ing/Platt scaling procedure as in Guo et al. (2017) on the calibration set. Thereafter, naive, APS,
and RAPS were applied, with RAPS using a data-driven choice of parameters described in Ap-
pendix E. We use the randomized versions of these algorithms—see Appendix B for a discussion.
3.1	Experiment 1: coverage vs set size on Imagenet
In this experiment, we calculated the coverage and mean set size of each procedure for two different
choices of α. Over 100 trials, we randomly sampled two subsets of Imagenet-Val: one conformal
calibration subset of size 20K and one evaluation subset of size 20K. The median-of-means over
trials for both coverage and set size are reported in Table 1. Figure 2 illustrates the performances of
naive, APS, and RAPS; RAPS has much smaller sets than both naive and APS, while achieving
coverage. We also report results from a conformalized fixed-k procedure, which finds the smallest
fixed set size achieving coverage on the holdout set, k*, then predicts sets of size k - 1 or k on
new examples in order to achieve exact coverage; see Algorithm 4 in Appendix E.
3.2	Experiment 2: coverage vs set size on Imagenet-V2
The same procedure as Experiment 1 was repeated on Imagenet-V2, with exactly the same normal-
ization, resize, and crop parameters. The size of the calibration and evaluation sets was 5K, since
Imagenet-V2 is a smaller dataset. The result shows that our method can still provide coverage even
for models trained on different distributions, as long as the conformal calibration set comes from the
new distribution. The variance of the coverage is higher due to having less data.
3.3	Experiment 3: Set sizes of NAIVE, APS, and RAPS on Imagenet
We investigate the effect of regularization in more detail. For three values of λ, we collected the set
sizes produced by each of naive, APS, and RAPS and report their histograms in Figure 4.
3.4	Experiment 4: Adaptiveness of RAPS on Imagenet
We now show that RAPS sets are smaller for easy images than hard ones, addressing the adaptiveness
desideratum. Table 4 reports the size-stratified coverages of RAPS at the 90% level with kreg = 5
and different choices of λ. When λ is small, RAPS allows sets to be large. But when λ = 1, RAPS
7
Published as a conference paper at ICLR 2021
Accuracy	Coverage	Size
Model	Top-1	Top-5	Top K	Naive	APS	RAPS	Top K	Naive	APS	RAPS
ResNeXt101	0.678	0.874	0.900	0.888	0.899	0.899	7.48	43.0	50.8	6.18
ResNet152	0.67	0.876	0.899	0.896	0.900	0.900	7.18	25.8	27.2	5.69
ResNet101	0.657	0.859	0.901	0.894	0.900	0.898	9.21	28.7	30.7	6.93
ResNet50	0.634	0.847	0.898	0.894	0.899	0.900	10.3	30.3	32.3	7.80
ResNet18	0.572	0.802	0.902	0.895	0.900	0.900	17.5	35.3	37.4	13.3
DenseNet161	0.653	0.862	0.902	0.895	0.901	0.901	8.6	29.9	32.4	6.93
VGG16	0.588	0.817	0.902	0.897	0.900	0.899	15.1	31.9	32.8	11.2
Inception	0.573	0.797	0.900	0.893	0.900	0.899	21.8	145.0	155.0	20.5
ShuffleNet	0.559	0.781	0.899	0.892	0.900	0.899	26.0	66.2	71.7	22.5
Table 2: Results on Imagenet-V2. We report coverage and size of the optimal, randomized fixed sets, naive,
APS, and RAPS sets for nine different Imagenet classifiers. The median-of-means for each column is reported
over 100 different trials at the 10% level. See Section 3.2 for full details.
size
size
Figure 4: Set sizes produced with ReSNet-152. See Section 3.3 for details.
size
clips sets to be a maximum of size 5. Table 7 (in the Appendix) stratifies by image difficulty, showing
that RAPS sets are small for easy examples and large for hard ones. Experiments 3 and 4 together
illustrate the tradeoff between adaptiveness and size: as the average set size decreases, the RAPS
procedure truncates sets larger than the smallest fixed set that provides coverage, taming the heavy
tail of the APS procedure. Since RAPS with large λ undercovers hard examples, it must compensate
by taking larger sets for easy examples to ensure the 1 - α marginal coverage guarantee. However,
the size only increases slightly since easy images are more common than hard ones, and the total
probability mass can often exceed Tccal by including only one more class. If this behavior is not
desired, we can instead automatically pick λ to optimize the adaptiveness of RAPS; see Section 4.
3.5	Experiment 5: choice of tuning parameters
While any value of the tuning parameters λ and kreg lead to coverage (Proposition 1), some values
will lead to smaller sets. In Experiments 1 and 2, we chose kreg and λ adaptively from data (see
Appendix E), achieving strong results for all models and choices of the coverage level. Table 3 gives
the performance of RAPS with many choices of kreg and λ for ResNet-152.
4	Adaptiveness and conditional coverage
In this section, we point to a definition of adaptiveness that is more natural for the image classifi-
cation setting than the existing notion of conditional coverage. We show that APS does not satisfy
conditional coverage, and that RAPS with small λ outperforms it in terms of adaptiveness.
We say that a set-valued predictor C : Rd → 2Y satisfies exact conditional coverage if P (Y ∈
C(X) | X = x) = 1 - α for each x. Distribution-free guarantees on conditional coverage are im-
possible (Vovk, 2012; Lei & Wasserman, 2014), but many algorithms try to satisfy it approximately
(Romano et al., 2019; 2020; Cauchois et al., 2020). In a similar spirit, Tibshirani et al. (2019) sug-
gest a notion of local conditional coverage, where one asks for coverage in a neighborhood of each
point, weighted according to a chosen kernel. Cauchois et al. (2020) introduce the worst-case slab
metric for measuring violations of the conditional coverage property. We present a different way of
measuring violations of conditional coverage.
Proposition 3. Suppose P(Y ∈ C(X) | X = x) = 1 - α for each x ∈ Rd. Then,
P(Y ∈ C(X) | {|C(X)| ∈ A}) = 1 -α for any A ⊂ {0,1,2,...}.
In words, if conditional coverage holds, then coverage holds after stratifying by set size. Based on
this result, In Appendix E, we introduce the size-stratified coverage violation criterion, a simple and
pragmatic way of quantifying adaptiveness. Then, we automatically tune λ on this metric so RAPS
markedly outperforms the adaptiveness of APS (see Table 8).
8
Published as a conference paper at ICLR 2021
kreg | λ	0	1e-4	1e-3	0.01	0.02	0.05	0.2	0.5	0.7	1.0
1	11.2	10.2	7.0	3.6	2.9	2.3	2.1	2.3	2.2	2.2
2	11.2	10.2	7.1	3.7	3.0	2.4	2.1	2.3	2.2	2.2
5	11.2	10.2	7.2	3.9	3.4	2.9	2.6	2.5	2.5	2.5
10	11.2	10.2	7.4	4.5	4.0	3.6	3.4	3.4	3.4	3.4
50	11.2	10.6	8.7	7.2	7.0	6.9	6.9	6.9	6.9	6.9
Table 3: Set sizes of RAPS with parameters kreg and λ, a ResNet-152, and coverage level 90%.
size	λ=	0	λ=	0.001	λ =	0.01	λ=	0.1	λ=	1
	cnt	cvg	cnt	cvg	cnt	cvg	cnt	cvg	cnt	cvg
0 to 1	11627	0.88	11539	0.88	11225	0.89	10476	0.92	10027	0.93
2 to 3	3687	0.91	3702	0.91	3741	0.92	3845	0.93	3922	0.94
4 to 6	1239	0.91	1290	0.91	1706	0.92	4221	0.89	6051	0.83
7 to 10	688	0.93	765	0.93	1314	0.91	1436	0.71	0	
11 to 100	2207	0.94	2604	0.93	2014	0.86	22	0.59	0	
101 to 1000	552	0.97	100	0.90	0		0		0	
Table 4: Coverage conditional on set size. We report average coverage of images stratified by the size of the
set output by RAPS using a ResNet-152 for varying λ. The marginal coverage rate is 90%.
In Table 4, we report on the coverage of APS and RAPS, stratified by the size of the prediction set.
Turning our attention to the λ = 0 column, we see that when APS outputs a set of size 101 - 1000,
APS has coverage 97%, substantially higher than 90% nominal rate. By Proposition 3, we conclude
that APS is not achieving exact conditional coverage, because the scores are far from the oracle
probabilities. The APS procedure still achieves marginal coverage by overcovering hard examples
and undercovering easy ones, an undesirable behavior. Alternatively, RAPS can be used to regularize
the set sizes—for λ = .001 to λ = .01 the coverage stratified by set size is more balanced. In
summary, even purely based on the adaptiveness desideratum, RAPS with light regularization is
preferable to APS. Note that as the size of the training data increases, as long as ∏ is consistent,
naive and APS will become more stable, and so we expect less regularization will be needed.
Lastly, we argue that conditional coverage is a poor notion of adaptiveness when the best possible
model (i.e., one fit on infinite data) has high accuracy. Given such a model, the oracle procedure
from Romano et al. (2020) would return the correct label with probability 1 - α and the empty set
with probability α. That is, having correct conditional coverage for high-signal problems where Y is
perfectly determined by X requires a perfect classifier. In our experiments on ImageNet, APS does
not approximate this behavior. Therefore, conditional coverage isn’t the right goal for prediction
sets with realistic sample sizes. Proposition 3 suggests a relaxation. We could require that we have
the right coverage, no matter the size of the prediction set: P(Y ∈ C(X) | {|C(x)| ∈ A}) ≥ 1 - α
for any A ⊂ {0, 1, 2, . . . }; Appendix E.2 develops this idea. We view this as a promising way to
reason about adaptiveness in high-signal problems such as image classification.
5	Discussion
For classification tasks with many possible labels, our method enables a researcher to take any base
classifier and return predictive sets guaranteed to achieve a pre-specified error level, such as 90%,
while retaining small average size. It is simple to deploy, so it is an attractive, automatic way to
quantify the uncertainty of image classifiers—an essential task in such settings as medical diag-
nostics, self-driving vehicles, and flagging dangerous internet content. Predictive sets in computer
vision (from RAPS and other conformal methods) have many further uses, since they systematically
identify hard test-time examples. Finding such examples is useful in active learning where one only
has resources to label a small number of points. In a different direction, one can improve efficiency
of a classifier by using a cheap classifier outputting a prediction set first, and an expensive one only
when the cheap classifier outputs a large set (a cascade; see, e.g., Li et al. 2015), and Fisch et al.
(2021) for an implementation of conformal prediction in this setting. One can also use predictive sets
during model development to identify failure cases and outliers and suggest strategies for improving
its performance. Prediction sets are most useful for problems with many classes; returning to our
initial medical motivation, we envision RAPS could be used by a doctor to automatically screen for
a large number of diseases (e.g. via a blood sample) and refer the patient to relevant specialists.
9
Published as a conference paper at ICLR 2021
References
Maxime Cauchois, Suyash Gupta, and John Duchi. Knowing what you know: valid and validated
confidence sets in multiclass and multilabel prediction. 2020. arXiv:2004.10181.
Adam Fisch, Tal Schuster, Tommi S. Jaakkola, and Regina Barzilay. Efficient conformal predic-
tion via cascaded inference with expanded admission. In International Conference on Learning
Representations, 2021.
Yarin Gal. Uncertainty in Deep Learning. University of Cambridge, 1(3), 2016.
Leying Guan and Rob Tibshirani. Prediction and outlier detection in classification problems. 2019.
arXiv:1905.04396.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. 2017. arXiv:1706.04599.
Chirag Gupta, Arun K Kuchibhotla, and Aaditya K Ramdas. Nested conformal prediction and
quantile out-of-bag ensemble methods. arXiv, pp. arXiv-1910, 2019.
Yotam Hechtlinger, BarnabaS Poczos, and Larry Wasserman. Cautious deep learning, 2018.
arXiv:1805.09460.
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. In
Advances in Neural Information Processing Systems, pp. 5541-5552, 2018.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. 2018. arXiv:1807.00263.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, pp. 6402-6413, 2017.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71-96, 2014.
Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-
free predictive inference for regression. Journal of the American Statistical Association, 113
(523):1094-1111, 2018. doi: 10.1080/01621459.2017.1307116.
Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua. A convolutional neural
network cascade for face detection. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5325-5334, 2015.
Qing Li, Weidong Cai, Xiaogang Wang, Yun Zhou, David Dagan Feng, and Mei Chen. Medical
image classification with convolutional neural network. In International Conference on Control
Automation Robotics & Vision, pp. 844-848. IEEE, 2014.
Alexander Selvikvag Lundervold and Arvid Lundervold. An overview of deep learning in medical
imaging focusing on MRI. ZeitSchriftfurMediziniSche PhySik, 29(2):102-127, 2019.
David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of
Technology, 1992.
Soundouss Messoudi, Sylvain Rousseau, and Sebastien Destercke. Deep conformal prediction for
robust models. In Information Processing and Management of Uncertainty in Knowledge-Based
Systems, pp. 528-540. Springer, 2020.
Radford M Neal. Bayesian Learning for Neural Networks. Springer Science & Business Media,
2012.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-
ing calibration in deep learning. In CVPR Workshops, pp. 38-41, 2019.
10
Published as a conference paper at ICLR 2021
Harris Papadopoulos, Kostas Proedrou, Vladimir Vovk, and Alex Gammerman. Inductive confi-
dence machines for regression. In Machine Learning: European Conference on Machine Learn-
ing ECML 2002,pp. 345-356, 2002.
Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. PAC confidence sets for deep neural
networks via calibrated prediction. 2019. arXiv:2001.00106.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8026-8037, 2019.
Tim Pearce, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. High-quality prediction inter-
vals for deep learning: A distribution-free, ensembled approach. In International Conference on
Machine Learning, pp. 6473-6482, 2018.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in Large Margin Classifiers, 10(3):61-74, 1999.
Joaquin Quinonero-Candela, Carl Edward Rasmussen, Fabian Sinz, Olivier Bousquet, and Bern-
hard Scholkopf. Evaluating predictive uncertainty challenge. In Machine Learning Challenges
Workshop, pp. 1-27. Springer, 2005.
Muhammad Imran Razzak, Saeeda Naz, and Ahmad Zaib. Deep learning for medical image process-
ing: Overview, challenges and the future. In Classification in BioApps, pp. 323-350. Springer,
2018.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An empir-
ical comparison of Bayesian deep networks for Thompson sampling. 2018. arXiv:1802.09127.
Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. In
Advances in Neural Information Processing Systems, pp. 3543-3553. 2019.
Yaniv Romano, Matteo Sesia, and Emmanuel J. Candes. Classification with valid and adaptive
coverage. 2020. arXiv:2006.02544.
Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. Journal of the American Statistical Association, 114(525):223-234, 2019.
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classifica-
tion uncertainty. In Advances in Neural Information Processing Systems, pp. 3179-3189, 2018.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal predic-
tion under covariate shift. In Advances in Neural Information Processing Systems, pp. 2530-2540.
2019.
Vladimir Vovk. Conditional validity of inductive conformal predictors. In Proceedings of the Asian
Conference on Machine Learning, volume 25, pp. 475-490, 2012.
Vladimir Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of
algorithmic randomness. In International Conference on Machine Learning, pp. 444-453, 1999.
Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence
machine. Technical report, 2003. Technical report.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World.
Springer, 2005.
Chong Zhang, Wenbo Wang, and Xingye Qiao. On reject and refine options in multicategory clas-
sification. Journal of the American Statistical Association, 113(522):730-745, 2018.
11
Published as a conference paper at ICLR 2021
A Proofs
Theorem 1. Let s(x, u, y) = infτ {y ∈ C(x, u, τ)}, and let si = s(Xi , Ui , Yi) for i = 1, . . . , n.
Then
{y : s(x, u, y) ≤ τ} = {y : y ∈ C(x, u, τ)}
because C(x, u, τ) is a finite set growing in τ by the assumption in Eq. (2). Thus,
{τ : |{i :si ≤ T}| ≥ d(1 - α)(n +1)]} = [τ : |{i : Yi W(CzU川 ≥ d(n +1)(1 - α)e ).
nn
Considering the left expression, the infimum over τ of the set on the left hand side is the
d(1 一 α)(n	+ I)] smallest value	of	the s%, so this is the value	of	/间.
Since s1, . . .	, sn, s(Xn+1, Un+1, Yn+1)	are exchangeable random variables, |{i :
s(Xn+1, Un+1, Yn+1) > si}| is stochastically dominated by the discrete uniform distribution
on {0, 1, . . . , n}. We thus have that
P(Yn+1 & C(Xn+1,Un+1,Tccal))= P (s(Xn+1,Un+1,Yn+1) >%#
=P(|{i: s(Xn+1,Un+1,Yn+1) > si}| ≥ d(n + 1)(1 - α)e)
=p ( ∣{i : S(Xn+1,Un+1,Yn+1) > si}| ≥ d(n + I)(I - a)] A
―1	n + 1	— n + 1	)
≤ α.
□
Proposition 1. The lower bound follows from Theorem 1. To prove the upper bound, using the
result from Theorem 2.2 of Lei et al. (2018) it suffices to show that the variables s(Xi, Ui, Yi) =
inf {T : Yi ∈ C(Xi, Ui, T)} are almost surely distinct. To this end, note that that
s(Xi,Ui,Yi) = PXi (Yi)+ ∏Xi (Yi) ∙ Ui + λ(0Xi (Yi) - kreg)+,
and due to the middle term of the sum, these values are distinct almost surely provided ∏Xi (Yi) >
0.	□
Proposition 2. We first show that Tccal ≤ 1 + k* 一 kreg. Note that since at least d(1 一 α)(n + 1)]
of the conformal calibration points are covered by a set of size k*, at least d(1 一 α)(n +1)] of the
Ei in Algorithm 2 are less than or equal to 1 + k* 一 kreg. Thus, by the definition of Tccal, We have
that it is less than or equal to 1 + k* 一 kreg. Then, note that by the definition of C * in Eq. (4), we
have that
∣C*(Xn+1,Un+1,Tccal)∣ ≤ k*.
as long as Tccal ≤ 1 + k* 一 kreg, since for the k* + 1 most likely class, the sum in Eq. (4) will exceed
λ ∙ (1 + k* 一 kreg) = (1 + k* 一 kreg) ≥ T%al, and so the k* + 1 class will not be in the set. □
Proposition 3. Suppose P(Y ∈ C(X) | X = x) = 1 一 α for each x ∈ Rd. Then,
P(Y ∈ C(X) | |C(X)| ∈ A)
Jx P(Y ∈ C(X) | X = x})I{∣C(χ)∣∈A}dP(X)
P(|C (X )∣∈A)
Rx(I- α)I{∣C(χ)∣∈A}dP(X)
P(|C (X )∣∈A)
1 一 α.
□
12
Published as a conference paper at ICLR 2021
Accuracy	Coverage	Size
Model	Top-1	Top-5	Top K	Naive	APS	RAPS	Top K	Naive	APS	RAPS
ResNeXt101	0.794	0.945	0.905	0.938	0.950	0.950	5.64	36.4	46.3	4.21
ResNet152	0.783	0.940	0.950	0.943	0.950	0.950	6.36	19.6	22.5	4.40
ResNet101	0.774	0.936	0.950	0.944	0.950	0.950	6.79	20.6	23.2	4.79
ResNet50	0.762	0.929	0.951	0.943	0.950	0.950	8.12	22.9	26.2	5.57
ResNet18	0.698	0.891	0.950	0.943	0.950	0.950	16.0	28.9	33.2	11.7
DenseNet161	0.772	0.936	0.950	0.942	0.950	0.950	6.84	23.4	28.0	5.09
VGG16	0.716	0.904	0.950	0.943	0.950	0.950	12.9	24.6	27.8	8.98
Inception	0.695	0.887	0.950	0.937	0.950	0.950	20.3	142.0	168.0	18.5
ShuffleNet	0.694	0.883	0.950	0.940	0.950	0.950	19.3	58.7	71.6	16.3
Table 5: Results on Imagenet-Val. We report coverage and size of the optimal, randomized fixed sets, naive,
APS, and RAPS sets for nine different Imagenet classifiers. The median-of-means for each column is reported
over 100 different trials. See Section 3.1 for full details.
Accuracy	Coverage	Size
Model	Top-1	Top-5	Top K	Naive	APS	RAPS	Top K	Naive	APS	RAPS
ResNeXt101	0.678	0.875	0.950	0.937	0.950	0.950	21.7	86.2	107.0	18.5
ResNet152	0.670	0.876	0.951	0.944	0.950	0.950	21.3	51.0	56.6	16.2
ResNet101	0.656	0.86	0.95	0.944	0.950	0.949	25.7	55.8	63.1	19.1
ResNet50	0.634	0.847	0.949	0.944	0.949	0.950	29.5	58.6	65.9	21.5
ResNet18	0.572	0.802	0.950	0.942	0.950	0.949	48.3	65.0	74.0	35.3
DenseNet161	0.653	0.861	0.951	0.941	0.950	0.949	25.9	60.0	72.7	20.4
VGG16	0.588	0.816	0.950	0.943	0.950	0.949	38.5	57.8	63.9	26.4
Inception	0.573	0.797	0.950	0.943	0.949	0.950	73.1	253.0	275.0	70.2
ShuffleNet	0.560	0.781	0.950	0.941	0.949	0.949	80.0	125.0	140.0	67.4
Table 6: Results on Imagenet-V2. We report coverage and size of the optimal, randomized fixed sets, naive,
APS, and RAPS sets for nine different Imagenet classifiers. The median-of-means for each column is reported
over 100 different trials at the 5% level. See Section 3.2 for full details.
B Randomized predictors
The reader may wonder why we choose to use a randomized procedure. The randomization is
needed to achieve 1 - α coverage exactly, which we will explain via an example. Note that the
randomization is of little practical importance, since the predictive set output by the randomized
procedure will differ from the that of the non-randomized procedure by at most one element.
Turning to an example, assume for a particular input image we expect a set of size k to have 91%
coverage, and a set of size k - 1 to have 89% coverage. In order to achieve our desired coverage of
90%, we randomly choose size k or k - 1 with equal probability. In general, the probabilities will
not be equal, but rather chosen so the weighted average of the two coverages is exactly 90%. If a
user of our method desires deterministic sets, it is easy to turn off this randomization with a single
flag, resulting in slightly conservative sets.
C IMAGENET AND IMAGENETV2 RESULTS FOR α = 5%
We repeated Experiments 1 and 2 with α = 5%. See the results in Tables 5 and 6.
D Coverage and size conditional on image difficulty
In order to probe the adaptiveness properties of APS and RAPS we stratified coverage and size by
image difficulty (the position of the true label in the list of most likely to least likely classes, based
on the classifier predictions) in Table 7. With increasing λ, coverage decreases for more difficult
images and increases for easier ones. In the most difficult regime, even though APS can output large
sets, those sets still rarely contain the true class. This suggests regularization is a sensible way to
stabilize the sets. As a final word on Table 7, notice that as λ increases, coverage improves for the
more common medium-difficulty examples, although not for very rare and difficult ones.
13
Published as a conference paper at ICLR 2021
difficulty	count	λ 二	0	λ=	0.001	λ =	0.01	λ =	0.1	λ =	1
		cvg	sz	cvg	sz	cvg	sz	cvg	sz	cvg	sz
1	15668	0.95	5.2	0.95	3.8	0.96	2.5	0.97	2.0	0.98	2.0
2 to 3	2578	0.78	15.7	0.78	10.5	0.80	6.0	0.84	3.9	0.86	3.6
4 to 6	717	0.68	31.7	0.68	19.7	0.70	9.7	0.71	5.3	0.64	4.4
7 to 10	334	0.63	41.0	0.63	24.9	0.60	11.6	0.22	5.7	0.00	4.5
11 to 100	622	0.55	57.8	0.51	34.1	0.26	14.7	0.00	6.4	0.00	4.6
101 to 1000	81	0.23	96.7	0.00	51.6	0.00	19.1	0.00	7.1	0.00	4.7
Table 7: Coverage and size conditional on difficulty. We report coverage and size of RAPS sets using ResNet-
152 with kreg = 5 and varying λ (recall that λ = 0 is the APS procedure). The desired coverage level is 90%.
The ‘difficulty’ is the ranking of the true class’s estimated probability.
Model	Violation at α = 10%		Violation at α = 5%	
	APS	RAPS	APS	RAPS
ResNeXt101	0.090	0.049	0.048	0.021
ResNet152	0.069	0.038	0.037	0.017
ResNet101	0.073	0.041	0.038	0.017
ResNet50	0.069	0.037	0.037	0.016
ResNet18	0.046	0.025	0.032	0.019
DenseNet161	0.080	0.047	0.040	0.016
VGG16	0.046	0.022	0.030	0.022
Inception	0.085	0.045	0.043	0.023
ShuffleNet	0.061	0.033	0.035	0.020
Table 8: Adaptiveness results after automatically tuning λ. We report the median size-stratified coverage
violations of APS and RAPS over 10 trials. See Appendix E.2 for experimental details.
E CHOOSING kreg AND λ TO OPTIMIZE SET SIZE AND ADAPTIVENES S
This section describes two procedures for picking kreg and λ that optimize for set size or adaptive-
ness, outperforming APS in both cases.
E.1 Optimizing set size with RAPS
Algorithm 4 Adaptive Fixed-K
Input: α; I ∈ {1, ..., K}n×K, and one-hot y ∈ {0, 1}K corresponding respectively to the classes
from highest to lowest estimated probability mass, and labels for each of n examples in the
dataset
1:	procedure GET-KSTAR(α,I,y)
2:	for i ∈ {1,…,n} do
3：	Li — { j : Iij= y }
4:	k — the d(1 一 α)(1 + n)] Iargestvaluem {Li}n=ι
5:	return k*
Output: The estimate of the smallest fixed size set that achieves coverage, k*
To produce Tables 1, 5, 2, and 6, we chose kreg and λ adaptively. This required an extra data splitting
step, where a small amount of tuning data xi, yi im=1 were used to estimate k*, and then kreg is set
to k*. Taking m ≈ 1000 was sufficient, since the algorithm is fairly insensitive to kreg (see Table 3).
Then, k* was calculated with Algorithm 4. We produced the Imagenet V2 tables with m = 1000
and the Imagenet tables with m = 10000.
After choosing fz*
We chose λ to have small set size. We used the same tuning data to pick Q and λ
for simplicity (this does not invalidate our coverage guarantee since conformal calibration still uses
fresh data). A coarse grid search on λ sufficed, since small parameter variations have little impact
on RAPS. For example, we chose the λ ∈ {0.001, 0.01, 0.1, 0.2, 0.5} that achieved the smallest size
on the m holdout samples in order to produce Tables 1, 5, 2, and 6. We include a subroutine that
automatically chooses k* and λ to optimize size in our GitHub codebase.
14
Published as a conference paper at ICLR 2021
E.2 Optimizing adaptiveness with RAPS
In this appendix, we show empirically that RAPS with an automatically chosen set of kreg and λ
improves the adaptiveness of APS. Recall our discussion in Section 4 and Proposition 3, wherein we
propose size-stratified coverage as a useful definition of adaptiveness in image classification. After
picking kreg as in Appendix E, we can choose λ using the same tuning data to optimize this notion
of adaptiveness.
We now describe a particular manifestation of our adaptiveness criterion that we will use to optimize
λ. Consider disjoint set-size strata {Si}ii==s1, where Sjj==s1 Si = {1, . . . , |Y |}. Then define the indexes
of examples stratified by the prediction set size of each example from algorithm C as Jj = i :
|C(Xi, Yi, Ui)| ∈ Sj . Then we can define the size-stratified coverage violation of an algorithm C
on strata {S}ii==s1 as
SSCV(C, {S}j=1) = SUp |{i: Yi ∈c(Xi,zYi,Ui), i ∈ Jj}l - (1 - α) .	(5)
j	|Jj |
In words, Eq. (5) is the worst-case deviation of C from exact coverage when it outputs sets of a
certain size. Computing the size-stratified coverage violation thus only requires post-stratifying the
results of C on a set of labeled examples. If conditional coverage held, the worst stratum coverage
violation would be 0 by Proposition 3.
To maximize adaptiveness, we’d like to choose λ to minimize the size-stratified coverage violation
of RAPS. Write Cλ to mean the RAPS procedure for a fixed choice of kreg and λ. Then we would
like to pick
λ = arg min SSCV(Cλ0, {S}jj==s1).	(6)
In our experiments, we choose a relatively coarse partitioning of the possible set sizes: 0-1, 2-3, 4-
10, 11-100, and 101-1000. Then, we chose the λ ∈ {0.00001, 0.0001, 0.0008, 0.001, 0.0015, 0.002}
which minimized the size-stratified coverage violation on the tuning set. The results in Table 8
show RAPS always outperforms the adaptiveness of APS on the test set, even with this coarse,
automated choice of parameters. The table reports the median size-stratified coverage violation over
10 independent trials of APS and RAPS with automated parameter tuning.
F	Comparison with Least Ambiguous S et-Valued Classifiers
In this section, we compare RAPS to the Least Ambiguous Set-valued Classifier (LAC) method
introduced in Sadinle et al. (2019), an alternative conformal procedure that is designed to have small
sets. The LAC method provable gives the smallest possible average set size in the case where the
input probabilities are correct, with the idea that these sets should be small even when the estimated
probabilities are only approximately correct. In the notation of this paper, the LAC method considers
nested sets of the following form:
CLAC(x,τ):= {y : ∏χ(y) ≥ 1 - T},
which can be calibrated using as before in using 九加 from Eq. (3).
We first compare naive, APS, RAPS, and LAC in terms of power and coverage in Table 9. In this
experiment, we tuned RAPS to have small set size as described in Appendix E.1. We see that LAC
also achieves correct coverage, as expected since it is a conformal method and satisfies the guarantee
from Theorem 1. We further see that it has systematically smaller sets that RAPS, although the
difference is slight compared to the gap between APS and RAPS or APS and LAC.
We next compare RAPS to LAC in terms of adaptiveness, tuning RAPS as in Section E.2. First, in
Table 10, we report on the coverage of LAC for images of different difficulties, and see that LAC
has dramatically worse coverage for hard images than for easy ones. Comparing this to RAPS in
Table 7, we see that RAPS also has worse coverage for more difficult images, although the gap is
much smaller for RAPS. Next, in Table 11, we report on the SSCV metric for of adaptiveness (and
conditional coverage) for APS, RAPS, and LAC. We find that APS and RAPS have much better
adaptiveness than LAC, with RAPS being the overall winner. The results of all of these comparisons
are expected: LAC is not targetting adpativeness and instead trying to achieve the smallest possible
set size. It succeeds at its goal, sacrificing adaptiveness to do so.
15
Model	Accuracy		Coverage					Size				
	Top-1	Top-5	Top K	Naive	APS	RAPS	LAC	Top K	Naive	APS	RAPS	LAC
ResNeXt101	0.793	0.945	0.900	0.889	0.900	0.900	0.900	2.42	17.2	19.9	2.01	1.65
ResNet152	0.783	0.941	0.900	0.894	0.900	0.900	0.900	2.64	9.68	10.4	2.09	1.76
ResNet101	0.774	0.936	0.900	0.895	0.900	0.900	0.900	2.83	10.0	10.8	2.25	1.87
ResNet50	0.761	0.929	0.899	0.896	0.900	0.900	0.900	3.13	11.7	12.3	2.55	2.05
ResNet18	0.698	0.891	0.900	0.895	0.900	0.900	0.900	5.74	15.3	16.1	4.38	3.64
DenseNet161	0.771	0.936	0.900	0.894	0.900	0.900	0.900	2.84	11.2	12.0	2.29	1.90
VGG16	0.716	0.904	0.900	0.896	0.901	0.900	0.900	4.75	13.4	14.1	3.54	3.00
Inception	0.695	0.886	0.899	0.884	0.900	0.899	0.900	6.27	74.8	88.8	5.24	4.06
ShuffleNet	0.694	0.883	0.900	0.892	0.900	0.899	0.900	6.45	28.8	32.1	5.01	4.14
Table 9: Results on Imagenet-Val. We report coverage and size of the optimal, randomized fixed sets, naive, APS, RAPS , and the LAC sets for nine different Imagenet classifiers.
The median-of-means for each column is reported over 100 different trials at the 10% level. See Section 3.1 for full details.
Published as a conference paper at ICLR 2021
difficulty	count	cvg	sz
1	15668	1.00	1.5
2 to 3	2578	0.81	2.6
4 to 6	717	0.23	3.0
7 to 10	334	0.00	2.9
11 to 100	622	0.00	2.7
101 to 1000	81	0.00	2.4
Table 10: Coverage and size conditional on difficulty. We report coverage and size of the LAC sets for
ResNet-152.
Model	Violation at α		二 10%	Violation at α		= 5%
	APS	RAPS	LAC	APS	RAPS	LAC
ResNeXt101	0.086	0.047	0.217	0.047	0.022	0.127
ResNet152	0.067	0.039	0.156	0.04	0.021	0.141
ResNet101	0.075	0.060	0.152	0.039	0.015	0.120
ResNet50	0.071	0.042	0.131	0.037	0.014	0.109
ResNet18	0.050	0.024	0.196	0.031	0.021	0.059
DenseNet161	0.076	0.055	0.140	0.039	0.016	0.110
VGG16	0.051	0.023	0.165	0.029	0.019	0.070
Inception	0.086	0.042	0.181	0.043	0.023	0.135
ShuffleNet	0.058	0.030	0.192	0.033	0.019	0.045
Table 11: Adaptiveness results after automatically tuning λ. We report the median SSCV of APS RAPS
and LAC over 10 trials. See Appendix E.2 for experimental details.
17