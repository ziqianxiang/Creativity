Published as a conference paper at ICLR 2021
On Statistical Bias In Active Learning: How
and When to Fix It
Sebastian Farquhart*, Yarin GaH, Tom RainfDrth兴
University of Oxford,t OATML, Department of Computer Science; ^Department of Statistics
Ab stract
Active learning is a powerful tool when labelling data is expensive, but it introduces
a bias because the training data no longer follows the population distribution. We
formalize this bias and investigate the situations in which it can be harmful and
sometimes even helpful. We further introduce novel corrective weights to remove
bias when doing so is beneficial. Through this, our work not only provides a
useful mechanism that can improve the active learning approach, but also an
explanation of the empirical successes of various existing approaches which ignore
this bias. In particular, we show that this bias can be actively helpful when training
overparameterized models—like neural networks—with relatively little data.
1	Introduction
In modern machine learning, unlabelled data can be plentiful while labelling requires scarce resources
and expert attention, for example in medical imaging or scientific experimentation. A promising
solution to this is active learning—picking the most informative datapoints to label that will hopefully
let the model be trained in the most sample-efficient way possible (Atlas et al., 1990; Settles, 2010).
However, active learning has a complication. By picking the most informative labels, the acquired
dataset is not drawn from the population distribution. This sampling bias, noted by e.g., MacKay
(1992); Dasgupta & Hsu (2008), is worrying: key results in machine learning depend on the training
data being identically and independently distributed (i.i.d.) samples from the population distribution.
For example, we train neural networks by minimizing a Monte Carlo estimator of the population risk.
If training data are actively sampled, that estimator is biased and we optimize the wrong objective.
The possibility of bias in active learning has been considered by e.g., Beygelzimer et al. (2009); Chu
et al. (2011); Ganti & Gray (2012), but the full problem is not well understood. In particular, methods
that remove active learning bias have been restricted to special cases, so it has been impossible to
even establish whether removing active learning bias is helpful or harmful in typical situations.
To this end, we show how to remove the bias introduced by active learning with minimal changes
to existing active learning methods. As a stepping stone, we build a Plain Unbiased Risk Estimator,
RPURE, which applies a corrective weighting to actively sampled datapoints in pool-based active
learning. Our Levelled Unbiased Risk Estimator, RLURE, builds on this and has lower variance
and additional desirable finite-sample properties. We prove that both estimators are unbiased and
consistent for arbitrary functions, and characterize their variance.
Interestingly, we find—both theoretically and empirically—that our bias corrections can simultane-
ously also reduce the variance of the estimator, with these gains becoming larger for more effective
acquisition strategies. We show that, in turn, these combined benefits can sometimes lead to signif-
icant improvements for both model evaluation and training. The benefits are most pronounced in
underparameterized models where each datapoint affects the learned function globally. For example,
in linear regression adopting our weighting allows better estimates of the parameters with less data.
On the other hand, in cases where the model is overparameterized and datapoints mostly affect the
learned function locally—like deep neural networks—we find that correcting active learning bias can
be ineffective or even harmful during model training. Namely, even though our corrections typically
produce strictly superior statistical estimators, we find that the bias from standard active learning
can actually be helpful by providing a regularising effect that aids generalization. Through this, our
work explains the known empirical successes of existing active learning approaches for training deep
models (Gal et al., 2017b; Shen et al., 2018), despite these ignoring the bias this induces.
* Equal contribution. Corresponding author Sebastian.farquhar@cs.ox.ac.uk.
1
Published as a conference paper at ICLR 2021
To summarize, our main contributions are:
1.	We offer a formalization of the problem of statistical bias in active learning.
2.	We introduce active learning risk estimators, RPURE and RLURE, and prove both are unbiased,
consistent, and with variance that can be less than the naive (biased) estimator.
3.	Using these, we show that active learning bias can hurt in underparameterized cases like
linear regression but help in overparameterized cases like neural networks and explain why.
2	Bias in Active Learning
We begin by characterizing the bias introduced by active learning. In supervised learning, generally,
we aim to find a decision rule fθ corresponding to inputs, x, and outputs, y, drawn from a population
data distribution pdata(x, y) which, given a loss function L(y, fθ(x)), minimizes the population risk:
r	Ex,y~pdata [L(y,fθ(x))] .
The population risk cannot be found exactly, so instead we consider the empirical distribution for
some dataset of N points drawn from the population. This gives the empirical risk: an unbiased and
consistent estimator of r when the data are drawn i.i.d from pdata and are independent of θ,
1N
R = N ΣSn=ι L(yn,fθ (Xn)).
In pool-based active learning (Lewis & Gale, 1994; Settles, 2010), we begin with a large unlabelled
dataset, known as the pool dataset Dpool ≡ {xn|1 ≤ n ≤ N}, and sequentially pick the most useful
points for which to acquire labels. The lack of most labels means We cannot evaluate R directly, so
we use the sub-sample empirical risk evaluated using the M actively sampled labelled points:
R= M XM=I Lymf (Xm)).	⑴
Though almost all active learning research uses this estimator (see Appendix D), it is not an unbiased
estimator of either R or r when the M points are actively sampled. Under active——i.e. non-uniform——
sampling the M datapoints are not drawn from the population distribution, resulting in a bias which
we formally characterize in §4. See Appendix A for a more general overview of active learning.
Note an important distinction between what we will call “statistical bias” and “overfitting bias.” The
bias from active learning above is a statistical bias in the sense that using R biases our estimation of
r, regardless of θ. As such, optimizing θ with respect to R induces bias into our optimization of θ. In
turn, this breaks any consistency guarantees for our learning process: if we keep M/N fixed, take
M → ∞, and optimize for θ, we no longer get the optimal θ that minimizes r. Almost all work on
active learning for neural networks currently ignores the issue of statistical bias.
However, even without this statistical bias, indeed even if we use R directly, the training process
itself also creates an overfitting bias: evaluating the risk using training data induces a dependency
between the data and θ. This is why we usually evaluate the risk on held-out test data when doing
model selection. Dealing with overfitting bias is beyond the scope of our work as this would equate to
solving the problem of generalization. The small amount of prior work which does consider statistical
bias in active learning entirely ignores this overfitting bias without commenting on it.
In §3-6, we focus on statistical bias in active learning, so that we can produce estimators that are valid
and consistent, and let us optimize the intended objective, not so they can miraculously close the
train-test gap. From a more formal perspective, our results all assume that θ is chosen independently
of the training data; an assumption that is almost always (implicitly) made in the literature. This
ensures our estimators form valid objectives, but also has important implications that are typically
overlooked. We return to this in §7, examining the interaction between statistical and overfitting bias.
3	Unbiased Active Learning： RPURE AND RLURE
We now show how to unbiasedly estimate the risk in the form of a weighted expectation over actively
sampled data points. We denote the set of actively sampled points Dtrain ≡ {(Xm, ym)|1 ≤ m ≤ M},
where ∀m : Xm ∈ Dpool. We begin by building a “plain” unbiased risk estimator, RPURE, as a stepping
stone——its construction is quite natural in that each term is individually an unbiased estimator of the
risk. We then use it to construct a “levelled” unbiased risk estimator, RLURE, which is an unbiased
2
Published as a conference paper at ICLR 2021
and consistent estimator of the population risk just like RPURE, but which reweights individual terms
to produce lower variance and resolve some pathologies of the first approach. Both estimators are
easy to implement and have trivial compute/memory requirements.
3.1	RRpure： Plain Unbiased Risk Estimator
For our estimators, we introduce an active sampling proposal distribution over indices rather than the
more typical distribution over datapoints. This simplifies our proofs, but the two are algorithmically
equivalent for pool-based active learning because of the one-to-one relationship between datapoints
and indices. We define the probability mass for each index being the next to be sampled, once
Dtrain contains m - 1 points, as q(im; i1:m-1, Dpool). Because we are learning actively, the proposal
distribution depends on the indices sampled so far (i1:m-1) and the available data (Dpool, note
though that it does not depend on the labels of unsampled points). The only requirement on this
proposal distribution for our theoretical results is that it must place non-zero probability on all of the
training data: anything else necessarily introduces bias. Considerations for the acquisition proposal
distribution are discussed further in §3.3. We first present the estimator before proving the main
results:
1 M	1	m-1
RPURE ≡ M m=m=1 am； Where am ≡ WmLim + N 2^t=1 Lit,	⑵
where the loss at a point Lim ≡ L(yim, fθ (xim)), the weights wm ≡ 1/Nq(im;i1:m-1,Dpool)
and im 〜q(im[iι∙m-ι, Dpool). For practical implementation, RPURE Can further be written in the
following more computationally friendly form that avoids a double summation:
1 M	1	M-m
PURE = M m=1 (Nq(im； il:m-l, Dpool) + JLim '	''
However, we focus on the first form for our analysis because am in (2) has some beneficial properties
not shared by the weighting factors in (3). in particular, in Appendix B.1 we prove that:
Lemma 1. The individual terms am of RPURE are unbiased estimators of the risk: E [am] = r.
The motivation for the construction of am directly originates from constructing an estimator where
Lemma 1 holds while only making use of the observed losses Li1 ,... , Lim , taking care with the fact
that each new proposal distribution does not have support over points that have already been acquired.
Except for trivial problems, am is essentially unique in this regard; naive importance sampling
(i.e. M PMM=I WmLim) does not lead to an unbiased, or even consistent, estimator. However, the
overall estimator RPURE is not the only unbiased estimator of the risk, as we discuss in §3.2.
We can now characterize the behaviour of RPURE as follows (see Appendix B.2 for proof)
Theorem 1. RPURE as defined above has the properties:
E [Rpure^ = r,
Var hRPUREi = Var' (N'θ' " + M2 X EDpool,ii：m-i [Var [wmLim |i1：m-1, Dpool]] ∙ (4)
m=1
Remark 1. The first term of (4) is the variance of the loss on the whole pool, while the second term
accounts for the variance originating from the active sampling itself given the pool. This second term
is O(N/M) times larger and so will generally dominate in practice as typically M N.
Armed with Theorem 1, we can prove the consistency of RPURE under standard assumptions: RPURE
converges in expectation (i.e. its mean squared error tends to zero) as M → ∞ under the assumptions
that N > M, L(y, fθ(x)) is integrable, and q(im; i1:m-1, Dpool) is a valid proposal in the sense that
it puts non-zero mass on each unlabelled datapoint. Formally, as proved in Appendix B.3,
Theorem 2. Let α = N/M and assume that α > 1. If E L(y, fθ(x))2 < ∞ and
∃β > 0 ： min	q(im = n； i1：m-1, Dpool) ≥ β∕N ∀N ∈ Z+,m ≤ N,
n∈{1:N\ii：m-i}
then RPURE converges in its L2 norm to r as M → ∞, i.e., limM →∞ E
kRPURE — r)2i
0.
3
Published as a conference paper at ICLR 2021
〜
3.2 RLURE : LEVELLED UNBIASED RISK ESTIMATOR
RPURE is natural in that each term is an unbiased estimator of r . However, this creates surprising
behaviour given the sequential structure of active learning. For example, with a uniform proposal
distribution—equivalent to not actively learning—points sampled earlier are more highly weighted
〜
〜
than later ones and RRPURE = RR. Specifically, a uniform proposal, q(im; ii：m-i, Dpooi)
1
N-m+1,
gives a weight on each sampled point of 1 + M-2m+1 = 1. Similarly, as M → N (such that We use
the full pool) the weights also fail to become uniform: setting M = N gives a weight for each point
of 1 + M-凯+1 = 1. RLURE fixes this. We first quote the estimator before proving key results:
1 M	N-M
RLURE ≡ Mr VmLim ； Vm ≡ 1 + N - m
m=1
1
(N - m + 1) q(im； iι:m-1 , Dpool)
This estimator ensures that the expected value of the weight, Vm , does not depend on the position it
was sampled in, but only on the probability with which it was sampled. That is, E [Vm] = 1 for all m,
M, N, and q(im； i1:m-1； Dpool). As a consequence the variance is generally lower. Moreover, we
resolve the finite-sample behaviour shown by RPURE. The weights become more even as M increases
for a given N, and when M = N, each Vm = 1 such that RLURE = R = R. Additionally, if the
proposal is uniform, all weights are always exactly 1 such that RLURE = R.
To derive RLURE note that each am estimates r without bias so for any normalized linear combination:
E
PM
m=1 cmam
P ,c
m=1 cm
r,
provided that the cm are constant with respect to the data and sampled indices (they can depend on
M, N, and m). In Appendix B.4 we show that the choice of :
N(N-M)
c
m	(N — m)(N — m + 1)
produces the Vm from (5) and in turn that these Vm have the desired property E [Vm] = 1, ∀m ∈
{1,...,M}. We note also that PM=Icm = M, such that RLURE - MM PM=Icmam. We further
characterise the variance and unbiasedness of RLURE as follows (see Appendix B.5 for proof)
Theorem 3.	RLURE as defined above has the following properties:
E [RLURE^ - r,
Var [L(y, fθ (x))]
N
1M
+ M2 ΣS CmEDPool,ii:m-i [Var [wmLim |i1：m-1, Dpool]] . (6)
m=1
Although not obvious from inspection of (6), in Appendix B.6 we prove that the variance of RLURE is
always less than that of RPURE subject to a mild assumption about the proposal which we detail there.
Theorem 4.	If Equation (14) in Appendix B.6 holds then Var[RLURE] ≤ Var[RPURE]. If M > 1 and
EDpool [Var[wmLi1 |Dpool]] > 0 also hold, then the inequality is strict: Var[RLURE] < Var[RPURE].
To provide intuition into why this result holds, remember that cm were introduced to ensure that E [Vm]
are all identically one. Therefore this weighting removes the tendency of RPURE to overemphasize
the earlier samples; essentially increasing the effective sample size by correcting the imbalance.
We finish by confirming that RLURE is a consistent estimator as M → ∞ (proof in Appendix B.7):
Theorem 5. Under the same assumptions as Theorem 2: IimM→∞ E [(RLURE — r)2] — 0.
3.3 From Active Learning Schemes to Proposals
We have introduced two elements of the active learning scheme: the risk estimators—RPURE and
RLURE—and the acquisition proposal distribution—q(im|i1:m-1, Dpool)—which has so far remained
general. So long as the acquisition proposal puts non-zero mass on all the training data, RPURE and
4
Published as a conference paper at ICLR 2021
RLURE are unbiased and consistent as proven above. This is in contrast to the naive risk estimator R,
for which the choice of proposal distribution affects the bias of the estimator.
It is easy to satisfy the requirement for non-zero mass everywhere. Even prior work which selects
points deterministically (e.g., Bayesian Active Learning by Disagreement (BALD) (Houlsby et al.,
2011) or a geometric heuristic like coreset construction (Sener & Savarese, 2018)) can be easily
adapted. Any scheme, like BALD, that selects the points with argmax can use softmax to return a
distribution. Alternatively, a distribution can be constructed analogous to epsilon-greedy exploration.
With probability we pick uniformly, otherwise we pick the point returned by an arbitrary acquisition
strategy. This adapts any deterministic active learning scheme to allow unbiased risk estimation.
It is also possible to use RLURE and RPURE with data collected using a proposal distribution that does
not fully support the training data, though they will not fully correct the bias in this case. Namely, if
we have a set of points, I, that are ignored by the proposal (i.e. that are assigned zero mass), we can
still use RLURE and RPURE in the same way but they both introduce the same following bias:
E[RIure] = E[RPure] = EE [Rlure ∣Dpooi] - E
Sometimes this bias will be small and may be acceptable if it enables a desired acquisition scheme,
but in general one of the stochastic adaptations described above is likely to be preferable. One
can naturally also extend this result to cases where I varies at each iteration of the active learning
(including deterministic acquisition strategies), for which We again have a non-zero bias.
Though the choices of acquisition proposal and risk estimator are algorithmically detached, choosing
a good proposal will still be critical to performance in practice. In the next section, we will discuss
how the proposal distribution can affect the variance of the estimators, and we will see that our
approaches also offer the potential to reduce the variance of the naive biased estimator. Later, in §7,
we will turn to a third element of active learning schemes—generalization—and consider the fact that
optimization introduces a bias separately from the choice of risk estimator and proposal distribution.
4 Understanding THE Effect of RLURE AND RPURE ON Variance
in order to show that the variance of our unbiased estimators can be lower than that of the biased R,
with a well-chosen acquisition function, we first introduce an analogous result to Theorems 1 and 3
for R, the proof for which is given in Appendix B.8:
Theorem 6.	Let μm := E [Lim ] and μm∣i,D := E [Lim |ii：m—i, Dpool] .For R (defined in (1)):
E hRi = Mm XM=I μm
(6= r in general)
Z	④	`	.—— -----------}∣---------------.
Var[R] = VarDpool [e [々回。^ + + XM=IEDPool,i1~1 [Var 以|ii：m-i, Dpool]]
1M
M2 E EDpool [Var [μm∣i,D ∣Dpool]] +2 ED
m=1'	- γ -	’	।
pool
hcov hLim, Xk<m Lik ∣∣Dpoolii
______________ - /
(7)
+
{.
③
Examining this expression suggests that the variances of RPURE and, in particular, RLURE will often
be lower than that of R, given a suitable proposal. Consider the terms of (7):① is analogous to
the shared first term of (4) and (6), Var [L(y, fθ(x))] /N. if R were an unbiased estimator of R then
these would be exactly equal, but the conditional bias introduced by R also varies between pool
datasets. In general,① will typically be larger than, or similar to, its unbiased counterparts. In any
case, recall that the first terms of (4) and (6) tend to be small contributors to the overall variance
anyway, thus ① provides negligible scope for R to provide notable benefits over our estimators.
We can also relate ② to terms in (4) and (6): it corresponds to the second half of (4), but where we
replace of the expected conditional variances of the weighted losses wm Lim with the unweighted
losses Lim. For effective proposals, wm and Lim should be anticorrelated: high loss points should
have higher density and thus lower weight. This means the expected conditional variance of wm Lim
should be less than Lim for a well-designed acquisition strategy. Variation in the expected value
5
Published as a conference paper at ICLR 2021
of the weights with m can complicate this slightly for RPURE, but the correction factors applied for
RLURE avoids this issue and ensure that the second half of (6) will be reliably smaller than ②.
We have shown that the variance of RLURE IS typically smaller than ① + ② under sensible proposals.
Expression (7) has additional terms:③ is trivially always positive and so contributes to higher variance
for R (it comes from variation in the bias in index sampling given Dpool);④ reflects correlations
between the losses at different iterations which have been eliminated by our estimators. This term is
harder to quantify and can be positive or negative depending on the problem. For example, sampling
points without replacement can cause negative correlation, while the proposal adaptation itself can
cause positive correlations (finding one high loss point can help find others). The former effect
diminishes as N grows, for fixed M, hinting that ④ may tend to be positive for N》M. Regardless,
if ④ is big enough to change which estimator has higher variance then correlation between losses in
different acquired points would lead to high bias in R.
In contrast, we prove in Appendix B.9 that under an optimal proposal distribution both RPURE and
RLURE become exact estimators of the empirical risk for any number of samples M —such that they
will inevitably have lower variance than R in this case. A similar result holds when we are estimating
gradients of the loss, though note that the optimal proposal is different in the two cases.
Theorem 7.	Given a non-negative loss, the optimal proposal distribution
q (im; il:m-l, DPOOI) = Lim /'nEii..m-i Ln
yields estimators exactly equal to the pool risk, that is RPURE = RLURE = R almost surely ∀M.
In practice, it is impossible to sample using the optimal proposal distribution. However, we make this
point in order to prove that adopting our unbiased estimator is certainly capable of reducing variance
relative to standard practice if appropriate acquisition strategies are used. It also provides interesting
insights into what makes a good acquisition strategy from the perspective of the risk estimation itself.
5	Related Work
Pool-based active learning (Lewis & Gale, 1994) is useful in cases where input data are prevalent
but labeling them is expensive (Atlas et al., 1990; Settles, 2010). The bias from selective sampling
was noted by MacKay (1992), but dismissed from a Bayesian perspective based on the likelihood
principle. Others have noted that the likelihood principle remains controversial (Rainforth, 2017),
and in this case would assume a well-specified model. Moreover, from a discriminative learning
perspective this bias is uncontentiously problematic. Lowell et al. (2019) observe that active learning
algorithms and datasets become coupled by active sampling and that datasets often outlive algorithms.
Despite the potential pitfalls, in deep learning this bias is generally ignored. As an informal survey,
we examined the 15 most-cited peer-reviewed papers citing Gal et al. (2017b), which considered
active learning to image data using neural networks. Of these, only two mentioned estimator bias but
did not address it while the rest either ignored or were unaware of this problem (see Appendix D).
There have been some attempts to address active learning bias, but these have generally required
fundamental changes to the active learning approach and only apply to particular setups. Beygelzimer
et al. (2009), Chu et al. (2011), and (Cortes et al., 2019) apply importance-sampling corrections
(Sugiyama, 2006; Bach, 2006) to online active learning. Unlike pool-based active learning, this
involves deciding whether or not to sample a new point as it arrives from an infinite distribution.
This makes importance-sampling estimators much easier to develop, but as Settles (2010) notes, “the
pool-based scenario appears to be much more common among application papers.”
Ganti & Gray (2012) address unbiased active learning in a pool-based setting by sampling from the
pool with replacement. This effectively converts pool-based learning into a stationary online learning
setting, although it overweights data that happens to be sampled early. Sampling with replacement is
unwanted in active learning because it requires retraining the model on duplicate data which is either
impossible or wasteful depending on details of the setting. Moreover, they only prove the consistency
of their estimator under very strong assumptions (well-specified linear models with noiseless labels
and a mean-squared-error loss). Imberg et al. (2020) consider optimal proposal distributions in an
importance-sampling setting. Outside the context of active learning, Byrd & Lipton (2019) question
the value of importance-weighting for deep learning, which aligns with our findings below.
6
Published as a conference paper at ICLR 2021
6	APPLYING RLURE AND RPURE
We first verify that RLURE and RPURE remove the
bias introduced by active learning and examine
the variance of the estimators. We do this by tak-
ing a fixed function whose parameters are inde-
pendent of Dtrain and estimating the risk using
actively sampled points. We note that this is equiv-
alent to the problem of estimating the risk of an
already trained model in a sample-efficient way
given unlabelled test data. We consider two set-
tings: an inflexible model (linear regression) on
toy but non-linear data and an overparameterized
model (convolutional Bayesian neural network)
on a modified version of MNIST with unbalanced
classes and noisy labels.
Linear regression. For linear functions, remov-
ing active learning bias (ALB), i.e., the statistical
bias introduced by active learning, is critical. We
illustrate this in Figure 1. Actively sampled points
overrepresent unusual parts of the distribution, so
a model learned using the unweighed Dtrain differs
Figure 1: Illustrative linear regression. Ac-
tive learning deliberately over-samples unusual
points (red x’s) which no longer match the pop-
ulation (black dots). Common practice uses the
biased unweighted estimator R which puts too
much emphasis on unusual points. Our unbiased
estimators RPURE and RLURE fix this, learning
a function using only Dtrain nearly equal to the
ideal you would get if you had labels for the
whole of Dpool, despite only using a few points.
from the ideal function fit to the whole of Dpool. Using our corrective weights more closely approxi-
mates the ideal line. The full details of the population distribution and geometric acquisition proposal
distribution are in Appendix C.1, where we also show results using an alternative epsilon-greedy
proposal. We inspect the ALB in Figure 2a by comparing the estimated risk (with squared error loss
and a fixed function) to the corresponding true population risk R. While M < N, the unweighted R
is biased (in practice we never have M = N as then actively learning is unnecessary). RPURE and
RLURE are unbiased throughout. However, they have high variance because the proposal is rather
poor. Shading represents the std. dev. of the bias over 1000 different acquisition trajectories.
Bayesian Neural Network. We actively classify MNIST and FashionMNIST images using a
convolutional Bayesian neural network (BNN) with roughly 80,000 parameters. In Figure 2b and 2c
we show that RPURE and RLURE remove the ALB. Here the variance of RPURE and RLURE is lower
or similar to the biased estimator. This is because the acquisition proposal distribution, a stochastic
relaxation of the Bayesian Active Learning by Disagreement (BALD) objective (Houlsby et al., 2011),
is effective (c.f. §4). A full description of the dataset and procedure is provided in Appendix C.2. Our
modified MNIST dataset is unbalanced and has noisy labels, which makes the bias more distorting.
Overall, Figure 2 shows that our estimators remove the bias introduced by active learning, as
expected, and can do so with reduced variance given an acquisition proposal distribution that puts a
high probability mass on more informative/surprising high-expected-loss points.
(a) Linear regression.
0.：
0.
Ξ -0.:
,1K -0..
≡ -0：
-1.
-1.:
∣,25 `
∣,00 `
∣.25 ∙
∣.50 ∙
∣.75 ∙
.00 ∙
.25 -
--RPURE
- RLURE
0	10	20	30	40	50	60	70
M
(b)	BNN: MNIST.
0.20 -
0.15 -
0.10 -
0.05 -
0.00 -
-0.05 -
-0.10 -
-0.15 -
-0.20 -
----RPURE
----RLURE
0	10	20	30	40	50	60	70
(c)	BNN: FashionMNIST.
Figure 2: RPURE and RLURE remove bias introduced by active learning, while unweighted R, which
most active learning work uses, is biased. Note the sign: R overestimates risk because active learning
samples the hardest points. Variance for RPURE and RLURE depends on the acquisition distribution
placing high weight on high-expected-loss points. In (b), the BALD-style distribution means that the
variance of the unbiased estimators is smaller. For FashionMNIST, (c), active learning bias is small
and high variance in all cases. Shading is ±1 standard deviation.
7
Published as a conference paper at ICLR 2021
20	40	60	80	100
M
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
10	20	30	40	50	60
Trained with R
t- Trained with RP URE
T- Trained with RL URE
Trained with R
—Trained with R PURE
T- Trained with R LURE
(a) Linear regression. Test MSE.
」is≈s31 -es=duJ3
10	20	30	40	50	60
M
50
25
00
75
50
25
00
75
一is≈s31-e∙-duJ1
(b) MNIST: Test NLL.
」is≈s31 -e∙-dE3
(d) FashionMNIST: Test NLL. (e) MNIST (MCDO): Test NLL. (f) MNIST (Balanced): Test NLL.
Figure 3: For linear regression, the models trained with RPURE or RLURE have lower ‘population’
risk. In contrast, BNNs trained with RLURE or RPURE perform either similarly (e) or slightly worse
(b,c,d,f), even though they remove bias and have lower variance. Shading is one standard deviation.
For (a) 1000 samples and ‘r’ estimated on 10,100 points from distribution. For (b)/(c) 45 samples
and ‘r’ estimated on the test dataset. (d)-(f) are alternative settings to validate consistency of result.
Next, we examine the overall effect of using the unbiased estimators to learn a model on downstream
performance. Intuitively, removing bias in training while also reducing the variance ought to improve
the downstream task objective: test loss and accuracy. To investigate this, We train models using R,
RLURE, and RPURE with actively sampled data and measure the population risk of each model.
For linear regression (Figure 3a), the new estimators improve the test loss—even with small numbers
of acquired points we have nearly optimal test loss (estimated with many samples). However, for
the BNN, there is a small but significant negative impact on the full test dataset loss of training with
RLURE or RPURE (Figure 3b) and a slightly larger negative impact on test accuracy (Figure 3c). That
is, we get a better model by training using a biased estimator with higher variance!
To validate this further, we consider models trained instead on FashionMNIST (Fig. 3d), on MNIST
but with Monte Carlo dropout (MCDO) (Gal & Ghahramani, 2015) (Fig. 3e), and on a balanced
version of the MNIST data (Fig. 3f). In all cases we find similar patterns, suggesting the effects are
not overly sensitive to the setting. Further experiments and ablations can be found in Appendix C.2.
7	Active Learning Bias in the Context of Overall Bias
In order to explain the finding that RLURE hurts training for the BNN, we return to the bias introduced
by overfitting, allowing us to examine the effect of removing statistical bias in the context of overall
bias. Namely, we need to consider the fact that training would induce am overfitting bias (OFB) even
if we had not used active learning. If we optimize parameters θ according to R, then E[R(θ*)] = r,
because the optimized parameters θ* tend to explain training data better than unseen data. Using
RLURE, which removes statistical bias, we can isolate OFB in an active learning setting. More
formally, supposing we are optimizing any of the discussed risk estimators (which we will write
using R(∙) as a placeholder to stand for any of them) we define the OFB as:
BOFB(R(.)) = r - Rlure(θ ) where θ = arg mm©(R(.))
BOFB(R(.)) depends on the details of the optimization algorithm and the dataset. Understanding
it fully means understanding generalization in machine learning and is outside our scope. We can
still gain insight into the interaction of active learning bias (ALB) and OFB. Consider the possible
relationships between the magnitudes of ALB and OFB: [ALB >> OFB] Removing ALB reduces
overall bias and is most likely to occur when fθ is not very expressive such that there is little chance of
overfitting. [ALB << OFB] Removing ALB is irrelevant as model has massively overfit regardless.
[ALB ≈ OFB] Here sign is critical. If ALB and OFB have opposite signs and similar scale, they
8
Published as a conference paper at ICLR 2021
(a) Linear regression.
(b) BNN: MNIST.
20.
17.,
5
0
5
0
7.,
5
0
2.,
5
0..
0
Trained with: R
Trained with: RlPURE
Trained with: RLURE
ɑ 15.
0 5.
^⅛ 12.1
if 10.
(c) BNN: FashionMNIST.
0
Figure 4: Overfitting bias—BOFB—for models trained using the three objectives. (a) Linear regression,
BOFB is small compared to ALB (c.f. Figure 2a). Shading IQR. 1000 trajectories. (b) BNN, BOFB is
similar scale and opposite magnitude to ALB (c.f. Figure 2b). (c) BNN on FashionMNIST, OFB is
somewhat larger than with MNIST, particularly for R (i.e. our approaches reduce overfitting) and
dominates active learning bias (c.f. Figure 2c). Shading ±1 standard error. 150 trajectories.
will tend to cancel each other out. Indeed, they usually have opposite signs. BOFB is usually positive:
θ* fits the training data better than unseen data. ALB is generally negative: we actively choose
unusual/surprising/informative points which are harder to fit than typical points.
Therefore, when significant overfitting is possible, unless ALB is also large, removing ALB will have
little effect and can even be harmful. This hypothesis would explain the observations in §6 ifwe were
to show that BOFB was small for linear regression but had a similar magnitude and opposite sign to
ALB for the BNN. This is exactly what we show in Figure 4.
Specifically, we see that for linear regression, the BOFB for models trained with R, RPURE, and RLURE
are all small (Figure 4a) when contrasted to the ALB shown in Figure 2a. Here ALB >> OFB;
removing ALB matters. For BNNs we instead see that the OFB has opposite sign to the ALB but
is either similar in scale for MNIST (Figures 2b and 4b), or the OFB is much larger than ALB for
Fashion MNIST (Figures 4c and 2c). The two sources of bias thus (partially) cancel out. Essentially,
using active learning can be treated (quite instrumentally) as an ad hoc form of regularization. This
explains why removing ALB can hurt active learning with neural networks.
8	Conclusions
Active learning is a powerful tool but raises potential problems with statistical bias. We offer
a corrective weighting which removes that bias with negligible compute/memory costs and few
requirements-it suits standard pool-based active learning without replacement. It requires a non-
zero proposal distribution over all unlabelled points but existing acquisition functions can be easily
transformed into sampling distributions. Indeed, estimates of scores like mutual information are so
noisy that many applications already have an implicit proposal distribution.
We show that removing active learning bias (ALB) can be helpful in some settings, like linear
regression, where the model is not sufficiently complex to perfectly match the data, such that the
exact loss function and input data distribution are essential in discriminating between different
possible (imperfect) model fits. We also find that removing ALB can be counter-productive for
overparameterized models like neural networks, even if its removal also reduces the variance of the
estimators, because here the ALB can help cancel out the bias originating from overfitting. This leads
to the interesting conclusion that active learning can be helpful not only as a mechanism to reduce
variance as it was originally designed, but also because it introduces a bias that can be actively helpful
by regularizing the model. This helps explain why active learning with neural networks has shown
success despite using a biased risk estimator.
We propose the following rules of thumb for deciding when to embrace or correct the bias, noting that
we should always prefer RLURE to RPURE. First, the more closely the acquisition proposal distribution
approaches the optimal distribution (as per Theorem 7), the relatively better RLURE will be to R.
Second, the less overfitting we expect, the more likely it is that RLURE will be useful as it reduces
the chance that the ALB will actually help. Third, RLURE will tend to have more of an effect for
highly imbalanced datasets, as the biased estimator will over-represent actively selected but unlikely
datapoints. Fourth, if the training data does not accurately represent the test data, using RLURE will
likely be less important as the ALB will tend to be dwarfed by bias from the distribution shift. Fifth,
at test-time, where optimization and overfitting bias are no-longer an issue, there is little cost to using
RLURE to evaluate a model and it will usually be beneficial. This final application, of active learning
for model evaluation, is an interesting new research direction that is opened up by our estimators.
9
Published as a conference paper at ICLR 2021
Acknowledgements
The authors would like to especially thank Lewis Smith for his helpful conversations and specifically
for his assistance with the proof of Theorem 4. In addition, we would like to thank for their
conversations and advice Joost van Amersfoort and Andreas Kirsch.
The authors are grateful to the Engineering and Physical Sciences Research Council for their support
of the Centre for Doctoral Training in Cyber Security, University of Oxford as well as the Alan Turing
Institute.
References
Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. In Eighth International
Conference on Learning Representations, April 2020.
Les Atlas, David Cohn, and Richard Ladner. Training Connectionist Networks with Queries and
Selective Sampling. Neural Information Processing Systems, 1990.
Francis Bach. Active learning for mispecified generalized linear models. Neural Information
Processing Systems, 19, 2006.
Sima Behpour, Anqi Liu, and Brian Ziebart. Active learning for probabilistic structured prediction of
cuts and matchings. volume 97 of Proceedings ofMachine Learning Research, pp. 563-572, Long
Beach, California, USA, 09-15 Jun 2019. PMLR.
William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M Kohler. The Power of Ensembles
for Active Learning in Image Classification. CVPR, 2018.
Alina Beygelzimer, Sanjoy Dasgupta, and John Langfor. Importance weighted active learning.
International Conference on Machine Learning, 26, 2009.
Jonathon Byrd and Zachary C. Lipton. What is the Effect of Importance Weighting in Deep Learning?
ICML, June 2019.
Jiaoyan Chen, Yan Zhou, Alexander Zipf, and Hongchao Fan. Deep Learning From Multiple Crowds:
A Case Study of Humanitarian Mapping. IEEE Transactions on Geoscience and Remote Sensing,
57(3):1713-1722, March 2019. ISSN 1558-0644. doi: 10.1109/TGRS.2018.2868748.
Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, and Belle Tseng. Unbiased online active
learning in data streams. KDD ’11, pp. 195-203, 2011.
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy
Liang, Jure Leskovec, and Matei Zaharia. Selection via Proxy: Efficient Data Selection for Deep
Learning. In Eighth International Conference on Learning Representations, April 2020.
Corinna Cortes, Giulia Desalvo, Mehryar Mohri, Ningshan Zhang, and Claudio Gentile. Active
learning with disagreement graphs. volume 97 of Proceedings of Machine Learning Research, pp.
1379-1387, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. International Conference
on Machine Learning, 25, 2008.
Sebastian Farquhar, Michael Osborne, and Yarin Gal. Radial Bayesian Neural Networks: Robust
Variational Inference In Big Models. Proceedings of the 23rd International Conference on Artificial
Intelligence and Statistics, 2020.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning. Proceedings of the 33rd International Conference on Machine
Learning, 48:1050-1059, 2015.
Yarin Gal, Jiri Hron, and Alex Kendall. Concrete Dropout. Neural Information Processing Systems,
pp. 10, 2017a.
10
Published as a conference paper at ICLR 2021
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian Active Learning with Image Data.
Proceedings of The 34th International Conference on Machine Learning, 2017b.
Ravi Ganti and Alexander Gray. Upal: Unbiased pool based active learning. Artificial Intelligence
and Statistics, 15, 2012.
Sambuddha Ghosal, Bangyou Zheng, Scott C. Chapman, Andries B. Potgieter, David R. Jordan,
Xuemin Wang, Asheesh K. Singh, Arti Singh, Masayuki Hirafuji, Seishi Ninomiya, Baskar
Ganapathysubramanian, Soumik Sarkar, and Wei Guo. A Weakly Supervised Deep Learning
Framework for Sorghum Head Detection and Counting, 2019. Publisher: Science Partner Journal
Volume: 2019.
Juan Mario Haut, Mercedes E. Paoletti, Javier Plaza, Jun Li, and Antonio Plaza. Active Learning With
Convolutional Neural Networks for Hyperspectral Image Classification Using a New Bayesian
Approach. IEEE Transactions on Geoscience and Remote Sensing, 56(11):6440-6461, November
2018. ISSN 1558-0644. doi: 10.1109/TGRS.2018.2838665. Conference Name: IEEE Transactions
on Geoscience and Remote Sensing.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. CVPR, 7(3):171-180, 2016.
Neil Houlsby, Ferenc Husz虹,Zoubin Ghahramani, and Mgt6 Lengyel. Bayesian Active Learning for
Classification and Preference Learning. arXiv, 2011.
Peiyun Hu, Zachary C. Lipton, Anima Anandkumar, and Deva Ramanan. Active Learning with
Partial Feedback. In International Conference on Learning Representations, 2019.
Sheng-Jun Huang, Jia-Wei Zhao, and Zhao-Yang Liu. Cost-Effective Training of Deep CNNs with
Active Model Adaptation. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD ’18, pp. 1580-1588, New York, NY, USA, July 2018.
Association for Computing Machinery. ISBN 978-1-4503-5552-0.
Henrik Imberg, Johan Jonasson, and Marina Axelson-Fisk. Optimal sampling in unbiased active
learning. Artificial Intelligence and Statistics, 23, 2020.
Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. Introduction to
variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.
Benjamin Kellenberger, Diego Marcos, Sylvain Lobry, and Devis Tuia. Half a Percent of Labels is
Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning.
IEEE Transactions on Geoscience and Remote Sensing, 57(12):9524-9533, December 2019. ISSN
0196-2892. Publisher: Institute of Electrical and Electronics Engineers.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. BatchBALD: Efficient and Diverse Batch
Acquisition for Deep Bayesian Active Learning. arXiv:1906.08158 [cs, stat], 2019.
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal DaumanL and John Langford. Active
learning for cost-sensitive classification. volume 70 of Proceedings of Machine Learning Research,
pp. 1915-1924, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
David D. Lewis and William A. Gale. A sequential algorithm for training text classifiers. In
Proceedings of the 17th annual international ACM SIGIR conference on Research and development
in information retrieval, SIGIR ’94, pp. 3-12, Berlin, Heidelberg, August 1994. Springer-Verlag.
ISBN 978-0-387-19889-7.
David Lowell, Zachary C. Lipton, and Byron C. Wallace. Practical Obstacles to Deploying Active
Learning. Empirical Methods in Natural Language Processing, November 2019.
David J C MacKay. Information-Based Objective Functions for Active Data Selection. Neural
Computation, 4(4):590-604, 1992.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized Prior Functions for Deep Reinforcement
Learning. Neural Information Processing Systems, 2018.
11
Published as a conference paper at ICLR 2021
Tom Rainforth. Automating Inference, Learning, and Design using Probabilistic Programming. PhD
thesis, University of Oxford, 2017.
Tom Rainforth, A Golinski, FrankWood, and Sheheryar Zaidi. Target-aware bayesian inference: how
to beat optimal conventional estimators. Journal of Machine Learning Research, 21(88), 2020.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond.
International Conference on Learning Representations, February 2018.
Ozan Sener and Silvio Savarese. Active Learning for Convolutional Neural Networks: A Core-Set
Approach. In International Conference on Learning Representations, February 2018.
Burr Settles. Active Learning Literature Survey. Machine Learning, 15(2):201-221, 2010.
Yanyao Shen, Hyokun Yun, Zachary Lipton, Yakov Kronrod, and Animashree Anandkumar. Deep
Active Learning for Named Entity Recognition. In International Conference on Learning Repre-
sentations, 2018.
Weishi Shi and Qi Yu. Fast direct search in an optimally compressed continuous target space for
efficient multi-label active learning. volume 97 of Proceedings of Machine Learning Research, pp.
5769-5778, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Aditya Siddhant and Zachary C. Lipton. Deep Bayesian Active Learning for Natural Language
Processing: Results of a Large-Scale Empirical Study. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, pp. 2904-2909, Brussels, Belgium, 2018.
Association for Computational Linguistics.
Samrath Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational Adversarial Active Learning. In
2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5971-5980, Seoul,
Korea (South), October 2019. IEEE. ISBN 978-1-72814-803-8. doi: 10.1109/ICCV.2019.00607.
Masashi Sugiyama. Active learning for misspecified models. Neural Information Processing Systems,
18:1305-1312, 2006.
Iiris Sundin, Peter Schulam, Eero Siivola, Aki Vehtari, Suchi Saria, and Samuel Kaski. Active
learning for decision-making from imbalanced observational data. volume 97 of Proceedings
of Machine Learning Research, pp. 6046-6055, Long Beach, California, USA, 09-15 Jun 2019.
PMLR.
Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-Effective Active Learning
for Deep Image Classification. IEEE Transactions on Circuits and Systems for Video Technology,
January 2017. arXiv: 1701.03551.
Si Wen, Tahsin M. Kurc, Le Hou, Joel H. Saltz, Rajarsi R. Gupta, Rebecca Batiste, Tianhao Zhao,
Vu Nguyen, Dimitris Samaras, and Wei Zhu. Comparison of Different Classifiers with Active
Learning to Support Quality Control in Nucleus Segmentation in Pathology Images. AMIA Joint
Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science,
2017:227-236, 2018. ISSN 2153-4063.
Songbai Yan, Kamalika Chaudhuri, and Tara Javidi. Active Learning with Logged Data. In Interna-
tional Conference on Machine Learning, pp. 5521-5530. PMLR, July 2018. ISSN: 2640-3498.
Jie Yang, Thomas Drake, Andreas Damianou, and Yoelle Maarek. Leveraging Crowdsourcing Data
for Deep Active Learning An Application: Learning Intents in Alexa. In Proceedings of the 2018
World Wide Web Conference, WWW ’18, pp. 23-32, Republic and Canton of Geneva, CHE, April
2018. International World Wide Web Conferences Steering Committee. ISBN 978-1-4503-5639-8.
Donggeun Yoo and In So Kweon. Learning Loss for Active Learning. In 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 93-102, Long Beach, CA, USA, June
2019. IEEE. ISBN 978-1-72813-293-8.
Yao Zhang and Alpha A. Lee. Bayesian semi-supervised learning for uncertainty-calibrated prediction
of molecular properties and active learning. Chemical Science, 10(35):8154-8163, September
2019. doi: 10.1039/C9SC00616H. Publisher: The Royal Society of Chemistry.
12
Published as a conference paper at ICLR 2021
A	Overview of Active Learning
Active learning selectively picks datapoints for which to acquire labels with the aim of more sample-
efficient learning. For an excellent overview of the general active learning problem setting we refer
the reader to Settles (2010).
Since that review was written, a number of significant advances have further developed active learning.
Houlsby et al. (2011) develop an efficient way to estimate the mutual information between model
parameters and the output distribution, which can be used for the Bayesian Active Learning by
Disagreement (BALD) score.
Active learning has been applied to deep learning, especially for vision Gal et al. (2017b); Wang et al.
(2017). In neural networks specifically, empirical work has suggested that simple geometric core-set
style approaches can outperform uncertainty-based acquisition functions (Sener & Savarese, 2018).
A lot of recent work in active learning has focused on speeding up acquisition from a computational
perspective (Coleman et al., 2020) and allowing batch acquisition in order to parallelize labelling
(Kirsch et al., 2019; Ash et al., 2020). Some work has also focused on applying active learning to
specific settings with particular constraints (Krishnamurthy et al., 2017; Yan et al., 2018; Sundin
et al., 2019; Behpour et al., 2019; Shi &Yu, 2019; Hu et al., 2019).
B Proofs
B.1	Proof of Lemma 1
Lemma 1. The individual terms am of RPURE are unbiased estimators of the risk: E [am] = r.
Proof. We begin by applying the tower property of expectations:
1 m-1
E [am] = E WmLim + N X Lit
t=1
Dpool ,i1:m-1
1 m-1
WmLim + N ELit
t=1
Dpool , i1
:m-1
))
By further noting that Eim [WmLim | Dpool, i1:m-1] can be written out analytically as a sum over all
the possible values of im while -N Pm-I Lit is deterministic given Dpooi and ii:m-i We have:
EDpool ,i1:m-1
EDpool ,i1:m-1
But Ln is now independent of the indices which have been sampled:
r.
□
B.2	PROOF OF THE UNBIASEDNESS AND VARIANCE FOR RPURE: THEOREM 1
Theorem 1.	RPURE as defined above has the properties:
E [rpure^ = r,
Var [RPUREi = Var "N∕θ(x))] + M2 X EDpooi,ii：m-i [Var [wmLim |i1:m-1, Dpool]] . (4)
m=1
13
Published as a conference paper at ICLR 2021
Proof. Having established Lemma 1, unbiasedness follows quickly from the linearity of expectations:
1M
M m=ι am
1M
M £E [am] = r.
For the variance we instead have (starting with the definition of RpURE):
Var [Rpure] = E [ (M X am
which by the tower property of expectations
- r2,
2
2
E
E
M XLam
m=1
Dpool
r2,
and from the definition of variance
E
E
Var
Var
M
am
m=1
M
am
m=1
Dpooij + RR2
- r2,
DpooJ + Var hRRi ,
1
M
1
M
which using the fact that RR is a standard Monte Carlo estimator
Var
1M
MEam Dpool	+
m=1
where x, y 〜pd&ta. Now considering the first term We have
Var
1
M E am Dpool
m=1
Var [L(y, fθ (x))]
M X am - R)	Dpool
m1
1MM
M XX E [(am - RR)Qk- R) IDpool]
(8)
(9)
m1 k1
We attack this term by first considering the terms for which m 6= k and show that these yield
E [(am - r)(ak - r)lDpool] = 0, returning to the m = k terms later. We will assume, without loss of
generality, that k < m, noting that by symmetry the same set of arguments can be similarly applied
when m < k . Substituting in the definition of am from equation (2):
E h(am - RR)(ak - RR) |Dpooli
=E ] (wmLim + N X Lit - R) (wk Lik + N X Lis- R)IDPool .
We introduce the notation RRmm = R - N Pm-I Lit to describe the remainder of the empirical risk
ascribable to the datapoint with index im . Then, by multiplying out the terms we have:
E h(am - RR) (ak - RR) IDpooli = E [wmLim wkLik lDpool] - E [wmLimRRkem
L	1	」 X-------------------}	. L
IIDpooli
-E hwkLikRmm ∣Dpooli + E hRRkem
X----------------} X------
■--
、，Dremkr-)
m I pool .
__ - /
^z-<^^^^^^^^^^
Now by the tower property:
(a = E [E [wmLimwkLik | Dpool, i1:m — 1] lDpool] ,
M
M
E
E
M
{z^
⑧
{z^
©
2
|
N
14
Published as a conference paper at ICLR 2021
and noting that because k < m, wkLik is deterministic given Dpool and i1:m-1:
= E [wkLikE [wmLim |Dpool,i1:m-1]|Dpool]
=E [wk Lik RmemlDpooii,
which thus cancels with ©.
The (b) and @ cancel similarly:
® = E [e [wmLim Rkem l Dpool, il:m-l] ∣Dpool]
=E [RkemE [WmLim | Dpool, irm-ι]∣Dpool]
=e [RkemR⅛m ∣Dpool] = @.
Putting this together, we have that:
E [(am — R)(ak — R)∣Dpool] =0 Nk = m.
Considering now the m = k terms we have
E	am
-R)2 Dpool
Eim	(am — R) |i1:m-1, Dpool	Dpool
Ei1:m-1 [Var [am |i1:m-1,Dpool]|Dpool]
Ei1:m-1 [Var [wmLim |i1:m-1, Dpool]|Dpool] .
Finally substituting everything back into (9) and then (8), and applying the tower property gives
Var [RPUREi = Var "N"""] + M2 X EDPool,ii:m-i [Var [wmLim |i1:m-1, Dpool]]
m=1
and We are done.	口
B.3 PROOF OF THE CONSISTENCY OF RPURE : THEOREM 2
Theorem 2.	Let α = N/M and assume that α > 1. If E L(y, fθ(x))2 < ∞ and
∃β> 0： n∈{i*m∕im = n；i1：m-1,DpOOI) ≥ IeN ∀N ∈ Z+,m ≤ N
then RPURE converges in its L2 norm to r as M → ∞, i.e., limM →∞ E
[(-RPURE — r)2i
0.
Proof. Theorem 1 showed that RPURE is an unbiased estimator and so we first note that its MSE is
simply its variance, which we found in (4). Substituting N = αM :
E (RPURE — r)	= Var [Rpure]
Var [L(y,fθ(X)] , 1 X E	Vj. L li D 11
=	OM	+ M2 / √ EDPool,ii:m-i IVar IwmLim |i1：m-1, Dpool]] ∙
m=1
The first term tends to zero as M → ∞ as our standard assumptions guarantee that
α1Var [L(y,fθ(x)] < ∞. For the second term we note that our assumptions about q guarantee
that Wm ≤ 1∕β and thus:
1 m-1	2
Var IwmLim |i1
:m-1 ,Dpool] = E [wmL2m lil:m-l, Dpool] — (R - M ^Lij
1	1 m-1	2
≤ ∣2 E [Lim |il：m-1, Dpool] — (R - M E Lij
< ∞	∀ilm-1, Dpool
15
Published as a conference paper at ICLR 2021
as our assumptions guarantee that 表
losses are finite.
Given that each Var [wmLim |i1:m-1 ,
< ∞, we have Eim Li2	< ∞ and so the empirical risk and
Dpool] is finite, it follows that:
1M
S= M〉： EDpooi,ii：m-i [Var IwmLim |i1：m-1, Dpool]] < ∞,
and we thus have:
Mli→m∞ E	RPURE —
as desired.
R2
lim
M→∞
Var [L(y, fθ (x)]
αM
s2
+ M)=
〜
□
一 . 一	„	I
B.4 DERIVATION OF THE CONSTANTS OF RLURE
We note from before that because of the unbiasedness of am :
E
M
C	Cm am
m=1
1
r where C
Pm=ICm
To construct our improved estimator RLURE, we now need to find the right constants cm , that in turn
lead to overall weights vm (as per (5)) such that E [vm] = 1. We start by substituting in the definition
of am :
RLURE ：= CEvmLim= CE
m=1
m=1
M	C	m-1
Cmam = C £ ( CmwmLim + N £ Lit )，
m=1	t=1
M
M
and then redistributing the Lit frOm later terms where they match m:
1M
vm = Cmwm + N): Ct.
t=m+1
~
Note that though RLURE remains an unbiased estimator of the risk, each individual term vm Lim
is not. Now we require: E [vm] = 1 ∀m ∈ {1, ..., M}. Remembering that wm =
1/(N q(im ; i1
:m-1 , Dpool)):
E [vm] = Cm E
一	1
q(im; i1:m-1,
1M
D^ + N tX+ιc
cm
N
X^	q(im =	n;	i1:m-1 , DPOOI)	+ 1
n∈⅛!-1	q(im =	n;	i1：m-1, DpOOl) N t=m+1	'
Cm (N — m + 1)	1
= N + N,”,Ct.
t=m+1
ImpOsing that each E [vm] = 1, we nOw have M equatiOns fOr M unknOwns C1 , . . . , CM, such that
we can find the required values Of Cm by sOlving the set Of simultaneOus equatiOns:
c 1M
(N — m + 1) N + n E Ct = 1 ∀m ∈{1,...,M}.
t=m+1
We do this by induction. First, consider E [vm] — E [vm+1 ] = 0, for which can be rewritten:
(10)
and thus:
c
(N — m + 1) — — (N — m)
cm+1
cm+1
N — m — 1
Cm = ~ττ	—rCm+1.
N —m+1
(11)
N
+
N
0
16
Published as a conference paper at ICLR 2021
By further noting that the solution for m = M is trivial:
cM
N
N - M +1
we have by induction
_ N	MY1 N -1 - 1
cm = N - M + 1 ] N N -1 + 1
t=m
=N - M +1 exp (X Iog(N -1 - I) - Iog(N -1 + I)).
Now we can exploit the fact that there is a canceling of most of the term in this sum. The exceptions
are - log(N - m + 1), - log(N - m), log(N - M + 1), and log(N - M). We thus have:
N
cm = N - M + J exp (log(N - M) + log(N - M + 1) - log(N - m) - log(N - m + 1))
_	N (N - M)
(N — m)(N — m + 1)
which is our final simple form for cm . We can now check that this satisfies the required recursive
relationship (noting it trivially gives the correct value for cM ) as per (11):
(N — m — 1
cm = (N - m + 1
cm+1
N - m - n	N (N - M)
N - m + 1J (N - m - 1)(N - m)
N (N - M)
(N — m)(N — m + 1)
as required. Similarly, we can straightforwardly show that this form of cm satisfies (10) by substitu-
tion.
We then find the form of vm given this expression for cm . Remember that:
1M
Vm = CmWm + ~N): ct.
t=m+1
We can rearrange (10) to:
1M
N X ct = 1
t=m+1
N — m + 1
-N-cm
—
from which it follows that:
vm = 1 + cm wm
N - m + 1
N
—
Substituting in our expressions for cm and wm we thus have:
V 一 = 1+ _N-M_	________1________________(N-m +1)
m + (N - m)(N - m + 1)	; ii:m-i,f (θm-i),Dp°°ι)	(	+ )
_ N - M (	1	A
vm ɪ + N - m ((N - m + 1) q(im; ii：m-i, Dpool)	ɪj ,
which is the form given in the original expression.
17
Published as a conference paper at ICLR 2021
To finish our definition, we simply need to derive C :
-1
C
cm
M
N(N—M) X
m=1
M
N(N—M) X
m=1
where we now have a telescopic sum so
-1
(N — m)(N - m + 1))
-1
1
N - m
1
N — m + 1
—
-M) (n⅛
11
N
—
1
--
M
We thus see that our vm always sum to M, giving the quoted form for RLURE in the main paper.
B.5 PROOF OF UNBIASEDNESS AND VARIANCE FOR RLURE: THEOREM 3
Theorem 3.	RLURE as defined above has the following properties:
E RRL.LURE^ = r,
Var RRL-.lure^ =
Var [L(y, fθ (x))]
N
1M
+ M2 £ CmEDPool,ii:m-i [Var [wmLim |i1：m-1, Dpool]] ∙⑹
m=1
Proof. LLURE is, by construction a linear combination of the weights am . By Lemma 1 each am is
an unbiased estimator of r. So by the linearity of expectation, E [Llurε] = r.
As in Theorem 1, the variance requires a degree of care because the am are not independent. Noting
that the expectation does not change through the weighting, we analogously have
Var
1M
M £ Cmam
m=1
Dpool	+
Var [L(y, fθ (x))]
N
Similarly, we also have
2
Var
1M
M £ Cmam
Dpool
E I(M X Cmam
Dpool	- RR2
MM
M ΣΣCmCkE [amak |Dpool] — L
m=1 k=1
MM
M XXCmCk (E[αmak IDpool] — R)
m=1 k=1
MM
M XXCm Ck E ham — L ak — LDpool i .
m=1 k=1
Using the result before that
E [(am — L) (ak
Ei1:m-1 [Var [wmLim |i1
:m-1,Dpool]|Dpool]	if m=k
0	otherwise
now gives the desired result by straightforward substitution.
□
18
Published as a conference paper at ICLR 2021
B.6 PROOF THAT RLURE HAS LOWER VARIANCE THAN RRPURE UNDER REASONABLE
Assumptions: Theorem 4
Recall from Theorems 1 and 3 that the variances of the RLURE and RPURE estimators are
Var [Rpure]= V^N≡ + 击 X Em	(12)
m=1
Var [Rlure] = Var Rff (X))] + M X CmEm,	(13)
m=1
where we have used the shorthand Em = EDpool,i1:m-1 [Var[wmLim|i1:m-1,Dpool]]. Recall also that
2 _	N2 (N - M)2
m = (N - m)2(N - m +1)2 .
Though the potential for one to use pathologically bad proposals means that it is not possible to show
that Var [Rlure] ≤ Var [Rpure]
universally holds, we can show this result under a relatively weak
assumption that ensures our proposal is “sufficiently good.
To formalize this assumption, we first define
wm	N - m + 1
Fm= EDpool,ii：m-1 [Var [e [wm | ii：m-i, Dpool] Lim	1 Dpool]] = (-N-)	Em
as the weight-normalized expected variance, where the second form comes from the fact that
E [wm|i1:m-1, Dpool] = (N - m + 1)/N. Our assumption is now that
Fm ≥ FM -m+1 ∀m : 1 ≤ m ≤ M/2.	(14)
Note that a sufficient, but not necessary, condition for this to hold is that the Fm do not increase with
m, i.e. Fm ≥ Fj ∀(m, j) : 1 ≤ m ≤ j ≤ M. Intuitively, this is is equivalent to saying that the
conditional variances of our normalized weighted losses should not increase as we acquire more
points. It is, for example, satisfied by a uniform sampling acquisition strategy (for which all Fm are
equal). More generally, it should hold in practice for sensible acquisition strategies as a) our proposal
should improve on average as we acquire more labels, leading to lower average variance; and b)
higher loss points will generally be acquired earlier so the scaling will typically decrease with m. In
particular, note that E [wmLim |i1:m-1, Dpool] < r and is monotonically decreasing with m because
it omits the already sampled losses (which is why these are added back in when calculating am).
This assumption is actually stronger than necessary: in practice the result will hold even if Fm
increases with m provided the rate of increase is sufficiently small. However, the assumption as
stated already holds for a broad range of sensible proposals and fully encapsulating the minimum
requirements on Fm is beyond the scope of this paper.
We are now ready to formally state and prove our result. For this, however, it is convenient to first
prove the following lemma, which we will invoke multiple times in the main proof.
Lemma 2. If a, b, M, N ∈ N+ are positive integers such that, M < N and a + b ≤ M, then
(N - a)2	(N - M)2
-N2- ≥ (N - b)2 .
Proof.
(N - a)2	(N - M)2 _ (N - a)2(N - b)2 - N2(N - M)2
N2	(N - b)2 =	N2(N - b)2
2N3(M - a - b) + N2(a2 + b2 + 4ab - M2) - 2abN(a + b) + a2b2
N2(N - b)2
1
N2(N - b)2
- M - a - b) + ab
- a - b) + ab
≥ 0 as 2N ≥ M + a + b and M ≥ a + b so all bracketed terms are positive.
□
19
Published as a conference paper at ICLR 2021
Theorem 4. If Equation (14) in Appendix B.6 holds then Var[RLURE] ≤ Var[RPURE]. If M > 1 and
EDpool [Var[wmLi1 |Dpool]] > 0 also hold, then the inequality is strict: Var[RLURE] < Var[RPURE].
Proof. We start by subtracting equation (13) from (12) yielding
1M
Var [Rpure] - Var [Rlure] = M X (1 - Cm)Em
m=1
=M X(1 - cm) (N-N+1 )2 Fm.
m=1
Assuming, for now, that M is even and M < N, we can now group terms into pairs by counting
from each end of the sequence (i.e. pairing the m-th and M - m+1-th terms) to yield
Var
Var
1 M/2
M XSm
m=1
where
Sm = (1 - cm) (N-N≡Fm + (1 - CM-m+1)FM-m+1
((N - m +1)2	(N - M)2 ∖	((N - M + m)2	(N - M)2
=V	N2	(N - m)2 ) m + V	N2	(N - M + m - 1)2
We will now show that Sm
Var [Rpure] ≥ Var [Rlure]
construction.
≥ 0, ∀1 ≤ m ≤ M/2, from which we can directly conclude that
.For this, note that Fm and FM-m+ι are themselves non-negative by
Consider first the case where (N - M + m)2/N2 ≥ (N - M)N/(N - M + m - 1)2. Here the
second term in Sm is non-negative. Furthermore, invoking Lemma (2) with a = m - 1 and b = m
(noting this satisfies a + b ≤ M for all 1 ≤ m ≤ M/2 as required) shows that
(N - m + 1)2	(N - M)2
N2	(N - m)2 ≥
and so the first term is also positive. It thus immediately follows that Sm ≥ 0 in this scenario.
When this does not hold, (N - M + m)2/N2 < (N - M)N/(N - M + m- 1)2 and so the second
term in Sm is negative. We now instead invoke our assumption that Fm ≥ FM-m+1, to yield
S	((N - m + 1)2	(N - M)2	(N - M + m)2	(N - M)2	∖
Sm - Fm V	N2	(N - m)2 +	N2	(N - M + m - 1)2 尸 )
We can now invoke Lemma (2) with a = m - 1 and b = M - m + 1 to show that
(N - m + 1)2	(N - M)2
N2	(N - M + m - 1)2 ≥
and again with a = M - m and b = m to show that
(N - M + m)2	(N - M)2
N2	(N - m)2 ≥ .
Substituting these back into (15) thus again yield the desired result that Sm ≥ 0 as required.
To cover the case where M is odd, we simply need to note that this adds the following additional
term as follows:
Var [Rpure] - Var [Rlure] = ((N - MN2…
—
(N - M)2 ʌ F +工 'MX/2 0
(N - M/2 - 1/2)2) m + MJ m
and we can again invoke Lemma (2) with a = M/2 - 1/2 and b = M/2+ 1/2 to show that this
additional term is non-negative.
20
Published as a conference paper at ICLR 2021
To cover the case where M = N, we simply note that here
S — (N - m +1)2 F l m2 F
Sm = N2 Fm + N2 FM—m+1
where both terms are clearly positive.
We have now shown that Sm ≥ 0 in all possible scenarios given our assumption on Fm, and so we
can conclude that Var [Rpure] ≥ Var [Rlure].
Finally, we need to show the inequality is strict if E1 > 0 and M > 1. For this we first note that
E1 > 0 ensures F1 > 0 and then consider S1 as follows:
(N (N - M)2
SI =(1- (N - 1)2
F1 +
(N -M + 1)2
N2
- 1 FM
and as the second term is clearly negative and F1 ≥ FM,
≥
(N-M+1)2	(N-M)2
N2
(N-1)2
F1
(M-1)(2N2-2MN+M-1)
N2(N - 1)2
F1
—
>0
as M > 1 and N ≥ M ensures that each bracketed term is strictly positive. Now as S1 > 0 and
Sm ≥ 0 for all other m, we can conclude that the sum of the Sm is strictly positive, and thus that the
inequality in strict.	□
B.7 PROOF OF THE CONSISTENCY OF RLURE : THEOREM 5
Theorem 5. Under the same assumptions as Theorem 2: limM →∞ E
h(RLURE - r)2i
0.
Proof. As before, since RLURE is unbiased the MSE is simply the variance so:
E (RPURE - r)	= Var [Rpure]
Var[L(y,fθ(X))] ,	1 XX _2 E	VJ. L ∣i D 11
=	N	+ M2)J cmEDpool,ii：m-i IVar IwmLim |i1：m-1, Dpool]].
m=1
Taking N = αM, we already showed in the proof of Theorem 2 that the first of these terms tends to
zero as M → ∞.
We also showed that EDpool,i1:m-1 Varq(im;i1:m-1,Dpool) IwmLim] is finite given our assumptions. As
such, there must be some finite constant d such that
EDpool,i1:m-1 Varq(im;i1:m-1 ,Dpool) Iwm Lim] < d
and thus
1M	dM
M2 Σ CmEDpool,ii：m-i [Var [wmLim ^l:m-1, Dpool]] < M2 £ Cm
m=1	m=1
_ d XX (	N (N - M)	Y
=M2 红((N - m)(N - m + 1))
21
Published as a conference paper at ICLR 2021
and by substituting N = ɑM
d X (	αM(αM - M)
M2	I(αM — m)(αM — m + 1)
2
αM
M
d
< M2 工 ∖αM - M + 1
m=1 、
d02 M
2
=((a - 1)M + 1)2
which clearly tends to zero as M → ∞ (remembering that α > 1) and We are done.
□
B.8 PROOF OF BIAS AND VARIANCE OF STANDARD ACTIVE LEARNING ESTIMATOR:
THEOREM 6
Theorem 6. Let μm := E [£兀] and μm∣i,D ：= E [£兀 |ii：m-i, Dpool]. For R (defined in (1)):
E [r] = ⅛ XM=1 μm	(= r in general)
① ®
/	人、Z	人、
Var[R] = VarDpoo1 [e [R ∣ Dpool]] + + XM=IEDPool,i1~1 [Var 丸瓦》ι, Dpool]]
1M
+ M2 ΣS EDpool [Var [μm∣i,D ∣ DPool]] +2 EDpool
m=1 |	一：-	J 、
③
⑺
[GOV [Lim ,Xk<m LikI DPo°l]].
一■――	,
{z
Proof. The result of the bias follows immediately from definition of μm and the linearity of expecta-
tions.
For the variance, we have
Var[R] = E [R2] -(E [R])2
=EDpool [e [R2 ∣ Dpool]] - (e [R])2
2
EDpool
Var
2,
同Dpool] + (E 同Dpool])	- (E 师
VarDpool [e [R ∣ Dp00l]] + EDpoojVar 网 Dp。/]
(16)
where the first term is ① from the result. For the second term, introducing the notations μ∣D
E [r∣ Dpool] and μm∣p = E [Cim ∣Dpool] we have
1M
Var [R∣ Dpool] = E I (M• X Lim
2
—μ∣D I Dpool
1 M M
而 XX E [(Lim - μ∣D) (Lik- μ∣D) ∣ Dpool]
m=1 k=1
M M
M XX E KJ
m=1 k=1
-μm∣D + μm∣D - μ∣θ) (Lik - μk∖D + μk∖D - M|D)∣Dpool],
22
Published as a conference paper at ICLR 2021
∣∣D
∣Dpool
multiplying out terms and using the symmetry of m and k
MM
=M E ∑E [(Lim - μm|D) (Lik- μk∣D) ∣Dpool]
m=1 k=1
2 M	1M
+ M E (Lim - μm∣D) I MZ^μk∣D - μ∣D
1M	1M
+ M ΣS(μm∣D - μ∣D) I M∑μk∣D - μ∣D )
where We have exploited symmetries in the indices. Now, as 吉 PM=I μk∣D = μ∣D, the second and
third terms are simply zero, so we have
MM
=M E ∑E [(Lim - μm|D)(Lik- μk∣D)|Dpool],
m=1 k=1
separating out the m = k and m < k terms, with symmetry
1M
=M2 E Var [Lim |DPool] +2 ΣS E [(Lim - μm∣D)(Lik- μk∣D)|Dpool]
m=1	k<m
1M	|
=M2 X Var [Lim |Dpool] + 2E [(Lim - μm∣D) (XJk <m(Lik - μk∣D)) |Dpool]
1M	|
=M2 X Var [Lim lDpool]+2 Cov [Lim , Xk <m LikIDPool] .	(17)
m=1
Here the second term will yield ④ in the result when substituted back into (16). For the first term, we
have by analogous arguments as those used at the start of the proof for Var[R],
Var [Lim IDpool] = Eii：m - 1 [Var [Lim |i1:m-1, Dpool] IDpool]+ Var [μm∣i,D∣Dpool]	(18)
where μm∣i,D := E [Lim ∣i±m-ι, Dpool] as per the definition in the theorem itself. Substituting this
back into (17) and then (16) in turn now yields the desired result through the tower property of
expectations, with the first term in (18) producing ② and the second term producing ③.	□
B.9 Proof of Optimal Proposal Distribution: Theorem 7
Theorem 7. Given a non-negative loss, the optimal proposal distribution
q*(im; il:m-l, Dpool) = Lim/夕九曲1皿-1 Ln
yields estimators exactly equal to the pool risk, that is RPURE = RLURE = R almost surely ∀M.
Proof. We start by proving the result for the simpler case of RPURE before considering RLURE. To
make the notation simpler, we will introduce hypothetical indices it for t > M, noting that their
exact values will not change the proof provided that they are all distinct to each other and the real
indices (i.e. that they are a possible realization of the active sampling process in the setting M = N).
23
Published as a conference paper at ICLR 2021
For RPURE, the proof follows straightforwardly from substituting the definition of the optimal proposal
into the am form of the estimator
1M
RRPURE = M〉: am
t=1
and because all possible indices are uniquely visited
RL
The proof proceeds identically for the case of RLim because the gradient passes through the Summa-
tions.
For RLURE, we similarly substitute the optimal proposal into the definition of the estimator
1M
RLURE = M	VmLim
m=1
__ 1 XX N — M
=M T Lm + N — m
m=1
Li
m
(N — m + 1)q^(im； il:m-l ,fθm-ι,
— Lim
Dpool)
1M
=M E Lim +
m=1
pulling out the loss
N - M P PLm Li
N - m (N - m+ 1)
- Li
m
(
∖
N-M
1 - N-m +
∖
m
(N-M)X
k=1
、
1
(N-k)(N-k+1)
_ - /
{z	.
=m/(N (N -m))
M
Lit(N-M)X
k=1
、
1N
+ M X
t=M +1
1
(N — k)(N — k + 1),
i ——	,
{^^^^^^^^^"
=M/(N (N -M))
24
Published as a conference paper at ICLR 2021
simplifying and rearranging
N - M N - Mm、
N - m + N - m N)
N - M
N-m
M1
Lim + N
N
X	Lit
t=M +1
1N
+ N X Lit
t=M +1
1N
+ N X Lit
t=M +1
R as required.
□
Remark 2. The optimal Proposalfor estimating the gradient of the pool risk, Vφ Rt with respect to
some scalar φ is instead1
q*" il:m-l, Dpool)= ∣VφLim | / X .	VφLn∣.
^^n∈iLm-1	A
Note that when taking gradients with respect to multiple variables, the optimal proposal for each will
be different for each.
1One can, in principle, actually construct an exact estimator in this scenario as well with the TABI approach
of RainfOrth et al. (2020) by employing two separate proposals that target max(VθR, 0) and — min(VθR, 0)
respectively, then taking the difference between the two resultant estimators.
25
Published as a conference paper at ICLR 2021
(a) Linear Regression	(b) Bayesian Neural Network
Figure 5:	For linear regression (a) the biased estimator has the lowest variance, and RLURE improves
7^^i	∕1∖TΓA . i' .l TΓA-ΛT-1LT.1	♦	1 1	.,1%	, 1	1	,
on RPURE. (b) But for the BNN the variances are more comparable, with RLURE the lowest.
C Experimental Details
C.1 Linear Regression
Our training dataset contains a small cluster of points near x = -1 and two larger clusters at
0 ≤ x ≤ 0.5 and 1 ≤ x ≤ 1.5, sampled proportionately to the ‘true’ data distribution. The data
distribution from which we select data in a Rao-Blackwellised manner has a probability density
function over x equal to:
0.12
P(X = X) = < 0.95
I 0.95
-1.2 ≤ x ≤ -0.8
0.0≤x≤0.5
1.0≤x≤1.5
while the distribution over y is then induced by:
y = max(0, x)
|x|2 +
sin(20x)
-4-
We set N = 101, where there are 5 points in the small cluster and 96 points in each of the other
two clusters, and consider 10 ≤ M ≤ 100. We actively sample points without replacement using a
geometric heuristic that scores the quadratic distance to previously sampled points and then selects
points based on a Boltzman distribution with β = 1 using the normalized scores.
Here, we also show in Figure 6 results that are collected using an epsilon-greedy acquisition proposal.
The results are aligned with those from the other acquisition distribution we consider in the main
body of the paper. This proposal selects the point that is has the highest total distance to all previously
selected points with probability 0.9 and uniformly at random with probability = 0.1. That is, the
acquistion proposal is given by:
P(im = j; i1:m-1, Dpool)
-e +成荷
pool |
arg maxj∈Dtrain Σk∈Dtoin lxk - xj 1
otherwise
where of course Dtrain are the i1:m-1 elements of Dpool.
For all graphs we use 1000 trajectories with different random seeds to calculate error bars. Although,
of course, each regression and scoring is deterministic, the acquistion distribution is stochastic.
Although the variance of the estimators can be inferred from Figure 2a, we also provide Figure 5a
which displays the variance of the estimator directly.
C.2 Bayesian Neural Network
We train a Bayesian neural network using variational inference (Jordan et al., 1999). In particular, we
use the radial Bayesian neural network approximating distribution (Farquhar et al., 2020). The details
of the hyperparameters used for training are provided in Table 1.
26
Published as a conference paper at ICLR 2021
--KIK s-m
M
(a) Bias (like Fig. 2a).
(b) TestMSE (like Fig. 3a).
Figure 6:	Adopting an alternative proposal distribution—here an epsilon-greedy adaptation of a
distance-based measure—does not change the overall picture for linear regression.
Hyperparameter	Setting description
Architecture	Convolutional Neural Network
Conv 1	1-16 channels, 5x5 kernel, 2x2 max pool
Conv 2	16-32 channels, 5x5 kernel, 2x2 max pool
Fully connected 1	128 hidden units
Fully connected 2	10 hidden units
Loss function	Negative log-likelihood
Activation	ReLU
Approximate Inference Algorithm	Radial BNN Variational Inference (Farquhar et al., 2020)
Optimization algorithm	Amsgrad (Reddi et al., 2018)
Learning rate	5∙10-4
Batch size	64
Variational training samples	8
Variational test samples	8
Variational acquisition samples	100
Epochs per acquisition	up to 100 (early stopping patience=20), with 1000 copies of data
Starting points	10
Points per acquistion	1
Acquisition proposal distribution	q(im; i1:m —1, DPool) = P eTsi
Temperature: T	10,000
Scoring scheme: s	BALD (M.I. between θ and outPut distribution)
Variational Posterior Initial Mean	He et al. (2016)
Variational Posterior Initial Standard Deviation	log[1 + e-4]
Prior	N(0, 0.252)
Dataset	Unbalanced MNIST
Preprocessing	Normalized mean and std of inPuts.
Validation Split	1000 train Points for validation
Runtime per result	2-4h
Computing Infrastructure	Nvidia RTX 2080 Ti
Table 1: Experimental Setting—Active MNIST.
27
Published as a conference paper at ICLR 2021
(a) Test loss
M
(b) Test accuracy
L♦	∣-t -rɪ τ	,	, ,1	ΓT∙ . Γ∙ ∙	T^i	,1	1	, ,1	, ∙	∙ ∙ , ∙	F	F
Figure 7:	We contrast the effect of using RLURE throughout the entire acquisition procedure and
training (rather than using the same acquisition procedure based on R for all estimators). The purple
test performance and orange are nearly identical, suggesting the result is not sensitive to this choice.
The unbalanced dataset is constructed by first noising 10% of the training labels, which are assigned
random labels, and then selecting a subset of the training dataset such that the numbers of examples
of each class is proportional to the ratio (1., 0.5, 0.5, 0.2, 0.2, 0.2, 0.1, 0.1, 0.01, 0.01)—that is, there
are 100 times as many zeros as nines in the unbalanced dataset. (Figure 3f shows a version of this
experiment which uses a balanced dataset instead, in order to make sure that any effects are not
entirely caused by this design choice.) In fact, we took only a quarter of this dataset in order to speed
up acquisition (since each model must be evaluated many times on each of the candidate datapoints
to estimate the mutual information). 1000 validation points were then removed from this pool to
allow early stopping. The remaining points were placed in Dpool. We then uniformly selected 10
points from Dpool to place in Dtrain. Adding noise to the labels and using an unbalanced dataset is
designed to mimic the difficult situations that active learning systems are deployed on in practice,
despite the relatively simple dataset. However, we used a simple dataset for a number of reasons.
Active learning is very costly because it requires constant retraining, and accurately measuring
the properties of estimators generally requires taking large numbers of samples. The combination
makes using more complicated datasets expensive. In addition, because our work establishes a lower
bound on architecture complexity for which correcting the active learning bias is no longer valuable,
establishing that lower bound with MNIST is in fact a stronger result than showing a similar result
with a more complex model.
The active learning loop then proceeds by:
1.	training the neural network on Dtrain using R;
2.	scoring Dpool;
3.	sampling a point to be added to Dtrain;
4.	Every 3 points, we separately trained models on Dtrain using R, RPURE, and RLURE and
evaluate them.
This ensures that all of the estimators are on data collected under the same sampling distribution for
fair comparison. As a sense-check, in Figures 7a and 7b we show an alternate version in which the
first step trains with RLURE instead of R, and find that this does not have a significant effect on the
results.
When we compute the bias of a fixed neural network in Figure 2b, we train a single neural network
on 1000 points. We then sample evaluation points using the acquisition proposal distribution from
the test dataset and evaluate the bias using those points.
In Figures 8a and 8b we review the graphs shown in Figures 3b and 3c, this time showing standard
errors in order to make clear that the biased R estimator has better performance, while the earlier
figures show that the performance is quite variable.
We considered a range of alternative proposal distributions. In addition to the Boltzman distribution
which we used, we considered a temperature range between 1,000 and 20,000 finding it had relatively
little effect. Higher temperatures correspond to more certainly picking the highest mutual information
28
Published as a conference paper at ICLR 2021
」-SEkjs∙J.∙y.l-dE,*j
---- Trained with R
---- Trained with RPURE
---- Trained with RLURE
----Trained with R
----Trained with RPURE
----Trained with RLURE
85 -
80 -
Trained with R
—Trained with .
0.6 -
10	20	30	40	50	60	10	20	30	40	50	60
M	M
(a) Test loss	(b) TeSt accuracy
Figure 8: Versions of Figures 3b and 3c shown with standard errors (45 points) instead of standard
deviations. This makes it clearer that the biased R has better performance, even if only marginally so.
Trained with .
Trained with R
—Trained with .
Trained with .
(a) T=5000 NLL.
Train TraIned with R
60
Trained With RPURE
—a- Trained with R LURE	55
Trained with R
—Trained with R PURE
T- Trained with R LURE
Train Tralned with R
Trained with RPJRE
Ta Trained with RLURE
(b) T=15000 NLL.	(c) T=20000 NLL.
10	20	30	40	50	60	10	20	30	40	50	60	10	20	30	40	50	60
M	M	M
(d) T=5000 Acc.	(e) T=15000 Acc.	(f) T=20000 Acc.
Figure 9: Higher temperatures approach a deterministic acquisition function. These also tend to
increase the variance of the risk estimator because the weight associated with unlikely points increases,
when it happens to be selected. The overall pattern seems fairly consistent, however.
xoe,lnoo431
point, which approaches a deterministic proposal. We found that because the mutual information had
to be estimated, and was itself a random variable, different trajectories still picked very different sets
of points. However, for very high temperatures the estimators became higher variance, and for lower
temperatures, the acquisition distribution became nearly uniform. In Figure 9 we show the results
of networks trained with a variety of temperatures other than the 10,000 ultimately used. We also
considered a proposal which was simply proportional to the scores, but found this was also too close
to sampling uniformly for any of the costs or benefits of active learning to be visible.
We considered Monte Carlo dropout as an alternative approximating distribution (Gal & Ghahramani,
2015) (see Figures 3e and 10b). We found that the mutual information estimates were compressed
in a fairly narrow range, consistent with the observation by Osband et al. (2018) that Monte Carlo
dropout uncertainties do not necessarily converge unless the dropout probabilities are also optimized
(Gal et al., 2017a). While this might be good enough when only the relative score is needed in order
to calculate the argmax, for our proposal distribution we would ideally prefer to have good absolute
scores as well. For this reason, we chose the richer approximate posterior distribution instead.
Last, we considered a different architecture, using a full-connected neural network with a single
hidden layer with 50 units, also trained as a Radial BNN. This showed higher variance in downstream
performance, but was broadly similar to the convolutional architecture (see Figures 10d and 10e).
29
Published as a conference paper at ICLR 2021
xoe,lnoo431
Trained with RPURE
t- Trained with R LURE
80
70
60
50
40
----Trained with R	40
Trained with RPURE
T- Trained with R LURE	30
s31
10	20	30	40	50	60
M
(c) MNIST (Balanced): Acc.
10	20	30	40	50	60
M
(a)	FaShionMNIST: Accuracy.
10	20	30	40	50	60
M
(b)	MNIST (MCDO): Acc.
0.8
Trained with R
—Trained with .
」s31 -edE3
10	15	20	25	30	35	40	45	50
M
(d) MNIST (MLP): NLL
----Trained with .
XoeLnoo^s31
10	15	20	25	30	35	40	45	50
M
(e) MNIST (MLP): Accuracy
Figure 10: Further downstream performance experiments. (a)-(c) are partners to Figures 3d, 3e, and
3f. (d) and (e) show similar results for a smaller multi-layer perceptron (with one hidden layer of 50
units). In all cases the results broadly mirror the results in the main paper.
Reference
Application Corrects Bias Acknowledges Bias Notes
(Sener & Savarese, 2018)
(Shen et al., 2018)
(Beluch et al., 2018)
(Haut et al., 2018)
(Sinha et al., 2019)
(Siddhant & Lipton, 2018)
(Ghosal et al., 2019)
(Yang et al., 2018)
(Yoo & Kweon, 2019)
(Kirsch et al., 2019)
(Huang et al., 2018)
(Wen et al., 2018)
(Chen et al., 2019)
(Zhang & Lee, 2019)
(Kellenberger et al., 2019)
Discusses bias in Dpool.
Discusses bias in Dpool.
Table 2: Existing applications of deep active learning rarely acknowledge the bias introduced by
actively sampling points and do not, to the best of our knowledge, try to correct it.
D	Deep Active Learning In Practice
In Table 2, we show an informal survey of highly cited papers citing Gal et al. (2017b), which
introduced active learning to computer vision using deep convolutional neural networks. Across a
range of papers including theory papers as well as applications ranging from agriculture to molecular
science only two papers acknowledged the bias introduced by actively sampling and none of the
papers took steps to address it. It is worth noting, though, that at least two papers motivated their use
of active learning by observing that they expected their training data to already be unrepresentative
of the population data and saw active learning as a way to address that bias. This does not quite
work, unless you explicitly assume that the actively chosen distribution is more like the population
distribution, but is an interesting phenomenon to observe in practical applications of active learning.
30