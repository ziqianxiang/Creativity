Published as a conference paper at ICLR 2021
Learning a Latent Simplex in Input-Sparsity
Time
Ainesh Bakshi
Carnegie Mellon University
abakshi@cs.cmu.edu
Chiranjib Bhattacharyya
Indian Institute of Science
chiru@iisc.ac.in
Ravi Kannan
Microsoft Research India
kannan@microsoft.com
David P. Woodruff
Carnegie Mellon University
dwoodruf@cs.cmu.edu
Samson Zhou
Carnegie Mellon University
samsonzhou@gmail.com
Ab stract
We consider the problem of learning a latent k-vertex simplex K ⊂ Rd, given
access to A ∈ Rd×n , which can be viewed as a data matrix with n points that are
obtained by randomly perturbing latent points in the simplex K (potentially beyond
K). A large class of latent variable models, such as adversarial clustering, mixed
membership stochastic block models, and topic models can be cast as learning a
latent simplex. Bhattacharyya and Kannan (SODA, 2020) give an algorithm for
learning such a latent simplex in time roughly O(k ∙ nnz(A)), where nnz(A) is
the number of non-zeros in A. We show that the dependence on k in the running
time is unnecessary given a natural assumption about the mass of the top k singular
values of A, which holds in many of these applications. Further, we show this
assumption is necessary, as otherwise an algorithm for learning a latent simplex
would imply an algorithmic breakthrough for spectral low rank approximation.
At a high level, Bhattacharyya and Kannan provide an adaptive algorithm that
makes k matrix-vector product queries to A and each query is a function of all
queries preceding it. Since each matrix-vector product requires nnz(A) time,
their overall running time appears unavoidable. Instead, we obtain a low-rank
approximation to A in input-sparsity time and show that the column space thus
obtained has small sin Θ (angular) distance to the right top-k singular space of A.
Our algorithm then selects k points in the low-rank subspace with the largest inner
product (in absolute value) with k carefully chosen random vectors. By working
in the low-rank subspace, we avoid reading the entire matrix in each iteration and
thus circumvent the Θ(k ∙ nnz(A)) running time.
1	Introduction
We study the problem of learning k vertices M*/，...，M*,k of a latent k-dimensional simplex K in
Rd using n data points generated from K and then possibly perturbed by a stochastic, deterministic, or
adversarial source before given to the algorithm. In particular, the resulting points observed as input
data could be heavily perturbed so that the initial points may no longer be discernible or they could
be outside the simplex K. Recent work of Bhattacharyya & Kannan (2020) unifies several stochastic
models for unsupervised learning problems, including k-means clustering, topic models (Blei, 2012),
and mixed membership stochastic block models (Airoldi et al., 2014) under the problem of learning a
latent simplex. In general, identifying the latent simplex can be computationally intractable. However
many special applications do not require the full generality. For example, in a mixture model like
Gaussian mixtures, the data is assumed to be generated from a convex combination of density
functions. Thus, it may be possible to efficiently approximately learn the latent simplex given certain
distributional properties in these models.
Indeed, Bhattacharyya & Kannan (2020) showed that given certain reasonable geometric assumptions
that are typically satisfied for real-world instances of Latent Dirichlet Allocation, Stochastic Block
1
Published as a conference paper at ICLR 2021
1
Models and Clustering, there exists an O(k ∙ nnz(A)) 1 time algorithm for recovering the vertices of
the underlying simplex. We show that, given an additional natural assumption, we can remove the
dependency on k and obtain a true input sparsity time algorithm. We begin by defining the model
along with our new assumption:
Definition 1.1 (Latent Simplex Model). Let M*,ι, M**,..., M*,k ∈ Rd denote the vertices of a
k-simplex, K. Let P*,ι, P*,2... P*,n ∈ Rd be n points in the convex hull of K. Given σ > 0, we
observe n points A*,1, A*,2 . . . A*,n ∈ Rd such that kA - P∣∣2 ≤ σ√n. Further, we make the
following assumptions on the data generation process:
1.	Well-Separateness. For all ' ∈ [k], M*,' has non-trivial mass in the orthogonal complement
of the span of the remaining vectors, i.e., for all ' ∈ [k], ∣Prcj(M*,',Null(M \ M*,'))∣ ≥
α max' ∣∣M*,'∣∣2 where Proj(x, U) denotes the orthogonal projection of X to the subspace U.
2.	Proximate Latent Points. For all' ∈ [k], there exists a set S' ⊆ [n] such that |S'| ≥ δn and for
a∏ j ∈ S', kM*,' - P*,j k2 ≤ 4σ/δ.
3.	Spectrally Bounded Perturbation. The spectrum of A — P is bounded, i.e., for a sufficiently
large constant c, σ∕√7 ≤ α2 min` ∣∣M*,'k2∕ck9.
4.	Significant Singular Values. Let A = Pi∈[d] σiuiviT be the singular value decomposition
and let 0 < φ ≤ nnz(A)∕(n ∙ poly(k)). We assume that for all i ∈ [k], σ% > φ ∙ σk+ι and
∣A - Ak ∣2F ≤ φ∣A - Ak ∣22.
These assumptions are natural across many interesting applications; see Section 2 for more details.
Bhattacharyya & Kannan (2020) introduced the Well-Separateness (1), Proximate Latent Points (2)
and Spectrally Bounded Perturbation (3) assumptions. We include an additional Significant Singular
Values assumption (4), which is crucial for obtaining a faster running time; we discuss this in more
detail below. Our main algorithmic result can then be stated as follows:
Theorem 1.2 (Learning a Latent Simplex in Input-Sparsity Time). Given k ≥ 2 and A ∈
Rd×n from the Latent Simplex Model (Definition 1.1), there exists an algorithm that runs in
O (nnz(A) + (n + d)poly(k∕φ)) time to output subsets Ar1,...,ARk such that upon permut-
ing the columns of M, with probability at least 1 一 1∕Ω(√k), for all ' ∈ [k], we have
∣∣Ar' 一 M*,'∣∣2 ≤ 300k4σ/(α√δ).
Our result implies faster algorithms for various stochastic models that can be formulated as special
cases of the Latent Simplex Model, including Latent Dirichlet Allocation for Topic Modeling, Mixed
Membership Stochastic Block Models and Adversarial Clustering. We summarize the connections to
these applications below. We describe our algorithm and provide an outline to our analysis; we defer
all formal proofs to the supplementary material.
2	Connection to Stochastic Models
We first formalize the connection between the Latent Simplex Model and numerous stochastic models.
In particular, we show that topic models like Latent Dirichlet Allocation (LDA) and Stochastic
Block Models can be viewed as special cases of the Latent Simplex Model; we defer discussion on
Adversarial Clustering to the supplementary material.
2.1	Topic Models
Probabilistic Topic Models attempt to identify abstract topics in a collection of documents by
discovering latent semantic structure (Blei & Jordan, 2003; Blei & Lafferty, 2006; Hoffman et al.,
2010; Zhu et al., 2012; Blei, 2012). Each document in the corpus is represented by a bag-of-words
vectorization with the corresponding word frequencies. The standard statistical assumption is that the
generative process for the corpus is a joint probability distribution over both the observed and hidden
random variables. The hidden random variables can be interpreted as representative documents for
each topic. The goal is to then design algorithms that can learn the underlying topics. The topics can
1Throughout the paper we use the notation Oe to suppress poly-logarithmic factors.
2
Published as a conference paper at ICLR 2021
be viewed geometrically as k latent vectors M*,ι, M**,..., M*,k ∈ Rd, where d is the size of the
dictionary and Mi,' is the expected frequency of word i in topic '. Since each vector M*,' represents
a probability distribution, Pi Mi,' = 1. Let M be the corresponding d X k matrix. One important
stochastic model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003), where each document
consists of m words is generated as follows :
•	For all' ∈ [k], we pick topic weights Wj,' 〜Dir(1∕k), where Dir(1∕k) is the Dirichlet distribu-
tion over the unit simplex. The topic distribution of document j is decided by the topic weights,
WjQ and given by P*,j = ∑'∈[k] Wj,' ∙ M*,', where P*,j are latent points.
•	We then generate the j-th document with m words by taking i.i.d. samples from Mult(P*,j ), the
multinomial distribution with P*,j as the probability vector. The resulting document observed
is denoted by the vector A*j, where for all i ∈ [d] Aij = ^^ Pmm=I X(j),, such that X(j) 〜
Bern(Pij), where Xi(jt) = 1 if the i-th word was chosen in the t-th draw while generating the j-th
document, and 0 otherwise.
The data generation process of LDA can be viewed as a special case of the Latent Simplex Model,
where the j-th document is the data point A*,j generated from the stochastic vector P*,j , a point in
the simplex K. The vertices of the simplex are the k topic vectors M*,1, . . . , M*,k; the goal is then
to recover the vertices of K. We formally justify our assumptions below.
Lemma 2.1 (LDA as a Latent Simplex). Given A, P, M following the LDA model as described above,
such that for all ' ∈ [k], ∣∣M*,'k2 = Ω(1), m, n = Ω(poly(k∕α)) and δ = cσ∕√k, assumptions
(2),(3) and (4) from Definition 1.1 are satisfied with high probability.
2.2	Mixed Membership Stochastic Block Models
The Stochastic Block Model (Airoldi et al., 2008; Miller et al., 2009; Xing et al., 2010; Fu et al.,
2009; Li et al., 2016; Fan et al., 2016) is a well-studied stochastic model for generating random
graphs, where the vertices are partitioned into k communities and edges within each community are
more likely to occur than edges across communities. Given communities C1 , C2, . . . Ck, there exists
a k × k symmetric latent matrix B, where, B'1,'2 is the probability that there exists an edge between
vertices in C'1 and C'2 . The MMBM can be formalized as the following stochastic process:
•	Forj ∈ [n], vertex j picks a probability vector W*,j ∈ Rk representing community membership
probabilities that sum to 1, i.e., Wij 〜Dir(1∕k) for all i ∈ [k].
•	For all pairs (j1, j2) ∈ [n], vertex j1 picks a community `1 proportional to Mult(W*,j1) and j2
picks a community `2 proportional to Mult(W*,j2). The edge (j1, j2) is included in the graph with
probability B'1,'2. Since P' ,' W'1,j1B'1,'2W'2,j2 represents the edge probability of the edge
(j1 , j2 ), the latent variable matrix P of edge probabilities can be represented as P = WT BWT .
However, our reduction is not straightforward since now P depends quadratically on W and the only
polynomial time algorithms for B directly rely on semidefinite programming. Further, they require
non-degeneracy assumptions in order to compute a tensor decomposition provably in polynomial
time (Anandkumar et al., 2014; Hopkins & Steurer, 2017). However, we can pose the problem of
recovery of the k underlying communities differently and first pick at random a subset V1 ⊂ [n] of d
vertices and represent the `-th community by a d-dimensional vector that represents the probabilities
of vertices in [n] \ V1 belonging to community ` and having an edge with each of the d vertices in
V1. We now define W(1) to be a k × d matrix representing the fractional membership of weights of
vertices in V1 and W(2) to be the analogous k × n matrix for vertices in [n] \ V1. Observe that the
probability matrix P can now be represented as W(T1)BW(2).
The reduction to the Latent Simplex Model can now be stated as follows: given a data matrix A
which is the adjacency matrix of the community graph, and the latent variable matrix P, recover the
simplex M = W(T1)B. Further, (Airoldi et al., 2008) assumes that each column of W(2) is picked
from the Dirichlet distribution with parameter 1/k. Combined with tools from random matrix theory
(Vershynin, 2010), (Bhattacharyya & Kannan, 2020) (Lemma 7.2) shows that the Proximate Latent
Points and Spectrally Bounded assumptions hold for Stochastic Block Models. As for the Significant
Singular Values assumption, it is satisfied when σ is a small enough polynomial in k.
3
Published as a conference paper at ICLR 2021
Justifying Significant Singular Values. We give the following further justification for assump-
tion (4) in the supplementary material: a faster algorithm using the assumptions from (Bhattacharyya
& Kannan, 2020) would imply an algorithmic breakthrough for spectral low-rank approximation and
partially resolve the first open question of (Woodruff, 2014).
Theorem 2.2 (Spectral LRA and Learning a Simplex (informal)). There exists a distribution over
instances such that learning a latent simplex in o(nnz(A) ∙ k) time with good probability implies a
constant factor spectral low-rank approximation algorithm in the same running time.
3	Algorithm and Analysis
Preliminaries. We use n, d, and k to denote the number of data points, the number of dimensions
of the space and the number of vertices of K respectively. We use the notation A*,j to denote the
j -th column of matrix A. For A ∈ Rd×n with rank r, its singular value decomposition, denoted
by SVD(A) = UΣVT, guarantees that U is a d × r matrix with orthonormal columns, VT is an
r × n matrix with orthonormal rows and Σ is an r × r diagonal matrix. The diagonal entries of Σ
are the singular values of A, denoted by σ1 ≥ σ2 ≥ . . . ≥ σr . Given an integer k ≤ r, we define
the truncated singular value decomposition of A that zeros out all but the top k singular values of
A, i.e., Ak = UΣkVT, where Σk has only k non-zero entries along the diagonal. It is well-known
that the truncated SVD computes the best rank-k approximation to A under the Frobenius norm, i.e.,
Ak = minrank(X)≤k kA - XkF. Given an orthonormal basis U for a subspace, we use PU = UUT
to denote the projection matrix corresponding to the subspace. We consider the following notion of
subspace distance:
Definition 3.1 (sin Θ Distance). For any two subspaces R, S of Rd, the sin Θ distance between R
and S is defined as sin Θ(R, S) = max min sin θ(u, v) = max minku - vk.
u∈R v∈S	u∈R,∣u∣ = 1 v∈S
We use the notion of spectral low-rank approximation to obtain a compact representation of the
input and compute matrix-vector products efficiently. We also require the notion of mixed spectral-
Frobenius low-rank approximation. This guarantee is weaker than spectral-low rank approximation
but admits faster algorithms.
Definition 3.2 (Spectral Low-rank Approximation, Spectral-Frobenius Low-rank Approximation).
Given a matrix A, an integer k and > 0, a rank-k matrix B satisfies a relative-error spectral low-rank
approximation guarantee if kA - Bk22 ≤ (1 + )kA - Akk22. B satisfies a mixed spectral-Frobenius
low-rank approximation guarantee if kA - BI∣2 ≤(I + e)kA - AkI∣2 + kkIIA - AkIlF.
3.1	Overview
In this section, we provide an overview of our algorithmic techniques and discuss the main challenges
we overcome to obtain an input-sparsity time algorithm.
Our Techniques. The starting point in Bhattacharyya & Kannan (2020) is that the smoothened
polytope, obtained by averaging points in the data matrix A is itself close to the latent points in the
convex hull of K in operator norm. This fact is captured by the following lemma:
Lemma 3.3 (Subset Smoothing). For any S ⊂ [n], let AS be a vector obtained by averaging
the columns of A indexed by S and define PS similarly. Thenfor ∣∣A 一 P∣2 ≤ σ√n, we have
Ias - Ps∣2 ≤ σpn∕∣^.
Our main insight is that we can approximately optimize a linear function on the smoothed polytope
by working with a rank-k spectral approximation to A instead. Geometrically, this implies that
while the smoothed polytope is perhaps d-dimensional, projecting it onto the k-dimensional space
spanned by the top-k singular values of the data matrix A suffices to recover the latent k-simplex, K.
This is surprising since the data matrix can contain points significantly far from the latent polytope.
Further, this approach presents several challenges: we do not have access to the left singular space
of A and even if we are provided this subspace exactly, it is unclear why it spans a set of points
that approximate vertices of K. Finally, the points obtained by smoothing the projected polytope
have no immediate relation to points in the smoothed high-dimensional polytope considered by
(Bhattacharyya & Kannan, 2020).
4
Published as a conference paper at ICLR 2021
We would like to begin by computing a spectral low-rank approximation (Definition 3.2) for A. Since
a low-rank approximation to A can be represented in factored form YZT, where Y is d × k and
ZT is k X n, any matrix-vector product of the form YZT ∙ X only requires (n + d)k time. Thus
optimizing a linear function k times over a smoothed low-rank polytope requires only (n+ d)k* 1 2 time,
circumventing the previous bound of k ∙ nnz(A). However, the best known algorithm for spectral
low-rank approximation (Theorem 1 in (MUSCO & Musco, 2015)) requires O(nnz(A) ∙ k/ʌ/e) time
and thus provides no improvement. A natural direction to pursue is then to compute a Frobenius
low-rank approximation (which requires nnz(A) time) for A and use this as our proxy. However, a
Frobenius low-rank approximation is too coarse to obtain a subspace that is close to the top-k singular
vectors of A.
Algorithm 1: Learning a Latent k-Simplex in Input Sparsity Time
Input: A matrix A ∈ Rd×n, integer k, and e > 0.
1. Using the algorithm from Lemma 3.4, compute rank-k matrices Y, Z such that YZT
is a spectral low-rank approximation to A, i.e., k A — YZTk2 ≤ (I + E)k A - Ak k2.
2. Let S = {0}. For each t ∈ [k],
(a) Let Ut be an orthonormal basis for the vectors in S.
(b) Compute the projection matrix Pt = UtUT that projects onto the row span of S.
(c) Let g 〜N(0,Ik) and let Ut = gYT(L - Pt)YZT be a random vector in
Rn. Compute Rt ⊂ [n], a subset of δn indices corresponding to the largest
coordinates of Ut in absolute value.
(d) Let ARt be the average of the columns of A indexed by Rt. Update S = SuAr=.
Output: The set of vectors Ar、, Ar? ,..., ARk as our approximation to the vertices of the
latent k-simplex K.
Instead we compute a mixed spectral-Frobenius low-rank approximation (see Definition 3.2) that
runs in O(nnz(A) + dk2) time, but the resulting error guarantee is weaker. In particular, it incurs
an additive EkA - Ak k2F /k term. Here, we use the assumption we introduced (the Significant
Singular Value assumption) to show that the low-rank matrix obtained from this algorithm also
satisfies a relative-error spectral low-rank approximation guarantee. The next challenge is that the
aforementioned guarantee only bounds the spectral norm of A - YZT in terms of the (k + 1)-st
singular value of A. This guarantee does not relate how close the subspaces spanned by the columns
and rows of the low-rank approximation are to the top-k singular space of A.
A key technical contribution of our work is thus to prove that the subspaces obtained via spectral
low-rank approximation are close to the true left and right top-k singular space in angular (sin Θ)
distance. We note that such a guarantee is crucial to approximately optimize a linear function over A.
Further, this result provides an intriguing connection between spectral low-rank approximation and
power iteration. It is well known that power iteration suffices to obtain a subspace that is close to
the top-k subspace of a matrix in sin Θ distance, which at first glance appears much stronger than
spectral low-rank approximation. However, our work implies that it suffices to compute a spectral
low-rank approximation, which provides a succinct representation of the data matrix and can be
computed faster than power iteration in several natural settings.
In the context of learning the latent simplex, given a spectral low-rank approximation, YZT, we first
restrict to the column span of Y, which w.l.o.g. has orthonormal columns, and iteratively generate
k vectors in this subspace. In the first iteration, we generate a random vector gYT and compute
gYT YZT . We then consider the largest δn indices of gYT YZT . While the resulting vector does
not have strong provable guarantees, we show that averaging the columns of A corresponding to
these indices results in a vector, AR1 , which intuitively corresponds to efficiently optimizing a linear
function over a low-rank approximation to the smoothened polytope, where the smoothened polytope
is obtained by averaging over all subsets of δn data points. Our next contribution is to show that AR1
obtained by the aforementioned algorithmic process is indeed close to a vertex of K.
5
Published as a conference paper at ICLR 2021
To obtain an approximation to the remaining vertices of K, we consider the following iterative
process: in the t-th iteration, consider the subspace YT (I - Pt), where (I - Pt) is the projection
onto the orthogonal complement of the span of AR1 , AR2 . . . ARt-1 . Then generate a random
vector gYT (I - Pt), and compute the largest δn coordinates of gYT (I - Pt)YZT. Average the
corresponding columns of A to obtain ARt and output this vector. We prove that after iterating k
times, the vectors AR1 , AR1 , . . . ARk approximate all the vertices of the latent simplex K within
the desired accuracy and running time.
In contrast, prior work of (Bhattacharyya & Kannan, 2020) uses power iteration to approximate the
left top-k singular space Uk of A using a subspace V that is poly(α∕k) close in Sin Θ distance. Each
step of the power iteration uses O(nnz(A) + dk2) time and is repeated log(d) times. Next, they
pick a random vector uι in the subspace spanned V and compute Ar、= argmaxs：s=6n ∣uι ∙ AS|,
using the resulting vector as an approximation to some vertex M*」.
They then repeat the above algorithm k times and in the i-th iteration, they pick ui to be a uniformly
random direction in the k-i dimensional subspace constructed as follows: let Ve i-1 be an orthonormal
basis for AR1 , AR2, . . . , ARi-1 . Intuitively, this corresponds to sampling a random vector from
the subspace orthogonal to the set of vertex approximations picked thus far. The resulting k vectors
AR1 , . . . , ARk are the approximation to the vertices of the latent simplex. Since they directly
optimize over the smoothened polytope, the correctness analysis is more straightforward.
However, each iteration of the algorithm requires optimizing a linear function over the smoothened
polytope and in particular requires computing Ui ∙ A, and thus, the overall running time is dominated
by k ∙ nnz(A). Since the latent simplex satisfies the Well-Separateness condition, the inner product
with a random direction is maximized by a unique vertex. Intuitively, it appears necessary to project
away from the set of vectors obtained up to the i-th iteration in order to learn new vertices of K.
The inherently iterative nature of the algorithm combined with matrix-vector product lower bounds
indicates that the new algorithmic ideas we introduce are in fact necessary.
3.2	Technical Discussion
In this section, we provide an outline of our proof. We defer the full proofs to the supplementary
material. We start with a spectral low-rank approximation for A. We then use the right factor as an
approximation to ΣkVkT and the left factor as an approximation to Uk.
Lemma 3.4. (Input-Sparsity Spectral LRA (Cohen et al., 2015; 2017).) Given a matrix A ∈ Rd×n,
, δ > 0 and k ∈ N, there exists an algorithm that outputs matrices Y, Z, such that with
probability at least 1 一 δ, ∣∣A — YZT∣∣2 ≤ (1 + e)∣∣A — Ak∣∣2 + 泉∣∣A 一 Ak∣∣F, in time
O (nnz(A) + (n + d)poly(k∕eδ)).
Under the Significant Singular Values condition (4), setting = φ in Lemma 3.4 implies
with probability 99∕100, poly(k) Pn=k+ι σ2 = poly(k)kA - AkkF ≤ σ2+ι = kA - Ak∣∣2
and thus ∣A — YZT ∣22 ≤ 2∣A — Ak ∣22. Further, such a matrix YZT can be computed in
O (nnz(A) + (n + d)poly(k∕φ)) time. Next, we show that if YZT is a good rank k spectral
approximation to A, then the subspace spanned by the columns of Y must be close to the column
span of Uk, the top-k left singular vectors of A. We begin by recalling Wedin’s sin Θ theorem that
relates norms of projectors to angular distance:
Theorem 3.5 (sin Θ theorem (Wedin, 1972)). Let R, S ∈ Rd×n and 0 < m ≤ ` be integers. Let
Rm and Sg` denote the subspaces spanned by the top m singular vectors of R and top ' singular
vectors of S, respectively. Suppose Y = σm1(R) — σ'+ι(S). Then, sinΘ(Rm,, S σQ ≤ kR-sk2.
We use the above theorem to bound the spectral norm of the projectors onto the relevant subspaces.
Lemma 3.6 (Proximity of Subspace Projections). Let Y be defined as in Algorithm 1 and let Uk
be the subspace spanned by the top k left singular vectors of A. Let PY and PUk be the d × d
projection matrices onto the row span ofY and Uk. Then ∣PY — PUk ∣2 ≤
1
1000k10.
ProofSketch. Suppose by way of contradiction that ∣∣Pγ — PuM∣2 ≥ ^0^0. This implies
IlUkUT — YYTIIF is large and thus ∣∣UkYT∣F is bounded by k —.。。几口产.Intuitively, we
6
Published as a conference paper at ICLR 2021
show that if the inner product term is small, we can obtain a lower bound on kA - PYAk2 , as fol-
lows: an upper bound on kUkYT k2F suffices to obtain an upper bound on the the k-th singular value
of Uk YYT UT via averaging. This, in turn lower bounds ∣∣A - PY A∣∣2 = IlUT Σ - YYT UT Σ∣∣2
by (iooθki0)2 σk(A), contradicting the Significant Singular Value assumption.	□
Our analysis proceeds via induction on the number of iterations performed by the algorithm. Suppose
our algorithm has selected t points from our approximation of the top k subspace and these points
are reasonably close to i points of the k-simplex. In the (t + 1)-st iteration, we again bound
the sin Θ distance between YT(I - Pt), which corresponds to our approximation of the top k
subspace projected away from the selected vectors, and the actual k-simplex projected away from
the corresponding points closest to our selected vectors. This argues that we can continue selecting
random vectors in the subspace spanned by YT(I - Pt) as a close approximation to random vectors
in M(I - Pt). We first bound the k-th singular values of the simplex vertices (M) and latent variables
(P):
Lemma 3.7. (Bhattacharyya & Kannan, 2020) If the underlying points M follow the Well-
Separateness and Spectrally Bounded Perturbations assumptions, then σk (M) ≥ 10(Ok . √, and
σk(P) ≥ 995k85√nσ.
Next, we prove our lemma relating angular distance of the subspace obtained in the i-th iteration of
the algorithm (Y(I - Pi)) to the optimal subspace (M(I - Pi)).
Lemma 3.8 (Angular Distance between Subspaces.). For some r ∈ [k], let M = M*,'ι ◦
...◦ M*,gr be the matrix with r columns corresponding to vertices of the latent k-simplex
M closest to the first r points selected by Algorithm 1, AR1 , . . . , ARr, respectively. Sup-
pose 11 ARi 一 M*,& ∣∣2 ≤ 30Ok4 √ for each i ∈ [r]. Let Pr be the projection matrix or-
thogonal to Ari ,..., ARRr. Then Sin Θ (Y(Id — Pr), Span (M) ∩ Null(M)) ≤ α∕100k4 and
sinΘ ^Span (M) ∩ Null(M), Y(Id - Pr)) ≤ α∕100k4.
Proof Sketch. Let y ∈ Y(Id - Pr) be a unit vector. It can be shown that using the sin Θ theorem and
the hypothesis that there exists X ∈ Span (M) with ∣∣x - y ∣2 ≤ 50蕊.5. Let Z = X - MMtX be the
component of x in Null(MM ). We can then bound ∣x - z∣2 ≤ ∣x - y∣2 + ∣MM(MMTMM)-1(MMT -
AbT)y∣2, where Ab is the set of vectors selected thus far and AbTy = 0. Combining the aforementioned
observations, We can bound ∣∣x - z∣2 by a2/(500k8.5) + k4∙5σ∕(α√δσk(M)). We then observe
that y ∈ Y(Id - Pr) and z ∈ Span (M) ∩ Null(M), and appeal to Lemma 10.1 in (Bhattacharyya
& Kannan, 2020) to yield sinΘ (Y(Id - Pr), Span (M) ∩ Null(M)) ≤ α∕100k4.
To prove the second half of the claim, it suffices to show that the dimension of Y(Id - Pr) is k - r,
since Span (M) ∩ Null(M) has dimension k - r and the sin Θ distance is symmetric between two
subspaces of the same dimension. By construction Y has dimension k so that Y(Id - Pr) has
dimension at least k - r. Therefore, there exist vectors u1 , . . . , uk-r+1 ∈ Y(Id - Pr) and vectors
一 C	/ 1* Λ-∖ TL T 11 ∕T⅜^Λr∖	1	.1	. Il	Il	,	/ -I l-∖l-∖ 1 Λ Λ Λ r	,1	1
vι,..., vk-r+ι ∈ Span (M) ∩ Null(M) such that ∣∣Ui - Vj∣2 < α∕100k4. We can then upper and
lower bound ∣Va ∙ vb| for all a = b to conclude that vι,..., vk-r+1 are orthogonal, contradicting
that Span (M) ∩ Null(MM) spans a k -
r dimensional space and the claim follows.
□
Now we need to show that our algorithm is (1) well-defined and (2) preserves the invariant that the
(i + 1)-st point sampled from YT(I - Pi) will also be reasonably close to some different point of
the k-simplex. We show the selected procedure is well-defined in Lemma 3.9 by arguing that there
exists a unique solution to the maximization problem.
Lemma 3.9 (Optimization is Well-Defined). Let u ∈ Rd be a random unit vector in the space of
YT (Id - Pr), where Pr is the orthogonal projection to AR1 , . . . , ARr. Then there exists a constant
c > 0 so that with probability at least 1 - c/k1.5:
1.	For all distinct a,b ∈ {'ι,...,'r}, then |u ∙ (M*,° - M*,b)∣ ≥ o-k097 a max' ∣M*,'∣2.
7
Published as a conference paper at ICLR 2021
2.	Forall a ∈ {'ι,...,'r} ,then |u ∙ M*,a∣ ≥ 00989 α max' kM*,'∣2.
We then show that the algorithm preserves the aforementioned invariant by showing that the unique
solution ARi cannot correspond to one of the vertices of the k-simplex that have been found in the
first i rounds, thus proving that we find a solution ARi that corresponds to a new vertex of M. We
then show ARi is close to the new vertex of M, preserving the inductive hypothesis.
Lemma 3.10 (Recovery Guarantees). Let 'r+ι = argmax' U ∙ M*,' if U ∙ Arf ≥ 0 and be
argmin` U ∙ M*,' otherwise. Then, kARr+ι 一 M*,'r+ι k2 ≤ 300k4σ∕α√δ.
ProofSketch. We consider the case U ∙ Ar，+、≥ 0 as the analysis for the case U ∙ ArT+i < 0
is symmetric. Let 'r+ι = argmax' U ∙ M*,'. It can be shown that 'r+ι ∈ {'1,..., 'r}. Thus
applying Lemma 3.9, U ∙ M*,'r+、≥ 0.0989amax' ∣∣M*,'k∕k4. By the Proximate Latent Points
assumption, there exists a set。'叶、of size δn so that ∣∣P*,j 一 M*,'r+、∣2 ≤ √ for all j ∈。'叶、
so that ∣∣P*,σ' 十ɪ 一 M*,'r+ι ∣∣2 ≤ √. Then by Lemma 3.1 in (Bhattacharyya & Kannan, 2020),
U ∙ A*,σg'+ 1 ≥ U ∙ M*,'r+ι ― 5σ∕√J. Similarly, We can show U ∙ Ar，+、≥ U ∙ M*,'r+、― 8σ∕√δ,
Further, using Lemma 3.9 and the Spectrally Bounded Perturbation assumption, we obtain U ∙
M*,a ≤ U ∙ M*,'r+ι 一 0.097amax' ∣∣M*,'k∕k4. This enables us to upper bound U ∙ Ar，+、by
U ∙ M*,'r+ι 一 0.097amax' ∣∣M*,'k2(l 一 w'r+ι)∕k4 + 4σ∕√δ. Combining the upper and lower
bounds, straightforward computations yield the claim.	□
In contrast to (Bhattacharyya & Kannan, 2020), we only need input sparsity time to compute the
low-rank approximation to A. The subsequent k iterations of selecting points from YT (I 一 Pi) are
computed in the low-dimensional space and use lower-order runtime. Hence, the dominating term in
the final runtime is just the input sparsity time used to compute YT (I 一 Pi).
4 Empirical Evaluation
In this section, we describe a series of experiments that demonstrate the advantage of our algorithm,
performed in Python 3.6.9 on an Intel Core i7-8700K 3.70 GHz CPU with 12 cores and 64GB DDR4
memory, using an Nvidia Geforce GTX 1080 Ti 11GB GPU, on both synthetic and real-world data.
Whereas previous work requires computing the top k subspace as a pre-processing step, our main
improvement is that we only require a crude approximation. Thus we compared the running times
for finding the top k subspace as required by (Bhattacharyya & Kannan, 2020) to finding a mixed
spectral-Frobenius approximation using an input sparsity algorithm, as required by our algorithm. For
the former, we use the svds method from the sparse scipy linalg package optimized by LAPACK.
For the latter, (Cohen et al., 2015; 2017) show that using a sparse CountSketch matrix (Clarkson
& Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyen, 2013), i.e., a matrix with O(k2 )
columns and a single nonzero entry in each row that is in a random location and is a random sign,
suffices to obtain a mixed spectral-Frobenius guarantee; we evaluate such a matrix with exactly k2
columns. Across all parameters and datasets, the input sparsity procedure used by our algorithm
significantly outperforms the optimized power iteration methods required by (Bhattacharyya &
Kannan, 2020).
Synthetic Data. Since our theoretical results are most interesting when k d	n, we set
n = 50000, d = 1000, k ∈ {20, 50, 100} and generate a random d × n matrix A that consists of
independent entries that are each 1 with probability P ∈ {忐,2^, 5⅛0} and 0 with probability
1 一 p. In Figure 4.1, we report the average running time of both algorithms, among 5 independent
runs for each choice of p and k.
Social Networks. We also evaluate the algorithms on the email-Eu-core network dataset of
interactions across email data between individuals from a large European research institution (Yin
et al., 2017; Leskovec et al., 2007) and the com-Youtube dataset of friendships on the Youtube
social network (Yang & Leskovec, 2015), both accessed through the Stanford Network Analysis
Project (SNAP). In the former, there are n = d = 1005 nodes in the adjacency matrix over 25571 total
edges, forming k = 42 communities. In the latter, there are 1134890 nodes with 8385 communities,
from which we extract a d × n matrix with n = 100000, d = 1000 to represent a bipartite graph, as
described in both Section 2.2 and (Bhattacharyya & Kannan, 2020). In Figure 4.2, we report the
8
Published as a conference paper at ICLR 2021
Mean Runtime of Algorithms across Parameters	p = 1/500	p= 1/2000	p = 1/5000
ToP k Subspace, k = 20	35.056s	29.725s	-16.45s-
Input Sparsity Approximation, k = 20	-0.595s-	0.329s	0.83s
Top k Subspace, k = 50	56.146s	54.613s	-53.213s-
Input Sparsity Approximation, k = 50	-0.658s-	0.657s	-0.434s-
Top k Subspace, k = 100	78.420s	79.410s	-71.424s-
Input Sparsity Approximation, k = 100	O501s	0.387s	0.440s	-
Figure 4.1: Mean runtime comparison of algorithms across parameters on synthetic data.
running time of both algorithms across each dataset among choices of k ∈ {20, 50, 100}. We observe
that the resulting matrix has sparsity roughly 1000, which is consistent with P ≈ 1 and is much less
than the sparsity parameters tested in our synthetic data.
	email-Eu-Core network	Com-Youtube
Top k Subspace, k = 20	0.387s	57Γ3s
Input Sparsity Approximation, k = 20	0.005s	0.379s
Top k Subspace, k = 50	0.556s	16.711s
Input Sparsity Approximation, k = 50	0.003s	0.373s
Top k Subspace, k = 100	TWs	41.788s
Input Sparsity Approximation, k = 100	O003s	0.366s
Figure 4.2: Mean runtime comparison of algorithms across parameters on real-world data.
Finally, we consider a full end-to-end implementation comparing the runtime and least squares loss
of the top k subspace algorithm and our input sparsity approximation algorithm over various ranges
of the parameter k and smoothening parameter δn on the com-Youtube dataset, from which we
randomly extract an n × d matrix, with n = 20000 and d = 1000 to represent a bipartite graph. Our
results in Figure 4.3 show that our algorithm not only significantly outperforms the top k subspace
algorithm in runtime, but also produces solutions with lower least squared loss.
ssoL derauqS tsaeL
00	5	10	15	20	25	30
Number of communities (k), with δn = 10
ssoL derauqS tsaeL
00	5	10	15	20	25	30
Smoothening parameter (δn), with k = 20
emitnuR
Figure 4.3: Comparison of least squares loss by power iteration algorithm (in blue triangles) and by
our algorithm (in red circles), over various ranges of the parameter k with smoothening parameter
δn = 10, and over various ranges of δn with k = 20, on the com-Youtube dataset. Also runtime
comparison over a range of k, with δn = 10.
Acknowledgments
A.B., and D. W. were supported by the Office of Naval Research (ONR) grant N00014-18-1-2562,
and the National Science Foundation (NSF) Grant No. CCF-1815840. D.W and S.Z were supported
by National Institute of Health (NIH) grant 5R01 HG 10798-2 and a Simons Investigator Award.
References
Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, and Eric P. Xing. Mixed membership
stochastic blockmodels. J. Mach. Learn. Res., 9:1981-2014, June 2008. ISSN 1532-4435.
Edoardo M. Airoldi, David M. Blei, Elena A. Erosheva, and Stephen E. Fienberg. Introduction to
mixed membership models and methods, 2014.
9
Published as a conference paper at ICLR 2021
Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham M. Kakade. A tensor approach to
learning mixed membership community models. Journal of Machine Learning Research, 15(1):
2239-2312, 2014.
Pranjal Awasthi. Cs 598: Theoretical machine learning lecture notes, 2017. https://www.cs.
rutgers.edu/~pa336/mlt_f17/lec-14.pdf.
Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. CoRR, abs/1206.3204,
2012. URL http://arxiv.org/abs/1206.3204.
Chiranjib Bhattacharyya and Ravindran Kannan. Finding a latent k -simplex in O * (k ∙ nnz(data)) time
via subset smoothing. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms,
pp. 122-140. SIAM, 2020.
David Blei, Andrew Ng, and Michael Jordan. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993-1022, 2003.
David M. Blei. Probabilistic topic models. Commun. ACM, 55(4):77-84, 2012.
David M Blei and Michael I Jordan. Modeling annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in informaion retrieval, pp.
127-134, 2003.
David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd international
conference on Machine learning, pp. 113-120, 2006.
Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceedings
of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 47-60, 2017.
Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity
time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pp. 81-90.
ACM, 2013.
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen-
sionality reduction for k-means clustering and low rank approximation. In Proceedings of the
forty-seventh annual ACM symposium on Theory of computing, pp. 163-172. ACM, 2015.
Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approxi-
mation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January
16-19, pp. 1758-1777, 2017.
Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation
and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing, pp. 1047-1060, 2018.
Xuhui Fan, Richard Yi Da Xu, and Longbing Cao. Copula mixed-membership stochastic block
model. In IJCAI International Joint Conference on Artificial Intelligence, 2016.
Wenjie Fu, Le Song, and Eric P Xing. Dynamic mixed membership blockmodel for evolving networks.
In Proceedings of the 26th annual international conference on machine learning, pp. 329-336,
2009.
Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for latent dirichlet allocation.
In advances in neural information processing systems, pp. 856-864, 2010.
Samuel B. Hopkins and David Steurer. Efficient bayesian estimation from few samples: Community
detection and related problems. In 58th IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pp. 379-390, 2017.
Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm.
FOCS, 2010. URL http://arxiv.org/abs/1004.1823.
Jure Leskovec, Jon M. Kleinberg, and Christos Faloutsos. Graph evolution: Densification and
shrinking diameters. ACM Trans. Knowl. Discov. Data, 1(1):2, 2007.
10
Published as a conference paper at ICLR 2021
Wenzhe Li, Sungjin Ahn, and Max Welling. Scalable mcmc for mixed membership stochastic
blockmodels. In Artificial Intelligence and Statistics, pp. 723-731, 2016.
Allen Liu and Ankur Moitra. Efficiently learning mixtures of mallows models. In 2018 IEEE 59th
Annual Symposium on Foundations of Computer Science (FOCS), pp. 627-638. IEEE, 2018.
Tyler Lu and Craig Boutilier. Learning mallows models with pairwise preferences, 2011.
Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM
symposium on Theory of computing, pp. 91-100. ACM, 2013.
Kurt Miller, Michael I Jordan, and Thomas L Griffiths. Nonparametric latent feature models for link
prediction. In Advances in neural information processing systems, pp. 1276-1284, 2009.
Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. In Advances in Neural Information Processing Systems,
pp. 1396-1404, 2015.
Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via sparser
subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science,
FOCS, pp. 117-126. IEEE Computer Society, 2013.
Arora Sanjeev and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the
thirty-third annual ACM symposium on Theory of computing, pp. 247-257, 2001.
Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of
Computer and System Sciences, 68(4):841-860, 2004.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Per-Ake Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical
Mathematics, 12(1):99-111, 1972.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and TrendsR in
Theoretical Computer Science, 10(1-2):1-157, 2014.
Eric P Xing, Wenjie Fu, Le Song, et al. A state-space mixed membership blockmodel for dynamic
network tomography. The Annals of Applied Statistics, 4(2):535-566, 2010.
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-truth.
Knowl. Inf. Syst., 42(1):181-213, 2015.
Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich. Local higher-order graph clustering.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 555-564. ACM, 2017.
Jun Zhu, Amr Ahmed, and Eric P Xing. Medlda: maximum margin supervised topic models. Journal
of Machine Learning Research, 13(Aug):2237-2278, 2012.
11