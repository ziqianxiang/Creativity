Published as a conference paper at ICLR 2021
Regularized Inverse Reinforcement Learning
Wonseok Jeon* ,1,2, Chen-Yang Su1,2, Paul Barde1,2, Thang Doan1,2,
Derek Nowrouzezahrai1,2, Joelle Pineau1,2,3
1	Mila, Quebec AI Institute
2	School of Computer Science, McGill University
3Facebook AI Research
Ab stract
Inverse Reinforcement Learning (IRL) aims to facilitate a learner’s ability to imitate
expert behavior by acquiring reward functions that explain the expert’s decisions.
Regularized IRL applies strongly convex regularizers to the learner’s policy in order
to avoid the expert’s behavior being rationalized by arbitrary constant rewards,
also known as degenerate solutions. We propose tractable solutions, and practical
methods to obtain them, for regularized IRL. Current methods are restricted to the
maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers,
as well as proposing solutions that are intractable in practice. We present theo-
retical backing for our proposed IRL method’s applicability to both discrete and
continuous controls, empirically validating our performance on a variety of tasks.
1	Introduction
Reinforcement learning (RL) has been successfully applied to many challenging domains including
games (Mnih et al., 2015; 2016) and robot control (Schulman et al., 2015; Fujimoto et al., 2018;
Haarnoja et al., 2018). Advanced RL methods often employ policy regularization motivated by, e.g.,
boosting exploration (Haarnoja et al., 2018) or safe policy improvement (Schulman et al., 2015).
While Shannon entropy is often used as a policy regularizer (Ziebart et al., 2008), Geist et al. (2019)
recently proposed a theoretical foundation of regularized Markov decision processes (MDPs)—a
framework that uses strongly convex functions as policy regularizers. Here, one crucial advantage is
that an optimal policy is shown to uniquely exist, whereas multiple optimal policies may exist in the
absence of policy regularization.
Meanwhile, since RL requires a given or known reward function (which can often involve non-trivial
reward engineering), Inverse Reinforcement Learning (IRL) (Russell, 1998; Ng et al., 2000)—the
problem of acquiring a reward function that promotes expert-like behavior—is more generally adopted
in practical scenarios like robotic manipulation (Finn et al., 2016b), autonomous driving (Sharifzadeh
et al., 2016; Wu et al., 2020) and clinical motion analysis (Li et al., 2018). In these scenarios, defining
a reward function beforehand is particularly challenging and IRL is simply more pragmatic. However,
complications with IRL in unregularized MDPs relate to the issue of degeneracy, where any constant
function can rationalize the expert’s behavior (Ng et al., 2000).
Fortunately, Geist et al. (2019) show that IRL in regularized MDPs—regularized IRL—does not
contain such degenerate solutions due to the uniqueness of the optimal policy for regularized MDPs.
Despite this, no tractable solutions of regularized IRL—other than maximum-Shannon-entropy IRL
(MaxEntIRL) (Ziebart et al., 2008; Ziebart, 2010; Ho & Ermon, 2016; Finn et al., 2016a; Fu et al.,
2018)—have been proposed.
In Geist et al. (2019), solutions for regularized IRL were introduced. However, they are generally
intractable since they require a closed-form relation between the policy and optimal value function
and the knowledge on model dynamics. Furthermore, practical algorithms for solving regularized
IRL problems have not yet been proposed.
We summarize our contributions as follows. Unlike the solutions in Geist et al. (2019), we propose
tractable solutions for regularized IRL problems that can be derived from policy regularization and
* Correspondence to: WonseokJeon <jeonwons@mila.quebec>
1
Published as a conference paper at ICLR 2021
its gradient in discrete control problems (Section 3.1). We additionally show that our solutions
are tractable for Tsallis entropy regularization with multi-variate Gaussian policies in continuous
control problems (Section 3.2). We devise Regularized Adversarial Inverse Reinforcement Learning
(RAIRL), a practical sample-based method for policy imitation and reward learning in regularized
MDPs, which generalizes adversarial IRL (AIRL, Fu et al. (2018)) (Section 4). Finally, we empirically
validate our RAIRL method on both discrete and continuous control tasks, evaluating RAIRL via
episodic scores and from divergence minimization perspective (Ke et al., 2019; Ghasemipour et al.,
2019; Dadashi et al., 2020) (Section 5).
2	Preliminaries
Notation For finite sets X and Y , Y X is a set of functions from X to Y . ∆X (∆YX) is a set of
(conditional) probabilities over X (conditioned on Y ). Especially for the conditional probabilities
Px∣y ∈ ∆χ, We say PX∣γ(∙∣y) ∈ ∆x for y ∈ Y. R is the set of real numbers. For functions
f1,f2 ∈ RX, we define hf1,f2iX := Px∈X f1(x) f2(x).
Regularized Markov Decision Processes and Reinforcement Learning We consider sequential
decision making problems Where an agent sequentially chooses its action after observing the state of
the environment, and the environment in turn emits a reWard With state transition. Such an interaction
betWeen the agent and the environment is modeled as an infinite-horizon Markov Decision Process
(MDP), Mr := hS, A, P0, P, r, γi and the agent’s policy π ∈ ∆SA. The terms Within the MDP
are defined as folloWs: S is a finite state space, A is a finite action space, P0 ∈ ∆S is an initial
state distribution, P ∈ ∆SS×A is a state transition probability, r ∈ RS×A is a reWard function, and
γ ∈ [0, 1) is the discount factor. We also define an MDP Without reWard as M- := hS, A, P0, P, γi.
The normalized state-action visitation distribution, dπ ∈ ∆S×A , associated With π is defined as the
expected discounted state-action visitation of π, i.e., d∏ (s, a) := (1 - γ) ∙ En [P∞=0 γiI{si = s,ai =
a}], Where the subscript π on E means that a trajectory (s0, a0, s1, a1, ...) is randomly generated
from M- and ∏, and I{∙} is an indicator function. Note that d∏ satisfies the transposed Bellman
recurrence (Boularias & Chaib-Draa, 2010; Zhang et al., 2019):
d∏(s, a) = (1 — Y)Po(s)π(a∣s) + γπ(a∣s) ɪ2 P($恒,a)d∏(s, a).
s,a
We consider RL in regularized MDPs (Geist et al., 2019), Where the policy is optimized With a causal
policy regularizer. Mathematically for an MDP Mr and a strongly convex function Ω : ∆A → R, the
objective in regularized MDPs is to seek π that maximizes the expected discounted sum of reWards,
or return in short, with policy regularizer Ω:
∞
arg max Jn(r,∏):= En EY i{r(si,ai) - Ω(π(∙∣Si))}
π∈∆SA	i=0
(1)
It turns out that the optimal solution of Eq.(1) is unique (Geist et al., 2019), whereas multiple
optimal policies may exist in unregularized MDPs (See Appendix A for a detailed explanation).
In later work (Yang et al., 2019), Ω(p) = -λEa~pφ(p(α)),p ∈ ∆A was considered for λ > 0
and φ : (0, 1] → R satisfying some mild conditions. For example, RL with Shannon entropy
regularization (Haarnoja et al., 2018) can be recovered by φ(x) = - log x, while RL with Tsallis
entropy regularization (Lee et al., 2020) can be recovered from φ(x) = q-1 (1 - XqT) for k >
0,q > 1. The optimal policy π* for Eq.(1) with Ω from Yang et al. (2019) is shown to be
π*(a∣s) = max {gφ ( 〃*(S)-f's,a)) , θ} ,	(2)
Q*(s, a) = r(s, a) + γEso^p(∙∣s,a)V*(s0), V*(s) = μ*(s) - λ X ∏*(a∣s)2φ0(∏*(a∣s)),	(3)
a∈A
where φ0(x) = ∂χφ(x), gφ is an inverse function of fφ for fφ(x) := xφ(x), X ∈ (0,1], and μ* is a
normalization term such that P°∈a ∏*(a∣s) = 1. Note that we still need to find out μ* to acquire
a closed-form relation between optimal policy ∏* and value function Q*. However, such relations
1-γE(s,a)~d∏ [r(S,a) — C(n(.|s)].
2
Published as a conference paper at ICLR 2021
have not been discovered except for Shannon-entropy regularization (Haarnoja et al., 2018) and
specific instances (q = 1, 2, ∞) of Tsallis-entropy regularization (Lee et al., 2019) to the best of our
knowledge.
Inverse Reinforcement Learning Given a set of demonstrations from an expert policy πE ,
IRL (Russell, 1998; Ng et al., 2000) is the problem of seeking a reward function from which we can
recover πE through RL. However, IRL in unregularized MDPs has been shown to be an ill-defined
problem since (1) any constant reward function can rationalize every expert and (2) multiple rewards
meet the criteria of being a solution (Ng et al., 2000). Maximum entropy IRL (MaxEntIRL) (Ziebart
et al., 2008; Ziebart, 2010) is capable of solving the first issue by seeking a reward function that
maximizes the expert’s return along with Shannon entropy of expert policy. Mathematically, for the
RL objective Jω in Eq.(1) and Ω = -H for negative Shannon entropy H(P) = Eo~p[- logp(a)]
(Ho & Ermon, 2016), the objective of MaxEntIRL is
)
MaxEntIRL(πE ) := arg max
r∈RS×A
J-H(r,πE)
max J-H (r, π)
π∈∆A
S
(4)
—
Another commonly used IRL method is Adversarial Inverse Reinforcement Learning (AIRL) (Fu
et al., 2018) which involves generative adversarial training (Goodfellow et al., 2014; Ho & Ermon,
2016) to acquire a solution of MaxEntIRL. AIRL considers the structured discriminator (Finn et al.,
2016a) D(s, a) = σ(r(s, a) - logπ(a∣s)) =「£；；(加)for σ(x) := 1/(1 + e-x) and iteratively
optimizes the following objective:
mgx，E(s,a)~d∏ [logDr,∏(s,a)] + E(s,a)^d∏ [log(1 - Dr,∏(s,a))],
r∈RS×A	E
maχ E(s,a)~d∏ [log Dr,∏(s, a) - log(1 - Dr,∏(s,a))] = maχ E(s,0)~d∏ [r(s, a) - logπ(a∣s)].
π∈∆SA	π∈∆SA
(5)
It turns out that AIRL minimizes the divergence between visitation distributions dπ and dπ by
solving min∏∈∆A KL(d∏ ∣∣d∏^) for Kullback-Leibler (KL) divergence KL(∙∣∣∙) (GhasemiPour et al.,
2019).	3 * S
3 Inverse Reinforcement Learning in Regularized MDPs
In this section, we propose the solution of IRL in regularized MDPs—regularized IRL—and relevant
properties in Section 3.1. We then discuss a specific instance of our proposed solution in Section 3.2
where Tsallis entropy regularizers and multi-variate Gaussian policies are used in continuous action
spaces.
3.1 Solutions of regularized IRL
We consider regularized IRL that generalizes MaxEntIRL in Eq.(4) to IRL with a class of strongly
convex policy regularizers:
IRLω(∏e )：
arg max
r∈RS×A
∣JΩ(r,πE )
max Jω(r,∏) ∖ .
π∈∆SA
(6)
—
For any strongly convex policy regularizer Ω, regularized IRL does not suffer from degenerate
solutions since there is a unique optimal policy in any regularized MDP (Geist et al., 2019). While
Geist et al. (2019) proposed solutions of regularized IRL, those are intractable solutions (See
Appendix F.1 for a detailed explanation). In the following lemma, we propose tractable solutions
that only requires the evaluation of the policy regularizer (Ω) and its gradient (VΩ) which are more
manageable in practice. Our solution is motivated from figuring out a reward function that is capable
of converting regularized RL into an equivalent divergence minimization problem associated with π
and πE :
Lemma 1. For a policy regularizer Ω : ∆A → R, let us define
t(s, a; π) := Ω0(s, a; π)—Ea0~n(.|s)[CO(S, a0； π)]+ C(n(Is))	⑺
for Ω0(s, ∙; π) := VΩ(π(∙∣s)) := [VpΩ(p)]p=∏(.∣s) ∈ RA, s ∈ S Then, t(s, a;怎)for expert policy
∏e is a solution of regularized IRL with Ω.
3
Published as a conference paper at ICLR 2021
Proof. (Abbreviated. See Appendix B for the full version.) With r(s, a) = t(s, a; πE), the RL
objective in Eq.(1) becomes equivalent to the problem of minimizing the discounted sum of Bregman
divergences between π and πE
∞
argmin En X YiDA(∏(∙∣Si)ll∏E (∙∣Si)) ,	(8)
π∈∆SA	i=0
where DA is the Bregman divergence (Bregman,1967) defined by DA(pι ∣∣P2) = Ω(pι) - Ω(p2)-
hVΩ(p2),p1 - P2)a for p1,p2 ∈ ∆A. Due to the non-negativity of the Bregman divergence, ∏ =怎
is a solution of Eq.(8) and is unique since Eq.(1) has the unique solution for arbitrary reward
functions (Geist et al., 2019).	口
In particular, for any policy regularizer Ω represented by an expectation over the policy (Yang et al.,
2019), Lemma 1 can be reduced to the following solution in Corollary 1:
Corollary 1. For Ω(p) = —λE°~pφ(p(a)) with P ∈ ∆A (Yanget al., 2019), Eq.(7) becomes
t(S,a； π) = -λ ∙ {fφ(n(a|S))- Ea0~π(∙∣s)[fφ(n(a1S))- 0(n(a1S))]}	(9)
for fφ(x) = ∂dX (xφ(x)). Theproofis in Appendix B.
Throughout the paper, We denote reward baseline by the expectation Ea~∏(.∣s)[fφ(∏(a∣s))-
φ(π(a∣s))]. Note that for continuous control tasks with Ω(p) = -λEo~pφ(p(a)), we can obtain
the same form of the reward in Eq.(9) (The proof is in Appendix D). Although the reward baseline
is generally not intractable in continuous control tasks, we derive a tractable reward baseline for
a special case (See Section 3.2). Additionally, when λ = 1 and φ(x) = - log x, it can be shown
that t(s,a; π) = logπ(a∣s)——for both discrete and continuous control problems——which was used
as a reward objective in previous work (Fu et al., 2018), and that the Bregman divergence in Eq.(8)
becomes the KL divergence KL(∏(∙∣s)∣∣∏e(∙∣s)).
Additional solutions of regularized IRL can be found by shaping t(S, a; πE) as stated in the following
lemma:
Lemma 2 (Potential-based reward shaping). Let π* be the solution ofEq.(1) in a regularizedMDP
Mr with a regularizer Ω : ∆A → R and a reward function r ∈ Rs×a. Thenfor Φ ∈ RS, using
either r(s, a) + γΦ(s0) — Φ(s) or r(s, a) + E§o~p(∙∣s,a)Φ(s0) — Φ(s) as a reward does not change
the solution of Eq.(1). The proof is in Appendix E.
From Lemma 1 and Lemma 2, we prove the sufficient condition of rewards being solutions of the
IRL problem. However, the necessary condition——a set of those solutions are the only possible
solutions for an arbitrary MDP——is not proved (Ng et al., 1999).
In the following lemma, we check how the proposed solution is related to the normalized state-
visitation distribution which can be discussed in the line of distribution matching perspective on
imitation learning problems (Ho & Ermon, 2016; Fu et al., 2018; Ke et al., 2019; Ghasemipour et al.,
2019):
Lemma 3. Given the policy regularizer Ω, let US define Ω(d) := E(s,a)^d[Ω(∏d(∙∣s))] for
arbitrary normalized state-action visitation distribution d ∈ ∆S×A and the policy ∏d(a∣s)
an
d(s,a)
Pa0∈A d(S,aO)
induced by d. Then, Eq.(7) is equal to
t(s,a; ∏d) = [VΩ(d)](s,a).
(10)
When Ω(d) is strictly convex and a solution t(s, a;怎)=VΩ(d∏^) of IRL in Eq.(10) is used, the RL
objective in Eq.(1) is equal to
arg min DS×A(d∏ ∣∣d∏E),
π∈∆SA
where D∣ ×A is the Bregman divergence among visitation distributions defined by DS ×A(dι∣∣d2)=
Ω(dι) - ΩΩ(d2) - hVΩΩ(d2),d1 — d2i for visitation distributions di and d2. The proofis in Appendix G
4
Published as a conference paper at ICLR 2021
Note that the strict convexity of Ω is required for DS×A to become a valid Bregman divergence.
Although the strict convexity of a policy regularize] Ω does not guarantee the assumption on the
strict convexity of Ω, it has been shown to be true for Shannon entropy regularizer (Ω = -H of
Lemma 3.1 in Ho & Ermon (2016)) and Tsallis entropy regularizer with its constants k = 2 ,q = 2
(Ω = -WW of Theorem 3 in Lee et al. (2018)).
3.2 IRL with Tsallis entropy regularization and Gaussian policies
For continuous controls, multi-variate Gaussian policies are often used in practice (Schulman et al.,
2015; 2017) and we consider IRL problems with those policies in this subsection. In particular, we
consider IRL with Tsallis entropy regularizer Ω(p) = -Tqk(P) = -Ea〜p[q-i(1 — p(a)q-1)] (Lee
et al., 2018; Yang et al., 2019; Lee et al., 2020) for a multi-variate Gaussian policy π(∙∣s) =
N (μ(s), ∑(s)) with μ(s) = [μι(s), ...,μd(s)]T, Σ(s) = diag{(σι(s))2,…,(σd(s))2}. Insucha
case, we can obtain tractable forms of the following quantities:
Tsallis entropy. The tractable form of Tsallis entropy for a multi-variate Gaussian policy is
7	k(1 - e(1—q)Rq(n(・|S)))	d	logq ]
τqk (∏(忖)=-1——q-1--------------), Rq (∏(∙∣s)) = X pog(√2∏σi(s)) - 2(τ-q)∣
for Renyi entropy Rq . Its derivation is given in Appendix I.
Reward baseline. The reward baseline term Ea〜n^⑸[fφ(π(a∣s)) — φ(π(a∣s))] in Corollary 1 is
generally intractable except for either discrete control problems or Shannon entropy regularization
where the reward baseline is equal to -1. Interestingly, as long as the tractable form of Tsallis
entropy can be derived, that of the corresponding reward baseline can also be derived since the reward
baseline satisfies
Ea-∏(∙∣s) [fφ(∏(a∣s)) - Φ(∏(a∣s))] = (q - 1)Ea-∏(∙∣s) [φ(∏(a∣s)] - - = (q - 1)Tqk(∏(∙∣s))--.
Here, the first equality holds with fφ (x) = q-ι (1 — qxq-1) = qφ(x) - - for Tsallis entropy
regularization.
Bregman divergence associated with Tsallis entropy. For two different multivariate Gaussian
policies, we derive the tractable form of the Bregman divergence (associated with Tsallis entropy)
between two policies. The resultant divergence has a complicated form, so we leave its derivation in
Appendix I.3.
Figure 1: Bregman divergence Dω(∏∣∣∏e) associated with Tsallis entropy (Ω(p) = -Tq(p))
between two uni-variate Gaussian distributions π = N(μ, σ2) and ∏ = N(0, (e-3)2) (green point
in each subplot). In each subplot, we normalized the Bregman divergence so that the maximum value
becomes 1. Note that for q = 1, Dω(∏∣∣∏e) becomes the KL divergence KL(π∣∣πe).
For deeper understanding of Tsallis entropy and its associated Bregman divergence in regularized
MDPs, we consider an example in Figure 1. We first assume that both learning agents’ and ex-
perts, policies follow uni-variate Gaussian distributions π = N(μ, σ2) and 旗 =N(0, (e-3)2),
respectively. We then evaluate the Bregman divergence in Figure 1 by using its tractable form and
varying q from 1.0—which corresponds to the KL divergence—to 2.0. We observe that the constant
q from the Tsallis entropy affects the sensitivity of the associated Bregman divergence w.r.t. the mean
and standard deviation of the learning agent’s policy π. Specifically, as q increases, the size of the
valley-relatively red region in Figure 1-across the μ-axis and log σ-axis decreases. This suggests
that for larger q, minimizing the Bregman divergence requires more tightly matching means and
variances of π and πE .
5
Published as a conference paper at ICLR 2021
Algorithm 1: Regularized Adversarial Inverse Reinforcement Learning (RAIRL)
1:	Input: A set DE of expert demonstration generated by expert policy ∏, a reward approximator
rθ, a policy πψ for neural network parameters θ and ψ
2:	for each iteration do
3:	Sample rollout trajectories by using the learners policy πψ .
4:	Optimize θ with the discriminator Drθ,πψ and the learning objective in Eq.(11).
5:	Optimize ψ with Regularized Actor Critic by using rθ as a reward function.
6:	end for
7:	Output: πψ ≈ πE, rθ(s, a) ≈ t(s, a; πE) (a solution of the IRL problem in Lemma 1).
4 Algorithmic Consideration
Based on a solution for regularized IRL in the previous section, we focus on developing an IRL
algorithm in this section. Particularly to recover the reward function t(s, a; πE ) in Lemma 1, we
design an adversarial training objective as follows. Motivated by AIRL (Fu et al., 2018), we consider
the following structured discriminator associated with π, r and t in Lemma 1:
Dr,π(s, a) = σ(r(s, a) - t(s, a; π)), σ(z)
1
1 + e-z
, z ∈ R.
Note that we can recover the discriminator of AIRL in Eq.(5) when t(s, a) = logπ(a∣s) (φ(x)=
log x and λ = 1). Then, we consider the following optimization objective of the discriminator which
is the same as that of AIRL:
t(s,a; ∏) := argmax E(s,a)〜d∏ [logDr,∏(s, a)] + E(s,θ)〜d∏ [log(1 - Dr,∏(s, a))].	(11)
r∈RS×A	E
Since the function X → a log σ(x) + b log(1 — σ(x)) attains its maximum at σ(x) = 0++^, or
equivalently at X = log b (Goodfellow et al., 2014; Mescheder et al., 2017), it can be shown that
dπ (s, a)
t(s,a； ∏) = t(s,a; ∏) + log 二TE——「.	(12)
dπ (s, a)
When π = πE in Eq.(12), we have t(s, a; πE) = t(s,a; πE) since d∏ = d∏ , which means the
maximizer t becomes the solution of IRL after the agent successfully imitates the expert policy πE .
To do so, we consider the following iterative algorithm. Assuming that we find out the optimal reward
approximator i(s, a; π(i)) in Eq.(12) for the policy π(i) of the i-th iteration, we get the policy π(i+1)
by optimizing the following objective with gradient ascent:
maximize E(s,a)〜d∏
π∈∆SA	π
p(s, a; π⑴)一Ω(π(∙∣s))J
(13)
The above expectation in Eq.(13) can be decomposed into the following two terms
E(s,a)〜d∏ [f(s,a; ∏(i)) — Ω(π(∙∣s))] = E(s,a)〜d∏ "s, a; π⑴)一Ω(π(∙∣s))] — KL(d∏∣∣d∏E)
—E(s,a)〜d∏ |DA(n(・|s)||n(i)(・|s))] - KLmn lldπE ),
'-----------------------------------} |
{z
(I)
-- J
{z
(II)
(14)
where the second equality follows since Lemma 1 tells us that t(s, a; π(i)) is a reward function that
makes π(i) an optimal policy in the Ω-regularized MDp Minimizing term (II) in Eq.(14) makes
π(i+1) close to πE while minimizing term (I) can be regarded as a conservative policy optimization
around the policy π(i) (Schulman et al., 2015).
In practice, we parameterize our reward and policy approximations with neural networks and train
them using an off-policy Regularized Actor-Critic (RAC) (Yang et al., 2019) as described in Algo-
rithm 1.Below, we evaluate our Regularized Adversarial Inverse Reinforcement Learning (RAIRL)
approach across various scenarios.
6
Published as a conference paper at ICLR 2021
5	Experiments
We summarize the experimental setup as follows. In our experiments, We consider Ω(p) =
-λEa〜P[φ(p(a)] with the following regularizers from Yang et al. (2019): (1) Shannon entropy
(φ(x) = — log x), (2) Tsallis entropy regularizer (φ(x) = q-ι (1 — xq-1)), (3) exp regularizer
(φ(x) = e — ex), (4) Cos regularizer (φ(x) = cos(2x)), (5) Sin regularizer (φ(x) = 1 — Sin 2x).
The above regularizers were chosen since other regularizers have not been empirically validated to
the best of our knowledge. We chose these regularizers to make our empirical analysis more tractable.
In addition, we model the reward approximator of RAIRL as a neural network with either one of
the following models: (1) Non-structured model (NSM)—a simple feed-forward neural network that
outputs real values used in AIRL (Fu et al., 2018)—and (2) Density-based model (DBM)—a model
using a neural network for π (softmax for discrete controls and multi-variate Gaussian model for
continuous controls) of the solution in Eq.(1) (See Appendix J.2 for a detailed explanation). For the
RL algorithm of RAIRL, we implement Regularized Actor Critic (RAC) (Yang et al., 2019) on top of
the SAC implementation from Rlpyt (Stooke & Abbeel, 2019). Other settings are summarized in
Appendix J. For all experiments, we use 5 runs and report 95% confidence intervals.
5.1	Experiment 1: Multi-armed Bandit (Discrete Action)
We consider a 4-armed bandit environment as shown in Figure 2 (left). An expert policy πE is
assumed to be either dense (with probability 0.1, 0.2, 0.3, 0.4 for a = 0, 1, 2, 3) or sparse (with
probability 0, 0, 1/3, 2/3 for a = 0, 1, 2, 3). For those experts, we use RAIRL with actions sampled
from πE and compare learned rewards with the ground truth reward t(s, a; πE) in Lemma 1. When
πE is dense, RAIRL successfully acquires the ground truth rewards irrespective of the reward model
choices. When sparse πE is used, however, RAIRL with a non-structured model (RAIRL-NSM) fails
to recover the rewards for a = 0, 1—where πE (a) = 0—due to the lack of samples at the end of
the imitation. On the other hand, RAIRL with a density-based model (RAIRL-DBM) can recover
the correct rewards due to the softmax layer which maintains the sum over the outputs equal to
1. Therefore, we argue that using DBM is necessary for correct reward acquisition since a set of
demonstrations is generally sparse. In the following experiment, we show that the choice of reward
models indeed affects the performance of rewards.
Figure 2: Expert policy (Left) and reward learned by RAIRL with different types of policy regularizers
(Right) in Multi-armed Bandit. Either one of dense (Top row) or sparse (Bottom row) expert policies
πE is considered.
5.2	Experiment 2: Bermuda World (Continuous Observation, Discrete Action)
We consider an environment with a 2-dimensional continuous state space as described in Figure 3.
At each episode, the learning agent is initialized uniformly on the x-axis between —5 and 5, and
there are 8 possible actions—an angle in {一∏, — 4∏,…，1 ∏, 4∏} that determines the direction of
movement. An expert in Bermuda World considers 3 target positions (—5, 10), (0, 10), (5, 10) and
behaves stochastically. We state how we mathematically define the expert policy πE in Appendix J.3.
During RAIRL’s training (Figure 3, Top row), we use 1000 demonstrations sampled from the expert
and periodically measure mean Bregman divergence, i.e., for DA(p1 ||p2) = Ea〜p1 [fφ(p2(a))—
7
Published as a conference paper at ICLR 2021
φ(PI (O))] - Ea〜P2[fφ(P2(O))- φ(P2(O))],
1N
N X DA(∏(∙∣Si )II∏E (∙∣Si)).
i=1
Here, the states s1, ..., sN come from 30 evaluation trajectories that are stochastically sampled from
the agent's policy π—which is fixed during evaluation—in a separate evaluation environment. During
the evaluation of learned reward (Figure 3, Bottom row), we train randomly initialized agents with
RAC and rewards acquired from RAIRL’s training and check if the mean Bregman divergence is
properly minimized. We measure the mean Bregman divergence as was done in RAIRL’s training.
RAIRL-DBM is shown to minimize the target divergence more effectively compared to RAIRL-NSM
during reward evaluation, although both achieve comparable performances during RAIRL’s training.
Moreover, we substitute λ with 1, 5, 10 and observe that learning with λ larger than 1 returns better
rewards—only λ = 1 was considered in AIRL (Fu et al., 2018). Note that in all cases, the minimum
divergence achieved by RAIRL is comparable with that of behavioral cloning (BC). This is because
BC performs sufficiently well when many demonstrations are given. We think the divergence of BC
may be the near-optimal divergence that can be achieved with our policy neural network model.
RAIRL-DBM (λ = 1)
RAIRL-DBM (λ = 5)
RAIRL-DBM (λ = 10)
RAIRL-NSM (λ = 1)
RAIRL-NSM (λ = 5)
RAIRL-NSM (λ = 10)
BC
Random
Expert
Figure 3: Mean Bregman divergence during training (Top row) and the divergence during reward
evaluation (Bottom row) in Bermuda World. In each column, different policy regularizers and their
respective target divergences are considered. The results are reported after normalization with the
divergence of uniform random policy, and that of behavioral cloning (BC) is reported for comparison.
5.3	Experiment 3: MuJoCo (Continuous Observation and Action)
We validate RAIRL on MuJoCo continuous control tasks (Hopper-v2, Walker-v2, HalfCheetah-v2,
Ant-v2) as follows. We assume multivariate-Gaussian policies (with diagonal covariance matrices)
for both learner’s policy π and expert policy πE. Instead of tanh-squashed policy in Soft-Actor
Critic (Haarnoja et al., 2018), we use hyperbolized environments—where tanh is regarded as a
part of the environment—with additional engineering on the policy networks (See Appendix J.4 for
details). We use 100 demonstrations stochastically sampled from πE to validate RAIRL. In MuJoCo
experiments, we focus on a set of Tsallis entropy regularizer (Ω = -Tq) with q = 1,1.5, 2—where
Tsallis entropy becomes Shannon entropy for q = 1. We then exploit the tractable quantities for
multi-variate Gaussian distributions in Section 3.2 to stabilize RAIRL and check its performance in
terms of its mean Bregman divergence similar to the previous experiment. Note that since both π
and πE are multi-variate Gaussian and can be evaluated, we can evaluate the individual Bregman
divergence DA(π(∙∣s)∣∣πe(∙∣s)) on S by using the derivation in Appendix I.3.
The performances during RAIRL’s training are described as follows. We report π with both an
episodic score (Figure 4) and mean Bregman divergences with respect to three types of Tsallis
entropies (Figure 5) Ω = -Tq with q0 = 1,1.5, 2. Note that the objective of RAIRL with Ω = -Tq
is to minimize the corresponding mean Bregman divergence with q0 = q . In Figure 4, both RAIRL-
DBM and RAIRL-NSM are shown to achieve the expert performance, irrespective of q, in Hopper-
v2, Walker-v2, and HalfCheetah-v2. In contrast, RAIRL in Ant-v2 fails to achieve the expert’s
performance within 2,000,000 steps and RAIRL-NSM highly outperforms RAIRL-DBM in our
8
Published as a conference paper at ICLR 2021
setting. Although the episodic scores are comparable for all methods in Hopper-v2, Walker-v2, and
HalfCheetah-v2, respective divergences are shown to be highly different from one another as shown in
Figure 5. RAIRL with q = 2 in most cases achieves the minimum mean Bregman divergence (for all
three divergences with q0 = 1, 1.5, 2), whereas RAIRL with q = 1—which corresponds to AIRL (Fu
et al., 2018)—achieves the maximum divergence in most cases. This result is in alignment with
our intuition from Section 3.2; as q increases, minimizing the Bregman divergence requires much
tighter matching between π and πE . Unfortunately, while evaluating the acquired reward—RAC with
a randomly initialized agent and acquired reward—the target divergence is not properly decreased
in continuous controls. We believe this is because π is a probability density function in continuous
controls and causes large variance during training, while π is a mass function and is well-bounded in
discrete control problems.
Figure 4: Averaged episodic score of RAIRL’s training in MuJoCo environments. RAIRL with Tq1
regularizer with q = 1, 1.5, 2 is considered.
Bregman
Div.
(q' = 2)
Bregman
Div.
(Qz = I)
Bregman
Div.
(q' = L5)
RAIRL-NSM (q = 1)	RAIRL-NSM (q = 1.5)	RAIRL-NSM (q = 2)
RAIRL-DBM (q = 1)	RAIRL-DBM (q = 1.5)	RAlRL-DBM (q = 2)
Figure 5: Bregman divergences with Tsallis entropy Tq10 with q0 = 1, 1.5, 2 during RAIRL’s training
in MuJoCo environments. We consider RAIRL with Tsallis entropy regularizer Tq1 with q = 1, 1.5, 2.
6	Discussion and Future Works
We consider the problem of IRL in regularized MDPs (Geist et al., 2019), assuming a class of strongly
convex policy regularizers. We theoretically derive its solution (a set of reward functions) and show
that learning with these rewards is equivalent to a specific instance of imitation learning—i.e., one
that minimizes the Bregman divergence associated with policy regularizers. We propose RAIRL—a
practical sampled-based IRL algorithm in regularized MDPs—and evaluate its applicability on policy
imitation (for discrete and continuous controls) and reward acquisition (for discrete control).
Finally, recent advances in imitation learning and IRL are built from the perspective of regarding
imitation learning as statistical divergence minimization problems (Ke et al., 2019; Ghasemipour
et al., 2019). Although Bregman divergence is known to cover various divergences, it does not
include some divergence families such as f-divergence (Csiszar, 1963; Amari, 2009). Therefore,
we believe that considering RL with policy regularization different from Geist et al. (2019) and
its inverse problem is a possible way of finding the links between imitation learning and various
statistical distances.
9
Published as a conference paper at ICLR 2021
Acknowledgments
We would like to thank Daewoo Kim at Waymo for his valuable comments on this work.
References
Shun-Ichi Amari. α-divergence is unique, belonging to both f -divergence and bregman divergence
classes. IEEE Transactions on Information Theory, 55(11):4925-4931, 2009.
Abdeslam Boularias and Brahim Chaib-Draa. Bootstrapping apprenticeship learning. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 289-297, 2010.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its
application to the solution of problems in convex programming. USSR computational mathematics
and mathematical physics, 7(3):200-217, 1967.
Imre Csiszdr. Eine informationstheoretische UngleiChUng Und ihre anwendung auf den beweis der
ergodizitat von markoffschen ketten. Magyar. Tud. Akad. Mat. Kutato Int. Kozl, 8:85-108, 1963.
Robert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation
learning. arXiv preprint arXiv:2006.04678, 2020.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016a.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In Proceedings of the 33rd International Conference on Machine Learning
(ICML), pp. 49-58, 2016b.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforce-
ment learning. In Proceedings of the 6th International Conference on Learning Representations
(ICLR), 2018.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In Proceedings of the 35th International Conference on Machine Learning (ICML),
pp. 1582-1591, 2018.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision
processes. In Proceedings of the 36th International Conference on Machine Learning (ICML), pp.
2160-2169, 2019.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Proceedings of the 3rd Conference on Robot Learning
(CoRL), 2019.
Ian Goodfellow, Jean Pouget Abadie, Mehdi Mirza, Bing Xu, David Warde Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 2672-2680, 2014.
Xin Guo, Johnny Hong, and Nan Yang. Ambiguity set and learning via Bregman and Wasserstein.
arXiv preprint arXiv:1705.08056, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning (ICML), pp. 1861-1870, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 4565-4573, 2016.
Lee K Jones and Charles L Byrne. General entropy criteria for inverse problems, with applications to
data compression, pattern classification, and cluster analysis. IEEE Transactions on Information
Theory, 36(1):23-30, 1990.
10
Published as a conference paper at ICLR 2021
Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha Srinivasa.
Imitation learning as f -divergence minimization. arXiv preprint arXiv:1905.12888, 2019.
Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Maximum causal tsallis entropy imitation learning.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 4403-4413, 2018.
Kyungjae Lee, Sungyub Kim, Sungbin Lim, Sungjoon Choi, and Songhwai Oh. Tsallis reinforcement
learning: A unified framework for maximum entropy reinforcement learning. arXiv preprint
arXiv:1902.00137, 2019.
Kyungjae Lee, Sungyub Kim, Sungbin Lim, Sungjoon Choi, Mineui Hong, Jaein Kim, Yong-Lae
Park, and Songhwai Oh. Generalized Tsallis entropy reinforcement learning and its application to
soft mobile robots. In Robotics: Science and Systems (RSS), 2020.
Kun Li, Mrinal Rath, and Joel W Burdick. Inverse reinforcement learning via function approximation
for clinical motion analysis. In 2018 IEEE International Conference on Robotics and Automation
(ICRA), pp. 610-617. IEEE, 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational Bayes: Unify-
ing variational autoencoders and generative adversarial networks. In Proceedings of the 34th
International Conference on Machine Learning (ICML), pp. 2391-2400, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pp.
1928-1937, 2016.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Proceedings of the 16th International Conference on
Machine Learning (ICML), pp. 278-287, 1999.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Proceedings
of the 17th International Conference on Machine Learning (ICML), pp. 663-670, 2000.
Frank Nielsen and Richard Nock. On Renyi and Tsallis entropies and divergences for exponential
families. arXiv preprint arXiv:1105.3259, 2011.
Stuart Russell. Learning agents for uncertain environments. In Proceedings of the 11th Annual
Conference on Computational Learning Theory, pp. 101-103, 1998.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML), pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Sahand Sharifzadeh, Ioannis Chiotellis, Rudolph Triebel, and Daniel Cremers. Learning to drive
using inverse reinforcement learning and deep Q-networks. arXiv preprint arXiv:1612.03653,
2016.
Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in
pytorch. arXiv preprint arXiv:1909.01500, 2019.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear pro-
gramming. In Proceedings of the 25th International Conference on Machine Learning (ICML), pp.
1032-1039, 2008.
Zheng Wu, Liting Sun, Wei Zhan, Chenyu Yang, and Masayoshi Tomizuka. Efficient sampling-based
maximum entropy inverse reinforcement learning with application to autonomous driving. IEEE
Robotics and Automation Letters, 5(4):5355-5362, 2020.
11
Published as a conference paper at ICLR 2021
Wenhao Yang, Xiang Li, and Zhihua Zhang. A regularized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), pp.
5938-5948, 2019.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDice: Generalized offline estimation of
stationary values. In International Conference on Learning Representations, 2019.
Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,
volume 8, pp. 1433-1438, 2008.
12
Published as a conference paper at ICLR 2021
A B ellman Operators, Value Functions in regularized MDPs
Let a policy regularize] Ω : ∆A → R be strongly convex, and define the convex conjugate of Ω is
Ω* : RA → R as
Ω*(Q(s, ∙))=	max h∏(∙∣s),Q(s, ∙)iA - Ω(π(∙∣s)),Q ∈ RS×A,s ∈ S.	(15)
π(∙∣s)∈∆A
Then, Bellman operators, equations and value functions in regularied MDPs are defined as follows.
Definition 1 (Regularized Bellman Operators). For V ∈ RS, let us define Q(s, a) = r(s, a) +
YE§o~p(∙∣s,α)[V(s0)]. The regularized Bellman evaluation operator is defined as
[TπV](S) := hπ(IS), Q(S, ∙)iA - a(n(IS)), S ∈ S,
and TπV = V is called the regularized Bellman equation. Also, the regularized Bellman optimality
operator is defined as
[T*V](s) :=	max [TπV](s) = Ω*(Q(s, ∙)), S ∈ S,
π(∙∣s)∈∆A
and T*V = V is called the regularized Bellman Optimality equation.
Definition 2 (Regularized value functions). The unique fixed point Vπ of the operator Tπ is called
the state value function, and Qn (s, a) = r(S, a) + γEs∕~p (∙∣s,α) [Vπ (s0)] is called the state-action
value function. Also, the unique fixed point V * ofthe operator T* is called the optimal state value
function, and Q*(s, a) = r(S, a) + YE§o~p(∙∣s,a)[V*(s0)] is called the optimal state-action value
function.
It should be noted that Proposition 1 in Geist et al. (2019) tells us VΩ*(Q(s, ∙)) is a policy that
uniquely maximizes Eq.(15). For example, when Ω(π(∙∣s)) = Pa~∏(.∣s) log∏(a∣S) (negative Shan-
non entropy), VΩ*(Q(s, ∙)) is a softmax policy, i.e., VΩ*(Q(s, ∙)) = P exp!?；'))",、、. Due to this
a0 ∈A exp(Q(s,a ))
property, the optimal policy π* of a regularized MDP is uniquely found for the optimal state-action
value function Q* which is also uniquely defined as the fixed point of T*.
B PROOF OF Lemma 1
Let US define πs = π(∙∣s). For r(S, a) = t(S, a; ∏), the RL objective Eq.(1) satisfies
∞
En X Yi {r(Si,ai) - Ω(πsi)}
i=0
∞
=En X Yi {t(S,a; ∏e) — Ω(πsi)}
i=0
∞
(=) En Xγi {a0(Si,ai；πe) — Ea~∏EiωO(Si,a;πe) + a(nEi)- a(nsi)}
i=0
∞
(=) En X Yi {Ea~nSia0(Si,a； ∏ ) — 叫~屈稔 ɑ'(si, a； ∏ ) + 虱嚏 ) — ω(π" )}
i=0
∞
(=) En XYi {hVΩ(∏Ei),πsiiA — hVΩ(∏Ei),∏EiiA + Ω(∏Ei) — Ω(πsi)}
i=0
∞
=En X Yi {Ω(∏Ei) — Ω(πsi) + hVΩ(∏Ei),πsiiA — hVΩ(∏Ei),∏EiiA}
i=0
∞
=-En X Yi {Ω(∏si) — Ω(∏Ei) — hVΩ(∏Ei),πsiiA + hVΩ(∏Ei),πjiiA}
i=0
∞
-En X Yi {Ω(πsi) — Ω(∏Ei) — hVΩ(∏Ei),πsi — ∏EiiA}
i=0
∞
XYiDΑ(∏sill∏Ei)),
i=0
(16)
13
Published as a conference paper at ICLR 2021
where (i) follows from the assumption r(s, a) = t(s, a; πE) in Lemma 1, (ii) follows from
the definition of t(s, a; π) in Eq.(7), (iii) follows since taking the inner expectation first does
not change the overall expectation, (iv) follows from the definition of Ω0 in Lemma 1 and
Pa∈Ap(a)[VΩ(p)](a) = hVΩ(p),pi/, and (V) follows from the definition of Bregman diver-
gence, i.e., DA(Pi||p2)= Ω(pι) 一 Ω(p2) — hVΩ(p2),p1 — p2)人. Due to the non-negativity of DA,
Eq.(16) is less than or equal to zero which can be achieved when π = πE.
C PROOF OF Corollary 1
Let a ∈ {1, ..., |A|} and πa = π(a) for simplicity. For
Ω(∏) = -λEa〜∏φ(∏a) = -λ £ ∏aφ(∏a)= 一1 £ fφ(∏a)
a∈A
a∈A
with fφ(x) = xφ(x), we have
Va(n) = -λv∏ι,…,∏∣a∣E fφ (πa) = -λ[fφ (πl), …,fφ (π∣A∣)]T
a∈A
for fφ(x) = ∂∂χ(xφ(x)). Therefore, for ∏α = ∏(∙∣s) we have
t(s,a; π) = [VΩ(πs)](a) — hVΩ(πs),πs)A + Ω(πs)
-λfφ(∏α)一 E ∏α,(-λfφ(∏sj) 十 一入 £ πφ(∏sj
a0∈A	a0∈A
-λ
-λ
f fφ(∏a) 一 (X ∏Sofφ(∏aj) + (X ∏SoΦ(∏αj) ∖
a0∈A	a0∈A
[fφ (∏a)一 X ∏So fφ (∏a,)- Φ(∏aa]
a0∈A
D Proof of Optimal Rewards on Continuous Controls
Note that for two continuous distributions P1 and P2 having probability density functions p1 (x) and
p2(x), respectively, the Bregman divergence can be defined as (Guo et al., 2017; Jones & Byrne,
1990)
DX (PIM):=(
{ω(p1 (x)) 一 ω(p2(x)) 一 ω0(p2(x))(p1(x) 一 p2(x))} dx,
(17)
where ω0(x) := ∂∂χω(χ) and the divergence is measured point-wisely on X ∈ X. Let us assume
Ω(∏) = / ω(π(a))da
A
(18)
for a probability density function π on A and define
t(s, a; π)
ω0 3TA
[πα(a0)ω0(πα(a0)) 一 ω(πα(a0))] da0.
(19)
14
Published as a conference paper at ICLR 2021
for πs = ∏(∙∣s). For r(s, a) = t(s, a; ∏), the RL objective in Eq.(1) satisfies
∞
En X Yi {r(si,ai) - Ω(πsi)}
i=0
∞
X Yi {t(si,ai; ∏E) - Ω(πsi)}
i=0
(=ii) Eπ
∞
XYi
i=0
∞
X
i=0
ω0(πEsi(ai)) - A πEsi(a)ω0(πEsi(a))-ω(πEsi(a))da- Aω(πsi(a))da
πsi (a)ω0(πsi (a))da -	πsi (a)ω0(πsi (a)) - ω(πsi (a)) + ω (π si (a)) da
E	AE	E	E
∞
X Yi / {πsi(a)ω0(πEi(a)) - [πEi(a)ω0(πEi(a))-ω(πEi(a)) + ω(πsi(a))]} da
i=0 A
∞
Xi=0YiZA
+ πsi (a)ω0(πEsi (a)) - πEsi (a)ω0(πEsi (a))} da
∞
= -Eπ XYi	{ω(πsi(a)) - ω(πEsi(a)) - πsi (a)ω0(πEsi (a)) +πEsi(a)ω0(πEsi(a))}da
i=0	A
∞
=-En Xγi/ {ω(πsi(a))-ω(πEi(O))-ω0(nEi(O))(πsi⑷-πEi⑷)}da
i=0	A
∞
=) -En X γiDA(πsi∣∣∏Ei)),
i=0
(20)
where (i) follows from r(s, a) = t(s, a; πE), (ii) follows from Eq.(18) and Eq.(19), and (iii) follows
from the definition of Bregman divergence in Eq.(17). Due to the non-negativity of Dω, Eq.(24) is
less than or equal to zero which can be achieved when π = πE . Also, π = πE is a unique solution
since Eq.(1) has a unique solution for arbitrary reward functions.
E PROOF OF Lemma 2
Since Lemma 2 was mentioned but not proved in Geist et al. (2019), we derive the rigorous proof for
Lemma 2 in this subsection. Note that we follow the proof idea in Ng et al. (1999).
First, let US assume an MDP Mr with a reward r and corresponding optimal policy π*. From
Definition 1, the optimal state-value function V*,r and its corresponding state-action value function
Q*,r(s, a) := r(s, a) + γEso~p(∙∣s,α)[V*,r(s0)] should satisfy the regularized Bellman optimality
equation
V *，r (s) = T *，r V *，r (s) = Ω*(Q*,r (s,a))
=max √π(∙∣s),Q*,r(S,a)〉A - Ω(π(∙∣s))
n(∙∣s)∈∆A
=HmaxAh∏(∙∣s),r(s,a) + γEs'~P(∙∣s,a)[V*'r(s')Da - Ω(∏(∙∣s)),	(21)
where we explicitize the dependencies on r. Also, the optimal policy ∏ is a unique maximizer for
the above maximization.
Now, let us consider the shaped rewards r(s,a) + Φ(s0) -Φ(s) and r(s,a)+Es，~p(.∣s,a)Φ(s0) -Φ(s).
Please note that for both rewards, the expectation over S0 for given S, a is equal to
r(s, a) = r(S, a) + Es0~P(∙∣s,α)φ(SO)- φ(S),
15
Published as a conference paper at ICLR 2021
and thus, it is sufficient to only consider the optimality for r. By subtracting Φ(s) from the regularized
optimality Bellman equation for r in Eq.(21), we have
V*，r (S) - Φ(s)
=	™ax √∏(∙∣s),r(s,a) + γEs0〜P(∙∣s,a)Φ(s0) — Φ(s) + γEs,〜P(∙∣s,α)[V*,r(s') - Φ(s')Da
π(∙∣s)∈Δa
-a(n(IS))
=	max h∏(∙∣s),r(s,a) + γEs,〜P (∙∣s,α)[V *,r (s') — Φ(s')]>A — Ω(π(∙∣s))
π(∙∣s)∈∆A
=[T『V *,r — Φ)](s).
That is, V*,r 一 Φ is the fixed point of the regularized Bellman optimality operator T*,r associated
with the shaped reward r. Also, a maximizer for the above maximization is ∏* since subtracting Φ(s)
from Eq.(21) does not change the unique maximizer ∏*.
F Comparison between our solution and existing solution in Geist
et al. (2019)
F.1 Issues with the solutions in Geist et al. (2019)
In Proposition 5 of Geist et al. (2019), a solution of regularized IRL is given, and we rewrite the
relevant theorem in this subsection. Let us consider a regularized IRL problem, where πE ∈ ∆SA is
an expert policy. Assuming that both the model (dynamics, discount factor and regularizer) and the
expert policy are known, Geist et al. (2019) proposed a solution of regularized IRL as follows:
Lemma 4 (A solution of regularized IRL from Geist et al. (2019)). Let QE ∈ RS×A be a function
such that ∏e(∙∣s) = VΩ*(Qe (s, ∙)), S ∈ S. Also, define
rE (S,a) := QE (S,a) — Y Es0 〜P (.|s,a)[C*(QE (S', •))]
=Qe (s,a) — γEs0〜P(∙∣s,α)[(怎”『)《(s', .)〉/ — Ωg (∙∣s'))].
Then, in the Ω-regularized MDP with the reward rE,怎 is an optimal policy.
Proof. Although the brief idea of the proof is given in Geist et al. (2019), we rewrite the proof in
a more rigorous way as follows. Let us define VE(s) = h∏E(∙∣s), Qe(s, •))/ 一 Ω(∏E(∙∣s)). Then,
rE (s,a) = Qe (s,a) — γEs0〜P(∙∣s,a) [VE (s')],and thus, QE (s,a) = rE (s,a) + YE§o〜P(∙∣s,a)[VE (s')].
By using this and regularized Bellman optimality operator, we have
[T*VE](s) = Ω*(Qe (s, ∙)) =)	max h∏(∙∣s), QE(s, .))/ — Ω(π(∙∣s))
π(∙∣s)∈∆A
(=) hπE (∙ls),QE (S, ∙)iA — ω(πe (1S)) = VE (S),
where (i) and (ii) follow from Definition 1, and (iii) follows since πE is a unique maximizer. Thus,
VE is the fixed point of T*, and ∏e(∙∣s) = VΩ*(Qe (s, ∙)),s ∈ S, becomes an optimal policy. □
For example, when negative Shannon entropy is used as a regularizer, we can get rE (s, a) =
log πE (a|s) by setting QE (s, a) = log πE (a|s). However, a solution proposed in Lemma 4 has two
crucial issues:
Issue 1. It requires the knowledge on the model dynamics, which is generally intractable.
Issue 2. We need to figure out QE that satisfies ∏(∙∣s) = VΩ*(Qe(s, ∙)), which comes from the
relationship between the optimal value function and optimal policy (Geist et al., 2019).
In the following subsection, we show how our solution in Lemma 1 is related to the solution from
Geist et al. (2019) in Lemma 4.
F.2 Relation between the solution of Geist et al. (2019) and our tractable
SOLUTION
Let us consider the expert policy πE and functions QE and rE satisfying the conditions in Lemma 4.
From Lemma 2, a regularized MDP with the shaped reward
rE (S,a) := rE (S,a)+ YES，〜P(∙∣s,a)φ(SO)- φ(S)
16
Published as a conference paper at ICLR 2021
for Φ ∈ RS has its optimal policy as πE . Since Φ can be arbitrarily chosen, let us assume Φ(s) =
Ω*(Qe (s, ∙)). Then, we have
rE (SM
=TE (S, a) + YES，〜P(∙∣s,a)φ(SO) - φ(S)
={Qe (s,a) - YES，〜P (∙∣s,a)[Ω*(Qm (s0,∙))]} + YES，〜P(∙∣s,a) Ω* (Qe (s0, •))- Ω*(Qe (s, ∙))
=Qe(s,a) - Ω*(Qe(s,∙)).	(22)
Note that the reward in Eq.(22) does not require the knowledge on the model dynamics, which
addresses Issue 1 in Appendix F.1. Also, by using VE (s) = Ω*(Qe (s, ∙)) in the proof of Lemma 4,
the reward in Eq.(22) can be written as
rE (s,a) = QE (s,a) - VE (S),
which means TE (s, a) is an advantage function for the optimal policy ∏ in the Ω-regularized MDP.
However, We still have Issue 2 in Appendix F.1 since Ω* in Eq.(22) is generally intractable (Geist
et al., 2019), which prevents us from finding the appropriate QE (S, a). Interestingly, we show that for
all S ∈ S and QE (s, •) = PΩ(∏ (∙∣s)),
VΩ*(Qe(s, ∙)) = arg max hπ(∙∣s), Qe (s, ∙))a - Ω(π(∙∣s))
π(∙∣ s)∈∆A
=arg max h∏(∙∣S), VΩ(∏E (∙∣s))>a — Ω(π(∙∣s))
π(∙∣ s)∈∆A
=arg min Ω(π(∙∣s)) - hVΩ(πE (∙∣s)), ∏(∙∣S)iA
π(∙∣ s)∈∆A
=argmin a(n(・|S)) - a(nE(IS))- "ω^e(IS)),π(IS)- πE(IS)〉a
π(∙∣ s)∈∆A
=argmin DA(n(1S)||nE (1S))= πE (∙|S),
π(∙∣ s)∈∆A
where the last equality holds since the Bregman divergence DA(∏(∙∣s)∣∣∏e(∙∣s)) is greater than
or equal to zero and its lowest value is achieved when ∏(∙∣s) = ∏E(∙∣s). This means that when
Qe(s, ∙) = VΩ(∏E(∙∣s)) is used, the condition πE(∙∣s) = VΩ*(QE(s, ∙)) in Lemma 4 is satisfied
without knowing the tractable form of Ω* or VΩ*. Thus, Issue 2 in Appendix F.1 is addressed; we
instead require the knowledge on the gradient VΩ of the policy regularizer Ω, which is practically
more tractable. Finally, by substituting QE (s, ∙) = Ω0(s, ∙; πE) for Ω0(s, ∙; π) := VΩ(π(∙∣s)), S ∈ S,
to Eq.(22), we have
rE (S,a) = C' (S, a； πE ) - ω*(ω'(s, •； πE ))
=C' (S, a； πE ) - {hnE(・|s),a0(s,二 πE )iA - ω(πe("S))}
=ω' (s, a； πE ) - Ea，〜∏e (∙∣s)[ω'(s, a'； πE )] +ω(πe (1S))
= t(S, a； πE),
where t(S, a； πE) is our proposed solution in Lemma 1.
G	PROOF OF Lemma 3
RL objective in Regularized MDPs w.r.t. normalized visitation distributions. For a reward
function r ∈ Rs×a and a strongly convex function Ω : ∆A → R, the RL objetive Jω(t, ∏) in Eq.(1)
is equivalent to
arg max J^(r,d∏) := (r,d∏is×A — Ω(d∏),	(23)
π
where for a set D of normalized visitation distributions (Syed et al., 2008)
D :=	d ∈ RS×A : X d(S', a') = (1 - Y)P0(S') +YXP (S'|S, a)d(S, a), ∀S' ∈ S ,
17
Published as a conference paper at ICLR 2021
We define Ω(d):=旧⑶。)〜d[Ω(∏d(∙∣s))] and ∏d(∙∣s) := P 认；([田)∈ ∆A for d ∈ D and use
∏dπ (∙∣s) = ∏(∙∣s) for all S ∈ S. For Ω : D → R, its convex conjugate Ω* is
①(rr = maχ J (r, d)
=max hr, d)s×A - Ω(d)
d∈D
(=i)
max hr, d∏>s×∕ - Ω(d∏)
π∈∆SA
max
π∈∆SA
ɪ2 d∏(s,a)[r(s,a) — Ω(π(a∣s))]
s,a
=(1 - Y) ∙ max JΩ(r,∏),
π∈∆SA
(24)
Where (i) folloWs from using the one-to-one correspondence betWeen policies and visitation distribu-
tions (Syed et al., 2008; Ho & Ermon, 2016). Note that Eq.(24) is equal to the optimal discounted
average return in regularized MDPs.
IRL objective in regularized MDPs w.r.t. normalized visitation distributions. By using the RL
objective in Eq.(23), We can reWrite the IRL objective in Eq.(6) W.r.t. the normalized visitation
distributions as the maximization of the folloWing objective over r ∈ RS×A:
(1 - Y) ∙ 5 JΩ(r,∏E) - max Jn(r,∏)卜
π∈∆SA
=jΩ(r, d∏E ) - max jΩ(r, d)
=m∈in {jΩ(r, d∏E ) - jΩ(r, d)}
=min { (hr, d∏E iS×A - C(dnE )) - (hr, diS×A -。⑷)}
min {Ω(d)
-C(d∏E ) -hr,d - d∏E iS×A
(25)
.
Note that if VΩΩ(d) is well-defined and r = VΩΩ(d∏j) for any strictly convex Ω, Eq.(25) is equal to
min
d∈D
{Ω(d) - Ω(d∏E)
-hVC(dnE), d - d∏Eis×a}
min DS ×a
d∈D ω
(dlld∏E ),
where the equality comes from the definition of Bregman divergence.
Proof of t(s, a; ∏d) = V[Ω(d)](s, a). For simpler notation, we use matrix-vector notation for the
proof when discrete state and action spaces S = {1, ..., |S|} and A = {1, ..., |A|} are considered.
For a normalized visitation distribution d ∈ D, let us define
da := d(s, a), s ∈ S , a ∈ A,
ds := [ds1,...,d|sA|]T ∈ RA,s ∈ S,
^ d1	…d1A|
D := [d1,...,d|S|]T
|S|
d||SA||
π(χ) ：= TT- = P^——E,…,χ∣A∣]T ∈ RA,χ ：= E,…,χ∣A∣]T ∈ RA,
1Ax	a∈A xa
where 1A = [1, ..., 1]T ∈ RA is an |A|-dimensional all-one vector. By using these notations, the
original Ω can be rewritten as
18
Published as a conference paper at ICLR 2021
The gradient of C w.r.t. D (using denominator-layout notation) is
Vd Ω(D)
- ∂ C(D)	∂Ω(D) 1t	s×a
∂d1 ,..∙, ∂d∣S∣	R
where each element of VD C(D) satisfies
T
∂Ω(D)
1 AdsΩ(∏(ds))}
∂ Ω(D)
,...,k
Ω(∏(ds))1 a + 1A ds
∂Ω(∏(ds))
Ω(∏(ds))1 a + 1A ds
∂ds
∂∏(ds) ∂Ω(∏(ds))
∂ds	∂∏(ds).
(26)
for
∂ds
∂(1 AdS )-1
∂∏a(ds)=些(1 τds)-1 + ds ∂(1 AdS)T
此，a%'A J α	此，
=I{a = a0}(1 Ads)-1 - da(1Ads)-2
=I{a = a0}(1 AdS)T- 3)(1 Ads)-1
=(1 AdS)T [I{a = a0}- ∏o(ds)],
and thus,
∂τr(ds)
∂ds
(1Ads)T{lA×A - 1 A^ds)τ}.
(27)
By substituting Eq.(27) into Eq.(26), we have
∂Ω(D)
∂ds
Ω(∏(ds))1 a + IAdS
∂∏(ds) ∂Ω(∏(ds))
∂ds	∂∏ (ds)
Ω(∏(ds))1 A + 1 Ads [(1 Ads)-1 {IA×A - 1 A[π(ds)]τ}]等胃
=Ω(∏(ds))1 A + {Ia×a - 1 A[π(ds)]τ} ^ɪɪndp
C传，s)" * aα(π(dS))	[z ,s^1τa°(π(dS)) /
= Omdy)IA +	∂∏(ds)	-[n(d)]	∂∏(ds) 1A
=Fs) - [∏(ds)]T 吁父 1A + Ω(∏(ds))1 A.
∂π(ds)	C 川	∂π(ds)	A L ” A
If we use the function notation, Eq.(28) can be written as
V[Ω(d)](s, a) = VΩ(∏d(∙∣s))(α) - E。，〜展小)[VΩ(∏d(∙∣s))(α0)] + Ω(∏d(∙∣s))
=t(s, a； ∏d)
(28)
for t of Eq.(7) in Lemma 1.
19
Published as a conference paper at ICLR 2021
H Derivation of Bregman-Divergence-Based Measure in
Continuous Controls
In Eq.(17), the Bregman divergence in the control task is defined as
DA(PI""= L
{ω(p1 (x)) - ω(p2(x)) - ω0(p2(x))(p1(x) - p2(x))} dx.
(29)
Note that We consider Ω(p) = JX ω(p(x))dx = JX [-fφ(p(x))] dx for fφ(x) = xφ(x), which
makes Eq.(29) equal to
I {-P1(x)Φ(P1(x))+ P2(x)Φ(p2(x)) + fφ(p2(x))(p1(x) - P2(x))} dx
X
=	p1(x) fφ0(p2(x)) - φ(p1(x))} dx -	p2(x) fφ0(p2(x)) - φ(p2(x))} dx
XX
=Eχ~pι fφ (P2 (X))- φ(pι (X))] - EX~P2 [fφ (P2 (X))- φ(p2 (X))].
Thus, by considering a learning agent's policy ∏s = ∏(∙∣s), expert policy ∏ = ∏ (∙∣s), and the
objective in Eq.(8) characterized by the Bregman divergence, we can think of the following measure
between expert and agent policies:
Es~d∏ [dA(∏sII∏eS )]
=Es~d∏ [Ea~∏s [fφ(∏E (a)) - φ(∏s(a))] - EaF fΦK ⑷)- Φ(∏E ⑷)〕]∙ G0)
I Tsallis entropy and associated B regman divergence among
multi-variate Gaus sian distributions
Based on the derivation in Nielsen & Nock (2011), we derive the Tsallis entropy and associated
Bremgan divergence as follows. We first consider the distributions in the exponential family
exp (hθ, t(X)i - F(θ) + k(X)) .
(31)
Note that for
Σ-1μ	_
1 ∑TJ = [θ2.
t(X)
X
XXT
F (θ) = - 4 θT θ-1θι + 1log I-∏θ-1l
2 μT 夕-1仙+2 log^mdM1,
k(X) = 0,
we can recover the multi-variate Gaussian distribution (Nielsen & Nock, 2011):
exp(hθ, t(X)i - F(θ) + k(X))
exp (T∑Σ-1x — ^tr(Σ-1xxτ) — gμτΣ-1μ —
∣log(2π)d∣∑P∣
(2∏)d∕2∣∑∣1∕2 exp (μ∑ς-1 X - 2x∑ςTX - 2μ∑g-1μ
(2∏)d∕2∣∑∣1∕2 exp(2(X - μ)∑ς I(X - μ)).
(32)
(33)
(34)
(35)
For two distributions with k(X) = 0,
,	、	,._	,	, .	— , ....... , . O ,	. . 一 ,△一
π(x) = exp({θ, t(X)i — F (θ)), π(x) = exp({θ, t(c)i — F (θ))
20
Published as a conference paper at ICLR 2021
that share t, F, and k, it can be shown that
I (∏,∏;
α,β) =卜…x dx
exp (F(αθ + βθ) - αF(θ) - βF(θ))
since
/…(x)e dx
ZeXP (aF (θ)+Iee 认 XX)TF (θ))dx
exp hαθ
ʌ,
ʌ,
+ βθ, t(x)i — F(αθ + βθ)) exp (F(αθ + βθ)—
αF(θ) — βF(θ)) dx
O
exp(F(αθ + βθ) — αF(θ) — βF(θ)) / exp ((αθ + βθ,t(x)) — F(αθ + βθ)) dx
exp(F(αθ + βθ) — αF(θ) — βF(θ)).
I.1 Tsallis Entropy
For φ(x) = q-ι (1 — Xq-I) and k = 1, the Tsallis entropy of π can be written as
1 — π(x)q-1
Tq(π) ♦ = Ex〜πφ(x) = I π(x)	d	dx
q—1
1 — π (x)q dx
q — 1
=-ɪ- (1 — I (∏,∏; q, 0))
q—1
1 — exp (F (qθ) — qF(θ))
q—1
If π is a multivariate Gaussian distribution, we have
F(qθ) = 2μτ∑-1μ + g log(2n)d|£| — | log qd,
qF (θ) = 2 μτ ∑-1μ + 2log(2∏)d∣∑∣,
F(qθ) — qF(θ) = F lθg(2π)d∣∑∣ — 1 log qd
=(1 — q) { d log 2π +1 log 闵—2d〔log ∖ }.
2	2	2(1 — q)
For Σ = diag{σ12, ..., σd2}, we have
F(qθ) — qF(θ) = (1 — q) { 1 log2π + 2 log 园—2^^ }
=(1	q) ( 2 log 2∏ +2log Y! σ2 - T )
_ ∕ι	∖G ʃ log2∏ , 1 logq I
=(1 — q) 22∖~^ + log σi — 2(1f 卜
21
Published as a conference paper at ICLR 2021
fφ0 (x)
I.2	Tractable form of baseline
For φ(χ) = q-ι(1 - xq-1), We have
—	k-j- (1 - qxq-1)
q-1
—	k-j-(q - qχq-1 - (q - 1))
q-1
-	qk-(1 - XqT) - k
q-1
qφ(x) - k.
Therefore, the baseline can be reWritten as
Eχ~∏ [-fφ(x) + φ(x)] = Eχ~∏ [k - qφ(x) + φ(x)] = (1 - q)Tq(∏) + k.
For a multivariate Gaussian distribution ∏,the tractable form of Eχ~∏ [-fφ(x) + φ(χ)] can be derived
by using that of Tsallis entropy Tq(π) of π.
I.3	B regman divergence with Tsallis entropy regularization
In Eq.(30), We consider the folloWing form of the Bregman divergence:
/ π(x){fφ (π(x))
- φ(π(x))}dx -
/ π(X){fφ (π(x))
—φ(∏(x))}dx.
For φ(x) = q-ι (1 - xq-1), fφ(x) = q-ι (1 - qxq-1) = qφ(x) - k, and k = 1, the above form is
equal to
ZΠ(x) [1 - qn(X)q 11 dx-Tq(∏)-(q - 1)Tq(∏) + 1
q-1
1
q - 1
q
q - 1
—
q--1 / π(x)π(x)q-1dx - Tq(π) - (q - 1)Tq(∏) + 1
—
—q-1 J π(x)π(x)q-1dx - T(∏) - (q - 1)Tq(∏).
For multivariate Gaussians
∏(x) = N(x; μ, ∑), μ = [νι,..., Vd]τ, Σ = diag(σ2,..., σd),
∏(χ) = N (χ; μ, ∑), μ = [^ι,..., Vd]τ, Σ = diag(σ2,..., σd),
We have
J π(x)π(x)q-1dx = I(π, π; 1, q — 1) = exp (F町)—F(θ) — (q — 1)F(θ)),
Where
θ
, Σ-1μ
-2 ∑-1
ʌ
θ
I- ʌ -.
ς-1μ
[-2 ∑-1
. . ʌ
θ0 = θ + (q - 1)θ
Σ-1μ + (q — 1)Σ-1μ
-2 (ς-1 +(q - 1)ς T)
θ10
θ20
θ10
θ20
T
4 + (q - 1) 4 S + (q - 1) 4
σ1	σ1	σd	σd
-2diag{ Jf+ (q -1) ⅛,…，⅛2+ (q -
22
Published as a conference paper at ICLR 2021
and
2μτΣ-14 十
2log(2π)d∣∑∣
1	,、一、
2log(2π)d∣Σ ∣
X ʃ互
d C入2
X｛豪
2=1 ' i
,log2π	1
+——2——+ log σ 卜
,log2π , 1 ʌ 1
+——2——+ log σ 卜
F⑻
ʌ
F⑻
1 ʌ TS—1人 I
2 μ ς μ ÷
. ʌ.
F(θ + (q - 1)θ)
-4(θ1 )τ(θ2)-1θ1+ 2log ∣ - π(θ2)-1∣
X J 1 (σ2 +(q- I)岩)2 + 9 +log________________1______
i=ι I 2 σ2 + (q - 1) ⅛	2	σ2 + (q - 1) ⅛
23
Published as a conference paper at ICLR 2021
J Experiment Setting
J.1 Policy Regularizers in Experiments
Table 1: Policy regularizers φ and their corresponding fφ (Yang et al., 2019).
reg. type.	condition	φ(x)	fφ0 (x)
Shannon	-	- log x	- log x - 1
Tsallis	k > 0,q > 1	q-ι (1 - XqT)	占(1 - qxq-1)
Exp	k ≥ 0, q ≥ 1	q- xkqx	q- xkqx(k + 1 +xlogq)
Cos	0 < θ ≤ π∕2	cos(θx) - cos(θ)	- cos(θ) + cos(θx) - θx sin(θx)
Sin	0 < θ ≤ π∕2	sin(θ) - sin(θx)	sin(θ) - sin(θx) - θx cos(θx)
J.2 Density-based model
By exploiting the knowledge on the reward in Corollary 1
-fφ (WaIS))- EaO 〜π(∙∣s)fφ (WalS))- φ(WalS))],
we consider the density-based model (DBM) which is defined by
rθ(S,a) ≈ -fφ0 (πθ1 (a|S)) + Bθ2 (S)
for θ = (θ1, θ2). Here, fφ0 is a function that can be known priorly, and πθ1 is a neural network which
is defined separately from the policy neural network πψ . The DBM for discrete control problems
are depicted in Figure 6 (Left). The model outputs rewards over all actions in parallel, where
Softmax is used for ng、(∙∣s) and -fφ is elementWisely applied to those Softmax outputs followed
by elementwisely adding Bθ2 (S). For continuous control (Figure 6, Right), we use the network
architecture similar to that in discrete control, where a multivariate Gaussian distribution is used
instead of a softmax layer.
Figure 6: Density-based model for discrete (Left) and continuous control (Right)
J.3 Expert in B ermuda World environment
We assume a stochastic expert defined by
πE (a|S)
Pt3=1(d(t))-1I{a = Proj(θ(t))}
P3=ι(d⑴)-1
θ(t) = arctan2(y(t) — y, x(t) — x), d(t) = ∣∣s(t) - s∣∣2 + e, t = 1, 2, 3,
24
Published as a conference paper at ICLR 2021
for S = (x, y),s(1) = (X(I),y⑴)=(-5,10), S⑵=(X⑵,y(2)) = (0,10), S⑶=(X⑶，y⑶)=
(5, 10), = 10-4 and an operator Proj(θ) : R → A that maps θ to the closest angle in A. In Figure 7,
we depicted the expert policy.
Figure 7: Visualization of the expert policy
J.4 MuJoCo experiment setting
Instead of directly using MuJoCo environments with tanh-squashed policies proposed in Soft Actor-
Critic (SAC) (Haarnoja et al., 2018), we move tanh to a part of the environment—named hyperbolized
environments in short—and assume Gaussian policies. Specifically, after an action a is sampled from
the policies, we pass tanh(a) to the environment. We then consider multi-variate Gaussian policy
πHs) = N (μ(s), ς(S))
With μ(s) = [μι(s),…,μd(s)]T, Σ(s) = diag{(σι(s))2,…,(σq(s))2}, where
一arctanh(0.99) ≤ μi(s) ≤ arctanh(0.99),log(0.01) ≤ logσi(s) ≤ log(2)
for all i = 1, ..., d. Instead of using clipping, we use tanh-activated outputs and scale them to fit in
the above ranges, which empirically improves the performance. Also, instead of using potential-based
reward shaping used in AIRL (Fu et al., 2018), we update the moving mean of intermediate reward
values and update the value network with mean-subtracted rewards—so that the value network gets
approximately mean-zero reward—to stabilize the RL part of RAIRL. Note that this is motivated by
Lemma 2 from which we can guarantee that any constant shift of reward functions does not change
optimality.
25
Published as a conference paper at ICLR 2021
J.5 Hyperparameters
Table 2, Table 3 and Table 4 list the parameters used in our Bandit, Bermuda World, and MuJoCo
experiments, respectively.
Table 2: Hyperparameters for Bandit environments.
Hyper-parameter	Bandit
Batch size	500
Initial exploration steps	10,000
Replay size	500,000
Target update rate (τ)	0.0005
Learning rate	0.0005
λ	5
q (Tsallis entropy Tqk)	2.0
k (Tsallis entropy Tqk)	1.0
Number of trajectories	1,000
Reward learning rate	0.0005
Steps per update	50
Total environment steps	500,000
Table 3: Hyperparameters for Bermuda World environment.
Hyper-parameter	Bermuda World
Batch size	500
Initial exploration steps	10,000
Replay size	500,000
Target update rate (τ)	0.0005
Learning rate	0.0005
q (Tsallis entropy Tqk)	2.0
k (Tsallis entropy Tqk)	1.0
Number of trajectories	1,000
Reward learning rate	0.0005
(For evaluation) λ	1
(For evaluation) Learning rate	0.001
(For evaluation) Target update rate (τ)	0.0005
Steps per update	50
Number of steps	500,000
Table 4: Hyperparameters for MuJoCo environments.
Hyper-parameter	Hopper	Walker2d	HalfCheetah	Ant
Batch size	256	256	256	256
Initial exploration steps	10,000	10,000	10,000	10,000
Replay size	1,000,000	1,000,000	1,000,000	1,000,000
Target update rate (τ)	0.005	0.005	0.005	0.005
Learning rate	0.001	0.001	0.001	0.001
λ	0.0001	0.000001	0.0001	0.000001
k (Tsallis entropy Tqk)	1.0	1.0	1.0	1.0
Number of trajectories	100	100	100	100
Reward learning rate	0.001	0.001	0.001	0.001
Steps per update	1	1	1	1
Number of steps	1,000,000	1,000,000	1,000,000	2,000,000
26