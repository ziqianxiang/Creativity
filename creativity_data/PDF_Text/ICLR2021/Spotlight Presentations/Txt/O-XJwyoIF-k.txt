Published as a conference paper at ICLR 2021
Minimum Width for Universal Approximation
Sejun Parkt Chulhee Yun^ Jaeho Leew JinWoo Shint *
tKAIST AI RrTEECS *KAISTEE
Ab stract
The universal approximation property of width-bounded networks has been studied
as a dual of classical universal approximation results on depth-bounded networks.
However, the critical width enabling the universal approximation has not been
exactly characterized in terms of the input dimension dx and the output dimension
dy . In this work, we provide the first definitive result in this direction for networks
using the ReLU activation functions: The minimum width required for the univer-
sal approximation of the Lp functions is exactly max{dx + 1, dy}. We also prove
that the same conclusion does not hold for the uniform approximation with ReLU,
but does hold with an additional threshold activation function. Our proof technique
can be also used to derive a tighter upper bound on the minimum width required
for the universal approximation using networks with general activation functions.
1	Introduction
The study of the expressive power of neural networks investigates what class of functions neural
networks can/cannot represent or approximate. Classical results in this field are mostly focused
on shallow neural networks. An example of such results is the universal approximation theorem
(Cybenko, 1989; Hornik et al., 1989; Pinkus, 1999), which shows that a neural network with fixed
depth and arbitrary width can approximate any continuous function on a compact set, up to arbitrary
accuracy, if the activation function is continuous and nonpolynomial. Another line of research studies
the memory capacity of neural networks (Baum, 1988; Huang and Babri, 1998; Huang, 2003), trying
to characterize the maximum number of data points that a given neural network can memorize.
After the advent of deep learning, researchers started to investigate the benefit of depth in the
expressive power of neural networks, in an attempt to understand the success of deep neural networks.
This has led to interesting results showing the existence of functions that require the network to be
extremely wide for shallow networks to approximate, while being easily approximated by deep and
narrow networks (Telgarsky, 2016; Eldan and Shamir, 2016; Lin et al., 2017; Poggio et al., 2017). A
similar trade-off between depth and width in expressive power is also observed in the study of the
memory capacity of neural networks (Yun et al., 2019; Vershynin, 2020).
In search of a deeper understanding of the depth in neural networks, a dual scenario of the classical
universal approximation theorem has also been studied (Lu et al., 2017; Hanin and Sellke, 2017;
Johnson, 2019; Kidger and Lyons, 2020). Instead of bounded depth and arbitrary width studied in
classical results, the dual problem studies whether universal approximation is possible with a network
of bounded width and arbitrary depth. A very interesting characteristic of this setting is that there
exists a critical threshold on the width that allows a neural network to be a universal approximator. For
example, one of the first results (Lu et al., 2017) in the literature shows that universal approximation
of L1 functions from Rdx to R is possible for a width-(dx + 4) RELU network, but impossible for a
width-dx RELU network. This implies that the minimum width required for universal approximation
lies between dx + 1 and dx + 4. Subsequent results have shown upper/lower bounds on the minimum
width, but none of the results has succeeded in a tight characterization of the minimum width.
1.1	What is known so far?
Before summarizing existing results, we first define function classes studied in the literature. For
a domain X ⊆ Rdx and a codomain Y ⊆ Rdy, we define C(X, Y) to be the class of continuous
* emails: Sejun.park@kaist.ac.kr, Chulheey @mit.edu,jaeho-lee@kaist.ac.kr,jinwoos@kaist.ac.kr
1
Published as a conference paper at ICLR 2021
Table 1: A summary of known upper/lower bounds on minimum width for universal approximation.
In the table, K ⊂ Rdx denotes a compact domain, and p ∈ [1, ∞). “Conti.” is short for continuous.
Reference	Function class	Activation P	Upper/lower bounds
LU et al. (2017)	-LI(Rdx, R) L1(K, R)	RELU RELU	dx + 1 ≤ Wmin ≤ dx + 4 	Wmin ≥ dx	
Hanin and Sellke (2017)	-C (K, Rdy)^^	RELU	dx + 1 ≤ Wmin ≤ dx + dy
Johnson (2019)	C(K, R)一	UnifOrmIy COntij	Wmin ≥ dx + 1
Kidger and Lyons (2020)	-C (K, Rdy)^^ C (κ, Rdy) Lp(Rdx, Rdy )	^^conti. nonpoly' nonaffine poly RELU	Wmin ≤ dx + dy + 1 Wmin ≤ dx + dy + 2 	Wmin ≤ dx + dy + 1	
Ours (Theorem 1) Ours (Theorem 2) Ours (Theorem 3) Ours (Theorem 4)	Lp(Rdx, Rdy) C ([0,1], R2) C (K, Rdy) Lp(K, Rdy)	RELU RELU ReLU+Step conti. nonpoly^	Wmin = maχ{dx + 1, dy} Wmin = 3 > maχ{dx + 1, dy} Wmin = maχ{dx + 1, dy} Wmin ≤ maχ{dx + 2, dy + 1}
t requires that P is uniformly approximated by a sequence of one-to-one functions.
^ requires that P is continuously differentiable at some Z with ρ0(z) = 0.
functions from X to Y, endowed with the uniform norm: kfk∞ := supx∈X kf (x)k∞. For p ∈
[1, ∞), we also define Lp(X , Y) to be the class of Lp functions from X to Y, endowed with the
Lp-norm: kfkp := ( X kf (x)kpp dx)1/p. The summary of known upper and lower bounds in the
literature, as well as our own results, is presented in Table 1. We use wmin to denote the minimum
width for universal approximation.
First progress. As aforementioned, Lu et al. (2017) show that universal approximation of
L1(Rdx , R) is possible for a width-(dx + 4) RELU network, but impossible for a width-dx RELU
network. These results translate into bounds on the minimum width: dx + 1 ≤ wmin ≤ dx + 4. Hanin
and Sellke (2017) consider approximation of C(K, Rdy), where K ⊂ Rdx is compact. They prove
that RELU networks of width dx + dy are dense in C(K, Rdy), while width-dx RELU networks are
not. Although this result fully characterizes wmin in case of dy = 1, it fails to do so for dy > 1.
General activations. Later, extensions to activation functions other than RELU have appeared
in the literature. Johnson (2019) shows that if the activation function P is uniformly continuous
and can be uniformly approximated by a sequence of one-to-one functions, a width-dx network
cannot universally approximate C(K, R). Kidger and Lyons (2020) show that if P is continuous,
nonpolynomial, and continuously differentiable at some z with P0(z) 6= 0, then networks of width
dx + dy + 1 with activation P are dense in C(K, Rdy). Furthermore, Kidger and Lyons (2020) prove
that RELU networks of width dx + dy + 1 are dense in Lp (Rdx , Rdy ).
Limitations of prior arts. Note that none of the existing works succeeds in closing the gap between
the upper bound (at least dx + dy) and the lower bound (at most dx + 1). This gap is significant
especially for applications with high-dimensional codomains (i.e., large dy), arising for many practical
applications of neural networks, e.g., image generation (Kingma and Welling, 2013; Goodfellow
et al., 2014), language modeling (Devlin et al., 2019; Liu et al., 2019), and molecule generation
(Gbmez-Bombarelli et al., 2018; Jin et al., 2018). In the prior arts, the main bottleneck for proving an
upper bound below dx + dy is that they maintain all dx neurons to store the input and all dy neurons
to construct the function output; this means every layer already requires at least dx + dy neurons. In
addition, the proof techniques for the lower bounds only consider the input dimension dx regardless
of the output dimension dy .
1.2	Summary of results
We mainly focus on characterizing the minimum width of ReLU networks for universal approxi-
mation. Nevertheless, our results are not restricted to ReLU networks; they can be generalized to
networks with general activation functions. Our contributions can be summarized as follows.
2
Published as a conference paper at ICLR 2021
•	Theorem 1 states that the minimum width for RELU networks to be dense in Lp(Rdx , Rdy) is
exactly max{dx + 1, dy}. This is the first result fully characterizing the minimum width of
ReLU networks for universal approximation. In particular, the upper bound on the minimum
width is significantly smaller than the best known result dx + dy + 1 (Kidger and Lyons, 2020).
•	Given the full characterization of wmin of RELU networks for approximating Lp(Rdx , Rdy),
a natural question arises: Is wmin also the same for C(K, Rdy)? We prove that it is not the case;
Theorem 2 shows that the minimum width for RELU networks to be dense in C([0, 1], R2) is 3.
Namely, RELU networks of width max{dx + 1, dy} are not dense in C(K, Rdy) in general.
•	In light of Theorem 2, is it impossible to approximate C(K, Rdy) in general while maintaining
width max{dx + 1, dy}? Theorem 3 shows that an additional activation comes to rescue. We
show that if networks use both RELU and threshold activation functions (which we refer to as
STEP)1 2, they can universally approximate C(K, Rdy) with the minimum width max{dx + 1, dy}.
•	Our proof techniques for tight upper bounds are not restricted to ReLU networks. In Theorem 4,
we extend our results to general activation functions covered in Kidger and Lyons (2020).
1.3	Organization
We first define necessary notation in Section 2. In Section 3, we formally state our main results and
discuss their implications. In Section 4, we present our “coding scheme” for proving upper bounds
on the minimum width in Theorems 1, 3 and 4. In Section 5, we prove the lower bound in Theorem 2
by explicitly constructing a counterexample. Finally, we conclude the paper in Section 6. We note
that all formal proofs of Theorems 1-4 are presented in Appendix.
2	Problem setup and notation
Throughout this paper, we consider fully-connected neural networks that can be described as an
alternating composition of affine transformations and activation functions. Formally, we consider
the following setup: Given a set of activation functions Σ, an L-layer neural network f of input
dimension dx, output dimension dy, and hidden layer dimensions d1, . . . , dL-12 is represented as
f ：= tL ◦ σL-i ◦•••◦ t2 ◦ σι ◦ tι,	(1)
where t` : Rd'-1 → Rd' is an affine transformation and σg is a vector of activation functions:
σ'(xι
,...,xd')=(ρι(xι),...,Pd'(xd')),
where ρi ∈ Σ. While we mostly consider the cases where Σ is a singleton (e.g., Σ = {RELU}), we
also consider the case where Σ contains both RELU and STEP activation functions as in Theorem 3.
We denote a neural network with Σ = {ρ} by a “p network” and a neural network with Σ = {ρ1,ρ2}
by a “pi +p2 network.” We define the width W of f as the maximum over di,..., dn We use "p
networks (or p1 + p2 networks) of width w” for denoting the collection of all p networks (or p1 + p2
networks) of width w having a finite number of layers.
For describing the universal approximation of neural networks, we say p networks (orpi+p2 networks)
of width w are dense in C(X, Y) if for any f * ∈ C(X, Y) and ε > 0, there exists a p network (or
a pi +p2 network) f of width w such that kf* - f k∞ ≤ ε. Likewise, we say p networks (or pi +p2
networks) are dense in Lp(X, Y) if for any f* ∈ Lp(X, Y) and ε > 0, there exists a p network (or a
pi +p2 network) f such that kf * - f kp ≤ ε.
3	Minimum width for universal approximation
Lp approximation with RELU. We present our main theorems in this section. First, for universal
approximation of Lp(Rdx , Rdy ) using RELU networks, we give the following theorem.
Theorem 1.	For any p ∈ [1, ∞), RELU networks of width w are dense in Lp (Rdx , Rdy) if and only
if w ≥ max{dx + 1, dy}.
1The threshold activation function (i.e., STEP) denotes x 7→ 1[x ≥ 0].
2For simplicity of notation, we let d0 := dx and dL := dy.
3
Published as a conference paper at ICLR 2021
This theorem shows that the minimum width wmin for universal approximation is exactly equal to
max{dx + 1, dy}. In order to provide a tight characterization of wmin, we show three new upper
and lower bounds: wmin ≤ max{dx + 1, dy} through a construction utilizing a coding approach,
wmin ≥ dy through a volumetric argument, and wmin ≥ dx + 1 through an extension of the same
lower bound for L1(Rdx , Rdy) (Lu et al., 2017). Combining these bounds gives the tight minimum
width wmin = max{dx + 1, dy}.
Notably, using our new proof technique, we overcome the limitation of existing upper bounds that
require width at least dx + dy . Our construction first encodes the dx dimensional input vectors into
one-dimensional codewords, and maps the codewords to target codewords using memorization, and
decodes the target codewords to dy dimensional output vectors. Since we construct the map from
input to target using scalar codewords, we bypass the need to use dx + dy hidden nodes. More details
are found in Section 4. Proofs of the lower bounds are deferred to Appendix B.
Uniform approximation with ReLU. In Theorem 1, we have seen the tight characterization
wmin = max{dx + 1, dy} for Lp(Rdx , Rdy) functions. Does the same hold for C(K, Rdy), for
a compact K ⊂ Rdx ? Surprisingly, we show that the same conclusion does not hold in general.
Indeed, we show the following result, proving that width max{dx + 1, dy} is provably insufficient
for dx = 1, dy = 2.
Theorem 2.	RELU networks of width w are dense in C ([0, 1], R2) if and only if w ≥ 3.
Theorem 2 translates to wmin = 3, and the upper bound wmin ≤ 3 = dx + dy is given by Hanin and
Sellke (2017). The key is to prove a lower bound wmin ≥ 3, i.e., width 2 is not sufficient. Recall from
Section 1.1 that all the known lower bounds are limited to showing that width dx is insufficient for
universal approximation. A closer look at their proof techniques reveals that they heavily rely on the
fact that the hidden layers have the same dimensions as the input space. As long as the width w > dx,
their arguments break because such a network maps the input space into a higher-dimensional space.
Although only for dx = 1 and dy = 2, we overcome this limitation of the prior arts and show that
width w = 2 > dx is insufficient for universal approximation, by providing a counterexample. We
use a novel topological argument which comes from a careful observation on the image created by
ReLU operations. In particular, we utilize the property of ReLU that it projects all negative inputs
to zero, without modifying any positive inputs. We believe that our proof will be of interest to readers
and inspire follow-up works. Please see Section 5 for more details.
Theorem 1 and Theorem 2 together imply that for RELU networks, approximating C(K, Rdy)
requires more width than approximating Lp(Rdx , Rdy). Interestingly, this is in stark contrast with
existing results, where the minimum depth of RELU networks for approximating C(K, Rdy ) is two
(Leshno et al., 1993) but it is greater than two for approximating Lp(Rdx , Rdy) (Wang and Qu, 2019).
Uniform approximation with RELU+STEP. While width max{dx + 1, dy} is insufficient for
RELU networks to be dense in C(K, Rdy), an additional STEP activation function helps achieve the
minimum width max{dx + 1, dy}, as stated in the theorem below.
Theorem 3.	RELU+STEP networks of width w are dense in C(K, Rdy) if and only if w ≥ max{dx +
1,dy}.
Theorem 2 and Theorem 3 indicate that the minimum width for universal approximation is indeed
dependent on the choice of activation functions. This is also in contrast to the classical results where
RELU networks of depth 2 are universal approximators (Leshno et al., 1993), i.e., the minimum
depths for universal approximation are identical for both ReLU networks and ReLU+Step networks.
Theorem 3 comes from a similar proof technique as Theorem 1. Due to its discontinuous nature, the
Step activation can be used in our encoder to quantize the input without introducing uniform norm
errors. Lower bounds on wmin can be proved in a similar way as Theorem 1.
General activations. Our proof technique for upper bounds in Theorems 1 and 3 can be easily
extended to networks using general activations. Indeed, we prove the following theorem, which
shows that adding a width of 1 is enough to cover the networks with general activations.
Theorem 4.	Let ρ : R → R be any continuous nonpolynomial function which is continuously
differentiable at some z with ρ0(z) 6= 0. Then, ρ networks of width w are dense in Lp(K, Rdy) for all
p ∈ [1, ∞) if w ≥ max{dx + 2, dy + 1}.
4
Published as a conference paper at ICLR 2021
x ∈ [0, l]ʤ (0.100 ∙ ∙∙ 5 0.010 ∙ ∙ ∙,		encodeκ(∕) 0.100010∙∙∙011	encodeM oʃ* o qκ(x) 	 0.001000∙∙∙100				qM0Γ o<iκ(χ) (o.ooι, 0.000, 一:
	encode j<-		memorize j<jjvf		decode ʌʃ	
0.011 …)						0.100)
Figure 1: Illustration of the coding scheme
Please notice that unlike other theorems, Theorem 4 only proves an upper bound wmin ≤ max{dx +
2, dy + 1}. We note that Theorem 4 significantly improves over the previous upper bound of width
dx + dy + 1 by (Kidger and Lyons, 2020, Remark 4.10).
4	Tight upper bound on minimum width
In this section, we present the main idea for constructing networks achieving the minimum width for
universal approximation, and then sketch the proofs of upper bounds in Theorems 1, 3, and 4.
4.1	Coding scheme for universal approximation
We now illustrate the main idea underlying the construction of neural networks that achieve the
minimum width. To this end, We consider an approximation of a target continuous function f * ∈
C([0, 1]dx , [0, 1]dy); however, our main idea can be easily generalized to other domain, codomain,
and Lp functions. Our construction can be viewed as a coding scheme in essence, consisting of
three parts: encoder, memorizer, and decoder. First, the encoder encodes an input vector x to a
one-dimensional codeword. Then, the memorizer maps the codeword to a one-dimensional target
codeword that is encoded with respect to the corresponding target f*(x). Finally, the decoder maps
the target codeword to a target vector which is sufficiently close to f*(x). Note that one can view the
encoder, memorizer, and decoder as functions mapping from dx-dimension to 1-dimension, then to
1-dimension, and finally to dy-dimension.
The spirit of the coding scheme is that the three functions can be constructed using the idea of the
prior results such as (Hanin and Sellke, 2017). Recall that Hanin and Sellke (2017) approximate any
continuous function mapping n-dimensional inputs to m-dimensional outputs using RELU networks
of width n + m. Under this intuition, we construct the encoder, the memorizer, and the decoder by
RELU+STEP networks (or RELU networks) of width dx + 1, 2, dy, respectively; these constructions
result in the tight upper bound max{dx + 1, dy}. Here, the decoder requires width dy instead of
dy + 1, as we only construct the first dy - 1 coordinates of the output, and recover the last output
coordinate from a linear combination of the target codeword and the first dy - 1 coordinates. Note
that we construct the exact encoder, decoder, and memorizer as prior results only approximate them.
Next, we describe the operation of each part. We explain their neural network constructions in
subsequent subsections.
Encoder. Before introducing the encoder, we first define a quantization function qn : [0, 1] → Cn
forn ∈ NandCn := {0, 2-n, 2 × 2-n, . . . , 1 - 2-n} as
qn(x) := max{c ∈ Cn : c ≤ x}.
In other words, given any x ∈ [0, 1), qn(x) preserves the first n bits in the binary representation of
x and discards the rest; x = 1 is mapped to 1 - 2-n. Note that the error from the quantization is
always less than or equal to 2-n.
The encoder encodes each input x ∈ [0, 1]dx to some scalar value via the function encodeK : Rdx →
Cdx K for some K ∈ N defined as
encodeK (x) := X x qK(xi) × 2-(i-1)K.
i=1
In other words, encodeK (x) quantizes each coordinate of x by a K-bit binary representation
and concatenates the quantized coordinates into a single scalar value having a (dxK)-bit binary
5
Published as a conference paper at ICLR 2021
representation. Note that if one “decodes" a codeword encodeκ (x) back to a vector X as3
{X} := (encodeK1 ◦ encodeκ(x)) ∩ CK,
then ∣∣x - X∣∣∞ ≤ 2-K. Namely, the “information loss" incurred by the encoding can be made
arbitrarily small by choosing large K.
Memorizer. The memorizer maps each codeword encodeK (x) ∈ CdxK to its target codeword via
the function memorizeK,M : CdxK → CdyM for some M ∈ N, defined as
memorizeK,M encodeK (x) := enCodeM (f * ◦ qκ(x))
where qK is applied coordinate-wise for a vector. We note that memorizeK,M is well-defined as each
encodeK (x) ∈ CdxK corresponds to a unique qK (x) ∈ CKdx. Here, one can observe that the target of
the memorizer contains the information of the target value since encodeM f* ◦ qK (x) contains
information of f* at a quantized version of x, and the information loss due to quantization can be
made arbitrarily small by choosing large enough K and M .
Decoder. The decoder decodes each codeword generated by the memorizer using the function
decodeM : CdyM → CMdy defined as
decodeM(C) := X where {X} := encode-M~(c) ∩ CMy.
Combining encode, memorize, and decode completes our coding scheme for approximating f*.
One can observe that our coding scheme is equivalent to qM ◦ f* ◦ qK which can approximate the
target function f* within any ε > 0 error, i.e.,
supx∈[0,1]dx ∣f* (X) - decodeM ◦ memorizeK,M ◦ encodeK (X)∣∞ ≤ ε
by choosing large enough K, M ∈ N so that ωf*(2-K) + 2-M ≤ ε.4
In the remainder of this section, we discuss how each part of the coding scheme can be implemented
with a neural network using ReLU+Step activations (Section 4.2), ReLU activation (Section 4.3),
and other general activations (Section 4.4).
4.2	Tight upper bound on minimum width of ReLU+Step networks (Theorem 3)
In this section, we discuss how we explicitly construct our coding scheme to approximate functions in
C(K, Rdy) using a width-(max{dx + 1, dy}) RELU+STEP network. This results in the tight upper
bound in Theorem 3.
First, the encoder consists of quantization functions qK and a linear transformation. However, as qK
is discontinuous and cannot be uniformly approximated by any continuous function, we utilize the
discontinuous Step activation to exactly construct the encoder via a ReLU+Step network of width
dx + 1. On the other hand, the memorizer and the decoder maps a finite number of scalar values (i.e.,
CdxK and CdyM, respectively) to their target values/vectors. Such maps can be easily implemented
by piecewise linear functions; hence, they can be exactly constructed by RELU networks of width 2
and dy, respectively, as discussed in Section 4.1. Note that STEP is used only for constructing the
encoder.
In summary, all parts of our coding scheme can be exactly constructed by RELU+STEP networks
of width dx + 1, 2, and dy. Thus, the overall RELU+STEP network has width max{dx + 1, dy}.
Furthermore, it can approximate the target continuous function f * within arbitrary uniform error by
choosing sufficiently large K and M .
4.3	Tight upper bound on minimum width of ReLU networks (Theorem 1)
The construction of width-(max{dx + 1, dy}) RELU network for approximating Lp (Rdx, Rdy) (i.e.,
the tight upper bound in Theorem 1) is almost identical to the ReLU+Step network construction
3Here, encode-K1 denotes the preimage of encodeK and CKdx is the Cartesian product of dx copies of CK .
4ωf* denotes the modulus of continuity of f *: kf *(x) — f *(x0)k∞ ≤ ωf*(∣∣x — x0k∞) ∀x,x0 ∈ [0,1]dx.
6
Published as a conference paper at ICLR 2021
in Section 4.2. Since any Lp function can be approximated by a continuous function with compact
support, We aim to approximate continuous f * : [0,1]dx → [0,1]dy here as in our coding scheme.
Since the memorizer and the decoder can be exactly constructed by ReLU networks, we only discuss
the encoder here. As We discussed in the last section, the encoder cannot be uniformly approximated
by continuous functions (i.e., ReLU netWorks). Nevertheless, it can be implemented by continuous
functions except for a subset of the domain around the discontinuities, and this subset can be made
arbitrarily small in terms of the Lebesgue measure. That is, We construct the encoder using a ReLU
netWork of Width dx + 1 for [0, 1]dx except for a small subset, Which enables us to approximate the
encoder in the Lp -norm. Combining With the memorizer and the decoder, We obtain a RELU netWork
of Width max{dx + 1, dy} that approximates the target function f* in the Lp-norm.
4.4	Tightening upper bound on minimum width of general networks (Theorem 4)
Our netWork construction can be generalized to general activation functions using existing results
on approximation of C(K, Rdy ) functions. For example, Kidger and Lyons (2020) shoW that if the
activation ρ is continuous, nonpolynomial, and continuously differentiable at some z With ρ0 (z) 6= 0,
then ρ netWorks of Width dx + dy + 1 are dense in C(K, Rdy ). Applying this result to our encoder,
memorizer, and decoder constructions of RELU netWorks, it folloWs that if ρ satisfies the conditions
above, then ρ netWorks of Width max{dx +2, dy + 1} are dense in Lp(K, Rdy ), i.e., Theorem 4 holds.
We note that any universal approximation result for C(K, Rdy ) by netWorks using other activation
functions, other than Kidger and Lyons (2020), can also be combined With our construction.
4.5	Number of layers for minimum width universal approximators
Lastly, We discuss the required number of layers for our netWork constructions achieving the tight
minimum Width in Theorem 1 and 3. First, our RELU + STEP netWork construction of Width
max{dx + 1, dy} for approximating f* ∈ C([0,1]dx,Rdy) requires O(2dx∙K + 2M) layers. Here,
O(2K) layers are for the encoder, O(2dx∙K) layers are for the memorizer, and O(2M) layers for the
decoder.5 Since K, M must satisfy ω∕*(2-K )+2-M ≤ ε for approximating f * in ε error (see Section
4.1), we choose K = d— log2(ω-*1 (ε∕2))],M = d— log2(ε∕2)e so that ωf*(2-K), 2-M ≤ ε∕2.
Namely, the overall required number of layers (i.e., parameters) for approximating f* in ε error is
O((ω-*1(ε∕2))-dχ + 1∕ε).
This number is closely related to the number of parameters for approximating arbitrary f * ∈
C ([0, 1]dx , Rdy ) in ε error: For RELU networks of a constantly bounded number of layers,
θ((ω-*1(Ω(ε)))-dx) parameters are necessary and sufficient (Yarotsky, 2018). On the other
hand, for ReLU networks whose number of layers can increase with the number of parameters,
O ((ω-*1(Ω(ε)))-dx/2) parameters are sufficient (Yarotsky, 2018). However, while the number of lay-
ers grows with the number of parameters in our construction, it requires O((ω-*1(Ω(ε)))-dx +1∕ε)
parameters. Namely, the number of parameters in constructions achieving the tight minimum width
can be improved.
Likewise, for proving the tight upper bound in Theorem 1, for approximating f* ∈ Lp(Rdx , Rdy ) in
ε error, our RELU network construction also requires O((ω-1(Ω(ε)))-dx + 1∕ε) layers where f is
some continuous function satisfying kf* — f kp = ε∕2.
5	Tight lower bound on minimum width
The purpose of this section is to prove the tight lower bound in Theorem 2, i.e., there exist f* ∈
C([0, 1],R2) and ε > 0 satisfying the following property: For any width-2 RELU network f, we
have kf * — f k∞ > ε. Our construction of f * is based on topological properties of RELU networks,
which we study in Section 5.1. Then, we introduce a counterexample f* and prove that f* cannot be
approximated by width-2 RELU networks in Section 5.2.
5dx , dy are considered as constants.
7
Published as a conference paper at ICLR 2021
5.1	Topological properties of ReLU networks
We first interpret a width-2 RELU network f : R → R2 as below, following (1):
f := tL ◦ σ ◦•••◦ σ ◦ t2 ◦ σ ◦ tι
where L ∈ N denotes the number of layers, t1 : R → R2 and t` : R2 → R2 for ` > 1 are affine
transformations, and σ is the coordinate-wise RELU. Without loss of generality, we assume that
t` is invertible for all ` > 1, as invertible affine transformations are dense in the space of affine
transformations on bounded support, endowed with the uniform norm. To illustrate the topological
properties of f better, we reformulate f as follows:
f = (Φ--ι ◦ σ ◦ Φl-i) ◦•・・◦ (φ-1 ◦ σ ◦ φ2) ◦ (φ-1 ◦ σ ◦ φι) ◦ tt	(2)
where φ' and tt are defined as tt :=/工◦•••◦ tι and φ' := (tL ◦•••◦ t'+1)-1, i.e., t` = φ' ◦ φ-1
for ` ≥ 2 and t1 = φ1 ◦ tt . Under the reformulation (2), f first maps inputs through an affine
transformation tt, then it sequentially applies φ-1 ◦ σ ◦ φ'. Here, φ-1 ◦ σ ◦ φ' can be viewed as
changing the coordinate system using φ', applying RELU in the modified coordinate system, and
then returning back to the original coordinate system via φ-1. Under this reformulation, we present
the following lemmas, whose proofs are presented in Appendices B.4-B.5.
Lemma 5. Let φ : R2 → R2 be an invertible affine transformation. Then, there exist a1 , a2 ∈ R2
and b1, b2 ∈ R determined by φ such that the following statements hold for S := {x : ha1, xi + b1 ≥
0, ha2, xi + b2 ≥ 0} and x0 := φ-1 ◦ σ ◦ φ(x).
•	If x ∈ S, then x0 = x.
•	If x ∈ R2 \ S, then x0 6= x and x0 ∈ ∂S.6
Lemma 6. Let φ : R2 → R2 be an invertible affine transformation. Suppose that x is in a bounded
path-component7 of R2 \ T for some T ⊂ R2. Then, the following statements hold for x0 :=
φ-1 ◦ σ ◦ φ(x) andT0 := φ-1 ◦ σ ◦ φ(T).
•	If x0 = x and x0 ∈/ T0, then x0 is in a bounded path-component of R2 \ T0.
•	If x0 6= x, then x0 ∈ T0.
Lemma 5 follows from the fact that output of ReLU is identity to nonnegative coordinates, and is
zero to negative coordinates. In particular, a1, b1 and a2, b2 in Lemma 5 correspond to the axes of
the “modified” coordinate system before applying σ. Under the same property of RELU, Lemma 6
states that if a point x is surrounded by a set T, after applying φ-1 ◦ σ ◦ φ, either the point stays at
the same position and surrounded by the image of T or intersects with the image of T. Based on
these observations, we are now ready to introduce our counterexample.
5.2	Counterexample
Our counterexample f * : [0,1] → R2 is illustrated in Figure 2(a) where f *([0,pι]) is drawn in red
from (4,3) to (0,0), f *((p1,p2)) is drawn in black from (0,0) to (-1,0), and f *([p2,1]) is drawn
in blue from (-1,0) to (l, 0), for some 0 < pi <p < 1, e.g., pi = 3,p2 = 3. In this section, we
suppose for contradiction that there exists a ReLU network f of width 2 such that kf * - f k∞ ≤ 忐.
To this end, consider the mapping by the first ` layers of f :
g` := (φ-1 ◦ σ ◦ φ') ◦•••◦ (φ-i ◦ σ ◦ φι) ◦ tt.
Our proof is based on the fact: If g'(x) = g`(χ0), then f (x) = f (x0). Thus, the following must hold:
if kf * - fk∞ ≤ 忐,then g`([0,pι]) ∩ g`([p2,1]) = 0 for all' ≥ 1.	(3)
Let B := (-2, 2) × (-1, 1) (the gray box in Figure 2(b)) and `* ∈ N be the largest ` such that
φ-i ◦ σ ◦ φ'(B) = B. This means that after the '*-th layer, everything inside the box B never gets
6 ∂S denotes the boundary set of S.
7S ⊂ T is a path-component of T if S is a maximal set satisfying the following condition: For any
x1, x2 ∈ S, there exists a continuous function f : [0, 1] → S such that f(0) = x1 and f(1) = x2.
8
Published as a conference paper at ICLR 2021
rcι = -5-4-3-2-l 0 1 2 3 4 5
(a)
(b)
(c)
Figure 2: (a) Illustration of the image of f * : [0,1] → R2 (b, C) Examples of g`* ([0,1]).
affected by RELU operations. In other words, f ([0, 1]) ∩ B must have been constructed in the first
'* layers. Under this observation, the boundary of S in Lemma 5 determined by φ'* must intersect
with B: If B ⊂ S, then φ-*1 ◦ σ ◦ φ'(B) = B which contradicts the definition of '* and if B∩S = 0,
then f ([0, 1]) ∩ B cannot be constructed in the first `* layers. Therefore, there must exist a line (e.g.,
a line containing one boundary of S, see the arrow in Figure 2(b)) intersecting with B, such that
the image g`* ([0, 1]) lies in one side of the line. Since the image of the entire network f([0,p1]) is
on both sides of the line, we have g`* ([0, p1]) 6= f ([0, p1]), which implies that the remaining layers
`* + 1, . . . , L - 1 must have moved the image g`* ([0,p1]) \ B to f ([0, p1]) \ B; this also implies
g`* ([0,pι]) \B = 0. A similar argument gives g`* ([p2,1]) \B = 0.
Since f ([0, 1]) ∩ B must have been constructed in the first `* layers, as illustrated in Figures 2(b)
and 2(c), the boundary ∂B intersects with g`* ([p2, 1]) (the blue line) near points (-1, 1) and (1, 1).
Hence, T := g`* ([p2, 1]) ∪ B forms a “closed loop.” Also, ∂B intersects with g`* ([0,p1]) near the
point (0, 1), so there must exist a point in g`* ([0,p1]) \ B that is “surrounded” by T. Given these
observations, we have the following lemma. The proof of Lemma 7 is presented in Appendix B.6.
Lemma 7. The image g`* ([0,p1]) \ B is contained in a bounded path-component ofR2 \ T unless
g'* ([0,P1]) ∩ g'* ([P2, 1]) = 0.
Figures 2(b) and 2(c) illustrates the two possible cases of Lemma 7. If g`* ([0,p1]) ∩ g`* ([p2, 1]) 6= 0
(Figure 2(c)), this contradicts (3). Then, g`* ([0,P1]) \ B must be contained in a bounded path-
component of R2 \ T. Recall that g`* ([0,P1]) \ B has to move to f([0,P1]) \ B by layers `* +
1, . . . , L - 1. However, by Lemma 6, if any point in g`* ([0,P1]) \ B moves, then it must intersect
with the image of T. If it intersects with the image of g`* ([P2 , 1]), then (3) is violated, hence a
contradiction. If it intersects with B at the /-th layer for some ' > '*, then B ⊂ S for S in Lemma 5
determined by 'L Hence, it violates the definition of '* as φ-1 ◦ σ ◦ φe↑ (B) = B by Lemma 5.
Namely, the approximation by f is impossible in any cases. This completes the proof of Theorem 2.
6 Conclusion
The universal approximation property of width-bounded networks is one of the fundamental problems
in the expressive power theory of deep learning. Prior arts attempt to characterize the minimum
width sufficient for universal approximation; however, they only provide upper and lower bounds
with large gaps. In this work, we provide the first exact characterization of the minimum width of
ReLU networks and ReLU+Step networks. In addition, we observe interesting dependence of the
minimum width on the target function classes and activation functions, in contrast to the minimum
depth of classical results. We believe that our results and analyses would contribute to a better
understanding of the performance of modern deep and narrow network architectures.
Acknowledgements
This work was supported by Institute of Information & Communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial
Intelligence Graduate School Program (KAIST) and No.2017-0-01779, A machine learning and
statistical inference framework for explainable artificial intelligence). Chulhee Yun acknowledges
financial supports from Korea Foundation for Advanced Studies, NSF CAREER grant 1846088, and
ONR grant N00014-20-1-2394.
9
Published as a conference paper at ICLR 2021
References
Eric B. Baum. On the capabilities of multilayer perceptrons. Journal OfComplexity, 4(3):193-215,
1988. ISSN 0885-064X.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, 1989.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on Learning Theory, 2016.
Rafael Gdmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose MigUel Herndndez-Lobato,
Benjamin SdnChez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Aldn Aspuru-Guzik. Automatic chemical design using a data-driven continuous
representation of molecules. ACS Central Science, 4(2):268-276, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, 2014.
Richard Hacker. Certification of algorithm 112: position of point relative to polygon. Communications
of the ACM, 5(12):606, 1962.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359-366, 1989.
Guang-Bin Huang. Learning capability and storage capacity of two-hidden-layer feedforward
networks. IEEE Transactions on Neural Networks, 14(2):274-281, 2003.
Guang-Bin Huang and Haroon A Babri. Upper bounds on the number of hidden neurons in feed-
forward networks with arbitrary bounded nonlinear activation functions. IEEE Transactions on
Neural Networks, 9(1):224-229, 1998.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In International Conference on Machine Learning, 2018.
Jesse Johnson. Deep, skinny neural networks are not universal approximators. In International
Conference on Learning Representations, 2019.
Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference
on Learning Theory, 2020.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2013.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural Networks,
6(6):861-867, 1993.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
10
Published as a conference paper at ICLR 2021
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems,
2017.
Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 8:
143-195,1999.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and
when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International
Journal of Automation and Computing, 14(5):503-519, 2017.
Moshe Shimrat. Algorithm 112: position of point relative to polygon. Communications of the ACM,
5(8):434, 1962.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, 2016.
Carsten Thomassen. The Jordan-SchGnflies theorem and the classification of surfaces. The American
Mathematical Monthly, 99(2):116-130, 1992.
Helge Tverberg. A proof of the Jordan curve theorem. Bulletin of the London Mathematical Society,
12(1):34-38, 1980.
Roman Vershynin. Memory capacity of neural networks with threshold and ReLU activations. arXiv
preprint 2001.06938, 2020.
Ming-Xi Wang and Yang Qu. Approximation capabilities of neural networks on unbounded domains.
arXiv preprint arXiv:1910.09293, 2019.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In
Conference on Learning Theory, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small ReLU networks are powerful memorizers: a tight
analysis of memorization capacity. In Advances in Neural Information Processing Systems, 2019.
11
Published as a conference paper at ICLR 2021
Appendix
We first provide proofs of upper bounds in Theorems 1, 3, 4 in Appendix A. In Appendix B, we
provide proofs of lower bounds in Theorem 1, 3 and proofs of Lemmas 5, 6, 7 used for proving the
lower bound in Theorem 2. Throughout Appendix, we denote the coordinate-wise RELU by σ and
we denote the i-th coordinate of an output of a function f(x) by (f(x))i.
A	Proofs of upper bounds
A.1 Proof of tight upper bound in Theorem 3
In this section, we prove the tight upper bound on the minimum width in Theorem 3, i.e., width-
(max{dx + 1, dy}) RELU+STEP networks are dense in C([0, 1]dx , Rdy). In particular, we prove
that for any f * ∈ C([0,1]dx, [0,1]dy ),forany ε> 0, there exists a ReLU+Step network f of width
max{dx + 1,dy} such that suPχ∈[o,i]dx Ilf*(x) - f (x)k∞ ≤ ε. Here, We note that the domain
and the codomain can be easily generalized to arbitrary compact support and arbitrary codomain,
respectively.
Our construction is based on the three-part coding scheme introduced in Section 4.1. First, consider
constructing a RELU+STEP network for the encoder. From the definition of qK, one can observe
that the mapping is discontinuous and piecewise constant. Hence, the exact construction (or even
the uniform approximation) of the encoder requires the use of discontinuous activation functions
such as STEP (recall its definition x 7→ 1[x ≥ 0]). We introduce the following lemma for the exact
construction of qK . The proof of Lemma 8 is presented in Appendix A.4.
Lemma 8. For any K ∈ N, there exists a RELU+STEP network f : R → R of width 2 and O(2K)
layers such that f(x) = qK (x) for all x ∈ [0, 1].
For constructing the encoder via a RELU+STEP network of width dx + 1, we sequentially apply qK
to each input coordinate, by utilizing the extra width 1 and Lemma 8. Here, when we apply qK to
some coordinate (requiring width 2), we preserve other coordinates by applying identity mapping
(requiring width dx - 1) for constructing the encoder within width dx + 1. Once we apply qK for all
input coordinates, we apply the linear transformation Pid=x 1 qK(xi) × 2-(i-1)K to obtain the output
of the encoder.
On the other hand, the memorizer only maps a finite number of scalar inputs to the corresponding
scalar targets, which can be easily implemented by piecewise linear continuous functions. We show
that the memorizer can be exactly constructed by a ReLU network of width 2 using the following
lemma. The proof of Lemma 9 is presented in Appendix A.5.
Lemma 9. For any function f * : R → R, any finite set X ⊂ R, and any compact interval I ⊂ R
containing X, there exists a RELU network f : R → R of width 2 and O(|X |) layers such that
f(x) = f* (x) for all x ∈ X and f(I) ⊂ min f*(X), max f*(X).
Likewise, the decoder maps a finite number of scalar inputs in CdyM to corresponding target vectors
in CMdy . Here, each coordinate of a target vector corresponds to some consequent bits of the binary
representation of the input. Under the similar idea used for our implementation of the memorizer, we
show that the decoder can be exactly constructed by a RELU network of width dy using the following
lemma. The proof of Lemma 10 is presented in Appendix A.6.
Lemma 10. For any dy, M ∈ N, for any δ > 0, there exists a RELU network f : R → R2 of width
dy and O(2M) layers such that for all c ∈ CdyM
f(c) = decodeM (c).
Furthermore, it holds that f(R) ⊂ [0, 1]dy.
Finally, as the encoder, the memorizer, and the decoder can be constructed by ReLU+Step networks
of width dx + 1, width 2, and width dy, respectively, the width of the overall RELU+STEP network
f is max{dx + 1, dy}. In addition, as mentioned in Section 4.1, choosing K, M ∈ N large enough
so that ωf * (2-K) + 2-M ≤ ε ensures ∣∣f * - f k∞ ≤ ε. This completes the proof of the tight upper
bound in Theorem 3.
12
Published as a conference paper at ICLR 2021
A.2 Proof of tight upper bound in Theorem 1
In this section, we derive the upper bound in Theorem 1. In particular, we prove that for any
P ∈ [1, ∞), for any f * ∈ Lp(Rdx, Rdy), for any ε > 0, there exists a RELU network f of width
max{dχ + 1, dy} such that ∣∣f * - f kp ≤ ε. To this end, We first note that since f * ∈ Lp(Rdx, Rdy),
there exists a continuous function f0 on a compact support such that
∣f* - f0kp ≤ ε.
Namely, if we construct a ReLU network f such that ∣f0 - f ∣p ≤ W, then it completes the proof.
Throughout this proof, we assume that the support of f0 is a subset of [0, 1]dx and its codomain to
be [0, 1]dy which can be easily generalized to arbitrary compact support and arbitrary codomain,
respectively.
We approximate f0 by a RELU network using the three-part coding scheme introduced in Section
4.1. We will refer to our implementations of the three parts as encode，(x), memorize，M, and
decodeM. That is, we will approximate f by a ReLU network
f := decodeM ◦ memorize，M ◦ encode，.
However, unlike our construction of ReLU+Step networks in Appendix A.1, Step is not available,
i.e., uniform approximation of q， is impossible. Nevertheless, one can approximate q， with some
continuous piecewise linear function by approximating regions around discontinuities with some
linear functions. Under this idea, we introduce the following lemma. The proof of Lemma 11 is
presented in Appendix A.7.
Lemma 11. For any dx, K ∈ N, for any γ > 0, there exist a RELU network f : Rdx → R of width
dx + 1 and O(2，) layers and Dγ ⊂ [0, 1]dx such that for all x ∈ [0, 1]dx \ Dγ,
f(x) = encode， (x),
μ(Dγ) < Y, f(Dγ) ⊂ [0,1], and f (Rdx \ [0,1]dx) = {1 — 2dxK} where μ denotes the Lebesgue
measure.
By Lemma 11, there exist a ReLU network encode，of width dχ + 1 and DY ⊂ [0,1]dx such that
μ(Dγ) < γ and
encode，(x) = encode，(x) for all X ∈ [0,1]dx \ DY,
encode，(Rdx \ [0,1]dx) = {1 - 2dx，}.
(4)
We approximate the encoder by encode，. Here, we note that inputs from DY would be mapped to
arbitrary values by encode，. Nevertheless, it is not critical to the error ∣∣f0 - f ∣p as μ①Y) < Y
can be made arbitrarily small by choosing a sufficiently small γ .
The implementation of the memorizer utilizes Lemma 9 as in Appendix A.1. However, as f0 (x) = 0
for all x ∈ Rdx \ [0,1]dx, we construct a ReLU network memorize，M of width 2 so that
memorize，M (encodeKL(Rdx \ [0,1]dx)) = {0}.
To achieve this, we design the memorizer for c ∈ Cdx， using Lemma 9 and based on (4) as
f	0	if C = 1 - 2dx，
memorize，,M (c) = memorize，,M (c) otherwise
We note that such a design incurs an undesired error that a subset of E， := [1 - 2-，, 1]dx might be
mapped to zero after applying memorizef，,M. Nevertheless, mapping E， to zero is not critical to the
error ∣∣f 0 - f ∣p as μ(Eχ) < 2-dx，can be made arbitrarily small by choosing a sufficiently large K.
We implement the decoder by a RELU network decodefM of width dy using Lemma 10 as in
Appendix A.1. Then, by Lemma 10, it holds that decodefM (R) ⊂ [0, 1]dy, and hence, f(Rdx) ⊂
[0, 1]dy.
13
Published as a conference paper at ICLR 2021
Finally, we bound the error kf0 - f kp utilizing the following inequality:
1
p
kf0-fkp
kf0(x) - f (x)kpp dx
x
kf0(x)-f(x)kppdx+	kf0(x)-f(x)kppdx
1
p
≤	dy(ω∕(2-K) + 2-M)p + (μ(Eκ) + μ(Dγ)) X
1
P
sup kf0(x) - f(x)kpp
x∈EK ∪Dγ
< dy(ωf0(2-K) + 2-M)p + (2-dxK + γ) X sup kf0(x)kp+	sup	kf(x)kp
x∈[0,1]dx	x∈[0,1]dx
1
P P
≤ dy(3f0(2-K) + 2-M)p + (2-dxK + Y)x ( sup	kf0(x)kp + (dy)p
x∈[0,1]dx
1
p
p
By choosing sufficiently large K, M and sufficiently small γ, one can make the RHS smaller than
ε∕2 as suPχ∈[o,i]dx kf0(x)kp < ∞. This completes the proof of the tight upper bound in Theorem 1.
A.3 Proof of Theorem 4
In this section, we prove Theorem 4 by proving the following statement: For any p ∈ [1, ∞), for any
f * ∈ Lp(K, Rdy), for any ε > 0, there exists a P network f of width max{dχ + 2,dy + 1} such that
Ilf * - f kp ≤ ε. Here, there exists a continuous function f ∈ C (K, Rdy) such that
ε
kf * - f0kp≤ 2
since f * ∈ Lp(K, Rdy). Namely, if we construct a P network f such that kf 0 一 fkp ≤ W, it completes
the proof. Throughout the proof, we assume that the support of f0 is a subset of [0, 1]dx and its
codomain is [0, 1]dy which can be easily generalized to arbitrary compact support and arbitrary
codomain, respectively.
Before describing our construction, we first introduce the following lemma.
Lemma 12 [Kidger and Lyons (2020, Proposition 4.9)]. Let P : R → R be any continuous non-
polynomial function which is continuously differentiable at some z with P0(z) 6= 0. Then, for any
f* ∈ C(K, Rdy ), for any ε > 0, there exists a P network f : K → Rdx X Rdy of width dx + dy + 1
such that for all x ∈ K,
f(x) := (y1(x),y2(x)), where ky1(x) - xk∞ ≤ ε and ky2(x) - f*(x)k∞ ≤ ε.
We note that Proposition 4.9 by Kidger and Lyons (2020) only ensures ky2(x) - f*(x)k∞ ≤ ε;
however, its proof provides ky1 (x) - xk∞ ≤ ε as well.
The proof of Theorem 4 also utilizes our coding scheme; here, we approximate ReLU network
constructions encode，, memorize，m, and decodeM in Appendix A.2 by P networks. Using
Lemma 12, for any ει > 0, we approximate encode，by a P network encode，of width dχ + 2 so
that
IencodeK(x) — encode，(x) ∣∣	≤ ει for all X ∈ [0,1]dx \ DY
and encode，([0,1]dx) ⊂ [-ει, 1 + ει]. We note that encode，([0,1]dx) ⊂ [—ει, 1 + ει] is possible
as encode，(Rdx) ⊂ [0,1] by Lemma 11.
Approximating the memorizer can be done in a similar manner. Using Lemma 12, for any compact
interval 12 ⊂ R containing CdxK, for any 22 > 0, we approximate memorize，M by a P network
memorize，M of width 3 so that
||memorize，M(c) — memorize，/(c)∣∣ ≤ ε for all C eCdx，
14
Published as a conference paper at ICLR 2021
and memorizeK m(I2) ⊂ [—ε2,1 + ε2]. We note that memorizeK m(I2) ⊂ ]—ε2,1 + ε2] is
possible as there exists memorizeK M (i∙e∙, a RELU network) such that memorizeK m (I2) ⊂ [0,1]
by Lemma 9.
For approximating the decoder, we introduce the following lemma. The proof of Lemma 13 is
presented in Appendix A.8.
Lemma 13. For any dy, M ∈ N, for any ε > 0, for any compact interval I ⊂ R containing [0, 1],
there exists a ρ network f : R → Rdy of width dy + 1 such that for all c ∈ I,
kf (c) — decodeM(c)k∞ ≤ ε.
Namely, f (I) ⊂ [—ε, 1 + ε]dy.
By Lemma 13, for any compact interval I3 ⊂ R containing [0, 1], for any ε3 > 0, there exists a ρ
network decodeM of Width dy + 1 such that
BcOdeM(c) — decodeM(C)Il	≤ ε3 for all C ∈ CdyM
and decodeM(&) ∈ [-ε3,1 + ε3]dy.
We approximate f0 by a ρ network f of width max{dx + 2, dy + 1} defined as
f := decodeM ◦ memorizeK M ◦ encode，.
Here, for any η > 0, by choosing sufficiently large K, M, sufficiently large I2,I3, and sufficiently
Small ε1,ε2,ε3 sothat ωf0(2-K ) + 2-M ≤ 2 and ωdecodeM ("memorizeK,. (εl)+ ε2) + ε3 ≤ 2, We
have
sup	kf0(x)- f(x)k∞ ≤ η and f([0,1]dx) ⊂ [—2, 1 + 2]dx
x∈[0,1]dχ ∖Dγ
where ω . *	and ω^ - are defined on ∣ and I3, respectively.
memorizeK,M	decodeM
Finally, we bound the error kf0 — f kp utilizing the following inequality:
1
p p	∖ P
(5)
kf0—fkp
kf0(x) —f(x)kppdx
[0,1]dx
kf0(x)—f(x)kppdx+	kf0(x) — f(x)kppdx
1
P
1
p
≤ I e∕up∖o kf0(χ)- f(χ)kp + μ(Dγ) × 徵 kf0(χ)- f(χ)kp
1
Jf(X)kp)py
≤ sup	kf0(x) — f (x)kpp +γ × sup kf 0 (x)kp + sup
∖x∈[0,1]dχ ∖Dγ	'x∈[0,1]dχ	x∈[0,1]d
By choosing sufficiently small ε1, ε2, ε3, γ, sufficiently large K, M, and sufficiently large I2, I3, one
can make the RHS smaller than ε∕2 due to (5) and the fact that suPχ∈[o,i]dx kf 0(χ) Ilp < ∞. This
completes the proof of Theorem 4.
A.4 Proof of Lemma 8
We construct f : R → R as f (x) := f2κ ◦…。fι (x) where each f` : R → R is defined for
x ∈ [0, 1] as
一、ʃ('T)X 2-k
f'(x):=
x
= g`3 ◦ g`2 ◦ g`1 (x)
ifx ∈ [(' — 1) × 2-k,1 × 2-K)
ifx ∈ [(' — 1) × 2-k,0 × 2-k)
15
Published as a conference paper at ICLR 2021
where g`1 : R → R2 , g`2 : R2 → R2, and g`3 : R2 → R are defined as
g'i(x)：= (σ(x),σ(x - '))
g'2(x, z) ：= (σ(x + z), -σ(x - ' +1))
g'3(x,z) ：= σ(x + z) + 1[x ≥ '].
This directly implies that f(x) = qK (x) for all x ∈ [0, 1] and completes the proof of Lemma 8.
A.5 Proof of Lemma 9
Let |X | = N and x(1), . . . , x(N) be distinct elements of X in an increasing order, i.e., x(i) < x(j) if
i < j. Let x(0) ：= minI and x(N +1) ：= maxI. Here, x(0) ≤ x(1) and x(N) ≤ x(N +1) as X ⊂ I.
Without loss of generality, we assume that x(0) = 0. Consider a continuous piecewise linear function
ft : [X(O), x(N+1)] → R of N + 1 linear pieces defined as
[f *(x(D)	if x ∈ [x(0),x(1))
f t(x) ：= ∖ f *(x(i)) + f*(xXi+1))-Xl)x(i)) (x - X(i)) if X ∈ [x(i),x(i+1)) for 1 ≤ i ≤ N - 1 .
If *(X(N))	if x ∈ [x(N), x(N+1)]
Now, we introduce the following lemma.
Lemma 14. For any compact interval I ⊂ R, for any continuous piecewise linear function f * :
I → R with P linear pieces, there exists a RELU network f of width 2 and O(P) layers such that
f* (X) = f(X) for all X ∈ I.
From Lemma 14, there exists a RELU network f of width 2 and O(|X |) layers such that ft (X) =
f(X) for allX ∈ X. Since X ⊂ [X(0), X(N+1)] = Iand ft (I) ⊂ minf*(X),maxf*(X), this
completes the proof of Lemma 9.
Proof of Lemma 14. Suppose that f* is linear on intervals [min I, X1), [X1, X2), . . . , [XP-1, maxI]
and parametrized as
{aι X x + bi	if X ∈ [minI, xι)
a2 × X + b2	if X ∈ [X1, X2)
..
aP × x + bP	if x ∈ [xP-1, max I]
for some ai, bi ∈ R satisfying ai × xi + bi = ai+1 × xi + bi+1. Without loss of generality, we
assume that minI = 0.
Now, we prove that for any P ≥ 1, there exists a RELU network f ： I → R2 of width 2 and
O(P) layers such that (f (x))1 = σ(x - xP-1) and (f (x))2 = f*(x). Then, (f (x))2 is the desired
RELU network and completes the proof. We use the mathematical induction on P for proving the
existence of such f. If P = 1, choosing (f (x))1 = σ(x) and (f (x))2 = a1 × σ(x) + b1 completes
the construction of f. Now, consider P > 1. From the induction hypothesis, there exists a RELU
network g of width 2 such that
(g(X))1	σ(X - XP-2) [a1 × X + b1 [a2 × X + b2	ifX∈ ifX∈	[min I, X1 ) [X1, X2)
(g(X))2	aP-1 × X + bP-1	. . . ifX∈	[XP-2, maxI]
Then, the following construction of f completes the proof of the mathematical induction:
f(x) = h2 ◦ h1 ◦ g(x)
h1(x, z) = σ(x - xP-1 + xP-2), σ(z - K) + K
h2(x, z) = x, z + (aP -1 - aP-2) × x
where K := mini minχ∈ι{ai × X + bi}. This completes the proof of Lemma 14.	□
16
Published as a conference paper at ICLR 2021
A.6 Proof of Lemma 10
Before describing our proof, we first introduce the following lemma. The proof of Lemma 15 is
presented in Appendix A.9.
Lemma 15. For any M ∈ N, for any δ > 0, there exists a RELU network f : R → R2 of width 2
and O(2M) layers such that for all x ∈ [0, 1] \ DM,δ,
f(x) := (y1(x), y2(x)), where y1(x) = qM (x), y2(x) = 2M × (x - qM (x)),	(6)
and DM,δ := Si2=1-1(i × 2-M - δ, i × 2-M). Furthermore, it holds that
f(R) ⊂ [0,1-2-M] × [0, 1].	(7)
In Lemma 15, one can observe that CdyM ⊂ [0, 1] \ DM,δ for δ < 2-dyM , i.e., there exists a
RELU network g of width 2 satisfying (6) on CdyM and (7). g enables us to extract the first M
bits of the binary representation of c ∈ CdyM. Consider outputs of g(c): (g(c))1 for c ∈ CdyM
is the first coordinate of decodeM (c) while (g(c))2 ∈ C(dy-1)M contains information on other
coordinates of decodeM (c). Now, consider further applying g to (g(c))2 and passing though the
output (g(c))1 via the identity function (RELU is identity for positive inputs). Then, g (g(c))2 1
is the second coordinate of decodeM (c) while g (g(c))2 2 contains information on coordinates
other than the first and the second ones of decodeM (c). Under this observation, if we iteratively
apply g to the second output of the prior g and pass through all first outputs of previous g’s, then
we recover all coordinates of decodeM (c) within dy - 1 applications of g. Note that both the first
and the second outputs of the (dy - 1)-th g correspond to the second last and the last coordinate of
decodeM (c), respectively. Our construction of f is such an iterative dy - 1 applications of g which
can be implemented by a RELU network of width dy. Here, (7) in Lemma 15 enables us to achieve
f(R) ⊂ [0, 1]dy. This completes the proof of Lemma 10.
A.7 Proof of Lemma 11
To begin with, we introduce the following Lemma. The proof of Lemma 16 is presented in Ap-
pendix A.10.
Lemma 16. For any dx, for any α ∈ (0, 0.5), there exists a RELU network f : Rdx → Rdx of width
dx + 1 and O(1) layers such that f(x) = (1, . . . , 1) for all x ∈ Rdx \ [0, 1]dx, f(x) = x for all
x ∈ [α, 1 - α]dx, andf(Rdx) ⊂ [0, 1]dx.
By Lemma 16, there exists a RELU network h1 of width dx + 1 and O(1) layers such that h1(x) =
(1, . . . , 1) for all x ∈ Rdx \ [0, 1]dx, h1(x) = x for all x ∈ [α, 1 - α]dx, and h1 (Rdx) ⊂ [0, 1]dx .
Furthermore, by Lemma 15, for any δ > 0, there exists a RELU network g : R → R of width 2 and
O(2K) layers such that g(c) = qK (c) for all c ∈ [0, 1] \ DK,δ (see Lemma 15 for the definition of
DK,δ).
We construct a network h2 : Rdx → Rdx of width dx + 1 by sequentially applying g for each
coordinate of an input x ∈ Rdx , utilizing the extra width 1. Then, h2 (x) = qK (x) for all x ∈
[0, 1]dx \ DK,δ,dx where
DK,δ,dx := {x ∈ Rdx : xi ∈ DK,δ for some i}.
Note that we use qK (x) for denoting the coordinate-wise qK for a vector x.
Now, we define Dγ := ([0, 1]dx \ [α, 1 - α]dx) ∪ DK,δ,dx ⊂ [0, 1]dx. Then, from constructions of h1
and h2, we have
h2 ◦ h1 (x)	=	qK (x)	for all x	∈	[0, 1]dx \ Dγ
h2 ◦h1(x)	=	(1 - 2-K,. . . ,1	-	2-K)	for all x	∈	Rdx	\ [0, 1]dx
h2 ◦ h1 (x)	⊂	[0, 1 - 2-K]dx	for all x	∈	Dγ
where we use the fact that (1, .	. . , 1) ∈/ DK,δ,dx	and qK ((1, . . . , 1)) = (1	- 2-K, . . . , 1 - 2-K).
17
Published as a conference paper at ICLR 2021
Finally, we construct a RELU network f of width dx + 1 as
dx
f(x):= X(h2 ◦ h1(x))i × 2-(i-1)K.
i=1
Then, it holds that
f (x) = encodeK (x)
f(x) = 1 - 2dxK
f(x) ⊂ [0, 1]
for all x ∈ [0, 1]dx \ Dγ
for all x ∈ Rdx \ [0, 1]dx
for all x ∈ Dγ .
In addition, if We choose sufficiently small a and δ so that μ(Dγ) < Y, then f satisfies all conditions
in Lemma 11. This completes the proof of Lemma 11.
A.8 Proof of Lemma 13
The proof of Lemma 13 is almost identical to that of Lemma 10. In particular, We approximate
the RELU netWork construction of iterative dy - 1 applications of g (see Appendix A.6 for the
definition of g) by a ρ netWork of Width dy + 1. To this end, We consider a ρ netWork h of Width
3 approximating g on some interval J Within α error using Lemma 12. Then, one can observe
that iterative dy - 1 applications of h (as in Appendix A.6) results in a ρ netWork f of Width
dy + 1. Here, passing through the identity function can be approximated using a ρ netWork of Width
1, i.e., same Width to RELU netWorks (see Lemma 4.1 by Kidger and Lyons (2020) for details).
Furthermore, since h is uniformly continuous on J, it holds that kf (c) - decodeM (c)k∞ ≤ ε for
all c ∈ CdyM and f(I) ⊂ [-ε, 1 + ε]dy by choosing sufficiently large J and sufficiently small α so
that ω%(•一ω%(ωh(α) + α) ∙∙∙) + α ≤ ε.8 This completes the proof of Lemma 13.
A.9 Proof of Lemma 15
We first clip the input to be in [0, 1] using the folloWing RELU netWork of Width 1.
min max{x, 0}, 1 = 1 - σ(1 - σ(x))
After that, We apply g` : [0, 1] → [0, 1]2 defined as
(g'(x))ι ：= X
，0	if	x	∈	[0, 2-M - δ]
δ-12-M	X	(x	-	2-M	+ δ)	if	x	∈	(2-M - δ, 2-M)
2-M	if	x	∈	[2-M, 2 × 2-M —	δ]
(g`(X))2 ：= < δ-12-M	×	(x	—	2 × 2-M +	δ) + 2-M	if X ∈	(2 × 2-M — δ, 2	×	2-M) . ⑻
.
.
.
、(' -1) × 2-M	if X ∈ [(' - 1) × 2-M, 1]
From the above definition of g`, one can observe that (g2M (x))2 = qM (x) for x ∈ [0, 1] \ DK,δ.
Once We implement a RELU netWork g of Width 2 such that g(x) = g2M (x), then, constructing f as
f (X) = ((O(Z))2, 2M X ((O(Z))I- (g(Z))2))
z ：= min max{x, 0}, 1
completes the proof. Note that as (O(x))2 ≤ x = (O(x))1 for all x ∈ [0, 1], f(x) ∈ [0, 1 -
2-M] X [0, 1]. NoW, We describe hoW to construct O2M by a RELU netWork. One can observe that
(O1(x))2 = 0 and
(g'+1(x))2 = min n` x 2-M, max {δ-12-M X (x — ' X 2-M + δ) + (' — 1) X 2-M, g'(x)}}
for all x, i.e., alternating applications of min{∙, ∙} and max{∙, ∙}. Finally, We introduce the following
definition and lemma.
8We consider ωh on J .
18
Published as a conference paper at ICLR 2021
Definition 1 [Hanin and Sellke (2017)]. f : Rdx → Rdy is a max-min string of length L if there exist
affine transformations h1 , . . . , hL such that
h(x) = TL-i(hL(x) ,TL-2(hL-i(x),…，T2 (h3(x),τ1(h2(x), hl(x))),…)
where each t` is either a coordinate-wise max{∙, ∙} or min{∙, ∙}.
Lemma 17 [Hanin and Sellke (2017, Proposition 2)]. For any max-min string f * : Rdx → Rdy of
length L, for any compact K ⊂ Rdx, there exists a RELU network f : Rdx → Rdx × Rdy of L layers
and width dx + dy such that for all x ∈ K,
f(x) = (y1(x),y2(x)),	where y1 (x) = x and y2(x) = f*(x).
We note that Proposition 2 by Hanin and Sellke (2017) itself only ensures y2 = f*(x); however, its
proof provides y1 = x as well.
From the definition of the max-min string, one can observe that (g2M (x))2 is a max-min string.
Hence, by Lemma 17, there exists a RELU network g of width 2 such that g(x) = g2M (x) = qM (x)
for all x ∈ DK,δ. This completes the proof of Lemma 15.
A.10 Proof of Lemma 16
Consider the following two functions from R to R:
(0	if X ≤ 1 — α
hι(x) :=	1 (x — 1 + a) if x ∈ (1 — a, 1)
[1	if x ≥ 1
=σ(1 — σ(1 — x)∕α)
(1	if x ≤ 0
h2 (x) := O- (α — x)	if x ∈ (0, a)
[0	if x ≥ α
=1 — σ(1 — σ(α — x)∕α).	(9)
Using h1 and h2, we first map all x ∈ Rdx \ [0, 1]dx to some vector whose coordinates are greater
than one via g : Rdx → Rdx , defined as
g ：= rdx ◦ Sdx …。ri ◦ si
r'(x) := σ(x + 1) — 1 + 10 X hi(xg)
S'(x) := σ(x + 1) — 1 + 10 × h2(x').
Here we use the addition between a vector and a scalar for denoting addition of the scalar to each
coordinate of the vector. Then, one can observe that if x ∈ [α, 1 — α]dx, then g(x) = x and if
x ∈ Rdx \ [0, 1]dx, then each coordinate of g(x) is greater than one. Furthermore, each r` (or s`) can
be implemented by a RELU network of width dx + 1 (width dx for computing σ(x + 1) — 1 and
width one for computing hi (xQ) due to (9). Hence, g can be implemented by a RELU network of
width dx + 1.
Finally, we construct a RELU network f : Rdx → Rdx of width dx + 1 as
f (x) := min max{g(x), 0}, 1
min max{x, 0}, 1 =1 — σ(1 — σ(x)).
Then, one can observe that if x ∈ [α, 1 — α]dx, then f(x) = x and if x ∈ Rdx \ [0, 1]dx, then
f(x) = (1, . . . , 1), and f (Rdx ) ⊂ [0, 1]dx. This completes the proof of Lemma 16.
19
Published as a conference paper at ICLR 2021
B Proofs of lower bounds
B.1 Proof of general lower bound
In this section, we prove that neural networks of width dy - 1 is not dense in both Lp(K, Rdy) and
C(K, Rdy), regardless of the activation functions.
Lemma 18. For any set of activation functions, networks of width dy - 1 are not dense in both
Lp(K, Rdy) and C(K, Rdy).
Proof. In this proof, we show that networks of width dy - 1 are not dense in Lp([0, 1]dx , Rdy), which
can be easily generalized to the cases of Lp(K, Rdy) and C(K, Rdy). In particular, we prove that
there exist a continuous function f * ∈ Lp([0,1]dx, Rdy) and ε > 0 such that for any network f of
width dy - 1, it holds that
kf*-fkp>ε.
Let ∆ be a dy -dimensional regular simplex with sidelength √2, that is isometrically embedded into
Rdy. The volume of this simplex is given as Voldy (∆) = VZdy+ 1.9 We denote the vertices of this
simplex by {v0, . . . , vdy }. Then, we can construct the counterexample as follows.
f*(x)
]vi	if x1	∈	[ 2dy⅛,	2¾⅛ ]	for some i
ɪ (2dy + 1)(vi+1	— Vi)x1	+ (2i + 2)vi	— (2i + 1)vi+1 if x1	∈	[2dy+11,2dy~2ι]	for some i
In other words, f* (x) travels the vertices of ∆ sequentially as we move x1 from 0 to 1, staying
at each vertex over an interval of length ?4 1+1 and traveling between vertices at a constant speed
otherwise, i.e., f* is continuous and in Lp([0, 1]dx , Rdy ).
Recalling (1), one can notice that any neural network f of width less than dy and L ≥ 2 layers can
be decomposed as tL ◦ g, where tL : Rk → Rdy is the last affine transformation and g denotes all the
preceding layers, i.e., g = σL-1 ◦力工_1 ◦•••◦ σ1 ◦ t1. Here, we consider k = dy - 1 as it suffices to
cover cases k < dy - 1. Now, we proceed as
kf*-fkp= Z Z	kf*(x) - f (x)kp dx! P
[0,1]dx
1
≥ Z	kf*(x) - tL ◦ g(x)kpp dx
[0,1]dx
1
≥ Z	inf	kf*(x) -tL(u*(x))kpp dx
∖∕[0,1]dχ U*(x)∈RdyT	I
≥ I 7rτ~iT I inf max inf	kvi - t(u*)Ilp
∖2dy + 1 /	t: affine map i∈[dy + 1]呜 ∈Rdy-1
≥
inf max inf
H∈H i∈[dy+1] ai∈H
kvi - ai kp,
where H denotes the set of all (dy - 1)-dimensional hyperplanes in Rdy and [dy+1] := {0, 1, . . . , dy}.
As the vertices of∆ are dy+1 distinct points in a general position, infH∈H maxi∈[dy+1] infai∈H kvi-
ai kp > 0. To make this argument more concrete, we take a volumetric approach; for any k-
dimensional hyperplane H ∈ Rdy , we have
VOIdy ⑷ ≤2 ∙ vWy-gH。)) ∙ i∈ma+1] ainH kvi - aik2,
9Vold(S) denotes the volume of S in the d-dimensional Euclidean space.
20
Published as a conference paper at ICLR 2021
where ∏h denotes the projection onto H. As projection is contraction and the distance between any
two points are at most √2, it holds that for any H,
max inf kvi - ai k2 ≥
i∈[dy+1] ai∈H
____________VOldy ©____________
2 ∙ VOldy-ι({χ ∈ RdyT : kxk2 ≤ 1})
_______ dy - 1
√dy + 1 γ (<d + 1A ( 2 A F
2 ∙ dy!	2 )⑺
where Γ denotes the gamma function, and we use the fact that Voldy-1 {x ∈ Rdy-1 : kxk2 ≤ 1} ≥
Voldy-1 (πH (∆)) as ∆can be contained in a dy-dimensional unit ball, and hence πH (∆) can be
contained in a (dy - 1)-dimensional unit ball. Thus we have ∣∣f * - f kp > ε with
ε
1 - 1	______
dy 2 Pdy + 1
2 ∙ (2dy + 1)p ∙ d
for p ≥ 2 and
ε
dydy + 1
2 ∙ (2dy + 1) P ∙
• Γ
• Γ
for p < 2. This completes the proof of Lemma 18.
□
B.2	Proof of tight lower bound in Theorem 3
In this section, we prove the tight lower bound in Theorem 3. Since we already have the width-dy
lower bound by Lemma 18 and it is already proven that RELU networks of width dx is not dense
in C(K, Rdy ) (Hanin and Sellke, 2017), we prove the tight lower bound in Theorem 3 by showing
the following statement: There exist f* ∈ C([0, 1]dx, R) and ε > 0 such that for any RELU+STEP
network f of width dx containing at least one STEP, it holds that
∣f* - f∣∞ > ε.
Without loss of generality, we assume that f has dx hidden neurons at each layer except for the output
layer and all affine transformations in f are invertible (see Section 5.1).
Our main idea is to utilize properties of level sets of width-dx RELU+STEP networks (Hanin and
Sellke, 2017) defined as follows: Given a network f of width dx , we call a connected component of
f-1(y) for some y as a level set. Level sets of RELU+STEP networks have a property described by
the following lemma. We note that the statement and the proof of Lemma 19 is motivated by Lemma
6 of (Hanin and Sellke, 2017).
Lemma 19. Let f be a RELU+STEP network of width dx containing at least one S TEP. Then, for
any level set S of f, S is unbounded unless it is empty.
ProofofLemma 19. Let '* be the smallest number such that STEP appears at the '*-th layer. In this
proof, we show that all level sets of the first `* layers of f are either unbounded or empty. Then the
claim of Lemma 19 directly follows. We prove this using the mathematical induction on `*. Recalling
(1), we denote by f` the mapping of the first ` layers of f :
f` ：= Q' ◦ t` ◦•••◦ σι ◦ tι.
First, consider the base case: `* = 1. Assume without loss of generality that the activation function
of the first hidden node in σ1 is STEP. Then for any x, the STEP activation maps the first component
of t1 (x) to 1 if (t1 (x))1 ≥ 0, and to 0 otherwise. This means that there exists a ray R starting from x
such that f1(R) = {f1(x)}. Hence, any level set of f1 is either unbounded or empty.
Now, consider the case that `* > 1. Then, until the (`* - 1)-th layer, the network only utilizes RELU.
Here, the level sets of f`*- can be characterized using the following lemma.
Lemma 20 [Hanin and Sellke (2017, Lemma 6)]. Given a RELU network g of width dx, letS ⊂ Rdx
be a set such that x ∈ S if and only if inputs to all RELU in g are strictly positive, when computing
g(x). Then, S is open and convex, g is affine on S, and any bounded level set of g is contained in S.
21
Published as a conference paper at ICLR 2021
Consider S of f`*T as in Lemma 20 and consider a level set T of f`* containing some x, i.e., T = 0.
If x ∈/ S, then T is unbounded by Lemma 20. If x ∈ S, we argue as the base case. The preimage of
f*(x) of the '*-th layer (i.e., σ ◦ t`*) contains a ray. If this ray is contained in ft*-ι (S), then T is
unbounded as ft*-ι is invertible and affine on S. Otherwise, T\S = 0 and it must be unbounded as
any level set of ft*-ι not contained in S is unbounded by Lemma 20. This completes the proof of
Lemma 19.	□
Now, we continue the proof of the tight lower bound in Theorem 3 based on Lemma 19. We note that
our argument is also from the proof of the lower bound in Theorem 1 of (Hanin and Sellke, 2017).
Consider f * : [0,1]dx → R defined as
f * (X1,...,xdχ ) := X (xi - 1).
Then, for a = 4 and b = 0, one can observe that (f *)-1(a) is a sphere of radius 11 centered
at (f *)-1(b) = {(1,..., 1)}. Namely, any path from (f *)-1(b) to infinity must intersect with
(f *)-1(a). Now, suppose that a ReLU+Step network f of width dχ satisfies that kf * - fk∞ ≤ ɪ6.
Then, the level set of f containing (1,..., 1) must be unbounded by Lemma 19, and hence, must
intersect with (f *)-1(a). However, as f * ◦ (f *)-1(a) = 4 and f * ◦ (f *)-1(b) = 0, this contradicts
with kf * 一 f k∞ ≤ 116. This completes the proof of the tight lower bound max{dχ + 1, dy} in
Theorem 3.
B.3	Proof of tight lower bound in Theorem 1
In this section, we prove the tight lower bound in Theorem 1. Since we already have the dy lower
bound by Lemma 18, we prove the tight lower bound in Theorem 1 by showing the following
statement: There exist f* ∈ Lp(Rdx , R) and ε > 0 such that for any continuous function f
represented by a RELU network of width dx, it holds that
kf*-fk∞>ε.
Note that this statement can be easily generalized to an arbitrary codomain. To derive the statement,
we prove a stronger statement: For any RELU network f of width dx , either
f ∈/ Lp(Rdx,R) or f = 0	(10)
where f = 0 denotes that f is a constant function mapping any input to zero. Then it leads us to the
desired result directly. Without loss of generality, we assume that f has dx hidden neurons at each
layer except for the output layer and all affine transformations in f are invertible (see Section 5.1).
As in the proof of the tight upper bound in Theorem 3, we utilize properties of level sets of f given
by Lemma 20. Let S be a set defined in Lemma 20 of f. By the definition of S, one can observe that
Rdx \ S 6= 0. Then, a level set T containing some x ∈ Rdx \ S must be unbounded by Lemma 20.
Here, if y := f (x) > 0, then for δ := ω-1( 1), We have f (x0) ≥ y for all
x0 ∈ T0 := {x0 ∈ Rdx : ∃x ∈ T such that kx0 - xk∞ ≤ δ}.
Since T0 contains T which is an unbounded set, one can easily observe that μ(T0) = ∞ and hence,
RT0 |f(x)|pd(x) = ∞, i.e., f ∈/ Lp(Rdx , R).10 One can derive the same result for f(x) < 0.
Suppose that f(x) = 0 for all x ∈ Rdx \ S. Then, f(x) = 0 for all x ∈ ∂S as S is open (see Lemma
20). Furthermore, we claim that f(S) = {0} or f ∈/ Lp(Rdx , R). For any s ∈ S, consider any two
rays of opposite directions starting from s. If one ray is contained in S and f ∈ Lp(Rdx , R), then
its image for f must be {0}. If the image of f is not {0}, using the similar argument for the case
f(x) > 0 leads us to f ∈/ Lp(Rdx , R). Then, one can conclude that f(s) = 0. If both rays are not
contained in S, they must both intersect with ∂S. Then, since f is affine on S, f(s) must be zero as
f(∂S) = {0}. Hence, we prove the claim.
This completes the proof of the tight upper bound in Theorem 1.
10μ denotes the Lebesgue measure.
22
Published as a conference paper at ICLR 2021
B.4	Proof of Lemma 5
Let φ(x) = Ax + b for some invertible matrix A = [a1, a2]> ∈ R2×2 and for some vectors
a1, a2, b ∈ R2. Then, it is easy to see that if
ha1 , xi + b1 ≥ 0 and ha2 , xi + b2 ≥ 0,
i.e., x ∈ S, then φ-1 ◦ σ ◦ φ(x) = x. Hence, the first statement of Lemma 5 holds.
Now, consider the second statement of Lemma 5. Suppose that ha1, xi + b1 ≥ 0 but ha2, xi + b2 < 0.
Then, one can easily observe that φ-1 ◦ σ ◦ φ maps a ray
{x0 ∈ R2 : ha1 , x0i = ha1 , xi, ha2 , x0i + b2 < 0}
containing x to a single point φ-1 (ha1, xi + b1, 0) , which is on ∂S. In addition, similar arguments
hold for cases that ha1, xi + b1 < 0, ha2, xi + b2 ≥ 0 and ha1, xi + b1 < 0, ha2, xi + b2 < 0. This
completes the proof of Lemma 5.
B.5	Proof of Lemma 6
We first prove the first statement of Lemma 6 using the proof by contradiction. Suppose that x0 = x
and x0 ∈/ T0 but x0 is not in a bounded path-component of R2 \ T0 . Here, note that x = x0 ∈ S
for S defined in Lemma 5. Then, there exists a path P from x0 to infinity such that P ∩T0 = 0.
If P ⊂ int(S)11, then the preimages ofP and T0 ∩ int(S) under φ-1 ◦ σ ◦ φ stay identical to their
corresponding images, i.e., P and T0 ∩ int(S) (by Lemma 5). This contradicts the assumption that x
is in a bounded path-component of R2 \ T. Hence, it must hold that P 6⊂ int(S).
Let x* ∈ T0 be the first point in P ∩ ∂S in the trajectory of P starting from χ0. Then, the preimage
of x* contains a ray R starting from x* (see the proof of Lemma 5 for the details) which must not
intersect with T; had the ray R intersected with T, then R ∩ T must have mapped to x* , which
contradicts x* ∈ T0 and the definition of P. Furthermore, from the definition of x*, the subpath Pt
of P from x0 to x* excluding x* satisfies Pt ⊂ int(S). Hence, the preimages of Pt and T0 ∩ int(S)
under φ-1 ◦ σ ◦ φ stay identical by Lemma 5. This implies that there exist a path Pt from X to x*,
and then a path R from x* to infinity, not intersecting with T. This contradicts the assumption of
Lemma 6. This completes the proof of the first statement of Lemma 6.
Now, consider the second statement of Lemma 6. By Lemma 5, x 6= x0 implies that x ∈/ S and
x0 ∈ ∂S. Here, as the preimage of x0 contains a ray from x0 containing x, this ray must intersect
with T from the assumption of Lemma 6. Hence, x0 ∈ T0 and this completes the proof of the second
statement of Lemma 6.
By combining the proofs of the first and the second statements of Lemma 6, we complete the proof
of Lemma 6.
B.6	Proof of Lemma 7
Before starting our proof, we first introduce the following definitions and lemma. The proof of
Lemma 21 is presented in Appendix B.7.
Definition 2. Definitions related to curves, loops, and polygons are listed as follows: For U ⊂ R2
andF(U) :={f ∈C([0,1],R2) :f([0,1])=U},
•	U is a “curve” if there exists f ∈ F(U).
•	U is a “simple curve” if there exists an injective f ∈ F (U).
•	U is a “loop” if there exists f ∈ F(U) such that f(1) = f(0).
•	U is a “simple loop” if there exists f ∈ F(U) such that f(1) = f(0) and f is injective on
[0, 1).
•	U is a “polygon” if there exists a piecewise linear f ∈ F(U) such that f(1) = f (0).
11int(S) denotes the interior of S.
23
Published as a conference paper at ICLR 2021
Figure 3: (a) Illustration of U, g`* (q). (b) Illustration of V, g`* (q), v. (C) Illustration of U, g`* (q), v.
•	U is a “simple polygon” if there exists a piecewise linear f ∈ F(U) such that f(1) = f(0) and
f is injective on [0, 1).
Lemma 21. Suppose that g`* ([0,pι]) ∩g`* ([p2,1]) = 0 andg`* ([0,pι])\B is contained in a bounded
path-component of R2 \ U for some U ⊂ g`* ([p2, 1]) ∪ B. Then, g`* ([0,p1]) \ B is contained in a
bounded path-component of R2 \ (g`* ([p2, 1]) ∪ B).
In this proof, we prove that if g`* ([0,p1]) ∩ g`* ([p2, 1]) = 0, then g`* ([0,p1]) \ B is contained in
a bounded path-component of R2 \ U for some simple polygon U ⊂ g`* ([p2, 1]) ∪ B. Then, the
statement of Lemma 7 directly follows by Lemma 21.
To begin with, consider a loop g`* ([p2, 1]) ∪ L g`* (p2), g`* (1) where L(x, x0) denotes the line
segment from x to x0, i.e.,
L(x, x0) := {λ ∙ x + (1 — λ) ∙ X : λ ∈ [0,1]}.
Then, the loop consists of a finite number of line segments, as an image of an interval of a ReLU
network is piecewise linear as well as L g`* (p2), g`* (1) , i.e., the loop is a polygon.
Since g`* ([p2, 1]) ∪L (g`* (p2), g`* (1)) consists of line segments, under the assumption ∣∣f * 一 f k∞ ≤
100, one can easily construct a simple loop U in g`* ([p2,1]) ∪ L(g`* (p2), g`* ⑴)so that U contains
simple curves from the midpoint of L g`* (p2), g`* (1) to a point near the point (一1, 1), then to a
point near the point (1, 1), and finally to the midpoint of L g`* (p2), g`* (1) . We note that U also
consists of line segments, i.e., U is a simple polygon. Figure 3(a) illustrates U where line segments
from g`* ([p2, 1]) is drawn in blue and line segments from L g`* (p2), g`* (1) indicated by dotted
black line.
Now, choose q ∈ (0,pι) such that f *(q) = (0, 2). Since ∣∣f * 一 f ∣∞ ≤ 忐 and by the definition of
`*,
f(q) = g`*(q) ∈ {x ∈ R2 ： IIx -(0,1 )k∞ ≤ 110}
which is illustrated by the red dot in Figure 3. Then, we claim the following statement:
g`* (q) is contained in a bounded path-component of R2 \ U.	(11)
From the definition of q and the path-connectedness of g`* ([0, q]), one can observe that proving the
claim (11) leads us to that g`* ([0, q]) is contained in a bounded path-component of R2 \ U unless
U∩ g`* ([0, q]) 6= 0. Since g`* ([0,p1]) \ B ⊂ g`* ([0, q]) by the definitions of q, `* and the assumption
that IIf * 一 f ∣∞ ≤ 110, this implies that if g`* ([0,p1]) ∩ g`* ([p2,1]) = 0, then g`* ([0,pι]) \ B is
contained in a bounded path-component of R2 \ U. Hence, (11) implies the statement of Lemma 7.
Proof of claim (11). To prove the claim (11), we first introduce the following lemma.
Lemma 22 [Jordan curve theorem (Tverberg, 1980)]. For any simple loop O ⊂ R2, R2 \ O consists
of exactly two path-components where one is bounded and another is unbounded.
Lemma 22 ensures the existence of a bounded path-component of R2 \ U .
Furthermore, to prove the claim (11), we introduce the parity function πU : R2 \ U → {0, 1}: For
x ∈ R2 \ U and a ray starting from x, πU (x) counts the number of times that the ray “properly”
24
Published as a conference paper at ICLR 2021
intersects with U (reduced modulo 2) where the proper intersection is an intersection where U enters
and leaves on different sides of the ray. Here, it is well-known that πU (x) does not depend on the
choice of the ray, i.e., πU is well-defined. We refer the proof of Lemma 2.3 by Thomassen (1992)
and the proof of Lemma 1 by Tverberg (1980) for more details. Here, πU characterizes the “position”
of x as πU (x) = 0 if and only if x is in the unbounded path-component of R2 \ U, which is known
as the even-odd rule (Shimrat, 1962; Hacker, 1962). Hence, proving that ∏u(g'*(q)) = 1 would
complete the proof of the claim (11).
Recall that there exists the line (e.g., the black arrow in Figure 3) that intersects with B and the image
of g`* can be at only “one side” of the line (see Section 5.2 for details). Since B is open, there exists
a “vertex” v ∈ ∂B (e.g., the green dot in Figures 3(b) and 3(c)) such that v is in the “other side” of
the line.12 We prove πU (g`* (q)) = 1 by counting the number of proper intersections between the ray
R from g`* (q) passing through v (the red arrow in Figures 3(b) and 3(c) illustrates R).
To simplify showing πU (g`* (q)) = 1, we consider two points z1, z2 ∈ U ∩ ∂B near the points
(-1, 1), (1, 1), respectively, such that the simple curve P in U from z1 to z2 is contained in B except
for z1, z2. Then, one can observe that P and L(z1, z2) forms a simple loop which we call V. Figure
3(b) illustrates V where the black dotted line indicates the line segment from L g`* (p2), g`* (1) , the
blue line indicates the line segments from g`* ([p2, 1]), and the green dotted line indicates L(z1, z2);
from the definition of P, the blue and green lines together correspond to P .
Then, πV (g`* (q)) = 1 as a ray from g`* (q) of the downward direction (the blue arrow in Figure 3(b))
only properly intersects once with V at some point in L g`* (p2), g`* (1) , under the assumption that
kf * - f k∞ ≤ ι0o. From the property of ∏v, this implies that the ray R starting from g`* (q) and
passing through v (e.g., the red arrow in Figures 3(b) and 3(c)) must properly intersect with V odd
times. Furthermore, from the construction ofU and V, definition of `*, and under the assumption
that kf * - f k∞ ≤ ι0o, one can observe that the simple curve in U from z1 to z? not contained in B
(i.e., U \P) can only intersect with B within the '∞ balls of radius 系 centered at the points (-1,1)
and (1,1). This is because if U \ P intersects with B outside these '∞ balls, then by definition
of `* , the network cannot make further modifications in B, hence contradicting the approximation
assumption kf * - f k∞ ≤ 忐.In other words, all proper intersections between U and R are identical
to those between V and R. This implies that πU (g`* (q)) = 1 and hence, g`* (q) is in the bounded
path-component of R2 \ U. This completes the proof of the claim (11) and therefore, completes the
proof of Lemma 7.
B.7 Proof of Lemma 21
Suppose that g`* ([0,pι]) ∩ g`* ([p2,1]) = 0 and g`* ([0,pι]) \ B is contained in a bounded path-
component of R2 \ U for some U ⊂ g`* ([p2, 1]) ∪ B. If g`* ([0,p1]) \ B is path-connected, then the
statement of Lemma 21 directly follows. Hence, suppose that g`* ([0, p1]) \ B has more than one
path-components. To help the proof, we introduce the following Lemma.
Lemma 23. If g`* (p) ∈ ∂B for some p ∈ [0, 1], then f(p) = g`* (p).
ProofofLemma 23. Suppose that f (P) = g`* (p). Then, φ-1 ◦ σ ◦ φ'(g'* (P)) = g`* (P) for some
' > '*. By Lemma 5, there exist a1,a2 ∈ R2 and b1,b2 ∈ R such that φ-1 ◦ σ◦ φ'(x) = X if and only
if ha1, xi +b1 ≥ 0, ha2, xi +b2 ≥ 0. Without loss of generality, we assume that ha1, g`* (P)i +b1 < 0.
Since g`* (p) ∈ ∂B, there exists Z ∈ B such that〈ai,z〉+ bi < 0, i.e., φ-1 ◦ σ ◦ φ'(z) = z, which
contradicts to the definition of '* by Lemma 5. This completes the proof of Lemma 23	□
By Lemma 23 and the assumption that kf * - f k ≤ 忐,g`* ([0,P1]) \ B can only intersect with
∂B within the '∞ ball O of radius 品 centered at the point (0,1). Hence, all path-components
of g`* ([0,Pi]) \ B intersect with the line segment ∂B ∩ O. In other words, g`* ([0,Pi]) \ B is in a
path-component of R2 \ (g`* ([P2, 1]) ∪ B) unless g`* ([P2, 1]) intersects with ∂B ∩ O. However, by
Lemma 23 and the assumption that kf * - f k ≤ 忐,g`* ([p2,1]) must not intersect with ∂B ∩ O.
This completes the proof of Lemma 21.
12A vertex denotes one of the points (2, -1), (2, 1), (-2, -1), (-2, 1).
25