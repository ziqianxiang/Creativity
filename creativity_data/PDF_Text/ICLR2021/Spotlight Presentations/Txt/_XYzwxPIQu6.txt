Published as a conference paper at ICLR 2021
Identifying nonlinear dynamical systems
with multiple time scales and long-range
DEPENDENCIES
Dominik Schmidt1*, Georgia Koppe1,2*, Zahra Monfared1, Max Beutelspacher1,
Daniel DursteWitz1,31
Ab stract
A main theoretical interest in biology and physics is to identify the nonlinear dy-
namical system (DS) that generated observed time series. Recurrent Neural Net-
works (RNNs) are, in principle, powerful enough to approximate any underlying
DS, but in their vanilla form suffer from the exploding vs. vanishing gradients
problem. Previous attempts to alleviate this problem resulted either in more com-
plicated, mathematically less tractable RNN architectures, or strongly limited the
dynamical expressiveness of the RNN. Here we address this issue by suggesting
a simple regularization scheme for vanilla RNNs with ReLU activation which en-
ables them to solve long-range dependency problems and express slow time scales,
while retaining a simple mathematical structure which makes their DS properties
partly analytically accessible. We prove two theorems that establish a tight con-
nection between the regularized RNN dynamics and its gradients, illustrate on DS
benchmarks that our regularization approach strongly eases the reconstruction of
DS which harbor widely differing time scales, and show that our method is also
en par with other long-range architectures like LSTMs on several tasks.
1	Introduction
Theories in the natural sciences are often formulated in terms of sets of stochastic differential or dif-
ference equations, i.e. as stochastic dynamical systems (DS). Such systems exhibit a range of com-
mon phenomena, like (limit) cycles, chaotic attractors, or specific bifurcations, which are the subject
of nonlinear dynamical systems theory (DST; Strogatz (2015); Ott (2002)). A long-standing desire
is to retrieve the generating dynamical equations directly from observed time series data (Kantz
& Schreiber, 2004), and thus to ‘automatize’ the laborious process of scientific theory building to
some degree. A variety of machine and deep learning methodologies toward this goal have been
introduced in recent years (Chen et al., 2017; Champion et al., 2019; Ayed et al., 2019; Koppe et al.,
2019; Hamilton et al., 2017; Razaghi & Paninski, 2019; Hernandez et al., 2020). Often these are
based on sufficiently expressive series expansions for approximating the unknown system of genera-
tive equations, such as polynomial basis expansions (Brunton et al., 2016; Champion et al., 2019) or
recurrent neural networks (RNNs) (Vlachas et al., 2018; Hernandez et al., 2020; Durstewitz, 2017;
Koppe et al., 2019). Formally, RNNs are (usually discrete-time) nonlinear DS that are dynamically
universal in the sense that they can approximate to arbitrary precision the flow field of any other DS
on compact sets of the real space (Funahashi & Nakamura, 1993; Kimura & Nakano, 1998; Hanson
& Raginsky, 2020). Hence, RNNs Seem like a good choice for reconstructing - in this sense of
dynamically equivalent behavior - the set of governing equations underlying real time series data.
However, RNNs in their vanilla form suffer from the ‘vanishing or exploding gradients’ problem
(Hochreiter & Schmidhuber, 1997; Bengio et al., 1994): During training, error gradients tend to
either exponentially explode or decay away across successive time steps, and hence vanilla RNNs
face severe problems in capturing long time scales or long-range dependencies in the data. Specially
designed RNN architectures equipped with gating mechanisms and linear memory cells have been
proposed for mitigating this issue (Hochreiter & Schmidhuber, 1997; Cho et al., 2014). However,
from a DST perspective, simpler models that can be more easily analyzed and interpreted in DS
1Department of Theoretical Neuroscience, 2 3Clinic for Psychiatry and Psychotherapy,
Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University
3Faculty of Physics and Astronomy, Heidelberg University & Bernstein Center Computational Neuroscience
* These authors contributed equally
,Corresponding author: daniel.durstewitz@zi-mannheim.de
1
Published as a conference paper at ICLR 2021
terms (Monfared & Durstewitz, 2020a;b), and for which more efficient inference algorithms exist
that emphasize approximation of the true underlying DS (Koppe et al., 2019; Hernandez et al., 2020;
Zhao & Park, 2020), would be preferable. More recent solutions to the vanishing vs. exploding
gradient problem attempt to retain the simplicity of vanilla RNNs by initializing or constraining the
recurrent weight matrix to be the identity (Le et al., 2015), orthogonal (Henaff et al., 2016; Helfrich
et al., 2018) or unitary (Arjovsky et al., 2016). While merely initialization-based solutions, however,
may be unstable and quickly dissolve during training, orthogonal or unitary constraints, on the other
hand, are too restrictive for reconstructing DS, and more generally from a computational perspective
as well (Kerg et al., 2019): For instance, neither chaotic behavior (that requires diverging directions)
nor multi-stability, that is the coexistence of several distinct attractors, are possible.
Here we therefore suggest a different solution to the problem which takes inspiration from computa-
tional neuroscience: Supported by experimental evidence (Daie et al., 2015; Brody et al., 2003), line
or plane attractors have been suggested as a dynamical mechanism for maintaining arbitrary infor-
mation in working memory (Seung, 1996; Machens et al., 2005), a goal-related active form of short-
term memory. A line or plane attractor is a continuous set of marginally stable fixed points to which
the system’s state converges from some neighborhood, while along the line itself there is neither con-
nor divergence (Fig. 1A). Hence, a line attractor will perform a perfect integration of inputs and re-
tain updated states indefinitely, while a slightly detuned line attractor will equip the system with arbi-
trarily slow time constants (Fig. 1B). This latter configuration has been suggested as a dynamical ba-
sis for neural interval timing (Durstewitz, 2003; 2004). The present idea is to exploit this dynamical
setup for long short-term memory and arbitrary slow time scales by forcing part of the RNN’s sub-
space toward a plane (line) attractor configuration through specifically designed regularization terms.
Specifically, our goal here is not so much to beat the state of the art on long short-term memory tasks,
but rather to address the exploding vs. vanishing gradient problem within a simple, dynamically
tractable RNN, optimized for DS reconstruction and interpretation. For this we build on piecewise-
linear RNNs (PLRNNs) (Koppe et al., 2019; Monfared & Durstewitz, 2020b) which employ ReLU
activation functions. PLRNNs have a simple mathematical structure (see eq. 1) which makes them
dynamically interpretable in the sense that many geometric properties of the system’s state space
can in principle be computed analytically, including fixed points, cycles, and their stability (Suppl.
6.1.2; Koppe et al. (2019); Monfared & Durstewitz (2020a)), i.e. do not require numerical techniques
(Sussillo & Barak, 2013). Moreover, PLRNNs constitute a type of piecewise linear (PWL) map
for which many important bifurcations have been comparatively well characterized (Monfared &
Durstewitz, 2020a; Avrutin et al., 2019). PLRNNs can furthermore be translated into equivalent
continuous time ordinary differential equation (ODE) systems (Monfared & Durstewitz, 2020b)
which comes with further advantages for analysis, e.g. continuous flow fields (Fig. 1A,B).
We retain the PLRNN’s structural simplicity and analytical tractability while mitigating the explod-
ing vs. vanishing gradient problem by adding special regularization terms for a subset of PLRNN
units to the loss function. These terms are designed to push the system toward line attractor config-
Urations, without strictly enforcing them, along some - but not all - directions in state space. We
further establish a tight mathematical relationship between the PLRNN dynamics and the behav-
ior of its gradients during training. Finally, we demonstrate that our approach outperforms LSTM
and other, initialization-based, methods on a number of ‘classical’ machine learning benchmarks
(Hochreiter & Schmidhuber, 1997). Much more importantly in the present DST context, we demon-
strate that our new regularization-supported inference efficiently captures all relevant time scales
when reconstructing challenging nonlinear DS with multiple short- and long-range phenomena.
2	Related work
Dynamical systems reconstruction. From a natural science perspective, the goal of reconstructing or
identifying the underlying DS is substantially more ambitious than (and different from) building a
system that ‘merely’ yields good ahead predictions: In DS identification we require that the inferred
model can freely reproduce (when no longer guided by the data) the underlying attractor geometries
and state space properties (see section 3.5, Fig. S2; Kantz & Schreiber (2004)).
Earlier work using RNNs for DS reconstruction (Roweis & Ghahramani, 2002; Yu et al., 2005)
mainly focused on inferring the posterior over latent trajectories Z = {z1, . . . , zT} given time se-
ries data X = {x1, . . . , xT}, p(Z|X), and on ahead predictions (Lu et al., 2017), as does much of
2
Published as a conference paper at ICLR 2021
C
B
inputs
add
s1∈[0,1]
idx
s2∈{0,1}
Figure 1: A)-B): Illustration of the state space of a 2-unit RNN with flow field (grey) and nullclines
(set of points at which the flow of one of the variables vanishes, in blue and red). Insets: Time
graphs of z1 for T = 30 000. A) Perfect line attractor. The flow converges to the line attractor, thus
retaining states indefinitely in the absence of perturbations, as illustrated for 3 example trajectories
(green). B) Slightly detuned line attractor. The system’s state still converges toward the ”attractor
ghost”, but then very slowly crawls up within the ‘attractor tunnel’ (green trajectory) until it hits
the stable fixed point at the intersection of nullclines. Within the tunnel, flow velocity is smoothly
regulated by the gap between nullclines, thus enabling arbitrary time constants. C) Simple 2-unit
solution to the addition problem exploiting the line attractor properties of ReLUs. The output unit
serves as a perfect integrator (see Suppl. 6.1.1 for complete parameters).
the recent work on variational inference of DS (Duncker et al., 2019; Zhao & Park, 2020; Hernan-
dez et al., 2020). Although this enables insight into the dynamics along the empirically observed
trajectories, both - posterior inference and good ahead predictions - do not per se guarantee that the
inferred models can generate the underlying attractor geometries on their own (see Fig. S2, Koppe
et al. (2019)). In contrast, if fully generative reconstruction of the underlying DS in this latter sense
were achieved, formal analysis or simulation of the resulting RNN equations could provide a much
deeper understanding of the dynamical mechanisms underlying empirical observations (Fig. 1 C).
Some approaches geared toward this latter goal of full DS reconstruction make specific structural
assumptions about the form of the DS equations (‘white box approach’; Meeds et al. (2019); Raissi
(2018); Gorbach et al. (2017)), e.g. based on physical or biological domain knowledge, and focus
on estimating the system’s latent states and parameters, rather than approximating an unknown DS
based on the observed time series information alone (‘black box approach’). Others (Trischler &
D’Eleuterio, 2016; Brunton et al., 2016; Champion et al., 2019) attempt to approximate the flow
field, obtained e.g. by numerical differentiation, directly through basis expansions or neural net-
works. However, numerical derivatives are problematic for their high variance and other numerical
issues (Raissi, 2018; Baydin et al., 2018; Chen et al., 2017). Another factor to consider is that in
many biological systems like the brain the intrinsic dynamics are highly stochastic with many noise
sources, like probabilistic synaptic release (Stevens, 2003). Models that do not explicitly account for
dynamical process noise (Ayed et al., 2019; Champion et al., 2019; Rudy et al., 2019) are therefore
less suited and more vulnerable to model misspecification. Finally, some fully probabilistic models
for DS reconstruction based on GRU (Fraccaro et al., 2016), LSTM (Zheng et al., 2017; Vlachas
et al., 2018), or radial basis function (Zhao & Park, 2020) networks, are not easily interpretable and
amenable to DS analysis in the sense defined in sect. 3.3. Most importantly, none of these previous
approaches consider the long-range dependency problem within more easily tractable RNNs for DS.
Long-range dependency problems in RNNs. Error gradients in vanilla RNNs tend to either explode
or vanish due to the large product of derivative terms that results from recursive application of the
chain rule over time steps (Hochreiter, 1991; Bengio et al., 1994; Hochreiter & Schmidhuber, 1997).
To address this issue, RNNs with gated memory cells (Hochreiter & Schmidhuber, 1997; Cho et al.,
2014) have been specifically designed, but their more complicated mathematical structure makes
them less amenable to a systematic DS analysis. Even simple objects like fixed points of these sys-
tems have to be found by numerical techniques (Sussillo & Barak, 2013; Jordan et al., 2019). Thus,
approaches which retain the simplicity of vanilla RNNs while solving the exploding vs. vanishing
gradients problem would be desirable. Recently, Le et al. (2015) observed that initialization of the
recurrent weight matrix W to the identity in ReLU-based RNNs may yield performance en par with
LSTMs on standard machine learning benchmarks. Talathi & Vartak (2016) expanded on this idea
by initializing the recurrence matrix such that its largest absolute eigenvalue is 1. Later work en-
3
Published as a conference paper at ICLR 2021
forced orthogonal (Henaff et al., 2016; Helfrich et al., 2018; Jing et al., 2019) or unitary (Arjovsky
et al., 2016) constraints on the recurrent weight matrix during training. While this appears to yield
long-term memory performance sometimes superior to that of LSTMs (but see (Henaff et al., 2016)),
these networks are limited in their computational power (Kerg et al., 2019). This may be a conse-
quence of the fact that RNNs with orthogonal recurrence matrix are quite restricted in the range
of dynamical phenomena they can produce, e.g. chaotic attractors are not possible since (locally)
diverging eigen-directions are disabled.
Our approach therefore is to establish line/plane attractors only along some but not all directions
in state space, and to only push the RNN toward these configurations but not strictly enforce them,
such that convergence or (local) divergence of RNN dynamics is still possible. We furthermore im-
plement these concepts through regularization terms in the loss functions, rather than through mere
initialization. This way plane attractors are encouraged throughout training without fading away.
3	Model formulation and theoretical analysis
3.1	Basic model formulation
Assume we are given two multivariate time series S = {st } and X = {xt }, one we will denote as
‘inputs’ (S) and the other as ‘outputs’ (X). In the ‘classical’ (supervised) machine learning setting,
we usually wish to map S on X through a RNN with latent state equation zt = Fθ (zt-1, st)
and outputs Xt 〜pλ (xt∣zt), as for instance in the 'addition problem' (Hochreiter & Schmidhuber,
1997). In DS reconstruction, in contrast, we usually have a dense time series X from which we wish
to infer (unsupervised) the underlying DS, where S may provide an additional forcing function or
sparse experimental inputs or perturbations. While our focus in this paper is on this latter task, DS
reconstruction, we will demonstrate that our approach brings benefits in both these settings.
Here we consider for the latent model a PLRNN (Koppe et al., 2019) which takes the form
Zt = Azt-1 + Wφ(zt-ι) + Cst + h + εt, εt 〜N(0, Σ),	(1)
where zt ∈ RM×1 is the hidden state (column) vector of dimension M, A ∈ RM×M a diagonal and
W ∈ RM ×M an off-diagonal matrix, st ∈ RK ×1 the external input of dimension K, C ∈ RM ×K
the input mapping, h ∈ RM ×1 a bias, and εt a Gaussian noise term with diagonal covariance matrix
diag(Σ) ∈ R+M. The nonlinearity φ(z) is a ReLU, φ(z)i = max(0, zi), i ∈ {1, . . . , M}. This
specific formulation represents a discrete-time version of firing rate (population) models as used in
computational neuroscience (Song et al., 2016; Durstewitz, 2017; Engelken et al., 2020).
We will assume that the latent RNN states zt are coupled to the actual observations xt through a
simple observation model of the form
Xt = Bg(Zt) + ηt, ηt 〜N(0, Γ)	⑵
in the case of observations xt ∈ RN×1, where B ∈ RN×M is a factor loading matrix, g some (usu-
ally monotonic) nonlinear transfer function (e.g., ReLU), and diag(Γ) ∈ R+N the diagonal covari-
ance matrix of the Gaussian observation noise, or through a softmax function in case of categorical
observations xi,t ∈ {0, 1} (see Suppl. 6.1.7 for details).
3.2	Regularization approach
First note that by letting A = I, W = 0, and h = 0 in eq. 1, every point in Z space will be a
marginally stable fixed point of the system, leading it to perform a perfect integration of external
inputs as in parametric working memory (Machens et al., 2005; Brody et al., 2003).1 This is similar
in spirit to Le et al. (2015) who initialized RNN parameters such that it performs an identity mapping
for zi,t ≥ 0. However, here 1) we use a neuroscientifically motivated network architecture (eq. 1)
that enables the identity mapping across the variables’ entire support, zi,t ∈ [-∞, +∞], which we
conjecture will be of advantage for establishing long short-term memory properties, 2) we encourage
1 Note that this very property of marginal stability required for input integration also makes the system
sensitive to noise perturbations directly on the manifold attractor. Interestingly, this property has indeed been
observed experimentally for real neural integrator systems (Major et al., 2004; Mizumori & Williams, 1993).
4
Published as a conference paper at ICLR 2021
this mapping only for a subset Mreg ≤ M of units (Fig. S1), leaving others free to perform arbitrary
computations, and 3) we stabilize this configuration throughout training by introducing a specific
L2 regularization for parameters A, W , and h in eq. 1. When embedded into a larger, (locally)
convergent system, we will call this configuration more generally a manifold attractor.
That way, we divide the units into two types, where the regularized units serve as a memory that
tends to decay very slowly (depending on the size of the regularization term), while the remaining
units maintain the flexibility to approximate any underlying DS, yet retaining the simplicity of the
original PLRNN (eq. 1). Specifically, the following penalty is added to the loss function (Fig. S1):
Mreg	Mreg M	Mreg
Lreg=τAX(Ai,i-1)2+τW	XXWi2,j+τhXhi2	(3)
i=1	i=1 j=1	i=1
j6=i
(Recall from sect. 3.1 that A is a diagonal and W is an off-diagonal matrix.) While this formulation
allows us to trade off, for instance, the tendency toward a manifold attractor (A → I, h → 0) vs.
the sensitivity to other units’ inputs (W → 0), for all experiments performed here a common value,
τA = τW = τh = τ, was assumed for the three regularization factors. We will refer to (z1 . . . zMreg)
as the regularized (‘memory’) subsystem, and to (zMreg+1 . . . zM) as the non-regularized (‘compu-
tational’) subsystem. Note that in the limit τ → ∞ exact manifold attractors would be enforced.
3.3	Theoretical analysis
We will now establish a tight connection between the PLRNN dynamics and its error gradients.
Similar ideas appeared in Chang et al. (2019), but these authors focused only on fixed point dynam-
ics, while here we will consider the more general case including cycles of any order. First, note that
by interpretability of model eq. 1 we mean that it is easily amenable to a rigorous DS analysis: As
shown in Suppl. 6.1.2, we can explicitly determine all the system’s fixed points and cycles and their
stability. Moreover, as shown in Monfared & Durstewitz (2020b), We can - under certain conditions
-transform the PLRNN into an equivalent continuous-time (ODE) piecewise-linear system, which
brings further advantages for DS analysis.
Let us rewrite eq. 1 in the form
Zt = F (Zt-I) = (A + W DΩ(t-1) )zt-1 + h := WΩ(t-1) zt-1 + h,	(4)
where Dn(t-i) is the diagonal matrix of outer derivatives of the ReLU function evaluated at zt-ι
(see Suppl. 6.1.2), and we ignore external inputs and noise terms for now. Starting from some initial
condition Z1, we can recursively develop ZT as (see Suppl. 6.1.2 for more details):
T-1	T-1 j-1
ZT = FTT(ZI) = Y WΩ(T-i) Z1 + X Y WΩ(T-i) + Ih
i=1	j=2 i=1
(5)
Likewise, for some common loss function L(A, W, h) = PtT=2 Lt, we can recursively develop the
derivatives w.r.t. weights wmk (and similar for components of A and h) as
T
∂L	∂Lt ∂zt
∂Wmk	t=2 ∂Zt ∂Wmk
with
∂Zt
∂wmk
1(m,k)DΩ(t-1) zt-1
(6)
t-2 j-1
+X Y
j=2	i=1
WΩ(t-i))
t-2
1(m,k)DΩ(t-j)zt-j + ɪɪ WΩ(t-i)
i=1
∂Z2
∂wmk
where 1(m,k) is an M × M indicator matrix with a 1 for the (m, k)’th entry and 0 everywhere else.
Observing that eqs. 5 and 6 contain similar product terms which determine the system’s long-term
behavior, our first theorem links the PLRNN dynamics to its total error gradients:
Theorem 1.	Consider a PLRNN given by eq. 4, and assume that it converges to a stable fixed point,
say Zt*ι := z*1, or a k-cycle (k > 1) with the periodic points {zt*k, Zt*k-ι,∙∙∙ , Zt*k-(k-i)} ,for
T → ∞. Suppose that,for k ≥ 1 and i ∈ {0,1,…，k - 1}, σmaχ(WΩ(t*k-i)) = ∣∣WΩ(t*J) ∣∣ <
1, where WΩ(t*k-i) denotes the Jacobian of the System at zt*k-i and σmaχ indicates the largest
5
Published as a conference paper at ICLR 2021
singular value of a matrix. Then, the 2-norms of the tensors collecting all derivatives, ∣∣ ∂WT ∣∣2,
Il dAT ∣∣2, ∣∣ dzhT∣∣2, Willbeboundedfromabove, i.e. Willnotdivergefor T → ∞.
Proof. See Suppl. sect. 6.1 (subsection 6.1.3).
While Theorem 1 is a general statement about PLRNN dynamics and total gradients, our next the-
orem more specifically provides conditions under which Jacobians linking temporally distant states
zT and zt , T t, will neither vanish nor explode in the regularized PLRNN:
Theorem 2.	Assume a PLRNN With matrix A + W partitioned as in Fig. S1, i.e. With the first
Mreg roWs corresponding to those of an M × M identity matrix. Suppose that the non-regularized
subsystem (zMreg +1 . . . zM), if considered in isolation, satisfies Theorem 1, i.e. converges to a k-
cycle With k ≥ 1. Then, for the full system (z1 . . . zM), the 2-norm of the Jacobians connecting
temporally distal states zT and zt Will be bounded from above and beloW for all T > t, i.e. ∞ >
Pup ≥ ∣∣ dZT∣∣ = ∣∣Qt<k≤τ	Wn(k)∣∣	≥ Plow	>	0.	In particular, for state variables ZiT and
Zjt such that i ∈ {Mreg + 1,	∙ ∙ ∙ , M}	and j ∈	{1,	•…,Mreg}, i.e. that connect states from the
'memory' to those of the 'computational' subsystem, one also has ∞ > λup ≥ ∣ dZiT ∣ ≥ 入房位 > 0
as T - t → ∞, i.e. these derivatives Will never vanish nor explode.
Proof. See Suppl. sect. 6.1 (subsection 6.1.4).
The bounds Pup, Plow, λup, λlow, are given in Suppl. sect. 6.1.4. We remark that when the regular-
ization conditions are not exactly met, i.e. when parameters A and W slightly deviate from those in
Fig. S1, memory (and gradients) may ultimately dissipate, but only very slowly, as actually required
for temporal processes with very slow yet not infinite time constants (Fig. 1B).
3.4	Training procedures
For the (supervised) machine learning problems, all networks were trained by stochastic gradient
descent (SGD) to minimize the squared-error loss between estimated and actual outputs for the
addition and multiplication problems, and the cross entropy loss for sequential MNIST (see Suppl.
6.1.7). Adam (Kingma & Ba, 2014) from PyTorch package (Paszke et al., 2017) was used as the
optimizer, with a learning rate of 0.001, gradient clip parameter of 10, and batch size of 500. SGD
was stopped after 100 epochs and the fit with the lowest loss across all epochs was taken, except
for LSTM which was allowed to run for up to 200 epochs as it took longer to converge (Fig. S10).
For comparability, the PLRNN latent state dynamics eq. 1 was assumed to be deterministic in this
setting (i.e., Σ = 0), g(zt) = zt and Γ = IN in eq. 2. For the regularized PLRNN (rPLRNN),
penalty eq. 3 was added to the loss function. For the (unsupervised) DS reconstruction problems,
the fully probabilistic, generative RNN eq. 1 was considered. Together with eq. 2 (where we take
g(zt) = φ(zt)) this gives the typical form of a nonlinear state space model (Durbin & Koopman,
2012) with observation and process noise, and an Expectation-Maximization (EM) algorithm that
efficiently exploits the model’s piecewise linear structure (Durstewitz, 2017; Koppe et al., 2019) was
used to solve for the parameters by maximum likelihood. Details are given in Suppl. 6.1.5. All code
used here will be made openly available at https://github.com/DurstewitzLab/reg-PLRNN.
3.5	Performance measures
For the machine learning benchmarks we employed the same criteria as used for optimization (MSE
or cross-entropy, Suppl. 6.1.7) as performance metrics, evaluated across left-out test sets. In ad-
dition, we report the relative frequency Pcorrect of correctly predicted trials across the test set (see
Suppl. 6.1.7 for details). For DS reconstruction problems, itis not sufficient or even sensible to judge
a method’s ability to infer the underlying DS purely based on some form of (ahead-)prediction error
like the MSE defined on the time series itself (Ch.12 in Kantz & Schreiber (2004)). Rather, we
require that the inferred model can freely reproduce (when no longer guided by the data) the un-
derlying attractor geometries and state space properties. This is not automatically guaranteed for a
model that yields agreeable ahead predictions on a time series (Fig. S2A; cf. Koppe et al. (2019);
Wood (2010)). We therefore followed Koppe et al. (2019) and used the Kullback-Leibler divergence
between true and reproduced probability distributions across states in state space to quantify how
well an inferred PLRNN captured the underlying dynamics, thus assessing the agreement in attractor
geometries (cf. Takens (1981); Sauer et al. (1991)) (see Suppl. 6.1.6 for more details).
6
Published as a conference paper at ICLR 2021
Addition problem
Multiplication problem
—J— RNN	—5— πpRNN (Talathi & Vartak3 2016)
——J——iRNN (Le et aL, 2015)	——⅞——LSTM (Hochreiter & Schmidhuber5 1997) ——⅞——
——S——oRNN (Vorontsov et aL, 2017)——S——PLRNN (Koppe et al.3 2019)	——⅞——
Sequential MNIST
iPLRNN
L2pPLRNN
rPLRNN
ι
0.8
0.6
0.4
0.2
20 30 50 70	150 250400
T
1
0.8
0.6
0.4
0.2
20 30 50 70	150 250 400	900
T
0.8
g 0.6
Q≡ 0.4
0.2
Figure 2: Comparison of rPLRNN (τ = 5, MMreg = 0.5, cf. Fig. S3) to other methods for A)
addition problem, B) multiplication problem and C) sequential MNIST. Top row gives loss as a
function of time series length T (error bars = SEM, n ≥ 5), bottom row shows relative frequency of
correct trials. Note that better performance (lower values in top row, higher values in bottom row)
is reflected in a more rightward shift of curves. Dashed lines indicate chance level, black dots in C
indicate individual repetitions.
Figure 3: Reconstruction of a 2-time scale DS in limit cycle regime. A) KL divergence (DKL)
between true and generated state space distributions. Globally diverging system estimates were re-
moved. B) Average MSE between power spectra of true and reconstructed DS and C) split according
to low (≤ 50 Hz) and high (> 50 Hz) frequency components. Error bars = SEM (n = 33). D) Ex-
ample of (best) generated time series (red=reconstruction with T = 2). See Fig. S5A for variable n.
E) Dynamics of regularized and non-regularized latent states for the example in D.

2
4	Numerical experiments
4.1	Machine learning benchmarks
Although not our prime interest here, we first examined how the rPLRNN would fare on supervised
machine learning benchmarks where inputs (S) are to be mapped onto target outputs (X) across long
7
Published as a conference paper at ICLR 2021
time spans (i.e., requiring long short-term maintenance of information), namely the addition and
multiplication problems (Talathi & Vartak, 2016; Hochreiter & Schmidhuber, 1997), and sequential
MNIST (LeCun et al., 2010). Details of these experimental setups are in Suppl. 6.1.7. Performance
of the rPLRNN (eq. 1, eq. 3) on all 3 benchmarks was compared to several other models summarized
in Suppl. Table 1. To achieve a meaningful comparison, all models have the same number M = 40
(based on Fig. S3) of hidden states (which gives LSTMs overall about 4 times as many trainable
parameters). On all three problems the rPLRNN outperforms all other tested methods, including
LSTM, iRNN (RNN initialized by the identity matrix as in Le et al. (2015)), and a version of the
orthogonal RNN (oRNN; Vorontsov et al. (2017)) (similar results were obtained for other settings of
M and batch size). LSTM performs even worse than iRNN and iPLRNN (PLRNN initialized with
the identity as the iRNN), although it had 4 times as many parameters and was given twice as many
epochs (and thus opportunities) for training, as it also took longer to converge (Fig. S10). In addition,
the iPLRNN tends to perform slightly better than the iRNN on all three problems, suggesting that the
specific structure eq. 1 of the PLRNN that allows for a manifold attractor across the variables’ full
range may be advantageous to begin with, while the regularization further improves performance.
4.2	Numerical experiments on dynamical systems with different time scales
While it is encouraging that the rPLRNN may perform even better than several previous approaches
to the vanishing vs. exploding gradients problem, our major goal here was to examine whether our
regularization scheme would help with the (unsupervised) identification of DS that harbor widely
different time scales. To test this, we used a biophysical, bursting cortical neuron model with one
voltage (V ) and two conductance recovery variables (see Durstewitz (2009)), one slow (h) and one
fast (n; Suppl. 6.1.8). Reproduction of this DS is challenging since it produces very fast spikes on
top of a slow nonlinear oscillation (Fig. 3D). Only short time series (as in scientific data) of length
T = 1500 from this model were provided for training. rPLRNNs with M = {8 . . . 18} states were
trained, with the regularization factor varied within τ ∈ {0, 101 , 102, 103, 104, 105}/T. Note that
for τ = 0 (no regularization), the approach reduces to the standard PLRNN (Koppe et al., 2019).
Fig. 3A confirms our intuition that stronger regularization leads to better DS reconstruction as
assessed by the KL divergence between true and generated state distributions (similar results were
obtained with ahead-prediction errors as a metric, Fig. S4A), accompanied by a likewise decrease
in the MSE between the power spectra of true (suppl. eq. 55) and generated (rPLRNN) voltage
traces (Fig. 3B). Fig. 3D gives an example of voltage traces (V ) and the slower of the two gating
variables (h; see Fig. S5A for variable n) freely simulated (i.e., sampled) from the autonomously
running rPLRNN. This illustrates that our model is in principle capable of capturing both the stiff
spike dynamics and the slower oscillations in the second gating variable at the same time. Fig. 3C
provides more insight into how the regularization worked: While the high frequency components
(> 50 Hz) related to the repetitive spiking activity hardly benefited from increasing τ , there was
a strong reduction in the MSE computed on the power spectrum for the lower frequency range
(≤ 50 Hz), suggesting that increased regularization helps to map slowly evolving components of the
dynamics. This result is more general as shown in Fig. S6 for another DS example. In contrast, an
orthogonality (Vorontsov et al., 2017) or plain L2 constraint on weight matrices did not help at all
on this problem (Fig. S4B).
Further insight into the dynamical mechanisms by which the rPLRNN solves the problem can be
obtained by examining the latent dynamics: As shown in Fig. 3E (see also Fig. S5), regularized
states indeed help to map the slow components of the dynamics, while non-regularized states focus
on the fast spikes. These observations further corroborate the findings in Fig. 3C and Fig. S6C.
4.3	Regularization properties and manifold attractors
In Figs. 2 and 3 we demonstrated that the rPLRNN is able to solve problems and reconstruct dy-
namics that involve long-range dependencies. Figs. 3A,B furthermore directly confirm that solutions
improve with stronger regularization, while Figs. 3C,E give insight into the mechanism by which
the regularization works. To further verify empirically that our specific form of regularization, eq.
3, is important, Fig. 2 also shows results for a PLRNN with standard L2 norm on a fraction of
Mreg/M = 0.5 states (L2pPLRNN). Fig. S7 provides additional results for PLRNNs with L2 norm
on all weights and for vanilla L2-regularized RNNs. All these systems fell far behind the perfor-
mance of the rPLRNN on all tasks tested. Moreover, Fig. 4 reveals that the specific regularization
8
Published as a conference paper at ICLR 2021
Figure 4: A) Distribution of maximum absolute eigenvalues λ of Jacobians around fixed points for
rPLRNN for different τ and L2PLRNN trained on bursting neuron DS. B) Absolute deviations of
max. ∣λ∣ from 1 (using for each system the one eigenvalue with smallest deviation). C) Same as
A for addition problem for rPLRNN (τ = 5) vs. standard, fully L2- (L2f), and partially L2 (L2p)-
regularized PLRNN. D) Same as B for the models from C. Error bars = stdv. See also Fig. S8.
proposed indeed encourages manifold attractors, and that this is not achieved by a standard L2 reg-
ularization: In contrast to L2PLRNN, as the regularization factor τ is increased, more and more of
the maximum absolute eigenvalues around the system’s fixed points (computed according to eq. 8,
sect. 6.1.2) cluster on or near 1, indicating directions of marginal stability in state space. Also, the
deviations from 1 become smaller for strongly regularized PLRNNs (Fig. 4B,D), indicating a higher
precision in attractor tuning. Fig. S9 in addition confirms that rPLRNN parameters are increasingly
driven toward values that would support manifold attractors with stronger regularization. Fig. 3E
furthermore suggests that both regularized and non-regularized states are utilized to map the full
dynamics. But how should the ratio Mreg /M be chosen in practice? While for the problems here
this meta-parameter was determined through ‘classical’ grid-search and cross-validation, Figs. S3
C - E suggest that the precise setting of Mreg/M is actually not overly important: Nearly optimal
performance is achieved for a broader range Mreg/M ∈ [0.3, 0.6] on all problems tested. Hence, in
practice, setting Mreg/M = 0.5 should mostly work fine.
5	Conclusions
In this work we introduced a simple solution to the long short-term memory problem in RNNs that
retains the simplicity and tractability of PLRNNs, yet does not curtail their universal computational
capabilities (Koiran et al., 1994; Siegelmann & Sontag, 1995) and their ability to approximate arbi-
trary DS (Funahashi & Nakamura, 1993; Kimura & Nakano, 1998; Trischler & D’Eleuterio, 2016).
We achieved this by adding regularization terms to the loss function that encourage the system to
form a ‘memory subspace’ (Seung, 1996; Durstewitz, 2003) which would store arbitrary values for,
if unperturbed, arbitrarily long periods. At the same time we did not rigorously enforce this con-
straint, which allowed the system to capture slow time scales by slightly departing from a perfect
manifold attractor. In neuroscience, this has been discussed as a dynamical mechanism for regu-
lating the speed of flow in DS and learning of arbitrary time constants not naturally included qua
RNN design (Durstewitz, 2003; 2004) (Fig. 1B). While other RNN architectures, including vanilla
RNNs, can, in principle, also develop line attractors to solve specific tasks (Maheswaranathan et al.,
2019), they are generally much harder to train to achieve this and may exhibit less precise attractor
tuning (cf. Fig. 4), which is needed to bridge long time scales (Durstewitz, 2003). Moreover, part of
the PLRNN’s latent space was not regularized at all, leaving the system enough degrees of freedom
for realizing arbitrary computations or dynamics (see also Fig. S11 for a chaotic example). We
showed that the rPLRNN is en par with or outperforms initialization-based approaches, orthogonal
RNNs, and LSTMs on a number of classical benchmarks. More importantly, however, the regular-
ization strongly facilitates the identification of challenging DS with widely different time scales in
PLRNN-based algorithms for DS reconstruction. Similar regularization schemes as proposed here
(eq. 3) may, in principle, also be designed for other architectures, but the convenient mathematical
form of the PLRNN makes their implementation particularly powerful and straightforward.
Acknowledgements
This work was funded by grants from the German Research Foundation (DFG) to DD (Du 354/10-1,
Du 354/8-2 within SPP 1665) and to GK (TRR265: A06 & B08), and under Germany’s Excellence
Strategy - EXC-2181 - 390900948 (,Structures,).
9
Published as a conference paper at ICLR 2021
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
Proceedings of the 33rd International Conference on Machine Learning, 2016.
Viktor Avrutin, Laura Gardini, Iryna Sushko, and Fabio Tramontana. Continuous and Discontinuous
Piecewise-Smooth One-Dimensional Maps. WORLD SCIENTIFIC, 2019.
Ibrahim Ayed, Emmanuel de Bezenac, Arthur Pajot, Julien Brajard, and Patrick Gallinari. Learning
Dynamical Systems from Partial Observations. arXiv preprint, 2019.
Atilim GuneS Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research,
18:1-43,2018.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.
Carlos D Brody, Ranulfo Romo, and Adam Kepecs. Basic mechanisms for graded persistent activ-
ity: discrete attractors, continuous attractors, and dynamic representations. Current Opinion in
Neurobiology, 13(2):204-211, 2003.
Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations
from data by sparse identification of nonlinear dynamical systems. Proceedings of the National
Academy of Sciences of the United States of America, 113(15):3932-3937, 2016.
Kathleen Champion, Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Data-driven discovery
of coordinates and governing equations. arXiv preprint, 2019.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. AntisymmetricRNN: A Dynamical System
View on Recurrent Neural Networks. arXiv preprint, 2019.
Shizhe Chen, Ali Shojaie, and Daniela M. Witten. Network Reconstruction From High-Dimensional
Ordinary Differential Equations. Journal of the American Statistical Association, 112(520):1697-
1707, 2017.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing, pp. 1724-1734. Association for Computational Linguistics (ACL),
2014.
Kayvon Daie, Mark S. Goldman, and Emre R.F. Aksay. Spatial patterns of persistent neural activity
vary with the behavioral context of short-term memory. Neuron, 85(4):847-860, 2015.
Lea Duncker, Gergo Bohner, Julien Boussard, and Maneesh Sahani. Learning interpretable
continuous-time models of latent stochastic dynamical systems. arXiv preprint, 2019.
James Durbin and Siem Jan Koopman. Time Series Analysis by State Space Methods. Oxford
University Press, 2012.
Daniel Durstewitz. Self-Organizing Neural Integrator Predicts Interval Times. Journal of Neuro-
science, 23(12):5342-5353, 2003.
Daniel Durstewitz. Neural representation of interval time. NeuroReport, 15(5):745-749, 2004.
Daniel Durstewitz. Implications of synaptic biophysics for recurrent network dynamics and active
memory. Neural Networks, 22(8):1189-1200, 2009.
Daniel Durstewitz. A State Space Approach for Piecewise-Linear Recurrent Neural Networks for
Reconstructing Nonlinear Dynamics from Neural Measurements. PLoS Computational Biology,
13(6):e1005542, 2017.
Rainer Engelken, Fred Wolf, and L. F. Abbott. Lyapunov spectra of chaotic recurrent neural net-
works. arXiv preprint, 2020.
10
Published as a conference paper at ICLR 2021
Ludwig Fahrmeir and Gerhard Tutz. Multivariate Statistical Modelling Based on Generalized Linear
Models. Springer New York, 2001.
Jianqing Fan and Qiwei Yao. Nonlinear Time Series. Springer New York, 2003.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in Neural Information Processing Systems, 2016.
Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous
time recurrent neural networks. Neural Networks, 6(6):801-806,1993.
Nico S. Gorbach, Stefan Bauer, and Joachim M. Buhmann. Scalable variational inference for dy-
namical systems. In Advances in Neural Information Processing Systems, 2017.
Franz Hamilton, Alun L. Lloyd, and Kevin B. Flores. Hybrid modeling and prediction of dynamical
systems. PLOS Computational Biology, 13(7):e1005655, 2017.
Joshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent
neural nets. In Proceedings of Machine Learning Research, volume 120, pp. 384-392, 2020.
Kyle E. Helfrich, Devin Whimott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
Cayley transform. Proceedings of the 35th International Conference on Machine Learning, 2018.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In Proceedings of the 33rd International Conference on Machine Learning, 2016.
Daniel Hernandez, Antonio Khalil Moretti, Ziqiang Wei, Shreya Saxena, John Cunningham, and
Liam Paninski. Nonlinear evolution via spatially-dependent linear dynamics for electrophysiol-
ogy and calcium data. NBDT, 3(3), 2020.
John R. Hershey and Peder A. Olsen. Approximating the Kullback Leibler divergence between
gaussian mixture models. In 2007 IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 4, pp. IV-317-IV-320, 2007.
SePP Hochreiter. Untersuchungen Zu dynamischen neuronalen Netzen. Diploma thesis, Institut fur
Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen, 1991.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-80, 1997.
Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua
Bengio. Gated Orthogonal Recurrent Units: On Learning to Forget. Neural Computation, 31(4):
765-783, 2019.
Ian D. Jordan, Piotr A. Sokol, and Il Memming Park. Gated recurrent units viewed through the lens
of continuous time dynamical systems. arXiv preprint, 2019.
Holger Kantz and Thomas Schreiber. Nonlinear Time Series Analysis. Cambridge University Press,
2. edition, 2004.
Giancarlo Kerg, Kyle Goyette, Maximilian P. Touzel, Gauthier Gidel, Eugene Vorontsov, Yoshua
Bengio, and Guillaume Lajoie. Non-normal Recurrent Neural Network (nnRNN): learning long
time dependencies while improving expressivity with transient dynamics. arXiv preprint, 2019.
M. Kimura and R. Nakano. Learning dynamical systems by recurrent neural networks from orbits.
Neural Networks, 11(9):1589-1599, 1998.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint,
2014.
Pascal Koiran, Michel Cosnard, and Max H. Garzon. Computability with low-dimensional dynami-
cal systems. Theoretical Computer Science, 132:113-128, 1994.
11
Published as a conference paper at ICLR 2021
Georgia Koppe, Hazem Toutounji, Peter Kirsch, Stefanie Lis, and Daniel Durstewitz. Identifying
nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI.
PLOS Computational Biology, 15(8):e1007263, 2019.
Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A Simple Way to Initialize Recurrent Networks
of Rectified Linear Units. arXiv preprint, 2015.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2010.
Zhixin Lu, Jaideep Pathak, Brian Hunt, Michelle Girvan, Roger Brockett, and Edward Ott. Reservoir
observers: Model-free inference of unmeasured variables in chaotic systems. Chaos, 27(4), 2017.
Christian K. Machens, Ranulfo Romo, and Carlos D. Brody. Flexible control of mutual inhibition:
A neural model of two-interval discrimination. Science, 307(5712):1121-1124, 2005. doi: 10.
1126/science.1104171.
Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Re-
verse engineering recurrent networks for sentiment classification reveals line attractor dynamics.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 32, pp. 15696-15705. Curran As-
sociates, Inc., 2019.
G.	Major, R. Baker, E. Aksay, B. Mensh, H. S. Seung, and D. W. Tank. Plasticity and tuning by
visual feedback of the stability of a neural integrator. Proceedings of the National Academy of
Sciences, 101(20):7739-7744, 2004.
P.E. McSharry, G.D. Clifford, L. Tarassenko, and L.A. Smith. A dynamical model for generating
synthetic electrocardiogram signals. IEEE Transactions on Biomedical Engineering, 50(3):289-
294, March 2003.
Ted Meeds, Geoffrey Roeder, Paul Grant, Andrew Phillips, and Neil Dalchau. Efficient amortised
Bayesian inference for hierarchical and nonlinear dynamical systems. In Proceedings of the 36th
International Conference on Machine Learning, 2019.
SJ Mizumori and JD Williams. Directionally selective mnemonic properties of neurons in the lateral
dorsal nucleus of the thalamus of rats. The Journal of Neuroscience, 13(9):4015-4028, 1993.
Zahra Monfared and Daniel Durstewitz. Existence of n-cycles and border-collision bifurcations
in piecewise-linear continuous maps with applications to recurrent neural networks. Nonlinear
Dynamics, 101(2):1037-1052, 2020a.
Zarah Monfared and Daniel Durstewitz. Transformation of ReLU-based recurrent neural networks
from discrete-time to continuous-time. In Proceedings of the 37th International Conference on
Machine Learning, 2020b.
Edward Ott. Chaos in Dynamical Systems. Cambridge University Press, 2002.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop, 2017.
Maziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential equa-
tions. Journal of Machine Learning Research, 19:1-24, 2018.
Hooshmand Shokri Razaghi and Liam Paninski. Filtering normalizing flows. 4th workshop on
Bayesian Deep Learning (NeurIPS 2019), 2019.
Sam Roweis and Zoubin Ghahramani. Learning Nonlinear Dynamical Systems Using the
Expectation-Maximization Algorithm. In Kalman Filtering and Neural Networks, chapter 6, pp.
175-220. Wiley-Blackwell, 2002.
Samuel H. Rudy, Steven L. Brunton, and J. Nathan Kutz. Smoothing and parameter estimation by
soft-adherence to governing equations. Journal of Computational Physics, 398:108860, Decem-
ber 2019.
12
Published as a conference paper at ICLR 2021
Tim Sauer, James A. Yorke, and Martin Casdagli. Embedology. Journal of Statistical Physics, 65
(3-4):579-616,1991.
H.	Sebastian Seung. How the brain keeps the eyes still. Proceedings of the National Academy of
Sciences, 93(23):13339-13344, 1996.
Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. Journal
of Computer and System Sciences, 50(1):132-150, 1995.
H.	Francis Song, Guangyu R. Yang, and Xiao Jing Wang. Training Excitatory-Inhibitory Recurrent
Neural Networks for Cognitive Tasks: A Simple and Flexible Framework. PLoS Computational
Biology, 12(2):1-30, 2016.
Charles F Stevens. Neurotransmitter release at central synapses. Neuron, 40(2):381-388, 2003.
Steven H. Strogatz. Nonlinear Dynamics and Chaos: Applications to Physics, Biology, Chemistry,
and Engineering: With Applications to Physics, Biology, Chemistry and Engineering. CRC Press,
2015.
David Sussillo and Omri Barak. Opening the black box: low-dimensional dynamics in high-
dimensional recurrent neural networks. Neural computation, 25(3):626-49, 2013.
Floris Takens. Detecting strange attractors in turbulence. In Lecture Notes in Mathematics, pp.
366-381. Springer Berlin Heidelberg, 1981.
Sachin S. Talathi and Aniket Vartak. Improving performance of recurrent neural network with ReLU
nonlinearity. ICLR Workshop submission, 2016.
Adam P. Trischler and Gabriele M.T. D’Eleuterio. Synthesis of recurrent neural networks for dy-
namical system simulation. Neural Networks, 80:67-78, 2016.
Pantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumout-
sakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory
networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
474(2213), 2018.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. Proceedings of the 34th International Confer-
ence on Machine Learning Research, 70:3570-3578, 2017.
Simon N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466:
1102-1104, 2010.
Byron M. Yu, Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, and Maneesh
Sahani. Extracting dynamical structure embedded in neural activity. In Advances in Neural
Information Processing Systems 18, pp. 1545-1552. MIT Press, 2005.
Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-Term Forecasting using
Higher-Order Tensor RNNs. arXiv preprint, 2019.
Yuan Zhao and Il Memming Park. Variational online learning of neural dynamics. arXiv preprint,
2020.
Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P. Xing, and Alexander J. Smola. State
Space LSTM Models with Particle MCMC Inference. arXiv preprint, 2017.
13
Published as a conference paper at ICLR 2021
6 Appendix
6.1	Supplementary text
6.1.1	Simple exact PLRNN solution for addition problem
The exact PLRNN parameter settings (cf. eq. 1, eq. 2) for solving the addition problem with 2 units
(cf. Fig. 1C) are as follows:
A= 10 00,W= 00 01 ,h= -01,C= 01 01,B=(1 0)	(7)
6.1.2	Computation of fixed points and cycles in PLRNN
Consider the PLRNN in the form of eq. 4. For clarity, let Us define dΩ(t) ：= (d1,d2,…,dM) as
an indicator vector with dm (zm,t) := dm = 1 for all states zm,t > 0 and zeros otherwise, and
Dn(t) ：= diag(dn(t)) as the diagonal matrix formed from this vector. Note that there are at most
2M distinct matrices WΩ(t) as defined in eq. 4, depending on the sign of the components of zt.
If h = 0 and WΩ(t) is the identity matrix, then the map F becomes the identity map and so every
point z will be a fixed point ofF. Otherwise, the fixed points ofF can be foUnd solving the eqUation
F (z*1) = z*1 as
z*1 = (I — WΩ(t*i))-1 h = H *1 h,	(8)
where z*1 = Zt*ι = Zt*i-1, if det(I - Wc(t*i)) = Pwα(t*ι)(1) = 0, i.e. Wc(t*i) has
no eigenvalUe eqUal to 1. Stability and type of fixed points (node, saddle, spiral) can then be
determined from the eigenvalues of the Jacobian A + WDΩ(t*i) = W^Ω(t*ι) (Strogatz (2015)).
For k > 1, solving Fk(z*k) = z*k, one can obtain a k-cycle of the map F with the periodic points
{z*k, F(z*k), F2(z*k), ∙∙∙ , Fk-1(z*k)}. For this, we first compute Fk as follows:
Zt = F(Zt-I) = WΩ(t-1) zt-1 + h,
Zt+1 = F2(zt-1) = F(Zt) = WΩ(t) WΩ(t-1) Zt-1 + (WΩ(t) + I)h,
zt+2 = F3(Zt-I) = F(Zt+1) = WΩ(t+1) WΩ(t) WΩ(t-1) Zt-1
+ (WΩ(t+1) WΩ(t) + WΩ(t+1) + I)h,
k+1	k k-j+2
Zt+(k-1) = Fk(Zt-1) = Y WΩ(t+(k-i)) zt-1 + X Y WΩ(t+(k-i)) + I h,	(9)
i=2	j=2 i=2
in which Qk=+21 WΩ(t+(k-i)) = WΩ(t+(k-2)) WΩ(t+(k-3))…WΩ(t-1).
Assuming t+ (k - 1) := t*k, then the k-cycle is given by the fixed point of the k-times iterated map
Fk as
k	-1 k k-j+1
z*k = II — YWΩ(t*k-i))	X Y WΩ(t*k-i) + I h = H*kh,	(10)
i=1	j=2 i=1
where Z*k = Zt*k = Zt*k-k, provided that I 一 Qk=1 WΩ(t*k-i) is invertible. That is
det (I - Qk=1 WΩ(t*k-i)) = PQk=IWΩ(t*k-i) (1) = 0 and Qk=1 WΩ(t*k-i) := WΩ*k has no
eigenvalue equal to 1. As for the fixed points, we can determine stability of the k-cycle from the
eigenvalues of the Jacobians Qk=1 WΩ(t*j).
14
Published as a conference paper at ICLR 2021
It may also be helpful to spell out the recursions in eq. 5 and eq. 6 in section 3.3 ina bit more detail.
Analogously to the derivations above, for t = 1,2,...,T We can recursively compute z2, zɜ,..., ZT
(T ∈ N) as
Z2 = F(Z1) = Wω⑴ Z1 + h,
Z3 = F2(z1) = F(Z2)= Wω(2) Wω(1) Z1 + (Wω(2) + I)h,
zT = FTT(ZI) = F(ZT-I) = WΩ(T-I) wω(T-2)…wω(1) z1
+ (wΩ(T-I) wΩ(T-2)…wΩ(2)
+ wΩ(T-I) wΩ(T-2)…wΩ(3) +-------+ wΩ(T-I) + 1)h
T-1	「T-2 T-j-1
=∏ Wω(t-i) Z1 + X Y Wω(t-i) + I h
i=1	Lj=1	i=1	-
T-1	「T-1 j-1	-∣
=∏ wΩ(T-i) z1 + X ∏ wΩ(T-i) + 1 h∙	(II)
i=1	L j=2 i=1	-
LikeWise, We can Write out the derivatives eq. 6 more explicitly as
∂Zt	∂F(zt-1)	,	, Wn ∖∂zt-1
∂- = ʒ---------- = l(m,k)DΩ(t-1)zt-1 + (A + WDΩ(t-1)) ∂--
dwmk	dwmk	dwmk
=1(m,k)DΩ(t-1) zt-1 + (A + WDΩ(t-1)) 1(m,k)DΩ(t-2) zt-2
+ (A + WDΩ(t-I))(A + WDΩ(t-2)) J2
∂Wmk
=1(m,k)DΩ(t-1) zt-1 + (A + WDΩ(t-1)) 1(m,k)DΩ(t-2)zt-2
+ (A + WDΩ(t-I))(A + WDΩ(t-2)) l(rn,k)DΩ(t-3)zt-3
+ (A + WDΩ(t-I))(A + WDΩ(t-2)) (A + WDΩ(t-3))内 t 3
∂Wmk
t-2
1(m,k)DΩ(t-1) zt-1 + E
j=2
(W WΩ(t-i) ) 1(m,k)DΩ(t-j) zt-j
、i=1	)
t-2
+ ∏ WΩ(t-i)
i=1
∂Z2
dwmk
(12)
where 产2 =(俨,2 •…dzM,2) with dz1,2 = 0 V l = m and dzm,2 = dkZk 1. The derivatives
∂Wmk	∂Wmk	∂Wmk	∂Wmk	∂Wmk	k k,1
w.r.t. the elements of A and h can be expanded in a similar way, only that the terms Dn(t)Zt on the
last line of eq. 12 need to be replaced by just Zt for Et , and by just a vector of 1’s for 穿^ (also,
oamm	onm
in these cases, the indicator matrix will be the diagonal matrix 1(m；m)).
6.1.3 Proof of Theorem 1
To state the proof, let us rewrite the derivatives of the loss function L(W, A, h) = PT=I Lt in the
following tensor form:
∂ L
∂W
X dLL
M dW,
where
∂Lt	∂Lt ∂zt
----=----------
∂ W-∂Zt ∂W
(13)
15
Published as a conference paper at ICLR 2021
for which the 3D tensor
/ dzLt ∖
∂W
dz2,t
∂zt	∂W
----=
∂W	.
.
.
dzM,t
∖ ∂W /
(14)
of dimension M × M × M, consists of all the gradient matrices
where Wi*
∂zi,t
∂W
i = 1,2,…，M,	(15)
Now, suppose that {z1, z2, zɜ,...} is an orbit of the system which converges to a stable fixed point,
i.e. lim ZT = z*k. Then
τ →∞
lim zτ = lim (Wq(t-1) ZT-ι + h)
τ→∞	τ→∞ '	(	)	)
z*1 = Wc(t*i)z*1+ h,
(16)
and so
TIimO (WQ(T-a z*1
Wc(t*i)z*1
(17)
Assume that ^lim (Wq(t-i)
= L. Since eq. 17 holds for every z*1, then substituting z*1
eT = (1,0,…，0)τ in eq. 17, we can prove that the first column of L equals the first column of
Wo(t*i). Performing the same procedure for z*1 = eT, i = 2,3, ∙ ∙ ∙ , M, yields
Tlimo wΩ(T-1) = WΩ(t*1).
Also, for every i ∈ N (1 < i < ∞)
∕imo W^(T-i) = wΩ(t*1),
i.e.
∀e > 0 ∃N ∈ N s.t. T — i ≥ N =⇒ ∣∣Wω(t-i) - WQ(t*ι)∣∣ ≤ e.
ThUSjWω(t-i) I I -1 I wΩ(t*1) I I ≤ I I wω(t-i)- wΩ(t*ι)∣∣ gives
∀e> 0 ∃N ∈ N St T - i ≥ N =⇒ ∣∣Wωct-i)∣∣ ≤ ∣∣WΩ(t*i)∣∣+ e.
Since T - 1 > T - 2 > …> T - i ≥ N, so
∀e> 0	∣∣Wωct-i)∣∣ ≤ ∣∣WΩ(t*i)∣∣+ e, i = 1, 2, ∙∙∙ ,T - N.
Hence
∀e > 0
T-N
∏ Wω(t-i)
i=1
T-N
≤ ∏ ∣∣wω(t-i)∣∣ ≤
i=1
(∣∣wΩ(t*i)∣∣ + e)
(18)
(19)
(20)
(21)
(22)
(23)
If ∣∣ Wc(t*i) ∣∣ < 1, then for any e < 1, considering e ≤ "kW2"*I) k < 1, it is concluded that
T-N
Timo ∏ WQ(T-i)
i=1
lim
T →∞
T-N
∏ Wω(t-i)
i=1
/	、T-N
≤ TlimO(I∣WΩ(t*i) II+Ir)	=0.
(24)
16
Published as a conference paper at ICLR 2021
Therefore
T-1
Iim Y Wω(t-i)=0.	(25)
T→∞
i=1
If the orbit {z1, z2, z3, . . .} tends to a stable k-cycle (k > 1) with the periodic points
{Fk(z*k), FkT(z*k), Fk-2(z*k),…，F(z*k)} = {s,…卜…，…(一)},
then, denoting the stable k-cycle by
rk = {zt*k , zt*k-1,…,zt*k-(k-1), zt*k , zt*k-1,…,zt*k-(k-1), …},	(26)
we have
lim d(zT , Γk ) = 0.	(27)
T→∞
Hence, there exists a neighborhood U of Γk and k sub-sequences {zTkn}∞=ι, {zTkn+ι}∞=ι, ∙∙∙,
{zTkn+(k-1) }n∞=1 of the sequence {zT}T∞=1 such that these sub-sequences belong to U and
⑴ ZTkn+s = Fk (ZTk(n-1) + s ),s = 0,1, 2,…,k - 1,
(ii)
lim zT
T→∞ Tkn+s
zt*k-s, S = 0, 1, 2, ∙
,k- 1,
(iii) for every ZT ∈ U there is some S ∈ {0,1,2,…，k - 1} such that ZT ∈ {zTkn+s }n∞=ι
In this case, for every zT ∈ U with zT ∈ {zTkn+s }n∞=1 we have lim zT = zt*k-s for some
T→∞
s = 0,1,2,…，k — 1. Therefore, continuity of F implies that lim F (ZT) = F (zt*k-s) and so
T→∞
lim
T→∞
(wω(t) zτ + h)
WΩ(t*k-s) zt*k-s + h.
(28)
Thus, similarly, we can prove that
∃ S ∈ {0, 1,2,…，k - 1} s.t. ^lim WΩ(T) = WΩ(t*k-s).
Analogously, for every i ∈ N (1 < i < ∞)
∃ Si ∈{0,1, 2,…，k - 1} S.t. 7lim Wω(t-i) = WΩ(t*k-Si),
(29)
(30)
OntheOtherhand, J] Wn(t*k-Si) I] < 1 for all Si ∈ {0,1,2,…，k - 1}. So, without loss of
generality, assuming
0≤maχ-1 nlWΩ(t*k-Si)J 0 = llWΩ(t*k)∣∣ < 1,	(31)
We can again obtain some relations similar to eq. 23-eq. 25 for t*k, k ≥ 1.
Since {ZT -1}T∞=1 is a convergent sequence, so it is bounded, i.e. there exists a real number q > 0
such that ||zt-ι || ≤ q for all T ∈ N. Furthermore, ∣∣Dω(t-i) ∣∣ ≤ 1 for all T. Therefore, by eq. 12
and eq. 23 (for t*k, k ≥ 1)
∂zt
dwmk
T-1 j-1
1(m,k)DΩ(T-1) ZT-1 + ∑ π wΩ(T-i) ) 1(m,k) DΩ(T-j) zT-j
j=2	i=1
T-1
+ ɪɪ WΩ(T-i) dΩ(1) z1
i=1
(32)
≤ kZT-1k +
T-1
kzT-jk + Y wΩ(T-i) kz1k
∣ i=1	∣
j-1
ɪɪ wΩ(T-i)
i=1
T-1
≤ q 1 + X (∣∣WΩ(t*k)
j=2
+ (II wΩ(t*k)∣∣ + 0 kzιk ∙
(33)
17
Published as a conference paper at ICLR 2021
Thus, by H Wn(t*k) ∣∣ + W < 1,we have
lim
T→∞
∂zτ	/ ，一	l∣WΩ(t*k)ll + W ∖	〃
≤ q 1 +	Il= M < ∞
dwmk	1	1 - U WΩ(t*k) U - *≡∕
(34)
i.e., by eq. 14 and eq. 15, the 2-norm of total gradient matrices and hence ∣∣ IWt b will not diverge
(explode) under the assumptions of Theorem 1.
Analogously, we can prove that ∣∣ d∣AT ∣∣? and ∣∣ d∂hT ∣∣? will not diverge either. Since, similar as in
the derivations above, it can be shown that relation eq. 34 is true for ∣∣ ET ∣∣ with q = q, where W
is the upper bound of kZT k, as {ZT}T∞=1 is convergent. Furthermore, relation eq. 34 also holds for
∣∣ ∂⅛∣ with q = 1.
Remark 2.1. By eq. 24 the Jacobian parts ∣∣ IzT ∣∣ connecting any two states ZT and Zt, T > t,
will not diverge either.
Corollary 2.1. The results ofTheorem 1 are also true if WΩ(t*k) is a normal matrix with no eigen-
value equal to one.
Proof. If WΩ(t*k) is normal, then ∣∣ Wn(t*k) ∣∣ = ρ(WΩ(t*k)) < 1 which satisfies the conditions of
Theorem 1.	□
6.1.4	Proof of Theorem 2
Let A, W and DΩ(k), t <k ≤ T, be partitioned as follows
A
Ireg
O
OT
Wnreg
DΩ(k)
Drkeg
O
DOnkrTeg	,
(35)
where IMreg×Mreg := Ireg ∈ RMreg×Mreg, OMreg×Mreg := Oreg ∈ RMreg×Mreg, O,S ∈
R(M-Mreg)×Mreg, A{Mreg+1:M,Mreg+1:M} := Anreg ∈ R(M-Mreg)×(M-Mreg) is a diagonal sub-
matrix, W{Mreg+1:M,Mreg+1:M} := Wnreg ∈ R(M-Mreg)×(M-Mreg) is an off-diagonal sub-matrix
(cf. Fig. S1). Moreover, DMk reg ×Mreg := Drkeg ∈ RMreg×Mreg and D{kMreg+1:M,Mreg+1:M} :=
Dnkreg ∈ R(M-Mreg)×(M-Mreg) are diagonal sub-matrices. Then, we have
Π WΩ(k)
t<k≤T
Y	SIDrekg
t<k≤T	reg
OT
Anreg + Wnreg Dnreg
π
t<k≤T
Ireg	OT
SDreg	Wnreg
OT
Ireg
SDr+g1 + Pj=2 (Qt<k≤t+j-1 Wkreg
Therefore, considering the 2-norm, we obtain
)sDr+j I Qt<k≤τ Wnreg.
(36)
∂zτ
∂zt
Π WΩ(k)
t<k≤T
Ireg
S Dr+g1+PT=2( Qt<k≤t+j-ι Wnreg)S Dr+j
OT
Qt<k≤T Wnreg
< ∞.
(37)
18
Published as a conference paper at ICLR 2021
Moreover
∂z
1 ≤ maχ口，P(WT-t)} = ρ( Y WΩ(k)) ≤ Il Y WΩ(k)ll = Il-dZτll (38)
t<k≤T	t<k≤T	zt
where WT-t := Qt<k≤T Wnkreg. Therefore, eq. 37 and eq. 38 yield
1 ≤ ρlow ≤
∂zτ
∂ Zt
≤ ρup < ∞.
Furthermore, we assumed that the non-regularized subsystem (zMreg+1 . . . zM ), if considered in
isolation, satisfies Theorem 1. Hence, similar to the proof of Theorem 1, it is concluded that
T
Tl→im∞ Y Wnkreg = Onreg
k=t
(39)
On the other hand, by definition of Dn(k), for every t < k ≤ T, we have llDkeg ll ≤ 1 and so
lS Drkeg l ≤ kSk lDrkeg l ≤ kSk,	(40)
which, in accordance with the the assumptions of Theorem 1, by convergence of
PjT=2 Qtk+=jt-+11 llWnkreg ll implies
lim
T→∞
T	t+j-1
SDwg + X( Y Wnkreg) SD^
j=2	k=t+1
T
≤ kSk 1+Tl→im∞Xj=2
t+j-1
Y llWnkreg
k=t+1
≤ kSkMnreg.
(41)
Thus, denoting Q := SDreg1 + PT=2 (Qt<k≤t+j-1 Wreg SDr+jj , from eq. 41 We deduce
that
λmaχ( lim (QT Q)) = lim P(QTQ) ≤ lim llQτQll = lim kQk2 ≤ ( ∣∣S∣∣Mηreg)2.
max T→∞	T→∞	T→∞	T→∞	nreg
(42)
Now, if T - t tends to ∞, then eq. 37, eq. 39 and eq. 42 result in
1 = Plow ≤
∂ ZT
∂zt
OT
Onreg
λmax(Ireg + lim (QT Q))
T→∞
Pup < ∞.
(43)
Remark2.2. If ∣∣Sk = 0, then l⅞ZTll → 1 as T 一 t → ∞.
6.1.5	Details on EM algorithm and DS reconstruction
For DS reconstruction we request that the latent RNN approximates the true generating system of
equations, which is a taller order than learning the mapping S → X or predicting future values
in a time series (cf. sect. 3.5).2 This point has important implications for the design of models,
inference algorithms and performance metrics if the primary goal is DS reconstruction rather than
‘mere’ time series forecasting.3 In this context we consider the fully probabilistic, generative RNN
eq. 1. Together with eq. 2 (where we take g(zt) = φ(zt)) this gives the typical form of a nonlinear
2By reconstructing the governing equations we mean their approximation in the sense of the universal
approximation theorems for DS (Funahashi & Nakamura, 1993; Kimura & Nakano, 1998), i.e. such that the
behavior of the reconstructed system becomes dynamically equivalent to that of the true underlying system.
3In this context we also remark that models which include longer histories of hidden activations (Yu et al.,
2019), as in many statistical time series models (Fan & Yao, 2003), are not formally valid DS models anymore
since they violate the uniqueness of flow in state space (Strogatz, 2015).
19
Published as a conference paper at ICLR 2021
state space model (Durbin & Koopman, 2012) with observation and process noise. We solve for
the parameters θ = {A, W, C, h, μo, Σ, B, Γ} by maximum likelihood, for which an efficient
Expectation-Maximization (EM) algorithm has recently been suggested (Durstewitz, 2017; Koppe
et al., 2019), which we will summarize here. Since the involved integrals are not tractable, we start
off from the evidence-lower bound (ELBO) to the log-likelihood which can be rewritten in various
useful ways:
logp(X∣θ) ≥ EZ〜q[logPθ(X, Z)] + H (q(ZX))
=logp(X∣θ) - DKL (q(Z∣X)kpθ(ZX))
=: L(θ,q)	(44)
In the E-step, given a current estimate θ* for the parameters, we seek to determine the posterior
pθ (Z|X) which we approximate by a global Gaussian q(Z|X) instantiated by the maximizer
(mode) Z* of pθ(Z|X) as an estimator of the mean, and the negative inverse Hessian around this
maximizer as an estimator of the state covariance, i.e.
E[Z|X] ≈ Z* = argmaxlogpθ(ZX)
Z
=argmax[logpθ(X|Z) + logpθ(Z) - logpθ(X)]
Z
=arg max [logPθ(X |Z) + logPθ (Z)],	(45)
Z
since Z integrates out inpθ(X) (equivalently, this result can be derived from a Laplace approxima-
tion to the log-likelihood, logP(X∣θ) ≈ logpθ(X|Z*)+logpθ(Z*) -1 log | - L*|+const, where
L* is the Hessian evaluated at the maximizer). We solve this optimization problem by a fixed-point
iteration scheme that efficiently exploits the model’s piecewise linear structure, as detailed below.
Using this approximate posterior for pθ (Z |X), based on the model’s piecewise-linear structure most
of the expectation values Ez〜q [φ(z)], Ez〜q [φ(z)zT], and Ez〜q [φ(z)φ(z)T], could be solved for
(semi-)analytically (where z is the concatenated vector form of Z, see below). In the M-step, we
seek θ* := arg maxθ L(θ, q*), assuming proposal density q* to be given from the E-step, which for
a Gaussian observation model amounts to a simple linear regression problem (see Suppl. eq. 49). To
force the PLRNN to really capture the underlying DS in its governing equations, we use a previously
suggested (Koppe et al., 2019) stepwise annealing protocol that gradually shifts the burden of fitting
the observations X from the observation model eq. 2 to the latent RNN model eq. 1 during training,
the idea of which is to establish a mapping from latent states Z to observations X first, fixing this,
and then enforcing the temporal consistency constraints implied by eq. 1 while accounting for the
actual observations.
Now we briefly outline the fixed-point-iteration algorithm for solving the maximization problem
in eq. 45 (for more details see Durstewitz (2017); Koppe et al. (2019)). Given a Gaussian latent
PLRNN and a Gaussian observation model, the joint density p(X, Z) will be piecewise Gaussian,
hence eq. 45 piecewise quadratic in Z. Let us concatenate all state variables across m and t into
one long column vector z = (z1,1, . . . , zM,1, . . . , z1,T , . . . , zM,T )T, arrange matrices A, W into
large MT X MT block tri-diagonal matrices, define dn ：= (lz1,1>0, Iz2,1>0,..., 1zm,t>o)T as an
indicator vector with a 1 for all states zm,t > 0 and zeros otherwise, and Dω ：= diag(dn) as the
diagonal matrix formed from this vector. Collecting all terms quadratic, linear, or constant in z, we
can then write down the optimization criterion in the form
QΩ(z) = -2[zT (Uo + DωUi + UTDω + DωU2Dω)z
-zτ (v0 + DωVi) - (v0 + DωVi)t z] + const.	(46)
In essence, the algorithm now iterates between the two steps:
1.	Given fixed Dω, solve
z* = (Uo	+ DωUi	+	UTDω	+	DqSDq)-1	∙	(vo	+	DωVi)	(47)
2.	Given fixed z*, recompute Dω
20
Published as a conference paper at ICLR 2021
until either convergence or one of several stopping criteria (partly likelihood-based, partly to avoid
loops) is reached. The solution may afterwards be refined by one quadratic programming step.
Numerical experiments showed this algorithm to be very fast and efficient (Durstewitz, 2017; Koppe
et al., 2019). At z*, an estimate of the state covariance is then obtained as the inverse negative
Hessian,
V = (Uo + DωUi + UTDω + Dω5Dω)T .	(48)
In the M-step, using the proposal density q* from the E-step, the solution to the maximization
problem θ* := arg max L(θ, q*), can generally be expressed in the form
θ
θ*
(49)
where, for the latent model, eq. 1,	αt	=zt	and βt :=	zTt-1,	φ(zt-1)T,	sTt,	1T ∈	R2M+K+1, and
for the observation model, eq. 2, αt = xt and βt = g (zt).
6.1.6	More details on DS performance measure
As argued before (Koppe et al., 2019; Wood, 2010), in DS reconstruction we require that the RNN
captures the underlying attractor geometries and state space properties. This does not necessarily
entail that the reconstructed system could predict future time series observations more than a few
time steps ahead, and vice versa. For instance, if the underlying attractor is chaotic, even if we had
the exact true system available, with a tiny bit of noise trajectories starting from the same initial
condition will quickly diverge and ahead-prediction errors become essentially meaningless as a DS
performance metric (Fig. S2B).
To quantify how well an inferred PLRNN captured the underlying dynamics we therefore followed
Koppe et al. (2019) and used the Kullback-Leibler divergence between the true and reproduced prob-
ability distributions across states in state space, thus assessing the agreement in attractor geometries
(cf. Takens (1981); Sauer et al. (1991)) rather than in precise matching of time series,
DKL (Ptrue(X) kPgen(XIz)) ≈ Xp(kUe(X) log ( Pkr)Ue(X) ) ,	(5O)
k = 1	∖pge4(XIz) /
where Ptrue(X) is the true distribution of observations across state space (not time!), Pgen(X|z) is
the distribution of observations generated by running the inferred PLRNN, and the sum indicates a
spatial discretization (binning) of the observed state space. We emphasize thatPgen (x∣z) is obtained
from freely simulated trajectories, i.e. drawn from the prior p(z) specified by eq. 1, not from the
inferred posteriors p(z∣Xtrain). In addition, to assess reproduction of time scales by the inferred
PLRNN, the average MSE between the power spectra of the true and generated time series was
computed, as displayed in Fig. 3B-C.
The measure DKL introduced above only works for situations where the ground truth Ptrue(X)
is known. Following Koppe et al. (2019), we next briefly indicate how a proxy for DKL may be
obtained in empirical situations where no ground truth is available. Reasoning that for a well recon-
structed DS the inferred posteriorPinf(zIX) given the observations should be a good representative
of the prior generative dynamics Pgen (z), one may use the Kullback-Leibler divergence between the
distribution over latent states, obtained by sampling from the prior density Pgen(z), and the (data-
constrained) posterior distribution Pinf (zIX) (where z ∈ RM×1 and X ∈ RN×1), taken across the
system’s state space:
DKL (Pinf(zIX)kPgen(z))
P	Pinf (z∣X)log Pmf(ZIX) dz
z∈RM×1	Pgen(z)
(51)
As evaluating this integral is difficult, one could further approximate Pinf (zIX) and Pgen(z) by
Gaussian mixtures across trajectories, i.e. Pinf(z∣x) ≈ T PT=IP(zt∣Xi:T) and Pgen(z) ≈
L PL=IP(zι∣zι-ι), where the mean and covariance of P(zt∣Xi:T) and p(z∣zι-ι) are obtained
by marginalizing over the multivariate distributions P(ZIX) and Pgen(Z), respectively, yielding
E[zt IX1:T], E[zl Izl-1], and covariance matrices Var(zt IX1:T) and Var(zl Izl-1). Supplementary
21
Published as a conference paper at ICLR 2021
eq. 51 may then be numerically approximated through Monte Carlo sampling (Hershey & Olsen,
2007) by
DKL (pinf (z|x)kpgen(z))
〜1 XX lo Mpinf (z(i)lx)
≈ n Pg pgen(z(i)),
z(i) 〜pinf (z|x)
(52)
Alternatively, there is also a variational approximation of eq. 51 available (Hershey & Olsen, 2007):
1 T	PT	e-DKL(p(zt|x1:T)kp(zj|x1:T))
DKrtiOnal (Pinf (ZIx) kpgen(Z)) ≈ T X log Pj1 6-小(皿叫：T)	∣三一))，
(53)
where the KL divergences in the exponentials are among Gaussians for which we have an analytical
expression.
6.1.7	More details on benchmark tasks and model comparisons
We compared the performance of our rPLRNN to the other models summarized in Suppl. Table 1
on the following three benchmarks requiring long short-term maintenance of information (Talathi
& Vartak (2016); Hochreiter & Schmidhuber (1997)): 1) The addition problem of time length T
consists of 100 000 training and 10 000 test samples of 2 × T input series S = {s1, . . ., sT}, where
entries s1,: ∈ [0, 1] are drawn from a uniform random distribution and s2,: ∈ {0, 1} contains zeros
except for two indicator bits placed randomly at times t1 < 10 and t2 < T/2. Constraints on
t1 and t2 are chosen such that every trial requires a long memory of at least T/2 time steps. At
the last time step T, the target output of the network is the sum of the two inputs in s1,: indicated
by the 1-entries in s2,:, xtTarget
s1,t1 + s1,t2 . 2) The multiplication problem is the same as the
addition problem, only that the product instead of the sum has to be produced by the RNN as an
output at time T, XTarget = s1,tι ∙ s1,t2. 3) The MNIST dataset (LeCun et al., 2010) consists of
60 000 training and 10 000 28 × 28 test images of hand written digits. To make this a time series
problem, in sequential MNIST the images are presented sequentially, pixel-by-pixel, scanning lines
from upper left to bottom-right, resulting in time series of fixed length T = 784.
For training on the addition and multiplication problems, the mean squared-error loss across R
samples, L = R PR=I (XTn) - XTn)) , between estimated and actual outputs was used, while
the cross-entropy loss L = PR=I (- P1=ι XinT log(p(nT)) was employed for sequential MNIST,
where
pi,t := Pt (Xi,t = 1∣Zt)= (eBi,:Zt) (XX eBj,:Z)
(54)
with xi,t ∈ {0, 1},	i xi,t = 1. We remark that as long as the observation model takes the form of
a generalized linear model (Fahrmeir & Tutz, 2001), as assumed here, meaning may be assigned to
the latent states zm by virtue of their association with specific sets of observations xn through the
factor loading matrix B . This adds another layer of model interpretability (besides its accessibility
in DS terms).
The large error bars in Fig. 2 at the transition from good to bad performance result from the fact
that the networks mostly learn these tasks in an all-or-none fashion. While the rPLRNN in gen-
eral outperformed the pure initialization-based models (iRNN, npRNN, iPLRNN), confirming that
a manifold attractor subspace present at initialization may be lost throughout training, we conjec-
ture that this difference in performance will become even more pronounced as noise levels or task
complexity increase.
22
Published as a conference paper at ICLR 2021
Table 1: Overview over the different models used for comparison
NAME	DESCRIPTION
RNN	Vanilla ReLU based RNN
iRNN	RNN with initialization Wo = I and ho = 0 (Le et al., 2015)
npRNN	RNN with weights initialized to a normalized positive definite matrix with largest eigenvalue of 1 and biases initialized to zero (Talathi & Vartak, 2016)
PLRNN	PLRNN as given in eq. 1 (KoPPe et al., 2019)
iPLRNN	PLRNN with initialization Ao = I, Wo = 0 and ho = 0
rPLRNN	PLRNN initialized as illustrated in Fig. S1, with additional regular- ization term (eq. 3)
LSTM	Long Short-Term Memory (Hochreiter & Schmidhuber, 1997)
oRNN	ReLU RNN with W regularized toward orthogonality (WWT → I) (Vorontsov et al., 2017)
L2RNN	Vanilla RNN with standard L2 regularization on all weights
L2pPLRNN	PLRNN with (partial) standard L2 regularization for proportion Mreg/M = 0.5 of units (i.e., pushing all terms in A and W for these units to 0)
L2fPLRNN	PLRNN with (full) standard L2 regularization on all weights
6.1.8	More details on single neuron model
The neuron model used in section 4.2 is described by
_ _________ _ 、 _______________ _ 、
-CmVr = gL (V - EL) + gNam∞(V)(V - ENa)
+ gK n(V - EK ) + gM h(V - EK)
+ gNMDAσ(V)(V - ENMDA)	(55)
；h∞(V) - h
h =----------
τh
n∞ (V) - n
n =----------
τn
σ(V) = 1 +.33e-.0625V -1
(56)
(57)
(58)
where Cm refers to the neuron's membrane capacitance, the g∙ to different membrane conductances,
E∙ to the respective reversal potentials, and m, h, and n are gating variables with limiting values
given by
{m∞,n∞,h∞} = h1 + e({VhNa,VhK,VhM}-V)/{kNa,kK,kM}i-1	(59)
Different parameter settings in this model lead to different dynamical phenomena, including regular
spiking, slow bursting or chaos (see Durstewitz (2009) for details). Parameter settings used here
were: Cm = 6 μF, gL = 8mS, EL = -80 mV, gNa = 20mS, ENa = 60mV, VhNa = -20mV,
kNa = 15, gK = 10 mS, EK = -90 mV, VhK = -25 mV, kK = 5, τn = 1 ms, gM = 25 mS,
VhM = -15 mV, kM = 5, τh = 200 ms, gNMDA = 10.2 mS.
23
Published as a conference paper at ICLR 2021
6.2 Supplementary Figures
O O 0∖
OOO
OOO
× O O
OxO
O O × /
×	×	×	O	×	×
×	×	×	×	O	×
∖×	×	×	×	×	O/
W
×
∖×∕
h
A
Figure S1:	Illustration of the ‘manifold-attractor-regularization’ for the PLRNN’s auto-regression
matrix A, coupling matrix W , and bias terms h. Regularized values are indicated in red, crosses
mark arbitrary values (all other values set to 0 as indicated).
Figure S2:	MSE evaluated between time series is not a good measure for DS reconstruction. A) Time
graph (top) and state space (bottom) for the single neuron model (see section 4.2 and Suppl. 6.1.8)
with parameters in the chaotic regime (blue curves) and with simple fixed point dynamics in the limit
(red line). Although the system has vastly different limiting behaviors (attractor geometries) in these
two cases, as visualized in the state space, the agreement in time series initially seems to indicate
a perfect fit. B) Same as in A) for two trajectories drawn from exactly the same DS (i.e., same
parameters) with slightly different initial conditions. Despite identical dynamics, the trajectories
immediately diverge, resulting in a high MSE. Dash-dotted grey lines in top graphs indicate the
point from which onward the state space trajectories were depicted.
24
Published as a conference paper at ICLR 2021
Mτee∕M	Mτeg∕M
Figure S3:	Performance of the rPLRNN for different A) numbers of latent states M, B) values of τ ,
and C-E) proportions Mreg/M of regularized states. A-C are for the addition problem, D for the
multiplication problem, and E for sequential MNIST. Dashed lines denote the values used for the
results reported in section 4.1.

Figure S4: A) 20-step-ahead prediction error between true and generated observations for rPLRNN
as a function of regularization τ . B) KL divergence (DKL) between true and generated state space
distributions for orthogonal PLRNN (oPLRNN; i.e., the PLRNN with the ‘manifold attractor regu-
larization’ replaced by an orthogonality regularization, (A + W)(A + W)T → I), as well as for
the partially (L2p) and fully (L2f) standard L2-regularized PLRNNs (i.e., with all weight parameters
for all (L2f) or only a fraction Mreg/M of states (L2p) driven to 0). Note that the quality of the DS
reconstruction does not significantly depend on the strength of regularization τ, or becomes even
slightly worse, for the oPLRNN, L2pPLRNN and L2fPLRNN. Globally diverging estimates were
removed.
25
Published as a conference paper at ICLR 2021

Figure S5:	A) Reconstruction of fast gating variable n (rightmost) not shown in Fig. 3D. For com-
pleteness and comparison, other variables have been re-plotted from Fig. 3D as well. B) Example
of reconstruction of voltage (V , left) and slow gating (h, center) observations, and underlying latent
state dynamics (right) for oPLRNN (with orthogonality regularization on A + W, see Fig. S4 leg-
end). C) Example of V (left) and h (center) observations for standard PLRNN, and underlying latent
state dynamics (right). In general, both the standard and the oPLRNN tended to produce many fixed
point solutions. In those cases where this was not the case, the standard PLRNN tended to reproduce
only the fast components of the dynamics as in the example in C (in agreement with the results in
Figs. 3C & 3E), while the oPLRNN tended to capture only the slow components as in the example
in B (as expected from the fact that the orthogonality constraint tends to produce solutions similar
to those obtained for the regularized states only, cf. Fig. 3E).
26
Published as a conference paper at ICLR 2021
0	2	4	6	8
time (s)
0	2	4	6	8
time (s)
0	2	4	6	8
time (s)
Figure S6:	Reconstruction of aDS with multiple time scales like fast spikes and slow T-waves (simu-
lated ECG signal, see McSharry et al. (2003)). A) KL divergence (DKL) between true and generated
state space distributions as a function of τ . Unstable (globally diverging) system estimates were re-
moved. B) Average MSE between power spectra (slightly smoothed) of true and reconstructed DS.
C) Average normalized MSE between power spectra of true and reconstructed DS split according
to low (≤ 2.5 Hz) and high (> 2.5 Hz) frequency components. Error bars = SEM in all graphs. D)
Example of (best) generated time series (standardized, red=reconstruction with τ = 1000/3600).
——⅞——LSTM (Hochreiter & Schmidhuber, 1997)
—⅞- L2pPLRNN
—⅞—RNN
—⅞—L2fPLRNN
—⅞— rPLRNN
—⅞— L2RNN
1
0.8
I 0.6
位0.4
0.2
Figure S7:	Same as Fig. 2, illustrating performance for L2RNN (vanilla RNN with L2 regulariza-
tion on all weights) and L2fPLRNN (PLRNN with L2 regularization on all weights) on the three
problems shown in Fig. 2. Note that the L2fPLRNN is essentially not able to learn any of the
tasks, likely because a conventional L2 norm drives the PLRNN parameters away from a manifold
attractor configuration (as supported by Fig. 4 and Fig. S8). Results for rPLRNN, vanilla RNN,
L2pPLRNN, and LSTM have been re-plotted from Fig. 2 for comparison.
27
Published as a conference paper at ICLR 2021
B M — &1Y=m日
1.86 4.2O
ʌ0.SS0.
AQUenbag eAEelal
.86.4.20.2
0.0.0.0.。
*
L
Γp
f
2
L
P
2
L
Figure S8:	Same as Fig. 4 for A, B) multiplication problem, and C, D) sequential MNIST. Error
bars = stdv.
T ∙1500
Figure S9:	Effect of regularization strength T on rPLRNN network parameters (cf. eq. 1) (regular-
ized parameters for states m ≤ Mreg, eq. 1, in red). Note that some of the non-regularized network
parameters (in blue) appear to systematically change as well as T is varied.
28
Published as a conference paper at ICLR 2021
Figure S10:	Cross-entropy loss as a function of training epochs for the best model fits on the
sequential MNIST task. Note that LSTM takes longer to converge than the other models. LSTM
training was therefore allowed to proceed for 200 epochs, after which convergence was usually
reached, while training for all other models was stopped after 100 epochs. Also note that although
for the best test performance on seq. MNIST shown here LSTM slightly supersedes rPLRNN, on
average rPLRNN performed better than LSTM (as shown in Fig. 2C), despite having much fewer
trainable parameters (when LSTM was given about the same number of parameters as rPLRNN, i.e.
M/4, its performance fell behind even more).
Figure S11:	Example reconstruction of a chaotic system, the famous 3d Lorenz equations, by the
rPLRNN. Left: True state space trajectory of Lorenz system; right: trajectory simulated by rPLRNN
(τ = 100/T, M = 14) after training on time series of length T = 1000 from the Lorenz system.
29