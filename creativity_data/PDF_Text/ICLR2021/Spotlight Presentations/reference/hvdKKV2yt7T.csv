title,year,conference
 Unlabeleddata improves adversarial robustness,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Copycat cnn: Stealing knowledge by persuading confession with randomnon-labeled data,2018, 2018 International Joint Conference on Neural Networks (IJCNN)
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Data-freeadversarial distillation,2019, arXiv preprint arXiv:1912
 Logan: Membership infer-ence attacks against generative models,2019, Proceedings on Privacy Enhancing Technologies
 Learning multiple layers of features from tiny images,2012, University of Toronto
 Legal risks ofadversarial machine learning research,2020, arXiv preprint arXiv:2006
 Stolen memories: Leveraging model memorization for calibratedwhite-box membership inference,2020, In USENIX Security Symposium
 Principled detection of out-of-distribution examples inneural networks,2017, arXiv preprint arXiv:1706
 Fine-pruning: Defending against back-dooring attacks on deep neural networks,2018, In International Symposium on Research in Attacks
 Uniform convergence may be unable to explain gener-alization in deep learning,2019, In Advances in Neural Information Processing Systems 32
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning 2011
 Feature visualization,2017, Distill
 The building blocks of interpretability,2018, Distill
 On the robustness of thebackdoor-based watermarking in deep neural networks,2019, arXiv preprint arXiv:1906
 Membership inference at-tacks against machine learning models,2017, In 2017 IEEE Symposium on Security and Privacy (SP)
 Overlearning reveals sensitive attributes,2019,	CoRR
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Stealing machinelearning models via prediction apis,2016, In 25th USENIX Security Symposium (USENIX Security 16)
 Embedding watermarksinto deeP neural networks,2017, CoRR
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, In 2019IEEE Symposium on Security and Privacy (SP)
 The harmonic mean P-value for combining dePendent tests,2018, bioRxiv
 Privacy Risk in Machine Learning: Analyzingthe Connection to Overfitting,2018, In 2018 IEEE 31st Computer Security Foundations Symposium(CSF)
 Let tV represent the mean of the ‘prediction margin’ of all points in SV for a classifierf,2021, Similarly
