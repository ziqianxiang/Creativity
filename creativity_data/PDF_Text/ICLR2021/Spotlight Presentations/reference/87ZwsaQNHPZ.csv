title,year,conference
 Scalable methods for 8-bit training ofneural networks,2018, In Advances in neural information processing Systems
 signsgd:Compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Understanding andoptimizing asynchronous low-precision stochastic gradient descent,2017, In Proceedings of the 44thAnnual International Symposium on Computer Architecture
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Releq: A reinforcement learning approach for automatic deep quantization ofneural networks,2020, IEEE Micro
 Learned step size quantization,2019, arXiv preprint arXiv:1902
 Fractrain: Fractionally squeezing bit savings both temporally and spatiallyfor efficient dnn training,2020, arXiv preprint arXiv:2012
 Deep learning withlimited numerical precision,2015, In International Conference on Machine Learning
 Long short-term memory,1997, Neural computation
 Normalization helps trainingof quantized lstm,2019, In Advances in Neural Information Processing Systems
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Adaptive quantization of neural networks,2018, In International Conferenceon Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 HALO: Hardware-aware learning to optimize,2020, In The 16th European Conference on Computer Vision (ECCV 2020)
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, In Advances in Neural Information Processing Systems
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 Mixedprecision training,2017, CoRR
 Apprentice: Using knowledge distillation techniqUes to improvelow-precision network accUracy,2017, arXiv preprint arXiv:1711
 Wrpn: wide redUced-precisionnetworks,2017, arXiv preprint arXiv:1709
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 Weighted-entropy-based quantization for deep neu-ral networks,2017, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 On the spectral bias of neural networks,2019, In Proceedings of the 36thInternational Conference on Machine Learning
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European conference on computervision
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Fractionalskipping: Towards finer-grained dynamic cnn inference,2020, 2020
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE Winter Conferenceon Applications of Computer Vision (WACV)
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Haq: Hardware-aware automatedquantization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Trainingdeep neural networks with 8-bit floating point numbers,2018, In Advances in neural informationprocessing systems
 Skipnet: Learning dynamicrouting in convolutional networks,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 Deepk-means: Re-training and parameter sharing with harder cluster assignments for compressing deepconvolutions,2018, arXiv preprint arXiv:1806
 Dnq: Dynamicnetwork quantization,2018, arXiv preprint arXiv:1812
 Fracbits: Mixed precision quantization via fractional bit-widths,2020, arXivpreprint arXiv:2007
 Training high-performanceand large-scale deep neural networks with full 8-bit integers,2020, Neural Networks
 Drawing early-bird tickets: Toward more efficienttraining of deep networks,2020, In International Conference on Learning Representations
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Adaptivequantization for deep neural network,2017, arXiv preprint arXiv:1712
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
 Towards effective low-bitwidth convolutional neural networks,2018, In Proceedings of the IEEE conference on computer visionand pattern recognition
