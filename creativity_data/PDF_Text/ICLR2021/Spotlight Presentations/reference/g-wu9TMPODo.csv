title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In Proceedings of the 35th InternationalConference on Machine Learning
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Overfitting or perfect fitting? risk bounds for classifi-cation and regression rules that interpolate,2018, In S
 Wild patterns,2018, In Proceedings of the 2018 ACM SIGSAC Conferenceon Computer and Communications Security
 Finite-sample analysis of interpolating linear classifiers inthe overparameterized regime,2020, arXiv:2004
 Computational limitations inrobust classification and win-win results,2019, In Alina Beygelzimer and Daniel Hsu (eds
 Does learning require memorization? a short tale about a long tail,2019, arXiv:1906
 Explaining and Harnessing AdversarialExamples,2014, arXiv preprint arXiv:1412
 Complexity of linear regions in deep networks,2019, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv:1903
 Deep Residual Learning for ImageRecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Using pre-training can improve model robustnessand uncertainty,2019, Proceedings of the International Conference on Machine Learning
 Natural adversarialexamples,2019, arXiv:1907
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Excessive invariancecauses adversarial vulnerability,2019, In International Conference on Learning Representations
 Invariance vs robustness of neuralnetworks,2020, 2020
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Adversarial camera stickers: A physical camera-based attack on deep learning systems,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2019, arXiv:1903
 An intriguing failing of convolutional neural networks and the coordconv solution,2018, InSamy Bengio
 Multitask learning strengthens adversarial robustness,2020, 2020
 Distillation as adefense to adversarial perturbations against deep neural networks,2015, arXiv:1511
 Practical black-box attacks against machine learning,2017, In ProCeedings of the 2017ACM on Asia ConferenCe on Computer and CommuniCations SeCurity
 Adversarialtraining can hurt generalization,2019, arXiv:1906
 Robustness via deep low-rankrepresentations,2020, 2020a
 Adver-sarially robust generalization requires more data,2018, In AdvanCes in Neural Information ProCessingSystems
 Adversarialattacks against automatic speech recognition systems via psychoacoustic hiding,2018, arXiv:1808
 Learning with bad training data via iterative trimmed lossminimization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 On adaptive attacks toadversarial example defenses,2020, arXiv:2002
 Ro-bustness may be at odds with accuracy,2019, In International ConferenCe on Learning Representations
 Fast is better than free: Revisiting adversarial training,2020, InInternational ConferenCe on Learning Representations
 Adversarial robustness through local lipschitzness,2020, arXiv:2003
 Rademacher complexity for adversariallyrobust generalization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 What neural networks memorize and why: Discovering the longtail via influence estimation,2020, Unpublished manuscript
 Understanding deeplearning requires rethinking generalization,2016, International Conference on Learning Representations(ICLR)
