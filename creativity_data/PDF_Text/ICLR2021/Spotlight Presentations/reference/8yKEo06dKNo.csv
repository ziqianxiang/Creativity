title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, arXiv preprint arXiv:1904
 Dropout: Explicit forms andcapacity control,2020, arXiv preprint arXiv:2003
 Stronger generalization bounds for deep nets viaa compression approach,2018, In 35th International Conference on Machine Learning
 Localized rademacher complexities,2002, InInternational Conference on Computational Learning Theory
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Mixmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 On mixup regular-ization,2020, arXiv preprint arXiv:2006
 Deep learning for classical japanese literature,2019, In NeurIPS Creativity Workshop 2019
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Patchup: A regularization technique for convolutional neural networks,2020, arXiv preprintarXiv:2006
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Mixup as locally linear out-of-manifold regulariza-tion,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Puzzle mix: Exploiting saliency and localstatistics for optimal mixup,2020, International Conference on Machine Learning
 Interpolated adversarial training:Achieving robust neural networks without sacrificing too much accuracy,2019, In Proceedings of the12th ACM Workshop on Artificial Intelligence and Security
 On the implicit bias of dropout,2018, In InternationalConference on Machine Learning
 Readingdigits in natural images with unsupervised feature learning,2011, NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Towards understanding the role of over-parametrization in gen-eralization of neural networks,2019, In International Conference on Learning Representations (ICLR)
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 On the depth of deep neuralnetworks: A theoretical view,2015, arXiv preprint arXiv:1506
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Interpolation con-sistency training for semi-supervised learning,2019, In Proceedings of the 28th International JointConference on Artificial Intelligence
 Dropout training as adaptive regularization,2013, InAdvances in neural information processing systems
 Fast dropout training,2013, In international conference on machinelearning
 The implicit and explicit regularization effects ofdropout,2020, arXiv preprint arXiv:2002
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Robustness and generalization,2012, Machine learning
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
