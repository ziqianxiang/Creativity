title,year,conference
 SIGNSGD: Compressedoptimisation for non-convex problems,2018, Arxiv:1802
 On the convergence of a class of Adam-type algorithmsfor non-convex optimization,2019, International Conference on Learning Representations
 Convergence guarantees for RMSProp and Adam in non-convex optimization and an empirical comparison to Nesterov acceleration,2018, Arxiv:1807
 Understanding RMSprop and Adam: Theoretical and empiricalstudies,2019, Arxiv:1807
 On the convergence of Adam andAdagrad,2020, CoRR
 Nostalgic Adam: Weighting more of the past gradients whendesigning the adaptive learning rate,2019, IJCAI
 Adam: A method for stochastic optimization,2015, International Conferenceon Learning Representations
 On thevariance of the adaptive learning rate and beyond,2020, In International Conference on Learning Representations
 Adaptive gradient methods with dynamic bound oflearning rate,2019, International Conference on Learning Representations
 Asynchronous methods for deep reinforcement learning,2016, InternationalConference on Machine Learning
 Unsupervised representation learning with deep convolutionalgenerative adversarial networks,2016, International Conference on Learning Representations
 Explicit regularization of stochastic gradient methods through duality,2020, Arxiv
 On the convergence of Adam and beyond,2018, InternationalConference on Learning Representations
 First order generativeadversarial networks,2018, In Proceedings of the 35th International Conference on Machine Learning
 Adaptive methods fornonconvex optimization,2018, Neural Information Processing Systems
 Adashift: decorrela-tion and convergence of adaptive learning methods,2019, International Conference on Learning Representations
 A sufficient condition for convergences ofAdam and RMSProp,2019, CVPR
