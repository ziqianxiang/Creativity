title,year,conference
 Deep learning with differential privacy,2016, In SIGSAC
 Improving queryefficiency of black-box adversarial attack,2020, In ECCV
 A new backdoor attack in cnns by training setcorruption without label poisoning,2019, In ICIP
 Towards evaluating the robustness of neural networks,2017, In SP
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In ICML
 Arcface: Additive angular marginloss for deep face recognition,2019, In CVPR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Adversarialcamouflage: Hiding physical-world attacks with natural styles,2020, In CVPR
 Robustness of classifiers:from adversarial to random noise,2016, In NeurIPS
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Deep residual learning for imagerecognition,2016, In CVPR
 Densely connectedconvolutional networks,2017, In CVPR
 Labeled faces in the wild: Adatabase forstudying face recognition in unconstrained environments,2008, 2008
 Black-box adversarialattacks on video recognition models,2019, In ACMMM
 Adam: A method for stochastic optimization,2015, In ICLR
 Understanding black-box predictions via influence functions,2017, InICML
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Fast autoaugment,2019, InNeurIPS
 Reflection backdoor: A natural backdoor attackon deep neural networks,2020, In ECCV
 Deep learning face attributes in the wild,2015, InICCV
 SGDR: stochastic gradient descent with warm restarts,2017, In ICLR
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, In ICLR
 Understandingadversarial attacks on deep learning based medical image analysis systems,2020, Pattern Recognition
 Universaladversarial perturbations,2017, In CVPR
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In AISec
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Deep face recognition,2015, 2015
 Differential privacy preservation for deepauto-encoders: an application of human behavior prediction,2016, In AAAI
 Adaptive laplace mechanism: Differentialprivacy preservation in deep learning,2017, In ICDM
 Privacy-preserving deep learning,2015, In SIGSAC
 Membership inference attacksagainst machine learning models,2017, In SP
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Deepface: Closing the gap tohuman-level performance in face verification,2014, In CVPR
 Clean-label backdoor attacks,2018, 2018
 High-frequency component helps explainthe generalization of convolutional neural networks,2020, In CVPR
 A unifiedapproach to interpreting and boosting adversarial transferability,2020, arXiv preprint arXiv:2010
 On theconvergence and robustness of adversarial training,2019, In ICML
 Improvingadversarial robustness requires revisiting misclassified examples,2020, In ICLR
 Skip connections matter:On the transferability of adversarial examples generated with resnets,2020, In ICLR
 Generative poisoning attack method against neuralnetworks,2017, arXiv preprint arXiv:1703
 Learning face representation from scratch,2014, arXivpreprint arXiv:1411
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
 Clean-label backdoor attacks on video recognition models,2020, In CVPR
 Anotherimportant property of the unlearnable examplescreated by error-minimizing noise is its resis-tance to data augmentations,2019, Since standard dataaugmentation techniques like random shift
