title,year,conference
 Data-free learning of student networks,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Distilling portable generative adversarial networks for image translation,2020, arXivpreprint arXiv:2003
 Long live the lottery:The existence of winning tickets in lifelong learning,2021, In International Conference on LearningRepresentations
 Robust overfittingmay be mitigated by properly learned smoothening,2021, In International Conference on LearningRepresentations
 Optical flowdistillation: Towards efficient and stable video style transfer,2020, arXiv preprint arXiv:2007
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Rethinking deep neural network ownership veri-fication: Embedding passports to defeat ambiguity attacks,2019, In Advances in Neural InformationProcessing Systems
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv preprint arXiv:1708
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Residual distillation: Towards portable deep neural networks withoutshortcuts,2020, Advances in Neural Information Processing Systems
 Structuredknowledge distillation for semantic segmentation,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Data-free knowledge distillation for deepneural networks,2017, arXiv preprint arXiv:1710
 Goodstudents play big lottery better,2021, arXiv
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Improved knowledge distillation via teacher assistant,2019, arXiv preprintarXiv:1902
 Prediction poisoning: Towards defensesagainst dnn model stealing attacks,2020, In International Conference on Learning Representations
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Embedding watermarksinto deep neural networks,2017, In Proceedings of the 2017 ACM on International Conference onMultimedia Retrieval
 Towards privacy-preserving visualrecognition via adversarial training: A pilot study,2018, In Proceedings of the European Conferenceon Computer Vision (ECCV)
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Model watermarking for image processing networks,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 Passport-awarenormalization for deep model protection,2020, Advances in Neural Information Processing Systems
