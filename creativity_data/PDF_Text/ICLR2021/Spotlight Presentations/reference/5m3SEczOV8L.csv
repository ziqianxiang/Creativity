title,year,conference
 Generalized energy based models,2020, arXiv preprintarXiv:2003
 Wasserstein gan,2017, Proceedings of the 34thInternational Conference on Machine Learning
 A note on the inception score,2018, arXiv preprint arXiv:1801
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Diagnosing and enhancing vae models,2019, International Conference onLearning Representations
 Prescribed generativeadversarial networks,2019, arXiv preprint arXiv:1910
 Density estimation using real nvp,2016, arXivpreprint arXiv:1605
 Implicit generation and modeling with energy based models,2019, InAdvances in Neural Information Processing Systems
 Augmented neural ODEs,2019, In Advances inNeural Information Processing Systems
 Generative adversarial nets,2014, In Z
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conferenceon Artificial Intelligence and Statistics
 Training products of experts by minimizing contrastive divergence,2002, Neuralcomputation
 A practical guide to training restricted boltzmann machines,2012, In Neural networks:Tricks of the trade
 Denoising diffusion probabilistic models,2020, arXiv preprintarXiv:2006
 Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport,2019, arXivpreprint arXiv:1903
 Semi-supervised learn-ing with normalizing flows,2020, In ICML
 Equivariant flow-based samplingfor lattice gaUge theory,2020, Phys
 A style-based generator architecture for generativeadversarial networks,2019, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Traininggenerative adversarial networks with limited data,2020, arXiv preprint arXiv:2006
 Adam: A method for stochastic optimization,2015, In ICLR
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in neural information processing systems
 Im-proved variational inference with inverse autoregressive flow,2016, In Advances in neural informationprocessing systems
 A tutorial onenergy-based learning,2006, In G
 Annealed denoising score matching: Learningenergy-based models in high-dimensional spaces,2019, arXiv preprint arXiv:1910
 Enhancing the reliability of out-of-distribution imagedetection in neural networks,2018, In International Conference on Learning Representations
 Coco-gan: generation by parts via conditional coordinating,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Hybrid discriminative-generative training via contrastive learning,2020, arXivpreprint arXiv:2007
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning Representations
 NeuralImportance Sampling,0730, ACM Trans
 Probabilistic inference using Markov chain Monte Carlo methods,1993, 1993
 Annealed importance sampling,2001, Statistics and computing
 Mcmc using hamiltonian dynamics,2011, Handbook of markov chain monte carlo
 On the anatomy of mcmc-based maximum likelihood learning of energy-based models,2019, arXiv preprint arXiv:1903
 Learning energy-based model with flow-based backbone by neural transportmcmc,2020, arXiv preprint arXiv:2006
 Boltzmann generators: Sampling equilibriumstates of many-body systems with deep learning,0036, Science
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Autoregressive quantile networks for generativemodeling,2018, arXiv preprint arXiv:1806
 Learning latent spaceenergy-based prior model,2020, arXiv preprint arXiv:2006
 Waveflow: A compact flow-based model forraw audio,2020, ICML
 Learning implicit generativemodels with the method of learned moments,2018, arXiv preprint arXiv:1806
 Generating diverse high-fidelity images withvq-vae-2,2019, In Advances in Neural Information Processing Systems
 Variational inference with normalizing flows,2015, InInternational Conference on Machine Learning
 Deep boltzmann machines,2009, In Artificial intelligenceand statistics
 Restricted boltzmann machines forcollaborative filtering,2007, In ICML
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing systems
 Inverse molecular design using machinelearning: Generative models for matter engineering,2018, Science
 Discriminator contrastive divergence:Semi-amortized generative modeling by exploring energy of the discriminator,2020, arXiv preprintarXiv:2004
 Adversarial discriminative domainadaptation,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Undirected graphical models as ap-proximate posteriors,2020, In International Conference on Machine Learning (ICML)
 Notes on contrastive divergence,2006, Department of Engineering Science
 Generative latent flow: A framework for non-adversarialimage generation,2019, arXiv preprint arXiv:1905
 Exponential tilting of generative models: Improvingsample quality by training and sampling from latent energy,2020, arXiv preprint arXiv:2006
 A theory of generative convnet,2016, InInternational Conference on Machine Learning
 Lsun:Construction of a large-scale image dataset using deep learning with humans in the loop,2015, arXivpreprint arXiv:1506
 Training deep energy-based models withf-divergence minimization,2020, ICML
 Inclusive gan: Improvingdata and minority coverage in generative models,2020, arXiv preprint arXiv:2004
 Unpaired image-to-image translationusing cycle-consistent adversarial networks,2017, In Proceedings of the IEEE international conferenceon computer vision
