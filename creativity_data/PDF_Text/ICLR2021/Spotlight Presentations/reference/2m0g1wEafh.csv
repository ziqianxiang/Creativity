title,year,conference
 Fine-grained analysis of optimization and gen-eralization for overparameterized two-layer neural networks,2019, arXiv preprint arXiv:1901
 Beyond linearization: On quadratic and higher-order approximation of wideneural networks,2020, In International Conference on Learning Representations
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETransactions on Information theory
 A generalization theory of gradient descent for learning over-parameterized deepReLU networks,2019, arXiv preprint arXiv:1902
 Optimal rates for regularized least-squares algorithm,2007, Foundationsof Computational Mathematics
 Towards understanding hier-archical learning: Benefits of neural representations,2020, Advances in Neural Information ProcessingSystems
 Sparse optimization on measures with over-parameterized gradient descent,2019, arXivpreprint arXiv:1907
 A note on lazy training in supervised differentiable programming,2018, arXivpreprint arXiv:1812
 Implicit bias of gradient descent for wide two-layer neural networks trainedwith the logistic loss,2020, arXiv preprint arXiv:2002
 Ergodicity for Infinite Dimensional Systems,1996, London MathematicalSociety Lecture Note Series
 Density estimation by waveletthresholding,1996, The Annals of Statistics
 Gradient descent finds global minima of deep neuralnetworks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizes over-parameterizedneural networks,2019, International Conference on Learning Representations 7
 A comparative analysis of optimization and generalization properties oftwo-layer neural network and random feature models under gradient descent dynamics,2019, ScienceChina Mathematics
 Global non-convex optimization with discretized diffu-sions,2018, In Advances in Neural Information Processing Systems 31
 Linearized two-layers neural networks inhigh dimension,2019, arXiv preprint arXiv:1904
 Concentration inequalities and asymptotic results for ratio type empir-ical processes,2006, TheAnnalsofProbabilily
 Implicit bias of gradient descent on linearconvolutional networks,2018, In Advances in Neural Information Processing Systems
 On the minimax optimality and superiority of deep neural networklearning over sparse parameter spaces,0893, Neural Networks
 Universal approximation of an unknown mapping andits derivatives using multilayer feedforward networks,1990, Neural Networks
 Deep neural networks learn non-smooth functions effectively,2019, InK
 Neural tangent kernel: Convergence and generalization inneural networks,2018, In Advances in Neural Information Processing Systems 31
 Serie I,1995, Mathematique
 Risk bounds for high-dimensional ridge function combinationsincluding neural networks,2016, arXiv preprint arXiv:1607
 Local Rademacher complexities and oracle inequalities in risk minimization,2006, TheAnnals of Statistics
 A mean field view of the landscape of two-layer neuralnetworks,2018, Proceedings of the National Academy of Sciences
 Mean-field theory of two-layers neural networks:dimension-free bounds and kernel limit,2019, In A
 Foundations of Machine Learning,2012, The MIT Press
 Dimension-free convergence rates for gradientLangevin dynamics in RKHS,2020, arXiv preprint 2003
 Stochastic particle gradient descent for infinite ensembles,2017, arXiv preprintarXiv:1712
 Parameters as interacting particles: long time convergenceand asymptotic error scaling of neural networks,2018, In Advances in Neural Information ProcessingSystems 31
 Trainability and accuracy of neural networks: An interactingparticle system approach,2019, arXiv preprint arXiv:1805
 Nonparametric regression using deep neural networks with ReLU activationfunction,2020, The Annals of Statistics
 Large deviations for the invariant measure of a reaction-diffusion equation with non-Gaussian perturbations,0178, Probab
 Generalization bound of globally optimal non-convex neural network training: Trans-portation map estimation by infinite dimensional langevin dynamics,2020, In Advances in NeuralInformation Processing Systems 33
 Deep learning is adaptive to intrinsic dimensionality of model smoothnessin anisotropic Besov space,2019, arXiv preprint arXiv:1910
 Bayesian learning via stochastic gradient Langevin dynamics,2011, In ICML
 Kernel and rich regimes in overparametrized models,2020, volume 125 of Proceedings of MachineLearning Research
 Gradient descent optimizes over-parameterized deep ReLUnetworks,2020, Machine Learning
