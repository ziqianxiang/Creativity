title,year,conference
 The second PASCAL recognising textual entailment challenge,2006, In Proceedings ofthe second PASCAL challenges workshop on recognising textual entailment
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 The PASCAL recognising textUal entailmentchallenge,2005, In Machine Learning Challenges Workshop
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Span-bert: Improving pre-training by representing and predicting spans,2020, Transactions of the Associationfor Computational Linguistics
 Reviewing and evaluating auto-matic term recognition techniques,2008, In International Conference on Natural Language Processing
 Race: Large-scale readingcomprehension dataset from examinations,2017, arXiv preprint arXiv:1704
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arxiv preprint arXiv:1907
 Languagemodels are unsupervised multitask learners,2019, arXiv preprint
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 A broad evaluation of techniques forautomatic acquisition of multiword expressions,2012, In Proceedings of ACL 2012 Student ResearchWorkshop
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Empirical Methods in Natural Language Processing (EMNLP)
 ERNIE: Enhanced representation through knowledgeintegration,2019, arXiv preprint arXiv:1904
 Ambert: A pre-trained language model with multi-grained tokeniza-tion,2020, arXiv preprint arXiv:2008
