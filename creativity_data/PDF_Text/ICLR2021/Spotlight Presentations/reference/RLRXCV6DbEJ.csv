title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Generative pretraining from pixels,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Variational lossy autoencoder,2016, arXiv preprint arXiv:1611
 Pixelsnail: An improved autore-gressive generative model,2017, arXiv preprint arXiv:1712
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Diagnosing and enhancing vae models,2019, arXiv preprint arXiv:1903
 Nice: Non-linear independent components esti-mation,2014, arXiv preprint arXiv:1410
 Density estimation using real nvp,2016, arXivpreprint arXiv:1605
 Pixelvae: A latent variable model for natural images,2016, arXiv preprintarXiv:1611
 Flow++: Improving flow-based generative models with variational dequantization and architecture design,2019, arXiv preprintarXiv:1902
 Denoising diffusion probabilistic models,2020, arXiv preprintarxiv:2006
 Learnable explicit density for continuous latent space and varia-tional inference,2017, arXiv preprint arXiv:1710
 Neural autoregressiveflows,2018, arXiv preprint arXiv:1804
 Stochastic gradient vb and the variational auto-encoder,2014, InSecond International Conference on Learning Representations
 An introduction to variational autoencoders,2019, arXiv preprintarXiv:1906
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Im-proved variational inference with inverse autoregressive flow,2016, In Advances in neural informationprocessing systems
 Pixelcnn models with auxiliary variables fornatural image modeling,1905, In International Conference on Machine Learning
 Biva: A very deep hierarchy oflatent variables for generative modeling,2019, In Advances in neural information processing systems
 Generating high fidelity images with subscale pixel networksand multidimensional upscaling,2018, arXiv preprint arXiv:1812
 Wavenet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Normalizing flows for probabilistic modeling and inference,2019, arXiv preprintarXiv:1912
 Image transformer,2018, arXiv preprint arXiv:1802
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Parallel multiscale autoregressive density estimation,2017, arXiv preprintarXiv:1703
 Stochastic backpropagation andapproximate inference in deep generative models,2014, arXiv preprint arXiv:1401
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In Advances in neural information processing systems
 Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications,2017, arXiv preprintarXiv:1701
 Energy and policy considerations for deeplearning in nlp,2019, arXiv preprint arXiv:1906
 Rnade: The real-valued neural autoregressivedensity-estimator,2013, In Advances in Neural Information Processing Systems
 Nvae: A deep hierarchical variational autoencoder,2020, arXiv preprintarXiv:2007
 Con-ditional image generation with pixelcnn decoders,2016, In Advances in neural information processingsystems
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
 Learning hierarchical features from generativemodels,2017, arXiv preprint arXiv:1702
