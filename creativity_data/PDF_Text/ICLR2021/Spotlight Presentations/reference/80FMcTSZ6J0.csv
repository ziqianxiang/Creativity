title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, In International Conference on Machine Learning
 Implicit regularization in deep matrixfactorization,2019, In Advances in Neural Information Processing Systems
 Mixmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 A meta approach to defendnoisy labels by the manifold regularizer psdr,2019, arXiv preprint arXiv:1906
 Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise,2020, arXivpreprint arXiv:2012
 Robustness of ac-curacy metric and its inspirations in learning with noisy labels,2020, arXiv preprint arXiv:2012
 Escaping saddles withstochastic gradients,2018, In International Conference on Machine Learning
 A model for a binary response with misclassifications,1982, InGLIM 82: Proceedings of the international conference on generalised linear models
 Escaping from saddle points—online stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 Robust loss functions under label noise for deepneural networks,2017, In AAAI Conference on Artificial Intelligence
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InAdvances in Neural Information Processing Systems
 Shape matters: Understanding theimplicit bias of the noise covariance,2020, arXiv preprint arXiv:2006
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In Advances in Neural Information ProcessingSystems
 Flat minima,1997, Neural Computation
 Mentornet: Learning data-driven curriculum for very deeP neural networks on corruPted labels,2018, In International Conferenceon Machine Learning
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2016, arXivpreprint arXiv:1609
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InInternational Conference on Learning Representations
 TemPoral ensembling for semi-suPervised learning,2017, In InternationalConference on Learning Representations
 Visualizing the loss land-scaPe of neural nets,2018, In Advances in Neural Information Processing Systems
 Learning to learn from noisylabeled data,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 Dividemix: Learning with noisy labels as semi-suPervised learning,2020, In International Conference on Learning Representations
 Curriculum loss: Robust learning and generalization against labelcorruPtion,2020, In International Conference on Learning Representations
 DecouPling “when to uPdate” from “how to uPdate”,2017, InAdvances in Neural Information Processing Systems
 Virtual adversarial training: aregularization method for suPervised and semi-suPervised learning,2018, IEEE Transactions on PatternAnalysis and Machine Intelligence
 Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints,2018, In Conference on Learning Theory
 Learning withnoisy labels,2013, In Advances in Neural Information Processing Systems
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Self: learning to filter noisy labelswith self-ensembling,2020, In International Conference on Learning Representations
 Understanding the exploding gradient prob-lem,2012, CoRR
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In IEEE Conference onComputer Vision and Pattern Recognition
 Training deep neural networks on noisy labels with bootstrapping,2015, In InternationalConference on Learning Representations
 Classification with asymmetric label noise:Consistency and maximal denoising,2013, In Conference on Learning Theory
 Rethinkingthe inception architecture for computer vision,2016, In IEEE Conference on Computer Vision andPattern Recognition
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in Neural InformationProcessing Systems
 Implicit regularization for optimalsparse recovery,2019, In Advances in Neural Information Processing Systems
 Symmetric crossentropy for robust learning with noisy labels,2019, In IEEE International Conference on ComputerVision
 Combating noisy labels by agreement: A jointtraining method with co-regularization,2020, In IEEE Conference on Computer Vision and PatternRecognition
 Kernel and rich regimes in overparametrized models,2020, arXivpreprint arXiv:2002
 On thenoisy gradient descent that generalizes as sgd,2020, In International Conference on Machine Learning
 Parts-dependent label noise: Towards instance-dependentlabel noise,2020, In Advances in Neural Information Processing Systems
 Disturblabel: Regularizing cnnon the loss layer,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Learning frommUltiple annotators with varying expertise,2014, Machine learning
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference
 Analysis of gradient clipping andadaptive scaling with a relaxed smoothness condition,2019, arXiv preprint arXiv:1905
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in Neural Information Processing Systems
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
