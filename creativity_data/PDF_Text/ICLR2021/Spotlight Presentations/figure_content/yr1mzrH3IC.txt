Figure 1: Return vs. timesteps, for four algorithms (columns) and four environments (rows).
Figure 2: Final return vs. single hyperparameter change. "Rollout Timesteps" refers to the number of state-actionsamples used for training between policy updates.
Figure 3: Return with different amount of training samples with error bars from 10 random seeds. Regularizedmodels can reach similar performance as baseline with less data, showing their stronger generalization ability.
Figure 4: Return distribution (frequency vs. return value) over 100 trajectories. Regularized models generalizeto unseen samples more stably with high return.
Figure 5: Return, policy networkL2 norm, and policy entropy forPPO Humanoid.
Figure 6: Quantile-Quantile (Q-Q) plot for our z-score metric on all environments under the default hyperparame-ter setting in Section 3. As an example, in the first figure, the x-axis denotes the theoretical quantile for the normaldistribution with mean and standard deviation from {freg(e, s) : e ∈ E, s ∈ S} when we apply L2 regularizationon TRPO. The blue points denote the actual quantiles of {freg (e, s) : e ∈ E, s ∈ S}. The red line denotes whatthe quantile-quantile relation looks like if {freg (e, s) : e ∈ E, s ∈ S} were perfectly normal. We observe thatthe blue points are close to the red line, suggesting that the distribution of {freg (e, s) : e ∈ E, s ∈ S} is close toa normal distribution. Similarly, we observe that {fbaseline(e, s) : e ∈ E, s ∈ S} is close to normal.
Figure 7: Quantile-Quantile (Q-Q) plot for our z-score metric on all environments under the 5 sampledhyperparameters in Section 4. We observe that the blue points are close to the red line, suggesting that thedistribution of {freg (h, e, s) : h ∈ H, e ∈ E, s ∈ S} is close to a normal distribution. Similarly, we observethat {fbaseline (h, e, s) : h ∈ H, e ∈ E, s ∈ S} is close to normal.
Figure 8: The effect of combining L2 regularization with entropy regularization. For PPO HumanoidStandup,we use the third randomly sampled hyperparameter setting. For A2C HumanoidStandup and TRPO Ant, we usethe baseline as in Section 3.
Figure 9: Comparison between L2 regularization and weight decay. For PPO Humanoid and HumanoidStandup,we use the third randomly sampled hyperparameter setting.
Figure 10: Return vs. timesteps, for four algorithms (columns) and five environments (rows).
Figure 11: Training curves of A2C regularizations under five randomly sampled hyperparameters.
Figure 12: Training curves of TRPO regularizations under five randomly sampled hyperparameters.
Figure 13: Training curves of PPO regularizations under five randomly sampled hyperparameters.
Figure 14: Training curves of SAC regularizations under five randomly sampled hyperparameters.
Figure 15: The interaction between policy and value network regularization for A2C. The optimal policyregularization and value regularization strengths are listed in the legends. Results of regularizing both policy andvalue networks are obtained by combining the optimal policy and value regularization strengths.
Figure 16: The interaction between policy and value network regularization for TRPO.
Figure 17: The interaction between policy and value network regularization for PPO.
Figure 18:The interaction between policy and value network regularization for SAC.
