Figure 1: Generalization bounds throughout distillation. These two subfigures track a sequenceof increasingly distilled/compressed ResNet8 networks along their horizontal axes, respectively forcifar10 and mnist data. This horizontal axis measures distillation distance Φγ,m, as definedbelow in eq. (1.1). The bottom curves measure various training and testing errors, whereas thetop two curves measure respectively a generalization bound presented here (cf. Theorem 1.3 andLemma 3.1), and a generalization measure. Notably, the top two curves drop throughout a longinterval during which test error remains small. For further experimental details, see Section 2.
Figure 2: Performance of stable rank bound (cf. Theorem 1.4). Figure 2a compares Theorem 1.4to Lemma 3.1 and the VC bound (Bartlett et al., 2017b), and Figure 2b normalizes the marginhistogram by Theorem 1.4, showing an unfortunate failure of width independence (cf. Figure 3).
Figure 3: Width independence. Fully-connected 6-layer networks of widths {64, 256, 1024} weretrained on mnist until training error zero; the margin histograms, normalized by the generalizationbound in Lemma 3.1, all differ, and are close to zero. After distillation, the margin distributionsare far from zero and nearly the same. In the distillation legend, the second term Φγ,m denotes thedistillation distance, as defined in Equation (1.1). Experiment details and an explanation of marginhistograms appear in Section 2.
Figure 4: Label randomization. Here {0%, 25%, 50%, 75%, 100%} of the labels were permutedacross the respective experiments. In all cases, the margin distribution is collapsed to zero. Fordetails, including an explanation of margin histograms, see Section 2.
Figure 1: effect of distillation on generalization bounds. This figure was described before;briefly, a highlight is that in the initial phase, training and testing errors hardly change while boundsdrop by a factor of nearly 1010 . Regarding “generalization measure”, this term appears in studiesof quantities which correlate with generalization, but are not necessarily rigorous generalizationbounds (Jiang et al., 2019b; Dziugaite et al., 2020); in this specific case, the product of Frobeniusnorms requires a dense ReLU network (Golowich et al., 2018), and is invalid for the ResNet (e.g., acomplicated ResNet with a single identity residual block yields a value 0 by this measure).
Figure 2a: comparison of Theorem 1.4, Lemma 3.1 and VC bounds. Theorem 1.4 was intendedto internalize distillation, but as in Figure 2a, clearly a subsequent distillation still greatly reduces thebound. While initially the bound is better than Lemma 3.1 (which does not internalize distillation),eventually the n1/4 factor causes it to lose out. Also note that eventually the bounds beat the VCbound, which has been identified as a surprisingly challenging baseline (Arora et al., 2018).
Figure 3:	width independence. Prior work has identified that generalization bounds are quitebad at handling changes in width, even if predictions and test error don’t change much (Nagarajan& Kolter, 2019; Jiang et al., 2019b; Dziugaite et al., 2020). This is captured in Figure 3a, wherethe margin distributions (see above) with different widths are all very different, despite similar testerrors. However, following distillation, the margin histograms in Figure 3b are nearly identical!That is to say: distillation not only decreases loose upper bounds as before, it tightens them to thepoint where they capture intrinsic properties of the predictors.
Figure 2b: failure of width independence with Theorem 1.4. The bound in Theorem 1.4 wasdesigned to internalize compression, and there was some hope of this due to the stable rank term.
Figure 4:	random labels. A standard sanity check for generalization bounds is whether they canreflect the difficulty of fitting random labels (Zhang et al., 2016). While it has been empiricallyshown that Rademacher bounds do sharply reflect the presence of random labels (Bartlett et al.,2017a, Figures 2 & 3), the effect is amplified with distillation: even randomizing just 25% shrinksthe margin distribution significantly.
