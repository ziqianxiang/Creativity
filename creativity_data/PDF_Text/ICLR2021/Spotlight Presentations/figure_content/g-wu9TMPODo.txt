Figure 1: Label noise in CIFAR10 and MNIST. Text above the image indicates the training set label.
Figure 2: Adversarial Error increases with increasing label noise Î·. Shaded region indicates 95%confidence interval. Absence of shaded region indicates that it is invisible due to low variance.
Figure 3: Two dimensional PCA projections of the original correctly labelled (blue and orange),original mis-labelled (green and red), and adversarial examples (purple and brown) at different stagesof training. The correct label for True 0 (blue), Noisy 0 (green), Adv 0 (purple +) are the same i.e. 0and similar for the other class.
Figure 4: Each pair is a training (left) and test (right) image mis-classified by the adversarially trainedmodel. They were both correctly classified by the naturally-trained model.
Figure 5: The blue represents the points mis-classified by an adversarially trained model. The orangerepresents the distribution for all points in the dataset (of the concerned class for CIFAR10).
Figure 6: Visualization of the distribution and classifiers used in the Proof of Theorem 2 and 3. TheRed and Blue indicate the two classes.
Figure 7: Adversarial training (AT) leads to larger margin, and thus adversarial robustness aroundhigh density regions (larger circles) but causes training error on low density sub-populations (smallercircles) whereas naturally trained models (NAT) minimizes the training error but leads to regionswith very small margins.
Figure 8: Decision boundaries of neural networks are much simpler than they should be.
Figure 9: Assigning a separate class to each sub-population within the original class during trainingincreases robustness by learning more meaningful representations.
