Figure 1: The magnitudes (y-axis) of channel-wise activation at the penultimate layer (512 channelsat x-axis) for both standard (‘STD’) and adversarially trained (‘ADV’) models. In each plot, themagnitudes are averaged and displayed separately for natural and adversarial test examples. The512 channels are sorted in a descending order of the magnitude.
Figure 2: The activation frequency (y-axis) of channel-wise activation at the penultimate layer (512channels at x-axis) of ResNet-18 trained using (a) standard training (‘STD’), (b) adversarial train-ing (‘ADV’), and (c) our CAS-based adversarial training (‘CAS’). The activation frequencies arecounted separately for the natural test examples and their PGD-20 adversarial examples. Channelsare sorted in a descending order of activation frequency of natural examples.
Figure 3: Framework of our proposed Channel-wise Activation Suppressing (CAS).
Figure 4: Comparisons of aCtivation frequenCy distribution between adversarial and natural exam-ples on different aCtivation or feature oriented defense methods (kWTA, SAP, PCL and our CAS).
Figure 5:	The distributions of channel activation frequency of both natural and adversarial examplesin different defense models (e.g. TRADES and MART). The frequency distribution gap betweennatural and adversarial examples is effectively narrowed down by our CAS training, and the redun-dant channels (channel #200 - #512) are significantly suppressed by CAS.
Figure 6:	The distributions of channel activation frequency of both natural and PGD-20 adversarialexamples for ResNet-18 on SVHN.
Figure 7:	The diStributionS of channel activation frequency of both natural and PGD-30 adverSarialexampleS for ReSNet-152 on ImageNet.
Figure 8: Robustness of AT+CAS against white-box attacks FGSM, PGD-20 and CW∞ under dif-ferent β. As β increases, the robustness is also improved, especially against the CW∞ attack.
Figure 9:	The t-SNE 2D embeddings of deep features extracted at the penultimate layer of ResNet-18 models trained using natural (‘STD’) or adversarial training (‘ADV’) on CIFAR-10. The em-beddings are shown separately for natural versus adversarial examples. Our CAS training can helpimprove inter-class separation and intra-class compactness for both types of training.
Figure 10:	The activation frequency (y-axiS) of channel-wiSe activation at the penultimate layerof adversarial training (AT) and our CAS-based adversarial training (AT+CAS) for ResNet-18 onCIFAR-10. The activation frequencies are visualized with respect to different thresholds (e.g. 0.5%,1% and 5%).
