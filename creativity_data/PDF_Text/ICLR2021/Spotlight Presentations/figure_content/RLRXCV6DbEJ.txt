Figure 1: Selected samples from our very deep VAE on FFHQ-256, and a demonstration ofthe learned generative process. VAEs can learn to first generate global features at low resolution,then fill in local details in parallel at higher resolutions. When made sufficiently deep, this learned,parallel, multiscale generative procedure attains a higher log-likelihood than the PixelCNN.
Figure 2: Different possible learned generative models in a VAE. Left: A hierarchical VAE canlearn an autoregressive model by using the deterministic identity function as an encoder, and learningthe autoregression in the prior. Right: Learning the encoder can lead to efficient hierarchies of latentvariables (black). If the bottom group of three latent variables is conditionally independent given thefirst, they can be generated in parallel within a single layer, potentially leading to faster sampling.
Figure 3: A diagram of our top-down VAE architecture. Residual blocks are similar to bottleneckResNet blocks (He et al., 2016). Each convolution is preceded by the GELU nonlinearity (Hendrycks& Gimpel, 2016). qφ(.) and pθ(.) are diagonal Gaussian distributions. z is sampled from qφ(.)during training, andpθ(.) when sampling. We use average pooling and nearest-neighbor upsamplingfor pool and unpool layers.
Figure 4: Cumulative percentage of latent variables at a given resolution, and reconstructionsof samples on FFHQ-256. We sample latent variables from the approximate posterior until thegiven resolution, and sample the rest from the prior at low temperature. This shows what imagesare likely given a subset of latent variables. Low-resolution latents comprise a small fraction of thetotal latents, but encode significant portions of the global structure. This suggests deep VAEs learnefficient, hierarchical representations of the data.
Figure 5: Relationship between architecture and posterior collapse. We visualize the cumulativeKL divergence (or “rate”, in bits per dimension) for several different architectures across a 73 layernetwork on ImageNet-32. When residual connections are removed from the “res block” in the top-down path (Figure 3), the model encodes no information in the first 45 layers of the network and theloss is highest (”FFN”). When a learned convolutional upsampler is used as the “unpool” layer, thefirst 13 layers of the network encode no information. When nearest-neighbor upsampling is used,the first layers all encode information, and the loss is the lowest.
Figure 6: Effect of gradient skipping. We plot the max gradient norm encountered per 500 updatesfor our best models across datasets. The dashed black line indicates the “skip threshold”, or valueabove which the update is skipped. We choose a high threshold that affects fewer than 0.01 percentof training updates. Without this skip heuristic, networks will diverge when extreme updates areencountered. These updates can have norm as high as 1e15.
Figure 7: Non-cherrypicked, temperature 1.0 samples on FFHQ-256. Cover images were eachcherrypicked from a batch of 16 (unadjusted temperature) samples. Here we show a random batchof 16 images for comparison.
Figure 8: Non-cherrypicked, temperature 0.85 samples on FFHQ-256. Lower temperature sam-ples result in greater regularity in images.
Figure 9: Non-cherrypicked, temperature 0.60 samples on FFHQ-256. We visualize temperature0.60 samples for comparison with Vahdat & Kautz (2020)Table 4: Key hyperparameters for experiments. We detail here the main hyperparameters used intraining. FFHQ-1024 has reduced hidden size for higher resolutions; see code for details.
Figure 10: ImageNet-32 (left) and ImageNet-64 (right) reconstructions and samples. Recon-structions of validation images from various stages in the latent hierarchy (top), and unconditionalsamples from the model at temperature 1.0 (bottom).
Figure 11: FFHQ-1024 samples. These are generated with reduced temperature (top) and tempera-ture 1.0 (bottom). The model we train has similar capacity to smaller ones we use on 32x32, 64x64,and 256x256 images, and so fails to capture the intricacies of this more complex distribution well.
