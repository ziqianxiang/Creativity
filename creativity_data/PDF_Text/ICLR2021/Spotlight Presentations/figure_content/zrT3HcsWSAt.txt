Figure 1: A toy example of the weighted action sampling procedure at each iteration in our algorithmwhen given a state S ∈ S∏+ *. On both rows, the horizontal lines are the action domains. The leftand right dotted lines on the top row describe π ∈ Π \ {π* } and π* (a|s), respectively. The dottedlines on the bottom row describe the mixture distribution πe(a∣s) = eπ(a|s) + (1 — 6)π*(a|s) with= 0.4. The solid lines on the top row describe πθ (a|s) that are optimized with objective (8) ateach iteration. The solid lines on the bottom row describe distributions which draw actions, that werealready drawn by πe(a∣s) in the noisy demonstrations, according to the current importance weightπθ (a|s) at each iteration. πθ (a|s) are optimized at each iteration so that the weighted distribution atthe previous iteration is the target distribution.
Figure 2: (A)-(C) The performance of policies vs. given 50000 state-action pairs of the noisyexpert demonstrations where the non-optimal policies π ∈ Π \ {π* } are (A) U(-u, u), (B) N(a*, I)with a 〜π* (∙∣s), and (C) the deterministic one a = 0, respectively. (D) The performance of policiesvs. the number of state-action pairs N of the noisy demonstrations with e = 0.3 where π(a|s)=U (-u, u). BC-Single is a policy learned by BC. BC-Ensemble is an ensemble of policies, each ofwhich was learned by BC. Shaded regions indicate the standard deviation over five experiments.
Figure 3: The performance of policies vs. given 50000 state-action pairs of the noisy expertdemonstrations where the non-optimal policies π ∈ Π \ {π*} select actions uniformly at random.
