Figure 1: Overview of Offline RL via DAC-MDPs. Given a static experience dataset, we first compile it intoa finite tabular MDP which is at most the size of the dataset. This MDP contains the “core” states of the fullcontinuous DAC-MDP. The finite core-state MDP is then solved via value iteration, resulting in a policy andQ-value function for the core states. This finite Q-function is used to define a non-parametric Q-function forthe continuous DAC-MDP, which allows for Q-values and hence a policy to be computed for previously unseenstates.
Figure 3: Results on Atari 100K (left)BCQ (right) DQN. Each agent is trained for 100K iterations(trainingsteps), and evaluated on 10 episodes every 10K steps. At each of these evaluation checkpoints, we use theinternal representation to compile DAC-MDPs. We then evaluate the DAC-MDPs for Ne = 6. Runs averagedover 5 seeds and error bars plot the 95% confidence interval.
Figure 5: Atari results for 2.5M dataset. We show the final performance of BCQ and DQN trained for 2.5Miterations. We also use the same representation for the DAC-MDPs named as DAC-BCQ and DAC-DQNrespectively. All DAC-MDPs are evaluated with Ne = 6.
Figure 6: Agent and top view for 3D Navigation domains. (left) Simple Room, (center) Box and Pillar Roomand (right) Tunnel Room.
Figure 8: (a) Ablation study for WA and SKNN in CartPole Domain. Greedy and eps-greedy policy returns fordifferent sets of hyperparameters and dataset versions of size 100k. (left) and 10K (right) Hyperparameters:[k = 5,k∏ = 11,C = 1, Ne = 1]Average Return vs Cost Parameter COooo302010(I..onω) XO=Od APəalo, Q・ 1θ-031e+031e+051θ+07Average return vs MDP build parameter kS300IlQJ 200of10°Φ'0
Figure 9: EPS-Greedy performance for CartPole on different dataset sizes.
Figure 10:	Greedy performance for CartPole on different dataset sizes. (a) Top row: datset SiZe 10k (b) BottomRow: dataset size 50kis able to benefit by reasoning about the stochastic transitions of the DAC-MDP for k > 1. However,DAC-MDP suffers from high values of k when dataset is sparse.
Figure 11:	(a) Greedy performance for Atari using different learnt representations and evaluation candidatepolicies Ne[100k dataset]. Runs averaged over 3 runs. Error bars show the 95% confidence interval.
Figure 12: (a) Greedy performance for Atari using different learnt representations and evaluation candidatepolicies Ne [2.5M dataset]not spared from hyperparameter search and stopping criteria. Hence it is not clear how to evaluateQ-iterating policies such as DQN or BCQ for different values of Ne where we are already using thebest parameters reported from previous works.
Figure 13: Results on Medium dataset size of 1M for (a) BCQ representation and BCQ agent is trained for1m timesteps, and evaluated on 10 episodes every 50k steps. At each of 6 uniformly distributed evaluationcheckpoints, we use the internal representation to compile DAC-MDPs. We then evaluate the DAC-MDPs forNe = 6. (b) Final BCQ policy along with the corresponding performance of DAC-BCQ for different values ofNe . Runs averaged over 3 runs, error bars so the 95% confidence interval.
Figure 14: Comparison of DAC-MDPs using randomly initialized representation networks (DAC-RND) withbaselines DQN and BCQ for 3D navigation Domains. (a) Simple Room Domain, (b) Simple Room Domain:DQN vs DAC-RND on right actuator failure[LF] (c) BCQ vs DAC-RND on right actuator failure[LF](d) Boxand Pillar Room: short horizon, maximum episode length of 20, (e) Box and Pillar Room: Long horizon, max-imum episode lengths of 100, (f) Tunnel Room. Rewards across the rooms are clipped at (-1,2) for normalizedresults.
Figure 15: Visualization of DAC-MDP policies for 3D Navigation Domain (a) Simple Room Solid arrow: pol-icy rollout of standard DAC-MDP. Dotted arrow: policy rollout of modified DAC-MDP ;Right Turn Penalized .
Figure 16: Compares the performance results for serial VI solvers With its GPU optimized implementation.
