Figure 1: An illustration of the hard instance. Recall that dthe first level (h = 1), while states at the bottom are those in the last level (h = H). Solid line(with arrow) corresponds to transitions associated with action a1, while dotted line (with arrow)corresponds to transitions associated with action a2. For each level h ∈ [H], reward values andQ-values associated with s，sh,..., Sh are marked on the left, while reward values and Q-valuesassociated with s∕+1 are mark on the right. Rewards and transitions are all deterministic, except^	^for the reward distributions associated with SH, SH,..., SH. We mark the expectation of the rewardvalue when it is stochastic. For each level h ∈ [H], for the data distribution μh, the state is chosenuniformly at random from those states in the dashed rectangle, i.e., {s}, Sh,..., sdι}, while the action^is chosen uniformly at random from {α1, a2}. Suppose the initial state is sd+ . When ro = 0, thevalue of the policy is 0. When ro = d-H/2, the value of the policy is r0 ∙ 2H/2 = 1.
Figure 2: An illustration of the hard instance. Recall that d = d/2 - 1. States on the top are thosein the first level (h = 1), while states at the bottom are those in the last level (h = H). Dottedline (with arrow) corresponds to transitions associated with actions α1,α2,…,a^, while solid line(with arrow) corresponds to transitions associated with actions af^+1, a^+2, ∙∙∙,ad. We omit thetransition associated with a1,α2, ∙∙∙,α^ in the figure if all actions give the same transition. Foreach level h ∈ [H], Q-values associated with s1, ∙⅛.∙∙, s，, s，s- are marked on the left, whiletransition distributions and Q-values associated with s，+1 are marked on the right. Rewards are alldeterministic, and the only two states (SH and SH) with non-zero reward values are marked in blackand grey. Consider the fixed policy that returns ad for all input states. When r0 = 0, the value of thepolicy is 0. When p = 2-(H-2)/2, the value of the policy is = p疝H-2)/2 = 1.
