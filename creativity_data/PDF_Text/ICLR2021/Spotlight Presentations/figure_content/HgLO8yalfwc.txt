Figure 1: Bregman divergence Dω(∏∣∣∏e) associated with Tsallis entropy (Ω(p) = -Tq(p))between two uni-variate Gaussian distributions π = N(μ, σ2) and ∏ = N(0, (e-3)2) (green pointin each subplot). In each subplot, we normalized the Bregman divergence so that the maximum valuebecomes 1. Note that for q = 1, Dω(∏∣∣∏e) becomes the KL divergence KL(π∣∣πe).
Figure 2: Expert policy (Left) and reward learned by RAIRL with different types of policy regularizers(Right) in Multi-armed Bandit. Either one of dense (Top row) or sparse (Bottom row) expert policiesπE is considered.
Figure 3: Mean Bregman divergence during training (Top row) and the divergence during rewardevaluation (Bottom row) in Bermuda World. In each column, different policy regularizers and theirrespective target divergences are considered. The results are reported after normalization with thedivergence of uniform random policy, and that of behavioral cloning (BC) is reported for comparison.
Figure 4: Averaged episodic score of RAIRL’s training in MuJoCo environments. RAIRL with Tq1regularizer with q = 1, 1.5, 2 is considered.
Figure 5: Bregman divergences with Tsallis entropy Tq10 with q0 = 1, 1.5, 2 during RAIRL’s trainingin MuJoCo environments. We consider RAIRL with Tsallis entropy regularizer Tq1 with q = 1, 1.5, 2.
Figure 6: Density-based model for discrete (Left) and continuous control (Right)J.3 Expert in B ermuda World environmentWe assume a stochastic expert defined byπE (a|S)Pt3=1(d(t))-1I{a = Proj(θ(t))}P3=ι(d⑴)-1θ(t) = arctan2(y(t) — y, x(t) — x), d(t) = ∣∣s(t) - s∣∣2 + e, t = 1, 2, 3,24Published as a conference paper at ICLR 2021for S = (x, y),s(1) = (X(I),y⑴)=(-5,10), S⑵=(X⑵,y(2)) = (0,10), S⑶=(X⑶，y⑶)=(5, 10), = 10-4 and an operator Proj(θ) : R → A that maps θ to the closest angle in A. In Figure 7,we depicted the expert policy.
Figure 7: Visualization of the expert policyJ.4 MuJoCo experiment settingInstead of directly using MuJoCo environments with tanh-squashed policies proposed in Soft Actor-Critic (SAC) (Haarnoja et al., 2018), we move tanh to a part of the environment—named hyperbolizedenvironments in short—and assume Gaussian policies. Specifically, after an action a is sampled fromthe policies, we pass tanh(a) to the environment. We then consider multi-variate Gaussian policyπHs) = N (μ(s), ς(S))With μ(s) = [μι(s),…,μd(s)]T, Σ(s) = diag{(σι(s))2,…,(σq(s))2}, where一arctanh(0.99) ≤ μi(s) ≤ arctanh(0.99),log(0.01) ≤ logσi(s) ≤ log(2)for all i = 1, ..., d. Instead of using clipping, we use tanh-activated outputs and scale them to fit inthe above ranges, which empirically improves the performance. Also, instead of using potential-basedreward shaping used in AIRL (Fu et al., 2018), we update the moving mean of intermediate rewardvalues and update the value network with mean-subtracted rewards—so that the value network getsapproximately mean-zero reward—to stabilize the RL part of RAIRL. Note that this is motivated byLemma 2 from which we can guarantee that any constant shift of reward functions does not changeoptimality.
