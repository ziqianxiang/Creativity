Figure 1: Contrastive divergence as an adversarial process. In the first step, the distribution model isused to generate an MCMC process which is used to generate a chain of samples. In the second stepthe distribution model is updated using a gradient descent step, using the MCMC transition rule.
Figure 2: From CNCE to CD-1. (a) In CNCE, each contrastive sample is generated using a fixedconditional distribution q(∙∣∙) (which usually corresponds to additive noise). The real and fake Sam-ples are then concatenated and presented to a discriminator in a random order, which is trained topredict the correct order. (b) CD-1 can be viewed as CNCE with a q(∙∣∙) that corresponds to thetransition rule of a Markov chain with stationary distribution pθ . Since q depends on pθ (hence thesubscript θ), during training the distribution of contrastive samples becomes more similar to that ofthe real samples, making the discrimination task harder.
Figure 3: A toy example illustrating the importance of the adversarial nature of CD. Here, thedata lies close to a 2D spiral embedded in a 10-dimensional space. (a) The training samples inthe first 3 dimensions. (b) Three different approaches for learning the distribution: CNCE withlarge contrastive variance (top), CNCE with small contrastive variance (middle), and CD based onLangevin dynamics MCMC with the weight adjustment described in Sec. 3.4 (bottom). As can beseen in the first two columns, CD adapts the contrastive samples according to the data distribution,whereas CNCE does not. Therefore, CNCE with large variance fails to learn the distribution becausethe vast majority of its contrastive samples are far from the manifold and quickly become irrelevant(as indicated by the weights αθ in the third column). And CNCE with small variance fails to learnthe global structure of the distribution because its contrastive samples are extremely close to thedataset samples. CD, on the other hand, adjusts the contrastive distribution during training, so as togenerate samples that are close to the manifold yet traverse large distances along it.
Figure 4: Here, we use different CD configurations for learning the model of Fig. 3. All configu-rations use Langevin dynamics as their MCMC process, but with different ways of compensatingfor the lack of detailed balance. From left to right we have the ground-truth density, CD w/o anycorrection, CD with Metropolis-Hastings rejection, and CD with our proposed adjustment.
Figure 5: The architecture.
Figure 6: The step size sweep. In the case of CNCE, the step size is in fact the STD of the conditionaldistribution. For the case of CD, the training has diverged for large step sizes even after the learningrate has been significantly reduced. The highlighted figures indicate the selected step sizes.
Figure 7: Selecting the number of training steps.
