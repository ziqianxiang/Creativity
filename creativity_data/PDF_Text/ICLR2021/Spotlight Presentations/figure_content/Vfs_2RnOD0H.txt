Figure 1: (Top) Pseudocode for DTR's basic logic (independent of heuristic), and (Bottom) DTR'ssequence of events in an operator call. Note that PerformOp () may make further recursive calls inorder to rematerialize arguments.
Figure 2: Simulated results comparing different heuristics on various models, showing the rate ofcomputational slowdown for different budgets (fractions of the original peak memory usage). Theblack area in each graph corresponds to the memory required to store inputs and weights, while thegray area denotes the single operator requiring the most memory to be live at once. The dashed anddotted lines represent the last ratio before thrashing (≥ 2× slowdown) and out-of-memory errors,respectively. All logs were produced by running each model 50 times on a single input on a machinewith an NVIDIA Titan V GPU (CUDA 10.1, CuDNN 7.6.4) and a 16-core AMD Ryzen Threadripper1950X on Ubuntu 18.04, logging the final “warmed-up” run.
Figure 3: DTR’s overhead from operators is competitive with Checkmate’s, which uses ILP toproduce an optimal rematerialization schedule. This comparison extends Figure 5 in Jain et al.
Figure 4: We profiled the running time of our prototype for various models and memory budgetson a machine with an NVIDIA Titan V GPU (CUDA 10.1, CuDNN 7.6.4) and a 16-core AMDRyzen Threadripper 1950X on Ubuntu 18.04. The red dotted lines correspond to trials that eitherran out of memory or thrashed (≥ 2× unmodified PyTorch’s time). Model batch sizes are givenin parentheses. To ensure the accuracy of the DTR prototype’s profiling, we used PyTorch’ssynchronous computation mode (see Appendix E.1). Results (mean of 100 trials) are comparedagainst unmodified PyTorch. “Cost compute” (computing heuristic scores) and “eviction loop”(comparing scores over tensors) correspond to overhead from the DTR runtime itself, which can bereduced by a more efficient implementation. “Unprofiled time” is the remainder of the time per batch;it may be due to runtime overhead from parts of PyTorch not modified in the prototype, like theoperator dispatch system. The large proportion of unprofiled time in Unrolled GAN is likely due toits extensive use of Python reflection. The budgets with asterisks were run with the random samplingoptimization (see Appendix E.2) disabled, as sampling caused occasional failures at those budgets.
Figure 5: Visualization of the state of memory for DTR with N = 200, B = 2 d√∕N], and heuristiche*. A value of 0 (black) indicates the tensor is evicted or banished, 1 (red) indicates the tensor is aforward value in memory, and 1.5 (white) denotes an in-memory gradient tensor corresponding to theforward tensor. The backward pass begins at the red vertical line; note the presence of evenly spacedcheckpoint tensors (red horizontal lines) that persist in memory throughout the backward pass. Notealso the recursive checkpointing behavior visible in the early gaps of the backward pass, and finallythe completely red triangles of the later gaps, when there is enough free memory to avoid repeatedrematerialization altogether.
Figure 6: An example construction of an adversarial graph. Gray tensors are in memory (t0 mustalways be in memory). The initial tensor t0 has B paths descending from it, so there is always somepath from t0 with no resident tensors. The adversarial construction chooses to place the next node atthe end of such an entirely evicted path.
Figure 7: Results for fixed C = e*, varying S and m.
Figure 8: Results for fixed c = EqClass, varying s and m.
Figure 9: Results for fixed c = local, varying s and m.
Figure 10: Results for fixed c = no, varying s and m.
Figure 11: Results for the hDTR heuristic, comparing banishing and eager evictions.
Figure 12: Total storages accesses incurred by heuristic evaluations and metadata maintenance,compared across different memory ratios, for the 3 main h0DTR variants.
