Figure 1: (left) Error rate reduction obtained by switching to SAM. Each point is a different dataset/ model / data augmentation. (middle) A sharp minimum to which a ResNet trained with SGDconverged. (right) A wide minimum to which the same ResNet trained with SAM converged.
Figure 2: Schematic of the SAM param-eter update.
Figure 3: (left) Evolution of the spectrum of the Hessian during training of a model with standardSGD (lefthand column) or SAM (righthand column). (middle) Test error as a function of œÅ for dif-ferent values of m. (right) Predictive power of m-sharpness for the generalization gap, for differentvalues of m (higher means the sharpness measure is more correlated with actual generalization gap).
Figure 4: Training and test error for the firstand second order version of the algorithm.
Figure 5: Cosine similarity between the first andsecond order updates.
Figure 6: Test accuracy for a wide resnet trained on CIFAR10 with SAM, for different perturbationnorms.
Figure 7: Evolution of max L(w + ) - L(w) vs. training step, for different numbers of innerprojected gradient steps.
