Figure 1: Sparsity: Evolution over search phase epochs of the average entropy of the operation-weights for GAEA and approaches it modifies when run on the DARTS search space (left), NAS-Bench-1Shot1 Search Space 1 (middle), and NASBench-201 on CIFAR-10 (right). GAEA reducesentropy much more quickly, allowing it to quickly obtain sparse architecture weights. This leads toboth faster convergence to a single architecture and a lower loss when pruning at the end of search.
Figure 2: NAS-Bench-1Shot1: Online comparison of PC-DARTS and GAEA PC-DARTS in termsof the test regret at each epoch of shared-weights training, i.e. the difference between the ground truthtest error of the proposed architecture and that of the best architecture in the search space. The darklines indicate the mean of four random trials and the light colored bands Â± one standard deviation.
Figure 3: The best normal and reduction cells found by GAEA PC-DARTS on CIFAR-10 (top) andImageNet (bottom).
Figure 4: NAS-Bench-201: Learning Curves. Evolution over search phase epochs of the best archi-tecture according to the NAS method. DARTS (first-order) converges to nearly all skip connectionswhile GAEA is able to suppress overfitting to the mixture relaxation by encouraging sparsity inoperation weights.
Figure 5: NAS-Bench-201: XNAS Learning Curves. Evolution over search phase epochs of thebest architecture according 4 runs of XNAS. XNAS exhibits the same behavior as DARTS andconverges to nearly all skip connections.
