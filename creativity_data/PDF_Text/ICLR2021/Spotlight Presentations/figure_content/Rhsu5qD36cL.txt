Figure 1: Conceptual figure explaining the SPRT. The SPRT calculates the log-likelihood ratio (LLR) of two competing hypotheses and updatesthe LLR every time a new sample (x(t) at time t) is acquired, until the LLR reaches one of the two thresholds. For data that is easy tobe classified, the SPRT outputs an answer after taking a few samples, whereas for difficult data, the SPRT takes in numerous samplesin order to make a “careful” decision. For formal definitions and the optimality in early classification of time series, see Appendix A.
Figure 2: Conceptual diagram of neural network for the SPRT-TANDEM where the order of approximation N = 1. The feature extractor(red) extracts the feature vector for classification and outputs it to the temporal integrator (blue). Note that the temporal integratormemorizes up to N preceding states in order to calculate the TANDEM formula (Equation (4)). LLR is calculated using the estimatedprobability densities that are output from the temporal integrator. We use ∙ to highlight a quantity estimated by a neural network.
Figure 3: Experimental results. (a-c) Speed-accuracy tradeoff (SAT) curves for three databases: NMNIST, UCF, and SiW. Note that onlyrepresentative results are shown. Error bars show the standard error of the mean (SEM). (d) Example LLR trajectories calculatedon the NMNIST database with the 10th-order SPRT-TANDEM. Red and blue trajectories represent odd and even digits, respectively.
Figure 4: The speed-accuracy tradeoff curve. Compare this with Figure 3 in the main text. "10th TANDEM" means the 10-th order SPRT-TANDEM. "19th TANDEM" means the 19-th order SPRT-TANDEM. The numbers of trials for hyperparameter tuning if 200 for allthe models. The error bars are standard error of mean (SEM). The numbers of trials for statistics are 440, 240, 200, and 200 for 10thTANDEM, 19th TANDEM, LSTM-s, and LSTM-m, respectively.
Figure 5: Visualization of the notation given in Table 4. We assume that the network has L fully-connected layers W l and the activationfunction σ, with the final softmax with cross-entropy loss.
Figure 6: Depth-from-motion dataset for the face 3D-ness detection task. Only three frames out of ten frames are shown. Top and bottom facesare 3D and 2D faces, respectively. (a) Video of faces taken from various angles. (b) Facial feature points that are extracted with thefeature extractor, fwFE (x(t) )We tested the effectiveness of LLLR on a 3D-ness detection task on a depth-from-motion (DfM)dataset 4. The DfM dataset was a small dataset containing 2320 and 2609 3D- and 2D- facial videos,respectively, each of which consisted of 10 frames. In each of the video, a face was filmed fromvarious angles (Figure 6a), so that the dynamics of the facial features could be used to determinewhether the face appearing in a video is flat, 2D face, or had a 3D structure. The recording device wasan iPhone7. Here, the feature extractor fwFE (x(t)) was the LBP-AdaBoost algorithm (Viola & Jones(2001)) combined with the supervised descent method (Xiong & De la Torre (2013)), which took thet-th frame of the given video as input x(t) and output facial feature points (Figure 6b). The featurepoints were output as a vector of 152 lengths, consisted of vertical and horizontal pixel positions of76 facial feature points. The temporal integrator gwTI(x(t)) is an LSTM, whose number of hiddenunits was the same as that of feature points. The 1st-order SPRT-TANDEM was evaluated on twohypotheses, y = 1: 2D face, and y = 0: 3D face. We assumed a flat prior, p(y = 1) = p(y = 0). Thevalidation and test data were 10% of the entire data randomly selected at the beginning of training,4For the protection of personal information, this database cannot be made public.
Figure 7: Statistical test of equal error rates (EERs) in 10-fold cross-validation test. Two-way ANOVA are conducted with a loss factor(LLLR + Lmultiplet, LKLIEP + Lmultiplet, and Lmultiplet) and a epoch factor (21 - 100-th epoch). P-values with asterisks are statisticallysignificant: one, two and three asterisks show p < 0.05, p < 0.01, and p < 0.001, respectively. Error bars show the standarderrors of the mean (SEM).
Figure 8: Normalized mean squared error (NMSE) - iteration curve. A multi-layer perceptron is trained either cross-entropy loss (blue) or theLLLR (red). Shades show the standard error of the mean (SEM).
Figure 9: Speed-accuracy tradeoff (SAT) curves of all the models. The right three panels show magnified views of the left three panels. Themagnified region is same as the region plotted in the insets in Figure 3a, 3b, and 3c. Error bars show the standard error of the mean(SEM).(a,b) NMNIST database. (c,d) UCF database. (e,f) SiW database.
Figure 10: Log-likelihood ratio (LLR) trajectories calculated on NMNIST database. Red and blue trajectories represent odd and even class,respectively. Panels (a-i) shows results of 0th, 1st, 2nd, 3rd, 5th, 10th, 14th, 24th, and 49th-order SPRT-TANDEM, respectively.
Figure 11: Log-likelihood ratio (LLR) trajectories calculated on UCF database. Red and blue trajectories represent handstand-pushups andhandstand-walking class, respectively. Panels (a-i) shows results of 0th, 1st, 2nd, 3rd, 5th, 10th, 14th, 24th, and 49th-order SPRT-TANDEM, respectively.
Figure 12: Log-likelihood ratio (LLR) trajectories calculated on SiW database. Red and blue trajectories represent live and spoof class, respec-tively. Panels (a-i) shows results of 0th, 1st, 2nd, 3rd, 5th, 10th, 14th, 24th, and 49th-order SPRT-TANDEM, respectively.
Figure 13: Nosaic MNIST (NMNIST) database consists of videos of 20 frames, each of which has 28 × 28 × 1 pixels. The frames are buriedwith noise at the first frame, gradually denoised toward the last frame. NMNIST provides a typical task in early classification oftime series.
Figure 14: Speed-accuracy tradeoff (SAT) curves of the Moving MNIST database. Error bars show the standard error of the mean (SEM).
Figure 15: Speed-accuracy tradeoff (SAT) curves of the ablation experiment with the 19th-order SPRT-TANDEM on NMNIST database. Errorbars show the standard error of the mean (SEM).
