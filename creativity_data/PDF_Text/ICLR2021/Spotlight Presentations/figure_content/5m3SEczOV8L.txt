Figure 1: Our VAEBM is composed of a VAE generator (including the prior and decoder) and anenergy function that operates on samples X generated by the VAE. The VAE component is trainedfirst, using the standard VAE objective; then, the energy function is trained while the generator isfixed. Using the VAE generator, We can express the data variable X as a deterministic function ofwhite noise samples 金 and eχ. This allows us to reparameterize sampling from our VAEBM bysampling in thejoint space of EZ and Ex. We use this in the negative training phase (see Sec. 3.1).
Figure 2: (a) CIFAR-10 samples generated by VAEBM. (b) Visualizing MCMC sampling chains.
Figure 3: Qualitative results on CelebA 64, LSUN Church 64 and CelebA HQ 256. For CelebA HQ256, we initialize the MCMC chains with low temperature NVAE samples (t = 0.7) for better visualquality. On this dataset samples are selected for diversity. See Appendix H for additional qualitativeresults and uncurated CelebA HQ 256 samples obtained from higher temperature initializations.
Figure 4: Qualitative results on the 25-Gaussians dataset5.5	Sampling EfficiencyDespite their impressive sample quality, denoising score matching models (Song & Ermon, 2019; Hoet al., 2020) are slow at sampling, often requiring & 1000 MCMC steps. Since VAEBM uses shortMCMC chains, it takes only 8.79 seconds to generate 50 CIFAR-10 samples, whereas NCSN (Song& Ermon, 2019) takes 107.9 seconds, which is about 12× slower (see Appendix J for details).
Figure 5: Qualitative samples obtained from sampling in (x, z)-space with different step sizes.
Figure 6: Histogram of unnormalized log-likelihoods on 10k CIFAR-10 train and test set imagesNVAE: VAEBM uses NVAE as the pθ (x) component in the model. We train the NVAE with itsofficial implementation3. We largely follow the default settings, with one major difference that weuse a Gaussian decoder instead of a discrete logistic mixture decoder as in Vahdat & Kautz (2020).
Figure 7: Qualitative results of ablation study in Sec. 5.2. and Appendix Cbefore and after MCMC in Figure 15. Note that the samples used to visualize MCMC are generatedby initializing MCMC chains With VAE samples With full temperature 1.0.
Figure 8: Additional CIFAR-10 samplesI Nearest NeighborsWe show nearest neighbors in the training set with generated samples on CIFAR-10 (in Figure 16and 17) and CelebA HQ 256 (in Figure 18 and 19). We observe that the nearest neighbors aresignificantly different from the samples, suggesting that our models generalize well.
Figure 9: Additional visualizations of MCMC chains when sampling from the model for CIFAR-1024Published as a conference paper at ICLR 2021Figure 10: Additional CelebA 64 samples25Published as a conference paper at ICLR 2021Figure 11: Additional LSUN Church 64 samples26Published as a conference paper at ICLR 2021Figure 12: Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfig-ure, the top row contains initial samples from the VAE, and the bottom row contains correspondingsamples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples andrefines the details.
Figure 10: Additional CelebA 64 samples25Published as a conference paper at ICLR 2021Figure 11: Additional LSUN Church 64 samples26Published as a conference paper at ICLR 2021Figure 12: Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfig-ure, the top row contains initial samples from the VAE, and the bottom row contains correspondingsamples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples andrefines the details.
Figure 11: Additional LSUN Church 64 samples26Published as a conference paper at ICLR 2021Figure 12: Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfig-ure, the top row contains initial samples from the VAE, and the bottom row contains correspondingsamples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples andrefines the details.
Figure 12: Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfig-ure, the top row contains initial samples from the VAE, and the bottom row contains correspondingsamples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples andrefines the details.
Figure 13: Additional CelebA HQ 256 samples. Initial samples from VAE for MCMC initializationsare generated with temperature 0.7. Samples are uncurated.
Figure 14: Additional CelebA HQ 256 samples. Initial samples from VAE for MCMC initializationsare generated with full temperature 1.0. Samples are uncurated.
Figure 15: Visualizing the effect of MCMC sampling on CelebA HQ 256 dataset. Samples aregenerated by initializing MCMC with full temperature VAE samples. MCMC sampling fixes theartifacts of VAE samples, especially on hairs.
Figure 16: CIFAR-10 nearest neighbors in pixel distance. Generated samples are in the leftmostcolumn, and training set nearest neighbors are in the remaining columns.
Figure 17: CIFAR-10 nearest neighbors in Inception feature distance. Generated samples are in theleftmost column, and training set nearest neighbors are in the remaining columns.
Figure 18: CelebA HQ 256 nearest neighbors in pixel distance, computed on a 160 × 160 centercrop to focus more on faces rather than backgrounds. Generated samples are in the leftmost column,and training set nearest neighbors are in the remaining columns.
Figure 19: CelebA HQ 256 nearest neighbors in Inception feature distance, computed on a 160×160center crop. Generated samples are in the leftmost column, and training set nearest neighbors are inthe remaining columns.
