Figure 1: DDP backward propagates the valuederivatives (Vx, Vxx) instead of Vxt J and up-dates weight using layer-wise feedback policy,δUt*(δXt), with additional forward propagation.
Figure 2: (a) A toy illustration of the standard update (green) and the DDP feedback (red). The DDPpolicy in this case is a line lying at the valley of objective L. (bc) Trajectory optimization viewpointof DNN training. Green and orange arrows represent the proposed updates from GD and DDP.
Figure 3: Runtime comparison on MNIST.
Figure 4: (a) Performance difference between DDPNOpt and baselines on DIGITS across hyper-parameter grid. Blue (resp. red) indicates an improvement (resp. degradation) over baselines. Weobserve similar behaviors on other datasets. (b) Examples of the actual training dynamics.
Figure 5: Visualization of thefeedback policies on MNIST.
Figure 6: Training a 9-layer sigmoid-activated FCN onDIGITS using MMC loss. DDPNOpt2nd denotes when the layerdynamics is fully expanded to the second order.
Figure 7: Pictorial illustration for Alg. 4.
Figure 8: Spectrum distribution on synthetic dataset.
Figure 9: Additional experiment for Fig. 4a where we compare the performance difference betweenDDPNOpt and Adam. Again, all grids report values averaged over 10 random seeds.
Figure 10: Vanishing gradient experiment for different losses and nonlinear activation functions.
Figure 11:	Vanishing gradient experiment for other optimizers. The legend “DDPNOpt*” denotesDDPNOpt with adaptive diagonal matrix.
Figure 12:	Vanishing gradient experiment for different learning rate setups.
