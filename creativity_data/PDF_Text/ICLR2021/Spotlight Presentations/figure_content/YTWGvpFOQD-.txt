Figure 1: Highest test accuracy achieved for each DP budget (ε, δ = 10-5) for ScatterNet classifiersand the end-to-end CNNs of Papernot et al. (2020b). We plot the mean and standard deviation acrossfive runs.
Figure 2:	Highest test accuracy achieved for each DP budget (ε, δ = 10-5) for linear ScatterNetclassifiers, CNNs on top of ScatterNet features, and end-to-end CNNs. Shows mean and standarddeviation across five runs.
Figure 3:	ConvergenCe of DP-SGD with and without noise on CIFAR-10, for SCatterNet Classifiersand end-to-end CNNs. (Left): low learning rate. (Right): high learning rate.
Figure 4:	CIFAR-10 test accuracy for a trainingset of size N and a DP budget of (ε = 3, δ =1/2N). For N > 50K, we augment CIFAR-10 with pseudo-labeled Tiny Images collectedby Carmon et al. (2019).
Figure 5:	Privacy-utility tradeoffs for trans-fer learning on CIFAR-10. We fine-tune lin-ear models on features from a ResNeXt modeltrained on CIFAR-100, and from a SimCLRmodel trained on unlabeled ImageNet.
Figure 6: Noise scale σ for DP-SGD that results in a privacy guarantee of (ε = 3, δ = 10-5) after 60training epochs, for different batch sampling rates B/N.
Figure 7: Convergence rate of DP-SGD for different batch sizes, with a fixed targeted privacy budgetof (ε = 3,δ = 10-5) after T = 40 or T = 60 epochs, and linear scaling of the learning rate η ∙ B/512.
Figure 8: Median and maximum test accuracy of linear ScatterNet classifiers and end-to-end CNNswhen we fix one hyper-parameter in Table 12 and run a grid-search over all others (for a privacybudget of (ε = 3, δ = 10-5)).
Figure 9: Gradient noise scale σ required for aprivacy guarantee of (ε, δ = 10-5) after 10 train-ing epochs with batch sampling rate 512/50000.
Figure 10: Comparison of DP-SGD (Abadi et al.,2016) and Privacy Amplification by Iteration(PAI) (Feldman et al., 2018) for training a privatelinear ScatterNet classifier on CIFAR-10. Showsthe maximum accuracy achieved for each privacybudget, averaged over five runs.
Figure 3: with a high learning rate, all models converge quickly when trained without gradient noise,but the addition of noise is detrimental to the learning process. In contrast, with a much lower learningrate the training curves for DP-SGD are nearly identical, whether we add noise or not. In this regime,the ScatterNet classifiers converge significantly faster than end-to-end CNNs when trained withoutprivacy.
Figure 11: Comparison of convergence rates of linear classifiers fine-tuned on ScatterNet features,CNNs fine-tuned on ScatterNet features), and end-to-end CNNs with and without noise addition inDP-SGD. (Left): low learning rate. (Right): high learning rate. See Figure 3 for results on CIFAR-10.
