Figure 1: SnAp in dense (bottom) and sparse (top) graphs: As the figure proceeds to the right wepropagate the influence of the red connection i on the nodes j of the graph through further RNNsteps. Nodes are colored in pink if they are influenced on or before that step. The entry Ji,j is kept ifnode j is colored pink, but all other entries in row i are set to zero. When n = 1 in both cases onlyone node is influenced. In the dense case the red connection influences all nodes when n >= 2.
Figure 2: Depiction of RTRL, RTRL with spar-sity and SnAp. White indicates zeros. (a)It + DtJt-1 (b) It + DtJt-1 when W is sparse(C) It + Dt Jt-1 (d) SnAP-2 (e) SnAP-L Rosecolored squares are non-zero in Dt but not usedin uPdating Jt .
Figure 4: Copy task results by sparsity and architecture. Solid lines indicate that updates are donefully online (at every step). Dotted lines indicate that updates are done at the end of a sequence. Seethe heading “Fully online training” within section 5.2 for more details.
Figure 5: BPC vs Sparsity for Constant Param-eter Count. Shows the same results as Table 3.
Figure 6: Influence matrix for 75% sparse GRUwith 8 units after Processing a full sequence with35 timestePs (target length 16), at various Pointsduring training (“steP” corresPonds to trainingsteP, not e.g. the steP within a sequence). ThisHinton-diagram shows the magnitude ofan entrywith the size of a square. Grey entries are nearzero. Entries filled in with red are those includedby SnAP-1. Blue entries are those included bySnAP-2, and white ones are ignored by both aP-Proximations.
