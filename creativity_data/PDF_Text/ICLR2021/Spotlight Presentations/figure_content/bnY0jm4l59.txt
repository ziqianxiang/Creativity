Figure 1: Memory Optimized Network Training (MONeT), an automatic framework that mini-mizes the memory footprint of deep networks by jointly optimizing global and local techniques.
Figure 2: Schematic overview of the forward and backward passes. The algorithms include ag-gressive memory savings by greedily freeing unused tensors, and allow for a general checkpointingschedule (s, r) to be executed.
Figure 3: Comparing MONeT with PyTorch and Checkmate. MONeT reduces memory by 3×compared to PyTorch, with 9-16% compute overhead. It achieves a better memory-compute trade-off than default Checkmate-D and conv-optimized Checkmate-O.
Figure 4: Ablation results for memory ratio 0.53. Lowest compute overhead across models is seenonly when all optimizations are jointly optimized.
Figure 5: Detailed case study on ResNet-50. Top : memory usage along execution (forward andbackward). Middle: memory saving of MONeT over PyTorch for each layer. Bottom: computeoverhead of MONeT over PyTorch. MONeT saves memory in early layers to reduce peak memory.
Figure 6: Ablation results on ResNet-50, GoogleNet, UNet, VGG-16, MobileNet-V2.
Figure 7:	Memory vs. compute for 7 convo-lution algorithms with 256×64×56×56 in-put, 3×3 kernel, 64 output channels.
Figure 8:	Runtime-memory trade-offcurve for 3D-UNet using MONET. Thegreen point denotes the PyTorch baseline.
