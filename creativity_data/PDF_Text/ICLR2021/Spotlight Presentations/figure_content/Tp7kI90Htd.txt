Figure 1: Scans and training sets. Overview of thedatasets and how they are combined into different trainingsets. Each scan was performed on a specific set of neurons(rows) using a specific set of unique images (columns). Re-peatedly presented test images were the same for all scans.
Figure 2: Using retinotopy to learn thereadout position from anatomical data.
Figure 3: Performance of end-to-end trainednetworks. Performance for different subsets of neu-rons (linestyle) and number of training examples(x-axis). The same core architecture was trainedfor two different readouts with and without fea-ture sharing (color) on the matched neurons of the4-S:matched core set (Fig. 1, green). Both net-works show increasing performance with numberof images. However, the network with the Gaus-sian readout achieves a higher final performance(light blue vs. orange). While the Gaussian readoutprofits from feature sharing (light vs. dark blue),the factorized readout is hurt by it (yellow vs. or-ange). Shaded areas depict 95% confidence inter-vals across random picks of the neuron subsets.
Figure 4: Generalization to other neurons in the same animal. A core trained on 3597 neuronsand up to 17.5k images generalizes to new neurons (pink and yellow line). A core trained on thefull data yields very good predictive performance even when the readout is trained on far less data(yellow). If the readout is trained with all data, even a core trained on few data can yield a goodperformance (pink). Both transfer conditions outperform a network directly trained end-to-end on thetransfer dataset (brown). For the full dataset, all training conditions converge to the same performance.
Figure 5: Generalization across ani-mals. Prediction performance in frac-tion oracle correlation as a function oftraining examples in the transfer set fora Gaussian readout (x-axis) and differ-ent ways to obtain the core (colors).
