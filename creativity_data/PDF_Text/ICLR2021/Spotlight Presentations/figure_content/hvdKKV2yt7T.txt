Figure 1: The effect of includ-ing (x, ‘A’) in the train set. Ifx is in the train set, the classi-fier will learn to maximize thedecision boundary’s distanceto Y \ {‘A’}. If x is in the testset, it has no direct impact onthe learned landscape.
Figure 2: Training (dotted) the confidence regres-sor with embeddings of public and private data,and victim’s model fV ; Dataset Inference (solid)Using m private samples and adversary model f∕*Black-Box Setting: Blind Walk. V may want to perform DI on a publicly deployed model fthat only allows label query access. This makes them incapable of computing gradients required forMinGD. Moreover, querying f would be costly for V. Therefore, we introduce a new method calledBlind Walk which estimates the ‘prediction margin’ of any given data point through its robustness torandom noise rather than a gradient search. We sample a random initial direction δ. Starting froman input (x, y), we take k ∈ N steps in the same direction until f(x + kδ) = t; t 6= y. Then,∆(x, x + kδ) is used as a proxy for the ‘prediction margin’ of the model. Thus, the approach onlyrequires label access to f. We repeat the search over multiple random initial directions to increasethe information about the point’s robustness, and use each of these distance values as features inthe generated embedding. In practice, we find Blind Walk to perform better than MinGD with theownership tester from §5.2. We discuss further details justifying these observations in Appendix C.
Figure 3: p-value against number of revealed samples (m). Significance levels (FPR) α = 0.01and 0.05 (dotted lines) have been drawn. Under most attack scenarios, the victim V can dispute theadversary's ownership of f∕* (with FPR of at most 1%) by revealing fewer than 50 private samples.
Figure 4: p-value vs. distance-embedding size0	5	10	15	20	25	30Distance-Embedding SizeDataset Inference on SVHN (Blind Walk Attack)			Model Stealing Attack		∆μ	p-valueV	Source	0.950	10-8	Distillation	0.537	10-3AD	Diff. Architecture	0.450	10-2A ,ʃ AM	Zero-Shot Learning	0.512	10-3	Fine-tuning	0.581	10-4A	Label-query	0.513	10-03AQ	Logit-query	0.515	10-02	Random-query	0.475	10-02I	Independent	-0.322	10-01Table 2: Ownership Tester’s effect size in a small-data regime (using only m = 10 samples) on theSVHN dataset using Blind Walk attack. 2nd highest and lowest effect size is marked in red & blue.
Figure 5: Ownership Tester’s p-value depicted as a function of number of training samples revealed(m). In the figure to the left, for an honest adversary whose dataset has no overlap with the victim’sdataset, the p-value increases as the number of revealed samples increases, indicating decrease inconfidence of claim for knowledge theft. For all adversaries with fractional data overlap (to theright), DI is able to achieve a p-value of less than 0.01 in under 10 samples.
