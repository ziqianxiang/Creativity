Under review as a conference paper at ICLR 2021
Towards Understanding Label Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
Label smoothing regularization (LSR) has a great success in training deep neu-
ral networks by stochastic algorithms such as stochastic gradient descent and its
variants. However, the theoretical understanding of its power from the view of
optimization is still rare. This study opens the door to a deep understanding of
LSR by initiating the analysis. In this paper, we analyze the convergence behav-
iors of stochastic gradient descent with label smoothing regularization for solving
non-convex problems and show that an appropriate LSR can help to speed up the
convergence by reducing the variance. More interestingly, we proposed a simple
yet effective strategy, namely Two-Stage LAbel smoothing algorithm (TSLA),
that uses LSR in the early training epochs and drops it off in the later training
epochs. We observe from the improved convergence result of TSLA that it bene-
fits from LSR in the first stage and essentially converges faster in the second stage.
To the best of our knowledge, this is the first work for understanding the power
of LSR via establishing convergence complexity of stochastic methods with LSR
in non-convex optimization. We empirically demonstrate the effectiveness of the
proposed method in comparison with baselines on training ResNet models over
benchmark data sets.
1	Introduction
In training deep neural networks, one common strategy is to minimize cross-entropy loss with one-
hot label vectors, which may lead to overfitting during the training progress that would lower the
generalization accuracy (Muller et al., 2019). To overcome the overfitting issue, several regular-
ization techniques such as '「norm or '2-norm penalty over the model weights, Dropout which
randomly sets the outputs of neurons to zero (Hinton et al., 2012b), batch normalization (Ioffe &
Szegedy, 2015), and data augmentation (Simard et al., 1998), are employed to prevent the deep
learning models from becoming over-confident. However, these regularization techniques conduct
on the hidden activations or weights of a neural network. As an output regularizer, label smoothing
regularization (LSR) (Szegedy et al., 2016) is proposed to improve the generalization and learn-
ing efficiency of a neural network by replacing the one-hot vector labels with the smoothed labels
that average the hard targets and the uniform distribution of other labels. Specifically, for a K-
class classification problem, the one-hot label is smoothed by yLS = (1 - θ)y + θyb, where y is
the one-hot label, θ ∈ (0,1) is the smoothing strength and b = K is a uniform distribution for
all labels. Extensive experimental results have shown that LSR has significant successes in many
deep learning applications including image classification (Zoph et al., 2018; He et al., 2019), speech
recognition (Chorowski & Jaitly, 2017; Zeyer et al., 2018), and language translation (Vaswani et al.,
2017; Nguyen & Salazar, 2019).
Due to the importance of LSR, researchers try to explore its behavior in training deep neural net-
works. Muller et al. (2019) have empirically shown that the LSR can help improve model calibration,
however, they also have found that LSR could impair knowledge distillation, that is, if one trains
a teacher model with LSR, then a student model has worse performance. Yuan et al. (2019a) have
proved that LSR provides a virtual teacher model for knowledge distillation. As a widely used trick,
Lukasik et al. (2020) have shown that LSR works since it can successfully mitigate label noise.
However, to the best of our knowledge, it is unclear, at least from a theoretical viewpoint, how the
introduction of label smoothing will help improve the training of deep learning models, and to what
stage, it can help. In this paper, we aim to provide an affirmative answer to this question and try
to deeply understand why and how the LSR works from the view of optimization. Our theoretical
analysis will show that an appropriate LSR can essentially reduce the variance of stochastic gradient
1
Under review as a conference paper at ICLR 2021
in the assigned class labels and thus it can speed up the convergence. Moreover, we will propose a
novel strategy of employing LSR that tells when to use LSR. We summarize the main contributions
of this paper as follows.
•	It is the first work that establishes improved iteration complexities of stochastic gradient
descent (SGD) (Robbins & Monro, 1951) with LSR for finding an -approximate stationary
point (Definition 1) in solving a smooth non-convex problem in the presence of an appro-
priate label smoothing. The results theoretically explain why an appropriate LSR can help
speed up the convergence. (Section 4)
•	We propose a simple yet effective strategy, namely Two-Stage LAbel smoothing (TSLA)
algorithm, where in the first stage it trains models for certain epochs using a stochastic
method with LSR while in the second stage it runs the same stochastic method without
LSR. The proposed TSLA is a generic strategy that can incorporate many stochastic algo-
rithms. With an appropriate label smoothing, we show that TSLA integrated with SGD has
an improved iteration complexity, compared to the SGD with LSR and the SGD without
LSR. (Section 5)
2	Related Work
In this section, we introduce some related work. A closely related idea to LSR is confidence penalty
proposed by Pereyra et al. (2017), an output regularizer that penalizes confident output distribu-
tions by adding its negative entropy to the negative log-likelihood during the training process. The
authors (Pereyra et al., 2017) presented extensive experimental results in training deep neural net-
works to demonstrate better generalization comparing to baselines with only focusing on the existing
hyper-parameters. They have shown that LSR is equivalent to confidence penalty with a reversing
direction of KL divergence between uniform distributions and the output distributions.
DisturbLabel introduced by Xie et al. (2016) imposes the regularization within the loss layer, where
it randomly replaces some of the ground truth labels as incorrect values at each training iteration.
Its effect is quite similar to LSR that can help to prevent the neural network training from overfit-
ting. The authors have verified the effectiveness of DisturbLabel via several experiments on training
image classification tasks.
Recently, many works (Zhang et al., 2018; Bagherinezhad et al., 2018; Goibert & Dohmatob, 2019;
Shen et al., 2019; Li et al., 2020b) explored the idea of LSR technique. Ding et al. (2019) extended
an adaptive label regularization method, which enables the neural network to use both correctness
and incorrectness during training. Pang et al. (2018) used the reverse cross-entropy loss to smooth
the classifier’s gradients. Wang et al. (2020) proposed a graduated label smoothing method that uses
the higher smoothing penalty for high-confidence predictions than that for low-confidence predic-
tions. They found that the proposed method can improve both inference calibration and translation
performance for neural machine translation models. By contrast, in this paper, we will try to un-
derstand the power of LSR from an optimization perspective and try to study how and when to use
LSR.
3	Preliminaries and Notations
We first present some notations. Let NwF(W) denote the gradient of a function F(w). When the
variable to be taken a gradient is obvious, We use VF(w) for simplicity. We use ∣∣ ∙ ∣∣ to denote the
Euclidean norm. Let(,, •)be the inner product.
In classification problem, We aim to seek a classifier to map an example x ∈ X onto one of K
labels y ∈ Y ⊂ RK, Where y = (y1, y2, . . . , yK) is a one-hot label, meaning that yi is “1” for the
correct class and “0” for the rest. Suppose the example-label pairs are draWn from a distribution
P, i.e., (x, y)〜 P = (Pχ, Py). we denote by E(χ,y)[∙] the expectation that takes over a random
variable (x, y). When the randomness is obvious, we write EH for simplicity. Our goal is to learn a
prediction function f(w; x) : W × X → RK that is as close as possible to y, where w ∈ W is the
parameter and W is a closed convex set. To this end, we want to minimize the following expected
loss under P:
min F(W)= E(x，y) ['(y, f (w； X))I,	⑴
2
Under review as a conference paper at ICLR 2021
where ` : Y × RK → R+ is a cross-entropy loss function given by
'(y,f(w; x))
K
-yi log
i=1
exp(fi(w; x))
PK=I exP(fj(W；X))
(2)
The objective function F(W) is not convex since f(W; x) is non-convex in terms of W. To solve the
problem (1), one can simply use some iterative methods such as stochastic gradient descent (SGD).
Specifically, at each training iteration t, SGD updates solutions iteratively by
Wt+1 = Wt - ηVw'(yt,f (wt; Xt)),
where η > 0 is a learning rate.
Next, we present some notations and assumptions that will be used in the convergence analysis.
Throughout this paper, we also make the following assumptions for solving the problem (1).
Assumption 1. Assume the following conditions hold:
(i)	The stochastic gradient of F (w) is unbiased, i.e., E(χ,y)[V'(y, f (w; x))] = VF (w), and
the variance of stochastic gradient is bounded, i.e., there exists a constant σ2 > 0, such
that E(x,y)[∣∣V'(y,f(w; x)) - VF(W)『]=σ2.
(ii)	F(W) is smooth with an L-Lipchitz continuous gradient, i.e., it is differentiable and there
exists a constant L > 0 such that kVF (W) - VF (u)k ≤ LkW - uk, ∀W, u ∈ W.
Remark. Assumption 1 (i) and (ii) are commonly used assumptions in the literature of non-convex
optimization (Ghadimi & Lan, 2013; Yan et al., 2018; Yuan et al., 2019b; Wang et al., 2019; Li
et al., 2020a). Assumption 1 (ii) says the objective function is L-smooth, and it has an equivalent
expression (Nesterov, 2004) which is F(w) - F(u) ≤ <VF(u), W — Ui + LL∣∣w — u∣∣2,∀w, U ∈ W.
For a classification problem, the smoothed label yLS is given by
yLS = (1 - θ)y + θyb,	(3)
where θ ∈ (0, 1) is the smoothing strength, y is the one-hot label, yb is an introduced label. For
example, one can simply use b = KK (Szegedy et al., 2016) for K-class problems. Similar to label
y, we suppose the label yb is drawn from a distribution Pby . We introduce the variance of stochastic
gradient using label yb as follows.
E(χ,y) h∣V'(b,f (w; x)) — VF(W)『i = b2 := δσ2.	(4)
where δ > 0 is a constant and σ2 is defined in Assumption 1 (i). We make several remarks for (4).
Remark. (a) We do not require that the stochastic gradient V'(b, f (w; x)) is unbiased, i.e., it could
be E[V'(b, f (w; x))] = Vf(w). (b) The variance b2 is defined based on the label y rather than
the smoothed label yLS. (c) We do not assume the variance σb2 is bounded since δ could be an
arbitrary value, however, we will discuss the different cases of δ in our analysis. If δ ≥ 1, then
σb2 ≥ σ2 ; while if 0 < δ < 1, then σb2 < σ2 . It is worth mentioning that δ could be small when
an appropriate label is used in the label smoothing. For example, one can smooth labels by using
a teacher model (Hinton et al., 2014) or the model’s own distribution (Reed et al., 2014). In the
first paper of label smoothing (Szegedy et al., 2016) and the following related studies (Muller et al.,
2019; Yuan et al., 2019a), researchers consider a uniform distribution over all K classes of labels as
the label b, i.e., set b = K.
We now introduce an important property regarding F(w), i.e. Polyak-Eojasiewicz (PL) condi-
tion (Polyak, 1963). More specifically, the following assumption holds.
Assumption 2. There exists a COnStant μ > 0 such that 2μ(F(w) 一 F*) ≤ ∣∣VF(w)∣2, ∀w ∈ W,
where F = minw∈w F(w) is the optimal value.
Remark. This property has been theoretically and empirically observed in training deep neural
networks (Allen-Zhu et al., 2019; Yuan et al., 2019b). This condition is widely used to establish
convergence in the literature of non-convex optimization, please see (Yuan et al., 2019b; Wang
et al., 2019; Karimi et al., 2016; Li & Li, 2018; Charles & Papailiopoulos, 2018; Li et al., 2020a)
and references therein.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 SGD with Label Smoothing Regularization
1:	Initialize: w0 ∈ W, θ ∈ (0, 1), set η as the value in Theorem 3.
2:	for t = 0, 1, . . . , T - 1 do
3:	sample (xt,yt), set ytLS = (1 - θ)yt + θybt
4:	update wt+ι = Wt - ηVw'(yL* 4 s, f (wt; Xt))
5:	end for
To measure the convergence of non-convex and smooth optimization problems as in (Nesterov,
1998; Ghadimi & Lan, 2013; Yan et al., 2018), we need the following definition of the first-order
stationary point.
Definition 1 (First-order stationary point). For the problem of minw∈W F (W), a point W ∈ W is
called a first-order stationary point if kVF (W)k = 0. Moreover, if kVF (W)k ≤ , then the point
W is said to be an -stationary point, where ∈ (0, 1) is a small positive value.
4	Convergence Analysis of sGD with LsR
To understand LsR from the optimization perspective, we consider sGD with LsR in Algorithm 1
for the sake of simplicity. The only difference between Algorithm 1 and standard sGD is the use
of the output label for constructing a stochastic gradient. The following theorem shows that Al-
gorithm 1 converges to an approximate stationary point in expectation under some conditions. We
include its proof in Appendix B.
Theorem 3. Under Assumption 1, run Algorithm 1 with η = L and θ = i+§, then
Er[∣∣VF(wr )k2] ≤ 2 F/O) + 2δσ2, where R is uniformly sampled from {0,1,..., T — 1}. Fur-
thermore, given a target accuracy level , we have the following two results.
(1) when δ ≤ 枭,if we Set T = 4今,0', then Algorithm 1 converges to an e-stationary point in
expectation, i.e., ER[∣∣VF(wr)∣∣2] ≤ e2. The total sample complexity is T = O (*).
2	F(w )
(2) when δ > 甘,if we set T = η6^0 , then Algorithm 1 does not converge to an e-stationary
point, but we have ER [kVF (wR)k2] ≤ 4δσ2 ≤ O(δ).
Remark. We observe that the variance term is 2δσ2, instead of ηLσ2 for standard analysis of
sGD without LsR (i.e., θ = 0, please see the detailed analysis of Theorem 5 in Appendix C).
For the convergence analysis, the difference between sGD with LsR and sGD without LsR is that
V'(b, f (w; x)) is not an unbiased estimator of VF(w) When using LSR. The convergence behavior
of Algorithm 1 heavily depends on the parameter δ. When δ is small enough, say δ ≤ O(e2)
With a small positive value e ∈ (0, 1), then Algorithm 1 converges to an e-stationary point With
the total sample complexity of O G). Recall that the total sample complexity of standard SGD
without LSR for finding an e-stationary point is O (±) ((Ghadimi & Lan, 2016; Ghadimi et al.,
2016), please also see the detailed analysis of Theorem 5 in Appendix C). The convergence result
shows that if we could find a label yb that has a reasonably small amount of δ, we will be able to
reduce sample complexity for training a machine learning model from O (』)to O G). Thus, the
reduction in variance will happen when an appropriate label smoothing with δ ∈ (0, 1) is introduced.
We will find in the empirical evaluations that different label yb lead to different performances and an
appropriate selection of label yb has a better performance (see the performances of LSR and LSR-pre
in Table 3). On the other hand, when the parameter δ is large such that δ > Ω(e2), that is to say,
if an inappropriate label smoothing is used, then Algorithm 1 does not converge to an e-stationary
point, but it converges to a worse level of O(δ).
5 TSLA: A Generic Two-Stage Label Smoothing Algorithm
Despite superior outcomes in training deep neural networks, some real applications have shown the
adverse effect of LSR. Muller et al. (2019) have empirically observed that LSR impairs distilla-
tion, that is, after training teacher models with LSR, student models perform worse. The authors
believed that LSR reduces mutual information between the input example and output logit. Ko-
rnblith et al. (2019) have found that LSR impairs the accuracy of transfer learning when training
deep neural network models on ImageNet data set. Seo et al. (2020) trained deep neural net-
work models for few-shot learning on mini-ImageNet and found a significant performance drop
4
Under review as a conference paper at ICLR 2021
Algorithm 2 The TSLA algorithm
1:	Initialize: w° ∈ W, Ti, θ ∈ (0,1), η1,η2 › 0
2:	Input: stochastic algorithm A (e.g., SGD)
// First stage: A with LSR
3:	for t = 0, 1, . . . , T1 - 1 do
4:	sample (xt, yt), set ytLS = (1 - θ)yt + θybt
5：	update wt+i = A-SteP(wt； xt, yLs,ηι)
6:	end for
// Second stage: A without LSR
7:	for t = Ti , 1, . . . , Ti + T2 - 1 do
8:	sample (xt, yt)
9:	update wt+i = A-step(wt； xt, yt, η2)
10:	end for
one update step of A
one update step of A
with LSR. This motivates us to investigate a strategy that combines the algorithm with and with-
out LSR during the training progress. Let think in this way, one possible scenario is that training
one-hot label is “easier” than training smoothed label. Taking the cross entropy loss in (2) for an
example, one need to optimize a single loss function - log exp(fk(w； x))/ PjK=i exp(fj (w； x))
when one-hot label (e.g, yk = 1 and yi = 0 for all i 6= k) is used, but need to optimize all K
loss functions - PiK=i yiLS log exp(fi(w； x))/ PjK=i exp(fj(w； x)) when smoothed label (e.g.,
yLS = (1 一 θ)y + θk1 so that yLS = 1 一 (K 一 1)θ∕K and yL = θ∕K for all i = k) is used. NeV-
ertheless, training deep neural networks is gradually focusing on hard examples with the increase
of training epochs. It seems that training with smoothed label in the late epochs makes the learning
progress more difficult. In addition, after LSR, we focus on optimizing the oVerall distribution that
contains the minor classes, which are probably not important at the end of training progress. One
question is whether LSR helps at the early training epochs but it has less (eVen negatiVe) effect dur-
ing the later training epochs? This question encourages us to propose and analyze a simple strategy
with LSR dropping that switches a stochastic algorithm with LSR to the algorithm without LSR.
In this subsection, we propose a generic framework that consists of two stages, wherein the first
stage it runs a stochastic algorithm A (e.g., SGD) with LSR for Ti iterations and the second stage
it runs the same algorithm without LSR up to T2 iterations. This framework is referred to as Two-
Stage LAbel smoothing (TSLA) algorithm, whose updating details are presented in Algorithm 2.
The notation A-SteP(∙; ∙, η) is one update step of a stochastic algorithm A with learning rate η. For
example, if we select SGD as algorithm A, then
SGD-Step(wt； xt, yLS, ηι) = Wt 一 ηιV'(yLS,f (wt； xt)),	(5)
SGD-Step(wt； xt, yt, η2) = wt 一 η2V'(yt, f (wt； xt)).	(6)
Although SGD is considered as the subroutine algorithm A in the conVergence analysis, in prac-
tice, algorithm A can be replaced by any stochastic algorithms such as momentum SGD (Polyak,
1964), Stochastic NesteroV’s Accelerated Gradient (NesteroV, 1983), and adaptiVe algorithms in-
cluding ADAGRAD (Duchi et al., 2011), RMSProp (Hinton et al., 2012a), AdaDelta (Zeiler, 2012),
Adam (Kingma & Ba, 2015), Nadam (Dozat, 2016) and AMSGRAD (Reddi et al., 2018). In this
paper, we will not study the theoretical guarantees and empirical eValuations of other optimizers,
which can be considered as future work. Please note that the algorithm can use different learning
rates ηi and η2 during the two stages. The last solution of the first stage will be used as the initial
solution of the second stage. If Ti = 0, then TSLA reduces to the baseline, i.e., a standard stochas-
tic algorithm A without LSR; while if T2 = 0, TSLA becomes to LSR method, i.e., a standard
stochastic algorithm A with LSR.
5.1 Convergence Result of TSLA
In this subsection, we will giVe the conVergence result of the proposed TSLA algorithm. For sim-
plicity, we use SGD as the subroutine algorithm A in the analysis. The conVergence result in the
following theorem shows the power of LSR from the optimization perspectiVe. Its proof is presented
in Appendix D. It is easy to see from the proof that by using the last output of the first stage as the
initial point of the second stage, TSLA can enjoy the adVantage of LSR in the second stage with an
improVed conVergence.
5
Under review as a conference paper at ICLR 2021
Table 1: Comparisons of Total Sample Complexity
Possibilities on δ t	TSLA	LSR	baseline
Ω(eI 2) < δ	δ e4	∞	i _J4	
δ = O(e2)	1 eɪ	,	i	i eɪ	
Ω(e4) <δ<O(e2)	一	e2-θ		i 寻	i e4	
Ω(e4+c) ≤ δ ≤ O(e4)** 一	log( e)	i e2		i eɪ	
'given a target accuracy level e
*θ ∈ (0, 2); **c ≥ 0 is a constant
Theorem 4. Under Assumptions 1, 2, suppose σ2δ∕μ ≤ F(w0), run Algorithm 2 with A =
SGD, θ = ι++δ, ηι = L, TI = log (2μF (wδσ)2'1+δ)) /(ηιμ), η2 = 2LLσ2 and T2 = μη∣2e2, then
Er[∣∣VF(WR)k2] ≤ e2, where R is uniformly SamPledfrom {Tι,..., Ti + T — 1}.
Remark. It is obvious that the learning rate η2 in the second stage is roughly smaller than the
learning rate ηi in the first stage, which matches the widely used stage-wise learning rate decay
scheme in training neural networks. To explore the total sample complexity of TSLA, we consider
different conditions on δ. We summarize the total sample complexities of finding e-stationary points
for SGD with TSLA (TSLA), SGD with LSR (LSR), and SGD without LSR (baseline) in Table 1,
where e ∈ (0, 1) is the target convergence level, and we only present the orders of the complexities
but ignore all constants. When Ω(e2) < δ < 1, LSR does not converge to an e-stationary point
(denoted by ∞), while TSLA reduces sample complexity from O (妥)to O ), compared to the
baseline. When δ < O(e2), the total complexity of TSLA is between log(1/e) and 1/e2, which
is always better than LSR and the baseline. In summary, TSLA achieves the best total sample
complexity by enjoying the good property of an appropriate label smoothing (i.e., when 0 < δ < 1).
However, when δ ≥ 1, baseline has better convergence than TSLA, meaning that the selection of
label yb is not appropriate.
6 Experiments
To further evaluate the performance of the proposed TSLA method, we trained deep neural networks
on three benchmark data sets, CIFAR-100 (Krizhevsky & Hinton, 2009), Stanford Dogs (Khosla
et al., 2011) and CUB-2011 (Wah et al., 2011), for image classification tasks. CIFAR-100 1 has
50,000 training images and 10,000 testing images of 32×32 resolution with 100 classes. Stanford
Dogs data set 2 contains 20,580 images of 120 breeds of dogs, where 100 images from each breed
is used for training. CUB-2011 3 is a birds image data set with 11,788 images of 200 birds species.
The ResNet-18 model (He et al., 2016) is applied as the backbone in the experiments. We compare
the proposed TSLA incorporating with SGD (TSLA) with two baselines, SGD with LSR (LSR) and
SGD without LSR (baseline). The mini-batch size of training instances for all methods is 256 as
suggested by He et al. (2019) and He et al. (2016). The momentum parameter is fixed as 0.9.
6.1 Stanford Dogs and CUB-2011
We separately train ResNet-18 (He et al., 2016) up to 90 epochs over two data sets Stanford Dogs
and CUB-2011. We use weight decay with the parameter value of 10-4. For all algorithms, the
initial learning rates for FC are set to be 0.1, while that for the pre-trained backbones are 0.001 and
0.01 for Standford Dogs and CUB-2011, respectively. The learning rates are divided by 10 every 30
epochs. For LSR, we fix the value of smoothing strength θ = 0.4 for the best performance, and the
label b used for label smoothing is set to be a uniform distribution over all K classes, i.e., b = K.
The same values of the smoothing strength θ and the same yb are used during the first stage of TSLA.
For TSLA, we drop off the LSR (i.e., let θ = 0) after s epochs during the training process, where s ∈
{20, 30, 40, 50, 60, 70, 80}. We first report the highest top-1 and top-5 accuracy on the testing data
sets for different methods. All top-1 and top-5 accuracy are averaged over 5 independent random
trails with their standard deviations. The results of the comparison are summarized in Table 2, where
the notation “TSLA(s)” means that the TSLA algorithm drops off LSR after epoch s. It can be
seen from Table 2 that under an appropriate hyperparameter setting the models trained using TSLA
Ihttps://www.cs.toronto.edu/~kriz/cifar.html
2http://vision.stanford.edu/aditya86/ImageNetDogs/
3http://www.vision.caltech.edu/visipedia/CUB-200.html
6
Under review as a conference paper at ICLR 2021
Table 2: Comparison of Testing Accuracy for Different Methods (mean ± standard deviation, in %).
Algorithm*	Stanford Dogs		CUB-2011	
	Top-1 accuracy	Top-5 accuracy	Top-1 accuracy	Top-5 accuracy
baseline	82.31 ± 0.18	97.76 ± 0.06^^	75.31 ± 0.25	93.14 ± 0.31
LSR	82.80 ± 0.07	97.41 ± 0.09^^	76.97 ± 0.19	92.73 ± 0.12
TSLA(20)	83.15 ± 0.02	97.91 ± 0.08^^	76.62 ± 0.15	93.60 ± 0.18
TSLA(30)	83.89 ± 0.16	98.05 ± 0.08	77.44 ± 0.19	93.92 ± 0.16
TSLA(40)	83.93 ± 0.13	98.03 ± 0.05	77.50 ± 0.20	93.99 ± 0.11
TSLA(50)	83.91 ± 0.15	98.07 ± 0.06	77.57 ± 0.21	93.86 ± 0.14
TSLA(60)	83.51 ± 0.11	97.99 ± 0.06	77.25 ± 0.29	94.43 ± 0.18
TSLA(70)	83.38 ± 0.09	97.90 ± 0.09	77.21 ± 0.15	93.31 ± 0.12
TSLA(80)	83.14 ± 0.09	97.73 ± 0.07	77.05 ± 0.14	93.05 ± 0.08
*TSLA(s): TSLA drops offLSR after epoch s.
Stanford Dogs
20	30	40	50	60	70	80	90
# epoch
CUB-2011
8 7
9 9
AUeJnydol
Stanford Dogs
20	30	40	50	60	70	80	90
# epoch
CUB-2011
Stanford Dogs
2 0 8 6
■ ■ ■ ■
Iloo
SSCrl
0	20	40	60	80
# epoch
CUB-2011
75
Aejro3vdol
TSLA (30)
TSLA (40)
TSLA (50)
TSLA (60)
—TSLA (70)
—TSLA (80)
--LSR
""■ ' baseline
3~
9 <
AUeJnydol



20	30	40	50	60	70	80	90	20	30	40	50	60	70	80	90	0	20	40	60	80
# epoch	# epoch	# epoch
Figure 1: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over Stanford Dogs and CUB-
2011. TSLA(s) means TSLA drops off LSR after epoch s.
outperform that trained using LSR and baseline, which supports the convergence result in Section 5.
We notice that the best top-1 accuracy of TSLA are TSLA(40) and TSLA(50) for Stanford Dogs
and CUB-2011, respectively, meaning that the performance of TSLA(s) is not monotonic over the
dropping epoch s. For CUB-2011, the top-1 accuracy of TSLA(20) is smaller than that of LSR.
This result matches the convergence analysis of TSLA showing that it can not drop off LSR too
early. For top-5 accuracy, we found that TSLA(80) is slightly worse than baseline. This is because
of dropping LSR too late so that the update iterations (i.e., T2) in the second stage of TSLA is too
small to converge to a good solution. We also observe that LSR is better than baseline regarding top-
1 accuracy but the result is opposite as to top-5 accuracy. We then plot the averaged top-1 accuracy,
averaged top-5 accuracy, and averaged loss among 5 trails of different methods in Figure 1. We
remove the results for TSLA(20) since it dropped off LSR too early as mentioned before. The
figure shows TSLA improves the top-1 and top-5 testing accuracy immediately once it drops off
LSR. Although TSLA may not converge if it drops off LSR too late, see TSLA(60), TSLA(70), and
TSLA(80) from the third column of Figure 1, it still has the best performance compared to LSR and
baseline. TSLA(30), TSLA(40), and TSLA(50) can converge to lower objective levels, comparing
to LSR and baseline.
6.2 CIFAR- 1 00
The total epochs of training ResNet-18 (He et al., 2016) on CIFAR-100 is set to be 200.The weight
decay with the parameter value of 5 × 10-4 is used. We use 0.1 as the initial learning rates for all
algorithms and divide them by 10 every 60 epochs as suggested in (He et al., 2016; Zagoruyko &
Komodakis, 2016). For LSR and the first stage of TSLA, the value of smoothing strength θ is fixed
as θ = 0.1, which shows the best performance for LSR. We use two different labels yb to smooth the
7
Under review as a conference paper at ICLR 2021
Table 3: Comparison of Testing Accuracy for Different Methods (mean ± standard deviation, in %).
Algorithm*	CIFAR-100		
	Top-1 accuracy	Top-5 accuracy
baseline	76.87 ± 0.04^^	93.47 ± 0.15
LSR	77.77 ± 0.18^^	93.55 ± 0.11
TSLA(120)	77.92 ± 0.21 ^^	94.13 ± 0.23
TSLA(140)	77.93 ± 0.19	94.11 ± 0.22
TSLA(160)	77.96 ± 0.20	94.19 ± 0.21
TSLA(180)	78.04 ± 0.27	94.23 ± 0.15
LSR-Pre	78.07 ± 0.31 ^^	94.70 ± 0.14
TSLA-Pre(120)	78.34 ± 0.31 ^^	94.68 ± 0.14
TSLA-Pre(140)	78.39 ± 0.25	94.73 ± 0.11
TSLA-Pre(160)	78.55 ± 0.28	94.83 ± 0.08
TSLA-Pre(180)	78.53 ± 003	94.96 ± 0.23
*TSLA(s)/TSLA-Pre(s): TSLA/TSLA-Pre drops offLSR/LSR-Pre after epoch s.
CifarlOO
2.0-
1.8
CifarlOO
78
75
>υ≡3υ⅛doɪ
CifarlOO
95
9493
>υ≡3υ⅛dol
6 4 2 0
■ ■ ■ ■
Illl
SSa-I
—TSLA (180)	—	TSLA-pne (1∞)
—TSLA (160)	—	TSLA-pne (140)
----TSLA (140)	----- TSLA-pne (120)
—TSLA (120)	- -	LSR-Pre
--LSR	■ ■ ■	baseline
一TSLA-pne (180)
60	80 100 120 140 160 180 200	60	80 100 120 140 160 180 200	0 25 50 75 100 125 150 175 200
# epoch	# epoch	# epoch
Figure 2: Testing ToP-1, ToP-5 Accuracy and Loss on ResNet-18 over CIFAR-100. TSLA(s)/TSLA-
Pre(s) meansTSLA/TSLA-Pre droPs off LSR/LSR-Pre after ePoch s.
one-hot label, the uniform distribution over all labels and the distribution Predicted by an ImageNet
Pre-trained model which is downloaded directly from PyTorch 4 (Paszke et al., 2019). For TSLA, we
try to droP off the LSR after s ePochs during the training Process, where s ∈ {120, 140, 160, 180}.
All toP-1 and toP-5 accuracy on the testing data set are averaged over 5 indePendent random trails
with their standard deviations. We summarize the results in Table 3, where LSR-Pre and TSLA-Pre
indicate LSR and TSLA use the label yb based on the ImageNet Pre-trained model. The results show
that LSR-Pre/TSLA-Pre has a better Performance than LSR/TSLA. The reason might be that the
Pre-trained model-based Prediction is closer to the ground truth than the uniform Prediction and it
has lower variance (smaller δ). Then, TSLA (LSR) with such Pre-trained model-based Prediction
converges faster than TSLA (LSR) with uniform Prediction, which verifies our theoretical findings
in Section 5 (Section 4). This observation also emPirically tells us the selection of the Prediction
function yb used for smoothing label is the key to the success of TSLA as well as LSR. Among all
methods, the Performance of TSLA-Pre is the best. For toP-1 accuracy, TSLA-Pre(160) outPerforms
all other algorithms, while for toP-5 accuracy, TSLA-Pre(180) has the best Performance. Finally, we
observe from Figure 2 that both TSLA and TSLA-Pre converge, while TSLA-Pre converges to the
lowest objective value. Similarly, the toP-1 and toP-5 accuracies show the imProvements of TSLA
and TSLA-Pre at the Point of droPPing off LSR.
7 Conclusions
In this PaPer, we have studied the Power of LSR in training deeP neural networks by analyzing
SGD with LSR in different non-convex oPtimization settings. The convergence results show that an
aPProPriate LSR with reduced label variance can helP sPeed uP the convergence. We have ProPosed
a simPle and efficient strategy so-called TSLA that can incorPorate many stochastic algorithms. The
basic idea of TSLA is to switch the training from smoothed label to one-hot label. Integrating TSLA
with SGD, we observe from its imProved convergence result that TSLA benefits from LSR in the
first stage and essentially converges faster in the second stage. Throughout extensive exPeriments,
we have shown that TSLA imProves the generalization accuracy of deeP models on benchmark data
sets.
4httPs://Pytorch.org/docs/stable/torchvision/models.html
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery:
Improving imagenet classification through label progression. arXiv preprint arXiv:1805.02641,
2018.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754,
2018.
Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in
sequence to sequence models. Proc. Interspeech 2017, pp. 523-527, 2017.
Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Shu-Tao Xia. Adaptive regularization of
labels. arXiv preprint arXiv:1908.05474, 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156(1-2):59-99, 2016.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Morgane Goibert and Elvis Dohmatob. Adversarial robustness via adversarial label-smoothing.
arXiv preprint arXiv:1906.11567, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 558-567, 2019.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. 2012a.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
NeurIPS Deep Learning Workshop, 2014.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-
grained image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained Visual
Categorization (FGVC), volume 2, 2011.
9
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661-2671,
2019.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Mas-
ter’s thesis, Technical report, University of Tronto, 2009.
Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex opti-
mization. arXiv preprint arXiv:2002.05273, 2020a.
Xingjian Li, Haoyi Xiong, Haozhe An, Dejing Dou, and Chengzhong Xu. Colam: Co-
learning of deep neural networks and soft labels via alternating minimization. arXiv preprint
arXiv:2004.12443, 2020b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5564-5574, 2018.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020.
Rafael Muller, Simon Komblith, and Geoffrey E Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems, pp. 4696-4705, 2019.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1/k2). Soviet Mathematics Doklady, 27:372-376, 1983.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., 2004. ISBN 1-4020-7553-7.
Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of
self-attention. arXiv preprint arXiv:1910.05895, 2019.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial exam-
ples. In Advances in Neural Information Processing Systems, pp. 4579-4589, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024-8035, 2019. URL https://pytorch.org/docs/stable/torchvision/
models.html.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548,
2017.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
10
Under review as a conference paper at ICLR 2021
Jin-Woo Seo, Hong-Gyu Jung, and Seong-Whan Lee. Self-augmentation: Generalizing deep net-
works to unseen classes for few-shot learning. arXiv preprint arXiv:2004.00251, 2020.
Chaomin Shen, Yaxin Peng, Guixu Zhang, and Jinsong Fan. Defending against adversarial
attacks by suppressing the largest eigenvalue of fisher information matrix. arXiv preprint
arXiv:1909.06137, 2019.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of
the trade, pp. 239-274. SPringer, 1998.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the incePtion architecture for comPuter vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, PP. 2818-2826, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, PP. 5998-6008, 2017.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical rePort, 2011.
Shuo Wang, ZhaoPeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of neural
machine translation. arXiv preprint arXiv:2005.00963, 2020.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. SPiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, PP.
2403-2413, 2019.
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi Tian. Disturblabel: Regularizing
cnn on the loss layer. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, PP. 4753-4762, 2016.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momen-
tum methods for deeP learning. In International Joint Conference on Artificial Intelligence, PP.
2955-2961, 2018.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit knowledge distillation: a
teacher-free framework. arXiv preprint arXiv:1909.11723, 2019a.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over sgd. In Advances in Neural Information Processing Systems, PP. 2604-2614,
2019b.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Matthew D Zeiler. Adadelta: an adaPtive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Albert Zeyer, KazUki Irie, Ralf Schluter, and Hermann Ney. Improved training of end-to-end atten-
tion models for sPeech recognition. Proc. Interspeech 2018, PP. 7-11, 2018.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8697-8710, 2018.
11
Under review as a conference paper at ICLR 2021
A Technical Lemma
Recall that the optimization problem is
min. F(w) := E(x,y) ['(y,f (w； X))],
w∈W
where the cross-entropy loss function ` is given by
'(y,f (w； x)) = X -yi log (	"(w； X))
,；	i	K
i=1	j=1exp(fj (w； x))
(7)
(8)
If we set
p(w； x) = (Pi(w； x),...,Pk (w； x)) ∈ RK, Pi(w； x) = - log [	epf(w X))	) , (9)
j=1 exp(fj (w； X))
the problem (7) becomes
min F(w) := E(x,y) [hy, p(w； x)i] .
w∈W
Then the stochastic gradient with respective to w is
V'(y,f (w； x)) = hy, Vp(w； X).
Lemma 1. Under Assumption 1 (i), we have
E [∣∣V'(y/, f (wt；xt)) - VF(wt)∣∣2i ≤ (1 - θ)σ2 + θδσ2.
(10)
(11)
Proof. By the facts of ytLS = (1 - θ)yt + θybt and the equation in (11), we have
V'(yLS, f (wt; Xt)) = (1 - θ)V'(yt, f (wt； Xt)) + θv`(bt, f (wt; Xt)).
Therefore,
E[∣∣v'(yLS,f (wt； χt)) -VF (wt)∣∣2]
=E [k(1 — θ)[V'(yt,f(wt; Xt))- VF(wt)] + θ[V'(bt, f (wt； xt)) — VF(wt)]『]
≤)(1 - θ)E [∣∣V'(yt,f(wt; xt)) — VF(wt)k[ + θE [∣∣V'(yt, f(wt； Xt))-VF(Wt)『]
(b)
≤ (1 - θ)σ2 + θδσ2,
where (a) uses the convexity of norm, i.e., k(1 - θ)a + θbk2 ≤ (1 - θ)kak2 + θkbk2; (b) uses
assumption 1 (i) and the definitions in (4), and Assumption 1 (i).	□
B Proof of Theorem 3
Proof. By the smoothness of objective function F(w) in Assumption 1 (ii) and its remark, we have
F(wt+1) - F(wt)
≤hVF(wt), wt+ι - Wt) + IL∣∣wt+ι - wtk2
=-η (VF(wt), V'(yLS,f(wt；xt))) + * ∣∣V'(yLs,f(wt；xt))∣∣2
I= - 2 ∣VF (wt)k2 + 2 ∣∣VF (wt) - V'(yLs,f(wt ； Xt))『+ 叫- I) ∣V'(yLS ,f(wt Xt))『
≤-2 ∣VF (wt)k2 + 2 ∣∣VF (wt) - V'(yLs,f(wt ； Xt))『，	(12)
12
Under review as a conference paper at ICLR 2021
where (a) is due to the update of wt+ι; (b) is due to ha, -bi = 2 (∣∣a - b∣∣2 - ∣∣ak2 - ∣∣b∣∣2)； (C)
is due to η = L. Taking the expectation over (xt, yLS) on the both sides of (12), we have
E [F (wt+1) - F (wt)]
≤-2e h∣VF (Wt)『]+ 2e IjIVF (wt) - V'(yLS,f(wt; Xt))『]
≤-2e [kVF (wt)k2] + 2 ((1 - θ)σ2 + θδσ2) .	(13)
where the last inequality is due to Lemma 1. Then inequality (13) implies
T XI E [∣VF(wt)k2] ≤2F(w0) + (1 - θ)σ2 + θδσ2
t=0	η
=)2F (w0) + 2δ 2
=ηT +T+7 Q
≤) 2F (wo)
≤ ηT
+ 2δσ2,
where (a) is due to θ = ɪ+^; (b) is due to ɪ+^ ≤ 1.
□
C CONVERGENCE ANALYSIS OF SGD WITHOUT LSR (θ = 0)
Theorem 5. Under Assumption 1, the solutions wt from Algorithm 1 with θ = 0 satisfy
T X1 E [∣VF(wt)k2] ≤ 2F(w^ + ηLσ2.
t=0	η
In order to have Er[∣VF(wr )∣2] ≤ e2, it suffices to set η = min (1, 2^) and T = 4；(胃0) ,the
total complexity is O ).
Proof. By the smoothness of objective function F(w) in Assumption 1 (ii) and its remark, we have
F(wt+1) - F(wt)
≤hVF(wt), wt+ι - Wti + LI∣wt+1 - wtk2
=-η EF(Wt), V'(yt,f (wt; xt))i + η2L ∣V'(yt, f (wt; Xt))∣2,	(14)
where (a) is due to the update of wt+1 . Taking the expectation over (Xt; yt) on the both sides of
(14), we have
E [F(wt+1) - F(wt)]
(a)	η2 L	2
≤ - ηE [∣VF(wt)k2] + -^-E [∣∣V'(yt,f(wt;Xt))- VF(wt) + VF(wt)∣ J
= - ηE [∣VF(wt)k2] + η22LE h∣V'(yt, f(wt; Xt))- VF(Wt)『]十 号E ∣∣VF(Wt)『]
≤) - 2e [∣VF(wt)k2] + η2Lσ2.	(15)
where (a) and (b) use Assumption 1 (i); (c) uses the facts that η ≤ L and Assumption 1 (i). The
inequality (15) implies
T-1
T X E[∣VF(wt)k2] ≤ —(wɪ + ηLσ2.
t=0	η
By setting η ≤ 品 and T = 4F(WO), We have T PT-,1 E [k VF(Wt)∣2] ≤
complexity is in the order of O (e)=O (4).
e2 . Thus the total
□
13
Under review as a conference paper at ICLR 2021
D Proof of Theorem 4
Proof. Following the similar analysis of inequality (13) from the proof of Theorem 3, we have
E [F(wt+1) - F(wt)]
≤- ηE [kvF (wt)k2 ]+η ((I-O)σ2+θδσ2).
Using the condition in Assumption 2 we can simplify the inequality from (16) as
E[F(wt+ι) - F*]
≤(1 - ηιμ)E [F(wt) - F*] + η ((1 - θ)σ2 + θδσ2)
t
≤ (1 - ηιμ)t+1 E [F(W0)- F*] + η21 ((1 - θ)σ2 + θδσ2) X (1 - ηιμ∕2)i
t
≤(I- ηιμ)t+1 E [F(WO)] + η ((I- θ)σ2 + θδσ2) X(I- η1μ∕2y,
i=0
(16)
where the last inequality is due to the definition of loss function that F* ≥ 0. Since ηι ≤ L < ɪ,
then (1 - η1μ)t+1 < exp(-η1μ(t + 1)) and Pt=0 (1 - ηιμ)i ≤ nγμ. As a result, for any T11, We
have
E [F(WTI)- F*] ≤ exp(-ηιμT1)F(wo) + 2μ ((1 — θ)σ2 + θδσ2) .	(17)
Let θ = ι++δ and b2 ：= (1 一 θ)σ2 + θδσ2 = ɪ2^ σ2 then 2μ ((1 - θ)σ2 + θδσ2) ≤ F (wo) since δ
is small enough and η1L ≤ 1. By setting
T1 = log f0)) /(ηιμ)
we have
E[F(wτ1) - F*] ≤ σ2 ≤ 2δσ2.
μ μ
(18)
After T1 iterations, we drop off the label smoothing, i.e. θ = 0, then we know for any t ≥ T1,
following the inequality (15) from the proof of Theorem 5, we have
E[F (wt+1) - F (wt)] ≤-η22 E [kVF (wt)k2] + η2L2σ2
Therefore, we get
1 T1 +T2 -1	2
E-	E E [kVF(Wt)k2] ≤ʒ÷ΓE[F(WTI)- F(WTι+T2-1)]+ η2Lσ2
T2	η2T2	1	1	2
t=T1
(a)	2
≤ —不-E [F(WTI) - F*] + η2Lσ2
η2T2	1
(18) 4δσ2	2
≤	+ η2Lσ ,
μη2T2
(19)
where (a) is due to F(wT1+T2-1) ≥
⅛ PT=+T2-1 E [kVF(wt)k2] ≤ e2.
2
F*. By setting η2 = 2Lσ2 and T2
8δ 2
μσ,we have
□
14
Under review as a conference paper at ICLR 2021
E Additional Experiments
In this section, we study an ablation study for the smoothing parameter θ. We follow the same
settings in Subsection 6.2 but use different values of θ in LSR and TSLA. We summarize the results
in Table 4. The results show that the different values of θ can affect the performances of LSR and
TSLA. Besides, TSLA(180) has the best performance for each value of θ.
Table 4: Comparison of Testing Top-1 Accuracy for Different θ of LSR and TSLA (in %).
θ	LSR	TSLA(120)	TSLA(140)	TSLA(160)	TSLA(180)
0.2	77.75	77.92	78.06	78.09	78.10
0.4	77.72	77.68	77.61	77.54	77.89
0.9	76.40	76.05	76.27	76.37	76.60
*TSLA(s): TSLA drops offLSR after epoch s.
15